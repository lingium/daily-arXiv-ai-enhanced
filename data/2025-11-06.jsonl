{"id": "2511.01925", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.01925", "abs": "https://arxiv.org/abs/2511.01925", "authors": ["Safa' Alsheyab"], "title": "A new stochastic diffusion process to model and predict electricity production from natural gas sources in the United States", "comment": null, "summary": "This paper introduces a new stochastic diffusion process to model the\nelectricity production from natural gas sources (as a percentage of total\nelectricity production) in the United States. The method employs trend function\nanalysis to generate fits and forecasts with both conditional and unconditional\nestimated trend functions. Parameters are estimated using the maximum\nlikelihood (ML) method, based on discrete sampling paths of the variable\n\"electricity production from natural gas sources in the United States\" with\nannual data from 1990 to 2021. The results show that the proposed model\neffectively fits the data and provides dependable medium-term forecasts for\n2022-2023."}
{"id": "2511.02102", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.02102", "abs": "https://arxiv.org/abs/2511.02102", "authors": ["Melanie Mayer", "Kimberly Lactaoen", "Gary E. Weissman", "Blanca E. Himes", "Rebecca A. Hubbard"], "title": "Enhancing Phenotype Discovery in Electronic Health Records through Prior Knowledge-Guided Unsupervised Learning", "comment": "Submitted to JAMIA; preprint is the author's original version. Github\n  repo: https://github.com/mm4963/prior-guided-EHR-phenotyping/tree/main", "summary": "Objectives: Unsupervised learning with electronic health record (EHR) data\nhas shown promise for phenotype discovery, but approaches typically disregard\nexisting clinical information, limiting interpretability. We operationalize a\nBayesian latent class framework for phenotyping that incorporates\ndomain-specific knowledge to improve clinical meaningfulness of EHR-derived\nphenotypes and illustrate its utility by identifying an asthma sub-phenotype\ninformed by features of Type 2 (T2) inflammation.\n  Materials and methods: We illustrate a framework for incorporating clinical\nknowledge into a Bayesian latent class model via informative priors to guide\nunsupervised clustering toward clinically relevant subgroups. This approach\nmodels missingness, accounting for potential missing-not-at-random patterns,\nand provides patient-level probabilities for phenotype assignment with\nuncertainty. Using reusable and flexible code, we applied the model to a large\nasthma EHR cohort, specifying informative priors for T2 inflammation-related\nfeatures and weakly informative priors for other clinical variables, allowing\nthe data to inform posterior distributions.\n  Results and Conclusion: Using encounter data from January 2017 to February\n2024 for 44,642 adult asthma patients, we found a bimodal posterior\ndistribution of phenotype assignment, indicating clear class separation. The T2\ninflammation-informed class (38.7%) was characterized by elevated eosinophil\nlevels and allergy markers, plus high healthcare utilization and medication\nuse, despite weakly informative priors on the latter variables. These patterns\nsuggest an \"uncontrolled T2-high\" sub-phenotype. This demonstrates how our\nBayesian latent class modeling approach supports hypothesis generation and\ncohort identification in EHR-based studies of heterogeneous diseases without\nwell-established phenotype definitions."}
{"id": "2511.02174", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.02174", "abs": "https://arxiv.org/abs/2511.02174", "authors": ["Jack Kissell", "Vijini Lakmini", "Brani Vidakovic"], "title": "Wavelet Based Cross Correlations with Applications", "comment": null, "summary": "Wavelet Transforms are a widely used technique for decomposing a signal into\ncoefficient vectors that correspond to distinct frequency/scale bands while\nretaining time localization. This property enables an adaptive analysis of\nsignals at different scales, capturing both temporal and spectral patterns. By\nexamining how correlations between two signals vary across these scales, we\nobtain a more nuanced understanding of their relationship than what is possible\nfrom a single global correlation measure. In this work, we expand on the theory\nof wavelet-based correlations already used in the literature and elaborate on\nwavelet correlograms, partial wavelet correlations, and additive wavelet\ncorrelations using the Pearson and Kendall definitions. We use both Orthogonal\nand Non-decimated discrete Wavelet Transforms, and assess the robustness of\nthese correlations under different wavelet bases. Simulation studies are\nconducted to illustrate these methods, and we conclude with applications to\nreal-world datasets."}
{"id": "2511.02289", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.02289", "abs": "https://arxiv.org/abs/2511.02289", "authors": ["Gaurav Kottari", "Qazi J. Azhad", "Niteesh Sahni"], "title": "A generic network theoretic based model to classify SDG indicators", "comment": "22 pages", "summary": "To achieve the United Nations Sustainable Development Goals, coordinated\naction across their interlinked indicators is required. Although most of the\nresearch on the interlinkages of the SDGs is done at the goal level, policies\nare usually made and implemented at the level of indicators (or targets). Our\nstudy examines the existing literature on SDG interlinkages and indicator (or\ntarget) prioritization, highlighting important drawbacks of current\nmethodologies. To address these limitations, we propose a generic network-based\nmodel that can quantify the importance of the SDG indicators and help\npolicymakers in identifying indicators for maximum synergistic impact. Our\nmodel applies to any country, offering a tool for national policymakers. We\nillustrate the application of this model using data from India, identifying\nimportant indicators that are crucial for accelerating progress in the SDGs.\nWhile our main contribution lies in developing this network-theoretic\nmethodology, we also provide supporting empirical evidence from existing\nliterature for selected key observations."}
{"id": "2511.02053", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.02053", "abs": "https://arxiv.org/abs/2511.02053", "authors": ["Jinchao Feng", "Charles Kulick", "Sui Tang"], "title": "Data-driven Learning of Interaction Laws in Multispecies Particle Systems with Gaussian Processes: Convergence Theory and Applications", "comment": "40 pages, Appendix 17 pages", "summary": "We develop a Gaussian process framework for learning interaction kernels in\nmulti-species interacting particle systems from trajectory data. Such systems\nprovide a canonical setting for multiscale modeling, where simple microscopic\ninteraction rules generate complex macroscopic behaviors. While our earlier\nwork established a Gaussian process approach and convergence theory for\nsingle-species systems, and later extended to second-order models with\nalignment and energy-type interactions, the multi-species setting introduces\nnew challenges: heterogeneous populations interact both within and across\nspecies, the number of unknown kernels grows, and asymmetric interactions such\nas predator-prey dynamics must be accommodated. We formulate the learning\nproblem in a nonparametric Bayesian setting and establish rigorous statistical\nguarantees. Our analysis shows recoverability of the interaction kernels,\nprovides quantitative error bounds, and proves statistical optimality of\nposterior estimators, thereby unifying and generalizing previous single-species\ntheory. Numerical experiments confirm the theoretical predictions and\ndemonstrate the effectiveness of the proposed approach, highlighting its\nadvantages over existing kernel-based methods. This work contributes a complete\nstatistical framework for data-driven inference of interaction laws in\nmulti-species systems, advancing the broader multiscale modeling program of\nconnecting microscopic particle dynamics with emergent macroscopic behavior."}
{"id": "2511.01875", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01875", "abs": "https://arxiv.org/abs/2511.01875", "authors": ["Deborah Sulem", "Jack Jewson", "David Rossell"], "title": "Bayesian computation for high-dimensional Gaussian Graphical Models with spike-and-slab priors", "comment": null, "summary": "Gaussian graphical models are widely used to infer dependence structures.\nBayesian methods are appealing to quantify uncertainty associated with\nstructural learning, i.e., the plausibility of conditional independence\nstatements given the data, and parameter estimates. However, computational\ndemands have limited their application when the number of variables is large,\nwhich prompted the use of pseudo-Bayesian approaches. We propose fully Bayesian\nalgorithms that provably scale to high dimensions when the data-generating\nprecision matrix is sparse, at a similar cost to the best pseudo-Bayesian\nmethods. First, a Metropolis-Hastings-within-Block-Gibbs algorithm that allows\nrow-wise updates of the precision matrix, using local moves. Second, a global\nproposal that enables adding or removing multiple edges within a row, which can\nhelp explore multi-modal posteriors. We obtain spectral gap bounds for both\nsamplers that are dimension-free under suitable settings. We also provide\nworst-case polynomial bounds on per-iteration costs, though in practice the\ncost is lower by using sparse linear algebra. Our examples show that the\nmethods extend the applicability of exact Bayesian inference from roughly 100\nto roughly 1000 variables (equivalently, from 5,000 edges to 500,000 edges)."}
{"id": "2511.02053", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.02053", "abs": "https://arxiv.org/abs/2511.02053", "authors": ["Jinchao Feng", "Charles Kulick", "Sui Tang"], "title": "Data-driven Learning of Interaction Laws in Multispecies Particle Systems with Gaussian Processes: Convergence Theory and Applications", "comment": "40 pages, Appendix 17 pages", "summary": "We develop a Gaussian process framework for learning interaction kernels in\nmulti-species interacting particle systems from trajectory data. Such systems\nprovide a canonical setting for multiscale modeling, where simple microscopic\ninteraction rules generate complex macroscopic behaviors. While our earlier\nwork established a Gaussian process approach and convergence theory for\nsingle-species systems, and later extended to second-order models with\nalignment and energy-type interactions, the multi-species setting introduces\nnew challenges: heterogeneous populations interact both within and across\nspecies, the number of unknown kernels grows, and asymmetric interactions such\nas predator-prey dynamics must be accommodated. We formulate the learning\nproblem in a nonparametric Bayesian setting and establish rigorous statistical\nguarantees. Our analysis shows recoverability of the interaction kernels,\nprovides quantitative error bounds, and proves statistical optimality of\nposterior estimators, thereby unifying and generalizing previous single-species\ntheory. Numerical experiments confirm the theoretical predictions and\ndemonstrate the effectiveness of the proposed approach, highlighting its\nadvantages over existing kernel-based methods. This work contributes a complete\nstatistical framework for data-driven inference of interaction laws in\nmulti-species systems, advancing the broader multiscale modeling program of\nconnecting microscopic particle dynamics with emergent macroscopic behavior."}
{"id": "2511.02156", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.02156", "abs": "https://arxiv.org/abs/2511.02156", "authors": ["Xunmo Yang", "Taylor Pospisil", "Omkar Muralidharan", "Dennis L. Sun"], "title": "A Grammar of Data Analysis", "comment": null, "summary": "This paper outlines a grammar of data analysis, as distinct from grammars of\ndata manipulation, in which the primitives are metrics and dimensions. We\ndescribe a Python implementation of this grammar called Meterstick, which is\nagnostic to the underlying data source, which may be a DataFrame or a SQL\ndatabase."}
{"id": "2511.02509", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.02509", "abs": "https://arxiv.org/abs/2511.02509", "authors": ["R. Alberich", "N. A. Cruz", "R. Fernández", "I. García Mosquera", "A. Mir", "F. Roselló"], "title": "Identification of Separable OTUs for Multinomial Classification in Compositional Data Analysis", "comment": null, "summary": "High-throughput sequencing has transformed microbiome research, but it also\nproduces inherently compositional data that challenge standard statistical and\nmachine learning methods. In this work, we propose a multinomial classification\nframework for compositional microbiome data based on penalized log-ratio\nregression and pairwise separability screening. The method quantifies the\ndiscriminative ability of each OTU through the area under the receiver\noperating characteristic curve ($AUC$) for all pairwise log-ratios and\naggregates these values into a global separability index $S_k$, yielding\ninterpretable rankings of taxa together with confidence intervals. We\nillustrate the approach by reanalyzing the Baxter colorectal adenoma dataset\nand comparing our results with Greenacre's ordination-based analysis using\nCorrespondence Analysis and Canonical Correspondence Analysis. Our models\nconsistently recover a core subset of taxa previously identified as\ndiscriminant, thereby corroborating Greenacre's main findings, while also\nrevealing additional OTUs that become important once demographic covariates are\ntaken into account. In particular, adjustment for age, gender, and diabetes\nmedication improves the precision of the separation index and highlights new,\npotentially relevant taxa, suggesting that part of the original signal may have\nbeen influenced by confounding. Overall, the integration of log-ratio modeling,\ncovariate adjustment, and uncertainty estimation provides a robust and\ninterpretable framework for OTU selection in compositional microbiome data. The\nproposed method complements existing ordination-based approaches by adding a\nprobabilistic and inferential perspective, strengthening the identification of\nbiologically meaningful microbial signatures."}
{"id": "2511.02258", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.02258", "abs": "https://arxiv.org/abs/2511.02258", "authors": ["Parsa Rangriz"], "title": "Limit Theorems for Stochastic Gradient Descent in High-Dimensional Single-Layer Networks", "comment": null, "summary": "This paper studies the high-dimensional scaling limits of online stochastic\ngradient descent (SGD) for single-layer networks. Building on the seminal work\nof Saad and Solla, which analyzed the deterministic (ballistic) scaling limits\nof SGD corresponding to the gradient flow of the population loss, we focus on\nthe critical scaling regime of the step size. Below this critical scale, the\neffective dynamics are governed by ballistic (ODE) limits, but at the critical\nscale, new correction term appears that changes the phase diagram. In this\nregime, near the fixed points, the corresponding diffusive (SDE) limits of the\neffective dynamics reduces to an Ornstein-Uhlenbeck process under certain\nconditions. These results highlight how the information exponent controls\nsample complexity and illustrates the limitations of deterministic scaling\nlimit in capturing the stochastic fluctuations of high-dimensional learning\ndynamics."}
{"id": "2511.02149", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.02149", "abs": "https://arxiv.org/abs/2511.02149", "authors": ["Yovna Junglee", "Vianey Leos Barajas", "Meredith Franklin"], "title": "Bayesian spatio-temporal weighted regression for integrating missing and misaligned environmental data", "comment": "16 pages, 6 figures", "summary": "Estimating environmental exposures from multi-source data is central to\npublic health research and policy. Integrating data from satellite products and\nground monitors are increasingly used to produce exposure surfaces. However,\nspatio-temporal misalignment often induced from missing data introduces\nsubstantial uncertainty and reduces predictive accuracy. We propose a Bayesian\nweighted predictor regression framework that models spatio-temporal\nrelationships when predictors are observed on irregular supports or have\nsubstantial missing data, and are not concurrent with the outcome. The key\nfeature of our model is a spatio-temporal kernel that aggregates the predictor\nover local space-time neighborhoods, built directly into the likelihood,\neliminating any separate gap-filling or forced data alignment stage. We\nintroduce a numerical approximation using a Voronoi-based spatial quadrature\ncombined with irregular temporal increments for estimation under data\nmissingness and misalignment. We showed that misspecification of the spatial\nand temporal lags induced bias in the mean and parameter estimates, indicating\nthe need for principled parameter selection. Simulation studies confirmed these\ntheoretical findings, where careful tuning was critical to control bias and\nachieve accurate prediction, while the proposed quadrature performed well under\nsevere missingness. As an illustrative application, we estimated fine\nparticulate matter (PM$_{2.5}$) in northern California using satellite-derived\naerosol optical depth (AOD) and wildfire smoke plume indicators. Relative to a\ntraditional collocated linear model, our approach improved out-of-sample\npredictive performance (over 50\\% increase in R$^2$), reduced uncertainty, and\nyielded robust temporal predictions and spatial surface estimation. Our\nframework is extensible to additional spatio-temporally varying covariates and\nother kernel families."}
{"id": "2511.02137", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.02137", "abs": "https://arxiv.org/abs/2511.02137", "authors": ["Dongze Wu", "Feng Qiu", "Yao Xie"], "title": "DoFlow: Causal Generative Flows for Interventional and Counterfactual Time-Series Prediction", "comment": null, "summary": "Time-series forecasting increasingly demands not only accurate observational\npredictions but also causal forecasting under interventional and counterfactual\nqueries in multivariate systems. We present DoFlow, a flow based generative\nmodel defined over a causal DAG that delivers coherent observational and\ninterventional predictions, as well as counterfactuals through the natural\nencoding and decoding mechanism of continuous normalizing flows (CNFs). We also\nprovide a supporting counterfactual recovery result under certain assumptions.\nBeyond forecasting, DoFlow provides explicit likelihoods of future\ntrajectories, enabling principled anomaly detection. Experiments on synthetic\ndatasets with various causal DAG and real world hydropower and cancer treatment\ntime series show that DoFlow achieves accurate system-wide observational\nforecasting, enables causal forecasting over interventional and counterfactual\nqueries, and effectively detects anomalies. This work contributes to the\nbroader goal of unifying causal reasoning and generative modeling for complex\ndynamical systems."}
{"id": "2511.02430", "categories": ["stat.CO", "cs.MS", "cs.SE", "stat.ML", "62-04", "G.3; D.2.13"], "pdf": "https://arxiv.org/pdf/2511.02430", "abs": "https://arxiv.org/abs/2511.02430", "authors": ["Johan Larsson", "Malgorzata Bogdan", "Krystyna Grzesiak", "Mathurin Massias", "Jonas Wallin"], "title": "Efficient Solvers for SLOPE in R, Python, Julia, and C++", "comment": "30 pages, 8 figures", "summary": "We present a suite of packages in R, Python, Julia, and C++ that efficiently\nsolve the Sorted L-One Penalized Estimation (SLOPE) problem. The packages\nfeature a highly efficient hybrid coordinate descent algorithm that fits\ngeneralized linear models (GLMs) and supports a variety of loss functions,\nincluding Gaussian, binomial, Poisson, and multinomial logistic regression. Our\nimplementation is designed to be fast, memory-efficient, and flexible. The\npackages support a variety of data structures (dense, sparse, and out-of-memory\nmatrices) and are designed to efficiently fit the full SLOPE path as well as\nhandle cross-validation of SLOPE models, including the relaxed SLOPE. We\npresent examples of how to use the packages and benchmarks that demonstrate the\nperformance of the packages on both real and simulated data and show that our\npackages outperform existing implementations of SLOPE in terms of speed."}
{"id": "2511.02682", "categories": ["stat.AP", "math.DG", "stat.CO", "62R30 (Primary), 53Z30 (Secondary)"], "pdf": "https://arxiv.org/pdf/2511.02682", "abs": "https://arxiv.org/abs/2511.02682", "authors": ["Jordi-Lluís Figueras", "Aron Persson", "Lauri Viitasaari"], "title": "Extended Kalman Filtering on Stiefel Manifolds", "comment": null, "summary": "A generalisation of the extended Kalman filter for Stiefel manifold-valued\nmeasurements is presented. We provide simulations on the 2-sphere and the space\nof orthogonal 4-by-2 matrices which show significant improvement of the\nExtended Kalman Filter compared to only relying on raw measurements."}
{"id": "2511.02199", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.02199", "abs": "https://arxiv.org/abs/2511.02199", "authors": ["Seong-ho Lee", "Yongho Jeon"], "title": "DOD: Detection of outliers in high dimensional data with distance of distances", "comment": null, "summary": "Reliable outlier detection in high-dimensional data is crucial in modern\nscience, yet it remains a challenging task. Traditional methods often break\ndown in these settings due to their reliance on asymptotic behaviors with\nrespect to sample size under fixed dimension. Furthermore, many modern\nalternatives introduce sophisticated statistical treatments and computational\ncomplexities. To overcome these issues, our approach leverages intuitive\ngeometric properties of high-dimensional space, effectively turning the curse\nof dimensionality into an advantage. We propose two new outlyingness statistics\nbased on observation's relational patterns with all other points, measured via\npairwise distances or inner products. We establish a theoretical foundation for\nour statistics demonstrating that as the dimension grows, our statistics create\na non-vanishing margin that asymptotically separates outliers from\nnon-outliers. Based on this foundation, we develop practical outlier detection\nprocedures, including a simple clustering-based algorithm and a\ndistribution-free test using random rotations. Through simulation experiments\nand real data applications, we demonstrate that our proposed methods achieve a\nsuperior balance between detection power and false positive control,\noutperforming existing methods and establishing their practical utility in\nhigh-dimensional settings."}
{"id": "2511.02258", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.02258", "abs": "https://arxiv.org/abs/2511.02258", "authors": ["Parsa Rangriz"], "title": "Limit Theorems for Stochastic Gradient Descent in High-Dimensional Single-Layer Networks", "comment": null, "summary": "This paper studies the high-dimensional scaling limits of online stochastic\ngradient descent (SGD) for single-layer networks. Building on the seminal work\nof Saad and Solla, which analyzed the deterministic (ballistic) scaling limits\nof SGD corresponding to the gradient flow of the population loss, we focus on\nthe critical scaling regime of the step size. Below this critical scale, the\neffective dynamics are governed by ballistic (ODE) limits, but at the critical\nscale, new correction term appears that changes the phase diagram. In this\nregime, near the fixed points, the corresponding diffusive (SDE) limits of the\neffective dynamics reduces to an Ornstein-Uhlenbeck process under certain\nconditions. These results highlight how the information exponent controls\nsample complexity and illustrates the limitations of deterministic scaling\nlimit in capturing the stochastic fluctuations of high-dimensional learning\ndynamics."}
{"id": "2511.02149", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.02149", "abs": "https://arxiv.org/abs/2511.02149", "authors": ["Yovna Junglee", "Vianey Leos Barajas", "Meredith Franklin"], "title": "Bayesian spatio-temporal weighted regression for integrating missing and misaligned environmental data", "comment": "16 pages, 6 figures", "summary": "Estimating environmental exposures from multi-source data is central to\npublic health research and policy. Integrating data from satellite products and\nground monitors are increasingly used to produce exposure surfaces. However,\nspatio-temporal misalignment often induced from missing data introduces\nsubstantial uncertainty and reduces predictive accuracy. We propose a Bayesian\nweighted predictor regression framework that models spatio-temporal\nrelationships when predictors are observed on irregular supports or have\nsubstantial missing data, and are not concurrent with the outcome. The key\nfeature of our model is a spatio-temporal kernel that aggregates the predictor\nover local space-time neighborhoods, built directly into the likelihood,\neliminating any separate gap-filling or forced data alignment stage. We\nintroduce a numerical approximation using a Voronoi-based spatial quadrature\ncombined with irregular temporal increments for estimation under data\nmissingness and misalignment. We showed that misspecification of the spatial\nand temporal lags induced bias in the mean and parameter estimates, indicating\nthe need for principled parameter selection. Simulation studies confirmed these\ntheoretical findings, where careful tuning was critical to control bias and\nachieve accurate prediction, while the proposed quadrature performed well under\nsevere missingness. As an illustrative application, we estimated fine\nparticulate matter (PM$_{2.5}$) in northern California using satellite-derived\naerosol optical depth (AOD) and wildfire smoke plume indicators. Relative to a\ntraditional collocated linear model, our approach improved out-of-sample\npredictive performance (over 50\\% increase in R$^2$), reduced uncertainty, and\nyielded robust temporal predictions and spatial surface estimation. Our\nframework is extensible to additional spatio-temporally varying covariates and\nother kernel families."}
{"id": "2511.02227", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.02227", "abs": "https://arxiv.org/abs/2511.02227", "authors": ["Hsuan-Chen Kao", "Jerome P. Reiter"], "title": "Interval Estimation for Binomial Proportions Under Differential Privacy", "comment": null, "summary": "When releasing binary proportions computed using sensitive data, several\ngovernment agencies and other data stewards protect confidentiality of the\nunderlying values by ensuring the released statistics satisfy differential\nprivacy. Typically, this is done by adding carefully chosen noise to the sample\nproportion computed using the confidential data. In this article, we describe\nand compare methods for turning this differentially private proportion into an\ninterval estimate for an underlying population probability. Specifically, we\nconsider differentially private versions of the Wald and Wilson intervals,\nBayesian credible intervals based on denoising the differentially private\nproportion, and an exact interval motivated by the Clopper-Pearson confidence\ninterval. We examine the repeated sampling performances of the intervals using\nsimulation studies under both the Laplace mechanism and discrete Gaussian\nmechanism across a range of privacy guarantees. We find that while several\nmethods can offer reasonable performances, the Bayesian credible intervals are\nthe most attractive."}
{"id": "2511.02373", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.02373", "abs": "https://arxiv.org/abs/2511.02373", "authors": ["Jean-Baptiste Courbot", "Hugo Gangloff", "Bruno Colicchio"], "title": "A new class of Markov random fields enabling lightweight sampling", "comment": null, "summary": "This work addresses the problem of efficient sampling of Markov random fields\n(MRF). The sampling of Potts or Ising MRF is most often based on Gibbs\nsampling, and is thus computationally expensive. We consider in this work how\nto circumvent this bottleneck through a link with Gaussian Markov Random\nfields. The latter can be sampled in several cost-effective ways, and we\nintroduce a mapping from real-valued GMRF to discrete-valued MRF. The resulting\nnew class of MRF benefits from a few theoretical properties that validate the\nnew model. Numerical results show the drastic performance gain in terms of\ncomputational efficiency, as we sample at least 35x faster than Gibbs sampling\nusing at least 37x less energy, all the while exhibiting empirical properties\nclose to classical MRFs."}
{"id": "2511.02235", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.02235", "abs": "https://arxiv.org/abs/2511.02235", "authors": ["Bin Chen", "Yuefeng Han", "Qiyang Yu"], "title": "Diffusion Index Forecast with Tensor Data", "comment": null, "summary": "In this paper, we consider diffusion index forecast with both tensor and\nnon-tensor predictors, where the tensor structure is preserved with a Canonical\nPolyadic (CP) tensor factor model. When the number of non-tensor predictors is\nsmall, we study the asymptotic properties of the least-squared estimator in\nthis tensor factor-augmented regression, allowing for factors with different\nstrengths. We derive an analytical formula for prediction intervals that\naccounts for the estimation uncertainty of the latent factors. In addition, we\npropose a novel thresholding estimator for the high-dimensional covariance\nmatrix that is robust to cross-sectional dependence. When the number of\nnon-tensor predictors exceeds or diverges with the sample size, we introduce a\nmulti-source factor-augmented sparse regression model and establish the\nconsistency of the corresponding penalized estimator. Simulation studies\nvalidate our theoretical results and an empirical application to US trade flows\ndemonstrates the advantages of our approach over other popular methods in the\nliterature."}
{"id": "2511.02452", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02452", "abs": "https://arxiv.org/abs/2511.02452", "authors": ["Junghee Pyeon", "Davide Cacciarelli", "Kamran Paynabar"], "title": "An Adaptive Sampling Framework for Detecting Localized Concept Drift under Label Scarcity", "comment": null, "summary": "Concept drift and label scarcity are two critical challenges limiting the\nrobustness of predictive models in dynamic industrial environments. Existing\ndrift detection methods often assume global shifts and rely on dense\nsupervision, making them ill-suited for regression tasks with local drifts and\nlimited labels. This paper proposes an adaptive sampling framework that\ncombines residual-based exploration and exploitation with EWMA monitoring to\nefficiently detect local concept drift under labeling budget constraints.\nEmpirical results on synthetic benchmarks and a case study on electricity\nmarket demonstrate superior performance in label efficiency and drift detection\naccuracy."}
{"id": "2511.02306", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.02306", "abs": "https://arxiv.org/abs/2511.02306", "authors": ["Mahdi Nouraie", "Houying Zhu", "Samuel Muller"], "title": "A Stable Lasso", "comment": null, "summary": "The Lasso has been widely used as a method for variable selection, valued for\nits simplicity and empirical performance. However, Lasso's selection stability\ndeteriorates in the presence of correlated predictors. Several approaches have\nbeen developed to mitigate this limitation. In this paper, we provide a brief\nreview of existing approaches, highlighting their limitations. We then propose\na simple technique to improve the selection stability of Lasso by integrating a\nweighting scheme into the Lasso penalty function, where the weights are defined\nas an increasing function of a correlation-adjusted ranking that reflects the\npredictive power of predictors. Empirical evaluations on both simulated and\nreal-world datasets demonstrate the efficacy of the proposed method. Additional\nnumerical results demonstrate the effectiveness of the proposed approach in\nstabilizing other regularization-based selection methods, indicating its\npotential as a general-purpose solution."}
{"id": "2511.02706", "categories": ["stat.ML", "cs.CG", "cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.02706", "abs": "https://arxiv.org/abs/2511.02706", "authors": ["Deyao Chen", "François Clément", "Carola Doerr", "Nathan Kirk"], "title": "Optimizing Kernel Discrepancies via Subset Selection", "comment": null, "summary": "Kernel discrepancies are a powerful tool for analyzing worst-case errors in\nquasi-Monte Carlo (QMC) methods. Building on recent advances in optimizing such\ndiscrepancy measures, we extend the subset selection problem to the setting of\nkernel discrepancies, selecting an m-element subset from a large population of\nsize $n \\gg m$. We introduce a novel subset selection algorithm applicable to\ngeneral kernel discrepancies to efficiently generate low-discrepancy samples\nfrom both the uniform distribution on the unit hypercube, the traditional\nsetting of classical QMC, and from more general distributions $F$ with known\ndensity functions by employing the kernel Stein discrepancy. We also explore\nthe relationship between the classical $L_2$ star discrepancy and its\n$L_\\infty$ counterpart."}
{"id": "2511.02551", "categories": ["stat.ME", "stat.CO", "62H05, 62H11, 62M30"], "pdf": "https://arxiv.org/pdf/2511.02551", "abs": "https://arxiv.org/abs/2511.02551", "authors": ["Alan Pearse", "David Gunawan", "Noel Cressie"], "title": "Bayesian copula-based spatial random effects models for inference with complex spatial data", "comment": "Main text: 21 pages, 6 figures, 1 table. Supplement: 36 pages, 14\n  figures, 9 tables", "summary": "In this article, we develop fully Bayesian, copula-based, spatial-statistical\nmodels for large, noisy, incomplete, and non-Gaussian spatial data. Our\napproach includes novel constructions of copulas that accommodate a\nspatial-random-effects structure, enabling low-rank representations and\ncomputationally efficient Bayesian inference. The spatial copula is used in a\nlatent process model of the Bayesian hierarchical spatial-statistical model,\nand, conditional on the latent copula-based spatial process, the data model\nhandles measurement errors and missing data. Our simulation studies show that a\nfully Bayesian approach delivers accurate and fast inference for both parameter\nestimation and spatial-process prediction, outperforming several benchmark\nmethods, including fixed rank kriging (FRK). The new class of copula-based\nmodels is used to map atmospheric methane in the Bowen Basin, Queensland,\nAustralia, from Sentinel 5P satellite data."}
{"id": "2511.02102", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.02102", "abs": "https://arxiv.org/abs/2511.02102", "authors": ["Melanie Mayer", "Kimberly Lactaoen", "Gary E. Weissman", "Blanca E. Himes", "Rebecca A. Hubbard"], "title": "Enhancing Phenotype Discovery in Electronic Health Records through Prior Knowledge-Guided Unsupervised Learning", "comment": "Submitted to JAMIA; preprint is the author's original version. Github\n  repo: https://github.com/mm4963/prior-guided-EHR-phenotyping/tree/main", "summary": "Objectives: Unsupervised learning with electronic health record (EHR) data\nhas shown promise for phenotype discovery, but approaches typically disregard\nexisting clinical information, limiting interpretability. We operationalize a\nBayesian latent class framework for phenotyping that incorporates\ndomain-specific knowledge to improve clinical meaningfulness of EHR-derived\nphenotypes and illustrate its utility by identifying an asthma sub-phenotype\ninformed by features of Type 2 (T2) inflammation.\n  Materials and methods: We illustrate a framework for incorporating clinical\nknowledge into a Bayesian latent class model via informative priors to guide\nunsupervised clustering toward clinically relevant subgroups. This approach\nmodels missingness, accounting for potential missing-not-at-random patterns,\nand provides patient-level probabilities for phenotype assignment with\nuncertainty. Using reusable and flexible code, we applied the model to a large\nasthma EHR cohort, specifying informative priors for T2 inflammation-related\nfeatures and weakly informative priors for other clinical variables, allowing\nthe data to inform posterior distributions.\n  Results and Conclusion: Using encounter data from January 2017 to February\n2024 for 44,642 adult asthma patients, we found a bimodal posterior\ndistribution of phenotype assignment, indicating clear class separation. The T2\ninflammation-informed class (38.7%) was characterized by elevated eosinophil\nlevels and allergy markers, plus high healthcare utilization and medication\nuse, despite weakly informative priors on the latter variables. These patterns\nsuggest an \"uncontrolled T2-high\" sub-phenotype. This demonstrates how our\nBayesian latent class modeling approach supports hypothesis generation and\ncohort identification in EHR-based studies of heterogeneous diseases without\nwell-established phenotype definitions."}
{"id": "2511.02632", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.02632", "abs": "https://arxiv.org/abs/2511.02632", "authors": ["Taehyeon Koo", "Zijian Guo"], "title": "Distributionally Robust Synthetic Control: Ensuring Robustness Against Highly Correlated Controls and Weight Shifts", "comment": null, "summary": "The synthetic control method estimates the causal effect by comparing the\noutcomes of a treated unit to a weighted average of control units that closely\nmatch the pre-treatment outcomes of the treated unit. This method presumes that\nthe relationship between the potential outcomes of the treated and control\nunits remains consistent before and after treatment. However, the estimator may\nbecome unreliable when these relationships shift or when control units are\nhighly correlated. To address these challenges, we introduce the\nDistributionally Robust Synthetic Control (DRoSC) method by accommodating\npotential shifts in relationships and addressing high correlations among\ncontrol units. The DRoSC method targets a new causal estimand defined as the\noptimizer of a worst-case optimization problem that checks through all possible\nsynthetic weights that comply with the pre-treatment period. When the\nidentification conditions for the classical synthetic control method hold, the\nDRoSC method targets the same causal effect as the synthetic control. When\nthese conditions are violated, we show that this new causal estimand is a\nconservative proxy of the non-identifiable causal effect. We further show that\nthe limiting distribution of the DRoSC estimator is non-normal and propose a\nnovel inferential approach to characterize this non-normal limiting\ndistribution. We demonstrate its finite-sample performance through numerical\nstudies and an analysis of the economic impact of terrorism in the Basque\nCountry."}
{"id": "2511.02306", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.02306", "abs": "https://arxiv.org/abs/2511.02306", "authors": ["Mahdi Nouraie", "Houying Zhu", "Samuel Muller"], "title": "A Stable Lasso", "comment": null, "summary": "The Lasso has been widely used as a method for variable selection, valued for\nits simplicity and empirical performance. However, Lasso's selection stability\ndeteriorates in the presence of correlated predictors. Several approaches have\nbeen developed to mitigate this limitation. In this paper, we provide a brief\nreview of existing approaches, highlighting their limitations. We then propose\na simple technique to improve the selection stability of Lasso by integrating a\nweighting scheme into the Lasso penalty function, where the weights are defined\nas an increasing function of a correlation-adjusted ranking that reflects the\npredictive power of predictors. Empirical evaluations on both simulated and\nreal-world datasets demonstrate the efficacy of the proposed method. Additional\nnumerical results demonstrate the effectiveness of the proposed approach in\nstabilizing other regularization-based selection methods, indicating its\npotential as a general-purpose solution."}
{"id": "2511.02754", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02754", "abs": "https://arxiv.org/abs/2511.02754", "authors": ["Zebin Wang", "Ziming Gan", "Weijing Tang", "Zongqi Xia", "Tianrun Cai", "Tianxi Cai", "Junwei Lu"], "title": "DANIEL: A Distributed and Scalable Approach for Global Representation Learning with EHR Applications", "comment": null, "summary": "Classical probabilistic graphical models face fundamental challenges in\nmodern data environments, which are characterized by high dimensionality,\nsource heterogeneity, and stringent data-sharing constraints. In this work, we\nrevisit the Ising model, a well-established member of the Markov Random Field\n(MRF) family, and develop a distributed framework that enables scalable and\nprivacy-preserving representation learning from large-scale binary data with\ninherent low-rank structure. Our approach optimizes a non-convex surrogate loss\nfunction via bi-factored gradient descent, offering substantial computational\nand communication advantages over conventional convex approaches. We evaluate\nour algorithm on multi-institutional electronic health record (EHR) datasets\nfrom 58,248 patients across the University of Pittsburgh Medical Center (UPMC)\nand Mass General Brigham (MGB), demonstrating superior performance in global\nrepresentation learning and downstream clinical tasks, including relationship\ndetection, patient phenotyping, and patient clustering. These results highlight\na broader potential for statistical inference in federated, high-dimensional\nsettings while addressing the practical challenges of data complexity and\nmulti-institutional integration."}
{"id": "2511.02430", "categories": ["stat.CO", "cs.MS", "cs.SE", "stat.ML", "62-04", "G.3; D.2.13"], "pdf": "https://arxiv.org/pdf/2511.02430", "abs": "https://arxiv.org/abs/2511.02430", "authors": ["Johan Larsson", "Malgorzata Bogdan", "Krystyna Grzesiak", "Mathurin Massias", "Jonas Wallin"], "title": "Efficient Solvers for SLOPE in R, Python, Julia, and C++", "comment": "30 pages, 8 figures", "summary": "We present a suite of packages in R, Python, Julia, and C++ that efficiently\nsolve the Sorted L-One Penalized Estimation (SLOPE) problem. The packages\nfeature a highly efficient hybrid coordinate descent algorithm that fits\ngeneralized linear models (GLMs) and supports a variety of loss functions,\nincluding Gaussian, binomial, Poisson, and multinomial logistic regression. Our\nimplementation is designed to be fast, memory-efficient, and flexible. The\npackages support a variety of data structures (dense, sparse, and out-of-memory\nmatrices) and are designed to efficiently fit the full SLOPE path as well as\nhandle cross-validation of SLOPE models, including the relaxed SLOPE. We\npresent examples of how to use the packages and benchmarks that demonstrate the\nperformance of the packages on both real and simulated data and show that our\npackages outperform existing implementations of SLOPE in terms of speed."}
{"id": "2511.02792", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.02792", "abs": "https://arxiv.org/abs/2511.02792", "authors": ["Daniel Ting", "Kenneth Hung"], "title": "The Bias-Variance Tradeoff in Long-Term Experimentation", "comment": null, "summary": "As we exhaust methods that reduces variance without introducing bias,\nreducing variance in experiments often requires accepting some bias, using\nmethods like winsorization or surrogate metrics. While this bias-variance\ntradeoff can be optimized for individual experiments, bias may accumulate over\ntime, raising concerns for long-term optimization. We analyze whether bias is\never acceptable when it can accumulate, and show that a bias-variance tradeoff\npersists in long-term settings. Improving signal-to-noise remains beneficial,\neven if it introduces bias. This implies we should shift from thinking there is\na single ``correct'', unbiased metric to thinking about how to make the best\nestimates and decisions when better precision can be achieved at the expense of\nbias.\n  Furthermore, our model adds nuance to previous findings that suggest less\nstringent launch criterion leads to improved gains. We show while this is\nbeneficial when the system is far from the optimum, more stringent launch\ncriterion is preferable as the system matures."}
{"id": "2511.02137", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.02137", "abs": "https://arxiv.org/abs/2511.02137", "authors": ["Dongze Wu", "Feng Qiu", "Yao Xie"], "title": "DoFlow: Causal Generative Flows for Interventional and Counterfactual Time-Series Prediction", "comment": null, "summary": "Time-series forecasting increasingly demands not only accurate observational\npredictions but also causal forecasting under interventional and counterfactual\nqueries in multivariate systems. We present DoFlow, a flow based generative\nmodel defined over a causal DAG that delivers coherent observational and\ninterventional predictions, as well as counterfactuals through the natural\nencoding and decoding mechanism of continuous normalizing flows (CNFs). We also\nprovide a supporting counterfactual recovery result under certain assumptions.\nBeyond forecasting, DoFlow provides explicit likelihoods of future\ntrajectories, enabling principled anomaly detection. Experiments on synthetic\ndatasets with various causal DAG and real world hydropower and cancer treatment\ntime series show that DoFlow achieves accurate system-wide observational\nforecasting, enables causal forecasting over interventional and counterfactual\nqueries, and effectively detects anomalies. This work contributes to the\nbroader goal of unifying causal reasoning and generative modeling for complex\ndynamical systems."}
