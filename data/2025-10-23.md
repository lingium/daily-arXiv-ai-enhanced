<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 10]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Semi-Implicit Approaches for Large-Scale Bayesian Spatial Interpolation](https://arxiv.org/abs/2510.19722)
*Sébastien Garneau,Carlos T. P. Zanini,Alexandra M. Schmidt*

Main category: stat.CO

TL;DR: 提出使用半隐式变分推断(SIVI)进行可扩展的贝叶斯空间插值，显著降低了高斯过程在空间统计中的计算成本，同时保持与哈密顿蒙特卡洛相当的预测性能。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在空间统计中计算成本高昂，即使中等样本量也可能需要数小时计算时间，需要开发更高效的贝叶斯近似方法。

Method: 使用半隐式变分推断(SIVI)结合高斯过程先验和最近邻高斯过程先验，与ADVI、Pathfinder和HMC方法进行比较。

Result: SIVI方法在保持与HMC相似预测性能的同时，计算时间从约6小时大幅减少到130秒，并能处理15万个位置的大规模数据集。

Conclusion: SIVI作为一种灵活且可扩展的推断技术，在空间统计中具有巨大潜力，能够显著提高计算效率而不牺牲预测精度。

Abstract: Spatial statistics often rely on Gaussian processes (GPs) to capture
dependencies across locations. However, their computational cost increases
rapidly with the number of locations, potentially needing multiple hours even
for moderate sample sizes. To address this, we propose using Semi-Implicit
Variational Inference (SIVI), a highly flexible Bayesian approximation method,
for scalable Bayesian spatial interpolation. We evaluated SIVI with a GP prior
and a Nearest-Neighbour Gaussian Process (NNGP) prior compared to Automatic
Differentiation Variational Inference (ADVI), Pathfinder, and Hamiltonian Monte
Carlo (HMC), the reference method in spatial statistics. Methods were compared
based on their predictive ability measured by the CRPS, the interval score, and
the negative log-predictive density across 50 replicates for both Gaussian and
Poisson outcomes. SIVI-based methods achieved similar results to HMC, while
being drastically faster. On average, for the Poisson scenario with 500
training locations, SIVI reduced the computational time from roughly 6 hours
for HMC to 130 seconds. Furthermore, SIVI-NNGP analyzed a simulated land
surface temperature dataset of 150,000 locations while estimating all unknown
model parameters in under two minutes. These results highlight the potential of
SIVI as a flexible and scalable inference technique in spatial statistics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [2] [Calibrated Principal Component Regression](https://arxiv.org/abs/2510.19020)
*Yixuan Florence Wu,Yilun Zhu,Lei Cao and,Naichen Shi*

Main category: stat.ML

TL;DR: 提出校准主成分回归(CPCR)方法，通过软阈值减少主成分回归(PCR)在过参数化设置中的截断偏差，提高预测性能


<details>
  <summary>Details</summary>
Motivation: 在过参数化设置中，主成分回归(PCR)通过投影到低维主成分子空间来降低方差，但当真实回归向量在保留主成分之外有分量时会产生截断偏差

Method: CPCR首先在主成分子空间学习低方差先验，然后通过中心化Tikhonov步骤在原始特征空间校准模型，使用交叉拟合并软化PCR的硬截断

Result: 理论分析显示CPCR在随机矩阵机制下的样本外风险优于标准PCR，当回归信号在低方差方向有不可忽略分量时表现更好；实证结果在多个过参数化问题中一致提升预测性能

Conclusion: CPCR在过参数化设置中表现出稳定性和灵活性，能有效处理PCR的截断偏差问题

Abstract: We propose a new method for statistical inference in generalized linear
models. In the overparameterized regime, Principal Component Regression (PCR)
reduces variance by projecting high-dimensional data to a low-dimensional
principal subspace before fitting. However, PCR incurs truncation bias whenever
the true regression vector has mass outside the retained principal components
(PC). To mitigate the bias, we propose Calibrated Principal Component
Regression (CPCR), which first learns a low-variance prior in the PC subspace
and then calibrates the model in the original feature space via a centered
Tikhonov step. CPCR leverages cross-fitting and controls the truncation bias by
softening PCR's hard cutoff. Theoretically, we calculate the out-of-sample risk
in the random matrix regime, which shows that CPCR outperforms standard PCR
when the regression signal has non-negligible components in low-variance
directions. Empirically, CPCR consistently improves prediction across multiple
overparameterized problems. The results highlight CPCR's stability and
flexibility in modern overparameterized settings.

</details>


### [3] [Signature Kernel Scoring Rule as Spatio-Temporal Diagnostic for Probabilistic Forecasting](https://arxiv.org/abs/2510.19110)
*Archer Dodson,Ritabrata Dutta*

Main category: stat.ML

TL;DR: 本文介绍了基于粗糙路径理论的签名核评分规则，用于评估和训练数据驱动的天气预报模型，能够捕捉时空依赖关系，在WeatherBench 2上表现出高判别力。


<details>
  <summary>Details</summary>
Motivation: 传统评分规则如MSE忽略了天气数据的高度相关性结构，需要新的评分方法来更好地量化天气预报的不确定性。

Method: 使用签名核评分规则，将天气变量重构为连续路径，通过迭代积分编码时空依赖关系，并采用路径增强保证唯一性。

Result: 在WeatherBench 2模型上的实证评估显示签名核评分规则具有高判别力，基于该规则的轻量级模型在15个时间步的预测路径上优于气候学基准。

Conclusion: 签名核评分规则为天气预报验证和模型训练提供了理论稳健的度量标准，能够有效捕捉路径依赖的相互作用。

Abstract: Modern weather forecasting has increasingly transitioned from numerical
weather prediction (NWP) to data-driven machine learning forecasting
techniques. While these new models produce probabilistic forecasts to quantify
uncertainty, their training and evaluation may remain hindered by conventional
scoring rules, primarily MSE, which ignore the highly correlated data
structures present in weather and atmospheric systems. This work introduces the
signature kernel scoring rule, grounded in rough path theory, which reframes
weather variables as continuous paths to encode temporal and spatial
dependencies through iterated integrals. Validated as strictly proper through
the use of path augmentations to guarantee uniqueness, the signature kernel
provides a theoretically robust metric for forecast verification and model
training. Empirical evaluations through weather scorecards on WeatherBench 2
models demonstrate the signature kernel scoring rule's high discriminative
power and unique capacity to capture path-dependent interactions. Following
previous demonstration of successful adversarial-free probabilistic training,
we train sliding window generative neural networks using a
predictive-sequential scoring rule on ERA5 reanalysis weather data. Using a
lightweight model, we demonstrate that signature kernel based training
outperforms climatology for forecast paths of up to fifteen timesteps.

</details>


### [4] [Learning Upper Lower Value Envelopes to Shape Online RL: A Principled Approach](https://arxiv.org/abs/2510.19528)
*Sebastian Reboul,Hélène Halconruy,Randal Douc*

Main category: stat.ML

TL;DR: 提出一个两阶段框架，利用离线数据学习价值函数的上界和下界（价值包络），并将其融入在线强化学习算法中，显著加速学习过程并减少遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决利用离线数据加速在线强化学习这一具有潜力但缺乏理论支撑的基本问题，建立离线预训练与在线微调之间的正式桥梁。

Method: 提出两阶段框架：第一阶段使用离线数据推导价值函数的上界和下界；第二阶段将这些学习到的边界融入在线算法。该方法将上下界解耦，实现更灵活和紧密的近似。

Result: 在表格MDP上的实证结果显示，相比UCBVI和先前方法，该方法能显著减少遗憾。

Conclusion: 通过数据驱动的价值包络方法，成功建立了离线预训练与在线强化学习之间的理论联系，提供了可解释的遗憾界限，并在实践中表现出优越性能。

Abstract: We investigate the fundamental problem of leveraging offline data to
accelerate online reinforcement learning - a direction with strong potential
but limited theoretical grounding. Our study centers on how to learn and apply
value envelopes within this context. To this end, we introduce a principled
two-stage framework: the first stage uses offline data to derive upper and
lower bounds on value functions, while the second incorporates these learned
bounds into online algorithms. Our method extends prior work by decoupling the
upper and lower bounds, enabling more flexible and tighter approximations. In
contrast to approaches that rely on fixed shaping functions, our envelopes are
data-driven and explicitly modeled as random variables, with a filtration
argument ensuring independence across phases. The analysis establishes
high-probability regret bounds determined by two interpretable quantities,
thereby providing a formal bridge between offline pre-training and online
fine-tuning. Empirical results on tabular MDPs demonstrate substantial regret
reductions compared with both UCBVI and prior methods.

</details>


### [5] [Extreme Event Aware ($η$-) Learning](https://arxiv.org/abs/2510.19161)
*Kai Chang,Themistoklis P. Sapsis*

Main category: stat.ML

TL;DR: 提出了一种名为η-learning的新方法，用于在训练数据中缺乏极端事件的情况下预测罕见极端事件，通过统计正则化减少极端事件区域的不确定性。


<details>
  <summary>Details</summary>
Motivation: 量化预测罕见极端事件对理解复杂动力系统至关重要，但现有方法通常假设训练数据中存在多个极端事件，导致在极端事件区域存在高认知不确定性。

Method: 引入η-learning方法，通过在训练过程中强制实施指示极端性的可观测量的统计特性，即使训练数据中缺乏极端事件，也能减少极端事件区域的不确定性。

Result: 理论分析基于最优输运提供了严格证明，数值实验在多个原型问题和实际降水降尺度问题上展示了该框架的优越性能。

Conclusion: η-learning能够在训练数据缺乏极端事件的情况下，通过统计正则化生成前所未有的极端事件，有效解决了极端事件预测中的不确定性挑战。

Abstract: Quantifying and predicting rare and extreme events persists as a crucial yet
challenging task in understanding complex dynamical systems. Many practical
challenges arise from the infrequency and severity of these events, including
the considerable variance of simple sampling methods and the substantial
computational cost of high-fidelity numerical simulations. Numerous data-driven
methods have recently been developed to tackle these challenges. However, a
typical assumption for the success of these methods is the occurrence of
multiple extreme events, either within the training dataset or during the
sampling process. This leads to accurate models in regions of quiescent events
but with high epistemic uncertainty in regions associated with extremes. To
overcome this limitation, we introduce Extreme Event Aware (e2a or eta) or
$\eta$-learning which does not assume the existence of extreme events in the
available data. $\eta$-learning reduces the uncertainty even in `uncharted'
extreme event regions, by enforcing the extreme event statistics of an
observable indicative of extremeness during training, which can be available
through qualitative arguments or estimated with unlabeled data. This type of
statistical regularization results in models that fit the observed data, while
enforcing consistency with the prescribed observable statistics, enabling the
generation of unprecedented extreme events even when the training data lack
extremes therein. Theoretical results based on optimal transport offer a
rigorous justification and highlight the optimality of the introduced method.
Additionally, extensive numerical experiments illustrate the favorable
properties of the $\eta$-learning framework on several prototype problems and
real-world precipitation downscaling problems.

</details>


### [6] [Topology of Currencies: Persistent Homology for FX Co-movements: A Comparative Clustering Study](https://arxiv.org/abs/2510.19306)
*Pattravadee de Favereau de Jeneret,Ioannis Diamantis*

Main category: stat.ML

TL;DR: 拓扑数据分析(TDA)在聚类货币行为方面比传统统计方法提供更紧凑和分离良好的聚类，特别是在外汇市场中捕捉结构模式方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 外汇市场是一个复杂的系统，经常表现出非线性和高维动态，传统技术可能无法完全捕捉这些特征。研究旨在探索TDA是否能提供超越传统统计方法的额外洞察。

Method: 使用13种主要货币兑欧元的月度对数收益率，比较基于TDA衍生特征与传统统计特征的聚类结果。应用k均值和层次聚类两种算法，通过轮廓系数和Calinski-Harabasz指数评估聚类质量。

Result: TDA特征聚类产生比传统统计特征更紧凑和分离良好的聚类，Calinski-Harabasz得分显著更高。但所有聚类方法的轮廓系数都较低，表明外汇时间序列聚类的固有难度。

Conclusion: TDA是分析金融时间序列的有价值补充工具，在风险管理等需要理解结构共动性的应用中具有潜力。

Abstract: This study investigates whether Topological Data Analysis (TDA) can provide
additional insights beyond traditional statistical methods in clustering
currency behaviours. We focus on the foreign exchange (FX) market, which is a
complex system often exhibiting non-linear and high-dimensional dynamics that
classical techniques may not fully capture. We compare clustering results based
on TDA-derived features versus classical statistical features using monthly
logarithmic returns of 13 major currency exchange rates (all against the euro).
Two widely-used clustering algorithms, \(k\)-means and Hierarchical clustering,
are applied on both types of features, and cluster quality is evaluated via the
Silhouette score and the Calinski-Harabasz index. Our findings show that
TDA-based feature clustering produces more compact and well-separated clusters
than clustering on traditional statistical features, particularly achieving
substantially higher Calinski-Harabasz scores. However, all clustering
approaches yield modest Silhouette scores, underscoring the inherent difficulty
of grouping FX time series. The differing cluster compositions under TDA vs.
classical features suggest that TDA captures structural patterns in currency
co-movements that conventional methods might overlook. These results highlight
TDA as a valuable complementary tool for analysing financial time series, with
potential applications in risk management where understanding structural
co-movements is crucial.

</details>


### [7] [Metadata Extraction Leveraging Large Language Models](https://arxiv.org/abs/2510.19334)
*Cuize Han,Sesh Jalagam*

Main category: stat.ML

TL;DR: 本文提出了一个基于大语言模型的合同元数据提取系统，通过优化文本转换、分块策略和LLM特定技术，显著提高了法律条款识别的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型自动化法律文档分析，特别是合同审查中的关键条款识别，以降低合同审查的时间和成本。

Method: 结合公开的CUAD数据集和专有合同数据集，采用鲁棒文本转换、策略性分块选择以及CoT提示和结构化工具调用等LLM特定技术。

Result: 实验结果显示条款识别准确性和效率有显著提升，能够有效减少合同审查的时间和成本。

Conclusion: 精心优化的LLM系统可以作为法律专业人士的有价值工具，为各种规模的组织提供高效的合同审查服务。

Abstract: The advent of Large Language Models has revolutionized tasks across domains,
including the automation of legal document analysis, a critical component of
modern contract management systems. This paper presents a comprehensive
implementation of LLM-enhanced metadata extraction for contract review,
focusing on the automatic detection and annotation of salient legal clauses.
Leveraging both the publicly available Contract Understanding Atticus Dataset
(CUAD) and proprietary contract datasets, our work demonstrates the integration
of advanced LLM methodologies with practical applications. We identify three
pivotal elements for optimizing metadata extraction: robust text conversion,
strategic chunk selection, and advanced LLM-specific techniques, including
Chain of Thought (CoT) prompting and structured tool calling. The results from
our experiments highlight the substantial improvements in clause identification
accuracy and efficiency. Our approach shows promise in reducing the time and
cost associated with contract review while maintaining high accuracy in legal
clause identification. The results suggest that carefully optimized LLM systems
could serve as valuable tools for legal professionals, potentially increasing
access to efficient contract review services for organizations of all sizes.

</details>


### [8] [On the hardness of RL with Lookahead](https://arxiv.org/abs/2510.19372)
*Corentin Pla,Hugo Richard,Marc Abeille,Nadav Merlis,Vianney Perchet*

Main category: stat.ML

TL;DR: 本文研究了具有转移前瞻的强化学习，发现单步前瞻（ℓ=1）可通过线性规划在多项式时间内求解，而两步及以上前瞻（ℓ≥2）则变为NP难问题。


<details>
  <summary>Details</summary>
Motivation: 研究在强化学习中，当智能体能够观察执行任意ℓ步动作序列后可能访问的状态时，这种前瞻信息如何影响可实现的性能以及计算复杂度。

Method: 提出了一种新颖的线性规划方法来解决单步前瞻（ℓ=1）的最优规划问题，并证明了该方法的多项式时间复杂度。对于ℓ≥2的情况，通过计算复杂性理论证明了其NP难性。

Result: 单步前瞻规划可在多项式时间内求解，而两步及以上前瞻规划是NP难的。这为规划问题在转移前瞻强化学习中的可处理性划定了精确边界。

Conclusion: 转移前瞻强化学习中的最优规划问题在ℓ=1时是易处理的，但在ℓ≥2时变得计算困难，这为实际应用提供了重要的理论指导。

Abstract: We study reinforcement learning (RL) with transition look-ahead, where the
agent may observe which states would be visited upon playing any sequence of
$\ell$ actions before deciding its course of action. While such predictive
information can drastically improve the achievable performance, we show that
using this information optimally comes at a potentially prohibitive
computational cost. Specifically, we prove that optimal planning with one-step
look-ahead ($\ell=1$) can be solved in polynomial time through a novel linear
programming formulation. In contrast, for $\ell \geq 2$, the problem becomes
NP-hard. Our results delineate a precise boundary between tractable and
intractable cases for the problem of planning with transition look-ahead in
reinforcement learning.

</details>


### [9] [Square root Cox's survival analysis by the fittest linear and neural networks model](https://arxiv.org/abs/2510.19374)
*Maxime van Cutsem,Sylvain Sardy*

Main category: stat.ML

TL;DR: 提出一种改进生存分析中特征选择的新方法，通过直接调整惩罚参数λ进行特征选择，相比传统的交叉验证LASSO和BIC方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统生存分析方法（如交叉验证或BIC）在特征选择方面存在不足，需要改进Cox比例风险模型和LASSO的特征选择能力。

Method: 采用Cox偏似然函数的平方根，直接调整惩罚参数λ进行特征选择，该方法具有渐近枢轴性，可应用于线性模型和人工神经网络。

Result: 相比交叉验证LASSO和BIC子集选择方法，新方法在恢复所有且仅恢复良好特征的概率方面表现出相变特性，类似于压缩感知中的表现。

Conclusion: 该方法显著提升了生存分析中的特征选择性能，具有类似压缩感知的相变特性，可广泛应用于线性模型和神经网络。

Abstract: We revisit Cox's proportional hazard models and LASSO in the aim of improving
feature selection in survival analysis. Unlike traditional methods relying on
cross-validation or BIC, the penalty parameter $\lambda$ is directly tuned for
feature selection and is asymptotically pivotal thanks to taking the square
root of Cox's partial likelihood. Substantially improving over both
cross-validation LASSO and BIC subset selection, our approach has a phase
transition on the probability of retrieving all and only the good features,
like in compressed sensing. The method can be employed by linear models but
also by artificial neural networks.

</details>


### [10] [A Derandomization Framework for Structure Discovery: Applications in Neural Networks and Beyond](https://arxiv.org/abs/2510.19382)
*Nikos Tsikouras,Yorgos Pantis,Ioannis Mitliagkas,Christos Tzamos*

Main category: stat.ML

TL;DR: 该论文研究神经网络特征学习的结构发现，在更弱假设下证明任意大小和深度的神经网络在达到二阶平稳点时能自动学习到低秩结构。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络特征学习的动态特性仍具挑战性。先前工作(Mousavi-Hosseini et al., 2023)在强正则化条件下分析了两层学生网络，本工作旨在在更弱假设下研究结构发现。

Method: 使用去随机化引理，证明在任意大小和深度的神经网络中，当所有参数可训练、使用平滑损失函数、微小正则化、且训练达到二阶平稳点时，能自动发现低秩结构。

Result: 证明了神经网络在达到二阶平稳点时，其权重矩阵会收敛到零矩阵，这直接解释了结构发现现象，并可用于MAXCUT近似和Johnson-Lindenstrauss嵌入计算。

Conclusion: 去随机化引理是理解神经网络结构发现的核心工具，该发现具有广泛的应用价值，包括图分割问题和降维技术。

Abstract: Understanding the dynamics of feature learning in neural networks (NNs)
remains a significant challenge. The work of (Mousavi-Hosseini et al., 2023)
analyzes a multiple index teacher-student setting and shows that a two-layer
student attains a low-rank structure in its first-layer weights when trained
with stochastic gradient descent (SGD) and a strong regularizer. This
structural property is known to reduce sample complexity of generalization.
Indeed, in a second step, the same authors establish algorithm-specific
learning guarantees under additional assumptions. In this paper, we focus
exclusively on the structure discovery aspect and study it under weaker
assumptions, more specifically: we allow (a) NNs of arbitrary size and depth,
(b) with all parameters trainable, (c) under any smooth loss function, (d) tiny
regularization, and (e) trained by any method that attains a second-order
stationary point (SOSP), e.g.\ perturbed gradient descent (PGD). At the core of
our approach is a key $\textit{derandomization}$ lemma, which states that
optimizing the function $\mathbb{E}_{\mathbf{x}}
\left[g_{\theta}(\mathbf{W}\mathbf{x} + \mathbf{b})\right]$ converges to a
point where $\mathbf{W} = \mathbf{0}$, under mild conditions. The fundamental
nature of this lemma directly explains structure discovery and has immediate
applications in other domains including an end-to-end approximation for MAXCUT,
and computing Johnson-Lindenstrauss embeddings.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [11] [Multiple Imputation for Small, Extremely High Efficacy Clinical Trials with Binary Endpoints](https://arxiv.org/abs/2510.19011)
*Yaoyuan Vincent Tan,Gang Xu,Chenkun Wang*

Main category: stat.AP

TL;DR: 本文针对细胞和基因治疗试验中样本量小、疗效极高且存在缺失数据的二分类终点问题，比较并开发了3种新的多重插补方法，在真实临床研究中验证了方法的良好性能。


<details>
  <summary>Details</summary>
Motivation: 细胞和基因治疗试验样本量小、疗效极高，当出现缺失数据时监管机构要求处理策略，但二分类终点的多重插补方法文献缺乏。

Method: 比较并开发了3种新的多重插补方法来处理小样本量、高疗效情况下的二分类终点缺失数据，关注成功率的总体比例参数。

Result: 提出的方法表现良好，产生了良好的95%覆盖率，并在NIH进行的临床胰岛移植07方案中实际应用验证。

Conclusion: 针对细胞和基因治疗试验中二分类终点缺失数据问题，开发的新多重插补方法在小样本高疗效情况下表现优异，具有实际应用价值。

Abstract: There has been an increasing interest in using cell and gene therapy (CGT) to
treat/cure difficult diseases. The hallmark of CGT trials are the small sample
size and extremely high efficacy. Due to the innovation and novelty of such
therapies, when there is missing data, more scrutiny is exercised, and
regulators often request for missing data handling strategy when missing data
occurs. Often, multiple imputation (MI) will be used. MI for continuous
endpoint is well established but literature of MI for binary endpoint is
lacking. In this work, we compare and develop 3 new methods to handle missing
data using MI for binary endpoints when the sample size is small and efficacy
extremely high. The parameter of interest is population proportion of success.
We show that our proposed methods performed well and produced good 95%
coverage. We also applied our methods to an actual clinical study, the Clinical
Islet Transplantation (CIT) Protocol 07, conducted by National Institutes of
Health (NIH).

</details>


### [12] [Simulation-Guided Planning of a Target Trial Emulated Cluster Randomized Trial for Mass Small-Quantity Lipid Nutrient Supplementation Combined with Expanded Program on Immunization in Rural Niger](https://arxiv.org/abs/2510.19077)
*Rebecca K. Metcalfe,Nathaniel Dyrkton,Yichen Yan,Shomoita Alam,Susan Shepherd,Ibrahim Sana,Kevin Phelan,Jay JH Park*

Main category: stat.AP

TL;DR: 该研究通过模拟方法为模拟群组随机试验的非随机研究设计数据收集方案，比较了四种统计方法，发现beta回归在控制I类错误率的同时具有足够的统计功效。


<details>
  <summary>Details</summary>
Motivation: 目标试验模拟方法在非随机研究中应用日益广泛，但在模拟群组随机试验方面的应用有限。当群组随机化不可行时，需要开发稳健的研究设计方法。

Method: 使用模拟研究来前瞻性规划数据收集，比较beta回归、准二项回归、逆概率处理加权和朴素Wald检验四种统计方法的I类错误率和统计功效。

Result: 只有逆概率处理加权和beta回归能够将I类错误率控制在0.05水平，但逆概率处理加权的统计功效较差。beta回归在控制I类错误率的同时具有足够的统计功效。

Conclusion: 在目标试验模拟中采用模拟引导的设计原则可以实现稳健的群组水平非随机研究规划，这些经验也适用于个体水平随机试验的目标试验模拟规划。

Abstract: While target trial emulation (TTE) is increasingly used to improve the
analysis of non-randomized studies by applying trial design principles, TTE
applications to emulate cluster randomized trials (RCTs) have been limited. We
performed simulations to prospectively plan data collection of a non-randomized
study intended to emulate a village-level cluster RCT when
cluster-randomization was infeasible. The planned study will assess the impact
of mass distribution of nutritional supplements embedded within an existing
immunization program to improve pentavalent vaccination rates among children
12-24 months old in Niger. The design included covariate-constrained random
selection of villages for outcome ascertainment at follow-up. Simulations used
baseline census data on pentavalent vaccination rates and cluster-level
covariates to compare the type I error rate and power of four statistical
methods: beta-regression; quasi-binomial regression; inverse probability of
treatment weighting (IPTW); and na\"ive Wald test. Of these methods, only IPTW
and beta-regression controlled the type I error rate at 0.05, but IPTW yielded
poor statistical power. Beta-regression, which showed adequate statistical
power, was chosen as our primary analysis. Adopting simulation-guided design
principles within TTE can enable robust planning of a group-level
non-randomized study emulating a cluster RCT. Lessons from this study also
apply to TTE planning of individually-RCTs.

</details>


### [13] [Green Finance and Carbon Emissions: A Nonlinear and Interaction Analysis Using Bayesian Additive Regression Trees](https://arxiv.org/abs/2510.19785)
*Mengxiang zhu,Riccardo Rastelli*

Main category: stat.AP

TL;DR: 本研究使用2000-2022年中国省级面板数据，通过贝叶斯加性回归树方法分析绿色金融对碳排放强度的非线性影响，发现绿色金融指数对碳排放强度存在显著倒U型效应，且具有区域异质性。


<details>
  <summary>Details</summary>
Motivation: 作为中国应对气候风险的核心政策工具，绿色金融在塑造碳减排成果方面具有战略重要性，需要研究其非线性效应和交互作用。

Method: 使用中国省级面板数据(2000-2022)，采用贝叶斯加性回归树(BART)捕捉复杂非线性关系和交互路径，并使用SHAP值增强模型可解释性。

Result: 绿色金融指数对碳排放强度有显著倒U型效应，存在区域异质性；气候物理风险指数对碳排放无显著影响；在高能耗情景下，更强的绿色金融发展有助于降低碳排放强度。

Conclusion: 绿色金融是降低碳强度的有效工具，特别是在能源密集型情境下，设计和实施绿色金融政策时需要考虑非线性效应和区域差异。

Abstract: As a core policy tool for China in addressing climate risks, green finance
plays a strategically important role in shaping carbon mitigation outcomes.
This study investigates the nonlinear and interaction effects of green finance
on carbon emission intensity (CEI) using Chinese provincial panel data from
2000 to 2022. The Climate Physical Risk Index (CPRI) is incorporated into the
analytical framework to assess its potential role in shaping carbon outcomes.
We employ Bayesian Additive Regression Trees (BART) to capture complex
nonlinear relationships and interaction pathways, and use SHapley Additive
exPlanations values to enhance model interpretability. Results show that the
Green Finance Index (GFI) has a statistically significant inverted U-shaped
effect on CEI, with notable regional heterogeneity. Contrary to expectations,
CPRI does not show a significant impact on carbon emissions. Further analysis
reveals that in high energy consumption scenarios, stronger green finance
development contributes to lower CEI. These findings highlight the potential of
green finance as an effective instrument for carbon intensity reduction,
especially in energy-intensive contexts, and underscore the importance of
accounting for nonlinear effects and regional disparities when designing and
implementing green financial policies.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [14] [Efficient scenario analysis in real-time Bayesian election forecasting via sequential meta-posterior sampling](https://arxiv.org/abs/2510.19133)
*Geonhee Han,Andrew Gelman,Aki Vehtari*

Main category: stat.ME

TL;DR: 提出了一种结合元建模和顺序采样方案的方法，用于贝叶斯聚合选举预测中的实时推理和敏感性分析，避免重复拟合模型。


<details>
  <summary>Details</summary>
Motivation: 选举预测需要结合多种信息来源，但实时后验更新、模型检查和敏感性分析在标准MCMC方法下计算成本高昂。

Method: 采用元建模策略配合顺序采样方案，通过遍历后验元模型实现实时推理和结构化场景分析。

Result: 在模型回测中显示出显著的计算优势，并揭示了非平凡的敏感性模式，例如预测对基本面预测的先验置信度保持敏感，但对随机游走尺度不敏感。

Conclusion: 该方法能够在不重复拟合的情况下进行实时敏感性分析，有助于澄清投票数据与结构假设的相对影响。

Abstract: Bayesian aggregation lets election forecasters combine diverse sources of
information, such as state polls and economic and political indicators: as in
our collaboration with The Economist magazine. However, the demands of
real-time posterior updating, model checking, and communication introduce
practical methodological challenges. In particular, sensitivity and scenario
analysis help trace forecast shifts to model assumptions and understand model
behavior. Yet, under standard Markov chain Monte Carlo, even small tweaks to
the model (e.g., in priors, data, hyperparameters) require full refitting,
making such real-time analysis computationally expensive. To overcome the
bottleneck, we introduce a meta-modeling strategy paired with a sequential
sampling scheme; by traversing posterior meta-models, we enable real-time
inference and structured scenario and sensitivity analysis without repeated
refitting. In a back-test of the model, we show substantial computational gains
and uncover non-trivial sensitivity patterns. For example, forecasts remain
responsive to prior confidence in fundamentals-based forecasts, but less so to
random walk scale; these help clarify the relative influence of polling data
versus structural assumptions. Code is available at
https://github.com/geonhee619/SMC-Sense.

</details>


### [15] [Centered MA Dirichlet ARMA for Financial Compositions: Theory & Empirical Evidence](https://arxiv.org/abs/2510.18903)
*Harrison Katz*

Main category: stat.ME

TL;DR: 提出了一种改进的Dirichlet组合时间序列模型，通过中心化创新项解决传统MA回归中条件均值偏差问题


<details>
  <summary>Details</summary>
Motivation: 传统B-DARMA模型中的MA回归项在Dirichlet似然下具有非零条件均值，这会偏置均值路径并模糊MA系数的解释

Method: 将原始回归项替换为中心化创新项，通过digamma函数闭式计算，在不改变似然或ALR链接的情况下恢复MA块的零均值创新

Result: 中心化公式在固定保留和滚动一步预测中提高了对数预测分数，点误差基本相同，且HMC诊断更清晰

Conclusion: 中心化创新项是改进Dirichlet组合时间序列模型的简单有效方法，能提供更好的预测性能和计算稳定性

Abstract: Observation-driven Dirichlet models for compositional time series often use
the additive log-ratio (ALR) link and include a moving-average (MA) term built
from ALR residuals. In the standard B--DARMA recursion, the usual MA regressor
$\alr(\mathbf{Y}_t)-\boldsymbol{\eta}_t$ has nonzero conditional mean under the
Dirichlet likelihood, which biases the mean path and blurs the interpretation
of MA coefficients. We propose a minimal change: replace the raw regressor with
a \emph{centered} innovation
$\boldsymbol{\epsilon}_t^{\circ}=\alr(\mathbf{Y}_t)-\mathbb{E}\{\alr(\mathbf{Y}_t)\mid
\boldsymbol{\eta}_t,\phi_t\}$, computable in closed form via digamma functions.
Centering restores mean-zero innovations for the MA block without altering
either the likelihood or the ALR link. We provide simple identities for the
conditional mean and the forecast recursion, show first-order equivalence to a
digamma-link DARMA while retaining a closed-form inverse to
$\boldsymbol{\mu}_t$, and give ready-to-use code. A weekly application to the
Federal Reserve H.8 bank-asset composition compares the original (raw-MA) and
centered specifications under a fixed holdout and rolling one-step origins. The
centered formulation improves log predictive scores with essentially identical
point error and markedly cleaner Hamiltonian Monte Carlo diagnostics.

</details>


### [16] [Estimation of causal dose-response functions under data fusion](https://arxiv.org/abs/2510.19094)
*Jaewon Lim,Alex Luedtke*

Main category: stat.ME

TL;DR: 提出一个数据融合框架，利用多个部分对齐的数据源来估计因果剂量-响应函数，通过Neyman正交损失函数和随机逼近方法提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 当单一数据源不足以精确估计所有暴露水平的响应时，需要利用多个数据源来克服这一限制。

Method: 推导了数据融合设置下的Neyman正交损失函数，提出保留正交性的随机逼近方法，并应用核岭回归获得闭式估计器。

Result: 理论分析显示加入额外数据源可获得更紧的有限样本遗憾界和更好的最坏情况性能，仿真研究证实了数据融合在估计精度上的优势。

Conclusion: 数据融合在估计非平滑参数（如因果剂量-响应函数）方面具有潜力。

Abstract: Estimating the causal dose-response function is challenging, particularly
when data from a single source are insufficient to estimate responses precisely
across all exposure levels. To overcome this limitation, we propose a data
fusion framework that leverages multiple data sources that are partially
aligned with the target distribution. Specifically, we derive a
Neyman-orthogonal loss function tailored for estimating the dose-response
function within data fusion settings. To improve computational efficiency, we
propose a stochastic approximation that retains orthogonality. We apply kernel
ridge regression with this approximation, which provides closed-form
estimators. Our theoretical analysis demonstrates that incorporating additional
data sources yields tighter finite-sample regret bounds and improved worst-case
performance, as confirmed via minimax lower bound comparison. Simulation
studies validate the practical advantages of our approach, showing improved
estimation accuracy when employing data fusion. This study highlights the
potential of data fusion for estimating non-smooth parameters such as causal
dose-response functions.

</details>


### [17] [Robust Rank Estimation for Noisy Matrices](https://arxiv.org/abs/2510.19583)
*Subhrajyoty Roy,Abhik Ghosh,Ayanendranath Basu*

Main category: stat.ME

TL;DR: 提出了一种新的矩阵秩估计准则DICMR，结合了鲁棒性和计算简单性，在保持准确性的同时大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有秩估计方法对异常值敏感或计算成本高，需要一种既鲁棒又计算简单的新方法。

Method: 基于密度幂散度框架推导出DICMR准则，继承了鲁棒性特性且计算简单。

Result: DICMR在准确性上与鲁棒化交叉验证方法相当，但计算成本显著降低；在微阵列插值应用中优于多种先进算法。

Conclusion: DICMR是一种有效的矩阵秩估计方法，成功平衡了鲁棒性和计算效率，具有实际应用价值。

Abstract: Estimating the true rank of a noisy data matrix is a fundamental problem
underlying techniques such as principal component analysis, matrix completion,
etc. Existing rank estimation criteria, including information-based and
cross-validation methods, are either highly sensitive to outliers or
computationally demanding when combined with robust estimators. This paper
proposes a new criterion, the Divergence Information Criterion for Matrix Rank
(DICMR), that achieves both robustness and computational simplicity. Derived
from the density power divergence framework, DICMR inherits the robustness
properties while being computationally very simple. We provide asymptotic
bounds on its overestimation and underestimation probabilities, and demonstrate
first-order B-robustness of the criteria. Extensive simulations show that DICMR
delivers accuracy comparable to the robustified cross-validation methods, but
with far lower computational cost. We also showcase a real-data application to
microarray imputation to further demonstrate its practical utility,
outperforming several state-of-the-art algorithms.

</details>


### [18] [Spatially Regularized Gaussian Mixtures for Clustering Spatial Transcriptomic Data](https://arxiv.org/abs/2510.19108)
*Andrea Sottosanti,Davide Risso,Francesco Denti*

Main category: stat.ME

TL;DR: 提出了一种基于高斯过程的基因聚类框架，利用空间转录组数据的空间结构来识别具有相似表达模式的基因。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术能够测量组织中基因表达的空间分布，识别具有相似空间表达模式的基因对于理解生物学过程至关重要。现有方法需要灵活利用空间数据结构进行基因聚类。

Method: 使用高斯过程建模，通过广义Cholesky分解将空间结构信息融入精度矩阵，避免核函数误设问题，估计非平稳空间协方差结构。

Result: 模型成功识别出在不同组织区域（如肿瘤和基质区域）具有不同空间相关模式的基因簇。

Conclusion: 该方法能够有效利用空间转录组数据的空间结构，识别具有生物学意义的基因表达模式，为空间基因表达分析提供了灵活的工具。

Abstract: Spatial transcriptomics measures the expression of thousands of genes in a
tissue sample while preserving its spatial structure. This class of
technologies has enabled the investigation of the spatial variation of gene
expressions and their impact on specific biological processes. Identifying
genes with similar expression profiles is of utmost importance, thus motivating
the development of flexible methods leveraging spatial data structure to
cluster genes. Here, we propose a modeling framework for clustering
observations measured over numerous spatial locations via Gaussian processes.
Rather than specifying their covariance kernels as a function of the spatial
structure, we use it to inform a generalized Cholesky decomposition of their
precision matrices. This approach prevents issues with kernel misspecification
and facilitates the estimation of a non-stationarity spatial covariance
structure. Applied to spatial transcriptomic data, our model identifies gene
clusters with distinctive spatial correlation patterns across tissue areas
comprising different cell types, like tumoral and stromal areas.

</details>


### [19] [Inverse-intensity weighted generalized estimating equations with irregularly measured longitudinal data and informative dropout](https://arxiv.org/abs/2510.19154)
*George Stefan,Eleanor Pullenayegum*

Main category: stat.ME

TL;DR: 本文扩展了IIW-GEE方法以处理纵向数据中的信息性脱落问题，通过模拟研究和临床试验数据验证了该方法能显著减少因忽略脱落机制而产生的偏差。


<details>
  <summary>Details</summary>
Motivation: 在生物医学纵向研究中，受试者的随访时间可能不规则，且健康结局可能影响研究脱落，现有IIW-GEE方法在这种情况下会产生偏差。

Method: 扩展IIW-GEE框架，使用访问强度模型估计作为权重，在GEE模型中调整信息性脱落。

Result: 模拟研究显示该方法能显著减少偏差，STAR*D临床试验数据应用表明忽略信息性脱落会高估疾病轨迹。

Conclusion: 提出的扩展IIW-GEE方法能有效处理纵向数据中的信息性脱落问题，提高估计的准确性。

Abstract: Longitudinal data are commonly encountered in biomedical research, including
randomized trials and retrospective cohort studies. Subjects are typically
followed over a period of time and may be scheduled for follow-up at
pre-determined time points. However, subjects may miss their appointments or
return at non-specified times, leading to irregularity in the visit process.
IIW-GEEs have been developed as one method to account for this irregularity,
whereby estimates from a visit intensity model are used as weights in a GEE
model with an independent correlation structure. We show that currently
available methods can be biased for situations in which the health outcome of
interest may influence a subject's dropout from the study. We have extended the
IIW-GEE framework to adjust for informative dropout and have demonstrated via
simulation studies that this bias can be significantly reduced. We have
illustrated this method using the STAR*D clinical trial data, and observed that
the disease trajectory was generally overestimated when informative dropout was
not accounted for.

</details>


### [20] [No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence](https://arxiv.org/abs/2510.19212)
*Ernest Fokoué*

Main category: stat.ME

TL;DR: 本文系统论证了统计学是机器学习和现代AI的不可或缺基础，将AI解构为9个统计支柱，强调统计提供理论框架而计算机科学提供计算能力。


<details>
  <summary>Details</summary>
Motivation: 纠正AI仅源于计算机科学的流行叙事，揭示AI的理论和方法论核心本质上是统计学的，强调认识这一统计基础对开发稳健、可解释AI系统的重要性。

Method: 将AI解构为9个基础支柱（推理、密度估计、序列学习、泛化、表示学习、可解释性、因果性、优化和统一），并展示每个支柱都建立在百年统计原理之上。

Result: 成功追溯了从假设检验到生成AI、从时间序列分析到因果模型等AI核心概念的完整统计谱系，证明统计是AI的"大脑"而计算机科学是"肌肉"。

Conclusion: 认识到统计基础不仅是学术练习，更是开发更稳健、可解释、可信赖智能系统的必要步骤，呼吁教育、研究和实践重新拥抱这一统计基础。

Abstract: The rapid ascent of artificial intelligence (AI) is often portrayed as a
revolution born from computer science and engineering. This narrative, however,
obscures a fundamental truth: the theoretical and methodological core of AI is,
and has always been, statistical. This paper systematically argues that the
field of statistics provides the indispensable foundation for machine learning
and modern AI. We deconstruct AI into nine foundational pillars-Inference,
Density Estimation, Sequential Learning, Generalization, Representation
Learning, Interpretability, Causality, Optimization, and
Unification-demonstrating that each is built upon century-old statistical
principles. From the inferential frameworks of hypothesis testing and
estimation that underpin model evaluation, to the density estimation roots of
clustering and generative AI; from the time-series analysis inspiring recurrent
networks to the causal models that promise true understanding, we trace an
unbroken statistical lineage. While celebrating the computational engines that
power modern AI, we contend that statistics provides the brain-the theoretical
frameworks, uncertainty quantification, and inferential goals-while computer
science provides the brawn-the scalable algorithms and hardware. Recognizing
this statistical backbone is not merely an academic exercise, but a necessary
step for developing more robust, interpretable, and trustworthy intelligent
systems. We issue a call to action for education, research, and practice to
re-embrace this statistical foundation. Ignoring these roots risks building a
fragile future; embracing them is the path to truly intelligent machines. There
is no machine learning without statistical learning; no artificial intelligence
without statistical thought.

</details>


### [21] [A New Targeted-Federated Learning Framework for Estimating Heterogeneity of Treatment Effects: A Robust Framework with Applications in Aging Cohorts](https://arxiv.org/abs/2510.19243)
*Rong Zhao,Jason Falvey,Xu Shi,Vernon M. Chinchilli,Chixiang Chen*

Main category: stat.ME

TL;DR: 提出了一种新的目标联邦学习框架，用于研究目标人群的治疗效果异质性，通过投影估计量整合多个数据源信息，无需共享原始数据，并考虑协变量分布偏移。


<details>
  <summary>Details</summary>
Motivation: 多源数据分析能提高因果估计效率，但面临人群异质性和数据隐私限制的挑战。现有联邦因果推断方法多关注基于差异的平均因果效应，缺乏对效应修饰的研究。

Method: 开发目标联邦学习框架，提出投影估计量研究治疗效果异质性，支持连续结果的差异估计量和二元结果的比值比估计量，具有双重稳健性，并开发通信高效的基于bootstrap的选择程序检测不可迁移数据源。

Result: 通过广泛模拟研究证明所提估计量优于现有方法，并在真实世界Medicare关联数据应用中验证了方法的实用性。

Conclusion: 该框架能有效整合多源信息研究治疗效果异质性，同时保护数据隐私，为联邦环境下的因果推断提供了有力工具。

Abstract: Analyzing data from multiple sources offers valuable opportunities to improve
the estimation efficiency of causal estimands. However, this analysis also
poses many challenges due to population heterogeneity and data privacy
constraints. While several advanced methods for causal inference in federated
settings have been developed in recent years, many focus on difference-based
averaged causal effects and are not designed to study effect modification. In
this study, we introduce a novel targeted-federated learning framework to study
the heterogeneity of treatment effects (HTEs) for a targeted population by
proposing a projection-based estimand. This HTE framework integrates
information from multiple data sources without sharing raw data, while
accounting for covariate distribution shifts among sources. Our proposed
approach is shown to be doubly robust, conveniently supporting both
difference-based estimands for continuous outcomes and odds ratio-based
estimands for binary outcomes. Furthermore, we develop a
communication-efficient bootstrap-based selection procedure to detect
non-transportable data sources, thereby enhancing robust information
aggregation without introducing bias. The superior performance of the proposed
estimator over existing methods is demonstrated through extensive simulation
studies, and the utility of our approach has been shown in a real-world data
application using nationwide Medicare-linked data.

</details>


### [22] [Hierarchical Overlapping Group Lasso for GMANOVA Model](https://arxiv.org/abs/2510.19311)
*M. Ohishi,I. Nagai,R. Oda,H. Yanagihara*

Main category: stat.ME

TL;DR: 提出层次重叠组Lasso方法，在GMANOVA模型中同时进行变量选择和多项式次数选择，通过正交基变换解决高次多项式拟合问题。


<details>
  <summary>Details</summary>
Motivation: GMANOVA模型存在两个模型选择问题：解释变量选择和多项式次数选择，高次多项式拟合常导致模型选择困难。

Method: 使用层次重叠组Lasso方法，通过将回归系数向量分层置零来统一处理两个选择问题，并对多项式基函数矩阵进行正交变换。

Result: 开发了具有最优性和收敛性的算法，并通过数值模拟评估了方法的性能。

Conclusion: 提出的方法能有效同时处理变量选择和多项式次数选择问题，解决了高次多项式拟合带来的模型选择困难。

Abstract: This paper deals with the GMANOVA model with a matrix of polynomial basis
functions as a within-individual design matrix. The model involves two model
selection problems: the selection of explanatory variables and the selection of
the degrees of the polynomials. The two problems can be uniformly addressed by
hierarchically incorporating zeros into the vectors of regression coefficients.
Based on this idea, we propose hierarchical overlapping group Lasso (HOGL) to
perform the variable and degree selections simultaneously. Importantly, when
using a polynomial basis, fitting a highdegree polynomial often causes problems
in model selection. In the approach proposed here, these problems are handled
by using a matrix of orthonormal basis functions obtained by transforming the
matrix of polynomial basis functions. Algorithms are developed with optimality
and convergence to optimize the method. The performance of the proposed method
is evaluated using numerical simulation.

</details>


### [23] [Living Synthetic Benchmarks: A Neutral and Cumulative Framework for Simulation Studies](https://arxiv.org/abs/2510.19489)
*František Bartoš,Samuel Pawel,Björn S. Siepe*

Main category: stat.ME

TL;DR: 提出"活体合成基准"概念，将方法开发与模拟研究分离，通过持续更新基准来促进统计方法的中立、可复现和累积性评估。


<details>
  <summary>Details</summary>
Motivation: 当前模拟研究中，新方法通常由作者自己设计数据生成机制进行评估，这种耦合导致激励错位，可能影响评估的中立性。同时，模拟研究结果因数据生成机制、竞争方法和性能指标的差异而难以比较，阻碍方法学进展。

Method: 引入活体合成基准概念，将方法开发与模拟研究解耦，每当新的数据生成机制、方法或性能指标出现时持续更新基准。建立了用于发表偏倚调整方法的原型基准。

Result: 证明了活体合成基准的可行性，通过原型基准展示了该方法的实施潜力。

Conclusion: 活体合成基准有潜力促进方法的中立、可复现和累积性评估，使方法开发者和使用者都能受益。

Abstract: Simulation studies are widely used to evaluate statistical methods. However,
new methods are often introduced and evaluated using data-generating mechanisms
(DGMs) devised by the same authors. This coupling creates misaligned
incentives, e.g., the need to demonstrate the superiority of new methods,
potentially compromising the neutrality of simulation studies. Furthermore,
results of simulation studies are often difficult to compare due to differences
in DGMs, competing methods, and performance measures. This fragmentation can
lead to conflicting conclusions, hinder methodological progress, and delay the
adoption of effective methods. To address these challenges, we introduce the
concept of living synthetic benchmarks. The key idea is to disentangle method
and simulation study development and continuously update the benchmark whenever
a new DGM, method, or performance measure becomes available. This separation
benefits the neutrality of method evaluation, emphasizes the development of
both methods and DGMs, and enables systematic comparisons. In this paper, we
outline a blueprint for building and maintaining such benchmarks, discuss the
technical and organizational challenges of implementation, and demonstrate
feasibility with a prototype benchmark for publication bias adjustment methods.
We conclude that living synthetic benchmarks have the potential to foster
neutral, reproducible, and cumulative evaluation of methods, benefiting both
method developers and users.

</details>
