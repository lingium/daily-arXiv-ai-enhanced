{"id": "2602.13362", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13362", "abs": "https://arxiv.org/abs/2602.13362", "authors": ["Ádám Jung", "Domokos M. Kelen", "András A. Benczúr"], "title": "Nonparametric Distribution Regression Re-calibration", "comment": null, "summary": "A key challenge in probabilistic regression is ensuring that predictive distributions accurately reflect true empirical uncertainty. Minimizing overall prediction error often encourages models to prioritize informativeness over calibration, producing narrow but overconfident predictions. However, in safety-critical settings, trustworthy uncertainty estimates are often more valuable than narrow intervals. Realizing the problem, several recent works have focused on post-hoc corrections; however, existing methods either rely on weak notions of calibration (such as PIT uniformity) or impose restrictive parametric assumptions on the nature of the error. To address these limitations, we propose a novel nonparametric re-calibration algorithm based on conditional kernel mean embeddings, capable of correcting calibration error without restrictive modeling assumptions. For efficient inference with real-valued targets, we introduce a novel characteristic kernel over distributions that can be evaluated in $\\mathcal{O}(n \\log n)$ time for empirical distributions of size $n$. We demonstrate that our method consistently outperforms prior re-calibration approaches across a diverse set of regression benchmarks and model classes."}
{"id": "2602.13421", "categories": ["stat.ML", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.13421", "abs": "https://arxiv.org/abs/2602.13421", "authors": ["Hadi Vafaii", "Jacob L. Yates"], "title": "Metabolic cost of information processing in Poisson variational autoencoders", "comment": null, "summary": "Computation in biological systems is fundamentally energy-constrained, yet standard theories of computation treat energy as freely available. Here, we argue that variational free energy minimization under a Poisson assumption offers a principled path toward an energy-aware theory of computation. Our key observation is that the Kullback-Leibler (KL) divergence term in the Poisson free energy objective becomes proportional to the prior firing rates of model neurons, yielding an emergent metabolic cost term that penalizes high baseline activity. This structure couples an abstract information-theoretic quantity -- the *coding rate* -- to a concrete biophysical variable -- the *firing rate* -- which enables a trade-off between coding fidelity and energy expenditure. Such a coupling arises naturally in the Poisson variational autoencoder (P-VAE) -- a brain-inspired generative model that encodes inputs as discrete spike counts and recovers a spiking form of *sparse coding* as a special case -- but is absent from standard Gaussian VAEs. To demonstrate that this metabolic cost structure is unique to the Poisson formulation, we compare the P-VAE against Grelu-VAE, a Gaussian VAE with ReLU rectification applied to latent samples, which controls for the non-negativity constraint. Across a systematic sweep of the KL term weighting coefficient $β$ and latent dimensionality, we find that increasing $β$ monotonically increases sparsity and reduces average spiking activity in the P-VAE. In contrast, Grelu-VAE representations remain unchanged, confirming that the effect is specific to Poisson statistics rather than a byproduct of non-negative representations. These results establish Poisson variational inference as a promising foundation for a resource-constrained theory of computation."}
{"id": "2602.13619", "categories": ["stat.ML", "cs.IT", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.13619", "abs": "https://arxiv.org/abs/2602.13619", "authors": ["Anuj Kumar Yadav", "Cemre Cadir", "Yanina Shkel", "Michael Gastpar"], "title": "Locally Private Parametric Methods for Change-Point Detection", "comment": "43 pages, 20 figures", "summary": "We study parametric change-point detection, where the goal is to identify distributional changes in time series, under local differential privacy. In the non-private setting, we derive improved finite-sample accuracy guarantees for a change-point detection algorithm based on the generalized log-likelihood ratio test, via martingale methods. In the private setting, we propose two locally differentially private algorithms based on randomized response and binary mechanisms, and analyze their theoretical performance. We derive bounds on detection accuracy and validate our results through empirical evaluation. Our results characterize the statistical cost of local differential privacy in change-point detection and show how privacy degrades performance relative to a non-private benchmark. As part of this analysis, we establish a structural result for strong data processing inequalities (SDPI), proving that SDPI coefficients for Rényi divergences and their symmetric variants (Jeffreys-Rényi divergences) are achieved by binary input distributions. These results on SDPI coefficients are also of independent interest, with applications to statistical estimation, data compression, and Markov chain mixing."}
{"id": "2602.13906", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13906", "abs": "https://arxiv.org/abs/2602.13906", "authors": ["Shaan Ul Haque", "Zedong Wang", "Zixuan Zhang", "Siva Theja Maguluri"], "title": "Quantifying Normality: Convergence Rate to Gaussian Limit for Stochastic Approximation and Unadjusted OU Algorithm", "comment": "42 pages", "summary": "Stochastic approximation (SA) is a method for finding the root of an operator perturbed by noise. There is a rich literature establishing the asymptotic normality of rescaled SA iterates under fairly mild conditions. However, these asymptotic results do not quantify the accuracy of the Gaussian approximation in finite time. In this paper, we establish explicit non-asymptotic bounds on the Wasserstein distance between the distribution of the rescaled iterate at time k and the asymptotic Gaussian limit for various choices of step-sizes including constant and polynomially decaying. As an immediate consequence, we obtain tail bounds on the error of SA iterates at any time.\n  We obtain the sharp rates by first studying the convergence rate of the discrete Ornstein-Uhlenbeck (O-U) process driven by general noise, whose stationary distribution is identical to the limiting Gaussian distribution of the rescaled SA iterates. We believe that this is of independent interest, given its connection to sampling literature. The analysis involves adapting Stein's method for Gaussian approximation to handle the matrix weighted sum of i.i.d. random variables. The desired finite-time bounds for SA are obtained by characterizing the error dynamics between the rescaled SA iterate and the discrete time O-U process and combining it with the convergence rate of the latter process."}
{"id": "2602.14061", "categories": ["stat.CO", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.14061", "abs": "https://arxiv.org/abs/2602.14061", "authors": ["Sourabh Bhattacharya"], "title": "MPL-HMC: A Tunable Parameterized Leapfrog Framework for Robust Hamiltonian Monte Carlo", "comment": "Feedback welcome", "summary": "This article introduces the Modified Parameterized Leapfrog Hamiltonian Monte Carlo (MPL-HMC) method, a novel extension of HMC addressing key limitations through tunable integration parameters $α(δt)$ and $β(δt)$, enabling controlled perturbations to Hamiltonian dynamics. Theoretical analysis demonstrates MPL-HMC maintains approximate detailed balance. Extensive empirical evaluation reveals systematic performance improvements. The damping variant ($α_2=-0.1$, $β_2=-0.05$) achieves a 14-fold increase in effective sample size for Neal's funnel and 27\\% better efficiency for pharmacokinetic models. The anti-damping variant ($α_2=0.1$, $β_2=0.05$) achieves $\\hat{R}=1.026$ for Bayesian neural networks versus $\\hat{R}=1.981$ for standard HMC. We introduce aggressive MPL-HMC for multimodal distributions, employing extreme parameters ($α_2=8.0$--$15.0$, $β_2=5.0$--$8.0$) with enhanced sampling to achieve full mode exploration where standard methods fail. All variants maintain computational efficiency identical to standard HMC while providing systematic control over damping, exploration, stability, and accuracy. The article provides rigorous mathematical foundations, implementation specifications, parameter tuning strategies, and comprehensive performance comparisons, extending HMC's applicability to previously challenging domains."}
{"id": "2602.14284", "categories": ["stat.OT", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.14284", "abs": "https://arxiv.org/abs/2602.14284", "authors": ["Evelyn Hughes", "Rohan Alexander"], "title": "Benchmarking AI Performance on End-to-End Data Science Projects", "comment": null, "summary": "Data science is an integrated workflow of technical, analytical, communication, and ethical skills, but current AI benchmarks focus mostly on constituent parts. We test whether AI models can generate end-to-end data science projects. To do this we create a benchmark of 40 end-to-end data science projects with associated rubric evaluations. We use these to build an automated grading pipeline that systematically evaluates the data science projects produced by generative AI models. We find the extent to which generative AI models can complete end-to-end data science projects varies considerably by model. Most recent models did well on structured tasks, but there were considerable differences on tasks that needed judgment. These findings suggest that while AI models could approximate entry-level data scientists on routine tasks, they require verification."}
{"id": "2602.14198", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14198", "abs": "https://arxiv.org/abs/2602.14198", "authors": ["Byeongchan Choi", "Junwon You", "Myung Ock Kim", "Jae-Hun Jung"], "title": "Zipf-Mandelbrot Scaling in Korean Court Music: Universal Patterns in Music", "comment": "20 pages, 5 figures, 4 tables", "summary": "Zipf's law, originally discovered in natural language and later generalized to the Zipf-Mandelbrot law, describes a power-law relationship between the frequency of a Zipfian element and its rank. Due to the semantic characteristics of this law, it has also been observed in musical data. However, most such studies have focused on Western music, and its applicability to non-Western music remains not well investigated. We analyzed 43 Korean court music pieces called Jeong-ak, spanning several centuries and written in the traditional Korean musical notation Jeongganbo. These pieces were transcribed into Western staff notation, and musical data such as pitch and duration were extracted. Using pitch, duration, and their paired combinations as Zipfian units, we found that Korean music also fits the Zipf-Mandelbrot law to a high degree, particularly for the paired pitch-duration unit. Korean music has evolved collectively over long periods, smoothing idiosyncratic variations and producing forms that are widely understandable among people. This collective evolution appears to have played a significant role in shaping the characteristics that lead to the satisfaction of Zipf-Mandelbrot law. Our findings provide additional evidence that Zipf-Mandelbrot scaling in musical data is universal across cultures. We further show that the joint distribution of two independent Zipfian data sets follows the Zipf-Mandelbrot law; in this sense, our result does not merely extend Zipf's law but deepens our understanding of how scaling laws behave under composition and interaction, offering a more unified perspective on rank-based statistical regularities."}
{"id": "2602.13942", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13942", "abs": "https://arxiv.org/abs/2602.13942", "authors": ["Zexuan Sun", "Garvesh Raskutti"], "title": "A Theoretical Framework for LLM Fine-tuning Using Early Stopping for Non-random Initialization", "comment": null, "summary": "In the era of large language models (LLMs), fine-tuning pretrained models has become ubiquitous. Yet the theoretical underpinning remains an open question. A central question is why only a few epochs of fine-tuning are typically sufficient to achieve strong performance on many different tasks. In this work, we approach this question by developing a statistical framework, combining rigorous early stopping theory with the attention-based Neural Tangent Kernel (NTK) for LLMs, offering new theoretical insights on fine-tuning practices. Specifically, we formally extend classical NTK theory [Jacot et al., 2018] to non-random (i.e., pretrained) initializations and provide a convergence guarantee for attention-based fine-tuning. One key insight provided by the theory is that the convergence rate with respect to sample size is closely linked to the eigenvalue decay rate of the empirical kernel matrix induced by the NTK. We also demonstrate how the framework can be used to explain task vectors for multiple tasks in LLMs. Finally, experiments with modern language models on real-world datasets provide empirical evidence supporting our theoretical insights."}
{"id": "2602.14280", "categories": ["stat.CO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14280", "abs": "https://arxiv.org/abs/2602.14280", "authors": ["Nick Polson", "Vadim Sokolov"], "title": "Fast Compute for ML Optimization", "comment": null, "summary": "We study optimization for losses that admit a variance-mean scale-mixture representation. Under this representation, each EM iteration is a weighted least squares update in which latent variables determine observation and parameter weights; these play roles analogous to Adam's second-moment scaling and AdamW's weight decay, but are derived from the model. The resulting Scale Mixture EM (SM-EM) algorithm removes user-specified learning-rate and momentum schedules. On synthetic ill-conditioned logistic regression benchmarks with $p \\in \\{20, \\ldots, 500\\}$, SM-EM with Nesterov acceleration attains up to $13\\times$ lower final loss than Adam tuned by learning-rate grid search. For a 40-point regularization path, sharing sufficient statistics across penalty values yields a $10\\times$ runtime reduction relative to the same tuned-Adam protocol. For the base (non-accelerated) algorithm, EM monotonicity guarantees nonincreasing objective values; adding Nesterov extrapolation trades this guarantee for faster empirical convergence."}
{"id": "2602.14203", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14203", "abs": "https://arxiv.org/abs/2602.14203", "authors": ["Lu Gao", "Pan Lu", "Fengxiang Qiao", "Joshua Qiang Li", "Yunpeng Zhang", "Yihao Ren"], "title": "Evaluating the Impact of COVID-19 on Transportation Infrastructure Funding", "comment": null, "summary": "The coronavirus disease 2019 (COVID-19) pandemic has caused a reduction in business and routine activity and resulted in less motor fuel consumption. Thus, the gas tax revenue is reduced which is the major funding resource supporting the rehabilitation and maintenance of transportation infrastructure systems. The focus of this study is to evaluate the impact of the COVID-19 pandemic on transportation infrastructure funds in the United States through analyzing the motor fuel consumption data. Machine learning models were developed by integrating COVID-19 scenarios, fuel consumptions, and demographic data. The best model achieves an R2-score of more than 95% and captures the fluctuations of fuel consumption during the pandemic. Using the developed model, we project future motor gas consumption for each state. For some states, the gas tax revenues are going to be 10%-15% lower than the pre-pandemic level for at least one or two years."}
{"id": "2602.14020", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14020", "abs": "https://arxiv.org/abs/2602.14020", "authors": ["Even He", "Zaizai Yan"], "title": "Computable Bernstein Certificates for Cross-Fitted Clipped Covariance Estimation", "comment": null, "summary": "We study operator-norm covariance estimation from heavy-tailed samples that may include a small fraction of arbitrary outliers. A simple and widely used safeguard is \\emph{Euclidean norm clipping}, but its accuracy depends critically on an unknown clipping level. We propose a cross-fitted clipped covariance estimator equipped with \\emph{fully computable} Bernstein-type deviation certificates, enabling principled data-driven tuning via a selector (\\emph{MinUpper}) that balances certified stochastic error and a robust hold-out proxy for clipping bias. The resulting procedure adapts to intrinsic complexity measures such as effective rank under mild tail regularity and retains meaningful guarantees under only finite fourth moments. Experiments on contaminated spiked-covariance benchmarks illustrate stable performance and competitive accuracy across regimes."}
{"id": "2602.14616", "categories": ["stat.CO", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14616", "abs": "https://arxiv.org/abs/2602.14616", "authors": ["Richard D. Paul", "Anton Stratmann", "Johann F. Jadebeck", "Martin Beyß", "Hanno Scharr", "David Rügamer", "Katharina Nöh"], "title": "Higher-Order Hit-&-Run Samplers for Linearly Constrained Densities", "comment": null, "summary": "Markov chain Monte Carlo (MCMC) sampling of densities restricted to linearly constrained domains is an important task arising in Bayesian treatment of inverse problems in the natural sciences. While efficient algorithms for uniform polytope sampling exist, much less work has dealt with more complex constrained densities. In particular, gradient information as used in unconstrained MCMC is not necessarily helpful in the constrained case, where the gradient may push the proposal's density out of the polytope. In this work, we propose a novel constrained sampling algorithm, which combines strengths of higher-order information, like the target's log-density's gradients and curvature, with the Hit-&-Run proposal, a simple mechanism which guarantees the generation of feasible proposals, fulfilling the linear constraints. Our extensive experiments demonstrate improved sampling efficiency on complex constrained densities over various constrained and unconstrained samplers."}
{"id": "2602.14349", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14349", "abs": "https://arxiv.org/abs/2602.14349", "authors": ["Jiaxin Cui", "Rohan Alexander"], "title": "Same Prompt, Different Outcomes: Evaluating the Reproducibility of Data Analysis by LLMs", "comment": null, "summary": "We systematically evaluate the reproducibility of data analysis conducted by Large Language Models (LLMs). We evaluate two prompting strategies, six models, and four temperature settings, with ten independent executions per configuration, yielding 480 total attempts. We assess the completion, concordance, validity, and consistency of each attempt and find considerable variation in the analytical results even for consistent configurations. This suggests, as with human data analysis, the data analysis conducted by LLMs can vary, even given the same task, data, and settings. Our results mean that if an LLM is being used to conduct data analysis, then it should be run multiple times independently and the distribution of results considered."}
{"id": "2602.13533", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.13533", "abs": "https://arxiv.org/abs/2602.13533", "authors": ["Yi Liu", "Huiman Barnhart", "Sean O'Brien", "Yuliya Lokhnygina", "Roland A. Matsouaka"], "title": "Estimation and Inference of the Win Ratio for Two Hierarchical Endpoints Subject to Censoring and Missing Data", "comment": null, "summary": "The win ratio (WR) is a widely used metric to compare treatments in randomized clinical trials with hierarchically ordered endpoints. Counting-based approaches, such as Pocock's algorithm, are the standard for WR estimation. However, this algorithm treats participants with censored or missing data inadequately, which may lead to biased and inefficient estimates, particularly in the presence of heterogeneous censoring or missing data between treatment groups. Although recent extensions have addressed some of these limitations for hierarchical time-to-event endpoints, no existing methods -- aside from the computationally intensive multiple imputation approach -- can accommodate settings that include non-survival endpoints that are subject to missing data. In this paper, we propose a simple nonparametric maximum likelihood estimator (NPMLE) of WR for two hierarchical endpoints that are subject to censoring and missing data. Our method uses all observed data, avoids strong parametric assumptions, and comes with a closed-form asymptotic variance estimator. We demonstrate its performance using simulation studies and two data examples, based on the HEART-FID and ISCHEMIA trials. The proposed method provides a consistent estimator, improves estimation efficiency, and is robust under non-informative censoring and missing at random (MAR) assumptions, offering a flexible alternative to existing WR estimation methods. A user-friendly R package, WinRS, is available to facilitate implementation."}
{"id": "2602.14029", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.14029", "abs": "https://arxiv.org/abs/2602.14029", "authors": ["Mingqi Wu", "Archer Y. Yang", "Qiang Sun"], "title": "Why Self-Training Helps and Hurts: Denoising vs. Signal Forgetting", "comment": "8 pages main, 29 pages in total", "summary": "Iterative self-training (self-distillation) repeatedly refits a model on pseudo-labels generated by its own predictions. We study this procedure in overparameterized linear regression: an initial estimator is trained on noisy labels, and each subsequent iterate is trained on fresh covariates with noiseless pseudo-labels from the previous model. In the high-dimensional regime, we derive deterministic-equivalent recursions for the prediction risk and effective noise across iterations, and prove that the empirical quantities concentrate sharply around these limits. The recursion separates two competing forces: a systematic component that grows with iteration due to progressive signal forgetting, and a stochastic component that decays due to denoising via repeated data-dependent projections. Their interaction yields a $U$-shaped test-risk curve and an optimal early-stopping time. In spiked covariance models, iteration further acts as an iteration-dependent spectral filter that preserves strong eigendirections while suppressing weaker ones, inducing an implicit form of soft feature selection distinct from ridge regression. Finally, we propose an iterated generalized cross-validation criterion and prove its uniform consistency for estimating the risk along the self-training trajectory, enabling fully data-driven selection of the stopping time and regularization. Experiments on synthetic covariances validate the theory and illustrate the predicted denoising-forgetting trade-off."}
{"id": "2602.14692", "categories": ["stat.CO", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.14692", "abs": "https://arxiv.org/abs/2602.14692", "authors": ["Mengxi Gao", "Gareth O. Roberts", "Andi Q. Wang"], "title": "Weak Poincaré inequalities for Deterministic-scan Metropolis-within-Gibbs samplers", "comment": "51 pages", "summary": "Using the framework of weak Poincaré inequalities, we analyze the convergence properties of deterministic-scan Metropolis-within-Gibbs samplers, an important class of Markov chain Monte Carlo algorithms. Our analysis applies to nonreversible Markov chains and yields explicit (subgeometric) convergence bounds through novel comparison techniques based on Dirichlet forms. We show that the joint chain inherits the convergence behavior of the marginal chain and conversely. In addition, we establish several fundamental results for weak Poincaré inequalities for discrete-time Markov chains, such as a tensorization property for independent chains. We apply our theoretical results through applications to algorithms for Bayesian inference for a hierarchical regression model and a diffusion model under discretely-observed data."}
{"id": "2602.14877", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14877", "abs": "https://arxiv.org/abs/2602.14877", "authors": ["Supun Manathunga", "Mart P. Janssen", "Yu Luo", "W. Alton Russell", "Mart Pothast"], "title": "When to repeat a biomarker test? Decomposing sources of variation from conditionally repeated measurements", "comment": "36 pages, 12 figures", "summary": "Repeating an imperfect biomarker test based on an initial result can introduce bias and influence misclassification risk. For example, in some blood donation settings, blood donors' hemoglobin is remeasured when the initial measurement falls below a minimum threshold for donor eligibility. This paper explores methods that use data resulting from processes with conditionally repeated biomarker measurement to decompose the variation in observed measurements of a continuous biomarker into population variability and variability arising from the measurement procedure. We present two frequentist approaches with analytical solutions, but these approaches perform poorly in a dataset of conditionally repeated blood donor hemoglobin measurements where normality assumptions are not met. We then develop a Bayesian hierarchical framework that allows for different distributional assumptions, which we apply to the blood donor hemoglobin dataset. Using a Bayesian hierarchical model that assumes normally distributed population hemoglobin and heavy tailed $t$-distributed measurement variation, we found that the total measurement variation accounted for 22\\% of the total variance among females and 25\\% among males, with population standard deviations of $1.07\\, \\rm g/dL$ for female donors and $1.28\\, \\rm g/dL$ for male donors. Our Bayesian framework can use data resulting from any clinical process with conditionally repeated biomarker measurements to estimate individuals' misclassification risk after one or more noisy continuous measurements and inform evidence-based conditional retesting decision rules."}
{"id": "2602.14029", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.14029", "abs": "https://arxiv.org/abs/2602.14029", "authors": ["Mingqi Wu", "Archer Y. Yang", "Qiang Sun"], "title": "Why Self-Training Helps and Hurts: Denoising vs. Signal Forgetting", "comment": "8 pages main, 29 pages in total", "summary": "Iterative self-training (self-distillation) repeatedly refits a model on pseudo-labels generated by its own predictions. We study this procedure in overparameterized linear regression: an initial estimator is trained on noisy labels, and each subsequent iterate is trained on fresh covariates with noiseless pseudo-labels from the previous model. In the high-dimensional regime, we derive deterministic-equivalent recursions for the prediction risk and effective noise across iterations, and prove that the empirical quantities concentrate sharply around these limits. The recursion separates two competing forces: a systematic component that grows with iteration due to progressive signal forgetting, and a stochastic component that decays due to denoising via repeated data-dependent projections. Their interaction yields a $U$-shaped test-risk curve and an optimal early-stopping time. In spiked covariance models, iteration further acts as an iteration-dependent spectral filter that preserves strong eigendirections while suppressing weaker ones, inducing an implicit form of soft feature selection distinct from ridge regression. Finally, we propose an iterated generalized cross-validation criterion and prove its uniform consistency for estimating the risk along the self-training trajectory, enabling fully data-driven selection of the stopping time and regularization. Experiments on synthetic covariances validate the theory and illustrate the predicted denoising-forgetting trade-off."}
{"id": "2602.13380", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.13380", "abs": "https://arxiv.org/abs/2602.13380", "authors": ["Luis G. Crespo"], "title": "Robust Design in the Presence of Aleatoric and Epistemic Uncertainty", "comment": null, "summary": "This paper proposes strategies for designing a system whose computational model is subject to aleatory and epistemic uncertainty. Aleatory variables, which are caused by randomness in physical parameters, are draws from a possibly unknown distribution; whereas epistemic variables, which are caused by ignorance in the value of fixed parameters, are free to take any value in a bounded set. Chance-constrained formulations enforcing the system requirements at a finite number of realizations of the uncertain parameters are proposed. These formulations trade off a lower objective value against a reduced robustness by eliminating an optimally chosen subset of such realizations. Risk-aware designs are obtained by accounting for the severity of the requirement violations resulting from this elimination process. Furthermore, we propose a computationally efficient design approach in which the training dataset is sequentially updated according to the results of high-fidelity reliability analyses of suboptimal designs. Robustness is evaluated by using Monte Carlo analysis and Robust Scenario Theory, with the latter approach accounting for the infinitely many values that the epistemic variables can take."}
{"id": "2602.14244", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14244", "abs": "https://arxiv.org/abs/2602.14244", "authors": ["Ala Emrani", "Amir Najafi", "Abolfazl Motahari"], "title": "Federated Ensemble Learning with Progressive Model Personalization", "comment": "42 pages", "summary": "Federated Learning provides a privacy-preserving paradigm for distributed learning, but suffers from statistical heterogeneity across clients. Personalized Federated Learning (PFL) mitigates this issue by considering client-specific models. A widely adopted approach in PFL decomposes neural networks into a shared feature extractor and client-specific heads. While effective, this design induces a fundamental tradeoff: deep or expressive shared components hinder personalization, whereas large local heads exacerbate overfitting under limited per-client data. Most existing methods rely on rigid, shallow heads, and therefore fail to navigate this tradeoff in a principled manner. In this work, we propose a boosting-inspired framework that enables a smooth control of this tradeoff. Instead of training a single personalized model, we construct an ensemble of $T$ models for each client. Across boosting iterations, the depth of the personalized component are progressively increased, while its effective complexity is systematically controlled via low-rank factorization or width shrinkage. This design simultaneously limits overfitting and substantially reduces per-client bias by allowing increasingly expressive personalization. We provide theoretical analysis that establishes generalization bounds with favorable dependence on the average local sample size and the total number of clients. Specifically, we prove that the complexity of the shared layers is effectively suppressed, while the dependence on the boosting horizon $T$ is controlled through parameter reduction. Notably, we provide a novel nonlinear generalization guarantee for decoupled PFL models. Extensive experiments on benchmark and real-world datasets (e.g., EMNIST, CIFAR-10/100, and Sent140) demonstrate that the proposed framework consistently outperforms state-of-the-art PFL methods under heterogeneous data distributions."}
{"id": "2602.13888", "categories": ["stat.ME", "stat.AP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13888", "abs": "https://arxiv.org/abs/2602.13888", "authors": ["The Tien Mai", "Zhi Zhao"], "title": "Mixture-of-experts Wishart model for covariance matrices with an application to Cancer drug screening", "comment": null, "summary": "Covariance matrices arise naturally in different scientific fields, including finance, genomics, and neuroscience, where they encode dependence structures and reveal essential features of complex multivariate systems. In this work, we introduce a comprehensive Bayesian framework for analyzing heterogeneous covariance data through both classical mixture models and a novel mixture-of-experts Wishart (MoE-Wishart) model. The proposed MoE-Wishart model extends standard Wishart mixtures by allowing mixture weights to depend on predictors through a multinomial logistic gating network. This formulation enables the model to capture complex, nonlinear heterogeneity in covariance structures and to adapt subpopulation membership probabilities to covariate-dependent patterns. To perform inference, we develop an efficient Gibbs-within-Metropolis-Hastings sampling algorithm tailored to the geometry of the Wishart likelihood and the gating network. We additionally derive an Expectation-Maximization algorithm for maximum likelihood estimation in the mixture-of-experts setting. Extensive simulation studies demonstrate that the proposed Bayesian and maximum likelihood estimators achieve accurate subpopulation recovery and estimation under a range of heterogeneous covariance scenarios. Finally, we present an innovative application of our methodology to a challenging dataset: cancer drug sensitivity profiles, illustrating the ability of the MoE-Wishart model to leverage covariance across drug dosages and replicate measurements.\n  Our methods are implemented in the \\texttt{R} package \\texttt{moewishart} available at https://github.com/zhizuio/moewishart ."}
{"id": "2602.15007", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.15007", "abs": "https://arxiv.org/abs/2602.15007", "authors": ["Dirk Douwes-Schultz", "Rob Deardon", "Alexandra M. Schmidt"], "title": "Hidden Markov Individual-level Models of Infectious Disease Transmission", "comment": null, "summary": "Individual-level epidemic models are increasingly being used to help understand the transmission dynamics of various infectious diseases. However, fitting such models to individual-level epidemic data is challenging, as we often only know when an individual's disease status was detected (e.g., when they showed symptoms) and not when they were infected or removed. We propose an autoregressive coupled hidden Markov model to infer unknown infection and removal times, as well as other model parameters, from a single observed detection time for each detected individual. Unlike more traditional data augmentation methods used in epidemic modelling, we do not assume that this detection time corresponds to infection or removal or that infected individuals must at some point be detected. Bayesian coupled hidden Markov models have been used previously for individual-level epidemic data. However, these approaches assumed each individual was continuously tested and that the tests were independent. In practice, individuals are often only tested until their first positive test, and even if they are continuously tested, only the initial detection times may be reported. In addition, multiple tests on the same individual may not be independent. We accommodate these scenarios by assuming that the probability of detecting the disease can depend on past observations, which allows us to fit a much wider range of practical applications. We illustrate the flexibility of our approach by fitting two examples: an experiment on the spread of tomato spot wilt virus in pepper plants and an outbreak of norovirus among nurses in a hospital."}
{"id": "2602.14981", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14981", "abs": "https://arxiv.org/abs/2602.14981", "authors": ["Tianni Zhang", "Yuyao Wang", "Yu Lu", "and Mengfei Ran"], "title": "Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models", "comment": null, "summary": "Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance."}
{"id": "2602.13442", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13442", "abs": "https://arxiv.org/abs/2602.13442", "authors": ["Jia Zhou", "Douglas Landsittel"], "title": "Measuring Neural Network Complexity via Effective Degrees of Freedom", "comment": "20 pages, 3 figures, 6 tables", "summary": "Quantifying the complexity of feed-forward neural networks (FFNNs) remains challenging due to their nonlinear, hierarchical structure and numerous parameters. We apply generalized degrees of freedom (GDF) to measure model complexity in FFNNs with binary outcomes, adapting the algorithm for discrete responses. We compare GDF with both the effective number of parameters derived via log-likelihood cross-validation and the null degrees of freedom of Landsittel et al. Through simulation studies and a real data analysis, we demonstrate that GDF provides a robust assessment of model complexity for neural network models, as it depends only on the sensitivity of fitted values to perturbations in the observed responses rather than on assumptions about the likelihood. In contrast, cross-validation-based estimates of model complexity and the null degrees of freedom rely on the correctness of the assumed likelihood and may exhibit substantial variability. We find that GDF, cross-validation-based measures, and null degrees of freedom yield similar assessments of model complexity only when the fitted model adequately represents the data-generating mechanism. These findings highlight GDF as a stable and broadly applicable measure of model complexity for neural networks in statistical modeling."}
{"id": "2602.14478", "categories": ["stat.ML", "cs.DS", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.14478", "abs": "https://arxiv.org/abs/2602.14478", "authors": ["Thanh Dang", "Jiaming Liang"], "title": "Constrained and Composite Sampling via Proximal Sampler", "comment": "The main paper is 13 pages; the rest are appendices", "summary": "We study two log-concave sampling problems: constrained sampling and composite sampling. First, we consider sampling from a target distribution with density proportional to $\\exp(-f(x))$ supported on a convex set $K \\subset \\mathbb{R}^d$, where $f$ is convex. The main challenge is enforcing feasibility without degrading mixing. Using an epigraph transformation, we reduce this task to sampling from a nearly uniform distribution over a lifted convex set in $\\mathbb{R}^{d+1}$. We then solve the lifted problem using a proximal sampler. Assuming only a separation oracle for $K$ and a subgradient oracle for $f$, we develop an implementation of the proximal sampler based on the cutting-plane method and rejection sampling. Unlike existing constrained samplers that rely on projection, reflection, barrier functions, or mirror maps, our approach enforces feasibility using only minimal oracle access, resulting in a practical and unbiased sampler without knowing the geometry of the constraint set.\n  Second, we study composite sampling, where the target is proportional to $\\exp(-f(x)-h(x))$ with closed and convex $f$ and $h$. This composite structure is standard in Bayesian inference with $f$ modeling data fidelity and $h$ encoding prior information. We reduce composite sampling via an epigraph lifting of $h$ to constrained sampling in $\\mathbb{R}^{d+1}$, which allows direct application of the constrained sampling algorithm developed in the first part. This reduction results in a double epigraph lifting formulation in $\\mathbb{R}^{d+2}$, on which we apply a proximal sampler. By keeping $f$ and $h$ separate, we further demonstrate how different combinations of oracle access (such as subgradient and proximal) can be leveraged to construct separation oracles for the lifted problem. For both sampling problems, we establish mixing time bounds measured in Rényi and $χ^2$ divergences."}
{"id": "2602.14607", "categories": ["stat.ME", "cs.LG", "math.NA", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.14607", "abs": "https://arxiv.org/abs/2602.14607", "authors": ["Nathan Kirk"], "title": "A Bayesian Approach to Low-Discrepancy Subset Selection", "comment": "13 pages, 3 figures, mODa14", "summary": "Low-discrepancy designs play a central role in quasi-Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP-hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP-hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria."}
{"id": "2602.13475", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13475", "abs": "https://arxiv.org/abs/2602.13475", "authors": ["Xiang Meng", "Lu Tian", "Kenneth Kehl", "Hajime Uno"], "title": "Efficient and Debiased Learning of Average Hazard Under Non-Proportional Hazards", "comment": "Main paper: 24 pages and 2 figures; Reference and Supplement: 22 pages and 8 Figures", "summary": "The hazard ratio from the Cox proportional hazards model is a ubiquitous summary of treatment effect. However, when hazards are non-proportional, the hazard ratio can lose a stable causal interpretation and become study-dependent because it effectively averages time-varying effects with weights determined by follow-up and censoring. We consider the average hazard (AH) as an alternative causal estimand: a population-level person-time event rate that remains well-defined and interpretable without assuming proportional hazards. Although AH can be estimated nonparametrically and regression-style adjustments have been proposed, existing approaches do not provide a general framework for flexible, high-dimensional nuisance estimation with valid sqrt{n} inference. We address this gap by developing a semiparametric, doubly robust framework for covariate-adjusted AH. We establish pathwise differentiability of AH in the nonparametric model, derive its efficient influence function, and construct cross-fitted, debiased estimators that leverage machine learning for nuisance estimation while retaining asymptotically normal, sqrt{n}-consistent inference under mild product-rate conditions. Simulations demonstrate that the proposed estimator achieves small bias and near-nominal confidence-interval coverage across proportional and non-proportional hazards settings, including crossing-hazards regimes where Cox-based summaries can be unstable. We illustrate practical utility in comparative effectiveness research by comparing immunotherapy regimens for advanced melanoma using SEER-Medicare linked data."}
{"id": "2602.13475", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13475", "abs": "https://arxiv.org/abs/2602.13475", "authors": ["Xiang Meng", "Lu Tian", "Kenneth Kehl", "Hajime Uno"], "title": "Efficient and Debiased Learning of Average Hazard Under Non-Proportional Hazards", "comment": "Main paper: 24 pages and 2 figures; Reference and Supplement: 22 pages and 8 Figures", "summary": "The hazard ratio from the Cox proportional hazards model is a ubiquitous summary of treatment effect. However, when hazards are non-proportional, the hazard ratio can lose a stable causal interpretation and become study-dependent because it effectively averages time-varying effects with weights determined by follow-up and censoring. We consider the average hazard (AH) as an alternative causal estimand: a population-level person-time event rate that remains well-defined and interpretable without assuming proportional hazards. Although AH can be estimated nonparametrically and regression-style adjustments have been proposed, existing approaches do not provide a general framework for flexible, high-dimensional nuisance estimation with valid sqrt{n} inference. We address this gap by developing a semiparametric, doubly robust framework for covariate-adjusted AH. We establish pathwise differentiability of AH in the nonparametric model, derive its efficient influence function, and construct cross-fitted, debiased estimators that leverage machine learning for nuisance estimation while retaining asymptotically normal, sqrt{n}-consistent inference under mild product-rate conditions. Simulations demonstrate that the proposed estimator achieves small bias and near-nominal confidence-interval coverage across proportional and non-proportional hazards settings, including crossing-hazards regimes where Cox-based summaries can be unstable. We illustrate practical utility in comparative effectiveness research by comparing immunotherapy regimens for advanced melanoma using SEER-Medicare linked data."}
{"id": "2602.14520", "categories": ["stat.ML", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2602.14520", "abs": "https://arxiv.org/abs/2602.14520", "authors": ["Farhana Taiyebah", "Abu Bucker Siddik", "Indronil Bhattacharjee", "Diane Oyen", "Soumi De", "Greg Olmschenk", "Constantinos Kalapotharakos"], "title": "Accelerating Posterior Inference from Pulsar Light Curves via Learned Latent Representations and Local Simulator-Guided Optimization", "comment": null, "summary": "Posterior inference from pulsar observations in the form of light curves is commonly performed using Markov chain Monte Carlo methods, which are accurate but computationally expensive. We introduce a framework that accelerates posterior inference while maintaining accuracy by combining learned latent representations with local simulator-guided optimization. A masked U-Net is first pretrained to reconstruct complete light curves from partial observations and to produce informative latent embeddings. Given a query light curve, we identify similar simulated light curves from the simulation bank by measuring similarity in the learned embedding space produced by pretrained U-Net encoder, yielding an initial empirical approximation to the posterior over parameters. This initialization is then refined using a local optimization procedure using hill-climbing updates, guided by a forward simulator, progressively shifting the empirical posterior toward higher-likelihood parameter regions. Experiments on the observed light curve of PSR J0030+0451, captured by NASA's Neutron Star Interior Composition Explorer (NICER), show that our method closely matches posterior estimates obtained using traditional MCMC methods while achieving 120 times reduction in inference time (from 24 hours to 12 minutes), demonstrating the effectiveness of learned representations and simulator-guided optimization for accelerated posterior inference."}
{"id": "2602.14981", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14981", "abs": "https://arxiv.org/abs/2602.14981", "authors": ["Tianni Zhang", "Yuyao Wang", "Yu Lu", "and Mengfei Ran"], "title": "Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models", "comment": null, "summary": "Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance."}
{"id": "2602.13533", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.13533", "abs": "https://arxiv.org/abs/2602.13533", "authors": ["Yi Liu", "Huiman Barnhart", "Sean O'Brien", "Yuliya Lokhnygina", "Roland A. Matsouaka"], "title": "Estimation and Inference of the Win Ratio for Two Hierarchical Endpoints Subject to Censoring and Missing Data", "comment": null, "summary": "The win ratio (WR) is a widely used metric to compare treatments in randomized clinical trials with hierarchically ordered endpoints. Counting-based approaches, such as Pocock's algorithm, are the standard for WR estimation. However, this algorithm treats participants with censored or missing data inadequately, which may lead to biased and inefficient estimates, particularly in the presence of heterogeneous censoring or missing data between treatment groups. Although recent extensions have addressed some of these limitations for hierarchical time-to-event endpoints, no existing methods -- aside from the computationally intensive multiple imputation approach -- can accommodate settings that include non-survival endpoints that are subject to missing data. In this paper, we propose a simple nonparametric maximum likelihood estimator (NPMLE) of WR for two hierarchical endpoints that are subject to censoring and missing data. Our method uses all observed data, avoids strong parametric assumptions, and comes with a closed-form asymptotic variance estimator. We demonstrate its performance using simulation studies and two data examples, based on the HEART-FID and ISCHEMIA trials. The proposed method provides a consistent estimator, improves estimation efficiency, and is robust under non-informative censoring and missing at random (MAR) assumptions, offering a flexible alternative to existing WR estimation methods. A user-friendly R package, WinRS, is available to facilitate implementation."}
{"id": "2602.13518", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.13518", "abs": "https://arxiv.org/abs/2602.13518", "authors": ["Nils Lid Hjort"], "title": "Towards Semiparametric Bandwidth Selectors for Kernel Density Estimators", "comment": "26 pages, no figures; technical report from 1999, needing additional numerical work to become a full paper", "summary": "There is an intense and partly recent literature focussing on the problem of selecting the bandwidth parameter for kernel density estimators. Available methods are largely `very nonparametric', in the sense of not requiring any knowledge about the underlying density, or `very parametric', like the normality-based reference rule. This report aims at widening the scope towards the inclusion of many semiparametric bandwidth selectors, via Hermite type expansions aroundthe normal distribution. The resulting bandwidths may be seen as carrying out suitable corrections on the normal reference rule, requiring a low number of extra coefficients to be estimated from data.\n  The present report introduces and discusses some basic ideas and develops the necessary initial theory, but modestly chooses to stop short of giving precise recommendations for specific procedures among the many possible constructions. This will require some further analysis, numerical work, and some simulation-based exploration."}
{"id": "2602.14642", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14642", "abs": "https://arxiv.org/abs/2602.14642", "authors": ["Matthaios Chatzopoulos", "Phaedon-Stelios Koutsourelakis"], "title": "GenPANIS: A Latent-Variable Generative Framework for Forward and Inverse PDE Problems in Multiphase Media", "comment": null, "summary": "Inverse problems and inverse design in multiphase media, i.e., recovering or engineering microstructures to achieve target macroscopic responses, require operating on discrete-valued material fields, rendering the problem non-differentiable and incompatible with gradient-based methods. Existing approaches either relax to continuous approximations, compromising physical fidelity, or employ separate heavyweight models for forward and inverse tasks. We propose GenPANIS, a unified generative framework that preserves exact discrete microstructures while enabling gradient-based inference through continuous latent embeddings. The model learns a joint distribution over microstructures and PDE solutions, supporting bidirectional inference (forward prediction and inverse recovery) within a single architecture. The generative formulation enables training with unlabeled data, physics residuals, and minimal labeled pairs. A physics-aware decoder incorporating a differentiable coarse-grained PDE solver preserves governing equation structure, enabling extrapolation to varying boundary conditions and microstructural statistics. A learnable normalizing flow prior captures complex posterior structure for inverse problems. Demonstrated on Darcy flow and Helmholtz equations, GenPANIS maintains accuracy on challenging extrapolative scenarios - including unseen boundary conditions, volume fractions, and microstructural morphologies, with sparse, noisy observations. It outperforms state-of-the-art methods while using 10 - 100 times fewer parameters and providing principled uncertainty quantification."}
{"id": "2602.13888", "categories": ["stat.ME", "stat.AP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13888", "abs": "https://arxiv.org/abs/2602.13888", "authors": ["The Tien Mai", "Zhi Zhao"], "title": "Mixture-of-experts Wishart model for covariance matrices with an application to Cancer drug screening", "comment": null, "summary": "Covariance matrices arise naturally in different scientific fields, including finance, genomics, and neuroscience, where they encode dependence structures and reveal essential features of complex multivariate systems. In this work, we introduce a comprehensive Bayesian framework for analyzing heterogeneous covariance data through both classical mixture models and a novel mixture-of-experts Wishart (MoE-Wishart) model. The proposed MoE-Wishart model extends standard Wishart mixtures by allowing mixture weights to depend on predictors through a multinomial logistic gating network. This formulation enables the model to capture complex, nonlinear heterogeneity in covariance structures and to adapt subpopulation membership probabilities to covariate-dependent patterns. To perform inference, we develop an efficient Gibbs-within-Metropolis-Hastings sampling algorithm tailored to the geometry of the Wishart likelihood and the gating network. We additionally derive an Expectation-Maximization algorithm for maximum likelihood estimation in the mixture-of-experts setting. Extensive simulation studies demonstrate that the proposed Bayesian and maximum likelihood estimators achieve accurate subpopulation recovery and estimation under a range of heterogeneous covariance scenarios. Finally, we present an innovative application of our methodology to a challenging dataset: cancer drug sensitivity profiles, illustrating the ability of the MoE-Wishart model to leverage covariance across drug dosages and replicate measurements.\n  Our methods are implemented in the \\texttt{R} package \\texttt{moewishart} available at https://github.com/zhizuio/moewishart ."}
{"id": "2602.13533", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.13533", "abs": "https://arxiv.org/abs/2602.13533", "authors": ["Yi Liu", "Huiman Barnhart", "Sean O'Brien", "Yuliya Lokhnygina", "Roland A. Matsouaka"], "title": "Estimation and Inference of the Win Ratio for Two Hierarchical Endpoints Subject to Censoring and Missing Data", "comment": null, "summary": "The win ratio (WR) is a widely used metric to compare treatments in randomized clinical trials with hierarchically ordered endpoints. Counting-based approaches, such as Pocock's algorithm, are the standard for WR estimation. However, this algorithm treats participants with censored or missing data inadequately, which may lead to biased and inefficient estimates, particularly in the presence of heterogeneous censoring or missing data between treatment groups. Although recent extensions have addressed some of these limitations for hierarchical time-to-event endpoints, no existing methods -- aside from the computationally intensive multiple imputation approach -- can accommodate settings that include non-survival endpoints that are subject to missing data. In this paper, we propose a simple nonparametric maximum likelihood estimator (NPMLE) of WR for two hierarchical endpoints that are subject to censoring and missing data. Our method uses all observed data, avoids strong parametric assumptions, and comes with a closed-form asymptotic variance estimator. We demonstrate its performance using simulation studies and two data examples, based on the HEART-FID and ISCHEMIA trials. The proposed method provides a consistent estimator, improves estimation efficiency, and is robust under non-informative censoring and missing at random (MAR) assumptions, offering a flexible alternative to existing WR estimation methods. A user-friendly R package, WinRS, is available to facilitate implementation."}
{"id": "2602.14862", "categories": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.14862", "abs": "https://arxiv.org/abs/2602.14862", "authors": ["Pierre-Alexandre Mattei", "Bruno Loureiro"], "title": "The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling", "comment": null, "summary": "Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model."}
{"id": "2602.14414", "categories": ["stat.ME", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14414", "abs": "https://arxiv.org/abs/2602.14414", "authors": ["Abhinandan Dalal", "Iris Horng", "Yang Feng", "Dylan S. Small"], "title": "The Role of Measured Covariates in Assessing Sensitivity to Unmeasured Confounding", "comment": null, "summary": "Sensitivity analysis is widely used to assess the robustness of causal conclusions in observational studies, yet its interaction with the structure of measured covariates is often overlooked. When latent confounders cannot be directly adjusted for and are instead controlled using proxy variables, strong associations between exposure and measured proxies can amplify sensitivity to residual confounding. We formalize this phenomenon in linear regression settings by showing that a simple ratio involving the exposure model coefficient and residual exposure variance provides an observable measure of this increased sensitivity. Applying our framework to smoking and lung cancer, we document how growing socioeconomic stratification in smoking behavior over time leads to heightened sensitivity to unmeasured confounding in more recent data. These results highlight the importance of multicollinearity when interpreting sensitivity analyses based on proxy adjustment."}
{"id": "2602.13538", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.13538", "abs": "https://arxiv.org/abs/2602.13538", "authors": ["Antik Chakraborty", "Fei Xue"], "title": "Empirical Bayes data integreation for multi-response regression", "comment": "To appear in Statistica Sinica", "summary": "Motivated by applications in tissue-wide association studies (TWAS), we develop a flexible and theoretically grounded empirical Bayes approach for integrating %vector-valued outcomes data obtained from different sources. We propose a linear shrinkage estimator that effectively shrinks singular values of a data matrix. This problem is closely connected to estimating covariance matrices under a specific loss, for which we develop asymptotically optimal estimators. The basic linear shrinkage estimator is then extended to a local linear shrinkage estimator, offering greater flexibility. Crucially, the proposed method works under sparse/dense or low-rank/non low-rank parameter settings unlike well-known sparse or reduced rank estimators in the literature. Furthermore, the empirical Bayes approach offers greater scalability in computation compared to intensive full Bayes procedures. The method is evaluated through an extensive set of numerical experiments, and applied to a real TWAS data obtained from the Genotype-Tissue Expression (GTEx) project."}
{"id": "2602.14934", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14934", "abs": "https://arxiv.org/abs/2602.14934", "authors": ["Richard Bergna", "Stefan Depeweg", "Sergio Calvo-Ordoñez", "Jonathan Plenk", "Alvaro Cartea", "Jose Miguel Hernández-Lobato"], "title": "Activation-Space Uncertainty Quantification for Pretrained Networks", "comment": null, "summary": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time."}
{"id": "2602.14616", "categories": ["stat.CO", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14616", "abs": "https://arxiv.org/abs/2602.14616", "authors": ["Richard D. Paul", "Anton Stratmann", "Johann F. Jadebeck", "Martin Beyß", "Hanno Scharr", "David Rügamer", "Katharina Nöh"], "title": "Higher-Order Hit-&-Run Samplers for Linearly Constrained Densities", "comment": null, "summary": "Markov chain Monte Carlo (MCMC) sampling of densities restricted to linearly constrained domains is an important task arising in Bayesian treatment of inverse problems in the natural sciences. While efficient algorithms for uniform polytope sampling exist, much less work has dealt with more complex constrained densities. In particular, gradient information as used in unconstrained MCMC is not necessarily helpful in the constrained case, where the gradient may push the proposal's density out of the polytope. In this work, we propose a novel constrained sampling algorithm, which combines strengths of higher-order information, like the target's log-density's gradients and curvature, with the Hit-&-Run proposal, a simple mechanism which guarantees the generation of feasible proposals, fulfilling the linear constraints. Our extensive experiments demonstrate improved sampling efficiency on complex constrained densities over various constrained and unconstrained samplers."}
{"id": "2602.13635", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.13635", "abs": "https://arxiv.org/abs/2602.13635", "authors": ["Genshiro Kitagawa"], "title": "Backward Smoothing versus Fixed-Lag Smoothing in Particle Filters", "comment": "17 pages, 5 tables, 7 figures", "summary": "Particle smoothing enables state estimation in nonlinear and non-Gaussian state-space models, but its practical use is often limited by high computational cost. Backward smoothing methods such as the Forward Filter Backward Smoother (FFBS) and its marginal form (FFBSm) can achieve high accuracy, yet typically require quadratic computational complexity in the number of particles. This paper examines the accuracy--computational cost trade-offs of particle smoothing methods through a trend-estimation example. Fixed-lag smoothing, FFBS, and FFBSm are compared under Gaussian and heavy-tailed (Cauchy-type) system noise, with particular attention to O(m) approximations of FFBSm based on subsampling and local neighborhood restrictions. The results show that FFBS and FFBSm outperform fixed-lag smoothing at a fixed particle number, while fixed-lag smoothing often achieves higher accuracy under equal computational time. Moreover, efficient FFBSm approximations are effective for Gaussian transitions but become less advantageous for heavy-tailed dynamics."}
{"id": "2602.13442", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13442", "abs": "https://arxiv.org/abs/2602.13442", "authors": ["Jia Zhou", "Douglas Landsittel"], "title": "Measuring Neural Network Complexity via Effective Degrees of Freedom", "comment": "20 pages, 3 figures, 6 tables", "summary": "Quantifying the complexity of feed-forward neural networks (FFNNs) remains challenging due to their nonlinear, hierarchical structure and numerous parameters. We apply generalized degrees of freedom (GDF) to measure model complexity in FFNNs with binary outcomes, adapting the algorithm for discrete responses. We compare GDF with both the effective number of parameters derived via log-likelihood cross-validation and the null degrees of freedom of Landsittel et al. Through simulation studies and a real data analysis, we demonstrate that GDF provides a robust assessment of model complexity for neural network models, as it depends only on the sensitivity of fitted values to perturbations in the observed responses rather than on assumptions about the likelihood. In contrast, cross-validation-based estimates of model complexity and the null degrees of freedom rely on the correctness of the assumed likelihood and may exhibit substantial variability. We find that GDF, cross-validation-based measures, and null degrees of freedom yield similar assessments of model complexity only when the fitted model adequately represents the data-generating mechanism. These findings highlight GDF as a stable and broadly applicable measure of model complexity for neural networks in statistical modeling."}
{"id": "2602.14835", "categories": ["stat.ME", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14835", "abs": "https://arxiv.org/abs/2602.14835", "authors": ["Evan Hadfield"], "title": "The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research", "comment": "2 figures, 9 tables. Open-source library: https://github.com/collect-intel/gri", "summary": "Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks."}
{"id": "2602.13872", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13872", "abs": "https://arxiv.org/abs/2602.13872", "authors": ["Chris Holmes", "Stephen Walker"], "title": "Predicting fixed-sample test decisions enables anytime-valid inference", "comment": null, "summary": "Statistical hypothesis tests typically use prespecified sample sizes, yet data often arrive sequentially. Interim analyses invalidate classical error guarantees, while existing sequential methods require rigid testing preschedules or incur substantial losses in statistical power. We introduce a simple procedure that transforms any fixed-sample hypothesis test into an anytime-valid test while ensuring Type-I error control and near-optimal power with substantial sample savings when the null hypothesis is false. At each step, the procedure predicts the probability that a classical test would reject the null hypothesis at its fixed-sample size, treating future observations as missing data under the null hypothesis. Thresholding this probability yields an anytime-valid stopping rule. In areas such as clinical trials, stopping early and safely can ensure that subjects receive the best treatments and accelerate the development of effective therapies."}
{"id": "2602.13475", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13475", "abs": "https://arxiv.org/abs/2602.13475", "authors": ["Xiang Meng", "Lu Tian", "Kenneth Kehl", "Hajime Uno"], "title": "Efficient and Debiased Learning of Average Hazard Under Non-Proportional Hazards", "comment": "Main paper: 24 pages and 2 figures; Reference and Supplement: 22 pages and 8 Figures", "summary": "The hazard ratio from the Cox proportional hazards model is a ubiquitous summary of treatment effect. However, when hazards are non-proportional, the hazard ratio can lose a stable causal interpretation and become study-dependent because it effectively averages time-varying effects with weights determined by follow-up and censoring. We consider the average hazard (AH) as an alternative causal estimand: a population-level person-time event rate that remains well-defined and interpretable without assuming proportional hazards. Although AH can be estimated nonparametrically and regression-style adjustments have been proposed, existing approaches do not provide a general framework for flexible, high-dimensional nuisance estimation with valid sqrt{n} inference. We address this gap by developing a semiparametric, doubly robust framework for covariate-adjusted AH. We establish pathwise differentiability of AH in the nonparametric model, derive its efficient influence function, and construct cross-fitted, debiased estimators that leverage machine learning for nuisance estimation while retaining asymptotically normal, sqrt{n}-consistent inference under mild product-rate conditions. Simulations demonstrate that the proposed estimator achieves small bias and near-nominal confidence-interval coverage across proportional and non-proportional hazards settings, including crossing-hazards regimes where Cox-based summaries can be unstable. We illustrate practical utility in comparative effectiveness research by comparing immunotherapy regimens for advanced melanoma using SEER-Medicare linked data."}
{"id": "2602.13888", "categories": ["stat.ME", "stat.AP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13888", "abs": "https://arxiv.org/abs/2602.13888", "authors": ["The Tien Mai", "Zhi Zhao"], "title": "Mixture-of-experts Wishart model for covariance matrices with an application to Cancer drug screening", "comment": null, "summary": "Covariance matrices arise naturally in different scientific fields, including finance, genomics, and neuroscience, where they encode dependence structures and reveal essential features of complex multivariate systems. In this work, we introduce a comprehensive Bayesian framework for analyzing heterogeneous covariance data through both classical mixture models and a novel mixture-of-experts Wishart (MoE-Wishart) model. The proposed MoE-Wishart model extends standard Wishart mixtures by allowing mixture weights to depend on predictors through a multinomial logistic gating network. This formulation enables the model to capture complex, nonlinear heterogeneity in covariance structures and to adapt subpopulation membership probabilities to covariate-dependent patterns. To perform inference, we develop an efficient Gibbs-within-Metropolis-Hastings sampling algorithm tailored to the geometry of the Wishart likelihood and the gating network. We additionally derive an Expectation-Maximization algorithm for maximum likelihood estimation in the mixture-of-experts setting. Extensive simulation studies demonstrate that the proposed Bayesian and maximum likelihood estimators achieve accurate subpopulation recovery and estimation under a range of heterogeneous covariance scenarios. Finally, we present an innovative application of our methodology to a challenging dataset: cancer drug sensitivity profiles, illustrating the ability of the MoE-Wishart model to leverage covariance across drug dosages and replicate measurements.\n  Our methods are implemented in the \\texttt{R} package \\texttt{moewishart} available at https://github.com/zhizuio/moewishart ."}
{"id": "2602.13872", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13872", "abs": "https://arxiv.org/abs/2602.13872", "authors": ["Chris Holmes", "Stephen Walker"], "title": "Predicting fixed-sample test decisions enables anytime-valid inference", "comment": null, "summary": "Statistical hypothesis tests typically use prespecified sample sizes, yet data often arrive sequentially. Interim analyses invalidate classical error guarantees, while existing sequential methods require rigid testing preschedules or incur substantial losses in statistical power. We introduce a simple procedure that transforms any fixed-sample hypothesis test into an anytime-valid test while ensuring Type-I error control and near-optimal power with substantial sample savings when the null hypothesis is false. At each step, the procedure predicts the probability that a classical test would reject the null hypothesis at its fixed-sample size, treating future observations as missing data under the null hypothesis. Thresholding this probability yields an anytime-valid stopping rule. In areas such as clinical trials, stopping early and safely can ensure that subjects receive the best treatments and accelerate the development of effective therapies."}
{"id": "2602.14286", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14286", "abs": "https://arxiv.org/abs/2602.14286", "authors": ["Weijie Su", "Ruodu Wang", "Zinan Zhao"], "title": "Online LLM watermark detection via e-processes", "comment": null, "summary": "Watermarking for large language models (LLMs) has emerged as an effective tool for distinguishing AI-generated text from human-written content. Statistically, watermark schemes induce dependence between generated tokens and a pseudo-random sequence, reducing watermark detection to a hypothesis testing problem on independence. We develop a unified framework for LLM watermark detection based on e-processes, providing anytime-valid guarantees for online testing. We propose various methods to construct empirically adaptive e-processes that can enhance the detection power. In addition, theoretical results are established to characterize the power properties of the proposed procedures. Some experiments demonstrate that the proposed framework achieves competitive performance compared to existing watermark detection methods."}
{"id": "2602.13888", "categories": ["stat.ME", "stat.AP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13888", "abs": "https://arxiv.org/abs/2602.13888", "authors": ["The Tien Mai", "Zhi Zhao"], "title": "Mixture-of-experts Wishart model for covariance matrices with an application to Cancer drug screening", "comment": null, "summary": "Covariance matrices arise naturally in different scientific fields, including finance, genomics, and neuroscience, where they encode dependence structures and reveal essential features of complex multivariate systems. In this work, we introduce a comprehensive Bayesian framework for analyzing heterogeneous covariance data through both classical mixture models and a novel mixture-of-experts Wishart (MoE-Wishart) model. The proposed MoE-Wishart model extends standard Wishart mixtures by allowing mixture weights to depend on predictors through a multinomial logistic gating network. This formulation enables the model to capture complex, nonlinear heterogeneity in covariance structures and to adapt subpopulation membership probabilities to covariate-dependent patterns. To perform inference, we develop an efficient Gibbs-within-Metropolis-Hastings sampling algorithm tailored to the geometry of the Wishart likelihood and the gating network. We additionally derive an Expectation-Maximization algorithm for maximum likelihood estimation in the mixture-of-experts setting. Extensive simulation studies demonstrate that the proposed Bayesian and maximum likelihood estimators achieve accurate subpopulation recovery and estimation under a range of heterogeneous covariance scenarios. Finally, we present an innovative application of our methodology to a challenging dataset: cancer drug sensitivity profiles, illustrating the ability of the MoE-Wishart model to leverage covariance across drug dosages and replicate measurements.\n  Our methods are implemented in the \\texttt{R} package \\texttt{moewishart} available at https://github.com/zhizuio/moewishart ."}
{"id": "2602.14303", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.14303", "abs": "https://arxiv.org/abs/2602.14303", "authors": ["Isqeel Ogunsola", "Nurudeen Ajadi", "Gboyega Adepoju"], "title": "A New SMP Transformed Standard Weibull Distribution for Health Data Modelling", "comment": null, "summary": "New methods of extending base distributions are always invoke to increase their adaptability in modeling real life data. Recently, SMP method was introduced but Weibull distribution is yet to be explored through this method. First, we provide updated review on SMP transformed distributions. We then proposed and developed another extended Weibull distribution through this technique named SMPtW. Importantly, twelve of its statistical properties - reliability measures, quantile function, moment, stress-strength, mean waiting time, moment generating function, characteristics function, renyi entropy, order statistics, mean residual life and mode, were derived and studied extensively. The hazard function has a decreasing, increasing and constant shapes. We found a relation between the quantile of SMPtW and that of SMP Pareto distribution despite their difference in density functions. We adopt the inverse transform approach in random number generation and through simulation we evaluate maximum likelihood estimates (MLE) performance of its parameters. The result showed that MLE is consistent all through. The performance of the distribution was then examined using health dataset compared with five similar distributions. The results showed that three parameters SMPtW performed best among the competing models."}
{"id": "2602.14286", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14286", "abs": "https://arxiv.org/abs/2602.14286", "authors": ["Weijie Su", "Ruodu Wang", "Zinan Zhao"], "title": "Online LLM watermark detection via e-processes", "comment": null, "summary": "Watermarking for large language models (LLMs) has emerged as an effective tool for distinguishing AI-generated text from human-written content. Statistically, watermark schemes induce dependence between generated tokens and a pseudo-random sequence, reducing watermark detection to a hypothesis testing problem on independence. We develop a unified framework for LLM watermark detection based on e-processes, providing anytime-valid guarantees for online testing. We propose various methods to construct empirically adaptive e-processes that can enhance the detection power. In addition, theoretical results are established to characterize the power properties of the proposed procedures. Some experiments demonstrate that the proposed framework achieves competitive performance compared to existing watermark detection methods."}
{"id": "2602.14387", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.14387", "abs": "https://arxiv.org/abs/2602.14387", "authors": ["Jon Wakefield", "Jitong Jiang", "Yunhan Wu"], "title": "Automatic Variance Adjustment for Small Area Estimation", "comment": null, "summary": "Small area estimation (SAE) is a common endeavor and is used in a variety of disciplines. In low- and middle-income countries (LMICs), in which household surveys provide the most reliable and timely source of data, SAE is vital for highlighting disparities in health and demographic indicators. Weighted estimators are ideal for inference, but for fine geographical partitions in which there are insufficient data, SAE models are required. The most common approach is Fay-Herriot area-level modeling in which the data requirements are a weighted estimate and an associated variance estimate. The latter can be undefined or unstable when data are sparse and so we propose a principled modification which is based on augmenting the available data with a prior sample from a hypothetical survey. This adjustment is generally available, respects the design and is simple to implement. We examine the empirical properties of the adjustment through simulation and illustrate its use with wasting data from a 2018 Zambian Demographic and Health Survey. The modification is implemented as an automatic remedy in the R package surveyPrev, which provides a comprehensive suite of tools for conducing SAE in LMICs."}
{"id": "2602.14440", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14440", "abs": "https://arxiv.org/abs/2602.14440", "authors": ["Harri Vanhems", "Yue Zhao", "Peng Shi", "Archer Y. Yang"], "title": "CAIRO: Decoupling Order from Scale in Regression", "comment": null, "summary": "Standard regression methods typically optimize a single pointwise objective, such as mean squared error, which conflates the learning of ordering with the learning of scale. This coupling renders models vulnerable to outliers and heavy-tailed noise. We propose CAIRO (Calibrate After Initial Rank Ordering), a framework that decouples regression into two distinct stages. In the first stage, we learn a scoring function by minimizing a scale-invariant ranking loss; in the second, we recover the target scale via isotonic regression. We theoretically characterize a class of \"Optimal-in-Rank-Order\" objectives -- including variants of RankNet and Gini covariance -- and prove that they recover the ordering of the true conditional mean under mild assumptions. We further show that subsequent monotone calibration guarantees recovery of the true regression function. Empirically, CAIRO combines the representation learning of neural networks with the robustness of rank-based statistics. It matches the performance of state-of-the-art tree ensembles on tabular benchmarks and significantly outperforms standard regression objectives in regimes with heavy-tailed or heteroskedastic noise."}
{"id": "2602.14414", "categories": ["stat.ME", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14414", "abs": "https://arxiv.org/abs/2602.14414", "authors": ["Abhinandan Dalal", "Iris Horng", "Yang Feng", "Dylan S. Small"], "title": "The Role of Measured Covariates in Assessing Sensitivity to Unmeasured Confounding", "comment": null, "summary": "Sensitivity analysis is widely used to assess the robustness of causal conclusions in observational studies, yet its interaction with the structure of measured covariates is often overlooked. When latent confounders cannot be directly adjusted for and are instead controlled using proxy variables, strong associations between exposure and measured proxies can amplify sensitivity to residual confounding. We formalize this phenomenon in linear regression settings by showing that a simple ratio involving the exposure model coefficient and residual exposure variance provides an observable measure of this increased sensitivity. Applying our framework to smoking and lung cancer, we document how growing socioeconomic stratification in smoking behavior over time leads to heightened sensitivity to unmeasured confounding in more recent data. These results highlight the importance of multicollinearity when interpreting sensitivity analyses based on proxy adjustment."}
{"id": "2602.14981", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14981", "abs": "https://arxiv.org/abs/2602.14981", "authors": ["Tianni Zhang", "Yuyao Wang", "Yu Lu", "and Mengfei Ran"], "title": "Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models", "comment": null, "summary": "Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance."}
{"id": "2602.14440", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14440", "abs": "https://arxiv.org/abs/2602.14440", "authors": ["Harri Vanhems", "Yue Zhao", "Peng Shi", "Archer Y. Yang"], "title": "CAIRO: Decoupling Order from Scale in Regression", "comment": null, "summary": "Standard regression methods typically optimize a single pointwise objective, such as mean squared error, which conflates the learning of ordering with the learning of scale. This coupling renders models vulnerable to outliers and heavy-tailed noise. We propose CAIRO (Calibrate After Initial Rank Ordering), a framework that decouples regression into two distinct stages. In the first stage, we learn a scoring function by minimizing a scale-invariant ranking loss; in the second, we recover the target scale via isotonic regression. We theoretically characterize a class of \"Optimal-in-Rank-Order\" objectives -- including variants of RankNet and Gini covariance -- and prove that they recover the ordering of the true conditional mean under mild assumptions. We further show that subsequent monotone calibration guarantees recovery of the true regression function. Empirically, CAIRO combines the representation learning of neural networks with the robustness of rank-based statistics. It matches the performance of state-of-the-art tree ensembles on tabular benchmarks and significantly outperforms standard regression objectives in regimes with heavy-tailed or heteroskedastic noise."}
{"id": "2602.14607", "categories": ["stat.ME", "cs.LG", "math.NA", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.14607", "abs": "https://arxiv.org/abs/2602.14607", "authors": ["Nathan Kirk"], "title": "A Bayesian Approach to Low-Discrepancy Subset Selection", "comment": "13 pages, 3 figures, mODa14", "summary": "Low-discrepancy designs play a central role in quasi-Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP-hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP-hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria."}
{"id": "2602.14813", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.14813", "abs": "https://arxiv.org/abs/2602.14813", "authors": ["Gian Pietro Bellocca", "Ignacio Garrón", "Vladimir Rodríguez-Caballero", "Esther Ruiz"], "title": "The empirical distribution of sequential LS factors in Multi-level Dynamic Factor Models", "comment": null, "summary": "The research question we answer in this paper is whether the asymptotic distribution derived by Bai (2003) for Principal Components (PC) factors in dynamic factor models (DFMs) can approximate the empirical distribution of the sequential Least Squares (SLS) estimator of global and group-specific factors in multi-level dynamic factor models (ML-DFMs). Monte Carlo experiments confirm that under general forms of the idiosyncratic covariance matrix, the finite-sample distribution of SLS global and group-specific factors can be well approximated using the asymptotic distribution of PC factors. We also analyse the performance of alternative estimators of the asymptotic mean squared error (MSE) of the SLS factors and show that the MSE estimator that allows for idiosyncratic cross-sectional correlation and accounts for estimation uncertainty of factor loadings is best."}
{"id": "2602.14835", "categories": ["stat.ME", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.14835", "abs": "https://arxiv.org/abs/2602.14835", "authors": ["Evan Hadfield"], "title": "The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research", "comment": "2 figures, 9 tables. Open-source library: https://github.com/collect-intel/gri", "summary": "Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks."}
{"id": "2602.14942", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.14942", "abs": "https://arxiv.org/abs/2602.14942", "authors": ["Yichao Chen", "Weijing Tang", "Ji Zhu"], "title": "Balanced Stochastic Block Model for Community Detection in Signed Networks", "comment": null, "summary": "Community detection, discovering the underlying communities within a network from observed connections, is a fundamental problem in network analysis, yet it remains underexplored for signed networks. In signed networks, both edge connection patterns and edge signs are informative, and structural balance theory (e.g., triangles aligned with ``the enemy of my enemy is my friend'' and ``the friend of my friend is my friend'' are more prevalent) provides a global higher-order principle that guides community formation. We propose a Balanced Stochastic Block Model (BSBM), which incorporates balance theory into the network generating process such that balanced triangles are more likely to occur. We develop a fast profile pseudo-likelihood estimation algorithm with provable convergence and establish that our estimator achieves strong consistency under weaker signal conditions than methods for the binary SBM that rely solely on edge connectivity. Extensive simulation studies and two real-world signed networks demonstrate strong empirical performance."}
{"id": "2602.14981", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14981", "abs": "https://arxiv.org/abs/2602.14981", "authors": ["Tianni Zhang", "Yuyao Wang", "Yu Lu", "and Mengfei Ran"], "title": "Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models", "comment": null, "summary": "Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance."}
{"id": "2602.14991", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.14991", "abs": "https://arxiv.org/abs/2602.14991", "authors": ["Yue Zhan", "Cheng Zheng", "Ying Zhang"], "title": "Joint analysis for multivariate longitudinal and event time data with a change point anchored at interval-censored event time", "comment": null, "summary": "Huntington's disease (HD) is an autosomal dominant neurodegenerative disorder characterized by motor dysfunction, psychiatric disturbances, and cognitive decline. The onset of HD is marked by severe motor impairment, which may be predicted by prior cognitive decline and, in turn, exacerbate cognitive deficits. Clinical data, however, are often collected at discrete time points, so the timing of disease onset is subject to interval censoring. To address the challenges posed by such data, we develop a joint model for multivariate longitudinal biomarkers with a change point anchored at an interval-censored event time. The model simultaneously assesses the effects of longitudinal biomarkers on the event time and the changes in biomarker trajectories following the event. We conduct a comprehensive simulation study to demonstrate the finite-sample performance of the proposed method for causal inference. Finally, we apply the method to PREDICT-HD, a multisite observational cohort study of prodromal HD individuals, to ascertain how cognitive impairment and motor dysfunction interact during disease progression."}
{"id": "2602.13619", "categories": ["stat.ML", "cs.IT", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.13619", "abs": "https://arxiv.org/abs/2602.13619", "authors": ["Anuj Kumar Yadav", "Cemre Cadir", "Yanina Shkel", "Michael Gastpar"], "title": "Locally Private Parametric Methods for Change-Point Detection", "comment": "43 pages, 20 figures", "summary": "We study parametric change-point detection, where the goal is to identify distributional changes in time series, under local differential privacy. In the non-private setting, we derive improved finite-sample accuracy guarantees for a change-point detection algorithm based on the generalized log-likelihood ratio test, via martingale methods. In the private setting, we propose two locally differentially private algorithms based on randomized response and binary mechanisms, and analyze their theoretical performance. We derive bounds on detection accuracy and validate our results through empirical evaluation. Our results characterize the statistical cost of local differential privacy in change-point detection and show how privacy degrades performance relative to a non-private benchmark. As part of this analysis, we establish a structural result for strong data processing inequalities (SDPI), proving that SDPI coefficients for Rényi divergences and their symmetric variants (Jeffreys-Rényi divergences) are achieved by binary input distributions. These results on SDPI coefficients are also of independent interest, with applications to statistical estimation, data compression, and Markov chain mixing."}
{"id": "2602.14862", "categories": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.14862", "abs": "https://arxiv.org/abs/2602.14862", "authors": ["Pierre-Alexandre Mattei", "Bruno Loureiro"], "title": "The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling", "comment": null, "summary": "Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model."}
{"id": "2602.15007", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.15007", "abs": "https://arxiv.org/abs/2602.15007", "authors": ["Dirk Douwes-Schultz", "Rob Deardon", "Alexandra M. Schmidt"], "title": "Hidden Markov Individual-level Models of Infectious Disease Transmission", "comment": null, "summary": "Individual-level epidemic models are increasingly being used to help understand the transmission dynamics of various infectious diseases. However, fitting such models to individual-level epidemic data is challenging, as we often only know when an individual's disease status was detected (e.g., when they showed symptoms) and not when they were infected or removed. We propose an autoregressive coupled hidden Markov model to infer unknown infection and removal times, as well as other model parameters, from a single observed detection time for each detected individual. Unlike more traditional data augmentation methods used in epidemic modelling, we do not assume that this detection time corresponds to infection or removal or that infected individuals must at some point be detected. Bayesian coupled hidden Markov models have been used previously for individual-level epidemic data. However, these approaches assumed each individual was continuously tested and that the tests were independent. In practice, individuals are often only tested until their first positive test, and even if they are continuously tested, only the initial detection times may be reported. In addition, multiple tests on the same individual may not be independent. We accommodate these scenarios by assuming that the probability of detecting the disease can depend on past observations, which allows us to fit a much wider range of practical applications. We illustrate the flexibility of our approach by fitting two examples: an experiment on the spread of tomato spot wilt virus in pepper plants and an outbreak of norovirus among nurses in a hospital."}
