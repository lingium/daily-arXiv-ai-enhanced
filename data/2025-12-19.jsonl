{"id": "2512.15436", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15436", "abs": "https://arxiv.org/abs/2512.15436", "authors": ["John D. Foley", "Justin T. Lee"], "title": "Online Partitioned Local Depth for semi-supervised applications", "comment": "19 pages, 2 figures", "summary": "We introduce an extension of the partitioned local depth (PaLD) algorithm that is adapted to online applications such as semi-supervised prediction. The new algorithm we present, online PaLD, is well-suited to situations where it is a possible to pre-compute a cohesion network from a reference dataset. After $O(n^3)$ steps to construct a queryable data structure, online PaLD can extend the cohesion network to a new data point in $O(n^2)$ time. Our approach complements previous speed up approaches based on approximation and parallelism. For illustrations, we present applications to online anomaly detection and semi-supervised classification for health-care datasets."}
{"id": "2512.15606", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15606", "abs": "https://arxiv.org/abs/2512.15606", "authors": ["Carlos Couto", "José Mourão", "Mário A. T. Figueiredo", "Pedro Ribeiro"], "title": "A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point", "comment": "25 pages, 9 figures", "summary": "Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank."}
{"id": "2512.15684", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15684", "abs": "https://arxiv.org/abs/2512.15684", "authors": ["Victor Léger", "Florent Chatelain"], "title": "High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations", "comment": null, "summary": "Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets. Despite decades of practical success, a precise theoretical understanding of its behavior in high-dimensional regimes remains limited. In this paper, we study a data integration model in which two high-dimensional data matrices share a low-rank common latent structure while also containing individual-specific components. We analyze the singular vectors of the associated cross-covariance matrix using tools from random matrix theory and derive asymptotic characterizations of the alignment between estimated and true latent directions. These results provide a quantitative explanation of the reconstruction performance of the PLS variant based on Singular Value Decomposition (PLS-SVD) and identify regimes where the method exhibits counter-intuitive or limiting behavior. Building on this analysis, we compare PLS-SVD with principal component analysis applied separately to each dataset and show its asymptotic superiority in detecting the common latent subspace. Overall, our results offer a comprehensive theoretical understanding of high-dimensional PLS-SVD, clarifying both its advantages and fundamental limitations."}
{"id": "2512.15383", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15383", "abs": "https://arxiv.org/abs/2512.15383", "authors": ["Yunjie Fan", "Matteo Sesia"], "title": "Interpretable Multivariate Conformal Prediction with Fast Transductive Standardization", "comment": "23 pages (48 pages including references and appendices)", "summary": "We propose a conformal prediction method for constructing tight simultaneous prediction intervals for multiple, potentially related, numerical outputs given a single input. This method can be combined with any multi-target regression model and guarantees finite-sample coverage. It is computationally efficient and yields informative prediction intervals even with limited data. The core idea is a novel \\emph{coordinate-wise} standardization procedure that makes residuals across output dimensions directly comparable, estimating suitable scaling parameters using the calibration data themselves. This does not require modeling of cross-output dependence nor auxiliary sample splitting. Implementing this idea requires overcoming technical challenges associated with transductive or full conformal prediction. Experiments on simulated and real data demonstrate this method can produce tighter prediction intervals than existing baselines while maintaining valid simultaneous coverage."}
{"id": "2512.14903", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.14903", "abs": "https://arxiv.org/abs/2512.14903", "authors": ["Matthew Heaney", "Olive Healy", "Jason Wyse", "Arthur White"], "title": "Bayesian Latent Class Regression and Variable Selection with Applications to Sleep Patterns Data", "comment": "24 pages, 4 figures", "summary": "Sleep difficulties in children are heterogeneous in presentation, yet conventional assessment tools like the Children's Sleep Habits Questionnaire (CSHQ) reduce this complexity to a single cumulative score, obscuring distinct patterns of sleep disturbance that require different interventions. Latent Class Regression (LCR) models offer a principled approach to identify subgroups with shared sleep behaviour profiles whilst incorporating predictors of group membership, but Bayesian inference for these models has been hindered by computational challenges and the absence of variable selection methods. We propose a fully Bayesian framework for LCR that uses Pólya-Gamma data augmentation, enabling efficient sampling of regression coefficients. We extend this framework to include variable selection for both predictors and item responses: predictor variable selection via latent inclusion indicators and item selection through a partially collapsed approach. Through simulation studies, we show that the proposed methods yield accurate parameter estimates, resolve identifiability issues arising in full models and successfully identify informative predictors and items while excluding noise variables. Applying this methodology to CSHQ data from 148 children reveals distinct latent subgroups with different sleep behaviour profiles, anxious nighttime sleepers, short/light sleepers and those with more pervasive sleep problems, with each carrying distinct implications for intervention. Results also highlight the predictive role of Autism Spectrum Disorder diagnosis in subgroup membership. These findings demonstrate the limitations of conventional CSHQ scoring and illustrate the benefits of a probabilistic subgroup-based approach as an alternative for understanding paediatric sleep difficulties."}
{"id": "2512.14930", "categories": ["stat.AP", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14930", "abs": "https://arxiv.org/abs/2512.14930", "authors": ["Jaume Anguera Peris", "Songtao Cheng", "Hanzhao Zhang", "Wei Ouyang", "Joakim Jaldén"], "title": "Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies", "comment": null, "summary": "High-content screening microscopy generates large amounts of live-cell imaging data, yet its potential remains constrained by the inability to determine when and where to image most effectively. Optimally balancing acquisition time, computational capacity, and photobleaching budgets across thousands of dynamically evolving regions of interest remains an open challenge, further complicated by limited field-of-view adjustments and sensor sensitivity. Existing approaches either rely on static sampling or heuristics that neglect the dynamic evolution of biological processes, leading to inefficiencies and missed events. Here, we introduce the restless multi-process multi-armed bandit (RMPMAB), a new decision-theoretic framework in which each experimental region is modeled not as a single process but as an ensemble of Markov chains, thereby capturing the inherent heterogeneity of biological systems such as asynchronous cell cycles and heterogeneous drug responses. Building upon this foundation, we derive closed-form expressions for transient and asymptotic behaviors of aggregated processes, and design scalable Whittle index policies with sub-linear complexity in the number of imaging regions. Through both simulations and a real biological live-cell imaging dataset, we show that our approach achieves substantial improvements in throughput under resource constraints. Notably, our algorithm outperforms Thomson Sampling, Bayesian UCB, epsilon-Greedy, and Round Robin by reducing cumulative regret by more than 37% in simulations and capturing 93% more biologically relevant events in live imaging experiments, underscoring its potential for transformative smart microscopy. Beyond improving experimental efficiency, the RMPMAB framework unifies stochastic decision theory with optimal autonomous microscopy control, offering a principled approach to accelerate discovery across multidisciplinary sciences."}
{"id": "2512.14764", "categories": ["stat.ME", "cs.AI", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.14764", "abs": "https://arxiv.org/abs/2512.14764", "authors": ["Alessandro Casadei", "Sreyoshi Bhaduri", "Rohit Malshe", "Pavan Mullapudi", "Raj Ratan", "Ankush Pole", "Arkajit Rakshit"], "title": "Scaling Causal Mediation for Complex Systems: A Framework for Root Cause Analysis", "comment": null, "summary": "Modern operational systems ranging from logistics and cloud infrastructure to industrial IoT, are governed by complex, interdependent processes. Understanding how interventions propagate through such systems requires causal inference methods that go beyond direct effects to quantify mediated pathways. Traditional mediation analysis, while effective in simple settings, fails to scale to the high-dimensional directed acyclic graphs (DAGs) encountered in practice, particularly when multiple treatments and mediators interact. In this paper, we propose a scalable mediation analysis framework tailored for large causal DAGs involving multiple treatments and mediators. Our approach systematically decomposes total effects into interpretable direct and indirect components. We demonstrate its practical utility through applied case studies in fulfillment center logistics, where complex dependencies and non-controllable factors often obscure root causes."}
{"id": "2512.15643", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15643", "abs": "https://arxiv.org/abs/2512.15643", "authors": ["Jairo Fúquene-Patiño"], "title": "Fully Bayesian Spectral Clustering and Benchmarking with Uncertainty Quantification for Small Area Estimation", "comment": null, "summary": "In this work, inspired by machine learning techniques, we propose a new Bayesian model for Small Area Estimation (SAE), the Fay-Herriot model with Spectral Clustering (FH-SC). Unlike traditional approaches, clustering in FH-SC is based on spectral clustering algorithms that utilize external covariates, rather than geographical or administrative criteria. A major advantage of the FH-SC model is its flexibility in integrating existing SAE approaches, with or without clustering random effects. To enable benchmarking, we leverage the theoretical framework of posterior projections for constrained Bayesian inference and derive closed form expressions for the new Rao-Blackwell (RB) estimators of the posterior mean under the FH-SC model. Additionally, we introduce a novel measure of uncertainty for the benchmarked estimator, the Conditional Posterior Mean Square Error (CPMSE), which is generalizable to other Bayesian SAE estimators. We conduct model-based and data-based simulation studies to evaluate the frequentist properties of the CPMSE. The proposed methodology is motivated by a real case study involving the estimation of the proportion of households with internet access in the municipalities of Colombia. Finally, we also illustrate the advantages of FH-SC over existing Bayesian and frequentist approaches through our case study."}
{"id": "2512.15057", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.15057", "abs": "https://arxiv.org/abs/2512.15057", "authors": ["Ehsan Mohammadi", "Fanghua Chen", "Yizhou Cai", "Yun Yang", "Ting Fung Ma", "Lu Zhou"], "title": "Stratified Bootstrap Test Package", "comment": null, "summary": "The Stratified Bootstrap Test (SBT) provides a nonparametric, resampling-based framework for assessing the stability of group-specific ranking patterns in multivariate survey or rating data. By repeatedly resampling observations and examining whether a group's top-ranked items remain among the highest-scoring categories across bootstrap samples, SBT quantifies ranking robustness through a non-containment index. In parallel, the stratified bootstrap test extends this framework to formal statistical inference by testing ordering hypotheses among population means. Through resampling within groups, the method approximates the null distribution of ranking-based test statistics without relying on distributional assumptions. Together, these techniques enable both descriptive and inferential evaluation of ranking consistency, detection of aberrant or adversarial response patterns, and rigorous comparison of groups in applications such as survey analysis, item response assessment, and fairness auditing in AI systems."}
{"id": "2512.15056", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.15056", "abs": "https://arxiv.org/abs/2512.15056", "authors": ["Bingjie Li", "Jiadai Xu", "Yiqing Sun", "Peng Liu", "Zhigang Yao"], "title": "Early CRAB-like Biomarker Signatures Reveal a Preclinical Susceptibility Continuum for Multiple Myeloma", "comment": "13 pages", "summary": "Multiple myeloma (MM) evolves over decades, yet robust tools for identifying individuals at risk long before clinical onset remain limited. Using data from 378,930 UK Biobank participants, we systematically characterized the longitudinal dynamics and predictive value of routinely measured \"CRAB-like\" biomarkers, including hematologic indices, protein metabolism markers, renal function, and serum calcium. Across multivariable models, biomarkers reflecting anemia and protein imbalance (including hemoglobin, red blood cell indices, total protein, albumin, and the albumin/globulin ratio) showed strong and consistent associations with future MM, independent of demographic, lifestyle, clinical, and genetic risk factors. These markers displayed pronounced non-linear dose-response relationships and contributed substantially to 5- and 10-year MM risk discrimination, with the C-index improving from 0.66 to 0.76. Longitudinal analyses revealed progressive shifts in red cell morphology and protein metabolism profiles up to a decade before diagnosis, supporting the existence of a preclinical susceptibility continuum detectable in the general population. Our findings suggest that subtle yet quantifiable deviations in common laboratory tests reflect early microenvironmental changes that precede malignant plasma cell expansion, offering opportunities for risk stratification and targeted surveillance."}
{"id": "2512.14903", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.14903", "abs": "https://arxiv.org/abs/2512.14903", "authors": ["Matthew Heaney", "Olive Healy", "Jason Wyse", "Arthur White"], "title": "Bayesian Latent Class Regression and Variable Selection with Applications to Sleep Patterns Data", "comment": "24 pages, 4 figures", "summary": "Sleep difficulties in children are heterogeneous in presentation, yet conventional assessment tools like the Children's Sleep Habits Questionnaire (CSHQ) reduce this complexity to a single cumulative score, obscuring distinct patterns of sleep disturbance that require different interventions. Latent Class Regression (LCR) models offer a principled approach to identify subgroups with shared sleep behaviour profiles whilst incorporating predictors of group membership, but Bayesian inference for these models has been hindered by computational challenges and the absence of variable selection methods. We propose a fully Bayesian framework for LCR that uses Pólya-Gamma data augmentation, enabling efficient sampling of regression coefficients. We extend this framework to include variable selection for both predictors and item responses: predictor variable selection via latent inclusion indicators and item selection through a partially collapsed approach. Through simulation studies, we show that the proposed methods yield accurate parameter estimates, resolve identifiability issues arising in full models and successfully identify informative predictors and items while excluding noise variables. Applying this methodology to CSHQ data from 148 children reveals distinct latent subgroups with different sleep behaviour profiles, anxious nighttime sleepers, short/light sleepers and those with more pervasive sleep problems, with each carrying distinct implications for intervention. Results also highlight the predictive role of Autism Spectrum Disorder diagnosis in subgroup membership. These findings demonstrate the limitations of conventional CSHQ scoring and illustrate the benefits of a probabilistic subgroup-based approach as an alternative for understanding paediatric sleep difficulties."}
{"id": "2512.15383", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15383", "abs": "https://arxiv.org/abs/2512.15383", "authors": ["Yunjie Fan", "Matteo Sesia"], "title": "Interpretable Multivariate Conformal Prediction with Fast Transductive Standardization", "comment": "23 pages (48 pages including references and appendices)", "summary": "We propose a conformal prediction method for constructing tight simultaneous prediction intervals for multiple, potentially related, numerical outputs given a single input. This method can be combined with any multi-target regression model and guarantees finite-sample coverage. It is computationally efficient and yields informative prediction intervals even with limited data. The core idea is a novel \\emph{coordinate-wise} standardization procedure that makes residuals across output dimensions directly comparable, estimating suitable scaling parameters using the calibration data themselves. This does not require modeling of cross-output dependence nor auxiliary sample splitting. Implementing this idea requires overcoming technical challenges associated with transductive or full conformal prediction. Experiments on simulated and real data demonstrate this method can produce tighter prediction intervals than existing baselines while maintaining valid simultaneous coverage."}
{"id": "2512.15650", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15650", "abs": "https://arxiv.org/abs/2512.15650", "authors": ["Stephen Tivenan", "Indranil Sahoo", "Yanjun Qian"], "title": "A Statistical Framework for Spatial Boundary Estimation and Change Detection: Application to the Sahel Sahara Climate Transition", "comment": "40 pages, 12 figures", "summary": "Spatial boundaries, such as ecological transitions or climatic regime interfaces, capture steep environmental gradients, and shifts in their structure can signal emerging environmental changes. Quantifying uncertainty in spatial boundary locations and formally testing for temporal shifts remains challenging, especially when boundaries are derived from noisy, gridded environmental data. We present a unified framework that combines heteroskedastic Gaussian process (GP) regression with a scaled Maximum Absolute Difference (MAD) Global Envelope Test (GET) to estimate spatial boundary curves and assess whether they evolve over time. The heteroskedastic GP provides a flexible probabilistic reconstruction of boundary lines, capturing spatially varying mean structure and location specific variability, while the test offers a rigorous hypothesis testing tool for detecting departures from expected boundary behaviors. Simulation studies show that the proposed method achieves the correct size under the null and high power for detecting local boundary shifts. Applying our framework to the Sahel Sahara transition zone, using annual Koppen Trewartha climate classifications from 1960 to 1989, we find no statistically significant decade scale changes in the arid and semi arid or semi arid and non arid interfaces. However, the method successfully identifies localized boundary shifts during the extreme drought years of 1983 and 1984, consistent with climate studies documenting regional anomalies in these interfaces during that period."}
{"id": "2512.15232", "categories": ["stat.AP", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15232", "abs": "https://arxiv.org/abs/2512.15232", "authors": ["Guillaume Koechlin", "Filippo Bovera", "Elena Degli Innocenti", "Barbara Santini", "Alessandro Venturi", "Simona Vazio", "Piercesare Secchi"], "title": "A Blind Source Separation Framework to Monitor Sectoral Power Demand from Grid-Scale Load Measurements", "comment": null, "summary": "As we are moving towards decentralized power systems dominated by intermittent electricity generation from renewable energy sources, demand-side flexibility is becoming a critical issue. In this context, it is essential to understand the composition of electricity demand at various scales of the power grid. At the regional or national scale, there is however little visibility on the relative contributions of different consumer categories, due to the complexity and costs of collecting end-users consumption data. To address this issue, we propose a blind source separation framework based on a constrained variant of non-negative matrix factorization to monitor the consumption of residential, services and industrial sectors at high frequency from aggregate high-voltage grid load measurements. Applying the method to Italy's national load curve between 2021 and 2023, we reconstruct accurate hourly consumption profiles for each sector. Results reveal that both households and services daily consumption behaviors are driven by two distinct regimes related to the season and day type whereas industrial demand follows a single, stable daily profile. Besides, the monthly consumption estimates of each sector derived from the disaggregated load are found to closely align with sample-based indices and be more precise than forecasting approaches based on these indices for real-time monitoring."}
{"id": "2512.14959", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14959", "abs": "https://arxiv.org/abs/2512.14959", "authors": ["Martin Bladt", "Kristian Vilhelm Dinesen"], "title": "Conditional Expert Kaplan-Meier Estimation: Asymptotic Theory and an Application to Loan Default Modelling", "comment": null, "summary": "We study the conditional expert Kaplan-Meier estimator, an extension of the classical Kaplan--Meier estimator designed for time-to-event data subject to both right-censoring and contamination. Such contamination, where observed events may not reflect true outcomes, is common in applied settings, including insurance and credit risk, where expert opinion is often used to adjudicate uncertain events. Building on previous work, we develop a comprehensive asymptotic theory for the conditional version incorporating covariates through kernel smoothing. We establish functional consistency and weak convergence under suitable regularity conditions and quantify the bias induced by imperfect expert information. The results show that unbiased expert judgments ensure consistency, while systematic deviations lead to a deterministic asymptotic bias that can be explicitly characterized. We examine finite-sample properties through simulation studies and illustrate the practical use of the estimator with an application to loan default data."}
{"id": "2512.15592", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.15592", "abs": "https://arxiv.org/abs/2512.15592", "authors": ["Tim Kutta", "Martin Schumann", "Holger Dette"], "title": "Inference for Forecasting Accuracy: Pooled versus Individual Estimators in High-dimensional Panel Data", "comment": null, "summary": "Panels with large time $(T)$ and cross-sectional $(N)$ dimensions are a key data structure in social sciences and other fields. A central question in panel data analysis is whether to pool data across individuals or to estimate separate models. Pooled estimators typically have lower variance but may suffer from bias, creating a fundamental trade-off for optimal estimation. We develop a new inference method to compare the forecasting performance of pooled and individual estimators. Specifically, we propose a confidence interval for the difference between their forecasting errors and establish its asymptotic validity. Our theory allows for complex temporal and cross-sectional dependence in the model errors and covers scenarios where $N$ can be much larger than $T$-including the independent case under the classical condition $N/T^2 \\to 0$. The finite-sample properties of the proposed method are examined in an extensive simulation study."}
{"id": "2512.15257", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.15257", "abs": "https://arxiv.org/abs/2512.15257", "authors": ["Bertrand Jouve", "Paul Rochet", "Mohamadou Salifou"], "title": "Cyclists route choice modeling from trip duration data in urban areas", "comment": null, "summary": "The lack of GPS data limits the ability to reconstruct the actual routes taken by cyclists in urban areas. This article introduces an inference method based solely on trip durations and origin-destination pairs from bike-sharing system (BSS) users. Travel time distributions are modeled using log-normal mixture models, allowing us to identify the presence of distinct behaviors. The approach is applied to 3.8 million trips recorded in 2022 in the Toulouse metropolitan area, with observed durations compared against travel times estimated by OpenStreetMap (OSM). Results show that, for many station pairs, trip durations align closely with the fastest route suggested by OSM, reflecting a dominant and routine practice. In other cases, mixture models reveal more heterogeneous behaviors, including longer trips, detours, or intermediate stops. This approach highlights both the stability and diversity of cycling practices, providing a robust tool for usage analysis in data-limited contexts, and offering new insights into urban mobility dynamics without relying on spatially explicit data."}
{"id": "2512.14983", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14983", "abs": "https://arxiv.org/abs/2512.14983", "authors": ["Roberto Vila"], "title": "Gamma family characterization and an alternative proof of Gini estimator unbiasedness", "comment": "7 pages", "summary": "In this paper, we derive a general representation for the expectation of the (upward adjusted) Gini coefficient estimator in terms of the Laplace transform of the underlying distribution. This representation leads to a characterization of the gamma family within the class of nonnegative scale families, based on a stability property of exponentially tilted distributions. As an application, we provide an alternative proof of the unbiasedness of the Gini coefficient estimator under gamma populations."}
{"id": "2512.15507", "categories": ["stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.15507", "abs": "https://arxiv.org/abs/2512.15507", "authors": ["Yanqing Yi", "Su-Fen Yang"], "title": "Change detection with adaptive sampling for binary responses", "comment": null, "summary": "We propose using an adaptive sampling method to detect changes for a system with multiple lines. The adaptive sampling utilizes the information in responses to learn on which line is more likely to have a change thus allocating more units to the line. The learning process is formatted as a Markov decision process by integrating sampling information with likelihood ratio for changes to define rewards and the optimal sampling is approximated by using the Bellman operator iteratively based on the average reward criterion. We demonstrate the performance of the proposed method for binary responses using the exact distribution method for adaptive sampling. Our numeric results show that the adaptive sampling samples more often the line that has a change and the statistical power to detect a change is better than those with the equal randomization for sample sizes of 20 or higher. When sample sizes increase or the difference between out-of-control and in-control probabilities increases, the adaptive sampling allocates higher proportion of units averagely to the line with a change and the statistical power to detect a change increases."}
{"id": "2512.14985", "categories": ["stat.ME", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.14985", "abs": "https://arxiv.org/abs/2512.14985", "authors": ["Jiaqing Lu", "Ziqi Li", "Lei Han", "Qianwen Guo"], "title": "Measuring Nonlinear Relationships and Spatial Heterogeneity of Influencing Factors on Traffic Crash Density Using GeoXAI", "comment": null, "summary": "This study applies a Geospatial Explainable AI (GeoXAI) framework to analyze the spatially heterogeneous and nonlinear determinants of traffic crash density in Florida. By combining a high-performing machine learning model with GeoShapley, the framework provides interpretable, tract-level insights into how roadway characteristics and socioeconomic factors contribute to crash risk. Specifically, results show that variables such as road density, intersection density, neighborhood compactness, and educational attainment exhibit complex nonlinear relationships with crashes. Extremely dense urban areas, such as Miami, show sharply elevated crash risk due to intensified pedestrian activities and roadway complexity. The GeoShapley approach also captures strong spatial heterogeneity in the influence of these factors. Major metropolitan areas including Miami, Orlando, Tampa, and Jacksonville display significantly higher intrinsic crash contributions, while rural tracts generally have lower baseline risk. Each factor exhibits pronounced spatial variation across the state. Based on these findings, the study proposes targeted, geography-sensitive policy recommendations, including traffic calming in compact neighborhoods, adaptive intersection design, speed management on high-volume corridors such as I-95 in Miami, and equity-focused safety interventions in disadvantaged rural areas of central and northern Florida. Moreover, this paper compares the results obtained from GeoShapley framework against other established methods (e.g., SHAP and MGWR), demonstrating its powerful ability to explain nonlinearity and spatial heterogeneity simultaneously."}
{"id": "2512.15650", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15650", "abs": "https://arxiv.org/abs/2512.15650", "authors": ["Stephen Tivenan", "Indranil Sahoo", "Yanjun Qian"], "title": "A Statistical Framework for Spatial Boundary Estimation and Change Detection: Application to the Sahel Sahara Climate Transition", "comment": "40 pages, 12 figures", "summary": "Spatial boundaries, such as ecological transitions or climatic regime interfaces, capture steep environmental gradients, and shifts in their structure can signal emerging environmental changes. Quantifying uncertainty in spatial boundary locations and formally testing for temporal shifts remains challenging, especially when boundaries are derived from noisy, gridded environmental data. We present a unified framework that combines heteroskedastic Gaussian process (GP) regression with a scaled Maximum Absolute Difference (MAD) Global Envelope Test (GET) to estimate spatial boundary curves and assess whether they evolve over time. The heteroskedastic GP provides a flexible probabilistic reconstruction of boundary lines, capturing spatially varying mean structure and location specific variability, while the test offers a rigorous hypothesis testing tool for detecting departures from expected boundary behaviors. Simulation studies show that the proposed method achieves the correct size under the null and high power for detecting local boundary shifts. Applying our framework to the Sahel Sahara transition zone, using annual Koppen Trewartha climate classifications from 1960 to 1989, we find no statistically significant decade scale changes in the arid and semi arid or semi arid and non arid interfaces. However, the method successfully identifies localized boundary shifts during the extreme drought years of 1983 and 1984, consistent with climate studies documenting regional anomalies in these interfaces during that period."}
{"id": "2512.15057", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.15057", "abs": "https://arxiv.org/abs/2512.15057", "authors": ["Ehsan Mohammadi", "Fanghua Chen", "Yizhou Cai", "Yun Yang", "Ting Fung Ma", "Lu Zhou"], "title": "Stratified Bootstrap Test Package", "comment": null, "summary": "The Stratified Bootstrap Test (SBT) provides a nonparametric, resampling-based framework for assessing the stability of group-specific ranking patterns in multivariate survey or rating data. By repeatedly resampling observations and examining whether a group's top-ranked items remain among the highest-scoring categories across bootstrap samples, SBT quantifies ranking robustness through a non-containment index. In parallel, the stratified bootstrap test extends this framework to formal statistical inference by testing ordering hypotheses among population means. Through resampling within groups, the method approximates the null distribution of ranking-based test statistics without relying on distributional assumptions. Together, these techniques enable both descriptive and inferential evaluation of ranking consistency, detection of aberrant or adversarial response patterns, and rigorous comparison of groups in applications such as survey analysis, item response assessment, and fairness auditing in AI systems."}
{"id": "2512.14903", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.14903", "abs": "https://arxiv.org/abs/2512.14903", "authors": ["Matthew Heaney", "Olive Healy", "Jason Wyse", "Arthur White"], "title": "Bayesian Latent Class Regression and Variable Selection with Applications to Sleep Patterns Data", "comment": "24 pages, 4 figures", "summary": "Sleep difficulties in children are heterogeneous in presentation, yet conventional assessment tools like the Children's Sleep Habits Questionnaire (CSHQ) reduce this complexity to a single cumulative score, obscuring distinct patterns of sleep disturbance that require different interventions. Latent Class Regression (LCR) models offer a principled approach to identify subgroups with shared sleep behaviour profiles whilst incorporating predictors of group membership, but Bayesian inference for these models has been hindered by computational challenges and the absence of variable selection methods. We propose a fully Bayesian framework for LCR that uses Pólya-Gamma data augmentation, enabling efficient sampling of regression coefficients. We extend this framework to include variable selection for both predictors and item responses: predictor variable selection via latent inclusion indicators and item selection through a partially collapsed approach. Through simulation studies, we show that the proposed methods yield accurate parameter estimates, resolve identifiability issues arising in full models and successfully identify informative predictors and items while excluding noise variables. Applying this methodology to CSHQ data from 148 children reveals distinct latent subgroups with different sleep behaviour profiles, anxious nighttime sleepers, short/light sleepers and those with more pervasive sleep problems, with each carrying distinct implications for intervention. Results also highlight the predictive role of Autism Spectrum Disorder diagnosis in subgroup membership. These findings demonstrate the limitations of conventional CSHQ scoring and illustrate the benefits of a probabilistic subgroup-based approach as an alternative for understanding paediatric sleep difficulties."}
{"id": "2512.15128", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.15128", "abs": "https://arxiv.org/abs/2512.15128", "authors": ["Kaoru Irie", "Tevfik Aktekin"], "title": "On non-stationarity of the Poisson gamma state space models", "comment": "10 pages, 1 figure", "summary": "The Poisson-gamma state space (PGSS) models have been utilized in the analysis of non-negative integer-valued time series to sequentially obtain closed form filtering and predictive densities. In this study, we show the underlying mechanics and non-stationary properties of multi-step ahead predictive distributions for the PGSS family of models. By exploiting the non-stationary structure of the PGSS model, we establish that the predictive mean remains constant while the predictive variance diverges with the forecast horizon, a property also found in Gaussian random walk models. We show that, in the long run, the predictive distribution converges to a zero-degenerated distribution, such that both point and interval forecasts eventually converge towards zero. In doing so, we comment on the effect of hyper-parameters and the discount factor on the long-run behavior of the forecasts."}
{"id": "2512.15676", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.15676", "abs": "https://arxiv.org/abs/2512.15676", "authors": ["Manuel M. Müller", "Björn Bornkamp", "Frank Bretz", "Timothy I. Cannings", "Wei Liu", "Henry W. J. Reeve", "Richard J. Samworth", "Nikolaos Sfikas", "Fang Wan", "Konstantinos Sechidis"], "title": "Data-driven controlled subgroup selection in clinical trials", "comment": "37 pages, 10 figures", "summary": "Subgroup selection in clinical trials is essential for identifying patient groups that react differently to a treatment, thereby enabling personalised medicine. In particular, subgroup selection can identify patient groups that respond particularly well to a treatment or that encounter adverse events more often. However, this is a post-selection inference problem, which may pose challenges for traditional techniques used for subgroup analysis, such as increased Type I error rates and potential biases from data-driven subgroup identification. In this paper, we present two methods for subgroup selection in regression problems: one based on generalised linear modelling and another on isotonic regression. We demonstrate how these methods can be used for data-driven subgroup identification in the analysis of clinical trials, focusing on two distinct tasks: identifying patient groups that are safe from manifesting adverse events and identifying patient groups with high treatment effect, while controlling for Type I error in both cases. A thorough simulation study is conducted to evaluate the strengths and weaknesses of each method, providing detailed insight into the sensitivity of the Type I error rate control to modelling assumptions."}
{"id": "2512.15244", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.15244", "abs": "https://arxiv.org/abs/2512.15244", "authors": ["Aditya Ghosh", "Stefan Wager"], "title": "Non-parametric Causal Inference in Dynamic Thresholding Designs", "comment": null, "summary": "Consider a setting where we regularly monitor patients' fasting blood sugar, and declare them to have prediabetes (and encourage preventative care) if this number crosses a pre-specified threshold. The sharp, threshold-based treatment policy suggests that we should be able to estimate the long-term benefit of this preventative care by comparing the health trajectories of patients with blood sugar measurements right above and below the threshold. A naive regression-discontinuity analysis, however, is not applicable here, as it ignores the temporal dynamics of the problem where, e.g., a patient just below the threshold on one visit may become prediabetic (and receive treatment) following their next visit. Here, we study thresholding designs in general dynamic systems, and show that simple reduced-form characterizations remain available for a relevant causal target, namely a dynamic marginal policy effect at the treatment threshold. We develop a local-linear-regression approach for estimation and inference of this estimand, and demonstrate promise of our approach in numerical experiments."}
{"id": "2512.15383", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15383", "abs": "https://arxiv.org/abs/2512.15383", "authors": ["Yunjie Fan", "Matteo Sesia"], "title": "Interpretable Multivariate Conformal Prediction with Fast Transductive Standardization", "comment": "23 pages (48 pages including references and appendices)", "summary": "We propose a conformal prediction method for constructing tight simultaneous prediction intervals for multiple, potentially related, numerical outputs given a single input. This method can be combined with any multi-target regression model and guarantees finite-sample coverage. It is computationally efficient and yields informative prediction intervals even with limited data. The core idea is a novel \\emph{coordinate-wise} standardization procedure that makes residuals across output dimensions directly comparable, estimating suitable scaling parameters using the calibration data themselves. This does not require modeling of cross-output dependence nor auxiliary sample splitting. Implementing this idea requires overcoming technical challenges associated with transductive or full conformal prediction. Experiments on simulated and real data demonstrate this method can produce tighter prediction intervals than existing baselines while maintaining valid simultaneous coverage."}
{"id": "2512.15429", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.15429", "abs": "https://arxiv.org/abs/2512.15429", "authors": ["Emma S. Simpson", "Paul J. Northrop"], "title": "Accounting for missing data when modelling block maxima", "comment": null, "summary": "Modelling block maxima using the generalised extreme value (GEV) distribution is a classical and widely used method for studying univariate extremes. It allows for theoretically motivated estimation of return levels, including extrapolation beyond the range of observed data. A frequently overlooked challenge in applying this methodology comes from handling datasets containing missing values. In this case, one cannot be sure whether the true maximum has been recorded in each block, and simply ignoring the issue can lead to biased parameter estimators and, crucially, underestimated return levels. We propose an extension of the standard block maxima approach to overcome such missing data issues. This is achieved by explicitly accounting for the proportion of missing values in each block within the GEV model. Inference is carried out using likelihood-based techniques, and we propose an update to commonly used diagnostic plots to assess model fit. We assess the performance of our method via a simulation study, with results that are competitive with the \"ideal\" case of having no missing values. The practical use of our methodology is demonstrated on sea surge data from Brest, France, and air pollution data from Plymouth, U.K."}
{"id": "2512.15570", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.15570", "abs": "https://arxiv.org/abs/2512.15570", "authors": ["Ioana Gavra", "Ketsia Guichard-Sustowski", "Loïc Le Marrec"], "title": "Optimal Transport-Based Clustering of Attributed Graphs with an Application to Road Traffic Data", "comment": "35 pages, 14 Figures and 3 Tables", "summary": "In many real-world contexts, such as social or transport networks, data exhibit both structural connectivity and node-level attributes. For example, roads in a transport network can be characterized not only by their connectivity but also by traffic flow or speed profiles. Understanding such systems therefore requires jointly analyzing the network structure and node attributes, a challenge addressed by attributed graph partitioning, which clusters nodes based on both connectivity and attributes. In this work, we adapt distance-based methods for this task, including Fréchet $k$-means and optimal transport-based approaches based on Gromov--Wasserstein (GW) discrepancy. We investigate how GW methods, traditionally used for general-purpose tasks such as graph matching, can be specifically adapted for node partitioning, an area that has been relatively underexplored. In the context of node-attributed graphs, we introduce an adaptation of the Fused GW method, offering theoretical guarantees and the ability to handle heterogeneous attribute types. Additionally, we propose to incorporate distance-based embeddings to enhance performance. The proposed approaches are systematically evaluated using a dedicated simulation framework and illustrated on a real-world transportation dataset. Experiments investigate the influence of target choice, assess robustness to noise, and provide practical guidance for attributed graph clustering. In the context of road networks, our results demonstrate that these methods can effectively leverage both structural and attribute information to reveal meaningful clusters, offering insights for improved network understanding."}
{"id": "2512.15592", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.15592", "abs": "https://arxiv.org/abs/2512.15592", "authors": ["Tim Kutta", "Martin Schumann", "Holger Dette"], "title": "Inference for Forecasting Accuracy: Pooled versus Individual Estimators in High-dimensional Panel Data", "comment": null, "summary": "Panels with large time $(T)$ and cross-sectional $(N)$ dimensions are a key data structure in social sciences and other fields. A central question in panel data analysis is whether to pool data across individuals or to estimate separate models. Pooled estimators typically have lower variance but may suffer from bias, creating a fundamental trade-off for optimal estimation. We develop a new inference method to compare the forecasting performance of pooled and individual estimators. Specifically, we propose a confidence interval for the difference between their forecasting errors and establish its asymptotic validity. Our theory allows for complex temporal and cross-sectional dependence in the model errors and covers scenarios where $N$ can be much larger than $T$-including the independent case under the classical condition $N/T^2 \\to 0$. The finite-sample properties of the proposed method are examined in an extensive simulation study."}
{"id": "2512.15643", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15643", "abs": "https://arxiv.org/abs/2512.15643", "authors": ["Jairo Fúquene-Patiño"], "title": "Fully Bayesian Spectral Clustering and Benchmarking with Uncertainty Quantification for Small Area Estimation", "comment": null, "summary": "In this work, inspired by machine learning techniques, we propose a new Bayesian model for Small Area Estimation (SAE), the Fay-Herriot model with Spectral Clustering (FH-SC). Unlike traditional approaches, clustering in FH-SC is based on spectral clustering algorithms that utilize external covariates, rather than geographical or administrative criteria. A major advantage of the FH-SC model is its flexibility in integrating existing SAE approaches, with or without clustering random effects. To enable benchmarking, we leverage the theoretical framework of posterior projections for constrained Bayesian inference and derive closed form expressions for the new Rao-Blackwell (RB) estimators of the posterior mean under the FH-SC model. Additionally, we introduce a novel measure of uncertainty for the benchmarked estimator, the Conditional Posterior Mean Square Error (CPMSE), which is generalizable to other Bayesian SAE estimators. We conduct model-based and data-based simulation studies to evaluate the frequentist properties of the CPMSE. The proposed methodology is motivated by a real case study involving the estimation of the proportion of households with internet access in the municipalities of Colombia. Finally, we also illustrate the advantages of FH-SC over existing Bayesian and frequentist approaches through our case study."}
{"id": "2512.15676", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.15676", "abs": "https://arxiv.org/abs/2512.15676", "authors": ["Manuel M. Müller", "Björn Bornkamp", "Frank Bretz", "Timothy I. Cannings", "Wei Liu", "Henry W. J. Reeve", "Richard J. Samworth", "Nikolaos Sfikas", "Fang Wan", "Konstantinos Sechidis"], "title": "Data-driven controlled subgroup selection in clinical trials", "comment": "37 pages, 10 figures", "summary": "Subgroup selection in clinical trials is essential for identifying patient groups that react differently to a treatment, thereby enabling personalised medicine. In particular, subgroup selection can identify patient groups that respond particularly well to a treatment or that encounter adverse events more often. However, this is a post-selection inference problem, which may pose challenges for traditional techniques used for subgroup analysis, such as increased Type I error rates and potential biases from data-driven subgroup identification. In this paper, we present two methods for subgroup selection in regression problems: one based on generalised linear modelling and another on isotonic regression. We demonstrate how these methods can be used for data-driven subgroup identification in the analysis of clinical trials, focusing on two distinct tasks: identifying patient groups that are safe from manifesting adverse events and identifying patient groups with high treatment effect, while controlling for Type I error in both cases. A thorough simulation study is conducted to evaluate the strengths and weaknesses of each method, providing detailed insight into the sensitivity of the Type I error rate control to modelling assumptions."}
