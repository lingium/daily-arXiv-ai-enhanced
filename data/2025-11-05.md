<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 17]
- [stat.ML](#stat.ML) [Total: 13]
- [stat.AP](#stat.AP) [Total: 6]
- [stat.OT](#stat.OT) [Total: 2]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Polynomial Mixing Times of Simulated Tempering for Mixture Targets by Conductance Decomposition](https://arxiv.org/abs/2511.00708)
*Quan Zhou*

Main category: stat.CO

TL;DR: 该论文建立了模拟回火与MALA算法结合在采样对数凹混合分布时的首个多项式时间保证，证明基于s-电导的状态分解定理，并获得了改进的随机游走Metropolis复杂度估计。


<details>
  <summary>Details</summary>
Motivation: 研究模拟回火算法在采样对数凹混合分布时的理论复杂性，特别是针对不同位置偏移的混合分布，填补该领域多项式时间保证的理论空白。

Method: 使用模拟回火与Metropolis-adjusted Langevin算法(MALA)结合，基于s-电导的状态分解定理构建辅助马尔可夫链，并分析逆温度阶梯的最优设置。

Result: 建立了首个关于维度d、最大模式位移D和对数精度log ε⁻¹的多项式时间保证，证明了逆温度阶梯设置β₁=O(D⁻²)和βᵢ₊₁/βᵢ=1+O(d⁻¹/²)的渐近最优性。

Conclusion: 模拟回火与MALA结合在对数凹混合分布采样中具有多项式时间复杂性，所提出的逆温度阶梯设置达到渐近最优，为相关算法提供了理论保证。

Abstract: We study the theoretical complexity of simulated tempering for sampling from
mixtures of log-concave components differing only by location shifts. The main
result establishes the first polynomial-time guarantee for simulated tempering
combined with the Metropolis-adjusted Langevin algorithm (MALA) with respect to
the problem dimension $d$, maximum mode displacement $D$, and logarithmic
accuracy $\log \epsilon^{-1}$. The proof builds on a general state
decomposition theorem for $s$-conductance, applied to an auxiliary Markov chain
constructed on an augmented space. We also obtain an improved complexity
estimate for simulated tempering combined with random-walk Metropolis. Our
bounds assume an inverse-temperature ladder with smallest value $\beta_1 =
O(D^{-2})$ and spacing $\beta_{i+1}/\beta_i = 1 + O( d^{-1/2} )$, both of which
are shown to be asymptotically optimal up to logarithmic factors.

</details>


### [2] [Particle Filter Made Simple: A Step-by-Step Beginner-friendly Guide](https://arxiv.org/abs/2511.01281)
*Sahil Rajesh Dhayalkar*

Main category: stat.CO

TL;DR: 本文介绍了粒子滤波器的核心概念，它通过随机样本表示不确定性，使用观测更新信念，在非线性或非高斯系统中保持鲁棒性。从卡尔曼滤波的局限性出发，展示了粒子滤波器如何通过预测、测量和重采样来演化加权的假设云。


<details>
  <summary>Details</summary>
Motivation: 粒子滤波器用于在不确定性、噪声和非线性主导的动态系统中估计隐藏状态。当线性或高斯假设失效时，粒子滤波器提供了一种鲁棒的替代方案。

Method: 使用随机样本（粒子）来表示概率分布，通过预测、测量更新和重采样步骤来演化信念。将贝叶斯推理以采样形式展开，用有限粒子集近似概率分布。

Result: 通过图示示例、数值演练和Python代码，使每个概念生动易懂，弥合了理论与实现之间的差距。读者能够理解粒子滤波器的算法流程。

Conclusion: 粒子滤波器通过结合随机性和结构，使系统能够实时推断、适应和理解噪声观测，为非线性非高斯状态估计问题提供了有效的解决方案。

Abstract: The particle filter is a powerful framework for estimating hidden states in
dynamic systems where uncertainty, noise, and nonlinearity dominate. This
mini-book offers a clear and structured introduction to the core ideas behind
particle filters-how they represent uncertainty through random samples, update
beliefs using observations, and maintain robustness where linear or Gaussian
assumptions fail. Starting from the limitations of the Kalman filter, the book
develops the intuition that drives the particle filter: belief as a cloud of
weighted hypotheses that evolve through prediction, measurement, and
resampling. Step by step, it connects these ideas to their mathematical
foundations, showing how probability distributions can be approximated by a
finite set of particles and how Bayesian reasoning unfolds in sampled form.
Illustrated examples, numerical walk-throughs, and Python code bring each
concept to life, bridging the gap between theory and implementation. By the
end, readers will not only understand the algorithmic flow of the particle
filter but also develop an intuitive grasp of how randomness and structure
together enable systems to infer, adapt, and make sense of noisy observations
in real time.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [Stochastic Derivative Estimation for Discontinuous Sample Performances: A Leibniz Integration Perspective](https://arxiv.org/abs/2511.00006)
*Xingyu Ren,Michael C. Fu,Pierre L'Ecuyer*

Main category: stat.ME

TL;DR: 提出了一种基于多维莱布尼茨积分法则的随机导数估计框架，用于处理在参数上不连续的样本性能函数。通过将不连续性嵌入样本空间，得到连续的性能函数，应用莱布尼茨法则产生单次运行的无偏导数估计器。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理在参数上不连续的样本性能函数的导数估计问题，需要开发新的框架来有效估计这类函数的导数。

Method: 对于由指示函数引起的不连续性，将指示函数嵌入样本空间；对于一般不连续函数，通过变量替换将参数依赖性转移到样本空间和基础概率测度中，然后应用莱布尼茨积分法则。

Result: 数值实验证明了该方法的有效性和鲁棒性，能够产生单次运行、易于实现的估计器。

Conclusion: 提出的莱布尼茨积分框架推广了广义似然比方法，为不连续函数的导数估计提供了直观的理论基础，并确定了表面积分消失的条件。

Abstract: We develop a novel stochastic derivative estimation framework for sample
performance functions that are discontinuous in the parameter of interest,
based on the multidimensional Leibniz integral rule. When discontinuities arise
from indicator functions, we embed the indicator functions into the sample
space, yielding a continuous performance function over a parameter-dependent
domain. Applying the Leibniz integral rule in this case produces a single-run,
unbiased derivative estimator. For general discontinuous functions, we apply a
change of variables to shift parameter dependence into the sample space and the
underlying probability measure. Applying the Leibniz integral rule leads to two
terms: a standard likelihood ratio (LR) term from differentiating the
underlying probability measure and a surface integral from differentiating the
boundary of the domain. Evaluating the surface integral may require simulating
multiple sample paths. Our proposed Leibniz integration framework generalizes
the generalized LR (GLR) method and provides intuition as to when the surface
integral vanishes, thereby enabling single-run, easily implementable
estimators. Numerical experiments demonstrate the effectiveness and robustness
of our methods.

</details>


### [4] [Is Representational Similarity Analysis Reliable? A Comparison with Regression](https://arxiv.org/abs/2511.00395)
*Chuanji Gao,Gang Chen,Svetlana V. Shinkareva,Rutvik H. Desai*

Main category: stat.ME

TL;DR: RSA在模型选择中的准确性和可靠性不如回归方法，尽管RSA在处理高维、跨模态和跨物种数据方面具有灵活性，但其将原始数据转换为相似性结构的过程可能导致关键刺激-响应信息的丢失。


<details>
  <summary>Details</summary>
Motivation: 评估RSA在模型选择中的准确性和可靠性，并与回归方法进行比较，探讨RSA在神经影像和行为数据分析中的实际效果。

Method: 通过广泛的模拟研究和实证分析，比较RSA和回归方法在不同样本大小、噪声水平、特征维度和多重共线性条件下的模型选择准确性。

Result: RSA在所有条件下都导致较低的模型选择准确性，即使使用主成分分析和特征重加权来缓解多重共线性问题，回归方法仍然更优。实证数据和fMRI模拟进一步支持这些结论。

Conclusion: 当有直接的刺激-响应映射可用时，研究人员应谨慎选择方法：对于模型选择和拟合，RSA不如线性回归有效。

Abstract: Representational Similarity Analysis (RSA) is a popular method for analyzing
neuroimaging and behavioral data. Here we evaluate the accuracy and reliability
of RSA in the context of model selection, and compare it to that of regression.
Although RSA offers flexibility in handling high-dimensional, cross-modal, and
cross-species data, its reliance on a transformation of raw data into
similarity structures may result in the loss of critical stimulus-response
information. Across extensive simulation studies and empirical analyses, we
show that RSA leads to lower model selection accuracy, regardless of sample
size, noise level, feature dimensionality, or multicollinearity, relative to
regression. While principal component analysis and feature reweighting mitigate
RSA's deficits driven by multicollinearity, regression remains superior in
accurately distinguishing between models. Empirical data and a follow-up fMRI
simulation further support these conclusions. Our findings suggest that
researchers should carefully consider which approach to use: RSA is less
effective than linear regression for model selection and fitting when direct
stimulus-response mappings are available.

</details>


### [5] [Latent Modularity in Multi-View Data](https://arxiv.org/abs/2511.00455)
*Andrea Cremaschi,Maria De Iorio,Garritt Page,Ajay Jasra*

Main category: stat.ME

TL;DR: 提出了一种用于多视图数据聚类的贝叶斯模型，引入"潜在模块性"概念，允许个体在不同视图中属于不同的聚类，同时保持一个基准聚类结构。


<details>
  <summary>Details</summary>
Motivation: 解决多视图数据聚类问题，即来自异构数据源的信息需要同时考虑，传统方法通常假设所有视图共享相同的聚类结构，而实际中不同视图可能呈现不同的聚类模式。

Method: 采用贝叶斯模型，在先验结构中假设每个个体属于一个基准聚类，并允许在每个视图中个体可能属于与基准不同的聚类。使用马尔可夫链蒙特卡洛算法进行推理。

Result: 推导了视图特定聚类标签和关联分区的边际先验表达式，通过模拟研究和详细案例研究验证了模型的有效性。

Conclusion: 提出的潜在模块性模型能够有效处理多视图数据中的异质聚类结构，为复杂数据集成分析提供了灵活框架。

Abstract: In this article, we consider the problem of clustering multi-view data, that
is, information associated to individuals that form heterogeneous data sources
(the views). We adopt a Bayesian model and in the prior structure we assume
that each individual belongs to a baseline cluster and conditionally allow each
individual in each view to potentially belong to different clusters than the
baseline. We call such a structure ''latent modularity''. Then for each
cluster, in each view we have a specific statistical model with an associated
prior. We derive expressions for the marginal priors on the view-specific
cluster labels and the associated partitions, giving several insights into our
chosen prior structure. Using simple Markov chain Monte Carlo algorithms, we
consider our model in a simulation study, along with a more detailed case study
that requires several modeling innovations.

</details>


### [6] [Modeling continuous monitoring glucose curves by Beta generalized non-parametric models](https://arxiv.org/abs/2511.00501)
*Nihan Acar-Denizli,Pedro Delicado*

Main category: stat.ME

TL;DR: 提出了一种基于Beta分布和局部线性最大似然平滑的功能数据分析方法，用于处理重复测量的连续血糖监测数据，该方法在样本量较少时仍能有效工作。


<details>
  <summary>Details</summary>
Motivation: 传统功能数据分析方法在处理时间依赖性连续血糖监测数据时需要大量观测值，且不适用于个体数量较少的情况，需要开发更高效的方法。

Method: 将血糖浓度曲线缩放到[0,1]区间，使用具有两个时变参数的Beta分布建模，开发了适用于多参数时变情况的局部线性最大似然平滑程序。

Result: 该方法在合成数据集和真实临床试验数据集上表现出良好的计算性能和模型拟合效果，与现有方法相比具有竞争力。

Conclusion: 该研究扩展了局部似然估计方法，使其能够处理两个时变参数，证明了基于核的方法可以与当前主流的功能回归模型相媲美。

Abstract: We present a functional data analysis approach for studying time-dependent,
continuous glucose monitoring data with repeated measures for each individual
in an experiment. After scaling the glucose concentration curves to the
interval [0, 1], we model them by using a Beta distribution with two
time-varying parameters. In this context, we develop a local linear maximum
likelihood smoothing procedure that is valid when more than one parameter
depends on time. Our approach requires much fewer observations than previous
functional methods for this setting and is also applicable when only one
individual (or a few) is available. We evaluate the performance of our
estimator in terms of computation time and model fit using a synthetic dataset
as well as a large, real clinical trial dataset. We also compare our method
with existing methods in the literature. From a methodological point of view,
we contribute to extend local likelihood estimation from one to two
time-varying parameters by developing theoretical expressions for estimation
and for approximating the leave-one-out cross-validation. Moreover, we show
that this kernel-based approach competes with spline-based estimation methods,
the dominant line of functional regression models today.

</details>


### [7] [Correcting the Coverage Bias of Quantile Regression](https://arxiv.org/abs/2511.00820)
*Isaac Gibbs,John J. Cherian,Emmanuel J. Candès*

Main category: stat.ME

TL;DR: 开发了调整分位数回归预测以确保覆盖率的方法，这些方法模型无关，能校正高维过拟合偏差，理论证明估计量具有一致性并在比例渐近框架下实现准确校准。


<details>
  <summary>Details</summary>
Motivation: 解决分位数回归预测的覆盖率问题，特别是在高维数据中存在的过拟合偏差，需要模型无关且计算高效的方法。

Method: 通过建立留一法覆盖率与分位数回归对偶问题中变量拟合值的新联系，利用交叉验证在各种设置下以显著降低的计算成本进行校准。

Result: 理论分析表明估计量具有一致性，在比例渐近框架下能实现准确校准，模拟和真实数据实验进一步验证了方法的有效性。

Conclusion: 提出了一种计算高效且理论可靠的方法来确保分位数回归预测的覆盖率，为高维数据分析提供了实用的校准工具。

Abstract: We develop a collection of methods for adjusting the predictions of quantile
regression to ensure coverage. Our methods are model agnostic and can be used
to correct for high-dimensional overfitting bias with only minimal assumptions.
Theoretical results show that the estimates we develop are consistent and
facilitate accurate calibration in the proportional asymptotic regime where the
ratio of the dimension of the data and the sample size converges to a constant.
This is further confirmed by experiments on both simulated and real data. A key
component of our work is a new connection between the leave-one-out coverage
and the fitted values of variables appearing in a dual formulation of the
quantile regression problem. This facilitates the use of cross-validation in a
variety of settings at significantly reduced computational costs.

</details>


### [8] [Robust Bayesian Inference of Causal Effects via Randomization Distributions](https://arxiv.org/abs/2511.00676)
*Easton Huch,Fred Feinberg,Walter Dempsey*

Main category: stat.ME

TL;DR: 提出了一个基于随机化设计的贝叶斯因果推断框架，该框架通过固定观测到的潜在结果并基于统计量的随机化分布构建似然函数，提供可证明的稳健推断。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯超总体方法需要指定边际结果分布，假设较强。本文旨在开发一个基于设计的贝叶斯方法，减少对边际分布假设的依赖，提供更稳健的因果效应推断。

Method: 固定观测到的潜在结果，基于治疗分配的随机化分布构建似然函数。需要指定治疗效果模型，但通常不需要指定边际结果分布。支持后验模型检验和后验平均随机化检验。

Result: 证明了方法的理论性质，包括Bernstein-von Mises定理和后验期望的大样本性质。后验均值渐近等价于Hodges-Lehmann估计量，与经典因果推断估计量（如逆概率加权估计量）建立联系。在营养实验案例中发现了强烈的效应异质性证据。

Conclusion: 该框架提供了基于设计的稳健贝叶斯因果推断方法，假设较弱，理论性质良好，并能扩展到协变量使用、敏感性分析、分配机制估计和非二元治疗等场景。

Abstract: We present a general framework for Bayesian inference of causal effects that
delivers provably robust inferences founded on design-based randomization of
treatments. The framework involves fixing the observed potential outcomes and
forming a likelihood based on the randomization distribution of a statistic.
The method requires specification of a treatment effect model; in many cases,
however, it does not require specification of marginal outcome distributions,
resulting in weaker assumptions compared to Bayesian superpopulation-based
methods. We show that the framework is compatible with posterior model checking
in the form of posterior-averaged randomization tests. We prove several
theoretical properties for the method, including a Bernstein-von Mises theorem
and large-sample properties of posterior expectations. In particular, we show
that the posterior mean is asymptotically equivalent to Hodges-Lehmann
estimators, which provides a bridge to many classical estimators in causal
inference, including inverse-probability-weighted estimators and H\'ajek
estimators. We evaluate the theory and utility of the framework in simulation
and a case study involving a nutrition experiment. In the latter, our framework
uncovers strong evidence of effect heterogeneity despite a lack of evidence for
moderation effects. The basic framework allows numerous extensions, including
the use of covariates, sensitivity analysis, estimation of assignment
mechanisms, and generalization to nonbinary treatments.

</details>


### [9] [A structural equation formulation for general quasi-periodic Gaussian processes](https://arxiv.org/abs/2511.01151)
*Unnati Nigam,Radhendushka Srivastava,Faezeh Marzbanrad,Michael Burke*

Main category: stat.ME

TL;DR: 提出了一种新的准周期高斯过程结构方程模型，简化了生成和预测过程，并提供了收敛的迭代估计算法。该方法显著降低了似然评估和预测的计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了处理自然和生理信号中的准周期模式，需要一种能够简化生成、预测并提供超参数估计的高斯过程模型。

Method: 基于结构方程构建准周期高斯过程，开发了收敛且一致的迭代估计算法，并提供了标准误差估计和置信区间的bootstrap方法。

Result: 在潮汐水位分析、CO₂排放数据和太阳黑子数据等多个问题上展示了计算和扩展性优势，将计算成本从O(k²p²)降低到O(p²)。

Conclusion: 该方法通过结构方程显著提高了高斯过程的可扩展性，为处理准周期信号提供了高效的计算框架。

Abstract: This paper introduces a structural equation formulation that gives rise to a
new family of quasi-periodic Gaussian processes, useful to process a broad
class of natural and physiological signals. The proposed formulation simplifies
generation and forecasting, and provides hyperparameter estimates, which we
exploit in a convergent and consistent iterative estimation algorithm. A
bootstrap approach for standard error estimation and confidence intervals is
also provided. We demonstrate the computational and scaling benefits of the
proposed approach on a broad class of problems, including water level tidal
analysis, CO$_{2}$ emission data, and sunspot numbers data. By leveraging the
structural equations, our method reduces the cost of likelihood evaluations and
predictions from $\mathcal{O}(k^2 p^2)$ to $\mathcal{O}(p^2)$, significantly
improving scalability.

</details>


### [10] [A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems](https://arxiv.org/abs/2511.00870)
*Maxime Bouton,Pierre-Antoine Thouvenin,Audrey Repetti,Pierre Chainais*

Main category: stat.ME

TL;DR: 提出了一种基于近似数据增强和PnP-ULA的分布式采样器，用于解决大规模成像逆问题，通过轻量级去噪卷积神经网络在SPMD架构上高效利用多个GPU。


<details>
  <summary>Details</summary>
Motivation: 解决高维成像逆问题中传统MCMC算法采样和存储高维样本的计算挑战，特别是在高分辨率图像场景下。

Method: 使用近似数据增强和PnP-ULA方法，结合轻量级去噪卷积神经网络，在单程序多数据架构上实现分布式采样。

Result: 该方法具有良好的可扩展性和不确定性量化能力，重建性能与其他PnP方法相当，同时显著降低了通信和计算开销。

Conclusion: 提出的分布式方法结合了可扩展性、不确定性量化和良好重建性能三个重要特性，为大规模成像逆问题提供了有效解决方案。

Abstract: Markov Chain Monte Carlo (MCMC) algorithms are standard approaches to solve
imaging inverse problems and quantify estimation uncertainties, a key
requirement in absence of ground-truth data. To improve estimation quality,
Plug-and-Play MCMC algorithms, such as PnP-ULA, have been recently developed to
accommodate priors encoded by a denoising neural network. Designing scalable
samplers for high-dimensional imaging inverse problems remains a challenge:
drawing and storing high-dimensional samples can be prohibitive, especially for
high-resolution images. To address this issue, this work proposes a distributed
sampler based on approximate data augmentation and PnP-ULA to solve very large
problems. The proposed sampler uses lightweight denoising convolutional neural
network, to efficiently exploit multiple GPUs on a Single Program Multiple Data
architecture. Reconstruction performance and scalability are evaluated on
several imaging problems. Communication and computation overheads due to the
denoiser are carefully discussed. The proposed distributed approach noticeably
combines three very precious qualities: it is scalable, enables uncertainty
quantification, for a reconstruction performance comparable to other PnP
methods.

</details>


### [11] [Classification of realisations of random sets](https://arxiv.org/abs/2511.00937)
*Bogdan Radović,Vesna Gotovac Đogaš,Kateřina Helisová*

Main category: stat.ME

TL;DR: 该论文提出了一种基于几何特征（边界曲率和周长面积比）的随机集模型分类方法，包括无监督和监督分类，并通过模拟研究和组织学图像分类验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决随机集模型的分类问题，特别是在医学图像分析中区分乳腺病变组织类型的需求。

Method: 使用基于边界曲率和周长面积比的几何特征计算随机集实现之间的相似性度量，采用无监督和监督分类方法。

Result: 通过模拟研究验证了方法的有效性，并成功应用于乳腺病变和乳腺癌组织学图像的分类。

Conclusion: 提出的基于几何特征的随机集分类方法在模拟和实际医学图像分类中都表现出良好性能，为组织病理学图像分析提供了有效工具。

Abstract: In this paper, the classification task for a family of sets representing the
realisation of some random set models is solved. Both unsupervised and
supervised classification methods are utilised using the similarity measure
between two realisations derived as empirical estimates of $\mathcal
N$-distances quantified based on geometric characteristics of the realisations,
namely the boundary curvature and the perimeter over area ratios of obtained
samples of connected components from the realisations. To justify the proposed
methodology, a simulation study is performed using random set models. The
methods are used further for classifying histological images of mastopathy and
mammary cancer tissue.

</details>


### [12] [On the estimation of leverage effect and volatility of volatility in the presence of jumps](https://arxiv.org/abs/2511.00944)
*Qiang Liu,Zhi Liu,Wang Zhou*

Main category: stat.ME

TL;DR: 本文提出了一种基于经验特征函数的高频数据估计方法，用于在跳跃存在的情况下估计杠杆效应和波动率的波动率。


<details>
  <summary>Details</summary>
Motivation: 现有估计方法在跳跃存在时表现不佳，特别是对于无限变差跳跃，需要更稳健的估计方法。

Method: 使用高频增量的经验特征函数构建现货波动率估计器，基于此提出杠杆效应和波动率波动率的估计器。

Result: 模拟研究表明新估计器在无限变差跳跃情况下表现优于现有方法，实证分析显示市场存在非零杠杆效应和波动率波动率。

Conclusion: 该方法在更一般的跳跃条件下有效，是实证应用中更好的替代方案，并建立了估计量的渐近正态性和方差估计的一致性。

Abstract: We study the estimation of leverage effect and volatility of volatility by
using high-frequency data with the presence of jumps. We first construct spot
volatility estimator by using the empirical characteristic function of the
high-frequency increments to deal with the effect of jumps, based on which the
estimators of leverage effect and volatility of volatility are proposed.
Compared with existing estimators, our method is valid under more general
jumps, making it a better alternative for empirical applications. Under some
mild conditions, the asymptotic normality of the estimators is established and
consistent estimators of the limiting variances are proposed based on the
estimation of volatility functionals. We conduct extensive simulation study to
verify the theoretical results. The results demonstrate that our estimators
have relative better performance than the existing ones, especially when the
jump is of infinite variation. Besides, we apply our estimators to a real
high-frequency dataset, which reveals nonzero leverage effect and volatility of
volatility in the market.

</details>


### [13] [Variance Estimation for the Inverse Probability of Treatment Weighted Kaplan Meier Estimator](https://arxiv.org/abs/2511.01110)
*Zhiwei Zhang,Yongwu Shao,Zhishen Ye*

Main category: stat.ME

TL;DR: 本文针对Xie和Liu提出的IPTW Kaplan-Meier估计量，开发了更准确的方差估计方法，考虑了倾向得分估计的变异性。


<details>
  <summary>Details</summary>
Motivation: Xie和Liu提出的IPTW KM估计量在观测性研究中广泛使用，但其方差估计未考虑倾向得分估计的变异性，导致方差高估。

Method: 通过严格的渐近分析，提出了考虑倾向得分估计变异性的plug-in方差估计方法，并通过模拟研究验证。

Result: 模拟结果显示，新提出的方差估计量比XL的方差估计量更准确，后者倾向于高估IPTW KM估计量的抽样方差。

Conclusion: 估计倾向得分确实会减小渐近方差，本文提出的方差估计方法能更准确地反映IPTW KM估计量的实际变异性。

Abstract: In a widely cited paper, Xie and Liu (henceforth XL) proposed to use inverse
probability of treatment weighting (IPTW) to account for possible confounding
in observational studies with survival endpoints subject to right censoring.
Their proposal includes an IPTW Kaplan-Meier (KM) estimator for the survival
function of a treatment-specific potential failure time, which can be used to
evaluate the causal effect of one treatment versus another. The IPTW KM
estimator is remarkably simple and highly effective for confounding bias
correction. The method has been implemented in SAS's popular procedure LIFETEST
for analyzing survival data and has seen widespread use.
  This letter is concerned with variance estimation for the IPTW KM estimator.
The variance estimator provided by XL does not account for the variability of
the IPTW weight when the propensity score is estimated from data, as is usually
the case in observational studies. In this letter, we provide a rigorous
asymptotic analysis of the IPTW KM estimator based on an estimated propensity
score. Our analysis indicates that estimating the propensity score does tend to
result in a smaller asymptotic variance, which can be estimated consistently
using a plug-in variance estimator. We also present a simulation study
comparing the variance estimator we propose with the XL variance estimator. Our
simulation results confirm that the proposed variance estimator is more
accurate than the XL variance estimator, which tends to over-estimate the
sampling variance of the IPTW KM estimator.

</details>


### [14] [Perturbed Double Machine Learning: Nonstandard Inference Beyond the Parametric Length](https://arxiv.org/abs/2511.01222)
*Mengchu Zheng,Matteo Bonvini,Zijian Guo*

Main category: stat.ME

TL;DR: 提出了Perturbed DML方法，在干扰参数估计收敛速度慢于n^{-1/4}时仍能保证有效的统计推断


<details>
  <summary>Details</summary>
Motivation: 传统基于Wald区间的推断方法要求干扰参数估计误差渐近可忽略（收敛速度快于n^{-1/4}），这一条件可能过于严格

Method: 1）在干扰参数估计步骤中注入随机性，生成一组扰动的干扰模型；2）过滤掉与原DML估计偏差超过阈值的扰动

Result: 对于Lasso干扰学习器，至少有一个扰动产生的干扰估计足够接近真实值，使得相关的β估计接近已知真实干扰参数的神谕估计器

Conclusion: Perturbed DML框架可扩展到一般机器学习干扰学习器，在DML估计器收敛速度慢于n^{-1/2}时仍能提供有效覆盖

Abstract: We study inference on a low dimensional functional $\beta$ in the presence of
possibly infinite dimensional nuisance parameters. Classical inferential
methods are typically based on the Wald interval, whose large sample validity
rests on the asymptotic negligibility of the nuisance error; for example,
estimators based on the influence curve of the parameter (Double/Debiased
Machine Learning DML estimators) are asymptotically Gaussian when the nuisance
estimators converge at rates faster than $n^{-1/4}$. Although, under suitable
conditions, such negligibility can hold even in nonparametric classes, it can
be restrictive. To relax this requirement, we propose Perturbed Double Machine
Learning (Perturbed DML) to ensure valid inference even when nuisance
estimators converge at rates slower than $n^{-1/4}$. Our proposal is to 1)
inject randomness into the nuisance estimation step to generate a collection of
perturbed nuisance models, each yielding an estimate of $\beta$ and a
corresponding Wald interval, and 2) filter out perturbations whose deviations
from the original DML estimate exceed a threshold. For Lasso nuisance learners,
we show that, with high probability, at least one perturbation produces
nuisance estimates sufficiently close to the truth, so that the associated
estimator of $\beta$ is close to an oracle estimator with knowledge of the true
nuisances. Taking the union of the retained intervals delivers valid coverage
even when the DML estimator converges more slowly than $n^{-1/2}$. The
framework extends to general machine learning nuisance learners, and
simulations show that Perturbed DML can have coverage when state of the art
methods fail.

</details>


### [15] [Z-Dip: a validated generalization of the Dip Test](https://arxiv.org/abs/2511.01705)
*Edoardo Di Martino,Matteo Cinelli,Roy Cerqueti*

Main category: stat.ME

TL;DR: 提出Z-Dip方法，标准化Hartigan's Dip Test以消除样本量依赖性，通过模拟和bootstrap校准决策阈值，并提供大样本下的降采样方法。


<details>
  <summary>Details</summary>
Motivation: 传统Hartigan's Dip Test存在样本量依赖性强且需要查找表的问题，限制了其在多模态检测中的实际应用。

Method: 通过模拟零分布标准化Dip值，使用bootstrap重采样校准通用决策阈值，并为大数据集提供降采样方法。

Result: 开发了Z-Dip方法，消除了样本量依赖性，提供了统一的决策标准，并发布了查找表和软件实现。

Conclusion: Z-Dip改进了传统Dip Test，提供了更可靠的多模态检测工具，特别适用于大样本数据分析。

Abstract: Detecting multimodality in empirical distributions is a fundamental problem
in statistics and data analysis, with applications ranging from clustering to
social science. Hartigan's Dip Test is a classical nonparametric procedure for
testing unimodality versus multimodality, but its interpretation is hindered by
strong dependence on sample size and the need for lookup tables. We introduce
the Z-Dip, a standardized extension of the Dip Test that removes sample-size
dependence by comparing observed Dip values to simulated null distributions. We
calibrate a universal decision threshold for Z-Dip via simulation and bootstrap
resampling, providing a unified criterion for multimodality detection. In the
final section, we also propose a downsampling-based approach to further
mitigate residual sample-size effects in very large datasets. Lookup tables and
software implementations are made available for efficient use in practice.

</details>


### [16] [Seamless Phase I--II Cancer Clinical Trials Using Kernel-Based Covariate Similarity](https://arxiv.org/abs/2511.01290)
*Kana Makino,Natsumi Makigusa,Masahiro Kojima*

Main category: stat.ME

TL;DR: 提出了一种非参数信息借用方法，通过核最大均值差异（MMD）量化I期和II期协变量分布的相似性，自适应地调整I期数据在II期疗效评估中的权重。


<details>
  <summary>Details</summary>
Motivation: 响应FDA的Project Optimus，加速肿瘤药物开发，提高无缝I/II期试验设计的效率，有效利用I期数据。

Method: 使用基于核的MMD度量I期和II期协变量分布相似性，将其转化为剂量特异性权重并整合到幂先验框架中，用于II期疗效评估（如客观缓解率ORR）。

Result: 模拟研究显示，该方法在有效剂量下提高了ORR 95%可信区间下限超过预设阈值的概率，同时在弱效剂量下避免了错误的阈值跨越。

Conclusion: 所提出的方法能够自适应地借用I期数据，提高早期肿瘤试验的统计效率，同时保持对协变量分布差异的敏感性。

Abstract: In response to the U.S.\ Food and Drug Administration's (FDA) Project
Optimus, a paradigm shift is underway in the design of early-phase oncology
trials. To accelerate drug development, seamless Phase I/II designs have gained
increasing attention, along with growing interest in the efficient reuse of
Phase I data. We propose a nonparametric information-borrowing method that
adaptively discounts Phase I observations according to the similarity of
covariate distributions between Phase I and Phase II. Similarity is quantified
using a kernel-based maximum mean discrepancy (MMD) and transformed into a
dose-specific weight incorporated into a power-prior framework for Phase II
efficacy evaluation, such as for the objective response rate (ORR). Considering
the small sample sizes typical of early-phase oncology studies, we analytically
derive a confidence interval for the weight, enabling assessment of borrowing
precision without resampling procedures. Simulation studies under four toxicity
scenarios and five baseline-covariate settings showed that the proposed method
improved the probability that the lower bound of the 95\% credible interval for
ORR exceeded a prespecified threshold at efficacious doses, while avoiding
false threshold crossings at weakly efficacious doses. A case study based on a
metastatic pancreatic ductal adenocarcinoma trial illustrates the resulting
borrowing weights and posterior estimates.

</details>


### [17] [Nonparametric Sensitivity Analysis for Unobserved Confounding with Survival Outcomes](https://arxiv.org/abs/2511.01412)
*Rui Hu,Ted Westling*

Main category: stat.ME

TL;DR: 提出了一种针对时间到事件数据的非参数敏感性分析框架，用于评估观察到的关联对潜在未观察混杂因素的稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有的敏感性分析方法依赖于未观察混杂因素结构的参数假设和Cox比例风险模型，如果这些假设不成立，敏感性分析的结论有效性存疑。此外，风险比的因果解释具有挑战性。

Method: 推导了观察生存曲线与反事实生存曲线之间差异的非参数边界，并使用半参数效率理论提出了这些边界的估计量和推断方法。

Result: 通过数值研究和分析高分级腮腺癌患者中择期颈部清扫对死亡率因果效应的实际案例，验证了所提出方法的性能。

Conclusion: 该非参数敏感性分析框架解决了现有方法的局限性，为时间到事件数据的因果推断提供了更稳健的工具。

Abstract: In observational studies, the observed association between an exposure and
outcome of interest may be distorted by unobserved confounding. Causal
sensitivity analysis can be used to assess the robustness of observed
associations to potential unobserved confounding. For time-to-event outcomes,
existing sensitivity analysis methods rely on parametric assumptions on the
structure of the unobserved confounders and Cox proportional hazards models for
the outcome regression. If these assumptions fail to hold, it is unclear
whether the conclusions of the sensitivity analysis remain valid. Additionally,
causal interpretation of the hazard ratio is challenging. To address these
limitations, in this paper we develop a nonparametric sensitivity analysis
framework for time-to-event data. Specifically, we derive nonparametric bounds
for the difference between the observed and counterfactual survival curves and
propose estimators and inference for these bounds using semiparametric
efficiency theory. We also provide nonparametric bounds and inference for the
difference between the observed and counterfactual restricted mean survival
times. We demonstrate the performance of our proposed methods using numerical
studies and an analysis of the causal effect of elective neck dissection on
mortality in patients with high-grade parotid carcinoma.

</details>


### [18] [Adaptive Change Point Inference for High Dimensional Time Series with Temporal Dependence](https://arxiv.org/abs/2511.01487)
*Xiaoyi Wang,Jixuan Liu,Long Feng*

Main category: stat.ME

TL;DR: 提出了一种用于高维时间序列变点推断的自适应方法，结合了基于max-L2范数的检验和max-L∞范数的检验，通过Cauchy组合方法实现稳健性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维时间序列中变点推断问题，特别是在不同稀疏度水平下保持稳健性能的需求。

Method: 首先提出基于max-L2范数的检验方法，然后证明该统计量与Wang和Feng(2023)提出的两个max-L∞统计量之间的渐近独立性，最后通过Cauchy组合方法整合这些检验。

Result: 模拟研究和实际数据分析表明，该方法在高维设置下具有优越的有效性，在不同稀疏度水平下都表现出稳健性能。

Conclusion: 提出的自适应推断方法在高维时间序列变点检测中表现出色，特别是在处理不同稀疏度模式时具有鲁棒性。

Abstract: This paper investigates change point inference in high-dimensional time
series. We begin by introducing a max-$L_2$-norm based test procedure, which
demonstrates strong performance under dense alternatives. We then establish the
asymptotic independence between our proposed statistic and the two
max-$L_\infty$-based statistics introduced by Wang and Feng (2023). Building on
this result, we develop an adaptive inference approach by applying the Cauchy
combination method to integrate these tests. This combined procedure exhibits
robust performance across varying levels of sparsity. Extensive simulation
studies and real data analysis further confirm the superior effectiveness of
our proposed methods in the high-dimensional setting.

</details>


### [19] [RESOLVE-IPD: High-Fidelity Individual Patient Data Reconstruction and Uncertainty-Aware Subgroup Meta-Analysis](https://arxiv.org/abs/2511.01785)
*Lang Lang,Yao Zhao,Qiuxin Gao,Yanxun Xu*

Main category: stat.ME

TL;DR: RESOLVE-IPD是一个统一的计算框架，用于高保真个体患者数据重建和不确定性感知的亚组荟萃分析，解决了现有KM曲线重建方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 肿瘤学试验的个体患者数据很少公开可用，需要从已发表的KM曲线重建。现有重建方法存在数字化误差、不现实的均匀删失假设，以及无法从汇总统计中恢复亚组水平IPD的问题。

Method: RESOLVE-IPD包含两个组件：1）高保真IPD重建（VEC-KM和CEN-KM模块），从矢量化图形精确提取KM坐标和删失标记；2）不确定性感知亚组恢复（MAPLE算法），推断患者水平亚组标签并与已发表汇总统计一致。

Result: 在晚期食管鳞状细胞癌的四个试验亚组荟萃分析中验证了RESOLVE-IPD，重点关注PD-L1低表达人群。

Conclusion: RESOLVE-IPD能够实现准确的IPD重建和稳健的不确定性感知亚组荟萃分析，增强了精准肿瘤学中二次证据合成的可靠性和透明度。

Abstract: Individual patient data (IPD) from oncology trials are essential for reliable
evidence synthesis but are rarely publicly available, necessitating
reconstruction from published Kaplan-Meier (KM) curves. Existing reconstruction
methods suffer from digitization errors, unrealistic uniform censoring
assumptions, and the inability to recover subgroup-level IPD when only
aggregate statistics are available. We developed RESOLVE-IPD, a unified
computational framework that enables high-fidelity IPD reconstruction and
uncertainty-aware subgroup meta-analysis to address these limitations.
RESOLVE-IPD comprises two components. The first component, High-Fidelity IPD
Reconstruction, integrates the VEC-KM and CEN-KM modules: VEC-KM extracts
precise KM coordinates and explicit censoring marks from vectorized figures,
minimizing digitization error, while CEN-KM corrects overlapping censor symbols
and eliminates the uniform censoring assumption. The second component,
Uncertainty-Aware Subgroup Recovery, employs the MAPLE (Marginal Assignment of
Plausible Labels and Evidence Propagation) algorithm to infer patient-level
subgroup labels consistent with published summary statistics (e.g., hazard
ratio, median overall survival) when subgroup KM curves are unavailable. MAPLE
generates ensembles of mathematically valid labelings, facilitating a
propagating meta-analysis that quantifies and reflects uncertainty from
subgroup reconstruction. RESOLVE-IPD was validated through a subgroup
meta-analysis of four trials in advanced esophageal squamous cell carcinoma,
focusing on the programmed death ligand 1 (PD-L1)-low population. RESOLVE-IPD
enables accurate IPD reconstruction and robust, uncertainty-aware subgroup
meta-analyses, strengthening the reliability and transparency of secondary
evidence synthesis in precision oncology.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [20] [Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data](https://arxiv.org/abs/2511.00217)
*Mitchell L. Prevett,Francis K. C. Hui,Zhi Yang Tho,A. H. Welsh,Anton H. Westveld*

Main category: stat.ML

TL;DR: 提出了GBMixed框架，将梯度提升扩展到混合模型，通过基于似然的梯度联合估计均值和方差分量，实现非参数估计同时保持可解释性


<details>
  <summary>Details</summary>
Motivation: 线性混合模型依赖参数形式限制了灵活性，而梯度提升方法虽然预测精度高但不支持聚类数据结构或不确定性量化

Method: 使用基于似然的梯度联合估计均值和方差分量，通过回归树或样条等灵活基学习器建模随机效应和残差方差

Result: 模拟和实际应用显示能准确恢复方差分量、校准预测区间，相比标准线性混合模型和非参数方法提高了预测精度

Conclusion: GBMixed提供了异方差不确定性量化，实现了异质随机效应的提升，支持协变量依赖的收缩和异质处理效应估计

Abstract: Linear mixed models are widely used for clustered data, but their reliance on
parametric forms limits flexibility in complex and high-dimensional settings.
In contrast, gradient boosting methods achieve high predictive accuracy through
nonparametric estimation, but do not accommodate clustered data structures or
provide uncertainty quantification.
  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and
algorithm that extends boosting to jointly estimate mean and variance
components via likelihood-based gradients. In addition to nonparametric mean
estimation, the method models both random effects and residual variances as
potentially covariate-dependent functions using flexible base learners such as
regression trees or splines, enabling nonparametric estimation while
maintaining interpretability.
  Simulations and real-world applications demonstrate accurate recovery of
variance components, calibrated prediction intervals, and improved predictive
accuracy relative to standard linear mixed models and nonparametric methods.
GBMixed provides heteroscedastic uncertainty quantification and introduces
boosting for heterogeneous random effects. This enables covariate-dependent
shrinkage for cluster-specific predictions to adapt between population and
cluster-level data. Under standard causal assumptions, the framework enables
estimation of heterogeneous treatment effects with reliable uncertainty
quantification.

</details>


### [21] [A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications](https://arxiv.org/abs/2511.00366)
*Krishna Prasath Logakannan,Shridhar Vashishtha,Jacob Hochhalter,Shandian Zhe,Robert M. Kirby*

Main category: stat.ML

TL;DR: 本文提出了一种增强高斯过程模型的方法，通过包含导数数据来提高数字孪生的预测精度，并使用稀疏GP近似来解决计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 数字孪生需要实时预测物理孪生的未来状态，而高精度代理模型比多物理模型更受青睐。为了适应特定物理孪生，数字孪生模型必须使用服役数据更新。

Method: 扩展高斯过程模型以包含导数数据，采用稀疏GP近似来降低计算复杂度，并开发动态更新机制来吸收物理孪生数据。

Result: 数值实验表明，导数增强的稀疏GP方法在动态数据添加后能产生改进的模型，预测精度得到提升。

Conclusion: 该方法成功应用于航空航天车辆疲劳裂纹增长的建模，证明了其在数字孪生框架中的有效性。

Abstract: Digital twins are developed to model the behavior of a specific physical
asset (or twin), and they can consist of high-fidelity physics-based models or
surrogates. A highly accurate surrogate is often preferred over multi-physics
models as they enable forecasting the physical twin future state in real-time.
To adapt to a specific physical twin, the digital twin model must be updated
using in-service data from that physical twin. Here, we extend Gaussian process
(GP) models to include derivative data, for improved accuracy, with dynamic
updating to ingest physical twin data during service. Including derivative
data, however, comes at a prohibitive cost of increased covariance matrix
dimension. We circumvent this issue by using a sparse GP approximation, for
which we develop extensions to incorporate derivatives. Numerical experiments
demonstrate that the prediction accuracy of the derivative-enhanced sparse GP
method produces improved models upon dynamic data additions. Lastly, we apply
the developed algorithm within a DT framework to model fatigue crack growth in
an aerospace vehicle.

</details>


### [22] [Accuracy estimation of neural networks by extreme value theory](https://arxiv.org/abs/2511.00490)
*Gero Junike,Marco Oesting*

Main category: stat.ML

TL;DR: 应用极值理论量化神经网络误差的大值分布，提出新的形状参数估计器


<details>
  <summary>Details</summary>
Motivation: 神经网络能近似任意连续函数，但难以量化误差大小，特别是在应用中重要的大误差值

Method: 使用极值理论分析误差分布，提出适合神经网络误差的广义帕累托分布形状参数估计器

Result: 误差超过阈值后的分布近似服从广义帕累托分布，数值实验验证了方法的有效性

Conclusion: 极值理论为量化神经网络误差提供了有效工具，新估计器能准确描述误差分布特征

Abstract: Neural networks are able to approximate any continuous function on a compact
set. However, it is not obvious how to quantify the error of the neural
network, i.e., the remaining bias between the function and the neural network.
Here, we propose the application of extreme value theory to quantify large
values of the error, which are typically relevant in applications. The
distribution of the error beyond some threshold is approximately generalized
Pareto distributed. We provide a new estimator of the shape parameter of the
Pareto distribution suitable to describe the error of neural networks.
Numerical experiments are provided.

</details>


### [23] [SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations](https://arxiv.org/abs/2511.00685)
*Haoting Zhang,Haoxian Chen,Donglin Zhan,Hanyang Zhao,Henry Lam,Wenpin Tang,David Yao,Zeyu Zheng*

Main category: stat.ML

TL;DR: SOCRATES是一个利用大语言模型自动设计仿真优化算法的两阶段框架，通过构建数字副本系统和元优化器分析来创建自适应的混合优化策略。


<details>
  <summary>Details</summary>
Motivation: 传统仿真优化方法需要人工设计，面对复杂系统时效率低下。大语言模型的出现为自动化选择和组合已有SO方法提供了新范式。

Method: 第一阶段：使用LLM从系统文本描述中实现因果发现，构建数字副本系统；第二阶段：在副本系统上评估基线算法，LLM作为元优化器分析性能轨迹，迭代修订并组合成最终的自适应混合优化策略。

Result: 开发了SOCRATES框架，将LLM驱动的推理与轨迹感知的元优化相结合，为复杂SO问题创建了有效且样本高效的解决方案。

Conclusion: SOCRATES通过集成LLM能力，实现了仿真优化算法的自动化设计，能够生成适应性强、样本效率高的混合优化策略。

Abstract: The field of simulation optimization (SO) encompasses various methods
developed to optimize complex, expensive-to-sample stochastic systems.
Established methods include, but are not limited to, ranking-and-selection for
finite alternatives and surrogate-based methods for continuous domains, with
broad applications in engineering and operations management. The recent advent
of large language models (LLMs) offers a new paradigm for exploiting system
structure and automating the strategic selection and composition of these
established SO methods into a tailored optimization procedure. This work
introduces SOCRATES (Simulation Optimization with Correlated Replicas and
Adaptive Trajectory Evaluations), a novel two-stage procedure that leverages
LLMs to automate the design of tailored SO algorithms. The first stage
constructs an ensemble of digital replicas of the real system. An LLM is
employed to implement causal discovery from a textual description of the
system, generating a structural `skeleton' that guides the sample-efficient
learning of the replicas. In the second stage, this replica ensemble is used as
an inexpensive testbed to evaluate a set of baseline SO algorithms. An LLM then
acts as a meta-optimizer, analyzing the performance trajectories of these
algorithms to iteratively revise and compose a final, hybrid optimization
schedule. This schedule is designed to be adaptive, with the ability to be
updated during the final execution on the real system when the optimization
performance deviates from expectations. By integrating LLM-driven reasoning
with LLM-assisted trajectory-aware meta-optimization, SOCRATES creates an
effective and sample-efficient solution for complex SO optimization problems.

</details>


### [24] [Perturbations in the Orthogonal Complement Subspace for Efficient Out-of-Distribution Detection](https://arxiv.org/abs/2511.00849)
*Zhexiao Huang,Weihao He,Shutao Deng,Junzhe Chen,Chao Yuan,Hongxin Wang,Changsheng Zhou*

Main category: stat.ML

TL;DR: 提出P-OCS方法，通过在ID特征主成分正交补空间中施加单次投影扰动来增强ID-OOD区分，实现高效OOD检测


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法依赖高维表示分离ID和OOD样本，需要更轻量且理论可靠的方法

Method: 在ID特征主成分正交补空间中应用单次投影扰动，增强ID-OOD差异同时保持ID表示几何结构

Result: 在多种架构和数据集上实现最先进的OOD检测性能，计算成本可忽略

Conclusion: P-OCS是轻量级、理论可靠且无需模型重训练、OOD数据或架构修改的OOD检测方法

Abstract: Out-of-distribution (OOD) detection is essential for deploying deep learning
models in open-world environments. Existing approaches, such as energy-based
scoring and gradient-projection methods, typically rely on high-dimensional
representations to separate in-distribution (ID) and OOD samples. We introduce
P-OCS (Perturbations in the Orthogonal Complement Subspace), a lightweight and
theoretically grounded method that operates in the orthogonal complement of the
principal subspace defined by ID features. P-OCS applies a single projected
perturbation restricted to this complementary subspace, enhancing subtle ID-OOD
distinctions while preserving the geometry of ID representations. We show that
a one-step update is sufficient in the small-perturbation regime and provide
convergence guarantees for the resulting detection score. Experiments across
multiple architectures and datasets demonstrate that P-OCS achieves
state-of-the-art OOD detection with negligible computational cost and without
requiring model retraining, access to OOD data, or changes to model
architecture.

</details>


### [25] [Binary perceptron computational gap -- a parametric fl RDT view](https://arxiv.org/abs/2511.01037)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 本文使用完全提升随机对偶理论（fl RDT）分析非对称二元感知器（ABP）的统计-计算间隙，发现随着提升层级增加，关键参数序列现象学发生变化，这与算法阈值αa相关。在第五层级获得约束密度估计α≈0.7764，与聚类碎片化范围一致。


<details>
  <summary>Details</summary>
Motivation: 研究非对称二元感知器（ABP）中统计-计算间隙的表现特征，特别是满足性阈值αc和算法阈值αa之间的关系，探索高效算法能否接近这些阈值。

Method: 采用完全提升随机对偶理论（fl RDT）的参数化方法，分析不同提升层级下关键参数序列的变化特性，并与约束密度阈值进行关联。

Result: 在第二层级获得临界约束密度αc≈0.8331，随着层级提升该估计值下降，在第五层级收敛到α≈0.7764，与聚类碎片化范围α∈(0.77,0.78)高度一致。参数序列现象学变化与负Hopfield模型相似。

Conclusion: fl RDT方法能够有效估计ABP的算法阈值，参数序列现象学变化与统计-计算间隙密切相关，为理解高效算法接近阈值的能力提供了理论依据。

Abstract: Recent studies suggest that asymmetric binary perceptron (ABP) likely
exhibits the so-called statistical-computational gap characterized with the
appearance of two phase transitioning constraint density thresholds:
\textbf{\emph{(i)}} the \emph{satisfiability threshold} $\alpha_c$, below/above
which ABP succeeds/fails to operate as a storage memory; and
\textbf{\emph{(ii)}} \emph{algorithmic threshold} $\alpha_a$, below/above which
one can/cannot efficiently determine ABP's weight so that it operates as a
storage memory.
  We consider a particular parametric utilization of \emph{fully lifted random
duality theory} (fl RDT) [85] and study its potential ABP's algorithmic
implications. A remarkable structural parametric change is uncovered as one
progresses through fl RDT lifting levels. On the first two levels, the
so-called $\c$ sequence -- a key parametric fl RDT component -- is of the
(natural) decreasing type. A change of such phenomenology on higher levels is
then connected to the $\alpha_c$ -- $\alpha_a$ threshold change. Namely, on the
second level concrete numerical values give for the critical constraint density
$\alpha=\alpha_c\approx 0.8331$. While progressing through higher levels
decreases this estimate, already on the fifth level we observe a satisfactory
level of convergence and obtain $\alpha\approx 0.7764$. This allows to draw two
striking parallels: \textbf{\emph{(i)}} the obtained constraint density
estimate is in a remarkable agrement with range $\alpha\in (0.77,0.78)$ of
clustering defragmentation (believed to be responsible for failure of locally
improving algorithms) [17,88]; and \textbf{\emph{(ii)}} the observed change of
$\c$ sequence phenomenology closely matches the one of the negative Hopfield
model for which the existence of efficient algorithms that closely approach
similar type of threshold has been demonstrated recently [87].

</details>


### [26] [Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry](https://arxiv.org/abs/2511.01064)
*Charles C. Margossian,Lawrence K. Saul*

Main category: stat.ML

TL;DR: 本文扩展了变分推断在位置-尺度族中的对称性保证，研究了更广泛的散度族和部分对称性情况下的理论保证。


<details>
  <summary>Details</summary>
Motivation: 扩展变分推断的对称性保证，特别是在更广泛的散度族和部分对称性情况下的理论分析，这对于贝叶斯层次模型等实际应用具有重要意义。

Method: 使用对称性分析方法，研究变分推断在不同散度族下的性质，特别关注目标密度在部分坐标上具有偶对称性和椭圆对称性的情况。

Result: 证明了在更广泛的散度族下，变分推断能够恢复目标分布的均值和相关矩阵，并获得了部分对称性情况下的进一步理论保证。

Conclusion: 变分推断的对称性保证可以扩展到更广泛的散度族和部分对称性情况，为贝叶斯层次模型等复杂模型提供了理论支持。

Abstract: We extend several recent results providing symmetry-based guarantees for
variational inference (VI) with location-scale families. VI approximates a
target density~$p$ by the best match $q^*$ in a family $Q$ of tractable
distributions that in general does not contain $p$. It is known that VI can
recover key properties of $p$, such as its mean and correlation matrix, when
$p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the
reverse Kullback-Leibler divergence. We extend these guarantees in two
important directions. First, we provide symmetry-based guarantees for a broader
family of divergences, highlighting the properties of variational objectives
under which VI provably recovers the mean and correlation matrix. Second, we
obtain further guarantees for VI when the target density $p$ exhibits even and
elliptical symmetries in some but not all of its coordinates. These partial
symmetries arise naturally in Bayesian hierarchical models, where the prior
induces a challenging geometry but still possesses axes of symmetry. We
illustrate these theoretical results in a number of experimental settings.

</details>


### [27] [Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes](https://arxiv.org/abs/2511.01096)
*Alex Boyd,Andrew Warrington,Taha Kass-Hout,Parminder Bhatia,Danica Xiao*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Foundational marked temporal point process (MTPP) models, such as the Hawkes
process, often use inexpressive model families in order to offer interpretable
parameterizations of event data. On the other hand, neural MTPPs models forego
this interpretability in favor of absolute predictive performance. In this
work, we present a new family MTPP models: the hyper Hawkes process (HHP),
which aims to be as flexible and performant as neural MTPPs, while retaining
interpretable aspects. To achieve this, the HHP extends the classical Hawkes
process to increase its expressivity by first expanding the dimension of the
process into a latent space, and then introducing a hypernetwork to allow time-
and data-dependent dynamics. These extensions define a highly performant MTPP
family, achieving state-of-the-art performance across a range of benchmark
tasks and metrics. Furthermore, by retaining the linearity of the recurrence,
albeit now piecewise and conditionally linear, the HHP also retains much of the
structure of the original Hawkes process, which we exploit to create direct
probes into how the model creates predictions. HHP models therefore offer both
state-of-the-art predictions, while also providing an opportunity to ``open the
box'' and inspect how predictions were generated.

</details>


### [28] [Few-Shot Multimodal Medical Imaging: A Theoretical Framework](https://arxiv.org/abs/2511.01140)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: stat.ML

TL;DR: 提出了一个统一的理论框架，用于在低资源医学影像条件下表征学习和推理，解决了数据稀缺情况下的样本效率、不确定性量化和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 医学影像面临数据获取困难、数据系统分散、数据集不平衡等结构障碍，导致诊断不确定性增加、模型鲁棒性降低和诊断决策偏差。现有方法缺乏在数据稀缺情况下成功或失败的理论依据。

Method: 基于PAC学习和PAC-Bayesian理论，形式化少样本条件下的学习目标，计算样本复杂度约束，提出多模态集成促进泛化的理论解释，并设计解释稳定性的形式化度量。

Result: 建立了样本效率、不确定性量化和可解释性在统一理论框架下的联合表征，为构建可靠的数据高效诊断系统提供了理论基础。

Conclusion: 该框架为构建可靠的、数据高效的诊断系统建立了原则性基础，通过统一理论设置共同表征样本效率、不确定性量化和可解释性。

Abstract: Medical imaging relies heavily on large, labeled datasets. But,
unfortunately, they are not always easily accessible in clinical settings.
Additionally, many practitioners often face various structural obstacles like
limited data availability, fragmented data systems, and unbalanced datasets.
These barriers often lead to the increased diagnostic uncertainty,
underrepresentation of certain conditions, reduced model robustness, and biased
diagnostic decisions. In response to these challenges, approaches such as
transfer learning, meta-learning, and multimodal fusion have made great
strides. However, they still need a solid theoretical justification for why
they succeed or fail in situations where data is scarce. To address this gap,
we propose a unified theoretical framework that characterizes learning and
inference under low-resource medical imaging conditions. We first formalize the
learning objective under few-shot conditions and compute sample complexity
constraints to estimate the smallest quantity of data needed to achieve
clinically reliable accuracy. Then based on ideas from PAC-learning and
PAC-Bayesian theory, we explain how multimodal integration encourages
generalization and quantifies uncertainty under sparse supervision. We further
propose a formal metric for explanation stability, offering interpretability
guarantees under low-data conditions. Taken together, the proposed framework
establishes a principled foundation for constructing dependable, data-efficient
diagnostic systems by jointly characterizing sample efficiency, uncertainty
quantification, and interpretability in a unified theoretical setting.

</details>


### [29] [An Interdisciplinary and Cross-Task Review on Missing Data Imputation](https://arxiv.org/abs/2511.01196)
*Jicong Fan*

Main category: stat.ML

TL;DR: 这是一篇关于缺失数据填补方法的系统性综述，涵盖了从经典统计方法到现代机器学习技术的全面分类，特别关注复杂数据类型和下游任务的整合。


<details>
  <summary>Details</summary>
Motivation: 缺失数据是数据科学中的基础挑战，严重影响各领域的分析和决策。现有文献在不同领域间分散，需要将统计基础与现代机器学习进展进行综合。

Method: 系统回顾核心概念（缺失机制、单/多重填补、不同填补目标），对填补方法进行全面分类，包括经典技术（回归、EM算法）和现代方法（低/高秩矩阵补全、深度学习模型、大语言模型），特别关注复杂数据类型。

Result: 提供了填补方法的系统分类框架，分析了不同领域的问题特征，评估了理论保证、基准资源和评估指标。

Conclusion: 识别了关键挑战和未来方向，包括模型选择和超参数优化、联邦学习中的隐私保护填补，以及跨领域和数据类型通用模型的发展，为未来研究提供了路线图。

Abstract: Missing data is a fundamental challenge in data science, significantly
hindering analysis and decision-making across a wide range of disciplines,
including healthcare, bioinformatics, social science, e-commerce, and
industrial monitoring. Despite decades of research and numerous imputation
methods, the literature remains fragmented across fields, creating a critical
need for a comprehensive synthesis that connects statistical foundations with
modern machine learning advances. This work systematically reviews core
concepts-including missingness mechanisms, single versus multiple imputation,
and different imputation goals-and examines problem characteristics across
various domains. It provides a thorough categorization of imputation methods,
spanning classical techniques (e.g., regression, the EM algorithm) to modern
approaches like low-rank and high-rank matrix completion, deep learning models
(autoencoders, GANs, diffusion models, graph neural networks), and large
language models. Special attention is given to methods for complex data types,
such as tensors, time series, streaming data, graph-structured data,
categorical data, and multimodal data. Beyond methodology, we investigate the
crucial integration of imputation with downstream tasks like classification,
clustering, and anomaly detection, examining both sequential pipelines and
joint optimization frameworks. The review also assesses theoretical guarantees,
benchmarking resources, and evaluation metrics. Finally, we identify critical
challenges and future directions, emphasizing model selection and
hyperparameter optimization, the growing importance of privacy-preserving
imputation via federated learning, and the pursuit of generalizable models that
can adapt across domains and data types, thereby outlining a roadmap for future
research.

</details>


### [30] [Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift](https://arxiv.org/abs/2511.01292)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 本文首次从理论和实证角度研究了注意力温度在分布偏移下的上下文学习中的作用，发现最优注意力温度可以最小化泛化误差，提高Transformer模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在上下文学习中表现优异，但在预训练和测试数据分布偏移时性能会急剧下降。虽然经验研究表明调整注意力温度可以提升性能，但其在分布偏移下的作用机制尚未被探索。

Method: 使用简化的"线性化softmax"框架推导闭式泛化误差表达式，通过理论证明和大量实验验证，包括线性回归任务模拟以及GPT-2和LLaMA2-7B在问答基准上的大规模实验。

Result: 研究发现输入协方差或标签噪声的偏移会显著损害上下文学习性能，但存在最优注意力温度可以最小化这种误差。实验验证了理论预测，表明注意力温度是提高预训练Transformer鲁棒性的有效机制。

Conclusion: 注意力温度是提高预训练Transformer在分布偏移下上下文学习鲁棒性的原理性且强大的机制，为理论理解提供了进展，并为实践中选择注意力温度提供了可操作的指导。

Abstract: Pretrained Transformers excel at in-context learning (ICL), inferring new
tasks from only a handful of examples. Yet, their ICL performance can degrade
sharply under distribution shift between pretraining and test data, a regime
increasingly common in real-world deployments. While recent empirical work
hints that adjusting the attention temperature in the softmax can enhance
Transformer performance, the attention temperature's role in ICL under
distribution shift remains unexplored. This paper provides the first
theoretical and empirical study of attention temperature for ICL under
distribution shift. Using a simplified but expressive "linearized softmax"
framework, we derive closed-form generalization error expressions and prove
that shifts in input covariance or label noise substantially impair ICL, but
that an optimal attention temperature exists which minimizes this error. We
then validate our predictions through extensive simulations on linear
regression tasks and large-scale experiments with GPT-2 and LLaMA2-7B on
question-answering benchmarks. Our results establish attention temperature as a
principled and powerful mechanism for improving the robustness of ICL in
pretrained Transformers, advancing theoretical understanding and providing
actionable guidance for selecting attention temperature in practice.

</details>


### [31] [Partial Trace-Class Bayesian Neural Networks](https://arxiv.org/abs/2511.01628)
*Arran Carter,Torben Sell*

Main category: stat.ML

TL;DR: 提出了三种部分迹类贝叶斯神经网络架构，在保持不确定性量化能力的同时显著减少贝叶斯参数数量，提高了计算效率和内存使用效率。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯神经网络虽然能提供严格的不确定性量化，但计算成本过高，限制了其实际应用。

Method: 基于迹类神经网络先验构建三种部分迹类贝叶斯神经网络架构，通过参数排序机制减少贝叶斯参数数量。

Result: 数值模拟验证了该方法在速度和内存需求方面的优势，并在真实数据集上展示了良好性能。

Conclusion: 该方法为神经网络提供了可靠、鲁棒且可扩展的不确定性量化解决方案。

Abstract: Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in
deep learning, but often come at a prohibitive computational cost. We propose
three different innovative architectures of partial trace-class Bayesian neural
networks (PaTraC BNNs) that enable uncertainty quantification comparable to
standard BNNs but use significantly fewer Bayesian parameters. These PaTraC
BNNs have computational and statistical advantages over standard Bayesian
neural networks in terms of speed and memory requirements. Our proposed
methodology therefore facilitates reliable, robust, and scalable uncertainty
quantification in neural networks. The three architectures build on trace-class
neural network priors which induce an ordering of the neural network
parameters, and are thus a natural choice in our framework. In a numerical
simulation study, we verify the claimed benefits, and further illustrate the
performance of our proposed methodology on a real-world dataset.

</details>


### [32] [A Proof of Learning Rate Transfer under $μ$P](https://arxiv.org/abs/2511.01734)
*Soufiane Hayou*

Main category: stat.ML

TL;DR: 该论文首次证明了在μP参数化的线性多层感知机中，学习率随宽度转移的现象，并指出最优学习率在宽度趋于无穷时收敛到非零常数。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解释神经网络中学习率随宽度转移的现象，特别是在μP参数化下，为无限宽度极限下的特征学习提供理论依据。

Method: 使用μP参数化方法，对比标准参数化(SP)和神经正切参数化(NTP)，通过理论证明和大量实验验证学习率转移现象。

Result: 结果表明在μP参数化下，最优学习率在宽度趋于无穷时收敛到非零常数，而在SP和NTP参数化下这一性质不成立。

Conclusion: μP参数化能够实现学习率转移，为无限宽度神经网络的特征学习提供了理论支持，而传统参数化方法无法实现这一特性。

Abstract: We provide the first proof of learning rate transfer with width in a linear
multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network
parameterization designed to ``maximize'' feature learning in the
infinite-width limit. We show that under $\mu P$, the optimal learning rate
converges to a \emph{non-zero constant} as width goes to infinity, providing a
theoretical explanation to learning rate transfer. In contrast, we show that
this property fails to hold under alternative parametrizations such as Standard
Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide
intuitive proofs and support the theoretical findings with extensive empirical
results.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [33] [Predicting the spatial distribution and demographics of commercial swine farms in the United States](https://arxiv.org/abs/2511.00132)
*Felipe E. Sanchez,Thomas A. Lake,Jason A. Galvis,Chris Jones,Gustavo Machado*

Main category: stat.AP

TL;DR: 开发了一个语义分割模型来识别美国东南部和中西部地区的猪舍位置，通过后处理随机森林分类器大幅减少误报，最终确认了45,580个猪舍多边形，并分类为16,976个农场和四种生产类型，同时使用随机森林回归模型估计种群规模。


<details>
  <summary>Details</summary>
Motivation: 获取牲畜农场位置和人口统计数据对于疾病监测、风险评估和开发空间显式流行病学模型至关重要，而现有美国生猪生产空间和人口统计数据存在显著差距。

Method: 使用语义分割模型识别猪舍候选位置，然后应用随机森林分类器进行后处理以减少误报，最后使用随机森林回归模型估计种群规模。

Result: 语义分割模型F2得分92%，平均IoU 76%；随机森林分类器在东南部和中西部分别减少82%和88%的误报；最终确认45,580个猪舍多边形，分组为16,976个农场；种群规模预测准确率因生产类型而异，87%的1,000-2,000头猪场预测误差在500头以内。

Conclusion: 该研究揭示了美国生猪生产现有空间和人口统计数据的显著差距，为疾病监测和流行病学建模提供了重要数据支持。

Abstract: Data on livestock farm locations and demographics are essential for disease
monitoring, risk assessment, and developing spatially explicit epidemiological
models. Our semantic segmentation model achieved an F2 score of 92 % and a mean
Intersection over Union of 76 %. An initial total of 194,474 swine barn
candidates were identified in the Southeast (North Carolina = 111,135, South
Carolina = 37,264 Virginia = 46,075) and 524,962 in the Midwest (Iowa = 168,866
Minnesota = 165,714 Ohio = 190,382). The post processing Random Forest
classifier reduced false positives by 82 % in the Southeast and 88 % in the
Midwest, resulting in 45,580 confirmed barn polygons. These were grouped into
16,976 predicted farms and classified into one of the four production types.
Population sizes were then estimated using the Random Forest regression model,
with prediction accuracy varying by production type. Across all farms, 87 % of
predictions for operations with 1,000 2,000 pigs were within 500 pigs of the
reference value, with nursery farms showing the highest agreement (R2= 0.82),
followed by finisher farms (R2 = 0.77) and sow farms (R2 = 0.56). Our results
revealed substantial gaps in the existing spatial and demographic data on U.S.
swine production.

</details>


### [34] [A Systematic Review of Spatio-Temporal Statistical Models: Theory, Structure, and Applications](https://arxiv.org/abs/2511.00422)
*Isabella Habereder,Thomas Kneib,Isao Echizen,Timo Spinde*

Main category: stat.AP

TL;DR: 本文对2021-2025年间83篇时空统计模型文献进行了系统性综述，提出了模型结构分类方案，分析了流行病学、生态学、公共卫生等主要应用领域的模型特点。


<details>
  <summary>Details</summary>
Motivation: 现有综述多局限于特定领域或模型类型，缺乏跨学科的综合概述，因此需要全面梳理不同学科中时空模型的应用情况。

Method: 遵循PRISMA指南进行系统性文献综述，搜索两个数据库，筛选出83篇符合标准的文献，提出时空模型结构分类方案。

Result: 发现层次模型使用最频繁，大多数模型包含加性组件处理时空依赖关系；不同应用领域的首选模型结构存在差异；研究集中在少数几个学科，可重复性有限。

Conclusion: 该综述为跨学科比较模型结构提供了参考，同时强调了提高透明度、可访问性和跨领域知识转移的必要性。

Abstract: Data with spatial-temporal attributes are prevalent across many research
fields, and statistical models for analyzing spatio-temporal relationships are
widely used. Existing reviews focus either on specific domains or model types,
creating a gap in comprehensive, cross-disciplinary overviews. To address this,
we conducted a systematic literature review following the PRISMA guidelines,
searched two databases for the years 2021-2025, and identified 83 publications
that met our criteria. We propose a classification scheme for spatio-temporal
model structures and highlight their application in the most common fields:
epidemiology, ecology, public health, economics, and criminology. Although
tasks vary by domain, many models share similarities. We found that
hierarchical models are the most frequently used, and most models incorporate
additive components to account for spatial-temporal dependencies. The preferred
model structures differ among fields of application. We also observe that
research efforts are concentrated in only a few specific disciplines, despite
the broader relevance of spatio-temporal data. Furthermore, we notice that
reproducibility remains limited. Our review, therefore, not only offers
inspiration for comparing model structures in an interdisciplinary manner but
also highlights opportunities for greater transparency, accessibility, and
cross-domain knowledge transfer.

</details>


### [35] [Spatiotemporal Dynamics of Conflict Occurrence and Fatalities in Ethiopia: A Bayesian Model and Predictive Insights Using Event-level Data (1997--2024)](https://arxiv.org/abs/2511.00867)
*Yassin Tesfaw Abebe,Abdu Mohammed Seid,Lassi Roininen,Mohammed Seid Ali*

Main category: stat.AP

TL;DR: 该研究提出了一个时空双贝叶斯模型，使用埃塞俄比亚1997-2024年的冲突事件数据，同时分析冲突死亡的发生概率和死亡人数。模型结合了协变量的固定效应和捕捉时空影响的随机效应，结果显示空袭、炮击和攻击造成最高死亡风险，夏季多发伤亡事件，边境地区暴力更严重。


<details>
  <summary>Details</summary>
Motivation: 传统冲突分析往往单独处理死亡发生和死亡人数，缺乏对两者关联性的建模。本研究旨在开发一个能同时分析冲突死亡发生概率和死亡人数的综合模型，以更好地理解冲突动态和预测风险。

Method: 使用时空双贝叶斯模型，将死亡视为两个关联结果：死亡发生的二元变量和死亡人数的计数变量。模型包含协变量的加性固定效应和捕捉时空影响的随机效应，采用Matérn场先验建模空间结构，使用INLA进行推理。

Result: 结果显示强烈的空间聚集性和时间变异性。空袭、炮击和攻击具有最高的死亡可能性和死亡人数；社区和反叛行为体造成最多死亡；夏季多发伤亡事件；靠近边境地区暴力更严重，而远离城市中心与低强度事件相关。

Conclusion: 同时建模时空维度对于更好地理解和预测冲突死亡风险至关重要。研究结果为保护脆弱社区的规划、政策和资源分配提供了重要见解。

Abstract: This study presents a spatiotemporal dual Bayesian model that examines both
the occurrence and number of conflict fatalities using event-level data from
Ethiopia (1997-2024), sourced from the Armed Conflict Location and Event Data
(ACLED) project. Fatalities are treated as two linked outcomes: the binary
occurrence of deaths and the count of deaths when they occur. The model
combines additive fixed effects for covariates with random effects capturing
spatiotemporal influences, allowing for outcome-specific effects. Covariates
include event type and season as categorical variables, proximity to cities and
borders as nonlinear effects, and population as an offset term in the count
model. A latent spatiotemporal process accounts for shared spatial and temporal
dependence, with the spatial structure modeled using a Mat\'ern field prior and
inference via Integrated Nested Laplace Approximation (INLA). Results show
strong spatial clustering and temporal variation in fatality risk, emphasizing
the importance of modeling both dimensions for better understanding and
prediction. Airstrikes, shelling, and attacks show the highest fatality
likelihood and counts, while communal and rebel actors cause the most deaths.
Multiple fatalities are more likely in summer, and proximity to borders drives
intense violence, whereas remoteness from urban centers is linked to
lower-intensity events. These results provide insight for planning, policy, and
resource allocation to protect vulnerable communities.

</details>


### [36] [Geometric Modeling of Hippocampal Tau Deposition: A Surface-Based Framework for Covariate Analysis and Off-Target Contamination Detection](https://arxiv.org/abs/2511.01732)
*Liangkang Wang,Akhil Ambekar,Ani Eloyan*

Main category: stat.AP

TL;DR: 该论文提出了一个结合几何建模和疾病进展分析的框架，用于研究阿尔茨海默病中的tau蛋白沉积。通过构建海马体的主表面，量化tau覆盖度、强度和厚度，识别AD亚型和阶段，揭示不同的tau动态模式。


<details>
  <summary>Details</summary>
Motivation: 研究阿尔茨海默病中tau蛋白沉积的空间分布和形态变化，通过几何建模方法解决多比较问题和保持空间特异性，分析不同AD亚型的tau沉积动态。

Method: 构建海马体主表面，通过双向投影距离和标准化摄取值比率(SUVR)插值量化tau参数。使用两阶段回归模型分析协变量效应，采用SuStaIn模型识别AD亚型和阶段。

Result: 识别出两种AD亚型：边缘主导型显示年龄相关的非线性tau积累，后部亚型显示均匀SUVR增加。海马体tau沉积呈现双向扩展的结构化空间轨迹，亚型差异与全脑模式一致。

Conclusion: 该框架成功揭示了AD中tau沉积的结构化空间轨迹和亚型特异性动态，证明了其在多种成像模态中的广泛应用潜力。

Abstract: We introduce a framework combining geometric modeling with disease
progression analysis to investigate tau deposition in Alzheimer's disease (AD)
using positron emission tomography (PET) data. Focusing on the hippocampus, we
construct a principal surface that captures the spatial distribution and
morphological changes of tau pathology. By projecting voxels onto this surface,
we quantify tau coverage, intensity, and thickness through bidirectional
projection distances and interpolated standardized uptake value ratios (SUVR).
This low-dimensional embedding preserves spatial specificity while mitigating
multiple comparison issues. Covariate effects are analyzed using a two-stage
regression model with inverse probability weighting to adjust for signal
sparsity and selection bias. Using the SuStaIn model, we identify subtypes and
stages of AD, revealing distinct tau dynamics: the limbic-predominant subtype
shows age-related nonlinear accumulation in coverage and thickness, whereas the
posterior subtype exhibits uniform SUVR increases across disease progression.
Model-based predictions show that hippocampal tau deposition follows a
structured spatial trajectory expanding bidirectionally with increasing
thickness, while subtype differences highlight posterior hippocampal
involvement consistent with whole-brain patterns. Finally, directional signal
patterns on the principal surface reveal contamination from the choroid plexus,
demonstrating the broader applicability of the proposed framework across
modalities including amyloid PET.

</details>


### [37] [The Multidimensional Index of Child Growth (MICG) of the Task Force "Towards a Multidimensional Approach for Child Growth" of the International Union for Nutrition Sciences](https://arxiv.org/abs/2511.01607)
*Rolando Gonzales Martinez,Hinke Haisma*

Main category: stat.AP

TL;DR: 本文介绍了多维儿童成长指数(MICG)，这是一个基于能力和权利框架的儿童福祉评估工具，涵盖14个维度，能够揭示传统身体测量指标无法发现的剥夺问题。


<details>
  <summary>Details</summary>
Motivation: 传统儿童成长测量主要关注身高体重等身体指标，无法全面反映儿童福祉。需要开发一个多维度的评估框架来更好地监测儿童全面发展。

Method: 使用能力-权利框架构建14个维度的儿童福祉指标体系，包括健康、照料、心理健康、参与度等。采用多种加权方法比较，并应用贝叶斯方法估计未实现的机会。

Result: 研究发现等权重方法提供稳健且政策相关的结果；MICG能够揭示传统指标无法发现的剥夺问题，如秘鲁农村女孩在教育与心理健康方面的劣势；社区参与WASH项目与多维结果改善相关。

Conclusion: MICG提供了一个实用的、关注公平的工具，可用于监测、评估和加强支持可持续发展目标的干预措施，确保不让任何儿童掉队。

Abstract: Children's growth extends beyond height and weight. This paper introduces the
Multidimensional Index of Child Growth (MICG), developed by the IUNS Task Force
"Towards a Multidimensional Approach for Child Growth." The IUNS-MICG applies a
capability- and rights-based framework covering 14 dimensions of child
wellbeing, including health, care, mental wellbeing, participation, autonomy,
mobility, and safety. Using data from the Young Lives Study in Ethiopia, India,
Peru, and Vietnam, we tested the framework with 29 indicators. Comparisons of
different weighting methods show that equal weights provide robust and
policy-relevant results. MICG uncovers deprivations hidden by physical measures
alone; for instance, rural girls in Peru face educational and mental wellbeing
disadvantages despite similar physical growth. Further analyses show that
community participation in WASH programs is linked to higher multidimensional
outcomes, especially for the most deprived. We also extend MICG with a Bayesian
approach to estimate children's unrealized opportunities and propose a
spiderweb growth chart for visualizing multidimensional progress. MICG offers a
practical, equity-focused tool to monitor, evaluate, and strengthen
interventions that support the Sustainable Development Goals and ensure no
child is left behind.

</details>


### [38] [Large Language Model-Derived Priors Can Improve Bayesian Survival Analyses: A Glioblastoma Application](https://arxiv.org/abs/2511.01778)
*Richard Evans,Max Felland,Susanna Evans,Lindsey Sloan*

Main category: stat.AP

TL;DR: 使用生成式AI为贝叶斯胶质母细胞瘤生存分析构建先验分布，替代传统专家意见获取方法


<details>
  <summary>Details</summary>
Motivation: 传统从放射肿瘤学家获取专家意见来构建贝叶斯先验分布的过程困难、不可靠且耗时，AI提供了一种替代方法

Method: 使用三个聊天机器人各生成两个替代先验分布，由放射肿瘤学家评估后用于敏感性分析以评估后验稳定性

Result: 生成式AI能够快速提出合理的危险比先验分布，在敏感性分析中显示出良好的后验稳定性

Conclusion: 对于癌症生存分析，生成式AI提供的先验分布是专家意见获取的首选替代方法

Abstract: This report describes an application of artificial intelligence (AI) to the
Bayesian analysis of glioblastoma survival data. It has been suggested that AI
can be used to construct prior distributions for parameters in Bayesian models
rather than using the difficult, unreliable, and time-consuming process of
eliciting expert opinion from radiation oncologists. Here, we show how
generative AI can quickly propose sensible prior distributions of the hazard
ratio comparing two glioblastoma therapies, for a standard Bayesian survival
model on real data. Three Chatbots generated two alternative priors each which
were evaluated by a radiation oncologist and then used in a sensitivity
analysis to assess posterior stability. The results suggest that, for this
cancer survival analysis, priors from generative AI are a preferred alternative
method to expert elicitation.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [39] [The Neutrality Boundary Framework: Quantifying Statistical Robustness Geometrically](https://arxiv.org/abs/2511.00982)
*Thomas F. Heston*

Main category: stat.OT

TL;DR: 提出了中立性边界框架(NBF)，这是一组几何度量指标，用于量化统计稳健性和脆弱性，通过测量与中立性边界（效应为零的流形）的归一化距离来实现。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法依赖阈值和p值，缺乏对统计稳健性的几何量化。NBF旨在提供一种无阈值、样本大小不变的稳定性度量，补充传统效应大小和p值。

Method: 推导出一般形式nb = |Δ - Δ₀| / (|Δ - Δ₀| + S)，其中S>0是归一化尺度参数。提供了领域特定实现：风险商（二元结果）、偏η²（ANOVA）和基于Fisher z的度量（相关性）。

Result: 证明了NBF的有界性和单调性，能够跨任意显著性水平和统计背景几何量化稳健性。

Conclusion: NBF提供了一种新的几何方法来量化统计稳健性，克服了传统阈值依赖脆弱性指数的限制，为统计推断提供了更全面的稳定性评估框架。

Abstract: We introduce the Neutrality Boundary Framework (NBF), a set of geometric
metrics for quantifying statistical robustness and fragility as the normalized
distance from the neutrality boundary, the manifold where the effect equals
zero. The neutrality boundary value nb in [0,1) provides a threshold-free,
sample-size invariant measure of stability that complements traditional effect
sizes and p-values. We derive the general form nb = |Delta - Delta_0| / (|Delta
- Delta_0| + S), where S>0 is a scale parameter for normalization; we prove
boundedness and monotonicity, and provide domain-specific implementations: Risk
Quotient (binary outcomes), partial eta^2 (ANOVA), and Fisher z-based measures
(correlation). Unlike threshold-dependent fragility indices, NBF quantifies
robustness geometrically across arbitrary significance levels and statistical
contexts.

</details>


### [40] [From Path Coefficients to Targeted Estimands: A Comparison of Structural Equation Models (SEM) and Targeted Maximum Likelihood Estimation (TMLE)](https://arxiv.org/abs/2511.01040)
*Junjie Ma,Xiaoya Zhang,Guangye He,Yuting Han,Ting Ge,Feng Ji*

Main category: stat.OT

TL;DR: 本文比较了结构方程模型(SEM)和目标最大似然估计(TMLE)在因果推断中的表现，发现TMLE在模型误设情况下比SEM更稳健。


<details>
  <summary>Details</summary>
Motivation: 超越SEM的参数假设限制，探索基于机器学习的双稳健方法TMLE在非参数SEM框架下的应用。

Method: 通过模拟研究在正确和误设模型条件下比较SEM和TMLE，并应用于真实数据集分析贫困对高中入学的中介效应。

Result: TMLE在模型误设情况下在偏差、均方误差和置信区间有效性方面始终优于SEM；真实数据分析显示TMLE下直接效应不显著而SEM显示显著。

Conclusion: 提供了在因果推断中结合SEM和TMLE的实用指导，强调了TMLE对模型误设的稳健性优势。

Abstract: Structural Equation Modeling (SEM) has gained popularity in the social
sciences and causal inference due to its flexibility in modeling complex
relationships between variables and its availability in modern statistical
software. To move beyond the parametric assumptions of SEM, this paper reviews
targeted maximum likelihood estimation (TMLE), a doubly robust, machine
learning-based approach that builds on nonparametric SEM. We demonstrate that
both TMLE and SEM can be used to estimate standard causal effects and show that
TMLE is robust to model misspecification. We conducted simulation studies under
both correct and misspecified model conditions, implementing SEM and TMLE to
estimate these causal effects. The simulations confirm that TMLE consistently
outperforms SEM under misspecification in terms of bias, mean squared error,
and the validity of confidence intervals. We applied both approaches to a
real-world dataset to analyze the mediation effects of poverty on access to
high school, revealing that the direct effect is no longer significant under
TMLE, whereas SEM indicates significance. We conclude with practical guidance
on using SEM and TMLE in light of recent developments in targeted learning for
causal inference.

</details>
