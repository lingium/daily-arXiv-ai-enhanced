{"id": "2511.04873", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04873", "abs": "https://arxiv.org/abs/2511.04873", "authors": ["Jordan Eckert", "Elvan Ceyhan", "Henry Schenck"], "title": "Prototype Selection Using Topological Data Analysis", "comment": "Code is found on www.github.com/JordanEckert", "summary": "Recently, there has been an explosion in statistical learning literature to\nrepresent data using topological principles to capture structure and\nrelationships. We propose a topological data analysis (TDA)-based framework,\nnamed Topological Prototype Selector (TPS), for selecting representative\nsubsets (prototypes) from large datasets. We demonstrate the effectiveness of\nTPS on simulated data under different data intrinsic characteristics, and\ncompare TPS against other currently used prototype selection methods in real\ndata settings. In all simulated and real data settings, TPS significantly\npreserves or improves classification performance while substantially reducing\ndata size. These contributions advance both algorithmic and geometric aspects\nof prototype learning and offer practical tools for parallelized,\ninterpretable, and efficient classification."}
{"id": "2511.05050", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.05050", "abs": "https://arxiv.org/abs/2511.05050", "authors": ["Masahiro Tanaka"], "title": "Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning", "comment": "Accepted for publication in Proceedings of the 2025 International\n  Conference on Data Science and Intelligent Systems (DSIS 2025)", "summary": "In this study, a scalable online kernel learning framework is proposed for\nestimating bidirectional causal effects in systems characterized by mutual\ndependence and heteroskedasticity. Traditional causal inference often focuses\non unidirectional effects, overlooking the common bidirectional relationships\nin real-world phenomena. Building on heteroskedasticity-based identification,\nthe proposed method integrates a quasi-maximum likelihood estimator for\nsimultaneous equation models with large scale online kernel learning. It\nemploys random Fourier feature approximations to flexibly model nonlinear\nconditional means and variances, while an adaptive online gradient descent\nalgorithm ensures computational efficiency for streaming and high-dimensional\ndata. Results from extensive simulations demonstrate that the proposed method\nachieves superior accuracy and stability than single equation and polynomial\napproximation baselines, exhibiting lower bias and root mean squared error\nacross various data-generating processes. These results confirm that the\nproposed approach effectively captures complex bidirectional causal effects\nwith near-linear computational scaling. By combining econometric identification\nwith modern machine learning techniques, the proposed framework offers a\npractical, scalable, and theoretically grounded solution for large scale causal\ninference in natural/social science, policy making, business, and industrial\napplications."}
{"id": "2511.05159", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05159", "abs": "https://arxiv.org/abs/2511.05159", "authors": ["Shubhayan Pan", "Saptarshi Chakraborty", "Debolina Paul", "Kushal Bose", "Swagatam Das"], "title": "A New Framework for Convex Clustering in Kernel Spaces: Finite Sample Bounds, Consistency and Performance Insights", "comment": null, "summary": "Convex clustering is a well-regarded clustering method, resembling the\nsimilar centroid-based approach of Lloyd's $k$-means, without requiring a\npredefined cluster count. It starts with each data point as its centroid and\niteratively merges them. Despite its advantages, this method can fail when\ndealing with data exhibiting linearly non-separable or non-convex structures.\nTo mitigate the limitations, we propose a kernelized extension of the convex\nclustering method. This approach projects the data points into a Reproducing\nKernel Hilbert Space (RKHS) using a feature map, enabling convex clustering in\nthis transformed space. This kernelization not only allows for better handling\nof complex data distributions but also produces an embedding in a\nfinite-dimensional vector space. We provide a comprehensive theoretical\nunderpinnings for our kernelized approach, proving algorithmic convergence and\nestablishing finite sample bounds for our estimates. The effectiveness of our\nmethod is demonstrated through extensive experiments on both synthetic and\nreal-world datasets, showing superior performance compared to state-of-the-art\nclustering techniques. This work marks a significant advancement in the field,\noffering an effective solution for clustering in non-linear and non-convex data\nscenarios."}
{"id": "2511.05452", "categories": ["stat.ML", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.05452", "abs": "https://arxiv.org/abs/2511.05452", "authors": ["Wenqian Chen", "Amanda Howard", "Panos Stinis"], "title": "Self-adaptive weighting and sampling for physics-informed neural networks", "comment": "11 figures", "summary": "Physics-informed deep learning has emerged as a promising framework for\nsolving partial differential equations (PDEs). Nevertheless, training these\nmodels on complex problems remains challenging, often leading to limited\naccuracy and efficiency. In this work, we introduce a hybrid adaptive sampling\nand weighting method to enhance the performance of physics-informed neural\nnetworks (PINNs). The adaptive sampling component identifies training points in\nregions where the solution exhibits rapid variation, while the adaptive\nweighting component balances the convergence rate across training points.\nNumerical experiments show that applying only adaptive sampling or only\nadaptive weighting is insufficient to consistently achieve accurate\npredictions, particularly when training points are scarce. Since each method\nemphasizes different aspects of the solution, their effectiveness is problem\ndependent. By combining both strategies, the proposed framework consistently\nimproves prediction accuracy and training efficiency, offering a more robust\napproach for solving PDEs with PINNs."}
{"id": "2511.04785", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04785", "abs": "https://arxiv.org/abs/2511.04785", "authors": ["Luca Danese", "Riccardo Corradin", "Andrea Ongaro"], "title": "BayesChange: an R package for Bayesian Change Point Analysis", "comment": null, "summary": "We introduce BayesChange, a computationally efficient R package, built on\nC++, for Bayesian change point detection and clustering of observations sharing\ncommon change points. While many R packages exist for change point analysis,\nBayesChange offers methods not currently available elsewhere. The core\nfunctions are implemented in C++ to ensures computational efficiency, while an\nR user interface simplifies the package usage. The BayesChange package includes\ntwo R wrappers that integrate the C++ backend functions, along with S3 methods\nfor summarizing the results. We present the theory beyond each method, the\nalgorithms for posterior simulation and we illustrate the package's usage\nthrough synthetic examples."}
{"id": "2511.04974", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.04974", "abs": "https://arxiv.org/abs/2511.04974", "authors": ["Isaías Bañales", "Tomoaki Nishikawa", "Yoshihiro Ito", "Manuel J. Aguilar-Velázquez"], "title": "Estimating Inhomogeneous Spatio-Temporal Background Intensity Functions using Graphical Dirichlet Processes", "comment": null, "summary": "An enhancement in seismic measuring instrumentation has been proven to have\nimplications in the quantity of observed earthquakes, since denser networks\nusually allow recording more events. However, phenomena such as strong\nearthquakes or even aseismic transients, as slow slip earthquakes, may alter\nthe occurrence of earthquakes. In the field of seismology, it is a standard\npractice to model background seismicity as a Poisson process. Based on this\nidea, this work proposes a model that can incorporate the evolving spatial\nintensity of Poisson processes over time (i.e., we include temporal changes in\nthe background seismicity when modeling). In recent years, novel methodologies\nhave been developed for quantifying the uncertainty in the estimation of the\nbackground seismicity in homogeneous cases using Bayesian non-parametric\ntechniques. This work proposes a novel methodology based on graphical Dirichlet\nprocesses for incorporating spatial and temporal inhomogeneities in background\nseismicity. The proposed model in this work is applied to study the seismicity\nin the southern Mexico, using recorded data from 2000 to 2015."}
{"id": "2511.04721", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.04721", "abs": "https://arxiv.org/abs/2511.04721", "authors": ["Malte C. Tichy"], "title": "The Kaplan-Meier Estimator as a Sum over Units", "comment": "10 pages, 5 figures", "summary": "A sum-wise formulation is proposed for the Kaplan-Meier product limit\nestimator of partially right-censored survival data. The derived representation\npermits to write the population's estimator as a sum over its individual units'\nsemi-empirical estimators. This intuitive decomposition is applied to visualize\nthe different contributions of failed and censored units to the overall\npopulation estimator."}
{"id": "2511.04833", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04833", "abs": "https://arxiv.org/abs/2511.04833", "authors": ["Krystyna Grzesiak", "Christophe Muller", "Julie Josse", "Jeffrey Näf"], "title": "Do we Need Dozens of Methods for Real World Missing Value Imputation?", "comment": null, "summary": "Missing values pose a persistent challenge in modern data science.\nConsequently, there is an ever-growing number of publications introducing new\nimputation methods in various fields. While many studies compare imputation\napproaches, they often focus on a limited subset of algorithms and evaluate\nperformance primarily through pointwise metrics such as RMSE, which are not\nsuitable to measure the preservation of the true data distribution. In this\nwork, we provide a systematic benchmarking method based on the idea of treating\nimputation as a distributional prediction task. We consider a large number of\nalgorithms and, for the first time, evaluate them not only on synthetic missing\nmechanisms, but also on real-world missingness scenarios, using the concept of\nImputation Scores. Finally, while the focus of previous benchmark has often\nbeen on numerical data, we also consider mixed data sets in our study. The\nanalysis overwhelmingly confirms the superiority of iterative imputation\nalgorithms, especially the methods implemented in the mice R package."}
{"id": "2511.05071", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.05071", "abs": "https://arxiv.org/abs/2511.05071", "authors": ["Tommaso Proietti", "Alessandro Giovannelli"], "title": "On the Estimation of Climate Normals and Anomalies", "comment": null, "summary": "The quantification of the interannual component of variability in\nclimatological time series is essential for the assessment and prediction of\nthe El Ni\\~{n}o - Southern Oscillation phenomenon. This is achieved by\nestimating the deviation of a climate variable (e.g., temperature, pressure,\nprecipitation, or wind strength) from its normal conditions, defined by its\nbaseline level and seasonal patterns. Climate normals are currently estimated\nby simple arithmetic averages calculated over the most recent 30-year period\nending in a year divisible by 10. The suitability of the standard methodology\nhas been questioned in the context of a changing climate, characterized by\nnonstationary conditions. The literature has focused on the choice of the\nbandwidth and the ability to account for trends induced by climate change. The\npaper contributes to the literature by proposing a regularized real time filter\nbased on local trigonometric regression, optimizing the estimation\nbias-variance trade-off in the presence of climate change, and by introducing a\nclass of seasonal kernels enhancing the localization of the estimates of\nclimate normals. Application to sea surface temperature series in the \\nino 3.4\nregion and zonal and trade winds strength in the equatorial and tropical\nPacific region, illustrates the relevance of our proposal."}
{"id": "2511.04816", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.04816", "abs": "https://arxiv.org/abs/2511.04816", "authors": ["Yinjun Zhao", "Yuanjia Wang", "Ying LIu"], "title": "An Integrative Approach for Subtyping Mental Disorders Using Multimodal Data", "comment": null, "summary": "Understanding the biological and behavioral heterogeneity underlying\npsychiatric disorders is critical for advancing precision diagnosis, treatment,\nand prevention. This paper addresses the scientific question of how multimodal\ndata, spanning clinical, cognitive, and neuroimaging measures, can be\nintegrated to identify biologically meaningful subtypes of mental disorders. We\nintroduce Mixed INtegrative Data Subtyping (MINDS), a Bayesian hierarchical\nmodel designed to jointly analyze mixed-type data for simultaneous dimension\nreduction and clustering. Using data from the Adolescent Brain Cognitive\nDevelopment (ABCD) Study, MINDS integrates clinical symptoms, cognitive\nperformance, and brain structure measures to subtype\nAttention-Deficit/Hyperactivity Disorder (ADHD) and Obsessive-Compulsive\nDisorder (OCD). Our method leverages Polya-Gamma augmentation for computational\nefficiency and robust inference. Simulations demonstrate improved stability and\naccuracy compared to existing clustering approaches. Application to the ABCD\ndata reveals clinically interpretable subtypes of ADHD and OCD with distinct\ncognitive and neurodevelopmental profiles. These findings show how integrative\nmultimodal modeling can enhance the reproducibility and clinical relevance of\npsychiatric subtyping, supporting data-driven policies for early identification\nand targeted interventions in mental health."}
{"id": "2511.04975", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04975", "abs": "https://arxiv.org/abs/2511.04975", "authors": ["Abylay Zhumekenov", "Alexandros Beskos", "Dan Crisan", "Matthew Graham", "Ajay Jasra", "Nikolas Kantas"], "title": "Sequential Markov chain Monte Carlo for Filtering of State-Space Models with Low or Degenerate Observation Noise", "comment": "21 pages, 11 figures", "summary": "We consider the discrete-time filtering problem in scenarios where the\nobservation noise is degenerate or low. More precisely, one is given access to\na discrete time observation sequence which at any time $k$ depends only on the\nstate of an unobserved Markov chain. We specifically assume that the functional\nrelationship between observations and hidden Markov chain has either degenerate\nor low noise. In this article, under suitable assumptions, we derive the\nfiltering density and its recursions for this class of problems on a specific\nsequence of manifolds defined through the observation function. We then design\nsequential Markov chain Monte Carlo methods to approximate the filter serially\nin time. For a certain linear observation model, we show that using sequential\nMarkov chain Monte Carlo for low noise will converge as the noise disappears to\nthat of using sequential Markov chain Monte Carlo for degenerate noise. We\nillustrate the performance of our methodology on several challenging stochastic\nmodels deriving from Statistics and Applied Mathematics."}
{"id": "2511.05126", "categories": ["stat.AP", "62M10, 62H11, 62P05"], "pdf": "https://arxiv.org/pdf/2511.05126", "abs": "https://arxiv.org/abs/2511.05126", "authors": ["Ariane Nidelle Meli Chrisko", "Philipp Otto", "Wolfgang Schmid"], "title": "Exponential Spatiotemporal GARCH Model with Asymmetric Volatility Spillovers", "comment": "Submitted to the Journal of Econometrics and Statistics", "summary": "This paper introduces a spatiotemporal exponential generalised autoregressive\nconditional heteroscedasticity (spatiotemporal E-GARCH) model, extending\ntraditional spatiotemporal GARCH models by incorporating asymmetric volatility\nspillovers, while also generalising the time-series E-GARCH model to a\nspatiotemporal setting with instantaneous, potentially asymmetric volatility\nspillovers across space. The model allows for both temporal and spatial\ndependencies in volatility dynamics, capturing how financial shocks propagate\nacross time, space, and network structures. We establish the theoretical\nproperties of the model, deriving stationarity conditions and moment existence\nresults. For estimation, we propose a quasi-maximum likelihood (QML) estimator\nand assess its finite-sample performance through Monte Carlo simulations.\nEmpirically, we apply the model to financial networks, specifically analysing\nvolatility spillovers in stock markets. We compare different network structures\nand analyse asymmetric effects in instantaneous volatility interactions."}
{"id": "2511.04852", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04852", "abs": "https://arxiv.org/abs/2511.04852", "authors": ["Erjia Cui", "Angela Zhao", "Ciprian M. Crainiceanu"], "title": "Inference for the Extended Functional Cox Model: A UK Biobank Case Study", "comment": "33 pages, 4 figures, 1 table", "summary": "Multiple studies have shown that scalar summaries of objectively measured\nphysical activity (PA) using accelerometers are the strongest predictors of\nmortality, outperforming all traditional risk factors, including age, sex, body\nmass index (BMI), and smoking. Here we show that diurnal patterns of PA and\ntheir day-to-day variability provide additional information about mortality. To\ndo that, we introduce a class of extended functional Cox models and\ncorresponding inferential tools designed to quantify the association between\nmultiple functional and scalar predictors with time-to-event outcomes in\nlarge-scale (large $n$) high-dimensional (large $p$) datasets. Methods are\napplied to the UK Biobank study, which collected PA at every minute of the day\nfor up to seven days, as well as time to mortality ($93{,}370$ participants\nwith good quality accelerometry data and $931$ events). Simulation studies show\nthat methods perform well in realistic scenarios and scale up to studies an\norder of magnitude larger than the UK Biobank accelerometry study. Establishing\nthe feasibility and scalability of these methods for such complex and large\ndata sets is a major milestone in applied Functional Data Analysis (FDA)."}
{"id": "2511.04852", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04852", "abs": "https://arxiv.org/abs/2511.04852", "authors": ["Erjia Cui", "Angela Zhao", "Ciprian M. Crainiceanu"], "title": "Inference for the Extended Functional Cox Model: A UK Biobank Case Study", "comment": "33 pages, 4 figures, 1 table", "summary": "Multiple studies have shown that scalar summaries of objectively measured\nphysical activity (PA) using accelerometers are the strongest predictors of\nmortality, outperforming all traditional risk factors, including age, sex, body\nmass index (BMI), and smoking. Here we show that diurnal patterns of PA and\ntheir day-to-day variability provide additional information about mortality. To\ndo that, we introduce a class of extended functional Cox models and\ncorresponding inferential tools designed to quantify the association between\nmultiple functional and scalar predictors with time-to-event outcomes in\nlarge-scale (large $n$) high-dimensional (large $p$) datasets. Methods are\napplied to the UK Biobank study, which collected PA at every minute of the day\nfor up to seven days, as well as time to mortality ($93{,}370$ participants\nwith good quality accelerometry data and $931$ events). Simulation studies show\nthat methods perform well in realistic scenarios and scale up to studies an\norder of magnitude larger than the UK Biobank accelerometry study. Establishing\nthe feasibility and scalability of these methods for such complex and large\ndata sets is a major milestone in applied Functional Data Analysis (FDA)."}
{"id": "2511.04852", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04852", "abs": "https://arxiv.org/abs/2511.04852", "authors": ["Erjia Cui", "Angela Zhao", "Ciprian M. Crainiceanu"], "title": "Inference for the Extended Functional Cox Model: A UK Biobank Case Study", "comment": "33 pages, 4 figures, 1 table", "summary": "Multiple studies have shown that scalar summaries of objectively measured\nphysical activity (PA) using accelerometers are the strongest predictors of\nmortality, outperforming all traditional risk factors, including age, sex, body\nmass index (BMI), and smoking. Here we show that diurnal patterns of PA and\ntheir day-to-day variability provide additional information about mortality. To\ndo that, we introduce a class of extended functional Cox models and\ncorresponding inferential tools designed to quantify the association between\nmultiple functional and scalar predictors with time-to-event outcomes in\nlarge-scale (large $n$) high-dimensional (large $p$) datasets. Methods are\napplied to the UK Biobank study, which collected PA at every minute of the day\nfor up to seven days, as well as time to mortality ($93{,}370$ participants\nwith good quality accelerometry data and $931$ events). Simulation studies show\nthat methods perform well in realistic scenarios and scale up to studies an\norder of magnitude larger than the UK Biobank accelerometry study. Establishing\nthe feasibility and scalability of these methods for such complex and large\ndata sets is a major milestone in applied Functional Data Analysis (FDA)."}
{"id": "2511.04859", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04859", "abs": "https://arxiv.org/abs/2511.04859", "authors": ["Yik Lun Kei", "Oscar Hernan Madrid Padilla", "Rebecca Killick", "James Wilson", "Xi Chen", "Robert Lund"], "title": "Clustering in Networks with Time-varying Nodal Attributes", "comment": null, "summary": "This manuscript studies nodal clustering in graphs having a time series at\neach node. The framework includes priors for low-dimensional representations\nand a decoder that bridges the latent representations and time series. The\nstructural and temporal patterns are fused into representations that facilitate\nclustering, addressing the limitation that the evolution of nodal attributes is\noften overlooked. Parameters are learned via maximum approximate likelihood,\nwith a graph-fused LASSO regularization imposed on prior parameters. The\noptimization problem is solved via alternating direction method of multipliers;\nLangevin dynamics are employed for posterior inference. Simulation studies on\nblock and grid graphs with autoregressive dynamics, and applications to\nCalifornia county temperatures and a book word co-occurrence network\ndemonstrate the effectiveness of the proposed method."}
{"id": "2511.04859", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04859", "abs": "https://arxiv.org/abs/2511.04859", "authors": ["Yik Lun Kei", "Oscar Hernan Madrid Padilla", "Rebecca Killick", "James Wilson", "Xi Chen", "Robert Lund"], "title": "Clustering in Networks with Time-varying Nodal Attributes", "comment": null, "summary": "This manuscript studies nodal clustering in graphs having a time series at\neach node. The framework includes priors for low-dimensional representations\nand a decoder that bridges the latent representations and time series. The\nstructural and temporal patterns are fused into representations that facilitate\nclustering, addressing the limitation that the evolution of nodal attributes is\noften overlooked. Parameters are learned via maximum approximate likelihood,\nwith a graph-fused LASSO regularization imposed on prior parameters. The\noptimization problem is solved via alternating direction method of multipliers;\nLangevin dynamics are employed for posterior inference. Simulation studies on\nblock and grid graphs with autoregressive dynamics, and applications to\nCalifornia county temperatures and a book word co-occurrence network\ndemonstrate the effectiveness of the proposed method."}
{"id": "2511.05487", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.05487", "abs": "https://arxiv.org/abs/2511.05487", "authors": ["Lily Koffman", "Sunan Gao", "Xinkai Zhou", "Andrew Leroux", "Ciprian Crainiceanu", "John Muschelli III"], "title": "Function on Scalar Regression with Complex Survey Designs", "comment": null, "summary": "Large health surveys increasingly collect high-dimensional functional data\nfrom wearable devices, and function on scalar regression (FoSR) is often used\nto quantify the relationship between these functional outcomes and scalar\ncovariates such as age and sex. However, existing methods for FoSR fail to\naccount for complex survey design. We introduce inferential methods for FoSR\nfor studies with complex survey designs. The method combines fast univariate\ninference (FUI) developed for functional data outcomes and survey sampling\ninferential methods developed for scalar outcomes. Our approach consists of\nthree steps: (1) fit survey weighted GLMs at each point along the functional\ndomain, (2) smooth coefficients along the functional domain, and (3) use\nbalanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap\nto obtain pointwise and joint confidence bands for the functional coefficients.\nThe method is motivated by association studies between continuous physical\nactivity data and covariates collected in the National Health and Nutrition\nExamination Survey (NHANES). A first-of-its-kind analytical simulation study\nand empirical simulation using the NHANES data demonstrates that our method\nperforms better than existing methods that do not account for the survey\nstructure. Finally, application of the method in NHANES shows the practical\nimplications of accounting for survey structure. The method is implemented in\nthe R package svyfosr."}
{"id": "2511.05004", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.05004", "abs": "https://arxiv.org/abs/2511.05004", "authors": ["Siu-Ming Tam", "Min Wang", "Alicia Rambaldi", "Dehua Tao"], "title": "On linkage bias-correction for estimators using iterated bootstraps", "comment": "31 pages, 4 tables", "summary": "By amalgamating data from disparate sources, the resulting integrated dataset\nbecomes a valuable resource for statistical analysis. In probabilistic record\nlinkage, the effectiveness of such integration relies on the availability of\nlinkage variables free from errors. Where this is lacking, the linked data set\nwould suffer from linkage errors and the resultant analyses, linkage bias. This\npaper proposes a methodology leveraging the bootstrap technique to devise\nlinkage bias-corrected estimators. Additionally, it introduces a test to assess\nwhether increasing the number of bootstrap iterations meaningfully reduces\nlinkage bias or merely inflates variance without further improving accuracy. An\napplication of these methodologies is demonstrated through the analysis of a\nsimulated dataset featuring hormone information, along with a dataset obtained\nfrom linking two data sets from the Australian Bureau of Statistics' labour\nmobility surveys."}
{"id": "2511.05281", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.05281", "abs": "https://arxiv.org/abs/2511.05281", "authors": ["Ritwik Bhaduri", "Aabesh Bhattacharyya", "Rina Foygel Barber", "Lucas Janson"], "title": "Conditioning on posterior samples for flexible frequentist goodness-of-fit testing", "comment": null, "summary": "Tests of goodness of fit are used in nearly every domain where statistics is\napplied. One powerful and flexible approach is to sample artificial data sets\nthat are exchangeable with the real data under the null hypothesis (but not\nunder the alternative), as this allows the analyst to conduct a valid test\nusing any test statistic they desire. Such sampling is typically done by\nconditioning on either an exact or approximate sufficient statistic, but\nexisting methods for doing so have significant limitations, which either\npreclude their use or substantially reduce their power or computational\ntractability for many important models. In this paper, we propose to condition\non samples from a Bayesian posterior distribution, which constitute a very\ndifferent type of approximate sufficient statistic than those considered in\nprior work. Our approach, approximately co-sufficient sampling via Bayes\n(aCSS-B), considerably expands the scope of this flexible type of\ngoodness-of-fit testing. We prove the approximate validity of the resulting\ntest, and demonstrate its utility on three common null models where no existing\nmethods apply, as well as its outperformance on models where existing methods\ndo apply."}
{"id": "2511.05487", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.05487", "abs": "https://arxiv.org/abs/2511.05487", "authors": ["Lily Koffman", "Sunan Gao", "Xinkai Zhou", "Andrew Leroux", "Ciprian Crainiceanu", "John Muschelli III"], "title": "Function on Scalar Regression with Complex Survey Designs", "comment": null, "summary": "Large health surveys increasingly collect high-dimensional functional data\nfrom wearable devices, and function on scalar regression (FoSR) is often used\nto quantify the relationship between these functional outcomes and scalar\ncovariates such as age and sex. However, existing methods for FoSR fail to\naccount for complex survey design. We introduce inferential methods for FoSR\nfor studies with complex survey designs. The method combines fast univariate\ninference (FUI) developed for functional data outcomes and survey sampling\ninferential methods developed for scalar outcomes. Our approach consists of\nthree steps: (1) fit survey weighted GLMs at each point along the functional\ndomain, (2) smooth coefficients along the functional domain, and (3) use\nbalanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap\nto obtain pointwise and joint confidence bands for the functional coefficients.\nThe method is motivated by association studies between continuous physical\nactivity data and covariates collected in the National Health and Nutrition\nExamination Survey (NHANES). A first-of-its-kind analytical simulation study\nand empirical simulation using the NHANES data demonstrates that our method\nperforms better than existing methods that do not account for the survey\nstructure. Finally, application of the method in NHANES shows the practical\nimplications of accounting for survey structure. The method is implemented in\nthe R package svyfosr."}
{"id": "2511.05050", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.05050", "abs": "https://arxiv.org/abs/2511.05050", "authors": ["Masahiro Tanaka"], "title": "Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning", "comment": "Accepted for publication in Proceedings of the 2025 International\n  Conference on Data Science and Intelligent Systems (DSIS 2025)", "summary": "In this study, a scalable online kernel learning framework is proposed for\nestimating bidirectional causal effects in systems characterized by mutual\ndependence and heteroskedasticity. Traditional causal inference often focuses\non unidirectional effects, overlooking the common bidirectional relationships\nin real-world phenomena. Building on heteroskedasticity-based identification,\nthe proposed method integrates a quasi-maximum likelihood estimator for\nsimultaneous equation models with large scale online kernel learning. It\nemploys random Fourier feature approximations to flexibly model nonlinear\nconditional means and variances, while an adaptive online gradient descent\nalgorithm ensures computational efficiency for streaming and high-dimensional\ndata. Results from extensive simulations demonstrate that the proposed method\nachieves superior accuracy and stability than single equation and polynomial\napproximation baselines, exhibiting lower bias and root mean squared error\nacross various data-generating processes. These results confirm that the\nproposed approach effectively captures complex bidirectional causal effects\nwith near-linear computational scaling. By combining econometric identification\nwith modern machine learning techniques, the proposed framework offers a\npractical, scalable, and theoretically grounded solution for large scale causal\ninference in natural/social science, policy making, business, and industrial\napplications."}
