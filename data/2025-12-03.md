<div id=toc></div>

# Table of Contents

- [stat.OT](#stat.OT) [Total: 1]
- [stat.AP](#stat.AP) [Total: 11]
- [stat.ML](#stat.ML) [Total: 18]
- [stat.ME](#stat.ME) [Total: 25]
- [stat.CO](#stat.CO) [Total: 3]


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [1] [Incorporating Missingness in a Framework for Generating Realistic Synthetic Randomized Controlled Trial Data](https://arxiv.org/abs/2512.00183)
*Niki Z. Petrakos,Erica E. M. Moodie,Nicolas Savy*

Main category: stat.OT

TL;DR: 提出并比较了多种处理缺失数据的合成数据生成框架，发现在MAR设置下，考虑缺失机制的模型始终优于完整案例分析


<details>
  <summary>Details</summary>
Motivation: 当前关于复杂、现实的合成表格数据生成的研究，特别是针对随机对照试验(RCT)的数据，常常忽略缺失数据问题。然而，缺失数据在RCT数据中很常见，且通常不是完全随机缺失。本文旨在填补这一空白，探索如何在生成现实合成数据的同时考虑缺失机制。

Method: 提出并实证比较了多种数据生成框架，采用不同的缺失数据处理策略（完整案例分析、逆概率加权和多重插补），通过一系列指标量化生成性能。重点关注"随机缺失"（MAR）设置。

Result: 研究发现，在MAR设置下，纳入额外模型来考虑缺失机制的框架始终优于完整案例分析的方法。

Conclusion: 生成合成数据时需要考虑缺失机制，特别是在随机对照试验等实际应用中，采用考虑缺失机制的模型能生成更现实、更高质量的合成数据。

Abstract: The current literature regarding generation of complex, realistic synthetic tabular data, particularly for randomized controlled trials (RCTs), often ignores missing data. However, missing data are common in RCT data and often are not Missing Completely At Random. We bridge the gap of determining how best to generate realistic synthetic data while also accounting for the missingness mechanism. We demonstrate how to generate synthetic missing values while ensuring that synthetic data mimic the targeted real data distribution. We propose and empirically compare several data generation frameworks utilizing various strategies for handling missing data (complete case, inverse probability weighting, and multiple imputation) by quantifying generation performance through a range of metrics. Focusing on the Missing At Random setting, we find that incorporating additional models to account for the missingness always outperformed a complete case approach.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [2] [Beyond Expected Goals: A Probabilistic Framework for Shot Occurrences in Soccer](https://arxiv.org/abs/2512.00203)
*Jonathan Pipping,Tianshu Feng,R. Paul Sabin*

Main category: stat.AP

TL;DR: xG+框架在射门前预测射门概率和预期进球值，解决传统xG模型只分析已发生射门的局限性


<details>
  <summary>Details</summary>
Motivation: 传统预期进球(xG)模型仅分析已发生的射门，存在"射门条件限制"问题，无法评估射门前的情况和射门决策质量

Method: 提出xG+框架：1) 预测下一秒内发生射门的概率；2) 预测如果射门发生的预期进球值；3) 在整个控球过程中聚合这些联合概率估计

Result: xG+在球队层面提高了预测准确性，相比传统xG模型产生了更持久的球员技能信号

Conclusion: 通过联合建模射门行为和射门质量，xG+解决了传统xG模型的局限性，提供了更全面的进攻表现评估框架

Abstract: Expected goals (xG) models estimate the probability that a shot results in a goal from its context (e.g., location, pressure), but they operate only on observed shots. We propose xG+, a possession-level framework that first estimates the probability that a shot occurs within the next second and its corresponding xG if it were to occur. We also introduce ways to aggregate this joint probability estimate over the course of a possession. By jointly modeling shot-taking behavior and shot quality, xG+ remedies the conditioning-on-shots limitation of standard xG. We show that this improves predictive accuracy at the team level and produces a more persistent player skill signal than standard xG models.

</details>


### [3] [Bayesian Analysis of Hotel Booking Cancellations: A Hierarchical Modeling Approach](https://arxiv.org/abs/2512.00240)
*Yingdong Yang*

Main category: stat.AP

TL;DR: 使用贝叶斯方法分析酒店预订取消因素，发现提前预订时间增加取消概率，特殊需求和停车需求降低取消风险，且这些效应在城市酒店和度假酒店中存在差异。


<details>
  <summary>Details</summary>
Motivation: 酒店预订取消对收益管理有重要影响，需要量化不同预订特征对取消概率的影响，并考虑酒店类型的调节作用，为收益管理提供数据驱动的决策支持。

Method: 使用PyMC进行贝叶斯分析，比较三种复杂度递增的模型：基础模型、主效应模型和完整交互模型。采用MCMC采样（NUTS算法）分析5,000条预订记录，通过WAIC进行模型比较。

Result: 提前预订时间显著增加取消概率（后验均值：0.600，95% HDI：[0.532, 0.661]），特殊需求（后验均值：-0.642）和停车需求（后验均值：-3.879）显著降低取消风险。完整交互模型预测性能最佳，表明预订特征效应在不同酒店类型中存在系统性差异。

Conclusion: 贝叶斯方法提供了完整的不确定性量化，揭示了酒店预订取消的关键驱动因素及其在不同酒店类型中的差异，为收益管理提供了可操作的见解。

Abstract: This study presents a comprehensive Bayesian analysis of hotel booking cancellations using PyMC, comparing three model specifications of increasing complexity. We investigate how lead time, special requests, and parking requirements affect cancellation probability, and explore interaction effects with hotel type. Using MCMC sampling (NUTS algorithm) on 5,000 booking records, we find strong evidence that longer lead times increase cancellation probability (posterior mean: 0.600, 95\% HDI: [0.532, 0.661]), while special requests (posterior mean: -0.642) and parking (posterior mean: -3.879) significantly reduce cancellation risk. Model comparison via WAIC reveals that the full interaction model provides the best predictive performance, suggesting that the effects of booking characteristics vary systematically between city and resort hotels. This Bayesian approach enables full uncertainty quantification and provides actionable insights for revenue management.

</details>


### [4] [Robust Wrapped Gaussian Process Inference for Noisy Angular Data](https://arxiv.org/abs/2512.00277)
*Andrew Cooper,Justin Strait,Mary Frances Dorn,Robert B. Gramacy,Brendon Parsons,Alessandro Cattaneo*

Main category: stat.AP

TL;DR: 提出一种新颖的单调包裹高斯过程模型，用于处理单向包裹的角度数据，通过估计包裹发生位置并分割输入空间来提高推断准确性。


<details>
  <summary>Details</summary>
Motivation: 角度数据在方向或定向场景中常见，现有包裹方法在噪声响应设置中推断困难，因为潜在未包裹分布和包裹行为的错误设定会相互影响。无线电波分析和生物医学工程等应用中的径向数据通常只发生单向包裹。

Method: 提出单调包裹高斯过程模型，通过估计包裹发生位置并相应分割输入空间。采用更稳健的Student's t响应似然，利用椭圆切片采样算法从潜在高斯过程空间进行无拒绝采样。

Result: 在模拟示例中展示了优于现有包裹高斯过程方法的性能，并将该方法应用于射频识别标签定位问题，通过建模频率与相位角的关系来推断标签与天线的距离。

Conclusion: 提出的单调包裹高斯过程模型能够更准确地处理单向包裹的角度数据，在噪声环境中提供更稳健的推断，在射频识别定位等实际应用中表现出良好性能。

Abstract: Angular data are commonly encountered in settings with a directional or orientational component. Regressing an angular response on real-valued features requires intrinsically capturing the circular or spherical manifold the data lie on, or using an appropriate extrinsic transformation. A popular example of the latter is the technique of distributional wrapping, in which functions are "wrapped" around the unit circle via a modulo-$2π$ transformation. This approach enables flexible, non-linear models like Gaussian processes (GPs) to properly account for circular structure. While straightforward in concept, the need to infer the latent unwrapped distribution along with its wrapping behavior makes inference difficult in noisy response settings, as misspecification of one can severely hinder estimation of the other. However, applications such as radiowave analysis (Shangguan et al., 2015) and biomedical engineering (Kurz and Hanebeck, 2015) encounter radial data where wrapping occurs in only one direction. We therefore propose a novel wrapped GP (WGP) model formulation that recognizes monotonic wrapping behavior for more accurate inference in these situations. This is achieved by estimating the locations where wrapping occurs and partitioning the input space accordingly. We also specify a more robust Student's t response likelihood, and take advantage of an elliptical slice sampling (ESS) algorithm for rejection-free sampling from the latent GP space. We showcase our model's preferable performance on simulated examples compared to existing WGP methodologies. We then apply our method to the problem of localizing radiofrequency identification (RFID) tags, in which we model the relationship between frequency and phase angle to infer how far away an RFID tag is from an antenna.

</details>


### [5] [Kicking for Goal or Touch? An Expected Points Framework for Penalty Decisions in Rugby Union](https://arxiv.org/abs/2512.00312)
*Kenny Watts,Jonathan Pipping*

Main category: stat.AP

TL;DR: 开发了一个预期得分框架，用于量化橄榄球比赛中罚球时选择射门还是踢向边线的价值，基于场地位置和比赛情境提供数据驱动的决策建议。


<details>
  <summary>Details</summary>
Motivation: 在橄榄球比赛中，球队面临罚球时需要在射门得分和踢向边线追求达阵之间做出选择，目前缺乏基于数据的系统化决策框架来评估这些选择的相对价值。

Method: 使用2018/19赛季英超橄榄球联赛的35,199个阶段数据，结合国际比赛记录的罚球成功率角度-距离模型，构建了两个预期得分曲面：从边线球开始的进攻预期得分和射门预期得分（考虑射中与射失的比赛后果）。

Result: 通过比较两个曲面生成了决策地图，显示了在哪些场地位置选择射门或踢向边线能最大化预期回报，并分析了边界如何随比赛情境和预期触地距离变化。

Conclusion: 该研究提供了首个基于预期得分的橄榄球罚球策略综合评估方法，为球队决策提供了统一的数据驱动框架，并可针对特定球队的踢球手和边线球单元进行定制。

Abstract: Following a penalty in rugby union, teams typically choose between attempting a shot at goal or kicking to touch to pursue a try. We develop an Expected Points (EP) framework that quantifies the value of each option as a function of both field location and game context. Using phase-level data from the 2018/19 Premiership Rugby season (35,199 phases across 132 matches) and an angle-distance model of penalty kick success estimated from international records, we construct two surfaces: (i) the expected points of a possession beginning with a lineout, and (ii) the expected points of a kick at goal, taking into account the in-game consequences of made and missed kicks. We then compare these surfaces to produce decision maps that indicate where kicking for goal or kicking to touch maximizes expected return, and we analyze how the boundary shifts with game context and the expected meters gained to touch. Our results provide a unified, data-driven method for evaluating penalty decisions and can be tailored to team-specific kickers and lineout units. This study offers, to our knowledge, the first comprehensive EP-based assessment of penalty strategy in rugby union and outlines extensions to win-probability analysis and richer tracking data.

</details>


### [6] [Bayesian Statistical Inversion for High-Dimensional Computer Model Output and Spatially Distributed Counts](https://arxiv.org/abs/2512.01927)
*Steven D. Barnett,Robert B. Gramacy,Lauren J. Beesley,Dave Osthus,Yifan Huang,Fan Guo,Daniel B. Reisenfeld*

Main category: stat.AP

TL;DR: 提出一种新的贝叶斯反演方法，用于处理IBEX卫星的ENA数据，解决计算模拟慢、输出为高分辨率图像、观测为计数数据三大挑战。


<details>
  <summary>Details</summary>
Motivation: IBEX卫星收集的太阳系边界ENA数据需要与计算机模型拟合，但面临三大挑战：计算模拟速度慢、输出为高分辨率图像、观测数据为计数而非高斯场数据。

Method: 提出耦合泊松响应与稀疏高斯过程代理模型的新方法，使用Vecchia近似处理高维输出，构建贝叶斯反演框架。

Result: 通过模拟实验验证方法能有效恢复"真实"模型参数并进行准确样本外预测，优于现有替代方法。

Conclusion: 成功将新方法应用于IBEX卫星数据和洛斯阿拉莫斯国家实验室开发的计算机模型，为解决类似高维计数数据反演问题提供了有效工具。

Abstract: Data collected by the Interstellar Boundary Explorer (IBEX) satellite, recording heliospheric energetic neutral atoms (ENAs), exhibit a phenomenon that has caused space scientists to revise hypotheses about the physical processes, and computer simulations under those models, in play at the boundary of our solar system. Evaluating the fit of these computer models involves tuning their parameters to observational data from IBEX. This would be a classic (Bayesian) inverse problem if not for three challenges: (1) the computer simulations are slow, limiting the size of campaigns of runs; so (2) surrogate modeling is essential, but outputs are high-resolution images, thwarting conventional methods; and (3) IBEX observations are counts, whereas most inverse problem techniques assume Gaussian field data. To fill that gap we propose a novel approach to Bayesian inverse problems coupling a Poisson response with a sparse Gaussian process surrogate using the Vecchia approximation. We demonstrate the capabilities of our proposed framework, which compare favorably to alternatives, through multiple simulated examples in terms of recovering "true" computer model parameters and accurate out-of-sample prediction. We then apply this new technology to IBEX satellite data and associated computer models developed at Los Alamos National Laboratory.

</details>


### [7] [Model-based indicators for co-clustered environments and species communities](https://arxiv.org/abs/2512.00678)
*Braden Scherting,Otso Ovaskainen,Tomas Roslin,David B. Dunson*

Main category: stat.AP

TL;DR: 提出基于模型的框架，通过贝叶斯解耦的泊松分解从物种调查数据中推断生态亚群落及相应指标物种，实现可扩展、可重复的生物多样性监测。


<details>
  <summary>Details</summary>
Motivation: 当前生物多样性监测实践依赖任意定义的生态系统、群落和临时指标物种，限制了成本效益和可重复性，需要更科学、可扩展的方法。

Method: 使用贝叶斯解耦的泊松分解对环境和物种进行共聚类，通过潜在层次回归将可观测生境特征与每个亚群落关联，并提出基于模型的新指标物种排序方法。

Result: 该框架能够从大规模节肢动物丰度数据等物种调查数据中推断生态亚群落和相应指标物种，为生态系统分类和指标物种选择提供模型基础。

Conclusion: 该集成方法为生物多样性监测和知情保护提供了可扩展、可重复的途径，推动了基于模型的生态系统分类和指标物种选择。

Abstract: Accurate biodiversity monitoring is essential for effective environmental policy, yet current practices often rely on arbitrarily defined ecosystems, communities, and ad-hoc indicator species, limiting cost-efficiency and reproducibility. We present a model-based framework that infers ecological sub-communities and corresponding indicators in terms of habitat and species from species survey data, such as large-scale arthropod abundance data used here as example. Environments and species are co-clustered using Bayesian decoupling for Poisson factorization. Latent, hierarchical regression relates observable habitat features to each subcommunity. Additionally, we propose a novel, model-based ranking of indicator species based on the learned subcommunities, generalizing classical approaches. This integrated approach motivates model-based ecosystem classification and indicator species selection, offering a scalable, reproducible pathway for biodiversity monitoring and informed conservation.

</details>


### [8] [A Clinical Instrument to Measure Patient Anecdotes in Clinical Trials](https://arxiv.org/abs/2512.01041)
*Ian Miller,Ann Hyslop,Colin Decker*

Main category: stat.AP

TL;DR: 本文介绍了Clinical IMPACT工具，用于在神经科临床试验中捕捉非癫痫发作相关的定性改善，通过患者轶事收集和专家盲评来评估治疗对生活质量的影响。


<details>
  <summary>Details</summary>
Motivation: 传统癫痫治疗研究主要使用癫痫发作频率作为主要结局指标，这可能忽略了患者生活质量方面的有意义改善。神经科临床试验面临大脑功能多样性和难以量化的挑战，需要能够捕捉非癫痫发作相关改善的工具。

Method: 开发了Clinical IMPACT工具，包含开放式问题让参与者或照顾者识别并选择最重要的治疗益处轶事证据。由盲法专家小组对这些轶事进行排名，使用Wilcoxon秩和检验进行严格的统计分析来检测治疗效果。

Result: 该方法能够抵抗I型错误，同时全面捕捉治疗对生活质量的真实世界影响。工具具有增强敏感性的潜力，并能提供定性见解，为患者、医疗提供者和监管机构提供治疗效果的全面信息。

Conclusion: Clinical IMPACT工具应作为神经科试验的关键次要终点，为传统上难以测量和解释的非癫痫发作结局提供重要视角，即使在临床试验结果为阳性时也能提供有价值的补充信息。

Abstract: Clinical trials assessing neurological treatment are challenging due to the diversity of brain function, and the difficulty in quantifying it. Traditional treatment studies in epilepsy use seizure frequency as the primary outcome measure, which may overlooking meaningful improvements in patients' quality of life. This paper introduces the Clinical Instrument for Measuring Patient Anecdotes in Clinical Trials (Clinical IMPACT), a novel tool designed to capture qualitative non-seizure improvement across neurological domains.
  The Clinical IMPACT incorporates open-ended inquiries that allow participants or caregivers to identify and select anecdotal evidence of their most significant treatment benefits. A blinded panel of experts ranks these anecdotes, facilitating a rigorous statistical analysis using the Wilcoxon Rank-Sum Test to detect treatment efficacy. The approach is resistant to type 1 error, yet comprehensive in its ability to capture real-world effects on quality of life.
  The potential of the Clinical IMPACT tool to enhance sensitivity while also providing qualitative insights that can inform patients, healthcare providers, and regulatory bodies about treatment effects makes it important to consider in any neurological trial. We describe how it can be used in epilepsy, and advocate for its inclusion as a key secondary endpoint to provide a perspective on non-seizure outcomes, which have previously been challenging to measure, let alone to interpret, even when the clinical trial is positive.

</details>


### [9] [pvEBayes: An R Package for Empirical Bayes Methods in Pharmacovigilance](https://arxiv.org/abs/2512.01057)
*Yihao Tan,Marianthi Markatou,Saptarshi Chakraborty*

Main category: stat.AP

TL;DR: pvEBayes是一个R包，实现了用于药物警戒的非参数经验贝叶斯方法，帮助从自发报告系统中识别药物与不良事件的关联。


<details>
  <summary>Details</summary>
Motivation: 药物安全监测是药物警戒的核心，自发报告系统收集已批准医疗产品的可疑不良事件报告，是识别临床试验中未出现的潜在安全问题的关键资源。然而，目前缺乏这些非参数经验贝叶斯方法的全面且易于访问的实现。

Method: 开发了R包pvEBayes，实现了一套用于药物警戒的非参数经验贝叶斯方法，包括后处理工具和图形摘要，以简化这些方法的应用。

Result: 提供了详细示例，通过分析从FDA FAERS数据库整理的两个真实世界SRS数据集，展示了该包的应用。

Conclusion: pvEBayes包为非参数经验贝叶斯方法在药物警戒中的应用提供了全面且易于访问的实现，有助于药物安全评估。

Abstract: Monitoring the safety of medical products is a core concern of contemporary pharmacovigilance. To support drug safety assessment, Spontaneous Reporting Systems (SRS) collect reports of suspected adverse events of approved medical products offering a critical resource for identifying potential safety concerns that may not emerge during clinical trials. Modern nonparametric empirical Bayes methods are flexible statistical approaches that can accurately identify and estimate the strength of the association between an adverse event and a drug from SRS data. However, there is currently no comprehensive and easily accessible implementation of these methods. Here, we introduce the R package pvEBayes, which implements a suite of nonparametric empirical Bayes methods for pharmacovigilance, along with post-processing tools and graphical summaries for streamlining the application of these methods. Detailed examples are provided to demonstrate the application of the package through analyses of two real-world SRS datasets curated from the publicly available FDA FAERS database.

</details>


### [10] [COVID-19 Forecasting from U.S. Wastewater Surveillance Data: A Retrospective Multi-Model Study (2022-2024)](https://arxiv.org/abs/2512.01074)
*Faharudeen Alhassan,Hamed Karami,Amanda Bleichrodt,James M. Hyman,Isaac C. H. Fung,Ruiyan Luo,Gerardo Chowell*

Main category: stat.AP

TL;DR: 使用CDC废水监测数据评估11种模型对美国COVID-19疫情的周度预测性能，发现n-sub-epidemic非加权集成模型在3-4周预测期表现最佳，ARIMA和GAM在1-2周预测期表现最好。


<details>
  <summary>Details</summary>
Motivation: 准确的预测模型对公共卫生应对和政策制定至关重要。本研究旨在通过回顾性评估不同模型在COVID-19废水数据上的表现，提高疫情预测能力，为实时疫情预测提供指导。

Method: 使用CDC国家废水监测系统数据，从2022年3月至2024年9月对美国全国及四大区域（东北部、中西部、南部、西部）进行133周序列预测。评估了11种模型：ARIMA、广义加性模型(GAM)、简单线性回归(SLR)、Prophet和n-sub-epidemic框架（包括最优模型、加权集成和非加权集成变体）。使用MAE、MSE、WIS和95%预测区间覆盖率评估性能。

Result: n-sub-epidemic非加权集成模型在3-4周预测期表现最佳，特别是在全国层面以及中西部和西部地区。ARIMA和GAM在1-2周预测期在大多数区域表现最好。Prophet和SLR在所有区域和预测期均表现不佳。

Conclusion: 研究强调了区域特异性建模策略的价值，并证明了n-sub-epidemic框架在利用废水监测数据进行实时疫情预测中的实用性。不同预测期需要不同的最优模型选择。

Abstract: Accurate and reliable forecasting models are critical for guiding public health responses and policy decisions during pandemics such as COVID-19. Retrospective evaluation of model performance is essential for improving epidemic forecasting capabilities. In this study, we used COVID-19 wastewater data from CDC's National Wastewater Surveillance System to generate sequential weekly retrospective forecasts for the United States from March 2022 through September 2024, both at the national level and for four major regions (Northeast, Midwest, South, and West). We produced 133 weekly forecasts using 11 models, including ARIMA, generalized additive models (GAM), simple linear regression (SLR), Prophet, and the n-sub-epidemic framework (top-ranked, weighted-ensemble, and unweighted-ensemble variants). Forecast performance was assessed using mean absolute error (MAE), mean squared error (MSE), weighted interval score (WIS), and 95% prediction interval coverage. The n-sub-epidemic unweighted ensembles outperformed all other models at 3-4-week horizons, particularly at the national level and in the Midwest and West. ARIMA and GAM performed best at 1-2-week horizons in most regions, whereas Prophet and SLR consistently underperformed across regions and horizons. These findings highlight the value of region-specific modeling strategies and demonstrate the utility of the n-sub-epidemic framework for real-time outbreak forecasting using wastewater surveillance data.

</details>


### [11] [Improved Disease Outbreak Detection from Out-of-sequence measurements Using Markov-switching Fixed-lag Particle Filters](https://arxiv.org/abs/2512.01639)
*Conor Rosato,Joshua Murphy,Siân E. Jenkins,Paul Horridge,Alessandro Varsi,Martyn Bull,Alessandro Gerada,Alex Howard,Veronica Bowman,Simon Maskell*

Main category: stat.AP

TL;DR: 提出马尔可夫切换固定滞后粒子滤波器（FL-PF），用于处理流行病监测中的延迟或乱序数据，通过重采样历史轨迹提高爆发检测准确性和减少误报。


<details>
  <summary>Details</summary>
Motivation: 流行病监测中，数据常因延迟或乱序产生"过时测量"（OOS），现有粒子滤波器方法只能通过粒子重加权处理OOS，无法充分调整过去的潜在轨迹，限制了监测系统的准确性和及时性。

Method: 引入马尔可夫切换固定滞后粒子滤波器（FL-PF），在用户指定的滞后窗口内重新模拟粒子轨迹，允许OOS测量回溯性地更新状态和模型估计，并展示了如何在FL-PF框架中计算对数似然，实现参数估计。

Result: FL-PF通过显式重新评估历史样本，提高了爆发检测的准确性和及时性，减少了误报，扩展了粒子滤波器在具有回顾性数据的监测系统中的适用性。

Conclusion: 该方法为监测疾病爆发和参数推断提供了更稳健的框架，特别适用于具有延迟或乱序数据的流行病监测系统。

Abstract: Particle filters (PFs) have become an essential tool for disease surveillance, as they can estimate hidden epidemic states in nonlinear and non-Gaussian models. In epidemic modelling, population dynamics may be governed by distinct regimes such as endemic or outbreak phases which can be represented using Markov-switching state-space models. In many real-world surveillance systems, data often arrives with delays or in the wrong temporal order, producing out-of-sequence (OOS) measurements that pertain to past time points rather than the current one. While existing PF methods can incorporate OOS measurements through particle reweighting, these approaches are limited in their ability to fully adjust past latent trajectories. To address this, we introduce a Markov-switching fixed-lag particle filter (FL-PF) that resimulates particle trajectories within a user-specified lag window, allowing OOS measurements to retroactively update both state and model estimates. By explicitly reevaluating historical samples, the FL-PF improves the accuracy and timeliness of outbreak detection and reduces false alarms. We also show how to compute the log-likelihood within the FL-PF framework, enabling parameter estimation using Sequential Monte Carlo squared (SMC$^2$). Together, these contributions extend the applicability of PFs to surveillance systems where retrospective data are common, offering a more robust framework for monitoring disease outbreaks and parameter inference.

</details>


### [12] [Predicting Onsets and Dry Spells of the West African Monsoon Season Using Machine Learning Methods](https://arxiv.org/abs/2512.01965)
*Colin Bobocea,Yves Atchadé*

Main category: stat.AP

TL;DR: 该研究开发了基于海表温度遥相关的机器学习模型，用于预测西非雨季开始时间和干旱期，为农民种植决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 西非雨季开始时间和干旱期对农业生产至关重要，但难以预测。虽然已有研究表明全球海表温度与西非季风特征相关，但很少有研究将这些信息有效整合到机器学习预测模型中。

Method: 1. 结合两种已知定义来定义目标变量（雨季开始和干旱期）；2. 构建线性模型和自适应阈值逻辑回归模型；3. 应用自定义统计技术（如总变差正则化和预测因子选择）；4. 使用海表温度遥相关数据进行预测。

Result: 雨季开始预测结果不一：空间验证显示有显著技能，但时间验证显示技能有限。干旱期预测通过多个二分类指标分析显示出显著准确性。模型克服了现有方法的计算密集和需要偏差校正等限制。

Conclusion: 该研究为使用机器学习方法基于气候相关变量预测特定天气现象提供了一个框架，展示了机器学习在气象学等领域的应用潜力，并指出了进一步研究的新方向。

Abstract: The beginning of the rainy season and the occurrence of dry spells in West Africa is notoriously difficult to predict, however these are the key indicators farmers use to decide when to plant crops, having a major influence on their overall yield. While many studies have shown correlations between global sea surface temperatures and characteristics of the West African monsoon season, there are few that effectively implementing this information into machine learning (ML) prediction models. In this study we investigated the best ways to define our target variables, onset and dry spell, and produced methods to predict them for upcoming seasons using sea surface temperature teleconnections. Defining our target variables required the use of a combination of two well known definitions of onset. We then applied custom statistical techniques -- like total variation regularization and predictor selection -- to the two models we constructed, the first being a linear model and the other an adaptive-threshold logistic regression model. We found mixed results for onset prediction, with spatial verification showing signs of significant skill, while temporal verification showed little to none. For dry spell though, we found significant accuracy through the analysis of multiple binary classification metrics. These models overcome some limitations that current approaches have, such as being computationally intensive and needing bias correction. We also introduce this study as a framework to use ML methods for targeted prediction of certain weather phenomenon using climatologically relevant variables. As we apply ML techniques to more problems, we see clear benefits for fields like meteorology and lay out a few new directions for further research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [13] [DAISI: Data Assimilation with Inverse Sampling using Stochastic Interpolants](https://arxiv.org/abs/2512.00252)
*Martin Andrae,Erik Larsson,So Takao,Tomas Landelius,Fredrik Lindsten*

Main category: stat.ML

TL;DR: DAISI是一种基于流生成模型的滤波算法，使用数据驱动的先验进行灵活概率推断，通过逆采样步骤结合预报信息，在非线性观测系统中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法（如集合卡尔曼滤波）依赖高斯近似和启发式调参，在状态和观测分布显著偏离高斯性时可能不稳定或不准确，需要更灵活的概率推断方法

Method: 使用预训练的生成先验，通过基于引导的条件采样同化观测，同时通过新颖的逆采样步骤将预报集合映射到潜在空间，为条件采样提供初始条件，无需在每个同化步骤重新训练生成先验

Result: 在具有稀疏、噪声和非线性观测的挑战性非线性系统中，DAISI实现了准确的滤波结果，而传统方法在这些情况下表现不佳

Conclusion: DAISI通过结合生成模型和预报信息的创新方法，为数据同化提供了更灵活、准确的解决方案，特别适用于非高斯分布和非线性观测场景

Abstract: Data assimilation (DA) is a cornerstone of scientific and engineering applications, combining model forecasts with sparse and noisy observations to estimate latent system states. Classical DA methods, such as the ensemble Kalman filter, rely on Gaussian approximations and heuristic tuning (e.g., inflation and localization) to scale to high dimensions. While often successful, these approximations can make the methods unstable or inaccurate when the underlying distributions of states and observations depart significantly from Gaussianity. To address this limitation, we introduce DAISI, a scalable filtering algorithm built on flow-based generative models that enables flexible probabilistic inference using data-driven priors. The core idea is to use a stationary, pre-trained generative prior to assimilate observations via guidance-based conditional sampling while incorporating forecast information through a novel inverse-sampling step. This step maps the forecast ensemble into a latent space to provide initial conditions for the conditional sampling, allowing us to encode model dynamics into the DA pipeline without having to retrain or fine-tune the generative prior at each assimilation step. Experiments on challenging nonlinear systems show that DAISI achieves accurate filtering results in regimes with sparse, noisy, and nonlinear observations where traditional methods struggle.

</details>


### [14] [An RKHS Perspective on Tree Ensembles](https://arxiv.org/abs/2512.00397)
*Mehdi Dagdoug,Clement Dombry,Jean-Jil Duchamps*

Main category: stat.ML

TL;DR: 该论文提出了一个基于再生核希尔伯特空间的理论框架来分析随机森林和梯度提升等树集成方法，建立了随机森林核的基本分析性质，并将梯度提升解释为该RKHS上的梯度流。


<details>
  <summary>Details</summary>
Motivation: 随机森林和梯度提升是表格数据监督学习中最有效的算法，但缺乏统一的理论框架来解释其强大性能。论文旨在通过构建基于树集成的RKHS来提供理论解释。

Method: 基于随机回归树生成的分区构建再生核希尔伯特空间，分析随机森林核的性质（有界性、连续性、普适性），将随机森林预测器表征为该RKHS中惩罚经验风险泛函的唯一最小化器，并将梯度提升扩展为RKHS上的梯度流。

Result: 建立了随机森林核的基本分析性质，证明了随机森林预测器的变分解释，将梯度提升与RKHS上的梯度流联系起来，并提出了基于随机森林核的核主成分分析和新的几何变量重要性准则GVI。

Conclusion: 该理论框架为树集成方法提供了统一的理论解释，揭示了其核和数据依赖几何的本质，有助于增强模型可解释性并开发新的分析方法。

Abstract: Random Forests and Gradient Boosting are among the most effective algorithms for supervised learning on tabular data. Both belong to the class of tree-based ensemble methods, where predictions are obtained by aggregating many randomized regression trees. In this paper, we develop a theoretical framework for analyzing such methods through Reproducing Kernel Hilbert Spaces (RKHSs) constructed on tree ensembles -- more precisely, on the random partitions generated by randomized regression trees. We establish fundamental analytical properties of the resulting Random Forest kernel, including boundedness, continuity, and universality, and show that a Random Forest predictor can be characterized as the unique minimizer of a penalized empirical risk functional in this RKHS, providing a variational interpretation of ensemble learning. We further extend this perspective to the continuous-time formulation of Gradient Boosting introduced by Dombry and Duchamps, and demonstrate that it corresponds to a gradient flow on a Hilbert manifold induced by the Random Forest RKHS. A key feature of this framework is that both the kernel and the RKHS geometry are data-dependent, offering a theoretical explanation for the strong empirical performance of tree-based ensembles. Finally, we illustrate the practical potential of this approach by introducing a kernel principal component analysis built on the Random Forest kernel, which enhances the interpretability of ensemble models, as well as GVI, a new geometric variable importance criterion.

</details>


### [15] [No-Regret Gaussian Process Optimization of Time-Varying Functions](https://arxiv.org/abs/2512.00517)
*Eliabelle Mauduit,Eloïse Berthier,Andrea Simonetto*

Main category: stat.ML

TL;DR: 提出W-SparQ-GP-UCB算法，通过不确定性注入和稀疏推理优化时变黑盒函数，在允许少量额外查询的条件下实现无遗憾性能


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程bandit算法（如GP-UCB）在平稳环境下能保证无遗憾性能，但对于时变目标函数，在纯bandit反馈下无法实现无遗憾，除非施加强假设。需要开发能处理时变奖励函数的新方法。

Method: 提出W-SparQ-GP-UCB算法：1）使用不确定性注入（UI）进行异方差高斯过程回归，将过去观测适应到当前时间步；2）放松严格bandit设置，允许对先前观察点进行额外查询；3）基于稀疏推理和UI对遗憾的影响设计在线算法

Result: 算法在每次迭代仅需消失性数量的额外查询下实现无遗憾性能。建立了额外查询数量的下界，证明方法的高效性。提供了函数时变程度与可达到遗憾率之间关系的全面分析，以及各机制下所需额外查询数量的上下界。

Conclusion: 通过不确定性注入和稀疏推理，W-SparQ-GP-UCB算法在允许少量额外查询的条件下有效优化时变黑盒函数，填补了时变环境下无遗憾优化的理论空白，为实际应用提供了理论保证。

Abstract: Sequential optimization of black-box functions from noisy evaluations has been widely studied, with Gaussian Process bandit algorithms such as GP-UCB guaranteeing no-regret in stationary settings. However, for time-varying objectives, it is known that no-regret is unattainable under pure bandit feedback unless strong and often unrealistic assumptions are imposed.
  In this article, we propose a novel method to optimize time-varying rewards in the frequentist setting, where the objective has bounded RKHS norm. Time variations are captured through uncertainty injection (UI), which enables heteroscedastic GP regression that adapts past observations to the current time step. As no-regret is unattainable in general in the strict bandit setting, we relax the latter allowing additional queries on previously observed points. Building on sparse inference and the effect of UI on regret, we propose \textbf{W-SparQ-GP-UCB}, an online algorithm that achieves no-regret with only a vanishing number of additional queries per iteration. To assess the theoretical limits of this approach, we establish a lower bound on the number of additional queries required for no-regret, proving the efficiency of our method. Finally, we provide a comprehensive analysis linking the degree of time-variation of the function to achievable regret rates, together with upper and lower bounds on the number of additional queries needed in each regime.

</details>


### [16] [Discriminative classification with generative features: bridging Naive Bayes and logistic regression](https://arxiv.org/abs/2512.01097)
*Zachary Terner,Alexander Petersen,Yuedong Wang*

Main category: stat.ML

TL;DR: Smart Bayes：一种结合生成式与判别式建模的分类框架，通过将似然比特征整合到逻辑回归式分类器中，在生成式与判别式方法间架起桥梁。


<details>
  <summary>Details</summary>
Motivation: 传统朴素贝叶斯使用固定的单位权重，而逻辑回归缺乏生成式结构。需要一种方法既能利用生成式建模的似然比信息，又能保持判别式方法的灵活性。

Method: 将边际对数密度比作为变换后的输入特征，这些特征明确量化了每个特征值在一个类别下比另一个类别更可能出现的程度。使用样条基估计器估计单变量对数密度比，该估计器灵活、鲁棒且计算高效。

Result: 通过大量模拟和真实数据研究，Smart Bayes通常优于逻辑回归和朴素贝叶斯。该方法在多个数据集上表现出更好的分类性能。

Conclusion: Smart Bayes展示了利用生成式结构增强判别式性能的混合方法的潜力，为分类问题提供了一种有效的替代方案。

Abstract: We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance.

</details>


### [17] [Statistical-computational gap in multiple Gaussian graph alignment](https://arxiv.org/abs/2512.00610)
*Bertrand Even,Luca Ganassali*

Main category: stat.ML

TL;DR: 本文研究了多高斯图对齐问题中的统计-计算差距，发现了信息阈值与计算障碍之间的分离，表明多项式时间算法无法处理复杂的组合二维结构。


<details>
  <summary>Details</summary>
Motivation: 研究多高斯图对齐问题中是否存在统计-计算差距，即信息理论上可解但计算上困难的情况。先前工作主要关注固定数量的图，本文扩展到了图数量随节点数增长的情况。

Method: 首先将Vassaux和Massoulié (2025)的信息阈值推广到图数量p随节点数n增长的情况，分析不同p/n关系下的恢复阈值。然后使用低度框架首次为多高斯图对齐问题建立了计算障碍。

Result: 发现：1) 当p ≤ O(n/log n)时恢复先前结果；2) 当p ≥ Ω(n/log n)时问题难度等同于对齐单个图与未知信号图；3) 当log p = ω(log n)时部分恢复与精确恢复的信息阈值不再重合；4) 当相关性ρ < 1时，低度非平凡估计失败，表明多项式时间对齐p个图与对齐两个图难度相当。

Conclusion: 多高斯图对齐问题存在统计-计算差距，多项式时间算法无法有效处理这种复杂的组合二维结构，为统计-计算差距理论提供了新的例证。

Abstract: We investigate the existence of a statistical-computational gap in multiple Gaussian graph alignment. We first generalize a previously established informational threshold from Vassaux and Massoulié (2025) to regimes where the number of observed graphs $p$ may also grow with the number of nodes $n$: when $p \leq O(n/\log(n))$, we recover the results from Vassaux and Massoulié (2025), and $p \geq Ω(n/\log(n))$ corresponds to a regime where the problem is as difficult as aligning one single graph with some unknown "signal" graph. Moreover, when $\log p = ω(\log n)$, the informational thresholds for partial and exact recovery no longer coincide, in contrast to the all-or-nothing phenomenon observed when $\log p=O(\log n)$. Then, we provide the first computational barrier in the low-degree framework for (multiple) Gaussian graph alignment. We prove that when the correlation $ρ$ is less than $1$, up to logarithmic terms, low degree non-trivial estimation fails. Our results suggest that the task of aligning $p$ graphs in polynomial time is as hard as the problem of aligning two graphs in polynomial time, up to logarithmic factors. These results characterize the existence of a statistical-computational gap and provide another example in which polynomial-time algorithms cannot handle complex combinatorial bi-dimensional structures.

</details>


### [18] [Self-sufficient Independent Component Analysis via KL Minimizing Flows](https://arxiv.org/abs/2512.00665)
*Song Liu*

Main category: stat.ML

TL;DR: 提出一种基于自充分信号学习的非线性ICA方法，通过最小化条件KL散度实现解耦，无需先验分布或观测模型，避免了对抗训练的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 受自监督学习启发，研究如何从数据中学习解耦信号。传统最大似然估计需要先验分布和观测模型，限制了模型灵活性。希望开发一种无需先验、无需似然函数的方法。

Method: 提出学习"自充分信号"：恢复的信号应能仅从自身剩余分量重建缺失值，不依赖其他信号。将问题形式化为条件KL散度最小化。提出顺序算法，在每次迭代中减少KL散度并学习最优解混流模型，完全避免不稳定的对抗训练。

Result: 在玩具数据集和真实世界数据集上的实验证明了该方法的有效性。

Conclusion: 该方法为非线性ICA提供了一种无需先验、无需似然函数的解耦学习框架，避免了对抗训练的不稳定性，在实际应用中表现有效。

Abstract: We study the problem of learning disentangled signals from data using non-linear Independent Component Analysis (ICA). Motivated by advances in self-supervised learning, we propose to learn self-sufficient signals: A recovered signal should be able to reconstruct a missing value of its own from all remaining components without relying on any other signals. We formulate this problem as the minimization of a conditional KL divergence. Compared to traditional maximum likelihood estimation, our algorithm is prior-free and likelihood-free, meaning that we do not need to impose any prior on the original signals or any observational model, which often restricts the model's flexibility. To tackle the KL divergence minimization problem, we propose a sequential algorithm that reduces the KL divergence and learns an optimal de-mixing flow model at each iteration. This approach completely avoids the unstable adversarial training, a common issue in minimizing the KL divergence. Experiments on toy and real-world datasets show the effectiveness of our method.

</details>


### [19] [Restricted Block Permutation for Two-Sample Testing](https://arxiv.org/abs/2512.00668)
*Jungwoo Ho*

Main category: stat.ML

TL;DR: 提出一种基于块限制单交换置换的两样本检验方法，相比完全置换检验能获得更高统计功效，同时保持精确有限样本有效性。


<details>
  <summary>Details</summary>
Motivation: 传统完全置换检验的置换方差较大，导致临界值较保守，统计功效有限。需要一种既能保持精确有效性又能提高检验功效的置换方案。

Method: 设计结构化置换方案：将样本分块，在每个块内选择代表样本，仅允许块间代表样本的单次交叉交换。对样本均值差和无偏MMD²估计量推导闭式单交换增量恒等式。

Result: 1) 提供适用于任何固定限制置换集的精确有效性构造；2) 单交换增量条件方差为O(h²)，相比完全置换的Θ(h)显著缩小；3) 获得显式的数据依赖临界值和统计功率表达式。

Conclusion: 块限制单交换置换能在保持精确有限样本有效性的前提下，获得比经典完全置换检验更高的统计功效，无需依赖保守的Lipschitz边界。

Abstract: We study a structured permutation scheme for two-sample testing that restricts permutations to single cross-swaps between block-selected representatives. Our analysis yields three main results. First, we provide an exact validity construction that applies to any fixed restricted permutation set. Second, for both the difference of sample means and the unbiased $\widehat{\mathrm{MMD}}^{2}$ estimator, we derive closed-form one-swap increment identities whose conditional variances scale as $O(h^{2})$, in contrast to the $Θ(h)$ increment variability under full relabeling. This increment-level variance contraction sharpens the Bernstein--Freedman variance proxy and leads to substantially smaller permutation critical values. Third, we obtain explicit, data-dependent expressions for the resulting critical values and statistical power. Together, these results show that block-restricted one-swap permutations can achieve strictly higher power than classical full permutation tests while maintaining exact finite-sample validity, without relying on pessimistic worst-case Lipschitz bounds.

</details>


### [20] [Dimension-free error estimate for diffusion model and optimal scheduling](https://arxiv.org/abs/2512.01820)
*Valentin de Bortoli,Romuald Elie,Anna Kazeykina,Zhenjie Ren,Jiacheng Zhang*

Main category: stat.ML

TL;DR: 该论文提出了一种新的维度无关的误差界，用于分析扩散生成模型中的采样误差，并推导出最优时间调度策略。


<details>
  <summary>Details</summary>
Motivation: 现有分析扩散生成模型误差的方法存在局限性：KL散度要求分布绝对连续，Wasserstein距离在高维情况下误差界随维度增长而变得不实用。需要一种能在高维设置下提供实用保证的误差度量方法。

Method: 使用具有有界一阶和二阶导数的光滑测试泛函作为误差度量，推导出维度无关的误差界。通过变分问题最小化时间离散化误差，推导出反向扩散的最优时间调度策略。

Result: 获得了维度无关的误差界，该界仅依赖于测试函数的正则性。推导出的最优时间调度策略与文献中已有的调度器一致，但本文为其最优性提供了新的理论依据。

Conclusion: 通过使用更弱的泛函度量，可以在高维情况下获得实用的误差保证。推导的最优时间调度策略为扩散生成模型的实践应用提供了理论指导。

Abstract: Diffusion generative models have emerged as powerful tools for producing synthetic data from an empirically observed distribution. A common approach involves simulating the time-reversal of an Ornstein-Uhlenbeck (OU) process initialized at the true data distribution. Since the score function associated with the OU process is typically unknown, it is approximated using a trained neural network. This approximation, along with finite time simulation, time discretization and statistical approximation, introduce several sources of error whose impact on the generated samples must be carefully understood. Previous analyses have quantified the error between the generated and the true data distributions in terms of Wasserstein distance or Kullback-Leibler (KL) divergence. However, both metrics present limitations: KL divergence requires absolute continuity between distributions, while Wasserstein distance, though more general, leads to error bounds that scale poorly with dimension, rendering them impractical in high-dimensional settings. In this work, we derive an explicit, dimension-free bound on the discrepancy between the generated and the true data distributions. The bound is expressed in terms of a smooth test functional with bounded first and second derivatives. The key novelty lies in the use of this weaker, functional metric to obtain dimension-independent guarantees, at the cost of higher regularity on the test functions. As an application, we formulate and solve a variational problem to minimize the time-discretization error, leading to the derivation of an optimal time-scheduling strategy for the reverse-time diffusion. Interestingly, this scheduler has appeared previously in the literature in a different context; our analysis provides a new justification for its optimality, now grounded in minimizing the discretization bias in generative sampling.

</details>


### [21] [Outcome-Aware Spectral Feature Learning for Instrumental Variable Regression](https://arxiv.org/abs/2512.00919)
*Dimitri Meunier,Jakub Wornbard,Vladimir R. Kostic,Antoine Moulin,Alek Fröhlich,Karim Lounici,Massimiliano Pontil,Arthur Gretton*

Main category: stat.ML

TL;DR: 提出增强谱特征学习框架，通过结合结果信息学习任务特定特征，解决传统非参数工具变量回归中谱特征与因果函数不对齐的问题


<details>
  <summary>Details</summary>
Motivation: 传统基于谱特征的工具变量回归方法虽然强大，但这些特征与结果变量无关，当真实因果函数无法被主导奇异函数很好表示时，方法会失效

Method: 提出增强谱特征学习框架，通过最小化源自增强算子的对比损失来学习特征，该增强算子结合了结果信息，使特征学习过程对结果敏感

Result: 该方法在具有挑战性的基准测试中验证有效，即使在谱不对齐情况下仍能保持效果，提供了理论分析支持

Conclusion: 通过使特征学习过程对结果敏感，增强谱特征学习框架解决了传统方法在谱不对齐时的局限性，提高了因果效应估计的鲁棒性

Abstract: We address the problem of causal effect estimation in the presence of hidden confounders using nonparametric instrumental variable (IV) regression. An established approach is to use estimators based on learned spectral features, that is, features spanning the top singular subspaces of the operator linking treatments to instruments. While powerful, such features are agnostic to the outcome variable. Consequently, the method can fail when the true causal function is poorly represented by these dominant singular functions. To mitigate, we introduce Augmented Spectral Feature Learning, a framework that makes the feature learning process outcome-aware. Our method learns features by minimizing a novel contrastive loss derived from an augmented operator that incorporates information from the outcome. By learning these task-specific features, our approach remains effective even under spectral misalignment. We provide a theoretical analysis of this framework and validate our approach on challenging benchmarks.

</details>


### [22] [Thompson Sampling for Multi-Objective Linear Contextual Bandit](https://arxiv.org/abs/2512.00930)
*Somangchan Park,Heesang Ann,Min-hwan Oh*

Main category: stat.ML

TL;DR: 提出了首个具有Pareto遗憾保证的Thompson Sampling算法MOL-TS，用于多目标线性上下文赌博机问题，实现了与单目标算法相同阶数的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 多目标线性上下文赌博机问题中，多个可能冲突的目标需要同时优化。现有方法通常每轮计算经验Pareto前沿，缺乏理论保证的Thompson Sampling方法。

Method: 提出MOL-TS算法：1）跨目标采样参数；2）从新颖的"有效Pareto前沿"中选择臂，该前沿考虑了随时间重复选择的影响。

Result: 理论分析显示MOL-TS实现了最坏情况Pareto遗憾界$\widetilde{O}(d^{3/2}\sqrt{T})$，与单目标随机线性赌博机算法的最佳已知阶数匹配。实证结果验证了该方法在遗憾最小化和多目标性能方面的优势。

Conclusion: MOL-TS是首个具有Pareto遗憾理论保证的Thompson Sampling算法，在多目标线性上下文赌博机问题中实现了最优阶数的遗憾界，并通过有效Pareto前沿的创新设计提升了性能。

Abstract: We study the multi-objective linear contextual bandit problem, where multiple possible conflicting objectives must be optimized simultaneously. We propose \texttt{MOL-TS}, the \textit{first} Thompson Sampling algorithm with Pareto regret guarantees for this problem. Unlike standard approaches that compute an empirical Pareto front each round, \texttt{MOL-TS} samples parameters across objectives and efficiently selects an arm from a novel \emph{effective Pareto front}, which accounts for repeated selections over time. Our analysis shows that \texttt{MOL-TS} achieves a worst-case Pareto regret bound of $\widetilde{O}(d^{3/2}\sqrt{T})$, where $d$ is the dimension of the feature vectors, $T$ is the total number of rounds, matching the best known order for randomized linear bandit algorithms for single objective. Empirical results confirm the benefits of our proposed approach, demonstrating improved regret minimization and strong multi-objective performance.

</details>


### [23] [An Approach to Variable Clustering: K-means in Transposed Data and its Relationship with Principal Component Analysis](https://arxiv.org/abs/2512.00979)
*Victor Saquicela,Kenneth Palacio-Baus,Mario Chifla*

Main category: stat.ML

TL;DR: 提出一种创新方法，分析K-means应用于转置数据得到的变量聚类与PCA主成分之间的关系，量化每个变量聚类对每个主成分的贡献。


<details>
  <summary>Details</summary>
Motivation: PCA和K-means是多变量分析中的基本技术，虽然它们经常被独立或顺序地应用于观测聚类，但对于K-means用于变量聚类（而非观测聚类）时与PCA的关系却很少被探索。本研究旨在填补这一空白。

Method: 对原始数据应用PCA，对转置数据集应用K-means（将原始变量转换为观测值），然后基于变量载荷量化每个变量聚类对每个主成分的贡献。

Result: 该方法提供了一个工具，用于探索和理解变量聚类以及这些聚类如何对PCA识别的主要变异维度做出贡献。

Conclusion: 本研究建立了K-means变量聚类与PCA主成分之间的关联，为理解多变量分析中这两种基本技术的关系提供了新的视角和方法。

Abstract: Principal Component Analysis (PCA) and K-means constitute fundamental techniques in multivariate analysis. Although they are frequently applied independently or sequentially to cluster observations, the relationship between them, especially when K-means is used to cluster variables rather than observations, has been scarcely explored. This study seeks to address this gap by proposing an innovative method that analyzes the relationship between clusters of variables obtained by applying K-means on transposed data and the principal components of PCA. Our approach involves applying PCA to the original data and K-means to the transposed data set, where the original variables are converted into observations. The contribution of each variable cluster to each principal component is then quantified using measures based on variable loadings. This process provides a tool to explore and understand the clustering of variables and how such clusters contribute to the principal dimensions of variation identified by PCA.

</details>


### [24] [High-dimensional Mean-Field Games by Particle-based Flow Matching](https://arxiv.org/abs/2512.01172)
*Jiajia Yu,Junghwan Lee,Yao Xie,Xiuyuan Cheng*

Main category: stat.ML

TL;DR: 提出基于粒子的深度流匹配方法求解高维平均场博弈，通过近端不动点迭代结合流神经网络，理论证明收敛性并在非势能MFG和高维最优传输问题上验证有效性


<details>
  <summary>Details</summary>
Motivation: 平均场博弈为连续体交互智能体的纳什均衡提供了统一框架，涵盖最优传输和生成模型等应用，但高维MFG求解面临计算和分析上的根本性挑战

Method: 提出粒子基础的深度流匹配方法：在近端不动点迭代中，粒子使用一阶信息更新，训练流神经网络以无模拟方式匹配样本轨迹的速度，通过流匹配从拉格朗日坐标（粒子基础）诱导欧拉坐标（密度基础）

Result: 理论上证明在最优控制设置下算法次线性收敛到稳定点，在附加凸性假设下升级为线性（指数）收敛；实验表明方法在非势能MFG和通过松弛终端成本公式转化为MFG的高维OT问题上表现良好

Conclusion: 提出的基于粒子的深度流匹配方法能有效解决高维平均场博弈计算问题，建立了拉格朗日和欧拉公式在MFG求解中的等价性，为高维最优传输等应用提供了实用解决方案

Abstract: Mean-field games (MFGs) study the Nash equilibrium of systems with a continuum of interacting agents, which can be formulated as the fixed-point of optimal control problems. They provide a unified framework for a variety of applications, including optimal transport (OT) and generative models. Despite their broad applicability, solving high-dimensional MFGs remains a significant challenge due to fundamental computational and analytical obstacles. In this work, we propose a particle-based deep Flow Matching (FM) method to tackle high-dimensional MFG computation. In each iteration of our proximal fixed-point scheme, particles are updated using first-order information, and a flow neural network is trained to match the velocity of the sample trajectories in a simulation-free manner. Theoretically, in the optimal control setting, we prove that our scheme converges to a stationary point sublinearly, and upgrade to linear (exponential) convergence under additional convexity assumptions. Our proof uses FM to induce an Eulerian coordinate (density-based) from a Lagrangian one (particle-based), and this also leads to certain equivalence results between the two formulations for MFGs when the Eulerian solution is sufficiently regular. Our method demonstrates promising performance on non-potential MFGs and high-dimensional OT problems cast as MFGs through a relaxed terminal-cost formulation.

</details>


### [25] [Implicitly Normalized Online PCA: A Regularized Algorithm with Exact High-Dimensional Dynamics](https://arxiv.org/abs/2512.01231)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: INO-PCA是一种在线PCA算法，通过放松单位范数约束，允许参数范数动态演化，利用范数编码统计结构信息，在理论和实验上均优于传统Oja算法。


<details>
  <summary>Details</summary>
Motivation: 传统在线PCA方法强制进行显式归一化步骤，丢弃了参数向量演化范数中的信息。作者认为这些范数实际上编码了问题底层统计结构的有意义信息，利用这些信息可以改善学习行为。

Method: 提出隐式归一化在线PCA（INO-PCA），移除单位范数约束，通过简单的正则化更新允许参数范数动态演化。在高维极限下分析估计量与真实分量的联合经验分布收敛到确定性测值过程，由非线性PDE控制。

Result: 分析显示参数范数服从与余弦相似度耦合的闭式ODE，形成调节学习率、稳定性和信噪比敏感度的内部状态变量。揭示了范数、SNR和最优步长之间的三向关系，并暴露了稳态性能的急剧相变。INO-PCA在理论和实验上均优于Oja算法，在非平稳环境中快速适应。

Conclusion: 放松范数约束可以成为在线算法中编码和利用问题相关信息的原则性有效方法。INO-PCA通过动态范数演化实现了更好的学习性能和对统计结构的自适应。

Abstract: Many online learning algorithms, including classical online PCA methods, enforce explicit normalization steps that discard the evolving norm of the parameter vector. We show that this norm can in fact encode meaningful information about the underlying statistical structure of the problem, and that exploiting this information leads to improved learning behavior. Motivated by this principle, we introduce Implicitly Normalized Online PCA (INO-PCA), an online PCA algorithm that removes the unit-norm constraint and instead allows the parameter norm to evolve dynamically through a simple regularized update. We prove that in the high-dimensional limit the joint empirical distribution of the estimate and the true component converges to a deterministic measure-valued process governed by a nonlinear PDE. This analysis reveals that the parameter norm obeys a closed-form ODE coupled with the cosine similarity, forming an internal state variable that regulates learning rate, stability, and sensitivity to signal-to-noise ratio (SNR). The resulting dynamics uncover a three-way relationship between the norm, SNR, and optimal step size, and expose a sharp phase transition in steady-state performance. Both theoretically and experimentally, we show that INO-PCA consistently outperforms Oja's algorithm and adapts rapidly in non-stationary environments. Overall, our results demonstrate that relaxing norm constraints can be a principled and effective way to encode and exploit problem-relevant information in online learning algorithms.

</details>


### [26] [LPCD: Unified Framework from Layer-Wise to Submodule Quantization](https://arxiv.org/abs/2512.01546)
*Yuma Ichikawa,Yudai Fujimoto,Akira Sakai*

Main category: stat.ML

TL;DR: 提出LPCD框架，将后训练量化从单层扩展到任意子模块，通过优化松弛目标并投影到分层量化器，提升量化效果


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法主要关注单个线性层，即使最近的扩展方法如QEP和LoaQ也依赖于分层公式，无法捕捉更大子模块的行为

Method: 引入层投影坐标下降（LPCD）框架，通过优化任意子模块的松弛目标，并使用分层量化器投影解决方案，统一扩展PTQ方法

Result: 在多种LLM架构和比特宽度下，基于LPCD的子模块量化持续提升分层PTQ方法和现有子模块方法的性能

Conclusion: LPCD为量化复杂子模块提供了原则性方法，同时保持了分层PTQ流程的效率和兼容性，是后训练量化的统一框架

Abstract: Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.

</details>


### [27] [Differentially Private and Federated Structure Learning in Bayesian Networks](https://arxiv.org/abs/2512.01708)
*Ghita Fassy El Fehri,Aurélien Bellet,Philippe Bastien*

Main category: stat.ML

TL;DR: 提出Fed-Sparse-BNSL方法，在保护隐私和降低通信成本的前提下，从分散数据中学习线性高斯贝叶斯网络结构


<details>
  <summary>Details</summary>
Motivation: 从分散数据学习贝叶斯网络结构面临两大挑战：1) 确保参与者的严格隐私保护；2) 避免通信成本随维度急剧增加。现有方法难以同时解决这两个问题。

Method: 结合差分隐私和贪婪更新策略，每个参与者只针对少量相关边进行更新，高效利用隐私预算并保持低通信成本。精心设计的算法保持模型可识别性，实现准确的结构估计。

Result: 在合成和真实数据集上的实验表明，Fed-Sparse-BNSL在提供更强隐私保护和通信效率的同时，实现了接近非隐私基线的性能。

Conclusion: Fed-Sparse-BNSL成功解决了分散数据学习贝叶斯网络结构的隐私和通信效率问题，为实际应用提供了可行的解决方案。

Abstract: Learning the structure of a Bayesian network from decentralized data poses two major challenges: (i) ensuring rigorous privacy guarantees for participants, and (ii) avoiding communication costs that scale poorly with dimensionality. In this work, we introduce Fed-Sparse-BNSL, a novel federated method for learning linear Gaussian Bayesian network structures that addresses both challenges. By combining differential privacy with greedy updates that target only a few relevant edges per participant, Fed-Sparse-BNSL efficiently uses the privacy budget while keeping communication costs low. Our careful algorithmic design preserves model identifiability and enables accurate structure estimation. Experiments on synthetic and real datasets demonstrate that Fed-Sparse-BNSL achieves utility close to non-private baselines while offering substantially stronger privacy and communication efficiency.

</details>


### [28] [Common Structure Discovery in Collections of Bipartite Networks: Application to Pollination Systems](https://arxiv.org/abs/2512.01716)
*Louis Lacoste,Pierre Barbillon,Sophie Donnet*

Main category: stat.ML

TL;DR: 提出colBiSBM模型，用于联合分析多个二分网络的共享结构，通过共同块间连接参数捕捉网络间的模式相似性，提高聚类和链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有二分网络结构检测方法忽略了网络集合中的共享模式，无法有效比较不同网络的生态相互作用组织方式，限制了理解环境因素如何塑造群落结构和恢复力。

Method: 扩展经典潜在块模型(LBM)，提出colBiSBM概率模型族，假设网络是共享中尺度结构的独立实现，具有共同的块间连接参数。建立了可识别性条件，开发了变分EM算法进行参数估计，并采用ICL准则进行模型选择。

Result: 模拟研究显示colBiSBM能有效恢复共同结构、提高聚类性能、通过跨网络借用信息增强链接预测。植物-传粉者网络应用揭示了共享生态角色，并能将网络划分为具有相似连接模式的子集合。

Conclusion: 联合建模在二分系统研究中具有方法学和应用优势，优于单独网络分析，能更好地揭示网络间的共享组织模式。

Abstract: Bipartite networks are widely used to encode the ecological interactions. Being able to compare the organization of bipartite networks is a first step toward a better understanding of how environmental factors shape community structure and resilience. Yet current methods for structure detection in bipartite networks overlook shared patterns across collections of networks. We introduce the \emph{colBiSBM}, a family of probabilistic models for collections of bipartite networks that extends the classical Latent Block Model (LBM). The proposed framework assumes that networks are independent realizations of a shared mesoscale structure, encoded through common inter-block connectivity parameters. We establish identifiability conditions for the different variants of \emph{colBiSBM} and develop a variational EM algorithm for parameter estimation, coupled with an adaptation of the Integrated Classification Likelihood (ICL) criterion for model selection. We demonstrate how our approach can be used to classify networks based on their topology or organization. Simulation studies highlight the ability of \emph{colBiSBM} to recover common structures, improve clustering performance, and enhance link prediction by borrowing strength across networks. An application to plant--pollinator networks highlights how the method uncovers shared ecological roles and partitions networks into sub-collections with similar connectivity patterns. These results illustrate the methodological and practical advantages of joint modeling over separate network analyses in the study of bipartite systems.

</details>


### [29] [Decision Tree Embedding by Leaf-Means](https://arxiv.org/abs/2512.01819)
*Cencheng Shen,Yuexiao Dong,Carey E. Priebe*

Main category: stat.ML

TL;DR: 决策树嵌入（DTE）是一种快速有效的方法，利用训练好的分类树的叶子分区构建可解释的特征表示，通过叶子区域内的样本均值作为锚点，将输入映射到嵌入空间，避免决策树分裂规则的高方差问题。


<details>
  <summary>Details</summary>
Motivation: 决策树和随机森林在中等规模标准数据集上具有竞争力，但单棵树存在高估计方差，而大型集成方法虽然减少方差却带来计算开销和可解释性降低的问题。需要一种既能保持可解释性又能提高准确性的方法。

Method: 提出决策树嵌入（DTE）方法：1）利用训练好的分类树的叶子分区结构构建特征表示；2）使用每个叶子区域内的样本均值作为锚点；3）将输入映射到基于树分区结构的嵌入空间；4）引入基于额外bootstrap树的集成扩展；5）将生成的嵌入与线性判别分析结合进行分类。

Result: 理论分析表明DTE在温和条件下能保持条件密度，并描述了分类误差特性。实证研究表明DTE在准确性和计算效率之间取得良好平衡，在大多数情况下优于或匹配随机森林和浅层神经网络，且训练时间只需它们的一小部分。

Conclusion: DTE方法既可以看作改进标准分裂规则的可扩展决策树分类器，也可以看作权重从树派生锚点学习的神经网络模型，实现了两种范式的有趣整合，在保持可解释性的同时提高了分类性能。

Abstract: Decision trees and random forest remain highly competitive for classification on medium-sized, standard datasets due to their robustness, minimal preprocessing requirements, and interpretability. However, a single tree suffers from high estimation variance, while large ensembles reduce this variance at the cost of substantial computational overhead and diminished interpretability. In this paper, we propose Decision Tree Embedding (DTE), a fast and effective method that leverages the leaf partitions of a trained classification tree to construct an interpretable feature representation. By using the sample means within each leaf region as anchor points, DTE maps inputs into an embedding space defined by the tree's partition structure, effectively circumventing the high variance inherent in decision-tree splitting rules. We further introduce an ensemble extension based on additional bootstrap trees, and pair the resulting embedding with linear discriminant analysis for classification. We establish several population-level theoretical properties of DTE, including its preservation of conditional density under mild conditions and a characterization of the resulting classification error. Empirical studies on synthetic and real datasets demonstrate that DTE strikes a strong balance between accuracy and computational efficiency, outperforming or matching random forest and shallow neural networks while requiring only a fraction of their training time in most cases. Overall, the proposed DTE method can be viewed either as a scalable decision tree classifier that improves upon standard split rules, or as a neural network model whose weights are learned from tree-derived anchor points, achieving an intriguing integration of both paradigms.

</details>


### [30] [Fundamentals of Regression](https://arxiv.org/abs/2512.01920)
*Miguel A. Mendez*

Main category: stat.ML

TL;DR: 本章回顾了回归分析这一机器学习子领域，从经典统计方法到物理信息机器学习的发展，介绍了如何将机器学习与传统计算科学相结合。


<details>
  <summary>Details</summary>
Motivation: 随着科学机器学习的发展，回归分析已从纯粹的数据驱动统计形式转向融合物理知识的约束形式。本章旨在介绍这一转变，展示如何将机器学习与传统计算工程方法相结合。

Method: 1. 介绍回归分析的基本概念及其与曲线拟合的区别；2. 概述传统机器学习方法及其分类；3. 探讨将机器学习与传统计算科学连接的方法；4. 讨论结合机器学习与物理数值方法的技术。

Result: 本章系统性地介绍了回归分析从统计形式到物理信息形式的发展历程，提供了将机器学习与传统计算工程相结合的框架和方法论。

Conclusion: 物理信息机器学习代表了回归分析的重要发展方向，通过整合物理知识和传统计算方法，能够更有效地解决科学和工程问题。

Abstract: This chapter opens with a review of classic tools for regression, a subset of machine learning that seeks to find relationships between variables. With the advent of scientific machine learning this field has moved from a purely data-driven (statistical) formalism to a constrained or ``physics-informed'' formalism, which integrates physical knowledge and methods from traditional computational engineering. In the first part, we introduce the general concepts and the statistical flavor of regression versus other forms of curve fitting. We then move to an overview of traditional methods from machine learning and their classification and ways to link these to traditional computational science. Finally, we close with a note on methods to combine machine learning and numerical methods for physics

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [31] [Rényi's $α$-divergence variational Bayes for spike-and-slab high-dimensional linear regression](https://arxiv.org/abs/2512.00627)
*Chadi Bsila,Yiqi Tang,Kaiwen Wang,Laurie Heyer*

Main category: stat.ME

TL;DR: 提出基于Rényi α散度的变分贝叶斯方法，用于稀疏高维线性回归，替代标准KL散度，提供零强制与质量覆盖之间的灵活性


<details>
  <summary>Details</summary>
Motivation: 稀疏高维线性回归是统计学中的核心问题，通常涉及变量选择和系数估计。标准变分贝叶斯方法使用KL散度，但缺乏灵活性，需要一种能够平衡零强制和质量覆盖行为的方法

Method: 提出基于Rényi α散度的均值场变分贝叶斯近似，使用尖峰-平板Laplace先验。推导了基于二阶delta方法的坐标上升变分推断更新，并开发了基于蒙特卡洛代理Rényi下界的随机变分推断算法

Result: 在模拟中，两种方法在多种稀疏配置和α值下，在变量选择和估计方面与最先进的贝叶斯变量选择方法表现相当。数值结果表明不同α值在不同稀疏配置下具有优势

Conclusion: Rényi α散度为稀疏回归的变分推断提供了灵活框架，允许根据具体稀疏配置调整零强制与质量覆盖行为，为贝叶斯变量选择提供了有效替代方案

Abstract: Sparse high-dimensional linear regression is a central problem in statistics, where the goal is often variable selection and/or coefficient estimation. We propose a mean-field variational Bayes approximation for sparse regression with spike-and-slab Laplace priors that replaces the standard Kullback-Leibler (KL) divergence objective with the Rényi's $α$ divergence: a one-parameter generalization of KL divergence indexed by $α\in (0, \infty) \setminus \{1\}$ that allows flexibility between zero-forcing and mass-covering behavior. We derive coordinate ascent variational inference (CAVI) updates via a second-order delta method and develop a stochastic variational inference algorithm based on a Monte Carlo surrogate Rényi lower bound. In simulations, our two methods perform comparably to state-of-the-art Bayesian variable selection procedures across a range of sparsity configurations and $α$ values for both variable selection and estimation, and our numerical results illustrate how different choices of $α$ can be advantageous in different sparsity configurations.

</details>


### [32] [Comparing Two Proxy Methods for Causal Identification](https://arxiv.org/abs/2512.00175)
*Helen Guo,Elizabeth L. Ogburn,Ilya Shpitser*

Main category: stat.ME

TL;DR: 对比两种代理变量方法：桥方程法通过解积分方程恢复因果目标，阵列分解法通过特征空间唯一性恢复潜在因子，分析两者模型假设和适用范围


<details>
  <summary>Details</summary>
Motivation: 在存在未测量变量的情况下识别因果效应是因果推断的基本挑战，代理变量方法为此提供了强大解决方案。需要对比该框架下的两种主要方法，阐明其假设和适用范围

Method: 对比分析两种代理变量方法：1) 桥方程法：利用积分方程的解恢复因果目标；2) 阵列分解法：通过特征空间的唯一确定性恢复构成反事实量的潜在因子

Result: 对比了两种方法的模型限制，提供了对基础假设含义的深入理解，澄清了每种方法的适用范围

Conclusion: 桥方程法和阵列分解法是代理变量框架下两种主要方法，各有不同的模型假设和适用场景，理解这些差异有助于在实际因果推断问题中选择合适的方法

Abstract: Identifying causal effects in the presence of unmeasured variables is a fundamental challenge in causal inference, for which proxy variable methods have emerged as a powerful solution. We contrast two major approaches in this framework: (1) bridge equation methods, which leverage solutions to integral equations to recover causal targets, and (2) array decomposition methods, which recover latent factors composing counterfactual quantities by exploiting unique determination of eigenspaces. We compare the model restrictions underlying these two approaches and provide insight into implications of the underlying assumptions, clarifying the scope of applicability for each method.

</details>


### [33] [A Scalable Variational Bayes Approach for Fitting Non-Conjugate Spatial Generalized Linear Mixed Models via Basis Expansions](https://arxiv.org/abs/2512.00895)
*Jin Hyung Lee,Ben Seiyon Lee*

Main category: stat.ME

TL;DR: 提出了一种结合半隐式变分推理与空间广义线性混合模型的可扩展变分框架，用于处理大规模非高斯空间数据，在保持预测准确性的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着环境监测、生态学和遥感领域大规模非高斯响应空间数据的增加，传统的贝叶斯推断方法面临挑战。MCMC方法计算成本过高，现有变分贝叶斯方法依赖共轭性或强近似，限制了应用范围且可能低估后验方差。

Method: 提出了一种可扩展的变分框架，将半隐式变分推理（SIVI）与空间广义线性混合模型（SGLMMs）的基表示相结合。该方法不依赖共轭性，适用于伽马、负二项、泊松、伯努利和高斯响应，处理连续空间域上的大规模数据。

Result: 在包含50,000个位置的20个模拟场景中，SIVI方法在预测准确性和后验分布方面与Metropolis-Hastings和Hamiltonian Monte Carlo相当，同时提供了显著的计算加速。在MODIS地表温度和蓝松鸦丰度数据上的应用进一步证明了该方法对大规模非高斯空间数据集的有效性。

Conclusion: 提出的SIVI框架为大规模非高斯空间数据提供了可扩展的贝叶斯推断解决方案，在保持统计准确性的同时显著提高了计算效率，解决了现有方法在可扩展性和准确性方面的局限性。

Abstract: Large spatial datasets with non-Gaussian responses are increasingly common in environmental monitoring, ecology, and remote sensing, yet scalable Bayesian inference for such data remains challenging. Markov chain Monte Carlo (MCMC) methods are often prohibitive for large datasets, and existing variational Bayes methods rely on conjugacy or strong approximations that limit their applicability and can underestimate posterior variances. We propose a scalable variational framework that incorporates semi-implicit variational inference (SIVI) with basis representations of spatial generalized linear mixed models (SGLMMs), which may not have conjugacy. Our approach accommodates gamma, negative binomial, Poisson, Bernoulli, and Gaussian responses on continuous spatial domains. Across 20 simulation scenarios with 50,000 locations, SIVI achieves predictive accuracy and posterior distributions comparable to Metropolis-Hastings and Hamiltonian Monte Carlo while providing notable computational speedups. Applications to MODIS land surface temperature and Blue Jay abundance further demonstrate the utility of the approach for large non-Gaussian spatial datasets.

</details>


### [34] [On Statistical Inference for High-Dimensional Binary Time Series](https://arxiv.org/abs/2512.00338)
*Dehao Dai,Yunyi Zhang*

Main category: stat.ME

TL;DR: 提出了一种用于高维广义二元向量自回归过程的系数矩阵估计方法，包括后选择估计器、高斯近似定理和二阶wild bootstrap算法，用于统计推断。


<details>
  <summary>Details</summary>
Motivation: 近年来，对非实值数据（如二元时间序列）的分析引起了广泛关注。高维二元向量自回归过程在金融、医疗等领域有重要应用，但现有方法在处理高维情况下的系数估计和统计推断方面存在挑战。

Method: 1. 提出后选择估计器用于估计高维广义二元向量自回归过程的系数矩阵；2. 建立该估计器的高斯近似定理；3. 引入二阶wild bootstrap算法进行系数矩阵的统计推断。

Result: 数值研究和实证应用表明，所提方法具有良好的有限样本性能，能够有效处理高维二元时间序列的系数估计和统计推断问题。

Conclusion: 该研究为高维广义二元向量自回归过程提供了一种有效的后选择估计和统计推断框架，解决了非实值时间序列分析中的重要问题，具有理论和实用价值。

Abstract: The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method.

</details>


### [35] [Finite-Sample Valid Rank Confidence Sets for a Broad Class of Statistical and Machine Learning Models](https://arxiv.org/abs/2512.00316)
*Onrina Chandra,Min-ge Xie*

Main category: stat.ME

TL;DR: 提出Repro Samples方法用于估计群体排名的置信集，解决传统方法因离散参数和中心极限定理不适用而难以量化排名不确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 基于样本估计的机构排名存在不确定性，特别是当潜在特征区分度不高时，许多排名估计可能错误排序。由于排名参数是离散的且中心极限定理不适用，量化这种不确定性具有挑战性。

Method: 提出Repro Samples方法，通过开发真实未观测群体排名的置信集来解决这一推断问题。该方法提供有限样本覆盖保证，适用于各种排名问题。

Result: 通过模拟研究和真实数据示例（包括传统统计模型和现代数据科学算法）验证了该方法的有效性，并与几种已发表的大样本排名方法进行了比较。

Conclusion: Repro Samples方法为解决排名推断问题提供了有效的解决方案，能够量化排名不确定性并帮助缓解错误排序问题，为决策提供更全面的信息。

Abstract: Ranking populations such as institutions based on certain characteristics is often of interest, and these ranks are typically estimated using samples drawn from the populations. Due to sample randomness, it is important to quantify the uncertainty associated with the estimated ranks. This becomes crucial when latent characteristics are poorly separated and where many rank estimates may be incorrectly ordered. Understanding uncertainty can help quantify and mitigate these issues and provide a fuller picture. However, this task is especially challenging because the rank parameters are discrete and the central limit theorem does not apply to the rank estimates. In this article, we propose a Repro Samples Method to address this nontrivial inference problem by developing a confidence set for the true, unobserved population ranks. This method provides finite-sample coverage guarantees and is broadly applicable to ranking problems. The effectiveness of the method is illustrated and compared with several published large sample ranking approaches using simulation studies and real data examples involving samples both from traditional statistical models and modern data science algorithms.

</details>


### [36] [Detecting Model Misspecification in Bayesian Inverse Problems via Variational Gradient Descent](https://arxiv.org/abs/2512.01667)
*Qingyang Liu,Matthew A. Fisher,Zheyang Shen,Katy Tant,Xuebin Zhao,Andrew Curtis,Chris. J. Oates*

Main category: stat.ME

TL;DR: 提出一种通过比较标准贝叶斯后验与预测导向(PrO)后验来检测模型错误设定的方法，并提供了基于变分梯度下降的高效数值算法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断在模型设定正确时是最优的，但在模型错误设定时可能灾难性失败。现有的后贝叶斯方法需要一种能够自动检测模型错误设定的实用方法。

Method: 提出预测导向(PrO)方法，将统计模型提升为无限混合模型，通过最小化熵正则化目标函数来拟合预测分布。通过比较标准贝叶斯后验与PrO后验来检测模型错误设定，并开发了基于变分梯度下降的高效数值算法。

Result: 仿真研究和地震学贝叶斯反问题的详细案例研究证实，该框架能够自动检测模型错误设定。

Conclusion: 通过比较标准贝叶斯后验与PrO后验，可以有效地检测模型错误设定，为后贝叶斯方法提供了实用的模型诊断工具。

Abstract: Bayesian inference is optimal when the statistical model is well-specified, while outside this setting Bayesian inference can catastrophically fail; accordingly a wealth of post-Bayesian methodologies have been proposed. Predictively oriented (PrO) approaches lift the statistical model $P_θ$ to an (infinite) mixture model $\int P_θ\; \mathrm{d}Q(θ)$ and fit this predictive distribution via minimising an entropy-regularised objective functional. In the well-specified setting one expects the mixing distribution $Q$ to concentrate around the true data-generating parameter in the large data limit, while such singular concentration will typically not be observed if the model is misspecified. Our contribution is to demonstrate that one can empirically detect model misspecification by comparing the standard Bayesian posterior to the PrO `posterior' $Q$. To operationalise this, we present an efficient numerical algorithm based on variational gradient descent. A simulation study, and a more detailed case study involving a Bayesian inverse problem in seismology, confirm that model misspecification can be automatically detected using this framework.

</details>


### [37] [High-dimensional Autoregressive Modeling for Time Series with Hierarchical Structures](https://arxiv.org/abs/2512.00508)
*Lan Li,Shibo Yu,Yingzhou Wang,Guodong Li*

Main category: stat.ME

TL;DR: 提出一种监督因子建模框架，利用层次结构信息进行高维时间序列分析，通过顺序提取低维特征来适应一般层次结构


<details>
  <summary>Details</summary>
Motivation: 高维时间序列常呈现张量表示的层次结构，但现有方法难以有效利用这种结构信息，需要开发能处理层次结构并允许结构不纯的统计方法

Method: 提出监督因子建模框架，按尊重层次结构的模式顺序顺序提取低维特征，选择少量顺序以允许层次结构中的不纯，得到保持层次关系的可解释载荷矩阵

Result: 推导了回归和自回归设置中估计量的非渐近误差界，在IPIP-NEO-120人格面板应用中显示出优于现有张量分解和层次因子分析方法的预测性能和结构解释性

Conclusion: 该方法能有效利用高维时间序列的层次结构信息，提供更好的预测性能和更清晰的结构解释，为处理具有层次结构的数据提供了实用框架

Abstract: High-dimensional time series often exhibit hierarchical structures represented by tensors, while statistical methodologies that can effectively exploit the structural information remain limited. We propose a supervised factor modeling framework that accommodates general hierarchical structures by extracting low-dimensional features sequentially in the mode orders that respect the hierarchical structure. Our method can select a small collection of such orders to allow for impurities in the hierarchical structures, yielding interpretable loading matrices that preserve the hierarchical relationships. A practical estimation procedure is proposed, with a hyperparameter selection scheme that identifies a parsimonious set of action orders and interim ranks, thereby revealing the possibly latent hierarchical structures. Theoretically, non-asymptotic error bounds are derived for the proposed estimators in both regression and autoregressive settings. An application to the IPIP-NEO-120 personality panel illustrates superior forecasting performance and clearer structural interpretation compared with existing methods based on tensor decompositions and hierarchical factor analysis.

</details>


### [38] [Testing similarity of competing risks models by comparing transition probabilities](https://arxiv.org/abs/2512.00583)
*Zoe Kristin Lange,Maryam Farhadizadeh,Holger Dette,Nadine Binder*

Main category: stat.ME

TL;DR: 提出一种基于转移概率的竞争风险模型相似性检验框架，用于评估患者群体事件动态的相似性


<details>
  <summary>Details</summary>
Motivation: 评估患者群体是否具有可比的事件动态对于治疗等效性评估、跨队列数据合并以及跨医院或策略的临床路径比较至关重要

Method: 定义两个多状态过程转移概率矩阵之间的最大型距离，并采用新颖的约束参数自助法检验来评估相似性，支持管理和随机右删失

Result: 理论证明了自助法检验的渐近有效性和一致性；模拟研究表明方法可靠地控制I类错误，比现有基于强度的方法具有更高的统计功效；在前列腺癌患者数据应用中确定了相似性阈值

Conclusion: 该方法为量化事件历史模型的相似性提供了一个稳健且可解释的工具

Abstract: Assessing whether two patient populations exhibit comparable event dynamics is essential for evaluating treatment equivalence, pooling data across cohorts, or comparing clinical pathways across hospitals or strategies. We introduce a statistical framework for formally testing the similarity of competing risks models based on transition probabilities, which represent the cumulative risk of each event over time. Our method defines a maximum-type distance between the transition probability matrices of two multistate processes and employs a novel constrained parametric bootstrap test to evaluate similarity under both administrative and random right censoring. We theoretically establish the asymptotic validity and consistency of the bootstrap test. Through extensive simulation studies, we show that our method reliably controls the type I error and achieves higher statistical power than existing intensity-based approaches. Applying the framework to routine clinical data of prostate cancer patients treated with radical prostatectomy, we identify the smallest similarity threshold at which patients with and without prior in-house fusion biopsy exhibit comparable readmission dynamics. The proposed method provides a robust and interpretable tool for quantifying similarity in event history models.

</details>


### [39] [Maximum Likelihood Estimation of the Vector AutoRegressive To Anything (VARTA) model](https://arxiv.org/abs/2512.00631)
*Jonas Andersson,Dimitris Karlis*

Main category: stat.ME

TL;DR: 提出VARTA模型，通过转换将潜在高斯VAR模型扩展到任意分布的多变量时间序列，并推导最大似然估计器


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列模型要么基于多元高斯分布，要么针对特定应用开发，缺乏通用方法。需要一种能同时捕捉时间动态和分布特性的通用框架

Method: 基于潜在未观测的高斯向量自回归(VAR)模型，通过转换函数将高斯分布映射到任意分布，构建VARTA模型。推导最大似然估计器，提供诊断分析和预测分布计算方法

Result: VARTA模型能提供比标准VAR模型更好的预测分布估计，适用于任意分布类型，不限于高斯分布

Conclusion: VARTA模型为多变量时间序列分析提供了通用框架，能同时捕捉时间动态和分布特性，扩展了传统VAR模型的应用范围

Abstract: The literature on multivariate time series is, largely, limited to either models based on the multivariate Gaussian distribution or models specifically developed for a given application. In this paper we develop a general approach which is based on an underlying, unobserved, Gaussian Vector Autoregressive (VAR) model. Using a transformation, we can capture the time dynamics as well as the distributional properties of a multivariate time series. The model is called the Vector AutoRegressive To Anyting (VARTA) model and was originally presented by Biller and Nelson (2003) who used it for the purpose of simulation. In this paper we derive a maximum likelihood estimator for the model and investigate its performance. We also provide diagnostic analysis and how to compute the predictive distribution. The proposed approach can provide better estimates about the forecasting distributions which can be of every kind not necessarily Gaussian distributions as for the standard VAR models.

</details>


### [40] [NOVA: Coordinated Test Selection and Bayes-Optimized Constrained Randomization for Accelerated Coverage Closure](https://arxiv.org/abs/2512.00688)
*Weijie Peng,Nanbing Li,Jin Luo,Shuai Wang,Yihui Li,Jun Fang,Yun,Liang*

Main category: stat.ME

TL;DR: NOVA框架通过协调覆盖感知测试选择与贝叶斯优化的约束随机化，实现功能验证加速，无需人工启发式规则。


<details>
  <summary>Details</summary>
Motivation: 传统功能验证存在两大问题：1) 基于静态特征的测试选择忽略实际覆盖行为，浪费大量仿真时间；2) 约束随机激励生成依赖难以设计且通常无效的人工分布。

Method: NOVA框架包含两个核心组件：1) 提取细粒度覆盖特征来过滤冗余测试；2) 修改约束求解器以暴露参数化决策策略，通过贝叶斯优化调整参数设置来最大化覆盖增长。

Result: 在多个RTL设计中，NOVA实现了高达2.82倍的覆盖收敛加速，且不需要人工设计的启发式规则。

Conclusion: NOVA通过协调测试选择和激励生成，有效解决了传统功能验证的效率问题，实现了自动化的高效覆盖收敛。

Abstract: Functional verification relies on large simulation-based regressions. Traditional test selection relies on static test features and overlooks actual coverage behavior, wasting substantial simulation time, while constrained random stimuli generation depends on manually crafted distributions that are difficult to design and often ineffective. We present NOVA, a framework that coordinates coverage-aware test selection with Bayes-optimized constrained randomization. NOVA extracts fine-grained coverage features to filter redundant tests and modifies the constraint solver to expose parameterized decision strategies whose settings are tuned via Bayesian optimization to maximize coverage growth. Across multiple RTL designs, NOVA achieves up to a 2.82$\times$ coverage convergence speedup without requiring human-crafted heuristics.

</details>


### [41] [Grouped Competition Test with Unified False Discovery Rate Control](https://arxiv.org/abs/2512.00901)
*Mingzhou Deng,Yan Fu*

Main category: stat.ME

TL;DR: 提出基于竞争测试的统一框架，通过分组校正和结果整合处理异质性和依赖性问题，在控制FDR的同时保持较高检验功效


<details>
  <summary>Details</summary>
Motivation: 现有竞争测试方法在控制FDR方面有效，但难以处理具有强异质性或依赖结构的数据，需要开发更灵活的方法

Method: 提出统一竞争测试框架，对具有特定结构的数据进行分组校正，然后整合各组结果，利用竞争测试的优良特性证明全局FDR控制

Result: 通过模拟实验和质谱数据分析验证了方法的灵活性和有效性，校正参数虽可能导致轻微功效损失，但损失通常很小

Conclusion: 提出的分组校正整合方法能够有效处理异质性和依赖性问题，在控制FDR的同时保持较高检验功效，为多重假设检验提供了更灵活的解决方案

Abstract: This paper discusses several p-value-free multiple hypothesis testing methods proposed in recent years and organizes them by introducing a unified framework termed competition test. Although existing competition tests are effective in controlling the False Discovery Rate (FDR), they struggle with handling data with strong heterogeneity or dependency structures. Based on this framework, the paper proposes a novel approach that applies a corrected competition procedure to group data with certain structure, and then integrates the results from each group. Using the favorable properties of competition test, the paper proposes a theorem demonstrating that this approach controls the global FDR. We further show that although the correction parameters may lead to a slight loss in power, such loss is typically minimal. Through simulation experiments and mass spectrometry data analysis, we illustrate the flexibility and efficacy of our approach.

</details>


### [42] [An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts](https://arxiv.org/abs/2512.00916)
*Sotirios D. Nikolopoulos*

Main category: stat.ME

TL;DR: 提出RES（稀有事件稳定）指标家族，解决传统指标在稀有事件预测评估中的阈值退化问题，确保在极端稀有情况下仍能保持稳定的决策阈值。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如F1分数、AUPRC、MCC、准确率）在稀有事件预测中存在严重问题：随着事件发生率的降低，这些指标的阈值会退化到0或1，导致评估结果被类别不平衡主导而非尾部判别能力。

Method: 开发了RES（稀有事件稳定）指标家族，这些指标的最优阈值在事件概率趋近于零时仍能保持严格内部值，确保在极端稀有情况下的一致决策规则。通过模拟实验（事件概率从0.01到百万分之一）验证其稳定性。

Result: RES指标在模拟中保持稳定阈值、一致的模型排名和近乎完全的流行度不变性，而传统指标表现出显著的阈值漂移和结构崩溃。在信用违约应用中，RES指标产生可解释的违约概率阈值（4-9%）并在子采样下保持稳健。

Conclusion: RES框架为评估极端风险预测提供了原则性、流行度不变的基础，解决了传统指标在稀有事件评估中的操作失败问题。

Abstract: Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts.

</details>


### [43] [Correlated Confounding Variables Are Not Easily Controlled for in Large Survey Research](https://arxiv.org/abs/2512.01003)
*William H. Press*

Main category: stat.ME

TL;DR: 论文通过两个例子展示线性回归在控制混杂变量时的局限性：第一个例子用真实调查数据"证明"了一个极不可能的假设，尽管控制了20多个混杂变量；第二个例子通过模拟显示，即使控制越来越多的混杂变量，潜在变量仍会导致虚假关联。


<details>
  <summary>Details</summary>
Motivation: 流行病学和社会科学中常用线性回归控制混杂变量来测量变量间的相关性，但无法确定是否所有可能的混杂变量都被识别或是否都是可观测的（而非潜在的）。本文旨在揭示这一根本问题。

Method: 采用两个示例方法：1）从大型调查数据中选取极不可能的假设，并控制20多个混杂变量进行回归分析；2）构建"元模型"，模拟单个潜在变量影响多个相互关联的混杂变量的情况，通过模拟推导公式。

Result: 第一个例子成功"证明"了极不可能的假设，表明即使控制大量混杂变量，虚假关联仍可能存在。第二个例子通过模拟发现，随着被控制的混杂变量数量增加，虚假关联的强度仍会持续存在。

Conclusion: 线性回归方法在控制混杂变量方面存在根本局限性，即使控制大量混杂变量，未识别的潜在变量仍可能导致虚假关联。这些例子应作为警示，提醒研究者谨慎解释回归分析结果。

Abstract: Results in epidemiology and social science often require the removal of confounding effects from measurements of the pairwise correlation of variables in survey data. This is typically accomplished by some variant of linear regression (e.g., ``logistic" or ``Cox proportional"). But, knowing whether all possible confounders have been identified, or are even visible (not latent), is in general impossible. Here, we exhibit two examples that frame the issue. The first example proposes a highly unlikely hypothesis on drug use, draws data from a large, respected survey, and succeeds in ``proving" the implausible hypothesis, despite regressing out more than 20 confounding variables. The second constructs a ``metamodel" in which a single (by hypothesis unmeasurable) latent variable affects many mutually correlated confounders. From simulations, we derive formulas for the magnitude of spurious association that persists even as increasing numbers of confounders are regressed out. The intent of these examples is for them to serve as cautionary tales.

</details>


### [44] [The Dual Wavelet Spectra: An Alternative Perspective on Hurst Exponent Estimation with Application to Mammogram Classification](https://arxiv.org/abs/2512.00996)
*Raymond J. Hinton,,Pepa Ramírez Cobo,Brani Vidakovic*

Main category: stat.ME

TL;DR: 提出基于"对偶"小波谱的Hurst指数新估计器，通过反转标准小波谱建立斜率与Hurst指数的关系，并应用于乳腺癌检测的特征提取。


<details>
  <summary>Details</summary>
Motivation: 传统小波谱方法通过分解层级与能量值的关系估计Hurst指数，本文扩展"对偶"小波谱概念，将分解层级视为能量值的函数，探索其斜率与Hurst指数的关系，以开发新的估计器。

Method: 提出对偶小波谱方法，通过反转标准小波谱建立分解层级作为能量值函数的数学关系，推导其斜率与Hurst指数的关联，构建新的估计器。通过模拟研究验证估计器效果和参数敏感性，并应用于乳腺癌检测的特征提取。

Result: 模拟研究表明新估计器有效且对参数设置敏感。应用于乳腺X光图像乳腺癌检测时，对偶小波谱特征对癌症的对数几率具有统计显著性影响。

Conclusion: 对偶小波谱方法为Hurst指数估计提供了新途径，同时作为有效的特征提取技术，在医学图像分析中展现出应用潜力，特别是在乳腺癌检测任务中表现出统计显著性。

Abstract: The wavelet spectra is a common starting point for estimating the Hurst exponent of a self-similar signal using wavelet-based techniques. The decay of the $\log_2$ average energy of the detail wavelet coefficients as a function of the level of signal decomposition can be used to construct estimators for this parameter. In this paper, we expand on previous work which introduced the ``dual" wavelet spectra, where decomposition levels are instead treated as a function of energy values, and propose a relationship between its slope and the Hurst exponent by inverting the standard wavelet spectra, thereby creating a new estimator. The effectiveness of this estimator and its sensitivity to several settings are demonstrated through a simulation study. Finally, we show how the technique performs as a feature extraction method by applying it to the task of detecting the presence of breast cancer in mammogram images. Dual spectra wavelet features had a statistically significant effect on the log-odds of Cancer.

</details>


### [45] [A mixture of distributed lag non-linear models to account for spatially heterogeneous exposure-lag-response associations](https://arxiv.org/abs/2512.01508)
*Álvaro Briz-Redón,Ana Corberán-Vallet,Adina Iftimi,Carmen Íñiguez*

Main category: stat.ME

TL;DR: DLNM-Clust：一种贝叶斯混合DLNM模型，通过空间聚类捕捉环境暴露-滞后-响应关系的空间异质性，应用于比利时空气污染与COVID-19发病率的关系研究。


<details>
  <summary>Details</summary>
Motivation: 传统分布式滞后非线性模型（DLNM）假设暴露-滞后-响应关系在研究区域内是均匀的，忽略了空间异质性，可能导致风险估计偏差。需要开发能够捕捉空间差异的建模方法。

Method: 提出DLNM-Clust：一种DLNM混合模型，在贝叶斯框架下，将每个地理单元概率分配到C个潜在空间聚类中，每个聚类由不同的DLNM规范定义，从而捕捉暴露-滞后-响应曲面的共同模式和局部偏差。

Result: 使用比利时市镇级时间序列数据，分析空气污染与COVID-19发病率的关系，展示了该方法在捕捉空间异质性方面的有效性，强调了空间感知建模策略的重要性。

Conclusion: DLNM-Clust为环境流行病学提供了空间感知的建模工具，能够进行区域特异性风险评估，支持针对性公共卫生干预措施的制定。

Abstract: Environmental exposures, such as air pollution and extreme temperatures, have complex effects on human health. These effects are often characterized by non-linear exposure-lag-response relationships and delayed impacts over time. Accurately capturing these dynamics is crucial for informing public health interventions. The Distributed Lag Non-Linear Model (DLNM) is a flexible statistical framework for estimating such effects in epidemiological research. However, standard DLNM implementations typically assume a homogeneous exposure-lag-response association across the study region, overlooking potential spatial heterogeneity, which can lead to biased risk estimates. To address this limitation, we introduce DLNM-Clust: a novel mixture of DLNMs that extends the traditional DLNM. Within a Bayesian framework, DLNM-Clust probabilistically assigns each geographic unit to one of $C$ latent spatial clusters, each of which is defined by a distinct DLNM specification. This approach allows capturing both common patterns and singular deviations in the exposure-lag-response surface. We demonstrate the method using municipality-level time-series data on the relationship between air pollution and the incidence of COVID-19 in Belgium. Our results emphasize the importance of spatially aware modeling strategies in environmental epidemiology, facilitating region-specific risk assessment and supporting the development of targeted public health initiatives.

</details>


### [46] [Weight a Minute: Understanding Variability in PATE Estimates Across Target Populations](https://arxiv.org/abs/2512.01157)
*William Stewart,Carly L. Brantner,Elizabeth A. Stuart,Laine Thomas*

Main category: stat.ME

TL;DR: 当研究人群与目标人群存在差异时，IPSW加权方法可用于估计总体平均处理效应，但如果目标人群数据集选择不当，即使正确指定IPSW也可能引入比不加权更大的偏差。


<details>
  <summary>Details</summary>
Motivation: 临床研究人群通常与目标推广人群存在显著差异，虽然IPSW等加权方法可以重新加权研究参与者以匹配目标人群，但这些估计的准确性高度依赖于所选人群是否能代表实际感兴趣的人群。

Method: 基于多个真实世界数据源的协变量分布进行模拟研究，涵盖从高度选择性到广泛包容性的人群连续体。在不同效应修正水平下评估多个候选目标人群的IPSW估计器，量化当目标人群数据集与预期推断人群不同时产生的偏差。

Result: 随着目标人群与代表性良好的人群差异增大，偏差系统性增加；当加权到匹配不佳的目标人群时，可能比完全不加权引入更大的偏差。

Conclusion: 选择适当的目标人群数据集是确保有效推广的关键设计决策，研究人员需要谨慎考虑目标人群的代表性以避免引入偏差。

Abstract: Clinical study populations often differ meaningfully from the broader populations to which results are intended to generalize. Weighting methods such as inverse probability of sampling weights (IPSW) reweight study participants to resemble a target population, but the accuracy of these estimates depends heavily on how well the chosen population represents the population of substantive interest. We conduct a simulation study grounded in empirical covariate distributions from several real-world data sources spanning a continuum from highly selective to broadly inclusive populations. Using treatment effect scenarios with varying levels of effect modification, we evaluate IPSW estimators of the population average treatment effect (PATE) across multiple candidate target populations. We quantify the bias that arises when the dataset used to operationalize the target population differs from the intended inference population, even when IPSW is correctly specified. Our results show that bias increases systematically as target populations diverge from a well-representative population, and that weighting to a poorly aligned target can introduce more bias than not weighting at all. These findings highlight that selecting an appropriate target population dataset is a critical design choice for valid generalization.

</details>


### [47] [Gaussian Process State-Space Modeling and Particle Filtering for Time Series Decomposition and Nonlinear Signal Extraction](https://arxiv.org/abs/2512.01162)
*Genshiro Kitagawa*

Main category: stat.ME

TL;DR: 本文开发了高斯过程状态空间模型的粒子滤波框架，并与卡尔曼滤波在趋势提取和季节调整方面进行比较，展示了GP-SSM在复杂时间序列分析中的优势。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统需要比线性高斯模型更灵活的表达能力，高斯过程状态空间模型提供了非线性或难以参数化建模的时间序列动态的非参数替代方案。

Method: 开发了GP-SSM的粒子滤波框架，结合高斯过程建模和顺序蒙特卡洛方法，用于贝叶斯状态估计，即使后验分布偏离高斯性。

Result: GP-SSM在趋势提取、季节调整和非线性信号提取任务中表现优异，能够恢复尖锐或非对称动态下的潜在状态。

Conclusion: 结合高斯过程建模和顺序蒙特卡洛方法对于复杂时间序列分析具有重要实用价值，GP-SSM为非线性动态系统提供了有效的建模框架。

Abstract: Gaussian-process state-space models (GP-SSMs) provide a flexible nonparametric alternative for modeling time-series dynamics that are nonlinear or difficult to specify parametrically. While the Kalman filter is effective for linear-Gaussian trend and seasonal components, many real-world systems require more expressive representations. GP-SSMs address this need by learning transition functions directly from data, while particle filtering enables Bayesian state estimation even when posterior distributions deviate from Gaussianity. This paper develops a particle-filtering framework for GP-SSM inference and compares its performance with the Kalman filter in trend extraction and seasonal adjustment. We further evaluate nonlinear signal-extraction tasks, demonstrating that GP-SSMs can recover latent states under sharp or asymmetric dynamics. The results highlight the utility of combining GP modeling with sequential Monte Carlo methods for complex time-series analysis.

</details>


### [48] [The Dynamical Model Representation of Convolution-Generated Spatio-Temporal Gaussian Processes and Its Applications](https://arxiv.org/abs/2512.01279)
*Yutong Zhang,Xiao Liu*

Main category: stat.ME

TL;DR: 提出了一种卷积生成时空高斯过程的动态模型表示方法，通过无限维线性状态空间表示和Galerkin方法获得有限维近似，便于计算和参数估计，特别适用于过程建模、监测和异常检测。


<details>
  <summary>Details</summary>
Motivation: 卷积生成的时空模型通过空间和时间上的卷积操作产生重要的非可分平稳高斯过程，与随机偏微分方程有紧密联系，为科学和工程过程提供物理解释。然而，需要更有效的计算和参数估计方法。

Method: 首先获得无限维线性状态空间表示，其中状态转移由随机微分方程控制；然后使用Galerkin方法对无限维SDE进行有限维近似，得到便于计算的状态空间模型；推导近似模型的时空协方差并量化与精确协方差的误差。

Result: 通过仿真研究验证了模型性能，并应用于2025年洛杉矶野火期间的遥感气溶胶数据。模型在监测时空过程的一阶时间导数方面特别有效，适用于过程建模、监测和异常检测问题。

Conclusion: 提出的动态模型表示方法为卷积生成的时空高斯过程提供了有效的计算框架，特别在过程导数监测方面表现优异，代码和数据已公开。

Abstract: Convolution-generated space-time models yield an important class of non-separable stationary Gaussian Processes (GP) through a sequence of convolution operations, in both space and time, on spatially correlated Brownian motion with a Gaussian convolution kernel. Because of its solid connection to stochastic partial differential equations, such a modeling approach offers strong physical interpretations when it is applied to scientific and engineering processes. In this paper, we obtain a new dynamical model representation for convolution-generated spatio-temporal GP. In particular, an infinite-dimensional linear state-space representation is firstly obtained where the state transition is governed by a stochastic differential equation (SDE) whose solution has the same space-time covariance as the original convolution-generated process. Then, using the Galerkin's method, a finite-dimension approximation to the infinite-dimensional SDE is obtained, yielding a dynamical model with finite states that facilitates the computation and parameter estimation. The space-time covariance of the approximated dynamical model is obtained, and the error between the approximate and exact covariance matrices is quantified. We investigate the performance of the proposed model through a simulation-based study, and apply the approach to a real case study utilizing the remote-sensing aerosol data during the recent 2025 Los Angeles wildfire. The modeling capability of the proposed approach has been well demonstrated, and the proposed approach is found particularly effective in monitoring the first-order time derivative of the underlying space-time process, making it a good candidate for process modeling, monitoring and anomaly detection problems. Computer code and data have been made publicly available.

</details>


### [49] [Inferring Dynamic Hidden Graph Structure in Heterogeneous Correlated Time Series](https://arxiv.org/abs/2512.01301)
*Jeshwanth Mohan,Bharath Ramsundar,Sandya Subramanian*

Main category: stat.ME

TL;DR: 提出窗口化方差-相关性度量(WVC)来量化信号间时变相关性，直接恢复指定时间间隔内的隐藏关系作为加权邻接矩阵，从而推断隐藏的动态图结构。


<details>
  <summary>Details</summary>
Motivation: 建模异质相关时间序列需要能够学习具有可能变化周期性和生成过程的分量时间序列之间的隐藏动态关系。

Method: 提出窗口化方差-相关性度量(WVC)，通过计算信号间时变相关性，将隐藏关系恢复为加权邻接矩阵，从而推断动态图结构。

Result: 在模拟数据上，该方法能够捕捉其他方法遗漏的相关性，扩展了在单一凝聚动态图模型中学习显著不同信号间动态图结构的能力。

Conclusion: WVC方法能够有效量化时变相关性，恢复隐藏的动态图结构，特别适用于处理具有不同周期性和生成过程的异质时间序列。

Abstract: Modeling heterogeneous correlated time series requires the ability to learn hidden dynamic relationships between component time series with possibly varying periodicities and generative processes. To address this challenge, we formulate and evaluate a windowed variance-correlation metric (WVC) designed to quantify time-varying correlations between signals. This method directly recovers hidden relationships in an specified time interval as a weighted adjacency matrix, consequently inferring hidden dynamic graph structure. On simulated data, our method captures correlations that other methods miss. The proposed method expands the ability to learn dynamic graph structure between significantly different signals within a single cohesive dynamical graph model.

</details>


### [50] [Convolution-smoothing based locally sparse estimation for functional quantile regression](https://arxiv.org/abs/2512.01341)
*Hua Liu,Boyi Hu,Jinhong You,Jiguo Cao*

Main category: stat.ME

TL;DR: 提出了一种稀疏半参数函数分位数模型（CLoSE方法），用于研究温度、降水和灌溉对大豆产量的影响，能同时进行变量选择、非零区域识别和系数估计。


<details>
  <summary>Details</summary>
Motivation: 研究温度、降水和灌溉对大豆产量的影响，需要处理函数型协变量（如每日温度）在特定时间区域对响应的局部影响，同时需要处理分位数回归的优化挑战。

Method: 提出卷积平滑的局部稀疏估计（CLoSE）方法：1）使用稀疏半参数函数分位数模型；2）通过凹惩罚处理分位数损失函数优化；3）同时进行变量选择、非零区域识别和系数估计；4）提出分割野自助法构建置信区间。

Result: 建立了函数型oracle性质、同时置信带和参数估计的渐近正态性；证明了分割野自助法的一致性；模拟研究验证了有限样本性能；实际应用识别了影响大豆产量的温度活跃时间区域。

Conclusion: CLoSE方法能有效处理函数型协变量的局部稀疏效应，在理论和计算上都有良好表现，适用于环境因素对农作物产量的影响分析。

Abstract: Motivated by an application to study the impact of temperature, precipitation and irrigation on soybean yield, this article proposes a sparse semi-parametric functional quantile model. The model is called ``sparse'' because the functional coefficients are only nonzero in the local time region where the functional covariates have significant effects on the response under different quantile levels. To tackle the computational and theoretical challenges in optimizing the quantile loss function added with a concave penalty, we develop a novel Convolution-smoothing based Locally Sparse Estimation (CLoSE) method, to do three tasks in one step, including selecting significant functional covariates, identifying the nonzero region of functional coefficients to enhance the interpretability of the model and estimating the functional coefficients. We establish the functional oracle properties and simultaneous confidence bands for the estimated functional coefficients, along with the asymptotic normality for the estimated parameters. In addition, because it is difficult to estimate the conditional density function given the scalar and functional covariates, we propose the split wild bootstrap method to construct the confidence interval of the estimated parameters and simultaneous confidence band for the functional coefficients. We also establish the consistency of the split wild bootstrap method. The finite sample performance of the proposed CLoSE method is assessed with simulation studies. The proposed model and estimation procedure are also illustrated by identifying the active time regions when the daily temperature influences the soybean yield.

</details>


### [51] [Active Hypothesis Testing under Computational Budgets with Applications to GWAS and LLM](https://arxiv.org/abs/2512.01423)
*Qi Kuang,Bowen Gang,Yin Xia*

Main category: stat.ME

TL;DR: 提出一种主动假设检验框架，利用廉价辅助统计量分配全局计算预算，在保证p值或e值有效性的同时满足预算约束


<details>
  <summary>Details</summary>
Motivation: 在大规模假设检验中，计算精确的p值或e值通常计算成本高昂，需要开发预算感知的推断方法

Method: 基于廉价辅助统计量的数据自适应程序，概率性地决定是否计算精确检验统计量或转换后的代理统计量，保证p值/e值有效性并在期望上满足预算约束

Result: 理论证明：对于e值达到最优性，对于独立情况下的p值达到最优性，对于一般依赖情况下的p值达到可接受性。实证结果显示在固定资源限制下提高了统计效率

Conclusion: 该框架为大规模假设检验提供了一种有效的预算感知解决方案，在基因组关联研究和临床预测等实际应用中表现出优越性能

Abstract: In large-scale hypothesis testing, computing exact $p$-values or $e$-values is often resource-intensive, creating a need for budget-aware inferential methods. We propose a general framework for active hypothesis testing that leverages inexpensive auxiliary statistics to allocate a global computational budget. For each hypothesis, our data-adaptive procedure probabilistically decides whether to compute the exact test statistic or a transformed proxy, guaranteeing a valid $p$-value or $e$-value while satisfying the budget constraint in expectation. Theoretical guarantees are established for our constructions, showing that the procedure achieves optimality for $e$-values and for $p$-values under independence, and admissibility for $p$-values under general dependence. Empirical results from simulations and two real-world applications, including a large-scale genome-wide association study (GWAS) and a clinical prediction task leveraging large language models (LLM), demonstrate that our framework improves statistical efficiency under fixed resource limits.

</details>


### [52] [Model-Based Clustering of Functional Data Via Random Projection Ensembles](https://arxiv.org/abs/2512.01450)
*Matteo Mori,Laura Anderlucci*

Main category: stat.ME

TL;DR: 提出基于随机投影的函数数据聚类框架，通过降维和集成共识提升鲁棒性，使用高斯混合模型和KL散度自动选择最优投影


<details>
  <summary>Details</summary>
Motivation: 函数数据聚类面临无限维度和稳定自适应分区的挑战，需要克服单一表示的不稳定性

Method: 基于随机投影的集成聚类：1) 生成多个随机投影降维表示；2) 对每个投影独立聚类（使用高斯混合模型）；3) 按KL散度排序选择最优投影；4) 通过集成共识聚合分区结果

Result: 模拟研究和两个真实数据应用（光谱食品认证和语音对数周期图）表明该方法在函数数据聚类中表现有效

Conclusion: 提出的随机投影集成聚类框架为函数数据提供了一种有效工具，具有鲁棒性、无需预先指定聚类数、计算高效等优点

Abstract: Clustering functional data is a challenging task due to intrinsic infinite-dimensionality and the need for stable, data-adaptive partitioning. In this work, we propose a clustering framework based on Random Projections, which simultaneously performs dimensionality reduction and generates multiple stochastic representations of the original functions. Each projection is clustered independently, and the resulting partitions are then aggregated through an ensemble consensus procedure, enhancing robustness and mitigating the influence of any single projection. To focus on the most informative representations, projections are ranked according to clustering quality criteria, and only a selected subset is retained. In particular, we adopt Gaussian Mixture Models as base clusterers and employ the Kullback-Leibler divergence to order the random projections; these choices enable fast computation and eliminate the need to specify the number of clusters a priori. The performance of the proposed methodology is assessed through an extensive simulation study and two real-data applications, one from spectroscopy data for food authentication and one from log-periodograms of speech recording; the obtained results suggest that the proposal represents an effective tool for the clustering of functional data.

</details>


### [53] [Dynamic functional brain connectivity results depend on modeling assumptions: comparing frequentist and Bayesian hypothesis tests](https://arxiv.org/abs/2512.01513)
*Hester Huijsdens,Linda Geerligs,Max Hinne*

Main category: stat.ME

TL;DR: 该研究比较了检测功能脑连接动态性的频率学派和贝叶斯方法，发现建模假设对个体层面的结论有重要影响，而群体结果更稳健。


<details>
  <summary>Details</summary>
Motivation: 理解功能脑连接的时间动态性对认知和疾病研究很重要，但评估连接是真正动态还是静态存在挑战。现有滑动窗口方法需要定义超参数和分布假设，这些假设可能影响动态连接的检测。

Method: 提出基于Wishart过程的贝叶斯假设检验框架，通过先验分布编码时间依赖结构假设。使用模拟实验比较频率学派（滑动窗口）和贝叶斯方法，并应用于fMRI工作记忆任务数据。

Result: 模拟显示不同建模假设显著影响动态连接的检测。在fMRI数据分析中，个体层面的结论随建模选择而变化，而群体层面的结果更加稳健。

Conclusion: 评估动态连接时需要仔细考虑建模假设，贝叶斯框架提供更灵活的先验知识整合和证据评估，但个体层面的推断对方法选择敏感。

Abstract: Understanding the temporal dynamics of functional brain connectivity is important for addressing various questions in network neuroscience, such as how connectivity affects cognition and changes with disease. A fundamental challenge is to evaluate whether connectivity truly exhibits dynamics, or simply is static. The most common frequentist approach uses sliding-window methods to model functional connectivity over time, but this requires defining appropriate sampling distributions and hyperparameters, such as window length, which imposes specific assumptions on the dynamics. Here, we explore how these assumptions influence the detection of dynamic connectivity, and introduce an alternative approach based on Bayesian hypothesis testing with Wishart processes. This framework encodes assumptions through prior distributions, allowing prior knowledge on the time-dependent structure of connectivity to be incorporated into the model. Moreover, this framework provides evidence for both dynamic and static connectivity, offering additional information. Using simulations, we compare the frequentist and Bayesian approaches and demonstrate how different assumptions affect the detection of dynamic connectivity. Finally, by applying both approaches to an fMRI working-memory task, we find that conclusions at the individual level vary with modeling choices, while group-level results are more robust. Our work highlights the importance of carefully considering modeling assumptions when evaluating dynamic connectivity.

</details>


### [54] [The partial K function](https://arxiv.org/abs/2512.01823)
*Jake P. Grainger,Tuomas A. Rajala,David J. Murrell,Sofia C. Olhede*

Main category: stat.ME

TL;DR: 提出部分K函数，用于在分析点-点相互作用时考虑其他点类型的影响，解决传统K函数可能隐藏的依赖关系


<details>
  <summary>Details</summary>
Motivation: 传统K函数在分析空间点过程时虽然易于计算和解释，但在分析不同类型点之间的相互作用时，无法有效考虑其他点类型的影响，可能隐藏真实的依赖关系

Method: 引入部分K函数，在分析点-点相互作用时能够考虑其他点类型的影响，当其他点与感兴趣点独立时，部分K函数退化为传统K函数

Result: 部分K函数能够揭示传统K函数可能隐藏的点类型间依赖关系，通过Lansing Woods数据集验证了方法的有效性

Conclusion: 部分K函数是传统K函数的有用扩展，能够更准确地分析空间点过程中的相互作用，特别是在存在多种点类型的情况下，同时讨论了偏差校正和超参数选择等重要问题

Abstract: The K function and its related statistics have been an enduring tool in the analysis of spatial point processes, providing an easy to compute and interpret summary statistic for characterising the interactions between points of one type, or between two different types of points. In this paper, introduce a partial K function, enabling us to account for some of the effects of the other point types when analysing point-point interactions. The partial K function we introduce reduces to the usual K function when the other points are independent of the points of interest and has a similar interpretation. Using examples, we demonstrate how the partial K function can unpick dependence between point types that would otherwise be hidden in the usual K function. We also discuss important bias correction steps and hyperparameter selection. In addition, we discuss an extension to account for other spatial covariates, and demonstrate the methodology on the Lansing Woods dataset.

</details>


### [55] [A discomfort-informed adaptive Gibbs sampler for finite mixture models](https://arxiv.org/abs/2512.01847)
*Davide Fabbrico,Andi Q. Wang,Sebastiano Grazzi,Alice Corbella,Gareth O. Roberts,Sylvia Richardson,Filippo Pagani,Paul D. W. Kirk*

Main category: stat.ME

TL;DR: 提出一种自适应Gibbs采样器，通过聚焦更新可能被错误分类的观测点，提高有限混合模型中的收敛效率


<details>
  <summary>Details</summary>
Motivation: 传统Gibbs采样器在有限混合模型中进行贝叶斯推断时存在计算资源利用效率低的问题，特别是对于那些高度可能保持当前聚类的观测点进行不必要的更新

Method: 提出一种新的自适应Gibbs采样器，使用一个函数在每次迭代中利用链的历史信息，专注于更新在当前聚类中可能被错误分类的观测点（即属于当前组分的概率较低的观测点）

Result: 通过模拟研究和两个真实数据分析，实证表明该方法在收敛时间方面比现有最先进方法表现更高效

Conclusion: 提出的自适应Gibbs采样器能够有效提高有限混合模型中贝叶斯推断的收敛效率，特别适用于高维数据集中的潜在结构发现

Abstract: Finite mixture models are frequently used to uncover latent structures in high-dimensional datasets (e.g.\ identifying clusters of patients in electronic health records). The inference of such structures can be performed in a Bayesian framework, and involves the use of sampling algorithms such as Gibbs samplers aimed at deriving posterior distribution of the probabilities of observations to belong to specific clusters. Unfortunately, traditional implementations of Gibbs samplers in this context often face critical challenges, such as inefficient use of computational resources and unnecessary updates for observations that are highly likely to remain in their current cluster. This paper introduces a new adaptive Gibbs sampler that improves the convergence efficiency over existing methods. In particular, our sampler is guided by a function that, at each iteration, uses the past of the chain to focus the updating on observations potentially misclassified in the current clustering, i.e.\ those with a low probability of belonging to their current component. Through simulation studies and two real data analyses, we empirically demonstrate that, in terms of convergence time, our method tends to perform more efficiently compared to state-of-the-art approaches.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [56] [Iterated sampling importance resampling with adaptive number of proposals](https://arxiv.org/abs/2512.00220)
*Pietari Laitinen,Matti Vihola*

Main category: stat.CO

TL;DR: 本文提出了一种自适应i-SIR算法，通过近似渐近方差和广义i-SIR转移来优化提议数量N，平衡计算成本与样本独立性。


<details>
  <summary>Details</summary>
Motivation: i-SIR算法中，提议数量N的增加会提高样本独立性但增加计算成本，需要找到最优的N来平衡效率与计算开销。

Method: 基于i-SIR的近似渐近方差和具有分数"提议数量"的广义i-SIR转移，开发自适应i-SIR算法，在采样过程中自动调整提议数量N。

Result: 实验表明近似效率和自适应i-SIR算法具有有前景的实证表现，同时证明了i-SIR渐近方差在提议数量上的凸性等新理论结果。

Conclusion: 提出的自适应i-SIR算法能有效优化提议数量，平衡计算成本与采样效率，相关理论结果对独立研究也有价值。

Abstract: Iterated sampling importance resampling (i-SIR) is a Markov chain Monte Carlo (MCMC) algorithm which is based on $N$ independent proposals. As $N$ grows, its samples become nearly independent, but with an increased computational cost. We discuss a method which finds an approximately optimal number of proposals $N$ in terms of the asymptotic efficiency. The optimal $N$ depends on both the mixing properties of the i-SIR chain and the (parallel) computing costs. Our method for finding an appropriate $N$ is based on an approximate asymptotic variance of the i-SIR, which has similar properties as the i-SIR asymptotic variance, and a generalised i-SIR transition having fractional `number of proposals.' These lead to an adaptive i-SIR algorithm, which tunes the number of proposals automatically during sampling. Our experiments demonstrate that our approximate efficiency and the adaptive i-SIR algorithm have promising empirical behaviour. We also present new theoretical results regarding the i-SIR, such as the convexity of asymptotic variance in the number of proposals, which can be of independent interest.

</details>


### [57] [Yet Another Smacof -- Square Symmetric Case](https://arxiv.org/abs/2512.00232)
*Jan de Leeuw*

Main category: stat.CO

TL;DR: 将smacof程序重写为单一C程序，通过R调用，性能提升5-50倍


<details>
  <summary>Details</summary>
Motivation: 现有R smacof包中的smacofSym()函数计算速度较慢，需要优化性能以处理大规模数据

Method: 将度量/非度量和加权/非加权版本的smacof算法重写为单一C程序，使用R处理数据设置、I/O和通过.C()调用C程序

Result: 新的smacofSS()程序比原R版本快5-50倍，包含多种初始配置和绘图工具

Conclusion: 通过C语言重写smacof算法显著提升了计算性能，同时保持了R的易用性，为大规模多维尺度分析提供了高效解决方案

Abstract: We rewrite the metric/nonmetric and weighted/unweighted versions of the smacof program for square symmetric data as one monolithic C program. R is used for taking care of the data and parameter setup, the I/O, and of issuing a single call to .C() to start the computations. This makes this new smacofSS() program five to fifty times as fast (for our examples) as the smacofSym() function from the R smacof package. Utilities for various initial configurations and plots are included in the package. Examples are included to compare output and time for the R and C versions and to illustrate the various plots.

</details>


### [58] [An hybrid stochastic Newton algorithm for logistic regression](https://arxiv.org/abs/2512.01790)
*Bernard Bercu,Luis Fredes,Eméric Gbaguidi*

Main category: stat.CO

TL;DR: 提出一种用于大规模二分类问题的二阶随机算法，结合了自然海森估计和随机梯度信息的混合随机牛顿方法，证明了算法的几乎必然收敛性和中心极限定理。


<details>
  <summary>Details</summary>
Motivation: 针对大规模二分类问题，需要高效的随机优化算法。传统随机梯度下降收敛慢，而二阶方法计算成本高。本文动机在于利用逻辑回归中真实参数处自然海森估计和随机梯度信息都等于海森矩阵的特性，构建更有效的混合随机牛顿算法。

Method: 提出混合随机牛顿算法，在海森矩阵估计中包含两个加权分量：一个来自自然海森估计，另一个来自随机梯度信息。这种混合设计利用了两种估计在逻辑回归真实参数处都等于真实海森矩阵的性质。

Result: 证明了算法几乎必然收敛到真实参数；显著提高了海森矩阵的几乎必然收敛速率；建立了算法的中心极限定理；展示了累积超额风险的几乎必然收敛性这一令人惊讶的结果。

Conclusion: 提出的混合随机牛顿算法在大规模二分类问题中具有理论保证，包括收敛性、收敛速率和统计性质，为随机二阶优化提供了新的有效方法。

Abstract: In this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk.

</details>
