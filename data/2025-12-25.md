<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 6]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 7]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Fast and Exact Least Absolute Deviations Line Fitting via Piecewise Affine Lower-Bounding](https://arxiv.org/abs/2512.20682)
*Stefan Volz,Martin Storath,Andreas Weinmann*

Main category: stat.ML

TL;DR: 提出PALB方法，一种用于最小绝对偏差线拟合的精确算法，比现有方法更快且易于实现


<details>
  <summary>Details</summary>
Motivation: 最小绝对偏差线拟合对异常值鲁棒但计算复杂，现有算法难以实现且缺乏维护的公开实现，实践中常使用不保证精确解的近似方法

Method: 使用子梯度推导支撑线构建分段仿射下界，采用涉及这些下界最小值的细分方案，证明正确性并提供迭代次数界限

Result: 在合成数据集和真实NOAA数据集上表现出经验对数线性缩放，始终比基于线性规划和IRLS的公开实现更快

Conclusion: PALB方法为LAD线拟合提供了高效的精确算法，填补了现有方法实现困难且缺乏维护的空白，提供了Rust实现和Python API

Abstract: Least-absolute-deviations (LAD) line fitting is robust to outliers but computationally more involved than least squares regression. Although the literature includes linear and near-linear time algorithms for the LAD line fitting problem, these methods are difficult to implement and, to our knowledge, lack maintained public implementations. As a result, practitioners often resort to linear programming (LP) based methods such as the simplex-based Barrodale-Roberts method and interior-point methods, or on iteratively reweighted least squares (IRLS) approximation which does not guarantee exact solutions. To close this gap, we propose the Piecewise Affine Lower-Bounding (PALB) method, an exact algorithm for LAD line fitting. PALB uses supporting lines derived from subgradients to build piecewise-affine lower bounds, and employs a subdivision scheme involving minima of these lower bounds. We prove correctness and provide bounds on the number of iterations. On synthetic datasets with varied signal types and noise including heavy-tailed outliers as well as a real dataset from the NOAA's Integrated Surface Database, PALB exhibits empirical log-linear scaling. It is consistently faster than publicly available implementations of LP based and IRLS based solvers. We provide a reference implementation written in Rust with a Python API.

</details>


### [2] [Diffusion Models in Simulation-Based Inference: A Tutorial Review](https://arxiv.org/abs/2512.20685)
*Jonas Arruda,Niels Bracher,Ullrich Köthe,Jan Hasenauer,Stefan T. Radev*

Main category: stat.ML

TL;DR: 这篇教程综述系统总结了扩散模型在基于模拟推理中的应用，涵盖了训练、推理和评估的设计选择，并讨论了效率与统计准确性的影响因素。


<details>
  <summary>Details</summary>
Motivation: 扩散模型最近在基于模拟推理中展现出强大能力，能够从模拟和真实数据中快速准确地估计潜在参数。其基于分数的公式为学习参数和观测的条件或联合分布提供了灵活方法，为解决各种建模问题提供了通用解决方案。

Method: 这是一篇教程综述，综合了扩散模型在SBI中的最新发展，涵盖了训练、推理和评估的设计选择。重点介绍了指导、分数组合、流匹配、一致性模型和联合建模等概念，并讨论了噪声调度、参数化和采样器如何影响效率和统计准确性。

Result: 通过跨参数维度、模拟预算和模型类型的案例研究，展示了这些概念的实际应用效果，为扩散模型在SBI领域的应用提供了系统指导。

Conclusion: 扩散模型为基于模拟推理提供了强大而灵活的框架，通过适当的训练、推理和评估设计，能够在各种建模问题中实现高效准确的参数估计。未来研究需要解决该领域面临的开放性问题。

Abstract: Diffusion models have recently emerged as powerful learners for simulation-based inference (SBI), enabling fast and accurate estimation of latent parameters from simulated and real data. Their score-based formulation offers a flexible way to learn conditional or joint distributions over parameters and observations, thereby providing a versatile solution to various modeling problems. In this tutorial review, we synthesize recent developments on diffusion models for SBI, covering design choices for training, inference, and evaluation. We highlight opportunities created by various concepts such as guidance, score composition, flow matching, consistency models, and joint modeling. Furthermore, we discuss how efficiency and statistical accuracy are affected by noise schedules, parameterizations, and samplers. Finally, we illustrate these concepts with case studies across parameter dimensionalities, simulation budgets, and model types, and outline open questions for future research.

</details>


### [3] [Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights](https://arxiv.org/abs/2512.20811)
*Rommel Cortez,Bala Krishnamoorthy*

Main category: stat.ML

TL;DR: 提出加权Pearson-Matthews相关系数（MCC）用于评估带权重的分类任务，能区分在重要样本上表现更好的分类器


<details>
  <summary>Details</summary>
Motivation: 现有分类评估指标无法处理样本具有不同权重的情况，需要能反映分类器在重要样本上性能的评估指标

Method: 提出加权MCC及其多分类扩展版本，证明权重变化ε时指标变化最多ε（二分类）或ε²（多分类）

Result: 加权指标能有效识别在重要样本上表现更好的分类器，而未加权指标对权重变化完全不敏感

Conclusion: 加权MCC为带权重分类任务提供了鲁棒的评估工具，填补了现有评估指标的空白

Abstract: Several performance measures are used to evaluate binary and multiclass classification tasks.
  But individual observations may often have distinct weights, and none of these measures are sensitive to such varying weights.
  We propose a new weighted Pearson-Matthews Correlation Coefficient (MCC) for binary classification as well as weighted versions of related multiclass measures. The weighted MCC varies between $-1$ and $1$. But crucially, the weighted MCC values are higher for classifiers that perform better on highly weighted observations, and hence is able to distinguish them from classifiers that have a similar overall performance and ones that perform better on the lowly weighted observations.
  Furthermore, we prove that the weighted measures are robust with respect to the choice of weights in a precise manner:
  if the weights are changed by at most $ε$, the value of the weighted measure changes at most by a factor of $ε$ in the binary case
  and by a factor of $ε^2$ in the multiclass case.
  Our computations demonstrate that the weighted measures clearly identify classifiers that perform better on higher weighted observations, while the unweighted measures remain completely indifferent to the choices of weights.

</details>


### [4] [Learning from Neighbors with PHIBP: Predicting Infectious Disease Dynamics in Data-Sparse Environments](https://arxiv.org/abs/2512.21005)
*Edwin Fong,Lancelot F. James,Juho Lee*

Main category: stat.ML

TL;DR: PHIBP模型通过层次化泊松过程和绝对丰度概念，有效处理传染病预测中的稀疏计数数据，特别是在历史零病例地区的疫情预测。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏计数数据在传染病预测中的统计挑战，特别是针对历史报告零病例的地理区域进行准确疫情预测的需求。

Method: 采用泊松层次化印度自助餐过程（PHIBP），基于绝对丰度概念，从相关区域系统性地借用统计强度，避免相对速率方法对零计数的敏感性。

Result: PHIBP在传染病数据实验中表现出色，能够生成连贯的预测分布，有效利用α和β多样性等比较度量，在数据稀疏环境下实现准确的疫情预测。

Conclusion: 该统一框架为稀疏计数数据提供了稳健的统计基础，既能实现准确的疫情预测，又能提供有意义的流行病学见解，特别适用于数据稀疏的环境。

Abstract: Modeling sparse count data, which arise across numerous scientific fields, presents significant statistical challenges. This chapter addresses these challenges in the context of infectious disease prediction, with a focus on predicting outbreaks in geographic regions that have historically reported zero cases. To this end, we present the detailed computational framework and experimental application of the Poisson Hierarchical Indian Buffet Process (PHIBP), with demonstrated success in handling sparse count data in microbiome and ecological studies. The PHIBP's architecture, grounded in the concept of absolute abundance, systematically borrows statistical strength from related regions and circumvents the known sensitivities of relative-rate methods to zero counts. Through a series of experiments on infectious disease data, we show that this principled approach provides a robust foundation for generating coherent predictive distributions and for the effective use of comparative measures such as alpha and beta diversity. The chapter's emphasis on algorithmic implementation and experimental results confirms that this unified framework delivers both accurate outbreak predictions and meaningful epidemiological insights in data-sparse settings.

</details>


### [5] [Enhancing diffusion models with Gaussianization preprocessing](https://arxiv.org/abs/2512.21020)
*Li Cunzhi,Louis Kang,Hideaki Shimazaki*

Main category: stat.ML

TL;DR: 提出使用高斯化预处理训练数据，使目标分布更接近独立高斯分布，从而改善扩散模型早期重建质量，特别适用于小规模网络架构


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成等任务中表现出色，但存在采样速度慢的问题，主要原因是轨迹分岔延迟导致早期重建质量差，特别是对于小规模网络架构

Method: 对训练数据应用高斯化预处理，使目标分布更接近独立高斯分布（重建过程的初始密度），简化模型学习目标分布的任务

Result: 该方法改善了扩散模型在早期重建阶段的生成质量，特别适用于小规模网络架构，使采样过程更稳定高效

Conclusion: 高斯化预处理能有效缓解扩散模型的分岔相关问题，提高生成质量，该方法原则上适用于广泛的生成任务

Abstract: Diffusion models are a class of generative models that have demonstrated remarkable success in tasks such as image generation. However, one of the bottlenecks of these models is slow sampling due to the delay before the onset of trajectory bifurcation, at which point substantial reconstruction begins. This issue degrades generation quality, especially in the early stages. Our primary objective is to mitigate bifurcation-related issues by preprocessing the training data to enhance reconstruction quality, particularly for small-scale network architectures. Specifically, we propose applying Gaussianization preprocessing to the training data to make the target distribution more closely resemble an independent Gaussian distribution, which serves as the initial density of the reconstruction process. This preprocessing step simplifies the model's task of learning the target distribution, thereby improving generation quality even in the early stages of reconstruction with small networks. The proposed method is, in principle, applicable to a broad range of generative tasks, enabling more stable and efficient sampling processes.

</details>


### [6] [Causal-driven attribution (CDA): Estimating channel influence without user-level data](https://arxiv.org/abs/2512.21211)
*Georgios Filippou,Boi Mai Quach,Diana Lenghel,Arthur White,Ashish Kumar Jha*

Main category: stat.ML

TL;DR: 提出CDA框架，仅使用聚合展示数据推断渠道影响力，无需用户路径追踪，结合因果发现与效应估计实现隐私保护的归因分析


<details>
  <summary>Details</summary>
Motivation: 现有归因模型依赖用户级路径数据，但隐私法规和平台限制使这些数据难以获取，需要不依赖用户标识符的新方法

Method: CDA框架整合时序因果发现（PCMCI）与结构因果模型的因果效应估计，从聚合展示数据中恢复渠道关系并量化转化贡献

Result: 在模拟真实营销动态的大规模合成数据上，CDA在已知真实因果图时相对RMSE为9.50%，使用预测图时为24.23%，表现良好

Conclusion: CDA能捕捉跨渠道依赖关系，提供可解释、隐私保护的归因洞察，为传统路径模型提供可扩展且面向未来的替代方案

Abstract: Attribution modelling lies at the heart of marketing effectiveness, yet most existing approaches depend on user-level path data, which are increasingly inaccessible due to privacy regulations and platform restrictions. This paper introduces a Causal-Driven Attribution (CDA) framework that infers channel influence using only aggregated impression-level data, avoiding any reliance on user identifiers or click-path tracking. CDA integrates temporal causal discovery (using PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions to conversions. Using large-scale synthetic data designed to replicate real marketing dynamics, we show that CDA achieves an average relative RMSE of 9.50% when given the true causal graph, and 24.23% when using the predicted graph, demonstrating strong accuracy under correct structure and meaningful signal recovery even under structural uncertainty. CDA captures cross-channel interdependencies while providing interpretable, privacy-preserving attribution insights, offering a scalable and future-proof alternative to traditional path-based models.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [7] [A Profit-Based Measure of Lending Discrimination](https://arxiv.org/abs/2512.20753)
*Madison Coots,Robert Bartlett,Julian Nyarko,Sharad Goel*

Main category: stat.AP

TL;DR: 研究发现美国金融科技平台的贷款算法对黑人和男性借款人存在系统性风险误判，导致这些群体获得更优惠的贷款条件，揭示了算法公平性中的校准问题。


<details>
  <summary>Details</summary>
Motivation: 随着算法贷款在消费信贷领域的普及，虽然算法排除了受法律保护的种族、性别等特征，但仍可能无意中偏袒某些群体。需要开发新的审计方法来检测算法贷款中潜在的歧视行为。

Method: 基于先前理论研究，引入基于利润的贷款歧视衡量方法，并将其应用于美国主要金融科技平台约80,000笔个人贷款数据进行分析。

Result: 研究发现：1）对男性和黑人借款人的贷款利润较低，表明这些群体获得了相对优惠的贷款条件；2）这些差异源于平台承销模型的风险误判，低估了黑人借款人的信用风险，高估了女性的风险；3）通过在模型中明确包含种族和性别可以纠正这种误判，但会引发不同公平概念之间的紧张关系。

Conclusion: 算法贷款中的公平问题不仅涉及特征排除，更关键的是模型校准的准确性。在承销模型中包含受保护特征可能有助于纠正系统性误判，但这与传统的公平概念存在冲突，需要在算法公平的不同维度之间进行权衡。

Abstract: Algorithmic lending has transformed the consumer credit landscape, with complex machine learning models now commonly used to make or assist underwriting decisions. To comply with fair lending laws, these algorithms typically exclude legally protected characteristics, such as race and gender. Yet algorithmic underwriting can still inadvertently favor certain groups, prompting new questions about how to audit lending algorithms for potentially discriminatory behavior. Building on prior theoretical work, we introduce a profit-based measure of lending discrimination in loan pricing. Applying our approach to approximately 80,000 personal loans from a major U.S. fintech platform, we find that loans made to men and Black borrowers yielded lower profits than loans to other groups, indicating that men and Black applicants benefited from relatively favorable lending decisions. We trace these disparities to miscalibration in the platform's underwriting model, which underestimates credit risk for Black borrowers and overestimates risk for women. We show that one could correct this miscalibration -- and the corresponding lending disparities -- by explicitly including race and gender in underwriting models, illustrating a tension between competing notions of fairness.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [8] [Adjusted Kolmogorov Complexity of Binary Words with Empirical Entropy Normalization](https://arxiv.org/abs/2512.21193)
*Brani Vidakovic*

Main category: stat.CO

TL;DR: 提出一种熵归一化的Kolmogorov复杂度度量，通过除以经验熵来分离符号不平衡的纯组合效应，从而隔离内在描述复杂度


<details>
  <summary>Details</summary>
Motivation: 传统Kolmogorov复杂度同时反映算法结构和符号经验分布，符号频率远离1/2的单词由于组合丰富度较小而显得复杂度较低，需要分离这两种效应

Method: 引入熵归一化复杂度度量：将单词的Kolmogorov复杂度除以其观察到的0和1分布的经验熵，调整后隔离内在描述复杂度与符号不平衡的纯组合效应

Result: 对于构造可交换测度下的Martin Löf随机序列，调整后的复杂度线性增长并收敛到1；病理性构造表明底层测度的规律性至关重要

Conclusion: 该框架以自然方式连接Kolmogorov复杂度、经验熵和随机性，建议在随机性测试和结构化二进制数据分析中应用

Abstract: Kolmogorov complexity of a finite binary word reflects both algorithmic structure and the empirical distribution of symbols appearing in the word. Words with symbol frequencies far from one half have smaller combinatorial richness and therefore appear less complex under the standard definition. In this paper an entropy-normalized complexity measure is introduced that divides the Kolmogorov complexity of a word by the empirical entropy of its observed distribution of zeros and ones. This adjustment isolates intrinsic descriptive complexity from the purely combinatorial effect of symbol imbalance. For Martin Löf random sequences under constructive exchangeable measures, the adjusted complexity grows linearly and converges to one. A pathological construction shows that regularity of the underlying measure is essential. The proposed framework connects Kolmogorov complexity, empirical entropy, and randomness in a natural manner and suggests applications in randomness testing and in the analysis of structured binary data.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [9] [The Whittle likelihood for mixed models with application to groundwater level time series](https://arxiv.org/abs/2512.20810)
*Jakub J. Pypkowski,Adam M. Sykulski,James S. Martin,Ben P. Marchant*

Main category: stat.ME

TL;DR: 提出使用Whittle似然联合估计混合模型所有参数的方法，解决最大似然估计计算复杂度高的问题，适用于大规模、缺失和非高斯数据


<details>
  <summary>Details</summary>
Motivation: 地下水水位预测对应对地下水干旱等灾害至关重要。混合模型用于地下水时间序列分析，但最大似然估计计算复杂度高，导致实际应用中需简化假设，限制了处理大规模数据的能力

Method: 使用Whittle似然（频域准似然）联合估计混合模型的所有参数。该方法对缺失数据和非高斯数据具有鲁棒性，能处理更大规模的数据集

Result: 通过模拟研究和真实数据验证了方法的有效性，与最大似然估计和分两阶段估计固定效应与随机效应参数的替代方法进行了比较

Conclusion: 提出的Whittle似然方法能够有效估计混合模型参数，解决了传统最大似然估计的计算瓶颈，为大规模地下水时间序列分析提供了实用工具

Abstract: Understanding the processes that influence groundwater levels is crucial for forecasting and responding to hazards such as groundwater droughts. Mixed models, which combine a fixed mean, expressed using independent predictors, with autocorrelated random errors, are used for inference, forecasting and filling in missing values in groundwater level time series. Estimating parameters of mixed models using maximum likelihood has high computational complexity. For large datasets, this leads to restrictive simplifying assumptions such as fixing certain free parameters in practical implementations. In this paper, we propose a method to jointly estimate all parameters of mixed models using the Whittle likelihood, a frequency-domain quasi-likelihood. Our method is robust to missing and non-Gaussian data and can handle much larger data sizes. We demonstrate the utility of our method both in a simulation study and with real-world data, comparing against maximum likelihood and an alternative two-stage approach that estimates fixed and random effect parameters separately.

</details>


### [10] [Modeling gap acceptance behavior allowing for perceptual distortions and exogenous influences](https://arxiv.org/abs/2512.21136)
*Ankita Sharma,Partha Chakroborty,Pranamesh Chakraborty*

Main category: stat.ME

TL;DR: 该研究提出基于感知间隙而非测量间隙的间隙接受决策模型，将临界间隙视为受多种外生变量影响的潜变量，并开发了包含感知过程中系统性和随机性扭曲的数学模型。


<details>
  <summary>Details</summary>
Motivation: 传统间隙接受研究基于测量间隙，但实际决策发生在人的思维中，应基于感知间隙。临界间隙作为心理概念也是潜变量，且受车辆类型、等待时间等多种因素影响。

Method: 开发了包含感知过程中系统性和随机性扭曲的数学模型，考虑车辆类型、等待时间等外生变量对临界间隙的影响。使用最大似然法估计两个数据集参数，并提出可观测的临界间隙模拟器估计方法。

Result: 参数估计揭示了各影响因素如何影响临界间隙，结果验证了初始预测。提出的模拟器方法能改进等待时间等衍生量的估计，并发现被拒绝间隙数可作为等待时间的合理替代指标。

Conclusion: 基于感知间隙的间隙接受决策框架得到验证，提出的数学模型和模拟器方法能更准确地描述驾驶行为，为交通工程应用提供改进工具。

Abstract: This work on gap acceptance is based on the premise that the decision to accept/reject a gap happens in a person's mind and therefore must be based on the perceived gap and not the measured gap. The critical gap must also exist in a person's mind and hence, together with the perceived gap, is a latent variable. Finally, it is also proposed that the critical gap is influenced by various exogenous variables such as subject and opposing vehicle types, and perceived waiting time. Mathematical models that (i) incorporate systematic and random distortions during the perception process and (ii) account for the effect of the various influencing variables are developed. The parameters of these models are estimated for two different gap acceptance data sets using the maximum likelihood technique. The data is collected as part of this study. The estimated parameters throw valuable insights into how these influencing variables affect the critical gap. The results corroborate the initial predictions on the nature of influence these variables must exert and give strength to the gap acceptance decision-making construct proposed here. This work also proposes a methodology to estimate a measurable/observable world emulator of the latent variable critical gap. The use of the emulator critical gap provides improved estimates of derived quantities like the average waiting time of subject vehicles. Finally, studies are also conducted to show that the number of rejected gaps can work as a reasonable surrogate for the influencing variable, waiting time.

</details>


### [11] [Improving optimal subsampling through stratification](https://arxiv.org/abs/2512.20837)
*Jasper B. Yang,Thomas Lumley,Bryan E. Shepherd,Pamela A. Shaw*

Main category: stat.ME

TL;DR: 论文比较了个体化抽样与分层抽样在逻辑回归中的效率，发现最优分层设计通常比最优个体化设计更高效，因为能消除层间方差贡献


<details>
  <summary>Details</summary>
Motivation: 现有最优子抽样方法主要分为两类：个体化抽样规则（为每个单位分配特定概率）和分层抽样（在层内进行简单随机抽样）。需要比较这两种方法在逻辑回归设置中的效率差异

Method: 在逻辑回归设置下，推导两种抽样方法下估计量的渐近方差，通过大量模拟实验和范德比尔特综合护理诊所队列数据的应用进行数值比较

Result: 分层抽样不仅仅是个体化抽样的近似，最优分层设计通常比最优个体化设计更高效，因为它能消除层间方差贡献。优化个体化抽样规则会忽略高效的分层抽样设计

Conclusion: 分层抽样具有常被低估的优势，优化个体化抽样规则会错过高效的分层抽样设计。研究结果强调了分层抽样在抽样设计中的重要性

Abstract: Recent works have proposed optimal subsampling algorithms to improve computational efficiency in large datasets and to design validation studies in the presence of measurement error. Existing approaches generally fall into two categories: (i) designs that optimize individualized sampling rules, where unit-specific probabilities are assigned and applied independently, and (ii) designs based on stratified sampling with simple random sampling within strata. Focusing on the logistic regression setting, we derive the asymptotic variances of estimators under both approaches and compare them numerically through extensive simulations and an application to data from the Vanderbilt Comprehensive Care Clinic cohort. Our results reinforce that stratified sampling is not merely an approximation to individualized sampling, showing instead that optimal stratified designs are often more efficient than optimal individualized designs through their elimination of between-stratum contributions to variance. These findings suggest that optimizing over the class of individualized sampling rules overlooks highly efficient sampling designs and highlight the often underappreciated advantages of stratified sampling.

</details>


### [12] [A Unified Inference Method for FROC-type Curves and Related Summary Indices](https://arxiv.org/abs/2512.20922)
*Jiarui Sun,Kaiyuan Liu,Xiao-Hua Zhou*

Main category: stat.ME

TL;DR: 提出基于初始检测与候选模型的方法，用于估计自由响应接收器操作特性（FROC）曲线及其置信区间，并应用于医疗软件设备的诊断性能评估。


<details>
  <summary>Details</summary>
Motivation: 自由响应观察者性能研究在评估多目标检测任务中很重要，但现有方法在估计FROC曲线和置信区间方面存在局限，特别是在医疗软件设备（SaMD）的性能评估中需要更可靠的统计方法。

Method: 提出基于初始检测与候选模型的统一方法，使用最大似然估计器同时估计平滑FROC曲线，推导置信区间，并证明估计器的渐近正态性。

Result: 通过模拟研究验证了该方法在有限样本下的性能，并将其应用于评估检测肺部病变的医疗软件设备的诊断性能。

Conclusion: 该方法为自由响应观察者性能评估提供了统一的统计框架，能够可靠地估计FROC曲线和置信区间，适用于医疗软件设备等实际应用场景。

Abstract: Free-response observer performance studies are of great importance for accuracy evaluation and comparison in tasks related to the detection and localization of multiple targets or signals. The free-response receiver operating characteristic (FROC) curve and many similar curves based on the free-response observer performance assessment data are important tools to display the accuracy of detection under different thresholds. The true positive rate at a fixed false positive rate and summary indices such as the area under the FROC curve are also commonly used as the figures of merit in the statistical evaluation of these studies. Motivated by a free-response observer performance assessment research of a Software as a Medical Device (SaMD), we propose a unified method based on the initial-detection-and-candidate model to simultaneously estimate a smooth curve and derive confidence intervals for summary indices and the true positive rate at a fixed false positive rate. A maximum likelihood estimator is proposed and its asymptotic normality property is derived. Confidence intervals are constructed based on the asymptotic normality of our maximum likelihood estimator. Simulation studies are conducted to evaluate the finite sample performance of the proposed method. We apply the proposed method to evaluate the diagnostic performance of the SaMD for detecting pulmonary lesions.

</details>


### [13] [Two-level D- and A-optimal designs of Ehlich type with run sizes three more than a multiple of four](https://arxiv.org/abs/2512.21060)
*Mohammed Saif Ismail Hameed,Eric D. Schoen,Jose Nunez Ares,Peter Goos*

Main category: stat.ME

TL;DR: 提出算法生成所有非同构的D-和A-最优主效应设计，针对运行规模为4的倍数加3的情况，枚举了最多19个运行规模的设计


<details>
  <summary>Details</summary>
Motivation: 在运行规模N≤20的情况下，文献已报告了大多数运行规模的最佳D-和A-最优主效应设计，但运行规模为4的倍数加3的系列设计仍然未知，需要填补这一空白

Method: 提出一种算法来生成所有非同构的D-和A-最优主效应设计，针对运行规模为4的倍数加3的情况，枚举了最多19个运行规模的设计

Result: 枚举了运行规模最多为19的所有此类设计，报告了获得的设计数量，并识别出那些最小化主效应与交互效应之间以及交互效应之间混淆的设计

Conclusion: 解决了运行规模为4的倍数加3的D-和A-最优主效应设计的未知问题，为这一系列运行规模提供了完整的设计枚举和最优设计识别

Abstract: For the majority of run sizes N where N <= 20, the literature reports the best D- and A-optimal designs for the main-effects model which sequentially minimizes the aliasing between main effects and interaction effects and among interaction effects. The only series of run sizes for which all the minimally aliased D- and A-optimal main-effects designs remain unknown are those with run sizes three more than a multiple of four. To address this, in our paper, we propose an algorithm to generate all non-isomorphic D- and A-optimal main-effects designs for run sizes three more than a multiple of four. We enumerate all such designs for run sizes up to 19, report the numbers of designs we obtained, and identify those that minimize the aliasing between main effects and interaction effects and among interaction effects.

</details>


### [14] [Measuring Variable Importance via Accumulated Local Effects](https://arxiv.org/abs/2512.21124)
*Jingyu Zhu,Daniel W. Apley*

Main category: stat.ME

TL;DR: 提出基于ALE的新变量重要性度量方法，解决传统方法在预测变量高度相关时的外推和重要性低估问题。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒监督学习模型缺乏可解释性，现有变量重要性度量方法（如边际Shapley和置换方法）在预测变量高度相关时会产生不可靠结果，要么需要外推，要么低估相关变量的重要性。

Method: 提出基于累积局部效应（ALE）概念的新变量重要性度量方法，避免外推问题，同时不低估相关变量的重要性。

Result: ALE VIMs在预测变量相关性较弱时与传统方法产生相似的变量重要性排序，在相关性较强时提供更可靠的排序，且计算成本显著更低。

Conclusion: ALE VIMs是一种有效解决预测变量相关性问题的新变量重要性度量方法，兼具可靠性和计算效率优势。

Abstract: A shortcoming of black-box supervised learning models is their lack of interpretability or transparency. To facilitate interpretation, post-hoc global variable importance measures (VIMs) are widely used to assign to each predictor or input variable a numerical score that represents the extent to which that predictor impacts the fitted model's response predictions across the training data. It is well known that the most common existing VIMs, namely marginal Shapley and marginal permutation-based methods, can produce unreliable results if the predictors are highly correlated, because they require extrapolation of the response at predictor values that fall far outside the training data. Conditional versions of Shapley and permutation VIMs avoid or reduce the extrapolation but can substantially deflate the importance of correlated predictors. For the related goal of visualizing the effects of each predictor when strong predictor correlation is present, accumulated local effects (ALE) plots were recently introduced and have been widely adopted. This paper presents a new VIM approach based on ALE concepts that avoids both the extrapolation and the VIM deflation problems when predictors are correlated. We contrast, both theoretically and numerically, ALE VIMs with Shapley and permutation VIMs. Our results indicate that ALE VIMs produce similar variable importance rankings as Shapley and permutation VIMs when predictor correlations are mild and more reliable rankings when correlations are strong. An additional advantage is that ALE VIMs are far less computationally expensive.

</details>


### [15] [Proximal Survival Analysis for Dependent Left Truncation](https://arxiv.org/abs/2512.21283)
*Yuyao Wang,Andrew Ying,Ronghui Xu*

Main category: stat.ME

TL;DR: 提出一种处理左截断数据的新方法，使用代理变量框架处理未观测的依赖因素，应用于认知障碍生存分析


<details>
  <summary>Details</summary>
Motivation: 现有处理左截断的方法依赖于（准）独立性假设，但在实践中，观测协变量可能只是潜在机制的不完美代理，导致条件准独立性假设失效，需要新方法处理未观测的依赖因素

Method: 提出近端加权识别框架，允许依赖诱导因素不完全观测，构建基于该框架的估计器并研究其渐近性质

Result: 通过综合模拟检验了所提估计器的有限样本性能，并将其应用于檀香山亚洲老龄化研究，分析认知障碍无生存概率

Conclusion: 提出的近端加权框架能够处理左截断数据中未观测的依赖因素，为存在潜在依赖机制的时间到事件数据分析提供了有效方法

Abstract: In prevalent cohort studies with delayed entry, time-to-event outcomes are often subject to left truncation where only subjects that have not experienced the event at study entry are included, leading to selection bias. Existing methods for handling left truncation mostly rely on the (quasi-)independence assumption or the weaker conditional (quasi-)independence assumption which assumes that conditional on observed covariates, the left truncation time and the event time are independent on the observed region. In practice, however, our analysis of the Honolulu Asia Aging Study (HAAS) suggests that the conditional quasi-independence assumption may fail because measured covariates often serve only as imperfect proxies for the underlying mechanisms, such as latent health status, that induce dependence between truncation and event times. To address this gap, we propose a proximal weighting identification framework that admits the dependence-inducing factors may not be fully observed. We then construct an estimator based on the framework and study its asymptotic properties. We examine the finite sample performance of the proposed estimator by comprehensive simulations, and apply it to analyzing the cognitive impairment-free survival probabilities using data from the Honolulu Asia Aging Study.

</details>
