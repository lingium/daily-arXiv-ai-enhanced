<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 6]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.ME](#stat.ME) [Total: 10]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [A new stochastic diffusion process to model and predict electricity production from natural gas sources in the United States](https://arxiv.org/abs/2511.01925)
*Safa' Alsheyab*

Main category: stat.AP

TL;DR: 本文提出了一种新的随机扩散过程来建模美国天然气发电量占总发电量的百分比，使用趋势函数分析进行拟合和预测，参数通过最大似然法估计。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个有效的统计模型来分析和预测美国天然气发电量占比的变化趋势，为能源政策制定提供可靠依据。

Method: 采用随机扩散过程建模，结合趋势函数分析，使用最大似然法基于1990-2021年年度数据进行参数估计。

Result: 模型能有效拟合历史数据，并为2022-2023年提供了可靠的中期预测结果。

Conclusion: 提出的随机扩散过程模型在天然气发电量占比的建模和预测方面表现出良好的性能。

Abstract: This paper introduces a new stochastic diffusion process to model the
electricity production from natural gas sources (as a percentage of total
electricity production) in the United States. The method employs trend function
analysis to generate fits and forecasts with both conditional and unconditional
estimated trend functions. Parameters are estimated using the maximum
likelihood (ML) method, based on discrete sampling paths of the variable
"electricity production from natural gas sources in the United States" with
annual data from 1990 to 2021. The results show that the proposed model
effectively fits the data and provides dependable medium-term forecasts for
2022-2023.

</details>


### [2] [Enhancing Phenotype Discovery in Electronic Health Records through Prior Knowledge-Guided Unsupervised Learning](https://arxiv.org/abs/2511.02102)
*Melanie Mayer,Kimberly Lactaoen,Gary E. Weissman,Blanca E. Himes,Rebecca A. Hubbard*

Main category: stat.AP

TL;DR: 提出了一种贝叶斯潜在类别框架，通过融入领域特定知识来改进电子健康记录（EHR）衍生表型的临床意义，并在哮喘队列中识别出T2炎症相关的亚表型。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督学习方法通常忽略临床知识，限制了EHR衍生表型的可解释性。本研究旨在通过融入领域知识来提高表型的临床相关性。

Method: 使用贝叶斯潜在类别模型，通过信息性先验将临床知识融入无监督聚类，同时处理缺失数据并提供患者级别的表型分配概率。

Result: 在44,642名成人哮喘患者中发现了双峰后验分布，识别出T2炎症相关类别（38.7%），其特征包括嗜酸性粒细胞升高、过敏标志物、高医疗利用率和药物使用。

Conclusion: 该贝叶斯潜在类别建模方法支持在缺乏明确表型定义的异质性疾病EHR研究中进行假设生成和队列识别。

Abstract: Objectives: Unsupervised learning with electronic health record (EHR) data
has shown promise for phenotype discovery, but approaches typically disregard
existing clinical information, limiting interpretability. We operationalize a
Bayesian latent class framework for phenotyping that incorporates
domain-specific knowledge to improve clinical meaningfulness of EHR-derived
phenotypes and illustrate its utility by identifying an asthma sub-phenotype
informed by features of Type 2 (T2) inflammation.
  Materials and methods: We illustrate a framework for incorporating clinical
knowledge into a Bayesian latent class model via informative priors to guide
unsupervised clustering toward clinically relevant subgroups. This approach
models missingness, accounting for potential missing-not-at-random patterns,
and provides patient-level probabilities for phenotype assignment with
uncertainty. Using reusable and flexible code, we applied the model to a large
asthma EHR cohort, specifying informative priors for T2 inflammation-related
features and weakly informative priors for other clinical variables, allowing
the data to inform posterior distributions.
  Results and Conclusion: Using encounter data from January 2017 to February
2024 for 44,642 adult asthma patients, we found a bimodal posterior
distribution of phenotype assignment, indicating clear class separation. The T2
inflammation-informed class (38.7%) was characterized by elevated eosinophil
levels and allergy markers, plus high healthcare utilization and medication
use, despite weakly informative priors on the latter variables. These patterns
suggest an "uncontrolled T2-high" sub-phenotype. This demonstrates how our
Bayesian latent class modeling approach supports hypothesis generation and
cohort identification in EHR-based studies of heterogeneous diseases without
well-established phenotype definitions.

</details>


### [3] [Wavelet Based Cross Correlations with Applications](https://arxiv.org/abs/2511.02174)
*Jack Kissell,Vijini Lakmini,Brani Vidakovic*

Main category: stat.AP

TL;DR: 本文扩展了小波变换在信号相关性分析中的应用，提出了小波相关图、偏小波相关性和加性小波相关性等方法，使用Pearson和Kendall相关性定义，评估了不同小波基下的稳健性，并通过模拟研究和实际数据集验证了这些方法。


<details>
  <summary>Details</summary>
Motivation: 小波变换能够将信号分解为不同频率/尺度带的系数向量，同时保持时间局部化特性，这使得能够在不同尺度上自适应分析信号，捕捉时间和频谱模式。通过研究两个信号在不同尺度上的相关性变化，可以获得比单一全局相关性度量更细致的理解。

Method: 使用正交和非抽取离散小波变换，扩展了小波相关性理论，提出了小波相关图、偏小波相关性和加性小波相关性等方法，采用Pearson和Kendall相关性定义，并评估了不同小波基下的稳健性。

Result: 通过模拟研究验证了所提出方法的有效性，并在实际数据集上进行了应用，展示了这些方法在不同尺度上分析信号相关性的能力。

Conclusion: 小波变换为信号相关性分析提供了多尺度的视角，所提出的方法能够更细致地理解信号间的关系，在不同小波基下表现出稳健性，具有实际应用价值。

Abstract: Wavelet Transforms are a widely used technique for decomposing a signal into
coefficient vectors that correspond to distinct frequency/scale bands while
retaining time localization. This property enables an adaptive analysis of
signals at different scales, capturing both temporal and spectral patterns. By
examining how correlations between two signals vary across these scales, we
obtain a more nuanced understanding of their relationship than what is possible
from a single global correlation measure. In this work, we expand on the theory
of wavelet-based correlations already used in the literature and elaborate on
wavelet correlograms, partial wavelet correlations, and additive wavelet
correlations using the Pearson and Kendall definitions. We use both Orthogonal
and Non-decimated discrete Wavelet Transforms, and assess the robustness of
these correlations under different wavelet bases. Simulation studies are
conducted to illustrate these methods, and we conclude with applications to
real-world datasets.

</details>


### [4] [A generic network theoretic based model to classify SDG indicators](https://arxiv.org/abs/2511.02289)
*Gaurav Kottari,Qazi J. Azhad,Niteesh Sahni*

Main category: stat.AP

TL;DR: 开发了一个基于网络的通用模型，用于量化SDG指标的重要性，帮助政策制定者识别具有最大协同效应的指标。


<details>
  <summary>Details</summary>
Motivation: 为实现联合国可持续发展目标，需要在相互关联的指标间采取协调行动。现有研究多在目标层面分析SDG关联性，而政策制定和实施通常在指标层面进行。

Method: 提出了一个基于网络理论的通用模型，可以量化SDG指标的重要性，适用于任何国家。以印度数据为例进行了应用演示。

Result: 识别出了对加速SDG进展至关重要的关键指标，并为选定的关键观察提供了现有文献的实证支持。

Conclusion: 该网络理论方法为政策制定者提供了识别关键SDG指标的工具，有助于实现最大协同效应。

Abstract: To achieve the United Nations Sustainable Development Goals, coordinated
action across their interlinked indicators is required. Although most of the
research on the interlinkages of the SDGs is done at the goal level, policies
are usually made and implemented at the level of indicators (or targets). Our
study examines the existing literature on SDG interlinkages and indicator (or
target) prioritization, highlighting important drawbacks of current
methodologies. To address these limitations, we propose a generic network-based
model that can quantify the importance of the SDG indicators and help
policymakers in identifying indicators for maximum synergistic impact. Our
model applies to any country, offering a tool for national policymakers. We
illustrate the application of this model using data from India, identifying
important indicators that are crucial for accelerating progress in the SDGs.
While our main contribution lies in developing this network-theoretic
methodology, we also provide supporting empirical evidence from existing
literature for selected key observations.

</details>


### [5] [Identification of Separable OTUs for Multinomial Classification in Compositional Data Analysis](https://arxiv.org/abs/2511.02509)
*R. Alberich,N. A. Cruz,R. Fernández,I. García Mosquera,A. Mir,F. Roselló*

Main category: stat.AP

TL;DR: 提出了一种基于惩罚对数比回归和成对可分性筛选的组合微生物组数据多项式分类框架，通过AUC评估OTU的判别能力并生成可解释的分类排名。


<details>
  <summary>Details</summary>
Motivation: 高通量测序产生的组合数据对传统统计和机器学习方法构成挑战，需要开发专门处理组合数据的分析方法。

Method: 使用惩罚对数比回归和成对可分性筛选，通过计算所有成对对数比的AUC值并聚合成全局可分性指数Sk，生成OTU的可解释排名和置信区间。

Result: 在Baxter结直肠腺瘤数据集上验证，方法能稳定恢复先前识别的核心分类群，并在考虑人口统计学协变量后揭示新的重要OTU。

Conclusion: 该方法为组合微生物组数据提供了稳健可解释的OTU选择框架，通过整合对数比建模、协变量调整和不确定性估计，补充了现有基于排序的方法。

Abstract: High-throughput sequencing has transformed microbiome research, but it also
produces inherently compositional data that challenge standard statistical and
machine learning methods. In this work, we propose a multinomial classification
framework for compositional microbiome data based on penalized log-ratio
regression and pairwise separability screening. The method quantifies the
discriminative ability of each OTU through the area under the receiver
operating characteristic curve ($AUC$) for all pairwise log-ratios and
aggregates these values into a global separability index $S_k$, yielding
interpretable rankings of taxa together with confidence intervals. We
illustrate the approach by reanalyzing the Baxter colorectal adenoma dataset
and comparing our results with Greenacre's ordination-based analysis using
Correspondence Analysis and Canonical Correspondence Analysis. Our models
consistently recover a core subset of taxa previously identified as
discriminant, thereby corroborating Greenacre's main findings, while also
revealing additional OTUs that become important once demographic covariates are
taken into account. In particular, adjustment for age, gender, and diabetes
medication improves the precision of the separation index and highlights new,
potentially relevant taxa, suggesting that part of the original signal may have
been influenced by confounding. Overall, the integration of log-ratio modeling,
covariate adjustment, and uncertainty estimation provides a robust and
interpretable framework for OTU selection in compositional microbiome data. The
proposed method complements existing ordination-based approaches by adding a
probabilistic and inferential perspective, strengthening the identification of
biologically meaningful microbial signatures.

</details>


### [6] [Extended Kalman Filtering on Stiefel Manifolds](https://arxiv.org/abs/2511.02682)
*Jordi-Lluís Figueras,Aron Persson,Lauri Viitasaari*

Main category: stat.AP

TL;DR: 提出了针对Stiefel流形值测量的扩展卡尔曼滤波器的推广方法，在2-球面和4×2正交矩阵空间上的仿真显示相比原始测量有显著改进


<details>
  <summary>Details</summary>
Motivation: 现有的扩展卡尔曼滤波器在处理Stiefel流形值测量时存在局限性，需要专门的方法来处理这种几何结构

Method: 开发了针对Stiefel流形值测量的扩展卡尔曼滤波器推广方法

Result: 在2-球面和4×2正交矩阵空间上的仿真结果显示，相比仅依赖原始测量，该方法有显著性能提升

Conclusion: 提出的Stiefel流形值测量扩展卡尔曼滤波器推广方法有效，能够显著改善滤波性能

Abstract: A generalisation of the extended Kalman filter for Stiefel manifold-valued
measurements is presented. We provide simulations on the 2-sphere and the space
of orthogonal 4-by-2 matrices which show significant improvement of the
Extended Kalman Filter compared to only relying on raw measurements.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [7] [A Grammar of Data Analysis](https://arxiv.org/abs/2511.02156)
*Xunmo Yang,Taylor Pospisil,Omkar Muralidharan,Dennis L. Sun*

Main category: stat.CO

TL;DR: 本文提出了数据分析的语法框架，与数据操作语法区分，其基本元素是度量和维度。介绍了Python实现Meterstick，支持DataFrame和SQL数据库等多种数据源。


<details>
  <summary>Details</summary>
Motivation: 现有数据分析工具缺乏统一语法框架，难以在不同数据源间保持一致性。需要建立专门针对数据分析（而非数据操作）的语法体系。

Method: 定义了以度量和维度为基本元素的数据分析语法，开发了Python库Meterstick实现该语法，支持多种后端数据源。

Result: 成功实现了数据分析语法框架，Meterstick库能够统一处理DataFrame和SQL数据库等不同数据源的分析任务。

Conclusion: 提出的数据分析语法框架有效解决了跨数据源分析的一致性问题，Meterstick为数据分析提供了灵活且统一的工具。

Abstract: This paper outlines a grammar of data analysis, as distinct from grammars of
data manipulation, in which the primitives are metrics and dimensions. We
describe a Python implementation of this grammar called Meterstick, which is
agnostic to the underlying data source, which may be a DataFrame or a SQL
database.

</details>


### [8] [Efficient Solvers for SLOPE in R, Python, Julia, and C++](https://arxiv.org/abs/2511.02430)
*Johan Larsson,Malgorzata Bogdan,Krystyna Grzesiak,Mathurin Massias,Jonas Wallin*

Main category: stat.CO

TL;DR: 开发了多语言SLOPE求解套件，包含高效混合坐标下降算法，支持多种广义线性模型和损失函数，在速度和内存效率上优于现有实现。


<details>
  <summary>Details</summary>
Motivation: 现有SLOPE实现效率不足，需要开发快速、内存高效且灵活的多语言解决方案来支持各种数据结构和模型验证需求。

Method: 采用混合坐标下降算法，支持高斯、二项、泊松和多类逻辑回归等损失函数，兼容稠密、稀疏和内存外矩阵数据结构。

Result: 套件在真实和模拟数据上表现优异，速度显著超越现有SLOPE实现，并能高效拟合完整SLOPE路径和进行交叉验证。

Conclusion: 成功开发了高效的多语言SLOPE求解工具，为统计建模提供了快速、灵活且可扩展的解决方案。

Abstract: We present a suite of packages in R, Python, Julia, and C++ that efficiently
solve the Sorted L-One Penalized Estimation (SLOPE) problem. The packages
feature a highly efficient hybrid coordinate descent algorithm that fits
generalized linear models (GLMs) and supports a variety of loss functions,
including Gaussian, binomial, Poisson, and multinomial logistic regression. Our
implementation is designed to be fast, memory-efficient, and flexible. The
packages support a variety of data structures (dense, sparse, and out-of-memory
matrices) and are designed to efficiently fit the full SLOPE path as well as
handle cross-validation of SLOPE models, including the relaxed SLOPE. We
present examples of how to use the packages and benchmarks that demonstrate the
performance of the packages on both real and simulated data and show that our
packages outperform existing implementations of SLOPE in terms of speed.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [9] [Data-driven Learning of Interaction Laws in Multispecies Particle Systems with Gaussian Processes: Convergence Theory and Applications](https://arxiv.org/abs/2511.02053)
*Jinchao Feng,Charles Kulick,Sui Tang*

Main category: stat.ML

TL;DR: 开发了一个高斯过程框架，用于从轨迹数据中学习多物种相互作用粒子系统中的相互作用核，建立了严格的统计保证并证明了后验估计量的统计最优性。


<details>
  <summary>Details</summary>
Motivation: 多物种系统为多尺度建模提供了典型设置，其中简单的微观相互作用规则产生复杂的宏观行为。多物种设置引入了新的挑战：异质种群在物种内部和跨物种之间相互作用，未知核的数量增加，必须适应不对称的相互作用（如捕食者-猎物动态）。

Method: 在非参数贝叶斯设置中制定学习问题，使用高斯过程框架来学习多物种相互作用粒子系统中的相互作用核。

Result: 分析显示了相互作用核的可恢复性，提供了定量误差界限，并证明了后验估计量的统计最优性。数值实验证实了理论预测，并证明了所提出方法的有效性。

Conclusion: 这项工作为数据驱动的多物种系统中相互作用定律推断提供了一个完整的统计框架，推进了连接微观粒子动力学与涌现宏观行为的更广泛多尺度建模计划。

Abstract: We develop a Gaussian process framework for learning interaction kernels in
multi-species interacting particle systems from trajectory data. Such systems
provide a canonical setting for multiscale modeling, where simple microscopic
interaction rules generate complex macroscopic behaviors. While our earlier
work established a Gaussian process approach and convergence theory for
single-species systems, and later extended to second-order models with
alignment and energy-type interactions, the multi-species setting introduces
new challenges: heterogeneous populations interact both within and across
species, the number of unknown kernels grows, and asymmetric interactions such
as predator-prey dynamics must be accommodated. We formulate the learning
problem in a nonparametric Bayesian setting and establish rigorous statistical
guarantees. Our analysis shows recoverability of the interaction kernels,
provides quantitative error bounds, and proves statistical optimality of
posterior estimators, thereby unifying and generalizing previous single-species
theory. Numerical experiments confirm the theoretical predictions and
demonstrate the effectiveness of the proposed approach, highlighting its
advantages over existing kernel-based methods. This work contributes a complete
statistical framework for data-driven inference of interaction laws in
multi-species systems, advancing the broader multiscale modeling program of
connecting microscopic particle dynamics with emergent macroscopic behavior.

</details>


### [10] [Limit Theorems for Stochastic Gradient Descent in High-Dimensional Single-Layer Networks](https://arxiv.org/abs/2511.02258)
*Parsa Rangriz*

Main category: stat.ML

TL;DR: 本文研究了单层神经网络在线随机梯度下降(SGD)的高维尺度极限，重点关注步长的临界尺度机制。在临界尺度下，会出现新的修正项改变相图，有效动力学在固定点附近简化为Ornstein-Uhlenbeck过程。


<details>
  <summary>Details</summary>
Motivation: 基于Saad和Solla的开创性工作，研究SGD的确定性尺度极限无法捕捉高维学习动力学中的随机波动，需要分析临界尺度机制来理解信息指数如何控制样本复杂度。

Method: 分析在线SGD在临界步长尺度下的高维尺度极限，研究有效动力学在固定点附近的扩散极限行为。

Result: 在临界尺度下，有效动力学出现新的修正项改变相图，在固定点附近简化为Ornstein-Uhlenbeck过程，揭示了确定性尺度极限在捕捉随机波动方面的局限性。

Conclusion: 信息指数控制样本复杂度，确定性尺度极限无法完全描述高维学习动力学的随机波动特性，临界尺度机制提供了更完整的理论框架。

Abstract: This paper studies the high-dimensional scaling limits of online stochastic
gradient descent (SGD) for single-layer networks. Building on the seminal work
of Saad and Solla, which analyzed the deterministic (ballistic) scaling limits
of SGD corresponding to the gradient flow of the population loss, we focus on
the critical scaling regime of the step size. Below this critical scale, the
effective dynamics are governed by ballistic (ODE) limits, but at the critical
scale, new correction term appears that changes the phase diagram. In this
regime, near the fixed points, the corresponding diffusive (SDE) limits of the
effective dynamics reduces to an Ornstein-Uhlenbeck process under certain
conditions. These results highlight how the information exponent controls
sample complexity and illustrates the limitations of deterministic scaling
limit in capturing the stochastic fluctuations of high-dimensional learning
dynamics.

</details>


### [11] [DoFlow: Causal Generative Flows for Interventional and Counterfactual Time-Series Prediction](https://arxiv.org/abs/2511.02137)
*Dongze Wu,Feng Qiu,Yao Xie*

Main category: stat.ML

TL;DR: DoFlow是一个基于因果DAG的流生成模型，使用连续归一化流实现观测、干预和反事实预测，并提供异常检测功能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测不仅需要准确的观测预测，还需要在多元系统中进行干预和反事实查询的因果预测。

Method: 使用基于因果DAG的流生成模型，通过连续归一化流的自然编码和解码机制实现观测、干预和反事实预测。

Result: 在合成数据集和真实世界水电、癌症治疗时间序列上的实验表明，DoFlow实现了准确的系统级观测预测，支持因果预测，并能有效检测异常。

Conclusion: 这项工作为复杂动态系统中因果推理和生成建模的统一做出了贡献。

Abstract: Time-series forecasting increasingly demands not only accurate observational
predictions but also causal forecasting under interventional and counterfactual
queries in multivariate systems. We present DoFlow, a flow based generative
model defined over a causal DAG that delivers coherent observational and
interventional predictions, as well as counterfactuals through the natural
encoding and decoding mechanism of continuous normalizing flows (CNFs). We also
provide a supporting counterfactual recovery result under certain assumptions.
Beyond forecasting, DoFlow provides explicit likelihoods of future
trajectories, enabling principled anomaly detection. Experiments on synthetic
datasets with various causal DAG and real world hydropower and cancer treatment
time series show that DoFlow achieves accurate system-wide observational
forecasting, enables causal forecasting over interventional and counterfactual
queries, and effectively detects anomalies. This work contributes to the
broader goal of unifying causal reasoning and generative modeling for complex
dynamical systems.

</details>


### [12] [A new class of Markov random fields enabling lightweight sampling](https://arxiv.org/abs/2511.02373)
*Jean-Baptiste Courbot,Hugo Gangloff,Bruno Colicchio*

Main category: stat.ML

TL;DR: 提出了一种通过高斯马尔可夫随机场(GMRF)高效采样马尔可夫随机场(MRF)的新方法，比传统吉布斯采样快35倍以上，能耗降低37倍以上。


<details>
  <summary>Details</summary>
Motivation: 传统基于吉布斯采样的Potts或Ising MRF采样方法计算成本高，需要寻找更高效的替代方案。

Method: 建立从实值GMRF到离散值MRF的映射关系，利用GMRF的高效采样方法来采样MRF。

Result: 新方法在计算效率上显著提升，采样速度比吉布斯采样快35倍以上，能耗降低37倍以上，同时保持与经典MRF相似的实证特性。

Conclusion: 通过GMRF映射方法可以有效解决MRF采样效率低的问题，为离散随机场的高效采样提供了新途径。

Abstract: This work addresses the problem of efficient sampling of Markov random fields
(MRF). The sampling of Potts or Ising MRF is most often based on Gibbs
sampling, and is thus computationally expensive. We consider in this work how
to circumvent this bottleneck through a link with Gaussian Markov Random
fields. The latter can be sampled in several cost-effective ways, and we
introduce a mapping from real-valued GMRF to discrete-valued MRF. The resulting
new class of MRF benefits from a few theoretical properties that validate the
new model. Numerical results show the drastic performance gain in terms of
computational efficiency, as we sample at least 35x faster than Gibbs sampling
using at least 37x less energy, all the while exhibiting empirical properties
close to classical MRFs.

</details>


### [13] [An Adaptive Sampling Framework for Detecting Localized Concept Drift under Label Scarcity](https://arxiv.org/abs/2511.02452)
*Junghee Pyeon,Davide Cacciarelli,Kamran Paynabar*

Main category: stat.ML

TL;DR: 提出一种结合残差探索开发和EWMA监控的自适应采样框架，用于在标注预算约束下高效检测局部概念漂移


<details>
  <summary>Details</summary>
Motivation: 概念漂移和标签稀缺是动态工业环境中预测模型面临的两个关键挑战，现有漂移检测方法假设全局漂移且依赖密集监督，不适用于具有局部漂移和有限标签的回归任务

Method: 结合残差探索开发和EWMA监控的自适应采样框架

Result: 在合成基准和电力市场案例研究中显示出在标签效率和漂移检测准确性方面的优越性能

Conclusion: 所提框架能有效解决局部概念漂移检测问题，在标注预算约束下实现高效漂移检测

Abstract: Concept drift and label scarcity are two critical challenges limiting the
robustness of predictive models in dynamic industrial environments. Existing
drift detection methods often assume global shifts and rely on dense
supervision, making them ill-suited for regression tasks with local drifts and
limited labels. This paper proposes an adaptive sampling framework that
combines residual-based exploration and exploitation with EWMA monitoring to
efficiently detect local concept drift under labeling budget constraints.
Empirical results on synthetic benchmarks and a case study on electricity
market demonstrate superior performance in label efficiency and drift detection
accuracy.

</details>


### [14] [Optimizing Kernel Discrepancies via Subset Selection](https://arxiv.org/abs/2511.02706)
*Deyao Chen,François Clément,Carola Doerr,Nathan Kirk*

Main category: stat.ML

TL;DR: 本文提出了一种新的子集选择算法，用于在核差异度量下从大规模总体中选择低差异样本，适用于单位超立方体上的均匀分布和更一般的已知密度分布。


<details>
  <summary>Details</summary>
Motivation: 核差异是分析拟蒙特卡洛方法最坏情况误差的有力工具，但现有方法在从大规模总体中选择子集时效率不高。

Method: 引入了一种适用于一般核差异的子集选择算法，使用核斯坦差异来处理已知密度函数的一般分布。

Result: 该算法能够高效生成低差异样本，并探讨了经典L2星差异与其L∞对应物之间的关系。

Conclusion: 所提出的算法扩展了核差异在子集选择问题中的应用，为拟蒙特卡洛方法提供了更灵活和高效的采样工具。

Abstract: Kernel discrepancies are a powerful tool for analyzing worst-case errors in
quasi-Monte Carlo (QMC) methods. Building on recent advances in optimizing such
discrepancy measures, we extend the subset selection problem to the setting of
kernel discrepancies, selecting an m-element subset from a large population of
size $n \gg m$. We introduce a novel subset selection algorithm applicable to
general kernel discrepancies to efficiently generate low-discrepancy samples
from both the uniform distribution on the unit hypercube, the traditional
setting of classical QMC, and from more general distributions $F$ with known
density functions by employing the kernel Stein discrepancy. We also explore
the relationship between the classical $L_2$ star discrepancy and its
$L_\infty$ counterpart.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [15] [Bayesian computation for high-dimensional Gaussian Graphical Models with spike-and-slab priors](https://arxiv.org/abs/2511.01875)
*Deborah Sulem,Jack Jewson,David Rossell*

Main category: stat.ME

TL;DR: 提出了两种完全贝叶斯算法用于高斯图模型，可在高维稀疏精度矩阵情况下进行可扩展的结构学习，将精确贝叶斯推理的应用范围从约100个变量扩展到约1000个变量。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯方法在高斯图模型中用于量化结构学习和参数估计的不确定性很有吸引力，但计算需求限制了其在高维情况下的应用，而伪贝叶斯方法虽然计算效率高但缺乏完全贝叶斯推理的理论保证。

Method: 提出了两种算法：1) 块Gibbs内的Metropolis-Hastings算法，允许逐行更新精度矩阵；2) 全局提议算法，可在单行内添加或删除多条边，有助于探索多峰后验分布。两种算法在适当设置下都能获得与维度无关的谱间隙界限。

Result: 算法在稀疏精度矩阵情况下可扩展到高维度，计算成本与最佳伪贝叶斯方法相当。通过使用稀疏线性代数，实际计算成本更低。示例显示方法将精确贝叶斯推理的应用范围从约100个变量扩展到约1000个变量（相当于从5,000条边扩展到500,000条边）。

Conclusion: 提出的完全贝叶斯算法在高维稀疏高斯图模型中实现了可扩展的结构学习，填补了完全贝叶斯推理与计算效率之间的差距，为高维依赖结构推断提供了理论保证和实用工具。

Abstract: Gaussian graphical models are widely used to infer dependence structures.
Bayesian methods are appealing to quantify uncertainty associated with
structural learning, i.e., the plausibility of conditional independence
statements given the data, and parameter estimates. However, computational
demands have limited their application when the number of variables is large,
which prompted the use of pseudo-Bayesian approaches. We propose fully Bayesian
algorithms that provably scale to high dimensions when the data-generating
precision matrix is sparse, at a similar cost to the best pseudo-Bayesian
methods. First, a Metropolis-Hastings-within-Block-Gibbs algorithm that allows
row-wise updates of the precision matrix, using local moves. Second, a global
proposal that enables adding or removing multiple edges within a row, which can
help explore multi-modal posteriors. We obtain spectral gap bounds for both
samplers that are dimension-free under suitable settings. We also provide
worst-case polynomial bounds on per-iteration costs, though in practice the
cost is lower by using sparse linear algebra. Our examples show that the
methods extend the applicability of exact Bayesian inference from roughly 100
to roughly 1000 variables (equivalently, from 5,000 edges to 500,000 edges).

</details>


### [16] [Bayesian spatio-temporal weighted regression for integrating missing and misaligned environmental data](https://arxiv.org/abs/2511.02149)
*Yovna Junglee,Vianey Leos Barajas,Meredith Franklin*

Main category: stat.ME

TL;DR: 提出了一种贝叶斯加权预测回归框架，用于处理多源环境数据中的时空错位和缺失数据问题，通过时空核函数直接在似然函数中聚合预测因子，无需单独的数据填补或强制对齐步骤。


<details>
  <summary>Details</summary>
Motivation: 环境暴露估计对于公共卫生研究和政策至关重要，但卫星产品和地面监测数据的时空错位和缺失数据会引入显著不确定性并降低预测准确性。

Method: 采用贝叶斯加权预测回归框架，结合Voronoi空间求积法和不规则时间增量的数值近似方法，在似然函数中直接建模时空核函数来聚合预测因子。

Result: 在加州北部细颗粒物(PM2.5)估计应用中，相比传统共位线性模型，该方法提高了样本外预测性能(R²增加超过50%)，减少了不确定性，并产生了稳健的时空预测。

Conclusion: 该框架可扩展到额外的时空变化协变量和其他核函数族，为处理环境数据中的时空错位和缺失问题提供了有效解决方案。

Abstract: Estimating environmental exposures from multi-source data is central to
public health research and policy. Integrating data from satellite products and
ground monitors are increasingly used to produce exposure surfaces. However,
spatio-temporal misalignment often induced from missing data introduces
substantial uncertainty and reduces predictive accuracy. We propose a Bayesian
weighted predictor regression framework that models spatio-temporal
relationships when predictors are observed on irregular supports or have
substantial missing data, and are not concurrent with the outcome. The key
feature of our model is a spatio-temporal kernel that aggregates the predictor
over local space-time neighborhoods, built directly into the likelihood,
eliminating any separate gap-filling or forced data alignment stage. We
introduce a numerical approximation using a Voronoi-based spatial quadrature
combined with irregular temporal increments for estimation under data
missingness and misalignment. We showed that misspecification of the spatial
and temporal lags induced bias in the mean and parameter estimates, indicating
the need for principled parameter selection. Simulation studies confirmed these
theoretical findings, where careful tuning was critical to control bias and
achieve accurate prediction, while the proposed quadrature performed well under
severe missingness. As an illustrative application, we estimated fine
particulate matter (PM$_{2.5}$) in northern California using satellite-derived
aerosol optical depth (AOD) and wildfire smoke plume indicators. Relative to a
traditional collocated linear model, our approach improved out-of-sample
predictive performance (over 50\% increase in R$^2$), reduced uncertainty, and
yielded robust temporal predictions and spatial surface estimation. Our
framework is extensible to additional spatio-temporally varying covariates and
other kernel families.

</details>


### [17] [DOD: Detection of outliers in high dimensional data with distance of distances](https://arxiv.org/abs/2511.02199)
*Seong-ho Lee,Yongho Jeon*

Main category: stat.ME

TL;DR: 提出两种基于几何特性的高维离群点检测统计量，利用维度增长时离群点与非离群点间的渐近分离特性，开发了聚类算法和随机旋转检验方法。


<details>
  <summary>Details</summary>
Motivation: 传统离群点检测方法在高维数据中失效，现代方法又过于复杂。需要利用高维空间的几何特性来简化检测过程。

Method: 基于观测点与其他所有点的关系模式，通过成对距离或内积定义两种离群度统计量，并开发聚类算法和随机旋转检验。

Result: 模拟实验和真实数据应用表明，该方法在检测能力和假阳性控制方面优于现有方法。

Conclusion: 该方法将维度诅咒转化为优势，在高维设置中具有实用价值，实现了检测能力与假阳性控制的良好平衡。

Abstract: Reliable outlier detection in high-dimensional data is crucial in modern
science, yet it remains a challenging task. Traditional methods often break
down in these settings due to their reliance on asymptotic behaviors with
respect to sample size under fixed dimension. Furthermore, many modern
alternatives introduce sophisticated statistical treatments and computational
complexities. To overcome these issues, our approach leverages intuitive
geometric properties of high-dimensional space, effectively turning the curse
of dimensionality into an advantage. We propose two new outlyingness statistics
based on observation's relational patterns with all other points, measured via
pairwise distances or inner products. We establish a theoretical foundation for
our statistics demonstrating that as the dimension grows, our statistics create
a non-vanishing margin that asymptotically separates outliers from
non-outliers. Based on this foundation, we develop practical outlier detection
procedures, including a simple clustering-based algorithm and a
distribution-free test using random rotations. Through simulation experiments
and real data applications, we demonstrate that our proposed methods achieve a
superior balance between detection power and false positive control,
outperforming existing methods and establishing their practical utility in
high-dimensional settings.

</details>


### [18] [Interval Estimation for Binomial Proportions Under Differential Privacy](https://arxiv.org/abs/2511.02227)
*Hsuan-Chen Kao,Jerome P. Reiter*

Main category: stat.ME

TL;DR: 本文比较了在差分隐私保护下估计二元比例的多种区间估计方法，包括Wald和Wilson区间、贝叶斯可信区间以及基于Clopper-Pearson的精确区间，发现贝叶斯方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 政府机构和其他数据管理者在发布基于敏感数据计算的二元比例时，需要通过差分隐私保护底层数据的机密性。本文旨在找到在差分隐私约束下最有效的区间估计方法。

Method: 比较了四种区间估计方法：差分隐私版本的Wald和Wilson区间、基于去噪差分私有比例的贝叶斯可信区间，以及受Clopper-Pearson置信区间启发的精确区间。通过模拟研究在拉普拉斯机制和离散高斯机制下评估这些方法。

Result: 模拟研究表明，虽然多种方法都能提供合理的性能，但贝叶斯可信区间在重复抽样性能方面表现最为出色。

Conclusion: 在差分隐私保护下估计二元比例时，贝叶斯可信区间是最具吸引力的选择，能够在保护隐私的同时提供准确的区间估计。

Abstract: When releasing binary proportions computed using sensitive data, several
government agencies and other data stewards protect confidentiality of the
underlying values by ensuring the released statistics satisfy differential
privacy. Typically, this is done by adding carefully chosen noise to the sample
proportion computed using the confidential data. In this article, we describe
and compare methods for turning this differentially private proportion into an
interval estimate for an underlying population probability. Specifically, we
consider differentially private versions of the Wald and Wilson intervals,
Bayesian credible intervals based on denoising the differentially private
proportion, and an exact interval motivated by the Clopper-Pearson confidence
interval. We examine the repeated sampling performances of the intervals using
simulation studies under both the Laplace mechanism and discrete Gaussian
mechanism across a range of privacy guarantees. We find that while several
methods can offer reasonable performances, the Bayesian credible intervals are
the most attractive.

</details>


### [19] [Diffusion Index Forecast with Tensor Data](https://arxiv.org/abs/2511.02235)
*Bin Chen,Yuefeng Han,Qiyang Yu*

Main category: stat.ME

TL;DR: 本文提出了一个结合张量因子模型和非张量预测变量的扩散指数预测方法，建立了张量因子增强回归模型，并针对不同维度的非张量预测变量提出了相应的估计方法和预测区间构造。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散指数预测方法主要处理向量或矩阵形式的预测变量，但实际应用中经常遇到张量结构的数据。同时，非张量预测变量的存在也需要新的建模方法。

Method: 使用CP张量因子模型保持张量结构，针对少量非张量预测变量研究最小二乘估计的渐近性质；提出稳健的阈值协方差矩阵估计器；针对高维非张量预测变量提出多源因子增强稀疏回归模型。

Result: 推导了预测区间的解析公式，考虑了潜在因子估计的不确定性；提出的阈值协方差估计器对横截面依赖具有稳健性；惩罚估计器在高维情况下具有一致性。

Conclusion: 模拟研究验证了理论结果，对美国贸易流的实证应用表明该方法优于文献中其他流行方法，为张量和非张量混合预测变量的扩散指数预测提供了有效解决方案。

Abstract: In this paper, we consider diffusion index forecast with both tensor and
non-tensor predictors, where the tensor structure is preserved with a Canonical
Polyadic (CP) tensor factor model. When the number of non-tensor predictors is
small, we study the asymptotic properties of the least-squared estimator in
this tensor factor-augmented regression, allowing for factors with different
strengths. We derive an analytical formula for prediction intervals that
accounts for the estimation uncertainty of the latent factors. In addition, we
propose a novel thresholding estimator for the high-dimensional covariance
matrix that is robust to cross-sectional dependence. When the number of
non-tensor predictors exceeds or diverges with the sample size, we introduce a
multi-source factor-augmented sparse regression model and establish the
consistency of the corresponding penalized estimator. Simulation studies
validate our theoretical results and an empirical application to US trade flows
demonstrates the advantages of our approach over other popular methods in the
literature.

</details>


### [20] [A Stable Lasso](https://arxiv.org/abs/2511.02306)
*Mahdi Nouraie,Houying Zhu,Samuel Muller*

Main category: stat.ME

TL;DR: 提出一种改进Lasso变量选择稳定性的简单技术，通过将基于相关调整排名的权重方案整合到Lasso惩罚函数中，在存在相关预测变量的情况下提高选择稳定性。


<details>
  <summary>Details</summary>
Motivation: Lasso在变量选择中广泛应用，但在存在相关预测变量时选择稳定性会恶化。现有方法存在局限性，需要改进Lasso的选择稳定性。

Method: 在Lasso惩罚函数中整合权重方案，权重定义为反映预测变量预测能力的相关调整排名的递增函数。

Result: 在模拟和真实数据集上的实证评估证明了所提方法的有效性。数值结果表明该方法也能稳定其他基于正则化的选择方法。

Conclusion: 所提出的方法是一种有前景的通用解决方案，能够显著提高Lasso及其他正则化方法在相关预测变量情况下的选择稳定性。

Abstract: The Lasso has been widely used as a method for variable selection, valued for
its simplicity and empirical performance. However, Lasso's selection stability
deteriorates in the presence of correlated predictors. Several approaches have
been developed to mitigate this limitation. In this paper, we provide a brief
review of existing approaches, highlighting their limitations. We then propose
a simple technique to improve the selection stability of Lasso by integrating a
weighting scheme into the Lasso penalty function, where the weights are defined
as an increasing function of a correlation-adjusted ranking that reflects the
predictive power of predictors. Empirical evaluations on both simulated and
real-world datasets demonstrate the efficacy of the proposed method. Additional
numerical results demonstrate the effectiveness of the proposed approach in
stabilizing other regularization-based selection methods, indicating its
potential as a general-purpose solution.

</details>


### [21] [Bayesian copula-based spatial random effects models for inference with complex spatial data](https://arxiv.org/abs/2511.02551)
*Alan Pearse,David Gunawan,Noel Cressie*

Main category: stat.ME

TL;DR: 开发了基于copula的完全贝叶斯空间统计模型，用于处理大规模、有噪声、不完整和非高斯空间数据，通过新颖的copula构造实现低秩表示和高效计算。


<details>
  <summary>Details</summary>
Motivation: 处理大规模、有噪声、不完整和非高斯空间数据的挑战，传统方法在计算效率和模型灵活性方面存在局限。

Method: 构建基于copula的空间统计模型，包含空间随机效应结构，使用copula作为潜在过程模型，数据模型处理测量误差和缺失数据。

Result: 模拟研究表明完全贝叶斯方法在参数估计和空间过程预测方面优于包括固定秩克里金法在内的基准方法。

Conclusion: 新模型成功应用于澳大利亚昆士兰州鲍文盆地大气甲烷的卫星数据制图，证明了方法的有效性。

Abstract: In this article, we develop fully Bayesian, copula-based, spatial-statistical
models for large, noisy, incomplete, and non-Gaussian spatial data. Our
approach includes novel constructions of copulas that accommodate a
spatial-random-effects structure, enabling low-rank representations and
computationally efficient Bayesian inference. The spatial copula is used in a
latent process model of the Bayesian hierarchical spatial-statistical model,
and, conditional on the latent copula-based spatial process, the data model
handles measurement errors and missing data. Our simulation studies show that a
fully Bayesian approach delivers accurate and fast inference for both parameter
estimation and spatial-process prediction, outperforming several benchmark
methods, including fixed rank kriging (FRK). The new class of copula-based
models is used to map atmospheric methane in the Bowen Basin, Queensland,
Australia, from Sentinel 5P satellite data.

</details>


### [22] [Distributionally Robust Synthetic Control: Ensuring Robustness Against Highly Correlated Controls and Weight Shifts](https://arxiv.org/abs/2511.02632)
*Taehyeon Koo,Zijian Guo*

Main category: stat.ME

TL;DR: 提出了分布鲁棒合成控制方法（DRoSC），通过考虑潜在的关系变化和控制单元间的高相关性，改进了传统合成控制方法，并提供了非正态极限分布的推断方法。


<details>
  <summary>Details</summary>
Motivation: 传统合成控制方法假设处理单元和控制单元在治疗前后的潜在结果关系保持不变，当这种关系发生变化或控制单元高度相关时，估计结果可能不可靠。

Method: 引入DRoSC方法，通过最坏情况优化问题定义新的因果估计量，检查所有符合治疗前期的可能合成权重，并提出了非正态极限分布的推断方法。

Result: 当传统合成控制方法的识别条件成立时，DRoSC方法估计相同的因果效应；当条件违反时，新估计量是未识别因果效应的保守代理。数值研究和巴斯克地区恐怖主义经济影响分析验证了其有限样本性能。

Conclusion: DRoSC方法能够处理传统合成控制方法的局限性，在关系变化或高相关性情况下提供更可靠的因果效应估计，并提供了有效的统计推断工具。

Abstract: The synthetic control method estimates the causal effect by comparing the
outcomes of a treated unit to a weighted average of control units that closely
match the pre-treatment outcomes of the treated unit. This method presumes that
the relationship between the potential outcomes of the treated and control
units remains consistent before and after treatment. However, the estimator may
become unreliable when these relationships shift or when control units are
highly correlated. To address these challenges, we introduce the
Distributionally Robust Synthetic Control (DRoSC) method by accommodating
potential shifts in relationships and addressing high correlations among
control units. The DRoSC method targets a new causal estimand defined as the
optimizer of a worst-case optimization problem that checks through all possible
synthetic weights that comply with the pre-treatment period. When the
identification conditions for the classical synthetic control method hold, the
DRoSC method targets the same causal effect as the synthetic control. When
these conditions are violated, we show that this new causal estimand is a
conservative proxy of the non-identifiable causal effect. We further show that
the limiting distribution of the DRoSC estimator is non-normal and propose a
novel inferential approach to characterize this non-normal limiting
distribution. We demonstrate its finite-sample performance through numerical
studies and an analysis of the economic impact of terrorism in the Basque
Country.

</details>


### [23] [DANIEL: A Distributed and Scalable Approach for Global Representation Learning with EHR Applications](https://arxiv.org/abs/2511.02754)
*Zebin Wang,Ziming Gan,Weijing Tang,Zongqi Xia,Tianrun Cai,Tianxi Cai,Junwei Lu*

Main category: stat.ME

TL;DR: 提出了一种分布式框架，用于从具有内在低秩结构的大规模二元数据中进行可扩展且保护隐私的表示学习，基于Ising模型优化非凸替代损失函数。


<details>
  <summary>Details</summary>
Motivation: 传统概率图模型在现代数据环境中面临高维度、来源异质性和严格数据共享约束的挑战，需要开发能够处理这些问题的分布式学习方法。

Method: 通过双因子梯度下降优化非凸替代损失函数，相比传统凸方法具有显著的计算和通信优势。

Result: 在来自58,248名患者的跨机构电子健康记录数据集上评估，在全局表示学习和下游临床任务中表现出优越性能，包括关系检测、患者表型分析和患者聚类。

Conclusion: 该方法展示了在联邦高维环境中进行统计推断的广泛潜力，同时解决了数据复杂性和多机构集成的实际挑战。

Abstract: Classical probabilistic graphical models face fundamental challenges in
modern data environments, which are characterized by high dimensionality,
source heterogeneity, and stringent data-sharing constraints. In this work, we
revisit the Ising model, a well-established member of the Markov Random Field
(MRF) family, and develop a distributed framework that enables scalable and
privacy-preserving representation learning from large-scale binary data with
inherent low-rank structure. Our approach optimizes a non-convex surrogate loss
function via bi-factored gradient descent, offering substantial computational
and communication advantages over conventional convex approaches. We evaluate
our algorithm on multi-institutional electronic health record (EHR) datasets
from 58,248 patients across the University of Pittsburgh Medical Center (UPMC)
and Mass General Brigham (MGB), demonstrating superior performance in global
representation learning and downstream clinical tasks, including relationship
detection, patient phenotyping, and patient clustering. These results highlight
a broader potential for statistical inference in federated, high-dimensional
settings while addressing the practical challenges of data complexity and
multi-institutional integration.

</details>


### [24] [The Bias-Variance Tradeoff in Long-Term Experimentation](https://arxiv.org/abs/2511.02792)
*Daniel Ting,Kenneth Hung*

Main category: stat.ME

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As we exhaust methods that reduces variance without introducing bias,
reducing variance in experiments often requires accepting some bias, using
methods like winsorization or surrogate metrics. While this bias-variance
tradeoff can be optimized for individual experiments, bias may accumulate over
time, raising concerns for long-term optimization. We analyze whether bias is
ever acceptable when it can accumulate, and show that a bias-variance tradeoff
persists in long-term settings. Improving signal-to-noise remains beneficial,
even if it introduces bias. This implies we should shift from thinking there is
a single ``correct'', unbiased metric to thinking about how to make the best
estimates and decisions when better precision can be achieved at the expense of
bias.
  Furthermore, our model adds nuance to previous findings that suggest less
stringent launch criterion leads to improved gains. We show while this is
beneficial when the system is far from the optimum, more stringent launch
criterion is preferable as the system matures.

</details>
