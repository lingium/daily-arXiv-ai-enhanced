<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 5]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.ME](#stat.ME) [Total: 8]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [The Impact of Formations on Football Matches Using Double Machine Learning. Is it worth parking the bus?](https://arxiv.org/abs/2602.16830)
*Genís Ruiz-Menárguez,Llorenç Badiella*

Main category: stat.AP

TL;DR: 使用双重机器学习框架分析足球阵型对比赛结果的因果影响，发现进攻阵型在控球和角球上有优势但对进球影响有限，防守阵型并不能提高胜率，红牌不受阵型影响。


<details>
  <summary>Details</summary>
Motivation: 解决足球教练面临的核心战术困境：应该采用防守策略（"摆大巴"）还是进攻策略，为教练提供基于数据的战术决策工具。

Method: 采用双重机器学习框架，处理超过22,000场欧洲顶级联赛比赛数据，将阵型分为六种代表类型，通过创新的基于矩阵的残差化过程处理分类处理变量（阵型组合），估计阵型对阵型的效果。

Result: 进攻阵型（如4-3-3和4-2-3-1）在控球和角球方面有适度统计优势，但对进球影响有限；没有证据表明防守阵型能提高获胜概率；红牌不受阵型选择影响。

Conclusion: 虽然该方法未能完全捕捉比赛风格或球队实力的所有方面，但为教练分析战术效率提供了有价值框架，并为体育分析未来研究树立了先例。

Abstract: This study addresses a central tactical dilemma for football coaches: whether to employ a defensive strategy, colloquially known as "parking the bus", or a more offensive one. Using an advanced Double Machine Learning (DML) framework, this project provides a robust and interpretable tool to estimate the causal impact of different formations on key match outcomes such as goal difference, possession, corners, and disciplinary actions. Leveraging a dataset of over 22,000 matches from top European leagues, formations were categorized into six representative types based on tactical structure and expert consultation. A major methodological contribution lies in the adaptation of DML to handle categorical treatments, specifically formation combinations, through a novel matrix-based residualization process, allowing for a detailed estimation of formation-versus-formation effects that can inform a coach's tactical decision-making. Results show that while offensive formations like 4-3-3 and 4-2-3-1 offer modest statistical advantages in possession and corners, their impact on goals is limited. Furthermore, no evidence supports the idea that defensive formations, commonly associated with parking the bus, increase a team's winning potential. Additionally, red cards appear unaffected by formation choice, suggesting other behavioral factors dominate. Although this approach does not fully capture all aspects of playing style or team strength, it provides a valuable framework for coaches to analyze tactical efficiency and sets a precedent for future research in sports analytics.

</details>


### [2] [Temperature and Respiratory Emergency Department Visits: A Mediation Analysis with Ambient Ozone Exposure](https://arxiv.org/abs/2602.16970)
*Chen Li,Thomas W. Hsiao,Stefanie Ebelt,Rebecca H. Zhang,Howard H. Chang*

Main category: stat.AP

TL;DR: 高温通过臭氧部分介导影响呼吸系统急诊就诊，特别是在中等高温条件下


<details>
  <summary>Details</summary>
Motivation: 高温与不良呼吸健康结局相关，但空气污染在温度和呼吸系统发病率关系中的介导作用研究有限，需要量化臭氧在其中的中介效应

Method: 使用因果中介分析分解温度对呼吸急诊就诊的总效应，重点关注臭氧作为中介变量，采用贝叶斯加性回归树(BART)灵活建模温度和臭氧的非线性关系，通过后验预测和贝叶斯自举法量化不确定性

Result: 臭氧部分介导了高温与呼吸急诊就诊之间的关联，特别是在中等高温条件下，模拟研究验证了建模方法的有效性

Conclusion: 该研究通过考虑急性呼吸系统发病率和采用灵活建模方法，扩展了现有文献，为理解温度相关健康风险的机制提供了新见解

Abstract: High temperatures are associated with adverse respiratory health outcomes and increases in ambient air pollution. Limited research has quantified air pollution's mediating role in the relationship between temperature and respiratory morbidity, such as emergency department (ED) visits. In this study, we conducted a causal mediation analysis to decompose the total effect of daily temperature on respiratory ED visits in Los Angeles from 2005 to 2016. We focused on ambient ozone as a mediator because its precursors and formation are directly driven by sunlight and temperature. We estimated natural direct, indirect, and total effects on the relative risk scale across deciles of temperature exposure compared to the median. We utilized Bayesian additive regression trees (BART) to flexibly characterize the nonlinear relationship between temperature and ozone and quantified uncertainty via posterior prediction and the Bayesian bootstrap. Our results showed that ozone partially mediated the association between high temperatures and respiratory ED visits, particularly at moderately high temperatures. We also validated our modeling approach through simulation studies. This study extends the existing literature by considering acute respiratory morbidity and employing a flexible modeling approach, offering new insights into the mechanisms underlying temperature-related health risks.

</details>


### [3] [Using Time Series Measures to Explore Family Planning Survey Data and Model-based Estimates](https://arxiv.org/abs/2602.17034)
*Oluwayomi Akinfenwa,Niamh Cahill,Catherine Hurley*

Main category: stat.AP

TL;DR: 该论文评估了联合国人口司的家庭计划生育估算模型（FPEM）与调查数据的匹配程度，使用时间序列诊断指标来检验模型性能。


<details>
  <summary>Details</summary>
Motivation: 家庭计划生育是全球发展优先事项，但由于各国调查数据存在差距，监测进展面临挑战。需要评估联合国人口司开发的FPEM模型估计值与实际调查数据的匹配程度。

Method: 使用wdisexplorer R包中的时间序列诊断指标，考虑国家嵌套在子区域内的层次结构，通过可视化调查数据、模型轨迹和诊断结果来评估模型性能。

Result: 通过可视化分析能够评估模型性能，显示趋势在何处一致以及在何处存在差异，从而识别模型与调查数据的匹配程度和偏差情况。

Conclusion: 该方法能够有效评估FPEM模型的性能，帮助识别模型估计与调查数据的一致性和差异，为改进家庭计划生育监测提供依据。

Abstract: Family planning is a global development priority and a key indicator of reproductive health. Monitoring progress is challenged by gaps in survey data across countries. The United Nations Population Division addresses this with the Family Planning Estimation Model (FPEM), a Bayesian hierarchical time series model producing annual estimates of modern contraceptive use while sharing information across countries and regions. This paper evaluates how well FPEM estimates align with survey data using time series diagnostic indices from the wdiexplorer R package, which account for countries nested within sub-regions. Visualisation of survey data, modelled trajectories, and diagnostics enables assessment of model performance, highlighting where trends align and where discrepancies occur.

</details>


### [4] [Quantifying the limits of human athletic performance: A Bayesian analysis of elite decathletes](https://arxiv.org/abs/2602.17043)
*Paul-Hieu V. Nguyen,James M. Smoliga,Benton Lindaman,Sameer K. Deshpande*

Main category: stat.AP

TL;DR: 本文开发了一个贝叶斯组合模型来预测运动员在十项全能各项目中的表现，用于估计人类运动潜力的上限和可能达到的最高总分。


<details>
  <summary>Details</summary>
Motivation: 十项全能被认为是衡量运动员综合能力的终极测试，了解其最高可能得分有助于揭示人类运动潜力的上限。通过估计最大十项全能得分和分析实现该得分所需的条件，可以深入理解人类运动能力的极限。

Method: 开发了一个贝叶斯组合模型来预测运动员在十项全能10个项目中的表现。该模型不仅捕捉了成绩随时间变化的潜在非线性趋势，还仔细考虑了每个项目表现与之前所有项目表现之间的依赖关系。

Result: 使用该模型可以模拟和评估最大可能得分的分布，并识别那些能够现实地接近这一极限得分的运动员特征。

Conclusion: 贝叶斯组合模型为理解十项全能运动员的表现极限提供了有效工具，能够识别接近人类运动潜力上限的运动员特征，为评估人类运动能力极限提供了新的方法论。

Abstract: Because the decathlon tests many facets of athleticism, including sprinting, throwing, jumping, and endurance, many consider it to be the ultimate test of athletic ability. On this view, estimating the maximal decathlon score and understanding what it would take to achieve that score provides insight into the upper limits of human athletic potential. To this end, we develop a Bayesian composition model for forecasting how individual athletes perform in each of the 10 decathlon events of time. Besides capturing potential non-linear temporal trends in performance, our model carefully captures the dependence between performance in an event and all preceding events. Using our model, we can simulate and evaluate the distribution of the maximal possible scores and identify profiles of athletes who could realistically attain scores approaching this limit.

</details>


### [5] [huff: A Python package for Market Area Analysis](https://arxiv.org/abs/2602.17640)
*Thomas Wieland*

Main category: stat.AP

TL;DR: huff Python包提供完整的市场区域分析工作流，包括数据导入、OD矩阵构建、Huff模型分析、参数估计、距离/时间指标计算和地图可视化，适用于零售、医疗等多个领域。


<details>
  <summary>Details</summary>
Motivation: 市场区域模型（如Huff模型）广泛应用于零售、服务、医疗等领域的市场份额和客户流分析，但目前缺乏一个完整的Python工具包来支持从数据导入到可视化分析的完整工作流程。

Method: 开发了一个模块化、面向对象的Python包，提供数据导入、OD交互矩阵构建、基本模型分析、经验数据参数估计、距离/时间指标计算、地图可视化以及多种空间可达性分析方法。

Result: huff包已在PyPI公开发布，源代码托管在GitHub，并在Zenodo存档，为经济地理学、区域经济学、空间规划、市场营销、地理信息科学和健康地理学等领域的研究者提供了完整的分析工具。

Conclusion: huff Python包填补了市场区域分析工具的空缺，提供了一个全面、开源的工作流程解决方案，支持多种应用场景，有助于推动相关领域的研究和实践。

Abstract: Market area models, such as the Huff model and its extensions, are widely used to estimate regional market shares and customer flows of retail and service locations. Another, now very common, area of application is the analysis of catchment areas, supply structures and the accessibility of healthcare locations. The huff Python package provides a complete workflow for market area analysis, including data import, construction of origin-destination interaction matrices, basic model analysis, parameter estimation from empirical data, calculation of distance or travel time indicators, and map visualization. Additionally, the package provides several methods of spatial accessibility analysis. The package is modular and object-oriented. It is intended for researchers in economic geography, regional economics, spatial planning, marketing, geoinformation science, and health geography. The software is openly available via the [Python Package Index (PyPI)](https://pypi.org/project/huff/); its development and version history are managed in a public [GitHub Repository](https://github.com/geowieland/huff_official) and archived at [Zenodo](https://doi.org/10.5281/zenodo.18639559).

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [6] [Beyond Procedure: Substantive Fairness in Conformal Prediction](https://arxiv.org/abs/2602.16794)
*Pengqi Liu,Zijun Yu,Mouloud Belbahri,Arthur Charpentier,Masoud Asgharian,Jesse C. Cresswell*

Main category: stat.ML

TL;DR: 本文研究共形预测（CP）与下游决策公平性的关系，提出通过标签聚类CP控制不公平性，并引入LLM评估器分析实质性公平，发现均衡预测集大小能提升公平性。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽然提供分布无关的不确定性量化，但其与下游决策公平性的关系尚未充分探索。现有研究多关注CP作为独立操作的"程序公平性"，而本文旨在分析整个决策流程，评估"实质性公平性"——即下游结果的公平性。

Method: 1) 理论推导：建立预测集大小差异的上界分解为可解释组件，阐明标签聚类CP如何控制方法驱动的不公平性；2) 实证分析：引入LLM-in-the-loop评估器，近似人类对多模态实质性公平的评估；3) 实验验证：比较不同CP变体的公平性表现。

Result: 1) 标签聚类CP变体始终提供更优的实质性公平性；2) 均衡预测集大小（而非覆盖率）与改善的实质性公平性强相关；3) LLM评估器能有效近似人类评估，支持可扩展的公平性分析。

Conclusion: 标签聚类CP能有效控制方法驱动的不公平性，均衡预测集大小是提升实质性公平性的关键因素，为实践者设计更公平的CP系统提供了理论指导和实用工具。

Abstract: Conformal prediction (CP) offers distribution-free uncertainty quantification for machine learning models, yet its interplay with fairness in downstream decision-making remains underexplored. Moving beyond CP as a standalone operation (procedural fairness), we analyze the holistic decision-making pipeline to evaluate substantive fairness-the equity of downstream outcomes. Theoretically, we derive an upper bound that decomposes prediction-set size disparity into interpretable components, clarifying how label-clustered CP helps control method-driven contributions to unfairness. To facilitate scalable empirical analysis, we introduce an LLM-in-the-loop evaluator that approximates human assessment of substantive fairness across diverse modalities. Our experiments reveal that label-clustered CP variants consistently deliver superior substantive fairness. Finally, we empirically show that equalized set sizes, rather than coverage, strongly correlate with improved substantive fairness, enabling practitioners to design more fair CP systems. Our code is available at https://github.com/layer6ai-labs/llm-in-the-loop-conformal-fairness.

</details>


### [7] [Poisson-MNL Bandit: Nearly Optimal Dynamic Joint Assortment and Pricing with Decision-Dependent Customer Arrivals](https://arxiv.org/abs/2602.16923)
*Junhui Cai,Ran Chen,Qitao Huang,Linda Zhao,Wu Zhu*

Main category: stat.ML

TL;DR: 本文提出了一种考虑到达率依赖性的动态联合品类定价优化方法，使用Poisson-MNL模型和UCB算法实现近乎最优的后悔界。


<details>
  <summary>Details</summary>
Motivation: 传统MNL模型假设顾客到达率固定，忽略了品类和价格对顾客到达数量的影响，这可能导致次优决策。实际中，商家需要同时优化品类和定价，而这两者都会影响顾客到达量。

Method: 提出Poisson-MNL模型，将上下文MNL选择模型与依赖品类和价格的Poisson到达模型结合。基于UCB思想开发PMNL算法，通过置信上界平衡探索与利用。

Result: 证明了PMNL算法具有√(T log T)阶的非渐近后悔上界，并给出了匹配的下界（相差log T因子）。仿真表明PMNL能有效学习顾客选择和到达模型，优于假设固定到达率的方法。

Conclusion: 考虑到达率对品类和价格的依赖性至关重要。Poisson-MNL模型和PMNL算法为动态联合品类定价问题提供了理论保证和实际有效的解决方案。

Abstract: We study dynamic joint assortment and pricing where a seller updates decisions at regular accounting/operating intervals to maximize the cumulative per-period revenue over a horizon $T$. In many settings, assortment and prices affect not only what an arriving customer buys but also how many customers arrive within the period, whereas classical multinomial logit (MNL) models assume arrivals as fixed, potentially leading to suboptimal decisions. We propose a Poisson-MNL model that couples a contextual MNL choice model with a Poisson arrival model whose rate depends on the offered assortment and prices. Building on this model, we develop an efficient algorithm PMNL based on the idea of upper confidence bound (UCB). We establish its (near) optimality by proving a non-asymptotic regret bound of order $\sqrt{T\log{T}}$ and a matching lower bound (up to $\log T$). Simulation studies underscore the importance of accounting for the dependency of arrival rates on assortment and pricing: PMNL effectively learns customer choice and arrival models and provides joint assortment-pricing decisions that outperform others that assume fixed arrival rates.

</details>


### [8] [Semi-Supervised Learning on Graphs using Graph Neural Networks](https://arxiv.org/abs/2602.17115)
*Juntong Chen,Claire Donnat,Olga Klopp,Johannes Schmidt-Hieber*

Main category: stat.ML

TL;DR: 该论文为图神经网络在半监督节点回归任务中的性能提供了首个严格的理论分析框架，通过分离近似误差、随机误差和优化误差，建立了明确的性能边界。


<details>
  <summary>Details</summary>
Motivation: 虽然图神经网络在半监督节点回归任务中表现出色，但缺乏解释其成功条件和原因的理论基础。现有研究缺乏对GNN性能的严格理论分析，特别是在标签稀缺情况下的性能理解。

Method: 提出一个聚合-读出模型框架，包含常见的消息传递架构。采用线性图卷积和深度ReLU读出函数的GNN进行最小二乘估计，推导出分离近似误差、随机误差和优化误差的非渐近风险边界。进一步推导图平滑后接平滑非线性读出的近似保证。

Result: 证明了尖锐的非渐近风险边界，明确展示了性能如何随标记节点比例和图诱导依赖关系变化。在完全监督下恢复经典非参数行为，在标签稀缺时表征性能。数值实验验证了理论框架。

Conclusion: 该研究为理解GNN性能和局限性提供了系统的理论框架，填补了图神经网络理论分析的空白，为半监督节点回归任务中的GNN设计提供了理论指导。

Abstract: Graph neural networks (GNNs) work remarkably well in semi-supervised node regression, yet a rigorous theory explaining when and why they succeed remains lacking. To address this gap, we study an aggregate-and-readout model that encompasses several common message passing architectures: node features are first propagated over the graph then mapped to responses via a nonlinear function. For least-squares estimation over GNNs with linear graph convolutions and a deep ReLU readout, we prove a sharp non-asymptotic risk bound that separates approximation, stochastic, and optimization errors. The bound makes explicit how performance scales with the fraction of labeled nodes and graph-induced dependence. Approximation guarantees are further derived for graph-smoothing followed by smooth nonlinear readouts, yielding convergence rates that recover classical nonparametric behavior under full supervision while characterizing performance when labels are scarce. Numerical experiments validate our theory, providing a systematic framework for understanding GNN performance and limitations.

</details>


### [9] [MGD: Moment Guided Diffusion for Maximum Entropy Generation](https://arxiv.org/abs/2602.17211)
*Etienne Lempereur,Nathanaël Cuvelle--Magar,Florentin Coeurdoux,Stéphane Mallat,Eric Vanden-Eijnden*

Main category: stat.ML

TL;DR: Moment Guided Diffusion (MGD) 结合了最大熵方法和生成模型，通过随机微分方程在有限时间内引导矩向指定值收敛，避免了传统方法的指数级慢化问题。


<details>
  <summary>Details</summary>
Motivation: 传统最大熵方法需要MCMC或Langevin动力学采样，在高维空间中存在指数级慢化问题；而基于扩散和流匹配的生成模型虽然采样高效，但理论保证有限且在数据稀缺时容易过拟合。

Method: 基于随机插值框架，MGD通过求解随机微分方程在有限时间内引导矩向指定值收敛，在大波动极限下证明收敛到最大熵分布，并推导出可直接从动力学计算熵的易处理估计器。

Result: 应用于金融时间序列、湍流和宇宙学场，使用小波散射矩估计高维多尺度过程的负熵。

Conclusion: MGD成功结合了最大熵方法的理论保证和生成模型的高效采样能力，为高维数据稀缺情况下的不确定性量化提供了新方法。

Abstract: Generating samples from limited information is a fundamental problem across scientific domains. Classical maximum entropy methods provide principled uncertainty quantification from moment constraints but require sampling via MCMC or Langevin dynamics, which typically exhibit exponential slowdown in high dimensions. In contrast, generative models based on diffusion and flow matching efficiently transport noise to data but offer limited theoretical guarantees and can overfit when data is scarce. We introduce Moment Guided Diffusion (MGD), which combines elements of both approaches. Building on the stochastic interpolant framework, MGD samples maximum entropy distributions by solving a stochastic differential equation that guides moments toward prescribed values in finite time, thereby avoiding slow mixing in equilibrium-based methods. We formally obtain, in the large-volatility limit, convergence of MGD to the maximum entropy distribution and derive a tractable estimator of the resulting entropy computed directly from the dynamics. Applications to financial time series, turbulent flows, and cosmological fields using wavelet scattering moments yield estimates of negentropy for high-dimensional multiscale processes.

</details>


### [10] [SOLVAR: Fast covariance-based heterogeneity analysis with pose refinement for cryo-EM](https://arxiv.org/abs/2602.17603)
*Roey Yadgar,Roy R. Lederman,Yoel Shkolnisky*

Main category: stat.ML

TL;DR: SOLVAR是一种用于冷冻电镜连续异质性分析的随机优化方法，通过低秩协方差矩阵假设实现高效的主成分估计，并能同时优化粒子图像姿态。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜中连续异质性分析是一个关键挑战，现有协方差方法在估计协方差矩阵时面临计算困难，且大多数方法无法同时优化输入粒子图像的姿态。

Method: 提出SOLVAR方法，利用协方差矩阵的低秩假设，将主成分估计转化为可快速准确求解的优化问题，同时框架支持输入粒子图像姿态的优化。

Result: 在合成和实验数据集上的数值实验表明，算法能准确捕捉变异的主要成分并保持计算效率，在最近的异质性基准测试中达到最先进性能。

Conclusion: SOLVAR为冷冻电镜连续异质性分析提供了一种高效、准确的协方差估计方法，具有姿态优化能力，代码已开源。

Abstract: Cryo-electron microscopy (cryo-EM) has emerged as a powerful technique for resolving the three-dimensional structures of macromolecules. A key challenge in cryo-EM is characterizing continuous heterogeneity, where molecules adopt a continuum of conformational states. Covariance-based methods offer a principled approach to modeling structural variability. However, estimating the covariance matrix efficiently remains a challenging computational task. In this paper, we present SOLVAR (Stochastic Optimization for Low-rank Variability Analysis), which leverages a low-rank assumption on the covariance matrix to provide a tractable estimator for its principal components, despite the apparently prohibitive large size of the covariance matrix. Under this low-rank assumption, our estimator can be formulated as an optimization problem that can be solved quickly and accurately. Moreover, our framework enables refinement of the poses of the input particle images, a capability absent from most heterogeneity-analysis methods, and all covariance-based methods. Numerical experiments on both synthetic and experimental datasets demonstrate that the algorithm accurately captures dominant components of variability while maintaining computational efficiency. SOLVAR achieves state-of-the-art performance across multiple datasets in a recent heterogeneity benchmark. The code of the algorithm is freely available at https://github.com/RoeyYadgar/SOLVAR.

</details>


### [11] [genriesz: A Python Package for Automatic Debiased Machine Learning with Generalized Riesz Regression](https://arxiv.org/abs/2602.17543)
*Masahiro Kato*

Main category: stat.ML

TL;DR: genriesz是一个Python包，通过Riesz表示定理和去偏机器学习自动估计因果和结构参数，提供模块化接口和多种估计器。


<details>
  <summary>Details</summary>
Motivation: 自动化因果和结构参数的高效估计，统一Riesz表示器估计框架，简化复杂因果推断问题的实现。

Method: 基于广义Riesz回归，通过最小化经验Bregman散度估计Riesz表示器，采用自动回归器平衡(ARB)设计原则，提供模块化接口配置目标函数、表示器模型和Bregman生成器。

Result: 实现了回归调整(RA)、Riesz加权(RW)、增强Riesz加权(ARW)和TMLE风格估计器，支持交叉拟合、置信区间和p值计算，适用于ATE、ATT和平均边际效应等估计问题。

Conclusion: genriesz提供了一个统一、自动化的框架来估计因果参数，简化了复杂因果推断任务的实现，可作为开源工具供研究者和实践者使用。

Abstract: Efficient estimation of causal and structural parameters can be automated using the Riesz representation theorem and debiased machine learning (DML). We present genriesz, an open-source Python package that implements automatic DML and generalized Riesz regression, a unified framework for estimating Riesz representers by minimizing empirical Bregman divergences. This framework includes covariate balancing, nearest-neighbor matching, calibrated estimation, and density ratio estimation as special cases. A key design principle of the package is automatic regressor balancing (ARB): given a Bregman generator $g$ and a representer model class, genriesz} automatically constructs a compatible link function so that the generalized Riesz regression estimator satisfies balancing (moment-matching) optimality conditions in a user-chosen basis. The package provides a modulr interface for specifying (i) the target linear functional via a black-box evaluation oracle, (ii) the representer model via basis functions (polynomial, RKHS approximations, random forest leaf encodings, neural embeddings, and a nearest-neighbor catchment basis), and (iii) the Bregman generator, with optional user-supplied derivatives. It returns regression adjustment (RA), Riesz weighting (RW), augmented Riesz weighting (ARW), and TMLE-style estimators with cross-fitting, confidence intervals, and $p$-values. We highlight representative workflows for estimation problems such as the average treatment effect (ATE), ATE on treated (ATT), and average marginal effect estimation. The Python package is available at https://github.com/MasaKat0/genriesz and on PyPI.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [12] [First versus full or first versus last: U-statistic change-point tests under fixed and local alternatives](https://arxiv.org/abs/2602.16789)
*Herold Dehling,Daniel Vogel,Martin Wendler*

Main category: stat.ME

TL;DR: 比较两种基于U统计量的变点检验方法（first-vs-full vs first-vs-last），分析它们在大小样本中的差异，推导选择更优检验的简单准则，并以Gini均值差、样本方差和Kendall's tau为例详细说明。


<details>
  <summary>Details</summary>
Motivation: 在变点分析中，基于U统计量的CUSUM型检验有两种主要构造方法（first-vs-full和first-vs-last），不同研究者采用了不同方法。自然产生的问题是：这两种检验是否有显著差异？如果有，在什么数据情况下哪种方法更好？

Method: 比较两种检验方法的理论性质：在大样本下分析渐近等价性，在小样本下研究差异；推导判断哪种检验更有效的简单准则；以Gini均值差、样本方差和Kendall's tau三个具体统计量为例进行详细分析；在弱依赖假设下进行渐近推导；通过数值模拟和实际数据示例验证结果。

Result: 大样本下两种检验渐近等价（在原假设和局部备择假设下）；小样本下可能存在明显差异，这与固定备择下的不同渐近行为一致；推导出判断哪种检验更有效的简单准则；特别地，对于Gini均值差的尺度变化检验，first-vs-full方法当且仅当尺度从小变大时更有效，且与总体分布和变点位置无关。

Conclusion: 两种基于U统计量的变点检验方法在大样本下相似，但在小样本和特定应用场景下存在差异。研究提供了选择更优检验的准则，特别在尺度变化检验中，first-vs-full方法在尺度从小变大时具有更高功效。这些结果为实际应用中的方法选择提供了理论指导。

Abstract: The use of U-statistics in the change-point context has received considerable attention in the literature. We compare two approaches of constructing CUSUM-type change-point tests, which we call the first-vs-full and first-vs-last approach. Both have been pursued by different authors. The question naturally arises if the two tests substantially differ and, if so, which of them is better in which data situation. In large samples, both tests are similar: they are asymptotically equivalent under the null hypothesis and under sequences of local alternatives. In small samples, there may be quite noticeable differences, which is in line with a different asymptotic behavior under fixed alternatives. We derive a simple criterion for deciding which test is more powerful. We examine the examples Gini's mean difference, the sample variance, and Kendall's tau in detail. Particularly, when testing for changes in scale by Gini's mean difference, we show that the first-vs-full approach has a higher power if and only if the scale changes from a smaller to a larger value -- regardless of the population distribution or the location of the change. The asymptotic derivations are under weak dependence. The results are illustrated by numerical simulations and data examples.

</details>


### [13] [Modeling Multivariate Missingness with Tree Graphs and Conjugate Odds](https://arxiv.org/abs/2602.16992)
*Daniel Suen,Yen-Chi Chen*

Main category: stat.ME

TL;DR: 该论文分析了一类特定的非随机缺失(MNAR)假设——树图，扩展了模式图的工作，引入了共轭优势族概念，在树图假设下能够优雅地建模完整数据分布。


<details>
  <summary>Details</summary>
Motivation: 研究非随机缺失(MNAR)数据问题，特别是扩展模式图方法，开发更灵活且易于处理的缺失数据建模框架，以处理现实世界中常见的复杂缺失模式。

Method: 引入共轭优势族概念，在树图假设下建模完整数据分布；开发观测数据的共轭模型和缺失数据的简单插补模型；研究图选择、敏感性分析和统计推断问题。

Result: 通过模拟和真实数据验证了方法的适用性，展示了在共轭优势族和树图假设下能够获得优雅的完整数据分布模型，观测数据保持共轭性，缺失数据有简单插补模型。

Conclusion: 提出的树图框架扩展了MNAR建模能力，共轭优势族提供了保持数据分布族的参数化方法，为复杂缺失数据模式的分析提供了实用的统计工具。

Abstract: In this paper, we analyze a specific class of missing not at random (MNAR) assumptions called tree graphs, extending upon the work of pattern graphs. We build off previous work by introducing the idea of a conjugate odds family in which certain parametric models on the selection odds can preserve the data distribution family across all missing data patterns. Under a conjugate odds family and a tree graph assumption, we are able to model the full data distribution elegantly in the sense that for the observed data, we obtain a model that is conjugate from the complete-data, and for the missing entries, we create a simple imputation model. In addition, we investigate the problem of graph selection, sensitivity analysis, and statistical inference. Using both simulations and real data, we illustrate the applicability of our method.

</details>


### [14] [Reframing Population-Adjusted Indirect Comparisons as a Transportability Problem: An Estimand-Based Perspective and Implications for Health Technology Assessment](https://arxiv.org/abs/2602.17041)
*Conor Chandler,Jack Ishak*

Main category: stat.ME

TL;DR: PAIC方法虽能调整试验间人口差异，但调整本身不能保证效果估计能外推到HTA决策相关人群。本文从估计量角度形式化PAIC的可迁移性，区分条件与边际治疗效果估计量，并展示可迁移性取决于效应修正、可折叠性及效应修正尺度与效应度量的一致性。


<details>
  <summary>Details</summary>
Motivation: PAIC方法广泛应用于HTA，但调整人口差异后估计的效果是否能外推到决策相关人群存在不确定性。当前缺乏对PAIC可迁移性的系统分析，这可能导致HTA决策中错误应用间接证据。

Method: 从估计量角度形式化PAIC的可迁移性，区分条件与边际治疗效果估计量，分析效应修正、可折叠性及效应度量尺度对可迁移性的影响，通过示例说明不同效应度量的可迁移性差异。

Result: 对于常用的非可折叠度量（如风险比、优势比），边际效应通常具有人群依赖性；而在线性预测尺度上定义的可折叠和条件效应则表现出更好的可迁移性。配对PAIC方法通常识别比较人群中的效应，将其应用于其他人群需要额外的可迁移假设。

Conclusion: PAIC推导的治疗效果应用于目标人群需要谨慎，需明确何时可迁移、何时需要额外假设、何时应解释为人群特异性而非决策相关。这支持在HTA和相关决策中更透明、更有原则地使用间接证据。

Abstract: Population-adjusted indirect comparisons (PAICs) are widely used to synthesize evidence when randomized controlled trials enroll different patient populations and head-to-head comparisons are unavailable. Although PAICs adjust for observed population differences across trials, adjustment alone does not ensure transportability of estimated effects to decision-relevant populations for health technology assessment (HTA). We examine and formalize transportability in PAICs from an estimand-based perspective. We distinguish conditional and marginal treatment effect estimands and show how transportability depends on effect modification, collapsibility, and alignment between the scale of effect modification and the effect measure. Using illustrative examples, we demonstrate that even when effect modifiers are shared across treatments, marginal effects are generally population-dependent for commonly used non-collapsible measures, including hazard ratios and odds ratios. Conversely, collapsible and conditional effects defined on the linear predictor scale exhibit more favorable transportability properties. We further show that pairwise PAIC approaches typically identify effects defined in the comparator population and that applying these estimates to other populations entails an additional, often implicit, transport step requiring further assumptions. This has direct implications for HTA, where PAIC-derived effects are routinely applied within cost-effectiveness and decision models defined for different target populations. Our results clarify when applying PAIC-derived treatment effects to desired target populations is justified, when doing so requires additional assumptions, and when results should instead be interpreted as population-specific rather than decision-relevant, supporting more transparent and principled use of indirect evidence in HTA and related decision-making contexts.

</details>


### [15] [General sample size analysis for probabilities of causation: a delta method approach](https://arxiv.org/abs/2602.17070)
*Tianyuan Cheng,Ruirui Mao,Judea Pearl,Ang Li*

Main category: stat.ME

TL;DR: 提出基于delta方法的样本量分析框架，用于估计因果概率边界，确保达到期望的误差范围


<details>
  <summary>Details</summary>
Motivation: 因果概率（如必要性和充分性概率）对决策很重要，但通常无法点识别。现有研究使用实验和观测数据推导边界，但缺乏样本量分析研究，即需要多少实验和观测样本才能达到期望的误差范围

Method: 提出基于delta方法的通用样本量框架，适用于因果概率边界可表示为实验和观测概率线性组合的有限最小值或最大值的情况

Result: 通过模拟研究证明，提出的样本量计算方法能够稳定估计这些边界

Conclusion: 该研究填补了因果概率边界估计中样本量分析的空白，为实际应用提供了可靠的样本量规划工具

Abstract: Probabilities of causation (PoCs), such as the probability of necessity and sufficiency (PNS), are important tools for decision making but are generally not point identifiable. Existing work has derived bounds for these quantities using combinations of experimental and observational data. However, there is very limited research on sample size analysis, namely, how many experimental and observational samples are required to achieve a desired margin of error. In this paper, we propose a general sample size framework based on the delta method. Our approach applies to settings in which the target bounds of PoCs can be expressed as finite minima or maxima of linear combinations of experimental and observational probabilities. Through simulation studies, we demonstrate that the proposed sample size calculations lead to stable estimation of these bounds.

</details>


### [16] [M-estimation under Two-Phase Multiwave Sampling with Applications to Prediction-Powered Inference](https://arxiv.org/abs/2602.16933)
*Dan M. Kluger,Stephen Bates*

Main category: stat.ME

TL;DR: 提出一种用于自适应两阶段多波抽样的M估计方法，结合代理变量和昂贵测量进行有效无偏估计


<details>
  <summary>Details</summary>
Motivation: 两阶段多波抽样中，自适应收集昂贵测量能提高效率但使统计推断复杂化，需要有效的估计方法和置信区间

Method: 提出Multiwave Predict-Then-Debias估计器，结合所有单元的代理变量（如预训练ML模型预测）和昂贵测量，同时开发近似贪婪抽样策略

Result: 建立了估计量的渐近线性和正态性，提出渐近有效的置信区间，数据模拟研究支持理论结果并展示效率提升

Conclusion: 该方法能在自适应两阶段多波抽样中实现有效的统计推断，通过结合代理信息和昂贵测量提高效率，近似贪婪抽样策略优于均匀抽样

Abstract: In two-phase multiwave sampling, inexpensive measurements are collected on a large sample and expensive, more informative measurements are adaptively obtained on subsets of units across multiple waves. Adaptively collecting the expensive measurements can increase efficiency but complicates statistical inference. We give valid estimators and confidence intervals for M-estimation under adaptive two-phase multiwave sampling. We focus on the case where proxies for the expensive variables -- such as predictions from pretrained machine learning models -- are available for all units and propose a Multiwave Predict-Then-Debias estimator that combines proxy information with the expensive, higher-quality measurements to improve efficiency while removing bias. We establish asymptotic linearity and normality and propose asymptotically valid confidence intervals. We also develop an approximately greedy sampling strategy that improves efficiency relative to uniform sampling. Data-based simulation studies support the theoretical results and demonstrate efficiency gains.

</details>


### [17] [Parametric or nonparametric: the FIC approach for stationary time series](https://arxiv.org/abs/2602.17261)
*Gudmund Hermansen,Nils Lid Hjort,Martin Jullum*

Main category: stat.ME

TL;DR: 提出新版本的聚焦信息准则(FIC)，用于比较参数化时间序列模型与非参数化模型的性能，通过特定聚焦参数的均方误差来评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 缩小参数化与非参数化时间序列建模之间的差距，利用聚焦推断和模型选择技术的最新进展，为特定关注参数提供更精确的模型比较方法。

Method: 开发新版本的聚焦信息准则(FIC)，通过比较模型对特定聚焦参数估计量的均方误差来直接评估参数化与非参数化模型的性能。扩展了加权平均版本(AFIC)，用于估计相关序列等目的。

Result: 推导出适用于特定滞后协方差或相关性、阈值概率等聚焦参数的FIC公式，并开发了AFIC模型选择策略，能够找到用于估计相关序列等特定目的的最佳模型。

Conclusion: 新FIC方法有效缩小了参数化与非参数化时间序列建模的差距，为特定关注参数提供了实用的模型选择工具，特别适用于协方差、相关性和概率估计等应用场景。

Abstract: We seek to narrow the gap between parametric and nonparametric modelling of stationary time series processes. The approach is inspired by recent advances in focused inference and model selection techniques. The paper generalises and extends recent work by developing a new version of the focused information criterion (FIC), directly comparing the performance of parametric time series models with a nonparametric alternative. For a pre-specified focused parameter, for which scrutiny is considered valuable, this is achieved by comparing the mean squared error of the model-based estimators of this quantity. In particular, this yields FIC formulae for covariances or correlations at specified lags, for the probability of reaching a threshold, etc. Suitable weighted average versions, the AFIC, also lead to model selection strategies for finding the best model for the purpose of estimating e.g.~a sequence of correlations.

</details>


### [18] [An extension to reversible jump Markov chain Monte Carlo for change point problems with heterogeneous temporal dynamics](https://arxiv.org/abs/2602.17503)
*Emily Gribbin,Benjamin Davis,Daniel Rolfe,Hannah Mitchell*

Main category: stat.ME

TL;DR: 提出一种基于RJMCMC的复合变化点检测算法（CRJMCMC），用于处理具有异质时间动态的时间序列数据，特别针对单分子定位显微镜中的光漂白步骤分析，能有效处理闪烁和暗态等短寿命事件。


<details>
  <summary>Details</summary>
Motivation: 单分子定位显微镜中，用于标记蛋白质寡聚体的荧光分子表现出异质光物理行为，这会使光漂白步骤分析复杂化。现有方法通常需要大量过滤或先验校准，且难以准确处理可能污染下游分析的闪烁或可逆暗态。

Method: 提出RJMCMC的扩展方法，引入复合变化点对移动（compound change point pair moves），以更好地处理短寿命事件（闪烁和暗态）。该方法应用于从FLImP数据中估计每帧活性荧光团计数。

Result: 在模拟和实验数据上验证了该方法，相比当前光漂白步骤分析方法和现有FLImP数据分析方法，表现出更高的准确性和鲁棒性。CRJMCMC算法在宽范围的荧光团计数（高达17个）和低信噪比（低至0.001）条件下均表现可靠，并能有效估计EGFR寡聚化研究中的低计数。

Conclusion: CRJMCMC算法为具有异质状态持续性的时间序列变化点检测问题提供了有效解决方案，不仅适用于单分子成像，还可应用于脑电图脑状态分割、工业过程监测中的故障检测和金融时间序列中的已实现波动率等多种领域。

Abstract: Detecting brief changes in time-series data remains a major challenge in fields where short-lived states carry meaning. In single-molecule localisation microscopy, this problem is particularly acute as fluorescent molecules used to tag protein oligomers display heterogenous photophysical behaviour that can complicate photobleach step analysis; a key step in resolving nanoscale protein organisation. Existing methods often require extensive filtering or prior calibration, and can fail to accurately account for blinking or reversible dark states that may contaminate downstream analysis. In this paper, an extension to RJMCMC is proposed for change point detection with heterogeneous temporal dynamics. This approach is applied to the problem of estimating per-frame active fluorophore counts from one-dimensional integrated intensity traces derived from Fluorescence Localisation Imaging with Photobleaching (FLImP), where compound change point pair moves are introduced to better account for short-lived events known as blinking and dark states. The approach is validated using simulated and experimental data, demonstrating improved accuracy and robustness when compared with current photobleach step analysis methods and with the existing analysis approach for FLImP data. This Compound RJMCMC (CRJMCMC) algorithm performs reliably across a wide range of fluorophore counts and signal-to-noise conditions, with signal-to-noise ratio (SNR) down to 0.001 and counts as high as seventeen fluorophores, while also effectively estimating low counts observed when studying EGFR oligomerisation. Beyond single molecule imaging, this work has applications for a variety of time series change point detection problems with heterogeneous state persistence. For example, electrocorticography brain-state segmentation, fault detection in industrial process monitoring and realised volatility in financial time series.

</details>


### [19] [BMW: Bayesian Model-Assisted Adaptive Phase II Clinical Trial Design for Win Ratio Statistic](https://arxiv.org/abs/2602.17592)
*Di Zhu,Yong Zang*

Main category: stat.ME

TL;DR: 提出了一种基于胜率统计量的贝叶斯模型辅助自适应设计（BMW设计），用于随机化II期临床试验，无需指定基础结果分布，支持灵活的中期监测和早期终止。


<details>
  <summary>Details</summary>
Motivation: 胜率统计量越来越多地用于基于优先复合终点评估治疗效果，但现有的贝叶斯自适应设计不直接适用，因为胜率是从成对比较中得出的汇总统计量，不对应于唯一的数据生成机制。

Method: 提出BMW设计，利用胜率检验统计量在中期和最终分析中的联合渐近分布计算后验概率，无需指定基础结果分布。设计支持灵活的中期监测（早期终止无效或优效），并扩展到使用图形测试程序联合评估疗效和毒性，控制家族错误率。

Result: 模拟研究表明，BMW设计保持了有效的I类错误和家族错误率控制，达到与传统方法相当的检验效能，并显著减少了预期样本量。提供了R Shiny应用程序以促进实际实施。

Conclusion: BMW设计为基于胜率统计量的II期临床试验提供了一种有效的贝叶斯自适应设计框架，解决了现有方法不适用的问题，具有实际应用价值。

Abstract: The win ratio (WR) statistic is increasingly used to evaluate treatment effects based on prioritized composite endpoints, yet existing Bayesian adaptive designs are not directly applicable because the WR is a summary statistic derived from pairwise comparisons and does not correspond to a unique data-generating mechanism. We propose a Bayesian model-assisted adaptive design for randomized phase II clinical trials based on the WR statistic, referred to as the BMW design. The proposed design uses the joint asymptotic distribution of WR test statistics across interim and final analyses to compute posterior probabilities without specifying the underlying outcome distribution. The BMW design allows flexible interim monitoring with early stopping for futility or superiority and is extended to jointly evaluate efficacy and toxicity using a graphical testing procedure that controls the family-wise error rate (FWER). Simulation studies demonstrate that the BMW design maintains valid type I error and FWER control, achieves power comparable to conventional methods, and substantially reduces expected sample size. An R Shiny application is provided to facilitate practical implementation.

</details>
