<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 10]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.AP](#stat.AP) [Total: 2]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Fast Compute via MC Boosting](https://arxiv.org/abs/2602.05032)
*Sarah Polson,Vadim Sokolov*

Main category: stat.CO

TL;DR: 论文提出Monte Carlo boosting作为线性系统求解的实用替代方案，特别适用于需要重复求解或仅需部分信息的情况，通过随机游走估计器和序列残差校正来加速计算。


<details>
  <summary>Details</summary>
Motivation: 现代统计学习和深度学习训练推理流程中，线性系统求解作为内循环被反复调用，但高精度确定性求解器在需要多次重复求解或仅需部分信息时计算成本过高。

Method: 采用Monte Carlo boosting方法，包括随机游走估计器、序列残差校正（Neumann级数表示、前向/伴随估计器、Halton式序列校正），并扩展到超定/最小二乘问题，与IRLS式更新、数据增强和EM/ECM算法建立联系。

Result: 通过实验比较Jacobi、Gauss-Seidel迭代与普通Monte Carlo、精确序列Monte Carlo以及子采样序列变体，展示了Monte Carlo boosting在不同规模下的性能表现，说明了其在现代统计学习工作流中的适用场景。

Conclusion: Monte Carlo boosting可以作为现代统计学习工作流中的关键计算原语，在特定规模下能够显著提升线性系统求解的效率，为需要重复求解或部分信息提取的场景提供实用替代方案。

Abstract: Modern training and inference pipelines in statistical learning and deep learning repeatedly invoke linear-system solves as inner loops, yet high-accuracy deterministic solvers can be prohibitively expensive when solves must be repeated many times or when only partial information (selected components or linear functionals) is required. We position \emph{Monte Carlo boosting} as a practical alternative in this regime, surveying random-walk estimators and sequential residual correction in a unified notation (Neumann-series representation, forward/adjoint estimators, and Halton-style sequential correction), with extensions to overdetermined/least-squares problems and connections to IRLS-style updates in data augmentation and EM/ECM algorithms. Empirically, we compare Jacobi and Gauss--Seidel iterations with plain Monte Carlo, exact sequential Monte Carlo, and a subsampled sequential variant, illustrating scaling regimes that motivate when Monte Carlo boosting can be an enabling compute primitive for modern statistical learning workflows.

</details>


### [2] [Optimal Accelerated Life Testing Sampling Plan Design with Piecewise Linear Function based Modeling of Lifetime Characteristics](https://arxiv.org/abs/2602.05377)
*Sandip Barui,Shovan Chowdhury*

Main category: stat.CO

TL;DR: 该论文提出了一种基于非线性链接的加速寿命试验优化抽样计划，使用分段线性函数近似非线性关系，优于传统的线性链接模型。


<details>
  <summary>Details</summary>
Motivation: 传统加速寿命试验假设寿命特性与加速应力因子之间存在已知的线性关系，但在实际应用中真实关系往往是未知的，这种假设可能导致有偏估计和低效的抽样计划。

Method: 采用I型截尾方案，使用类似样条回归的广义链接结构捕捉寿命特性与应力水平之间的非线性关系。假设产品寿命服从威布尔分布，其尺度参数和形状参数通过分段线性函数与应力因子关联。详细计算Fisher信息矩阵元素以制定合格批次的验收标准，通过约束聚合成本最小化和方差最小化方法确定抽样计划的决策变量。

Result: 模拟案例研究表明，基于非线性链接的分段线性近似模型优于基于线性链接的模型。

Conclusion: 提出的方法能够更准确地捕捉寿命与应力之间的非线性关系，从而设计出更有效的加速寿命试验抽样计划，减少因模型假设不当导致的偏差。

Abstract: Researchers have widely used accelerated life tests to determine an optimal inspection plan for lot acceptance. All such plans are proposed by assuming a known relationship between the lifetime characteristic(s) and the accelerating stress factor(s) under a parametric framework of the product lifetime distribution. As the true relationship is rarely known in practical scenarios, the assumption itself may produce biased estimates that may lead to an inefficient sampling plan. To this endeavor, an optimal accelerating life test plan is designed under a Type-I censoring scheme with a generalized link structure similar to a spline regression, to capture the nonlinear relationship between the lifetime characteristics and the stress levels. Product lifetime is assumed to follow Weibull distribution with non-identical scale and shape parameters linked with the stress factor through a piecewise linear function. The elements of the Fisher information matrix are computed in detail to formulate the acceptability criterion for the conforming lots. The decision variables of the sampling plan including sample size, stress factors, and others are determined using a constrained aggregated cost minimization approach and variance minimization approach. A simulated case study demonstrates that the nonlinear link-based piecewise linear approximation model outperforms the linear link-based model.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [Double Variable Importance Matching to Estimate Distinct Causal Effects on Event Probability and Timing](https://arxiv.org/abs/2602.05022)
*Yuqi Li,Quinn Lanners,Matthew M. Engelhard*

Main category: stat.ME

TL;DR: 提出基于匹配的框架，在存在治愈亚群的时间-事件数据中分别估计治疗对治愈概率和事件时间的异质性效应


<details>
  <summary>Details</summary>
Motivation: 临床时间-事件数据分析面临多重挑战：混杂、删失、异质性，以及存在治愈亚群（事件永不发生）。传统生存分析和因果推断方法无法区分治愈与非治愈个体，掩盖了治疗对治愈概率和事件时间的不同作用机制。

Method: 提出匹配框架：1) 使用混合治愈模型识别特征重要性；2) 基于特征重要性构建加权距离度量进行高维匹配；3) 在匹配组内使用Kaplan-Meier估计器估计治愈概率和预期事件时间；4) 推导个体水平治疗效果。

Result: 提供了估计器一致性和距离度量最优性的理论保证，并将估计误差分解为删失、模型拟合和不可约噪声的贡献。仿真和真实数据分析表明，该方法能在时间-事件设置中提供可解释且稳健的异质性治疗效果估计。

Conclusion: 该方法能够有效分离治疗对治愈概率和事件时间的不同影响机制，为存在治愈亚群的时间-事件数据提供了更精细的因果推断框架。

Abstract: In many clinical contexts, estimating effects of treatment in time-to-event data is complicated not only by confounding, censoring, and heterogeneity, but also by the presence of a cured subpopulation in which the event of interest never occurs. In such settings, treatment may have distinct effects on (1) the probability of being cured and (2) the event timing among non-cured individuals. Standard survival analysis and causal inference methods typically do not separate cured from non-cured individuals, obscuring distinct treatment mechanisms on cure probability and event timing. To address these challenges, we propose a matching-based framework that constructs distinct match groups to estimate heterogeneous treatment effects (HTE) on cure probability and event timing, respectively. We use mixture cure models to identify feature importance for both estimands, which in turn informs weighted distance metrics for matching in high-dimensional spaces. Within matched groups, Kaplan-Meier estimators provide estimates of cure probability and expected time to event, from which individual-level treatment effects are derived. We provide theoretical guarantees for estimator consistency and distance metric optimality under an equal-scale constraint. We further decompose estimation error into contributions from censoring, model fitting, and irreducible noise. Simulations and real-world data analyses demonstrate that our approach delivers interpretable and robust HTE estimates in time-to-event settings.

</details>


### [4] [A Flexible Modeling of Extremes in the Presence of Inliers](https://arxiv.org/abs/2602.05351)
*Shivshankar Nila,Ishapathik Das,N. Balakrishna*

Main category: stat.ME

TL;DR: 提出一个能同时处理极值、零值内点和尾部比例的灵活模型，通过最大似然估计参数，在模拟和实际数据中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 许多随机现象（如寿命测试和环境数据）存在正值和过多的零值，这给建模带来挑战。零值内点（如立即失效）难以用标准方法建模，且会影响极值分析的准确性、参数估计偏差，甚至导致严重事件。现有极值混合模型在处理内点方面不足，缺乏统一框架来定义极值混合模型特别是尾部比例。

Method: 提出一个灵活的模型，能同时处理极值、零值内点和尾部比例。使用最大似然估计进行参数估计，并与经典方法（平均超额图、参数稳定性图、Pickands图）进行比较。

Result: 建立了理论结果，在模拟研究和实际数据分析中，所提出的模型优于传统方法。

Conclusion: 该模型提供了一个统一框架来处理包含极值、内点和尾部比例的复杂数据，在统计性能和实际应用方面都表现出优越性。

Abstract: Many random phenomena, including life-testing and environmental data, show positive values and excess zeros, which pose modeling challenges. In life testing, immediate failures result in zero lifetimes, often due to defects or poor quality, especially in electronics and clinical trials. These failures, called inliers at zero, are difficult to model using standard approaches. The presence and proportion of inliers may influence the accuracy of extreme value analysis, bias parameter estimates, or even lead to severe events or extreme effects, such as drought or crop failure. In such scenarios, a key issue in extreme value analysis is determining a suitable threshold to capture tail behaviour accurately. Although some extreme value mixture models address threshold and tail estimation, they often inadequately handle inliers, resulting in suboptimal results. Bulk model misspecification can affect the threshold, extreme value estimates, and, in particular, the tail proportion. There is no unified framework for defining extreme value mixture models, especially the tail proportion. This paper proposes a flexible model that handles extremes, inliers, and the tail proportion. Parameters are estimated using maximum likelihood estimation. Compared the proposed model estimates with the classical mean excess plot, parameter stability plot, and Pickands plot estimates. Theoretical results are established, and the proposed model outperforms traditional methods in both simulation studies and real data analysis.

</details>


### [5] [Billions-Scale Forecast Reconciliation](https://arxiv.org/abs/2602.05030)
*Tianyu,Wang,Matthew C. Johnson,Steven Klee,Matthew L. Malloy*

Main category: stat.ME

TL;DR: 本文研究大规模分层预测协调问题，实现了可处理超过40亿预测值的优化算法，并建立了最小二乘预测协调与基于份额的预测协调之间的理论等价关系。


<details>
  <summary>Details</summary>
Motivation: 解决大型零售商面临的大规模分层预测协调问题，将多个相关量的预测结果组合起来，同时满足预期的等式和可加性约束。

Method: 实现并基准测试了多种优化算法来解决预测协调问题，将其表述为一个简单的优化问题，特别关注大规模场景下的计算效率。

Result: 成功解决了维度超过40亿预测值的最大规模预测协调问题，可能是迄今为止解决的最大约束最小二乘问题之一。证明了在特定条件下，最小二乘预测协调等价于基于份额的预测协调。

Conclusion: 优化方法可以看作是基于份额的预测协调的推广，适用于多个重叠的数据层次结构，为大规模分层预测协调提供了有效的解决方案。

Abstract: The problem of combining multiple forecasts of related quantities that obey expected equality and additivity constraints, often referred to a hierarchical forecast reconciliation, is naturally stated as a simple optimization problem. In this paper we explore optimization-based point forecast reconciliation at scales faced by large retailers. We implement and benchmark several algorithms to solve the forecast reconciliation problem, showing efficacy when the dimension of the problem exceeds four billion forecasted values. To the best of our knowledge, this is the largest forecast reconciliation problem, and perhaps on-par with the largest constrained least-squares-problem ever solved. We also make several theoretical contributions. We show that for a restricted class of problems and when the loss function is weighted appropriately, least-squares forecast reconciliation is equivalent to share-based forecast reconciliation. This formalizes how the optimization based approach can be thought of as a generalization of share-based reconciliation, applicable to multiple, overlapping data hierarchies.

</details>


### [6] [MixMashNet: An R Package for Single and Multilayer Networks](https://arxiv.org/abs/2602.05716)
*Maria De Martino,Federico Triolo,Adrien Perigord,Alice Margherita Ornago,Davide Liborio Vetrano,Caterina Gregorio*

Main category: stat.ME

TL;DR: MixMashNet是一个R包，提供使用混合图模型估计和分析单层及多层网络的集成框架，支持连续、计数和分类变量，包含置信区间计算、社区稳定性评估和交互式可视化工具。


<details>
  <summary>Details</summary>
Motivation: 现有网络分析工具在处理混合类型变量和多层网络结构方面存在局限，需要开发一个能够处理不同类型变量、支持多层拓扑结构、并提供统计推断和可视化支持的集成框架。

Method: 基于混合图模型（MGMs）方法，支持连续、计数和分类变量的网络建模。在多层设置中，允许各层包含不同类型和数量的变量，并可预定义多层拓扑结构。采用bootstrap方法计算边权重和节点中心性指标的置信区间，提供节点社区成员稳定性评估和社区得分计算工具。

Result: 开发了MixMashNet R包，实现了对混合类型变量的单层和多层网络估计与分析，提供了统计推断（置信区间）、社区稳定性评估、网络聚类分析等功能，并包含交互式Shiny应用支持网络探索和可视化。

Conclusion: MixMashNet为处理复杂数据类型和多层网络结构的研究者提供了一个全面、灵活的工具包，通过集成统计建模、推断分析和交互可视化，支持更可靠的网络科学分析。

Abstract: The R package MixMashNet provides an integrated framework for estimating and analyzing single and multilayer networks using Mixed Graphical Models (MGMs), accommodating continuous, count, and categorical variables. In the multilayer setting, layers may comprise different types and numbers of variables, and users can explicitly impose a predefined multilayer topology. Bootstrap procedures are implemented to derive confidence intervals for edge weights and node-level centrality indices. In addition, the package includes tools to assess the stability of node community membership and to compute community scores that summarize the latent dimensions identified through network clustering. MixMashNet also offers interactive Shiny applications to support exploration, visualization, and interpretation of the estimated networks.

</details>


### [7] [A Weighting Framework for Clusters as Confounders in Observational Studies](https://arxiv.org/abs/2602.05041)
*Eli Ben-Michael,Avi Feller,Luke Keele*

Main category: stat.ME

TL;DR: 本文提出了一个统一的加权框架，用于处理聚类观测数据中的混杂控制，区分全局平衡和局部平衡，并比较了不同方法的假设和适用性。


<details>
  <summary>Details</summary>
Motivation: 在观测研究中，当单位（如学生、患者）被聚类在群体（如学校、医院）中时，研究者通常通过调整聚类层面的协变量或聚类成员身份来处理混杂。现有方法（如带随机效应倾向得分模型的IPW）只关注全局平衡，而忽略了局部平衡的重要性。

Method: 1) 提出了统一的加权框架，明确区分全局平衡（跨聚类的处理组与对照组差异）和局部平衡（聚类内的差异）；2) 开发了层次平衡加权法，通过约束优化直接控制全局和局部平衡；3) 基于广义Mundlak方法，提出了新颖的Mundlak平衡加权估计器，调整聚类层面的充分统计量而非聚类指标。

Result: 研究表明：1) 标准的随机效应倾向得分IPW仅针对全局平衡和跨聚类的常数水平偏移，对局部平衡没有约束；2) 层次平衡加权法只需要给定协变量和聚类成员身份时处理可忽略的假设；3) Mundlak方法需要额外的指数族结构假设，但能处理所有单位都被处理或未处理的小聚类。

Conclusion: 本文为聚类观测数据中的混杂控制提供了理论框架和实用方法，强调了同时考虑全局和局部平衡的重要性，并通过模拟研究和教育、健康服务领域的应用展示了不同方法在不同聚类结构下的表现。

Abstract: When units in observational studies are clustered in groups, such as students in schools or patients in hospitals, researchers often address confounding by adjusting for cluster-level covariates or cluster membership. In this paper, we develop a unified weighting framework that clarifies how different estimation methods control two distinct sources of imbalance: global balance (differences between treated and control units across clusters) and local balance (differences within clusters). We show that inverse propensity score weighting (IPW) with a random effects propensity score model -- the current standard in the literature -- targets only global balance and constant level shifts across clusters, but imposes no constraints on local balance. We then present two approaches that target both forms of balance. First, hierarchical balancing weights directly control global and local balance through a constrained optimization problem. Second, building on the recently proposed Generalized Mundlak approach, we develop a novel Mundlak balancing weights estimator that adjusts for cluster-level sufficient statistics rather than cluster indicators; this approach can accommodate small clusters where all units are treated or untreated. Critically, these approaches rest on different assumptions: hierarchical balancing weights require only that treatment is ignorable given covariates and cluster membership, while Mundlak methods additionally require an exponential family structure. We then compare these methods in a simulation study and in two applications in education and health services research that exhibit very different cluster structures.

</details>


### [8] [Impact Range Assessment (IRA): An Interpretable Sensitivity Measure for Regression Modelling](https://arxiv.org/abs/2602.05239)
*Jihao You,Dan Tulpan,Jiaojiao Diao,Jennifer L. Ellis*

Main category: stat.ME

TL;DR: 提出Impact Range Assessment (IRA)方法，通过量化预测变量在其取值范围内对响应变量的最大潜在影响，提供回归模型的可解释性分析工具。


<details>
  <summary>Details</summary>
Motivation: 回归模型虽然能捕捉预测变量与响应变量之间的关系，但缺乏直观的方法来理解预测变量对结果的影响程度。需要一种简单直观的方法来量化和解释预测变量的影响力。

Method: Impact Range Assessment (IRA)方法，通过测量每个预测变量在其取值范围内对响应变量的总潜在变化，量化预测变量的最大影响力。

Result: 在合成线性与非线性数据集上的验证显示，相关预测变量比无关预测变量产生更高的IRA值；重复评估结果与单次分析结果高度一致，证明方法的稳健性；在预测颗粒质量的案例研究中，IRA提供了简单直观的方法来解释和排序预测变量的影响力。

Conclusion: IRA方法为回归模型提供了一种简单直观的可解释性工具，能够有效量化和排序预测变量的影响力，从而提高模型的透明度和可靠性。

Abstract: While regression models capture the relationship between predictors and the response variable, they often lack intuitive accompanying methods to understand the influence of predictors on the outcome. To address this, we introduce an interpretability method called Impact Range Assessment (IRA), which quantifies the maximal influence of each predictor by measuring the total potential change in the response variable, across the predictor range. Validation using synthetic linear and nonlinear datasets demonstrates that relevant predictors produced higher IRA values than irrelevant ones. Moreover, repeated evaluations produced results closely aligned with those from the single-execution analysis, confirming the robustness of the method. A case study using a model that predicts pellet quality demonstrated that the IRA provides a simple and intuitive approach to interpret and rank predictor influence, thereby improving model transparency and reliability.

</details>


### [9] [A Bayesian approach to differential prevalence analysis with applications in microbiome studies](https://arxiv.org/abs/2602.05938)
*Juho Pelto,Kari Auranen,Janne V. Kujala,Leo Lahti*

Main category: stat.ME

TL;DR: DiPPER是一种基于贝叶斯层次建模的微生物组差异流行度分析方法，相比现有方法具有更高的敏感性和错误控制能力，能提供多重检验校正的估计和不确定性区间。


<details>
  <summary>Details</summary>
Motivation: 微生物组研究中，分析分类特征的存/缺（流行度）可作为差异丰度分析的替代方法，但标准方法在处理边界情况和多重检验时面临挑战。

Method: 开发了DiPPER（Differential Prevalence via Probabilistic Estimation in R），一种基于贝叶斯层次建模的方法，用于估计差异流行度并提供不确定性区间。

Result: 在67个公开人类肠道微生物组研究中，DiPPER在性能上优于现有差异流行度和丰度方法，结合了高敏感性和有效的错误控制，并在独立研究间表现出更好的结果可重复性。

Conclusion: DiPPER为微生物组差异流行度分析提供了一个强大的工具，能够解决标准方法的局限性，并提供多重检验校正的统计推断。

Abstract: Recent evidence suggests that analyzing the presence/absence of taxonomic features can offer a compelling alternative to differential abundance analysis in microbiome studies. However, standard approaches face challenges with boundary cases and multiple testing. To address these challenges, we developed DiPPER (Differential Prevalence via Probabilistic Estimation in R), a method based on Bayesian hierarchical modeling. We benchmarked our method against existing differential prevalence and abundance methods using data from 67 publicly available human gut microbiome studies. We observed considerable variation in performance across methods, with DiPPER outperforming alternatives by combining high sensitivity with effective error control. DiPPER also demonstrated superior replication of findings across independent studies. Furthermore, DiPPER provides differential prevalence estimates and uncertainty intervals that are inherently adjusted for multiple testing.

</details>


### [10] [The stochastic view used in climate sciences: (some) perspectives from (some of) mathematical statistics](https://arxiv.org/abs/2602.05611)
*Nils Lid Hjort*

Main category: stat.ME

TL;DR: 本章从统计学方法角度探讨与气候统计相关的几个主题，包括时间序列建模、预测方法、海洋生物学影响、模型监测、信息源整合及极端事件分析。


<details>
  <summary>Details</summary>
Motivation: 本章旨在指出与气候统计相关且具有共同兴趣的几个统计方法主题，为气候统计领域提供方法论视角的补充。

Method: 从统计学方法视角提出六个主题：(1)气象时间序列的建模与模型选择策略；(2)时间序列预测及趋势跨越阈值的评估方法；(3)气候对海洋生物学的影响；(4)模型参数随时间稳定性的监测过程；(5)不同信息源输出的整合；(6)极端事件概率及其不确定性的分析。

Result: 本章未呈现具体实证结果，而是提出了一个统计方法论框架，为气候统计研究提供了六个关键的方法论方向和研究主题。

Conclusion: 本章强调了统计方法在气候研究中的重要性，提出了多个方法论主题，旨在促进气候统计领域的方法论发展和跨学科合作。

Abstract: Climate statistics is of course a very broad field, along with the many connections and impacts for yet other areas, with a history as long as mankind has been recording temperatures, describing drastic weather events, etc. The important work of Klaus Hasselmann, with crucial contributions to the field, along with various other connected strands of work,is being reviewed and discussed in other chapters. The aim of the present chapter is to point to a few statistical methodology themes of relevance for and joint interest with climate statistics. These themes, presented from a statistical methods perspective, include (i) more careful modelling and model selection strategies for meteorological type time series; (ii) methods for prediction, not only for future values of a time series, but for assessing when a trend might be crossing a barrier, along with relevant measures of uncertainty for these; (iii) climatic influence on marine biology; (iv) monitoring processes to assess whether and then to what extent models and their parameters have stayed reasonably constant over time; (v) combination of outputs from different information sources; and (vi) analysing probabilities and their uncertainties related to extreme events.

</details>


### [11] [Correcting Measurement Error and Zero Inflation in Functional Covariates for Scalar-on-Function Quantile Regression](https://arxiv.org/abs/2602.05784)
*Caihong Qin,Lan Xue,Ufuk Beyaztas,Roger S. Zoh,Mark Benden,Jeff Goldsmith,Carmen D. Tekwe*

Main category: stat.ME

TL;DR: 提出一个针对零膨胀且有测量误差的功能性数据的新建模框架，通过主体特异性时变有效性指标区分结构性零值和内在值，结合基展开和线性混合模型校正测量误差，并通过联合分位数回归评估恢复的潜在协变量效应。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备收集的时变生物行为数据常包含测量误差和过量零值（由于未佩戴、久坐行为或连接问题），这些零值具有主体特异性分布。现有统计方法无法同时处理这些问题，需要新框架来准确分析这些复杂数据。

Method: 引入主体特异性时变有效性指标来明确区分结构性零值和内在值；通过最大似然迭代估计潜在功能协变量和零膨胀概率；使用基展开和线性混合模型调整测量误差；应用跨多个分位数水平的联合分位数回归来评估恢复的潜在协变量效应。

Result: 通过大量模拟实验证明，该方法相比仅处理测量误差的方法显著提高了估计精度；联合估计相比单独拟合分位数回归有实质性改进；在儿童肥胖研究中，该方法有效校正了步数数据的零膨胀和测量误差，结果与能量消耗密切吻合，支持步数作为体力活动的代理指标。

Conclusion: 该框架为处理零膨胀且有测量误差的功能性数据提供了有效的统计方法，能够更准确地从可穿戴设备数据中提取信息，在健康研究中具有重要应用价值，特别是在分析体力活动等生物行为数据时。

Abstract: Wearable devices collect time-varying biobehavioral data, offering opportunities to investigate how behaviors influence health outcomes. However, these data often contain measurement error and excess zeros (due to nonwear, sedentary behavior, or connectivity issues), each characterized by subject-specific distributions. Current statistical methods fail to address these issues simultaneously. We introduce a novel modeling framework for zero-inflated and error-prone functional data by incorporating a subject-specific time-varying validity indicator that explicitly distinguishes structural zeros from intrinsic values. We iteratively estimate the latent functional covariates and zero-inflation probabilities via maximum likelihood, using basis expansions and linear mixed models to adjust for measurement error. To assess the effects of the recovered latent covariates, we apply joint quantile regression across multiple quantile levels. Through extensive simulations, we demonstrate that our approach significantly improves estimation accuracy over methods that only address measurement error, and joint estimation yields substantial improvements compared with fitting separate quantile regressions. Applied to a childhood obesity study, our approach effectively corrects for zero inflation and measurement error in step counts, yielding results that closely align with energy expenditure and supporting their use as a proxy for physical activity.

</details>


### [12] [Learning False Discovery Rate Control via Model-Based Neural Networks](https://arxiv.org/abs/2602.05798)
*Arnau Vilella,Jasin Machkour,Michael Muma,Daniel P. Palomar*

Main category: stat.ME

TL;DR: 提出一种基于学习增强的T-Rex Selector框架，用神经网络替代传统FDP估计器，在控制错误发现率的同时显著提高统计功效


<details>
  <summary>Details</summary>
Motivation: 现有高维变量选择方法在控制错误发现率(FDR)时过于保守，导致实际错误发现比例(FDP)与目标FDR水平之间存在较大差距，限制了统计功效

Method: 在T-Rex Selector框架中，用神经网络替代传统的分析性FDP估计器。该神经网络仅使用多样化的合成数据集进行训练，能够提供更紧致、更准确的FDP近似

Result: 通过大量模拟和具有挑战性的合成全基因组关联研究(GWAS)验证，该方法相比现有方法能更有效地检测真实变量，同时保持对FDR的有效近似控制

Conclusion: 学习增强的T-Rex Selector框架能够显著缩小实际FDP与目标FDR之间的差距，在维持错误控制的同时提高发现能力，为高维变量选择提供了更优的解决方案

Abstract: Controlling the false discovery rate (FDR) in high-dimensional variable selection requires balancing rigorous error control with statistical power. Existing methods with provable guarantees are often overly conservative, creating a persistent gap between the realized false discovery proportion (FDP) and the target FDR level. We introduce a learning-augmented enhancement of the T-Rex Selector framework that narrows this gap. Our approach replaces the analytical FDP estimator with a neural network trained solely on diverse synthetic datasets, enabling a substantially tighter and more accurate approximation of the FDP. This refinement allows the procedure to operate much closer to the desired FDR level, thereby increasing discovery power while maintaining effective approximate control. Through extensive simulations and a challenging synthetic genome-wide association study (GWAS), we demonstrate that our method achieves superior detection of true variables compared to existing approaches.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [13] [Variance Reduction Based Experience Replay for Policy Optimization](https://arxiv.org/abs/2602.05379)
*Hua Zheng,Wei Xie,M. Ben Feng,Keilung Choy*

Main category: stat.ML

TL;DR: 提出VRER框架，通过选择性重用信息样本减少策略梯度估计方差，开发PG-VRER算法并建立理论收敛保证


<details>
  <summary>Details</summary>
Motivation: 传统经验回放对所有历史观测一视同仁，未能考虑不同样本对学习的贡献差异，且缺乏严格的理论分析

Method: 提出方差减少经验回放（VRER）框架，选择性重用信息样本；开发PG-VRER算法；建立捕获马尔可夫动态和行为策略交互依赖的理论分析框架

Result: 建立PG-VRER的有限时间收敛保证，揭示偏差-方差权衡：重用旧经验增加偏差但减少梯度方差；实验显示VRER加速策略学习并超越SOTA算法

Conclusion: VRER为经验回放提供理论框架，通过选择性重用样本实现偏差-方差权衡，显著提升策略优化效率和性能

Abstract: Effective reinforcement learning (RL) for complex stochastic systems requires leveraging historical data collected in previous iterations to accelerate policy optimization. Classical experience replay treats all past observations uniformly and fails to account for their varying contributions to learning. To overcome this limitation, we propose Variance Reduction Experience Replay (VRER), a principled framework that selectively reuses informative samples to reduce variance in policy gradient estimation. VRER is algorithm-agnostic and integrates seamlessly with existing policy optimization methods, forming the basis of our sample-efficient off-policy algorithm, Policy Gradient with VRER (PG-VRER). Motivated by the lack of rigorous theoretical analysis of experience replay, we develop a novel framework that explicitly captures dependencies introduced by Markovian dynamics and behavior-policy interactions. Using this framework, we establish finite-time convergence guarantees for PG-VRER and reveal a fundamental bias-variance trade-off: reusing older experience increases bias but simultaneously reduces gradient variance. Extensive empirical experiments demonstrate that VRER consistently accelerates policy learning and improves performance over state-of-the-art policy optimization algorithms.

</details>


### [14] [Optimal scaling laws in learning hierarchical multi-index models](https://arxiv.org/abs/2602.05846)
*Leonardo Defilippis,Florent Krzakala,Bruno Loureiro,Antoine Maillard*

Main category: stat.ML

TL;DR: 该论文为两层神经网络在分层多索引目标上的缩放定律提供了精确的信息论分析，揭示了特征学习的级联相变过程，并证明简单谱估计器能达到最优速率。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在表示受限机制下学习分层多索引目标的缩放定律，解释训练中出现的缩放规律、平台现象和谱结构等观察到的现象。

Method: 采用信息论方法推导子空间恢复和预测误差的精确缩放定律，提出目标无关的简单谱估计器（可解释为梯度下降在小学习率下的极限），并设计高效的读出学习过程。

Result: 获得了子空间恢复和预测误差的精确信息论缩放定律，揭示了目标分层特征通过级联相变顺序学习的过程，证明谱估计器能达到最优速率，一旦获得适应表示，读出可统计最优地学习。

Conclusion: 为浅层神经网络在分层目标上的缩放定律、平台现象和谱结构提供了统一且严格的理论解释，建立了表示学习与统计最优性之间的联系。

Abstract: In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. We derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. We further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. Once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. As a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets.

</details>


### [15] [Distribution-free two-sample testing with blurred total variation distance](https://arxiv.org/abs/2602.05862)
*Rohan Hore,Rina Foygel Barber*

Main category: stat.ML

TL;DR: 该论文研究两样本测试问题，提出模糊总变差距离作为传统总变差距离的松弛，能够在无分布假设下进行推断，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 在两样本测试中，当无法对两个分布的性质做出假设时，验证分布相等性或提供总变差距离的紧上界在无分布条件下是不可能的。需要一种能够在无分布假设下进行推断的方法。

Method: 提出模糊总变差距离作为传统总变差距离的松弛，研究其理论性质，提供无分布条件下模糊总变差距离的上界和下界理论保证，并分析其在高维空间中的特性。

Result: 建立了模糊总变差距离的理论框架，提供了无分布条件下的上下界理论保证，并分析了该距离在高维空间中的性质。

Conclusion: 模糊总变差距离是传统总变差距离的有效松弛，能够在无分布假设条件下进行两样本测试推断，为解决分布自由环境下的统计推断问题提供了新途径。

Abstract: Two-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. In particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (TV) distance between the distributions, is impossible to achieve in a distribution-free regime. In this work, we examine the blurred TV distance, a relaxation of TV distance that enables us to perform inference without assumptions on the distributions. We provide theoretical guarantees for distribution-free upper and lower bounds on the blurred TV distance, and examine its properties in high dimensions.

</details>


### [16] [Transformers Are Born Biased: Structural Inductive Biases at Random Initialization and Their Practical Consequences](https://arxiv.org/abs/2602.05927)
*Siquan Li,Yao Tong,Haonan Wang,Tianyang Hu*

Main category: stat.ML

TL;DR: 随机初始化的Transformer模型已表现出强烈的结构化偏好，某些token的预测概率远高于其他token，这种偏好源于MLP激活不对称和自注意力机制的共同作用，形成了稳定的模型身份特征。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点——认为随机初始化的Transformer是行为无结构的，所有有意义的偏好都通过大规模训练才出现。研究发现随机初始化的Transformer已具有强烈的结构化偏见。

Method: 通过分析Transformer架构在初始化时的机制：1）MLP子层中的不对称非线性激活导致全局表示集中；2）自注意力通过局部聚合放大这一效应；3）提出SeedPrint指纹识别方法区分仅随机初始化不同的模型。

Result: 发现随机初始化模型已存在极端token偏好，某些token预测概率比其他token高几个数量级；这种初始化诱导的偏见在整个训练过程中持续存在；成功区分仅随机初始化不同的模型；解释了注意力下沉现象的成因。

Conclusion: Transformer在随机初始化时已具有强烈的结构化偏好，这种偏好形成稳定的模型身份，可用于模型指纹识别，并为注意力下沉现象提供了原理性解释和控制途径。

Abstract: Transformers underpin modern large language models (LLMs) and are commonly assumed to be behaviorally unstructured at random initialization, with all meaningful preferences emerging only through large-scale training. We challenge this assumption by showing that randomly initialized transformers already exhibit strong and systematic structural biases. In particular, untrained models display extreme token preferences: across random input sequences, certain tokens are predicted with probabilities orders of magnitude larger.
  We provide a mechanistic explanation for this phenomenon by dissecting the transformer architecture at initialization. We show that extreme token preference arises from a contraction of token representations along a random seed-dependent direction. This contraction is driven by two interacting forces: (i) asymmetric nonlinear activations in MLP sublayers induce global (inter-sequence) representation concentration, and (ii) self-attention further amplifies this effect through local (intra-sequence) aggregation. Together, these mechanisms align hidden representations along a direction determined solely by the random initialization, producing highly non-uniform next-token predictions.
  Beyond mechanistic insight, we demonstrate that these initialization-induced biases persist throughout training, forming a stable and intrinsic model identity. Leveraging this property, we introduce SeedPrint, a fingerprinting method that can reliably distinguish models that differ only in their random initialization, even after extensive training and under substantial distribution shift. Finally, we identify a fundamental positional discrepancy inherent to the attention mechanism's intra-sequence contraction that is causally linked to the attention-sink phenomenon. This discovery provides a principled explanation for the emergence of sinks and offers a pathway for their control.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [17] [Predictive Synthesis under Sporadic Participation: Evidence from Inflation Density Surveys](https://arxiv.org/abs/2602.05226)
*Matthew C. Johnson,Matteo Luciani,Minzhengxiong Zhang,Kenichiro McAlinn*

Main category: stat.AP

TL;DR: 本文提出了一种贝叶斯更新方法，用于处理专业预测调查中参与者不规律参与的问题，避免传统聚合方法因面板组成变化而产生的人为跳跃，提高通胀密度预测的准确性和平滑性。


<details>
  <summary>Details</summary>
Motivation: 央行依赖专业调查的密度预测来评估通胀风险和沟通不确定性，但预测者不规律参与（进入、退出、跳过轮次、长期缺席后重新出现）导致面板组成随时间大幅变化。传统聚合方法（如等权重池化、剔除缺失预测者后重新归一化、临时插补）会因面板组成而非经济信息产生人为跳跃，干扰实时解读并掩盖预测者表现。

Method: 开发了针对偶发性参与的预测组合的连贯贝叶斯更新规则，即使预测者的预测未被观察到，也能为每个预测者维持明确定义的潜在预测状态。不依赖重新归一化或插补，而是通过面板的隐含条件结构更新组合预测分布。

Result: 在欧洲央行专业预测者调查中，该方法相对于等权重基准提高了预测准确性，特别是在高流动率时期，提供了更平滑、校准更好的通胀密度预测。能够将真正的表现差异与机械的参与效应区分开来，并产生可解释的预测者影响力动态。

Conclusion: 提出的贝叶斯方法有效解决了专业预测调查中不规律参与的问题，避免了传统聚合方法的人为跳跃，提高了预测质量，为央行提供了更可靠的不确定性评估工具。

Abstract: Central banks rely on density forecasts from professional surveys to assess inflation risks and communicate uncertainty. A central challenge in using these surveys is irregular participation: forecasters enter and exit, skip rounds, and reappear after long gaps. In the European Central Bank's Survey of Professional Forecasters, turnover and missingness vary substantially over time, causing the set of submitted predictions to change from quarter to quarter. Standard aggregation rules -- such as equal-weight pooling, renormalization after dropping missing forecasters, or ad hoc imputation -- can generate artificial jumps in combined predictions driven by panel composition rather than economic information, complicating real-time interpretation and obscuring forecaster performance. We develop coherent Bayesian updating rules for forecast combination under sporadic participation that maintain a well-defined latent predictive state for each forecaster even when their forecast is unobserved. Rather than relying on renormalization or imputation, the combined predictive distribution is updated through the implied conditional structure of the panel. This approach isolates genuine performance differences from mechanical participation effects and yields interpretable dynamics in forecaster influence. In the ECB survey, it improves predictive accuracy relative to equal-weight benchmarks and delivers smoother and better-calibrated inflation density forecasts, particularly during periods of high turnover.

</details>


### [18] [Active Simulation-Based Inference for Scalable Car-Following Model Calibration](https://arxiv.org/abs/2602.05246)
*Menglin Kong,Chengyuan Zhang,Lijun Sun*

Main category: stat.AP

TL;DR: 提出基于主动仿真推理的可扩展跟驰模型校准框架，结合残差增强仿真器和摊销条件密度估计器，实现单次前向传播即可获得驾驶员特定参数后验分布，在HighD数据集上优于贝叶斯校准基线。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的跟驰模型校准多为确定性方法，无法捕捉驾驶员和情境的显著变异性，而贝叶斯校准虽然能推断参数后验分布，但传统MCMC方法计算成本过高，难以应用于大规模自然驾驶数据集。

Method: 提出主动仿真推理框架：1) 残差增强跟驰仿真器，提供两种残差过程替代方案；2) 摊销条件密度估计器，将观察到的前车-后车轨迹直接映射到驾驶员特定参数后验分布；3) 联合主动设计策略，选择信息丰富的参数提议和代表性驾驶情境，在训练期间降低仿真成本。

Result: 在HighD数据集上的实验显示，相比贝叶斯校准基线，该方法具有更好的预测准确性和更接近的仿真与观测轨迹分布一致性，收敛性和消融研究支持所提设计选择的鲁棒性。

Conclusion: 该框架实现了可扩展、不确定性感知的驾驶员群体建模，为交通流仿真和风险敏感交通分析提供了有效工具。

Abstract: Credible microscopic traffic simulation requires car-following models that capture both the average response and the substantial variability observed across drivers and situations. However, most data-driven calibrations remain deterministic, producing a single best-fit parameter vector and offering limited guidance for uncertainty-aware prediction, risk-sensitive evaluation, and population-level simulation. Bayesian calibration addresses this gap by inferring a posterior distribution over parameters, but per-trajectory sampling methods such as Markov chain Monte Carlo (MCMC) are computationally infeasible for modern large-scale naturalistic driving datasets. This paper proposes an active simulation-based inference framework for scalable car-following model calibration. The approach combines (i) a residual-augmented car-following simulator with two alternatives for the residual process and (ii) an amortized conditional density estimator that maps an observed leader--follower trajectory directly to a driver-specific posterior over model parameters with a single forward pass at test time. To reduce simulation cost during training, we introduce a joint active design strategy that selects informative parameter proposals together with representative driving contexts, focusing simulations where the current inference model is most uncertain while maintaining realism. Experiments on the HighD dataset show improved predictive accuracy and closer agreement between simulated and observed trajectory distributions relative to Bayesian calibration baselines, with convergence and ablation studies supporting the robustness of the proposed design choices. The framework enables scalable, uncertainty-aware driver population modeling for traffic flow simulation and risk-sensitive transportation analysis.

</details>
