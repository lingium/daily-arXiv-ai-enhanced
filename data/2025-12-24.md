<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 8]
- [stat.CO](#stat.CO) [Total: 3]
- [stat.ME](#stat.ME) [Total: 30]
- [stat.ML](#stat.ML) [Total: 5]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Empirical parameterization of the Elo Rating System](https://arxiv.org/abs/2512.18013)
*Shirsa Maitra,Tathagata Banerjee,Anushka De,Diganta Mukherjee,Tridib Mukherjee*

Main category: stat.AP

TL;DR: 提出基于数据驱动的Elo等评分系统参数调优方法，通过最大化比赛结果预测准确率来学习最优参数


<details>
  <summary>Details</summary>
Motivation: 现有评分系统（如Elo、Glicko、TrueSkill）的参数通常基于概率假设或惯例选择，未充分利用游戏特定数据，导致预测准确性受限

Method: 提出通用参数调优框架，通过最大化比赛结果预测准确率来学习最优参数值，可扩展到任何评分系统（包括多人游戏）

Result: 在真实和模拟游戏数据上实现评分系统，证明数据驱动方法在建模玩家表现方面的适用性

Conclusion: 数据驱动的参数调优方法能够提高评分系统的预测准确性，为各种游戏场景提供更精确的玩家表现建模

Abstract: This study aims to provide a data-driven approach for empirically tuning and validating rating systems, focusing on the Elo system. Well-known rating frameworks, such as Elo, Glicko, TrueSkill systems, rely on parameters that are usually chosen based on probabilistic assumptions or conventions, and do not utilize game-specific data. To address this issue, we propose a methodology that learns optimal parameter values by maximizing the predictive accuracy of match outcomes. The proposed parameter-tuning framework is a generalizable method that can be extended to any rating system, even for multiplayer setups, through suitable modification of the parameter space. Implementation of the rating system on real and simulated gameplay data demonstrates the suitability of the data-driven rating system in modeling player performance.

</details>


### [2] [Distribution-Free Selection of Low-Risk Oncology Patients for Survival Beyond a Time Horizon](https://arxiv.org/abs/2512.18118)
*Matteo Sesia,Vladimir Svetnik*

Main category: stat.AP

TL;DR: 该研究比较了两种基于黑盒生存模型输出校准筛选规则的方法，用于识别在规定时间范围内不太可能发生事件的病人子集，在医学应用中具有重要价值。


<details>
  <summary>Details</summary>
Motivation: 在医学领域，需要识别在规定时间范围内不太可能发生事件的病人子集，用于治疗降级候选人的选择和有限医疗资源的优先分配。现有方法需要提供分布无关的统计保证，但不同框架之间存在概念差异。

Method: 比较两种方法：1）高概率风险控制方法；2）基于期望的假发现率控制方法（使用conformal p-values）。两种方法都通过逆删失概率加权来适应时间到事件数据的分析。

Result: 在Flatiron Health Research Database的半合成和真实肿瘤学数据实验中，两种方法通常都能在选定患者中达到期望的生存率，但具有不同的效率特征：conformal方法往往更强大，而高概率风险控制方法以一定的保守性为代价提供更强的保证。

Conclusion: 两种方法各有优势：conformal方法更强大，高概率风险控制提供更强保证但更保守。研究澄清了两种框架的关系，并提供了实施和参数调整的实用指导。

Abstract: We study the problem of selecting a subset of patients who are unlikely to experience an event within a specified time horizon, by calibrating a screening rule based on the output of a black-box survival model. This statistics problem has many applications in medicine, including identifying candidates for treatment de-escalation and prioritizing the allocation of limited medical resources. In this paper, we compare two families of methods that can provide different types of distribution-free guarantees for this task: (i) high-probability risk control and (ii) expectation-based false discovery rate control using conformal $p$-values. We clarify the relation between these two frameworks, which have important conceptual differences, and explain how each can be adapted to analyze time-to-event data using inverse probability of censoring weighting. Through experiments on semi-synthetic and real oncology data from the Flatiron Health Research Database, we find that both approaches often achieve the desired survival rate among selected patients, but with distinct efficiency profiles. The conformal method tends to be more powerful, whereas high-probability risk control offers stronger guarantees at the cost of some additional conservativeness. Finally, we provide practical guidance on implementation and parameter tuning.

</details>


### [3] [Analysing Skill Predominance in Generalized Fantasy Cricket](https://arxiv.org/abs/2512.18467)
*Supratim Das,Sarthak Sarkar,Subhamoy Maitra,Tridib Mukherjee*

Main category: stat.AP

TL;DR: 该研究通过模拟实验和真实IPL数据，在幻想板球比赛中证明了策略性团队选择相比随机选择具有明显技能优势，为玩家和平台设计提供了实用见解。


<details>
  <summary>Details</summary>
Motivation: 随着幻想板球在印度的流行，需要明确区分成功是源于技能还是运气，这既是分析问题也是监管问题。研究旨在评估在这种有限选择框架下是否能检测到可测量的技能。

Method: 引入新的有限选择竞赛框架，参与者从四个专家设计的团队中选择，基于最高累计分数分享奖金。结合模拟实验和2024年印度超级联赛（IPL）的真实表现数据来评估技能表现。

Result: 策略性和信息充分的团队选择始终优于随机选择，显示出明显的技能优势，尽管存在随机变异性。研究量化了团队组成、团队间相关性和参与者行为如何共同影响获胜概率。

Conclusion: 研究结果为寻求通过策略最大化回报的玩家提供了可操作的见解，也为平台设计者开发公平、透明、引人入胜的技能型游戏生态系统提供了指导，平衡了竞争与监管合规性。

Abstract: In fantasy sports, strategic thinking-not mere luck-often defines who wins and who falls short. As fantasy cricket grows in popularity across India, understanding whether success stems from skill or chance has become both an analytical and regulatory question. This study introduces a new limited-selection contest framework in which participants choose from four expert-designed teams and share prizes based on the highest cumulative score. By combining simulation experiments with real performance data from the 2024 Indian Premier League (IPL), we evaluate whether measurable skill emerges within this structure. Results reveal that strategic and informed team selection consistently outperforms random choice, underscoring a clear skill advantage that persists despite stochastic variability. The analysis quantifies how team composition, inter-team correlation, and participant behaviour jointly influence winning probabilities, highlighting configurations where skill becomes statistically dominant. These findings provide actionable insights for players seeking to maximise returns through strategy and for platform designers aiming to develop fair, transparent, and engaging skill-based gaming ecosystems that balance competition with regulatory compliance.

</details>


### [4] [Functional Modeling of Learning and Memory Dynamics in Cognitive Disorders](https://arxiv.org/abs/2512.18760)
*Maria Laura Battagliola,Laura J. Benoit,Sarah Canetta,Shizhe Zhang,R. Todd Ogden*

Main category: stat.AP

TL;DR: 该研究通过功能数据分析方法，从动物工作记忆任务的重复二元成功/失败数据中估计连续成功概率曲线，并通过曲线配准分离振幅（整体表现）和相位（学习/反应速度），以分析认知障碍如何影响学习记忆的速度和表现。


<details>
  <summary>Details</summary>
Motivation: 工作记忆缺陷是许多认知障碍的标志，但传统方法难以区分认知障碍是影响学习记忆的表现（峰值表现）还是速度（学习/反应速度）。研究旨在开发方法分离这两个维度，以更精确地理解认知障碍的机制。

Method: 从动物工作记忆任务的重复二元成功/失败数据中估计连续成功概率曲线，采用功能数据分析框架，通过曲线配准技术将每个函数分解为振幅（整体表现）和相位（学习/反应速度），然后联合分析这两个成分。

Result: 该方法能够成功分离工作记忆任务中的表现和速度维度，允许研究者分析这两个成分如何共同变化，并分别比较不同组别，以确定组间差异是源于峰值表现缺陷还是速度变化。

Conclusion: 该研究提供了一种新的分析方法，能够区分认知障碍对学习记忆表现和速度的不同影响，为理解认知障碍的机制提供了更精细的工具，有助于更准确地诊断和干预认知障碍。

Abstract: Deficits in working memory, which includes both the ability to learn and to retain information short-term, are a hallmark of many cognitive disorders. Our study analyzes data from a neuroscience experiment on animal subjects, where performance on a working memory task was recorded as repeated binary success or failure data. We estimate continuous probability of success curves from this binary data in the context of functional data analysis, which is largely used in biological processes that are intrinsically continuous. We then register these curves to decompose each function into its amplitude, representing overall performance, and its phase, representing the speed of learning or response. Because we are able to separate speed from performance, we can address the crucial question of whether a cognitive disorder impacts not only how well subjects can learn and remember, but also how fast. This allows us to analyze the components jointly to uncover how speed and performance co-vary, and to compare them separately to pinpoint whether group differences stem from a deficit in peak performance or a change in speed.

</details>


### [5] [Dyadic Flow Models for Nonstationary Gene Flow in Landscape Genomics](https://arxiv.org/abs/2512.19035)
*Michael R. Schwob,Nicholas M. Calzada,Justin J. Van Ee,Diana Gamba,Rebecca A. Nelson,Megan L. Vahsen,Peter B. Adler,Jesse R. Lasky,Mevin B. Hooten*

Main category: stat.AP

TL;DR: 提出了一种新的景观基因组学框架，通过引入空间变化的系数来捕捉入侵物种基因流动的非平稳性和不对称性，特别适用于范围变化和入侵物种。


<details>
  <summary>Details</summary>
Motivation: 传统景观基因组学模型主要基于距离隔离和阻力隔离假设，但这些模型无法捕捉入侵物种特有的基因流动机制，如奠基者效应和多次引入。需要开发能够处理非平稳、不对称基因流动的模型。

Method: 扩展了二元模型，引入了空间变化的二元系数（DSVCs），允许景观对基因流动的影响在空间上变化。同时纳入明确的景观特征作为连接性协变量，这些协变量在空间域特定区域局部化，可作为基因流动的屏障或走廊。

Result: 通过对高度入侵的cheatgrass（Bromus tectorum）的案例研究，证明了在范围变化物种中考虑非平稳基因流动的必要性。新框架能够捕捉入侵物种特有的基因流动模式。

Conclusion: 提出的DSVC框架能够更好地模拟入侵物种和范围变化物种的基因流动，考虑了非平稳性和不对称性，为景观基因组学提供了更灵活的分析工具，特别适用于生态入侵和物种扩散研究。

Abstract: The field of landscape genomics aims to infer how landscape features affect gene flow across space. Most landscape genomic frameworks assume the isolation-by-distance and isolation-by-resistance hypotheses, which propose that genetic dissimilarity increases as a function of distance and as a function of cumulative landscape resistance, respectively. While these hypotheses are valid in certain settings, other mechanisms may affect gene flow. For example, the gene flow of invasive species may depend on founder effects and multiple introductions. Such mechanisms are not considered in modern landscape genomic models. We extend dyadic models to allow for mechanisms that range-shifting and/or invasive species may experience by introducing dyadic spatially-varying coefficients (DSVCs) defined on source-destination pairs. The DSVCs allow the effects of landscape on gene flow to vary across space, capturing nonstationary and asymmetric connectivity. Additionally, we incorporate explicit landscape features as connectivity covariates, which are localized to specific regions of the spatial domain and may function as barriers or corridors to gene flow. Such covariates are central to colonization and invasion, where spread accelerates along corridors and slows across landscape barriers. The proposed framework accommodates colonization-specific processes while retaining the ability to assess landscape influences on gene flow. Our case study of the highly invasive cheatgrass (Bromus tectorum) demonstrates the necessity of accounting for nonstationarity gene flow in range-shifting species.

</details>


### [6] [Unraveling time-varying causal effects of multiple exposures: integrating Functional Data Analysis with Multivariable Mendelian Randomization](https://arxiv.org/abs/2512.19064)
*Nicole Fontana,Francesca Ieva,Luisa Zuccolo,Emanuele Di Angelantonio,Piercesare Secchi*

Main category: stat.AP

TL;DR: MV-FMR扩展了功能孟德尔随机化，同时建模多个时变暴露，结合功能主成分分析和数据驱动的交叉验证策略，能恢复时变因果效应并优于单变量方法。


<details>
  <summary>Details</summary>
Motivation: 许多暴露因素在生命周期中具有时变因果效应，常与其他暴露因素共同影响结果或通过中介路径间接作用。现有多变量孟德尔随机化方法假设效应恒定，无法捕捉这些动态关系。

Method: MV-FMR结合功能主成分分析与数据驱动的交叉验证策略进行基函数选择，考虑重叠工具变量和中介效应，通过联合与单独暴露效应估计策略比较性能。

Result: 在各种数据生成场景下（非线性效应、水平多效性、中介效应、稀疏数据），MV-FMR能一致恢复真实因果函数，优于单变量方法。应用于UK Biobank数据成功分析收缩压和BMI对冠心病的时变因果效应。

Conclusion: MV-FMR提供了一个灵活可解释的框架，用于解析复杂的时变因果过程，为识别生命周期关键期和疾病预防相关可干预驱动因素提供新机会。

Abstract: Mendelian Randomization is a widely used instrumental variable method for assessing causal effects of lifelong exposures on health outcomes. Many exposures, however, have causal effects that vary across the life course and often influence outcomes jointly with other exposures or indirectly through mediating pathways. Existing approaches to multivariable Mendelian Randomization assume constant effects over time and therefore fail to capture these dynamic relationships. We introduce Multivariable Functional Mendelian Randomization (MV-FMR), a new framework that extends functional Mendelian Randomization to simultaneously model multiple time-varying exposures. The method combines functional principal component analysis with a data-driven cross-validation strategy for basis selection and accounts for overlapping instruments and mediation effects. Through extensive simulations, we assessed MV-FMR's ability to recover time-varying causal effects under a range of data-generating scenarios and compared the performance of joint versus separate exposure effect estimation strategies. Across scenarios involving nonlinear effects, horizontal pleiotropy, mediation, and sparse data, MV-FMR consistently recovered the true causal functions and outperformed univariable approaches. To demonstrate its practical value, we applied MV-FMR to UK Biobank data to investigate the time-varying causal effects of systolic blood pressure and body mass index on coronary artery disease. MV-FMR provides a flexible and interpretable framework for disentangling complex time-dependent causal processes and offers new opportunities for identifying life-course critical periods and actionable drivers relevant to disease prevention.

</details>


### [7] [Ant Colony Optimisation applied to the Travelling Santa Problem](https://arxiv.org/abs/2512.19627)
*Elliot Fisher,Robin Smith*

Main category: stat.AP

TL;DR: TSaP-ACO框架为圣诞老人全球配送设计优化路线，结合夜间窗口、能量预算和蚁群算法，显著减少能量消耗和路线长度。


<details>
  <summary>Details</summary>
Motivation: 圣诞老人全球配送面临严格的时间窗口（夜间配送）和能量预算约束，需要设计一个能同时满足时间窗口和能量效率的优化调度方案。

Method: 开发了Travelling-Santa Ant-Colony Optimisation (TSaP-ACO)启发式框架，使用人工蚁群迭代构建路径，通过信息素引导决策，并引入四个关键创新：(i) 将本地黑暗可行性嵌入信息素启发式；(ii) 通过缩小雪橇截面积最小化空气动力功；(iii) 使用低成本"流氓蚁"反转捕捉方向敏感时区；(iv) 动态调整路段巡航速度。

Result: 在15和30个首都城市的基准测试中，TSaP-ACO消除了所有白天违规，相比仅考虑距离的ACO减少了10%的总功。在40个首都城市的压力测试中，能量使用减少88%，路线长度缩短约67%。

Conclusion: 滚动窗口、能量感知的蚁群优化算法在现实全球配送场景中具有应用潜力，工作最小化自然产生了人口优先的路由策略。

Abstract: The hypothetical global delivery schedule of Santa Claus must follow strict rolling night-time windows that vary with the Earth's rotation and obey an energy budget that depends on payload size and cruising speed. To design this schedule, the Travelling-Santa Ant-Colony Optimisation framework (TSaP-ACO) was developed. This heuristic framework constructs potential routes via a population of artificial ants that iteratively extend partial paths. Ants make their decisions much like they do in nature, following pheromones left by other ants, but with a degree of permitted exploration. This approach: (i) embeds local darkness feasibility directly into the pheromone heuristic, (ii) seeks to minimise aerodynamic work via a shrinking sleigh cross sectional area, (iii) uses a low-cost "rogue-ant" reversal to capture direction-sensitive time-zones, and (iv) tunes leg-specific cruise speeds on the fly. On benchmark sets of 15 and 30 capital cities, the TSaP-ACO eliminates all daylight violations and reduces total work by up to 10% compared to a distance-only ACO. In a 40-capital-city stress test, it cuts energy use by 88%, and shortens tour length by around 67%. Population-first routing emerges naturally from work minimisation (50% served by leg 11 of 40). These results demonstrate that rolling-window, energy-aware ACO has potential applications more realistic global delivery scenarios.

</details>


### [8] [An Adaptive Graphical Lasso Approach to Modeling Symptom Networks of Common Mental Disorders in Eritrean Refugee Population](https://arxiv.org/abs/2512.19681)
*Elizabeth B. Amona,Indranil Sahoo,David Chan,Marianne B. Lund,Miriam Kuttikat*

Main category: stat.AP

TL;DR: 该研究使用自适应惩罚的图LASSO方法，分析了厄立特里亚难民中PTSD、抑郁、焦虑和躯体困扰的症状网络结构，识别了6个症状簇和关键中心症状。


<details>
  <summary>Details</summary>
Motivation: 难民群体中常见精神障碍的公共卫生负担严重，但其症状结构尚未充分探索。本研究旨在填补这一空白，特别是在小样本高维条件下改进症状网络分析方法。

Method: 采用高斯图模型和自适应惩罚的图LASSO方法，处理小样本高维数据（n < p）。通过bootstrap重采样评估网络可靠性，使用中心性度量识别关键症状。

Result: 识别出6个不同的症状簇，其中躯体-焦虑症状形成最互连的群体。恶心和重温过去经历等症状成为连接PTSD、焦虑、抑郁和躯体困扰的中心症状。恐惧感、睡眠问题和兴趣丧失是关键症状，维持整体网络连接。

Conclusion: 该研究为理解难民群体精神障碍的症状结构提供了新见解，识别出的中心症状可作为潜在干预目标。自适应惩罚图LASSO方法在小样本高维条件下表现良好，为类似研究提供了方法学参考。

Abstract: Despite the significant public health burden of common mental disorders (CMDs) among refugee populations, their underlying symptom structures remain underexplored. This study uses Gaussian graphical modeling to examine the symptom network of post-traumatic stress disorder (PTSD), depression, anxiety, and somatic distress among Eritrean refugees in the Greater Washington, DC area. Given the small sample size (n) and high-dimensional symptom space (p), we propose a novel extension of the standard graphical LASSO by incorporating adaptive penalization, which improves sparsity selection and network estimation stability under n < p conditions. To evaluate the reliability of the network, we apply bootstrap resampling and use centrality measures to identify the most influential symptoms. Our analysis identifies six distinct symptom clusters, with somatic-anxiety symptoms forming the most interconnected group. Notably, symptoms such as nausea and reliving past experiences emerge as central symptoms linking PTSD, anxiety, depression, and somatic distress. Additionally, we identify symptoms like feeling fearful, sleep problems, and loss of interest in activities as key symptoms, either being closely positioned to many others or acting as important bridges that help maintain the overall network connectivity, thereby highlighting their potential importance as possible intervention targets.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [9] [A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework](https://arxiv.org/abs/2512.17968)
*Ravi Prasad*

Main category: stat.CO

TL;DR: 本文对蒙特卡洛算法的演进进行了批判性分析，重点关注统计效率与计算成本之间的持续张力，从Metropolis-Hastings到Hamiltonian Monte Carlo等当代方法，讨论了时间空间复杂度边界，并提供了算法选择的理论框架。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛算法是现代计算科学的基石，但其有效应用需要深入理解性能权衡。本文旨在分析蒙特卡洛算法的演进，特别是统计效率与计算成本之间的持续张力，为算法选择提供理论指导。

Method: 采用历史发展分析的方法，从基础Metropolis-Hastings算法到当代Hamiltonian Monte Carlo方法进行系统梳理。重点对每个主要算法类别进行严格的时间空间复杂度分析，包括上界、下界和渐近紧界。同时构建算法选择的合理性框架。

Result: 系统梳理了蒙特卡洛算法的发展脉络，明确了各算法的复杂度边界，提供了梯度信息和自适应调优等关键理论观察。建立了在不同问题情境下选择最优算法的理论框架，能够明确判断何时使用特定算法优于其他算法。

Conclusion: 蒙特卡洛算法的发展体现了统计效率与计算成本之间的持续平衡演进。虽然已有显著进展，但仍面临重大研究挑战。本文的分析框架为算法选择提供了理论基础，对实际应用具有重要指导意义。

Abstract: Monte Carlo algorithms are a foundational pillar of modern computational science, yet their effective application hinges on a deep understanding of their performance trade offs. This paper presents a critical analysis of the evolution of Monte Carlo algorithms, focusing on the persistent tension between statistical efficiency and computational cost. We describe the historical development from the foundational Metropolis Hastings algorithm to contemporary methods like Hamiltonian Monte Carlo. A central emphasis of this survey is the rigorous discussion of time and space complexity, including upper, lower, and asymptotic tight bounds for each major algorithm class. We examine the specific motivations for developing these methods and the key theoretical and practical observations such as the introduction of gradient information and adaptive tuning in HMC that led to successively better solutions. Furthermore, we provide a justification framework that discusses explicit situations in which using one algorithm is demonstrably superior to another for the same problem. The paper concludes by assessing the profound significance and impact of these algorithms and detailing major current research challenges.

</details>


### [10] [Fast simulation of Gaussian random fields with flexible correlation models in Euclidean spaces](https://arxiv.org/abs/2512.18884)
*Moreno Bevilacqua,Xavier Emery,Francisco Cuevas-Pacheco*

Main category: stat.CO

TL;DR: 本文重新审视了谱转向带（STB）方法，将其扩展用于模拟具有灵活相关结构的各向同性高斯随机场，特别是针对两种新的相关模型：允许多项式衰减相关和长程依赖的Kummer-Tricomi模型，以及允许紧支撑相关的Gauss-Hypergeometric模型。


<details>
  <summary>Details</summary>
Motivation: 高斯随机场的高效模拟在空间统计、机器学习和不确定性量化中至关重要。传统方法在处理灵活相关结构时存在局限性，需要开发能够处理更广泛协方差模型的可扩展框架。

Method: 采用谱转向带（STB）方法，并扩展其适用范围。针对Kummer-Tricomi模型推导了Beta-prime混合表示，针对Gauss-Hypergeometric模型推导了互补Beta-和Gasper-混合表示。这些表示实现了线性复杂度的精确、数值稳定且计算高效的模拟。

Result: 数值实验证实了所提算法在各种参数配置下的准确性和计算稳定性。通过在大规模气候数据集分析中应用STB模拟器进行参数bootstrap，展示了其在标准误差估计和模型选择中的实际可行性。

Conclusion: STB方法为模拟具有灵活相关结构的高斯随机场提供了一个通用且可扩展的框架，特别适用于大规模空间建模，能够处理包括长程依赖和紧支撑相关在内的多种相关模型。

Abstract: The efficient simulation of Gaussian random fields with flexible correlation structures is fundamental in spatial statistics, machine learning, and uncertainty quantification. In this work, we revisit the \emph{spectral turning-bands} (STB) method as a versatile and scalable framework for simulating isotropic Gaussian random fields with a broad range of covariance models. Beyond the classical Matérn family, we show that the STB approach can be extended to two recent and flexible correlation classes that generalize the Matérn model: the Bummer-Tricomi model, which allows for polynomially decaying correlations and long-range dependence, and the Gauss-Hypergeometric model, which admits compactly supported correlations, including the Generalized Wendland family as a special case. We derive exact stochastic representations for both families: a Beta-prime mixture formulation for the Kummer-Tricomi model and complementary Beta- and Gasper-mixture representations for the Gauss-Hypergeometric model. These formulations enable exact, numerically stable, and computationally efficient simulation with linear complexity in the number of spectral components. Numerical experiments confirm the accuracy and computational stability of the proposed algorithms across a wide range of parameter configurations, demonstrating their practical viability for large-scale spatial modeling. As an application, we use the proposed STB simulators to perform parametric bootstrap for standard error estimation and model selection under weighted pairwise composite likelihood in the analysis of a large climate dataset.

</details>


### [11] [srvar-toolkit: A Python Implementation of Shadow-Rate Vector Autoregressions with Stochastic Volatility](https://arxiv.org/abs/2512.19589)
*Charles Shaw*

Main category: stat.CO

TL;DR: srvar-toolkit是一个用于贝叶斯向量自回归的Python包，特别处理利率有效下限约束和随机波动率，提供完整的贝叶斯推断工具链。


<details>
  <summary>Details</summary>
Motivation: 当利率达到有效下限时，传统VAR模型无法正确处理约束条件，需要开发能够处理影子利率约束的贝叶斯VAR工具。

Method: 采用贝叶斯框架，包含共轭正态-逆Wishart先验、Minnesota式收缩、通过Gibbs采样的潜在影子利率数据增强、使用Kim-Shephard-Chib混合近似的对角随机波动率，以及随机搜索变量选择。

Result: 开发了开源Python包srvar-toolkit，实现了Grammatikopoulos (2025)的方法，提供完整的贝叶斯推断工具链，支持命令行接口和可视化。

Conclusion: srvar-toolkit为宏观经济变量在利率有效下限约束下的预测提供了实用的贝叶斯VAR工具，填补了现有软件生态的空白。

Abstract: We introduce srvar-toolkit, an open-source Python package for Bayesian vector autoregression with shadow-rate constraints and stochastic volatility. The toolkit implements the methodology of Grammatikopoulos (2025, Journal of Forecasting) for forecasting macroeconomic variables when interest rates hit the effective lower bound. We provide conjugate Normal-Inverse-Wishart priors with Minnesota-style shrinkage, latent shadow-rate data augmentation via Gibbs sampling, diagonal stochastic volatility using the Kim-Shephard-Chib mixture approximation, and stochastic search variable selection. Core dependencies are NumPy, SciPy, and Pandas, with optional extras for plotting and a configuration-driven command-line interface. We release the software under the MIT licence at https://github.com/shawcharles/srvar-toolkit.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [12] [Deep Gaussian Processes with Gradients](https://arxiv.org/abs/2512.18066)
*Annie S. Booth*

Main category: stat.ME

TL;DR: 提出了一种新的贝叶斯框架，用于深度高斯过程（DGP）中整合梯度信息，通过开源软件包实现，在非平稳模拟中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 深度高斯过程（DGPs）是处理复杂非平稳计算机实验的流行代理模型，但传统的DGP难以整合梯度信息，而梯度信息对于平滑核的高斯过程来说很自然。目前尚未有在DGP中整合梯度的探索，这限制了DGP在梯度增强应用中的潜力。

Method: 提出了一种新颖且全面的贝叶斯框架，用于在深度高斯过程中整合梯度信息，支持梯度增强和梯度后验预测分布。通过"deepgp"开源软件包实现，并采用Vecchia近似来解决立方计算瓶颈问题。

Result: 在各种非平稳模拟中进行了基准测试，结果显示该方法在性能上优于传统的梯度增强高斯过程和常规的深度高斯过程。

Conclusion: 成功开发了一个能够有效整合梯度信息的深度高斯过程框架，通过开源软件实现，为处理复杂非平稳计算机实验提供了更强大的工具，在多个测试场景中表现出优越性能。

Abstract: Deep Gaussian processes (DGPs) are popular surrogate models for complex nonstationary computer experiments. DGPs use one or more latent Gaussian processes (GPs) to warp the input space into a plausibly stationary regime, then use typical GP regression on the warped domain. While this composition of GPs is conceptually straightforward, the functional nature of the multi-dimensional latent warping makes Bayesian posterior inference challenging. Traditional GPs with smooth kernels are naturally suited for the integration of gradient information, but the integration of gradients within a DGP presents new challenges and has yet to be explored. We propose a novel and comprehensive Bayesian framework for DGPs with gradients that facilitates both gradient-enhancement and gradient posterior predictive distributions. We provide open-source software in the "deepgp" package on CRAN, with optional Vecchia approximation to circumvent cubic computational bottlenecks. We benchmark our DGPs with gradients on a variety of nonstationary simulations, showing improvement over both GPs with gradients and conventional DGPs.

</details>


### [13] [quollr: An R Package for Visualizing 2-D Models from Nonlinear Dimension Reductions in High-Dimensional Space](https://arxiv.org/abs/2512.18166)
*Jayani P. Gamage,Dianne Cook,Paul Harrison,Michael Lydeamore,Thiyanga S. Talagala*

Main category: stat.ME

TL;DR: 开发了R包quollr作为可视化工具，用于评估非线性降维方法及其超参数选择的准确性


<details>
  <summary>Details</summary>
Motivation: 非线性降维方法会产生不同的数据表示，难以判断哪种方法或超参数选择能提供最准确的高维数据表示

Method: 开发R包quollr作为可视化工具，使用scurve数据和单细胞RNA测序数据进行算法演示和验证

Result: quollr包能够帮助确定哪种方法和超参数选择能提供最准确的高维数据表示

Conclusion: quollr是一个有效的可视化工具，可用于评估非线性降维方法的准确性，帮助研究人员选择最佳方法和参数

Abstract: Nonlinear dimension reduction methods provide a low-dimensional representation of high-dimensional data by applying a Nonlinear transformation. However, the complexity of the transformations and data structures can create wildly different representations depending on the method and hyper-parameter choices. It is difficult to determine whether any of these representations are accurate, which one is the best, or whether they have missed important structures. The R package quollr has been developed as a new visual tool to determine which method and which hyper-parameter choices provide the most accurate representation of high-dimensional data. The scurve data from the package is used to illustrate the algorithm. Single-cell RNA sequencing (scRNA-seq) data from mouse limb muscles are used to demonstrate the usability of the package.

</details>


### [14] [Copula Entropy: Theory and Applications](https://arxiv.org/abs/2512.18168)
*Jian Ma*

Main category: stat.ME

TL;DR: 关于copula熵（CE）理论和应用的综合专著，涵盖理论定义、性质、估计方法，以及在结构学习、因果发现、系统识别等多个领域的应用


<details>
  <summary>Details</summary>
Motivation: 提供copula熵理论的系统介绍，展示其在统计独立性和条件独立性测量方面的优势，以及在各科学工程领域的广泛应用潜力

Method: 首先介绍CE理论背景、定义、定理、性质和估计方法，然后构建基于CE的统计独立性和条件独立性测量框架，通过模拟评估其优势

Result: 建立了完整的CE理论体系，展示了CE在结构学习、关联发现、变量选择、因果发现、系统识别等多个理论应用中的有效性，并比较了与其他方法的优势

Conclusion: Copula熵是一个强大的理论框架，能够有效测量统计独立性和条件独立性，在多个科学工程领域具有广泛的应用价值，其方法相比其他类似框架具有明显优势

Abstract: This is the monograph on the theory and applications of copula entropy (CE). This book first introduces the theory of CE, including its background, definition, theorems, properties, and estimation methods. The theoretical applications of CE to structure learning, association discovery, variable selection, causal discovery, system identification, time lag estimation, domain adaptation, multivariate normality test, copula hypothesis test, two-sample test, change point detection, and symmetry test are reviewed. The relationships between the theoretical applications and their connections to correlation and causality are discussed. The framework based on CE for measuring statistical independence and conditional independence is compared to the other similar ones. The advantages of CE based methodologies over the other comparable ones are evaluated with simulations. The mathematical generalizations of CE are reviewed. The real applications of CE to every branch of science and engineering are briefly introduced.

</details>


### [15] [Data adaptive covariate balancing for causal effect estimation for high dimensional data](https://arxiv.org/abs/2512.18069)
*Simion De,Jared D. Huling*

Main category: stat.ME

TL;DR: 提出一种基于随机森林的非参数直接平衡方法，通过联合建模处理变量和结果变量来自适应地强调混杂因素，构建基于树相似性的距离度量，获得一致的处理效应估计。


<details>
  <summary>Details</summary>
Motivation: 现有加权方法在处理观测数据因果效应估计时面临两个主要问题：1) 参数方法容易受到模型误设影响；2) 平衡方法通常对所有协变量赋予同等重要性，可能忽视真正的混杂因素，且存在维度灾难问题。现有变量选择方法如结果自适应LASSO仍是参数方法且主要关注倾向得分估计而非直接平衡。

Method: 提出非参数直接平衡方法，使用随机森林联合建模处理变量和结果变量，让数据自动识别同时影响两个过程的协变量。构建相似性度量（定义为两个观测落入相同叶节点的树的比例），得到对相关协变量敏感且能捕捉混杂结构的处理组和对照组分布距离。

Result: 在适当假设下，证明所得权重在L2范数下收敛到归一化逆倾向得分，并提供一致的处理效应估计。通过大量模拟和真实数据集应用证明了方法的有效性。

Conclusion: 该方法通过随机森林自适应地强调混杂因素，解决了参数方法模型误设和传统平衡方法维度灾难问题，为观测数据因果效应估计提供了有效的非参数直接平衡解决方案。

Abstract: A key challenge in estimating causal effects from observational data is handling confounding and is commonly achieved through weighting methods that balance distribution of covariates between treatment and control groups. Weighting approaches can be classified by whether weights are estimated using parametric or nonparametric methods, and by whether the model relies on modeling and inverting the propensity score or directly estimates weights to achieve distributional balance by minimizing a measure of dissimilarity between groups. Parametric methods, both for propensity score modeling and direct balancing, are prone to model misspecification. In addition, balancing approaches often suffer from the curse of dimensionality, as they assign equal importance to all covariates, thus potentially de-emphasizing true confounders. Several methods, such as the outcome adaptive lasso, attempt to mitigate this issue through variable selection, but are parametric and focus on propensity score estimation rather than direct balancing. In this paper, we propose a nonparametric direct balancing approach that uses random forests to adaptively emphasize confounders. Our method jointly models treatment and outcome using random forests, allowing the data to identify covariates that influence both processes. We construct a similarity measure, defined by the proportion of trees in which two observations fall into the same leaf node, yielding a distance between treatment and control distributions that is sensitive to relevant covariates and captures the structure of confounding. Under suitable assumptions, we show that the resulting weights converge to normalized inverse propensity scores in the L2 norm and provide consistent treatment effect estimates. We demonstrate the effectiveness of our approach through extensive simulations and an application to a real dataset.

</details>


### [16] [Accuracy of Uniform Inference on Fine Grid Points](https://arxiv.org/abs/2512.18627)
*Shunsuke Imai*

Main category: stat.ME

TL;DR: 论文研究了在构建函数均匀置信带时，如何选择足够精细的评估网格以确保乘数自助法的有效性，并量化了覆盖误差的界限。


<details>
  <summary>Details</summary>
Motivation: 实践中构建函数均匀置信带时，由于连续索引集计算不可行，通常使用有限评估网格。但现有研究未明确量化网格需要多精细才能保证乘数自助法的有效性，这给实际应用带来不确定性。

Method: 提出理论框架，推导出覆盖误差的显式界限，将离散化效应与网格上的高维自助近似误差分离。通过核密度估计实例说明实现方法。

Result: 得到了明确的误差界限，为实践中选择网格大小提供了透明的工作流程。界限表明网格精细度需要与样本大小和置信水平相适应。

Conclusion: 为乘数自助法在有限网格上构建均匀置信带提供了理论保证和实用指导，解决了实际应用中网格选择的关键问题。

Abstract: Uniform confidence bands for functions are widely used in empirical analysis. A variety of simple implementation methods (most notably multiplier bootstrap) have been proposed and theoretically justified. However, an implementation over a literally continuous index set is generally computationally infeasible, and practitioners therefore compute the critical value by evaluating the statistic on a finite evaluation grid. This paper quantifies how fine the evaluation grid must be for a multiplier bootstrap procedure over finite grid points to deliver valid uniform confidence bands. We derive an explicit bound on the resulting coverage error that separates discretization effects from the intrinsic high-dimensional bootstrap approximation error on the grid. The bound yields a transparent workflow for choosing the grid size in practice, and we illustrate the implementation through an example of kernel density estimation.

</details>


### [17] [State-Space Modeling of Time-Varying Spillovers on Networks](https://arxiv.org/abs/2512.18584)
*Marios Papamichalis,Regina Ruane,Theofanis Papamichalis*

Main category: stat.ME

TL;DR: 提出网络状态空间模型，将网络结构与时变参数分离，通过低维潜在状态控制网络溢出效应、自滞后持续性和节点协变量效应


<details>
  <summary>Details</summary>
Motivation: 传统时变参数VAR模型忽略网络结构，而网络自回归模型通常假设恒定参数和平稳性。需要一种能同时考虑网络结构和时变参数的框架

Method: 开发网络状态空间模型，其中低维潜在状态控制时变网络溢出效应。关键特例是网络时变参数VAR（NTVP-VAR），将滞后矩阵约束为已知网络算子的线性组合，相关系数随时间随机演化

Result: 框架包含高斯和泊松网络自回归、网络ARIMA模型、动态边模型等。给出确保NTVP-VAR在二阶矩中定义良好的条件，描述网络版本的稳定性和局部平稳性

Conclusion: 网络状态空间模型将交互位置（网络图）与交互强度（潜在状态）分离，为无结构TVP-VAR和现有网络时间序列模型提供了可解释的替代方案

Abstract: Many modern time series arise on networks, where each component is attached to a node and interactions follow observed edges. Classical time-varying parameter VARs (TVP-VARs) treat all series symmetrically and ignore this structure, while network autoregressive models exploit a given graph but usually impose constant parameters and stationarity. We develop network state-space models in which a low-dimensional latent state controls time-varying network spillovers, own-lag persistence and nodal covariate effects. A key special case is a network time-varying parameter VAR (NTVP-VAR) that constrains each lag matrix to be a linear combination of known network operators, such as a row-normalised adjacency and the identity, and lets the associated coefficients evolve stochastically in time. The framework nests Gaussian and Poisson network autoregressions, network ARIMA models with graph differencing, and dynamic edge models driven by multivariate logistic regression. We give conditions ensuring that NTVP-VARs are well-defined in second moments despite nonstationary states, describe network versions of stability and local stationarity, and discuss shrinkage, thresholding and low-rank tensor structures for high-dimensional graphs. Conceptually, network state-space models separate where interactions may occur (the graph) from how strong they are at each time (the latent state), providing an interpretable alternative to both unstructured TVP-VARs and existing network time-series models.

</details>


### [18] [Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences](https://arxiv.org/abs/2512.18119)
*Kohei Watanabe*

Main category: stat.ME

TL;DR: 作者提出了一种名为分布式非对称分配（DAA）的新主题模型，解决了LDA在处理大型语料库时的效率低下、主题碎片化和种子词定义特定主题失败等问题。


<details>
  <summary>Details</summary>
Motivation: 社会科学家使用LDA从大型语料库中发现特定主题时面临三个主要问题：1) LDA在大型语料库上拟合耗时过长；2) 无监督LDA在短文档中将主题碎片化为子主题；3) 半监督LDA无法识别使用种子词定义的特定主题。

Method: 开发了分布式非对称分配（DAA）主题模型，该模型集成了多种算法，用于高效识别大型语料库中关于重要主题的句子。使用1991-2017年联合国大会演讲记录作为评估数据集。

Result: DAA在句子分类的准确性和速度上都显著优于LDA。结果表明，社会科学家需要优化LDA的Dirichlet先验才能准确进行内容分析。

Conclusion: DAA模型有效解决了LDA在大型语料库分析中的局限性，为社会科学家的内容分析提供了更高效、更准确的工具，并强调了优化Dirichlet先验的重要性。

Abstract: Social scientists employ latent Dirichlet allocation (LDA) to find highly specific topics in large corpora, but they often struggle in this task because (1) LDA, in general, takes a significant amount of time to fit on large corpora; (2) unsupervised LDA fragments topics into sub-topics in short documents; (3) semi-supervised LDA fails to identify specific topics defined using seed words. To solve these problems, I have developed a new topic model called distributed asymmetric allocation (DAA) that integrates multiple algorithms for efficiently identifying sentences about important topics in large corpora. I evaluate the ability of DAA to identify politically important topics by fitting it to the transcripts of speeches at the United Nations General Assembly between 1991 and 2017. The results show that DAA can classify sentences significantly more accurately and quickly than LDA thanks to the new algorithms. More generally, the results demonstrate that it is important for social scientists to optimize Dirichlet priors of LDA to perform content analysis accurately.

</details>


### [19] [A Reduced Basis Decomposition Approach to Efficient Data Collection in Pairwise Comparison Studies](https://arxiv.org/abs/2512.19398)
*Jiahua Jiang,Joseph Marsh,Rowland G Seymour*

Main category: stat.ME

TL;DR: 提出一种基于降基分解的可扩展方法，用于大规模比较判断研究的实验设计优化，计算效率提升100倍以上


<details>
  <summary>Details</summary>
Motivation: 比较判断研究通常使用Bradley-Terry模型分析，但实验设计需要计算成对比较的协方差矩阵谱分解，当研究对象超过150个时计算变得不可行

Method: 提出基于降基分解的可扩展方法，避免显式构建协方差矩阵，利用设计矩阵的秩结构特性，保证近似质量的特征值边界

Result: 计算效率提升2-3个数量级，对64个以上对象的研究速度提升超过100倍，近似误差可忽略；452区域空间研究设计在7分钟内完成，课堂同伴评估实时设计更新从15分钟减少到15秒

Conclusion: 该方法解决了大规模比较判断研究的计算瓶颈，使实时实验设计更新成为可能，为大规模应用提供了实用解决方案

Abstract: Comparative judgement studies elicit quality assessments through pairwise comparisons, typically analysed using the Bradley-Terry model. A challenge in these studies is experimental design, specifically, determining the optimal pairs to compare to maximize statistical efficiency. Constructing static experimental designs for these studies requires spectral decomposition of a covariance matrix over pairs of pairs, which becomes computationally infeasible for studies with more than approximately 150 objects. We propose a scalable method based on reduced basis decomposition that bypasses explicit construction of this matrix, achieving computational savings of two to three orders of magnitude. We establish eigenvalue bounds guaranteeing approximation quality and characterise the rank structure of the design matrix. Simulations demonstrate speedup factors exceeding 100 for studies with 64 or more objects, with negligible approximation error. We apply the method to construct designs for a 452-region spatial study in under 7 minutes and enable real-time design updates for classroom peer assessment, reducing computation time from 15 minutes to 15 seconds.

</details>


### [20] [Testing for latent structure via the Wilcoxon--Wigner random matrix of normalized rank statistics](https://arxiv.org/abs/2512.18924)
*Jonquil Z. Liao,Joshua Cape*

Main category: stat.ME

TL;DR: 提出Wilcoxon-Wigner随机矩阵，基于秩统计量构建对称矩阵，用于检测大规模对称数据矩阵中的潜在结构，提供参数无关、分布无关的谱方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在灵活性、计算效率和极端数据变化敏感性方面存在局限，需要开发统计原理严谨、适用范围广、计算高效且对极端数据变化不敏感的方法来检测大规模对称数据矩阵中的潜在结构。

Method: 引入Wilcoxon-Wigner随机矩阵，其元素是从独立同分布绝对连续随机变量样本中导出的归一化秩统计量。建立这些矩阵主特征值和特征向量的渐近高斯波动理论，基于此开发参数无关、分布无关的谱方法，用于社区检测和主子矩阵检测两个假设检验问题。

Result: 证明了Wilcoxon-Wigner随机矩阵的主特征值和对应特征向量具有渐近高斯分布，给出了明确的中心化和尺度项。这些渐近结果使得能够开发严格的参数无关、分布无关的谱方法。数值实验展示了所提方法的性能。

Conclusion: Wilcoxon-Wigner随机矩阵为检测大规模对称数据矩阵中的潜在结构提供了灵活、计算高效且对极端数据变化不敏感的统计方法，克服了现有方法的局限性，在非参数统计、多元分析和数据降维的交叉领域具有重要价值。

Abstract: This paper considers the problem of testing for latent structure in large symmetric data matrices. The goal here is to develop statistically principled methodology that is flexible in its applicability, computationally efficient, and insensitive to extreme data variation, thereby overcoming limitations facing existing approaches. To do so, we introduce and systematically study certain symmetric matrices, called Wilcoxon--Wigner random matrices, whose entries are normalized rank statistics derived from an underlying independent and identically distributed sample of absolutely continuous random variables. These matrices naturally arise as the matricization of one-sample problems in statistics and conceptually lie at the interface of nonparametrics, multivariate analysis, and data reduction. Among our results, we establish that the leading eigenvalue and corresponding eigenvector of Wilcoxon--Wigner random matrices admit asymptotically Gaussian fluctuations with explicit centering and scaling terms. These asymptotic results enable rigorous parameter-free and distribution-free spectral methodology for addressing two hypothesis testing problems, namely community detection and principal submatrix detection. Numerical examples illustrate the performance of the proposed approach. Throughout, our findings are juxtaposed with existing results based on the spectral properties of independent entry symmetric random matrices in signal-plus-noise data settings.

</details>


### [21] [Efficient Bayesian inference for two-stage models in environmental epidemiology](https://arxiv.org/abs/2512.18143)
*Konstantin Larin,Daniel R. Kowal*

Main category: stat.ME

TL;DR: 提出贝叶斯两阶段模型的联合推断算法，解决环境流行病学中空气污染暴露估计不完整的问题，避免传统方法导致的推断偏差。


<details>
  <summary>Details</summary>
Motivation: 在环境流行病学中，空气污染暴露通常需要通过复杂模型估计，但分析者通常只能获得第一阶段模型的输出，而无法获取其完整规范或输入数据，这使得联合贝叶斯推断看似不可行。传统方法（使用点估计或仅使用第一阶段后验而不考虑第二阶段反馈）会导致推断校准错误。

Method: 提出高效的算法来促进联合贝叶斯推断，提供更准确的估计和良好校准的不确定性。通过比较不同方法，研究美国中南部地区PM2.5暴露与县级死亡率之间的关联。

Result: 新提出的算法能够提供更准确的估计和良好校准的不确定性，相比传统方法（使用点估计或仅使用第一阶段后验）能避免推断校准错误。

Conclusion: 当统计模型的输入不完全已知时，传统两阶段推断方法存在缺陷。提出的联合贝叶斯推断算法能够解决这一问题，为环境流行病学中的暴露-健康关系研究提供更可靠的方法。

Abstract: Statistical models often require inputs that are not completely known. This can occur when inputs are measured with error, indirectly, or when they are predicted using another model. In environmental epidemiology, air pollution exposure is a key determinant of health, yet typically must be estimated for each observational unit by a complex model. Bayesian two-stage models combine this stage-one model with a stage-two model for the health outcome given the exposure. However, analysts usually only have access to the stage-one model output without all of its specifications or input data, making joint Bayesian inference apparently intractable. We show that two prominent workarounds-using a point estimate or using the posterior from the stage-one model without feedback from the stage-two model-lead to miscalibrated inference. Instead, we propose efficient algorithms to facilitate joint Bayesian inference and provide more accurate estimates and well-calibrated uncertainties. Comparing different approaches, we investigate the association between PM2.5 exposure and county-level mortality rates in the South-Central USA.

</details>


### [22] [Frequentist forecasting in regime-switching models with extended Hamilton filter](https://arxiv.org/abs/2512.18149)
*Kento Okuyama,Tim Fabian Schaffland,Pascal Kilian,Holger Brandt,Augustin Kelava*

Main category: stat.ME

TL;DR: 本文提出了一种用于体制转换状态空间(RSSS)模型的频域滤波器，首次实现了在动态潜变量框架中同时考虑个体内和个体间特征作为体制转换预测因子的频域估计方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究在心理变化过程（如大学生数学辍学）的体制转换模型估计方法上存在局限，特别是缺乏能够同时处理个体内变化和个体间差异作为体制转换预测因子的频域估计方法，以及需要能够在数据收集过程中进行实时推断和预测的方法。

Method: 基于Chow和Zhang(2013)的扩展Kim滤波器，开发了RSSS模型的第一个频域滤波器，允许隐马尔可夫转换模型同时依赖于潜个体内和个体间特征，作为Kelava等人(2022)贝叶斯预测滤波器的频域对应方法。

Result: 通过模拟研究评估了参数恢复和体制及动态潜变量的预测性能，并在实证研究中应用该滤波器预测与数学辍学相关的情感和行为。

Conclusion: 提出的频域滤波器填补了非线性动态潜类别结构方程模型(NDLC-SEM)中频域方法的空白，为心理变化过程的实时预测提供了新的工具，特别适用于大学生数学辍学等具有离散潜状态转换的过程研究。

Abstract: Psychological change processes, such as university student dropout in math, often exhibit discrete latent state transitions and can be studied using regime-switching models with intensive longitudinal data (ILD). Recently, regime-switching state-space (RSSS) models have been extended to allow for latent variables and their autoregressive effects. Despite this progress, estimation methods for handling both intra-individual changes and inter-individual differences as predictors of regime-switches need further exploration. Specifically, there's a need for frequentist estimation methods in dynamic latent variable frameworks that allow real-time inferences and forecasts of latent or observed variables during ongoing data collection. Building on Chow and Zhang's (2013) extended Kim filter, we introduce a first frequentist filter for RSSS models which allows hidden Markov(-switching) models to depend on both latent within- and between-individual characteristics. As a counterpart of Kelava et al.'s (2022) Bayesian forecasting filter for nonlinear dynamic latent class structural equation models (NDLC-SEM), our proposed method is the first frequentist approach within this general class of models. In an empirical study, the filter is applied to forecast emotions and behavior related to student dropout in math. Parameter recovery and prediction of regime and dynamic latent variables are evaluated through simulation study.

</details>


### [23] [cardinalR: Generating Interesting High-Dimensional Data Structures](https://arxiv.org/abs/2512.18172)
*Jayani P. Gamage,Dianne Cook,Paul Harrison,Michael Lydeamore,Thiyanga S. Talagala*

Main category: stat.ME

TL;DR: 开发了R包cardinalR，用于生成具有各种依赖结构的高维模拟数据，特别关注非线性降维方法的评估


<details>
  <summary>Details</summary>
Motivation: 需要高质量的模拟高维数据来测试、验证和改进降维、监督和无监督学习算法。现有工具集需要丰富，特别关注非线性依赖结构

Method: 使用数学函数和统计分布构建新的高维数据生成方法，组织成R包cardinalR，提供多种示例数据集

Result: 开发了cardinalR包，能够生成具有线性、非线性、聚类和异常等不同依赖结构的高维数据，丰富了算法评估的基准数据集

Conclusion: cardinalR包为研究人员提供了强大的工具，帮助他们更好地理解不同分析方法的工作原理和改进方式，特别关注非线性降维方法的评估

Abstract: Simulated high-dimensional data is useful for testing, validating, and improving algorithms used in dimension reduction, supervised and unsupervised learning. High-dimensional data is characterized by multiple variables that are dependent or associated in some way, such as linear, nonlinear, clustering or anomalies. Here we provide new methods for generating a variety of high-dimensional structures using mathematical functions and statistical distributions organized into the R package cardinalR. Several example data sets are also provided. These will be useful for researchers to better understand how different analytical methods work and can be improved, with a special focus on nonlinear dimension reduction methods. This package enriches the existing toolset of benchmark datasets for evaluating algorithms.

</details>


### [24] [Applying non-negative matrix factorization with covariates to structural equation modeling for blind input-output analysis](https://arxiv.org/abs/2512.18250)
*Kenichi Satoh*

Main category: stat.ME

TL;DR: 提出NMF-SEM框架，将非负矩阵分解嵌入结构方程模型，实现可解释的因果反馈分析


<details>
  <summary>Details</summary>
Motivation: 结合结构方程模型（SEM）的因果依赖与反馈机制，以及非负矩阵分解（NMF）的可解释性，为处理非负数据提供统一的因果分析框架

Method: 提出NMF-SEM框架，将NMF嵌入联立方程结构，使用正则化乘法更新估计方法，包含正交性和稀疏性惩罚，并引入结构评估指标

Result: 在Holzinger-Swineford数据中恢复经典三因子结构；在洛杉矶系统中识别气候和污染物驱动的死亡率路径（反馈可忽略）；在密西西比健康结果中分离贫困、一般发病率和绝望死亡成分（反馈较弱）

Conclusion: NMF-SEM成功统一了SEM的因果反馈机制和NMF的可解释性，为分析非负数据的因果结构提供了有效框架

Abstract: Structural equation modeling (SEM) describes directed dependence and feedback, whereas non-negative matrix factorization (NMF) provides interpretable, parts-based representations for non-negative data. We propose NMF-SEM, a unified non-negative framework that embeds NMF within a simultaneous-equation structure, enabling latent feedback loops and a reduced-form input-output mapping when intermediate flows are unobserved. The mapping separates direct effects from cumulative propagation effects and summarizes reinforcement using an amplification ratio.
  We develop regularized multiplicative-update estimation with orthogonality and sparsity penalties, and introduce structural evaluation metrics for input-output fidelity, second-moment (covariance-like) agreement, and feedback strength. Applications show that NMF-SEM recovers the classical three-factor structure in the Holzinger-Swineford data, identifies climate- and pollutant-driven mortality pathways with negligible feedback in the Los Angeles system, and separates deprivation, general morbidity, and deaths-of-despair components with weak feedback in Mississippi health outcomes.

</details>


### [25] [Consistent Bayesian meta-analysis on subgroup specific effects and interactions](https://arxiv.org/abs/2512.18785)
*Renato Panaro,Christian Röver,Tim Friede*

Main category: stat.ME

TL;DR: 提出贝叶斯贡献调整亚组荟萃分析(CAMS)方法，通过调整试验内亚组贡献的信息分数，实现亚组数据交互作用荟萃分析的一致性推断。


<details>
  <summary>Details</summary>
Motivation: 临床试验通常报告全人群和亚组效应，但不同试验的亚组权重不同，导致亚组特异性效应和治疗-亚组交互作用的荟萃分析结果不一致，需要一种能调整亚组贡献差异的方法。

Method: 提出贝叶斯贡献调整亚组荟萃分析(CAMS)框架，通过信息分数(IF)量化试验内亚组相对贡献并进行调整，采用自包含的模型化方法，可纳入先验信息，适用于少量研究的交互作用荟萃分析。

Result: 在多发性硬化症疾病修饰治疗的7个试验示例中，CAMS显示在低残疾(EDSS ≤ 3.5)患者中治疗-残疾交互作用(复发率降低)比未调整模型更强，而年轻患者(<40岁)结果不变。

Conclusion: CAMS方法通过控制亚组贡献同时保持亚组可解释性，能够在已发表亚组数据可用时实现可靠的交互作用决策，该方法虽在贝叶斯背景下提出，也可在频率主义或似然框架中实现。

Abstract: Commonly, clinical trials report effects not only for the full study population but also for patient subgroups. Meta-analyses of subgroup-specific effects and treatment-by-subgroup interactions may be inconsistent, especially when trials apply different subgroup weightings. We show that meta-regression can, in principle, with a contribution adjustment, recover the same interaction inference regardless of whether interaction data or subgroup data are used. Our Bayesian framework for subgroup-data interaction meta-analysis inherently (i) adjusts for varying relative subgroup contribution, quantified by the information fraction (IF) within a trial; (ii) is robust to prevalence imbalance and variation; (iii) provides a self-contained, model-based approach; and (iv) can be used to incorporate prior information into interaction meta-analyses with few studies.The method is demonstrated using an example with as few as seven trials of disease-modifying therapies in relapsing-remitting multiple sclerosis. The Bayesian Contribution-adjusted Meta-analysis by Subgroup (CAMS) indicates a stronger treatment-by-disability interaction (relapse rate reduction) in patients with lower disability (EDSS <= 3.5) compared with the unadjusted model, while results for younger patients (age < 40 years) are unchanged.By controlling subgroup contribution while retaining subgroup interpretability, this approach enables reliable interaction decision-making when published subgroup data are available.Although the proposed CAMS approach is presented in a Bayesian context, it can also be implemented in frequentist or likelihood frameworks.

</details>


### [26] [On Efficient Adjustment for Micro Causal Effects in Summary Causal Graphs](https://arxiv.org/abs/2512.18315)
*Isabela Belciug,Simon Ferreira,Charles K. Assaad*

Main category: stat.ME

TL;DR: 本文针对摘要因果图（SCGs）中的因果效应识别问题，提出了更简单的可识别性条件公式，并引入了能识别更广泛有效调整集的新准则，同时确定了其中渐近方差最小的准最优调整集。


<details>
  <summary>Details</summary>
Motivation: 现有摘要因果图（SCGs）中的因果效应可识别性条件存在两个主要局限：1）条件复杂，依赖繁琐定义且需要枚举SCG中的多条路径，计算成本高；2）当条件满足时，仅提供两个有效调整集，限制了实际应用的灵活性。

Method: 提出了与现有可识别性条件等价但更简单的公式化表达，并引入了一个新的准则来识别SCGs中更广泛类别的有效调整集。此外，还表征了这些调整集中的准最优调整集，即最小化因果效应估计器渐近方差的调整集。

Result: 开发了更简单实用的可识别性条件公式，能够识别更广泛的调整集，并确定了其中渐近方差最小的准最优调整集，为抽象因果图中的因果推断提供了更灵活高效的工具。

Conclusion: 本文的贡献为抽象因果图中的因果推断提供了理论进步和实用工具，实现了更灵活高效的因果效应识别和估计。

Abstract: Observational studies in fields such as epidemiology often rely on covariate adjustment to estimate causal effects. Classical graphical criteria, like the back-door criterion and the generalized adjustment criterion, are powerful tools for identifying valid adjustment sets in directed acyclic graphs (DAGs). However, these criteria are not directly applicable to summary causal graphs (SCGs), which are abstractions of DAGs commonly used in dynamic systems. In SCGs, each node typically represents an entire time series and may involve cycles, making classical criteria inapplicable for identifying causal effects. Recent work established complete conditions for determining whether the micro causal effect of a treatment or an exposure $X_{t-γ}$ on an outcome $Y_t$ is identifiable via covariate adjustment in SCGs, under the assumption of no hidden confounding. However, these identifiability conditions have two main limitations. First, they are complex, relying on cumbersome definitions and requiring the enumeration of multiple paths in the SCG, which can be computationally expensive. Second, when these conditions are satisfied, they only provide two valid adjustment sets, limiting flexibility in practical applications. In this paper, we propose an equivalent but simpler formulation of those identifiability conditions and introduce a new criterion that identifies a broader class of valid adjustment sets in SCGs. Additionally, we characterize the quasi-optimal adjustment set among these, i.e., the one that minimizes the asymptotic variance of the causal effect estimator. Our contributions offer both theoretical advancement and practical tools for more flexible and efficient causal inference in abstracted causal graphs.

</details>


### [27] [A Universal Framework for Factorial Matched Observational Studies with General Treatment Types: Design, Analysis, and Applications](https://arxiv.org/abs/2512.18997)
*Jianan Zhu,Tianruo Zhang,Diana Silver,Ellicott Matthay,Omar El-Shahawy,Hyunseung Kang,Siyu Heng*

Main category: stat.ME

TL;DR: 提出通用框架处理多因子处理（包括连续型）的匹配观测研究，包含两阶段匹配算法、广义Neyman型估计量及推断方法


<details>
  <summary>Details</summary>
Motivation: 现有匹配方法只能处理单处理（任意类型）或多因子处理（仅二值），无法处理多因子且非二值的处理类型，限制了匹配方法在现实问题中的应用

Method: 提出两阶段非二分匹配算法构建协变量相似但处理剂量不同的匹配集；引入广义因子Neyman型估计量定义边际和交互因果效应；开发随机化推断方法

Result: 开发了完整的框架，包括匹配算法、效应定义和推断程序，并在COVID-19疫情期间评估工作与非工作出行减少对健康结局的因果影响中应用

Conclusion: 填补了匹配方法在多因子非二值处理研究中的空白，提供了通用框架，扩展了匹配方法在现实复杂因果推断问题中的应用范围

Abstract: Matching is one of the most widely used causal inference frameworks in observational studies. However, all the existing matching-based causal inference methods are designed for either a single treatment with general treatment types (e.g., binary, ordinal, or continuous) or factorial (multiple) treatments with binary treatments only. To our knowledge, no existing matching-based causal methods can handle factorial treatments with general treatment types. This critical gap substantially hinders the applicability of matching in many real-world problems, in which there are often multiple, potentially non-binary (e.g., continuous) treatment components. To address this critical gap, this work develops a universal framework for the design and analysis of factorial matched observational studies with general treatment types (e.g., binary, ordinal, or continuous). We first propose a two-stage non-bipartite matching algorithm that constructs matched sets of units with similar covariates but distinct combinations of treatment doses, thereby enabling valid estimation of both main and interaction effects. We then introduce a new class of generalized factorial Neyman-type estimands that provide model-free, finite-population-valid definitions of marginal and interaction causal effects under factorial treatments with general treatment types. Randomization-based Fisher-type and Neyman-type inference procedures are developed, including unbiased estimators, asymptotically valid variance estimators, and variance adjustments incorporating covariate information for improved efficiency. Finally, we illustrate the proposed framework through a county-level application that evaluates the causal impacts of work- and non-work-trip reductions (social distancing practices) on COVID-19-related and drug-related outcomes during the COVID-19 pandemic in the United States.

</details>


### [28] [Bayesian Brain Edge-Based Connectivity (BBeC): a Bayesian model for brain edge-based connectivity inference](https://arxiv.org/abs/2512.18403)
*Zijing Li,Chenhao Zeng,Shufei Ge*

Main category: stat.ME

TL;DR: 提出基于有限维Dirichlet分布的贝叶斯模型，用于推断大脑网络拓扑结构和高维协方差矩阵，解决维度灾难问题并保证协方差矩阵正定性。


<details>
  <summary>Details</summary>
Motivation: 基于磁共振成像的大脑连接分析面临维度灾难问题，现有方法难以处理大脑边缘的未知潜在拓扑结构，导致参数估计不准确和推断不稳定。

Method: 使用有限维Dirichlet分布建模大脑网络拓扑结构，重新构建协方差矩阵结构以保证正定性，采用Metropolis-Hastings算法同时推断网络拓扑和相关参数。

Result: 模拟验证了网络拓扑和相关参数的恢复能力；应用于阿尔茨海默病神经影像数据集，成功识别结构子网络，组内连接强度强于组间连接强度。

Conclusion: 该贝叶斯框架为大规模神经影像研究提供了可靠工具，能降低参数维度同时保证协方差矩阵正定性，有助于探索内在大脑连接性。

Abstract: Brain connectivity analysis based on magnetic resonance imaging is crucial for understanding neurological mechanisms. However, edge-based connectivity inference faces significant challenges, particularly the curse of dimensionality when estimating high-dimensional covariance matrices. Existing methods often struggle to account for the unknown latent topological structure among brain edges, leading to inaccurate parameter estimation and unstable inference. To address these issues, this study proposes a Bayesian model based on a finite-dimensional Dirichlet distribution. Unlike non-parametric approaches, our method utilizes a finite-dimensional Dirichlet distribution to model the topological structure of brain networks, ensuring constant parameter dimensionality and improving algorithmic stability. We reformulate the covariance matrix structure to guarantee positive definiteness and employ a Metropolis-Hastings algorithm to simultaneously infer network topology and correlation parameters. Simulations validated the recovery of both network topology and correlation parameters. When applied to the Alzheimer's Disease Neuroimaging Initiative dataset, the model successfully identified structural subnetworks. The identified clusters were not only validated by composite anatomical metrics but also consistent with established findings in the literature, collectively demonstrating the model's reliability. The estimated covariance matrix also revealed that intragroup connection strength is stronger than intergroup connection strength. This study introduces a Bayesian framework for inferring brain network topology and high-dimensional covariance structures. The model configuration reduces parameter dimensionality while ensuring the positive definiteness of covariance matrices. As a result, it offers a reliable tool for investigating intrinsic brain connectivity in large-scale neuroimaging studies.

</details>


### [29] [A Markov Chain Modeling Approach for Predicting Relative Risks of Spatial Clusters in Public Health](https://arxiv.org/abs/2512.19635)
*Lyza Iamrache,Kamel Rekab,Majid Bani-Yagoub,Julia Pluta,Abdelghani Mehailia*

Main category: stat.ME

TL;DR: 提出一种基于马尔可夫链建模的方法，在有限纵向数据下预测空间簇的相对风险，在COVID-19发病率数据上表现优于先前死亡率研究


<details>
  <summary>Details</summary>
Motivation: 公共卫生中预测空间簇的相对风险通常需要高分辨率纵向数据，但这类数据往往难以获得。本研究旨在改进先前工作，在有限纵向数据情况下实现准确的序列相对风险预测。

Method: 首先使用似然比检验识别用户定义时间区间内的显著空间簇，然后应用马尔可夫链建模方法预测每个时间区间的相对风险值。

Result: 提出的方法在COVID-19发病率数据上比先前死亡率研究表现更好，增加时间区间数量能提高马尔可夫链建模方法的准确性。

Conclusion: 该方法为有限纵向数据下的空间簇相对风险预测提供了有效解决方案，在公共卫生监测中具有应用价值。

Abstract: Predicting relative risk (RR) of spatial clusters is a complex task in public health that can be achieved through various statistical and machine-learning methods for different time intervals. However, high-resolution longitudinal data is often unavailable to successfully apply such methods. The goal of the present study is to further develop and test a new methodology proposed in our previous work for accurate sequential RR predictions in the case of limited lon gitudinal data. In particular, we first use a well-known likelihood ratio test to identify significant spatial clusters over user-defined time intervals. Then we apply a Markov chain modeling ap approach to predict RR values for each time interval. Our findings demonstrate that the proposed approach yields better performance with COVID-19 morbidity data compared to the previous study on mortality data. Additionally, increasing the number of time intervals enhances the accuracy of the proposed Markov chain modeling method.

</details>


### [30] [Calibrating hierarchical Bayesian domain inference for a proportion](https://arxiv.org/abs/2512.18479)
*Rayleigh Lei,Yajuan Si*

Main category: stat.ME

TL;DR: 该论文将FAB置信区间扩展到二元结果的小区域估计，通过MRP校正选择偏差，并在COVID-19感染率估计中验证了其改进名义覆盖率的性能。


<details>
  <summary>Details</summary>
Motivation: 小区域估计在数据稀疏时很重要，但传统的贝叶斯分层模型倾向于向总体均值收缩，可能导致置信区间无法维持名义覆盖率。虽然Hoff等人为连续数据开发了FAB区间，但二元数据面临新挑战，需要扩展方法。

Method: 将FAB区间扩展到二元结果的比例估计，通过数值计算FAB区间；利用多水平回归和后分层(MRP)校正样本选择偏差，为MRP估计构建FAB区间；通过重复模拟研究评估性能。

Result: FAB区间改善了名义覆盖率，但代价是区间更宽；在COVID-19感染率的地理和人口亚组估计应用中验证了方法的有效性。

Conclusion: 该研究成功将FAB区间扩展到二元数据的小区域估计，结合MRP校正选择偏差，为稀疏数据的亚组推断提供了改进的统计推断方法。

Abstract: Small area estimation (SAE) improves estimates for local communities or groups, such as counties, neighborhoods, or demographic subgroups, when data are insufficient for each area. This is important for targeting local resources and policies, especially when national-level or large-area data mask variation at a more granular level. Researchers often fit hierarchical Bayesian models to stabilize SAE when data are sparse. Ideally, Bayesian procedures also exhibit good frequentist properties, as demonstrated by calibrated Bayes metrics. However, hierarchical Bayesian models tend to shrink domain estimates toward the overall mean and may produce credible intervals that do not maintain nominal coverage. Hoff et al. developed the Frequentist, but Assisted by Bayes (FAB) intervals for subgroup estimates with normally distributed outcomes. However, non-normally distributed data present new challenges, and multiple types of intervals have been proposed for estimating proportions. We examine domain inference with binary outcomes and extend FAB intervals to improve nominal coverage. We describe how to numerically compute FAB intervals for a proportion and evaluate their performance through repeated simulation studies. Leveraging multilevel regression and poststratification (MRP), we further refine SAE to correct for sample selection bias, construct the FAB intervals for MRP estimates and assess their repeated sampling properties. Finally, we apply the proposed inference methods to estimate COVID-19 infection rates across geographic and demographic subgroups. We find that the FAB intervals improve nominal coverage, at the cost of wider intervals.

</details>


### [31] [A Bayesian likely responder approach for the analysis of randomized controlled trials](https://arxiv.org/abs/2512.18492)
*Annan Deng,Carole Siegel,Hyung G. Park*

Main category: stat.ME

TL;DR: 提出一种贝叶斯两阶段方法，将亚组识别与亚组特异性治疗效应推断相结合，通过第一阶段的后验分布考虑模型估计不确定性，提高置信区间的校准性。


<details>
  <summary>Details</summary>
Motivation: 精准医学需要识别最可能从特定治疗中获益的个体，但现有的可能应答者框架和数据驱动的亚组分析往往忽略模型估计的不确定性，导致推断结果不可靠。

Method: 提出简单的两阶段方法：第一阶段进行亚组识别，第二阶段进行亚组特异性治疗效应推断。利用第一阶段贝叶斯后验分布将模型估计不确定性纳入第二阶段的治疗效应估计。

Result: 模拟研究表明，提出的贝叶斯两阶段模型比朴素方法产生更好校准的置信区间。在国际COVID-19治疗试验中应用，显示数据驱动亚组间治疗效应存在显著差异。

Conclusion: 该方法通过整合亚组识别与亚组特异性推断，并考虑模型估计不确定性，为精准医学中的个性化治疗决策提供了更可靠的统计框架。

Abstract: An important goal of precision medicine is to personalize medical treatment by identifying individuals who are most likely to benefit from a specific treatment. The Likely Responder (LR) framework, which identifies a subpopulation where treatment response is expected to exceed a certain clinical threshold, plays a role in this effort. However, the LR framework, and more generally, data-driven subgroup analyses, often fail to account for uncertainty in the estimation of model-based data-driven subgrouping. We propose a simple two-stage approach that integrates subgroup identification with subsequent subgroup-specific inference on treatment effects. We incorporate model estimation uncertainty from the first stage into subgroup-specific treatment effect estimation in the second stage, by utilizing Bayesian posterior distributions from the first stage. We evaluate our method through simulations, demonstrating that the proposed Bayesian two-stage model produces better calibrated confidence intervals than naïve approaches. We apply our method to an international COVID-19 treatment trial, which shows substantial variation in treatment effects across data-driven subgroups.

</details>


### [32] [The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics](https://arxiv.org/abs/2512.18508)
*Barak Or*

Main category: stat.ME

TL;DR: 论文分析了卡尔曼滤波中验证门控对创新统计量的影响，证明门控操作会改变创新过程的统计特性，使其收敛到门控条件统计量而非名义统计量。


<details>
  <summary>Details</summary>
Motivation: 验证门控是经典卡尔曼跟踪系统的核心组件，但传统方法隐含地将无条件创新过程替换为受限于验证事件的条件观测过程。需要分析门控操作如何影响创新统计量的理论特性。

Method: 在线性-高斯假设下，推导椭圆门控条件下创新过程的一阶和二阶矩的精确表达式，分析门控引起的创新协方差收缩效应，并将分析扩展到最近邻关联方法。

Result: 门控会导致创新协方差发生确定性、维度依赖的收缩；最近邻关联作为额外的统计选择算子，会引入不可避免的能量收缩；在非平凡门控和关联下无法保持名义创新统计量。

Conclusion: 验证门控和关联操作会系统性地改变创新过程的统计特性，这一发现对跟踪系统设计和性能评估具有重要实际意义，特别是在二维情况下得到了量化结果。

Abstract: Validation gating is a fundamental component of classical Kalman-based tracking systems. Only measurements whose normalized innovation squared (NIS) falls below a prescribed threshold are considered for state update. While this procedure is statistically motivated by the chi-square distribution, it implicitly replaces the unconditional innovation process with a conditionally observed one, restricted to the validation event. This paper shows that innovation statistics computed after gating converge to gate-conditioned rather than nominal quantities. Under classical linear--Gaussian assumptions, we derive exact expressions for the first- and second-order moments of the innovation conditioned on ellipsoidal gating, and show that gating induces a deterministic, dimension-dependent contraction of the innovation covariance. The analysis is extended to NN association, which is shown to act as an additional statistical selection operator. We prove that selecting the minimum-norm innovation among multiple in-gate measurements introduces an unavoidable energy contraction, implying that nominal innovation statistics cannot be preserved under nontrivial gating and association. Closed-form results in the two-dimensional case quantify the combined effects and illustrate their practical significance.

</details>


### [33] [Non-stationary Spatial Modeling Using Fractional SPDEs](https://arxiv.org/abs/2512.18768)
*Elling Svee,Geir-Arne Fuglstad*

Main category: stat.ME

TL;DR: 提出一种结合分数平滑度和空间变化各向异性的高斯随机场模型，通过SPDE参数化实现空间变化的协方差结构，采用惩罚项防止过拟合，并开发高效梯度优化方法进行参数估计。


<details>
  <summary>Details</summary>
Motivation: 现有空间统计模型通常假设平稳性或各向同性，无法充分捕捉真实世界数据的复杂空间结构。需要开发能够同时建模分数平滑度和空间变化各向异性的灵活协方差模型。

Method: 通过随机偏微分方程（SPDE）定义高斯随机场，其中范围、边际方差和各向异性通过SPDE系数的谱参数化实现空间变化。构建先验防止过拟合，采用结合自动微分和稀疏矩阵运算的高效梯度优化方法进行参数估计。

Result: 模拟研究表明，在考虑的场景中，需要至少500个观测点才能可靠估计分数平滑度和非平稳性。惩罚项能有效防止不同观测位置数量下的过拟合。案例研究显示分数平滑度和非平稳性的相对重要性取决于具体应用。

Conclusion: 提出的方法不仅能用于预测，还可作为探索分数平滑度和非平稳性存在的工具。模型在海洋盐度和降水预测应用中分别展示了非平稳性和分数平滑度的重要性，表明灵活协方差结构对提升预测性能的关键作用。

Abstract: We construct a Gaussian random field (GRF) that combines fractional smoothness with spatially varying anisotropy. The GRF is defined through a stochastic partial differential equation (SPDE), where the range, marginal variance, and anisotropy vary spatially according to a spectral parametrization of the SPDE coefficients. Priors are constructed to reduce overfitting in this flexible covariance model, and parameter estimation is done with an efficient gradient-based optimization approach that combines automatic differentiation with sparse matrix operations. In a simulation study, we investigate how many observations are required to reliably estimate fractional smoothness and non-stationarity, and find that one realization containing 500 observations or more is needed in the scenario considered. We also find that the proposed penalization prevents overfitting across varying numbers of observation locations. Two case studies demonstrate that the relative importance of fractional smoothness and non-stationarity is application dependent. Non-stationarity improves predictions in an application to ocean salinity, whereas fractional smoothness improves predictions in an application to precipitation. Predictive ability is assessed using mean squared error and the continuous ranked probability score. In addition to prediction, the proposed approach can be used as a tool to explore the presence of fractional smoothness and non-stationarity.

</details>


### [34] [Effect measures for comparing paired event times](https://arxiv.org/abs/2512.18860)
*Merle Munko,Simon Mack,Marc Ditzhaus,Stefan Fröhling,Dennis Dobler,Dominic Edelmann*

Main category: stat.ME

TL;DR: 提出新的统计方法来可靠地评估个性化肿瘤试验中的无进展生存比，解决了现有方法无法控制I类错误率的问题。


<details>
  <summary>Details</summary>
Motivation: 个性化肿瘤试验中广泛使用的无进展生存比（PFSr）评估方法存在严重缺陷，大多数基于PFSr的检验即使在随机右删失等温和假设下也无法控制名义I类错误率，导致结果不可靠。

Method: 1) 采用最近开发的配对事件时间相对治疗效果方法来估计PFSr相关概率；2) 基于限制平均生存时间的差异和比率开发新的推断程序。

Result: 广泛的模拟研究证实，新方法能提供可靠的推断，而先前提出的技术在多种现实场景中会失效。通过分子辅助肿瘤试验的真实数据分析进一步证明了新方法的实用性。

Conclusion: 提出的新方法解决了PFSr推断中的可靠性问题，为个性化肿瘤试验提供了更稳健的统计工具，确保治疗效果的评估更加准确可靠。

Abstract: The progression-free survival ratio (PFSr) is a widely used measure in personalized oncology trials. It evaluates the effectiveness of treatment by comparing two consecutive event times - one under standard therapy and one under an experimental treatment. However, most proposed tests based on the PFSr cannot control the nominal type I error rate, even under mild assumptions such as random right-censoring. Consequently the results of these tests are often unreliable.
  As a remedy, we propose to estimate the relevant probabilities related to the PFSr by adapting recently developed methodology for the relative treatment effect between paired event times. As an additional alternative, we develop inference procedures based on differences and ratios of restricted mean survival times.
  An extensive simulation study confirms that the proposed novel methodology provides reliable inference, whereas previously proposed techniques break down in many realistic settings. The utility of our methods is further illustrated through an analysis of real data from a molecularly aided tumor trial.

</details>


### [35] [Integrating Prioritized and Non-Prioritized Structures in Win Statistics](https://arxiv.org/abs/2512.18946)
*Yunhan Mou,Scott Hummel,Yuan Huang*

Main category: stat.ME

TL;DR: 提出Rotation Win Ratio (RWR)方法，解决传统Win Statistics中端点优先级排序的挑战，允许等优先级端点块，适用于重要性相近的临床终点、复发事件和个体化决策场景。


<details>
  <summary>Details</summary>
Motivation: 传统Win Statistics分析需要严格的端点层次排序，但确定明确的优先级顺序具有挑战性，且无法充分反映端点重要性相近的情况。需要一种能同时处理优先化和非优先化结构的混合框架。

Method: 提出Rotation Win Ratio (RWR)混合优先级框架，允许设置等优先级端点块。基于U统计量理论开发统计推断方法，包括假设检验和置信区间构建。扩展到Rotation Net Benefit和Rotation Win Odds两种WS度量。

Result: 通过包含复发事件在内的多时间-事件端点的广泛模拟研究，证明RWR能有效控制I类错误，获得理想的统计功效和准确的置信区间覆盖。在SPRINT临床试验案例中展示了方法学和应用价值。

Conclusion: RWR框架为心血管临床试验中的端点优先级问题提供了灵活解决方案，既能保持临床相关性，又能处理端点重要性相近的情况，对真实世界临床研究具有重要应用价值。

Abstract: Composite endpoints are frequently used as primary or secondary analyses in cardiovascular clinical trials to increase clinical relevance and statistical efficiency. Alternatively, the Win Ratio (WR) and other Win Statistics (WS) analyses rely on a strict hierarchical ordering of endpoints, assigning higher priority to clinically important endpoints. However, determining a definitive endpoint hierarchy can be challenging and may not adequately reflect situations where endpoints have comparable importance. In this study, we discuss the challenges of endpoint prioritization, underscore its critical role in WS analyses, and propose Rotation WR (RWR), a hybrid prioritization framework that integrates both prioritized and non-prioritized structures. By permitting blocks of equally-prioritized endpoints, RWR accommodates endpoints of equal or near equal clinical importance, recurrent events, and contexts requiring individualized shared decision making. Statistical inference for RWR is developed using U-statistics theory, including the hypothesis testing procedure and confidence interval construction. Extensions to two additional WS measures, Rotation Net Benefit and Rotation Win Odds, are also provided. Through extensive simulation studies involving multiple time-to-event endpoints, including recurrent events, we demonstrate that RWR achieves valid type I error control, desirable statistical power, and accurate confidence interval coverage. We illustrate both the methodological and practical insights of our work in a case study on endpoint prioritization with the SPRINT clinical trial, highlighting its implications for real-world clinical trial studies.

</details>


### [36] [Smoothed Quantile Estimation: A Unified Framework Interpolating to the Mean](https://arxiv.org/abs/2512.19187)
*Saïd Maanan,Azzouz Dermoune,Ahmed El Ghini*

Main category: stat.ME

TL;DR: 本文提出了三种基于平滑L1损失的估计器族，在经典分位数和样本均值之间连续插值，建立了统一的M-估计框架，分析了不同分布下的效率特性。


<details>
  <summary>Details</summary>
Motivation: 传统分位数估计使用L1损失，而均值估计使用L2损失，两者在统计推断中各有优劣。本文旨在构建一个统一的框架，通过平滑参数在分位数和均值估计之间连续过渡，从而系统研究平滑对估计效率的影响。

Method: 基于平滑L1损失函数，引入位置参数z和平滑参数h，构建统一的M-估计框架。根据(z,h)的设定方式，生成三类估计器：固定参数平滑分位数估计器、固定分位数的插件估计器、以及新的均值估计连续族。使用一致渐近等连续性论证建立一致性和渐近正态性。

Result: 推导了渐近方差的闭式表达式，允许透明比较不同族和平滑水平的效率。参数空间的几何分解表明，对于固定分位数水平τ，可容许的(z,h)对位于直线上。理论分析揭示两种效率机制：在轻尾分布下平滑单调降低方差；在重尾分布下存在最优平滑参数h*(τ)>0能严格提高分位数估计效率。

Conclusion: 平滑L1损失框架统一了分位数和均值估计，揭示了平滑对估计效率的分布依赖性。数值实验验证了理论结论，并表明均值估计族在渐近和有限样本下都不优于样本均值。

Abstract: This paper develops and analyzes three families of estimators that continuously interpolate between classical quantiles and the sample mean. The construction begins with a smoothed version of the $L_{1}$ loss, indexed by a location parameter $z$ and a smoothing parameter $h \ge 0$, whose minimizer $\hat q(z,h)$ yields a unified M-estimation framework. Depending on how $(z, h)$ is specified, this framework generates three distinct classes of estimators: fixed-parameter smoothed quantile estimators, plug-in estimators of fixed quantiles, and a new continuum of mean-estimating procedures. For all three families we establish consistency and asymptotic normality via a uniform asymptotic equicontinuity argument. The limiting variances admit closed forms, allowing a transparent comparison of efficiency across families and smoothing levels. A geometric decomposition of the parameter space shows that, for fixed quantile level $τ$, admissible pairs $(z, h)$ lie on straight lines along which the estimator targets the same population quantile while its asymptotic variance evolves. The theoretical analysis reveals two efficiency regimes. Under light-tailed distributions (e.g., Gaussian), smoothing yields a monotone variance reduction. Under heavy-tailed distributions (e.g., Laplace), a finite smoothing parameter $h^{*}(τ) > 0$ strictly improves efficiency for quantile estimation. Numerical experiments -- based on simulated data and real financial returns -- validate these conclusions and show that, both asymptotically and in finite samples, the mean-estimating family does not improve upon the sample mean.

</details>


### [37] [Scale-Invariant Robust Estimation of High-Dimensional Kronecker-Structured Matrices](https://arxiv.org/abs/2512.19273)
*Xiaoyu Zhang,Zhiyun Fan,Wenyang Zhang,Di Wang*

Main category: stat.ME

TL;DR: 提出SRGD和SHT方法解决高维Kronecker结构估计中的尺度模糊性和统计鲁棒性冲突，通过梯度去尺度化和硬阈值实现鲁棒优化和变量选择


<details>
  <summary>Details</summary>
Motivation: 高维Kronecker结构估计面临非凸尺度模糊性与统计鲁棒性的冲突，任意因子尺度会扭曲梯度幅度，使标准固定阈值鲁棒方法失效

Method: 提出Scaled Robust Gradient Descent (SRGD)在截断前对梯度进行去尺度化稳定优化，引入Scaled Hard Thresholding (SHT)实现不变变量选择，构建基于鲁棒初始化和SRGD-SHT迭代更新的两步估计程序

Result: 建立了重尾预测变量和噪声下的收敛速率，识别了相变现象：有限噪声方差下恢复最优收敛速率，重尾情况下退化最优。模拟数据和两个实际应用验证了方法的优越鲁棒性和效率

Conclusion: 提出的SRGD-SHT方法有效解决了高维Kronecker结构估计中的尺度模糊性和鲁棒性问题，在重尾噪声下仍能保持良好性能

Abstract: High-dimensional Kronecker-structured estimation faces a conflict between non-convex scaling ambiguities and statistical robustness. The arbitrary factor scaling distorts gradient magnitudes, rendering standard fixed-threshold robust methods ineffective. We resolve this via Scaled Robust Gradient Descent (SRGD), which stabilizes optimization by de-scaling gradients before truncation. To further enforce interpretability, we introduce Scaled Hard Thresholding (SHT) for invariant variable selection. A two-step estimation procedure, built upon robust initialization and SRGD--SHT iterative updates, is proposed for canonical matrix problems, such as trace regression, matrix GLMs, and bilinear models. The convergence rates are established for heavy-tailed predictors and noise, identifying a phase transition where optimal convergence rates recover under finite noise variance and degrade optimally for heavier tails. Experiments on simulated data and two real-world applications confirm superior robustness and efficiency of the proposed procedure.

</details>


### [38] [High dimensional matrix estimation through elliptical factor models](https://arxiv.org/abs/2512.19325)
*Xinyue Xu,Huifang Ma,Hongfei Wang,Long Feng*

Main category: stat.ME

TL;DR: 本文提出了一种基于Tyler's M估计的POET-TME方法，用于椭圆因子模型中的高维协方差矩阵和精度矩阵估计，特别适用于重尾分布场景。


<details>
  <summary>Details</summary>
Motivation: 椭圆因子模型在高维数据分析中很重要，能捕捉重尾和异质依赖结构。Tyler's M估计器具有最优性和鲁棒性优势，但需要适应高维场景。

Method: 1) 将空间符号协方差矩阵作为初始估计器，适配POET框架；2) 提出专门针对Tyler's M估计的POET-TME方法；3) 在椭圆因子模型下建立估计器的一致性速率理论。

Result: 建立了POET-TME估计器的一致性理论，模拟研究和实际数据应用表明该方法在重尾分布下表现优异，验证了方法的实用价值。

Conclusion: POET-TME方法结合了Tyler's M估计的鲁棒性和POET框架的优势，为椭圆因子模型下的高维协方差矩阵和精度矩阵估计提供了有效解决方案，特别适用于重尾数据场景。

Abstract: Elliptical factor models play a central role in modern high-dimensional data analysis, particularly due to their ability to capture heavy-tailed and heterogeneous dependence structures. Within this framework, Tyler's M-estimator (Tyler, 1987a) enjoys several optimality properties and robustness advantages. In this paper, we develop high-dimensional scatter matrix, covariance matrix and precision matrix estimators grounded in Tyler's M-estimation. We first adapt the Principal Orthogonal complEment Thresholding (POET) framework (Fan et al., 2013) by incorporating the spatial-sign covariance matrix as an effective initial estimator. Building on this idea, we further propose a direct extension of POET tailored for Tyler's M-estimation, referred to as the POET-TME method. We establish the consistency rates for the resulting estimators under elliptical factor models. Comprehensive simulation studies and a real data application illustrate the superior performance of POET-TME, especially in the presence of heavy-tailed distributions, demonstrating the practical value of our methodological contributions.

</details>


### [39] [A Statistical Framework for Understanding Causal Effects that Vary by Treatment Initiation Time in EHR-based Studies](https://arxiv.org/abs/2512.19553)
*Luke Benz,Rajarshi Mukherjee,Rui Wang,David Arterburn,Heidi Fischer,Catherine Lee,Susan M. Shortreed,Alexander W. Levis,Sebastien Haneuse*

Main category: stat.ME

TL;DR: 提出一个统计框架来估计基于电子健康记录研究中日历时间特定的平均治疗效果，并量化协变量偏移在解释观察到的效应变化中的作用。


<details>
  <summary>Details</summary>
Motivation: 在基于电子健康记录的比较效果研究中，真实世界的治疗会随时间演变，同时患者人群也在变化，这使得评估治疗效果本身是否随时间变化变得复杂。需要一种方法来区分治疗效果的变化与接受治疗的患者人群变化。

Method: 提出一个统计框架，将双重稳健的时间特定治疗效果估计投影到候选的边际结构模型上，使用模型选择程序来描述效果如何随治疗开始时间变化。引入基于标准化分析的新颖汇总指标来量化协变量偏移在解释观察到的效应变化中的作用。

Result: 通过使用凯撒医疗集团电子健康记录数据的广泛模拟验证了该框架的实用性，并将其应用于研究2005-2011年间严重肥胖患者接受两种减重手术与不手术相比的相对减重效果变化。

Conclusion: 该框架能够估计日历时间特定的平均治疗效果，描述效果如何随时间变化，并区分治疗效果的变化与患者人群变化，为基于电子健康记录的比较效果研究提供了重要工具。

Abstract: Comparative effectiveness studies using electronic health records (EHR) consider data from patients who could ``enter'' the study cohort at any point during an interval that spans many years in calendar time. Unlike treatments in tightly controlled trials, real-world treatments can evolve over calendar time, especially if comparators include standard of care, or procedures where techniques may improve. Efforts to assess whether treatment efficacy itself is changing are complicated by changing patient populations, with potential covariate shift in key effect modifiers. In this work, we propose a statistical framework to estimate calendar-time specific average treatment effects and describe both how and why effects vary across treatment initiation time in EHR-based studies. Our approach projects doubly robust, time-specific treatment effect estimates onto candidate marginal structural models and uses a model selection procedure to best describe how effects vary by treatment initiation time. We further introduce a novel summary metric, based on standardization analysis, to quantify the role of covariate shift in explaining observed effect changes and disentangle changes in treatment effects from changes in the patient population receiving treatment. Extensive simulations using EHR data from Kaiser Permanente are used to validate the utility of the framework, which we apply to study changes in relative weight loss following two bariatric surgical interventions versus no surgery among patients with severe obesity between 2005-2011.

</details>


### [40] [Possibilistic Inferential Models for Post-Selection Inference in High-Dimensional Linear Regression](https://arxiv.org/abs/2512.19588)
*Yaohui Lin*

Main category: stat.ME

TL;DR: 提出RSPIM方法，通过正则化分割可能性构造，结合高维选择器和样本分割，为高维线性回归中的模型选择后推断提供有效的可能性推断模型。


<details>
  <summary>Details</summary>
Motivation: 高维线性回归中模型选择后的不确定性量化仍然具有挑战性，特别是在可能性推断模型框架下。需要开发能够在模型选择后提供有效推断的方法。

Method: 提出正则化分割可能性构造方法：1）使用第一个子样本选择稀疏模型；2）在独立的推断子样本上使用普通最小二乘法重新拟合，得到经典的t/F枢轴统计量；3）将这些枢轴统计量转化为一致的似然轮廓。还开发了正交化和基于bootstrap的扩展，以及最大多分割聚合方法。

Result: 在高斯线性模型中，该方法产生具有精确有限样本强有效性的坐标区间，条件于分割和选择的模型。模拟和核黄素基因表达示例显示，校准的RSPIM区间在Gaussian和异方差误差下表现良好，与最先进的模型选择后方法具有竞争力。

Conclusion: RSPIM为高维线性回归中的模型选择后推断提供了有效的可能性推断框架，具有强有效性保证，同时似然轮廓为模型选择后不确定性提供了透明的诊断工具。

Abstract: Valid uncertainty quantification after model selection remains challenging in high-dimensional linear regression, especially within the possibilistic inferential model (PIM) framework. We develop possibilistic inferential models for post-selection inference based on a regularized split possibilistic construction (RSPIM) that combines generic high-dimensional selectors with PIM validification through sample splitting. A first subsample is used to select a sparse model; ordinary least-squares refits on an independent inference subsample yield classical t/F pivots, which are then turned into consonant plausibility contours. In Gaussian linear models this leads to coor-dinatewise intervals with exact finite-sample strong validity conditional on the split and selected model, uniformly over all selectors that use only the selection data. We further analyze RSPIM in a sparse p >> n regime under high-level screening conditions, develop orthogonalized and bootstrap-based extensions for low-dimensional targets with high-dimensional nuisance, and study a maxitive multi-split aggregation that stabilizes inference across random splits while preserving strong validity. Simulations and a riboflavin gene-expression example show that calibrated RSPIM intervals are well behaved under both Gaussian and heteroskedastic errors and are competitive with state-of-the-art post-selection methods, while plausibility contours provide transparent diagnostics of post-selection uncertainty.

</details>


### [41] [Testing for Conditional Independence in Binary Single-Index Models](https://arxiv.org/abs/2512.19641)
*John H. J. Einmahl,Denis Kojevnikov,Bas J. M. Werker*

Main category: stat.ME

TL;DR: 提出一种检验方法，用于判断在已有变量X的情况下，变量Z是否对二元变量Y有额外的解释能力，通过构造两样本经验过程并研究其渐近性质来实现分布无关的检验。


<details>
  <summary>Details</summary>
Motivation: 在回归分析中，经常需要检验某个变量是否对响应变量有额外的解释能力。当X是高维变量时，直接检验条件独立性会面临维度灾难问题。本文旨在开发一种避免维度灾难的检验方法，用于检验在给定X的情况下，Z是否对二元变量Y有额外的预测能力。

Method: 1. 假设Y和Z对X的依赖都通过单指标X⊤β；2. 根据Y值将样本分成两组；3. 将X空间划分为平行条带；4. 构造变换后Z变量的两样本经验过程；5. 研究该经验过程的渐近性质，发现经过适当标准化后收敛到标准布朗桥；6. 基于此构造分布无关的检验。

Result: 成功构造了一个两样本经验过程，虽然该过程本身不弱收敛到标准布朗桥，但经过适当标准化后确实收敛到标准布朗桥。基于这一理论结果，开发出了分布无关的检验方法。

Conclusion: 提出了一种有效的方法来检验在给定高维变量X的情况下，变量Z是否对二元响应变量Y有额外的解释能力。该方法通过单指标假设避免了维度灾难，并基于经验过程的渐近理论构造了分布无关的检验。

Abstract: We wish to test whether a real-valued variable $Z$ has explanatory power, in addition to a multivariate variable $X$, for a binary variable $Y$. Thus, we are interested in testing the hypothesis $\mathbb{P}(Y=1\, | \, X,Z)=\mathbb{P}(Y=1\, | \, X)$, based on $n$ i.i.d.\ copies of $(X,Y,Z)$. In order to avoid the curse of dimensionality, we follow the common approach of assuming that the dependence of both $Y$ and $Z$ on $X$ is through a single-index $X^\topβ$ only. Splitting the sample on both $Y$-values, we construct a two-sample empirical process of transformed $Z$-variables, after splitting the $X$-space into parallel strips. Studying this two-sample empirical process is challenging: it does not converge weakly to a standard Brownian bridge, but after an appropriate normalization it does. We use this result to construct distribution-free tests.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [42] [Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler](https://arxiv.org/abs/2512.17977)
*Holden Lee,Matheau Santana-Gijzen*

Main category: stat.ML

TL;DR: 提出Re-ALPS算法，通过重加权和分区函数估计改进ALPS，无需高斯近似假设，在多模态分布采样中实现多项式时间混合


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法在多模态分布采样中面临指数级混合时间问题，需要利用额外信息（如各模态的暖启动点）来实现跨模态快速混合

Method: 改进ALPS算法，通过重加权消除高斯近似假设，使用蒙特卡洛估计分区函数，在最冷层级通过暖启动点间的传送实现跨模态混合

Result: 在一般设置下证明了多项式时间混合界，数值实验显示在重尾分布混合上比ALPS有更好的混合性能

Conclusion: Re-ALPS算法无需Hessian信息，能处理复杂几何形状的目标分布，为多模态采样提供了更通用的多项式时间解决方案

Abstract: Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. In light of hardness results for sampling -- classical MCMC methods, even with tempering, can suffer from exponential mixing times -- a natural question is how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes. To address this, we introduce Reweighted ALPS (Re-ALPS), a modified version of the Annealed Leap-Point Sampler (ALPS) that dispenses with the Gaussian approximation assumption. We prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. Similarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and at the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is crucial in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance compared to ALPS on a mixture of heavy-tailed distributions.

</details>


### [43] [Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty](https://arxiv.org/abs/2512.18083)
*Ashley Zhang*

Main category: stat.ML

TL;DR: 该论文将因果推断方法重新框架为机器学习中的域适应问题，提出联合稳健估计器(JRE)，通过联合优化倾向得分和结果模型来减少模型误设的影响。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断方法（如IPWRA）通常基于缺失数据填补和识别理论，作者希望从机器学习角度统一这些方法，将ATE估计重新框架为分布偏移下的域适应问题，并改进现有双重稳健估计器的优化目标。

Method: 提出联合稳健估计器(JRE)：1) 将ATE估计重新框架为域适应问题；2) 定义真正的"ATE风险函数"，要求处理组和对照组模型的偏差结构性地抵消；3) 使用基于bootstrap的倾向得分不确定性量化来联合训练结果模型；4) 在倾向得分分布上优化期望ATE风险。

Result: 模拟研究表明，在有限样本且结果模型误设的情况下，JRE相比标准IPWRA能够实现高达15%的均方误差(MSE)减少。

Conclusion: 从机器学习视角统一因果推断方法提供了新的理论洞察，提出的JRE通过联合优化倾向得分和结果模型，在模型误设情况下展现出更好的稳健性和性能提升。

Abstract: Standard approaches to causal inference, such as Outcome Regression and Inverse Probability Weighted Regression Adjustment (IPWRA), are typically derived through the lens of missing data imputation and identification theory. In this work, we unify these methods from a Machine Learning perspective, reframing ATE estimation as a \textit{domain adaptation problem under distribution shift}. We demonstrate that the canonical Hajek estimator is a special case of IPWRA restricted to a constant hypothesis class, and that IPWRA itself is fundamentally Importance-Weighted Empirical Risk Minimization designed to correct for the covariate shift between the treated sub-population and the target population.
  Leveraging this unified framework, we critically examine the optimization objectives of Doubly Robust estimators. We argue that standard methods enforce \textit{sufficient but not necessary} conditions for consistency by requiring outcome models to be individually unbiased. We define the true "ATE Risk Function" and show that minimizing it requires only that the biases of the treated and control models structurally cancel out. Exploiting this insight, we propose the \textbf{Joint Robust Estimator (JRE)}. Instead of treating propensity estimation and outcome modeling as independent stages, JRE utilizes bootstrap-based uncertainty quantification of the propensity score to train outcome models jointly. By optimizing for the expected ATE risk over the distribution of propensity scores, JRE leverages model degrees of freedom to achieve robustness against propensity misspecification. Simulation studies demonstrate that JRE achieves up to a 15\% reduction in MSE compared to standard IPWRA in finite-sample regimes with misspecified outcome models.

</details>


### [44] [Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning](https://arxiv.org/abs/2512.18720)
*Feng Yu,MD Saifur Rahman Mazumder,Ying Su,Oscar Contreras Velasco*

Main category: stat.ML

TL;DR: 提出RAEUFS模型，通过深度自编码器学习非线性特征表示，提高对异常值的鲁棒性，在无监督特征选择任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督特征选择方法存在两个关键限制：1）过度简化的线性映射无法捕捉复杂特征关系；2）假设均匀聚类分布，忽略了现实数据中普遍存在的异常值。

Method: 提出鲁棒自编码器无监督特征选择模型（RAEUFS），利用深度自编码器学习非线性特征表示，同时提高对异常值的鲁棒性，并开发了高效的优化算法。

Result: 在干净数据和异常值污染的数据设置下，RAEUFS方法在广泛实验中均优于最先进的无监督特征选择方法。

Conclusion: RAEUFS通过深度自编码器有效解决了现有无监督特征选择方法的局限性，在非线性特征学习和异常值鲁棒性方面表现出色。

Abstract: Effective feature selection is essential for high-dimensional data analysis and machine learning. Unsupervised feature selection (UFS) aims to simultaneously cluster data and identify the most discriminative features. Most existing UFS methods linearly project features into a pseudo-label space for clustering, but they suffer from two critical limitations: (1) an oversimplified linear mapping that fails to capture complex feature relationships, and (2) an assumption of uniform cluster distributions, ignoring outliers prevalent in real-world data. To address these issues, we propose the Robust Autoencoder-based Unsupervised Feature Selection (RAEUFS) model, which leverages a deep autoencoder to learn nonlinear feature representations while inherently improving robustness to outliers. We further develop an efficient optimization algorithm for RAEUFS. Extensive experiments demonstrate that our method outperforms state-of-the-art UFS approaches in both clean and outlier-contaminated data settings.

</details>


### [45] [On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction](https://arxiv.org/abs/2512.18971)
*Shuntuo Xu,Zhou Yu,Jian Huang*

Main category: stat.ML

TL;DR: 提出GenSDR方法，利用生成模型解决非线性充分降维中低维充分结构的识别问题，具有理论保证和广泛适用性


<details>
  <summary>Details</summary>
Motivation: 非线性充分降维中识别低维充分结构是一个基础但具有挑战性的问题，现有方法缺乏在总体和样本层面识别低维结构的理论完备性保证

Method: 提出生成充分降维方法，利用现代生成模型，通过集成技术扩展以处理非欧几里得响应，从而扩大应用范围

Result: GenSDR能够在总体和样本层面完全恢复中心σ场的信息，在样本层面建立了条件分布视角的一致性性质，数值结果显示了出色的经验性能

Conclusion: GenSDR通过生成模型解决了充分降维中的结构识别问题，具有理论保证和广泛的实际应用潜力

Abstract: Identifying low-dimensional sufficient structures in nonlinear sufficient dimension reduction (SDR) has long been a fundamental yet challenging problem. Most existing methods lack theoretical guarantees of exhaustiveness in identifying lower dimensional structures, either at the population level or at the sample level. We tackle this issue by proposing a new method, generative sufficient dimension reduction (GenSDR), which leverages modern generative models. We show that GenSDR is able to fully recover the information contained in the central $σ$-field at both the population and sample levels. In particular, at the sample level, we establish a consistency property for the GenSDR estimator from the perspective of conditional distributions, capitalizing on the distributional learning capabilities of deep generative models. Moreover, by incorporating an ensemble technique, we extend GenSDR to accommodate scenarios with non-Euclidean responses, thereby substantially broadening its applicability. Extensive numerical results demonstrate the outstanding empirical performance of GenSDR and highlight its strong potential for addressing a wide range of complex, real-world tasks.

</details>


### [46] [Cluster-Based Generalized Additive Models Informed by Random Fourier Features](https://arxiv.org/abs/2512.19373)
*Xin Huang,Jia Li,Jun Yu*

Main category: stat.ML

TL;DR: 提出一种基于随机傅里叶特征和混合广义可加模型的解释性机器学习方法，通过软聚类构建局部可解释模型，在保持透明度的同时提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒预测模型（如深度神经网络、核方法）虽然预测性能强但难以解释的问题，在预测准确性和模型透明度之间寻求平衡。

Method: 1) 学习随机傅里叶特征嵌入并进行主成分分析压缩；2) 使用高斯混合模型进行软聚类；3) 基于聚类结果构建混合广义可加模型框架，每个局部GAM通过可解释的单变量平滑函数捕捉非线性效应。

Result: 在真实世界回归基准测试（加州房价、NASA翼型自噪声、自行车共享数据集）中，相比经典可解释模型展现出改进的预测性能。

Conclusion: 该方法为将表示学习与透明统计建模相结合提供了原则性方法，实现了预测性能与模型可解释性的良好平衡。

Abstract: Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.

</details>
