<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 12]
- [stat.AP](#stat.AP) [Total: 5]
- [stat.ML](#stat.ML) [Total: 12]
- [stat.CO](#stat.CO) [Total: 1]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Introducing the b-value: combining unbiased and biased estimators from a sensitivity analysis perspective](https://arxiv.org/abs/2602.16310)
*Zhexiao Lin,Peter J. Bickel,Peng Ding*

Main category: stat.ME

TL;DR: 提出一种从敏感性分析角度结合无偏和有偏估计量的策略，引入b值概念，评估未知偏差下统计推断的稳健性。


<details>
  <summary>Details</summary>
Motivation: 在实证研究中，当存在多个同一参数的估计量时，如何结合无偏但精度较低与有偏但精度较高的估计量以改进推断？现有研究主要关注点估计，本文聚焦于较少研究的推断问题：在未知偏差情况下如何进行有效的统计推断。

Method: 提出从敏感性分析角度结合无偏和有偏估计量的策略，推导出以偏差幅度为索引的置信区间序列，引入b值概念（未知最大相对偏差的临界值）。将该策略应用于三种经典组合估计量：精度加权估计量、预检验估计量和软阈值估计量。

Result: 为每种估计量表征了置信区间序列，确定了结论改变的偏差阈值。理论分析表明，基于软阈值估计量的b值及其相关置信区间对未知偏差具有稳健性，且在备选方案中实现了最低的最坏情况风险。

Conclusion: 推荐报告基于软阈值估计量的b值及其相关置信区间，这种方法对未知偏差具有稳健性，在备选方案中实现了最低的最坏情况风险，为研究人员评估偏差水平对结论的影响提供了实用工具。

Abstract: In empirical research, when we have multiple estimators for the same parameter of interest, a central question arises: how do we combine unbiased but less precise estimators with biased but more precise ones to improve the inference? Under this setting, the point estimation problem has attracted considerable attention. In this paper, we focus on a less studied inference question: how can we conduct valid statistical inference in such settings with unknown bias? We propose a strategy to combine unbiased and biased estimators from a sensitivity analysis perspective. We derive a sequence of confidence intervals indexed by the magnitude of the bias, which enable researchers to assess how conclusions vary with the bias levels. Importantly, we introduce the notion of the b-value, a critical value of the unknown maximum relative bias at which combining estimators does not yield a significant result. We apply this strategy to three canonical combined estimators: the precision-weighted estimator, the pretest estimator, and the soft-thresholding estimator. For each estimator, we characterize the sequence of confidence intervals and determine the bias threshold at which the conclusion changes. Based on the theory, we recommend reporting the b-value based on the soft-thresholding estimator and its associated confidence intervals, which are robust to unknown bias and achieve the lowest worst-case risk among the alternatives.

</details>


### [2] [Generalised Linear Models Driven by Latent Processes: Asymptotic Theory and Applications](https://arxiv.org/abs/2602.16540)
*Wagner Barreto-Souza,Ngai Hang Chan*

Main category: stat.ME

TL;DR: 提出一种由潜过程驱动的广义线性模型，用于建模计数、实值、二元和正连续时间序列，扩展了现有框架，允许响应条件分布属于双参数指数族，潜过程以乘法方式进入条件均值。


<details>
  <summary>Details</summary>
Motivation: 扩展现有的基于泊松或单参数指数族的潜过程回归框架，使其能处理更广泛的数据类型（如正连续数据），允许估计未知的离散参数，并避免现有公式中对连接函数的限制性条件。

Method: 提出一类由潜过程驱动的广义线性模型，其中响应条件分布属于双参数指数族，潜过程以乘法方式进入条件均值。建立了忽略潜过程的GLM估计量的渐近正态性，推导了有效推断的正确信息矩阵，并提供了预测和预报的原则性方法。

Result: 该框架显著扩展了潜过程GLM的范围，自然地适应伽马响应处理正连续数据，通过矩方法估计未知离散参数，避免了对连接函数的限制性条件。在麻疹感染和古气候冰川纹泥两个实际数据应用中展示了实用优势和增强的灵活性。

Conclusion: 提出的由潜过程驱动的广义线性模型框架提供了更灵活和通用的建模方法，能够处理多种数据类型，建立了有效的统计推断理论，并填补了该领域预测和预报方法的空白。

Abstract: This paper introduces a class of generalised linear models (GLMs) driven by latent processes for modelling count, real-valued, binary, and positive continuous time series. Extending earlier latent-process regression frameworks based on Poisson or one-parameter exponential family assumptions, we allow the conditional distribution of the response to belong to a bi-parameter exponential family, with the latent process entering the conditional mean multiplicatively. This formulation substantially broadens the scope of latent-process GLMs, for instance, it naturally accommodates gamma responses for positive continuous data, enables estimation of an unknown dispersion parameter via method of moments, and avoids restrictive conditions on link functions that arise under existing formulations. We establish the asymptotic normality of the GLM estimators obtained from the GLM likelihood that ignores the latent process, and we derive the correct information matrix for valid inference. In addition, we provide a principled approach to prediction and forecasting in GLMs driven by latent processes, a topic not previously addressed in the literature. We present two real data applications on measles infections in North Rhine-Westphalia (Germany) and paleoclimatic glacial varves, which highlight the practical advantages and enhanced flexibility of the proposed modelling framework.

</details>


### [3] [Nonparametric Identification and Inference for Counterfactual Distributions with Confounding](https://arxiv.org/abs/2602.15916)
*Jianle Sun,Kun Zhang*

Main category: stat.ME

TL;DR: 该论文提出在混杂存在下联合潜在结果分布的非参数识别和半参数估计方法。通过条件copula推导协变量信息边界，并利用工具变量进行因果表示学习来应对未观测混杂。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，当存在观测或未观测混杂时，估计联合潜在结果分布具有挑战性。传统方法往往无法充分利用协变量信息或处理未观测混杂，需要更稳健的统计基础来进行分布性和反事实推断。

Method: 1. 对于观测混杂：利用条件copula推导协变量信息边界，提出直接估计器（多项式边界条件）和平滑近似（log-sum-exp算子）两种方法
2. 对于未观测混杂：引入因果表示学习框架，利用工具变量在单射性和完备性条件下证明潜在混杂子空间的非参数可识别性，开发"三重机器学习"估计器，采用交叉拟合处理学习表示、干扰参数和目标函数

Result: 建立了两种估计器的渐近性质，表征了表示学习误差引起的方差膨胀，提供了半参数效率的条件。提出了实用的基于VAE的混杂表示学习算法。模拟和实际数据分析验证了方法的有效性。

Conclusion: 该工作通过将经典半参数理论与现代表示学习相结合，为复杂因果系统中的分布性和反事实推断提供了稳健的统计基础，能够处理观测和未观测混杂情况下的联合潜在结果分布估计问题。

Abstract: We propose nonparametric identification and semiparametric estimation of joint potential outcome distributions in the presence of confounding. First, in settings with observed confounding, we derive tighter, covariate-informed bounds on the joint distribution by leveraging conditional copulas. To overcome the non-differentiability of bounding min/max operators, we establish the asymptotic properties for both a direct estimator with polynomial margin condition and a smooth approximation with log-sum-exp operator, facilitating valid inference for individual-level effects under the canonical rank-preserving assumption. Second, we tackle the challenge of unmeasured confounding by introducing a causal representation learning framework. By utilizing instrumental variables, we prove the nonparametric identifiability of the latent confounding subspace under injectivity and completeness conditions. We develop a ``triple machine learning" estimator that employs cross-fitting scheme to sequentially handle the learned representation, nuisance parameters, and target functional. We characterize the asymptotic distribution with variance inflation induced by representation learning error, and provide conditions for semiparametric efficiency. We also propose a practical VAE-based algorithm for confounding representation learning. Simulations and real-world analysis validate the effectiveness of proposed methods. By bridging classical semiparametric theory with modern representation learning, this work provides a robust statistical foundation for distributional and counterfactual inference in complex causal systems.

</details>


### [4] [Competing Risk Analysis in Cardiovascular Outcome Trials: A Simulation Comparison of Cox and Fine-Gray Models](https://arxiv.org/abs/2602.16031)
*Tuo Wang,Yu Du*

Main category: stat.ME

TL;DR: 在心血管结局试验中，当非心血管死亡作为竞争风险时，Cox比例风险模型与Fine-Gray亚分布风险模型在典型低竞争风险率(~1%/年)下结果几乎相同，仅在高竞争风险率且治疗效果方向不一致时出现显著差异，但两者均无法提供无偏估计。在典型试验设置中，Cox模型因其更好的可解释性仍适合作为主要分析方法。


<details>
  <summary>Details</summary>
Motivation: 心血管结局试验常面临竞争风险问题，非心血管死亡可能阻碍主要不良心血管事件的观察。传统Cox模型将竞争事件视为独立删失，而Fine-Gray模型则明确处理竞争风险，针对不同的估计目标。需要系统比较这两种方法在不同竞争风险场景下的表现，为临床试验设计提供指导。

Method: 使用双变量copula模型进行模拟研究，系统变化竞争事件发生率(0.5%-5%/年)、对竞争事件的治疗效果(50%减少到50%增加)和相关结构，比较Cox比例风险模型和Fine-Gray亚分布风险模型的表现。

Result: 在典型心血管试验的竞争事件发生率(~1%/年)下，无论相关强度或治疗效果方向如何，Cox和Fine-Gray模型产生几乎相同的风险比估计。仅在高竞争率且治疗效果方向不一致时出现显著差异，但两种估计量在这些条件下均无法提供真实的边际风险比的无偏估计。

Conclusion: 在典型低竞争风险率的心血管试验设置中，Cox模型因其更好的可解释性仍适合作为主要分析方法，不应为竞争风险方法而放弃预先指定的Cox模型。Fine-Gray模型不构成对Cox模型的适当敏感性分析，因为它们针对不同的估计目标而非检验假设。在高竞争风险场景下，应考虑逆概率删失加权、多重插补或将全因死亡率纳入主要终点等替代方法。

Abstract: Cardiovascular outcome trials commonly face competing risks when non-CV death prevents observation of major adverse cardiovascular events (MACE). While Cox proportional hazards models treat competing events as independent censoring, Fine-Gray subdistribution hazard models explicitly handle competing risks, targeting different estimands. This simulation study using bivariate copula models systematically varies competing event rates (0.5%-5% annually), treatment effects on competing events (50% reduction to 50% increase), and correlation structures to compare these approaches. At competing event rates typical of CV outcome trials (~1% annually), Cox and Fine-Gray produce nearly identical hazard ratio estimates regardless of correlation strength or treatment effect direction. Substantial divergence occurs only with high competing rates and directionally discordant treatment effects, though neither estimator provides unbiased estimates of true marginal hazard ratios under these conditions. In typical CV trial settings with low competing event rates, Cox models remain appropriate for primary analysis due to superior interpretability. Pre-specified Cox models should not be abandoned for competing risk methods. Importantly, Fine-Gray models do not constitute proper sensitivity analyses to Cox models per ICH E9(R1), as they target different estimands rather than testing assumptions. As supplementary analysis, cumulative incidence using Aalen-Johansen estimator can provide transparency about competing risk impact. Under high competing-risk scenarios, alternative approaches such as inverse probability of censoring weighting, multiple imputation, or inclusion of all-cause mortality in primary endpoints warrant consideration.

</details>


### [5] [Covariate Adjustment for Wilcoxon Two Sample Statistic and Test](https://arxiv.org/abs/2602.16040)
*Zhilan Lou,Jun Shao,Ting Ye,Tuo Wang,Yanyao Yi,Yu Du*

Main category: stat.ME

TL;DR: 将协变量调整应用于Wilcoxon两样本统计量和Wilcoxon-Mann-Whitney检验，通过校准提高效率并扩展其在协变量自适应随机化中的应用范围。


<details>
  <summary>Details</summary>
Motivation: 传统的Wilcoxon检验在协变量自适应随机化中应用受限，且未充分利用协变量信息来提高效率。研究旨在通过协变量调整扩展Wilcoxon检验的应用范围，同时提高估计和推断的效率。

Method: 通过校准方法对Wilcoxon两样本统计量和Wilcoxon-Mann-Whitney检验进行协变量调整，建立调整后统计量的渐近分布理论，并明确保证效率提升的条件。

Result: 调整后的Wilcoxon统计量对所有常用协变量自适应随机化方案具有不变性，可使用统一公式进行推断，同时明确保证了效率增益。

Conclusion: 协变量调整不仅提高了Wilcoxon检验的效率，还扩展了其在协变量自适应随机化中的应用，为实际临床试验提供了更灵活和高效的统计工具。

Abstract: We apply covariate adjustment to the Wincoxon two sample statistic and Wincoxon-Mann-Whitney test in comparing two treatments. The covariate adjustment through calibration not only improves efficiency in estimation/inference but also widens the application scope of the Wilcoxon two sample statistic and Wincoxon-Mann-Whitney test to situations where covariate-adaptive randomization is used. We motivate how to adjust covariates to reduce variance, establish the asymptotic distribution of adjusted Wincoxon two sample statistic, and provide explicitly the guaranteed efficiency gain. The asymptotic distribution of adjusted Wincoxon two sample statistic is invariant to all commonly used covariate-adaptive randomization schemes so that a unified formula can be used in inference regardless of which covariate-adaptive randomization is applied.

</details>


### [6] [Predictive Subsampling for Scalable Inference in Networks](https://arxiv.org/abs/2602.16041)
*Arpan Kumar,Minh Tang,Srijan Sengupta*

Main category: stat.ME

TL;DR: 提出基于子采样的网络推断方法，通过随机选择节点子集构建子图进行推断，再利用子样本与其余节点的连接关系插值估计整个图，以降低大规模网络的计算负担。


<details>
  <summary>Details</summary>
Motivation: 现代网络数据规模常超出传统方法的存储和计算能力，使得统计推断（如估计和假设检验）难以进行。需要开发能降低计算负担的推断方法。

Method: 基于子采样的方法：随机选择小规模节点子集构建子图进行推断，然后利用子样本与网络中其他节点的连接关系进行插值，估计整个图的结构。该方法在广义随机点积图框架下开发。

Result: 建立了方法的一致性保证，并通过全面的模拟研究验证了方法的实际有效性。

Conclusion: 提出的子采样方法能够显著降低大规模网络推断的计算负担，同时保持统计严谨性，适用于生物学、物理学和社会科学等多个领域的网络数据分析。

Abstract: Network datasets appear across a wide range of scientific fields, including biology, physics, and the social sciences. To enable data-driven discoveries from these networks, statistical inference techniques like estimation and hypothesis testing are crucial. However, the size of modern networks often exceeds the storage and computational capacities of existing methods, making timely, statistically rigorous inference difficult. In this work, we introduce a subsampling-based approach aimed at reducing the computational burden associated with estimation and two-sample hypothesis testing. Our strategy involves selecting a small random subset of nodes from the network, conducting inference on the resulting subgraph, and then using interpolation based on the observed connections between the subsample and the rest of the nodes to estimate the entire graph. We develop the methodology under the generalized random dot product graph framework, which affords broad applicability and permits rigorous analysis. Within this setting, we establish consistency guarantees and corroborate the practical effectiveness of the approach through comprehensive simulation studies.

</details>


### [7] [Experimental Assortments for Choice Estimation and Nest Identification](https://arxiv.org/abs/2602.16137)
*Xintong Yu,Will Ma,Michael Zhao*

Main category: stat.ME

TL;DR: 提出一种结构化非自适应实验设计，仅需O(log n)个不同商品组合即可高效估计选择模型，并在嵌套Logit模型中实现巢的自动识别


<details>
  <summary>Details</summary>
Motivation: 传统选择模型估计需要大量商品组合实验，成本高昂。如何设计高效实验来收集数据以准确估计选择模型，特别是在商品间存在复杂替代模式的情况下

Method: 1. 提出结构化非自适应实验设计，仅需O(log n)个不同商品组合，每个组合重复提供；2. 针对嵌套Logit模型，开发新算法从数据中自动识别商品巢结构

Result: 1. 实验设计在多种选择模型和不同真实数据下均优于随机和其他启发式设计；2. 算法能保证在任意嵌套Logit真实模型下正确识别巢结构；3. 在Dream11平台部署，从7000万用户收集数据，识别出的巢结构比基于特征的先验聚类具有更好的样本外预测性能

Conclusion: 提出的结构化实验设计和巢识别算法为高效估计选择模型提供了有效解决方案，特别适用于存在复杂替代模式的商品组合优化问题，在实际商业应用中验证了其价值

Abstract: What assortments (subsets of items) should be offered, to collect data for estimating a choice model over $n$ total items? We propose a structured, non-adaptive experiment design requiring only $O(\log n)$ distinct assortments, each offered repeatedly, that consistently outperforms randomized and other heuristic designs across an extensive numerical benchmark that estimates multiple different choice models under a variety of (possibly mis-specified) ground truths.
  We then focus on Nested Logit choice models, which cluster items into "nests" of close substitutes. Whereas existing Nested Logit estimation procedures assume the nests to be known and fixed, we present a new algorithm to identify nests based on collected data, which when used in conjunction with our experiment design, guarantees correct identification of nests under any Nested Logit ground truth.
  Our experiment design was deployed to collect data from over 70 million users at Dream11, an Indian fantasy sports platform that offers different types of betting contests, with rich substitution patterns between them. We identify nests based on the collected data, which lead to better out-of-sample choice prediction than ex-ante clustering from contest features. Our identified nests are ex-post justifiable to Dream11 management.

</details>


### [8] [Uncertainty-Aware Neural Multivariate Geostatistics](https://arxiv.org/abs/2602.16146)
*Yeseul Jeon,Aaron Scheffler,Rajarshi Guhaniyogi*

Main category: stat.ME

TL;DR: 提出Deep Neural Coregionalization（DNC）框架，通过深度高斯过程先验建模多变量空间效应，利用变分推断将DGP训练转化为带权重衰减和MC dropout的DNN训练，实现可扩展的不确定性量化多变量地统计学。


<details>
  <summary>Details</summary>
Motivation: 传统多变量地统计学方法在处理非平稳性和复杂交叉依赖时存在局限性，且计算成本高，难以扩展到大规模数据集。需要一种既能灵活建模空间依赖关系，又能高效进行不确定性量化的可扩展框架。

Method: DNC通过空间变化的潜在因子和载荷矩阵建模多变量空间效应，对因子和载荷矩阵元素都赋予深度高斯过程先验。提出变分公式将DGP证据下界最大化转化为训练带权重衰减和MC dropout的深度神经网络，实现小批量随机优化而无需MCMC采样。

Result: 在模拟实验中，DNC在强非平稳性和复杂交叉依赖情况下优于现有空间因子模型，同时计算效率大幅提升。在环境案例研究中，DNC能捕捉空间变化的跨变量交互，生成可解释的多变量结果图，并将不确定性量化扩展到大规模数据集，运行时间减少数个数量级。

Conclusion: DNC提供了一个可扩展的多变量地统计学框架，结合了深度学习的灵活性和贝叶斯方法的不确定性量化能力，为大规模空间数据分析提供了高效且原则性的解决方案。

Abstract: We propose Deep Neural Coregionalization, a scalable framework for uncertainty-aware multivariate geostatistics. DNC models multivariate spatial effects through spatially varying latent factors and loadings, assigning deep Gaussian process (DGP) priors to both the factors and the entries of the loading matrix. This joint construction learns shared latent spatial structure together with response-specific, location-dependent mixing weights, enabling flexible nonlinear and space-dependent associations within and across variables. A key contribution is a variational formulation that makes the DGP to deep neural network (DNN) correspondence explicit: maximizing the DGP evidence lower bound (ELBO) is equivalent to training DNNs with weight decay and Monte Carlo (MC) dropout. This yields fast mini-batch stochastic optimization without Markov Chain Monte Carlo (MCMC), while providing principled uncertainty quantification through MC-dropout forward passes as approximate posterior draws, producing calibrated credible surfaces for prediction and spatial effect estimation. Across simulations, DNC is competitive with existing spatial factor models, particularly under strong nonstationarity and complex cross-dependence, while delivering substantial computational gains. In a multivariate environmental case study, DNC captures spatially varying cross-variable interactions, produces interpretable maps of multivariate outcomes, and scales uncertainty quantification to large datasets with orders-of-magnitude reductions in runtime.

</details>


### [9] [A general framework for modeling Gaussian process with qualitative and quantitative factors](https://arxiv.org/abs/2602.16328)
*Linsui Deng,C. F. Jeff Wu*

Main category: stat.ME

TL;DR: 提出一个通用框架，将标准核函数应用于连续潜变量，扩展了基于潜变量的高斯过程方法，用于处理同时包含定性和定量因子的计算机实验。


<details>
  <summary>Details</summary>
Motivation: 计算机实验中同时包含定性和定量因子的情况日益增多，需要有效的高斯过程建模方法。现有方法虽然有效，但需要专门的协方差函数，缺乏通用框架。

Method: 扩展基于潜变量的高斯过程方法，建立通用框架将标准核函数应用于连续潜变量，自然融入序数结构，使用贝叶斯信息准则和留一交叉验证进行模型选择和平均。

Result: 该方法为现有QQ因子GP模型提供了新的解释视角，在某些情况下引入了新的协方差结构，并在多个示例中表现出良好性能。

Conclusion: 提出的通用框架扩展了基于潜变量的GP方法，为处理定性和定量因子提供了更灵活和可解释的建模工具，能自然融入序数结构并实现有效的模型选择。

Abstract: Computer experiments involving both qualitative and quantitative (QQ) factors have attracted increasing attention. Gaussian process (GP) models have proven effective in this context by choosing specialized covariance functions for QQ factors. In this work, we extend the latent variable-based GP approach, which maps qualitative factors into a continuous latent space, by establishing a general framework to apply standard kernel functions to continuous latent variables. This approach provides a novel perspective for interpreting some existing GP models for QQ factors and introduces new covariance structures in some situations. The ordinal structure can be incorporated naturally and seamlessly in this framework. Furthermore, the Bayesian information criterion and leave-one-out cross-validation are employed for model selection and model averaging. The performance of the proposed method is comprehensively studied on several examples.

</details>


### [10] [Focused Relative Risk Information Criterion for Variable Selection in Linear Regression](https://arxiv.org/abs/2602.16463)
*Nils Lid Hjort*

Main category: stat.ME

TL;DR: 提出FRIC方法用于线性回归变量选择，基于相对均方误差风险和置信分布理论，可处理单个或多个关注参数，并推导出新的整体变量选择准则。


<details>
  <summary>Details</summary>
Motivation: 传统变量选择方法通常基于整体模型拟合，但实际应用中我们可能更关注特定个体或特定参数的预测精度。需要一种能够针对具体"关注点"（focus）进行变量选择的方法。

Method: 1. 计算子模型相对于全模型的相对均方误差风险；2. 使用置信分布理论准确评估这些相对风险；3. 构建FRIC评分、FRIC图和置信图；4. 扩展到多个关注参数，使用平均FRIC评分；5. 推导出新的整体变量选择准则。

Result: 开发了FRIC方法及其扩展AFRIC方法，能够针对特定预测目标进行变量选择，并在所有协变量同等重要时推导出新的整体变量选择准则，与Mallows准则建立了联系并提出了改进。

Conclusion: FRIC方法提供了一种新颖的、关注特定预测目标的变量选择框架，能够更精确地评估子模型性能，并在实际数据中展示了有效性，为变量选择提供了新的理论工具。

Abstract: This paper motivates and develops a novel and focused approach to variable selection in linear regression models. For estimating the regression mean $μ=\E\,(Y\midd x_0)$, for the covariate vector of a given individual, there is a list of competing estimators, say $\hattμ_S$ for each submodel $S$. Exact expressions are found for the relative mean squared error risks, when compared to the widest model available, say $\mse_S/\mse_\wide$. The theory of confidence distributions is used for accurate assessments of these relative risks. This leads to certain Focused Relative Risk Information Criterion scores, and associated FRIC plots and FRIC tables, as well as to Confidence plots to exhibit the confidence the data give in the submodels. The machinery is extended to handle many focus parameters at the same time, with appropriate averaged FRIC scores. The particular case where all available covariate vectors have equal importance yields a new overall criterion for variable selection, balancing complexity and fit in a natural fashion. A connection to the Mallows criterion is demonstrated, leading also to natural modifications of the latter. The FRIC and AFRIC strategies are illustrated for real data.

</details>


### [11] [Factor-Adjusted Multiple Testing for High-Dimensional Individual Mediation Effects](https://arxiv.org/abs/2602.16497)
*Chen Shi,Zhao Chen,Christina Dan Wang*

Main category: stat.ME

TL;DR: 提出FADMT框架，通过因子调整解决高维中介分析中依赖结构导致的推断偏差和FDR膨胀问题，实现个体中介效应的大规模推断


<details>
  <summary>Details</summary>
Motivation: 高维中介分析中，中介变量间的普遍依赖会使标准去偏推断失效，导致假发现率(FDR)显著膨胀，需要开发能处理复杂依赖结构的推断方法

Method: 提出因子调整去偏中介检验(FADMT)框架：假设中介模型误差具有近似因子结构，提取潜在公共因子，构建去相关的伪中介变量进行后续推断

Result: 建立了去偏估计量的渐近正态性，开发了具有理论FDR控制的多重检验程序，模拟显示在各种相关结构下具有优越的有限样本性能

Conclusion: FADMT框架能有效控制高维中介分析中的FDR，提高对观测研究中共享潜在变异驱动的虚假关联的稳健性，在TCGA-BRCA多组学数据和沪港通研究中展示了实用价值

Abstract: Identifying individual mediators is a central goal of high-dimensional mediation analysis, yet pervasive dependence among mediators can invalidate standard debiased inference and lead to substantial false discovery rate (FDR) inflation. We propose a Factor-Adjusted Debiased Mediation Testing (FADMT) framework that enables large-scale inference for individual mediation effects with FDR control under complex dependence structures. Our approach posits an approximate factor structure on the unobserved errors of the mediator model, extracts common latent factors, and constructs decorrelated pseudo-mediators for the subsequent inferential procedure. We establish the asymptotic normality of the debiased estimator and develop a multiple testing procedure with theoretical FDR control under mild high-dimensional conditions. By adjusting for latent factor induced dependence, FADMT also improves robustness to spurious associations driven by shared latent variation in observational studies. Extensive simulations demonstrate the superior finite-sample performance across a wide range of correlation structures. Applications to TCGA-BRCA multi-omics data and to China's stock connect study further illustrate the practical utility of the proposed method.

</details>


### [12] [Synthetic-Powered Multiple Testing with FDR Control](https://arxiv.org/abs/2602.16690)
*Yonghoon Lee,Meshi Bashari,Edgar Dobriban,Yaniv Romano*

Main category: stat.ME

TL;DR: SynthBH是一种利用合成数据增强多重假设检验的方法，在保证FDR控制的同时提升统计功效


<details>
  <summary>Details</summary>
Motivation: 在多重假设检验中，研究人员通常只能获得有限的真实实验数据，但可能拥有来自相关实验或生成模型的辅助合成数据。如何安全有效地利用这些合成数据来提升检验功效，同时保证FDR控制，是一个重要问题。

Method: 提出SynthBH方法，将真实观测数据和合成数据结合使用。该方法不需要合成数据在零假设下产生有效的p值，而是在温和的PRDS型正相关条件下，保证有限样本、分布无关的FDR控制。方法能够自适应合成数据的质量。

Result: SynthBH在表格异常检测基准测试和药物-癌症敏感性关联的基因组分析中表现出色。当合成数据质量高时，能显著提升样本效率和统计功效；即使合成数据质量差，仍能保证用户指定水平的FDR控制。

Conclusion: SynthBH为多重假设检验提供了一种安全利用合成数据的框架，在保证FDR控制的前提下，能够自适应合成数据质量并提升统计功效，具有广泛的应用前景。

Abstract: Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [13] [Evidence for Daily and Weekly Periodic Variability in GPT-4o Performance](https://arxiv.org/abs/2602.15889)
*Paul Tschisgale,Peter Wulff*

Main category: stat.AP

TL;DR: 研究发现GPT-4o在固定条件下（相同模型、超参数和提示）的性能会随时间周期性变化，挑战了LLM性能时间不变性的假设。


<details>
  <summary>Details</summary>
Motivation: 当前研究通常假设LLM在固定条件下的性能是时间不变的，如果平均输出质量随时间系统性变化，将威胁研究结果的可靠性、有效性和可重复性。

Method: 对GPT-4o进行纵向研究，使用固定模型快照、固定超参数和相同提示，每3小时通过API查询相同的多项选择物理任务，持续约3个月，每个时间点生成10个独立响应并计算平均分。

Result: 频谱分析显示模型平均性能存在显著的周期性变化，约占总方差的20%，特别是观察到的周期性模式可以用日节奏和周节奏的相互作用来解释。

Conclusion: 即使在受控条件下，LLM性能也可能随时间周期性变化，这质疑了时间不变性假设，对确保使用或研究LLM的研究的有效性和可重复性具有重要意义。

Abstract: Large language models (LLMs) are increasingly used in research both as tools and as objects of investigation. Much of this work implicitly assumes that LLM performance under fixed conditions (identical model snapshot, hyperparameters, and prompt) is time-invariant. If average output quality changes systematically over time, this assumption is violated, threatening the reliability, validity, and reproducibility of findings. To empirically examine this assumption, we conducted a longitudinal study on the temporal variability of GPT-4o's average performance. Using a fixed model snapshot, fixed hyperparameters, and identical prompting, GPT-4o was queried via the API to solve the same multiple-choice physics task every three hours for approximately three months. Ten independent responses were generated at each time point and their scores were averaged. Spectral (Fourier) analysis of the resulting time series revealed notable periodic variability in average model performance, accounting for approximately 20% of the total variance. In particular, the observed periodic patterns are well explained by the interaction of a daily and a weekly rhythm. These findings indicate that, even under controlled conditions, LLM performance may vary periodically over time, calling into question the assumption of time invariance. Implications for ensuring validity and replicability of research that uses or investigates LLMs are discussed.

</details>


### [14] [Surrogate-Based Prevalence Measurement for Large-Scale A/B Testing](https://arxiv.org/abs/2602.16111)
*Zehao Xu,Tony Paek,Kevin O'Sullivan,Attila Dobi*

Main category: stat.AP

TL;DR: 提出一個可擴展的代理基礎流行度測量框架，使用分數分桶作為代理信號，將昂貴的標註與實驗評估解耦，實現快速、低延遲的流行度測量。


<details>
  <summary>Details</summary>
Motivation: 線上媒體平台需要測量用戶接觸特定內容屬性的頻率來評估A/B實驗的權衡。直接方法（採樣內容並使用高質量標註）對於每個實驗組和細分群體來說成本太高、速度太慢，無法作為默認的大規模測量方法。

Method: 提出代理基礎流行度測量框架：1) 將模型分數離散化為多個桶；2) 從離線標註樣本估計桶級流行度；3) 結合校準後的桶級流行度與每個實驗組中印象的桶分佈，獲得基於日誌的快速估計。

Result: 在多個大規模A/B測試中驗證，代理估計值與參考估計值在組級流行度和處理-控制差異方面密切匹配。

Conclusion: 該框架實現了可擴展、低延遲的流行度測量，無需為每個實驗進行標註工作，使流行度測量成為實驗中的默認測量方法。

Abstract: Online media platforms often need to measure how frequently users are exposed to specific content attributes in order to evaluate trade-offs in A/B experiments. A direct approach is to sample content, label it using a high-quality rubric (e.g., an expert-reviewed LLM prompt), and estimate impression-weighted prevalence. However, repeatedly running such labeling for every experiment arm and segment is too costly and slow to serve as a default measurement at scale.
  We present a scalable \emph{surrogate-based prevalence measurement} framework that decouples expensive labeling from per-experiment evaluation. The framework calibrates a surrogate signal to reference labels offline and then uses only impression logs to estimate prevalence for arbitrary experiment arms and segments. We instantiate this framework using \emph{score bucketing} as the surrogate: we discretize a model score into buckets, estimate bucket-level prevalences from an offline labeled sample, and combine these calibrated bucket level prevalences with the bucket distribution of impressions in each arm to obtain fast, log-based estimates.
  Across multiple large-scale A/B tests, we validate that the surrogate estimates closely match the reference estimates for both arm-level prevalence and treatment--control deltas. This enables scalable, low-latency prevalence measurement in experimentation without requiring per-experiment labeling jobs.

</details>


### [15] [Phase Transitions in Collective Damage of Civil Structures under Natural Hazards](https://arxiv.org/abs/2602.16195)
*Sebin Oh,Jinyan Zhao,Raul Rincon,Jamie E. Padgett,Ziqi Wang*

Main category: stat.AP

TL;DR: 城市结构损伤在自然灾害中表现出相变现象，类似于统计物理中的一级相变，建筑多样性可平滑转变但多尺度损伤聚类会抑制可预测的无序相出现。


<details>
  <summary>Details</summary>
Motivation: 理解城市在自然灾害下的命运不仅取决于灾害强度，还取决于结构损伤的耦合过程，这一集体过程目前仍知之甚少。

Method: 使用随机场伊辛模型来描述城市结构损伤，将外部场、无序强度和温度分别解释为有效灾害需求、结构多样性和建模不确定性。

Result: 应用该框架到真实城市库存显示，广泛使用的工程建模实践可使城市损伤模式在同步和波动状态间转换，在中等地震下系统性偏差超过概率风险指标达50%，相当于修复成本数倍的差距。

Conclusion: 这种相感知描述将民用基础设施损伤的集体行为转化为城市风险评估和规划的可操作诊断工具。

Abstract: The fate of cities under natural hazards depends not only on hazard intensity but also on the coupling of structural damage, a collective process that remains poorly understood. Here we show that urban structural damage exhibits phase-transition phenomena. As hazard intensity increases, the system can shift abruptly from a largely safe to a largely damaged state, analogous to a first-order phase transition in statistical physics. Higher diversity in the building portfolio smooths this transition, but multiscale damage clustering traps the system in an extended critical-like regime (analogous to a Griffiths phase), suppressing the emergence of a more predictable disordered (Gaussian) phase. These phenomenological patterns are characterized by a random-field Ising model, with the external field, disorder strength, and temperature interpreted as the effective hazard demand, structural diversity, and modeling uncertainty, respectively. Applying this framework to real urban inventories reveals that widely used engineering modeling practices can shift urban damage patterns between synchronized and volatile regimes, systematically biasing exceedance-based risk metrics by up to 50% under moderate earthquakes ($M_w \approx 5.5$--$6.0$), equivalent to a several-fold gap in repair costs. This phase-aware description turns the collective behavior of civil infrastructure damage into actionable diagnostics for urban risk assessment and planning.

</details>


### [16] [Physical Activity Trajectories Preceding Incident Major Depressive Disorder Diagnosis Using Consumer Wearable Devices in the All of Us Research Program: Case-Control Study](https://arxiv.org/abs/2602.16583)
*Yuezhou Zhang,Amos Folarin,Hugh Logan Ellis,Rongrong Zhong,Callum Stewart,Heet Sankesara,Hyunju Kim,Shaoxiong Sun,Abhishek Pratap,Richard JB Dobson*

Main category: stat.AP

TL;DR: 使用可穿戴设备数据发现，在首次重度抑郁症诊断前数月，患者会出现持续的身体活动下降，步数在诊断前4个月开始显著减少，中高强度活动在诊断前5个月开始减少。


<details>
  <summary>Details</summary>
Motivation: 虽然低身体活动是重度抑郁症的已知风险因素，但在首次临床诊断前身体活动的变化模式尚不清楚，特别是缺乏长期客观测量数据。本研究旨在使用可穿戴设备数据描述重度抑郁症诊断前一年的身体活动轨迹。

Method: 采用回顾性巢式病例对照研究设计，使用All of Us研究项目的电子健康记录和Fitbit数据。纳入在诊断前一年至少有6个月有效可穿戴数据的成年人，将829例新发重度抑郁症患者与3,275名对照按年龄、性别、BMI和索引时间匹配。将每日步数和中高强度身体活动汇总为月平均值，使用线性混合效应模型比较诊断前12个月至诊断时的活动轨迹。

Result: 与对照组相比，病例组在整个诊断前一年表现出持续较低的身体活动水平，且步数和中高强度身体活动均呈现显著下降趋势。步数在诊断前约4个月开始显著下降，中高强度身体活动在诊断前约5个月开始显著下降。探索性分析显示存在亚组差异：男性下降更陡峭，年龄较大者强度减少更明显，肥胖个体活动水平持续较低。

Conclusion: 在重度抑郁症诊断前数月，个体内部会出现持续的身体活动下降。纵向可穿戴设备监测可能提供早期信号，支持风险分层和早期干预。

Abstract: Low physical activity is a known risk factor for major depressive disorder (MDD), but changes in activity before a first clinical diagnosis remain unclear, especially using long-term objective measurements. This study characterized trajectories of wearable-measured physical activity during the year preceding incident MDD diagnosis.
  We conducted a retrospective nested case-control study using linked electronic health record and Fitbit data from the All of Us Research Program. Adults with at least 6 months of valid wearable data in the year before diagnosis were eligible. Incident MDD cases were matched to controls on age, sex, body mass index, and index time (up to four controls per case). Daily step counts and moderate-to-vigorous physical activity (MVPA) were aggregated into monthly averages. Linear mixed-effects models compared trajectories from 12 months before diagnosis to diagnosis. Within cases, contrasts identified when activity first significantly deviated from levels 12 months prior.
  The cohort included 4,104 participants (829 cases and 3,275 controls; 81.7% women; median age 48.4 years). Compared with controls, cases showed consistently lower activity and significant downward trajectories in both step counts and MVPA during the year before diagnosis (P < 0.001). Significant declines appeared about 4 months before diagnosis for step counts and 5 months for MVPA. Exploratory analyses suggested subgroup differences, including steeper declines in men, greater intensity reductions at older ages, and persistently low activity among individuals with obesity.
  Sustained within-person declines in physical activity emerged months before incident MDD diagnosis. Longitudinal wearable monitoring may provide early signals to support risk stratification and earlier intervention.

</details>


### [17] [Design and Analysis Strategies for Pooling in High Throughput Screening: Application to the Search for a New Anti-Microbial](https://arxiv.org/abs/2602.16616)
*Byran Smucker,Benjamin Brennan,Emily Rego,Meng Wu,Zhihong Lin,Brian Ahmer,Blake Peterson*

Main category: stat.AP

TL;DR: 该论文研究抗生素耐药性问题，探讨了通过化合物池化策略提高高通量筛选效率的方法，为实践者提供方法选择指导。


<details>
  <summary>Details</summary>
Motivation: 细菌对抗生素的耐药性日益严重，需要开发新的抗菌策略。传统高通量筛选方法效率较低，需要更高效的化合物筛选策略来加速新药发现。

Method: 研究比较了多种近期提出的化合物池化构建方法以及不同的池化高通量筛选分析方法，包括在抗菌药物发现应用中的实际测试。

Result: 通过广泛的试点研究和一个小型筛选活动，展示了池化方法的成功案例和面临的挑战，为实践者提供了方法选择的具体指导。

Conclusion: 化合物池化策略是提高高通量筛选效率的有效方法，但需要根据具体应用场景选择适当的构建和分析方法，该方法在抗菌药物发现中具有重要应用价值。

Abstract: A major public health issue is the growing resistance of bacteria to antibiotics. An important part of the needed response is the discovery and development of new antimicrobial strategies. These require the screening of potential new drugs, typically accomplished using high-throughput screening (HTS). Traditionally, HTS is performed by examining one compound per well, but a more efficient strategy pools multiple compounds per well. In this work, we study several recently proposed pooling construction methods, as well as a variety of pooled high-throughput screening analysis methods, in order to provide guidance to practitioners on which methods to use. This is done in the context of an application of the methods to the search for new drugs to combat bacterial infection. We discuss both an extensive pilot study as well as a small screening campaign, and highlight both the successes and challenges of the pooling approach.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [18] [Generalized Leverage Score for Scalable Assessment of Privacy Vulnerability](https://arxiv.org/abs/2602.15919)
*Valentin Dorseuil,Jamal Atif,Olivier Cappé*

Main category: stat.ML

TL;DR: 论文提出了一种无需重新训练模型或模拟攻击即可评估数据点隐私漏洞的方法，通过建立成员推理攻击风险与数据点对模型影响之间的理论联系，并提出了计算高效的深度学习泛化杠杆分数。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据点隐私漏洞的方法通常需要重新训练模型或显式模拟攻击，计算成本高昂。本文旨在开发一种无需这些昂贵操作即可评估个体数据点成员推理攻击风险的方法。

Method: 在线性模型设置中，建立了成员推理攻击风险与杠杆分数之间的理论对应关系。基于此理论，提出了适用于深度学习的计算高效的杠杆分数泛化方法，作为评估个体隐私风险的实用替代指标。

Result: 实证评估显示，提出的分数与成员推理攻击成功率之间存在强相关性，验证了该指标作为个体隐私风险评估实用替代方法的有效性。

Conclusion: 研究表明，个体数据点的成员推理攻击风险本质上由其对该模型的影响决定，杠杆分数可作为评估隐私漏洞的原则性指标，为高效隐私风险评估提供了理论基础和实用工具。

Abstract: Can the privacy vulnerability of individual data points be assessed without retraining models or explicitly simulating attacks? We answer affirmatively by showing that exposure to membership inference attack (MIA) is fundamentally governed by a data point's influence on the learned model. We formalize this in the linear setting by establishing a theoretical correspondence between individual MIA risk and the leverage score, identifying it as a principled metric for vulnerability. This characterization explains how data-dependent sensitivity translates into exposure, without the computational burden of training shadow models. Building on this, we propose a computationally efficient generalization of the leverage score for deep learning. Empirical evaluations confirm a strong correlation between the proposed score and MIA success, validating this metric as a practical surrogate for individual privacy risk assessment.

</details>


### [19] [Including Node Textual Metadata in Laplacian-constrained Gaussian Graphical Models](https://arxiv.org/abs/2602.15920)
*Jianhua Wang,Killian Cressant,Pedro Braconnot Velloso,Arnaud Breloy*

Main category: stat.ML

TL;DR: 提出一种基于拉普拉斯约束高斯图模型的图学习方法，同时利用节点信号和元数据，通过MM算法求解，在金融数据集上显著提升图聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯图模型图学习方法通常忽略节点附带的元数据（如文本描述），而这些辅助信息可能包含有价值的图结构信息。需要开发能够同时利用节点信号和元数据的方法来提升图学习性能。

Method: 提出基于拉普拉斯约束高斯图模型的图学习方法，将节点信号和元数据融合到一个统一的优化框架中。开发了高效的majorization-minimization（MM）算法，每次迭代都有闭式更新。

Result: 在真实世界金融数据集上的实验结果表明，该方法相比仅使用信号或仅使用元数据的最先进方法，显著提高了图聚类性能。

Conclusion: 融合节点信号和元数据的高斯图模型图学习方法能够有效提升图聚类性能，证明了同时利用这两种信息源的价值。

Abstract: This paper addresses graph learning in Gaussian Graphical Models (GGMs). In this context, data matrices often come with auxiliary metadata (e.g., textual descriptions associated with each node) that is usually ignored in traditional graph estimation processes. To fill this gap, we propose a graph learning approach based on Laplacian-constrained GGMs that jointly leverages the node signals and such metadata. The resulting formulation yields an optimization problem, for which we develop an efficient majorization-minimization (MM) algorithm with closed-form updates at each iteration. Experimental results on a real-world financial dataset demonstrate that the proposed method significantly improves graph clustering performance compared to state-of-the-art approaches that use either signals or metadata alone, thus illustrating the interest of fusing both sources of information.

</details>


### [20] [Robust Stochastic Gradient Posterior Sampling with Lattice Based Discretisation](https://arxiv.org/abs/2602.15925)
*Zier Mensch,Lars Holdijk,Samuel Duffield,Maxwell Aifer,Patrick J. Coles,Max Welling,Miranda C. N. Cheng*

Main category: stat.ML

TL;DR: SGLRW是一种新的随机梯度MCMC方法，通过仅在更新协方差矩阵的非对角元素引入随机噪声，相比传统SGLD对minibatch大小更鲁棒，同时保持渐近正确性。


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度MCMC方法（如SGLD）对minibatch大小和梯度噪声敏感，这限制了其在实际大规模应用中的稳定性和性能。

Method: 提出随机梯度格点随机游走(SGLRW)，扩展了格点随机游走离散化方法。与SGLD不同，SGLRW仅在更新协方差矩阵的非对角元素引入随机噪声，从而获得更好的鲁棒性。

Result: 在贝叶斯回归和分类任务上的实验验证表明，SGLRW在SGLD失效的情况下仍保持稳定（包括存在重尾梯度噪声时），预测性能相当或更好。

Conclusion: SGLRW提供了一种更鲁棒的随机梯度MCMC方法，对minibatch大小不敏感，在保持渐近正确性的同时提高了实际应用的稳定性。

Abstract: Stochastic-gradient MCMC methods enable scalable Bayesian posterior sampling but often suffer from sensitivity to minibatch size and gradient noise. To address this, we propose Stochastic Gradient Lattice Random Walk (SGLRW), an extension of the Lattice Random Walk discretization. Unlike conventional Stochastic Gradient Langevin Dynamics (SGLD), SGLRW introduces stochastic noise only through the off-diagonal elements of the update covariance; this yields greater robustness to minibatch size while retaining asymptotic correctness. Furthermore, as comparison we analyze a natural analogue of SGLD utilizing gradient clipping. Experimental validation on Bayesian regression and classification demonstrates that SGLRW remains stable in regimes where SGLD fails, including in the presence of heavy-tailed gradient noise, and matches or improves predictive performance.

</details>


### [21] [Partial Identification under Missing Data Using Weak Shadow Variables from Pretrained Models](https://arxiv.org/abs/2602.16061)
*Hongyu Chen,David Simchi-Levi,Ruoxuan Xiong*

Main category: stat.ML

TL;DR: 提出一个部分识别框架，通过线性规划获得估计量的尖锐边界，利用预训练模型（如LLM）的预测作为弱影子变量约束，在MNAR数据下实现有效推断。


<details>
  <summary>Details</summary>
Motivation: 在平台评估和社会科学研究中，从用户反馈估计总体指标（如平均结果）至关重要，但反馈通常存在非随机缺失（MNAR）问题：意见更强烈的用户更可能回应，导致标准估计量有偏且无法识别。现有方法依赖强参数假设或特殊辅助变量，实践中可能不可用。

Method: 开发部分识别框架，通过求解一对线性规划获得估计量的尖锐边界，约束条件编码观测数据结构。将预训练模型（包括LLM）的预测作为弱影子变量纳入线性约束：这些预测满足关于缺失的条件独立性，但无需满足经典影子变量方法的完备性条件。提出集合扩展估计量，在集合识别机制下实现慢于√n的收敛速率，在点识别下保持标准√n速率。

Result: 在模拟和客户服务对话的半合成实验中，发现LLM预测虽然不适合经典影子变量方法，但在本框架中非常有效：将识别区间缩小75-83%，同时在现实MNAR机制下保持有效覆盖。当预测足够信息丰富时，边界可收缩为点，恢复标准识别。

Conclusion: 该框架为MNAR数据下的因果推断提供了实用解决方案，利用现代预测模型作为弱影子变量，在无需强参数假设或特殊辅助变量的情况下实现部分识别和有效推断，特别适用于LLM等预测模型的应用场景。

Abstract: Estimating population quantities such as mean outcomes from user feedback is fundamental to platform evaluation and social science, yet feedback is often missing not at random (MNAR): users with stronger opinions are more likely to respond, so standard estimators are biased and the estimand is not identified without additional assumptions. Existing approaches typically rely on strong parametric assumptions or bespoke auxiliary variables that may be unavailable in practice. In this paper, we develop a partial identification framework in which sharp bounds on the estimand are obtained by solving a pair of linear programs whose constraints encode the observed data structure. This formulation naturally incorporates outcome predictions from pretrained models, including large language models (LLMs), as additional linear constraints that tighten the feasible set. We call these predictions weak shadow variables: they satisfy a conditional independence assumption with respect to missingness but need not meet the completeness conditions required by classical shadow-variable methods. When predictions are sufficiently informative, the bounds collapse to a point, recovering standard identification as a special case. In finite samples, to provide valid coverage of the identified set, we propose a set-expansion estimator that achieves slower-than-$\sqrt{n}$ convergence rate in the set-identified regime and the standard $\sqrt{n}$ rate under point identification. In simulations and semi-synthetic experiments on customer-service dialogues, we find that LLM predictions are often ill-conditioned for classical shadow-variable methods yet remain highly effective in our framework. They shrink identification intervals by 75--83\% while maintaining valid coverage under realistic MNAR mechanisms.

</details>


### [22] [Empirical Cumulative Distribution Function Clustering for LLM-based Agent System Analysis](https://arxiv.org/abs/2602.16131)
*Chihiro Watanabe,Jingyu Sun*

Main category: stat.ML

TL;DR: 提出基于经验累积分布函数(ECDF)的评估框架，通过分析生成回答与参考答案的余弦相似度分布来更细致地评估LLM代理性能，超越传统的多数投票聚合方法。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法通过多数投票将多个LLM代理回答聚合为单一最终答案，这会掩盖原始回答的质量和分布特征，无法全面反映代理性能。

Method: 1) 基于生成回答与参考答案的余弦相似度构建ECDF；2) 使用ECDF距离和k-medoids算法对ECDF进行聚类分析；3) 在不同代理配置(温度、角色设定、问题主题)下进行实验。

Result: ECDF框架能够区分具有相似最终准确率但质量分布不同的代理设置；聚类分析揭示了回答中可解释的群体结构，展示了温度、角色设定和问题主题对回答质量分布的影响。

Conclusion: 提出的ECDF评估框架提供了比传统聚合方法更细致的LLM代理性能评估，能够揭示回答质量的分布特征，为理解和优化代理配置提供了新视角。

Abstract: Large language models (LLMs) are increasingly used as agents to solve complex tasks such as question answering (QA), scientific debate, and software development. A standard evaluation procedure aggregates multiple responses from LLM agents into a single final answer, often via majority voting, and compares it against reference answers. However, this process can obscure the quality and distributional characteristics of the original responses. In this paper, we propose a novel evaluation framework based on the empirical cumulative distribution function (ECDF) of cosine similarities between generated responses and reference answers. This enables a more nuanced assessment of response quality beyond exact match metrics. To analyze the response distributions across different agent configurations, we further introduce a clustering method for ECDFs using their distances and the $k$-medoids algorithm. Our experiments on a QA dataset demonstrate that ECDFs can distinguish between agent settings with similar final accuracies but different quality distributions. The clustering analysis also reveals interpretable group structures in the responses, offering insights into the impact of temperature, persona, and question topics.

</details>


### [23] [Conjugate Learning Theory: Uncovering the Mechanisms of Trainability and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.16177)
*Binchuan Qi*

Main category: stat.ML

TL;DR: 提出基于凸共轭对偶的共轭学习理论框架，从有限样本角度定义实用可学习性，分析SGD训练DNN的全局最优性、模型架构影响、经验风险下界，并推导基于广义条件熵的泛化误差确定性/概率性边界。


<details>
  <summary>Details</summary>
Motivation: 现有学习理论大多基于无限样本假设，缺乏对有限样本实际训练场景的理论指导。需要建立更贴近实际的理论框架来分析深度神经网络在有限样本下的可学习性、优化特性和泛化行为。

Method: 提出基于凸共轭对偶的共轭学习理论框架，通过控制结构矩阵极端特征值和梯度能量证明SGD达到经验风险全局最优，推导模型无关的经验风险下界，并基于广义条件熵建立泛化误差的确定性和概率性边界。

Result: 理论证明：1) SGD训练DNN能达到经验风险全局最优；2) 数据决定训练性的根本极限；3) 泛化误差受模型不可逆性、最大损失值和特征条件熵三个因素影响；4) 批量大小、网络深度、参数数量、稀疏性、跳跃连接等架构特性影响非凸优化。实验验证了所有理论预测。

Conclusion: 提出的共轭学习理论框架为深度学习的实用可学习性提供了统一的理论视角，揭示了数据、模型架构和优化算法之间的内在联系，为理解深度神经网络的行为提供了理论基础。

Abstract: In this work, we propose a notion of practical learnability grounded in finite sample settings, and develop a conjugate learning theoretical framework based on convex conjugate duality to characterize this learnability property. Building on this foundation, we demonstrate that training deep neural networks (DNNs) with mini-batch stochastic gradient descent (SGD) achieves global optima of empirical risk by jointly controlling the extreme eigenvalues of a structure matrix and the gradient energy, and we establish a corresponding convergence theorem. We further elucidate the impact of batch size and model architecture (including depth, parameter count, sparsity, skip connections, and other characteristics) on non-convex optimization. Additionally, we derive a model-agnostic lower bound for the achievable empirical risk, theoretically demonstrating that data determines the fundamental limit of trainability. On the generalization front, we derive deterministic and probabilistic bounds on generalization error based on generalized conditional entropy measures. The former explicitly delineates the range of generalization error, while the latter characterizes the distribution of generalization error relative to the deterministic bounds under independent and identically distributed (i.i.d.) sampling conditions. Furthermore, these bounds explicitly quantify the influence of three key factors: (i) information loss induced by irreversibility in the model, (ii) the maximum attainable loss value, and (iii) the generalized conditional entropy of features with respect to labels. Moreover, they offer a unified theoretical lens for understanding the roles of regularization, irreversible transformations, and network depth in shaping the generalization behavior of deep neural networks. Extensive experiments validate all theoretical predictions, confirming the framework's correctness and consistency.

</details>


### [24] [On sparsity, extremal structure, and monotonicity properties of Wasserstein and Gromov-Wasserstein optimal transport plans](https://arxiv.org/abs/2602.16265)
*Titouan Vayer*

Main category: stat.ML

TL;DR: 本文系统比较了Gromov-Wasserstein距离与线性最优传输的差异，重点探讨了GW最优传输计划的稀疏性、置换支撑条件以及循环单调性等关键性质。


<details>
  <summary>Details</summary>
Motivation: 研究Gromov-Wasserstein距离与标准线性最优传输框架的重要性质差异，特别是GW最优传输计划的结构特性，包括稀疏性、置换支撑条件和循环单调性。

Method: 采用理论分析方法，基于条件负半定性质，推导GW最优传输计划的结构特征，并与线性最优传输框架进行系统性比较。

Result: 当条件负半定性质成立时，存在稀疏的GW最优传输计划，且这些计划可以支撑在置换上，揭示了GW距离与线性OT在结构上的重要差异。

Conclusion: GW距离在条件负半定性质下具有特殊的结构特性，其最优传输计划可以是稀疏的且支撑在置换上，这为GW距离的理论理解和实际应用提供了重要洞见。

Abstract: This note gives a self-contained overview of some important properties of the Gromov-Wasserstein (GW) distance, compared with the standard linear optimal transport (OT) framework. More specifically, I explore the following questions: are GW optimal transport plans sparse? Under what conditions are they supported on a permutation? Do they satisfy a form of cyclical monotonicity? In particular, I present the conditionally negative semi-definite property and show that, when it holds, there are GW optimal plans that are sparse and supported on a permutation.

</details>


### [25] [Machine Learning in Epidemiology](https://arxiv.org/abs/2602.16352)
*Marvin N. Wright,Lukas Burk,Pegah Golchian,Jan Kapar,Niklas Koenen,Sophie Hanna Langbein*

Main category: stat.ML

TL;DR: 本章为流行病学中应用机器学习提供方法论基础，涵盖监督/无监督学习原理、重要方法、模型评估策略、超参数优化和可解释机器学习，并配有R语言代码示例。


<details>
  <summary>Details</summary>
Motivation: 数字流行病学时代，流行病学家面临数据量激增、复杂性和维度增加的挑战，需要机器学习工具来分析海量复杂数据。

Method: 建立机器学习在流行病学中应用的方法论框架，包括监督学习和无监督学习原理、重要机器学习方法、模型评估策略、超参数优化方法，并引入可解释机器学习概念。

Result: 提供完整的机器学习应用方法论，通过心脏病数据集贯穿全章的R代码示例，使流行病学家能够实际应用机器学习工具分析复杂数据。

Conclusion: 本章为流行病学家提供了在数字流行病学时代应用机器学习分析复杂高维数据的系统方法论和实用工具，有助于应对现代流行病学数据分析的挑战。

Abstract: In the age of digital epidemiology, epidemiologists are faced by an increasing amount of data of growing complexity and dimensionality. Machine learning is a set of powerful tools that can help to analyze such enormous amounts of data. This chapter lays the methodological foundations for successfully applying machine learning in epidemiology. It covers the principles of supervised and unsupervised learning and discusses the most important machine learning methods. Strategies for model evaluation and hyperparameter optimization are developed and interpretable machine learning is introduced. All these theoretical parts are accompanied by code examples in R, where an example dataset on heart disease is used throughout the chapter.

</details>


### [26] [Learning Preference from Observed Rankings](https://arxiv.org/abs/2602.16476)
*Yu-Chang Chen,Chen Chian Fuh,Shang En Tsai*

Main category: stat.ML

TL;DR: 该论文提出了一个从部分排序信息中学习消费者偏好的灵活框架，通过将观察到的排序解释为具有逻辑选择概率的成对比较集合，并纠正选择偏差，使用逆概率加权和正则化方法进行估计。


<details>
  <summary>Details</summary>
Motivation: 在经济学和市场营销中，准确估计消费者偏好至关重要。现有方法面临两个主要挑战：1）如何从部分排序信息中有效学习个体偏好；2）如何纠正选择偏差，因为观察到的比较仅发生在消费者考虑集中的物品之间，导致对频繁接触物品的曝光偏差。

Method: 将观察到的排序解释为具有逻辑选择概率的成对比较集合。潜在效用建模为可解释的产品属性、物品固定效应和低秩用户-物品因子结构之和。通过逻辑模型估计物品级别的可观察倾向，使用逆概率加权（IPW）的岭正则化对数似然函数估计偏好参数。为扩展计算，提出了基于逆概率重采样的随机梯度下降（SGD）算法。

Result: 在在线葡萄酒零售商交易数据的应用中，该方法相对于基于流行度的基准模型，在样本外推荐性能上有所改善，特别是在预测先前未消费产品的购买方面表现出特别强的增益。

Conclusion: 该框架能够从部分排序信息中学习消费者偏好，同时解决选择偏差问题，通过可解释的效用建模和有效的计算算法，在实际应用中显示出优越的推荐性能，特别是在新产品推荐方面。

Abstract: Estimating consumer preferences is central to many problems in economics and marketing. This paper develops a flexible framework for learning individual preferences from partial ranking information by interpreting observed rankings as collections of pairwise comparisons with logistic choice probabilities. We model latent utility as the sum of interpretable product attributes, item fixed effects, and a low-rank user-item factor structure, enabling both interpretability and information sharing across consumers and items. We further correct for selection in which comparisons are observed: a comparison is recorded only if both items enter the consumer's consideration set, inducing exposure bias toward frequently encountered items. We model pair observability as the product of item-level observability propensities and estimate these propensities with a logistic model for the marginal probability that an item is observable. Preference parameters are then estimated by maximizing an inverse-probability-weighted (IPW), ridge-regularized log-likelihood that reweights observed comparisons toward a target comparison population. To scale computation, we propose a stochastic gradient descent (SGD) algorithm based on inverse-probability resampling, which draws comparisons in proportion to their IPW weights. In an application to transaction data from an online wine retailer, the method improves out-of-sample recommendation performance relative to a popularity-based benchmark, with particularly strong gains in predicting purchases of previously unconsumed products.

</details>


### [27] [Functional Decomposition and Shapley Interactions for Interpreting Survival Models](https://arxiv.org/abs/2602.16505)
*Sophie Hanna Langbein,Hubert Baniecki,Fabian Fumagalli,Niklas Koenen,Marvin N. Wright,Julia Herbinger*

Main category: stat.ML

TL;DR: SurvFD和SurvSHAP-IQ：用于生存模型特征交互分析的新方法，解决传统可解释性方法在非加性生存函数中的局限性


<details>
  <summary>Details</summary>
Motivation: 生存函数和风险函数是时间到事件预测中的自然、可解释的目标，但其固有的非加性特性从根本上限制了标准加性解释方法。需要一种能够分析生存模型中特征交互作用的新方法。

Method: 提出Survival Functional Decomposition (SurvFD)，将高阶效应分解为时间依赖和时间独立分量，从理论上分析特征交互。基于此提出SurvSHAP-IQ，将Shapley交互扩展到时间索引函数，提供高阶时间依赖交互的实用估计器。

Result: 建立了一个交互和时间感知的可解释性框架，能够明确表征加性解释何时以及为何失败，为生存建模提供了新的解释视角。

Conclusion: SurvFD和SurvSHAP-IQ共同建立了生存建模中交互和时间感知的可解释性方法，在时间到事件预测任务中具有广泛适用性。

Abstract: Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.

</details>


### [28] [Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study](https://arxiv.org/abs/2602.16601)
*Nail B. Khelifa,Richard E. Turner,Ramji Venkataramanan*

Main category: stat.ML

TL;DR: 该论文理论分析了扩散模型递归训练中使用合成数据导致的性能退化现象，通过上下界量化生成分布与目标分布之间的累积偏差，并识别了不同的漂移机制。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地在合成数据上进行训练或微调。递归训练会导致性能显著下降，表现为逐渐偏离目标分布。本文旨在理论分析扩散模型中这一现象。

Method: 在基于分数的扩散模型框架下，分析每轮训练同时使用合成数据和目标分布新鲜样本的流水线。通过理论推导获得生成分布与目标分布之间累积偏差的上界和下界。

Result: 能够根据分数估计误差和每轮生成中使用的新鲜数据比例来表征不同的漂移机制。在合成数据和图像上的实证结果验证了理论分析。

Conclusion: 该研究为理解递归训练中合成数据导致的分布漂移提供了理论框架，揭示了分数估计误差和新鲜数据比例对漂移机制的关键影响。

Abstract: Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, we obtain upper and lower bounds on the accumulated divergence between the generated and target distributions. This allows us to characterize different regimes of drift, depending on the score estimation error and the proportion of fresh data used in each generation. We also provide empirical results on synthetic data and images to illustrate the theory.

</details>


### [29] [Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models](https://arxiv.org/abs/2602.16634)
*Yu Xie,Ludwig Winkler,Lixin Sun,Sarah Lewis,Adam E. Foster,José Jiménez Luna,Tim Hempel,Michael Gastegger,Yaoyi Chen,Iryna Zaporozhets,Cecilia Clementi,Christopher M. Bishop,Frank Noé*

Main category: stat.ML

TL;DR: 提出增强扩散采样方法，通过精确引导协议生成偏置系综，再通过精确重加权恢复平衡统计，解决了扩散模型在稀有事件采样中的局限性


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型（如BioEmu）已成为强大的平衡采样器，能生成复杂分子分布的独立样本，但在计算依赖于平衡状态下稀有构象的观测量（如折叠自由能）时，采样问题仍然存在

Method: 提出增强扩散采样框架，通过精确引导协议生成偏置系综，然后通过精确重加权恢复平衡统计。具体实现三种算法：UmbrellaDiff（扩散模型的伞形采样）、ΔG-Diff（通过倾斜系综计算自由能差）和MetaDiff（元动力学的批量模拟版本）

Result: 在玩具系统、蛋白质折叠景观和折叠自由能计算中，这些方法实现了快速、准确、可扩展的平衡性质估计，每个系统仅需GPU分钟到小时级别的时间，填补了扩散模型平衡采样器出现后仍然存在的稀有事件采样空白

Conclusion: 增强扩散采样方法能够高效探索稀有事件区域，同时保持无偏的热力学估计器，解决了分子动力学中长期的稀有事件采样瓶颈问题

Abstract: The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $Δ$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [30] [Quantifying and Attributing Submodel Uncertainty in Stochastic Simulation Models and Digital Twins](https://arxiv.org/abs/2602.16099)
*Mohammadmahdi Ghasemloo,David J. Eckman,Yaxian Li*

Main category: stat.CO

TL;DR: 该论文提出了一个量化随机仿真中子模型不确定性的框架，通过引导和贝叶斯模型平均构建置信区间，并使用树状方法分解总输出变异性，为数字孪生等应用提供不确定性归因方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统仿真中，由于子过程未知或计算成本高，常使用估计或学习的近似模型替代，这引入了子模型不确定性。现有研究缺乏量化这种不确定性及其对系统性能指标估计影响的方法，特别是在数字孪生等需要重复仿真的场景中。

Method: 基于输入不确定性分析方法，采用引导法和贝叶斯模型平均构建性能指标的分位数置信区间或可信区间。提出树状方法分解总输出变异性，将不确定性归因到各个子模型，生成重要性评分。框架支持参数化和非参数化子模型，兼容频率学派和贝叶斯建模范式。

Result: 开发了一个模型无关的框架，能够量化子模型不确定性并构建置信区间。通过合成数值实验和呼叫中心数字孪生仿真实例，验证了理解各子模型对总体不确定性贡献的重要性。树状方法成功将不确定性分解并归因到具体子模型。

Conclusion: 子模型不确定性是随机仿真中的重要问题，特别是在数字孪生应用中。提出的框架能够有效量化这种不确定性，帮助理解各子模型对总体不确定性的贡献，为仿真结果的可靠性评估提供工具。

Abstract: Stochastic simulation is widely used to study complex systems composed of various interconnected subprocesses, such as input processes, routing and control logic, optimization routines, and data-driven decision modules. In practice, these subprocesses may be inherently unknown or too computationally intensive to directly embed in the simulation model. Replacing these elements with estimated or learned approximations introduces a form of epistemic uncertainty that we refer to as submodel uncertainty. This paper investigates how submodel uncertainty affects the estimation of system performance metrics. We develop a framework for quantifying submodel uncertainty in stochastic simulation models and extend the framework to digital-twin settings, where simulation experiments are repeatedly conducted with the model initialized from observed system states. Building on approaches from input uncertainty analysis, we leverage bootstrapping and Bayesian model averaging to construct quantile-based confidence or credible intervals for key performance indicators. We propose a tree-based method that decomposes total output variability and attributes uncertainty to individual submodels in the form of importance scores. The proposed framework is model-agnostic and accommodates both parametric and nonparametric submodels under frequentist and Bayesian modeling paradigms. A synthetic numerical experiment and a more realistic digital-twin simulation of a contact center illustrate the importance of understanding how and how much individual submodels contribute to overall uncertainty.

</details>
