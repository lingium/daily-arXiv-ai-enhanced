<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 15]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.ME](#stat.ME) [Total: 14]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Dimension-Free Minimax Rates for Learning Pairwise Interactions in Attention-Style Models](https://arxiv.org/abs/2510.11789)
*Shai Zucker,Xiong Wang,Fei Lu,Inbar Seroussi*

Main category: stat.ML

TL;DR: 本文研究了单层注意力风格模型中成对交互学习的最小化收敛率，证明了收敛率仅依赖于激活函数的平滑度，与token数量、环境维度或权重矩阵秩无关。


<details>
  <summary>Details</summary>
Motivation: 研究注意力风格模型的统计效率，理解注意力机制及其训练的理论基础，特别是在权重矩阵和激活函数不可分离识别情况下的性能。

Method: 通过理论分析证明单层注意力风格模型中成对交互学习的最小化收敛率，其中token通过权重矩阵和非线性激活函数进行交互。

Result: 证明了最小化收敛率为$M^{-\frac{2\beta}{2\beta+1}}$，其中M为样本大小，仅依赖于激活函数的平滑度β，与token数量、环境维度或权重矩阵秩无关。

Conclusion: 注意力风格非局部模型具有基本的无维度统计效率，即使权重矩阵和激活函数不可分离识别，这为理解注意力机制及其训练提供了理论基础。

Abstract: We study the convergence rate of learning pairwise interactions in
single-layer attention-style models, where tokens interact through a weight
matrix and a non-linear activation function. We prove that the minimax rate is
$M^{-\frac{2\beta}{2\beta+1}}$ with $M$ being the sample size, depending only
on the smoothness $\beta$ of the activation, and crucially independent of token
count, ambient dimension, or rank of the weight matrix. These results highlight
a fundamental dimension-free statistical efficiency of attention-style nonlocal
models, even when the weight matrix and activation are not separately
identifiable and provide a theoretical understanding of the attention mechanism
and its training.

</details>


### [2] [Improved Central Limit Theorem and Bootstrap Approximations for Linear Stochastic Approximation](https://arxiv.org/abs/2510.12375)
*Bogdan Butyrin,Eric Moulines,Alexey Naumov,Sergey Samsonov,Qi-Man Shao,Zhuo-Song Zhang*

Main category: stat.ML

TL;DR: 本文改进了Polyak-Ruppert平均迭代的多变量正态逼近的Berry-Esseen界，针对线性随机逼近算法，建立了凸距离下高达n^{-1/3}阶的逼近率，并证明了乘子自助法的有效性。


<details>
  <summary>Details</summary>
Motivation: 改进线性随机逼近算法中Polyak-Ruppert平均迭代的多变量正态逼近的误差界，特别是Berry-Esseen界，以提供更精确的统计推断。

Method: 使用凸距离度量，分析Polyak-Ruppert平均迭代的多变量正态逼近，并研究乘子自助法对重标度误差分布的逼近。

Result: 建立了凸距离下高达n^{-1/3}阶的逼近率，证明了乘子自助法的非渐近有效性，并获得了高达1/√n阶的逼近率，显著改进了Samsonov等人的先前结果。

Conclusion: 本文显著改进了线性随机逼近算法中平均迭代的多变量正态逼近误差界，为统计推断提供了更精确的理论基础。

Abstract: In this paper, we refine the Berry-Esseen bounds for the multivariate normal
approximation of Polyak-Ruppert averaged iterates arising from the linear
stochastic approximation (LSA) algorithm with decreasing step size. We consider
the normal approximation by the Gaussian distribution with covariance matrix
predicted by the Polyak-Juditsky central limit theorem and establish the rate
up to order $n^{-1/3}$ in convex distance, where $n$ is the number of samples
used in the algorithm. We also prove a non-asymptotic validity of the
multiplier bootstrap procedure for approximating the distribution of the
rescaled error of the averaged LSA estimator. We establish approximation rates
of order up to $1/\sqrt{n}$ for the latter distribution, which significantly
improves upon the previous results obtained by Samsonov et al. (2024).

</details>


### [3] [On Thompson Sampling and Bilateral Uncertainty in Additive Bayesian Optimization](https://arxiv.org/abs/2510.11792)
*Nathan Wycoff*

Main category: stat.ML

TL;DR: 本文研究了贝叶斯优化中忽略双边不确定性(BU)的实践影响，发现虽然精确方法性能略好，但差异在实际应用中不显著，支持了理论上的近似有效性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中常用的加性假设忽略了维度间的协方差关系(双边不确定性)，虽然理论上渐近损失不大，但在小预算情况下的实际影响尚不清楚。

Method: 利用条件独立性，实现了考虑双边不确定性的Thompson采样，并与忽略BU的加性近似方法进行实证比较。

Result: 精确Thompson采样方法在性能上确实优于加性近似方法，但两者差异在实际应用中并不显著。

Conclusion: 忽略双边不确定性的近似方法在贝叶斯优化实践中已经足够有效，即使在非渐近情况下也是如此。

Abstract: In Bayesian Optimization (BO), additive assumptions can mitigate the twin
difficulties of modeling and searching a complex function in high dimension.
However, common acquisition functions, like the Additive Lower Confidence
Bound, ignore pairwise covariances between dimensions, which we'll call
\textit{bilateral uncertainty} (BU), imposing a second layer of approximations.
While theoretical results indicate that asymptotically not much is lost in
doing so, little is known about the practical effects of this assumption in
small budgets. In this article, we show that by exploiting conditional
independence, Thompson Sampling respecting BU can be efficiently conducted. We
use this fact to execute an empirical investigation into the loss incurred by
ignoring BU, finding that the additive approximation to Thompson Sampling does
indeed have, on balance, worse performance than the exact method, but that this
difference is of little practical significance. This buttresses the theoretical
understanding and suggests that the BU-ignoring approximation is sufficient for
BO in practice, even in the non-asymptotic regime.

</details>


### [4] [Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps](https://arxiv.org/abs/2510.12744)
*Do Tien Hai,Trung Nguyen Mai,TrungTin Nguyen,Nhat Ho,Binh T. Nguyen,Christopher Drovandi*

Main category: stat.ML

TL;DR: 提出了一个统一的统计框架来解决软最大门控高斯混合专家模型中的参数估计和模型选择问题，包括非可识别性、门控-专家交互和软最大条件密度耦合等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决软最大门控高斯混合专家模型中长期存在的三个障碍：门控参数的非可识别性、门控-专家交互导致的似然耦合微分关系，以及软最大诱导条件密度中的分子-分母紧密耦合。

Method: 引入与门控分区几何对齐的Voronoi型损失函数，建立最大似然估计的有限样本收敛率，在过参数化模型中揭示MLE收敛率与多项式方程组可解性的联系，并采用混合测度的树状图进行模型选择。

Result: 在合成数据上准确恢复专家数量并达到预测的参数估计率，在模型误设情况下树状图选择标准稳健，在玉米蛋白质组学数据集上选择两个专家并优于标准准则。

Conclusion: 该框架为软最大门控高斯混合专家模型提供了统一的统计基础，解决了参数估计和模型选择的关键问题，在理论和实际应用中均表现出色。

Abstract: We develop a unified statistical framework for softmax-gated Gaussian mixture
of experts (SGMoE) that addresses three long-standing obstacles in parameter
estimation and model selection: (i) non-identifiability of gating parameters up
to common translations, (ii) intrinsic gate-expert interactions that induce
coupled differential relations in the likelihood, and (iii) the tight
numerator-denominator coupling in the softmax-induced conditional density. Our
approach introduces Voronoi-type loss functions aligned with the gate-partition
geometry and establishes finite-sample convergence rates for the maximum
likelihood estimator (MLE). In over-specified models, we reveal a link between
the MLE's convergence rate and the solvability of an associated system of
polynomial equations characterizing near-nonidentifiable directions. For model
selection, we adapt dendrograms of mixing measures to SGMoE, yielding a
consistent, sweep-free selector of the number of experts that attains
pointwise-optimal parameter rates under overfitting while avoiding multi-size
training. Simulations on synthetic data corroborate the theory, accurately
recovering the expert count and achieving the predicted rates for parameter
estimation while closely approximating the regression function. Under model
misspecification (e.g., $\epsilon$-contamination), the dendrogram selection
criterion is robust, recovering the true number of mixture components, while
the Akaike information criterion, the Bayesian information criterion, and the
integrated completed likelihood tend to overselect as sample size grows. On a
maize proteomics dataset of drought-responsive traits, our dendrogram-guided
SGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes
the likelihood early, and yields interpretable genotype-phenotype maps,
outperforming standard criteria without multi-size training.

</details>


### [5] [Active Subspaces in Infinite Dimension](https://arxiv.org/abs/2510.11871)
*Poorbita Kundu,Nathan Wycoff*

Main category: stat.ML

TL;DR: 将主动子空间分析从欧几里得空间扩展到希尔伯特空间中的实值泛函，定义相应算子并证明其保持原有优良性质，提出蒙特卡洛方法并应用于复杂测试问题的可视化和优化。


<details>
  <summary>Details</summary>
Motivation: 传统的主动子空间分析局限于欧几里得空间，需要将其扩展到更一般的希尔伯特空间中的实值泛函，以处理更复杂的函数分析问题。

Method: 定义希尔伯特空间中与主动子空间矩阵对应的算子，证明其保持欧几里得空间中的优良性质，提出蒙特卡洛计算程序并分析其收敛性。

Result: 成功将主动子空间分析扩展到希尔伯特空间，证明了扩展方法保持原有性质，蒙特卡洛方法具有良好收敛性，并在复杂测试问题上实现了有效的可视化和优化改进。

Conclusion: 主动子空间分析可以成功扩展到希尔伯特空间中的实值泛函，扩展方法保持原有优良特性，蒙特卡洛程序有效，为复杂问题的建模和优化提供了新工具。

Abstract: Active subspace analysis uses the leading eigenspace of the gradient's second
moment to conduct supervised dimension reduction. In this article, we extend
this methodology to real-valued functionals on Hilbert space. We define an
operator which coincides with the active subspace matrix when applied to a
Euclidean space. We show that many of the desirable properties of Active
Subspace analysis extend directly to the infinite dimensional setting. We also
propose a Monte Carlo procedure and discuss its convergence properties.
Finally, we deploy this methodology to create visualizations and improve
modeling and optimization on complex test problems.

</details>


### [6] [Learning Latent Energy-Based Models via Interacting Particle Langevin Dynamics](https://arxiv.org/abs/2510.12311)
*Joanna Marks,Tim Y. J. Wang,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: 提出了一种基于相互作用粒子的算法，用于学习具有能量先验的隐变量模型，通过定义随机微分方程来解决最大边际似然估计问题。


<details>
  <summary>Details</summary>
Motivation: 开发能够学习具有能量先验的隐变量模型的有效算法，利用粒子方法解决最大边际似然估计问题。

Method: 定义随机微分方程来求解最大边际似然估计问题，并通过离散化这些方程得到实用算法。

Result: 提供了算法的理论收敛保证，并在合成和图像数据集上验证了方法的经验有效性。

Conclusion: 提出的基于相互作用粒子的算法能够有效学习具有能量先验的隐变量模型，并具有理论保证和实证支持。

Abstract: We develop interacting particle algorithms for learning latent variable
models with energy-based priors. To do so, we leverage recent developments in
particle-based methods for solving maximum marginal likelihood estimation
(MMLE) problems. Specifically, we provide a continuous-time framework for
learning latent energy-based models, by defining stochastic differential
equations (SDEs) that provably solve the MMLE problem. We obtain a practical
algorithm as a discretisation of these SDEs and provide theoretical guarantees
for the convergence of the proposed algorithm. Finally, we demonstrate the
empirical effectiveness of our method on synthetic and image datasets.

</details>


### [7] [High-Probability Bounds For Heterogeneous Local Differential Privacy](https://arxiv.org/abs/2510.11895)
*Maryam Aliakbarpour,Alireza Fallah,Swaha Roy,Ria Stevens*

Main category: stat.ML

TL;DR: 该论文研究在局部差分隐私(LDP)下的统计估计问题，考虑了用户具有异构隐私级别的情况，并提供了高概率保证的准确性分析。


<details>
  <summary>Details</summary>
Motivation: 现有的LDP分析通常基于期望分析，而实际应用中需要高概率保证。此外，用户可能具有不同的隐私需求，现有方法未能充分处理这种异构隐私级别的情况。

Method: 针对一维和多维均值估计问题，开发了在ℓ₂范数下的有限样本上界，并设计了在ℓ∞距离下的分布学习算法，这些方法都考虑了异构隐私需求。

Result: 获得了与极小极大下界匹配的上界，证明了在异构LDP机制下所提方法的理论最优性（达到常数因子）。

Conclusion: 该研究为在用户特定隐私级别设置下设计机制提供了原则性指导，填补了异构LDP分析的理论空白。

Abstract: We study statistical estimation under local differential privacy (LDP) when
users may hold heterogeneous privacy levels and accuracy must be guaranteed
with high probability. Departing from the common in-expectation analyses, and
for one-dimensional and multi-dimensional mean estimation problems, we develop
finite sample upper bounds in $\ell_2$-norm that hold with probability at least
$1-\beta$. We complement these results with matching minimax lower bounds,
establishing the optimality (up to constants) of our guarantees in the
heterogeneous LDP regime. We further study distribution learning in
$\ell_\infty$-distance, designing an algorithm with high-probability guarantees
under heterogeneous privacy demands. Our techniques offer principled guidance
for designing mechanisms in settings with user-specific privacy levels.

</details>


### [8] [Simplifying Optimal Transport through Schatten-$p$ Regularization](https://arxiv.org/abs/2510.11910)
*Tyler Maunu*

Main category: stat.ML

TL;DR: 提出了一种基于Schatten-p范数正则化的最优传输低秩结构恢复通用框架，扩展了现有方法，提供统一的凸优化程序族来促进低维结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要促进稀疏和可解释的传输映射或计划，但缺乏对低秩结构的系统恢复框架。本文旨在填补这一空白，提供统一的凸优化方法来鼓励低维结构。

Method: 使用Schatten-p范数正则化构建凸优化程序，开发了具有收敛保证的镜像下降算法（适用于p≥1）。

Result: 在合成和真实数据上的实验证明了该方法的高效性、可扩展性以及恢复低秩传输结构的能力。

Conclusion: 该框架为最优传输中的低秩结构恢复提供了统一的理论基础和实践工具，具有理论保证和实际应用价值。

Abstract: We propose a new general framework for recovering low-rank structure in
optimal transport using Schatten-$p$ norm regularization. Our approach extends
existing methods that promote sparse and interpretable transport maps or plans,
while providing a unified and principled family of convex programs that
encourage low-dimensional structure. The convexity of our formulation enables
direct theoretical analysis: we derive optimality conditions and prove recovery
guarantees for low-rank couplings and barycentric maps in simplified settings.
To efficiently solve the proposed program, we develop a mirror descent
algorithm with convergence guarantees for $p \geq 1$. Experiments on synthetic
and real data demonstrate the method's efficiency, scalability, and ability to
recover low-rank transport structures.

</details>


### [9] [Statistical Guarantees for High-Dimensional Stochastic Gradient Descent](https://arxiv.org/abs/2510.12013)
*Jiaqi Li,Zhipeng Lou,Johannes Schmidt-Hieber,Wei Biao Wu*

Main category: stat.ML

TL;DR: 本文为高维环境下的常数学习率SGD和ASGD提供了严格的统计保证，通过将SGD视为非线性自回归过程并应用耦合技术，证明了高维SGD的几何矩收缩性。


<details>
  <summary>Details</summary>
Motivation: 现代大规模学习的核心SGD及其平均变体ASGD在高维设置下的理论性质很少被理解，需要提供严格的理论保证。

Method: 将SGD视为非线性自回归过程，应用高维时间序列的耦合技术，证明高维SGD的几何矩收缩性，建立迭代的渐近平稳性。

Result: 推导了SGD和ASGD在任意q≥2的ℓ^s范数下的q阶矩收敛，特别关注高维稀疏模型中常用的ℓ^∞范数，并提供了高概率浓度分析。

Conclusion: 该框架不仅填补了SGD理论的关键空白，还为分析广泛的高维学习算法提供了新的工具包。

Abstract: Stochastic Gradient Descent (SGD) and its Ruppert-Polyak averaged variant
(ASGD) lie at the heart of modern large-scale learning, yet their theoretical
properties in high-dimensional settings are rarely understood. In this paper,
we provide rigorous statistical guarantees for constant learning-rate SGD and
ASGD in high-dimensional regimes. Our key innovation is to transfer powerful
tools from high-dimensional time series to online learning. Specifically, by
viewing SGD as a nonlinear autoregressive process and adapting existing
coupling techniques, we prove the geometric-moment contraction of
high-dimensional SGD for constant learning rates, thereby establishing
asymptotic stationarity of the iterates. Building on this, we derive the $q$-th
moment convergence of SGD and ASGD for any $q\ge2$ in general $\ell^s$-norms,
and, in particular, the $\ell^{\infty}$-norm that is frequently adopted in
high-dimensional sparse or structured models. Furthermore, we provide sharp
high-probability concentration analysis which entails the probabilistic bound
of high-dimensional ASGD. Beyond closing a critical gap in SGD theory, our
proposed framework offers a novel toolkit for analyzing a broad class of
high-dimensional learning algorithms.

</details>


### [10] [Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory](https://arxiv.org/abs/2510.12077)
*Einar Urdshals,Edmund Lau,Jesse Hoogland,Stan van Wingerden,Daniel Murfet*

Main category: stat.ML

TL;DR: 使用奇异学习理论扩展MDL原理到神经网络等奇异模型，通过实验发现局部学习系数与模型压缩性存在紧密相关性


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的可压缩性，为严格评估模型压缩极限提供理论基础

Method: 使用奇异学习理论扩展最小描述长度原理，在Pythia套件上进行量化、分解等压缩技术的广泛实验

Result: 基于局部学习系数的复杂度估计与压缩性密切相关，在某些情况下呈线性相关

Conclusion: 为严格评估模型压缩极限提供了可行路径

Abstract: We study neural network compressibility by using singular learning theory to
extend the minimum description length (MDL) principle to singular models like
neural networks. Through extensive experiments on the Pythia suite with
quantization, factorization, and other compression techniques, we find that
complexity estimates based on the local learning coefficient (LLC) are closely,
and in some cases, linearly correlated with compressibility. Our results
provide a path toward rigorously evaluating the limits of model compression.

</details>


### [11] [Follow-the-Perturbed-Leader for Decoupled Bandits: Best-of-Both-Worlds and Practicality](https://arxiv.org/abs/2510.12152)
*Chaiwon Kim,Jongyeong Lee,Min-hwan Oh*

Main category: stat.ML

TL;DR: 提出了一种基于FTPL框架和Pareto扰动的解耦多臂老虎机策略，实现了最佳两界性能：在随机环境中获得常数遗憾，在对抗环境中获得极小极大最优遗憾，且计算效率比现有方法快约20倍。


<details>
  <summary>Details</summary>
Motivation: 解决解耦多臂老虎机问题，其中学习者在每轮选择一个探索臂和一个利用臂。探索臂的损失被观察但不计入，利用臂的损失被计入但不被观察。目标是设计一个既高效又实用的策略。

Method: 在Follow-the-Perturbed-Leader框架中使用Pareto扰动，避免了之前方法所需的凸优化步骤和重采样步骤。

Result: 该策略在随机环境中实现常数遗憾，在对抗环境中实现极小极大最优遗憾。计算效率比Decoupled-Tsallis-INF快约20倍，并在两种环境中都表现出更好的实证性能。

Conclusion: 提出的FTPL with Pareto扰动策略在解耦多臂老虎机问题中实现了最佳两界性能，具有显著的计算优势和实践价值，同时证明了纯探索策略与标准利用策略的简单组合是次优的。

Abstract: We study the decoupled multi-armed bandit (MAB) problem, where the learner
selects one arm for exploration and one arm for exploitation in each round. The
loss of the explored arm is observed but not counted, while the loss of the
exploited arm is incurred without being observed. We propose a policy within
the Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations.
Our policy achieves (near-)optimal regret regardless of the environment, i.e.,
Best-of-Both-Worlds (BOBW): constant regret in the stochastic regime, improving
upon the optimal bound of the standard MABs, and minimax optimal regret in the
adversarial regime. Moreover, the practicality of our policy stems from
avoiding both the convex optimization step required by the previous BOBW
policy, Decoupled-Tsallis-INF (Rouyer & Seldin, 2020), and the resampling step
that is typically necessary in FTPL. Consequently, it achieves substantial
computational improvement, about $20$ times faster than Decoupled-Tsallis-INF,
while also demonstrating better empirical performance in both regimes. Finally,
we empirically show that our approach outperforms a pure exploration policy,
and that naively combining a pure exploration with a standard exploitation
policy is suboptimal.

</details>


### [12] [Geopolitics, Geoeconomics and Risk:A Machine Learning Approach](https://arxiv.org/abs/2510.12416)
*Alvaro Ortiz,Tomasa Rodrigo*

Main category: stat.ML

TL;DR: 该研究引入了一个包含42个国家的高频每日面板数据集，结合市场数据和新闻指标，分析情绪动态对主权风险的影响及其预测价值。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索新闻情绪指标如何影响主权风险，并评估其相对于传统驱动因素（如全球货币政策和市场波动性）的预测能力。

Method: 使用包含42个国家的高频每日面板数据集，结合新闻指标和传统金融变量，通过赛马分析和非线性机器学习方法（特别是随机森林）进行预测模型比较。

Result: 研究发现新闻指标显著提升预测准确性，非线性机器学习方法带来最大增益。地缘政治风险和经济政策不确定性对主权风险有显著影响，且其效应通过与全球金融条件的非线性交互作用被放大。

Conclusion: 虽然全球金融变量仍是主权风险的主要驱动因素，但新闻情绪指标具有重要的预测价值，且不同地区和资产类别对冲击的敏感性存在显著异质性。

Abstract: We introduce a novel high-frequency daily panel dataset of both markets and
news-based indicators -- including Geopolitical Risk, Economic Policy
Uncertainty, Trade Policy Uncertainty, and Political Sentiment -- for 42
countries across both emerging and developed markets. Using this dataset, we
study how sentiment dynamics shape sovereign risk, measured by Credit Default
Swap (CDS) spreads, and evaluate their forecasting value relative to
traditional drivers such as global monetary policy and market volatility. Our
horse-race analysis of forecasting models demonstrates that incorporating
news-based indicators significantly enhances predictive accuracy and enriches
the analysis, with non-linear machine learning methods -- particularly Random
Forests -- delivering the largest gains. Our analysis reveals that while global
financial variables remain the dominant drivers of sovereign risk, geopolitical
risk and economic policy uncertainty also play a meaningful role. Crucially,
their effects are amplified through non-linear interactions with global
financial conditions. Finally, we document pronounced regional heterogeneity,
as certain asset classes and emerging markets exhibit heightened sensitivity to
shocks in policy rates, global financial volatility, and geopolitical risk.

</details>


### [13] [Universal Adaptive Environment Discovery](https://arxiv.org/abs/2510.12547)
*Madi Matymov,Ba-Hien Tran,Maurizio Filippone*

Main category: stat.ML

TL;DR: 提出Universal Adaptive Environment Discovery (UAED)框架，通过自适应学习数据变换分布来创建环境，无需预定义环境组，提升模型对虚假相关性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中模型利用数据中虚假相关性的问题，如Waterbirds数据集中的背景-标签捷径，传统方法需要预定义环境但现实中环境通常未知。

Method: UAED框架学习数据变换的分布来实例化环境，并优化任何鲁棒目标函数在该学习分布上的平均值，无需预定义组或手动环境设计。

Result: 实验表明UAED能发现可解释的环境分布，在标准基准上提高最差情况准确率，同时保持平均准确率的竞争力。

Conclusion: 使环境自适应是实现分布外泛化的实用途径。

Abstract: An open problem in Machine Learning is how to avoid models to exploit
spurious correlations in the data; a famous example is the background-label
shortcut in the Waterbirds dataset. A common remedy is to train a model across
multiple environments; in the Waterbirds dataset, this corresponds to training
by randomizing the background. However, selecting the right environments is a
challenging problem, given that these are rarely known a priori. We propose
Universal Adaptive Environment Discovery (UAED), a unified framework that
learns a distribution over data transformations that instantiate environments,
and optimizes any robust objective averaged over this learned distribution.
UAED yields adaptive variants of IRM, REx, GroupDRO, and CORAL without
predefined groups or manual environment design. We provide a theoretical
analysis by providing PAC-Bayes bounds and by showing robustness to test
environment distributions under standard conditions. Empirically, UAED
discovers interpretable environment distributions and improves worst-case
accuracy on standard benchmarks, while remaining competitive on mean accuracy.
Our results indicate that making environments adaptive is a practical route to
out-of-distribution generalization.

</details>


### [14] [Adapting Noise to Data: Generative Flows from 1D Processes](https://arxiv.org/abs/2510.12636)
*Jannis Chemseddine,Gregor Kornhardt,Richard Duong,Gabriele Steidl*

Main category: stat.ML

TL;DR: 提出了一种基于一维噪声过程的通用生成模型框架，其中噪声过程本身可学习，通过分位数函数参数化噪声分布以适应数据特征。


<details>
  <summary>Details</summary>
Motivation: 现有扩散过程等生成模型存在局限性，需要更灵活的框架来适应不同数据分布特征，如重尾分布和紧凑支持集。

Method: 使用可学习的一维噪声过程，通过分位数函数参数化噪声分布，能够与流匹配和一致性模型等标准目标无缝集成。

Result: 数值实验表明该方法既灵活又有效，能够自然地捕捉重尾分布和紧凑支持集等数据特征。

Conclusion: 该框架为构建生成模型提供了更通用的方法，通过可学习的噪声过程更好地适应数据分布特性。

Abstract: We introduce a general framework for constructing generative models using
one-dimensional noising processes. Beyond diffusion processes, we outline
examples that demonstrate the flexibility of our approach. Motivated by this,
we propose a novel framework in which the 1D processes themselves are
learnable, achieved by parameterizing the noise distribution through quantile
functions that adapt to the data. Our construction integrates seamlessly with
standard objectives, including Flow Matching and consistency models. Learning
quantile-based noise naturally captures heavy tails and compact supports when
present. Numerical experiments highlight both the flexibility and the
effectiveness of our method.

</details>


### [15] [Contraction and entropy production in continuous-time Sinkhorn dynamics](https://arxiv.org/abs/2510.12639)
*Anand Srinivasan,Jean-Jacques Slotine*

Main category: stat.ML

TL;DR: 本文分析了Sinkhorn算法在有限正则化参数下的连续时间极限，证明了其作为概率测度空间中的镜像下降，并建立了L^2收缩性、熵产生率恒等式、Onsager梯度流结构，以及严格正的谱间隙和指数熵衰减条件。


<details>
  <summary>Details</summary>
Motivation: 研究Sinkhorn算法在连续时间极限下的动力学性质，建立其与扩散过程的系统性联系，为算法分析和应用提供理论基础。

Method: 通过镜像Hessian诱导的时间依赖度量分析L^2收缩性，建立熵产生率恒等式，构造Onsager梯度流框架，证明Poincaré不等式和谱间隙严格正性。

Result: 证明了Sinkhorn流在ε>0时具有严格正谱间隙，熵衰减指数当且仅当对数Sobolev不等式成立，并给出了生成模型和离散算法中的实际应用。

Conclusion: Sinkhorn流具有与扩散过程相似的正则性性质，其分析框架可系统性地扩展到Sinkhorn算法，为算法设计和停止准则提供了理论依据。

Abstract: Recently, the vanishing-step-size limit of the Sinkhorn algorithm at finite
regularization parameter $\varepsilon$ was shown to be a mirror descent in the
space of probability measures. We give $L^2$ contraction criteria in two
time-dependent metrics induced by the mirror Hessian, which reduce to the
coercivity of certain conditional expectation operators. We then give an exact
identity for the entropy production rate of the Sinkhorn flow, which was
previously known only to be nonpositive. Examining this rate shows that the
standard semigroup analysis of diffusion processes extends systematically to
the Sinkhorn flow. We show that the flow induces a reversible Markov dynamics
on the target marginal as an Onsager gradient flow. We define the Dirichlet
form associated to its (nonlocal) infinitesimal generator, prove a Poincar\'e
inequality for it, and show that the spectral gap is strictly positive along
the Sinkhorn flow whenever $\varepsilon > 0$. Lastly, we show that the entropy
decay is exponential if and only if a logarithmic Sobolev inequality (LSI)
holds. We give for illustration two immediate practical use-cases for the
Sinkhorn LSI: as a design principle for the latent space in which generative
models are trained, and as a stopping heuristic for discrete-time algorithms.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [16] [Multi-objective Bayesian optimization for blocking in extreme value analysis and its application in additive manufacturing](https://arxiv.org/abs/2510.11960)
*Shehzaib Irfan,Nabeel Ahmad,Alexander Vinel,Daniel F. Silva,Shuai Shao,Nima Shamsaei,Jia Liu*

Main category: stat.AP

TL;DR: 提出MOBO-D*框架，使用多目标贝叶斯优化自动选择极值理论中的块大小，平衡拟合优度和极端事件预测准确性


<details>
  <summary>Details</summary>
Motivation: 极值理论中块最大值方法的块大小选择需要专家输入，缺乏自动化解决方案

Method: MOBO-D*框架，通过多目标贝叶斯优化构建帕累托前沿，优化块大小选择，考虑分布拟合优度和极端事件预测准确性两个目标

Result: 在增材制造案例和合成数据集上优于多个基准方法，可扩展到高维情况

Conclusion: MOBO-D*是重复自动块大小选择应用中的有前景方法

Abstract: Extreme value theory (EVT) is well suited to model extreme events, such as
floods, heatwaves, or mechanical failures, which is required for reliability
assessment of systems across multiple domains for risk management and loss
prevention. The block maxima (BM) method, a particular approach within EVT,
starts by dividing the historical observations into blocks. Then the sample of
the maxima for each block can be shown, under some assumptions, to converge to
a known class of distributions, which can then be used for analysis. The
question of automatic (i.e., without explicit expert input) selection of the
block size remains an open challenge. This work proposes a novel Bayesian
framework, namely, multi-objective Bayesian optimization (MOBO-D*), to optimize
BM blocking for accurate modeling and prediction of extremes in EVT. MOBO-D*
formulates two objectives: goodness-of-fit of the distribution of extreme
events and the accurate prediction of extreme events to construct an estimated
Pareto front for optimal blocking choices. The efficacy of the proposed
framework is illustrated by applying it to a real-world case study from the
domain of additive manufacturing as well as a synthetic dataset. MOBO-D*
outperforms a number of benchmarks and can be naturally extended to
high-dimensional cases. The computational experiments show that it can be a
promising approach in applications that require repeated automated block size
selection, such as optimization or analysis of many datasets at once.

</details>


### [17] [The Living Forecast: Evolving Day-Ahead Predictions into Intraday Reality](https://arxiv.org/abs/2510.12271)
*Kutay Bölat,Peter Palensky,Simon Tindemans*

Main category: stat.AP

TL;DR: 提出了一种贝叶斯更新机制，将全概率日前预测转换为日内预测，无需重新训练或推理，提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的日内预测对电力系统运行至关重要，而日前预测随着新信息的出现会逐渐失去相关性。

Method: 使用贝叶斯更新机制，基于观测数据更新条件变分自编码器预测器的高斯混合输出，保持概率结构的同时生成更新的分布。

Result: 在家庭用电和光伏发电数据集上的实验表明，该方法在似然、样本、分位数和点预测指标上提高了高达25%的预测精度。

Conclusion: 为现代电力系统提供了一个理论基础的日内预测框架，在具有强时间相关性的时间步长中效果最佳。

Abstract: Accurate intraday forecasts are essential for power system operations,
complementing day-ahead forecasts that gradually lose relevance as new
information becomes available. This paper introduces a Bayesian updating
mechanism that converts fully probabilistic day-ahead forecasts into intraday
forecasts without retraining or re-inference. The approach conditions the
Gaussian mixture output of a conditional variational autoencoder-based
forecaster on observed measurements, yielding an updated distribution for the
remaining horizon that preserves its probabilistic structure. This enables
consistent point, quantile, and ensemble forecasts while remaining
computationally efficient and suitable for real-time applications. Experiments
on household electricity consumption and photovoltaic generation datasets
demonstrate that the proposed method improves forecast accuracy up to 25%
across likelihood-, sample-, quantile-, and point-based metrics. The largest
gains occur in time steps with strong temporal correlation to observed data,
and the use of pattern dictionary-based covariance structures further enhances
performance. The results highlight a theoretically grounded framework for
intraday forecasting in modern power systems.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [18] [Contrastive Dimension Reduction: A Systematic Review](https://arxiv.org/abs/2510.11847)
*Sam Hawke,Eric Zhang,Jiawen Chen,Didong Li*

Main category: stat.ME

TL;DR: 本文系统综述了对比降维(CDR)方法，提出了分析病例对照研究的流程和CDR方法的分类体系，统一了不同方法的概念框架，并指出了该领域的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 在基因组学、成像和时间序列分析等科学领域中，传统降维方法如PCA可能无法有效分离感兴趣信号，需要专门方法提取处理组相对于对照组的独特或富集信号。

Method: 提出了分析病例对照研究的系统流程，建立了基于假设、目标和数学公式的CDR方法分类体系，将不同方法统一在共享概念框架下。

Result: 系统梳理了现有CDR方法，突出了关键应用和挑战，为CDR及其应用提供了清晰框架。

Conclusion: 通过提供CDR的清晰框架，旨在促进该新兴领域的更广泛采用并推动进一步发展，同时识别了开放问题和未来研究方向。

Abstract: Contrastive dimension reduction (CDR) methods aim to extract signal unique to
or enriched in a treatment (foreground) group relative to a control
(background) group. This setting arises in many scientific domains, such as
genomics, imaging, and time series analysis, where traditional dimension
reduction techniques such as Principal Component Analysis (PCA) may fail to
isolate the signal of interest. In this review, we provide a systematic
overview of existing CDR methods. We propose a pipeline for analyzing
case-control studies together with a taxonomy of CDR methods based on their
assumptions, objectives, and mathematical formulations, unifying disparate
approaches under a shared conceptual framework. We highlight key applications
and challenges in existing CDR methods, and identify open questions and future
directions. By providing a clear framework for CDR and its applications, we aim
to facilitate broader adoption and motivate further developments in this
emerging field.

</details>


### [19] [A Martingale Kernel Two-Sample Test](https://arxiv.org/abs/2510.11853)
*Anirban Chatterjee,Aaditya Ramdas*

Main category: stat.ME

TL;DR: 提出基于鞅解释的MMD变体mMMD，具有标准高斯零分布，计算效率显著优于标准MMD检验，仅轻微损失检验功效


<details>
  <summary>Details</summary>
Motivation: 标准MMD检验统计量的零分布难以处理，需要昂贵的重采样或置换方法进行校准，计算成本高

Method: 利用估计平方MMD的鞅解释，提出鞅MMD(mMMD)统计量，该统计量在零假设下具有极限标准高斯分布

Result: mMMD对任何固定备择假设具有一致性，在大样本情况下相比标准MMD检验显著节省计算时间

Conclusion: mMMD在保持良好检验功效的同时，大大降低了计算复杂度，为两样本检验提供了更高效的替代方案

Abstract: The Maximum Mean Discrepancy (MMD) is a widely used multivariate distance
metric for two-sample testing. The standard MMD test statistic has an
intractable null distribution typically requiring costly resampling or
permutation approaches for calibration. In this work we leverage a martingale
interpretation of the estimated squared MMD to propose martingale MMD (mMMD), a
quadratic-time statistic which has a limiting standard Gaussian distribution
under the null. Moreover we show that the test is consistent against any fixed
alternative and for large sample sizes, mMMD offers substantial computational
savings over the standard MMD test, with only a minor loss in power.

</details>


### [20] [Optimal Treatment Rules under Missing Predictive Covariates: A Covariate-Balancing Doubly Robust Approach](https://arxiv.org/abs/2510.12321)
*Yue Zhang,Shanshan Luo,Zhi Geng,Yangbo He*

Main category: stat.ME

TL;DR: 提出一种协变量平衡双重稳健估计器，用于构建最优个体化治疗规则，特别适用于存在额外预测性协变量的情况。该方法具有双重稳健性、最小方差和潜在效率提升等优势。


<details>
  <summary>Details</summary>
Motivation: 在精准医疗中，估计最优个体化治疗规则时，实践中可能存在缺失的协变量，这些协变量不一定是混杂因素，但不确定是否应该包含它们来学习最优ITRs。

Method: 基于两个主要步骤：首先通过求解协变量平衡方程估计倾向得分；其次通过最小化目标函数估计结果模型，该函数由正确指定倾向得分下的渐近方差定义。

Result: 该方法在数值模拟和两个真实世界数据集上得到验证，显示出良好的性能。

Conclusion: 提出的协变量平衡双重稳健估计器为存在缺失协变量的个体化治疗规则估计提供了一种有效方法，具有双重稳健性、最小方差和效率提升等优势。

Abstract: In precision medicine, one of the most important problems is estimating the
optimal individualized treatment rules (ITR), which typically involves
recommending treatment decisions based on fully observed individual
characteristics of patients to maximize overall clinical benefit. In practice,
however, there may be missing covariates that are not necessarily confounders,
and it remains uncertain whether these missing covariates should be included
for learning optimal ITRs. In this paper, we propose a covariate-balancing
doubly robust estimator for constructing optimal ITRs, which is particularly
suitable for situations with additional predictive covariates. The proposed
method is based on two main steps: First, the propensity scores are estimated
by solving the covariate-balancing equation. Second, an objective function is
minimized to estimate the outcome model, with the function defined by the
asymptotic variance under the correctly specified propensity score. The method
has three significant advantages: (i) It is doubly robust, ensuring consistency
when either the propensity score or outcome model is correctly specified. (ii)
It minimizes variance within the class of augmented inverse probability
weighted estimators. (iii) When applied to partially observed covariates
related to the outcome, the method may further improve estimation efficiency.
We demonstrate the proposed method through extensive numerical simulations and
two real-world datasets.

</details>


### [21] [Monitoring 3D Lattice Structures in Additive Manufacturing Using Topological Data Analysis](https://arxiv.org/abs/2510.11740)
*Yulin An,Xueqi Zhao,Enrique del Castillo*

Main category: stat.ME

TL;DR: 提出了一种使用拓扑数据分析工具进行晶格结构统计过程控制的新方法，通过监测零维和一维同调特征来检测晶格拓扑变化。


<details>
  <summary>Details</summary>
Motivation: 受增材制造应用（如航空航天部件和生物医学植入物）的推动，这些应用中空心晶格几何结构至关重要，需要监控其拓扑特性。

Method: 基于监测零件的持久同调特性，重点关注零维和一维同调特征（连通分量和一维环），引入非参数假设检验程序和控制图方案来监控生产过程中的这些特征。

Result: 通过大量模拟但真实的晶格结构零件进行运行长度分析，结果表明持久同调非常适合检测复杂几何结构中的拓扑异常。

Conclusion: 持久同调为网格和点数据的其他SPC方法提供了稳健的、内在几何的替代方案。

Abstract: We present a new method for the statistical process control of lattice
structures using tools from Topological Data Analysis. Motivated by
applications in additive manufacturing, such as aerospace components and
biomedical implants, where hollow lattice geometries are critical, the proposed
framework is based on monitoring the persistent homology properties of parts.
Specifically, we focus on homological features of dimensions zero and one,
corresponding to connected components and one-dimensional loops, to
characterize and detect changes in the topology of lattice structures. A
nonparametric hypothesis testing procedure and a control charting scheme are
introduced to monitor these features during production. Furthermore, we conduct
extensive run-length analysis via various simulated but real-life
lattice-structured parts. Our results demonstrate that persistent homology is
well-suited for detecting topological anomalies in complex geometries and
offers a robust, intrinsically geometrical alternative to other SPC methods for
mesh and point data.

</details>


### [22] [Robust Functional Logistic Regression](https://arxiv.org/abs/2510.12048)
*Berkay Akturk,Ufuk Beyaztas,Han Lin Shang*

Main category: stat.ME

TL;DR: 提出了一种用于功能逻辑回归的稳健估计方法，通过稳健功能主成分分析降低异常值影响，使用M型估计器提高参数估计和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 传统功能逻辑回归方法对异常值敏感，导致参数估计不准确和分类性能下降，需要开发稳健的估计方法。

Method: 将功能预测变量投影到有限维子空间进行降维，使用稳健功能主成分分析，基于二元响应和稳健主成分得分采用M型估计器。

Result: 蒙特卡洛模拟和手部X光数据实验表明，该方法在存在异常值时优于现有方法，无异常值时表现相当，且计算效率更高。

Conclusion: 所提出的稳健功能逻辑回归方法能有效处理异常值，提高估计精度和分类性能，同时具有计算效率优势。

Abstract: Functional logistic regression is a popular model to capture a linear
relationship between binary response and functional predictor variables.
However, many methods used for parameter estimation in functional logistic
regression are sensitive to outliers, which may lead to inaccurate parameter
estimates and inferior classification accuracy. We propose a robust estimation
procedure for functional logistic regression, in which the observations of the
functional predictor are projected onto a set of finite-dimensional subspaces
via robust functional principal component analysis. This dimension-reduction
step reduces the outlying effects in the functional predictor. The logistic
regression coefficient is estimated using an M-type estimator based on binary
response and robust principal component scores. In doing so, we provide robust
estimates by minimizing the effects of outliers in the binary response and
functional predictor variables. Via a series of Monte-Carlo simulations and
using hand radiograph data, we examine the parameter estimation and
classification accuracy for the response variable. We find that the robust
procedure outperforms some existing robust and non-robust methods when outliers
are present, while producing competitive results when outliers are absent. In
addition, the proposed method is computationally more efficient than some
existing robust alternatives.

</details>


### [23] [Hypothesis testing for the dimension of random geometric graph](https://arxiv.org/abs/2510.11844)
*Mingao Yuan,Feng Yu*

Main category: stat.ME

TL;DR: 本文提出了第一个用于检验随机几何图(RGG)维度的统计检验方法，通过假设检验验证预设维度是否等于真实维度。


<details>
  <summary>Details</summary>
Motivation: 随机几何图是分析真实网络几何和依赖结构的强大工具，但在拟合RGG到真实网络时，需要输入或估计维度m，但无法确定预设维度是否等于真实维度。

Method: 提出基于退化U统计量的统计检验方法，利用依赖于节点数的核函数的渐近理论，推导检验统计量的渐近分布，并基于邻接矩阵提出高效计算方法。

Result: 在原假设下，检验统计量依分布收敛于标准正态分布；在备择假设下，检验统计量在概率上无界。模拟研究表明该检验表现良好。

Conclusion: 该检验方法为验证RGG模型维度提供了有效的统计工具，可应用于多种真实网络维度的检验。

Abstract: Random geometric graphs (RGGs) offer a powerful tool for analyzing the
geometric and dependence structures in real-world networks. For example, it has
been observed that RGGs are a good model for protein-protein interaction
networks. In RGGs, nodes are randomly distributed over an $m$-dimensional
metric space, and edges connect the nodes if and only if their distance is less
than some threshold. When fitting RGGs to real-world networks, the first step
is probably to input or estimate the dimension $m$. However, it is not clear
whether the prespecified dimension is equal to the true dimension. In this
paper, we investigate this problem using hypothesis testing. Under the null
hypothesis, the dimension is equal to a specific value, while the alternative
hypothesis asserts the dimension is not equal to that value. We propose the
first statistical test. Under the null hypothesis, the proposed test statistic
converges in law to the standard normal distribution, and under the alternative
hypothesis, the test statistic is unbounded in probability. We derive the
asymptotic distribution by leveraging the asymptotic theory of degenerate
U-statistics with kernel function dependent on the number of nodes. This
approach differs significantly from prevailing methods used in network
hypothesis testing problems. Moreover, we also propose an efficient approach to
compute the test statistic based on the adjacency matrix. Simulation studies
show that the proposed test performs well. We also apply the proposed test to
multiple real-world networks to test their dimensions.

</details>


### [24] [On the permutation invariance principle for causal estimands](https://arxiv.org/abs/2510.11863)
*Jiaqi Tong,Fan Li*

Main category: stat.ME

TL;DR: 该论文提出因果推断中的排列不变性原则，即因果估计量应在变量重新标记时保持不变，并开发了满足此原则的加权估计量。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，多个行动变量常具有相同的因果角色但缺乏自然排序，为避免解释歧义，需要确保估计量在变量重新标记时保持不变。

Method: 形式化描述排列不变性原则，分析其代数与组合结构，提出一类加权估计量，这些估计量在排列下不变且能捕捉所有阶数的交互作用。

Result: 开发出满足排列不变性的加权估计量，并提供权重选择指导以获得无残差估计量，其包含-排除和能捕捉最大效应。

Conclusion: 排列不变性是因果推断中的重要原则，提出的加权估计量框架能有效处理无自然排序变量的因果分析问题，并可扩展到比率效应度量。

Abstract: In many causal inference problems, multiple action variables share the same
causal role, such as mediators, factors, network units, or genotypes, yet lack
a natural ordering. To avoid ambiguity in interpretation, causal estimands
should remain unchanged under relabeling, an implicit principle we refer to as
permutation invariance. We formally characterize this principle, analyze its
algebraic and combinatorial structure for verification, and present a class of
weighted estimands that are permutation-invariant while capturing interactions
of all orders. We further provide guidance on selecting weights that yield
residual-free estimands, whose inclusion-exclusion sums capture the maximal
effect, and extend our results to ratio effect measures.

</details>


### [25] [Inhomogeneous continuous-time Markov chains to infer flexible time-varying evolutionary rates](https://arxiv.org/abs/2510.11982)
*Pratyusa Datta,Philippe Lemey,Marc A. Suchard*

Main category: stat.ME

TL;DR: 提出了一个灵活的贝叶斯系统发育推断框架，通过将序列特征替换过程建模为沿未知系统发育树作用的不均匀连续时间马尔可夫链，来估计随时间变化的进化速率。


<details>
  <summary>Details</summary>
Motivation: 重建进化历史并从分子序列数据估计进化速率在进化生物学和传染病研究中至关重要。现有方法通常假设恒定进化速率，但实际进化速率可能随时间变化。

Method: 使用分段恒定速率函数（多纪元时钟模型）参数化速率函数，采用高斯马尔可夫随机场先验实现时间平滑，并通过哈密顿蒙特卡洛采样进行高效计算。

Result: 通过两种不同进化情景的模拟评估了多纪元时钟模型在恢复真实时间尺度和速率方面的性能，并应用于西尼罗河病毒、登革热病毒、甲型H3N2流感病毒和SARS-CoV-2在欧洲传播的速率估计。

Conclusion: 该框架能够灵活捕捉随时间变化的进化速率，计算效率高，在系统发育分析和传染病传播速率估计中具有重要应用价值。

Abstract: Reconstructing evolutionary histories and estimating the rate of evolution
from molecular sequence data is of central importance in evolutionary biology
and infectious disease research. We introduce a flexible Bayesian phylogenetic
inference framework that accommodates changing evolutionary rates over time by
modeling sequence character substitution processes as inhomogeneous
continuous-time Markov chains (ICTMCs) acting along the unknown phylogeny,
where the rate remains as an unknown, positive and integrable function of time.
The integral of the rate function appears in the finite-time transition
probabilities of the ICTMCs that must be efficiently computed for all branches
of the phylogeny to evaluate the observed data likelihood. Circumventing
computational challenges that arise from a fully nonparametric function, we
successfully parameterize the rate function as piecewise constant with a large
number of epochs that we call the polyepoch clock model. This makes the
transition probability computation relatively inexpensive and continues to
flexibly capture rate change over time. We employ a Gaussian Markov random
field prior to achieve temporal smoothing of the estimated rate function.
Hamiltonian Monte Carlo sampling enabled by scalable gradient evaluation under
this model makes our framework computationally efficient. We assess the
performance of the polyepoch clock model in recovering the true timescales and
rates through simulations under two different evolutionary scenarios. We then
apply the polyepoch clock model to examine the rates of West Nile virus, Dengue
virus and influenza A/H3N2 evolution, and estimate the time-varying rate of
SARS-CoV-2 spread in Europe in 2020.

</details>


### [26] [Edgington's Method for Random-Effects Meta-Analysis Part I: Estimation](https://arxiv.org/abs/2510.12301)
*David Kronthaler,Leonhard Held*

Main category: stat.ME

TL;DR: 本文扩展了基于联合p值函数的元分析估计框架，通过采用置信分布方法来纳入异质性估计的不确定性，提高了置信区间的覆盖精度。


<details>
  <summary>Details</summary>
Motivation: 传统元分析方法在异质性估计中存在不确定性，这会影响点估计和置信区间的准确性。本文旨在通过置信分布方法来解决异质性估计的不确定性问题。

Method: 使用置信分布方法调整Edgington方法的置信分布，基于广义异质性统计量构建异质性参数的置信分布。

Result: 模拟结果显示，在涉及超过三个研究和异质性的大多数情况下，95%置信区间接近名义覆盖水平。在无异质性或仅有三个研究时，置信区间通常过度覆盖，但比Hartung-Knapp-Sidik-Jonkman区间更窄。点估计在模型错误设定和中等到大异质性下表现出小偏差。

Conclusion: Edgington方法为经典方法提供了实用的替代方案，通过调整异质性估计不确定性通常能改善置信区间的覆盖效果。

Abstract: Meta-analysis can be formulated as combining $p$-values across studies into a
joint $p$-value function, from which point estimates and confidence intervals
can be derived. We extend the meta-analytic estimation framework based on
combined $p$-value functions to incorporate uncertainty in heterogeneity
estimation by employing a confidence distribution approach. Specifically, the
confidence distribution of Edgington's method is adjusted according to the
confidence distribution of the heterogeneity parameter constructed from the
generalized heterogeneity statistic. Simulation results suggest that 95%
confidence intervals approach nominal coverage under most scenarios involving
more than three studies and heterogeneity. Under no heterogeneity or for only
three studies, the confidence interval typically overcovers, but is often
narrower than the Hartung-Knapp-Sidik-Jonkman interval. The point estimator
exhibits small bias under model misspecification and moderate to large
heterogeneity. Edgington's method provides a practical alternative to classical
approaches, with adjustment for heterogeneity estimation uncertainty often
improving confidence interval coverage.

</details>


### [27] [Sliding-Window Signatures for Time Series: Application to Electricity Demand Forecasting](https://arxiv.org/abs/2510.12337)
*Nina Drobac,Margaux Brégère,Joseph de Vilmarest,Olivier Wintenberger*

Main category: stat.ME

TL;DR: 提出基于岭回归和滑动窗口签名特征的预测框架，能够有效捕捉非线性时延效应，无需学习或手工特征工程。


<details>
  <summary>Details</summary>
Motivation: 非线性效应和时延效应使时间序列预测变得困难，需要能够自动捕捉复杂时间动态的方法。

Method: 使用滑动窗口上的签名特征结合岭回归，建立了理论保证（近似普适性和签名平稳性），并开发了高效的顺序计算算法。

Result: 在合成数据和真实电力需求数据上的评估表明，签名特征能有效编码时间和非线性依赖关系，预测精度与基于专家知识的方法相当。

Conclusion: 签名特征为时间序列预测提供了一种有效的方法，能够自动捕捉复杂的时间动态，无需依赖专业知识。

Abstract: Nonlinear and delayed effects of covariates often render time series
forecasting challenging. To this end, we propose a novel forecasting framework
based on ridge regression with signature features calculated on sliding
windows. These features capture complex temporal dynamics without relying on
learned or hand-crafted representations. Focusing on the discrete-time setting,
we establish theoretical guarantees, namely universality of approximation and
stationarity of signatures. We introduce an efficient sequential algorithm for
computing signatures on sliding windows. The method is evaluated on both
synthetic and real electricity demand data. Results show that signature
features effectively encode temporal and nonlinear dependencies, yielding
accurate forecasts competitive with those based on expert knowledge.

</details>


### [28] [Variational Inference for Count Response Semiparametric Regression: A Convex Solution](https://arxiv.org/abs/2510.12356)
*Virginia Murru,Matt P. Wand*

Main category: stat.ME

TL;DR: 开发了一种用于贝叶斯计数响应回归模型的变分推断方法，具有凸性和闭式更新等优点，使用Pólya-Gamma增强负二项似然和结构化平均场变分贝叶斯。


<details>
  <summary>Details</summary>
Motivation: 为贝叶斯计数响应回归模型开发具有数值稳定性和快速实现优势的变分推断方法。

Method: 使用Pólya-Gamma增强负二项似然，对形状参数采用有限值先验，采用结构化平均场变分贝叶斯范式。

Result: 获得了具有凸性和闭式更新的变分推断方法，实现了数值稳定的拟合算法和快速实现。

Conclusion: 该方法适用于一般计数响应情况，特别关注半参数回归类中的广义线性混合模型，并描述了实时拟合。

Abstract: We develop a version of variational inference for Bayesian count response
regression-type models that possesses attractive attributes such as convexity
and closed form updates. The convex solution aspect entails numerically stable
fitting algorithms, whilst the closed form aspect makes the methodology fast
and easy to implement. The essence of the approach is the use of P\'olya-Gamma
augmentation of a Negative Binomial likelihood, a finite-valued prior on the
shape parameter and the structured mean field variational Bayes paradigm. The
approach applies to general count response situations. For concreteness, we
focus on generalized linear mixed models within the semiparametric regression
class of models. Real-time fitting is also described.

</details>


### [29] [Causal inference of post-transcriptional regulation timelines from long-read sequencing in Arabidopsis thaliana](https://arxiv.org/abs/2510.12504)
*Rubén Martos,Christophe Ambroise,Guillem Rigaill*

Main category: stat.ME

TL;DR: 提出基于Pearl因果理论的新框架，通过因果发现、因果推断和时间线构建三阶段重建基因调控时序，应用于拟南芥叶绿体基因，解决了缺失数据和正则化参数选择等挑战。


<details>
  <summary>Details</summary>
Motivation: 需要重建基因调控的精确时序，传统方法在可靠性和模型拟合方面存在局限，因果推理能提供更可靠的时间线模型。

Method: 三阶段框架：因果发现（使用HC、PC、LiNGAM、NOTEARS四种算法）、因果推断、时序构建；处理缺失数据使用EM算法联合插补和贝叶斯网络估计；NOTEARS的ℓ₁正则化参数通过稳定性选择策略确定。

Result: 生成的因果模型在可靠性和模型拟合方面均优于参考时间线；结合领域知识能够制定可检验假设和基于理论预测的靶向实验干预设计。

Conclusion: 该因果推理框架为基因调控时序重建提供了可靠方法，能够生成更准确的时间线模型并支持实验设计，在计算生物学中具有重要应用价值。

Abstract: We propose a novel framework for reconstructing the chronology of genetic
regulation using causal inference based on Pearl's theory. The approach
proceeds in three main stages: causal discovery, causal inference, and
chronology construction. We apply it to the ndhB and ndhD genes of the
chloroplast in Arabidopsis thaliana, generating four alternative maturation
timeline models per gene, each derived from a different causal discovery
algorithm (HC, PC, LiNGAM, or NOTEARS). Two methodological challenges are
addressed: the presence of missing data, handled via an EM algorithm that
jointly imputes missing values and estimates the Bayesian network, and the
selection of the $\ell_1$-regularization parameter in NOTEARS, for which we
introduce a stability selection strategy. The resulting causal models
consistently outperform reference chronologies in terms of both reliability and
model fit. Moreover, by combining causal reasoning with domain expertise, the
framework enables the formulation of testable hypotheses and the design of
targeted experimental interventions grounded in theoretical predictions.

</details>


### [30] [On goodness-of-fit testing for volatility in McKean-Vlasov models](https://arxiv.org/abs/2510.12607)
*Akram Heidari,Mark Podolskij*

Main category: stat.ME

TL;DR: 为McKean-Vlasov随机微分方程中的波动率函数开发了一个拟合优度检验的统计框架，基于离散粒子系统观测数据，在粒子数和采样频率同时增加的联合机制下进行分析。


<details>
  <summary>Details</summary>
Motivation: 虽然经典SDE中的积分波动率估计已很成熟，但McKean-Vlasov系统的正式模型验证和拟合优度检验仍未被充分探索，特别是在大粒子极限和高频采样的联合机制下。

Method: 基于离散粒子系统观测提出检验统计量，在粒子数和采样频率同时增加的联合机制下分析，证明所涉估计量的一致性，检验统计量满足中心极限定理。

Result: 检验统计量被证明服从中心极限定理，收敛于中心高斯分布，估计量具有一致性。

Conclusion: 为McKean-Vlasov SDE的波动率函数拟合优度检验提供了有效的统计框架，填补了该领域的研究空白。

Abstract: This paper develops a statistical framework for goodness-of-fit testing of
volatility functions in McKean-Vlasov stochastic differential equations, which
describe large systems of interacting particles with distribution-dependent
dynamics. While integrated volatility estimation in classical SDEs is now well
established, formal model validation and goodness-of-fit testing for
McKean-Vlasov systems remain largely unexplored, particularly in regimes with
both large particle limits and high-frequency sampling. We propose a test
statistic based on discrete observations of particle systems, analysed in a
joint regime where both the number of particles and the sampling frequency
increase. The estimators involved are proven to be consistent, and the test
statistic is shown to satisfy a central limit theorem, converging in
distribution to a centred Gaussian law.

</details>


### [31] [The $α$--regression for compositional data: a unified framework for standard, spatially-lagged, and geographically-weighted regression models](https://arxiv.org/abs/2510.12663)
*Michail Tsagris*

Main category: stat.ME

TL;DR: 本文重新审视α-回归框架，通过灵活的参数化幂变换处理成分数据，无需插补零值，并扩展到空间设置，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法在处理成分数据（非负分量和为1）时面临单位总和约束和零值的挑战，需要一种能自然处理零值且允许数据驱动变换选择的方法。

Method: 将α-回归构建为非线性最小二乘问题，使用Levenberg-Marquardt算法进行高效估计，推导显式梯度和Hessian矩阵，并扩展到两个空间模型：α-空间滞后X回归模型和地理加权α-回归。

Result: 在希腊农业土地利用数据上的应用表明，空间扩展显著提高了预测性能。

Conclusion: α-回归框架为成分数据分析提供了灵活且有效的方法，其空间扩展能够更好地捕捉空间依赖性和局部关系。

Abstract: Compositional data-vectors of non--negative components summing to
unity--frequently arise in scientific applications where covariates influence
the relative proportions of components, yet traditional regression approaches
struggle with the unit-sum constraint and zero values. This paper revisits the
$\alpha$--regression framework, which uses a flexible power transformation
parameterized by $\alpha$ to interpolate between raw data analysis and
log-ratio methods, naturally handling zeros without imputation while allowing
data-driven transformation selection. We formulate $\alpha$--regression as a
non-linear least squares problem, provide efficient estimation via the
Levenberg-Marquardt algorithm with explicit gradient and Hessian derivations,
establish asymptotic normality of the estimators, and derive marginal effects
for interpretation. The framework is extended to spatial settings through two
models: the $\alpha$--spatially lagged X regression model, which incorporates
spatial spillover effects via spatially lagged covariates with decomposition
into direct and indirect effects, and the geographically weighted
$\alpha$--regression, which allows coefficients to vary spatially for capturing
local relationships. Application to Greek agricultural land-use data
demonstrates that spatial extensions substantially improve predictive
performance.

</details>
