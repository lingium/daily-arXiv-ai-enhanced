{"id": "2511.19960", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.19960", "abs": "https://arxiv.org/abs/2511.19960", "authors": ["Deepra Ghosh", "Sanat K. Sarkar"], "title": "Dependence-Aware False Discovery Rate Control in Two-Sided Gaussian Mean Testing", "comment": null, "summary": "This paper develops a general framework for controlling the false discovery rate (FDR) in multiple testing of Gaussian means against two-sided alternatives. The widely used Benjamini-Hochberg (BH) procedure provides exact FDR control under independence or conservative control under specific one-sided dependence structures, but its validity for correlated two-sided tests has remained an open question. We introduce the notion of positive left-tail dependence under the null (PLTDN), extending classical dependence assumptions to two-sided settings, and show that it ensures valid FDR control for BH-type procedures. Building on this framework, we propose a family of generalized shifted BH (GSBH) methods that incorporate correlation information through simple p-value adjustments. Simulation results demonstrate reliable FDR control and improved power across a range of dependence structures, while an application to an HIV gene expression dataset illustrates the practical effectiveness of the proposed approach."}
{"id": "2511.20087", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.20087", "abs": "https://arxiv.org/abs/2511.20087", "authors": ["Marco Battiston", "Yu Luo"], "title": "An Infinite BART model", "comment": null, "summary": "Bayesian additive regression trees (BART) are popular Bayesian ensemble models used in regression and classification analysis. Under this modeling framework, the regression function is approximated by an ensemble of decision trees, interpreted as weak learners that capture different features of the data. In this work, we propose a generalization of the BART model that has two main features: first, it automatically selects the number of decision trees using the given data; second, the model allows clusters of observations to have different regression functions since each data point can only use a selection of weak learners, instead of all of them. This model generalization is accomplished by including a binary weight matrix in the conditional distribution of the response variable, which activates only a specific subset of decision trees for each observation. Such a matrix is endowed with an Indian Buffet process prior, and sampled within the MCMC sampler, together with the other BART parameters. We then compare the Infinite BART model with the classic one on simulated and real datasets. Specifically, we provide examples illustrating variable importance, partial dependence and causal estimation."}
{"id": "2511.19574", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19574", "abs": "https://arxiv.org/abs/2511.19574", "authors": ["Ruizhe Zhang", "Jooyoung Kong", "Dylan S. Small", "William Bekerman"], "title": "Beyond the ACE Score: Replicable Combinations of Adverse Childhood Experiences That Worsen Depression Risk", "comment": null, "summary": "Adverse childhood experiences (ACEs) are categories of childhood abuse, neglect, and household dysfunction. Screening by a single additive ACE score (e.g., a $\\ge 4$ cutoff) has poor individual-level discrimination. We instead identify replicable combinations of ACEs that elevate adult depression risk. Our data turnover framework enables a single research team to explore, confirm, and replicate within one observational dataset while controlling the family-wise error rate. We integrate isotonic subgroup selection (ISS) to estimate a higher-risk subgroup under a monotonicity assumption -- additional ACE exposure or higher intensity cannot reduce depression risk. We pre-specify a risk threshold $τ$ corresponding to roughly a two-fold increase in the odds of depression relative to the no-ACE baseline. Within data turnover, the prespecified component improves power while maintaining FWER control, as demonstrated in simulations. Guided by EDA, we adopt frequency coding for ACE items, retaining intensity information that reduces false positives relative to binary or score codings. The result is a replicable, pattern-based higher-risk subgroup. On held-out BRFSS 2022, we show that, at the same level of specificity (0.95), using our replicable subgroup as the screening rule increases sensitivity by 26\\% compared with an ACE-score cutoff, yielding concrete triggers that are straightforward to implement and help target scarce clinical screening resources toward truly higher-risk profiles."}
{"id": "2511.19677", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19677", "abs": "https://arxiv.org/abs/2511.19677", "authors": ["Benjamin Stockton", "Michele Santacatterina", "Soutrik Mandal", "Charles M. Cleland", "Erinn M. Hade", "Nicholas Illenberger", "Sharon Meropol", "Andrea B. Troxel", "Eva Petkova", "Chang Yu", "Thaddeus Tarpey"], "title": "Clarifying identification and estimation of treatment effects in the Sequential Parallel Comparison Design", "comment": null, "summary": "Sequential parallel comparison design (SPCD) clinical trials aim to adjust active treatment effect estimates for placebo response to minimize the impact of placebo responders on the estimates. This is potentially accomplished using a two stage design by measuring treatment effects among all participants during the first stage, then classifying some placebo arm participants as placebo non-responders who will be re-randomized in the second stage. In this paper, we use causal inference tools to clarify under what assumptions treatment effects can be identified in SPCD trials and what effects the conventional estimators target at each stage of the SPCD trial. We further illustrate the highly influential impact of placebo response misclassification on the second stage estimate. We conclude that the conventional SPCD estimators do not target meaningful treatment effects."}
{"id": "2511.19476", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19476", "abs": "https://arxiv.org/abs/2511.19476", "authors": ["Jin Cui", "Boran Zhao", "Jiajun Xu", "Jiaqi Guo", "Shuo Guan", "Pengju Ren"], "title": "FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection", "comment": null, "summary": "Coreset selection compresses large datasets into compact, representative subsets, reducing the energy and computational burden of training deep neural networks. Existing methods are either: (i) DNN-based, which are tied to model-specific parameters and introduce architectural bias; or (ii) DNN-free, which rely on heuristics lacking theoretical guarantees. Neither approach explicitly constrains distributional equivalence, largely because continuous distribution matching is considered inapplicable to discrete sampling. Moreover, prevalent metrics (e.g., MSE, KL, MMD, CE) cannot accurately capture higher-order moment discrepancies, leading to suboptimal coresets. In this work, we propose FAST, the first DNN-free distribution-matching coreset selection framework that formulates the coreset selection task as a graph-constrained optimization problem grounded in spectral graph theory and employs the Characteristic Function Distance (CFD) to capture full distributional information in the frequency domain. We further discover that naive CFD suffers from a \"vanishing phase gradient\" issue in medium and high-frequency regions; to address this, we introduce an Attenuated Phase-Decoupled CFD. Furthermore, for better convergence, we design a Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high, preserving global structure before refining local details and enabling accurate matching with fewer frequencies while avoiding overfitting. Extensive experiments demonstrate that FAST significantly outperforms state-of-the-art coreset selection methods across all evaluated benchmarks, achieving an average accuracy gain of 9.12%. Compared to other baseline coreset methods, it reduces power consumption by 96.57% and achieves a 2.2x average speedup, underscoring its high performance and energy efficiency."}
{"id": "2511.20575", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.20575", "abs": "https://arxiv.org/abs/2511.20575", "authors": ["Nick Polson", "Vadim Sokolov"], "title": "$MC^2$ Mixed Integer and Linear Programming", "comment": null, "summary": "In this paper, we design $MC^2$ algorithms for Mixed Integer and Linear Programming. By expressing a constrained optimisation as one of simulation from a Boltzmann distribution, we reformulate integer and linear programming as Monte Carlo optimisation problems. The key insight is that solving these optimisation problems requires the ability to simulate from truncated distributions, namely multivariate exponentials and Gaussians. Efficient simulation can be achieved using the algorithms of Kent and Davis. We demonstrate our methodology on portfolio optimisation and the classical farmer problem from stochastic programming. Finally, we conclude with directions for future research."}
{"id": "2511.19642", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19642", "abs": "https://arxiv.org/abs/2511.19642", "authors": ["Wuhuan Deng"], "title": "A Win-Expectancy Framework for Contextualizing Runs Batted In: Introducing ARBI and CRBI", "comment": null, "summary": "Runs Batted IN (RBI) records the number of runs a hitter directly drives in during their plate appearances and reflects a batter's ability to convert opportunities into scoring. Because producing runs determines game outcomes, RBI has long served as a central statistic in evaluating offensive performance. However, traditional RBI treats all batted-in runs equally and ignores th game context in which they occur, such as leverage, score state, and the actual impact of a run on a team's chance of winning. In this paper, we introduce two new context-aware metrics-Adjusted RBI (ARBI) and Contextual RBI (CRBI)-that address the fundamental limitations of RBI by incorporating Win Expectancy (WE). ARBI rescales each RBI according to the change in WE before and after the scoring event, assigning more value to runs that meaningfully shift the likelihood of winning and less to runs scored in low-leverage situations. We then extend this framework to CRBI, which further differentiates RBIs with the same WE change by accounting for the terminal WE at the end of the event. This refinement captures the idea that an RBI increasing WE from, for example, 0.45 to 0.65 has a larger competitive impact than one increasing WE from 0.05 to 0.25, even though both represent a 20% increase. Together, ARBI and CRBI provide calibrated, context-sensitive measures of offensive contribution that more accurately reflect the true value of run production. These metrics modernize the interpretation of RBI and have broad applications in player evaluation, forecasting, contract evaluation, and decision-making in baseball analytics."}
{"id": "2511.19735", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19735", "abs": "https://arxiv.org/abs/2511.19735", "authors": ["Shu Yang", "Margaret Gamalo", "Haoda Fu"], "title": "Integrating RCTs, RWD, AI/ML and Statistics: Next-Generation Evidence Synthesis", "comment": null, "summary": "Randomized controlled trials (RCTs) have been the cornerstone of clinical evidence; however, their cost, duration, and restrictive eligibility criteria limit power and external validity. Studies using real-world data (RWD), historically considered less reliable for establishing causality, are now recognized to be important for generating real-world evidence (RWE). In parallel, artificial intelligence and machine learning (AI/ML) are being increasingly used throughout the drug development process, providing scalability and flexibility but also presenting challenges in interpretability and rigor that traditional statistics do not face. This Perspective argues that the future of evidence generation will not depend on RCTs versus RWD, or statistics versus AI/ML, but on their principled integration. To this end, a causal roadmap is needed to clarify inferential goals, make assumptions explicit, and ensure transparency about tradeoffs. We highlight key objectives of integrative evidence synthesis, including transporting RCT results to broader populations, embedding AI-assisted analyses within RCTs, designing hybrid controlled trials, and extending short-term RCTs with long-term RWD. We also outline future directions in privacy-preserving analytics, uncertainty quantification, and small-sample methods. By uniting statistical rigor with AI/ML innovation, integrative approaches can produce robust, transparent, and policy-relevant evidence, making them a key component of modern regulatory science."}
{"id": "2511.19628", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.19628", "abs": "https://arxiv.org/abs/2511.19628", "authors": ["Jared N. Lakhani", "Etienne Pienaar"], "title": "Optimization and Regularization Under Arbitrary Objectives", "comment": "46 pages, 28 figures, 16 tables", "summary": "This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode."}
{"id": "2511.19628", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.19628", "abs": "https://arxiv.org/abs/2511.19628", "authors": ["Jared N. Lakhani", "Etienne Pienaar"], "title": "Optimization and Regularization Under Arbitrary Objectives", "comment": "46 pages, 28 figures, 16 tables", "summary": "This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode."}
{"id": "2511.19672", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19672", "abs": "https://arxiv.org/abs/2511.19672", "authors": ["Wuhuan Deng", "Scott Nestler"], "title": "Introducing Discipline Score Based on League Overall Swinging Probability", "comment": null, "summary": "Plate discipline is an important feature of a hitter's success. Hitter who are able to recognize good pitches to swing at and balls to take are generally recognized as disciplined hitters. Although there are some metrics that can provide insight into the patience of a hitter, most do not capture the ability of a batter to take balls. In this research, we introduce two new metrics, Discipline Score (DS) and Adjusted Discipline Score (ADS), which evaluate batters' discipline when the pitch is a ball compared with the predicted tendencies of all batters in the league."}
{"id": "2511.19761", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19761", "abs": "https://arxiv.org/abs/2511.19761", "authors": ["Michael Hellstern", "Ali Shojaie"], "title": "Order Selection in Vector Autoregression by Mean Square Information Criterion", "comment": "28 pages, 11 figures, 3 tables", "summary": "Vector autoregressive (VAR) processes are ubiquitously used in economics, finance, and biology. Order selection is an essential step in fitting VAR models. While many order selection methods exist, all come with weaknesses. Order selection by minimizing AIC is a popular approach but is known to consistently overestimate the true order for processes of small dimension. On the other hand, methods based on BIC or the Hannan-Quinn (HQ) criteria are shown to require large sample sizes in order to accurately estimate the order for larger-dimensional processes. We propose the mean square information criterion (MIC) based on the observation that the expected squared error loss is flat once the fitted order reaches or exceeds the true order. MIC is shown to consistently estimate the order of the process under relatively mild conditions. Our simulation results show that MIC offers better performance relative to AIC, BIC, and HQ under misspecification. This advantage is corroborated when forecasting COVID-19 outcomes in New York City. Order selection by MIC is implemented in the micvar R package available on CRAN."}
{"id": "2511.19755", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19755", "abs": "https://arxiv.org/abs/2511.19755", "authors": ["Badih Ghattas", "Alvaro Sanchez San-Benito"], "title": "Clustering Approaches for Mixed-Type Data: A Comparative Study", "comment": null, "summary": "Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R."}
{"id": "2511.20191", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.20191", "abs": "https://arxiv.org/abs/2511.20191", "authors": ["Camilo Cárdenas-Hurtado", "Yunxiao Chen", "Irini Moustaki"], "title": "A Generalized Additive Partial-Mastery Cognitive Diagnosis Model", "comment": "28 pages, 5 figures. Includes online appendix", "summary": "Cognitive diagnosis models (CDMs) are restricted latent class models widely used for measuring attributes of interest in diagnostic assessments in education, psychology, biomedical sciences, and related fields. Partial-mastery CDMs (PM-CDMs) are an important extension of CDMs. They model individuals' status for each attribute to be continuous for measuring the partial mastery level, which relaxes the restrictive discrete-attribute assumption of classical CDMs. As a result, PM-CDMs often yield better fits for real-world data and refined measurement of the substantive attributes of interest. However, these models inherit some strong parametric assumptions from the traditional CDMs about the item response functions and, thus, still suffer from a significant risk of model misspecification. This paper proposes a generalized additive PM-CDM (GaPM-CDM) that substantially relaxes the parametric assumptions of PM-CDMs. This proposal leverages model parsimony and interpretability by modeling each item response function as a mixture of nonparametric monotone functions of attributes. A method for the estimation of GaPM-CDM is developed, which combines the marginal maximum likelihood estimator with a sieve approximation of the nonparametric functions. The new model is applicable under both confirmatory and exploratory settings, depending on whether prior knowledge is available about the relationship between observed variables and attributes. The proposed method is applied to two measurement problems from educational testing and healthcare research, respectively, and further evaluated and compared with PM-CDMs through extensive simulation studies."}
{"id": "2511.19742", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19742", "abs": "https://arxiv.org/abs/2511.19742", "authors": ["Nathaniel Dyrkton", "Shomoita Alam", "Susan Shepherd", "Ibrahim Sana", "Kevin Phelan", "Jay JH Park"], "title": "Anchoring Convenience Survey Samples to a Baseline Census for Vaccine Coverage Monitoring in Global Health", "comment": "5 figures, 2 tables", "summary": "While conducting probabilistic surveys is the gold standard for assessing vaccine coverage, implementing these surveys poses challenges for global health. There is a need for more convenient option that is more affordable and practical. Motivated by childhood vaccine monitoring programs in rural areas of Chad and Niger, we conducted a simulation study to evaluate calibration-weighted design-based and logistic regression-based imputation estimators of the finite-population proportion of MCV1 coverage. These estimators use a hybrid approach that anchors non-probabilistic follow-up survey to probabilistic baseline census to account for selection bias. We explored varying degrees of non-ignorable selection bias (odds ratios from 1.0-1.5), percentage of villages sampled (25-75%), and village-level survey response rate to the follow-up survey (50-80%). Our performance metrics included bias, coverage, and proportion of simulated 95% confidence intervals falling within equivalence margins of 5% and 7.5% (equivalence tolerance). For both adjustment methods, the performance worsened with higher selection bias and lower response rate and generally improved as a larger proportion of villages was sampled. Under the worst scenario with 1.5 OR, 25% village sampled, and 50% survey response rate, both methods showed empirical biases of 2.1% or less, below 95% coverage, and low equivalence tolerances. In more realistic scenarios, the performance of our estimators showed lower biases and close to 95% coverage. For example, at OR$\\leq$1.2, both methods showed high performance, except at the lowest village sampling and participation rates. Our simulations show that a hybrid anchoring survey approach is a feasible survey option for vaccine monitoring."}
{"id": "2511.19771", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19771", "abs": "https://arxiv.org/abs/2511.19771", "authors": ["Wenjie Lan", "Jerome P. Reiter"], "title": "Differentially Private Computation of the Gini Index for Income Inequality", "comment": null, "summary": "The Gini index is a widely reported measure of income inequality. In some settings, the underlying data used to compute the Gini index are confidential. The organization charged with reporting the Gini index may be concerned that its release could leak information about the underlying data. We present an approach for bounding this information leakage by releasing a differentially private version of the Gini index. In doing so, we analyze how adding, deleting, or altering a single observation in any specific dataset can affect the computation of the Gini index; this is known as the local sensitivity. We then derive a smooth upper bound on the local sensitivity. Using this bound, we define a mechanism that adds noise to the Gini index, thereby satisfying differential privacy. Using simulated and genuine income data, we show that the mechanism can reduce the errors from noise injection substantially relative to differentially private algorithms that rely on the global sensitivity, that is, the maximum of the local sensitivities over all possible datasets. We characterize settings where using smooth sensitivity can provide highly accurate estimates, as well as settings where the noise variance is simply too large to provide reliably useful results. We also present a Bayesian post-processing step that provides interval estimates about the value of the Gini index computed with the confidential data."}
{"id": "2511.20457", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20457", "abs": "https://arxiv.org/abs/2511.20457", "authors": ["Afra Kilic", "Kim Batselier"], "title": "A Fully Probabilistic Tensor Network for Regularized Volterra System Identification", "comment": "6 pages, 3 figures, 1 table. Submitted to IFAC 2026. Code available at: https://github.com/afrakilic/BTN_Volterra_Sys_ID", "summary": "Modeling nonlinear systems with Volterra series is challenging because the number of kernel coefficients grows exponentially with the model order. This work introduces Bayesian Tensor Network Volterra kernel machines (BTN-V), extending the Bayesian Tensor Network framework to Volterra system identification. BTN-V represents Volterra kernels using canonical polyadic decomposition, reducing model complexity from O(I^D) to O(DIR). By treating all tensor components and hyperparameters as random variables, BTN-V provides predictive uncertainty estimation at no additional computational cost. Sparsity-inducing hierarchical priors enable automatic rank determination and the learning of fading-memory behavior directly from data, improving interpretability and preventing overfitting. Empirical results demonstrate competitive accuracy, enhanced uncertainty quantification, and reduced computational cost."}
{"id": "2511.20069", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.20069", "abs": "https://arxiv.org/abs/2511.20069", "authors": ["Dáire Healy", "Ilaria Prosdocimi", "Isadora Antoniano-Villalobos"], "title": "Non-stationarities in extreme hourly precipitation over the Piave Basin, northern Italy", "comment": null, "summary": "We study the spatio-temporal features of extremal sub-daily precipitation data over the Piave river basin in northeast Italy using a rich database of observed hourly rainfall. Empirical evidence suggests that both the marginal and dependence structures for extreme precipitation in the area exhibit seasonal patterns, and spatial dependence appears to weaken as events become more extreme. We investigate factors affecting the marginal distributions, the spatial dependence and the interplay between them. Capturing these features is essential to provide a realistic description of extreme precipitation processes in order to better estimate their associated risks. With this aim, we identify various climatic covariates at different spatio-temporal scales and explore their usefulness. We go beyond existing literature by investigating and comparing the performance of recently proposed covariate-dependent models for both the marginal and dependence structures of extremes. Furthermore, a flexible max-id model, which encompasses both asymptotic dependence and independence, is used to learn about the spatio-temporal variability of rainfall processes at extreme levels. We find that modelling non-stationarity only at the marginal level does not fully capture the variability of precipitation extremes, and that it is important to also capture the seasonal variation of extremal dependence."}
{"id": "2511.19796", "categories": ["stat.ME", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19796", "abs": "https://arxiv.org/abs/2511.19796", "authors": ["Stevenson Bolivar", "Rong Chen", "Yuefeng Han"], "title": "Threshold Tensor Factor Model in CP Form", "comment": null, "summary": "This paper proposes a new Threshold Tensor Factor Model in Canonical Polyadic (CP) form for tensor time series. By integrating a thresholding autoregressive structure for the latent factor process into the tensor factor model in CP form, the model captures regime-switching dynamics in the latent factor processes while retaining the parsimony and interpretability of low-rank tensor representations. We develop estimation procedures for the model and establish the theoretical properties of the resulting estimators. Numerical experiments and a real-data application illustrate the practical performance and usefulness of the proposed framework."}
{"id": "2511.20503", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20503", "abs": "https://arxiv.org/abs/2511.20503", "authors": ["Rui Tong"], "title": "Generative Modeling with Manifold Percolation", "comment": "13 pages, 7 figures. Correspondence: Rui.Tong@warwick.ac.uk", "summary": "Generative modeling is typically framed as learning mapping rules, but from an observer's perspective without access to these rules, the task manifests as disentangling the geometric support from the probability distribution. We propose that Continuum Percolation is uniquely suited for this support analysis, as the sampling process effectively projects high-dimensional density estimation onto a geometric counting problem on the support. In this work, we establish a rigorous isomorphism between the topological phase transitions of Random Geometric Graphs and the underlying data manifold in high-dimensional space. By analyzing the relationship between our proposed Percolation Shift metric and FID, we demonstrate that our metric captures structural pathologies (such as implicit mode collapse) where statistical metrics fail. Finally, we translate this topological phenomenon into a differentiable loss function to guide training. Experimental results confirm that this approach not only prevents manifold shrinkage but drives the model toward a state of \"Hyper-Generalization,\" achieving good fidelity and verified topological expansion."}
{"id": "2511.20183", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.20183", "abs": "https://arxiv.org/abs/2511.20183", "authors": ["Nils Baillie", "Baptiste Kerleguer", "Cyril Feau", "Josselin Garnier"], "title": "Efficient multi-fidelity Gaussian process regression for noisy outputs and non-nested experimental designs", "comment": null, "summary": "This paper presents a multi-fidelity Gaussian process surrogate modeling that generalizes the recursive formulation of the auto-regressive model when the high-fidelity and low-fidelity data sets are noisy and not necessarily nested. The estimation of high-fidelity parameters by the EM (expectation-maximization) algorithm is shown to be still possible in this context and a closed-form update formula is derived when the scaling factor is a parametric linear predictor function. This yields a decoupled optimization strategy for the parameter selection that is more efficient and scalable than the direct maximum likelihood maximization. The proposed approach is compared to other multi-fidelity models, and benchmarks for different application cases of increasing complexity are provided."}
{"id": "2511.19960", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.19960", "abs": "https://arxiv.org/abs/2511.19960", "authors": ["Deepra Ghosh", "Sanat K. Sarkar"], "title": "Dependence-Aware False Discovery Rate Control in Two-Sided Gaussian Mean Testing", "comment": null, "summary": "This paper develops a general framework for controlling the false discovery rate (FDR) in multiple testing of Gaussian means against two-sided alternatives. The widely used Benjamini-Hochberg (BH) procedure provides exact FDR control under independence or conservative control under specific one-sided dependence structures, but its validity for correlated two-sided tests has remained an open question. We introduce the notion of positive left-tail dependence under the null (PLTDN), extending classical dependence assumptions to two-sided settings, and show that it ensures valid FDR control for BH-type procedures. Building on this framework, we propose a family of generalized shifted BH (GSBH) methods that incorporate correlation information through simple p-value adjustments. Simulation results demonstrate reliable FDR control and improved power across a range of dependence structures, while an application to an HIV gene expression dataset illustrates the practical effectiveness of the proposed approach."}
{"id": "2511.20558", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20558", "abs": "https://arxiv.org/abs/2511.20558", "authors": ["Xintong Li", "Haoran Zhang", "Xiao Zhou"], "title": "Spatio-Temporal Hierarchical Causal Models", "comment": null, "summary": "The abundance of fine-grained spatio-temporal data, such as traffic sensor networks, offers vast opportunities for scientific discovery. However, inferring causal relationships from such observational data remains challenging, particularly due to unobserved confounders that are specific to units (e.g., geographical locations) yet influence outcomes over time. Most existing methods for spatio-temporal causal inference assume that all confounders are observed, an assumption that is often violated in practice. In this paper, we introduce Spatio-Temporal Hierarchical Causal Models (ST-HCMs), a novel graphical framework that extends hierarchical causal modeling to the spatio-temporal domain. At the core of our approach is the Spatio-Temporal Collapse Theorem, which shows that a complex ST-HCM converges to a simpler flat causal model as the amount of subunit data increases. This theoretical result enables a general procedure for causal identification, allowing ST-HCMs to recover causal effects even in the presence of unobserved, time-invariant unit-level confounders, a scenario where standard non-hierarchical models fail. We validate the effectiveness of our framework on both synthetic and real-world datasets, demonstrating its potential for robust causal inference in complex dynamic systems."}
{"id": "2511.20481", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.20481", "abs": "https://arxiv.org/abs/2511.20481", "authors": ["Leonardo Cefalo", "Crescenza Calculli", "Alessio Pollice"], "title": "Investigating access to support centers for Violence Against Women in Apulia: A Spatial analysis over multiple years", "comment": null, "summary": "In this study, we address the challenge of modelling the spatial variability in violence against women across municipalities in a Southern Italian region by proposing a Bayesian spatio-temporal Poisson regression model. Using data on access to Local Anti-Violence Centers in the Apulia region from 2021 to 2024, we investigate the impact of municipality-level socioeconomic characteristics and local vulnerabilities on both the incidence and reporting of gender-based violence. To explicitly account for spatial dependence, we compare four spatial models within the Integrated Nested Laplace Approximation framework for Bayesian model estimation. We assess the relative fit of the competing models, discussing their prior assumptions, spatial confounding effects, and inferential implications. Our findings indicate that access to support services decreases with distance from the residential municipality, highlighting spatial constraints in reporting and the strategic importance of support center location. Furthermore, lower education levels appear to contribute to under-reporting in disadvantaged areas, while higher economic development may be associated with a lower incidence of reported violence. This study emphasises the critical role of spatial modelling in capturing reporting dynamics and informing policy interventions."}
{"id": "2511.20021", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20021", "abs": "https://arxiv.org/abs/2511.20021", "authors": ["Sjoerd Hermes", "Joost van Heerwaarden", "Fred van Eeuwijk", "Pariya Behrouzi"], "title": "Hierarchical Causal Structure Learning", "comment": null, "summary": "Traditional statistical approaches primarily aim to model associations between variables, but many scientific and practical questions require causal methods instead. These approaches rely on assumptions about an underlying structure, often represented by a directed acyclic graph (DAG). When all variables are measured at the same level, causal structures can be learned using existing techniques. However, no suitable methods exist when data are organized hierarchically or across multiple levels. This paper addresses such cases, where both unit-level and group-level variables are present. These multi-level structures frequently arise in fields such as agriculture, where plants (units) grow within different environments (groups). Building on nonlinear structural causal models, or additive noise models, we propose a method that accommodates unobserved confounders as well as group-specific causal functions. The approach is implemented in the R package HSCM, available at https://CRAN.R-project.org/package=HSCM."}
{"id": "2409.12348", "categories": ["stat.ME", "stat.AP", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2409.12348", "abs": "https://arxiv.org/abs/2409.12348", "authors": ["Heeju Lim", "Jose Alejandro Ordonez", "Victor H. Lachos", "Antonio Punzo"], "title": "Heckman Selection Contaminated Normal Model", "comment": null, "summary": "The Heckman selection model is one of the most well-renounced econometric models in the analysis of data with sample selection. This model is designed to rectify sample selection biases based on the assumption of bivariate normal error terms. However, real data diverge from this assumption in the presence of heavy tails and/or atypical observations. Recently, this assumption has been relaxed via a more flexible Student's t-distribution, which has appealing statistical properties. This paper introduces a novel Heckman selection model using a bivariate contaminated normal distribution for the error terms. We present an efficient ECM algorithm for parameter estimation with closed-form expressions at the E-step based on truncated multinormal distribution formulas. The identifiability of the proposed model is also discussed, and its properties have been examined. Through simulation studies, we compare our proposed model with the normal and Student's t counterparts and investigate the finite-sample properties and the variation in missing rate. Results obtained from two real data analyses showcase the usefulness and effectiveness of our model. The proposed algorithms are implemented in the R package HeckmanEM."}
{"id": "2511.20616", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.20616", "abs": "https://arxiv.org/abs/2511.20616", "authors": ["Yueming Shen", "Christian Pean", "David Dunson", "Samuel Berchuck"], "title": "Discovering Spatial Patterns of Readmission Risk Using a Bayesian Competing Risks Model with Spatially Varying Coefficients", "comment": null, "summary": "Time-to-event models are commonly used to study associations between risk factors and disease outcomes in the setting of electronic health records (EHR). In recent years, focus has intensified on social determinants of health, highlighting the need for methods that account for patients' locations. We propose a Bayesian approach for introducing point-referenced spatial effects into a competing risks proportional hazards model. Our method leverages Gaussian process (GP) priors for spatially varying intercept and slope. To improve computational efficiency under a large number of spatial locations, we implemented a Hilbert space low-rank approximation of the GP. We modeled the baseline hazard curves as piecewise constant, and introduced a novel multiplicative gamma process prior to induce shrinkage and smoothing. A loss-based clustering method was then used on the spatial random effects to identify high-risk regions. We demonstrate the utility of this method through simulation and a real-world analysis of EHR data from Duke Hospital to study readmission risk of elderly patients with upper extremity fractures. Our results showed that the proposed method improved inference efficiency and provided valuable insights for downstream policy decisions."}
{"id": "2511.20052", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20052", "abs": "https://arxiv.org/abs/2511.20052", "authors": ["Hans-Peter Piepho", "Emlyn Williams"], "title": "Rectangular augmented row-column designs generated from contractions", "comment": null, "summary": "Row-column designs play an important role in applications where two orthogonal sources of error need to be controlled for by blocking. Field or greenhouse experiments, in which experimental units are arranged as a rectangular array of experimental units are a prominent example. In plant breeding, the amount of seed available for the treatments to be tested may be so limited that only one experimental unit per treatment can be accommodated. In such settings, augmented designs become an interesting option, where a small set of treatments, for which sufficient seed is available, are replicated across the rectangular layout so that row and column effects, as well as the error variance can be estimated. Here, we consider the use of an auxiliary design, also known as a contraction, to generate an augmented row-column design. We make use of the fact that the efficiency factors of the contraction and the associated augmented design are closely interlinked. A major advantage of this approach is that an efficient contraction can be found by computer search at much higher computational speed than is required for direct search for an efficient augmented design. Two examples are used to illustrate the proposed method."}
{"id": "2511.19761", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19761", "abs": "https://arxiv.org/abs/2511.19761", "authors": ["Michael Hellstern", "Ali Shojaie"], "title": "Order Selection in Vector Autoregression by Mean Square Information Criterion", "comment": "28 pages, 11 figures, 3 tables", "summary": "Vector autoregressive (VAR) processes are ubiquitously used in economics, finance, and biology. Order selection is an essential step in fitting VAR models. While many order selection methods exist, all come with weaknesses. Order selection by minimizing AIC is a popular approach but is known to consistently overestimate the true order for processes of small dimension. On the other hand, methods based on BIC or the Hannan-Quinn (HQ) criteria are shown to require large sample sizes in order to accurately estimate the order for larger-dimensional processes. We propose the mean square information criterion (MIC) based on the observation that the expected squared error loss is flat once the fitted order reaches or exceeds the true order. MIC is shown to consistently estimate the order of the process under relatively mild conditions. Our simulation results show that MIC offers better performance relative to AIC, BIC, and HQ under misspecification. This advantage is corroborated when forecasting COVID-19 outcomes in New York City. Order selection by MIC is implemented in the micvar R package available on CRAN."}
{"id": "2409.12348", "categories": ["stat.ME", "stat.AP", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2409.12348", "abs": "https://arxiv.org/abs/2409.12348", "authors": ["Heeju Lim", "Jose Alejandro Ordonez", "Victor H. Lachos", "Antonio Punzo"], "title": "Heckman Selection Contaminated Normal Model", "comment": null, "summary": "The Heckman selection model is one of the most well-renounced econometric models in the analysis of data with sample selection. This model is designed to rectify sample selection biases based on the assumption of bivariate normal error terms. However, real data diverge from this assumption in the presence of heavy tails and/or atypical observations. Recently, this assumption has been relaxed via a more flexible Student's t-distribution, which has appealing statistical properties. This paper introduces a novel Heckman selection model using a bivariate contaminated normal distribution for the error terms. We present an efficient ECM algorithm for parameter estimation with closed-form expressions at the E-step based on truncated multinormal distribution formulas. The identifiability of the proposed model is also discussed, and its properties have been examined. Through simulation studies, we compare our proposed model with the normal and Student's t counterparts and investigate the finite-sample properties and the variation in missing rate. Results obtained from two real data analyses showcase the usefulness and effectiveness of our model. The proposed algorithms are implemented in the R package HeckmanEM."}
{"id": "2511.20191", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.20191", "abs": "https://arxiv.org/abs/2511.20191", "authors": ["Camilo Cárdenas-Hurtado", "Yunxiao Chen", "Irini Moustaki"], "title": "A Generalized Additive Partial-Mastery Cognitive Diagnosis Model", "comment": "28 pages, 5 figures. Includes online appendix", "summary": "Cognitive diagnosis models (CDMs) are restricted latent class models widely used for measuring attributes of interest in diagnostic assessments in education, psychology, biomedical sciences, and related fields. Partial-mastery CDMs (PM-CDMs) are an important extension of CDMs. They model individuals' status for each attribute to be continuous for measuring the partial mastery level, which relaxes the restrictive discrete-attribute assumption of classical CDMs. As a result, PM-CDMs often yield better fits for real-world data and refined measurement of the substantive attributes of interest. However, these models inherit some strong parametric assumptions from the traditional CDMs about the item response functions and, thus, still suffer from a significant risk of model misspecification. This paper proposes a generalized additive PM-CDM (GaPM-CDM) that substantially relaxes the parametric assumptions of PM-CDMs. This proposal leverages model parsimony and interpretability by modeling each item response function as a mixture of nonparametric monotone functions of attributes. A method for the estimation of GaPM-CDM is developed, which combines the marginal maximum likelihood estimator with a sieve approximation of the nonparametric functions. The new model is applicable under both confirmatory and exploratory settings, depending on whether prior knowledge is available about the relationship between observed variables and attributes. The proposed method is applied to two measurement problems from educational testing and healthcare research, respectively, and further evaluated and compared with PM-CDMs through extensive simulation studies."}
{"id": "2511.20087", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.20087", "abs": "https://arxiv.org/abs/2511.20087", "authors": ["Marco Battiston", "Yu Luo"], "title": "An Infinite BART model", "comment": null, "summary": "Bayesian additive regression trees (BART) are popular Bayesian ensemble models used in regression and classification analysis. Under this modeling framework, the regression function is approximated by an ensemble of decision trees, interpreted as weak learners that capture different features of the data. In this work, we propose a generalization of the BART model that has two main features: first, it automatically selects the number of decision trees using the given data; second, the model allows clusters of observations to have different regression functions since each data point can only use a selection of weak learners, instead of all of them. This model generalization is accomplished by including a binary weight matrix in the conditional distribution of the response variable, which activates only a specific subset of decision trees for each observation. Such a matrix is endowed with an Indian Buffet process prior, and sampled within the MCMC sampler, together with the other BART parameters. We then compare the Infinite BART model with the classic one on simulated and real datasets. Specifically, we provide examples illustrating variable importance, partial dependence and causal estimation."}
{"id": "2511.19677", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19677", "abs": "https://arxiv.org/abs/2511.19677", "authors": ["Benjamin Stockton", "Michele Santacatterina", "Soutrik Mandal", "Charles M. Cleland", "Erinn M. Hade", "Nicholas Illenberger", "Sharon Meropol", "Andrea B. Troxel", "Eva Petkova", "Chang Yu", "Thaddeus Tarpey"], "title": "Clarifying identification and estimation of treatment effects in the Sequential Parallel Comparison Design", "comment": null, "summary": "Sequential parallel comparison design (SPCD) clinical trials aim to adjust active treatment effect estimates for placebo response to minimize the impact of placebo responders on the estimates. This is potentially accomplished using a two stage design by measuring treatment effects among all participants during the first stage, then classifying some placebo arm participants as placebo non-responders who will be re-randomized in the second stage. In this paper, we use causal inference tools to clarify under what assumptions treatment effects can be identified in SPCD trials and what effects the conventional estimators target at each stage of the SPCD trial. We further illustrate the highly influential impact of placebo response misclassification on the second stage estimate. We conclude that the conventional SPCD estimators do not target meaningful treatment effects."}
{"id": "2511.20308", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20308", "abs": "https://arxiv.org/abs/2511.20308", "authors": ["Marian Grendar"], "title": "Wilcoxon-Mann-Whitney Test of No Group Discrimination", "comment": "6 pages", "summary": "The traditional WMW null hypothesis $H_0: F = G$ is erroneously too broad. WMW actually tests narrower $H_0: AUC = 0.5$. Asymptotic distribution of the standardized $U$ statistic (i.e., the empirical AUC) under the correct $H_0$ is derived along with finite sample bias corrections. The traditional alternative hypothesis of stochastic dominance is too narrow. WMW is consistent against $H_1: AUC \\neq 0.5$, as established by Van Dantzig in 1951."}
{"id": "2511.19755", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19755", "abs": "https://arxiv.org/abs/2511.19755", "authors": ["Badih Ghattas", "Alvaro Sanchez San-Benito"], "title": "Clustering Approaches for Mixed-Type Data: A Comparative Study", "comment": null, "summary": "Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R."}
{"id": "2511.20318", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20318", "abs": "https://arxiv.org/abs/2511.20318", "authors": ["Shanshan Luo", "Peng Wu", "Zhi Geng"], "title": "Pseudo-strata learning via maximizing misclassification reward", "comment": null, "summary": "Online advertising aims to increase user engagement and maximize revenue, but users respond heterogeneously to ad exposure. Some users purchase only when exposed to ads, while others purchase regardless of exposure, and still others never purchase. This heterogeneity can be characterized by latent response types, commonly referred to as principal strata, defined by users' joint potential outcomes under exposure and non-exposure. However, users' true strata are unobserved, making direct analysis infeasible. In this article, instead of learning the true strata, we propose a novel approach that learns users' pseudo-strata by leveraging information from an outcome (revenue) observed after the response (purchase). We construct pseudo-strata to classify users and introduce misclassification rewards to quantify the expected revenue gain of pseudo-strata-based policies relative to true strata. Within a Bayesian classification framework, we learn the pseudo-strata by optimizing the expected revenue. To implement these procedures, we introduce identification assumptions and estimation methods, and establish their large-sample properties. Simulation studies show that the proposed method achieves more accurate strata classification and substantially higher revenue than baselines. We further illustrate the method using a large-scale industrial dataset from the Criteo Predictive Search Platform."}
{"id": "2511.19796", "categories": ["stat.ME", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19796", "abs": "https://arxiv.org/abs/2511.19796", "authors": ["Stevenson Bolivar", "Rong Chen", "Yuefeng Han"], "title": "Threshold Tensor Factor Model in CP Form", "comment": null, "summary": "This paper proposes a new Threshold Tensor Factor Model in Canonical Polyadic (CP) form for tensor time series. By integrating a thresholding autoregressive structure for the latent factor process into the tensor factor model in CP form, the model captures regime-switching dynamics in the latent factor processes while retaining the parsimony and interpretability of low-rank tensor representations. We develop estimation procedures for the model and establish the theoretical properties of the resulting estimators. Numerical experiments and a real-data application illustrate the practical performance and usefulness of the proposed framework."}
{"id": "2511.20412", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20412", "abs": "https://arxiv.org/abs/2511.20412", "authors": ["Neng Wang", "Eric V. Slud", "Tianzhou Ma"], "title": "A novel multi-exposure-to-multi-mediator mediation model for imaging genetic study of brain disorders", "comment": null, "summary": "Common psychiatric and brain disorders are highly heritable and affected by a number of genetic risk factors, yet the mechanism by which these genetic factors contribute to the disorders through alterations in brain structure and function remain poorly understood. Contemporary imaging genetic studies integrate genetic and neuroimaging data to investigate how genetic variation contributes to brain disorders via intermediate neuroimaging endophenotypes. However, the large number of potential exposures (genes) and mediators (neuroimaging features) pose new challenges to the traditional mediation analysis. In this paper, we propose a novel multi-exposure-to-multi-mediator mediation model that integrates genetic, neuroimaging and phenotypic data to investigate the \"geneneuroimaging-brain disorder\" mediation pathway. Our method jointly reduces the dimensions of exposures and mediators into low-dimensional aggregators where the mediation effect is maximized. We further introduce sparsity into the loadings to improve the interpretability. To target the bi-convex optimization problem, we implement an efficient alternating direction method of multipliers algorithm with block coordinate updates. We provide theoretical guarantees for the convergence of our algorithm and establish the asymptotic properties of the resulting estimators. Through extensive simulations, we demonstrate that our method outperforms other competing methods in recovering true loadings and true mediation proportions across a wide range of signal strengths, noise levels, and correlation structures. We further illustrate the utility of the method through a mediation analysis that integrates genetic, brain functional connectivity and smoking behavior data from UK Biobank, and identifies critical genes that impact nicotine dependence via changing the functional connectivity in specific brain regions."}
{"id": "2511.20466", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20466", "abs": "https://arxiv.org/abs/2511.20466", "authors": ["Alexis Boulin", "Erik Haufs"], "title": "Extrapolating into the Extremes with Minimum Distance Estimation", "comment": null, "summary": "Understanding complex dependencies and extrapolating beyond observations are key challenges in modeling environmental space-time extremes. To address this, we introduce a simplifying approach that projects a wide range of multivariate exceedance problems onto a univariate peaks-over-threshold problem. In this framework, an estimator is computed by minimizing the $L_2$-distance between the empirical distribution function of the data and the theoretical distribution of the model. Asymptotic properties of this estimator are derived and validated in a simulation study. We evaluated our estimator in the EVA (2025) conference Data Challenge as part of Team Bochum's submission. The challenge provided precipitation data from four runs of LENS2, an ensemble of long-term weather simulations, on a $5 \\times 5$ grid of locations centered at the grid point closest to Asheville, NC. Our estimator achieved a top-three rank in two of six competitive categories and won the overall preliminary challenge against ten competing teams."}
{"id": "2511.19574", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19574", "abs": "https://arxiv.org/abs/2511.19574", "authors": ["Ruizhe Zhang", "Jooyoung Kong", "Dylan S. Small", "William Bekerman"], "title": "Beyond the ACE Score: Replicable Combinations of Adverse Childhood Experiences That Worsen Depression Risk", "comment": null, "summary": "Adverse childhood experiences (ACEs) are categories of childhood abuse, neglect, and household dysfunction. Screening by a single additive ACE score (e.g., a $\\ge 4$ cutoff) has poor individual-level discrimination. We instead identify replicable combinations of ACEs that elevate adult depression risk. Our data turnover framework enables a single research team to explore, confirm, and replicate within one observational dataset while controlling the family-wise error rate. We integrate isotonic subgroup selection (ISS) to estimate a higher-risk subgroup under a monotonicity assumption -- additional ACE exposure or higher intensity cannot reduce depression risk. We pre-specify a risk threshold $τ$ corresponding to roughly a two-fold increase in the odds of depression relative to the no-ACE baseline. Within data turnover, the prespecified component improves power while maintaining FWER control, as demonstrated in simulations. Guided by EDA, we adopt frequency coding for ACE items, retaining intensity information that reduces false positives relative to binary or score codings. The result is a replicable, pattern-based higher-risk subgroup. On held-out BRFSS 2022, we show that, at the same level of specificity (0.95), using our replicable subgroup as the screening rule increases sensitivity by 26\\% compared with an ACE-score cutoff, yielding concrete triggers that are straightforward to implement and help target scarce clinical screening resources toward truly higher-risk profiles."}
{"id": "2511.19742", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19742", "abs": "https://arxiv.org/abs/2511.19742", "authors": ["Nathaniel Dyrkton", "Shomoita Alam", "Susan Shepherd", "Ibrahim Sana", "Kevin Phelan", "Jay JH Park"], "title": "Anchoring Convenience Survey Samples to a Baseline Census for Vaccine Coverage Monitoring in Global Health", "comment": "5 figures, 2 tables", "summary": "While conducting probabilistic surveys is the gold standard for assessing vaccine coverage, implementing these surveys poses challenges for global health. There is a need for more convenient option that is more affordable and practical. Motivated by childhood vaccine monitoring programs in rural areas of Chad and Niger, we conducted a simulation study to evaluate calibration-weighted design-based and logistic regression-based imputation estimators of the finite-population proportion of MCV1 coverage. These estimators use a hybrid approach that anchors non-probabilistic follow-up survey to probabilistic baseline census to account for selection bias. We explored varying degrees of non-ignorable selection bias (odds ratios from 1.0-1.5), percentage of villages sampled (25-75%), and village-level survey response rate to the follow-up survey (50-80%). Our performance metrics included bias, coverage, and proportion of simulated 95% confidence intervals falling within equivalence margins of 5% and 7.5% (equivalence tolerance). For both adjustment methods, the performance worsened with higher selection bias and lower response rate and generally improved as a larger proportion of villages was sampled. Under the worst scenario with 1.5 OR, 25% village sampled, and 50% survey response rate, both methods showed empirical biases of 2.1% or less, below 95% coverage, and low equivalence tolerances. In more realistic scenarios, the performance of our estimators showed lower biases and close to 95% coverage. For example, at OR$\\leq$1.2, both methods showed high performance, except at the lowest village sampling and participation rates. Our simulations show that a hybrid anchoring survey approach is a feasible survey option for vaccine monitoring."}
{"id": "2511.19755", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19755", "abs": "https://arxiv.org/abs/2511.19755", "authors": ["Badih Ghattas", "Alvaro Sanchez San-Benito"], "title": "Clustering Approaches for Mixed-Type Data: A Comparative Study", "comment": null, "summary": "Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R."}
{"id": "2511.20087", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.20087", "abs": "https://arxiv.org/abs/2511.20087", "authors": ["Marco Battiston", "Yu Luo"], "title": "An Infinite BART model", "comment": null, "summary": "Bayesian additive regression trees (BART) are popular Bayesian ensemble models used in regression and classification analysis. Under this modeling framework, the regression function is approximated by an ensemble of decision trees, interpreted as weak learners that capture different features of the data. In this work, we propose a generalization of the BART model that has two main features: first, it automatically selects the number of decision trees using the given data; second, the model allows clusters of observations to have different regression functions since each data point can only use a selection of weak learners, instead of all of them. This model generalization is accomplished by including a binary weight matrix in the conditional distribution of the response variable, which activates only a specific subset of decision trees for each observation. Such a matrix is endowed with an Indian Buffet process prior, and sampled within the MCMC sampler, together with the other BART parameters. We then compare the Infinite BART model with the classic one on simulated and real datasets. Specifically, we provide examples illustrating variable importance, partial dependence and causal estimation."}
