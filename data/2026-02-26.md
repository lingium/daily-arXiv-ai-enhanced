<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 8]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Detecting and Mitigating Group Bias in Heterogeneous Treatment Effects](https://arxiv.org/abs/2602.20383)
*Joel Persson,Jurriën Bakker,Dennis Bohle,Stefan Feuerriegel,Florian von Wangenheim*

Main category: stat.ME

TL;DR: 聚合个体层面预测治疗效果到群体层面会产生系统性偏差，即使CATE模型正确且在随机实验数据上训练，聚合后的预测GATE也不等于真实GATE。本文开发了检测和校正这种群体偏差的统计框架。


<details>
  <summary>Details</summary>
Motivation: 在实践中，预测的治疗效果很少在个体层面解释、报告或审计，而是聚合到更广泛的子群体（如人口统计细分、风险分层或市场）。这种聚合可能诱导群体层面因果效应的系统性偏差。

Method: 开发统一的统计框架：1) 将群体偏差定义为模型隐含GATE与实验识别GATE之间的差异；2) 推导渐近正态估计量；3) 提供易于实施的统计检验；4) 提出基于收缩的偏差校正方法，展示理论最优和实证可行解具有闭式表达式。

Result: 框架完全通用，假设最少，仅需计算样本矩。分析校正检测到的群体偏差对利润最大化个性化定向的经济影响，描述偏差校正何时改变定向决策和利润，以及涉及的权衡。在主要数字平台的大规模实验数据应用中验证了理论结果和实证性能。

Conclusion: 即使CATE模型正确且在随机实验数据上训练，聚合预测CATE到群体层面通常无法恢复相应的GATE。提出的统计框架能够检测和校正这种群体偏差，对个性化定向决策具有重要经济意义。

Abstract: Heterogeneous treatment effects (HTEs) are increasingly estimated using machine learning models that produce highly personalized predictions of treatment effects. In practice, however, predicted treatment effects are rarely interpreted, reported, or audited at the individual level but, instead, are often aggregated to broader subgroups, such as demographic segments, risk strata, or markets. We show that such aggregation can induce systematic bias of the group-level causal effect: even when models for predicting the individual-level conditional average treatment effect (CATE) are correctly specified and trained on data from randomized experiments, aggregating the predicted CATEs up to the group level does not, in general, recover the corresponding group average treatment effect (GATE). We develop a unified statistical framework to detect and mitigate this form of group bias in randomized experiments. We first define group bias as the discrepancy between the model-implied and experimentally identified GATEs, derive an asymptotically normal estimator, and then provide a simple-to-implement statistical test. For mitigation, we propose a shrinkage-based bias-correction, and show that the theoretically optimal and empirically feasible solutions have closed-form expressions. The framework is fully general, imposes minimal assumptions, and only requires computing sample moments. We analyze the economic implications of mitigating detected group bias for profit-maximizing personalized targeting, thereby characterizing when bias correction alters targeting decisions and profits, and the trade-offs involved. Applications to large-scale experimental data at major digital platforms validate our theoretical results and demonstrate empirical performance.

</details>


### [2] [Posterior Mode Guided Dimension Reduction for Bayesian Model Averaging in Heavy-Tailed Linear Regression](https://arxiv.org/abs/2602.20448)
*Shamriddha De,Joyee Ghosh*

Main category: stat.ME

TL;DR: 提出混合MAP与MCMC的两步算法：第一步用ECM进行后验最大化与变量选择，缩小模型空间；第二步在缩减空间上运行Gibbs采样，兼顾计算效率与不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法在大模型空间中易陷入局部最优，而MAP估计无法提供不确定性量化。需要同时解决这两个问题。

Method: 在双曲误差框架下，开发两步ECM引导的MCMC算法：第一步用ECM进行后验最大化与变量选择，识别高后验概率的缩减模型空间；第二步在缩减空间上运行Gibbs采样器进行后验计算。

Result: 通过模拟研究和基准真实案例，该方法在变量选择和不确定性量化方面优于多种最先进方法。

Conclusion: 提出的混合方法能同时提高后验计算效率并增强推断丰富性，有效解决了大模型空间中MCMC的局部最优问题和MAP缺乏不确定性量化的问题。

Abstract: For large model spaces, the potential entrapment of Markov chain Monte Carlo (MCMC) based methods with spike-and-slab priors poses significant challenges in posterior computation in regression models. On the other hand, maximum a posteriori (MAP) estimation, which is a more computationally viable alternative, fails to provide uncertainty quantification. To address these problems simultaneously and efficiently, this paper proposes a hybrid method that blends MAP estimation with MCMC-based stochastic search algorithms within a heavy-tailed error framework. Under hyperbolic errors, the current work develops a two-step expectation conditional maximization (ECM) guided MCMC algorithm. In the first step, we conduct an ECM-based posterior maximization and perform variable selection, thereby identifying a reduced model space in a high posterior probability region. In the second step, we execute a Gibbs sampler on the reduced model space for posterior computation. Such a method is expected to improve the efficiency of posterior computation and enhance its inferential richness. Through simulation studies and benchmark real life examples, our proposed method is shown to exhibit several advantages in variable selection and uncertainty quantification over various state-of-the-art methods.

</details>


### [3] [Fast Algorithms for Exact Confidence Intervals in Randomized Experiments with Binary Outcomes](https://arxiv.org/abs/2602.20498)
*Peng Zhang*

Main category: stat.ME

TL;DR: 提出了一种在二元结果随机化实验中构建平均处理效应精确置信区间的方法，相比暴力方法实现了指数级效率提升


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖大样本近似，要么计算成本过高。需要一种既精确又计算高效的方法来构建小样本下的置信区间

Method: 使用随机化检验序列构建精确置信区间。在平衡伯努利设计或匹配对设计中，仅需O(log n)次检验；在一般伯努利设计中需O(n²)次检验

Result: 实现了指数级效率提升，证明了信息论下界表明该速率是最优的。在平衡完全随机化设计中，现有最佳方法需要O(n log n)次检验，形成了鲜明对比

Conclusion: 该方法为小样本随机化实验提供了精确且计算高效的置信区间构建方案，特别适用于平衡伯努利和匹配对设计

Abstract: We construct exact confidence intervals for the average treatment effect in randomized experiments with binary outcomes using sequences of randomization tests. Our approach does not rely on large-sample approximations and is valid for all sample sizes. Under a balanced Bernoulli design or a matched-pairs design, we show that exact confidence intervals can be computed using only $O(\log n)$ randomization tests, yielding an exponential reduction in the number of tests compared to brute-force. We further prove an information-theoretic lower bound showing that this rate is optimal. In contrast, under balanced complete randomization, the most efficient known procedures require $O(n\log n)$ randomization tests (Aronow et al., 2023), establishing a sharp separation between these designs. In addition, we extend our algorithm to general Bernoulli designs using $O(n^2)$ randomization tests.

</details>


### [4] [Error-Controlled Borrowing from External Data Using Wasserstein Ambiguity Sets](https://arxiv.org/abs/2602.20503)
*Yui Kimura,Shu Tamano*

Main category: stat.ME

TL;DR: BOND框架通过Wasserstein模糊集形式化数据非可比性，推导出连续和二元结果的最坏情况均值漂移闭式界，构建分布鲁棒的偏差校正Wald统计量，并通过最大化最坏情况功率代理自动优化借用强度。


<details>
  <summary>Details</summary>
Motivation: 外部数据可以提高临床试验效率，但当前人群与外部人群之间的分布不匹配会威胁推断有效性。现有动态借用方法主要依赖启发式的模拟调参，缺乏理论保证。

Method: 提出BOND框架，通过以当前试验分布为中心的Wasserstein模糊集形式化数据非可比性；推导连续和二元结果的最坏情况均值漂移闭式界；构建分布鲁棒的偏差校正Wald统计量；通过最大化最坏情况功率代理自动确定最优借用强度。

Result: BOND在未测量的异质性下保持名义水平，同时比标准借用方法获得更高的效率增益。模拟研究和真实临床试验应用验证了该方法的有效性。

Conclusion: BOND将启发式参数调优转化为透明、解析可处理的优化问题，为数据借用提供了理论保证的校准框架，具有广泛的适用性。

Abstract: Incorporating external data can improve the efficiency of clinical trials, but distributional mismatches between current and external populations threaten the validity of inference. While numerous dynamic borrowing methods exist, the calibration of their borrowing parameters relies mainly on ad hoc, simulation-based tuning. To overcome this, we propose BOND (Borrowing under Optimal Nonparametric Distributional robustness), a framework that formalizes data noncommensurability through Wasserstein ambiguity sets centered at the current-trial distribution. By deriving sharp, closed-form bounds on the worst-case mean drift for both continuous and binary outcomes, we construct a distributionally robust, bias-corrected Wald statistic that ensures asymptotic type I error control uniformly over the ambiguity set. Importantly, BOND determines the optimal borrowing strength by maximizing a worst-case power proxy, converting heuristic parameter tuning into a transparent, analytically tractable optimization problem. Furthermore, we demonstrate that many prominent borrowing methods can be reparameterized via an effective borrowing weight, rendering our calibration framework broadly applicable. Simulation studies and a real-world clinical trial application confirm that BOND preserves the nominal size under unmeasured heterogeneity while achieving efficiency gains over standard borrowing methods.

</details>


### [5] [Local Fréchet regression with toroidal predictors](https://arxiv.org/abs/2602.20572)
*Chang Jun Im,Jeong Min Jeon*

Main category: stat.ME

TL;DR: 首次提出同时处理度量空间响应变量和环面预测变量的回归框架，引入尊重几何结构的局部常数和局部线性估计器，建立渐近性质，并通过模拟和实际数据验证优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有回归方法通常假设响应变量和预测变量在欧几里得空间中，但实际应用中响应变量可能位于一般度量空间（如形状空间、流形），预测变量可能位于环面（如角度、周期性变量）。需要开发能同时处理这两种非欧几里得结构的回归框架。

Method: 提出内在局部常数和局部线性估计器，这些估计器尊重响应空间（一般度量空间）和预测空间（一般环面）的底层几何结构。局部线性估计器即使在标量响应情况下也是新颖的。方法考虑了数据的几何特性，避免了将数据强行嵌入欧几里得空间带来的扭曲。

Result: 建立了估计器的渐近性质，包括一致性和收敛速率。通过模拟研究验证了方法的有效性，并与现有方法比较显示了优越性能。实际数据应用进一步证明了方法在真实场景中的实用价值。

Conclusion: 该研究首次提供了同时处理度量空间响应和环面预测变量的回归框架，提出的几何感知估计器在理论和实际应用中均表现出色，为处理复杂几何结构数据的回归问题提供了有力工具。

Abstract: We provide the first regression framework that simultaneously accommodates responses taking values in a general metric space and predictors lying on a general torus. We propose intrinsic local constant and local linear estimators that respect the underlying geometries of both the response and predictor spaces. Our local linear estimator is novel even in the case of scalar responses. We further establish their asymptotic properties, including consistency and convergence rates. Simulation studies, together with an application to real data, illustrate the superior performance of the proposed methodology.

</details>


### [6] [Estimating the Partially Linear Zero-Inflated Poisson Regression Model: a Robust Approach Using a EM-like Algorithm](https://arxiv.org/abs/2602.20965)
*María José Llop,Andrea Bergesio,Anne-Françoise Yao*

Main category: stat.ME

TL;DR: 本文提出了PLZIP模型的第一个稳健估计方法，使用EM-like算法处理异常值，证明了算法收敛性和估计量一致性，模拟显示比经典估计更稳健高效。


<details>
  <summary>Details</summary>
Motivation: 计数数据常有过多的零值，传统计数模型难以处理非线性关系和异常值。现有PLZIP模型估计方法基于似然，对异常值和模型假设偏差高度敏感。

Method: 提出了PLZIP模型的第一个稳健估计方法，使用EM-like算法利用模型的混合性质，处理响应变量和协变量中的极端观测值。

Result: 证明了算法收敛性和估计量的一致性。模拟研究在各种污染方案下显示，与经典估计相比，所提估计在有限样本中具有稳健性和效率。

Conclusion: 提出的稳健估计方法能有效处理PLZIP模型中的异常值问题，通过实际数据示例验证了方法的实用性。

Abstract: Count data with an excessive number of zeros frequently arise in fields such as economics, medicine, and public health. Traditional count models often fail to adequately handle such data, especially when the relationship between the response and some predictors is nonlinear. To overcome these limitations, the partially linear zero-inflated Poisson (PLZIP) model has been proposed as a flexible alternative. However, all existing estimation approaches for this model are based on likelihood, which is known to be highly sensitive to outliers and slight deviations from the model assumptions. This article presents the first robust estimation method specifically developed for the PLZIP model. An Expectation-Maximization-like algorithm is used to take advantage of the mixture nature of the model and to address extreme observations in both the response and the covariates. Results of the algorithm convergence and the consistency of the estimators are proved. A simulation study under various contamination schemes showed the robustness and efficiency of the proposed estimators in finite samples, compared to classical estimators. Finally, the application of the methodology is illustrated through an example using real data.

</details>


### [7] [Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation](https://arxiv.org/abs/2602.21031)
*Hayk Gevorgyan,Konstantinos Kalogeropoulos,Angelos Alexopoulos*

Main category: stat.ME

TL;DR: 使用可交换多任务高斯过程进行面板数据因果推断，适用于单处理单元和多处理单元交错处理设计


<details>
  <summary>Details</summary>
Motivation: 为面板数据中的政策评估提供灵活的非线性建模工具，特别是针对交错处理设计场景

Method: 采用可交换多任务高斯过程先验，对处理组和对照组结果进行联合建模，通过后验预测分布获得反事实路径

Result: 开发了点处理和累积处理效应估计方法，提供可信区间量化不确定性，并通过安慰剂验证评估预测准确性

Conclusion: 可交换高斯过程为面板数据政策评估提供了灵活工具，并为交错处理设计提出了新方法

Abstract: We study the use of exchangeable multi-task Gaussian processes (GPs) for causal inference in panel data, applying the framework to two settings: one with a single treated unit subject to a once-and-for-all treatment and another with multiple treated units and staggered treatment adoption. Our approach models the joint evolution of outcomes for treated and control units through a GP prior that ensures exchangeability across units while allowing for flexible nonlinear trends over time. The resulting posterior predictive distribution for the untreated potential outcomes of the treated unit provides a counterfactual path, from which we derive pointwise and cumulative treatment effects, along with credible intervals to quantify uncertainty. We implement several variations of the exchangeable GP model using different kernel functions. To assess prediction accuracy, we conduct a placebo-style validation within the pre-intervention window by selecting a ``fake'' intervention date. Ultimately, this study illustrates how exchangeable GPs serve as a flexible tool for policy evaluation in panel data settings and proposes a novel approach to staggered-adoption designs with a large number of treated and control units.

</details>


### [8] [Empirically Calibrated Conditional Independence Tests](https://arxiv.org/abs/2602.21036)
*Milleno Pan,Antoine de Mathelin,Wesley Tansey*

Main category: stat.ME

TL;DR: ECCIT是一种经验校准的条件独立性检验方法，通过优化对抗性选择来测量和校正基检验的校准误差，从而在保持检验无关性的同时实现有效的FDR控制。


<details>
  <summary>Details</summary>
Motivation: 现有条件独立性检验（CIT）即使在采用FDR控制程序的情况下，在实践中也常常无法提供频率保证。论文指出了两种常见的失败模式：小样本时渐近保证不准确，以及大样本但模型误设时未考虑的依赖关系扭曲检验行为。

Method: ECCIT方法首先为选定的基CIT（如GCM、HRT）优化一个对抗器，该对抗器选择特征和响应函数以最大化校准误差度量。然后拟合一个单调校准映射，根据观察到的校准误差按比例调整基检验的p值。

Result: 在合成和真实数据的实证基准测试中，ECCIT相比现有校准策略，在保持检验无关性的同时，以更高的统计功效实现了有效的FDR控制。

Conclusion: ECCIT通过经验校准方法解决了条件独立性检验中的校准问题，能够在各种情况下提供有效的FDR控制，同时保持较高的统计功效，为因果发现和特征选择提供了更可靠的检验工具。

Abstract: Conditional independence tests (CIT) are widely used for causal discovery and feature selection. Even with false discovery rate (FDR) control procedures, they often fail to provide frequentist guarantees in practice. We highlight two common failure modes: (i) in small samples, asymptotic guarantees for many CITs can be inaccurate and even correctly specified models fail to estimate the noise levels and control the error, and (ii) when sample sizes are large but models are misspecified, unaccounted dependencies skew the test's behavior and fail to return uniform p-values under the null. We propose Empirically Calibrated Conditional Independence Tests (ECCIT), a method that measures and corrects for miscalibration. For a chosen base CIT (e.g., GCM, HRT), ECCIT optimizes an adversary that selects features and response functions to maximize a miscalibration metric. ECCIT then fits a monotone calibration map that adjusts the base-test p-values in proportion to the observed miscalibration. Across empirical benchmarks on synthetic and real data, ECCIT achieves valid FDR with higher power than existing calibration strategies while remaining test agnostic.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [9] [A Corrected Welch Satterthwaite Equation. And: What You Always Wanted to Know About Kish's Effective Sample but Were Afraid to Ask](https://arxiv.org/abs/2602.20912)
*Matthias von Davier*

Main category: stat.AP

TL;DR: 修正Satterthwaite近似公式的自由度估计偏差，通过精确矩匹配调整第四矩估计的影响，在小样本时表现更优


<details>
  <summary>Details</summary>
Motivation: Satterthwaite (1941, 1946) 近似公式在方差分量自由度较小时会产生有偏估计，需要修正以提高估计精度

Method: 通过精确矩匹配推导校正因子，考虑第四矩的估计偏差，对原始Satterthwaite公式进行修正

Result: 修正后的估计量在小样本时能准确匹配期望自由度，而原始Satterthwaite估计量存在显著的下偏；Kish (1965) 有效样本量公式是特殊情况

Conclusion: 提出的校正方法有效解决了Satterthwaite近似在小自由度时的偏差问题，适用于刀切法方差估计、多重插补总方差和Welch异方差检验等应用

Abstract: This article presents a corrected version of the Satterthwaite (1941, 1946) approximation for the degrees of freedom of a weighted sum of independent variance components. The original formula is known to yield biased estimates when component degrees of freedom are small. The correction, derived from exact moment matching, adjusts for the bias by incorporating a factor that accounts for the estimation of fourth moments. We show that Kish's (1965) effective sample size formula emerges as a special case when all variance components are equal, and component degrees of freedom are ignored. Simulation studies demonstrate that the corrected estimator closely matches the expected degrees of freedom even for small component sizes, while the original Satterthwaite estimator exhibits substantial downward bias. Additional applications are discussed, including jackknife variance estimation, multiple imputation total variance, and the Welch test for unequal variances.

</details>


### [10] [On the non-uniformity of the 2026 FIFA World Cup draw](https://arxiv.org/abs/2602.21029)
*László Csató,Martin Becker,Karel Devriesere,Dries Goossens*

Main category: stat.AP

TL;DR: 分析2026年世界杯抽签程序的不均匀性，评估官方抽签机制及47种替代方案，发现官方机制在四种不均匀性度量上最优，但通过调整可大幅降低对第一档球队的不公平性。


<details>
  <summary>Details</summary>
Motivation: 体育赛事小组抽签常引入约束条件（如最小化区域内比赛）以提升吸引力，但这些约束可能导致传统抽签程序不均匀，即各队分配到不同小组的概率不均等。本文旨在量化2026年世界杯抽签程序的不均匀性。

Method: 使用整数规划方法替代递归回溯算法（因计算不可行），分析官方抽签程序及47种合理替代方案（基于四个抽签池和两种小组标签策略的所有排列组合）。

Result: 官方抽签机制在四种不均匀性度量上表现最优。然而，若组织者旨在平等对待第一档抽出的最佳球队，不均匀性可降低超过一半。

Conclusion: 虽然官方世界杯抽签机制在整体不均匀性度量上最优，但通过调整抽签程序可以显著减少对顶级球队的不公平待遇，为赛事组织者提供了改进抽签公平性的可行方案。

Abstract: The group stage of a sports tournament is often made more appealing by introducing additional constraints in the group draw that promote an attractive and balanced group composition. For example, the number of intra-regional group matches is minimised in several World Cups. However, under such constraints, the traditional draw procedure may become non-uniform, meaning that the feasible allocations of the teams into groups are not equally likely to occur. Our paper quantifies this non-uniformity of the 2026 FIFA World Cup draw for the official draw procedure, as well as for 47 reasonable alternatives implied by all permutations of the four pots and two group labelling policies. We show why simulating with a recursive backtracking algorithm is intractable, and propose a workable implementation using integer programming. The official draw mechanism is found to be optimal based on four measures of non-uniformity. Nonetheless, non-uniformity can be more than halved if the organiser aims to treat the best teams drawn from the first pot equally.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [11] [cyclinbayes: Bayesian Causal Discovery with Linear Non-Gaussian Directed Acyclic and Cyclic Graphical Models](https://arxiv.org/abs/2602.21170)
*Robert Lee,Raymond K. W. Wong,Yang Ni*

Main category: stat.CO

TL;DR: cyclinbayes是一个用于发现线性和循环因果关系的R包，采用贝叶斯方法和尖峰-平板先验，提供全面的不确定性量化，并实现了可扩展的混合MCMC算法。


<details>
  <summary>Details</summary>
Motivation: 现有软件存在两个主要限制：1) 线性非高斯DAG学习方法通常缺乏适当的不确定性量化；2) 线性非高斯DCG的可靠实现仍然稀缺。

Method: 使用尖峰-平板先验的贝叶斯方法学习有向无环图和有向循环图，采用计算高效的混合MCMC算法，并提出基于后验期望损失的新决策理论方法来总结图的后验样本。

Result: 开发了cyclinbayes R包，提供全面的不确定性量化（包括后验边包含概率、网络基序后验概率和整个图结构的后验概率），并能扩展到大型数据集。

Conclusion: cyclinbayes填补了现有软件在不确定性量化和循环因果结构学习方面的空白，为研究人员提供了一个强大的工具来发现线性和循环因果关系并进行可靠的统计推断。

Abstract: We introduce cyclinbayes, an open-source R package for discovering linear causal relationships with both acyclic and cyclic structures. The package employs scalable Bayesian approaches with spike-and-slab priors to learn directed acyclic graphs (DAGs) and directed cyclic graphs (DCGs) under non-Gaussian noise. A central feature of cyclinbayes is comprehensive uncertainty quantification, including posterior edge inclusion probabilities, posterior probabilities of network motifs, and posterior probabilities over entire graph structures. Our implementation addresses two limitations in existing software: (1) while methods for linear non-Gaussian DAG learning are available in R and Python, they generally lack proper uncertainty quantification, and (2) reliable implementations for linear non-Gaussian DCG remain scarce. The package implements computationally efficient hybrid MCMC algorithms that scale to large datasets. Beyond uncertainty quantification, we propose a new decision-theoretic approach to summarize posterior samples of graphs, yielding principled point estimates based on posterior expected loss such as posterior expected structural Hamming distance and structural intervention distance. The package, a supplementary material, and a tutorial are available on GitHub at https://github.com/roblee01/cyclinbayes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [12] [Gap-Dependent Bounds for Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation](https://arxiv.org/abs/2602.20297)
*Haochen Zhang,Zhong Zheng,Lingzhou Xue*

Main category: stat.ML

TL;DR: 该论文为近极小极大最优算法LSVI-UCB++提供了首个间隙依赖的遗憾界，改进了对特征维度和时间跨度的依赖，并提出了支持多智能体并行探索的并发版本。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然在线性函数逼近的强化学习中建立了间隙依赖的遗憾界，但这些分析不适用于达到近极小极大最优最坏情况遗憾界的算法（如LSVI-UCB++）。需要填补这一空白，为这类算法提供间隙依赖的性能保证。

Method: 1. 为LSVI-UCB++算法提供首个间隙依赖的遗憾界分析；2. 利用LSVI-UCB++的低策略切换特性，设计并发版本支持多智能体并行探索；3. 建立在线多智能体RL的间隙依赖样本复杂度上界。

Result: 1. 获得了改进的间隙依赖遗憾界，相比先前结果在特征维度d和时间跨度H上都有更好依赖；2. 实现了多智能体RL的线性加速，样本复杂度随智能体数量线性提升。

Conclusion: 该工作首次为近极小极大最优算法提供了间隙依赖的理论保证，并扩展到了多智能体设置，为高效并行探索提供了理论支持。

Abstract: We study gap-dependent performance guarantees for nearly minimax-optimal algorithms in reinforcement learning with linear function approximation. While prior works have established gap-dependent regret bounds in this setting, existing analyses do not apply to algorithms that achieve the nearly minimax-optimal worst-case regret bound $\tilde{O}(d\sqrt{H^3K})$, where $d$ is the feature dimension, $H$ is the horizon length, and $K$ is the number of episodes. We bridge this gap by providing the first gap-dependent regret bound for the nearly minimax-optimal algorithm LSVI-UCB++ (He et al., 2023). Our analysis yields improved dependencies on both $d$ and $H$ compared to previous gap-dependent results. Moreover, leveraging the low policy-switching property of LSVI-UCB++, we introduce a concurrent variant that enables efficient parallel exploration across multiple agents and establish the first gap-dependent sample complexity upper bound for online multi-agent RL with linear function approximation, achieving linear speedup with respect to the number of agents.

</details>


### [13] [Selecting Optimal Variable Order in Autoregressive Ising Models](https://arxiv.org/abs/2602.20394)
*Shiba Biswal,Marc Vuffray,Andrey Y. Lokhov*

Main category: stat.ML

TL;DR: 提出利用数据底层马尔可夫随机场的图结构来优化自回归模型的变量排序，从而提高采样质量


<details>
  <summary>Details</summary>
Motivation: 自回归模型的性能严重依赖于变量排序，但传统排序方法可能不是最优的。需要一种基于数据底层结构的方法来优化排序

Method: 学习数据的马尔可夫随机场，利用推断出的图结构构建优化的变量排序，在二维图像类模型中实现结构感知排序，减少条件集复杂度

Result: 在离散数据的Ising模型实验中，基于图结构的排序相比朴素排序能产生更高保真度的生成样本

Conclusion: 利用底层图结构信息优化变量排序能显著提升自回归模型的采样质量，为复杂概率分布建模提供了有效方法

Abstract: Autoregressive models enable tractable sampling from learned probability distributions, but their performance critically depends on the variable ordering used in the factorization via complexities of the resulting conditional distributions. We propose to learn the Markov random field describing the underlying data, and use the inferred graphical model structure to construct optimized variable orderings. We illustrate our approach on two-dimensional image-like models where a structure-aware ordering leads to restricted conditioning sets, thereby reducing model complexity. Numerical experiments on Ising models with discrete data demonstrate that graph-informed orderings yield higher-fidelity generated samples compared to naive variable orderings.

</details>


### [14] [Standard Transformers Achieve the Minimax Rate in Nonparametric Regression with $C^{s,λ}$ Targets](https://arxiv.org/abs/2602.20555)
*Yanming Lai,Defeng Sun*

Main category: stat.ML

TL;DR: 本文首次证明标准Transformer能够以任意精度逼近Hölder函数，并证明其在非参数回归中达到极小极大最优速率，为Transformer的强大能力提供理论依据。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在大型语言模型和计算机视觉等领域取得了巨大成功，需要进行严格的理论研究。目前缺乏对标准Transformer逼近能力的理论证明，特别是对于Hölder函数的逼近能力。

Method: 引入两个度量指标：大小元组和维度向量，对Transformer结构进行细粒度刻画。证明标准Transformer能够以任意精度逼近Hölder函数，并推导出标准Transformer的Lipschitz常数上界和记忆容量上界。

Result: 1. 首次证明标准Transformer能够以任意精度逼近Hölder函数；2. 证明标准Transformer在Hölder目标函数的非参数回归中达到极小极大最优速率；3. 推导出标准Transformer的Lipschitz常数上界和记忆容量上界。

Conclusion: 本文为Transformer模型的强大能力提供了理论依据，提出的细粒度结构刻画方法有助于未来研究不同结构Transformer的泛化和优化误差。这些发现从理论上解释了Transformer模型的有效性。

Abstract: The tremendous success of Transformer models in fields such as large language models and computer vision necessitates a rigorous theoretical investigation. To the best of our knowledge, this paper is the first work proving that standard Transformers can approximate Hölder functions $ C^{s,λ}\left([0,1]^{d\times n}\right) $$ (s\in\mathbb{N}_{\geq0},0<λ\leq1) $ under the $L^t$ distance ($t \in [1, \infty]$) with arbitrary precision. Building upon this approximation result, we demonstrate that standard Transformers achieve the minimax optimal rate in nonparametric regression for Hölder target functions. It is worth mentioning that, by introducing two metrics: the size tuple and the dimension vector, we provide a fine-grained characterization of Transformer structures, which facilitates future research on the generalization and optimization errors of Transformers with different structures. As intermediate results, we also derive the upper bounds for the Lipschitz constant of standard Transformers and their memorization capacity, which may be of independent interest. These findings provide theoretical justification for the powerful capabilities of Transformer models.

</details>


### [15] [Characterizing Online and Private Learnability under Distributional Constraints via Generalized Smoothness](https://arxiv.org/abs/2602.20585)
*Moïse Blanchard,Abhishek Shetty,Alexander Rakhlin*

Main category: stat.ML

TL;DR: 该论文研究了在分布对抗环境下的序列决策学习问题，其中对抗者可以从固定分布族U中自适应选择数据生成分布。论文建立了广义平滑性作为可学习性的关键条件，并给出了通用算法和细化界限。


<details>
  <summary>Details</summary>
Motivation: 研究在分布对抗环境下学习的最小假设条件，其中对抗者可以从固定分布族U中自适应选择数据生成分布。目标是理解何时这种问题可以像独立数据情况一样以有利的样本复杂度进行学习。

Method: 1. 引入广义平滑性概念作为分布族U可学习性的关键条件；2. 开发通用算法，无需明确知道U即可在广义平滑对抗者下实现低遗憾；3. 当U已知时，使用组合参数（碎片化数）提供细化界限；4. 建立在线学习与差分隐私的联系，证明广义平滑性也表征了分布约束下的私有可学习性。

Result: 1. 建立了分布族U可学习性的近乎完整刻画：分布族允许每个有限VC维假设类获得VC维依赖的遗憾界限当且仅当它是广义平滑的；2. 提供了通用算法；3. 给出了基于碎片化数的细化界限；4. 证明了广义平滑性也表征了分布约束下的私有可学习性。

Conclusion: 该研究提供了在分布对抗者下可学习性的近乎完整理解，建立了广义平滑性作为关键条件，并展示了与差分隐私的深刻联系，为学习理论中最小假设条件的研究做出了重要贡献。

Abstract: Understanding minimal assumptions that enable learning and generalization is perhaps the central question of learning theory. Several celebrated results in statistical learning theory, such as the VC theorem and Littlestone's characterization of online learnability, establish conditions on the hypothesis class that allow for learning under independent data and adversarial data, respectively. Building upon recent work bridging these extremes, we study sequential decision making under distributional adversaries that can adaptively choose data-generating distributions from a fixed family $U$ and ask when such problems are learnable with sample complexity that behaves like the favorable independent case. We provide a near complete characterization of families $U$ that admit learnability in terms of a notion known as generalized smoothness i.e. a distribution family admits VC-dimension-dependent regret bounds for every finite-VC hypothesis class if and only if it is generalized smooth. Further, we give universal algorithms that achieve low regret under any generalized smooth adversary without explicit knowledge of $U$. Finally, when $U$ is known, we provide refined bounds in terms of a combinatorial parameter, the fragmentation number, that captures how many disjoint regions can carry nontrivial mass under $U$. These results provide a nearly complete understanding of learnability under distributional adversaries. In addition, building upon the surprising connection between online learning and differential privacy, we show that the generalized smoothness also characterizes private learnability under distributional constraints.

</details>


### [16] [Amortized Bayesian inference for actigraph time sheet data from mobile devices](https://arxiv.org/abs/2602.20611)
*Daniel Zhou,Sudipto Banerjee*

Main category: stat.ML

TL;DR: 本文提出了一种用于活动记录仪时间表的摊销贝叶斯推断方法，使用分层动态线性模型分析高分辨率活动数据，实现了概率插补并学习解释变量对加速度大小的时间变化影响。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备和相关技术的发展，健康数据库中积累了大量的运动数据用于研究移动模式和健康结果。现有的统计方法需要与迁移学习和摊销等AI框架兼容，同时需要确保不确定性的完全传播和量化。

Method: 采用贝叶斯分层动态线性模型进行摊销推断，基于加州大学洛杉矶分校公共卫生学院进行的PASTA-LA研究中的活动记录仪数据，实现活动记录仪时间表的概率插补。

Result: 方法不仅实现了活动记录仪时间表的概率插补，还能统计学习解释变量对受试者加速度大小的时间变化影响，为高分辨率活动数据提供了与AI框架兼容的分析工具。

Conclusion: 提出的摊销贝叶斯推断方法为分析高分辨率活动记录仪数据提供了有效的统计框架，能够处理不确定性传播并与现代AI技术兼容，在健康移动数据分析中具有应用价值。

Abstract: Mobile data technologies use ``actigraphs'' to furnish information on health variables as a function of a subject's movement. The advent of wearable devices and related technologies has propelled the creation of health databases consisting of human movement data to conduct research on mobility patterns and health outcomes. Statistical methods for analyzing high-resolution actigraph data depend on the specific inferential context, but the advent of Artificial Intelligence (AI) frameworks require that the methods be congruent to transfer learning and amortization. This article devises amortized Bayesian inference for actigraph time sheets. We pursue a Bayesian approach to ensure full propagation of uncertainty and its quantification using a hierarchical dynamic linear model. We build our analysis around actigraph data from the Physical Activity through Sustainable Transport Approaches in Los Angeles (PASTA-LA) study conducted by the Fielding School of Public Health in the University of California, Los Angeles. Apart from achieving probabilistic imputation of actigraph time sheets, we are also able to statistically learn about the time-varying impact of explanatory variables on the magnitude of acceleration (MAG) for a cohort of subjects.

</details>


### [17] [DANCE: Doubly Adaptive Neighborhood Conformal Estimation](https://arxiv.org/abs/2602.20652)
*Brandon R. Feng,Brian J. Reich,Daniel Beaglehole,Xihaier Luo,David Keetae Park,Shinjae Yoo,Zhechao Huang,Xueyu Mao,Olcay Boz,Jungeum Kim*

Main category: stat.ML

TL;DR: DANCE：基于双重局部自适应最近邻的共形算法，利用嵌入表示直接构建非共形分数，提高预训练模型不确定性量化的效率


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法通常使用logit分数，但对于预训练模型，这可能导致预测集过于保守和低效，特别是当模型未针对目标任务进行校准时

Method: 提出DANCE算法：1）首先从嵌入层拟合任务自适应核回归模型；2）利用学习的核空间生成最终预测集进行不确定性量化；结合两种新颖的非共形分数直接使用数据的嵌入表示

Result: 在多个数据集上测试，DANCE在预测集大小效率和鲁棒性方面优于最先进的局部、任务适应和零样本共形基线方法

Conclusion: DANCE通过双重局部自适应方法有效解决了预训练模型共形预测中的保守性问题，实现了更高效的预测集大小和更好的鲁棒性

Abstract: The recent developments of complex deep learning models have led to unprecedented ability to accurately predict across multiple data representation types. Conformal prediction for uncertainty quantification of these models has risen in popularity, providing adaptive, statistically-valid prediction sets. For classification tasks, conformal methods have typically focused on utilizing logit scores. For pre-trained models, however, this can result in inefficient, overly conservative set sizes when not calibrated towards the target task. We propose DANCE, a doubly locally adaptive nearest-neighbor based conformal algorithm combining two novel nonconformity scores directly using the data's embedded representation. DANCE first fits a task-adaptive kernel regression model from the embedding layer before using the learned kernel space to produce the final prediction sets for uncertainty quantification. We test against state-of-the-art local, task-adapted and zero-shot conformal baselines, demonstrating DANCE's superior blend of set size efficiency and robustness across various datasets.

</details>


### [18] [Is Multi-Distribution Learning as Easy as PAC Learning: Sharp Rates with Bounded Label Noise](https://arxiv.org/abs/2602.21039)
*Rafael Hanashiro,Abhishek Shetty,Patrick Jaillet*

Main category: stat.ML

TL;DR: 多分布学习在标签噪声下无法实现单任务学习的快速1/ε收敛率，而是需要k/ε²的慢速率，除非单独学习每个分布。与贝叶斯最优误差竞争时还会产生k的乘法惩罚，揭示了多源学习的基本统计障碍。


<details>
  <summary>Details</summary>
Motivation: 研究多分布学习的统计复杂性，探索在异构数据源共享结构的情况下，单任务学习中可实现的快速1/ε收敛率是否能扩展到多分布设置，且对分布数量k的依赖最小化。

Method: 在有限标签噪声设置下研究多分布学习问题，开发结构化假设检验框架来量化在有限噪声下验证接近最优性的统计成本。

Result: 发现多分布学习本质上需要k/ε²的慢收敛率，即使在恒定噪声水平下也是如此，除非单独学习每个分布。与每个分布的最优贝叶斯误差竞争时，样本复杂度会产生k的乘法惩罚。

Conclusion: 多分布学习在标签噪声下存在基本统计障碍，无法实现单任务学习的快速收敛率，揭示了随机分类噪声和Massart噪声之间的统计分离，这是多源学习特有的根本性障碍。

Abstract: Towards understanding the statistical complexity of learning from heterogeneous sources, we study the problem of multi-distribution learning. Given $k$ data sources, the goal is to output a classifier for each source by exploiting shared structure to reduce sample complexity. We focus on the bounded label noise setting to determine whether the fast $1/ε$ rates achievable in single-task learning extend to this regime with minimal dependence on $k$. Surprisingly, we show that this is not the case. We demonstrate that learning across $k$ distributions inherently incurs slow rates scaling with $k/ε^2$, even under constant noise levels, unless each distribution is learned separately. A key technical contribution is a structured hypothesis-testing framework that captures the statistical cost of certifying near-optimality under bounded noise-a cost we show is unavoidable in the multi-distribution setting.
  Finally, we prove that when competing with the stronger benchmark of each distribution's optimal Bayes error, the sample complexity incurs a \textit{multiplicative} penalty in $k$. This establishes a \textit{statistical} separation between random classification noise and Massart noise, highlighting a fundamental barrier unique to learning from multiple sources.

</details>


### [19] [An Enhanced Projection Pursuit Tree Classifier with Visual Methods for Assessing Algorithmic Improvements](https://arxiv.org/abs/2602.21130)
*Natalia da Silva,Dianne Cook,Eun-Kyung Lee*

Main category: stat.ML

TL;DR: 本文改进了投影追踪树分类器，通过允许更多分裂和更灵活的类别分组来提升高维多分类问题的性能，并开发了可视化诊断方法来验证改进效果。


<details>
  <summary>Details</summary>
Motivation: 原始投影追踪树分类器的深度受限于类别数量，这种刚性限制在处理复杂分类问题时表现不佳，特别是在具有不等方差协方差结构和非线性类别分离的多分类场景中。

Method: 扩展原始算法，允许更多分裂和更灵活的类别分组；开发两种可视化诊断方法，使用高维可视化技术检查模型拟合；创建交互式网络应用让用户探索原始和改进分类器的行为。

Result: 改进的算法在具有不等方差协方差结构和非线性类别分离的多分类问题上表现更好；可视化诊断方法成功验证了算法改进的实际效用；实现已集成到R包PPtreeExt中。

Conclusion: 通过算法改进和可视化诊断方法的结合，有效提升了投影追踪树分类器在复杂高维分类问题中的性能，同时提供了验证改进效果的系统方法。

Abstract: This paper presents enhancements to the projection pursuit tree classifier and visual diagnostic methods for assessing their impact in high dimensions. The original algorithm uses linear combinations of variables in a tree structure where depth is constrained to be less than the number of classes -- a limitation that proves too rigid for complex classification problems. Our extensions improve performance in multi-class settings with unequal variance-covariance structures and nonlinear class separations by allowing more splits and more flexible class groupings in the projection pursuit computation. Proposing algorithmic improvements is straightforward; demonstrating their actual utility is not. We therefore develop two visual diagnostic approaches to verify that the enhancements perform as intended. Using high-dimensional visualization techniques, we examine model fits on benchmark datasets to assess whether the algorithm behaves as theorized. An interactive web application enables users to explore the behavior of both the original and enhanced classifiers under controlled scenarios. The enhancements are implemented in the R package PPtreeExt.

</details>
