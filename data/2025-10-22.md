<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 10]
- [stat.ME](#stat.ME) [Total: 14]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.CO](#stat.CO) [Total: 1]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Graphical model for tensor factorization by sparse sampling](https://arxiv.org/abs/2510.17886)
*Angelo Giorgio,Riki Nagasawa,Shuta Yokoi,Tomoyuki Obuchi,Hajime Yoshino*

Main category: stat.ML

TL;DR: 该论文研究了基于张量分量稀疏测量的张量分解方法，适用于数据大量缺失的场景（如推荐系统）。在密集极限的高维统计推断框架下，开发了消息传递算法和复制理论来分析性能。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统等场景中大量数据缺失情况下的张量分解问题，通过随机图设计测量方式来处理稀疏交互数据。

Method: 使用消息传递算法进行贝叶斯最优的师生设置测试，并开发了在密集极限下精确的复制理论来分析统计推断性能。

Result: 在密集极限条件下，所提出的方法能够有效处理稀疏测量的张量分解问题，复制理论为性能分析提供了精确的理论基础。

Conclusion: 该研究为处理稀疏数据张量分解提供了有效的算法和理论框架，在推荐系统等实际应用中具有重要价值。

Abstract: We consider tensor factorizations based on sparse measurements of the tensor
components. The measurements are designed in a way that the underlying graph of
interactions is a random graph. The setup will be useful in cases where a
substantial amount of data is missing, as in recommendation systems heavily
used in social network services. In order to obtain theoretical insights on the
setup, we consider statistical inference of the tensor factorization in a high
dimensional limit, which we call as dense limit, where the graphs are large and
dense but not fully connected. We build message-passing algorithms and test
them in a Bayes optimal teacher-student setting. We also develop a replica
theory, which becomes exact in the dense limit,to examine the performance of
statistical inference.

</details>


### [2] [Learning Time-Varying Graphs from Incomplete Graph Signals](https://arxiv.org/abs/2510.17903)
*Chuansen Peng,Xiaojing Shen*

Main category: stat.ML

TL;DR: 提出联合推断时变网络拓扑和填补缺失数据的非凸优化框架，通过ADMM算法同时恢复图拉普拉斯矩阵序列和重建未观测信号，在高度缺失数据情况下表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决从部分观测图信号中联合推断时变网络拓扑和填补缺失数据的挑战性问题，传统解耦方法无法充分利用图与信号域之间的双向信息流。

Method: 提出统一的非凸优化框架，引入融合套索正则化器促进时间平滑性，开发高效的ADMM算法，利用问题结构获得闭式解。

Result: 理论证明ADMM方案收敛到稳定点，获得非渐近统计保证，数值实验显示在收敛速度和图学习与信号恢复的联合精度上显著优于现有方法。

Conclusion: 该联合方法通过双向信息流在高度缺失数据情况下具有优越鲁棒性，能有效捕获网络动态，适用于大规模网络和长时间序列。

Abstract: This paper tackles the challenging problem of jointly inferring time-varying
network topologies and imputing missing data from partially observed graph
signals. We propose a unified non-convex optimization framework to
simultaneously recover a sequence of graph Laplacian matrices while
reconstructing the unobserved signal entries. Unlike conventional decoupled
methods, our integrated approach facilitates a bidirectional flow of
information between the graph and signal domains, yielding superior robustness,
particularly in high missing-data regimes. To capture realistic network
dynamics, we introduce a fused-lasso type regularizer on the sequence of
Laplacians. This penalty promotes temporal smoothness by penalizing large
successive changes, thereby preventing spurious variations induced by noise
while still permitting gradual topological evolution. For solving the joint
optimization problem, we develop an efficient Alternating Direction Method of
Multipliers (ADMM) algorithm, which leverages the problem's structure to yield
closed-form solutions for both the graph and signal subproblems. This design
ensures scalability to large-scale networks and long time horizons. On the
theoretical front, despite the inherent non-convexity, we establish a
convergence guarantee, proving that the proposed ADMM scheme converges to a
stationary point. Furthermore, we derive non-asymptotic statistical guarantees,
providing high-probability error bounds for the graph estimator as a function
of sample size, signal smoothness, and the intrinsic temporal variability of
the graph. Extensive numerical experiments validate the approach, demonstrating
that it significantly outperforms state-of-the-art baselines in both
convergence speed and the joint accuracy of graph learning and signal recovery.

</details>


### [3] [A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models](https://arxiv.org/abs/2510.18777)
*Yen-Chi Chen*

Main category: stat.ML

TL;DR: 本文从频率主义视角统一解释了变分推断、变分自编码器和去噪扩散模型的理论基础，填补了统计学与机器学习之间的教学鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前变分推断的教学在不同学科中存在分裂：统计学中将其视为贝叶斯后验近似方法，而机器学习中VAE和DDM则从频率主义视角开发。这种分裂给统计学家理解现代生成模型带来了障碍。

Method: 从经典的期望最大化算法出发，展示变分推断如何作为难处理E步骤的可扩展解决方案，并说明VAE和DDM是该框架的自然深度学习扩展。

Result: 建立了从经典统计推断到现代生成AI的桥梁，为统计学家提供了理解VAE和DDM的频率主义理论基础。

Conclusion: 通过频率主义视角统一解释VI、VAE和DDM，有助于弥合统计学与机器学习之间的教学鸿沟，促进跨学科理解。

Abstract: While Variational Inference (VI) is central to modern generative models like
Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its
pedagogical treatment is split across disciplines. In statistics, VI is
typically framed as a Bayesian method for posterior approximation. In machine
learning, however, VAEs and DDMs are developed from a Frequentist viewpoint,
where VI is used to approximate a maximum likelihood estimator. This creates a
barrier for statisticians, as the principles behind VAEs and DDMs are hard to
contextualize without a corresponding Frequentist introduction to VI. This
paper provides that introduction: we explain the theory for VI, VAEs, and DDMs
from a purely Frequentist perspective, starting with the classical
Expectation-Maximization (EM) algorithm. We show how VI arises as a scalable
solution for intractable E-steps and how VAEs and DDMs are natural,
deep-learning-based extensions of this framework, thereby bridging the gap
between classical statistical inference and modern generative AI.

</details>


### [4] [Arbitrated Indirect Treatment Comparisons](https://arxiv.org/abs/2510.18071)
*Yixin Fang,Weili He*

Main category: stat.ML

TL;DR: 本文提出了一种新的仲裁间接治疗比较方法，旨在解决MAIC悖论问题，通过估计共同目标人群（重叠人群）的治疗效果来消除不同赞助商分析同一数据时得出矛盾结论的问题。


<details>
  <summary>Details</summary>
Motivation: MAIC方法在健康技术评估中应用日益广泛，但存在"MAIC悖论"问题，即不同赞助商分析相同数据时会得出相互矛盾的治疗效果结论，这是因为每个赞助商隐式地针对不同人群。

Method: 提出仲裁间接治疗比较方法，通过估计共同目标人群（特别是重叠人群）的治疗效果来解决MAIC悖论。该方法重新加权具有个体参与者数据的试验对象，以匹配仅具有汇总数据的试验的协变量汇总统计量。

Result: 新方法能够消除不同赞助商分析同一数据时产生的矛盾结论，确保治疗效果估计的一致性。

Conclusion: 仲裁间接治疗比较方法通过明确定义共同目标人群（重叠人群），有效解决了MAIC悖论问题，为健康技术评估提供了更可靠的分析框架。

Abstract: Matching-adjusted indirect comparison (MAIC) has been increasingly employed
in health technology assessments (HTA). By reweighting subjects from a trial
with individual participant data (IPD) to match the covariate summary
statistics of another trial with only aggregate data (AgD), MAIC facilitates
the estimation of a treatment effect defined with respect to the AgD trial
population. This manuscript introduces a new class of methods, termed
arbitrated indirect treatment comparisons, designed to address the ``MAIC
paradox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox
arises when different sponsors, analyzing the same data, reach conflicting
conclusions regarding which treatment is more effective. The underlying issue
is that each sponsor implicitly targets a different population. To resolve this
inconsistency, the proposed methods focus on estimating treatment effects in a
common target population, specifically chosen to be the overlap population.

</details>


### [5] [Generalization Below the Edge of Stability: The Role of Data Geometry](https://arxiv.org/abs/2510.18120)
*Tongtong Liang,Alexander Cloninger,Rahul Parhi,Yu-Xiang Wang*

Main category: stat.ML

TL;DR: 本文研究了过参数化两层ReLU网络在低于稳定性边界训练时的泛化性能，发现数据几何结构控制着隐式偏差，推导了适应内在维度的泛化边界，并揭示了数据可分割性对梯度下降学习模式的影响。


<details>
  <summary>Details</summary>
Motivation: 理解过参数化神经网络泛化性能的关键在于数据几何结构、网络架构和训练动态之间的相互作用，本文旨在从理论上探索数据几何如何控制这种隐式偏差。

Method: 对支持在低维球混合上的数据分布推导泛化边界，对一系列各向同性分布研究概率质量向单位球面集中的影响，分析ReLU神经元激活阈值与数据可分割性的关系。

Result: 获得了适应内在维度的泛化边界，发现当概率质量向球面集中时泛化率会恶化，建立了数据可分割性与梯度下降学习模式的关系：难以分割的数据促进共享模式学习，易分割数据导致记忆化。

Conclusion: 数据几何结构决定了梯度下降的隐式偏差，难以被ReLU神经元分割的数据促使学习共享模式从而获得良好泛化，而易于分割的数据则导致记忆化，这些理论结果统一了文献中的经验发现。

Abstract: Understanding generalization in overparameterized neural networks hinges on
the interplay between the data geometry, neural architecture, and training
dynamics. In this paper, we theoretically explore how data geometry controls
this implicit bias. This paper presents theoretical results for
overparameterized two-layer ReLU networks trained below the edge of stability.
First, for data distributions supported on a mixture of low-dimensional balls,
we derive generalization bounds that provably adapt to the intrinsic dimension.
Second, for a family of isotropic distributions that vary in how strongly
probability mass concentrates toward the unit sphere, we derive a spectrum of
bounds showing that rates deteriorate as the mass concentrates toward the
sphere. These results instantiate a unifying principle: When the data is harder
to "shatter" with respect to the activation thresholds of the ReLU neurons,
gradient descent tends to learn representations that capture shared patterns
and thus finds solutions that generalize well. On the other hand, for data that
is easily shattered (e.g., data supported on the sphere) gradient descent
favors memorization. Our theoretical results consolidate disparate empirical
findings that have appeared in the literature.

</details>


### [6] [Beating the Winner's Curse via Inference-Aware Policy Optimization](https://arxiv.org/abs/2510.18161)
*Hamsa Bastani,Osbert Bastani,Bryce McLaughlin*

Main category: stat.ML

TL;DR: 提出了一种新的推理感知策略优化方法，通过同时优化估计目标值和策略在统计上显著优于观察策略的概率，来解决优胜者诅咒问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在策略优化过程中容易受到优胜者诅咒的影响，即优化过程会利用预测误差而非找到真正的改进，导致预测的性能改进在下游策略评估中无法得到证实。

Method: 提出了推理感知策略优化方法，该方法不仅优化估计的目标值，还优化策略在统计上显著优于观察策略的概率。通过数学描述这两个目标的帕累托前沿，设计了一个使用机器学习预测反事实结果的策略优化算法。

Result: 通过模拟验证了该方法的有效性，能够帮助决策者根据期望的权衡选择策略，并在测试集上正常进行策略评估。

Conclusion: 推理感知策略优化方法能够有效解决优胜者诅咒问题，为基于丰富个体协变量的自动学习治疗决策策略提供了更可靠的优化框架。

Abstract: There has been a surge of recent interest in automatically learning policies
to target treatment decisions based on rich individual covariates. A common
approach is to train a machine learning model to predict counterfactual
outcomes, and then select the policy that optimizes the predicted objective
value. In addition, practitioners also want confidence that the learned policy
has better performance than the incumbent policy according to downstream policy
evaluation. However, due to the winner's curse-an issue where the policy
optimization procedure exploits prediction errors rather than finding actual
improvements-predicted performance improvements are often not substantiated by
downstream policy optimization. To address this challenge, we propose a novel
strategy called inference-aware policy optimization, which modifies policy
optimization to account for how the policy will be evaluated downstream.
Specifically, it optimizes not only for the estimated objective value, but also
for the chances that the policy will be statistically significantly better than
the observational policy used to collect data. We mathematically characterize
the Pareto frontier of policies according to the tradeoff of these two goals.
Based on our characterization, we design a policy optimization algorithm that
uses machine learning to predict counterfactual outcomes, and then plugs in
these predictions to estimate the Pareto frontier; then, the decision-maker can
select the policy that optimizes their desired tradeoff, after which policy
evaluation can be performed on the test set as usual. Finally, we perform
simulations to illustrate the effectiveness of our methodology.

</details>


### [7] [Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data](https://arxiv.org/abs/2510.18548)
*Ying Yao,Daniel J. Graham*

Main category: stat.ML

TL;DR: 该研究提出了一种结合分位数随机森林和主成分分析的区间预测方法，用于估算未测量道路的年均日交通量，并量化预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有交通量估算模型大多只提供点预测，忽略了预测不确定性，导致在次要道路覆盖不足的情况下交通规划不够稳健。

Method: 采用分位数随机森林模型与主成分分析相结合的方法，生成包含最小值和最大值的交通量预测区间。

Result: 在英格兰和威尔士2000多条次要道路数据上测试，该方法达到88.22%的区间覆盖概率、0.23的归一化平均宽度和7468.47的Winkler评分。

Conclusion: 该框架通过结合机器学习和空间分析，提高了交通量估算的准确性和可解释性，为交通规划提供更稳健的决策支持。

Abstract: Accurate annual average daily traffic (AADT) data are vital for transport
planning and infrastructure management. However, automatic traffic detectors
across national road networks often provide incomplete coverage, leading to
underrepresentation of minor roads. While recent machine learning advances have
improved AADT estimation at unmeasured locations, most models produce only
point predictions and overlook estimation uncertainty. This study addresses
that gap by introducing an interval prediction approach that explicitly
quantifies predictive uncertainty. We integrate a Quantile Random Forest model
with Principal Component Analysis to generate AADT prediction intervals,
providing plausible traffic ranges bounded by estimated minima and maxima.
Using data from over 2,000 minor roads in England and Wales, and evaluated with
specialized interval metrics, the proposed method achieves an interval coverage
probability of 88.22%, a normalized average width of 0.23, and a Winkler Score
of 7,468.47. By combining machine learning with spatial and high-dimensional
analysis, this framework enhances both the accuracy and interpretability of
AADT estimation, supporting more robust and informed transport planning.

</details>


### [8] [The Bias-Variance Tradeoff in Data-Driven Optimization: A Local Misspecification Perspective](https://arxiv.org/abs/2510.18215)
*Haixiang Lan,Luofeng Liao,Adam N. Elmachtoub,Christian Kroer,Henry Lam,Haofeng Zhang*

Main category: stat.ML

TL;DR: 该论文研究了在局部误设情况下数据驱动随机优化方法的相对性能，包括样本平均近似(SAA)、集成估计优化(IEO)和估计后优化(ETO)方法，揭示了它们之间的偏差-方差权衡关系。


<details>
  <summary>Details</summary>
Motivation: 理解SAA和模型驱动方法在复杂上下文相关问题中的相对性能，特别是在模型接近正确设定但存在局部误设的情况下，目前缺乏细粒度的分析结果。

Method: 利用统计学中的连续性理论工具，在局部误设框架下分析SAA、IEO和ETO方法的性能，推导决策偏差的显式表达式。

Result: 发现在局部误设下，SAA、IEO和ETO方法之间存在偏差-方差权衡，偏差和方差的相对重要性取决于局部误设的程度，并识别了(非)重要误设方向。

Conclusion: 提供了对模型驱动优化方法在局部误设情况下性能的几何理解，为方法选择提供了理论指导。

Abstract: Data-driven stochastic optimization is ubiquitous in machine learning and
operational decision-making problems. Sample average approximation (SAA) and
model-based approaches such as estimate-then-optimize (ETO) or integrated
estimation-optimization (IEO) are all popular, with model-based approaches
being able to circumvent some of the issues with SAA in complex
context-dependent problems. Yet the relative performance of these methods is
poorly understood, with most results confined to the dichotomous cases of the
model-based approach being either well-specified or misspecified. We develop
the first results that allow for a more granular analysis of the relative
performance of these methods under a local misspecification setting, which
models the scenario where the model-based approach is nearly well-specified. By
leveraging tools from contiguity theory in statistics, we show that there is a
bias-variance tradeoff between SAA, IEO, and ETO under local misspecification,
and that the relative importance of the bias and the variance depends on the
degree of local misspecification. Moreover, we derive explicit expressions for
the decision bias, which allows us to characterize (un)impactful
misspecification directions, and provide further geometric understanding of the
variance.

</details>


### [9] [Learning under Quantization for High-Dimensional Linear Regression](https://arxiv.org/abs/2510.18259)
*Dechen Zhang,Junwei Su,Difan Zou*

Main category: stat.ML

TL;DR: 本文首次系统性地理论分析了低比特量化对高维线性回归中SGD学习性能的影响，涵盖了数据、标签、参数、激活和梯度等多种量化目标，建立了精确的风险界限。


<details>
  <summary>Details</summary>
Motivation: 尽管低比特量化在大规模模型高效训练中广泛应用，但其对学习性能的理论影响即使在最简单的线性回归场景中也缺乏严谨分析。

Method: 采用新颖的分析框架，分析高维线性回归中有限步SGD在不同量化目标下的表现，包括算法依赖和数据依赖的过量风险界限。

Result: 发现参数、激活和梯度量化会放大训练噪声；数据量化会扭曲数据谱；数据和标签量化引入额外近似和量化误差。乘法量化可消除谱失真，加法量化随批量大小产生有益缩放效应。

Conclusion: 该理论为理解量化如何塑造优化算法学习动态提供了有力视角，为在实用硬件约束下进一步探索学习理论铺平了道路。

Abstract: The use of low-bit quantization has emerged as an indispensable technique for
enabling the efficient training of large-scale models. Despite its widespread
empirical success, a rigorous theoretical understanding of its impact on
learning performance remains notably absent, even in the simplest linear
regression setting. We present the first systematic theoretical study of this
fundamental question, analyzing finite-step stochastic gradient descent (SGD)
for high-dimensional linear regression under a comprehensive range of
quantization targets: data, labels, parameters, activations, and gradients. Our
novel analytical framework establishes precise algorithm-dependent and
data-dependent excess risk bounds that characterize how different quantization
affects learning: parameter, activation, and gradient quantization amplify
noise during training; data quantization distorts the data spectrum; and data
and label quantization introduce additional approximation and quantized error.
Crucially, we prove that for multiplicative quantization (with input-dependent
quantization step), this spectral distortion can be eliminated, and for
additive quantization (with constant quantization step), a beneficial scaling
effect with batch size emerges. Furthermore, for common polynomial-decay data
spectra, we quantitatively compare the risks of multiplicative and additive
quantization, drawing a parallel to the comparison between FP and integer
quantization methods. Our theory provides a powerful lens to characterize how
quantization shapes the learning dynamics of optimization algorithms, paving
the way to further explore learning theory under practical hardware
constraints.

</details>


### [10] [Parametrising the Inhomogeneity Inducing Capacity of a Training Set, and its Impact on Supervised Learning](https://arxiv.org/abs/2510.18332)
*Gargi Roy,Dalia Chakrabarty*

Main category: stat.ML

TL;DR: 本文提出了训练数据集的"不均匀性参数"概念，用于衡量数据中函数关系的不均匀相关性结构，并证明当该参数非零时，高斯过程建模必须使用非平稳过程。


<details>
  <summary>Details</summary>
Motivation: 传统的数据非平稳性概念不足以描述训练数据中函数关系的不均匀相关性结构，需要新的参数化方法来量化这种特性。

Method: 定义了训练数据集的"不均匀性参数"，该参数易于计算，并在多个公开数据集上进行了验证。采用概率高斯过程学习方法，证明当训练集的不均匀性参数非零时，必须使用非平稳过程进行建模。

Result: 在多个公开数据集上计算了不均匀性参数，发现传统的数据非平稳性并不一定导致非零的不均匀性参数。使用非平稳过程对真实世界多元函数进行学习后，测试输入的预测质量和可靠性受到训练数据不均匀性参数的影响。

Conclusion: 训练数据的不均匀性参数是影响模型预测性能的重要因素，当该参数非零时，必须采用非平稳过程进行建模以获得可靠的预测结果。

Abstract: We introduce parametrisation of that property of the available
  training dataset, that necessitates an inhomogeneous correlation
  structure for the function that is learnt as a model of the
  relationship between the pair of variables, observations of which
  comprise the considered training data. We refer to a parametrisation
  of this property of a given training set, as its ``inhomogeneity
  parameter''. It is easy to compute this parameter for small-to-large
  datasets, and we demonstrate such computation on multiple
  publicly-available datasets, while also demonstrating that
  conventional ``non-stationarity'' of data does not imply a non-zero
  inhomogeneity parameter of the dataset. We prove that - within the
  probabilistic Gaussian Process-based learning approach - a training
  set with a non-zero inhomogeneity parameter renders it imperative,
  that the process that is invoked to model the sought function, be
  non-stationary. Following the learning of a real-world multivariate
  function with such a Process, quality and reliability of predictions
  at test inputs, are demonstrated to be affected by the inhomogeneity
  parameter of the training data.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [11] [Measuring deviations from spherical symmetry](https://arxiv.org/abs/2510.18598)
*Lujia Bai,Holger Dette*

Main category: stat.ME

TL;DR: 提出了一种基于最小距离的球对称性偏差度量方法，并开发了相应的估计器，通过模拟研究和真实数据示例验证了方法的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注精确球对称性的统计检验，本文从不同角度出发，提出衡量球对称性偏差的度量方法。

Method: 基于向量(||Y||, Y/||Y||)^T的分布与球对称分布向量(||Y_s||, Y_s/||Y_s||)^T分布之间的最小距离来度量偏差，开发了相应的估计器并提供了渐近理论保证。

Result: 通过模拟研究和真实数据示例验证了所提出方法的适用性和有效性。

Conclusion: 提出的球对称性偏差度量方法为评估分布偏离球对称性提供了新的工具，具有理论和实践价值。

Abstract: Most of the work on checking spherical symmetry assumptions on the
distribution of the $p$-dimensional random vector $Y$ has its focus on
statistical tests for the null hypothesis of exact spherical symmetry. In this
paper, we take a different point of view and propose a measure for the
deviation from spherical symmetry, which is based on the minimum distance
between the distribution of the vector $\big (\|Y\|, Y/ \|Y\| )^\top $ and its
best approximation by a distribution of a vector $\big (\|Y_s\|, Y_s/ \|Y_s \|
)^\top $ corresponding to a random vector $Y_s$ with a spherical distribution.
We develop estimators for the minimum distance with corresponding statistical
guarantees (provided by asymptotic theory) and demonstrate the applicability of
our approach by means of a simulation study and a real data example.

</details>


### [12] [Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects](https://arxiv.org/abs/2510.18843)
*Pawel Morzywolek,Peter B. Gilbert,Alex Luedtke*

Main category: stat.ME

TL;DR: 提出了一个用于评估异质性治疗效果中变量重要性的推断框架，特别适用于医学等高风险领域，帮助决策者理解黑盒治疗推荐算法。


<details>
  <summary>Details</summary>
Motivation: 在医学等高风险领域，决策者不愿依赖黑盒治疗推荐算法，需要可解释的变量重要性评估方法。

Method: 基于半参数理论中的函数值参数最新发展，构建推断框架，即使使用统计机器学习算法量化治疗效果异质性时也有效。

Result: 该方法能够测试给定变量是否对任何个体都重要，变量重要性度量是局部的（可能因人而异），而推断是全局的。

Conclusion: 该框架适用于传染病预防策略等实际应用，为高风险决策提供了可靠的变量重要性评估工具。

Abstract: We provide an inferential framework to assess variable importance for
heterogeneous treatment effects. This assessment is especially useful in
high-risk domains such as medicine, where decision makers hesitate to rely on
black-box treatment recommendation algorithms. The variable importance measures
we consider are local in that they may differ across individuals, while the
inference is global in that it tests whether a given variable is important for
any individual. Our approach builds on recent developments in semiparametric
theory for function-valued parameters, and is valid even when statistical
machine learning algorithms are employed to quantify treatment effect
heterogeneity. We demonstrate the applicability of our method to infectious
disease prevention strategies.

</details>


### [13] [A new test for assessing the covariate effect in ROC curves](https://arxiv.org/abs/2411.17464)
*Arís Fanjul-Hevia,Juan Carlos Pardo-Fernández,Wenceslao González-Manteiga*

Main category: stat.ME

TL;DR: 本文提出了一种新的统计检验方法，用于比较协变量调整ROC曲线和合并ROC曲线，帮助判断是否需要在ROC分析中考虑协变量的影响。


<details>
  <summary>Details</summary>
Motivation: 在诊断测试中，除了诊断变量外通常还有其他协变量信息。这些协变量可能影响ROC曲线的性能表现，因此需要判断是否应该将协变量纳入分析。

Method: 提出了新的统计检验方法来比较协变量调整ROC曲线与合并ROC曲线，通过分析协变量调整、协变量特定和合并ROC曲线来评估协变量的影响。

Result: 开发的方法能够帮助研究者决定是否可以从研究中排除协变量，并分析了这对后续ROC曲线分析的影响。通过真实数据库分析验证了方法的实用性。

Conclusion: 该研究提供了判断协变量在ROC分析中重要性的新工具，有助于更准确地评估诊断测试的性能，特别是在存在协变量影响的情况下。

Abstract: The ROC curve is a statistical tool that analyses the accuracy of a
diagnostic test in which a variable is used to decide whether an individual is
healthy or not. Along with that diagnostic variable it is usual to have
information of some other covariates. In some situations it is advisable to
incorporate that information into the study, as the performance of the ROC
curves can be affected by them. Using the covariate-adjusted, the
covariate-specific or the pooled ROC curves we discuss how to decide if we can
exclude the covariates from our study or not, and the implications this may
have in further analyses of the ROC curve. A new test for comparing the
covariate-adjusted and the pooled ROC curve is proposed, and the problem is
illustrated by analysing a real database.

</details>


### [14] [Accelerating Bayesian Inference via Multi-Fidelity Transport Map Coupling](https://arxiv.org/abs/2510.17946)
*Sanjan C. Muchandimath,Joaquim R. R. A. Martins,Alex A. Gorodetsky*

Main category: stat.ME

TL;DR: 提出了一种多保真度框架，结合MCMC和MLMC方法，通过传输映射耦合算法实现不同保真度级别的相关采样，显著降低了湍流模型参数贝叶斯推断的计算成本。


<details>
  <summary>Details</summary>
Motivation: 计算物理中的湍流模型（如SA模型）存在参数不确定性，影响预测精度。传统贝叶斯推断方法计算成本过高，特别是在依赖高保真度模拟时，难以满足飞机认证分析中对预测可信度的要求。

Method: 开发多保真度框架，结合马尔可夫链蒙特卡洛（MCMC）方法和多级蒙特卡洛（MLMC）估计器，通过新颖的传输映射耦合算法实现不同保真度级别的相关采样。

Result: 在高攻角近失速的NACA0012翼型上，与传统单保真度方法相比，推断成本降低了50%，并为复杂分离流状态下的模型预测提供了现实的置信区间。

Conclusion: 多保真度方法显著改进了湍流参数校准，为实现更准确高效的飞机认证分析铺平了道路。

Abstract: Mathematical models in computational physics contain uncertain parameters
that impact prediction accuracy. In turbulence modeling, this challenge is
especially significant: Reynolds averaged Navier-Stokes (RANS) models, such as
the Spalart-Allmaras (SA) model, are widely used for their speed and robustness
but often suffer from inaccuracies and associated uncertainties due to
imperfect model parameters. Reliable quantification of these uncertainties is
becoming increasingly important in aircraft certification by analysis, where
predictive credibility is critical. Bayesian inference provides a framework to
estimate these parameters and quantify output uncertainty, but traditional
methods are prohibitively expensive, especially when relying on high-fidelity
simulations. We address the challenge of expensive Bayesian parameter
estimation by developing a multi-fidelity framework that combines Markov chain
Monte Carlo (MCMC) methods with multilevel Monte Carlo (MLMC) estimators to
efficiently solve inverse problems. The MLMC approach requires correlated
samples across different fidelity levels, achieved through a novel transport
map-based coupling algorithm. We demonstrate a 50% reduction in inference cost
compared to traditional single-fidelity methods on the challenging NACA0012
airfoil at high angles of attack near stall, while delivering realistic
uncertainty bounds for model predictions in complex separated flow regimes.
These results demonstrate that multi-fidelity approaches significantly improve
turbulence parameter calibration, paving the way for more accurate and
efficient aircraft certification by analysis.

</details>


### [15] [Assessing Monotone Dependence: Area Under the Curve Meets Rank Correlation](https://arxiv.org/abs/2510.17994)
*Eva-Maria Walz,Andreas Eberl,Tilmann Gneiting*

Main category: stat.ME

TL;DR: 本文提出了一种新的单调关联度量方法——非对称等级相关(AGC)和单调关联系数(CMA)，统一了连续变量和二分变量的单调依赖性评估，将Spearman's Rho和AUC等经典方法纳入统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有文献中评估随机变量间单调依赖性的方法分散且不统一，特别是连续变量和二分变量的评估方法存在割裂。需要一种统一的框架来桥接这两种设置。

Method: 引入非对称等级相关(AGC)作为X和Y的中间分布函数变换的协方差除以Y的等级方差，单调关联系数CMA = 1/2(AGC + 1)。建立了AGC和CMA样本版本的中心极限定理，并开发了DeLong型检验。

Result: 当X和Y连续时，AGC对称且等于Spearman's Rho；当Y为二分变量时，CMA等于AUC。新方法成功应用于数据驱动天气预报和大型语言模型不确定性量化的评估。

Conclusion: AGC和CMA提供了一个统一的框架来评估各种线性有序结果的单调依赖性，填补了连续和二分变量评估方法之间的空白，具有广泛的应用价值。

Abstract: The assessment of monotone dependence between random variables $X$ and $Y$ is
a classical problem in statistics and a gamut of application domains.
Consequently, researchers have sought measures of association that are
invariant under strictly increasing transformations of the margins, with the
extant literature being splintered. Rank correlation coefficients, such as
Spearman's Rho and Kendall's Tau, have been studied at great length in the
statistical literature, mostly under the assumption that $X$ and $Y$ are
continuous. In the case of a dichotomous outcome $Y$, receiver operating
characteristic analysis and the asymmetric area under the curve (AUC) measure
are used to assess monotone dependence of $Y$ on a covariate $X$. Here we unify
and extend thus far disconnected strands of literature, by developing common
population level theory, estimators, and tests that bridge continuous and
dichotomous settings and apply to all linearly ordered outcomes. In particular,
we introduce asymmetric grade correlation, AGC$(X,Y)$, as the covariance of the
mid distribution function transforms, or grades, of $X$ and $Y$, divided by the
variance of the grade of $Y$. The coefficient of monotone association then is
CMA$(X,Y) = \frac{1}{2} ($AGC$(X,Y) + 1)$. When $X$ and $Y$ are continuous, AGC
is symmetric and equals Spearman's Rho. When $Y$ is dichotomous, CMA equals
AUC. We establish central limit theorems for the sample versions of AGC and CMA
and develop a test of DeLong type for the equality of AGC or CMA values with a
shared outcome $Y$. In case studies, we apply the new measures to assess
progress in data-driven weather prediction, and to evaluate methods of
uncertainty quantification for large language models.

</details>


### [16] [Cartesian Statistics on Spheres](https://arxiv.org/abs/2510.18068)
*Rudolf Beran*

Main category: stat.ME

TL;DR: 本文提出了方向/轴向数据的非参数估计方法和bootstrap置信集，基于笛卡尔坐标中的经验分布，解决了传统指数族模型因归一化常数难以处理而受限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方向/轴向数据分析主要基于少数低阶指数族分布模型，由于高阶指数族模型的归一化常数缺乏易处理的代数形式，限制了更灵活的建模。需要开发非参数方法来估计方向/轴向均值、离散度等函数。

Method: 基于方向/轴向样本在笛卡尔坐标中的经验分布，提出非参数估计器和bootstrap置信集。还包括多均值方向比较、均值方向趋势估计和限定在紧子集内的q维观测分析。

Result: 开发了方向/轴向数据的非参数估计框架，能够处理各种函数估计问题，包括均值、离散度和分布本身的估计。

Conclusion: 提出的非参数方法克服了传统指数族模型的限制，为方向/轴向数据分析提供了更灵活和通用的工具，适用于各种统计推断问题。

Abstract: Directional data consists of unit vectors in q-dimensions that can be
described in polar or Cartesian coordinates. Axial data can be viewed as a pair
of directions pointed in opposite directions or as a projection matrix of rank
1. Historically, their statistical analysis has largely been based on a few
low-order exponential family models of distributions for random directions or
axes. A lack of tractable algebraic forms for the normalizing constants has
hindered the use of higher-order exponential families for less constrained
modeling. Of interest are functionals of the unknown distribution of the
directional/axial data, such as the directional/axial mean, dispersion, or
distribution itself. This paper outlines nonparametric estimators and bootstrap
confidence sets for such functionals. The procedures are based on the empirical
distribution of the directional/axial sample expressed in Cartesian
coordinates. Sketched as well are nonparametric comparisons among multiple mean
directions or axes, estimation of trend in mean directions, and analysis of
q-dimensional observations restricted to lie in a specified compact subset.

</details>


### [17] [Adaptive Grid-Based Thompson Sampling for Efficient Trajectory Discovery](https://arxiv.org/abs/2510.18099)
*Arindam Fadikar,Abby Stevens,Mickael Binois,Nicholson Collier,Jonathan Ozik*

Main category: stat.ME

TL;DR: 提出了一个面向轨迹的贝叶斯优化方法，通过结合参数和随机种子作为高斯过程代理模型的输入，直接在轨迹层面进行推断，提高了随机模型参数估计的效率。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法通常依赖汇总统计量（如均值、中位数等），这在需要轨迹级别信息时可能限制其有效性。随机模型中每个参数集和随机种子组合都会生成从底层随机过程中采样的个体实现（轨迹）。

Method: 使用高斯过程代理模型，将输入参数和随机种子作为输入；采用公共随机数方法定义基于代理的轨迹似然；引入自适应Thompson采样算法，通过基于似然的过滤和Metropolis-Hastings密集化来优化固定大小的输入网格。

Result: 在随机流行病模型（简单隔室模型和计算要求更高的基于代理的模型）上应用该方法，相对于仅参数推断，显示出改进的采样效率和更快识别数据一致轨迹的能力。

Conclusion: 所提出的轨迹导向贝叶斯优化方法能够将计算集中在输入空间的统计有希望区域，同时平衡探索和利用，在轨迹级别推断方面优于传统方法。

Abstract: Bayesian optimization (BO) is a powerful framework for estimating parameters
of computationally expensive simulation models, particularly in settings where
the likelihood is intractable and evaluations are costly. In stochastic models
every simulation is run with a specific parameter set and an implicit or
explicit random seed, where each parameter set and random seed combination
generates an individual realization, or trajectory, sampled from an underlying
random process. Existing BO approaches typically rely on summary statistics
over the realizations, such as means, medians, or quantiles, potentially
limiting their effectiveness when trajectory-level information is desired. We
propose a trajectory-oriented Bayesian optimization method that incorporates a
Gaussian process (GP) surrogate using both input parameters and random seeds as
inputs, enabling direct inference at the trajectory level. Using a common
random number (CRN) approach, we define a surrogate-based likelihood over
trajectories and introduce an adaptive Thompson Sampling algorithm that refines
a fixed-size input grid through likelihood-based filtering and
Metropolis-Hastings-based densification. This approach concentrates computation
on statistically promising regions of the input space while balancing
exploration and exploitation. We apply the method to stochastic epidemic
models, a simple compartmental and a more computationally demanding agent-based
model, demonstrating improved sampling efficiency and faster identification of
data-consistent trajectories relative to parameter-only inference.

</details>


### [18] [Copula Structural Equation Models for Mediation Pathway Analysis](https://arxiv.org/abs/2510.18115)
*Canyi Chen,Ritoban Kundu,Wei Hao,Peter X. -K. Song*

Main category: stat.ME

TL;DR: 本文对结构方程模型（SEMs）在因果中介路径发现中的应用进行了选择性综述，重点介绍了使用copula依赖建模方法扩展经典线性SEM以处理非高斯数据的最新进展。


<details>
  <summary>Details</summary>
Motivation: 传统SEM方法在处理复杂数据结构（如混合数据类型或非正态数据）时往往依赖临时模型设定，高斯误差假设过于严格，需要更灵活的建模方法。

Method: 通过引入copula依赖建模方法来扩展经典线性SEM，以缓解关键技术限制，提供更大的建模灵活性来分析非高斯数据。

Result: copula方法显著增强了SEM处理非高斯数据的能力，提供了更灵活的建模框架。

Conclusion: copula依赖建模为SEM提供了重要的扩展，能够更好地处理现实世界中的复杂数据结构，推动了因果中介路径发现方法的发展。

Abstract: Structural equation models (SEMs) are fundamental to causal mediation pathway
discovery. However, traditional SEM approaches often rely on \emph{ad hoc}
model specifications when handling complex data structures such as mixed data
types or non-normal data in which Gaussian assumptions for errors are rather
restrictive. The invocation of copula dependence modeling methods to extend the
classical linear SEMs mitigates several of key technical limitations, offering
greater modeling flexibility to analyze non-Gaussian data. This paper presents
a selective review of major developments in this area, highlighting recent
advancements and their methodological implications.

</details>


### [19] [Conformal Inference For Missing Data under Multiple Robust Learning](https://arxiv.org/abs/2510.18149)
*Wenlu Tang,Hongni Wang,Xingcai Zhou,Bei Jiang,Linglong Kong*

Main category: stat.ME

TL;DR: 提出了一种新的CM-MRL方法，用于处理缺失数据下的保形推理问题，结合了分裂保形校准和多稳健经验似然重加权方案，在部分工作模型错误设定时仍能提供可靠的预测区间。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中常见的缺失数据（特别是随机缺失数据）下的保形推理挑战，确保预测区间在边际和条件上的可靠覆盖。

Method: CM-MRL方法结合分裂保形校准与多稳健经验似然重加权方案，通过双重校准重加权完整案例分数，使其分布与MAR假设下的完整校准分布匹配。

Result: 通过经验过程理论证明了估计量的渐近行为，提供了可靠的预测区间覆盖，并展示了区间长度优势结果。数值实验验证了该方法在缺失数据下的有效性。

Conclusion: 提出的CM-MRL方法能够有效处理缺失数据下的保形推理问题，即使在部分模型错误设定时仍能保持预测区间的可靠覆盖性能。

Abstract: We develop a novel approach to tackle the common but challenging problem of
conformal inference for missing data in machine learning, focusing on Missing
at Random (MAR) data. We propose a new procedure Conformal prediction for
Missing data under Multiple Robust Learning (CM--MRL) that combines split
conformal calibration with a multiple robust empirical-likelihood (EL)
reweighting scheme. The method proceeds via a double calibration by reweighting
the complete-case scores by EL so that their distribution matches the full
calibration distribution implied by MAR, even when some working models are
misspecified. We demonstrate the asymptotic behavior of our estimators through
empirical process theory and provide reliable coverage for our prediction
intervals, both marginally and conditionally and we further show an
interval-length dominance result. We show the effectiveness of the proposed
method by several numerical experiments in the presence of missing data.

</details>


### [20] [Non-Parametric Estimation Techniques of Factor Copula Model using Proxies](https://arxiv.org/abs/2510.18241)
*Bahareh Ghanbari,Pavel Krupskiy,Laleh Tafakori,Yan Wang*

Main category: stat.ME

TL;DR: 提出一种基于非参数核估计的新方法来估计因子copula模型中的连接copula，相比传统参数方法能更准确地捕捉复杂依赖结构。


<details>
  <summary>Details</summary>
Motivation: 参数因子copula模型在建模多元依赖方面表现良好，但在高维数据中准确估计连接copula仍然具有挑战性。

Method: 使用非参数核密度估计器来估计连接copula，利用核估计的灵活性来捕捉潜在的依赖结构。

Result: 证明所提出的估计器在温和条件下是一致的，并通过广泛的模拟研究展示了其有效性。

Conclusion: 该方法为建模多元依赖提供了一个有前景的途径，特别是在需要稳健高效估计copula模型的应用中。

Abstract: Parametric factor copula models typically work well in modeling multivariate
dependencies due to their flexibility and ability to capture complex dependency
structures. However, accurately estimating the linking copulas within these
mod- els remains challenging, especially when working with high-dimensional
data. This paper proposes a novel approach for estimating linking copulas based
on a non-parametric kernel estimator. Unlike conventional parametric methods,
our approach utilizes the flexibility of kernel density estimation to capture
the un- derlying dependencies more accurately, particularly in scenarios where
the un- derlying copula structure is complex or unknown. We show that the
proposed estimator is consistent under mild conditions and demonstrate its
effectiveness through extensive simulation studies. Our findings suggest that
the proposed approach offers a promising avenue for modeling multivariate
dependencies, par- ticularly in applications requiring robust and efficient
estimation of copula-based models.

</details>


### [21] [Differentially Private E-Values](https://arxiv.org/abs/2510.18654)
*Daniel Csillag,Diego Mesquita*

Main category: stat.ME

TL;DR: 提出了一个将非私有e值转换为差分私有e值的通用框架，通过新颖的有偏乘性噪声机制确保统计有效性，在保护敏感数据隐私的同时保持强大的统计功效。


<details>
  <summary>Details</summary>
Motivation: e值作为统计推断和风险控制的灵活工具，在最小假设下支持随时和事后有效程序，但许多实际应用依赖敏感数据，e值可能泄露这些数据，需要确保安全发布。

Method: 开发了一种新颖的有偏乘性噪声机制，将非私有e值转换为差分私有e值，同时保持统计有效性。

Result: 差分私有e值具有强大的统计功效，并且渐近地与其非私有对应物一样强大。在在线风险监控、私有医疗保健和保形e预测等实验中证明了方法的有效性。

Conclusion: 该框架为e值提供了实用的隐私保护解决方案，在保护敏感数据的同时保持了统计推断的有效性，具有广泛的应用前景。

Abstract: E-values have gained prominence as flexible tools for statistical inference
and risk control, enabling anytime- and post-hoc-valid procedures under minimal
assumptions. However, many real-world applications fundamentally rely on
sensitive data, which can be leaked through e-values. To ensure their safe
release, we propose a general framework to transform non-private e-values into
differentially private ones. Towards this end, we develop a novel biased
multiplicative noise mechanism that ensures our e-values remain statistically
valid. We show that our differentially private e-values attain strong
statistical power, and are asymptotically as powerful as their non-private
counterparts. Experiments across online risk monitoring, private healthcare,
and conformal e-prediction demonstrate our approach's effectiveness and
illustrate its broad applicability.

</details>


### [22] [Quantifying Periodicity in Non-Euclidean Random Objects](https://arxiv.org/abs/2510.18247)
*Jiazhen Xu,Andrew T. A. Wood,Tao Zou*

Main category: stat.ME

TL;DR: 提出了一种在一般度量空间中量化非欧几里得随机对象周期性的非参数框架，解决了周期估计、参数选择和周期成分提取问题。


<details>
  <summary>Details</summary>
Motivation: 时间变化的非欧几里得随机对象在现代数据分析中日益重要，但量化这类对象的周期性仍缺乏系统方法。现有方法主要依赖欧几里得数据的线性特性，无法直接应用于一般度量空间。

Method: 将周期性估计构建为模型选择问题，开发了周期估计方法、数据驱动的调参策略和周期成分提取技术。理论分析不依赖线性特性，适用于一般度量空间。

Result: 通过模拟研究（成分数据、网络数据、函数数据）验证了方法的准确性。在真实数据集（美国电力构成、纽约交通网络、德国用水曲线）中成功识别了有意义的周期性模式。

Conclusion: 该框架为量化非欧几里得随机对象的周期性提供了系统解决方案，具有理论保证和实际应用价值。

Abstract: Time-varying non-Euclidean random objects are playing a growing role in
modern data analysis, and periodicity is a fundamental characteristic of
time-varying data. However, quantifying periodicity in general non-Euclidean
random objects remains largely unexplored. In this work, we introduce a novel
nonparametric framework for quantifying periodicity in random objects within a
general metric space that lacks Euclidean structures. Our approach formulates
periodicity estimation as a model selection problem and provides methodologies
for period estimation, data-driven tuning parameter selection, and periodic
component extraction. Our theoretical contributions include establishing the
consistency of period estimation without relying on linearity properties used
in the literature for Euclidean data, providing theoretical support for
data-driven tuning parameter selection, and deriving uniform convergence
results for periodic component estimation. Through extensive simulation studies
covering three distinct types of time-varying random objects such as
compositional data, networks, and functional data, we showcase the superior
accuracy achieved by our approach in periodicity quantification. Finally, we
apply our method to various real datasets, including U.S. electricity
generation compositions, New York City transportation networks, and Germany's
water consumption curves, highlighting its practical relevance in identifying
and quantifying meaningful periodic patterns.

</details>


### [23] [A new implementation of Network GARCH Model](https://arxiv.org/abs/2510.18599)
*Peiyi Zhou*

Main category: stat.ME

TL;DR: 提出了广义网络GARCH（GNGARCH）模型，将GARCH动态嵌入广义网络自回归框架中，通过构建的虚拟网络捕捉金融资产收益率的动态波动性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多元ARCH-GARCH模型难以表示结构化网络依赖性和保持简洁性的问题，同时改进现有网络GARCH研究的局限性。

Method: 将GARCH动态嵌入GNAR框架，适应邻近波动持续性、动态条件协方差更新，并允许高阶邻近效应而不仅是直接邻居。

Result: GNGARCH是一个有效的波动率模型，满足金融收益率序列的典型事实，在75只最活跃的美国股票上展示了良好的波动率估计和预测能力。

Conclusion: GNGARCH模型成功捕捉了网络结构中的波动率聚类和溢出效应，为金融时间序列分析提供了更强大的工具。

Abstract: Volatility clustering and spillovers are key features of real-world financial
time series when there are a lot of cross-sectional financial assets. While
network analysis helps connect stocks that are 'similar' or 'correlated', which
is effective to link volatility spillovers between stocks, contemporary
multivariate ARCH-GARCH formulations struggle to represent structured network
dependence and remain parsimonious. We introduce the Generalised Network GARCH
(GNGARCH) model as a network volatility model that embeds the GARCH dynamics
within the Generalised Network Autoregressive (GNAR) framework, to capture the
dynamic volatility of financial asset return by both the asset itself and its
'neighbouring' assets from the constructed virtual network. The proposed
volatility model GNGARCH also addresses the limitations for current studies of
network GARCH by adapting neighbouring volatility persistence, dynamic
conditional covariance updates, and allowing higher-order neighbouring effects
rather than only immediate neighbours. This paper provides the model
derivation, vectorisation and conversion, stationarity conditions, and also an
extension by incorporating threshold coefficients to capture leverage effects.
We show that the GNGARCH is a valid volatility model satisfying the stylised
facts of financial return series through simulation. Parameter estimation is
then performed by using squared returns as variance proxy and minimising a loss
function that is either mean squared error (MSE) or quasi-likelihood (QLIKE).
We apply our model on 75 of the most active US stocks under a virtual network,
and highlight the model's ability in volatility estimation and forecast.

</details>


### [24] [Testing Risk Difference of Two Proportions for Combined Unilateral and Bilateral Data](https://arxiv.org/abs/2510.18834)
*Jia Zhou,Chang-Xing Ma*

Main category: stat.ME

TL;DR: 开发了三种基于似然的检验统计量（似然比检验、Wald型检验和得分检验）来评估两个比例之间的风险差异，适用于具有配对器官和混合单侧/双侧观察的临床研究。


<details>
  <summary>Details</summary>
Motivation: 在具有配对器官的临床研究中，二元结果通常存在受试者内相关性，并可能包含单侧和双侧观察的混合。需要开发适用于这种数据结构的统计方法来评估风险差异。

Method: 在Donner的恒定相关模型下，开发了三种基于似然的检验统计量：似然比检验、Wald型检验和得分检验，用于评估两个比例之间的风险差异。

Result: 模拟研究表明三种检验都能很好地控制I类错误，且功效相当，其中得分检验显示出稍好的稳定性。在耳鼻喉科和眼科数据中的应用验证了这些方法。

Conclusion: 得分检验被推荐用于实际应用和未来具有混合单侧和双侧二元数据的研究。还提供了一个在线计算器用于功效分析和风险差异检验。

Abstract: In clinical studies with paired organs, binary outcomes often exhibit
intra-subject correlation and may include a mixture of unilateral and bilateral
observations. Under Donner's constant correlation model, we develop three
likelihood-based test statistics (the likelihood ratio, Wald-type, and score
tests) for assessing the risk difference between two proportions. Simulation
studies demonstrate good control of type I error and comparable power among the
three tests, with the score test showing slightly better stability.
Applications to otolaryngologic and ophthalmologic data illustrate the methods.
An online calculator is also provided for power analysis and risk difference
testing. The score test is recommended for practical use and future studies
with combined unilateral and bilateral binary data.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [25] [Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN](https://arxiv.org/abs/2510.18252)
*Luis H. Chia*

Main category: stat.AP

TL;DR: 本研究系统评估了信用评分中数据增强的最优比例，发现ADASYN方法在1倍扩增时表现最佳，最优类别不平衡比例为6.6:1，而非常用的1:1平衡。


<details>
  <summary>Details</summary>
Motivation: 信用评分模型面临严重的类别不平衡问题（违约率通常低于10%），现有数据增强技术如SMOTE和ADASYN的最优扩增比例不明确，从业者常默认使用1:1平衡而缺乏实证依据。

Method: 使用Give Me Some Credit数据集（97,243个观测值，7%违约率），系统评估10种数据增强场景，比较SMOTE、BorderlineSMOTE和ADASYN在不同扩增倍数（1x、2x、3x）下的表现，所有模型使用XGBoost训练，在29,173个真实测试集上评估，通过1,000次自助法检验统计显著性。

Result: ADASYN在1倍扩增时表现最优，AUC为0.6778，Gini系数为0.3557，相比基准分别显著提升0.77%和3.00%。更高扩增倍数（2x和3x）导致性能下降，3x扩增时AUC下降0.48%，呈现收益递减规律。最优类别不平衡比例为6.6:1。

Conclusion: 本研究首次提供了信用评分中数据增强最优比例的实证证据，为行业从业者和研究者提供了实用的指导原则，并建立了可复现的框架用于确定其他不平衡领域的最优扩增比例。

Abstract: Credit scoring models face a critical challenge: severe class imbalance, with
default rates typically below 10%, which hampers model learning and predictive
performance. While synthetic data augmentation techniques such as SMOTE and
ADASYN have been proposed to address this issue, the optimal augmentation ratio
remains unclear, with practitioners often defaulting to full balancing (1:1
ratio) without empirical justification.
  This study systematically evaluates 10 data augmentation scenarios using the
Give Me Some Credit dataset (97,243 observations, 7% default rate), comparing
SMOTE, BorderlineSMOTE, and ADASYN at different multiplication factors (1x, 2x,
3x). All models were trained using XGBoost and evaluated on a held-out test set
of 29,173 real observations. Statistical significance was assessed using
bootstrap testing with 1,000 iterations.
  Key findings reveal that ADASYN with 1x multiplication (doubling the minority
class) achieved optimal performance with AUC of 0.6778 and Gini coefficient of
0.3557, representing statistically significant improvements of +0.77% and
+3.00% respectively (p = 0.017, bootstrap test). Higher multiplication factors
(2x and 3x) resulted in performance degradation, with 3x showing a -0.48%
decrease in AUC, suggesting a "law of diminishing returns" for synthetic
oversampling. The optimal class imbalance ratio was found to be 6.6:1
(majority:minority), contradicting the common practice of balancing to 1:1.
  This work provides the first empirical evidence of an optimal "sweet spot"
for data augmentation in credit scoring, with practical guidelines for industry
practitioners and researchers working with imbalanced datasets. While
demonstrated on a single representative dataset, the methodology provides a
reproducible framework for determining optimal augmentation ratios in other
imbalanced domains.

</details>


### [26] [Distributional regression for seasonal data: an application to river flows](https://arxiv.org/abs/2510.18639)
*Samuel Perreault,Silvana M. Pesenti,Daniyal Shahzad*

Main category: stat.AP

TL;DR: 提出一个建模框架来估计环境变量的完整日分布，作为时间的函数，结合季节变化和长期趋势，补充传统极端值方法。


<details>
  <summary>Details</summary>
Motivation: 传统保险风险评估方法主要关注罕见极端事件，无法捕捉环境变量的完整动态，特别是中等或频繁损失事件。需要一种能同时考虑季节变化和长期趋势的分布建模方法。

Method: 采用GAMLSS框架，让分布的参数随季节周期变化，作为仅依赖于一年中时间的解释变量的函数，忽略时间依赖性以简化建模。

Result: 将该框架应用于加拿大不列颠哥伦比亚省弗雷泽河三个水文站的日流量数据，分析了2021年初冬的弗雷泽河洪水事件。

Conclusion: 提出的框架能够有效估计环境变量的完整分布，同时捕捉季节变化和长期趋势，为风险评估提供更全面的信息。

Abstract: Risk assessment in casualty insurance, such as flood risk, traditionally
relies on extreme-value methods that emphasizes rare events. These approaches
are well-suited for characterizing tail risk, but do not capture the broader
dynamics of environmental variables such as moderate or frequent loss events.
To complement these methods, we propose a modelling framework for estimating
the full (daily) distribution of environmental variables as a function of time,
that is a distributional version of typical climatological summary statistics,
thereby incorporating both seasonal variation and gradual long-term changes.
Aside from the time trend, to capture seasonal variation our approach
simultaneously estimates the distribution for each instant of the seasonal
cycle, without explicitly modelling the temporal dependence present in the
data. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive
Models for Location, Scale, and Shape), where the parameters of the
distribution vary over the seasonal cycle as a function of explanatory
variables depending only on the time of year, and not on the past values of the
process under study. Ignoring the temporal dependence in the seasonal variation
greatly simplifies the modelling but poses inference challenges that we clarify
and overcome.
  We apply our framework to daily river flow data from three hydrometric
stations along the Fraser River in British Columbia, Canada, and analyse the
flood of the Fraser River in early winter of 2021.

</details>


### [27] [Comparison of Simulation-Guided Design to Closed-Form Power Calculations in Planning a Cluster Randomized Trial with Covariate-Constrained Randomization: A Case Study in Rural Chad](https://arxiv.org/abs/2510.18818)
*Jay JH Park,Rebecca K. Metcalfe,Nathaniel Dyrkton,Yichen Yan,Shomoita Alam,Kevin Phelan,Ibrahim Sana,Susan Shepherd*

Main category: stat.AP

TL;DR: 该研究比较了使用模拟方法与传统公式方法在规划嵌套整群随机试验时的效果，发现传统方法在复杂设计下可能产生误导性结果，而模拟方法能更准确地反映试验设计特点。


<details>
  <summary>Details</summary>
Motivation: 传统整群随机试验设计依赖闭式公式进行功效计算，但在使用协变量约束随机化和嵌套数据结构时，传统方法的实用性有限，可能导致不准确的试验规划。

Method: 通过生成100万次健康区域分配，使用协变量约束随机化平衡基线特征，比较模拟方法与基于WHO推荐ICC值的传统功效计算方法。

Result: 传统计算显示在现实治疗效果下无法达到目标功效，且功效在达到一定群组规模后出现平台期；而模拟方法显示功效随群组规模增加而持续提升，无平台期现象。

Conclusion: 对于具有协变量约束随机化和多层嵌套数据结构的复杂整群随机试验，传统闭式公式规划可能产生误导，模拟方法能显著改善试验规划质量。

Abstract: Current practices for designing cluster-randomized trials (cRCTs) typically
rely on closed-form formulas for power calculations. For cRCTs using
covariate-constrained randomization, the utility of conventional calculations
might be limited, particularly when data is nested. We compared
simulation-based planning of a nested cRCT using covariate-constrained
randomization to conventional power calculations using OptiMAx-Chad as a case
study. OptiMAx-Chad will examine the impact of embedding mass distribution of
small-quantity lipid-based nutrient supplements within an expanded programme on
immunization on first-dose measles-containing vaccine (MCV1) coverage among
children aged 12-24 months in rural villages in Ngouri. Within the 12 health
areas to be randomized, a random subset of villages will be selected for
outcome collection. 1,000,000 assignments of health areas with different
possible village selections were generated using covariate-constrained
randomization to balance baseline village characteristics. The empirically
estimated intracluster correlation coefficient (ICC) and the World Health
Organization (WHO) recommended values of 1/3 and 1/6 were considered. The
desired operating characteristics were 80% power at 0.05 one-sided type I error
rate. Using conventional calculations target power for a realistic treatment
effect could not be achieved with the WHO recommended values. Conventional
calculations also showed a plateau in power after a certain cluster size. Our
simulations matched the design of OptiMAx-Chad with covariate adjustment and
random selection, and showed that power did not plateau. Instead, power
increased with increasing cluster size. Planning complex cRCTs with covariate
constrained randomization and a multi-nested data structure with conventional
closed-form formulas can be misleading. Simulations can improve the planning of
cRCTs.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [28] [Principled Argo Modeling using Vecchia-based Gaussian Processes](https://arxiv.org/abs/2510.18067)
*Nian Liu,Jian Cao*

Main category: stat.CO

TL;DR: 提出了一种基于高斯过程回归的统一框架，用于联合建模全球Argo温度数据，无需空间分区或参数化回归，通过Vecchia近似实现可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有Argo温度建模方法依赖空间分区和预设均值结构，导致建模选择随意且面临设计挑战，需要更统一的数据驱动方法。

Method: 使用通用时空协方差函数的高斯过程回归框架，结合Vecchia近似降低计算复杂度，实现大规模海洋数据分析。

Result: 在2007-2016年1-3月Argo数据上，该方法相比需要分区或参数回归的方法具有更优的预测性能。

Conclusion: 该方法为大规模海洋分析提供了原则性、可扩展且可解释的工具。

Abstract: Argo is an international program that collects temperature and salinity
observations in the upper two kilometers of the global ocean. Most existing
approaches for modeling Argo temperature rely on spatial partitioning, where
data are locally modeled by first estimating a prescribed mean structure and
then fitting Gaussian processes (GPs) to the mean-subtracted anomalies. Such
strategies introduce challenges in designing suitable mean structures and
defining domain partitions, often resulting in ad hoc modeling choices. In this
work, we propose a one-stop Gaussian process regression framework with a
generic spatio-temporal covariance function to jointly model Argo temperature
data across broad spatial domains. Our fully data-driven approach achieves
superior predictive performance compared with methods that require domain
partitioning or parametric regression. To ensure scalability over large spatial
regions, we employ the Vecchia approximation, which reduces the computational
complexity from cubic to quasi-linear in the number of observations while
preserving predictive accuracy. Using Argo data from January to March over the
years 2007-2016, the same dataset used in prior benchmark studies, we
demonstrate that our approach provides a principled, scalable, and
interpretable tool for large-scale oceanographic analysis.

</details>
