<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 14]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Deep Neural Networks as Iterated Function Systems and a Generalization Bound](https://arxiv.org/abs/2601.19958)
*Jonathan Vacher*

Main category: stat.ML

TL;DR: 该论文将深度神经网络与随机迭代函数系统理论联系起来，为生成建模提供了稳定性分析和泛化边界。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在数学分析上存在碎片化问题：稳定性和泛化性通常在不同框架下分别研究。DNN的递归结构可能导致不稳定和训练困难，而生成建模中缺乏严格的泛化理论结果。

Method: 利用随机迭代函数系统理论，将两种重要深度架构视为或规范关联于位置依赖的IFS。从随机动力系统引入结果：(1)在适当收缩性假设下建立不变测度的存在唯一性；(2)为生成建模推导Wasserstein泛化边界，并基于此提出新的训练目标。

Result: 在受控2D示例上验证理论，并在标准图像数据集（MNIST、CelebA、CIFAR-10）上实证评估提出的训练目标。

Conclusion: 通过将深度神经网络与随机迭代函数系统理论连接，为深度架构提供了统一的稳定性分析和泛化理论框架，并推导出可直接控制数据分布与学习转移算子之间近似误差的新训练目标。

Abstract: Deep neural networks (DNNs) achieve remarkable performance on a wide range of tasks, yet their mathematical analysis remains fragmented: stability and generalization are typically studied in disparate frameworks and on a case-by-case basis. Architecturally, DNNs rely on the recursive application of parametrized functions, a mechanism that can be unstable and difficult to train, making stability a primary concern. Even when training succeeds, there are few rigorous results on how well such models generalize beyond the observed data, especially in the generative setting. In this work, we leverage the theory of stochastic Iterated Function Systems (IFS) and show that two important deep architectures can be viewed as, or canonically associated with, place-dependent IFS. This connection allows us to import results from random dynamical systems to (i) establish the existence and uniqueness of invariant measures under suitable contractivity assumptions, and (ii) derive a Wasserstein generalization bound for generative modeling. The bound naturally leads to a new training objective that directly controls the collage-type approximation error between the data distribution and its image under the learned transfer operator. We illustrate the theory on a controlled 2D example and empirically evaluate the proposed objective on standard image datasets (MNIST, CelebA, CIFAR-10).

</details>


### [2] [Minimax Rates for Hyperbolic Hierarchical Learning](https://arxiv.org/abs/2601.20047)
*Divit Rawal,Sriram Vishwanath*

Main category: stat.ML

TL;DR: 论文证明了在标准Lipschitz正则化下，欧几里得空间和双曲空间在分层数据学习中的样本复杂度存在指数级差异：欧几里得表示需要指数级样本，而双曲表示达到信息论最优。


<details>
  <summary>Details</summary>
Motivation: 研究分层数据表示学习中的几何选择问题，探索不同几何空间（欧几里得vs双曲）对学习效率和样本复杂度的影响，为分层结构数据的表示学习提供理论指导。

Method: 1. 建立欧几里得空间的几何障碍：证明有界半径嵌入会导致体积塌缩，将指数级多的树距离点映射到邻近位置；2. 证明双曲空间可避免此障碍：常数失真双曲嵌入允许O(1)-Lipschitz可实现性；3. 使用Fano不等式建立匹配下界；4. 分析秩-k预测空间的几何无关瓶颈。

Result: 1. 欧几里得表示需要指数级Lipschitz常数（exp(Ω(R))）来实现简单分层目标，导致指数级样本复杂度；2. 双曲表示仅需O(1)-Lipschitz常数，可用n=O(mR log m)样本学习；3. 匹配下界Ω(mR log m)证明双曲表示达到信息论最优；4. 任何秩-k预测空间最多只能捕获O(k)个规范分层对比。

Conclusion: 双曲几何在分层数据学习中具有本质优势，能够避免欧几里得空间的体积塌缩问题，实现常数Lipschitz可实现性和最优样本复杂度，为分层结构数据的表示学习提供了理论依据。

Abstract: We prove an exponential separation in sample complexity between Euclidean and hyperbolic representations for learning on hierarchical data under standard Lipschitz regularization. For depth-$R$ hierarchies with branching factor $m$, we first establish a geometric obstruction for Euclidean space: any bounded-radius embedding forces volumetric collapse, mapping exponentially many tree-distant points to nearby locations. This necessitates Lipschitz constants scaling as $\exp(Ω(R))$ to realize even simple hierarchical targets, yielding exponential sample complexity under capacity control. We then show this obstruction vanishes in hyperbolic space: constant-distortion hyperbolic embeddings admit $O(1)$-Lipschitz realizability, enabling learning with $n = O(mR \log m)$ samples. A matching $Ω(mR \log m)$ lower bound via Fano's inequality establishes that hyperbolic representations achieve the information-theoretic optimum. We also show a geometry-independent bottleneck: any rank-$k$ prediction space captures only $O(k)$ canonical hierarchical contrasts.

</details>


### [3] [Efficient Evaluation of LLM Performance with Statistical Guarantees](https://arxiv.org/abs/2601.20251)
*Skyler Wu,Yash Nair,Emmanuel J. Candés*

Main category: stat.ML

TL;DR: FAQ方法通过贝叶斯因子模型和主动查询策略，在固定查询预算下实现更高效的LLM基准测试，相比均匀采样可减少5倍查询量。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在大量基准测试上进行全面评估成本高昂，需要一种在固定查询预算下获得准确置信区间的高效方法。

Method: 提出因子化主动查询(FAQ)：1)使用贝叶斯因子模型利用历史信息；2)采用混合方差减少/主动学习采样策略自适应选择问题；3)通过主动主动推断保持统计有效性。

Result: 在两个基准测试套件上，FAQ相比强基线实现了高达5倍的有效样本量增益，即用5倍更少的查询达到相同置信区间宽度。

Conclusion: FAQ提供了一种高效、有效的LLM基准测试方法，显著降低评估成本，同时保持统计有效性，为可重复评估和未来研究提供支持。

Abstract: Exhaustively evaluating many large language models (LLMs) on a large suite of benchmarks is expensive. We cast benchmarking as finite-population inference and, under a fixed query budget, seek tight confidence intervals (CIs) for model accuracy with valid frequentist coverage. We propose Factorized Active Querying (FAQ), which (a) leverages historical information through a Bayesian factor model; (b) adaptively selects questions using a hybrid variance-reduction/active-learning sampling policy; and (c) maintains validity through Proactive Active Inference -- a finite-population extension of active inference (Zrnic & Candes, 2024) that enables direct question selection while preserving coverage. With negligible overhead cost, FAQ delivers up to $5\times$ effective sample size gains over strong baselines on two benchmark suites, across varying historical-data missingness levels: this means that it matches the CI width of uniform sampling while using up to $5\times$ fewer queries. We release our source code and our curated datasets to support reproducible evaluation and future research.

</details>


### [4] [VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring](https://arxiv.org/abs/2601.20830)
*Waldyn G. Martinez*

Main category: stat.ML

TL;DR: VSCOUT是一个针对高维、非高斯、受污染数据的分布无关框架，用于SPC的回顾性（Phase I）监测，通过ARD-VAE架构、集成潜在异常过滤和变点检测，实现两阶段精炼以获得干净的IC基线。


<details>
  <summary>Details</summary>
Motivation: 现代工业和服务过程产生的高维、非高斯、易受污染数据挑战了经典统计过程控制的基本假设，重尾、多模态、非线性依赖和稀疏特殊原因观测会扭曲基线估计、掩盖真实异常，并阻碍可靠识别受控参考集。

Method: VSCOUT结合自动相关确定变分自编码器（ARD-VAE）架构与基于集成的潜在异常过滤和变点检测。ARD先验隔离最有信息的潜在维度，集成和变点过滤器在确定的潜在空间中识别点状和结构性污染。第二阶段重新训练步骤移除标记观测，仅使用保留的内点重新估计潜在结构。

Result: 在基准数据集上的广泛实验表明，VSCOUT在保持可控误报的同时，对特殊原因结构具有卓越的敏感性，优于经典SPC程序、稳健估计器和现代机器学习基线。

Conclusion: VSCOUT的可扩展性、分布灵活性和对复杂污染模式的鲁棒性，使其成为AI赋能环境中回顾性建模和异常检测的实用有效方法。

Abstract: Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (IC) reference set. To address these challenges, we introduce VSCOUT, a distribution-free framework designed specifically for retrospective (Phase I) monitoring in high-dimensional settings. VSCOUT combines an Automatic Relevance Determination Variational Autoencoder (ARD-VAE) architecture with ensemble-based latent outlier filtering and changepoint detection. The ARD prior isolates the most informative latent dimensions, while the ensemble and changepoint filters identify pointwise and structural contamination within the determined latent space. A second-stage retraining step removes flagged observations and re-estimates the latent structure using only the retained inliers, mitigating masking and stabilizing the IC latent manifold. This two-stage refinement produces a clean and reliable IC baseline suitable for subsequent Phase II deployment. Extensive experiments across benchmark datasets demonstrate that VSCOUT achieves superior sensitivity to special-cause structure while maintaining controlled false alarms, outperforming classical SPC procedures, robust estimators, and modern machine-learning baselines. Its scalability, distributional flexibility, and resilience to complex contamination patterns position VSCOUT as a practical and effective method for retrospective modeling and anomaly detection in AI-enabled environments.

</details>


### [5] [Empirical Likelihood-Based Fairness Auditing: Distribution-Free Certification and Flagging](https://arxiv.org/abs/2601.20269)
*Jie Tang,Chuanlong Xie,Xianli Zeng,Lixing Zhu*

Main category: stat.ML

TL;DR: 提出基于经验似然（EL）的非参数公平性审计框架，用于验证模型公平性约束和识别存在差异性能的人口亚群，无需分布假设且计算高效。


<details>
  <summary>Details</summary>
Motivation: 高风险机器学习应用（如累犯预测、自动化人员筛选）中常存在敏感亚群间的系统性性能差异，引发算法偏见担忧。现有公平性审计方法受限于严格的分布假设或高昂计算成本。

Method: 提出基于经验似然的非参数框架，构建模型性能差异的稳健统计量。该方法通过约束优化配置生成渐近服从卡方或混合卡方分布的差异统计量，无需假设数据分布，支持大规模认证和高效亚群发现。

Result: 经验似然方法优于基于bootstrap的方法，覆盖率更接近名义水平，计算延迟降低数个数量级。在COMPAS数据集上成功识别交叉偏见：非裔美国25岁以下男性阳性预测率显著更高，白人女性相对于总体均值存在系统性低估。

Conclusion: 经验似然框架为公平性审计提供了一种稳健、高效的非参数方法，能够有效验证公平性约束并识别存在差异性能的敏感亚群，解决了现有方法在分布假设和计算效率方面的限制。

Abstract: Machine learning models in high-stakes applications, such as recidivism prediction and automated personnel selection, often exhibit systematic performance disparities across sensitive subpopulations, raising critical concerns regarding algorithmic bias. Fairness auditing addresses these risks through two primary functions: certification, which verifies adherence to fairness constraints; and flagging, which isolates specific demographic groups experiencing disparate treatment. However, existing auditing techniques are frequently limited by restrictive distributional assumptions or prohibitive computational overhead. We propose a novel empirical likelihood-based (EL) framework that constructs robust statistical measures for model performance disparities. Unlike traditional methods, our approach is non-parametric; the proposed disparity statistics follow asymptotically chi-square or mixed chi-square distributions, ensuring valid inference without assuming underlying data distributions. This framework uses a constrained optimization profile that admits stable numerical solutions, facilitating both large-scale certification and efficient subpopulation discovery. Empirically, the EL methods outperform bootstrap-based approaches, yielding coverage rates closer to nominal levels while reducing computational latency by several orders of magnitude. We demonstrate the practical utility of this framework on the COMPAS dataset, where it successfully flags intersectional biases, specifically identifying a significantly higher positive prediction rate for African-American males under 25 and a systemic under-prediction for Caucasian females relative to the population mean.

</details>


### [6] [Physics-informed Blind Reconstruction of Dense Fields from Sparse Measurements using Neural Networks with a Differentiable Simulator](https://arxiv.org/abs/2601.20496)
*Ofek Aloni,Barak Fishbain*

Main category: stat.ML

TL;DR: 提出一种无需密集场训练样本或空间统计假设，仅通过稀疏测量生成密集物理场的重建方法，在流体力学问题上优于统计和神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 从稀疏测量生成密集物理场是采样和信号处理中的基本问题。现有方法要么需要空间统计知识，要么依赖训练阶段的密集场示例，而这些数据通常难以获得，只能依赖合成数据。

Method: 在训练阶段引入自动可微数值模拟器，无需空间统计假设或密集场示例，仅通过稀疏测量就能重建密集物理场。

Result: 在三个标准流体力学问题上，该方法表现出优于统计方法和神经网络方法的结果。

Conclusion: 通过引入可微数值模拟器，实现了无需密集场训练数据或统计假设的稀疏到密集物理场重建，在流体力学应用中具有优越性能。

Abstract: Generating dense physical fields from sparse measurements is a fundamental question in sampling, signal processing, and many other applications. State-of-the-art methods either use spatial statistics or rely on examples of dense fields in the training phase, which often are not available, and thus rely on synthetic data. Here, we present a reconstruction method that generates dense fields from sparse measurements, without assuming availability of the spatial statistics, nor of examples of the dense fields. This is made possible through the introduction of an automatically differentiable numerical simulator into the training phase of the method. The method is shown to have superior results over statistical and neural network based methods on a set of three standard problems from fluid mechanics.

</details>


### [7] [Incorporating data drift to perform survival analysis on credit risk](https://arxiv.org/abs/2601.20533)
*Jianwei Peng,Stefan Lessmann*

Main category: stat.ML

TL;DR: 该研究提出了一种动态联合建模框架，用于在非平稳环境下提高生存分析信用风险模型的鲁棒性，通过整合纵向行为标记和离散时间风险模型，在数据漂移场景中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统生存分析方法通常假设数据生成过程是平稳的，但在实际抵押贷款组合中，借款人行为、宏观经济条件、政策制度等变化会导致各种形式的数据漂移，这会影响信用风险模型的性能。

Method: 提出动态联合建模框架，整合从余额动态中提取的纵向行为标记与离散时间风险模型，结合地标one-hot编码和等渗校准技术。研究了三种数据漂移类型（突发、渐进和周期性）。

Result: 在Freddie Mac抵押贷款数据集上的实验表明，提出的地标联合模型在所有漂移场景下，在区分度和校准方面均优于经典生存模型、基于树的漂移自适应学习器和梯度提升方法。

Conclusion: 该研究证明了提出的动态联合建模框架在处理数据漂移时的优越性，为在非平稳环境下构建更稳健的信用风险模型提供了有效解决方案。

Abstract: Survival analysis has become a standard approach for modelling time to default by time-varying covariates in credit risk. Unlike most existing methods that implicitly assume a stationary data-generating process, in practise, mortgage portfolios are exposed to various forms of data drift caused by changing borrower behaviour, macroeconomic conditions, policy regimes and so on. This study investigates the impact of data drift on survival-based credit risk models and proposes a dynamic joint modelling framework to improve robustness under non-stationary environments. The proposed model integrates a longitudinal behavioural marker derived from balance dynamics with a discrete-time hazard formulation, combined with landmark one-hot encoding and isotonic calibration. Three types of data drift (sudden, incremental and recurring) are simulated and analysed on mortgage loan datasets from Freddie Mac. Experiments and corresponding evidence show that the proposed landmark-based joint model consistently outperforms classical survival models, tree-based drift-adaptive learners and gradient boosting methods in terms of discrimination and calibration across all drift scenarios, which confirms the superiority of our model design.

</details>


### [8] [Sparse clustering via the Deterministic Information Bottleneck algorithm](https://arxiv.org/abs/2601.20628)
*Efthymios Costa,Ioanna Papatsouma,Angelos Markos*

Main category: stat.ML

TL;DR: 提出基于信息论的聚类框架，能同时进行特征加权和聚类，特别适用于稀疏数据


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理特征空间子集存在聚类结构的稀疏数据时面临挑战，需要能同时进行特征选择和聚类的解决方案

Method: 基于信息论的框架，通过联合特征加权和聚类来处理稀疏数据

Result: 在合成数据模拟中表现出色，成为现有稀疏数据聚类算法的竞争性替代方案；在真实基因组数据集上验证了有效性

Conclusion: 提出的信息论框架能有效解决稀疏数据聚类问题，通过联合特征加权和聚类提供了一种强大的分析方法

Abstract: Cluster analysis relates to the task of assigning objects into groups which ideally present some desirable characteristics. When a cluster structure is confined to a subset of the feature space, traditional clustering techniques face unprecedented challenges. We present an information-theoretic framework that overcomes the problems associated with sparse data, allowing for joint feature weighting and clustering. Our proposal constitutes a competitive alternative to existing clustering algorithms for sparse data, as demonstrated through simulations on synthetic data. The effectiveness of our method is established by an application on a real-world genomics data set.

</details>


### [9] [Demystifying Prediction Powered Inference](https://arxiv.org/abs/2601.20819)
*Yilin Song,Dan M. Kluger,Harsh Parikh,Tian Gu*

Main category: stat.ML

TL;DR: PPI是利用预测模型提升统计推断效率的框架，通过小规模标注数据校正预测偏差，但需注意数据复用和缺失机制问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习预测常被用作真实数据的补充，但直接使用预测会引入偏差，而忽略预测又浪费信息。PPI方法变体繁多且差异细微，使得实践者难以选择合适方法。

Method: 提出统一的PPI实践工作流，包括理论基础、方法扩展、与现有统计文献的联系以及诊断工具。使用Mosaiks房价数据展示不同PPI变体的表现。

Result: PPI变体比完整案例分析产生更紧的置信区间，但数据复用会导致置信区间不保守和覆盖率问题。在非随机缺失机制下，所有方法（包括仅使用标注数据的经典推断）都会产生有偏估计。

Conclusion: 将PPI框架化为通用流程而非单一估计器，提供决策流程图、方法总结表和诊断策略，帮助研究者负责任地将预测整合到有效推断中。

Abstract: Machine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from large unlabeled datasets to improve statistical efficiency while maintaining valid inference through explicit bias correction using a smaller labeled subset. Despite its potential, the growing PPI variants and the subtle distinctions between them have made it challenging for practitioners to determine when and how to apply these methods responsibly. This paper demystifies PPI by synthesizing its theoretical foundations, methodological extensions, connections to existing statistics literature, and diagnostic tools into a unified practical workflow. Using the Mosaiks housing price data, we show that PPI variants produce tighter confidence intervals than complete-case analysis, but that double-dipping, i.e. reusing training data for inference, leads to anti-conservative confidence intervals and coverages. Under missing-not-at-random mechanisms, all methods, including classical inference using only labeled data, yield biased estimates. We provide a decision flowchart linking assumption violations to appropriate PPI variants, a summary table of selective methods, and practical diagnostic strategies for evaluating core assumptions. By framing PPI as a general recipe rather than a single estimator, this work bridges methodological innovation and applied practice, helping researchers responsibly integrate predictions into valid inference.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [10] [SunBURST: Deterministic GPU-Accelerated Bayesian Evidence via Mode-Centric Laplace Integration](https://arxiv.org/abs/2601.19957)
*Ira Wolfson*

Main category: stat.CO

TL;DR: SunBURST是一种基于GPU的确定性算法，用于贝叶斯证据计算，通过模式中心几何积分替代全局采样，在高达1024维的高斯/近高斯后验中实现双精度容差。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯证据评估在高维空间中因维度灾难和采样方法的顺序性质而计算成本极高，需要更高效的确定性方法。

Method: 结合径向模式发现、批量L-BFGS优化和基于拉普拉斯近似的解析积分，将模式独立处理，将大量似然评估转换为大规模并行GPU工作负载。

Result: 对于高斯和近高斯后验，在高达1024维的基准测试中实现双精度容差的数值一致性，具有亚线性时间缩放；在多模态高斯混合中，保守配置实现亚百分比精度。

Conclusion: SunBURST不是采样推理的通用替代品，而是针对物理参数估计和逆问题中后验质量在有限模式周围具有高斯结构的场景，可作为快速几何感知证据估计器或混合工作流的预处理阶段。

Abstract: Bayesian evidence evaluation becomes computationally prohibitive in high dimensions due to the curse of dimensionality and the sequential nature of sampling-based methods. We introduce SunBURST, a deterministic GPU-native algorithm for Bayesian evidence calculation that replaces global volume exploration with mode-centric geometric integration. The pipeline combines radial mode discovery, batched L-BFGS refinement, and Laplace-based analytic integration, treating modes independently and converting large batches of likelihood evaluations into massively parallel GPU workloads.
  For Gaussian and near-Gaussian posteriors, where the Laplace approximation is exact or highly accurate, SunBURST achieves numerical agreement at double-precision tolerance in dimensions up to 1024 in our benchmarks, with sub-linear wall-clock scaling across the tested range. In multimodal Gaussian mixtures, conservative configurations yield sub-percent accuracy while maintaining favorable scaling.
  SunBURST is not intended as a universal replacement for sampling-based inference. Its design targets regimes common in physical parameter estimation and inverse problems, where posterior mass is locally well approximated by Gaussian structure around a finite number of modes. In strongly non-Gaussian settings, the method can serve as a fast geometry-aware evidence estimator or as a preprocessing stage for hybrid workflows. These results show that high-precision Bayesian evidence evaluation can be made computationally tractable in very high dimensions through deterministic integration combined with massive parallelism.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [11] [Scalable Decisions using a Bayesian Decision-Theoretic Approach](https://arxiv.org/abs/2601.20031)
*Hoiyi Ng,Guido Imbens*

Main category: stat.AP

TL;DR: 提出贝叶斯决策理论框架，通过结合多目标与权衡，利用层次模型和历史实验信息，提高估计效率并简化决策过程


<details>
  <summary>Details</summary>
Motivation: 传统方法独立评估指标，忽略相关性，混合结果需要人工判断，难以扩展。需要系统化处理多目标和权衡的决策框架

Method: 贝叶斯决策理论框架，结合实验者定义的损失函数和观测证据，使用层次模型利用历史实验信息作为先验知识

Result: 相比零假设统计检验，方法通过信息性层次先验提高估计效率，系统整合商业偏好和成本，实现全面可扩展的决策

Conclusion: 提出的框架能够有效处理多目标决策问题，提高实验评估的效率和可扩展性，适用于供应链等实际应用场景

Abstract: Randomized controlled experiments assess new policy impacts on performance metrics to inform launch decisions. Traditional approaches evaluate metrics independently despite correlations, and mixed results (e.g., positive revenue impact, negative customer experience) require manual judgment, hindering scalability. We propose a Bayesian decision-theoretic framework that systematically incorporates multiple objectives and trade-offs by comparing expected risks across decisions. Our approach combines experimenter-defined loss functions with observed evidence, using hierarchical models to leverage historical experiment learnings for prior information on treatment effects. Through real and simulated Amazon supply chain experiments, we demonstrate that compared to null hypothesis statistical testing, our method increases estimation efficiency via informative hierarchical priors and simplifies decision-making by systematically incorporating business preferences and costs for comprehensive, scalable decisions.

</details>


### [12] [Comparing causal estimands from sequential nested versus single point target trials: A simulation study](https://arxiv.org/abs/2601.20725)
*Catherine Wiener,Chase D. Latour,Kathleen Hurwitz,Xiaojuan Li,Catherine R. Lesko,Alexander Breskin,M. Alan Brookhart*

Main category: stat.AP

TL;DR: SNT模拟与单点试验的因果估计量比较：当疾病严重程度不改变治疗效果时，两者估计一致；当存在效果修饰时，即使调整混杂后估计仍存在差异，表明SNT模拟的目标人群与单点试验不对应。


<details>
  <summary>Details</summary>
Motivation: 虽然序贯嵌套试验（SNT）模拟是提高精度和避免时间相关偏倚的强大方法，但对其隐含的因果估计量与真实世界单点试验的比较讨论较少。需要明确SNT模拟的因果解释及其与单点试验的差异。

Method: 使用蒙特卡洛模拟比较两种SNT模拟方法（年度重新索引和基于治疗决策设计）与单点试验的治疗效果估计。生成5000个队列，每个队列5000人，随访3年。单点试验在访视1随机分配治疗，SNT模拟中患者最多可贡献两个索引日期。

Result: 当疾病严重程度不改变治疗效果时，两种SNT方法返回的估计与单点试验相同。当存在疾病严重程度对治疗效果的修饰作用时，即使调整混杂因素后，两种SNT方法的估计仍与单点试验存在差异。

Conclusion: SNT模拟的因果估计量解释存在困难：其目标人群与单点试验不对应。这一发现对基于证据的决策沟通具有重要意义，需要明确说明研究结果的目标人群和因果解释。

Abstract: Sequential nested trial (SNT) emulation is a powerful approach for maximizing precision and avoiding time-related biases. However, there exists little discussion about the implied causal estimands in comparison to a real-world single point trial. We used Monte Carlo simulation to compare treatment effect estimates from an SNT emulation that re-indexed patients annually and a SNT emulation with a treatment decision design to the estimates from a single point trial. We generated 5,000 cohorts of 5,000 people with 3 years of follow-up. For the single point trial, patients were randomized to initiate or not initiate treatment at Visit 1. For the SNT emulations, simulated patients could contribute up to two index dates. When disease severity did not modify the treatment effect, both SNT approaches returned treatment effect estimates identical to the single point trial. In the presence of treatment effect modification by disease severity, both SNT approaches returned treatment effect estimates that diverged from the single point trial even after confounding-adjustment. These findings underscore the difficulties of interpreting causal estimands from a SNT emulation: the target population does not correspond to a single time point trial. Such implications are important for communicating study results for evidence-based decision-making.

</details>


### [13] [A Survival Framework for Estimating Child Mortality Rates using Multiple Data Types](https://arxiv.org/abs/2601.20821)
*Katherine R Paulson,Taylor Okonek,Jon Wakefield*

Main category: stat.AP

TL;DR: 提出贝叶斯生存框架，通过单一模型估计5岁以下儿童生存概率的年龄函数，整合多种数据源，生成完整生存曲线


<details>
  <summary>Details</summary>
Motivation: 许多国家缺乏高质量生命登记数据来精确测量儿童死亡率，现有方法（如UN IGME和GBD）采用分离模型或多步骤建模过程，需要更统一的框架

Method: 提出贝叶斯生存框架，使用log-logistic和分段指数生存函数，整合家庭调查、生命登记和其他预处理死亡率数据，通过单一模型估计生存概率的年龄函数

Result: 模型适用于任何国家，在肯尼亚、巴西、爱沙尼亚和叙利亚四个数据特征不同的国家中，模型产生的三个生存汇总指标与数据和UN IGME估计基本一致，同时提供完整生存曲线

Conclusion: 贝叶斯生存框架提供了一种统一的方法来估计儿童生存趋势，不仅产生标准死亡率指标，还能提供完整的生存曲线，为儿童死亡率估计提供了更全面的工具

Abstract: Child mortality is an important population health indicator. However, many countries lack high-quality vital registration to measure child mortality rates precisely and reliably over time. Research endeavors such as those by the United Nations Inter-agency Group for Child Mortality Estimation (UN IGME) and the Global Burden of Disease (GBD) study leverage statistical models and available data to estimate child survival summaries including neonatal, infant, and under-five mortality rates. UN IGME fits separate models for each age group and the GBD uses a multi-step modeling process. We propose a Bayesian survival framework to estimate temporal trends in the probability of survival as a function of age, up to the fifth birthday, with a single model. Our framework integrates all data types that are used by UN IGME: household surveys, vital registration, and other pre-processed mortality rates. We demonstrate that our framework is applicable to any country using log-logistic and piecewise-exponential survival functions, and discuss findings for four example countries with diverse data profiles: Kenya, Brazil, Estonia, and Syrian Arab Republic. Our model produces estimates of the three survival summaries that are in broad agreement with both the data and the UN IGME estimates, but in addition gives the complete survival curve.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [14] [Connecting reflective asymmetries in multivariate spatial and spatio-temporal covariances](https://arxiv.org/abs/2601.20132)
*Drew Yarger*

Main category: stat.ME

TL;DR: 提出了一种新的"反射不对称"时空协方差构建范式，通过扩展多变量空间模型到时空领域，参数更少且包含可分离模型作为特例，在爱尔兰风数据中表现出更好的拟合和预测性能。


<details>
  <summary>Details</summary>
Motivation: 在多元空间和单变量时空数据分析中，通常存在不对称依赖关系，需要在高斯过程框架中使用不对称协方差函数来处理。现有方法（如拉格朗日不对称时空协方差）存在局限性，需要新的构建范式。

Method: 首先扩展了反射不对称多变量空间模型的应用范围，然后将其推广到时空领域，提出了反射不对称时空协方差模型。这种方法与拉格朗日不对称时空协方差有本质不同，参数更少，其中一个参数同时控制空间和时间边际协方差，且标准可分离模型是其特例。

Result: 在模拟研究和爱尔兰风数据分析中，新模型在模型拟合和预测性能方面都有改进，并且更容易估计。这些特性表明新模型在环境和其他时空数据分析中具有广泛适用性。

Conclusion: 提出的反射不对称时空协方差模型为时空数据分析提供了新的有效工具，具有参数少、包含可分离模型特例、拟合预测性能好、易于估计等优点，在环境科学等领域有广泛应用前景。

Abstract: In the analysis of multivariate spatial and univariate spatio-temporal data, it is commonly recognized that asymmetric dependence may exist, which can be addressed using an asymmetric (matrix or space-time, respectively) covariance function within a Gaussian process framework. This paper introduces a new paradigm for constructing asymmetric space-time covariances, which we refer to as "reflective asymmetric," by leveraging recently-introduced models for multivariate spatial data. We first provide new results for reflective asymmetric multivariate spatial models that extends their applicability. We then propose their asymmetric space-time extension, which come from a substantially different perspective than Lagrangian asymmetric space-time covariances. There are fewer parameters in the new models, one controls both the spatial and temporal marginal covariances, and the standard separable model is a special case. In simulation studies and analysis of the frequently-studied Irish wind data, these new models also improve model fit and prediction performance, and they can be easier to estimate. These features indicate broad applicability for improved analysis in environmental and other space-time data.

</details>


### [15] [Online Change Point Detection for Multivariate Inhomogeneous Poisson Processes Time Series](https://arxiv.org/abs/2601.20192)
*Xiaokai Luo,Haotian Xu,Carlos Misael Madrid Padilla,Oscar Hernan Madrid Padilla*

Main category: stat.ME

TL;DR: 提出了一种用于多元非齐次泊松点过程时间序列的在线变点检测方法，使用低秩矩阵表示强度函数，具有单次遍历和恒定计算成本的特点。


<details>
  <summary>Details</summary>
Motivation: 多元非齐次泊松点过程时间序列的在线变点检测在多个应用领域（如地震学、气候监测、流行病监测）中很重要，但在机器学习和统计文献中研究不足。

Method: 使用低秩矩阵表示多元泊松强度函数，提出自适应非参数检测程序，算法为单次遍历，每个新观测只需恒定计算成本。

Result: 提供了控制总体误报概率的理论保证，在时间依赖性下表征了检测延迟，开发了适用于时间依赖性泊松点过程时间序列的新矩阵伯恩斯坦不等式。

Conclusion: 该方法在统计上稳健且计算高效，数值实验验证了其有效性，新开发的矩阵不等式具有独立的理论价值。

Abstract: We study online change point detection for multivariate inhomogeneous Poisson point process time series. This setting arises commonly in applications such as earthquake seismology, climate monitoring, and epidemic surveillance, yet remains underexplored in the machine learning and statistics literature. We propose a method that uses low-rank matrices to represent the multivariate Poisson intensity functions, resulting in an adaptive nonparametric detection procedure. Our algorithm is single-pass and requires only constant computational cost per new observation, independent of the elapsed length of the time series. We provide theoretical guarantees to control the overall false alarm probability and characterize the detection delay under temporal dependence. We also develop a new Matrix Bernstein inequality for temporally dependent Poisson point process time series, which may be of independent interest. Numerical experiments demonstrate that our method is both statistically robust and computationally efficient.

</details>


### [16] [Bias-Reduced Estimation of Finite Mixtures: An Application to Latent Group Structures in Panel Data](https://arxiv.org/abs/2601.20197)
*Raphaël Langevin*

Main category: stat.ME

TL;DR: 本文发现有限混合模型的最大似然估计存在显著有限样本偏差，提出使用分类-混合似然函数进行估计以减少偏差，并在模拟和实证中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 有限混合模型在计量经济学中广泛用于捕捉未观测异质性，但研究发现最大似然估计在有限样本下存在显著偏差，特别是在组分密度重叠程度高时，需要改进估计方法。

Method: 提出最大化分类-混合似然函数的方法，配合一致的分类器，推导了估计量的渐近分布，并在满足特定条件下实现oracle效率。

Result: 蒙特卡洛模拟显示传统混合MLE存在明显有限样本偏差，而提出的方法在偏差和均方误差方面表现更优；实证应用中，新方法将样本外预测误差降低了约17.6%。

Conclusion: 分类-混合似然估计方法能有效减少有限混合模型参数估计的偏差，在有限样本下优于标准MLE，具有实际应用价值。

Abstract: Finite mixture models are widely used in econometric analyses to capture unobserved heterogeneity. This paper shows that maximum likelihood estimation of finite mixtures of parametric densities can suffer from substantial finite-sample bias in all parameters under mild regularity conditions. The bias arises from the influence of outliers in component densities with unbounded or large support and increases with the degree of overlap among mixture components. I show that maximizing the classification-mixture likelihood function, equipped with a consistent classifier, yields parameter estimates that are less biased than those obtained by standard maximum likelihood estimation (MLE). I then derive the asymptotic distribution of the resulting estimator and provide conditions under which oracle efficiency is achieved. Monte Carlo simulations show that conventional mixture MLE exhibits pronounced finite-sample bias, which diminishes as the sample size or the statistical distance between component densities tends to infinity. The simulations further show that the proposed estimation strategy generally outperforms standard MLE in finite samples in terms of both bias and mean squared errors under relatively weak assumptions. An empirical application to latent group panel structures using health administrative data shows that the proposed approach reduces out-of-sample prediction error by approximately 17.6% relative to the best results obtained from standard MLE procedures.

</details>


### [17] [Causal Inference in Biomedical Imaging via Functional Linear Structural Equation Models](https://arxiv.org/abs/2601.20610)
*Ting Li,Ethan Fan,Tengfei Li,Hongtu Zhu*

Main category: stat.ME

TL;DR: 提出功能性线性结构方程模型（FLSEM），利用标量工具变量处理医学影像的功能性暴露与临床结局的因果推断，开发FGS-DAR算法进行变量选择，并在UK Biobank数据上验证


<details>
  <summary>Details</summary>
Motivation: 医学影像中器官特异性特征对临床结局的因果效应理解对生物医学研究和患者护理至关重要，但传统方法难以处理无限维度的功能性暴露和复杂协变量

Method: 提出功能性线性结构方程模型（FLSEM），利用标量工具变量建立可识别条件，开发功能性组支持检测和根查找算法（FGS-DAR）进行变量选择，并提供理论保证

Result: 通过大量模拟验证方法有效性，应用于UK Biobank数据，展示了从医学影像中检测因果关系的稳健性能

Conclusion: FLSEM模型成功解决了医学影像功能性暴露的因果推断问题，为生物医学研究提供了有效的分析工具

Abstract: Understanding the causal effects of organ-specific features from medical imaging on clinical outcomes is essential for biomedical research and patient care. We propose a novel Functional Linear Structural Equation Model (FLSEM) to capture the relationships among clinical outcomes, functional imaging exposures, and scalar covariates like genetics, sex, and age. Traditional methods struggle with the infinite-dimensional nature of exposures and complex covariates. Our FLSEM overcomes these challenges by establishing identifiable conditions using scalar instrumental variables. We develop the Functional Group Support Detection and Root Finding (FGS-DAR) algorithm for efficient variable selection, supported by rigorous theoretical guarantees, including selection consistency and accurate parameter estimation. We further propose a test statistic to test the nullity of the functional coefficient, establishing its null limit distribution. Our approach is validated through extensive simulations and applied to UK Biobank data, demonstrating robust performance in detecting causal relationships from medical imaging.

</details>


### [18] [Joint Estimation of Edge Probabilities for Multi-layer Networks via Neighborhood Smoothing](https://arxiv.org/abs/2601.20219)
*Yong He,Zizhou Huang,Bingyi Jing,Diqing Li*

Main category: stat.ME

TL;DR: 提出了一种用于多层网络边概率联合估计的新方法，通过引入三层图函数和两步邻域平滑算法，在计算效率和预测性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图函数方法主要针对单层网络，缺乏对多层网络边概率联合估计的有效方法。多层网络在现实世界中广泛存在（如国际贸易网络），需要能够同时利用层间和节点间相似性的模型。

Method: 提出三层图函数概念（引入潜在层位置参数），开发了两步邻域平滑算法：第一步在层间平滑，第二步在节点间平滑。该方法计算高效、调参少，能充分利用层和节点的相似性。

Result: 数值实验显示该方法优于现有最优方法。在真实的世界食品进出口网络数据集上，链接预测性能显著优于基准方法。

Conclusion: 提出的三层图函数框架和两步邻域平滑算法为多层网络边概率估计提供了有效解决方案，在理论和实际应用中均表现出优越性能。

Abstract: In this paper we focus on jointly estimating the edge probabilities for multi-layer networks. We define a novel multi-layer graphon, a ternary function in contrast to the bivariate graphon function in the literature by introducing an additional latent layer position parameter, which is model-free and covers a wide range of multi-layer networks. We develop a computationally efficient two-step neighborhood smoothing algorithm to estimate the edge probabilities of multi-layer networks, which requires little tuning and fully utilize the similarity across both network layers and nodes. Numerical experiments demonstrate the advantages of our method over the existing state-of-the-art ones. A real Worldwide Food Import/Export Network dataset example is analyzed to illustrate the better performance of the proposed method over benchmark methods in terms of link prediction.

</details>


### [19] [Effective Sample Size for Functional Spatial Data](https://arxiv.org/abs/2601.20812)
*Alfredo Alegría,John Gómez,Jorge Mateu,Ronny Vallejos*

Main category: stat.ME

TL;DR: 本文提出了针对功能性地统计数据的有效样本量新定义，使用迹协方差图衡量相关性，并展示了其在功能自回归过程和真实气象数据中的应用。


<details>
  <summary>Details</summary>
Motivation: 有效样本量在标量地统计学中广泛应用，但针对功能空间数据的扩展研究仍然不足。功能空间数据存在相关性导致的冗余，需要量化其中的独立信息量。

Method: 引入功能性地统计数据的有效样本量新定义，使用迹协方差图作为相关性度量，通过功能自回归过程验证其性质，并在真实气象数据中应用。

Result: 新定义的功能性ESS保留了经典标量ESS的直观性质，能够量化序列依赖性和特征方向变异性分配的影响，成功应用于气象数据集分析。

Conclusion: 提出的功能性ESS定义有效量化了功能空间数据集中的冗余信息，能够确定独立曲线的有效数量，为功能性地统计分析提供了新工具。

Abstract: The effective sample size quantifies the amount of independent information contained in a dataset, accounting for redundancy due to correlation between observations. While widely used in geostatistics for scalar data, its extension to functional spatial data has remained largely unexplored. In this work, we introduce a novel definition of the effective sample size for functional geostatistical data, employing the trace-covariogram as a measure of correlation, and show that it retains the intuitive properties of the classical scalar ESS. We illustrate the behavior of this measure using a functional autoregressive process, demonstrating how serial dependence and the allocation of variability across eigen-directions influence the resulting functional ESS. Finally, the approach is applied to a real meteorological dataset of geometric vertical velocities over a portion of the Earth, showing how the method can quantify redundancy and determine the effective number of independent curves in functional spatial datasets.

</details>


### [20] [Wavelet Tree Ensembles for Triangulable Manifolds](https://arxiv.org/abs/2601.20254)
*Hengrui Luo,Akira Horiguchi,Li Ma*

Main category: stat.ME

TL;DR: 提出了一种用于三角化流形回归的非平衡Haar小波树集成方法，将经典UH小波从欧几里得网格扩展到通用三角化流形，在球面等流形上显著优于传统树集成和非自适应网格小波。


<details>
  <summary>Details</summary>
Motivation: 现有小波方法主要针对规则欧几里得网格，缺乏对三角化流形上回归问题的适应性。需要开发能够在流形几何结构上保持正交性、精确重建和数据驱动分区的非平衡Haar小波树集成方法。

Method: 基于三角化流形上的采样数据，构建非平衡Haar小波树，其原子支撑在测地三角形上，在经验测度L^2空间中形成正交系统。通过测地三角剖分实现递归的、数据驱动的分区，保持正交性和精确重建特性，可作为加性集成中的弱学习器。

Result: 在球面合成回归和气候异常场实验中，三角化流形上的UH集成显著优于经典树集成和非自适应网格小波。贝叶斯变体(RUHWT)为流形上的函数估计提供了后验不确定性量化。

Conclusion: 成功将非平衡Haar小波树扩展到三角化流形，保持了正交性、精确重建和数据驱动分区等关键特性，在流形回归任务中表现出优越性能，为流形上的函数估计提供了有效的集成学习方法。

Abstract: We develop unbalanced Haar (UH) wavelet tree ensembles for regression on triangulable manifolds. Given data sampled on a triangulated manifold, we construct UH wavelet trees whose atoms are supported on geodesic triangles and form an orthonormal system in $L^2(μ_n)$, where $μ_n$ is the empirical measure on the sample, which allows us to use UH trees as weak learners in additive ensembles. Our construction extends classical UH wavelet trees from regular Euclidean grids to generic triangulable manifolds while preserving three key properties: (i) orthogonality and exact reconstruction at the sampled locations, (ii) recursive, data-driven partitions adapted to the geometry of the manifold via geodesic triangulations, and (iii) compatibility with optimization-based and Bayesian ensemble building. In Euclidean settings, the framework reduces to standard UH wavelet tree regression and provides a baseline for comparison. We illustrate the method on synthetic regression on the sphere and on climate anomaly fields on a spherical mesh, where UH ensembles on triangulated manifolds substantially outperform classical tree ensembles and non-adaptive mesh-based wavelets. For completeness, we also report results on image denoising on regular grids. A Bayesian variant (RUHWT) provides posterior uncertainty quantification for function estimates on manifolds. Our implementation is available at http://www.github.com/hrluo/WaveletTrees.

</details>


### [21] [Confidence intervals for maximum unseen probabilities, with application to sequential sampling design](https://arxiv.org/abs/2601.20320)
*Alessandro Colombi,Mario Beraha,Amichai Painsky,Stefano Favaro*

Main category: stat.ME

TL;DR: 该论文研究在伯努利乘积模型下，如何判断是否需要额外采样来检测所有超过预设阈值的类别。提出了最大未观测概率的非渐近、分布无关置信上界，并开发了具有有限样本保证的顺序停止规则。


<details>
  <summary>Details</summary>
Motivation: 在发现类问题中，经常需要决定是否需要额外采样来检测所有超过预设阈值的类别。特别是在伯努利乘积（发生率）模型下，类别仅通过采样单元的存在-缺失来观察，需要量化未观测类别中最大流行率的不确定性。

Method: 开发了最大未观测概率的非渐近、分布无关置信上界，针对两种场景：有界字母表（有限已知类别数）和无界字母表（可数无限且满足温和可和条件）。分析了数据无关最坏情况界限的极限，并提出了数据依赖的界限方法。

Result: 建立了匹配的下界，证明了所提方法的近最优性。在模拟和真实数据集上进行了实证比较，并构建了具有有限样本保证的顺序停止规则，展示了对引入虚假低流行率类别污染的鲁棒性。

Conclusion: 该研究为发现类问题提供了严格的统计框架，特别是在无界字母表场景下，数据依赖的置信界限是必要的。所提方法在实际应用中具有鲁棒性和实用性，为采样决策提供了统计保证。

Abstract: Discovery problems often require deciding whether additional sampling is needed to detect all categories whose prevalence exceeds a prespecified threshold. We study this question under a Bernoulli product (incidence) model, where categories are observed only through presence--absence across sampling units. Our inferential target is the \emph{maximum unseen probability}, the largest prevalence among categories not yet observed. We develop nonasymptotic, distribution-free upper confidence bounds for this quantity in two regimes: bounded alphabets (finite and known number of categories) and unbounded alphabets (countably infinite under a mild summability condition). We characterise the limits of data-independent worst-case bounds, showing that in the unbounded regime no nontrivial data-independent procedure can be uniformly valid. We then propose data-dependent bounds in both regimes and establish matching lower bounds demonstrating their near-optimality. We compare empirically the resulting procedures in both simulated and real datasets. Finally, we use these bounds to construct sequential stopping rules with finite-sample guarantees, and demonstrate robustness to contamination that introduces spurious low-prevalence categories.

</details>


### [22] [A General Mixture Loss Function to Optimize a Personalized PredictiveModel](https://arxiv.org/abs/2601.20788)
*Tatiana Krikella,Joel A. Dubin*

Main category: stat.ME

TL;DR: 提出一种广义损失函数，用于优化个性化预测模型（PPM）的子群体规模，平衡区分度和校准性能，并推荐子群体规模范围为训练数据的20%-70%。


<details>
  <summary>Details</summary>
Motivation: 精准医疗的发展推动了健康研究方法论的创新，个性化预测模型（PPM）相比"一刀切"模型在区分度方面表现更好，但需要优化子群体规模以平衡区分度和校准性能。

Method: 提出广义损失函数，允许用户指定性能指标及其权重，联合优化区分度和校准；通过模拟研究确定子群体规模的实用边界（训练数据的20%-70%），降低计算负担。

Result: 模拟和真实数据集验证显示，子群体规模与模型性能的关系具有稳健性；损失函数中性能指标的选择会影响最优子群体规模的选择。

Conclusion: 该方法支持在精准健康研究中灵活且计算高效地实现个性化预测模型，为PPM的实际应用提供了实用指导。

Abstract: Advances in precision medicine increasingly drive methodological innovation in health research. A key development is the use of personalized prediction models (PPMs), which are fit using a similar subpopulation tailored to a specific index patient, and have been shown to outperform one-size-fits-all models, particularly in terms of model discrimination performance. We propose a generalized loss function that enables tuning of the subpopulation size used to fit a PPM. This loss function allows joint optimization of discrimination and calibration, allowing both the performance measures and their relative weights to be specified by the user. To reduce computational burden, we conducted extensive simulation studies to identify practical bounds for the grid of subpopulation sizes. Based on these results, we recommend using a lower bound of 20\% and an upper bound of 70\% of the entire training dataset. We apply the proposed method to both simulated and real-world datasets and demonstrate that previously observed relationships between subpopulation size and model performance are robust. Furthermore, we show that the choice of performance measures in the loss function influences the optimal subpopulation size selected. These findings support the flexible and computationally efficient implementation of PPMs in precision health research.

</details>


### [23] [SCORE: A Unified Framework for Overshoot Refund in Online FDR Control](https://arxiv.org/abs/2601.20386)
*Qi Kuang,Bowen Gang,Yin Xia*

Main category: stat.ME

TL;DR: 提出SCORE框架，通过回收超过拒绝阈值的证据来增强基于e值的在线多重假设检验方法的统计功效，同时保持有效的FDR控制。


<details>
  <summary>Details</summary>
Motivation: 基于e值的在线多重假设检验方法虽然能在最小假设下提供稳健的FDR控制，但通常会丢弃超过拒绝阈值的证据，导致统计功效损失。

Method: 提出SCORE框架，利用不等式𝕀(y ≥ 1) ≤ y - (y-1)+回收"浪费"的证据，开发了SCORE-LOND、SCORE-LORD、SCORE-SAFFRON等增强版本，并允许通过最新决策两次使用来追溯更新alpha-wealth。

Result: SCORE增强版本严格优于原始方法，在保持有限样本FDR控制有效性的同时提高统计功效，并通过模拟和真实数据实验验证了有效性。

Conclusion: SCORE框架为增强在线多重假设检验方法提供了一个统一原则，通过回收浪费的证据显著提高统计功效，同时保持严格的FDR控制。

Abstract: We propose a unified framework to enhance the power of online multiple hypothesis testing procedures based on $e$-values. While $e$-value-based methods offer robust online False Discovery Rate (FDR) control under minimal assumptions, they often suffer from power loss by discarding evidence that exceeds the rejection threshold. We address this inefficiency via the \textbf{S}equential \textbf{C}ontrol with \textbf{O}vershoot \textbf{R}efund for \textbf{E}-values (SCORE) framework, which leverages the inequality $\mathbb{I}(y \ge 1) \le y - (y-1)_+$ to reclaim this otherwise ``wasted'' evidence. This simple yet powerful insight yields a unified principle for improving a broad class of online testing algorithms. Building on this framework, we develop SCORE-enhanced versions of several state-of-the-art procedures, including SCORE-LOND, SCORE-LORD, and SCORE-SAFFRON, all of which strictly dominate their original counterparts while preserving valid finite-sample FDR control. Furthermore, under mild assumptions, SCORE permits retroactive updates of alpha-wealth by using the latest decision twice: first to determine its reward or loss, and then to refresh past wealth. Such a mechanism enables more aggressive testing strategies while maintaining valid FDR control, thereby further improving statistical power. The effectiveness of the proposed methods is validated through extensive simulation and real-data experiments.

</details>


### [24] [Exact Graph Learning via Integer Programming](https://arxiv.org/abs/2601.20589)
*Lucas Kook,Søren Wengel Mogensen*

Main category: stat.ME

TL;DR: 本文提出了一种基于非参数条件独立性检验和整数规划的非参数图学习框架GLIP，能够为图学习问题提供全局最优解，支持多种图类型，并在效率和性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图学习方法存在局限性：要么依赖对数据生成过程的限制性假设，要么使用贪婪算法或近似公式，导致对假设违反敏感或无法保证全局最优解。需要一种能够提供全局最优解且不依赖严格假设的方法。

Method: 将图学习问题重新表述为整数规划问题，利用非参数条件独立性检验和图形分离准则的高效编码。通过解决整数规划问题获得原始图学习问题的全局最优解。实现了R包'glip'，支持学习有向（混合）无环图和链图。

Result: GLIP能够精确恢复比以前方法更大的图，在大量实例和各种尺寸的图上比其他精确图学习方法更快。在模拟数据和基准数据集上，对于所有支持的图类型都达到了最先进的性能。

Conclusion: 提出的非参数图学习框架通过整数规划和非参数条件独立性检验，克服了现有方法的局限性，提供了全局最优解，且不依赖限制性假设，在效率和准确性上都有显著提升。

Abstract: Learning the dependence structure among variables in complex systems is a central problem across medical, natural, and social sciences. These structures can be naturally represented by graphs, and the task of inferring such graphs from data is known as graph learning or as causal discovery if the graphs are given a causal interpretation. Existing approaches typically rely on restrictive assumptions about the data-generating process, employ greedy oracle algorithms, or solve approximate formulations of the graph learning problem. As a result, they are either sensitive to violations of central assumptions or fail to guarantee globally optimal solutions. We address these limitations by introducing a nonparametric graph learning framework based on nonparametric conditional independence testing and integer programming. We reformulate the graph learning problem as an integer-programming problem and prove that solving the integer-programming problem provides a globally optimal solution to the original graph learning problem. Our method leverages efficient encodings of graphical separation criteria, enabling the exact recovery of larger graphs than was previously feasible. We provide an implementation in the openly available R package 'glip' which supports learning (acyclic) directed (mixed) graphs and chain graphs. From the resulting output one can compute representations of the corresponding Markov equivalence classes or weak equivalence classes. Empirically, we demonstrate that our approach is faster than other existing exact graph learning procedures for a large fraction of instances and graphs of various sizes. GLIP also achieves state-of-the-art performance on simulated data and benchmark datasets across all aforementioned classes of graphs.

</details>


### [25] [Two-dose vs. Three-Dose Optimization Under Sample Size Constraint](https://arxiv.org/abs/2601.20710)
*Linda Sun,Yixin Ren,Cong Chen*

Main category: stat.ME

TL;DR: 在肿瘤药物开发中，即使总样本量固定，包含三个剂量通常比两个剂量更有利于剂量优化，除非有非常强的证据表明可以放弃一个剂量。


<details>
  <summary>Details</summary>
Motivation: Project Optimus要求进行剂量优化研究，但早期开发阶段往往不清楚应该包含多少个剂量。实践中存在困惑：在总样本量固定的情况下，是选择两个剂量还是三个剂量更有利于优化决策。

Method: 采用数学近似方法进行分析，辅以模拟研究验证理论发现。同时考虑随机化和非随机化剂量优化场景，以及人群同质性因素。

Result: 研究表明，即使总样本量固定，包含三个剂量通常比两个剂量更优，除非有非常强的证据支持可以放弃一个剂量。研究提供了半定量指导原则。

Conclusion: 在肿瘤药物剂量优化研究中，推荐包含三个剂量而非两个，除非有充分证据表明可以简化。这为Project Optimus的实施提供了实用指导。

Abstract: Dose optimization is a hallmark of Project Optimus for oncology drug development. The number of doses to include in a dose optimization study depends on the totality of evidence, which is often unclear in early-phase development. With equal sample sizes per dose, carrying three doses is clearly more advantageous than two for optimization. In this paper, we show that, even when the total sample size is fixed, it is still preferable to carry three unless there is very strong evidence that one can be dropped. A mathematical approximation is applied to guide the investigation, followed by a simulation study to complement the theoretical findings. Semi-quantitative guidance is provided for practitioners, addressing both randomized and non-randomized dose optimization while considering population homogeneity.

</details>


### [26] [Plotting correlated data](https://arxiv.org/abs/2601.20805)
*Lukas Koch*

Main category: stat.ME

TL;DR: 论文讨论数据可视化中误差条显示的问题，特别是当数据点存在相关性时，传统误差条无法准确反映模型与数据的拟合程度，提出了显示主成分和条件不确定性的改进方法。


<details>
  <summary>Details</summary>
Motivation: 在数据可视化中，通常用垂直误差条表示数据点的不确定性（68%置信区间）。但当数据点之间存在相关性时，仅显示协方差矩阵对角元素的平方根（即传统误差条）无法提供足够信息来判断模型预测与数据的吻合程度，导致直觉判断失效。

Method: 通过显式展示不确定性的第一主成分贡献，以及显示所有数据点的条件不确定性，为图表添加更多信息，帮助判断数据与模型预测的一致性。

Result: 论文展示了传统误差条在数据相关情况下的局限性，并提出了改进的可视化方法，使读者能够更准确地评估模型拟合质量，并识别模型可能存在的缺陷。

Conclusion: 当数据点存在相关性时，传统误差条可视化方法存在严重不足。通过显示主成分贡献和条件不确定性，可以显著改善图表的信息传达能力，帮助研究者更好地判断模型与数据的吻合程度。

Abstract: A very common task in data visualization is to plot many data points with some measured y-value as a function of fixed x-values. Uncertainties on the y-values are typically presented as vertical error bars that represent either a Frequentist confidence interval or Bayesian credible interval for each data point. Most of the time, these error bars represent a 68\% confidence/credibility level, which leads to the intuition that a model fits the data reasonably well if its prediction lies within the error bars of roughly two thirds of the data points. Unfortunately, this and other intuitions no longer work when the uncertainties of the data points are correlated. If the error bars only show the square root of diagonal elements of some covariance matrix with non-negligible off-diagonal elements, we simply do not have enough information in the plot to judge whether a drawn model line agrees well with the data or not. In this paper we will demonstrate this problem and discuss ways to add more information to the plots to make it easier to judge the agreement between the data and some model prediction in the plot, as well as glean some insight where the model might be deficient. This is done by explicitly showing the contribution of the first principal component of the uncertainties, and by displaying the conditional uncertainties of all data points.

</details>


### [27] [Joint estimation of the basic reproduction number and serial interval using Sequential Bayes](https://arxiv.org/abs/2601.20809)
*Tatiana Krikella,Jane M. Heffernan,Hanna Jankowski*

Main category: stat.ME

TL;DR: 提出贝叶斯框架联合估计R0和序列间隔，使用序列贝叶斯方法更新先验，相比现有方法在早期疫情中提供更精确稳定的R0估计


<details>
  <summary>Details</summary>
Motivation: 在传染病暴发早期，及时准确估计基本再生数R0和序列间隔SI对理解传播动态和指导公共卫生应对至关重要。现有联合估计方法多为基于似然的方法，未能充分利用先验信息。

Method: 提出新颖的贝叶斯框架，仅使用病例计数数据联合估计R0和SI。采用序列贝叶斯方法，假设SIR模型，使用通过高斯copula连接的对数Gamma边际分布构建轻度信息先验，明确考虑两者依赖关系。随着新发病例数据的获得，先验被顺序更新，实现实时推断。

Result: 模拟研究表明，与广泛使用的White和Pagano似然基联合估计器相比，该方法在R0估计上显著更精确稳定，偏差相当或更小，尤其在疫情早期。SI估计对先验误设更敏感，但当先验信息合理准确时，能提供可靠SI估计且比竞争方法更稳定。使用加拿大COVID-19数据验证了实际效用。

Conclusion: 提出的贝叶斯框架为传染病暴发早期联合估计R0和SI提供了有效方法，特别在实时推断和利用先验信息方面具有优势，有助于更及时准确的公共卫生决策。

Abstract: Early in an infectious disease outbreak, timely and accurate estimation of the basic reproduction number ($R_0$) and the serial interval (SI) is critical for understanding transmission dynamics and informing public health responses. While many methods estimate these quantities separately, and a small number jointly estimate them from incidence data, existing joint approaches are largely likelihood-based and do not fully exploit prior information. We propose a novel Bayesian framework for the joint estimation of $R_0$ and the serial interval using only case count data, implemented through a sequential Bayes approach. Our method assumes an SIR model and employs a mildly informative joint prior constructed by linking log-Gamma marginal distributions for $R_0$ and the SI via a Gaussian copula, explicitly accounting for their dependence. The prior is updated sequentially as new incidence data become available, allowing for real-time inference. We assess the performance of the proposed estimator through extensive simulation studies under correct model specification as well as under model misspecification, including when the true data come from an SEIR or SEAIR model, and under varying degrees of prior misspecification. Comparisons with the widely used White and Pagano likelihood-based joint estimator show that our approach yields substantially more precise and stable estimates of $R_0$, with comparable or improved bias, particularly in the early stages of an outbreak. Estimation of the SI is more sensitive to prior misspecification; however, when prior information is reasonably accurate, our method provides reliable SI estimates and remains more stable than the competing approach. We illustrate the practical utility of the proposed method using Canadian COVID-19 incidence data at both national and provincial levels.

</details>
