<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 20]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [VAE-MS: An Asymmetric Variational Autoencoder for Mutational Signature Extraction](https://arxiv.org/abs/2602.22239)
*Ida Egendal,Rasmus Froberg Brøndum,Dan J Woodcock,Christopher Yau,Martin Bøgsted*

Main category: stat.AP

TL;DR: 提出VAE-MS模型，结合非对称架构和概率方法提取突变特征，在真实癌症数据上重建更准确，但特征提取一致性方面无模型明显占优。


<details>
  <summary>Details</summary>
Motivation: 现有突变特征提取方法（主要基于非负矩阵分解NMF）在可靠性和临床适用性方面存在不足，需要更准确且能捕捉数据自然变异的方法。

Method: 提出VAE-MS（变分自编码器突变特征提取模型），结合非对称架构和概率方法，并与三种最先进模型（SigProfilerExtractor、MUSE-XAE、SigneR）进行比较。

Result: 概率模型（VAE-MS、SigneR）在数据重建和泛化能力上显著优于非概率模型；在模拟数据上NMF模型重建最准确，但在真实癌症数据上VAE-MS重建更准确；各模型在特征提取一致性方面无明显优劣。

Conclusion: VAE-MS通过结合非线性提取和概率建模，在真实癌症数据上表现出更好的重建性能，为突变特征分析提供了新工具，但特征提取一致性仍需改进。

Abstract: Mutational signature analysis has emerged as a powerful method for uncovering the underlying biological processes driving cancer development. However, the signature extraction process, typically performed using non-negative matrix factorization (NMF), often lacks reliability and clinical applicability. To address these limitations, several solutions have been introduced, including the use of neural networks to achieve more accurate estimates and probabilistic methods to better capture natural variation in the data. In this work, we introduce a Variational Autoencoder for Mutational Signatures (VAE-MS), a novel model that leverages both an asymmetric architecture and probabilistic methods for the extraction of mutational signatures. VAE-MS is compared to with three state-of-the-art models for mutational signature extraction: SigProfilerExtractor, the NMF-based gold standard; MUSE-XAE, an autoencoder that employs an asymmetric design without probabilistic components; and SigneR, a Bayesian NMF model, to illustrate the strength in combining a nonlinear extraction with a probabilistic model. In the ability to reconstruct input data and generalize to unseen data, models with probabilistic components (VAE-MS, SigneR) dramatically outperformed models without (SigProfilerExtractor, MUSE-XAE). The NMF-baed models (SigneR, SigProfilerExtractor) had the most accurate reconstructions in simulated data, while VAE-MS reconstructed more accurately on real cancer data. Upon evaluating the ability to extract signatures consistently, no model exhibited a clear advantage over the others. Software for VAE-MS is available at https://github.com/CLINDA-AAU/VAE-MS.

</details>


### [2] [Robust optimal reconciliation for hierarchical time series forecasting with M-estimation](https://arxiv.org/abs/2602.22694)
*Zhichao Wang,Shanshan Wang,Wei Cao,Fei Yang*

Main category: stat.AP

TL;DR: 提出了一种用于层次时间序列预测的稳健协调方法，通过M估计最小化稳健损失函数来处理异常序列


<details>
  <summary>Details</summary>
Motivation: 层次时间序列预测中经常存在地理或部门划分带来的聚合约束，需要生成符合层次结构的协调预测。传统方法对异常序列敏感，需要增强对不规则序列的鲁棒性。

Method: 采用M估计方法，通过最小化稳健损失函数来协调基础预测，使其满足聚合约束。开发了基于局部二次近似的改进牛顿-拉夫森算法来实现最小化过程。

Result: 大量数值实验表明该方法能有效处理多种异常情况（如非正态误差序列），在没有异常值时也表现出色。在澳大利亚国内旅游数据的实际应用中验证了其有效性。

Conclusion: 提出的稳健协调方法能够增强层次时间序列预测对异常序列的鲁棒性，同时在没有异常值时保持高效，具有实际应用价值。

Abstract: Aggregation constraints, arising from geographical or sectoral division, frequently emerge in a large set of time series. Coherent forecasts of these constrained series are anticipated to conform to their hierarchical structure organized by the aggregation rules. To enhance its resilience against potential irregular series, we explore the robust reconciliation process for hierarchical time series (HTS) forecasting. We incorporate M-estimation to obtain the reconciled forecasts by minimizing a robust loss function of transforming a group of base forecasts subject to the aggregation constraints. The related minimization procedure is developed and implemented through a modified Newton-Raphson algorithm via local quadratic approximation. Extensive numerical experiments are carried out to evaluate the performance of the proposed method, and the results suggest its feasibility in handling numerous abnormal cases (for instance, series with non-normal errors). The proposed robust reconciliation also demonstrates excellent efficiency when no outliers exist in HTS. Finally, we showcase the practical application of the proposed method in a real-data study on Australian domestic tourism.

</details>


### [3] [The Counterfactual Combine: A Causal Framework for Player Evaluation](https://arxiv.org/abs/2602.23233)
*Herbert P. Susmann,Antonio D'Alessandro*

Main category: stat.AP

TL;DR: 将体育运动员评估纳入因果推断框架，提出基于随机干预的球员表现评估方法，使用双重稳健估计器，并在NFL和MLB案例中应用


<details>
  <summary>Details</summary>
Motivation: 体育运动员评估与医疗提供者评估存在相似挑战，需要建立严谨的因果推断框架来准确评估球员表现，避免混杂因素影响

Method: 采用随机干预方法，将球员在重复任务中的成功率与反事实成功率进行比较；提出"随机替换性能"估计量；使用双重稳健估计器和目标最小损失估计，结合机器学习方法

Result: 建立了灵活的因果球员评估估计量类别，涵盖直接和间接标准化参数；在NFL踢球手和MLB击球手案例中展示了不同因果估计量产生不同解释和见解

Conclusion: 将体育运动员评估纳入因果推断框架提供了严谨的评估方法，不同因果估计量提供互补视角，机器学习方法能捕捉复杂关系，为体育分析提供新工具

Abstract: Evaluating sports players based on their performance shares core challenges with evaluating healthcare providers based on patient outcomes. Drawing on recent advances in healthcare provider profiling, we cast sports player evaluation within a rigorous causal inference framework and define a flexible class of causal player evaluation estimands. Using stochastic interventions, we compare player success rates on repeated tasks (such as field goal attempts or plate appearance) to counterfactual success rates had those same attempts been randomly reassigned to players according to prespecified reference distributions. This setup encompasses direct and indirect standardization parameters familiar from healthcare provider profiling, and we additionally propose a "performance above random replacement" estimand designed for interpretability in sports settings. We develop doubly robust estimators for these evaluation metrics based on modern semiparametric statistical methods, with a focus on Targeted Minimum Loss-based Estimation, and incorporate machine learning methods to capture complex relationships driving player performance. We illustrate our framework in detailed case studies of field goal kickers in the National Football League and batters in Major League Baseball, highlighting how different causal estimands yield distinct interpretations and insights about player performance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [4] [Renewable estimation in linear expectile regression models with streaming data sets](https://arxiv.org/abs/2602.22687)
*Wei Cao,Shanshan Wanga,Xiaoxue Hua*

Main category: stat.ME

TL;DR: 提出基于期望回归的在线可再生方法，用于处理具有异方差或非均匀协变量效应的流数据，相比分位数回归具有更好的计算效率


<details>
  <summary>Details</summary>
Motivation: 流数据常存在异质性（异方差或非均匀协变量效应），现有在线可再生分位数回归方法因非光滑检查函数而计算代价高，需要更高效的方法

Method: 基于期望回归的在线可再生方法，利用当前观测和历史摘要统计量更新估计，利用期望损失函数的平滑性提高计算效率

Result: 在温和正则条件下证明了估计量的一致性和渐近正态性，达到与基于完整个体数据的oracle估计量相同的统计效率；数值实验和实际应用表明方法性能与oracle估计量相当，同时保持高计算效率和低存储成本

Conclusion: 提出的在线可再生期望回归方法在统计效率和计算效率之间取得了良好平衡，特别适合处理具有异质性的流数据，为异方差检测提供了高效工具

Abstract: Streaming data often exhibit heterogeneity due to heteroscedastic variances or inhomogeneous covariate effects. Online renewable quantile and expectile regression methods provide valuable tools for detecting such heteroscedasticity by combining current data with summary statistics from historical data. However, quantile regression can be computationally demanding because of the non-smooth check function. To address this, we propose a novel online renewable method based on expectile regression, which efficiently updates estimates using both current observations and historical summaries, thereby reducing storage requirements. By exploiting the smoothness of the expectile loss function, our approach achieves superior computational efficiency compared with existing online renewable methods for streaming data with heteroscedastic variances or inhomogeneous covariate effects. We establish the consistency and asymptotic normality of the proposed estimator under mild regularity conditions, demonstrating that it achieves the same statistical efficiency as oracle estimators based on full individual-level data. Numerical experiments and real-data applications demonstrate that our method performs comparably to the oracle estimator while maintaining high computational efficiency and minimal storage costs.

</details>


### [5] [A note on the area under the likelihood and the fake evidence for model selection](https://arxiv.org/abs/2602.22965)
*L. Martino,F. Llorente*

Main category: stat.ME

TL;DR: 论文探讨了在特定模型选择问题中使用非正常先验的可能性，提出了"伪证据"概念，并指出扩散先验无法渐近恢复均匀非正常先验下的似然面积。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯证据计算中不允许使用非正常先验，因为这会引入任意常数。但本文发现，在特定模型选择场景下（同一参数族内的模型比较），非正常先验仍可被使用，需要重新定义相关概念。

Method: 从理论角度分析非正常先验在模型选择中的适用性，提出"伪证据"和"似然面积"概念。以非线性基的贝叶斯回归模型为例，分别考虑均匀非正常先验和高斯先验两种情况，并通过数值实验验证理论分析。

Result: 证明了在特定模型选择问题中可以使用非正常先验，但相关量不能称为贝叶斯证据，应称为"伪证据"。发现扩散先验的尺度参数趋于无穷时，无法渐近恢复均匀非正常先验下的似然面积。数值实验证实了理论结论。

Conclusion: 非正常先验可在同一参数族内的模型选择中使用，但需要重新定义相关概念为"伪证据"。扩散先验无法渐近逼近均匀非正常先验的结果，这对贝叶斯模型选择方法有重要启示。

Abstract: Improper priors are not allowed for the computation of the Bayesian evidence $Z=p({\bf y})$ (a.k.a., marginal likelihood), since in this case $Z$ is not completely specified due to an arbitrary constant involved in the computation. However, in this work, we remark that they can be employed in a specific type of model selection problem: when we have several (possibly infinite) models belonging to the same parametric family (i.e., for tuning parameters of a parametric model). However, the quantities involved in this type of selection cannot be considered as Bayesian evidences: we suggest to use the name ``fake evidences'' (or ``areas under the likelihood'' in the case of uniform improper priors). We also show that, in this model selection scenario, using a diffuse prior and increasing its scale parameter asymptotically to infinity, we cannot recover the value of the area under the likelihood, obtained with a uniform improper prior. We first discuss it from a general point of view. Then we provide, as an applicative example, all the details for Bayesian regression models with nonlinear bases, considering two cases: the use of a uniform improper prior and the use of a Gaussian prior, respectively. A numerical experiment is also provided confirming and checking all the previous statements.

</details>


### [6] [The global structure of the time delay likelihood](https://arxiv.org/abs/2602.22307)
*Namu Kroupa,Will Handley*

Main category: stat.ME

TL;DR: 论文揭示了时间延迟推断中似然函数存在边界驱动的"W"形病理结构，导致标准推断方法失效，需要更严格的收敛标准和更多采样点


<details>
  <summary>Details</summary>
Motivation: 识别时间延迟推断中似然函数的基本病理结构，这种结构挑战标准推断方法，可能导致错误的边缘模式和H0估计偏差

Method: 通过分析高斯过程光变曲线模型的时间延迟似然函数，使用模拟验证病理结构，并提供具体指导（如增加活点数量）确保收敛

Result: 发现似然函数在真实延迟处有全局最大值，但在观测窗口边缘逐渐上升形成"W"形，全局采样器会被误导到虚假边缘模式，数据密度越高效应越强

Conclusion: 阐明了时间延迟推断的失效模式，提供了确保稳健全贝叶斯时间延迟推断的实际补救措施，特别是需要更严格的收敛标准和更多采样点

Abstract: We identify a fundamental pathology in the likelihood for time delay inference which challenges standard inference methods. By analysing the likelihood for time delay inference with Gaussian process light curve models, we show that it generically develops a boundary-driven "W"-shape with a global maximum at the true delay and gradual rises towards the edges of the observation window. This arises because time delay estimation is intrinsically extrapolative. In practice, global samplers such as nested sampling are steered towards spurious edge modes unless strict convergence criteria are adopted. We demonstrate this with simulations and show that the effect strengthens with higher data density over a fixed time span. To ensure convergence, we provide concrete guidance, notably increasing the number of live points. Further, we show that methods implicitly favouring small delays, for example optimisers and local MCMC, induce a bias towards larger $H_0$. Our results clarify failure modes and offer practical remedies for robust fully Bayesian time delay inference.

</details>


### [7] [Predictive variational inference for flexible regression models](https://arxiv.org/abs/2602.22582)
*Lucas Kock,Scott A. Sisson,G. S. Rodrigues,David J. Nott*

Main category: stat.ME

TL;DR: 提出GM-PVI方法，将预测变分推断扩展为高斯混合形式，建立与专家混合模型插值预测的等价关系，提升模型诊断能力


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯预测使用后验分布积分参数，但真实后验难以处理时需用变分近似。现有预测变分推断(PVI)方法虽能提升预测性能并诊断模型缺陷，但在线性预测器模型中解释性不足。本文旨在改进PVI方法的可解释性，使其更好地作为诊断工具。

Method: 采用高斯混合形式的PVI后验(GM-PVI)，建立与专家混合模型插值预测的等价关系。主要贡献：1) 证明在广义线性模型及其层次扩展中，GM-PVI预测等价于协变量独立权重的专家混合模型插值预测；2) 扩展标准PVI，允许GM-PVI后验随预测协变量变化，建立与协变量依赖权重专家混合模型的等价关系；3) 在多个模型中验证诊断价值。

Result: GM-PVI方法成功建立了预测变分推断与专家混合模型之间的理论联系，提升了PVI方法的可解释性。在广义线性模型、线性混合模型和潜在高斯过程模型等示例中，该方法能有效诊断模型缺陷，展示原始模型参数如何在协变量空间中变化以改进预测。

Conclusion: GM-PVI方法通过高斯混合形式的变分后验，显著提升了预测变分推断作为模型诊断工具的可解释性。该方法建立了与专家混合模型的理论等价关系，能有效识别模型在协变量空间中的适应性需求，为模型改进提供指导。

Abstract: A conventional Bayesian approach to prediction uses the posterior distribution to integrate out parameters in a density for unobserved data conditional on the observed data and parameters. When the true posterior is intractable, it is replaced by an approximation; here we focus on variational approximations. Recent work has explored methods that learn posteriors optimized for predictive accuracy under a chosen scoring rule, while regularizing toward the prior or conventional posterior. Our work builds on an existing predictive variational inference (PVI) framework that improves prediction, but also diagnoses model deficiencies through implicit model expansion. In models where the sampling density depends on the parameters through a linear predictor, we improve the interpretability of existing PVI methods as a diagnostic tool. This is achieved by adopting PVI posteriors of Gaussian mixture form (GM-PVI) and establishing connections with plug-in prediction for mixture-of-experts models. We make three main contributions. First, we show that GM-PVI prediction is equivalent to plug-in prediction for certain mixture-of-experts models with covariate-independent weights in generalized linear models and hierarchical extensions of them. Second, we extend standard PVI by allowing GM-PVI posteriors to vary with the prediction covariate and in this case an equivalence to plug-in prediction for mixtures of experts with covariate-dependent weights is established. Third, we demonstrate the diagnostic value of this approach across several examples, including generalized linear models, linear mixed models, and latent Gaussian process models, demonstrating how the parameters of the original model must vary across the covariate space to achieve improvements in prediction.

</details>


### [8] [A General (Non-Markovian) Framework for Covariate Adaptive Randomization: Achieving Balance While Eliminating the Shift](https://arxiv.org/abs/2602.22648)
*Hengjia Fang,Wei Ma*

Main category: stat.ME

TL;DR: 提出一种新的协变量自适应随机化方法，解决现有方法中的偏移问题，通过参数化分配函数和非马尔可夫过程实现协变量平衡。


<details>
  <summary>Details</summary>
Motivation: 现有协变量自适应随机化方法在支持不等目标分配比例时存在偏移问题，即某些额外协变量的分配比例偏离目标值，这源于条件平均分配比例与目标值之间的不匹配。

Method: 推导新的分配函数形式，要求平衡协变量能确保分配比例匹配目标值；设计参数化分配函数类；提出基于收集协变量信息更新参数的非马尔可夫随机化过程；建立允许非马尔可夫过程的CAR框架。

Result: 建立了关键理论性质：协变量不平衡的概率有界性，以及额外协变量不平衡的渐近分布；证明可行随机化过程能实现协变量平衡并消除偏移。

Conclusion: 提出的可行随机化方法能够实现协变量平衡并消除偏移问题，为支持不等目标分配比例的灵活CAR方法提供了有效解决方案。

Abstract: Emerging applications increasingly demand flexible covariate adaptive randomization (CAR) methods that support unequal targeted allocation ratios. While existing procedures can achieve covariate balance, they often suffer from the shift problem. This occurs when the allocation ratios of some additional covariates deviate from the target. We show that this problem is equivalent to a mismatch between the conditional average allocation ratio and the target among units sharing specific covariate values, revealing a failure of existing procedures in the long run. To address it, we derive a new form of allocation function by requiring that balancing covariates ensures the ratio matches the target. Based on this form, we design a class of parameterized allocation functions. When the parameter roughly matches certain characteristics of the covariate distribution, the resulting procedure can balance covariates. Thus, we propose a feasible randomization procedure that updates the parameter based on collected covariate information, rendering the procedure non-Markovian. To accommodate this, we introduce a CAR framework that allows non-Markovian procedure. We then establish its key theoretical properties, including the boundedness of covariate imbalance in probability and the asymptotic distribution of the imbalance for additional covariates. Ultimately, we conclude that the feasible randomization procedure can achieve covariate balance and eliminate the shift.

</details>


### [9] [Modeling Covariate Feedback, Reversal, and Latent Traits in Longitudinal Data: A Joint Hierarchical Framework](https://arxiv.org/abs/2602.22588)
*Niloofar Ramezani,Pascal Nitiema,Jeffrey R. Wilson*

Main category: stat.ME

TL;DR: 提出分层联合建模框架，处理纵向研究中具有双向反馈、角色转换和潜在异质性的时变协变量


<details>
  <summary>Details</summary>
Motivation: 传统统计框架假设协变量角色固定且外生，无法处理具有动态反馈、角色转换和个体异质性的复杂系统

Method: 三层建模框架：反馈驱动的纵向过程、基于潜在特质的协变量联合建模、连接二元终点的结果模型；使用最大似然和贝叶斯推断（Hamiltonian Monte Carlo）进行估计

Result: 模拟研究显示模型具有良好的校准覆盖、小偏差和优于传统方法的预测性能；在美国面板数据分析中成功捕捉了体力活动与BMI的协同演化及其对收入流动性的影响

Conclusion: 该框架为分析具有反馈、协变量角色转换和未观测能力的纵向决策系统提供了灵活实用的工具

Abstract: Time-varying covariates in longitudinal studies frequently evolve through reciprocal feedback, undergo role reversal, and reflect unobserved individual heterogeneity. Standard statistical frameworks often assume fixed covariate roles and exogenous predictors, limiting their utility in systems governed by dynamic behavioral or physiological processes. We develop a hierarchical joint modeling framework that unifies three key features of such systems: (i) bidirectional feedback between a binary and a continuous covariate, (ii) role reversal in which these covariates become jointly modeled outcomes at a prespecified decision phase, and (iii) a shared latent trait influencing both intermediate covariates and a final binary endpoint. The model proceeds in three phases: a feedback-driven longitudinal process, a reversal phase in which the two covariates are jointly modeled conditional on the latent trait, and an outcome model linking a binary, decision-relevant endpoint to observed and latent components. Estimation is carried out using maximum likelihood and Bayesian inference, with Hamiltonian Monte Carlo supporting robust posterior estimation for models with latent structure and mixed outcome types. Simulation studies show that the model yields well calibrated coverage, small bias, and improved predictive performance compared to standard generalized linear mixed models, marginal approaches, and models that ignore feedback or latent traits. In an analysis of nationally representative U.S. panel data, the model captures the co-evolution of physical activity and body mass index and their joint influence, moderated by a latent behavioral resilience factor, on income mobility. The framework offers a flexible, practically implementable tool for analyzing longitudinal decision systems in which feedback, covariate role transition, and unmeasured capacity are central to prediction and intervention.

</details>


### [10] [Testing Partially-Identifiable Causal Queries Using Ternary Tests](https://arxiv.org/abs/2602.23020)
*Sourbh Bhadane,Joris M. Mooij,Philip Boeken,Onno Zoeter*

Main category: stat.ME

TL;DR: 提出使用三元（三结果）统计检验来测试部分可识别的因果查询，建立了三元检验的可测试性要求，证明了通过组合二元检验获得三元检验的完备性，并展示了拓扑条件在构建具体因果假设检验问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 由于因果模型到观测分布的映射不是一一对应的，因果查询通常只能部分识别。当使用二元统计检验测试部分可识别的因果查询时，其结果不能直接转化为因果假设检验问题，因此需要新的检验方法。

Method: 提出使用三元统计检验（接受、拒绝、不确定三种结果）来测试部分可识别的因果查询。建立了三元检验在一致一致性方面的可测试性要求，并给出了等价的拓扑条件。通过证明组合二元检验获得三元检验的完备性来利用现有二元检验工具箱。最后展示了如何利用拓扑条件为两个具体问题构建三元检验。

Result: 建立了三元检验的可测试性理论框架，证明了通过组合二元检验获得三元检验的完备性，成功将拓扑条件应用于构建工具变量不等式检验和治疗效果比较检验等具体因果假设检验问题。

Conclusion: 三元统计检验为解决部分可识别因果查询的假设检验问题提供了有效框架，通过拓扑条件指导检验构建，并能够充分利用现有的二元检验工具箱，为因果推断中的假设检验提供了新的方法论。

Abstract: We consider hypothesis testing of binary causal queries using observational data. Since the mapping of causal models to the observational distribution that they induce is not one-to-one, in general, causal queries are often only partially identifiable. When binary statistical tests are used for testing partially-identifiable causal queries, their results do not translate in a straightforward manner to the causal hypothesis testing problem. We propose using ternary (three-outcome) statistical tests to test partially-identifiable causal queries. We establish testability requirements that ternary tests must satisfy in terms of uniform consistency and present equivalent topological conditions on the hypotheses. To leverage the existing toolbox of binary tests, we prove that obtaining ternary tests by combining binary tests is complete. Finally, we demonstrate how topological conditions serve as a guide to construct ternary tests for two concrete causal hypothesis testing problems, namely testing the instrumental variable (IV) inequalities and comparing treatment efficacy.

</details>


### [11] [Data-Efficient Generative Modeling of Non-Gaussian Global Climate Fields via Scalable Composite Transformations](https://arxiv.org/abs/2602.23311)
*Johannes Brachem,Paul F. V. Wiemann,Matthias Katzfuss*

Main category: stat.ME

TL;DR: 提出一个数据高效的气候场内部变率模拟框架，通过复合变换构建联合分布，仅需10个训练样本即可超越需要80个样本的现有方法


<details>
  <summary>Details</summary>
Motivation: 物理气候模型计算成本过高，导致训练数据稀缺，限制了未来气候预测的不确定性量化。需要开发数据高效的方法来模拟全球气候场的内部变率。

Method: 受copula建模启发，通过复合变换到多元标准正态空间构建联合分布。结合非参数贝叶斯传输图进行空间依赖建模，以及灵活的空间变化边际模型（参数模型+B样条校正）。边际参数使用低秩高斯过程先验进行空间平滑，计算成本与空间维度呈线性关系。

Result: 应用于全球5万多个网格点的对数降水率场，随机代理模型实现了高保真度，准确量化了气候分布的空间依赖和边际特征（包括尾部）。仅用10个训练样本就超越了需要80个样本的最先进方法，相当于将气候研究的计算预算提高了8倍。

Conclusion: 该框架为数据稀缺条件下的气候场内部变率模拟提供了高效解决方案，显著降低了计算需求，有助于改进未来气候预测的不确定性量化。

Abstract: Quantifying uncertainty in future climate projections is hindered by the prohibitive computational cost of running physical climate models, which severely limits the availability of training data. We propose a data-efficient framework for emulating the internal variability of global climate fields, specifically designed to overcome these sample-size constraints. Inspired by copula modeling, our approach constructs a highly expressive joint distribution via a composite transformation to a multivariate standard normal space. We combine a nonparametric Bayesian transport map for spatial dependence modeling with flexible, spatially varying marginal models, essential for capturing non-Gaussian behavior and heavy-tailed extremes. These marginals are defined by a parametric model followed by a semi-parametric B-spline correction to capture complex distributional features. The marginal parameters are spatially smoothed using Gaussian-process priors with low-rank approximations, rendering the computational cost linear in the spatial dimension. When applied to global log-precipitation-rate fields at more than 50,000 grid locations, our stochastic surrogate achieves high fidelity, accurately quantifying the climate distribution's spatial dependence and marginal characteristics, including the tails. Using only 10 training samples, it outperforms a state-of-the-art competitor trained on 80 samples, effectively octupling the computational budget for climate research. We provide a Python implementation at https://github.com/jobrachem/ppptm .

</details>


### [12] [Beyond Vintage Rotation: Bias-Free Sparse Representation Learning with Oracle Inference](https://arxiv.org/abs/2602.22590)
*Chengyu Cui,Yunxiao Chen,Jing Ouyang,Gongjun Xu*

Main category: stat.ME

TL;DR: 传统旋转方法存在不可估计的偏差，阻碍了有效推断；本文提出无偏旋转方法，在表示学习中实现理想推断性能。


<details>
  <summary>Details</summary>
Motivation: 尽管旋转方法在统计学和机器学习中已使用近一个世纪来获得稀疏可解释表示，但缺乏对学习表示的有效推断保证。研究发现传统旋转方法存在普遍的非可估计偏差，这从根本上阻碍了置信区间构建和假设检验等有效推断程序的开发。

Method: 提出了一种新颖的无偏旋转方法，基于潜变量的一般表示学习框架。建立了学习稀疏表示的oracle推断性质：估计器达到与理想情况下观测到潜变量相同的渐近方差。开发了高效计算框架，证明其输出估计器保持相同的oracle性质。

Result: 为旋转估计器提供了严格的推断程序，实现了统计有效且可解释的表示学习。估计器在渐近意义上达到与理想观测情况相同的方差性能。

Conclusion: 解决了传统旋转方法长期存在的推断问题，提出了首个具有严格统计保证的无偏旋转方法，为表示学习提供了有效的推断工具，弥合了理论与计算之间的差距。

Abstract: Learning low-dimensional latent representations is a central topic in statistics and machine learning, and rotation methods have long been used to obtain sparse and interpretable representations. Despite nearly a century of widespread use across many fields, rigorous guarantees for valid inference for the learned representation remain lacking. In this paper, we identify a surprisingly prevalent phenomenon that suggests a reason for this gap: for a broad class of vintage rotations, the resulting estimators exhibit a non-estimable bias. Because this bias is independent of the data, it fundamentally precludes the development of valid inferential procedures, including the construction of confidence intervals and hypothesis testing. To address this challenge, we propose a novel bias-free rotation method within a general representation learning framework based on latent variables. We establish an oracle inference property for the learned sparse representations: the estimators achieve the same asymptotic variance as in the ideal setting where the latent variables are observed. To bridge the gap between theory and computation, we develop an efficient computational framework and prove that its output estimators retain the same oracle property. Our results provide a rigorous inference procedure for the rotated estimators, yielding statistically valid and interpretable representation learning.

</details>


### [13] [Semiparametric Joint Inference for Sensitivity and Specificity at the Youden-Optimal Cut-Off](https://arxiv.org/abs/2602.23045)
*Siyan Liu,Qinglong Tian,Chunlin Wang,Pengfei Li*

Main category: stat.ME

TL;DR: 提出基于密度比模型的半参数框架，用于在Youden最优截断点下对灵敏度和特异性进行联合推断，使用最大经验似然估计，相比现有方法在效率和稳健性上有优势。


<details>
  <summary>Details</summary>
Motivation: 连续生物标志物用于疾病诊断时，在最优诊断截断点评估的灵敏度和特异性是分类准确性的基本指标。现有方法主要基于参数假设或完全非参数程序，可能对模型误设敏感或在中等样本中缺乏效率。

Method: 提出基于密度比模型的半参数框架，使用最大经验似然方法推导最优截断点及相应灵敏度和特异性的估计量，建立它们的联合渐近正态性，从而得到Wald型和保持范围的对数变换置信区域。

Result: 模拟研究表明，该方法在各种分布设置下相对于现有参数和非参数替代方法，实现了准确的覆盖率并提高了效率。COVID-19抗体数据分析展示了该方法在诊断决策中的实际优势。

Conclusion: 提出的半参数框架为在Youden最优截断点下对灵敏度和特异性进行联合推断提供了稳健且高效的方法，在诊断准确性评估中具有实用价值。

Abstract: Sensitivity and specificity evaluated at an optimal diagnostic cut-off are fundamental measures of classification accuracy when continuous biomarkers are used for disease diagnosis. Joint inference for these quantities is challenging because their estimators are evaluated at a common, data-driven threshold estimated from both diseased and healthy samples, inducing statistical dependence. Existing approaches are largely based on parametric assumptions or fully nonparametric procedures, which may be sensitive to model misspecification or lack efficiency in moderate samples. We propose a semiparametric framework for joint inference on sensitivity and specificity at the Youden-optimal cut-off under the density ratio model. Using maximum empirical likelihood, we derive estimators of the optimal threshold and the corresponding sensitivity and specificity, and establish their joint asymptotic normality. This leads to Wald-type and range-preserving logit-transformed confidence regions. Simulation studies show that the proposed method achieves accurate coverage with improved efficiency relative to existing parametric and nonparametric alternatives across a variety of distributional settings. An analysis of COVID-19 antibody data demonstrates the practical advantages of the proposed approach for diagnostic decision-making.

</details>


### [14] [Feasible Fusion: Constrained Joint Estimation under Structural Non-Overlap](https://arxiv.org/abs/2602.22612)
*Yuxi Du,Zhiheng Zhang,Haoxuan Li,Cong Fang,Jixing Xu,Peng Zhen,Jiecheng Guo*

Main category: stat.ME

TL;DR: 提出一个约束联合估计框架，通过正交实验矩条件强制因果有效性，解决处理诱导的结构性非重叠问题，在原始协变量空间不可行时学习表示和预测器。


<details>
  <summary>Details</summary>
Motivation: 现代大规模因果推断面临高维协变量、多值处理、海量观测数据和有限随机对照试验样本的挑战。处理诱导的结构性非重叠导致常用加权融合方法无法满足随机识别限制。

Method: 提出约束联合估计框架，最小化观测风险的同时通过正交实验矩条件强制因果有效性。当原始协变量空间存在可行性障碍时，开发惩罚原始对偶算法联合学习表示和预测器。

Result: 建立了将误差分解为重叠恢复、矩违反和统计项的oracle不等式。合成实验显示在不同非重叠程度下具有鲁棒性能。大规模网约车应用显示方法显著优于现有基线，达到使用更多RCT数据训练模型的性能。

Conclusion: 提出的框架有效解决了处理诱导的结构性非重叠问题，通过联合学习表示和预测器，在有限RCT数据下实现了与使用更多RCT数据相当的因果推断性能。

Abstract: Causal inference in modern largescale systems faces growing challenges, including highdimensional covariates, multi-valued treatments, massive observational (OBS) data, and limited randomized controlled trial (RCT) samples due to cost constraints. We formalize treatment-induced structural non-overlap and show that, under this regime, commonly used weighted fusion methods provably fail to satisfy randomized identifying restrictions.To address this issue,we propose a constrained joint estimation framework that minimizes observational risk while enforcing causal validity through orthogonal experimental moment conditions. We further show that structural non-overlap creates a feasibility obstruction for moment enforcement in the original covariate space.We also derive a penalized primaldual algorithm that jointly learns representations and predictors, and establish oracle inequalities decomposing error into overlap recovery, moment violation, and statistical terms.Extensive synthetic experiments demonstrate robust performance under varying degrees of nonoverlap. A largescale ridehailing application shows that our method achieves substantial gains over existing baselines, matching the performance of models trained with significantly more RCT data.

</details>


### [15] [Identifiability of Treatment Effects with Unobserved Spatially Varying Confounders](https://arxiv.org/abs/2602.23291)
*Tommy Tang,Xinran Li,Bo Li*

Main category: stat.ME

TL;DR: 本文研究了存在未测量空间混杂因素时的因果效应识别问题，建立了线性模型下处理效应可识别的理论框架，并指出了不可识别的情况。


<details>
  <summary>Details</summary>
Motivation: 在存在未测量空间混杂因素的情况下进行因果推断时，缺乏一个通用的可识别性框架。这对于从观测数据中进行可靠的因果推断至关重要。

Method: 使用线性模型，对未测量混杂因素与感兴趣暴露之间的协方差结构做出各种参数模型假设。在观测位置结构和暴露-混杂关联的温和条件下，建立了处理效应的可识别性。

Result: 对于许多常用的离散和连续数据空间模型，在温和条件下建立了处理效应的可识别性。同时强调了可能不可识别的模型或场景。

Conclusion: 为存在未测量空间混杂因素的因果推断提供了可识别性理论框架，指出了可识别和不可识别的情况，为统计推断提供了指导。

Abstract: The study of causal effects in the presence of unmeasured spatially varying confounders has garnered increasing attention. However, a general framework for identifiability, which is critical for reliable causal inference from observational data, has yet to be advanced. In this paper, we study a linear model with various parametric model assumptions on the covariance structure between the unmeasured confounder and the exposure of interest. We establish identifiability of the treatment effect for many commonly 20 used spatial models for both discrete and continuous data, under mild conditions on the structure of observation locations and the exposure-confounder association. We also emphasize models or scenarios where identifiability may not hold, under which statistical inference should be conducted with caution.

</details>


### [16] [Learning about Corner Kicks in Soccer by Analysis of Event Times Using a Frailty Model](https://arxiv.org/abs/2602.22684)
*Riley L Isaacs,X. Joan Hu,K. Ken Peng,Tim Swartz*

Main category: stat.ME

TL;DR: 本文扩展了Peng等人(2024)的角球时间模型，通过引入脆弱模型处理同一比赛中同一球队角球之间的相关性，并使用MCEM算法进行参数估计。


<details>
  <summary>Details</summary>
Motivation: 角球是足球中的重要事件，但现有模型未考虑同一比赛中同一球队角球之间的相关性。本文旨在扩展Peng等人(2024)的模型，通过引入脆弱模型来捕捉这种相关性。

Method: 采用脆弱模型处理角球事件时间的相关性，应用蒙特卡洛期望最大化(MCEM)算法获得模型参数的最大似然估计，并与Peng等人的模型进行似然比检验比较。

Result: 使用2019年中超联赛数据进行实证分析，通过似然比检验验证了所提模型相比Peng等人模型的改进效果，表明考虑角球相关性能够更好地描述角球时间模式。

Conclusion: 提出的脆弱模型能够有效捕捉同一比赛中同一球队角球之间的相关性，相比现有模型提供了更准确的角球时间分析框架，对足球分析和预测有实际应用价值。

Abstract: Corner kicks are an important event in soccer because they are often the result of strong attacking play and can be of keen interest to sports fans and bettors. Peng, Hu, and Swartz (2024, Computational Statistics) formulate the mixture feature of corner kick times caused by previous corner kicks, frame the commonly available corner kick data as right-censored event times, and explore patterns of corner kicks. This paper extends their modeling to accommodate the potential correlations between corner kicks by the same teams within the same games. We con- sider a frailty model for event times and apply the Monte Carlo Expec- tation Maximization (MCEM) algorithm to obtain the maximum like- lihood estimates for the model parameters. We compare the proposed model with the model in Peng, Hu, and Swartz (2024) using likelihood ratio tests. The 2019 Chinese Super League (CSL) data are employed throughout the paper for motivation and illustration.

</details>


### [17] [Asymptotic Theory and Sequential Test for General Multi-Armed Bandit Process](https://arxiv.org/abs/2602.22768)
*Li Yang,Xiaodong Yan,Dandan Jiang*

Main category: stat.ME

TL;DR: 提出Urn Bandit (UNB)过程，将瓮概率模型的强化机制与多臂老虎机原理结合，在非独立同分布、非次高斯和次线性奖励样本下建立联合函数中心极限定理，实现资源分配几乎必然收敛到最优臂，同时保持统计检验性能。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机过程局限于同时自适应分配和顺序检验，缺乏非独立同分布序列和次线性信息下的渐近理论。需要解决这一开放挑战，建立既能优化奖励又能保持统计检验性能的框架。

Method: 提出Urn Bandit (UNB)过程，整合瓮概率模型的强化机制与MAB原理。建立非独立同分布、非次高斯、次线性奖励样本下期望奖励一致估计量的联合函数中心极限定理(FCLT)。支持臂比较、A/B测试和政策评估等顺序检验。

Result: UNB确保资源分配几乎必然收敛到最优臂。仿真和真实数据分析表明，UNB保持等随机化设计的统计检验性能，同时获得类似经典MAB过程的更高平均奖励。

Conclusion: UNB过程解决了传统MAB在非独立同分布和次线性信息下的理论限制，实现了自适应分配与统计检验的统一框架，为顺序决策提供了理论保证。

Abstract: Multi-armed bandit (MAB) processes constitute a foundational subclass of reinforcement learning problems and represent a central topic in statistical decision theory, but are limited to simultaneous adaptive allocation and sequential test, because of the absence of asymptotic theory under non-i.i.d sequence and sublinear information. To address this open challenge, we propose Urn Bandit (UNB) process to integrate the reinforcement mechanism of urn probabilistic models with MAB principles, ensuring almost sure convergence of resource allocation to optimal arms. We establish the joint functional central limit theorem (FCLT) for consistent estimators of expected rewards under non-i.i.d., non-sub-Gaussian and sublinear reward samples with pairwise correlations across arms. To overcome the limitations of existing methods that focus mainly on cumulative regret, we establish the asymptotic theory along with adaptive allocation that serves powerful sequential test, such as arms comparison, A/B testing, and policy valuation. Simulation studies and real data analysis demonstrate that UNB maintains statistical test performance of equal randomization (ER) design but obtain more average rewards like classical MAB processes.

</details>


### [18] [Rejoinder to the discussants of the two JASA articles `Frequentist Model Averaging' and `The Focused Information Ctierion', by Nils Lid Hjort and Gerda Claeskens](https://arxiv.org/abs/2602.22803)
*Nils Lid Hjort,Gerda Claeskens*

Main category: stat.ME

TL;DR: 作者对专家讨论的回应，感谢评论并回应主要问题


<details>
  <summary>Details</summary>
Motivation: 回应专家对FMA和FIC两篇论文的深入讨论和评论，表达感谢并针对主要问题进行回复

Method: 按主题而非按讨论者组织回应，参考两篇论文（FMA论文和FIC论文）

Result: 无法回应所有问题，但针对主要议题进行了系统性回复

Conclusion: 感谢专家讨论，通过主题式回应澄清和深化了论文的核心观点

Abstract: We are honoured to have our work read and discussed at such a thorough level by several experts. Words of appreciation and encouragement are gratefully received, while the many supplementary comments, thoughtful reminders, new perspectives and additional themes raised are warmly welcomed and deeply appreciated. Our thanks go also to JASA Editor Francisco Samaniego and his editorial helpers for organising this discussion.
  Space does not allow us answering all of the many worthwhile points raised by our discussants, but in the following we make an attempt to respond to what we perceive of as being the major issues. Our responses are organised by themes rather than by discussants. We shall refer to our two articles as `the FMA paper' (Hjort and Claeskens) and `the FIC paper' (Claeskens and Hjort).

</details>


### [19] [Projection depth for functional data: Practical issues, computation and applications](https://arxiv.org/abs/2602.22877)
*Filip Bočinec,Stanislav Nagy,Hyemin Yeon*

Main category: stat.ME

TL;DR: 本文研究了正则化投影深度(RPD)在函数数据分析中的实际应用，提出了调参方法和高效计算策略，证明了RPD在异常检测、分类和假设检验等任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 函数数据具有复杂的模式，传统的统计分析方法面临挑战。函数深度提供了一种有效反映函数数据排序结构的方法，但需要更好地处理复杂形状特征。

Method: 研究正则化投影深度(RPD)的调参策略，提出基于随机投影的高效计算方法，并通过理论分析支持该方法。调参参数控制数据降维的程度。

Result: 通过全面的数值研究，RPD在函数异常检测、分类和两样本假设检验等任务中优于其他基于深度的方法，特别擅长揭示函数数据的形状特征。

Conclusion: 正则化投影深度(RPD)能够有效处理函数数据的复杂形状特征，通过适当的调参和高效计算策略，在多种统计应用中表现出色，为函数数据分析提供了有力的工具。

Abstract: Statistical analysis of functional data is challenging due to their complex patterns, for which functional depth provides an effective means of reflecting their ordering structure. In this work, we investigate practical aspects of the recently proposed regularized projection depth (RPD), which induces a meaningful ordering of functional data while appropriately accommodating their complex shape features. Specifically, we examine the impact and choice of its tuning parameter, which regulates the degree of effective dimension reduction applied to the data, and propose a random projection-based approach for its efficient computation, supported by theoretical justification. Through comprehensive numerical studies, we explore a wide range of statistical applications of the RPD and demonstrate its particular usefulness in uncovering shape features in functional data analysis. This ability allows the RPD to outperform competing depth-based methods, especially in tasks such as functional outlier detection, classification, and two-sample hypothesis testing.

</details>


### [20] [permApprox: a general framework for accurate permutation p-value approximation](https://arxiv.org/abs/2602.22975)
*Stefanie Peschel,Anne-Laure Boulesteix,Erika von Mutius,Christian L. Müller*

Main category: stat.ME

TL;DR: 提出permApprox方法解决置换检验中p值为零的问题，通过GPD尾部建模和约束参数估计确保p值非零且准确


<details>
  <summary>Details</summary>
Motivation: 置换检验中当置换次数较少时，经验p值可能为零（当观测统计量超过所有置换值时），这种零p值在统计上无效且阻碍多重检验校正。现有GPD尾部建模方法在形状参数为负时仍可能产生零p值。

Method: 基于广义帕累托分布(GPD)尾部建模，在参数估计时强制执行支持约束以确保对观测统计量之外的有效外推，严格避免零p值。工作流还包括稳健参数估计、数据驱动的阈值选择，以及处理离散-连续混合p值的原理方法。

Result: 通过两样本t检验和Wilcoxon秩和检验的广泛模拟显示，permApprox在各种样本量和效应量下都能产生准确、稳健且非零的p值近似。在单细胞RNA-seq和微生物组数据应用中，即使置换次数很少也能产生平滑可解释的p值分布。

Conclusion: permApprox通过解决零p值问题同时保持准确性和计算效率，使得在高维和计算密集型设置中能够进行可靠的基于置换的推断。

Abstract: Permutation procedures are common practice in hypothesis testing when distributional assumptions about the test statistic are not met or unknown. With only few permutations, empirical p-values lie on a coarse grid and may even be zero when the observed test statistic exceeds all permuted values. Such zero p-values are statistically invalid and hinder multiple testing correction. Parametric tail modeling with the Generalized Pareto Distribution (GPD) has been proposed to address this issue, but existing implementations can again yield zero p-values when the estimated shape parameter is negative and the fitted distribution has a finite upper bound.
  We introduce a method for accurate and zero-free p-value approximation in permutation testing, embedded in the permApprox workflow and R package. Building on GPD tail modeling, the method enforces a support constraint during parameter estimation to ensure valid extrapolation beyond the observed statistic, thereby strictly avoiding zero p-values. The workflow further integrates robust parameter estimation, data-driven threshold selection, and principled handling of hybrid p-values that are discrete in the bulk and continuous in the extreme tail.
  Extensive simulations using two-sample t-tests and Wilcoxon rank-sum tests show that permApprox produces accurate, robust, and zero-free p-value approximations across a wide range of sample and effect sizes. Applications to single-cell RNA-seq and microbiome data demonstrate its practical utility: permApprox yields smooth and interpretable p-value distributions even with few permutations. By resolving the zero-p-value problem while preserving accuracy and computational efficiency, permApprox enables reliable permutation-based inference in high-dimensional and computationally intensive settings.

</details>


### [21] [Dimension Reduction in Multivariate Extremes via Latent Linear Factor Models](https://arxiv.org/abs/2602.23143)
*Alexis Boulin,Axel Bücher*

Main category: stat.ME

TL;DR: 提出基于潜在线性因子结构的高维尾部依赖模型，通过低维因子实现维度约简，具有可解释性和可扩展性，适用于高维尾部风险分析。


<details>
  <summary>Details</summary>
Motivation: 高维尾部依赖建模需要可解释且可扩展的方法，传统方法在高维情况下难以处理且缺乏可解释性。本文旨在开发基于潜在因子结构的尾部依赖模型，实现维度约简和可解释性。

Method: 提出基于潜在K因子模型的尾部依赖结构，其中K远小于维度d，实现维度约简。模型支持稀疏加载结构，每个分量仅受少量因子影响。建立了模型参数的可识别性，并基于无边际尾部成对依赖矩阵提供构造性恢复程序，开发了基于秩的估计方法。

Result: 建立了模型参数的可识别性条件，开发了基于秩的实用估计方法。模型能自然结合边际尾部模型，特别适合高维设置。在空间风能应用中展示了实用性，通过潜在因子结构可估计大量风机同时低于切入风速阈值的风险。

Conclusion: 提出的基于潜在线性因子结构的高维尾部依赖模型具有可解释性、可扩展性和实用性，为高维尾部风险分析提供了有效框架，特别适用于需要同时考虑大量变量极端依赖关系的应用场景。

Abstract: We propose a new and interpretable class of high-dimensional tail dependence models based on latent linear factor structures. Specifically, extremal dependence of an observable vector is assumed to be driven by a lower-dimensional latent $K$-factor model, where $K \ll d$, thereby inducing an explicit form of dimension reduction. Geometrically, this is reflected in the support of the associated spectral dependence measure, whose intrinsic dimension is at most $K-1$. The loading structure may additionally exhibit sparsity, meaning that each component is influenced by only a small number of latent factors, which further enhances interpretability and scalability. Under mild structural assumptions, we establish identifiability of the model parameters and provide a constructive recovery procedure based on a margin-free tail pairwise dependence matrix, which also yields practical rank-based estimation methods. The framework combines naturally with marginal tail models and is particularly well suited to high-dimensional settings. We illustrate its applicability in a spatial wind energy application, where the latent factor structure enables tractable estimation of the risk that a large proportion of turbines simultaneously fall below their cut-in wind speed thresholds.

</details>


### [22] [Randomization Tests in Switchback Experiments](https://arxiv.org/abs/2602.23257)
*Jizhou Liu,Liang Zhong*

Main category: stat.ME

TL;DR: 开发了一种用于时间切换实验的随机化检验框架，提供有限样本有效、无分布假设的p值，处理序列依赖、季节性和时间干扰问题


<details>
  <summary>Details</summary>
Motivation: 时间切换实验广泛应用于单位级随机化不可行、结果聚合或用户干扰不可避免的场景。实践中需要支持快速产品周期，但时间序列数据存在序列依赖、季节性和厚尾冲击，时间干扰（滞后或预期效应）使标准渐近方法和朴素随机化检验不可靠

Method: 开发随机化检验框架，基于已知分配机制提供有限样本有效的p值。对因果效应施加两个基本条件：非预期性和有限滞后窗口m，构建条件随机化检验（CRT），通过将设计块预先聚合为"区段"来获得可处理的分配律并确保焦点结果的可推算性。提供滞后窗口学习和非预期性评估的诊断方法，引入学生化CRT处理会话内季节性

Result: 在具有AR(1)噪声的分布滞后效应下进行功率近似，指导设计和分析选择。模拟显示相对于常见替代方法具有有利的规模和功率。框架可自然扩展到其他时间索引设计

Conclusion: 提出了一个处理时间切换实验中复杂时间依赖性和干扰问题的随机化检验框架，提供有限样本有效、无分布假设的推断方法，具有实际应用价值

Abstract: Switchback experiments--alternating treatment and control over time--are widely used when unit-level randomization is infeasible, outcomes are aggregated, or user interference is unavoidable. In practice, experimentation must support fast product cycles, so teams often run studies for limited durations and make decisions with modest samples. At the same time, outcomes in these time-indexed settings exhibit serial dependence, seasonality, and occasional heavy-tailed shocks, and temporal interference (carryover or anticipation) can render standard asymptotics and naive randomization tests unreliable. In this paper, we develop a randomization-test framework that delivers finite-sample valid, distribution-free p-values for several null hypotheses of interest using only the known assignment mechanism, without parametric assumptions on the outcome process. For causal effects of interests, we impose two primitive conditions--non-anticipation and a finite carryover horizon m--and construct conditional randomization tests (CRTs) based on an ex ante pooling of design blocks into "sections," which yields a tractable conditional assignment law and ensures imputability of focal outcomes. We provide diagnostics for learning the carryover window and assessing non-anticipation, and we introduce studentized CRTs for a session-wise weak null that accommodates within-session seasonality with asymptotic validity. Power approximations under distributed-lag effects with AR(1) noise guide design and analysis choices, and simulations demonstrate favorable size and power relative to common alternatives. Our framework extends naturally to other time-indexed designs.

</details>


### [23] [Robust model selection using likelihood as data](https://arxiv.org/abs/2602.23355)
*Jongwoo Choi,Neil A. Spencer,Jeffrey W. Miller*

Main category: stat.ME

TL;DR: 提出基于似然值建模的新模型选择方法，在模型误设情况下提供校准推断


<details>
  <summary>Details</summary>
Motivation: 传统模型选择方法（信息准则、贝叶斯后验）在模型误设情况下不稳健，无法量化候选模型与真实数据生成过程之间的近似程度不确定性

Method: 将n×K负对数似然值矩阵视为随机数据矩阵，利用每行期望等于Kullback-Leibler散度向量（加常数）的性质，采用多元正态模型估计并量化该期望的不确定性

Result: 方法计算简单、可解释性强，具有理论保证（包括一致性），能在模型误设情况下提供校准的稳健模型选择推断

Conclusion: 通过直接建模似然值，提出了一种在模型误设情况下量化模型近似不确定性并进行稳健模型选择的新框架

Abstract: Model selection is a central task in statistics, but standard methods are not robust in misspecified settings where the true data-generating process (DGP) is not in the set of candidate models. The key limitation is that existing methods -- including information criteria and Bayesian posteriors -- do not quantify uncertainty about how well each candidate model approximates the true DGP. In this paper, we introduce a novel approach to model selection based on modeling the likelihood values themselves. Specifically, given $K$ candidate models and $n$ observations, we view the $n\times K$ matrix of negative log-likelihood values as a random data matrix and observe that the expectation of each row is equal to the vector of Kullback--Leibler divergences between the $K$ models and the true DGP, up to an additive constant. We use a multivariate normal model to estimate and quantify uncertainty in this expectation, providing calibrated inferences for robust model selection under misspecification. The procedure is easy to compute, interpretable, and comes with theoretical guarantees, including consistency.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [24] [Multiproposal Elliptical Slice Sampling](https://arxiv.org/abs/2602.22358)
*Guillermina Senn,Nathan Glatt-Holtz,Giulia Carigi,Andrew Holbrook,Håkon Tjelmeland*

Main category: stat.CO

TL;DR: 提出多提案椭圆切片采样方法，一种适用于高斯先验贝叶斯推断的自调谐多提案MCMC方法，通过并行采样多个候选提案和基于距离的接受矩阵实现更快的混合


<details>
  <summary>Details</summary>
Motivation: 传统椭圆切片采样方法在状态空间中的移动有限，混合速度较慢，特别是在计算昂贵的似然函数场景下，需要一种能够实现更大状态空间移动和更快自调谐的方法

Method: 扩展椭圆切片采样算法：1) 在每一步并行采样多个候选提案；2) 使用基于距离信息的转移矩阵进行接受决策，可以优先选择远离当前状态的提案；3) 保持计算效率，对昂贵似然函数基本不增加计算时间

Result: 方法实现了状态空间中更大的移动和更快的自调谐，改善了混合性能；理论分析和实验结果都表明该方法具有维度鲁棒的混合行为，特别适合贝叶斯PDE反问题

Conclusion: 多提案椭圆切片采样是一种有效的自调谐MCMC方法，能够在保持计算效率的同时显著改善混合性能，特别适用于具有高斯先验和昂贵似然函数的贝叶斯推断问题

Abstract: We introduce Multiproposal Elliptical Slice Sampling, a self-tuning multiproposal Markov chain Monte Carlo method for Bayesian inference with Gaussian priors. Our method generalizes the Elliptical Slice Sampling algorithm by 1) allowing multiple candidate proposals to be sampled in parallel at each self-tuning step, and 2) basing the acceptance step on a distance-informed transition matrix that can favor proposals far from the current state. This allows larger moves in state space and faster self-tuning, at essentially no additional wall clock time for expensive likelihoods, and results in improved mixing. We additionally provide theoretical arguments and experimental results suggesting dimension-robust mixing behavior, making the algorithm particularly well suited for Bayesian PDE inverse problems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [25] [LoBoost: Fast Model-Native Local Conformal Prediction for Gradient-Boosted Trees](https://arxiv.org/abs/2602.22432)
*Vagner Santos,Victor Coscrato,Luben Cabezas,Rafael Izbicki,Thiago Ramos*

Main category: stat.ML

TL;DR: LoBoost：基于梯度提升决策树的局部保形预测方法，利用树结构进行多尺度校准，无需重新训练或额外数据划分


<details>
  <summary>Details</summary>
Motivation: 梯度提升决策树在表格回归中表现优异但缺乏不确定性量化；传统保形预测使用全局残差分位数，在异方差情况下适应性差；现有改进方法需要拟合辅助模型或额外数据划分，成本高且数据效率低

Method: 利用已训练集成模型的叶节点结构定义多尺度校准组：每个输入通过其在树中访问的叶节点序列编码；在第k层分辨率，根据前k棵树中叶节点索引的前缀匹配对点进行分组，并在每个组内校准残差分位数

Result: 实验显示具有竞争力的区间质量，在大多数数据集上改善了测试MSE，并显著提高了校准速度

Conclusion: LoBoost是一种模型原生的局部保形方法，无需重新训练、辅助模型或额外数据划分，在保持预测性能的同时有效量化不确定性

Abstract: Gradient-boosted decision trees are among the strongest off-the-shelf predictors for tabular regression, but point predictions alone do not quantify uncertainty. Conformal prediction provides distribution-free marginal coverage, yet split conformal uses a single global residual quantile and can be poorly adaptive under heteroscedasticity. Methods that improve adaptivity typically fit auxiliary nuisance models or introduce additional data splits/partitions to learn the conformal score, increasing cost and reducing data efficiency. We propose LoBoost, a model-native local conformal method that reuses the fitted ensemble's leaf structure to define multiscale calibration groups. Each input is encoded by its sequence of visited leaves; at resolution level k, we group points by matching prefixes of leaf indices across the first k trees and calibrate residual quantiles within each group. LoBoost requires no retraining, auxiliary models, or extra splitting beyond the standard train/calibration split. Experiments show competitive interval quality, improved test MSE on most datasets, and large calibration speedups.

</details>


### [26] [Flow Matching is Adaptive to Manifold Structures](https://arxiv.org/abs/2602.22486)
*Shivam Kumar,Yixin Wang,Lizhen Lin*

Main category: stat.ML

TL;DR: 本文理论分析了流匹配在线性插值下，当目标分布支撑在光滑流形时的收敛性，证明了学习到的速度场的非渐近收敛保证，并获得了流匹配目标诱导的隐式密度估计器的统计一致性。


<details>
  <summary>Details</summary>
Motivation: 流匹配已成为扩散生成建模的无模拟替代方案，在文本到图像合成、视频生成和分子结构生成等高维流形支撑数据场景中表现出色。然而，现有理论分析假设目标分布具有光滑的全维密度，未能解释其在流形支撑设置中的有效性。

Method: 理论分析流匹配在线性插值下，当目标分布支撑在光滑流形时的性能。建立学习速度场的非渐近收敛保证，并通过ODE传播估计误差以获得流匹配目标诱导的隐式密度估计器的统计一致性。

Result: 获得了接近极小极大最优的收敛速率，该速率仅依赖于内在维度，并反映了流形和目标分布的光滑性。结果表明流匹配能够适应内在数据几何结构并规避维度诅咒。

Conclusion: 本文为流匹配如何适应内在数据几何结构并规避维度诅咒提供了理论解释，填补了流形支撑设置下流匹配理论分析的空白。

Abstract: Flow matching has emerged as a simulation-free alternative to diffusion-based generative modeling, producing samples by solving an ODE whose time-dependent velocity field is learned along an interpolation between a simple source distribution (e.g., a standard normal) and a target data distribution. Flow-based methods often exhibit greater training stability and have achieved strong empirical performance in high-dimensional settings where data concentrate near a low-dimensional manifold, such as text-to-image synthesis, video generation, and molecular structure generation. Despite this success, existing theoretical analyses of flow matching assume target distributions with smooth, full-dimensional densities, leaving its effectiveness in manifold-supported settings largely unexplained. To this end, we theoretically analyze flow matching with linear interpolation when the target distribution is supported on a smooth manifold. We establish a non-asymptotic convergence guarantee for the learned velocity field, and then propagate this estimation error through the ODE to obtain statistical consistency of the implicit density estimator induced by the flow-matching objective. The resulting convergence rate is near minimax-optimal, depends only on the intrinsic dimension, and reflects the smoothness of both the manifold and the target distribution. Together, these results provide a principled explanation for how flow matching adapts to intrinsic data geometry and circumvents the curse of dimensionality.

</details>


### [27] [From Shallow Bayesian Neural Networks to Gaussian Processes: General Convergence, Identifiability and Scalable Inference](https://arxiv.org/abs/2602.22492)
*Gracielle Antunes de Araújo,Flávio B. Gonçalves*

Main category: stat.ML

TL;DR: 该论文研究了浅层贝叶斯神经网络到高斯过程的缩放极限，提出了新的混合协方差函数，并开发了可扩展的MAP推断方法。


<details>
  <summary>Details</summary>
Motivation: 研究贝叶斯神经网络到高斯过程的缩放极限，旨在建立更严格的统计理论基础，解决模型可识别性问题，并开发可扩展的推断方法，以在实际应用中实现稳定的超参数估计和竞争性的预测性能。

Method: 首先建立了BNN到GP的一般收敛结果，放松了先前公式中的假设；提出了由四种常用激活函数诱导的凸混合协方差函数；开发了使用Nyström近似的可扩展MAP训练和预测方法，通过Nyström秩和锚点选择控制成本-精度权衡。

Result: 在受控模拟和真实世界表格数据集上的实验表明，该方法能够获得稳定的超参数估计，在现实计算成本下实现竞争性的预测性能，验证了理论分析和所提方法的有效性。

Conclusion: 该研究为浅层贝叶斯神经网络提供了更严格的统计理论基础，提出的混合协方差函数和可扩展推断方法在实际应用中表现出色，为BNN-GP连接的理论和应用发展做出了贡献。

Abstract: In this work, we study scaling limits of shallow Bayesian neural networks (BNNs) via their connection to Gaussian processes (GPs), with an emphasis on statistical modeling, identifiability, and scalable inference. We first establish a general convergence result from BNNs to GPs by relaxing assumptions used in prior formulations, and we compare alternative parameterizations of the limiting GP model. Building on this theory, we propose a new covariance function defined as a convex mixture of components induced by four widely used activation functions, and we characterize key properties including positive definiteness and both strict and practical identifiability under different input designs. For computation, we develop a scalable maximum a posterior (MAP) training and prediction procedure using a Nyström approximation, and we show how the Nyström rank and anchor selection control the cost-accuracy trade-off. Experiments on controlled simulations and real-world tabular datasets demonstrate stable hyperparameter estimates and competitive predictive performance at realistic computational cost.

</details>


### [28] [Unsupervised Continual Learning for Amortized Bayesian Inference](https://arxiv.org/abs/2602.22884)
*Aayush Mishra,Šimon Kucharský,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 提出一种用于摊销贝叶斯推理的持续学习框架，通过解耦模拟预训练和无监督顺序自洽微调，结合回放缓冲和弹性权重巩固来缓解灾难性遗忘，在模型错误设定下提升后验估计性能。


<details>
  <summary>Details</summary>
Motivation: 摊销贝叶斯推理在模型错误设定下性能下降，现有自洽训练方法局限于静态单任务设置，无法处理顺序到达的数据或分布漂移，需要一种能持续适应真实世界数据的鲁棒框架。

Method: 提出持续学习框架，将模拟预训练与无监督顺序自洽微调解耦。采用两种适应策略：1）基于回放缓冲的自洽训练，利用过去观测的记忆缓冲区；2）基于弹性权重巩固的自洽训练，通过正则化更新来保护任务关键参数。

Result: 在三个不同案例研究中，该方法显著缓解了灾难性遗忘，产生的后验估计优于标准模拟训练，更接近MCMC参考结果，为跨任务可信赖的摊销贝叶斯推理提供了可行路径。

Conclusion: 提出的持续学习框架通过解耦模拟预训练和顺序自洽微调，结合回放和正则化策略，有效解决了摊销贝叶斯推理在模型错误设定和顺序数据下的鲁棒性问题，实现了跨任务的可靠后验估计。

Abstract: Amortized Bayesian Inference (ABI) enables efficient posterior estimation using generative neural networks trained on simulated data, but often suffers from performance degradation under model misspecification. While self-consistency (SC) training on unlabeled empirical data can enhance network robustness, current approaches are limited to static, single-task settings and fail to handle sequentially arriving data or distribution shifts. We propose a continual learning framework for ABI that decouples simulation-based pre-training from unsupervised sequential SC fine-tuning on real-world data. To address the challenge of catastrophic forgetting, we introduce two adaptation strategies: (1) SC with episodic replay, utilizing a memory buffer of past observations, and (2) SC with elastic weight consolidation, which regularizes updates to preserve task-critical parameters. Across three diverse case studies, our methods significantly mitigate forgetting and yield posterior estimates that outperform standard simulation-based training, achieving estimates closer to MCMC reference, providing a viable path for trustworthy ABI across a range of different tasks.

</details>


### [29] [Beyond NNGP: Large Deviations and Feature Learning in Bayesian Neural Networks](https://arxiv.org/abs/2602.22925)
*Katerina Papagiannouli,Dario Trevisan,Giuseppe Pio Zitto*

Main category: stat.ML

TL;DR: 研究宽贝叶斯神经网络中控制后验集中的罕见但统计主导的波动，超越高斯过程极限，通过大偏差理论在函数层面提供复杂性概念和特征学习。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯过程极限（如NNGP）无法充分描述宽贝叶斯神经网络的后验行为，特别是那些罕见但统计主导的波动，这些波动对后验集中有重要影响。需要超越固定核的理论框架来理解特征学习和复杂性。

Method: 采用大偏差理论，在预测器层面构建变分目标（率函数），通过联合优化预测器和内部核来获得后验输出率函数，与固定核（NNGP）理论形成对比。

Result: 数值实验表明，该方法能准确描述中等规模网络的有限宽度行为，捕捉非高斯尾部、后验变形和数据依赖的核选择效应。

Conclusion: 大偏差理论为宽贝叶斯神经网络提供了超越高斯过程极限的分析框架，在函数层面直接刻画特征学习和复杂性，能更准确地描述实际网络行为。

Abstract: We study wide Bayesian neural networks focusing on the rare but statistically dominant fluctuations that govern posterior concentration, beyond Gaussian-process limits. Large-deviation theory provides explicit variational objectives-rate functions-on predictors, providing an emerging notion of complexity and feature learning directly at the functional level. We show that the posterior output rate function is obtained by a joint optimization over predictors and internal kernels, in contrast with fixed-kernel (NNGP) theory. Numerical experiments demonstrate that the resulting predictions accurately describe finite-width behavior for moderately sized networks, capturing non-Gaussian tails, posterior deformation, and data-dependent kernel selection effects.

</details>


### [30] [Kernel Integrated $R^2$: A Measure of Dependence](https://arxiv.org/abs/2602.22985)
*Pouya Roudaki,Shakeel Gavioli-Akilagun,Florian Kalinke,Mona Azadkia,Zoltán Szabó*

Main category: stat.ML

TL;DR: 提出核积分R²，一种新的统计依赖性度量，结合了积分R²的局部归一化原理和再生核希尔伯特空间的灵活性，可处理多元、函数和结构化数据。


<details>
  <summary>Details</summary>
Motivation: 现有依赖性度量方法在处理多元、函数和结构化数据时存在局限性，需要一种既能保持对尾部行为和振荡依赖结构敏感性，又能扩展到更一般响应空间的度量方法。

Method: 将积分R²扩展到配备特征核的一般空间，提出两种估计器：基于K近邻的图方法和基于条件均值嵌入的RKHS方法。

Result: 新度量取值在[0,1]区间，零当且仅当独立，一当且仅当响应是协变量的可测函数。证明了图基估计器的一致性和收敛速率，数值实验显示在非线性和结构化关系场景中优于现有方法。

Conclusion: 核积分R²是一个灵活且强大的依赖性度量工具，特别适用于复杂数据结构和非线性关系，在统计依赖性测试中具有竞争优势。

Abstract: We introduce kernel integrated $R^2$, a new measure of statistical dependence that combines the local normalization principle of the recently introduced integrated $R^2$ with the flexibility of reproducing kernel Hilbert spaces (RKHSs). The proposed measure extends integrated $R^2$ from scalar responses to responses taking values on general spaces equipped with a characteristic kernel, allowing to measure dependence of multivariate, functional, and structured data, while remaining sensitive to tail behaviour and oscillatory dependence structures. We establish that (i) this new measure takes values in $[0,1]$, (ii) equals zero if and only if independence holds, and (iii) equals one if and only if the response is almost surely a measurable function of the covariates. Two estimators are proposed: a graph-based method using $K$-nearest neighbours and an RKHS-based method built on conditional mean embeddings. We prove consistency and derive convergence rates for the graph-based estimator, showing its adaptation to intrinsic dimensionality. Numerical experiments on simulated data and a real data experiment in the context of dependency testing for media annotations demonstrate competitive power against state-of-the-art dependence measures, particularly in settings involving non-linear and structured relationships.

</details>


### [31] [Regular Fourier Features for Nonstationary Gaussian Processes](https://arxiv.org/abs/2602.23006)
*Arsalan Jawaid,Abdullah Karatas,Jörg Seewig*

Main category: stat.ML

TL;DR: 提出正则傅里叶特征方法用于可调和过程，通过直接离散化谱表示来避免传统谱方法对概率分布假设的限制，提供高效的低秩正半定近似。


<details>
  <summary>Details</summary>
Motivation: 传统谱方法将谱密度视为概率分布进行蒙特卡洛近似，这仅适用于平稳过程。对于非平稳过程，谱密度通常不是概率测度，因此需要一种更通用的方法。

Method: 提出正则傅里叶特征方法，直接离散化可调和过程的谱表示，保持谱权重间的相关结构，无需概率假设。在有限谱支撑假设下，构建高效的低秩正半定近似。

Result: 该方法能够处理局部平稳核和具有复值谱密度的可调和混合核，当谱密度未知时，可自然扩展到从数据中学习核函数。

Conclusion: 正则傅里叶特征方法克服了传统谱方法对概率分布假设的限制，为可调和过程提供了一种更通用的高效近似框架，特别适用于非平稳情况。

Abstract: Simulating a Gaussian process requires sampling from a high-dimensional Gaussian distribution, which scales cubically with the number of sample locations. Spectral methods address this challenge by exploiting the Fourier representation, treating the spectral density as a probability distribution for Monte Carlo approximation. Although this probabilistic interpretation works for stationary processes, it is overly restrictive for the nonstationary case, where spectral densities are generally not probability measures. We propose regular Fourier features for harmonizable processes that avoid this limitation. Our method discretizes the spectral representation directly, preserving the correlation structure among spectral weights without requiring probability assumptions. Under a finite spectral support assumption, this yields an efficient low-rank approximation that is positive semi-definite by construction. When the spectral density is unknown, the framework extends naturally to kernel learning from data. We demonstrate the method on locally stationary kernels and on harmonizable mixture kernels with complex-valued spectral densities.

</details>
