<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 12]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ML](#stat.ML) [Total: 18]
- [stat.AP](#stat.AP) [Total: 2]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Geographically Weighted Canonical Correlation Analysis: Local Spatial Associations Between Two Sets of Variables](https://arxiv.org/abs/2602.10241)
*Zhenzhi Jiao,Angela Yao,Ran Tao,Jean-Claude Thill*

Main category: stat.ME

TL;DR: 提出地理加权典型相关分析(GWCCA)作为研究两组变量间局部空间关联的新方法，通过空间距离加权实现局部化分析。


<details>
  <summary>Details</summary>
Motivation: 经典典型相关分析(CCA)在研究空间关联方面存在局限性，无法捕捉地理空间上的局部变化。需要一种能够分析两组变量间局部空间关联的方法，以更好地理解地理空间中的复杂多元关系。

Method: 提出地理加权典型相关分析(GWCCA)，通过根据观测点与目标位置的空间距离进行加权，将标准CCA局部化，从而估计位置特定的典型相关系数。

Result: 使用合成数据验证了GWCCA在恢复空间结构和捕捉空间效应方面的有效性。美国县级健康结果与社会决定因素的案例研究进一步证明了该方法的实证能力。

Conclusion: GWCCA在理解局部多元空间关联至关重要的领域具有广泛应用潜力，如城市规划、环境科学、公共卫生和交通等空间数据密集型领域。

Abstract: This article critically assesses the utility of the classical statistical technique of Canonical Correlation Analysis (CCA) for studying spatial associations and proposes a new approach to enhance it. Unlike bivariate correlation analysis, which focuses on the relationship between two individual variables, CCA investigates associations between two sets of variables by identifying pairs of linear combinations that are maximally correlated. CCA has strong potential for uncovering complex multivariate relationships that vary across geographic space. We propose Geographically Weighted Canonical Correlation Analysis (GWCCA) as a new technique for exploring local spatial associations between two sets of variables. GWCCA localizes standard CCA by weighting each observation according to its spatial distance from a target location, thereby estimating location-specific canonical correlations. The effectiveness of GWCCA in recovering spatial structure and capturing spatial effects is evaluated using synthetic data. A case study of US county-level health outcomes and social determinants of health further demonstrates the empirical capabilities of the proposed method. The results indicate that GWCCA has broad potential applications in spatial data-intensive fields such as urban planning, environmental science, public health, and transportation, where understanding local multivariate spatial associations is critical.

</details>


### [2] [Generalized Prediction-Powered Inference, with Application to Binary Classifier Evaluation](https://arxiv.org/abs/2602.10332)
*Runjia Zou,Daniela Witten,Brian Williamson*

Main category: stat.ME

TL;DR: 本文扩展了预测驱动推断（PPI）方法，将其推广到任何正则渐近线性估计量，并揭示了PPI在半参数效率理论中的位置，提出了处理协变量分布偏移的改进PPI估计量。


<details>
  <summary>Details</summary>
Motivation: 现有PPI方法仅适用于M-估计量，且在半参数效率方面存在局限。本文旨在将PPI推广到更广泛的估计量类别，并探索其在缺失数据和半参数效率理论框架下的性质，同时解决协变量分布偏移问题。

Method: 将PPI置于缺失数据和半参数效率理论框架中，分析其效率界限；提出改进的PPI估计量来处理三种协变量分布偏移；通过数值研究构建真阳性率、假阳性率和AUC的PPI估计量。

Result: PPI可以推广到任何正则渐近线性估计量；在非限制性场景下，PPI无法达到半参数效率下界，但可作为计算简单的替代方案；提出的改进PPI估计量能有效处理协变量分布偏移。

Conclusion: PPI是半参数效率理论中现有方法的计算简化替代方案，通过推广到更广泛的估计量类别和解决协变量分布偏移问题，扩展了其应用范围，为实际应用提供了更灵活的推断工具。

Abstract: In the partially-observed outcome setting, a recent set of proposals known as "prediction-powered inference" (PPI) involve (i) applying a pre-trained machine learning model to predict the response, and then (ii) using these predictions to obtain an estimator of the parameter of interest with asymptotic variance no greater than that which would be obtained using only the labeled observations. While existing PPI proposals consider estimators arising from M-estimation, in this paper we generalize PPI to any regular asymptotically linear estimator. Furthermore, by situating PPI within the context of an existing rich literature on missing data and semi-parametric efficiency theory, we show that while PPI does not achieve the semi-parametric efficiency lower bound outside of very restrictive and unrealistic scenarios, it can be viewed as a computationally-simple alternative to proposals in that literature. We exploit connections to that literature to propose modified PPI estimators that can handle three distinct forms of covariate distribution shift. Finally, we illustrate these developments by constructing PPI estimators of true positive rate, false positive rate, and area under the curve via numerical studies.

</details>


### [3] [Optimizing precision in stepped-wedge designs via machine learning and quadratic inference functions](https://arxiv.org/abs/2602.10348)
*Liangbo Lyu,Bingkai Wang*

Main category: stat.ME

TL;DR: 提出一种用于阶梯楔形设计的因果效应估计新方法，结合机器学习协变量调整和二次推断函数优化精度


<details>
  <summary>Details</summary>
Motivation: 阶梯楔形设计在随机实验中应用日益广泛，但现有分析方法主要依赖参数模型和预设相关结构，限制了实际可达到的精度

Method: 提出新估计器类，通过基于机器学习的灵活协变量调整捕获复杂结果-协变量关系，结合二次推断函数自适应学习相关结构

Result: 在温和条件下建立一致性和渐近正态性，即使模型误设也成立；证明估计器相对于独立工作相关结构不会降低效率；能处理暴露时间和日历时间的处理效应异质性

Conclusion: 通过模拟研究和两个实证研究的重新分析验证了方法的有效性，为阶梯楔形设计提供了更灵活、更精确的因果效应估计框架

Abstract: Stepped-wedge designs are increasingly used in randomized experiments to accommodate logistical and ethical constraints by staggering treatment roll-out over time. Despite their popularity, existing analytical methods largely rely on parametric models with linear covariate adjustment and prespecified correlation structures, which may limit achievable precision in practice. We propose a new class of estimators for the causal average treatment effect in stepped-wedge designs that optimizes precision through flexible, machine-learning-based covariate adjustment to capture complex outcome-covariate relationships, together with quadratic inference functions to adaptively learn the correlation structure. We establish consistency and asymptotic normality under mild conditions requiring only $L_2$ convergence of nuisance estimators, even under model misspecification, and characterize when the estimator attains the minimal asymptotic variance. Moreover, we prove that the proposed estimator never reduces efficiency relative to an independence working correlation. The proposed method further accommodates treatment-effect heterogeneity across both exposure duration and calendar time. Finally, we demonstrate our methods through simulation studies and reanalyses of two empirical studies that differ substantially in research area and key design parameters.

</details>


### [4] [Prior Smoothing for Multivariate Disease Mapping Models](https://arxiv.org/abs/2602.10955)
*Garazi Retegui,María Dolores Ugarte,Jaione Etxeberria,Alan E. Gelfand*

Main category: stat.ME

TL;DR: 该论文研究了多元疾病映射中的平滑效应，比较了三种不同多元先验的平滑特性，提出了理论和实证指标来评估先验对疾病风险估计的影响。


<details>
  <summary>Details</summary>
Motivation: 现有多元疾病映射文献主要关注特定先验下的平滑效果与经验风险的比较，但缺乏对不同多元先验平滑特性的系统研究。本文旨在扩展单变量区域数据模型的平滑研究到多元情境。

Method: 采用分层建模框架：第一阶段使用泊松模型处理疾病计数，第二阶段用空间随机效应建模率函数。研究三种不同的多元先验，通过理论和实证指标评估其平滑特性，包括先验内平滑和先验间平滑。

Result: 通过模拟和真实数据验证，展示了不同多元先验在疾病风险估计中的平滑效果差异，揭示了这些先验与完美拟合之间的预期偏离特性。

Conclusion: 该研究为多元疾病映射中的先验选择提供了重要指导，帮助用户理解不同先验对平滑效果的影响，从而更好地解释模型结果与观测数据之间的差异。

Abstract: To date, we have seen the emergence of a large literature on multivariate disease mapping. That is, incidence of (or mortality from) multiple diseases is recorded at the scale of areal units where incidence (mortality) across the diseases is expected to manifest dependence. The modeling involves a hierarchical structure: a Poisson model for disease counts (conditioning on the rates) at the first stage, and a specification of a function of the rates using spatial random effects at the second stage. These random effects are specified as a prior and introduce spatial smoothing to the rate (or risk) estimates. What we see in the literature is the amount of smoothing induced under a given prior across areal units compared with the observed/empirical risks. Our contribution here extends previous research on smoothing in univariate areal data models. Specifically, for three different choices of multivariate prior, we investigate both within prior smoothing according to hyperparameters and across prior smoothing. Its benefit to the user is to illuminate the expected nature of departure from perfect fit associated with these priors since model performance is not a question of goodness of fit. We propose both theoretical and empirical metrics for our investigation and illustrate with both simulated and real data.

</details>


### [5] [CoVaR under Asymptotic Independence](https://arxiv.org/abs/2602.10484)
*Zhaowen Wang,Yutao Liu,Deyuan Li*

Main category: stat.ME

TL;DR: 本文提出了一种半参数方法来估计渐近独立对的CoVaR，结合参数极值建模解决联合尾部数据稀疏问题，证明了估计量的相合性和渐近正态性。


<details>
  <summary>Details</summary>
Motivation: CoVaR是系统性风险的重要度量指标，定义为相关变量极端条件下的高分位数。然而，在渐近独立对的联合尾部区域，数据稀疏问题使得CoVaR估计具有挑战性，需要开发新的统计方法来解决这一问题。

Method: 提出半参数方法估计渐近独立对的CoVaR，在二元极值理论框架下，使用参数建模来描述二元极值结构，以解决联合尾部区域的数据稀疏问题。

Result: 证明了所提估计量的相合性和渐近正态性，通过模拟研究展示了估计量的稳健性能，应用于美国股票收益率数据产生了有洞察力的动态CoVaR预测。

Conclusion: 该方法为渐近独立对的CoVaR估计提供了有效的半参数解决方案，能够处理联合尾部数据稀疏问题，在系统性风险管理和金融风险预测中具有实际应用价值。

Abstract: Conditional value-at-risk (CoVaR) is one of the most important measures of systemic risk. It is defined as the high quantile conditional on a related variable being extreme, widely used in the field of quantitative risk management. In this work, we develop a semi-parametric methodology to estimate CoVaR for asymptotically independent pairs within the framework of bivariate extreme value theory. We use parametric modelling of the bivariate extremal structure to address data sparsity in the joint tail regions and prove consistency and asymptotic normality of the proposed estimator. The robust performance of the estimator is illustrated via simulation studies. Its application to the US stock returns data produces insightful dynamic CoVaR forecasts.

</details>


### [6] [Inferring the presence and abundance of rare waterbirds species from scarce data](https://arxiv.org/abs/2602.10673)
*Barbara Bricout,Laura Dami,Pierre Defos du Rau,Sophie Donnet,Thomas Galewski,Stephane Robin*

Main category: stat.ME

TL;DR: 本文提出了一种基于对数正态泊松模型的生态数据缺失值填补方法，特别针对具有高零值比例、高方差和大量缺失数据的物种监测数据。


<details>
  <summary>Details</summary>
Motivation: 生态学中的物种丰度数据通常具有大量缺失值、高方差和高零值比例的特点，特别是对于稀有物种监测。现有方法难以有效处理这些复杂特征，需要开发能够同时进行缺失值填补和协变量效应估计的模型。

Method: 基于对数正态泊松模型，增加了处理零值过表示的潜变量。通过假设潜变量方差矩阵具有低秩结构并纳入协变量来实现缺失值填补。使用变分期望最大化算法进行参数推断，因为最大似然推断不可行。

Result: 证明了模型在缺失数据情况下的可识别性，提供了估计量的渐近方差估计，并推导了填补值的预测区间、时间趋势估计以及趋势变化检测程序。通过人工降质监测数据集验证了填补效果和预测区间。

Conclusion: 该方法能够有效处理生态监测数据中的复杂特征，为物种监测和保护提供了实用的统计工具，并通过水鸟监测数据集的实际应用展示了其有效性。

Abstract: Abundance data are used in ecology for species monitoring and conservation. These count data often display several specific characteristics like numerous missing data, high variance, and a high proportion of zeros, particularly when monitoring rare species. We present a model that aims to impute missing data and estimate the effect of covariates on species presence and abundance. It is based on the log-normal Poisson model, which offers more flexibility in the variance of counts than a Poisson model. A latent variable is added for the overrepresentation of zeros in the data. The imputation of missing data is made possible by assuming that the latent variance matrix has low rank and the inclusion of covariates. \\ We demonstrate the identifiability in the presence of missing data. Since maximum likelihood inference is intractable, we use a variational expectation-maximization algorithm to infer the parameters. We provide an estimate of the asymptotic variance of the estimators and derive prediction intervals for the imputations, an estimate of the temporal trend, and a procedure for detecting a potential change in this trend. \\ We evaluate our imputations and associated prediction intervals using artificially degraded monitoring data set. We conclude with an illustration on a monitoring waterbirds data set.

</details>


### [7] [A closed form solution for Bayesian analysis of a simple linear mixed model](https://arxiv.org/abs/2602.10730)
*Hilde Vinje,Lars Erik Gangsei*

Main category: stat.ME

TL;DR: 本文提出使用四参数广义Beta分布作为线性混合模型的共轭先验，为平衡设计提供闭式贝叶斯解，性能与频率派方法相当且均方误差略低。


<details>
  <summary>Details</summary>
Motivation: 线性混合模型是分析层次和纵向数据的核心工具，但传统基于似然的近似方法在小样本下可能不可靠。本文旨在开发一种闭式贝叶斯解，避免数值积分和模拟的复杂性。

Method: 使用四参数广义Beta密度作为简单线性混合模型的共轭先验分布，为平衡混合模型设计推导出闭式贝叶斯解。该方法超越了标准的近似或基于模拟的贝叶斯方法。

Result: 贝叶斯方法与基于似然估计的标准频率派方法表现相当，同时产生略低的均方误差。研究还讨论了使用经验贝叶斯策略进行超参数设定。

Conclusion: 虽然推导限于平衡设计，但该框架为更复杂混合模型结构的解析可处理贝叶斯推断提供了途径，并概述了将方法扩展到非平衡情况的潜在方向。

Abstract: Linear mixed-effects models are a central analytical tool for modeling hierarchical and longitudinal data, as they allow simultaneous representation of fixed and random sources of variation. In practice, inference for such models is most often based on likelihood-based approximations, which are computationally efficient, but rely on numerical integration and may be unreliable example wise in small-sample settings. In this study, the somewhat obscure four-parameter generalized beta density is shown to be usable as a conjugate prior distribution for a simple linear mixed model. This leads to a closed-form Bayesian solution for a balanced mixed-model design, representing a methodological development beyond standard approximate or simulation-based Bayesian approaches. Although the derivation is restricted to a balanced setting, the proposed framework suggests a pathway toward analytically tractable Bayesian inference for more complex mixed-model structures. The method is evaluated through comparison with a standard frequentist solution based on likelihood estimation for linear mixed-effects models. Results indicate that the Bayesian approach performs just as well as the frequentist alternative, while yielding slightly reduced mean squared error. The study further discusses the use of empirical Bayes strategies for hyperparameter specification and outlines potential directions for extending the approach beyond the balanced case.

</details>


### [8] [Constrained Fiducial Inference for Gaussian Models](https://arxiv.org/abs/2602.11080)
*Hank Flury,Jan Hannig,Richard Smith*

Main category: stat.ME

TL;DR: 提出一种新的基准MCMC方法用于拟合参数化高斯模型，通过Cayley变换分解协方差矩阵，无需先验即可获得后验类基准分布，适用于时间序列和空间数据。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法需要先验分布，而基准推断方法可以在没有先验的情况下获得类似后验的分布。现有方法在处理参数化高斯模型时存在局限性，特别是对于时间序列和空间数据等非独立同分布情况。

Method: 使用Cayley变换分解参数化协方差矩阵，构建高斯数据生成算法。基于约束广义基准推断建立MCMC算法框架，该算法可轻松适配各种参数化模型，无需假设数据独立同分布。

Result: 提出的MCMC算法能够正确收敛到目标基准分布，在MA(1)和Matérn模型上的模拟实验表现良好，证明了方法的有效性。

Conclusion: 该方法为参数化高斯模型提供了一种无需先验的基准推断框架，适用范围广，易于实现，特别适用于时间序列和空间数据分析。

Abstract: We propose a new fiducial Markov Chain Monte Carlo (MCMC) method for fitting parametric Gaussian models. We utilize the Cayley transform to decompose the parametric covariance matrix, which in turn allows us to formulate a general data generating algorithm for Gaussian data. Leveraging constrained generalized fiducial inference, we are able to create the basis of an MCMC algorithm, which can be specified to parametric models with minimal effort. The appeal of this novel approach is the wide class of models which it permits, ease of implementation and the posterior-like fiducial distribution without the need for a prior. We provide background information for the derivation of the relevant fiducial quantities, and a proof that the proposed MCMC algorithm targets the correct fiducial distribution. We need not assume independence nor identical distribution of the data, which makes the method attractive for application to time series and spatial data. Well-performing simulation results of the MA(1) and Matérn models are presented.

</details>


### [9] [Non-centred Bayesian inference for discrete-valued state-transition models: the Rippler algorithm](https://arxiv.org/abs/2602.10924)
*James Neill,Lloyd A. C. Chapman,Chris Jewell*

Main category: stat.ME

TL;DR: 提出名为Rippler的新型贝叶斯算法，用于拟合个体层面的传染病传播随机状态转移模型，相比现有方法在疾病状态增多时表现更优


<details>
  <summary>Details</summary>
Motivation: 传染病传播的随机状态转移模型可用于推断传播驱动因素，但拟合个体层面数据需要推断个体未观察到的疾病状态，这些状态形成高维且高度相关的状态空间，现有方法面临挑战

Method: 提出Rippler算法，这是一种非中心化的贝叶斯数据增强马尔可夫链蒙特卡洛方法，可应用于任何基于个体的状态转移模型，联合估计模型参数和未观察到的疾病状态

Result: 与个体随机流行病模型的最先进推断方法相比，Rippler算法在模型疾病状态数量增加时表现更好

Conclusion: Rippler算法为个体随机传染病模型提供了一种有效的推断方法，特别是在疾病状态较多的复杂模型中具有优势

Abstract: Stochastic state-transition models of infectious disease transmission can be used to deduce relevant drivers of transmission when fitted to data using statistically principled methods. Fitting this individual-level data requires inference on individuals' unobserved disease statuses over time, which form a high-dimensional and highly correlated state space. We introduce a novel Bayesian (data-augmentation Markov chain Monte Carlo) algorithm for jointly estimating the model parameters and unobserved disease statuses, which we call the Rippler algorithm. This is a non-centred method that can be applied to any individual-based state-transition model. We compare the Rippler algorithm to the state-of-the-art inference methods for individual-based stochastic epidemic models and find that it performs better than these methods as the number of disease states in the model increases.

</details>


### [10] [Weighting-Based Identification and Estimation in Graphical Models of Missing Data](https://arxiv.org/abs/2602.10969)
*Anna Guo,Razieh Nabi*

Main category: stat.ME

TL;DR: 提出了一种用于识别缺失数据图形模型中完整数据分布的构造性算法，通过干预缺失指标来识别分布，并处理选择偏差问题。


<details>
  <summary>Details</summary>
Motivation: 在缺失数据图形模型中，即使倾向得分对可用干预保持不变，对缺失指标的干预序列也可能引发和传播选择偏差，导致识别失败。需要一种能明确跟踪和处理选择偏差的识别方法。

Method: 采用干预主义视角，将缺失指标视为可干预变量。提出基于树的识别算法，显式跟踪选择偏差的创建和传播，判断是否可通过可接受的干预策略避免偏差。开发递归逆概率加权程序，匹配识别算法的干预逻辑。

Result: 算法提供了给定缺失机制下可识别性的诊断和构造性特征。模拟研究和实际数据应用展示了方法的实际性能。开发了R包flexMissing实现所有程序。

Conclusion: 该方法为缺失数据图形模型中的分布识别提供了系统的构造性框架，能有效处理干预引发的选择偏差问题，具有实际应用价值。

Abstract: We propose a constructive algorithm for identifying complete data distributions in graphical models of missing data. The complete data distribution is unrestricted, while the missingness mechanism is assumed to factorize according to a conditional directed acyclic graph. Our approach follows an interventionist perspective in which missingness indicators are treated as variables that can be intervened on. A central challenge in this setting is that sequences of interventions on missingness indicators may induce and propagate selection bias, so that identification can fail even when a propensity score is invariant to available interventions. To address this challenge, we introduce a tree-based identification algorithm that explicitly tracks the creation and propagation of selection bias and determines whether it can be avoided through admissible intervention strategies. The resulting tree provides both a diagnostic and a constructive characterization of identifiability under a given missingness mechanism. Building on these results, we develop recursive inverse probability weighting procedures that mirror the intervention logic of the identification algorithm, yielding valid estimating equations for both the missingness mechanism and functionals of the complete data distribution. Simulation studies and a real-data application illustrate the practical performance of the proposed methods. An accompanying R package, flexMissing, implements all proposed procedures.

</details>


### [11] [Renet: Principled and Efficient Relaxation for the Elastic Net via Dynamic Objective Selection](https://arxiv.org/abs/2602.11107)
*Albert Dorador*

Main category: stat.ME

TL;DR: Renet：将松弛Lasso推广到Elastic Net家族的新方法，通过自适应松弛机制解决标准Elastic Net的收缩偏差问题，在保持计算效率的同时提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 标准Elastic Net虽然结合了ℓ₁正则化的变量选择能力和ℓ₂正则化的稳定性，但存在收缩偏差问题，导致预测精度下降。现有松弛方法使用简单的线性插值，忽略了正则化路径的非线性几何特性，可能违反KKT条件。

Method: 提出Renet框架，通过自适应松弛程序强制符号一致性，动态在凸混合和高效子路径重新拟合之间切换。结合"One-Standard-Error"规则作为稳健的去偏机制，并包含超高维情况下的自动稳定性保护。

Result: 在20个合成和真实数据集上的综合基准测试表明，Renet始终优于标准Elastic Net，并在高维、低信噪比和高多重共线性情况下比Adaptive Elastic Net更稳健。计算性能与最先进的坐标下降实现相当。

Conclusion: Renet为Elastic Net家族提供了一个原则性的松弛推广，解决了收缩偏差问题，同时保持了计算效率，为高维统计建模提供了更强大的工具。

Abstract: We introduce Renet, a principled generalization of the Relaxed Lasso to the Elastic Net family of estimators. While, on the one hand, $\ell_1$-regularization is a standard tool for variable selection in high-dimensional regimes and, on the other hand, the $\ell_2$ penalty provides stability and solution uniqueness through strict convexity, the standard Elastic Net nevertheless suffers from shrinkage bias that frequently yields suboptimal prediction accuracy. We propose to address this limitation through a framework called \textit{relaxation}. Existing relaxation implementations rely on naive linear interpolations of penalized and unpenalized solutions, which ignore the non-linear geometry that characterizes the entire regularization path and risk violating the Karush-Kuhn-Tucker conditions. Renet addresses these limitations by enforcing sign consistency through an adaptive relaxation procedure that dynamically dispatches between convex blending and efficient sub-path refitting. Furthermore, we identify and formalize a unique synergy between relaxation and the ``One-Standard-Error'' rule: relaxation serves as a robust debiasing mechanism, allowing practitioners to leverage the parsimony of the 1-SE rule without the traditional loss in predictive fidelity. Our theoretical framework incorporates automated stability safeguards for ultra-high dimensional regimes and is supported by a comprehensive benchmarking suite across 20 synthetic and real-world datasets, demonstrating that Renet consistently outperforms the standard Elastic Net and provides a more robust alternative to the Adaptive Elastic Net in high-dimensional, low signal-to-noise ratio and high-multicollinearity regimes. By leveraging an adaptive solver backend, Renet delivers these statistical gains while offering a computational profile that remains competitive with state-of-the-art coordinate descent implementations.

</details>


### [12] [A Doubly Robust Machine Learning Approach for Disentangling Treatment Effect Heterogeneity with Functional Outcomes](https://arxiv.org/abs/2602.11118)
*Filippo Salmaso,Lorenzo Testa,Francesca Chiaromonte*

Main category: stat.ME

TL;DR: 提出FOCaL方法，用于估计功能性异质处理效应（F-CATE），解决传统元学习框架仅适用于标量结果、无法处理连续域功能数据的问题。


<details>
  <summary>Details</summary>
Motivation: 因果推断对理解干预效果至关重要，但现有CATE估计方法主要针对标量结果，无法有效处理连续域（如时间、空间）的功能性数据，限制了在科学应用中的实用性。

Method: FOCaL是一种双重稳健的元学习器，整合了先进的功能回归技术进行结果建模和功能性伪结果重构，能够直接且稳健地估计F-CATE。

Result: 通过综合模拟研究证明FOCaL相比现有非稳健功能方法具有更好的性能和鲁棒性，并在多个真实世界功能数据集上展示了实用价值。

Conclusion: FOCaL提升了从复杂数据中推断细微个体化因果效应的能力，为个性化医疗、适应性政策设计和基础科学发现等领域更精确可信的AI系统铺平道路。

Abstract: Causal inference is paramount for understanding the effects of interventions, yet extracting personalized insights from increasingly complex data remains a significant challenge for modern machine learning. This is the case, in particular, when considering functional outcomes observed over a continuous domain (e.g., time, or space). Estimation of heterogeneous treatment effects, known as CATE, has emerged as a crucial tool for personalized decision-making, but existing meta-learning frameworks are largely limited to scalar outcomes, failing to provide satisfying results in scientific applications that leverage the rich, continuous information encoded in functional data. Here, we introduce FOCaL (Functional Outcome Causal Learning), a novel, doubly robust meta-learner specifically engineered to estimate a functional heterogeneous treatment effect (F-CATE). FOCaL integrates advanced functional regression techniques for both outcome modeling and functional pseudo-outcome reconstruction, thereby enabling the direct and robust estimation of F-CATE. We provide a rigorous theoretical derivation of FOCaL, demonstrate its performance and robustness compared to existing non-robust functional methods through comprehensive simulation studies, and illustrate its practical utility on diverse real-world functional datasets. FOCaL advances the capabilities of machine intelligence to infer nuanced, individualized causal effects from complex data, paving the way for more precise and trustworthy AI systems in personalized medicine, adaptive policy design, and fundamental scientific discovery.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [13] [A Non-asymptotic Analysis for Learning and Applying a Preconditioner in MCMC](https://arxiv.org/abs/2602.10714)
*Max Hird,Florian Maire,Jeffrey Negrea*

Main category: stat.CO

TL;DR: 本文分析了在马尔可夫链蒙特卡洛方法中学习预条件器的计算成本，并与未预条件的方法进行比较，为预条件ULA建立了非渐近保证。


<details>
  <summary>Details</summary>
Motivation: 预条件是提高MCMC算法效率的常用方法，但在实践中即使从链中学习预条件器也往往非常有效。需要分析学习预条件器（基于目标协方差或目标势的期望Hessian）与不使用预条件的方法之间的有限时间计算成本差异。

Method: 建立非渐近保证，分析学习预条件器的方案在Wasserstein-2距离收缩条件下的时间成本。将结果应用于未调整朗之万算法(ULA)，并扩展到未调整欠阻尼朗之万算法。通过形式化的近似独立性条件，将现代MCMC理论中的非渐近界限与有效样本量和混合时间的经典启发式方法联系起来。

Result: 为学习预条件器的预条件ULA建立了非渐近保证，能够量化学习预条件器所需的时间成本，并证明这些成本可以通过使用预条件器生成的大量样本来分摊。

Conclusion: 通过形式化的近似独立性框架，成功分析了学习预条件器的MCMC方法的计算效率，为预条件ULA提供了理论保证，并展示了如何分摊学习预条件器的成本，使预条件方法在实际应用中更具优势。

Abstract: Preconditioning is a common method applied to modify Markov chain Monte Carlo algorithms with the goal of making them more efficient. In practice it is often extremely effective, even when the preconditioner is learned from the chain. We analyse and compare the finite-time computational costs of schemes which learn a preconditioner based on the target covariance or the expected Hessian of the target potential with that of a corresponding scheme that does not use preconditioning. We apply our results to the Unadjusted Langevin Algorithm (ULA) for an appropriately regular target, establishing non-asymptotic guarantees for preconditioned ULA which learns its preconditioner. Our results are also applied to the unadjusted underdamped Langevin algorithm in the supplementary material. To do so, we establish non-asymptotic guarantees on the time taken to collect $N$ approximately independent samples from the target for schemes that learn their preconditioners under the assumption that the underlying Markov chain satisfies a contraction condition in the Wasserstein-2 distance. This approximate independence condition, that we formalize, allows us to bridge the non-asymptotic bounds of modern MCMC theory and classical heuristics of effective sample size and mixing time, and is needed to amortise the costs of learning a preconditioner across the many samples it will be used to produce.

</details>


### [14] [Large Scale High-Dimensional Reduced-Rank Linear Discriminant Analysis](https://arxiv.org/abs/2602.11108)
*Jocelyn T. Chi*

Main category: stat.CO

TL;DR: 提出一种快速简单的迭代算法，用于处理大规模数据的经典和高维降秩线性判别分析，无需额外假设或调参，具有理论保证。


<details>
  <summary>Details</summary>
Motivation: 降秩线性判别分析（RRLDA）在特征数大于样本数的高维设置和大规模数据中存在计算困难，现有方法通常需要额外假设或调参参数。

Method: 提出一种快速简单的迭代算法，适用于经典和高维RRLDA，无需额外要求，具有理论保证，并解释了RRLDA-RK如何隐式正则化趋向最小范数解。

Result: 在真实数据上演示了算法并突出了一些结果，表明该方法能有效处理大规模高维数据。

Conclusion: 该算法为大规模数据的降秩线性判别分析提供了一个简单、快速且无需额外调参的解决方案，具有实际应用价值。

Abstract: Reduced-rank linear discriminant analysis (RRLDA) is a foundational method of dimension reduction for classification that has been useful in a wide range of applications. The goal is to identify an optimal subspace to project the observations onto that simultaneously maximizes between-group variation while minimizing within-group differences. The solution is straight forward when the number of observations is greater than the number of features but computational difficulties arise in both the high-dimensional setting, where there are more features than there are observations, and when the data are very large. Many works have proposed solutions for the high-dimensional setting and frequently involve additional assumptions or tuning parameters. We propose a fast and simple iterative algorithm for both classical and high-dimensional RRLDA on large data that is free from these additional requirements and that comes with guarantees. We also explain how RRLDA-RK provides implicit regularization towards the least norm solution without explicitly incorporating penalties. We demonstrate our algorithm on real data and highlight some results.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [15] [When LLMs get significantly worse: A statistical approach to detect model degradations](https://arxiv.org/abs/2602.10144)
*Jonas Kübler,Kailash Budhathoki,Matthäus Kleindessner,Xiong Zhou,Junming Yin,Ashish Khetan,George Karypis*

Main category: stat.ML

TL;DR: 提出基于McNemar检验的统计假设检验框架，用于检测基础模型优化后的性能退化，能区分真实退化与评估噪声，可检测0.3%的准确率下降。


<details>
  <summary>Details</summary>
Motivation: 基础模型优化（如量化）后需要确保模型质量不退化，但即使理论上无损的优化也会因数值误差导致生成结果变化。需要统计工具区分有限样本的准确率偏差是真实退化还是评估噪声。

Method: 基于McNemar检验的假设检验框架，在样本级别对比模型得分而非任务级聚合。提出三种方法将多个基准的准确率估计聚合为单一决策。在LM Evaluation Harness上实现。

Result: 方法能正确标记退化模型，不标记理论上无损的优化。测试显示即使0.3%的经验准确率下降也能可靠归因于真实退化而非噪声。

Conclusion: 提出的统计框架能有效检测模型优化后的性能退化，控制假阳性率，为模型优化提供可靠的评估工具。

Abstract: Minimizing the inference cost and latency of foundation models has become a crucial area of research. Optimization approaches include theoretically lossless methods and others without accuracy guarantees like quantization. In all of these cases it is crucial to ensure that the model quality has not degraded. However, even at temperature zero, model generations are not necessarily robust even to theoretically lossless model optimizations due to numerical errors. We thus require statistical tools to decide whether a finite-sample accuracy deviation is an evidence of a model's degradation or whether it can be attributed to (harmless) noise in the evaluation. We propose a statistically sound hypothesis testing framework based on McNemar's test allowing to efficiently detect model degradations, while guaranteeing a controlled rate of false positives. The crucial insight is that we have to confront the model scores on each sample, rather than aggregated on the task level. Furthermore, we propose three approaches to aggregate accuracy estimates across multiple benchmarks into a single decision. We provide an implementation on top of the largely adopted open source LM Evaluation Harness and provide a case study illustrating that the method correctly flags degraded models, while not flagging model optimizations that are provably lossless. We find that with our tests even empirical accuracy degradations of 0.3% can be confidently attributed to actual degradations rather than noise.

</details>


### [16] [Dissecting Performative Prediction: A Comprehensive Survey](https://arxiv.org/abs/2602.10176)
*Thomas Kehrenberg,Javier Sanguino,Jose A. Lozano,Novi Quadrianto*

Main category: stat.ML

TL;DR: 这篇综述论文回顾了自2020年Perdomo等人提出"表演性预测"概念以来的研究进展，系统梳理了该领域的不同设置、优化目标、分类方法、实现技术和与其他领域的联系。


<details>
  <summary>Details</summary>
Motivation: 表演性预测作为一个新兴的机器学习领域，自2020年提出以来已经积累了丰富的研究成果。作者希望通过这篇综述系统性地梳理该领域的发展脉络，建立统一的分类框架，并揭示与其他领域的潜在联系，以促进未来研究。

Method: 作者首先阐述了表演性预测的基本设置，解释了表演稳定性和表演最优性两种优化目标。然后提出了基于分布图信息量的新分类方法，综述了现有分布图的实现方式以及解决表演性预测问题的方法，并探讨了不同的分类维度。

Result: 论文提供了对表演性预测领域的全面综述，建立了基于分布图信息量的分类框架，系统整理了现有技术方法，并指出了该领域与其他研究领域（如因果推断、博弈论等）的已知和潜在联系。

Conclusion: 表演性预测是一个快速发展的重要研究领域，具有丰富的理论内涵和实际应用价值。通过建立统一的分类框架和揭示跨领域联系，这篇综述为未来研究提供了重要的参考和方向指引。

Abstract: The field of performative prediction had its beginnings in 2020 with the seminal paper "Performative Prediction" by Perdomo et al., which established a novel machine learning setup where the deployment of a predictive model causes a distribution shift in the environment, which in turn causes a mismatch between the distribution expected by the predictive model and the real distribution. This shift is defined by a so-called distribution map. In the half-decade since, a literature has emerged which has, among other things, introduced new solution concepts to the original setup, extended the setup, offered new theoretical analyses, and examined the intersection of performative prediction and other established fields. In this survey, we first lay out the performative prediction setting and explain the different optimization targets: performative stability and performative optimality. We introduce a new way of classifying different performative prediction settings, based on how much information is available about the distribution map. We survey existing implementations of distribution maps and existing methods to address the problem of performative prediction, while examining different ways to categorize them. Finally, we point out known and previously unknown connections that can be drawn to other fields, in the hopes of stimulating future research.

</details>


### [17] [Power-SMC: Low-Latency Sequence-Level Power Sampling for Training-Free LLM Reasoning](https://arxiv.org/abs/2602.10273)
*Seyedarmin Azizi,Erfan Baghaei Potraghloo,Minoo Ahmadi,Souvik Kundu,Massoud Pedram*

Main category: stat.ML

TL;DR: Power-SMC：一种基于序列蒙特卡洛的训练免费推理方法，通过并行粒子推进和重要性权重修正，在保持接近标准解码延迟的同时，实现与Metropolis-Hastings功率采样相当的推理性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于Metropolis-Hastings的功率采样方法虽然能提升大语言模型的推理性能，但会导致推理速度大幅下降（16-28倍延迟）。需要一种既能保持推理性能提升，又不会显著增加延迟的高效方法。

Method: 提出Power-SMC方法：1）使用序列蒙特卡洛框架，并行推进少量粒子集；2）逐token修正重要性权重；3）必要时进行重采样；4）所有操作在单个GPU友好的批处理解码中完成；5）证明温度τ=1/α是唯一最小化增量权重方差的前缀提议；6）引入指数桥接调度改善粒子稳定性。

Result: 在MATH500数据集上，Power-SMC匹配或超过Metropolis-Hastings功率采样的性能，同时将延迟从16-28倍降低到1.4-3.3倍（相对于基线解码）。

Conclusion: Power-SMC提供了一种高效实用的训练免费推理增强方法，在保持接近标准解码延迟的同时，实现了与现有方法相当的推理性能提升，为大语言模型的推理优化提供了新思路。

Abstract: Many recent reasoning gains in large language models can be explained as distribution sharpening: biasing generation toward high-likelihood trajectories already supported by the pretrained model, rather than modifying its weights. A natural formalization is the sequence-level power distribution $π_α(y\mid x)\propto p_θ(y\mid x)^α$ ($α>1$), which concentrates mass on whole sequences instead of adjusting token-level temperature. Prior work shows that Metropolis--Hastings (MH) sampling from this distribution recovers strong reasoning performance, but at order-of-magnitude inference slowdowns. We introduce Power-SMC, a training-free Sequential Monte Carlo scheme that targets the same objective while remaining close to standard decoding latency. Power-SMC advances a small particle set in parallel, corrects importance weights token-by-token, and resamples when necessary, all within a single GPU-friendly batched decode. We prove that temperature $τ=1/α$ is the unique prefix-only proposal minimizing incremental weight variance, interpret residual instability via prefix-conditioned Rényi entropies, and introduce an exponent-bridging schedule that improves particle stability without altering the target. On MATH500, Power-SMC matches or exceeds MH power sampling while reducing latency from $16$--$28\times$ to $1.4$--$3.3\times$ over baseline decoding.

</details>


### [18] [Causal Effect Estimation with Learned Instrument Representations](https://arxiv.org/abs/2602.10370)
*Frances Dean,Jenna Fields,Radhika Bhalerao,Marie Charpignon,Ahmed Alaa*

Main category: stat.ML

TL;DR: ZNet：一种从观测协变量中学习构建工具变量表示的方法，可在没有显式工具变量的情况下进行IV估计


<details>
  <summary>Details</summary>
Motivation: 工具变量方法需要有效的工具变量，但在实践中往往难以识别或不可得。现有方法依赖显式工具变量的可用性，限制了在一般观测数据中的应用。

Method: 提出ZNet模型，通过表示学习方法从观测协变量中构建工具变量表示。模型架构反映工具变量的结构因果模型，将特征空间分解为混杂因素和工具变量成分，通过强制满足工具变量定义属性（相关性、排除限制、工具变量无混杂性）的经验矩条件进行训练。

Result: 实验表明ZNet能够：(1)在环境特征空间中恢复真实存在的工具变量；(2)在没有显式工具变量的情况下在嵌入空间中构建潜在工具变量。模型可作为"即插即用"模块与多种下游两阶段IV估计器兼容。

Conclusion: ZNet为一般观测环境中的因果推断提供了一种灵活解决方案，无论是否满足无混杂性假设，都能进行工具变量估计，扩展了IV方法的应用范围。

Abstract: Instrumental variable (IV) methods mitigate bias from unobserved confounding in observational causal inference but rely on the availability of a valid instrument, which can often be difficult or infeasible to identify in practice. In this paper, we propose a representation learning approach that constructs instrumental representations from observed covariates, which enable IV-based estimation even in the absence of an explicit instrument. Our model (ZNet) achieves this through an architecture that mirrors the structural causal model of IVs; it decomposes the ambient feature space into confounding and instrumental components, and is trained by enforcing empirical moment conditions corresponding to the defining properties of valid instruments (i.e., relevance, exclusion restriction, and instrumental unconfoundedness). Importantly, ZNet is compatible with a wide range of downstream two-stage IV estimators of causal effects. Our experiments demonstrate that ZNet can (i) recover ground-truth instruments when they already exist in the ambient feature space and (ii) construct latent instruments in the embedding space when no explicit IVs are available. This suggests that ZNet can be used as a ``plug-and-play'' module for causal inference in general observational settings, regardless of whether the (untestable) assumption of unconfoundedness is satisfied.

</details>


### [19] [Generalized Robust Adaptive-Bandwidth Multi-View Manifold Learning in High Dimensions with Noise](https://arxiv.org/abs/2602.10530)
*Xiucai Ding,Chao Shen,Hau-Tieng Wu*

Main category: stat.ML

TL;DR: GRAB-MDM是一个新的基于核的多视图扩散几何框架，通过视图相关的自适应带宽选择策略，在存在异构高维噪声的情况下稳健地恢复共享内在结构。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图融合方法在异构高维噪声环境下理论保证有限，需要一种能够适应不同视图几何和噪声水平的稳健融合方法。

Method: 提出GRAB-MDM框架，采用视图相关的自适应带宽选择策略，根据每个视图的几何结构和噪声水平调整带宽，构建稳健的多视图扩散算子。

Result: 理论证明自适应带宽能稳健恢复共享内在结构，数值实验显示GRAB-MDM在鲁棒性和嵌入质量上显著优于固定带宽和等带宽基线方法。

Conclusion: GRAB-MDM为高维噪声环境下的多视图传感器融合提供了实用且有理论基础的解决方案。

Abstract: Multiview datasets are common in scientific and engineering applications, yet existing fusion methods offer limited theoretical guarantees, particularly in the presence of heterogeneous and high-dimensional noise. We propose Generalized Robust Adaptive-Bandwidth Multiview Diffusion Maps (GRAB-MDM), a new kernel-based diffusion geometry framework for integrating multiple noisy data sources. The key innovation of GRAB-MDM is a {view}-dependent bandwidth selection strategy that adapts to the geometry and noise level of each view, enabling a stable and principled construction of multiview diffusion operators. Under a common-manifold model, we establish asymptotic convergence results and show that the adaptive bandwidths lead to provably robust recovery of the shared intrinsic structure, even when noise levels and sensor dimensions differ across views. Numerical experiments demonstrate that GRAB-MDM significantly improves robustness and embedding quality compared with fixed-bandwidth and equal-bandwidth baselines, and usually outperform existing algorithms. The proposed framework offers a practical and theoretically grounded solution for multiview sensor fusion in high-dimensional noisy environments.

</details>


### [20] [From Collapse to Improvement: Statistical Perspectives on the Evolutionary Dynamics of Iterative Training on Contaminated Sources](https://arxiv.org/abs/2602.10531)
*Soham Bakshi,Sunrit Chakraborty*

Main category: stat.ML

TL;DR: 论文从统计角度分析模型崩溃问题，发现在真实数据和合成数据混合训练时，只要有足够真实信息，适当调整混合权重和样本量可以避免崩溃甚至恢复真实分布。


<details>
  <summary>Details</summary>
Motivation: 模型崩溃问题在生成模型的迭代训练中带来了新挑战，使用合成数据进行训练会导致性能整体下降。本文旨在从统计角度分析这一问题，探索在数据被合成样本污染的情况下，只要有真实目标分布的新鲜信息，模型性能实际上可能得到改善的可能性。

Method: 从统计视角分析迭代训练过程，考虑从真实目标和合成分布的混合中采样的数据。分析整个迭代演化过程，特别关注在下一个token预测语言模型中，混合权重和样本大小如何共同控制长期性能。采用污染不可知的方式训练模型，研究混合权重和样本量的相互作用。

Result: 只要有非平凡的（即使随时间衰减的）真实分布混合权重，通过适当的样本大小进行污染不可知训练，可以避免模型崩溃。在某些条件下，甚至可以恢复真实目标分布。模拟研究支持这些发现，并表明这种行为对其他类型的模型也具有普遍性。

Conclusion: 模型崩溃并非不可避免，通过适当控制真实数据和合成数据的混合比例以及样本大小，迭代训练可以避免性能下降，甚至能够恢复真实分布。这为生成模型的可持续迭代训练提供了理论依据和实用指导。

Abstract: The problem of model collapse has presented new challenges in iterative training of generative models, where such training with synthetic data leads to an overall degradation of performance. This paper looks at the problem from a statistical viewpoint, illustrating that one can actually hope for improvement when models are trained on data contaminated with synthetic samples, as long as there is some amount of fresh information from the true target distribution. In particular, we consider iterative training on samples sourced from a mixture of the true target and synthetic distributions. We analyze the entire iterative evolution in a next-token prediction language model, capturing how the interplay between the mixture weights and the sample size controls the overall long-term performance. With non-trivial mixture weight of the true distribution, even if it decays over time, simply training the model in a contamination-agnostic manner with appropriate sample sizes can avoid collapse and even recover the true target distribution under certain conditions. Simulation studies support our findings and also show that such behavior is more general for other classes of models.

</details>


### [21] [Statistical Inference and Learning for Shapley Additive Explanations (SHAP)](https://arxiv.org/abs/2602.10532)
*Justin Whitehouse,Ayush Sawarni,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 该论文提出了一种半参数方法，用于对SHAP值的p次幂进行统计推断，包括构建渐近正态估计量，并针对不同p值范围设计了不同的去偏策略。


<details>
  <summary>Details</summary>
Motivation: SHAP框架已成为预测任务中变量重要性归因的重要工具，但现有的全局重要性度量（如平均绝对SHAP或平均平方SHAP）缺乏统计推断方法，无法量化估计的不确定性。

Method: 采用半参数方法，将SHAP曲线视为需要从数据中估计的冗余函数。对于p≥2的情况，使用结合U统计量和Neyman正交分数的去偏估计量；对于1≤p<2的情况，构建平滑替代的去偏U统计量，并精心调整温度参数以获得对未平滑p次幂的推断。

Result: 能够可靠地构建SHAP值p次幂的渐近正态估计量，并提出了用于通过经验风险最小化学习SHAP曲线的Neyman正交损失函数，讨论了常用函数类的超额风险保证。

Conclusion: 该研究为SHAP值的全局重要性度量提供了统计推断框架，填补了现有方法的空白，提高了模型可解释性和特征选择的可靠性。

Abstract: The SHAP (short for Shapley additive explanation) framework has become an essential tool for attributing importance to variables in predictive tasks. In model-agnostic settings, SHAP uses the concept of Shapley values from cooperative game theory to fairly allocate credit to the features in a vector $X$ based on their contribution to an outcome $Y$. While the explanations offered by SHAP are local by nature, learners often need global measures of feature importance in order to improve model explainability and perform feature selection. The most common approach for converting these local explanations into global ones is to compute either the mean absolute SHAP or mean squared SHAP. However, despite their ubiquity, there do not exist approaches for performing statistical inference on these quantities.
  In this paper, we take a semi-parametric approach for calibrating confidence in estimates of the $p$th powers of Shapley additive explanations. We show that, by treating the SHAP curve as a nuisance function that must be estimated from data, one can reliably construct asymptotically normal estimates of the $p$th powers of SHAP. When $p \geq 2$, we show a de-biased estimator that combines U-statistics with Neyman orthogonal scores for functionals of nested regressions is asymptotically normal. When $1 \leq p < 2$ (and the hence target parameter is not twice differentiable), we construct de-biased U-statistics for a smoothed alternative. In particular, we show how to carefully tune the temperature parameter of the smoothing function in order to obtain inference for the true, unsmoothed $p$th power. We complement these results by presenting a Neyman orthogonal loss that can be used to learn the SHAP curve via empirical risk minimization and discussing excess risk guarantees for commonly used function classes.

</details>


### [22] [A Gibbs posterior sampler for inverse problem based on prior diffusion model](https://arxiv.org/abs/2602.11059)
*Jean-François Giovannelli*

Main category: stat.ML

TL;DR: 提出一种Gibbs算法用于解决线性观测系统加噪声、病态问题贝叶斯正则化、先验基于扩散过程的后验采样难题，该方法简单有效且有收敛保证。


<details>
  <summary>Details</summary>
Motivation: 解决在特定条件下（线性观测系统加噪声、病态问题贝叶斯正则化、先验基于扩散过程）后验采样的困难问题，该问题在现有研究中尚未得到有效解决。

Method: 提出一种Gibbs算法，该方法在给定条件下特别有效且简单，提供了在明确识别情况下的收敛保证。

Result: 数值模拟结果明确证实了该方法的有效性，表明这种先前未被探索的途径特别有效。

Conclusion: Gibbs算法为解决特定条件下的后验采样问题提供了一种简单有效的解决方案，具有收敛保证，并通过数值模拟验证了其有效性。

Abstract: This paper addresses the issue of inversion in cases where (1) the observation system is modeled by a linear transformation and additive noise, (2) the problem is ill-posed and regularization is introduced in a Bayesian framework by an a prior density, and (3) the latter is modeled by a diffusion process adjusted on an available large set of examples. In this context, it is known that the issue of posterior sampling is a thorny one. This paper introduces a Gibbs algorithm. It appears that this avenue has not been explored, and we show that this approach is particularly effective and remarkably simple. In addition, it offers a guarantee of convergence in a clearly identified situation. The results are clearly confirmed by numerical simulations.

</details>


### [23] [Robust Assortment Optimization from Observational Data](https://arxiv.org/abs/2602.10696)
*Miao Lu,Yuxuan Han,Han Zhong,Zhengyuan Zhou,Jose Blanchet*

Main category: stat.ML

TL;DR: 提出一个考虑客户选择行为分布偏移的鲁棒性数据驱动商品组合优化框架，在已知名义模型时计算可处理，在数据驱动场景中设计统计最优算法，并建立样本复杂度的上下界理论保证。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的商品组合优化方法通常假设客户偏好稳定且选择模型正确，但这些假设在现实场景中经常失效（偏好偏移和模型误设），导致泛化能力差和收入损失。需要开发能应对分布偏移的鲁棒框架。

Method: 提出鲁棒框架，从生成数据的名义选择模型出发，建模潜在偏好偏移，最大化最坏情况期望收入。首先在已知名义模型时证明计算可处理性，然后在数据驱动场景中设计统计最优算法，最小化数据需求同时保持鲁棒性。

Result: 理论分析提供了样本复杂度的上界和匹配下界，为鲁棒泛化提供理论保证。特别地，发现了"鲁棒逐项覆盖"概念作为实现样本高效鲁棒商品组合学习的最小数据需求。

Conclusion: 该工作弥合了商品组合学习中鲁棒性和统计效率之间的差距，为不确定性下的可靠商品组合优化提供了新见解和工具。

Abstract: Assortment optimization is a fundamental challenge in modern retail and recommendation systems, where the goal is to select a subset of products that maximizes expected revenue under complex customer choice behaviors. While recent advances in data-driven methods have leveraged historical data to learn and optimize assortments, these approaches typically rely on strong assumptions -- namely, the stability of customer preferences and the correctness of the underlying choice models. However, such assumptions frequently break in real-world scenarios due to preference shifts and model misspecification, leading to poor generalization and revenue loss. Motivated by this limitation, we propose a robust framework for data-driven assortment optimization that accounts for potential distributional shifts in customer choice behavior. Our approach models potential preference shift from a nominal choice model that generates data and seeks to maximize worst-case expected revenue. We first establish the computational tractability of robust assortment planning when the nominal model is known, then advance to the data-driven setting, where we design statistically optimal algorithms that minimize the data requirements while maintaining robustness. Our theoretical analysis provides both upper bounds and matching lower bounds on the sample complexity, offering theoretical guarantees for robust generalization. Notably, we uncover and identify the notion of ``robust item-wise coverage'' as the minimal data requirement to enable sample-efficient robust assortment learning. Our work bridges the gap between robustness and statistical efficiency in assortment learning, contributing new insights and tools for reliable assortment optimization under uncertainty.

</details>


### [24] [Why Agentic Theorem Prover Works: A Statistical Provability Theory of Mathematical Reasoning Models](https://arxiv.org/abs/2602.10538)
*Sho Sonoda,Shunta Akiyama,Yuya Uezato*

Main category: stat.ML

TL;DR: 论文提出统计可证性概念，将定理证明系统建模为时间有界MDP，分析各组件性能并给出理论解释


<details>
  <summary>Details</summary>
Motivation: 尽管智能定理证明系统在实践中取得显著成功，但尚不清楚各组件如何驱动性能，以及为何能在经典证明搜索难题下工作。需要理论框架解释其成功原因和局限性。

Method: 提出统计可证性概念，定义为在实例分布上达到已验证证明的有限时域成功概率。将现代定理证明流程建模为时间有界MDP，利用贝尔曼结构分析最优策略存在性，通过子/超解不等式获得可证性证书，并分析评分引导规划的性能差距。

Result: 证明了在温和正则条件下最优策略的存在性，推导了可证性证书，并量化了评分引导规划（贪婪/top-k/束搜索/回滚）的性能差距，涉及近似误差、序列统计复杂度、表示几何和动作间隙尾部分布。

Conclusion: 该理论为智能定理证明系统在偏置现实问题分布上的成功提供了原理性、组件敏感的解释，同时阐明了在最坏情况或对抗机制下的局限性。

Abstract: Agentic theorem provers -- pipelines that couple a mathematical reasoning model with library retrieval, subgoal-decomposition/search planner, and a proof assistant verifier -- have recently achieved striking empirical success, yet it remains unclear which components drive performance and why such systems work at all despite classical hardness of proof search. We propose a distributional viewpoint and introduce **statistical provability**, defined as the finite-horizon success probability of reaching a verified proof, averaged over an instance distribution, and formalize modern theorem-proving pipelines as time-bounded MDPs. Exploiting Bellman structure, we prove existence of optimal policies under mild regularity, derive provability certificates via sub-/super-solution inequalities, and bound the performance gap of score-guided planning (greedy/top-\(k\)/beam/rollouts) in terms of approximation error, sequential statistical complexity, representation geometry (metric entropy/doubling structure), and action-gap margin tails. Together, our theory provides a principled, component-sensitive explanation of when and why agentic theorem provers succeed on biased real-world problem distributions, while clarifying limitations in worst-case or adversarial regimes.

</details>


### [25] [Deep Bootstrap](https://arxiv.org/abs/2602.10587)
*Jinyuan Chang,Yuling Jiao,Lican Kang,Junjie Shi*

Main category: stat.ML

TL;DR: 提出基于条件扩散模型的深度自助法框架，用于非参数回归，通过生成式方法统一学习条件分布、采样和回归估计


<details>
  <summary>Details</summary>
Motivation: 传统自助法将条件分布估计、采样和非参数回归解耦处理，缺乏统一框架。需要一种能够处理高维或多模态分布的高效方法，同时保证理论可靠性

Method: 构建条件扩散模型学习响应变量给定协变量的分布，生成自助样本（原始协变量+合成响应），将非参数回归重构为条件样本均值估计，通过学习的扩散模型直接实现

Result: 建立了学习条件分布与目标条件分布之间Wasserstein距离的最优端到端收敛率，证明了自助过程的收敛保证，数值研究展示了方法在复杂回归任务中的有效性和可扩展性

Conclusion: 提出的深度自助框架通过扩散模型的表达能力，统一了条件分布学习、采样和回归估计，为复杂回归问题提供了高效且理论可靠的新方法

Abstract: In this work, we propose a novel deep bootstrap framework for nonparametric regression based on conditional diffusion models. Specifically, we construct a conditional diffusion model to learn the distribution of the response variable given the covariates. This model is then used to generate bootstrap samples by pairing the original covariates with newly synthesized responses. We reformulate nonparametric regression as conditional sample mean estimation, which is implemented directly via the learned conditional diffusion model. Unlike traditional bootstrap methods that decouple the estimation of the conditional distribution, sampling, and nonparametric regression, our approach integrates these components into a unified generative framework. With the expressive capacity of diffusion models, our method facilitates both efficient sampling from high-dimensional or multimodal distributions and accurate nonparametric estimation. We establish rigorous theoretical guarantees for the proposed method. In particular, we derive optimal end-to-end convergence rates in the Wasserstein distance between the learned and target conditional distributions. Building on this foundation, we further establish the convergence guarantees of the resulting bootstrap procedure. Numerical studies demonstrate the effectiveness and scalability of our approach for complex regression tasks.

</details>


### [26] [Bayesian Inference of Contextual Bandit Policies via Empirical Likelihood](https://arxiv.org/abs/2602.10608)
*Jiangrong Ouyang,Mingming Gong,Howard Bondell*

Main category: stat.ML

TL;DR: 提出一种基于经验似然的贝叶斯推断方法，用于有限样本下多个上下文赌博机策略的联合分析，提供准确的策略价值评估和不确定性量化


<details>
  <summary>Details</summary>
Motivation: 在上下文赌博机问题中，策略推断至关重要。现有方法在有限样本情况下可能不够稳健，需要一种能够在小样本下提供准确不确定性量化的推断方法

Method: 使用经验似然开发贝叶斯推断方法，对多个上下文赌博机策略进行联合分析。该方法能够灵活进行策略比较，并提供完整的不确定性量化

Result: 提出的推断方法对小样本具有鲁棒性，能够为策略价值评估提供准确的不确定性测量。通过蒙特卡洛模拟和青少年BMI数据集应用验证了方法的有效性

Conclusion: 基于经验似然的贝叶斯推断方法为上下文赌博机策略分析提供了有效的工具，特别适用于有限样本情况下的策略评估和比较

Abstract: Policy inference plays an essential role in the contextual bandit problem. In this paper, we use empirical likelihood to develop a Bayesian inference method for the joint analysis of multiple contextual bandit policies in finite sample regimes. The proposed inference method is robust to small sample sizes and is able to provide accurate uncertainty measurements for policy value evaluation. In addition, it allows for flexible inferences on policy comparison with full uncertainty quantification. We demonstrate the effectiveness of the proposed inference method using Monte Carlo simulations and its application to an adolescent body mass index data set.

</details>


### [27] [Highly Adaptive Principal Component Regression](https://arxiv.org/abs/2602.10613)
*Mingxun Wang,Alejandro Schuler,Mark van der Laan,Carlos García Meixide*

Main category: stat.ML

TL;DR: 提出基于主成分的HAL和HAR方法（PCHAL/PCHAR），通过无监督降维提高计算效率，同时保持原方法的性能优势，并揭示了HAL/HAR Gram算子的谱结构与离散正弦基的傅里叶联系。


<details>
  <summary>Details</summary>
Motivation: HAL方法虽然具有优越的理论性质，但在高维情况下计算代价过高；HAR作为可扩展替代方案仍面临计算挑战。需要开发既能保持理论优势又能提高计算效率的新方法。

Method: 基于主成分分析对HAL和HAR进行改进，提出PCHAL和PCHAR方法。通过Gram算子的主成分进行结果盲降维，利用谱分析揭示其与离散正弦基的傅里叶结构联系。

Result: PCHAL和PCHAR在计算效率上获得显著提升，同时保持了与原始HAL和HAR相当的实证性能。发现了HAL/HAR Gram算子主成分与离散正弦基的谱联系，揭示了傅里叶型结构。

Conclusion: 基于主成分的改进方法在保持非参数回归理论优势的同时，有效解决了高维计算瓶颈问题，为大规模数据分析提供了实用工具，并揭示了底层数学结构的理论洞察。

Abstract: The Highly Adaptive Lasso (HAL) is a nonparametric regression method that achieves almost dimension-free convergence rates under minimal smoothness assumptions, but its implementation can be computationally prohibitive in high dimensions due to the large basis matrix it requires. The Highly Adaptive Ridge (HAR) has been proposed as a scalable alternative. Building on both procedures, we introduce the Principal Component based Highly Adaptive Lasso (PCHAL) and Principal Component based Highly Adaptive Ridge (PCHAR). These estimators constitute an outcome-blind dimension reduction which offer substantial gains in computational efficiency and match the empirical performances of HAL and HAR. We also uncover a striking spectral link between the leading principal components of the HAL/HAR Gram operator and a discrete sinusoidal basis, revealing an explicit Fourier-type structure underlying the PC truncation.

</details>


### [28] [Beyond Kemeny Medians: Consensus Ranking Distributions Definition, Properties and Statistical Learning](https://arxiv.org/abs/2602.10640)
*Stephan Clémençon,Ekhine Irurozki*

Main category: stat.ML

TL;DR: 提出了一种新的排序分布摘要方法——共识排序分布(CRD)，作为稀疏混合模型来近似排序分布，并开发了基于树结构的统计学习算法。


<details>
  <summary>Details</summary>
Motivation: 传统共识和Kemeny中位数方法在总结排序分布方面存在局限，需要开发超越经典理论的新方法，以更好地近似排序分布并减少质量运输视角下的失真。

Method: 基于局部排序中位数概念，提出共识排序分布(CRD)作为Dirac质量的稀疏混合模型；使用Kendall τ距离作为成本函数，将最优失真表达为成对概率函数；开发了自上而下的树结构统计算法，从Kemeny中位数的Dirac质量逐步细化到经验排序数据分布。

Result: 理论证明使用Kendall τ距离时，最优失真可以表示为成对概率的函数；提出的树结构算法能够有效学习CRD；数值实验支持了算法的相关性和有效性。

Conclusion: CRD方法为排序分布摘要提供了新的理论框架，克服了对称群上缺乏向量空间结构的困难，提出的学习算法在实践中表现良好，为排序数据分析提供了有效工具。

Abstract: In this article we develop a new method for summarizing a ranking distribution, \textit{i.e.} a probability distribution on the symmetric group $\mathfrak{S}_n$, beyond the classical theory of consensus and Kemeny medians. Based on the notion of \textit{local ranking median}, we introduce the concept of \textit{consensus ranking distribution} ($\crd$), a sparse mixture model of Dirac masses on $\mathfrak{S}_n$, in order to approximate a ranking distribution with small distortion from a mass transportation perspective. We prove that by choosing the popular Kendall $τ$ distance as the cost function, the optimal distortion can be expressed as a function of pairwise probabilities, paving the way for the development of efficient learning methods that do not suffer from the lack of vector space structure on $\mathfrak{S}_n$. In particular, we propose a top-down tree-structured statistical algorithm that allows for the progressive refinement of a CRD based on ranking data, from the Dirac mass at a Kemeny median at the root of the tree to the empirical ranking data distribution itself at the end of the tree's exhaustive growth. In addition to the theoretical arguments developed, the relevance of the algorithm is empirically supported by various numerical experiments.

</details>


### [29] [A solvable high-dimensional model where nonlinear autoencoders learn structure invisible to PCA while test loss misaligns with generalization](https://arxiv.org/abs/2602.10680)
*Vicente Conde Mendes,Lorenzo Bardone,Cédric Koller,Jorge Medina Moreira,Vittorio Erba,Emanuele Troiani,Lenka Zdeborová*

Main category: stat.ML

TL;DR: 论文提出一个高维尖峰模型，包含两个潜在因子：一个可通过协方差检测，另一个仅出现在高阶矩中。非线性自编码器能提取两者，而线性方法只能提取前者。


<details>
  <summary>Details</summary>
Motivation: 现实数据常包含线性相关无法检测的隐藏结构（如潜在因子）。虽然非线性神经网络在实践中能提取这些结构，但缺乏可严格分析的最小高维模型。本文旨在构建一个理论可处理的模型来研究非线性方法的优势。

Method: 引入一个高维尖峰模型，包含两个潜在因子：一个可通过协方差检测（可见），另一个统计相关但不相关，仅出现在高阶矩中。分析非线性自编码器与线性方法（PCA、线性自编码器）在该模型上的表现。

Result: PCA和线性自编码器只能恢复协方差可见的潜在因子，而最小非线性自编码器可证明地提取两个因子。同时发现自监督测试损失与表示质量不一致：非线性自编码器恢复线性方法遗漏的结构，尽管其重构损失更高。

Conclusion: 该模型为分析非线性方法提取隐藏结构的优势提供了理论框架，揭示了自监督学习中损失函数与表示质量可能不一致的现象，强调了高阶统计量在数据表示中的重要性。

Abstract: Many real-world datasets contain hidden structure that cannot be detected by simple linear correlations between input features. For example, latent factors may influence the data in a coordinated way, even though their effect is invisible to covariance-based methods such as PCA. In practice, nonlinear neural networks often succeed in extracting such hidden structure in unsupervised and self-supervised learning. However, constructing a minimal high-dimensional model where this advantage can be rigorously analyzed has remained an open theoretical challenge. We introduce a tractable high-dimensional spiked model with two latent factors: one visible to covariance, and one statistically dependent yet uncorrelated, appearing only in higher-order moments. PCA and linear autoencoders fail to recover the latter, while a minimal nonlinear autoencoder provably extracts both. We analyze both the population risk, and empirical risk minimization. Our model also provides a tractable example where self-supervised test loss is poorly aligned with representation quality: nonlinear autoencoders recover latent structure that linear methods miss, even though their reconstruction loss is higher.

</details>


### [30] [Convergence Rates for Distribution Matching with Sliced Optimal Transport](https://arxiv.org/abs/2602.10691)
*Gauthier Thurin,Claire Boyer,Kimia Nadjahi*

Main category: stat.ML

TL;DR: 研究基于切片最优传输的切片匹配方案，分析其收敛性和非渐近收敛速率，特别针对高斯分布建立了理论保证


<details>
  <summary>Details</summary>
Motivation: 切片匹配方案是一种基于切片最优传输的高效迭代分布匹配方法，但缺乏对其收敛性和收敛速率的理论分析，特别是在高维情况下的性能保证

Method: 建立切片Wasserstein目标的Łojasiewicz型不等式，控制轨迹上的常数；特别针对高斯分布，通过每轮迭代中随机正交基的采样来控制特征值

Result: 推导了切片匹配方案的定量非渐近收敛速率，证明了对于高斯分布，通过随机正交基采样可以控制特征值，从而保证收敛性

Conclusion: 切片匹配方案在理论上有收敛保证，特别是对于高斯分布，随机正交基采样具有稳定效果；数值实验验证了维度、步长依赖关系以及正交基采样的稳定作用

Abstract: We study the slice-matching scheme, an efficient iterative method for distribution matching based on sliced optimal transport. We investigate convergence to the target distribution and derive quantitative non-asymptotic rates. To this end, we establish __ojasiewicz-type inequalities for the Sliced-Wasserstein objective. A key challenge is to control along the trajectory the constants in these inequalities. We show that this becomes tractable for Gaussian distributions. Specifically, eigenvalues are controlled when matching along random orthonormal bases at each iteration. We complement our theory with numerical experiments and illustrate the predicted dependence on dimension and step-size, as well as the stabilizing effect of orthonormal-basis sampling.

</details>


### [31] [Deep Learning of Compositional Targets with Hierarchical Spectral Methods](https://arxiv.org/abs/2602.10867)
*Hugo Tabanelli,Yatin Dandi,Luca Pesce,Florent Krzakala*

Main category: stat.ML

TL;DR: 论文研究了深度学习方法相比浅层方法的计算优势，通过高斯设置下的组合目标函数分析，发现三层模型通过分层学习能获得比两层模型更低的样本复杂度


<details>
  <summary>Details</summary>
Motivation: 理解为什么深度学习方法比浅层方法具有真正的计算优势是学习理论中的核心开放问题。论文旨在通过受控的高维高斯设置，研究组合目标函数的可学习性，揭示深度学习的优势来源。

Method: 使用三层拟合模型，通过分层谱估计器进行训练。模型在组合目标函数上进行学习，利用高斯普适性进行分析，比较两层和三层学习策略的样本复杂度。

Result: 三层模型通过分层学习能够逐步揭示中间表示的结构，将学习问题简化为更简单的谱估计问题。而浅层估计器必须同时解决所有组件。分析显示两层和三层学习策略在样本复杂度上存在显著差异。

Conclusion: 深度学习方法通过分层学习组合结构，能够获得比浅层方法更低的样本复杂度，这解释了深度学习的计算优势。中间表示揭示了输入层面无法访问的结构，使得学习过程可以分阶段进行。

Abstract: Why depth yields a genuine computational advantage over shallow methods remains a central open question in learning theory. We study this question in a controlled high-dimensional Gaussian setting, focusing on compositional target functions. We analyze their learnability using an explicit three-layer fitting model trained via layer-wise spectral estimators. Although the target is globally a high-degree polynomial, its compositional structure allows learning to proceed in stages: an intermediate representation reveals structure that is inaccessible at the input level. This reduces learning to simpler spectral estimation problems, well studied in the context of multi-index models, whereas any shallow estimator must resolve all components simultaneously. Our analysis relies on Gaussian universality, leading to sharp separations in sample complexity between two and three-layer learning strategies.

</details>


### [32] [Optimal Initialization in Depth: Lyapunov Initialization and Limit Theorems for Deep Leaky ReLU Networks](https://arxiv.org/abs/2602.10949)
*Constantin Kogler,Tassilo Schwarz,Samuel Kittle*

Main category: stat.ML

TL;DR: 本文对深度无偏Leaky ReLU网络进行了严格的概率分析，证明了网络激活范数对数的极限定理，揭示了激活消失/爆炸的相变由Lyapunov指数控制，并提出了Lyapunov初始化方法。


<details>
  <summary>Details</summary>
Motivation: 开发有效的初始化方法需要理解随机神经网络的行为。现有标准初始化方法（如He初始化或正交初始化）不能保证深度窄网络的激活稳定性。

Method: 对深度无偏Leaky ReLU网络进行严格的概率分析，证明激活范数对数的强大数定律和中心极限定理，计算高斯或正交权重矩阵下的Lyapunov指数，并提出基于Lyapunov指数归零的初始化方法。

Result: 证明了随着网络层数增加，激活增长由Lyapunov指数控制，该指数表征了激活消失和爆炸之间的尖锐相变。发现标准初始化方法不能保证深度窄网络的稳定性，而提出的Lyapunov初始化方法能确保网络尽可能稳定。

Conclusion: Lyapunov指数是理解深度神经网络激活稳定性的关键参数，基于该理论提出的Lyapunov初始化方法能有效提高网络稳定性，从而改善学习效果。

Abstract: The development of effective initialization methods requires an understanding of random neural networks. In this work, a rigorous probabilistic analysis of deep unbiased Leaky ReLU networks is provided. We prove a Law of Large Numbers and a Central Limit Theorem for the logarithm of the norm of network activations, establishing that, as the number of layers increases, their growth is governed by a parameter called the Lyapunov exponent. This parameter characterizes a sharp phase transition between vanishing and exploding activations, and we calculate the Lyapunov exponent explicitly for Gaussian or orthogonal weight matrices. Our results reveal that standard methods, such as He initialization or orthogonal initialization, do not guarantee activation stabilty for deep networks of low width. Based on these theoretical insights, we propose a novel initialization method, referred to as Lyapunov initialization, which sets the Lyapunov exponent to zero and thereby ensures that the neural network is as stable as possible, leading empirically to improved learning.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [33] [The Dataset of Daily Air Quality for the Years 2013-2023 in Italy](https://arxiv.org/abs/2602.10749)
*Fusta Moro Alessandro,Alessandro Fassò,Jacopo Rodeschini*

Main category: stat.AP

TL;DR: GRINS AQCLIM数据集提供了意大利700多个监测站点的空气污染物浓度和气候变量的每日统计数据，涵盖长时间序列，数据经过严格质量控制并开放获取。


<details>
  <summary>Details</summary>
Motivation: 意大利社会面临空气质量和气候问题，这些问题是公共卫生和政策规划等多个研究领域的交叉点。目前缺乏易于获取、即用且文档完善的空气质量和气候数据集。

Method: 从欧洲环境署和哥白尼计划获取原始数据，通过自动修复原始文件、人工检查站点信息、检测和移除异常值、以及基于日期的时序协调等多重处理步骤确保数据可靠性和质量。

Result: 创建了GRINS AQCLIM数据集，包含意大利700多个监测站点的空气污染物浓度和气候变量的每日统计数据（最小值、四分位数、均值、中位数、最大值等），数据托管在Zenodo平台遵循开放获取原则。

Conclusion: 该数据集为意大利空气质量和气候研究提供了高质量、易获取的标准化数据资源，支持跨学科研究和政策规划。

Abstract: Air quality and climate are major issues in Italian society and lie at the intersection of many research fields, including public health and policy planning. There is an increasing need for readily available, easily accessible, ready-to-use and well-documented datasets on air quality and climate. In this paper, we present the GRINS AQCLIM dataset, created under the GRINS project framework covering the Italian domain for an extensive time period. It includes daily statistics (e.g., minimum, quartiles, mean, median and maximum) for a collection of air pollutant concentrations and climate variables at the locations of the 700+ available monitoring stations. Input data are retrieved from the European Environmental Agency and Copernicus Programme and were subjected to multiple processing steps to ensure their reliability and quality. These steps include automatic procedures for fixing raw files, manual inspection of stations information, the detection and removal of anomalies, and the temporal harmonisation on a daily basis. Datasets are hosted on Zenodo under open-access principles.

</details>


### [34] [Integrating Unsupervised and Supervised Learning for the Prediction of Defensive Schemes in American football](https://arxiv.org/abs/2602.10784)
*Rouven Michels,Robert Bajons,Jan-Ole Fischer*

Main category: stat.AP

TL;DR: 开发了一个结合监督和无监督学习的统计框架，利用球员追踪数据预测美式足球防守阵型（人盯人或区域防守），通过弹性网络逻辑回归和梯度提升决策树，并引入基于隐马尔可夫模型的特征来提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 在美式足球比赛中，预测防守阵型对进攻方至关重要但极具挑战性，因为防守方的任务分配在开球前会被故意隐藏，难以实时识别。

Method: 结合监督和无监督学习，使用球员追踪数据。首先使用开球前的特征，然后加入球员移动轨迹，最后引入基于隐马尔可夫模型（HMM）的特征。HMM推断进攻和防守球员之间的潜在任务分配，并将解码的状态序列转化为监督模型的特征。

Result: 基于HMM的特征显著提升了预测性能，并且与防守阵型结果有显著关联。估计的随机效应提供了关于不同防守阵型和位置如何调整防守责任的解释性见解。

Conclusion: 提出的统计框架通过结合监督和无监督学习，有效预测美式足球防守阵型，HMM特征不仅提升预测准确性，还提供了对防守策略调整的深入理解。

Abstract: Anticipating defensive coverage schemes is a crucial yet challenging task for offenses in American football. Because defenders' assignments are intentionally disguised before the snap, they remain difficult to recognize in real time. To address this challenge, we develop a statistical framework that integrates supervised and unsupervised learning using player tracking data. Our goal is to forecast the defensive coverage scheme -- man or zone -- through elastic net logistic regression and gradient-boosted decision trees with incrementally derived features. We first use features from the pre-motion situation, then incorporate players' trajectories during motion in a naive way, and finally include features derived from a hidden Markov model (HMM). Based on player movements, the non-homogeneous HMM infers latent defensive assignments between offensive and defensive players during motion and transforms decoded state sequences into informative features for the supervised models. These HMM-based features enhance predictive performance and are significantly associated with coverage outcomes. Moreover, estimated random effects offer interpretable insights into how different defenses and positions adjust their coverage responsibilities.

</details>
