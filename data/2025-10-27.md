<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 10]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [To MCMC or not to MCMC: Evaluating non-MCMC methods for Bayesian penalized regression](https://arxiv.org/abs/2510.20947)
*Florian D. van Leeuwen,Sara van Erp*

Main category: stat.CO

TL;DR: 本文比较了MCMC和非MCMC方法在高维惩罚回归中的表现，发现平均场变分推断在大多数情况下与MCMC方法性能相当，但计算速度显著提升。


<details>
  <summary>Details</summary>
Motivation: MCMC采样计算成本高昂，而替代方法虽然简化了后验分布假设，但其对预测性能的影响尚不明确。

Method: 通过高维表格数据的综合模拟研究，并使用包含连续和二元结果的实证数据集验证发现，同时提供R语言实现教程。

Result: 平均场变分推断在模拟中MSE增加3-90%，但运行时间减少7-30倍；在实证数据中某些情况下速度提升100-400倍且预测性能相似或更优，但性能存在较大差异。

Conclusion: 计算捷径的合理性具有情境依赖性，在某些情况下可以显著加速计算而保持预测性能，但在其他情况下可能以预测精度为代价。

Abstract: Markov Chain Monte Carlo (MCMC) sampling is computationally expensive,
especially for complex models. Alternative methods make simplifying assumptions
about the posterior to reduce computational burden, but their impact on
predictive performance remains unclear. This paper compares MCMC and non-MCMC
methods for high-dimensional penalized regression, examining when computational
shortcuts are justified for prediction tasks.
  We conduct a comprehensive simulation study using high-dimensional tabular
data, then validate findings with empirical datasets featuring both continuous
and binary outcomes. An in-depth analysis of one dataset provides a
step-by-step tutorial implementing various algorithms in R.
  Our results show that mean-field variational inference consistently performs
comparably to MCMC methods. In simulations, mean-field VI exhibited 3-90\%
higher MSE across scenarios while reducing runtime by 7-30x compared to
Hamiltonian Monte Carlo. Empirical datasets revealed dramatic speed-ups
(100-400x) in some cases with similar or superior predictive performance.
However, performance varied: some cases showed over 100x MSE increases with
only 30x speed-ups, highlighting the context-dependent nature of these
trade-offs.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [2] [Sensitivity Analysis when Generalizing Causal Effects from Multiple Studies to a Target Population: Motivation from the ECHO Program](https://arxiv.org/abs/2510.21116)
*Bolun Liu,Trang Quynh Nguyen,Elizabeth A. Stuart,Bryan Lau,Amii M. Kress,Michael R. Elliott,Kyle R. Busse,Ellen C. Caniglia,Yajnaseni Chakraborti,Amy J. Elliott,James E. Gern,Alison E. Hipwell,Catherine J. Karr,Kaja Z. LeWinn,Li Luo,Hans-Georg Müller,Sunni L. Mumford,Ruby H. N. Nguyen,Emily Oken,Janet L. Peacock,Enrique F. Schisterman,Arjun Sondhi,Rosalind J. Wright,Yidong Zhou,Elizabeth L. Ogburn*

Main category: stat.ME

TL;DR: 提出了一个可解释的敏感性分析框架，用于评估因果效应估计在目标群体中推广时对未观察效应修饰的稳健性，适用于多种研究设计场景。


<details>
  <summary>Details</summary>
Motivation: 未观察的效应修饰因子在将因果效应估计推广到目标群体时可能引入偏差，需要开发不依赖未知参数分布或函数假设的敏感性分析方法。

Method: 扩展敏感性分析框架，适应多种推广场景（包括多个随机试验、观察性研究或其组合），利用多研究设置通过假设检验检测推广假设的违反。

Result: 模拟显示所提出的检验在现实样本量下具有高功效，并在ECHO研究中应用于二手烟暴露对出生体重影响的广义效应估计分析。

Conclusion: 该框架为评估因果效应推广的稳健性提供了实用工具，特别适用于存在未观察效应修饰因子的复杂研究环境。

Abstract: Unobserved effect modifiers can induce bias when generalizing causal effect
estimates to target populations. In this work, we extend a sensitivity analysis
framework assessing the robustness of study results to unobserved effect
modification that adapts to various generalizability scenarios, including
multiple (conditionally) randomized trials, observational studies, or
combinations thereof. This framework is interpretable and does not rely on
distributional or functional assumptions about unknown parameters. We
demonstrate how to leverage the multi-study setting to detect violation of the
generalizability assumption through hypothesis testing, showing with
simulations that the proposed test achieves high power under real-world sample
sizes. Finally, we apply our sensitivity analysis framework to analyze the
generalized effect estimate of secondhand smoke exposure on birth weight using
cohort sites from the Environmental influences on Child Health Outcomes (ECHO)
study.

</details>


### [3] [Forecast reconciliation with non-linear constraints](https://arxiv.org/abs/2510.21249)
*Daniele Girolimetto,Anastasios Panagiotelis,Tommaso Di Fonzo,Han Li*

Main category: stat.ME

TL;DR: 本文提出非线性约束协调（NLCR）方法，用于处理非线性约束时间序列预测问题，通过约束优化将预测投影到非线性曲面，并在理论和实证上证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有预测协调方法主要处理线性约束，但现实中存在许多非线性约束的时间序列（如死亡率、失业率等比率变量），需要扩展预测协调方法到非线性约束领域。

Method: 提出NLCR算法，将不满足非线性约束的预测通过约束优化问题投影到非线性曲面上，确保新预测满足约束条件。

Result: 理论分析给出了NLCR方法保证提高预测精度的充分条件，在人口学和经济学两个数据集上的实证应用显示NLCR显著优于相关基准方法。

Conclusion: NLCR方法成功将预测协调扩展到非线性约束时间序列，在理论和实证上都证明了其有效性，能够显著提高预测精度。

Abstract: Methods for forecasting time series adhering to linear constraints have seen
notable development in recent years, especially with the advent of forecast
reconciliation. This paper extends forecast reconciliation to the open question
of non-linearly constrained time series. Non-linear constraints can emerge with
variables that are formed as ratios such as mortality rates and unemployment
rates. On the methodological side, Non-linearly Constrained Reconciliation
(NLCR) is proposed. This algorithm adjusts forecasts that fail to meet
non-linear constraints, in a way that ensures the new forecasts meet the
constraints. The NLCR method is a projection onto a non-linear surface,
formulated as a constrained optimisation problem. On the theoretical side,
optimisation methods are again used, this time to derive sufficient conditions
for when the NLCR methodology is guaranteed to improve forecast accuracy.
Finally on the empirical side, NLCR is applied to two datasets from demography
and economics and shown to significantly improve forecast accuracy relative to
relevant benchmarks.

</details>


### [4] [Autocorrelation Test under Frequent Mean Shifts](https://arxiv.org/abs/2510.21047)
*Ziyang Liu,Ning Hao,Yue Selena Niu,Han Xiao,Hongxu Ding*

Main category: stat.ME

TL;DR: 提出了一个在频繁均值偏移情况下检验时间序列自相关的新框架——Shift-Immune Portmanteau (SIP) 检验，该方法对均值偏移具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 经典的自相关检验方法（如Box-Pierce检验）需要平稳性假设，要求先去除非平稳成分如趋势或均值偏移，但在均值结构复杂（如频繁变化的片段常数）时这不切实际。

Method: 开发了Shift-Immune Portmanteau (SIP) 检验，这是一个能够可靠检验自相关且对均值偏移具有鲁棒性的新检验方法。

Result: SIP检验在存在频繁均值偏移的情况下仍能可靠地检测自相关，并在纳米孔测序数据中展示了应用效果。

Conclusion: SIP检验为存在频繁均值偏移的时间序列数据提供了一种有效的自相关检验框架，解决了经典方法的局限性。

Abstract: Testing for the presence of autocorrelation is a fundamental problem in time
series analysis. Classical methods such as the Box-Pierce test rely on the
assumption of stationarity, necessitating the removal of non-stationary
components such as trends or shifts in the mean prior to application. However,
this is not always practical, particularly when the mean structure is complex,
such as being piecewise constant with frequent shifts. In this work, we propose
a new inferential framework for autocorrelation in time series data under
frequent mean shifts. In particular, we introduce a Shift-Immune Portmanteau
(SIP) test that reliably tests for autocorrelation and is robust against mean
shifts. We illustrate an application of our method to nanopore sequencing data.

</details>


### [5] [Handling Missing Responses under Cluster Dependence with Applications to Language Model Evaluation](https://arxiv.org/abs/2510.20928)
*Zhenghao Zeng,David Arbour,Avi Feller,Ishita Dasgupta,Atanu R Sinha,Edward H. Kennedy*

Main category: stat.ME

TL;DR: 本文分析了在存在缺失标注和聚类依赖的情况下，如何对GenAI模型进行可靠评估。作者研究了双重稳健估计器在此场景下的应用，并建立了聚类依赖下的新理论性质。


<details>
  <summary>Details</summary>
Motivation: 人类标注在评估GenAI模型性能中至关重要，但实践中常面临两个挑战：缺失标注和聚类依赖（如同用户的问题高度相关）。可靠推断需要同时解决这两个问题以实现无偏估计和适当量化不确定性。

Method: 使用双重稳健估计器（广泛用于缺失数据分析和因果推断的方法），并在聚类依赖下建立理论性质。通过模拟和真实对话质量数据集验证发现。

Result: 理论和实证结果都表明，在缺失响应问题中必须考虑聚类依赖才能进行有效的统计推断。

Conclusion: 在评估GenAI模型时，必须同时处理缺失标注和聚类依赖问题，双重稳健估计器在此场景下具有重要价值，且聚类依赖的纳入对有效统计推断至关重要。

Abstract: Human annotations play a crucial role in evaluating the performance of GenAI
models. Two common challenges in practice, however, are missing annotations
(the response variable of interest) and cluster dependence among human-AI
interactions (e.g., questions asked by the same user may be highly correlated).
Reliable inference must address both these issues to achieve unbiased
estimation and appropriately quantify uncertainty when estimating average
scores from human annotations. In this paper, we analyze the doubly robust
estimator, a widely used method in missing data analysis and causal inference,
applied to this setting and establish novel theoretical properties under
cluster dependence. We further illustrate our findings through simulations and
a real-world conversation quality dataset. Our theoretical and empirical
results underscore the importance of incorporating cluster dependence in
missing response problems to perform valid statistical inference.

</details>


### [6] [Expectation-propagation for Bayesian empirical likelihood inference](https://arxiv.org/abs/2510.21174)
*Kenyon Ng,Weichang Yu,Howard D. Bondell*

Main category: stat.ME

TL;DR: 提出基于期望传播的变分方法，近似贝叶斯经验似然后验分布，在计算成本和精度之间取得平衡，无需通过伪观测等调整改变目标后验。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯经验似然虽然避免了参数模型设定，但面临计算挑战，包括每次似然评估需要求解约束优化问题，以及小样本情况下后验支持非凸的困难。

Method: 采用基于期望传播的变分方法，近似贝叶斯经验似然后验分布，不改变目标后验。

Result: 经验研究表明，该方法相对于哈密顿蒙特卡洛和变分贝叶斯等方法，能够实现更优的成本-精度权衡。

Conclusion: 理论上证明了近似方法与贝叶斯经验似然后验是渐近等价的。

Abstract: Bayesian inference typically relies on specifying a parametric model that
approximates the data-generating process. However, misspecified models can
yield poor convergence rates and unreliable posterior calibration. Bayesian
empirical likelihood offers a semi-parametric alternative by replacing the
parametric likelihood with a profile empirical likelihood defined through
moment constraints, thereby avoiding explicit distributional assumptions.
Despite these advantages, Bayesian empirical likelihood faces substantial
computational challenges, including the need to solve a constrained
optimization problem for each likelihood evaluation and difficulties with
non-convex posterior support, particularly in small-sample settings. This paper
introduces a variational approach based on expectation-propagation to
approximate the Bayesian empirical-likelihood posterior, balancing
computational cost and accuracy without altering the target posterior via
adjustments such as pseudo-observations. Empirically, we show that our approach
can achieve a superior cost-accuracy trade-off relative to existing methods,
including Hamiltonian Monte Carlo and variational Bayes. Theoretically, we show
that the approximation and the Bayesian empirical-likelihood posterior are
asymptotically equivalent.

</details>


### [7] [Bayesian analysis of flexible Heckman selection models using Hamiltonian Monte Carlo](https://arxiv.org/abs/2510.20942)
*Heeju Lim,Victor E. Lachos,Victor H. Lachos*

Main category: stat.ME

TL;DR: 本文提出了一种贝叶斯分析Heckman选择模型的方法，用尺度混合正态分布（如Student's-t和污染正态分布）替代传统的高斯假设，以处理现实数据中的厚尾行为。


<details>
  <summary>Details</summary>
Motivation: 现实数据经常偏离Heckman选择模型中常用的独立二元正态分布假设，表现出厚尾行为，如果不正确处理会导致估计不一致。

Method: 使用尺度混合正态分布类中的知名成员（Student's-t和污染正态分布）替代高斯假设，并利用Stan的No-U-Turn采样器进行后验模拟。

Result: 通过广泛的模拟研究比较了正态、Student's-t和污染正态分布在Heckman选择模型中的性能，并在医疗保健和劳动力供给数据上展示了方法的广泛适用性。

Conclusion: 提出的方法在HeckmanStan R包中实现，为处理样本选择偏差提供了更稳健的贝叶斯框架。

Abstract: The Heckman selection model is widely used in econometric analysis and other
social sciences to address sample selection bias in data modeling. A common
assumption in Heckman selection models is that the error terms follow an
independent bivariate normal distribution. However, real-world data often
deviates from this assumption, exhibiting heavy-tailed behavior, which can lead
to inconsistent estimates if not properly addressed. In this paper, we propose
a Bayesian analysis of Heckman selection models that replace the Gaussian
assumption with well-known members of the class of scale mixture of normal
distributions, such as the Student's-t and contaminated normal distributions.
For these complex structures, Stan's default No-U-Turn sampler is utilized to
obtain posterior simulations. Through extensive simulation studies, we compare
the performance of the Heckman selection models with normal, Student's-t and
contaminated normal distributions. We also demonstrate the broad applicability
of this methodology by applying it to medical care and labor supply data. The
proposed algorithms are implemented in the R package HeckmanStan.

</details>


### [8] [MECfda: An R Package for Bias Correction Due to Measurement Error in Functional and Scalar Covariates in Scalar-on-Function Regression Models](https://arxiv.org/abs/2510.21661)
*Heyang Ji,Carmen Tekwe*

Main category: stat.ME

TL;DR: MECfda是一个R包，用于处理带测量误差的函数型数据分析，支持标量对函数回归、广义标量对函数回归和函数分位数回归模型，并提供偏误校正估计。


<details>
  <summary>Details</summary>
Motivation: 设备评估的物理活动或睡眠数据虽然是客观的，但仍然存在测量误差，需要开发能够处理带噪声函数型数据的稳健推断方法。

Method: 开发MECfda R包，统一标量对函数回归、广义标量对函数回归和函数分位数回归模型，并提供函数协变量存在测量误差时的偏误校正估计。

Result: MECfda包提供了处理带测量误差函数型数据的工具，通过一致的语法实现了对噪声函数数据的稳健推断。

Conclusion: MECfda为涉及噪声函数型数据的FDA应用提供了统一的、稳健的推断框架。

Abstract: Functional data analysis (FDA) deals with high-resolution data recorded over
a continuum, such as time, space or frequency. Device-based assessments of
physical activity or sleep are objective yet still prone to measurement error.
We present MECfda, an R package that (i) fits scalar-on-function, generalized
scalar-on-function, and functional quantile regression models, and (ii)
provides bias-corrected estimation when functional covariates are measured with
error. By unifying these tools under a consistent syntax, MECfda enables robust
inference for FDA applications that involve noisy functional data.

</details>


### [9] [Leveraging semantic similarity for experimentation with AI-generated treatments](https://arxiv.org/abs/2510.21119)
*Lei Shi,David Arbour,Raghavendra Addanki,Ritwik Sinha,Avi Feller*

Main category: stat.ME

TL;DR: 提出双核表示学习方法来处理LLM实验中高维治疗的表示问题，通过核方法学习低维表示，支持生成治疗变体和在线实验的自适应分配。


<details>
  <summary>Details</summary>
Motivation: LLM实验中的治疗组合了人类和模型生成的内容，需要在高维空间中保持语义意义的同时使分析可行。

Method: 双核表示学习，通过治疗和用户协变量的核表示内积建模因果效应，使用交替最小化算法学习表示。

Result: 在低秩因子模型下提供收敛保证，通过数值实验验证了方法的有效性。

Conclusion: 该方法能够有效学习高维治疗的语义表示，支持在线实验的自适应设计。

Abstract: Large Language Models (LLMs) enable a new form of digital experimentation
where treatments combine human and model-generated content in increasingly
sophisticated ways. The main methodological challenge in this setting is
representing these high-dimensional treatments without losing their semantic
meaning or rendering analysis intractable. Here, we address this problem by
focusing on learning low-dimensional representations that capture the
underlying structure of such treatments. These representations enable
downstream applications such as guiding generative models to produce meaningful
treatment variants and facilitating adaptive assignment in online experiments.
We propose double kernel representation learning, which models the causal
effect through the inner product of kernel-based representations of treatments
and user covariates. We develop an alternating-minimization algorithm that
learns these representations efficiently from data and provides convergence
guarantees under a low-rank factor model. As an application of this framework,
we introduce an adaptive design strategy for online experimentation and
demonstrate the method's effectiveness through numerical experiments.

</details>


### [10] [A Comparison for Non-Specialists of Workflow Steps and Similarity of Factor Rankings for Several Global Sensitivity Analysis Methods](https://arxiv.org/abs/2510.21579)
*Ken Newman,Shaini Naha,Leah Jackson-Blake,Cairistiona Topp,Miriam Glendell,Adam Butler*

Main category: stat.ME

TL;DR: 本文研究了多种全局敏感性分析(GSA)方法在三个不同复杂度的模拟器上的应用，发现各方法在因子排序上具有较高相似性，Sobol'一阶和总敏感性指数易于解释，回归树方法能提供额外的交互作用洞察。


<details>
  <summary>Details</summary>
Motivation: 由于GSA方法种类繁多，非GSA专家在选择方法时面临挑战，包括工作流程步骤和复杂性、GSA输出结果的解释以及不同方法在因子排序上的相似程度。

Method: 研究将广泛使用和较少使用的GSA方法应用于三个复杂度不同的模拟器，比较了各方法的实施问题和参数范围规范的重要性。

Result: 基于Kendall's W的因子排序相似性普遍较高，Sobol'一阶和总敏感性指数易于解释且信息丰富，回归树方法能提供额外的交互作用洞察。

Conclusion: 所有GSA方法在实施过程中都存在共同问题，其中参数范围的规范尤为关键，不同方法在因子排序结果上具有较高一致性。

Abstract: Global sensitivity analysis (GSA) is a recommended step in the use of
computer simulation models. GSA quantifies the relative importance of model
inputs on outputs (Factor Ranking), identifies inputs that could be fixed, thus
simplifying model calibration (Factor Fixing), and pinpointing areas for future
data collection (Factor Prioritization). Given the wide variety of GSA methods,
choosing between methods can be challenging for non-GSA experts. Issues include
workflow steps and complexity, interpretation of GSA outputs, and the degree of
similarity between methods in Factor Ranking. We conducted a study of both
widely and less commonly used GSA methods applied to three simulators of
differing complexity. All methods share common issues around implementation
with specification of parameter ranges particularly critical. Similarities in
Factor Rankings were generally high based on Kendall's W. Sobol' first order
and total sensitivity indices were easy to interpret and informative with
regression trees providing additional insight into interactions.

</details>


### [11] [Optimal weighted tests for replication studies and the two-trials rule](https://arxiv.org/abs/2510.21708)
*David S. Robertson,Thomas Jaki*

Main category: stat.ME

TL;DR: 提出了一种基于原始研究结果的最优加权Bonferroni程序，用于分析复制研究，旨在最大化试验的析取功效


<details>
  <summary>Details</summary>
Motivation: 在多重假设检验中控制整体族错误率，同时通过加权反映假设的先验重要性或最大化实验的整体功效

Method: 使用基于原始研究结果的最优加权Bonferroni程序，以最大化复制研究的析取功效为最优性准则

Result: 所提出的程序可以显著提高复制研究的析取功效，并且对两个研究之间效应大小的变化具有鲁棒性

Conclusion: 最优加权Bonferroni程序在复制研究中能够有效提高统计功效，同时保持对效应大小变化的稳健性

Abstract: Replication studies for scientific research are an important part of ensuring
the reliability and integrity of experimental findings. In the context of
clinical trials, the concept of replication has been formalised by the
'two-trials' rule, where two pivotal studies are required to show positive
results before a drug can be approved. In experiments testing multiple
hypotheses simultaneously, control of the overall familywise error rate (FWER)
is additionally required in many contexts. The well-known Bonferroni procedure
controls the FWER, and a natural extension is to introduce weights into this
procedure to reflect the a-priori importance of hypotheses or to maximise some
measure of the overall power of the experiment. In this paper, we consider
analysing a replication study using an optimal weighted Bonferroni procedure,
with the weights based on the results of the original study that is being
replicated and the optimality criterion being to maximise the disjunctive power
of the trial (the power to reject at least one non-null hypothesis). We show
that using the proposed procedure can lead to a substantial increase in the
disjunctive power of the replication study, and is robust to changes in the
effect sizes between the two studies.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [12] [Representing caregiver burden in observational studies: Development of the Caregiver Burden Index (CareBI) using NSOC](https://arxiv.org/abs/2510.21630)
*Forough Mahpouya,Sabrina Casucci,Suzanne Sullivan,Christopher Barrick*

Main category: stat.AP

TL;DR: 开发了Caregiver Burden Index (CareBI)工具，用于在观察性研究中量化照顾者负担，包含客观、主观和人际关系三个维度，并与现有工具Zarit Burden Interview保持一致。


<details>
  <summary>Details</summary>
Motivation: 现有照顾者负担评估工具与观察性数据集不兼容，在医疗服务和规划研究中代表性不足，需要开发适用于定量模型和观察性研究的工具。

Method: 采用多步骤开发验证过程，包括识别和准备NSOC调查项目、探索性和验证性因子分析、分数估计、解释和外部验证，使用NSOC第12轮数据。

Result: CareBI成功识别低、中、高负担照顾者，与照顾者和被照顾者结果相关，对已知负担相关风险和缓解因素敏感，具有构建效度。

Conclusion: CareBI是可重复的工具，可将照顾者指标嵌入健康运营、预测建模和公共政策框架，为应用运筹学和工业工程方法解决老年和长期护理中的心理社会测量挑战提供模板。

Abstract: Informal caregiving often carries a significant emotional, physical, and
financial toll, yet caregiver burden is often underrepresented in healthcare
research and methods. Existing caregiver burden instruments, while valuable in
clinical research, often lack compatibility with observational datasets
regularly used in health services research and planning. This study introduces
the Caregiver Burden Index (CareBI) developed for the National Study of
Caregiving (NSOC), that can be used to represent caregiver burden in
quantitative models and observational research studies. CareBI was developed
and validated using a multistep process that included the identification and
preparation of individual NSOC survey items, exploratory and confirmatory
factor analysis, score estimation, interpretation, and external validation. The
study used data from round 12 of the NSOC. CareBI represents three domains of
burden: objective, subjective, and interpersonal, providing a comprehensive
view of both the positive and negative aspects of caregiving. It also aligns
with the Zarit Burden Interview, a widely used tool for prospectively assessing
caregiver burden. Construct validity was assessed by comparing CareBI's
relationship with caregiver and care recipient outcomes, as well as sensitivity
to known burden-related risk and mitigation factors. Early findings affirm the
scale's utility in categorizing low-, moderate-, and high-burden caregivers and
guiding resource-oriented strategies. CareBI represents a reproducible tool for
embedding caregiver metrics into health operations, predictive modeling, and
public policy frameworks, and provides a template for applying operations
research and industrial engineering methods to psychosocial measurement
challenges in aging and long-term care.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [13] [Exponential Convergence Guarantees for Iterative Markovian Fitting](https://arxiv.org/abs/2510.20871)
*Marta Gentiloni Silveri,Giovanni Conforti,Alain Durmus*

Main category: stat.ML

TL;DR: 本文首次为迭代马尔可夫拟合(IMF)方法提供了非渐近指数收敛保证，在参考测度和边缘分布的温和结构假设下，假设足够大的时间范围。


<details>
  <summary>Details</summary>
Motivation: 虽然先前工作已建立了IMF的渐近收敛保证，但对定量、非渐近收敛的理解仍然未知。

Method: 基于马尔可夫投影算子的新收缩结果进行分析，涵盖边缘分布为对数凹和弱对数凹两种关键情况。

Result: 在温和结构假设下，IMF实现了非渐近指数收敛，为扩散薛定谔桥匹配(DSBM)的理论保证铺平了道路。

Conclusion: 该分析为IMF提供了首个非渐近收敛保证，并为DSBM的理论分析奠定了基础。

Abstract: The Schr\"odinger Bridge (SB) problem has become a fundamental tool in
computational optimal transport and generative modeling. To address this
problem, ideal methods such as Iterative Proportional Fitting and Iterative
Markovian Fitting (IMF) have been proposed-alongside practical approximations
like Diffusion Schr\"odinger Bridge and its Matching (DSBM) variant. While
previous work have established asymptotic convergence guarantees for IMF, a
quantitative, non-asymptotic understanding remains unknown. In this paper, we
provide the first non-asymptotic exponential convergence guarantees for IMF
under mild structural assumptions on the reference measure and marginal
distributions, assuming a sufficiently large time horizon. Our results
encompass two key regimes: one where the marginals are log-concave, and another
where they are weakly log-concave. The analysis relies on new contraction
results for the Markovian projection operator and paves the way to theoretical
guarantees for DSBM.

</details>


### [14] [Kernel Learning with Adversarial Features: Numerical Efficiency and Adaptive Regularization](https://arxiv.org/abs/2510.20883)
*Antônio H. Ribeiro,David Vävinggren,Dave Zachariah,Thomas B. Schön,Francis Bach*

Main category: stat.ML

TL;DR: 提出了一种在再生核希尔伯特空间中重新表述对抗训练的方法，从输入空间扰动转向特征空间扰动，能够精确求解内部最大化问题并提供高效优化。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗训练方法依赖于计算代价高昂的min-max问题，限制了实际应用。需要找到更高效的对抗训练方法。

Method: 在再生核希尔伯特空间中重新表述对抗训练，将输入空间扰动转换为特征空间扰动，使用迭代核岭回归进行高效优化，并扩展到多核学习。

Result: 该方法在干净和对抗性设置下都表现出良好性能，提供了正则化估计器，能够自然适应噪声水平和底层函数的平滑度。

Conclusion: 提出的特征空间扰动对抗训练方法能够有效解决传统对抗训练的计算效率问题，同时保持良好的鲁棒性性能。

Abstract: Adversarial training has emerged as a key technique to enhance model
robustness against adversarial input perturbations. Many of the existing
methods rely on computationally expensive min-max problems that limit their
application in practice. We propose a novel formulation of adversarial training
in reproducing kernel Hilbert spaces, shifting from input to feature-space
perturbations. This reformulation enables the exact solution of inner
maximization and efficient optimization. It also provides a regularized
estimator that naturally adapts to the noise level and the smoothness of the
underlying function. We establish conditions under which the feature-perturbed
formulation is a relaxation of the original problem and propose an efficient
optimization algorithm based on iterative kernel ridge regression. We provide
generalization bounds that help to understand the properties of the method. We
also extend the formulation to multiple kernel learning. Empirical evaluation
shows good performance in both clean and adversarial settings.

</details>


### [15] [A Short Note on Upper Bounds for Graph Neural Operator Convergence Rate](https://arxiv.org/abs/2510.20954)
*Roxanne Holden,Luana Ruiz*

Main category: stat.ML

TL;DR: 该论文总结了图序列极限图神经网络算子的谱收敛性，在不同假设条件下给出了收敛速率边界，并验证了经验紧致性。


<details>
  <summary>Details</summary>
Motivation: 分析图神经网络在图序列极限下的渐近行为，为GNN的可迁移性分析提供理论基础。

Method: 基于图序列的谱收敛性，在无假设、全局Lipschitz连续性和分段Lipschitz连续性三种条件下推导算子级收敛速率边界。

Result: 得到了不同假设条件下的收敛速率边界，并在合成和真实数据上验证了这些边界的经验紧致性。

Conclusion: 揭示了假设条件与收敛速率之间的权衡关系，为图神经网络的理论分析和可迁移性研究提供了重要工具。

Abstract: Graphons, as limits of graph sequences, provide a framework for analyzing the
asymptotic behavior of graph neural operators. Spectral convergence of sampled
graphs to graphons yields operator-level convergence rates, enabling
transferability analyses of GNNs. This note summarizes known bounds under no
assumptions, global Lipschitz continuity, and piecewise-Lipschitz continuity,
highlighting tradeoffs between assumptions and rates, and illustrating their
empirical tightness on synthetic and real data.

</details>


### [16] [Doubly-Regressing Approach for Subgroup Fairness](https://arxiv.org/abs/2510.21091)
*Kyungseon Lee,Kunwoong Kim,Jihu Lee,Dongyoon Yang,Yongdai Kim*

Main category: stat.ML

TL;DR: 提出了一种新的子群公平性学习算法DRAF，通过关注样本量充足的子群和边际公平性，解决了多敏感属性下子群数量激增带来的计算负担和数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 随着敏感属性数量的增加，子群数量呈指数增长，导致计算负担加重和数据稀疏问题（子群样本量过小），需要开发能够有效处理这些挑战的公平性算法。

Method: 提出了子群子集公平性概念和相应的分布公平性度量supIPM，并开发了双重回归对抗学习算法DRAF，通过减少代理公平性差距来间接优化supIPM，大幅降低计算复杂度。

Result: 理论证明代理公平性差距是supIPM的上界，实证研究表明DRAF算法在基准数据集上优于基线方法，特别是在敏感属性数量大、子群规模小的情况下表现更佳。

Conclusion: DRAF算法通过关注样本充足的子群和边际公平性，有效解决了多敏感属性场景下的子群公平性问题，为处理高维敏感属性提供了可行的解决方案。

Abstract: Algorithmic fairness is a socially crucial topic in real-world applications
of AI.
  Among many notions of fairness, subgroup fairness is widely studied when
multiple sensitive attributes (e.g., gender, race, age) are present.
  However, as the number of sensitive attributes grows, the number of subgroups
increases accordingly, creating heavy computational burdens and data sparsity
problem (subgroups with too small sizes).
  In this paper, we develop a novel learning algorithm for subgroup fairness
which resolves these issues by focusing on subgroups with sufficient sample
sizes as well as marginal fairness (fairness for each sensitive attribute).
  To this end, we formalize a notion of subgroup-subset fairness and introduce
a corresponding distributional fairness measure called the supremum Integral
Probability Metric (supIPM).
  Building on this formulation, we propose the Doubly Regressing Adversarial
learning for subgroup Fairness (DRAF) algorithm, which reduces a surrogate
fairness gap for supIPM with much less computation than directly reducing
supIPM.
  Theoretically, we prove that the proposed surrogate fairness gap is an upper
bound of supIPM.
  Empirically, we show that the DRAF algorithm outperforms baseline methods in
benchmark datasets, specifically when the number of sensitive attributes is
large so that many subgroups are very small.

</details>


### [17] [Enforcing Calibration in Multi-Output Probabilistic Regression with Pre-rank Regularization](https://arxiv.org/abs/2510.21273)
*Naomi Desobry,Elnura Zhalieva,Souhaib Ben Taieb*

Main category: stat.ML

TL;DR: 提出了一个多输出回归中的多元校准正则化框架，通过惩罚投影概率积分变换与均匀分布的偏差来强制校准，可应用于任意预排序函数。


<details>
  <summary>Details</summary>
Motivation: 多输出回归中的多元校准比单输出回归更具挑战性，现有文献主要关注基于预排序函数的诊断工具，但缺乏训练过程中的校准强制方法。

Method: 引入通用正则化框架，通过惩罚投影PIT与均匀分布的偏差来强制多元校准，可添加到任何概率预测器的损失函数中。提出了联合强制边际和多元预排序校准的正则化损失，以及基于PCA的新预排序方法。

Result: 在18个真实世界多输出回归数据集上，未正则化模型普遍存在校准错误，而该方法显著改善了所有预排序函数的校准性能，且不牺牲预测准确性。

Conclusion: 该框架为多输出回归提供了有效的多元校准方法，能够显著改善模型校准而不影响预测性能。

Abstract: Probabilistic models must be well calibrated to support reliable
decision-making. While calibration in single-output regression is well studied,
defining and achieving multivariate calibration in multi-output regression
remains considerably more challenging. The existing literature on multivariate
calibration primarily focuses on diagnostic tools based on pre-rank functions,
which are projections that reduce multivariate prediction-observation pairs to
univariate summaries to detect specific types of miscalibration. In this work,
we go beyond diagnostics and introduce a general regularization framework to
enforce multivariate calibration during training for arbitrary pre-rank
functions. This framework encompasses existing approaches such as highest
density region calibration and copula calibration. Our method enforces
calibration by penalizing deviations of the projected probability integral
transforms (PITs) from the uniform distribution, and can be added as a
regularization term to the loss function of any probabilistic predictor.
Specifically, we propose a regularization loss that jointly enforces both
marginal and multivariate pre-rank calibration. We also introduce a new
PCA-based pre-rank that captures calibration along directions of maximal
variance in the predictive distribution, while also enabling dimensionality
reduction. Across 18 real-world multi-output regression datasets, we show that
unregularized models are consistently miscalibrated, and that our methods
significantly improve calibration across all pre-rank functions without
sacrificing predictive accuracy.

</details>


### [18] [HollowFlow: Efficient Sample Likelihood Evaluation using Hollow Message Passing](https://arxiv.org/abs/2510.21542)
*Johann Flemming Gloy,Simon Olsson*

Main category: stat.ML

TL;DR: HollowFlow是一种基于流的生成模型，通过非回溯图神经网络实现常数级的似然计算复杂度，显著加速Boltzmann生成器在大规模系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于流和扩散的模型在大规模系统中因似然计算复杂度随系统规模增长而变得不可行的问题。

Method: 使用非回溯图神经网络(NoBGNN)强制块对角雅可比矩阵结构，使似然计算仅需常数次反向传播，实现O(n²)的加速。

Result: 在两个不同规模的系统上验证，采样和似然评估时间显著减少，较大系统获得100倍加速，符合理论缩放规律。

Conclusion: HollowFlow框架为之前受计算瓶颈限制的高维科学问题提供了有效的解决方案，可扩展到更大的系统。

Abstract: Flow and diffusion-based models have emerged as powerful tools for scientific
applications, particularly for sampling non-normalized probability
distributions, as exemplified by Boltzmann Generators (BGs). A critical
challenge in deploying these models is their reliance on sample likelihood
computations, which scale prohibitively with system size $n$, often rendering
them infeasible for large-scale problems. To address this, we introduce
$\textit{HollowFlow}$, a flow-based generative model leveraging a novel
non-backtracking graph neural network (NoBGNN). By enforcing a block-diagonal
Jacobian structure, HollowFlow likelihoods are evaluated with a constant number
of backward passes in $n$, yielding speed-ups of up to $\mathcal{O}(n^2)$: a
significant step towards scaling BGs to larger systems. Crucially, our
framework generalizes: $\textbf{any equivariant GNN or attention-based
architecture}$ can be adapted into a NoBGNN. We validate HollowFlow by training
BGs on two different systems of increasing size. For both systems, the sampling
and likelihood evaluation time decreases dramatically, following our
theoretical scaling laws. For the larger system we obtain a $10^2\times$
speed-up, clearly illustrating the potential of HollowFlow-based approaches for
high-dimensional scientific problems previously hindered by computational
bottlenecks.

</details>


### [19] [Fisher meets Feynman: score-based variational inference with a product of experts](https://arxiv.org/abs/2510.21598)
*Diana Cai,Robert M. Gower,David M. Blei,Lawrence K. Saul*

Main category: stat.ML

TL;DR: 提出了一种用于黑盒变分推断的高表达能力且易于处理的变分分布族，该分布族由多元t分布的加权乘积组成，能够建模偏斜、重尾和多峰分布。


<details>
  <summary>Details</summary>
Motivation: 现有的变分分布族在表达能力上存在限制，难以有效建模具有复杂特征（如偏斜、重尾、多峰）的目标分布。需要开发既具有高表达能力又易于采样的变分分布族。

Method: 使用多元t分布的加权乘积作为变分分布，通过费曼恒等式将其转化为包含辅助Dirichlet变量的潜变量模型，从而能够进行采样。通过最小化正则化Fisher散度来优化专家权重，该优化问题可转化为凸二次规划。

Result: 该方法在各种合成和真实世界目标分布上进行了评估，证明了其有效性。理论分析表明权重更新以指数速度收敛到接近最优的专家权重。

Conclusion: 提出的加权乘积专家分布族为黑盒变分推断提供了一种高表达能力且易于处理的变分分布选择，能够有效建模复杂的目标分布特征。

Abstract: We introduce a highly expressive yet distinctly tractable family for
black-box variational inference (BBVI). Each member of this family is a
weighted product of experts (PoE), and each weighted expert in the product is
proportional to a multivariate $t$-distribution. These products of experts can
model distributions with skew, heavy tails, and multiple modes, but to use them
for BBVI, we must be able to sample from their densities. We show how to do
this by reformulating these products of experts as latent variable models with
auxiliary Dirichlet random variables. These Dirichlet variables emerge from a
Feynman identity, originally developed for loop integrals in quantum field
theory, that expresses the product of multiple fractions (or in our case,
$t$-distributions) as an integral over the simplex. We leverage this simplicial
latent space to draw weighted samples from these products of experts -- samples
which BBVI then uses to find the PoE that best approximates a target density.
Given a collection of experts, we derive an iterative procedure to optimize the
exponents that determine their geometric weighting in the PoE. At each
iteration, this procedure minimizes a regularized Fisher divergence to match
the scores of the variational and target densities at a batch of samples drawn
from the current approximation. This minimization reduces to a convex quadratic
program, and we prove under general conditions that these updates converge
exponentially fast to a near-optimal weighting of experts. We conclude by
evaluating this approach on a variety of synthetic and real-world target
distributions.

</details>


### [20] [Multimodal Datasets with Controllable Mutual Information](https://arxiv.org/abs/2510.21686)
*Raheem Karim Hashmani,Garrett W. Merz,Helen Qu,Mariel Pettee,Kyle Cranmer*

Main category: stat.ML

TL;DR: 提出了一个生成高度多模态数据集的框架，能够明确计算模态间的互信息，用于构建基准数据集以系统研究互信息估计器和多模态自监督学习技术。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够明确计算模态间互信息的多模态数据集，这限制了互信息估计器和多模态自监督学习方法的系统评估。

Method: 使用基于流的生成模型和结构化因果框架生成相关的潜变量，从而构建具有已知互信息的现实数据集。

Result: 开发了一个能够生成具有可计算互信息的多模态数据集的框架，为系统研究提供了基准测试平台。

Conclusion: 该框架为互信息估计器和多模态自监督学习技术的评估提供了重要的基准数据集和系统研究工具。

Abstract: We introduce a framework for generating highly multimodal datasets with
explicitly calculable mutual information between modalities. This enables the
construction of benchmark datasets that provide a novel testbed for systematic
studies of mutual information estimators and multimodal self-supervised
learning techniques. Our framework constructs realistic datasets with known
mutual information using a flow-based generative model and a structured causal
framework for generating correlated latent variables.

</details>
