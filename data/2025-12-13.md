<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 13]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.ML](#stat.ML) [Total: 8]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Peace Sells, But Whose Songs Connect? Bayesian Multilayer Network Analysis of the Big 4 of Thrash Metal](https://arxiv.org/abs/2512.10254)
*Juan Sosa,Erika Martínez,Danna L. Cruz-Reyes*

Main category: stat.ME

TL;DR: 提出贝叶斯多层歌曲相似性网络框架，应用于四大激流金属乐队完整录音室作品，通过音频特征构建多层网络，比较不同模型规格，发现随机区块模型具有最佳预测性能


<details>
  <summary>Details</summary>
Motivation: 研究如何将高维音频和文本特征组织成连贯、有音乐意义的模式，通过贝叶斯多层网络模型分析歌曲相似性

Method: 从原始音频构建四个特征层（响度、亮度、调性、节奏），增强歌曲外生信息，表示为k最近邻图，拟合分层概率模型家族，包含全局和层特定基线、节点和层特定社交性效应、二元协变量以及潜在结构（双线性、距离基和随机区块社区）

Result: 在所有乐队中，最丰富的随机区块规格获得最佳预测性能和后验预测拟合，揭示稀疏但有结构的连接性、可解释的协变量效应（特别是专辑成员和时间接近性），以及跨越专辑和时代的潜在社区和枢纽

Conclusion: 贝叶斯多层网络模型可以帮助将高维音频和文本特征组织成连贯、有音乐意义的模式，为音乐分析提供统计框架

Abstract: We propose a Bayesian framework for multilayer song similarity networks and apply it to the complete studio discographies of the "Big 4" of thrash metal (Metallica, Slayer, Megadeth, Anthrax). Starting from raw audio, we construct four feature-specific layers (loudness, brightness, tonality, rhythm), augment them with song exogenous information, and represent each layer as a k-nearest neighbor graph. We then fit a family of hierarchical probit models with global and layer-specific baselines, node- and layer-specific sociability effects, dyadic covariates, and alternative forms of latent structure (bilinear, distance-based, and stochastic block communities), comparing increasingly flexible specifications using posterior predictive checks, discrimination and calibration metrics (AUC, Brier score, log-loss), and information criteria (DIC, WAIC). Across all bands, the richest stochastic block specification attains the best predictive performance and posterior predictive fit, while revealing sparse but structured connectivity, interpretable covariate effects (notably album membership and temporal proximity), and latent communities and hubs that cut across albums and eras. Taken together, these results illustrate how Bayesian multilayer network models can help organize high-dimensional audio and text features into coherent, musically meaningful patterns.

</details>


### [2] [A Primer on Bayesian Parameter Estimation and Model Selection for Battery Simulators](https://arxiv.org/abs/2512.10055)
*Yannick Kuhn,Masaki Adachi,Micha Philipp,David A. Howey,Birger Horstmann*

Main category: stat.ME

TL;DR: 论文提出两种新算法SOBER和BASQ，加速电池物理模型的贝叶斯推断，用于参数化和模型比较，解决数据可观测性、模型可识别性和数据驱动模型开发问题。


<details>
  <summary>Details</summary>
Motivation: 基于物理的电池建模在加速电池材料发现和性能评估方面具有潜力，但模型与实验数据对齐困难阻碍了其成功。贝叶斯方法能够以原则性方式结合先验假设和观测数据，改善数值条件，但需要更高效的算法。

Method: 引入两种新算法SOBER和BASQ到电池研究领域，这些算法显著加速贝叶斯推断过程，用于参数估计和模型比较。通过贝叶斯模型选择方法，同时处理数据可观测性、模型可识别性和数据驱动的模型开发问题。

Result: 提出的SOBER和BASQ算法大大提高了贝叶斯推断的效率，使电池物理模型的参数化和模型比较更加可行。贝叶斯模型选择方法能够有效解决数据可观测性和模型可识别性问题，为新型电池材料模型开发提供系统化框架。

Conclusion: 贝叶斯方法结合高效算法为电池物理建模提供了强大工具，能够加速模型参数化和比较过程，解决数据对齐难题。该方法特别适用于新型电池材料模型的开发，为电池研究提供了系统化的数据驱动建模框架。

Abstract: Physics-based battery modelling has emerged to accelerate battery materials discovery and performance assessment. Its success, however, is still hindered by difficulties in aligning models to experimental data. Bayesian approaches are a valuable tool to overcome these challenges, since they enable prior assumptions and observations to be combined in a principled manner that improves numerical conditioning. Here we introduce two new algorithms to the battery community, SOBER and BASQ, that greatly speed up Bayesian inference for parameterisation and model comparison. We showcase how Bayesian model selection allows us to tackle data observability, model identifiability, and data-informed model development together. We propose this approach for the search for battery models of novel materials.

</details>


### [3] [Incorporating Partial Adherence for Estimation of Dynamic Treatment Regimes](https://arxiv.org/abs/2512.10069)
*Chloe Si,David A. Stephens,Erica E. M. Moodie*

Main category: stat.ME

TL;DR: 提出两种新方法，通过灵活兼容机制放宽严格依从性标准，提高动态治疗方案的IPW估计效率


<details>
  <summary>Details</summary>
Motivation: 标准IPW估计器在动态治疗方案估计中存在方差不稳定和数据效率低的问题，主要原因是严格的二元依从性标准忽略了部分依从情况，导致大量数据被丢弃

Method: 提出两种新方法：通过灵活兼容机制放宽严格包含规则，允许部分依从数据参与估计，保持计算可处理性，易于集成到现有IPW工作流

Result: 理论分析表明两种估计器保持一致性同时获得更好的有限样本效率，模拟研究证实改进的稳定性，HIV治疗数据应用展示实际效用

Conclusion: 新方法通过处理部分依从性显著提高IPW估计效率，为动态治疗方案估计提供更稳定有效的工具

Abstract: Dynamic Treatment Regimes (DTRs) provide a systematic framework for optimizing sequential decision-making in chronic disease management, where therapies must adapt to patients' evolving clinical profiles. Inverse probability weighting (IPW) is a cornerstone methodology for estimating regime values from observational data due to its intuitive formulation and established theoretical properties, yet standard IPW estimators face significant limitations, including variance instability and data inefficiency. A fundamental but underexplored source of inefficiency lies in the strict binary adherence criterion that fails to account for partial adherence, thereby discarding substantial data from individuals with even minimal deviations from the target regime. We propose two novel methodologies that relax the strict inclusion rule through flexible compatibility mechanisms. Both methods provide computationally tractable alternatives that can be easily integrated into existing IPW workflows, offering more efficient approaches to DTR estimation. Theoretical analysis demonstrates that both estimators preserve consistency while achieving superior finite-sample efficiency compared to standard IPW, and comprehensive simulation studies confirm improved stability. We illustrate the practical utility of our methods through an application to HIV treatment data from the AIDS Clinical Trials Group Study 175 (ACTG175).

</details>


### [4] [Time-Averaged Drift Approximations are Inconsistent for Inference in Drift Diffusion Models](https://arxiv.org/abs/2512.10250)
*Sicheng Liu,Alexander Fengler,Michael J. Frank,Matthew T. Harrison*

Main category: stat.ME

TL;DR: TADA（时间平均漂移近似）作为DDM参数推断的便捷方法存在不一致性问题，无法收敛到真实漂移率，可能导致科学结论偏差


<details>
  <summary>Details</summary>
Motivation: DDM在计算神经科学等领域广泛应用，但当漂移率在试次内随时间变化且跨试次可变时，精确似然评估计算成本高，因此常用TADA作为计算方便的替代方法进行参数推断

Method: 1. 在布朗运动分段常数漂移的单侧上边界设置中，提供TADA不一致性的基本证明；2. 使用注意力DDM（aDDM）进行数值实验，展示TADA如何系统性地误估注意力在决策中的效应

Result: 证明TADA估计器是不一致的，不会收敛到真实漂移率；数值实验显示TADA会系统性地误估注意力效应，存在科学结论偏差风险

Conclusion: TADA等替代方法虽然计算方便，但存在不一致性问题，可能导致参数估计偏差，影响基于这些估计的科学结论可靠性

Abstract: Drift diffusion models (DDMs) have found widespread use in computational neuroscience and other fields. They model evidence accumulation in simple decision tasks as a stochastic process drifting towards a decision barrier. In models where the drift rate is both time-varying within a trial and variable across trials, the high computational cost for accurate likelihood evaluation has led to the common use of a computationally convenient surrogate for parameter inference, the time-averaged drift approximation (TADA). In each trial, the TADA assumes that the time-varying drift rate can be replaced by its temporal average throughout the trial. This approach enables fast parameter inference using analytical likelihood formulas for DDMs with constant drift. In this work, we show that such an estimator is inconsistent: it does not converge to the true drift, posing a risk of biasing scientific conclusions drawn from parameter estimates produced by TADA and similar surrogates. We provide an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a Brownian motion with piecewise constant drift hitting a one-sided upper boundary. Furthermore, we conduct numerical examples with an attentional DDM (aDDM) to show that the use of TADA systematically misestimates the effect of attention in decision making.

</details>


### [5] [Semiparametric rank-based regression models as robust alternatives to parametric mean-based counterparts for censored responses under detection-limit](https://arxiv.org/abs/2512.10212)
*Y. Xu,S. Tu L. Shao,T. Lin,X. M. Tu*

Main category: stat.ME

TL;DR: 该论文研究了检测限下删失数据的半参数秩回归模型，相比传统参数方法（如Tobit模型）在分布误设时更稳健。


<details>
  <summary>Details</summary>
Motivation: 在生物医学和环境研究中，检测限导致协变量或结果被删失。传统方法如完整案例分析、单值替代和参数Tobit模型要么效率低下，要么对分布误设敏感。

Method: 提出基于加速失效时间（AFT）类型的半参数秩回归模型，使用秩估计方程获得一致的斜率估计，无需指定误差分布。开发了统一的模拟框架，生成左删失和右删失数据，涵盖正态、Weibull和对数正态误差结构。

Result: 参数模型仅在正确设定时表现良好，而半参数秩AFT估计量即使在重度删失和分布误设下也能保持接近无偏的协变量效应和稳定的精度。

Conclusion: 当误差分布不确定时，半参数秩回归是处理检测限删失回归的实用默认方法。

Abstract: Detection limits are common in biomedical and environmental studies, where key covariates or outcomes are censored below an assay-specific threshold. Standard approaches such as complete-case analysis, single-value substitution, and parametric Tobit-type models are either inefficient or sensitive to distributional misspecification.
  We study semiparametric rank-based regression models as robust alternatives to parametric mean-based counterparts for censored responses under detection limits. Our focus is on accelerated failure time (AFT) type formulations, where rank-based estimating equations yield consistent slope estimates without specifying the error distribution. We develop a unifying simulation framework that generates left- and right-censored data under several data-generating mechanisms, including normal, Weibull, and log-normal error structures, with detection limits or administrative censoring calibrated to target censoring rates between 10\% and 60\%.
  Across scenarios, we compare semiparametric AFT estimators with parametric Weibull AFT, Tobit, and Cox proportional hazards models in terms of bias, empirical variability, and relative efficiency. Numerical results show that parametric models perform well only under correct specification, whereas rank-based semiparametric AFT estimators maintain near-unbiased covariate effects and stable precision even under heavy censoring and distributional misspecification. These findings support semiparametric rank-based regression as a practical default for censored regression with detection limits when the error distribution is uncertain.
  Keywords: Semiparametric models, Estimating equations, Left censoring, Right censoring, Tobit regression, Efficiency

</details>


### [6] [A Bayesian Two-Sample Mean Test for High-Dimensional Data](https://arxiv.org/abs/2512.10537)
*Daojiang He,Suren Xu,Jing Zhou*

Main category: stat.ME

TL;DR: 提出基于贝叶斯因子的两样本均值检验方法，适用于高维数据（p随n线性增长），具有渐近正态性和良好性能


<details>
  <summary>Details</summary>
Motivation: 针对高维数据（p随n线性增长）的两样本均值检验问题，需要开发在异方差、小样本情况下仍能保持稳健性的检验方法

Method: 基于贝叶斯因子构建检验统计量，使用无信息先验，建立统计量的渐近正态性理论

Result: 模拟显示该方法在异方差、小样本情况下表现优异，能有效检测稀疏和非稀疏均值差异，控制I类错误率，对分布误设保持稳健

Conclusion: 提出的贝叶斯因子检验在高维数据中具有理论保证和实际应用价值，特别适用于小样本和异方差场景

Abstract: We propose a two-sample Bayesian mean test based on the Bayes factor with non-informative priors, specifically designed for scenarios where $p$ grows with $n$ with a linear rate $p/n \to c_1 \in (0, \infty)$. We establish the asymptotic normality of the test statistic and the asymptotic power. Through extensive simulations, we demonstrate that the proposed test performs competitively, particularly when the diagonal elements have heterogeneous variances and for small sample sizes. Furthermore, our test remains robust under distribution misspecification. The proposed method not only effectively detects both sparse and non-sparse differences in mean vectors but also maintains a well-controlled type I error rate, even in small-sample scenarios. We also demonstrate the performance of our proposed test using the \texttt{SRBCTs} dataset.

</details>


### [7] [Learning Time-Varying Correlation Networks with FDR Control via Time-Varying P-values](https://arxiv.org/abs/2512.10467)
*Bufan Li,Lujia Bai,Weichi Wu*

Main category: stat.ME

TL;DR: 提出一个系统性框架，用于控制从高维、非线性、非高斯、非平稳时间序列中学习时变相关网络的错误发现率，能处理均值突变点增加的情况。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列数据（如脑电图、金融数据）通常具有高维、非线性、非高斯、非平稳特性，且可能存在多个突变点。现有方法难以在这些复杂条件下控制错误发现率，需要开发新的统计框架。

Method: 提出基于自助法的稳健时变相关函数估计方法，生成依赖且时变的P值；建立高维高斯近似理论来统一近似时间和坐标维度上的P值；基于此开发Benjamini-Hochberg和Benjamini-Yekutieli程序来控制错误发现率。

Result: 方法在理论上保证了依赖且时变的P值能够实现统一的错误发现率控制，并通过模拟研究和真实数据（脑电图和金融时间序列）验证了其有效性。

Conclusion: 该框架为从复杂时间序列数据中学习时变相关网络提供了可靠的错误发现率控制方法，具有理论保证和实际应用价值。

Abstract: This paper presents a systematic framework for controlling false discovery rate in learning time-varying correlation networks from high-dimensional, non-linear, non-Gaussian and non-stationary time series with an increasing number of potential abrupt change points in means. We propose a bootstrap-assisted approach to derive dependent and time-varying P-values from a robust estimate of time-varying correlation functions, which are not sensitive to change points. Our procedure is based on a new high-dimensional Gaussian approximation result for the uniform approximation of P-values across time and different coordinates. Moreover, we establish theoretically guaranteed Benjamini--Hochberg and Benjamini--Yekutieli procedures for the dependent and time-varying P-values, which can achieve uniform false discovery rate control. The proposed methods are supported by rigorous mathematical proofs and simulation studies. We also illustrate the real-world application of our framework using both brain electroencephalogram and financial time series data.

</details>


### [8] [Measures and Models of Non-Monotonic Dependence](https://arxiv.org/abs/2512.10828)
*Alexander J. McNeil,Johanna G. Neslehova,Andrew D. Smith*

Main category: stat.ME

TL;DR: 提出广义Spearman相关系数，推广Spearman's rho到非单调依赖情况，基于单位区间上的平方可积函数，研究其性质、边界和统计推断，并展示在探索性分析和建模中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统Spearman's rho仅适用于单调依赖关系，需要一种能够度量非单调依赖的关联度量，以更全面地描述变量间的复杂依赖结构。

Method: 定义基于两个单位区间上平方可积函数的广义Spearman相关系数，特别研究分段连续严格单调函数情况；使用copula理论框架，引入均匀分布保持变换；推导边界并利用随机逆变换技术构造奇异copula；提出样本估计量并研究其渐近和小样本性质。

Result: 建立了广义Spearman相关系数的理论框架，推导了边界条件，构造了达到边界的奇异copula和参数copula；证明了样本估计量的统计性质；展示了在依赖结构分析、函数优化和概率密度建模中的应用。

Conclusion: 广义Spearman相关系数为度量非单调依赖提供了灵活框架，通过正交基函数扩展可实现函数优化，构造的copula模型能够捕捉各种非单调依赖模式，在探索性分析和统计建模中具有广泛应用前景。

Abstract: A margin-free measure of bivariate association generalizing Spearman's rho to the case of non-monotonic dependence is defined in terms of two square integrable functions on the unit interval. Properties of generalized Spearman correlation are investigated when the functions are piecewise continuous and strictly monotonic, with particular focus on the special cases where the functions are drawn from orthonormal bases defined by Legendre polynomials and cosine functions. For continuous random variables, generalized Spearman correlation is treated as a copula-based measure and shown to depend on a pair of uniform-distribution-preserving (udp) transformations determined by the underlying functions. Bounds for generalized Spearman correlation are derived and a novel technique referred to as stochastic inversion of udp transformations is used to construct singular copulas that attain the bounds and parametric copulas with densities that interpolate between the bounds and model different degrees of non-monotonic dependence. Sample analogues of generalized Spearman correlation are proposed and their asymptotic and small-sample properties are investigated. Potential applications of the theory are demonstrated including: exploratory analyses of the dependence structures of datasets and their symmetries; elicitation of functions maximizing generalized Spearman correlation via expansions in orthonormal basis functions; and construction of tractable probability densities to model a wide variety of non-monotonic dependencies.

</details>


### [9] [Long memory network time series](https://arxiv.org/abs/2512.10446)
*Chiara Boetti,Matthew A. Nunes,Marina I. Knight*

Main category: stat.ME

TL;DR: 本文提出了两种考虑网络结构的长记忆时间序列模型，通过利用网络拓扑降低参数维度，提高计算效率和估计稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统多变量长记忆时间序列模型（如向量自回归）参数随变量数量呈二次方增长，计算困难。许多实际数据伴随网络结构信息，但现有模型未能有效利用这种网络拓扑信息。

Method: 提出两种新的网络结构长记忆时间序列模型，通过适应网络设置的似然估计算法，利用网络固有的低维参数空间来捕捉动态关系。

Result: 模拟研究表明，所提估计方法比传统模型更稳定，能够处理传统模型因计算挑战而失败的数据场景。在环境科学和金融数据集上验证了模型的有效性。

Conclusion: 新模型不仅促进了图结构长记忆时间序列的分析，还通过利用网络拓扑信息提高了计算效率，解决了传统多变量长记忆模型在高维场景下的计算挑战。

Abstract: Many scientific areas, from computer science to the environmental sciences and finance, give rise to multivariate time series which exhibit long memory, or loosely put, a slow decay in their autocorrelation structure. Efficient modelling and estimation in such settings is key for a number of analysis tasks, such as accurate prediction. However, traditional approaches for modelling such data, for example long memory vector autoregressive processes, are challenging even in modest dimensions, as the number of parameters grows quadratically with the number of modelled variables. Additionally, in many practical data settings, the observed series is accompanied by a (possibly inferred) network that provides information about the presence or absence of between-component associations via the graph edge topology. This article proposes two new models for capturing the dynamics of long memory time series where a network is accounted for. Our approach not only facilitates the analysis of graph-structured long memory time series, but also improves computational efficiency over traditional multivariate long memory models by leveraging the inherent low-dimensional parameter space by adapting likelihood-based estimation algorithms to the network setting. Simulation studies show that our proposed estimation is more stable than traditional models, and is able to tackle data scenarios where current models fail due to computational challenges. While widely applicable, here we demonstrate the efficacy of our proposed models on datasets arising in environmental science and finance.

</details>


### [10] [Lasso-Ridge Refitting: A Two-Stage Estimator for High-Dimensional Linear Regression](https://arxiv.org/abs/2512.10632)
*Guo Liu*

Main category: stat.ME

TL;DR: 提出一种新的Lasso-Ridge方法，在保持Lasso优势的同时改善预测和估计性能


<details>
  <summary>Details</summary>
Motivation: Lasso在高维统计中很流行，但存在估计偏差和预测误差的问题，需要改进方法

Method: 提出新颖的Lasso-Ridge方法，结合两种正则化的优点

Result: 新方法在多种设置下都优于Lasso，包括Lasso在理论最优调参率时，同时保持预测一致性和变量选择可靠性

Conclusion: Lasso-Ridge方法是一个强大的高维线性回归工具，在预测和估计精度上都优于Lasso

Abstract: The least absolute shrinkage and selection operator (Lasso) is a popular method for high-dimensional statistics. However, it is known that the Lasso often has estimation bias and prediction error. To address such disadvantages, many alternatives and refitting strategies have been proposed and studied. This work introduces a novel Lasso--Ridge method. Our analysis indicates that the proposed estimator achieves improved prediction performance in a range of settings, including cases where the Lasso is tuned at its theoretical optimal rate \(\sqrt{\log(p)/n}\). Moreover, the proposed method retains several key advantages of the Lasso, such as prediction consistency and reliable variable selection under mild conditions. Through extensive simulations, we further demonstrate that our estimator outperforms the Lasso in both prediction and estimation accuracy, highlighting its potential as a powerful tool for high-dimensional linear regression.

</details>


### [11] [Revisiting the apparent discrepancy between the frequentist and Bayesian interpretation of an adaptive design](https://arxiv.org/abs/2512.10697)
*Simon Bang Kristensen,Erik Thorlund Parner*

Main category: stat.ME

TL;DR: 本文探讨了适应性临床试验中频率学派和贝叶斯学派分析方法的差异，特别关注组序贯设计中中期分析对估计偏差的影响，并指出贝叶斯后验并非完全不受数据驱动设计调整的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清适应性临床试验分析中的常见误解。虽然普遍认为频率学派分析需要考虑中期分析以避免I类错误膨胀，但较少意识到估计偏差问题。同时，广泛存在一种观点认为贝叶斯方法可以规避适应性设计带来的统计问题，因为后验分布被认为不受数据驱动调整的影响。本文旨在检验这一主张。

Method: 通过分析具有单次中期分析的简单试验，从频率学派和贝叶斯学派两个角度解释试验数据，重点关注估计问题。特别考察了参数空间的定义方式如何影响分析结果，包括感兴趣参数与设计参数之间是否存在关联。

Result: 研究发现，在传统的参数空间定义下（先验分布抽样参数，然后在给定参数下从抽样模型中抽取数据），中期分析确实影响频率学派的估计过程，但不影响贝叶斯后验，这形成了两种范式之间的"悖论"。然而，当采用替代的试验解释时，贝叶斯后验均值会包含一个校正项，该校正项与频率学派的条件偏差非常相似。

Conclusion: 结论是贝叶斯后验并非完全不受数据驱动设计调整的影响，其影响程度取决于如何定义相关试验的宇宙。在构建适应性设计的先验分布时，需要仔细考虑辅助试验参数的作用，特别是感兴趣参数与设计参数之间的潜在关联。

Abstract: It is generally appreciated that a frequentist analysis of a group sequential trial must in order to avoid inflating type I error account for the fact that one or more interim analyses were performed. It is also to a lesser extent realised that it may be necessary to account for the ensuing estimation bias. A group sequential design is an instance of adaptive clinical trials where a study may change its design dynamically as a reaction to the observed data. There is a widespread perception that one may circumvent the statistical issues associated with the analysis of an adaptive clinical trial by performing the analysis under a Bayesian paradigm. The root of the argument is that the Bayesian posterior is perceived as unaltered by the data-driven adaptations. We examine this claim by analysing a simple trial with a single interim analysis. We approach the interpretation of the trial data under both a frequentist and Bayesian paradigm with a focus on estimation. The conventional result is that the interim analysis impacts the estimation procedure under the frequentist paradigm, but not under the Bayesian paradigm, which may be seen as expressing a "paradox" between the two paradigms. We argue that this result however relies heavily on what one would define as the universe of relevant trials defined by first samples of the parameters from a prior distribution and then the data from a sampling model given the parameters. In particular, in this set of trials, whether a connection exists between the parameter of interest and design parameters. We show how an alternative interpretation of the trial yields a Bayesian posterior mean that corrects for the interim analysis with a term that closely resembles the frequentist conditional bias. We conclude that the role of auxiliary trial parameters needs to be carefully considered when constructing a prior in an adaptive design.

</details>


### [12] [Dynamic sparse graphs with overlapping communities](https://arxiv.org/abs/2512.10717)
*Antreas Laos,Xenia Miscouridou,Francesca Panero*

Main category: stat.ME

TL;DR: 提出基于贝叶斯非参数框架的动态重叠社区检测方法，适用于稀疏、幂律分布网络，通过完全随机测度和隐马尔可夫过程建模节点隶属度的时间演化。


<details>
  <summary>Details</summary>
Motivation: 动态社区检测需要跟踪网络中节点群组的演化、合并与解散。现有方法在处理稀疏网络、幂律度分布和动态重叠社区结构方面存在局限。

Method: 使用贝叶斯非参数框架，将图表示为平面上的可交换点过程。基于完全随机测度向量和隐马尔可夫过程建模时间演化的节点隶属度，将现有重叠块模型推广到稀疏和尺度无关机制。

Result: 模型具有稀疏性和幂律行为的渐近性质，通过近似推理程序进行验证，能够在真实世界网络中揭示可解释的社区演化轨迹。

Conclusion: 该统计框架为动态重叠社区检测提供了灵活且可解释的方法，特别适用于稀疏、幂律分布的网络，能够有效追踪社区的时间演化。

Abstract: Dynamic community detection in networks addresses the challenge of tracking how groups of interconnected nodes evolve, merge, and dissolve within time-evolving networks. Here, we propose a novel statistical framework for sparse networks with power-law degree distribution and dynamic overlapping community structure. Using a Bayesian Nonparametric framework, we build on the idea to represent the graph as an exchangeable point process on the plane. We base the model construction on vectors of completely random measures and a latent Markov process for the time-evolving node affiliations. This construction provides a flexible and interpretable approach to model dynamic communities, naturally generalizing existing overlapping block models to the sparse and scale-free regimes. We provide the asymptotic properties of the model concerning sparsity and power-law behavior and propose inference through an approximate procedure which we validate empirically. We show how the model can uncover interpretable community trajectories in a real-world network.

</details>


### [13] [Identifiable factor analysis for mixed continuous and binary variables based on the Gaussian-Grassmann distribution](https://arxiv.org/abs/2512.10804)
*Takashi Arai*

Main category: stat.ME

TL;DR: 提出一种适用于混合连续和二元观测变量的因子分析方法，基于高斯-格拉斯曼分布，能够解析处理潜在变量，并通过范数约束避免不当解，确保模型可识别性。


<details>
  <summary>Details</summary>
Motivation: 传统因子分析主要针对连续变量，对于混合连续和二元变量的分析需求日益增长。现有方法在处理这类混合类型数据时存在局限性，需要开发能够同时处理两种变量类型且具有解析可处理性的因子分析方法。

Method: 利用高斯-格拉斯曼分布建立混合类型随机变量的概率模型，通过解析边缘化处理潜在变量，得到观测变量的分布表达式。采用基于梯度的优化技术估计参数，并提出因子载荷矩阵行向量范数相等的约束条件来避免不当解。

Result: 提出的因子分析方法具有解析可处理性，能够有效估计模型参数。范数约束成功避免了最大似然因子分析中的不当解问题，并证明了模型在该约束下的可识别性。在真实和合成数据集上的数值验证表明，该方法比量化方法能更好地再现变量间的相关性。

Conclusion: 基于高斯-格拉斯曼分布的混合类型因子分析方法在理论上是可行的，通过适当的范数约束可以确保模型可识别性并避免不当解。该方法为分析混合连续和二元变量提供了一种有效的工具，在相关性再现方面优于传统的量化方法。

Abstract: We develop a factor analysis for mixed continuous and binary observed variables. To this end, we utilized a recently developed multivariate probability distribution for mixed-type random variables, the Gaussian-Grassmann distribution. In the proposed factor analysis, marginalization over latent variables can be performed analytically, yielding an analytical expression for the distribution of the observed variables. This analytical tractability allows model parameters to be estimated using standard gradient-based optimization techniques. We also address improper solutions associated with maximum likelihood factor analysis. We propose a prescription to avoid improper solutions by imposing a constraint that row vectors of the factor loading matrix have the same norm for all features. Then, we prove that the proposed factor analysis is identifiable under the norm constraint. We demonstrate the validity of this norm constraint prescription and numerically verified the model's identifiability using both real and synthetic datasets. We also compare the proposed model with quantification method and found that the proposed model achieves better reproducibility of correlations than the quantification method.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [14] [Classifying Metamorphic versus Single-Fold Proteins with Statistical Learning and AlphaFold2](https://arxiv.org/abs/2512.10066)
*Yongkai Chen,Samuel WK Wong,SC Kou*

Main category: stat.AP

TL;DR: 利用AlphaFold2生成构象集合，提取特征训练随机森林分类器，有效区分蛋白质是单构象还是多构象（变形蛋白）


<details>
  <summary>Details</summary>
Motivation: AlphaFold2虽然能准确预测蛋白质结构，但其"一个序列对应一个结构"的范式无法处理具有多种稳定构象的变形蛋白。区分蛋白质是单构象还是变形蛋白对实验和计算方法都是重要挑战。

Method: 重新利用AlphaFold2通过多序列比对采样方法生成构象集合，从这些集合中提取表征构象集合模态和结构分散性的综合特征集，在精心策划的已知变形蛋白和单构象蛋白基准数据集上训练随机森林分类器。

Result: 交叉验证平均AUC达到0.869，证明方法的有效性。应用于PDB中600个随机采样蛋白质，识别出多个潜在的变形蛋白候选，包括40S核糖体蛋白S30，其构象变化对抗菌防御的次要功能至关重要。

Conclusion: 通过结合AI驱动的蛋白质结构预测与统计学习，为发现变形蛋白提供了强大的新方法，加深了对它们在分子功能中作用的理解。

Abstract: The remarkable success of AlphaFold2 in providing accurate atomic-level prediction of protein structures from their amino acid sequence has transformed approaches to the protein folding problem. However, its core paradigm of mapping one sequence to one structure may only be appropriate for single-fold proteins with one stable conformation. Metamorphic proteins, which can adopt multiple distinct conformations, have conformational diversity that cannot be adequately modeled by AlphaFold2. Hence, classifying whether a given protein is metamorphic or single-fold remains a critical challenge for both laboratory experiments and computational methods. To address this challenge, we developed a novel classification framework by re-purposing AlphaFold2 to generate conformational ensembles via a multiple sequence alignment sampling method. From these ensembles, we extract a comprehensive set of features characterizing the conformational ensemble's modality and structural dispersion. A random forest classifier trained on a carefully curated benchmark dataset of known metamorphic and single-fold proteins achieves a mean AUC of 0.869 with cross-validation, demonstrating the effectiveness of our integrated approach. Furthermore, by applying our classifier to 600 randomly sampled proteins from the Protein Data Bank, we identified several potential metamorphic protein candidates -- including the 40S ribosomal protein S30, whose conformational change is crucial for its secondary function in antimicrobial defense. By combining AI-driven protein structure prediction with statistical learning, our work provides a powerful new approach for discovering metamorphic proteins and deepens our understanding of their role in their molecular function.

</details>


### [15] [Alpha Power Harris-G Family of Distributions: Properties and Application to Burr XII Distribution](https://arxiv.org/abs/2512.10276)
*Gbenga A. Olalude,Taiwo A. Ojurongbe,Olalekan A. Bello,Kehinde A. Bashiru,Kazeem A. Alamu*

Main category: stat.AP

TL;DR: 提出新的alpha power Harris-generalized (APHG)分布族，特别研究了以Burr XII为基线的APHBXII模型，该五参数模型在拟合生存和可靠性数据方面优于竞争模型。


<details>
  <summary>Details</summary>
Motivation: 为了建模生存和可靠性数据，需要更灵活的分布族。通过将Harris-G框架的两个形状参数整合到alpha power变换中，创建更具灵活性的分布类。

Method: 1) 提出APHG分布族；2) 以两参数Burr XII分布为基线，开发APHBXII模型；3) 推导模型的解析性质（矩、偏差、曲线、顺序统计量、熵等）；4) 使用最大似然法进行参数估计；5) 通过蒙特卡洛模拟评估估计器性能；6) 用三个真实寿命数据集进行实证评估。

Result: 1) 成功推导了APHBXII模型的多种解析性质；2) 蒙特卡洛模拟显示估计器在有限样本中表现良好；3) 三个真实数据集分析表明，五参数APHBXII模型在所有数据集上都提供了最优拟合，模型选择标准和拟合优度统计均支持这一结论。

Conclusion: APHG分布族特别是APHBXII模型为生存和可靠性数据分析提供了更灵活有效的工具，在实证应用中表现出优于现有竞争模型的拟合能力。

Abstract: This study introduces a new family of probability distributions, termed the alpha power Harris-generalized (APHG) family. The generator arises by incorporating two shape parameters from the Harris-G framework into the alpha power transformation, resulting in a more flexible class for modelling survival and reliability data. A special member of this family, obtained using the two-parameter Burr XII distribution as the baseline, is developed and examined in detail. Several analytical properties of the proposed alpha power Harris Burr XII (APHBXII) model are derived, which include closed-form expressions for its moments, mean and median deviations, Bonferroni and Lorenz curves, order statistics, and Renyi and Tsallis entropies. Parameter estimation is performed via maximum likelihood, and a Monte Carlo simulation study is carried out to assess the finite-sample performance of the estimators. In addition, three real lifetime datasets are analyzed to evaluate the empirical performance of the APHBXII distribution relative to four competing models. The results show that the five-parameter APHBXII model provides superior fit across all datasets, as supported by model-selection criteria and goodness-of-fit statistics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [16] [LxCIM: a new rank-based binary classifier performance metric invariant to local exchange of classes](https://arxiv.org/abs/2512.10053)
*Tiago Brogueira,Mário A. T. Figueiredo*

Main category: stat.ML

TL;DR: 提出LxCIM（局部类别交换不变性度量），一种新的二元分类评估指标，针对具有局部类别交换不变性（LxC）的问题，解决了AUROC的局限性，并应用于因果发现问题。


<details>
  <summary>Details</summary>
Motivation: 尽管二元分类是机器学习中最古老、最普遍和研究最多的问题之一，但模型性能评估指标受到的关注相对较少。AUROC长期以来一直是模型比较的标准选择，但对于具有局部类别交换不变性（LxC）的问题，AUROC并不理想。需要一种新的度量标准来解决这一局限性。

Method: 提出LxCIM（LxC-invariant metric），这是一种基于排序的、在局部类别交换下不变的度量。它直观、逻辑一致、始终可计算，并通过累积准确率-决策率曲线支持更详细的分析。LxCIM与AUROC、准确率和AUDRC有明确的理论联系。

Result: LxCIM不仅具有局部类别交换不变性，还可以被解释为AUROC的对称形式、准确率的基于排序的类比，或AUDRC更具代表性和可解释性的变体。在双变量因果发现问题（具有局部类别交换不变性）中，LxCIM直接应用并解决了现有指标的公认局限性。

Conclusion: LxCIM为具有局部类别交换不变性的二元分类问题提供了一种新的评估指标，弥补了AUROC的不足，在因果发现等领域具有直接应用价值，代码已公开可用。

Abstract: Binary classification is one of the oldest, most prevalent, and studied problems in machine learning. However, the metrics used to evaluate model performance have received comparatively little attention. The area under the receiver operating characteristic curve (AUROC) has long been a standard choice for model comparison. Despite its advantages, AUROC is not always ideal, particularly for problems that are invariant to local exchange of classes (LxC), a new form of metric invariance introduced in this work. To address this limitation, we propose LxCIM (LxC-invariant metric), which is not only rank-based and invariant under local exchange of classes, but also intuitive, logically consistent, and always computable, while enabling more detailed analysis through the cumulative accuracy-decision rate curve. Moreover, LxCIM exhibits clear theoretical connections to AUROC, accuracy, and the area under the accuracy-decision rate curve (AUDRC). These relationships allow for multiple complementary interpretations: as a symmetric form of AUROC, a rank-based analogue of accuracy, or a more representative and more interpretable variant of AUDRC. Finally, we demonstrate the direct applicability of LxCIM to the bivariate causal discovery problem (which exhibits invariance to local exchange of classes) and show how it addresses the acknowledged limitations of existing metrics used in this field. All code and implementation details are publicly available at github.com/tiagobrogueira/Causal-Discovery-In-Exchangeable-Data.

</details>


### [17] [The Interplay of Statistics and Noisy Optimization: Learning Linear Predictors with Random Data Weights](https://arxiv.org/abs/2512.10188)
*Gabriel Clara,Yazan Mash'al*

Main category: stat.ML

TL;DR: 本文分析了线性回归中带随机权重数据点的梯度下降，建立了一个统一框架来研究不同噪声对训练轨迹的影响，揭示了随机权重诱导的隐式正则化，并讨论了权重分布选择对优化和统计性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究随机权重梯度下降在机器学习中的影响，提供一个统一框架来分析各种噪声（包括随机梯度下降、重要性采样等）对训练轨迹的作用，理解权重分布如何影响优化过程和统计性能。

Method: 在线性回归模型中分析带随机权重数据点的梯度下降，使用通用权重分布框架，包括连续值权重分布。通过几何矩收缩分析平稳分布，推导一阶和二阶矩收敛的非渐近界，并与加权线性回归建立联系。

Result: 表征了随机权重诱导的隐式正则化，建立了与加权线性回归的连接，推导了收敛的非渐近界。发现某些权重分布虽然能实现快速收敛，但会导致较差的统计性能，揭示了优化速度与统计准确性之间的权衡。

Conclusion: 随机权重梯度下降提供了一个统一框架来分析各种噪声对训练的影响，揭示了权重分布选择对优化和统计性能的重要影响。研究结果表明需要在收敛速度和统计性能之间进行权衡，为实际应用中的权重选择提供了理论指导。

Abstract: We analyze gradient descent with randomly weighted data points in a linear regression model, under a generic weighting distribution. This includes various forms of stochastic gradient descent, importance sampling, but also extends to weighting distributions with arbitrary continuous values, thereby providing a unified framework to analyze the impact of various kinds of noise on the training trajectory. We characterize the implicit regularization induced through the random weighting, connect it with weighted linear regression, and derive non-asymptotic bounds for convergence in first and second moments. Leveraging geometric moment contraction, we also investigate the stationary distribution induced by the added noise. Based on these results, we discuss how specific choices of weighting distribution influence both the underlying optimization problem and statistical properties of the resulting estimator, as well as some examples for which weightings that lead to fast convergence cause bad statistical performance.

</details>


### [18] [Error Analysis of Generalized Langevin Equations with Approximated Memory Kernels](https://arxiv.org/abs/2512.10256)
*Quanjun Lang,Jianfeng Lu*

Main category: stat.ML

TL;DR: 该论文分析了具有记忆的随机动力系统中的预测误差，建立了记忆核衰减与轨迹差异衰减率之间的关系，并证明核估计误差定量地界定了轨迹预测误差。


<details>
  <summary>Details</summary>
Motivation: 研究随机动力系统中预测误差的理论基础，特别是在具有记忆的广义朗之万方程中，理解记忆核衰减特性如何影响轨迹预测精度，以及核估计误差如何定量影响预测性能。

Method: 采用随机Volterra方程框架分析广义朗之万方程，结合同步噪声耦合和Volterra比较定理，使用加权空间中的预解估计推导矩和扰动界，对二阶模型使用亚协调Lyapunov型距离证明收缩性和稳定性。

Result: 在强凸势下，轨迹差异的衰减率由记忆核的衰减决定，且定量地由核在加权范数下的估计误差界定；该框架适用于非平移不变核和白噪声强迫，明确建立了改进核估计与增强轨迹预测之间的联系。

Conclusion: 该研究为具有记忆的随机动力系统提供了预测误差的理论分析框架，建立了记忆核衰减特性、核估计精度与轨迹预测质量之间的定量关系，并通过数值实验验证了理论结果。

Abstract: We analyze prediction error in stochastic dynamical systems with memory, focusing on generalized Langevin equations (GLEs) formulated as stochastic Volterra equations. We establish that, under a strongly convex potential, trajectory discrepancies decay at a rate determined by the decay of the memory kernel and are quantitatively bounded by the estimation error of the kernel in a weighted norm. Our analysis integrates synchronized noise coupling with a Volterra comparison theorem, encompassing both subexponential and exponential kernel classes. For first-order models, we derive moment and perturbation bounds using resolvent estimates in weighted spaces. For second-order models with confining potentials, we prove contraction and stability under kernel perturbations using a hypocoercive Lyapunov-type distance. This framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction. Numerical examples validate these theoretical findings.

</details>


### [19] [Diffusion differentiable resampling](https://arxiv.org/abs/2512.10401)
*Jennifer Rosina Andersson,Zheng Zhao*

Main category: stat.ML

TL;DR: 提出了一种基于集成分数扩散模型的路径可微分重采样方法，用于序列蒙特卡罗方法，在随机滤波和参数估计中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 在序列蒙特卡罗方法（如粒子滤波）中，需要可微分的重采样方法，以便进行端到端的梯度优化和参数估计

Method: 基于集成分数扩散模型，提出了一种新的信息性重采样方法，该方法具有瞬时路径可微分性

Result: 证明了扩散重采样方法对重采样分布提供了一致性估计，实验表明在随机滤波和参数估计任务中优于现有最先进的可微分重采样方法

Conclusion: 提出的扩散重采样方法为序列蒙特卡罗中的可微分重采样提供了有效的解决方案，在滤波和参数估计任务中表现出优越性能

Abstract: This paper is concerned with differentiable resampling in the context of sequential Monte Carlo (e.g., particle filtering). We propose a new informative resampling method that is instantly pathwise differentiable, based on an ensemble score diffusion model. We prove that our diffusion resampling method provides a consistent estimate to the resampling distribution, and we show by experiments that it outperforms the state-of-the-art differentiable resampling methods when used for stochastic filtering and parameter estimation.

</details>


### [20] [Supervised Learning of Random Neural Architectures Structured by Latent Random Fields on Compact Boundaryless Multiply-Connected Manifolds](https://arxiv.org/abs/2512.10407)
*Christian Soize*

Main category: stat.ML

TL;DR: 提出基于流形上潜在各向异性高斯随机场的概率框架，用于建模非高斯随机输出的神经网络架构生成与监督学习。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络难以处理确定性输入下产生强非高斯随机输出的复杂不确定系统，需要建立几何感知的随机架构生成框架。

Method: 在紧致无边多连通流形上定义潜在各向异性高斯随机场，通过非齐次泊松过程采样神经元位置，基于测地线邻近性和局部场亲和性建立连接，通过条件采样确定突触权重。

Result: 建立了模型适定性、可测性等数学基础，分析了诱导随机映射的表达变异性，为几何驱动随机学习理论奠定基础。

Conclusion: 该框架为神经架构提供了几何感知的随机生成过程，将拓扑和权重统一于潜在随机场，为复杂不确定系统的监督学习开辟了新途径。

Abstract: This paper introduces a new probabilistic framework for supervised learning in neural systems. It is designed to model complex, uncertain systems whose random outputs are strongly non-Gaussian given deterministic inputs. The architecture itself is a random object stochastically generated by a latent anisotropic Gaussian random field defined on a compact, boundaryless, multiply-connected manifold. The goal is to establish a novel conceptual and mathematical framework in which neural architectures are realizations of a geometry-aware, field-driven generative process. Both the neural topology and synaptic weights emerge jointly from a latent random field. A reduced-order parameterization governs the spatial intensity of an inhomogeneous Poisson process on the manifold, from which neuron locations are sampled. Input and output neurons are identified via extremal evaluations of the latent field, while connectivity is established through geodesic proximity and local field affinity. Synaptic weights are conditionally sampled from the field realization, inducing stochastic output responses even for deterministic inputs. To ensure scalability, the architecture is sparsified via percentile-based diffusion masking, yielding geometry-aware sparse connectivity without ad hoc structural assumptions. Supervised learning is formulated as inference on the generative hyperparameters of the latent field, using a negative log-likelihood loss estimated through Monte Carlo sampling from single-observation-per-input datasets. The paper initiates a mathematical analysis of the model, establishing foundational properties such as well-posedness, measurability, and a preliminary analysis of the expressive variability of the induced stochastic mappings, which support its internal coherence and lay the groundwork for a broader theory of geometry-driven stochastic learning.

</details>


### [21] [Maximum Risk Minimization with Random Forests](https://arxiv.org/abs/2512.10445)
*Francesco Freni,Anya Fries,Linus Kühne,Markus Reichstein,Jonas Peters*

Main category: stat.ML

TL;DR: 提出基于最大风险最小化原则的随机森林变体，用于处理分布外泛化问题，在模拟和真实数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在分布外泛化问题中，训练环境和测试环境的数据分布不同，传统方法可能无法很好泛化。最大风险最小化原则旨在最小化跨环境的最大风险，但需要适用于随机森林等方法的实现。

Method: 提出了基于最大风险最小化原则的随机森林变体，包括计算高效算法和统计一致性证明。可以使用三种风险度量：均方误差、负奖励（与解释方差相关）和遗憾（相对于最佳预测器的超额风险）。

Result: 提供了计算高效算法和统计一致性证明，对于使用遗憾作为风险的MaxRM，证明了未见测试分布上的新颖样本外保证。在模拟和真实世界数据上评估了所提方法。

Conclusion: 成功开发了基于最大风险最小化原则的随机森林方法，为分布外泛化问题提供了有效的解决方案，具有理论保证和实证效果。

Abstract: We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data.

</details>


### [22] [Flexible Deep Neural Networks for Partially Linear Survival Data](https://arxiv.org/abs/2512.10570)
*Asaf Ben Arie,Malka Gorfine*

Main category: stat.ML

TL;DR: 提出FLEXI-Haz框架，结合参数线性部分和非参数DNN部分，在保持可解释性的同时处理生存数据中的复杂时间-协变量交互，不依赖比例风险假设。


<details>
  <summary>Details</summary>
Motivation: 现有基于DNN的部分线性Cox模型依赖比例风险假设，限制了灵活性。需要一种既能保持主要协变量可解释性，又能捕捉复杂时间-协变量交互，且不依赖比例风险假设的生存分析方法。

Method: 提出FLEXI-Haz框架，采用部分线性回归结构：参数线性部分处理主要协变量（保持可解释性），非参数DNN部分处理混杂变量与时间的复杂交互。不依赖比例风险假设。

Result: 理论保证：DNN组件在复合Holder类上达到极小极大最优收敛率，线性估计量具有根号n一致性、渐近正态性和半参数有效性。模拟和真实数据分析显示能准确估计线性效应。

Conclusion: FLEXI-Haz为生存分析提供了一个原则性、可解释的替代方法，不依赖比例风险假设，在保持主要协变量可解释性的同时能灵活建模复杂时间-协变量交互。

Abstract: We propose a flexible deep neural network (DNN) framework for modeling survival data within a partially linear regression structure. The approach preserves interpretability through a parametric linear component for covariates of primary interest, while a nonparametric DNN component captures complex time-covariate interactions among nuisance variables. We refer to the method as FLEXI-Haz, a flexible hazard model with a partially linear structure. In contrast to existing DNN approaches for partially linear Cox models, FLEXI-Haz does not rely on the proportional hazards assumption. We establish theoretical guarantees: the neural network component attains minimax-optimal convergence rates based on composite Holder classes, and the linear estimator is root-n consistent, asymptotically normal, and semiparametrically efficient. Extensive simulations and real-data analyses demonstrate that FLEXI-Haz provides accurate estimation of the linear effect, offering a principled and interpretable alternative to modern methods based on proportional hazards. Code for implementing FLEXI-Haz, as well as scripts for reproducing data analyses and simulations, is available at: https://github.com/AsafBanana/FLEXI-Haz

</details>


### [23] [Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling](https://arxiv.org/abs/2512.10873)
*Qitian Lu,Himanshu Sharma,Michael D. Shields,Lukáš Novák*

Main category: stat.ML

TL;DR: 该研究提出了两种增强PC²（物理信息多项式混沌展开）框架的方法：采用SULM优化求解器替代传统KKT求解器以降低计算成本，以及使用D-最优采样策略选择信息丰富的虚拟点，从而在高维参数空间中提高计算效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: PC²框架虽然通过嵌入物理约束提高了代理模型的物理可解释性和计算效率，但在高维参数空间、数据有限或训练数据不具代表性时，其性能和效率仍会下降。需要解决这些限制以提升PC²在高维不确定性量化任务中的适用性。

Method: 1. 采用SULM（直接更新拉格朗日乘子）作为替代传统KKT求解器的数值高效约束优化求解器，特别适用于高维度和需要大量虚拟点的导数边界条件问题；2. 利用D-最优采样策略选择信息丰富的虚拟点，平衡精度和效率。

Result: 增强后的PC²框架在代表性物理系统（常微分方程和偏微分方程）的数值示例中表现出比标准PC²更好的综合能力，特别适合高维不确定性量化任务，计算成本显著降低，稳定性和效率得到改善。

Conclusion: 通过整合SULM求解器和D-最优采样策略，成功增强了PC²框架在高维参数空间中的性能和效率，为解决高维不确定性量化问题提供了更有效的物理约束代理建模工具。

Abstract: Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks.

</details>
