<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 1]
- [stat.CO](#stat.CO) [Total: 3]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.ME](#stat.ME) [Total: 18]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Integrating Spatial and Temporal Effects in Seat-Belt Compliance Assessment with Telematics Data](https://arxiv.org/abs/2511.20712)
*Ashutosh Dumka,Raghupathi Kandiboina,Skylar Knickerbocker,Neal Hawkins,Jonathan Wood,Anuj Sharma*

Main category: stat.AP

TL;DR: 本研究利用远程信息处理数据生成爱荷华州2022年县级安全带使用率指标，采用包含时空随机效应的贝塔回归模型，发现同时考虑地理聚类和时间动态的模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统路边调查在空间上稀疏、时间上间断且成本高昂，无法捕捉动态行为模式或局部差异。远程信息处理数据提供了连续、高分辨率的驾驶员事件记录，能够更准确地反映安全带使用的复杂交互作用。

Method: 使用远程信息处理数据，应用包含空间和时间随机效应的贝塔回归模型套件，生成县级安全带合规指标。

Result: 包含空间和时间组件的模型优于仅包含空间或时间效应的模型。车辆行驶里程和人均收入是合规率的重要预测因子。时空效应显著，表明基于远程信息处理的细粒度数据显著提高了模型拟合和推断质量。

Conclusion: 将细粒度的远程信息处理数据整合到复杂的时空框架中显著改善了推断能力，为政策制定者提供了精确的见解，以进行有针对性的干预并推进交通安全研究。

Abstract: Seat belt use remains one of the most effective measures for reducing vehicle occupant fatalities and injuries. Yet, seat-belt compliance across different locales demands far more granular data than traditional, roadside surveys can provide. These surveys are spatially sparse, temporally intermittent, and costly to administer, often providing coarse-grained snapshots insufficient for capturing dynamic behavioral patterns or localized disparities. Telematics data emerges as a transformative alternative, offering continuous, high-resolution driver event records, such as seat belt latch status, across vast geographic areas. This granular and scalable data enables the application of advanced spatiotemporal models that more accurately reflect the complex interactions driving seatbelt use. This study utilizes telematics data to generate county-level seat belt compliance metrics for Iowa in 2022, employing a suite of beta-regression models that incorporate spatial and temporal random effects. The study findings demonstrate that models including both spatial and temporal components outperform those with spatial or temporal effects alone, underscoring the importance of jointly accounting for geographic clustering and temporal dynamics. Among explanatory variables, vehicle miles traveled (VMT) and per capita income emerge as significant predictors of compliance rates. The significant spatial and temporal effects highlight that telematics-based granular data substantially enhances model fit and inference quality. The results demonstrate that integrating granular telematics data within sophisticated spatiotemporal frameworks significantly improves inference, providing policymakers with precise insights for targeted interventions and advancing traffic safety research.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [2] [SVEMnet: An R package for Self-Validated Elastic-Net Ensembles and Multi-Response Optimization in Small-Sample Mixture--Process Experiments](https://arxiv.org/abs/2511.20968)
*Andrew T. Karl*

Main category: stat.CO

TL;DR: SVEMnet是一个R包，用于在小样本混合物-过程实验设计中拟合自验证集成模型，提供多响应优化功能，支持数值、分类和混合物因子。


<details>
  <summary>Details</summary>
Motivation: 解决小样本混合物-过程实验设计中模型拟合不稳定和优化困难的问题，特别是在接近插值边界时。

Method: 采用弹性网络和松弛弹性网络作为基础学习器，结合分数随机权重重采样方案和反相关训练/验证权重，通过验证加权的AIC和BIC标准选择惩罚项，并通过跨重复平均预测来稳定拟合。

Result: 在模拟的脂质纳米颗粒配方研究中展示了工具的有效性，并通过基于稀疏二次响应面的模拟实验验证了SVEMnet相对于重复交叉验证弹性网络基线的性能优势。

Conclusion: SVEMnet为小样本混合物-过程实验设计提供了稳定的模型拟合和多响应优化解决方案，在模拟实验中表现出优于传统方法的性能。

Abstract: SVEMnet is an R package for fitting Self-Validated Ensemble Models (SVEM) with elastic-net base learners and for performing multi-response optimization in small-sample mixture--process design-of-experiments (DOE) studies with numeric, categorical, and mixture factors. SVEMnet wraps elastic-net and relaxed elastic-net models for Gaussian and binomial responses from glmnet in a fractional random-weight (FRW) resampling scheme with anti-correlated train/validation weights; penalties are selected by validation-weighted AIC- and BIC-type criteria, and predictions are averaged across replicates to stabilize fits near the interpolation boundary. In addition to the core SVEM engine, the package provides deterministic high-order formula expansion, a permutation-based whole-model test heuristic, and a mixture-constrained random-search optimizer that combines Derringer--Suich desirability functions, bootstrap-based uncertainty summaries, and optional mean-level specification-limit probabilities to generate scored candidate tables and diverse exploitation and exploration medoids for sequential fit--score--run--refit workflows. A simulated lipid nanoparticle (LNP) formulation study illustrates these tools in a small-sample mixture--process DOE setting, and simulation experiments based on sparse quadratic response surfaces benchmark SVEMnet against repeated cross-validated elastic-net baselines.

</details>


### [3] [Nested ensemble Kalman filter for static parameter inference in nonlinear state-space models](https://arxiv.org/abs/2511.21497)
*Andrew Golightly,Sarah E. Heaps,Chris Sherlock,Laura E. Wadkin,Darren J. Wilkinson*

Main category: stat.CO

TL;DR: 该论文提出了一种结合重加权和移位方法的新算法，用集成卡尔曼滤波器替换SMC^2算法中的粒子滤波器，用于状态空间模型中的联合参数和状态推断。


<details>
  <summary>Details</summary>
Motivation: 传统集成卡尔曼滤波器在联合参数和状态推断时通常假设参数和状态都遵循线性高斯状态空间模型，这在实践中可能不合理。需要一种能够结合重加权和移位方法优势的新方法。

Method: 将SMC^2算法中的粒子滤波器替换为集成卡尔曼滤波器，参数粒子根据EnKF计算的最新观测数据似然进行加权，并通过针对EnKF下边际参数后验的重采样-移动步骤来保持粒子多样性。

Result: 提出了算法的扩展，包括在更新步骤中使用延迟接受核和纳入非线性观测模型。通过多个应用实例展示了该方法的效果。

Conclusion: 该方法成功结合了重加权和移位方法的优势，为状态空间模型中的联合参数和状态推断提供了一种有效的替代方案。

Abstract: The ensemble Kalman filter (EnKF) is a popular technique for performing inference in state-space models (SSMs), particularly when the dynamic process is high-dimensional. Unlike reweighting methods such as sequential Monte Carlo (SMC, i.e. particle filters), the EnKF leverages either the linear Gaussian structure of the SSM or an approximation thereof, to maintain diversity of the sampled latent states (the so-called ensemble members) via shifting-based updates. Joint parameter and state inference using an EnKF is typically achieved by augmenting the state vector with the static parameter. In this case, it is assumed that both parameters and states follow a linear Gaussian state-space model, which may be unreasonable in practice. In this paper, we combine the reweighting and shifting methods by replacing the particle filter used in the SMC^2 algorithm of Chopin et al., with the ensemble Kalman filter. Hence, parameter particles are weighted according to the estimated observed-data likelihood from the latest observation computed by the EnKF, and particle diversity is maintained via a resample-move step that targets the marginal parameter posterior under the EnKF. Extensions to the resulting algorithm are proposed, such as the use of a delayed acceptance kernel in the rejuvenation step and incorporation of nonlinear observation models. We illustrate the resulting methodology via several applications.

</details>


### [4] [Some aspects of robustness in modern Markov Chain Monte Carlo](https://arxiv.org/abs/2511.21563)
*Sam Power,Giorgos Vasdekis*

Main category: stat.CO

TL;DR: 本文综述了针对马尔可夫链蒙特卡洛(MCMC)方法在面对目标分布粗糙性和平坦性这两种病理特征时的鲁棒性算法研究。


<details>
  <summary>Details</summary>
Motivation: MCMC方法在实际应用中效率高度依赖于目标分布的良好行为，但当目标分布存在病理特征时，标准算法的性能会显著下降。本文旨在探讨如何设计能够应对这些病理特征的鲁棒MCMC算法。

Method: 通过系统性地分析两种主要病理特征（粗糙性和平坦性），回顾了现有文献中提出的各种算法解决方案，并提出了未来研究方向。

Result: 识别了两种严重影响标准局部MCMC算法性能的病理特征，并总结了针对每种特征的多种鲁棒算法策略。

Conclusion: 鲁棒MCMC算法研究是一个新兴且重要的方向，针对粗糙性和平坦性等病理特征的算法改进具有广阔的研究前景和应用价值。

Abstract: Markov Chain Monte Carlo (MCMC) is a flexible approach to approximate sampling from intractable probability distributions, with a rich theoretical foundation and comprising a wealth of exemplar algorithms. While the qualitative correctness of MCMC algorithms is often easy to ensure, their practical efficiency is contingent on the `target' distribution being reasonably well-behaved.
  In this work, we concern ourself with the scenario in which this good behaviour is called into question, reviewing an emerging line of work on `robust' MCMC algorithms which can perform acceptably even in the face of certain pathologies.
  We focus on two particular pathologies which, while simple, can already have dramatic effects on standard `local' algorithms. The first is roughness, whereby the target distribution varies so rapidly that the numerical stability of the algorithm is tenuous. The second is flatness, whereby the landscape of the target distribution is instead so barren and uninformative that one becomes lost in uninteresting parts of the state space. In each case, we formulate the pathology in concrete terms, review a range of proposed algorithmic remedies to the pathology, and outline promising directions for future research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [5] [Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification](https://arxiv.org/abs/2511.20960)
*Soumojit Das,Nairanjana Dasgupta,Prashanta Dutta*

Main category: stat.ML

TL;DR: 提出基于信息几何的神经网络概率输出校准框架，使用Fisher-Rao度量和ALR校准映射，为多分类问题提供理论保证，并构建基于几何可靠性的预测延迟机制。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在不确定时经常静默失败，需要可靠的概率校准和不确定性量化方法来支持关键决策。

Method: 将概率向量视为概率单纯形上的点，使用Fisher-Rao度量构建ALR校准映射，并基于几何距离定义可靠性分数和中性区域用于预测延迟。

Result: 在腺相关病毒分类任务中，两阶段框架（校准+可靠性延迟）捕获72.5%的错误，同时延迟34.5%的样本。校准估计器具有O_p(n^{-1/2})一致性。

Conclusion: 该工作连接了信息几何和统计学习，为需要严格验证的应用提供理论保证，几何校准的贡献在于理论基础而非实证优势。

Abstract: Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain. We develop a geometric framework for post-hoc calibration of neural network probability outputs, treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric. Our approach yields Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems (Proposition~1) while extending naturally to multi-class settings -- providing a principled generalization that existing methods lack. Complementing calibration, we define geometric reliability scores based on Fisher--Rao distance and construct neutral zones for principled deferral of uncertain predictions.
  Theoretical contributions include: (i) consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and (ii) tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework (calibration followed by reliability-based deferral) captures 72.5\% of errors while deferring 34.5\% of samples. Notably, this operational gain is achievable with any well-calibrated probability output; the contribution of geometric calibration lies in its theoretical foundations rather than empirical superiority over simpler alternatives. This work bridges information geometry and statistical learning, offering formal guarantees relevant to applications requiring rigorous validation.

</details>


### [6] [When Features Beat Noise: A Feature Selection Technique Through Noise-Based Hypothesis Testing](https://arxiv.org/abs/2511.20851)
*Mousam Sinha,Tirtha Sarathi Ghosh,Ridam Pal*

Main category: stat.ML

TL;DR: 提出了一种基于非参数bootstrap假设检验的新特征选择方法，通过引入随机噪声特征并与噪声特征最大重要性值比较，解决了传统方法缺乏理论依据和统计驱动停止标准的问题。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法存在计算成本高、缺乏统计驱动的停止标准、重要性评分显著性评估不足等问题，而常用的噪声特征启发式方法缺乏理论依据。

Method: 引入多个随机噪声特征，将每个特征的重要性与噪声特征最大重要性值进行比较，结合非参数bootstrap假设检验框架建立理论基础。

Result: 在模拟数据集上相比Boruta和Knockoff方法能更稳定地恢复有意义的信号；在真实数据集上表现优于Boruta、RFE和Extra Trees等方法。

Conclusion: 该方法为特征选择提供了一个稳健的算法，能够提取支持可靠推理、增强预测性能和高效计算的信息预测因子。

Abstract: Feature selection has remained a daunting challenge in machine learning and artificial intelligence, where increasingly complex, high-dimensional datasets demand principled strategies for isolating the most informative predictors. Despite widespread adoption, many established techniques suffer from notable limitations; some incur substantial computational cost, while others offer no definite statistical driven stopping criteria or assesses the significance of their importance scores. A common heuristic approach introduces multiple random noise features and retains all predictors ranked above the strongest noise feature. Although intuitive, this strategy lacks theoretical justification and depends heavily on heuristics. This paper proposes a novel feature selection method that addresses these limitations. Our approach introduces multiple random noise features and evaluates each feature's importance against the maximum importance value among these noise features incorporating a non-parametric bootstrap-based hypothesis testing framework to establish a solid theoretical foundation. We establish the conceptual soundness of our approach through statistical derivations that articulate the principles guiding the design of our algorithm. To evaluate its reliability, we generated simulated datasets under controlled statistical settings and benchmarked performance against Boruta and Knockoff-based methods, observing consistently stronger recovery of meaningful signal. As a demonstration of practical utility, we applied the technique across diverse real-world datasets, where it surpassed feature selection techniques including Boruta, RFE, and Extra Trees. Hence, the method emerges as a robust algorithm for principled feature selection, enabling the distillation of informative predictors that support reliable inference, enhanced predictive performance, and efficient computation.

</details>


### [7] [Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets](https://arxiv.org/abs/2511.20888)
*Arthur Jacot*

Main category: stat.ML

TL;DR: DNNs通过计算奥卡姆剃刀原理找到拟合数据的最简单算法，这解释了它们相对于传统统计方法的成功。研究发现HTMC区域中函数近似变得凸，定义了HTMC范数，并与ResNet参数范数建立了几乎匹配的夹逼关系。


<details>
  <summary>Details</summary>
Motivation: 解释深度神经网络为何在广泛领域比传统统计方法更成功，探究其内在的计算原理。

Method: 定义HTMC范数和ResNet参数范数，建立两者之间的夹逼关系，证明最小化ResNet范数等价于寻找节点数几乎最优的电路。

Result: 在HTMC区域（γ>2）中，函数近似变得凸，HTMC范数与ResNet范数存在几乎匹配的夹逼关系。

Conclusion: ResNets是HTMC区域中计算实函数的替代模型，通过最小化参数范数实现计算奥卡姆剃刀原理，找到几乎最优的简单算法。

Abstract: This paper argues that DNNs implement a computational Occam's razor -- finding the `simplest' algorithm that fits the data -- and that this could explain their incredible and wide-ranging success over more traditional statistical methods. We start with the discovery that the set of real-valued function $f$ that can be $ε$-approximated with a binary circuit of size at most $cε^{-γ}$ becomes convex in the `Harder than Monte Carlo' (HTMC) regime, when $γ>2$, allowing for the definition of a HTMC norm on functions. In parallel one can define a complexity measure on the parameters of a ResNets (a weighted $\ell_1$ norm of the parameters), which induce a `ResNet norm' on functions. The HTMC and ResNet norms can then be related by an almost matching sandwich bound. Thus minimizing this ResNet norm is equivalent to finding a circuit that fits the data with an almost minimal number of nodes (within a power of 2 of being optimal). ResNets thus appear as an alternative model for computation of real functions, better adapted to the HTMC regime and its convexity.

</details>


### [8] [Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities (II)](https://arxiv.org/abs/2511.21526)
*Alexandra Carpentier,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: 本文证明了在随机块模型中，当社区数量K≥√n时，社区恢复的计算障碍位于Chin等人提出的阈值处，并通过构造特定结构图案和计数方法实现了阈值以上的社区恢复。


<details>
  <summary>Details</summary>
Motivation: 解决随机块模型中社区恢复的计算复杂性理论问题，特别是在社区数量较多(K≥√n)的情况下，确定多项式时间社区恢复的可行性阈值。

Method: 1. 构造满足特定结构性质的图案族；2. 通过计数这些图案来证明在提议阈值以上可以实现社区恢复。

Result: 完成了K≥√n时随机块模型中社区恢复计算障碍的完整图景，证明了在提议阈值以上可以实现社区恢复。

Conclusion: 在中等稀疏机制下，最优算法与谱方法有根本性不同，本文结果完善了多社区设置下社区恢复的计算障碍理论。

Abstract: A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics.
  When $K \geq \sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \geq \sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes.
  The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\ 1- Constructing a family of motifs satisfying specific structural properties; and\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \geq \sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods.

</details>


### [9] [Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms](https://arxiv.org/abs/2511.21115)
*Lechen Feng,Haoran Li,Lucky Li,Xingqiu Zhao*

Main category: stat.ML

TL;DR: 本文研究了基于最小绝对偏差(LAD)回归的部分线性模型，使用深度神经网络参数化非参数项，建立了惩罚LAD估计问题，并分析了估计量的一致性、收敛速度和渐近正态性。


<details>
  <summary>Details</summary>
Motivation: 部分线性模型结合了参数和非参数建模的优势，但传统方法在处理高维复杂数据时存在局限性。深度神经网络能够有效捕捉复杂非线性关系，但将其与LAD回归结合面临正则化项非凸非光滑、网络结构随样本增长、oracle问题高维非凸不连续等理论挑战。

Method: 使用深度神经网络参数化非参数项，构建惩罚LAD估计问题。采用无限维变分分析和非光滑分析处理非凸非光滑正则化项，研究网络宽度、稀疏性和深度随样本增长的扩展策略，分析oracle问题及其连续松弛形式。

Result: 建立了估计量的一致性、收敛速度和渐近正态性。发现松弛形式在计算上显著更高效，反映了统计精度与计算可处理性之间的内在权衡。

Conclusion: 深度神经网络与LAD回归的结合在部分线性模型中具有理论可行性，但需要在统计精度和计算效率之间做出权衡。松弛方法提供了更实用的计算方案，为高维非参数建模提供了新思路。

Abstract: This paper investigates the partial linear model by Least Absolute Deviation (LAD) regression. We parameterize the nonparametric term using Deep Neural Networks (DNNs) and formulate a penalized LAD problem for estimation. Specifically, our model exhibits the following challenges. First, the regularization term can be nonconvex and nonsmooth, necessitating the introduction of infinite dimensional variational analysis and nonsmooth analysis into the asymptotic normality discussion. Second, our network must expand (in width, sparsity level and depth) as more samples are observed, thereby introducing additional difficulties for theoretical analysis. Third, the oracle of the proposed estimator is itself defined through a ultra high-dimensional, nonconvex, and discontinuous optimization problem, which already entails substantial computational and theoretical challenges. Under such the challenges, we establish the consistency, convergence rate, and asymptotic normality of the estimator. Furthermore, we analyze the oracle problem itself and its continuous relaxation. We study the convergence of a proximal subgradient method for both formulations, highlighting their structural differences lead to distinct computational subproblems along the iterations. In particular, the relaxed formulation admits significantly cheaper proximal updates, reflecting an inherent trade-off between statistical accuracy and computational tractability.

</details>


### [10] [Maxitive Donsker-Varadhan Formulation for Possibilistic Variational Inference](https://arxiv.org/abs/2511.21223)
*Jasraj Singh,Shelvia Wongso,Jeremie Houssineau,Badr-Eddine Chérief-Abdellatif*

Main category: stat.ML

TL;DR: 本文开发了可能性变分推理的原则性公式，并将其应用于指数族函数，揭示了可能性理论的独特数学结构。


<details>
  <summary>Details</summary>
Motivation: 变分推理是现代贝叶斯学习的基石，但其依赖于高维积分定义，需要近似技术。可能性理论作为不精确概率框架，可以直接建模认知不确定性，但需要重新思考变分推理的核心概念。

Method: 开发了可能性变分推理的原则性公式，并将其应用于指数族函数，与概率对应物进行比较。

Result: 建立了可能性变分推理的数学框架，揭示了可能性理论与概率理论在数学结构上的差异。

Conclusion: 成功将变分推理扩展到可能性理论框架，为处理稀疏或不精确信息提供了新的鲁棒性和可解释性工具。

Abstract: Variational inference (VI) is a cornerstone of modern Bayesian learning, enabling approximate inference in complex models that would otherwise be intractable. However, its formulation depends on expectations and divergences defined through high-dimensional integrals, often rendering analytical treatment impossible and necessitating heavy reliance on approximate learning and inference techniques. Possibility theory, an imprecise probability framework, allows to directly model epistemic uncertainty instead of leveraging subjective probabilities. While this framework provides robustness and interpretability under sparse or imprecise information, adapting VI to the possibilistic setting requires rethinking core concepts such as entropy and divergence, which presuppose additivity. In this work, we develop a principled formulation of possibilistic variational inference and apply it to a special class of exponential-family functions, highlighting parallels with their probabilistic counterparts and revealing the distinctive mathematical structures of possibility theory.

</details>


### [11] [On Evolution-Based Models for Experimentation Under Interference](https://arxiv.org/abs/2511.21675)
*Sadegh Shirani,Mohsen Bayati*

Main category: stat.ML

TL;DR: 该论文提出了一种基于演化的因果效应估计方法，通过分析多轮观察中结果对干预的响应变化来补偿缺失的网络信息，无需恢复确切的网络结构。


<details>
  <summary>Details</summary>
Motivation: 在联网系统中，干预会从一个单元溢出到其他单元，但驱动这些干扰结构的交互路径通常未被观察到。作者认为识别总体因果效应无需恢复确切的网络结构。

Method: 采用基于演化的方法，研究结果在多轮观察中如何响应干预而变化。使用暴露映射视角，给出结果经验分布遵循低维递归方程的公理化特征，并确定此类演化映射存在的最小结构条件。

Result: 该方法作为差分中的分布对应物，利用跨治疗场景的平行演化模式来估计反事实轨迹。治疗随机化不仅消除潜在混杂，还诱导从隐藏干扰通道进行隐式采样，实现异质溢出效应的一致学习。

Conclusion: 该方法可实例化为密集网络中的因果消息传递，并扩展到更一般的干扰结构，包括影响者网络。但强时间趋势或内生干扰会削弱识别能力。

Abstract: Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [12] [Efficient bayesian spatially varying coefficients modeling for censored data using the vecchia approximation](https://arxiv.org/abs/2511.21553)
*Yacine Mohamed Idir,Thomas Romary*

Main category: stat.ME

TL;DR: 提出了一种基于Vecchia近似的贝叶斯高斯过程建模框架，用于高效处理空间变系数模型，解决了传统Bayes-GP模型在大数据集或多协变量情况下的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯高斯过程模型在处理空间变系数时计算成本过高，特别是在大数据集或多协变量情况下，由于需要在每个MCMC迭代中重复计算密集协方差矩阵的逆矩阵。

Method: 使用Vecchia近似来降低计算复杂度，同时保持准确性，构建了高效的Bayes-GP建模框架。

Result: 该方法应用于法国图卢兹的土壤污染数据集，该数据集具有高度删失（三分之二观测值被删失）和空间聚类特征，能够有效捕捉空间变化效应并提供对空间异质性的有意义的见解。

Conclusion: Vecchia-based Bayes-GP模型即使在删失数据约束下，也能有效捕捉空间变化效应，为空间异质性分析提供了可行的解决方案。

Abstract: Spatially varying coefficients (SVC) models allow for marginal effects to be non-stationary over space and thus offer a higher degree of flexibility with respect to standard geostatistical models with external drift. At the same time, SVC models have the advantage that they are easily interpretable. They offer a flexible framework for understanding how the relationships between dependent and independent variables vary across space. The most common methods for modelling such data are the Geographically Weighted Regression (GWR) and Bayesian Gaussian Process (Bayes-GP). The Bayesian SVC model, which assumes that the coefficients follow Gaussian processes, provides a rigorous approach to account for spatial non-stationarity. However, the computational cost of Bayes-GP models can be prohibitively high when dealing with large datasets or/and when using a large number of covariates, due to the repeated inversion of dense covariance matrices required at each Markov chain Monte Carlo (MCMC) iteration. In this study, we propose an efficient Bayes-GP modeling framework leveraging the Vecchia approximation to reduce computational complexity while maintaining accuracy. The proposed method is applied to a challenging soil pollution data set in Toulouse, France, characterized by a high degree of censorship (two-thirds censored observations) and spatial clustering. Our results demonstrate the ability of the Vecchia-based Bayes-GP model to capture spatially varying effects and provide meaningful insights into spatial heterogeneity, even under the constraints of censored data.

</details>


### [13] [A Set of Rules for Model Validation](https://arxiv.org/abs/2511.20711)
*José Camacho*

Main category: stat.ME

TL;DR: 提出了一套通用的模型验证规则，帮助从业者创建可靠的验证计划并透明地报告结果


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型的验证是评估模型在目标群体中对新未见数据的泛化能力的过程，需要建立标准化的验证方法

Method: 设计了一套通用的模型验证规则，用于指导验证计划的制定和结果报告

Result: 这些规则可以帮助从业者确保验证策略足够实用，公开讨论验证策略的局限性，并报告清晰可比较的性能指标

Conclusion: 虽然没有完美的验证方案，但提出的规则能够为模型验证提供实用的指导框架

Abstract: The validation of a data-driven model is the process of assessing the model's ability to generalize to new, unseen data in the population of interest. This paper proposes a set of general rules for model validation. These rules are designed to help practitioners create reliable validation plans and report their results transparently. While no validation scheme is flawless, these rules can help practitioners ensure their strategy is sufficient for practical use, openly discuss any limitations of their validation strategy, and report clear, comparable performance metrics.

</details>


### [14] [Calibrated Bayes analysis of cluster-randomized trials](https://arxiv.org/abs/2511.20833)
*Ruyi Liu,Joshua L. Warren,Yuki Ohnishi,Donna Spiegelman,Liangyuan Hu,Fan Li*

Main category: stat.ME

TL;DR: 提出了一种校准的贝叶斯方法用于集群随机试验分析，针对集群平均处理效应和个体平均处理效应，在模型可能错误指定的情况下仍能保持频率学覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯分层模型在集群随机试验中关注基于模型的处理效应系数，但在模型错误指定和信息性集群大小情况下可能解释模糊。需要开发与估计量对齐的稳健分析方法。

Method: 开发校准贝叶斯程序，提出针对集群ATE和个体ATE的估计器，探索后验样本汇总策略，在错误指定工作模型下仍能实现频率学覆盖保证。

Result: 通过模拟验证了所提贝叶斯估计器在集群随机试验中的模型稳健性，并研究了协变量调整和更灵活的贝叶斯非参数工作模型的影响。

Conclusion: 该方法为集群随机试验提供了在模型可能错误指定情况下的稳健贝叶斯分析框架，特别适用于信息性集群大小场景。

Abstract: In cluster-randomized trials (CRTs), entire clusters of individuals are randomized to treatment, and outcomes within a cluster are typically correlated. While frequentist approaches are standard practice for CRT analysis, Bayesian methods have emerged as a strong alternative. Previous work has investigated the use of Bayesian hierarchical models for continuous, binary, and count outcomes in CRTs, but these approaches focus on model-based treatment effect coefficients as the target estimands, which may have ambiguous interpretation under model misspecification and informative cluster size. In this article, we introduce a calibrated Bayesian procedure for estimand-aligned analysis of CRTs even in the presence of potentially misspecified models. We propose estimators targeting both the cluster-average treatment effect (cluster-ATE) and individual-average treatment effect (individual-ATE), particularly in scenarios with informative cluster sizes. We additionally explore strategies for summarizing the posterior samples that can achieve the frequentist coverage guarantee even under working model misspecification. We provide simulation evidence to demonstrate the model-robustness property of the proposed Bayesian estimators in CRTs, and further investigate the impact of covariate adjustment as well as the use of more flexible Bayesian nonparametric working models.

</details>


### [15] [Informed Burn-In Decisions in RAR: Harmonizing Adaptivity and Inferential Precision Based on Study Setting](https://arxiv.org/abs/2511.21376)
*Lukas Pin,Stef Baas,Gianmarco Caruso,David S. Robertson,Sofía S. Villar*

Main category: stat.ME

TL;DR: 本文提出了第一个系统化框架来确定响应自适应随机化(RAR)中的燃尽期长度，解决了燃尽期选择缺乏指导原则的问题。该框架综合考虑样本量、问题难度和两个新指标，通过公式计算最佳燃尽期长度。


<details>
  <summary>Details</summary>
Motivation: RAR在改善患者获益方面具有潜力，但其效用依赖于正则化方法来缓解早期不稳定性和保持统计完整性。燃尽期长度的选择缺乏系统性指导，过短会导致估计偏差和I类错误膨胀，过长则会阻碍自适应设计的优势。

Method: 引入系统化框架，综合核心因素（总样本量、问题难度、反应性和期望最终分配误差）构建原则性公式，通过模拟研究验证框架有效性。

Result: 模拟研究表明，基于公式计算的燃尽期长度能成功稳定试验，找到平衡点缓解I类错误膨胀和均方误差，同时保持更高功效和患者获益的优势。

Conclusion: 该框架使研究人员从猜测转向系统化、可靠的方法，为RAR设计中的燃尽期选择提供了首个系统性解决方案。

Abstract: Response-Adaptive Randomization (RAR) is recognized for its potential to deliver improvements in patient benefit. However, the utility of RAR is contingent on regularization methods to mitigate early instability and preserve statistical integrity. A standard regularization approach is the ''burn-in'' period, an initial phase of equal randomization before treatment allocation adapts based on accrued data. The length of this burn-in is a critical design parameter, yet its selection remains unsystematic and improvised, as no established guideline exists. A poorly chosen length poses significant risks: one that is too short leads to high estimation bias and type-I error rate inflation, while one that is too long impedes the intended patient and power benefits of using adaptation. The challenge of selecting the burn-in generalizes to a fundamental question: what is the statistically appropriate timing for the first adaptation? We introduce the first systematic framework for determining burn-in length. This framework synthesizes core factors - total sample size, problem difficulty, and two novel metrics (reactivity and expected final allocation error) - into a single, principled formula. Simulation studies, grounded in real-world designs, demonstrate that lengths derived from our formula successfully stabilize the trial. The formula identifies a ''sweet spot'' that mitigates type-I error rate inflation and mean-squared error, preserving the advantages of higher power and patient benefit. This framework moves researchers from conjecture toward a systematic, reliable approach.

</details>


### [16] [Closure Term Estimation in Spatiotemporal Models of Dynamical Systems](https://arxiv.org/abs/2511.20869)
*Eric Crislip,Mohammad Khalil,Teresa Portone,Oksana Chkrebtii,Kyle Neal*

Main category: stat.ME

TL;DR: 提出了一种新颖的、计算高效的闭包建模方法，能够在时空域上建模和估计闭包项，提供不确定性量化，并且在系统状态观测稀疏或包含中等噪声水平时仍然有效。


<details>
  <summary>Details</summary>
Motivation: 现有的闭包建模方法通常计算成本高、缺乏不确定性量化，或者需要系统状态时间导数的无噪声观测。本文旨在解决这些局限性。

Method: 开发了一种计算高效的时空闭包建模方法，该方法能够处理稀疏或含噪声的系统状态观测，并提供不确定性量化。

Result: 通过使用Fisher-KPP反应-扩散方程和平流-扩散方程作为示例，在一维和二维空间中的数值实验证明了该方法的有效性。

Conclusion: 所提出的方法在闭包建模方面具有显著优势，包括计算效率、不确定性量化和对噪声观测的鲁棒性。

Abstract: Closure modeling - the statistical modeling of missing dynamics in the natural sciences and engineering - is a growing and active area of research. Existing methods for closure modeling are often computationally prohibitive, lack uncertainty quantification, or require noise-free observations of the temporal derivatives over the system state. We propose a novel, computationally efficient approach for the modeling and estimation of closure terms over the spatiotemporal domain that provides uncertainty quantification and is effective even when the observations of the system state are sparse or contain moderate levels of noise. The efficacy of our approach is demonstrated in both one and two spatial dimensions through numerical experiments using the Fisher-KPP reaction-diffusion equation and the advection-diffusion equation as exemplars.

</details>


### [17] [Data Privatization in Vertical Federated Learning with Client-wise Missing Problem](https://arxiv.org/abs/2511.20876)
*Huiyun Tang,Long Feng,Yang Li,Feifei Wang*

Main category: stat.ME

TL;DR: 提出基于高斯copula的垂直联邦学习数据隐私化框架，处理客户端特征缺失问题，引入VDADP隐私概念，提供隐私保护和效用保证。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习面临客户端特征缺失和隐私泄露风险，传统方法在隐私保护方面存在不足，需要一种既能处理缺失数据又能提供强隐私保护的方法。

Method: 使用高斯copula框架，提出去偏随机响应机制估计相关矩阵，非参数化边缘分布估计，开发VCDS、EVCDS和IEVCDS三种方法处理不同缺失机制。

Result: 建立了VDADP隐私概念，证明了隐私和效用保证，在GLM系数估计和变量选择中表现良好，模拟和实际数据应用验证了有效性。

Conclusion: 该框架能有效处理垂直联邦学习中的缺失数据和隐私保护问题，支持多种数据类型，具有理论保证和实际应用价值。

Abstract: Vertical Federated Learning (VFL) often suffers from client-wise missingness, where entire feature blocks from some clients are unobserved, and conventional approaches are vulnerable to privacy leakage. We propose a Gaussian copulabased framework for VFL data privatization under missingness constraints, which requires no prior specification of downstream analysis tasks and imposes no restriction on the number of analyses. To privately estimate copula parameters, we introduce a debiased randomized response mechanism for correlation matrix estimation from perturbed ranks, together with a nonparametric privatized marginal estimation that yields consistent CDFs even under MAR. The proposed methods comprise VCDS for MCAR data, EVCDS for MAR data, and IEVCDS, which iteratively refines copula parameters to mitigate MAR-induced bias. Notably, EVCDS and IEVCDS also apply under MCAR, and the framework accommodates mixed data types, including discrete variables. Theoretically, we introduce the notion of Vertical Distributed Attribute Differential Privacy (VDADP), tailored to the VFL setting, establish corresponding privacy and utility guarantees, and investigate the utility of privatized data for GLM coefficient estimation and variable selection. We further establish asymptotic properties including estimation and variable selection consistency for VFL-GLMs. Extensive simulations and a real-data application demonstrate the effectiveness of the proposed framework.

</details>


### [18] [On the Degrees of Freedom of some Lasso procedures](https://arxiv.org/abs/2511.21595)
*Mauro Bernardi,Antonio Canale,Marco Stefanucci*

Main category: stat.ME

TL;DR: 提出了自适应Lasso和自适应Group Lasso的有效自由度无偏估计器，填补了理论空白，支持更准确的模型选择和预测误差估计。


<details>
  <summary>Details</summary>
Motivation: 惩罚回归模型的有效自由度对模型评估和选择至关重要，但自适应Lasso等方法的有效自由度一直缺乏理论表征。

Method: 在Stein无偏风险估计框架下，推导出正交和非正交设计的有效自由度无偏估计器，表达式包含正则化参数、系数符号和最小二乘估计的影响项。

Result: 获得了自适应Lasso和自适应Group Lasso的有效自由度无偏估计器，理论性质得到证明。

Conclusion: 为理解自适应回归中的模型复杂性提供了严谨的理论基础，弥合了理论与实践之间的关键差距。

Abstract: The effective degrees of freedom of penalized regression models quantify the actual amount of information used to generate predictions, playing a pivotal role in model evaluation and selection. Although a closed-form estimator is available for the Lasso penalty, adaptive extensions of widely used penalized approaches, including the Adaptive Lasso and Adaptive Group Lasso, have remained without analogous theoretical characterization. This paper presents the first unbiased estimator of the effective degrees of freedom for these methods, along with their main theoretical properties, for both orthogonal and non-orthogonal designs, derived within Stein's unbiased risk estimation framework. The resulting expressions feature inflation terms influenced by the regularization parameter, coefficient signs, and least-squares estimates. These advances enable more accurate model selection criteria and unbiased prediction error estimates, illustrated through synthetic and real data. These contributions offer a rigorous theoretical foundation for understanding model complexity in adaptive regression, bridging a critical gap between theory and practice.

</details>


### [19] [Differentially Private Fisher Randomization Tests for Binary Outcomes](https://arxiv.org/abs/2511.20884)
*Qingyang Sun,Jerome P. Reiter*

Main category: stat.ME

TL;DR: 本文开发了针对二元结果的Fisher随机化检验的差分隐私版本，包括直接扰动方法和机制感知的贝叶斯去噪框架，旨在在保护敏感结果数据隐私的同时进行有效的因果推断。


<details>
  <summary>Details</summary>
Motivation: 在随机实验中，二元结果数据往往是敏感的（如健康测量），直接发布检验统计量或p值可能泄露个体信息。需要确保发布的输出满足差分隐私以限制信息泄露。

Method: 开发了多种差分隐私Fisher随机化检验方法：1）直接扰动方法，向检验统计量或p值注入校准噪声；2）机制感知的贝叶斯去噪框架，显式建模隐私机制；3）隐私约束下的决策程序，包括贝叶斯风险最优规则和频率校准显著性检验。

Result: 通过理论结果、模拟研究和ADAPTABLE临床试验应用，证明所提方法能在确保差分隐私保证的同时，实现有效且可解释的因果推断。

Conclusion: 提出的差分隐私Fisher随机化检验方法能够平衡隐私保护和因果推断的有效性，为敏感数据的因果分析提供了实用的隐私保护解决方案。

Abstract: Across many disciplines, causal inference often relies on randomized experiments with binary outcomes. In such experiments, the Fisher randomization test provides exact, assumption-free tests for causal effects. Sometimes the outcomes are sensitive and must be kept confidential, for example, when they comprise physical or mental health measurements. Releasing test statistics or p-values computed with the confidential outcomes can leak information about the individuals in the study. Those responsible for sharing the analysis results may wish to bound this information leakage, which they can do by ensuring the released outputs satisfy differential privacy. In this article, we develop and compare several differentially private versions of the Fisher randomization test for binary outcomes. Specifically, we consider direct perturbation approaches that inject calibrated noise into test statistics or p-values, as well as a mechanism-aware, Bayesian denoising framework that explicitly models the privacy mechanism. We further develop decision-making procedures under privacy constraints, including a Bayes risk-optimal rule and a frequentist-calibrated significance test. Through theoretical results, simulation studies, and an application to the ADAPTABLE clinical trial, we demonstrate that our methods can achieve valid and interpretable causal inference while ensuring the differential privacy guarantee.

</details>


### [20] [Robustness intervals for competing risks analysis with causes of failure missing not at random](https://arxiv.org/abs/2511.20980)
*Giorgos Bakoyannis,Aristofanis Rontogiannis,Ying Zhang,Wanzhu Tu,Ann Mwangi,Constantin T. Yiannoutsos*

Main category: stat.ME

TL;DR: 提出了一种针对竞争风险数据中缺失原因的新敏感性分析框架，处理非随机缺失(MNAR)情况，通过敏感性参数量化缺失与未观察原因的关系，构建置信带和稳健性区间。


<details>
  <summary>Details</summary>
Motivation: 竞争风险数据分析常因失败原因信息不完整或选择性缺失而复杂化，标准方法假设缺失是随机的(MAR)，但这一假设在观察性研究中通常不可检验且不合理。

Method: 使用比例原因特定风险模型，引入敏感性参数量化缺失与未观察原因的关系，回归系数作为该参数的函数估计，通过wild bootstrap程序构建同时置信带。

Result: 理论通过经验过程理论证明有效性，模拟研究验证性能。在HIV队列研究中应用显示，关于护理中断和死亡风险因素的关键发现在广泛的MNAR情景下保持稳健。

Conclusion: 该方法在MNAR失败原因情况下具有实用性，能够识别效应保持统计显著的MNAR情景范围，为竞争风险数据分析提供了有效的敏感性分析工具。

Abstract: Analysis of competing risks data is often complicated by the incomplete or selectively missing information on the cause of failure. Standard approaches typically assume that the cause of failure is missing at random (MAR), an assumption that is generally untestable and frequently implausible in observational studies. We propose a novel sensitivity analysis framework for the proportional cause-specific hazards model that accommodates missing-not-at-random (MNAR) scenarios. A sensitivity parameter is used to quantify the association between missingness and the unobserved cause of failure. Regression coefficients are estimated as functions of this parameter, and a simultaneous confidence band is constructed via a wild bootstrap procedure. This allows identification of a range of MNAR scenarios for which effects remain statistically significant; we refer to this range as a robustness interval. The validity of the proposed approach is justified both theoretically, via empirical process theory, and empirically, through simulation studies. We apply the method to the analysis of data from an HIV cohort study in sub-Saharan Africa, where a substantial proportion of causes of failure are missing and the MAR assumption is implausible. The analysis shows that key findings regarding risk factors for care interruption and mortality are robust across a broad spectrum of MNAR scenarios, underscoring the method's utility in situations with MNAR causes of failure.

</details>


### [21] [Two-stage Estimation for Causal Inference Involving a Semi-continuous Exposure](https://arxiv.org/abs/2511.20985)
*Xiaoya Wang,Richard J. Cook,Yeying Zhu,Tugba Akkaya-Hocagil,R. Colin Carter,Sandra W. Jacobson,Joseph L. Jacobson,Louise M. Ryan*

Main category: stat.ME

TL;DR: 提出了一个用于半连续暴露（有大量零值）的因果推断框架和两阶段估计策略，通过两部分倾向得分结构分离暴露状态和暴露水平的影响。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法主要针对二元和连续暴露，但对于有大量零值的半连续暴露缺乏专门框架，需要能够同时处理暴露状态（是否暴露）和暴露水平（暴露程度）的因果效应。

Method: 引入两部分倾向得分结构：一部分用于暴露状态（暴露vs未暴露），另一部分用于暴露者的暴露水平；提出两阶段估计程序，先估计暴露者的因果剂量-反应关系，再估计参考剂量下的暴露状态因果效应。

Result: 建立了估计量的一致性和渐近正态性，描述了倾向得分模型错误设定下的极限值；模拟研究评估了有限样本性能和稳健性；应用于产前酒精暴露与儿童认知研究，展示了方法在回答暴露状态和暴露强度科学问题中的应用。

Conclusion: 该框架为半连续暴露提供了统一的因果推断方法，能够分离暴露状态和暴露水平的因果效应，具有理论保证和实际应用价值。

Abstract: Methods for causal inference are well developed for binary and continuous exposures, but in many settings, the exposure has a substantial mass at zero-such exposures are called semi-continuous. We propose a general causal framework for such semi-continuous exposures, together with a novel two-stage estimation strategy. A two-part propensity structure is introduced for the semi-continuous exposure, with one component for exposure status (exposed vs unexposed) and another for the exposure level among those exposed, and incorporates both into a marginal structural model that disentangles the effects of exposure status and dose. The two-stage procedure sequentially targets the causal dose-response among exposed individuals and the causal effect of exposure status at a reference dose, allowing flexibility in the choice of propensity score methods in the second stage. We establish consistency and asymptotic normality for the resulting estimators, and characterise their limiting values under misspecification of the propensity score models. Simulation studies evaluate finite sample performance and robustness, and an application to a study of prenatal alcohol exposure and child cognition demonstrates how the proposed methods can be used to address a range of scientific questions about both exposure status and exposure intensity.

</details>


### [22] [Semiparametric Models for Practice Effects in Longitudinal Cognitive Trajectories: Application to an Aging Cohort Study](https://arxiv.org/abs/2511.21001)
*Y. Xu,T. Wu,A. Van Dyne,E. Lee,L. Eyler,X. Tu*

Main category: stat.ME

TL;DR: 开发了一个建模框架来估计重复测试中的练习效应，该框架通过基线对齐和独立估计访问特定练习效应，能更准确地区分年龄相关变化和学习收益。


<details>
  <summary>Details</summary>
Motivation: 重复测试中的练习效应会掩盖真实的认知纵向衰退，需要开发方法来独立估计练习效应，以避免对认知稳定性和群体差异的估计偏差。

Method: 使用线性混合效应模型估计受试者内相关性，模拟健康对照和精神分裂症患者的纵向轨迹，应用广义估计方程比较包含和不包含练习效应的模型表现。

Result: 忽略练习效应的模型会高估认知稳定性并减弱群体差异，包含访问特定练习效应的模型能更准确地恢复真实轨迹并区分年龄效应与学习收益。

Conclusion: 练习效应如果不建模会显著偏倚纵向估计，提出的基于对齐的GEE框架提供了估计练习效应的原则性方法，在模拟和真实场景中都能提高准确性。

Abstract: Background: True cognitive longitudinal decline can be obscured by repeated testing, which is called practice effects (PEs). We developed a modeling framework that aligns participants by baseline and estimates visit-specific PEs independently of age-related change.
  Method: Using real data ($N=175$), we estimated within-subject correlations via linear mixed-effects modeling and applied these parameters to simulate longitudinal trajectories for healthy controls (HC) and individuals with schizophrenia (SZ). Simulations incorporated aging, diagnostic differences, and cumulative PE indicators. Generalized estimating equations (GEEs) were fit with and without PEs to compare model performance.
  Results: Models that ignored PEs inflated estimates of cognitive stability and attenuated HC--SZ group differences. Including visit-specific PEs improved recovery of true trajectories and more accurately distinguished aging effects from learning-related gains. Interaction models further identified that PEs may differ by diagnosis or by age at baseline.
  Conclusion: Practice effects meaningfully bias longitudinal estimates if left unmodeled. The proposed alignment-based GEE framework provides a principled method to estimate PEs and improves accuracy in both simulated and real-world settings.
  Keywords: practice effects; repeat testing; serial testing; longitudinal testing; mild cognitive impairment; cognitive change.

</details>


### [23] [Zipf Distributions from Two-Stage Symbolic Processes: Stability Under Stochastic Lexical Filtering](https://arxiv.org/abs/2511.21060)
*Vladimir Berman*

Main category: stat.ME

TL;DR: 该研究通过几何机制解释Zipf定律，无需语言元素。全组合词模型(FCWM)从有限字母表生成单词，产生几何分布的长度。指数力相互作用产生幂律秩频曲线，由字母表大小和空白符号概率决定。


<details>
  <summary>Details</summary>
Motivation: Zipf定律在语言中缺乏明确起源，各领域存在争议。本研究旨在通过纯几何机制解释Zipf类行为，排除语言因素。

Method: 使用全组合词模型(FCWM)，从有限字母表形成单词，生成几何分布的词长。通过指数力相互作用产生幂律秩频曲线。

Result: 模拟结果支持预测，与英语、俄语和混合体裁数据匹配。符号模型表明Zipf型定律源于几何约束。

Conclusion: Zipf型定律源于几何约束，而非沟通效率。几何机制足以解释语言中的Zipf类行为。

Abstract: Zipf's law in language lacks a definitive origin, debated across fields. This study explains Zipf-like behavior using geometric mechanisms without linguistic elements. The Full Combinatorial Word Model (FCWM) forms words from a finite alphabet, generating a geometric distribution of word lengths. Interacting exponential forces yield a power-law rank-frequency curve, determined by alphabet size and blank symbol probability. Simulations support predictions, matching English, Russian, and mixed-genre data. The symbolic model suggests Zipf-type laws arise from geometric constraints, not communicative efficiency.

</details>


### [24] [Convex Mixed-Integer Programming for Causal Additive Models with Optimization and Statistical Guarantees](https://arxiv.org/abs/2511.21126)
*Xiaozhu Zhang,Nir Keret,Ali Shojaie,Armeen Taeb*

Main category: stat.ME

TL;DR: 提出一种基于凸混合整数规划的非线性结构方程模型DAG学习方法，使用组l0正则化惩罚边数，具有优化和统计保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么假设误差方差相等，要么仅限于线性结构方程模型，或依赖启发式程序，缺乏优化和统计保证。

Method: 通过基展开表达非线性函数，使用带组l0正则化的最大似然估计器，构建凸混合整数规划问题，利用分支定界法求解。

Result: 在模拟和真实数据分析中，该方法显著提高了图恢复性能。

Conclusion: 该方法能自然编码背景知识，在变量数随样本量增长时仍保持图恢复的一致性保证，相比现有方法具有明显优势。

Abstract: We study the problem of learning a directed acyclic graph from data generated according to an additive, non-linear structural equation model with Gaussian noise. We express each non-linear function through a basis expansion, and derive a maximum likelihood estimator with a group l0-regularization that penalizes the number of edges in the graph. The resulting estimator is formulated through a convex mixed-integer program, enabling the use of branch-and-bound methods to obtain a solution that is guaranteed to be accurate up to a pre-specified optimality gap. Our formulation can naturally encode background knowledge, such as the presence or absence of edges and partial order constraints among the variables. We establish consistency guarantees for our estimator in terms of graph recovery, even when the number of variables grows with the sample size. Additionally, by connecting the optimality guarantees with our statistical error bounds, we derive an early stopping criterion that allows terminating the branch-and-bound procedure while preserving consistency. Compared with existing approaches that either assume equal error variances, restrict to linear structural equation models, or rely on heuristic procedures, our method enjoys both optimization and statistical guarantees. Extensive simulations and real-data analysis show that the proposed method achieves markedly better graph recovery performance.

</details>


### [25] [Treatment effect estimation by comparing observed and predicted outcomes: theory and practical illustration in radiotherapy](https://arxiv.org/abs/2511.21266)
*Lotta M. Meijerink,Artuur M. Leeuwenberg,Jungyeon Choi,Bas B. L. Penning de Vries,Johannes A. Langendijk,Judith G. M. van Loon,Remi A. Nout,Karel G. M. Moons,Ewoud Schuit*

Main category: stat.ME

TL;DR: 本文探讨了在引入新治疗前开发的预测模型如何用于估计新治疗的效果，特别是在放疗中的模型临床评估方法，并阐明了使用该方法进行有效平均治疗效果估计所需的条件。


<details>
  <summary>Details</summary>
Motivation: 在引入新治疗之前开发的预测模型可以用来估计新治疗的效果，但需要明确使用这种方法进行有效估计的条件。

Method: 使用潜在结果框架和实际案例研究，分析了模型临床评估方法，比较新治疗下观察到的结果与如果这些患者接受标准治疗的预测结果。

Result: 明确了使用模型临床评估方法进行有效平均治疗效果估计所需的相关条件。

Conclusion: 通过潜在结果框架和案例研究，为使用预测模型评估新治疗效果提供了理论依据和实践指导。

Abstract: Prediction models developed before the introduction of a new treatment may be used to estimate treatment effects of newly introduced treatments. One approach, known as model-based clinical evaluation in radiotherapy, does this by comparing observed outcomes under a new treatment with predicted outcomes had these patients received the standard treatment. This article clarifies the relevant conditions needed for valid average treatment effect estimation using this approach, using the potential outcomes framework and a practical case study.

</details>


### [26] [Enterprise Profit Prediction Using Multiple Data Sources with Missing Values through Vertical Federated Learning](https://arxiv.org/abs/2511.21278)
*Huiyun Tang,Feifei Wang,Long Feng,Yang Li*

Main category: stat.ME

TL;DR: 提出垂直联邦期望最大化（VFEM）方法，解决中小企业利润预测中的数据隔离和缺失值问题，在保护数据隐私的同时提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 中小企业数据分散存储在不同机构，存在数据安全和隐私问题，且各机构数据存在不同程度的缺失值，传统集中分析方法受限。

Method: 开发垂直联邦期望最大化（VFEM）方法，将新EM算法嵌入联邦学习框架，处理复杂缺失模式，建立线性收敛率和统计推断框架。

Result: 通过模拟研究和实际中小企业利润预测验证，VFEM能有效解决数据隔离和缺失值问题，提高模型解释性和预测性能。

Conclusion: VFEM为处理数据隔离和缺失值提供了有前景的解决方案，有助于更好理解中小企业财务表现。

Abstract: Small and medium-sized enterprises (SMEs) play a crucial role in driving economic growth. Monitoring their financial performance and discovering relevant covariates are essential for risk assessment, business planning, and policy formulation. This paper focuses on predicting profits for SMEs. Two major challenges are faced in this study: 1) SMEs data are stored across different institutions, and centralized analysis is restricted due to data security concerns; 2) data from various institutions contain different levels of missing values, resulting in a complex missingness issue. To tackle these issues, we introduce an innovative approach named Vertical Federated Expectation Maximization (VFEM), designed for federated learning under a missing data scenario. We embed a new EM algorithm into VFEM to address complex missing patterns when full dataset access is unfeasible. Furthermore, we establish the linear convergence rate for the VFEM and establish a statistical inference framework, enabling covariates to influence assessment and enhancing model interpretability. Extensive simulation studies are conducted to validate its finite sample performance. Finally, we thoroughly investigate a real-life profit prediction problem for SMEs using VFEM. Our findings demonstrate that VFEM provides a promising solution for addressing data isolation and missing values, ultimately improving the understanding of SMEs' financial performance.

</details>


### [27] [Learning Across Experiments and Time: Tackling Heterogeneity in A/B Testing](https://arxiv.org/abs/2511.21282)
*Xinran Li*

Main category: stat.ME

TL;DR: 提出了一种局部经验贝叶斯框架来解决A/B测试中因时间演化和实验间异质性导致的估计噪声问题，通过构建时间感知和上下文感知的比较集来选择性借用信息，提高治疗效果估计的可靠性。


<details>
  <summary>Details</summary>
Motivation: A/B测试在数据驱动产品开发中至关重要，但由于实验周期短、早期停止和长尾指标积累缓慢等因素，治疗效果估计往往存在噪声，导致早期结论不可靠。需要开发能够同时适应时间演化和实验间异质性的信息池化策略。

Method: 提出局部经验贝叶斯框架，在实验时间线中构建定制化的比较集：在实验内部具有时间感知性以考虑非平稳性，在实验间具有上下文感知性以仅从可比较的对应实验中提取信息。然后从该集合中选择性地借用信息，产生稳定的治疗效果估计。

Result: 通过理论分析和实证评估表明，所提出的局部池化策略在减少方差的同时避免偏差，始终优于全局池化方法。

Conclusion: 该框架提高了在实践约束下A/B测试的可靠性，从而支持更及时和明智的决策制定。

Abstract: A/B testing plays a central role in data-driven product development, guiding launch decisions for new features and designs. However, treatment effect estimates are often noisy due to short horizons, early stopping, and slowly accumulating long-tail metrics, making early conclusions unreliable. A natural remedy is to pool information across related experiments, but naive pooling potentially fails: within experiments, treatment effects may evolve over time, so mixing early and late outcomes without accounting for nonstationarity induces bias; across experiments, heterogeneity in product, user population, or season dilutes the signal with unrelated noise. These issues highlight the need for pooling strategies that adapt to both temporal evolution and cross-experiment variability. To address these challenges, we propose a local empirical Bayes framework that adapts to both temporal and cross-experiment heterogeneity. Throughout an experiment's timeline, our method builds a tailored comparison set: time-aware within the experiment to respect nonstationarity, and context-aware across experiments to draw only from comparable counterparts. The estimator then borrows strength selectively from this set, producing stabilized treatment effect estimates that remain sensitive to both time dynamics and experimental context. Through theoretical analysis and empirical evaluation, we show that the proposed local pooling strategy consistently outperforms global pooling by reducing variance while avoiding bias. Our proposed framework enhances the reliability of A/B testing under practical constraints, thereby enabling more timely and informed decision-making.

</details>


### [28] [A Sensitivity Analysis Framework for Causal Inference Under Interference](https://arxiv.org/abs/2511.21534)
*Matvey Ortyashov,AmirEmad Ghassami*

Main category: stat.ME

TL;DR: 提出了一个基于权重的敏感性分析框架，用于评估忽略干扰效应所产生的系统性偏差，同时考虑未测量混杂和缺乏可迁移性等问题。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，干扰效应（一个单位的处理影响另一个单位的结果）很常见，但实践者往往缺乏调整干扰效应所需的数据。现有文献大多未考虑未测量混杂与干扰效应的结合问题。

Method: 开发了一个基于权重的敏感性分析框架，通过几个易于解释的敏感性参数来同时评估干扰、未测量混杂和缺乏可迁移性三个因素对系统性偏差的影响。

Result: 证明了干扰与未测量混杂的结合对因果推断构成显著挑战，该框架能够反映数据中各种直觉，帮助实践者评估这些问题的综合影响。

Conclusion: 提出的敏感性分析框架为实践者提供了评估忽略干扰效应所产生系统性偏差的工具，特别适用于同时存在干扰、未测量混杂和缺乏可迁移性的复杂因果推断场景。

Abstract: In many applications of causal inference, the treatment received by one unit may influence the outcome of another, a phenomenon referred to as interference. Although there are several frameworks for conducting causal inference in the presence of interference, practitioners often lack the data necessary to adjust for its effects. In this paper, we propose a weighting-based sensitivity analysis framework that can be used to assess the systematic bias arising from ignoring interference. Unlike most of the existing literature, we allow for the presence of unmeasured confounding, and show that the combination of interference and unmeasured confounding is a notable challenge to causal inference. We also study a third factor contributing to systematic bias: lack of transportability. Our framework enables practitioners to assess the impact of these three issues simultaneously through several easily interpretable sensitivity parameters that can reflect a wide range of intuitions about the data.

</details>


### [29] [StaRQR-K: False Discovery Rate Controlled Regional Quantile Regression](https://arxiv.org/abs/2511.21562)
*Sang Kyu Lee,Tongwu Zhang,Hyokyoung G. Hong,Haolei Weng*

Main category: stat.ME

TL;DR: StaRQR-K是一个用于超高维数据的稳定区域分位数回归框架，结合模型X knockoffs控制错误发现率，用于识别与结果分布特定分位数区域相关的基因组特征。


<details>
  <summary>Details</summary>
Motivation: 研究LINE-1在癌症中的活性需要超越均值回归的统计工具，特别是在超高维设置中量化基因组特征如何影响结果分布的不同部分。

Method: 结合高效的区域分位数独立筛选程序与基于缩尾的模型X knockoff过滤器，提供区域分位数回归的错误发现率控制。

Result: 模拟研究显示StaRQR-K实现了有效的FDR控制，且比现有方法具有显著更高的功效。在头颈癌队列应用中，揭示了CpG甲基化与LINE-1活性之间的分位数区域特异性关联。

Conclusion: StaRQR-K能够检测异质性和尾部敏感效应，改善样本外预测，并突出具有已知功能相关性的基因组区域。

Abstract: Quantifying how genomic features influence different parts of an outcome distribution requires statistical tools that go beyond mean regression, especially in ultrahigh-dimensional settings. Motivated by the study of LINE-1 activity in cancer, we propose StaRQR-K, a stabilized regional quantile regression framework with model-X knockoffs for false discovery rate control. StaRQR-K identifies CpG sites whose methylation levels are associated with specific quantile regions of an outcome, allowing detection of heterogeneous and tail-sensitive effects. The method combines an efficient regional quantile sure independence screening procedure with a winsorizing-based model-X knockoff filter, providing false discovery rate (FDR) control for regional quantile regression. Simulation studies show that StaRQR-K achieves valid FDR control and substantially higher power than existing approaches. In an application to The Cancer Genome Atlas head and neck cancer cohort, StaRQR-K reveals quantile-region-specific associations between CpG methylation and LINE-1 activity that improve out-of-sample prediction and highlight genomic regions with known functional relevance.

</details>
