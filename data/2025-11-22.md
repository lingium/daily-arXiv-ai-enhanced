<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 16]
- [stat.OT](#stat.OT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Models with Accelerated Failure Conditionals](https://arxiv.org/abs/2511.15769)
*Jared N. Lakhani*

Main category: stat.ME

TL;DR: 本文扩展了Arnold和Arvanitis(2020)的条件分布模型，将条件框架推广到具有单峰和偏态边际密度的分布族，超越了原模型中边际密度必须单调递减的限制。


<details>
  <summary>Details</summary>
Motivation: 原模型基于指数分布构建条件框架，虽然提出了进一步推广，但在推导有效联合生存函数的充要条件时遇到困难。本研究旨在实现这种推广，使模型能适应更广泛的分布特征。

Method: 使用生存函数构建条件框架，将条件分布扩展到具有单峰和偏态边际密度的分布族。模型通过闭式表达式完全指定其矩，并采用copula方法或Metropolis-Hastings算法进行模拟。

Result: 在两个包含单峰和偏态变量的数据集上的实证应用表明，具有灵活非单调边际密度的模型相比边际密度限制为单调衰减形式的模型具有更好的拟合效果。

Conclusion: 扩展后的条件分布模型能够更好地适应现实数据中常见的单峰和偏态特征，提供了比原模型更优越的拟合能力。

Abstract: Arnold and Arvanitis (2020) introduced a novel bivariate conditionally specified distribution, a distribution in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This novel conditioning regime was achieved through the use of survival functions, and the approach was termed the accelerated failure conditionals model. In their work, the conditioning framework was constructed using the exponential distribution. Although further generalization was proposed, challenges emerged in deriving the necessary and sufficient conditions for valid joint survival functions. The present study achieves such generalization, extending the conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, moving beyond distributional families whose marginal densities are non-increasing. The resulting models are fully specified through closed-form expressions for their moments, with simulations implemented using either a copula-based procedure or the Metropolis-Hastings algorithm. Empirical applications to two datasets, each featuring variables which are unimodal and skewed, demonstrate that the models with flexible, non-monotonic marginal densities yield a superior fit relative to those models with marginal densities restricted to monotonically decaying forms.

</details>


### [2] [Multidimensional scaling of two-mode three-way asymmetric dissimilarities: finding archetypal profiles and clustering](https://arxiv.org/abs/2511.15813)
*Aleix Alcacer,Rafael Benitez,Vicente J. Bolos,Irene Epifanio*

Main category: stat.ME

TL;DR: 本文扩展了h-plot方法，用于分析三向不对称邻近数据，提供统一的欧几里得表示、解析解、尺度不变性、计算效率和拟合优度评估，并能识别原型轮廓和聚类结构。


<details>
  <summary>Details</summary>
Motivation: 现有的多维标度方法主要处理对称邻近数据，而三向不对称邻近数据（捕捉多个场合的关系）研究不足，缺乏提取原型轮廓的方法。

Method: 将h-plot方法扩展到三向邻近数据，在对称和不对称、条件和无条件框架下工作，提供基于特征向量的解析解。

Result: 提出的方法具有直观可解释性、无局部极小值、尺度不变性、计算效率高，并能识别原型轮廓和聚类结构，在金融应用中表现良好。

Conclusion: 该方法为三向不对称邻近数据分析提供了有效的解决方案，所有数据和代码都已提供以确保可重复性。

Abstract: Multidimensional scaling visualizes dissimilarities among objects and reduces data dimensionality. While many methods address symmetric proximity data, asymmetric and especially three-way proximity data (capturing relationships across multiple occasions) remain underexplored. Recent developments, such as the h-plot, enable the analysis of asymmetric and non-reflexive relationships by embedding dissimilarities in a Euclidean space, allowing further techniques like archetypoid analysis to identify representative extreme profiles. However, no existing methods extract archetypal profiles from three-way asymmetric proximity data. This work extends the h-plot methodology to three-way proximity data under both symmetric and asymmetric, conditional and unconditional frameworks. The proposed approach offers several advantages: intuitive interpretability through a unified Euclidean representation; an explicit, eigenvector-based analytical solution free from local minima; scale invariance under linear transformations; computational efficiency for large matrices; and a straightforward goodness-of-fit evaluation. Furthermore, it enables the identification of archetypal profiles and clustering structures for three-way asymmetric proximities. Its performance is compared with existing models for multidimensional scaling and clustering, and illustrated through a financial application. All data and code are provided to facilitate reproducibility.

</details>


### [3] [Bayesian semiparametric modelling of biomarker variability in joint models](https://arxiv.org/abs/2511.15882)
*Sida Chen,Jessica K. Barrett,Marco Palma,Jianxin Pan,Brian D. M. Tom*

Main category: stat.ME

TL;DR: 该研究提出了两种新的贝叶斯半参数方法（P-splines和FPCA）来估计生物标志物轨迹的个体内变异性，并在联合模型中评估其与临床结局的关联，在囊性纤维化数据中首次发现肺功能变异性与死亡风险的显著正相关。


<details>
  <summary>Details</summary>
Motivation: 个体内变异性（WIV）在疾病风险评估中日益重要，但现有方法在联合模型框架下存在稳定性问题，需要更稳健的纵向轨迹建模方法。

Method: 提出了三种贝叶斯半参数方法：基于贝叶斯惩罚样条（P-splines）和函数主成分分析（FPCA）的新方法，在联合模型框架下估计轨迹基WIV并与现有方法比较。

Result: 模拟研究表明提出的P-spline和FPCA方法在推断和预测方面优于现有方法；在囊性纤维化数据中首次发现肺功能WIV与死亡风险呈显著正相关。

Conclusion: 提出的半参数方法提高了WIV估计的稳定性和准确性，为疾病风险评估提供了改进的工具，并指导方法选择和结果解释。

Abstract: There is growing interest in the role of within-individual variability (WIV) in biomarker trajectories for assessing disease risk and progression. A trajectory-based definition that has attracted recent attention characterises WIV as the curvature-based roughness of the latent biomarker trajectory (TB-WIV). To rigorously evaluate the association between TB-WIV and clinical outcomes and to perform dynamic risk prediction, joint models for longitudinal and time-to-event data (JM) are necessary. However, specifying the longitudinal trajectory is critical in this framework and poses methodological challenges. In this work, we investigate three Bayesian semiparametric approaches for longitudinal modelling and TB-WIV estimation within the JM framework to improve stability and accuracy over existing approaches. Two key methods are newly introduced: one based on Bayesian penalised splines (P-splines) and another on functional principal component analysis (FPCA). Using extensive simulation studies, we compare their performance under two important TB-WIV definitions against established approaches. Our results demonstrate overall inferential and predictive advantages of the proposed P-spline and FPCA-based approaches while also providing insights that guide method choice and interpretation of inference results. The proposed approaches are applied to data from the UK Cystic Fibrosis Registry, where, for the first time, we identify a significant positive association between lung function TB-WIV and mortality risk in patients with cystic fibrosis and demonstrate improved predictive performance for survival.

</details>


### [4] [Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies](https://arxiv.org/abs/2511.15896)
*Ying Jin,José Zubizarreta*

Main category: stat.ME

TL;DR: 提出了一种名为交叉平衡(cross-balancing)的方法，通过样本分割将特征构建误差与权重估计误差分离，解决了在利用结果数据构建平衡特征时保持有效推断的长期挑战。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断仅使用观察到的协变量构建相似组，但如何将可用的结果数据纳入研究设计同时保持有效推断是一个长期挑战。

Method: 交叉平衡方法使用样本分割技术，将特征构建过程与权重估计过程分离。该方法处理两种情况：学习函数特征和从高维字典中选择特征。

Result: 在温和且一般的条件下，交叉平衡产生了一致的、渐近正态的和有效的估计量。在学习函数情况下实现有限样本偏差减少，在变量选择情况下仅需要乘积条件。

Conclusion: 仔细使用结果信息可以显著改善估计和推断，同时保持可解释性，交叉平衡方法为解决这一长期挑战提供了有效解决方案。

Abstract: Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability.

</details>


### [5] [A Simple and Robust Multi-Fidelity Data Fusion Method for Effective Modeling of Citizen-Science Air Pollution Data](https://arxiv.org/abs/2511.15942)
*Camilla Andreozzi,Pietro Colombo,Philipp Otto*

Main category: stat.ME

TL;DR: 提出一种鲁棒的多保真度高斯过程，通过Huber损失函数整合稀疏的高质量参考监测器和密集但嘈杂的公民科学传感器数据，在保持协同克里金灵活性的同时提供有界参数影响。


<details>
  <summary>Details</summary>
Motivation: 解决传统高斯最大似然估计在低保真度数据污染下的性能退化问题，需要开发能够整合不同质量监测数据的鲁棒方法。

Method: 用全局Huber损失替换高保真度通道的高斯对数似然，应用于精度加权残差，实现对所有参数（包括跨保真度耦合）的有界影响。

Result: 蒙特卡洛实验显示鲁棒估计器在异常幅度和频率增加时保持稳定的MAE和RMSE，而高斯MLE迅速恶化。在汉堡PM2.5浓度实证研究中，该方法持续提高交叉验证预测精度并产生连贯的不确定性地图。

Conclusion: 该方法通过对角或低秩白化保持计算可扩展性，为整合不同质量的环境监测数据提供了有效的鲁棒框架。

Abstract: We propose a robust multi-fidelity Gaussian process for integrating sparse, high-quality reference monitors with dense but noisy citizen-science sensors. The approach replaces the Gaussian log-likelihood in the high-fidelity channel with a global Huber loss applied to precision-weighted residuals, yielding bounded influence on all parameters, including the cross-fidelity coupling, while retaining the flexibility of co-kriging. We establish attenuation and unbounded influence of the Gaussian maximum likelihood estimator under low-fidelity contamination and derive explicit finite bounds for the proposed estimator that clarify how whitening and mean-shift sensitivity determine robustness. Monte Carlo experiments with controlled contamination show that the robust estimator maintains stable MAE and RMSE as anomaly magnitude and frequency increase, whereas the Gaussian MLE deteriorates rapidly. In an empirical study of PM2.5 concentrations in Hamburg, combining UBA monitors with openSenseMap data, the method consistently improves cross-validated predictive accuracy and yields coherent uncertainty maps without relying on auxiliary covariates. The framework remains computationally scalable through diagonal or low-rank whitening and is fully reproducible with publicly available code.

</details>


### [6] [Small Area Estimation Methods for Multivariate Health and Demographic Outcomes using Complex Survey Data](https://arxiv.org/abs/2511.15917)
*Austin E Schumacher,Jon Wakefield*

Main category: stat.ME

TL;DR: 开发了基于复杂调查数据的多元共享成分模型，通过联合建模多个相关健康指标来提高小区域估计的准确性，相比单变量模型有显著改进。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家缺乏全面的健康数据，主要依赖家庭调查。现有方法多为单变量建模，通过多元建模可以跨相关结果借用信息，提高估计精度。

Method: 开发了区域级和单元级多元共享成分模型框架，使用复杂调查数据联合建模多个健康结果，通过模拟研究验证方法，并在肯尼亚2014年调查数据中应用。

Result: 在儿童身高-年龄和体重-年龄指标以及女性避孕使用三个类别的建模中，相比单变量和朴素多元建模方法，提出的模型产生了改进的估计结果。

Conclusion: 多元共享成分模型能够有效提高小区域健康估计的准确性，为针对性干预和健康政策提供更可靠的数据支持。

Abstract: Improving health in the most disadvantaged populations requires reliable estimates of health and demographic indicators to inform policy and interventions. Low- and middle-income countries with the largest burden of disease and disability tend to have the least comprehensive data, relying primarily on household surveys. Subnational estimates are increasingly used to inform targeted interventions and health policies. Producing reliable estimates from these data at fine geographical scales requires statistical modeling, and small area estimation models are commonly used in this context. Although most current methods model univariate outcomes, improved estimates may be attained by borrowing strength across related outcomes via multivariate modeling. In this paper, we develop classes of area- and unit-level multivariate shared component models using complex survey data. This framework jointly models multiple outcomes to improve accuracy of estimates compared to separately fitting univariate models. We conduct simulation studies to validate the methodology and use the proposed approach on survey data from Kenya in 2014; first, to jointly model height-for-age and weight-for-age in children, and second, to model three categories of contraceptive use in women. These models produce improved estimates compared to univariate and naive multivariate modeling approaches.

</details>


### [7] [Robust Estimation under Outcome Dependent Right Censoring in Huntington Disease: Estimators for Low and High Censoring Rates](https://arxiv.org/abs/2511.15929)
*Jesus E. Vazquez,Yanyuan Ma,Karen Marder,Tanya P. Garcia*

Main category: stat.ME

TL;DR: 开发了三种针对结果依赖性删失场景的一致估计器：两个增强逆概率加权(AIPW)估计器和一个最大似然估计器(MLE)，用于处理事件时间数据中因结果依赖的删失导致的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 在健康应用中，当删失依赖于结果时（如神经退行性疾病研究中退出可能与疾病严重程度相关），标准回归估计器会产生有偏估计。

Method: 开发了两种AIPW估计器和一种MLE估计器，建立了它们的渐近性质，并推导了考虑nuisance参数估计的稳健三明治方差估计器。

Result: 发现估计器的选择取决于删失率：MLE在低删失率下表现最佳，而AIPW估计器在高删失率下产生更低的偏差和更高的名义覆盖率。在亨廷顿病数据分析中，具有稳健矩阵的AIPW估计器提供了临床支持的估计，精度优于逆概率加权。

Conclusion: 研究结果为基于删失率的估计器选择提供了实用指导，AIPW估计器在处理结果依赖性删失问题时具有优势。

Abstract: Across health applications, researchers model outcomes as a function of time to an event, but the event time is right-censored for participants who exit the study or otherwise do not experience the event during follow-up. When censoring depends on the outcome-as in neurodegenerative disease studies where dropout is potentially related to disease severity-standard regression estimators produce biased estimates. We develop three consistent estimators for this outcome-dependent censoring setting: two augmented inverse probability weighted (AIPW) estimators and one maximum likelihood estimator (MLE). We establish their asymptotic properties and derive their robust sandwich variance estimators that account for nuisance parameter estimation. A key contribution is demonstrating that the choice of estimator to use depends on the censoring rate-the MLE performs best under low censoring rates, while the AIPW estimators yield lower bias and a higher nominal coverage under high censoring rates. We apply our estimators to Huntington disease data to characterize health decline leading up to mild cognitive impairment onset. The AIPW estimator with robustness matrix provided clinically-backed estimates with improved precision over inverse probability weighting, while MLE exhibited bias. Our results provide practical guidance for estimator selection based on censoring rate.

</details>


### [8] [Bayesian Semiparametric Causal Inference: Targeted Doubly Robust Estimation of Treatment Effects](https://arxiv.org/abs/2511.15904)
*Gözde Sert,Abhishek Chakrabortty,Anirban Bhattacharya*

Main category: stat.ME

TL;DR: 提出了一种半参数贝叶斯方法，用于在潜在结果框架下使用具有高维干扰参数的观测数据估计平均处理效应。该方法通过贝叶斯去偏程序纠正干扰估计产生的偏差，并基于摘要统计量而非完整数据进行目标建模。


<details>
  <summary>Details</summary>
Motivation: 在观测数据中估计因果效应时，高维干扰参数的存在会导致估计偏差。需要一种能够纠正这种偏差并保持统计效率的方法，同时确保在干扰模型设定错误时的稳健性。

Method: 结合贝叶斯去偏程序和样本分割技术，使用摘要统计量进行目标建模。通过加权观测值估计干扰偏差，并分层学习ATE，将干扰估计与目标参数推断分离。

Result: 在温和条件下，ATE的边际后验满足Bernstein-von Mises定理。当两个干扰模型都正确设定时达到渐近效率，当只有一个正确时仍保持一致性和稳健性，实现贝叶斯双重稳健性。模拟研究证实了理论结果。

Conclusion: 该方法在高维设置下实现了准确的点估计和名义覆盖率的可信区间，可扩展到其他因果估计量，为推进贝叶斯半参数推断提供了通用基础。

Abstract: We propose a semiparametric Bayesian methodology for estimating the average treatment effect (ATE) within the potential outcomes framework using observational data with high-dimensional nuisance parameters. Our method introduces a Bayesian debiasing procedure that corrects for bias arising from nuisance estimation and employs a targeted modeling strategy based on summary statistics rather than the full data. These summary statistics are identified in a debiased manner, enabling the estimation of nuisance bias via weighted observables and facilitating hierarchical learning of the ATE. By combining debiasing with sample splitting, our approach separates nuisance estimation from inference on the target parameter, reducing sensitivity to nuisance model specification. We establish that, under mild conditions, the marginal posterior for the ATE satisfies a Bernstein-von Mises theorem when both nuisance models are correctly specified and remains consistent and robust when only one is correct, achieving Bayesian double robustness. This ensures asymptotic efficiency and frequentist validity. Extensive simulations confirm the theoretical results, demonstrating accurate point estimation and credible intervals with nominal coverage, even in high-dimensional settings. The proposed framework can also be extended to other causal estimands, and its key principles offer a general foundation for advancing Bayesian semiparametric inference more broadly.

</details>


### [9] [Causal Inference on Sequential Treatments via Tensor Completion](https://arxiv.org/abs/2511.15866)
*Chenyin Gao,Han Chen,Anru R. Zhang,Shu Yang*

Main category: stat.ME

TL;DR: 提出了一种基于张量因子模型的半参数历史限制边际结构模型（HRMSM），用于纵向观察研究中的因果推断，通过张量补全方法处理模型误设问题。


<details>
  <summary>Details</summary>
Motivation: 传统的边际结构模型（MSMs）对模型误设敏感，需要更灵活的方法来建模纵向观察研究中的序列治疗因果效应。

Method: 将潜在结果构建为三维张量（受试者×时间×治疗策略），采用半参数张量因子模型，结合逆治疗概率加权进行张量补全，利用预治疗协变量信息恢复反事实结果。

Result: 理论分析提供了估计量的Frobenius范数误差的非渐近上界，模拟研究表明该方法优于参数HRMSM和现有矩阵/张量补全方法。

Conclusion: 该方法在医学重症监护数据库的实际应用中展示了实用性，能够有效研究通气对器官功能障碍的影响。

Abstract: Marginal Structural Models (MSMs) are popular for causal inference of sequential treatments in longitudinal observational studies, which however are sensitive to model misspecification. To achieve flexible modeling, we envision the potential outcomes to form a three-dimensional tensor indexed by subject, time, and treatment regime and propose a tensorized history-restricted MSM (HRMSM). The semi-parametric tensor factor model allows us to leverage the underlying low-rank structure of the potential outcomes tensor and exploit the pre-treatment covariate information to recover the counterfactual outcomes. We incorporate the inverse probability of treatment weighting in the loss function for tensor completion to adjust for time-varying confounding. Theoretically, a non-asymptotic upper bound on the Frobenius norm error for the proposed estimator is provided. Empirically, simulation studies show that the proposed tensor completion approach outperforms the parametric HRMSM and existing matrix/tensor completion methods. Finally, we illustrate the practical utility of the proposed approach to study the effect of ventilation on organ dysfunction from the Medical Information Mart for Intensive Care database.

</details>


### [10] [Possibilistic Instrumental Variable Regression](https://arxiv.org/abs/2511.16029)
*Gregor Steiner,Jeremie Houssineau,Mark F. J. Steel*

Main category: stat.ME

TL;DR: 提出了一种基于可能性理论的新方法，用于在存在潜在无效工具变量时进行因果推断的后验推断。


<details>
  <summary>Details</summary>
Motivation: 工具变量回归是处理未观测混杂的常用方法，但在实践中识别有效工具变量往往很困难。

Method: 基于可能性理论，在用户指定的外生性假设可能违反条件下进行治疗效果的后验推断。

Result: 模拟实验和真实数据应用表明该方法表现良好，即使在只有一个潜在无效工具变量时也能提供信息性结果。

Conclusion: 该方法为敏感性分析提供了一个自然且有原则的框架，在工具变量有效性不确定时特别有用。

Abstract: Instrumental variable regression is a common approach for causal inference in the presence of unobserved confounding. However, identifying valid instruments is often difficult in practice. In this paper, we propose a novel method based on possibility theory that performs posterior inference on the treatment effect, conditional on a user-specified set of potential violations of the exogeneity assumption. Our method can provide informative results even when only a single, potentially invalid, instrument is available, offering a natural and principled framework for sensitivity analysis. Simulation experiments and a real-data application indicate strong performance of the proposed approach.

</details>


### [11] [Targeted Parameter Estimation for Robust Empirical Bayes Ranking](https://arxiv.org/abs/2511.16530)
*Nicholas C. Henderson,Nicholas Hartman*

Main category: stat.ME

TL;DR: 提出了一种新的经验贝叶斯排序方法和相关估计方法，用于寻找协变量调整模型的回归参数，通过估计协变量调整后聚类水平均值的近似百分位数来改进排序质量。


<details>
  <summary>Details</summary>
Motivation: 在医疗提供者评估等应用中，传统的协变量调整模型参数估计方法（如最大似然估计）不直接评估排序质量，需要开发能直接优化排序性能的方法。

Method: 构建基于协变量调整后聚类水平均值近似百分位数估计的排序方法，开发了可管理的期望排序平方误差损失表达式，并利用此生成无偏估计量。

Result: 通过模拟研究显示，该方法在排序平方误差性能上持续优于竞争方法（如后验期望排序和最佳线性无偏预测器排序）。

Conclusion: 所提出的方法比传统经验贝叶斯排序方法更稳健，在评估学校测试成绩的纵向研究中展示了实际应用价值。

Abstract: Ordering the expected outcomes across a collection of clusters after performing a covariate adjustment commonly arises in many applied settings, such as healthcare provider evaluation. Regression parameters in such covariate adjustment models are frequently estimated by maximum likelihood or through other criteria that do not directly evaluate the quality of the rankings resulting from using a particular set of parameter estimates. In this article, we propose both a novel empirical Bayes ranking procedure and an associated estimation approach for finding the regression parameters of the covariate adjustment model. By building our ranking approach around estimating approximate percentiles of the covariate-adjusted cluster-level means, we are able to develop manageable expressions for the expected ranking squared-error loss associated with any choice of the covariate-adjustment model parameters, and we harness this to generate a novel unbiased estimator for this expected loss. Minimization of this unbiased estimator directly leads to a novel ranking procedure that is often more robust than conventional empirical Bayes ranking methods. Through a series of simulation studies, we show that our approach consistently delivers improved ranking squared-error performance relative to competing methods, such as posterior expected ranks and ranking the components of the best linear unbiased predictor. Estimating rankings using our method is illustrated with an example from a longitudinal study evaluating test scores across a large group of schools.

</details>


### [12] [Estimation of the Coefficient of Variation of Weibull Distribution under Type-I Progressively Interval Censoring: A Simulation-based Approach](https://arxiv.org/abs/2511.16102)
*Bankitdor M Nongrum,Adarsha Kumar Jena*

Main category: stat.ME

TL;DR: 本文在I型逐步区间删失数据下估计Weibull分布的Pearson变异系数及其二阶替代指标，提出了非线性最小二乘方法，并通过模拟和实际数据验证了最小二乘和贝叶斯方法在点估计方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统上Weibull分布变异系数的估计仅在完整数据下进行，但在生存分析和可靠性理论中，I型逐步区间删失是典型场景，需要开发相应的估计方法。

Method: 使用最大似然法、最小二乘法（包括提出的非线性最小二乘法）和贝叶斯MCMC模拟进行点估计；使用渐近置信区间、基于最小二乘估计的Bootstrap区间和最高后验密度区间进行区间估计。

Result: 模拟研究表明，提出的最小二乘和贝叶斯方法对Pearson变异系数产生更好的点估计；最高后验密度区间在多数情况下优于其他区间估计方法。

Conclusion: 在I型逐步区间删失数据下，最小二乘和贝叶斯方法能有效估计Weibull分布的变异系数，最高后验密度区间是可靠的区间估计工具。

Abstract: Measures of relative variability, such as the Pearson's coefficient of variation (CV$_p$), give much insight into the spread of lifetime distributions, like the Weibull distribution. The estimation of the Weibull CV$_p$ in modern statistics has traditionally been prioritized only when complete data is available. In this article, we estimate the Weibull CV$_p$ and its second-order alternative, denoted as CV$_k$, under type-I progressively interval censoring, which is a typical scenario in survival analysis and reliability theory. Point estimates are obtained using the methods of maximum likelihood, least squares, and the Bayesian approach with MCMC simulation. A nonlinear least squares method is proposed for estimating the CV$_p$ and CV$_k$. We also perform interval estimation of the CV$_p$ and CV$_k$ using the asymptotic confidence intervals, bootstrap intervals through the least squares estimates, and the highest posterior density intervals. A comprehensive Monte Carlo simulation study is carried out to understand and compare the performance of the estimators. The proposed least squares and the Bayesian methods produce better point estimates for the CV$_p$. The highest posterior density intervals outperform other interval estimates in many cases. The methodologies are also applied to a real dataset to demonstrate the performance of the estimators.

</details>


### [13] [Sequential Testing for Assessing the Incremental Value of Biomarkers Under Biorepository Specimen Constraints with Robustness to Model Misspecification](https://arxiv.org/abs/2511.15918)
*Indrila Ganguly,Ying Huang*

Main category: stat.ME

TL;DR: 提出两阶段组序贯假设检验框架评估生物标志物的增量价值，在有限样本下通过早期终止节省资源，并在多中心癌症生物标志物验证中应用。


<details>
  <summary>Details</summary>
Motivation: 解决早期癌症检测中生物标志物验证的样本限制问题，特别是EDRN生物样本库研究中需要在有限样本下评估多个候选生物标志物的增量性能。

Method: 两阶段组序贯假设检验框架，在逻辑回归工作模型下推导渐近结果，结合轮转组成员设计支持多实验室验证。

Result: 模拟显示方法能有效控制I类错误并高效利用样本，在胰腺癌参考集研究中成功识别出与CA19-9结合具有增量性能的候选生物标志物。

Conclusion: 该方法为有限样本下多生物标志物验证提供了稳健高效的解决方案，特别适用于早期癌症检测的生物标志物开发。

Abstract: In cancer biomarker development, a key objective is to evaluate whether a new biomarker, when combined with an established one, improves early cancer detection compared to using the established biomarker alone. Incremental value is often quantified by changes at specific points on the ROC curve, such as an increase in sensitivity at a fixed specificity, which is especially relevant in early cancer detection. Our research is motivated by the Early Detection Research Network (EDRN) biorepository studies, which aim to validate multiple cancer biomarkers across laboratories using specimens obtained from a centralized biorepository, under the constraint of limited specimen availability. To address this challenge, we propose a two-stage group sequential hypothesis testing framework for assessing incremental effects, allowing early stopping for futility or efficacy to conserve valuable samples. Our asymptotic results are derived under a logistic working model and remain valid even under model misspecification, ensuring robustness and broad applicability. We further integrate a rotating group membership design to facilitate validation of multiple candidate biomarkers across laboratories. Through extensive simulations, we demonstrate valid type I error control and efficient utilization of specimens. Finally, we apply our method to data from a multicenter EDRN pancreatic cancer reference set study and show how the proposed approach identifies promising candidate biomarkers that provide incremental performance when combined with CA19-9, while enabling efficient evaluation of a large number of such candidates.

</details>


### [14] [Evaluating Variance Estimates with Relative Efficiency](https://arxiv.org/abs/2511.15961)
*Kedar Karhadkar,Jack Klys,Daniel Ting,Artem Vorozhtsov,Houssam Nassif*

Main category: stat.ME

TL;DR: 本文提出了一种比传统A/A测试更有效的统计方法来评估实验平台的方差估计可靠性，特别是引入了t²统计量来提高检测问题的样本效率。


<details>
  <summary>Details</summary>
Motivation: 工业实验平台需要确保置信区间的有效性以维护客户信任，但传统的A/A测试方法效率低下，仅将每次测试视为二元随机变量，浪费了实验结果幅度的信息。

Method: 提出使用t²统计量来监控方差估计，该方法比经验假阳性率更有效地检测问题，具有更高的样本效率。

Result: 研究表明，除了经验假阳性率之外的其他统计量在检测实验平台问题时更加有效，特别是t²统计量能够更高效地识别方差估计偏差。

Conclusion: t²统计量提供了一种比传统A/A测试更有效的方差估计监控方法，能够更可靠地确保实验平台的统计可靠性。

Abstract: Experimentation platforms in industry must often deal with customer trust issues. Platforms must prove the validity of their claims as well as catch issues that arise. As a central quantity estimated by experimentation platforms, the validity of confidence intervals is of particular concern. To ensure confidence intervals are reliable, we must understand and diagnose when our variance estimates are biased or noisy, or when the confidence intervals may be incorrect.
  A common method for this is A/A testing, in which both the control and test arms receive the same treatment. One can then test if the empirical false positive rate (FPR) deviates substantially from the target FPR over many tests. However, this approach turns each A/A test into a simple binary random variable. It is an inefficient estimate of the FPR as it throws away information about the magnitude of each experiment result. We show how to empirically evaluate the effectiveness of statistics that monitor the variance estimates that partly dictate a platform's statistical reliability. We also show that statistics other than empirical FPR are more effective at detecting issues. In particular, we propose a $t^2$-statistic that is more sample efficient.

</details>


### [15] [Uncertainty Quantification in Bayesian Clustering](https://arxiv.org/abs/2511.16040)
*Garritt L. Page,Andrés F. Barrientos,David B. Dahl,David B. Dunson*

Main category: stat.ME

TL;DR: 提出了一种用于贝叶斯聚类的后处理方法，生成易于使用、计算快速且直观解释的可信集合，并提供了新的聚类不确定性度量方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯聚类方法虽然能通过后验分布提供聚类不确定性的概率表征，但在总结后验不确定性方面的工作相对较少，分区空间的复杂性使这个问题具有挑战性。

Method: 为任何具有后验样本的贝叶斯聚类模型设计后处理程序，生成可信集合，并提供新的聚类不确定性度量，计算累积期望后验概率的簇特定参数估计和可信区域。

Result: 该方法易于使用、计算快速且直观解释，能够在不依赖分区估计或标签交换技术的情况下提供聚类不确定性分析。

Conclusion: 提出的后处理程序为贝叶斯聚类提供了有效的后验不确定性总结方法，在多个应用中展示了其有效性。

Abstract: Bayesian clustering methods have the widely touted advantage of providing a probabilistic characterization of uncertainty in clustering through the posterior distribution. An amazing variety of priors and likelihoods have been proposed for clustering in a broad array of settings. There is also a rich literature on Markov chain Monte Carlo (MCMC) algorithms for sampling from posterior clustering distributions. However, there is relatively little work on summarizing the posterior uncertainty. The complexity of the partition space corresponding to different clusterings makes this problem challenging. We propose a post-processing procedure for any Bayesian clustering model with posterior samples that generates a credible set that is easy to use, fast to compute, and intuitive to interpret. We also provide new measures of clustering uncertainty and show how to compute cluster-specific parameter estimates and credible regions that accumulate a desired posterior probability without having to condition on a partition estimate or employ label-switching techniques. We illustrate our procedure through several applications.

</details>


### [16] [A flexible quantile mixed-effects model for censored outcomes](https://arxiv.org/abs/2511.16589)
*Divan A. Burger,Sean van der Merwe,Emmanuel Lesaffre*

Main category: stat.ME

TL;DR: 提出了基于偏斜指数幂(SEP)误差分布的贝叶斯分位数混合效应模型，用于处理删失纵向数据。SEP分布能够将尾部行为和偏度与目标分位数分离，包含偏斜拉普拉斯(SL)分布作为特例。


<details>
  <summary>Details</summary>
Motivation: 现有基于SL分布的分位数混合效应模型在处理数据偏度与目标分位数隐含偏度不一致时会出现偏差和覆盖率不足的问题，需要更灵活的误差分布来适应不同的偏度模式。

Method: 开发了SEP误差分布的贝叶斯分位数混合效应模型，推导了左、右和区间删失情况下的解析似然贡献，无需在似然函数中进行数值积分。

Result: 模拟研究表明SEP模型在各种删失模式和偏度配置下都能保持接近名义水平的偏差和可信区间覆盖率，而SL模型在数据偏度与目标分位数冲突时会出现偏差和覆盖率不足。在HIV-1 RNA病毒载量案例中，SEP模型在所有分位数上都优于SL基准，提供了更稳定的治疗特异性病毒载量轨迹估计。

Conclusion: SEP误差分布为分位数混合效应模型提供了更灵活和稳健的框架，特别适用于处理删失纵向数据，能够适应不同的偏度模式并产生更稳定的估计结果。

Abstract: We introduce a Bayesian quantile mixed-effects model for censored longitudinal outcomes based on the skew exponential power (SEP) error distribution. The SEP family separates tail behavior and skewness from the targeted quantile and includes the skew Laplace (SL) distribution as a special case. We derive analytic likelihood contributions for left, right, and interval censoring under the SEP model, so censored observations are handled within a single parametric framework without numerical integration in the likelihood. In simulation studies with varying censoring patterns and skewness profiles, the SEP-based quantile mixed-effects model maintains near-nominal bias and credible interval coverage for regression coefficients. In contrast, the SL-based model can exhibit bias and undercoverage when the data's skewness conflicts with the skewness implied by the target quantile. In an HIV-1 RNA viral load case study with left censoring at the assay limit, bridge-sampled marginal likelihoods and simulation-based residual diagnostics favor the SEP specification across quantiles and yield more stable estimates of treatment-specific viral load trajectories than the SL benchmark.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [17] [Integrating Deep Learning and Spatial Statistics in Marine Ecosystem Monitoring](https://arxiv.org/abs/2511.16447)
*Gian Mario Sangiovanni,Gianluca Mastrantonio,Daniele Ventura,Alessio Pollice,Giovanna Jona Lasinio*

Main category: stat.OT

TL;DR: 本文提出了一种基于Thinned Log-Gaussian Cox Process的方法，用于校正海底动物（海参）分布估计中的检测偏差，通过建模检测概率来改进自动识别结果。


<details>
  <summary>Details</summary>
Motivation: 在生态学中，摄影测量法用于大规模收集自然环境样本，但自动目标检测会引入不确定性，因为检测概率受多种因素影响，需要校正检测偏差。

Method: 使用Thinned Log-Gaussian Cox Process建模物种分布，假设存在真实的强度函数，观测过程通过独立细化表示，检测函数控制细化机制，并受目标位置和其他检测相关特征影响。

Result: 通过比较自动检测与人工识别基准、未细化LGCP和细化模型，验证了所提方法在改进分布估计方面的有效性。

Conclusion: 该方法使研究人员能够利用摄影测量自动识别目标，并校正观测过程引起的偏差和近似误差。

Abstract: In ecology, photogrammetry is a crucial method for efficiently collecting non-destructive samples of natural environments. When estimating the spatial distribution of animals, detecting objects in large-scale images becomes crucial. Object detection models enable large-scale analysis but introduce uncertainty because detection probability depends on various factors. To address detection bias, we model the distribution of a species of benthic animals (holothurians) in an area of the Italian Tyrrhenian coast near Giglio Island using a Thinned Log-Gaussian Cox Process (LGCP). We assume that a "true" intensity function accurately describes the distribution, while the observed process, resulting from independent thinning, is represented by a degraded intensity. The detection function controls the thinning mechanism, influenced by the object's location and other detection-related features. We use manual identification of holothurians as our benchmark. We compare automatic detection with this benchmark, an unthinned LGCP, and the thinned model to highlight the improvements gained from the proposed approach.Our method allows researchers to use photogrammetry, automatically identify objects of interest, and correct biases and approximations caused by the observation process.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [18] [Atlas Gaussian processes on restricted domains and point clouds](https://arxiv.org/abs/2511.15822)
*Mu Niu,Yue Zhang,Ke Ye,Pokman Cheung,Yizhu Wang,Xiaochen Yang*

Main category: stat.ML

TL;DR: 提出了Riemannian校正的Atlas高斯过程(RC-AGPs)，用于在未知几何和拓扑结构的点云上进行热核估计和回归任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据常位于受限域或高维点云中，传统高斯过程难以捕捉底层几何结构，现有方法在稀疏或不规则采样点云上表现不佳。

Method: 建立Atlas布朗运动框架估计热核，结合全局热核与局部RBF核构建Riemannian校正核，形成RC-AGPs。

Result: 在合成和真实数据集上的回归任务中，该方法在热核估计和回归精度上均优于现有方法。

Conclusion: RC-AGPs通过有效弥合复杂高维观测与基于流形推断之间的差距，改进了统计推断。

Abstract: In real-world applications, data often reside in restricted domains with unknown boundaries, or as high-dimensional point clouds lying on a lower-dimensional, nontrivial, unknown manifold. Traditional Gaussian Processes (GPs) struggle to capture the underlying geometry in such settings. Some existing methods assume a flat space embedded in a point cloud, which can be represented by a single latent chart (latent space), while others exhibit weak performance when the point cloud is sparse or irregularly sampled. The goal of this work is to address these challenges. The main contributions are twofold: (1) We establish the Atlas Brownian Motion (BM) framework for estimating the heat kernel on point clouds with unknown geometries and nontrivial topological structures; (2) Instead of directly using the heat kernel estimates, we construct a Riemannian corrected kernel by combining the global heat kernel with local RBF kernel and leading to the formulation of Riemannian-corrected Atlas Gaussian Processes (RC-AGPs). The resulting RC-AGPs are applied to regression tasks across synthetic and real-world datasets. These examples demonstrate that our method outperforms existing approaches in both heat kernel estimation and regression accuracy. It improves statistical inference by effectively bridging the gap between complex, high-dimensional observations and manifold-based inferences.

</details>


### [19] [Angular Graph Fractional Fourier Transform: Theory and Application](https://arxiv.org/abs/2511.16111)
*Feiyue Zhao,Yangfan He,Zhichao Zhang*

Main category: stat.ML

TL;DR: 本文提出了角图分数傅里叶变换（AGFRFT），统一了分数阶和角谱分析，解决了GFRFT缺乏角度调节和AGFT在零角度退化缺陷的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图分数傅里叶变换（GFRFT）缺乏角度调节能力，而角图傅里叶变换（AGFT）在零角度无法退化到标准GFT，存在理论一致性和可解释性缺陷。

Method: 提出角图分数傅里叶变换（AGFRFT）框架，通过退化友好的旋转矩阵族确保在零角度精确退化到GFT，定义了两个变体（I-AGFRFT和II-AGFRFT），支持角度和分数阶的可学习联合参数化。

Result: 实验证明AGFRFT在真实数据去噪、图像去噪和点云去噪任务中，在谱集中度、重建质量和可控谱操作方面优于GFRFT和AGFT。

Conclusion: AGFRFT为图信号处理提供了一个稳健灵活的角度分数谱分析工具，具有单位性、可逆性和平滑参数依赖性。

Abstract: Graph spectral representations are fundamental in graph signal processing, offering a rigorous framework for analyzing and processing graph-structured data. The graph fractional Fourier transform (GFRFT) extends the classical graph Fourier transform (GFT) with a fractional-order parameter, enabling flexible spectral analysis while preserving mathematical consistency. The angular graph Fourier transform (AGFT) introduces angular control via GFT eigenvector rotation; however, existing constructions fail to degenerate to the GFT at zero angle, which is a critical flaw that undermines theoretical consistency and interpretability. To resolve these complementary limitations - GFRFT's lack of angular regulation and AGFT's defective degeneracy - this study proposes an angular GFRFT (AGFRFT), a unified framework that integrates fractional-order and angular spectral analyses with theoretical rigor. A degeneracy-friendly rotation matrix family ensures exact GFT degeneration at zero angle, with two AGFRFT variants (I-AGFRFT and II-AGFRFT) defined accordingly. Rigorous theoretical analyses confirm their unitarity, invertibility, and smooth parameter dependence. Both support learnable joint parameterization of the angle and fractional order, enabling adaptive spectral processing for diverse graph signals. Extensive experiments on real-world data denoising, image denoising, and point cloud denoising demonstrate that AGFRFT outperforms GFRFT and AGFT in terms of spectral concentration, reconstruction quality, and controllable spectral manipulation, establishing a robust and flexible tool for integrated angular fractional spectral analysis in graph signal processing.

</details>


### [20] [Spectral Identifiability for Interpretable Probe Geometry](https://arxiv.org/abs/2511.16288)
*William Hao-Cheng Huang*

Main category: stat.ML

TL;DR: 本文提出了谱可识别性原理(SIP)，揭示了线性探针可靠性问题的光谱机制，指出当任务相关方向的特征值间隙大于Fisher估计误差时，探针保持稳定，否则会出现相变式的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 线性探针被广泛用于解释和评估神经表示，但其可靠性尚不清楚，探针在某些情况下可能表现准确，但在其他情况下会不可预测地崩溃。

Method: 通过谱分析机制，提出了谱可识别性原理(SIP)，这是一个可验证的Fisher启发的探针稳定性条件。分析了特征值间隙、样本大小和误分类风险之间的关系。

Result: 当任务相关方向的特征值间隙大于Fisher估计误差时，估计子空间会集中且准确性保持一致；当间隙闭合时，会以相变方式引发不稳定性。受控合成研究证实了这些预测。

Conclusion: 谱检查可以在不可靠的探针扭曲下游评估之前预测其不可靠性，提供了一种可解释的诊断方法，而不是松散的泛化界限。

Abstract: Linear probes are widely used to interpret and evaluate neural representations, yet their reliability remains unclear, as probes may appear accurate in some regimes but collapse unpredictably in others. We uncover a spectral mechanism behind this phenomenon and formalize it as the Spectral Identifiability Principle (SIP), a verifiable Fisher-inspired condition for probe stability. When the eigengap separating task-relevant directions is larger than the Fisher estimation error, the estimated subspace concentrates and accuracy remains consistent, whereas closing this gap induces instability in a phase-transition manner. Our analysis connects eigengap geometry, sample size, and misclassification risk through finite-sample reasoning, providing an interpretable diagnostic rather than a loose generalization bound. Controlled synthetic studies, where Fisher quantities are computed exactly, confirm these predictions and show how spectral inspection can anticipate unreliable probes before they distort downstream evaluation.

</details>


### [21] [Time dependent loss reweighting for flow matching and diffusion models is theoretically justified](https://arxiv.org/abs/2511.16599)
*Lukas Billera,Hedwig Nora Nordlinder,Ben Murrell*

Main category: stat.ML

TL;DR: 本文澄清了在生成器匹配中，Bregman散度损失和生成器的线性参数化可以依赖于当前状态X_t和时间t，并且损失的时间期望可以在广泛的时间分布类上计算。


<details>
  <summary>Details</summary>
Motivation: 澄清生成器匹配框架中损失函数和参数化对时间t和状态X_t的依赖性，为实践中常用的时间依赖损失加权方案提供理论依据。

Method: 通过数学分析证明在生成器匹配框架下，Bregman散度损失和线性参数化可以依赖于X_t和t，并且时间期望可以取广泛的时间分布类。

Result: 证明了时间依赖的损失加权方案在生成器匹配（包括流匹配和扩散模型）的理论合理性，并简化了X_1预测器方案的构建。

Conclusion: 生成器匹配框架允许损失和参数化依赖于时间和状态，这为实践中的训练稳定化技术提供了理论支持，并简化了某些模型构建过程。

Abstract: This brief note clarifies that, in Generator Matching (which subsumes a large family of flow matching and diffusion models over continuous, manifold, and discrete spaces), both the Bregman divergence loss and the linear parameterization of the generator can depend on both the current state $X_t$ and the time $t$, and we show that the expectation over time in the loss can be taken with respect to a broad class of time distributions. We also show this for Edit Flows, which falls outside of Generator Matching. That the loss can depend on $t$ clarifies that time-dependent loss weighting schemes, often used in practice to stabilize training, are theoretically justified when the specific flow or diffusion scheme is a special case of Generator Matching (or Edit Flows). It also often simplifies the construction of $X_1$-predictor schemes, which are sometimes preferred for model-related reasons. We show examples that rely upon the dependence of linear parameterizations, and of the Bregman divergence loss, on $t$ and $X_t$.

</details>


### [22] [Rate-optimal community detection near the KS threshold via node-robust algorithms](https://arxiv.org/abs/2511.16613)
*Jingqiu Ding,Yiding Hua,Kasper Lindberg,David Steurer,Aleksandr Storozhenko*

Main category: stat.ML

TL;DR: 提出了一个多项式时间算法，在对称k-随机块模型中达到极小极大最优误分类率，匹配Kesten-Stigum阈值，即使在对手破坏部分节点的情况下也能保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决社区检测中计算效率与性能的权衡问题，现有方法要么计算效率低，要么需要更强的假设条件，特别是在节点鲁棒性设置下存在较大差距。

Method: 使用平方和框架鲁棒化多数投票，开发基于鲁棒多数投票的新图二分算法，显著改善初始估计的误分类率。

Result: 算法在C ≥ Kk²logk条件下达到极小极大最优误分类率exp(-(1±o(1))C/k)，即使在对手破坏η ≤ exp(-(1±o(1))C/k)比例节点的情况下也成立。

Conclusion: 首次提供了在KS阈值附近达到极小极大率的多项式时间算法，填补了计算效率与性能之间的空白，在标准设置和节点鲁棒设置下都取得了突破。

Abstract: We study community detection in the \emph{symmetric $k$-stochastic block model}, where $n$ nodes are evenly partitioned into $k$ clusters with intra- and inter-cluster connection probabilities $p$ and $q$, respectively.
  Our main result is a polynomial-time algorithm that achieves the minimax-optimal misclassification rate
  \begin{equation*}
  \exp \Bigl(-\bigl(1 \pm o(1)\bigr) \tfrac{C}{k}\Bigr),
  \quad \text{where } C = (\sqrt{pn} - \sqrt{qn})^2,
  \end{equation*}
  whenever $C \ge K\,k^2\,\log k$ for some universal constant $K$, matching the Kesten--Stigum (KS) threshold up to a $\log k$ factor.
  Notably, this rate holds even when an adversary corrupts an $η\le \exp\bigl(- (1 \pm o(1)) \tfrac{C}{k}\bigr)$ fraction of the nodes.
  To the best of our knowledge, the minimax rate was previously only attainable either via computationally inefficient procedures [ZZ15] or via polynomial-time algorithms that require strictly stronger assumptions such as $C \ge K k^3$ [GMZZ17].
  In the node-robust setting, the best known algorithm requires the substantially stronger condition $C \ge K k^{102}$ [LM22].
  Our results close this gap by providing the first polynomial-time algorithm that achieves the minimax rate near the KS threshold in both settings.
  Our work has two key technical contributions:
  (1) we robustify majority voting via the Sum-of-Squares framework,
  (2) we develop a novel graph bisection algorithm via robust majority voting, which allows us to significantly improve the misclassification rate to $1/\mathrm{poly}(k)$ for the initial estimation near the KS threshold.

</details>
