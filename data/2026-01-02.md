<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 4]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 21]
- [stat.ML](#stat.ML) [Total: 10]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Forecasting the Term Structure of Interest Rates with SPDE-Based Models](https://arxiv.org/abs/2512.23910)
*Qihao Duan,Alexandre B. Simas,David Bolin,Raphaël Huser*

Main category: stat.AP

TL;DR: 该论文提出了一种动态Nelson-Siegel模型的扩展，通过将残差建模为高斯随机场来捕捉时间和期限的依赖性，使用SPDE表示实现灵活协方差结构和可扩展贝叶斯推断，显著提高了收益率曲线预测精度。


<details>
  <summary>Details</summary>
Motivation: 动态Nelson-Siegel模型在期限结构预测中被广泛使用，但其残差通常假设为独立同分布，无法捕捉时间和期限维度上的依赖性。作者旨在通过更灵活的残差建模来提高预测性能。

Method: 提出SPDE-DNS模型，将DNS残差建模为高斯随机场，通过随机偏微分方程表示，支持平稳、非平稳、各向异性和不可分离等多种协方差结构。采用集成嵌套拉普拉斯近似进行贝叶斯推断，联合推断潜在DNS因子和残差场。

Result: SPDE扩展模型在点预测和概率预测方面均优于标准基准。在均值-方差债券投资组合框架中，预测结果产生了具有经济意义的效用增益。更重要的是，结构化SPDE残差显著减少了剩余测量误差中的跨期限和跨时间依赖性，使其更接近白噪声。

Conclusion: 将DNS模型与SPDE驱动的残差建模相结合，为收益率曲线预测提供了灵活、可解释且计算高效的方法，在预测精度和投资组合应用中都表现出显著优势。

Abstract: The Dynamic Nelson--Siegel (DNS) model is a widely used framework for term structure forecasting. We propose a novel extension that models DNS residuals as a Gaussian random field, capturing dependence across both time and maturity. The residual field is represented via a stochastic partial differential equation (SPDE), enabling flexible covariance structures and scalable Bayesian inference through sparse precision matrices. We consider a range of SPDE specifications, including stationary, non-stationary, anisotropic, and nonseparable models. The SPDE--DNS model is estimated in a Bayesian framework using the integrated nested Laplace approximation (INLA), jointly inferring latent DNS factors and the residual field. Empirical results show that the SPDE-based extensions improve both point and probabilistic forecasts relative to standard benchmarks. When applied in a mean--variance bond portfolio framework, the forecasts generate economically meaningful utility gains, measured as performance fees relative to a Bayesian DNS benchmark under monthly rebalancing. Importantly, incorporating the structured SPDE residual substantially reduces cross-maturity and intertemporal dependence in the remaining measurement error, bringing it closer to white noise. These findings highlight the advantages of combining DNS with SPDE-driven residual modeling for flexible, interpretable, and computationally efficient yield curve forecasting.

</details>


### [2] [Exposed: Shedding Blacklight on Online Privacy](https://arxiv.org/abs/2512.24041)
*Lucas Shen,Gaurav Sood*

Main category: stat.AP

TL;DR: 研究发现几乎所有美国网络用户都会遭遇广告追踪器和第三方cookie，超过一半用户在48小时内会访问使用会话记录、键盘记录或画布指纹识别等侵入性追踪技术的网站，Google等单一组织可以追踪超过一半用户50%以上的网络活动。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解网络用户被监视的程度、使用的技术以及监视者是谁，填补对网络监控实际规模和分布情况的认识空白。

Method: 结合被动观察的美国代表性样本的匿名浏览数据与Blacklight的域名级追踪数据进行分析。

Result: 99%以上用户遭遇广告追踪器或第三方cookie；超过一半用户在48小时内访问使用侵入性追踪技术的网站；单一组织（通常是Google）可以追踪超过一半用户50%以上的网络活动；人口统计差异存在，特别是年龄和种族差异。

Conclusion: 网络监控普遍存在且集中化，用户浏览内容（不仅仅是浏览量）影响监控风险，年龄和种族差异表明监控存在不平等现象。

Abstract: To what extent are users surveilled on the web, by what technologies, and by whom? We answer these questions by combining passively observed, anonymized browsing data of a large, representative sample of Americans with domain-level data on tracking from Blacklight. We find that nearly all users ($ > 99\%$) encounter at least one ad tracker or third-party cookie over the observation window. More invasive techniques like session recording, keylogging, and canvas fingerprinting are less widespread, but over half of the users visited a site employing at least one of these within the first 48 hours of the start of tracking. Linking trackers to their parent organizations reveals that a single organization, usually Google, can track over $50\%$ of web activity of more than half the users. Demographic differences in exposure are modest and often attenuate when we account for browsing volume. However, disparities by age and race remain, suggesting that what users browse, not just how much, shapes their surveillance risk.

</details>


### [3] [The Malaysian Election Corpus (MECo): Electoral Maps and Cartograms from 1954 to 2025](https://arxiv.org/abs/2512.24211)
*Thevesh Thevananthan,Danesh Prakash Chacko*

Main category: stat.AP

TL;DR: 马来西亚选举边界数据首次以机器可读形式公开，填补了选举数据基础设施的关键空白


<details>
  <summary>Details</summary>
Motivation: 马来西亚的选举边界数据缺乏机器可读的公开形式，阻碍了对选区划分不公和选区操纵等地理中心问题的严谨分析，也限制了选举结果的空间视角研究

Method: 创建马来西亚选举语料库(MECo)的第二部分，包含马来西亚历史上所有19次批准的选区划分数据，从1954年马来亚首次边界到2019年沙巴选区划分，并自动生成所有联邦和州选举的地图，包括等面积和选民加权地图

Result: 建立了首个完整、公开可用、机器可读的马来西亚选举边界记录，填补了国家选举数据基础设施的关键空白

Conclusion: 该数据集为马来西亚选举地理分析提供了重要基础，支持对选区划分不公、选区操纵等问题的深入研究，并促进了选举结果的空间视角分析

Abstract: Electoral boundaries in Malaysia are not publicly available in machine-readable form. This prevents rigorous analysis of geography-centric issues such as malapportionment and gerrymandering, and constrains spatial perspectives on electoral outcomes. We present the second component of the Malaysian Election Corpus (MECo), an open-access collection of digital electoral boundaries covering all 19 approved delimitation exercises in Malaysia's history, from the first set of Malayan boundaries in 1954 until the 2019 Sabah delimitation. We also auto-generate election-time maps for all federal and state elections up to 2025, and include equal-area and electorate-weighted cartograms to support deeper geospatial analysis. This is the first complete, publicly-available, and machine-readable record of Malaysia's electoral boundaries, and fills a critical gap in the country's electoral data infrastructure.

</details>


### [4] [$\ell_0$-Regularized Item Response Theory Model for Robust Ideal Point Estimation](https://arxiv.org/abs/2512.24642)
*Kwangok Seo,Johan Lim,Seokho Lee,Jong Hee Park*

Main category: stat.AP

TL;DR: 提出一种处理抗议投票的理想点估计新方法，使用ℓ₀正则化扩展EM算法，能准确识别极端议员位置，即使存在大量抗议投票。


<details>
  <summary>Details</summary>
Motivation: 传统理想点估计方法面临抗议投票的挑战——议员为表达不满而策略性反对本党投票，这会导致衰减偏差，使极端议员看起来人为地温和。

Method: 扩展Imai等人(2016)的快速EM估计方法，引入ℓ₀正则化处理抗议投票，比MCMC方法更快。

Result: 模拟研究显示，即使抗议投票比例很高，新方法仍保持估计准确性。应用于美国第116和117届众议院，成功恢复了"The Squad"的极端自由派立场，而传统方法将其误判为温和派。

Conclusion: 该方法既提供稳健的理想点估计，又能系统识别抗议投票，有助于深入分析立法机构的策略性投票行为。

Abstract: Ideal point estimation methods face a significant challenge when legislators engage in protest voting -- strategically voting against their party to express dissatisfaction. Such votes introduce attenuation bias, making ideologically extreme legislators appear artificially moderate. We propose a novel statistical framework that extends the fast EM-based estimation approach of \cite{Imai2016} using $\ell_0$ regularization method to handle protest votes. Through simulation studies, we demonstrate that our proposed method maintains estimation accuracy even with high proportions of protest votes, while being substantially faster than MCMC-based methods. Applying our method to the 116th and 117th U.S. House of Representatives, we successfully recover the extreme liberal positions of ``the Squad'', whose protest votes had caused conventional methods to misclassify them as moderates. While conventional methods rank Ocasio-Cortez as more conservative than 69\% of Democrats, our method places her firmly in the progressive wing, aligning with her documented policy positions. This approach provides both robust ideal point estimates and systematic identification of protest votes, facilitating deeper analysis of strategic voting behavior in legislatures.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [5] [A persistent-homology-based Bayesian prior to identify Robin coefficient in parabolic problems](https://arxiv.org/abs/2512.24046)
*Xiaomei Yang,Jiaying Jia*

Main category: stat.CO

TL;DR: 采用贝叶斯推断方法，结合基于持续同调的先验，估计对流热传递分析中的时变Robin系数，并使用分层贝叶斯方法自动选择正则化参数。


<details>
  <summary>Details</summary>
Motivation: 对流热传递分析中需要准确估计时变的Robin系数，传统先验方法可能不够理想，需要开发更有效的先验模型来提高估计精度。

Method: 采用贝叶斯推断框架，引入基于持续同调(PH)的先验分布，并应用分层贝叶斯方法来自动选择正则化参数。

Result: 数值结果表明，与高斯先验和全变分先验相比，PH先验在估计时变Robin系数时表现出持续改进的性能。

Conclusion: 基于持续同调的贝叶斯先验方法为对流热传递分析中的时变系数估计提供了更有效的工具，分层贝叶斯方法实现了正则化参数的自动选择。

Abstract: We adopt a Bayesian inference approach with persistent-homology-based prior to estimate a temporally dependent Robin coefficient arising in the analysis of convective heat transfer. And we also discuss the use of a hierarchical Bayesian method for automatic selection of the regularization parameter. Numerical results demonstrate that the PH prior shows consistent improvement compared to the Gaussian and the total variation prior.

</details>


### [6] [Generalized Poisson Matrix Factorization for Overdispersed Count Data](https://arxiv.org/abs/2512.24604)
*Ryo Ohashi,Hiroyasu Abe,Fumitake Sakaori*

Main category: stat.CO

TL;DR: 提出基于广义泊松分布的非负矩阵分解方法，能灵活处理过分散问题，比现有模型更具通用性


<details>
  <summary>Details</summary>
Motivation: 现有NMF方法在处理计数数据时存在局限性：基于泊松分布的NMF假设数据服从泊松分布，但实际数据常出现过分散现象；基于负二项分布的扩展虽然能处理过分散，但会产生过多的零值，限制了表达能力

Method: 提出基于广义泊松分布的非负矩阵分解方法，采用最大似然估计进行参数估计，能灵活适应不同程度的过分散

Result: 该方法提供了一个比现有模型更通用的框架，扩展了NMF在更广泛计数数据类别中的应用能力

Conclusion: 基于广义泊松分布的NMF方法能有效处理过分散问题，比泊松和负二项分布模型更具灵活性和表达能力，为计数数据的特征提取提供了更强大的工具

Abstract: Non-negative matrix factorization (NMF) is widely used as a feature extraction technique for matrices with non-negative entries, such as image data, purchase histories, and other types of count data. In NMF, a non-negative matrix is decomposed into the product of two non-negative matrices, and the approximation accuracy is evaluated by a loss function. If the Kullback-Leibler divergence is chosen as the loss function, the estimation coincides with maximum likelihood under the assumption that the data entries are distributed according to a Poisson distribution. To address overdispersion, negative binomial matrix factorization has recently been proposed as an extension of the Poisson-based model. However, the negative binomial distribution often generates an excessive number of zeros, which limits its expressive capacity. In this study, we propose a non-negative matrix factorization based on the generalized Poisson distribution, which can flexibly accommodate overdispersion, and we introduce a maximum likelihood approach for parameter estimation. This methodology provides a more versatile framework than existing models, thereby extending the applicability of NMF to a broader class of count data.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [7] [Marked point processes intensity estimation using sparse group Lasso method applied to locations of lucrative and cooperative banks in mainland France](https://arxiv.org/abs/2512.23772)
*Amélie Artis,Achmad Choiruddin,Jean-François Coeurjolly,Frédérique Letué*

Main category: stat.ME

TL;DR: 该研究通过空间点过程分析法国五大银行网点分布，发现其呈现高度非均匀的聚类模式，并建立参数化强度函数模型揭示合作银行与盈利银行的空间分布差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析法国不同类型银行（盈利性银行与合作性银行）的空间分布模式，探究其与社会经济因素的关系，特别是合作银行模式在行业中的特殊性及其与制度同构理论的关系。

Method: 方法包括：1）通过网络爬虫收集银行网点位置数据；2）使用非参数方法估计强度函数、Ripley's K函数和交叉K函数；3）从INSEE收集社会经济数据集；4）提出基于协变量的参数化强度函数模型；5）开发组惩罚双变量复合似然方法估计模型参数并建立渐近性质。

Result: 结果显示：银行网点分布高度非均匀，在小尺度上表现出明显的聚类效应，显著偏离双变量（非均匀）泊松点过程。参数模型分析揭示了合作银行模式在行业中的特殊性。

Conclusion: 该研究为理解法国银行系统的空间组织提供了新视角，特别是合作银行模式与制度同构理论的关系，展示了空间点过程方法在分析金融机构分布与社会经济因素关联中的有效性。

Abstract: In this paper, we model the locations of five major banks in mainland France, two lucrative and three cooperative institutions based on socio-economic considerations. Locations of banks are collected using web scrapping and constitute a bivariate spatial point process for which we estimate nonparametrically summary functions (intensity, Ripley and cross-Ripley's K functions). This shows that the pattern is highly inhomogenenous and exhibits a clustering effect especially at small scales, and thus a significant departure to the bivariate (inhomogeneous) Poisson point process is pointed out. We also collect socio-economic datasets (at the living area level) from INSEE and propose a parametric modelling of the intensity function using these covariates. We propose a group-penalized bivariate composite likelihood method to estimate the model parameters, and we establish its asymptotic properties. The application of the methodology to the banking dataset provides new insights into the specificity of the cooperative model within the sector, particularly in relation to the theories of institutional isomorphism.

</details>


### [8] [Robust reduced rank regression under heavy-tailed noise and missing data via non-convex penalization](https://arxiv.org/abs/2512.24450)
*The Tien Mai*

Main category: stat.ME

TL;DR: 提出一种鲁棒降秩回归框架，结合Huber损失和非凸谱正则化（MCP/SCAD），同时处理重尾噪声、异常值和缺失数据，无需插补。


<details>
  <summary>Details</summary>
Motivation: 传统降秩回归方法依赖平方损失和高斯噪声假设，对重尾误差、异常值和数据污染敏感，且现代应用中普遍存在缺失数据问题，需要更鲁棒的解决方案。

Method: 结合鲁棒Huber损失与非凸谱正则化（MCP/SCAD），采用交替更新的近端梯度算法和定制谱阈值处理，可直接处理响应矩阵中的缺失数据。

Result: 模拟研究表明，该方法在重尾噪声和污染条件下显著优于基于核范数的和非鲁棒替代方法；在癌细胞系数据集应用中展示了实际优势。

Conclusion: 提出的鲁棒降秩回归框架能有效处理重尾噪声、异常值和缺失数据，提供更准确的低秩结构恢复，已实现为R包rrpackrobust。

Abstract: Reduced rank regression (RRR) is a fundamental tool for modeling multiple responses through low-dimensional latent structures, offering both interpretability and strong predictive performance in high-dimensional settings. Classical RRR methods, however, typically rely on squared loss and Gaussian noise assumptions, rendering them sensitive to heavy-tailed errors, outliers, and data contamination. Moreover, the presence of missing data--common in modern applications--further complicates reliable low-rank estimation. In this paper, we propose a robust reduced rank regression framework that simultaneously addresses heavy-tailed noise, outliers, and missing data. Our approach combines a robust Huber loss with nonconvex spectral regularization, specifically the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). Unlike convex nuclear-norm regularization, the proposed nonconvex penalties alleviate excessive shrinkage and enable more accurate recovery of the underlying low-rank structure. The method also accommodates missing data in the response matrix without requiring imputation. We develop an efficient proximal gradient algorithm based on alternating updates and tailored spectral thresholding. Extensive simulation studies demonstrate that the proposed methods substantially outperform nuclear-norm-based and non-robust alternatives under heavy-tailed noise and contamination. An application to cancer cell line data set further illustrates the practical advantages of the proposed robust RRR framework.
  Our method is implemented in the R package rrpackrobust available at https://github.com/tienmt/rrpackrobust.

</details>


### [9] [A Fuzzy Approach for Randomized Confidence Intervals](https://arxiv.org/abs/2512.23866)
*Carlos Henrique Trigo Nasser Felix,Nancy Lopes Garcia,Alex Rodrigo dos Santos Sousa*

Main category: stat.ME

TL;DR: 提出基于Neyman-Pearson引理的随机化置信区间，适用于不满足正则条件的分布，通过模糊置信区间定义实现，在高方差情况下表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统置信区间方法通常需要分布满足正则条件，限制了其应用范围。本文旨在开发更广泛适用的置信区间方法，特别是针对不满足正则条件的分布。

Method: 基于Neyman-Pearson引理构建随机化置信区间，采用模糊置信区间的定义方法，使其能应用于更广泛的分布类型。

Result: 与正态、二项、泊松等常见分布的现有方法比较，新方法在高方差情况下表现更优。对于伯努利试验观测，能够计算期望长度的下界，并证明达到了最小最大期望长度。

Conclusion: 提出的随机化置信区间方法扩展了传统方法的适用范围，在高方差情况下具有更好的性能，并为伯努利试验提供了理论最优的区间估计。

Abstract: We propose randomized confidence intervals based on the Neyman-Pearson lemma, in order to make them more broadly applicable to distributions that do not satisfy regularity conditions. This is achieved by using the definition of fuzzy confidence intervals. These intervals are compared with methods described in the literature for well-known distributions such as normal, binomial, and Poisson. The results show that in high-variance situations, the new intervals provide better performance. Furthermore, through these intervals, it is possible to compute a lower bound for the expected length, demonstrating that they achieve the minimal maximum expected length for a Bernoulli trial observation.

</details>


### [10] [Completing and studentising Spearman's correlation in the presence of ties](https://arxiv.org/abs/2512.23993)
*Landon Hurley*

Main category: stat.ME

TL;DR: 提出基于ℓ₂范数的Kemeny相关系数ρκ，相比传统Spearman相关系数，在存在或不存在数据重复时满足高斯-马尔可夫条件，适用于离散和连续随机变量，且无需假设误差分布。


<details>
  <summary>Details</summary>
Motivation: 传统非参数相关系数（如Spearman ρ）在分析任意随机变量时，通常需要已知明确的误差分布假设，这在某些情况下是不可接受的。需要一种更灵活、数学性质更好的相关系数。

Method: 从统计估计量的角度研究ℓ₂范数表示的相关系数（Emond和Mason，2002），提出Kemeny相关系数ρκ，证明其满足高斯-马尔可夫条件，并构建零假设分布（Student-t分布）。

Result: Kemeny相关系数ρκ在存在或不存在数据重复时都满足高斯-马尔可夫条件，适用于离散和连续随机变量，无需连续随机变量或特定高斯误差假设，在高峰度数据模拟中表现出良好的经验覆盖率。

Conclusion: Kemeny相关系数ρκ是一种数学性质优越的非参数相关系数，比传统Spearman相关系数更灵活，适用于更广泛的数据类型，且具有理论保证的统计推断能力。

Abstract: Non-parametric correlation coefficients have been widely used for analysing arbitrary random variables upon common populations, when requiring an explicit error distribution to be known is an unacceptable assumption. We examine an \(\ell_{2}\) representation of a correlation coefficient (Emond and Mason, 2002) from the perspective of a statistical estimator upon random variables, and verify a number of interesting and highly desirable mathematical properties, mathematically similar to the Whitney embedding of a Hilbert space into the \(\ell_{2}\)-norm space. In particular, we show here that, in comparison to the traditional Spearman (1904) \(ρ\), the proposed Kemeny \(ρ_κ\) correlation coefficient satisfies Gauss-Markov conditions in the presence or absence of ties, thereby allowing both discrete and continuous marginal random variables. We also prove under standard regularity conditions a number of desirable scenarios, including the construction of a null hypothesis distribution which is Student-t distributed, parallel to standard practice with Pearson's r, but without requiring either continuous random variables nor particular Gaussian errors. Simulations in particular focus upon highly kurtotic data, with highly nominal empirical coverage consistent with theoretical expectation.

</details>


### [11] [Power Analysis is Essential: High-Powered Tests Suggest Minimal to No Effect of Rounded Shapes on Click-Through Rates](https://arxiv.org/abs/2512.24521)
*Ron Kohavi,Jakub Linowski,Lukas Vermeer,Fabrice Boisseranc,Joachim Furuseth,Andrew Gelman,Guido Imbens,Ravikiran Rajagopal*

Main category: stat.ME

TL;DR: 原始研究声称圆角按钮能提升55%点击率，但高统计功效的重复实验发现效应量小两个数量级且不显著，凸显统计功效不足导致夸大效应的问题。


<details>
  <summary>Details</summary>
Motivation: 质疑原始研究中圆角按钮提升55%点击率的惊人发现，基于大量A/B测试经验认为该效应量过于夸张，需要验证其可信度。

Method: 进行三个高统计功效的A/B测试，每个实验样本量是原始研究的2000倍以上，并参考两个独立第三方复制实验。

Result: 所有重复实验的效应量估计值比原始报告小约两个数量级，95%置信区间包含零（不显著），表明原始效应被严重夸大。

Conclusion: 统计功效不足的研究容易夸大效应（赢家诅咒），强调功效分析和实验设计对提高结果可信度和可重复性的重要性。

Abstract: Underpowered studies (below 50%) suffer from the winner's curse: a statistically significant result must exaggerate the true treatment effect to meet the significance threshold. A study by Dipayan Biswas, Annika Abell, and Roger Chacko published in the Journal of Consumer Research (2023) reported that in an A/B test simply rounding the corners of square buttons increased the online click-through rate by 55% (p-value 0.037)$\unicode{x2014}$a striking finding with potentially wide-ranging implications for the digital industry that is seeking to enhance consumer engagement. Drawing on our experience with tens of thousands of A/B tests, many involving similar user interface modifications, we found this dramatic claim implausibly large. To evaluate the claim, we conducted three high-powered A/B tests, each involving over two thousand times more users than the original study. All three experiments yielded effect size estimates that were approximately two orders of magnitude smaller than initially reported, with 95% confidence intervals that include zero, that is, not statistically significant at the 0.05 level. Two additional independent replications by Evidoo found similarly small effects. These findings underscore the critical importance of power analysis and experimental design to increase trust and reproducibility of results.

</details>


### [12] [An exact unbiased semi-parametric maximum quasi-likelihood framework which is complete in the presence of ties](https://arxiv.org/abs/2512.24009)
*Landon Hurley*

Main category: stat.ME

TL;DR: 该论文提出了一种基于广义Kendall τₐ估计器的准似然扩展，引入了Kemeny协方差系数τ_κ，构建了准最大似然估计框架，支持弱排序数据且能处理平局而不损失信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注边际或成对汇总，无法有效处理样本观测的弱排序和平局情况，需要一种能直接推断排序或弱排序数据集之间总体相关性的统一框架。

Method: 基于Emond和Mason的未中心化相关内积（希尔伯特空间）表述，通过Edgeworth展开解决ℓ₂范数下的子高斯似然优化问题，推导出Kemeny协方差系数τ_κ，构建准似然框架，支持成对比较-连续随机变量。

Result: 提出的系数τ_κ具有U统计量结构，自然诱导出准最大似然估计框架，产生一致的Wald和似然比检验统计量，建立了与Bradley-Terry和Thurstone模型的正式等价性，得到唯一识别的线性表示。

Conclusion: 该框架为排序或弱排序数据集之间的总体相关性推断提供了统一方法，能处理连续和离散随机变量，支持平局且不损失信息，扩展了经典U统计量，具有解析和基于似然的估计器。

Abstract: This paper introduces a novel quasi-likelihood extension of the generalised Kendall \(τ_{a}\) estimator, together with an extension of the Kemeny metric and its associated covariance and correlation forms. The central contribution is to show that the U-statistic structure of the proposed coefficient \(τ_κ\) naturally induces a quasi-maximum likelihood estimation (QMLE) framework, yielding consistent Wald and likelihood ratio test statistics. The development builds on the uncentred correlation inner-product (Hilbert space) formulation of Emond and Mason (2002) and resolves the associated sub-Gaussian likelihood optimisation problem under the \(\ell_{2}\)-norm via an Edgeworth expansion of higher-order moments. The Kemeny covariance coefficient \(τ_κ\) is derived within a novel likelihood framework for pairwise comparison-continuous random variables, enabling direct inference on population-level correlation between ranked or weakly ordered datasets. Unlike existing approaches that focus on marginal or pairwise summaries, the proposed framework supports sample-observed weak orderings and accommodates ties without information loss. Drawing parallels with Thurstone's Case V latent ordering model, we derive a quasi-likelihood-based tie model with analytic standard errors, generalising classical U-statistics. The framework applies to general continuous and discrete random variables and establishes formal equivalence to Bradley-Terry and Thurstone models, yielding a uniquely identified linear representation with both analytic and likelihood-based estimators.

</details>


### [13] [Quasi-Maximum Likelihood Estimation for a Genuinely Unbalanced Dynamic Network Panel Data Model](https://arxiv.org/abs/2512.24748)
*Zhijian Wang,Xingbai Xu,Tuo Liu*

Main category: stat.ME

TL;DR: 提出针对真实非平衡动态网络面板数据的QMLE估计器，包含网络溢出、时间依赖和首次出现效应，推导渐近性质并开发偏差校正方法


<details>
  <summary>Details</summary>
Motivation: 真实世界中的网络面板数据常常是非平衡的（单位在不同时间点进入和退出），现有方法通常假设平衡面板或简单处理非平衡性。需要开发能够处理真实非平衡性（包括首次出现效应）的动态网络面板模型估计方法

Method: 提出准最大似然估计器（QMLE）处理真实非平衡动态网络面板，模型包含：1）同期和滞后网络溢出效应；2）时间依赖性；3）首次出现效应（listing effect）。推导QMLE的渐近性质，识别当N相对于T较大时产生的渐近偏差，基于偏差表达式提出偏差校正估计器

Result: 证明QMLE在N和T都趋于无穷时的一致性，推导渐近分布，识别出当N相对于T较大时的渐近偏差。偏差校正估计器在适当正则条件下渐近无偏且服从正态分布。蒙特卡洛实验显示校正估计器在偏差、RMSE、覆盖概率和正态性方面表现良好。Airbnb实证应用揭示了新西兰和纽约市在空间和时间价格传导方面的区域特定模式

Conclusion: 提出的方法能够有效处理真实非平衡动态网络面板数据，偏差校正估计器具有良好的有限样本性质。实证结果表明建模真实非平衡性在动态网络设置中具有重要意义，特别是在处理首次出现效应时

Abstract: This paper develops a quasi-maximum likelihood estimator for genuinely unbalanced dynamic network panel data models with individual fixed effects. We propose a model that accommodates contemporaneous and lagged network spillovers, temporal dependence, and a listing effect that activates upon a unit's first appearance in the panel. We establish the consistency of the QMLE as both $N$ and $T$ go to infinity, derive its asymptotic distribution, and identify an asymptotic bias arising from incidental parameters when $N$ is asymptotically large relative to $T$. Based on the asymptotic bias expression, we propose a bias-corrected estimator that is asymptotically unbiased and normally distributed under appropriate regularity conditions. Monte Carlo experiments examine the finite sample performance of the bias-corrected estimator across different criteria, including bias, RMSE, coverage probability, and the normality of the estimator. The empirical application to Airbnb listings from New Zealand and New York City reveals region-specific patterns in spatial and temporal price transmission, illustrating the importance of modeling genuine unbalancedness in dynamic network settings.

</details>


### [14] [Least Square Estimation: SDEs Perturbed by Lévy Noise with Sparse Sample Paths](https://arxiv.org/abs/2512.24005)
*Brijesh Kumar Jha,Subhra Sankar Dhar,Akash Ashirbad Panda*

Main category: stat.ME

TL;DR: 该论文研究了受Lévy噪声影响的随机微分方程中未知参数的最小二乘估计，特别是在样本路径稀疏的情况下，推导了漂移系数、扩散系数和跳跃扩散系数的估计量，并建立了其渐近收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究在Lévy噪声影响下随机微分方程的参数估计问题，特别关注样本路径稀疏的情况，这在许多实际应用中很常见但传统方法难以处理。

Method: 提出最小二乘估计方法，基于n条稀疏观测曲线推导漂移系数、扩散系数和跳跃扩散系数的估计量，并建立其渐近收敛理论。

Result: 成功推导了Lévy噪声SDE参数的最小二乘估计量，建立了估计量的渐近收敛速率，并通过基准数据集和模拟研究验证了方法的有效性。

Conclusion: 该方法为稀疏观测下Lévy噪声SDE的参数估计提供了有效的解决方案，具有理论保证和实际应用价值。

Abstract: This article investigates the least squares estimators (LSE) for the unknown parameters in stochastic differential equations (SDEs) that are affected by Lévy noise, particularly when the sample paths are sparse. Specifically, given $n$ sparsely observed curves related to this model, we derive the least squares estimators for the unknown parameters: the drift coefficient, the diffusion coefficient, and the jump-diffusion coefficient. We also establish the asymptotic rate of convergence for the proposed LSE estimators. Additionally, in the supplementary materials, the proposed methodology is applied to a benchmark dataset of functional data/curves, and a small simulation study is conducted to illustrate the findings.

</details>


### [15] [Exact finite mixture representations for species sampling processes](https://arxiv.org/abs/2512.24414)
*Ramsés H. Mena,Christos Merkatas,Theodoros Nicoleris,Carlos E. Rodríguez*

Main category: stat.ME

TL;DR: 论文提出了一种将无限维物种采样过程表示为精确有限混合模型的方法，通过潜在截断变量和重加权原子，为贝叶斯混合模型提供可直接替换的有限表示。


<details>
  <summary>Details</summary>
Motivation: 随机概率度量在现代贝叶斯推断中扮演核心角色，但无限维先验在实际计算中面临挑战。需要一种既能保持原始先验分布特征，又能简化计算的方法。

Method: 证明任何适当的物种采样过程都可以在先验层面上表示为具有潜在截断变量和重加权原子的有限混合模型，保持其分布特征完全不变。

Result: 该方法为贝叶斯混合模型提供了可直接替换的有限表示，实现了简单的MCMC实现和可处理的表达式，避免了临时截断和特定模型构造。

Conclusion: 提出的有限混合表示在保持原始无限维先验完全一般性的同时，为算法设计和实现带来了实际增益，为贝叶斯混合模型提供了更实用的计算框架。

Abstract: Random probability measures, together with their constructions, representations, and associated algorithms, play a central role in modern Bayesian inference. A key class is that of proper species sampling processes, which offer a relatively simple yet versatile framework that extends naturally to non-exchangeable settings. We revisit this class from a computational perspective and show that they admit exact finite mixture representations. In particular, we prove that any proper species sampling process can be written, at the prior level, as a finite mixture with a latent truncation variable and reweighted atoms, while preserving its distributional features exactly. These finite formulations can be used as drop-in replacements in Bayesian mixture models, recasting posterior computation in terms of familiar finite-mixture machinery. This yields straightforward MCMC implementations and tractable expressions, while avoiding ad hoc truncations and model-specific constructions. The resulting representation preserves the full generality of the original infinite-dimensional priors while enabling practical gains in algorithm design and implementation.

</details>


### [16] [Bayesian inference for functional extreme events defined via partially unobserved processes](https://arxiv.org/abs/2512.24356)
*Max Thannheimer,Marco Oesting*

Main category: stat.ME

TL;DR: 提出两步骤MCMC算法进行贝叶斯空间极值分析，解决观测点稀疏时风险泛函r-Pareto过程推断问题


<details>
  <summary>Details</summary>
Motivation: 空间极值分析中，r-Pareto过程推断需要评估风险泛函r(X)，这需要完整过程X的知识。但实际应用中通常只有离散观测点数据，无法直接计算r(X)。

Method: 提出两步骤MCMC算法：第一步基于观测条件采样X，识别哪些观测导致r-超越；第二步用这些超越采样r-Pareto过程参数的后验分布。交替执行实现完整贝叶斯模型。

Result: 证明在适当假设下，第一步中观测被分类为r-超越的概率收敛到期望概率；给定第一步，第二步构建的马尔可夫链分布收敛到目标后验分布。通过模拟研究比较标准贝叶斯方法。

Conclusion: 该方法解决了空间极值分析中观测数据稀疏时的推断问题，为r-Pareto过程的贝叶斯推断提供了可行方案。

Abstract: In order to describe the extremal behaviour of some stochastic process $X$, approaches from univariate extreme value theory are typically generalized to the spatial domain. In particular, generalized peaks-over-threshold approaches allow for the consideration of single extreme events. These can be flexibly defined as exceedances of a risk functional $r$, such as a spatial average, applied to $X$. Inference for the resulting limit process, the so-called $r$-Pareto process, requires the evaluation of $r(X)$ and thus the knowledge of the whole process $X$. In many practical applications, however, observations of $X$ are only available at scattered sites. To overcome this issue, we propose a two-step MCMC-algorithm in a Bayesian framework. In a first step, we sample from $X$ conditionally on the observations in order to evaluate which observations lead to $r$-exceedances. In a second step, we use these exceedances to sample from the posterior distribution of the parameters of the limiting $r$-Pareto process. Alternating these steps results in a full Bayesian model for the extremes of $X$. We show that, under appropriate assumptions, the probability of classifying an observation as $r$-exceedance in the first step converges to the desired probability. Furthermore, given the first step, the distribution of the Markov chain constructed in the second step converges to the posterior distribution of interest. The procedure is compared to the Bayesian version of the standard procedure in a simulation study.

</details>


### [17] [A Robust Persistent Homology : Trimming Approach](https://arxiv.org/abs/2512.24222)
*Tuhin Subhra Mahato,Subhra Sankar Dhar*

Main category: stat.ME

TL;DR: 提出一种基于修剪方法的鲁棒持久同调，可处理数据云内部和外部的异常值，理论证明其与总体版本的Bottleneck距离可任意小，并在模拟和细胞生物学数据中验证实用性


<details>
  <summary>Details</summary>
Motivation: 传统持久同调对异常值敏感，需要鲁棒版本来处理数据云内部和外部的异常值，以更准确地捕捉数据的几何特征

Method: 基于修剪方法的鲁棒持久同调，通过修剪异常值来构建鲁棒的持久同调，适用于数据云内部和外部异常值

Result: 理论证明：样本量足够大时，鲁棒持久同调与总体版本的Bottleneck距离可任意小；实验验证：在多种模拟数据和细胞生物学基准数据中展示实用性

Conclusion: 提出的鲁棒持久同调方法能有效处理异常值，理论保证良好，实际应用可行，为存在异常值的数据分析提供了可靠工具

Abstract: This article studies the robust version of persistent homology based on trimming methodology to capture the geometric feature through support of the data in presence of outliers. Precisely speaking, the proposed methodology works when the outliers lie outside the main data cloud as well as inside the data cloud. In the course of theoretical study, it is established that the Bottleneck distance between the proposed robust version of persistent homology and its population analogue can be made arbitrary small with a certain rate for a sufficiently large sample size. The practicability of the methodology is shown for various simulated data and bench mark real data associated with cellular biology.

</details>


### [18] [Modewise Additive Factor Model for Matrix Time Series](https://arxiv.org/abs/2512.25025)
*Elynn Chen,Yuefeng Han,Jiayu Li,Ke Xu*

Main category: stat.ME

TL;DR: 提出了一种矩阵时间序列的模态加性因子模型(MAFM)，通过加性结构捕捉行特异和列特异潜在效应，比乘法框架更灵活，并开发了高效的两阶段估计算法。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵时间序列的乘法因子模型（如Tucker和CP因子模型）灵活性有限，需要更灵活的框架来分别建模不同模态的变异来源，捕捉行特异和列特异的潜在效应。

Method: 提出MAFM模型，将观测分解为行因子分量、列因子分量和噪声；开发两阶段估计算法：MINE初始化+COMPAS迭代优化，利用正交补投影消除跨模态干扰。

Result: 建立了因子载荷矩阵估计的收敛率，推导了载荷矩阵估计量的渐近分布，开发了协方差估计器，形成了数据驱动的推断框架，支持置信区间构建和假设检验。

Conclusion: MAFM模型比现有方法更灵活，算法计算高效，理论性质良好，在合成和真实数据实验中表现出优势，为矩阵时间序列分析提供了新的建模和推断工具。

Abstract: We introduce a Modewise Additive Factor Model (MAFM) for matrix-valued time series that captures row-specific and column-specific latent effects through an additive structure, offering greater flexibility than multiplicative frameworks such as Tucker and CP factor models. In MAFM, each observation decomposes into a row-factor component, a column-factor component, and noise, allowing distinct sources of variation along different modes to be modeled separately. We develop a computationally efficient two-stage estimation procedure: Modewise Inner-product Eigendecomposition (MINE) for initialization, followed by Complement-Projected Alternating Subspace Estimation (COMPAS) for iterative refinement. The key methodological innovation is that orthogonal complement projections completely eliminate cross-modal interference when estimating each loading space. We establish convergence rates for the estimated factor loading matrices under proper conditions. We further derive asymptotic distributions for the loading matrix estimators and develop consistent covariance estimators, yielding a data-driven inference framework that enables confidence interval construction and hypothesis testing. As a technical contribution of independent interest, we establish matrix Bernstein inequalities for quadratic forms of dependent matrix time series. Numerical experiments on synthetic and real data demonstrate the advantages of the proposed method over existing approaches.

</details>


### [19] [Valid and Efficient Two-Stage Latent Subgroup Analysis with Observational Data](https://arxiv.org/abs/2512.24223)
*Yuanhui Luo,Xinzhou Guo,Yuqi Gu*

Main category: stat.ME

TL;DR: 提出一种两阶段潜在亚组分析方法，通过谱方法处理二元项目反应数据，在高维混杂因素下实现一致的亚组效应估计和有效推断。


<details>
  <summary>Details</summary>
Motivation: 传统的一阶段联合建模方法在处理包含大量混杂因素的观察数据时不可行，而现有的两阶段方法由于潜在亚组成员身份的错误分类可能导致偏差。需要开发一种能够容忍一定错误分类率、适应高维混杂因素的两阶段方法。

Method: 采用两阶段框架：第一阶段使用谱方法从二元项目反应中估计潜在亚组成员身份，第二阶段使用估计的亚组成员身份进行亚组分析。该方法利用大量项目反应的优势，计算高效且对非信息性项目具有鲁棒性。

Result: 方法能够容忍的最大错误分类率，在观察性研究中实现一致的亚组效应估计和有效推断。模拟研究和教育评估数据应用证明了方法的优势。

Conclusion: 提出的两阶段潜在亚组分析方法能够在观察数据中实现有效的亚组分析，适应高维混杂因素，对潜在亚组成员身份的错误分类具有鲁棒性，为实际应用提供了可行的解决方案。

Abstract: Subgroup analysis evaluates treatment effects across multiple sub-populations. When subgroups are defined by latent memberships inferred from imperfect measurements, the analysis typically involves two inter-connected models, a latent class model and a subgroup outcome model. The classical one-stage framework, which models the joint distribution of the two models, may be infeasible with observational data containing many confounders. The two-stage framework, which first estimates the latent class model and then performs subgroup analysis using estimated latent memberships, can accommodate potential confounders but may suffer from bias issues due to misclassification of latent subgroup memberships. This paper focuses on latent subgroups inferred from binary item responses and addresses when and how a valid two-stage latent subgroup analysis can be made with observational data. We investigate the maximum misclassification rate that a valid two-stage framework can tolerate. Introducing a spectral method perspective, we propose a two-stage approach to achieve the desired misclassification rate with the blessing of many item responses. Our method accommodates high-dimensional confounders, is computationally efficient and robust to noninformative items. In observational studies, our methods lead to consistent estimation and valid inference on latent subgroup effects. We demonstrate its merit through simulation studies and an application to educational assessment data.

</details>


### [20] [A Novel Approach for Data Integration with Multiple Heterogeneous Data Sources](https://arxiv.org/abs/2512.24342)
*Farimah Shamsi,Andriy Derkach*

Main category: stat.ME

TL;DR: 提出一种新统计框架，利用辅助信息整合异质数据源与汇总数据，通过估计研究特定抽样权重和校准估计方程来获得完整模型参数。


<details>
  <summary>Details</summary>
Motivation: 多源数据整合可增加样本量和提高人群多样性，但现有方法通常假设随机抽样。需要开发能处理异质数据源且不依赖随机抽样假设的方法。

Method: 利用辅助信息估计研究特定抽样权重，校准估计方程以获得完整模型参数，从而整合汇总数据与异质数据源。

Result: 通过多种抽样设计的模拟研究验证方法性能，并应用于美国癌症登记数据与结直肠癌风险因素汇总OR估计的重新分析。

Conclusion: 该方法能有效整合异质数据源与汇总数据，放宽随机抽样假设，为多源数据整合提供灵活框架。

Abstract: The integration of data from multiple sources is increasingly used to achieve larger sample sizes and enhance population diversity. Our previous work established that, under random sampling from the same underlying population, integrating large incomplete datasets with summary-level data produces unbiased parameter estimates. In this study, we develop a novel statistical framework that enables the integration of summary-level data with information from heterogeneous data sources by leveraging auxiliary information. The proposed approach estimates study-specific sampling weights using this auxiliary information and calibrates the estimating equations to obtain the full set of model parameters. We evaluate the performance of the proposed method through simulation studies under various sampling designs and illustrate its application by reanalyzing U.S. cancer registry data combined with summary-level odds ratio estimates for selected colorectal cancer (CRC) risk factors, while relaxing the random sampling assumption.

</details>


### [21] [Geometric criteria for identifying extremal dependence and flexible modeling via additive mixtures](https://arxiv.org/abs/2512.24392)
*Jeongjin Lee,Jennifer Wadsworth*

Main category: stat.ME

TL;DR: 论文提出基于几何极值框架的极值依赖分类方法，通过极限集的几何特征（钝性/尖性）判断渐近依赖或独立，并引入可加混合规范函数平滑插值两种依赖结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要先验知识判断极值依赖结构，而几何极值框架通过极限集的几何特征提供更直观的判断标准，但钝性极限集对应渐近独立，而尖性极限集可能对应两种依赖结构，需要更明确的几何判据。

Method: 在双变量情况下，基于截断伽马模型假设和有界角密度，证明尖性极限集意味着渐近依赖；提出两种可加混合规范函数，能平滑插值渐近依赖和渐近独立结构；通过模拟研究评估几何方法的有效性。

Result: 建立了尖性极限集与渐近依赖的明确对应关系，提供了实用的几何判据；可加混合规范函数能有效捕捉不同极值依赖结构，计算高效且在各种极值依赖场景下表现可靠，性能与现有方法相当或更优。

Conclusion: 几何极值框架提供了一种简单有效的极值依赖分类方法，无需先验知识即可通过极限集的几何特征判断依赖结构，可加混合规范函数为建模提供了灵活工具，是传统参数copula模型的有力替代方案。

Abstract: The framework of geometric extremes is based on the convergence of scaled sample clouds onto a limit set, characterized by a gauge function, with the shape of the limit set determining extremal dependence structures. While it is known that a blunt limit set implies asymptotic independence, the absence of bluntness can be linked to both asymptotic dependence and independence. Focusing on the bivariate case, under a truncated gamma modeling assumption with bounded angular density, we show that a ``pointy'' limit set implies asymptotic dependence, thus offering practical geometric criteria for identifying extremal dependence classes. Suitable models for the gauge function offer the ability to capture asymptotically independent or dependent data structures, without requiring prior knowledge of the true extremal dependence structure. The geometric approach thus offers a simple alternative to various parametric copula models that have been developed for this purpose in recent years. We consider two types of additively mixed gauge functions that provide a smooth interpolation between asymptotic dependence and asymptotic independence. We derive their explicit forms, explore their properties, and establish connections to the developed geometric criteria. Through a simulation study, we evaluate the effectiveness of the geometric approach with additively mixed gauge functions, comparing its performance to existing methodologies that account for both asymptotic dependence and asymptotic independence. The methodology is computationally efficient and yields reliable performance across various extremal dependence scenarios.

</details>


### [22] [Demystifying Proximal Causal Inference](https://arxiv.org/abs/2512.24413)
*Grace V. Ringlein,Trang Quynh Nguyen,Peter P. Zandi,Elizabeth A. Stuart,Harsh Parikh*

Main category: stat.ME

TL;DR: 本文综述了代理因果推断（PCI）框架，该框架在存在未观测混杂因素时识别和估计因果效应，通过代理变量替代传统无未观测混杂假设，提供实用操作指南。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断方法依赖无未观测混杂假设，但该假设常被违反。PCI通过代理变量解决这一挑战，为实际应用中处理未观测混杂提供可行框架。

Method: 回顾现有识别结果，讨论PCI有效估计所需假设，比较不同PCI估计方法，提供代理变量选择和评估的实用指南（基于领域知识、测量误差视角和阴性控制类比）。

Result: 通过概念性示例展示代理变量选择的张力，强调明确定义未观测混杂机制的重要性，为实际应用提供操作化指导。

Conclusion: PCI为处理未观测混杂提供了有前景的框架，本文旨在澄清PCI原理、促进其在实际中的审慎应用，并指出方法论发展和实证研究的开放方向。

Abstract: Proximal causal inference (PCI) has emerged as a promising framework for identifying and estimating causal effects in the presence of unobserved confounders. While many traditional causal inference methods rely on the assumption of no unobserved confounding, this assumption is likely often violated. PCI mitigates this challenge by relying on an alternative set of assumptions regarding the relationships between treatment, outcome, and auxiliary variables that serve as proxies for unmeasured confounders. We review existing identification results, discuss the assumptions necessary for valid causal effect estimation via PCI, and compare different PCI estimation methods. We offer practical guidance on operationalizing PCI, with a focus on selecting and evaluating proxy variables using domain knowledge, measurement error perspectives, and negative control analogies. Through conceptual examples, we demonstrate tensions in proxy selection and discuss the importance of clearly defining the unobserved confounding mechanism. By bridging formal results with applied considerations, this work aims to demystify PCI, encourage thoughtful use in practice, and identify open directions for methodological development and empirical research.

</details>


### [23] [Model-Assisted Bayesian Estimators of Transparent Population Level Summary Measures for Ordinal Outcomes in Randomized Controlled Trials](https://arxiv.org/abs/2512.24442)
*Lindsey E. Turner,Carolyn T. Bramante,Thomas A. Murray*

Main category: stat.ME

TL;DR: 提出用于有序结局的透明化汇总指标，包括加权几何平均比值比、相对风险和加权平均风险差，并开发基于非比例优势模型的高效贝叶斯估计器。


<details>
  <summary>Details</summary>
Motivation: 在随机对照试验中，比例优势模型下的比值比作为有序结局的汇总指标，当比例优势假设被违反时缺乏透明度，无法明确显示对有序结局各组成部分的侧重。

Method: 提出多种透明化汇总指标（加权几何平均比值比、相对风险、加权平均风险差），开发基于非比例优势模型的高效贝叶斯估计器，使用贝叶斯自助法进行协变量调整和边际化，并提出具有良好不变性性质的加权方案。

Result: 模拟研究表明，基于所提出的总体水平汇总指标的比较检验相对于传统的比例优势方法表现良好。在COVID-OUT试验分析中，发现了非比例优势的证据。

Conclusion: 所提出的透明化汇总指标和高效贝叶斯估计方法为有序结局分析提供了更好的工具，特别是在比例优势假设不成立时，能够更清晰地解释治疗效果。

Abstract: In randomized controlled trials, ordinal outcomes typically improve statistical efficiency over binary outcomes. The treatment effect on an ordinal outcome is usually described by the odds ratio from a proportional odds model, but this summary measure lacks transparency with respect to its emphasis on the components of the ordinal outcome when proportional odds is violated. We propose various summary measures for ordinal outcomes that are fully transparent in this regard, including 'weighted geometric mean' odds ratios and relative risks, and 'weighted mean' risk differences. We also develop and evaluate efficient model-assisted Bayesian estimators for these population level summary measures based on non-proportional odds models that facilitate covariate adjustment with marginalization via the Bayesian bootstrap. We propose a weighting scheme that engenders appealing invariance properties, including to whether the ordinal outcome is ordered from best to worst versus worst to best. Using computer simulation, we show that comparative testing based on the proposed population level summary measures performs well relative to the conventional proportional odds approach. We also report an analysis of the COVID-OUT trial, which exhibits evidence of non-proportional odds.

</details>


### [24] [Multiple Testing of One-Sided Hypotheses with Conservative $p$-values](https://arxiv.org/abs/2512.24588)
*Kwangok Seo,Johan Lim,Hyungwon Choi,Jaesik Jeong*

Main category: stat.ME

TL;DR: 该论文提出了一种在复合零假设下修正p值的方法，通过经验贝叶斯框架估计零分布的边际分布，构造精确的p值，从而提升多重检验的统计功效。


<details>
  <summary>Details</summary>
Motivation: 在单侧多重检验问题中，当零假设是复合假设时（某些零均值可能为负），传统方法计算的p值会过于保守，导致统计功效显著降低。现有方法通常通过修改多重检验过程本身来解决问题，但作者希望直接修正p值本身。

Method: 在经验贝叶斯框架下估计检验统计量的边际零分布，基于这个估计的分布构造精确的p值。这些修正后的p值可以直接用于标准的多重检验程序（如BH或Storey-BH方法），无需修改检验过程本身。

Result: 模拟研究表明，当p值保守时，该方法能显著提升统计功效；当p值精确时，其性能与现有方法相当。在磷酸化数据应用中进一步验证了该方法的实际有效性。

Conclusion: 通过修正p值使其在零假设下精确，该方法为复合零假设下的多重检验问题提供了一个有效的解决方案，能够在不修改标准多重检验程序的情况下提升统计功效。

Abstract: We study a large-scale one-sided multiple testing problem in which test statistics follow normal distributions with unit variance, and the goal is to identify signals with positive mean effects. A common approach is to compute $p$-values under the assumption that all null means are exactly zero and then apply standard multiple testing procedures such as the Benjamini--Hochberg (BH) or Storey--BH method. However, because the null hypothesis is composite, some null means may be strictly negative. In this case, the resulting $p$-values are conservative, leading to a substantial loss of power. Existing methods address this issue by modifying the multiple testing procedure itself, for example through conditioning strategies or discarding rules. In contrast, we focus on correcting the $p$-values so that they are exact under the null. Specifically, we estimate the marginal null distribution of the test statistics within an empirical Bayes framework and construct refined $p$-values based on this estimated distribution. These refined $p$-values can then be directly used in standard multiple testing procedures without modification. Extensive simulation studies show that the proposed method substantially improves power when $p$-values are conservative, while achieving comparable performance to existing methods when $p$-values are exact. An application to phosphorylation data further demonstrates the practical effectiveness of our approach.

</details>


### [25] [Empirical Bayes Method for Large Scale Multiple Testing with Heteroscedastic Errors](https://arxiv.org/abs/2512.24611)
*Kwangok Seo,Johan Lim,Kaiwen Wang,Dohwan Park,Shota Katayama,Xinlei Wang*

Main category: stat.ME

TL;DR: 提出gg-Mix方法，用于异方差正态均值推断问题，无需对方差分布或非零均值分布做限制性假设，能更好地控制FDR并保持高功效


<details>
  <summary>Details</summary>
Motivation: 现有经验贝叶斯方法通常需要限制性假设（如方差服从尺度逆卡方分布、非零均值分布为单峰），当这些假设被违反时，方法要么无法控制FDR，要么功效大幅下降

Method: 提出gg-Mix方法，仅假设正态均值和方差独立，不对其分布施加任何结构限制，通过经验贝叶斯框架进行多重检验

Result: 通过大量数值研究验证了gg-Mix能有效控制FDR并保持高功效，性能优于现有方法；在三个实际数据应用中展示了其实际优势

Conclusion: gg-Mix方法在更宽松的假设下解决了异方差正态均值推断问题，提供了更稳健的FDR控制和更高的检验功效

Abstract: In this paper, we address the normal mean inference problem, which involves testing multiple means of normal random variables with heteroscedastic variances. Most existing empirical Bayes methods for this setting are developed under restrictive assumptions, such as the scaled inverse-chi-squared prior for variances and unimodality for the non-null mean distribution. However, when either of these assumptions is violated, these methods often fail to control the false discovery rate (FDR) at the target level or suffer from a substantial loss of power. To overcome these limitations, we propose a new empirical Bayes method, gg-Mix, which assumes only independence between the normal means and variances, without imposing any structural restrictions on their distributions. We thoroughly evaluate the FDR control and power of gg-Mix through extensive numerical studies and demonstrate its superior performance compared to existing methods. Finally, we apply gg-Mix to three real data examples to further illustrate the practical advantages of our approach.

</details>


### [26] [Bayesian Elastic Net Regression with Structured Prior Dependence](https://arxiv.org/abs/2512.25045)
*Christopher M. Hans,Ningyi Liu*

Main category: stat.ME

TL;DR: 论文提出了一种正交正态分布，用于在贝叶斯弹性网络回归模型中构建先验依赖性，解决了传统方法中回归系数先验独立性的限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯回归的正则化先验通常假设回归系数先验独立（如lasso和弹性网络的标准贝叶斯处理）。虽然独立性在某些数据分析场景中合理，但在先验分布中引入依赖性可以提供更大的建模灵活性。

Method: 引入正交正态分布的一般形式，用于在贝叶斯弹性网络回归模型中构建先验依赖性。提出L1正则化的Zellner's g先验作为特例，连接了惩罚优化文献和回归先验的重要类别。通过微调超参数的标准先验来避免计算中不可处理的归一化常数问题，实现简单快速的Gibbs采样。

Result: 通过模拟和近红外光谱数据示例，证明了在贝叶斯弹性网络回归模型中包含结构化先验依赖性的好处。该方法能够处理先验依赖性，同时保持计算效率。

Conclusion: 正交正态分布为贝叶斯弹性网络回归提供了一种灵活的先验依赖性建模框架，连接了惩罚优化和回归先验文献，并通过计算策略解决了归一化常数的挑战。

Abstract: Many regularization priors for Bayesian regression assume the regression coefficients are a priori independent. In particular this is the case for standard Bayesian treatments of the lasso and the elastic net. While independence may be reasonable in some data-analytic settings, incorporating dependence in these prior distributions provides greater modeling flexibility. This paper introduces the orthant normal distribution in its general form and shows how it can be used to structure prior dependence in the Bayesian elastic net regression model. An L1-regularized version of Zellner's g prior is introduced as a special case, creating a new link between the literature on penalized optimization and an important class of regression priors. Computation is challenging due to an intractable normalizing constant in the prior. We avoid this issue by modifying slightly a standard prior of convenience for the hyperparameters in such a way to enable simple and fast Gibbs sampling of the posterior distribution. The benefit of including structured prior dependence in the Bayesian elastic net regression model is demonstrated through simulation and a near-infrared spectroscopy data example.

</details>


### [27] [Sequential Bayesian parameter-state estimation in dynamical systems with noisy and incomplete observations via a variational framework](https://arxiv.org/abs/2512.25056)
*Liliang Wang,Alex Gorodetsky*

Main category: stat.ME

TL;DR: 提出在线变分推断框架，用于动态系统中未知参数和状态的联合估计与不确定性量化，通过两阶段递归更新实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 数字孪生等应用需要在线更新模型参数和状态知识以支持预测和决策，可靠性和计算速度至关重要。在线参数-状态估计确保计算效率，不确定性量化对可靠预测和决策必不可少。

Method: 提出在线变分推断框架，将联合后验近似分解为参数边际分布和条件状态分布。采用两阶段递归更新：首先通过变分推断近似参数后验，然后基于估计的参数后验使用高斯滤波计算条件状态分布。

Result: 数值实验表明：(1)在低维问题中性能匹配联合粒子滤波器，能准确推断动态和观测模型的未观测状态和未知参数；(2)在混沌Lorenz 96系统中对噪声、部分观测和模型差异保持鲁棒性；(3)在高维对流扩散系统中有效扩展，性能优于联合集合卡尔曼滤波器。

Conclusion: 该在线变分推断框架为动态系统中的参数-状态联合估计提供了高效、可扩展的解决方案，支持数字孪生等应用中的实时不确定性量化需求。

Abstract: Online joint estimation of unknown parameters and states in a dynamical system with uncertainty quantification is crucial in many applications. For example, digital twins dynamically update their knowledge of model parameters and states to support prediction and decision-making. Reliability and computational speed are vital for DTs. Online parameter-state estimation ensures computational efficiency, while uncertainty quantification is essential for making reliable predictions and decisions. In parameter-state estimation, the joint distribution of the state and model parameters conditioned on the data, termed the joint posterior, provides accurate uncertainty quantification. Because the joint posterior is generally intractable to compute, this paper presents an online variational inference framework to compute its approximation at each time step. The approximation is factorized into a marginal distribution over the model parameters and a state distribution conditioned on the parameters. This factorization enables recursive updates through a two-stage procedure: first, the parameter posterior is approximated via variational inference; second, the state distribution conditioned on the parameters is computed using Gaussian filtering based on the estimated parameter posterior. The algorithmic design is supported by a theorem establishing upper bounds on the joint posterior approximation error. Numerical experiments demonstrate that the proposed method (i) matches the performance of the joint particle filter in low-dimensional problems, accurately inferring both unobserved states and unknown parameters of dynamical and observation models; (ii) remains robust under noisy, partial observations and model discrepancies in a chaotic Lorenz 96 system; and (iii) scales effectively to a high-dimensional convection-diffusion system, where it outperforms the joint ensemble Kalman filter.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [28] [Fitted Q Evaluation Without Bellman Completeness via Stationary Weighting](https://arxiv.org/abs/2512.23805)
*Lars van der Laan,Nathan Kallus*

Main category: stat.ML

TL;DR: 提出一种改进的FQE方法，通过重新加权回归步骤来对齐贝尔曼算子的收缩范数，从而在无需贝尔曼完备性的情况下获得更强的评估保证。


<details>
  <summary>Details</summary>
Motivation: 标准FQE方法需要贝尔曼完备性假设，即假设类别在评估贝尔曼算子下是封闭的。这一要求具有挑战性，因为扩大假设类别可能会恶化完备性。问题的根源在于范数不匹配：贝尔曼算子在目标策略的平稳分布下是γ-收缩的，而FQE在行为分布下最小化贝尔曼误差。

Method: 提出一个简单的修正：在每次回归步骤中使用平稳密度比的估计进行重新加权，从而使FQE与贝尔曼算子收缩的范数对齐。这保持了基于回归评估的实用性，同时避免了标准FQE在无实现性或贝尔曼完备性情况下的几何误差放大问题。

Result: 该方法能够在无需实现性或贝尔曼完备性的情况下提供强大的评估保证，避免了标准FQE在此设置下的几何误差放大问题，同时保持了基于回归评估的实用性。

Conclusion: 通过重新加权回归步骤来对齐范数，可以克服标准FQE对贝尔曼完备性的依赖，为离策略评估提供更稳健和实用的解决方案。

Abstract: Fitted Q-evaluation (FQE) is a central method for off-policy evaluation in reinforcement learning, but it generally requires Bellman completeness: that the hypothesis class is closed under the evaluation Bellman operator. This requirement is challenging because enlarging the hypothesis class can worsen completeness. We show that the need for this assumption stems from a fundamental norm mismatch: the Bellman operator is gamma-contractive under the stationary distribution of the target policy, whereas FQE minimizes Bellman error under the behavior distribution. We propose a simple fix: reweight each regression step using an estimate of the stationary density ratio, thereby aligning FQE with the norm in which the Bellman operator contracts. This enables strong evaluation guarantees in the absence of realizability or Bellman completeness, avoiding the geometric error blow-up of standard FQE in this setting while maintaining the practicality of regression-based evaluation.

</details>


### [29] [Energy-Tweedie: Score meets Score, Energy meets Energy](https://arxiv.org/abs/2512.23818)
*Andrej Leban*

Main category: stat.ML

TL;DR: 本文扩展了Tweedie公式到更广泛的能量模型（椭圆分布），建立了去噪后验与能量评分规则之间的联系，并推导出连接能量评分路径导数与噪声边缘分布得分的基本恒等式。


<details>
  <summary>Details</summary>
Motivation: 传统Tweedie公式在去噪和得分估计之间建立了联系，但仅限于特定分布。本文旨在将这种联系扩展到更广泛的能量模型（椭圆分布），并探索通过能量评分规则视角理解去噪后验的新途径。

Method: 1. 将经典Tweedie公式扩展到能量模型（椭圆分布）；2. 将去噪后验视为能量评分规则的优化器；3. 推导连接能量评分路径导数与噪声边缘分布得分的基本恒等式；4. 将该恒等式视为能量评分版本的Tweedie恒等式。

Result: 建立了能量评分与噪声边缘分布得分之间的基本恒等式，该恒等式可用于：1. 得分估计；2. 噪声分布参数估计；3. 在传统扩散模型采样器中使用更广泛的噪声分布。

Conclusion: 本文扩展了Tweedie公式到更广泛的分布族，并通过能量评分规则视角提供了去噪后验的新理解。推导的基本恒等式为得分估计、参数估计以及扩展扩散模型采样器的噪声分布选择提供了理论工具。

Abstract: Denoising and score estimation have long been known to be linked via the classical Tweedie's formula. In this work, we first extend the latter to a wider range of distributions often called "energy models" and denoted elliptical distributions in this work. Next, we examine an alternative view: we consider the denoising posterior $P(X|Y)$ as the optimizer of the energy score (a scoring rule) and derive a fundamental identity that connects the (path-) derivative of a (possibly) non-Euclidean energy score to the score of the noisy marginal. This identity can be seen as an analog of Tweedie's identity for the energy score, and allows for several interesting applications; for example, score estimation, noise distribution parameter estimation, as well as using energy score models in the context of "traditional" diffusion model samplers with a wider array of noising distributions.

</details>


### [30] [Stationary Reweighting Yields Local Convergence of Soft Fitted Q-Iteration](https://arxiv.org/abs/2512.23927)
*Lars van der Laan,Nathan Kallus*

Main category: stat.ML

TL;DR: 论文分析了软FQI在函数近似下的不稳定性，提出了基于平稳分布重加权的改进方法，证明了局部线性收敛，并建议通过逐渐降低温度实现全局收敛。


<details>
  <summary>Details</summary>
Motivation: FQI及其熵正则化变体软FQI是离线强化学习的核心工具，但在函数近似和分布偏移下表现不佳。研究发现软贝尔曼算子在软最优策略的平稳范数下是局部收缩的，而非标准FQI使用的行为范数，这种几何不匹配解释了软Q迭代在缺乏贝尔曼完备性时的不稳定性。

Method: 提出平稳重加权软FQI方法，在每次回归更新中使用当前策略的平稳分布进行重加权。该方法在近似可实现性假设下，证明了局部线性收敛，且权重估计误差呈几何衰减。还建议通过逐渐降低softmax温度来恢复全局收敛性。

Result: 理论分析表明，平稳重加权软FQI在函数近似下具有局部线性收敛性，权重估计误差呈几何衰减。在温和的边界条件下，通过温度逐渐降低的延续方法可以扩展到硬最大值极限。

Conclusion: 通过识别软贝尔曼算子的几何不匹配问题，提出平稳重加权方法有效解决了软FQI在函数近似下的不稳定性，为离线强化学习提供了更稳定的价值迭代框架。

Abstract: Fitted Q-iteration (FQI) and its entropy-regularized variant, soft FQI, are central tools for value-based model-free offline reinforcement learning, but can behave poorly under function approximation and distribution shift. In the entropy-regularized setting, we show that the soft Bellman operator is locally contractive in the stationary norm of the soft-optimal policy, rather than in the behavior norm used by standard FQI. This geometric mismatch explains the instability of soft Q-iteration with function approximation in the absence of Bellman completeness. To restore contraction, we introduce stationary-reweighted soft FQI, which reweights each regression update using the stationary distribution of the current policy. We prove local linear convergence under function approximation with geometrically damped weight-estimation errors, assuming approximate realizability. Our analysis further suggests that global convergence may be recovered by gradually reducing the softmax temperature, and that this continuation approach can extend to the hardmax limit under a mild margin condition.

</details>


### [31] [Implicit geometric regularization in flow matching via density weighted Stein operators](https://arxiv.org/abs/2512.23956)
*Shinto Eguchi*

Main category: stat.ML

TL;DR: 提出γ-Flow Matching，一种密度加权的流匹配方法，通过动态密度加权策略在低密度区域降低回归损失，提高高维空间中的采样效率和向量场平滑度。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配在高维空间中存在根本性低效问题：大部分积分域是低密度"空洞"区域，这些区域的向量场往往混乱或定义不清。需要一种能对齐回归几何与底层概率流的方法。

Method: 提出γ-Flow Matching，采用动态密度加权策略，直接从训练粒子估计目标密度，在空洞区域动态降低回归损失，同时保持流匹配的无模拟特性。

Result: γ-FM显著提高了高维潜在数据集上的向量场平滑度和采样效率，同时表现出对异常值的固有鲁棒性。理论上最小化γ-Stein度量统计流形上的传输成本。

Conclusion: γ-Flow Matching通过密度加权有效解决了高维流匹配中的效率问题，在空洞区域提供隐式Sobolev正则化，抑制高频振荡，是流匹配框架的重要改进。

Abstract: Flow Matching (FM) has emerged as a powerful paradigm for continuous normalizing flows, yet standard FM implicitly performs an unweighted $L^2$ regression over the entire ambient space. In high dimensions, this leads to a fundamental inefficiency: the vast majority of the integration domain consists of low-density ``void'' regions where the target velocity fields are often chaotic or ill-defined. In this paper, we propose {$γ$-Flow Matching ($γ$-FM)}, a density-weighted variant that aligns the regression geometry with the underlying probability flow. While density weighting is desirable, naive implementations would require evaluating the intractable target density. We circumvent this by introducing a Dynamic Density-Weighting strategy that estimates the \emph{target} density directly from training particles. This approach allows us to dynamically downweight the regression loss in void regions without compromising the simulation-free nature of FM. Theoretically, we establish that $γ$-FM minimizes the transport cost on a statistical manifold endowed with the $γ$-Stein metric. Spectral analysis further suggests that this geometry induces an implicit Sobolev regularization, effectively damping high-frequency oscillations in void regions. Empirically, $γ$-FM significantly improves vector field smoothness and sampling efficiency on high-dimensional latent datasets, while demonstrating intrinsic robustness to outliers.

</details>


### [32] [Constructive Approximation of Random Process via Stochastic Interpolation Neural Network Operators](https://arxiv.org/abs/2512.24106)
*Sachin Saini,Uaday Singh*

Main category: stat.ML

TL;DR: 提出带随机系数的随机插值神经网络算子(SINNOs)，证明其在二阶随机过程空间中的有界性、插值精度和逼近能力，并给出定量误差估计。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够有效逼近随机过程的神经网络方法，特别针对COVID-19病例预测等实际应用场景，需要处理随机性和不确定性。

Method: 构建基于sigmoidal激活函数、带有随机系数的随机插值神经网络算子(SINNOs)，在二阶随机过程空间L^2(Ω,ℱ,ℙ)中分析其数学性质。

Result: 证明了SINNOs的有界性、插值精度和逼近能力，包括均方意义、概率意义和路径意义上的收敛性，并利用过程的连续性模给出了定量误差估计。

Conclusion: SINNOs为随机过程逼近提供了有效的神经网络方法，在COVID-19病例预测等实际应用中具有潜在价值，理论分析为其可靠性提供了数学保证。

Abstract: In this paper, we construct a class of stochastic interpolation neural network operators (SINNOs) with random coefficients activated by sigmoidal functions. We establish their boundedness, interpolation accuracy, and approximation capabilities in the mean square sense, in probability, as well as path-wise within the space of second-order stochastic (random) processes \( L^2(Ω, \mathcal{F},\mathbb{P}) \). Additionally, we provide quantitative error estimates using the modulus of continuity of the processes. These results highlight the effectiveness of SINNOs for approximating stochastic processes with potential applications in COVID-19 case prediction.

</details>


### [33] [Topological Spatial Graph Coarsening](https://arxiv.org/abs/2512.24327)
*Anna Calissano,Etienne Lasalle*

Main category: stat.ML

TL;DR: 提出一种基于拓扑保持的空间图粗化方法，通过折叠短边实现图简化，同时利用三角形感知图过滤来保持拓扑特征，无需参数且具有旋转、平移和缩放不变性。


<details>
  <summary>Details</summary>
Motivation: 空间图（如交通网络、分子结构、生物分支结构）的简化问题，需要在减少节点数量的同时保持原始图的整体结构和拓扑特征，这对包含空间信息的图特别重要。

Method: 提出拓扑空间图粗化方法：1）通过折叠短边实现图简化；2）引入三角形感知图过滤来构建拓扑描述符（持久图），捕捉拓扑信息以校准简化程度；3）方法无需参数，并证明具有旋转、平移和缩放不变性。

Result: 在合成和真实空间图上评估，方法能显著减少图的大小，同时保持相关的拓扑信息，证明了方法的有效性。

Conclusion: 提出了一种有效的空间图简化方法，在减少图规模的同时保持了重要的拓扑特征，具有理论保证和实际应用价值。

Abstract: Spatial graphs are particular graphs for which the nodes are localized in space (e.g., public transport network, molecules, branching biological structures). In this work, we consider the problem of spatial graph reduction, that aims to find a smaller spatial graph (i.e., with less nodes) with the same overall structure as the initial one. In this context, performing the graph reduction while preserving the main topological features of the initial graph is particularly relevant, due to the additional spatial information. Thus, we propose a topological spatial graph coarsening approach based on a new framework that finds a trade-off between the graph reduction and the preservation of the topological characteristics. The coarsening is realized by collapsing short edges. In order to capture the topological information required to calibrate the reduction level, we adapt the construction of classical topological descriptors made for point clouds (the so-called persistent diagrams) to spatial graphs. This construction relies on the introduction of a new filtration called triangle-aware graph filtration. Our coarsening approach is parameter-free and we prove that it is equivariant under rotations, translations and scaling of the initial spatial graph. We evaluate the performances of our method on synthetic and real spatial graphs, and show that it significantly reduces the graph sizes while preserving the relevant topological information.

</details>


### [34] [Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach](https://arxiv.org/abs/2512.24927)
*Yuchen Jiao,Na Li,Changxiao Cai,Gen Li*

Main category: stat.ML

TL;DR: 本文挑战了高阶ODE求解器是加速扩散模型采样的唯一途径的观点，提出了一种训练免费的一阶采样器，通过优化DPM评估的时间点布局来提升低NFE下的采样质量。


<details>
  <summary>Details</summary>
Motivation: 当前普遍认为高阶ODE求解器是加速扩散概率模型采样的标准工具，一阶方法天生较慢。本文挑战这一观点，认为除了求解器阶数外，DPM评估在反向时间动态中的时间点布局对低NFE下的采样精度有显著影响。

Method: 提出一种训练免费的一阶采样器，其主导离散化误差与DDIM符号相反。算法上，该方法通过廉价的一步前瞻预测器来近似前向值评估，理论上保证能够近似理想的前向值轨迹同时保持一阶收敛性。

Result: 在标准图像生成基准测试（CIFAR-10、ImageNet、FFHQ、LSUN）上，该采样器在相同NFE预算下持续提升采样质量，有时能够与最先进的高阶采样器竞争甚至超越它们。

Conclusion: DPM评估的时间点布局为加速扩散采样提供了一个额外且基本独立的设计维度，表明一阶方法通过优化评估布局可以在低NFE下实现高质量的采样。

Abstract: Higher-order ODE solvers have become a standard tool for accelerating diffusion probabilistic model (DPM) sampling, motivating the widespread view that first-order methods are inherently slower and that increasing discretization order is the primary path to faster generation. This paper challenges this belief and revisits acceleration from a complementary angle: beyond solver order, the placement of DPM evaluations along the reverse-time dynamics can substantially affect sampling accuracy in the low-neural function evaluation (NFE) regime.
  We propose a novel training-free, first-order sampler whose leading discretization error has the opposite sign to that of DDIM. Algorithmically, the method approximates the forward-value evaluation via a cheap one-step lookahead predictor. We provide theoretical guarantees showing that the resulting sampler provably approximates the ideal forward-value trajectory while retaining first-order convergence. Empirically, across standard image generation benchmarks (CIFAR-10, ImageNet, FFHQ, and LSUN), the proposed sampler consistently improves sample quality under the same NFE budget and can be competitive with, and sometimes outperform, state-of-the-art higher-order samplers. Overall, the results suggest that the placement of DPM evaluations provides an additional and largely independent design angle for accelerating diffusion sampling.

</details>


### [35] [Improving the stability of the covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling](https://arxiv.org/abs/2512.24515)
*Jiani Wei,Xiaocheng Shang*

Main category: stat.ML

TL;DR: 提出改进的CCAdL（mCCAdL）恒温器，通过数值方法近似CCAdL中的附加项，使用对称分裂方法代替欧拉离散化，显著提高数值稳定性，在大规模机器学习应用中优于其他随机梯度方法。


<details>
  <summary>Details</summary>
Motivation: CCAdL恒温器在贝叶斯采样中表现出色，但其使用的移动平均估计协方差矩阵会降低数值积分器的稳定性，限制了最大可用步长。需要改进数值稳定性。

Method: 提出mCCAdL恒温器，使用缩放和平方方法的缩放部分结合截断泰勒级数近似来数值近似CCAdL中附加项的精确解。采用对称分裂方法代替原CCAdL中的欧拉型离散化。

Result: mCCAdL在数值稳定性方面相比原CCAdL有显著改进，同时在大规模机器学习应用中，数值精度显著优于其他流行的随机梯度方法。

Conclusion: mCCAdL恒温器通过改进数值近似方法和离散化方案，成功解决了原CCAdL的稳定性问题，为大规模贝叶斯采样提供了更稳定高效的算法。

Abstract: Stochastic gradient Langevin dynamics and its variants approximate the likelihood of an entire dataset, via random (and typically much smaller) subsets, in the setting of Bayesian sampling. Due to the (often substantial) improvement of the computational efficiency, they have been widely used in large-scale machine learning applications. It has been demonstrated that the so-called covariance-controlled adaptive Langevin (CCAdL) thermostat, which incorporates an additional term involving the covariance matrix of the noisy force, outperforms popular alternative methods. A moving average is used in CCAdL to estimate the covariance matrix of the noisy force, in which case the covariance matrix will converge to a constant matrix in long-time limit. Moreover, it appears in our numerical experiments that the use of a moving average could reduce the stability of the numerical integrators, thereby limiting the largest usable stepsize. In this article, we propose a modified CCAdL (i.e., mCCAdL) thermostat that uses the scaling part of the scaling and squaring method together with a truncated Taylor series approximation to the exponential to numerically approximate the exact solution to the subsystem involving the additional term proposed in CCAdL. We also propose a symmetric splitting method for mCCAdL, instead of an Euler-type discretisation used in the original CCAdL thermostat. We demonstrate in our numerical experiments that the newly proposed mCCAdL thermostat achieves a substantial improvement in the numerical stability over the original CCAdL thermostat, while significantly outperforming popular alternative stochastic gradient methods in terms of the numerical accuracy for large-scale machine learning applications.

</details>


### [36] [MultiRisk: Multiple Risk Control via Iterative Score Thresholding](https://arxiv.org/abs/2512.24587)
*Sunay Joshi,Yan Sun,Hamed Hassani,Edgar Dobriban*

Main category: stat.ML

TL;DR: 本文提出MULTIRISK算法，通过动态规划实现多风险约束的测试时过滤，在LLM对齐任务中有效控制多个安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统在实际应用中的部署，需要调节模型行为的多个维度。现有方法需要轻量级的行为控制机制，能够在测试时根据性能分数与估计阈值的比较来修改输出，同时满足用户定义优先级的多个风险约束。

Method: 形式化了具有用户定义优先级的多风险约束执行问题，提出了两种高效的动态规划算法：MULTIRISK-BASE（直接有限样本阈值选择）和MULTIRISK（利用数据可交换性保证风险的同时控制）。通过引入中间对称风险函数和递归计数风险级别间的跳跃来上下界分析风险。

Result: 在PKU-SafeRLHF数据集的三约束LLM对齐任务中，算法能够将每个个体风险控制在接近目标水平，同时最大化帮助性。MULTIRISK算法在温和假设下实现了几乎紧密的风险控制。

Conclusion: 提出的MULTIRISK框架为多风险约束的测试时过滤提供了有效的解决方案，能够在实际应用中同时控制多个安全风险，为生成式AI系统的行为调节提供了实用工具。

Abstract: As generative AI systems are increasingly deployed in real-world applications, regulating multiple dimensions of model behavior has become essential. We focus on test-time filtering: a lightweight mechanism for behavior control that compares performance scores to estimated thresholds, and modifies outputs when these bounds are violated. We formalize the problem of enforcing multiple risk constraints with user-defined priorities, and introduce two efficient dynamic programming algorithms that leverage this sequential structure. The first, MULTIRISK-BASE, provides a direct finite-sample procedure for selecting thresholds, while the second, MULTIRISK, leverages data exchangeability to guarantee simultaneous control of the risks. Under mild assumptions, we show that MULTIRISK achieves nearly tight control of all constraint risks. The analysis requires an intricate iterative argument, upper bounding the risks by introducing several forms of intermediate symmetrized risk functions, and carefully lower bounding the risks by recursively counting jumps in symmetrized risk functions between appropriate risk levels. We evaluate our framework on a three-constraint Large Language Model alignment task using the PKU-SafeRLHF dataset, where the goal is to maximize helpfulness subject to multiple safety constraints, and where scores are generated by a Large Language Model judge and a perplexity filter. Our experimental results show that our algorithm can control each individual risk at close to the target level.

</details>


### [37] [Sparse Offline Reinforcement Learning with Corruption Robustness](https://arxiv.org/abs/2512.24768)
*Nam Phuong Tran,Andi Nika,Goran Radanovic,Long Tran-Thanh,Debmalya Mandal*

Main category: stat.ML

TL;DR: 该论文研究了高维稀疏马尔可夫决策过程中离线强化学习对强数据污染的鲁棒性，提出了避免使用逐点悲观奖励的actor-critic方法，首次在单策略集中性覆盖和污染条件下提供了非平凡的理论保证。


<details>
  <summary>Details</summary>
Motivation: 在高维稀疏离线强化学习中，当样本数量少于特征维度时，利用稀疏性对于获得非平凡的理论保证至关重要，但这一问题尚未在离线强化学习中得到系统研究。传统鲁棒离线强化学习方法（如LSVI）在稀疏设置下可能失效，需要新的方法来处理数据污染和稀疏性。

Method: 提出了基于稀疏鲁棒估计器oracle的actor-critic方法，避免了传统LSVI中使用的逐点悲观奖励机制。该方法在均匀覆盖和稀疏单集中性假设下进行分析，并扩展到污染设置，证明了在强污染下的鲁棒性。

Result: 首次在高维稀疏MDP中，在单策略集中性覆盖和污染条件下提供了非平凡的理论保证。证明了在传统鲁棒离线强化学习方法可能失败的机制下，学习接近最优策略仍然是可能的。

Conclusion: 该研究为高维稀疏离线强化学习在数据污染和有限覆盖条件下的鲁棒学习提供了新的理论框架和算法，填补了该领域的重要空白，展示了在传统方法失效的机制下仍然可以实现有效学习。

Abstract: We investigate robustness to strong data corruption in offline sparse reinforcement learning (RL). In our setting, an adversary may arbitrarily perturb a fraction of the collected trajectories from a high-dimensional but sparse Markov decision process, and our goal is to estimate a near optimal policy. The main challenge is that, in the high-dimensional regime where the number of samples $N$ is smaller than the feature dimension $d$, exploiting sparsity is essential for obtaining non-vacuous guarantees but has not been systematically studied in offline RL. We analyse the problem under uniform coverage and sparse single-concentrability assumptions. While Least Square Value Iteration (LSVI), a standard approach for robust offline RL, performs well under uniform coverage, we show that integrating sparsity into LSVI is unnatural, and its analysis may break down due to overly pessimistic bonuses. To overcome this, we propose actor-critic methods with sparse robust estimator oracles, which avoid the use of pointwise pessimistic bonuses and provide the first non-vacuous guarantees for sparse offline RL under single-policy concentrability coverage. Moreover, we extend our results to the contaminated setting and show that our algorithm remains robust under strong contamination. Our results provide the first non-vacuous guarantees in high-dimensional sparse MDPs with single-policy concentrability coverage and corruption, showing that learning a near-optimal policy remains possible in regimes where traditional robust offline RL techniques may fail.

</details>
