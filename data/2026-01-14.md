<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 4]
- [stat.CO](#stat.CO) [Total: 5]
- [stat.ML](#stat.ML) [Total: 16]
- [stat.ME](#stat.ME) [Total: 27]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Bayesian Handwriting Evidence Evaluation using MANOVA via Fourier-Based Extracted Features](https://arxiv.org/abs/2601.07534)
*Lampis Tzai,Ioannis Ntzoufras,Silvia Bozza*

Main category: stat.AP

TL;DR: 提出一种基于贝叶斯建模的手写检验模式识别方法，通过傅里叶分析重建字符轮廓，比较六种贝叶斯模型，发现带Normal-LogNormal-LKJ先验的贝叶斯MANOVA模型在区分能力和拟合度方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手写检验需要识别有效且有用的模式，传统方法可能无法充分捕捉书写者间的变异性。本文旨在通过贝叶斯建模方法，更好地描述手写特征并提高书写者区分能力。

Method: 从13名法语母语书写者中选取字符样本，通过傅里叶分析重建字符轮廓，使用前四对傅里叶系数和表面尺寸描述特征。构建六种贝叶斯模型：基于两种似然结构（多元正态模型和考虑字符级变异性的MANOVA模型），每种结合三种先验（共轭正态-逆Wishart先验、分层正态-逆Wishart先验、Normal-LogNormal-LKJ先验）。使用桥采样估计边际似然，通过贝叶斯因子比较模型性能。

Result: 贝叶斯MANOVA结合Normal-LogNormal-LKJ先验在区分能力和模型拟合方面表现最佳。分层先验能有效纳入书写者间变异性，这是区分不同书写者的关键因素。最后进行了先验分布参数设定的敏感性分析。

Conclusion: 提出的贝叶斯建模方法能有效识别手写检验中的有效模式，特别是结合分层先验的MANOVA模型在书写者区分方面表现优异，为法证手写分析提供了统计框架。

Abstract: This paper proposes a novel statistical approach that aims at the identification of valid and useful patterns in handwriting examination via Bayesian modeling. Starting from a sample of characters selected among 13 French native writers, an accurate loop reconstruction can be achieved through Fourier analysis. The contour shape of handwritten characters can be described by the first four pairs of Fourier coefficients and by the surface size. Six Bayesian models are considered for such handwritten features. These models arise from two likelihood structures: (a) a multivariate Normal model, and (b) a MANOVA model that accounts for character-level variability. For each likelihood, three different prior formulations are examined, resulting in distinct Bayesian models: (i) a conjugate Normal-Inverse-Wishart prior, (ii) a hierarchical Normal-Inverse-Wishart prior, and (iii) a Normal-LogNormal-LKJ prior specification. The hierarchical prior formulations are of primary interest because they can incorporate the between-writers variability, a distinguishing element that sets writers apart. These approaches do not allow calculation of the marginal likelihood in a closed-form expression. Therefore, bridge sampling is used to estimate it. The Bayes factor is estimated to compare the performance of the proposed models and to evaluate their efficiency for discriminating purposes. Bayesian MANOVA with Normal-LogNormal-LKJ prior showed an overall better performance, in terms of discriminatory capacity and model fitting. Finally, a sensitivity analysis for the elicitation of the prior distribution parameters is performed.

</details>


### [2] [Cauchy-Gaussian Overbound for Heavy-tailed GNSS Measurement Errors](https://arxiv.org/abs/2601.07299)
*Zhengdao Li,Penggao Yan,Weisong Wen,Li-Ta Hsu*

Main category: stat.AP

TL;DR: 提出一种混合柯西-高斯分布来紧密约束重尾GNSS测量误差，在核心区域使用柯西分布，尾部使用高斯分布，显著降低垂直保护水平


<details>
  <summary>Details</summary>
Motivation: 在完整性监测应用中，需要紧密约束重尾测量误差以满足严格的导航要求。现有的单CDF高斯过界方法对重尾误差约束不够紧密，导致保护水平过高。

Method: 提出混合柯西-高斯过界方法：在误差分布核心区域利用柯西分布的尖锐性，在尾部区域使用高斯分布。开发了确定对称单峰和非对称单峰重尾误差过界参数的流程，并证明了卷积后过界特性保持不变。

Result: 在模拟和真实数据集上，该方法能够在核心和尾部区域紧密约束重尾误差。在位置域中，相比单CDF高斯过界，对称重尾误差的平均垂直保护水平降低15%；相比导航离散包络和两步高斯过界，非对称重尾误差的保护水平降低21%到47%。

Conclusion: 提出的柯西-高斯混合过界方法能够有效紧密约束重尾GNSS测量误差，显著降低保护水平，提高导航完整性监测性能。

Abstract: Overbounds of heavy-tailed measurement errors are essential to meet stringent navigation requirements in integrity monitoring applications. This paper proposes to leverage the bounding sharpness of the Cauchy distribution in the core and the Gaussian distribution in the tails to tightly bound heavy-tailed GNSS measurement errors. We develop a procedure to determine the overbounding parameters for both symmetric unimodal (s.u.) and not symmetric unimodal (n.s.u.) heavy-tailed errors and prove that the overbounding property is preserved through convolution. The experiment results on both simulated and real-world datasets reveal that our method can sharply bound heavy-tailed errors at both core and tail regions. In the position domain, the proposed method reduces the average vertical protection level by 15% for s.u. heavy-tailed errors compared to the single-CDF Gaussian overbound, and by 21% to 47% for n.s.u. heavy-tailed errors compared to the Navigation Discrete ENvelope and two-step Gaussian overbounds.

</details>


### [3] [An evaluation of empirical equations for assessing local scour around bridge piers using global sensitivity analysis](https://arxiv.org/abs/2601.07594)
*Gianna Gavriel,Maria Pregnolato,Francesca Pianosi,Theo Tryfonas,Paul Vardanega*

Main category: stat.AP

TL;DR: 该研究比较了8种桥墩冲刷深度估算公式，使用USGS数据库验证其准确性，发现CIRIA和Froehlich公式在野外条件下最准确，攻击角、墩形和来流深度是最关键参数。


<details>
  <summary>Details</summary>
Motivation: 桥梁冲刷是导致桥梁倒塌的主要原因，现有经验公式大多基于实验室数据或少量现场数据校准，缺乏对多种公式在野外条件下的准确性评估，需要更可靠的冲刷深度估算方法。

Method: 使用USGS冲刷数据库（包含现场和实验室数据）比较8种冲刷公式（包括英国CIRIA C742方法），采用单参数敏感性分析和全局敏感性分析识别关键参数。

Result: CIRIA和Froehlich公式在野外条件下最准确；全局敏感性分析比传统单参数分析提供更多洞察；攻击角、墩形和来流深度是最具影响力的参数。

Conclusion: 减少攻击角、墩形和来流深度这三个参数的不确定性可以最大程度提高冲刷估算精度，全局敏感性分析方法比传统方法更有价值。

Abstract: Bridge scour is a complex phenomenon combining hydrological, geotechnical and structural processes. Bridge scour is the leading cause of bridge collapse, which can bring catastrophic consequences including the loss of life. Estimating scour on bridges is an important task for engineers assessing bridge system performance. Overestimation of scour depths during design may lead to excess spendings on construction whereas underestimation can lead to the collapse of a bridge. Many empirical equations have been developed over the years to assess scour depth at bridge piers. These equations have only been calibrated with laboratory data or very few field data. This paper compares eight equations including the UK CIRIA C742 approach to establish their accuracy using the open access USGS pier-scour database for both field and laboratory conditions. A one-at-the-time sensitivity assessment and a global sensitivity analysis were then applied to identify the most significant parameters in the eight scour equations. The paper shows that using a global approach, i.e. one where all parameters are varied simultaneously, provides more insights than a traditional one-at-the-time approach. The main findings are that the CIRIA and Froehlich equations are the most accurate equations for field conditions, and that angle of attack, pier shape and the approach flow depth are the most influential parameters. Efforts to reduce uncertainty of these three parameters would maximise increase of scour estimate precision.

</details>


### [4] [The Role of Confounders and Linearity in Ecological Inference: A Reassessment](https://arxiv.org/abs/2601.07668)
*Shiro Kuriwaki,Cory McCartan*

Main category: stat.AP

TL;DR: 重新评估生态推理问题，提出新的识别条件，揭示需要控制混杂因素，展示线性模型视角，提供新方法，并通过实际数据验证现有方法会高估种族极化和党派投票


<details>
  <summary>Details</summary>
Motivation: 生态推理问题（EI）仅使用聚合数据的边际均值来估计条件均值，但现有方法存在识别问题，容易产生生态谬误，需要重新评估和改进

Method: 提出新的识别条件形式化，展示聚合过程如何限制条件期望函数为线性，从线性模型视角分析现有EI方法差异，开发基于识别和线性结果的新方法以灵活控制混杂因素

Result: 识别条件显示类似因果推断，可信的生态推理需要控制混杂因素；线性模型视角澄清了文献中常用EI方法的差异；新方法能改进生态推理；实际数据表明所有方法都倾向于高估种族极化和党派投票

Conclusion: 生态推理需要控制混杂因素，聚合过程提供了线性结构辅助估计，但现有方法容易产生生态谬误，新方法能改善推理，但所有方法都倾向于高估某些社会现象

Abstract: Estimating conditional means using only the marginal means available from aggregate data is commonly known as the ecological inference problem (EI). We provide a reassessment of EI, including a new formalization of identification conditions and a demonstration of how these conditions fail to hold in common cases. The identification conditions reveal that, similar to causal inference, credible ecological inference requires controlling for confounders. The aggregation process itself creates additional structure to assist in estimation by restricting the conditional expectation function to be linear in the predictor variable. A linear model perspective also clarifies the differences between the EI methods commonly used in the literature, and when they lead to ecological fallacies. We provide an overview of new methodology which builds on both the identification and linearity results to flexibly control for confounders and yield improved ecological inferences. Finally, using datasets for common EI problems in which the ground truth is fortuitously observed, we show that, while covariates can help, all methods are prone to overestimating both racial polarization and nationalized partisan voting.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [5] [Efficient Data Reduction Via PCA-Guided Quantile Based Sampling](https://arxiv.org/abs/2601.06375)
*Foo Hui-Mean,Yuan-chin Ivan Chang*

Main category: stat.CO

TL;DR: 提出PCA-QS方法，通过主成分分析和分位数抽样在保持统计精度的同时减少数据规模，相比现有方法在计算效率和准确性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 在大规模统计建模中，需要在计算效率和统计准确性之间取得平衡。传统的数据缩减方法（如均匀随机抽样）可能无法充分保留数据的关键特征，需要更智能的抽样方法来同时保持代表性和多样性。

Method: 提出PCA-QS方法：首先将数据投影到主成分上，然后在主成分空间应用分位数抽样，以保留具有代表性和多样性的数据子集。该方法结合了降维和智能抽样策略。

Result: 与均匀随机抽样、杠杆得分抽样和核心集方法相比，PCA-QS在均方误差和关键数据特征保留方面表现更优，同时保持计算效率。该方法适用于多种数据场景。

Conclusion: PCA-QS是一种有效的大规模数据缩减方法，在统计计算中具有广泛的应用潜力，能够在保持计算效率的同时提高统计准确性。

Abstract: In large-scale statistical modeling, reducing data size through subsampling is essential for balancing computational efficiency and statistical accuracy. We propose a new method, Principal Component Analysis guided Quantile Sampling (PCA-QS), which projects data onto principal components and applies quantile-based sampling to retain representative and diverse subsets. Compared with uniform random sampling, leverage score sampling, and coreset methods, PCA-QS consistently achieves lower mean squared error and better preservation of key data characteristics, while also being computationally efficient. This approach is adaptable to a variety of data scenarios and shows strong potential for broad applications in statistical computing.

</details>


### [6] [Extensions of the solidarity principle of the spectral gap for Gibbs samplers to their blocked and collapsed variants](https://arxiv.org/abs/2601.06745)
*Xavier Mak,James P. Hobert*

Main category: stat.CO

TL;DR: 该论文建立了吉布斯采样器与其分块和折叠变体之间的谱联系，推广了谱间隙的团结原理，并证明了分块和折叠吉布斯采样器从完整吉布斯采样器继承谱间隙。


<details>
  <summary>Details</summary>
Motivation: 研究吉布斯采样器不同变体（分块吉布斯采样器和折叠吉布斯采样器）之间的谱关系，理解它们如何从完整吉布斯采样器继承收敛性质。

Method: 推广完整吉布斯采样器的谱间隙团结原理到不同循环和混合吉布斯步骤；建立分块和折叠吉布斯采样器与完整吉布斯采样器之间的谱关系；通过理论分析和反例说明。

Result: 证明了每个循环和混合吉布斯步骤（包括分块和折叠吉布斯采样器）都能从完整吉布斯采样器继承谱间隙；建立了分块和折叠变体之间的精确谱关系；给出了反例表明分块或折叠吉布斯采样器通常不能从另一个分块或折叠吉布斯采样器继承几何遍历性或谱间隙。

Conclusion: 完整吉布斯采样器的谱性质可以传递给其分块和折叠变体，但分块或折叠变体之间通常不能相互继承收敛性质，这为理解不同吉布斯采样器变体的收敛行为提供了理论基础。

Abstract: Connections of a spectral nature are formed between Gibbs samplers and their blocked and collapsed variants. The solidarity principle of the spectral gap for full Gibbs samplers is generalized to different cycles and mixtures of Gibbs steps. This generalized solidarity principle is employed to establish that every cycle and mixture of Gibbs steps, which includes blocked Gibbs samplers and collapsed Gibbs samplers, inherits a spectral gap from a full Gibbs sampler. Exact relations between the spectra corresponding to blocked and collapsed variants of a Gibbs sampler are also established. An example is given to show that a blocked or collapsed Gibbs sampler does not in general inherit geometric ergodicity or a spectral gap from another blocked or collapsed Gibbs sampler.

</details>


### [7] [FormulaCompiler.jl and Margins.jl: Efficient Marginal Effects in Julia](https://arxiv.org/abs/2601.07065)
*Eric Feltham*

Main category: stat.CO

TL;DR: 开发了两个Julia包（Margins.jl和FormulaCompiler.jl），实现了高效的大规模边际效应分析，相比R的marginaleffects包获得622倍速度提升和460倍内存减少。


<details>
  <summary>Details</summary>
Motivation: 现有边际效应分析实现面临计算限制，无法进行大规模分析。需要为Julia统计生态系统提供首个全面且高效的边际效应实现方案。

Method: 1. Margins.jl：提供简洁的双函数API，围绕2×2框架（评估上下文×分析目标）组织分析，支持交互分析、弹性测量、分类混合和稳健标准误。2. FormulaCompiler.jl：提供计算基础，将统计公式转换为零分配、类型专门化的评估器，实现与数据集大小无关的O(p)每行计算。

Result: 相比R的marginaleffects包，平均获得622倍速度提升和460倍内存减少。成功计算了50万观测值的平均边际效应和delta方法标准误，而R因内存耗尽而失败。

Conclusion: 这两个Julia包填补了Julia统计生态系统中高效大规模边际效应分析的空白，提供了首个全面且计算高效的实现方案。

Abstract: Marginal effects analysis is fundamental to interpreting statistical models, yet existing implementations face computational constraints that limit analysis at scale. We introduce two Julia packages that address this gap. Margins.jl provides a clean two-function API organizing analysis around a 2-by-2 framework: evaluation context (population vs profile) by analytical target (effects vs predictions). The package supports interaction analysis through second differences, elasticity measures, categorical mixtures for representative profiles, and robust standard errors. FormulaCompiler.jl provides the computational foundation, transforming statistical formulas into zero-allocation, type-specialized evaluators that enable O(p) per-row computation independent of dataset size. Together, these packages achieve 622x average speedup and 460x memory reduction compared to R's marginaleffects package, with successful computation of average marginal effects and delta-method standard errors on 500,000 observations where R fails due to memory exhaustion, providing the first comprehensive and efficient marginal effects implementation for Julia's statistical ecosystem.

</details>


### [8] [Penalized Likelihood Optimization for Adaptive Neighborhood Clustering in Time-to-Event Data with Group-Level Heterogeneity](https://arxiv.org/abs/2601.07446)
*Alessandra Ragni,Lara Cavinato,Francesca Ieva*

Main category: stat.CO

TL;DR: 提出一种同时进行患者聚类和共享脆弱性生存建模的新框架，通过惩罚似然方法自适应学习患者相似性矩阵，用于分层数据中的风险患者分层


<details>
  <summary>Details</summary>
Motivation: 现有生存模型通过脆弱项考虑群体异质性但不发现潜在患者亚组，而大多数聚类方法忽略层次结构且不与生存结果联合估计。需要一种能同时处理患者聚类和分层生存建模的方法

Method: 提出惩罚似然方法框架，通过改进的谱聚类自适应学习患者到患者相似性矩阵，同时进行患者聚类和共享脆弱性生存建模，考虑群体成员关系

Result: 模拟研究显示模型能恢复潜在聚类并正确估计风险参数。应用于2020-2021年伦巴第地区COVID-19住院心衰患者队列，识别出具有不同风险特征的临床有意义亚组，突出呼吸系统合并症和医院水平变异性的作用

Conclusion: 该框架为分层数据设置中的风险患者分层提供了灵活且可解释的工具，能同时考虑患者聚类和群体异质性

Abstract: The identification of patient subgroups with comparable event-risk dynamics plays a key role in supporting informed decision-making in clinical research. In such settings, it is important to account for the inherent dependence that arises when individuals are nested within higher-level units, such as hospitals. Existing survival models account for group-level heterogeneity through frailty terms but do not uncover latent patient subgroups, while most clustering methods ignore hierarchical structure and are not estimated jointly with survival outcomes. In this work, we introduce a new framework that simultaneously performs patient clustering and shared-frailty survival modeling through a penalized likelihood approach. The proposed methodology adaptively learns a patient-to-patient similarity matrix via a modified version of spectral clustering, enabling cluster formation directly from estimated risk profiles while accounting for group membership. A simulation study highlights the proposed model's ability to recover latent clusters and to correctly estimate hazard parameters. We apply our method to a large cohort of heart-failure patients hospitalized with COVID-19 between 2020 and 2021 in the Lombardy region (Italy), identifying clinically meaningful subgroups characterized by distinct risk profiles and highlighting the role of respiratory comorbidities and hospital-level variability in shaping mortality outcomes. This framework provides a flexible and interpretable tool for risk-based patient stratification in hierarchical data settings.

</details>


### [9] [Population-Adjusted Indirect Treatment Comparison with the outstandR Package in R](https://arxiv.org/abs/2601.07532)
*Nathan Green,Antonio Remiro-Azocar*

Main category: stat.CO

TL;DR: outstandR是一个R包，为人群调整间接比较（PAIC）提供统一框架，包含加权、回归、G计算和多重插补边缘化等方法，解决跨试验人群差异问题。


<details>
  <summary>Details</summary>
Motivation: 在卫生技术评估中，当缺乏头对头临床试验时需要进行间接治疗比较。常见挑战是：一个治疗有患者个体数据（IPD），另一个只有汇总数据（ALD），且试验人群存在效应修饰因子差异。现有方法（如MAIC和STC）的软件实现分散或功能有限。

Method: 开发outstandR R包，提供人群调整间接比较的统一框架。除了标准加权和回归方法外，还实现了最大似然和贝叶斯框架下的高级G计算方法，以及多重插补边缘化（MIM）来处理非可折叠性和缺失数据问题。

Result: outstandR通过简化协变量模拟、模型标准化和对比估计的工作流程，能够在复杂决策场景中实现稳健且兼容的证据合成。

Conclusion: outstandR为人群调整间接比较提供了全面统一的软件解决方案，解决了现有方法实现分散的问题，支持更稳健的卫生技术评估证据合成。

Abstract: Indirect treatment comparisons (ITCs) are essential in Health Technology Assessment (HTA) when head-to-head clinical trials are absent. A common challenge arises when attempting to compare a treatment with available individual patient data (IPD) against a competitor with only reported aggregate-level data (ALD), particularly when trial populations differ in effect modifiers. While methods such as Matching-Adjusted Indirect Comparison (MAIC) and Simulated Treatment Comparison (STC) exist to adjust for these cross-trial differences, software implementations have often been fragmented or limited in scope. This article introduces outstandR, an R package designed to provide a comprehensive and unified framework for population-adjusted indirect comparison (PAIC). Beyond standard weighting and regression approaches, outstandR implements advanced G-computation methods within both maximum likelihood and Bayesian frameworks, and Multiple Imputation Marginalization (MIM) to address non-collapsibility and missing data. By streamlining the workflow of covariate simulation, model standardization, and contrast estimation, outstandR enables robust and compatible evidence synthesis in complex decision-making scenarios.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [10] [Physics-informed Gaussian Process Regression in Solving Eigenvalue Problem of Linear Operators](https://arxiv.org/abs/2601.06462)
*Tianming Bai,Jiannan Yang*

Main category: stat.ML

TL;DR: 该论文提出了一种基于物理信息高斯过程回归的特征值问题求解方法，通过构建转移函数型指标来检测特征值/特征函数，解决了传统方法中零源项导致的预测均值平凡和边缘似然退化问题。


<details>
  <summary>Details</summary>
Motivation: 在应用物理信息高斯过程回归求解特征值问题 $(\mathcal{L}-λ)u = 0$ 时，零源项会导致预测均值变得平凡（trivial）且边缘似然退化，这构成了一个根本性挑战。需要一种新的方法来有效检测特征值和特征函数。

Method: 受系统辨识启发，构建了一个转移函数型指标来检测未知的特征值/特征函数，该指标基于物理信息高斯过程后验。当 $λ$ 对应于偏微分算子 $\mathcal{L}$ 的特征值时，后验协方差才非平凡，反映了非平凡特征空间的存在性，且后验的任何样本都位于线性算子的特征空间中。

Result: 通过多个数值示例（包括线性和非线性特征值问题）证明了所提方法的有效性。该方法能够准确识别特征值，且后验样本确实位于相应特征空间中。

Conclusion: 提出的基于物理信息高斯过程回归的特征值检测方法成功解决了零源项带来的挑战，为特征值问题提供了一种新的有效求解框架，适用于线性和非线性特征值问题。

Abstract: Applying Physics-Informed Gaussian Process Regression to the eigenvalue problem $(\mathcal{L}-λ)u = 0$ poses a fundamental challenge, where the null source term results in a trivial predictive mean and a degenerate marginal likelihood. Drawing inspiration from system identification, we construct a transfer function-type indicator for the unknown eigenvalue/eigenfunction using the physics-informed Gaussian Process posterior. We demonstrate that the posterior covariance is only non-trivial when $λ$ corresponds to an eigenvalue of the partial differential operator $\mathcal{L}$, reflecting the existence of a non-trivial eigenspace, and any sample from the posterior lies in the eigenspace of the linear operator. We demonstrate the effectiveness of the proposed approach through several numerical examples with both linear and non-linear eigenvalue problems.

</details>


### [11] [Inference-Time Alignment for Diffusion Models via Doob's Matching](https://arxiv.org/abs/2601.06514)
*Jinyuan Chang,Chenguang Duan,Yuling Jiao,Yi Xu,Jerry Zhijian Yang*

Main category: stat.ML

TL;DR: 提出Doob's matching框架，基于Doob's h-transform进行扩散模型推理时对齐，通过梯度惩罚回归同时估计h函数及其梯度，获得一致的引导估计器，并提供了理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的推理时对齐旨在不重新训练基础模型的情况下，将预训练扩散模型适配到目标分布，保持基础模型的生成能力的同时在推理时强制执行所需属性。现有引导方法通过额外漂移项修改采样动态，但需要更好的理论框架和估计方法。

Method: 提出Doob's matching框架，基于Doob's h-transform将引导公式化为底层Doob's h函数对数的梯度。采用梯度惩罚回归同时估计h函数及其梯度，获得一致的引导估计器。该方法在理论上建立了估计引导的非渐进收敛率，并分析了生成分布到目标分布的2-Wasserstein距离收敛保证。

Result: 建立了Doob's matching框架的理论基础，证明了引导估计器的非渐进收敛率，并分析了可控扩散过程的生成分布到目标分布的收敛保证。该方法为扩散模型推理时对齐提供了理论严谨的解决方案。

Conclusion: Doob's matching为扩散模型推理时对齐提供了基于Doob's h-transform的理论框架，通过梯度惩罚回归获得一致的引导估计，并建立了严格的理论收敛保证，为可控扩散过程提供了理论基础。

Abstract: Inference-time alignment for diffusion models aims to adapt a pre-trained diffusion model toward a target distribution without retraining the base score network, thereby preserving the generative capacity of the base model while enforcing desired properties at the inference time. A central mechanism for achieving such alignment is guidance, which modifies the sampling dynamics through an additional drift term. In this work, we introduce Doob's matching, a novel framework for guidance estimation grounded in Doob's $h$-transform. Our approach formulates guidance as the gradient of logarithm of an underlying Doob's $h$-function and employs gradient-penalized regression to simultaneously estimate both the $h$-function and its gradient, resulting in a consistent estimator of the guidance. Theoretically, we establish non-asymptotic convergence rates for the estimated guidance. Moreover, we analyze the resulting controllable diffusion processes and prove non-asymptotic convergence guarantees for the generated distributions in the 2-Wasserstein distance.

</details>


### [12] [Dimension-reduced outcome-weighted learning for estimating individualized treatment regimes in observational studies](https://arxiv.org/abs/2601.06782)
*Sungtaek Son,Eardi Lila,Kwun Chuen Gary Chan*

Main category: stat.ML

TL;DR: 提出一种新的充分降维方法，直接针对潜在结果间的差异，识别捕捉治疗效果异质性的低维协变量子空间，通过结果加权学习更准确估计最优个体化治疗策略，并整合核协变量平衡处理观察数据。


<details>
  <summary>Details</summary>
Motivation: 现有个体化治疗策略方法在处理高维协变量时存在困难，限制了准确性、可解释性和实际应用性。需要开发能够有效处理高维数据、同时保持解释性和实用性的方法。

Method: 提出一种新颖的充分降维方法，直接针对潜在结果间的差异，识别捕捉治疗效果异质性的低维协变量子空间。该方法整合核协变量平衡处理观察数据，允许治疗分配依赖于完整协变量集，避免限制性假设。

Result: 该方法在温和正则条件下实现普遍一致性（风险收敛于贝叶斯风险）。通过模拟研究和重症监护室脓毒症患者数据分析（确定谁应接受经胸超声心动图）验证了有限样本性能。

Conclusion: 提出的方法能够有效处理高维协变量，提高个体化治疗策略的准确性和可解释性，特别适用于观察性数据，具有实际临床应用价值。

Abstract: Individualized treatment regimes (ITRs) aim to improve clinical outcomes by assigning treatment based on patient-specific characteristics. However, existing methods often struggle with high-dimensional covariates, limiting accuracy, interpretability, and real-world applicability. We propose a novel sufficient dimension reduction approach that directly targets the contrast between potential outcomes and identifies a low-dimensional subspace of the covariates capturing treatment effect heterogeneity. This reduced representation enables more accurate estimation of optimal ITRs through outcome-weighted learning. To accommodate observational data, our method incorporates kernel-based covariate balancing, allowing treatment assignment to depend on the full covariate set and avoiding the restrictive assumption that the subspace sufficient for modeling heterogeneous treatment effects is also sufficient for confounding adjustment. We show that the proposed method achieves universal consistency, i.e., its risk converges to the Bayes risk, under mild regularity conditions. We demonstrate its finite sample performance through simulations and an analysis of intensive care unit sepsis patient data to determine who should receive transthoracic echocardiography.

</details>


### [13] [Constrained Density Estimation via Optimal Transport](https://arxiv.org/abs/2601.06830)
*Yinan Hu,Estaban Tabak*

Main category: stat.ML

TL;DR: 提出一种在期望约束下进行密度估计的新框架，通过最小化估计密度与先验之间的Wasserstein距离，同时满足一组函数的期望值达到或超过给定阈值


<details>
  <summary>Details</summary>
Motivation: 在密度估计中，经常需要满足特定的期望约束（如金融应用中的风险指标），传统方法难以同时处理这些约束并保持与先验分布的相似性

Method: 基于Wasserstein距离最小化框架，引入正则化不等式约束来减轻目标测度的伪影，针对非光滑约束开发了退火式算法

Result: 框架在合成数据和金融领域的实际概念验证示例中均表现出有效性，能够处理复杂的期望约束

Conclusion: 该框架为期望约束下的密度估计提供了一种灵活有效的解决方案，特别适用于金融等需要满足特定期望指标的应用领域

Abstract: A novel framework for density estimation under expectation constraints is proposed. The framework minimizes the Wasserstein distance between the estimated density and a prior, subject to the constraints that the expected value of a set of functions adopts or exceeds given values. The framework is generalized to include regularization inequalities to mitigate the artifacts in the target measure. An annealing-like algorithm is developed to address non-smooth constraints, with its effectiveness demonstrated through both synthetic and proof-of-concept real world examples in finance.

</details>


### [14] [Robust Mean Estimation under Quantization](https://arxiv.org/abs/2601.07074)
*Pedro Abdalla,Junren Chen*

Main category: stat.ML

TL;DR: 提出两种量化与对抗性污染下的均值估计方法：单比特量化与部分量化，均达到对数因子最优


<details>
  <summary>Details</summary>
Motivation: 在数据量化（如单比特量化）和存在对抗性污染的现实场景下，需要设计鲁棒的均值估计算法

Method: 构建了两种多变量鲁棒估计器：1）单比特设置，每个比特仅依赖单个样本；2）部分量化设置，允许使用少量未量化数据

Result: 两种估计器均达到对数因子的最优性能

Conclusion: 在量化与对抗性污染约束下，所提方法实现了接近理论极限的均值估计性能

Abstract: We consider the problem of mean estimation under quantization and adversarial corruption. We construct multivariate robust estimators that are optimal up to logarithmic factors in two different settings. The first is a one-bit setting, where each bit depends only on a single sample, and the second is a partial quantization setting, in which the estimator may use a small fraction of unquantized data.

</details>


### [15] [The Impact of Anisotropic Covariance Structure on the Training Dynamics and Generalization Error of Linear Networks](https://arxiv.org/abs/2601.06961)
*Taishi Watanabe,Ryo Karakida,Jun-nosuke Teramae*

Main category: stat.ML

TL;DR: 研究数据各向异性（尖峰协方差结构）对两层线性网络学习动态和泛化误差的影响，发现学习过程分为两个阶段，并推导出泛化误差的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的成功很大程度上取决于训练数据的统计结构。虽然各向同性数据上的学习动态和泛化已有充分研究，但显著各向异性对这些关键方面的影响尚未完全理解。

Method: 在线性回归设置中，使用具有尖峰协方差结构的数据各向异性模型，分析两层线性网络的学习动态和泛化误差。

Result: 学习动态分为两个不同阶段：初始阶段由输入-输出相关性主导，后续阶段由数据结构的其他主方向主导。推导出泛化误差的解析表达式，量化数据尖峰结构与学习任务的对齐如何改善性能。

Conclusion: 研究结果提供了关于数据各向异性如何塑造学习轨迹和最终性能的深刻理论见解，为理解更高级网络架构中的复杂交互奠定了基础。

Abstract: The success of deep neural networks largely depends on the statistical structure of the training data. While learning dynamics and generalization on isotropic data are well-established, the impact of pronounced anisotropy on these crucial aspects is not yet fully understood. We examine the impact of data anisotropy, represented by a spiked covariance structure, a canonical yet tractable model, on the learning dynamics and generalization error of a two-layer linear network in a linear regression setting. Our analysis reveals that the learning dynamics proceed in two distinct phases, governed initially by the input-output correlation and subsequently by other principal directions of the data structure. Furthermore, we derive an analytical expression for the generalization error, quantifying how the alignment of the spike structure of the data with the learning task improves performance. Our findings offer deep theoretical insights into how data anisotropy shapes the learning trajectory and final performance, providing a foundation for understanding complex interactions in more advanced network architectures.

</details>


### [16] [Optimal Transport under Group Fairness Constraints](https://arxiv.org/abs/2601.07144)
*Linus Bleistein,Mathieu Dagréou,Francisco Andrade,Thomas Boudou,Aurélien Bellet*

Main category: stat.ML

TL;DR: 提出了一种基于最优运输的群体公平性新定义，开发了FairSinkhorn算法计算完全公平运输方案，并提出了两种松弛策略来平衡公平性与匹配质量。


<details>
  <summary>Details</summary>
Motivation: 在资源分配和职位匹配中，确保匹配算法的公平性是一个关键挑战。现有方法在公平性和匹配质量之间存在权衡，需要新的方法来平衡这两方面。

Method: 1. 提出新的群体公平性定义，要求任意两个群体间个体匹配概率满足预定目标；2. 开发FairSinkhorn算法计算完全公平运输方案；3. 提出两种松弛策略：惩罚OT问题和双层优化学习成本函数。

Result: 1. FairSinkhorn能高效计算完全公平运输方案；2. 对惩罚OT问题推导了新颖的有限样本复杂度保证；3. 建立了学习成本函数在未见数据上产生公平匹配的保证界限；4. 实证结果展示了公平性与性能之间的权衡。

Conclusion: 该研究为最优运输中的公平性匹配提供了新的理论框架和实用算法，通过松弛策略有效平衡了公平性要求与匹配质量，为公平资源分配提供了有力工具。

Abstract: Ensuring fairness in matching algorithms is a key challenge in allocating scarce resources and positions. Focusing on Optimal Transport (OT), we introduce a novel notion of group fairness requiring that the probability of matching two individuals from any two given groups in the OT plan satisfies a predefined target. We first propose \texttt{FairSinkhorn}, a modified Sinkhorn algorithm to compute perfectly fair transport plans efficiently. Since exact fairness can significantly degrade matching quality in practice, we then develop two relaxation strategies. The first one involves solving a penalised OT problem, for which we derive novel finite-sample complexity guarantees. This result is of independent interest as it can be generalized to arbitrary convex penalties. Our second strategy leverages bilevel optimization to learn a ground cost that induces a fair OT solution, and we establish a bound guaranteeing that the learned cost yields fair matchings on unseen data. Finally, we present empirical results that illustrate the trade-offs between fairness and performance.

</details>


### [17] [Match Made with Matrix Completion: Efficient Learning under Matching Interference](https://arxiv.org/abs/2601.06982)
*Zhiyuan Tang,Wanning Chen,Kan Xu*

Main category: stat.ML

TL;DR: 论文提出利用矩阵补全技术加速双边匹配市场中高维匹配奖励的学习，解决了匹配干扰下的技术挑战，并开发了具有理论保证的新估计器。


<details>
  <summary>Details</summary>
Motivation: 双边匹配市场需要学习供需双方的匹配质量以设计有效的匹配策略，但实践中匹配奖励维度高且存在匹配干扰（匹配约束导致观测数据不独立），现有矩阵补全方法无法直接应用。

Method: 利用匹配奖励矩阵的低秩结构，采用核范数正则化进行矩阵补全；开发"双重增强"估计器以获得逐项保证；将方法扩展到在线学习场景中的最优匹配和稳定匹配问题。

Result: 证明了核范数正则化在匹配干扰下仍有效，获得了近最优的Frobenius范数保证；双重增强估计器提供了近最优的逐项保证；在线学习场景中获得了改进的遗憾界。

Conclusion: 提出的矩阵补全方法能有效处理匹配干扰，加速奖励学习，为双边匹配市场提供了理论保证和实用工具，在合成数据和劳动力市场数据上验证了实际价值。

Abstract: Matching markets face increasing needs to learn the matching qualities between demand and supply for effective design of matching policies. In practice, the matching rewards are high-dimensional due to the growing diversity of participants. We leverage a natural low-rank matrix structure of the matching rewards in these two-sided markets, and propose to utilize matrix completion to accelerate reward learning with limited offline data. A unique property for matrix completion in this setting is that the entries of the reward matrix are observed with matching interference -- i.e., the entries are not observed independently but dependently due to matching or budget constraints. Such matching dependence renders unique technical challenges, such as sub-optimality or inapplicability of the existing analytical tools in the matrix completion literature, since they typically rely on sample independence. In this paper, we first show that standard nuclear norm regularization remains theoretically effective under matching interference. We provide a near-optimal Frobenius norm guarantee in this setting, coupled with a new analytical technique. Next, to guide certain matching decisions, we develop a novel ``double-enhanced'' estimator, based off the nuclear norm estimator, with a near-optimal entry-wise guarantee. Our double-enhancement procedure can apply to broader sampling schemes even with dependence, which may be of independent interest. Additionally, we extend our approach to online learning settings with matching constraints such as optimal matching and stable matching, and present improved regret bounds in matrix dimensions. Finally, we demonstrate the practical value of our methods using both synthetic data and real data of labor markets.

</details>


### [18] [Multi-environment Invariance Learning with Missing Data](https://arxiv.org/abs/2601.07247)
*Yiran Jia*

Main category: stat.ML

TL;DR: 该论文提出了一种在缺失结果数据情况下的不变性学习估计器，用于处理领域泛化问题，通过理论分析和实验验证了其在有偏插补模型下的有效性。


<details>
  <summary>Details</summary>
Motivation: 在领域泛化中，不变性学习通过捕捉跨环境的稳定特征来提高模型泛化能力。然而，实际应用中由于数据收集成本高或复杂，往往无法获得每个环境的完整结果数据，这种数据缺失限制了模型充分利用环境异质性的能力，阻碍了因果洞察和鲁棒预测的发展。

Method: 从缺失结果的不变性目标中推导出估计器，建立了变量选择性质和ℓ₂误差收敛率的非渐近保证，这些保证受到缺失数据比例和跨环境插补模型质量的影响。

Result: 通过大量模拟实验和UCI自行车共享数据集的应用验证，结果显示即使依赖有偏的插补模型，只要偏差在合理范围内，该估计器仍然高效且能实现更低的预测误差。

Conclusion: 提出的估计器能够有效处理缺失结果数据的问题，在领域泛化中实现更好的因果解释和鲁棒预测，为实际应用中数据不完整的情况提供了可行的解决方案。

Abstract: Learning models that can handle distribution shifts is a key challenge in domain generalization. Invariance learning, an approach that focuses on identifying features invariant across environments, improves model generalization by capturing stable relationships, which may represent causal effects when the data distribution is encoded within a structural equation model (SEM) and satisfies modularity conditions. This has led to a growing body of work that builds on invariance learning, leveraging the inherent heterogeneity across environments to develop methods that provide causal explanations while enhancing robust prediction. However, in many practical scenarios, obtaining complete outcome data from each environment is challenging due to the high cost or complexity of data collection. This limitation in available data hinders the development of models that fully leverage environmental heterogeneity, making it crucial to address missing outcomes to improve both causal insights and robust prediction. In this work, we derive an estimator from the invariance objective under missing outcomes. We establish non-asymptotic guarantees on variable selection property and $\ell_2$ error convergence rates, which are influenced by the proportion of missing data and the quality of imputation models across environments. We evaluate the performance of the new estimator through extensive simulations and demonstrate its application using the UCI Bike Sharing dataset to predict the count of bike rentals. The results show that despite relying on a biased imputation model, the estimator is efficient and achieves lower prediction error, provided the bias is within a reasonable range.

</details>


### [19] [Conditional Normalizing Flows for Forward and Backward Joint State and Parameter Estimation](https://arxiv.org/abs/2601.07013)
*Luke S. Lagunowich,Guoxiang Grayson Tong,Daniele E. Schiavazzi*

Main category: stat.ML

TL;DR: 本文综述了基于条件归一化流的非线性滤波方法，用于处理非高斯、多模态分布的非线性系统状态估计问题，并在自动驾驶和流行病学等应用中评估了其性能。


<details>
  <summary>Details</summary>
Motivation: 传统滤波算法（如卡尔曼滤波、粒子滤波）在处理具有任意非高斯、多模态分布的非线性系统时性能下降，需要更先进的非线性滤波方法。

Method: 采用基于条件归一化流的非线性滤波方法，使用MLP、Transformer或Mamba-SSM等架构生成条件嵌入，并测试最优传输启发的动力学损失项来缓解过参数化问题。

Result: 在自动驾驶和患者群体动力学等应用中评估了这些方法的性能，特别关注时间反演和链式预测处理能力，并在COVID-19 SIR系统预测和参数估计中评估了不同条件策略的效果。

Conclusion: 基于条件归一化流的非线性滤波方法为处理非高斯、多模态分布的非线性系统状态估计提供了有前景的解决方案，特别是在复杂动态系统建模中表现出优势。

Abstract: Traditional filtering algorithms for state estimation -- such as classical Kalman filtering, unscented Kalman filtering, and particle filters - show performance degradation when applied to nonlinear systems whose uncertainty follows arbitrary non-Gaussian, and potentially multi-modal distributions. This study reviews recent approaches to state estimation via nonlinear filtering based on conditional normalizing flows, where the conditional embedding is generated by standard MLP architectures, transformers or selective state-space models (like Mamba-SSM). In addition, we test the effectiveness of an optimal-transport-inspired kinetic loss term in mitigating overparameterization in flows consisting of a large collection of transformations. We investigate the performance of these approaches on applications relevant to autonomous driving and patient population dynamics, paying special attention to how they handle time inversion and chained predictions. Finally, we assess the performance of various conditioning strategies for an application to real-world COVID-19 joint SIR system forecasting and parameter estimation.

</details>


### [20] [Variational Approximations for Robust Bayesian Inference via Rho-Posteriors](https://arxiv.org/abs/2601.07325)
*EL Mahdi Khribch,Pierre Alquier*

Main category: stat.ML

TL;DR: 提出一种基于温度依赖Gibbs后验的PAC-Bayesian框架，实现了ρ-后验的理论保证，同时通过变分近似解决了计算难题，首次实现了具有严格有限样本保证的实用ρ-后验推断。


<details>
  <summary>Details</summary>
Motivation: ρ-后验框架虽然提供了具有明确污染率和最优收敛保证的通用贝叶斯估计，但由于需要对参考分布进行优化，计算困难，阻碍了实际应用。

Method: 开发PAC-Bayesian框架，通过温度依赖的Gibbs后验恢复理论保证，推导具有明确速率的有限样本oracle不等式，并引入继承精确ρ-后验鲁棒性的可处理变分近似。

Result: 数值实验表明该方法在保持计算可行性的同时达到了理论污染率，首次实现了具有严格有限样本保证的实用ρ-后验推断。

Conclusion: 该工作成功地将ρ-后验的理论优势与实际计算可行性相结合，为鲁棒贝叶斯推断提供了首个实用的实现方案。

Abstract: The $ρ$-posterior framework provides universal Bayesian estimation with explicit contamination rates and optimal convergence guarantees, but has remained computationally difficult due to an optimization over reference distributions that precludes intractable posterior computation. We develop a PAC-Bayesian framework that recovers these theoretical guarantees through temperature-dependent Gibbs posteriors, deriving finite-sample oracle inequalities with explicit rates and introducing tractable variational approximations that inherit the robustness properties of exact $ρ$-posteriors. Numerical experiments demonstrate that this approach achieves theoretical contamination rates while remaining computationally feasible, providing the first practical implementation of $ρ$-posterior inference with rigorous finite-sample guarantees.

</details>


### [21] [Local EGOP for Continuous Index Learning](https://arxiv.org/abs/2601.07061)
*Alex Kokot,Anand Hemmady,Vydhourie Thiyageswaran,Marina Meila*

Main category: stat.ML

TL;DR: 提出连续索引学习设置，其中多变量函数在每个点仅沿少数方向变化。提出Local EGOP学习算法，利用期望梯度外积作为度量和逆协方差，在噪声流形上实现自适应核学习。


<details>
  <summary>Details</summary>
Motivation: 多变量函数在每个点通常只沿少数方向变化，为高效估计，学习算法需要在每个点自适应到捕获函数局部变异性的子空间。现有方法在处理高维噪声和自适应学习方面存在不足。

Method: 提出Local EGOP学习算法，将任务建模为噪声流形上的核自适应。算法利用期望梯度外积（EGOP）二次形式同时作为目标分布的度量和逆协方差，实现递归学习。

Result: 理论证明：在监督噪声流形假设下，Local EGOP学习能适应函数正则性，实现内在维度学习率，即使面对任意高维噪声。实证结果：与深度学习特征学习能力相当，在连续单索引设置中优于两层神经网络。

Conclusion: Local EGOP学习算法能有效适应函数局部变异结构，在噪声流形上实现高效学习，为高维数据中的自适应学习提供了理论保证和实证优势。

Abstract: We introduce the setting of continuous index learning, in which a function of many variables varies only along a small number of directions at each point. For efficient estimation, it is beneficial for a learning algorithm to adapt, near each point $x$, to the subspace that captures the local variability of the function $f$. We pose this task as kernel adaptation along a manifold with noise, and introduce Local EGOP learning, a recursive algorithm that utilizes the Expected Gradient Outer Product (EGOP) quadratic form as both a metric and inverse-covariance of our target distribution. We prove that Local EGOP learning adapts to the regularity of the function of interest, showing that under a supervised noisy manifold hypothesis, intrinsic dimensional learning rates are achieved for arbitrarily high-dimensional noise. Empirically, we compare our algorithm to the feature learning capabilities of deep learning. Additionally, we demonstrate improved regression quality compared to two-layer neural networks in the continuous single-index setting.

</details>


### [22] [Covariance-Driven Regression Trees: Reducing Overfitting in CART](https://arxiv.org/abs/2601.07281)
*Likun Zhang,Wei Ma*

Main category: stat.ML

TL;DR: 提出基于协方差的回归树分裂准则(CovRT)，相比CART的ERM准则更能抵抗过拟合，在高维设置下预测精度更优


<details>
  <summary>Details</summary>
Motivation: 决策树（如CART）容易过拟合，尤其是在深度较深或样本量较小时。传统的剪枝方法（预剪枝和后剪枝）虽然能约束无信息分支的生长，但需要补充方法

Method: 提出协方差驱动的分裂准则(CovRT)，替代CART中使用的经验风险最小化(ERM)准则。该方法通过协方差分析产生更平衡稳定的分裂，更有效识别具有真实信号的协变量

Result: 建立了CovRT的oracle不等式，证明在高维设置下其预测精度与CART相当。在模拟和真实任务中，CovRT相比CART实现了更优的预测精度

Conclusion: CovRT作为CART的补充方法，通过协方差驱动的分裂准则有效减少过拟合，在保持可解释性的同时提升预测性能

Abstract: Decision trees are powerful machine learning algorithms, widely used in fields such as economics and medicine for their simplicity and interpretability. However, decision trees such as CART are prone to overfitting, especially when grown deep or the sample size is small. Conventional methods to reduce overfitting include pre-pruning and post-pruning, which constrain the growth of uninformative branches. In this paper, we propose a complementary approach by introducing a covariance-driven splitting criterion for regression trees (CovRT). This method is more robust to overfitting than the empirical risk minimization criterion used in CART, as it produces more balanced and stable splits and more effectively identifies covariates with true signals. We establish an oracle inequality of CovRT and prove that its predictive accuracy is comparable to that of CART in high-dimensional settings. We find that CovRT achieves superior prediction accuracy compared to CART in both simulations and real-world tasks.

</details>


### [23] [Position: Don't be Afraid of Over-Smoothing And Over-Squashing](https://arxiv.org/abs/2601.07419)
*Niklas Kormann,Benjamin Doerr,Johannes F. Lutzeyer*

Main category: stat.ML

TL;DR: 该论文挑战了GNN研究中过度关注过平滑和过挤压问题的现状，认为这些现象在实际应用中并不像假设的那样关键，性能下降更多源于感受野信息不足而非过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 挑战当前GNN研究中过度关注过平滑和过挤压问题的现状，认为这些现象在实际应用中并不像假设的那样关键，性能下降更多源于感受野信息不足而非过平滑问题。

Method: 通过多个标准基准数据集上的广泛实验，分析准确性、过平滑和模型深度之间的关系；检验缓解过挤压的架构干预措施的实际效果；提出使用统计量测量标签相关信息的基础分布来理解其局部化和因子化。

Result: 实验表明：1) 准确性和过平滑大多不相关；2) 即使使用缓解技术，最优模型深度仍然很小；3) 缓解过挤压的架构干预未能带来显著的性能提升；4) 相关信息通常在图的k跳邻域内局部化和因子化分布。

Conclusion: 过平滑和过挤压在实际应用中作用有限，建议理论研究范式转变：通过分析学习任务和数据集的统计特性来理解相关信息分布，而非过度关注这些理论现象。

Abstract: Over-smoothing and over-squashing have been extensively studied in the literature on Graph Neural Networks (GNNs) over the past years. We challenge this prevailing focus in GNN research, arguing that these phenomena are less critical for practical applications than assumed. We suggest that performance decreases often stem from uninformative receptive fields rather than over-smoothing. We support this position with extensive experiments on several standard benchmark datasets, demonstrating that accuracy and over-smoothing are mostly uncorrelated and that optimal model depths remain small even with mitigation techniques, thus highlighting the negligible role of over-smoothing. Similarly, we challenge that over-squashing is always detrimental in practical applications. Instead, we posit that the distribution of relevant information over the graph frequently factorises and is often localised within a small k-hop neighbourhood, questioning the necessity of jointly observing entire receptive fields or engaging in an extensive search for long-range interactions. The results of our experiments show that architectural interventions designed to mitigate over-squashing fail to yield significant performance gains. This position paper advocates for a paradigm shift in theoretical research, urging a diligent analysis of learning tasks and datasets using statistics that measure the underlying distribution of label-relevant information to better understand their localisation and factorisation.

</details>


### [24] [Nonparametric Kernel Clustering with Bandit Feedback](https://arxiv.org/abs/2601.07535)
*Victor Thuot,Sebastian Vogt,Debarghya Ghoshdastidar,Nicolas Verzelen*

Main category: stat.ML

TL;DR: 提出非参数聚类与老虎机反馈框架，使用核方法将问题转化为在RKHS中聚类核均值嵌入，开发KABC算法并分析采样预算


<details>
  <summary>Details</summary>
Motivation: 现有基于老虎机反馈的聚类方法依赖子高斯分布假设，主要适用于线性可分聚类，缺乏实际应用价值。需要开发适用于真实世界数据集的非参数聚类方法

Method: 采用核方法，将非参数聚类问题转化为在再生核希尔伯特空间(RKHS)中根据核均值嵌入对臂进行聚类。提出KABC算法，该算法自适应于未知的信噪比参数

Result: 引入基于最大均值差异(MMD)和RKHS中方差的问题信噪比概念。KABC算法无需该参数作为输入，但能实现实例相关的理论保证

Conclusion: 提出的非参数聚类框架突破了现有方法的子高斯分布限制，适用于真实世界数据集，KABC算法具有理论正确性保证和自适应特性

Abstract: Clustering with bandit feedback refers to the problem of partitioning a set of items, where the clustering algorithm can sequentially query the items to receive noisy observations. The problem is formally posed as the task of partitioning the arms of an N-armed stochastic bandit according to their underlying distributions, grouping two arms together if and only if they share the same distribution, using samples collected sequentially and adaptively. This setting has gained attention in recent years due to its applicability in recommendation systems and crowdsourcing. Existing works on clustering with bandit feedback rely on a strong assumption that the underlying distributions are sub-Gaussian. As a consequence, the existing methods mainly cover settings with linearly-separable clusters, which has little practical relevance. We introduce a framework of ``nonparametric clustering with bandit feedback'', where the underlying arm distributions are not constrained to any parametric, and hence, it is applicable for active clustering of real-world datasets. We adopt a kernel-based approach, which allows us to reformulate the nonparametric problem as the task of clustering the arms according to their kernel mean embeddings in a reproducing kernel Hilbert space (RKHS). Building on this formulation, we introduce the KABC algorithm with theoretical correctness guarantees and analyze its sampling budget. We introduce a notion of signal-to-noise ratio for this problem that depends on the maximum mean discrepancy (MMD) between the arm distributions and on their variance in the RKHS. Our algorithm is adaptive to this unknown quantity: it does not require it as an input yet achieves instance-dependent guarantees.

</details>


### [25] [Dual-Level Models for Physics-Informed Multi-Step Time Series Forecasting](https://arxiv.org/abs/2601.07640)
*Mahdi Nasiri,Johanna Kortelainen,Simo Särkkä*

Main category: stat.ML

TL;DR: 提出一种结合概率输入预测与物理信息输出预测的双层策略，用于动态系统的多步预测，通过LSTM增强的随机状态转移模型预测输入，再通过物理信息神经网络生成输出预测。


<details>
  <summary>Details</summary>
Motivation: 准确的多步时间序列预测对物理过程的自动控制和优化至关重要。传统机理模型受限于不完全的数学知识，而纯数据驱动的机器学习模型在动态环境中泛化能力差。需要结合两者优势来解决这些限制。

Method: 提出双层策略：第一层使用混合方法预测输入变量，将LSTM网络集成到概率状态转移模型中；第二层将这些随机预测的输入序列输入到物理信息神经网络中，生成多步输出预测。

Result: 实验结果表明，混合输入预测模型相比传统状态转移模型获得更高的对数似然和更低的均方误差。物理信息神经网络在输入预测模型驱动下，在均方误差和对数似然方面优于纯数据驱动模型，在多个测试案例中表现出更强的泛化和预测性能。

Conclusion: 该研究提出的双层物理信息预测框架成功结合了概率输入预测和物理约束输出预测，为动态系统的多步预测提供了有效解决方案，克服了纯机理模型和纯数据驱动模型的局限性。

Abstract: This paper develops an approach for multi-step forecasting of dynamical systems by integrating probabilistic input forecasting with physics-informed output prediction. Accurate multi-step forecasting of time series systems is important for the automatic control and optimization of physical processes, enabling more precise decision-making. While mechanistic-based and data-driven machine learning (ML) approaches have been employed for time series forecasting, they face significant limitations. Incomplete knowledge of process mathematical models limits mechanistic-based direct employment, while purely data-driven ML models struggle with dynamic environments, leading to poor generalization. To address these limitations, this paper proposes a dual-level strategy for physics-informed forecasting of dynamical systems. On the first level, input variables are forecast using a hybrid method that integrates a long short-term memory (LSTM) network into probabilistic state transition models (STMs). On the second level, these stochastically predicted inputs are sequentially fed into a physics-informed neural network (PINN) to generate multi-step output predictions. The experimental results of the paper demonstrate that the hybrid input forecasting models achieve a higher log-likelihood and lower mean squared errors (MSE) compared to conventional STMs. Furthermore, the PINNs driven by the input forecasting models outperform their purely data-driven counterparts in terms of MSE and log-likelihood, exhibiting stronger generalization and forecasting performance across multiple test cases.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [26] [Graph structure learning for stable processes](https://arxiv.org/abs/2601.06264)
*Florian Brück,Sebastian Engelke,Stanislav Volgushev*

Main category: stat.ME

TL;DR: 提出Ising-Hüsler-Reiss过程，一种新的多元Lévy过程，用于稀疏建模具有不同稳定性指数的边缘稳定过程之间的路径条件独立结构


<details>
  <summary>Details</summary>
Motivation: 需要一种能够同时处理不同稳定性指数的多元稳定过程，并允许稀疏条件独立图建模的方法，同时能够数据驱动地建模跳跃不对称性

Method: 引入Ising-Hüsler-Reiss过程，通过精度矩阵的零元素编码条件独立图，使用Ising型参数化对Lévy测度的每个象限权重进行建模，开发基于均匀小时间近似的图形结构和不对称参数的一致估计器

Result: 开发了新的估计方法，在模拟和股票收益数据应用中验证了方法的有效性

Conclusion: 提出的Ising-Hüsler-Reiss过程为具有不同稳定性指数的多元稳定过程提供了灵活的稀疏建模框架，能够同时捕捉条件独立结构和跳跃不对称性

Abstract: We introduce Ising-Hüsler-Reiss processes, a new class of multivariate Lévy processes that allows for sparse modeling of the path-wise conditional independence structure between marginal stable processes with different stability indices. The underlying conditional independence graph is encoded as zeroes in a suitable precision matrix. An Ising-type parametrization of the weights for each orthant of the Lévy measure allows for data-driven modeling of asymmetry of the jumps while retaining an arbitrary sparse graph. We develop consistent estimators for the graphical structure and asymmetry parameters, relying on a new uniform small-time approximation for Lévy processes. The methodology is illustrated in simulations and a real data application to modeling dependence of stock returns.

</details>


### [27] [A Framework for Estimating Restricted Mean Survival Time Difference using Pseudo-observations](https://arxiv.org/abs/2601.06296)
*Man Jin,Yixin Fang*

Main category: stat.ME

TL;DR: 提出一个针对生存分析中限制平均生存时间差异的靶向学习框架，使用伪观测的TMLE估计器，并开发了复制参考方法进行敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 在临床试验中，需要可靠地估计时间到事件结果的限制平均生存时间差异，特别是在存在右删失的情况下，需要稳健的估计方法和敏感性分析工具。

Method: 开发靶向学习框架，将目标估计量定义为RMST差异，引入基于伪观测的靶向最小损失估计器，并开发复制参考方法进行右删失的敏感性分析。

Result: 提出的TL框架能够有效估计RMST差异，通过真实数据应用展示了方法的可行性和实用性。

Conclusion: 该靶向学习框架为临床试验中RMST差异估计提供了有效的统计方法，结合TMLE和敏感性分析，增强了结果的可信度。

Abstract: A targeted learning (TL) framework is developed to estimate the difference in the restricted mean survival time (RMST) for a clinical trial with time-to-event outcomes. The approach starts by defining the target estimand as the RMST difference between investigational and control treatments. Next, an efficient estimation method is introduced: a targeted minimum loss estimator (TMLE) utilizing pseudo-observations. Moreover, a version of the copy reference (CR) approach is developed to perform a sensitivity analysis for right-censoring. The proposed TL framework is demonstrated using a real data application.

</details>


### [28] [Empirical Likelihood Test for Common Invariant Subspace of Multilayer Networks based on Monte Carlo Approximation](https://arxiv.org/abs/2601.06390)
*Qianqian Yao*

Main category: stat.ME

TL;DR: 提出基于经验似然(EL)的检验方法，用于检测多层网络是否共享相同的潜在子空间


<details>
  <summary>Details</summary>
Motivation: 多层网络广泛用于表示现实复杂系统中对象间的多样化关系模式，识别跨网络层的共同不变子空间具有重要意义，可以过滤层特定噪声、促进跨网络比较、降低维度并提取共享结构特征

Method: 基于经验似然(EL)的假设检验方法，零假设为所有网络层共享相同的潜在子空间，备择假设为至少两层子空间不同。通过蒙特卡洛近似研究检验的渐近行为

Result: 模拟结果显示该方法达到满意的检验水平和功效，并通过实际数据应用展示了其实用性

Conclusion: 提出的经验似然检验方法能有效检测多层网络是否共享共同潜在子空间，具有理论和实际应用价值

Abstract: Multilayer (or multiple) networks are widely used to represent diverse patterns of relationships among objects in increasingly complex real-world systems. Identifying a common invariant subspace across network layers has become an active area of research, as such a subspace can filter out layer-specific noise, facilitate cross-network comparisons, reduce dimensionality, and extract shared structural features of scientific interest. One statistical approach to detecting a common subspace is hypothesis testing, which evaluates whether the observed networks share a common latent structure. In this paper, we propose an empirical likelihood (EL) based test for this purpose. The null hypothesis states that all network layers share the same invariant subspace, whereas under the alternative hypothesis at least two layers differ in their subspaces. We study the asymptotic behavior of the proposed test via Monte Carlo approximation and assess its finite-sample performance through extensive simulations. The simulation results demonstrate that the proposed method achieves satisfactory size and power, and its practical utility is further illustrated with a real-data application.

</details>


### [29] [Triple-dyad ratio estimation for the $p_1$ model](https://arxiv.org/abs/2601.06481)
*Qunqiang Feng,Yaru Tian,Ting Yan*

Main category: stat.ME

TL;DR: 针对p₁模型提出了一种新的参数估计方法——三重二元组比率估计器，解决了该模型40年来缺乏渐近理论的问题，证明了估计量的一致性和渐近正态性，并开发了检验互惠效应的统计方法。


<details>
  <summary>Details</summary>
Motivation: p₁模型提出40年来，其渐近理论一直是个未解决的开放性问题，特别是最大似然估计量的一致性缺乏理论保证。本文旨在填补这一理论空白，为p₁模型提供具有统计保证的参数估计方法。

Method: 提出基于三重二元组指标序列比率的参数估计方法。三重二元组指标是三个二元组指标的乘积。该方法称为三重二元组比率估计器，具有显式表达式，可扩展到数百万节点的大型网络。

Result: 建立了三重二元组比率估计器在节点数趋于无穷时的一致性和渐近正态性。基于渐近结果开发了检验有向网络中互惠效应的统计量。对密度和互惠参数估计量提出了解析偏差校正公式，确保有效推断。

Conclusion: 成功解决了p₁模型长期存在的渐近理论问题，提出的估计器在大规模网络中与MLE性能相当，为有向网络分析提供了理论保证的统计推断工具。

Abstract: Although the $p_1$ model was proposed 40 years ago, little progress has been made to address asymptotic theories in this model, that is, neither consistency of the maximum likelihood estimator (MLE) nor other parameter estimation with statistical guarantees is understood. This problem has been acknowledged as a long-standing open problem. To address it, we propose a novel parametric estimation method based on the ratios of the sum of a sequence of triple-dyad indicators to another one, where a triple-dyad indicator means the product of three dyad indicators. Our proposed estimators, called \emph{triple-dyad ratio estimator}, have explicit expressions and can be scaled to very large networks with millions of nodes. We establish the consistency and asymptotic normality of the triple-dyad ratio estimator when the number of nodes reaches infinity. Based on the asymptotic results, we develop a test statistic for evaluating whether is a reciprocity effect in directed networks. The estimators for the density and reciprocity parameters contain bias terms, where analytical bias correction formulas are proposed to make valid inference. Numerical studies demonstrate the findings of our theories and show that the estimator is comparable to the MLE in large networks.

</details>


### [30] [Bayesian Optimization of Noisy Log-Likelihoods Evaluated by Particle Filters -- One Parameter Case --](https://arxiv.org/abs/2601.06545)
*Genshiro Kitagawa*

Main category: stat.ME

TL;DR: 使用贝叶斯优化来最大化粒子滤波估计的似然函数，解决了传统优化方法在噪声、计算昂贵、不可微分似然函数上的困难。


<details>
  <summary>Details</summary>
Motivation: 粒子滤波估计的似然函数通常具有噪声、计算昂贵且不可微分的特性，这使得传统优化方法难以直接应用或不可靠。

Method: 使用高斯过程代理模型对噪声似然函数表面进行建模，并采用平衡探索与利用的采集函数，通过有限次似然评估找到最大化点。

Result: 数值实验表明，贝叶斯优化在观测噪声存在的情况下能够提供稳健稳定的估计，与卡尔曼滤波获得的精确最大似然解相比，使用均方误差指标定量评估了估计精度。

Conclusion: 贝叶斯优化是解决似然最大化问题的有前景的替代方法，特别是在穷举搜索或基于梯度的方法不切实际的情况下。

Abstract: Likelihood functions evaluated using particle filters are typically noisy, computationally expensive, and non-differentiable due to Monte Carlo variability. These characteristics make conventional optimization methods difficult to apply directly or potentially unreliable. This paper investigates the use of Bayesian optimization for maximizing log-likelihood functions estimated by particle filters. By modeling the noisy log-likelihood surface with a Gaussian process surrogate and employing an acquisition function that balances exploration and exploitation, the proposed approach identifies the maximizer using a limited number of likelihood evaluations. Through numerical experiments, we demonstrate that Bayesian optimization provides robust and stable estimation in the presence of observation noise. The results suggest that Bayesian optimization is a promising alternative for likelihood maximization problems where exhaustive search or gradient-based methods are impractical. The estimation accuracy is quantitatively assessed using mean squared error metrics by comparison with the exact maximum likelihood solution obtained via the Kalman filter.

</details>


### [31] [Censored Graphical Horseshoe: Bayesian sparse precision matrix estimation with censored and missing data](https://arxiv.org/abs/2601.06671)
*The Tien Mai,Sayantan Banerjee*

Main category: stat.ME

TL;DR: 提出了Censored Graphical Horseshoe (CGHS)方法，将Graphical Horseshoe扩展到能处理删失和缺失数据的高斯图模型


<details>
  <summary>Details</summary>
Motivation: 高斯图模型是研究多元数据条件依赖关系的重要工具，但现有方法（如Graphical Horseshoe）假设数据完全观测，无法处理现实研究中普遍存在的删失和缺失数据问题

Method: 通过引入潜变量表示，CGHS方法能够处理不完整观测，同时保留Horseshoe先验的自适应全局-局部收缩特性；开发了高效的Gibbs采样器进行后验计算

Result: 建立了删失和缺失情况下后验行为的新理论结果，填补了频率主义Lasso方法未解决的问题；通过大量模拟实验证明，CGHS相比惩罚似然方法能持续提高估计精度

Conclusion: CGHS为处理删失和缺失数据的高斯图模型提供了一个有效的贝叶斯框架，已在GitHub上开源实现为GHScenmis包

Abstract: Gaussian graphical models provide a powerful framework for studying conditional dependencies in multivariate data, with widespread applications spanning biomedical, environmental sciences, and other data-rich scientific domains. While the Graphical Horseshoe (GHS) method has emerged as a state-of-the-art Bayesian method for sparse precision matrix estimation, existing approaches assume fully observed data and thus fail in the presence of censoring or missingness, which are pervasive in real-world studies. In this paper, we develop the Censored Graphical Horseshoe (CGHS), a novel Bayesian framework that extends the GHS to censored and arbitrarily missing Gaussian data. By introducing a latent-variable representation, CGHS accommodates incomplete observations while retaining the adaptive global-local shrinkage properties of the Horseshoe prior. We derive efficient Gibbs samplers for posterior computation and establish new theoretical results on posterior behavior under censoring and missingness, filling a gap not addressed by frequentist Lasso-based methods. Through extensive simulations, we demonstrate that CGHS consistently improves estimation accuracy compared to penalized likelihood approaches. Our methods are implemented in the package GHScenmis available on Github: https://github.com/tienmt/ghscenmis .

</details>


### [32] [Mittag Leffler Distributions Estimation and Autoregressive Framework](https://arxiv.org/abs/2601.06610)
*Monika S. Dhull*

Main category: stat.ME

TL;DR: 本文提出使用经验拉普拉斯变换方法估计Mittag-Leffler分布参数，并建立包含该分布的AR(1)模型，应用于高频交易数据。


<details>
  <summary>Details</summary>
Motivation: Mittag-Leffler分布在金融时间序列建模中具有应用价值，特别是高频交易数据。需要有效的参数估计方法来应用该分布。

Method: 使用经验拉普拉斯变换方法估计Mittag-Leffler分布参数。建立两种AR(1)模型：一种以Mittag-Leffler分布作为边际分布，另一种作为创新项。对两种模型都应用经验拉普拉斯变换进行参数估计。

Result: 模拟研究表明提出的方法能提供满意的参数估计结果。成功将Mittag-Leffler分布应用于高频交易数据的实际分析，并展示了三参数Mittag-Leffler分布的估计。

Conclusion: 经验拉普拉斯变换方法是估计Mittag-Leffler分布参数的有效方法，建立的AR(1)模型框架为金融时间序列分析提供了新工具。

Abstract: This work deals with the estimation of parameters of Mittag-Leffler (ML($α, σ$)) distribution. We estimate the parameters of ML($α, σ$) using empirical Laplace transform method. The simulation study indicates that the proposed method provides satisfactory results. The real life application of ML($α, σ$) distribution on high frequency trading data is also demonstrated. We also provide the estimation of three-parameter Mittag-Leffler distribution using empirical Laplace transform. Additionally, we establish an autoregressive model of order 1, incorporating the Mittag-Leffler distribution as marginals in one scenario and as innovation terms in another. We apply empirical Laplace transform method to estimate the model parameters and provide the simulation study for the same.

</details>


### [33] [R-Estimation with Right-Censored Data](https://arxiv.org/abs/2601.06685)
*Glen A. Satten,Mo Li,Ni Zhao,Robert L. Strawderman*

Main category: stat.ME

TL;DR: 提出了一种针对右删失线性模型的R估计量推广方法，将Wilcoxon秩得分函数的R估计量推广到非线性秩得分函数，并建立了与现有估计方程类的联系。


<details>
  <summary>Details</summary>
Motivation: 解决右删失结果线性模型中R估计量的直接推广问题。现有方法主要针对完整数据，需要开发适用于删失数据的R估计量推广框架。

Method: 提出了Wilcoxon（线性秩）得分函数的秩和估计方程的自然推广，证明其可表示为Ritov (1990)和Tsiatis (1990)提出的估计方程类的成员。进一步将结果推广到有界非线性秩得分函数类。利用残差分布函数的自洽估计量和中累积分布函数及其推广。

Result: 建立了R估计量与现有估计方程类的精确表示关系，获得了渐近性质和方差估计作为这些表示结果的直接推论。

Conclusion: 成功将R估计量推广到右删失线性模型，建立了与经典估计方程理论的联系，为删失数据的秩方法提供了理论基础和实用工具。

Abstract: This paper considers the problem of directly generalizing the R-estimator under a linear model formulation with right-censored outcomes. We propose a natural generalization of the rank and corresponding estimating equation for the R-estimator in the case of the Wilcoxon (i.e., linear-in-ranks) score function, and show how it can respectively be exactly represented as members of the classes of estimating equations proposed in Ritov (1990) and Tsiatis (1990). We then establish analogous results for a large class of bounded nonlinear-in-ranks score functions. Asymptotics and variance estimation are obtained as straightforward consequences of these representation results. The self-consistent estimator of the residual distribution function, and the mid-cumulative distribution function (and, where needed, a generalization of it), play critical roles in these developments.

</details>


### [34] [Nonparametric contaminated Gaussian mixture of regressions](https://arxiv.org/abs/2601.06695)
*Sphiwe B. Skhosana,Weixin Yao*

Main category: stat.ME

TL;DR: 提出半参数和非参数污染高斯混合回归模型，用于在存在轻度异常值时稳健估计参数和非参数项，同时实现基于模型的聚类和异常检测。


<details>
  <summary>Details</summary>
Motivation: 传统的半参数和非参数混合回归模型基于高斯误差分布假设，对异常值和重尾误差分布敏感。需要一种能同时处理聚类和异常检测的稳健方法。

Method: 提出污染高斯混合回归模型，使用污染高斯误差分布。开发了两种算法：EM型算法用于最大似然估计，ECM型算法用于局部似然核估计。

Result: 通过广泛的模拟研究验证了模型的稳健性，并在实际数据中展示了实用价值。模型能同时进行基于模型的聚类和异常检测。

Conclusion: 提出的污染高斯混合回归模型为存在异常值时的半参数和非参数回归提供了稳健的解决方案，实现了聚类和异常检测的双重功能。

Abstract: Semi- and non-parametric mixture of regressions are a very useful flexible class of mixture of regressions in which some or all of the parameters are non-parametric functions of the covariates. These models are, however, based on the Gaussian assumption of the component error distributions. Thus, their estimation is sensitive to outliers and heavy-tailed error distributions. In this paper, we propose semi- and non-parametric contaminated Gaussian mixture of regressions to robustly estimate the parametric and/or non-parametric terms of the models in the presence of mild outliers. The virtue of using a contaminated Gaussian error distribution is that we can simultaneously perform model-based clustering of observations and model-based outlier detection. We propose two algorithms, an expectation-maximization (EM)-type algorithm and an expectation-conditional-maximization (ECM)-type algorithm, to perform maximum likelihood and local-likelihood kernel estimation of the parametric and non-parametric of the proposed models, respectively. The robustness of the proposed models is examined using an extensive simulation study. The practical utility of the proposed models is demonstrated using real data.

</details>


### [35] [Adversarially Perturbed Precision Matrix Estimation](https://arxiv.org/abs/2601.06807)
*Yiling Xie*

Main category: stat.ME

TL;DR: 提出对抗扰动精度矩阵估计框架，通过适应不同扰动几何，既能恢复现有分布鲁棒方法，又能启发新的矩自适应方法，具有稀疏恢复和对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 精度矩阵估计是多变量统计和现代机器学习的基础问题。受对抗训练最新发展的启发，需要开发既能处理分布不确定性又具有鲁棒性的精度矩阵估计方法。

Method: 提出对抗扰动精度矩阵估计框架，通过适应不同扰动几何，该框架不仅能恢复现有的分布鲁棒方法，还能启发新的矩自适应方法。框架被证明与正则化精度矩阵估计渐近等价。

Result: 理论证明该框架具有渐近正态性，能识别扰动引入的渐近偏差，并确定在何种条件下扰动估计在渐近意义上是无偏的。数值实验在合成和真实数据上展示了优越性能。

Conclusion: 对抗扰动精度矩阵估计框架具有理论保证和实践优势，既能实现稀疏恢复，又具有对抗鲁棒性，为精度矩阵估计提供了新的有效方法。

Abstract: Precision matrix estimation is a fundamental topic in multivariate statistics and modern machine learning. This paper proposes an adversarially perturbed precision matrix estimation framework, motivated by recent developments in adversarial training. The proposed framework is versatile for the precision matrix problem since, by adapting to different perturbation geometries, the proposed framework can not only recover the existing distributionally robust method but also inspire a novel moment-adaptive approach to precision matrix estimation, proven capable of sparsity recovery and adversarial robustness. Notably, the proposed perturbed precision matrix framework is proven to be asymptotically equivalent to regularized precision matrix estimation, and the asymptotic normality can be established accordingly. The resulting asymptotic distribution highlights the asymptotic bias introduced by perturbation and identifies conditions under which the perturbed estimation can be unbiased in the asymptotic sense. Numerical experiments on both synthetic and real data demonstrate the desirable performance of the proposed adversarially perturbed approach in practice.

</details>


### [36] [Compounded Linear Failure Rate Distribution: Properties, Simulation and Analysis](https://arxiv.org/abs/2601.07249)
*Suchismita Das,Akul Ameya,Cahyani Karunia Putri*

Main category: stat.ME

TL;DR: 提出线性失效率模型的新扩展，通过增加形状参数提高灵活性，用于建模LFR分布变量集的最小生存时间


<details>
  <summary>Details</summary>
Motivation: 为了更好地捕捉现实世界寿命数据，需要更灵活的失效时间模型。现有线性失效率模型在建模复杂寿命模式方面存在局限性

Method: 扩展线性失效率模型，增加形状参数；推导统计性质（平均剩余寿命、平均不活动时间、矩、分位数、顺序统计量）；使用最大似然估计方法估计参数；进行模拟研究和真实数据分析

Result: 模型具有递增、浴盆形和反浴盆形失效率函数；模拟研究表明估计量表现良好；三个真实数据集验证显示模型优于经典替代方案

Conclusion: 提出的扩展LFR模型在建模复杂寿命数据方面具有优越性，为可靠性分析和生存研究提供了更灵活的工具

Abstract: This paper proposes a new extension of the linear failure rate (LFR) model to better capture real-world lifetime data. The model incorporates an additional shape parameter to increase flexibility. It helps model the minimum survival time from a set of LFR distributed variables. We define the model, derive certain statistical properties such as the mean residual life, the mean inactivity time, moments, quantile, order statistics and also discuss the results on stochastic orders of the proposed distribution. The proposed model has increasing, bathtub shaped and inverse bathtub shaped hazard rate function. We use the method of maximum likelihood estimation to estimate the unknown parameters. We conduct simulation studies to examine the behavior of the estimators. We also use three real datasets to evaluate the model, which turns out superior compared to classical alternatives.

</details>


### [37] [Likelihood-Based Regression for Weibull Accelerated Life Testing Model Under Censored Data](https://arxiv.org/abs/2601.06890)
*Rahul Konar,Ramnivas Jat,Neeraj Joshi,Raghu Nandan Sengupta*

Main category: stat.ME

TL;DR: 提出基于Weibull分布的加速寿命测试模型，考虑温度和电压作为应力变量，采用渐进混合删失和自适应渐进混合删失方案，建立两步估计框架：先通过最大似然估计Weibull参数，再通过回归模型建立应力-寿命关系。


<details>
  <summary>Details</summary>
Motivation: 在加速寿命测试中，需要处理应力变量（如温度、电压）对寿命分布的影响，同时面对复杂的删失数据收集方案（如渐进混合删失），需要开发灵活可靠的统计方法来建模应力依赖的可靠性行为。

Method: 1) 基于Weibull分布建立加速寿命测试模型，形状参数和尺度参数均依赖于应力变量；2) 采用渐进混合删失(PHC)和自适应渐进混合删失(APHC)方案收集数据；3) 开发两步估计框架：首先通过最大似然估计Weibull参数，并建立估计量的一致性和渐近正态性；然后将参数估计与应力变量通过回归模型连接，量化应力-寿命关系。

Result: 通过大量模拟研究验证了所提方法在各种参数设置下的有限样本性能，并提供了实际数据示例展示方法的实用性。证明了估计量的一致性和渐近正态性，为加速寿命测试中复杂删失方案下的可靠性建模提供了灵活框架。

Conclusion: 所提出的框架为加速寿命测试研究中复杂删失方案下的应力依赖可靠性行为建模提供了灵活方法，能够有效处理温度和电压等应力变量对Weibull寿命分布的影响，具有理论和实践价值。

Abstract: In this paper, we investigate accelerated life testing (ALT) models based on the Weibull distribution with stress-dependent shape and scale parameters. Temperature and voltage are treated as stress variables influencing the lifetime distribution. Data are assumed to be collected under Progressive Hybrid Censoring (PHC) and Adaptive Progressive Hybrid Censoring (APHC). A two-step estimation framework is developed. First, the Weibull parameters are estimated via maximum likelihood, and the consistency and asymptotic normality of the estimators are established under both censoring schemes. Second, the resulting parameter estimates are linked to the stress variables through a regression model to quantify the stress-lifetime relationship. Extensive simulations are conducted to examine finite-sample performance under a range of parameter settings, and a data illustration is also presented to showcase practical relevance. The proposed framework provides a flexible approach for modeling stress-dependent reliability behavior in ALT studies under complex censoring schemes.

</details>


### [38] [Characterization of multi-way binary tables with uniform margins and fixed correlations](https://arxiv.org/abs/2601.07369)
*Roberto Fontana,Elisa Perrone,Fabio Rapallo*

Main category: stat.ME

TL;DR: 提出几何框架描述具有相同两两相关性但高阶交互可能不同的联合分布可行集，该集合形成凸多面体，可用于探索依赖结构的完整空间。


<details>
  <summary>Details</summary>
Motivation: 在许多涉及二元变量的应用中，通常只能获得成对依赖度量（如相关性）。但对于涉及两个以上变量的多向表，这些度量不能唯一确定联合分布，而是定义了一族具有相同成对依赖但高阶交互可能不同的可接受分布。需要理解这种分布族的完整结构。

Method: 引入几何框架来描述具有均匀边缘的此类联合分布的整个可行集。证明该可接受集形成凸多面体，分析其对称性质，并表征其极端射线。这些极值分布提供了关于高阶依赖结构如何在保持规定成对信息的同时变化的基本见解。

Result: 建立了具有给定成对相关性的联合分布的凸多面体结构，表征了极端分布，提供了探索依赖结构完整空间的理论基础。与传统方法只返回单个表不同，该框架允许探索和理解依赖结构的完整可接受空间。

Conclusion: 该几何框架为建模和模拟提供了更灵活的选择，通过示例和实际案例研究（评分者一致性）展示了理论结果的有用性，使研究人员能够探索和理解在保持规定成对信息的同时高阶依赖结构的完整变化范围。

Abstract: In many applications involving binary variables, only pairwise dependence measures, such as correlations, are available. However, for multi-way tables involving more than two variables, these quantities do not uniquely determine the joint distribution, but instead define a family of admissible distributions that share the same pairwise dependence while potentially differing in higher-order interactions. In this paper, we introduce a geometric framework to describe the entire feasible set of such joint distributions with uniform margins. We show that this admissible set forms a convex polytope, analyze its symmetry properties, and characterize its extreme rays. These extremal distributions provide fundamental insights into how higher-order dependence structures may vary while preserving the prescribed pairwise information. Unlike traditional methods for table generation, which return a single table, our framework makes it possible to explore and understand the full admissible space of dependence structures, enabling more flexible choices for modeling and simulation. We illustrate the usefulness of our theoretical results through examples and a real case study on rater agreement.

</details>


### [39] [Connections as treatment: causal inference with edge interventions in networks](https://arxiv.org/abs/2601.07267)
*Shuli Chen,Jie Hu,Zhichao Jiang*

Main category: stat.ME

TL;DR: 提出网络因果推断框架，关注边干预（edge interventions）而非传统单元干预，用于估计网络连接（如交通、社会关系）的因果效应


<details>
  <summary>Details</summary>
Motivation: 传统因果推断主要关注单元层面的干预，但许多应用的核心问题是单元间连接（如交通链接、社会关系、协作关系）的因果效应。需要开发专门针对网络边干预的因果框架

Method: 1. 开发网络边干预的因果框架，定义网络结构随机干预下的因果估计量；2. 在边分配无混杂假设下引入逆概率加权估计器；3. 使用指数随机图模型（ERGM）估计边概率；4. 建立估计量的一致性和渐近正态性

Result: 建立了网络边干预的因果推断理论框架，提出了可证明一致且渐近正态的估计方法，并应用于中国交通网络，估计铁路连接对区域经济发展的因果影响

Conclusion: 该研究扩展了因果推断到网络连接层面，为估计网络边干预的因果效应提供了理论和方法基础，在交通网络等实际应用中具有重要价值

Abstract: Causal inference has traditionally focused on interventions at the unit level. In many applications, however, the central question concerns the causal effects of connections between units, such as transportation links, social relationships, or collaborative ties. We develop a causal framework for edge interventions in networks, where treatments correspond to the presence or absence of edges. Our framework defines causal estimands under stochastic interventions on the network structure and introduces an inverse probability weighting estimator under an unconfoundedness assumption on edge assignment. We estimate edge probabilities using exponential random graph models, a widely used class of network models. We establish consistency and asymptotic normality of the proposed estimator. Finally, we apply our methodology to China's transportation network to estimate the causal impact of railroad connections on regional economic development.

</details>


### [40] [Minimum information Markov model](https://arxiv.org/abs/2601.06900)
*Issey Sukeda,Tomonari Sei*

Main category: stat.ME

TL;DR: 提出基于最小信息马尔可夫核参数化的统计模型，证明其正交结构，建立参数估计方法，并在模拟和实际数据中验证性能


<details>
  <summary>Details</summary>
Motivation: 高维时间序列分析日益重要，需要建立能够捕捉依赖结构且具有良好统计性质的模型。现有最小信息马尔可夫核方法需要进一步发展为完整的统计建模框架。

Method: 提出最小信息马尔可夫模型，基于依赖函数参数化，证明其诱导的正交结构，建立散度率最小化问题的理论框架，开发条件似然和伪似然估计方法

Result: 证明了模型的正交结构，建立了高斯自回归情况下的最优解存在性定理，开发了有效的参数估计方法，在模拟和实际数据中展示了良好性能

Conclusion: 最小信息马尔可夫模型为高维时间序列分析提供了理论严谨且实用的框架，其正交结构和最优性性质为统计推断提供了坚实基础

Abstract: The analysis of high-dimensional time series data has become increasingly important across a wide range of fields. Recently, a method for constructing the minimum information Markov kernel on finite state spaces was established. In this study, we propose a statistical model based on a parametrization of its dependence function, which we call the \textit{Minimum Information Markov Model}. We show that its parametrization induces an orthogonal structure between the stationary distribution and the dependence function, and that the model arises as the optimal solution to a divergence rate minimization problem. In particular, for the Gaussian autoregressive case, we establish the existence of the optimal solution to this minimization problem, a nontrivial result requiring a rigorous proof. For parameter estimation, our approach exploits the conditional independence structure inherent in the model, which is supported by the orthogonality. Specifically, we develop several estimators, including conditional likelihood and pseudo likelihood estimators, for the minimum information Markov model in both univariate and multivariate settings. We demonstrate their practical performance through simulation studies and applications to real-world time series data.

</details>


### [41] [Unity Forests: Improving Interaction Modelling and Interpretability in Random Forests](https://arxiv.org/abs/2601.07003)
*Roman Hornung,Alexander Hapfelmeier*

Main category: stat.ME

TL;DR: 提出Unity Forests (UFOs)及其重要性度量方法，专门用于检测无边际效应的交互作用变量，相比传统随机森林能更好地识别纯交互作用变量。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林(RFs)虽然广泛用于预测和变量重要性分析，但其基于局部分裂的特性只能可靠地捕获至少有一个变量具有边际效应的交互作用。对于完全基于交互作用而无边际效应的变量，传统RF方法效果有限。

Method: 1. 提出Unity Forests (UFOs)：在每棵树的前几个分裂中，在随机变量子集上联合优化形成"树根"以捕获交互作用；其余部分按常规方式生长。
2. 提出Unity变量重要性度量(VIM)：基于树根的分裂准则值，仅考虑每个变量在袋内准则值最高的少数树根分裂。
3. 提出协变量代表性树根(CRTRs)：为每个变量选择代表性树根，提供可解释的洞察。

Result: 1. 模拟研究中，Unity VIM能可靠识别无边际效应的交互作用变量，而传统RF-based VIMs无法做到。
2. 大规模真实数据比较中，UFOs比标准RFs具有更高的判别能力和预测准确性，校准性能相当。
3. CRTRs在模拟数据中可靠地再现了变量的真实效应类型，在真实数据分析中提供了有意义的洞察。

Conclusion: UFOs及其相关方法有效地解决了传统随机森林在检测纯交互作用变量方面的局限性，提供了更好的变量重要性分析和预测性能，同时保持了可解释性。

Abstract: Random forests (RFs) are widely used for prediction and variable importance analysis and are often believed to capture any types of interactions via recursive splitting. However, since the splits are chosen locally, interactions are only reliably captured when at least one involved covariate has a marginal effect. We introduce unity forests (UFOs), an RF variant designed to better exploit interactions involving covariates without marginal effects. In UFOs, the first few splits of each tree are optimized jointly across a random covariate subset to form a "tree root" capturing such interactions; the remainder is grown conventionally. We further propose the unity variable importance measure (VIM), which is based on out-of-bag split criterion values from the tree roots. Here, only a small fraction of tree root splits with the highest in-bag criterion values are considered per covariate, reflecting that covariates with purely interaction-based effects are discriminative only if a split in an interacting covariate occurred earlier in the tree. Finally, we introduce covariate-representative tree roots (CRTRs), which select representative tree roots per covariate and provide interpretable insight into the conditions - marginal or interactive - under which each covariate has its strongest effects. In a simulation study, the unity VIM reliably identified interacting covariates without marginal effects, unlike conventional RF-based VIMs. In a large-scale real-data comparison, UFOs achieved higher discrimination and predictive accuracy than standard RFs, with comparable calibration. The CRTRs reproduced the covariates' true effect types reliably in simulated data and provided interesting insights in a real data analysis.

</details>


### [42] [Localization Estimator for High Dimensional Tensor Covariance Matrices](https://arxiv.org/abs/2601.06989)
*Hao-Xuan Sun,Song Xi Chen,Yumou Qiu*

Main category: stat.ME

TL;DR: 提出高维张量数据的协方差矩阵估计方法，通过局部化函数调节样本协方差，建立了多带协方差类以适应多层格点结构，推导了谱范数和Frobenius范数下的极小极大收敛率。


<details>
  <summary>Details</summary>
Motivation: 高维张量数据的协方差矩阵估计面临挑战，需要适应多层格点结构的复杂协方差模式和一般协方差衰减模式。现有方法难以处理这种高维张量数据的协方差结构。

Method: 提出高维协方差局部化估计器，通过局部化函数调节样本协方差矩阵，建立多带协方差类来适应张量数据的复杂结构。

Result: 推导了谱范数和Frobenius范数下的极小极大收敛率，通过数值实验和海洋涡旋数据验证了方法的实用性。

Conclusion: 提出的高维协方差局部化估计器能有效处理张量数据的协方差矩阵估计问题，在理论和实践中都表现出良好性能。

Abstract: This paper considers covariance matrix estimation of tensor data under high dimensionality. A multi-bandable covariance class is established to accommodate the need for complex covariance structures of multi-layer lattices and general covariance decay patterns. We propose a high dimensional covariance localization estimator for tensor data, which regulates the sample covariance matrix through a localization function. The statistical properties of the proposed estimator are studied by deriving the minimax rates of convergence under the spectral and the Frobenius norms. Numerical experiments and real data analysis on ocean eddy data are carried out to illustrate the utility of the proposed method in practice.

</details>


### [43] [Semiparametric Analysis of Interval-Censored Data Subject to Inaccurate Diagnoses with A Terminal Event](https://arxiv.org/abs/2601.07044)
*Yuhao Deng,Donglin Zeng,Yuanjia Wang*

Main category: stat.ME

TL;DR: 提出一个半参数建模框架，使用Cox比例风险模型处理存在不准确疾病诊断的区间删失数据，通过纳入诊断敏感性和特异性来考虑疾病发作时间的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设完美的疾病诊断，但在实践中由于认知功能临床诊断的不完美或生物标志物测量误差，这一假设往往不成立。需要开发能够处理诊断不准确性的区间删失数据分析方法。

Method: 提出半参数建模框架，使用Cox比例风险模型，纳入诊断敏感性和特异性参数。开发非参数最大似然估计方法，设计高效的EM算法进行计算。框架还能处理包含终点事件和准确诊断（如尸检）的情况。

Result: 回归系数估计量被证明是渐近正态的，达到半参数效率界。模拟研究验证了方法的有效性。在阿尔茨海默病风险评估应用中，发现β-淀粉样蛋白与AD显著相关，而Tau蛋白既能预测AD也能预测死亡率。

Conclusion: 该框架为处理存在不准确诊断的区间删失数据提供了有效方法，在慢性疾病研究中具有重要应用价值，特别是在阿尔茨海默病等神经退行性疾病的风险评估中。

Abstract: Interval-censoring frequently occurs in studies of chronic diseases where disease status is inferred from intermittently collected biomarkers. Although many methods have been developed to analyze such data, they typically assume perfect disease diagnosis, which often does not hold in practice due to the inherent imperfect clinical diagnosis of cognitive functions or measurement errors of biomarkers such as cerebrospinal fluid. In this work, we introduce a semiparametric modeling framework using the Cox proportional hazards model to address interval-censored data in the presence of inaccurate disease diagnosis. Our model incorporates sensitivity and specificity of the diagnosis to account for uncertainty in whether the interval truly contains the disease onset. Furthermore, the framework accommodates scenarios involving a terminal event and when diagnosis is accurate, such as through postmortem analysis. We propose a nonparametric maximum likelihood estimation method for inference and develop an efficient EM algorithm to ensure computational feasibility. The regression coefficient estimators are shown to be asymptotically normal, achieving semiparametric efficiency bounds. We further validate our approach through extensive simulation studies and an application assessing Alzheimer's disease (AD) risk. We find that amyloid-beta is significantly associated with AD, but Tau is predictive of both AD and mortality.

</details>


### [44] [Robust Bayesian Optimization via Tempered Posteriors](https://arxiv.org/abs/2601.07094)
*Jiguang Li,Hengrui Luo*

Main category: stat.ME

TL;DR: 本文提出一种基于温度后验的贝叶斯优化方法，通过降低似然权重来缓解局部过采样导致的代理模型过度自信问题，并提供了理论保证和在线温度参数选择策略。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在实践中经常在最优解附近集中采样，导致高斯过程代理模型变得过度自信，低估预测不确定性，从而影响后续决策质量。

Method: 提出温度后验更新方法，将似然函数乘以幂α∈(0,1]来降低权重，缓解局部错误设定下的过度自信。同时提出在线选择α的预检验程序：当实际预测误差超过模型隐含不确定性时降低α，校准改善时恢复α到1。

Result: 理论分析表明温度后验比标准后验具有更严格的最坏情况遗憾界限，特别是在经典EI选择附近效果最佳。实验结果显示温度调节为局部采样下的BO代理模型提供了实用且理论可靠的工具。

Conclusion: 温度后验贝叶斯优化通过缓解代理模型过度自信问题，提供了更稳定和理论保证的优化框架，在线温度选择策略使其具有实际应用价值。

Abstract: Bayesian optimization (BO) iteratively fits a Gaussian process (GP) surrogate to accumulated evaluations and selects new queries via an acquisition function such as expected improvement (EI). In practice, BO often concentrates evaluations near the current incumbent, causing the surrogate to become overconfident and to understate predictive uncertainty in the region guiding subsequent decisions. We develop a robust GP-based BO via tempered posterior updates, which downweight the likelihood by a power $α\in (0,1]$ to mitigate overconfidence under local misspecification. We establish cumulative regret bounds for tempered BO under a family of generalized improvement rules, including EI, and show that tempering yields strictly sharper worst-case regret guarantees than the standard posterior $(α=1)$, with the most favorable guarantees occurring near the classical EI choice.
  Motivated by our theoretic findings, we propose a prequential procedure for selecting $α$ online: it decreases $α$ when realized prediction errors exceed model-implied uncertainty and returns $α$ toward one as calibration improves. Empirical results demonstrate that tempering provides a practical yet theoretically grounded tool for stabilizing BO surrogates under localized sampling.

</details>


### [45] [The Bayesian Intransitive Bradley-Terry Model via Combinatorial Hodge Theory](https://arxiv.org/abs/2601.07158)
*Hisaya Okahara,Tomoyuki Nakagawa,Shonosuke Sugasawa*

Main category: stat.ME

TL;DR: 提出贝叶斯不可传递Bradley-Terry模型，将成对比较数据分解为可传递强度分量和循环诱导分量，使用全局-局部收缩先验，提供可扩展的贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 传统Bradley-Terry模型假设完全可传递偏好，但许多竞争网络中循环效应是固有的，忽略它们会扭曲估计和不确定性量化。

Method: 将组合霍奇理论嵌入逻辑框架，分解成对关系为梯度流（可传递强度）和旋度流（循环结构）；对旋度分量施加全局-局部收缩先验；使用高效吉布斯采样器进行后验推断。

Result: 模拟研究显示估计精度提高、不确定性校准良好，计算效率优于现有贝叶斯不可传递模型；支持全局和三元组层面的不确定性感知量化。

Conclusion: 该框架能显式分离可传递和不可传递分量，提供数据自适应正则化，在无不可传递性时自然简化为经典模型，实现可扩展计算和完整贝叶斯不确定性量化。

Abstract: Pairwise comparison data are widely used to infer latent rankings in areas such as sports, social choice, and machine learning. The Bradley-Terry model provides a foundational probabilistic framework but inherently assumes transitive preferences, explaining all comparisons solely through subject-specific parameters. In many competitive networks, however, cycle-induced effects are intrinsic, and ignoring them can distort both estimation and uncertainty quantification. To address this limitation, we propose a Bayesian extension of the Bradley-Terry model that explicitly separates the transitive and intransitive components. The proposed Bayesian Intransitive Bradley-Terry model embeds combinatorial Hodge theory into a logistic framework, decomposing paired relationships into a gradient flow representing transitive strength and a curl flow capturing cycle-induced structure. We impose global-local shrinkage priors on the curl component, enabling data-adaptive regularization and ensuring a natural reduction to the classical Bradley-Terry model when intransitivity is absent. Posterior inference is performed using an efficient Gibbs sampler, providing scalable computation and full Bayesian uncertainty quantification. Simulation studies demonstrate improved estimation accuracy, well-calibrated uncertainty, and substantial computational advantages over existing Bayesian models for intransitivity. The proposed framework enables uncertainty-aware quantification of intransitivity at both the global and triad levels, while also characterizing cycle-induced competitive advantages among teams.

</details>


### [46] [Principal component-guided sparse reduced-rank regression](https://arxiv.org/abs/2601.07202)
*Kanji Goto,Shintaro Yuki,Kensuke Tanioka,Hiroshi Yadohisa*

Main category: stat.ME

TL;DR: 提出一种结合pcLasso和降秩回归的新方法，既能考虑响应变量相关性，又能将回归系数偏向主成分方向，还能处理解释变量的分组结构。


<details>
  <summary>Details</summary>
Motivation: 现有正则化降秩回归方法存在两个主要局限：1) 无法在考虑解释变量相关结构的同时，将回归系数偏向主成分方向；2) 当解释变量具有分组结构时，无法充分纳入组内相关性。

Method: 将pcLasso引入降秩回归框架，新方法既能考虑响应变量相关性，又能强烈地将回归系数矩阵偏向方差大的主成分方向，还能显式地纳入解释变量的分组结构。

Result: 通过数值模拟和实际数据应用验证了所提方法的有效性，表明该方法能提高预测准确性。

Conclusion: 提出的pcLasso降秩回归方法克服了现有方法的局限性，能够更好地处理多响应回归问题，特别是在解释变量具有相关性和分组结构的情况下。

Abstract: Reduced-rank regression estimates regression coefficients by imposing a low-rank constraint on the matrix of regression coefficients, thereby accounting for correlations among response variables. To further improve predictive accuracy and model interpretability, several regularized reduced-rank regression methods have been proposed. However, these existing methods cannot bias the regression coefficients toward the leading principal component directions while accounting for the correlation structure among explanatory variables. In addition, when the explanatory variables exhibit a group structure, the correlation structure within each group cannot be adequately incorporated. To overcome these limitations, we propose a new method that introduces pcLasso into the reduced-rank regression framework. The proposed method improves predictive accuracy by accounting for the correlation among response variables while strongly biasing the matrix of regression coefficients toward principal component directions with large variance. Furthermore, even in settings where the explanatory variables possess a group structure, the proposed method is capable of explicitly incorporating this structure into the estimation process. Finally, we illustrate the effectiveness of the proposed method through numerical simulations and real data application.

</details>


### [47] [Minimum Wasserstein distance estimator under covariate shift: closed-form, super-efficiency and irregularity](https://arxiv.org/abs/2601.07282)
*Junjun Lang,Qiong Zhang,Yukun Liu*

Main category: stat.ME

TL;DR: 提出基于最小Wasserstein距离的估计框架处理协变量偏移问题，避免显式建模结果回归或重要性权重，得到与1-最近邻估计等价的W-估计量


<details>
  <summary>Details</summary>
Motivation: 协变量偏移在源和目标群体协变量分布不同但响应条件分布不变时出现，常见于缺失数据和因果推断问题，需要避免显式建模的稳健估计方法

Method: 提出最小Wasserstein距离估计框架，通过最优传输理论构建估计量，该估计量有闭式解且数值上等价于经典的1-最近邻估计

Result: W-估计量具有根号n渐近正态性，非渐近线性，在某些情况下相对于半参数有效估计具有超效率，在缺失数据问题中表现一致优越

Conclusion: 提出的W-估计量为协变量偏移问题提供了新的最优传输视角，数值模拟和降雨数据分析验证了其优越性能，为最近邻方法提供了新的理论解释

Abstract: Covariate shift arises when covariate distributions differ between source and target populations while the conditional distribution of the response remains invariant, and it underlies problems in missing data and causal inference. We propose a minimum Wasserstein distance estimation framework for inference under covariate shift that avoids explicit modeling of outcome regressions or importance weights. The resulting W-estimator admits a closed-form expression and is numerically equivalent to the classical 1-nearest neighbor estimator, yielding a new optimal transport interpretation of nearest neighbor methods. We establish root-$n$ asymptotic normality and show that the estimator is not asymptotically linear, leading to super-efficiency relative to the semiparametric efficient estimator under covariate shift in certain regimes, and uniformly in missing data problems. Numerical simulations, along with an analysis of a rainfall dataset, underscore the exceptional performance of our W-estimator.

</details>


### [48] [Inference for Multiple Change-points in Piecewise Locally Stationary Time Series](https://arxiv.org/abs/2601.07400)
*Wai Leong Ng,Xinyi Tang,Mun Lau Cheung,Jiacheng Gao,Chun Yip Yau,Holger Dette*

Main category: stat.ME

TL;DR: 提出了一种新的基于似然的方法，用于在局部平稳时间序列中检测多个变点，能够同时处理跳跃和扭结两种类型的变点变化。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，时间序列可能同时包含突变和平滑变化，传统的变点检测方法只能识别参数值的突变，而局部平稳模型只能处理平滑变化，两者都无法单独处理混合变化类型的数据。

Method: 提出了一种基于似然的推断方法，用于检测局部平稳时间序列中的多个变点。该方法将变点分为两类：跳跃（参数曲线不连续）和扭结（参数曲线连续但导数不连续），并能够同时估计变点的数量、位置和类型。

Result: 理论证明该方法能够一致地估计变点的数量、位置和类型，并建立了跳跃和扭结估计量的两种不同渐近分布。通过大量模拟研究和金融时间序列的实际应用验证了方法的有效性。

Conclusion: 该方法填补了传统变点检测和局部平稳建模之间的空白，能够有效处理同时包含突变和平滑变化的非平稳时间序列数据，为复杂变化模式的分析提供了统一框架。

Abstract: Change-point detection and locally stationary time series modeling are two major approaches for the analysis of non-stationary data. The former aims to identify stationary phases by detecting abrupt changes in the dynamics of a time series model, while the latter employs (locally) time-varying models to describe smooth changes in dependence structure of a time series. However, in some applications, abrupt and smooth changes can co-exist, and neither of the two approaches alone can model the data adequately. In this paper, we propose a novel likelihood-based procedure for the inference of multiple change-points in locally stationary time series. In contrast to traditional change-point analysis where an abrupt change occurs in a real-valued parameter, a change in locally stationary time series occurs in a parameter curve, and can be classified as a jump or a kink depending on whether the curve is discontinuous or not. We show that the proposed method can consistently estimate the number, locations, and the types of change-points. Two different asymptotic distributions corresponding respectively to jump and kink estimators are also established.Extensive simulation studies and a real data application to financial time series are provided.

</details>


### [49] [Ridge-penalised spectral least-squares estimation for point processes](https://arxiv.org/abs/2601.07490)
*Miguel Martinez Herrera,Felix Cheysson*

Main category: stat.ME

TL;DR: 提出一种基于Ridge惩罚的谱最小二乘估计方法，用于二阶平稳点过程，特别针对单次观测的情况，通过p-稀疏交叉验证和谱最小二乘对比函数来优化参数选择。


<details>
  <summary>Details</summary>
Motivation: 传统的惩罚估计方法通常需要大量独立重复观测来进行交叉验证，但在实际应用中，特别是对于点过程，往往只能获得单次观测数据。现有交叉验证方法在这种情况下可能不适用，尤其是在模型选择方面存在局限性。

Method: 1. 提出Ridge惩罚的谱最小二乘估计方法；2. 开发基于p-稀疏的交叉验证方法来调整惩罚参数，该方法依赖于过程的谱表示；3. 引入基于样本周期图渐近性质的谱最小二乘对比函数。

Result: 通过线性Hawkes过程的模拟研究验证了该方法的性能，特别是在短观测窗口条件下，相比传统方法表现出更好的表现。

Conclusion: 该方法为单次观测的点过程提供了有效的惩罚估计框架，特别适用于短观测窗口的情况，解决了传统交叉验证方法在单次观测场景下的局限性。

Abstract: Penalised estimation methods for point processes usually rely on a large amount of independent repetitions for cross-validation purposes. However, in the case of a single realisation of the process, existing cross-validation methods may be impractical depending on the chosen model. To overcome this issue, this paper presents a Ridge-penalised spectral least-squares estimation method for second-order stationary point processes. This is achieved through two novel approaches: a p-thinning-based cross-validation method to tune the penalisation parameter, relying on the spectral representation of the process; and the introduction of a spectral least-squares contrast based around the asymptotic properties of the periodogram of the sample. The proposed method is then illustrated by a simulation study on linear Hawkes processes in the context of parametric estimation, highlighting its performances against more traditional approaches, specifically when working with short observation windows.

</details>


### [50] [Functional Synthetic Control Methods for Metric Space-Valued Outcomes](https://arxiv.org/abs/2601.07539)
*Ryo Okano,Daisuke Kurisu*

Main category: stat.ME

TL;DR: 提出功能性合成控制方法，将合成控制框架扩展到度量空间值结果，通过等距嵌入到希尔伯特空间解决非线性挑战，提供估计和推断的理论保证。


<details>
  <summary>Details</summary>
Motivation: 合成控制方法广泛应用于政策评估，但现有扩展框架缺乏对度量空间值结果（如分布、函数、网络等）的理论保证，需要开发具有理论基础的扩展方法。

Method: 提出功能性合成控制方法，利用度量空间到希尔伯特空间的等距嵌入解决非线性问题，开发FSC和增强FSC估计器（偏差校正版本），建立有限样本误差界，并构建预测集进行因果效应推断。

Result: 通过模拟研究和三个实证应用展示了该框架的有效性，为度量空间值结果的合成控制估计提供了理论保证和实用工具。

Conclusion: 功能性合成控制方法成功扩展了合成控制框架到度量空间值结果，通过等距嵌入技术解决了非线性挑战，为复杂结果的因果推断提供了理论和实践基础。

Abstract: The synthetic control method (SCM) is a widely used tool for evaluating causal effects of policy changes in panel data settings. Recent studies have extended its framework to accommodate complex outcomes that take values in metric spaces, such as distributions, functions, networks, covariance matrices, and compositional data. However, due to the lack of linear structure in general metric spaces, theoretical guarantees for estimation and inference within these extended frameworks remain underdeveloped. In this study, we propose the functional synthetic control (FSC) method as an extension of the SCM for metric space-valued outcomes. To address challenges arising from the nonlinearlity of metric spaces, we leverage isometric embeddings into Hilbert spaces. Building on this approach, we develop the FSC and augmented FSC estimators for counterfactual outcomes, with the latter being a bias-corrected version of the former. We then derive their finite-sample error bounds to establish theoretical guarantees for estimation, and construct prediction sets based on these estimators to conduct inference on causal effects. We demonstrate the usefulness of the proposed framework through simulation studies and three empirical applications.

</details>


### [51] [Omitted covariates bias and finite mixtures of regression models for longitudinal responses](https://arxiv.org/abs/2601.07609)
*Marco Alfo',Robrto Rocci*

Main category: stat.ME

TL;DR: 论文提出了一种改进的随机效应模型参数化方法，通过非参数离散分布处理可观测与不可观测异质性之间的相关性。


<details>
  <summary>Details</summary>
Motivation: 纵向研究中随机效应模型常被用于建模依赖性和处理遗漏变量，但传统方法对可观测协变量与不可观测随机效应之间的相关性不够稳健，需要更灵活的解决方案。

Method: 基于相关效应方法，提出参数化随机效应模型以处理可观测与不可观测异质性的相关性。特别地，当使用有限位置离散分布非参数估计随机效应分布时，开发了更通用的解决方案。

Result: 通过大规模模拟研究和基准数据集分析验证了所提方法的有效性，展示了改进的随机效应模型在处理相关性方面的优势。

Conclusion: 随机效应模型可以通过适当的参数化来有效处理可观测与不可观测异质性之间的相关性，非参数离散分布方法提供了更灵活的解决方案，增强了模型的稳健性和适用性。

Abstract: Individual-specific, time-constant, random effects are often used to model dependence and/or to account for omitted covariates in regression models for longitudinal responses. Longitudinal studies have known a huge and widespread use in the last few years as they allow to distinguish between so-called age and cohort effects; these relate to differences that can be observed at the beginning of the study and stay persistent through time, and changes in the response that are due to the temporal dynamics in the observed covariates. While there is a clear and general agreement on this purpose, the random effect approach has been frequently criticized for not being robust to the presence of correlation between the observed (i.e. covariates) and the unobserved (i.e. random effects) heterogeneity. Starting from the so-called correlated effect approach, we argue that the random effect approach may be parametrized to account for potential correlation between observables and unobservables. Specifically, when the random effect distribution is estimated non-parametrically using a discrete distribution on finite number of locations, a further, more general, solution is developed. This is illustrated via a large scale simulation study and the analysis of a benchmark dataset.

</details>


### [52] [Cluster-based name embeddings reduce ethnic disparities in record linkage quality under realistic name corruption: evidence from the North Carolina Voter Registry](https://arxiv.org/abs/2601.07693)
*Joseph Lam,Mario Cortina-Borja,Rob Aldridge,Ruth Blackburn,Katie Harron*

Main category: stat.ME

TL;DR: 研究通过北卡罗来纳州选民登记数据模拟姓名差异，评估不同记录链接方法在种族群体间的误差差异，发现TF调整的Jaro-Winkler方法总体误匹配率低但种族差异大，而基于聚类的姓名嵌入模型能显著减少种族差异但总体误匹配率较高。


<details>
  <summary>Details</summary>
Motivation: 基于种族的记录链接误差差异会偏倚流行病学估计，现有研究常混淆误差机制的异质性与误差暴露的不平等性。需要量化这两种因素对种族差异的影响，并评估不同链接方法在减少种族差异方面的效果。

Method: 使用北卡罗来纳州选民登记数据(2011-2022)建立姓名差异特征，模拟三种设置：仅机制异质性、仅暴露不平等性、两者结合。使用三种链接方法：未调整的Jaro-Winkler、TF调整的Jaro-Winkler、基于聚类的姓名嵌入模型结合TF调整的姓氏比较。评估误匹配率(FMR)、漏匹配率(MMR)和白人中心差异。

Result: TF调整的Jaro-Winkler在总体MMR 20.34%时FMR仅0.55%，但种族差异显著：西班牙裔选民FMR比非西班牙裔白人高36.3%，非西班牙裔黑人高8.6%。TF调整相比未调整方法减少了这些差异。基于聚类的姓名嵌入模型进一步减少了漏匹配差异(西班牙裔:+10.2%；黑人:+0.6%)，但总体FMR升至4.28%。误差暴露不平等性比机制异质性产生更大的种族差异。

Conclusion: 误差暴露不平等性是种族差异的主要驱动因素，基于聚类的姓名嵌入模型能显著减少链接差异，但需要在减少种族差异和控制总体误匹配率之间权衡。TF调整是有效的改进，但不足以消除所有种族差异。

Abstract: Differential ethnic-based record linkage errors can bias epidemiologic estimates. Prior evidence often conflates heterogeneity in error mechanisms with unequal exposure to error. Using snapshots of the North Carolina Voter Registry (Oct 2011-Oct 2022), we derived empirical name-discrepancy profiles to parameterise realistic corruptions. From an Oct 2022 extract (n=848,566), we generated five replicate corrupted datasets under three settings that separately varied mechanism heterogeneity and exposure inequality, and linked records back to originals using unadjusted Jaro-Winkler, Term Frequency (TF)-adjusted Jaro-Winkler, and a cluster-based forename-embedding comparator combined with TF-adjusted surname comparison. We evaluated false match rate (FMR), missed match rate (MMR) and white-centric disparities. At a fixed MMR near 0.20, overall error rates and ethnic disparities diverged substantially by model under disproportionate exposure to corruption. Term-frequency (TF)-adjusted Jaro-Winkler achieved very low overall FMR (0.55% (95% CI 0.54-0.57)) at overall MMR 20.34% (20.30-20.39), but large white-centric under-linkage disparities persisted: Hispanic voters had 36.3% (36.1-36.6) and Non-Hispanic Black voters 8.6% (8.6-8.7) higher FMRs compared to Non-Hispanic White groups. Relative to unadjusted string similarity, TF adjustment reduced these disparities (Hispanic: +60.4% (60.1-60.7) to +36.3%; Black: +13.1% (13.0-13.2) to +8.6%). The cluster-based forename-embedding model reduced missed-match disparities further (Hispanic: +10.2% (9.8-10.3); Black: +0.6% (0.4-0.7)), but at a cost of increasing overall FMR (4.28% (4.22-4.35)) at the same threshold. Unequal exposure to identifier error drove substantially larger disparities than mechanism heterogeneity alone; cluster-based embeddings markedly narrowed under-linkage disparities beyond TF adjustment.

</details>
