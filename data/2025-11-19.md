<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 11]
- [stat.ML](#stat.ML) [Total: 12]
- [stat.ME](#stat.ME) [Total: 21]
- [stat.CO](#stat.CO) [Total: 2]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Use of multi-pollutant air sensor data and geometric non-negative matrix factorization for source apportionment of air pollution burden in Curtis Bay, Baltimore, USA](https://arxiv.org/abs/2511.11833)
*Bora Jin,Bonita D. Salmerón,David McClosky,David H. Hagan,Russell R. Dickerson,Nicholas J. Spada,Lauren N. Deanes,Matthew A. Aubourg,Laura E. Schmidt,Gregory G. Sawtell,Christopher D. Heaney,Abhirup Datta*

Main category: stat.AP

TL;DR: 使用几何源解析方法分析高时间分辨率空气传感器数据，识别出三个稳定污染源：煤炭码头相关的颗粒物源、交通相关的燃烧源，以及特定的大颗粒物源。


<details>
  <summary>Details</summary>
Motivation: 传统的最小二乘非负矩阵分解方法在源解析中存在非唯一性和扩展性问题，需要一种可扩展且可靠的源解析方法来处理大规模空气传感器数据。

Method: 采用几何源解析框架，分析45万条1分钟分辨率的空气传感器记录，涵盖颗粒物、黑碳、一氧化碳、一氧化氮和二氧化氮等多种污染物。

Result: 识别出三个稳定污染源：源1主导细颗粒物和粗颗粒物（>70%），源2主导一氧化碳和黑碳（~70%），源3主要影响大颗粒物。回归分析显示源1和源3与煤炭码头活动相关，源2与交通相关。

Conclusion: 几何源解析方法结合高时间分辨率多污染物空气传感器网络数据，能够提供可扩展且可靠的证据来指导减排策略。

Abstract: Air sensor networks provide hyperlocal, high temporal resolution data on multiple pollutants that can support credible identification of common pollution sources. Source apportionment using least squares-based non-negative matrix factorization is non-unique and often does not scale. A recent geometric source apportionment framework focuses inference on the source attribution matrix, which is shown to remain identifiable even when the factorization is not. Recognizing that the method scales with and benefits from large data volumes, we use this geometric method to analyze 451,946 one-minute air sensor records from Curtis Bay (Baltimore, USA), collected from October 21, 2022 to June 16, 2023, covering size-resolved particulate matter (PM), black carbon (BC), carbon monoxide (CO), nitric oxide (NO), and nitrogen dioxide (NO2). The analysis identifies three stable sources. Source 1 explains > 70% of fine and coarse PM and ~30% of BC. Source 2 dominates CO and contributes ~70% of BC, NO, and NO2. Source 3 is specific to the larger PM fractions, PM10 to PM40. Regression analyses show Source 1 and Source 3 rise during bulldozer activity at a nearby coal terminal and under winds from the terminal, indicating a direct coal terminal influence, while Source 2 exhibits diurnal patterns consistent with traffic. A case-study on the day with a known bulldozer incident at the coal terminal further confirms the association of terminal activities with Sources 1 and 3. The results are stable under sensitivity analyses. The analysis demonstrates that geometric source apportionment, paired with high temporal resolution data from multi-pollutant air sensor networks, delivers scalable and reliable evidence to inform mitigation strategies.

</details>


### [2] [Violent event-related fatality patterns in Ethiopia: a Bayesian spatiotemporal perspective](https://arxiv.org/abs/2511.12219)
*Osafu Augustine Egbon,Asrat Mekonnen Belachew,Ezra Gayawan,Francisco Louzada*

Main category: stat.AP

TL;DR: 本研究开发了一种时空统计方法来分析埃塞俄比亚武装冲突中的暴力致死模式，发现9个行政区的暴力致死概率超过0.6，其中5个超过0.7，但平均每起事件的死亡人数随时间呈下降趋势。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚武装冲突中的暴力致死是一个严重的公共卫生问题，但缺乏全面的定量科学研究来阐明这些事件的发生顺序和动态。

Method: 采用两部分零膨胀贝叶斯广义加性混合模型，整合时空组件来绘制埃塞俄比亚各地区的致死模式，使用1997-2022年武装冲突位置和事件数据项目的数据集。

Result: 13个行政区中有9个暴力致死概率超过0.6，5个超过0.7；提格雷地区每起事件死亡超过20人的概率最高(0.558)；2020-2022年间每起事件死亡超过20人的概率从0.401降至0.148。

Conclusion: 这些发现对埃塞俄比亚政府、政策制定者和各级领导者具有重要价值，能够帮助他们做出明智的战略决策来减少和预防暴力相关死亡。

Abstract: Fatalities resulting from violence in armed conflict have long been a significant public health issue in Ethiopia. Despite the severity of this problem, more comprehensive quantitative scientific studies need to be conducted to elucidate the sequence and dynamics of these occurrences. In response, this study introduces a spatio-temporal statistical method designed to uncover the patterns of fatalities associated with violent events in Ethiopia. The research employs a two-part zero-inflated Bayesian generalized additive mixed model, which integrates a spatio-temporal component to map the fatality patterns across Ethiopian regions. The dataset utilized originates from the Armed Conflict Location and Event Data Project, covering fatality counts related to violent events from 1997 to 2022. The analysis revealed that nine out of thirteen administrative regions exhibited a probability greater than 0.6 for fatality occurrence due to violent events, with five regions surpassing a 0.7 probability threshold. These five regions include Benishangul Gumz, Gambela, Oromia, Somali, and the South West Ethiopian People's Region. Notably, the Tigray region displayed the highest probability (0.558) of experiencing more than 20 deaths per violent event, followed by the Benishangul Gumz region with a probability of 0.306. Encouragingly, the findings also indicate an average decline in fatalities per violent event over time. Specifically, the probability of more than 20 deaths per event was 0.401 in 2020, which decreased to 0.148 by 2022. These insights are invaluable for the government, policymakers, political leaders, and traditional or religious authorities in Ethiopia, enabling them to make informed, strategic decisions to mitigate and ultimately prevent violence-related fatalities in the country.

</details>


### [3] [A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment](https://arxiv.org/abs/2511.12234)
*Soham Sarkar,Arnab Hazra*

Main category: stat.AP

TL;DR: 本文综述了评估珊瑚白化的统计和机器学习方法，包括回归模型、贝叶斯模型、时空模型以及随机森林、决策树等机器学习技术，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 珊瑚白化是海洋生态系统的重大威胁，过去30年全球超过一半的珊瑚礁已白化或死亡。虽然统计和机器学习社区已关注环境因素，但关于珊瑚白化随机建模方法的文献极为稀缺。

Method: 使用统计框架（回归模型、广义线性模型、广义加性模型、贝叶斯回归模型、时空模型）和机器学习方法（随机森林、决策树、支持向量机、空间算子）来评估珊瑚白化。

Result: 统计方法常用于探索环境压力因素对珊瑚白化的影响，机器学习方法更适合检测非线性关系、分析高维数据并整合异构数据源。

Conclusion: 数据驱动策略对有效珊瑚礁管理至关重要，未来研究应聚焦于在珊瑚白化特定背景下构建统计和机器学习模型。

Abstract: Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching.

</details>


### [4] [Stochastic Predictive Analytics for Stocks in the Newsvendor Problem](https://arxiv.org/abs/2511.12397)
*Pedro A. Pury*

Main category: stat.AP

TL;DR: 开发了一个不依赖特定需求分布的随机库存模型，用于描述库存的动态分布，适用于数据有限和短期预测的场景，特别适合报童问题。


<details>
  <summary>Details</summary>
Motivation: 解决库存管理中的关键挑战，即在历史数据有限且需要短期预测的情况下，提供不依赖特定需求分布的灵活解决方案。

Method: 开发了一个随机模型来描述库存随时间的动态分布，不假设特定的需求分布形式。

Result: 使用大型电子市场的真实数据评估模型性能，证明其在实际预测场景中的有效性。

Conclusion: 该模型为库存管理提供了灵活且适用的解决方案，特别适合数据有限和短期预测需求的情况。

Abstract: This work addresses a key challenge in inventory management by developing a stochastic model that describes the dynamic distribution of inventory stock over time without assuming a specific demand distribution. Our model provides a flexible and applicable solution for situations with limited historical data and short-term predictions, making it well-suited for the Newsvendor problem. We evaluate our model's performance using real-world data from a large electronic marketplace, demonstrating its effectiveness in a practical forecasting scenario.

</details>


### [5] [Do Nineteenth-Century Graphics Still Work for Today's Readers?](https://arxiv.org/abs/2511.12510)
*Yingke He*

Main category: stat.AP

TL;DR: 通过对照实验评估19世纪经典图表与现代重设计版本的效果，发现部分历史设计仍然有效，部分在现代重设计后表现更好，而某些现代重设计反而表现更差。


<details>
  <summary>Details</summary>
Motivation: 研究19世纪的经典图表设计是否仍然适用于现代读者，以及现代重设计是否能改善这些历史图表的有效性。

Method: 采用对照实验方法，评估三种经典历史可视化图表（南丁格尔极区图、普莱费尔贸易平衡图、米纳尔战役地图）与现代重设计版本，测量54名参与者的准确性、响应时间和感知工作量。

Result: 南丁格尔图表在不同版本中保持高效；普莱费尔的双轴重设计表现较差；米纳尔地图在重设计后准确性大幅提升但仍需高工作量。

Conclusion: 感知编码选择、任务对齐和认知负荷决定了历史图表是否需要为当代使用进行适应性调整。

Abstract: Do nineteenth-century graphics still work for today's readers? To investigate this question, we conducted a controlled experiment evaluating three canonical historical visualizations- Nightingale's polar area diagram, Playfair's trade balance chart, and Minard's campaign map-against modern redesigns. Fifty-four participants completed structured question-answering tasks, allowing us to measure accuracy, response time, and perceived workload (NASA-TLX). We used mixed-effects regression models to find: Nightingale's diagram remained consistently effective across versions, achieving near-ceiling accuracy and low workload; Playfair's dual-axis redesign underperformed relative to both its historical and alternative versions; and Minard's map showed large accuracy gains under redesign but continued to impose high workload and long response times. These results demonstrate that some nineteenth-century designs remain effective, others degrade under certain modernizations, and some benefit from careful redesign. The findings indicate how perceptual encoding choices, task alignment, and cognitive load determine whether historical charts survive or require adaptation for contemporary use.

</details>


### [6] [A spatio-temporal statistical model for property valuation at country-scale with adjustments for regional submarkets](https://arxiv.org/abs/2511.12625)
*Brian O'Donovan,Andrew Finley,James Sweeney*

Main category: stat.AP

TL;DR: 开发了一个稳健的统计框架，用于爱尔兰住房市场的房产估值，通过将国家划分为六个子市场并使用广义可加模型，在数据稀疏区域显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统自动估值模型在考虑多个本地化子市场（城市、郊区和农村）时存在局限性，无法灵活捕捉空间变化，而先进机器学习方法需要大量数据集。

Method: 将国家划分为六个子市场（城市、大城镇和农村地区），采用广义可加模型捕捉房产特征的非线性效应，并允许特征贡献在不同子市场间变化。

Result: 在样本外验证中，模型在农村地区、城镇和都柏林的R平方值分别达到0.70、0.84和0.83，优于随机森林基准的0.52、0.71和0.82。模型的时间动态与报告的通胀数据高度一致。

Conclusion: 该方法为在复杂、数据稀疏的环境中提供准确、客观的房产估值提供了一个有效的解决方案，特别是在传统模型表现不佳的地区。

Abstract: Valuing residential property is inherently complex, requiring consideration of numerous environmental, economic, and property-specific factors. These complexities present significant challenges for automated valuation models (AVMs), which are increasingly used to provide objective assessments for property taxation and mortgage financing. The challenge of obtaining accurate and objective valuations for properties at a country level, and not just within major cities, is further compounded by the presence of multiple localised submarkets-spanning urban, suburban, and rural contexts-where property features contribute differently to value. Existing AVMs often struggle in such settings: traditional hedonic regression models lack the flexibility to capture spatial variation, while advanced machine learning approaches demand extensive datasets that are rarely available. In this article, we address these limitations by developing a robust statistical framework for property valuation in the Irish housing market. We segment the country into six submarkets encompassing cities, large towns, and rural areas, and employ a generalized additive model that captures non-linear effects of property characteristics while allowing feature contributions to vary across submarkets. Our approach outperforms both machine learning-based and traditional hedonic regression models, particularly in data-sparse regions. In out-of-sample validation, our model achieves R-squared values of 0.70, 0.84, and 0.83 for rural areas, towns, and Dublin, respectively, compared to 0.52, 0.71, and 0.82 from a random forest benchmark. Furthermore, the temporal dynamics of our model align closely with reported inflation figures for the study period, providing additional validation of its accuracy.

</details>


### [7] [Change-Point Detection Utilizing Normalized Entropy as a Fundamental Metric](https://arxiv.org/abs/2511.12703)
*Qingqing Song,Shaoliang Xia*

Main category: stat.AP

TL;DR: 本文提出基于归一化熵的变点检测方法，通过将熵值标准化到[0,1]区间，有效捕捉数据复杂度的相对变化，克服传统熵方法对数据分布假设和绝对尺度的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统熵方法在变点检测中依赖于对数据分布的假设和绝对尺度，难以适应复杂时间序列中尺度、分布和多样性的变化。

Method: 使用滑动窗口计算归一化熵，将复杂时间序列中的变点检测问题转化为识别归一化熵序列中的显著特征，避免参数假设的干扰。

Result: 实验结果显示，归一化熵在各种分布和参数组合下，在变点附近表现出显著的数值波动特征，波动时刻与实际变点的平均偏差仅为滑动窗口大小的2.4%。

Conclusion: 归一化熵为复杂数据环境中的变点检测提供了理论支持，并为基于该基础指标的精确自动化检测奠定了方法基础。

Abstract: This paper introduces a concept for change-point detection based on normalized entropy as a fundamental metric, aiming to overcome the dependence of traditional entropy methods on assumptions about data distribution and absolute scales. Normalized entropy maps entropy values to the [0,1] interval through standardization, accurately capturing relative changes in data complexity. By utilizing a sliding window to compute normalized entropy, this approach transforms the challenge of detecting change points in complex time series, arising from variations in scale, distribution, and diversity, into the task of identifying significant features within the normalized entropy sequence, thereby avoiding interference from parametric assumptions and effectively highlighting distributional shifts. Experimental results show that normalized entropy exhibits significant numerical fluctuation characteristics and patterns near change points across various distributions and parameter combinations. The average deviation between fluctuation moments and actual change points is only 2.4% of the sliding window size, demonstrating strong adaptability. This paper provides theoretical support for change-point detection in complex data environments and lays a methodological foundation for precise and automated detection based on normalized entropy as a fundamental metric.

</details>


### [8] [Scalable Vision-Guided Crop Yield Estimation](https://arxiv.org/abs/2511.12999)
*Harrison H. Li,Medhanie Irgau,Nabil Janmohamed,Karen Solveig Rieckmann,David B. Lobell*

Main category: stat.AP

TL;DR: 提出基于预测驱动推理的方法，利用田间照片补充作物收割测量数据，提高作物产量估计精度和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统作物收割测量方法耗时，需要更高效的数据收集方式来支持农业监测和决策。

Method: 训练计算机视觉模型从照片预测作物产量，学习控制函数重新校准预测值，结合空间坐标信息，利用预测驱动推理方法。

Result: 在仅有20个田地的区域，点估计比基线方法显著改进，水稻有效样本量增加73%，玉米增加12-23%，置信区间更短且覆盖率高。

Conclusion: 低成本图像有潜力使基于区域的作物保险更经济实惠，促进可持续农业实践投资。

Abstract: Precise estimation and uncertainty quantification for average crop yields are critical for agricultural monitoring and decision making. Existing data collection methods, such as crop cuts in randomly sampled fields at harvest time, are relatively time-consuming. Thus, we propose an approach based on prediction-powered inference (PPI) to supplement these crop cuts with less time-consuming field photos. After training a computer vision model to predict the ground truth crop cut yields from the photos, we learn a ``control function" that recalibrates these predictions with the spatial coordinates of each field. This enables fields with photos but not crop cuts to be leveraged to improve the precision of zone-wide average yield estimates. Our control function is learned by training on a dataset of nearly 20,000 real crop cuts and photos of rice and maize fields in sub-Saharan Africa. To improve precision, we pool training observations across different zones within the same first-level subdivision of each country. Our final PPI-based point estimates of the average yield are provably asymptotically unbiased and cannot increase the asymptotic variance beyond that of the natural baseline estimator -- the sample average of the crop cuts -- as the number of fields grows. We also propose a novel bias-corrected and accelerated (BCa) bootstrap to construct accompanying confidence intervals. Even in zones with as few as 20 fields, the point estimates show significant empirical improvement over the baseline, increasing the effective sample size by as much as 73% for rice and by 12-23% for maize. The confidence intervals are accordingly shorter at minimal cost to empirical finite-sample coverage. This demonstrates the potential for relatively low-cost images to make area-based crop insurance more affordable and thus spur investment into sustainable agricultural practices.

</details>


### [9] [TacEleven: generative tactic discovery for football open play](https://arxiv.org/abs/2511.13326)
*Siyao Zhao,Hao Ma,Zhiqiang Pu,Jingjing Huang,Yi Pan,Shijie Wang,Zhi Ming*

Main category: stat.AP

TL;DR: TacEleven是一个用于足球开放进攻战术发现的生成框架，包含战术生成器和战术评估器两个核心组件，能够快速探索战术提案并发现替代性进攻战术。


<details>
  <summary>Details</summary>
Motivation: 由于足球开放进攻具有高度动态性和长序列特性，战术空间随序列进展呈指数级增长，使得自动战术发现极具挑战性。

Method: TacEleven包含语言控制的战术生成器（生成多样化战术提案）和基于多模态大语言模型的战术评估器（选择与高层战术指令对齐的最优提案）。

Result: 在三个渐进复杂度的任务评估中，TacEleven发现的战术展现出强大的真实性和战术创造力，52.50%的多步战术替代方案被评为可在现实精英足球场景中采用。

Conclusion: TacEleven展示了创造性利用领域数据和生成模型来推进体育战术分析的潜力，能够为复杂长序列开放进攻情境快速生成大量高质量战术。

Abstract: Creating offensive advantages during open play is fundamental to football success. However, due to the highly dynamic and long-sequence nature of open play, the potential tactic space grows exponentially as the sequence progresses, making automated tactic discovery extremely challenging. To address this, we propose TacEleven, a generative framework for football open-play tactic discovery developed in close collaboration with domain experts from AJ Auxerre, designed to assist coaches and analysts in tactical decision-making. TacEleven consists of two core components: a language-controlled tactical generator that produces diverse tactical proposals, and a multimodal large language model-based tactical critic that selects the optimal proposal aligned with a high-level stylistic tactical instruction. The two components enables rapid exploration of tactical proposals and discovery of alternative open-play offensive tactics. We evaluate TacEleven across three tasks with progressive tactical complexity: counterfactual exploration, single-step discovery, and multi-step discovery, through both quantitative metrics and a questionnaire-based qualitative assessment. The results show that the TacEleven-discovered tactics exhibit strong realism and tactical creativity, with 52.50% of the multi-step tactical alternatives rated adoptable in real-world elite football scenarios, highlighting the framework's ability to rapidly generate numerous high-quality tactics for complex long-sequence open-play situations. TacEleven demonstrates the potential of creatively leveraging domain data and generative models to advance tactical analysis in sports.

</details>


### [10] [Variance Stabilizing Transformations for Electricity Price Forecasting in Periods of Increased Volatility](https://arxiv.org/abs/2511.13603)
*Bartosz Uniejewski*

Main category: stat.AP

TL;DR: 本文提出了一种新的参数化asinh变换方法，系统分析了参数敏感性和校准窗口大小，证明方差稳定变换能显著降低电价预测误差，在波动市场中效果尤为明显。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源渗透率提高和近期危机，电力市场价格波动性急剧增加，传统预测模型面临挑战，需要更有效的预处理方法来应对前所未有的价格波动。

Method: 引入参数化asinh变换作为方差稳定变换预处理工具，使用德国、西班牙和法国2015-2024年数据，结合NARX和LEAR两类模型，系统分析参数敏感性和校准窗口大小。

Result: 方差稳定变换显著降低预测误差，LEAR模型改善14.6%，NARX模型改善8.7%；参数化asinh变换始终优于标准形式，滚动平均变换组合可将误差降低达17.7%。

Conclusion: 方差稳定变换是增强电力价格预测的有力工具，特别是在波动市场条件下，为当今电力市场提供了有效的预测改进方案。

Abstract: Accurate day-ahead electricity price forecasts are critical for power system operation and market participation, yet growing renewable penetration and recent crises have caused unprecedented volatility that challenges standard models. This paper revisits variance-stabilizing transformations (VSTs) as a preprocessing tool by introducing a novel parametrization of the asinh transformation, systematically analyzing parameter sensitivity and calibration window size, and explicitly testing performance under volatile market regimes. Using data from Germany, Spain, and France over 2015-2024 with two model classes (NARX and LEAR), we show that VSTs substantially reduce forecast errors, with gains of up to 14.6% for LEAR and 8.7% for NARX relative to untransformed benchmarks. The new parametrized asinh consistently outperforms its standard form, while rolling averaging across transformations delivers the most robust improvements, reducing errors by up to 17.7%. Results demonstrate that VSTs are especially valuable in volatile regimes, making them a powerful tool for enhancing electricity price forecasting in today's power markets.

</details>


### [11] [Phase I Distribution-Free Control Charts for Individual Observations Using Runs and Patterns](https://arxiv.org/abs/2511.13672)
*Tung-Lung Wu*

Main category: stat.AP

TL;DR: 提出了用于监控连续和离散个体观测值未知目标值（或位置参数）的Phase I分布自由游程和模式类型控制图，通过有限马尔可夫链嵌入技术结合随机排列和条件论证来维持规定的控制内信号概率水平。


<details>
  <summary>Details</summary>
Motivation: 需要开发能够处理连续和离散个体观测值、不依赖分布假设的Phase I控制图，以监控未知目标值或位置参数。

Method: 采用有限马尔可夫链嵌入技术结合随机排列和条件论证，研究了两种流行的游程和模式类型统计量：成功游程数和扫描统计量。

Result: 数值结果表明，所提出的控制图性能与现有的Phase I非参数个体观测值控制图相当。

Conclusion: 提出的Phase I分布自由游程和模式类型控制图是有效的，性能与现有方法相当，适用于监控连续和离散个体观测值的未知目标值。

Abstract: Phase I distribution-free runs- and patterns-type control charts are proposed for monitoring the unknown target value (or location parameter) for both continuous and discrete individual observations. Our approach maintains the nominal in-control signal probability at a prescribed level by employing the finite Markov chain imbedding technique combined with random permutation and conditioning arguments. To elucidate the methodology, we examine two popular runs- and patterns-type statistics: the number of success runs and the scan statistic. Numerical results indicate that the performance of our proposed control charts is comparable to that of existing Phase I nonparametric control charts for individual observations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [12] [Generalized Inequality-based Approach for Probabilistic WCET Estimation](https://arxiv.org/abs/2511.11682)
*Hayate Toba,Atsushi Yano,Takuya Azumi*

Main category: stat.ML

TL;DR: 提出一种结合饱和函数（反正切和双曲正切）的改进切比雪夫不等式方法，用于减少概率最坏情况执行时间估计中的悲观性，特别适用于重尾分布。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两难：基于极值理论的方法需要确定分布尾部起始点，存在模型不确定性；而不等式方法虽然避免此问题，但对重尾分布会产生悲观结果。需要一种既能保持数学严谨性又能减少悲观性的方法。

Method: 将饱和函数（arctangent和hyperbolic tangent）整合到切比雪夫不等式中，通过限制大离群值的影响来减少悲观性，同时保持数学严谨性。

Result: 在合成数据和来自Autoware自动驾驶系统的真实数据上的评估表明，该方法能够为重尾分布提供更紧且安全的边界。

Conclusion: 所提出的方法成功解决了概率最坏情况执行时间估计中的悲观性问题，特别适用于重尾分布场景，在保持安全性的同时提供了更紧密的边界估计。

Abstract: Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions.

</details>


### [13] [FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition](https://arxiv.org/abs/2511.11817)
*Zhongde An,Jinhong You,Jiyanglin Li,Yiming Tang,Wen Li,Heming Du,Shouguo Du*

Main category: stat.ML

TL;DR: FreDN是一个频率分解网络，通过可学习的频率解耦器直接在频域分离趋势和周期成分，解决了非平稳时间序列中的频谱纠缠问题，并提出了ReIm块来降低复数运算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳时间序列中频谱纠缠问题，以及复数学习带来的计算负担。频谱纠缠指由于频谱泄漏和非平稳性导致的趋势、周期性和噪声在频谱上的重叠。

Method: 提出频率分解网络(FreDN)，包含可学习的频率解耦器模块直接在频域分离趋势和周期成分，并提出理论支持的ReIm块来降低复数运算复杂度。

Result: 在7个长期预测基准测试中，FreDN比最先进方法性能提升高达10%。与标准复数架构相比，实部-虚部共享参数设计将参数数量和计算成本降低了至少50%。

Conclusion: FreDN有效解决了非平稳时间序列中的频谱纠缠问题，同时显著降低了计算复杂度，在时间序列预测任务中表现出优越性能。

Abstract: Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\%.

</details>


### [14] [PCA recovery thresholds in low-rank matrix inference with sparse noise](https://arxiv.org/abs/2511.11927)
*Urte Adomaityte,Gabriele Sicuro,Pierpaolo Vivo*

Main category: stat.ML

TL;DR: 该论文研究高维秩一信号在稀疏噪声下的推断问题，使用统计物理的复本方法分析计算了最大特征值、特征向量分量密度以及信号向量与最大特征向量的重叠度，并识别了信号恢复的临界强度。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏噪声下秩一信号的恢复问题，将经典的BBP相变推广到稀疏噪声情况，解决实际应用中噪声通常具有稀疏特性的现实问题。

Method: 使用统计物理中的复本方法，通过辅助概率密度函数的递归分布方程进行分析求解，并采用群体动力学算法进行高效计算。

Result: 解析计算了最大特征值的典型值、特征向量分量密度和信号重叠度，识别了Poissonian和随机正则度分布下的临界信号强度，在大量连接极限下恢复了稠密噪声的已知结果。

Conclusion: 成功将BBP相变推广到稀疏噪声情况，解析结果与大型矩阵数值对角化一致，为稀疏噪声下的信号恢复提供了理论框架。

Abstract: We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices.

</details>


### [15] [Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence](https://arxiv.org/abs/2511.11983)
*Debashis Chatterjee*

Main category: stat.ML

TL;DR: 提出一个统一的贝叶斯和AI框架，将贝叶斯预测与贝叶斯超参数优化相结合，用于流行病学分析，提供校准的不确定性量化和优化的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代流行病学分析使用机器学习模型预测能力强但缺乏校准的不确定性，贝叶斯方法能提供原则性的不确定性量化但难以与当代AI工作流集成。

Method: 使用贝叶斯逻辑回归获得个体疾病风险和后验分布，同时使用高斯过程贝叶斯优化调整惩罚Cox生存模型，形成双层系统。

Result: 贝叶斯层提供可靠的覆盖率和改进的校准，贝叶斯收缩提高AUC、Brier得分和对数损失，贝叶斯优化使生存模型接近最优一致性。

Conclusion: 贝叶斯推理增强了推断和搜索能力，为流行病学决策提供校准风险和原则性超参数智能。

Abstract: Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization.
  We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective.
  Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making.

</details>


### [16] [PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning](https://arxiv.org/abs/2511.12278)
*Mingqi Wu,Qiang Sun,Yi Yang*

Main category: stat.ML

TL;DR: PCA++是一种硬均匀性约束的对比PCA方法，通过强制投影特征具有恒等协方差来恢复共享信号子空间，在高维噪声环境下比标准PCA和仅对齐的PCA+表现更好。


<details>
  <summary>Details</summary>
Motivation: 高维数据中的低维信号常被结构化背景噪声掩盖，标准PCA效果有限。对比学习启发了从正对中恢复共享信号子空间的需求，这些正对共享相同信号但背景不同。

Method: 提出PCA++方法，采用硬均匀性约束的对比PCA，强制投影特征具有恒等协方差。该方法有封闭解，通过广义特征问题求解，在高维下保持稳定，并能正则化背景干扰。

Result: 在固定长宽比和增长尖峰机制下提供了精确的高维渐近分析，展示了均匀性在鲁棒信号恢复中的作用。在模拟、损坏MNIST和单细胞转录组学实验中，PCA++优于标准PCA和仅对齐的PCA+。

Conclusion: 明确了均匀性在对比学习中的作用，显示显式特征分散能防御结构化噪声并增强鲁棒性，可靠地恢复条件不变结构。

Abstract: High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness.

</details>


### [17] [Accelerated Distributional Temporal Difference Learning with Linear Function Approximation](https://arxiv.org/abs/2511.12688)
*Kaicheng Jin,Yang Peng,Jiansheng Yang,Zhihua Zhang*

Main category: stat.ML

TL;DR: 本文研究了使用线性函数近似的分布时序差分学习的有限样本统计率，建立了与支持集大小K无关的紧样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 研究分布时序差分学习在函数近似下的统计效率，填补之前主要关注表格情况的研究空白。

Method: 首先对线性分类贝尔曼方程进行细粒度分析，然后结合方差缩减技术设计新算法。

Result: 当K很大时，建立了与支持集大小K无关的紧样本复杂度界限，表明学习回报函数的完整分布与学习其期望值难度相当。

Conclusion: 分布强化学习算法具有统计效率，学习回报分布并不比学习期望更困难。

Abstract: In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.

</details>


### [18] [TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting](https://arxiv.org/abs/2511.12749)
*Zong-Han Bai,Po-Yen Chu*

Main category: stat.ML

TL;DR: TSB-HB是一种层次贝叶斯扩展的TSB方法，用于间歇性需求预测，通过Beta-Binomial分布建模需求发生，Log-Normal分布建模非零需求规模，利用层次先验实现跨项目的部分池化，在稀疏或冷启动序列中稳定估计同时保持异质性。


<details>
  <summary>Details</summary>
Motivation: 间歇性需求预测面临稀疏观测、冷启动项目和过时性等独特挑战。经典模型如Croston、SBA和TSB方法提供简单启发式但缺乏原则性的生成基础。深度学习模型解决了这些限制但通常需要大数据集并牺牲可解释性。

Method: 引入TSB-HB，TSB的层次贝叶斯扩展。需求发生用Beta-Binomial分布建模，非零需求规模遵循Log-Normal分布。关键地，层次先验实现跨项目的部分池化，在稀疏或冷启动序列中稳定估计同时保持异质性。

Result: 在UCI Online Retail数据集上，TSB-HB比Croston、SBA、TSB、ADIDA、IMAPA、ARIMA和Theta方法获得更低的RMSE和RMSSE。在M5数据集的子集上，它优于所有评估的经典基线。

Conclusion: 该模型通过结合生成公式和层次收缩，为间歇性和块状项目提供校准的概率预测和改进的准确性，同时保持可解释性和可扩展性。

Abstract: Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability.
  We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing.
  On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable.

</details>


### [19] [Function-on-Function Bayesian Optimization](https://arxiv.org/abs/2511.12783)
*Jingru Huang,Haijie Xu,Manrui Jiang,Chen Zhang*

Main category: stat.ML

TL;DR: 提出了函数对函数贝叶斯优化框架，用于优化输入和输出都是函数的昂贵目标函数，解决了现有方法未处理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未能处理输入和输出都是函数的目标函数，这在复杂系统和先进传感技术中日益常见。

Method: 引入函数对函数高斯过程模型，使用可分离算子值核捕获函数值输入输出相关性；基于此定义标量上置信界获取函数，并开发可扩展函数梯度上升算法。

Result: 在合成和真实世界数据上的广泛实验表明，FFBO优于现有方法。

Conclusion: FFBO框架有效解决了函数对函数优化问题，在理论和实验上都表现出优越性能。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.

</details>


### [20] [Benign Overfitting in Linear Classifiers with a Bias Term](https://arxiv.org/abs/2511.12840)
*Yuta Kondo*

Main category: stat.ML

TL;DR: 本文扩展了Hashimoto等人关于线性分类中良性过拟合的研究，将分析从无偏置项的同质模型扩展到包含偏置项的非同质模型，证明了良性过拟合在更复杂的模型中仍然存在。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型即使完美插值带噪声的训练数据也能良好泛化，这种现象称为良性过拟合。Hashimoto等人的分析仅限于无偏置项的同质模型，而实践中偏置项是标准组件，因此需要研究偏置项对良性过拟合的影响。

Method: 将Hashimoto等人的结果直接扩展到包含偏置项的非同质模型，分析偏置项对数据协方差结构约束的影响，特别关注存在标签噪声时的情况。

Result: 证明良性过拟合在非同质模型中仍然存在，但偏置项的引入对数据协方差结构提出了新的泛化约束要求，这些约束在标签噪声存在时尤为明显。在各项同性情况下，这些新约束被同质模型继承的要求所主导。

Conclusion: 这项研究提供了对良性过拟合更完整的理解，揭示了偏置项对良好泛化所需条件的非平凡影响。

Abstract: Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of "homogeneous" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization.

</details>


### [21] [Reconstruction of Manifold Distances from Noisy Observations](https://arxiv.org/abs/2511.13025)
*Charles Fefferman,Jonathan Marty,Kevin Ren*

Main category: stat.ML

TL;DR: 提出了一种从带噪声的成对距离观测中重建流形内禀几何的新框架，改进了先前需要已知矩的加性噪声假设，在温和的几何假设下能恢复样本点间的真实距离，误差为O(ε log ε^{-1})。


<details>
  <summary>Details</summary>
Motivation: 从带噪声的成对距离观测中重建流形的内禀几何是一个重要问题，先前工作假设噪声具有已知矩的加性噪声，限制了实际应用。

Method: 基于估计期望函数f_x(y)=E[d'(x,y)]的L2范数来构建稳健的聚类，通过新的几何论证在曲率有界和正内射半径条件下恢复真实距离。提出了两种算法：第一种达到样本复杂度N≍ε^{-2d-2}log(1/ε)和运行时间o(N^3)；第二种引入了新颖的几何思想。

Result: 在温和的几何假设下，能够恢复样本点间的真实距离，达到加性误差O(ε log ε^{-1})。第一种算法在存在缺失观测时，通过修改聚类构造也能保持恢复保证。

Conclusion: 该框架改进了先前对噪声假设的限制，技术结果阐明了流形的哪些性质对距离恢复是必要的，为扩展到更广泛的度量概率空间提供了基础。

Abstract: We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $μ$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\dots,X_N$ are i.i.d. samples of $μ$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\varepsilon \log \varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \asymp \varepsilon^{-2d-2}\log(1/\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces.

</details>


### [22] [Likelihood-guided Regularization in Attention Based Models](https://arxiv.org/abs/2511.13221)
*Mohamed Salem,Inyoung Kim*

Main category: stat.ML

TL;DR: 提出了一种基于变分Ising正则化的Vision Transformer框架，通过贝叶斯稀疏化技术增强模型泛化能力并动态修剪冗余参数。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在分类任务中表现优异，但需要大量训练数据和精心正则化来防止过拟合。传统dropout方法采用固定稀疏模式，缺乏任务自适应性。

Method: 使用变分Ising正则化框架，通过贝叶斯稀疏化技术对模型权重施加结构化稀疏性，在训练过程中进行自适应架构搜索。

Result: 在MNIST、Fashion-MNIST、CIFAR-10和CIFAR-100等基准视觉数据集上验证了方法的有效性，提高了稀疏复杂数据下的泛化能力，并支持权重和选择参数的不确定性量化。

Conclusion: 结构化贝叶斯稀疏化能有效增强基于transformer的架构，为标准正则化技术提供了原则性替代方案，实现了更好的概率估计校准和结构化特征选择。

Abstract: The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.

</details>


### [23] [The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business](https://arxiv.org/abs/2511.13503)
*Ioannis Diamantis*

Main category: stat.ML

TL;DR: 本文介绍拓扑数据分析(TDA)在商业经济中的应用，通过持久同调方法揭示数据中的非线性多尺度结构模式，提出拓扑稳定性指数(TSI)并给出实践指南。


<details>
  <summary>Details</summary>
Motivation: 传统线性工具无法充分表征现代商业经济数据中的非线性多尺度结构，需要几何视角来发现跨尺度的稳健模式。

Method: 使用持久同调方法，构建TDA分析流程，包括距离度量选择、复形构建和解释，并引入拓扑稳定性指数(TSI)。

Result: 通过消费者行为、股票市场和外汇动态的案例研究，证明拓扑特征能揭示传统统计方法无法发现的细分模式和结构关系。

Conclusion: 为商业经济分析中的TDA实施、可视化和沟通提供实用指南，展示拓扑方法在复杂数据分析中的价值。

Abstract: Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [24] [Modeling group heterogeneity in spatio-temporal data via physics-informed semiparametric regression](https://arxiv.org/abs/2511.13203)
*Marco F. De Sanctis,Eleonora Arnone,Francesca Ieva,Laura M. Sangalli*

Main category: stat.ME

TL;DR: 提出了一种新的时空数据建模方法，通过引入受偏微分方程正则化的非参数时空组件来扩展混合效应回归模型，同时用随机效应捕捉数据中的组结构变异性。


<details>
  <summary>Details</summary>
Motivation: 针对具有组结构的时空数据，传统混合效应模型难以有效嵌入物理动态过程，需要开发能够同时处理时空依赖性和组结构变异性的方法。

Method: 采用两步估计程序，基于函数版本的迭代重加权最小二乘算法估计模型的固定和随机组件，通过偏微分方程正则化时空非参数组件。

Result: 通过模拟研究验证了方法的性能，并与现有先进方法进行比较，最后应用于意大利伦巴第地区二氧化氮浓度数据的分析。

Conclusion: 所提出的方法能够有效建模具有组结构的时空数据，在环境监测等应用中表现出良好性能。

Abstract: In this work we propose a novel approach for modeling spatio-temporal data characterized by group structures. In particular, we extend classical mixed effect regression models by introducing a space-time nonparametric component, regularized through a partial differential equation, to embed the physical dynamics of the underlying process, while random effects capture latent variability associated with the group structure present in the data. We propose a two-step procedure to estimate the fixed and random components of the model, relying on a functional version of the Iterative Reweighted Least Squares algorithm. We investigate the asymptotic properties of both fixed and random components, and we assess the performance of the proposed model through a simulation study, comparing it with state-of-the-art alternatives from the literature. The proposed methodology is finally applied to the study of hourly nitrogen dioxide concentration data in Lombardy (Italy), using random effects to account for measurement heterogeneity across monitoring stations equipped with different sensor technologies.

</details>


### [25] [Novel Tau-Informed Initialization for Maximum Likelihood Estimation of Copulas with Discrete Margins](https://arxiv.org/abs/2511.13337)
*Anna van Es,Eva Cantoni*

Main category: stat.ME

TL;DR: 本文针对低计数泊松数据的Gaussian-copula模型，提出了三种基于Kendall's tau的初始化方法，结合精确最大似然估计，在低计数情况下实现了更好的参数估计性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在低计数离散数据中，传统方法面临可识别性和数值稳定性问题，需要开发既能保持最大似然统计保证又计算可行的方法。

Method: 开发了三种Kendall's tau初始化方法（精确法、低强度近似法和变换法），结合无约束参数化、精确矩形概率计算和分析得分函数，嵌入IFM框架进行最大似然估计。

Result: 模拟研究表明，所提方法在维度、依赖水平和强度范围内，相比替代方法具有更低的RMSE、更小的偏差和更快的计算时间。

Conclusion: 该方法为中等至高维离散数据提供了一条实用路径，既保持了最大似然的统计保证，又保持了计算可行性，并讨论了向其他相关结构和不同边缘分布的扩展。

Abstract: We study Gaussian-copula models with discrete margins, with primary emphasis on low-count (Poisson) data. Our goal is exact yet computationally efficient maximum likelihood (ML) estimation in regimes where many observations contain small counts, which imperils both identifiability and numerical stability. We develop three novel Kendall's tau-based approaches for initialization tailored to discrete margins in the low-count regime and embed it within an inference functions for margins (IFM) inspired start. We present three practical initializers (exact, low-intensity approximation, and a transformation-based approach) that substantially reduce the number of ML iterations and improve convergence. For the ML stage, we use an unconstrained reparameterization of the model's parameters using the log and spherical-Cholesky and compute exact rectangle probabilities. Analytical score functions are supplied throughout to stabilize Newton-type optimization. A simulation study across dimensions, dependence levels, and intensity regimes shows that the proposed initialization combined with exact ML achieves lower root-mean-squared error, lower bias and faster computation times than the alternative procedures. The methodology provides a pragmatic path to retain the statistical guarantees of ML (consistency, asymptotic normality, efficiency under correct specification) while remaining tractable for moderate- to high-dimensional discrete data. We conclude with guidance on initializer choice and discuss extensions to alternative correlation structures and different margins.

</details>


### [26] [Handling outcome-dependent missingness with binary responses: A Heckman-like model](https://arxiv.org/abs/2511.11776)
*Marco Doretti,Elena Stanghellini,Alessandro Taraborrelli*

Main category: stat.ME

TL;DR: 扩展Heckman模型到二元结果和逻辑回归设置，基于相对风险推导类似逆米尔斯比的校正项，用于信息性缺失情况下的偏差校正。


<details>
  <summary>Details</summary>
Motivation: 当缺失机制依赖于结果本身时，回归模型中的选择偏差问题需要解决，特别是在二元结果和逻辑回归框架下。

Method: 将Heckman模型扩展到二元结果设置，使用逻辑回归建模选择过程和结果，基于相对风险推导校正项。

Result: 在给定假设下，该策略为信息性缺失情况下的偏差校正提供了有效工具。

Conclusion: 提出的方法能够有效校正二元结果模型中因信息性缺失引起的选择偏差。

Abstract: In regression models with missing outcomes, selection bias can arise when the missingness mechanism depends on the outcome itself. This proposal focuses on an extension of the Heckman model to a setting where the outcome is binary and both the selection process and the outcome are modeled through logistic regression. A correction term analogous to the inverse Mills' ratio is derived based on relative risks. Under given assumptions, such a strategy provides an effective tool for bias correction in the presence of informative missingness.

</details>


### [27] [Approximate Bayesian computation for stochastic hybrid systems with ergodic behaviour](https://arxiv.org/abs/2511.11782)
*Sascha Desmettre,Agnes Mallinger,Amira Meddah,Irene Tubikanec*

Main category: stat.ME

TL;DR: 提出了一种基于近似贝叶斯计算(ABC)的PDifMP参数推断框架，包含模拟算法、混合系统摘要统计量扩展，并在多个示例中验证了方法的有效性


<details>
  <summary>Details</summary>
Motivation: PDifMP模型的实际应用价值依赖于准确的参数估计，但现有方法在处理这类混合系统时存在挑战

Method: 基于ABC的参数推断框架，包括PDifMP样本路径模拟算法，以及针对混合系统特性扩展的ABC摘要统计量

Result: 该方法在所有示例中都能可靠地恢复模型参数，即使在只有部分跳跃和扩散信息或参数出现在状态依赖跳跃率函数的挑战性场景下

Conclusion: ABC可作为复杂随机混合系统中推断的实用工具，在PDifMP参数估计方面展现出良好潜力

Abstract: Piecewise diffusion Markov processes (PDifMPs) form a versatile class of stochastic hybrid systems that combine continuous diffusion processes with discrete event-driven dynamics, enabling flexible modelling of complex real-world hybrid phenomena. The practical utility of PDifMP models, however, depends critically on accurate estimation of their underlying parameters. In this work, we present a novel framework for parameter inference in PDifMPs based on approximate Bayesian computation (ABC). Our contributions are threefold. First, we provide detailed simulation algorithms for PDifMP sample paths. Second, we extend existing ABC summary statistics for diffusion processes to account for the hybrid nature of PDifMPs, showing particular effectiveness for ergodic systems. Third, we demonstrate our approach on several representative example PDifMPs that empirically exhibit ergodic behaviour. Our results show that the proposed ABC method reliably recovers model parameters across all examples, even in challenging scenarios where only partial information on jumps and diffusion is available or when parameters appear in state-dependent jump rate functions. These findings highlight the potential of ABC as a practical tool for inference in various complex stochastic hybrid systems.

</details>


### [28] [Graphical Model-based Inference on Persistent Homology](https://arxiv.org/abs/2511.11996)
*Zitian Wu,Arkaprava Roy,Leo L. Duan*

Main category: stat.ME

TL;DR: 提出了一种基于图形模型的持久同调分析方法，将拓扑特征与图结构关联，通过贝叶斯推断定位拓扑差异的来源。


<details>
  <summary>Details</summary>
Motivation: 传统持久同调方法主要作为特征提取器，虽然能检测全局差异但难以定位差异来源，需要更精细的统计建模方法。

Method: 将每个顶点与锥形空间中的潜在位置关联，使用指数分布对条形码的生死时间建模，采用贝叶斯方法推断潜在位置并支持层次模型扩展。

Result: 在阿尔茨海默病神经影像研究中成功定位了拓扑差异来源，提供了可解释的模型分析。

Conclusion: 该方法能够精确定位拓扑差异来源，为复杂数据中的拓扑结构提供了基于模型的解释性分析。

Abstract: Persistent homology is a cornerstone of topological data analysis, offering a multiscale summary of topology with robustness to nuisance transformations, such as rotations and small deformations. Persistent homology has seen broad use across domains such as computer vision and neuroscience. Most statistical treatments, however, use homology primarily as a feature extractor, relying on statistical distance-based tests or simple time-to-event models for inferential tasks. While these approaches can detect global differences, they rarely localize the source of those differences. We address this gap by taking a graphical model-based approach: we associate each vertex with a population latent position in a conic space and model each bar's key events (birth and death times) using an exponential distribution, whose rate is a transformation of the latent positions according to an event occurring on the graph. The low-dimensional bars have simple graph-event representations, such as the formation of a minimum spanning tree or the triangulation of a loop, and thus enjoy tractable likelihoods. Taking a Bayesian approach, we infer latent positions and enable model extensions such as hierarchical models that allow borrowing strength across groups. Applications to a neuroimaging study of Alzheimer's disease demonstrate that our method localizes sources of difference and provides interpretable, model-based analyses of topological structure in complex data. The code is provided and maintained at https://github.com/zitianwu/graphPH.

</details>


### [29] [Regression Analysis After Bipartite Bayesian Record Linkage](https://arxiv.org/abs/2511.12343)
*Xueyan Hu,Jerome P. Reiter*

Main category: stat.ME

TL;DR: 提出了一个多重插补框架，将记录链接和回归分析整合在一起，通过贝叶斯记录链接模型生成多个可能的链接数据集，并使用研究变量信息估计混合模型，从而提高回归模型参数的估计准确性。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法（先链接记录再进行分析）没有考虑链接不确定性对模型参数的影响，也没有利用两个文件中研究变量之间的关系来帮助确定链接。

Method: 使用二分贝叶斯记录链接模型生成多个可能的链接数据集，然后假设每个链接文件包含真实链接和错误链接的混合，利用研究变量信息估计混合模型。

Result: 在回归设置下的模拟研究表明，该方法对回归模型参数的估计比类似的两阶段方法更准确。

Conclusion: 提出的集成方法能够更准确地估计回归模型参数，通过同时考虑链接不确定性和研究变量信息来提高分析质量。

Abstract: In many settings, a data curator links records from two files to produce datasets that are shared with secondary analysts. Analysts use the linked files to estimate models of interest, such as regressions. Such two-stage approaches do not necessarily account for uncertainty in model parameters that results from uncertainty in the linkages. Further, they do not leverage the relationships among the study variables in the two files to help determine the linkages. We propose a multiple imputation framework to address these shortcomings. First, we use a bipartite Bayesian record linkage model to generate multiple plausible linked datasets, disregarding the information in the study variables. Second, we presume each linked file has a mixture of true links and false links. We estimate the mixture model using information from the study variables. Through simulation studies under a regression setting, we demonstrate that estimates of the regression model parameters can be more accurate than those based on an analogous two-stage approach. We illustrate the integrated approach using data from the Survey on Household Income and Wealth, examining a regression involving the persistence of income.

</details>


### [30] [MMDCP: A Distribution-free Approach to Outlier Detection and Classification with Coverage Guarantees and SCW-FDR Control](https://arxiv.org/abs/2511.12016)
*Youwu Lin,Xiaoyu Qian,Jinru Wu,Qi Liu,Pei Wang*

Main category: stat.ME

TL;DR: 提出了MMDCP框架，用于标签偏移下的多类分类和异常检测，结合类特定距离度量和完全保形预测，产生自适应预测集，有效控制类间错误发现率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在标签偏移情况下依赖重采样，计算成本高且预测集过于保守，覆盖不稳定，特别是在小样本中。

Method: 结合类特定距离度量和完全保形预测构建评分函数，建立收敛率理论，引入新的全局误差度量SCW-FDR。

Result: 理论保证了有效覆盖和CW-FDR控制，模拟和实际数据应用验证了方法的优势。

Conclusion: MMDCP框架在标签偏移下提供了计算高效、覆盖稳定且能有效控制错误发现率的解决方案。

Abstract: We propose the Modified Mahalanobis Distance Conformal Prediction (MMDCP), a unified framework for multi-class classification and outlier detection under label shift, where the training and test distributions may differ. In such settings, many existing methods construct nonconformity scores based on empirical cumulative or density functions combined with data-splitting strategies. However, these approaches are often computationally expensive due to their heavy reliance on resampling procedures and tend to produce overly conservative prediction sets with unstable coverage, especially in small samples. To address these challenges, MMDCP combines class-specific distance measures with full conformal prediction to construct a score function, thereby producing adaptive prediction sets that effectively capture both inlier and outlier structures. Under mild regularity conditions, we establish convergence rates for the resulting sets and provide the first theoretical characterization of the gap between oracle and empirical conformal $p$-values, which ensures valid coverage and effective control of the class-wise false discovery rate (CW-FDR). We further introduce the Summarized Class-Wise FDR (SCW-FDR), a novel global error metric aggregating false discoveries across classes, and show that it can be effectively controlled within the MMDCP framework. Extensive simulations and two real-data applications support our theoretical findings and demonstrate the advantages of the proposed method.

</details>


### [31] [Transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares](https://arxiv.org/abs/2511.13296)
*Michail Tsagris*

Main category: stat.ME

TL;DR: 本文提出了一种新的单纯形-单纯形回归方法，将模型重新表述为约束逻辑回归，并使用约束迭代重加权最小二乘法来估计回归系数，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的单纯形-单纯形回归模型虽然避免了数据转换，但在使用EM算法估计回归系数时计算效率较低。本文旨在开发一种更快的估计方法。

Method: 将单纯形-单纯形回归模型重新表述为约束逻辑回归问题，并采用约束迭代重加权最小二乘法来估计回归系数。

Result: 与使用EM算法的现有方法相比，新方法显著提高了回归系数的估计速度。

Conclusion: 通过将模型重新表述为约束逻辑回归并使用约束迭代重加权最小二乘法，可以更有效地估计单纯形-单纯形回归模型的系数，为处理成分数据回归问题提供了更实用的解决方案。

Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \cite{fiksel2022} proposed a transformation-free lienar regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions was recently proposed. To effectively estimate the regression coefficients the EM algorithm was employed. We formulate the model as a constrained logistic regression, in the spirit of \cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. This approach makes the estimation procedure significantly faster.

</details>


### [32] [Aggregating Conformal Prediction Sets via α-Allocation](https://arxiv.org/abs/2511.12065)
*Congbin Xu,Yue Yu,Haojie Ren,Zhaojun Wang,Changliang Zou*

Main category: stat.ME

TL;DR: 本文提出了COLA方法，通过最优分配置信水平来聚合多个共形预测集，在保证覆盖率的同时最小化预测集大小。


<details>
  <summary>Details</summary>
Motivation: 共形预测能构建具有有限样本覆盖率的预测集，但如何有效利用多个一致性分数来减小预测集大小仍是一个重大挑战。

Method: 提出了COLA方法，包括COLA-s（样本分割）和COLA-f（完全共形化）两个变体保证边际覆盖率，以及COLA-l实现渐近条件覆盖率。

Result: 在合成和真实数据集上的实验表明，COLA在保持有效覆盖率的同时，比现有最优方法获得了显著更小的预测集。

Conclusion: COLA提供了一种原则性的聚合策略，能有效减小共形预测集的大小，同时保持理论保证的覆盖率。

Abstract: Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage.

</details>


### [33] [Shortest fixed-width confidence intervals for a bounded parameter: The Push algorithm](https://arxiv.org/abs/2511.13694)
*Jay Bartroff,Asmit Chakraborty*

Main category: stat.ME

TL;DR: 提出了一种计算有界参数最优固定宽度置信区间的方法（Push算法），适用于离散或有单调似然比性质的连续参数，在二项、超几何和正态分布中表现优于标准方法


<details>
  <summary>Details</summary>
Motivation: 扩展Asparaouhov和Lorden针对二项分布的Push算法，为单一有界参数提供最优固定宽度置信区间计算方法

Method: Push算法，生成给定置信水平下最短的非递减置信区间，若不存在则证明无解

Result: 在二项、超几何和正态分布中均优于标准方法，对正态分布甚至改进z区间，并成功应用于WHO烟草使用数据

Conclusion: 该方法为有界参数提供了最优固定宽度置信区间计算的有效解决方案，在实际应用中表现出色

Abstract: We present a method for computing optimal fixed-width confidence intervals for a single, bounded parameter, extending a method for the binomial due to Asparaouhov and Lorden, who called it the Push algorithm. The method produces the shortest possible non-decreasing confidence interval for a given confidence level, and if the Push interval does not exist for a given width and level, then no such interval exists. The method applies to any bounded parameter that is discrete, or is continuous and has the monotone likelihood ratio property. We demonstrate the method on the binomial, hypergeometric, and normal distributions with our available R package. In each of these distributions the proposed method outperforms the standard ones, and in the latter case even improves upon the $z$-interval. We apply the proposed method to World Health Organization (WHO) data on tobacco use.

</details>


### [34] [Determinants of financial and digital inclusion in West and Central Africa: Evidence from binary models with cross-validation](https://arxiv.org/abs/2511.12179)
*Ismaila A. Jallow,Samya Tajmouati*

Main category: stat.ME

TL;DR: 本研究使用世界银行Findex 2021数据，分析了西非和中非地区金融与数字包容性的决定因素，通过K折交叉验证和Vuong检验等方法发现性别、教育、收入和居住地是主要影响因素，地区间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 传统研究仅依赖logit和probit模型，本研究旨在通过更稳健的方法分析西非和中非地区金融与数字包容性的决定因素，填补现有研究空白。

Method: 使用世界银行Findex 2021数据，结合国别分析和稳健性检验（包括K折交叉验证和Vuong检验），分析全样本、西非子样本和中非子样本。

Result: 性别、教育水平、收入水平和居住地是全样本和西非子样本的重要影响因素；在中非子样本中，性别不显著，但年龄、教育、收入和农村居住是关键因素。加纳在金融包容性方面表现最佳，尼日利亚在正式账户拥有率和储蓄方面领先，但移动货币账户和数字支付落后于加纳。

Conclusion: 西非和中非地区在金融与数字包容性方面存在显著差异，政策制定应考虑地区特异性，中非国家需要特别关注提升包容性水平。

Abstract: This study examines the determinants of financial and digital inclusion in West and Central Africa using the World Bank Findex 2021 data. Unlike prior works that rely solely on traditional logit and probit models, we combine country-by-country analysis with robustness checks including K-fold cross-validation and Vuong test. Three samples were considered : a full sample combin- ing both regions and two separate subsamples for West and Central Africa. The results indicate that gender, educational attainment, income level, and place of residence are significant factors influencing both financial and digital inclusion in the full sample and the West African subsam- ple. In the Central African subsample, gender is not significant; however, age, education, income, and rural residence emerge as key determinants of financial and digital inclusion. Overall, Ghana stands out as the most financially inclusive country, although it trails Senegal in terms of credit access and digital payment use. Nigeria leads in formal account ownership and formal savings but falls considerably behind Ghana in mobile money account ownership and digital payments. Central African countries generally report lower levels of inclusion compared to West Africa, with Cameroon performing comparatively better.

</details>


### [35] [Bayesian Causal Discovery with Cycles and Latent Confounders](https://arxiv.org/abs/2511.12333)
*Wei Jin,Lang Lang,Amanda B. Spence,Leah H. Rubin,Yanxun Xu*

Main category: stat.ME

TL;DR: 提出BayCausal框架，解决存在循环和潜在混杂因素时的因果发现问题，通过贝叶斯方法实现唯一因果识别，并开发了R软件包


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法通常假设无潜在混杂因素且因果图为无环，但这些假设在现实应用中经常被违反，需要能同时处理循环和潜在混杂因素的方法

Method: 结合噪声独立成分分析和因子分析的可识别性结果，开发完全贝叶斯因果结构学习方法BayCausal

Result: 在模拟研究中显示出优越性能，应用于女性HIV研究数据集获得可解释的临床见解，开发了首个能处理循环和潜在混杂因素的公开软件

Conclusion: BayCausal是首个能同时处理循环和潜在混杂因素并实现唯一因果识别的公开可用方法，在理论和应用上都有重要贡献

Abstract: Learning causality from observational data has received increasing interest across various scientific fields. However, most existing methods assume the absence of latent confounders and restrict the underlying causal graph to be acyclic, assumptions that are often violated in many real-world applications. In this paper, we address these challenges by proposing a novel framework for causal discovery that accommodates both cycles and latent confounders. By leveraging the identifiability results from noisy independent component analysis and recent advances in factor analysis, we establish the unique causal identifiability under mild conditions. Building on this foundation, we further develop a fully Bayesian approach for causal structure learning, called BayCausal, and evaluate its identifiability, utility, and superior performance against state-of-the-art alternatives through extensive simulation studies. Application to a dataset from the Women's Interagency HIV Study yields interpretable and clinically meaningful insights. To facilitate broader applications, we have implemented BayCausal in an R package, BayCausal, which is the first publicly available software capable of achieving unique causal identification in the presence of both cycles and latent confounders.

</details>


### [36] [Group Identification and Variable Selection in Multivariable Mendelian Randomization with Highly-Correlated Exposures](https://arxiv.org/abs/2511.12375)
*Yinxiang Wu,Neil M. Davies,Ting Ye*

Main category: stat.ME

TL;DR: MVMR-PACS是一种新的多变量孟德尔随机化方法，通过识别信号组（具有高遗传相关性或不可区分因果效应的风险因素集合）来估计每个组的直接效应，解决了现有方法的弱工具偏倚、可解释性差和缺乏有效后选择推断的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多变量孟德尔随机化方法面临三个主要限制：弱工具偏倚、有限的可解释性以及缺乏有效的后选择推断，特别是在高维设置中处理许多强相关候选风险因素时。

Method: MVMR-PACS最小化一个去偏的目标函数以减少弱工具偏倚，同时产生具有变量选择理论保证的可解释估计。该方法采用数据稀疏化策略到汇总数据MVMR中，以实现有效的后选择推断。

Result: 在模拟中，MVMR-PACS在估计准确性和变量选择方面优于现有方法。应用于27个脂蛋白亚组分特征和冠状动脉疾病风险时，该方法识别出具有生物学意义和稳健的信号组，并提供了可解释的直接因果效应。

Conclusion: MVMR-PACS通过识别信号组和估计直接效应，为高维多变量孟德尔随机化分析提供了一种改进的方法，具有更好的估计性能、可解释性和统计推断能力。

Abstract: Multivariable Mendelian Randomization (MVMR) estimates the direct causal effects of multiple risk factors on an outcome using genetic variants as instruments. The growing availability of summary-level genetic data has created opportunities to apply MVMR in high-dimensional settings with many strongly correlated candidate risk factors. However, existing methods face three major limitations: weak instrument bias, limited interpretability, and the absence of valid post-selection inference. Here we introduce MVMR-PACS, a method that identifies signal-groups -- sets of causal risk factors with high genetic correlation or indistinguishable causal effects -- and estimates the direct effect of each group. MVMR-PACS minimizes a debiased objective function that reduces weak instrument bias while yielding interpretable estimates with theoretical guarantees for variable selection. We adapt a data-thinning strategy to summary-data MVMR to enable valid post-selection inference. In simulations, MVMR-PACS outperforms existing approaches in both estimation accuracy and variable selection. When applied to 27 lipoprotein subfraction traits and coronary artery disease risk, MVMR-PACS identifies biologically meaningful and robust signal-groups with interpretable direct causal effects.

</details>


### [37] [Transfer learning for high-dimensional Factor-augmented sparse linear model](https://arxiv.org/abs/2511.12435)
*Bo Fu,Dandan Jiang*

Main category: stat.ME

TL;DR: 提出高维因子增强稀疏线性模型的迁移学习方法，通过利用异构辅助数据集提高估计精度，同时解决强相关预测变量和潜在因子结构的挑战。


<details>
  <summary>Details</summary>
Motivation: 经济和金融应用中，强相关预测变量和潜在因子结构给可靠估计带来重大挑战，传统线性模型容易出现模型设定错误。目标数据集通常有限，但多个异构辅助源可能提供额外信息。

Method: 开发迁移学习程序，有效利用辅助数据集提高估计精度；引入数据驱动的源检测算法识别信息性辅助数据集；提供因子模型充分性检验框架和回归系数同时置信区间构建方法。

Result: 建立了所提估计量的非渐近ℓ₁-和ℓ₂-误差界；证明了源检测算法的一致性；数值研究表明方法在估计精度上获得显著提升，并在数据集异构性下保持稳健。

Conclusion: 该框架为在具有高度相关特征和潜在因子结构的环境中整合异构辅助信息提供了理论基础和实际可扩展的解决方案。

Abstract: In this paper, we study transfer learning for high-dimensional factor-augmented sparse linear models, motivated by applications in economics and finance where strongly correlated predictors and latent factor structures pose major challenges for reliable estimation. Our framework simultaneously mitigates the impact of high correlation and removes the additional contributions of latent factors, thereby reducing potential model misspecification in conventional linear modeling. In such settings, the target dataset is often limited, but multiple heterogeneous auxiliary sources may provide additional information. We develop transfer learning procedures that effectively leverage these auxiliary datasets to improve estimation accuracy, and establish non-asymptotic $\ell_1$- and $\ell_2$-error bounds for the proposed estimators. To prevent negative transfer, we introduce a data-driven source detection algorithm capable of identifying informative auxiliary datasets and prove its consistency. In addition, we provide a hypothesis testing framework for assessing the adequacy of the factor model, together with a procedure for constructing simultaneous confidence intervals for the regression coefficients of interest. Numerical studies demonstrate that our methods achieve substantial gains in estimation accuracy and remain robust under heterogeneity across datasets. Overall, our framework offers a theoretical foundation and a practically scalable solution for incorporating heterogeneous auxiliary information in settings with highly correlated features and latent factor structures.

</details>


### [38] [The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias](https://arxiv.org/abs/2511.12459)
*Marco Pollanen*

Main category: stat.ME

TL;DR: 大规模监控系统在筛选大量属性时会产生大量误报，即使单个巧合的概率极低。这是由于高维筛选的数学特性导致的，无法通过调整阈值或算法改进来避免。


<details>
  <summary>Details</summary>
Motivation: 研究大规模监控系统中误报问题的根本原因，揭示即使单个属性匹配概率极低，在多属性筛选下仍会产生大量误报的数学机制。

Method: 通过概率分析和数学模型，研究高维空间中多属性筛选的统计特性，分析系统从可靠到不可靠的临界转变。

Result: 发现当系统监控1000个属性（每个误报率0.5%），在100万人口中匹配任意15个属性会产生约226个误报，证明看似不可能的事件实际上几乎必然发生。

Conclusion: 大规模监控系统存在根本性的概率限制，数据规模和相关性会导致系统进入不可靠状态，这种结构性偏差无法通过技术手段消除。

Abstract: For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.
  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \(10^{-35}\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.
  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement.

</details>


### [39] [Determining the K in K-fold cross-validation](https://arxiv.org/abs/2511.12698)
*Kenichiro McAlinn,Kosaku Takanashi*

Main category: stat.ME

TL;DR: 提出了一种基于效用函数的方法来确定交叉验证中最佳K值，通过平衡预测准确性和评估不确定性来选择最优数据分割比例。


<details>
  <summary>Details</summary>
Motivation: 传统交叉验证中K值选择依赖惯例（如K=5或80:20分割），但K值选择会影响模型推断和评估结果，需要更原则性的方法。

Method: 推导评估不确定性的有限样本上界，采用基于效用的方法明确权衡训练数据量（影响模型准确性）和测试数据量（影响评估不确定性）。

Result: 在真实数据集上的线性回归和随机森林实验表明，最优K值取决于数据和模型特性，传统选择隐含了对数据基本特征的假设。

Conclusion: 该框架使K值选择假设显式化，提供基于数据和模型的透明选择方法，替代一刀切的惯例选择，实现更可靠的跨领域预测性能比较。

Abstract: Cross-validation is a standard technique used across science to test how well a model predicts new data. Data are split into $K$ "folds," where one fold (i.e., hold-out set) is used to evaluate a model's predictive ability. Researchers typically rely on conventions when choosing $K$, commonly $K=5$, or $80{:}20$ split, even though the choice of $K$ can affect inference and model evaluation. Principally, this $K$ should be determined by balancing the predictive accuracy (bias) and the uncertainty of this accuracy (variance), which forms a tradeoff based on the size of the hold-out set. More training data means more accurate models, but fewer testing data lead to uncertain evaluation, and vice versa. The challenge is that this evaluation uncertainty cannot be estimated directly from data. We propose a procedure to determine the optimal $K$ by deriving a finite-sample upper bound on the evaluation uncertainty and adopting a utility-based approach to make this tradeoff explicit. Analyses of real-world datasets using linear regression and random forest demonstrate this procedure in practice, providing insight into implicit assumptions, robustness, and model performance. Critically, the results show that the optimal $K$ depends on both the data and the model, and that conventional choices implicitly make assumptions about the fundamental characteristics of the data. Our framework makes these assumptions explicit and provides a principled, transparent way to select $K$ based on the data and model rather than convention. By replacing a one-size-fits-all choice with context-specific reasoning, it enables more reliable comparisons of predictive performance across scientific domains.

</details>


### [40] [Scalable and Communication-Efficient Varying Coefficient Mixed Effect Models: Methodology, Theory, and Applications](https://arxiv.org/abs/2511.12732)
*Lida Chalangar Jalili Dehkharghani,Li-Hsiang Lin*

Main category: stat.ME

TL;DR: 开发了一种通信高效的变系数混合模型推断框架，用于建模大规模人类迁移数据中的复杂时空依赖模式，在通信受限环境下保持统计效率


<details>
  <summary>Details</summary>
Motivation: 人类迁移数据具有复杂的时空依赖性，且通常跨越多个行政边界。在分布式环境中，传输原始数据或大型设计矩阵成本高昂或受限，需要开发通信高效的统计方法

Method: 基于惩罚样条估计器的贝叶斯层次表示，提取充分统计量。在无通信约束时聚合这些统计量可重现集中式估计器；在通信受限时，使用相同统计量定义代理似然函数进行一步估计

Result: 该方法具有一阶统计效率，通过SVD增强实现确保数值稳定性和可扩展性。模拟研究证实了方法的准确性和鲁棒性，应用于美国迁移流数据成功揭示了动态空间模式

Conclusion: 提出的通信高效推断框架能够有效处理大规模分层迁移数据，在通信受限和不受限环境下均能提供统计保证，为分布式环境下的复杂时空建模提供了实用解决方案

Abstract: Human migration exhibits complex spatiotemporal dependence driven by environmental and socioeconomic forces. Modeling such patterns at scale-often across multiple administrative or institutional boundaries-requires statistically efficient methods that remain robust under limited communication, i.e., when transmitting raw data or large design matrices across distributed nodes is costly or restricted. This paper develops a communication-efficient inference framework for Varying Coefficient Mixed Models (VCMMs) that accommodates many input variables in the mean structure and rich correlation induced by numerous random effects in hierarchical migration data. We show that a penalized spline estimator admit a Bayesian hierarchical representation, which in turn yields sufficient statistics that preserve the full likelihood contribution of each node when communication is unconstrained; aggregating these summaries reproduces the centralized estimator exactly. Under communication constraints, the same summaries define a surrogate likelihood enabling one-step estimation with first-order statistical efficiency. The framework also incorporates an SVD-enhanced implementation to ensure numerical stability and scalability, extending applicability to settings with many random effects, with or without communication limits. Statistical and theoretical guarantees are provided. Extensive simulations confirm the accuracy and robustness of the method. An application to U.S. migration flow data demonstrates its ability to efficiently and precisely uncover dynamic spatial patterns.

</details>


### [41] [SIMBA: Scalable Image Modeling using a Bayesian Approach, A Consistent Framework for Including Spatial Dependencies in fMRI Studies](https://arxiv.org/abs/2511.12825)
*Yuan Zhong,Gang Chen,Paul A. Taylor,Jian Kang*

Main category: stat.ME

TL;DR: SIMBA是一个用于群体水平fMRI分析的贝叶斯空间建模框架，通过高斯过程先验和低秩核近似实现高效计算，在估计精度、激活检测和不确定性量化方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的大规模单变量fMRI分析方法存在信息浪费问题，需要一种能够显式纳入空间依赖性的灵活建模框架。

Method: 使用高斯过程先验捕捉空间关联模式，采用低秩核近似处理高维神经影像数据，开发了Gibbs采样器和变分推断两种高效后验计算算法。

Result: 模拟研究显示SIMBA在估计精度、激活检测灵敏度和不确定性量化方面优于竞争方法，特别是在低信噪比环境下；在NARPS和ABCD研究的大规模任务fMRI应用中验证了可扩展性和可解释性。

Conclusion: SIMBA提供了一个计算高效且统计严谨的贝叶斯框架，适用于大规模群体fMRI分析，能够捕捉有意义的空间模式并量化不确定性。

Abstract: Bayesian spatial modeling provides a flexible framework for whole-brain fMRI analysis by explicitly incorporating spatial dependencies, overcoming the limitations of traditional massive univariate approaches that lead to information waste. In this work, we introduce SIMBA, a Scalable Image Modeling using a Bayesian Approach, for group-level fMRI analysis, which places Gaussian process (GP) priors on spatially varying functions to capture smooth and interpretable spatial association patterns across the brain volume. To address the significant computational challenges of GP inference in high-dimensional neuroimaging data, we employ a low-rank kernel approximation that enables projection into a reduced-dimensional subspace. This allows for efficient posterior computation without sacrificing spatial resolution, and we have developed efficient algorithms for this implemented in Python that achieve fully Bayesian inference either within minutes using the Gibbs sampler or within seconds using mean-field variational inference (VI). Through extensive simulation studies, we first show that SIMBA outperforms competing methods in estimation accuracy, activation detection sensitivity, and uncertainty quantification, especially in low signal-to-noise settings. We further demonstrate the scalability and interpretability of SIMBA in large-scale task-based fMRI applications, analyzing both volumetric and cortical surface data from the NARPS and ABCD studies.

</details>


### [42] [Bayesian Variable Selection on Small Sample Trial Data via Adaptive Posterior-Informed Shrinkage Prior](https://arxiv.org/abs/2511.13000)
*Lingxuan Kong,Yumin Zhang,Chenkun Wang,Yaoyuan Vincent Tan*

Main category: stat.ME

TL;DR: 提出了APSP方法，一种贝叶斯变量选择方法，通过自适应地从外部数据源借用信息来提高小样本情况下的变量选择效率。


<details>
  <summary>Details</summary>
Motivation: 随着细胞和基因疗法以及超罕见疾病治疗的发展，急需能够在样本量严重受限情况下检测有意义关联的统计方法。

Method: APSP方法基于现有贝叶斯数据借用框架，结合数据驱动的自适应信息选择、混合收缩信息先验结构和经验零决策，以增强小样本量下的变量选择性能。

Result: 广泛的模拟研究表明，APSP相对于传统和流行的数据借用及贝叶斯变量选择方法具有更好的效率，同时在线性关系下保持稳健性。

Conclusion: APSP方法成功应用于临床胰岛移植研究，通过从CIT07研究借用信息来识别与第75天峰值C肽相关的变量。

Abstract: Identifying variables associated with clinical endpoints is of much interest in clinical trials. With the rapid growth of cell and gene therapy (CGT) and therapeutics for ultra-rare diseases, there is an urgent need for statistical methods that can detect meaningful associations under severe sample-size constraints. Motivated by data-borrowing strategies for historical controls, we propose the Adaptive Posterior-Informed Shrinkage Prior (APSP), a Bayesian approach that adaptively borrows information from external sources to improve variable-selection efficiency while preserving robustness across plausible scenarios. APSP builds upon existing Bayesian data borrowing frameworks, incorporating data-driven adaptive information selection, structure of mixture shrinkage informative priors and decision making with empirical null to enhance variable selection performances under small sample size. Extensive simulations show that APSP attains better efficiency relative to traditional and popular data-borrowing and Bayesian variable-selection methods while maintaining robustness under linear relationships. We further applied APSP to identify variables associated with peak C-peptide at Day 75 from the Clinical Islet Transplantation (CIT) Consortium study CIT06 by borrowing information from the study CIT07.

</details>


### [43] [A Gentle Introduction to Conformal Time Series Forecasting](https://arxiv.org/abs/2511.13608)
*M. Stocker,W. Małgorzewicz,M. Fontana,S. Ben Taieb*

Main category: stat.ME

TL;DR: 这篇综述统一了针对非交换数据的保形预测方法进展，提供了理论保证并比较了各种处理时间序列依赖性的技术。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测方法依赖于数据可交换性假设，但这一假设在时间序列数据中通常被违反，导致预测区间失效。

Method: 首先建立弱依赖条件下的理论保证，然后分类调查通过重新加权校准数据、动态更新残差分布或自适应调整覆盖水平来缓解序列依赖的方法。

Result: 通过综合模拟研究比较了不同技术在经验覆盖度、区间宽度和计算成本方面的表现。

Conclusion: 揭示了各种方法在实际应用中的权衡，并指出了未来的研究方向。

Abstract: Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions.

</details>


### [44] [The Bottom-Up Approach for Powerful Testing with FWER Control](https://arxiv.org/abs/2511.13624)
*Rajesh Karmakar,Ruth Heller,Saharon Rosset*

Main category: stat.ME

TL;DR: 提出了一种新颖的多重检验方法，通过自下而上的方法构建和谐封闭检验程序，在控制家族错误率的同时显著提高检验功效。


<details>
  <summary>Details</summary>
Motivation: 设计能够在控制家族错误率的同时考虑检验功效的多重检验程序，并确保计算效率。

Method: 采用自下而上的方法构建和谐封闭检验程序，在封闭检验层次结构的每个层级设计检验时考虑整体功效目标。

Result: 新方法在模拟和真实数据研究中相比现有方法显示出显著提高的检验功效，且计算可行。

Conclusion: 提出的自下而上方法为多重检验提供了一种通用框架，能够在控制错误率的同时实现更高的检验功效。

Abstract: We seek to design novel multiple testing procedures, which take into account a relevant notion of ''power'' or true discovery on the one hand, and allow computationally efficient test design and application on the other. Towards this end we characterize the optimal procedures that strongly control the family-wise error rate, for a range of power objectives measuring the success of multiple testing procedures in making true individual discoveries, and under a reasonable set of assumptions. While we cannot generally find these optimal solutions in practice, we propose the bottom-up approach, which constructs consonant closed testing procedures, while taking into account the overall power objective in designing the tests on every level of the closed testing hierarchy. This leads to a general recipe, yielding novel procedures which are computationally practical and demonstrate substantially improved power in both simulations and a real data study, compared to existing procedures.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [45] [SCoRES: An R Package for Simultaneous Confidence Region Estimates](https://arxiv.org/abs/2511.12242)
*Zhuoran Yu,Armin Schwartzman,Junting Ren,Julia Wrobel*

Main category: stat.CO

TL;DR: SCoRES R包实现了Ren等人(2024)的方法，用于估计逆区域并构建同时置信区域，解决了现有方法对域密度和函数连续性等限制性假设的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有逆域估计方法需要限制性假设（如域密度和阈值附近函数连续性）和大样本保证，限制了其适用性。此外，估计和覆盖依赖于难以确定的固定阈值水平。

Method: 通过反转同时置信带来非渐近地构建多个置信水平的同时置信集，SCoRES包实现了该方法的估计逆区域和相应同时内外置信区域的功能。

Result: SCoRES包提供了构建回归数据、函数数据和地理数据的同时置信带的功能，并包含可视化工具。通过三个严谨示例展示了其广泛适用性。

Conclusion: SCoRES R包提供了一个实用的工具，用于在气候学和医学等领域进行风险评估，克服了现有方法的局限性，具有广泛的适用性。

Abstract: The identification of domain sets whose outcomes belong to predefined subsets can address fundamental risk assessment challenges in climatology and medicine. Existing approaches for inverse domain estimates require restrictive assumptions, including domain density and continuity of function near thresholds, and large-sample guarantees, which limit the applicability. Besides, the estimation and coverage depend on setting a fixed threshold level, which is difficult to determine. Recently, Ren et al. (2024) proved that confidence sets of multiple levels can be simultaneously constructed with the desired confidence non-asymptotically through inverting simultaneous confidence bands. Here, we present the SCoRES R package, which implements Ren's approach for both the estimation of the inverse region and the corresponding simultaneous outer and inner confidence regions, along with visualization tools. Besides, the package also provides functions that help construct SCBs for regression data, functional data and geographical data. To illustrate its broad applicability, we present three rigorous examples that demonstrate the SCoRES workflow.

</details>


### [46] [Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems](https://arxiv.org/abs/2511.12257)
*Elhadji Cisse Faye,Mame Diarra Fall,Nicolas Dobigeon,Eric Barat*

Main category: stat.CO

TL;DR: 提出了一种基于贝叶斯框架的泊松逆问题求解方法，通过蒙特卡洛采样算法处理非欧几何结构，利用Bregman散度和数据增强技术实现高效采样。


<details>
  <summary>Details</summary>
Motivation: 泊松似然函数存在非Lipschitz梯度和正性约束等挑战，需要开发能够处理这些问题的有效采样算法。

Method: 使用基于Burg熵的Bregman散度推导数据增强模型，结合Gibbs采样和Hessian黎曼朗之万蒙特卡洛算法，在镜像流形上进行采样以满足正性约束。

Result: 在去噪、去模糊和正电子发射断层扫描实验中，该方法在重建质量方面与优化和采样方法相比具有竞争力。

Conclusion: 所提出的贝叶斯框架能够有效处理泊松逆问题，通过几何感知的采样算法获得高质量重建结果。

Abstract: This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.

</details>
