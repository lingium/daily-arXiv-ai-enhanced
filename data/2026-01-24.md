<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 9]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 5]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Distributional Balancing for Causal Inference: A Unified Framework via Characteristic Function Distance](https://arxiv.org/abs/2601.15449)
*Diptanil Santra,Guanhua Chen,Chan Park*

Main category: stat.ME

TL;DR: 提出基于特征函数距离的统一分布平衡加权框架，解决传统方法无法保证完整联合分布平衡的问题，提供理论保证和有效推断方法。


<details>
  <summary>Details</summary>
Motivation: 传统加权方法（如逆倾向得分加权、协变量矩匹配）只能间接平衡协变量，无法保证完整联合分布平衡。现有分布平衡方法缺乏统一框架、理论保证和有效推断程序。

Method: 基于特征函数距离（CFD）建立非参数分布平衡统一框架，将最大均值差异和能量距离等作为特例。提出子抽样推断方法，并扩展到工具变量设置以处理未测量混杂。

Result: 建立了CFD加权估计量的√n一致性条件，标准自助法可能失效但子抽样有效。模拟研究和实际应用显示该方法表现良好，与理论预测一致。

Conclusion: CFD框架为分布平衡加权提供了统一的理论基础和实践工具，解决了传统方法的局限性，为因果推断提供了更稳健的非参数方法。

Abstract: Weighting methods are essential tools for estimating causal effects in observational studies, with the goal of balancing pre-treatment covariates across treatment groups. Traditional approaches pursue this objective indirectly, for example, via inverse propensity score weighting or by matching a finite number of covariate moments, and therefore do not guarantee balance of the full joint covariate distributions. Recently, distributional balancing methods have emerged as robust, nonparametric alternatives that directly target alignment of entire covariate distributions, but they lack a unified framework, formal theoretical guarantees, and valid inferential procedures. We introduce a unified framework for nonparametric distributional balancing based on the characteristic function distance (CFD) and show that widely used discrepancy measures, including the maximum mean discrepancy and energy distance, arise as special cases. Our theoretical analysis establishes conditions under which the resulting CFD-based weighting estimator achieves $\sqrt{n}$-consistency. Since the standard bootstrap may fail for this estimator, we propose subsampling as a valid alternative for inference. We further extend our approach to an instrumental variable setting to address potential unmeasured confounding. Finally, we evaluate the performance of our method through simulation studies and a real-world application, where the proposed estimator performs well and exhibits results consistent with our theoretical predictions.

</details>


### [2] [Model-Free Inference for Characterizing Protein Mutations through a Coevolutionary Lens](https://arxiv.org/abs/2601.15566)
*Fan Yang,Zhao Ren,Wen Zhou,Kejue Jia,Robert Jernigan*

Main category: stat.ME

TL;DR: 将蛋白质接触预测转化为统计检验问题，通过偏相关图方法识别蛋白质位置间的接触关系，并提供统计推断量化预测不确定性


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质接触预测方法多为模型或算法驱动，缺乏统计推断来量化预测结果的不确定性。作者希望将接触预测转化为统计检验问题，提供更可靠的推断框架

Method: 提出新框架将接触预测转化为统计检验问题，基于连续变量的偏相关概念，通过one-hot编码MSA数据构建多元分类变量的偏相关图，引入新的基于谱的检验统计量来检验两个位置是否偏相关

Result: 数值实验表明该方法在控制I类错误方面有效且具有一般性的统计功效。在多种蛋白质家族的真实数据应用中验证了该方法在共进化和突变分析中的实际效用

Conclusion: 该框架不仅能够识别蛋白质接触，还能识别有助于已识别接触内相关性的氨基酸组合，为蛋白质突变研究提供了新的统计推断工具

Abstract: Multiple sequence alignment (MSA) data play a crucial role in the study of protein mutations, with contact prediction being a notable application. Existing methods are often model-based or algorithmic and typically do not incorporate statistical inference to quantify the uncertainty of the prediction outcomes. To address this, we propose a novel framework that transforms the task of contact prediction into a statistical testing problem. Our approach is motivated by the partial correlation for continuous random variables. With one-hot encoding of MSA data, we are able to construct a partial correlation graph for multivariate categorical variables. In this framework, two connected nodes in the graph indicate that the corresponding positions on the protein form a contact. A new spectrum-based test statistic is introduced to test whether two positions are partially correlated. Moreover, the new framework enables the identification of amino acid combinations that contribute to the correlation within the identified contacts, an important but largely unexplored aspect of protein mutations. Numerical experiments demonstrate that our proposed method is valid in terms of controlling Type I errors and powerful in general. Real data applications on various protein families further validate the practical utility of our approach in coevolution and mutation analysis.

</details>


### [3] [A two-sample pseudo-observation-based regression approach for the relative treatment effect](https://arxiv.org/abs/2601.15880)
*Dennis Dobler,Alina Schenk,Matthias Schmid*

Main category: stat.ME

TL;DR: 提出一种基于相对治疗效应的分布无关回归模型，适用于有序或右删失生存数据，使用伪观测方法估计系数，并通过bootstrap进行假设检验。


<details>
  <summary>Details</summary>
Motivation: 相对治疗效应是一种概率解释的效应度量，与ROC曲线下面积相关，已在文献中用于有序或右删失结局变量。需要建立一种分布无关的回归模型，将相对治疗效应与协变量的线性组合联系起来。

Method: 1) 提出分布无关回归模型，将相对治疗效应与协变量线性组合关联；2) 开发基于伪观测的估计程序，获得一致且渐近正态的系数估计；3) 提出基于bootstrap的假设检验方法，推断协变量对相对治疗效应的影响。

Result: 模拟研究显示，新方法的假设检验具有高功效，即使在Cox模型正确设定的情况下，也能与Cox模型的z检验相媲美。应用于SUCCESS-A试验的乳腺癌患者无进展生存数据重新分析。

Conclusion: 提出了一种灵活且稳健的回归方法，适用于有序和右删失结局，不依赖于分布假设，在多种场景下表现良好，为相对治疗效应的建模提供了实用工具。

Abstract: The relative treatment effect is an effect measure for the order of two sample-specific outcome variables. It has the interpretation of a probability and also a connection to the area under the ROC curve. In the literature it has been considered for both ordinal or right-censored time-to-event outcomes. For both cases, the present paper introduces a distribution-free regression model that relates the relative treatment effect to a linear combination of covariates. To fit the model, we develop a pseudo-observation-based procedure yielding consistent and asymptotically normal coefficient estimates. In addition, we propose bootstrap-based hypothesis tests to infer the effects of the covariates on the relative treatment effect. A simulation study compares the novel method to Cox regression, demonstrating that the proposed hypothesis tests have high power and keep up with the z-test of the Cox model even in scenarios where the latter is specified correctly. The new methods are used to re-analyze data from the SUCCESS-A trial for progression-free survival of breast cancer patients.

</details>


### [4] [Learning Functional Graphs with Nonlinear Sufficient Dimension Reduction](https://arxiv.org/abs/2601.15696)
*Kyongwon Kim,Bing Li*

Main category: stat.ME

TL;DR: 提出基于函数充分降维的非参数函数图模型，放松高斯假设，避免维度灾难，保持概率条件独立性作为边缺失标准


<details>
  <summary>Details</summary>
Motivation: 现有函数图模型要么依赖随机函数的参数分布形式，要么使用与概率条件独立性不同的加性条件独立性标准。需要一种既放松参数假设，又保持概率条件独立性标准的非参数方法

Method: 基于函数充分降维的非参数函数图模型，通过函数充分降维技术避免维度灾难，同时保持概率条件独立性作为图结构判断标准

Result: 通过模拟研究和f-MRI数据集分析，证明了该方法相比现有方法的优势，包括更好的估计精度和更合理的图结构推断

Conclusion: 提出的非参数函数图模型在放松分布假设、避免维度灾难和保持概率条件独立性方面具有综合优势，为函数图模型提供了更灵活有效的框架

Abstract: Functional graphical models have undergone extensive development during the recent years, leading to a variety models such as the functional Gaussian graphical model, the functional copula Gaussian graphical model, the functional Bayesian graphical model, the nonparametric functional additive graphical model, and the conditional functional graphical model. These models rely either on some parametric form of distributions on random functions, or on additive conditional independence, a criterion that is different from probabilistic conditional independence. In this paper we introduce a nonparametric functional graphical model based on functional sufficient dimension reduction. Our method not only relaxes the Gaussian or copula Gaussian assumptions, but also enhances estimation accuracy by avoiding the ``curse of dimensionality''. Moreover, it retains the probabilistic conditional independence as the criterion to determine the absence of edges. By doing simulation study and analysis of the f-MRI dataset, we demonstrate the advantages of our method.

</details>


### [5] [On the spherical cardioid distribution and its goodness-of-fit](https://arxiv.org/abs/2601.16095)
*Eduardo García-Portugués*

Main category: stat.ME

TL;DR: 本文研究了球面心形分布，这是圆形心形分布的高维高阶推广，具有旋转对称性，能生成单峰、多峰、轴向和带状密度。该分布具有高度可处理性：密度计算简单、卷积封闭、向量矩显式表达、高效模拟，且其矩与球面均匀分布一致。文中推导了矩估计和最大似然估计及其渐近性质，提出了基于投影经验分布函数的自助拟合优度检验方法，并以长周期彗星轨道建模展示了其实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 研究球面心形分布的动机在于推广圆形心形分布到高维空间，创建一个具有旋转对称性且能灵活生成多种密度形态（单峰、多峰、轴向、带状）的分布族。该分布需要具备良好的数学性质以便于实际应用，包括简单的密度计算、统计推断的可行性以及实际数据分析的有效性。

Method: 本文采用理论分析方法研究球面心形分布的数学性质，包括密度函数、卷积性质、矩计算和模拟方法。在统计推断方面，推导了矩估计法和最大似然估计法，建立了估计量的渐近分布和相对效率。同时开发了基于投影经验分布函数的自助拟合优度检验框架，包括投影分布推导和检验统计量的闭式表达式。

Result: 研究结果表明球面心形分布具有高度可处理性：密度计算简单、卷积封闭、向量矩有显式表达式、模拟效率高。其矩与球面均匀分布一致，显示了与均匀分布的接近性。成功推导了估计量的渐近分布和相对效率。建立了完整的自助拟合优度检验方法。在长周期彗星轨道建模的实际应用中证明了该分布的有效性。

Conclusion: 球面心形分布是一个数学性质优良的高维分布，具有旋转对称性和灵活的密度形态生成能力。其良好的可处理性使得统计推断和模拟变得简单高效。该分布在实际数据分析中具有应用价值，特别是在天体物理学等领域的球面数据建模中表现出色。

Abstract: In this paper, we study the spherical cardioid distribution, a higher-dimensional and higher-order generalization of the circular cardioid distribution. This distribution is rotationally symmetric and generates unimodal, multimodal, axial, and girdle-like densities. We show several characteristics of the spherical cardioid that make it highly tractable: simple density evaluation, closedness under convolution, explicit expressions for vectorized moments, and efficient simulation. The moments of the spherical cardioid up to a given order coincide with those of the uniform distribution on the sphere, highlighting its closeness to the latter. We derive estimators by the method of moments and maximum likelihood, their asymptotic distributions, and their asymptotic relative efficiencies. We give the machinery for a bootstrap goodness-of-fit test based on the projected-ecdf approach, including the projected distribution and closed-form expressions for test statistics. An application to modeling the orbits of long-period comets shows the usefulness of the spherical cardioid distribution in real data analyses.

</details>


### [6] [Leave-one-out testing for node-level differences in Gaussian graphical models](https://arxiv.org/abs/2601.15896)
*Davide Benussi,Ester Alongi,Erika Banzato*

Main category: stat.ME

TL;DR: 提出一种基于留一交叉验证Bartlett调整的节点级推断方法，用于高斯图模型中的两样本相等性检验，相比传统似然比检验具有更好的定位能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统似然比检验在可分解图上具有团分解特性，但定位能力有限且有限样本行为不稳定，需要更精确的节点级推断方法。

Method: 在完全连接图上使用留一交叉验证Bartlett调整检验进行节点级推断，得到的增量具有标准卡方零分布极限。

Result: 模拟实验验证了方法的有效性，案例研究展示了实际应用价值，能够为单个节点和固定大小子集提供校准的显著性水平。

Conclusion: 提出的节点级推断方法解决了传统检验的局限性，为高斯图模型中的两样本相等性检验提供了更精确、稳定的统计工具。

Abstract: We study two-sample equality testing in Gaussian graphical models. Classical likelihood ratio tests on decomposable graphs admit clique-wise factorizations, offering limited localization and unstable finite-sample behaviour. We propose node-level inference via a leave-one-out Bartlett-adjusted test on a fully connected graph. The resulting increments have standard chi-square null limits, enabling calibrated significance for single nodes and fixed-size subsets. Simulations confirm validity, and a case study shows practical utility.

</details>


### [7] [A Hierarchical Bayesian Framework for Model-based Prognostics](https://arxiv.org/abs/2601.15942)
*Xinyu Jia,Iason Papaioannou,Daniel Straub*

Main category: stat.ME

TL;DR: 提出基于分层贝叶斯建模的PHM框架，整合相似系统数据提升剩余使用寿命预测精度


<details>
  <summary>Details</summary>
Motivation: 当前基于模型的预测性维护算法主要依赖单个系统的实时数据，忽略了相似系统或组件的失效数据中包含的宝贵信息，这些信息可以显著提升预测准确性

Method: 采用分层贝叶斯建模框架，利用相似系统或组件的运行至失效数据学习超参数分布作为先验，随着更多信息可用时高效更新预测

Result: 通过裂纹增长和锂电池退化的真实数据实验验证，结果显示剩余使用寿命预测精度显著提升，框架通过预测分布促进不确定性管理

Conclusion: 分层贝叶斯建模框架有效整合相似系统数据，提高了预测性维护的预测精度和不确定性管理能力，为工程系统健康管理提供了更可靠的方法

Abstract: In prognostics and health management (PHM) of engineered systems, maintenance decisions are ideally informed by predictions of a system's remaining useful life (RUL) based on operational data. Model-based prognostics algorithms rely on a parametric model of the system degradation process. The model parameters are learned from real-time operational data collected on the system. However, there can be valuable information in data from similar systems or components, which is not typically utilized in PHM. In this contribution, we propose a hierarchical Bayesian modeling (HBM) framework for PHM that integrates both operational data and run-to-failure data from similar systems or components. The HBM framework utilizes hyperparameter distributions learned from data of similar systems or components as priors. It enables efficient updates of predictions as more information becomes available, allowing for increasingly accurate assessments of the degradation process and its associated variability. The effectiveness of the proposed framework is demonstrated through two experimental applications involving real-world data from crack growth and lithium battery degradation. Results show significant improvements in RUL prediction accuracy and demonstrate how the framework facilitates uncertainty management through predictive distributions.

</details>


### [8] [A Fast Monte Carlo Newton-Raphson Algorithm to Estimate Generalized Linear Mixed Models with Dense Covariance](https://arxiv.org/abs/2601.16022)
*Samuel I. Watson,Yixin Wang,Emanuele Giorgi*

Main category: stat.ME

TL;DR: 提出一种快速蒙特卡洛最大似然算法用于估计广义线性混合模型，通过随机牛顿-拉夫森方法近似对数似然的海森矩阵和梯度，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 广义线性混合模型（包括空间高斯过程模型）的估计通常被认为计算成本过高，即使对于中等规模的数据集也不可行，需要开发更高效的算法。

Method: 采用随机牛顿-拉夫森方法，通过抽取随机效应样本来近似对数似然的期望海森矩阵和梯度，提出新的停止准则和蒙特卡洛样本量选择策略。

Result: 与流行的集成嵌套拉普拉斯近似方法相比，新算法在估计器性能相似或更优的同时显著减少运行时间，在GPU硬件上可实现100倍以上的加速。

Conclusion: 该方法证明了全最大似然方法在大规模空间数据集上的可行性，为GLMM估计提供了高效的计算解决方案。

Abstract: Estimation of Generalised linear mixed models (GLMM) including spatial Gaussian process models is often considered computationally impractical for even moderately sized datasets. In this article, we propose a fast Monte Carlo maximum likelihood (MCML) algorithm for the estimation of GLMMs. The algorithm is a stochastic Newton-Raphson method, which approximates the expected Hessian and gradient of the log-likelihood by drawing samples of the random effects. We propose a new stopping criterion for efficient termination and preventing long runs of sampling in the stationary post-convergence phase of the algorithm and discuss Monte Carlo sample size choice. We run a series of simulation comparisons of spatial statistical models alongside the popular integrated nested Laplacian approximation method and demonstrate potential for similar or improved estimator performance and reduced running times. We also consider scaling of the algorithms to large datasets and demonstrate a greater than 100-fold reduction in running times using modern GPU hardware to illustrate the feasibility of full maximum likelihood methods with big spatial datasets.

</details>


### [9] [Inference on the Significance of Modalities in Multimodal Generalized Linear Models](https://arxiv.org/abs/2601.16196)
*Wanting Jin,Guorong Wu,Quefeng Li*

Main category: stat.ME

TL;DR: 提出了一种基于熵的统计推断方法，用于评估高维多模态广义线性模型中单个模态的显著性，通过期望相对熵量化模态信息增益，并开发了渐近分布为卡方的检验统计量。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态统计模型很流行，但缺乏严格的统计推断工具来评估单个模态在多模态模型中的显著性，特别是在高维模型中。现有方法难以量化一个模态相对于其他模态的信息增益。

Method: 提出了期望相对熵作为量化模态信息增益的度量，开发了基于偏差的统计量来估计期望相对熵，证明了其一致性和渐近分布近似于非中心卡方分布，从而能够计算置信区间和p值。

Result: 通过模拟实验验证了所提推断工具的经验性能，并将其应用于多模态神经影像数据集，展示了在各种高维多模态广义线性模型中的良好表现。

Conclusion: 该方法为高维多模态广义线性模型提供了有效的统计推断工具，能够可靠地评估单个模态的显著性，填补了该领域的方法学空白。

Abstract: Despite the popular of multimodal statistical models, there lacks rigorous statistical inference tools for inferring the significance of a single modality within a multimodal model, especially in high-dimensional models. For high-dimensional multimodal generalized linear models, we propose a novel entropy-based metric, called the expected relative entropy, to quantify the information gain of one modality in addition to all other modalities in the model. We propose a deviance-based statistic to estimate the expected relative entropy, prove that it is consistent and its asymptotic distribution can be approximated by a non-central chi-squared distribution. That enables the calculation of confidence intervals and p-values to assess the significance of the expected relative entropy for a given modality. We numerically evaluate the empirical performance of our proposed inference tool by simulations and apply it to a multimodal neuroimaging dataset to demonstrate its good performance on various high-dimensional multimodal generalized linear models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [10] [Robust X-Learner: Breaking the Curse of Imbalance and Heavy Tails via Robust Cross-Imputation](https://arxiv.org/abs/2601.15360)
*Eichi Uehara*

Main category: stat.ML

TL;DR: 提出Robust X-Learner (RX-Learner)框架，通过γ-散度目标函数解决X-Learner在极端类别不平衡和重尾分布下的异常值污染问题。


<details>
  <summary>Details</summary>
Motivation: 在广告技术和医疗等工业应用中，异质处理效应估计面临极端类别不平衡和重尾分布的双重挑战。X-Learner虽然通过交叉插补处理不平衡问题，但依赖MSE最小化时容易受到"异常值污染"影响，少数群体中的极端观测值偏差会传播到整个多数群体。

Method: 提出Robust X-Learner (RX-Learner)框架：1) 集成γ-散度目标函数（在Gaussian假设下等价于Welsch损失）到梯度提升机制中；2) 使用基于Majorization-Minimization原则的Proxy Hessian策略稳定非凸优化。

Result: 在半合成的Criteo Uplift数据集上评估，RX-Learner相比标准X-Learner将异质效应估计精度(PEHE)降低了98.6%，有效解耦了稳定的"核心"群体和波动的"边缘"群体。

Conclusion: RX-Learner通过稳健的损失函数和优化策略，成功解决了X-Learner在极端不平衡和重尾分布下的异常值污染问题，显著提升了异质处理效应估计的准确性。

Abstract: Estimating Heterogeneous Treatment Effects (HTE) in industrial applications such as AdTech and healthcare presents a dual challenge: extreme class imbalance and heavy-tailed outcome distributions. While the X-Learner framework effectively addresses imbalance through cross-imputation, we demonstrate that it is fundamentally vulnerable to "Outlier Smearing" when reliant on Mean Squared Error (MSE) minimization. In this failure mode, the bias from a few extreme observations ("whales") in the minority group is propagated to the entire majority group during the imputation step, corrupting the estimated treatment effect structure. To resolve this, we propose the Robust X-Learner (RX-Learner). This framework integrates a redescending γ-divergence objective -- structurally equivalent to the Welsch loss under Gaussian assumptions -- into the gradient boosting machinery. We further stabilize the non-convex optimization using a Proxy Hessian strategy grounded in Majorization-Minimization (MM) principles. Empirical evaluation on a semi-synthetic Criteo Uplift dataset demonstrates that the RX-Learner reduces the Precision in Estimation of Heterogeneous Effect (PEHE) metric by 98.6% compared to the standard X-Learner, effectively decoupling the stable "Core" population from the volatile "Periphery".

</details>


### [11] [Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization](https://arxiv.org/abs/2601.15500)
*Saptarshi Roy,Alessandro Rinaldo,Purnamrita Sarkar*

Main category: stat.ML

TL;DR: 本文研究了Rectified flow（RF）如何自动适应目标分布的内在低维性来加速采样，证明了RF采样器在精心设计的时间离散化方案下具有O(k/ε)的迭代复杂度，其中k是目标分布的内在维度。同时建立了DDPM与随机RF的等价关系，并设计了能适应低维性的随机RF采样器。


<details>
  <summary>Details</summary>
Motivation: Rectified flow（RF）因其生成效率和最先进性能而广受欢迎，但对其如何自动适应目标分布内在低维性以加速采样的理解有限。本文旨在研究RF采样器在多大程度上能利用目标分布的低维结构来降低采样复杂度。

Method: 1) 分析RF采样器在精心设计的时间离散化方案下的迭代复杂度；2) 建立DDPM与随机RF之间的等价关系，通过随机定位建立新连接；3) 基于此连接设计能适应目标分布低维性的随机RF采样器，并在更宽松的漂移估计精度要求下工作；4) 在合成数据和文本到图像数据上进行实验验证。

Result: 1) 证明RF采样器在适当时间离散化和足够准确的漂移估计下，具有O(k/ε)的迭代复杂度（忽略对数因子），其中k是目标分布的内在维度；2) 建立了DDPM与随机RF的等价关系；3) 设计了新的随机RF采样器，能在更宽松条件下适应目标分布的低维性；4) 实验表明新设计的时间离散化调度方案能提高性能。

Conclusion: RF采样器能有效利用目标分布的内在低维性来加速采样，通过精心设计的时间离散化方案可达到接近最优的迭代复杂度。建立的DDPM与随机RF的等价关系为理解扩散模型提供了新视角，设计的随机RF采样器在更宽松条件下仍能适应低维结构，实验验证了其优越性能。

Abstract: In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\varepsilon)$ (up to log factors), where $\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of
  the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.

</details>


### [12] [Non-Stationary Functional Bilevel Optimization](https://arxiv.org/abs/2601.15363)
*Jason Bohne,Ieva Petrulionyte,Michael Arbel,Julien Mairal,Paweł Polak*

Main category: stat.ML

TL;DR: SmoothFBO是首个针对非平稳函数双层优化的在线算法，通过时间平滑的随机超梯度估计器减少方差，在非平稳超参数优化和模型强化学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有函数双层优化方法局限于静态离线设置，在在线、非平稳场景中表现不佳，需要一种能在非平稳环境下有效工作的算法。

Method: 提出SmoothFBO算法，引入时间平滑的随机超梯度估计器，通过窗口参数减少方差，实现稳定的外层循环更新和次线性遗憾。

Result: SmoothFBO在非平稳超参数优化和基于模型的强化学习任务中持续优于现有FBO方法，展示了实际有效性。

Conclusion: SmoothFBO为在线非平稳场景下的双层优化提供了通用、理论可靠且实际可行的基础框架。

Abstract: Functional bilevel optimization (FBO) provides a powerful framework for hierarchical learning in function spaces, yet current methods are limited to static offline settings and perform suboptimally in online, non-stationary scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO with both theoretical guarantees and practical scalability. SmoothFBO introduces a time-smoothed stochastic hypergradient estimator that reduces variance through a window parameter, enabling stable outer-loop updates with sublinear regret. Importantly, the classical parametric bilevel case is a special reduction of our framework, making SmoothFBO a natural extension to online, non-stationary settings. Empirically, SmoothFBO consistently outperforms existing FBO methods in non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness. Together, these results establish SmoothFBO as a general, theoretically grounded, and practically viable foundation for bilevel optimization in online, non-stationary scenarios.

</details>


### [13] [On damage of interpolation to adversarial robustness in regression](https://arxiv.org/abs/2601.16070)
*Jingfu Peng,Yuhong Yang*

Main category: stat.ML

TL;DR: 本文研究了插值估计器在对抗性攻击下的鲁棒性，发现在非参数回归框架中，即使面对微妙的未来X攻击，插值估计器也必然表现次优，完美拟合会严重损害其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络通常参数众多且能达到零或接近零的训练误差，尽管这种插值现象存在，它们仍表现出良好的泛化性能。然而，DNN对对抗性扰动高度脆弱，这引发了一个自然问题：插值能否在未来X攻击下避免次优性能？

Method: 在非参数回归框架下研究插值估计器的对抗鲁棒性，分析插值估计器在面对未来X攻击时的理论性能，并进行数值实验验证理论发现。

Result: 研究发现插值估计器即使在微妙的未来X攻击下也必然表现次优，完美拟合会显著损害其鲁棒性。同时揭示了高插值区域中"简单尺寸诅咒"的有趣现象。

Conclusion: 插值估计器在对抗性攻击下存在固有缺陷，完美拟合会严重削弱其鲁棒性，这为理解深度神经网络在对抗环境中的行为提供了重要理论见解。

Abstract: Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.

</details>


### [14] [Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add](https://arxiv.org/abs/2601.16120)
*Zhengchi Ma,Anru R. Zhang*

Main category: stat.ML

TL;DR: 论文为不平衡分类中的合成数据增强建立了统一统计框架，发现合成数据并非总是有益，最佳合成数据量取决于生成器准确性和方向对齐性，并提出验证调优的合成数据量选择方法。


<details>
  <summary>Details</summary>
Motivation: 不平衡分类中少数类样本稀少导致标准训练偏向多数类，合成数据增强是常用方法，但两个基本问题未解决：何时合成增强真正有效？应生成多少合成样本？

Method: 开发统一统计框架研究不平衡数据添加合成少数类样本的模型，分析在平衡总体风险下的表现。提出验证调优合成数据量方法：通过最小化平衡验证损失来选择合成数据量，以完全平衡基线为中心但允许有意义偏离。

Result: 理论显示合成数据并非总是有益：在"局部对称"机制中，不平衡不是平衡最优附近的主要误差源，添加合成样本无法改善学习率甚至可能因放大生成器不匹配而降低性能。在"局部不对称"机制中，最佳合成数据量取决于生成器准确性和方向对齐性，可能偏离完全平衡基线。

Conclusion: 合成数据增强在不平衡学习中的效果具有条件性，需要根据数据特征和生成器质量谨慎调整。验证调优合成数据量方法在实践中有效，模拟和真实脓毒症预测研究支持理论发现。

Abstract: Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?
  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.

</details>


### [15] [Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints](https://arxiv.org/abs/2601.16174)
*Yiyao Yang*

Main category: stat.ML

TL;DR: 提出一个用于可靠表征学习的框架，将可靠性视为表征本身的一级属性，通过不确定性感知正则化和结构约束来建模表征级不确定性。


<details>
  <summary>Details</summary>
Motivation: 挑战传统机器学习中只关注预测阶段不确定性估计的假设，认为表征的可靠性应该被视为表征本身的一级属性，而不仅仅是预测输出的属性。

Method: 提出一个原则性框架，在表征空间中显式建模表征级不确定性，利用结构约束作为归纳偏置来正则化可行表征空间。引入不确定性感知正则化，并结合稀疏性、关系结构或特征组依赖等结构约束来定义有意义的几何结构。

Result: 该框架能够鼓励学习到不仅具有预测性，而且稳定、校准良好、对噪声和结构扰动具有鲁棒性的表征。结构约束有助于减少学习表征中的虚假变异性。

Conclusion: 该框架独立于特定模型架构，可以与广泛的表征学习方法集成，为可靠表征学习提供了新的视角和方法。

Abstract: Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [16] [Algebraic Statistics in OSCAR](https://arxiv.org/abs/2601.15807)
*Tobias Boege,Antony Della Vecchia,Marina Garrote-López,Benjamin Hollering*

Main category: stat.CO

TL;DR: OSCAR计算机代数系统新增代数统计模块，具有可扩展设计、序列化数据共享和先进隐式化算法


<details>
  <summary>Details</summary>
Motivation: 为OSCAR计算机代数系统开发代数统计功能，提供可扩展的模块化设计，支持数据共享和先进算法实现

Method: 在OSCAR系统中引入代数统计模块，采用可扩展架构设计，实现数据类型的序列化用于结果共享和数据库创建，集成最先进的隐式化算法

Result: 成功开发了OSCAR系统的代数统计模块，具备完整的数据序列化功能和高效的隐式化算法实现

Conclusion: OSCAR代数统计模块为代数统计研究提供了强大的计算工具，通过可扩展设计和先进算法支持数据共享和复杂计算

Abstract: We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.

</details>


### [17] [A forward-only scheme for online learning of proposal distributions in particle filters](https://arxiv.org/abs/2601.16089)
*Sylvain Procope-Mamert,Nicolas Chopin,Maud Delattre,Guillaume Kon Kam King*

Main category: stat.CO

TL;DR: 提出一种基于前向方案的粒子滤波提议分布构建方法，通过渐进融入未来观测来改进提议分布，相比需要完整数据集的后向方法具有更好的数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于后向方案的粒子滤波方法（如迭代辅助粒子滤波和受控序贯蒙特卡洛）需要访问整个数据集，通过后向递归利用所有未来观测，但存在数值不稳定的问题。

Method: 提出前向方案在线构建提议分布，渐进地融入未来观测来改进提议分布，逐步收敛到后向方法所针对的目标提议分布。

Result: 前向方法在简单设置下也表现出显著更强的鲁棒性，仅在边际似然估计器方差方面有轻微性能折衷。模拟和真实数据的数值实验验证了前向方法的增强稳定性。

Conclusion: 前向方案为粒子滤波提供了一种更稳健的提议分布构建方法，在保持接近后向方法性能的同时，显著改善了数值稳定性。

Abstract: We introduce a new online approach for constructing proposal distributions in particle filters using a forward scheme. Our method progressively incorporates future observations to refine proposals. This is in contrast to backward-scheme algorithms that require access to the entire dataset, such as the iterated auxiliary particle filters (Guarniero et al., 2017, arXiv:1511.06286) and controlled sequential Monte Carlo (Heng et al., 2020, arXiv:1708.08396) which leverage all future observations through backward recursion. In comparison, our forward scheme achieves a gradual improvement of proposals that converges toward the proposal targeted by these backward methods. We show that backward approaches can be numerically unstable even in simple settings. Our forward method, however, offers significantly greater robustness with only a minor trade-off in performance, measured by the variance of the marginal likelihood estimator. Numerical experiments on both simulated and real data illustrate the enhanced stability of our forward approach.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [18] [Statistical Reinforcement Learning in the Real World: A Survey of Challenges and Future Directions](https://arxiv.org/abs/2601.15353)
*Asim H. Gazi,Yongyi Guo,Daiqi Gao,Ziping Xu,Kelly W. Zhang,Susan A. Murphy*

Main category: stat.AP

TL;DR: 本文提出将强化学习实际应用视为包含在线学习、离线分析和重复部署三个组件的循环过程，并综述了统计强化学习在这些方面的最新进展。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在多个领域取得了显著成功，但在实际部署中仍存在巨大差距。主要挑战包括：1）实际环境中交互机会有限；2）目标环境经常发生重大变化，需要重新设计和部署。需要理论和方法来指导RL系统在实际环境中的设计、实施和持续改进。

Method: 将RL实际应用框架化为三组件过程：1）部署期间的在线学习和优化；2）部署后或部署间的离线分析；3）部署和重新部署的重复循环以持续改进RL系统。对统计RL在这些组件上的最新进展进行叙述性综述。

Result: 综述了包括最大化部署间推理数据效用、增强部署内在线学习样本效率、以及设计持续改进的部署序列等方法。这些统计RL方法有助于弥合RL研究与实际应用之间的差距。

Conclusion: 通过将RL应用视为包含在线学习、离线分析和重复部署的循环过程，并采用统计RL方法，可以更好地将RL研究转化为实际应用。未来研究方向应注重使用启发，旨在实现RL在实践中的有影响力应用。

Abstract: Reinforcement learning (RL) has achieved remarkable success in real-world decision-making across diverse domains, including gaming, robotics, online advertising, public health, and natural language processing. Despite these advances, a substantial gap remains between RL research and its deployment in many practical settings. Two recurring challenges often underlie this gap. First, many settings offer limited opportunity for the agent to interact extensively with the target environment due to practical constraints. Second, many target environments often undergo substantial changes, requiring redesign and redeployment of RL systems (e.g., advancements in science and technology that change the landscape of healthcare delivery). Addressing these challenges and bridging the gap between basic research and application requires theory and methodology that directly inform the design, implementation, and continual improvement of RL systems in real-world settings.
  In this paper, we frame the application of RL in practice as a three-component process: (i) online learning and optimization during deployment, (ii) post- or between-deployment offline analyses, and (iii) repeated cycles of deployment and redeployment to continually improve the RL system. We provide a narrative review of recent advances in statistical RL that address these components, including methods for maximizing data utility for between-deployment inference, enhancing sample efficiency for online learning within-deployment, and designing sequences of deployments for continual improvement. We also outline future research directions in statistical RL that are use-inspired -- aiming for impactful application of RL in practice.

</details>


### [19] [Geometric Morphometrics approach for classifying children's nutritional status on out of sample data](https://arxiv.org/abs/2601.15491)
*Laura Medialdea,Ana Arribas-Gil,Álvaro Pérez-Romero,Amador Gómez*

Main category: stat.AP

TL;DR: 该研究提出了一种处理几何形态测量学中样本外个体分类的方法，特别针对儿童营养评估中的身体形状图像分析，解决了传统对齐方法无法直接应用于新个体的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于对齐的几何形态测量分类方法通常无法处理未参与研究样本的新个体分类问题。在儿童营养评估的背景下，从参考样本获得的形状空间分类规则不能直接应用于样本外个体，因为需要样本依赖的处理步骤（如Procrustes对齐或异速生长回归）。

Method: 提出了获取新个体形状坐标的方法，并分析了使用不同模板配置作为样本外原始坐标配准目标的效果。研究强调理解样本特征和形状变量间的共线性对于优化分类结果的重要性。

Result: 开发了SAM Photo Diagnosis App© Program，这是一个离线智能手机工具，能够在不同营养筛查活动中更新训练样本，实现样本外个体的有效分类。

Conclusion: 该研究为几何形态测量学中的样本外个体分类提供了实用解决方案，特别适用于儿童营养评估的临床应用，通过智能手机工具实现了便捷的营养筛查。

Abstract: Current alignment-based methods for classification in geometric morphometrics do not generally address the classification of new individuals that were not part of the study sample. However, in the context of infant and child nutritional assessment from body shape images this is a relevant problem. In this setting, classification rules obtained on the shape space from a reference sample cannot be used on out-of-sample individuals in a straightforward way. Indeed, a series of sample dependent processing steps, such as alignment (Procrustes analysis, for instance) or allometric regression, need to be conducted before the classification rule can be applied. This work proposes ways of obtaining shape coordinates for a new individual and analyzes the effect of using different template configurations on the sample of study as target for registration of the out-of-sample raw coordinates. Understanding sample characteristics and collinearity among shape variables is crucial for optimal classification results when evaluating children's nutritional status using arm shape analysis from photos. The SAM Photo Diagnosis App\c{opyright} Program's goal is to develop an offline smartphone tool, enabling updates of the training sample across different nutritional screening campaigns.

</details>


### [20] [Assessing the informative value of macroeconomic indicators for public health forecasting](https://arxiv.org/abs/2601.15514)
*Shome Chakraborty,Fardil Khan,Soutik Ghosal*

Main category: stat.AP

TL;DR: 宏观经济指标可作为公共卫生系统容量的预测信号，对劳动力与基础设施指标预测效果较好，但需谨慎选择模型


<details>
  <summary>Details</summary>
Motivation: 宏观经济条件影响卫生系统运行环境，但其作为卫生系统容量领先信号的价值尚未得到系统评估。研究旨在探索宏观经济指标是否包含预测公共卫生目标的能力

Method: 使用美国月度时间序列数据，评估多种预测方法：包括不同优化策略的神经网络模型、广义加性模型、随机森林以及包含外生宏观经济指标的时间序列模型，采用替代模型拟合设计

Result: 宏观经济指标为某些公共卫生目标（特别是劳动力和基础设施指标）提供一致且可重复的预测信号，而其他目标则表现出较弱或不稳定的可预测性。强调稳定性和隐式正则化的模型在经济波动期间表现更可靠

Conclusion: 宏观经济指标可作为数字公共卫生监测的有用上游信号，但在将经济趋势转化为卫生系统预测工具时，需要谨慎的模型选择和验证

Abstract: Macroeconomic conditions influence the environments in which health systems operate, yet their value as leading signals of health system capacity has not been systematically evaluated. In this study, we examine whether selected macroeconomic indicators contain predictive information for several capacity-related public health targets, including employment in the health and social assistance workforce, new business applications in the sector, and health care construction spending. Using monthly U.S. time series data, we evaluate multiple forecasting approaches, including neural network models with different optimization strategies, generalized additive models, random forests, and time series models with exogenous macroeconomic indicators, under alternative model fitting designs. Across evaluation settings, we find that macroeconomic indicators provide a consistent and reproducible predictive signal for some public health targets, particularly workforce and infrastructure measures, while other targets exhibit weaker or less stable predictability. Models emphasizing stability and implicit regularization tend to perform more reliably during periods of economic volatility. These findings suggest that macroeconomic indicators may serve as useful upstream signals for digital public health monitoring, while underscoring the need for careful model selection and validation when translating economic trends into health system forecasting tools.

</details>


### [21] [Climate Vulnerability and Community Health: Identifying Greensboro Neighborhoods at Intersectional Risk](https://arxiv.org/abs/2601.15675)
*Rehinatu Usman,Onyedikachi J. Okeke*

Main category: stat.AP

TL;DR: 该研究为北卡罗来纳州格林斯伯勒市开发了一个综合性的交叉气候脆弱性评估，通过聚类分析识别出四种社区类型，发现气候风险集中在历史边缘化社区，揭示了环境正义差距。


<details>
  <summary>Details</summary>
Motivation: 超越一般化的地图绘制，研究旨在揭示气候相关风险在社会空间上的分布模式，特别是关注洪水暴露、健康负担和社会劣势在特定社区的交叉叠加，以识别最脆弱的社区。

Method: 结合人口统计、社会经济、健康和环境数据，在人口普查区层面进行分析。使用k-means和层次聚类方法识别不同的社区类型。

Result: 识别出四种不同的社区类型，其中包括一个极高风险集群，其特征是高洪水暴露、极端贫困、呼吸系统健康状况差和老化的住房。研究发现气候风险并非随机分布，而是系统性地聚集在历史边缘化社区。

Conclusion: 这种基于地点的类型学方法为政策制定者提供了一个有针对性的框架，可以设计综合干预措施，将洪水管理、公共卫生、住房和社会服务结合起来，以建立公平的城市韧性。

Abstract: This study develops an integrated, intersectional climate vulnerability assessment for Greensboro, North Carolina, a midsize city in the rapidly changing American Southeast. Moving beyond generalized mapping, we combine demographic, socioeconomic, health, and environmental data at the census tract level to identify neighborhoods where flood exposure, chronic health burdens, and social disadvantage spatially converge. Through k-means and hierarchical clustering, we identify four distinct neighborhood typologies, including a critically high-risk cluster characterized by high flood exposure, extreme poverty, poor respiratory health, and aging housing. The findings demonstrate that climate-related risks are not randomly distributed but systematically cluster in historically marginalized communities, revealing a clear environmental justice disparity. This place-based typology approach provides a targeted framework for policymakers to design integrated interventions that bridge flood management, public health, housing, and social services to build equitable urban resilience

</details>


### [22] [Detecting interpolation errors in infant mortality counts in 20th Century England and Wales](https://arxiv.org/abs/2601.15936)
*Tessa Wilkie,Idris Eckley,Paul Fearnhead,Ian Gregory*

Main category: stat.AP

TL;DR: 提出一种新的变点检测方法，用于识别区域插值处理中表现不佳的情况，并在英格兰和威尔士婴儿死亡率数据上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 分析历史数据集（如地方行政区婴儿死亡率数据）面临行政区划频繁变更的挑战，现有区域插值方法容易产生误差，需要有效检测这些误差的方法。

Method: 开发了一种新颖的变点检测方法，专门用于识别区域插值处理中表现不佳的实例，并在原始数据上进行验证。

Result: 该方法能够有效检测区域插值中的问题，并展示了纠正插值误差如何影响婴儿死亡率曲线的聚类分析结果。

Conclusion: 提出的变点检测方法为处理行政区划变更的历史数据分析提供了有效的误差检测工具，能够改善数据预处理质量并影响后续分析结果。

Abstract: Understanding historical datasets, such as the England and Wales infant mortality data, for local government districts can provide valuable insights into our changing society. Such analyses can prove challenging in practice, due to frequent changes in the boundaries of local government districts for which records are collected. One solution adopted in the literature to overcome such practical challenges is to pre-process data using areal interpolation to render the units consistent over the time period of focus. However, such methods are prone to errors. In this paper we introduce a novel changepoint method to detect instances where interpolation performs poorly. We demonstrate the utility of our method on original data, and also demonstrate how correcting interpolation errors can affect the clustering of the infant mortality curves.

</details>
