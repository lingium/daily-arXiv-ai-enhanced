{"id": "2511.15822", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15822", "abs": "https://arxiv.org/abs/2511.15822", "authors": ["Mu Niu", "Yue Zhang", "Ke Ye", "Pokman Cheung", "Yizhu Wang", "Xiaochen Yang"], "title": "Atlas Gaussian processes on restricted domains and point clouds", "comment": null, "summary": "In real-world applications, data often reside in restricted domains with unknown boundaries, or as high-dimensional point clouds lying on a lower-dimensional, nontrivial, unknown manifold. Traditional Gaussian Processes (GPs) struggle to capture the underlying geometry in such settings. Some existing methods assume a flat space embedded in a point cloud, which can be represented by a single latent chart (latent space), while others exhibit weak performance when the point cloud is sparse or irregularly sampled. The goal of this work is to address these challenges. The main contributions are twofold: (1) We establish the Atlas Brownian Motion (BM) framework for estimating the heat kernel on point clouds with unknown geometries and nontrivial topological structures; (2) Instead of directly using the heat kernel estimates, we construct a Riemannian corrected kernel by combining the global heat kernel with local RBF kernel and leading to the formulation of Riemannian-corrected Atlas Gaussian Processes (RC-AGPs). The resulting RC-AGPs are applied to regression tasks across synthetic and real-world datasets. These examples demonstrate that our method outperforms existing approaches in both heat kernel estimation and regression accuracy. It improves statistical inference by effectively bridging the gap between complex, high-dimensional observations and manifold-based inferences."}
{"id": "2511.15769", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15769", "abs": "https://arxiv.org/abs/2511.15769", "authors": ["Jared N. Lakhani"], "title": "Models with Accelerated Failure Conditionals", "comment": "26 pages, 3 figures, 7 tables", "summary": "Arnold and Arvanitis (2020) introduced a novel bivariate conditionally specified distribution, a distribution in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This novel conditioning regime was achieved through the use of survival functions, and the approach was termed the accelerated failure conditionals model. In their work, the conditioning framework was constructed using the exponential distribution. Although further generalization was proposed, challenges emerged in deriving the necessary and sufficient conditions for valid joint survival functions. The present study achieves such generalization, extending the conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, moving beyond distributional families whose marginal densities are non-increasing. The resulting models are fully specified through closed-form expressions for their moments, with simulations implemented using either a copula-based procedure or the Metropolis-Hastings algorithm. Empirical applications to two datasets, each featuring variables which are unimodal and skewed, demonstrate that the models with flexible, non-monotonic marginal densities yield a superior fit relative to those models with marginal densities restricted to monotonically decaying forms."}
{"id": "2511.16111", "categories": ["stat.ML", "cs.LG", "math.SP"], "pdf": "https://arxiv.org/pdf/2511.16111", "abs": "https://arxiv.org/abs/2511.16111", "authors": ["Feiyue Zhao", "Yangfan He", "Zhichao Zhang"], "title": "Angular Graph Fractional Fourier Transform: Theory and Application", "comment": null, "summary": "Graph spectral representations are fundamental in graph signal processing, offering a rigorous framework for analyzing and processing graph-structured data. The graph fractional Fourier transform (GFRFT) extends the classical graph Fourier transform (GFT) with a fractional-order parameter, enabling flexible spectral analysis while preserving mathematical consistency. The angular graph Fourier transform (AGFT) introduces angular control via GFT eigenvector rotation; however, existing constructions fail to degenerate to the GFT at zero angle, which is a critical flaw that undermines theoretical consistency and interpretability. To resolve these complementary limitations - GFRFT's lack of angular regulation and AGFT's defective degeneracy - this study proposes an angular GFRFT (AGFRFT), a unified framework that integrates fractional-order and angular spectral analyses with theoretical rigor. A degeneracy-friendly rotation matrix family ensures exact GFT degeneration at zero angle, with two AGFRFT variants (I-AGFRFT and II-AGFRFT) defined accordingly. Rigorous theoretical analyses confirm their unitarity, invertibility, and smooth parameter dependence. Both support learnable joint parameterization of the angle and fractional order, enabling adaptive spectral processing for diverse graph signals. Extensive experiments on real-world data denoising, image denoising, and point cloud denoising demonstrate that AGFRFT outperforms GFRFT and AGFT in terms of spectral concentration, reconstruction quality, and controllable spectral manipulation, establishing a robust and flexible tool for integrated angular fractional spectral analysis in graph signal processing."}
{"id": "2511.15813", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15813", "abs": "https://arxiv.org/abs/2511.15813", "authors": ["Aleix Alcacer", "Rafael Benitez", "Vicente J. Bolos", "Irene Epifanio"], "title": "Multidimensional scaling of two-mode three-way asymmetric dissimilarities: finding archetypal profiles and clustering", "comment": null, "summary": "Multidimensional scaling visualizes dissimilarities among objects and reduces data dimensionality. While many methods address symmetric proximity data, asymmetric and especially three-way proximity data (capturing relationships across multiple occasions) remain underexplored. Recent developments, such as the h-plot, enable the analysis of asymmetric and non-reflexive relationships by embedding dissimilarities in a Euclidean space, allowing further techniques like archetypoid analysis to identify representative extreme profiles. However, no existing methods extract archetypal profiles from three-way asymmetric proximity data. This work extends the h-plot methodology to three-way proximity data under both symmetric and asymmetric, conditional and unconditional frameworks. The proposed approach offers several advantages: intuitive interpretability through a unified Euclidean representation; an explicit, eigenvector-based analytical solution free from local minima; scale invariance under linear transformations; computational efficiency for large matrices; and a straightforward goodness-of-fit evaluation. Furthermore, it enables the identification of archetypal profiles and clustering structures for three-way asymmetric proximities. Its performance is compared with existing models for multidimensional scaling and clustering, and illustrated through a financial application. All data and code are provided to facilitate reproducibility."}
{"id": "2511.16288", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16288", "abs": "https://arxiv.org/abs/2511.16288", "authors": ["William Hao-Cheng Huang"], "title": "Spectral Identifiability for Interpretable Probe Geometry", "comment": null, "summary": "Linear probes are widely used to interpret and evaluate neural representations, yet their reliability remains unclear, as probes may appear accurate in some regimes but collapse unpredictably in others. We uncover a spectral mechanism behind this phenomenon and formalize it as the Spectral Identifiability Principle (SIP), a verifiable Fisher-inspired condition for probe stability. When the eigengap separating task-relevant directions is larger than the Fisher estimation error, the estimated subspace concentrates and accuracy remains consistent, whereas closing this gap induces instability in a phase-transition manner. Our analysis connects eigengap geometry, sample size, and misclassification risk through finite-sample reasoning, providing an interpretable diagnostic rather than a loose generalization bound. Controlled synthetic studies, where Fisher quantities are computed exactly, confirm these predictions and show how spectral inspection can anticipate unreliable probes before they distort downstream evaluation."}
{"id": "2511.15882", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15882", "abs": "https://arxiv.org/abs/2511.15882", "authors": ["Sida Chen", "Jessica K. Barrett", "Marco Palma", "Jianxin Pan", "Brian D. M. Tom"], "title": "Bayesian semiparametric modelling of biomarker variability in joint models", "comment": null, "summary": "There is growing interest in the role of within-individual variability (WIV) in biomarker trajectories for assessing disease risk and progression. A trajectory-based definition that has attracted recent attention characterises WIV as the curvature-based roughness of the latent biomarker trajectory (TB-WIV). To rigorously evaluate the association between TB-WIV and clinical outcomes and to perform dynamic risk prediction, joint models for longitudinal and time-to-event data (JM) are necessary. However, specifying the longitudinal trajectory is critical in this framework and poses methodological challenges. In this work, we investigate three Bayesian semiparametric approaches for longitudinal modelling and TB-WIV estimation within the JM framework to improve stability and accuracy over existing approaches. Two key methods are newly introduced: one based on Bayesian penalised splines (P-splines) and another on functional principal component analysis (FPCA). Using extensive simulation studies, we compare their performance under two important TB-WIV definitions against established approaches. Our results demonstrate overall inferential and predictive advantages of the proposed P-spline and FPCA-based approaches while also providing insights that guide method choice and interpretation of inference results. The proposed approaches are applied to data from the UK Cystic Fibrosis Registry, where, for the first time, we identify a significant positive association between lung function TB-WIV and mortality risk in patients with cystic fibrosis and demonstrate improved predictive performance for survival."}
{"id": "2511.16599", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16599", "abs": "https://arxiv.org/abs/2511.16599", "authors": ["Lukas Billera", "Hedwig Nora Nordlinder", "Ben Murrell"], "title": "Time dependent loss reweighting for flow matching and diffusion models is theoretically justified", "comment": "19 pages, 0 figures", "summary": "This brief note clarifies that, in Generator Matching (which subsumes a large family of flow matching and diffusion models over continuous, manifold, and discrete spaces), both the Bregman divergence loss and the linear parameterization of the generator can depend on both the current state $X_t$ and the time $t$, and we show that the expectation over time in the loss can be taken with respect to a broad class of time distributions. We also show this for Edit Flows, which falls outside of Generator Matching. That the loss can depend on $t$ clarifies that time-dependent loss weighting schemes, often used in practice to stabilize training, are theoretically justified when the specific flow or diffusion scheme is a special case of Generator Matching (or Edit Flows). It also often simplifies the construction of $X_1$-predictor schemes, which are sometimes preferred for model-related reasons. We show examples that rely upon the dependence of linear parameterizations, and of the Bregman divergence loss, on $t$ and $X_t$."}
{"id": "2511.15896", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15896", "abs": "https://arxiv.org/abs/2511.15896", "authors": ["Ying Jin", "José Zubizarreta"], "title": "Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies", "comment": null, "summary": "Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability."}
{"id": "2511.16447", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2511.16447", "abs": "https://arxiv.org/abs/2511.16447", "authors": ["Gian Mario Sangiovanni", "Gianluca Mastrantonio", "Daniele Ventura", "Alessio Pollice", "Giovanna Jona Lasinio"], "title": "Integrating Deep Learning and Spatial Statistics in Marine Ecosystem Monitoring", "comment": null, "summary": "In ecology, photogrammetry is a crucial method for efficiently collecting non-destructive samples of natural environments. When estimating the spatial distribution of animals, detecting objects in large-scale images becomes crucial. Object detection models enable large-scale analysis but introduce uncertainty because detection probability depends on various factors. To address detection bias, we model the distribution of a species of benthic animals (holothurians) in an area of the Italian Tyrrhenian coast near Giglio Island using a Thinned Log-Gaussian Cox Process (LGCP). We assume that a \"true\" intensity function accurately describes the distribution, while the observed process, resulting from independent thinning, is represented by a degraded intensity. The detection function controls the thinning mechanism, influenced by the object's location and other detection-related features. We use manual identification of holothurians as our benchmark. We compare automatic detection with this benchmark, an unthinned LGCP, and the thinned model to highlight the improvements gained from the proposed approach.Our method allows researchers to use photogrammetry, automatically identify objects of interest, and correct biases and approximations caused by the observation process."}
{"id": "2511.15942", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.15942", "abs": "https://arxiv.org/abs/2511.15942", "authors": ["Camilla Andreozzi", "Pietro Colombo", "Philipp Otto"], "title": "A Simple and Robust Multi-Fidelity Data Fusion Method for Effective Modeling of Citizen-Science Air Pollution Data", "comment": null, "summary": "We propose a robust multi-fidelity Gaussian process for integrating sparse, high-quality reference monitors with dense but noisy citizen-science sensors. The approach replaces the Gaussian log-likelihood in the high-fidelity channel with a global Huber loss applied to precision-weighted residuals, yielding bounded influence on all parameters, including the cross-fidelity coupling, while retaining the flexibility of co-kriging. We establish attenuation and unbounded influence of the Gaussian maximum likelihood estimator under low-fidelity contamination and derive explicit finite bounds for the proposed estimator that clarify how whitening and mean-shift sensitivity determine robustness. Monte Carlo experiments with controlled contamination show that the robust estimator maintains stable MAE and RMSE as anomaly magnitude and frequency increase, whereas the Gaussian MLE deteriorates rapidly. In an empirical study of PM2.5 concentrations in Hamburg, combining UBA monitors with openSenseMap data, the method consistently improves cross-validated predictive accuracy and yields coherent uncertainty maps without relying on auxiliary covariates. The framework remains computationally scalable through diagonal or low-rank whitening and is fully reproducible with publicly available code."}
{"id": "2511.15769", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15769", "abs": "https://arxiv.org/abs/2511.15769", "authors": ["Jared N. Lakhani"], "title": "Models with Accelerated Failure Conditionals", "comment": "26 pages, 3 figures, 7 tables", "summary": "Arnold and Arvanitis (2020) introduced a novel bivariate conditionally specified distribution, a distribution in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This novel conditioning regime was achieved through the use of survival functions, and the approach was termed the accelerated failure conditionals model. In their work, the conditioning framework was constructed using the exponential distribution. Although further generalization was proposed, challenges emerged in deriving the necessary and sufficient conditions for valid joint survival functions. The present study achieves such generalization, extending the conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, moving beyond distributional families whose marginal densities are non-increasing. The resulting models are fully specified through closed-form expressions for their moments, with simulations implemented using either a copula-based procedure or the Metropolis-Hastings algorithm. Empirical applications to two datasets, each featuring variables which are unimodal and skewed, demonstrate that the models with flexible, non-monotonic marginal densities yield a superior fit relative to those models with marginal densities restricted to monotonically decaying forms."}
{"id": "2511.15769", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15769", "abs": "https://arxiv.org/abs/2511.15769", "authors": ["Jared N. Lakhani"], "title": "Models with Accelerated Failure Conditionals", "comment": "26 pages, 3 figures, 7 tables", "summary": "Arnold and Arvanitis (2020) introduced a novel bivariate conditionally specified distribution, a distribution in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This novel conditioning regime was achieved through the use of survival functions, and the approach was termed the accelerated failure conditionals model. In their work, the conditioning framework was constructed using the exponential distribution. Although further generalization was proposed, challenges emerged in deriving the necessary and sufficient conditions for valid joint survival functions. The present study achieves such generalization, extending the conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, moving beyond distributional families whose marginal densities are non-increasing. The resulting models are fully specified through closed-form expressions for their moments, with simulations implemented using either a copula-based procedure or the Metropolis-Hastings algorithm. Empirical applications to two datasets, each featuring variables which are unimodal and skewed, demonstrate that the models with flexible, non-monotonic marginal densities yield a superior fit relative to those models with marginal densities restricted to monotonically decaying forms."}
{"id": "2511.16613", "categories": ["stat.ML", "cs.CC", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16613", "abs": "https://arxiv.org/abs/2511.16613", "authors": ["Jingqiu Ding", "Yiding Hua", "Kasper Lindberg", "David Steurer", "Aleksandr Storozhenko"], "title": "Rate-optimal community detection near the KS threshold via node-robust algorithms", "comment": null, "summary": "We study community detection in the \\emph{symmetric $k$-stochastic block model}, where $n$ nodes are evenly partitioned into $k$ clusters with intra- and inter-cluster connection probabilities $p$ and $q$, respectively.\n  Our main result is a polynomial-time algorithm that achieves the minimax-optimal misclassification rate\n  \\begin{equation*}\n  \\exp \\Bigl(-\\bigl(1 \\pm o(1)\\bigr) \\tfrac{C}{k}\\Bigr),\n  \\quad \\text{where } C = (\\sqrt{pn} - \\sqrt{qn})^2,\n  \\end{equation*}\n  whenever $C \\ge K\\,k^2\\,\\log k$ for some universal constant $K$, matching the Kesten--Stigum (KS) threshold up to a $\\log k$ factor.\n  Notably, this rate holds even when an adversary corrupts an $η\\le \\exp\\bigl(- (1 \\pm o(1)) \\tfrac{C}{k}\\bigr)$ fraction of the nodes.\n  To the best of our knowledge, the minimax rate was previously only attainable either via computationally inefficient procedures [ZZ15] or via polynomial-time algorithms that require strictly stronger assumptions such as $C \\ge K k^3$ [GMZZ17].\n  In the node-robust setting, the best known algorithm requires the substantially stronger condition $C \\ge K k^{102}$ [LM22].\n  Our results close this gap by providing the first polynomial-time algorithm that achieves the minimax rate near the KS threshold in both settings.\n  Our work has two key technical contributions:\n  (1) we robustify majority voting via the Sum-of-Squares framework,\n  (2) we develop a novel graph bisection algorithm via robust majority voting, which allows us to significantly improve the misclassification rate to $1/\\mathrm{poly}(k)$ for the initial estimation near the KS threshold."}
{"id": "2511.15917", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15917", "abs": "https://arxiv.org/abs/2511.15917", "authors": ["Austin E Schumacher", "Jon Wakefield"], "title": "Small Area Estimation Methods for Multivariate Health and Demographic Outcomes using Complex Survey Data", "comment": "23 pages, 9 figures, 3 tables", "summary": "Improving health in the most disadvantaged populations requires reliable estimates of health and demographic indicators to inform policy and interventions. Low- and middle-income countries with the largest burden of disease and disability tend to have the least comprehensive data, relying primarily on household surveys. Subnational estimates are increasingly used to inform targeted interventions and health policies. Producing reliable estimates from these data at fine geographical scales requires statistical modeling, and small area estimation models are commonly used in this context. Although most current methods model univariate outcomes, improved estimates may be attained by borrowing strength across related outcomes via multivariate modeling. In this paper, we develop classes of area- and unit-level multivariate shared component models using complex survey data. This framework jointly models multiple outcomes to improve accuracy of estimates compared to separately fitting univariate models. We conduct simulation studies to validate the methodology and use the proposed approach on survey data from Kenya in 2014; first, to jointly model height-for-age and weight-for-age in children, and second, to model three categories of contraceptive use in women. These models produce improved estimates compared to univariate and naive multivariate modeling approaches."}
{"id": "2511.15896", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15896", "abs": "https://arxiv.org/abs/2511.15896", "authors": ["Ying Jin", "José Zubizarreta"], "title": "Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies", "comment": null, "summary": "Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability."}
{"id": "2511.15813", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15813", "abs": "https://arxiv.org/abs/2511.15813", "authors": ["Aleix Alcacer", "Rafael Benitez", "Vicente J. Bolos", "Irene Epifanio"], "title": "Multidimensional scaling of two-mode three-way asymmetric dissimilarities: finding archetypal profiles and clustering", "comment": null, "summary": "Multidimensional scaling visualizes dissimilarities among objects and reduces data dimensionality. While many methods address symmetric proximity data, asymmetric and especially three-way proximity data (capturing relationships across multiple occasions) remain underexplored. Recent developments, such as the h-plot, enable the analysis of asymmetric and non-reflexive relationships by embedding dissimilarities in a Euclidean space, allowing further techniques like archetypoid analysis to identify representative extreme profiles. However, no existing methods extract archetypal profiles from three-way asymmetric proximity data. This work extends the h-plot methodology to three-way proximity data under both symmetric and asymmetric, conditional and unconditional frameworks. The proposed approach offers several advantages: intuitive interpretability through a unified Euclidean representation; an explicit, eigenvector-based analytical solution free from local minima; scale invariance under linear transformations; computational efficiency for large matrices; and a straightforward goodness-of-fit evaluation. Furthermore, it enables the identification of archetypal profiles and clustering structures for three-way asymmetric proximities. Its performance is compared with existing models for multidimensional scaling and clustering, and illustrated through a financial application. All data and code are provided to facilitate reproducibility."}
{"id": "2511.15813", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15813", "abs": "https://arxiv.org/abs/2511.15813", "authors": ["Aleix Alcacer", "Rafael Benitez", "Vicente J. Bolos", "Irene Epifanio"], "title": "Multidimensional scaling of two-mode three-way asymmetric dissimilarities: finding archetypal profiles and clustering", "comment": null, "summary": "Multidimensional scaling visualizes dissimilarities among objects and reduces data dimensionality. While many methods address symmetric proximity data, asymmetric and especially three-way proximity data (capturing relationships across multiple occasions) remain underexplored. Recent developments, such as the h-plot, enable the analysis of asymmetric and non-reflexive relationships by embedding dissimilarities in a Euclidean space, allowing further techniques like archetypoid analysis to identify representative extreme profiles. However, no existing methods extract archetypal profiles from three-way asymmetric proximity data. This work extends the h-plot methodology to three-way proximity data under both symmetric and asymmetric, conditional and unconditional frameworks. The proposed approach offers several advantages: intuitive interpretability through a unified Euclidean representation; an explicit, eigenvector-based analytical solution free from local minima; scale invariance under linear transformations; computational efficiency for large matrices; and a straightforward goodness-of-fit evaluation. Furthermore, it enables the identification of archetypal profiles and clustering structures for three-way asymmetric proximities. Its performance is compared with existing models for multidimensional scaling and clustering, and illustrated through a financial application. All data and code are provided to facilitate reproducibility."}
{"id": "2511.15929", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15929", "abs": "https://arxiv.org/abs/2511.15929", "authors": ["Jesus E. Vazquez", "Yanyuan Ma", "Karen Marder", "Tanya P. Garcia"], "title": "Robust Estimation under Outcome Dependent Right Censoring in Huntington Disease: Estimators for Low and High Censoring Rates", "comment": null, "summary": "Across health applications, researchers model outcomes as a function of time to an event, but the event time is right-censored for participants who exit the study or otherwise do not experience the event during follow-up. When censoring depends on the outcome-as in neurodegenerative disease studies where dropout is potentially related to disease severity-standard regression estimators produce biased estimates. We develop three consistent estimators for this outcome-dependent censoring setting: two augmented inverse probability weighted (AIPW) estimators and one maximum likelihood estimator (MLE). We establish their asymptotic properties and derive their robust sandwich variance estimators that account for nuisance parameter estimation. A key contribution is demonstrating that the choice of estimator to use depends on the censoring rate-the MLE performs best under low censoring rates, while the AIPW estimators yield lower bias and a higher nominal coverage under high censoring rates. We apply our estimators to Huntington disease data to characterize health decline leading up to mild cognitive impairment onset. The AIPW estimator with robustness matrix provided clinically-backed estimates with improved precision over inverse probability weighting, while MLE exhibited bias. Our results provide practical guidance for estimator selection based on censoring rate."}
{"id": "2511.15904", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15904", "abs": "https://arxiv.org/abs/2511.15904", "authors": ["Gözde Sert", "Abhishek Chakrabortty", "Anirban Bhattacharya"], "title": "Bayesian Semiparametric Causal Inference: Targeted Doubly Robust Estimation of Treatment Effects", "comment": "48 pages (including supplement)", "summary": "We propose a semiparametric Bayesian methodology for estimating the average treatment effect (ATE) within the potential outcomes framework using observational data with high-dimensional nuisance parameters. Our method introduces a Bayesian debiasing procedure that corrects for bias arising from nuisance estimation and employs a targeted modeling strategy based on summary statistics rather than the full data. These summary statistics are identified in a debiased manner, enabling the estimation of nuisance bias via weighted observables and facilitating hierarchical learning of the ATE. By combining debiasing with sample splitting, our approach separates nuisance estimation from inference on the target parameter, reducing sensitivity to nuisance model specification. We establish that, under mild conditions, the marginal posterior for the ATE satisfies a Bernstein-von Mises theorem when both nuisance models are correctly specified and remains consistent and robust when only one is correct, achieving Bayesian double robustness. This ensures asymptotic efficiency and frequentist validity. Extensive simulations confirm the theoretical results, demonstrating accurate point estimation and credible intervals with nominal coverage, even in high-dimensional settings. The proposed framework can also be extended to other causal estimands, and its key principles offer a general foundation for advancing Bayesian semiparametric inference more broadly."}
{"id": "2511.15866", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15866", "abs": "https://arxiv.org/abs/2511.15866", "authors": ["Chenyin Gao", "Han Chen", "Anru R. Zhang", "Shu Yang"], "title": "Causal Inference on Sequential Treatments via Tensor Completion", "comment": null, "summary": "Marginal Structural Models (MSMs) are popular for causal inference of sequential treatments in longitudinal observational studies, which however are sensitive to model misspecification. To achieve flexible modeling, we envision the potential outcomes to form a three-dimensional tensor indexed by subject, time, and treatment regime and propose a tensorized history-restricted MSM (HRMSM). The semi-parametric tensor factor model allows us to leverage the underlying low-rank structure of the potential outcomes tensor and exploit the pre-treatment covariate information to recover the counterfactual outcomes. We incorporate the inverse probability of treatment weighting in the loss function for tensor completion to adjust for time-varying confounding. Theoretically, a non-asymptotic upper bound on the Frobenius norm error for the proposed estimator is provided. Empirically, simulation studies show that the proposed tensor completion approach outperforms the parametric HRMSM and existing matrix/tensor completion methods. Finally, we illustrate the practical utility of the proposed approach to study the effect of ventilation on organ dysfunction from the Medical Information Mart for Intensive Care database."}
{"id": "2511.15896", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15896", "abs": "https://arxiv.org/abs/2511.15896", "authors": ["Ying Jin", "José Zubizarreta"], "title": "Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies", "comment": null, "summary": "Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability."}
{"id": "2511.15942", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.15942", "abs": "https://arxiv.org/abs/2511.15942", "authors": ["Camilla Andreozzi", "Pietro Colombo", "Philipp Otto"], "title": "A Simple and Robust Multi-Fidelity Data Fusion Method for Effective Modeling of Citizen-Science Air Pollution Data", "comment": null, "summary": "We propose a robust multi-fidelity Gaussian process for integrating sparse, high-quality reference monitors with dense but noisy citizen-science sensors. The approach replaces the Gaussian log-likelihood in the high-fidelity channel with a global Huber loss applied to precision-weighted residuals, yielding bounded influence on all parameters, including the cross-fidelity coupling, while retaining the flexibility of co-kriging. We establish attenuation and unbounded influence of the Gaussian maximum likelihood estimator under low-fidelity contamination and derive explicit finite bounds for the proposed estimator that clarify how whitening and mean-shift sensitivity determine robustness. Monte Carlo experiments with controlled contamination show that the robust estimator maintains stable MAE and RMSE as anomaly magnitude and frequency increase, whereas the Gaussian MLE deteriorates rapidly. In an empirical study of PM2.5 concentrations in Hamburg, combining UBA monitors with openSenseMap data, the method consistently improves cross-validated predictive accuracy and yields coherent uncertainty maps without relying on auxiliary covariates. The framework remains computationally scalable through diagonal or low-rank whitening and is fully reproducible with publicly available code."}
{"id": "2511.16029", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.16029", "abs": "https://arxiv.org/abs/2511.16029", "authors": ["Gregor Steiner", "Jeremie Houssineau", "Mark F. J. Steel"], "title": "Possibilistic Instrumental Variable Regression", "comment": null, "summary": "Instrumental variable regression is a common approach for causal inference in the presence of unobserved confounding. However, identifying valid instruments is often difficult in practice. In this paper, we propose a novel method based on possibility theory that performs posterior inference on the treatment effect, conditional on a user-specified set of potential violations of the exogeneity assumption. Our method can provide informative results even when only a single, potentially invalid, instrument is available, offering a natural and principled framework for sensitivity analysis. Simulation experiments and a real-data application indicate strong performance of the proposed approach."}
{"id": "2511.15882", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15882", "abs": "https://arxiv.org/abs/2511.15882", "authors": ["Sida Chen", "Jessica K. Barrett", "Marco Palma", "Jianxin Pan", "Brian D. M. Tom"], "title": "Bayesian semiparametric modelling of biomarker variability in joint models", "comment": null, "summary": "There is growing interest in the role of within-individual variability (WIV) in biomarker trajectories for assessing disease risk and progression. A trajectory-based definition that has attracted recent attention characterises WIV as the curvature-based roughness of the latent biomarker trajectory (TB-WIV). To rigorously evaluate the association between TB-WIV and clinical outcomes and to perform dynamic risk prediction, joint models for longitudinal and time-to-event data (JM) are necessary. However, specifying the longitudinal trajectory is critical in this framework and poses methodological challenges. In this work, we investigate three Bayesian semiparametric approaches for longitudinal modelling and TB-WIV estimation within the JM framework to improve stability and accuracy over existing approaches. Two key methods are newly introduced: one based on Bayesian penalised splines (P-splines) and another on functional principal component analysis (FPCA). Using extensive simulation studies, we compare their performance under two important TB-WIV definitions against established approaches. Our results demonstrate overall inferential and predictive advantages of the proposed P-spline and FPCA-based approaches while also providing insights that guide method choice and interpretation of inference results. The proposed approaches are applied to data from the UK Cystic Fibrosis Registry, where, for the first time, we identify a significant positive association between lung function TB-WIV and mortality risk in patients with cystic fibrosis and demonstrate improved predictive performance for survival."}
{"id": "2511.15904", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15904", "abs": "https://arxiv.org/abs/2511.15904", "authors": ["Gözde Sert", "Abhishek Chakrabortty", "Anirban Bhattacharya"], "title": "Bayesian Semiparametric Causal Inference: Targeted Doubly Robust Estimation of Treatment Effects", "comment": "48 pages (including supplement)", "summary": "We propose a semiparametric Bayesian methodology for estimating the average treatment effect (ATE) within the potential outcomes framework using observational data with high-dimensional nuisance parameters. Our method introduces a Bayesian debiasing procedure that corrects for bias arising from nuisance estimation and employs a targeted modeling strategy based on summary statistics rather than the full data. These summary statistics are identified in a debiased manner, enabling the estimation of nuisance bias via weighted observables and facilitating hierarchical learning of the ATE. By combining debiasing with sample splitting, our approach separates nuisance estimation from inference on the target parameter, reducing sensitivity to nuisance model specification. We establish that, under mild conditions, the marginal posterior for the ATE satisfies a Bernstein-von Mises theorem when both nuisance models are correctly specified and remains consistent and robust when only one is correct, achieving Bayesian double robustness. This ensures asymptotic efficiency and frequentist validity. Extensive simulations confirm the theoretical results, demonstrating accurate point estimation and credible intervals with nominal coverage, even in high-dimensional settings. The proposed framework can also be extended to other causal estimands, and its key principles offer a general foundation for advancing Bayesian semiparametric inference more broadly."}
{"id": "2511.16530", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.16530", "abs": "https://arxiv.org/abs/2511.16530", "authors": ["Nicholas C. Henderson", "Nicholas Hartman"], "title": "Targeted Parameter Estimation for Robust Empirical Bayes Ranking", "comment": null, "summary": "Ordering the expected outcomes across a collection of clusters after performing a covariate adjustment commonly arises in many applied settings, such as healthcare provider evaluation. Regression parameters in such covariate adjustment models are frequently estimated by maximum likelihood or through other criteria that do not directly evaluate the quality of the rankings resulting from using a particular set of parameter estimates. In this article, we propose both a novel empirical Bayes ranking procedure and an associated estimation approach for finding the regression parameters of the covariate adjustment model. By building our ranking approach around estimating approximate percentiles of the covariate-adjusted cluster-level means, we are able to develop manageable expressions for the expected ranking squared-error loss associated with any choice of the covariate-adjustment model parameters, and we harness this to generate a novel unbiased estimator for this expected loss. Minimization of this unbiased estimator directly leads to a novel ranking procedure that is often more robust than conventional empirical Bayes ranking methods. Through a series of simulation studies, we show that our approach consistently delivers improved ranking squared-error performance relative to competing methods, such as posterior expected ranks and ranking the components of the best linear unbiased predictor. Estimating rankings using our method is illustrated with an example from a longitudinal study evaluating test scores across a large group of schools."}
{"id": "2511.16102", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.16102", "abs": "https://arxiv.org/abs/2511.16102", "authors": ["Bankitdor M Nongrum", "Adarsha Kumar Jena"], "title": "Estimation of the Coefficient of Variation of Weibull Distribution under Type-I Progressively Interval Censoring: A Simulation-based Approach", "comment": null, "summary": "Measures of relative variability, such as the Pearson's coefficient of variation (CV$_p$), give much insight into the spread of lifetime distributions, like the Weibull distribution. The estimation of the Weibull CV$_p$ in modern statistics has traditionally been prioritized only when complete data is available. In this article, we estimate the Weibull CV$_p$ and its second-order alternative, denoted as CV$_k$, under type-I progressively interval censoring, which is a typical scenario in survival analysis and reliability theory. Point estimates are obtained using the methods of maximum likelihood, least squares, and the Bayesian approach with MCMC simulation. A nonlinear least squares method is proposed for estimating the CV$_p$ and CV$_k$. We also perform interval estimation of the CV$_p$ and CV$_k$ using the asymptotic confidence intervals, bootstrap intervals through the least squares estimates, and the highest posterior density intervals. A comprehensive Monte Carlo simulation study is carried out to understand and compare the performance of the estimators. The proposed least squares and the Bayesian methods produce better point estimates for the CV$_p$. The highest posterior density intervals outperform other interval estimates in many cases. The methodologies are also applied to a real dataset to demonstrate the performance of the estimators."}
{"id": "2511.15896", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15896", "abs": "https://arxiv.org/abs/2511.15896", "authors": ["Ying Jin", "José Zubizarreta"], "title": "Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies", "comment": null, "summary": "Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability."}
{"id": "2511.15904", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15904", "abs": "https://arxiv.org/abs/2511.15904", "authors": ["Gözde Sert", "Abhishek Chakrabortty", "Anirban Bhattacharya"], "title": "Bayesian Semiparametric Causal Inference: Targeted Doubly Robust Estimation of Treatment Effects", "comment": "48 pages (including supplement)", "summary": "We propose a semiparametric Bayesian methodology for estimating the average treatment effect (ATE) within the potential outcomes framework using observational data with high-dimensional nuisance parameters. Our method introduces a Bayesian debiasing procedure that corrects for bias arising from nuisance estimation and employs a targeted modeling strategy based on summary statistics rather than the full data. These summary statistics are identified in a debiased manner, enabling the estimation of nuisance bias via weighted observables and facilitating hierarchical learning of the ATE. By combining debiasing with sample splitting, our approach separates nuisance estimation from inference on the target parameter, reducing sensitivity to nuisance model specification. We establish that, under mild conditions, the marginal posterior for the ATE satisfies a Bernstein-von Mises theorem when both nuisance models are correctly specified and remains consistent and robust when only one is correct, achieving Bayesian double robustness. This ensures asymptotic efficiency and frequentist validity. Extensive simulations confirm the theoretical results, demonstrating accurate point estimation and credible intervals with nominal coverage, even in high-dimensional settings. The proposed framework can also be extended to other causal estimands, and its key principles offer a general foundation for advancing Bayesian semiparametric inference more broadly."}
{"id": "2511.15917", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15917", "abs": "https://arxiv.org/abs/2511.15917", "authors": ["Austin E Schumacher", "Jon Wakefield"], "title": "Small Area Estimation Methods for Multivariate Health and Demographic Outcomes using Complex Survey Data", "comment": "23 pages, 9 figures, 3 tables", "summary": "Improving health in the most disadvantaged populations requires reliable estimates of health and demographic indicators to inform policy and interventions. Low- and middle-income countries with the largest burden of disease and disability tend to have the least comprehensive data, relying primarily on household surveys. Subnational estimates are increasingly used to inform targeted interventions and health policies. Producing reliable estimates from these data at fine geographical scales requires statistical modeling, and small area estimation models are commonly used in this context. Although most current methods model univariate outcomes, improved estimates may be attained by borrowing strength across related outcomes via multivariate modeling. In this paper, we develop classes of area- and unit-level multivariate shared component models using complex survey data. This framework jointly models multiple outcomes to improve accuracy of estimates compared to separately fitting univariate models. We conduct simulation studies to validate the methodology and use the proposed approach on survey data from Kenya in 2014; first, to jointly model height-for-age and weight-for-age in children, and second, to model three categories of contraceptive use in women. These models produce improved estimates compared to univariate and naive multivariate modeling approaches."}
{"id": "2511.15918", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15918", "abs": "https://arxiv.org/abs/2511.15918", "authors": ["Indrila Ganguly", "Ying Huang"], "title": "Sequential Testing for Assessing the Incremental Value of Biomarkers Under Biorepository Specimen Constraints with Robustness to Model Misspecification", "comment": "33 pages including Supplementary Material, 3 figures", "summary": "In cancer biomarker development, a key objective is to evaluate whether a new biomarker, when combined with an established one, improves early cancer detection compared to using the established biomarker alone. Incremental value is often quantified by changes at specific points on the ROC curve, such as an increase in sensitivity at a fixed specificity, which is especially relevant in early cancer detection. Our research is motivated by the Early Detection Research Network (EDRN) biorepository studies, which aim to validate multiple cancer biomarkers across laboratories using specimens obtained from a centralized biorepository, under the constraint of limited specimen availability. To address this challenge, we propose a two-stage group sequential hypothesis testing framework for assessing incremental effects, allowing early stopping for futility or efficacy to conserve valuable samples. Our asymptotic results are derived under a logistic working model and remain valid even under model misspecification, ensuring robustness and broad applicability. We further integrate a rotating group membership design to facilitate validation of multiple candidate biomarkers across laboratories. Through extensive simulations, we demonstrate valid type I error control and efficient utilization of specimens. Finally, we apply our method to data from a multicenter EDRN pancreatic cancer reference set study and show how the proposed approach identifies promising candidate biomarkers that provide incremental performance when combined with CA19-9, while enabling efficient evaluation of a large number of such candidates."}
{"id": "2511.15929", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15929", "abs": "https://arxiv.org/abs/2511.15929", "authors": ["Jesus E. Vazquez", "Yanyuan Ma", "Karen Marder", "Tanya P. Garcia"], "title": "Robust Estimation under Outcome Dependent Right Censoring in Huntington Disease: Estimators for Low and High Censoring Rates", "comment": null, "summary": "Across health applications, researchers model outcomes as a function of time to an event, but the event time is right-censored for participants who exit the study or otherwise do not experience the event during follow-up. When censoring depends on the outcome-as in neurodegenerative disease studies where dropout is potentially related to disease severity-standard regression estimators produce biased estimates. We develop three consistent estimators for this outcome-dependent censoring setting: two augmented inverse probability weighted (AIPW) estimators and one maximum likelihood estimator (MLE). We establish their asymptotic properties and derive their robust sandwich variance estimators that account for nuisance parameter estimation. A key contribution is demonstrating that the choice of estimator to use depends on the censoring rate-the MLE performs best under low censoring rates, while the AIPW estimators yield lower bias and a higher nominal coverage under high censoring rates. We apply our estimators to Huntington disease data to characterize health decline leading up to mild cognitive impairment onset. The AIPW estimator with robustness matrix provided clinically-backed estimates with improved precision over inverse probability weighting, while MLE exhibited bias. Our results provide practical guidance for estimator selection based on censoring rate."}
{"id": "2511.15942", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.15942", "abs": "https://arxiv.org/abs/2511.15942", "authors": ["Camilla Andreozzi", "Pietro Colombo", "Philipp Otto"], "title": "A Simple and Robust Multi-Fidelity Data Fusion Method for Effective Modeling of Citizen-Science Air Pollution Data", "comment": null, "summary": "We propose a robust multi-fidelity Gaussian process for integrating sparse, high-quality reference monitors with dense but noisy citizen-science sensors. The approach replaces the Gaussian log-likelihood in the high-fidelity channel with a global Huber loss applied to precision-weighted residuals, yielding bounded influence on all parameters, including the cross-fidelity coupling, while retaining the flexibility of co-kriging. We establish attenuation and unbounded influence of the Gaussian maximum likelihood estimator under low-fidelity contamination and derive explicit finite bounds for the proposed estimator that clarify how whitening and mean-shift sensitivity determine robustness. Monte Carlo experiments with controlled contamination show that the robust estimator maintains stable MAE and RMSE as anomaly magnitude and frequency increase, whereas the Gaussian MLE deteriorates rapidly. In an empirical study of PM2.5 concentrations in Hamburg, combining UBA monitors with openSenseMap data, the method consistently improves cross-validated predictive accuracy and yields coherent uncertainty maps without relying on auxiliary covariates. The framework remains computationally scalable through diagonal or low-rank whitening and is fully reproducible with publicly available code."}
{"id": "2511.15961", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15961", "abs": "https://arxiv.org/abs/2511.15961", "authors": ["Kedar Karhadkar", "Jack Klys", "Daniel Ting", "Artem Vorozhtsov", "Houssam Nassif"], "title": "Evaluating Variance Estimates with Relative Efficiency", "comment": "Presented at CODE@MIT 2025", "summary": "Experimentation platforms in industry must often deal with customer trust issues. Platforms must prove the validity of their claims as well as catch issues that arise. As a central quantity estimated by experimentation platforms, the validity of confidence intervals is of particular concern. To ensure confidence intervals are reliable, we must understand and diagnose when our variance estimates are biased or noisy, or when the confidence intervals may be incorrect.\n  A common method for this is A/A testing, in which both the control and test arms receive the same treatment. One can then test if the empirical false positive rate (FPR) deviates substantially from the target FPR over many tests. However, this approach turns each A/A test into a simple binary random variable. It is an inefficient estimate of the FPR as it throws away information about the magnitude of each experiment result. We show how to empirically evaluate the effectiveness of statistics that monitor the variance estimates that partly dictate a platform's statistical reliability. We also show that statistics other than empirical FPR are more effective at detecting issues. In particular, we propose a $t^2$-statistic that is more sample efficient."}
{"id": "2511.16029", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.16029", "abs": "https://arxiv.org/abs/2511.16029", "authors": ["Gregor Steiner", "Jeremie Houssineau", "Mark F. J. Steel"], "title": "Possibilistic Instrumental Variable Regression", "comment": null, "summary": "Instrumental variable regression is a common approach for causal inference in the presence of unobserved confounding. However, identifying valid instruments is often difficult in practice. In this paper, we propose a novel method based on possibility theory that performs posterior inference on the treatment effect, conditional on a user-specified set of potential violations of the exogeneity assumption. Our method can provide informative results even when only a single, potentially invalid, instrument is available, offering a natural and principled framework for sensitivity analysis. Simulation experiments and a real-data application indicate strong performance of the proposed approach."}
{"id": "2511.16040", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.16040", "abs": "https://arxiv.org/abs/2511.16040", "authors": ["Garritt L. Page", "Andrés F. Barrientos", "David B. Dahl", "David B. Dunson"], "title": "Uncertainty Quantification in Bayesian Clustering", "comment": null, "summary": "Bayesian clustering methods have the widely touted advantage of providing a probabilistic characterization of uncertainty in clustering through the posterior distribution. An amazing variety of priors and likelihoods have been proposed for clustering in a broad array of settings. There is also a rich literature on Markov chain Monte Carlo (MCMC) algorithms for sampling from posterior clustering distributions. However, there is relatively little work on summarizing the posterior uncertainty. The complexity of the partition space corresponding to different clusterings makes this problem challenging. We propose a post-processing procedure for any Bayesian clustering model with posterior samples that generates a credible set that is easy to use, fast to compute, and intuitive to interpret. We also provide new measures of clustering uncertainty and show how to compute cluster-specific parameter estimates and credible regions that accumulate a desired posterior probability without having to condition on a partition estimate or employ label-switching techniques. We illustrate our procedure through several applications."}
{"id": "2511.16102", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.16102", "abs": "https://arxiv.org/abs/2511.16102", "authors": ["Bankitdor M Nongrum", "Adarsha Kumar Jena"], "title": "Estimation of the Coefficient of Variation of Weibull Distribution under Type-I Progressively Interval Censoring: A Simulation-based Approach", "comment": null, "summary": "Measures of relative variability, such as the Pearson's coefficient of variation (CV$_p$), give much insight into the spread of lifetime distributions, like the Weibull distribution. The estimation of the Weibull CV$_p$ in modern statistics has traditionally been prioritized only when complete data is available. In this article, we estimate the Weibull CV$_p$ and its second-order alternative, denoted as CV$_k$, under type-I progressively interval censoring, which is a typical scenario in survival analysis and reliability theory. Point estimates are obtained using the methods of maximum likelihood, least squares, and the Bayesian approach with MCMC simulation. A nonlinear least squares method is proposed for estimating the CV$_p$ and CV$_k$. We also perform interval estimation of the CV$_p$ and CV$_k$ using the asymptotic confidence intervals, bootstrap intervals through the least squares estimates, and the highest posterior density intervals. A comprehensive Monte Carlo simulation study is carried out to understand and compare the performance of the estimators. The proposed least squares and the Bayesian methods produce better point estimates for the CV$_p$. The highest posterior density intervals outperform other interval estimates in many cases. The methodologies are also applied to a real dataset to demonstrate the performance of the estimators."}
{"id": "2511.16530", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.16530", "abs": "https://arxiv.org/abs/2511.16530", "authors": ["Nicholas C. Henderson", "Nicholas Hartman"], "title": "Targeted Parameter Estimation for Robust Empirical Bayes Ranking", "comment": null, "summary": "Ordering the expected outcomes across a collection of clusters after performing a covariate adjustment commonly arises in many applied settings, such as healthcare provider evaluation. Regression parameters in such covariate adjustment models are frequently estimated by maximum likelihood or through other criteria that do not directly evaluate the quality of the rankings resulting from using a particular set of parameter estimates. In this article, we propose both a novel empirical Bayes ranking procedure and an associated estimation approach for finding the regression parameters of the covariate adjustment model. By building our ranking approach around estimating approximate percentiles of the covariate-adjusted cluster-level means, we are able to develop manageable expressions for the expected ranking squared-error loss associated with any choice of the covariate-adjustment model parameters, and we harness this to generate a novel unbiased estimator for this expected loss. Minimization of this unbiased estimator directly leads to a novel ranking procedure that is often more robust than conventional empirical Bayes ranking methods. Through a series of simulation studies, we show that our approach consistently delivers improved ranking squared-error performance relative to competing methods, such as posterior expected ranks and ranking the components of the best linear unbiased predictor. Estimating rankings using our method is illustrated with an example from a longitudinal study evaluating test scores across a large group of schools."}
{"id": "2511.16589", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.16589", "abs": "https://arxiv.org/abs/2511.16589", "authors": ["Divan A. Burger", "Sean van der Merwe", "Emmanuel Lesaffre"], "title": "A flexible quantile mixed-effects model for censored outcomes", "comment": null, "summary": "We introduce a Bayesian quantile mixed-effects model for censored longitudinal outcomes based on the skew exponential power (SEP) error distribution. The SEP family separates tail behavior and skewness from the targeted quantile and includes the skew Laplace (SL) distribution as a special case. We derive analytic likelihood contributions for left, right, and interval censoring under the SEP model, so censored observations are handled within a single parametric framework without numerical integration in the likelihood. In simulation studies with varying censoring patterns and skewness profiles, the SEP-based quantile mixed-effects model maintains near-nominal bias and credible interval coverage for regression coefficients. In contrast, the SL-based model can exhibit bias and undercoverage when the data's skewness conflicts with the skewness implied by the target quantile. In an HIV-1 RNA viral load case study with left censoring at the assay limit, bridge-sampled marginal likelihoods and simulation-based residual diagnostics favor the SEP specification across quantiles and yield more stable estimates of treatment-specific viral load trajectories than the SL benchmark."}
