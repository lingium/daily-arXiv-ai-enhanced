{"id": "2512.08948", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.08948", "abs": "https://arxiv.org/abs/2512.08948", "authors": ["Yihang Gao", "Michael K. Ng", "Michael W. Mahoney", "Sen Na"], "title": "Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming", "comment": "80 pages, 5 figures, 5 tables", "summary": "We study online statistical inference for the solutions of stochastic optimization problems with equality and inequality constraints. Such problems are prevalent in statistics and machine learning, encompassing constrained $M$-estimation, physics-informed models, safe reinforcement learning, and algorithmic fairness. We develop a stochastic sequential quadratic programming (SSQP) method to solve these problems, where the step direction is computed by sequentially performing a quadratic approximation of the objective and a linear approximation of the constraints. Despite having access to unbiased estimates of population gradients, a key challenge in constrained stochastic problems lies in dealing with the bias in the step direction. As such, we apply a momentum-style gradient moving-average technique within SSQP to debias the step. We show that our method achieves global almost-sure convergence and exhibits local asymptotic normality with an optimal primal-dual limiting covariance matrix in the sense of Hájek and Le Cam. In addition, we provide a plug-in covariance matrix estimator for practical inference. To our knowledge, the proposed SSQP method is the first fully online method that attains primal-dual asymptotic minimax optimality without relying on projection operators onto the constraint set, which are generally intractable for nonlinear problems. Through extensive experiments on benchmark nonlinear problems, as well as on constrained generalized linear models and portfolio allocation problems using both synthetic and real data, we demonstrate superior performance of our method, showing that the method and its asymptotic behavior not only solve constrained stochastic problems efficiently but also provide valid and practical online inference in real-world applications."}
{"id": "2512.09163", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09163", "abs": "https://arxiv.org/abs/2512.09163", "authors": ["Gabrielle Rives", "Olivier Lopez", "Nicolas Bousquet"], "title": "WTNN: Weibull-Tailored Neural Networks for survival analysis", "comment": null, "summary": "The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches."}
{"id": "2512.09266", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09266", "abs": "https://arxiv.org/abs/2512.09266", "authors": ["Ryosuke Nagumo", "Hironori Fujisawa"], "title": "Robust and Sparse Estimation of Unbounded Density Ratio under Heavy Contamination", "comment": null, "summary": "We examine the non-asymptotic properties of robust density ratio estimation (DRE) in contaminated settings. Weighted DRE is the most promising among existing methods, exhibiting doubly strong robustness from an asymptotic perspective. This study demonstrates that Weighted DRE achieves sparse consistency even under heavy contamination within a non-asymptotic framework. This method addresses two significant challenges in density ratio estimation and robust estimation. For density ratio estimation, we provide the non-asymptotic properties of estimating unbounded density ratios under the assumption that the weighted density ratio function is bounded. For robust estimation, we introduce a non-asymptotic framework for doubly strong robustness under heavy contamination, assuming that at least one of the following conditions holds: (i) contamination ratios are small, and (ii) outliers have small weighted values. This work provides the first non-asymptotic analysis of strong robustness under heavy contamination."}
{"id": "2512.09275", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09275", "abs": "https://arxiv.org/abs/2512.09275", "authors": ["Weiyi He", "Yue Xing"], "title": "Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression", "comment": "25 pages, 3 figures", "summary": "Positional encoding (PE) is a core architectural component of Transformers, yet its impact on the Transformer's generalization and robustness remains unclear. In this work, we provide the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for a completely trainable PE module. Our result shows that PE systematically enlarges the generalization gap. Extending to the adversarial setting, we derive the adversarial Rademacher generalization bound. We find that the gap between models with and without PE is magnified under attack, demonstrating that PE amplifies the vulnerability of models. Our bounds are empirically validated by a simulation study. Together, this work establishes a new framework for understanding the clean and adversarial generalization in ICL with PE."}
{"id": "2512.09060", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.09060", "abs": "https://arxiv.org/abs/2512.09060", "authors": ["Kellin N. Rumsey", "Graham C. Gibson", "Devin Francom", "Reid Morris"], "title": "All Emulators are Wrong, Many are Useful, and Some are More Useful Than Others: A Reproducible Comparison of Computer Model Surrogates", "comment": null, "summary": "Accurate and efficient surrogate modeling is essential for modern computational science, and there are a staggering number of emulation methods to choose from. With new methods being developed all the time, comparing the relative strengths and weaknesses of different methods remains a challenge due to inconsistent benchmarking practices and (sometimes) limited reproducibility and transparency. In this work, we present a large-scale, fully reproducible comparison of $29$ distinct emulators across $60$ canonical test functions and $40$ real emulation datasets. To facilitate rigorous, apples-to-apples comparisons, we introduce the R package \\texttt{duqling}, which streamlines reproducible simulation studies using a consistent, simple syntax, and automatic internal scaling of inputs. This framework allows researchers to compare emulators in a unified environment and makes it possible to replicate or extend previous studies with minimal effort, even across different publications. Our results provide detailed empirical insight into the strengths and weaknesses of state-of-the-art emulators and offer guidance for both method developers and practitioners selecting a surrogate for new data. We discuss best practices for emulator comparison and highlight how \\texttt{duqling} can accelerate research in emulator design and application."}
{"id": "2512.08948", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.08948", "abs": "https://arxiv.org/abs/2512.08948", "authors": ["Yihang Gao", "Michael K. Ng", "Michael W. Mahoney", "Sen Na"], "title": "Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming", "comment": "80 pages, 5 figures, 5 tables", "summary": "We study online statistical inference for the solutions of stochastic optimization problems with equality and inequality constraints. Such problems are prevalent in statistics and machine learning, encompassing constrained $M$-estimation, physics-informed models, safe reinforcement learning, and algorithmic fairness. We develop a stochastic sequential quadratic programming (SSQP) method to solve these problems, where the step direction is computed by sequentially performing a quadratic approximation of the objective and a linear approximation of the constraints. Despite having access to unbiased estimates of population gradients, a key challenge in constrained stochastic problems lies in dealing with the bias in the step direction. As such, we apply a momentum-style gradient moving-average technique within SSQP to debias the step. We show that our method achieves global almost-sure convergence and exhibits local asymptotic normality with an optimal primal-dual limiting covariance matrix in the sense of Hájek and Le Cam. In addition, we provide a plug-in covariance matrix estimator for practical inference. To our knowledge, the proposed SSQP method is the first fully online method that attains primal-dual asymptotic minimax optimality without relying on projection operators onto the constraint set, which are generally intractable for nonlinear problems. Through extensive experiments on benchmark nonlinear problems, as well as on constrained generalized linear models and portfolio allocation problems using both synthetic and real data, we demonstrate superior performance of our method, showing that the method and its asymptotic behavior not only solve constrained stochastic problems efficiently but also provide valid and practical online inference in real-world applications."}
{"id": "2512.09179", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09179", "abs": "https://arxiv.org/abs/2512.09179", "authors": ["Robert A. Rigby", "Mikis D. Stasinopoulos", "Achim Zeileis", "Sanja Stanojevic", "Gillian Heller", "Fernanda de Bastiani", "Thomas Kneib", "Andreas Mayr", "Reto Stauffer", "Nikolaus Umlauf"], "title": "Refuting \"Debunking the GAMLSS Myth: Simplicity Reigns in Pulmonary Function Diagnostics\"", "comment": "Letter to the editor of Respiratory Medicine", "summary": "We read with interest the above article by Zavorsky (2025, Respiratory Medicine, doi:10.1016/j.rmed.2024.107836) concerning reference equations for pulmonary function testing. The author compares a Generalized Additive Model for Location, Scale, and Shape (GAMLSS), which is the standard adopted by the Global Lung Function Initiative (GLI), with a segmented linear regression (SLR) model, for pulmonary function variables. The author presents an interesting comparison; however there are some fundamental issues with the approach. We welcome this opportunity for discussion of the issues that it raises. The author's contention is that (1) SLR provides \"prediction accuracies on par with GAMLSS\"; and (2) the GAMLSS model equations are \"complicated and require supplementary spline tables\", whereas the SLR is \"more straightforward, parsimonious, and accessible to a broader audience\". We respectfully disagree with both of these points."}
{"id": "2512.09130", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09130", "abs": "https://arxiv.org/abs/2512.09130", "authors": ["Tetiana Gorbach", "Xavier de Luna", "Juha Karvanen", "Ingeborg Waernbaum"], "title": "Complementary strengths of the Neyman-Rubin and graphical causal frameworks", "comment": "Under consideration at The American Statistician; not yet accepted", "summary": "This article contributes to the discussion on the relationship between the Neyman-Rubin and the graphical frameworks for causal inference. We present specific examples of data-generating mechanisms - such as those involving undirected or deterministic relationships and cycles - where analyses using a directed acyclic graph are challenging, but where the tools from the Neyman-Rubin causal framework are readily applicable. We also provide examples of data-generating mechanisms with M-bias, trapdoor variables, and complex front-door structures, where the application of the Neyman-Rubin approach is complicated, but the graphical approach is directly usable. The examples offer insights into commonly used causal inference frameworks and aim to improve comprehension of the languages for causal reasoning among a broad audience."}
{"id": "2512.09790", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2512.09790", "abs": "https://arxiv.org/abs/2512.09790", "authors": ["Hedibert F. Lopes", "Filippo Ascolani"], "title": "A Conversation with Mike West", "comment": null, "summary": "Mike West is currently the Arts & Sciences Distinguished Professor Emeritus of Statistics and Decision Sciences at Duke University. Mike's research in Bayesian analysis spans multiple interlinked areas: theory and methods of dynamic models in time series analysis, foundations of inference and decision analysis, multivariate and latent structure analysis, stochastic computation and optimisation, among others. Inter-disciplinary R&D has ranged across applications in commercial forecasting, dynamic networks, finance, econometrics, signal processing, climatology, systems biology, genomics and neuroscience, among other areas. Among Mike's currently active research areas are forecasting, causal prediction and decision analysis in business, economic policy and finance, as well as in personal decision making. Mike led the development of academic statistics at Duke University from 1990-2002, and has been broadly engaged in professional leadership elsewhere. He is past president of the International Society for Bayesian Analysis (ISBA), and has served in founding roles and as board member for several professional societies, national and international centres and institutes. Recipient of numerous awards, Mike has been active in research with various companies, banks, government agencies and academic centres, co-founder of a successful biotechnology company, and board member for several financial and IT companies. He has published 4 books, several edited volumes and over 200 papers. Mike has worked with many undergraduate and Master's research students, and as of 2025 has mentored around 65 primary PhD students and postdoctoral associates who moved to academic, industrial or governmental positions involving advanced statistical and data science research."}
{"id": "2512.09499", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.09499", "abs": "https://arxiv.org/abs/2512.09499", "authors": ["Sloan Nietert", "Ziv Goldfeld"], "title": "Estimation of Stochastic Optimal Transport Maps", "comment": "Appeared at NeurIPS 2025", "summary": "The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic."}
{"id": "2512.09358", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.09358", "abs": "https://arxiv.org/abs/2512.09358", "authors": ["Gaku Omiya", "Fumiyasu Komaki"], "title": "Minimization of Functions on Dually Flat Spaces Using Geodesic Descent Based on Dual Connections", "comment": "26 pages", "summary": "We propose geodesic-based optimization methods on dually flat spaces, where the geometric structure of the parameter manifold is closely related to the form of the objective function. A primary application is maximum likelihood estimation in statistical models, especially exponential families, whose model manifolds are dually flat. We show that an m-geodesic update, which directly optimizes the log-likelihood, can theoretically reach the maximum likelihood estimator in a single step. In contrast, an e-geodesic update has a practical advantage in cases where the parameter space is geodesically complete, allowing optimization without explicitly handling parameter constraints. We establish the theoretical properties of the proposed methods and validate their effectiveness through numerical experiments."}
{"id": "2512.09259", "categories": ["stat.ME", "math.ST", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2512.09259", "abs": "https://arxiv.org/abs/2512.09259", "authors": ["Yang Cao", "Zongming Ma"], "title": "MoDaH achieves rate optimal batch correction", "comment": null, "summary": "Batch effects pose a significant challenge in the analysis of single-cell omics data, introducing technical artifacts that confound biological signals. While various computational methods have achieved empirical success in correcting these effects, they lack the formal theoretical guarantees required to assess their reliability and generalization. To bridge this gap, we introduce Mixture-Model-based Data Harmonization (MoDaH), a principled batch correction algorithm grounded in a rigorous statistical framework.\n  Under a new Gaussian-mixture-model with explicit parametrization of batch effects, we establish the minimax optimal error rates for batch correction and prove that MoDaH achieves this rate by leveraging the recent theoretical advances in clustering data from anisotropic Gaussian mixtures. This constitutes, to the best of our knowledge, the first theoretical guarantee for batch correction. Extensive experiments on diverse single-cell RNA-seq and spatial proteomics datasets demonstrate that MoDaH not only attains theoretical optimality but also achieves empirical performance comparable to or even surpassing those of state-of-the-art heuristics (e.g., Harmony, Seurat-V5, and LIGER), effectively balancing the removal of technical noise with the conservation of biological signal."}
{"id": "2512.09217", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09217", "abs": "https://arxiv.org/abs/2512.09217", "authors": ["Saeed Saleh Namadi", "Jie Chen", "Deb Niemeier"], "title": "Access to healthcare for people with Alzheimer's Diseases and related dementias", "comment": null, "summary": "Background: Alzheimer's Disease and Related Dementias (ADRD) affects millions worldwide. Significant disparities exist in ADRD diagnosis and care, disproportionately impacting minority and socioeconomically vulnerable populations Objective: In this study, we investigate the relationship between ADRD density and accessibility to healthcare. We identify underserved and overserved areas in Maryland based on diagnosed cases and mortality due to ADRD, focusing on geographic disparities in care. Methods: 2023 Maryland ADRD patients were identified using ICD-10 codes from. Accessibility was measured using the Kernel Density Two-Step Floating Catchment Area (KD2SFCA) method. The Gini index and t-tests were used to analyze disparities between urban and rural areas. Hot Spot Analysis Getis-Ord Gi* and local bivariate relationships analysis were applied to assess spatial correlations. Principal component analysis (PCA) was applied to calculate the health risk index. Results: Hospital accessibility was unevenly distributed. Mortality rates from ADRD were higher in underserved areas with fewer hospitals. Hot spot analysis shows eastern and southern Maryland have zones with high mortality per population and per ADRD patient, surrounded by similarly high-rate zones. Central Maryland shows lower death rates per patient but more hospital facilities. In eastern Maryland, higher poverty areas are surrounded by zones with lower accessibility and higher health risk indices. Conclusion: Hospital accessibility is unevenly distributed, creating major rural disparities. Underserved regions in terms of access to healthcare facilities, particularly in eastern and southern Maryland, exhibit high ADRD mortality rates despite low diagnosis rates. This suggests that many ADRD cases remain undiagnosed, underdiagnosed, or subject to delayed treatment."}
{"id": "2512.09151", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09151", "abs": "https://arxiv.org/abs/2512.09151", "authors": ["Anna Chlingaryan", "Arman Melkumyan", "Raymond Leung"], "title": "IntegralGP: Volumetric estimation of subterranean geochemical properties in mineral deposits by fusing assay data with different spatial supports", "comment": "Keywords: Heteroscedastic Gaussian processes, IntegralGP covariance functions, volumetric regression, heterogeneous spatial supports, data fusion, ore grade estimation", "summary": "This article presents an Integral Gaussian Process (IntegralGP) framework for volumetric estimation of subterranean properties in mineral deposits. It provides a unified representation for data with different spatial supports, which enables blasthole geochemical assays to be properly modelled as interval observations rather than points. This approach is shown to improve regression performance and boundary delineation. A core contribution is a description of the mathematical changes to the covariance expressions which allow these benefits to be realised. The gradient and anti-derivatives are obtained to facilitate learning of the kernel hyperparameters. Numerical stability issues are also discussed. To illustrate its application, an IntegralGP data fusion algorithm is described. The objective is to assimilate line-based blasthole assays and update a block model that provides long-range prediction of Fe concentration beneath the drilled bench. Heteroscedastic GP is used to fuse chemically compatible but spatially incongruous data with different resolutions and sample spacings. Domain knowledge embodied in the structure and empirical distribution of the block model must be generally preserved while local inaccuracies are corrected. Using validation measurements within the predicted bench, our experiments demonstrate an improvement in bench-below grade prediction performance. For material classification, IntegralGP fusion reduces the absolute error and model bias in categorical prediction, especially instances where waste blocks are mistakenly classified as high-grade."}
{"id": "2512.09530", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09530", "abs": "https://arxiv.org/abs/2512.09530", "authors": ["Antonio Candelieri", "Alessandro Quadrio"], "title": "Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport", "comment": null, "summary": "This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.\n  Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R."}
{"id": "2512.09273", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.09273", "abs": "https://arxiv.org/abs/2512.09273", "authors": ["Ziyang Lyu", "S. A. Sisson", "A. H. Welsh"], "title": "On the inverse of covariance matrices for unbalanced crossed designs", "comment": "43 pages", "summary": "This paper addresses a long-standing open problem in the analysis of linear mixed models with crossed random effects under unbalanced designs: how to find an analytic expression for the inverse of $\\mathbf{V}$, the covariance matrix of the observed response. The inverse matrix $\\mathbf{V}^{-1}$ is required for likelihood-based estimation and inference. However, for unbalanced crossed designs, $\\mathbf{V}$ is dense and the lack of a closed-form representation for $\\mathbf{V}^{-1}$, until now, has made using likelihood-based methods computationally challenging and difficult to analyse mathematically. We use the Khatri--Rao product to represent $\\mathbf{V}$ and then to construct a modified covariance matrix whose inverse admits an exact spectral decomposition. Building on this construction, we obtain an elegant and simple approximation to $\\mathbf{V}^{-1}$ for asymptotic unbalanced designs. For non-asymptotic settings, we derive an accurate and interpretable approximation under mildly unbalanced data and establish an exact inverse representation as a low-rank correction to this approximation, applicable to arbitrary degrees of unbalance. Simulation studies demonstrate the accuracy, stability, and computational tractability of the proposed framework."}
{"id": "2512.09316", "categories": ["stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.09316", "abs": "https://arxiv.org/abs/2512.09316", "authors": ["Marios Papamichalis", "Nicholas Christakis", "Feng Fu"], "title": "Group Cooperation Diverges onto Durable Low versus High Paths: Public Goods Experiments in 134 Honduran Villages", "comment": "This is the initial version of the manuscript. The presentation of figures, tables, and analyses may be revised in future versions to better align with the requirements and scope of the target journal", "summary": "We performed large, lab-in-the-field experiment (2,591 participants across 134 Honduran villages; ten rounds) and tracked how contribution behavior unfolds in fixed, anonymous groups of size five. Contribution separates early into two durable paths, one low and one high, with rare convergence thereafter. High-path players can be identified with strong accuracy early on. Groups that begin with an early majority of above-norm contributors (about 60%) are very likely finish high. The empirical finding of a bifurcation, consistent with the theory, shows that early, high contributions by socially central people steer groups onto, and help keep them on, a high-cooperation path."}
{"id": "2512.09237", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09237", "abs": "https://arxiv.org/abs/2512.09237", "authors": ["Xiaoya Wang", "Richard J. Cook", "Yeying Zhu", "Tugba Akkaya-Hocagil", "R. Colin Carter", "Sandra W. Jacobson", "Joseph L. Jacobson", "Louise M. Ryan"], "title": "Prenatal alcohol exposure and child cognition: semi-continuous exposures, causal inference and evidence synthesis", "comment": null, "summary": "We address the challenge of causal inference status and the dose-response effects with a semi-continuous exposure. A two-stage approach is proposed using estimating equation for multiple outcomes with large sample properties derived for the resulting estimators. Homogeneity tests are developed to assess whether causal effects of exposure status and the dose-response effects are the same across multiple outcomes. A global homogeneity test is also developed to assess whether the effect of exposure status (exposed/not exposed) and the dose-response effect of the continuous exposure level are each equal across all outcomes. The methods of estimation and testing are rigorously evaluated in simulation studies and applied to a motivating study on the effects of prenatal alcohol exposure on childhood cognition defined by executive function (EF), academic achievement in math, and learning and memory (LM)."}
{"id": "2512.09538", "categories": ["stat.ML", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09538", "abs": "https://arxiv.org/abs/2512.09538", "authors": ["Ekaterina Fadeeva", "Maiya Goloburda", "Aleksandr Rubashevskii", "Roman Vashurin", "Artem Shelmanov", "Preslav Nakov", "Mrinmaya Sachan", "Maxim Panov"], "title": "Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search", "comment": null, "summary": "Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance."}
{"id": "2512.09499", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.09499", "abs": "https://arxiv.org/abs/2512.09499", "authors": ["Sloan Nietert", "Ziv Goldfeld"], "title": "Estimation of Stochastic Optimal Transport Maps", "comment": "Appeared at NeurIPS 2025", "summary": "The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic."}
{"id": "2512.09561", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09561", "abs": "https://arxiv.org/abs/2512.09561", "authors": ["Bao Anh Vu", "Andrew Zammit-Mangion", "David Gunawan", "Felicity S. McCormack", "Noel Cressie"], "title": "Neural posterior inference with state-space models for calibrating ice sheet simulators", "comment": null, "summary": "Ice sheet models are routinely used to quantify and project an ice sheet's contribution to sea level rise. In order for an ice sheet model to generate realistic projections, its parameters must first be calibrated using observational data; this is challenging due to the nonlinearity of the model equations, the high dimensionality of the underlying parameters, and limited data availability for validation. This study leverages the emerging field of neural posterior approximation for efficiently calibrating ice sheet model parameters and boundary conditions. We make use of a one-dimensional (flowline) Shallow-Shelf Approximation model in a state-space framework. A neural network is trained to infer the underlying parameters, namely the bedrock elevation and basal friction coefficient along the flowline, based on observations of ice velocity and ice surface elevation. Samples from the approximate posterior distribution of the parameters are then used within an ensemble Kalman filter to infer latent model states, namely the ice thickness along the flowline. We show through a simulation study that our approach yields more accurate estimates of the parameters and states than a state-augmented ensemble Kalman filter, which is the current state-of-the-art. We apply our approach to infer the bed elevation and basal friction along a flowline in Thwaites Glacier, Antarctica."}
{"id": "2512.09259", "categories": ["stat.ME", "math.ST", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2512.09259", "abs": "https://arxiv.org/abs/2512.09259", "authors": ["Yang Cao", "Zongming Ma"], "title": "MoDaH achieves rate optimal batch correction", "comment": null, "summary": "Batch effects pose a significant challenge in the analysis of single-cell omics data, introducing technical artifacts that confound biological signals. While various computational methods have achieved empirical success in correcting these effects, they lack the formal theoretical guarantees required to assess their reliability and generalization. To bridge this gap, we introduce Mixture-Model-based Data Harmonization (MoDaH), a principled batch correction algorithm grounded in a rigorous statistical framework.\n  Under a new Gaussian-mixture-model with explicit parametrization of batch effects, we establish the minimax optimal error rates for batch correction and prove that MoDaH achieves this rate by leveraging the recent theoretical advances in clustering data from anisotropic Gaussian mixtures. This constitutes, to the best of our knowledge, the first theoretical guarantee for batch correction. Extensive experiments on diverse single-cell RNA-seq and spatial proteomics datasets demonstrate that MoDaH not only attains theoretical optimality but also achieves empirical performance comparable to or even surpassing those of state-of-the-art heuristics (e.g., Harmony, Seurat-V5, and LIGER), effectively balancing the removal of technical noise with the conservation of biological signal."}
{"id": "2512.09912", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09912", "abs": "https://arxiv.org/abs/2512.09912", "authors": ["Erin Craig", "Robert Tibshirani"], "title": "Supervised learning pays attention", "comment": null, "summary": "In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.\n  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure."}
{"id": "2512.09724", "categories": ["stat.AP", "astro-ph.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09724", "abs": "https://arxiv.org/abs/2512.09724", "authors": ["Nikoloz Gigiberia"], "title": "Bayesian Model Selection with an Application to Cosmology", "comment": null, "summary": "We investigate cosmological parameter inference and model selection from a Bayesian perspective. Type Ia supernova data from the Dark Energy Survey (DES-SN5YR) are used to test the $Λ$CDM, $w$CDM, and CPL cosmological models. Posterior inference is performed via Hamiltonian Monte Carlo using the No-U-Turn Sampler (NUTS) implemented in NumPyro and analyzed with ArviZ in Python. Bayesian model comparison is conducted through Bayes factors computed using the bridgesampling library in R. The results indicate that all three models demonstrate similar predictive performance, but $w$CDM shows stronger evidence relative to $Λ$CDM and CPL. We conclude that, under the assumptions and data used in this study, $w$CDM provides a better description of cosmological expansion."}
{"id": "2512.09262", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09262", "abs": "https://arxiv.org/abs/2512.09262", "authors": ["James Peng", "Michal Juraska", "Pamela A. Shaw", "Peter B. Gilbert"], "title": "Vaccine sieve analysis on deep sequencing data using competing risks Cox regression with failure type subject to misclassification", "comment": null, "summary": "Understanding how vaccines perform against different pathogen genotypes is crucial for developing effective prevention strategies, particularly for highly genetically diverse pathogens like HIV. Sieve analysis is a statistical framework used to determine whether a vaccine selectively prevents acquisition of certain genotypes while allowing breakthrough of other genotypes that evade immune responses. Traditionally, these analyses are conducted with a single sequence available per individual acquiring the pathogen. However, modern sequencing technology can provide detailed characterization of intra-individual viral diversity by capturing up to hundreds of pathogen sequences per person. In this work, we introduce methodology that extends sieve analysis to account for intra-individual viral diversity. Our approach estimates vaccine efficacy against viral populations with varying true (unobservable) frequencies of vaccine-mismatched mutations. To account for differential resolution of information from differing sequence counts per person, we use competing risks Cox regression with modeled causes of failure and propose an empirical Bayes approach for the classification model. Simulation studies demonstrate that our approach reduces bias, provides nominal confidence interval coverage, and improves statistical power compared to conventional methods. We apply our method to the HVTN 705 Imbokodo trial, which assessed the efficacy of a heterologous vaccine regimen in preventing HIV-1 acquisition."}
{"id": "2512.09060", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.09060", "abs": "https://arxiv.org/abs/2512.09060", "authors": ["Kellin N. Rumsey", "Graham C. Gibson", "Devin Francom", "Reid Morris"], "title": "All Emulators are Wrong, Many are Useful, and Some are More Useful Than Others: A Reproducible Comparison of Computer Model Surrogates", "comment": null, "summary": "Accurate and efficient surrogate modeling is essential for modern computational science, and there are a staggering number of emulation methods to choose from. With new methods being developed all the time, comparing the relative strengths and weaknesses of different methods remains a challenge due to inconsistent benchmarking practices and (sometimes) limited reproducibility and transparency. In this work, we present a large-scale, fully reproducible comparison of $29$ distinct emulators across $60$ canonical test functions and $40$ real emulation datasets. To facilitate rigorous, apples-to-apples comparisons, we introduce the R package \\texttt{duqling}, which streamlines reproducible simulation studies using a consistent, simple syntax, and automatic internal scaling of inputs. This framework allows researchers to compare emulators in a unified environment and makes it possible to replicate or extend previous studies with minimal effort, even across different publications. Our results provide detailed empirical insight into the strengths and weaknesses of state-of-the-art emulators and offer guidance for both method developers and practitioners selecting a surrogate for new data. We discuss best practices for emulator comparison and highlight how \\texttt{duqling} can accelerate research in emulator design and application."}
{"id": "2512.09732", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09732", "abs": "https://arxiv.org/abs/2512.09732", "authors": ["Anastasios Apsemidis", "Dimitris Mavridis", "Nikolaos Demiris"], "title": "Network Meta Analysis of Mean Survival", "comment": "29 pages, 4 figures", "summary": "Decisions based upon pairwise comparisons of multiple treatments are naturally performed in terms of the mean survival of the selected study arms or functions thereof. However, synthesis of treatment comparisons is usually performed on surrogates of the mean survival, such as hazard ratios or restricted mean survival times. Thus, network meta-analysis techniques may suffer from the limitations of these approaches, such as incorrect proportional hazards assumption or short-term follow-up periods. We propose a Bayesian framework for the network meta-analysis of the main outcome informing the decision, the mean survival of a treatment. Its derivation involves extrapolation of the observed survival curves. We use methods for stable extrapolation that integrate long term evidence based upon mortality projections. Extrapolations are performed using flexible poly-hazard parametric models and M-spline-based methods. We assess the computational and statistical efficiency of different techniques using a simulation study and apply the developed methods to two real data sets. The proposed method is formulated within a decision theoretic framework for cost-effectiveness analyses, where the `best' treatment is to be selected and incorporating the associated cost information is straightforward."}
{"id": "2512.09273", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.09273", "abs": "https://arxiv.org/abs/2512.09273", "authors": ["Ziyang Lyu", "S. A. Sisson", "A. H. Welsh"], "title": "On the inverse of covariance matrices for unbalanced crossed designs", "comment": "43 pages", "summary": "This paper addresses a long-standing open problem in the analysis of linear mixed models with crossed random effects under unbalanced designs: how to find an analytic expression for the inverse of $\\mathbf{V}$, the covariance matrix of the observed response. The inverse matrix $\\mathbf{V}^{-1}$ is required for likelihood-based estimation and inference. However, for unbalanced crossed designs, $\\mathbf{V}$ is dense and the lack of a closed-form representation for $\\mathbf{V}^{-1}$, until now, has made using likelihood-based methods computationally challenging and difficult to analyse mathematically. We use the Khatri--Rao product to represent $\\mathbf{V}$ and then to construct a modified covariance matrix whose inverse admits an exact spectral decomposition. Building on this construction, we obtain an elegant and simple approximation to $\\mathbf{V}^{-1}$ for asymptotic unbalanced designs. For non-asymptotic settings, we derive an accurate and interpretable approximation under mildly unbalanced data and establish an exact inverse representation as a low-rank correction to this approximation, applicable to arbitrary degrees of unbalance. Simulation studies demonstrate the accuracy, stability, and computational tractability of the proposed framework."}
{"id": "2512.09358", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.09358", "abs": "https://arxiv.org/abs/2512.09358", "authors": ["Gaku Omiya", "Fumiyasu Komaki"], "title": "Minimization of Functions on Dually Flat Spaces Using Geodesic Descent Based on Dual Connections", "comment": "26 pages", "summary": "We propose geodesic-based optimization methods on dually flat spaces, where the geometric structure of the parameter manifold is closely related to the form of the objective function. A primary application is maximum likelihood estimation in statistical models, especially exponential families, whose model manifolds are dually flat. We show that an m-geodesic update, which directly optimizes the log-likelihood, can theoretically reach the maximum likelihood estimator in a single step. In contrast, an e-geodesic update has a practical advantage in cases where the parameter space is geodesically complete, allowing optimization without explicitly handling parameter constraints. We establish the theoretical properties of the proposed methods and validate their effectiveness through numerical experiments."}
{"id": "2512.09151", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09151", "abs": "https://arxiv.org/abs/2512.09151", "authors": ["Anna Chlingaryan", "Arman Melkumyan", "Raymond Leung"], "title": "IntegralGP: Volumetric estimation of subterranean geochemical properties in mineral deposits by fusing assay data with different spatial supports", "comment": "Keywords: Heteroscedastic Gaussian processes, IntegralGP covariance functions, volumetric regression, heterogeneous spatial supports, data fusion, ore grade estimation", "summary": "This article presents an Integral Gaussian Process (IntegralGP) framework for volumetric estimation of subterranean properties in mineral deposits. It provides a unified representation for data with different spatial supports, which enables blasthole geochemical assays to be properly modelled as interval observations rather than points. This approach is shown to improve regression performance and boundary delineation. A core contribution is a description of the mathematical changes to the covariance expressions which allow these benefits to be realised. The gradient and anti-derivatives are obtained to facilitate learning of the kernel hyperparameters. Numerical stability issues are also discussed. To illustrate its application, an IntegralGP data fusion algorithm is described. The objective is to assimilate line-based blasthole assays and update a block model that provides long-range prediction of Fe concentration beneath the drilled bench. Heteroscedastic GP is used to fuse chemically compatible but spatially incongruous data with different resolutions and sample spacings. Domain knowledge embodied in the structure and empirical distribution of the block model must be generally preserved while local inaccuracies are corrected. Using validation measurements within the predicted bench, our experiments demonstrate an improvement in bench-below grade prediction performance. For material classification, IntegralGP fusion reduces the absolute error and model bias in categorical prediction, especially instances where waste blocks are mistakenly classified as high-grade."}
{"id": "2512.09337", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.09337", "abs": "https://arxiv.org/abs/2512.09337", "authors": ["Kentaro Kawato"], "title": "Balancing Weights for Causal Mediation Analysis", "comment": "61 pages", "summary": "This paper develops methods for estimating the natural direct and indirect effects in causal mediation analysis. The efficient influence function-based estimator (EIF-based estimator) and the inverse probability weighting estimator (IPW estimator), which are standard in causal mediation analysis, both rely on the inverse of the estimated propensity scores, and thus they are vulnerable to two key issues (i) instability and (ii) finite-sample covariate imbalance. We propose estimators based on the weights obtained by an algorithm that directly penalizes weight dispersion while enforcing approximate covariate and mediator balance, thereby improving stability and mitigating bias in finite samples. We establish the convergence rates of the proposed weights and show that the resulting estimators are asymptotically normal and achieve the semiparametric efficiency bound. Monte Carlo simulations demonstrate that the proposed estimator outperforms not only the EIF-based estimator and the IPW estimator but also the regression imputation estimator in challenging scenarios with model misspecification. Furthermore, the proposed method is applied to a real dataset from a study examining the effects of media framing on immigration attitudes."}
{"id": "2512.09163", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09163", "abs": "https://arxiv.org/abs/2512.09163", "authors": ["Gabrielle Rives", "Olivier Lopez", "Nicolas Bousquet"], "title": "WTNN: Weibull-Tailored Neural Networks for survival analysis", "comment": null, "summary": "The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches."}
{"id": "2512.09430", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09430", "abs": "https://arxiv.org/abs/2512.09430", "authors": ["Kun Yi", "Lucy Xia"], "title": "Model-robust Inference for Seamless II/III Trials with Covariate Adaptive Randomization", "comment": null, "summary": "Seamless phase II/III trials have become a cornerstone of modern drug development, offering a means to accelerate evaluation while maintaining statistical rigor. However, most existing inference procedures are model-based, designed primarily for continuous outcomes, and often neglect the stratification used in covariate-adaptive randomization (CAR), limiting their practical relevance. In this paper, we propose a unified, model-robust framework for seamless phase II/III trials grounded in generalized linear models (GLMs), enabling valid inference across diverse outcome types, estimands, and CAR schemes. Using Z-estimation, we derive the asymptotic properties of treatment effect estimators and explicitly characterize how their variance depends on the underlying randomization procedure.Based on these results, we develop adjusted Wald tests that, together with Dunnett's multiple-comparison procedure and the inverse chi-square combination method, ensure valid overall Type I error. Extensive simulation studies and a trial example demonstrate that the proposed model-robust tests achieve superior power and reliable inference compared to conventional approaches."}
{"id": "2512.09262", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09262", "abs": "https://arxiv.org/abs/2512.09262", "authors": ["James Peng", "Michal Juraska", "Pamela A. Shaw", "Peter B. Gilbert"], "title": "Vaccine sieve analysis on deep sequencing data using competing risks Cox regression with failure type subject to misclassification", "comment": null, "summary": "Understanding how vaccines perform against different pathogen genotypes is crucial for developing effective prevention strategies, particularly for highly genetically diverse pathogens like HIV. Sieve analysis is a statistical framework used to determine whether a vaccine selectively prevents acquisition of certain genotypes while allowing breakthrough of other genotypes that evade immune responses. Traditionally, these analyses are conducted with a single sequence available per individual acquiring the pathogen. However, modern sequencing technology can provide detailed characterization of intra-individual viral diversity by capturing up to hundreds of pathogen sequences per person. In this work, we introduce methodology that extends sieve analysis to account for intra-individual viral diversity. Our approach estimates vaccine efficacy against viral populations with varying true (unobservable) frequencies of vaccine-mismatched mutations. To account for differential resolution of information from differing sequence counts per person, we use competing risks Cox regression with modeled causes of failure and propose an empirical Bayes approach for the classification model. Simulation studies demonstrate that our approach reduces bias, provides nominal confidence interval coverage, and improves statistical power compared to conventional methods. We apply our method to the HVTN 705 Imbokodo trial, which assessed the efficacy of a heterologous vaccine regimen in preventing HIV-1 acquisition."}
{"id": "2512.09433", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09433", "abs": "https://arxiv.org/abs/2512.09433", "authors": ["Yifei Tian", "Ying Wu"], "title": "Multiply-robust Estimator of Cumulative Incidence Function Difference for Right-Censored Competing Risks Data", "comment": null, "summary": "In causal inference, estimating the average treatment effect is a central objective, and in the context of competing risks data, this effect can be quantified by the cause-specific cumulative incidence function (CIF) difference. While doubly robust estimators give a more robust way to estimate the causal effect from the observational study, they remain inconsistent if both models are misspecified. To improve the robustness, we develop a multiply robust estimator for the difference in cause-specific CIFs using right-censored competing risks data. The proposed framework integrates the pseudo-value approach, which transforms the censored, time-dependent CIF into a complete-data outcome, with the multiply robust estimation framework. By specifying multiple candidate models for both the propensity score and the outcome regression, the resulting estimator is consistent and asymptotically unbiased, provided that at least one of the multiple propensity score or outcome regression models is correctly specified. Simulation studies show our multiply robust estimator remains virtually unbiased and maintains nominal coverage rates under various model misspecification scenarios and a wide range of choices for the censoring rate. Finally, the proposed multiply robust model is illustrated using the Right Heart Catheterization dataset."}
{"id": "2512.09505", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09505", "abs": "https://arxiv.org/abs/2512.09505", "authors": ["Caren Hasler", "Arnaud Tripet", "Yves Tillé"], "title": "Calibration with Bagging of the Principal Components on a Large Number of Auxiliary Variables", "comment": null, "summary": "Calibration is a widely used method in survey sampling to adjust weights so that estimated totals of some chosen calibration variables match known population totals or totals obtained from other sources. When a large number of auxiliary variables are included as calibration variables, the variance of the total estimator can increase, and the calibration weights can become highly dispersed. To address these issues, we propose a solution inspired by bagging and principal component decomposition. With our approach, the principal components of the auxiliary variables are constructed. Several samples of calibration variables are selected without replacement and with unequal probabilities from among the principal components. For each sample, a system of weights is obtained. The final weights are the average weights of these different weighting systems. With our proposed method, it is possible to calibrate exactly for some of the main auxiliary variables. For the other auxiliary variables, the weights cannot be calibrated exactly. The proposed method allows us to obtain a total estimator whose variance does not explode when new auxiliary variables are added and to obtain very low scatter weights. Finally, our proposed method allows us to obtain a single weighting system that can be applied to several variables of interest of a survey."}
{"id": "2512.09553", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09553", "abs": "https://arxiv.org/abs/2512.09553", "authors": ["Peng Zeng", "Yushan Mu"], "title": "A Bayesian Approach for Robust Longitudinal Envelope Models", "comment": null, "summary": "The envelope model provides a dimension-reduction framework for multivariate linear regression. However, existing envelope methods typically assume normally distributed random errors and do not accommodate repeated measures in longitudinal studies. To address these limitations, we propose the robust longitudinal envelope model (RoLEM). RoLEM employs a scale mixture of matrix-variate normal distributions to model random errors, allowing it to handle potential outliers, and incorporates flexible correlation structures for repeated measurements. In addition, we introduce new prior and proposal distributions on the Grassmann manifold to facilitate Bayesian inference for RoLEM. Simulation studies and real data analysis demonstrate the superior performance of the proposed method."}
{"id": "2512.09659", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09659", "abs": "https://arxiv.org/abs/2512.09659", "authors": ["Ritabrata Karmakar", "Joydeep Chowdhury", "Subhajit Dutta", "Marc G. Genton"], "title": "Uniform-over-dimension location tests for multivariate and high-dimensional data", "comment": "This paper replaces our earlier work (arXiv:2403.16328)", "summary": "Asymptotic methods for hypothesis testing in high-dimensional data usually require the dimension of the observations to increase to infinity, often with an additional relationship between the dimension (say, $p$) and the sample size (say, $n$). On the other hand, multivariate asymptotic testing methods are valid for fixed dimension only and their implementations typically require the sample size to be large compared to the dimension to yield desirable results. In practical scenarios, it is usually not possible to determine whether the dimension of the data conform to the conditions required for the validity of the high-dimensional asymptotic methods for hypothesis testing, or whether the sample size is large enough compared to the dimension of the data. In this work, we first describe the notion of uniform-over-$p$ convergences and subsequently, develop a uniform-over-dimension central limit theorem. An asymptotic test for the two-sample equality of locations is developed, which now holds uniformly over the dimension of the observations. Using simulated and real data, it is demonstrated that the proposed test exhibits better performance compared to several popular tests in the literature for high-dimensional data as well as the usual scaled two-sample tests for multivariate data, including the Hotelling's $T^2$ test for multivariate Gaussian data."}
{"id": "2512.09821", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09821", "abs": "https://arxiv.org/abs/2512.09821", "authors": ["Hung Kuan Lee"], "title": "RECAP Framework v1.0: A Multi-Layer Inheritance Architecture for Evidence Synthesis", "comment": "34 pages, 2 figures, 1 toy example, full methodological specification. Suitable for methodology-focused journals (e.g., Journal of Clinical Epidemiology, Research Synthesis Methods). (Modify page count if needed.)", "summary": "Evidence synthesis has advanced through improved reporting standards, bias assessment tools, and analytic methods, but current workflows remain limited by a single-layer structure in which conceptual, methodological, and procedural decisions are made on the same level. This forces each project to rebuild its methodological foundations from scratch, leading to inconsistencies, conceptual drift, and unstable reasoning across projects. RECAP Framework v1.0 introduces a three-layer meta-architecture consisting of methodological laws (Grandparent), domain-level abstractions (Parent), and project-level implementations (Child). The framework defines an inheritance system with strict rules for tiering, routing, and contamination control to preserve construct clarity, enforce inferential discipline, and support reproducibility across multi-project evidence ecosystems. RECAP provides a formal governance layer for evidence synthesis and establishes the foundation for a methodological lineage designed to stabilize reasoning across research programs."}
{"id": "2512.09826", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09826", "abs": "https://arxiv.org/abs/2512.09826", "authors": ["Md Yasin Ali Parh", "Jeremy T. Gaskins"], "title": "Predictor-Informed Bayesian Nonparametric Clustering", "comment": null, "summary": "In this project we are interested in performing clustering of observations such that the cluster membership is influenced by a set of predictors. To that end, we employ the Bayesian nonparameteric Common Atoms Model, which is a nested clustering algorithm that utilizes a (fixed) group membership for each observation to encourage more similar clustering of members of the same group. CAM operates by assuming each group has its own vector of cluster probabilities, which are themselves clustered to allow similar clustering for some groups. We extend this approach by treating the group membership as an unknown latent variable determined as a flexible nonparametric form of the covariate vector. Consequently, observations with similar predictor values will be in the same latent group and are more likely to be clustered together than observations with disparate predictors. We propose a pyramid group model that flexibly partitions the predictor space into these latent group memberships. This pyramid model operates similarly to a Bayesian regression tree process except that it uses the same splitting rule for at all nodes at the same tree depth which facilitates improved mixing. We outline a block Gibbs sampler to perform posterior inference from our model. Our methodology is demonstrated in simulation and real data examples. In the real data application, we utilize the RAND Health and Retirement Study to cluster and predict patient outcomes in terms of the number of overnight hospital stays."}
{"id": "2512.09163", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09163", "abs": "https://arxiv.org/abs/2512.09163", "authors": ["Gabrielle Rives", "Olivier Lopez", "Nicolas Bousquet"], "title": "WTNN: Weibull-Tailored Neural Networks for survival analysis", "comment": null, "summary": "The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches."}
{"id": "2512.09724", "categories": ["stat.AP", "astro-ph.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09724", "abs": "https://arxiv.org/abs/2512.09724", "authors": ["Nikoloz Gigiberia"], "title": "Bayesian Model Selection with an Application to Cosmology", "comment": null, "summary": "We investigate cosmological parameter inference and model selection from a Bayesian perspective. Type Ia supernova data from the Dark Energy Survey (DES-SN5YR) are used to test the $Λ$CDM, $w$CDM, and CPL cosmological models. Posterior inference is performed via Hamiltonian Monte Carlo using the No-U-Turn Sampler (NUTS) implemented in NumPyro and analyzed with ArviZ in Python. Bayesian model comparison is conducted through Bayes factors computed using the bridgesampling library in R. The results indicate that all three models demonstrate similar predictive performance, but $w$CDM shows stronger evidence relative to $Λ$CDM and CPL. We conclude that, under the assumptions and data used in this study, $w$CDM provides a better description of cosmological expansion."}
{"id": "2512.09732", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09732", "abs": "https://arxiv.org/abs/2512.09732", "authors": ["Anastasios Apsemidis", "Dimitris Mavridis", "Nikolaos Demiris"], "title": "Network Meta Analysis of Mean Survival", "comment": "29 pages, 4 figures", "summary": "Decisions based upon pairwise comparisons of multiple treatments are naturally performed in terms of the mean survival of the selected study arms or functions thereof. However, synthesis of treatment comparisons is usually performed on surrogates of the mean survival, such as hazard ratios or restricted mean survival times. Thus, network meta-analysis techniques may suffer from the limitations of these approaches, such as incorrect proportional hazards assumption or short-term follow-up periods. We propose a Bayesian framework for the network meta-analysis of the main outcome informing the decision, the mean survival of a treatment. Its derivation involves extrapolation of the observed survival curves. We use methods for stable extrapolation that integrate long term evidence based upon mortality projections. Extrapolations are performed using flexible poly-hazard parametric models and M-spline-based methods. We assess the computational and statistical efficiency of different techniques using a simulation study and apply the developed methods to two real data sets. The proposed method is formulated within a decision theoretic framework for cost-effectiveness analyses, where the `best' treatment is to be selected and incorporating the associated cost information is straightforward."}
