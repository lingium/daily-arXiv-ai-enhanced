<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 3]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.AP](#stat.AP) [Total: 5]
- [stat.ME](#stat.ME) [Total: 6]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Alzheimer's Clinical Research Data via R Packages: the alzverse](https://arxiv.org/abs/2510.02318)
*Michael C. Donohue,Kedir Hussen,Oliver Langford,Richard Gallardo,Gustavo Jimenez-Maggiora,Paul S. Aisen*

Main category: stat.CO

TL;DR: 本文介绍了使用R包解决阿尔茨海默病临床研究数据共享挑战的方法，包括A4LEARN、ADNIMERGE2和alzverse三个R包，旨在促进数据标准化、可访问性和可重复性分析。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病等领域的临床研究数据共享面临数据可访问性、标准化、文档化、可用性和可重复性等挑战，阻碍了研究进展。

Method: 开发了三个R包：A4LEARN包含随机试验数据，ADNIMERGE2包含观察性研究数据，alzverse使用通用数据标准整合不同研究数据包以支持荟萃分析。

Result: 这些R包将数据、文档和可重复分析案例整合成便携式捆绑包，可在常用R编程环境中安装和浏览。

Conclusion: 通过促进协作、透明度和可重复性，R数据包可以在加速临床研究中发挥重要作用。

Abstract: Sharing clinical research data is essential for advancing research in
Alzheimer's disease (AD) and other therapeutic areas. However, challenges in
data accessibility, standardization, documentation, usability, and
reproducibility continue to impede this goal. In this article, we highlight the
advantages of using R packages to overcome these challenges using two examples.
The A4LEARN R package includes data from a randomized trial (the Anti-Amyloid
Treatment in Asymptomatic Alzheimer's [A4] study) and its companion
observational study of biomarker negative individuals (the Longitudinal
Evaluation of Amyloid Risk and Neurodegeneration [LEARN] study). The ADNIMERGE2
R package includes data from the Alzheimer's Disease Neuroimaging Initiative
(ADNI), a longitudinal observational biomarker and imaging study. These
packages collect data, documentation, and reproducible analysis vignettes into
a portable bundle that can be installed and browsed within commonly used R
programming environments. We also introduce the alzverse package which
leverages a common data standard to combine study-specific data packages to
facilitate meta-analyses. By promoting collaboration, transparency, and
reproducibility, R data packages can play a vital role in accelerating clinical
research.

</details>


### [2] [HOMC: A MATLAB Package for Higher Order Markov Chains](https://arxiv.org/abs/2510.02664)
*Jianhong Xu*

Main category: stat.CO

TL;DR: 开发了一个MATLAB软件包，用于高阶马尔可夫链(HOMC)的计算和分析，包含k步转移张量、极限概率分布、首次到达时间张量等关键功能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门用于高阶马尔可夫链的软件工具，需要开发一个能够计算所有重要量的综合软件包，方便研究人员和从业者进行数值实验和算法原型设计。

Method: 开发MATLAB软件包，实现高阶马尔可夫链的关键计算功能，包括张量"box"积等特殊运算，并提供状态分类（遍历/正则、常返/暂态）等分析工具。

Result: 成功开发了首个专门用于高阶马尔可夫链的MATLAB软件包，能够计算所有重要量，并包含独特的张量运算功能。

Conclusion: 该HOMC软件包为高阶马尔可夫链的研究和应用提供了实用的计算工具，对数值实验和算法开发具有重要意义。

Abstract: We present a MATLAB package, which is the first of its kind, for Higher Order
Markov Chains (HOMC). It can be used to easily compute all important quantities
in our recent works relevant to higher order Markov chains, such as the
$k$-step transition tensor, limiting probability distribution, ever-reaching
probability tensor, and mean first passage time tensor. It can also be used to
check whether a higher order chain is ergodic or regular, to construct the
transition matrix of the associated reduced first order chain, and to determine
whether a state is recurrent or transient. A key function in the package is an
implementation of the tensor ``box'' product which has a probabilistic
interpretation and is different from other tensor products in the literature.
This HOMC package is useful to researchers and practitioners alike for tasks
such as numerical experimentation and algorithm prototyping involving higher
order Markov chains.

</details>


### [3] [A fast non-reversible sampler for Bayesian finite mixture models](https://arxiv.org/abs/2510.03226)
*Filippo Ascolani,Giacomo Zanella*

Main category: stat.CO

TL;DR: 提出了一种用于贝叶斯有限混合模型的新型非可逆采样方案，在收敛阶段和混合分量重叠时显著优于经典采样器，理论分析表明收敛时间可从O(n²)降至O(n)。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯有限混合模型的后验分布采样困难，特别是当观测数量n很大时，流行的可逆MCMC方案收敛缓慢。

Method: 引入简单非可逆采样方案，专门针对贝叶斯有限混合模型设计，理论证明其性能不会比标准方案差超过4倍。

Result: 新采样器在多种场景下大幅超越经典采样器，尤其在收敛阶段和混合分量有显著重叠时表现优异。

Conclusion: 混合模型的统计特性使其成为使用非可逆离散采样器的理想案例，新方法能显著提升采样效率。

Abstract: Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known
that sampling from the resulting posterior distribution can be a hard task. In
particular, popular reversible Markov chain Monte Carlo schemes are often slow
to converge when the number of observations $n$ is large. In this paper we
introduce a novel and simple non-reversible sampling scheme for Bayesian finite
mixture models, which is shown to drastically outperform classical samplers in
many scenarios of interest, especially during convergence phase and when
components in the mixture have non-negligible overlap. At the theoretical
level, we show that the performance of the proposed non-reversible scheme
cannot be worse than the standard one, in terms of asymptotic variance, by more
than a factor of four; and we provide a scaling limit analysis suggesting that
the non-reversible sampler can reduce the convergence time from O$(n^2)$ to
O$(n)$. We also discuss why the statistical features of mixture models make
them an ideal case for the use of non-reversible discrete samplers.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [4] [Higher-arity PAC learning, VC dimension and packing lemma](https://arxiv.org/abs/2510.02420)
*Artem Chernikov,Henry Towsner*

Main category: stat.ML

TL;DR: 本文概述了高阶VC理论的发展，包括Haussler打包引理的推广和超图正则性引理，并证明该理论能够刻画n重乘积空间中的高阶PAC学习。


<details>
  <summary>Details</summary>
Motivation: 发展高阶VC理论，以处理n重乘积空间中的学习问题，并建立与高阶PAC学习的联系。

Method: 推广Haussler打包引理，提出切片式超图正则性引理，构建高阶VC维度理论框架。

Result: 证明了高阶VC理论能够刻画n重乘积空间中的高阶PAC学习，并展示了近期相关研究结果可由该理论推导得出。

Conclusion: 高阶VC理论为处理复杂学习问题提供了有效工具，能够统一解释多个相关研究结果。

Abstract: The aim of this note is to overview some of our work in Chernikov, Towsner'20
(arXiv:2010.00726) developing higher arity VC theory (VC$_n$ dimension),
including a generalization of Haussler packing lemma, and an associated tame
(slice-wise) hypergraph regularity lemma; and to demonstrate that it
characterizes higher arity PAC learning (PAC$_n$ learning) in $n$-fold product
spaces with respect to product measures introduced by Kobayashi, Kuriyama and
Takeuchi'15. We also point out how some of the recent results in
arXiv:2402.14294, arXiv:2505.15688, arXiv:2509.20404 follow from our work in
arXiv:2010.00726.

</details>


### [5] [Predictive inference for time series: why is split conformal effective despite temporal dependence?](https://arxiv.org/abs/2510.02471)
*Rina Foygel Barber,Ashwin Pananjady*

Main category: stat.ML

TL;DR: 本文研究了时间序列预测中的不确定性量化问题，分析了在存在时间依赖性的情况下，分形保形预测方法的理论性质，并提出了衡量时间依赖性对交换性假设违反程度的"切换系数"。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测方法假设数据独立同分布或可交换，但在时间序列中，即使短期时间依赖也会严重违反这一假设。使用具有记忆的预测器（如自回归模型）会进一步加剧这个问题。

Method: 研究分形保形预测在时间序列设置下的理论性质，包括预测器具有记忆的情况。通过新提出的"切换系数"来衡量时间依赖性对交换性假设的违反程度。

Result: 在平稳β混合过程类上，对覆盖概率的刻画是尖锐的。研究结果为依赖数据的预测推断方法分析提供了有用工具。

Conclusion: 时间序列中的保形预测方法在理论上有覆盖损失，但可以通过切换系数来量化这种损失，为依赖数据的预测推断提供了理论分析框架。

Abstract: We consider the problem of uncertainty quantification for prediction in a
time series: if we use past data to forecast the next time point, can we
provide valid prediction intervals around our forecasts? To avoid placing
distributional assumptions on the data, in recent years the conformal
prediction method has been a popular approach for predictive inference, since
it provides distribution-free coverage for any iid or exchangeable data
distribution. However, in the time series setting, the strong empirical
performance of conformal prediction methods is not well understood, since even
short-range temporal dependence is a strong violation of the exchangeability
assumption. Using predictors with "memory" -- i.e., predictors that utilize
past observations, such as autoregressive models -- further exacerbates this
problem. In this work, we examine the theoretical properties of split conformal
prediction in the time series setting, including the case where predictors may
have memory. Our results bound the loss of coverage of these methods in terms
of a new "switch coefficient", measuring the extent to which temporal
dependence within the time series creates violations of exchangeability. Our
characterization of the coverage probability is sharp over the class of
stationary, $\beta$-mixing processes. Along the way, we introduce tools that
may prove useful in analyzing other predictive inference methods for dependent
data.

</details>


### [6] [Beyond Linear Diffusions: Improved Representations for Rare Conditional Generative Modeling](https://arxiv.org/abs/2510.02499)
*Kulunu Dharmakeerthi,Yousef El-Laham,Henry H. Wong,Vamsi K. Potluru,Changhong He,Taosong He*

Main category: stat.ML

TL;DR: 提出了一种尾部自适应扩散模型，通过数据驱动的非线性漂移项和适当的数据表示，在条件变量尾部区域显著提高条件分布建模性能


<details>
  <summary>Details</summary>
Motivation: 传统线性扩散模型在处理条件分布P(Y|X=x)时，当P(X=x)较小时面临样本不足的挑战，难以准确建模尾部事件

Method: 基于条件极值理论，采用数据驱动的非线性漂移项和适当的数据表示，使生成模型在条件空间低概率区域具有较小的样本复杂度

Result: 在两个合成数据集和一个真实金融数据集上的实验表明，该方法在极端尾部条件下准确捕捉响应分布方面显著优于标准扩散模型

Conclusion: 尾部自适应扩散模型能够有效解决条件分布建模中的尾部事件挑战，在低概率区域表现出优越性能

Abstract: Diffusion models have emerged as powerful generative frameworks with
widespread applications across machine learning and artificial intelligence
systems. While current research has predominantly focused on linear diffusions,
these approaches can face significant challenges when modeling a conditional
distribution, $P(Y|X=x)$, when $P(X=x)$ is small. In these regions, few
samples, if any, are available for training, thus modeling the corresponding
conditional density may be difficult. Recognizing this, we show it is possible
to adapt the data representation and forward scheme so that the sample
complexity of learning a score-based generative model is small in low
probability regions of the conditioning space. Drawing inspiration from
conditional extreme value theory we characterize this method precisely in the
special case in the tail regions of the conditioning variable, $X$. We show how
diffusion with a data-driven choice of nonlinear drift term is best suited to
model tail events under an appropriate representation of the data. Through
empirical validation on two synthetic datasets and a real-world financial
dataset, we demonstrate that our tail-adaptive approach significantly
outperforms standard diffusion models in accurately capturing response
distributions at the extreme tail conditions.

</details>


### [7] [Adaptive randomized pivoting and volume sampling](https://arxiv.org/abs/2510.02513)
*Ethan N. Epperly*

Main category: stat.ML

TL;DR: 本文重新解释了自适应随机枢轴（ARP）算法，将其与体积采样分布和线性回归的主动学习算法联系起来，提出了新的分析方法和使用拒绝采样的更快实现。


<details>
  <summary>Details</summary>
Motivation: ARP算法是一种高效的列子集选择方法，但需要更好的理论解释和性能改进。

Method: 通过将ARP与体积采样分布和主动学习算法建立联系，重新解释ARP算法，并使用拒绝采样技术实现更快的计算。

Result: 提出了对ARP算法的新分析，并开发了基于拒绝采样的更快速实现版本。

Conclusion: ARP算法可以通过与体积采样和主动学习的联系获得更好的理论理解，并通过拒绝采样显著提高计算效率。

Abstract: Adaptive randomized pivoting (ARP) is a recently proposed and highly
effective algorithm for column subset selection. This paper reinterprets the
ARP algorithm by drawing connections to the volume sampling distribution and
active learning algorithms for linear regression. As consequences, this paper
presents new analysis for the ARP algorithm and faster implementations using
rejection sampling.

</details>


### [8] [Learning Multi-Index Models with Hyper-Kernel Ridge Regression](https://arxiv.org/abs/2510.02532)
*Shuo Huang,Hippolyte Labarrière,Ernesto De Vito,Tomaso Poggio,Lorenzo Rosasco*

Main category: stat.ML

TL;DR: 该论文提出了超核岭回归(HKRR)方法，结合神经网络和核方法的优势，能够自适应学习多索引模型，克服维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在高维问题上表现出色，但其成功理论基础尚不明确。作者认为学习任务的组合结构是决定深度网络优于其他方法的关键因素。

Method: 引入超核岭回归(HKRR)方法，结合神经网络和核方法，并对比研究了交替最小化和交替梯度两种优化方法。

Result: 理论证明HKRR能够自适应学习多索引模型，克服维度灾难；数值结果进一步验证了理论发现。

Conclusion: HKRR方法成功融合了神经网络和核方法的优势，为理解深度网络在高维问题上的成功提供了理论依据。

Abstract: Deep neural networks excel in high-dimensional problems, outperforming models
such as kernel methods, which suffer from the curse of dimensionality. However,
the theoretical foundations of this success remain poorly understood. We follow
the idea that the compositional structure of the learning task is the key
factor determining when deep networks outperform other approaches. Taking a
step towards formalizing this idea, we consider a simple compositional model,
namely the multi-index model (MIM). In this context, we introduce and study
hyper-kernel ridge regression (HKRR), an approach blending neural networks and
kernel methods. Our main contribution is a sample complexity result
demonstrating that HKRR can adaptively learn MIM, overcoming the curse of
dimensionality. Further, we exploit the kernel nature of the estimator to
develop ad hoc optimization approaches. Indeed, we contrast alternating
minimization and alternating gradient methods both theoretically and
numerically. These numerical results complement and reinforce our theoretical
findings.

</details>


### [9] [Neural Jump ODEs as Generative Models](https://arxiv.org/abs/2510.02757)
*Robert A. Crowell,Florian Krach,Josef Teichmann*

Main category: stat.ML

TL;DR: NJODEs作为伊藤过程的生成模型，无需对抗训练，能处理不规则采样和缺失数据，通过学习漂移和扩散系数来生成与真实过程相同分布的样本。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型需要对抗训练和生成样本进行训练，而NJODE框架可以仅作为预测模型在观测样本上训练，无需生成样本，且能自然处理不规则采样和路径依赖动态。

Method: 使用NJODE框架近似伊藤过程的漂移和扩散系数，在标准正则性假设下证明能恢复真实参数，并利用学习到的系数从对应伊藤过程采样。

Result: 在极限情况下，NJODE能恢复真实参数，生成的样本与真实底层过程具有相同的分布规律。

Conclusion: NJODE为伊藤过程提供了一种有效的生成建模方法，特别适用于处理不规则采样、缺失数据和路径依赖动态的现实场景。

Abstract: In this work, we explore how Neural Jump ODEs (NJODEs) can be used as
generative models for It\^o processes. Given (discrete observations of) samples
of a fixed underlying It\^o process, the NJODE framework can be used to
approximate the drift and diffusion coefficients of the process. Under standard
regularity assumptions on the It\^o processes, we prove that, in the limit, we
recover the true parameters with our approximation. Hence, using these learned
coefficients to sample from the corresponding It\^o process generates, in the
limit, samples with the same law as the true underlying process. Compared to
other generative machine learning models, our approach has the advantage that
it does not need adversarial training and can be trained solely as a predictive
model on the observed samples without the need to generate any samples during
training to empirically approximate the distribution. Moreover, the NJODE
framework naturally deals with irregularly sampled data with missing values as
well as with path-dependent dynamics, allowing to apply this approach in
real-world settings. In particular, in the case of path-dependent coefficients
of the It\^o processes, the NJODE learns their optimal approximation given the
past observations and therefore allows generating new paths conditionally on
discrete, irregular, and incomplete past observations in an optimal way.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [10] [Typhoon Path Prediction Using Functional Data Analysis and Clustering-Based Regression](https://arxiv.org/abs/2510.02316)
*Jimin Kim*

Main category: stat.AP

TL;DR: 提出了一种结合功能数据分析和聚类的台风轨迹预测框架，通过将经纬度序列表示为平滑函数观测值，使用函数对函数回归模型捕捉台风移动的时间动态，并通过聚类改进非标准路径的预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的台风轨迹预测对于减轻极端天气事件的影响至关重要，但现有模型对非标准路径的预测精度有限。

Method: 将经纬度序列表示为平滑函数观测值，采用函数对函数回归模型，并引入基于聚类的扩展方法，先按轨迹形状对台风分组再进行回归。

Result: 基准模型对典型弓形轨迹表现合理，但对非标准路径精度有限；两阶段方法通过局部建模适应数据结构变化，提高了预测准确性。

Conclusion: 结合功能数据分析和聚类的方法为台风轨迹预测提供了稳健且灵活的解决方案，具有实际应用价值。

Abstract: Accurate prediction of typhoon trajectories is essential for mitigating the
impact of these extreme weather events. This study proposes a functional data
analysis (FDA) framework for modeling and forecasting typhoon paths using
historical trajectory data. Latitude and longitude sequences are represented as
smooth functional observations, and a function-on-function regression model is
employed to capture the temporal dynamics of typhoon movement. While the
baseline model demonstrates reasonable performance for typical bow-shaped
trajectories, it exhibits limited accuracy for non-standard paths. To address
this limitation, a clustering-based extension is introduced, wherein typhoons
are grouped by trajectory shape prior to regression. This two-stage approach
improves predictive accuracy by enabling localized modeling adapted to
structural variations in the data. The results demonstrate the practical
utility of combining FDA with clustering for robust and flexible typhoon
trajectory forecasting.

</details>


### [11] [Model Falsification for Predicting Dynamical Responses of Uncertain Structural Systems](https://arxiv.org/abs/2510.02612)
*Subhayan De,Tianhao Yu,Patrick T. Brewick,Erik A. Johnson,Steven F. Wojtkiewicz*

Main category: stat.AP

TL;DR: 提出了一种基于错误发现率边界的方法来筛选结构模型，通过排除不满足测量数据精度的模型，仅对未被证伪的模型进行近似贝叶斯响应预测，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 结构系统动力学响应的准确预测依赖于正确的建模，但当存在多个候选模型且参数存在不确定性时，建模变得困难。传统贝叶斯方法需要大量模型模拟，计算成本高。

Method: 使用错误发现率计算的边界来证伪那些无法充分再现测量数据的模型，仅对未被证伪的模型赋予基于似然度的权重，进行近似贝叶斯响应预测。

Result: 在三个结构示例中（4自由度建筑模型、1623自由度3D建筑模型、全尺寸4层隔震建筑），该方法实现了准确的响应预测和显著的计算节省。

Conclusion: 所提出的方法在保持预测精度的同时大幅降低了计算成本，展示了其在结构响应预测中的潜力。

Abstract: Accurate prediction of dynamical response of structural system depends on the
correct modeling of that system. However, modeling becomes increasingly
challenging when there are many candidate models available to describe the
system behavior. Furthermore, uncertainties can be present even for the
parameters of these model classes. The plausibility of each input-output model
class of the structures with uncertain components can be determined by a
Bayesian approach from measured dynamic responses to one or more input records;
predictions of the structural system response to alternate input records can
then be made. However, this approach may require many model simulations, even
though most of those model classes are quite implausible. An approach is
proposed herein to use a bound, computed from the false discovery rate, on the
likelihood of measured data to falsify models considering uncertainties in the
passive control devices that do not reproduce the measured data to sufficient
accuracy. Response prediction is then performed using the unfalsified models in
an approximate Bayesian sense by assigning weights, computed from the
likelihoods, only to the unfalsified models approach incurring only a fraction
of the computational cost of the standard Bayesian approach. The proposed
approach for response prediction is illustrated using three structural
examples: an earthquake-excited four--degree-of-freedom building model with a
hysteretic isolation layer; a 1623--degree-of-freedom three-dimensional
building model, with tuned mass dampers attached to its roof, subjected to wind
loads; and a full-scale four-story base-isolated building tested on world's
largest shake table in Japan's E-Defense lab. The results exhibit accurate
response predictions and significant computational savings, thereby
illustrating the potential of the proposed method.

</details>


### [12] [Deciphering the influence of demographic factors on the treatment of pediatric patients in the emergency department](https://arxiv.org/abs/2510.02841)
*Helena Coggan,Anne Bischops,Pradip Chaudhari,Yuval Barak-Corren,Andrew M. Fine,Ben Y. Reis,Jaya Aysola,William G. La Cava*

Main category: stat.AP

TL;DR: 该研究通过分析33.9万次儿科急诊就诊数据，发现非西班牙裔黑人和西班牙裔患者相比非西班牙裔白人患者在入院率、分诊评分等方面存在显著差异，这些差异在生命体征正常、公共保险等情况下更为明显。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解急诊科治疗中持续存在的人口统计学差异，特别是由潜意识偏见驱动的差异，这些偏见可能连医疗提供者自身都难以识别。

Method: 采用回顾性横断面分析，使用倾向得分匹配计算比值比，并训练机器学习模型识别入院预测因素，调整了主诉、保险类型、社会经济剥夺和合并症等混杂变量。

Result: 发现非西班牙裔黑人相对于非西班牙裔白人的入院比值比为0.77，西班牙裔为0.80；非西班牙裔黑人获得'紧急'分诊评分的可能性更低（OR 0.70），但即使获得相同分诊评分，其入院率仍较低（OR 0.86）。

Conclusion: 许多就诊特征（临床和非临床）可能影响潜意识偏见的运作，并影响机器学习驱动的决策支持工具，特别是在生命体征正常、公共保险等情况下差异更为明显。

Abstract: Persistent demographic disparities have been identified in the treatment of
patients seeking care in the emergency department (ED). These may be driven in
part by subconscious biases, which providers themselves may struggle to
identify. To better understand the operation of these biases, we performed a
retrospective cross-sectional analysis using electronic health records
describing 339,400 visits to the ED of a single US pediatric medical center
between 2019-2024. Odds ratios were calculated using propensity-score matching.
Analyses were adjusted for confounding variables, including chief complaint,
insurance type, socio-economic deprivation, and patient comorbidities. We also
trained a machine learning [ML] model on this dataset to identify predictors of
admission. We found significant demographic disparities in admission
(Non-Hispanic Black [NHB] relative to Non-Hispanic White [NHW]: OR 0.77, 95\%
CI 0.73-0.81; Hispanic relative to NHW: OR 0.80, 95\% CI 0.76-0.83). We also
identified disparities in individual decisions taken during the ED stay. For
example, NHB patients were significantly less likely than NHW patients to be
assigned an `emergent' triage acuity score of (OR 0.70, 95\% CI 0.67-0.72), but
emergent NHB patients were also significantly less likely to be admitted than
NHW patients with the same triage acuity (OR 0.86, 95\% CI 0.80-0.93).
Demographic disparities were particularly acute wherever patients had normal
vital signs, public insurance, moderate socio-economic deprivation, or a home
address distant from the hospital. An ML model assigned higher importance to
triage score for NHB than NHW patients when predicting admission, reflecting
these disparities in assignment. We conclude that many visit characteristics,
clinical and otherwise, may influence the operation of subconscious biases and
affect ML-driven decision support tools.

</details>


### [13] [Data-Driven Bed Occupancy Planning in Intensive Care Units Using $M_t/G_t/\infty$ Queueing Models](https://arxiv.org/abs/2510.02852)
*Maryam Akbari-Moghaddam,Douglas G. Down,Na Li,Catherine Eastwood,Ayman Abou Mehrem,Alexandra Howlett*

Main category: stat.AP

TL;DR: 提出基于时变队列模型的数据驱动框架，用于ICU床位占用率估计和容量规划，克服传统静态启发式方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 医院在ICU长期容量规划中面临需求不确定性的挑战，传统方法基于固定参数假设，无法捕捉真实世界的占用动态。

Method: 使用Mt/Gt/∞队列模型，结合统计分解和参数分布拟合，捕捉ICU入院率的时间变化和住院时长分布。

Result: 案例分析显示静态启发式方法在需求波动环境中不足，建模住院时长变异性对床位需求估计至关重要。

Conclusion: 该方法可推广到其他ICU环境，为医疗系统提供可解释的数据驱动容量规划支持。

Abstract: Hospitals struggle to make effective long-term capacity planning decisions
for intensive care units (ICUs) under uncertainty in future demand. Admission
rates fluctuate over time due to temporal factors, and length of stay (LOS)
distributions vary with patient heterogeneity, hospital location, case mix, and
clinical practices. Common planning approaches rely on steady-state queueing
models or heuristic rules that assume fixed parameters, but these methods often
fall short in capturing real-world occupancy dynamics. One widely used example
is the 85\% occupancy rule, which recommends maintaining average utilization
below this level to ensure responsiveness; however, this rule is based on
stationary assumptions and may be unreliable when applied to time-varying
systems. Our analysis shows that even when long-run utilization targets are
met, day-to-day occupancy frequently exceeds 100\% capacity.
  We propose a data-driven framework for estimating ICU bed occupancy using an
$M_t/G_t/\infty$ queueing model, which incorporates time-varying arrival rates
and empirically estimated LOS distributions. The framework combines statistical
decomposition and parametric distribution fitting to capture temporal patterns
in ICU admissions and LOS. We apply it to multi-year data from neonatal ICUs
(NICUs) in Calgary as a case study. Several capacity planning scenarios are
evaluated, including average-based thresholds and surge estimates from Poisson
overflow approximations. Results demonstrate the inadequacy of static
heuristics in environments with fluctuating demand and highlight the importance
of modeling LOS variability when estimating bed needs. Although the case study
focuses on NICUs, the methodology generalizes to other ICU settings and
provides interpretable, data-informed support for healthcare systems facing
rising demand and limited capacity.

</details>


### [14] [Neural Posterior Estimation with Autoregressive Tiling for Detecting Objects in Astronomical Images](https://arxiv.org/abs/2510.03074)
*Jeffrey Regier*

Main category: stat.AP

TL;DR: 提出了一种使用空间自回归变分分布的天文小目标检测方法，通过K色棋盘格模式划分和排序潜在空间，在斯隆数字巡天数据上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 应对未来天文巡天项目产生的大量高分辨率图像中检测和表征天文物体的挑战，特别是大多数天体微弱且视觉上重叠的问题。

Method: 采用摊销变分推断方法，使用空间自回归变分分布族，通过K色棋盘格模式划分潜在空间，并用卷积神经网络参数化变分分布，通过神经后验估计最小化前向KL散度。

Result: 在斯隆数字巡天图像上实现了最先进的性能，并且提出的自回归结构显著改善了后验校准。

Conclusion: 该方法为天文小目标检测提供了一种有效的解决方案，自回归变分分布在保持后验条件独立性的同时提高了性能和后验校准质量。

Abstract: Upcoming astronomical surveys will produce petabytes of high-resolution
images of the night sky, providing information about billions of stars and
galaxies. Detecting and characterizing the astronomical objects in these images
is a fundamental task in astronomy -- and a challenging one, as most of these
objects are faint and many visually overlap with other objects. We propose an
amortized variational inference procedure to solve this instance of
small-object detection. Our key innovation is a family of spatially
autoregressive variational distributions that partition and order the latent
space according to a $K$-color checkerboard pattern. By construction, the
conditional independencies of this variational family mirror those of the
posterior distribution. We fit the variational distribution, which is
parameterized by a convolutional neural network, using neural posterior
estimation (NPE) to minimize an expectation of the forward KL divergence. Using
images from the Sloan Digital Sky Survey, our method achieves state-of-the-art
performance. We further demonstrate that the proposed autoregressive structure
greatly improves posterior calibration.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [15] [Orthogonal Procrustes problem preserves correlations in synthetic data](https://arxiv.org/abs/2510.02405)
*Oussama Ounissi,Nicklas Jävergård,Adrian Muntean*

Main category: stat.ME

TL;DR: 将正交Procrustes问题应用于生成合成数据，确保合成数据保持特征间的Pearson相关性，作为现有生成模型的轻量级后处理步骤。


<details>
  <summary>Details</summary>
Motivation: 确保合成数据能够准确保持原始数据中特征间的重要统计关系，特别是Pearson相关性，提高合成数据的实用性。

Method: 使用正交Procrustes问题作为后处理方法，对已生成的合成数据集施加精确的Pearson相关性约束。

Result: 在大型真实世界能源消耗数据集上的实证研究表明该方法有效，能够保持特征间的统计关系。

Conclusion: 该方法作为轻量级后处理步骤，能够有效增强合成数据的质量，但不旨在替代现有的生成模型。

Abstract: This work introduces the application of the Orthogonal Procrustes problem to
the generation of synthetic data. The proposed methodology ensures that the
resulting synthetic data preserves important statistical relationships among
features, specifically the Pearson correlation. An empirical illustration using
a large, real-world, tabular dataset of energy consumption demonstrates the
effectiveness of the approach and highlights its potential for application in
practical synthetic data generation. Our approach is not meant to replace
existing generative models, but rather as a lightweight post-processing step
that enforces exact Pearson correlation to an already generated synthetic
dataset.

</details>


### [16] [Bridging the Prediction Error Method and Subspace Identification: A Weighted Null Space Fitting Method](https://arxiv.org/abs/2510.02529)
*Jiabao He,S. Joe Qin,Håkan Hjalmarsson*

Main category: stat.ME

TL;DR: 提出了一种加权零空间拟合方法，结合子空间识别方法和预测误差方法的优点，通过多步最小二乘过程从高阶ARX模型降阶得到状态空间模型，在规范参数化条件下具有一致性和渐近效率。


<details>
  <summary>Details</summary>
Motivation: 子空间识别方法数值稳健但效率不如最大似然估计，而预测误差方法虽然等效于最大似然估计但存在非凸优化问题。需要结合两者优势的新方法。

Method: 从高阶ARX模型的最小二乘估计开始，通过多步最小二乘过程将模型降阶为规范形式的状态空间模型。

Result: 统计分析表明，在规范参数化条件下，该方法具有一致性和渐近效率，解决了子空间识别方法长期存在的渐近效率问题。数值和实际示例显示其性能优于传统子空间识别方法。

Conclusion: 加权零空间拟合方法成功结合了子空间识别和预测误差方法的优点，提供了一种既数值稳健又渐近高效的状态空间模型估计方法。

Abstract: Subspace identification methods (SIMs) have proven to be very useful and
numerically robust for building state-space models. While most SIMs are
consistent, few if any can achieve the efficiency of the maximum likelihood
estimate (MLE). Conversely, the prediction error method (PEM) with a quadratic
criteria is equivalent to MLE, but it comes with non-convex optimization
problems and requires good initialization points. This contribution proposes a
weighted null space fitting (WNSF) approach for estimating state-space models,
combining some key advantages of the two aforementioned mainstream approaches.
It starts with a least-squares estimate of a high-order ARX model, and then a
multi-step least-squares procedure reduces the model to a state-space model on
canoncial form. It is demonstrated through statistical analysis that when a
canonical parameterization is admissible, the proposed method is consistent and
asymptotically efficient, thereby making progress on the long-standing open
problem about the existence of an asymptotically efficient SIM. Numerical and
practical examples are provided to illustrate that the proposed method performs
favorable in comparison with SIMs.

</details>


### [17] [Amortized Bayesian Inference for Spatio-Temporal Extremes: A Copula Factor Model with Autoregression](https://arxiv.org/abs/2510.02618)
*Carlos A. Pasquier,Luis A. Barboza*

Main category: stat.ME

TL;DR: 提出了一种贝叶斯时空极值分析框架，结合层次copula模型和自回归因子来捕捉阈值超限的剩余时间依赖性，并开发了可扩展的Gibbs采样器进行高效推理。


<details>
  <summary>Details</summary>
Motivation: 传统极值分析方法在处理时空依赖性和计算效率方面存在局限，需要开发能够灵活表征地理异质性并提供完整后验不确定性的可扩展方法。

Method: 使用层次copula模型增强自回归因子，将尺度参数与科学相关协变量结合，设计嵌入摊销神经后验估计的Gibbs采样器来避免完整删失似然的计算负担。

Result: 模拟研究表明该方法改进了MCMC混合和估计精度，特别是在使用更复杂网络架构时，同时保持了重尾行为。在哥斯达黎加瓜纳卡斯特的日降水应用中，选定的模型揭示了多年重现期的连贯空间模式。

Conclusion: 所提出的Gibbs方案可推广到其他参数可划分为推断同质块并通过摊销、无似然方法学习条件分布的场景，为基础设施规划和气候风险管理提供可操作信息。

Abstract: We develop a Bayesian spatio-temporal framework for extreme-value analysis
that augments a hierarchical copula model with an autoregressive factor to
capture residual temporal dependence in threshold exceedances. The factor can
be specified as spatially varying or spatially constant, and the scale
parameter incorporates scientifically relevant covariates (e.g., longitude,
latitude, altitude), enabling flexible representation of geographic
heterogeneity. To avoid the computational burden of the full censored
likelihood, we design a Gibbs sampler that embeds amortized neural posterior
estimation within each parameter block, yielding scalable inference with full
posterior uncertainty for parameters, predictive quantiles, and return levels.
Simulation studies demonstrate that the approach improves MCMC mixing and
estimation accuracy relative to baseline specifications, particularly when
using moderately more complex network architectures, while preserving
heavy-tail behavior. We illustrate the methodology with daily precipitation in
Guanacaste, Costa Rica, evaluating a suite of nested models and selecting the
best-performing factor combination via out-of-sample diagnostics. The chosen
specification reveals coherent spatial patterns in multi-year return periods
and provides actionable information for infrastructure planning and
climate-risk management in a tropical dry region strongly influenced by
climatic factors. The proposed Gibbs scheme generalizes to other settings where
parameters can be partitioned into inferentially homogeneous blocks and
conditionals learned via amortized, likelihood-free methods.

</details>


### [18] [What is in the model? A Comparison of variable selection criteria and model search approaches](https://arxiv.org/abs/2510.02628)
*Shuangshuang Xu,Marco A. R. Ferreira,Allison N. Tegge*

Main category: stat.ME

TL;DR: 本文对变量选择方法进行了全面比较，发现穷举搜索BIC和随机搜索BIC在不同模型空间大小下表现最优，具有最高的正确识别率和最低的假发现率。


<details>
  <summary>Details</summary>
Motivation: 理解科学问题的潜在机制需要识别最重要的相关回归变量，变量选择是实现这一目标的关键步骤。

Method: 使用BIC和AIC进行模型评估，采用穷举搜索、贪婪搜索、LASSO路径、随机搜索等方法搜索模型空间，同时考虑交叉验证的LASSO。在线性和广义线性模型中通过模拟研究探索不同样本量、效应大小和变量相关性的情况。

Result: 在小型模型空间中，穷举搜索BIC表现最佳；在大型模型空间中，随机搜索BIC表现最优。这些方法实现了最高的正确识别率和最低的假发现率。

Conclusion: 穷举搜索BIC和随机搜索BIC方法能够显著提高研究的可重复性，是变量选择的推荐方法。

Abstract: For many scientific questions, understanding the underlying mechanism is the
goal. To help investigators better understand the underlying mechanism,
variable selection is a crucial step that permits the identification of the
most associated regression variables of interest. A variable selection method
consists of model evaluation using an information criterion and a search of the
model space. Here, we provide a comprehensive comparison of variable selection
methods using performance measures of correct identification rate (CIR),
recall, and false discovery rate (FDR). We consider the BIC and AIC for
evaluating models, and exhaustive, greedy, LASSO path, and stochastic search
approaches for searching the model space; we also consider LASSO using cross
validation. We perform simulation studies for linear and generalized linear
models that parametrically explore a wide range of realistic sample sizes,
effect sizes, and correlations among regression variables. We consider model
spaces with a small and larger number of potential regressors. The results show
that the exhaustive search BIC and stochastic search BIC outperform the other
methods when considering the performance measures on small and large model
spaces, respectively. These approaches result in the highest CIR and lowest
FDR, which collectively may support long-term efforts towards increasing
replicability in research.

</details>


### [19] [Total Robustness in Bayesian Nonlinear Regression for Measurement Error Problems under Model Misspecification](https://arxiv.org/abs/2510.03131)
*Mengqi Chen,Charita Dellaporta,Thomas B. Berrett,Theodoros Damoulas*

Main category: stat.ME

TL;DR: 提出了首个贝叶斯非参数框架，解决非线性回归中的协变量测量误差、回归模型误设和测量误差分布误设三大挑战，实现完全鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代回归分析常因协变量测量误差、回归模型误设和测量误差分布误设而受到损害，现有方法无法同时解决这三个问题。

Method: 使用狄利克雷过程先验对潜在协变量-响应分布建模，通过后验伪样本更新潜在协变量，最小化狄利克雷过程实现与模型诱导联合分布之间的差异。

Result: 仿真和实际研究显示，与贝叶斯和频率学方法相比，该方法具有更低的估计误差和对误设的敏感性。

Conclusion: 该框架为数据模型不完美时的可信回归提供了一个实用且可解释的范式。

Abstract: Modern regression analyses are often undermined by covariate measurement
error, misspecification of the regression model, and misspecification of the
measurement error distribution. We present, to the best of our knowledge, the
first Bayesian nonparametric framework targeting total robustness that tackles
all three challenges in general nonlinear regression. The framework assigns a
Dirichlet process prior to the latent covariate-response distribution and
updates it with posterior pseudo-samples of the latent covariates, thereby
providing the Dirichlet process posterior with observation-informed latent
inputs and yielding estimators that minimise the discrepancy between Dirichlet
process realisations and the model-induced joint law. This design allows
practitioners to (i) encode prior beliefs, (ii) choose between pseudo-sampling
latent covariates or working directly with error-prone observations, and (iii)
tune the influence of prior and data. We establish generalisation bounds that
tighten whenever the prior or pseudo-sample generator aligns with the
underlying data generating process, ensuring robustness without sacrificing
consistency. A gradient-based algorithm enables efficient computations;
simulations and two real-world studies show lower estimation error and reduced
estimation sensitivity to misspecification compared to Bayesian and frequentist
competitors. The framework, therefore, offers a practical and interpretable
paradigm for trustworthy regression when data and models are jointly imperfect.

</details>


### [20] [TITE-Safety: Time-to-event Safety Monitoring for Clinical Trials](https://arxiv.org/abs/2510.03175)
*Michael J. Martens,Qinghua Lian,Brent R. Logan*

Main category: stat.ME

TL;DR: 提出了TITE-Safety方法用于临床试验安全性监测，将毒性事件作为时间到事件终点处理，相比传统二元方法能提高检测效能并减少信号检测时间。


<details>
  <summary>Details</summary>
Motivation: 现有安全性监测方法通常将毒性事件视为二元结果，而将其作为时间到事件终点处理可以提供更高的检测效能和更快的风险信号检测，但需要处理删失和竞争风险等复杂问题。

Method: 提出TITE-Safety方法，结合时间到事件信息，适当处理删失观察和竞争风险。应用得分检验、贝叶斯beta扩展二项模型和序贯概率比检验来制定停止规则。

Result: 模拟研究表明，与传统二元数据方法相比，所提技术可将预期毒性减少20%或更多，并在各种事件时间分布下将I类错误率维持在名义水平附近。

Conclusion: TITE-Safety方法为临床试验安全性监测提供了更有效的工具，通过R包"stoppingrule"为研究者提供了构建和评估停止规则的功能。

Abstract: Safety evaluation is an essential component of clinical trials. To protect
study participants, these studies often implement safety stopping rules that
will halt the trial if an excessive number of toxicity events occur. Existing
safety monitoring methods often treat these events as binary outcomes. A
strategy that instead handles these as time-to-event endpoints can offer higher
power and a reduced time to signal of excess risk, but must manage additional
complexities including censoring and competing risks. We propose the
TITE-Safety approach for safety monitoring, which incorporates time-to-event
information while handling censored observations and competing risks
appropriately. This strategy is applied to develop stopping rules using score
tests, Bayesian beta-extended binomial models, and sequential probability ratio
tests. The operating characteristics of these methods are studied via
simulation for common phase 2 and 3 trial scenarios. Across simulation
settings, the proposed techniques offer reductions in expected toxicities of
20% or more compared to binary data methods and maintain the type I error rate
near the nominal level across various event time distributions. These methods
are demonstrated through a redesign of the safety monitoring scheme for BMT CTN
0601, a single arm, phase 2 trial that evaluated bone marrow transplant as
treatment for severe sickle cell disease. Our R package "stoppingrule" offers
functions to construct and evaluate these stopping rules, providing valuable
tools for trial design to investigators.

</details>
