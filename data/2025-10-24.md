<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 8]
- [stat.ME](#stat.ME) [Total: 9]
- [stat.OT](#stat.OT) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models](https://arxiv.org/abs/2510.19999)
*Yixiao Wang,Zishan Shao,Ting Jiang,Aditya Devarakonda*

Main category: stat.ML

TL;DR: 提出了一种增强的循环坐标下降（ECCD）框架，用于求解带有弹性网络约束的广义线性模型，相比现有方法减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法在求解广义线性模型时训练时间较长，特别是梯度计算中的非线性操作和向量递归影响了效率。

Method: 通过在当前迭代点进行泰勒展开来避免梯度计算中的非线性操作，展开向量递归并将其重新表述为更高效的批量计算，引入可调参数s控制展开程度。

Result: 实验表明ECCD在正则化路径变体上平均实现了3倍的性能提升，且不影响收敛性。

Conclusion: ECCD框架有效加速了广义线性模型的训练，避免了块坐标下降的收敛延迟和数值不稳定问题，在多个基准数据集上表现优异。

Abstract: We present a novel enhanced cyclic coordinate descent (ECCD) framework for
solving generalized linear models with elastic net constraints that reduces
training time in comparison to existing state-of-the-art methods. We redesign
the CD method by performing a Taylor expansion around the current iterate to
avoid nonlinear operations arising in the gradient computation. By introducing
this approximation, we are able to unroll the vector recurrences occurring in
the CD method and reformulate the resulting computations into more efficient
batched computations. We show empirically that the recurrence can be unrolled
by a tunable integer parameter, $s$, such that $s > 1$ yields performance
improvements without affecting convergence, whereas $s = 1$ yields the original
CD method. A key advantage of ECCD is that it avoids the convergence delay and
numerical instability exhibited by block coordinate descent. Finally, we
implement our proposed method in C++ using Eigen to accelerate linear algebra
computations. Comparison of our method against existing state-of-the-art
solvers shows consistent performance improvements of $3\times$ in average for
regularization path variant on diverse benchmark datasets. Our implementation
is available at https://github.com/Yixiao-Wang-Stats/ECCD.

</details>


### [2] [Compositional Generation for Long-Horizon Coupled PDEs](https://arxiv.org/abs/2510.20141)
*Somayajulu L. N. Dhulipala,Deep Ray,Nicholas Forman*

Main category: stat.ML

TL;DR: 该论文提出了一种组合扩散方法，仅使用解耦PDE数据进行训练，在推理时组合以恢复耦合场，验证了在长时间网格下组合扩散策略的可行性。


<details>
  <summary>Details</summary>
Motivation: 模拟耦合PDE系统计算量大，传统方法需要在联合数据上训练代理模型，需要大量数据。研究是否可以通过仅使用解耦数据进行训练的组合扩散方法实现高效建模。

Method: 使用组合扩散方法，仅在解耦PDE数据上训练扩散模型，在推理时组合恢复耦合场。比较基线扩散模型与v参数化策略，引入基于欧拉方案的对称组合方案。

Result: 尽管只看到解耦训练数据，组合扩散模型能以低误差恢复耦合轨迹。v参数化相比基线扩散模型能提高准确性，但基于耦合数据训练的神经算子代理仍表现最强。

Conclusion: 组合扩散是高效、长时间建模耦合PDE的可行策略，为减少数据需求提供了有前景的方向。

Abstract: Simulating coupled PDE systems is computationally intensive, and prior
efforts have largely focused on training surrogates on the joint (coupled)
data, which requires a large amount of data. In the paper, we study
compositional diffusion approaches where diffusion models are only trained on
the decoupled PDE data and are composed at inference time to recover the
coupled field. Specifically, we investigate whether the compositional strategy
can be feasible under long time horizons involving a large number of time
steps. In addition, we compare a baseline diffusion model with that trained
using the v-parameterization strategy. We also introduce a symmetric
compositional scheme for the coupled fields based on the Euler scheme. We
evaluate on Reaction-Diffusion and modified Burgers with longer time grids, and
benchmark against a Fourier Neural Operator trained on coupled data. Despite
seeing only decoupled training data, the compositional diffusion models recover
coupled trajectories with low error. v-parameterization can improve accuracy
over a baseline diffusion model, while the neural operator surrogate remains
strongest given that it is trained on the coupled data. These results show that
compositional diffusion is a viable strategy towards efficient, long-horizon
modeling of coupled PDEs.

</details>


### [3] [Neural Networks for Censored Expectile Regression Based on Data Augmentation](https://arxiv.org/abs/2510.20344)
*Wei Cao,Shanshan Wang*

Main category: stat.ML

TL;DR: 提出了一种基于数据增强的期望回归神经网络(DAERNN)方法，用于处理包含删失数据的异质性建模问题


<details>
  <summary>Details</summary>
Motivation: 现有的期望回归神经网络主要关注完全观测数据，对包含删失观测的场景关注有限，需要开发能够处理删失数据的灵活方法

Method: 基于数据增强的ERNN算法(DAERNN)，完全数据驱动，假设要求最小，提供统一框架处理各种删失机制

Result: 模拟研究和实际数据应用表明DAERNN优于现有的删失ERNN方法，预测性能接近在完全观测数据上训练的模型

Conclusion: DAERNN为实际删失数据分析提供了灵活且广泛适用的统一框架，无需显式参数模型设定

Abstract: Expectile regression neural networks (ERNNs) are powerful tools for capturing
heterogeneity and complex nonlinear structures in data. However, most existing
research has primarily focused on fully observed data, with limited attention
paid to scenarios involving censored observations. In this paper, we propose a
data augmentation based ERNNs algorithm, termed DAERNN, for modeling
heterogeneous censored data. The proposed DAERNN is fully data driven, requires
minimal assumptions, and offers substantial flexibility. Simulation studies and
real data applications demonstrate that DAERNN outperforms existing censored
ERNNs methods and achieves predictive performance comparable to models trained
on fully observed data. Moreover, the algorithm provides a unified framework
for handling various censoring mechanisms without requiring explicit parametric
model specification, thereby enhancing its applicability to practical censored
data analysis.

</details>


### [4] [Testing Most Influential Sets](https://arxiv.org/abs/2510.20372)
*Lucas Darius Konrad,Nikolas Kuschnig*

Main category: stat.ML

TL;DR: 开发了一个用于评估最具影响力数据集统计显著性的理论框架，能够检测数据影响是否超出自然采样变异范围。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏正式理论来判断数据子集的影响力是否反映真实问题而非自然采样变异，现有方法多为临时敏感性检查。

Method: 建立了最大影响力极值分布的理论特征，开发了用于检测过度影响力的严格假设检验框架。

Result: 理论结果能够表征最大影响力的极值分布，并在经济学、生物学和机器学习基准测试中展示了实际价值。

Conclusion: 提出的原则性框架能够替代当前的临时敏感性检查，为最具影响力数据集的统计显著性评估提供理论基础。

Abstract: Small subsets of data with disproportionate influence on model outcomes can
have dramatic impacts on conclusions, with a few data points sometimes
overturning key findings. While recent work has developed methods to identify
these \emph{most influential sets}, no formal theory exists to determine when
their influence reflects genuine problems rather than natural sampling
variation. We address this gap by developing a principled framework for
assessing the statistical significance of most influential sets. Our
theoretical results characterize the extreme value distributions of maximal
influence and enable rigorous hypothesis tests for excessive influence,
replacing current ad-hoc sensitivity checks. We demonstrate the practical value
of our approach through applications across economics, biology, and machine
learning benchmarks.

</details>


### [5] [Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks](https://arxiv.org/abs/2510.20436)
*Federico Lozano-Cuadra,Beatriz Soret,Marc Sanchez Net,Abhishek Cauligi,Federico Rossi*

Main category: stat.ML

TL;DR: 提出了一种基于图注意力多智能体强化学习的完全去中心化路由框架，用于月球延迟容忍网络中的多机器人探索任务，在间歇性连接和未知移动模式下实现高效数据传输。


<details>
  <summary>Details</summary>
Motivation: 解决月球探索任务中自主漫游车在间歇性连接和未知移动模式下向着陆器中继数据的挑战，传统方法如最短路径和受控洪泛算法需要全局拓扑更新或数据包复制，不适合这种分布式环境。

Method: 将问题建模为部分可观测马尔可夫决策过程，提出基于图注意力的多智能体强化学习策略，采用集中训练分散执行的方式，仅依赖局部观测，无需全局拓扑更新或数据包复制。

Result: 通过蒙特卡洛模拟在随机探索环境中验证，GAT-MARL方法提供了更高的交付率、无重复数据包、更少的数据包丢失，并能利用短期移动预测，成功推广到更大的漫游车团队。

Conclusion: 该方法为未来空间机器人系统的行星探索提供了一个可扩展的解决方案，在完全去中心化的框架下实现了高效的数据路由。

Abstract: We present a fully decentralized routing framework for multi-robot
exploration missions operating under the constraints of a Lunar Delay-Tolerant
Network (LDTN). In this setting, autonomous rovers must relay collected data to
a lander under intermittent connectivity and unknown mobility patterns. We
formulate the problem as a Partially Observable Markov Decision Problem (POMDP)
and propose a Graph Attention-based Multi-Agent Reinforcement Learning
(GAT-MARL) policy that performs Centralized Training, Decentralized Execution
(CTDE). Our method relies only on local observations and does not require
global topology updates or packet replication, unlike classical approaches such
as shortest path and controlled flooding-based algorithms. Through Monte Carlo
simulations in randomized exploration environments, GAT-MARL provides higher
delivery rates, no duplications, and fewer packet losses, and is able to
leverage short-term mobility forecasts; offering a scalable solution for future
space robotic systems for planetary exploration, as demonstrated by successful
generalization to larger rover teams.

</details>


### [6] [Concentration and excess risk bounds for imbalanced classification with synthetic oversampling](https://arxiv.org/abs/2510.20472)
*Touqeer Ahmad,Mohammadreza M. Kalan,François Portier,Gilles Stupfler*

Main category: stat.ML

TL;DR: 本文为SMOTE及其变种方法建立了理论框架，分析了在合成数据上训练分类器的行为，提供了经验风险与总体风险之间差异的浓度界限，并给出了核分类器的超额风险保证。


<details>
  <summary>Details</summary>
Motivation: 尽管SMOTE及其变种方法在实践中成功处理不平衡分类问题，但其理论基础仍然研究不足，需要建立理论框架来分析这些方法的行为。

Method: 开发理论框架分析SMOTE方法，推导合成少数样本经验风险与真实少数分布总体风险之间差异的均匀浓度界限，提供核分类器在合成数据上训练的非参数超额风险保证。

Result: 获得了理论结果，为SMOTE和下游学习算法的参数调优提供了实用指导，并通过数值实验验证了理论发现。

Conclusion: 建立了SMOTE方法的理论基础，为实践中的参数调优提供了理论指导，并通过实验验证了理论结果的实用性。

Abstract: Synthetic oversampling of minority examples using SMOTE and its variants is a
leading strategy for addressing imbalanced classification problems. Despite the
success of this approach in practice, its theoretical foundations remain
underexplored. We develop a theoretical framework to analyze the behavior of
SMOTE and related methods when classifiers are trained on synthetic data. We
first derive a uniform concentration bound on the discrepancy between the
empirical risk over synthetic minority samples and the population risk on the
true minority distribution. We then provide a nonparametric excess risk
guarantee for kernel-based classifiers trained using such synthetic data. These
results lead to practical guidelines for better parameter tuning of both SMOTE
and the downstream learning algorithm. Numerical experiments are provided to
illustrate and support the theoretical findings

</details>


### [7] [Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences](https://arxiv.org/abs/2510.20595)
*Yunyi Shen,Alexander Gagliano*

Main category: stat.ML

TL;DR: 提出DAEP（扩散自编码器与感知器）架构，用于处理不规则、多模态序列数据的表示学习，在科学领域数据上优于传统VAE和MAEP基线模型。


<details>
  <summary>Details</summary>
Motivation: 自监督学习主要针对图像、音频等规则采样数据，但科学领域数据多为不规则、多模态序列，需要专门架构来处理这种异构测量数据。

Method: 使用感知器编码器对异构测量进行标记化和压缩，通过感知器-IO扩散解码器进行重构，实现可扩展学习。

Result: 在多种光谱和光度天文数据集上，DAEP获得更低重构误差、更判别性的潜在空间，并更好保留精细结构。

Conclusion: DAEP是处理不规则、异构序列数据的有效框架，特别适用于科学领域。

Abstract: Self-supervised learning has become a central strategy for representation
learning, but the majority of architectures used for encoding data have only
been validated on regularly-sampled inputs such as images, audios. and videos.
In many scientific domains, data instead arrive as long, irregular, and
multimodal sequences. To extract semantic information from these data, we
introduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes
heterogeneous measurements, compresses them with a Perceiver encoder, and
reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable
learning in diverse data settings. To benchmark the daep architecture, we adapt
the masked autoencoder to a Perceiver encoder/decoder design, and establish a
strong baseline (maep) in the same architectural family as daep. Across diverse
spectroscopic and photometric astronomical datasets, daep achieves lower
reconstruction errors, produces more discriminative latent spaces, and better
preserves fine-scale structure than both VAE and maep baselines. These results
establish daep as an effective framework for scientific domains where data
arrives as irregular, heterogeneous sequences.

</details>


### [8] [Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection](https://arxiv.org/abs/2510.20653)
*Jack Butler,Nikita Kozodoi,Zainab Afolabi,Brian Tyacke,Gaiar Baimuratov*

Main category: stat.ML

TL;DR: 本文系统比较了自反思和预算调优在数学推理和翻译任务中的表现，发现自反思在不同领域效果差异显著，在数学推理中性能提升可达220%。研究为特定领域和资源约束下选择最优推理策略提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，从业者面临更多无需重新训练模型即可提升推理性能的方法选择，但这些方法在准确性、成本和延迟之间产生了复杂的权衡关系，且在不同领域中的表现尚未得到充分理解。

Method: 在数学推理和翻译任务上系统比较自反思和预算调优方法，评估包括Anthropic Claude、Amazon Nova和Mistral等主流LLM家族，在不同反思深度和计算预算下推导帕累托最优性能边界。

Result: 自反思效果存在显著的领域依赖性，在数学推理任务中性能提升高达220%。反思轮次深度和反馈机制质量对不同模型家族的性能影响各异。在实际部署中，自反思增强的营销内容本地化系统显示出市场依赖的有效性。

Conclusion: 研究强调了在部署这些技术时进行领域特定评估的重要性，并为给定特定领域和资源约束条件下选择最优推理策略提供了可操作的指导。开源了自反思实现以确保可复现性。

Abstract: As Large Language Models (LLMs) continue to evolve, practitioners face
increasing options for enhancing inference-time performance without model
retraining, including budget tuning and multi-step techniques like
self-reflection. While these methods improve output quality, they create
complex trade-offs among accuracy, cost, and latency that remain poorly
understood across different domains. This paper systematically compares
self-reflection and budget tuning across mathematical reasoning and translation
tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and
Mistral families, along with other models under varying reflection depths and
compute budgets to derive Pareto optimal performance frontiers. Our analysis
reveals substantial domain dependent variation in self-reflection
effectiveness, with performance gains up to 220\% in mathematical reasoning. We
further investigate how reflection round depth and feedback mechanism quality
influence performance across model families. To validate our findings in a
real-world setting, we deploy a self-reflection enhanced marketing content
localisation system at Lounge by Zalando, where it shows market-dependent
effectiveness, reinforcing the importance of domain specific evaluation when
deploying these techniques. Our results provide actionable guidance for
selecting optimal inference strategies given specific domains and resource
constraints. We open source our self-reflection implementation for
reproducibility at
https://github.com/aws-samples/sample-genai-reflection-for-bedrock.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [9] [Unifying Boxplots: A Multiple Testing Perspective](https://arxiv.org/abs/2510.20259)
*Bowen Gang,Hongmei Lin,Tiejun Tong*

Main category: stat.ME

TL;DR: 将Tukey箱线图重新解释为多重检验程序的图形实现，提出统一框架将经典方法和现代变体系统化，并引入基于FDR的新箱线图构造方法。


<details>
  <summary>Details</summary>
Motivation: 经典箱线图的异常值检测规则未考虑样本量，后续改进方法多为启发式调整且缺乏统一框架。

Method: 建立多重检验框架，将箱线图视为图形化多重检验程序，Tukey方法对应未调整检验，现有改进对应FWER或PFER控制，并引入FDR控制的新箱线图。

Result: 系统化了现有箱线图方法，提供了比较和扩展异常值检测规则的原理性语言，并展示了如何将现代稳健估计技术整合到箱线图格式中。

Conclusion: 通过将经典图形工具与多重检验原则连接，为现代探索性分析提供了系统化的异常值检测框架。

Abstract: Tukey's boxplot is a foundational tool for exploratory data analysis, but its
classic outlier-flagging rule does not account for the sample size, and
subsequent modifications have often been presented as separate, heuristic
adjustments. In this paper, we propose a unifying framework that recasts the
boxplot and its variants as graphical implementations of multiple testing
procedures. We demonstrate that Tukey's original method is equivalent to an
unadjusted procedure, while existing sample-size-aware modifications correspond
to controlling the Family-Wise Error Rate (FWER) or the Per-Family Error Rate
(PFER). This perspective not only systematizes existing methods but also
naturally leads to new, more adaptive constructions. We introduce a boxplot
motivated by the False Discovery Rate (FDR), and show how our framework
provides a flexible pipeline for integrating state-of-the-art robust estimation
techniques directly into the boxplot's graphical format. By connecting a
classic graphical tool to the principles of multiple testing, our work provides
a principled language for comparing, critiquing, and extending outlier
detection rules for modern exploratory analysis.

</details>


### [10] [Kernel Density Estimation and Convolution Revisited](https://arxiv.org/abs/2510.19960)
*Nicholas Tenkorang,Kwesi Appau Ohene-Obeng,Xiaogang Su*

Main category: stat.ME

TL;DR: SHIDE是一种基于卷积框架的新型密度估计方法，通过向观测值添加有界噪声生成伪数据，并使用样条插值对生成的直方图进行估计，解决了传统核密度估计的带宽选择、边界偏差和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统核密度估计对带宽选择敏感，存在边界偏差和计算效率低的问题。本研究旨在通过卷积框架重新审视KDE，开发一种理论严谨且实用的替代方法。

Method: 提出SHIDE方法：1）通过向观测值添加有界多项式核密度噪声生成伪数据；2）对生成的直方图应用样条插值；3）噪声来自均匀分布卷积构造的有界多项式核密度类；4）开发了两种数据驱动的带宽选择方法。

Result: SHIDE在广泛模型范围内表现与KDE相当或更优，特别在边界分布和重尾分布中具有优势。理论分析表明SHIDE达到经典n^{-4/5}收敛率，同时缓解边界偏差。

Conclusion: SHIDE是传统核密度估计的理论严谨且实际稳健的替代方法，在保持经典收敛率的同时解决了边界偏差问题，计算效率更高。

Abstract: Kernel Density Estimation (KDE) is a cornerstone of nonparametric statistics,
yet it remains sensitive to bandwidth choice, boundary bias, and computational
inefficiency. This study revisits KDE through a principled convolutional
framework, providing an intuitive model-based derivation that naturally extends
to constrained domains, such as positive-valued random variables. Building on
this perspective, we introduce SHIDE (Simulation and Histogram Interpolation
for Density Estimation), a novel and computationally efficient density
estimator that generates pseudo-data by adding bounded noise to observations
and applies spline interpolation to the resulting histogram. The noise is
sampled from a class of bounded polynomial kernel densities, constructed
through convolutions of uniform distributions, with a natural bandwidth
parameter defined by the kernel's support bound. We establish the theoretical
properties of SHIDE, including pointwise consistency, bias-variance
decomposition, and asymptotic MISE, showing that SHIDE attains the classical
$n^{-4/5}$ convergence rate while mitigating boundary bias. Two data-driven
bandwidth selection methods are developed, an AMISE-optimal rule and a
percentile-based alternative, which are shown to be asymptotically equivalent.
Extensive simulations demonstrate that SHIDE performs comparably to or
surpasses KDE across a broad range of models, with particular advantages for
bounded and heavy-tailed distributions. These results highlight SHIDE as a
theoretically grounded and practically robust alternative to traditional KDE.

</details>


### [11] [Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses](https://arxiv.org/abs/2510.20147)
*Qingyang Liu,Sanvesh Srivastava,Dipankar Bandyopadhyay*

Main category: stat.ME

TL;DR: 提出了REGMVST回归模型，用于分析具有偏斜、对称或重尾特征的不规则纵向数据。该模型使用矩阵变量偏斜t分布处理偏斜和重尾，采用阻尼指数相关结构处理不规则时间剖面，并通过异步分布式ECME算法实现高效参数估计。


<details>
  <summary>Details</summary>
Motivation: 处理不规则纵向数据中的偏斜性、对称性和重尾特征，这些特征在传统方法中往往难以有效建模。

Method: 使用矩阵变量偏斜t分布建模响应变量，采用阻尼指数相关结构处理行间依赖关系，列协方差保持非结构化。开发ECME算法进行参数估计，并进一步扩展为异步分布式ECME算法以提升计算效率。

Result: 在合成数据和牙周病案例研究中，ADECME算法在效率和收敛性方面优于替代方法，证明了其优越性能。

Conclusion: REGMVST模型和ADECME算法为分析具有复杂分布特征的不规则纵向数据提供了有效的解决方案，具有实际应用价值。

Abstract: We propose a regression model with matrix-variate skew-t response (REGMVST)
for analyzing irregular longitudinal data with skewness, symmetry, or heavy
tails. REGMVST models matrix-variate responses and predictors, with rows
indexing longitudinal measurements per subject. It uses the matrix-variate
skew-t (MVST) distribution to handle skewness and heavy tails, a damped
exponential correlation (DEC) structure for row-wise dependencies across
irregular time profiles, and leaves the column covariance unstructured. For
estimation, we initially develop an ECME algorithm for parameter estimation and
further mitigate its computational bottleneck via an asynchronous and
distributed ECME (ADECME) extension. ADECME accelerates the E-step through
parallelization, and retains the simplicity of the conditional M-step, enabling
scalable inference. Simulations using synthetic data and a case study exploring
matrix-variate periodontal disease endpoints derived from electronic health
records demonstrate ADECME's superiority in efficiency and convergence, over
the alternatives. We also provide theoretical support for our empirical
observations and identify regularity assumptions for ADECME's optimal
performance. An accompanying R package is available at
https://github.com/rh8liuqy/STMATREG.

</details>


### [12] [Identification and Debiased Learning of Causal Effects with General Instrumental Variables](https://arxiv.org/abs/2510.20404)
*Shuyuan Chen,Peng Zhang,Yifan Cui*

Main category: stat.ME

TL;DR: 提出了一个非参数框架，通过加性工具变量方法识别平均潜在结果和平均处理效应，利用半参数理论构建高效估计器，并扩展到纵向数据和动态处理机制。


<details>
  <summary>Details</summary>
Motivation: 当处理分配受到未观测变量混淆时，工具变量方法是因果推断的基础，需要发展适用于多分类或连续工具变量的通用非参数框架。

Method: 开发加性工具变量框架，使用加权函数识别平均潜在结果和平均处理效应；基于半参数理论推导高效影响函数，通过去偏机器学习构建一致且渐近正态的估计器。

Result: 通过模拟研究和实际数据分析（如职业培训合作法案项目）验证了所提方法的有效性。

Conclusion: 提出的加性工具变量框架为因果推断提供了有效的非参数识别和估计方法，能够处理多种类型的工具变量并扩展到复杂场景。

Abstract: Instrumental variable methods are fundamental to causal inference when
treatment assignment is confounded by unobserved variables. In this article, we
develop a general nonparametric framework for identification and learning with
multi-categorical or continuous instrumental variables. Specifically, we
propose an additive instrumental variable framework to identify mean potential
outcomes and the average treatment effect with a weighting function. Leveraging
semiparametric theory, we derive efficient influence functions and construct
consistent, asymptotically normal estimators via debiased machine learning.
Extensions to longitudinal data, dynamic treatment regimes, and multiplicative
instrumental variables are further developed. We demonstrate the proposed
method by employing simulation studies and analyzing real data from the Job
Training Partnership Act program.

</details>


### [13] [On Multiple Robustness of Proximal Dynamic Treatment Regimes](https://arxiv.org/abs/2510.20451)
*Yuanshan Gao,Yang Bai,Yifan Cui*

Main category: stat.ME

TL;DR: 提出了在未观测混杂因素存在下学习最优动态治疗策略的因果推断方法，包括三种非参数识别方法、半参数效率界、多阶段稳健估计方法，以及静态策略下反事实均值的识别和估计。


<details>
  <summary>Details</summary>
Motivation: 动态治疗策略在精准医学等领域有重要应用，但通过随机试验估计最优策略面临成本和伦理障碍，需要使用观察性数据。当无混杂性假设不成立时，需要新的因果推断方法。

Method: 采用近端因果推断框架，提出三种非参数识别方法、建立值函数的半参数效率界、开发(K+1)-稳健方法学习最优动态治疗策略，以及静态策略下反事实均值的识别和估计。

Result: 数值实验验证了所提方法的效率和多重稳健性。

Conclusion: 该工作为在未观测混杂因素存在下学习最优动态治疗策略提供了有效的因果推断框架和方法，具有重要的理论和应用价值。

Abstract: Dynamic treatment regimes are sequential decision rules that adapt treatment
according to individual time-varying characteristics and outcomes to achieve
optimal effects, with applications in precision medicine, personalized
recommendations, and dynamic marketing. Estimating optimal dynamic treatment
regimes via sequential randomized trials might face costly and ethical hurdles,
often necessitating the use of historical observational data. In this work, we
utilize proximal causal inference framework for learning optimal dynamic
treatment regimes when the unconfoundedness assumption fails. Our contributions
are four-fold: (i) we propose three nonparametric identification methods for
optimal dynamic treatment regimes; (ii) we establish the semiparametric
efficiency bound for the value function of a given regime; (iii) we propose a
(K+1)-robust method for learning optimal dynamic treatment regimes, where K is
the number of stages; (iv) as a by-product for marginal structural models, we
establish identification and estimation of counterfactual means under a static
regime. Numerical experiments validate the efficiency and multiple robustness
of our proposed methods.

</details>


### [14] [Throwing Vines at the Wall: Structure Learning via Random Search](https://arxiv.org/abs/2510.20035)
*Thibault Vatter,Thomas Nagler*

Main category: stat.ME

TL;DR: 提出了改进藤状copula结构学习的随机搜索算法和基于模型置信集的统计框架，在多个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 藤状copula在机器学习中广泛应用，但结构学习仍是关键挑战。现有的启发式方法如Dissmann贪心算法常次优，需要更有效的结构选择方法。

Method: 开发了改进结构选择的随机搜索算法，并建立了基于模型置信集的统计框架，为集成学习提供理论基础。

Result: 在多个真实数据集上的实证结果表明，该方法持续优于最先进的方法。

Conclusion: 提出的随机搜索算法和统计框架显著提升了藤状copula结构学习的性能，为依赖建模提供了更可靠的工具。

Abstract: Vine copulas offer flexible multivariate dependence modeling and have become
widely used in machine learning, yet structure learning remains a key
challenge. Early heuristics like the greedy algorithm of Dissmann are still
considered the gold standard, but often suboptimal. We propose random search
algorithms that improve structure selection and a statistical framework based
on model confidence sets, which provides theoretical guarantees on selection
probabilities and a powerful foundation for ensembling. Empirical results on
several real-world data sets show that our methods consistently outperform
state-of-the-art approaches.

</details>


### [15] [Clustering of multivariate tail dependence using conditional methods](https://arxiv.org/abs/2510.20424)
*Patrick O'Toole,Christian Rohrbeck,Jordan Richards*

Main category: stat.ME

TL;DR: 提出了一种基于条件极值框架的多变量极值聚类方法，使用偏斜几何Jensen-Shannon散度作为距离度量，能够识别具有相似尾部依赖结构的随机向量组。


<details>
  <summary>Details</summary>
Motivation: 条件极值框架在分析随机向量联合尾部行为时很有用，但在多个位置或变量应用时，高维向量的极值依赖结构难以解释和比较。

Method: 基于偏斜几何Jensen-Shannon散度构建闭式、计算高效的多元尾部距离度量，应用标准聚类算法对成对距离矩阵进行聚类。

Result: 模拟研究表明该方法在二元极值聚类中优于现有方法，并能唯一扩展到多元设置；在爱尔兰气象数据应用中识别出具有相似极端降水与风速依赖结构的空间一致区域。

Conclusion: 该方法提供了一种可解释的多变量极值聚类框架，能够有效识别具有同质尾部依赖的随机向量组，适用于高维数据分析。

Abstract: The conditional extremes (CE) framework has proven useful for analysing the
joint tail behaviour of random vectors. However, when applied across many
locations or variables, it can be difficult to interpret or compare the
resulting extremal dependence structures, particularly for high dimensional
vectors. To address this, we propose a novel clustering method for multivariate
extremes using the CE framework. Our approach introduces a closed-form,
computationally efficient dissimilarity measure for multivariate tails, based
on the skew-geometric Jensen-Shannon divergence, and is applicable in arbitrary
dimensions. Applying standard clustering algorithms to a matrix of pairwise
distances, we obtain interpretable groups of random vectors with homogeneous
tail dependence. Simulation studies demonstrate that our method outperforms
existing approaches for clustering bivariate extremes, and uniquely extends to
the multivariate setting. In our application to Irish meteorological data, our
clustering identifies spatially coherent regions with similar extremal
dependence between precipitation and wind speeds.

</details>


### [16] [Bias-Variance Tradeoff of Matching Prior to Difference-in-Differences When Parallel Trends is Violated](https://arxiv.org/abs/2510.20191)
*Mingxuan Ge,Dae Woong Ham*

Main category: stat.ME

TL;DR: 本文分析了在双重差分法(DiD)前进行匹配的偏差-方差权衡，发现匹配观测协变量并不总是优于经典DiD，但匹配预处理结果总是有益的，建议使用均方误差作为最终指标并提供实践指南。


<details>
  <summary>Details</summary>
Motivation: 补充现有研究仅关注偏差的不足，分析匹配-DiD方法的完整偏差-方差权衡，为实证运营管理中的因果推断提供更全面的指导。

Method: 在线性结构模型下，分析匹配观测协变量和预处理结果对DiD估计的偏差和方差影响，使用均方误差作为评估指标。

Result: 匹配观测协变量存在样本量权衡，不总是优于经典DiD；匹配预处理结果总是有益的；在知识共享平台案例中，匹配-DiD方法足够稳健。

Conclusion: 建议使用均方误差作为最终评估指标，匹配预处理结果总是有益的，匹配观测协变量需谨慎考虑样本量权衡，匹配-DiD方法能有效解决平行趋势假设违反问题。

Abstract: Quasi-experimental causal inference methods have become central in empirical
operations management (OM) for guiding managerial decisions. Among these,
empiricists utilize the Difference-in-Differences (DiD) estimator, which relies
on the parallel trends assumption. To improve its plausibility, researchers
often match treated and control units before applying DiD, with the intuition
that matched groups are more likely to evolve similarly absent treatment.
Existing work that analyze this practice, however, has focused solely on bias.
We complement and fill an important gap by analyzing the full bias-variance
tradeoff. Under a linear structural model with unobserved time-varying
confounders, we show that variance results contrast with established bias
insights: matching on observed covariates prior to DiD is not always
recommended over the classic (unmatched) DiD due to a sample size tradeoff;
furthermore, matching additionally on pre-treatment outcomes is always
beneficial as such tradeoff no longer exists once matching is performed. We
therefore advocate mean squared error (MSE) as a final metric and give
practitioner-friendly guidelines with theoretical guarantees on when (and on
what variables) they should match on. We apply these insights to a recent study
on how the introduction of monetary incentives by a knowledge-sharing platform
affects its general engagement and show that the authors' matching choice prior
to DiD was both warranted and critical. In particular, we provide new
managerial insights that after a full bias correction, their estimated effect
with matching still remains statistically significant, demonstrating that the
chosen matching-DiD approach is sufficiently robust to address managerial
concerns over violations of parallel trends.

</details>


### [17] [A comparison of methods for designing hybrid type 2 cluster-randomized trials with continuous effectiveness and implementation endpoints](https://arxiv.org/abs/2510.20741)
*Melody Owen,Fan Li,Ruyi Liu,Donna Spiegelman*

Main category: stat.ME

TL;DR: 比较了混合类型2研究中五种设计方法的统计功效，发现p值调整方法总是比组合结果方法和单加权1-DF检验功效低，并确定了不同条件下各方法的相对优势。


<details>
  <summary>Details</summary>
Motivation: 混合类型2研究越来越受欢迎，但缺乏对不同设计方法功效比较的系统分析，需要为研究者提供方法选择的指导。

Method: 通过理论比较功率方程和数值模拟（使用crt2power R包在30,000个输入场景下），系统比较了五种设计方法：p值调整、组合结果、单加权1-DF检验、析取2-DF检验和合取检验。

Result: p值调整方法总是比其他方法功效低；当处理效应不等时，析取2-DF检验功效更高；当处理效应相同时，单加权1-DF检验功效更高。

Conclusion: 为混合类型2研究的方法选择提供了清晰指导：根据处理效应是否相等来选择最优方法，开发了实用的R包工具支持功率计算。

Abstract: Hybrid type 2 studies are gaining popularity for their ability to assess both
implementation and health outcomes as co-primary endpoints. Often conducted as
cluster-randomized trials (CRTs), five design methods can validly power these
studies: p-value adjustment methods, combined outcomes approach, single
weighted 1-DF test, disjunctive 2-DF test, and conjunctive test. We compared
all of the methods theoretically and numerically. Theoretical comparisons of
the power equations allowed us to identify if any method globally had more or
less power than other methods. It was shown that the p-value adjustment methods
are always less powerful than the combined outcomes approach and the single
1-DF test. We also identified the conditions under which the disjunctive 2-DF
test is less powerful than the single 1-DF test. Because our theoretical
comparison showed that some methods could be more powerful than others under
certain conditions, and less powerful under others, we conducted a numerical
study to understand these differences. The crt2power R package was created to
calculate the power or sample size for CRTs with two continuous co-primary
endpoints. Using this package, we conducted a numerical evaluation across
30,000 input scenarios to compare statistical power. Specific patterns were
identified where a certain method consistently achieved the highest power. When
the treatment effects are unequal, the disjunctive 2-DF test tends to have
higher power. When the treatment effect sizes are the same, the single 1-DF
test tends to have higher power. Together, these comparisons provide clearer
insights to guide method selection for powering hybrid type 2 studies.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [18] [Change, dependence, and discovery: Celebrating the work of T.L. Lai](https://arxiv.org/abs/2510.20023)
*Alexander G. Tartakovskya,Jay Bartroff,Cheng-Der Fuh,Haipeng Xing*

Main category: stat.OT

TL;DR: 回顾Tze Leung Lai在序贯分析领域的开创性贡献，包括序贯假设检验、变点检测和非线性更新理论


<details>
  <summary>Details</summary>
Motivation: 总结和评述Lai在序贯分析领域的重要研究成果，将其置于更广泛的序贯分析背景中

Method: 文献综述方法，系统梳理Lai在序贯概率比检验、变点检测和生物统计应用等方面的核心工作

Result: Lai建立了序贯概率比检验及其扩展的基本最优性结果，为复合假设检验提供了通用框架，并在变点检测中引入了新的最优性准则和计算高效方法

Conclusion: Lai的工作在序贯分析领域具有深远影响，其建立的框架和方法至今仍具有重要意义

Abstract: Tze Leung Lai made seminal contributions to sequential analysis, particularly
in sequential hypothesis testing, changepoint detection and nonlinear renewal
theory. His work established fundamental optimality results for the sequential
probability ratio test and its extensions, and provided a general framework for
testing composite hypotheses. In changepoint detection, he introduced new
optimality criteria and computationally efficient procedures that remain
influential. He applied these and related tools to problems in biostatistics.
In this article, we review these key results in the broader context of
sequential analysis.

</details>


### [19] [Factors Associated with Unit-Specific Failure in a University-Level Statistics Course](https://arxiv.org/abs/2510.20100)
*Biviana Marcela Suarez Sierra*

Main category: stat.OT

TL;DR: 该研究分析了哥伦比亚一所私立大学统计学课程四个主题单元的失败因素，发现非工程专业学生在概念密集型单元（如统计推断和线性回归）中处于结构性劣势，建议针对性的教学干预。


<details>
  <summary>Details</summary>
Motivation: 传统分析将统计学课程表现视为单一结果，但本研究旨在通过按单元（探索性数据分析、概率与随机变量、统计推断、线性回归）分解结果，揭示不同内容领域的独特挑战。

Method: 基于186名工程、地质和交互设计专业本科生的样本，结合考试成绩和自我感知准备度调查，开发了单元特定的逻辑回归模型。

Result: 研究发现非工程专业学生在概念密集型单元（如推断和回归）中持续处于结构性劣势，学术阶段和能力感知也是重要预测因素，但其影响因单元而异。

Conclusion: 这种分解方法提供了对统计学教育中学术脆弱性的更细致理解，支持设计基于证据、情境敏感的策略来减少失败并改善学习成果。

Abstract: This study investigates the factors associated with failure in each of the
four thematic units of a General Statistics course offered at a private
university in Colombia. Unlike traditional analyses that treat performance as a
single outcome, this research disaggregates results by unit: Exploratory Data
Analysis, Probability and Random Variables, Statistical Inference, and Linear
Regression -- highlighting distinct challenges across content areas. Based on a
sample of 186 undergraduate students from Engineering, Geology, and Interactive
Design programs, the study combines exam performance data with self-perceived
preparedness surveys to develop unit-specific logistic regression models. The
findings reveal consistent structural disadvantages for students from
non-engineering programs, especially in concept-heavy units such as Inference
and Regression. Academic stage and perception of competence also emerged as
important predictors, though their effects varied across units. The results
align with prior research on statistical thinking and self-efficacy, and
support the need for targeted pedagogical interventions and curricular
alignment. This disaggregated approach offers a more nuanced understanding of
academic vulnerability in statistics education and contributes to the design of
evidence-based, context-sensitive strategies to reduce failure and improve
learning outcomes.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [20] [Approximating evidence via bounded harmonic means](https://arxiv.org/abs/2510.20617)
*Dana Naderi,Christian P Robert,Kaniav Kamary,Darren Wraith§*

Main category: stat.CO

TL;DR: 提出了一种新的椭圆覆盖方法（ECMLE）来解决传统调和均值估计器（HME）的无限方差问题，该方法通过非重叠椭球体覆盖高后验密度区域，在多模态设置中表现优异。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯模型选择依赖于模型证据或边际似然，但计算涉及难以处理的积分。传统调和均值估计器虽然计算简单，但存在无限方差问题，需要改进。

Method: 基于Gelfand和Dey的标准化表示方法，使用高后验密度指标作为工具函数，提出椭圆覆盖方法，通过非重叠椭球体覆盖HPD区域。

Result: ECMLE不仅消除了原始HME的无限方差问题，允许精确体积计算，还能用于多模态设置。在多个示例中，ECMLE优于THAMES和Mixture THAMES等最新方法，具有更低的方差和更稳定的证据近似。

Conclusion: ECMLE方法有效解决了传统调和均值估计器的关键问题，在具有挑战性的设置中提供了更可靠的模型证据估计。

Abstract: Efficient Bayesian model selection relies on the model evidence or marginal
likelihood, whose computation often requires evaluating an intractable
integral. The harmonic mean estimator (HME) has long been a standard method of
approximating the evidence. While computationally simple, the version
introduced by Newton and Raftery (1994) potentially suffers from infinite
variance. To overcome this issue, Gelfand and Dey (1994) defined a standardized
representation of the estimator based on an instrumental function and Robert
and Wraith (2009) later proposed to use higher posterior density (HPD)
indicators as instrumental functions. Following this approach, a practical
method is proposed, based on an elliptical covering of the HPD region with
non-overlapping ellipsoids. The resulting estimator (ECMLE) not only eliminates
the infinite-variance issue of the original HME and allows exact volume
computations, but is also able to be used in multi-modal settings. Through
several examples, we illustrate that ECMLE outperforms other recent methods
such as THAMES and Mixture THAMES (Metodiev et al., 2025). Moreover, ECMLE
demonstrates lower variance, a key challenge that subsequent HME variants have
sought to address, and provides more stable evidence approximations, even in
challenging settings.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [21] [AI Pose Analysis and Kinematic Profiling of Range-of-Motion Variations in Resistance Training](https://arxiv.org/abs/2510.20012)
*Adam Diamant*

Main category: stat.AP

TL;DR: 开发基于AI的姿态估计流程，用于精确量化抗阻训练中的运动学参数，比较部分范围运动(pROM)和全范围运动(fROM)在运动范围、节奏和执行动态方面的差异。


<details>
  <summary>Details</summary>
Motivation: 利用AI技术精确量化抗阻训练中的运动学参数，比较不同运动范围训练方式的执行特征差异，为训练处方提供科学依据。

Method: 使用Wolf等人(2025)的视频数据，通过AI姿态估计提取关节角度轨迹，经过滤波平滑处理后计算运动范围、节奏、向心/离心阶段时长等指标，应用随机效应元分析模型分析变异性。

Result: pROM重复动作的运动范围更小、总时长更短，特别是离心阶段；参与者个体差异是变异的主要来源；新指标%ROM显示部分运动的定义在不同练习中相对一致。

Conclusion: 部分范围运动与全范围运动不仅在运动范围上不同，在执行动态和一致性方面也存在差异，AI方法在推进抗阻训练研究和改善训练处方方面具有潜力。

Abstract: This study develops an AI-based pose estimation pipeline to enable precise
quantification of movement kinematics in resistance training. Using video data
from Wolf et al. (2025), which compared lengthened partial (pROM) and full
range-of-motion (fROM) training across eight upper-body exercises in 26
participants, 280 recordings were processed to extract frame-level joint-angle
trajectories. After filtering and smoothing, per-set metrics were derived,
including range of motion (ROM), tempo, and concentric/eccentric phase
durations. A random-effects meta-analytic model was applied to account for
within-participant and between-exercise variability. Results show that pROM
repetitions were performed with a smaller ROM and shorter overall durations,
particularly during the eccentric phase of movement. Variance analyses revealed
that participant-level differences, rather than exercise-specific factors, were
the primary driver of variation, although there is substantial evidence of
heterogeneous treatment effects. We then introduce a novel metric, \%ROM, which
is the proportion of full ROM achieved during pROM, and demonstrate that this
definition of lengthened partials remains relatively consistent across
exercises. Overall, these findings suggest that lengthened partials differ from
full ROM training not only in ROM, but also in execution dynamics and
consistency, highlighting the potential of AI-based methods for advancing
research and improving resistance training prescription.

</details>


### [22] [Treatment Effect Learning Under Sequential Randomization](https://arxiv.org/abs/2510.20078)
*Rina Friedberg,Richard Mudd,Patrick Johnstone,Melissa Pothen,Vishal Vaingankar,Vishwanath Sangale,Abbas Zaidi*

Main category: stat.AP

TL;DR: 针对在线实验中序列治疗分配导致的复杂依赖结构问题，本文提出将T-Learner与G-Formula结合的方法，以处理治疗效果的持续性影响。


<details>
  <summary>Details</summary>
Motivation: 在线实验中的序列治疗分配会产生复杂的依赖结构，当前治疗的效果可能持续到后续会话中，导致标准识别和推断方法失效。

Method: 将T-Learner分层整合到G-Formula中，结合因果机器学习和序列设置中的识别方法。

Result: 在简单模拟中，该方法在存在遗留效应的情况下防止了准确性衰减。

Conclusion: 针对科技领域常见系统特性，需要定制化的识别和推断策略。

Abstract: Sequential treatment assignments in online experiments lead to complex
dependency structures, often rendering identification, estimation and inference
over treatments a challenge. Treatments in one session (e.g., a user logging
on) can have an effect that persists into subsequent sessions, leading to
cumulative effects on outcomes measured at a later stage. This can render
standard methods for identification and inference trivially misspecified. We
propose T-Learners layered into the G-Formula for this setting, building on
literature from causal machine learning and identification in sequential
settings. In a simple simulation, this approach prevents decaying accuracy in
the presence of carry-over effects, highlighting the importance of
identification and inference strategies tailored to the nature of systems often
seen in the tech domain.

</details>


### [23] [Reorienting Age-Friendly Frameworks for Rural Contexts: A Spatial Competence-Press Framework for Aging in Chinese Villages](https://arxiv.org/abs/2510.20343)
*Ziyuan Gao*

Main category: stat.AP

TL;DR: 本研究开发了一个基于GIS的空间压力分析框架，用于量化农村老龄化相关压力因素，并将农村村庄按干预需求分类，为农村老年友好型干预提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 现有农村老龄化研究存在方法学空白，系统性地低估了老年人面临的空间压力因素，包括地形障碍、基础设施限制、气候暴露和农业劳动负担。现有乡村振兴政策强调标准化干预，但未能充分解决空间异质性和老龄化人口的空间差异化需求。

Method: 开发了基于GIS的空间压力分析框架，应用Lawton和Nahemow的能力-压力模型，建立了四个空间压力指标：坡度指数(SGI)、太阳辐射暴露指数(SREI)、步行性指数(WI)和农业强度指数(AII)，使用方差分析和层次聚类方法分析27个村庄数据。

Result: 方差分析和层次聚类显示不同村庄间空间压力存在显著差异，识别出需要针对性干预策略的独特类型。该框架既提供单个村庄的定量压力测量，也提供具有相似压力模式的村庄分类系统。

Conclusion: 该框架为规划者和政策制定者提供了实用的工具，可用于设计中国农村及类似背景下具有空间针对性的老年友好型干预措施。

Abstract: While frameworks such as the WHO Age-Friendly Cities have advanced urban
aging policy, rural contexts demand fundamentally different analytical
approaches. The spatial dispersion, terrain variability, and agricultural labor
dependencies that characterize rural aging experiences require moving beyond
service-domain frameworks toward spatial stress assessment models. Current
research on rural aging in China exhibits methodological gaps, systematically
underrepresenting the spatial stressors that older adults face daily, including
terrain barriers, infrastructure limitations, climate exposure, and
agricultural labor burdens. Existing rural revitalization policies emphasize
standardized interventions while inadequately addressing spatial heterogeneity
and the spatially-differentiated needs of aging populations. This study
developed a GIS-based spatial stress analysis framework that applies Lawton and
Nahemow's competence-press model to quantify aging-related stressors and
classify rural villages by intervention needs. Using data from 27 villages in
Mamuchi Township, Shandong Province, we established four spatial stress
indicators: slope gradient index (SGI), solar radiation exposure index (SREI),
walkability index (WI), and agricultural intensity index (AII). Analysis of
variance and hierarchical clustering revealed significant variation in spatial
pressures across villages and identified distinct typologies that require
targeted intervention strategies. The framework produces both quantitative
stress measurements for individual villages and a classification system that
groups villages with similar stress patterns, providing planners and
policymakers with practical tools for designing spatially-targeted age-friendly
interventions in rural China and similar contexts.

</details>
