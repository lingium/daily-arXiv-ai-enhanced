<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 9]
- [stat.AP](#stat.AP) [Total: 5]
- [stat.ME](#stat.ME) [Total: 13]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Optimal Control of the Future via Prospective Foraging](https://arxiv.org/abs/2511.08717)
*Yuxin Bai,Aranyak Acharyya,Ashwin De Silva,Zeyu Shen,James Hassett,Joshua T. Vogelstein*

Main category: stat.ML

TL;DR: 本文提出了"前瞻控制"框架，将PAC学习扩展到非平稳环境中的学习和控制问题，证明了经验风险最小化(ERM)在特定条件下能够渐近达到贝叶斯最优策略，并在觅食任务中展示了该方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在最优控制问题上的方法主要基于强化学习或在线学习，但这些框架与PAC学习在数学上存在差异。为了将PAC学习扩展到非平稳环境中的控制问题，需要开发新的理论框架。

Method: 基于前瞻学习理论，提出了"前瞻控制"框架，将PAC学习扩展到非平稳环境中的学习和控制。使用经验风险最小化(ERM)方法，并在觅食任务这一典型移动智能体任务中进行具体实现。

Result: 证明了在相当一般的假设下，经验风险最小化能够渐近达到贝叶斯最优策略。在觅食任务中，现有强化学习算法在非平稳环境中学习失败，即使经过修改，其效率也比前瞻控制代理低几个数量级。

Conclusion: 前瞻控制框架成功地将PAC学习扩展到非平稳环境中的控制问题，为AI在动态环境中的最优控制提供了新的理论基础和实用方法。

Abstract: Optimal control of the future is the next frontier for AI. Current approaches to this problem are typically rooted in either reinforcement learning or online learning. While powerful, these frameworks for learning are mathematically distinct from Probably Approximately Correct (PAC) learning, which has been the workhorse for the recent technological achievements in AI. We therefore build on the prior work of prospective learning, an extension of PAC learning (without control) in non-stationary environments (De Silva et al., 2023; Silva et al., 2024; Bai et al., 2026). Here, we further extend the PAC learning framework to address learning and control in non-stationary environments. Using this framework, called ''Prospective Control'', we prove that under certain fairly general assumptions, empirical risk minimization (ERM) asymptotically achieves the Bayes optimal policy. We then consider a specific instance of prospective control, foraging, which is a canonical task for any mobile agent, be it natural or artificial. We illustrate that existing reinforcement learning algorithms fail to learn in these non-stationary environments, and even with modifications, they are orders of magnitude less efficient than our prospective foraging agents. Code is available at: https://github.com/neurodata/ProspectiveLearningwithControl.

</details>


### [2] [The Probably Approximately Correct Learning Model in Computational Learning Theory](https://arxiv.org/abs/2511.08791)
*Rocco A. Servedio*

Main category: stat.ML

TL;DR: 这篇综述论文概述了在Valiant的PAC学习模型及其常见变体中学习布尔函数类的各种已知结果


<details>
  <summary>Details</summary>
Motivation: 为布尔函数类在PAC学习框架下的研究成果提供系统性概述和总结

Method: 采用文献综述方法，整理和分析在PAC学习模型及其变体下学习布尔函数类的现有研究成果

Result: 系统梳理了布尔函数类在PAC学习模型中的各类学习结果和理论界限

Conclusion: 该综述为理解布尔函数类在PAC学习框架下的学习能力提供了全面的理论视角

Abstract: This survey paper gives an overview of various known results on learning classes of Boolean functions in Valiant's Probably Approximately Correct (PAC) learning model and its commonly studied variants.

</details>


### [3] [Effects of label noise on the classification of outlier observations](https://arxiv.org/abs/2511.08808)
*Matheus Vinícius Barreto de Farias,Mario de Castro*

Main category: stat.ML

TL;DR: 研究探讨了在BCOPS算法训练集中添加噪声对分类任务的影响，发现即使是少量噪声也会显著影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 评估在训练集类别中添加噪声时BCOPS算法的鲁棒性，这是之前未测试过的场景。

Method: 使用合成和真实数据集，在BCOPS算法训练集中添加噪声，评估异常观测的预测弃权率和模型鲁棒性。

Result: 添加噪声（即使是少量）对模型性能有显著影响。

Conclusion: 噪声对BCOPS算法性能有重要影响，需要在实践中考虑训练数据质量。

Abstract: This study investigates the impact of adding noise to the training set classes in classification tasks using the BCOPS algorithm (Balanced and Conformal Optimized Prediction Sets), proposed by Guan & Tibshirani (2022). The BCOPS algorithm is an application of conformal prediction combined with a machine learning method to construct prediction sets such that the probability of the true class being included in the prediction set for a test observation meets a specified coverage guarantee. An observation is considered an outlier if its true class is not present in the training set. The study employs both synthetic and real datasets and conducts experiments to evaluate the prediction abstention rate for outlier observations and the model's robustness in this previously untested scenario. The results indicate that the addition of noise, even in small amounts, can have a significant effect on model performance.

</details>


### [4] [Robust Sampling for Active Statistical Inference](https://arxiv.org/abs/2511.08991)
*Puheng Li,Tijana Zrnic,Emmanuel Candès*

Main category: stat.ML

TL;DR: 提出了一种用于AI辅助数据收集的稳健主动统计推断方法，通过最优插值均匀采样和主动采样，确保估计结果不会比均匀采样更差，并在不确定性估计可靠时优于标准主动推断。


<details>
  <summary>Details</summary>
Motivation: 传统主动统计推断方法依赖AI预测模型的不确定性估计来优先收集标签，但不准确的不确定性估计可能导致结果比均匀采样更差，需要解决这一稳健性问题。

Method: 使用稳健优化思想，根据不确定性评分质量在均匀采样和主动采样之间进行最优插值，确保估计器性能不会劣于均匀采样。

Result: 在计算社会科学和调查研究的一系列真实数据集上验证了方法的有效性，稳健采样在不确定性估计可靠时优于标准主动推断。

Conclusion: 提出的稳健采样策略为主动统计推断提供了可靠保障，既避免了不准确不确定性估计带来的风险，又能在条件合适时获得性能提升。

Abstract: Active statistical inference is a new method for inference with AI-assisted data collection. Given a budget on the number of labeled data points that can be collected and assuming access to an AI predictive model, the basic idea is to improve estimation accuracy by prioritizing the collection of labels where the model is most uncertain. The drawback, however, is that inaccurate uncertainty estimates can make active sampling produce highly noisy results, potentially worse than those from naive uniform sampling. In this work, we present robust sampling strategies for active statistical inference. Robust sampling ensures that the resulting estimator is never worse than the estimator using uniform sampling. Furthermore, with reliable uncertainty estimates, the estimator usually outperforms standard active inference. This is achieved by optimally interpolating between uniform and active sampling, depending on the quality of the uncertainty scores, and by using ideas from robust optimization. We demonstrate the utility of the method on a series of real datasets from computational social science and survey research.

</details>


### [5] [Convergence and Stability Analysis of Self-Consuming Generative Models with Heterogeneous Human Curation](https://arxiv.org/abs/2511.09002)
*Hongru Zhao,Jinwen Fu,Tuan Pham*

Main category: stat.ML

TL;DR: 研究自消耗生成模型在异质偏好下的渐进行为，改进了Ferbach等人的分析，提供了在Banach压缩映射不适用的收敛结果


<details>
  <summary>Details</summary>
Motivation: 研究自消耗生成模型在异质偏好下的训练动态，改进现有分析方法的局限性

Method: 使用非线性Perron-Frobenius理论等方法，分析模型在多轮训练中的动态行为

Result: 给出了四个不同机制下的收敛结果，以及训练动态的稳定性和非稳定性分析

Conclusion: 改进了Ferbach等人的分析框架，在Banach压缩映射不适用的场景下提供了收敛保证

Abstract: Self-consuming generative models have received significant attention over the last few years. In this paper, we study a self-consuming generative model with heterogeneous preferences that is a generalization of the model in Ferbach et al. (2024). The model is retrained round by round using real data and its previous-round synthetic outputs. The asymptotic behavior of the retraining dynamics is investigated across four regimes using different techniques including the nonlinear Perron--Frobenius theory. Our analyses improve upon that of Ferbach et al. (2024) and provide convergence results in settings where the well-known Banach contraction mapping arguments do not apply. Stability and non-stability results regarding the retraining dynamics are also given.

</details>


### [6] [Learning to Validate Generative Models: a Goodness-of-Fit Approach](https://arxiv.org/abs/2511.09118)
*Pietro Cappelli,Gaia Grosso,Marco Letizia,Humberto Reyes-González,Marco Zanetti*

Main category: stat.ML

TL;DR: 提出使用NPLM方法验证高维科学数据生成模型，在混合高斯模型和LHC喷注数据上展示了其有效性


<details>
  <summary>Details</summary>
Motivation: 生成模型在科学工作流中日益重要，但传统验证方法在高维数据上存在可扩展性、统计功效和可解释性问题

Method: 采用基于Neyman-Pearson构造的NPLM学习方法进行拟合优度检验，验证高维科学数据的生成网络

Result: NPLM在混合高斯模型和FlashSim生成器上表现出强大的验证能力，并能诊断数据建模不足的区域

Conclusion: NPLM可作为强大的生成模型验证方法，同时提供诊断数据建模问题的能力

Abstract: Generative models are increasingly central to scientific workflows, yet their systematic use and interpretation require a proper understanding of their limitations through rigorous validation. Classic approaches struggle with scalability, statistical power, or interpretability when applied to high-dimensional data, making it difficult to certify the reliability of these models in realistic, high-dimensional scientific settings. Here, we propose the use of the New Physics Learning Machine (NPLM), a learning based approach to goodness-of-fit testing inspired by the Neyman-Pearson construction, to test generative networks trained on high-dimensional scientific data. We demonstrate the performance of NPLM for validation in two benchmark cases: generative models trained on mixtures of Gaussian models with increasing dimensionality, and a public end-to-end generator for the Large Hadron Collider called FlashSim, trained on jet data, typical in the field of high-energy physics. We demonstrate that the NPLM can serve as a powerful validation method while also providing a means to diagnose sub-optimally modeled regions of the data.

</details>


### [7] [Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions](https://arxiv.org/abs/2511.09465)
*Hedwig Nora Nordlinder,Lukas Billera,Jack Collier Ryder,Anton Oresten,Aron Stålmarck,Theodor Mosetti Björk,Ben Murrell*

Main category: stat.ML

TL;DR: 提出了Branching Flows生成建模框架，通过分支树结构控制序列元素数量，适用于多模态空间


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流匹配方法在状态空间元素数量固定时表现良好，但在序列长度未知时（如蛋白质链、语言模型响应）需要特殊处理

Method: 使用分支树结构，元素在二叉树上随机分支和消亡，通过学习的速率控制生成过程中的元素数量，可与各种流匹配基础过程组合

Result: 在小分子生成、抗体序列生成和蛋白质骨架生成三个领域验证了框架的有效性

Conclusion: Branching Flows是一个稳定的分布学习器，具有新能力，可处理多模态空间生成问题

Abstract: Diffusion and flow matching approaches to generative modeling have shown promise in domains where the state space is continuous, such as image generation or protein folding & design, and discrete, exemplified by diffusion large language models. They offer a natural fit when the number of elements in a state is fixed in advance (e.g. images), but require ad hoc solutions when, for example, the length of a response from a large language model, or the number of amino acids in a protein chain is not known a priori.
  Here we propose Branching Flows, a generative modeling framework that, like diffusion and flow matching approaches, transports a simple distribution to the data distribution. But in Branching Flows, the elements in the state evolve over a forest of binary trees, branching and dying stochastically with rates that are learned by the model. This allows the model to control, during generation, the number of elements in the sequence. We also show that Branching Flows can compose with any flow matching base process on discrete sets, continuous Euclidean spaces, smooth manifolds, and `multimodal' product spaces that mix these components. We demonstrate this in three domains: small molecule generation (multimodal), antibody sequence generation (discrete), and protein backbone generation (multimodal), and show that Branching Flows is a capable distribution learner with a stable learning objective, and that it enables new capabilities.

</details>


### [8] [Distributional Shrinkage I: Universal Denoisers in Multi-Dimensions](https://arxiv.org/abs/2511.09500)
*Tengyuan Liang*

Main category: stat.ML

TL;DR: 提出了一种通用去噪器，能够在仅知道噪声水平而不知道噪声分布的情况下，从含噪测量中恢复信号分布，相比贝叶斯最优去噪器在分布恢复方面有数量级提升。


<details>
  <summary>Details</summary>
Motivation: 解决从含噪测量中恢复信号分布的问题，其中只知道噪声水平而不知道噪声分布的具体形式，旨在开发对信号和噪声分布具有广泛适应性的通用去噪器。

Method: 基于最优传输理论，提出两种分布去噪器T1和T2，通过减少分布收缩的激进程度，以更高阶精度逼近Monge-Ampère方程，可通过分数匹配高效实现。

Result: 提出的分布去噪器在匹配广义矩和密度函数方面达到O(σ⁴)和O(σ⁶)精度，相比贝叶斯最优去噪器在分布恢复方面有数量级改进。

Conclusion: 当关注整个信号分布而非单个实现时，提出的通用分布去噪器相比传统贝叶斯最优去噪器在分布恢复方面具有显著优势，为分布去噪问题提供了有效解决方案。

Abstract: We revisit the problem of denoising from noisy measurements where only the noise level is known, not the noise distribution. In multi-dimensions, independent noise $Z$ corrupts the signal $X$, resulting in the noisy measurement $Y = X + σZ$, where $σ\in (0, 1)$ is a known noise level. Our goal is to recover the underlying signal distribution $P_X$ from denoising $P_Y$. We propose and analyze universal denoisers that are agnostic to a wide range of signal and noise distributions. Our distributional denoisers offer order-of-magnitude improvements over the Bayes-optimal denoiser derived from Tweedie's formula, if the focus is on the entire distribution $P_X$ rather than on individual realizations of $X$. Our denoisers shrink $P_Y$ toward $P_X$ optimally, achieving $O(σ^4)$ and $O(σ^6)$ accuracy in matching generalized moments and density functions. Inspired by optimal transport theory, the proposed denoisers are optimal in approximating the Monge-Ampère equation with higher-order accuracy, and can be implemented efficiently via score matching.
  Let $q$ represent the density of $P_Y$; for optimal distributional denoising, we recommend replacing the Bayes-optimal denoiser, \[ \mathbf{T}^*(y) = y + σ^2 \nabla \log q(y), \] with denoisers exhibiting less aggressive distributional shrinkage, \[ \mathbf{T}_1(y) = y + \frac{σ^2}{2} \nabla \log q(y), \] \[ \mathbf{T}_2(y) = y + \frac{σ^2}{2} \nabla \log q(y) - \frac{σ^4}{8} \nabla \left( \frac{1}{2} \| \nabla \log q(y) \|^2 + \nabla \cdot \nabla \log q(y) \right) . \]

</details>


### [9] [A general framework for adaptive nonparametric dimensionality reduction](https://arxiv.org/abs/2511.09486)
*Antonio Di Noia,Federico Ravenda,Antonietta Mira*

Main category: stat.ML

TL;DR: 提出一种利用内在维度估计器自动优化局部邻域大小的方法，显著改进依赖局部邻域结构的降维算法性能


<details>
  <summary>Details</summary>
Motivation: 现有非线性降维方法依赖局部邻域结构，需要手动调整邻域数量和降维维度等超参数，这些选择严重影响嵌入质量

Method: 利用最近提出的内在维度估计器，该估计器能根据特定标准返回最优的自适应局部邻域大小，将此自适应框架应用于任何依赖局部邻域结构的降维算法

Result: 在真实世界和模拟数据集上的数值实验表明，该方法能显著改进知名投影方法在各种学习任务中的表现，通过定量指标和低维可视化质量均可测量到改进

Conclusion: 提出的自适应框架能有效优化降维算法的超参数调优，提高降维质量

Abstract: Dimensionality reduction is a fundamental task in modern data science. Several projection methods specifically tailored to take into account the non-linearity of the data via local embeddings have been proposed. Such methods are often based on local neighbourhood structures and require tuning the number of neighbours that define this local structure, and the dimensionality of the lower-dimensional space onto which the data are projected. Such choices critically influence the quality of the resulting embedding. In this paper, we exploit a recently proposed intrinsic dimension estimator which also returns the optimal locally adaptive neighbourhood sizes according to some desirable criteria. In principle, this adaptive framework can be employed to perform an optimal hyper-parameter tuning of any dimensionality reduction algorithm that relies on local neighbourhood structures. Numerical experiments on both real-world and simulated datasets show that the proposed method can be used to significantly improve well-known projection methods when employed for various learning tasks, with improvements measurable through both quantitative metrics and the quality of low-dimensional visualizations.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [10] [Backcasting biodiversity at high spatiotemporal resolution using flexible site-occupancy models for opportunistically sampled citizen science data](https://arxiv.org/abs/2511.08802)
*Maxime Fajgenblat,Marc Herremans,Pieter Vanormelingen,Kristijn Swinnen,Dirk Maes,Robby Stoks,Luc De Meester,Christel Faes,Thomas Neyens*

Main category: stat.AP

TL;DR: 提出了一种灵活的贝叶斯时空站点占用模型，用于分析公民科学数据门户收集的机会性数据，能够进行高分辨率的时空回溯预测和丰富的生物学推断。


<details>
  <summary>Details</summary>
Motivation: 现有的站点占用模型往往忽略检测过程的重要方面，未能充分利用公民科学数据集中的信息，错失了精细时空回溯预测的机会。

Method: 开发了一个灵活的贝叶斯时空站点占用模型，旨在模拟公共生物多样性门户网站常见公民科学数据集的数据生成过程。

Result: 将该模型应用于比利时超过300万条蝴蝶记录数据集，能够进行高分辨率的时空回溯预测，并推断年度分布趋势、范围动态、栖息地偏好、物候模式等。

Conclusion: 该模型可以提高自然学家和公民科学家收集的机会性数据的价值，有助于理解缺乏严格收集数据的物种的时空动态。

Abstract: For many taxonomic groups, online biodiversity portals used by naturalists and citizen scientists constitute the primary source of distributional information. Over the last decade, site-occupancy models have been advanced as a promising framework to analyse such loosely structured, opportunistically collected datasets. Current approaches often ignore important aspects of the detection process and do not fully capitalise on the information present in these datasets, leaving opportunities for fine-grained spatiotemporal backcasting untouched. We propose a flexible Bayesian spatiotemporal site-occupancy model that aims to mimic the data-generating process that underlies common citizen science datasets sourced from public biodiversity portals, and yields rich biological output. We illustrate the use of the model to a dataset containing over 3M butterfly records in Belgium, collected through the citizen science data portal Observations.be. We show that the proposed approach enables retrospective predictions on the occupancy of species through time and space at high resolution, as well as inference on inter-annual distributional trends, range dynamics, habitat preferences, phenological patterns, detection patterns and observer heterogeneity. The proposed model can be used to increase the value of opportunistically collected data by naturalists and citizen scientists, and can aid the understanding of spatiotemporal dynamics of species for which rigorously collected data are absent or too costly to collect.

</details>


### [11] [Generalisable prediction model of surgical case duration: multicentre development and temporal validation](https://arxiv.org/abs/2511.08994)
*Daijiro Kabata,Mari Ito,Tokito Koga,Kazuma Yunoki*

Main category: stat.AP

TL;DR: 开发了一个仅使用广泛可得的术前变量的堆叠机器学习模型，用于预测手术时长，在时间外部验证中表现准确且校准良好，支持跨机构和跨时间的可移植性。


<details>
  <summary>Details</summary>
Motivation: 现有手术时长预测模型通常依赖特定地点或外科医生的输入，且很少进行外部验证，限制了其通用性。需要开发更通用的工具来改进手术室调度。

Method: 回顾性多中心研究，使用日本两家综合医院的常规围手术期数据。包含四个学习器（弹性网络、广义加性模型、随机森林、梯度提升树），通过内部-外部交叉验证调优，并通过堆叠泛化组合来预测对数转换的手术时长。

Result: 分析了63,206例手术（开发集45,647例；时间测试集17,559例）。在2024年时间测试队列中，校准良好（截距0.423，斜率0.921），性能在不同中心和年份间保持一致。

Conclusion: 仅使用广泛可得的术前变量的堆叠机器学习模型在时间外部验证中实现了准确且校准良好的预测，支持跨地点和跨时间的可移植性，可改进手术室调度而无需依赖特殊输入。

Abstract: Background: Accurate prediction of surgical case duration underpins operating room (OR) scheduling, yet existing models often depend on site- or surgeon-specific inputs and rarely undergo external validation, limiting generalisability.
  Methods: We undertook a retrospective multicentre study using routinely collected perioperative data from two general hospitals in Japan (development: 1 January 2021-31 December 2023; temporal test: 1 January-31 December 2024). Elective weekday procedures with American Society of Anesthesiologists (ASA) Physical Status 1-4 were included. Pre-specified preoperative predictors comprised surgical context (year, month, weekday, scheduled duration, general anaesthesia indicator, body position) and patient factors (sex, age, body mass index, allergy, infection, comorbidity, ASA). Missing data were addressed by multiple imputation by chained equations. Four learners (elastic-net, generalised additive models, random forest, gradient-boosted trees) were tuned within internal-external cross-validation (IECV; leave-one-cluster-out by centre-year) and combined by stacked generalisation to predict log-transformed duration.
  Results: We analysed 63,206 procedures (development 45,647; temporal test 17,559). Cluster-specific and pooled errors and calibrations from IECV are provided with consistent performance across centres and years. In the 2024 temporal test cohort, calibration was good (intercept 0.423, 95%CI 0.372 to 0.474; slope 0.921, 95%CI 0.911 to 0.932).
  Conclusions: A stacked machine-learning model using only widely available preoperative variables achieved accurate, well-calibrated predictions in temporal external validation, supporting transportability across sites and over time. Such general-purpose tools may improve OR scheduling without relying on idiosyncratic inputs.

</details>


### [12] [Second-order spatial analysis of shapes of tumor cell nuclei](https://arxiv.org/abs/2511.09023)
*Ye Jin Choi,Sebastian Kurtek,Simeng Zhu,Karthik Bharath*

Main category: stat.AP

TL;DR: 提出了一种基于标记点过程的框架，用于评估平面闭合曲线形状之间的空间相关性，应用于乳腺癌组织病理学图像中的细胞核分析。


<details>
  <summary>Details</summary>
Motivation: 现有的工具难以量化细胞及其核形态与空间模式之间的相关性，且现有方法基于低维数值摘要，无法充分编码形状信息。

Method: 使用标记点过程框架，基于标记加权的K函数这一二阶空间统计量，通过捕捉细胞和核形状的测试函数来评估空间相关性。

Result: 在乳腺癌组织病理学图像中发现了与临床预期一致的不同相关模式。

Conclusion: 该框架能够有效量化细胞核形状之间的空间相关性，为肿瘤异质性研究提供了新工具。

Abstract: Intra-tumor heterogeneity driving disease progression is characterized by distinct growth and spatial proliferation patterns of cells and their nuclei within tumor and non-tumor tissues. A widely accepted hypothesis is that these spatial patterns are correlated with morphology of the cells and their nuclei. Nevertheless, tools to quantify the correlation, with uncertainty, are scarce, and the state-of-the-art is based on low-dimensional numerical summaries of the shapes that are inadequate to fully encode shape information. To this end, we propose a marked point process framework to assess spatial correlation among shapes of planar closed curves, which represent cell or nuclei outlines. With shapes of curves as marks, the framework is based on a mark-weighted $K$ function, a second-order spatial statistic that accounts for the marks' variation by using test functions that capture only the shapes of cells and their nuclei. We then develop local and global hypothesis tests for spatial dependence between the marks using the $K$ function. The framework is brought to bear on the cell nuclei extracted from histopathology images of breast cancer, where we uncover distinct correlation patterns that are consistent with clinical expectations.

</details>


### [13] [Scaling behavioral incentives for low-carbon mobility through digital platforms](https://arxiv.org/abs/2511.09237)
*Bing Liu,Yuan Liao,Sonia Yeh,Oded Cats,Kristian S. Nielsen,Zhenning Dong,Yong Wang,Yi Li,Yanli Liu,Zirui Ni,Xiaolei Ma*

Main category: stat.AP

TL;DR: 该研究评估了北京MaaS平台中的碳激励计划，通过对390万参与者和48亿次多模式出行数据的分析，发现该计划每月增加公共交通和自行车出行20.3%，每日减少汽油车使用1.8%，年碳减排约94,000吨。


<details>
  <summary>Details</summary>
Motivation: 实现全球碳减排目标需要大规模改变日常出行行为，但关于如何激励这种大规模行为改变的现实证据仍然稀缺。

Method: 使用北京MaaS平台中嵌入的碳激励计划数据，分析390万参与者和48亿次多模式出行记录，时间跨度395天。

Result: 计划使公共交通和自行车出行每月增加20.3%，汽油车使用每日减少1.8%，年碳减排约94,000吨，相当于北京碳市场认证减排量的5.7%。虽然效果随时间减弱，但8个月后参与者每月绿色出行仍增加12.8%。

Conclusion: 这为MaaS中的碳激励提供了首个大规模实证证据，突显了其潜力，可为针对性的、城市特定的干预措施提供信息，支持全球低碳交通转型。

Abstract: Meeting global carbon reduction targets requires large-scale behavioral shifts in everyday travel. Yet, real-world evidence on how to motivate such large-scale behavioral change remains scarce. We evaluate a carbon incentive program embedded in a MaaS platform in Beijing, China, using data from 3.9 million participants and 4.8 billion multimodal trips over 395 days. The program increased reported public transport and bike travel by 20.3% per month and reduced gasoline car use by 1.8% per day, yielding an annual carbon reduction of ~94,000 tons, or 5.7% of certified reductions in Beijing's carbon market. Although effects diminished over time, participants still made 12.8% more green trips per month after eight months, indicating persistence. These results provide the first large-scale empirical evidence of carbon incentives in MaaS and highlight their potential to inform targeted, city-specific interventions that can scale to support global low-carbon mobility transitions.

</details>


### [14] [The trade-off between model flexibility and accuracy of the Expected Threat model in football](https://arxiv.org/abs/2511.09457)
*Koen W. van Arem,Jakob Söhl,Mirjam Bruinsma,Geurt Jongbloed*

Main category: stat.AP

TL;DR: 本文分析了Expected Threat模型在足球数据分析中的网格尺寸选择问题，通过理论分析和模拟实验提出了平衡模型灵活性和准确性的实用指导原则。


<details>
  <summary>Details</summary>
Motivation: 足球比赛中包含大量事件数据，Expected Threat模型因其可解释性而受到青睐，但网格尺寸的选择需要在模型灵活性和估计准确性之间权衡，这给实践者带来了挑战。

Method: 从理论角度分析Expected Threat模型，基于模型的马尔可夫链进行模拟实验，研究其实际行为，建立误差上界并提供更准确的误差表征。

Result: 理论结果建立了不同灵活性下Expected Threat模型的误差上界，模拟实验提供了比理论边界更准确的模型误差表征。

Conclusion: 将研究结果转化为实用经验法则，帮助实践者在Expected Threat模型的灵活性和期望准确性之间选择适当的平衡点。

Abstract: With an average football (soccer) match recording over 3,000 on-ball events, effective use of this event data is essential for practitioners at football clubs to obtain meaningful insights. Models can extract more information from this data, and explainable methods can make them more accessible to practitioners. The Expected Threat model has been praised for its explainability and offers an accessible option. However, selecting the grid size is a challenging key design choice that has to be made when applying the Expected Threat model. Using a finer grid leads to a more flexible model that can better distinguish between different situations, but the accuracy of the estimates deteriorates with a more flexible model. Consequently, practitioners face challenges in balancing the trade-off between model flexibility and model accuracy. In this study, the Expected Threat model is analyzed from a theoretical perspective and simulations are performed based on the Markov chain of the model to examine its behavior in practice. Our theoretical results establish an upper bound on the error of the Expected Threat model for different flexibilities. Based on the simulations, a more accurate characterization of the model's error is provided, improving over the theoretical bound. Finally, these insights are converted into a practical rule of thumb to help practitioners choose the right balance between the model flexibility and the desired accuracy of the Expected Threat model.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [15] [Deep neural expected shortfall regression with tail-robustness](https://arxiv.org/abs/2511.08772)
*Myeonghun Yu,Kean Ming Tan,Huixia Judy Wang,Wen-Xin Zhou*

Main category: stat.ME

TL;DR: 提出了一种基于深度神经网络的两步非参数预期损失回归框架，通过整合Huber损失函数来增强对厚尾分布的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 预期损失回归目前处于早期发展阶段，因为传统的经验风险最小化框架不直接适用，且ES估计对厚尾响应或误差分布敏感。

Method: 采用两步法，将条件分位数函数作为冗余参数，利用前馈ReLU神经网络构建两步ES估计器，并整合调谐的Huber损失来增强鲁棒性。

Result: 模拟研究和厄尔尼诺对极端降水影响的实证分析表明，该方法具有准确性和鲁棒性。

Conclusion: 提出的鲁棒深度ES估计器在非渐近意义上对厚尾性具有可证明的抵抗能力，并且对第一阶段分位数估计误差具有一阶不敏感性。

Abstract: Expected shortfall (ES), also known as conditional value-at-risk, is a widely recognized risk measure that complements value-at-risk by capturing tail-related risks more effectively. Compared with quantile regression, which has been extensively developed and applied across disciplines, ES regression remains in its early stage, partly because the traditional empirical risk minimization framework is not directly applicable. In this paper, we develop a nonparametric framework for expected shortfall regression based on a two-step approach that treats the conditional quantile function as a nuisance parameter. Leveraging the representational power of deep neural networks, we construct a two-step ES estimator using feedforward ReLU networks, which can alleviate the curse of dimensionality when the underlying functions possess hierarchical composition structures. However, ES estimation is inherently sensitive to heavy-tailed response or error distributions. To address this challenge, we integrate a properly tuned Huber loss into the neural network training, yielding a robust deep ES estimator that is provably resistant to heavy-tailedness in a non-asymptotic sense and first-order insensitive to quantile estimation errors in the first stage. Comprehensive simulation studies and an empirical analysis of the effect of El Niño on extreme precipitation illustrate the accuracy and robustness of the proposed method.

</details>


### [16] [Practical considerations when designing an online learning algorithm for an app-based mHealth intervention](https://arxiv.org/abs/2511.08719)
*Rachel T Gonzalez,Madeline R Abbott,Brahmajee Nallamothu,Scott Hummel,Michael Dorsch,Walter Dempsey*

Main category: stat.ME

TL;DR: 该论文介绍了在mHealth试验LS4L2中实施强化学习算法的经验，提供了解决关键挑战的模板方案，包括奖励定义、优化时间尺度、统计模型规范、模型灵活性与计算成本平衡以及缺失数据处理。


<details>
  <summary>Details</summary>
Motivation: 将强化学习整合到mHealth临床试验中，通过学习个性化治疗策略来改善基于应用程序的干预效果，减少参与者负担并更有效地促进行为改变。

Method: 在LS4L2试验的一个臂中部署强化学习算法，该算法设计用于在通知可能有效的背景下发送提醒通知（即参与者可能在接下来30分钟内打开应用程序时），而不是在先前数据显示效果降低时发送。

Result: 成功实施了强化学习算法，并识别了实施过程中的各种挑战，为未来试验提供了解决方案模板。

Conclusion: 通过LS4L2试验的经验，为在mHealth试验中部署强化学习算法提供了实用的挑战解决模板，有助于改进基于应用程序的干预措施。

Abstract: The ubiquitous nature of mobile health (mHealth) technology has expanded opportunities for the integration of reinforcement learning into traditional clinical trial designs, allowing researchers to learn individualized treatment policies during the study. LowSalt4Life 2 (LS4L2) is a recent trial aimed at reducing sodium intake among hypertensive individuals through an app-based intervention. A reinforcement learning algorithm, which was deployed in one of the trial arms, was designed to send reminder notifications to promote app engagement in contexts where the notification would be effective, i.e., when a participant is likely to open the app in the next 30-minute and not when prior data suggested reduced effectiveness. Such an algorithm can improve app-based mHealth interventions by reducing participant burden and more effectively promoting behavior change. We encountered various challenges during the implementation of the learning algorithm, which we present as a template to solving challenges in future trials that deploy reinforcement learning algorithms. We provide template solutions based on LS4L2 for solving the key challenges of (i) defining a relevant reward, (ii) determining a meaningful timescale for optimization, (iii) specifying a robust statistical model that allows for automation, (iv) balancing model flexibility with computational cost, and (v) addressing missing values in gradually collected data.

</details>


### [17] [Enhanced Rank-Based Correlation Estimation Using Smoothed Wilcoxon Rank Scores](https://arxiv.org/abs/2511.08979)
*Feridun Tasdan,Rukiye Dagalp*

Main category: stat.ME

TL;DR: 提出基于Wilcoxon秩得分函数的改进Spearman秩相关方法，使用平滑经验累积分布函数计算平滑秩，在Wilcoxon秩得分函数中替代常规秩，用于估计Spearman相关性。


<details>
  <summary>Details</summary>
Motivation: 改进Spearman秩相关方法，使其能够更好地处理数据中的结（ties）问题，并在单调关联下获得更高的效率。

Method: 使用平滑经验累积分布函数计算平滑秩，将其应用于Wilcoxon秩得分函数，然后用这些平滑Wilcoxon秩得分来估计Spearman相关性。还提出了Wald型假设检验并展示了渐近性质。

Result: 新方法在保持Spearman秩相关基本思想的同时，改进了处理结的能力，并在单调关联下具有更高的效率。

Conclusion: 提出的基于平滑Wilcoxon秩得分的Spearman相关改进方法优于传统Spearman方法，特别是在处理结和提高效率方面表现更好。

Abstract: This article proposes an improved version of the Spearman rank correlation based on using Wilcoxon rank score function. A smoothed empirical cumulative distribution function (ecdf)computes the smoothed ranks and replaces the regular ranks in the Wilcoxon rank score function. The smoothed Wilcoxon rank scores are then used for estimation of the Spearman's correlation. The proposed approach is similar to the Spearman's rho estimator which uses ranks of the random samples of X and Y but the proposed method improves Spearman's approach such as handling ties and gaining higher efficiency under monotone associations. A Wald type hypothesis test has been proposed for the new estimator and the asymptotic properties are shown.

</details>


### [18] [Valid and efficient possibilistic structure learning in Gaussian linear regression](https://arxiv.org/abs/2511.09305)
*Ryan Martin,Naomi Singer,Jonathan Williams*

Main category: stat.ME

TL;DR: 本文提出了一种基于可能性理论推理模型(IM)框架的扩展方法，用于可靠地量化回归模型结构选择的不确定性，解决了传统方法在有限样本覆盖方面的不足。


<details>
  <summary>Details</summary>
Motivation: 回归模型拟合中确定解释变量子集的结构选择存在不确定性，但缺乏令人满意的量化方法。频率学派没有通用的置信集构造，贝叶斯后验可信集无法达到期望的有限样本覆盖。

Method: 扩展可能性理论推理模型(IM)框架，允许包含关于未知结构的不完全先验信息以促进正则化，证明这种正则化可能性IM的不确定性量化相对于数据生成过程和假设的部分先验知识是校准的。

Result: 推导出的未知模型结构置信集在有限样本中达到名义覆盖概率，在两个基准数据集上的分析结果与现有方法进行了比较。

Conclusion: 提出的正则化可能性IM框架为模型结构选择提供了可靠的数据驱动不确定性量化，解决了现有方法的覆盖问题。

Abstract: A crucial step in fitting a regression model to data is determining the model's structure, i.e., the subset of explanatory variables to be included. However, the uncertainty in this step is often overlooked due to a lack of satisfactory methods. Frequentists have no broadly applicable confidence set constructions for a model's structure, and Bayesian posterior credible sets do not achieve the desired finite-sample coverage. In this paper, we propose an extension of the possibility-theoretic inferential model (IM) framework that offers reliable, data-driven uncertainty quantification about the unknown model structure. This particular extension allows for the inclusion of incomplete prior information about the unknown structure that facilitates regularization. We prove that this new, regularized, possibilistic IM's uncertainty quantification is suitably calibrated relative to the set of joint distributions compatible with the data-generating process and assumed partial prior knowledge about the structure. This implies, among other things, that the derived confidence sets for the unknown model structure attain the nominal coverage probability in finite samples. We provide background and guidance on quantifying prior knowledge in this new context and analyze two benchmark data sets, comparing our results to those obtained by existing methods.

</details>


### [19] [A new approach to reliability assessment based on Exploratory factor analysis](https://arxiv.org/abs/2511.08952)
*Shibo Diao*

Main category: stat.ME

TL;DR: 论文讨论了科学测量中的可靠性问题，介绍了传统可靠性计算方法及其局限性，并提出了一种新方法。


<details>
  <summary>Details</summary>
Motivation: 科学测量中可靠性是基础问题，传统方法如测试-重测、KR20和探索性因子分析存在局限性，需要更有效的方法来处理多变量情况。

Method: 传统方法包括测试-重测可靠性、Kuder-Richardson KR20方法和探索性因子分析。论文提出了一种新的可靠性计算方法。

Result: 传统可靠性计算方法存在记忆效应、学习效应等局限，且主要针对单变量情况，无法有效处理多变量测量场景。

Conclusion: 需要开发新的可靠性计算方法来克服传统方法的局限性，特别是在处理多变量测量时。

Abstract: We need to collect data in any science and reliability is a fundamental problem for measurement in all of science. Reliability means calculation the variance ratio. Reliability was defined as the fraction of an observed score variance that was not error. here are a lot of methods to estimated reliability. All of these indicators of dependability and stability are in contradiction to the long held belief that a problem with test-retest reliability is that it introduces memory effects of learning and practice. As a result, Kuder and Richardson developed a method named KR20 before advances in computational speed made it trivial to find the factor structure of tests, and were based upon test and item variances. These procedures were essentially short cuts for estimating reliability. Exploratory factor analysis is also a Traditional method to calculate the reliability. It focus on only one variable in the liner model, a statistical method that can be used to collect an important type of validity evidence. but in reality, we need to focus on many more variables. So we will introduce a novel method following.

</details>


### [20] [rfBLT: Random Feature Bayesian Lasso Takens Model for time series forecasting](https://arxiv.org/abs/2511.08957)
*Thu Nguyen,Lam Si Tung Ho*

Main category: stat.ME

TL;DR: 提出rfBLT方法，结合Takens定理和贝叶斯套索回归，用于时间序列预测并量化不确定性


<details>
  <summary>Details</summary>
Motivation: 传统统计模型难以捕捉非线性模式，机器学习模型无法量化预测不确定性，需要结合两者优势

Method: 使用Takens定理构建延迟嵌入，通过随机特征映射到高维空间，在贝叶斯框架下应用正则化选择相关项

Result: 在模拟数据上与统计模型相当，在真实数据上显著优于传统统计和机器学习模型

Conclusion: rfBLT方法能有效预测时间序列并量化不确定性，已实现为R包rfBLT

Abstract: Time series prediction is challenging due to our limited understanding of the underlying dynamics. Conventional models such as ARIMA and Holt's linear trend model experience difficulty in identifying nonlinear patterns in time series. In contrast, machine learning models excel at learning complex patterns and handling high-dimensional data; however, they are unable to quantify the uncertainty associated with their predictions, as statistical models do. To overcome these drawbacks, we propose Random Feature Bayesian Lasso Takens (rfBLT) for forecasting time series data. This non-parametric model captures the underlying system via the Takens' theorem and measures the degree of uncertainty with credible intervals. This is achieved by projecting delay embeddings into a higher-dimensional space via random features and applying regularization within the Bayesian framework to identify relevant terms. Our results demonstrate that the rfBLT method is comparable to traditional statistical models on simulated data, while significantly outperforming both conventional and machine learning models when evaluated on real-world data. The proposed algorithm is implemented in an R package, rfBLT.

</details>


### [21] [Instrumental variables system identification with $L^p$ consistency](https://arxiv.org/abs/2511.09024)
*Simon Kuang,Xinfan Lin*

Main category: stat.ME

TL;DR: 提出一种从数据中合成工具变量的IV估计器，用于消除最小二乘法在识别动态系统时的偏差，适用于非线性时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 传统的工具变量方法依赖外部工具变量，但在非线性时间序列数据中很少可用。需要一种能够从数据本身合成工具变量的方法来消除最小二乘法的偏差。

Method: 提出一种IV估计器，通过从数据中合成工具变量来识别动态系统。该方法仅假设模型在未知参数上是线性的，因此广泛适用于现代稀疏促进的动态学习模型。

Result: 在强迫洛伦兹系统上，该估计器相对于最小二乘法将参数偏差降低了200倍（连续时间）和500倍（离散时间），并将RMSE降低了高达10倍。建立了有限样本L^p一致性，恢复了非参数√n收敛率。

Conclusion: 该方法通过从数据中合成工具变量，有效解决了非线性时间序列数据中动态系统识别的偏差问题，具有广泛的应用前景。

Abstract: Instrumental variables (eliminate the bias that afflicts least-squares identification of dynamical systems through noisy data, yet traditionally relies on external instruments that are seldom available for nonlinear time series data. We propose an IV estimator that synthesizes instruments from the data. We establish finite-sample $L^{p}$ consistency for all $p \ge 1$ in both discrete- and continuous-time models, recovering a nonparametric $\sqrt{n}$-convergence rate. On a forced Lorenz system our estimator reduces parameter bias by 200x (continuous-time) and 500x (discrete-time) relative to least squares and reduces RMSE by up to tenfold. Because the method only assumes that the model is linear in the unknown parameters, it is broadly applicable to modern sparsity-promoting dynamics learning models.

</details>


### [22] [Designing Efficient Hybrid and Single-Arm Trials: External Control Borrowing and Sample Size Calculation](https://arxiv.org/abs/2511.09353)
*Yujing Gao,Xiang Zhang,Shu Yang*

Main category: stat.ME

TL;DR: 提出了一个统一的实验设计框架，将标准RCT、混合试验和单臂试验纳入其中，利用外部对照数据来减少当前研究所需的样本量。


<details>
  <summary>Details</summary>
Motivation: 当平衡随机化不可行时，来自历史临床试验或真实世界数据的外部对照可以作为混合试验和单臂试验的补充，但现有研究主要关注试验后推断，其在前瞻性试验设计中的作用仍较少探索。

Method: 基于有效影响函数推导估计器，开发了利用可比外部对照数据的混合和单臂设计策略，推导了这些估计器的渐近方差表达式，并引入了预实验方差估计程序来指导样本量计算。

Result: 模拟研究表明，所提出的混合和单臂设计在保持有效I类错误的同时，在各种场景下都能达到目标功效，且所需当前研究受试者数量显著少于RCT设计。

Conclusion: 提出的混合和单臂设计具有实际效用和优势，能够有效利用外部对照数据来优化临床试验设计。

Abstract: External controls (ECs) from historical clinical trials or real-world data have gained increasing attention as a way to augment hybrid and single-arm trials, especially when balanced randomization is infeasible. While most existing work has focused on post-trial inference using ECs, their role in prospective trial design remains less explored. We propose a unified experimental design framework that encompasses standard randomized controlled trials (RCTs), hybrid trials, and single-arm trials, focusing on sample size determination and power analysis. Building on estimators derived from the efficient influence function, we develop hybrid and single-arm design strategies that leverage comparable EC data to reduce the required sample size of the current study. We derive asymptotic variance expressions for these estimators in terms of interpretable, population-level quantities and introduce a pre-experimental variance estimation procedure to guide sample size calculation, ensuring prespecified type I error and power for the relevant hypothesis test. Simulation studies demonstrate that the proposed hybrid and single-arm designs maintain valid type I error and achieve target power across diverse scenarios while requiring substantially fewer subjects in the current study than RCT designs. A real data application further illustrates the practical utility and advantages of the proposed hybrid and single-arm designs.

</details>


### [23] [Principled analysis of crossover designs: causal effects, efficient estimation, and robust inference](https://arxiv.org/abs/2511.09215)
*Zhichao Jiang,Peng Ding*

Main category: stat.ME

TL;DR: 本文采用基于设计的框架分析交叉设计，通过潜在结果定义因果估计量，统一使用带约束的最小二乘法进行估计，即使在模型误设情况下也能获得一致有效的点估计和方差估计。


<details>
  <summary>Details</summary>
Motivation: 标准交叉设计分析依赖强参数模型，使推断易受模型误设影响。本文旨在开发更稳健的分析方法。

Method: 采用基于设计的框架，使用潜在结果定义因果估计量，通过带约束的最小二乘法统一分析交叉设计，强调处理分配机制的核心作用。

Result: 提出了识别和估计的通用程序，推荐了回归函数规范、加权方案和系数约束，能够获得一致有效的点估计和方差估计。

Conclusion: 基于设计的最小二乘法为交叉设计提供了简单易行且稳健的分析框架，即使在模型误设情况下也能保证推断的有效性。

Abstract: Crossover designs randomly assign each unit to receive a sequence of treatments. By comparing outcomes within the same unit, these designs can effectively eliminate between-unit variation and facilitate the identification of both instantaneous effects of current treatments and carryover effects from past treatments. They are widely used in traditional biomedical studies and are increasingly adopted in modern digital platforms. However, standard analyses of crossover designs often rely on strong parametric models, making inference vulnerable to model misspecification. This paper adopts a design-based framework to analyze general crossover designs. We make two main contributions. First, we use potential outcomes to formally define the causal estimands and assumptions on the data-generating process. For any given type of crossover design and assumptions on potential outcomes, we outline a procedure for identification and estimation, emphasizing the central role of the treatment assignment mechanism in design-based inference. Second, we unify the analysis of crossover designs using least squares, with restrictions on the coefficients and weights on the units. Based on the theory, we recommend the specification of the regression function, weighting scheme, and coefficient restrictions to assess identifiability, construct efficient estimators, and estimate variances in a unified fashion. Crucially, the least squares procedure is simple to implement, and yields not only consistent and efficient point estimates but also valid variance estimates even when the working regression model is misspecified.

</details>


### [24] [Density ratio model for multiple types of survival data with empirical likelihood](https://arxiv.org/abs/2511.09398)
*James Hugh McVittie,Archer Gong Zhang*

Main category: stat.ME

TL;DR: 本文扩展了密度比模型(DRM)方法，将其应用于审查/截断的失效时间数据，开发了基于经验似然的EM算法来估计模型参数和生存函数，并通过模拟和实际医院数据验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当在观察性研究中收集到多种类型的部分观测（审查/截断）失效时间数据时，需要一种统一的分析方法来有效结合这些数据。密度比模型能够通过指数倾斜将多个样本的分布与非参数定义的参考分布联系起来。

Method: 将审查/长度偏倚/截断数据的方法扩展到DRM框架中，使用经验似然进行推断，开发了基于DRM的最大经验似然估计器的EM算法来计算模型参数和生存函数。

Result: 通过广泛的模拟研究，在正确模型设定、过度设定和错误设定情况下，以及不同失效时间分布和审查比例下评估了方法的性能。使用加拿大蒙特利尔地区医院从入院到出院的时间数据验证了方法的有效性。

Conclusion: 提出的方法能够有效处理多种类型的部分观测失效时间数据，为结合不同来源的生存数据提供了统一的统计框架，相关R代码已在GitHub上开源。

Abstract: The density ratio model (DRM) is a semiparametric model that relates the distributions from multiple samples to a nonparametrically defined reference distribution via exponential tilting, with finite-dimensional parameters governing their differences in shape. When multiple types of partially observed (censored/truncated) failure time data are collected in an observational study, the DRM can be utilized to conduct a single unified analysis of the combined data. In this paper, we extend the methodology for censored length-biased/truncated data to the DRM framework and formulate the inference using empirical likelihood. We develop an EM algorithm to compute the DRM-based maximum empirical likelihood estimators of the model parameters and survival function, and assess its performance through extensive simulations under correct model specification, overspecification, and misspecification, across a range of failure-time distributions and censoring proportions. We also illustrate the efficacy of our method by analyzing the duration of time spent from admission to discharge in a Montreal-area hospital in Canada. The R code that implements our method is available on GitHub at \href{https://github.com/gozhang/DRM-combined-survival}{DRM-combined-survival}.

</details>


### [25] [Nonparametric intensity estimation of spatial point processes by random forests](https://arxiv.org/abs/2511.09307)
*Christophe Biscio,Frédéric Lavancier*

Main category: stat.ME

TL;DR: 提出了一种用于空间点过程强度估计的随机森林方法，无论有无协变量均可使用，具有处理大量协变量、袋外交叉验证和变量重要性评估等优势，无需边界校正且适应不规则形状域和流形。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够处理空间点过程强度估计的灵活方法，既能在有协变量时利用协变量信息，也能在无协变量时有效工作，同时避免传统方法中的边界校正问题。

Method: 使用随机森林估计器来估计空间点过程的强度，该方法能够处理大量协变量，支持袋外交叉验证和变量重要性评估，无需边界校正且适应不规则形状域和流形。

Result: 建立了在各种渐近机制下的一致性和收敛速率，表明在有协变量时使用协变量能带来益处。数值实验显示该方法与最先进方法具有竞争力。

Conclusion: 所提出的随机森林估计器是一种灵活有效的空间点过程强度估计方法，具有处理协变量、无需边界校正、适应复杂域等优势，在性能上与现有最佳方法相当。

Abstract: We propose a random forest estimator for the intensity of spatial point processes, applicable with or without covariates. It retains the well-known advantages of a random forest approach, including the ability to handle a large number of covariates, out-of-bag cross-validation, and variable importance assessment. Importantly, even in the absence of covariates, it requires no border correction and adapts naturally to irregularly shaped domains and manifolds. Consistency and convergence rates are established under various asymptotic regimes, revealing the benefit of using covariates when available. Numerical experiments illustrate the methodology and demonstrate that it performs competitively with state-of-the-art methods.

</details>


### [26] [Family-wise error rate control in clinical trials with overlapping populations](https://arxiv.org/abs/2511.09449)
*Remi Luschei,Werner Brannath*

Main category: stat.ME

TL;DR: 本文讨论了多重叠患者群体的临床试验中多重检验问题，发现在异质性零效应抵消的情况下，传统的ANOVA方法无法控制族系错误率(FWER)，并提出了替代方法进行比较。


<details>
  <summary>Details</summary>
Motivation: 临床试验中多个重叠患者群体测试多个治疗策略会导致多重性问题，传统方法在异质性零效应抵消时无法保证FWER控制。

Method: 分析了传统ANOVA假设下的FWER控制方法，提出了不同的替代方法，并在FWER控制和统计功效方面进行比较。

Result: 研究表明在异质性零效应抵消的情况下，传统ANOVA程序无法提供FWER控制。

Conclusion: 需要采用替代方法来确保在异质性零效应情况下的FWER控制，并对不同方法在FWER控制和统计功效方面进行了比较。

Abstract: We consider clinical trials with multiple, overlapping patient populations, that test multiple treatment policies specifically tailored to these populations. Such designs may lead to multiplicity issues, as false statements will affect several populations. For type I error control, often the family-wise error rate (FWER) is controlled, which is the probability to reject at least one true null hypothesis. If the joint distribution of the test statistics is known, the FWER level can be exhausted by determining critical values or adjusted $α$-levels. The adjustment is typically done under the common ANOVA assumptions. However, the performed tests are then only valid under the rather strong assumption of homogeneous null effects, i.e., when the null hypothesis applies to all subpopulations and their intersections. We show that under cancelling null effects, when heterogeneous effects cancel out in some or all subpopulations, this procedure does not provide FWER control. We also suggest different alternatives and compare them in terms of FWER control and their power.

</details>


### [27] [Local Interaction Autoregressive Model for High Dimension Time Series Data](https://arxiv.org/abs/2511.09542)
*Jingyang Li,Yang Chen*

Main category: stat.ME

TL;DR: 提出了一个局部交互自回归(LIAR)框架，用于高维矩阵/张量时间序列预测，通过考虑局部依赖关系来降低参数空间维度，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 高维矩阵和张量时间序列通常表现出局部依赖性，每个元素主要与小的邻域交互。在预测模型中考虑局部交互可以大大减少参数空间的维度，从而获得更高效的推断和更准确的预测。

Method: 提出了局部交互自回归(LIAR)框架，特别是可分离LIAR变体，通过并行最小二乘法进行参数估计，并使用BIC类型的邻域选择器。

Result: 数值模拟显示BIC选择器能以高成功率恢复真实邻域，LIAR实现小估计误差，预测性能优于矩阵时间序列基线。在TEC案例研究中，模型能识别局部时空传播并改善预测。

Conclusion: LIAR框架通过考虑局部依赖关系，在高维矩阵/张量时间序列预测中表现出优越性能，能有效识别局部交互模式并提高预测精度。

Abstract: High-dimensional matrix and tensor time series often exhibit local dependency, where each entry interacts mainly with a small neighborhood. Accounting for local interactions in a prediction model can greatly reduce the dimensionality of the parameter space, leading to more efficient inference and more accurate predictions. We propose a Local Interaction Autoregressive (LIAR) framework and study Separable LIAR, a variant with shared row and column components, for high-dimensional matrix/tensor time series forecasting problems. We derive a scalable parameter estimation algorithm via parallel least squares with a BIC-type neighborhood selector. Theoretically, we show consistency of neighborhood selection and derive error bounds for kernel and auto-covariance estimation. Numerical simulations show that the BIC selector recovers the true neighborhood with high success rates, the LIAR achieves small estimation errors, and the forecasts outperform matrix time-series baselines. In real data applications, a Total Electron Content (TEC) case study shows the model can identify localized spatio-temporal propagation and improved prediction as compared with non-local time series prediction models.

</details>
