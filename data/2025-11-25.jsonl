{"id": "2511.16718", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.16718", "abs": "https://arxiv.org/abs/2511.16718", "authors": ["Lorenza Cotugno", "Mark de Rooij", "Roberta Siciliano"], "title": "Regularized Reduced Rank Regression for mixed predictor and response variables", "comment": null, "summary": "In this paper, we introduce the Generalized Mixed Regularized Reduced Rank Regression model (GMR4), an extension of the GMR3 model designed to improve performance in high-dimensional settings. GMR3 is a regression method for a mix of numeric, binary and ordinal response variables, while also allowing for mixed-type predictors through optimal scaling. GMR4 extends this approach by incorporating regularization techniques, such as Ridge, Lasso, Group Lasso, or any combination thereof, making the model suitable for datasets with a large number of predictors or collinearity among them. In addition, we propose a cross-validation procedure that enables the estimation of the rank S and the penalty parameter lambda. Through a simulation study, we evaluate the performance of the model under different scenarios, varying the sample size, the number of non-informative predictors and response dimension. The results of the simulation study guide the choice of the penalty parameter lambda in the empirical application ISSP: Health and Healthcare I-II (2023), which includes mixed-type predictors and ordinal responses. In this application, the model results in a sparse and interpretable solution, with a limited set of influential predictors that provide insights into public attitudes toward healthcare."}
{"id": "2511.16773", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.16773", "abs": "https://arxiv.org/abs/2511.16773", "authors": ["Yunhan Mou", "Fan Li", "Denise Esserman", "Yuan Huang"], "title": "Correlation Matters! Streamlining the Sample Size Procedure with Composite Time-to-event Endpoints", "comment": null, "summary": "Composite endpoints are widely used in cardiovascular clinical trials to improve statistical efficiency while preserving clinical relevance. The Win Ratio (WR) measure and more general frameworks of Win Statistics have emerged as increasingly popular alternatives to traditional time-to-first-event analyses. Although analytic sample size formulas for WR have been developed, they rely on design parameters that are often not straightforward to specify. Consequently, sample size determination in clinical trials with WR as the primary analysis is most often based on simulations, which can be computationally intensive. Moreover, these simulations commonly assume independence among component endpoints, an assumption that may not hold in practice and can lead to misleading power estimates. To address this challenge, we derive refined formulas to calculate the proportions of wins, losses, and ties for multiple prioritized time-to-event endpoints. These formulas rely on familiar design inputs and become directly applicable when integrated with existing sample size methods. We conduct a comprehensive assessment of how correlation among endpoints affects sample size requirements across varying design features. We further demonstrate the role of correlations through two case studies based on the landmark SPRINT and STICH clinical trials to generate further insights."}
{"id": "2511.17064", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17064", "abs": "https://arxiv.org/abs/2511.17064", "authors": ["František Bartoš", "Suzanne Hoogeveen", "Alexandra Sarafoglou", "Samuel Pawel"], "title": "Single-Dataset Meta-Analysis For Many-Analysts And Multiverse Studies", "comment": null, "summary": "Empirical claims often rely on one population, design, and analysis. Many-analysts, multiverse, and robustness studies expose how results can vary across plausible analytic choices. Synthesizing these results, however, is nontrivial as all results are computed from the same dataset. We introduce single-dataset meta-analysis, a weighted-likelihood approach that incorporates the information in the dataset at most once. It prevents overconfident inferences that would arise if a standard meta-analysis was applied to the data. Single-dataset meta-analysis yields meta-analytic point and interval estimates of the average effect across analytic approaches and of between-analyst heterogeneity, and can be supplied by classical and Bayesian hypothesis tests. Both the common-effect and random-effects versions of the model can be estimated by standard meta-analytic software with small input adjustments. We demonstrate the method via application to the many-analysts study on racial bias in soccer, the many-analysts study of marital status and cardiovascular disease, and the multiverse study on technology use and well-being. The results show how single-dataset meta-analysis complements the qualitative evaluation of many-analysts and multiverse studies."}
{"id": "2511.17065", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17065", "abs": "https://arxiv.org/abs/2511.17065", "authors": ["Perrine Chassat", "Juhyun Park", "Nicolas Brunel"], "title": "Shape Analysis of Euclidean Curves under Frenet-Serret Framework", "comment": null, "summary": "Geometric frameworks for analyzing curves are common in applications as they focus on invariant features and provide visually satisfying solutions to standard problems such as computing invariant distances, averaging curves, or registering curves. We show that for any smooth curve in R^d, d>1, the generalized curvatures associated with the Frenet-Serret equation can be used to define a Riemannian geometry that takes into account all the geometric features of the shape. This geometry is based on a Square Root Curvature Transform that extends the square root-velocity transform for Euclidean curves (in any dimensions) and provides likely geodesics that avoid artefacts encountered by representations using only first-order geometric information. Our analysis is supported by simulated data and is especially relevant for analyzing human motions. We consider trajectories acquired from sign language, and show the interest of considering curvature and also torsion in their analysis, both being physically meaningful."}
{"id": "2511.16815", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16815", "abs": "https://arxiv.org/abs/2511.16815", "authors": ["Kyla D. Jones", "Alexander W. Dowling"], "title": "BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates", "comment": null, "summary": "We introduce the Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates (BITS for GAPS) framework to emulate latent components in hybrid physical systems. BITS for GAPS supports serial hybrid modeling, where known physics governs part of the system and residual dynamics are represented as a latent function inferred from data. A Gaussian process prior is placed over the latent function, with hierarchical priors on its hyperparameters to encode physically meaningful structure in the predictive posterior.\n  To guide data acquisition, we derive entropy-based acquisition functions that quantify expected information gain from candidate input locations, identifying samples most informative for training the surrogate. Specifically, we obtain a closed-form expression for the differential entropy of the predictive posterior and establish a tractable lower bound for efficient evaluation. These derivations approximate the predictive posterior as a finite, uniformly weighted mixture of Gaussian processes.\n  We demonstrate the framework's utility by modeling activity coefficients in vapor-liquid equilibrium systems, embedding the surrogate into extended Raoult's law for distillation design. Numerical results show that entropy-guided sampling improves sample efficiency by targeting regions of high uncertainty and potential information gain. This accelerates surrogate convergence, enhances predictive accuracy in non-ideal regimes, and preserves physical consistency. Overall, BITS for GAPS provides an efficient, interpretable, and uncertainty-aware framework for hybrid modeling of complex physical systems."}
{"id": "2511.17379", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17379", "abs": "https://arxiv.org/abs/2511.17379", "authors": ["Lan Wen", "Aaron L. Sarvet", "Jessica G. Young"], "title": "On treating right-censoring events like treatments", "comment": null, "summary": "In causal inference literature, potential outcomes are often indexed by the \"elimination of all right-censoring events,\" leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present."}
{"id": "2511.16816", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.16816", "abs": "https://arxiv.org/abs/2511.16816", "authors": ["Lekha Patel", "Craig Ulmer", "Stephen J. Verzi", "Daniel J. Krofcheck", "Indu Manickam", "Asmeret Naugle", "Jaideep Ray"], "title": "Trust-Aware Multimodal Data Fusion for Yield Estimation: A Case Study of the 2020 Beirut Explosion", "comment": "19 pages, 4 figures, supplementary material, journal", "summary": "The estimation of explosive yield from heterogeneous observational data presents fundamental challenges in inverse problems, particularly when combining traditional physical measurements with modern artificial intelligence-interpreted modalities. We present a novel Bayesian fractional posterior framework that fuses seismic waves, crater dimensions, synthetic aperture radar imagery, and vision-language model interpreted ground-level images to estimate the yield of the 2020 Beirut explosion. Unlike conventional approaches that may treat data sources equally, our method learns trust weights for each modality through a Dirichlet prior, automatically calibrating the relative information content of disparate observations. Applied to the Beirut explosion, the framework yields an estimate of 0.34--0.48 kt TNT equivalent, representing 12 to 17 percent detonation efficiency relative to the 2.75 kt theoretical maximum from the blast's stored ammonium nitrate. The fractional posterior approach demonstrates superior uncertainty quantification compared to single-modality estimates while providing robustness against systematic biases. This work establishes a principled framework for integrating qualitative assessments with quantitative physical measurements, with applications to explosion monitoring, disaster response, and forensic analysis."}
{"id": "2511.17117", "categories": ["stat.CO", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.17117", "abs": "https://arxiv.org/abs/2511.17117", "authors": ["Masahiro Tanaka"], "title": "Modified Delayed Acceptance MCMC for Quasi-Bayesian Inference with Linear Moment Conditions", "comment": null, "summary": "We develop a computationally efficient framework for quasi-Bayesian inference based on linear moment conditions. The approach employs a delayed acceptance Markov chain Monte Carlo (DA-MCMC) algorithm that uses a surrogate target kernel and a proposal distribution derived from an approximate conditional posterior, thereby exploiting the structure of the quasi-likelihood. Two implementations are introduced. DA-MCMC-Exact fully incorporates prior information into the proposal distribution and maximizes per-iteration efficiency, whereas DA-MCMC-Approx omits the prior in the proposal to reduce matrix inversions, improving numerical stability and computational speed in higher dimensions. Simulation studies on heteroskedastic linear regressions show substantial gains over standard MCMC and conventional DA-MCMC baselines, measured by multivariate effective sample size per iteration and per second. The Approx variant yields the best overall throughput, while the Exact variant attains the highest per-iteration efficiency. Applications to two empirical instrumental variable regressions corroborate these findings: the Approx implementation scales to larger designs where other methods become impractical, while still delivering precise inference. Although developed for moment-based quasi-posteriors, the proposed approach also extends to risk-based quasi-Bayesian formulations when first-order conditions are linear and can be transformed analogously. Overall, the proposed algorithms provide a practical and robust tool for quasi-Bayesian analysis in statistical applications."}
{"id": "2511.17071", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17071", "abs": "https://arxiv.org/abs/2511.17071", "authors": ["Jan-Ole Koslik", "Fanny Dupont", "Marie Auger-Méthé", "Marianne Marcoux", "Nancy Heckman"], "title": "Flexible unimodal density estimation in hidden Markov models", "comment": "29 pages, 7 figures", "summary": "1. Hidden Markov models (HMMs) are powerful tools for modelling time-series data with underlying state structure. However, selecting appropriate parametric forms for the state-dependent distributions is often challenging and can lead to model misspecification. To address this, P-spline-based nonparametric estimation of state-dependent densities has been proposed. While offering great flexibility, these approaches can result in overly complex densities (e.g. bimodal) that hinder interpretability. 2. We propose a straightforward method that builds on shape-constrained spline theory to enforce unimodality in the estimated state-dependent densities through enforcing unimodality of the spline coefficients. This constraint strikes a practical balance between model flexibility, interpretability, and parsimony. 3. Through two simulation studies and a real-world case study using narwhal (Monodon monoceros) dive data, we demonstrate the proposed approach yields more stable estimates compared to fully flexible, unconstrained models improving model performance and interpretability. 4. Our method bridges a key methodological gap, by providing a parsimonious HMM framework that balances the interpretability of parametric models with the flexibility of nonparametric estimation. This provides ecologists with a powerful tool to derive ecologically meaningful inference from telemetry data while avoiding the pitfalls of overly complex models."}
{"id": "2511.17438", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.17438", "abs": "https://arxiv.org/abs/2511.17438", "authors": ["Jesse Wheeler", "Aaron J. Abkemeier", "Edward L. Ionides"], "title": "Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models", "comment": null, "summary": "Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis."}
{"id": "2511.16932", "categories": ["stat.AP", "econ.EM", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.16932", "abs": "https://arxiv.org/abs/2511.16932", "authors": ["Chang Zhai", "Ping Chen", "Zhuo Jin", "David Pitt"], "title": "Optimising pandemic response through vaccination strategies using neural networks", "comment": null, "summary": "Epidemic risk assessment poses inherent challenges, with traditional approaches often failing to balance health outcomes and economic constraints. This paper presents a data-driven decision support tool that models epidemiological dynamics and optimises vaccination strategies to control disease spread whilst minimising economic losses. The proposed economic-epidemiological framework comprises three phases: modelling, optimising, and analysing. First, a stochastic compartmental model captures epidemic dynamics. Second, an optimal control problem is formulated to derive vaccination strategies that minimise pandemic-related expenditure. Given the analytical intractability of epidemiological models, neural networks are employed to calibrate parameters and solve the high-dimensional control problem. The framework is demonstrated using COVID-19 data from Victoria, Australia, empirically deriving optimal vaccination strategies that simultaneously minimise disease incidence and governmental expenditure. By employing this three-phase framework, policymakers can adjust input values to reflect evolving transmission dynamics and continuously update strategies, thereby minimising aggregate costs, aiding future pandemic preparedness."}
{"id": "2511.16816", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.16816", "abs": "https://arxiv.org/abs/2511.16816", "authors": ["Lekha Patel", "Craig Ulmer", "Stephen J. Verzi", "Daniel J. Krofcheck", "Indu Manickam", "Asmeret Naugle", "Jaideep Ray"], "title": "Trust-Aware Multimodal Data Fusion for Yield Estimation: A Case Study of the 2020 Beirut Explosion", "comment": "19 pages, 4 figures, supplementary material, journal", "summary": "The estimation of explosive yield from heterogeneous observational data presents fundamental challenges in inverse problems, particularly when combining traditional physical measurements with modern artificial intelligence-interpreted modalities. We present a novel Bayesian fractional posterior framework that fuses seismic waves, crater dimensions, synthetic aperture radar imagery, and vision-language model interpreted ground-level images to estimate the yield of the 2020 Beirut explosion. Unlike conventional approaches that may treat data sources equally, our method learns trust weights for each modality through a Dirichlet prior, automatically calibrating the relative information content of disparate observations. Applied to the Beirut explosion, the framework yields an estimate of 0.34--0.48 kt TNT equivalent, representing 12 to 17 percent detonation efficiency relative to the 2.75 kt theoretical maximum from the blast's stored ammonium nitrate. The fractional posterior approach demonstrates superior uncertainty quantification compared to single-modality estimates while providing robustness against systematic biases. This work establishes a principled framework for integrating qualitative assessments with quantitative physical measurements, with applications to explosion monitoring, disaster response, and forensic analysis."}
{"id": "2511.17091", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17091", "abs": "https://arxiv.org/abs/2511.17091", "authors": ["Mustafa Cavus"], "title": "ggskewboxplots: Enhanced Boxplots for Skewed Data in R", "comment": "16 pages, 4 figures", "summary": "Traditional boxplots are widely used for summarizing and visualizing the distribution of numerical data, yet they exhibit significant limitations when applied to skewed or heavy-tailed distributions, often leading to misclassification of outliers through swamping -- flagging typical observations as outliers -- or masking -- failing to detect true outliers. This paper addresses these limitations by systematically evaluating several alternative boxplots specifically designed to accommodate distributional asymmetry. We introduce ggskewboxplots, an R package that integrates multiple robust and skewness-aware boxplot variants, providing a unified and user-friendly framework for exploratory data analysis. Using extensive Monte Carlo simulations under controlled skewness and kurtosis conditions, implemented via the mosaic approach based on the Skewed Exponential Power distribution, we assess the sensitivity and specificity of each method. Simulation results indicate that classical Tukey-style boxplots are highly prone to swamping and masking, whereas robust skewness-adjusted variants -- particularly those leveraging quartile-based skewness measures or medcouple-based adjustments -- achieve substantially better performance. These findings offer practical guidance for selecting reliable boxplot methods in applied settings and demonstrate how the ggskewboxplots package facilitates accessible, distribution-aware visualizations within the familiar ggplot2 workflow."}
{"id": "2511.17463", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17463", "abs": "https://arxiv.org/abs/2511.17463", "authors": ["Jared N. Lakhani"], "title": "Extending the Accelerated Failure Conditionals Model to Location-Scale Families", "comment": "17 pages, 3 figures, 5 tables", "summary": "Arnold and Arvanitis (2020) introduced a novel class of bivariate conditionally specified distributions, in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This conditioning regime was formulated through survival functions and termed the accelerated failure conditionals model. Subsequently, Lakhani (2025) extended this conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, thereby moving beyond families with non-increasing densities. The present study builds on this line of work by proposing a conditional survival specification derived from a location-scale distributional family, where the dependence between $X$ and $Y$ arises not only through the acceleration function but also via a location function. An illustrative example of this new specification is developed using a Weibull marginal for $X$. The resulting models are fully characterized by closed-form expressions for their moments, and simulations are implemented using the Metropolis-Hastings algorithm. Finally, the model is applied to a dataset in which the empirical distribution of $Y$ lies on the real line, demonstrating the models' capacity to accommodate $Y$ marginals defined over $\\mathbb{R}$."}
{"id": "2511.16954", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.16954", "abs": "https://arxiv.org/abs/2511.16954", "authors": ["Qiyuan Liu", "Qirui Zhang", "Jinhong Du", "Siming Zhao", "Jingshu Wang"], "title": "Effects of Distance Metrics and Scaling on the Perturbation Discrimination Score", "comment": null, "summary": "The Perturbation Discrimination Score (PDS) is increasingly used to evaluate whether predicted perturbation effects remain distinguishable, including in Systema and the Virtual Cell Challenge. However, its behavior in high-dimensional gene-expression settings has not been examined in detail. We show that PDS is highly sensitive to the choice of similarity or distance measure and to the scale of predicted effects. Analysis of observed perturbation responses reveals that $\\ell_1$ and $\\ell_2$-based PDS behave very differently from cosine-based measures, even after norm matching. We provide geometric insight and discuss implications for future discrimination-based evaluation metrics."}
{"id": "2511.16954", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.16954", "abs": "https://arxiv.org/abs/2511.16954", "authors": ["Qiyuan Liu", "Qirui Zhang", "Jinhong Du", "Siming Zhao", "Jingshu Wang"], "title": "Effects of Distance Metrics and Scaling on the Perturbation Discrimination Score", "comment": null, "summary": "The Perturbation Discrimination Score (PDS) is increasingly used to evaluate whether predicted perturbation effects remain distinguishable, including in Systema and the Virtual Cell Challenge. However, its behavior in high-dimensional gene-expression settings has not been examined in detail. We show that PDS is highly sensitive to the choice of similarity or distance measure and to the scale of predicted effects. Analysis of observed perturbation responses reveals that $\\ell_1$ and $\\ell_2$-based PDS behave very differently from cosine-based measures, even after norm matching. We provide geometric insight and discuss implications for future discrimination-based evaluation metrics."}
{"id": "2511.17180", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17180", "abs": "https://arxiv.org/abs/2511.17180", "authors": ["Qingzhao Zhong", "Yanxi Hou"], "title": "Nonparametric Inference for Extreme CoVaR and CoES", "comment": null, "summary": "Systemic risk measures quantify the potential risk to an individual financial constituent arising from the distress of entire financial system. As a generalization of two widely applied risk measures, Value-at-Risk and Expected Shortfall, the Conditional Value-at-Risk (CoVaR) and Conditional Expected Shortfall (CoES) have recently been receiving growing attention on applications in economics and finance, since they serve as crucial metrics for systemic risk measurement. However, existing approaches confront some challenges in statistical inference and asymptotic theories when estimating CoES, particularly at high risk levels. In this paper, within a framework of upper tail dependence, we propose several extrapolative methods to estimate both extreme CoVaR and CoES nonparametrically via an adjustment factor, which are intimately related to the nonparametric modelling of the tail dependence function. In addition, we study the asymptotic theories of all proposed extrapolative methods based on multivariate extreme value theory. Finally, some simulations and real data analyses are conducted to demonstrate the empirical performances of our methods."}
{"id": "2511.17148", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17148", "abs": "https://arxiv.org/abs/2511.17148", "authors": ["David Solano", "Marta Solans", "Xavier Perafita", "Anna Ruiz-Comellas", "Marc Saez", "Maria A. Barceló"], "title": "A spatiotemporal Bayesian hierarchical model of heat-related mortality in Catalonia, Spain (2012--2022): The role of environmental and socioeconomic modifiers", "comment": null, "summary": "Background: Extreme heat is a major public health risk, yet its relationship with mortality may be confounded or modified by air pollution and social determinants. Objectives: We aimed to quantify the effects of extreme maximum temperatures and heatwaves on daily mortality in Catalonia (2012--2022), and to assess the modifying and confounding roles of air pollutants and socioeconomic factors. Methods: We conducted a time--series ecological study across 379 basic health areas (ABS) during summer months. Mortality data from the Spanish National Statistics Institute were linked with meteorological and air pollution data. A hierarchical Bayesian spatiotemporal model, incorporating structured and unstructured random effects, was used to account for spatial and temporal dependencies, as well as observed socioeconomic confounders. Results: In total, 730,634 deaths occurred, with 216,989 in summer. Extreme heat alone was not independently associated with mortality, as its effect was fully confounded by high ozone levels and partly by socioeconomic indicators. Ozone concentrations ($\\ge 120 μg/m^3$) significantly increased mortality risk, especially among individuals aged $\\ge 85$ years. Greater income inequality and higher proportions of older residents also amplified vulnerability. Conclusion: Mortality risks from extreme heat in Catalonia were strongly influenced by ozone levels and social determinants. Adaptation strategies should address both compound environmental exposures together with socioeconomic vulnerability to better protect older and disadvantaged populations."}
{"id": "2511.17302", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.17302", "abs": "https://arxiv.org/abs/2511.17302", "authors": ["Zeyu Hu", "Wenrui Li", "Jun Yan", "Panpan Zhang"], "title": "Covariate Connectivity Combined Clustering for Weighted Networks", "comment": null, "summary": "Community detection is a central task in network analysis, with applications in social, biological, and technological systems. Traditional algorithms rely primarily on network topology, which can fail when community signals are partly encoded in node-specific attributes. Existing covariate-assisted methods often assume the number of clusters is known, involve computationally intensive inference, or are not designed for weighted networks. We propose $\\text{C}^4$: Covariate Connectivity Combined Clustering, an adaptive spectral clustering algorithm that integrates network connectivity and node-level covariates into a unified similarity representation. $\\text{C}^4$ balances the two sources of information through a data-driven tuning parameter, estimates the number of communities via an eigengap heuristic, and avoids reliance on costly sampling-based procedures. Simulation studies show that $\\text{C}^4$ achieves higher accuracy and robustness than competing approaches across diverse scenarios. Application to an airport reachability network demonstrates the method's scalability, interpretability, and practical utility for real-world weighted networks."}
{"id": "2511.17292", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17292", "abs": "https://arxiv.org/abs/2511.17292", "authors": ["Leonhard Held", "Fadoua Balabdaoui", "Samuel Pawel"], "title": "The Experimental Unit Information Index: Balancing Evidentiary Value and Sample Size of Adaptive Designs", "comment": "43 pages, 8 figures, 4 tables", "summary": "Reducing the number of experimental units is one of the three pillars of the 3R principles (Replace, Reduce, Refine) in animal research. At the same time, statistical error rates need to be controlled to enable reliable inferences and decisions. This paper proposes a novel measure to quantify the evidentiary value of one experimental unit for a given study design. The experimental unit information index (EUII) is based on power, Type-I error and sample size, and has attractive interpretations both in terms of frequentist error rates and Bayesian posterior odds. We introduce the EUII in simple statistical test settings and show that its asymptotic value depends only on the assumed relative effect size under the alternative. We then extend the definition to adaptive designs where early stopping for efficacy or futility may cause reductions in sample size. Applications to group-sequential designs and a recently proposed adaptive statistical test procedure show the usefulness of the approach when the goal is to maximize the evidentiary value of one experimental unit."}
{"id": "2511.17071", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17071", "abs": "https://arxiv.org/abs/2511.17071", "authors": ["Jan-Ole Koslik", "Fanny Dupont", "Marie Auger-Méthé", "Marianne Marcoux", "Nancy Heckman"], "title": "Flexible unimodal density estimation in hidden Markov models", "comment": "29 pages, 7 figures", "summary": "1. Hidden Markov models (HMMs) are powerful tools for modelling time-series data with underlying state structure. However, selecting appropriate parametric forms for the state-dependent distributions is often challenging and can lead to model misspecification. To address this, P-spline-based nonparametric estimation of state-dependent densities has been proposed. While offering great flexibility, these approaches can result in overly complex densities (e.g. bimodal) that hinder interpretability. 2. We propose a straightforward method that builds on shape-constrained spline theory to enforce unimodality in the estimated state-dependent densities through enforcing unimodality of the spline coefficients. This constraint strikes a practical balance between model flexibility, interpretability, and parsimony. 3. Through two simulation studies and a real-world case study using narwhal (Monodon monoceros) dive data, we demonstrate the proposed approach yields more stable estimates compared to fully flexible, unconstrained models improving model performance and interpretability. 4. Our method bridges a key methodological gap, by providing a parsimonious HMM framework that balances the interpretability of parametric models with the flexibility of nonparametric estimation. This provides ecologists with a powerful tool to derive ecologically meaningful inference from telemetry data while avoiding the pitfalls of overly complex models."}
{"id": "2511.17438", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.17438", "abs": "https://arxiv.org/abs/2511.17438", "authors": ["Jesse Wheeler", "Aaron J. Abkemeier", "Edward L. Ionides"], "title": "Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models", "comment": null, "summary": "Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis."}
{"id": "2511.17302", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.17302", "abs": "https://arxiv.org/abs/2511.17302", "authors": ["Zeyu Hu", "Wenrui Li", "Jun Yan", "Panpan Zhang"], "title": "Covariate Connectivity Combined Clustering for Weighted Networks", "comment": null, "summary": "Community detection is a central task in network analysis, with applications in social, biological, and technological systems. Traditional algorithms rely primarily on network topology, which can fail when community signals are partly encoded in node-specific attributes. Existing covariate-assisted methods often assume the number of clusters is known, involve computationally intensive inference, or are not designed for weighted networks. We propose $\\text{C}^4$: Covariate Connectivity Combined Clustering, an adaptive spectral clustering algorithm that integrates network connectivity and node-level covariates into a unified similarity representation. $\\text{C}^4$ balances the two sources of information through a data-driven tuning parameter, estimates the number of communities via an eigengap heuristic, and avoids reliance on costly sampling-based procedures. Simulation studies show that $\\text{C}^4$ achieves higher accuracy and robustness than competing approaches across diverse scenarios. Application to an airport reachability network demonstrates the method's scalability, interpretability, and practical utility for real-world weighted networks."}
{"id": "2511.17091", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17091", "abs": "https://arxiv.org/abs/2511.17091", "authors": ["Mustafa Cavus"], "title": "ggskewboxplots: Enhanced Boxplots for Skewed Data in R", "comment": "16 pages, 4 figures", "summary": "Traditional boxplots are widely used for summarizing and visualizing the distribution of numerical data, yet they exhibit significant limitations when applied to skewed or heavy-tailed distributions, often leading to misclassification of outliers through swamping -- flagging typical observations as outliers -- or masking -- failing to detect true outliers. This paper addresses these limitations by systematically evaluating several alternative boxplots specifically designed to accommodate distributional asymmetry. We introduce ggskewboxplots, an R package that integrates multiple robust and skewness-aware boxplot variants, providing a unified and user-friendly framework for exploratory data analysis. Using extensive Monte Carlo simulations under controlled skewness and kurtosis conditions, implemented via the mosaic approach based on the Skewed Exponential Power distribution, we assess the sensitivity and specificity of each method. Simulation results indicate that classical Tukey-style boxplots are highly prone to swamping and masking, whereas robust skewness-adjusted variants -- particularly those leveraging quartile-based skewness measures or medcouple-based adjustments -- achieve substantially better performance. These findings offer practical guidance for selecting reliable boxplot methods in applied settings and demonstrate how the ggskewboxplots package facilitates accessible, distribution-aware visualizations within the familiar ggplot2 workflow."}
{"id": "2511.17376", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17376", "abs": "https://arxiv.org/abs/2511.17376", "authors": ["Anaïs Andrillon", "Sandrine Micallef", "Moreno Ursino", "Pavel Mozgunov", "Marie-Karelle Riviere"], "title": "U-DESPE: a Bayesian Utility-based methodology for dosing regimen optimization in early-phase oncology trials based on Dose-Exposure, Safety, Pharmacodynamics, Efficacy", "comment": "Main manuscript and Supplementary materials", "summary": "With the development of novel therapies such as molecularly targeted agents and immunotherapy, the maximum tolerated dose paradigm that \"more is better\" does not necessarily hold anymore. In this context, doses and schedules of novel therapies may be inadequately characterized and oncology drug dose-finding approaches should be revised. This is increasingly recognized by health authorities, notably through the Optimus project. We developed a Bayesian dose-finding design, called U-DESPE, which allows to either determine the optimal dosing regimen at the end of the dose-escalation phase, or use of dedicated cohorts for randomizing patients to candidate optimal dosing regimens after that safe dosing regimens have been found. U-DESPE design relies on a dose-exposure model built from pharmacokinetic data using non-linear mixed-effect modeling approaches. Then three models are built to assess the relationships between exposure and the probability of selected relevant endpoints on safety, efficacy, and pharmacodynamics. These models are then combined to predict the different endpoints for every candidate dosing regimens. Finally, a utility function is proposed to quantify the trade-off between these endpoints and to determine the optimal dosing regimen. We applied the proposed method on a clinical trial case study and performed an extensive simulation study to evaluate the operating characteristics of the method."}
{"id": "2511.17379", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17379", "abs": "https://arxiv.org/abs/2511.17379", "authors": ["Lan Wen", "Aaron L. Sarvet", "Jessica G. Young"], "title": "On treating right-censoring events like treatments", "comment": null, "summary": "In causal inference literature, potential outcomes are often indexed by the \"elimination of all right-censoring events,\" leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present."}
{"id": "2511.17379", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17379", "abs": "https://arxiv.org/abs/2511.17379", "authors": ["Lan Wen", "Aaron L. Sarvet", "Jessica G. Young"], "title": "On treating right-censoring events like treatments", "comment": null, "summary": "In causal inference literature, potential outcomes are often indexed by the \"elimination of all right-censoring events,\" leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present."}
{"id": "2511.17463", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17463", "abs": "https://arxiv.org/abs/2511.17463", "authors": ["Jared N. Lakhani"], "title": "Extending the Accelerated Failure Conditionals Model to Location-Scale Families", "comment": "17 pages, 3 figures, 5 tables", "summary": "Arnold and Arvanitis (2020) introduced a novel class of bivariate conditionally specified distributions, in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This conditioning regime was formulated through survival functions and termed the accelerated failure conditionals model. Subsequently, Lakhani (2025) extended this conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, thereby moving beyond families with non-increasing densities. The present study builds on this line of work by proposing a conditional survival specification derived from a location-scale distributional family, where the dependence between $X$ and $Y$ arises not only through the acceleration function but also via a location function. An illustrative example of this new specification is developed using a Weibull marginal for $X$. The resulting models are fully characterized by closed-form expressions for their moments, and simulations are implemented using the Metropolis-Hastings algorithm. Finally, the model is applied to a dataset in which the empirical distribution of $Y$ lies on the real line, demonstrating the models' capacity to accommodate $Y$ marginals defined over $\\mathbb{R}$."}
{"id": "2511.17415", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17415", "abs": "https://arxiv.org/abs/2511.17415", "authors": ["Minshen Xu", "Shiwei Lan", "Lulu Kang"], "title": "Bayesian Bridge Gaussian Process Regression", "comment": null, "summary": "The performance of Gaussian Process (GP) regression is often hampered by the curse of dimensionality, which inflates computational cost and reduces predictive power in high-dimensional problems. Variable selection is thus crucial for building efficient and accurate GP models. Inspired by Bayesian bridge regression, we propose the Bayesian Bridge Gaussian Process Regression (B\\textsuperscript{2}GPR) model. This framework places $\\ell_q$-norm constraints on key GP parameters to automatically induce sparsity and identify active variables. We formulate two distinct versions: one for $q=2$ using conjugate Gaussian priors, and another for $0<q<2$ that employs constrained flat priors, leading to non-standard, norm-constrained posterior distributions. To enable posterior inference, we design a Gibbs sampling algorithm that integrates Spherical Hamiltonian Monte Carlo (SphHMC) to efficiently sample from the constrained posteriors when $0<q<2$. Simulations and a real-data application confirm that B\\textsuperscript{2}GPR offers superior variable selection and prediction compared to alternative approaches."}
{"id": "2511.17438", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.17438", "abs": "https://arxiv.org/abs/2511.17438", "authors": ["Jesse Wheeler", "Aaron J. Abkemeier", "Edward L. Ionides"], "title": "Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models", "comment": null, "summary": "Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis."}
{"id": "2511.17463", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17463", "abs": "https://arxiv.org/abs/2511.17463", "authors": ["Jared N. Lakhani"], "title": "Extending the Accelerated Failure Conditionals Model to Location-Scale Families", "comment": "17 pages, 3 figures, 5 tables", "summary": "Arnold and Arvanitis (2020) introduced a novel class of bivariate conditionally specified distributions, in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This conditioning regime was formulated through survival functions and termed the accelerated failure conditionals model. Subsequently, Lakhani (2025) extended this conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, thereby moving beyond families with non-increasing densities. The present study builds on this line of work by proposing a conditional survival specification derived from a location-scale distributional family, where the dependence between $X$ and $Y$ arises not only through the acceleration function but also via a location function. An illustrative example of this new specification is developed using a Weibull marginal for $X$. The resulting models are fully characterized by closed-form expressions for their moments, and simulations are implemented using the Metropolis-Hastings algorithm. Finally, the model is applied to a dataset in which the empirical distribution of $Y$ lies on the real line, demonstrating the models' capacity to accommodate $Y$ marginals defined over $\\mathbb{R}$."}
