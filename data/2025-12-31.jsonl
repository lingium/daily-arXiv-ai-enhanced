{"id": "2512.22282", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.22282", "abs": "https://arxiv.org/abs/2512.22282", "authors": ["Qianqian Qi", "Peter G. M. van der Heijden"], "title": "A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue", "comment": null, "summary": "Across fields such as machine learning, social science, geography, considerable attention has been given to models that factorize a nonnegative matrix into the product of two or three matrices, subject to nonnegative or row-sum-to-1 constraints. Although these models are to a large extend similar or even equivalent, they are presented under different names, and their similarity is not well known. This paper highlights similarities among five popular models, latent budget analysis (LBA), latent class analysis (LCA), end-member analysis (EMA), probabilistic latent semantic analysis (PLSA), and nonnegative matrix factorization (NMF). We focus on an essential issue-identifiability-of these models and prove that the solution of LBA, EMA, LCA, PLSA is unique if and only if the solution of NMF is unique. We also provide a brief review for algorithms of these models. We illustrate the models with a time budget dataset from social science, and end the paper with a discussion of closely related models such as archetypal analysis."}
{"id": "2512.22284", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22284", "abs": "https://arxiv.org/abs/2512.22284", "authors": ["Ernest Fokoué"], "title": "On Fibonacci Ensembles: An Alternative Approach to Ensemble Learning Inspired by the Timeless Architecture of the Golden Ratio", "comment": "33 pages, 4 figures", "summary": "Nature rarely reveals her secrets bluntly, yet in the Fibonacci sequence she grants us a glimpse of her quiet architecture of growth, harmony, and recursive stability \\citep{Koshy2001Fibonacci, Livio2002GoldenRatio}. From spiral galaxies to the unfolding of leaves, this humble sequence reflects a universal grammar of balance. In this work, we introduce \\emph{Fibonacci Ensembles}, a mathematically principled yet philosophically inspired framework for ensemble learning that complements and extends classical aggregation schemes such as bagging, boosting, and random forests \\citep{Breiman1996Bagging, Breiman2001RandomForests, Friedman2001GBM, Zhou2012Ensemble, HastieTibshiraniFriedman2009ESL}. Two intertwined formulations unfold: (1) the use of normalized Fibonacci weights -- tempered through orthogonalization and Rao--Blackwell optimization -- to achieve systematic variance reduction among base learners, and (2) a second-order recursive ensemble dynamic that mirrors the Fibonacci flow itself, enriching representational depth beyond classical boosting. The resulting methodology is at once rigorous and poetic: a reminder that learning systems flourish when guided by the same intrinsic harmonies that shape the natural world. Through controlled one-dimensional regression experiments using both random Fourier feature ensembles \\citep{RahimiRecht2007RFF} and polynomial ensembles, we exhibit regimes in which Fibonacci weighting matches or improves upon uniform averaging and interacts in a principled way with orthogonal Rao--Blackwellization. These findings suggest that Fibonacci ensembles form a natural and interpretable design point within the broader theory of ensemble learning."}
{"id": "2512.22286", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22286", "abs": "https://arxiv.org/abs/2512.22286", "authors": ["Ernest Fokoué"], "title": "A General Weighting Theory for Ensemble Learning: Beyond Variance Reduction via Spectral and Geometric Structure", "comment": "25 pages", "summary": "Ensemble learning is traditionally justified as a variance-reduction strategy, explaining its strong performance for unstable predictors such as decision trees. This explanation, however, does not account for ensembles constructed from intrinsically stable estimators-including smoothing splines, kernel ridge regression, Gaussian process regression, and other regularized reproducing kernel Hilbert space (RKHS) methods whose variance is already tightly controlled by regularization and spectral shrinkage. This paper develops a general weighting theory for ensemble learning that moves beyond classical variance-reduction arguments. We formalize ensembles as linear operators acting on a hypothesis space and endow the space of weighting sequences with geometric and spectral constraints. Within this framework, we derive a refined bias-variance approximation decomposition showing how non-uniform, structured weights can outperform uniform averaging by reshaping approximation geometry and redistributing spectral complexity, even when variance reduction is negligible. Our main results provide conditions under which structured weighting provably dominates uniform ensembles, and show that optimal weights arise as solutions to constrained quadratic programs. Classical averaging, stacking, and recently proposed Fibonacci-based ensembles appear as special cases of this unified theory, which further accommodates geometric, sub-exponential, and heavy-tailed weighting laws. Overall, the work establishes a principled foundation for structure-driven ensemble learning, explaining why ensembles remain effective for smooth, low-variance base learners and setting the stage for distribution-adaptive and dynamically evolving weighting schemes developed in subsequent work."}
{"id": "2512.22473", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22473", "abs": "https://arxiv.org/abs/2512.22473", "authors": ["Naman Aggarwal", "Siddhartha R. Dalal", "Vishal Misra"], "title": "Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds", "comment": null, "summary": "Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \\emph{advantage-based routing law} for attention scores, \\[ \\frac{\\partial L}{\\partial s_{ij}} = α_{ij}\\bigl(b_{ij}-\\mathbb{E}_{α_i}[b]\\bigr), \\qquad b_{ij} := u_i^\\top v_j, \\] coupled with a \\emph{responsibility-weighted update} for values, \\[ Δv_j = -η\\sum_i α_{ij} u_i, \\] where $u_i$ is the upstream gradient at position $i$ and $α_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning)."}
{"id": "2512.23053", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2512.23053", "abs": "https://arxiv.org/abs/2512.23053", "authors": ["Emanuela Furfaro", "Simone Mosciatti"], "title": "LLteacher: A Tool for the Integration of Generative AI into Statistics Assignments", "comment": null, "summary": "As generative AI becomes increasingly embedded in everyday life, the thoughtful and intentional integration of AI-based tools into statistics education has become essential. We address this need with a focus on homework assignments and we propose the use of LLMs as a companion to complete homework by developing an open-source tool named LLteacher. This LLM-based tool preserves learning processes and it guides students to engage with AI in ways that support their learning, while ensuring alignment with course content and equitable access. We illustrate LLteacher's design and functionality with examples from an undergraduate Statistical Computing course in R, showing how it supports two distinct pedagogical goals: recalling prior knowledge and discovering new concepts. While this is an initial version, LLteacher demonstrates one possible pathway for integrating generative AI into statistics courses, with strong potential for adaptation to other types of classes and assignments."}
{"id": "2512.22153", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22153", "abs": "https://arxiv.org/abs/2512.22153", "authors": ["Nicolas Zilberstein", "Santiago Segarra", "Luiz Chamon"], "title": "Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials", "comment": null, "summary": "We introduce shielded Langevin Monte Carlo (LMC), a constrained sampler inspired by navigation functions, capable of sampling from unnormalized target distributions defined over punctured supports. In other words, this approach samples from non-convex spaces defined as convex sets with convex holes. This defines a novel and challenging problem in constrained sampling. To do so, the sampler incorporates a combination of a spatially adaptive temperature and a repulsive drift to ensure that samples remain within the feasible region. Experiments on a 2D Gaussian mixture and multiple-input multiple-output (MIMO) symbol detection showcase the advantages of the proposed shielded LMC in contrast to unconstrained cases."}
{"id": "2512.22282", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.22282", "abs": "https://arxiv.org/abs/2512.22282", "authors": ["Qianqian Qi", "Peter G. M. van der Heijden"], "title": "A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue", "comment": null, "summary": "Across fields such as machine learning, social science, geography, considerable attention has been given to models that factorize a nonnegative matrix into the product of two or three matrices, subject to nonnegative or row-sum-to-1 constraints. Although these models are to a large extend similar or even equivalent, they are presented under different names, and their similarity is not well known. This paper highlights similarities among five popular models, latent budget analysis (LBA), latent class analysis (LCA), end-member analysis (EMA), probabilistic latent semantic analysis (PLSA), and nonnegative matrix factorization (NMF). We focus on an essential issue-identifiability-of these models and prove that the solution of LBA, EMA, LCA, PLSA is unique if and only if the solution of NMF is unique. We also provide a brief review for algorithms of these models. We illustrate the models with a time budget dataset from social science, and end the paper with a discussion of closely related models such as archetypal analysis."}
{"id": "2512.22472", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22472", "abs": "https://arxiv.org/abs/2512.22472", "authors": ["Wenhao Cui", "Jie Hu"], "title": "Random Subset Averaging", "comment": null, "summary": "We propose a new ensemble prediction method, Random Subset Averaging (RSA), tailored for settings with many covariates, particularly in the presence of strong correlations. RSA constructs candidate models via binomial random subset strategy and aggregates their predictions through a two-round weighting scheme, resulting in a structure analogous to a two-layer neural network. All tuning parameters are selected via cross-validation, requiring no prior knowledge of covariate relevance. We establish the asymptotic optimality of RSA under general conditions, allowing the first-round weights to be data-dependent, and demonstrate that RSA achieves a lower finite-sample risk bound under orthogonal design. Simulation studies demonstrate that RSA consistently delivers superior and stable predictive performance across a wide range of sample sizes, dimensional settings, sparsity levels and correlation structures, outperforming conventional model selection and ensemble learning methods. An empirical application to financial return forecasting further illustrates its practical utility."}
{"id": "2512.22515", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22515", "abs": "https://arxiv.org/abs/2512.22515", "authors": ["Ayad Habib Shemail", "Ahmed Razzaq Al-Lami", "Amal Hadi Rashid"], "title": "Robust Liu-Type Estimation for Multicollinearity in Fuzzy Logistic Regression", "comment": null, "summary": "This article addresses the fuzzy logistic regression model under conditions of multicollinearity, which causes instability and inflated variance in parameter estimation. In this model, both the response variable and parameters are represented as fuzzy triangular numbers. To overcome the multicollinearity problem, various Liu-type estimators were employed: Fuzzy Maximum Likelihood Estimators (FMLE), Fuzzy Logistic Ridge Estimators (FLRE), Fuzzy Logistic Liu Estimators (FLLE), Fuzzy Logistic Liu-type Estimators (FLLTE), and Fuzzy Logistic Liu-type Parameter Estimators (FLLTPE). Through simulations with various sample sizes and application to real fuzzy data on kidney failure, model performance was evaluated using mean square error (MSE) and goodness of fit criteria. Results demonstrated superior performance of FLLTPE and FLLTE compared to other estimators."}
{"id": "2512.22638", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22638", "abs": "https://arxiv.org/abs/2512.22638", "authors": ["Deniz Akdemir"], "title": "Likelihood-Preserving Embeddings for Statistical Inference", "comment": null, "summary": "Modern machine learning embeddings provide powerful compression of high-dimensional data, yet they typically destroy the geometric structure required for classical likelihood-based statistical inference. This paper develops a rigorous theory of likelihood-preserving embeddings: learned representations that can replace raw data in likelihood-based workflows -- hypothesis testing, confidence interval construction, model selection -- without altering inferential conclusions. We introduce the Likelihood-Ratio Distortion metric $Δ_n$, which measures the maximum error in log-likelihood ratios induced by an embedding. Our main theoretical contribution is the Hinge Theorem, which establishes that controlling $Δ_n$ is necessary and sufficient for preserving inference. Specifically, if the distortion satisfies $Δ_n = o_p(1)$, then (i) all likelihood-ratio based tests and Bayes factors are asymptotically preserved, and (ii) surrogate maximum likelihood estimators are asymptotically equivalent to full-data MLEs. We prove an impossibility result showing that universal likelihood preservation requires essentially invertible embeddings, motivating the need for model-class-specific guarantees. We then provide a constructive framework using neural networks as approximate sufficient statistics, deriving explicit bounds connecting training loss to inferential guarantees. Experiments on Gaussian and Cauchy distributions validate the sharp phase transition predicted by exponential family theory, and applications to distributed clinical inference demonstrate practical utility."}
{"id": "2512.23371", "categories": ["stat.OT", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.23371", "abs": "https://arxiv.org/abs/2512.23371", "authors": ["Yilin Bi", "Junhao Bian", "Shuyan Wan", "Shuaijia Wang", "Tao Zhou"], "title": "Domain matters: Towards domain-informed evaluation for link prediction", "comment": null, "summary": "Link prediction, a foundational task in complex network analysis, has extensive applications in critical scenarios such as social recommendation, drug target discovery, and knowledge graph completion. However, existing evaluations of algorithmic often rely on experiments conducted on a limited number of networks, assuming consistent performance rankings across domains. Despite the significant disparities in generative mechanisms and semantic contexts, previous studies often improperly highlight ``universally optimal\" algorithms based solely on naive average over networks across domains. This paper systematically evaluates 12 mainstream link prediction algorithms across 740 real-world networks spanning seven domains. We present substantial empirical evidence elucidating the performance of algorithms in specific domains. This findings reveal a notably low degree of consistency in inter-domain algorithm rankings, a phenomenon that stands in stark contrast to the high degree of consistency observed within individual domains. Principal Component Analysis shows that response vectors formed by the rankings of the 12 algorithms cluster distinctly by domain in low-dimensional space, thus confirming domain attributes as a pivotal factor affecting algorithm performance. We propose a metric called Winner Score that could identify the superior algorithm in each domain: Non-Negative Matrix Factorization for social networks, Neighborhood Overlap-aware Graph Neural Networks for economics, Graph Convolutional Networks for chemistry, and L3-based Resource Allocation for biology. However, these domain-specific top-performing algorithms tend to exhibit suboptimal performance in other domains. This finding underscores the importance of aligning an algorithm's mechanism with the network structure."}
{"id": "2512.22504", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.22504", "abs": "https://arxiv.org/abs/2512.22504", "authors": ["Joyee Ghosh"], "title": "On the Choice of Model Space Priors and Multiplicity Control in Bayesian Variable Selection: An Application to Streaming Logistic Regression", "comment": "25 pages, 8 figures", "summary": "Bayesian variable selection (BVS) depends critically on the specification of a prior distribution over the model space, particularly for controlling sparsity and multiplicity. This paper examines the practical consequences of different model space priors for BVS in logistic regression, with an emphasis on streaming data settings. We review some popular and well-known Beta--Binomial priors alongside the recently proposed matryoshka doll (MD) prior. We introduce a simple approximation to the MD prior that yields independent inclusion indicators and is convenient for scalable inference. Using BIC-based approximations to marginal likelihoods, we compare the effect of different model space priors on posterior inclusion probabilities and coefficient estimation at intermediate and final stages of the data stream via simulation studies. Overall, the results indicate that no single model space prior uniformly dominates across scenarios, and that the recently proposed MD prior provides a useful additional option that occupies an intermediate position between commonly used Beta--Binomial priors with differing degrees of sparsity."}
{"id": "2512.22638", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22638", "abs": "https://arxiv.org/abs/2512.22638", "authors": ["Deniz Akdemir"], "title": "Likelihood-Preserving Embeddings for Statistical Inference", "comment": null, "summary": "Modern machine learning embeddings provide powerful compression of high-dimensional data, yet they typically destroy the geometric structure required for classical likelihood-based statistical inference. This paper develops a rigorous theory of likelihood-preserving embeddings: learned representations that can replace raw data in likelihood-based workflows -- hypothesis testing, confidence interval construction, model selection -- without altering inferential conclusions. We introduce the Likelihood-Ratio Distortion metric $Δ_n$, which measures the maximum error in log-likelihood ratios induced by an embedding. Our main theoretical contribution is the Hinge Theorem, which establishes that controlling $Δ_n$ is necessary and sufficient for preserving inference. Specifically, if the distortion satisfies $Δ_n = o_p(1)$, then (i) all likelihood-ratio based tests and Bayes factors are asymptotically preserved, and (ii) surrogate maximum likelihood estimators are asymptotically equivalent to full-data MLEs. We prove an impossibility result showing that universal likelihood preservation requires essentially invertible embeddings, motivating the need for model-class-specific guarantees. We then provide a constructive framework using neural networks as approximate sufficient statistics, deriving explicit bounds connecting training loss to inferential guarantees. Experiments on Gaussian and Cauchy distributions validate the sharp phase transition predicted by exponential family theory, and applications to distributed clinical inference demonstrate practical utility."}
{"id": "2512.22504", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.22504", "abs": "https://arxiv.org/abs/2512.22504", "authors": ["Joyee Ghosh"], "title": "On the Choice of Model Space Priors and Multiplicity Control in Bayesian Variable Selection: An Application to Streaming Logistic Regression", "comment": "25 pages, 8 figures", "summary": "Bayesian variable selection (BVS) depends critically on the specification of a prior distribution over the model space, particularly for controlling sparsity and multiplicity. This paper examines the practical consequences of different model space priors for BVS in logistic regression, with an emphasis on streaming data settings. We review some popular and well-known Beta--Binomial priors alongside the recently proposed matryoshka doll (MD) prior. We introduce a simple approximation to the MD prior that yields independent inclusion indicators and is convenient for scalable inference. Using BIC-based approximations to marginal likelihoods, we compare the effect of different model space priors on posterior inclusion probabilities and coefficient estimation at intermediate and final stages of the data stream via simulation studies. Overall, the results indicate that no single model space prior uniformly dominates across scenarios, and that the recently proposed MD prior provides a useful additional option that occupies an intermediate position between commonly used Beta--Binomial priors with differing degrees of sparsity."}
{"id": "2512.22892", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.22892", "abs": "https://arxiv.org/abs/2512.22892", "authors": ["Amit N. Sawant", "Mats J. Stensrud"], "title": "Counterfactual Harm: A Counter-argument", "comment": null, "summary": "As AI systems are increasingly used to guide decisions, it is essential that they follow ethical principles. A core principle in medicine is non-maleficence, often equated with ``do no harm''. A formal definition of harm based on counterfactual reasoning has been proposed and popularized. This notion of harm has been promoted in simple settings with binary treatments and outcomes. Here, we highlight a problem with this definition in settings involving multiple treatment options. Illustrated by an example with three tuberculosis treatments (say, A, B, and C), we demonstrate that the counterfactual definition of harm can produce intransitive results: B is less harmful than A, C is less harmful than B, yet C is more harmful than A when compared pairwise. This intransitivity poses a challenge as it may lead to practical (clinical) decisions that are difficult to justify or defend. In contrast, an interventionist definition of harm based on expected utility forgoes counterfactual comparisons and ensures transitive treatment rankings."}
{"id": "2512.22999", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22999", "abs": "https://arxiv.org/abs/2512.22999", "authors": ["Niels Bracher", "Lars Kühmichel", "Desi R. Ivanova", "Xavier Intes", "Paul-Christian Bürkner", "Stefan T. Radev"], "title": "JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference", "comment": null, "summary": "We consider problems of parameter estimation where design variables can be actively optimized to maximize information gain. To this end, we introduce JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, a history network, and an inference network end-to-end. The networks minimize a generic loss that aggregates incremental reductions in posterior error along experimental sequences. Inference networks are instantiated with diffusion-based posterior estimators that can approximate high-dimensional and multimodal posteriors at every experimental step. Across standard adaptive design benchmarks, JADAI achieves superior or competitive performance."}
{"id": "2512.23395", "categories": ["stat.ME", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.23395", "abs": "https://arxiv.org/abs/2512.23395", "authors": ["David Bolin", "Peter Braunsteins", "Sebastian Engelke", "Raphaël Huser"], "title": "Intrinsic Whittle--Matérn fields and sparse spatial extremes", "comment": null, "summary": "Intrinsic Gaussian fields are used in many areas of statistics as models for spatial or spatio-temporal dependence, or as priors for latent variables. However, there are two major gaps in the literature: first, the number and flexibility of existing intrinsic models are very limited; second, theory, fast inference, and software are currently underdeveloped for intrinsic fields. We tackle these challenges by introducing the new flexible class of intrinsic Whittle--Matérn Gaussian random fields obtained as the solution to a stochastic partial differential equation (SPDE). Exploiting sparsity resulting from finite-element approximations, we develop fast estimation and simulation methods for these models. We demonstrate the benefits of this intrinsic SPDE approach for the important task of kriging under extrapolation settings. Leveraging the connection of intrinsic fields to spatial extreme value processes, we translate our theory to an SPDE approach for Brown--Resnick processes for sparse modeling of spatial extreme events. This new paradigm paves the way for efficient inference in unprecedented dimensions. To demonstrate the wide applicability of our new methodology, we apply it in two very different areas: a longitudinal study of renal function data, and the modeling of marine heat waves using high-resolution sea surface temperature data."}
{"id": "2512.23395", "categories": ["stat.ME", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.23395", "abs": "https://arxiv.org/abs/2512.23395", "authors": ["David Bolin", "Peter Braunsteins", "Sebastian Engelke", "Raphaël Huser"], "title": "Intrinsic Whittle--Matérn fields and sparse spatial extremes", "comment": null, "summary": "Intrinsic Gaussian fields are used in many areas of statistics as models for spatial or spatio-temporal dependence, or as priors for latent variables. However, there are two major gaps in the literature: first, the number and flexibility of existing intrinsic models are very limited; second, theory, fast inference, and software are currently underdeveloped for intrinsic fields. We tackle these challenges by introducing the new flexible class of intrinsic Whittle--Matérn Gaussian random fields obtained as the solution to a stochastic partial differential equation (SPDE). Exploiting sparsity resulting from finite-element approximations, we develop fast estimation and simulation methods for these models. We demonstrate the benefits of this intrinsic SPDE approach for the important task of kriging under extrapolation settings. Leveraging the connection of intrinsic fields to spatial extreme value processes, we translate our theory to an SPDE approach for Brown--Resnick processes for sparse modeling of spatial extreme events. This new paradigm paves the way for efficient inference in unprecedented dimensions. To demonstrate the wide applicability of our new methodology, we apply it in two very different areas: a longitudinal study of renal function data, and the modeling of marine heat waves using high-resolution sea surface temperature data."}
{"id": "2512.22655", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22655", "abs": "https://arxiv.org/abs/2512.22655", "authors": ["Jiaming Liu", "Meng Li"], "title": "Bend to Mend: Toward Trustworthy Variational Bayes with Valid Uncertainty Quantification", "comment": null, "summary": "Variational Bayes (VB) is a popular and computationally efficient method to approximate the posterior distribution in Bayesian inference, especially when the exact posterior is analytically intractable and sampling-based approaches are computationally prohibitive. While VB often yields accurate point estimates, its uncertainty quantification (UQ) is known to be unreliable. For example, credible intervals derived from VB posteriors tend to exhibit undercoverage, failing to achieve nominal frequentist coverage probabilities. In this article, we address this challenge by proposing Trustworthy Variational Bayes (TVB), a method to recalibrate the UQ of broad classes of VB procedures. Our approach follows a bend-to-mend strategy: we intentionally misspecify the likelihood to correct VB's flawed UQ. In particular, we first relax VB by building on a recent fractional VB method, and then identify the optimal fraction parameter using conformal techniques such as sample splitting and bootstrapping. This yields recalibrated UQ for any given parameter of interest. On the theoretical front, we establish that the calibrated credible intervals achieve asymptotically correct frequentist coverage for a given parameter of interest; this, to the best of our knowledge, is the first such theoretical guarantee for VB. On the practical front, we introduce the \"TVB table\", which enables (1) massive parallelization and remains agnostic to the parameter of interest during its construction, and (2) efficient identification of the optimal fraction parameter for any specified parameter of interest. The proposed method is illustrated via Gaussian mixture models and Bayesian mixture linear regression models, and numerical experiments demonstrate that the TVB method outperforms standard VB and achieves normal frequentist coverage in finite samples. A real data application is also discussed."}
{"id": "2512.23019", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.23019", "abs": "https://arxiv.org/abs/2512.23019", "authors": ["Afshin Yaghoubi", "Esmaile Khorram", "Omid Naghshineh Arjmand"], "title": "Reliability Analysis of a 1-out-of-n Cold Standby Redundant System under the Generalized Lindley Distribution", "comment": null, "summary": "Cold standby 1-out-of-n redundant systems are well-established models in system reliability engineering. To date, reliability analyses of such systems have predominantly assumed exponential, Erlang, or Weibull failure distributions for their components. The Lindley distribution and its generalizations represent a significant class of statistical distributions in reliability engineering. Certain generalized Lindley distributions, due to the appealing characteristics of their hazard functions, can serve as suitable alternatives to other well-known lifetime distributions like the Weibull. This study investigates the reliability of a 1-out-of-n cold standby redundant system with perfect and imperfect switching, assuming that the active component failure times follow the Generalized Lindley distribution. We derive a closed-form expression for the system reliability. To achieve this, the distribution of the sum of n independent and identically distributed random variables following the Generalized Lindley distribution is first determined using the moment-generating function approach."}
{"id": "2512.23071", "categories": ["stat.ML", "cs.DC", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23071", "abs": "https://arxiv.org/abs/2512.23071", "authors": ["Krishna Harsha Kovelakuntla Huthasana", "Alireza Olama", "Andreas Lundell"], "title": "Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity", "comment": null, "summary": "Federated Learning (FL) is a distributed machine learning setting that requires multiple clients to collaborate on training a model while maintaining data privacy. The unaddressed inherent sparsity in data and models often results in overly dense models and poor generalizability under data and client participation heterogeneity. We propose FL with an L0 constraint on the density of non-zero parameters, achieved through a reparameterization using probabilistic gates and their continuous relaxation: originally proposed for sparsity in centralized machine learning. We show that the objective for L0 constrained stochastic minimization naturally arises from an entropy maximization problem of the stochastic gates and propose an algorithm based on federated stochastic gradient descent for distributed learning. We demonstrate that the target density (rho) of parameters can be achieved in FL, under data and client participation heterogeneity, with minimal loss in statistical performance for linear and non-linear models: Linear regression (LR), Logistic regression (LG), Softmax multi-class classification (MC), Multi-label classification with logistic units (MLC), Convolution Neural Network (CNN) for multi-class classification (MC). We compare the results with a magnitude pruning-based thresholding algorithm for sparsity in FL. Experiments on synthetic data with target density down to rho = 0.05 and publicly available RCV1, MNIST, and EMNIST datasets with target density down to rho = 0.005 demonstrate that our approach is communication-efficient and consistently better in statistical performance."}
{"id": "2512.22659", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22659", "abs": "https://arxiv.org/abs/2512.22659", "authors": ["Nabil Awan", "Richard J. Chappell"], "title": "Ranked Set Sampling in Survival Analysis", "comment": "This manuscript is a preprint prepared for dissemination and feedback. The results are complete, and submission to a peer-reviewed statistics journal is planned", "summary": "Ranked set sampling (RSS) is a cost-efficient study design that uses inexpensive baseline ranking to select a more informative subset of individuals for full measurement. While RSS is well known to improve precision over simple random sampling (SRS) for uncensored outcomes, survival analysis under RSS has largely been limited to estimation of the Kaplan-Meier survival curve under random censoring. Consequently, many standard tools routinely used with SRS data, including log-rank and weighted log-rank tests, restricted mean survival time summaries, and window-based mean life measures, are not yet fully developed for RSS settings, particularly when ranking is imperfect and censoring is present.\n  This work develops a unified survival analysis framework for balanced RSS designs that preserves efficiency gains while providing the inferential tools expected in applied practice. We formalize Kaplan-Meier and Nelson-Aalen estimators for right-censored data under both perfect and concomitant-based imperfect ranking and establish their large-sample properties using martingale and empirical process methods adapted to the rank-wise RSS structure. Rank-aware Greenwood-type variance estimators are proposed, and efficiency relative to SRS is evaluated through simulation studies varying set size, number of cycles, censoring proportion, and ranking quality. The framework is further extended to log-rank and Fleming-Harrington weighted tests, as well as restricted and window mean life functionals with asymptotic variance formulas and two-sample comparisons. An implementation plan with real-data illustrations is provided to facilitate practical use."}
{"id": "2512.23571", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.23571", "abs": "https://arxiv.org/abs/2512.23571", "authors": ["Fendler Julie", "Guihenneuc Chantal", "Ancelet Sophie"], "title": "Considering parallel tempering and comparing post-treatment procedures in Bayesian Profile Regression Models for a survival outcome and correlated exposures", "comment": null, "summary": "Bayesian profile regression mixture models (BPRM) allow to assess a health risk in a multi-exposed population. These mixture models cluster individuals according to their exposure profile and their health risk. However, their results, based on Monte-Carlo Markov Chain (MCMC) algorithms, turned out to be unstable in different application cases. We suppose two reasons for this instability. The MCMC algorithm can be trapped in local modes of the posterior distribution and the choice of post-treatment procedures used on the output of the MCMC algorithm leads to different clustering structures. In this work, we propose improvements of the MCMC algorithms proposed in previous works in order to avoid the local modes of the posterior distribution while reducing the computation time. We also carry out a simulation study to compare the performances of the MCMC algorithms and different post-processing in order to provide guidelines on their use. An application in radiation epidemiology is considered."}
{"id": "2512.23408", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23408", "abs": "https://arxiv.org/abs/2512.23408", "authors": ["Bruno Mlodozeniec", "David Krueger", "Richard E. Turner"], "title": "Probabilistic Modelling is Sufficient for Causal Inference", "comment": null, "summary": "Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \\emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility."}
{"id": "2512.23250", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23250", "abs": "https://arxiv.org/abs/2512.23250", "authors": ["Shaoxin Wang", "Ziyun Ma"], "title": "Robust and Well-conditioned Sparse Estimation for High-dimensional Covariance Matrices", "comment": "25 pages, 2 figures", "summary": "Estimating covariance matrices with high-dimensional complex data presents significant challenges, particularly concerning positive definiteness, sparsity, and numerical stability. Existing robust sparse estimators often fail to guarantee positive definiteness in finite samples, while subsequent positive-definite correction can degrade sparsity and lack explicit control over the condition number. To address these limitations, we propose a novel robust and well-conditioned sparse covariance matrix estimator. Our key innovation is the direct incorporation of a condition number constraint within a robust adaptive thresholding framework. This constraint simultaneously ensures positive definiteness, enforces a controllable level of numerical stability, and preserves the desired sparse structure without resorting to post-hoc modifications that compromise sparsity. We formulate the estimation as a convex optimization problem and develop an efficient alternating direction algorithm with guaranteed convergence. Theoretically, we establish that the proposed estimator achieves the minimax optimal convergence rate under the Frobenius norm. Comprehensive simulations and real-data applications demonstrate that our method consistently produces positive definite, well-conditioned, and sparse estimates, and achieves comparable or superior numerical stability to eigenvalue-bound methods while requiring less tuning parameters."}
{"id": "2512.23596", "categories": ["stat.ML", "cs.LG", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2512.23596", "abs": "https://arxiv.org/abs/2512.23596", "authors": ["Agostino Capponi", "Chengpiao Huang", "J. Antonio Sidaoui", "Kaizheng Wang", "Jiacheng Zou"], "title": "The Nonstationarity-Complexity Tradeoff in Return Prediction", "comment": null, "summary": "We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries."}
{"id": "2512.23251", "categories": ["stat.ME", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.23251", "abs": "https://arxiv.org/abs/2512.23251", "authors": ["Wang Yinbu", "Xu Yong"], "title": "A Wide-Sense Stationarity Test Based on the Geometric Structure of Covariance", "comment": null, "summary": "This paper presents a test for wide-sense stationarity (WSS) based on the geometry of the covariance function. We estimate local patches of the covariance surface and then check whether the directional derivative in the $(1,1,0)$ direction is zero on each patch. The method only requires the covariance function to be locally smooth and does not assume stationarity in advance. It can be applied to general stochastic dynamical systems and provides a time-resolved view. We apply the test method to an SDOF system and to a stochastic Duffing oscillator. These examples show that the method is numerically stable and can detect departures from WSS in practice."}
{"id": "2512.23671", "categories": ["stat.ML", "cs.LG", "math.OC", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23671", "abs": "https://arxiv.org/abs/2512.23671", "authors": ["Tiffany Ding", "Isaac Gibbs", "Ryan J. Tibshirani"], "title": "Calibrated Multi-Level Quantile Forecasting", "comment": null, "summary": "We present an online method for guaranteeing calibration of quantile forecasts at multiple quantile levels simultaneously. A sequence of $α$-level quantile forecasts is calibrated if the forecasts are larger than the target value at an $α$-fraction of time steps. We introduce a lightweight method called Multi-Level Quantile Tracker (MultiQT) that wraps around any existing point or quantile forecaster to produce corrected forecasts guaranteed to achieve calibration, even against adversarial distribution shifts, while ensuring that the forecasts are ordered -- e.g., the 0.5-level quantile forecast is never larger than the 0.6-level forecast. Furthermore, the method comes with a no-regret guarantee that implies it will not worsen the performance of an existing forecaster, asymptotically, with respect to the quantile loss. In experiments, we find that MultiQT significantly improves the calibration of real forecasters in epidemic and energy forecasting problems."}
{"id": "2512.23395", "categories": ["stat.ME", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.23395", "abs": "https://arxiv.org/abs/2512.23395", "authors": ["David Bolin", "Peter Braunsteins", "Sebastian Engelke", "Raphaël Huser"], "title": "Intrinsic Whittle--Matérn fields and sparse spatial extremes", "comment": null, "summary": "Intrinsic Gaussian fields are used in many areas of statistics as models for spatial or spatio-temporal dependence, or as priors for latent variables. However, there are two major gaps in the literature: first, the number and flexibility of existing intrinsic models are very limited; second, theory, fast inference, and software are currently underdeveloped for intrinsic fields. We tackle these challenges by introducing the new flexible class of intrinsic Whittle--Matérn Gaussian random fields obtained as the solution to a stochastic partial differential equation (SPDE). Exploiting sparsity resulting from finite-element approximations, we develop fast estimation and simulation methods for these models. We demonstrate the benefits of this intrinsic SPDE approach for the important task of kriging under extrapolation settings. Leveraging the connection of intrinsic fields to spatial extreme value processes, we translate our theory to an SPDE approach for Brown--Resnick processes for sparse modeling of spatial extreme events. This new paradigm paves the way for efficient inference in unprecedented dimensions. To demonstrate the wide applicability of our new methodology, we apply it in two very different areas: a longitudinal study of renal function data, and the modeling of marine heat waves using high-resolution sea surface temperature data."}
{"id": "2512.23694", "categories": ["stat.ML", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.23694", "abs": "https://arxiv.org/abs/2512.23694", "authors": ["Lars van der Laan", "Nathan Kallus"], "title": "Bellman Calibration for V-Learning in Offline Reinforcement Learning", "comment": null, "summary": "We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model's predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability."}
{"id": "2512.23444", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23444", "abs": "https://arxiv.org/abs/2512.23444", "authors": ["Zina-Sabrina Duma", "Otto Lamminpää", "Jouni Susiluoto", "Heikki Haario", "Ting Zheng", "Tuomas Sihvonen", "Amy Braverman", "Philip A. Townsend", "Satu-Pia Reinikainen"], "title": "Uncertainty calibration for latent-variable regression models", "comment": null, "summary": "Uncertainty quantification is essential for scientific analysis, as it allows for the evaluation and interpretation of variability and reliability in complex systems and datasets. In their original form, multivariate statistical regression models (partial least-squares regression, PLS, principal component regression, PCR) along with their kernelized versions (kernel partial least-squares regression, K-PLS, kernel principal component regression, K-PCR), do not incorporate uncertainty quantification as part of their output. In this study, we propose a method inspired by conformal inference to estimate and calibrate the uncertainty of multivariate statistical models. The result of this method is a point prediction accompanied by prediction intervals that depend on the input data. We tested the proposed method on both traditional and kernelized versions of PLS and PCR. The method is demonstrated using synthetic data, as well as laboratory near-infrared (NIR) and airborne hyperspectral regression models for estimating functional plant traits. The model was able to successfully identify the uncertain regions in the simulated data and match the magnitude of the uncertainty. In real-case scenarios, the optimised model was not overconfident nor underconfident when estimating from test data: for example, for a 95% prediction interval, 95% of the true observations were inside the prediction interval."}
{"id": "2512.22153", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22153", "abs": "https://arxiv.org/abs/2512.22153", "authors": ["Nicolas Zilberstein", "Santiago Segarra", "Luiz Chamon"], "title": "Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials", "comment": null, "summary": "We introduce shielded Langevin Monte Carlo (LMC), a constrained sampler inspired by navigation functions, capable of sampling from unnormalized target distributions defined over punctured supports. In other words, this approach samples from non-convex spaces defined as convex sets with convex holes. This defines a novel and challenging problem in constrained sampling. To do so, the sampler incorporates a combination of a spatially adaptive temperature and a repulsive drift to ensure that samples remain within the feasible region. Experiments on a 2D Gaussian mixture and multiple-input multiple-output (MIMO) symbol detection showcase the advantages of the proposed shielded LMC in contrast to unconstrained cases."}
{"id": "2512.23467", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23467", "abs": "https://arxiv.org/abs/2512.23467", "authors": ["Hajime Ogawa", "Shonosuke Sugasawa"], "title": "Propensity Patchwork Kriging for Scalable Inference on Heterogeneous Treatment Effects", "comment": "24 pages (main) + 11 pages (supplement)", "summary": "Gaussian process-based models are attractive for estimating heterogeneous treatment effects (HTE), but their computational cost limits scalability in causal inference settings. In this work, we address this challenge by extending Patchwork Kriging into the causal inference framework. Our proposed method partitions the data according to the estimated propensity score and applies Patchwork Kriging to enforce continuity of HTE estimates across adjacent regions. By imposing continuity constraints only along the propensity score dimension, rather than the full covariate space, the proposed approach substantially reduces computational cost while avoiding discontinuities inherent in simple local approximations. The resulting method can be interpreted as a smoothing extension of stratification and provides an efficient approach to HTE estimation. The proposed method is demonstrated through simulation studies and a real data application."}
{"id": "2512.23474", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23474", "abs": "https://arxiv.org/abs/2512.23474", "authors": ["Junyu Chen", "Pratik Nag", "Huixia Judy-Wang", "Ying Sun"], "title": "Deep classifier kriging for probabilistic spatial prediction of air quality index", "comment": null, "summary": "Accurate spatial interpolation of the air quality index (AQI), computed from concentrations of multiple air pollutants, is essential for regulatory decision-making, yet AQI fields are inherently non-Gaussian and often exhibit complex nonlinear spatial structure. Classical spatial prediction methods such as kriging are linear and rely on Gaussian assumptions, which limits their ability to capture these features and to provide reliable predictive distributions. In this study, we propose \\textit{deep classifier kriging} (DCK), a flexible, distribution-free deep learning framework for estimating full predictive distribution functions for univariate and bivariate spatial processes, together with a \\textit{data fusion} mechanism that enables modeling of non-collocated bivariate processes and integration of heterogeneous air pollution data sources. Through extensive simulation experiments, we show that DCK consistently outperforms conventional approaches in predictive accuracy and uncertainty quantification. We further apply DCK to probabilistic spatial prediction of AQI by fusing sparse but high-quality station observations with spatially continuous yet biased auxiliary model outputs, yielding spatially resolved predictive distributions that support downstream tasks such as exceedance and extreme-event probability estimation for regulatory risk assessment and policy formulation."}
{"id": "2512.23474", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23474", "abs": "https://arxiv.org/abs/2512.23474", "authors": ["Junyu Chen", "Pratik Nag", "Huixia Judy-Wang", "Ying Sun"], "title": "Deep classifier kriging for probabilistic spatial prediction of air quality index", "comment": null, "summary": "Accurate spatial interpolation of the air quality index (AQI), computed from concentrations of multiple air pollutants, is essential for regulatory decision-making, yet AQI fields are inherently non-Gaussian and often exhibit complex nonlinear spatial structure. Classical spatial prediction methods such as kriging are linear and rely on Gaussian assumptions, which limits their ability to capture these features and to provide reliable predictive distributions. In this study, we propose \\textit{deep classifier kriging} (DCK), a flexible, distribution-free deep learning framework for estimating full predictive distribution functions for univariate and bivariate spatial processes, together with a \\textit{data fusion} mechanism that enables modeling of non-collocated bivariate processes and integration of heterogeneous air pollution data sources. Through extensive simulation experiments, we show that DCK consistently outperforms conventional approaches in predictive accuracy and uncertainty quantification. We further apply DCK to probabilistic spatial prediction of AQI by fusing sparse but high-quality station observations with spatially continuous yet biased auxiliary model outputs, yielding spatially resolved predictive distributions that support downstream tasks such as exceedance and extreme-event probability estimation for regulatory risk assessment and policy formulation."}
{"id": "2512.23567", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.23567", "abs": "https://arxiv.org/abs/2512.23567", "authors": ["Liyuan Cui", "Guanhao Feng", "Yuefeng Han", "Jiayan Li"], "title": "Panel Coupled Matrix-Tensor Clustering Model with Applications to Asset Pricing", "comment": null, "summary": "We tackle the challenge of estimating grouping structures and factor loadings in asset pricing models, where traditional regressions struggle due to sparse data and high noise. Existing approaches, such as those using fused penalties and multi-task learning, often enforce coefficient homogeneity across cross-sectional units, reducing flexibility. Clustering methods (e.g., spectral clustering, Lloyd's algorithm) achieve consistent recovery under specific conditions but typically rely on a single data source. To address these limitations, we introduce the Panel Coupled Matrix-Tensor Clustering (PMTC) model, which simultaneously leverages a characteristics tensor and a return matrix to identify latent asset groups. By integrating these data sources, we develop computationally efficient tensor clustering algorithms that enhance both clustering accuracy and factor loading estimation. Simulations demonstrate that our methods outperform single-source alternatives in clustering accuracy and coefficient estimation, particularly under moderate signal-to-noise conditions. Empirical application to U.S. equities demonstrates the practical value of PMTC, yielding higher out-of-sample total $R^2$ and economically interpretable variation in factor exposures."}
{"id": "2512.23571", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.23571", "abs": "https://arxiv.org/abs/2512.23571", "authors": ["Fendler Julie", "Guihenneuc Chantal", "Ancelet Sophie"], "title": "Considering parallel tempering and comparing post-treatment procedures in Bayesian Profile Regression Models for a survival outcome and correlated exposures", "comment": null, "summary": "Bayesian profile regression mixture models (BPRM) allow to assess a health risk in a multi-exposed population. These mixture models cluster individuals according to their exposure profile and their health risk. However, their results, based on Monte-Carlo Markov Chain (MCMC) algorithms, turned out to be unstable in different application cases. We suppose two reasons for this instability. The MCMC algorithm can be trapped in local modes of the posterior distribution and the choice of post-treatment procedures used on the output of the MCMC algorithm leads to different clustering structures. In this work, we propose improvements of the MCMC algorithms proposed in previous works in order to avoid the local modes of the posterior distribution while reducing the computation time. We also carry out a simulation study to compare the performances of the MCMC algorithms and different post-processing in order to provide guidelines on their use. An application in radiation epidemiology is considered."}
{"id": "2512.23581", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23581", "abs": "https://arxiv.org/abs/2512.23581", "authors": ["Courtney Kyger", "James Fernandez", "John A. Grunenwald", "James Braun", "Annie Booth"], "title": "Profile Bayesian Optimization for Expensive Computer Experiments", "comment": null, "summary": "We propose a novel Bayesian optimization (BO) procedure aimed at identifying the ``profile optima'' of a deterministic black-box computer simulation that has a single control parameter and multiple nuisance parameters. The profile optima capture the optimal response values as a function of the control parameter. Our objective is to identify them across the entire plausible range of the control parameter. Classic BO, which targets a single optimum over all parameters, does not explore the entire control parameter range. Instead, we develop a novel two-stage acquisition scheme to balance exploration across the control parameter and exploitation of the profile optima, leveraging deep and shallow Gaussian process surrogates to facilitate uncertainty quantification. We are motivated by a computer simulation of a diffuser in a rotating detonation combustion engine, which returns the energy lost through diffusion as a function of various design parameters. We aim to identify the lowest possible energy loss as a function of the diffuser's length; understanding this relationship will enable well-informed design choices. Our ``profile Bayesian optimization'' procedure outperforms traditional BO and profile optimization methods on a variety of benchmarks and proves effective in our motivating application."}
{"id": "2512.23627", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23627", "abs": "https://arxiv.org/abs/2512.23627", "authors": ["Nithisha Suryadevara", "Vivek Reddy Srigiri"], "title": "Joint Modeling of Longitudinal and Survival Data: A Bayesian Approach for Predicting Disease Progression", "comment": "Preprint. Simulation study and application to a real-world clinical dataset", "summary": "Joint modeling of longitudinal and survival data has become increasingly important in medical research, particularly for understanding disease progression in chronic conditions where both repeated biomarker measurements and time-to-event outcomes are available. Traditional two-stage methods, which analyze longitudinal and survival components separately, often result in biased estimates and suboptimal predictions due to failure to account for their interdependence.\n  In this study, we propose a Bayesian hierarchical joint modeling framework with an emphasis on predictive evaluation and clinical interpretability. The model simultaneously characterizes the longitudinal trajectory of a biomarker and the associated survival outcome through shared random effects, capturing the intrinsic association between disease dynamics and event risk. The Bayesian formulation allows flexible incorporation of prior information, accommodates irregular measurement times and missing data, and provides full posterior distributions for uncertainty quantification via credible intervals.\n  We evaluate the proposed framework using both simulated data designed to mimic realistic patient trajectories and a real-world clinical dataset involving patients with chronic liver disease. Results demonstrate that the Bayesian joint model consistently outperforms conventional two-stage approaches in terms of parameter estimation accuracy and predictive performance, as measured by time-dependent area under the curve and Brier scores. The proposed approach provides a robust and interpretable tool for dynamic, patient-specific prognosis, supporting clinical decision-making in personalized medicine."}
{"id": "2512.22515", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22515", "abs": "https://arxiv.org/abs/2512.22515", "authors": ["Ayad Habib Shemail", "Ahmed Razzaq Al-Lami", "Amal Hadi Rashid"], "title": "Robust Liu-Type Estimation for Multicollinearity in Fuzzy Logistic Regression", "comment": null, "summary": "This article addresses the fuzzy logistic regression model under conditions of multicollinearity, which causes instability and inflated variance in parameter estimation. In this model, both the response variable and parameters are represented as fuzzy triangular numbers. To overcome the multicollinearity problem, various Liu-type estimators were employed: Fuzzy Maximum Likelihood Estimators (FMLE), Fuzzy Logistic Ridge Estimators (FLRE), Fuzzy Logistic Liu Estimators (FLLE), Fuzzy Logistic Liu-type Estimators (FLLTE), and Fuzzy Logistic Liu-type Parameter Estimators (FLLTPE). Through simulations with various sample sizes and application to real fuzzy data on kidney failure, model performance was evaluated using mean square error (MSE) and goodness of fit criteria. Results demonstrated superior performance of FLLTPE and FLLTE compared to other estimators."}
{"id": "2512.22638", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22638", "abs": "https://arxiv.org/abs/2512.22638", "authors": ["Deniz Akdemir"], "title": "Likelihood-Preserving Embeddings for Statistical Inference", "comment": null, "summary": "Modern machine learning embeddings provide powerful compression of high-dimensional data, yet they typically destroy the geometric structure required for classical likelihood-based statistical inference. This paper develops a rigorous theory of likelihood-preserving embeddings: learned representations that can replace raw data in likelihood-based workflows -- hypothesis testing, confidence interval construction, model selection -- without altering inferential conclusions. We introduce the Likelihood-Ratio Distortion metric $Δ_n$, which measures the maximum error in log-likelihood ratios induced by an embedding. Our main theoretical contribution is the Hinge Theorem, which establishes that controlling $Δ_n$ is necessary and sufficient for preserving inference. Specifically, if the distortion satisfies $Δ_n = o_p(1)$, then (i) all likelihood-ratio based tests and Bayes factors are asymptotically preserved, and (ii) surrogate maximum likelihood estimators are asymptotically equivalent to full-data MLEs. We prove an impossibility result showing that universal likelihood preservation requires essentially invertible embeddings, motivating the need for model-class-specific guarantees. We then provide a constructive framework using neural networks as approximate sufficient statistics, deriving explicit bounds connecting training loss to inferential guarantees. Experiments on Gaussian and Cauchy distributions validate the sharp phase transition predicted by exponential family theory, and applications to distributed clinical inference demonstrate practical utility."}
{"id": "2512.23408", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23408", "abs": "https://arxiv.org/abs/2512.23408", "authors": ["Bruno Mlodozeniec", "David Krueger", "Richard E. Turner"], "title": "Probabilistic Modelling is Sufficient for Causal Inference", "comment": null, "summary": "Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \\emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility."}
{"id": "2512.23671", "categories": ["stat.ML", "cs.LG", "math.OC", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23671", "abs": "https://arxiv.org/abs/2512.23671", "authors": ["Tiffany Ding", "Isaac Gibbs", "Ryan J. Tibshirani"], "title": "Calibrated Multi-Level Quantile Forecasting", "comment": null, "summary": "We present an online method for guaranteeing calibration of quantile forecasts at multiple quantile levels simultaneously. A sequence of $α$-level quantile forecasts is calibrated if the forecasts are larger than the target value at an $α$-fraction of time steps. We introduce a lightweight method called Multi-Level Quantile Tracker (MultiQT) that wraps around any existing point or quantile forecaster to produce corrected forecasts guaranteed to achieve calibration, even against adversarial distribution shifts, while ensuring that the forecasts are ordered -- e.g., the 0.5-level quantile forecast is never larger than the 0.6-level forecast. Furthermore, the method comes with a no-regret guarantee that implies it will not worsen the performance of an existing forecaster, asymptotically, with respect to the quantile loss. In experiments, we find that MultiQT significantly improves the calibration of real forecasters in epidemic and energy forecasting problems."}
