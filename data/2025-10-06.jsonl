{"id": "2510.02405", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH", "47A55, 15A18, 15-03"], "pdf": "https://arxiv.org/pdf/2510.02405", "abs": "https://arxiv.org/abs/2510.02405", "authors": ["Oussama Ounissi", "Nicklas Jävergård", "Adrian Muntean"], "title": "Orthogonal Procrustes problem preserves correlations in synthetic data", "comment": null, "summary": "This work introduces the application of the Orthogonal Procrustes problem to\nthe generation of synthetic data. The proposed methodology ensures that the\nresulting synthetic data preserves important statistical relationships among\nfeatures, specifically the Pearson correlation. An empirical illustration using\na large, real-world, tabular dataset of energy consumption demonstrates the\neffectiveness of the approach and highlights its potential for application in\npractical synthetic data generation. Our approach is not meant to replace\nexisting generative models, but rather as a lightweight post-processing step\nthat enforces exact Pearson correlation to an already generated synthetic\ndataset."}
{"id": "2510.02420", "categories": ["stat.ML", "cs.DM", "cs.LG", "math.CO", "math.LO", "math.ST", "stat.TH", "68Q32, 05C65, 05C35, 03C45"], "pdf": "https://arxiv.org/pdf/2510.02420", "abs": "https://arxiv.org/abs/2510.02420", "authors": ["Artem Chernikov", "Henry Towsner"], "title": "Higher-arity PAC learning, VC dimension and packing lemma", "comment": "12 pages, 1 figure", "summary": "The aim of this note is to overview some of our work in Chernikov, Towsner'20\n(arXiv:2010.00726) developing higher arity VC theory (VC$_n$ dimension),\nincluding a generalization of Haussler packing lemma, and an associated tame\n(slice-wise) hypergraph regularity lemma; and to demonstrate that it\ncharacterizes higher arity PAC learning (PAC$_n$ learning) in $n$-fold product\nspaces with respect to product measures introduced by Kobayashi, Kuriyama and\nTakeuchi'15. We also point out how some of the recent results in\narXiv:2402.14294, arXiv:2505.15688, arXiv:2509.20404 follow from our work in\narXiv:2010.00726."}
{"id": "2510.02471", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02471", "abs": "https://arxiv.org/abs/2510.02471", "authors": ["Rina Foygel Barber", "Ashwin Pananjady"], "title": "Predictive inference for time series: why is split conformal effective despite temporal dependence?", "comment": "22 pages", "summary": "We consider the problem of uncertainty quantification for prediction in a\ntime series: if we use past data to forecast the next time point, can we\nprovide valid prediction intervals around our forecasts? To avoid placing\ndistributional assumptions on the data, in recent years the conformal\nprediction method has been a popular approach for predictive inference, since\nit provides distribution-free coverage for any iid or exchangeable data\ndistribution. However, in the time series setting, the strong empirical\nperformance of conformal prediction methods is not well understood, since even\nshort-range temporal dependence is a strong violation of the exchangeability\nassumption. Using predictors with \"memory\" -- i.e., predictors that utilize\npast observations, such as autoregressive models -- further exacerbates this\nproblem. In this work, we examine the theoretical properties of split conformal\nprediction in the time series setting, including the case where predictors may\nhave memory. Our results bound the loss of coverage of these methods in terms\nof a new \"switch coefficient\", measuring the extent to which temporal\ndependence within the time series creates violations of exchangeability. Our\ncharacterization of the coverage probability is sharp over the class of\nstationary, $\\beta$-mixing processes. Along the way, we introduce tools that\nmay prove useful in analyzing other predictive inference methods for dependent\ndata."}
{"id": "2510.02420", "categories": ["stat.ML", "cs.DM", "cs.LG", "math.CO", "math.LO", "math.ST", "stat.TH", "68Q32, 05C65, 05C35, 03C45"], "pdf": "https://arxiv.org/pdf/2510.02420", "abs": "https://arxiv.org/abs/2510.02420", "authors": ["Artem Chernikov", "Henry Towsner"], "title": "Higher-arity PAC learning, VC dimension and packing lemma", "comment": "12 pages, 1 figure", "summary": "The aim of this note is to overview some of our work in Chernikov, Towsner'20\n(arXiv:2010.00726) developing higher arity VC theory (VC$_n$ dimension),\nincluding a generalization of Haussler packing lemma, and an associated tame\n(slice-wise) hypergraph regularity lemma; and to demonstrate that it\ncharacterizes higher arity PAC learning (PAC$_n$ learning) in $n$-fold product\nspaces with respect to product measures introduced by Kobayashi, Kuriyama and\nTakeuchi'15. We also point out how some of the recent results in\narXiv:2402.14294, arXiv:2505.15688, arXiv:2509.20404 follow from our work in\narXiv:2010.00726."}
{"id": "2510.02471", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02471", "abs": "https://arxiv.org/abs/2510.02471", "authors": ["Rina Foygel Barber", "Ashwin Pananjady"], "title": "Predictive inference for time series: why is split conformal effective despite temporal dependence?", "comment": "22 pages", "summary": "We consider the problem of uncertainty quantification for prediction in a\ntime series: if we use past data to forecast the next time point, can we\nprovide valid prediction intervals around our forecasts? To avoid placing\ndistributional assumptions on the data, in recent years the conformal\nprediction method has been a popular approach for predictive inference, since\nit provides distribution-free coverage for any iid or exchangeable data\ndistribution. However, in the time series setting, the strong empirical\nperformance of conformal prediction methods is not well understood, since even\nshort-range temporal dependence is a strong violation of the exchangeability\nassumption. Using predictors with \"memory\" -- i.e., predictors that utilize\npast observations, such as autoregressive models -- further exacerbates this\nproblem. In this work, we examine the theoretical properties of split conformal\nprediction in the time series setting, including the case where predictors may\nhave memory. Our results bound the loss of coverage of these methods in terms\nof a new \"switch coefficient\", measuring the extent to which temporal\ndependence within the time series creates violations of exchangeability. Our\ncharacterization of the coverage probability is sharp over the class of\nstationary, $\\beta$-mixing processes. Along the way, we introduce tools that\nmay prove useful in analyzing other predictive inference methods for dependent\ndata."}
{"id": "2510.02318", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.02318", "abs": "https://arxiv.org/abs/2510.02318", "authors": ["Michael C. Donohue", "Kedir Hussen", "Oliver Langford", "Richard Gallardo", "Gustavo Jimenez-Maggiora", "Paul S. Aisen"], "title": "Alzheimer's Clinical Research Data via R Packages: the alzverse", "comment": null, "summary": "Sharing clinical research data is essential for advancing research in\nAlzheimer's disease (AD) and other therapeutic areas. However, challenges in\ndata accessibility, standardization, documentation, usability, and\nreproducibility continue to impede this goal. In this article, we highlight the\nadvantages of using R packages to overcome these challenges using two examples.\nThe A4LEARN R package includes data from a randomized trial (the Anti-Amyloid\nTreatment in Asymptomatic Alzheimer's [A4] study) and its companion\nobservational study of biomarker negative individuals (the Longitudinal\nEvaluation of Amyloid Risk and Neurodegeneration [LEARN] study). The ADNIMERGE2\nR package includes data from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI), a longitudinal observational biomarker and imaging study. These\npackages collect data, documentation, and reproducible analysis vignettes into\na portable bundle that can be installed and browsed within commonly used R\nprogramming environments. We also introduce the alzverse package which\nleverages a common data standard to combine study-specific data packages to\nfacilitate meta-analyses. By promoting collaboration, transparency, and\nreproducibility, R data packages can play a vital role in accelerating clinical\nresearch."}
{"id": "2510.02316", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.02316", "abs": "https://arxiv.org/abs/2510.02316", "authors": ["Jimin Kim"], "title": "Typhoon Path Prediction Using Functional Data Analysis and Clustering-Based Regression", "comment": null, "summary": "Accurate prediction of typhoon trajectories is essential for mitigating the\nimpact of these extreme weather events. This study proposes a functional data\nanalysis (FDA) framework for modeling and forecasting typhoon paths using\nhistorical trajectory data. Latitude and longitude sequences are represented as\nsmooth functional observations, and a function-on-function regression model is\nemployed to capture the temporal dynamics of typhoon movement. While the\nbaseline model demonstrates reasonable performance for typical bow-shaped\ntrajectories, it exhibits limited accuracy for non-standard paths. To address\nthis limitation, a clustering-based extension is introduced, wherein typhoons\nare grouped by trajectory shape prior to regression. This two-stage approach\nimproves predictive accuracy by enabling localized modeling adapted to\nstructural variations in the data. The results demonstrate the practical\nutility of combining FDA with clustering for robust and flexible typhoon\ntrajectory forecasting."}
{"id": "2510.02499", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02499", "abs": "https://arxiv.org/abs/2510.02499", "authors": ["Kulunu Dharmakeerthi", "Yousef El-Laham", "Henry H. Wong", "Vamsi K. Potluru", "Changhong He", "Taosong He"], "title": "Beyond Linear Diffusions: Improved Representations for Rare Conditional Generative Modeling", "comment": null, "summary": "Diffusion models have emerged as powerful generative frameworks with\nwidespread applications across machine learning and artificial intelligence\nsystems. While current research has predominantly focused on linear diffusions,\nthese approaches can face significant challenges when modeling a conditional\ndistribution, $P(Y|X=x)$, when $P(X=x)$ is small. In these regions, few\nsamples, if any, are available for training, thus modeling the corresponding\nconditional density may be difficult. Recognizing this, we show it is possible\nto adapt the data representation and forward scheme so that the sample\ncomplexity of learning a score-based generative model is small in low\nprobability regions of the conditioning space. Drawing inspiration from\nconditional extreme value theory we characterize this method precisely in the\nspecial case in the tail regions of the conditioning variable, $X$. We show how\ndiffusion with a data-driven choice of nonlinear drift term is best suited to\nmodel tail events under an appropriate representation of the data. Through\nempirical validation on two synthetic datasets and a real-world financial\ndataset, we demonstrate that our tail-adaptive approach significantly\noutperforms standard diffusion models in accurately capturing response\ndistributions at the extreme tail conditions."}
{"id": "2510.02664", "categories": ["stat.CO", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.02664", "abs": "https://arxiv.org/abs/2510.02664", "authors": ["Jianhong Xu"], "title": "HOMC: A MATLAB Package for Higher Order Markov Chains", "comment": null, "summary": "We present a MATLAB package, which is the first of its kind, for Higher Order\nMarkov Chains (HOMC). It can be used to easily compute all important quantities\nin our recent works relevant to higher order Markov chains, such as the\n$k$-step transition tensor, limiting probability distribution, ever-reaching\nprobability tensor, and mean first passage time tensor. It can also be used to\ncheck whether a higher order chain is ergodic or regular, to construct the\ntransition matrix of the associated reduced first order chain, and to determine\nwhether a state is recurrent or transient. A key function in the package is an\nimplementation of the tensor ``box'' product which has a probabilistic\ninterpretation and is different from other tensor products in the literature.\nThis HOMC package is useful to researchers and practitioners alike for tasks\nsuch as numerical experimentation and algorithm prototyping involving higher\norder Markov chains."}
{"id": "2510.02612", "categories": ["stat.AP", "math.PR", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.02612", "abs": "https://arxiv.org/abs/2510.02612", "authors": ["Subhayan De", "Tianhao Yu", "Patrick T. Brewick", "Erik A. Johnson", "Steven F. Wojtkiewicz"], "title": "Model Falsification for Predicting Dynamical Responses of Uncertain Structural Systems", "comment": "50 pages, 22 figures", "summary": "Accurate prediction of dynamical response of structural system depends on the\ncorrect modeling of that system. However, modeling becomes increasingly\nchallenging when there are many candidate models available to describe the\nsystem behavior. Furthermore, uncertainties can be present even for the\nparameters of these model classes. The plausibility of each input-output model\nclass of the structures with uncertain components can be determined by a\nBayesian approach from measured dynamic responses to one or more input records;\npredictions of the structural system response to alternate input records can\nthen be made. However, this approach may require many model simulations, even\nthough most of those model classes are quite implausible. An approach is\nproposed herein to use a bound, computed from the false discovery rate, on the\nlikelihood of measured data to falsify models considering uncertainties in the\npassive control devices that do not reproduce the measured data to sufficient\naccuracy. Response prediction is then performed using the unfalsified models in\nan approximate Bayesian sense by assigning weights, computed from the\nlikelihoods, only to the unfalsified models approach incurring only a fraction\nof the computational cost of the standard Bayesian approach. The proposed\napproach for response prediction is illustrated using three structural\nexamples: an earthquake-excited four--degree-of-freedom building model with a\nhysteretic isolation layer; a 1623--degree-of-freedom three-dimensional\nbuilding model, with tuned mass dampers attached to its roof, subjected to wind\nloads; and a full-scale four-story base-isolated building tested on world's\nlargest shake table in Japan's E-Defense lab. The results exhibit accurate\nresponse predictions and significant computational savings, thereby\nillustrating the potential of the proposed method."}
{"id": "2510.02405", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH", "47A55, 15A18, 15-03"], "pdf": "https://arxiv.org/pdf/2510.02405", "abs": "https://arxiv.org/abs/2510.02405", "authors": ["Oussama Ounissi", "Nicklas Jävergård", "Adrian Muntean"], "title": "Orthogonal Procrustes problem preserves correlations in synthetic data", "comment": null, "summary": "This work introduces the application of the Orthogonal Procrustes problem to\nthe generation of synthetic data. The proposed methodology ensures that the\nresulting synthetic data preserves important statistical relationships among\nfeatures, specifically the Pearson correlation. An empirical illustration using\na large, real-world, tabular dataset of energy consumption demonstrates the\neffectiveness of the approach and highlights its potential for application in\npractical synthetic data generation. Our approach is not meant to replace\nexisting generative models, but rather as a lightweight post-processing step\nthat enforces exact Pearson correlation to an already generated synthetic\ndataset."}
{"id": "2510.02513", "categories": ["stat.ML", "cs.DS", "cs.LG", "cs.NA", "math.NA", "stat.CO", "65F55, 68W20"], "pdf": "https://arxiv.org/pdf/2510.02513", "abs": "https://arxiv.org/abs/2510.02513", "authors": ["Ethan N. Epperly"], "title": "Adaptive randomized pivoting and volume sampling", "comment": "13 pages, 2 figures", "summary": "Adaptive randomized pivoting (ARP) is a recently proposed and highly\neffective algorithm for column subset selection. This paper reinterprets the\nARP algorithm by drawing connections to the volume sampling distribution and\nactive learning algorithms for linear regression. As consequences, this paper\npresents new analysis for the ARP algorithm and faster implementations using\nrejection sampling."}
{"id": "2510.03226", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03226", "abs": "https://arxiv.org/abs/2510.03226", "authors": ["Filippo Ascolani", "Giacomo Zanella"], "title": "A fast non-reversible sampler for Bayesian finite mixture models", "comment": null, "summary": "Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known\nthat sampling from the resulting posterior distribution can be a hard task. In\nparticular, popular reversible Markov chain Monte Carlo schemes are often slow\nto converge when the number of observations $n$ is large. In this paper we\nintroduce a novel and simple non-reversible sampling scheme for Bayesian finite\nmixture models, which is shown to drastically outperform classical samplers in\nmany scenarios of interest, especially during convergence phase and when\ncomponents in the mixture have non-negligible overlap. At the theoretical\nlevel, we show that the performance of the proposed non-reversible scheme\ncannot be worse than the standard one, in terms of asymptotic variance, by more\nthan a factor of four; and we provide a scaling limit analysis suggesting that\nthe non-reversible sampler can reduce the convergence time from O$(n^2)$ to\nO$(n)$. We also discuss why the statistical features of mixture models make\nthem an ideal case for the use of non-reversible discrete samplers."}
{"id": "2510.02841", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.02841", "abs": "https://arxiv.org/abs/2510.02841", "authors": ["Helena Coggan", "Anne Bischops", "Pradip Chaudhari", "Yuval Barak-Corren", "Andrew M. Fine", "Ben Y. Reis", "Jaya Aysola", "William G. La Cava"], "title": "Deciphering the influence of demographic factors on the treatment of pediatric patients in the emergency department", "comment": "Accepted as an oral presentation at the Pacific Symposium on\n  Biocomputing 2026", "summary": "Persistent demographic disparities have been identified in the treatment of\npatients seeking care in the emergency department (ED). These may be driven in\npart by subconscious biases, which providers themselves may struggle to\nidentify. To better understand the operation of these biases, we performed a\nretrospective cross-sectional analysis using electronic health records\ndescribing 339,400 visits to the ED of a single US pediatric medical center\nbetween 2019-2024. Odds ratios were calculated using propensity-score matching.\nAnalyses were adjusted for confounding variables, including chief complaint,\ninsurance type, socio-economic deprivation, and patient comorbidities. We also\ntrained a machine learning [ML] model on this dataset to identify predictors of\nadmission. We found significant demographic disparities in admission\n(Non-Hispanic Black [NHB] relative to Non-Hispanic White [NHW]: OR 0.77, 95\\%\nCI 0.73-0.81; Hispanic relative to NHW: OR 0.80, 95\\% CI 0.76-0.83). We also\nidentified disparities in individual decisions taken during the ED stay. For\nexample, NHB patients were significantly less likely than NHW patients to be\nassigned an `emergent' triage acuity score of (OR 0.70, 95\\% CI 0.67-0.72), but\nemergent NHB patients were also significantly less likely to be admitted than\nNHW patients with the same triage acuity (OR 0.86, 95\\% CI 0.80-0.93).\nDemographic disparities were particularly acute wherever patients had normal\nvital signs, public insurance, moderate socio-economic deprivation, or a home\naddress distant from the hospital. An ML model assigned higher importance to\ntriage score for NHB than NHW patients when predicting admission, reflecting\nthese disparities in assignment. We conclude that many visit characteristics,\nclinical and otherwise, may influence the operation of subconscious biases and\naffect ML-driven decision support tools."}
{"id": "2510.02529", "categories": ["stat.ME", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.02529", "abs": "https://arxiv.org/abs/2510.02529", "authors": ["Jiabao He", "S. Joe Qin", "Håkan Hjalmarsson"], "title": "Bridging the Prediction Error Method and Subspace Identification: A Weighted Null Space Fitting Method", "comment": null, "summary": "Subspace identification methods (SIMs) have proven to be very useful and\nnumerically robust for building state-space models. While most SIMs are\nconsistent, few if any can achieve the efficiency of the maximum likelihood\nestimate (MLE). Conversely, the prediction error method (PEM) with a quadratic\ncriteria is equivalent to MLE, but it comes with non-convex optimization\nproblems and requires good initialization points. This contribution proposes a\nweighted null space fitting (WNSF) approach for estimating state-space models,\ncombining some key advantages of the two aforementioned mainstream approaches.\nIt starts with a least-squares estimate of a high-order ARX model, and then a\nmulti-step least-squares procedure reduces the model to a state-space model on\ncanoncial form. It is demonstrated through statistical analysis that when a\ncanonical parameterization is admissible, the proposed method is consistent and\nasymptotically efficient, thereby making progress on the long-standing open\nproblem about the existence of an asymptotically efficient SIM. Numerical and\npractical examples are provided to illustrate that the proposed method performs\nfavorable in comparison with SIMs."}
{"id": "2510.02532", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02532", "abs": "https://arxiv.org/abs/2510.02532", "authors": ["Shuo Huang", "Hippolyte Labarrière", "Ernesto De Vito", "Tomaso Poggio", "Lorenzo Rosasco"], "title": "Learning Multi-Index Models with Hyper-Kernel Ridge Regression", "comment": null, "summary": "Deep neural networks excel in high-dimensional problems, outperforming models\nsuch as kernel methods, which suffer from the curse of dimensionality. However,\nthe theoretical foundations of this success remain poorly understood. We follow\nthe idea that the compositional structure of the learning task is the key\nfactor determining when deep networks outperform other approaches. Taking a\nstep towards formalizing this idea, we consider a simple compositional model,\nnamely the multi-index model (MIM). In this context, we introduce and study\nhyper-kernel ridge regression (HKRR), an approach blending neural networks and\nkernel methods. Our main contribution is a sample complexity result\ndemonstrating that HKRR can adaptively learn MIM, overcoming the curse of\ndimensionality. Further, we exploit the kernel nature of the estimator to\ndevelop ad hoc optimization approaches. Indeed, we contrast alternating\nminimization and alternating gradient methods both theoretically and\nnumerically. These numerical results complement and reinforce our theoretical\nfindings."}
{"id": "2510.02513", "categories": ["stat.ML", "cs.DS", "cs.LG", "cs.NA", "math.NA", "stat.CO", "65F55, 68W20"], "pdf": "https://arxiv.org/pdf/2510.02513", "abs": "https://arxiv.org/abs/2510.02513", "authors": ["Ethan N. Epperly"], "title": "Adaptive randomized pivoting and volume sampling", "comment": "13 pages, 2 figures", "summary": "Adaptive randomized pivoting (ARP) is a recently proposed and highly\neffective algorithm for column subset selection. This paper reinterprets the\nARP algorithm by drawing connections to the volume sampling distribution and\nactive learning algorithms for linear regression. As consequences, this paper\npresents new analysis for the ARP algorithm and faster implementations using\nrejection sampling."}
{"id": "2510.02852", "categories": ["stat.AP", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.02852", "abs": "https://arxiv.org/abs/2510.02852", "authors": ["Maryam Akbari-Moghaddam", "Douglas G. Down", "Na Li", "Catherine Eastwood", "Ayman Abou Mehrem", "Alexandra Howlett"], "title": "Data-Driven Bed Occupancy Planning in Intensive Care Units Using $M_t/G_t/\\infty$ Queueing Models", "comment": "This paper has been submitted to the Operations Research, Data\n  Analytics and Logistics journal", "summary": "Hospitals struggle to make effective long-term capacity planning decisions\nfor intensive care units (ICUs) under uncertainty in future demand. Admission\nrates fluctuate over time due to temporal factors, and length of stay (LOS)\ndistributions vary with patient heterogeneity, hospital location, case mix, and\nclinical practices. Common planning approaches rely on steady-state queueing\nmodels or heuristic rules that assume fixed parameters, but these methods often\nfall short in capturing real-world occupancy dynamics. One widely used example\nis the 85\\% occupancy rule, which recommends maintaining average utilization\nbelow this level to ensure responsiveness; however, this rule is based on\nstationary assumptions and may be unreliable when applied to time-varying\nsystems. Our analysis shows that even when long-run utilization targets are\nmet, day-to-day occupancy frequently exceeds 100\\% capacity.\n  We propose a data-driven framework for estimating ICU bed occupancy using an\n$M_t/G_t/\\infty$ queueing model, which incorporates time-varying arrival rates\nand empirically estimated LOS distributions. The framework combines statistical\ndecomposition and parametric distribution fitting to capture temporal patterns\nin ICU admissions and LOS. We apply it to multi-year data from neonatal ICUs\n(NICUs) in Calgary as a case study. Several capacity planning scenarios are\nevaluated, including average-based thresholds and surge estimates from Poisson\noverflow approximations. Results demonstrate the inadequacy of static\nheuristics in environments with fluctuating demand and highlight the importance\nof modeling LOS variability when estimating bed needs. Although the case study\nfocuses on NICUs, the methodology generalizes to other ICU settings and\nprovides interpretable, data-informed support for healthcare systems facing\nrising demand and limited capacity."}
{"id": "2510.02618", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.02618", "abs": "https://arxiv.org/abs/2510.02618", "authors": ["Carlos A. Pasquier", "Luis A. Barboza"], "title": "Amortized Bayesian Inference for Spatio-Temporal Extremes: A Copula Factor Model with Autoregression", "comment": null, "summary": "We develop a Bayesian spatio-temporal framework for extreme-value analysis\nthat augments a hierarchical copula model with an autoregressive factor to\ncapture residual temporal dependence in threshold exceedances. The factor can\nbe specified as spatially varying or spatially constant, and the scale\nparameter incorporates scientifically relevant covariates (e.g., longitude,\nlatitude, altitude), enabling flexible representation of geographic\nheterogeneity. To avoid the computational burden of the full censored\nlikelihood, we design a Gibbs sampler that embeds amortized neural posterior\nestimation within each parameter block, yielding scalable inference with full\nposterior uncertainty for parameters, predictive quantiles, and return levels.\nSimulation studies demonstrate that the approach improves MCMC mixing and\nestimation accuracy relative to baseline specifications, particularly when\nusing moderately more complex network architectures, while preserving\nheavy-tail behavior. We illustrate the methodology with daily precipitation in\nGuanacaste, Costa Rica, evaluating a suite of nested models and selecting the\nbest-performing factor combination via out-of-sample diagnostics. The chosen\nspecification reveals coherent spatial patterns in multi-year return periods\nand provides actionable information for infrastructure planning and\nclimate-risk management in a tropical dry region strongly influenced by\nclimatic factors. The proposed Gibbs scheme generalizes to other settings where\nparameters can be partitioned into inferentially homogeneous blocks and\nconditionals learned via amortized, likelihood-free methods."}
{"id": "2510.02757", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02757", "abs": "https://arxiv.org/abs/2510.02757", "authors": ["Robert A. Crowell", "Florian Krach", "Josef Teichmann"], "title": "Neural Jump ODEs as Generative Models", "comment": null, "summary": "In this work, we explore how Neural Jump ODEs (NJODEs) can be used as\ngenerative models for It\\^o processes. Given (discrete observations of) samples\nof a fixed underlying It\\^o process, the NJODE framework can be used to\napproximate the drift and diffusion coefficients of the process. Under standard\nregularity assumptions on the It\\^o processes, we prove that, in the limit, we\nrecover the true parameters with our approximation. Hence, using these learned\ncoefficients to sample from the corresponding It\\^o process generates, in the\nlimit, samples with the same law as the true underlying process. Compared to\nother generative machine learning models, our approach has the advantage that\nit does not need adversarial training and can be trained solely as a predictive\nmodel on the observed samples without the need to generate any samples during\ntraining to empirically approximate the distribution. Moreover, the NJODE\nframework naturally deals with irregularly sampled data with missing values as\nwell as with path-dependent dynamics, allowing to apply this approach in\nreal-world settings. In particular, in the case of path-dependent coefficients\nof the It\\^o processes, the NJODE learns their optimal approximation given the\npast observations and therefore allows generating new paths conditionally on\ndiscrete, irregular, and incomplete past observations in an optimal way."}
{"id": "2510.02612", "categories": ["stat.AP", "math.PR", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.02612", "abs": "https://arxiv.org/abs/2510.02612", "authors": ["Subhayan De", "Tianhao Yu", "Patrick T. Brewick", "Erik A. Johnson", "Steven F. Wojtkiewicz"], "title": "Model Falsification for Predicting Dynamical Responses of Uncertain Structural Systems", "comment": "50 pages, 22 figures", "summary": "Accurate prediction of dynamical response of structural system depends on the\ncorrect modeling of that system. However, modeling becomes increasingly\nchallenging when there are many candidate models available to describe the\nsystem behavior. Furthermore, uncertainties can be present even for the\nparameters of these model classes. The plausibility of each input-output model\nclass of the structures with uncertain components can be determined by a\nBayesian approach from measured dynamic responses to one or more input records;\npredictions of the structural system response to alternate input records can\nthen be made. However, this approach may require many model simulations, even\nthough most of those model classes are quite implausible. An approach is\nproposed herein to use a bound, computed from the false discovery rate, on the\nlikelihood of measured data to falsify models considering uncertainties in the\npassive control devices that do not reproduce the measured data to sufficient\naccuracy. Response prediction is then performed using the unfalsified models in\nan approximate Bayesian sense by assigning weights, computed from the\nlikelihoods, only to the unfalsified models approach incurring only a fraction\nof the computational cost of the standard Bayesian approach. The proposed\napproach for response prediction is illustrated using three structural\nexamples: an earthquake-excited four--degree-of-freedom building model with a\nhysteretic isolation layer; a 1623--degree-of-freedom three-dimensional\nbuilding model, with tuned mass dampers attached to its roof, subjected to wind\nloads; and a full-scale four-story base-isolated building tested on world's\nlargest shake table in Japan's E-Defense lab. The results exhibit accurate\nresponse predictions and significant computational savings, thereby\nillustrating the potential of the proposed method."}
{"id": "2510.03074", "categories": ["stat.AP", "astro-ph.IM", "cs.CV", "62P35", "G.3"], "pdf": "https://arxiv.org/pdf/2510.03074", "abs": "https://arxiv.org/abs/2510.03074", "authors": ["Jeffrey Regier"], "title": "Neural Posterior Estimation with Autoregressive Tiling for Detecting Objects in Astronomical Images", "comment": null, "summary": "Upcoming astronomical surveys will produce petabytes of high-resolution\nimages of the night sky, providing information about billions of stars and\ngalaxies. Detecting and characterizing the astronomical objects in these images\nis a fundamental task in astronomy -- and a challenging one, as most of these\nobjects are faint and many visually overlap with other objects. We propose an\namortized variational inference procedure to solve this instance of\nsmall-object detection. Our key innovation is a family of spatially\nautoregressive variational distributions that partition and order the latent\nspace according to a $K$-color checkerboard pattern. By construction, the\nconditional independencies of this variational family mirror those of the\nposterior distribution. We fit the variational distribution, which is\nparameterized by a convolutional neural network, using neural posterior\nestimation (NPE) to minimize an expectation of the forward KL divergence. Using\nimages from the Sloan Digital Sky Survey, our method achieves state-of-the-art\nperformance. We further demonstrate that the proposed autoregressive structure\ngreatly improves posterior calibration."}
{"id": "2510.02628", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02628", "abs": "https://arxiv.org/abs/2510.02628", "authors": ["Shuangshuang Xu", "Marco A. R. Ferreira", "Allison N. Tegge"], "title": "What is in the model? A Comparison of variable selection criteria and model search approaches", "comment": null, "summary": "For many scientific questions, understanding the underlying mechanism is the\ngoal. To help investigators better understand the underlying mechanism,\nvariable selection is a crucial step that permits the identification of the\nmost associated regression variables of interest. A variable selection method\nconsists of model evaluation using an information criterion and a search of the\nmodel space. Here, we provide a comprehensive comparison of variable selection\nmethods using performance measures of correct identification rate (CIR),\nrecall, and false discovery rate (FDR). We consider the BIC and AIC for\nevaluating models, and exhaustive, greedy, LASSO path, and stochastic search\napproaches for searching the model space; we also consider LASSO using cross\nvalidation. We perform simulation studies for linear and generalized linear\nmodels that parametrically explore a wide range of realistic sample sizes,\neffect sizes, and correlations among regression variables. We consider model\nspaces with a small and larger number of potential regressors. The results show\nthat the exhaustive search BIC and stochastic search BIC outperform the other\nmethods when considering the performance measures on small and large model\nspaces, respectively. These approaches result in the highest CIR and lowest\nFDR, which collectively may support long-term efforts towards increasing\nreplicability in research."}
{"id": "2510.02405", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH", "47A55, 15A18, 15-03"], "pdf": "https://arxiv.org/pdf/2510.02405", "abs": "https://arxiv.org/abs/2510.02405", "authors": ["Oussama Ounissi", "Nicklas Jävergård", "Adrian Muntean"], "title": "Orthogonal Procrustes problem preserves correlations in synthetic data", "comment": null, "summary": "This work introduces the application of the Orthogonal Procrustes problem to\nthe generation of synthetic data. The proposed methodology ensures that the\nresulting synthetic data preserves important statistical relationships among\nfeatures, specifically the Pearson correlation. An empirical illustration using\na large, real-world, tabular dataset of energy consumption demonstrates the\neffectiveness of the approach and highlights its potential for application in\npractical synthetic data generation. Our approach is not meant to replace\nexisting generative models, but rather as a lightweight post-processing step\nthat enforces exact Pearson correlation to an already generated synthetic\ndataset."}
{"id": "2510.03131", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03131", "abs": "https://arxiv.org/abs/2510.03131", "authors": ["Mengqi Chen", "Charita Dellaporta", "Thomas B. Berrett", "Theodoros Damoulas"], "title": "Total Robustness in Bayesian Nonlinear Regression for Measurement Error Problems under Model Misspecification", "comment": "77 pages, 13 figures", "summary": "Modern regression analyses are often undermined by covariate measurement\nerror, misspecification of the regression model, and misspecification of the\nmeasurement error distribution. We present, to the best of our knowledge, the\nfirst Bayesian nonparametric framework targeting total robustness that tackles\nall three challenges in general nonlinear regression. The framework assigns a\nDirichlet process prior to the latent covariate-response distribution and\nupdates it with posterior pseudo-samples of the latent covariates, thereby\nproviding the Dirichlet process posterior with observation-informed latent\ninputs and yielding estimators that minimise the discrepancy between Dirichlet\nprocess realisations and the model-induced joint law. This design allows\npractitioners to (i) encode prior beliefs, (ii) choose between pseudo-sampling\nlatent covariates or working directly with error-prone observations, and (iii)\ntune the influence of prior and data. We establish generalisation bounds that\ntighten whenever the prior or pseudo-sample generator aligns with the\nunderlying data generating process, ensuring robustness without sacrificing\nconsistency. A gradient-based algorithm enables efficient computations;\nsimulations and two real-world studies show lower estimation error and reduced\nestimation sensitivity to misspecification compared to Bayesian and frequentist\ncompetitors. The framework, therefore, offers a practical and interpretable\nparadigm for trustworthy regression when data and models are jointly imperfect."}
{"id": "2510.02628", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02628", "abs": "https://arxiv.org/abs/2510.02628", "authors": ["Shuangshuang Xu", "Marco A. R. Ferreira", "Allison N. Tegge"], "title": "What is in the model? A Comparison of variable selection criteria and model search approaches", "comment": null, "summary": "For many scientific questions, understanding the underlying mechanism is the\ngoal. To help investigators better understand the underlying mechanism,\nvariable selection is a crucial step that permits the identification of the\nmost associated regression variables of interest. A variable selection method\nconsists of model evaluation using an information criterion and a search of the\nmodel space. Here, we provide a comprehensive comparison of variable selection\nmethods using performance measures of correct identification rate (CIR),\nrecall, and false discovery rate (FDR). We consider the BIC and AIC for\nevaluating models, and exhaustive, greedy, LASSO path, and stochastic search\napproaches for searching the model space; we also consider LASSO using cross\nvalidation. We perform simulation studies for linear and generalized linear\nmodels that parametrically explore a wide range of realistic sample sizes,\neffect sizes, and correlations among regression variables. We consider model\nspaces with a small and larger number of potential regressors. The results show\nthat the exhaustive search BIC and stochastic search BIC outperform the other\nmethods when considering the performance measures on small and large model\nspaces, respectively. These approaches result in the highest CIR and lowest\nFDR, which collectively may support long-term efforts towards increasing\nreplicability in research."}
{"id": "2510.03175", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.03175", "abs": "https://arxiv.org/abs/2510.03175", "authors": ["Michael J. Martens", "Qinghua Lian", "Brent R. Logan"], "title": "TITE-Safety: Time-to-event Safety Monitoring for Clinical Trials", "comment": null, "summary": "Safety evaluation is an essential component of clinical trials. To protect\nstudy participants, these studies often implement safety stopping rules that\nwill halt the trial if an excessive number of toxicity events occur. Existing\nsafety monitoring methods often treat these events as binary outcomes. A\nstrategy that instead handles these as time-to-event endpoints can offer higher\npower and a reduced time to signal of excess risk, but must manage additional\ncomplexities including censoring and competing risks. We propose the\nTITE-Safety approach for safety monitoring, which incorporates time-to-event\ninformation while handling censored observations and competing risks\nappropriately. This strategy is applied to develop stopping rules using score\ntests, Bayesian beta-extended binomial models, and sequential probability ratio\ntests. The operating characteristics of these methods are studied via\nsimulation for common phase 2 and 3 trial scenarios. Across simulation\nsettings, the proposed techniques offer reductions in expected toxicities of\n20% or more compared to binary data methods and maintain the type I error rate\nnear the nominal level across various event time distributions. These methods\nare demonstrated through a redesign of the safety monitoring scheme for BMT CTN\n0601, a single arm, phase 2 trial that evaluated bone marrow transplant as\ntreatment for severe sickle cell disease. Our R package \"stoppingrule\" offers\nfunctions to construct and evaluate these stopping rules, providing valuable\ntools for trial design to investigators."}
{"id": "2510.03131", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03131", "abs": "https://arxiv.org/abs/2510.03131", "authors": ["Mengqi Chen", "Charita Dellaporta", "Thomas B. Berrett", "Theodoros Damoulas"], "title": "Total Robustness in Bayesian Nonlinear Regression for Measurement Error Problems under Model Misspecification", "comment": "77 pages, 13 figures", "summary": "Modern regression analyses are often undermined by covariate measurement\nerror, misspecification of the regression model, and misspecification of the\nmeasurement error distribution. We present, to the best of our knowledge, the\nfirst Bayesian nonparametric framework targeting total robustness that tackles\nall three challenges in general nonlinear regression. The framework assigns a\nDirichlet process prior to the latent covariate-response distribution and\nupdates it with posterior pseudo-samples of the latent covariates, thereby\nproviding the Dirichlet process posterior with observation-informed latent\ninputs and yielding estimators that minimise the discrepancy between Dirichlet\nprocess realisations and the model-induced joint law. This design allows\npractitioners to (i) encode prior beliefs, (ii) choose between pseudo-sampling\nlatent covariates or working directly with error-prone observations, and (iii)\ntune the influence of prior and data. We establish generalisation bounds that\ntighten whenever the prior or pseudo-sample generator aligns with the\nunderlying data generating process, ensuring robustness without sacrificing\nconsistency. A gradient-based algorithm enables efficient computations;\nsimulations and two real-world studies show lower estimation error and reduced\nestimation sensitivity to misspecification compared to Bayesian and frequentist\ncompetitors. The framework, therefore, offers a practical and interpretable\nparadigm for trustworthy regression when data and models are jointly imperfect."}
{"id": "2510.03226", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03226", "abs": "https://arxiv.org/abs/2510.03226", "authors": ["Filippo Ascolani", "Giacomo Zanella"], "title": "A fast non-reversible sampler for Bayesian finite mixture models", "comment": null, "summary": "Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known\nthat sampling from the resulting posterior distribution can be a hard task. In\nparticular, popular reversible Markov chain Monte Carlo schemes are often slow\nto converge when the number of observations $n$ is large. In this paper we\nintroduce a novel and simple non-reversible sampling scheme for Bayesian finite\nmixture models, which is shown to drastically outperform classical samplers in\nmany scenarios of interest, especially during convergence phase and when\ncomponents in the mixture have non-negligible overlap. At the theoretical\nlevel, we show that the performance of the proposed non-reversible scheme\ncannot be worse than the standard one, in terms of asymptotic variance, by more\nthan a factor of four; and we provide a scaling limit analysis suggesting that\nthe non-reversible sampler can reduce the convergence time from O$(n^2)$ to\nO$(n)$. We also discuss why the statistical features of mixture models make\nthem an ideal case for the use of non-reversible discrete samplers."}
{"id": "2510.03226", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03226", "abs": "https://arxiv.org/abs/2510.03226", "authors": ["Filippo Ascolani", "Giacomo Zanella"], "title": "A fast non-reversible sampler for Bayesian finite mixture models", "comment": null, "summary": "Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known\nthat sampling from the resulting posterior distribution can be a hard task. In\nparticular, popular reversible Markov chain Monte Carlo schemes are often slow\nto converge when the number of observations $n$ is large. In this paper we\nintroduce a novel and simple non-reversible sampling scheme for Bayesian finite\nmixture models, which is shown to drastically outperform classical samplers in\nmany scenarios of interest, especially during convergence phase and when\ncomponents in the mixture have non-negligible overlap. At the theoretical\nlevel, we show that the performance of the proposed non-reversible scheme\ncannot be worse than the standard one, in terms of asymptotic variance, by more\nthan a factor of four; and we provide a scaling limit analysis suggesting that\nthe non-reversible sampler can reduce the convergence time from O$(n^2)$ to\nO$(n)$. We also discuss why the statistical features of mixture models make\nthem an ideal case for the use of non-reversible discrete samplers."}
