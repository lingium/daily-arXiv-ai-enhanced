{"id": "2512.14272", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14272", "abs": "https://arxiv.org/abs/2512.14272", "authors": ["Brian Buckley", "Adrian O'Hagan", "Marie Galligan"], "title": "A variational Bayes latent class approach for EHR-based patient phenotyping in R", "comment": "19 pages, 6 figures, to be submitted to The Journal of Statistical Software", "summary": "The VBphenoR package for R provides a closed-form variational Bayes approach to patient phenotyping using Electronic Health Records (EHR) data. We implement a variational Bayes Gaussian Mixture Model (GMM) algorithm using closed-form coordinate ascent variational inference (CAVI) to determine the patient phenotype latent class. We then implement a variational Bayes logistic regression, where we determine the probability of the phenotype in the supplied EHR cohort, the shift in biomarkers for patients with the phenotype of interest versus a healthy population and evaluate predictive performance of binary indicator clinical codes and medication codes. The logistic model likelihood applies the latent class from the GMM step to inform the conditional."}
{"id": "2512.14512", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.14512", "abs": "https://arxiv.org/abs/2512.14512", "authors": ["Kezhuo Li", "Marco Grzegorczyk"], "title": "Two Bayesian Approaches to Dynamic Gaussian Bayesian Networks with Intra- and Inter-Slice Edges", "comment": "24 pages, 12 figures", "summary": "Gaussian Dynamic Bayesian Networks (GDBNs) are a widely used tool for learning network structures from continuous time-series data. To capture both time-lagged and contemporaneous dependencies, advanced GDBNs allow for dynamic inter-slice edges as well as static intra-slice edges. In the literature, two Bayesian modeling approaches have been developed for GDBNs. Both build on and extend the well-known Gaussian BGe score. We refer to them as the mean-adjusted BGe (mBGe) and the extended BGe (eBGe) models. In this paper, we contrast the two models and compare their performance empirically. The main finding of our study is that the two models induce different equivalence classes of network structures. In particular, the equivalence classes implied by the eBGe model are non-standard, and we propose a new variant of the DAG-to-CPDAG algorithm to identify them. To the best of our knowledge, these non-standard equivalence classes have not been previously reported."}
{"id": "2512.13962", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.13962", "abs": "https://arxiv.org/abs/2512.13962", "authors": ["Benjamin Christoffersen", "Keith Humphreys", "Alessandro Gasparini", "Birzhan Akynkozhayev", "Hedvig Kjellström", "Mark Clements"], "title": "Joint Models with Multiple Markers and Multiple Time-to-event Outcomes Using Variational Approximations", "comment": null, "summary": "Joint models are well suited to modelling linked data from laboratories and health registers. However, there are few examples of joint models that allow for (a) multiple markers, (b) multiple survival outcomes (including terminal events, competing events, and recurrent events), (c) delayed entry and (d) scalability. We propose a full likelihood approach for joint models based on a Gaussian variational approximation to satisfy criteria (a)-(d). We provide an open-source implementation for this approach, allowing for flexible sets of models for the longitudinal markers and survival outcomes. Through simulations, we find that the lower bound for the variational approximation is close to the full likelihood. We also find that our approach and implementation are fast and scalable. We provide an application with a joint model for longitudinal measurements of dense and fatty breast tissue and time to first breast cancer diagnosis. The use of variational approximations provides a promising approach for extending current joint models."}
{"id": "2512.14308", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.14308", "abs": "https://arxiv.org/abs/2512.14308", "authors": ["Šimon Kucharský", "Aayush Mishra", "Daniel Habermann", "Stefan T. Radev", "Paul-Christian Bürkner"], "title": "Improving the Accuracy of Amortized Model Comparison with Self-Consistency", "comment": "17 pages, 9 figures", "summary": "Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification."}
{"id": "2512.13892", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13892", "abs": "https://arxiv.org/abs/2512.13892", "authors": ["Albert Dorador"], "title": "One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing", "comment": null, "summary": "Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner."}
{"id": "2512.13763", "categories": ["stat.AP", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13763", "abs": "https://arxiv.org/abs/2512.13763", "authors": ["Huw Llewelyn"], "title": "Understanding statistics for biomedical research through the lens of replication", "comment": "25 pages, 3 figures, 2 tables. This paper includes a large amount of work that was done subsequently after comments and supersedes a previous paper submitted to arXiv (Reference number: 2403.16906. I have not deleted or replaced the latter in case the moderators prefer both papers to be readable side-by-side", "summary": "Clinicians and scientists have traditionally focussed on whether their findings will be replicated and are very familiar with the concept. The probability that a replication study yields an effect with the same sign, or the same statistical significance as an original study depends on the sum of the variances of the effect estimates. On this basis, when P equals 0.025 one-sided and the replication study has the same sample size and variance as the original study, the probability of achieving a one-sided P is less than or equal to 0.025 a second time is only about 0.283, consistent with currently observed modest replication rates. A higher replication probability would require a larger sample size than that derived from current single variance power calculations. However, if the replication study is based on an infinitely large sample size and thus has negligible variance then the probability that its estimated mean is same sign is 1 - P = 0.975. The reasoning is made clearer by changing continuous distributions to discretised scales and probability masses, thus avoiding ambiguity and improper flat priors. This perspective is consistent with Frequentist and Bayesian interpretations and also requires further reasoning when testing scientific hypotheses and making decisions."}
{"id": "2512.13841", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13841", "abs": "https://arxiv.org/abs/2512.13841", "authors": ["Eduardo Gutiérrez-Peña", "Carlos Octavio Pérez-Mendoza", "Alan Riva Palacio", "Arno Siri-Jégousse"], "title": "Parameter Estimation for Partially Observed Stable Continuous-State Branching Processes", "comment": null, "summary": "In this article, we present a novel inference framework for estimating the parameters of Continuous-State Branching Processes (CSBPs). We do so by leveraging their subordinator representation. Our method reformulates the estimation problem by shifting the stochastic dynamics to the associated subordinator, enabling a parametric estimation procedure without requiring additional assumptions. This reformulation allows for efficient numerical recovery of the likelihood function via Laplace transform inversion, even in models where closed-form transition densities are unavailable. In addition to offering a flexible approach to parameter estimation, we propose a dynamic simulation framework that generates discrete-time trajectories of CSBPs using the same subordinator-based structure."}
{"id": "2512.13763", "categories": ["stat.AP", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13763", "abs": "https://arxiv.org/abs/2512.13763", "authors": ["Huw Llewelyn"], "title": "Understanding statistics for biomedical research through the lens of replication", "comment": "25 pages, 3 figures, 2 tables. This paper includes a large amount of work that was done subsequently after comments and supersedes a previous paper submitted to arXiv (Reference number: 2403.16906. I have not deleted or replaced the latter in case the moderators prefer both papers to be readable side-by-side", "summary": "Clinicians and scientists have traditionally focussed on whether their findings will be replicated and are very familiar with the concept. The probability that a replication study yields an effect with the same sign, or the same statistical significance as an original study depends on the sum of the variances of the effect estimates. On this basis, when P equals 0.025 one-sided and the replication study has the same sample size and variance as the original study, the probability of achieving a one-sided P is less than or equal to 0.025 a second time is only about 0.283, consistent with currently observed modest replication rates. A higher replication probability would require a larger sample size than that derived from current single variance power calculations. However, if the replication study is based on an infinitely large sample size and thus has negligible variance then the probability that its estimated mean is same sign is 1 - P = 0.975. The reasoning is made clearer by changing continuous distributions to discretised scales and probability masses, thus avoiding ambiguity and improper flat priors. This perspective is consistent with Frequentist and Bayesian interpretations and also requires further reasoning when testing scientific hypotheses and making decisions."}
{"id": "2512.14399", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14399", "abs": "https://arxiv.org/abs/2512.14399", "authors": ["Dániel Pfeifer", "Edith Alice Kovács"], "title": "Trunc-Opt vine building algorithms", "comment": null, "summary": "Vine copula models have become highly popular and practical tools for modelling multivariate probability distributions due to their flexibility in modelling different kinds of dependences between the random variables involved. However, their flexibility comes with the drawback of a high-dimensional parameter space. To tackle this problem, truncated vine copulas were introduced by Kurowicka (2010) (Gaussian case) and Brechmann and Czado (2013) (general case). Truncated vine copulas contain conditionally independent pair copulas after the truncation level. So far, in the general case, truncated vine constructing algorithms started from the lowest tree in order to encode the largest dependences in the lower trees. The novelty of this paper starts from the observation that a truncated vine is determined by the first tree after the truncation level (see Kovács and Szántai (2017)). This paper introduces a new score for fitting truncated vines to given data, called the Weight of the truncated vine. Then we propose a completely new methodology for constructing truncated vines. We prove theorems which motivate this new approach. While earlier algorithms did not use conditional independences, we give algorithms for constructing and encoding truncated vines which do exploit them. Finally, we illustrate the algorithms on real datasets and compare the results with well-known methods included in R packages. Our method generally compare favorably to previously known methods."}
{"id": "2512.13997", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13997", "abs": "https://arxiv.org/abs/2512.13997", "authors": ["Aaron Wei", "Milad Jalali", "Danica J. Sutherland"], "title": "Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics", "comment": null, "summary": "Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations."}
{"id": "2512.13992", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13992", "abs": "https://arxiv.org/abs/2512.13992", "authors": ["Jyotishka Datta", "Nick Polson", "Vadim Sokolov"], "title": "Bayesian Global-Local Regularization", "comment": null, "summary": "We propose a unified framework for global-local regularization that bridges the gap between classical techniques -- such as ridge regression and the nonnegative garotte -- and modern Bayesian hierarchical modeling. By estimating local regularization strengths via marginal likelihood under order constraints, our approach generalizes Stein's positive-part estimator and provides a principled mechanism for adaptive shrinkage in high-dimensional settings. We establish that this isotonic empirical Bayes estimator achieves near-minimax risk (up to logarithmic factors) over sparse ordered model classes, constituting a significant advance in high-dimensional statistical inference. Applications to orthogonal polynomial regression demonstrate the methodology's flexibility, while our theoretical results clarify the connections between empirical Bayes, shape-constrained estimation, and degrees-of-freedom adjustments."}
{"id": "2512.13875", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13875", "abs": "https://arxiv.org/abs/2512.13875", "authors": ["Michael C. Stanley", "Peter W. Spaeth", "James E. Warner", "Matthew R. Webster"], "title": "Bond strength uncertainty quantification via confidence intervals for nondestructive evaluation of bonded composites", "comment": null, "summary": "As bonded composite materials are used more frequently for aerospace applications, it is necessary to certify that parts achieve desired levels of certain physical characteristics (e.g., strength) for safety and performance. Nondestructive evaluation (NDE) of adhesively bonded structures enables verification of bond physical characteristics, but uncertainty quantification (UQ) of NDE estimates is crucial for understanding risks, especially for NDE estimates like bond strength. To address the critical need for NDE UQ for adhesive bond strength estimates, we propose an optimization--based approach to computing finite--sample confidence intervals showing the range of bond strengths that could feasibly be produced by the observed data. A statistical inverse model approach is used to compute a confidence interval of specimen interfacial stiffness from swept--frequency ultrasonic phase observations and a method for propagating the interval to bond strength via a known interfacial stiffness regression is proposed. This approach requires innovating the optimization--based confidence interval to handle both a nonlinear forward model and unknown variance and developing a calibration approach to ensure that the final bond strength interval achieves at least the desired coverage level. Using model assumptions in line with current literature, we demonstrate our approach on simulated measurement data using a variety of low to high noise settings under two prototypical parameter settings. Relative to a baseline approach, we show that our method achieves better coverage and smaller intervals in high--noise settings and when a nuisance parameter is near the constraint boundary."}
{"id": "2512.14292", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.14292", "abs": "https://arxiv.org/abs/2512.14292", "authors": ["Emiliano Ceccarelli", "Jorge Castillo-Mateo", "Sandra Gudžiūnaitė", "Giada Minelli", "Giovanna Jona Lasinio", "Marta Blangiardo"], "title": "A two-stage approach to heat-mortality risk assessment comparing multiple exposure-to-temperature models: the case study in Lazio, Italy", "comment": null, "summary": "This study investigates how different spatiotemporal temperature models affect the estimation of heat-related mortality in Lazio, Italy (2008--2022). First, we compare three methods to reconstruct daily maximum temperature at the municipality level: 1. a Bayesian quantile regression model with spatial interpolation, 2. a Bayesian Gaussian regression model, 3. the gridded reanalysis data from ERA5-Land. Both Bayesian models are station-based and exhibit higher and more spatially variable temperatures compared to ERA5-Land. Then, using individual mortality data for cardiovascular and respiratory causes, we estimate temperature-mortality associations through Bayesian conditional Poisson models in a case-crossover design. Exposure is defined as the mean maximum temperature over the previous three days. Additional models include heatwave definitions combining different thresholds and durations. All models exhibit a marked increase in relative risk at high temperatures; however, the temperature of minimum risk varies significantly across methods. Stratified analyses reveal higher relative risk increases in females and the elderly (80+). Heatwave effects depend on the definitions used, but all methods capture an increased mortality risk associated with prolonged heat exposure. Results confirm the importance of temperature model choice in epidemiology and provide insights for early warning systems and climate-health adaptation strategies."}
{"id": "2512.14000", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14000", "abs": "https://arxiv.org/abs/2512.14000", "authors": ["Zheng He", "Roman Pogodin", "Yazhe Li", "Namrata Deka", "Arthur Gretton", "Danica J. Sutherland"], "title": "On the Hardness of Conditional Independence Testing In Practice", "comment": "Published at NeurIPS 2025: https://openreview.net/forum?id=Tn1M71PDfF", "summary": "Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on \"hiding\" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error."}
{"id": "2512.13997", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13997", "abs": "https://arxiv.org/abs/2512.13997", "authors": ["Aaron Wei", "Milad Jalali", "Danica J. Sutherland"], "title": "Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics", "comment": null, "summary": "Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations."}
{"id": "2512.13939", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13939", "abs": "https://arxiv.org/abs/2512.13939", "authors": ["Alessandro Casa", "Thomas Brendan Murphy", "Michael Fop"], "title": "A latent variable model for identifying and characterizing food adulteration", "comment": null, "summary": "Recently, growing consumer awareness of food quality and sustainability has led to a rising demand for effective food authentication methods. Vibrational spectroscopy techniques have emerged as a promising tool for collecting large volumes of data to detect food adulteration. However, spectroscopic data pose significant challenges from a statistical viewpoint, highlighting the need for more sophisticated modeling strategies. To address these challenges, in this work we propose a latent variable model specifically tailored for food adulterant detection, while accommodating the features of spectral data. Our proposal offers greater granularity with respect to existing approaches, since it does not only identify adulterated samples but also estimates the level of adulteration, and detects the spectral regions most affected by the adulterant. Consequently, the methodology offers deeper insights, and could facilitate the development of portable and faster instruments for efficient data collection in food authenticity studies. The method is applied to both synthetic and real honey mid-infrared spectroscopy data, delivering precise estimates of the adulteration level and accurately identifying which portions of the spectra are most impacted by the adulterant."}
{"id": "2512.13875", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13875", "abs": "https://arxiv.org/abs/2512.13875", "authors": ["Michael C. Stanley", "Peter W. Spaeth", "James E. Warner", "Matthew R. Webster"], "title": "Bond strength uncertainty quantification via confidence intervals for nondestructive evaluation of bonded composites", "comment": null, "summary": "As bonded composite materials are used more frequently for aerospace applications, it is necessary to certify that parts achieve desired levels of certain physical characteristics (e.g., strength) for safety and performance. Nondestructive evaluation (NDE) of adhesively bonded structures enables verification of bond physical characteristics, but uncertainty quantification (UQ) of NDE estimates is crucial for understanding risks, especially for NDE estimates like bond strength. To address the critical need for NDE UQ for adhesive bond strength estimates, we propose an optimization--based approach to computing finite--sample confidence intervals showing the range of bond strengths that could feasibly be produced by the observed data. A statistical inverse model approach is used to compute a confidence interval of specimen interfacial stiffness from swept--frequency ultrasonic phase observations and a method for propagating the interval to bond strength via a known interfacial stiffness regression is proposed. This approach requires innovating the optimization--based confidence interval to handle both a nonlinear forward model and unknown variance and developing a calibration approach to ensure that the final bond strength interval achieves at least the desired coverage level. Using model assumptions in line with current literature, we demonstrate our approach on simulated measurement data using a variety of low to high noise settings under two prototypical parameter settings. Relative to a baseline approach, we show that our method achieves better coverage and smaller intervals in high--noise settings and when a nuisance parameter is near the constraint boundary."}
{"id": "2512.14221", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14221", "abs": "https://arxiv.org/abs/2512.14221", "authors": ["Jiarong Fan", "Juhyun Park. Thi Phuong Thuy Vo", "Nicolas Brunel"], "title": "Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms", "comment": null, "summary": "Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees."}
{"id": "2512.13944", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13944", "abs": "https://arxiv.org/abs/2512.13944", "authors": ["Souhardya Sengupta", "Kosuke Imai", "Georgia Papadogeorgou"], "title": "Low-rank Covariate Balancing Estimators under Interference", "comment": null, "summary": "A key methodological challenge in observational studies with interference between units is twofold: (1) each unit's outcome may depend on many others' treatments, and (2) treatment assignments may exhibit complex dependencies across units. We develop a general statistical framework for constructing robust causal effect estimators to address these challenges. We first show that, without restricting the patterns of interference, the standard inverse probability weighting (IPW) estimator is the only uniformly unbiased estimator when the propensity score is known. In contrast, no estimator has such a property if the propensity score is unknown. We then introduce a \\emph{low-rank structure} of potential outcomes as a broad class of structural assumptions about interference. This framework encompasses common assumptions such as anonymous, nearest-neighbor, and additive interference, while flexibly allowing for more complex study-specific interference assumptions. Under this low-rank assumption, we show how to construct an unbiased weighting estimator for a large class of causal estimands. The proposed weighting estimator does not require knowledge of true propensity scores and is therefore robust to unknown treatment assignment dependencies that often exist in observational studies. If the true propensity score is known, we can obtain an unbiased estimator that is more efficient than the IPW estimator by leveraging a low-rank structure. We establish the finite sample and asymptotic properties of the proposed weighting estimator, develop a data-driven procedure to select among candidate low-rank structures, and validate our approach through simulation and empirical studies."}
{"id": "2512.13939", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13939", "abs": "https://arxiv.org/abs/2512.13939", "authors": ["Alessandro Casa", "Thomas Brendan Murphy", "Michael Fop"], "title": "A latent variable model for identifying and characterizing food adulteration", "comment": null, "summary": "Recently, growing consumer awareness of food quality and sustainability has led to a rising demand for effective food authentication methods. Vibrational spectroscopy techniques have emerged as a promising tool for collecting large volumes of data to detect food adulteration. However, spectroscopic data pose significant challenges from a statistical viewpoint, highlighting the need for more sophisticated modeling strategies. To address these challenges, in this work we propose a latent variable model specifically tailored for food adulterant detection, while accommodating the features of spectral data. Our proposal offers greater granularity with respect to existing approaches, since it does not only identify adulterated samples but also estimates the level of adulteration, and detects the spectral regions most affected by the adulterant. Consequently, the methodology offers deeper insights, and could facilitate the development of portable and faster instruments for efficient data collection in food authenticity studies. The method is applied to both synthetic and real honey mid-infrared spectroscopy data, delivering precise estimates of the adulteration level and accurately identifying which portions of the spectra are most impacted by the adulterant."}
{"id": "2512.14308", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.14308", "abs": "https://arxiv.org/abs/2512.14308", "authors": ["Šimon Kucharský", "Aayush Mishra", "Daniel Habermann", "Stefan T. Radev", "Paul-Christian Bürkner"], "title": "Improving the Accuracy of Amortized Model Comparison with Self-Consistency", "comment": "17 pages, 9 figures", "summary": "Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification."}
{"id": "2512.13962", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.13962", "abs": "https://arxiv.org/abs/2512.13962", "authors": ["Benjamin Christoffersen", "Keith Humphreys", "Alessandro Gasparini", "Birzhan Akynkozhayev", "Hedvig Kjellström", "Mark Clements"], "title": "Joint Models with Multiple Markers and Multiple Time-to-event Outcomes Using Variational Approximations", "comment": null, "summary": "Joint models are well suited to modelling linked data from laboratories and health registers. However, there are few examples of joint models that allow for (a) multiple markers, (b) multiple survival outcomes (including terminal events, competing events, and recurrent events), (c) delayed entry and (d) scalability. We propose a full likelihood approach for joint models based on a Gaussian variational approximation to satisfy criteria (a)-(d). We provide an open-source implementation for this approach, allowing for flexible sets of models for the longitudinal markers and survival outcomes. Through simulations, we find that the lower bound for the variational approximation is close to the full likelihood. We also find that our approach and implementation are fast and scalable. We provide an application with a joint model for longitudinal measurements of dense and fatty breast tissue and time to first breast cancer diagnosis. The use of variational approximations provides a promising approach for extending current joint models."}
{"id": "2512.14353", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.14353", "abs": "https://arxiv.org/abs/2512.14353", "authors": ["Ritabrata Dutta", "Yuehao Xu", "Sherman Khoo", "Francesca Basini", "Andreas Futschik"], "title": "Signature-Informed Selection Detection: A Novel Method for Multi-Locus Temporal Population Genetic Model with Recombination", "comment": null, "summary": "In population genetics, there is often interest in inferring selection coefficients. This task becomes more challenging if multiple linked selected loci are considered simultaneously. For such a situation, we propose a novel generalized Bayesian framework where we compute a scoring rule posterior for the selection coefficients in multi-locus temporal population genetics models. As we consider trajectories of allele frequencies over time as our data, we choose to use a signature kernel scoring rule - a kernel scoring rule defined for high-dimensional time-series data using iterated path integrals of a path (called signatures). We can compute an unbiased estimate of the signature kernel score using model simulations. This enables us to sample asymptotically from the signature kernel scoring rule posterior of the selection coefficients using pseudo-marginal MCMC-type algorithms. Through a simulation study, we were able to show the inferential efficacy of our method compared to existing benchmark methods for two and three selected locus scenarios under the standard Wright-Fisher model with recombination and selection. We also consider a negative frequency-dependent selection model for one and two locus scenarios, and also joint inference of selection coefficients and initial haplotype frequencies under the standard Wright-Fisher model. Finally, we illustrate the application of our inferential method for two real-life dataset. More specifically, we consider a data set on Yeast, as well as data from an Evolve and Resequence (E\\&R) experiment on {\\em Drosophila simulans}."}
{"id": "2512.14311", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14311", "abs": "https://arxiv.org/abs/2512.14311", "authors": ["Pablo García-Santaclara", "Bruno Fernández-Castro", "Rebeca P. Díaz-Redondo", "Carlos Calvo-Moa", "Henar Mariño-Bodelón"], "title": "Continual Learning at the Edge: An Agnostic IIoT Architecture", "comment": null, "summary": "The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution."}
{"id": "2512.13992", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13992", "abs": "https://arxiv.org/abs/2512.13992", "authors": ["Jyotishka Datta", "Nick Polson", "Vadim Sokolov"], "title": "Bayesian Global-Local Regularization", "comment": null, "summary": "We propose a unified framework for global-local regularization that bridges the gap between classical techniques -- such as ridge regression and the nonnegative garotte -- and modern Bayesian hierarchical modeling. By estimating local regularization strengths via marginal likelihood under order constraints, our approach generalizes Stein's positive-part estimator and provides a principled mechanism for adaptive shrinkage in high-dimensional settings. We establish that this isotonic empirical Bayes estimator achieves near-minimax risk (up to logarithmic factors) over sparse ordered model classes, constituting a significant advance in high-dimensional statistical inference. Applications to orthogonal polynomial regression demonstrate the methodology's flexibility, while our theoretical results clarify the connections between empirical Bayes, shape-constrained estimation, and degrees-of-freedom adjustments."}
{"id": "2512.14404", "categories": ["stat.ML", "cs.LG", "math.OC", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2512.14404", "abs": "https://arxiv.org/abs/2512.14404", "authors": ["Hangjun Cho", "Fabio V. G. Amaral", "Andrei A. Klishin", "Cassio M. Oishi", "Steven L. Brunton"], "title": "From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification", "comment": "34 pages, 11 figures", "summary": "In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations."}
{"id": "2512.14131", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14131", "abs": "https://arxiv.org/abs/2512.14131", "authors": ["Prasanjit Dubey", "Xiaoming Huo"], "title": "Most Powerful Test with Exact Family-Wise Error Rate Control: Necessary Conditions and a Path to Fast Computing", "comment": null, "summary": "Identifying the most powerful test in multiple hypothesis testing under strong family-wise error rate (FWER) control is a fundamental problem in statistical methodology. State-of-the-art approaches formulate this as a constrained optimisation problem, for which a dual problem with strong duality has been established in a general sense. However, a constructive method for solving the dual problem is lacking, leaving a significant computational gap. This paper fills this gap by deriving novel, necessary optimality conditions for the dual optimisation. We show that these conditions motivate an efficient coordinate-wise algorithm for computing the optimal dual solution, which, in turn, provides the most powerful test for the primal problem. We prove the linear convergence of our algorithm, i.e., the computational complexity of our proposed algorithm is proportional to the logarithm of the reciprocal of the target error. To the best of our knowledge, this is the first time such a fast and computationally efficient algorithm has been proposed for finding the most powerful test with family-wise error rate control. The method's superior power is demonstrated through simulation studies, and its practical utility is shown by identifying new, significant findings in both clinical and financial data applications."}
{"id": "2512.14604", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14604", "abs": "https://arxiv.org/abs/2512.14604", "authors": ["Prasanjit Dubey", "Aritra Guha", "Zhengyi Zhou", "Qiong Wu", "Xiaoming Huo", "Paromita Dubey"], "title": "LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts", "comment": null, "summary": "Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines."}
{"id": "2512.14353", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.14353", "abs": "https://arxiv.org/abs/2512.14353", "authors": ["Ritabrata Dutta", "Yuehao Xu", "Sherman Khoo", "Francesca Basini", "Andreas Futschik"], "title": "Signature-Informed Selection Detection: A Novel Method for Multi-Locus Temporal Population Genetic Model with Recombination", "comment": null, "summary": "In population genetics, there is often interest in inferring selection coefficients. This task becomes more challenging if multiple linked selected loci are considered simultaneously. For such a situation, we propose a novel generalized Bayesian framework where we compute a scoring rule posterior for the selection coefficients in multi-locus temporal population genetics models. As we consider trajectories of allele frequencies over time as our data, we choose to use a signature kernel scoring rule - a kernel scoring rule defined for high-dimensional time-series data using iterated path integrals of a path (called signatures). We can compute an unbiased estimate of the signature kernel score using model simulations. This enables us to sample asymptotically from the signature kernel scoring rule posterior of the selection coefficients using pseudo-marginal MCMC-type algorithms. Through a simulation study, we were able to show the inferential efficacy of our method compared to existing benchmark methods for two and three selected locus scenarios under the standard Wright-Fisher model with recombination and selection. We also consider a negative frequency-dependent selection model for one and two locus scenarios, and also joint inference of selection coefficients and initial haplotype frequencies under the standard Wright-Fisher model. Finally, we illustrate the application of our inferential method for two real-life dataset. More specifically, we consider a data set on Yeast, as well as data from an Evolve and Resequence (E\\&R) experiment on {\\em Drosophila simulans}."}
{"id": "2512.14272", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14272", "abs": "https://arxiv.org/abs/2512.14272", "authors": ["Brian Buckley", "Adrian O'Hagan", "Marie Galligan"], "title": "A variational Bayes latent class approach for EHR-based patient phenotyping in R", "comment": "19 pages, 6 figures, to be submitted to The Journal of Statistical Software", "summary": "The VBphenoR package for R provides a closed-form variational Bayes approach to patient phenotyping using Electronic Health Records (EHR) data. We implement a variational Bayes Gaussian Mixture Model (GMM) algorithm using closed-form coordinate ascent variational inference (CAVI) to determine the patient phenotype latent class. We then implement a variational Bayes logistic regression, where we determine the probability of the phenotype in the supplied EHR cohort, the shift in biomarkers for patients with the phenotype of interest versus a healthy population and evaluate predictive performance of binary indicator clinical codes and medication codes. The logistic model likelihood applies the latent class from the GMM step to inform the conditional."}
{"id": "2512.14378", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14378", "abs": "https://arxiv.org/abs/2512.14378", "authors": ["Emmanouil Androulakis", "Kashinath Chatterjee", "Haralambos Evangelaras"], "title": "On the E(s^2)-optimality of two-level supersaturated designs constructed using Wu's method of partially aliased interactions on certain two-level orthogonal arrays", "comment": null, "summary": "Wu [10] proposed a method for constructing two-level supersaturated designs by using a Hadamard design with n runs and n-1 columns as a staring design and by supplementing it with two-column interactions, as long as they are partially aliased. Bulutoglu and Cheng [2] proved that this method results in E(s^2)-optimal supersaturated designs when certain interaction columns are selected. In this paper, we extend these results and prove E(s^2)-optimality for supersaturated designs that are constructed using Wu's method when the starting design is any orthogonal array with n runs and n-1, n-2 or n-3 columns, as long as its main effects and two-column interactions are partially aliased with two-column interactions."}
{"id": "2512.14399", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14399", "abs": "https://arxiv.org/abs/2512.14399", "authors": ["Dániel Pfeifer", "Edith Alice Kovács"], "title": "Trunc-Opt vine building algorithms", "comment": null, "summary": "Vine copula models have become highly popular and practical tools for modelling multivariate probability distributions due to their flexibility in modelling different kinds of dependences between the random variables involved. However, their flexibility comes with the drawback of a high-dimensional parameter space. To tackle this problem, truncated vine copulas were introduced by Kurowicka (2010) (Gaussian case) and Brechmann and Czado (2013) (general case). Truncated vine copulas contain conditionally independent pair copulas after the truncation level. So far, in the general case, truncated vine constructing algorithms started from the lowest tree in order to encode the largest dependences in the lower trees. The novelty of this paper starts from the observation that a truncated vine is determined by the first tree after the truncation level (see Kovács and Szántai (2017)). This paper introduces a new score for fitting truncated vines to given data, called the Weight of the truncated vine. Then we propose a completely new methodology for constructing truncated vines. We prove theorems which motivate this new approach. While earlier algorithms did not use conditional independences, we give algorithms for constructing and encoding truncated vines which do exploit them. Finally, we illustrate the algorithms on real datasets and compare the results with well-known methods included in R packages. Our method generally compare favorably to previously known methods."}
{"id": "2512.14399", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14399", "abs": "https://arxiv.org/abs/2512.14399", "authors": ["Dániel Pfeifer", "Edith Alice Kovács"], "title": "Trunc-Opt vine building algorithms", "comment": null, "summary": "Vine copula models have become highly popular and practical tools for modelling multivariate probability distributions due to their flexibility in modelling different kinds of dependences between the random variables involved. However, their flexibility comes with the drawback of a high-dimensional parameter space. To tackle this problem, truncated vine copulas were introduced by Kurowicka (2010) (Gaussian case) and Brechmann and Czado (2013) (general case). Truncated vine copulas contain conditionally independent pair copulas after the truncation level. So far, in the general case, truncated vine constructing algorithms started from the lowest tree in order to encode the largest dependences in the lower trees. The novelty of this paper starts from the observation that a truncated vine is determined by the first tree after the truncation level (see Kovács and Szántai (2017)). This paper introduces a new score for fitting truncated vines to given data, called the Weight of the truncated vine. Then we propose a completely new methodology for constructing truncated vines. We prove theorems which motivate this new approach. While earlier algorithms did not use conditional independences, we give algorithms for constructing and encoding truncated vines which do exploit them. Finally, we illustrate the algorithms on real datasets and compare the results with well-known methods included in R packages. Our method generally compare favorably to previously known methods."}
{"id": "2512.14413", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14413", "abs": "https://arxiv.org/abs/2512.14413", "authors": ["Aymen Echarghaoui", "Robert Tibshirani"], "title": "Univariate-Guided Interaction Modeling", "comment": null, "summary": "We propose a procedure for sparse regression with pairwise interactions, by generalizing the Univariate Guided Sparse Regression (UniLasso) methodology. A central contribution is our introduction of a concept of univariate (or marginal) interactions. Using this concept, we propose two algorithms -- uniPairs and uniPairs-2stage -- , and evaluate their performance against established methods, including Glinternet and Sprinter. We show that our framework yields sparser models with more interpretable interactions. We also prove support recovery results for our proposal under suitable conditions."}
{"id": "2512.14492", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14492", "abs": "https://arxiv.org/abs/2512.14492", "authors": ["Martin Slawski"], "title": "Causal Secondary Analysis of Linked Data in the Presence of Mismatch Error", "comment": null, "summary": "The increased prevalence of observational data and the need to integrate information from multiple sources are critical challenges in contemporary data analysis. Record linkage is a widely used tool for combining datasets in the absence of unique identifiers. The presence of linkage errors such as mismatched records, however, often hampers the analysis of data sets obtained in this way. This issue is more difficult to address in secondary analysis settings, where linkage and subsequent analysis are performed separately, and analysts have limited information about linkage quality. In this paper, we investigate the estimation of average treatment effects in the conventional potential outcome-based causal inference framework under linkage uncertainty. To mitigate the bias that would be incurred with naive analyses, we propose an approach based on estimating equations that treats the unknown match status indicators as missing data. Leveraging a variant of the Expectation-Maximization algorithm, these indicators are imputed based on a corresponding two-component mixture model. The approach is amenable to asymptotic inference. Simulation studies and a case study highlight the importance of accounting for linkage uncertainty and demonstrate the effectiveness of the proposed approach."}
{"id": "2512.14504", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14504", "abs": "https://arxiv.org/abs/2512.14504", "authors": ["Emanuele Giorgi", "Jonas Wallin"], "title": "A flexible class of latent variable models for the analysis of antibody response data", "comment": "This is a working paper, and updated versions will be released in the future. For further information about this research, please contact Emanuele Giorgi at e.giorgi@bham.ac.uk", "summary": "Existing approaches to modelling antibody concentration data are mostly based on finite mixture models that rely on the assumption that individuals can be divided into two distinct groups: seronegative and seropositive. Here, we challenge this dichotomous modelling assumption and propose a latent variable modelling framework in which the immune status of each individual is represented along a continuum of latent seroreactivity, ranging from minimal to strong immune activation. This formulation provides greater flexibility in capturing age-related changes in antibody distributions while preserving the full information content of quantitative measurements. We show that the proposed class of models can accommodate a great variety of model formulations, both mechanistic and regression-based, and also includes finite mixture models as a special case. We demonstrate the advantages of this approach using malaria serology data and its ability to develop joint analyses across all ages that account for changes in transmission patterns. We conclude by outlining extensions of the proposed modelling framework and its relevance to other omics applications."}
{"id": "2512.14609", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14609", "abs": "https://arxiv.org/abs/2512.14609", "authors": ["Marc-Oliver Pohle", "Jan-Lukas Wermuth", "Christian H. Weiß"], "title": "Asymptotic Inference for Rank Correlations", "comment": null, "summary": "Kendall's tau and Spearman's rho are widely used tools for measuring dependence. Surprisingly, when it comes to asymptotic inference for these rank correlations, some fundamental results and methods have not yet been developed, in particular for discrete random variables and in the time series case, and concerning variance estimation in general. Consequently, asymptotic confidence intervals are not available. We provide a comprehensive treatment of asymptotic inference for classical rank correlations, including Kendall's tau, Spearman's rho, Goodman-Kruskal's gamma, Kendall's tau-b, and grade correlation. We derive asymptotic distributions for both iid and time series data, resorting to asymptotic results for U-statistics, and introduce consistent variance estimators. This enables the construction of confidence intervals and tests, generalizes classical results for continuous random variables and leads to corrected versions of widely used tests of independence. We analyze the finite-sample performance of our variance estimators, confidence intervals, and tests in simulations and illustrate their use in case studies."}
{"id": "2512.13997", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13997", "abs": "https://arxiv.org/abs/2512.13997", "authors": ["Aaron Wei", "Milad Jalali", "Danica J. Sutherland"], "title": "Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics", "comment": null, "summary": "Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations."}
{"id": "2512.14000", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14000", "abs": "https://arxiv.org/abs/2512.14000", "authors": ["Zheng He", "Roman Pogodin", "Yazhe Li", "Namrata Deka", "Arthur Gretton", "Danica J. Sutherland"], "title": "On the Hardness of Conditional Independence Testing In Practice", "comment": "Published at NeurIPS 2025: https://openreview.net/forum?id=Tn1M71PDfF", "summary": "Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on \"hiding\" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error."}
