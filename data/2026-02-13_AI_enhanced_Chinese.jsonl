{"id": "2602.10144", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10144", "abs": "https://arxiv.org/abs/2602.10144", "authors": ["Jonas K\u00fcbler", "Kailash Budhathoki", "Matth\u00e4us Kleindessner", "Xiong Zhou", "Junming Yin", "Ashish Khetan", "George Karypis"], "title": "When LLMs get significantly worse: A statistical approach to detect model degradations", "comment": "https://openreview.net/forum?id=cM3gsqEI4K", "summary": "Minimizing the inference cost and latency of foundation models has become a crucial area of research. Optimization approaches include theoretically lossless methods and others without accuracy guarantees like quantization. In all of these cases it is crucial to ensure that the model quality has not degraded. However, even at temperature zero, model generations are not necessarily robust even to theoretically lossless model optimizations due to numerical errors. We thus require statistical tools to decide whether a finite-sample accuracy deviation is an evidence of a model's degradation or whether it can be attributed to (harmless) noise in the evaluation. We propose a statistically sound hypothesis testing framework based on McNemar's test allowing to efficiently detect model degradations, while guaranteeing a controlled rate of false positives. The crucial insight is that we have to confront the model scores on each sample, rather than aggregated on the task level. Furthermore, we propose three approaches to aggregate accuracy estimates across multiple benchmarks into a single decision. We provide an implementation on top of the largely adopted open source LM Evaluation Harness and provide a case study illustrating that the method correctly flags degraded models, while not flagging model optimizations that are provably lossless. We find that with our tests even empirical accuracy degradations of 0.3% can be confidently attributed to actual degradations rather than noise.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMcNemar\u68c0\u9a8c\u7684\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u57fa\u7840\u6a21\u578b\u4f18\u5316\u540e\u7684\u6027\u80fd\u9000\u5316\uff0c\u80fd\u533a\u5206\u771f\u5b9e\u9000\u5316\u4e0e\u8bc4\u4f30\u566a\u58f0\uff0c\u53ef\u68c0\u6d4b0.3%\u7684\u51c6\u786e\u7387\u4e0b\u964d\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u4f18\u5316\uff08\u5982\u91cf\u5316\uff09\u540e\u9700\u8981\u786e\u4fdd\u6a21\u578b\u8d28\u91cf\u4e0d\u9000\u5316\uff0c\u4f46\u5373\u4f7f\u7406\u8bba\u4e0a\u65e0\u635f\u7684\u4f18\u5316\u4e5f\u4f1a\u56e0\u6570\u503c\u8bef\u5dee\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u53d8\u5316\u3002\u9700\u8981\u7edf\u8ba1\u5de5\u5177\u533a\u5206\u6709\u9650\u6837\u672c\u7684\u51c6\u786e\u7387\u504f\u5dee\u662f\u771f\u5b9e\u9000\u5316\u8fd8\u662f\u8bc4\u4f30\u566a\u58f0\u3002", "method": "\u57fa\u4e8eMcNemar\u68c0\u9a8c\u7684\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u5728\u6837\u672c\u7ea7\u522b\u5bf9\u6bd4\u6a21\u578b\u5f97\u5206\u800c\u975e\u4efb\u52a1\u7ea7\u805a\u5408\u3002\u63d0\u51fa\u4e09\u79cd\u65b9\u6cd5\u5c06\u591a\u4e2a\u57fa\u51c6\u7684\u51c6\u786e\u7387\u4f30\u8ba1\u805a\u5408\u4e3a\u5355\u4e00\u51b3\u7b56\u3002\u5728LM Evaluation Harness\u4e0a\u5b9e\u73b0\u3002", "result": "\u65b9\u6cd5\u80fd\u6b63\u786e\u6807\u8bb0\u9000\u5316\u6a21\u578b\uff0c\u4e0d\u6807\u8bb0\u7406\u8bba\u4e0a\u65e0\u635f\u7684\u4f18\u5316\u3002\u6d4b\u8bd5\u663e\u793a\u5373\u4f7f0.3%\u7684\u7ecf\u9a8c\u51c6\u786e\u7387\u4e0b\u964d\u4e5f\u80fd\u53ef\u9760\u5f52\u56e0\u4e8e\u771f\u5b9e\u9000\u5316\u800c\u975e\u566a\u58f0\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u8ba1\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u6a21\u578b\u4f18\u5316\u540e\u7684\u6027\u80fd\u9000\u5316\uff0c\u63a7\u5236\u5047\u9633\u6027\u7387\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.10176", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10176", "abs": "https://arxiv.org/abs/2602.10176", "authors": ["Thomas Kehrenberg", "Javier Sanguino", "Jose A. Lozano", "Novi Quadrianto"], "title": "Dissecting Performative Prediction: A Comprehensive Survey", "comment": null, "summary": "The field of performative prediction had its beginnings in 2020 with the seminal paper \"Performative Prediction\" by Perdomo et al., which established a novel machine learning setup where the deployment of a predictive model causes a distribution shift in the environment, which in turn causes a mismatch between the distribution expected by the predictive model and the real distribution. This shift is defined by a so-called distribution map. In the half-decade since, a literature has emerged which has, among other things, introduced new solution concepts to the original setup, extended the setup, offered new theoretical analyses, and examined the intersection of performative prediction and other established fields. In this survey, we first lay out the performative prediction setting and explain the different optimization targets: performative stability and performative optimality. We introduce a new way of classifying different performative prediction settings, based on how much information is available about the distribution map. We survey existing implementations of distribution maps and existing methods to address the problem of performative prediction, while examining different ways to categorize them. Finally, we point out known and previously unknown connections that can be drawn to other fields, in the hopes of stimulating future research.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u56de\u987e\u4e86\u81ea2020\u5e74Perdomo\u7b49\u4eba\u63d0\u51fa\"\u8868\u6f14\u6027\u9884\u6d4b\"\u6982\u5ff5\u4ee5\u6765\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u8be5\u9886\u57df\u7684\u4e0d\u540c\u8bbe\u7f6e\u3001\u4f18\u5316\u76ee\u6807\u3001\u5206\u7c7b\u65b9\u6cd5\u3001\u5b9e\u73b0\u6280\u672f\u548c\u4e0e\u5176\u4ed6\u9886\u57df\u7684\u8054\u7cfb\u3002", "motivation": "\u8868\u6f14\u6027\u9884\u6d4b\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u5174\u7684\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u81ea2020\u5e74\u63d0\u51fa\u4ee5\u6765\u5df2\u7ecf\u79ef\u7d2f\u4e86\u4e30\u5bcc\u7684\u7814\u7a76\u6210\u679c\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u8be5\u9886\u57df\u7684\u53d1\u5c55\u8109\u7edc\uff0c\u5efa\u7acb\u7edf\u4e00\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e0e\u5176\u4ed6\u9886\u57df\u7684\u6f5c\u5728\u8054\u7cfb\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u9996\u5148\u9610\u8ff0\u4e86\u8868\u6f14\u6027\u9884\u6d4b\u7684\u57fa\u672c\u8bbe\u7f6e\uff0c\u89e3\u91ca\u4e86\u8868\u6f14\u7a33\u5b9a\u6027\u548c\u8868\u6f14\u6700\u4f18\u6027\u4e24\u79cd\u4f18\u5316\u76ee\u6807\u3002\u7136\u540e\u63d0\u51fa\u4e86\u57fa\u4e8e\u5206\u5e03\u56fe\u4fe1\u606f\u91cf\u7684\u65b0\u5206\u7c7b\u65b9\u6cd5\uff0c\u7efc\u8ff0\u4e86\u73b0\u6709\u5206\u5e03\u56fe\u7684\u5b9e\u73b0\u65b9\u5f0f\u4ee5\u53ca\u89e3\u51b3\u8868\u6f14\u6027\u9884\u6d4b\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u7684\u5206\u7c7b\u7ef4\u5ea6\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf9\u8868\u6f14\u6027\u9884\u6d4b\u9886\u57df\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u5206\u5e03\u56fe\u4fe1\u606f\u91cf\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u7cfb\u7edf\u6574\u7406\u4e86\u73b0\u6709\u6280\u672f\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u4e0e\u5176\u4ed6\u7814\u7a76\u9886\u57df\uff08\u5982\u56e0\u679c\u63a8\u65ad\u3001\u535a\u5f08\u8bba\u7b49\uff09\u7684\u5df2\u77e5\u548c\u6f5c\u5728\u8054\u7cfb\u3002", "conclusion": "\u8868\u6f14\u6027\u9884\u6d4b\u662f\u4e00\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u91cd\u8981\u7814\u7a76\u9886\u57df\uff0c\u5177\u6709\u4e30\u5bcc\u7684\u7406\u8bba\u5185\u6db5\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u901a\u8fc7\u5efa\u7acb\u7edf\u4e00\u7684\u5206\u7c7b\u6846\u67b6\u548c\u63ed\u793a\u8de8\u9886\u57df\u8054\u7cfb\uff0c\u8fd9\u7bc7\u7efc\u8ff0\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u53c2\u8003\u548c\u65b9\u5411\u6307\u5f15\u3002"}}
{"id": "2602.10273", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10273", "abs": "https://arxiv.org/abs/2602.10273", "authors": ["Seyedarmin Azizi", "Erfan Baghaei Potraghloo", "Minoo Ahmadi", "Souvik Kundu", "Massoud Pedram"], "title": "Power-SMC: Low-Latency Sequence-Level Power Sampling for Training-Free LLM Reasoning", "comment": null, "summary": "Many recent reasoning gains in large language models can be explained as distribution sharpening: biasing generation toward high-likelihood trajectories already supported by the pretrained model, rather than modifying its weights. A natural formalization is the sequence-level power distribution $\u03c0_\u03b1(y\\mid x)\\propto p_\u03b8(y\\mid x)^\u03b1$ ($\u03b1>1$), which concentrates mass on whole sequences instead of adjusting token-level temperature. Prior work shows that Metropolis--Hastings (MH) sampling from this distribution recovers strong reasoning performance, but at order-of-magnitude inference slowdowns. We introduce Power-SMC, a training-free Sequential Monte Carlo scheme that targets the same objective while remaining close to standard decoding latency. Power-SMC advances a small particle set in parallel, corrects importance weights token-by-token, and resamples when necessary, all within a single GPU-friendly batched decode. We prove that temperature $\u03c4=1/\u03b1$ is the unique prefix-only proposal minimizing incremental weight variance, interpret residual instability via prefix-conditioned R\u00e9nyi entropies, and introduce an exponent-bridging schedule that improves particle stability without altering the target. On MATH500, Power-SMC matches or exceeds MH power sampling while reducing latency from $16$--$28\\times$ to $1.4$--$3.3\\times$ over baseline decoding.", "AI": {"tldr": "Power-SMC\uff1a\u4e00\u79cd\u57fa\u4e8e\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u7684\u8bad\u7ec3\u514d\u8d39\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u7c92\u5b50\u63a8\u8fdb\u548c\u91cd\u8981\u6027\u6743\u91cd\u4fee\u6b63\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u6807\u51c6\u89e3\u7801\u5ef6\u8fdf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e0eMetropolis-Hastings\u529f\u7387\u91c7\u6837\u76f8\u5f53\u7684\u63a8\u7406\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMetropolis-Hastings\u7684\u529f\u7387\u91c7\u6837\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u4f1a\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u5927\u5e45\u4e0b\u964d\uff0816-28\u500d\u5ef6\u8fdf\uff09\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u63d0\u5347\uff0c\u53c8\u4e0d\u4f1a\u663e\u8457\u589e\u52a0\u5ef6\u8fdf\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPower-SMC\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u6846\u67b6\uff0c\u5e76\u884c\u63a8\u8fdb\u5c11\u91cf\u7c92\u5b50\u96c6\uff1b2\uff09\u9010token\u4fee\u6b63\u91cd\u8981\u6027\u6743\u91cd\uff1b3\uff09\u5fc5\u8981\u65f6\u8fdb\u884c\u91cd\u91c7\u6837\uff1b4\uff09\u6240\u6709\u64cd\u4f5c\u5728\u5355\u4e2aGPU\u53cb\u597d\u7684\u6279\u5904\u7406\u89e3\u7801\u4e2d\u5b8c\u6210\uff1b5\uff09\u8bc1\u660e\u6e29\u5ea6\u03c4=1/\u03b1\u662f\u552f\u4e00\u6700\u5c0f\u5316\u589e\u91cf\u6743\u91cd\u65b9\u5dee\u7684\u524d\u7f00\u63d0\u8bae\uff1b6\uff09\u5f15\u5165\u6307\u6570\u6865\u63a5\u8c03\u5ea6\u6539\u5584\u7c92\u5b50\u7a33\u5b9a\u6027\u3002", "result": "\u5728MATH500\u6570\u636e\u96c6\u4e0a\uff0cPower-SMC\u5339\u914d\u6216\u8d85\u8fc7Metropolis-Hastings\u529f\u7387\u91c7\u6837\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u5ef6\u8fdf\u4ece16-28\u500d\u964d\u4f4e\u52301.4-3.3\u500d\uff08\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u89e3\u7801\uff09\u3002", "conclusion": "Power-SMC\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u8bad\u7ec3\u514d\u8d39\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u6807\u51c6\u89e3\u7801\u5ef6\u8fdf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u63a8\u7406\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.10370", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.10370", "abs": "https://arxiv.org/abs/2602.10370", "authors": ["Frances Dean", "Jenna Fields", "Radhika Bhalerao", "Marie Charpignon", "Ahmed Alaa"], "title": "Causal Effect Estimation with Learned Instrument Representations", "comment": null, "summary": "Instrumental variable (IV) methods mitigate bias from unobserved confounding in observational causal inference but rely on the availability of a valid instrument, which can often be difficult or infeasible to identify in practice. In this paper, we propose a representation learning approach that constructs instrumental representations from observed covariates, which enable IV-based estimation even in the absence of an explicit instrument. Our model (ZNet) achieves this through an architecture that mirrors the structural causal model of IVs; it decomposes the ambient feature space into confounding and instrumental components, and is trained by enforcing empirical moment conditions corresponding to the defining properties of valid instruments (i.e., relevance, exclusion restriction, and instrumental unconfoundedness). Importantly, ZNet is compatible with a wide range of downstream two-stage IV estimators of causal effects. Our experiments demonstrate that ZNet can (i) recover ground-truth instruments when they already exist in the ambient feature space and (ii) construct latent instruments in the embedding space when no explicit IVs are available. This suggests that ZNet can be used as a ``plug-and-play'' module for causal inference in general observational settings, regardless of whether the (untestable) assumption of unconfoundedness is satisfied.", "AI": {"tldr": "ZNet\uff1a\u4e00\u79cd\u4ece\u89c2\u6d4b\u534f\u53d8\u91cf\u4e2d\u5b66\u4e60\u6784\u5efa\u5de5\u5177\u53d8\u91cf\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u6ca1\u6709\u663e\u5f0f\u5de5\u5177\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\u8fdb\u884cIV\u4f30\u8ba1", "motivation": "\u5de5\u5177\u53d8\u91cf\u65b9\u6cd5\u9700\u8981\u6709\u6548\u7684\u5de5\u5177\u53d8\u91cf\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u96be\u4ee5\u8bc6\u522b\u6216\u4e0d\u53ef\u5f97\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u5de5\u5177\u53d8\u91cf\u7684\u53ef\u7528\u6027\uff0c\u9650\u5236\u4e86\u5728\u4e00\u822c\u89c2\u6d4b\u6570\u636e\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faZNet\u6a21\u578b\uff0c\u901a\u8fc7\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4ece\u89c2\u6d4b\u534f\u53d8\u91cf\u4e2d\u6784\u5efa\u5de5\u5177\u53d8\u91cf\u8868\u793a\u3002\u6a21\u578b\u67b6\u6784\u53cd\u6620\u5de5\u5177\u53d8\u91cf\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u5c06\u7279\u5f81\u7a7a\u95f4\u5206\u89e3\u4e3a\u6df7\u6742\u56e0\u7d20\u548c\u5de5\u5177\u53d8\u91cf\u6210\u5206\uff0c\u901a\u8fc7\u5f3a\u5236\u6ee1\u8db3\u5de5\u5177\u53d8\u91cf\u5b9a\u4e49\u5c5e\u6027\uff08\u76f8\u5173\u6027\u3001\u6392\u9664\u9650\u5236\u3001\u5de5\u5177\u53d8\u91cf\u65e0\u6df7\u6742\u6027\uff09\u7684\u7ecf\u9a8c\u77e9\u6761\u4ef6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eZNet\u80fd\u591f\uff1a(1)\u5728\u73af\u5883\u7279\u5f81\u7a7a\u95f4\u4e2d\u6062\u590d\u771f\u5b9e\u5b58\u5728\u7684\u5de5\u5177\u53d8\u91cf\uff1b(2)\u5728\u6ca1\u6709\u663e\u5f0f\u5de5\u5177\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6784\u5efa\u6f5c\u5728\u5de5\u5177\u53d8\u91cf\u3002\u6a21\u578b\u53ef\u4f5c\u4e3a\"\u5373\u63d2\u5373\u7528\"\u6a21\u5757\u4e0e\u591a\u79cd\u4e0b\u6e38\u4e24\u9636\u6bb5IV\u4f30\u8ba1\u5668\u517c\u5bb9\u3002", "conclusion": "ZNet\u4e3a\u4e00\u822c\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u8bba\u662f\u5426\u6ee1\u8db3\u65e0\u6df7\u6742\u6027\u5047\u8bbe\uff0c\u90fd\u80fd\u8fdb\u884c\u5de5\u5177\u53d8\u91cf\u4f30\u8ba1\uff0c\u6269\u5c55\u4e86IV\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2602.10241", "categories": ["stat.ME", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10241", "abs": "https://arxiv.org/abs/2602.10241", "authors": ["Zhenzhi Jiao", "Angela Yao", "Ran Tao", "Jean-Claude Thill"], "title": "Geographically Weighted Canonical Correlation Analysis: Local Spatial Associations Between Two Sets of Variables", "comment": null, "summary": "This article critically assesses the utility of the classical statistical technique of Canonical Correlation Analysis (CCA) for studying spatial associations and proposes a new approach to enhance it. Unlike bivariate correlation analysis, which focuses on the relationship between two individual variables, CCA investigates associations between two sets of variables by identifying pairs of linear combinations that are maximally correlated. CCA has strong potential for uncovering complex multivariate relationships that vary across geographic space. We propose Geographically Weighted Canonical Correlation Analysis (GWCCA) as a new technique for exploring local spatial associations between two sets of variables. GWCCA localizes standard CCA by weighting each observation according to its spatial distance from a target location, thereby estimating location-specific canonical correlations. The effectiveness of GWCCA in recovering spatial structure and capturing spatial effects is evaluated using synthetic data. A case study of US county-level health outcomes and social determinants of health further demonstrates the empirical capabilities of the proposed method. The results indicate that GWCCA has broad potential applications in spatial data-intensive fields such as urban planning, environmental science, public health, and transportation, where understanding local multivariate spatial associations is critical.", "AI": {"tldr": "\u63d0\u51fa\u5730\u7406\u52a0\u6743\u5178\u578b\u76f8\u5173\u5206\u6790(GWCCA)\u4f5c\u4e3a\u7814\u7a76\u4e24\u7ec4\u53d8\u91cf\u95f4\u5c40\u90e8\u7a7a\u95f4\u5173\u8054\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u8ddd\u79bb\u52a0\u6743\u5b9e\u73b0\u5c40\u90e8\u5316\u5206\u6790\u3002", "motivation": "\u7ecf\u5178\u5178\u578b\u76f8\u5173\u5206\u6790(CCA)\u5728\u7814\u7a76\u7a7a\u95f4\u5173\u8054\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5730\u7406\u7a7a\u95f4\u4e0a\u7684\u5c40\u90e8\u53d8\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5206\u6790\u4e24\u7ec4\u53d8\u91cf\u95f4\u5c40\u90e8\u7a7a\u95f4\u5173\u8054\u7684\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5730\u7406\u7a7a\u95f4\u4e2d\u7684\u590d\u6742\u591a\u5143\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u5730\u7406\u52a0\u6743\u5178\u578b\u76f8\u5173\u5206\u6790(GWCCA)\uff0c\u901a\u8fc7\u6839\u636e\u89c2\u6d4b\u70b9\u4e0e\u76ee\u6807\u4f4d\u7f6e\u7684\u7a7a\u95f4\u8ddd\u79bb\u8fdb\u884c\u52a0\u6743\uff0c\u5c06\u6807\u51c6CCA\u5c40\u90e8\u5316\uff0c\u4ece\u800c\u4f30\u8ba1\u4f4d\u7f6e\u7279\u5b9a\u7684\u5178\u578b\u76f8\u5173\u7cfb\u6570\u3002", "result": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u9a8c\u8bc1\u4e86GWCCA\u5728\u6062\u590d\u7a7a\u95f4\u7ed3\u6784\u548c\u6355\u6349\u7a7a\u95f4\u6548\u5e94\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7f8e\u56fd\u53bf\u7ea7\u5065\u5eb7\u7ed3\u679c\u4e0e\u793e\u4f1a\u51b3\u5b9a\u56e0\u7d20\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u8bc1\u80fd\u529b\u3002", "conclusion": "GWCCA\u5728\u7406\u89e3\u5c40\u90e8\u591a\u5143\u7a7a\u95f4\u5173\u8054\u81f3\u5173\u91cd\u8981\u7684\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u5982\u57ce\u5e02\u89c4\u5212\u3001\u73af\u5883\u79d1\u5b66\u3001\u516c\u5171\u536b\u751f\u548c\u4ea4\u901a\u7b49\u7a7a\u95f4\u6570\u636e\u5bc6\u96c6\u578b\u9886\u57df\u3002"}}
{"id": "2602.10749", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.10749", "abs": "https://arxiv.org/abs/2602.10749", "authors": ["Fusta Moro Alessandro", "Alessandro Fass\u00f2", "Jacopo Rodeschini"], "title": "The Dataset of Daily Air Quality for the Years 2013-2023 in Italy", "comment": null, "summary": "Air quality and climate are major issues in Italian society and lie at the intersection of many research fields, including public health and policy planning. There is an increasing need for readily available, easily accessible, ready-to-use and well-documented datasets on air quality and climate. In this paper, we present the GRINS AQCLIM dataset, created under the GRINS project framework covering the Italian domain for an extensive time period. It includes daily statistics (e.g., minimum, quartiles, mean, median and maximum) for a collection of air pollutant concentrations and climate variables at the locations of the 700+ available monitoring stations. Input data are retrieved from the European Environmental Agency and Copernicus Programme and were subjected to multiple processing steps to ensure their reliability and quality. These steps include automatic procedures for fixing raw files, manual inspection of stations information, the detection and removal of anomalies, and the temporal harmonisation on a daily basis. Datasets are hosted on Zenodo under open-access principles.", "AI": {"tldr": "GRINS AQCLIM\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u610f\u5927\u5229700\u591a\u4e2a\u76d1\u6d4b\u7ad9\u70b9\u7684\u7a7a\u6c14\u6c61\u67d3\u7269\u6d53\u5ea6\u548c\u6c14\u5019\u53d8\u91cf\u7684\u6bcf\u65e5\u7edf\u8ba1\u6570\u636e\uff0c\u6db5\u76d6\u957f\u65f6\u95f4\u5e8f\u5217\uff0c\u6570\u636e\u7ecf\u8fc7\u4e25\u683c\u8d28\u91cf\u63a7\u5236\u5e76\u5f00\u653e\u83b7\u53d6\u3002", "motivation": "\u610f\u5927\u5229\u793e\u4f1a\u9762\u4e34\u7a7a\u6c14\u8d28\u91cf\u548c\u6c14\u5019\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u662f\u516c\u5171\u536b\u751f\u548c\u653f\u7b56\u89c4\u5212\u7b49\u591a\u4e2a\u7814\u7a76\u9886\u57df\u7684\u4ea4\u53c9\u70b9\u3002\u76ee\u524d\u7f3a\u4e4f\u6613\u4e8e\u83b7\u53d6\u3001\u5373\u7528\u4e14\u6587\u6863\u5b8c\u5584\u7684\u7a7a\u6c14\u8d28\u91cf\u548c\u6c14\u5019\u6570\u636e\u96c6\u3002", "method": "\u4ece\u6b27\u6d32\u73af\u5883\u7f72\u548c\u54e5\u767d\u5c3c\u8ba1\u5212\u83b7\u53d6\u539f\u59cb\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u52a8\u4fee\u590d\u539f\u59cb\u6587\u4ef6\u3001\u4eba\u5de5\u68c0\u67e5\u7ad9\u70b9\u4fe1\u606f\u3001\u68c0\u6d4b\u548c\u79fb\u9664\u5f02\u5e38\u503c\u3001\u4ee5\u53ca\u57fa\u4e8e\u65e5\u671f\u7684\u65f6\u5e8f\u534f\u8c03\u7b49\u591a\u91cd\u5904\u7406\u6b65\u9aa4\u786e\u4fdd\u6570\u636e\u53ef\u9760\u6027\u548c\u8d28\u91cf\u3002", "result": "\u521b\u5efa\u4e86GRINS AQCLIM\u6570\u636e\u96c6\uff0c\u5305\u542b\u610f\u5927\u5229700\u591a\u4e2a\u76d1\u6d4b\u7ad9\u70b9\u7684\u7a7a\u6c14\u6c61\u67d3\u7269\u6d53\u5ea6\u548c\u6c14\u5019\u53d8\u91cf\u7684\u6bcf\u65e5\u7edf\u8ba1\u6570\u636e\uff08\u6700\u5c0f\u503c\u3001\u56db\u5206\u4f4d\u6570\u3001\u5747\u503c\u3001\u4e2d\u4f4d\u6570\u3001\u6700\u5927\u503c\u7b49\uff09\uff0c\u6570\u636e\u6258\u7ba1\u5728Zenodo\u5e73\u53f0\u9075\u5faa\u5f00\u653e\u83b7\u53d6\u539f\u5219\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u610f\u5927\u5229\u7a7a\u6c14\u8d28\u91cf\u548c\u6c14\u5019\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u6613\u83b7\u53d6\u7684\u6807\u51c6\u5316\u6570\u636e\u8d44\u6e90\uff0c\u652f\u6301\u8de8\u5b66\u79d1\u7814\u7a76\u548c\u653f\u7b56\u89c4\u5212\u3002"}}
{"id": "2602.10530", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.10530", "abs": "https://arxiv.org/abs/2602.10530", "authors": ["Xiucai Ding", "Chao Shen", "Hau-Tieng Wu"], "title": "Generalized Robust Adaptive-Bandwidth Multi-View Manifold Learning in High Dimensions with Noise", "comment": "4 figures", "summary": "Multiview datasets are common in scientific and engineering applications, yet existing fusion methods offer limited theoretical guarantees, particularly in the presence of heterogeneous and high-dimensional noise. We propose Generalized Robust Adaptive-Bandwidth Multiview Diffusion Maps (GRAB-MDM), a new kernel-based diffusion geometry framework for integrating multiple noisy data sources. The key innovation of GRAB-MDM is a {view}-dependent bandwidth selection strategy that adapts to the geometry and noise level of each view, enabling a stable and principled construction of multiview diffusion operators. Under a common-manifold model, we establish asymptotic convergence results and show that the adaptive bandwidths lead to provably robust recovery of the shared intrinsic structure, even when noise levels and sensor dimensions differ across views. Numerical experiments demonstrate that GRAB-MDM significantly improves robustness and embedding quality compared with fixed-bandwidth and equal-bandwidth baselines, and usually outperform existing algorithms. The proposed framework offers a practical and theoretically grounded solution for multiview sensor fusion in high-dimensional noisy environments.", "AI": {"tldr": "GRAB-MDM\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e\u6838\u7684\u591a\u89c6\u56fe\u6269\u6563\u51e0\u4f55\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u56fe\u76f8\u5173\u7684\u81ea\u9002\u5e94\u5e26\u5bbd\u9009\u62e9\u7b56\u7565\uff0c\u5728\u5b58\u5728\u5f02\u6784\u9ad8\u7ef4\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u7a33\u5065\u5730\u6062\u590d\u5171\u4eab\u5185\u5728\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u89c6\u56fe\u878d\u5408\u65b9\u6cd5\u5728\u5f02\u6784\u9ad8\u7ef4\u566a\u58f0\u73af\u5883\u4e0b\u7406\u8bba\u4fdd\u8bc1\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89c6\u56fe\u51e0\u4f55\u548c\u566a\u58f0\u6c34\u5e73\u7684\u7a33\u5065\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGRAB-MDM\u6846\u67b6\uff0c\u91c7\u7528\u89c6\u56fe\u76f8\u5173\u7684\u81ea\u9002\u5e94\u5e26\u5bbd\u9009\u62e9\u7b56\u7565\uff0c\u6839\u636e\u6bcf\u4e2a\u89c6\u56fe\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u566a\u58f0\u6c34\u5e73\u8c03\u6574\u5e26\u5bbd\uff0c\u6784\u5efa\u7a33\u5065\u7684\u591a\u89c6\u56fe\u6269\u6563\u7b97\u5b50\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u81ea\u9002\u5e94\u5e26\u5bbd\u80fd\u7a33\u5065\u6062\u590d\u5171\u4eab\u5185\u5728\u7ed3\u6784\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793aGRAB-MDM\u5728\u9c81\u68d2\u6027\u548c\u5d4c\u5165\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u5e26\u5bbd\u548c\u7b49\u5e26\u5bbd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GRAB-MDM\u4e3a\u9ad8\u7ef4\u566a\u58f0\u73af\u5883\u4e0b\u7684\u591a\u89c6\u56fe\u4f20\u611f\u5668\u878d\u5408\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u6709\u7406\u8bba\u57fa\u7840\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.10714", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10714", "abs": "https://arxiv.org/abs/2602.10714", "authors": ["Max Hird", "Florian Maire", "Jeffrey Negrea"], "title": "A Non-asymptotic Analysis for Learning and Applying a Preconditioner in MCMC", "comment": null, "summary": "Preconditioning is a common method applied to modify Markov chain Monte Carlo algorithms with the goal of making them more efficient. In practice it is often extremely effective, even when the preconditioner is learned from the chain. We analyse and compare the finite-time computational costs of schemes which learn a preconditioner based on the target covariance or the expected Hessian of the target potential with that of a corresponding scheme that does not use preconditioning. We apply our results to the Unadjusted Langevin Algorithm (ULA) for an appropriately regular target, establishing non-asymptotic guarantees for preconditioned ULA which learns its preconditioner. Our results are also applied to the unadjusted underdamped Langevin algorithm in the supplementary material. To do so, we establish non-asymptotic guarantees on the time taken to collect $N$ approximately independent samples from the target for schemes that learn their preconditioners under the assumption that the underlying Markov chain satisfies a contraction condition in the Wasserstein-2 distance. This approximate independence condition, that we formalize, allows us to bridge the non-asymptotic bounds of modern MCMC theory and classical heuristics of effective sample size and mixing time, and is needed to amortise the costs of learning a preconditioner across the many samples it will be used to produce.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u4e2d\u5b66\u4e60\u9884\u6761\u4ef6\u5668\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e0e\u672a\u9884\u6761\u4ef6\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u4e3a\u9884\u6761\u4ef6ULA\u5efa\u7acb\u4e86\u975e\u6e10\u8fd1\u4fdd\u8bc1\u3002", "motivation": "\u9884\u6761\u4ef6\u662f\u63d0\u9ad8MCMC\u7b97\u6cd5\u6548\u7387\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5373\u4f7f\u4ece\u94fe\u4e2d\u5b66\u4e60\u9884\u6761\u4ef6\u5668\u4e5f\u5f80\u5f80\u975e\u5e38\u6709\u6548\u3002\u9700\u8981\u5206\u6790\u5b66\u4e60\u9884\u6761\u4ef6\u5668\uff08\u57fa\u4e8e\u76ee\u6807\u534f\u65b9\u5dee\u6216\u76ee\u6807\u52bf\u7684\u671f\u671bHessian\uff09\u4e0e\u4e0d\u4f7f\u7528\u9884\u6761\u4ef6\u7684\u65b9\u6cd5\u4e4b\u95f4\u7684\u6709\u9650\u65f6\u95f4\u8ba1\u7b97\u6210\u672c\u5dee\u5f02\u3002", "method": "\u5efa\u7acb\u975e\u6e10\u8fd1\u4fdd\u8bc1\uff0c\u5206\u6790\u5b66\u4e60\u9884\u6761\u4ef6\u5668\u7684\u65b9\u6848\u5728Wasserstein-2\u8ddd\u79bb\u6536\u7f29\u6761\u4ef6\u4e0b\u7684\u65f6\u95f4\u6210\u672c\u3002\u5c06\u7ed3\u679c\u5e94\u7528\u4e8e\u672a\u8c03\u6574\u6717\u4e4b\u4e07\u7b97\u6cd5(ULA)\uff0c\u5e76\u6269\u5c55\u5230\u672a\u8c03\u6574\u6b20\u963b\u5c3c\u6717\u4e4b\u4e07\u7b97\u6cd5\u3002\u901a\u8fc7\u5f62\u5f0f\u5316\u7684\u8fd1\u4f3c\u72ec\u7acb\u6027\u6761\u4ef6\uff0c\u5c06\u73b0\u4ee3MCMC\u7406\u8bba\u4e2d\u7684\u975e\u6e10\u8fd1\u754c\u9650\u4e0e\u6709\u6548\u6837\u672c\u91cf\u548c\u6df7\u5408\u65f6\u95f4\u7684\u7ecf\u5178\u542f\u53d1\u5f0f\u65b9\u6cd5\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u4e3a\u5b66\u4e60\u9884\u6761\u4ef6\u5668\u7684\u9884\u6761\u4ef6ULA\u5efa\u7acb\u4e86\u975e\u6e10\u8fd1\u4fdd\u8bc1\uff0c\u80fd\u591f\u91cf\u5316\u5b66\u4e60\u9884\u6761\u4ef6\u5668\u6240\u9700\u7684\u65f6\u95f4\u6210\u672c\uff0c\u5e76\u8bc1\u660e\u8fd9\u4e9b\u6210\u672c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u9884\u6761\u4ef6\u5668\u751f\u6210\u7684\u5927\u91cf\u6837\u672c\u6765\u5206\u644a\u3002", "conclusion": "\u901a\u8fc7\u5f62\u5f0f\u5316\u7684\u8fd1\u4f3c\u72ec\u7acb\u6027\u6846\u67b6\uff0c\u6210\u529f\u5206\u6790\u4e86\u5b66\u4e60\u9884\u6761\u4ef6\u5668\u7684MCMC\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u9884\u6761\u4ef6ULA\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5206\u644a\u5b66\u4e60\u9884\u6761\u4ef6\u5668\u7684\u6210\u672c\uff0c\u4f7f\u9884\u6761\u4ef6\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2602.10332", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10332", "abs": "https://arxiv.org/abs/2602.10332", "authors": ["Runjia Zou", "Daniela Witten", "Brian Williamson"], "title": "Generalized Prediction-Powered Inference, with Application to Binary Classifier Evaluation", "comment": null, "summary": "In the partially-observed outcome setting, a recent set of proposals known as \"prediction-powered inference\" (PPI) involve (i) applying a pre-trained machine learning model to predict the response, and then (ii) using these predictions to obtain an estimator of the parameter of interest with asymptotic variance no greater than that which would be obtained using only the labeled observations. While existing PPI proposals consider estimators arising from M-estimation, in this paper we generalize PPI to any regular asymptotically linear estimator. Furthermore, by situating PPI within the context of an existing rich literature on missing data and semi-parametric efficiency theory, we show that while PPI does not achieve the semi-parametric efficiency lower bound outside of very restrictive and unrealistic scenarios, it can be viewed as a computationally-simple alternative to proposals in that literature. We exploit connections to that literature to propose modified PPI estimators that can handle three distinct forms of covariate distribution shift. Finally, we illustrate these developments by constructing PPI estimators of true positive rate, false positive rate, and area under the curve via numerical studies.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u9884\u6d4b\u9a71\u52a8\u63a8\u65ad\uff08PPI\uff09\u65b9\u6cd5\uff0c\u5c06\u5176\u63a8\u5e7f\u5230\u4efb\u4f55\u6b63\u5219\u6e10\u8fd1\u7ebf\u6027\u4f30\u8ba1\u91cf\uff0c\u5e76\u63ed\u793a\u4e86PPI\u5728\u534a\u53c2\u6570\u6548\u7387\u7406\u8bba\u4e2d\u7684\u4f4d\u7f6e\uff0c\u63d0\u51fa\u4e86\u5904\u7406\u534f\u53d8\u91cf\u5206\u5e03\u504f\u79fb\u7684\u6539\u8fdbPPI\u4f30\u8ba1\u91cf\u3002", "motivation": "\u73b0\u6709PPI\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8eM-\u4f30\u8ba1\u91cf\uff0c\u4e14\u5728\u534a\u53c2\u6570\u6548\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u65e8\u5728\u5c06PPI\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u4f30\u8ba1\u91cf\u7c7b\u522b\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u7f3a\u5931\u6570\u636e\u548c\u534a\u53c2\u6570\u6548\u7387\u7406\u8bba\u6846\u67b6\u4e0b\u7684\u6027\u8d28\uff0c\u540c\u65f6\u89e3\u51b3\u534f\u53d8\u91cf\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "method": "\u5c06PPI\u7f6e\u4e8e\u7f3a\u5931\u6570\u636e\u548c\u534a\u53c2\u6570\u6548\u7387\u7406\u8bba\u6846\u67b6\u4e2d\uff0c\u5206\u6790\u5176\u6548\u7387\u754c\u9650\uff1b\u63d0\u51fa\u6539\u8fdb\u7684PPI\u4f30\u8ba1\u91cf\u6765\u5904\u7406\u4e09\u79cd\u534f\u53d8\u91cf\u5206\u5e03\u504f\u79fb\uff1b\u901a\u8fc7\u6570\u503c\u7814\u7a76\u6784\u5efa\u771f\u9633\u6027\u7387\u3001\u5047\u9633\u6027\u7387\u548cAUC\u7684PPI\u4f30\u8ba1\u91cf\u3002", "result": "PPI\u53ef\u4ee5\u63a8\u5e7f\u5230\u4efb\u4f55\u6b63\u5219\u6e10\u8fd1\u7ebf\u6027\u4f30\u8ba1\u91cf\uff1b\u5728\u975e\u9650\u5236\u6027\u573a\u666f\u4e0b\uff0cPPI\u65e0\u6cd5\u8fbe\u5230\u534a\u53c2\u6570\u6548\u7387\u4e0b\u754c\uff0c\u4f46\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6848\uff1b\u63d0\u51fa\u7684\u6539\u8fdbPPI\u4f30\u8ba1\u91cf\u80fd\u6709\u6548\u5904\u7406\u534f\u53d8\u91cf\u5206\u5e03\u504f\u79fb\u3002", "conclusion": "PPI\u662f\u534a\u53c2\u6570\u6548\u7387\u7406\u8bba\u4e2d\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u7b80\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u4f30\u8ba1\u91cf\u7c7b\u522b\u548c\u89e3\u51b3\u534f\u53d8\u91cf\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u63a8\u65ad\u5de5\u5177\u3002"}}
{"id": "2602.10784", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.10784", "abs": "https://arxiv.org/abs/2602.10784", "authors": ["Rouven Michels", "Robert Bajons", "Jan-Ole Fischer"], "title": "Integrating Unsupervised and Supervised Learning for the Prediction of Defensive Schemes in American football", "comment": null, "summary": "Anticipating defensive coverage schemes is a crucial yet challenging task for offenses in American football. Because defenders' assignments are intentionally disguised before the snap, they remain difficult to recognize in real time. To address this challenge, we develop a statistical framework that integrates supervised and unsupervised learning using player tracking data. Our goal is to forecast the defensive coverage scheme -- man or zone -- through elastic net logistic regression and gradient-boosted decision trees with incrementally derived features. We first use features from the pre-motion situation, then incorporate players' trajectories during motion in a naive way, and finally include features derived from a hidden Markov model (HMM). Based on player movements, the non-homogeneous HMM infers latent defensive assignments between offensive and defensive players during motion and transforms decoded state sequences into informative features for the supervised models. These HMM-based features enhance predictive performance and are significantly associated with coverage outcomes. Moreover, estimated random effects offer interpretable insights into how different defenses and positions adjust their coverage responsibilities.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u7403\u5458\u8ffd\u8e2a\u6570\u636e\u9884\u6d4b\u7f8e\u5f0f\u8db3\u7403\u9632\u5b88\u9635\u578b\uff08\u4eba\u76ef\u4eba\u6216\u533a\u57df\u9632\u5b88\uff09\uff0c\u901a\u8fc7\u5f39\u6027\u7f51\u7edc\u903b\u8f91\u56de\u5f52\u548c\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u7684\u7279\u5f81\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u7f8e\u5f0f\u8db3\u7403\u6bd4\u8d5b\u4e2d\uff0c\u9884\u6d4b\u9632\u5b88\u9635\u578b\u5bf9\u8fdb\u653b\u65b9\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u9632\u5b88\u65b9\u7684\u4efb\u52a1\u5206\u914d\u5728\u5f00\u7403\u524d\u4f1a\u88ab\u6545\u610f\u9690\u85cf\uff0c\u96be\u4ee5\u5b9e\u65f6\u8bc6\u522b\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u4f7f\u7528\u7403\u5458\u8ffd\u8e2a\u6570\u636e\u3002\u9996\u5148\u4f7f\u7528\u5f00\u7403\u524d\u7684\u7279\u5f81\uff0c\u7136\u540e\u52a0\u5165\u7403\u5458\u79fb\u52a8\u8f68\u8ff9\uff0c\u6700\u540e\u5f15\u5165\u57fa\u4e8e\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u7684\u7279\u5f81\u3002HMM\u63a8\u65ad\u8fdb\u653b\u548c\u9632\u5b88\u7403\u5458\u4e4b\u95f4\u7684\u6f5c\u5728\u4efb\u52a1\u5206\u914d\uff0c\u5e76\u5c06\u89e3\u7801\u7684\u72b6\u6001\u5e8f\u5217\u8f6c\u5316\u4e3a\u76d1\u7763\u6a21\u578b\u7684\u7279\u5f81\u3002", "result": "\u57fa\u4e8eHMM\u7684\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u9632\u5b88\u9635\u578b\u7ed3\u679c\u6709\u663e\u8457\u5173\u8054\u3002\u4f30\u8ba1\u7684\u968f\u673a\u6548\u5e94\u63d0\u4f9b\u4e86\u5173\u4e8e\u4e0d\u540c\u9632\u5b88\u9635\u578b\u548c\u4f4d\u7f6e\u5982\u4f55\u8c03\u6574\u9632\u5b88\u8d23\u4efb\u7684\u89e3\u91ca\u6027\u89c1\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u8ba1\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u6709\u6548\u9884\u6d4b\u7f8e\u5f0f\u8db3\u7403\u9632\u5b88\u9635\u578b\uff0cHMM\u7279\u5f81\u4e0d\u4ec5\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u9632\u5b88\u7b56\u7565\u8c03\u6574\u7684\u6df1\u5165\u7406\u89e3\u3002"}}
{"id": "2602.10531", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.10531", "abs": "https://arxiv.org/abs/2602.10531", "authors": ["Soham Bakshi", "Sunrit Chakraborty"], "title": "From Collapse to Improvement: Statistical Perspectives on the Evolutionary Dynamics of Iterative Training on Contaminated Sources", "comment": null, "summary": "The problem of model collapse has presented new challenges in iterative training of generative models, where such training with synthetic data leads to an overall degradation of performance. This paper looks at the problem from a statistical viewpoint, illustrating that one can actually hope for improvement when models are trained on data contaminated with synthetic samples, as long as there is some amount of fresh information from the true target distribution. In particular, we consider iterative training on samples sourced from a mixture of the true target and synthetic distributions. We analyze the entire iterative evolution in a next-token prediction language model, capturing how the interplay between the mixture weights and the sample size controls the overall long-term performance. With non-trivial mixture weight of the true distribution, even if it decays over time, simply training the model in a contamination-agnostic manner with appropriate sample sizes can avoid collapse and even recover the true target distribution under certain conditions. Simulation studies support our findings and also show that such behavior is more general for other classes of models.", "AI": {"tldr": "\u8bba\u6587\u4ece\u7edf\u8ba1\u89d2\u5ea6\u5206\u6790\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u53d1\u73b0\u5728\u771f\u5b9e\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u6df7\u5408\u8bad\u7ec3\u65f6\uff0c\u53ea\u8981\u6709\u8db3\u591f\u771f\u5b9e\u4fe1\u606f\uff0c\u9002\u5f53\u8c03\u6574\u6df7\u5408\u6743\u91cd\u548c\u6837\u672c\u91cf\u53ef\u4ee5\u907f\u514d\u5d29\u6e83\u751a\u81f3\u6062\u590d\u771f\u5b9e\u5206\u5e03\u3002", "motivation": "\u6a21\u578b\u5d29\u6e83\u95ee\u9898\u5728\u751f\u6210\u6a21\u578b\u7684\u8fed\u4ee3\u8bad\u7ec3\u4e2d\u5e26\u6765\u4e86\u65b0\u6311\u6218\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6027\u80fd\u6574\u4f53\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u4ece\u7edf\u8ba1\u89d2\u5ea6\u5206\u6790\u8fd9\u4e00\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u6570\u636e\u88ab\u5408\u6210\u6837\u672c\u6c61\u67d3\u7684\u60c5\u51b5\u4e0b\uff0c\u53ea\u8981\u6709\u771f\u5b9e\u76ee\u6807\u5206\u5e03\u7684\u65b0\u9c9c\u4fe1\u606f\uff0c\u6a21\u578b\u6027\u80fd\u5b9e\u9645\u4e0a\u53ef\u80fd\u5f97\u5230\u6539\u5584\u7684\u53ef\u80fd\u6027\u3002", "method": "\u4ece\u7edf\u8ba1\u89c6\u89d2\u5206\u6790\u8fed\u4ee3\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u8003\u8651\u4ece\u771f\u5b9e\u76ee\u6807\u548c\u5408\u6210\u5206\u5e03\u7684\u6df7\u5408\u4e2d\u91c7\u6837\u7684\u6570\u636e\u3002\u5206\u6790\u6574\u4e2a\u8fed\u4ee3\u6f14\u5316\u8fc7\u7a0b\uff0c\u7279\u522b\u5173\u6ce8\u5728\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u6df7\u5408\u6743\u91cd\u548c\u6837\u672c\u5927\u5c0f\u5982\u4f55\u5171\u540c\u63a7\u5236\u957f\u671f\u6027\u80fd\u3002\u91c7\u7528\u6c61\u67d3\u4e0d\u53ef\u77e5\u7684\u65b9\u5f0f\u8bad\u7ec3\u6a21\u578b\uff0c\u7814\u7a76\u6df7\u5408\u6743\u91cd\u548c\u6837\u672c\u91cf\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u53ea\u8981\u6709\u975e\u5e73\u51e1\u7684\uff08\u5373\u4f7f\u968f\u65f6\u95f4\u8870\u51cf\u7684\uff09\u771f\u5b9e\u5206\u5e03\u6df7\u5408\u6743\u91cd\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u6837\u672c\u5927\u5c0f\u8fdb\u884c\u6c61\u67d3\u4e0d\u53ef\u77e5\u8bad\u7ec3\uff0c\u53ef\u4ee5\u907f\u514d\u6a21\u578b\u5d29\u6e83\u3002\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\uff0c\u751a\u81f3\u53ef\u4ee5\u6062\u590d\u771f\u5b9e\u76ee\u6807\u5206\u5e03\u3002\u6a21\u62df\u7814\u7a76\u652f\u6301\u8fd9\u4e9b\u53d1\u73b0\uff0c\u5e76\u8868\u660e\u8fd9\u79cd\u884c\u4e3a\u5bf9\u5176\u4ed6\u7c7b\u578b\u7684\u6a21\u578b\u4e5f\u5177\u6709\u666e\u904d\u6027\u3002", "conclusion": "\u6a21\u578b\u5d29\u6e83\u5e76\u975e\u4e0d\u53ef\u907f\u514d\uff0c\u901a\u8fc7\u9002\u5f53\u63a7\u5236\u771f\u5b9e\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u7684\u6df7\u5408\u6bd4\u4f8b\u4ee5\u53ca\u6837\u672c\u5927\u5c0f\uff0c\u8fed\u4ee3\u8bad\u7ec3\u53ef\u4ee5\u907f\u514d\u6027\u80fd\u4e0b\u964d\uff0c\u751a\u81f3\u80fd\u591f\u6062\u590d\u771f\u5b9e\u5206\u5e03\u3002\u8fd9\u4e3a\u751f\u6210\u6a21\u578b\u7684\u53ef\u6301\u7eed\u8fed\u4ee3\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.11108", "categories": ["stat.CO", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.11108", "abs": "https://arxiv.org/abs/2602.11108", "authors": ["Jocelyn T. Chi"], "title": "Large Scale High-Dimensional Reduced-Rank Linear Discriminant Analysis", "comment": null, "summary": "Reduced-rank linear discriminant analysis (RRLDA) is a foundational method of dimension reduction for classification that has been useful in a wide range of applications. The goal is to identify an optimal subspace to project the observations onto that simultaneously maximizes between-group variation while minimizing within-group differences. The solution is straight forward when the number of observations is greater than the number of features but computational difficulties arise in both the high-dimensional setting, where there are more features than there are observations, and when the data are very large. Many works have proposed solutions for the high-dimensional setting and frequently involve additional assumptions or tuning parameters. We propose a fast and simple iterative algorithm for both classical and high-dimensional RRLDA on large data that is free from these additional requirements and that comes with guarantees. We also explain how RRLDA-RK provides implicit regularization towards the least norm solution without explicitly incorporating penalties. We demonstrate our algorithm on real data and highlight some results.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5feb\u901f\u7b80\u5355\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u7684\u7ecf\u5178\u548c\u9ad8\u7ef4\u964d\u79e9\u7ebf\u6027\u5224\u522b\u5206\u6790\uff0c\u65e0\u9700\u989d\u5916\u5047\u8bbe\u6216\u8c03\u53c2\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u964d\u79e9\u7ebf\u6027\u5224\u522b\u5206\u6790\uff08RRLDA\uff09\u5728\u7279\u5f81\u6570\u5927\u4e8e\u6837\u672c\u6570\u7684\u9ad8\u7ef4\u8bbe\u7f6e\u548c\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u5b58\u5728\u8ba1\u7b97\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u5047\u8bbe\u6216\u8c03\u53c2\u53c2\u6570\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5feb\u901f\u7b80\u5355\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u7ecf\u5178\u548c\u9ad8\u7ef4RRLDA\uff0c\u65e0\u9700\u989d\u5916\u8981\u6c42\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u89e3\u91ca\u4e86RRLDA-RK\u5982\u4f55\u9690\u5f0f\u6b63\u5219\u5316\u8d8b\u5411\u6700\u5c0f\u8303\u6570\u89e3\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u6f14\u793a\u4e86\u7b97\u6cd5\u5e76\u7a81\u51fa\u4e86\u4e00\u4e9b\u7ed3\u679c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u7684\u964d\u79e9\u7ebf\u6027\u5224\u522b\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u5feb\u901f\u4e14\u65e0\u9700\u989d\u5916\u8c03\u53c2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.10348", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.10348", "abs": "https://arxiv.org/abs/2602.10348", "authors": ["Liangbo Lyu", "Bingkai Wang"], "title": "Optimizing precision in stepped-wedge designs via machine learning and quadratic inference functions", "comment": null, "summary": "Stepped-wedge designs are increasingly used in randomized experiments to accommodate logistical and ethical constraints by staggering treatment roll-out over time. Despite their popularity, existing analytical methods largely rely on parametric models with linear covariate adjustment and prespecified correlation structures, which may limit achievable precision in practice. We propose a new class of estimators for the causal average treatment effect in stepped-wedge designs that optimizes precision through flexible, machine-learning-based covariate adjustment to capture complex outcome-covariate relationships, together with quadratic inference functions to adaptively learn the correlation structure. We establish consistency and asymptotic normality under mild conditions requiring only $L_2$ convergence of nuisance estimators, even under model misspecification, and characterize when the estimator attains the minimal asymptotic variance. Moreover, we prove that the proposed estimator never reduces efficiency relative to an independence working correlation. The proposed method further accommodates treatment-effect heterogeneity across both exposure duration and calendar time. Finally, we demonstrate our methods through simulation studies and reanalyses of two empirical studies that differ substantially in research area and key design parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u9636\u68af\u6954\u5f62\u8bbe\u8ba1\u7684\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u534f\u53d8\u91cf\u8c03\u6574\u548c\u4e8c\u6b21\u63a8\u65ad\u51fd\u6570\u4f18\u5316\u7cbe\u5ea6", "motivation": "\u9636\u68af\u6954\u5f62\u8bbe\u8ba1\u5728\u968f\u673a\u5b9e\u9a8c\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u53c2\u6570\u6a21\u578b\u548c\u9884\u8bbe\u76f8\u5173\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u53ef\u8fbe\u5230\u7684\u7cbe\u5ea6", "method": "\u63d0\u51fa\u65b0\u4f30\u8ba1\u5668\u7c7b\uff0c\u901a\u8fc7\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7075\u6d3b\u534f\u53d8\u91cf\u8c03\u6574\u6355\u83b7\u590d\u6742\u7ed3\u679c-\u534f\u53d8\u91cf\u5173\u7cfb\uff0c\u7ed3\u5408\u4e8c\u6b21\u63a8\u65ad\u51fd\u6570\u81ea\u9002\u5e94\u5b66\u4e60\u76f8\u5173\u7ed3\u6784", "result": "\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u5efa\u7acb\u4e00\u81f4\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u5373\u4f7f\u6a21\u578b\u8bef\u8bbe\u4e5f\u6210\u7acb\uff1b\u8bc1\u660e\u4f30\u8ba1\u5668\u76f8\u5bf9\u4e8e\u72ec\u7acb\u5de5\u4f5c\u76f8\u5173\u7ed3\u6784\u4e0d\u4f1a\u964d\u4f4e\u6548\u7387\uff1b\u80fd\u5904\u7406\u66b4\u9732\u65f6\u95f4\u548c\u65e5\u5386\u65f6\u95f4\u7684\u5904\u7406\u6548\u5e94\u5f02\u8d28\u6027", "conclusion": "\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u548c\u4e24\u4e2a\u5b9e\u8bc1\u7814\u7a76\u7684\u91cd\u65b0\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9636\u68af\u6954\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u66f4\u7cbe\u786e\u7684\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u6846\u67b6"}}
{"id": "2602.10955", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.10955", "abs": "https://arxiv.org/abs/2602.10955", "authors": ["Garazi Retegui", "Mar\u00eda Dolores Ugarte", "Jaione Etxeberria", "Alan E. Gelfand"], "title": "Prior Smoothing for Multivariate Disease Mapping Models", "comment": null, "summary": "To date, we have seen the emergence of a large literature on multivariate disease mapping. That is, incidence of (or mortality from) multiple diseases is recorded at the scale of areal units where incidence (mortality) across the diseases is expected to manifest dependence. The modeling involves a hierarchical structure: a Poisson model for disease counts (conditioning on the rates) at the first stage, and a specification of a function of the rates using spatial random effects at the second stage. These random effects are specified as a prior and introduce spatial smoothing to the rate (or risk) estimates. What we see in the literature is the amount of smoothing induced under a given prior across areal units compared with the observed/empirical risks. Our contribution here extends previous research on smoothing in univariate areal data models. Specifically, for three different choices of multivariate prior, we investigate both within prior smoothing according to hyperparameters and across prior smoothing. Its benefit to the user is to illuminate the expected nature of departure from perfect fit associated with these priors since model performance is not a question of goodness of fit. We propose both theoretical and empirical metrics for our investigation and illustrate with both simulated and real data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u5143\u75be\u75c5\u6620\u5c04\u4e2d\u7684\u5e73\u6ed1\u6548\u5e94\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u4e0d\u540c\u591a\u5143\u5148\u9a8c\u7684\u5e73\u6ed1\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u6307\u6807\u6765\u8bc4\u4f30\u5148\u9a8c\u5bf9\u75be\u75c5\u98ce\u9669\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u591a\u5143\u75be\u75c5\u6620\u5c04\u6587\u732e\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u5148\u9a8c\u4e0b\u7684\u5e73\u6ed1\u6548\u679c\u4e0e\u7ecf\u9a8c\u98ce\u9669\u7684\u6bd4\u8f83\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u591a\u5143\u5148\u9a8c\u5e73\u6ed1\u7279\u6027\u7684\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55\u5355\u53d8\u91cf\u533a\u57df\u6570\u636e\u6a21\u578b\u7684\u5e73\u6ed1\u7814\u7a76\u5230\u591a\u5143\u60c5\u5883\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5efa\u6a21\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6cca\u677e\u6a21\u578b\u5904\u7406\u75be\u75c5\u8ba1\u6570\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528\u7a7a\u95f4\u968f\u673a\u6548\u5e94\u5efa\u6a21\u7387\u51fd\u6570\u3002\u7814\u7a76\u4e09\u79cd\u4e0d\u540c\u7684\u591a\u5143\u5148\u9a8c\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u6307\u6807\u8bc4\u4f30\u5176\u5e73\u6ed1\u7279\u6027\uff0c\u5305\u62ec\u5148\u9a8c\u5185\u5e73\u6ed1\u548c\u5148\u9a8c\u95f4\u5e73\u6ed1\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u591a\u5143\u5148\u9a8c\u5728\u75be\u75c5\u98ce\u9669\u4f30\u8ba1\u4e2d\u7684\u5e73\u6ed1\u6548\u679c\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u5148\u9a8c\u4e0e\u5b8c\u7f8e\u62df\u5408\u4e4b\u95f4\u7684\u9884\u671f\u504f\u79bb\u7279\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u5143\u75be\u75c5\u6620\u5c04\u4e2d\u7684\u5148\u9a8c\u9009\u62e9\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u4e0d\u540c\u5148\u9a8c\u5bf9\u5e73\u6ed1\u6548\u679c\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u66f4\u597d\u5730\u89e3\u91ca\u6a21\u578b\u7ed3\u679c\u4e0e\u89c2\u6d4b\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002"}}
{"id": "2602.10532", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.10532", "abs": "https://arxiv.org/abs/2602.10532", "authors": ["Justin Whitehouse", "Ayush Sawarni", "Vasilis Syrgkanis"], "title": "Statistical Inference and Learning for Shapley Additive Explanations (SHAP)", "comment": "48 pages, 1 figure", "summary": "The SHAP (short for Shapley additive explanation) framework has become an essential tool for attributing importance to variables in predictive tasks. In model-agnostic settings, SHAP uses the concept of Shapley values from cooperative game theory to fairly allocate credit to the features in a vector $X$ based on their contribution to an outcome $Y$. While the explanations offered by SHAP are local by nature, learners often need global measures of feature importance in order to improve model explainability and perform feature selection. The most common approach for converting these local explanations into global ones is to compute either the mean absolute SHAP or mean squared SHAP. However, despite their ubiquity, there do not exist approaches for performing statistical inference on these quantities.\n  In this paper, we take a semi-parametric approach for calibrating confidence in estimates of the $p$th powers of Shapley additive explanations. We show that, by treating the SHAP curve as a nuisance function that must be estimated from data, one can reliably construct asymptotically normal estimates of the $p$th powers of SHAP. When $p \\geq 2$, we show a de-biased estimator that combines U-statistics with Neyman orthogonal scores for functionals of nested regressions is asymptotically normal. When $1 \\leq p < 2$ (and the hence target parameter is not twice differentiable), we construct de-biased U-statistics for a smoothed alternative. In particular, we show how to carefully tune the temperature parameter of the smoothing function in order to obtain inference for the true, unsmoothed $p$th power. We complement these results by presenting a Neyman orthogonal loss that can be used to learn the SHAP curve via empirical risk minimization and discussing excess risk guarantees for commonly used function classes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u53c2\u6570\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9SHAP\u503c\u7684p\u6b21\u5e42\u8fdb\u884c\u7edf\u8ba1\u63a8\u65ad\uff0c\u5305\u62ec\u6784\u5efa\u6e10\u8fd1\u6b63\u6001\u4f30\u8ba1\u91cf\uff0c\u5e76\u9488\u5bf9\u4e0d\u540cp\u503c\u8303\u56f4\u8bbe\u8ba1\u4e86\u4e0d\u540c\u7684\u53bb\u504f\u7b56\u7565\u3002", "motivation": "SHAP\u6846\u67b6\u5df2\u6210\u4e3a\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d8\u91cf\u91cd\u8981\u6027\u5f52\u56e0\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u7684\u5168\u5c40\u91cd\u8981\u6027\u5ea6\u91cf\uff08\u5982\u5e73\u5747\u7edd\u5bf9SHAP\u6216\u5e73\u5747\u5e73\u65b9SHAP\uff09\u7f3a\u4e4f\u7edf\u8ba1\u63a8\u65ad\u65b9\u6cd5\uff0c\u65e0\u6cd5\u91cf\u5316\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u534a\u53c2\u6570\u65b9\u6cd5\uff0c\u5c06SHAP\u66f2\u7ebf\u89c6\u4e3a\u9700\u8981\u4ece\u6570\u636e\u4e2d\u4f30\u8ba1\u7684\u5197\u4f59\u51fd\u6570\u3002\u5bf9\u4e8ep\u22652\u7684\u60c5\u51b5\uff0c\u4f7f\u7528\u7ed3\u5408U\u7edf\u8ba1\u91cf\u548cNeyman\u6b63\u4ea4\u5206\u6570\u7684\u53bb\u504f\u4f30\u8ba1\u91cf\uff1b\u5bf9\u4e8e1\u2264p<2\u7684\u60c5\u51b5\uff0c\u6784\u5efa\u5e73\u6ed1\u66ff\u4ee3\u7684\u53bb\u504fU\u7edf\u8ba1\u91cf\uff0c\u5e76\u7cbe\u5fc3\u8c03\u6574\u6e29\u5ea6\u53c2\u6570\u4ee5\u83b7\u5f97\u5bf9\u672a\u5e73\u6ed1p\u6b21\u5e42\u7684\u63a8\u65ad\u3002", "result": "\u80fd\u591f\u53ef\u9760\u5730\u6784\u5efaSHAP\u503cp\u6b21\u5e42\u7684\u6e10\u8fd1\u6b63\u6001\u4f30\u8ba1\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u7528\u4e8e\u901a\u8fc7\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5b66\u4e60SHAP\u66f2\u7ebf\u7684Neyman\u6b63\u4ea4\u635f\u5931\u51fd\u6570\uff0c\u8ba8\u8bba\u4e86\u5e38\u7528\u51fd\u6570\u7c7b\u7684\u8d85\u989d\u98ce\u9669\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aSHAP\u503c\u7684\u5168\u5c40\u91cd\u8981\u6027\u5ea6\u91cf\u63d0\u4f9b\u4e86\u7edf\u8ba1\u63a8\u65ad\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u7279\u5f81\u9009\u62e9\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2602.10484", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.10484", "abs": "https://arxiv.org/abs/2602.10484", "authors": ["Zhaowen Wang", "Yutao Liu", "Deyuan Li"], "title": "CoVaR under Asymptotic Independence", "comment": null, "summary": "Conditional value-at-risk (CoVaR) is one of the most important measures of systemic risk. It is defined as the high quantile conditional on a related variable being extreme, widely used in the field of quantitative risk management. In this work, we develop a semi-parametric methodology to estimate CoVaR for asymptotically independent pairs within the framework of bivariate extreme value theory. We use parametric modelling of the bivariate extremal structure to address data sparsity in the joint tail regions and prove consistency and asymptotic normality of the proposed estimator. The robust performance of the estimator is illustrated via simulation studies. Its application to the US stock returns data produces insightful dynamic CoVaR forecasts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u53c2\u6570\u65b9\u6cd5\u6765\u4f30\u8ba1\u6e10\u8fd1\u72ec\u7acb\u5bf9\u7684CoVaR\uff0c\u7ed3\u5408\u53c2\u6570\u6781\u503c\u5efa\u6a21\u89e3\u51b3\u8054\u5408\u5c3e\u90e8\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u4f30\u8ba1\u91cf\u7684\u76f8\u5408\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u6027\u3002", "motivation": "CoVaR\u662f\u7cfb\u7edf\u6027\u98ce\u9669\u7684\u91cd\u8981\u5ea6\u91cf\u6307\u6807\uff0c\u5b9a\u4e49\u4e3a\u76f8\u5173\u53d8\u91cf\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u9ad8\u5206\u4f4d\u6570\u3002\u7136\u800c\uff0c\u5728\u6e10\u8fd1\u72ec\u7acb\u5bf9\u7684\u8054\u5408\u5c3e\u90e8\u533a\u57df\uff0c\u6570\u636e\u7a00\u758f\u95ee\u9898\u4f7f\u5f97CoVaR\u4f30\u8ba1\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u7edf\u8ba1\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u534a\u53c2\u6570\u65b9\u6cd5\u4f30\u8ba1\u6e10\u8fd1\u72ec\u7acb\u5bf9\u7684CoVaR\uff0c\u5728\u4e8c\u5143\u6781\u503c\u7406\u8bba\u6846\u67b6\u4e0b\uff0c\u4f7f\u7528\u53c2\u6570\u5efa\u6a21\u6765\u63cf\u8ff0\u4e8c\u5143\u6781\u503c\u7ed3\u6784\uff0c\u4ee5\u89e3\u51b3\u8054\u5408\u5c3e\u90e8\u533a\u57df\u7684\u6570\u636e\u7a00\u758f\u95ee\u9898\u3002", "result": "\u8bc1\u660e\u4e86\u6240\u63d0\u4f30\u8ba1\u91cf\u7684\u76f8\u5408\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u5c55\u793a\u4e86\u4f30\u8ba1\u91cf\u7684\u7a33\u5065\u6027\u80fd\uff0c\u5e94\u7528\u4e8e\u7f8e\u56fd\u80a1\u7968\u6536\u76ca\u7387\u6570\u636e\u4ea7\u751f\u4e86\u6709\u6d1e\u5bdf\u529b\u7684\u52a8\u6001CoVaR\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6e10\u8fd1\u72ec\u7acb\u5bf9\u7684CoVaR\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u534a\u53c2\u6570\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u8054\u5408\u5c3e\u90e8\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u5728\u7cfb\u7edf\u6027\u98ce\u9669\u7ba1\u7406\u548c\u91d1\u878d\u98ce\u9669\u9884\u6d4b\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.11059", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.11059", "abs": "https://arxiv.org/abs/2602.11059", "authors": ["Jean-Fran\u00e7ois Giovannelli"], "title": "A Gibbs posterior sampler for inverse problem based on prior diffusion model", "comment": null, "summary": "This paper addresses the issue of inversion in cases where (1) the observation system is modeled by a linear transformation and additive noise, (2) the problem is ill-posed and regularization is introduced in a Bayesian framework by an a prior density, and (3) the latter is modeled by a diffusion process adjusted on an available large set of examples. In this context, it is known that the issue of posterior sampling is a thorny one. This paper introduces a Gibbs algorithm. It appears that this avenue has not been explored, and we show that this approach is particularly effective and remarkably simple. In addition, it offers a guarantee of convergence in a clearly identified situation. The results are clearly confirmed by numerical simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdGibbs\u7b97\u6cd5\u7528\u4e8e\u89e3\u51b3\u7ebf\u6027\u89c2\u6d4b\u7cfb\u7edf\u52a0\u566a\u58f0\u3001\u75c5\u6001\u95ee\u9898\u8d1d\u53f6\u65af\u6b63\u5219\u5316\u3001\u5148\u9a8c\u57fa\u4e8e\u6269\u6563\u8fc7\u7a0b\u7684\u540e\u9a8c\u91c7\u6837\u96be\u9898\uff0c\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u4e14\u6709\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff08\u7ebf\u6027\u89c2\u6d4b\u7cfb\u7edf\u52a0\u566a\u58f0\u3001\u75c5\u6001\u95ee\u9898\u8d1d\u53f6\u65af\u6b63\u5219\u5316\u3001\u5148\u9a8c\u57fa\u4e8e\u6269\u6563\u8fc7\u7a0b\uff09\u540e\u9a8c\u91c7\u6837\u7684\u56f0\u96be\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u5c1a\u672a\u5f97\u5230\u6709\u6548\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e00\u79cdGibbs\u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u7ed9\u5b9a\u6761\u4ef6\u4e0b\u7279\u522b\u6709\u6548\u4e14\u7b80\u5355\uff0c\u63d0\u4f9b\u4e86\u5728\u660e\u786e\u8bc6\u522b\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u6570\u503c\u6a21\u62df\u7ed3\u679c\u660e\u786e\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8fd9\u79cd\u5148\u524d\u672a\u88ab\u63a2\u7d22\u7684\u9014\u5f84\u7279\u522b\u6709\u6548\u3002", "conclusion": "Gibbs\u7b97\u6cd5\u4e3a\u89e3\u51b3\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u540e\u9a8c\u91c7\u6837\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2602.10696", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.10696", "abs": "https://arxiv.org/abs/2602.10696", "authors": ["Miao Lu", "Yuxuan Han", "Han Zhong", "Zhengyuan Zhou", "Jose Blanchet"], "title": "Robust Assortment Optimization from Observational Data", "comment": "65 pages, 9 figures", "summary": "Assortment optimization is a fundamental challenge in modern retail and recommendation systems, where the goal is to select a subset of products that maximizes expected revenue under complex customer choice behaviors. While recent advances in data-driven methods have leveraged historical data to learn and optimize assortments, these approaches typically rely on strong assumptions -- namely, the stability of customer preferences and the correctness of the underlying choice models. However, such assumptions frequently break in real-world scenarios due to preference shifts and model misspecification, leading to poor generalization and revenue loss. Motivated by this limitation, we propose a robust framework for data-driven assortment optimization that accounts for potential distributional shifts in customer choice behavior. Our approach models potential preference shift from a nominal choice model that generates data and seeks to maximize worst-case expected revenue. We first establish the computational tractability of robust assortment planning when the nominal model is known, then advance to the data-driven setting, where we design statistically optimal algorithms that minimize the data requirements while maintaining robustness. Our theoretical analysis provides both upper bounds and matching lower bounds on the sample complexity, offering theoretical guarantees for robust generalization. Notably, we uncover and identify the notion of ``robust item-wise coverage'' as the minimal data requirement to enable sample-efficient robust assortment learning. Our work bridges the gap between robustness and statistical efficiency in assortment learning, contributing new insights and tools for reliable assortment optimization under uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8003\u8651\u5ba2\u6237\u9009\u62e9\u884c\u4e3a\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u6570\u636e\u9a71\u52a8\u5546\u54c1\u7ec4\u5408\u4f18\u5316\u6846\u67b6\uff0c\u5728\u5df2\u77e5\u540d\u4e49\u6a21\u578b\u65f6\u8ba1\u7b97\u53ef\u5904\u7406\uff0c\u5728\u6570\u636e\u9a71\u52a8\u573a\u666f\u4e2d\u8bbe\u8ba1\u7edf\u8ba1\u6700\u4f18\u7b97\u6cd5\uff0c\u5e76\u5efa\u7acb\u6837\u672c\u590d\u6742\u5ea6\u7684\u4e0a\u4e0b\u754c\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u7684\u5546\u54c1\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5ba2\u6237\u504f\u597d\u7a33\u5b9a\u4e14\u9009\u62e9\u6a21\u578b\u6b63\u786e\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7ecf\u5e38\u5931\u6548\uff08\u504f\u597d\u504f\u79fb\u548c\u6a21\u578b\u8bef\u8bbe\uff09\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u6536\u5165\u635f\u5931\u3002\u9700\u8981\u5f00\u53d1\u80fd\u5e94\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u9c81\u68d2\u6846\u67b6\uff0c\u4ece\u751f\u6210\u6570\u636e\u7684\u540d\u4e49\u9009\u62e9\u6a21\u578b\u51fa\u53d1\uff0c\u5efa\u6a21\u6f5c\u5728\u504f\u597d\u504f\u79fb\uff0c\u6700\u5927\u5316\u6700\u574f\u60c5\u51b5\u671f\u671b\u6536\u5165\u3002\u9996\u5148\u5728\u5df2\u77e5\u540d\u4e49\u6a21\u578b\u65f6\u8bc1\u660e\u8ba1\u7b97\u53ef\u5904\u7406\u6027\uff0c\u7136\u540e\u5728\u6570\u636e\u9a71\u52a8\u573a\u666f\u4e2d\u8bbe\u8ba1\u7edf\u8ba1\u6700\u4f18\u7b97\u6cd5\uff0c\u6700\u5c0f\u5316\u6570\u636e\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u6837\u672c\u590d\u6742\u5ea6\u7684\u4e0a\u754c\u548c\u5339\u914d\u4e0b\u754c\uff0c\u4e3a\u9c81\u68d2\u6cdb\u5316\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002\u7279\u522b\u5730\uff0c\u53d1\u73b0\u4e86\"\u9c81\u68d2\u9010\u9879\u8986\u76d6\"\u6982\u5ff5\u4f5c\u4e3a\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u9c81\u68d2\u5546\u54c1\u7ec4\u5408\u5b66\u4e60\u7684\u6700\u5c0f\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86\u5546\u54c1\u7ec4\u5408\u5b66\u4e60\u4e2d\u9c81\u68d2\u6027\u548c\u7edf\u8ba1\u6548\u7387\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53ef\u9760\u5546\u54c1\u7ec4\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u548c\u5de5\u5177\u3002"}}
{"id": "2602.10538", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10538", "abs": "https://arxiv.org/abs/2602.10538", "authors": ["Sho Sonoda", "Shunta Akiyama", "Yuya Uezato"], "title": "Why Agentic Theorem Prover Works: A Statistical Provability Theory of Mathematical Reasoning Models", "comment": null, "summary": "Agentic theorem provers -- pipelines that couple a mathematical reasoning model with library retrieval, subgoal-decomposition/search planner, and a proof assistant verifier -- have recently achieved striking empirical success, yet it remains unclear which components drive performance and why such systems work at all despite classical hardness of proof search. We propose a distributional viewpoint and introduce **statistical provability**, defined as the finite-horizon success probability of reaching a verified proof, averaged over an instance distribution, and formalize modern theorem-proving pipelines as time-bounded MDPs. Exploiting Bellman structure, we prove existence of optimal policies under mild regularity, derive provability certificates via sub-/super-solution inequalities, and bound the performance gap of score-guided planning (greedy/top-\\(k\\)/beam/rollouts) in terms of approximation error, sequential statistical complexity, representation geometry (metric entropy/doubling structure), and action-gap margin tails. Together, our theory provides a principled, component-sensitive explanation of when and why agentic theorem provers succeed on biased real-world problem distributions, while clarifying limitations in worst-case or adversarial regimes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7edf\u8ba1\u53ef\u8bc1\u6027\u6982\u5ff5\uff0c\u5c06\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u5efa\u6a21\u4e3a\u65f6\u95f4\u6709\u754cMDP\uff0c\u5206\u6790\u5404\u7ec4\u4ef6\u6027\u80fd\u5e76\u7ed9\u51fa\u7406\u8bba\u89e3\u91ca", "motivation": "\u5c3d\u7ba1\u667a\u80fd\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5404\u7ec4\u4ef6\u5982\u4f55\u9a71\u52a8\u6027\u80fd\uff0c\u4ee5\u53ca\u4e3a\u4f55\u80fd\u5728\u7ecf\u5178\u8bc1\u660e\u641c\u7d22\u96be\u9898\u4e0b\u5de5\u4f5c\u3002\u9700\u8981\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u5176\u6210\u529f\u539f\u56e0\u548c\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u7edf\u8ba1\u53ef\u8bc1\u6027\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e3a\u5728\u5b9e\u4f8b\u5206\u5e03\u4e0a\u8fbe\u5230\u5df2\u9a8c\u8bc1\u8bc1\u660e\u7684\u6709\u9650\u65f6\u57df\u6210\u529f\u6982\u7387\u3002\u5c06\u73b0\u4ee3\u5b9a\u7406\u8bc1\u660e\u6d41\u7a0b\u5efa\u6a21\u4e3a\u65f6\u95f4\u6709\u754cMDP\uff0c\u5229\u7528\u8d1d\u5c14\u66fc\u7ed3\u6784\u5206\u6790\u6700\u4f18\u7b56\u7565\u5b58\u5728\u6027\uff0c\u901a\u8fc7\u5b50/\u8d85\u89e3\u4e0d\u7b49\u5f0f\u83b7\u5f97\u53ef\u8bc1\u6027\u8bc1\u4e66\uff0c\u5e76\u5206\u6790\u8bc4\u5206\u5f15\u5bfc\u89c4\u5212\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6e29\u548c\u6b63\u5219\u6761\u4ef6\u4e0b\u6700\u4f18\u7b56\u7565\u7684\u5b58\u5728\u6027\uff0c\u63a8\u5bfc\u4e86\u53ef\u8bc1\u6027\u8bc1\u4e66\uff0c\u5e76\u91cf\u5316\u4e86\u8bc4\u5206\u5f15\u5bfc\u89c4\u5212\uff08\u8d2a\u5a6a/top-k/\u675f\u641c\u7d22/\u56de\u6eda\uff09\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u6d89\u53ca\u8fd1\u4f3c\u8bef\u5dee\u3001\u5e8f\u5217\u7edf\u8ba1\u590d\u6742\u5ea6\u3001\u8868\u793a\u51e0\u4f55\u548c\u52a8\u4f5c\u95f4\u9699\u5c3e\u90e8\u5206\u5e03\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u667a\u80fd\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u5728\u504f\u7f6e\u73b0\u5b9e\u95ee\u9898\u5206\u5e03\u4e0a\u7684\u6210\u529f\u63d0\u4f9b\u4e86\u539f\u7406\u6027\u3001\u7ec4\u4ef6\u654f\u611f\u7684\u89e3\u91ca\uff0c\u540c\u65f6\u9610\u660e\u4e86\u5728\u6700\u574f\u60c5\u51b5\u6216\u5bf9\u6297\u673a\u5236\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.10673", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.10673", "abs": "https://arxiv.org/abs/2602.10673", "authors": ["Barbara Bricout", "Laura Dami", "Pierre Defos du Rau", "Sophie Donnet", "Thomas Galewski", "Stephane Robin"], "title": "Inferring the presence and abundance of rare waterbirds species from scarce data", "comment": "31 pages, 9 figures", "summary": "Abundance data are used in ecology for species monitoring and conservation. These count data often display several specific characteristics like numerous missing data, high variance, and a high proportion of zeros, particularly when monitoring rare species. We present a model that aims to impute missing data and estimate the effect of covariates on species presence and abundance. It is based on the log-normal Poisson model, which offers more flexibility in the variance of counts than a Poisson model. A latent variable is added for the overrepresentation of zeros in the data. The imputation of missing data is made possible by assuming that the latent variance matrix has low rank and the inclusion of covariates. \\\\ We demonstrate the identifiability in the presence of missing data. Since maximum likelihood inference is intractable, we use a variational expectation-maximization algorithm to infer the parameters. We provide an estimate of the asymptotic variance of the estimators and derive prediction intervals for the imputations, an estimate of the temporal trend, and a procedure for detecting a potential change in this trend. \\\\ We evaluate our imputations and associated prediction intervals using artificially degraded monitoring data set. We conclude with an illustration on a monitoring waterbirds data set.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6570\u6b63\u6001\u6cca\u677e\u6a21\u578b\u7684\u751f\u6001\u6570\u636e\u7f3a\u5931\u503c\u586b\u8865\u65b9\u6cd5\uff0c\u7279\u522b\u9488\u5bf9\u5177\u6709\u9ad8\u96f6\u503c\u6bd4\u4f8b\u3001\u9ad8\u65b9\u5dee\u548c\u5927\u91cf\u7f3a\u5931\u6570\u636e\u7684\u7269\u79cd\u76d1\u6d4b\u6570\u636e\u3002", "motivation": "\u751f\u6001\u5b66\u4e2d\u7684\u7269\u79cd\u4e30\u5ea6\u6570\u636e\u901a\u5e38\u5177\u6709\u5927\u91cf\u7f3a\u5931\u503c\u3001\u9ad8\u65b9\u5dee\u548c\u9ad8\u96f6\u503c\u6bd4\u4f8b\u7684\u7279\u70b9\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7a00\u6709\u7269\u79cd\u76d1\u6d4b\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7279\u5f81\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u7f3a\u5931\u503c\u586b\u8865\u548c\u534f\u53d8\u91cf\u6548\u5e94\u4f30\u8ba1\u7684\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u5bf9\u6570\u6b63\u6001\u6cca\u677e\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u5904\u7406\u96f6\u503c\u8fc7\u8868\u793a\u7684\u6f5c\u53d8\u91cf\u3002\u901a\u8fc7\u5047\u8bbe\u6f5c\u53d8\u91cf\u65b9\u5dee\u77e9\u9635\u5177\u6709\u4f4e\u79e9\u7ed3\u6784\u5e76\u7eb3\u5165\u534f\u53d8\u91cf\u6765\u5b9e\u73b0\u7f3a\u5931\u503c\u586b\u8865\u3002\u4f7f\u7528\u53d8\u5206\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u8fdb\u884c\u53c2\u6570\u63a8\u65ad\uff0c\u56e0\u4e3a\u6700\u5927\u4f3c\u7136\u63a8\u65ad\u4e0d\u53ef\u884c\u3002", "result": "\u8bc1\u660e\u4e86\u6a21\u578b\u5728\u7f3a\u5931\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u63d0\u4f9b\u4e86\u4f30\u8ba1\u91cf\u7684\u6e10\u8fd1\u65b9\u5dee\u4f30\u8ba1\uff0c\u5e76\u63a8\u5bfc\u4e86\u586b\u8865\u503c\u7684\u9884\u6d4b\u533a\u95f4\u3001\u65f6\u95f4\u8d8b\u52bf\u4f30\u8ba1\u4ee5\u53ca\u8d8b\u52bf\u53d8\u5316\u68c0\u6d4b\u7a0b\u5e8f\u3002\u901a\u8fc7\u4eba\u5de5\u964d\u8d28\u76d1\u6d4b\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u586b\u8865\u6548\u679c\u548c\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u751f\u6001\u76d1\u6d4b\u6570\u636e\u4e2d\u7684\u590d\u6742\u7279\u5f81\uff0c\u4e3a\u7269\u79cd\u76d1\u6d4b\u548c\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7edf\u8ba1\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u6c34\u9e1f\u76d1\u6d4b\u6570\u636e\u96c6\u7684\u5b9e\u9645\u5e94\u7528\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2602.10730", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.10730", "abs": "https://arxiv.org/abs/2602.10730", "authors": ["Hilde Vinje", "Lars Erik Gangsei"], "title": "A closed form solution for Bayesian analysis of a simple linear mixed model", "comment": null, "summary": "Linear mixed-effects models are a central analytical tool for modeling hierarchical and longitudinal data, as they allow simultaneous representation of fixed and random sources of variation. In practice, inference for such models is most often based on likelihood-based approximations, which are computationally efficient, but rely on numerical integration and may be unreliable example wise in small-sample settings. In this study, the somewhat obscure four-parameter generalized beta density is shown to be usable as a conjugate prior distribution for a simple linear mixed model. This leads to a closed-form Bayesian solution for a balanced mixed-model design, representing a methodological development beyond standard approximate or simulation-based Bayesian approaches. Although the derivation is restricted to a balanced setting, the proposed framework suggests a pathway toward analytically tractable Bayesian inference for more complex mixed-model structures. The method is evaluated through comparison with a standard frequentist solution based on likelihood estimation for linear mixed-effects models. Results indicate that the Bayesian approach performs just as well as the frequentist alternative, while yielding slightly reduced mean squared error. The study further discusses the use of empirical Bayes strategies for hyperparameter specification and outlines potential directions for extending the approach beyond the balanced case.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u56db\u53c2\u6570\u5e7f\u4e49Beta\u5206\u5e03\u4f5c\u4e3a\u7ebf\u6027\u6df7\u5408\u6a21\u578b\u7684\u5171\u8f6d\u5148\u9a8c\uff0c\u4e3a\u5e73\u8861\u8bbe\u8ba1\u63d0\u4f9b\u95ed\u5f0f\u8d1d\u53f6\u65af\u89e3\uff0c\u6027\u80fd\u4e0e\u9891\u7387\u6d3e\u65b9\u6cd5\u76f8\u5f53\u4e14\u5747\u65b9\u8bef\u5dee\u7565\u4f4e\u3002", "motivation": "\u7ebf\u6027\u6df7\u5408\u6a21\u578b\u662f\u5206\u6790\u5c42\u6b21\u548c\u7eb5\u5411\u6570\u636e\u7684\u6838\u5fc3\u5de5\u5177\uff0c\u4f46\u4f20\u7edf\u57fa\u4e8e\u4f3c\u7136\u7684\u8fd1\u4f3c\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u4e0b\u53ef\u80fd\u4e0d\u53ef\u9760\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u95ed\u5f0f\u8d1d\u53f6\u65af\u89e3\uff0c\u907f\u514d\u6570\u503c\u79ef\u5206\u548c\u6a21\u62df\u7684\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u56db\u53c2\u6570\u5e7f\u4e49Beta\u5bc6\u5ea6\u4f5c\u4e3a\u7b80\u5355\u7ebf\u6027\u6df7\u5408\u6a21\u578b\u7684\u5171\u8f6d\u5148\u9a8c\u5206\u5e03\uff0c\u4e3a\u5e73\u8861\u6df7\u5408\u6a21\u578b\u8bbe\u8ba1\u63a8\u5bfc\u51fa\u95ed\u5f0f\u8d1d\u53f6\u65af\u89e3\u3002\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u6807\u51c6\u7684\u8fd1\u4f3c\u6216\u57fa\u4e8e\u6a21\u62df\u7684\u8d1d\u53f6\u65af\u65b9\u6cd5\u3002", "result": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u4f3c\u7136\u4f30\u8ba1\u7684\u6807\u51c6\u9891\u7387\u6d3e\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u540c\u65f6\u4ea7\u751f\u7565\u4f4e\u7684\u5747\u65b9\u8bef\u5dee\u3002\u7814\u7a76\u8fd8\u8ba8\u8bba\u4e86\u4f7f\u7528\u7ecf\u9a8c\u8d1d\u53f6\u65af\u7b56\u7565\u8fdb\u884c\u8d85\u53c2\u6570\u8bbe\u5b9a\u3002", "conclusion": "\u867d\u7136\u63a8\u5bfc\u9650\u4e8e\u5e73\u8861\u8bbe\u8ba1\uff0c\u4f46\u8be5\u6846\u67b6\u4e3a\u66f4\u590d\u6742\u6df7\u5408\u6a21\u578b\u7ed3\u6784\u7684\u89e3\u6790\u53ef\u5904\u7406\u8d1d\u53f6\u65af\u63a8\u65ad\u63d0\u4f9b\u4e86\u9014\u5f84\uff0c\u5e76\u6982\u8ff0\u4e86\u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u5e73\u8861\u60c5\u51b5\u7684\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2602.10587", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10587", "abs": "https://arxiv.org/abs/2602.10587", "authors": ["Jinyuan Chang", "Yuling Jiao", "Lican Kang", "Junjie Shi"], "title": "Deep Bootstrap", "comment": null, "summary": "In this work, we propose a novel deep bootstrap framework for nonparametric regression based on conditional diffusion models. Specifically, we construct a conditional diffusion model to learn the distribution of the response variable given the covariates. This model is then used to generate bootstrap samples by pairing the original covariates with newly synthesized responses. We reformulate nonparametric regression as conditional sample mean estimation, which is implemented directly via the learned conditional diffusion model. Unlike traditional bootstrap methods that decouple the estimation of the conditional distribution, sampling, and nonparametric regression, our approach integrates these components into a unified generative framework. With the expressive capacity of diffusion models, our method facilitates both efficient sampling from high-dimensional or multimodal distributions and accurate nonparametric estimation. We establish rigorous theoretical guarantees for the proposed method. In particular, we derive optimal end-to-end convergence rates in the Wasserstein distance between the learned and target conditional distributions. Building on this foundation, we further establish the convergence guarantees of the resulting bootstrap procedure. Numerical studies demonstrate the effectiveness and scalability of our approach for complex regression tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u6df1\u5ea6\u81ea\u52a9\u6cd5\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u53c2\u6570\u56de\u5f52\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u7edf\u4e00\u5b66\u4e60\u6761\u4ef6\u5206\u5e03\u3001\u91c7\u6837\u548c\u56de\u5f52\u4f30\u8ba1", "motivation": "\u4f20\u7edf\u81ea\u52a9\u6cd5\u5c06\u6761\u4ef6\u5206\u5e03\u4f30\u8ba1\u3001\u91c7\u6837\u548c\u975e\u53c2\u6570\u56de\u5f52\u89e3\u8026\u5904\u7406\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u9ad8\u7ef4\u6216\u591a\u6a21\u6001\u5206\u5e03\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u8bc1\u7406\u8bba\u53ef\u9760\u6027", "method": "\u6784\u5efa\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b66\u4e60\u54cd\u5e94\u53d8\u91cf\u7ed9\u5b9a\u534f\u53d8\u91cf\u7684\u5206\u5e03\uff0c\u751f\u6210\u81ea\u52a9\u6837\u672c\uff08\u539f\u59cb\u534f\u53d8\u91cf+\u5408\u6210\u54cd\u5e94\uff09\uff0c\u5c06\u975e\u53c2\u6570\u56de\u5f52\u91cd\u6784\u4e3a\u6761\u4ef6\u6837\u672c\u5747\u503c\u4f30\u8ba1\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u6269\u6563\u6a21\u578b\u76f4\u63a5\u5b9e\u73b0", "result": "\u5efa\u7acb\u4e86\u5b66\u4e60\u6761\u4ef6\u5206\u5e03\u4e0e\u76ee\u6807\u6761\u4ef6\u5206\u5e03\u4e4b\u95f4Wasserstein\u8ddd\u79bb\u7684\u6700\u4f18\u7aef\u5230\u7aef\u6536\u655b\u7387\uff0c\u8bc1\u660e\u4e86\u81ea\u52a9\u8fc7\u7a0b\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u6570\u503c\u7814\u7a76\u5c55\u793a\u4e86\u65b9\u6cd5\u5728\u590d\u6742\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u81ea\u52a9\u6846\u67b6\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u7edf\u4e00\u4e86\u6761\u4ef6\u5206\u5e03\u5b66\u4e60\u3001\u91c7\u6837\u548c\u56de\u5f52\u4f30\u8ba1\uff0c\u4e3a\u590d\u6742\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u53ef\u9760\u7684\u65b0\u65b9\u6cd5"}}
{"id": "2602.11080", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.11080", "abs": "https://arxiv.org/abs/2602.11080", "authors": ["Hank Flury", "Jan Hannig", "Richard Smith"], "title": "Constrained Fiducial Inference for Gaussian Models", "comment": null, "summary": "We propose a new fiducial Markov Chain Monte Carlo (MCMC) method for fitting parametric Gaussian models. We utilize the Cayley transform to decompose the parametric covariance matrix, which in turn allows us to formulate a general data generating algorithm for Gaussian data. Leveraging constrained generalized fiducial inference, we are able to create the basis of an MCMC algorithm, which can be specified to parametric models with minimal effort. The appeal of this novel approach is the wide class of models which it permits, ease of implementation and the posterior-like fiducial distribution without the need for a prior. We provide background information for the derivation of the relevant fiducial quantities, and a proof that the proposed MCMC algorithm targets the correct fiducial distribution. We need not assume independence nor identical distribution of the data, which makes the method attractive for application to time series and spatial data. Well-performing simulation results of the MA(1) and Mat\u00e9rn models are presented.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u51c6MCMC\u65b9\u6cd5\u7528\u4e8e\u62df\u5408\u53c2\u6570\u5316\u9ad8\u65af\u6a21\u578b\uff0c\u901a\u8fc7Cayley\u53d8\u6362\u5206\u89e3\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u65e0\u9700\u5148\u9a8c\u5373\u53ef\u83b7\u5f97\u540e\u9a8c\u7c7b\u57fa\u51c6\u5206\u5e03\uff0c\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u548c\u7a7a\u95f4\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u5206\u5e03\uff0c\u800c\u57fa\u51c6\u63a8\u65ad\u65b9\u6cd5\u53ef\u4ee5\u5728\u6ca1\u6709\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u7c7b\u4f3c\u540e\u9a8c\u7684\u5206\u5e03\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u53c2\u6570\u5316\u9ad8\u65af\u6a21\u578b\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u65f6\u95f4\u5e8f\u5217\u548c\u7a7a\u95f4\u6570\u636e\u7b49\u975e\u72ec\u7acb\u540c\u5206\u5e03\u60c5\u51b5\u3002", "method": "\u4f7f\u7528Cayley\u53d8\u6362\u5206\u89e3\u53c2\u6570\u5316\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u6784\u5efa\u9ad8\u65af\u6570\u636e\u751f\u6210\u7b97\u6cd5\u3002\u57fa\u4e8e\u7ea6\u675f\u5e7f\u4e49\u57fa\u51c6\u63a8\u65ad\u5efa\u7acbMCMC\u7b97\u6cd5\u6846\u67b6\uff0c\u8be5\u7b97\u6cd5\u53ef\u8f7b\u677e\u9002\u914d\u5404\u79cd\u53c2\u6570\u5316\u6a21\u578b\uff0c\u65e0\u9700\u5047\u8bbe\u6570\u636e\u72ec\u7acb\u540c\u5206\u5e03\u3002", "result": "\u63d0\u51fa\u7684MCMC\u7b97\u6cd5\u80fd\u591f\u6b63\u786e\u6536\u655b\u5230\u76ee\u6807\u57fa\u51c6\u5206\u5e03\uff0c\u5728MA(1)\u548cMat\u00e9rn\u6a21\u578b\u4e0a\u7684\u6a21\u62df\u5b9e\u9a8c\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53c2\u6570\u5316\u9ad8\u65af\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u7684\u57fa\u51c6\u63a8\u65ad\u6846\u67b6\uff0c\u9002\u7528\u8303\u56f4\u5e7f\uff0c\u6613\u4e8e\u5b9e\u73b0\uff0c\u7279\u522b\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u548c\u7a7a\u95f4\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2602.10608", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10608", "abs": "https://arxiv.org/abs/2602.10608", "authors": ["Jiangrong Ouyang", "Mingming Gong", "Howard Bondell"], "title": "Bayesian Inference of Contextual Bandit Policies via Empirical Likelihood", "comment": "Accepted for publication in JMLR", "summary": "Policy inference plays an essential role in the contextual bandit problem. In this paper, we use empirical likelihood to develop a Bayesian inference method for the joint analysis of multiple contextual bandit policies in finite sample regimes. The proposed inference method is robust to small sample sizes and is able to provide accurate uncertainty measurements for policy value evaluation. In addition, it allows for flexible inferences on policy comparison with full uncertainty quantification. We demonstrate the effectiveness of the proposed inference method using Monte Carlo simulations and its application to an adolescent body mass index data set.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ecf\u9a8c\u4f3c\u7136\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u6709\u9650\u6837\u672c\u4e0b\u591a\u4e2a\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u7b56\u7565\u7684\u8054\u5408\u5206\u6790\uff0c\u63d0\u4f9b\u51c6\u786e\u7684\u7b56\u7565\u4ef7\u503c\u8bc4\u4f30\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316", "motivation": "\u5728\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u95ee\u9898\u4e2d\uff0c\u7b56\u7565\u63a8\u65ad\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u60c5\u51b5\u4e0b\u53ef\u80fd\u4e0d\u591f\u7a33\u5065\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u5c0f\u6837\u672c\u4e0b\u63d0\u4f9b\u51c6\u786e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u63a8\u65ad\u65b9\u6cd5", "method": "\u4f7f\u7528\u7ecf\u9a8c\u4f3c\u7136\u5f00\u53d1\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\uff0c\u5bf9\u591a\u4e2a\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u7b56\u7565\u8fdb\u884c\u8054\u5408\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u7075\u6d3b\u8fdb\u884c\u7b56\u7565\u6bd4\u8f83\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316", "result": "\u63d0\u51fa\u7684\u63a8\u65ad\u65b9\u6cd5\u5bf9\u5c0f\u6837\u672c\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u4e3a\u7b56\u7565\u4ef7\u503c\u8bc4\u4f30\u63d0\u4f9b\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u6d4b\u91cf\u3002\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u9752\u5c11\u5e74BMI\u6570\u636e\u96c6\u5e94\u7528\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u57fa\u4e8e\u7ecf\u9a8c\u4f3c\u7136\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\u4e3a\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u7b56\u7565\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6709\u9650\u6837\u672c\u60c5\u51b5\u4e0b\u7684\u7b56\u7565\u8bc4\u4f30\u548c\u6bd4\u8f83"}}
{"id": "2602.10924", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.10924", "abs": "https://arxiv.org/abs/2602.10924", "authors": ["James Neill", "Lloyd A. C. Chapman", "Chris Jewell"], "title": "Non-centred Bayesian inference for discrete-valued state-transition models: the Rippler algorithm", "comment": "18 pages, 7 figures (plus supplementary material with an additional 9 pages, 8 figures)", "summary": "Stochastic state-transition models of infectious disease transmission can be used to deduce relevant drivers of transmission when fitted to data using statistically principled methods. Fitting this individual-level data requires inference on individuals' unobserved disease statuses over time, which form a high-dimensional and highly correlated state space. We introduce a novel Bayesian (data-augmentation Markov chain Monte Carlo) algorithm for jointly estimating the model parameters and unobserved disease statuses, which we call the Rippler algorithm. This is a non-centred method that can be applied to any individual-based state-transition model. We compare the Rippler algorithm to the state-of-the-art inference methods for individual-based stochastic epidemic models and find that it performs better than these methods as the number of disease states in the model increases.", "AI": {"tldr": "\u63d0\u51fa\u540d\u4e3aRippler\u7684\u65b0\u578b\u8d1d\u53f6\u65af\u7b97\u6cd5\uff0c\u7528\u4e8e\u62df\u5408\u4e2a\u4f53\u5c42\u9762\u7684\u4f20\u67d3\u75c5\u4f20\u64ad\u968f\u673a\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u75be\u75c5\u72b6\u6001\u589e\u591a\u65f6\u8868\u73b0\u66f4\u4f18", "motivation": "\u4f20\u67d3\u75c5\u4f20\u64ad\u7684\u968f\u673a\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u53ef\u7528\u4e8e\u63a8\u65ad\u4f20\u64ad\u9a71\u52a8\u56e0\u7d20\uff0c\u4f46\u62df\u5408\u4e2a\u4f53\u5c42\u9762\u6570\u636e\u9700\u8981\u63a8\u65ad\u4e2a\u4f53\u672a\u89c2\u5bdf\u5230\u7684\u75be\u75c5\u72b6\u6001\uff0c\u8fd9\u4e9b\u72b6\u6001\u5f62\u6210\u9ad8\u7ef4\u4e14\u9ad8\u5ea6\u76f8\u5173\u7684\u72b6\u6001\u7a7a\u95f4\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6311\u6218", "method": "\u63d0\u51faRippler\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u975e\u4e2d\u5fc3\u5316\u7684\u8d1d\u53f6\u65af\u6570\u636e\u589e\u5f3a\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u57fa\u4e8e\u4e2a\u4f53\u7684\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\uff0c\u8054\u5408\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\u548c\u672a\u89c2\u5bdf\u5230\u7684\u75be\u75c5\u72b6\u6001", "result": "\u4e0e\u4e2a\u4f53\u968f\u673a\u6d41\u884c\u75c5\u6a21\u578b\u7684\u6700\u5148\u8fdb\u63a8\u65ad\u65b9\u6cd5\u76f8\u6bd4\uff0cRippler\u7b97\u6cd5\u5728\u6a21\u578b\u75be\u75c5\u72b6\u6001\u6570\u91cf\u589e\u52a0\u65f6\u8868\u73b0\u66f4\u597d", "conclusion": "Rippler\u7b97\u6cd5\u4e3a\u4e2a\u4f53\u968f\u673a\u4f20\u67d3\u75c5\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u65ad\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u75be\u75c5\u72b6\u6001\u8f83\u591a\u7684\u590d\u6742\u6a21\u578b\u4e2d\u5177\u6709\u4f18\u52bf"}}
{"id": "2602.10613", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10613", "abs": "https://arxiv.org/abs/2602.10613", "authors": ["Mingxun Wang", "Alejandro Schuler", "Mark van der Laan", "Carlos Garc\u00eda Meixide"], "title": "Highly Adaptive Principal Component Regression", "comment": null, "summary": "The Highly Adaptive Lasso (HAL) is a nonparametric regression method that achieves almost dimension-free convergence rates under minimal smoothness assumptions, but its implementation can be computationally prohibitive in high dimensions due to the large basis matrix it requires. The Highly Adaptive Ridge (HAR) has been proposed as a scalable alternative. Building on both procedures, we introduce the Principal Component based Highly Adaptive Lasso (PCHAL) and Principal Component based Highly Adaptive Ridge (PCHAR). These estimators constitute an outcome-blind dimension reduction which offer substantial gains in computational efficiency and match the empirical performances of HAL and HAR. We also uncover a striking spectral link between the leading principal components of the HAL/HAR Gram operator and a discrete sinusoidal basis, revealing an explicit Fourier-type structure underlying the PC truncation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e3b\u6210\u5206\u7684HAL\u548cHAR\u65b9\u6cd5\uff08PCHAL/PCHAR\uff09\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u964d\u7ef4\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u65b9\u6cd5\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u5e76\u63ed\u793a\u4e86HAL/HAR Gram\u7b97\u5b50\u7684\u8c31\u7ed3\u6784\u4e0e\u79bb\u6563\u6b63\u5f26\u57fa\u7684\u5085\u91cc\u53f6\u8054\u7cfb\u3002", "motivation": "HAL\u65b9\u6cd5\u867d\u7136\u5177\u6709\u4f18\u8d8a\u7684\u7406\u8bba\u6027\u8d28\uff0c\u4f46\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u8ba1\u7b97\u4ee3\u4ef7\u8fc7\u9ad8\uff1bHAR\u4f5c\u4e3a\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\u4ecd\u9762\u4e34\u8ba1\u7b97\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u7406\u8bba\u4f18\u52bf\u53c8\u80fd\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4e3b\u6210\u5206\u5206\u6790\u5bf9HAL\u548cHAR\u8fdb\u884c\u6539\u8fdb\uff0c\u63d0\u51faPCHAL\u548cPCHAR\u65b9\u6cd5\u3002\u901a\u8fc7Gram\u7b97\u5b50\u7684\u4e3b\u6210\u5206\u8fdb\u884c\u7ed3\u679c\u76f2\u964d\u7ef4\uff0c\u5229\u7528\u8c31\u5206\u6790\u63ed\u793a\u5176\u4e0e\u79bb\u6563\u6b63\u5f26\u57fa\u7684\u5085\u91cc\u53f6\u7ed3\u6784\u8054\u7cfb\u3002", "result": "PCHAL\u548cPCHAR\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u539f\u59cbHAL\u548cHAR\u76f8\u5f53\u7684\u5b9e\u8bc1\u6027\u80fd\u3002\u53d1\u73b0\u4e86HAL/HAR Gram\u7b97\u5b50\u4e3b\u6210\u5206\u4e0e\u79bb\u6563\u6b63\u5f26\u57fa\u7684\u8c31\u8054\u7cfb\uff0c\u63ed\u793a\u4e86\u5085\u91cc\u53f6\u578b\u7ed3\u6784\u3002", "conclusion": "\u57fa\u4e8e\u4e3b\u6210\u5206\u7684\u6539\u8fdb\u65b9\u6cd5\u5728\u4fdd\u6301\u975e\u53c2\u6570\u56de\u5f52\u7406\u8bba\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u5e95\u5c42\u6570\u5b66\u7ed3\u6784\u7684\u7406\u8bba\u6d1e\u5bdf\u3002"}}
{"id": "2602.10640", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10640", "abs": "https://arxiv.org/abs/2602.10640", "authors": ["Stephan Cl\u00e9men\u00e7on", "Ekhine Irurozki"], "title": "Beyond Kemeny Medians: Consensus Ranking Distributions Definition, Properties and Statistical Learning", "comment": null, "summary": "In this article we develop a new method for summarizing a ranking distribution, \\textit{i.e.} a probability distribution on the symmetric group $\\mathfrak{S}_n$, beyond the classical theory of consensus and Kemeny medians. Based on the notion of \\textit{local ranking median}, we introduce the concept of \\textit{consensus ranking distribution} ($\\crd$), a sparse mixture model of Dirac masses on $\\mathfrak{S}_n$, in order to approximate a ranking distribution with small distortion from a mass transportation perspective. We prove that by choosing the popular Kendall $\u03c4$ distance as the cost function, the optimal distortion can be expressed as a function of pairwise probabilities, paving the way for the development of efficient learning methods that do not suffer from the lack of vector space structure on $\\mathfrak{S}_n$. In particular, we propose a top-down tree-structured statistical algorithm that allows for the progressive refinement of a CRD based on ranking data, from the Dirac mass at a Kemeny median at the root of the tree to the empirical ranking data distribution itself at the end of the tree's exhaustive growth. In addition to the theoretical arguments developed, the relevance of the algorithm is empirically supported by various numerical experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6392\u5e8f\u5206\u5e03\u6458\u8981\u65b9\u6cd5\u2014\u2014\u5171\u8bc6\u6392\u5e8f\u5206\u5e03(CRD)\uff0c\u4f5c\u4e3a\u7a00\u758f\u6df7\u5408\u6a21\u578b\u6765\u8fd1\u4f3c\u6392\u5e8f\u5206\u5e03\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u6811\u7ed3\u6784\u7684\u7edf\u8ba1\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5171\u8bc6\u548cKemeny\u4e2d\u4f4d\u6570\u65b9\u6cd5\u5728\u603b\u7ed3\u6392\u5e8f\u5206\u5e03\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u8d85\u8d8a\u7ecf\u5178\u7406\u8bba\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u8fd1\u4f3c\u6392\u5e8f\u5206\u5e03\u5e76\u51cf\u5c11\u8d28\u91cf\u8fd0\u8f93\u89c6\u89d2\u4e0b\u7684\u5931\u771f\u3002", "method": "\u57fa\u4e8e\u5c40\u90e8\u6392\u5e8f\u4e2d\u4f4d\u6570\u6982\u5ff5\uff0c\u63d0\u51fa\u5171\u8bc6\u6392\u5e8f\u5206\u5e03(CRD)\u4f5c\u4e3aDirac\u8d28\u91cf\u7684\u7a00\u758f\u6df7\u5408\u6a21\u578b\uff1b\u4f7f\u7528Kendall \u03c4\u8ddd\u79bb\u4f5c\u4e3a\u6210\u672c\u51fd\u6570\uff0c\u5c06\u6700\u4f18\u5931\u771f\u8868\u8fbe\u4e3a\u6210\u5bf9\u6982\u7387\u51fd\u6570\uff1b\u5f00\u53d1\u4e86\u81ea\u4e0a\u800c\u4e0b\u7684\u6811\u7ed3\u6784\u7edf\u8ba1\u7b97\u6cd5\uff0c\u4eceKemeny\u4e2d\u4f4d\u6570\u7684Dirac\u8d28\u91cf\u9010\u6b65\u7ec6\u5316\u5230\u7ecf\u9a8c\u6392\u5e8f\u6570\u636e\u5206\u5e03\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4f7f\u7528Kendall \u03c4\u8ddd\u79bb\u65f6\uff0c\u6700\u4f18\u5931\u771f\u53ef\u4ee5\u8868\u793a\u4e3a\u6210\u5bf9\u6982\u7387\u7684\u51fd\u6570\uff1b\u63d0\u51fa\u7684\u6811\u7ed3\u6784\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60CRD\uff1b\u6570\u503c\u5b9e\u9a8c\u652f\u6301\u4e86\u7b97\u6cd5\u7684\u76f8\u5173\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "CRD\u65b9\u6cd5\u4e3a\u6392\u5e8f\u5206\u5e03\u6458\u8981\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u514b\u670d\u4e86\u5bf9\u79f0\u7fa4\u4e0a\u7f3a\u4e4f\u5411\u91cf\u7a7a\u95f4\u7ed3\u6784\u7684\u56f0\u96be\uff0c\u63d0\u51fa\u7684\u5b66\u4e60\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u6392\u5e8f\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.10969", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10969", "abs": "https://arxiv.org/abs/2602.10969", "authors": ["Anna Guo", "Razieh Nabi"], "title": "Weighting-Based Identification and Estimation in Graphical Models of Missing Data", "comment": null, "summary": "We propose a constructive algorithm for identifying complete data distributions in graphical models of missing data. The complete data distribution is unrestricted, while the missingness mechanism is assumed to factorize according to a conditional directed acyclic graph. Our approach follows an interventionist perspective in which missingness indicators are treated as variables that can be intervened on. A central challenge in this setting is that sequences of interventions on missingness indicators may induce and propagate selection bias, so that identification can fail even when a propensity score is invariant to available interventions. To address this challenge, we introduce a tree-based identification algorithm that explicitly tracks the creation and propagation of selection bias and determines whether it can be avoided through admissible intervention strategies. The resulting tree provides both a diagnostic and a constructive characterization of identifiability under a given missingness mechanism. Building on these results, we develop recursive inverse probability weighting procedures that mirror the intervention logic of the identification algorithm, yielding valid estimating equations for both the missingness mechanism and functionals of the complete data distribution. Simulation studies and a real-data application illustrate the practical performance of the proposed methods. An accompanying R package, flexMissing, implements all proposed procedures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc6\u522b\u7f3a\u5931\u6570\u636e\u56fe\u5f62\u6a21\u578b\u4e2d\u5b8c\u6574\u6570\u636e\u5206\u5e03\u7684\u6784\u9020\u6027\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e72\u9884\u7f3a\u5931\u6307\u6807\u6765\u8bc6\u522b\u5206\u5e03\uff0c\u5e76\u5904\u7406\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u5728\u7f3a\u5931\u6570\u636e\u56fe\u5f62\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u503e\u5411\u5f97\u5206\u5bf9\u53ef\u7528\u5e72\u9884\u4fdd\u6301\u4e0d\u53d8\uff0c\u5bf9\u7f3a\u5931\u6307\u6807\u7684\u5e72\u9884\u5e8f\u5217\u4e5f\u53ef\u80fd\u5f15\u53d1\u548c\u4f20\u64ad\u9009\u62e9\u504f\u5dee\uff0c\u5bfc\u81f4\u8bc6\u522b\u5931\u8d25\u3002\u9700\u8981\u4e00\u79cd\u80fd\u660e\u786e\u8ddf\u8e2a\u548c\u5904\u7406\u9009\u62e9\u504f\u5dee\u7684\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5e72\u9884\u4e3b\u4e49\u89c6\u89d2\uff0c\u5c06\u7f3a\u5931\u6307\u6807\u89c6\u4e3a\u53ef\u5e72\u9884\u53d8\u91cf\u3002\u63d0\u51fa\u57fa\u4e8e\u6811\u7684\u8bc6\u522b\u7b97\u6cd5\uff0c\u663e\u5f0f\u8ddf\u8e2a\u9009\u62e9\u504f\u5dee\u7684\u521b\u5efa\u548c\u4f20\u64ad\uff0c\u5224\u65ad\u662f\u5426\u53ef\u901a\u8fc7\u53ef\u63a5\u53d7\u7684\u5e72\u9884\u7b56\u7565\u907f\u514d\u504f\u5dee\u3002\u5f00\u53d1\u9012\u5f52\u9006\u6982\u7387\u52a0\u6743\u7a0b\u5e8f\uff0c\u5339\u914d\u8bc6\u522b\u7b97\u6cd5\u7684\u5e72\u9884\u903b\u8f91\u3002", "result": "\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7ed9\u5b9a\u7f3a\u5931\u673a\u5236\u4e0b\u53ef\u8bc6\u522b\u6027\u7684\u8bca\u65ad\u548c\u6784\u9020\u6027\u7279\u5f81\u3002\u6a21\u62df\u7814\u7a76\u548c\u5b9e\u9645\u6570\u636e\u5e94\u7528\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u6027\u80fd\u3002\u5f00\u53d1\u4e86R\u5305flexMissing\u5b9e\u73b0\u6240\u6709\u7a0b\u5e8f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7f3a\u5931\u6570\u636e\u56fe\u5f62\u6a21\u578b\u4e2d\u7684\u5206\u5e03\u8bc6\u522b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u6784\u9020\u6027\u6846\u67b6\uff0c\u80fd\u6709\u6548\u5904\u7406\u5e72\u9884\u5f15\u53d1\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.10680", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10680", "abs": "https://arxiv.org/abs/2602.10680", "authors": ["Vicente Conde Mendes", "Lorenzo Bardone", "C\u00e9dric Koller", "Jorge Medina Moreira", "Vittorio Erba", "Emanuele Troiani", "Lenka Zdeborov\u00e1"], "title": "A solvable high-dimensional model where nonlinear autoencoders learn structure invisible to PCA while test loss misaligns with generalization", "comment": null, "summary": "Many real-world datasets contain hidden structure that cannot be detected by simple linear correlations between input features. For example, latent factors may influence the data in a coordinated way, even though their effect is invisible to covariance-based methods such as PCA. In practice, nonlinear neural networks often succeed in extracting such hidden structure in unsupervised and self-supervised learning. However, constructing a minimal high-dimensional model where this advantage can be rigorously analyzed has remained an open theoretical challenge. We introduce a tractable high-dimensional spiked model with two latent factors: one visible to covariance, and one statistically dependent yet uncorrelated, appearing only in higher-order moments. PCA and linear autoencoders fail to recover the latter, while a minimal nonlinear autoencoder provably extracts both. We analyze both the population risk, and empirical risk minimization. Our model also provides a tractable example where self-supervised test loss is poorly aligned with representation quality: nonlinear autoencoders recover latent structure that linear methods miss, even though their reconstruction loss is higher.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u9ad8\u7ef4\u5c16\u5cf0\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u6f5c\u5728\u56e0\u5b50\uff1a\u4e00\u4e2a\u53ef\u901a\u8fc7\u534f\u65b9\u5dee\u68c0\u6d4b\uff0c\u53e6\u4e00\u4e2a\u4ec5\u51fa\u73b0\u5728\u9ad8\u9636\u77e9\u4e2d\u3002\u975e\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u80fd\u63d0\u53d6\u4e24\u8005\uff0c\u800c\u7ebf\u6027\u65b9\u6cd5\u53ea\u80fd\u63d0\u53d6\u524d\u8005\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u5e38\u5305\u542b\u7ebf\u6027\u76f8\u5173\u65e0\u6cd5\u68c0\u6d4b\u7684\u9690\u85cf\u7ed3\u6784\uff08\u5982\u6f5c\u5728\u56e0\u5b50\uff09\u3002\u867d\u7136\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u5728\u5b9e\u8df5\u4e2d\u80fd\u63d0\u53d6\u8fd9\u4e9b\u7ed3\u6784\uff0c\u4f46\u7f3a\u4e4f\u53ef\u4e25\u683c\u5206\u6790\u7684\u6700\u5c0f\u9ad8\u7ef4\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u7406\u8bba\u53ef\u5904\u7406\u7684\u6a21\u578b\u6765\u7814\u7a76\u975e\u7ebf\u6027\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u9ad8\u7ef4\u5c16\u5cf0\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u6f5c\u5728\u56e0\u5b50\uff1a\u4e00\u4e2a\u53ef\u901a\u8fc7\u534f\u65b9\u5dee\u68c0\u6d4b\uff08\u53ef\u89c1\uff09\uff0c\u53e6\u4e00\u4e2a\u7edf\u8ba1\u76f8\u5173\u4f46\u4e0d\u76f8\u5173\uff0c\u4ec5\u51fa\u73b0\u5728\u9ad8\u9636\u77e9\u4e2d\u3002\u5206\u6790\u975e\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u4e0e\u7ebf\u6027\u65b9\u6cd5\uff08PCA\u3001\u7ebf\u6027\u81ea\u7f16\u7801\u5668\uff09\u5728\u8be5\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u3002", "result": "PCA\u548c\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u53ea\u80fd\u6062\u590d\u534f\u65b9\u5dee\u53ef\u89c1\u7684\u6f5c\u5728\u56e0\u5b50\uff0c\u800c\u6700\u5c0f\u975e\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u53ef\u8bc1\u660e\u5730\u63d0\u53d6\u4e24\u4e2a\u56e0\u5b50\u3002\u540c\u65f6\u53d1\u73b0\u81ea\u76d1\u7763\u6d4b\u8bd5\u635f\u5931\u4e0e\u8868\u793a\u8d28\u91cf\u4e0d\u4e00\u81f4\uff1a\u975e\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u6062\u590d\u7ebf\u6027\u65b9\u6cd5\u9057\u6f0f\u7684\u7ed3\u6784\uff0c\u5c3d\u7ba1\u5176\u91cd\u6784\u635f\u5931\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5206\u6790\u975e\u7ebf\u6027\u65b9\u6cd5\u63d0\u53d6\u9690\u85cf\u7ed3\u6784\u7684\u4f18\u52bf\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u635f\u5931\u51fd\u6570\u4e0e\u8868\u793a\u8d28\u91cf\u53ef\u80fd\u4e0d\u4e00\u81f4\u7684\u73b0\u8c61\uff0c\u5f3a\u8c03\u4e86\u9ad8\u9636\u7edf\u8ba1\u91cf\u5728\u6570\u636e\u8868\u793a\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.10691", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10691", "abs": "https://arxiv.org/abs/2602.10691", "authors": ["Gauthier Thurin", "Claire Boyer", "Kimia Nadjahi"], "title": "Convergence Rates for Distribution Matching with Sliced Optimal Transport", "comment": null, "summary": "We study the slice-matching scheme, an efficient iterative method for distribution matching based on sliced optimal transport. We investigate convergence to the target distribution and derive quantitative non-asymptotic rates. To this end, we establish __ojasiewicz-type inequalities for the Sliced-Wasserstein objective. A key challenge is to control along the trajectory the constants in these inequalities. We show that this becomes tractable for Gaussian distributions. Specifically, eigenvalues are controlled when matching along random orthonormal bases at each iteration. We complement our theory with numerical experiments and illustrate the predicted dependence on dimension and step-size, as well as the stabilizing effect of orthonormal-basis sampling.", "AI": {"tldr": "\u7814\u7a76\u57fa\u4e8e\u5207\u7247\u6700\u4f18\u4f20\u8f93\u7684\u5207\u7247\u5339\u914d\u65b9\u6848\uff0c\u5206\u6790\u5176\u6536\u655b\u6027\u548c\u975e\u6e10\u8fd1\u6536\u655b\u901f\u7387\uff0c\u7279\u522b\u9488\u5bf9\u9ad8\u65af\u5206\u5e03\u5efa\u7acb\u4e86\u7406\u8bba\u4fdd\u8bc1", "motivation": "\u5207\u7247\u5339\u914d\u65b9\u6848\u662f\u4e00\u79cd\u57fa\u4e8e\u5207\u7247\u6700\u4f18\u4f20\u8f93\u7684\u9ad8\u6548\u8fed\u4ee3\u5206\u5e03\u5339\u914d\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u6536\u655b\u6027\u548c\u6536\u655b\u901f\u7387\u7684\u7406\u8bba\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u4fdd\u8bc1", "method": "\u5efa\u7acb\u5207\u7247Wasserstein\u76ee\u6807\u7684\u0141ojasiewicz\u578b\u4e0d\u7b49\u5f0f\uff0c\u63a7\u5236\u8f68\u8ff9\u4e0a\u7684\u5e38\u6570\uff1b\u7279\u522b\u9488\u5bf9\u9ad8\u65af\u5206\u5e03\uff0c\u901a\u8fc7\u6bcf\u8f6e\u8fed\u4ee3\u4e2d\u968f\u673a\u6b63\u4ea4\u57fa\u7684\u91c7\u6837\u6765\u63a7\u5236\u7279\u5f81\u503c", "result": "\u63a8\u5bfc\u4e86\u5207\u7247\u5339\u914d\u65b9\u6848\u7684\u5b9a\u91cf\u975e\u6e10\u8fd1\u6536\u655b\u901f\u7387\uff0c\u8bc1\u660e\u4e86\u5bf9\u4e8e\u9ad8\u65af\u5206\u5e03\uff0c\u901a\u8fc7\u968f\u673a\u6b63\u4ea4\u57fa\u91c7\u6837\u53ef\u4ee5\u63a7\u5236\u7279\u5f81\u503c\uff0c\u4ece\u800c\u4fdd\u8bc1\u6536\u655b\u6027", "conclusion": "\u5207\u7247\u5339\u914d\u65b9\u6848\u5728\u7406\u8bba\u4e0a\u6709\u6536\u655b\u4fdd\u8bc1\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9ad8\u65af\u5206\u5e03\uff0c\u968f\u673a\u6b63\u4ea4\u57fa\u91c7\u6837\u5177\u6709\u7a33\u5b9a\u6548\u679c\uff1b\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ef4\u5ea6\u3001\u6b65\u957f\u4f9d\u8d56\u5173\u7cfb\u4ee5\u53ca\u6b63\u4ea4\u57fa\u91c7\u6837\u7684\u7a33\u5b9a\u4f5c\u7528"}}
{"id": "2602.11107", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.11107", "abs": "https://arxiv.org/abs/2602.11107", "authors": ["Albert Dorador"], "title": "Renet: Principled and Efficient Relaxation for the Elastic Net via Dynamic Objective Selection", "comment": null, "summary": "We introduce Renet, a principled generalization of the Relaxed Lasso to the Elastic Net family of estimators. While, on the one hand, $\\ell_1$-regularization is a standard tool for variable selection in high-dimensional regimes and, on the other hand, the $\\ell_2$ penalty provides stability and solution uniqueness through strict convexity, the standard Elastic Net nevertheless suffers from shrinkage bias that frequently yields suboptimal prediction accuracy. We propose to address this limitation through a framework called \\textit{relaxation}. Existing relaxation implementations rely on naive linear interpolations of penalized and unpenalized solutions, which ignore the non-linear geometry that characterizes the entire regularization path and risk violating the Karush-Kuhn-Tucker conditions. Renet addresses these limitations by enforcing sign consistency through an adaptive relaxation procedure that dynamically dispatches between convex blending and efficient sub-path refitting. Furthermore, we identify and formalize a unique synergy between relaxation and the ``One-Standard-Error'' rule: relaxation serves as a robust debiasing mechanism, allowing practitioners to leverage the parsimony of the 1-SE rule without the traditional loss in predictive fidelity. Our theoretical framework incorporates automated stability safeguards for ultra-high dimensional regimes and is supported by a comprehensive benchmarking suite across 20 synthetic and real-world datasets, demonstrating that Renet consistently outperforms the standard Elastic Net and provides a more robust alternative to the Adaptive Elastic Net in high-dimensional, low signal-to-noise ratio and high-multicollinearity regimes. By leveraging an adaptive solver backend, Renet delivers these statistical gains while offering a computational profile that remains competitive with state-of-the-art coordinate descent implementations.", "AI": {"tldr": "Renet\uff1a\u5c06\u677e\u5f1bLasso\u63a8\u5e7f\u5230Elastic Net\u5bb6\u65cf\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u677e\u5f1b\u673a\u5236\u89e3\u51b3\u6807\u51c6Elastic Net\u7684\u6536\u7f29\u504f\u5dee\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u6807\u51c6Elastic Net\u867d\u7136\u7ed3\u5408\u4e86\u2113\u2081\u6b63\u5219\u5316\u7684\u53d8\u91cf\u9009\u62e9\u80fd\u529b\u548c\u2113\u2082\u6b63\u5219\u5316\u7684\u7a33\u5b9a\u6027\uff0c\u4f46\u5b58\u5728\u6536\u7f29\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u6d4b\u7cbe\u5ea6\u4e0b\u964d\u3002\u73b0\u6709\u677e\u5f1b\u65b9\u6cd5\u4f7f\u7528\u7b80\u5355\u7684\u7ebf\u6027\u63d2\u503c\uff0c\u5ffd\u7565\u4e86\u6b63\u5219\u5316\u8def\u5f84\u7684\u975e\u7ebf\u6027\u51e0\u4f55\u7279\u6027\uff0c\u53ef\u80fd\u8fdd\u53cdKKT\u6761\u4ef6\u3002", "method": "\u63d0\u51faRenet\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u677e\u5f1b\u7a0b\u5e8f\u5f3a\u5236\u7b26\u53f7\u4e00\u81f4\u6027\uff0c\u52a8\u6001\u5728\u51f8\u6df7\u5408\u548c\u9ad8\u6548\u5b50\u8def\u5f84\u91cd\u65b0\u62df\u5408\u4e4b\u95f4\u5207\u6362\u3002\u7ed3\u5408\"One-Standard-Error\"\u89c4\u5219\u4f5c\u4e3a\u7a33\u5065\u7684\u53bb\u504f\u673a\u5236\uff0c\u5e76\u5305\u542b\u8d85\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u7684\u81ea\u52a8\u7a33\u5b9a\u6027\u4fdd\u62a4\u3002", "result": "\u572820\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cRenet\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6Elastic Net\uff0c\u5e76\u5728\u9ad8\u7ef4\u3001\u4f4e\u4fe1\u566a\u6bd4\u548c\u9ad8\u591a\u91cd\u5171\u7ebf\u6027\u60c5\u51b5\u4e0b\u6bd4Adaptive Elastic Net\u66f4\u7a33\u5065\u3002\u8ba1\u7b97\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u5750\u6807\u4e0b\u964d\u5b9e\u73b0\u76f8\u5f53\u3002", "conclusion": "Renet\u4e3aElastic Net\u5bb6\u65cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u677e\u5f1b\u63a8\u5e7f\uff0c\u89e3\u51b3\u4e86\u6536\u7f29\u504f\u5dee\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u9ad8\u7ef4\u7edf\u8ba1\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2602.11118", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.11118", "abs": "https://arxiv.org/abs/2602.11118", "authors": ["Filippo Salmaso", "Lorenzo Testa", "Francesca Chiaromonte"], "title": "A Doubly Robust Machine Learning Approach for Disentangling Treatment Effect Heterogeneity with Functional Outcomes", "comment": "20 pages, 4 figures", "summary": "Causal inference is paramount for understanding the effects of interventions, yet extracting personalized insights from increasingly complex data remains a significant challenge for modern machine learning. This is the case, in particular, when considering functional outcomes observed over a continuous domain (e.g., time, or space). Estimation of heterogeneous treatment effects, known as CATE, has emerged as a crucial tool for personalized decision-making, but existing meta-learning frameworks are largely limited to scalar outcomes, failing to provide satisfying results in scientific applications that leverage the rich, continuous information encoded in functional data. Here, we introduce FOCaL (Functional Outcome Causal Learning), a novel, doubly robust meta-learner specifically engineered to estimate a functional heterogeneous treatment effect (F-CATE). FOCaL integrates advanced functional regression techniques for both outcome modeling and functional pseudo-outcome reconstruction, thereby enabling the direct and robust estimation of F-CATE. We provide a rigorous theoretical derivation of FOCaL, demonstrate its performance and robustness compared to existing non-robust functional methods through comprehensive simulation studies, and illustrate its practical utility on diverse real-world functional datasets. FOCaL advances the capabilities of machine intelligence to infer nuanced, individualized causal effects from complex data, paving the way for more precise and trustworthy AI systems in personalized medicine, adaptive policy design, and fundamental scientific discovery.", "AI": {"tldr": "\u63d0\u51faFOCaL\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u529f\u80fd\u6027\u5f02\u8d28\u5904\u7406\u6548\u5e94\uff08F-CATE\uff09\uff0c\u89e3\u51b3\u4f20\u7edf\u5143\u5b66\u4e60\u6846\u67b6\u4ec5\u9002\u7528\u4e8e\u6807\u91cf\u7ed3\u679c\u3001\u65e0\u6cd5\u5904\u7406\u8fde\u7eed\u57df\u529f\u80fd\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u56e0\u679c\u63a8\u65ad\u5bf9\u7406\u89e3\u5e72\u9884\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709CATE\u4f30\u8ba1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6807\u91cf\u7ed3\u679c\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fde\u7eed\u57df\uff08\u5982\u65f6\u95f4\u3001\u7a7a\u95f4\uff09\u7684\u529f\u80fd\u6027\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "FOCaL\u662f\u4e00\u79cd\u53cc\u91cd\u7a33\u5065\u7684\u5143\u5b66\u4e60\u5668\uff0c\u6574\u5408\u4e86\u5148\u8fdb\u7684\u529f\u80fd\u56de\u5f52\u6280\u672f\u8fdb\u884c\u7ed3\u679c\u5efa\u6a21\u548c\u529f\u80fd\u6027\u4f2a\u7ed3\u679c\u91cd\u6784\uff0c\u80fd\u591f\u76f4\u63a5\u4e14\u7a33\u5065\u5730\u4f30\u8ba1F-CATE\u3002", "result": "\u901a\u8fc7\u7efc\u5408\u6a21\u62df\u7814\u7a76\u8bc1\u660eFOCaL\u76f8\u6bd4\u73b0\u6709\u975e\u7a33\u5065\u529f\u80fd\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u529f\u80fd\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "FOCaL\u63d0\u5347\u4e86\u4ece\u590d\u6742\u6570\u636e\u4e2d\u63a8\u65ad\u7ec6\u5fae\u4e2a\u4f53\u5316\u56e0\u679c\u6548\u5e94\u7684\u80fd\u529b\uff0c\u4e3a\u4e2a\u6027\u5316\u533b\u7597\u3001\u9002\u5e94\u6027\u653f\u7b56\u8bbe\u8ba1\u548c\u57fa\u7840\u79d1\u5b66\u53d1\u73b0\u7b49\u9886\u57df\u66f4\u7cbe\u786e\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2602.10867", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10867", "abs": "https://arxiv.org/abs/2602.10867", "authors": ["Hugo Tabanelli", "Yatin Dandi", "Luca Pesce", "Florent Krzakala"], "title": "Deep Learning of Compositional Targets with Hierarchical Spectral Methods", "comment": null, "summary": "Why depth yields a genuine computational advantage over shallow methods remains a central open question in learning theory. We study this question in a controlled high-dimensional Gaussian setting, focusing on compositional target functions. We analyze their learnability using an explicit three-layer fitting model trained via layer-wise spectral estimators. Although the target is globally a high-degree polynomial, its compositional structure allows learning to proceed in stages: an intermediate representation reveals structure that is inaccessible at the input level. This reduces learning to simpler spectral estimation problems, well studied in the context of multi-index models, whereas any shallow estimator must resolve all components simultaneously. Our analysis relies on Gaussian universality, leading to sharp separations in sample complexity between two and three-layer learning strategies.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\u6d45\u5c42\u65b9\u6cd5\u7684\u8ba1\u7b97\u4f18\u52bf\uff0c\u901a\u8fc7\u9ad8\u65af\u8bbe\u7f6e\u4e0b\u7684\u7ec4\u5408\u76ee\u6807\u51fd\u6570\u5206\u6790\uff0c\u53d1\u73b0\u4e09\u5c42\u6a21\u578b\u901a\u8fc7\u5206\u5c42\u5b66\u4e60\u80fd\u83b7\u5f97\u6bd4\u4e24\u5c42\u6a21\u578b\u66f4\u4f4e\u7684\u6837\u672c\u590d\u6742\u5ea6", "motivation": "\u7406\u89e3\u4e3a\u4ec0\u4e48\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6bd4\u6d45\u5c42\u65b9\u6cd5\u5177\u6709\u771f\u6b63\u7684\u8ba1\u7b97\u4f18\u52bf\u662f\u5b66\u4e60\u7406\u8bba\u4e2d\u7684\u6838\u5fc3\u5f00\u653e\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u53d7\u63a7\u7684\u9ad8\u7ef4\u9ad8\u65af\u8bbe\u7f6e\uff0c\u7814\u7a76\u7ec4\u5408\u76ee\u6807\u51fd\u6570\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u63ed\u793a\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\u6765\u6e90\u3002", "method": "\u4f7f\u7528\u4e09\u5c42\u62df\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u8c31\u4f30\u8ba1\u5668\u8fdb\u884c\u8bad\u7ec3\u3002\u6a21\u578b\u5728\u7ec4\u5408\u76ee\u6807\u51fd\u6570\u4e0a\u8fdb\u884c\u5b66\u4e60\uff0c\u5229\u7528\u9ad8\u65af\u666e\u9002\u6027\u8fdb\u884c\u5206\u6790\uff0c\u6bd4\u8f83\u4e24\u5c42\u548c\u4e09\u5c42\u5b66\u4e60\u7b56\u7565\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u4e09\u5c42\u6a21\u578b\u901a\u8fc7\u5206\u5c42\u5b66\u4e60\u80fd\u591f\u9010\u6b65\u63ed\u793a\u4e2d\u95f4\u8868\u793a\u7684\u7ed3\u6784\uff0c\u5c06\u5b66\u4e60\u95ee\u9898\u7b80\u5316\u4e3a\u66f4\u7b80\u5355\u7684\u8c31\u4f30\u8ba1\u95ee\u9898\u3002\u800c\u6d45\u5c42\u4f30\u8ba1\u5668\u5fc5\u987b\u540c\u65f6\u89e3\u51b3\u6240\u6709\u7ec4\u4ef6\u3002\u5206\u6790\u663e\u793a\u4e24\u5c42\u548c\u4e09\u5c42\u5b66\u4e60\u7b56\u7565\u5728\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u5206\u5c42\u5b66\u4e60\u7ec4\u5408\u7ed3\u6784\uff0c\u80fd\u591f\u83b7\u5f97\u6bd4\u6d45\u5c42\u65b9\u6cd5\u66f4\u4f4e\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u8fd9\u89e3\u91ca\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u8ba1\u7b97\u4f18\u52bf\u3002\u4e2d\u95f4\u8868\u793a\u63ed\u793a\u4e86\u8f93\u5165\u5c42\u9762\u65e0\u6cd5\u8bbf\u95ee\u7684\u7ed3\u6784\uff0c\u4f7f\u5f97\u5b66\u4e60\u8fc7\u7a0b\u53ef\u4ee5\u5206\u9636\u6bb5\u8fdb\u884c\u3002"}}
{"id": "2602.10949", "categories": ["stat.ML", "cs.LG", "math.DS", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.10949", "abs": "https://arxiv.org/abs/2602.10949", "authors": ["Constantin Kogler", "Tassilo Schwarz", "Samuel Kittle"], "title": "Optimal Initialization in Depth: Lyapunov Initialization and Limit Theorems for Deep Leaky ReLU Networks", "comment": "45 pages", "summary": "The development of effective initialization methods requires an understanding of random neural networks. In this work, a rigorous probabilistic analysis of deep unbiased Leaky ReLU networks is provided. We prove a Law of Large Numbers and a Central Limit Theorem for the logarithm of the norm of network activations, establishing that, as the number of layers increases, their growth is governed by a parameter called the Lyapunov exponent. This parameter characterizes a sharp phase transition between vanishing and exploding activations, and we calculate the Lyapunov exponent explicitly for Gaussian or orthogonal weight matrices. Our results reveal that standard methods, such as He initialization or orthogonal initialization, do not guarantee activation stabilty for deep networks of low width. Based on these theoretical insights, we propose a novel initialization method, referred to as Lyapunov initialization, which sets the Lyapunov exponent to zero and thereby ensures that the neural network is as stable as possible, leading empirically to improved learning.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6df1\u5ea6\u65e0\u504fLeaky ReLU\u7f51\u7edc\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u6982\u7387\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u7f51\u7edc\u6fc0\u6d3b\u8303\u6570\u5bf9\u6570\u7684\u6781\u9650\u5b9a\u7406\uff0c\u63ed\u793a\u4e86\u6fc0\u6d3b\u6d88\u5931/\u7206\u70b8\u7684\u76f8\u53d8\u7531Lyapunov\u6307\u6570\u63a7\u5236\uff0c\u5e76\u63d0\u51fa\u4e86Lyapunov\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u6709\u6548\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u9700\u8981\u7406\u89e3\u968f\u673a\u795e\u7ecf\u7f51\u7edc\u7684\u884c\u4e3a\u3002\u73b0\u6709\u6807\u51c6\u521d\u59cb\u5316\u65b9\u6cd5\uff08\u5982He\u521d\u59cb\u5316\u6216\u6b63\u4ea4\u521d\u59cb\u5316\uff09\u4e0d\u80fd\u4fdd\u8bc1\u6df1\u5ea6\u7a84\u7f51\u7edc\u7684\u6fc0\u6d3b\u7a33\u5b9a\u6027\u3002", "method": "\u5bf9\u6df1\u5ea6\u65e0\u504fLeaky ReLU\u7f51\u7edc\u8fdb\u884c\u4e25\u683c\u7684\u6982\u7387\u5206\u6790\uff0c\u8bc1\u660e\u6fc0\u6d3b\u8303\u6570\u5bf9\u6570\u7684\u5f3a\u5927\u6570\u5b9a\u5f8b\u548c\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\uff0c\u8ba1\u7b97\u9ad8\u65af\u6216\u6b63\u4ea4\u6743\u91cd\u77e9\u9635\u4e0b\u7684Lyapunov\u6307\u6570\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eLyapunov\u6307\u6570\u5f52\u96f6\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u968f\u7740\u7f51\u7edc\u5c42\u6570\u589e\u52a0\uff0c\u6fc0\u6d3b\u589e\u957f\u7531Lyapunov\u6307\u6570\u63a7\u5236\uff0c\u8be5\u6307\u6570\u8868\u5f81\u4e86\u6fc0\u6d3b\u6d88\u5931\u548c\u7206\u70b8\u4e4b\u95f4\u7684\u5c16\u9510\u76f8\u53d8\u3002\u53d1\u73b0\u6807\u51c6\u521d\u59cb\u5316\u65b9\u6cd5\u4e0d\u80fd\u4fdd\u8bc1\u6df1\u5ea6\u7a84\u7f51\u7edc\u7684\u7a33\u5b9a\u6027\uff0c\u800c\u63d0\u51fa\u7684Lyapunov\u521d\u59cb\u5316\u65b9\u6cd5\u80fd\u786e\u4fdd\u7f51\u7edc\u5c3d\u53ef\u80fd\u7a33\u5b9a\u3002", "conclusion": "Lyapunov\u6307\u6570\u662f\u7406\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6fc0\u6d3b\u7a33\u5b9a\u6027\u7684\u5173\u952e\u53c2\u6570\uff0c\u57fa\u4e8e\u8be5\u7406\u8bba\u63d0\u51fa\u7684Lyapunov\u521d\u59cb\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u7f51\u7edc\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u6539\u5584\u5b66\u4e60\u6548\u679c\u3002"}}
