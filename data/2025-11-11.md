<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 4]
- [stat.CO](#stat.CO) [Total: 3]
- [stat.ME](#stat.ME) [Total: 7]
- [stat.AP](#stat.AP) [Total: 3]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Prototype Selection Using Topological Data Analysis](https://arxiv.org/abs/2511.04873)
*Jordan Eckert,Elvan Ceyhan,Henry Schenck*

Main category: stat.ML

TL;DR: 提出了基于拓扑数据分析的Topological Prototype Selector框架，用于从大数据集中选择代表性子集，在保持或改进分类性能的同时显著减少数据规模。


<details>
  <summary>Details</summary>
Motivation: 统计学习文献中涌现了大量使用拓扑原理表示数据的方法，但需要开发能够有效选择代表性原型的框架。

Method: 基于拓扑数据分析的Topological Prototype Selector框架，通过拓扑原理选择数据子集作为原型。

Result: 在模拟数据和真实数据实验中，TPS显著保持或改进分类性能，同时大幅减少数据规模。

Conclusion: 该研究推进了原型学习的算法和几何方面，为并行化、可解释和高效的分类提供了实用工具。

Abstract: Recently, there has been an explosion in statistical learning literature to
represent data using topological principles to capture structure and
relationships. We propose a topological data analysis (TDA)-based framework,
named Topological Prototype Selector (TPS), for selecting representative
subsets (prototypes) from large datasets. We demonstrate the effectiveness of
TPS on simulated data under different data intrinsic characteristics, and
compare TPS against other currently used prototype selection methods in real
data settings. In all simulated and real data settings, TPS significantly
preserves or improves classification performance while substantially reducing
data size. These contributions advance both algorithmic and geometric aspects
of prototype learning and offer practical tools for parallelized,
interpretable, and efficient classification.

</details>


### [2] [Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning](https://arxiv.org/abs/2511.05050)
*Masahiro Tanaka*

Main category: stat.ML

TL;DR: 提出了一种可扩展的在线核学习框架，用于估计具有相互依赖性和异方差性系统的双向因果效应。该方法结合了准最大似然估计和在线核学习，通过随机傅里叶特征近似灵活建模非线性条件均值和方差。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断通常关注单向效应，忽略了现实世界中常见的双向关系。需要一种能够处理相互依赖性和异方差性的方法，特别是在流式和高维数据场景下。

Method: 基于异方差性识别，将准最大似然估计器与大规模在线核学习相结合。使用随机傅里叶特征近似建模非线性条件均值和方差，采用自适应在线梯度下降算法确保计算效率。

Result: 广泛的模拟实验表明，该方法在准确性和稳定性方面优于单方程和多项式近似基线，在各种数据生成过程中表现出更低的偏差和均方根误差。

Conclusion: 该框架有效捕捉复杂的双向因果效应，具有接近线性的计算扩展性。通过将计量经济学识别与现代机器学习技术相结合，为大规模因果推断提供了实用、可扩展且理论基础的解决方案。

Abstract: In this study, a scalable online kernel learning framework is proposed for
estimating bidirectional causal effects in systems characterized by mutual
dependence and heteroskedasticity. Traditional causal inference often focuses
on unidirectional effects, overlooking the common bidirectional relationships
in real-world phenomena. Building on heteroskedasticity-based identification,
the proposed method integrates a quasi-maximum likelihood estimator for
simultaneous equation models with large scale online kernel learning. It
employs random Fourier feature approximations to flexibly model nonlinear
conditional means and variances, while an adaptive online gradient descent
algorithm ensures computational efficiency for streaming and high-dimensional
data. Results from extensive simulations demonstrate that the proposed method
achieves superior accuracy and stability than single equation and polynomial
approximation baselines, exhibiting lower bias and root mean squared error
across various data-generating processes. These results confirm that the
proposed approach effectively captures complex bidirectional causal effects
with near-linear computational scaling. By combining econometric identification
with modern machine learning techniques, the proposed framework offers a
practical, scalable, and theoretically grounded solution for large scale causal
inference in natural/social science, policy making, business, and industrial
applications.

</details>


### [3] [A New Framework for Convex Clustering in Kernel Spaces: Finite Sample Bounds, Consistency and Performance Insights](https://arxiv.org/abs/2511.05159)
*Shubhayan Pan,Saptarshi Chakraborty,Debolina Paul,Kushal Bose,Swagatam Das*

Main category: stat.ML

TL;DR: 提出了一种凸聚类方法的核化扩展，通过将数据映射到再生核希尔伯特空间来解决线性不可分和非凸结构数据的聚类问题。


<details>
  <summary>Details</summary>
Motivation: 传统凸聚类方法在处理线性不可分或非凸结构数据时会失效，需要一种能够处理复杂数据分布的改进方法。

Method: 使用特征映射将数据点投影到再生核希尔伯特空间，在该变换空间中进行凸聚类，产生有限维向量空间的嵌入。

Result: 通过理论和实验验证，该方法在合成和真实数据集上表现出色，优于现有最先进的聚类技术。

Conclusion: 这项工作在非线性非凸数据聚类领域取得了重要进展，提供了有效的解决方案。

Abstract: Convex clustering is a well-regarded clustering method, resembling the
similar centroid-based approach of Lloyd's $k$-means, without requiring a
predefined cluster count. It starts with each data point as its centroid and
iteratively merges them. Despite its advantages, this method can fail when
dealing with data exhibiting linearly non-separable or non-convex structures.
To mitigate the limitations, we propose a kernelized extension of the convex
clustering method. This approach projects the data points into a Reproducing
Kernel Hilbert Space (RKHS) using a feature map, enabling convex clustering in
this transformed space. This kernelization not only allows for better handling
of complex data distributions but also produces an embedding in a
finite-dimensional vector space. We provide a comprehensive theoretical
underpinnings for our kernelized approach, proving algorithmic convergence and
establishing finite sample bounds for our estimates. The effectiveness of our
method is demonstrated through extensive experiments on both synthetic and
real-world datasets, showing superior performance compared to state-of-the-art
clustering techniques. This work marks a significant advancement in the field,
offering an effective solution for clustering in non-linear and non-convex data
scenarios.

</details>


### [4] [Self-adaptive weighting and sampling for physics-informed neural networks](https://arxiv.org/abs/2511.05452)
*Wenqian Chen,Amanda Howard,Panos Stinis*

Main category: stat.ML

TL;DR: 提出了一种混合自适应采样和加权方法来提升物理信息神经网络（PINNs）的性能，结合自适应采样和自适应加权策略，显著提高了PDE求解的准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 物理信息深度学习在求解偏微分方程方面显示出潜力，但在复杂问题上的训练仍然具有挑战性，通常导致精度和效率受限。

Method: 引入混合自适应采样和加权方法：自适应采样识别解变化剧烈区域的训练点，自适应加权平衡训练点间的收敛速度。

Result: 数值实验表明，单独使用自适应采样或自适应加权不足以获得准确预测，特别是在训练点稀缺时。结合两种策略能一致提高预测精度和训练效率。

Conclusion: 提出的混合框架为使用PINNs求解PDE提供了更稳健的方法，通过结合两种互补策略克服了单一方法的局限性。

Abstract: Physics-informed deep learning has emerged as a promising framework for
solving partial differential equations (PDEs). Nevertheless, training these
models on complex problems remains challenging, often leading to limited
accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling
and weighting method to enhance the performance of physics-informed neural
networks (PINNs). The adaptive sampling component identifies training points in
regions where the solution exhibits rapid variation, while the adaptive
weighting component balances the convergence rate across training points.
Numerical experiments show that applying only adaptive sampling or only
adaptive weighting is insufficient to consistently achieve accurate
predictions, particularly when training points are scarce. Since each method
emphasizes different aspects of the solution, their effectiveness is problem
dependent. By combining both strategies, the proposed framework consistently
improves prediction accuracy and training efficiency, offering a more robust
approach for solving PDEs with PINNs.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [5] [BayesChange: an R package for Bayesian Change Point Analysis](https://arxiv.org/abs/2511.04785)
*Luca Danese,Riccardo Corradin,Andrea Ongaro*

Main category: stat.CO

TL;DR: BayesChange是一个基于C++的高效R包，用于贝叶斯变点检测和共享共同变点的观测聚类，提供其他R包不具备的方法。


<details>
  <summary>Details</summary>
Motivation: 现有R包在变点分析方面存在功能限制，BayesChange旨在提供其他包不具备的贝叶斯变点检测和聚类方法。

Method: 核心函数用C++实现以确保计算效率，通过R用户界面简化使用，包含两个集成C++后端函数的R包装器和用于结果汇总的S3方法。

Result: 开发了计算高效的BayesChange R包，包含完整的理论框架、后验模拟算法，并通过合成示例展示了包的使用方法。

Conclusion: BayesChange填补了R生态系统中贝叶斯变点检测工具的空白，提供了高效且用户友好的解决方案。

Abstract: We introduce BayesChange, a computationally efficient R package, built on
C++, for Bayesian change point detection and clustering of observations sharing
common change points. While many R packages exist for change point analysis,
BayesChange offers methods not currently available elsewhere. The core
functions are implemented in C++ to ensures computational efficiency, while an
R user interface simplifies the package usage. The BayesChange package includes
two R wrappers that integrate the C++ backend functions, along with S3 methods
for summarizing the results. We present the theory beyond each method, the
algorithms for posterior simulation and we illustrate the package's usage
through synthetic examples.

</details>


### [6] [Do we Need Dozens of Methods for Real World Missing Value Imputation?](https://arxiv.org/abs/2511.04833)
*Krystyna Grzesiak,Christophe Muller,Julie Josse,Jeffrey Näf*

Main category: stat.CO

TL;DR: 本文提出了一种基于分布预测的系统性插补方法基准测试框架，评估了多种算法在合成和真实缺失机制下的表现，发现迭代插补方法（特别是mice R包）表现最优。


<details>
  <summary>Details</summary>
Motivation: 当前插补方法研究存在局限：算法覆盖不全、主要使用点状指标（如RMSE）评估、缺乏对真实数据分布的保留能力评估、对混合数据类型关注不足。

Method: 采用基于分布预测的系统性基准测试方法，使用插补分数评估大量算法，同时在合成缺失机制和真实缺失场景下进行测试，并考虑混合数据集。

Result: 分析结果明确显示迭代插补算法的优越性，特别是mice R包中实现的方法表现最佳。

Conclusion: 迭代插补方法在保持真实数据分布方面具有显著优势，建议在缺失值处理中优先考虑此类方法。

Abstract: Missing values pose a persistent challenge in modern data science.
Consequently, there is an ever-growing number of publications introducing new
imputation methods in various fields. While many studies compare imputation
approaches, they often focus on a limited subset of algorithms and evaluate
performance primarily through pointwise metrics such as RMSE, which are not
suitable to measure the preservation of the true data distribution. In this
work, we provide a systematic benchmarking method based on the idea of treating
imputation as a distributional prediction task. We consider a large number of
algorithms and, for the first time, evaluate them not only on synthetic missing
mechanisms, but also on real-world missingness scenarios, using the concept of
Imputation Scores. Finally, while the focus of previous benchmark has often
been on numerical data, we also consider mixed data sets in our study. The
analysis overwhelmingly confirms the superiority of iterative imputation
algorithms, especially the methods implemented in the mice R package.

</details>


### [7] [Sequential Markov chain Monte Carlo for Filtering of State-Space Models with Low or Degenerate Observation Noise](https://arxiv.org/abs/2511.04975)
*Abylay Zhumekenov,Alexandros Beskos,Dan Crisan,Matthew Graham,Ajay Jasra,Nikolas Kantas*

Main category: stat.CO

TL;DR: 本文研究了观测噪声退化或低噪声情况下的离散时间滤波问题，提出了在特定流形序列上的滤波密度推导和递归方法，并设计了序列马尔可夫链蒙特卡洛方法来近似滤波器。


<details>
  <summary>Details</summary>
Motivation: 在观测噪声退化或低噪声的实际场景中，传统滤波方法可能失效，需要专门处理这类特殊噪声结构的滤波问题。

Method: 在观测函数定义的特定流形序列上推导滤波密度及其递归，设计序列马尔可夫链蒙特卡洛方法进行时间序列近似。

Result: 对于特定线性观测模型，证明了低噪声情况下序列MCMC方法在噪声消失时会收敛到退化噪声情况下的方法。

Conclusion: 所提出的方法在统计学和应用数学中的多个挑战性随机模型上表现良好，能够有效处理退化或低噪声观测的滤波问题。

Abstract: We consider the discrete-time filtering problem in scenarios where the
observation noise is degenerate or low. More precisely, one is given access to
a discrete time observation sequence which at any time $k$ depends only on the
state of an unobserved Markov chain. We specifically assume that the functional
relationship between observations and hidden Markov chain has either degenerate
or low noise. In this article, under suitable assumptions, we derive the
filtering density and its recursions for this class of problems on a specific
sequence of manifolds defined through the observation function. We then design
sequential Markov chain Monte Carlo methods to approximate the filter serially
in time. For a certain linear observation model, we show that using sequential
Markov chain Monte Carlo for low noise will converge as the noise disappears to
that of using sequential Markov chain Monte Carlo for degenerate noise. We
illustrate the performance of our methodology on several challenging stochastic
models deriving from Statistics and Applied Mathematics.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [8] [The Kaplan-Meier Estimator as a Sum over Units](https://arxiv.org/abs/2511.04721)
*Malte C. Tichy*

Main category: stat.ME

TL;DR: 提出了Kaplan-Meier乘积限估计器的和式表达，将总体估计器分解为个体半经验估计器的和，用于可视化失效和删失单元对总体估计的不同贡献。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解Kaplan-Meier估计器中失效和删失单元对总体生存函数估计的贡献，需要一个直观的分解方法。

Method: 提出了Kaplan-Meier乘积限估计器的和式表达，将总体估计器表示为个体半经验估计器的和。

Result: 得到了一个直观的分解，允许可视化失效单元和删失单元对总体Kaplan-Meier估计器的不同贡献。

Conclusion: 和式表达为理解Kaplan-Meier估计器提供了新的视角，有助于分析不同类型观测对生存函数估计的影响。

Abstract: A sum-wise formulation is proposed for the Kaplan-Meier product limit
estimator of partially right-censored survival data. The derived representation
permits to write the population's estimator as a sum over its individual units'
semi-empirical estimators. This intuitive decomposition is applied to visualize
the different contributions of failed and censored units to the overall
population estimator.

</details>


### [9] [An Integrative Approach for Subtyping Mental Disorders Using Multimodal Data](https://arxiv.org/abs/2511.04816)
*Yinjun Zhao,Yuanjia Wang,Ying LIu*

Main category: stat.ME

TL;DR: 提出了MINDS模型，一种贝叶斯分层模型，用于整合多模态数据（临床、认知、神经影像）来识别精神疾病的生物学亚型，应用于ADHD和OCD的亚型分析。


<details>
  <summary>Details</summary>
Motivation: 解决精神疾病生物和行为异质性问题，推进精准诊断、治疗和预防，探索如何整合多模态数据识别生物学有意义的精神疾病亚型。

Method: 开发MINDS（Mixed INtegrative Data Subtyping）贝叶斯分层模型，使用Polya-Gamma增强提高计算效率和推理稳健性，整合临床症状、认知表现和脑结构数据进行降维和聚类。

Result: 模拟显示相比现有聚类方法具有更好的稳定性和准确性；在ABCD研究中识别出具有不同认知和神经发育特征的ADHD和OCD临床可解释亚型。

Conclusion: 多模态整合建模可以提高精神疾病亚型分析的可重复性和临床相关性，支持数据驱动的早期识别和针对性干预策略。

Abstract: Understanding the biological and behavioral heterogeneity underlying
psychiatric disorders is critical for advancing precision diagnosis, treatment,
and prevention. This paper addresses the scientific question of how multimodal
data, spanning clinical, cognitive, and neuroimaging measures, can be
integrated to identify biologically meaningful subtypes of mental disorders. We
introduce Mixed INtegrative Data Subtyping (MINDS), a Bayesian hierarchical
model designed to jointly analyze mixed-type data for simultaneous dimension
reduction and clustering. Using data from the Adolescent Brain Cognitive
Development (ABCD) Study, MINDS integrates clinical symptoms, cognitive
performance, and brain structure measures to subtype
Attention-Deficit/Hyperactivity Disorder (ADHD) and Obsessive-Compulsive
Disorder (OCD). Our method leverages Polya-Gamma augmentation for computational
efficiency and robust inference. Simulations demonstrate improved stability and
accuracy compared to existing clustering approaches. Application to the ABCD
data reveals clinically interpretable subtypes of ADHD and OCD with distinct
cognitive and neurodevelopmental profiles. These findings show how integrative
multimodal modeling can enhance the reproducibility and clinical relevance of
psychiatric subtyping, supporting data-driven policies for early identification
and targeted interventions in mental health.

</details>


### [10] [Inference for the Extended Functional Cox Model: A UK Biobank Case Study](https://arxiv.org/abs/2511.04852)
*Erjia Cui,Angela Zhao,Ciprian M. Crainiceanu*

Main category: stat.ME

TL;DR: 该研究开发了扩展功能Cox模型，用于分析昼夜体力活动模式及其变异性与死亡率的关系，在英国生物银行大数据集上验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究表明体力活动的标量总结是死亡率的最强预测因子，但昼夜活动模式及其变异性可能提供额外信息，需要开发新方法来分析这种复杂的时间模式数据。

Method: 引入了扩展功能Cox模型和相应推断工具，用于量化多个功能和标量预测因子与时间到事件结果之间的关联，适用于大规模高维数据集。

Result: 在英国生物银行研究中成功应用该方法，分析了93,370名参与者的分钟级体力活动数据，模拟研究表明方法在现实场景中表现良好且可扩展到更大规模研究。

Conclusion: 为复杂大规模数据集建立这些方法的可行性和可扩展性是应用功能数据分析领域的一个重要里程碑。

Abstract: Multiple studies have shown that scalar summaries of objectively measured
physical activity (PA) using accelerometers are the strongest predictors of
mortality, outperforming all traditional risk factors, including age, sex, body
mass index (BMI), and smoking. Here we show that diurnal patterns of PA and
their day-to-day variability provide additional information about mortality. To
do that, we introduce a class of extended functional Cox models and
corresponding inferential tools designed to quantify the association between
multiple functional and scalar predictors with time-to-event outcomes in
large-scale (large $n$) high-dimensional (large $p$) datasets. Methods are
applied to the UK Biobank study, which collected PA at every minute of the day
for up to seven days, as well as time to mortality ($93{,}370$ participants
with good quality accelerometry data and $931$ events). Simulation studies show
that methods perform well in realistic scenarios and scale up to studies an
order of magnitude larger than the UK Biobank accelerometry study. Establishing
the feasibility and scalability of these methods for such complex and large
data sets is a major milestone in applied Functional Data Analysis (FDA).

</details>


### [11] [Clustering in Networks with Time-varying Nodal Attributes](https://arxiv.org/abs/2511.04859)
*Yik Lun Kei,Oscar Hernan Madrid Padilla,Rebecca Killick,James Wilson,Xi Chen,Robert Lund*

Main category: stat.ME

TL;DR: 提出了一种结合图结构和时间序列数据的节点聚类方法，通过潜在表示学习和图融合LASSO正则化来融合结构与时序模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽视节点属性的时序演化，需要同时考虑图结构和时间序列动态来进行更有效的节点聚类。

Method: 使用低维表示先验和解码器连接潜在表示与时间序列，通过最大近似似然学习参数，并施加图融合LASSO正则化，采用ADMM优化和Langevin动态进行后验推断。

Result: 在块图和网格图上的仿真研究，以及在加州县级温度数据和书籍词共现网络上的应用都证明了方法的有效性。

Conclusion: 该方法成功融合了图结构和时序动态，为节点聚类提供了有效的解决方案。

Abstract: This manuscript studies nodal clustering in graphs having a time series at
each node. The framework includes priors for low-dimensional representations
and a decoder that bridges the latent representations and time series. The
structural and temporal patterns are fused into representations that facilitate
clustering, addressing the limitation that the evolution of nodal attributes is
often overlooked. Parameters are learned via maximum approximate likelihood,
with a graph-fused LASSO regularization imposed on prior parameters. The
optimization problem is solved via alternating direction method of multipliers;
Langevin dynamics are employed for posterior inference. Simulation studies on
block and grid graphs with autoregressive dynamics, and applications to
California county temperatures and a book word co-occurrence network
demonstrate the effectiveness of the proposed method.

</details>


### [12] [Function on Scalar Regression with Complex Survey Designs](https://arxiv.org/abs/2511.05487)
*Lily Koffman,Sunan Gao,Xinkai Zhou,Andrew Leroux,Ciprian Crainiceanu,John Muschelli III*

Main category: stat.ME

TL;DR: 提出了针对复杂调查设计的功能标量回归(FoSR)推断方法，结合了功能数据的快速单变量推断和调查抽样方法，通过加权GLM、系数平滑和重复抽样技术来获得功能系数的置信区间。


<details>
  <summary>Details</summary>
Motivation: 现有FoSR方法未能考虑复杂调查设计，而大型健康调查越来越多地收集来自可穿戴设备的高维功能数据，需要能够处理调查设计的方法。

Method: 三步骤方法：(1)在功能域每个点拟合调查加权GLM；(2)沿功能域平滑系数；(3)使用平衡重复复制(BRR)或RWYB自助法获得点态和联合置信带。

Result: 通过NHANES数据的分析和模拟研究表明，该方法比不考虑调查结构的方法表现更好，并在实际应用中显示出考虑调查结构的实际意义。

Conclusion: 该方法成功解决了复杂调查设计下的FoSR推断问题，已实现在R包svyfosr中，为处理可穿戴设备数据提供了有效工具。

Abstract: Large health surveys increasingly collect high-dimensional functional data
from wearable devices, and function on scalar regression (FoSR) is often used
to quantify the relationship between these functional outcomes and scalar
covariates such as age and sex. However, existing methods for FoSR fail to
account for complex survey design. We introduce inferential methods for FoSR
for studies with complex survey designs. The method combines fast univariate
inference (FUI) developed for functional data outcomes and survey sampling
inferential methods developed for scalar outcomes. Our approach consists of
three steps: (1) fit survey weighted GLMs at each point along the functional
domain, (2) smooth coefficients along the functional domain, and (3) use
balanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap
to obtain pointwise and joint confidence bands for the functional coefficients.
The method is motivated by association studies between continuous physical
activity data and covariates collected in the National Health and Nutrition
Examination Survey (NHANES). A first-of-its-kind analytical simulation study
and empirical simulation using the NHANES data demonstrates that our method
performs better than existing methods that do not account for the survey
structure. Finally, application of the method in NHANES shows the practical
implications of accounting for survey structure. The method is implemented in
the R package svyfosr.

</details>


### [13] [On linkage bias-correction for estimators using iterated bootstraps](https://arxiv.org/abs/2511.05004)
*Siu-Ming Tam,Min Wang,Alicia Rambaldi,Dehua Tao*

Main category: stat.ME

TL;DR: 该论文提出了一种基于bootstrap技术的连接偏差校正估计方法，并开发了测试来评估增加bootstrap迭代次数是否能有效减少连接偏差。


<details>
  <summary>Details</summary>
Motivation: 在概率记录连接中，由于连接变量可能存在错误，导致连接数据集存在连接误差和连接偏差，影响统计分析的有效性。

Method: 使用bootstrap技术开发连接偏差校正估计器，并引入测试方法来评估bootstrap迭代次数对偏差减少的有效性。

Result: 通过模拟激素数据集和澳大利亚统计局劳动力流动调查数据的实际应用，验证了所提方法的有效性。

Conclusion: 提出的bootstrap方法能够有效校正连接偏差，且测试方法可以指导选择合适的bootstrap迭代次数，避免方差膨胀。

Abstract: By amalgamating data from disparate sources, the resulting integrated dataset
becomes a valuable resource for statistical analysis. In probabilistic record
linkage, the effectiveness of such integration relies on the availability of
linkage variables free from errors. Where this is lacking, the linked data set
would suffer from linkage errors and the resultant analyses, linkage bias. This
paper proposes a methodology leveraging the bootstrap technique to devise
linkage bias-corrected estimators. Additionally, it introduces a test to assess
whether increasing the number of bootstrap iterations meaningfully reduces
linkage bias or merely inflates variance without further improving accuracy. An
application of these methodologies is demonstrated through the analysis of a
simulated dataset featuring hormone information, along with a dataset obtained
from linking two data sets from the Australian Bureau of Statistics' labour
mobility surveys.

</details>


### [14] [Conditioning on posterior samples for flexible frequentist goodness-of-fit testing](https://arxiv.org/abs/2511.05281)
*Ritwik Bhaduri,Aabesh Bhattacharyya,Rina Foygel Barber,Lucas Janson*

Main category: stat.ME

TL;DR: 提出了一种基于贝叶斯后验采样的近似充分统计量方法（aCSS-B），用于扩展拟合优度检验的应用范围，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有拟合优度检验方法在条件化充分统计量时存在显著限制，要么无法使用，要么在重要模型中大大降低检验功效或计算可行性。

Method: 通过从贝叶斯后验分布中采样来条件化近似充分统计量，这是一种与先前工作不同的近似充分统计量类型。

Result: 证明了所得检验的近似有效性，并在三个常见零模型上展示了其效用，同时在现有方法适用的模型上也表现出更好的性能。

Conclusion: aCSS-B方法显著扩展了这种灵活拟合优度检验类型的适用范围，为统计应用提供了更强大的工具。

Abstract: Tests of goodness of fit are used in nearly every domain where statistics is
applied. One powerful and flexible approach is to sample artificial data sets
that are exchangeable with the real data under the null hypothesis (but not
under the alternative), as this allows the analyst to conduct a valid test
using any test statistic they desire. Such sampling is typically done by
conditioning on either an exact or approximate sufficient statistic, but
existing methods for doing so have significant limitations, which either
preclude their use or substantially reduce their power or computational
tractability for many important models. In this paper, we propose to condition
on samples from a Bayesian posterior distribution, which constitute a very
different type of approximate sufficient statistic than those considered in
prior work. Our approach, approximately co-sufficient sampling via Bayes
(aCSS-B), considerably expands the scope of this flexible type of
goodness-of-fit testing. We prove the approximate validity of the resulting
test, and demonstrate its utility on three common null models where no existing
methods apply, as well as its outperformance on models where existing methods
do apply.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [15] [Estimating Inhomogeneous Spatio-Temporal Background Intensity Functions using Graphical Dirichlet Processes](https://arxiv.org/abs/2511.04974)
*Isaías Bañales,Tomoaki Nishikawa,Yoshihiro Ito,Manuel J. Aguilar-Velázquez*

Main category: stat.AP

TL;DR: 提出了一种基于图形狄利克雷过程的新方法，用于在背景地震活动性建模中纳入时空不均匀性，并应用于墨西哥南部2000-2015年的地震数据。


<details>
  <summary>Details</summary>
Motivation: 传统的地震活动性建模通常假设背景地震活动为泊松过程，但实际中地震活动在空间和时间上存在不均匀性，需要开发能够捕捉这种时空变化的新方法。

Method: 使用图形狄利克雷过程的贝叶斯非参数技术，构建能够随时间演化空间强度的泊松过程模型，以纳入背景地震活动性的时空不均匀性。

Result: 该方法成功应用于墨西哥南部2000-2015年的地震数据，能够有效捕捉背景地震活动性的时空变化模式。

Conclusion: 提出的基于图形狄利克雷过程的方法为地震活动性建模提供了一种灵活的工具，能够更好地描述实际地震活动的时空不均匀特征。

Abstract: An enhancement in seismic measuring instrumentation has been proven to have
implications in the quantity of observed earthquakes, since denser networks
usually allow recording more events. However, phenomena such as strong
earthquakes or even aseismic transients, as slow slip earthquakes, may alter
the occurrence of earthquakes. In the field of seismology, it is a standard
practice to model background seismicity as a Poisson process. Based on this
idea, this work proposes a model that can incorporate the evolving spatial
intensity of Poisson processes over time (i.e., we include temporal changes in
the background seismicity when modeling). In recent years, novel methodologies
have been developed for quantifying the uncertainty in the estimation of the
background seismicity in homogeneous cases using Bayesian non-parametric
techniques. This work proposes a novel methodology based on graphical Dirichlet
processes for incorporating spatial and temporal inhomogeneities in background
seismicity. The proposed model in this work is applied to study the seismicity
in the southern Mexico, using recorded data from 2000 to 2015.

</details>


### [16] [On the Estimation of Climate Normals and Anomalies](https://arxiv.org/abs/2511.05071)
*Tommaso Proietti,Alessandro Giovannelli*

Main category: stat.AP

TL;DR: 提出了一种基于局部三角回归的正则化实时滤波器，用于在气候变化背景下优化气候常态估计的偏差-方差权衡，并引入季节性核函数增强估计的局部化。


<details>
  <summary>Details</summary>
Motivation: 当前基于30年简单算术平均的气候常态估计方法在非平稳的气候变化背景下存在适用性问题，需要改进以更好地处理气候变化趋势。

Method: 使用局部三角回归的正则化实时滤波器，通过优化偏差-方差权衡来估计气候常态，并引入季节性核函数提高估计的局部化程度。

Result: 应用于厄尔尼诺3.4区海表温度序列和赤道太平洋区域纬向风及信风强度，验证了该方法的有效性。

Conclusion: 提出的方法在气候变化背景下能够更准确地估计气候常态，为厄尔尼诺-南方涛动现象的评估和预测提供了改进工具。

Abstract: The quantification of the interannual component of variability in
climatological time series is essential for the assessment and prediction of
the El Ni\~{n}o - Southern Oscillation phenomenon. This is achieved by
estimating the deviation of a climate variable (e.g., temperature, pressure,
precipitation, or wind strength) from its normal conditions, defined by its
baseline level and seasonal patterns. Climate normals are currently estimated
by simple arithmetic averages calculated over the most recent 30-year period
ending in a year divisible by 10. The suitability of the standard methodology
has been questioned in the context of a changing climate, characterized by
nonstationary conditions. The literature has focused on the choice of the
bandwidth and the ability to account for trends induced by climate change. The
paper contributes to the literature by proposing a regularized real time filter
based on local trigonometric regression, optimizing the estimation
bias-variance trade-off in the presence of climate change, and by introducing a
class of seasonal kernels enhancing the localization of the estimates of
climate normals. Application to sea surface temperature series in the \nino 3.4
region and zonal and trade winds strength in the equatorial and tropical
Pacific region, illustrates the relevance of our proposal.

</details>


### [17] [Exponential Spatiotemporal GARCH Model with Asymmetric Volatility Spillovers](https://arxiv.org/abs/2511.05126)
*Ariane Nidelle Meli Chrisko,Philipp Otto,Wolfgang Schmid*

Main category: stat.AP

TL;DR: 提出时空E-GARCH模型，扩展传统时空GARCH模型，引入非对称波动率溢出效应，将时间序列E-GARCH推广到时空环境，允许跨空间的瞬时非对称波动率溢出。


<details>
  <summary>Details</summary>
Motivation: 传统时空GARCH模型缺乏对非对称波动率溢出的建模能力，需要将时间序列E-GARCH的非对称特性扩展到时空环境中，以更好地捕捉金融冲击在时间、空间和网络结构中的传播。

Method: 建立时空E-GARCH模型理论框架，推导平稳性条件和矩存在性结果，提出拟极大似然估计量，通过蒙特卡洛模拟评估有限样本性能。

Result: 模型成功应用于金融网络分析，特别是股票市场波动率溢出研究，比较了不同网络结构并分析了瞬时波动率交互中的非对称效应。

Conclusion: 时空E-GARCH模型有效捕捉了金融波动在时空维度上的非对称传播特性，为金融网络分析提供了更精确的建模工具。

Abstract: This paper introduces a spatiotemporal exponential generalised autoregressive
conditional heteroscedasticity (spatiotemporal E-GARCH) model, extending
traditional spatiotemporal GARCH models by incorporating asymmetric volatility
spillovers, while also generalising the time-series E-GARCH model to a
spatiotemporal setting with instantaneous, potentially asymmetric volatility
spillovers across space. The model allows for both temporal and spatial
dependencies in volatility dynamics, capturing how financial shocks propagate
across time, space, and network structures. We establish the theoretical
properties of the model, deriving stationarity conditions and moment existence
results. For estimation, we propose a quasi-maximum likelihood (QML) estimator
and assess its finite-sample performance through Monte Carlo simulations.
Empirically, we apply the model to financial networks, specifically analysing
volatility spillovers in stock markets. We compare different network structures
and analyse asymmetric effects in instantaneous volatility interactions.

</details>
