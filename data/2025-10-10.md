<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 14]
- [stat.AP](#stat.AP) [Total: 4]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 9]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death](https://arxiv.org/abs/2510.07501)
*Sihyung Park,Wenbin Lu,Shu Yang*

Main category: stat.ML

TL;DR: 提出一种基于主分层的方法来评估存在死亡截断问题的动态治疗策略，聚焦于始终存活者的价值函数。


<details>
  <summary>Details</summary>
Motivation: 死亡截断是重症监护中的常见挑战，使得传统动态治疗策略评估方法不适用，因为潜在结果定义不明确。

Method: 基于主分层的方法，推导出半参数高效、多重稳健的估计器，用于多阶段动态治疗策略。

Result: 证明了该估计器的稳健性和效率，并通过实证验证和电子健康记录应用展示了其在个性化治疗优化中的实用性。

Conclusion: 该方法为解决死亡截断问题下的动态治疗策略评估提供了有效的解决方案。

Abstract: Truncation by death, a prevalent challenge in critical care, renders
traditional dynamic treatment regime (DTR) evaluation inapplicable due to
ill-defined potential outcomes. We introduce a principal stratification-based
method, focusing on the always-survivor value function. We derive a
semiparametrically efficient, multiply robust estimator for multi-stage DTRs,
demonstrating its robustness and efficiency. Empirical validation and an
application to electronic health records showcase its utility for personalized
treatment optimization.

</details>


### [2] [From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation](https://arxiv.org/abs/2510.07624)
*Abdelhakim Benechehab,Gabriel Singer,Corentin Léger,Youssef Attia El Hili,Giuseppe Paolo,Albert Thomas,Maurizio Filippone,Balázs Kégl*

Main category: stat.ML

TL;DR: 本文提出了一种双层优化框架来解决生成模型对齐问题，当只有高质量数据集可用时，通过将奖励函数作为外层优化变量，策略梯度目标作为内层问题来训练生成模型。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然估计在泛化和抗灾难性遗忘方面存在局限性，而强化学习方法需要明确的奖励信号，这在实践中往往不可得。本文旨在解决当只有高质量数据集可用时如何对齐生成模型的问题。

Method: 采用双层优化框架，将奖励函数作为外层优化变量，策略梯度目标作为内层优化问题。在可处理设置下进行理论分析，并将洞察推广到表格分类和基于模型的强化学习等应用。

Result: 提出的方法在理论分析中显示出有效性，并成功应用于表格分类和基于模型的强化学习任务，证明了该框架的通用性。

Conclusion: 双层优化框架为仅有高质量数据集时的生成模型对齐问题提供了有效的解决方案，通过理论分析和实际应用验证了其可行性和通用性。

Abstract: Generative models form the backbone of modern machine learning, underpinning
state-of-the-art systems in text, vision, and multimodal applications. While
Maximum Likelihood Estimation has traditionally served as the dominant training
paradigm, recent work have highlighted its limitations, particularly in
generalization and susceptibility to catastrophic forgetting compared to
Reinforcement Learning techniques, such as Policy Gradient methods. However,
these approaches depend on explicit reward signals, which are often unavailable
in practice, leaving open the fundamental problem of how to align generative
models when only high-quality datasets are accessible. In this work, we address
this challenge via a Bilevel Optimization framework, where the reward function
is treated as the optimization variable of an outer-level problem, while a
policy gradient objective defines the inner-level. We then conduct a
theoretical analysis of this optimization problem in a tractable setting and
extract insights that, as we demonstrate, generalize to applications such as
tabular classification and model-based reinforcement learning. We release the
code at https://github.com/abenechehab/nll_to_po .

</details>


### [3] [A Honest Cross-Validation Estimator for Prediction Performance](https://arxiv.org/abs/2510.07649)
*Tianyu Pan,Vincent Z. Yu,Viswanath Devanarayan,Lu Tian*

Main category: stat.ML

TL;DR: 提出一种新方法来评估在特定训练集上训练的模型性能，通过结合交叉验证估计器改进朴素单次分割估计器，使用随机效应模型框架。


<details>
  <summary>Details</summary>
Motivation: 传统交叉验证不能直接估计特定训练集上训练模型的性能，需要更准确评估特定模型在真实应用中的表现。

Method: 开发了分层贝叶斯估计器和经验贝叶斯估计器，利用其他随机分割的交叉验证估计器来改进朴素单次分割估计器。

Result: 模拟和真实数据实验表明，提出的方法性能优于传统交叉验证估计器和朴素单次分割估计器。

Conclusion: 新方法能够更准确地评估特定训练集上训练模型的性能，为模型选择提供了更好的工具。

Abstract: Cross-validation is a standard tool for obtaining a honest assessment of the
performance of a prediction model. The commonly used version repeatedly splits
data, trains the prediction model on the training set, evaluates the model
performance on the test set, and averages the model performance across
different data splits. A well-known criticism is that such cross-validation
procedure does not directly estimate the performance of the particular model
recommended for future use. In this paper, we propose a new method to estimate
the performance of a model trained on a specific (random) training set. A naive
estimator can be obtained by applying the model to a disjoint testing set.
Surprisingly, cross-validation estimators computed from other random splits can
be used to improve this naive estimator within a random-effects model
framework. We develop two estimators -- a hierarchical Bayesian estimator and
an empirical Bayes estimator -- that perform similarly to or better than both
the conventional cross-validation estimator and the naive single-split
estimator. Simulations and a real-data example demonstrate the superior
performance of the proposed method.

</details>


### [4] [When Robustness Meets Conservativeness: Conformalized Uncertainty Calibration for Balanced Decision Making](https://arxiv.org/abs/2510.07750)
*Wenbin Zhou,Shixiang Zhu*

Main category: stat.ML

TL;DR: 提出了一种新的鲁棒优化框架，通过构建误覆盖-遗憾帕累托前沿，为决策者提供分布无关的有限样本保证，帮助在风险和成本偏好之间平衡选择鲁棒性水平。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化依赖预设的鲁棒性水平，选择往往随意，导致保护不足或过于保守。现有方法虽然提供覆盖保证，但仍需预先设定覆盖目标，缺乏选择鲁棒性水平的指导。

Method: 构建有效估计器来追踪误覆盖-遗憾帕累托前沿，为任何鲁棒预测-优化策略族提供分布无关的有限样本保证。

Result: 该方法实现简单，适用于经典优化公式，相比现有方法获得更优的有限样本性能。

Conclusion: 这是第一个为鲁棒性选择提供原则性数据驱动方法的研究，使从业者能够在高风险决策中平衡鲁棒性和保守性。

Abstract: Robust optimization safeguards decisions against uncertainty by optimizing
against worst-case scenarios, yet their effectiveness hinges on a prespecified
robustness level that is often chosen ad hoc, leading to either insufficient
protection or overly conservative and costly solutions. Recent approaches using
conformal prediction construct data-driven uncertainty sets with finite-sample
coverage guarantees, but they still fix coverage targets a priori and offer
little guidance for selecting robustness levels. We propose a new framework
that provides distribution-free, finite-sample guarantees on both miscoverage
and regret for any family of robust predict-then-optimize policies. Our method
constructs valid estimators that trace out the miscoverage-regret Pareto
frontier, enabling decision-makers to reliably evaluate and calibrate
robustness levels according to their cost-risk preferences. The framework is
simple to implement, broadly applicable across classical optimization
formulations, and achieves sharper finite-sample performance than existing
approaches. These results offer the first principled data-driven methodology
for guiding robustness selection and empower practitioners to balance
robustness and conservativeness in high-stakes decision-making.

</details>


### [5] [On the Optimality of the Median-of-Means Estimator under Adversarial Contamination](https://arxiv.org/abs/2510.07867)
*Xabier de Juan,Santiago Mazuelas*

Main category: stat.ML

TL;DR: 本文研究了中位数均值(MoM)估计器在对抗性污染下的性能，证明了其在有限方差分布和无限方差但有限绝对(1+r)阶矩分布类中的极小极大最优性，并指出其在轻尾分布中的次优性。


<details>
  <summary>Details</summary>
Motivation: MoM估计器在i.i.d.样本下已知是最优的，但在对抗性污染场景下的最优性和局限性仍不清楚，特别是超出高斯分布的情况。

Method: 通过推导MoM在对抗性污染下的误差上下界，分析其在多个分布类中的性能。

Result: MoM在有限方差分布类以及无限方差但有限绝对(1+r)阶矩分布类中是极小极大最优的，但在轻尾分布中是次优的。

Conclusion: MoM在对抗性污染下对重尾分布具有鲁棒性和最优性，但对轻尾分布表现不佳。

Abstract: The Median-of-Means (MoM) is a robust estimator widely used in machine
learning that is known to be (minimax) optimal in scenarios where samples are
i.i.d. In more grave scenarios, samples are contaminated by an adversary that
can inspect and modify the data. Previous work has theoretically shown the
suitability of the MoM estimator in certain contaminated settings. However, the
(minimax) optimality of MoM and its limitations under adversarial contamination
remain unknown beyond the Gaussian case. In this paper, we present upper and
lower bounds for the error of MoM under adversarial contamination for multiple
classes of distributions. In particular, we show that MoM is (minimax) optimal
in the class of distributions with finite variance, as well as in the class of
distributions with infinite variance and finite absolute $(1+r)$-th moment. We
also provide lower bounds for MoM's error that match the order of the presented
upper bounds, and show that MoM is sub-optimal for light-tailed distributions.

</details>


### [6] [Surrogate Graph Partitioning for Spatial Prediction](https://arxiv.org/abs/2510.07832)
*Yuta Shikuri,Hironori Fujisawa*

Main category: stat.ML

TL;DR: 提出了一种图分割方法来构建空间段，以最小化个体预测的段内方差之和，并通过近似方案解决计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 尽管空间预测建模能力有所提升，但在需要可解释性的行业中应用仍然有限。替代模型为可解释决策提供了有前景的路径。

Method: 将数据点分配到段的问题建模为混合整数二次规划问题，并开发基于图分割结构特性的近似方案。

Result: 实验结果表明该近似方案在识别空间段方面具有计算效率。

Conclusion: 提出的图分割方法和近似方案能够有效构建可解释的空间段，为实际应用提供了可行的解决方案。

Abstract: Spatial prediction refers to the estimation of unobserved values from
spatially distributed observations. Although recent advances have improved the
capacity to model diverse observation types, adoption in practice remains
limited in industries that demand interpretability. To mitigate this gap,
surrogate models that explain black-box predictors provide a promising path
toward interpretable decision making. In this study, we propose a graph
partitioning problem to construct spatial segments that minimize the sum of
within-segment variances of individual predictions. The assignment of data
points to segments can be formulated as a mixed-integer quadratic programming
problem. While this formulation potentially enables the identification of exact
segments, its computational complexity becomes prohibitive as the number of
data points increases. Motivated by this challenge, we develop an approximation
scheme that leverages the structural properties of graph partitioning.
Experimental results demonstrate the computational efficiency of this
approximation in identifying spatial segments.

</details>


### [7] [On the Optimality of Tracking Fisher Information in Adaptive Testing with Stochastic Binary Responses](https://arxiv.org/abs/2510.07862)
*Sanghwa Kim,Dohyun Ahn,Seungki Min*

Main category: stat.ML

TL;DR: 提出了一种自适应测试算法，通过最大化Fisher信息选择问题，使用矩估计更新能力参数估计，并设计新的检验统计量来确定估计精度，在固定置信度和固定预算两种机制下都达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究从顺序二元响应中估计连续能力参数的问题，这在自适应测试和在线偏好学习中很常见。目标是用尽可能少的查询来保证估计值在期望误差范围内。

Method: 提出Fisher追踪策略：自适应选择问题以最大化Fisher信息，使用矩估计方法更新参数估计，并设计新的检验统计量来决定何时估计足够准确。

Result: 该算法在固定置信度和固定预算两种机制下都达到最优性能，通过利用模型的结构对称性，结合大偏差工具和Ville不等式克服了固定预算设置中的关键技术挑战。

Conclusion: 结果为简单高效的自适应测试程序提供了严格的理论支持，解决了估计演变与查询分布之间依赖关系的关键技术难题。

Abstract: We study the problem of estimating a continuous ability parameter from
sequential binary responses by actively asking questions with varying
difficulties, a setting that arises naturally in adaptive testing and online
preference learning. Our goal is to certify that the estimate lies within a
desired margin of error, using as few queries as possible. We propose a simple
algorithm that adaptively selects questions to maximize Fisher information and
updates the estimate using a method-of-moments approach, paired with a novel
test statistic to decide when the estimate is accurate enough. We prove that
this Fisher-tracking strategy achieves optimal performance in both
fixed-confidence and fixed-budget regimes, which are commonly invested in the
best-arm identification literature. Our analysis overcomes a key technical
challenge in the fixed-budget setting -- handling the dependence between the
evolving estimate and the query distribution -- by exploiting a structural
symmetry in the model and combining large deviation tools with Ville's
inequality. Our results provide rigorous theoretical support for simple and
efficient adaptive testing procedures.

</details>


### [8] [Stick-Breaking Mixture Normalizing Flows with Component-Wise Tail Adaptation for Variational Inference](https://arxiv.org/abs/2510.07965)
*Seungsu Han,Juyoung Hwang,Won Chang*

Main category: stat.ML

TL;DR: 提出了一种基于棒断裂混合分布和分量级尾部适应的归一化流方法（StiCTAF），用于改进贝叶斯推断中复杂后验分布的近似，特别是处理多峰和重尾分布。


<details>
  <summary>Details</summary>
Motivation: 传统基于高斯分布的归一化流在近似复杂后验分布时存在困难，特别是难以捕捉多峰性和重尾特征。

Method: 首先学习灵活的混合基分布以缓解反向KL散度的模式寻求偏差，然后估计未归一化密度的局部尾部指数，最后使用共享主干网络和分量特定的尾部变换来细化每个混合分量。

Result: 在合成后验分布上的实验表明，相比基准模型，该方法在尾部恢复和多峰覆盖方面表现更好。

Conclusion: StiCTAF方法能够准确覆盖模式并进行各向异性尾部建模，同时保持精确的密度评估和稳定优化，在实际数据分析中具有实用价值。

Abstract: Normalizing flows with a Gaussian base provide a computationally efficient
way to approximate posterior distributions in Bayesian inference, but they
often struggle to capture complex posteriors with multimodality and heavy
tails. We propose a stick-breaking mixture base with component-wise tail
adaptation (StiCTAF) for posterior approximation. The method first learns a
flexible mixture base to mitigate the mode-seeking bias of reverse KL
divergence through a weighted average of component-wise ELBOs. It then
estimates local tail indices of unnormalized densities and finally refines each
mixture component using a shared backbone combined with component-specific tail
transforms calibrated by the estimated indices. This design enables accurate
mode coverage and anisotropic tail modeling while retaining exact density
evaluation and stable optimization. Experiments on synthetic posteriors
demonstrate improved tail recovery and better coverage of multiple modes
compared to benchmark models. We also present a real-data analysis illustrating
the practical benefits of our approach for posterior inference.

</details>


### [9] [Beyond Real Data: Synthetic Data through the Lens of Regularization](https://arxiv.org/abs/2510.08095)
*Amitis Shidani,Tyler Farghly,Yang Sun,Habib Ganjgahi,George Deligiannidis*

Main category: stat.ML

TL;DR: 提出了一个学习理论框架来量化合成数据和真实数据之间的权衡，通过算法稳定性推导泛化误差界限，确定了最小化测试误差的最优合成-真实数据比例。


<details>
  <summary>Details</summary>
Motivation: 合成数据在真实数据稀缺时可以改善泛化性能，但过度依赖可能引入分布不匹配导致性能下降。需要量化合成与真实数据之间的权衡关系。

Method: 利用算法稳定性推导泛化误差界限，基于Wasserstein距离表征真实和合成分布之间的差异，在核岭回归混合数据设置下进行详细分析。

Result: 理论预测存在最优的合成数据比例，导致测试误差呈现U形行为。在CIFAR-10和临床脑MRI数据集上实证验证了这一预测。

Conclusion: 精心混合合成目标数据与有限源数据可以缓解域偏移并增强泛化性能。为域内和域外场景提供了实用指导。

Abstract: Synthetic data can improve generalization when real data is scarce, but
excessive reliance may introduce distributional mismatches that degrade
performance. In this paper, we present a learning-theoretic framework to
quantify the trade-off between synthetic and real data. Our approach leverages
algorithmic stability to derive generalization error bounds, characterizing the
optimal synthetic-to-real data ratio that minimizes expected test error as a
function of the Wasserstein distance between the real and synthetic
distributions. We motivate our framework in the setting of kernel ridge
regression with mixed data, offering a detailed analysis that may be of
independent interest. Our theory predicts the existence of an optimal ratio,
leading to a U-shaped behavior of test error with respect to the proportion of
synthetic data. Empirically, we validate this prediction on CIFAR-10 and a
clinical brain MRI dataset. Our theory extends to the important scenario of
domain adaptation, showing that carefully blending synthetic target data with
limited source data can mitigate domain shift and enhance generalization. We
conclude with practical guidance for applying our results to both in-domain and
out-of-domain scenarios.

</details>


### [10] [High-dimensional Analysis of Synthetic Data Selection](https://arxiv.org/abs/2510.08123)
*Parham Rezaei,Filip Kovacevic,Francesco Locatello,Marco Mondelli*

Main category: stat.ML

TL;DR: 该论文通过高维回归理论分析发现，对于线性模型，合成数据与目标分布之间的协方差偏移会影响泛化误差，但均值偏移不会。在深度神经网络中，匹配目标分布的协方差是最优策略，且这一理论见解在实际应用中表现优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型有所发展，但其在创建能提高分类器预测性能的合成数据方面的有效性受到质疑。需要明确影响泛化误差的具体属性，而不仅仅是遵循"合成数据应接近真实数据分布"的经验法则。

Method: 通过高维回归理论分析，研究线性模型中协方差偏移和均值偏移对泛化误差的影响。将理论见解扩展到深度神经网络和生成模型，并实证验证协方差匹配策略。

Result: 理论证明协方差偏移影响泛化误差而均值偏移不影响，在某些设置下匹配目标分布的协方差是最优的。实证结果表明协方差匹配方法在各种训练范式、架构、数据集和生成模型中均优于现有合成数据选择方法。

Conclusion: 协方差匹配是提升合成数据有效性的关键因素，这一理论发现从线性模型扩展到深度神经网络，为生成模型在数据增强中的应用提供了重要指导。

Abstract: Despite the progress in the development of generative models, their
usefulness in creating synthetic data that improve prediction performance of
classifiers has been put into question. Besides heuristic principles such as
"synthetic data should be close to the real data distribution", it is actually
not clear which specific properties affect the generalization error. Our paper
addresses this question through the lens of high-dimensional regression.
Theoretically, we show that, for linear models, the covariance shift between
the target distribution and the distribution of the synthetic data affects the
generalization error but, surprisingly, the mean shift does not. Furthermore we
prove that, in some settings, matching the covariance of the target
distribution is optimal. Remarkably, the theoretical insights from linear
models carry over to deep neural networks and generative models. We empirically
demonstrate that the covariance matching procedure (matching the covariance of
the synthetic data with that of the data coming from the target distribution)
performs well against several recent approaches for synthetic data selection,
across training paradigms, architectures, datasets and generative models used
for augmentation.

</details>


### [11] [PAC Learnability in the Presence of Performativity](https://arxiv.org/abs/2510.08335)
*Ivan Kirev,Lyuben Baltadzhiev,Nikola Konstantinov*

Main category: stat.ML

TL;DR: 本文研究了在机器学习模型部署中出现的表现性偏移问题，即在测试分布中由于模型依赖而产生的变化。作者通过PAC学习框架分析表现性二元分类问题的可学习性，并提出了表现性经验风险函数来应对分布偏移。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在现实应用中的广泛采用，表现性现象（模型依赖的测试分布偏移）日益普遍。由于模型通常仅基于原始分布样本进行训练，这种表现性偏移可能导致测试时性能下降。

Method: 构建了一个表现性经验风险函数，该函数仅依赖于原始分布数据和表现性效应类型，但能够无偏估计分类器在偏移分布上的真实风险。通过最小化这种表现性风险，证明了在标准二元分类设置中任何PAC可学习的假设空间在所考虑的表现性场景下仍然保持PAC可学习性。

Result: 理论分析表明，对于线性标签分布偏移以及更一般的标签和特征变化，所提出的方法能够保持学习能力。在合成和真实数据上的广泛实验评估展示了该方法的优势。

Conclusion: 通过表现性风险最小化方法，可以在存在表现性偏移的情况下保持分类问题的可学习性，为处理现实应用中常见的分布偏移问题提供了理论保证和实用解决方案。

Abstract: Following the wide-spread adoption of machine learning models in real-world
applications, the phenomenon of performativity, i.e. model-dependent shifts in
the test distribution, becomes increasingly prevalent. Unfortunately, since
models are usually trained solely based on samples from the original
(unshifted) distribution, this performative shift may lead to decreased
test-time performance. In this paper, we study the question of whether and when
performative binary classification problems are learnable, via the lens of the
classic PAC (Probably Approximately Correct) learning framework. We motivate
several performative scenarios, accounting in particular for linear shifts in
the label distribution, as well as for more general changes in both the labels
and the features. We construct a performative empirical risk function, which
depends only on data from the original distribution and on the type
performative effect, and is yet an unbiased estimate of the true risk of a
classifier on the shifted distribution. Minimizing this notion of performative
risk allows us to show that any PAC-learnable hypothesis space in the standard
binary classification setting remains PAC-learnable for the considered
performative scenarios. We also conduct an extensive experimental evaluation of
our performative risk minimization method and showcase benefits on synthetic
and real data.

</details>


### [12] [Optimal Stopping in Latent Diffusion Models](https://arxiv.org/abs/2510.08409)
*Yu-Han Wu,Quentin Berthet,Gérard Biau,Claire Boyer,Romuald Elie,Pierre Marion*

Main category: stat.ML

TL;DR: 研究发现潜在扩散模型存在一个反直觉现象：扩散过程的最后步骤会降低样本质量。与传统的数值稳定性解释不同，这种现象源于潜在空间的降维特性。理论分析表明，低维潜在表示需要提前停止，而高维潜在空间则需要更晚停止。


<details>
  <summary>Details</summary>
Motivation: 识别和分析潜在扩散模型中一个令人惊讶的现象——扩散过程的最后步骤会损害样本质量，这与传统认为早期停止仅出于数值稳定性的观点不同。

Method: 在具有线性自编码器的高斯框架下，分析了潜在维度与停止时间之间的相互作用。通过理论推导和实验验证，建立了潜在维度与得分匹配参数约束等超参数的关系。

Result: 理论分析表明，低维潜在表示受益于提前终止，而高维潜在空间需要更晚的停止时间。合成和真实数据集的实验验证了这些特性，证实早期停止可以改善生成质量。

Conclusion: 研究结果为理解潜在维度如何影响样本质量提供了理论基础，并强调停止时间是潜在扩散模型中的一个关键超参数。

Abstract: We identify and analyze a surprising phenomenon of Latent Diffusion Models
(LDMs) where the final steps of the diffusion can degrade sample quality. In
contrast to conventional arguments that justify early stopping for numerical
stability, this phenomenon is intrinsic to the dimensionality reduction in
LDMs. We provide a principled explanation by analyzing the interaction between
latent dimension and stopping time. Under a Gaussian framework with linear
autoencoders, we characterize the conditions under which early stopping is
needed to minimize the distance between generated and target distributions.
More precisely, we show that lower-dimensional representations benefit from
earlier termination, whereas higher-dimensional latent spaces require later
stopping time. We further establish that the latent dimension interplays with
other hyperparameters of the problem such as constraints in the parameters of
score matching. Experiments on synthetic and real datasets illustrate these
properties, underlining that early stopping can improve generative quality.
Together, our results offer a theoretical foundation for understanding how the
latent dimension influences the sample quality, and highlight stopping time as
a key hyperparameter in LDMs.

</details>


### [13] [Accelerated Aggregated D-Optimal Designs for Estimating Main Effects in Black-Box Models](https://arxiv.org/abs/2510.08465)
*Chih-Yu Chang,Ming-Chung Chang*

Main category: stat.ML

TL;DR: 提出了A2D2E方法，基于加速聚合D最优设计，用于改进黑盒模型解释中的主效应估计，解决了现有方法在可扩展性、分布外采样敏感性和相关特征下的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有解释黑盒模型的方法存在可扩展性差、对分布外采样敏感、在相关特征下不稳定等关键限制，需要更高效和稳健的主效应估计方法。

Method: A2D2E方法基于加速聚合D最优设计，利用原则性实验设计来提高主效应估计的效率和鲁棒性。

Result: 建立了理论保证（包括收敛性和方差减少），通过广泛模拟验证了A2D2E的有效性，并在真实数据和语言模型应用中展示了其潜力。

Conclusion: A2D2E为黑盒模型解释提供了一种高效且稳健的主效应估计方法，在理论和实证上都表现出优越性能。

Abstract: Recent advances in supervised learning have driven growing interest in
explaining black-box models, particularly by estimating the effects of input
variables on model predictions. However, existing approaches often face key
limitations, including poor scalability, sensitivity to out-of-distribution
sampling, and instability under correlated features. To address these issues,
we propose A2D2E, an $\textbf{E}$stimator based on $\textbf{A}$ccelerated
$\textbf{A}$ggregated $\textbf{D}$-Optimal $\textbf{D}$esigns. Our method
leverages principled experimental design to improve efficiency and robustness
in main effect estimation. We establish theoretical guarantees, including
convergence and variance reduction, and validate A2D2E through extensive
simulations. We further provide the potential of the proposed method with a
case study on real data and applications in language models. The code to
reproduce the results can be found at https://github.com/cchihyu/A2D2E.

</details>


### [14] [Permutation-Invariant Spectral Learning via Dyson Diffusion](https://arxiv.org/abs/2510.08535)
*Tassilo Schwarz,Cai Dieball,Constantin Kogler,Kevin Lam,Renaud Lambiotte,Arnaud Doucet,Aljaž Godec,George Deligiannidis*

Main category: stat.ML

TL;DR: 提出了Dyson扩散模型，通过将归纳偏置从架构转移到扩散动力学中，利用随机矩阵理论分析扩散过程的谱特性，解决了现有图扩散模型在区分某些图族方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的图扩散模型虽然计算效率高，但难以区分某些图族，除非使用临时特征增强图数据。这种局限性源于在架构中强制施加归纳偏置。

Method: 利用随机矩阵理论分析提取扩散过程的谱特性，将归纳偏置从架构转移到动力学中。引入Dyson扩散模型，使用Dyson布朗运动捕捉邻接矩阵上Ornstein-Uhlenbeck过程的谱动力学，同时保留所有非谱信息。

Result: Dyson扩散模型能够准确学习图谱，并且在性能上优于现有的图扩散模型。

Conclusion: 通过将归纳偏置从学习架构转移到扩散动力学中，Dyson扩散模型有效解决了现有图扩散模型在区分图族方面的局限性，实现了更好的图谱学习性能。

Abstract: Diffusion models are central to generative modeling and have been adapted to
graphs by diffusing adjacency matrix representations. The challenge of having
up to $n!$ such representations for graphs with $n$ nodes is only partially
mitigated by using permutation-equivariant learning architectures. Despite
their computational efficiency, existing graph diffusion models struggle to
distinguish certain graph families, unless graph data are augmented with ad hoc
features. This shortcoming stems from enforcing the inductive bias within the
learning architecture. In this work, we leverage random matrix theory to
analytically extract the spectral properties of the diffusion process, allowing
us to push the inductive bias from the architecture into the dynamics. Building
on this, we introduce the Dyson Diffusion Model, which employs Dyson's Brownian
Motion to capture the spectral dynamics of an Ornstein-Uhlenbeck process on the
adjacency matrix while retaining all non-spectral information. We demonstrate
that the Dyson Diffusion Model learns graph spectra accurately and outperforms
existing graph diffusion models.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [15] [Modeling and forecasting of European Carbon Emission Allowance futures by ARIMA-TX-GARCH models with correlation threshold](https://arxiv.org/abs/2510.07568)
*Jaeho Lee,Eunju Hwang*

Main category: stat.AP

TL;DR: 提出ARIMA-TX-GARCH模型用于预测欧洲碳排放配额期货价格，并引入布伦特原油期货价格作为外生变量


<details>
  <summary>Details</summary>
Motivation: 需要更准确地预测欧洲碳排放配额期货价格，考虑到能源市场（特别是原油价格）对其的影响

Method: 开发ARIMA-TX-GARCH模型，将布伦特原油期货价格作为外生变量纳入碳排放配额期货价格预测

Result: 建立了包含外生变量的综合预测模型，能够更好地捕捉碳排放配额价格动态

Conclusion: ARIMA-TX-GARCH模型结合外生变量能够有效提升欧洲碳排放配额期货价格的预测精度

Abstract: We propose an ARIMA-TX-GARCH model and use it to forecast European Carbon
Emission Allowance futures prices, incorporating Brent crude oil futures prices
as an exogenous variable.

</details>


### [16] [Large-scale spatial variable gene atlas for spatial transcriptomics](https://arxiv.org/abs/2510.07653)
*Jiawen Chen,Jinwei Zhang,Dongshen Peng,Yutong Song,Aitong Ruan,Yun Li,Didong Li*

Main category: stat.AP

TL;DR: 该研究对20种最先进的空间可变基因检测方法进行了全面基准测试，使用来自18种组织类型的662张切片数据，评估了这些方法在生物学和技术指标上的表现，并构建了首个跨组织SVG图谱。


<details>
  <summary>Details</summary>
Motivation: 随着空间转录组学技术的快速发展，准确识别不同平台、组织类型和疾病背景下的空间可变基因已成为重要机遇和计算挑战，需要系统评估现有方法的性能。

Method: 使用STimage-1K4M大规模资源中的662张人类组织切片数据，对20种SVG检测方法进行基准测试，评估其在病理学家注释的域特异性标记物恢复、跨切片可重复性、高分辨率数据扩展性和技术变异鲁棒性等方面的表现。

Result: 研究发现不同方法的表现因组织类型、空间分辨率和研究设计而异显著。构建了首个跨组织SVG图谱，揭示了组织间相似性（如胸腺和淋巴结高度重叠），并发现了与转移、免疫浸润和组织起源身份相关的空间基因程序。

Conclusion: 该研究为评估和解释空间基因表达定义了一个框架，并为空间转录组学界建立了一个参考资源。

Abstract: Spatial variable genes (SVGs) reveal critical information about tissue
architecture, cellular interactions, and disease microenvironments. As spatial
transcriptomics (ST) technologies proliferate, accurately identifying SVGs
across diverse platforms, tissue types, and disease contexts has become both a
major opportunity and a significant computational challenge. Here, we present a
comprehensive benchmarking study of 20 state-of-the-art SVG detection methods
using human slides from STimage-1K4M, a large-scale resource of ST data
comprising 662 slides from more than 18 tissue types. We evaluate each method
across a range of biologically and technically meaningful criteria, including
recovery of pathologist-annotated domain-specific markers, cross-slide
reproducibility, scalability to high-resolution data, and robustness to
technical variation. Our results reveal marked differences in performance
depending on tissue type, spatial resolution, and study design. Beyond
benchmarking, we construct the first cross-tissue atlas of SVGs, enabling
comparative analysis of spatial gene programs across cancer and normal tissues.
We observe similarities between pairs of tissues that reflect developmental and
functional relationships, such as high overlap between thymus and lymph node,
and uncover spatial gene programs associated with metastasis, immune
infiltration, and tissue-of-origin identity in cancer. Together, our work
defines a framework for evaluating and interpreting spatial gene expression and
establishes a reference resource for the ST community.

</details>


### [17] [Evaluating multi-season occupancy models with autocorrelation fitted to heterogeneous datasets](https://arxiv.org/abs/2510.08151)
*André Luís Luza,Didier Alard,Frédéric Barraquand*

Main category: stat.AP

TL;DR: 该研究评估了包含时空自相关效应的多季节占据模型在高度异质性蝴蝶占据数据集中的表现，发现模型对数据异质性和协变量重叠具有鲁棒性，但在存在严重时空数据缺失时会出现可识别性问题，导致占据率预测偏向平均值且被高估。


<details>
  <summary>Details</summary>
Motivation: 生态学中常用占据模型预测物种分布，但数据缺乏重复性常导致模型不可识别。虽然已有研究提出通过建模时空自相关来解决此问题，但这些模型在高度异质性数据集（特别是缺失数据或单次访问数据占主导的情况）中的表现仍不清楚。

Method: 基于异质性精细尺度蝴蝶占据数据集，评估多季节占据模型在以下情况下的性能：调查次数呈偏态分布、占据和检测子模型间协变量重叠、观测存在时空聚类。模型包含空间和时间随机效应。

Result: 模型对异质性数据和协变量重叠具有鲁棒性。但当添加时空数据缺失时，站点占据率预测偏向平均占据率且被高估。随机效应未能纠正数据缺失的影响，因为方差和自相关参数存在可识别性问题。两种蝴蝶物种的占据分析进一步证实了这些结果。

Conclusion: 包含自相关的多季节占据模型对异质性数据和协变量重叠具有鲁棒性，但仍存在可识别性问题，在严重数据缺失情况下预测能力受限，即使在数据丰富的区域也会受到影响。

Abstract: Predicting species distributions using occupancy models accounting for
imperfect detection is now commonplace in ecology. Recently, modelling spatial
and temporal autocorrelation was proposed to alleviate the lack of replication
in occupancy data, which often prevents model identifiability. However, how
such models perform in highly heterogeneous datasets where missing or
single-visit data dominates remains an open question. Motivated by an
heterogeneous fine-scale butterfly occupancy dataset, we evaluate the
performance of a multi-season occupancy model with spatial and temporal random
effects to a skewed (Poisson) distribution of the number of surveys per site,
overlap of covariates between occupancy and detection submodels, and
spatiotemporal clustering of observations. Results showed that the model is
robust to heterogeneous data and covariate overlap. However, when
spatiotemporal gaps were added, site occupancy was biased towards the average
occupancy, itself overestimated. Random effects did not correct the influence
of gaps, due to identifiability issues of variance and autocorrelation
parameters. Occupancy analysis of two butterfly species further confirmed these
results. Overall, multi-season occupancy models with autocorrelation are robust
to heterogeneous data and covariate overlap, but still present identifiability
issues and are challenged by severe data gaps, which compromise predictions
even in data-rich areas.

</details>


### [18] [Two-Stage Trigonometric Regression for Modeling Circadian Rhythms](https://arxiv.org/abs/2510.08309)
*Michael T. Gorczyca,Jenna D. Li,Charissa M. Newkirk,Arjun S. Srivatsa,Hugo F. M. Milan*

Main category: stat.AP

TL;DR: 提出了一种改进的两阶段(RTS)三角回归方法，用于分析昼夜节律数据，解决了传统方法因忽略个体峰值时间差异而产生的衰减偏差问题。


<details>
  <summary>Details</summary>
Motivation: 昼夜节律研究中，个体间振荡峰值时间存在差异，传统三角回归方法忽略这些差异会导致种群水平振幅参数的衰减偏差，从而影响研究结论的准确性。

Method: 开发了改进的两阶段(RTS)方法：第一阶段估计个体水平模型参数，第二阶段将这些个体水平估计的变换聚合以进行种群水平参数估计和推断。

Result: 模拟研究表明RTS方法相比标准两阶段(STS)方法能减轻参数估计偏差，获得更高的统计功效，并保持适当的I类错误控制。在皮质醇水平和心率数据应用中，RTS方法获得了更大的种群水平振幅参数估计值和更小的p值。

Conclusion: RTS方法能有效解决昼夜节律数据分析中的衰减偏差问题，在大多数情况下优于忽略个体峰值时间差异的标准方法，特别是在振幅相对较强且样本量足够时表现更佳。

Abstract: Gene expression levels, hormone secretion, and internal body temperature each
oscillate over an approximately 24-hour cycle, or display circadian rhythms.
Many circadian biology studies have investigated how these rhythms vary across
cohorts, uncovering associations between atypical rhythms and diseases such as
cancer, metabolic syndrome, and sleep disorders. A challenge in analyzing
circadian biology data is that the oscillation peak and trough times for a
phenomenon differ across individuals. If these individual-level differences are
not accounted for in trigonometric regression, which is prevalent in circadian
biology studies, then estimates of the population-level amplitude parameters
can suffer from attenuation bias. This attenuation bias could lead to
inaccurate study conclusions. To address attenuation bias, we propose a refined
two-stage (RTS) method for trigonometric regression given longitudinal data
obtained from each individual participating in a study. In the first stage, the
parameters of individual-level models are estimated. In the second stage,
transformations of these individual-level estimates are aggregated to produce
population-level parameter estimates for inference. Simulation studies show
that our RTS method mitigates bias in parameter estimation, obtains greater
statistical power, and maintains appropriate type I error control when compared
to the standard two-stage (STS) method, which ignores individual-level
differences in peak and trough times. The only exception for parameter
estimation and statistical power occurs when the oscillation amplitudes are
weak relative to random variability in the data and the sample size is small.
Illustrations with cortisol level data and heart rate data show that our RTS
method obtains larger population-level amplitude parameter estimates and
smaller $p$-values for multiple hypothesis tests when compared to the STS
method.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [19] [A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo](https://arxiv.org/abs/2510.07559)
*Adrien Corenflos,Hai-Dang Dau*

Main category: stat.CO

TL;DR: 该论文提出了首个基于f-散度的通用MCMC收敛诊断方法，通过耦合权重协调方案和交互马尔可夫链的一致加权，提供可计算且一致的f-散度上界估计。


<details>
  <summary>Details</summary>
Motivation: 解决MCMC理论分析（基于统计散度）与实际诊断工具之间的长期差距，让用户能够直接监控KL散度、χ²散度、Hellinger距离和总变差距离等关键指标。

Method: 提出耦合基的'权重协调'方案，对交互马尔可夫链产生相对于目标分布的直接、可计算且一致的加权，并证明这种一致加权的经验测度可用于提供f-散度的上界。

Result: 数值实验表明该方法是一个实用且具有竞争力的诊断工具，所提出的上界保证随时间收紧并在链接近平稳时收敛到零。

Conclusion: 该方法填补了MCMC理论分析与实际诊断之间的重要空白，提供了基于f-散度的通用收敛诊断框架，具有理论保证和实际可行性。

Abstract: A long-standing gap exists between the theoretical analysis of Markov chain
Monte Carlo convergence, which is often based on statistical divergences, and
the diagnostics used in practice. We introduce the first general convergence
diagnostics for Markov chain Monte Carlo based on any f-divergence, allowing
users to directly monitor, among others, the Kullback--Leibler and the $\chi^2$
divergences as well as the Hellinger and the total variation distances. Our
first key contribution is a coupling-based `weight harmonization' scheme that
produces a direct, computable, and consistent weighting of interacting Markov
chains with respect to their target distribution. The second key contribution
is to show how such consistent weightings of empirical measures can be used to
provide upper bounds to f-divergences in general. We prove that these bounds
are guaranteed to tighten over time and converge to zero as the chains approach
stationarity, providing a concrete diagnostic. Numerical experiments
demonstrate that our method is a practical and competitive diagnostic tool.

</details>


### [20] [Rotated Mean-Field Variational Inference and Iterative Gaussianization](https://arxiv.org/abs/2510.07732)
*Yifan Chen,Sifan Liu*

Main category: stat.CO

TL;DR: 提出在旋转坐标系中进行平均场变分推断，通过PCA减少变量相关性，比标准MFVI更准确且计算成本低


<details>
  <summary>Details</summary>
Motivation: 标准平均场变分推断在原始坐标轴上处理相关变量效果不佳，需要更有效的方法来减少变量相关性

Method: 使用PCA对涉及目标评分函数的交叉协方差矩阵进行旋转，在旋转坐标系中执行MFVI，迭代应用此过程逐步将目标分布转换为高斯分布

Result: 该方法比标准MFVI更准确，同时比传统归一化流计算成本低得多

Conclusion: 旋转坐标系中的MFVI提供了一种计算高效的方式来构建流式传输映射，在贝叶斯推断任务中表现优异

Abstract: We propose to perform mean-field variational inference (MFVI) in a rotated
coordinate system that reduces correlations between variables. The rotation is
determined by principal component analysis (PCA) of a cross-covariance matrix
involving the target's score function. Compared with standard MFVI along the
original axes, MFVI in this rotated system often yields substantially more
accurate approximations with negligible additional cost.
  MFVI in a rotated coordinate system defines a rotation and a coordinatewise
map that together move the target closer to Gaussian. Iterating this procedure
yields a sequence of transformations that progressively transforms the target
toward Gaussian. The resulting algorithm provides a computationally efficient
way to construct flow-like transport maps: it requires only MFVI subproblems,
avoids large-scale optimization, and yields transformations that are easy to
invert and evaluate. In Bayesian inference tasks, we demonstrate that the
proposed method achieves higher accuracy than standard MFVI, while maintaining
much lower computational cost than conventional normalizing flows.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [21] [Zero-Inflated Bayesian Multi-Study Infinite Non-Negative Matrix Factorization](https://arxiv.org/abs/2510.07518)
*Blake Hansen,Dafne Zorzetto,Valeria Edefonti,Roberta De Vito*

Main category: stat.ME

TL;DR: 提出了一种新的贝叶斯非负矩阵分解模型，用于分析多研究饮食数据，能处理零膨胀、个体异质性，并支持跨研究信息共享


<details>
  <summary>Details</summary>
Motivation: 饮食数据具有高维、稀疏、零膨胀和个体异质性等统计挑战，现有NMF方法缺乏灵活性，且多研究数据整合需要跨研究信息共享机制

Method: 开发贝叶斯NMF模型，包含混合组分处理零膨胀，使用贝叶斯非参数先验表征个体协变量引起的异质性，支持多研究数据联合建模

Result: 模拟研究表明该模型显著提高了估计准确性，在上消化道癌症病例对照研究中识别出有营养学意义的饮食模式

Conclusion: 该方法为饮食与健康关联研究提供了更灵活的建模框架，能有效处理多研究饮食数据的复杂统计特性

Abstract: Understanding the association between dietary patterns and health outcomes,
such as the cancer risk, is crucial to inform public health guidelines and
shaping future dietary interventions. However, dietary intake data present
several statistical challenges: they are high-dimensional, often sparse with
excess zeros, and exhibit heterogeneity driven by individual-level covariates.
Non-Negative Matrix Factorization (NMF), commonly used to estimate patterns in
high-dimensional count data, typically relies on Poisson assumptions and lacks
the flexibility to fully address these complexities. Additionally, integrating
data across multiple studies, such as case-control studies on cancer risk,
requires models that can share information across sources while preserving
study-specific structure.
  In this paper, we introduce a novel Bayesian NMF model that (i) jointly
models multi-study count data to enable cross-study information sharing, (ii)
incorporate a mixture component to account for zero inflation, and (iii)
leverage flexible Bayesian non-parametric priors for characterizing the
heterogeneity in pattern scores induced by the individual covariates. This
structure allows for clustering of individuals based on dietary profiles,
enabling downstream association analyses with health outcomes. Through
extensive simulation studies, we demonstrate that our model significantly
improves estimation accuracy compared to existing Bayesian NMF methods.
  We further illustrate its utility through an application to multiple
case-control studies on diet and upper aero-digestive tract cancers,
identifying nutritionally meaningful dietary patterns. An R package
implementing our approach is available at
https://github.com/blhansen/ZIMultiStudyNMF.

</details>


### [22] [Integrating smart surveys with traditional surveys](https://arxiv.org/abs/2510.07521)
*Danielle Mccool,Peter Lugtig,Bella Struminskaya*

Main category: stat.ME

TL;DR: 本文区分了智能调查与传统日记调查的两种整合方法：混合模式方法（优先结果对齐）和多源方法（保持模式差异），并提出了决策框架来选择合适的整合策略。


<details>
  <summary>Details</summary>
Motivation: 智能调查使用传感器和机器学习来减轻受访者负担并提高数据质量，但与传统日记调查在测量和代表性上存在差异，这使得在时间序列或混合/多源背景下整合数据变得困难。

Method: 使用旅行调查作为示例，探讨两种整合方法的优缺点，并提出决策框架来指导研究人员选择合适的整合策略。

Result: 区分了混合模式方法（注重结果对齐和最小化测量差异）与多源方法（保持模式差异并在建模阶段整合数据），分析了各自的利弊。

Conclusion: 提出了一个决策框架，帮助研究人员根据具体需求选择适当的整合策略，以充分利用不同数据源的优势。

Abstract: Smart surveys are surveys that make use of sensors and machine intelligence
to reduce respondent burden and increase data quality. Smart surveys have been
tests as a way to improve diary surveys in official statistics, where data are
collected on topics such as travel, time use and household expenditures. There
are often inherent differences both in measurement and representation between
smart surveys and traditional diaries, which makes it difficult to integrate
both data sources in producing statistics over time, or within a mixed- or
multi-source context. This paper distinguishes two different approaches to
integration: the mixed-mode approach, which prioritizes outcome alignment and
minimizes measurement differences for straightforward data merging, and the
multisource approach, which maintains inherent mode differences and integrates
data at the modeling stage, allowing exploitation of the strengths of each
source. Using travel surveys as an illustrative example, we explore the
benefits and drawbacks of each approach, and propose a decision framework to
guide researchers in selecting the appropriate integration strategy.

</details>


### [23] [Density estimation for compositional data using nonparametric mixtures](https://arxiv.org/abs/2510.07608)
*Jiajin Xie,Yong Wang,Eduardo García-Portugués*

Main category: stat.ME

TL;DR: 提出了一种基于非参数狄利克雷混合的组成数据密度估计框架，能够自然处理边界值，避免变换或零替换，在边界值数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 组成数据（比例数据）在多个领域普遍存在，现有非参数密度估计方法依赖变换，可能在边界附近产生显著偏差。

Method: 使用非参数狄利克雷混合模型，自然适应边界值，无需变换或零替换，同时识别边界支撑的组分。

Result: 广泛模拟显示所提估计器优于现有方法，在GDP数据分析、手写数字识别和皮肤检测三个实际应用中证明其有效性。

Conclusion: 非参数狄利克雷混合为组成数据密度估计提供了实用框架，特别适合处理含零值或近零值的数据。

Abstract: Compositional data, representing proportions constrained to the simplex,
arise in diverse fields such as geosciences, ecology, genomics, and microbiome
research. Existing nonparametric density estimation methods often rely on
transformations, which may induce substantial bias near the simplex boundary.
We propose a nonparametric mixture-based framework for density estimation on
compositions. Nonparametric Dirichlet mixtures are employed to naturally
accommodate boundary values, thereby avoiding the transformation or
zero-replacement, while also identifying components supported on the boundary,
providing reliable estimates for data with zero or near-zero values. Bandwidth
selection and initialization schemes are addressed. For comparison,
nonparametric Gaussian mixtures, coupled with log-ratio transformations, are
also considered. Extensive simulations show that the proposed estimators
outperform existing approaches. Three real data applications, including GDP
data analysis, handwritten digit recognition, and skin detection, demonstrate
the usefulness of nonparametric Dirichlet mixtures in practice.

</details>


### [24] [Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data](https://arxiv.org/abs/2510.07770)
*Zhi Yang Tho,Raymond Chambers,A. H. Welsh*

Main category: stat.ME

TL;DR: 提出了两种适用于不平衡聚类数据的自助法：比例随机效应块自助法和改进随机效应块自助法，用于线性混合模型的参数推断。


<details>
  <summary>Details</summary>
Motivation: 传统随机效应块自助法仅适用于平衡聚类数据，而实际应用中聚类大小往往高度不平衡，需要开发适用于不平衡情况的推断方法。

Method: 通过比例随机效应块自助法和改进随机效应块自助法，扩展了原始随机效应块自助法，使其能够处理一般分布的随机效应和误差项。

Result: 模拟研究表明，提出的方法在有限样本下具有优越的推断性能，应用于阿曼降雨增强试验数据集（聚类大小1-58）显示出改进的置信区间和电离技术对降雨的显著影响。

Conclusion: 提出的两种自助法在不平衡聚类数据下具有Fisher一致性，相比传统方法和其他现有方法表现更优，为线性混合模型在不平衡聚类数据下的推断提供了有效工具。

Abstract: Clustered data arise naturally in many scientific and applied research
settings where units are grouped within clusters. They are commonly analyzed
using linear mixed models to account for within-cluster correlations. This
article focuses on the scenario in which cluster sizes might be highly
unbalanced and proposes a proportional random effect block bootstrap and a
modified random effect block bootstrap, which are applicable in such cases and
accommodate general distributions of random effects and error terms. These
methods generalize the random effect block bootstrap, originally designed for
the balanced case, and can be used for inference on parameters of linear mixed
models or functions thereof. Both proposed bootstraps are shown to enjoy Fisher
consistency under general cluster sizes, while the original random effect block
bootstrap is consistent only for balanced clusters. Simulations demonstrate
strong finite sample inferential performance of the proposed bootstraps
relative to the random effect block bootstrap and other existing bootstrap
methods for clustered data. Application to the Oman rainfall enhancement trial
dataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap
confidence intervals using the proposed bootstraps over the random effect block
bootstrap and a statistically significant effect of the ionization technology
on rainfall.

</details>


### [25] [Detection of mean changes in partially observed functional data](https://arxiv.org/abs/2510.07854)
*Šárka Hudecová,Claudia Kirch*

Main category: stat.ME

TL;DR: 提出了一种针对部分观测函数序列均值变化的检验方法，适用于突变和渐变场景，使用置换检验评估显著性。


<details>
  <summary>Details</summary>
Motivation: 处理函数观测序列中均值变化的问题，特别是当观测只在定义域的子集上部分可用时，缺乏完整信息的情况。

Method: 使用置换检验方法，包括固定置换样本数的经典方法和控制重采样风险的变体（基于随机数据驱动的置换样本数）。

Result: 通过蒙特卡洛模拟研究和真实数据应用验证了该方法在小样本情况下的性能表现。

Conclusion: 该方法能够有效检测部分观测函数序列中的均值变化，适用于多种变化类型，并通过模拟和应用验证了其有效性。

Abstract: We propose a test for a change in the mean for a sequence of functional
observations that are only partially observed on subsets of the domain, with no
information available on the complement. The framework accommodates important
scenarios, including both abrupt and gradual changes. The significance of the
test statistic is assessed via a permutation test. In addition to the classical
permutation approach with a fixed number of permutation samples, we also
discuss a variant with controlled resampling risk that relies on a random
(data-driven) number of permutation samples. The small sample performance of
the proposed methodology is illustrated in a Monte Carlo simulation study and
an application to real data.

</details>


### [26] [Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles](https://arxiv.org/abs/2510.08204)
*Soham Ghosh,Saloni Bhogale,Sameer K. Deshpande*

Main category: stat.ME

TL;DR: 提出sparseVCBART方法，用于高维变系数模型，通过回归树集成和稀疏先验实现变量选择和效应修饰符识别


<details>
  <summary>Details</summary>
Motivation: 在p和/或R超过观测数的高维设置中，现有变系数模型方法无法识别哪些协变量具有非零效应以及哪些效应修饰符驱动这些效应

Method: 使用回归树集成近似变系数模型中的每个系数函数，通过全局-局部收缩先验和分层先验鼓励稀疏性

Result: sparseVCBART后验以接近极小极大最优速率收缩，自动适应未知的稀疏结构和真实系数函数的平滑度，相比现有方法具有竞争性的预测精度和更窄、校准更好的不确定性区间

Conclusion: sparseVCBART在预测准确性和不确定性量化方面优于现有方法，特别适用于零协变量效应的情况，并成功应用于研究人际对话对偏见影响的政治和人口特征效应修饰

Abstract: By allowing the effects of $p$ covariates in a linear regression model to
vary as functions of $R$ additional effect modifiers, varying-coefficient
models (VCMs) strike a compelling balance between interpretable-but-rigid
parametric models popular in classical statistics and flexible-but-opaque
methods popular in machine learning. But in high-dimensional settings where $p$
and/or $R$ exceed the number of observations, existing approaches to fitting
VCMs fail to identify which covariates have a non-zero effect and which effect
modifiers drive these effects. We propose sparseVCBART, a fully Bayesian model
that approximates each coefficient function in a VCM with a regression tree
ensemble and encourages sparsity with a global--local shrinkage prior on the
regression tree leaf outputs and a hierarchical prior on the splitting
probabilities of each tree. We show that the sparseVCBART posterior contracts
at a near-minimax optimal rate, automatically adapting to the unknown sparsity
structure and smoothness of the true coefficient functions. Compared to
existing state-of-the-art methods, sparseVCBART achieved competitive predictive
accuracy and substantially narrower and better-calibrated uncertainty
intervals, especially for null covariate effects. We use sparseVCBART to
investigate how the effects of interpersonal conversations on prejudice could
vary according to the political and demographic characteristics of the
respondents.

</details>


### [27] [Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data](https://arxiv.org/abs/2510.08304)
*Matteo Amestoy,Mark van de Wiel,Jeroen Lakerveld,Wessel van Wieringen*

Main category: stat.ME

TL;DR: 提出了一种结合贝叶斯轮廓回归和线性混合模型的新统计框架，用于分析暴露组数据中的高共线性、纵向测量和复杂交互作用问题。


<details>
  <summary>Details</summary>
Motivation: 暴露组分析面临高共线性、纵向测量特性和复杂交互作用等挑战，需要新的统计方法来有效处理这些问题。

Method: 将轮廓回归（处理共线性）整合到线性混合模型（处理纵向数据）中，形成轮廓-LMM方法，能够同时考虑个体内时间变异性和暴露簇与个体特征的交互作用。

Result: 模拟数据验证表明该方法能准确识别模型参数并恢复真实的潜在暴露簇结构；在Lifelines队列数据中成功识别出与舒张压显著相关的暴露组合。

Conclusion: 轮廓-LMM框架为暴露组分析提供了一个有效的统计工具，能够同时处理共线性、纵向数据和复杂交互作用等挑战。

Abstract: Exposure to diverse non-genetic factors, known as the exposome, is a critical
determinant of health outcomes. However, analyzing the exposome presents
significant methodological challenges, including: high collinearity among
exposures, the longitudinal nature of repeated measurements, and potential
complex interactions with individual characteristics. In this paper, we address
these challenges by proposing a novel statistical framework that extends
Bayesian profile regression. Our method integrates profile regression, which
handles collinearity by clustering exposures into latent profiles, into a
linear mixed model (LMM), a framework for longitudinal data analysis. This
profile-LMM approach effectively accounts for within-person variability over
time while also incorporating interactions between the latent exposure clusters
and individual characteristics. We validate our method using simulated data,
demonstrating its ability to accurately identify model parameters and recover
the true latent exposure cluster structure. Finally, we apply this approach to
a large longitudinal data set from the Lifelines cohort to identify
combinations of exposures that are significantly associated with diastolic
blood pressure.

</details>


### [28] [Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials](https://arxiv.org/abs/2510.08359)
*Jinho Cha,Eunchan Cha*

Main category: stat.ME

TL;DR: 提出了一种双重稳健的估计均值偏移效应(DR-EMEE)方法，结合逆概率加权和结果回归，在微随机化试验中实现稳定高效的因果推断。


<details>
  <summary>Details</summary>
Motivation: 标准逆概率加权(IPW)估计器在小样本或极端随机化下不稳定，而估计均值偏移效应(EMEE)虽然提高了效率但缺乏双重稳健性。

Method: 开发了双重稳健的EMEE方法，使用稳定化和截断权重，结合逐决策IPW和结果回归，并扩展到机器学习干扰估计器。

Result: 在模拟中，DR-EMEE降低了均方根误差，提高了覆盖率，相比IPW实现了高达两倍的效率提升，相比EMEE提升了5-10%的效率。

Conclusion: DR-EMEE在HeartSteps、PAMAP2和mHealth数据集的应用证实了该方法在随机化和观察性设置下都能提供稳定高效的推断。

Abstract: Micro-randomized trials (MRTs) are increasingly used to evaluate mobile
health interventions with binary proximal outcomes. Standard inverse
probability weighting (IPW) estimators are unbiased but unstable in small
samples or under extreme randomization. Estimated mean excursion effect (EMEE)
improves efficiency but lacks double robustness. We propose a doubly robust
EMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision
IPW and outcome regression. We prove double robustness, asymptotic efficiency,
and provide finite-sample variance corrections, with extensions to machine
learning nuisance estimators. In simulations, DR-EMEE reduces root mean squared
error, improves coverage, and achieves up to twofold efficiency gains over IPW
and five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and
mHealth datasets confirm stable and efficient inference across both randomized
and observational settings.

</details>


### [29] [Estimands and doubly robust estimation for cluster-randomized trials with survival outcomes](https://arxiv.org/abs/2510.08438)
*Xi Fang,Bingkai Wang,Liangyuan Hu,Fan Li*

Main category: stat.ME

TL;DR: 本文针对聚类随机试验中的生存结局数据，提出了双重稳健估计方法，用于估计聚类水平和个体水平的处理效应，在协变量依赖删失情况下具有良好的统计性质。


<details>
  <summary>Details</summary>
Motivation: 聚类随机试验中需要区分聚类水平和个体水平的处理效应，特别是在生存结局分析中，需要明确定义潜在结果框架下的估计量，并处理右删失数据的问题。

Method: 提出了双重稳健估计方法，当删失模型或结局模型之一正确指定时即可保证一致性，并探索了删失和结局模型的不同建模选项，使用基于删除的刀切法进行方差和区间估计。

Result: 广泛的模拟研究表明，所提出的方法在有限样本中表现良好，能够准确估计处理效应。

Conclusion: 该方法为聚类随机试验中生存结局的分析提供了有效的统计工具，特别适用于协变量依赖删失的情况，并通过实际临床试验数据验证了其应用价值。

Abstract: Cluster-randomized trials (CRTs) are experimental designs where groups or
clusters of participants, rather than the individual participants themselves,
are randomized to intervention groups. Analyzing CRT requires distinguishing
between treatment effects at the cluster level and the individual level, which
requires a clear definition of the estimands under the potential outcomes
framework. For analyzing survival outcomes, it is common to assess the
treatment effect by comparing survival functions or restricted mean survival
times between treatment groups. In this article, we formally characterize
cluster-level and individual-level treatment effect estimands with
right-censored survival outcomes in CRTs and propose doubly robust estimators
for targeting such estimands. Under covariate-dependent censoring, our
estimators ensure consistency when either the censoring model or the outcome
model is correctly specified, but not necessarily both. We explore different
modeling options for the censoring and outcome models to estimate the censoring
and survival distributions, and investigate a deletion-based jackknife method
for variance and interval estimation. Extensive simulations demonstrate that
the proposed methods perform adequately in finite samples. Finally, we
illustrate our method by analyzing a completed CRT with survival endpoints.

</details>
