<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 21]
- [stat.AP](#stat.AP) [Total: 6]
- [stat.CO](#stat.CO) [Total: 3]
- [stat.ML](#stat.ML) [Total: 35]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [On the calibration of survival models with competing risks](https://arxiv.org/abs/2602.00194)
*Julie Alberge,Tristan Haugomat,Gaël Varoquaux,Judith Abécassis*

Main category: stat.ME

TL;DR: 提出针对竞争风险生存分析的新校准框架，包含两个新的校准度量指标和相应的估计、检验、校正方法，在保持判别能力的同时改善概率估计质量。


<details>
  <summary>Details</summary>
Motivation: 竞争风险生存分析中，现有校准度量不适用于该场景，且现有模型无法提供行为良好的概率估计，而准确的概率估计对决策至关重要。

Method: 提出专门框架，包含两个新的校准度量（对oracle估计器最小化），并开发了估计、检验和校正校准的方法，包括重新校准方法。

Result: 新校准方法能够在保持判别能力的同时获得良好的概率估计，解决了竞争风险设置下的校准问题。

Conclusion: 该研究填补了竞争风险生存分析中校准方法的空白，提供了有效的框架和工具来评估和改进概率估计的可靠性。

Abstract: Survival analysis deals with modeling the time until an event occurs, and accurate probability estimates are crucial for decision-making, particularly in the competing-risks setting where multiple events are possible. While recent work has addressed calibration in standard survival analysis, the competing-risks setting remains under-explored as it is harder (the calibration applies to both probabilities across classes and time horizon). We show that existing calibration measures are not suited to the competing-risk setting and that recent models do not give well-behaved probabilities. To address this, we introduce a dedicated framework with two novel calibration measures that are minimized for oracle estimators (i.e., both measures are proper). We also introduce some methods to estimate, test, and correct the calibration. Our recalibration methods yield good probabilities while preserving discrimination.

</details>


### [2] [Data-Driven Uniform Inference for General Continuous Treatment Models via Minimum-Variance Weighting](https://arxiv.org/abs/2602.01595)
*Chunrong Ai,Wei Huang,Zheng Zhang*

Main category: stat.ME

TL;DR: 该论文提出了一种非参数加权局部线性回归方法来估计广义剂量-响应函数，并开发了构建均匀置信带的bootstrap程序，权重具有最小样本方差且能消除处理与混杂变量间的关联。


<details>
  <summary>Details</summary>
Motivation: 现有研究将广义剂量-响应函数（GDRF）设定为仅依赖于处理状态的参数函数，并使用最大熵方法估计权重。本文旨在改进这一方法，将GDRF设定为非参数函数，以更灵活地捕捉剂量-响应关系。

Method: 提出加权局部线性回归估计GDRF，权重具有最小样本方差且能消除处理与混杂变量间的关联，权重有闭式解便于bootstrap计算。开发了bootstrap程序构建均匀置信带，包含数据驱动的欠平滑调参方法和偏差控制置信带。

Result: 在特定条件下推导了GDRF估计量的均匀Bahadur表示，建立了相应均匀置信带的有效性。模拟研究和实际应用证明了所提方法的有效性。

Conclusion: 提出的非参数加权局部线性回归方法能有效估计广义剂量-响应函数，数据驱动的置信带构建方法具有实用价值，为剂量-响应分析提供了更灵活可靠的统计工具。

Abstract: Ai et al. (2021) studied the estimation of a general dose-response function (GDRF) of a continuous treatment that includes the average dose-response function, the quantile dose-response function, and other expectiles of the dose-response distribution. They specified the GDRF as a parametric function of the treatment status only and proposed a weighted regression with the weighting function estimated using the maximum entropy approach. This paper specifies the GDRF as a nonparametric function of the treatment status, proposes a weighted local linear regression for estimating GDRF, and develops a bootstrap procedure for constructing the uniform confidence bands. We propose stable weights with minimum sample variance while eliminating the sample association between the treatment and the confounding variables. The proposed weights admit a closed-form expression, allowing them to be computed efficiently in the bootstrap sampling. Under certain conditions, we derive the uniform Bahadur representation for the proposed estimator of GDRF and establish the validity of the corresponding uniform confidence bands. A fully data-driven approach to choosing the undersmooth tuning parameters and a data-driven bias-control confidence band are included. A simulation study and an application demonstrate the usefulness of the proposed approach.

</details>


### [3] [A Bayesian Prevalence Incidence Cure model for estimating survival using Electronic Health Records with incomplete baseline diagnoses](https://arxiv.org/abs/2602.00291)
*Matilda Pitt,Robert J. B. Goudie*

Main category: stat.ME

TL;DR: 提出PIC模型（Prevalence Incidence Cure model），結合PI模型與治癒模型，用於處理電子病歷中缺失基線疾病狀態且包含治癒個體的情況。


<details>
  <summary>Details</summary>
Motivation: 電子病歷數據存在複雜性，如患者基線疾病狀態缺失，且部分患者可能永遠不會經歷終點事件（治癒）。現有的PI模型在這種情況下存在偏差。

Method: 提出三成分混合模型PIC，結合PI模型框架與治癒模型，採用貝葉斯推斷方法，重點關注先驗分布的可解釋性。

Result: 模擬研究顯示PIC模型對生存概率的偏差小於PI模型；在糖尿病黃斑水腫患者數據中，PIC模型顯示出更好的擬合效果。

Conclusion: PIC模型能夠同時估計患病率、發病時間和治癒比例，並允許協變量影響這些參數，為處理電子病歷中的複雜數據提供了有效工具。

Abstract: Retrospective cohorts can be extracted from Electronic Health Records (EHR) to study prevalence, time until disease or event occurrence and cure proportion in real world scenarios. However, EHR are collected for patient care rather than research, so typically have complexities, such as patients with missing baseline disease status. Prevalence-Incidence (PI) models, which use a two-component mixture model to account for this missing data, have been proposed. However, PI models are biased in settings in which some individuals will never experience the endpoint (they are 'cured'). To address this, we propose a Prevalence Incidence Cure (PIC) model, a 3 component mixture model that combines the PI model framework with a cure model. Our PIC model enables estimation of the prevalence, time-to-incidence, and the cure proportion, and allows for covariates to affect these. We adopt a Bayesian inference approach, and focus on the interpretability of the prior. We show in a simulation study that the PIC model has smaller bias than a PI model for the survival probability; and compare inference under vague, informative and misspecified priors. We illustrate our model using a dataset of 1964 patients undergoing treatment for Diabetic Macular Oedema, demonstrating improved fit under the PIC model.

</details>


### [4] [Exchangeable random permutations with an application to Bayesian graph matching](https://arxiv.org/abs/2602.01993)
*Francesco Gaffi,Nathaniel Josephs,Lizhen Lin*

Main category: stat.ME

TL;DR: 提出基于可交换随机排列的贝叶斯图匹配框架，通过循环表示和位置感知广义中餐馆过程构建理论，开发节点分块Gibbs采样器进行后验推断，并引入perSALSO进行不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 为图匹配问题建立统一的概率框架，处理排列相关的推断问题，提供灵活的排列先验分布，并量化匹配结果的不确定性。

Method: 基于可交换随机排列理论，利用排列的循环表示和可交换随机分割文献，提出位置感知广义中餐馆过程作为构造基础。将图匹配建模为贝叶斯模型，结合相关随机块模型和新型排列先验，通过节点分块Gibbs采样器进行后验推断。

Result: 建立了可交换随机排列的理论框架，开发了实用的贝叶斯图匹配模型和推断算法，提出了perSALSO方法用于排列域的点估计和后验总结，实现了排列建模、推断和不确定性量化的统一框架。

Conclusion: 该工作为排列相关的推断问题提供了统一的概率框架，特别适用于图匹配任务，通过理论创新和算法设计实现了灵活的建模、高效推断和可靠的不确定性量化。

Abstract: We introduce a general Bayesian framework for graph matching grounded in a new theory of exchangeable random permutations. Leveraging the cycle representation of permutations and the literature on exchangeable random partitions, we define, characterize, and study the structural and predictive properties of these probabilistic objects. A novel sequential metaphor, the position-aware generalized Chinese restaurant process, provides a constructive foundation for this theory and supports practical algorithmic design. Exchangeable random permutations offer flexible priors for a wide range of inferential problems centered on permutations. As an application, we develop a Bayesian model for graph matching that integrates a correlated stochastic block model with our novel class of priors. The cycle structure of the matching is linked to latent node partitions that explain connectivity patterns, an assumption consistent with the homogeneity requirement underlying the graph matching task itself. Posterior inference is performed through a node-wise blocked Gibbs sampler directly enabled by the proposed sequential construction. To summarize posterior uncertainty, we introduce perSALSO, an adaptation of SALSO to the permutation domain that provides principled point estimation and interpretable posterior summaries. Together, these contributions establish a unified probabilistic framework for modeling, inference, and uncertainty quantification over permutations.

</details>


### [5] [Dynamic causal inference with time series data](https://arxiv.org/abs/2602.00836)
*Tanique Schaffe-Odeleye,Kōsaku Takanashi,Vishesh Karwa,Edoardo M. Airoldi,Kenichiro McAlinn*

Main category: stat.ME

TL;DR: 将潜在结果框架推广到时序干预，定义随机过程上的因果效应，提出动态平均处理效应(DATE)及其估计方法


<details>
  <summary>Details</summary>
Motivation: 传统因果推断方法主要关注静态或截面数据，但动态系统中的干预不仅改变结果水平，还改变演化动态（持久性和转移规律）。需要将因果推断扩展到时间序列，以捕捉随时间演化的因果效应。

Method: 将潜在结果定义为整个轨迹，在路径空间上直接定义因果估计量、识别条件和估计器。提出动态平均处理效应(DATE)，推导动态逆概率加权估计器，在受处理单元稀缺时使用线性状态空间表示实现动态线性模型。

Result: DATE能表征因果效应随时间演化，在单期时简化为经典平均处理效应。动态逆概率加权估计器在动态可忽略性和正性条件下是无偏的。模拟显示建模时间作为因果机制内在部分能暴露静态方法系统误估的动态效应。

Conclusion: 该框架为时间序列干预提供了统一的因果推断方法，能估计和分解随时间演化的处理效应，在COVID-19封锁等实证研究中具有实用价值。

Abstract: We generalize the potential outcome framework to time series with an intervention by defining causal effects on stochastic processes. Interventions in dynamic systems alter not only outcome levels but also evolutionary dynamics -- changing persistence and transition laws. Our framework treats potential outcomes as entire trajectories, enabling causal estimands, identification conditions, and estimators to be formulated directly on path space. The resulting Dynamic Average Treatment Effect (DATE) characterizes how causal effects evolve through time and reduces to the classical average treatment effect under one period of time. For observational data, we derive a dynamic inverse-probability weighting estimator that is unbiased under dynamic ignorability and positivity. When treated units are scarce, we show that conditional mean trajectories underlying the DATE admit a linear state-space representation, yielding a dynamic linear model implementation. Simulations demonstrate that modeling time as intrinsic to the causal mechanism exposes dynamic effects that static methods systematically misestimate. An empirical study of COVID-19 lockdowns illustrates the framework's practical value for estimating and decomposing treatment effects.

</details>


### [6] [Posterior Uncertainty for Targeted Parameters in Bayesian Bootstrap Procedures](https://arxiv.org/abs/2602.02216)
*Magid Sabbagh,David A. Stephens*

Main category: stat.ME

TL;DR: 提出一种针对有限维目标参数在存在有限维干扰参数时的有效贝叶斯分析方法，应用于基于估计方程的因果推断，是"链接贝叶斯自助法"的推广版本。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯因果推断依赖"似然×先验"框架，但"链接贝叶斯自助法"偏离这一经典设置，使用狄利克雷过程和贝叶斯自助法获得有效贝叶斯推断。需要处理倾向得分调整及其不确定性，并确保后验分布具有良好的频率性质。

Method: 提出"链接贝叶斯自助法"的推广版本，用于处理有限维目标参数和干扰参数的贝叶斯分析。方法基于估计方程，通过倾向得分调整处理因果推断中的不确定性，使用狄利克雷过程和贝叶斯自助法框架。

Result: 理论分析显示所提方法的后验分布具有理想的频率性质，置信区间具有渐近正确的覆盖性质。方法可应用于误设模型和单稳健模型的因果推断问题。

Conclusion: 提出的广义"链接贝叶斯自助法"为存在干扰参数时的目标参数提供了有效的贝叶斯分析框架，在因果推断中具有理论保证的良好频率性质，扩展了传统贝叶斯方法的适用范围。

Abstract: We propose a general method to carry out a valid Bayesian analysis of a finite-dimensional `targeted' parameter in the presence of a finite-dimensional nuisance parameter. We apply our methods to causal inference based on estimating equations. While much of the literature in Bayesian causal inference has relied on the conventional 'likelihood times prior' framework, a recently proposed method, the 'Linked Bayesian Bootstrap', deviated from this classical setting to obtain valid Bayesian inference using the Dirichlet process and the Bayesian bootstrap. These methods rely on an adjustment based on the propensity score and explain how to handle the uncertainty concerning it when studying the posterior distribution of a treatment effect. We examine theoretically the asymptotic properties of the posterior distribution obtained and show that our proposed method, a generalized version of the 'Linked Bayesian Bootstrap', enjoys desirable frequentist properties. In addition, we show that the credible intervals have asymptotically the correct coverage properties. We discuss the applications of our method to mis-specified and singly-robust models in causal inference.

</details>


### [7] [A Graph-based Framework for Coverage Analysis in Autonomous Driving](https://arxiv.org/abs/2602.00903)
*Thomas Muehlenstädt,Marius Bause*

Main category: stat.ME

TL;DR: 提出基于图的覆盖分析框架，将交通场景表示为分层图，结合地图拓扑和参与者关系，通过子图同构和图嵌入两种方法评估自动驾驶系统安全性。


<details>
  <summary>Details</summary>
Motivation: 现有覆盖分析方法通常单独或有限组合评估覆盖因素，难以捕捉交通场景中复杂的交互关系，需要更系统的方法来验证自动驾驶系统的安全性。

Method: 提出基于图的框架，将交通场景表示为分层图。采用两阶段图构建算法系统捕捉交通参与者空间关系。提供两种互补方法：1）子图同构匹配预定义原型图；2）使用带边特征的图同构网络（GINE）通过自监督对比学习进行图嵌入。

Result: 在Argoverse 2.0真实数据和CARLA合成数据上验证。子图同构方法能计算节点覆盖百分比，嵌入方法在潜在空间中显示有意义的聚类结构，适合异常检测。方法能高效扩展到多样交通场景，无需场景特定处理，自然适应不同参与者数量。

Conclusion: 提出的基于图的覆盖分析框架相比传统方法具有显著优势，能系统捕捉交通场景复杂交互，为自动驾驶系统安全性验证提供更全面的覆盖分析工具。

Abstract: Coverage analysis is essential for validating the safety of autonomous driving systems, yet existing approaches typically assess coverage factors individually or in limited combinations, struggling to capture the complex interactions inherent in traffic scenes. This paper proposes a graph-based framework for coverage analysis that represents traffic scenes as hierarchical graphs, combining map topology with actor relationships. The framework introduces a two-phase graph construction algorithm that systematically captures spatial relationships between traffic participants, including leading, following, neighboring, and opposing configurations. Two complementary coverage analysis methods are presented. First, a sub-graph isomorphism approach matches traffic scenes against a set of manually defined archetype graphs representing common driving scenarios. Second, a graph embedding approach utilizes Graph Isomorphism Networks with Edge features (GINE) trained via self-supervised contrastive learning to project traffic scenes into a vector space, enabling similarity-based coverage assessment. The framework is validated on both real-world data from the Argoverse 2.0 dataset and synthetic data from the CARLA simulator. The subgraph isomorphism method is used to calculate node coverage percentages using predefined archetypes, while the embedding approach reveals meaningful structure in the latent space suitable for clustering and anomaly detection. The proposed approach offers significant advantages over traditional methods by scaling efficiently to diverse traffic scenarios without requiring scenario-specific handling, and by naturally accommodating varying numbers of actors in a scene.

</details>


### [8] [Estimation of Tsallis entropy and its applications to goodness-of-fit tests](https://arxiv.org/abs/2602.01228)
*Siddhartha Chakraborty,Asok K. Nanda,Narayanaswamy Balakrishnan*

Main category: stat.ME

TL;DR: 本文提出了基于高阶样本间距的Tsallis熵估计器，包括渐进II型截尾下的估计器，并开发了基于Tsallis散度的正态和指数分布拟合优度检验。


<details>
  <summary>Details</summary>
Motivation: 需要从数据集中估计Tsallis熵，并开发相关的统计推断方法，特别是在存在异常值和截尾数据的情况下。

Method: 提出了四种基于高阶样本间距的Tsallis熵估计器，包括渐进II型截尾下的间距估计器和基于分位数函数的估计器，并利用Tsallis散度开发了正态和指数分布的拟合优度检验。

Result: 通过偏差和均方误差比较，提出的估计器表现良好且对异常值具有鲁棒性。基于Tsallis散度的检验在模拟中优于现有方法，并在实际数据分析中得到验证。

Conclusion: 提出的Tsallis熵估计器和基于Tsallis散度的检验方法在理论和实际应用中均表现优异，为统计推断提供了有效的工具。

Abstract: In this paper, we consider the problem of estimating Tsallis entropy from a given data set. We propose four different estimators for Tsallis entropy measure based on higher-order sample spacings, and then discuss estimation of Tsallis divergence measure. We compare the performance of the proposed estimators by means of bias and mean squared error and also examine their robustness to outliers. Next, we propose a spacings-based estimator for Tsallis entropy under progressive type-II censoring and study its performance using Monte Carlo simulations. Another estimator for Tsallis entropy is proposed using quantile function and its consistency and asymptotic normality are studied, and its performance is evaluated through Monte Carlo simulations. Goodness-of-fit tests for normal and exponential distributions as applications are developed using Tsallis divergence measure. The performance of the proposed tests are then compared with some known tests using simulations and it is shown that the proposed tests perform very well. Also, an exponentiality test under progressive type-II censoring is proposed, its performance is compared with existing entropy-based tests using simulation. It is observed that the proposed test performs well. Finally, some real data sets are analysed for illustrative purposes.

</details>


### [9] [Explicit Expressions for Multidimensional Value-at-Risk under Archimedean Copulas](https://arxiv.org/abs/2602.01245)
*Dotamana Yéo,Saralees Nadarajah,Amadou Sawadogo*

Main category: stat.ME

TL;DR: 该论文研究了金融投资组合的多变量风险价值(VaR)，通过阿基米德连接函数建模依赖结构，推导出任意维度下边际下尾多变量VaR的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖数值或模拟方法计算多变量VaR，缺乏解析解。论文旨在为理论研究和应用风险分析提供可处理的解析替代方案，提高多变量风险测量的透明度。

Method: 利用阿基米德连接函数的生成元表示，推导出边际下尾多变量VaR的显式解析表达式。针对Clayton、Frank、Gumbel-Hougaard、Joe和Ali-Mikhail-Haq等常用连接函数族获得闭式公式，并通过蒙特卡洛模拟评估有限样本性能。

Result: 获得了多个常用连接函数族的闭式解析公式，可以直接评估依赖结构对多变量风险的影响。蒙特卡洛模拟验证了所提VaR估计器的有限样本性能，并展示了不同依赖结构的作用。

Conclusion: 提出的解析框架为多变量风险测量和系统性风险评估提供了透明工具，补充了现有的数值方法，使理论分析和实际应用更加便捷。

Abstract: This paper studies multivariate Value-at-Risk (VaR) for financial portfolios with a focus on modeling dependence structures through Archimedean copulas. Using the generator representation of Archimedean copulas, we derive explicit analytical expressions for the marginal lower-tail multivariate VaR in arbitrary dimensions.
  Closed-form formulas are obtained for several commonly used copula families, including Clayton, Frank, Gumbel-Hougaard, Joe and Ali--Mikhail--Haq copulas, allowing a direct assessment of the impact of dependence on multivariate risk. These results complement existing approaches, which largely rely on numerical or simulation-based methods, by providing tractable alternatives for theoretical and applied risk analysis.
  Monte Carlo simulations are conducted to evaluate the finite-sample performance of the proposed VaR estimator and to illustrate the role of different dependence structures. The proposed analytical setting offers transparent tools for multivariate risk measurement and systemic risk assessment.

</details>


### [10] [A Fractional M/M/1 Queue Governed by Stretched Non-Local Time Operators](https://arxiv.org/abs/2602.01366)
*Mehmet Sıddık Çadırcı*

Main category: stat.ME

TL;DR: 该论文提出了一个非马尔可夫M/M/1队列的推广模型，通过引入扩展的Caputo型算子来纳入非局部时间动力学，保留了经典队列的生死转移结构，同时引入了记忆效应。


<details>
  <summary>Details</summary>
Motivation: 经典M/M/1队列基于马尔可夫假设，缺乏对具有记忆效应的现实排队系统的建模能力。作者旨在开发一个能够捕捉长期记忆尾动态的分数阶排队模型。

Method: 将标准时间导数替换为扩展的Caputo型算子，使用拉普拉斯变换技术推导瞬态状态概率的显式表示（用Kilbas-Saigo函数表示），并构建时间变化解释，将分数阶队列视为经典M/M/1过程在非递减随机时间上的分布。

Result: 在稳定性条件ρ<1下，稳态分布保持几何分布并与经典队列一致，但分数阶参数显著影响瞬态收敛速率。蒙特卡洛模拟显示参数(α,γ)对空状态分布、尾长分布和平均尾演化的影响，验证了框架在捕捉长期记忆尾动态方面的灵活性。

Conclusion: 提出的分数阶排队模型成功地将记忆效应纳入经典M/M/1队列，同时保持其稳态特性，为建模具有长期记忆的排队系统提供了灵活的理论框架。

Abstract: We introduce a non-Markovian generalization of the classical M/M/1 queue by incorporating extended nonlocal time dynamics into Kolmogorov forward equations. We obtain the model by replacing the standard time derivative with an extended Caputo-type operator. It preserves the birth-death transition structure of the standard queue while introducing memory effects into the temporal evolution. We derive explicit representations for transient state probabilities in terms of the Kilbas-Saigo function, which naturally emerges as the relaxation kernel associated with the stretched operator, using Laplace transform techniques. We construct a time-varying interpretation and show that the fractional queue can be viewed as a distribution of a classical M/M/1 process evaluated at a non-decreasing random time. It is observed that the fractional queue can be viewed as a distribution of a classical M/M/1 process evaluated at a non-decreasing random time. We prove that under the standard stability condition $ρ<1$, the steady-state distribution remains geometric and coincides with the distribution of the classical queue, whilst we prove that the stretched fractional parameters significantly affect the convergence rate in the transient regime. Numerical examples based on Monte Carlo simulations highlight the effect of the parameters $(α,γ)$ on the distribution of empty states, tail length distributions, and the average tail evolution, and validate the flexibility of the proposed framework in capturing long-memory tail dynamics.

</details>


### [11] [When Is Generalized Bayes Bayesian? A Decision-Theoretic Characterization of Loss-Based Updating](https://arxiv.org/abs/2602.01573)
*Kenichiro McAlinn,Kōsaku Takanashi*

Main category: stat.ME

TL;DR: 论文区分了信念后验和决策后验，指出损失函数后验只有在损失函数为负对数似然时才等同于普通贝叶斯，并证明在决策后验体系中需要非线性偏好才能产生非退化后验。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为基于损失的更新方法（包括广义贝叶斯、吉布斯和拟后验）提供决策理论基础，明确区分两种不同类型的后验分布，并澄清它们在统计推断中的理论基础和适用条件。

Method: 采用决策理论框架，基于Savage和Anscombe-Aumann的公理体系，通过偏好关系分析决策规则，建立损失函数后验与普通贝叶斯后验的等价条件，并探讨非线性偏好对后验分布的影响。

Result: 证明了损失函数后验等同于普通贝叶斯后验当且仅当损失函数（除尺度因子和数据相关项外）为负对数似然；在决策后验体系中，非退化后验需要非线性偏好，且广义边际似然不能作为决策后验的证据。

Conclusion: 损失函数后验可分为信念后验和决策后验两类，它们有不同的理论基础和性质。在决策后验框架下，广义贝叶斯可通过熵惩罚变分表示作为最优决策规则，但需要非线性偏好来保证后验的非退化性。

Abstract: Loss-based updating, including generalized Bayes, Gibbs, and quasi-posteriors, replaces likelihoods by a user-chosen loss and produces a posterior-like distribution via exponential tilt. We give a decision-theoretic characterization that separates \emph{belief posteriors} --  conditional beliefs justified by the foundations of Savage and Anscombe-Aumann under a joint probability mode l-- from \emph{decision posteriors} -- randomized decision rules justified by preferences over decision rules. We make explicit that a loss-based posterior coincides with ordinary Bayes if and only if the loss is, up to scale and a data-only term, negative log-likelihood. We then show that generalized marginal likelihood is not evidence for decision posteriors, and Bayes factors are not well-defined without additional structure. In the decision posterior regime, non-degenerate posteriors require nonlinear preferences over decision rules. Under sequential coherence and separability, these lead to an entropy-penalized variational representation yielding generalized Bayes as the optimal rule.

</details>


### [12] [Difference-in-Differences under Local Dependence on Networks](https://arxiv.org/abs/2602.01631)
*Akihiro Sato,Shonosuke Sugasawa*

Main category: stat.ME

TL;DR: 本文提出了一种在网络干扰下识别直接和间接平均处理效应的非参数方法，基于双重差分框架，开发了逆概率加权和双重稳健估计器。


<details>
  <summary>Details</summary>
Motivation: 在区域经济学和公共经济学等领域，存在干扰（即稳定单位处理值假设被违反）时估计因果效应至关重要。现有研究大多依赖预设的"暴露映射"，本文旨在解决这一问题。

Method: 基于条件邻域处理向量的平行趋势假设，提出了非参数识别策略，开发了逆概率加权估计器和双重稳健估计器，并建立了它们的渐近性质。

Result: 在模拟研究和实证应用中证明了所提方法的有效性，即使在干扰模型设定错误的情况下，估计器在特定正则条件下仍能保持一致性。

Conclusion: 本文为网络干扰下的因果推断提供了一种有效的双重差分方法，能够识别直接和间接处理效应，特别是提出了衡量干预向外总影响的间接效应新概念。

Abstract: Estimating causal effects under interference, where the stable unit treatment value assumption is violated, is critical in fields such as regional and public economics. Much of the existing research on causal inference under interference relies on a pre-specified "exposure mapping". This paper focuses on difference-in-difference and proposes a nonparametric identification strategy for direct and indirect average treatment effects under local interference on an observed network. In particular, we proposed a new concept of an indirect effect measuring the total outward influence of the intervension. Based on parallel trends assumption conditional on the neighborhood treatment vector, we develop inverse probability weighted and doubly robust estimators. We establish their asymptotic properties, including consistency under misspecification of nuisance models under some regularity conditions. Simulation studies and an empirical application demonstrate the effectiveness of the proposed method.

</details>


### [13] [Demystify Doubly-Robust Estimation: The Role of Overlap](https://arxiv.org/abs/2602.01648)
*Chengxin Yang,Laine E. Thomas,Fan Li*

Main category: stat.ME

TL;DR: 双重稳健估计器在协变量重叠差时表现不佳，甚至不如IPW或结果模型估计器，建议应用时先检查重叠程度，重叠差时考虑调整目标人群。


<details>
  <summary>Details</summary>
Motivation: 双重稳健估计器在因果推断中被认为比IPW或结果模型更稳健，但这是渐近性质，有限样本下表现可能不同。研究旨在探索DR估计器在有限样本下的表现如何受协变量重叠程度影响。

Method: 通过理论分析和大量模拟实验，在不同协变量重叠程度和模型设定场景下，比较DR估计器与IPW、结果模型估计器的偏差和方差。

Result: 发现：(1) 结果模型设定对DR估计的影响比倾向得分模型更强，且重叠越差这种主导作用越明显；(2) 重叠差时，DR估计器通常会放大极端权重的不良后果（大偏差和/或方差），往往不如IPW和结果模型估计器。

Conclusion: 实际应用中应首先检查协变量重叠程度。如果重叠差，建议通过修剪或重叠加权等方法将目标人群调整到有足够重叠的子人群。

Abstract: The doubly-robust (DR) estimator is popular for evaluating causal effects in observational studies and is often perceived as more desirable than inverse probability weighting (IPW) or outcome modeling alone because it provides extra protection against model misspecification. However, double robustness is an asymptotic property that may not hold in finite samples. We investigate how the finite sample performance of the DR estimator depends on the degree of covariate overlap between comparison groups. Using analytical illustrations and extensive simulations under various scenarios with different degrees of covariate overlap and model specifications, we examine the bias and variance of the DR estimator relative to IPW and outcome modeling estimators. We find that: (i) specification of the outcome model has a stronger influence on the DR estimates than specification of the propensity score model, and this dominance increases as overlap decreases; (ii) with poor overlap, the DR estimator generally amplifies the adverse consequences of extreme weights (large bias and/or variance) regardless of model specifications, and is often inferior to both the IPW and outcome modeling estimators. As a practical guide, we recommend always first checking the degree of overlap in applications. In the case of poor overlap, analysts should consider shifting the target population to a subpopulation with adequate overlap via methods such as trimming or overlap weighting.

</details>


### [14] [Locally sparse estimation for simultaneous functional quantile regression](https://arxiv.org/abs/2602.01691)
*Boyi Hu,Jiguo Cao*

Main category: stat.ME

TL;DR: 本文提出了一种同时处理多个分位数的函数分位数回归模型，该模型具有局部稀疏的双变量斜率函数，能够识别功能预测变量对响应变量无影响的时段，提高了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究日温度对大豆产量的影响，传统函数分位数回归模型一次只能拟合一个分位数且存在多种限制，需要一种能够同时处理多个分位数并能识别预测变量无影响时段的方法。

Method: 提出同时函数分位数回归模型，具有局部稀疏的双变量斜率函数，该函数在分位数和时间两个维度上变化，且仅在特定域段非零，其余区域为零。这些零斜率区域随分位数变化，表示功能预测变量对响应变量无影响的时段。

Result: 通过模拟研究验证了新方法的明显优势，并在大豆产量数据应用中成功识别出日温度对产量无影响的时间段，为农业规划和作物管理提供了重要见解。

Conclusion: 提出的同时函数分位数回归模型能够同时处理多个分位数，通过局部稀疏的斜率函数提高了模型的可解释性，并能识别预测变量无影响的时段，在理论和实际应用中都具有重要价值。

Abstract: Motivated by the study of how daily temperature affects soybean yield, this article proposes a simultaneous functional quantile regression (FQR) model featuring a locally sparse bivariate slope function indexed by both quantile and time and linked to a functional predictor. The slope function's local sparsity means it holds non-zero values only in certain segments of its domain, remaining zero elsewhere. These zero-slope regions, which vary by quantile, indicate times when the functional predictor has no discernible impact on the response variable. This feature boosts the model's interpretability. Unlike traditional FQR models, which fit one quantile at a time and have several limitations, our proposed method can handle a spectrum of quantiles simultaneously. We tested the new approach through simulation studies, demonstrating its clear advantages over standard techniques. To validate its practical use, we applied the method to soybean yield data, pinpointing the time periods when daily temperature doesn't affect yield. This insight could be crucial for agricultural planning and crop management.

</details>


### [15] [Learning Sequential Decisions from Multiple Sources via Group-Robust Markov Decision Processes](https://arxiv.org/abs/2602.01825)
*Mingyuan Xu,Zongqi Xia,Tianxi Cai,Doudou Zhou,Nian Si*

Main category: stat.ME

TL;DR: 从异构多站点离线数据中学习鲁棒顺序决策策略，使用具有组线性结构的分布鲁棒MDP，提出特征级不确定性集和悲观值迭代算法


<details>
  <summary>Details</summary>
Motivation: 现实世界中经常从多个站点（如医院）收集数据，这些数据具有共同结构但存在异质性。需要从这种离线、多站点数据集中学习鲁棒的顺序决策策略，以应对跨站点的不确定性。

Method: 1. 使用具有组线性结构的分布鲁棒MDP：所有站点共享共同特征映射，转移核和期望奖励函数在这些共享特征上是线性的。2. 引入特征级（d-矩形）不确定性集，保持可处理的鲁棒Bellman递归。3. 开发基于悲观值迭代的离线算法：包括站点级岭回归、特征级最坏情况聚合、基于逆设计矩阵对角线的数据依赖悲观惩罚。4. 提出聚类级扩展，通过先验站点相似性知识池化相似站点。

Result: 在鲁棒部分覆盖假设下，证明了所得策略的次优性边界。算法能够处理多站点异构数据源，提供鲁棒规划的原则性方法。

Conclusion: 该框架解决了多站点学习中的异质数据源问题，提供了不依赖强状态-动作矩形假设的鲁棒规划原则方法，通过特征级不确定性集和聚类扩展提高了样本效率。

Abstract: We often collect data from multiple sites (e.g., hospitals) that share common structure but also exhibit heterogeneity. This paper aims to learn robust sequential decision-making policies from such offline, multi-site datasets. To model cross-site uncertainty, we study distributionally robust MDPs with a group-linear structure: all sites share a common feature map, and both the transition kernels and expected reward functions are linear in these shared features. We introduce feature-wise (d-rectangular) uncertainty sets, which preserve tractable robust Bellman recursions while maintaining key cross-site structure. Building on this, we then develop an offline algorithm based on pessimistic value iteration that includes: (i) per-site ridge regression for Bellman targets, (ii) feature-wise worst-case (row-wise minimization) aggregation, and (iii) a data-dependent pessimism penalty computed from the diagonals of the inverse design matrices. We further propose a cluster-level extension that pools similar sites to improve sample efficiency, guided by prior knowledge of site similarity. Under a robust partial coverage assumption, we prove a suboptimality bound for the resulting policy. Overall, our framework addresses multi-site learning with heterogeneous data sources and provides a principled approach to robust planning without relying on strong state-action rectangularity assumptions.

</details>


### [16] [Neural Network Machine Regression (NNMR): A Deep Learning Framework for Uncovering High-order Synergistic Effects](https://arxiv.org/abs/2602.02172)
*Jiuchen Zhang,Ling Zhou,Peter Song*

Main category: stat.ME

TL;DR: 提出NNMR框架，通过可训练输入门控和自适应深度正则化联合进行特征选择和函数估计，支持后选择推断


<details>
  <summary>Details</summary>
Motivation: 现有方法在高维特征空间中难以同时实现高效的特征选择、复杂非线性关系建模和有效的统计推断

Method: NNMR框架整合可训练输入门控（特征选择）和自适应深度正则化（冗余层惩罚），并开发基于分割样本、置换假设检验的后选择推断程序

Result: 相比贝叶斯核机器回归等现有方法，NNMR在高维特征空间中扩展效率更高，能严格控制I型错误，模拟研究显示其选择准确性和推断可靠性更优

Conclusion: NNMR提供了一种端到端的稀疏可解释架构，能捕捉高阶协同效应的复杂非线性关系，在墨西哥城青少年生长研究中发现了具有生物学意义的稀疏食物组预测因子

Abstract: We propose a new neural network framework, termed Neural Network Machine Regression (NNMR), which integrates trainable input gating and adaptive depth regularization to jointly perform feature selection and function estimation in an end-to-end manner. By penalizing both gating parameters and redundant layers, NNMR yields sparse and interpretable architectures while capturing complex nonlinear relationships driven by high-order synergistic effects. We further develop a post-selection inference procedure based on split-sample, permutation-based hypothesis testing, enabling valid inference without restrictive parametric assumptions. Compared with existing methods, including Bayesian kernel machine regression and widely used post hoc attribution techniques, NNMR scales efficiently to high-dimensional feature spaces while rigorously controlling type I error. Simulation studies demonstrate its superior selection accuracy and inference reliability. Finally, an empirical application reveals sparse, biologically meaningful food group predictors associated with somatic growth among adolescents living in Mexico City.

</details>


### [17] [Causal Inference for Preprocessed Outcomes with an Application to Functional Connectivity](https://arxiv.org/abs/2602.02240)
*Zihang Wang,Razieh Nabi,Benjamin B. Risk*

Main category: stat.ME

TL;DR: 提出一个半参数框架，用于处理经过受试者内处理得到的衍生结果进行因果推断，特别适用于模块化结构的研究设计。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究中，受试者内重复测量数据常被处理以去除伪影和变异，得到的衍生结果作为不可直接观测科学结果的代理。尽管广泛使用，但受试者内处理对受试者间统计推断的影响尚未系统研究，缺乏该场景下的因果分析原则框架。

Method: 提出半参数因果推断框架，适用于模块化结构（受试者内分析独立进行，然后基于受试者内参数进行受试者间分析）。开发多重稳健估计器，允许使用灵活机器学习方法。专门化到中介分析场景，聚焦自然直接效应。高维推断采用控制错误发现比例超限率的逐步下降程序。

Result: 模拟研究显示所提方法性能优越。应用于估计兴奋剂药物对自闭症谱系障碍儿童大脑连接性的影响。

Conclusion: 为经过受试者内处理得到的衍生结果的因果推断提供了系统框架，填补了该重要研究领域的空白，支持在模块化研究设计中应用灵活机器学习方法进行稳健因果推断。

Abstract: In biomedical research, repeated measurements within each subject are often processed to remove artifacts and unwanted sources of variation. The resulting data are used to construct derived outcomes that act as proxies for scientific outcomes that are not directly observable. Although intra-subject processing is widely used, its impact on inter-subject statistical inference has not been systematically studied, and a principled framework for causal analysis in this setting is lacking. In this article, we propose a semiparametric framework for causal inference with derived outcomes obtained after intra-subject processing. This framework applies to settings with a modular structure, where intra-subject analyses are conducted independently across subjects and are followed by inter-subject analyses based on parameters from the intra-subject stage. We develop multiply robust estimators of causal parameters under rate conditions on both intra-subject and inter-subject models, which allows the use of flexible machine learning. We specialize the framework to a mediation setting and focus on the natural direct effect. For high dimensional inference, we employ a step-down procedure that controls the exceedance rate of the false discovery proportion. Simulation studies demonstrate the superior performance of the proposed approach. We apply our method to estimate the impact of stimulant medication on brain connectivity in children with autism spectrum disorder.

</details>


### [18] [Cumulative Treatment Effect Testing under Continuous Time Reinforcement Learning](https://arxiv.org/abs/2602.02246)
*Jiuchen Zhang,Annie Qu*

Main category: stat.ME

TL;DR: 提出一种基于连续时间强化学习框架的检验方法，用于测试治疗随时间的影响，特别关注治疗的滞后效应，在非规则观测时间设置中优于离散时间方法。


<details>
  <summary>Details</summary>
Motivation: 理解治疗效应随时间的影响是许多科学和医学研究的基本方面。现有方法在非规则观测时间设置中表现有限，需要更准确的方法来检验治疗的滞后效应。

Method: 在连续时间强化学习框架下，利用平均治疗效应（ATE）检验治疗的滞后效应。ATE定义为无限时间范围内价值函数的差异，考虑了累积治疗效应（包括即时和滞后效应）。通过连续时间估计价值函数，能够捕捉较短持续时间的治疗效应。

Result: 该方法在非规则观测时间的多分辨率观测设置中优于现有的离散时间强化学习策略。建立了检验统计量的渐近正态性，并在OhioT1DM糖尿病数据上成功应用，评估了推注胰岛素对患者血糖水平的累积治疗效应。

Conclusion: 提出的连续时间强化学习方法为检验治疗随时间的影响提供了有效工具，特别适用于非规则观测时间设置，能够更准确地捕捉治疗效应，包括短期效应和滞后效应。

Abstract: Understanding the impact of treatment effect over time is a fundamental aspect of many scientific and medical studies. In this paper, we introduce a novel approach under a continuous-time reinforcement learning framework for testing a treatment effect. Specifically, our method provides an effective test on carryover effects of treatment over time utilizing the average treatment effect (ATE). The average treatment effect is defined as difference of value functions over an infinite horizon, which accounts for cumulative treatment effects, both immediate and carryover. The proposed method outperforms existing testing procedures such as discrete time reinforcement learning strategies in multi-resolution observation settings where observation times can be irregular. Another advantage of the proposed method is that it can capture treatment effects of a shorter duration and provide greater accuracy compared to discrete-time approximations, through the use of continuous-time estimation for the value function. We establish the asymptotic normality of the proposed test statistics and apply it to OhioT1DM diabetes data to evaluate the cumulative treatment effects of bolus insulin on patients' glucose levels.

</details>


### [19] [Nonparametric Inference with an Instrumental Variable under a Separable Binary Treatment Choice Model](https://arxiv.org/abs/2602.02265)
*Chan Park,Eric Tchetgen Tchetgen*

Main category: stat.ME

TL;DR: 本文提出了一种新的非参数高效估计方法，用于在可分离二元处理选择模型下进行工具变量分析，解决了传统方法中由于反事实量定义导致的变分依赖问题。


<details>
  <summary>Details</summary>
Motivation: 工具变量方法在存在未测量混杂因素时被广泛用于推断处理效应。然而，在可分离二元处理选择模型下，尽管已经建立了对处理组中处理自由潜在结果的平滑泛函的非参数识别，但由于反事实量定义的变分依赖问题，相应的非参数高效估计一直难以实现。

Method: 引入基于观测数据直接定义的变分独立参数化方法，结合新颖的不动点论证，利用现代机器学习方法进行干扰函数估计。该方法能够表征处理组中处理自由潜在结果的任何平滑泛函的半参数效率界，并构建相应的半参数高效估计器。

Result: 建立了非参数高效估计框架，能够在不强加不必要限制的情况下进行干扰函数估计。该方法可无缝扩展到非线性处理效应、总体水平效应和非可忽略缺失数据设置。通过模拟研究和Job Corps研究的应用验证了方法的有效性。

Conclusion: 本文提出的新参数化方法和不动点论证解决了工具变量分析中的变分依赖问题，实现了非参数高效估计，为实际应用提供了实用的统计推断框架，并可通过经验可证伪的隐含条件来评估假设的合理性。

Abstract: Instrumental variable (IV) methods are widely used to infer treatment effects in the presence of unmeasured confounding. In this paper, we study nonparametric inference with an IV under a separable binary treatment choice model, which posits that the odds of the probability of taking the treatment, conditional on the instrument and the treatment-free potential outcome, factor into separable components for each variable. While nonparametric identification of smooth functionals of the treatment-free potential outcome among the treated, such as the average treatment effect on the treated, has been established under this model, corresponding nonparametric efficient estimation has proven elusive due to variationally dependent nuisance parameters defined in terms of counterfactual quantities. To address this challenge, we introduce a new variationally independent parameterization based on nuisance functions defined directly from the observed data. This parameterization, coupled with a novel fixed-point argument, enables the use of modern machine learning methods for nuisance function estimation. We characterize the semiparametric efficiency bound for any smooth functional of the treatment-free potential outcome among the treated and construct a corresponding semiparametric efficient estimator without imposing any unnecessary restriction on nuisance functions. Furthermore, we describe a straightforward generative model justifying our identifying assumptions and characterize empirically falsifiable implications of the framework to evaluate our assumptions in practical settings. Our approach seamlessly extends to nonlinear treatment effects, population-level effects, and nonignorable missing data settings. We illustrate our methods through simulation studies and an application to the Job Corps study.

</details>


### [20] [A spatial random forest algorithm for population-level epidemiological risk assessment](https://arxiv.org/abs/2602.02277)
*Duncan Lee,Vinny Davies*

Main category: stat.ME

TL;DR: 本文提出了SPAR-Forest-ERF算法，首次将随机森林（用于捕捉非线性交互效应）与贝叶斯空间自相关模型（用于估计可解释的暴露响应函数）相结合，以改进空间流行病学研究中的暴露风险评估。


<details>
  <summary>Details</summary>
Motivation: 传统空间流行病学使用泊松回归模型，需要预先指定混杂因素的交互作用和功能关系，而不是从数据中学习。这限制了模型捕捉非线性效应和复杂交互作用的能力。研究旨在开发更灵活的方法来评估空气污染对健康的影响。

Method: 提出SPAR-Forest-ERF算法，融合随机森林（捕捉非线性混杂因素-响应效应）和贝叶斯空间自相关模型（估计可解释的暴露响应函数）。方法包括：1）在ML和统计模型之间传播不确定性；2）开发新的停止准则确保推断目标稳定性；3）整合多种ERF以最大化模型灵活性。

Result: 该方法应用于苏格兰2022年全国人口普查数据，量化空气污染浓度对自评健康的影响。算法能够从数据中学习混杂因素的交互作用和功能关系，并提供完整的暴露响应函数不确定性量化。

Conclusion: SPAR-Forest-ERF算法是首个将随机森林与贝叶斯空间模型融合的方法，能够更好地捕捉非线性效应和复杂交互作用，为空间流行病学研究提供了更灵活、可解释的暴露风险评估框架。

Abstract: Spatial epidemiology identifies the drivers of elevated population-level disease risks, using disease counts, exposures and known confounders at the areal unit level. Poisson regression models are typically used for inference, which incorporate a linear/additive regression component and allow for unmeasured confounding via a set of spatially autocorrelated random effects. This approach requires the confounder interactions and their functional relationships with disease risk to be specified in advance, rather than being learned from the data. Therefore, this paper proposes the SPAR-Forest-ERF algorithm, which is the first fusion of random forests for capturing non-linear and interacting confounder-response effects with Bayesian spatial autocorrelation models that can estimate interpretable exposure response functions (ERF) with full uncertainty quantification. Methodologically, we extend existing methods set in a prediction context by propagating uncertainty between both the ML and statistical models, developing a new stopping criteria designed to ensure the stability of the primary inferential target, and incorporating a range of different ERFs for maximum model flexibility. This methodology is motivated by a new study quantifying the impact of air pollution concentrations on self-rated health in Scotland, using data from the recently released 2022 national census.

</details>


### [21] [Leave-One-Out Neighborhood Smoothing for Graphons: Berry-Esseen Bounds, Confidence Intervals, and Honest Tuning](https://arxiv.org/abs/2602.02319)
*Behzad Aalipur,Rachel Kilby*

Main category: stat.ME

TL;DR: 提出一种留一法邻域平滑方法，用于无向简单图的边概率估计和统计推断，解决了传统方法因数据驱动邻域选择导致的依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 传统邻域平滑方法在图模型边概率估计中能达到极小极大最优率，但由于使用相同邻接矩阵进行邻域选择和边平均，导致复杂依赖关系，使得统计推断受限。需要一种能恢复条件独立性、支持有效推断的方法。

Method: 提出留一法邻域平滑：在估计单个条目P_ij时，从第j行和第j列置零的邻接矩阵构建节点i的邻域，从而将邻域选择与待平均边解耦。这种构造恢复了中心化项的条件独立性。

Result: 在分段Lipschitz图模型假设和对数度增长条件下，基于Bousquet不等式推导了方差自适应集中不等式，建立了归一化估计误差的Berry-Esseen界限。该方法保持了传统邻域平滑的最优行均方误差率，同时提供了有效的逐点不确定性量化。

Conclusion: 留一法邻域平滑方法解决了传统邻域平滑的统计推断障碍，恢复了条件独立性，支持有限样本和渐近置信区间构建，同时支持诚实的交叉验证调参选择，实现了最优估计率与有效推断的统一。

Abstract: Neighborhood smoothing methods achieve minimax-optimal rates for estimating edge probabilities under graphon models, but their use for statistical inference has remained limited. The main obstacle is that classical neighborhood smoothers select data-driven neighborhoods and average edges using the same adjacency matrix, inducing complex dependencies that invalidate standard concentration and normal approximation arguments.
  We introduce a leave-one-out modification of neighborhood smoothing for undirected simple graphs. When estimating a single entry P_ij, the neighborhood of node i is constructed from an adjacency matrix in which the jth row and column are set to zero, thereby decoupling neighborhood selection from the edges being averaged. We show that this construction restores conditional independence of the centered summands, enabling the use of classical probabilistic tools for inference.
  Under piecewise Lipschitz graphon assumptions and logarithmic degree growth, we derive variance-adaptive concentration inequalities based on Bousquet's inequality and establish Berry-Esseen bounds with explicit rates for the normalized estimation error. These results yield both finite-sample and asymptotic confidence intervals for individual edge probabilities. The same leave-one-out structure also supports an honest cross-validation scheme for tuning parameter selection, for which we prove an oracle inequality. The proposed estimator retains the optimal row-wise mean-squared error rates of classical neighborhood smoothing while providing valid entrywise uncertainty quantification.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [22] [Benchmarking covariate-adjustment strategies for randomized clinical trials](https://arxiv.org/abs/2602.00434)
*Yulin Shao,Liangbo Lyu,Menggang Yu,Bingkai Wang*

Main category: stat.AP

TL;DR: 大规模实证研究比较了随机临床试验中的协变量调整策略，发现简约回归方法（如协方差分析）在中等样本量下表现稳定，而机器学习算法在默认超参数设置下并未提供额外效率增益。


<details>
  <summary>Details</summary>
Motivation: 尽管协变量调整被广泛推荐用于提高随机临床试验的统计效率，但不同调整策略之间的实证比较证据仍然有限。这种缺乏实际评估的情况导致在实际应用中关于使用哪种调整方法和包含哪些协变量的实际问题仍未解决。

Method: 使用来自50个公开可获取的随机临床试验的个体水平数据进行大规模实证基准测试，涵盖29,094名参与者和574个治疗-结果配对。评估了18种分析策略，这些策略由六种估计器（包括经典回归、逆概率加权和机器学习方法）与三种协变量选择规则组合而成。

Result: 协变量调整在不同治疗领域中一致提高了精确度，相对于未调整分析，连续结果的方差中位数减少了13.3%，二元结果减少了4.6%。然而，使用默认超参数设置的机器学习算法并未比简单线性模型带来额外的效率增益。简约回归方法（如协方差分析）即使在中等样本量下也能提供稳定、可重复的性能。

Conclusion: 这项研究首次提供了大规模实证证据，表明透明且简约的协变量调整对于常规随机临床试验分析是足够的，并且通常是更可取的。所有整理的数据集和分析代码都已作为可复现的基准资源公开发布，以支持未来的临床研究和方法学发展。

Abstract: Covariate adjustment is widely recommended to improve statistical efficiency in randomized clinical trials (RCTs), yet empirical evidence comparing available strategies remains limited. This lack of real-world evaluation leaves unresolved practical questions about which adjustment methods to use and which covariates to include. To address this gap, we conduct a large-scale empirical benchmarking using individual-level data from 50 publicly accessible RCTs comprising 29,094 participants and 574 treatment-outcome pairs. We evaluate 18 analytical strategies formed by combining six estimators-including classical regression, inverse probability weighting, and machine-learning methods-with three covariate-selection rules. Across diverse therapeutic areas, covariate adjustment consistently improves precision, yielding median variance reductions of 13.3% relative to unadjusted analyses for continuous outcomes and 4.6% for binary outcomes. However, machine-learning algorithms implemented with default hyperparameter settings do not yield efficiency gains beyond simple linear models. Parsimonious regression approaches, such as analysis of covariance, deliver stable, reproducible performance even in moderate sample sizes. Together, these findings provide the first large-scale empirical evidence that transparent and parsimonious covariate adjustment is sufficient and often preferable for routine RCT analysis. All curated datasets and analysis code are openly released as a reproducible benchmark resource to support future clinical research and methodological development.

</details>


### [23] [Boundary-Induced Biases in Climate Networks of Extreme Precipitation and Temperature](https://arxiv.org/abs/2602.00890)
*Behzad Ghanbarian,Victor Oladoja,Kehinde Bosikun,Tayeb Jamali,Jürgen Kurths*

Main category: stat.AP

TL;DR: 研究比较了气候网络中两种边界效应校正方法（减法和除法）的统计差异，发现尽管空间模式相似，但校正后的网络指标在统计上显著不同，其中聚类系数和平均地理距离对校正方法更敏感。


<details>
  <summary>Details</summary>
Motivation: 气候网络中广泛使用减法和除法两种边界效应校正方法，但此前没有研究评估这两种方法是否会产生统计上不同的结果。本研究旨在填补这一空白，比较两种校正方法在极端降水和温度事件网络中的差异。

Method: 构建美国本土极端降水和温度事件的气候网络，计算关键网络指标（度中心性、聚类系数、平均地理距离、介数中心性），分别应用减法和除法两种校正方法，并进行统计显著性检验。

Result: 尽管校正后的空间模式视觉上相似，但统计分析显示减法和除法方法得到的网络指标在95%置信水平上显著不同。聚类系数和平均地理距离比度中心性和介数中心性对校正方法更敏感，特别是在极端降水网络中。

Conclusion: 两种边界效应校正方法在统计上产生显著不同的结果，研究者在选择校正方法时需要谨慎考虑，特别是对于聚类系数和平均地理距离等敏感指标。极端降水网络中心随季节变化，而极端温度网络则表现出更强的空间一致性和遥相关模式。

Abstract: To address spatial boundary effects in climate networks, two surrogate-based correction methods, (1) subtraction and (2) division, have been widely applied in the literature. In the subtraction method, an original network measure is adjusted by subtracting the expected value obtained from a surrogate ensemble, whereas in the division method, it is normalized by dividing by this expected value. However, to the best of our knowledge, no prior study has assessed whether these two correction approaches yield statistically different results. In this study, we constructed complex networks of extreme precipitation and temperature events (EPEs and ETEs) across the CONUS for both summer (June-August, JJA) and winter (December-February, DJF) seasons. We computed key network metrics degree centrality (DC), clustering coefficient (CC), mean geographic distance (MGD), and betweenness centrality (BC) and applied both correction methods. Although the corrected spatial patterns generally appeared visually similar, statistical analyses revealed that the network measures derived from the subtraction and division methods were significantly different at the 95 percent confidence level. Across the CONUS, network hubs of EPEs were primarily concentrated in the northwestern United States during summer and shifted toward the east during winter, reflecting seasonal differences in the dominant atmospheric drivers. In contrast, the ETE networks showed strong spatial coherence and pronounced regional teleconnections in both seasons, with higher connectivity and longer synchronization distances in winter, consistent with large-scale circulation patterns such as the Pacific-North American and North Atlantic Oscillation modes. Our results indicated that the network metrics CC and MGD were more sensitive to the correction methods than the DC and BC, particularly in the EPE networks.

</details>


### [24] [Simultaneous Estimation of Seabed and Its Roughness With Longitudinal Waves](https://arxiv.org/abs/2602.01099)
*Babak Maboudi Afkham,Ana Carpio*

Main category: stat.AP

TL;DR: 提出无限维贝叶斯框架用于声学海底层析成像，利用波散射同时估计海底地形及其粗糙度，通过统计各向同性和分数可微性识别粗糙度，提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 海底层析成像是一个不适定问题，多种海底配置可能产生相似的测量模式，需要同时估计海底地形和粗糙度并量化不确定性。

Method: 采用无限维贝叶斯框架，利用波散射原理，基于海底的统计各向同性假设，使用分数可微性识别粗糙度，开发鲁棒数值算法。

Result: 大量数值实验验证了该方法的有效性，能够同时估计海底地形和粗糙度，并提供不确定性量化，为大规模海底勘探提供了有前景的途径。

Conclusion: 该方法成功解决了海底层析成像的不适定问题，通过统计各向同性和分数可微性框架实现了海底地形和粗糙度的同时估计，为海洋勘探提供了新工具。

Abstract: This paper introduces an infinite-dimensional Bayesian framework for acoustic seabed tomography, leveraging wave scattering to simultaneously estimate the seabed and its roughness. Tomography is considered an ill-posed problem where multiple seabed configurations can result in similar measurement patterns. We propose a novel approach focusing on the statistical isotropy of the seabed. Utilizing fractional differentiability to identify seabed roughness, the paper presents a robust numerical algorithm to estimate the seabed and quantify uncertainties. Extensive numerical experiments validate the effectiveness of this method, offering a promising avenue for large-scale seabed exploration.

</details>


### [25] [Bayesian brain mapping: population-informed individualized functional topography and connectivity](https://arxiv.org/abs/2602.01551)
*Nohelia Da Silva Sanchez,Diego Derman,Damon D. Pham,Ellyn R. Butler,Mary Beth Nebel,Amanda F. Mejia*

Main category: stat.AP

TL;DR: BBM是一种利用群体信息进行个体功能地形图和连接性分析的贝叶斯方法，通过群体先验指导个体估计来应对fMRI的低信噪比问题，适用于临床环境。


<details>
  <summary>Details</summary>
Motivation: 大脑功能组织的空间地形在认知和疾病中起重要作用，但个体差异显著。从fMRI准确估计个体功能网络面临低信噪比挑战，且需要大量扫描时间。

Method: 提出贝叶斯脑图（BBM）技术，利用基于现有空间模板（如分区或连续网络图）的群体先验来指导个体层面的功能地形和连接性估计。该方法避免强时空约束，允许网络重叠和异质性参与模式。

Result: 开发了BayesBrainMap R包，提供从构建群体先验、拟合模型到执行推断的完整工具链。提供了基于人类连接组计划数据库的先验和代码，降低BBM的采用门槛。

Conclusion: BBM是一种计算高效、适用于单个体分析的方法，可准确估计个体功能地形和连接性，特别适合临床环境，有助于研究个体大脑组织差异。

Abstract: The spatial topography of brain functional organization is increasingly recognized to play an important role in cognition and disease. Accounting for individual differences in functional topography is also crucial for accurately distinguishing spatial and temporal aspects of brain organization. Yet, accurate estimation of individual functional brain networks from functional magnetic resonance imaging (fMRI) without extensive scanning remains challenging, due to low signal-to-noise ratio. Here, we describe Bayesian brain mapping (BBM), a technique for individual functional topography and connectivity leveraging population information. Population-derived priors for both spatial topography and functional connectivity based on existing spatial templates, such as parcellations or continuous network maps, are used to guide subject-level estimation and combat noise. BBM is highly flexible, avoiding strong spatial or temporal constraints and allowing for overlap between networks and heterogeneous patterns of engagement. Unlike multi-subject hierarchical models, BBM is designed for single-subject analysis, making it highly computationally efficient and translatable to clinical settings. Here, we describe the BBM model and illustrate the use of the BayesBrainMap R package to construct population-derived priors, fit the model, and perform inference to identify engagements. A demo is provided in an accompanying Github repo. We also share priors derived from the Human Connectome Project database and provide code to support the construction of priors from different data sources, lowering the barrier to adoption of BBM for studies of individual brain organization.

</details>


### [26] [Bootstrap-based estimation and inference for measurement precision under ISO 5725](https://arxiv.org/abs/2602.01931)
*Jun-ichi Takeshita,Kazuhiro Morita,Tomomichi Suzuki*

Main category: stat.AP

TL;DR: 研究ISO 5725标准下实验室间精度分析的自助法应用，提出调整后的点估计和置信区间方法，通过模拟和案例验证性能。


<details>
  <summary>Details</summary>
Motivation: ISO 5725系列标准虽然定义了实验室间精度的重复性、实验室间和再现性方差，但在单因素随机效应模型中实际应用自助法的指导有限。需要研究适合ISO 5725数据的重采样策略。

Method: 1) 研究针对ISO 5725数据的重采样策略；2) 扩展偏差校正思想，获得调整后的方差分量点估计和置信区间；3) 通过模拟实验评估准确性、稳定性和覆盖率；4) 与ANOVA估计器和常用近似区间对比；5) 使用ISO 5725-4数据集进行案例研究。

Result: 1) 调整后的实验室内重采样在小到中等规模设计中提供准确稳定的点估计；2) 两阶段策略（先重采样实验室，再重采样每个实验室内部）配合偏差校正加速区间提供最可靠的置信区间；3) 在极端设计（样本量极小或实验室间变异占主导）下性能下降；4) 案例研究展示了推荐方法在实际中的表现。

Conclusion: 为实验室间研究中的精度分析提供具体实施指导：使用调整后的实验室内重采样进行点估计，采用两阶段策略配合偏差校正加速区间进行区间估计。

Abstract: The ISO 5725 series frames interlaboratory precision through repeatability, between-laboratory, and reproducibility variances, yet practical guidance on deploying bootstrap methods within this one-way random-effects setting remains limited. We study resampling strategies tailored to ISO 5725 data and extend a bias-correction idea to obtain simple adjusted point estimators and confidence intervals for the variance components. Using extensive simulations that mirror realistic study sizes and variance ratios, we evaluate accuracy, stability, and coverage, and we contrast the resampling-based procedures with ANOVA-based estimators and common approximate intervals. The results yield a clear division of labor: adjusted within-laboratory resampling provides accurate and stable point estimation in small-to-moderate designs, whereas a two-stage strategy-resampling laboratories and then resampling within each-paired with bias-corrected and accelerated intervals offers the most reliable (near-nominal or conservative) confidence intervals. Performance degrades under extreme designs, such as very small samples or dominant between-laboratory variation, clarifying when additional caution is warranted. A case study from an ISO 5725-4 dataset illustrates how the recommended procedures behave in practice and how they compare with ANOVA and approximate methods. We conclude with concrete guidance for implementing resampling-based precision analysis in interlaboratory studies: use adjusted within-laboratory resampling for point estimation, and adopt the two-stage strategy with bias-corrected and accelerated intervals for interval estimation.

</details>


### [27] [Counting models with excessive zeros ensuring stochastic monotonicity](https://arxiv.org/abs/2602.02398)
*Hyemin Lee,Dohee Kim,Banghee So,Jae Youn Ahn*

Main category: stat.AP

TL;DR: 论文分析了传统零膨胀和障碍模型在保险精算中可能违反随机单调性的问题，提出了新的随机效应计数模型，既能处理过度零值又能保证随机单调性。


<details>
  <summary>Details</summary>
Motivation: 保险数据中常出现大量零索赔，传统计数模型（如泊松和负二项分布）无法充分处理这种过度零值问题。虽然零膨胀和障碍模型通过额外参数解决了这一问题，并扩展到随机效应模型以处理纵向依赖性和未观测异质性，但这些模型在保险基本概率原则（特别是随机单调性）方面的一致性尚未得到正式检验。

Method: 首先对标准计数随机效应模型进行严格分析，证明它们可能违反随机单调性。然后提出新的计数随机效应模型类别，这些模型既能容纳过度零值，又能确保随机单调性，从而提供公平且理论上一致的信度调整。

Result: 分析表明传统零膨胀和障碍随机效应模型可能违反随机单调性，导致后验信度不一致。提出的新模型类别成功解决了这一问题，确保了随着索赔历史演变，信度调整的公平性和理论一致性。

Conclusion: 论文填补了保险精算中随机效应模型与基本概率原则一致性的研究空白，提出的新模型为处理保险数据中的过度零值问题提供了理论可靠的方法，确保了信度调整的公平性和理论一致性。

Abstract: Standard count models such as the Poisson and Negative Binomial models often fail to capture the large proportion of zero claims commonly observed in insurance data. To address such issue of excessive zeros, zero-inflated and hurdle models introduce additional parameters that explicitly account for excess zeros, thereby improving the joint representation of zero and positive claim outcomes. These models have further been extended with random effects to accommodate longitudinal dependence and unobserved heterogeneity. However, their consistency with fundamental probabilistic principles in insurance, particularly stochastic monotonicity, has not been formally examined. This paper provides a rigorous analysis showing that standard counting random-effect models for excessive zeros may violate this property, leading to inconsistencies in posterior credibility. We then propose new classes of counting random-effect models that both accommodate excessive zeros and ensure stochastic monotonicity, thereby providing fair and theoretically coherent credibility adjustments as claim histories evolve.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [28] [Exact Gibbs sampling for stochastic differential equations with gradient drift and constant diffusion](https://arxiv.org/abs/2602.00512)
*Xinyi Pei,Minhyeok Kim,Vinayak Rao*

Main category: stat.CO

TL;DR: 提出了一种用于单位扩散系数SDE的精确MCMC采样算法，无需离散化误差，适用于广泛SDE类别


<details>
  <summary>Details</summary>
Motivation: 现有SDE路径模拟方法存在偏差、需要复杂拒绝采样或仅限于特定扩散类型，需要更通用且精确的采样方法

Method: 开发了Gibbs采样框架，适用于单位扩散系数SDE，通过适当变换可扩展到多元SDE和大多数一维SDE，无需离散化误差

Result: 在合成和真实数据集上评估，性能优于粒子MCMC方法，仅需相对简单的模拟步骤

Conclusion: 提出的精确MCMC算法为广泛SDE类别提供了无偏差的采样框架，可扩展到参数模拟并利用高斯过程工具

Abstract: Stochastic differential equations (SDEs) are an important class of time-series models, used to describe stochastic systems evolving in continuous time. Simulating paths from these processes, particularly after conditioning on noisy observations of the latent path, remains a challenge. Existing methods often introduce bias through time-discretization, require involved rejection sampling or debiasing schemes or are restricted to a narrow family of diffusions. In this work, we propose an exact Markov chain Monte Carlo (MCMC) sampling algorithm that is applicable to a broad subset of all SDEs with unit diffusion coefficient; after suitable transformation, this includes an even larger class of multivariate SDEs and most 1-d SDEs. We develop a Gibbs sampling framework that allows exact MCMC for such diffusions, without any discretization error. We demonstrate how our MCMC methodology requires only fairly straightforward simulation steps. Our framework can be extended to include parameter simulation, and allows tools from the Gaussian process literature to be easily applied. We evaluate our method on synthetic and real datasets, demonstrating superior performance to particle MCMC approaches.

</details>


### [29] [Complexity bounds for Dirichlet process slice samplers](https://arxiv.org/abs/2602.00878)
*Beatrice Franzolini,Francesco Gaffi*

Main category: stat.CO

TL;DR: 本文对狄利克雷过程（DP）模型中的切片采样算法进行了计算复杂度分析，证明了切片变量的额外开销相对于后验簇数量为O(log n)，即使在最坏情况下，每次迭代出现超线性计算成本的概率也趋于零。


<details>
  <summary>Details</summary>
Motivation: 切片采样是DP模型中广泛使用的标准蒙特卡洛技术，但对其可扩展性的正式评估一直缺乏，主要是因为切片采样每次迭代的计算成本是随机的且可能无界。本文旨在填补这一理论空白。

Method: 作者获得了DP切片采样器计算复杂度的高概率界限，分析了切片变量相对于后验簇数量的额外开销，证明了均匀跨越后验簇增长机制，该开销为O(log n)。

Result: 主要结果表明，切片变量的额外开销相对于后验支持的簇数量为O(log n)，即使在最坏配置下，每次迭代出现超线性计算成本的概率也趋于零。该分析适用于广泛的DP模型，无需特定似然假设。

Conclusion: 这些结果为评估DP模型中切片采样的实际可扩展性建立了理论基础，证明了切片采样在DP模型中的计算效率具有理论保证。

Abstract: Slice sampling is a standard Monte Carlo technique for Dirichlet process (DP)-based models, widely used in posterior simulation. However, formal assessments of the scalability of posterior slice samplers have remained largely unexplored, primarily because the computational cost of a slice-sampling iteration is random and potentially unbounded. In this work, we obtain high-probability bounds on the computational complexity of DP slice samplers. Our main results show that, uniformly across posterior cluster-growth regimes, the overhead induced by slice variables, relatively to the number of clusters supported by the posterior, is $O_{\mathbb P}(\log n)$. As a consequence, even in worst-case configurations, superlinear blow-ups in per-iteration computational cost occur with vanishing probability. Our analysis applies broadly to DP-based models without any likelihood-specific assumptions, still providing complexity guarantees for posterior sampling on arbitrary datasets. These results establish a theoretical foundation for assessing the practical scalability of slice sampling in DP-based models.

</details>


### [30] [A multifidelity approximate Bayesian computation with pre-filtering](https://arxiv.org/abs/2602.01770)
*Xuefei Cao,Shijia Wang,Yongdao Zhou*

Main category: stat.CO

TL;DR: 提出了一种基于多保真度模拟模型的预过滤分层重要性采样算法，用于加速近似贝叶斯计算，并开发了自适应预过滤策略的多保真度ABC序列蒙特卡洛方法。


<details>
  <summary>Details</summary>
Motivation: 传统ABC方法需要大量模拟计算，计算成本高。多保真度模拟模型提供了不同精度和计算成本的模拟选项，但如何有效利用这些模型来加速ABC计算是一个挑战。

Method: 提出了预过滤分层重要性采样算法，利用多保真度模型进行预筛选。从理论上证明了算法的后验集中性质，给出了误差上界和算法效率与预过滤准则的关系。还提供了评估多保真度模型适用性的实用策略，并开发了自适应预过滤的多保真度ABC序列蒙特卡洛方法。

Result: 数值实验证明了所提方法的有效性。开发了相应的R软件包（MAPS），已在GitHub上开源。

Conclusion: 提出的多保真度预过滤方法能有效加速ABC计算，在理论上有良好性质，在实际应用中有实用价值，为高计算成本的贝叶斯推断问题提供了高效解决方案。

Abstract: Approximate Bayesian Computation (ABC) methods often require extensive simulations, resulting in high computational costs. This paper focuses on multifidelity simulation models and proposes a pre-filtering hierarchical importance sampling algorithm. Under mild assumptions, we theoretically prove that the proposed algorithm satisfies posterior concentration properties, characterize the error upper bound and the relationship between algorithmic efficiency and pre-filtering criteria. Additionally, we provide a practical strategy to assess the suitability of multifidelity models for the proposed method. Finally, we develop a multifidelity ABC sequential Monte Carlo with adaptive pre-filtering strategy. Numerical experiments are used to demonstrate the effectiveness of the proposed approach. We develop an R package that is available at https://github.com/caofff/MAPS

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [31] [Uncertainty-Aware Multimodal Learning via Conformal Shapley Intervals](https://arxiv.org/abs/2602.00171)
*Mathew Chandy,Michael Johnson,Judong Shen,Devan V. Mehrotra,Hua Zhou,Jin Zhou,Xiaowu Dai*

Main category: stat.ML

TL;DR: 提出conformal Shapley intervals框架，结合Shapley值和conformal inference为多模态学习中的每个模态构建不确定性感知的重要性区间，并基于此提出具有理论最优性保证的模态选择方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中各模态贡献不均衡且数据依赖性强，难以确定哪些模态真正具有信息量以及其贡献的可信度。量化模态级别的重要性及其不确定性对于可解释和可靠的多模态学习至关重要。

Method: 提出conformal Shapley intervals框架，将Shapley值与conformal inference结合，为每个模态构建不确定性感知的重要性区间。基于这些区间，设计模态选择程序，该程序具有可证明的最优性保证：在给定观测特征条件下，所选模态子集的性能接近最优子集。

Result: 在多个数据集上验证了方法的有效性，表明该方法能提供有意义的不确定性量化、强大的预测性能，同时仅依赖少量信息丰富的模态。

Conclusion: conformal Shapley intervals框架为多模态学习提供了可靠的不确定性感知模态重要性评估，结合理论保证的模态选择方法，实现了可解释、高效的多模态学习。

Abstract: Multimodal learning combines information from multiple data modalities to improve predictive performance. However, modalities often contribute unequally and in a data dependent way, making it unclear which data modalities are genuinely informative and to what extent their contributions can be trusted. Quantifying modality level importance together with uncertainty is therefore central to interpretable and reliable multimodal learning. We introduce conformal Shapley intervals, a framework that combines Shapley values with conformal inference to construct uncertainty-aware importance intervals for each modality. Building on these intervals, we propose a modality selection procedure with a provable optimality guarantee: conditional on the observed features, the selected subset of modalities achieves performance close to that of the optimal subset. We demonstrate the effectiveness of our approach on multiple datasets, showing that it provides meaningful uncertainty quantification and strong predictive performance while relying on only a small number of informative modalities.

</details>


### [32] [Neuron Block Dynamics for XOR Classification with Zero-Margin](https://arxiv.org/abs/2602.00172)
*Guillaume Braun,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 论文研究零边际非线性分类问题，通过分析高斯XOR问题，开发了神经元块动态框架，揭示了神经元聚类为四个方向的现象，并提出了不依赖边际假设的泛化分析方法。


<details>
  <summary>Details</summary>
Motivation: 现有理论分析主要关注回归或具有正边际的分类任务，而零边际非线性分类（如高斯XOR问题）中，有相当比例数据靠近决策边界，破坏了标准的基于边际的论证，需要新的分析框架。

Method: 基于Glasgow (2024)的分析，将训练动态研究从离散输入扩展到高斯输入，开发了神经元块动态框架，分析神经元如何聚类为四个方向，并采用平均情况视角区分可靠预测区域和持续误差区域。

Result: 发现神经元聚类为四个方向，块级信号演化具有一致性，这在个体神经元信号变化显著的高斯设置中至关重要。数值实验证实了预测的两阶段块动态，并展示了其在非高斯设置中的鲁棒性。

Conclusion: 论文提出了一个分析零边际非线性分类的新框架，通过神经元块动态视角揭示了神经网络在高斯XOR问题中的学习机制，为不依赖边际假设的泛化分析提供了理论基础。

Abstract: The ability of neural networks to learn useful features through stochastic gradient descent (SGD) is a cornerstone of their success. Most theoretical analyses focus on regression or on classification tasks with a positive margin, where worst-case gradient bounds suffice. In contrast, we study zero-margin nonlinear classification by analyzing the Gaussian XOR problem, where inputs are Gaussian and the XOR decision boundary determines labels. In this setting, a non-negligible fraction of data lies arbitrarily close to the boundary, breaking standard margin-based arguments. Building on Glasgow's (2024) analysis, we extend the study of training dynamics from discrete to Gaussian inputs and develop a framework for the dynamics of neuron blocks. We show that neurons cluster into four directions and that block-level signals evolve coherently, a phenomenon essential in the Gaussian setting where individual neuron signals vary significantly. Leveraging this block perspective, we analyze generalization without relying on margin assumptions, adopting an average-case view that distinguishes regions of reliable prediction from regions of persistent error. Numerical experiments confirm the predicted two-phase block dynamics and demonstrate their robustness beyond the Gaussian setting.

</details>


### [33] [Singular Bayesian Neural Networks](https://arxiv.org/abs/2602.00387)
*Mame Diarra Toure,David A. Stephens*

Main category: stat.ML

TL;DR: 提出一种低秩参数化的贝叶斯神经网络，通过权重矩阵分解W=AB^T减少参数数量，同时保持不确定性校准能力


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯神经网络需要O(mn)参数，但许多权重矩阵具有快速奇异值衰减特性，这种参数效率不高。需要一种既能减少参数又能保持不确定性校准的方法

Method: 将权重参数化为W=AB^T的低秩形式，其中A∈ℝ^{m×r}, B∈ℝ^{n×r}，诱导后验集中在秩r流形上。这种方法通过共享潜在因子捕获结构化权重相关性

Result: 在MLP、LSTM和Transformer等标准基准测试中，该方法使用比5成员深度集成少15倍的参数，达到竞争性预测性能。显著改善了OOD检测，通常比均值场和扰动基线有更好的校准

Conclusion: 低秩参数化贝叶斯神经网络通过利用权重矩阵的奇异值衰减特性，在减少参数数量的同时保持不确定性校准能力，为大规模贝叶斯深度学习提供了高效解决方案

Abstract: Bayesian neural networks promise calibrated uncertainty but require $O(mn)$ parameters for standard mean-field Gaussian posteriors. We argue this cost is often unnecessary, particularly when weight matrices exhibit fast singular value decay. By parameterizing weights as $W = AB^{\top}$ with $A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{n \times r}$, we induce a posterior that is singular with respect to the Lebesgue measure, concentrating on the rank-$r$ manifold. This singularity captures structured weight correlations through shared latent factors, geometrically distinct from mean-field's independence assumption. We derive PAC-Bayes generalization bounds whose complexity term scales as $\sqrt{r(m+n)}$ instead of $\sqrt{m n}$, and prove loss bounds that decompose the error into optimization and rank-induced bias using the Eckart-Young-Mirsky theorem. We further adapt recent Gaussian complexity bounds for low-rank deterministic networks to Bayesian predictive means. Empirically, across MLPs, LSTMs, and Transformers on standard benchmarks, our method achieves predictive performance competitive with 5-member Deep Ensembles while using up to $15\times$ fewer parameters. Furthermore, it substantially improves OOD detection and often improves calibration relative to mean-field and perturbation baselines.

</details>


### [34] [Reinforcement Learning for Control Systems with Time Delays: A Comprehensive Survey](https://arxiv.org/abs/2602.00399)
*Armando Alves Neto*

Main category: stat.ML

TL;DR: 这篇论文是关于强化学习方法处理控制系统时延问题的全面综述，系统分析了时延对马尔可夫性质的影响，并将现有方法分为五大类进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 实际网络物理系统中的传感延迟、执行延迟和通信约束会违反强化学习依赖的马尔可夫决策过程假设，这些时延引入的记忆效应会显著降低性能并危及系统稳定性，特别是在网络化和多智能体环境中。

Method: 首先形式化主要时延类别并分析其对马尔可夫性质的影响，然后系统地将现有方法分为五大类：状态增强和历史表示方法、具有学习记忆的循环策略、基于预测器和模型感知的方法、鲁棒和域随机化训练策略、以及具有显式约束处理的强化学习框架。

Result: 通过比较分析揭示了这些方法之间的关键权衡，并为不同时延特性和安全要求下的方法选择提供了实用指南。同时识别了稳定性认证、大时延学习、多智能体通信协同设计和标准化基准测试等开放挑战。

Conclusion: 这篇综述旨在为在受时延影响的网络物理系统中开发可靠强化学习控制器的研究人员和从业者提供统一参考，推动该领域的发展。

Abstract: In the last decade, Reinforcement Learning (RL) has achieved remarkable success in the control and decision-making of complex dynamical systems. However, most RL algorithms rely on the Markov Decision Process assumption, which is violated in practical cyber-physical systems affected by sensing delays, actuation latencies, and communication constraints. Such time delays introduce memory effects that can significantly degrade performance and compromise stability, particularly in networked and multi-agent environments. This paper presents a comprehensive survey of RL methods designed to address time delays in control systems. We first formalize the main classes of delays and analyze their impact on the Markov property. We then systematically categorize existing approaches into five major families: state augmentation and history-based representations, recurrent policies with learned memory, predictor-based and model-aware methods, robust and domain-randomized training strategies, and safe RL frameworks with explicit constraint handling. For each family, we discuss underlying principles, practical advantages, and inherent limitations. A comparative analysis highlights key trade-offs among these approaches and provides practical guidelines for selecting suitable methods under different delay characteristics and safety requirements. Finally, we identify open challenges and promising research directions, including stability certification, large-delay learning, multi-agent communication co-design, and standardized benchmarking. This survey aims to serve as a unified reference for researchers and practitioners developing reliable RL-based controllers in delay-affected cyber-physical systems.

</details>


### [35] [Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation](https://arxiv.org/abs/2602.00413)
*Yidong Ouyang,Liyan Xie,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: 提出了一种新的对齐框架，利用对齐问题的本质（从奖励加权分布中采样），适用于扩散模型和流匹配模型，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法主要关注微调预训练生成模型以最大化奖励函数，但需要大量计算资源且难以泛化到不同目标。本文利用对齐问题的本质——从奖励加权分布中采样——来构建更高效的对齐框架。

Method: 1. 将奖励加权分布所需的分数函数（速度场）分解为预训练分数（速度场）加上奖励的条件期望；2. 对于扩散模型，提出免微调框架，训练指导网络估计奖励的条件期望；3. 对于流匹配模型，提出免训练框架，无需额外计算成本。

Result: 1. 在扩散模型上，达到与基于微调模型相当的性能，且计算成本降低至少60%；2. 在流匹配模型上，无需额外计算成本即可提升生成质量；3. 两种模型都能实现一步生成。

Conclusion: 本文提出的对齐框架通过利用奖励加权分布采样的本质，为扩散模型和流匹配模型提供了高效的对齐方法，显著降低了计算成本，同时保持了生成质量。

Abstract: Diffusion models and flow matching have demonstrated remarkable success in text-to-image generation. While many existing alignment methods primarily focus on fine-tuning pre-trained generative models to maximize a given reward function, these approaches require extensive computational resources and may not generalize well across different objectives. In this work, we propose a novel alignment framework by leveraging the underlying nature of the alignment problem -- sampling from reward-weighted distributions -- and show that it applies to both diffusion models (via score guidance) and flow matching models (via velocity guidance). The score function (velocity field) required for the reward-weighted distribution can be decomposed into the pre-trained score (velocity field) plus a conditional expectation of the reward. For the alignment on the diffusion model, we identify a fundamental challenge: the adversarial nature of the guidance term can introduce undesirable artifacts in the generated images. Therefore, we propose a finetuning-free framework that trains a guidance network to estimate the conditional expectation of the reward. We achieve comparable performance to finetuning-based models with one-step generation with at least a 60% reduction in computational cost. For the alignment on flow matching, we propose a training-free framework that improves the generation quality without additional computational cost.

</details>


### [36] [Sampling from multi-modal distributions on Riemannian manifolds with training-free stochastic interpolants](https://arxiv.org/abs/2602.00641)
*Alain Durmus,Maxence Noble,Thibaut Pellerin*

Main category: stat.ML

TL;DR: 提出一种在黎曼流形上从非归一化多模态密度采样的训练自由方法，基于确定性动力学将噪声分布传输到目标分布，无需机器学习。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法在处理黎曼流形上的多模态目标分布时面临挑战，特别是高维和重尾分布。需要一种无需训练、能处理复杂几何结构的采样方法。

Method: 基于扩散模型框架，通过模拟非平衡确定性动力学，将易采样的噪声分布传输到目标分布。密度路径遵循噪声与目标分布之间的随机插值，尊重黎曼几何结构。采用迭代后验采样过程，仅使用标准蒙特卡洛技术。

Result: 方法在多种多模态采样问题上表现出有效性，包括高维和重尾分布示例。理论分析为方法提供了严格基础。

Conclusion: 提出了一种完全训练自由的黎曼流形采样方法，将基于扩散的采样方法扩展到非欧几里得设置，为处理复杂几何结构的多模态分布提供了有效解决方案。

Abstract: In this paper, we propose a general methodology for sampling from un-normalized densities defined on Riemannian manifolds, with a particular focus on multi-modal targets that remain challenging for existing sampling methods. Inspired by the framework of diffusion models developed for generative modeling, we introduce a sampling algorithm based on the simulation of a non-equilibrium deterministic dynamics that transports an easy-to-sample noise distribution toward the target. At the marginal level, the induced density path follows a prescribed stochastic interpolant between the noise and target distributions, specifically constructed to respect the underlying Riemannian geometry. In contrast to related generative modeling approaches that rely on machine learning, our method is entirely training-free. It instead builds on iterative posterior sampling procedures using only standard Monte Carlo techniques, thereby extending recent diffusion-based sampling methodologies beyond the Euclidean setting. We complement our approach with a rigorous theoretical analysis and demonstrate its effectiveness on a range of multi-modal sampling problems, including high-dimensional and heavy-tailed examples.

</details>


### [37] [Multivariate Time Series Data Imputation via Distributionally Robust Regularization](https://arxiv.org/abs/2602.00844)
*Che-Yi Liao,Zheng Dong,Gian-Gabriel Garcia,Kamran Paynabar*

Main category: stat.ML

TL;DR: 提出DRIO方法，通过分布鲁棒正则化解决多元时间序列插补中的分布不匹配问题，在随机和非随机缺失场景下均能提升性能。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列插补常因观测数据与真实数据分布不匹配而受损，这种偏差在非平稳性和系统性缺失情况下更加严重。传统方法最小化重构误差或鼓励分布对齐，容易过拟合这些有偏观测。

Method: 提出分布鲁棒正则化插补目标(DRIO)，联合最小化重构误差和插补器与Wasserstein模糊集内最坏情况分布之间的散度。推导出可处理的对偶形式，将无限维测度优化简化为样本轨迹的对抗搜索，并提出与灵活深度学习骨干兼容的对抗学习算法。

Result: 在多样化的真实世界数据集上的综合实验表明，DRIO在完全随机缺失和非随机缺失设置下均能持续改进插补性能，达到重构精度和分布对齐之间的帕累托最优权衡。

Conclusion: DRIO通过分布鲁棒正则化有效解决了多元时间序列插补中的分布不匹配问题，在多种缺失机制下都能实现更好的插补质量。

Abstract: Multivariate time series (MTS) imputation is often compromised by mismatch between observed and true data distributions -- a bias exacerbated by non-stationarity and systematic missingness. Standard methods that minimize reconstruction error or encourage distributional alignment risk overfitting these biased observations. We propose the Distributionally Robust Regularized Imputer Objective (DRIO), which jointly minimizes reconstruction error and the divergence between the imputer and a worst-case distribution within a Wasserstein ambiguity set. We derive a tractable dual formulation that reduces infinite-dimensional optimization over measures to adversarial search over sample trajectories, and propose an adversarial learning algorithm compatible with flexible deep learning backbones. Comprehensive experiments on diverse real-world datasets show DRIO consistently improves imputation under both missing-completely-at-random and missing-not-at-random settings, reaching Pareto-optimal trade-offs between reconstruction accuracy and distributional alignment.

</details>


### [38] [Shuffle and Joint Differential Privacy for Generalized Linear Contextual Bandits](https://arxiv.org/abs/2602.00417)
*Sahasrajit Sarmasarkar*

Main category: stat.ML

TL;DR: 本文首次提出了在洗牌差分隐私和联合差分隐私下的广义线性上下文赌博机算法，解决了GLM带来的新挑战，在不同隐私模型和上下文设置下实现了接近非私有算法的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有私有上下文赌博机研究仅限于线性奖励模型（具有闭式解），而广义线性模型（GLM）带来新挑战：无闭式解需要私有凸优化、隐私需跨多个演化设计矩阵跟踪、优化误差需明确纳入遗憾分析。

Method: 针对两种隐私模型和上下文设置设计不同算法：对于随机上下文，设计洗牌差分隐私算法；对于对抗性上下文，提供联合差分隐私算法。两种算法都消除了对实例特定参数κ的依赖，且不需要超出ℓ₂有界性的谱假设。

Result: 随机上下文下洗牌差分隐私算法达到$\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$遗憾；对抗性上下文下联合差分隐私算法达到$\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$遗憾，与非私有率相比仅差$1/\sqrt{\varepsilon}$因子。

Conclusion: 本文首次解决了广义线性上下文赌博机在差分隐私下的算法设计问题，在不同隐私模型下实现了接近最优的遗憾界，消除了对问题特定参数的依赖，为私有GLM学习提供了理论保证。

Abstract: We present the first algorithms for generalized linear contextual bandits under shuffle differential privacy and joint differential privacy. While prior work on private contextual bandits has been restricted to linear reward models -- which admit closed-form estimators -- generalized linear models (GLMs) pose fundamental new challenges: no closed-form estimator exists, requiring private convex optimization; privacy must be tracked across multiple evolving design matrices; and optimization error must be explicitly incorporated into regret analysis.
  We address these challenges under two privacy models and context settings. For stochastic contexts, we design a shuffle-DP algorithm achieving $\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$ regret. For adversarial contexts, we provide a joint-DP algorithm with $\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$ regret -- matching the non-private rate up to a $1/\sqrt{\varepsilon}$ factor. Both algorithms remove dependence on the instance-specific parameter $κ$ (which can be exponential in dimension) from the dominant $\sqrt{T}$ term. Unlike prior work on locally private GLM bandits, our methods require no spectral assumptions on the context distribution beyond $\ell_2$ boundedness.

</details>


### [39] [Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning](https://arxiv.org/abs/2602.01427)
*Haixiang Sun,Andrew L. Liu*

Main category: stat.ML

TL;DR: PG-DRO：一种原型引导的分布鲁棒优化框架，通过分层最优传输从丰富基类数据中学习类别自适应先验，并将其嵌入Sinkhorn DRO公式，在少样本场景中实现更强的鲁棒泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Sinkhorn分布鲁棒优化方法依赖固定参考分布，限制了其适应性。少样本学习需要在有限监督下泛化并保持对分布偏移的鲁棒性，需要更灵活的方法。

Method: 提出原型引导分布鲁棒优化框架：1）通过分层最优传输从丰富基类数据学习类别自适应先验；2）将这些先验嵌入Sinkhorn DRO公式；3）有机整合少样本信息生成类别特定的鲁棒决策。

Result: 实验表明PG-DRO在少样本场景中实现了更强的鲁棒泛化能力，超越了标准学习器和DRO基线方法。

Conclusion: PG-DRO框架通过将可迁移的结构知识融入不确定性集合，实现了理论保证与效率的平衡，为少样本学习提供了有效的分布鲁棒优化解决方案。

Abstract: Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.

</details>


### [40] [Topological Residual Asymmetry for Bivariate Causal Direction](https://arxiv.org/abs/2602.00427)
*Mouad El Bouchattaoui*

Main category: stat.ML

TL;DR: 提出TRA方法，基于拓扑几何判断因果方向，通过残差云形状对比（二维块状vs一维管状），使用持续同调量化差异，在低噪声和固定噪声下均有效。


<details>
  <summary>Details</summary>
Motivation: 现有因果方向推断方法在模糊或接近不可识别的情况下容易出错，需要更稳健的几何基础方法。

Method: TRA方法：1) 秩基copula标准化；2) 交叉拟合回归残差云；3) 使用0维持续同调功能（基于欧几里得MST边长度剖面）量化残差云形状差异；4) 扩展为TRA-s（分箱版本）处理固定噪声；5) 提出TRA-C混淆感知弃权规则。

Result: 在三角阵列小噪声机制下证明一致性，通过大量合成和真实数据实验验证方法优越性。

Conclusion: TRA提供了一种基于几何的稳健因果方向推断方法，在具有挑战性的场景中表现优异，并能通过TRA-C识别混淆情况。

Abstract: Inferring causal direction from purely observational bivariate data is fragile: many methods commit to a direction even in ambiguous or near non-identifiable regimes. We propose Topological Residual Asymmetry (TRA), a geometry-based criterion for additive-noise models. TRA compares the shapes of two cross-fitted regressor-residual clouds after rank-based copula standardization: in the correct direction, residuals are approximately independent, producing a two-dimensional bulk, while in the reverse direction -- especially under low noise -- the cloud concentrates near a one-dimensional tube. We quantify this bulk-tube contrast using a 0D persistent-homology functional, computed efficiently from Euclidean MST edge-length profiles. We prove consistency in a triangular-array small-noise regime, extend the method to fixed noise via a binned variant (TRA-s), and introduce TRA-C, a confounding-aware abstention rule calibrated by a Gaussian-copula plug-in bootstrap. Extensive experiments across many challenging synthetic and real-data scenarios demonstrate the method's superiority.

</details>


### [41] [Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations](https://arxiv.org/abs/2602.00474)
*Yang Xu,Vaneet Aggarwal*

Main category: stat.ML

TL;DR: 该论文提出了一个用于非遍历马尔可夫链（多链和周期链）的泊松方程稳定学习方法，通过商空间分析和投影随机逼近实现收敛。


<details>
  <summary>Details</summary>
Motivation: 传统平均奖励强化学习中的泊松方程在遍历性假设下成立，但在非遍历（可约或周期）马尔可夫链中可能病态，导致解不唯一且固定点迭代振荡。需要开发适用于更一般链结构的稳定求解方法。

Method: 1) 分析马尔可夫链的实外围不变子空间，证明在商空间上的诱导算子是严格压缩的；2) 构建端到端流程：学习链结构、估计锚定规范映射、运行投影随机逼近来估计规范固定代表和外围残差。

Result: 证明了在投影估计误差范围内达到$\widetilde{O}(T^{-1/2})$收敛率，实现了多链和周期机制下泊松方程的稳定学习，扩展了平均奖励强化学习的性能评估范围。

Conclusion: 通过商空间分析和投影随机逼近，成功解决了非遍历马尔可夫链中泊松方程的病态问题，为超越遍历性的平均奖励强化学习提供了理论基础和实用算法。

Abstract: Poisson equations underpin average-reward reinforcement learning, but beyond ergodicity they can be ill-posed, meaning that solutions are non-unique and standard fixed point iterations can oscillate on reducible or periodic chains. We study finite-state Markov chains with $n$ states and transition matrix $P$. We show that all non-decaying modes are captured by a real peripheral invariant subspace $\mathcal{K}(P)$, and that the induced operator on the quotient space $\mathbb{R}^n/\mathcal{K}(P)$ is strictly contractive, yielding a unique quotient solution. Building on this viewpoint, we develop an end-to-end pipeline that learns the chain structure, estimates an anchor based gauge map, and runs projected stochastic approximation to estimate a gauge-fixed representative together with an associated peripheral residual. We prove $\widetilde{O}(T^{-1/2})$ convergence up to projection estimation error, enabling stable Poisson equation learning for multichain and periodic regimes with applications to performance evaluation of average-reward reinforcement learning beyond ergodicity.

</details>


### [42] [Action-Free Offline-to-Online RL via Discretised State Policies](https://arxiv.org/abs/2602.00629)
*Natinael Solomon Neggatu,Jeremie Houssineau,Giovanni Montana*

Main category: stat.ML

TL;DR: 提出一种从无动作标签的离线数据中学习状态策略的方法，通过状态离散化转换和状态价值学习，加速在线强化学习


<details>
  <summary>Details</summary>
Motivation: 现实场景中离线数据集常缺失动作标签（由于隐私、存储或传感器限制），但现有离线RL方法都假设动作标签可用。需要解决从仅包含状态-奖励-下一状态三元组的数据中学习，并加速在线学习的问题。

Method: 1. 提出状态离散化转换，将连续状态空间离散化以避免不稳定和过拟合；2. 提出Offline State-Only DecQN算法，从无动作数据中预训练状态策略（推荐期望的下一状态转移而非动作）；3. 提出引导在线学习机制，利用预训练的状态策略加速在线代理学习。

Result: 在多个基准测试中，该方法提高了收敛速度和渐近性能。分析表明状态离散化和正则化对方法有效性至关重要。

Conclusion: 提出了一个可扩展的实用框架，能够利用无动作数据集加速在线强化学习，解决了实际场景中动作标签缺失的问题。

Abstract: Most existing offline RL methods presume the availability of action labels within the dataset, but in many practical scenarios, actions may be missing due to privacy, storage, or sensor limitations. We formalise the setting of action-free offline-to-online RL, where agents must learn from datasets consisting solely of $(s,r,s')$ tuples and later leverage this knowledge during online interaction. To address this challenge, we propose learning state policies that recommend desirable next-state transitions rather than actions. Our contributions are twofold. First, we introduce a simple yet novel state discretisation transformation and propose Offline State-Only DecQN (\algo), a value-based algorithm designed to pre-train state policies from action-free data. \algo{} integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction. Second, we propose a novel mechanism for guided online learning that leverages these pre-trained state policies to accelerate the learning of online agents. Together, these components establish a scalable and practical framework for leveraging action-free datasets to accelerate online RL. Empirical results across diverse benchmarks demonstrate that our approach improves convergence speed and asymptotic performance, while analyses reveal that discretisation and regularisation are critical to its effectiveness.

</details>


### [43] [Emergence of Distortions in High-Dimensional Guided Diffusion Models](https://arxiv.org/abs/2602.00716)
*Enrico Ventura,Beatrice Achilli,Luca Ambrogioni,Carlo Lucibello*

Main category: stat.ML

TL;DR: CFG导致生成样本多样性损失，作者将其形式化为生成失真，分析了高维条件下的失真相变，提出负引导窗口方法缓解多样性损失


<details>
  <summary>Details</summary>
Motivation: 分类器自由引导（CFG）是扩散模型中条件采样的标准方法，但常常导致生成样本多样性损失。作者旨在形式化这一现象并理解其根本原因

Method: 使用高斯混合模型及其精确分数，借助统计物理工具分析高维条件下的失真现象；进行动力学平均场分析；提出负引导窗口的引导调度方法

Result: 失真通过引导动力学有效势的相变出现；当模式数量随维度指数增长时失真持续存在，但在次指数增长时消失；标准CFG无法防止方差收缩；提出的负引导窗口方法能缓解多样性损失

Conclusion: CFG导致的生成失真是一个相变现象，标准CFG调度无法防止方差收缩，但通过负引导窗口的改进调度可以在保持类别可分性的同时缓解多样性损失

Abstract: Classifier-free guidance (CFG) is the de facto standard for conditional sampling in diffusion models, yet it often leads to a loss of diversity in generated samples. We formalize this phenomenon as generative distortion, defined as the mismatch between the CFG-induced sampling distribution and the true conditional distribution. Considering Gaussian mixtures and their exact scores, and leveraging tools from statistical physics, we characterize the onset of distortion in a high-dimensional regime as a function of the number of classes. Our analysis reveals that distortions emerge through a phase transition in the effective potential governing the guided dynamics. In particular, our dynamical mean-field analysis shows that distortion persists when the number of modes grows exponentially with dimension, but vanishes in the sub-exponential regime. Consistent with prior finite-dimensional results, we further demonstrate that vanilla CFG shifts the mean and shrinks the variance of the conditional distribution. We show that standard CFG schedules are fundamentally incapable of preventing variance shrinkage. Finally, we propose a theoretically motivated guidance schedule featuring a negative-guidance window, which mitigates loss of diversity while preserving class separability.

</details>


### [44] [Zero-Flow Encoders](https://arxiv.org/abs/2602.00797)
*Yakun Wang,Leyang Wang,Song Liu,Taiji Suzuki*

Main category: stat.ML

TL;DR: 提出基于流的表示学习框架，利用零流准则验证条件独立性，学习马尔可夫毯和潜在表示


<details>
  <summary>Details</summary>
Motivation: 现有流方法主要关注生成任务，未能充分利用其捕捉精细结构细节的能力进行表示学习。本文旨在探索流模型在表示学习中的应用潜力。

Method: 1) 证明使用独立耦合训练的整流流在t=0.5时处处为零当且仅当源分布和目标分布相同（零流准则）；2) 展示该准则可验证条件独立性，提取数据的充分信息；3) 将该准则转化为可处理的、无需模拟的损失函数，用于学习图模型中的摊销马尔可夫毯和自监督学习中的潜在表示。

Result: 在模拟和真实数据集上的实验证明了该方法的有效性，能够成功学习马尔可夫毯和潜在表示。

Conclusion: 提出了一种基于流的表示学习框架，通过零流准则实现了条件独立性的验证和充分信息的提取，为流模型在表示学习领域的应用开辟了新途径。

Abstract: Flow-based methods have achieved significant success in various generative modeling tasks, capturing nuanced details within complex data distributions. However, few existing works have exploited this unique capability to resolve fine-grained structural details beyond generation tasks. This paper presents a flow-inspired framework for representation learning. First, we demonstrate that a rectified flow trained using independent coupling is zero everywhere at $t=0.5$ if and only if the source and target distributions are identical. We term this property the \emph{zero-flow criterion}. Second, we show that this criterion can certify conditional independence, thereby extracting \emph{sufficient information} from the data. Third, we translate this criterion into a tractable, simulation-free loss function that enables learning amortized Markov blankets in graphical models and latent representations in self-supervised learning tasks. Experiments on both simulated and real-world datasets demonstrate the effectiveness of our approach. The code reproducing our experiments can be found at: https://github.com/probabilityFLOW/zfe.

</details>


### [45] [Hessian Spectral Analysis at Foundation Model Scale](https://arxiv.org/abs/2602.00816)
*Diego Granziol,Khurshid Juarev*

Main category: stat.ML

TL;DR: 首次在百亿参数规模上实现了真实Hessian矩阵的谱分析，揭示了传统块对角近似在大型语言模型中的严重失效问题


<details>
  <summary>Details</summary>
Motivation: 以往由于计算困难，基础模型的Hessian谱分析只能在小模型或强结构近似下进行，缺乏对前沿规模模型真实Hessian谱的准确理解

Method: 使用与完全分片数据并行兼容的分片局部有限差分Hessian向量乘积，结合随机Lanczos求积法，在fp32和bf16精度下分析数值行为

Result: 成功在100B参数规模上获得首个大规模谱密度估计，发现传统块对角曲率近似存在阶一相对误差和方向对齐问题，而全算子谱探测仅带来适度常数因子开销

Conclusion: 基础模型的Hessian谱不仅可计算，而且现有近似方法严重失真，为大规模曲率分析开辟了新途径

Abstract: Accurate Hessian spectra of foundation models have remained out of reach, leading most prior work to rely on small models or strong structural approximations. We show that faithful spectral analysis of the true Hessian is tractable at frontier scale. Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism, we perform stochastic Lanczos quadrature on open-source language models with up to 100B parameters, producing the first large-scale spectral density estimates beyond the sub-10B regime. We characterize the numerical behavior of this pipeline, including finite-difference bias, floating-point noise amplification, and their effect on Krylov stability in fp32 and bf16, and derive practical operating regimes that are validated empirically. We further provide end-to-end runtime and memory scaling laws, showing that full-operator spectral probing incurs only a modest constant-factor overhead over first-order training. Crucially, direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error and poor directional alignment even in mid-scale LLMs. Together, our results demonstrate that foundation-model Hessian spectra are both computable and qualitatively misrepresented by prevailing approximations, opening the door to principled curvature-based analysis at scale.

</details>


### [46] [Safety-Efficacy Trade Off: Robustness against Data-Poisoning](https://arxiv.org/abs/2602.00822)
*Diego Granziol*

Main category: stat.ML

TL;DR: 论文证明后门攻击和数据投毒攻击通过输入空间的几何机制实现高攻击成功率并规避现有防御，揭示了攻击效果与谱可见性之间的滞后关系，提出了通过输入梯度正则化进行防御的方法。


<details>
  <summary>Details</summary>
Motivation: 现有谱分析和优化防御方法无法有效检测后门和数据投毒攻击，需要从理论上理解攻击为何能同时实现高成功率和隐蔽性，并建立系统的攻击-检测-防御理论框架。

Method: 使用核岭回归作为宽神经网络的精确模型，分析投毒攻击在输入空间中的几何机制；证明聚集的脏标签投毒会在输入Hessian中产生秩一尖峰；识别非线性核中的近克隆机制；提出输入梯度正则化防御方法。

Result: 理论证明：投毒攻击效果与输入曲率之间存在二次缩放关系；在近克隆机制下，攻击效果保持量级而输入曲率消失，使攻击谱不可检测；输入梯度正则化收缩毒物对齐的Fisher和Hessian特征模，产生安全-效能权衡；实验验证了攻击成功率和谱可见性之间的滞后关系。

Conclusion: 后门攻击在特定条件下具有固有的不可见性；输入梯度正则化通过收缩输入空间曲率提供防御，但存在安全-效能权衡；首次通过输入空间曲率完整刻画了投毒攻击、可检测性和防御机制。

Abstract: Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels we identify a near clone regime in which poison efficacy remains order one while the induced input curvature vanishes, making the attack provably spectrally undetectable. We further show that input gradient regularisation contracts poison aligned Fisher and Hessian eigenmodes under gradient flow, yielding an explicit and unavoidable safety efficacy trade off by reducing data fitting capacity. For exponential kernels, this defence admits a precise interpretation as an anisotropic high pass filter that increases the effective length scale and suppresses near clone poisons. Extensive experiments on linear models and deep convolutional networks across MNIST and CIFAR 10 and CIFAR 100 validate the theory, demonstrating consistent lags between attack success and spectral visibility, and showing that regularisation and data augmentation jointly suppress poisoning. Our results establish when backdoors are inherently invisible, and provide the first end to end characterisation of poisoning, detectability, and defence through input space curvature.

</details>


### [47] [Harmful Overfitting in Sobolev Spaces](https://arxiv.org/abs/2602.00825)
*Kedar Karhadkar,Alexander Sietsema,Deanna Needell,Guido Montufar*

Main category: stat.ML

TL;DR: 研究Sobolev空间中范数最小化插值器的泛化行为，发现在标签噪声和足够正则的数据分布下，即使训练样本量趋于无穷，泛化误差仍以高概率保持正下界，表明存在有害过拟合。


<details>
  <summary>Details</summary>
Motivation: 受近期过参数化机器学习中良性过拟合研究的启发，探索Sobolev空间中完美拟合噪声训练数据的函数的泛化行为，特别关注平滑性偏置选择的规范最小化插值器。

Method: 在标签噪声和数据分布足够正则的假设下，研究Sobolev空间W^{k,p}(ℝ^d)中的函数，使用几何论证方法，通过Sobolev不等式识别训练数据的有害邻域。

Result: 近似范数最小化插值器表现出有害过拟合：即使训练样本量n→∞，泛化误差仍以高概率保持正下界。该结果适用于任意p∈[1,∞)，扩展了先前仅研究希尔伯特空间(p=2)的情况。

Conclusion: 在Sobolev空间中，平滑性偏置选择的规范最小化插值器在存在标签噪声时会导致有害过拟合，即使样本量无限增大也无法避免泛化误差的正下界，这一现象在任意p值下均成立。

Abstract: Motivated by recent work on benign overfitting in overparameterized machine learning, we study the generalization behavior of functions in Sobolev spaces $W^{k, p}(\mathbb{R}^d)$ that perfectly fit a noisy training data set. Under assumptions of label noise and sufficient regularity in the data distribution, we show that approximately norm-minimizing interpolators, which are canonical solutions selected by smoothness bias, exhibit harmful overfitting: even as the training sample size $n \to \infty$, the generalization error remains bounded below by a positive constant with high probability. Our results hold for arbitrary values of $p \in [1, \infty)$, in contrast to prior results studying the Hilbert space case ($p = 2$) using kernel methods. Our proof uses a geometric argument which identifies harmful neighborhoods of the training data using Sobolev inequalities.

</details>


### [48] [Score-based Metropolis-Hastings for Fractional Langevin Algorithms](https://arxiv.org/abs/2602.00835)
*Ahmed Aloui,Junyi Liao,Ali Hasan,Jose Blanchet,Vahid Tarokh*

Main category: stat.ML

TL;DR: MAFLA是一种基于分数的Metropolis-Hastings校正方法，用于改进α稳定Lévy驱动的分数Langevin算法在重尾多峰分布采样中的性能。


<details>
  <summary>Details</summary>
Motivation: 当目标分布和提议分布都无法直接评估时（如α稳定Lévy驱动的分数Langevin算法），传统的基于密度的Metropolis-Hastings校正不可行。现有的分数Langevin方法在未校正状态下运行，存在显著的有限时间误差和对尾部行为的控制不足问题。

Method: 提出Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA)，这是一种基于分数的MH校正机制。方法包括：1) 在各项同性对称α稳定噪声下设计分数提议梯度代理；2) 通过Score Balance Matching学习接受函数。

Result: MAFLA在一系列任务中表现出色，特别是在组合优化问题中，相比未校正的分数Langevin动力学，显著提高了有限时间采样的准确性。

Conclusion: MAFLA为分数Langevin算法提供了一种有效的基于分数的Metropolis-Hastings校正机制，解决了重尾多峰分布采样中的挑战，特别是在目标分布和提议分布都无法直接评估的情况下。

Abstract: Sampling from heavy-tailed and multimodal distributions is challenging when neither the target density nor the proposal density can be evaluated, as in $α$-stable Lévy-driven fractional Langevin algorithms. While the target distribution can be estimated from data via score-based or energy-based models, the $α$-stable proposal density and its score are generally unavailable, rendering classical density-based Metropolis--Hastings (MH) corrections impractical. Consequently, existing fractional Langevin methods operate in an unadjusted regime and can exhibit substantial finite-time errors and poor empirical control of tail behavior. We introduce the Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA), an MH-inspired, fully score-based correction mechanism. MAFLA employs designed proxies for fractional proposal score gradients under isotropic symmetric $α$-stable noise and learns an acceptance function via Score Balance Matching. We empirically illustrate the strong performance of MAFLA on a series of tasks including combinatorial optimization problems where the method significantly improves finite time sampling accuracy over unadjusted fractional Langevin dynamics.

</details>


### [49] [Optimal Decision-Making Based on Prediction Sets](https://arxiv.org/abs/2602.00989)
*Tao Wang,Edgar Dobriban*

Main category: stat.ML

TL;DR: 提出Risk-Optimal Conformal Prediction (ROCP)框架，在保证覆盖概率的前提下最小化决策风险，特别关注集合外错误的代价。


<details>
  <summary>Details</summary>
Motivation: 预测集合虽然能为ML模型提供概率保证的覆盖，但如何将其最优地用于下游决策仍不明确。现有方法未充分考虑决策风险，特别是集合外错误的代价。

Method: 1) 建立决策理论框架，在预测集合覆盖保证下最小化最坏情况分布的期望损失；2) 推导固定预测集合下的极小极大最优策略；3) 设计最小化鲁棒风险的最优预测集合构造；4) 提出ROCP实用算法，在保持有限样本分布无关边际覆盖的同时实现风险最小化。

Result: 在医疗诊断和安全关键决策任务上的实证评估表明，ROCP相比基线方法减少了关键错误，特别是在集合外错误代价高昂的情况下表现更优。

Conclusion: ROCP框架将预测集合与决策风险优化相结合，为高风险应用提供了更可靠的决策支持，在保证覆盖概率的同时显著降低了关键错误的发生。

Abstract: Prediction sets can wrap around any ML model to cover unknown test outcomes with a guaranteed probability. Yet, it remains unclear how to use them optimally for downstream decision-making. Here, we propose a decision-theoretic framework that seeks to minimize the expected loss (risk) against a worst-case distribution consistent with the prediction set's coverage guarantee. We first characterize the minimax optimal policy for a fixed prediction set, showing that it balances the worst-case loss inside the set with a penalty for potential losses outside the set. Building on this, we derive the optimal prediction set construction that minimizes the resulting robust risk subject to a coverage constraint. Finally, we introduce Risk-Optimal Conformal Prediction (ROCP), a practical algorithm that targets these risk-minimizing sets while maintaining finite-sample distribution-free marginal coverage. Empirical evaluations on medical diagnosis and safety-critical decision-making tasks demonstrate that ROCP reduces critical mistakes compared to baselines, particularly when out-of-set errors are costly.

</details>


### [50] [Online Social Welfare Function-based Resource Allocation](https://arxiv.org/abs/2602.01400)
*Kanad Pardeshi,Samsara Foubert,Aarti Singh*

Main category: stat.ML

TL;DR: 提出了一个基于社会福利函数（SWF）的在线学习和推断通用置信序列框架，适用于任何单调、凹且Lipschitz连续的社会福利函数，并设计了SWF-UCB算法实现近乎最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，集中决策者需要多次将有限资源分配给人群，个体获得资源后产生随机效用，需要通过社会福利函数来评估分配效果。现有方法缺乏适用于各种社会福利函数的统一在线学习和推断框架。

Method: 提出了一个通用置信序列框架，关键洞察是单调性足以将置信序列从个体效用提升到最优福利的任意时间有效边界。基于此设计了SWF-UCB算法，并在三个规范不同的社会福利函数族（加权幂均值、Kolm和Gini）上实例化框架。

Result: SWF-UCB算法实现了近乎最优的$\tilde{O}(n+\sqrt{nkT})$遗憾界（k个资源在T个时间步中分配给n个个体）。实验证实了$\sqrt{T}$缩放，并揭示了k与SWF参数之间的丰富交互作用。

Conclusion: 该框架为基于社会福利函数的在线学习和推断提供了统一方法，支持顺序假设检验、最优停止和政策评估等推断应用，具有广泛的适用性和理论保证。

Abstract: In many real-world settings, a centralized decision-maker must repeatedly allocate finite resources to a population over multiple time steps. Individuals who receive a resource derive some stochastic utility; to characterize the population-level effects of an allocation, the expected individual utilities are then aggregated using a social welfare function (SWF). We formalize this setting and present a general confidence sequence framework for SWF-based online learning and inference, valid for any monotonic, concave, and Lipschitz-continuous SWF. Our key insight is that monotonicity alone suffices to lift confidence sequences from individual utilities to anytime-valid bounds on optimal welfare. Building on this foundation, we propose SWF-UCB, a SWF-agnostic online learning algorithm that achieves near-optimal $\tilde{O}(n+\sqrt{nkT})$ regret (for $k$ resources distributed among $n$ individuals at each of $T$ time steps). We instantiate our framework on three normatively distinct SWF families: Weighted Power Mean, Kolm, and Gini, providing bespoke oracle algorithms for each. Experiments confirm $\sqrt{T}$ scaling and reveal rich interactions between $k$ and SWF parameters. This framework naturally supports inference applications such as sequential hypothesis testing, optimal stopping, and policy evaluation.

</details>


### [51] [Importance Weighted Variational Inference without the Reparameterization Trick](https://arxiv.org/abs/2602.01412)
*Kamélia Daudel,Minh-Ngoc Tran,Cheng Zhang*

Main category: stat.ML

TL;DR: 该论文分析了重要性加权变分推断中REINFORCE梯度估计器的理论缺陷，提出了VIMCO-*梯度估计器来解决现有VIMCO估计器随样本数增加而信号噪声比消失的问题。


<details>
  <summary>Details</summary>
Motivation: 重要性加权变分推断通过蒙特卡洛样本优化边界，但标准重参数化梯度估计器对数据生成过程和变分近似有限制。REINFORCE梯度估计器虽然不受这些限制，但缺乏严格理论分析，现有VIMCO梯度估计器存在信号噪声比随样本数增加而消失的问题。

Method: 对重要性加权VI中的REINFORCE梯度估计器进行首次全面理论分析，引入并研究广义的VIMCO梯度估计器家族。提出新的VIMCO-*梯度估计器，通过理论证明其能避免现有VIMCO估计器的信号噪声比崩溃问题，实现√N的信号噪声比缩放。

Result: 理论证明现有VIMCO梯度估计器随样本数N增加会出现信号噪声比消失的问题，而提出的VIMCO-*估计器能避免这一问题，实现√N的信号噪声比缩放。在重参数化梯度不可用的挑战性设置中，VIMCO-*表现出比现有VIMCO实现更优越的实证性能。

Conclusion: 该研究为重要性加权变分推断中的REINFORCE梯度估计器提供了首个全面理论分析，揭示了现有VIMCO估计器的根本缺陷，并提出了有效的VIMCO-*解决方案，在重参数化梯度不可用的情况下提供了更可靠的优化方法。

Abstract: Importance weighted variational inference (VI) approximates densities known up to a normalizing constant by optimizing bounds that tighten with the number of Monte Carlo samples $N$. Standard optimization relies on reparameterized gradient estimators, which are well-studied theoretically yet restrict both the choice of the data-generating process and the variational approximation. While REINFORCE gradient estimators do not suffer from such restrictions, they lack rigorous theoretical justification. In this paper, we provide the first comprehensive analysis of REINFORCE gradient estimators in importance weighted VI, leveraging this theoretical foundation to diagnose and resolve fundamental deficiencies in current state-of-the-art estimators. Specifically, we introduce and examine a generalized family of variational inference for Monte Carlo objectives (VIMCO) gradient estimators. We prove that state-of-the-art VIMCO gradient estimators exhibit a vanishing signal-to-noise ratio (SNR) as $N$ increases, which prevents effective optimization. To overcome this issue, we propose the novel VIMCO-$\star$ gradient estimator and show that it averts the SNR collapse of existing VIMCO gradient estimators by achieving a $\sqrt{N}$ SNR scaling instead. We demonstrate its superior empirical performance compared to current VIMCO implementations in challenging settings where reparameterized gradients are typically unavailable.

</details>


### [52] [Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function](https://arxiv.org/abs/2602.01466)
*Tuan Minh Pham,Thinh Cao,Viet Nguyen,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

TL;DR: 本文系统分析了多专家混合模型中sigmoid门控相对于softmax门控的优势，解决了现有文献中的三个关键问题，并提出了改进的Euclidean评分方法以解决温度参数带来的指数级样本复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 尽管sigmoid门控在混合专家模型中已被实证证明优于softmax门控，但现有研究存在三个未解决的关键问题：1）sigmoid门控在分类任务中的优势未得到理论证明；2）现有sigmoid门控模型可能无法收敛到真实参数；3）温度参数对sigmoid门控的理论影响尚未充分探索。

Method: 采用改进的sigmoid门控的多项式逻辑混合专家模型，通过理论分析比较sigmoid和softmax门控的样本复杂度。为解决温度参数带来的问题，提出用欧几里得评分替代传统的点积评分，消除温度参数与门控参数之间的内在交互作用。

Result: sigmoid门控在参数估计和专家估计方面都表现出比softmax门控更低的样本复杂度。然而，引入温度参数的sigmoid门控会导致指数级的样本复杂度。使用欧几里得评分替代点积评分后，样本复杂度从指数级降低到多项式级。

Conclusion: sigmoid门控在混合专家模型中确实优于softmax门控，特别是在分类任务中。通过采用欧几里得评分方法，可以克服温度参数带来的样本复杂度问题，使模型在实际应用中更加高效可行。

Abstract: The sigmoid gate in mixture-of-experts (MoE) models has been empirically shown to outperform the softmax gate across several tasks, ranging from approximating feed-forward networks to language modeling. Additionally, recent efforts have demonstrated that the sigmoid gate is provably more sample-efficient than its softmax counterpart under regression settings. Nevertheless, there are three notable concerns that have not been addressed in the literature, namely (i) the benefits of the sigmoid gate have not been established under classification settings; (ii) existing sigmoid-gated MoE models may not converge to their ground-truth; and (iii) the effects of a temperature parameter in the sigmoid gate remain theoretically underexplored. To tackle these open problems, we perform a comprehensive analysis of multinomial logistic MoE equipped with a modified sigmoid gate to ensure model convergence. Our results indicate that the sigmoid gate exhibits a lower sample complexity than the softmax gate for both parameter and expert estimation. Furthermore, we find that incorporating a temperature into the sigmoid gate leads to a sample complexity of exponential order due to an intrinsic interaction between the temperature and gating parameters. To overcome this issue, we propose replacing the vanilla inner product score in the gating function with a Euclidean score that effectively removes that interaction, thereby substantially improving the sample complexity to a polynomial order.

</details>


### [53] [Density-Informed Pseudo-Counts for Calibrated Evidential Deep Learning](https://arxiv.org/abs/2602.01477)
*Pietro Carlotti,Nevena Gligić,Arya Farahi*

Main category: stat.ML

TL;DR: EDL存在理论缺陷，将认知和偶然不确定性混为一谈，导致OOD数据过度自信。作者提出DIP-EDL新方法，通过分离条件标签分布和边际协变量密度来解耦不确定性，改善分布偏移下的鲁棒性和校准。


<details>
  <summary>Details</summary>
Motivation: 尽管EDL在不确定性感知分类中很流行，但其理论基础和在分布偏移下的行为仍不清楚。作者发现标准EDL将认知不确定性和偶然不确定性混为一谈，导致对OOD输入的系统性过度自信，需要解决这一缺陷。

Method: 提出DIP-EDL（密度信息伪计数EDL），通过分层贝叶斯模型中的摊销变分推断视角重新解释EDL。新方法分离条件标签分布和边际协变量密度估计，在高密度区域保留证据，对OOD数据将预测收缩到均匀先验。

Result: 理论上证明DIP-EDL实现渐近集中性。实证显示该方法提高了可解释性，改善了分布偏移下的鲁棒性和不确定性校准性能。

Conclusion: DIP-EDL解决了标准EDL的核心缺陷，通过解耦类别预测和不确定性大小，在分布偏移下提供更可靠的不确定性估计，为不确定性感知分类提供了更坚实的理论基础。

Abstract: Evidential Deep Learning (EDL) is a popular framework for uncertainty-aware classification that models predictive uncertainty via Dirichlet distributions parameterized by neural networks. Despite its popularity, its theoretical foundations and behavior under distributional shift remain poorly understood. In this work, we provide a principled statistical interpretation by proving that EDL training corresponds to amortized variational inference in a hierarchical Bayesian model with a tempered pseudo-likelihood. This perspective reveals a major drawback: standard EDL conflates epistemic and aleatoric uncertainty, leading to systematic overconfidence on out-of-distribution (OOD) inputs. To address this, we introduce Density-Informed Pseudo-count EDL (DIP-EDL), a new parametrization that decouples class prediction from the magnitude of uncertainty by separately estimating the conditional label distribution and the marginal covariate density. This separation preserves evidence in high-density regions while shrinking predictions toward a uniform prior for OOD data. Theoretically, we prove that DIP-EDL achieves asymptotic concentration. Empirically, we show that our method enhances interpretability and improves robustness and uncertainty calibration under distributional shift.

</details>


### [54] [Inference-Aware Meta-Alignment of LLMs via Non-Linear GRPO](https://arxiv.org/abs/2602.01603)
*Shokichi Takakura,Akifumi Wachi,Rei Higuchi,Kohei Miyaguchi,Taiji Suzuki*

Main category: stat.ML

TL;DR: 提出IAMA方法，通过元学习使基础模型能够通过不同的推理时对齐算法有效对齐到多个任务，解决多标准对齐的挑战并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐到多样化人类偏好具有根本性挑战，因为标准经常相互冲突。推理时对齐方法虽然允许通过不同算法对齐多个标准，但计算成本高，需要多次前向传播。

Method: 提出推理感知元对齐(IAMA)方法，训练基础模型使其能够通过不同的推理时对齐算法有效对齐到多个任务。为解决IAMA中的非线性优化问题，提出非线性GRPO算法，在概率测度空间中可证明收敛到最优解。

Result: IAMA使LLMs能够在推理时以有限计算预算对齐到多个标准，通过元学习优化基础模型，使其对各种推理时对齐算法都有效。

Conclusion: IAMA为解决大语言模型多标准对齐问题提供了一种高效方法，通过推理感知的元学习降低计算成本，同时保持对齐效果。

Abstract: Aligning large language models (LLMs) to diverse human preferences is fundamentally challenging since criteria can often conflict with each other. Inference-time alignment methods have recently gained popularity as they allow LLMs to be aligned to multiple criteria via different alignment algorithms at inference time. However, inference-time alignment is computationally expensive since it often requires multiple forward passes of the base model. In this work, we propose inference-aware meta-alignment (IAMA), a novel approach that enables LLMs to be aligned to multiple criteria with limited computational budget at inference time. IAMA trains a base model such that it can be effectively aligned to multiple tasks via different inference-time alignment algorithms. To solve the non-linear optimization problems involved in IAMA, we propose non-linear GRPO, which provably converges to the optimal solution in the space of probability measures.

</details>


### [55] [ST-BCP: Tightening Coverage Bound for Backward Conformal Prediction via Non-Conformity Score Transformation](https://arxiv.org/abs/2602.01733)
*Junxian Liu,Hao Zeng,Hongxin Wei*

Main category: stat.ML

TL;DR: ST-BCP是一种改进的反向共形预测方法，通过数据依赖的分数变换缩小覆盖率差距，将平均覆盖率差距从4.20%降至1.12%。


<details>
  <summary>Details</summary>
Motivation: 传统反向共形预测(BCP)使用马尔可夫不等式导致估计覆盖率与实际覆盖率之间存在显著差距，需要改进方法来缩小这一差距。

Method: 提出ST-BCP方法，引入数据依赖的非共形分数变换，开发可计算的变换函数，并证明其优于基线恒等变换。

Result: 在常见基准测试中，该方法将平均覆盖率差距从4.20%显著降低至1.12%，有效缩小了覆盖率差距。

Conclusion: ST-BCP通过数据依赖的分数变换有效改进了反向共形预测，显著缩小了覆盖率差距，为不确定性量化提供了更精确的方法。

Abstract: Conformal Prediction (CP) provides a statistical framework for uncertainty quantification that constructs prediction sets with coverage guarantees. While CP yields uncontrolled prediction set sizes, Backward Conformal Prediction (BCP) inverts this paradigm by enforcing a predefined upper bound on set size and estimating the resulting coverage guarantee. However, the looseness induced by Markov's inequality within the BCP framework causes a significant gap between the estimated coverage bound and the empirical coverage. In this work, we introduce ST-BCP, a novel method that introduces a data-dependent transformation of nonconformity scores to narrow the coverage gap. In particular, we develop a computable transformation and prove that it outperforms the baseline identity transformation. Extensive experiments demonstrate the effectiveness of our method, reducing the average coverage gap from 4.20\% to 1.12\% on common benchmarks.

</details>


### [56] [Transformers as Measure-Theoretic Associative Memory: A Statistical Perspective and Minimax Optimality](https://arxiv.org/abs/2602.01863)
*Ryotaro Kawata,Taiji Suzuki*

Main category: stat.ML

TL;DR: 本文提出了一种基于概率测度的Transformer理论框架，将上下文视为token分布，注意力视为测度上的积分算子，证明了浅层Transformer在谱假设下能够学习"回忆-预测"映射，并建立了匹配的极小极大下界。


<details>
  <summary>Details</summary>
Motivation: Transformer通过内容寻址检索和利用理论上无限长上下文的能力表现出色。本文旨在从概率测度的角度重新构建关联记忆，将上下文视为token分布，注意力视为测度上的积分算子，为设计和分析能够从任意长分布上下文中回忆的Transformer提供理论框架。

Method: 将上下文重新表述为混合分布ν = I⁻¹∑μ⁽ⁱ*⁾，查询为x_q(i*)。任务分解为：(1)回忆相关分量μ⁽ⁱ*⁾，(2)从(μ⁽ⁱ*⁾, x_q)进行预测。研究通过经验风险最小化训练的softmax注意力（非冻结核），证明在输入密度的谱假设下，浅层测度论Transformer与MLP组合能够学习回忆-预测映射。

Result: 证明了浅层测度论Transformer能够学习回忆-预测映射，并建立了匹配的极小极大下界（收敛阶相同，仅差乘法常数），证明了收敛阶的尖锐性。该框架为设计和分析能够从任意长分布上下文中回忆的Transformer提供了理论保证。

Conclusion: 该框架为设计和分析能够从任意长分布上下文中回忆的Transformer提供了原则性方法，具有可证明的泛化保证。通过将Transformer重新表述为测度上的积分算子，为理解其处理分布上下文的能力提供了理论基础。

Abstract: Transformers excel through content-addressable retrieval and the ability to exploit contexts of, in principle, unbounded length. We recast associative memory at the level of probability measures, treating a context as a distribution over tokens and viewing attention as an integral operator on measures. Concretely, for mixture contexts $ν= I^{-1} \sum_{i=1}^I μ^{(i^*)}$ and a query $x_{\mathrm{q}}(i^*)$, the task decomposes into (i) recall of the relevant component $μ^{(i^*)}$ and (ii) prediction from $(μ_{i^*},x_\mathrm{q})$. We study learned softmax attention (not a frozen kernel) trained by empirical risk minimization and show that a shallow measure-theoretic Transformer composed with an MLP learns the recall-and-predict map under a spectral assumption on the input densities. We further establish a matching minimax lower bound with the same rate exponent (up to multiplicative constants), proving sharpness of the convergence order. The framework offers a principled recipe for designing and analyzing Transformers that recall from arbitrarily long, distributional contexts with provable generalization guarantees.

</details>


### [57] [Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration](https://arxiv.org/abs/2602.01912)
*Du-Yi Wang,Guo Liang,Kun Zhang,Qianwen Zhu*

Main category: stat.ML

TL;DR: 提出基于离线模拟在线估计框架的分位数回归森林方法，结合保形校准实现实时风险价值可靠估计


<details>
  <summary>Details</summary>
Motivation: 市场条件快速变化需要实时风险监控，但风险价值的在线估计仍然具有挑战性，需要准确可靠的实时风险价值估计来进行及时风险控制和明智决策

Method: 采用离线模拟在线估计框架，使用分位数回归森林离线学习在线风险价值与风险因素之间的关系，然后在线结合观测到的风险因素生成实时风险价值估计，并开发保形化估计器来校准在线风险价值估计以确保可靠性

Result: 理论分析建立了所提估计器的一致性和覆盖有效性，数值实验证实了该方法的有效性，并展示了在实际应用中的优势

Conclusion: 首次基于离线模拟在线估计框架利用保形校准实现可靠实时风险价值估计，为实时风险监控提供了有效解决方案

Abstract: Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk control and informed decision-making. We propose to use the quantile regression forest in the offline-simulation-online-estimation (OSOA) framework. Specifically, the quantile regression forest is trained offline to learn the relationship between the online VaR and risk factors, and real-time VaR estimates are then produced online by incorporating observed risk factors. To further ensure reliability, we develop a conformalized estimator that calibrates the online VaR estimates. To the best of our knowledge, we are the first to leverage conformal calibration to estimate real-time VaR reliably based on the OSOA formulation. Theoretical analysis establishes the consistency and coverage validity of the proposed estimators. Numerical experiments confirm the proposed method and demonstrate its effectiveness in practice.

</details>


### [58] [Privacy Amplification by Missing Data](https://arxiv.org/abs/2602.01928)
*Simon Roburin,Rafaël Pinot,Erwan Scornet*

Main category: stat.ML

TL;DR: 论文提出缺失数据可作为隐私增强机制，首次证明不完整数据能为差分隐私算法提供隐私放大效果


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融等高敏感领域，隐私保护是基本要求，但这些领域的数据常存在缺失值。传统上缺失数据被视为限制，但本文从隐私保护角度重新审视缺失数据，认为缺失性可能天然增强隐私保护

Method: 在差分隐私框架下，将缺失数据形式化为隐私放大机制进行分析。通过理论分析证明不完整数据如何为差分隐私算法提供隐私保护增强

Result: 首次证明不完整数据能产生隐私放大效果，即缺失数据可以增强差分隐私算法的隐私保护能力

Conclusion: 缺失数据不应仅被视为限制，而应被重新评估为有价值的隐私增强机制，为高敏感领域的数据分析提供了新的隐私保护视角

Abstract: Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.

</details>


### [59] [Stochastic Interpolants in Hilbert Spaces](https://arxiv.org/abs/2602.01988)
*James Boran Yu,RuiKang OuYang,Julien Horwood,José Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: 本文提出了一个在无限维希尔伯特空间中构建随机插值的理论框架，将原本局限于有限维的随机插值方法扩展到函数值数据，为任意函数分布之间的生成桥接提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成功扩展到函数值数据，但随机插值方法（提供连接任意分布的灵活方式）仍局限于有限维设置。本文旨在填补这一空白，为无限维希尔伯特空间中的随机插值建立严格的理论框架。

Method: 建立无限维希尔伯特空间中随机插值的理论框架，包括证明适定性和提供显式误差界。将框架应用于条件生成，特别关注复杂的基于PDE的基准测试。

Result: 提出的框架能够实现任意函数分布之间的生成桥接，在条件生成任务中取得了最先进的结果，特别是在复杂的PDE基准测试上表现出色。

Conclusion: 该工作为无限维空间中的随机插值提供了全面的理论基础，使其成为科学发现的强大通用工具，成功将随机插值方法从有限维扩展到函数值数据领域。

Abstract: Although diffusion models have successfully extended to function-valued data, stochastic interpolants -- which offer a flexible way to bridge arbitrary distributions -- remain limited to finite-dimensional settings. This work bridges this gap by establishing a rigorous framework for stochastic interpolants in infinite-dimensional Hilbert spaces. We provide comprehensive theoretical foundations, including proofs of well-posedness and explicit error bounds. We demonstrate the effectiveness of the proposed framework for conditional generation, focusing particularly on complex PDE-based benchmarks. By enabling generative bridges between arbitrary functional distributions, our approach achieves state-of-the-art results, offering a powerful, general-purpose tool for scientific discovery.

</details>


### [60] [Training-free score-based diffusion for parameter-dependent stochastic dynamical systems](https://arxiv.org/abs/2602.02113)
*Minglei Yang,Sicheng He*

Main category: stat.ML

TL;DR: 提出无需训练的条件扩散模型框架，用于学习参数依赖SDE的随机流映射，通过联合核加权蒙特卡洛估计器近似条件得分函数，实现参数域内插值。


<details>
  <summary>Details</summary>
Motivation: 模拟参数依赖随机微分方程计算成本高，传统方法需要对每个参数值单独进行高保真模拟。现有机器学习方法要么需要昂贵的神经网络训练来估计得分函数，要么无法处理连续参数依赖。

Method: 提出训练免费的条件扩散模型框架，使用联合核加权蒙特卡洛估计器近似条件得分函数。该估计器利用离散参数值采样的轨迹数据，实现状态空间和连续参数域的双重插值。

Result: 该方法在三个复杂度递增的数值示例中表现出色，能够准确近似不同参数值下的条件分布。训练后的生成模型可在训练范围内为任意参数值生成样本轨迹，无需重新训练。

Conclusion: 该方法显著加速了参数研究、不确定性量化和实时滤波应用，为解决参数依赖SDE的模拟问题提供了高效的计算框架。

Abstract: Simulating parameter-dependent stochastic differential equations (SDEs) presents significant computational challenges, as separate high-fidelity simulations are typically required for each parameter value of interest. Despite the success of machine learning methods in learning SDE dynamics, existing approaches either require expensive neural network training for score function estimation or lack the ability to handle continuous parameter dependence. We present a training-free conditional diffusion model framework for learning stochastic flow maps of parameter-dependent SDEs, where both drift and diffusion coefficients depend on physical parameters. The key technical innovation is a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values, enabling interpolation across both state space and the continuous parameter domain. Once trained, the resulting generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. The performance of the proposed approach is demonstrated via three numerical examples of increasing complexity, showing accurate approximation of conditional distributions across varying parameter values.

</details>


### [61] [Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model](https://arxiv.org/abs/2602.02153)
*Onat Ure,Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 该研究通过构建可控制高阶统计量的非高斯数据模型，揭示了神经网络学习过程中从低阶统计量到高阶统计量的渐进学习规律。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解数据的高阶统计量（如偏度和峰度）如何影响神经网络的学习动态。当前研究多基于简化的高斯数据假设，而实际数据具有复杂的非高斯特性，需要更真实的数据模型来探究分布特性对学习的影响。

Method: 方法包括：1）构建基于生成式两层神经网络的可控非高斯数据模型，使用Hermite多项式展开激活函数以控制高阶累积量；2）用该数据模型生成样本进行受控在线学习实验；3）在Fashion-MNIST数据集上预训练生成模型，验证结论在实际场景中的适用性。

Result: 实验结果揭示了神经网络训练中的"矩级渐进"现象：网络首先学习低阶统计量（均值和协方差），然后逐步学习高阶累积量。在Fashion-MNIST上的实验进一步证实了这一结论，并展示了数据模型在实际场景中的实用性。

Conclusion: 该研究提出的方法在简化数据假设和实际数据复杂性之间建立了桥梁，为机器学习和信号处理中研究分布效应提供了一个原则性框架，有助于深入理解神经网络如何从数据中学习统计规律。

Abstract: We study the effect of high-order statistics of data on the learning dynamics of neural networks (NNs) by using a moment-controllable non-Gaussian data model. Considering the expressivity of two-layer neural networks, we first construct the data model as a generative two-layer NN where the activation function is expanded by using Hermite polynomials. This allows us to achieve interpretable control over high-order cumulants such as skewness and kurtosis through the Hermite coefficients while keeping the data model realistic. Using samples generated from the data model, we perform controlled online learning experiments with a two-layer NN. Our results reveal a moment-wise progression in training: networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants. Finally, we pretrain the generative model on the Fashion-MNIST dataset and leverage the generated samples for further experiments. The results of these additional experiments confirm our conclusions and show the utility of the data model in a real-world scenario. Overall, our proposed approach bridges simplified data assumptions and practical data complexity, which offers a principled framework for investigating distributional effects in machine learning and signal processing.

</details>


### [62] [PCA of probability measures: Sparse and Dense sampling regimes](https://arxiv.org/abs/2602.02190)
*Gachon Erell,Jérémie Bigot,Elsa Cazelles*

Main category: stat.ML

TL;DR: 该论文研究了概率测度PCA的双重渐近框架，推导了协方差算子和PCA超额风险的收敛速率，揭示了从稀疏到密集的过渡行为，并证明了密集区域的最优性。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要关注从m个样本估计单个测度嵌入的收敛速率，但缺乏对涉及多个测度的PCA收敛分析。本文旨在填补这一空白，研究当观测到n个概率测度且每个测度有m个样本时的PCA收敛行为。

Method: 采用双重渐近框架，将概率测度嵌入希尔伯特空间，然后应用标准函数PCA技术。理论分析推导了经验协方差算子和PCA超额风险的收敛速率，形式为n^{-1/2} + m^{-α}，其中α取决于嵌入选择。通过数值实验验证理论结果，并探索子采样策略。

Result: 建立了收敛速率n^{-1/2} + m^{-α}，揭示了从稀疏（小m）到密集（大m）的过渡行为。证明了密集区域的经验协方差误差是最小最大最优的。数值实验验证了理论速率，并显示适当子采样能在保持PCA精度的同时降低计算成本。

Conclusion: 本文首次系统分析了多测度PCA的双重渐近收敛行为，提供了理论保证和实用指导。研究揭示了样本数量与测度数量之间的权衡关系，为实际应用中资源分配和算法设计提供了理论依据。

Abstract: A common approach to perform PCA on probability measures is to embed them into a Hilbert space where standard functional PCA techniques apply. While convergence rates for estimating the embedding of a single measure from $m$ samples are well understood, the literature has not addressed the setting involving multiple measures. In this paper, we study PCA in a double asymptotic regime where $n$ probability measures are observed, each through $m$ samples. We derive convergence rates of the form $n^{-1/2} + m^{-α}$ for the empirical covariance operator and the PCA excess risk, where $α>0$ depends on the chosen embedding. This characterizes the relationship between the number $n$ of measures and the number $m$ of samples per measure, revealing a sparse (small $m$) to dense (large $m$) transition in the convergence behavior. Moreover, we prove that the dense-regime rate is minimax optimal for the empirical covariance error. Our numerical experiments validate these theoretical rates and demonstrate that appropriate subsampling preserves PCA accuracy while reducing computational cost.

</details>


### [63] [Transfer Learning Through Conditional Quantile Matching](https://arxiv.org/abs/2602.02358)
*Yikun Zhang,Steven Wilkins-Reeves,Wesley Lee,Aude Hofleitner*

Main category: stat.ML

TL;DR: 提出一种用于回归的迁移学习框架，通过异质源域改善数据稀缺目标域的预测性能，使用条件生成模型和分位数匹配进行分布对齐


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺目标域的预测问题，利用异质源域的知识提升性能，避免传统迁移学习中严格的分布假设限制

Method: 为每个源域学习独立的条件生成模型，通过条件分位数匹配将生成的响应校准到目标域，实现分布对齐而不需要协变量或标签偏移等限制性假设

Result: 理论证明：在增强数据集上训练的ERM比仅使用目标数据的ERM具有更紧的过剩风险界；实践验证：在模拟和真实数据应用中，该方法持续优于仅目标域学习和竞争迁移学习方法

Conclusion: 该框架为下游学习任务提供了原则性、灵活的高质量数据增强方法，在理论和实践上均表现出优越性

Abstract: We introduce a transfer learning framework for regression that leverages heterogeneous source domains to improve predictive performance in a data-scarce target domain. Our approach learns a conditional generative model separately for each source domain and calibrates the generated responses to the target domain via conditional quantile matching. This distributional alignment step corrects general discrepancies between source and target domains without imposing restrictive assumptions such as covariate or label shift. The resulting framework provides a principled and flexible approach to high-quality data augmentation for downstream learning tasks in the target domain. From a theoretical perspective, we show that an empirical risk minimizer (ERM) trained on the augmented dataset achieves a tighter excess risk bound than the target-only ERM under mild conditions. In particular, we establish new convergence rates for the quantile matching estimator that governs the transfer bias-variance tradeoff. From a practical perspective, extensive simulations and real data applications demonstrate that the proposed method consistently improves prediction accuracy over target-only learning and competing transfer learning methods.

</details>


### [64] [Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function](https://arxiv.org/abs/2602.02406)
*Tung Quoc Le,Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: stat.ML

TL;DR: 提出首个多维超参数调优的泛化保证框架，利用实代数几何工具，为数据驱动的算法设计提供理论支撑


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的算法设计缺乏统计理论基础，特别是对于多维超参数调优问题，现有理论只解决了标量超参数的情况，而实际应用中超参数往往是多维的且性能依赖关系复杂

Method: 通过加强半代数函数类的泛化保证框架，利用实代数几何工具，建立多维超参数调优的泛化理论，并在验证损失最小化假设下扩展分析，推导出改进的边界

Result: 建立了首个多维超参数调优的泛化保证框架，获得了更尖锐、更广泛适用的保证，展示了框架在加权组lasso和加权融合lasso等新可学习性结果中的应用

Conclusion: 该工作填补了数据驱动算法设计中多维超参数调优的理论空白，为实际应用提供了坚实的统计基础，并通过具体算法案例展示了框架的广泛适用性

Abstract: Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.

</details>


### [65] [Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning](https://arxiv.org/abs/2602.02431)
*Filip Kovačević,Hong Chang Ji,Denny Wu,Mahdi Soltanolkotabi,Marco Mondelli*

Main category: stat.ML

TL;DR: 本文研究了全批次梯度下降(GD)与单次随机梯度下降(online SGD)在统计效率上的理论差异，发现在学习二次激活的单索引模型时，通过截断激活函数，全批次GD可以在n≈d样本下实现更好的优化景观，从而超越单次SGD。


<details>
  <summary>Details</summary>
Motivation: 虽然传统观点认为重复使用训练数据可以提高梯度学习的统计效率，但除了线性回归之外，全批次梯度下降(GD)相对于单次随机梯度下降(online SGD)的理论优势尚不清楚。本文旨在探索在更复杂的模型（如单索引模型）中，数据重用是否能带来统计效率的显著提升。

Method: 研究d维二次激活的单索引模型学习问题。首先分析全批次球形GD在相关性损失上的样本复杂度，发现其与单次SGD一样需要n≳d log d样本。然后通过截断激活函数，分析全批次GD在平方损失上的优化景观。最后对从小初始化开始的全批次GD轨迹进行分析。

Result: 1. 全批次球形GD在相关性损失上需要n≳d log d样本才能实现弱恢复，与单次SGD相同；2. 通过截断激活函数，全批次GD在n≈d样本下展现出有利的优化景观，统计效率优于单次SGD；3. 从小初始化开始的全批次GD在平方损失上，n≳d样本和T≳log d梯度步数足以实现强恢复。

Conclusion: 数据重用确实能提高统计效率，但需要适当的激活函数设计（如截断）。对于二次激活的单索引模型，全批次GD通过截断激活可以在n≈d样本下超越单次SGD，而单次SGD需要n≳d log d样本。这为理解数据重用在不同学习算法中的优势提供了理论依据。

Abstract: It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery. We first show that this $\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \gtrsim d$ samples and $T \gtrsim\log d$ gradient steps suffice to achieve strong (exact) recovery.

</details>
