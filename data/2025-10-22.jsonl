{"id": "2510.18598", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.18598", "abs": "https://arxiv.org/abs/2510.18598", "authors": ["Lujia Bai", "Holger Dette"], "title": "Measuring deviations from spherical symmetry", "comment": null, "summary": "Most of the work on checking spherical symmetry assumptions on the\ndistribution of the $p$-dimensional random vector $Y$ has its focus on\nstatistical tests for the null hypothesis of exact spherical symmetry. In this\npaper, we take a different point of view and propose a measure for the\ndeviation from spherical symmetry, which is based on the minimum distance\nbetween the distribution of the vector $\\big (\\|Y\\|, Y/ \\|Y\\| )^\\top $ and its\nbest approximation by a distribution of a vector $\\big (\\|Y_s\\|, Y_s/ \\|Y_s \\|\n)^\\top $ corresponding to a random vector $Y_s$ with a spherical distribution.\nWe develop estimators for the minimum distance with corresponding statistical\nguarantees (provided by asymptotic theory) and demonstrate the applicability of\nour approach by means of a simulation study and a real data example."}
{"id": "2510.18843", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.18843", "abs": "https://arxiv.org/abs/2510.18843", "authors": ["Pawel Morzywolek", "Peter B. Gilbert", "Alex Luedtke"], "title": "Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects", "comment": "40 pages, 7 figures", "summary": "We provide an inferential framework to assess variable importance for\nheterogeneous treatment effects. This assessment is especially useful in\nhigh-risk domains such as medicine, where decision makers hesitate to rely on\nblack-box treatment recommendation algorithms. The variable importance measures\nwe consider are local in that they may differ across individuals, while the\ninference is global in that it tests whether a given variable is important for\nany individual. Our approach builds on recent developments in semiparametric\ntheory for function-valued parameters, and is valid even when statistical\nmachine learning algorithms are employed to quantify treatment effect\nheterogeneity. We demonstrate the applicability of our method to infectious\ndisease prevention strategies."}
{"id": "2510.18252", "categories": ["stat.AP", "cs.AI", "cs.LG", "62H30", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.18252", "abs": "https://arxiv.org/abs/2510.18252", "authors": ["Luis H. Chia"], "title": "Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN", "comment": "25 pages, 3 figures, 6 tables", "summary": "Credit scoring models face a critical challenge: severe class imbalance, with\ndefault rates typically below 10%, which hampers model learning and predictive\nperformance. While synthetic data augmentation techniques such as SMOTE and\nADASYN have been proposed to address this issue, the optimal augmentation ratio\nremains unclear, with practitioners often defaulting to full balancing (1:1\nratio) without empirical justification.\n  This study systematically evaluates 10 data augmentation scenarios using the\nGive Me Some Credit dataset (97,243 observations, 7% default rate), comparing\nSMOTE, BorderlineSMOTE, and ADASYN at different multiplication factors (1x, 2x,\n3x). All models were trained using XGBoost and evaluated on a held-out test set\nof 29,173 real observations. Statistical significance was assessed using\nbootstrap testing with 1,000 iterations.\n  Key findings reveal that ADASYN with 1x multiplication (doubling the minority\nclass) achieved optimal performance with AUC of 0.6778 and Gini coefficient of\n0.3557, representing statistically significant improvements of +0.77% and\n+3.00% respectively (p = 0.017, bootstrap test). Higher multiplication factors\n(2x and 3x) resulted in performance degradation, with 3x showing a -0.48%\ndecrease in AUC, suggesting a \"law of diminishing returns\" for synthetic\noversampling. The optimal class imbalance ratio was found to be 6.6:1\n(majority:minority), contradicting the common practice of balancing to 1:1.\n  This work provides the first empirical evidence of an optimal \"sweet spot\"\nfor data augmentation in credit scoring, with practical guidelines for industry\npractitioners and researchers working with imbalanced datasets. While\ndemonstrated on a single representative dataset, the methodology provides a\nreproducible framework for determining optimal augmentation ratios in other\nimbalanced domains."}
{"id": "2510.17886", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17886", "abs": "https://arxiv.org/abs/2510.17886", "authors": ["Angelo Giorgio", "Riki Nagasawa", "Shuta Yokoi", "Tomoyuki Obuchi", "Hajime Yoshino"], "title": "Graphical model for tensor factorization by sparse sampling", "comment": "75 pages, 26 figures", "summary": "We consider tensor factorizations based on sparse measurements of the tensor\ncomponents. The measurements are designed in a way that the underlying graph of\ninteractions is a random graph. The setup will be useful in cases where a\nsubstantial amount of data is missing, as in recommendation systems heavily\nused in social network services. In order to obtain theoretical insights on the\nsetup, we consider statistical inference of the tensor factorization in a high\ndimensional limit, which we call as dense limit, where the graphs are large and\ndense but not fully connected. We build message-passing algorithms and test\nthem in a Bayes optimal teacher-student setting. We also develop a replica\ntheory, which becomes exact in the dense limit,to examine the performance of\nstatistical inference."}
{"id": "2510.18067", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18067", "abs": "https://arxiv.org/abs/2510.18067", "authors": ["Nian Liu", "Jian Cao"], "title": "Principled Argo Modeling using Vecchia-based Gaussian Processes", "comment": null, "summary": "Argo is an international program that collects temperature and salinity\nobservations in the upper two kilometers of the global ocean. Most existing\napproaches for modeling Argo temperature rely on spatial partitioning, where\ndata are locally modeled by first estimating a prescribed mean structure and\nthen fitting Gaussian processes (GPs) to the mean-subtracted anomalies. Such\nstrategies introduce challenges in designing suitable mean structures and\ndefining domain partitions, often resulting in ad hoc modeling choices. In this\nwork, we propose a one-stop Gaussian process regression framework with a\ngeneric spatio-temporal covariance function to jointly model Argo temperature\ndata across broad spatial domains. Our fully data-driven approach achieves\nsuperior predictive performance compared with methods that require domain\npartitioning or parametric regression. To ensure scalability over large spatial\nregions, we employ the Vecchia approximation, which reduces the computational\ncomplexity from cubic to quasi-linear in the number of observations while\npreserving predictive accuracy. Using Argo data from January to March over the\nyears 2007-2016, the same dataset used in prior benchmark studies, we\ndemonstrate that our approach provides a principled, scalable, and\ninterpretable tool for large-scale oceanographic analysis."}
{"id": "2510.18639", "categories": ["stat.AP", "q-fin.RM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18639", "abs": "https://arxiv.org/abs/2510.18639", "authors": ["Samuel Perreault", "Silvana M. Pesenti", "Daniyal Shahzad"], "title": "Distributional regression for seasonal data: an application to river flows", "comment": null, "summary": "Risk assessment in casualty insurance, such as flood risk, traditionally\nrelies on extreme-value methods that emphasizes rare events. These approaches\nare well-suited for characterizing tail risk, but do not capture the broader\ndynamics of environmental variables such as moderate or frequent loss events.\nTo complement these methods, we propose a modelling framework for estimating\nthe full (daily) distribution of environmental variables as a function of time,\nthat is a distributional version of typical climatological summary statistics,\nthereby incorporating both seasonal variation and gradual long-term changes.\nAside from the time trend, to capture seasonal variation our approach\nsimultaneously estimates the distribution for each instant of the seasonal\ncycle, without explicitly modelling the temporal dependence present in the\ndata. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive\nModels for Location, Scale, and Shape), where the parameters of the\ndistribution vary over the seasonal cycle as a function of explanatory\nvariables depending only on the time of year, and not on the past values of the\nprocess under study. Ignoring the temporal dependence in the seasonal variation\ngreatly simplifies the modelling but poses inference challenges that we clarify\nand overcome.\n  We apply our framework to daily river flow data from three hydrometric\nstations along the Fraser River in British Columbia, Canada, and analyse the\nflood of the Fraser River in early winter of 2021."}
{"id": "2510.17903", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17903", "abs": "https://arxiv.org/abs/2510.17903", "authors": ["Chuansen Peng", "Xiaojing Shen"], "title": "Learning Time-Varying Graphs from Incomplete Graph Signals", "comment": null, "summary": "This paper tackles the challenging problem of jointly inferring time-varying\nnetwork topologies and imputing missing data from partially observed graph\nsignals. We propose a unified non-convex optimization framework to\nsimultaneously recover a sequence of graph Laplacian matrices while\nreconstructing the unobserved signal entries. Unlike conventional decoupled\nmethods, our integrated approach facilitates a bidirectional flow of\ninformation between the graph and signal domains, yielding superior robustness,\nparticularly in high missing-data regimes. To capture realistic network\ndynamics, we introduce a fused-lasso type regularizer on the sequence of\nLaplacians. This penalty promotes temporal smoothness by penalizing large\nsuccessive changes, thereby preventing spurious variations induced by noise\nwhile still permitting gradual topological evolution. For solving the joint\noptimization problem, we develop an efficient Alternating Direction Method of\nMultipliers (ADMM) algorithm, which leverages the problem's structure to yield\nclosed-form solutions for both the graph and signal subproblems. This design\nensures scalability to large-scale networks and long time horizons. On the\ntheoretical front, despite the inherent non-convexity, we establish a\nconvergence guarantee, proving that the proposed ADMM scheme converges to a\nstationary point. Furthermore, we derive non-asymptotic statistical guarantees,\nproviding high-probability error bounds for the graph estimator as a function\nof sample size, signal smoothness, and the intrinsic temporal variability of\nthe graph. Extensive numerical experiments validate the approach, demonstrating\nthat it significantly outperforms state-of-the-art baselines in both\nconvergence speed and the joint accuracy of graph learning and signal recovery."}
{"id": "2510.18777", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18777", "abs": "https://arxiv.org/abs/2510.18777", "authors": ["Yen-Chi Chen"], "title": "A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models", "comment": "This is an introduction paper. 28 pages, 2 figures", "summary": "While Variational Inference (VI) is central to modern generative models like\nVariational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\npedagogical treatment is split across disciplines. In statistics, VI is\ntypically framed as a Bayesian method for posterior approximation. In machine\nlearning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\nwhere VI is used to approximate a maximum likelihood estimator. This creates a\nbarrier for statisticians, as the principles behind VAEs and DDMs are hard to\ncontextualize without a corresponding Frequentist introduction to VI. This\npaper provides that introduction: we explain the theory for VI, VAEs, and DDMs\nfrom a purely Frequentist perspective, starting with the classical\nExpectation-Maximization (EM) algorithm. We show how VI arises as a scalable\nsolution for intractable E-steps and how VAEs and DDMs are natural,\ndeep-learning-based extensions of this framework, thereby bridging the gap\nbetween classical statistical inference and modern generative AI."}
{"id": "2510.18818", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18818", "abs": "https://arxiv.org/abs/2510.18818", "authors": ["Jay JH Park", "Rebecca K. Metcalfe", "Nathaniel Dyrkton", "Yichen Yan", "Shomoita Alam", "Kevin Phelan", "Ibrahim Sana", "Susan Shepherd"], "title": "Comparison of Simulation-Guided Design to Closed-Form Power Calculations in Planning a Cluster Randomized Trial with Covariate-Constrained Randomization: A Case Study in Rural Chad", "comment": null, "summary": "Current practices for designing cluster-randomized trials (cRCTs) typically\nrely on closed-form formulas for power calculations. For cRCTs using\ncovariate-constrained randomization, the utility of conventional calculations\nmight be limited, particularly when data is nested. We compared\nsimulation-based planning of a nested cRCT using covariate-constrained\nrandomization to conventional power calculations using OptiMAx-Chad as a case\nstudy. OptiMAx-Chad will examine the impact of embedding mass distribution of\nsmall-quantity lipid-based nutrient supplements within an expanded programme on\nimmunization on first-dose measles-containing vaccine (MCV1) coverage among\nchildren aged 12-24 months in rural villages in Ngouri. Within the 12 health\nareas to be randomized, a random subset of villages will be selected for\noutcome collection. 1,000,000 assignments of health areas with different\npossible village selections were generated using covariate-constrained\nrandomization to balance baseline village characteristics. The empirically\nestimated intracluster correlation coefficient (ICC) and the World Health\nOrganization (WHO) recommended values of 1/3 and 1/6 were considered. The\ndesired operating characteristics were 80% power at 0.05 one-sided type I error\nrate. Using conventional calculations target power for a realistic treatment\neffect could not be achieved with the WHO recommended values. Conventional\ncalculations also showed a plateau in power after a certain cluster size. Our\nsimulations matched the design of OptiMAx-Chad with covariate adjustment and\nrandom selection, and showed that power did not plateau. Instead, power\nincreased with increasing cluster size. Planning complex cRCTs with covariate\nconstrained randomization and a multi-nested data structure with conventional\nclosed-form formulas can be misleading. Simulations can improve the planning of\ncRCTs."}
{"id": "2510.18071", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18071", "abs": "https://arxiv.org/abs/2510.18071", "authors": ["Yixin Fang", "Weili He"], "title": "Arbitrated Indirect Treatment Comparisons", "comment": null, "summary": "Matching-adjusted indirect comparison (MAIC) has been increasingly employed\nin health technology assessments (HTA). By reweighting subjects from a trial\nwith individual participant data (IPD) to match the covariate summary\nstatistics of another trial with only aggregate data (AgD), MAIC facilitates\nthe estimation of a treatment effect defined with respect to the AgD trial\npopulation. This manuscript introduces a new class of methods, termed\narbitrated indirect treatment comparisons, designed to address the ``MAIC\nparadox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox\narises when different sponsors, analyzing the same data, reach conflicting\nconclusions regarding which treatment is more effective. The underlying issue\nis that each sponsor implicitly targets a different population. To resolve this\ninconsistency, the proposed methods focus on estimating treatment effects in a\ncommon target population, specifically chosen to be the overlap population."}
{"id": "2411.17464", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2411.17464", "abs": "https://arxiv.org/abs/2411.17464", "authors": ["Arís Fanjul-Hevia", "Juan Carlos Pardo-Fernández", "Wenceslao González-Manteiga"], "title": "A new test for assessing the covariate effect in ROC curves", "comment": "17 pages, 5 figures, 5 tables", "summary": "The ROC curve is a statistical tool that analyses the accuracy of a\ndiagnostic test in which a variable is used to decide whether an individual is\nhealthy or not. Along with that diagnostic variable it is usual to have\ninformation of some other covariates. In some situations it is advisable to\nincorporate that information into the study, as the performance of the ROC\ncurves can be affected by them. Using the covariate-adjusted, the\ncovariate-specific or the pooled ROC curves we discuss how to decide if we can\nexclude the covariates from our study or not, and the implications this may\nhave in further analyses of the ROC curve. A new test for comparing the\ncovariate-adjusted and the pooled ROC curve is proposed, and the problem is\nillustrated by analysing a real database."}
{"id": "2510.18120", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18120", "abs": "https://arxiv.org/abs/2510.18120", "authors": ["Tongtong Liang", "Alexander Cloninger", "Rahul Parhi", "Yu-Xiang Wang"], "title": "Generalization Below the Edge of Stability: The Role of Data Geometry", "comment": "Under Review. Comments welcome!", "summary": "Understanding generalization in overparameterized neural networks hinges on\nthe interplay between the data geometry, neural architecture, and training\ndynamics. In this paper, we theoretically explore how data geometry controls\nthis implicit bias. This paper presents theoretical results for\noverparameterized two-layer ReLU networks trained below the edge of stability.\nFirst, for data distributions supported on a mixture of low-dimensional balls,\nwe derive generalization bounds that provably adapt to the intrinsic dimension.\nSecond, for a family of isotropic distributions that vary in how strongly\nprobability mass concentrates toward the unit sphere, we derive a spectrum of\nbounds showing that rates deteriorate as the mass concentrates toward the\nsphere. These results instantiate a unifying principle: When the data is harder\nto \"shatter\" with respect to the activation thresholds of the ReLU neurons,\ngradient descent tends to learn representations that capture shared patterns\nand thus finds solutions that generalize well. On the other hand, for data that\nis easily shattered (e.g., data supported on the sphere) gradient descent\nfavors memorization. Our theoretical results consolidate disparate empirical\nfindings that have appeared in the literature."}
{"id": "2510.17946", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.17946", "abs": "https://arxiv.org/abs/2510.17946", "authors": ["Sanjan C. Muchandimath", "Joaquim R. R. A. Martins", "Alex A. Gorodetsky"], "title": "Accelerating Bayesian Inference via Multi-Fidelity Transport Map Coupling", "comment": null, "summary": "Mathematical models in computational physics contain uncertain parameters\nthat impact prediction accuracy. In turbulence modeling, this challenge is\nespecially significant: Reynolds averaged Navier-Stokes (RANS) models, such as\nthe Spalart-Allmaras (SA) model, are widely used for their speed and robustness\nbut often suffer from inaccuracies and associated uncertainties due to\nimperfect model parameters. Reliable quantification of these uncertainties is\nbecoming increasingly important in aircraft certification by analysis, where\npredictive credibility is critical. Bayesian inference provides a framework to\nestimate these parameters and quantify output uncertainty, but traditional\nmethods are prohibitively expensive, especially when relying on high-fidelity\nsimulations. We address the challenge of expensive Bayesian parameter\nestimation by developing a multi-fidelity framework that combines Markov chain\nMonte Carlo (MCMC) methods with multilevel Monte Carlo (MLMC) estimators to\nefficiently solve inverse problems. The MLMC approach requires correlated\nsamples across different fidelity levels, achieved through a novel transport\nmap-based coupling algorithm. We demonstrate a 50% reduction in inference cost\ncompared to traditional single-fidelity methods on the challenging NACA0012\nairfoil at high angles of attack near stall, while delivering realistic\nuncertainty bounds for model predictions in complex separated flow regimes.\nThese results demonstrate that multi-fidelity approaches significantly improve\nturbulence parameter calibration, paving the way for more accurate and\nefficient aircraft certification by analysis."}
{"id": "2510.17946", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.17946", "abs": "https://arxiv.org/abs/2510.17946", "authors": ["Sanjan C. Muchandimath", "Joaquim R. R. A. Martins", "Alex A. Gorodetsky"], "title": "Accelerating Bayesian Inference via Multi-Fidelity Transport Map Coupling", "comment": null, "summary": "Mathematical models in computational physics contain uncertain parameters\nthat impact prediction accuracy. In turbulence modeling, this challenge is\nespecially significant: Reynolds averaged Navier-Stokes (RANS) models, such as\nthe Spalart-Allmaras (SA) model, are widely used for their speed and robustness\nbut often suffer from inaccuracies and associated uncertainties due to\nimperfect model parameters. Reliable quantification of these uncertainties is\nbecoming increasingly important in aircraft certification by analysis, where\npredictive credibility is critical. Bayesian inference provides a framework to\nestimate these parameters and quantify output uncertainty, but traditional\nmethods are prohibitively expensive, especially when relying on high-fidelity\nsimulations. We address the challenge of expensive Bayesian parameter\nestimation by developing a multi-fidelity framework that combines Markov chain\nMonte Carlo (MCMC) methods with multilevel Monte Carlo (MLMC) estimators to\nefficiently solve inverse problems. The MLMC approach requires correlated\nsamples across different fidelity levels, achieved through a novel transport\nmap-based coupling algorithm. We demonstrate a 50% reduction in inference cost\ncompared to traditional single-fidelity methods on the challenging NACA0012\nairfoil at high angles of attack near stall, while delivering realistic\nuncertainty bounds for model predictions in complex separated flow regimes.\nThese results demonstrate that multi-fidelity approaches significantly improve\nturbulence parameter calibration, paving the way for more accurate and\nefficient aircraft certification by analysis."}
{"id": "2510.18161", "categories": ["stat.ML", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2510.18161", "abs": "https://arxiv.org/abs/2510.18161", "authors": ["Hamsa Bastani", "Osbert Bastani", "Bryce McLaughlin"], "title": "Beating the Winner's Curse via Inference-Aware Policy Optimization", "comment": null, "summary": "There has been a surge of recent interest in automatically learning policies\nto target treatment decisions based on rich individual covariates. A common\napproach is to train a machine learning model to predict counterfactual\noutcomes, and then select the policy that optimizes the predicted objective\nvalue. In addition, practitioners also want confidence that the learned policy\nhas better performance than the incumbent policy according to downstream policy\nevaluation. However, due to the winner's curse-an issue where the policy\noptimization procedure exploits prediction errors rather than finding actual\nimprovements-predicted performance improvements are often not substantiated by\ndownstream policy optimization. To address this challenge, we propose a novel\nstrategy called inference-aware policy optimization, which modifies policy\noptimization to account for how the policy will be evaluated downstream.\nSpecifically, it optimizes not only for the estimated objective value, but also\nfor the chances that the policy will be statistically significantly better than\nthe observational policy used to collect data. We mathematically characterize\nthe Pareto frontier of policies according to the tradeoff of these two goals.\nBased on our characterization, we design a policy optimization algorithm that\nuses machine learning to predict counterfactual outcomes, and then plugs in\nthese predictions to estimate the Pareto frontier; then, the decision-maker can\nselect the policy that optimizes their desired tradeoff, after which policy\nevaluation can be performed on the test set as usual. Finally, we perform\nsimulations to illustrate the effectiveness of our methodology."}
{"id": "2510.17994", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.17994", "abs": "https://arxiv.org/abs/2510.17994", "authors": ["Eva-Maria Walz", "Andreas Eberl", "Tilmann Gneiting"], "title": "Assessing Monotone Dependence: Area Under the Curve Meets Rank Correlation", "comment": null, "summary": "The assessment of monotone dependence between random variables $X$ and $Y$ is\na classical problem in statistics and a gamut of application domains.\nConsequently, researchers have sought measures of association that are\ninvariant under strictly increasing transformations of the margins, with the\nextant literature being splintered. Rank correlation coefficients, such as\nSpearman's Rho and Kendall's Tau, have been studied at great length in the\nstatistical literature, mostly under the assumption that $X$ and $Y$ are\ncontinuous. In the case of a dichotomous outcome $Y$, receiver operating\ncharacteristic analysis and the asymmetric area under the curve (AUC) measure\nare used to assess monotone dependence of $Y$ on a covariate $X$. Here we unify\nand extend thus far disconnected strands of literature, by developing common\npopulation level theory, estimators, and tests that bridge continuous and\ndichotomous settings and apply to all linearly ordered outcomes. In particular,\nwe introduce asymmetric grade correlation, AGC$(X,Y)$, as the covariance of the\nmid distribution function transforms, or grades, of $X$ and $Y$, divided by the\nvariance of the grade of $Y$. The coefficient of monotone association then is\nCMA$(X,Y) = \\frac{1}{2} ($AGC$(X,Y) + 1)$. When $X$ and $Y$ are continuous, AGC\nis symmetric and equals Spearman's Rho. When $Y$ is dichotomous, CMA equals\nAUC. We establish central limit theorems for the sample versions of AGC and CMA\nand develop a test of DeLong type for the equality of AGC or CMA values with a\nshared outcome $Y$. In case studies, we apply the new measures to assess\nprogress in data-driven weather prediction, and to evaluate methods of\nuncertainty quantification for large language models."}
{"id": "2510.18548", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.18548", "abs": "https://arxiv.org/abs/2510.18548", "authors": ["Ying Yao", "Daniel J. Graham"], "title": "Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data", "comment": null, "summary": "Accurate annual average daily traffic (AADT) data are vital for transport\nplanning and infrastructure management. However, automatic traffic detectors\nacross national road networks often provide incomplete coverage, leading to\nunderrepresentation of minor roads. While recent machine learning advances have\nimproved AADT estimation at unmeasured locations, most models produce only\npoint predictions and overlook estimation uncertainty. This study addresses\nthat gap by introducing an interval prediction approach that explicitly\nquantifies predictive uncertainty. We integrate a Quantile Random Forest model\nwith Principal Component Analysis to generate AADT prediction intervals,\nproviding plausible traffic ranges bounded by estimated minima and maxima.\nUsing data from over 2,000 minor roads in England and Wales, and evaluated with\nspecialized interval metrics, the proposed method achieves an interval coverage\nprobability of 88.22%, a normalized average width of 0.23, and a Winkler Score\nof 7,468.47. By combining machine learning with spatial and high-dimensional\nanalysis, this framework enhances both the accuracy and interpretability of\nAADT estimation, supporting more robust and informed transport planning."}
{"id": "2510.18215", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18215", "abs": "https://arxiv.org/abs/2510.18215", "authors": ["Haixiang Lan", "Luofeng Liao", "Adam N. Elmachtoub", "Christian Kroer", "Henry Lam", "Haofeng Zhang"], "title": "The Bias-Variance Tradeoff in Data-Driven Optimization: A Local Misspecification Perspective", "comment": null, "summary": "Data-driven stochastic optimization is ubiquitous in machine learning and\noperational decision-making problems. Sample average approximation (SAA) and\nmodel-based approaches such as estimate-then-optimize (ETO) or integrated\nestimation-optimization (IEO) are all popular, with model-based approaches\nbeing able to circumvent some of the issues with SAA in complex\ncontext-dependent problems. Yet the relative performance of these methods is\npoorly understood, with most results confined to the dichotomous cases of the\nmodel-based approach being either well-specified or misspecified. We develop\nthe first results that allow for a more granular analysis of the relative\nperformance of these methods under a local misspecification setting, which\nmodels the scenario where the model-based approach is nearly well-specified. By\nleveraging tools from contiguity theory in statistics, we show that there is a\nbias-variance tradeoff between SAA, IEO, and ETO under local misspecification,\nand that the relative importance of the bias and the variance depends on the\ndegree of local misspecification. Moreover, we derive explicit expressions for\nthe decision bias, which allows us to characterize (un)impactful\nmisspecification directions, and provide further geometric understanding of the\nvariance."}
{"id": "2510.18068", "categories": ["stat.ME", "62H11, 62G09"], "pdf": "https://arxiv.org/pdf/2510.18068", "abs": "https://arxiv.org/abs/2510.18068", "authors": ["Rudolf Beran"], "title": "Cartesian Statistics on Spheres", "comment": null, "summary": "Directional data consists of unit vectors in q-dimensions that can be\ndescribed in polar or Cartesian coordinates. Axial data can be viewed as a pair\nof directions pointed in opposite directions or as a projection matrix of rank\n1. Historically, their statistical analysis has largely been based on a few\nlow-order exponential family models of distributions for random directions or\naxes. A lack of tractable algebraic forms for the normalizing constants has\nhindered the use of higher-order exponential families for less constrained\nmodeling. Of interest are functionals of the unknown distribution of the\ndirectional/axial data, such as the directional/axial mean, dispersion, or\ndistribution itself. This paper outlines nonparametric estimators and bootstrap\nconfidence sets for such functionals. The procedures are based on the empirical\ndistribution of the directional/axial sample expressed in Cartesian\ncoordinates. Sketched as well are nonparametric comparisons among multiple mean\ndirections or axes, estimation of trend in mean directions, and analysis of\nq-dimensional observations restricted to lie in a specified compact subset."}
{"id": "2510.18259", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18259", "abs": "https://arxiv.org/abs/2510.18259", "authors": ["Dechen Zhang", "Junwei Su", "Difan Zou"], "title": "Learning under Quantization for High-Dimensional Linear Regression", "comment": null, "summary": "The use of low-bit quantization has emerged as an indispensable technique for\nenabling the efficient training of large-scale models. Despite its widespread\nempirical success, a rigorous theoretical understanding of its impact on\nlearning performance remains notably absent, even in the simplest linear\nregression setting. We present the first systematic theoretical study of this\nfundamental question, analyzing finite-step stochastic gradient descent (SGD)\nfor high-dimensional linear regression under a comprehensive range of\nquantization targets: data, labels, parameters, activations, and gradients. Our\nnovel analytical framework establishes precise algorithm-dependent and\ndata-dependent excess risk bounds that characterize how different quantization\naffects learning: parameter, activation, and gradient quantization amplify\nnoise during training; data quantization distorts the data spectrum; and data\nand label quantization introduce additional approximation and quantized error.\nCrucially, we prove that for multiplicative quantization (with input-dependent\nquantization step), this spectral distortion can be eliminated, and for\nadditive quantization (with constant quantization step), a beneficial scaling\neffect with batch size emerges. Furthermore, for common polynomial-decay data\nspectra, we quantitatively compare the risks of multiplicative and additive\nquantization, drawing a parallel to the comparison between FP and integer\nquantization methods. Our theory provides a powerful lens to characterize how\nquantization shapes the learning dynamics of optimization algorithms, paving\nthe way to further explore learning theory under practical hardware\nconstraints."}
{"id": "2510.18099", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18099", "abs": "https://arxiv.org/abs/2510.18099", "authors": ["Arindam Fadikar", "Abby Stevens", "Mickael Binois", "Nicholson Collier", "Jonathan Ozik"], "title": "Adaptive Grid-Based Thompson Sampling for Efficient Trajectory Discovery", "comment": null, "summary": "Bayesian optimization (BO) is a powerful framework for estimating parameters\nof computationally expensive simulation models, particularly in settings where\nthe likelihood is intractable and evaluations are costly. In stochastic models\nevery simulation is run with a specific parameter set and an implicit or\nexplicit random seed, where each parameter set and random seed combination\ngenerates an individual realization, or trajectory, sampled from an underlying\nrandom process. Existing BO approaches typically rely on summary statistics\nover the realizations, such as means, medians, or quantiles, potentially\nlimiting their effectiveness when trajectory-level information is desired. We\npropose a trajectory-oriented Bayesian optimization method that incorporates a\nGaussian process (GP) surrogate using both input parameters and random seeds as\ninputs, enabling direct inference at the trajectory level. Using a common\nrandom number (CRN) approach, we define a surrogate-based likelihood over\ntrajectories and introduce an adaptive Thompson Sampling algorithm that refines\na fixed-size input grid through likelihood-based filtering and\nMetropolis-Hastings-based densification. This approach concentrates computation\non statistically promising regions of the input space while balancing\nexploration and exploitation. We apply the method to stochastic epidemic\nmodels, a simple compartmental and a more computationally demanding agent-based\nmodel, demonstrating improved sampling efficiency and faster identification of\ndata-consistent trajectories relative to parameter-only inference."}
{"id": "2510.18332", "categories": ["stat.ML", "cs.LG", "62H20, 60G10, 68T05, 68T27, 60J20"], "pdf": "https://arxiv.org/pdf/2510.18332", "abs": "https://arxiv.org/abs/2510.18332", "authors": ["Gargi Roy", "Dalia Chakrabarty"], "title": "Parametrising the Inhomogeneity Inducing Capacity of a Training Set, and its Impact on Supervised Learning", "comment": null, "summary": "We introduce parametrisation of that property of the available\n  training dataset, that necessitates an inhomogeneous correlation\n  structure for the function that is learnt as a model of the\n  relationship between the pair of variables, observations of which\n  comprise the considered training data. We refer to a parametrisation\n  of this property of a given training set, as its ``inhomogeneity\n  parameter''. It is easy to compute this parameter for small-to-large\n  datasets, and we demonstrate such computation on multiple\n  publicly-available datasets, while also demonstrating that\n  conventional ``non-stationarity'' of data does not imply a non-zero\n  inhomogeneity parameter of the dataset. We prove that - within the\n  probabilistic Gaussian Process-based learning approach - a training\n  set with a non-zero inhomogeneity parameter renders it imperative,\n  that the process that is invoked to model the sought function, be\n  non-stationary. Following the learning of a real-world multivariate\n  function with such a Process, quality and reliability of predictions\n  at test inputs, are demonstrated to be affected by the inhomogeneity\n  parameter of the training data."}
{"id": "2510.18115", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18115", "abs": "https://arxiv.org/abs/2510.18115", "authors": ["Canyi Chen", "Ritoban Kundu", "Wei Hao", "Peter X. -K. Song"], "title": "Copula Structural Equation Models for Mediation Pathway Analysis", "comment": null, "summary": "Structural equation models (SEMs) are fundamental to causal mediation pathway\ndiscovery. However, traditional SEM approaches often rely on \\emph{ad hoc}\nmodel specifications when handling complex data structures such as mixed data\ntypes or non-normal data in which Gaussian assumptions for errors are rather\nrestrictive. The invocation of copula dependence modeling methods to extend the\nclassical linear SEMs mitigates several of key technical limitations, offering\ngreater modeling flexibility to analyze non-Gaussian data. This paper presents\na selective review of major developments in this area, highlighting recent\nadvancements and their methodological implications."}
{"id": "2510.18548", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.18548", "abs": "https://arxiv.org/abs/2510.18548", "authors": ["Ying Yao", "Daniel J. Graham"], "title": "Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data", "comment": null, "summary": "Accurate annual average daily traffic (AADT) data are vital for transport\nplanning and infrastructure management. However, automatic traffic detectors\nacross national road networks often provide incomplete coverage, leading to\nunderrepresentation of minor roads. While recent machine learning advances have\nimproved AADT estimation at unmeasured locations, most models produce only\npoint predictions and overlook estimation uncertainty. This study addresses\nthat gap by introducing an interval prediction approach that explicitly\nquantifies predictive uncertainty. We integrate a Quantile Random Forest model\nwith Principal Component Analysis to generate AADT prediction intervals,\nproviding plausible traffic ranges bounded by estimated minima and maxima.\nUsing data from over 2,000 minor roads in England and Wales, and evaluated with\nspecialized interval metrics, the proposed method achieves an interval coverage\nprobability of 88.22%, a normalized average width of 0.23, and a Winkler Score\nof 7,468.47. By combining machine learning with spatial and high-dimensional\nanalysis, this framework enhances both the accuracy and interpretability of\nAADT estimation, supporting more robust and informed transport planning."}
{"id": "2510.18149", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18149", "abs": "https://arxiv.org/abs/2510.18149", "authors": ["Wenlu Tang", "Hongni Wang", "Xingcai Zhou", "Bei Jiang", "Linglong Kong"], "title": "Conformal Inference For Missing Data under Multiple Robust Learning", "comment": null, "summary": "We develop a novel approach to tackle the common but challenging problem of\nconformal inference for missing data in machine learning, focusing on Missing\nat Random (MAR) data. We propose a new procedure Conformal prediction for\nMissing data under Multiple Robust Learning (CM--MRL) that combines split\nconformal calibration with a multiple robust empirical-likelihood (EL)\nreweighting scheme. The method proceeds via a double calibration by reweighting\nthe complete-case scores by EL so that their distribution matches the full\ncalibration distribution implied by MAR, even when some working models are\nmisspecified. We demonstrate the asymptotic behavior of our estimators through\nempirical process theory and provide reliable coverage for our prediction\nintervals, both marginally and conditionally and we further show an\ninterval-length dominance result. We show the effectiveness of the proposed\nmethod by several numerical experiments in the presence of missing data."}
{"id": "2510.18777", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18777", "abs": "https://arxiv.org/abs/2510.18777", "authors": ["Yen-Chi Chen"], "title": "A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models", "comment": "This is an introduction paper. 28 pages, 2 figures", "summary": "While Variational Inference (VI) is central to modern generative models like\nVariational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\npedagogical treatment is split across disciplines. In statistics, VI is\ntypically framed as a Bayesian method for posterior approximation. In machine\nlearning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\nwhere VI is used to approximate a maximum likelihood estimator. This creates a\nbarrier for statisticians, as the principles behind VAEs and DDMs are hard to\ncontextualize without a corresponding Frequentist introduction to VI. This\npaper provides that introduction: we explain the theory for VI, VAEs, and DDMs\nfrom a purely Frequentist perspective, starting with the classical\nExpectation-Maximization (EM) algorithm. We show how VI arises as a scalable\nsolution for intractable E-steps and how VAEs and DDMs are natural,\ndeep-learning-based extensions of this framework, thereby bridging the gap\nbetween classical statistical inference and modern generative AI."}
{"id": "2510.18241", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18241", "abs": "https://arxiv.org/abs/2510.18241", "authors": ["Bahareh Ghanbari", "Pavel Krupskiy", "Laleh Tafakori", "Yan Wang"], "title": "Non-Parametric Estimation Techniques of Factor Copula Model using Proxies", "comment": null, "summary": "Parametric factor copula models typically work well in modeling multivariate\ndependencies due to their flexibility and ability to capture complex dependency\nstructures. However, accurately estimating the linking copulas within these\nmod- els remains challenging, especially when working with high-dimensional\ndata. This paper proposes a novel approach for estimating linking copulas based\non a non-parametric kernel estimator. Unlike conventional parametric methods,\nour approach utilizes the flexibility of kernel density estimation to capture\nthe un- derlying dependencies more accurately, particularly in scenarios where\nthe un- derlying copula structure is complex or unknown. We show that the\nproposed estimator is consistent under mild conditions and demonstrate its\neffectiveness through extensive simulation studies. Our findings suggest that\nthe proposed approach offers a promising avenue for modeling multivariate\ndependencies, par- ticularly in applications requiring robust and efficient\nestimation of copula-based models."}
{"id": "2510.18654", "categories": ["stat.ME", "cs.CR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.18654", "abs": "https://arxiv.org/abs/2510.18654", "authors": ["Daniel Csillag", "Diego Mesquita"], "title": "Differentially Private E-Values", "comment": null, "summary": "E-values have gained prominence as flexible tools for statistical inference\nand risk control, enabling anytime- and post-hoc-valid procedures under minimal\nassumptions. However, many real-world applications fundamentally rely on\nsensitive data, which can be leaked through e-values. To ensure their safe\nrelease, we propose a general framework to transform non-private e-values into\ndifferentially private ones. Towards this end, we develop a novel biased\nmultiplicative noise mechanism that ensures our e-values remain statistically\nvalid. We show that our differentially private e-values attain strong\nstatistical power, and are asymptotically as powerful as their non-private\ncounterparts. Experiments across online risk monitoring, private healthcare,\nand conformal e-prediction demonstrate our approach's effectiveness and\nillustrate its broad applicability."}
{"id": "2510.18247", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18247", "abs": "https://arxiv.org/abs/2510.18247", "authors": ["Jiazhen Xu", "Andrew T. A. Wood", "Tao Zou"], "title": "Quantifying Periodicity in Non-Euclidean Random Objects", "comment": null, "summary": "Time-varying non-Euclidean random objects are playing a growing role in\nmodern data analysis, and periodicity is a fundamental characteristic of\ntime-varying data. However, quantifying periodicity in general non-Euclidean\nrandom objects remains largely unexplored. In this work, we introduce a novel\nnonparametric framework for quantifying periodicity in random objects within a\ngeneral metric space that lacks Euclidean structures. Our approach formulates\nperiodicity estimation as a model selection problem and provides methodologies\nfor period estimation, data-driven tuning parameter selection, and periodic\ncomponent extraction. Our theoretical contributions include establishing the\nconsistency of period estimation without relying on linearity properties used\nin the literature for Euclidean data, providing theoretical support for\ndata-driven tuning parameter selection, and deriving uniform convergence\nresults for periodic component estimation. Through extensive simulation studies\ncovering three distinct types of time-varying random objects such as\ncompositional data, networks, and functional data, we showcase the superior\naccuracy achieved by our approach in periodicity quantification. Finally, we\napply our method to various real datasets, including U.S. electricity\ngeneration compositions, New York City transportation networks, and Germany's\nwater consumption curves, highlighting its practical relevance in identifying\nand quantifying meaningful periodic patterns."}
{"id": "2510.18843", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.18843", "abs": "https://arxiv.org/abs/2510.18843", "authors": ["Pawel Morzywolek", "Peter B. Gilbert", "Alex Luedtke"], "title": "Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects", "comment": "40 pages, 7 figures", "summary": "We provide an inferential framework to assess variable importance for\nheterogeneous treatment effects. This assessment is especially useful in\nhigh-risk domains such as medicine, where decision makers hesitate to rely on\nblack-box treatment recommendation algorithms. The variable importance measures\nwe consider are local in that they may differ across individuals, while the\ninference is global in that it tests whether a given variable is important for\nany individual. Our approach builds on recent developments in semiparametric\ntheory for function-valued parameters, and is valid even when statistical\nmachine learning algorithms are employed to quantify treatment effect\nheterogeneity. We demonstrate the applicability of our method to infectious\ndisease prevention strategies."}
{"id": "2510.18598", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.18598", "abs": "https://arxiv.org/abs/2510.18598", "authors": ["Lujia Bai", "Holger Dette"], "title": "Measuring deviations from spherical symmetry", "comment": null, "summary": "Most of the work on checking spherical symmetry assumptions on the\ndistribution of the $p$-dimensional random vector $Y$ has its focus on\nstatistical tests for the null hypothesis of exact spherical symmetry. In this\npaper, we take a different point of view and propose a measure for the\ndeviation from spherical symmetry, which is based on the minimum distance\nbetween the distribution of the vector $\\big (\\|Y\\|, Y/ \\|Y\\| )^\\top $ and its\nbest approximation by a distribution of a vector $\\big (\\|Y_s\\|, Y_s/ \\|Y_s \\|\n)^\\top $ corresponding to a random vector $Y_s$ with a spherical distribution.\nWe develop estimators for the minimum distance with corresponding statistical\nguarantees (provided by asymptotic theory) and demonstrate the applicability of\nour approach by means of a simulation study and a real data example."}
{"id": "2510.18599", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18599", "abs": "https://arxiv.org/abs/2510.18599", "authors": ["Peiyi Zhou"], "title": "A new implementation of Network GARCH Model", "comment": "Codes for this paper are accessible at\n  https://github.com/PZhou114/GNGARCH_coding, detailed instructions are in the\n  repository's README. For this paper, AI tools like GPT-4o and Grammarly were\n  used to copy-edit the introduction and conclusion text, also improving the\n  quality of written English", "summary": "Volatility clustering and spillovers are key features of real-world financial\ntime series when there are a lot of cross-sectional financial assets. While\nnetwork analysis helps connect stocks that are 'similar' or 'correlated', which\nis effective to link volatility spillovers between stocks, contemporary\nmultivariate ARCH-GARCH formulations struggle to represent structured network\ndependence and remain parsimonious. We introduce the Generalised Network GARCH\n(GNGARCH) model as a network volatility model that embeds the GARCH dynamics\nwithin the Generalised Network Autoregressive (GNAR) framework, to capture the\ndynamic volatility of financial asset return by both the asset itself and its\n'neighbouring' assets from the constructed virtual network. The proposed\nvolatility model GNGARCH also addresses the limitations for current studies of\nnetwork GARCH by adapting neighbouring volatility persistence, dynamic\nconditional covariance updates, and allowing higher-order neighbouring effects\nrather than only immediate neighbours. This paper provides the model\nderivation, vectorisation and conversion, stationarity conditions, and also an\nextension by incorporating threshold coefficients to capture leverage effects.\nWe show that the GNGARCH is a valid volatility model satisfying the stylised\nfacts of financial return series through simulation. Parameter estimation is\nthen performed by using squared returns as variance proxy and minimising a loss\nfunction that is either mean squared error (MSE) or quasi-likelihood (QLIKE).\nWe apply our model on 75 of the most active US stocks under a virtual network,\nand highlight the model's ability in volatility estimation and forecast."}
{"id": "2510.18654", "categories": ["stat.ME", "cs.CR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.18654", "abs": "https://arxiv.org/abs/2510.18654", "authors": ["Daniel Csillag", "Diego Mesquita"], "title": "Differentially Private E-Values", "comment": null, "summary": "E-values have gained prominence as flexible tools for statistical inference\nand risk control, enabling anytime- and post-hoc-valid procedures under minimal\nassumptions. However, many real-world applications fundamentally rely on\nsensitive data, which can be leaked through e-values. To ensure their safe\nrelease, we propose a general framework to transform non-private e-values into\ndifferentially private ones. Towards this end, we develop a novel biased\nmultiplicative noise mechanism that ensures our e-values remain statistically\nvalid. We show that our differentially private e-values attain strong\nstatistical power, and are asymptotically as powerful as their non-private\ncounterparts. Experiments across online risk monitoring, private healthcare,\nand conformal e-prediction demonstrate our approach's effectiveness and\nillustrate its broad applicability."}
{"id": "2510.18834", "categories": ["stat.ME", "62F03"], "pdf": "https://arxiv.org/pdf/2510.18834", "abs": "https://arxiv.org/abs/2510.18834", "authors": ["Jia Zhou", "Chang-Xing Ma"], "title": "Testing Risk Difference of Two Proportions for Combined Unilateral and Bilateral Data", "comment": "19 pages, 1 figure, 12 tables", "summary": "In clinical studies with paired organs, binary outcomes often exhibit\nintra-subject correlation and may include a mixture of unilateral and bilateral\nobservations. Under Donner's constant correlation model, we develop three\nlikelihood-based test statistics (the likelihood ratio, Wald-type, and score\ntests) for assessing the risk difference between two proportions. Simulation\nstudies demonstrate good control of type I error and comparable power among the\nthree tests, with the score test showing slightly better stability.\nApplications to otolaryngologic and ophthalmologic data illustrate the methods.\nAn online calculator is also provided for power analysis and risk difference\ntesting. The score test is recommended for practical use and future studies\nwith combined unilateral and bilateral binary data."}
{"id": "2510.18843", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.18843", "abs": "https://arxiv.org/abs/2510.18843", "authors": ["Pawel Morzywolek", "Peter B. Gilbert", "Alex Luedtke"], "title": "Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects", "comment": "40 pages, 7 figures", "summary": "We provide an inferential framework to assess variable importance for\nheterogeneous treatment effects. This assessment is especially useful in\nhigh-risk domains such as medicine, where decision makers hesitate to rely on\nblack-box treatment recommendation algorithms. The variable importance measures\nwe consider are local in that they may differ across individuals, while the\ninference is global in that it tests whether a given variable is important for\nany individual. Our approach builds on recent developments in semiparametric\ntheory for function-valued parameters, and is valid even when statistical\nmachine learning algorithms are employed to quantify treatment effect\nheterogeneity. We demonstrate the applicability of our method to infectious\ndisease prevention strategies."}
{"id": "2510.18067", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18067", "abs": "https://arxiv.org/abs/2510.18067", "authors": ["Nian Liu", "Jian Cao"], "title": "Principled Argo Modeling using Vecchia-based Gaussian Processes", "comment": null, "summary": "Argo is an international program that collects temperature and salinity\nobservations in the upper two kilometers of the global ocean. Most existing\napproaches for modeling Argo temperature rely on spatial partitioning, where\ndata are locally modeled by first estimating a prescribed mean structure and\nthen fitting Gaussian processes (GPs) to the mean-subtracted anomalies. Such\nstrategies introduce challenges in designing suitable mean structures and\ndefining domain partitions, often resulting in ad hoc modeling choices. In this\nwork, we propose a one-stop Gaussian process regression framework with a\ngeneric spatio-temporal covariance function to jointly model Argo temperature\ndata across broad spatial domains. Our fully data-driven approach achieves\nsuperior predictive performance compared with methods that require domain\npartitioning or parametric regression. To ensure scalability over large spatial\nregions, we employ the Vecchia approximation, which reduces the computational\ncomplexity from cubic to quasi-linear in the number of observations while\npreserving predictive accuracy. Using Argo data from January to March over the\nyears 2007-2016, the same dataset used in prior benchmark studies, we\ndemonstrate that our approach provides a principled, scalable, and\ninterpretable tool for large-scale oceanographic analysis."}
{"id": "2510.18071", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18071", "abs": "https://arxiv.org/abs/2510.18071", "authors": ["Yixin Fang", "Weili He"], "title": "Arbitrated Indirect Treatment Comparisons", "comment": null, "summary": "Matching-adjusted indirect comparison (MAIC) has been increasingly employed\nin health technology assessments (HTA). By reweighting subjects from a trial\nwith individual participant data (IPD) to match the covariate summary\nstatistics of another trial with only aggregate data (AgD), MAIC facilitates\nthe estimation of a treatment effect defined with respect to the AgD trial\npopulation. This manuscript introduces a new class of methods, termed\narbitrated indirect treatment comparisons, designed to address the ``MAIC\nparadox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox\narises when different sponsors, analyzing the same data, reach conflicting\nconclusions regarding which treatment is more effective. The underlying issue\nis that each sponsor implicitly targets a different population. To resolve this\ninconsistency, the proposed methods focus on estimating treatment effects in a\ncommon target population, specifically chosen to be the overlap population."}
{"id": "2510.18639", "categories": ["stat.AP", "q-fin.RM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18639", "abs": "https://arxiv.org/abs/2510.18639", "authors": ["Samuel Perreault", "Silvana M. Pesenti", "Daniyal Shahzad"], "title": "Distributional regression for seasonal data: an application to river flows", "comment": null, "summary": "Risk assessment in casualty insurance, such as flood risk, traditionally\nrelies on extreme-value methods that emphasizes rare events. These approaches\nare well-suited for characterizing tail risk, but do not capture the broader\ndynamics of environmental variables such as moderate or frequent loss events.\nTo complement these methods, we propose a modelling framework for estimating\nthe full (daily) distribution of environmental variables as a function of time,\nthat is a distributional version of typical climatological summary statistics,\nthereby incorporating both seasonal variation and gradual long-term changes.\nAside from the time trend, to capture seasonal variation our approach\nsimultaneously estimates the distribution for each instant of the seasonal\ncycle, without explicitly modelling the temporal dependence present in the\ndata. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive\nModels for Location, Scale, and Shape), where the parameters of the\ndistribution vary over the seasonal cycle as a function of explanatory\nvariables depending only on the time of year, and not on the past values of the\nprocess under study. Ignoring the temporal dependence in the seasonal variation\ngreatly simplifies the modelling but poses inference challenges that we clarify\nand overcome.\n  We apply our framework to daily river flow data from three hydrometric\nstations along the Fraser River in British Columbia, Canada, and analyse the\nflood of the Fraser River in early winter of 2021."}
{"id": "2510.18777", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18777", "abs": "https://arxiv.org/abs/2510.18777", "authors": ["Yen-Chi Chen"], "title": "A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models", "comment": "This is an introduction paper. 28 pages, 2 figures", "summary": "While Variational Inference (VI) is central to modern generative models like\nVariational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\npedagogical treatment is split across disciplines. In statistics, VI is\ntypically framed as a Bayesian method for posterior approximation. In machine\nlearning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\nwhere VI is used to approximate a maximum likelihood estimator. This creates a\nbarrier for statisticians, as the principles behind VAEs and DDMs are hard to\ncontextualize without a corresponding Frequentist introduction to VI. This\npaper provides that introduction: we explain the theory for VI, VAEs, and DDMs\nfrom a purely Frequentist perspective, starting with the classical\nExpectation-Maximization (EM) algorithm. We show how VI arises as a scalable\nsolution for intractable E-steps and how VAEs and DDMs are natural,\ndeep-learning-based extensions of this framework, thereby bridging the gap\nbetween classical statistical inference and modern generative AI."}
{"id": "2510.18818", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18818", "abs": "https://arxiv.org/abs/2510.18818", "authors": ["Jay JH Park", "Rebecca K. Metcalfe", "Nathaniel Dyrkton", "Yichen Yan", "Shomoita Alam", "Kevin Phelan", "Ibrahim Sana", "Susan Shepherd"], "title": "Comparison of Simulation-Guided Design to Closed-Form Power Calculations in Planning a Cluster Randomized Trial with Covariate-Constrained Randomization: A Case Study in Rural Chad", "comment": null, "summary": "Current practices for designing cluster-randomized trials (cRCTs) typically\nrely on closed-form formulas for power calculations. For cRCTs using\ncovariate-constrained randomization, the utility of conventional calculations\nmight be limited, particularly when data is nested. We compared\nsimulation-based planning of a nested cRCT using covariate-constrained\nrandomization to conventional power calculations using OptiMAx-Chad as a case\nstudy. OptiMAx-Chad will examine the impact of embedding mass distribution of\nsmall-quantity lipid-based nutrient supplements within an expanded programme on\nimmunization on first-dose measles-containing vaccine (MCV1) coverage among\nchildren aged 12-24 months in rural villages in Ngouri. Within the 12 health\nareas to be randomized, a random subset of villages will be selected for\noutcome collection. 1,000,000 assignments of health areas with different\npossible village selections were generated using covariate-constrained\nrandomization to balance baseline village characteristics. The empirically\nestimated intracluster correlation coefficient (ICC) and the World Health\nOrganization (WHO) recommended values of 1/3 and 1/6 were considered. The\ndesired operating characteristics were 80% power at 0.05 one-sided type I error\nrate. Using conventional calculations target power for a realistic treatment\neffect could not be achieved with the WHO recommended values. Conventional\ncalculations also showed a plateau in power after a certain cluster size. Our\nsimulations matched the design of OptiMAx-Chad with covariate adjustment and\nrandom selection, and showed that power did not plateau. Instead, power\nincreased with increasing cluster size. Planning complex cRCTs with covariate\nconstrained randomization and a multi-nested data structure with conventional\nclosed-form formulas can be misleading. Simulations can improve the planning of\ncRCTs."}
