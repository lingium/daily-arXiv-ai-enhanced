{"id": "2601.08610", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.08610", "abs": "https://arxiv.org/abs/2601.08610", "authors": ["Wenxuan Guo", "Panos Toulis", "Yuhao Wang"], "title": "Permutation Inference under Multi-way Clustering and Missing Data", "comment": "55 pages, 5 figures", "summary": "Econometric applications with multi-way clustering often feature a small number of effective clusters or heavy-tailed data, making standard cluster-robust and bootstrap inference unreliable in finite samples. In this paper, we develop a framework for finite-sample valid permutation inference in linear regression with multi-way clustering under an assumption of conditional exchangeability of the errors. Our assumption is closely related to the notion of separate exchangeability studied in earlier work, but can be more realistic in many economic settings as it imposes minimal restrictions on the covariate distribution. We construct permutation tests of significance that are valid in finite samples and establish theoretical power guarantees, in contrast to existing methods that are justified only asymptotically. We also extend our methodology to settings with missing data and derive power results that reveal phase transitions in detectability. Through simulation studies, we demonstrate that the proposed tests maintain correct size and competitive power, while standard cluster-robust and bootstrap procedures can exhibit substantial size distortions."}
{"id": "2601.08707", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.08707", "abs": "https://arxiv.org/abs/2601.08707", "authors": ["Kosuke Morikawa", "Jae Kwang Kim"], "title": "Semiparametric Efficient Data Integration Using the Dual-Frame Sampling Framework", "comment": null, "summary": "Integrating probability and non-probability samples is increasingly important, yet unknown sampling mechanisms in non-probability sources complicate identification and efficient estimation. We develop semiparametric theory for dual-frame data integration and propose two complementary estimators. The first models the non-probability inclusion probability parametrically and attains the semiparametric efficiency bound. We introduce an identifiability condition based on strong monotonicity that identifies sampling-model parameters without instrumental variables, even under informative (non-ignorable) selection, using auxiliary information from the probability sample; it remains valid without record linkage between samples. The second estimator, motivated by a two-stage sampling approximation, avoids explicit modeling of the non-probability mechanism; though not fully efficient, it is efficient within a restricted augmentation class and is robust to misspecification. Simulations and an application to the Culture and Community in a Time of Crisis public simulation dataset show efficiency gains under correct specification and stable performance under misspecification and weak identification. Methods are implemented in the R package \\texttt{dfSEDI}."}
{"id": "2601.07864", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07864", "abs": "https://arxiv.org/abs/2601.07864", "authors": ["Mengta Chung"], "title": "A Symmetric Random Scan Collapsed Gibbs Sampler for Fully Bayesian Variable Selection with Spike-and-Slab Priors", "comment": null, "summary": "We introduce a symmetric random scan Gibbs sampler for scalable Bayesian variable selection that eliminates storage of the full cross-product matrix by computing required quantities on-the-fly. Data-informed proposal weights, constructed from marginal correlations, concentrate sampling effort on promising candidates while a uniform mixing component ensures theoretical validity. We provide explicit guidance for selecting tuning parameters based on the ratio of signal to null correlations, ensuring adequate posterior exploration. The posterior-mean-size selection rule provides an adaptive alternative to the median probability model that automatically calibrates to the effective signal density without requiring an arbitrary threshold. In simulations with one hundred thousand predictors, the method achieves sensitivity of 1.000 and precision above 0.76. Application to a genomic dataset studying riboflavin production in Bacillus subtilis identifies six genes, all validated by previous studies using alternative methods. The underlying model combines a Dirac spike-and-slab prior with Laplace-type shrinkage: the Dirac spike enforces exact sparsity by setting inactive coefficients to precisely zero, while the Laplace-type slab provides adaptive regularization for active coefficients through a local-global scale mixture."}
{"id": "2601.07977", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07977", "abs": "https://arxiv.org/abs/2601.07977", "authors": ["K. Ken Peng", "Charmaine B. Dean", "Robert Delatolla", "X. Joan Hu", "Elizabeth Renouf"], "title": "Joint Modeling of Two Stochastic Processes, with Application to Learning Hospitalization Dynamics from Wastewater Viral Concentrations", "comment": null, "summary": "In the post-pandemic era of COVID-19, hospitalization remains a primary public health concern and wastewater surveillance has become an important tool for monitoring its dynamics at the level of community. However, there is usually no sufficient information to know the infection process that results in both wastewater viral signals and hospital admissions. That key challenge has motived a statistical framework proposed in this paper. We formulate the connection of overtime wastewater viral signals and hospitalization counts through a latent process of infection at the level of individual subject. We provide a strategy for accommodating aggregated data, a typical form of surveillance data. Moreover, we ease the conventional procedure of the statistical learning with the joint modeling using available information on the infection process, which can be under-reporting. A simulation study demonstrates that the proposed approach yields stable inference under different degrees of under-ascertainment. The COVID-19 surveillance data from Ottawa, Canada shows that the framework recovers coherent temporal patterns in infection prevalence and variant-specific hospitalization risk under several reporting assumptions."}
{"id": "2601.07901", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07901", "abs": "https://arxiv.org/abs/2601.07901", "authors": ["Hao Qiu", "Mengxiao Zhang", "Juliette Achddou"], "title": "Decentralized Online Convex Optimization with Unknown Feedback Delays", "comment": null, "summary": "Decentralized online convex optimization (D-OCO), where multiple agents within a network collaboratively learn optimal decisions in real-time, arises naturally in applications such as federated learning, sensor networks, and multi-agent control.  In this paper, we study D-OCO under unknown, time-and agent-varying feedback delays. While recent work has addressed this problem (Nguyen et al., 2024), existing algorithms assume prior knowledge of the total delay over agents and still suffer from suboptimal dependence on both the delay and network parameters. To overcome these limitations, we propose a novel algorithm that achieves an improved regret bound of O N $\\sqrt$ d tot + N $\\sqrt$ T  (1-$σ$2) 1/4 , where T is the total horizon, d tot denotes the average total delay across agents, N is the number of agents, and 1 -$σ$ 2 is the spectral gap of the network. Our approach builds upon recent advances in D-OCO (Wan et al., 2024a), but crucially incorporates an adaptive learning rate mechanism via a decentralized communication protocol. This enables each agent to estimate delays locally using a gossip-based strategy without the prior knowledge of the total delay. We further extend our framework to the strongly convex setting and derive a sharper regret bound of O N $δ$max ln T $α$  , where $α$ is the strong convexity parameter and $δ$ max is the maximum number of missing observations averaged over agents. We also show that our upper bounds for both settings are tight up to logarithmic factors. Experimental results validate the effectiveness of our approach, showing improvements over existing benchmark algorithms."}
{"id": "2601.08411", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2601.08411", "abs": "https://arxiv.org/abs/2601.08411", "authors": ["Abylay Zhumekenov", "Alexandros Beskos", "Dan Crisan", "Ajay Jasra", "Nikolas Kantas"], "title": "Particle Filtering for a Class of State-Space Models with Low and Degenerate Observational Noise", "comment": "21 pages, 12 figures", "summary": "We consider the discrete-time filtering problem in scenarios where the observation noise is degenerate or low. We focus on the case where the observation equation is a linear function of the state and that additive noise is low or degenerate, however, we place minimal assumptions on the hidden state process. In this scenario we derive new particle filtering (PF) algorithms and, under assumptions, in such a way that as the noise becomes more degenerate a PF which approximates the low noise filtering problem provably inherits the properties of the PF used in the degenerate case. We extend our framework to the case where the hidden states are drawn from a diffusion process. In this scenario we develop new PFs which are robust to both low noise and fine levels of time discretization. We illustrate our algorithms numerically on several examples."}
{"id": "2601.07961", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07961", "abs": "https://arxiv.org/abs/2601.07961", "authors": ["Benjamin Brindle", "George Bonanno", "Thomas Derrick Hull", "Nicolas Charon", "Matteo Malgaroli"], "title": "Language markers of emotion flexibility predict depression and anxiety treatment outcomes", "comment": null, "summary": "Predicting treatment non-response for anxiety and depression is challenging, in part because of sparse symptom assessments in real-world care. We examined whether passively captured, fine-grained emotions serve as linguistic markers of treatment outcomes by analyzing 12 weeks of de-identified teletherapy transcripts from 12,043 U.S. patients with moderate-to-severe anxiety and depression symptoms. A transformer-based small language model extracted patients' emotions at the talk-turn level; a state-space model (VISTA) clustered subgroups based on emotion dynamics over time and produced temporal networks. Two groups emerged: an improving group (n=8,230) and a non-response group (n=3813) showing increased odds of symptom deterioration, and lower likelihood of clinically significant improvement. Temporal networks indicated that sadness and fear exerted most influence on emotion dynamics in non-responders, whereas improving patients showed balanced joy, sadness, and neutral expressions. Findings suggest that linguistic markers of emotional inflexibility can serve as scalable, interpretable, and theoretically grounded indicators for treatment risk stratification."}
{"id": "2601.07979", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.07979", "abs": "https://arxiv.org/abs/2601.07979", "authors": ["Hanzhang Lu", "Keiran Malott", "Venkat Suprabath Bitra", "Kirsty Milligan", "Sanjeena Subedi", "Edana Cassol", "Vinita Chauhan", "Connor McNairn", "Bryan Muir", "Prarthana Pasricha", "Sangeeta Murugkar", "Rowan Thomson", "Andrew Jirasek", "Jeffrey L. Andrews"], "title": "Spatial Covariance Constraints for Gaussian Mixture Models", "comment": "19 pages, 7 figures", "summary": "Although extensive research exists in spatial modeling, few studies have addressed finite mixture model-based clustering methods for spatial data. Finite mixture models, especially Gaussian mixture models, particularly suffer from high dimensionality due to the number of free covariance parameters. This study introduces a spatial covariance constraint for Gaussian mixture models that requires only four free parameters for each component, independent of dimensionality. Using a coordinate system, the spatially constrained Gaussian mixture model enables clustering of multi-way spatial data and inference of spatial patterns. The parameter estimation is conducted by combining the expectation-maximization (EM) algorithm with the generalized least squares (GLS) estimator. Simulation studies and applications to Raman spectroscopy data are provided to demonstrate the proposed model."}
{"id": "2601.07944", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.07944", "abs": "https://arxiv.org/abs/2601.07944", "authors": ["Roy Shivam Ram Shreshtth", "Arnab Hazra", "Gourab Mukherjee"], "title": "A Statistical Assessment of Amortized Inference Under Signal-to-Noise Variation and Distribution Shift", "comment": "26 pages, 5 figures, 3 tables", "summary": "Since the turn of the century, approximate Bayesian inference has steadily evolved as new computational techniques have been incorporated to handle increasingly complex and large-scale predictive problems. The recent success of deep neural networks and foundation models has now given rise to a new paradigm in statistical modeling, in which Bayesian inference can be amortized through large-scale learned predictors. In amortized inference, substantial computation is invested upfront to train a neural network that can subsequently produce approximate posterior or predictions at negligible marginal cost across a wide range of tasks. At deployment, amortized inference offers substantial computational savings compared with traditional Bayesian procedures, which generally require repeated likelihood evaluations or Monte Carlo simulations for predictions for each new dataset.\n  Despite the growing popularity of amortized inference, its statistical interpretation and its role within Bayesian inference remain poorly understood. This paper presents statistical perspectives on the working principles of several major neural architectures, including feedforward networks, Deep Sets, and Transformers, and examines how these architectures naturally support amortized Bayesian inference. We discuss how these models perform structured approximation and probabilistic reasoning in ways that yield controlled generalization error across a wide range of deployment scenarios, and how these properties can be harnessed for Bayesian computation. Through simulation studies, we evaluate the accuracy, robustness, and uncertainty quantification of amortized inference under varying signal-to-noise ratios and distributional shifts, highlighting both its strengths and its limitations."}
{"id": "2601.07944", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.07944", "abs": "https://arxiv.org/abs/2601.07944", "authors": ["Roy Shivam Ram Shreshtth", "Arnab Hazra", "Gourab Mukherjee"], "title": "A Statistical Assessment of Amortized Inference Under Signal-to-Noise Variation and Distribution Shift", "comment": "26 pages, 5 figures, 3 tables", "summary": "Since the turn of the century, approximate Bayesian inference has steadily evolved as new computational techniques have been incorporated to handle increasingly complex and large-scale predictive problems. The recent success of deep neural networks and foundation models has now given rise to a new paradigm in statistical modeling, in which Bayesian inference can be amortized through large-scale learned predictors. In amortized inference, substantial computation is invested upfront to train a neural network that can subsequently produce approximate posterior or predictions at negligible marginal cost across a wide range of tasks. At deployment, amortized inference offers substantial computational savings compared with traditional Bayesian procedures, which generally require repeated likelihood evaluations or Monte Carlo simulations for predictions for each new dataset.\n  Despite the growing popularity of amortized inference, its statistical interpretation and its role within Bayesian inference remain poorly understood. This paper presents statistical perspectives on the working principles of several major neural architectures, including feedforward networks, Deep Sets, and Transformers, and examines how these architectures naturally support amortized Bayesian inference. We discuss how these models perform structured approximation and probabilistic reasoning in ways that yield controlled generalization error across a wide range of deployment scenarios, and how these properties can be harnessed for Bayesian computation. Through simulation studies, we evaluate the accuracy, robustness, and uncertainty quantification of amortized inference under varying signal-to-noise ratios and distributional shifts, highlighting both its strengths and its limitations."}
{"id": "2601.08350", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.08350", "abs": "https://arxiv.org/abs/2601.08350", "authors": ["Pierre Ailliot", "Carlo Gaetan", "Philippe Naveau"], "title": "A parsimonious tail compliant multiscale statistical model for aggregated rainfall", "comment": null, "summary": "Modeling rainfall intensity distributions across aggregation scales (from sub-hourly to weekly) is essential for hydrological risk analysis and IDF curves. Aggregation naturally imposes mathematical constraints: return levels must be ordered by time scale, as daily accumulations necessarily exceed sub-daily ones. From a statistical perspective, each aggregation step should ideally not require additional parameters, yet parsimonious models describing the full distribution remain scarce, as most literature focuses on seasonal block maxima.\n  In this study, we propose a parsimonious framework to model all rainfall intensities (low to large) across scales. We utilize the Extended Generalized Pareto Distribution (EGPD), which aligns with extreme value theory for both tails while remaining flexible for the bulk of the distribution. We establish a general result on the behavior of EGPD variables under various aggregation procedures.\n  To overcome the difficulty of direct likelihood inference, we link the EGPD class to Poisson compound sums. This allows the use of the Panjer algorithm for efficient composite likelihood evaluation. Our approach ensures that return levels do not cross across scales and enables estimation for return periods below annual or seasonal levels.\n  We demonstrate the method using sub-hourly series from six French stations with diverse climates. Only eight parameters are needed per station to capture scales from six minutes to three days. IDF curves above and below the annual scale are provided."}
{"id": "2601.07980", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07980", "abs": "https://arxiv.org/abs/2601.07980", "authors": ["K. Ken Peng", "X. Joan Hu", "Tim B. Swartz"], "title": "Modeling Event Dynamics by Self-Exciting Processes with Random Memory", "comment": null, "summary": "Event history data from sports competitions have recently drawn increasing attention in sports analytics to generate data-driven strategies. Such data often exhibit self-excitation in the event occurrence and dependence within event clusters. The conventional event models based on gap times may struggle to capture those features. In particular, while consecutive events may occur within a short timeframe, the self-excitation effect caused by previous events is often transient and continues for a period of uncertain time. This paper introduces an extended Hawkes process model with random self-excitation duration to formulate the dynamics of event occurrence. We present examples of the proposed model and procedures for estimating the associated model parameters. We employ the collection of the corner kicks in the games of the 2019 regular season of the Chinese Super League to motivate and illustrate the modeling and its usefulness. We also design algorithms for simulating the event process under proposed models. The proposed approach can be adapted with little modification in many other research fields such as Criminology and Infectious Disease."}
{"id": "2601.08100", "categories": ["stat.ML", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08100", "abs": "https://arxiv.org/abs/2601.08100", "authors": ["Xinping Yi", "Gaojie Jin", "Xiaowei Huang", "Shi Jin"], "title": "Towards A Unified PAC-Bayesian Framework for Norm-based Generalization Bounds", "comment": null, "summary": "Understanding the generalization behavior of deep neural networks remains a fundamental challenge in modern statistical learning theory. Among existing approaches, PAC-Bayesian norm-based bounds have demonstrated particular promise due to their data-dependent nature and their ability to capture algorithmic and geometric properties of learned models. However, most existing results rely on isotropic Gaussian posteriors, heavy use of spectral-norm concentration for weight perturbations, and largely architecture-agnostic analyses, which together limit both the tightness and practical relevance of the resulting bounds. To address these limitations, in this work, we propose a unified framework for PAC-Bayesian norm-based generalization by reformulating the derivation of generalization bounds as a stochastic optimization problem over anisotropic Gaussian posteriors. The key to our approach is a sensitivity matrix that quantifies the network outputs with respect to structured weight perturbations, enabling the explicit incorporation of heterogeneous parameter sensitivities and architectural structures. By imposing different structural assumptions on this sensitivity matrix, we derive a family of generalization bounds that recover several existing PAC-Bayesian results as special cases, while yielding bounds that are comparable to or tighter than state-of-the-art approaches. Such a unified framework provides a principled and flexible way for geometry-/structure-aware and interpretable generalization analysis in deep learning."}
{"id": "2601.08655", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.08655", "abs": "https://arxiv.org/abs/2601.08655", "authors": ["Shi-Shun Chen", "Dong-Hua Niu", "Wen-Bin Chen", "Jia-Yun Song", "Ya-Fei Zhang", "Xiao-Yang Li", "Enrico Zio"], "title": "Reliability Modeling of Single-Sided Aluminized Polyimide Films during Storage Considering Stress-Induced Degradation Mechanism Transition", "comment": null, "summary": "Single-sided aluminized polyimide films (SAPF) are widely used in thermal management of aerospace systems. Although the reliability of SAPF in space environments has been thoroughly studied, its reliability in ground environments during storage is always ignored, potentially leading to system failure. This paper aims to investigate the reliability of SAPF in storage environments, focusing on the effects of temperature and relative humidity. Firstly, the relationship between the performance degradation of SAPF and aluminum corrosion is identified. Next, considering the presence of two distinct stages in the influence of temperature on aluminum corrosion, a novel degradation model accounting for the degradation mechanism transition is developed. Additionally, a parameter analysis method is proposed for determining SAPF degradation mechanism based on experimental data. Then, a statistical analysis method incorporating an improved rime optimization algorithm is employed for parameter estimation, and the reliability model is established. Experimental results demonstrate that the proposed method effectively identifies two distinct stages in the impact of temperature on SAPF performance degradation. Furthermore, the proposed degradation model outperforms traditional degradation models with unchanged degradation mechanism in terms of degradation prediction accuracy, extrapolation capability and robustness, indicating its suitability for describing the degradation pattern of SAPFs."}
{"id": "2601.08067", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.08067", "abs": "https://arxiv.org/abs/2601.08067", "authors": ["André F. B. Menezes", "Andrew C. Parnell", "Keefe Murphy"], "title": "Bayesian nonparametric models for zero-inflated count-compositional data using ensembles of regression trees", "comment": null, "summary": "Count-compositional data arise in many different fields, including high-throughput microbiome sequencing and palynology experiments, where a common, important goal is to understand how covariates relate to the observed compositions. Existing methods often fail to simultaneously address key challenges inherent in such data, namely: overdispersion, an excess of zeros, cross-sample heterogeneity, and nonlinear covariate effects. To address these concerns, we propose novel Bayesian models based on ensembles of regression trees. Specifically, we leverage the recently introduced zero-and-$N$-inflated multinomial distribution and assign independent nonparametric Bayesian additive regression tree (BART) priors to both the compositional and structural zero probability components of our model, to flexibly capture covariate effects. We further extend this by adding latent random effects to capture overdispersion and more general dependence structures among the categories. We develop an efficient inferential algorithm combining recent data augmentation schemes with established BART sampling routines. We evaluate our proposed models in simulation studies and illustrate their applicability with two case studies in microbiome and palaeoclimate modelling."}
{"id": "2601.08236", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08236", "abs": "https://arxiv.org/abs/2601.08236", "authors": ["Pei Heng", "Yi Sun", "Jianhua Guo"], "title": "Structural Dimension Reduction in Bayesian Networks", "comment": "13 pages", "summary": "This work introduces a novel technique, named structural dimension reduction, to collapse a Bayesian network onto a minimum and localized one while ensuring that probabilistic inferences between the original and reduced networks remain consistent. To this end, we propose a new combinatorial structure in directed acyclic graphs called the directed convex hull, which has turned out to be equivalent to their minimum localized Bayesian networks. An efficient polynomial-time algorithm is devised to identify them by determining the unique directed convex hulls containing the variables of interest from the original networks. Experiments demonstrate that the proposed technique has high dimension reduction capability in real networks, and the efficiency of probabilistic inference based on directed convex hulls can be significantly improved compared with traditional methods such as variable elimination and belief propagation algorithms. The code of this study is open at \\href{https://github.com/Balance-H/Algorithms}{https://github.com/Balance-H/Algorithms} and the proofs of the results in the main body are postponed to the appendix."}
{"id": "2601.07864", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07864", "abs": "https://arxiv.org/abs/2601.07864", "authors": ["Mengta Chung"], "title": "A Symmetric Random Scan Collapsed Gibbs Sampler for Fully Bayesian Variable Selection with Spike-and-Slab Priors", "comment": null, "summary": "We introduce a symmetric random scan Gibbs sampler for scalable Bayesian variable selection that eliminates storage of the full cross-product matrix by computing required quantities on-the-fly. Data-informed proposal weights, constructed from marginal correlations, concentrate sampling effort on promising candidates while a uniform mixing component ensures theoretical validity. We provide explicit guidance for selecting tuning parameters based on the ratio of signal to null correlations, ensuring adequate posterior exploration. The posterior-mean-size selection rule provides an adaptive alternative to the median probability model that automatically calibrates to the effective signal density without requiring an arbitrary threshold. In simulations with one hundred thousand predictors, the method achieves sensitivity of 1.000 and precision above 0.76. Application to a genomic dataset studying riboflavin production in Bacillus subtilis identifies six genes, all validated by previous studies using alternative methods. The underlying model combines a Dirac spike-and-slab prior with Laplace-type shrinkage: the Dirac spike enforces exact sparsity by setting inactive coefficients to precisely zero, while the Laplace-type slab provides adaptive regularization for active coefficients through a local-global scale mixture."}
{"id": "2601.08084", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.08084", "abs": "https://arxiv.org/abs/2601.08084", "authors": ["Xiaoping Shi", "Baisuo Jin", "Xianhui Liu", "Qiong Li"], "title": "REAMP: A Stochastic Resonance Approach for Multi-Change Point Detection in High-Dimensional Data", "comment": "15 pages, 5 figures", "summary": "Detecting multiple structural breaks in high-dimensional data remains a challenge, particularly when changes occur in higher-order moments or within complex manifold structures. In this paper, we propose REAMP (Resonance-Enhanced Analysis of Multi-change Points), a novel framework that integrates optimal transport theory with the physical principles of stochastic resonance. By utilizing a two-stage dimension reduction via the Earth Movers Distance (EMD) and Shortest Hamiltonian Paths (SHP), we map high-dimensional observations onto a graph-based count statistic. To overcome the locality constraints of traditional search algorithms, we implement a stochastic resonance system that utilizes randomized Beta-density priors to vibrate the objective function. This process allows multiple change points to resonate as global minima across iterative simulations, generating a candidate point cloud. A double-sharpening procedure is then applied to these candidates to pinpoint precise change point locations. We establish the asymptotic consistency of the resonance estimator and demonstrate through simulations that REAMP outperforms state-of-the-art methods, especially in scenarios involving simultaneous mean and variance shifts. The practical utility of the method is further validated through an application to time-lapse embryo monitoring, where REAMP provides both accurate detection and intuitive visualization of cell division stages."}
{"id": "2601.08618", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.08618", "abs": "https://arxiv.org/abs/2601.08618", "authors": ["The Tien Mai"], "title": "Robust low-rank estimation with multiple binary responses using pairwise AUC loss", "comment": null, "summary": "Multiple binary responses arise in many modern data-analytic problems. Although fitting separate logistic regressions for each response is computationally attractive, it ignores shared structure and can be statistically inefficient, especially in high-dimensional and class-imbalanced regimes. Low-rank models offer a natural way to encode latent dependence across tasks, but existing methods for binary data are largely likelihood-based and focus on pointwise classification rather than ranking performance. In this work, we propose a unified framework for learning with multiple binary responses that directly targets discrimination by minimizing a surrogate loss for the area under the ROC curve (AUC). The method aggregates pairwise AUC surrogate losses across responses while imposing a low-rank constraint on the coefficient matrix to exploit shared structure. We develop a scalable projected gradient descent algorithm based on truncated singular value decomposition. Exploiting the fact that the pairwise loss depends only on differences of linear predictors, we simplify computation and analysis. We establish non-asymptotic convergence guarantees, showing that under suitable regularity conditions, leading to linear convergence up to the minimax-optimal statistical precision. Extensive simulation studies demonstrate that the proposed method is robust in challenging settings such as label switching and data contamination and consistently outperforms likelihood-based approaches."}
{"id": "2601.07977", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07977", "abs": "https://arxiv.org/abs/2601.07977", "authors": ["K. Ken Peng", "Charmaine B. Dean", "Robert Delatolla", "X. Joan Hu", "Elizabeth Renouf"], "title": "Joint Modeling of Two Stochastic Processes, with Application to Learning Hospitalization Dynamics from Wastewater Viral Concentrations", "comment": null, "summary": "In the post-pandemic era of COVID-19, hospitalization remains a primary public health concern and wastewater surveillance has become an important tool for monitoring its dynamics at the level of community. However, there is usually no sufficient information to know the infection process that results in both wastewater viral signals and hospital admissions. That key challenge has motived a statistical framework proposed in this paper. We formulate the connection of overtime wastewater viral signals and hospitalization counts through a latent process of infection at the level of individual subject. We provide a strategy for accommodating aggregated data, a typical form of surveillance data. Moreover, we ease the conventional procedure of the statistical learning with the joint modeling using available information on the infection process, which can be under-reporting. A simulation study demonstrates that the proposed approach yields stable inference under different degrees of under-ascertainment. The COVID-19 surveillance data from Ottawa, Canada shows that the framework recovers coherent temporal patterns in infection prevalence and variant-specific hospitalization risk under several reporting assumptions."}
{"id": "2601.08596", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.08596", "abs": "https://arxiv.org/abs/2601.08596", "authors": ["Marcus Gehrmann", "Håkon Tjelmeland"], "title": "Sparsifying transform priors in Gaussian graphical models", "comment": null, "summary": "Bayesian methods constitute a popular approach for estimating the conditional independence structure in Gaussian graphical models, since they can quantify the uncertainty through the posterior distribution. Inference in this framework is typically carried out with Markov chain Monte Carlo (MCMC). However, the most widely used choice of prior distribution for the precision matrix, the so called G-Wishart distribution, suffers from an intractable normalizing constant, which gives rise to the problem of double intractability in the updating steps of the MCMC algorithm. In this article, we propose a new class of prior distributions for the precision matrix, termed ST priors, that allow for the construction of MCMC algorithms that do not suffer from double intractability issues. A realization from an ST prior distribution is obtained by applying a sparsifying transform on a matrix from a distribution with support in the set of all positive definite matrices. We carefully present the theory behind the construction of our proposed class of priors and also perform some numerical experiments, where we apply our methods on a human gene expression dataset. The results suggest that our proposed MCMC algorithm is able to converge and achieve acceptable mixing when applied on the real data."}
{"id": "2601.08784", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08784", "abs": "https://arxiv.org/abs/2601.08784", "authors": ["Arturo Pérez-Peralta", "Sandra Benítez-Peña", "Rosa E. Lillo"], "title": "On the use of graph models to achieve individual and group fairness", "comment": "75 pages, 46 figures", "summary": "Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs."}
{"id": "2601.07980", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07980", "abs": "https://arxiv.org/abs/2601.07980", "authors": ["K. Ken Peng", "X. Joan Hu", "Tim B. Swartz"], "title": "Modeling Event Dynamics by Self-Exciting Processes with Random Memory", "comment": null, "summary": "Event history data from sports competitions have recently drawn increasing attention in sports analytics to generate data-driven strategies. Such data often exhibit self-excitation in the event occurrence and dependence within event clusters. The conventional event models based on gap times may struggle to capture those features. In particular, while consecutive events may occur within a short timeframe, the self-excitation effect caused by previous events is often transient and continues for a period of uncertain time. This paper introduces an extended Hawkes process model with random self-excitation duration to formulate the dynamics of event occurrence. We present examples of the proposed model and procedures for estimating the associated model parameters. We employ the collection of the corner kicks in the games of the 2019 regular season of the Chinese Super League to motivate and illustrate the modeling and its usefulness. We also design algorithms for simulating the event process under proposed models. The proposed approach can be adapted with little modification in many other research fields such as Criminology and Infectious Disease."}
{"id": "2601.08600", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.08600", "abs": "https://arxiv.org/abs/2601.08600", "authors": ["Rodrigo M. R. de Medeiros", "Francisco F. Queiroz"], "title": "Flexible modeling of nonnegative continuous data: Box-Cox symmetric regression and its zero-adjusted extension", "comment": null, "summary": "The Box-Cox symmetric distributions constitute a broad class of probability models for positive continuous data, offering flexibility in modeling skewness and tail behavior. Their parameterization allows a straightforward quantile-based interpretation, which is particularly useful in regression modeling. Despite their potential, only a few specific distributions within this class have been explored in regression contexts, and zero-adjusted extensions have not yet been formally addressed in the literature. This paper formalizes the class of Box-Cox symmetric regression models and introduces a new zero-adjusted extension suitable for modeling data with a non-negligible proportion of observations equal to zero. We discuss maximum likelihood estimation, assess finite-sample performance through simulations, and develop diagnostic tools including residual analysis, local influence measures, and goodness-of-fit statistics. An empirical application on basic education expenditure illustrates the models' ability to capture complex patterns in zero-inflated and highly skewed nonnegative data. To support practical use, we developed the new BCSreg R package, which implements all proposed methods."}
{"id": "2601.07979", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.07979", "abs": "https://arxiv.org/abs/2601.07979", "authors": ["Hanzhang Lu", "Keiran Malott", "Venkat Suprabath Bitra", "Kirsty Milligan", "Sanjeena Subedi", "Edana Cassol", "Vinita Chauhan", "Connor McNairn", "Bryan Muir", "Prarthana Pasricha", "Sangeeta Murugkar", "Rowan Thomson", "Andrew Jirasek", "Jeffrey L. Andrews"], "title": "Spatial Covariance Constraints for Gaussian Mixture Models", "comment": "19 pages, 7 figures", "summary": "Although extensive research exists in spatial modeling, few studies have addressed finite mixture model-based clustering methods for spatial data. Finite mixture models, especially Gaussian mixture models, particularly suffer from high dimensionality due to the number of free covariance parameters. This study introduces a spatial covariance constraint for Gaussian mixture models that requires only four free parameters for each component, independent of dimensionality. Using a coordinate system, the spatially constrained Gaussian mixture model enables clustering of multi-way spatial data and inference of spatial patterns. The parameter estimation is conducted by combining the expectation-maximization (EM) algorithm with the generalized least squares (GLS) estimator. Simulation studies and applications to Raman spectroscopy data are provided to demonstrate the proposed model."}
{"id": "2601.08610", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.08610", "abs": "https://arxiv.org/abs/2601.08610", "authors": ["Wenxuan Guo", "Panos Toulis", "Yuhao Wang"], "title": "Permutation Inference under Multi-way Clustering and Missing Data", "comment": "55 pages, 5 figures", "summary": "Econometric applications with multi-way clustering often feature a small number of effective clusters or heavy-tailed data, making standard cluster-robust and bootstrap inference unreliable in finite samples. In this paper, we develop a framework for finite-sample valid permutation inference in linear regression with multi-way clustering under an assumption of conditional exchangeability of the errors. Our assumption is closely related to the notion of separate exchangeability studied in earlier work, but can be more realistic in many economic settings as it imposes minimal restrictions on the covariate distribution. We construct permutation tests of significance that are valid in finite samples and establish theoretical power guarantees, in contrast to existing methods that are justified only asymptotically. We also extend our methodology to settings with missing data and derive power results that reveal phase transitions in detectability. Through simulation studies, we demonstrate that the proposed tests maintain correct size and competitive power, while standard cluster-robust and bootstrap procedures can exhibit substantial size distortions."}
{"id": "2601.08707", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.08707", "abs": "https://arxiv.org/abs/2601.08707", "authors": ["Kosuke Morikawa", "Jae Kwang Kim"], "title": "Semiparametric Efficient Data Integration Using the Dual-Frame Sampling Framework", "comment": null, "summary": "Integrating probability and non-probability samples is increasingly important, yet unknown sampling mechanisms in non-probability sources complicate identification and efficient estimation. We develop semiparametric theory for dual-frame data integration and propose two complementary estimators. The first models the non-probability inclusion probability parametrically and attains the semiparametric efficiency bound. We introduce an identifiability condition based on strong monotonicity that identifies sampling-model parameters without instrumental variables, even under informative (non-ignorable) selection, using auxiliary information from the probability sample; it remains valid without record linkage between samples. The second estimator, motivated by a two-stage sampling approximation, avoids explicit modeling of the non-probability mechanism; though not fully efficient, it is efficient within a restricted augmentation class and is robust to misspecification. Simulations and an application to the Culture and Community in a Time of Crisis public simulation dataset show efficiency gains under correct specification and stable performance under misspecification and weak identification. Methods are implemented in the R package \\texttt{dfSEDI}."}
{"id": "2601.08736", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.08736", "abs": "https://arxiv.org/abs/2601.08736", "authors": ["Ping Zhao", "Long Feng"], "title": "Note on High Dimensional Spatial-Sign Test for One Sample Problem", "comment": null, "summary": "We revisit the null distribution of the high-dimensional spatial-sign test of Wang et al. (2015) under mild structural assumptions on the scatter matrix. We show that the standardized test statistic converges to a non-Gaussian limit, characterized as a mixture of a normal component and a weighted chi-square component. To facilitate practical implementation, we propose a wild bootstrap procedure for computing critical values and establish its asymptotic validity. Numerical experiments demonstrate that the proposed bootstrap test delivers accurate size control across a wide range of dependence settings and dimension-sample-size regimes."}
{"id": "2601.08618", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.08618", "abs": "https://arxiv.org/abs/2601.08618", "authors": ["The Tien Mai"], "title": "Robust low-rank estimation with multiple binary responses using pairwise AUC loss", "comment": null, "summary": "Multiple binary responses arise in many modern data-analytic problems. Although fitting separate logistic regressions for each response is computationally attractive, it ignores shared structure and can be statistically inefficient, especially in high-dimensional and class-imbalanced regimes. Low-rank models offer a natural way to encode latent dependence across tasks, but existing methods for binary data are largely likelihood-based and focus on pointwise classification rather than ranking performance. In this work, we propose a unified framework for learning with multiple binary responses that directly targets discrimination by minimizing a surrogate loss for the area under the ROC curve (AUC). The method aggregates pairwise AUC surrogate losses across responses while imposing a low-rank constraint on the coefficient matrix to exploit shared structure. We develop a scalable projected gradient descent algorithm based on truncated singular value decomposition. Exploiting the fact that the pairwise loss depends only on differences of linear predictors, we simplify computation and analysis. We establish non-asymptotic convergence guarantees, showing that under suitable regularity conditions, leading to linear convergence up to the minimax-optimal statistical precision. Extensive simulation studies demonstrate that the proposed method is robust in challenging settings such as label switching and data contamination and consistently outperforms likelihood-based approaches."}
