<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 20]
- [stat.CO](#stat.CO) [Total: 4]
- [stat.ML](#stat.ML) [Total: 13]
- [stat.AP](#stat.AP) [Total: 5]
- [stat.OT](#stat.OT) [Total: 1]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Estimation and Inference of the Win Ratio for Two Hierarchical Endpoints Subject to Censoring and Missing Data](https://arxiv.org/abs/2602.13533)
*Yi Liu,Huiman Barnhart,Sean O'Brien,Yuliya Lokhnygina,Roland A. Matsouaka*

Main category: stat.ME

TL;DR: 提出一种基于非参数最大似然估计的胜率估计方法，能够处理删失和缺失数据，相比传统方法更高效稳健


<details>
  <summary>Details</summary>
Motivation: 传统胜率估计方法（如Pocock算法）在处理删失或缺失数据时存在不足，可能导致有偏且低效的估计，特别是在治疗组间存在异质性删失或缺失数据时。现有方法无法有效处理包含非生存终点且存在缺失数据的情况。

Method: 提出简单的非参数最大似然估计器（NPMLE）用于估计两个分层终点的胜率，该方法能够处理删失和缺失数据，利用所有观测数据，避免强参数假设，并提供闭式渐近方差估计器。

Result: 通过模拟研究和基于HEART-FID和ISCHEMIA试验的两个数据示例验证了方法的性能。该方法提供了一致估计器，提高了估计效率，在非信息性删失和随机缺失假设下具有稳健性。

Conclusion: 提出的NPMLE方法为现有胜率估计方法提供了灵活替代方案，能够更好地处理删失和缺失数据，并提供了用户友好的R包WinRS以促进实施。

Abstract: The win ratio (WR) is a widely used metric to compare treatments in randomized clinical trials with hierarchically ordered endpoints. Counting-based approaches, such as Pocock's algorithm, are the standard for WR estimation. However, this algorithm treats participants with censored or missing data inadequately, which may lead to biased and inefficient estimates, particularly in the presence of heterogeneous censoring or missing data between treatment groups. Although recent extensions have addressed some of these limitations for hierarchical time-to-event endpoints, no existing methods -- aside from the computationally intensive multiple imputation approach -- can accommodate settings that include non-survival endpoints that are subject to missing data. In this paper, we propose a simple nonparametric maximum likelihood estimator (NPMLE) of WR for two hierarchical endpoints that are subject to censoring and missing data. Our method uses all observed data, avoids strong parametric assumptions, and comes with a closed-form asymptotic variance estimator. We demonstrate its performance using simulation studies and two data examples, based on the HEART-FID and ISCHEMIA trials. The proposed method provides a consistent estimator, improves estimation efficiency, and is robust under non-informative censoring and missing at random (MAR) assumptions, offering a flexible alternative to existing WR estimation methods. A user-friendly R package, WinRS, is available to facilitate implementation.

</details>


### [2] [Robust Design in the Presence of Aleatoric and Epistemic Uncertainty](https://arxiv.org/abs/2602.13380)
*Luis G. Crespo*

Main category: stat.ME

TL;DR: 提出针对同时存在随机性和认知不确定性的系统设计策略，通过机会约束公式在有限个不确定参数实现中权衡目标值与鲁棒性，并考虑违反约束的严重程度获得风险感知设计。


<details>
  <summary>Details</summary>
Motivation: 现实系统设计中同时存在两种不确定性：随机性（物理参数随机变化）和认知不确定性（固定参数值未知）。传统方法难以同时处理这两种不确定性，需要开发能权衡目标性能与鲁棒性的设计策略。

Method: 1) 提出机会约束公式，在有限个不确定参数实现中执行系统要求；2) 通过最优选择消除部分实现来权衡目标值与鲁棒性；3) 考虑违反约束的严重程度获得风险感知设计；4) 提出计算高效的设计方法，根据高保真可靠性分析结果顺序更新训练数据集；5) 使用蒙特卡洛分析和鲁棒场景理论评估鲁棒性。

Result: 开发了一套完整的系统设计框架，能够同时处理随机性和认知不确定性，在目标性能与鲁棒性之间实现最优权衡，并通过风险感知设计考虑约束违反的严重性。

Conclusion: 提出的策略为具有混合不确定性的系统设计提供了有效解决方案，通过机会约束公式、风险感知设计和顺序更新方法，在保证计算效率的同时实现了鲁棒性与性能的平衡。

Abstract: This paper proposes strategies for designing a system whose computational model is subject to aleatory and epistemic uncertainty. Aleatory variables, which are caused by randomness in physical parameters, are draws from a possibly unknown distribution; whereas epistemic variables, which are caused by ignorance in the value of fixed parameters, are free to take any value in a bounded set. Chance-constrained formulations enforcing the system requirements at a finite number of realizations of the uncertain parameters are proposed. These formulations trade off a lower objective value against a reduced robustness by eliminating an optimally chosen subset of such realizations. Risk-aware designs are obtained by accounting for the severity of the requirement violations resulting from this elimination process. Furthermore, we propose a computationally efficient design approach in which the training dataset is sequentially updated according to the results of high-fidelity reliability analyses of suboptimal designs. Robustness is evaluated by using Monte Carlo analysis and Robust Scenario Theory, with the latter approach accounting for the infinitely many values that the epistemic variables can take.

</details>


### [3] [Mixture-of-experts Wishart model for covariance matrices with an application to Cancer drug screening](https://arxiv.org/abs/2602.13888)
*The Tien Mai,Zhi Zhao*

Main category: stat.ME

TL;DR: 提出贝叶斯混合专家Wishart模型，通过门控网络让混合权重依赖于预测变量，用于分析异质性协方差数据，并在癌症药物敏感性数据中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 协方差矩阵在金融、基因组学、神经科学等领域广泛存在，能够编码依赖结构并揭示复杂多元系统的特征。现有方法难以处理协方差结构的复杂非线性异质性，需要能够适应协变量依赖模式的模型。

Method: 提出混合专家Wishart模型，通过多项式逻辑门控网络让混合权重依赖于预测变量。开发了高效的Gibbs-within-Metropolis-Hastings采样算法，并推导了期望最大化算法用于最大似然估计。

Result: 模拟研究表明，提出的贝叶斯和最大似然估计器在各种异质性协方差场景下都能实现准确的亚群恢复和估计。在癌症药物敏感性数据集上的应用展示了模型能够利用药物剂量和重复测量间的协方差。

Conclusion: MoE-Wishart模型能够有效捕捉协方差结构的复杂非线性异质性，并适应协变量依赖的亚群成员概率。该方法为分析异质性协方差数据提供了强大的框架，并在实际应用中展现出良好性能。

Abstract: Covariance matrices arise naturally in different scientific fields, including finance, genomics, and neuroscience, where they encode dependence structures and reveal essential features of complex multivariate systems. In this work, we introduce a comprehensive Bayesian framework for analyzing heterogeneous covariance data through both classical mixture models and a novel mixture-of-experts Wishart (MoE-Wishart) model. The proposed MoE-Wishart model extends standard Wishart mixtures by allowing mixture weights to depend on predictors through a multinomial logistic gating network. This formulation enables the model to capture complex, nonlinear heterogeneity in covariance structures and to adapt subpopulation membership probabilities to covariate-dependent patterns. To perform inference, we develop an efficient Gibbs-within-Metropolis-Hastings sampling algorithm tailored to the geometry of the Wishart likelihood and the gating network. We additionally derive an Expectation-Maximization algorithm for maximum likelihood estimation in the mixture-of-experts setting. Extensive simulation studies demonstrate that the proposed Bayesian and maximum likelihood estimators achieve accurate subpopulation recovery and estimation under a range of heterogeneous covariance scenarios. Finally, we present an innovative application of our methodology to a challenging dataset: cancer drug sensitivity profiles, illustrating the ability of the MoE-Wishart model to leverage covariance across drug dosages and replicate measurements.
  Our methods are implemented in the \texttt{R} package \texttt{moewishart} available at https://github.com/zhizuio/moewishart .

</details>


### [4] [Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models](https://arxiv.org/abs/2602.14981)
*Tianni Zhang,Yuyao Wang,Yu Lu,and Mengfei Ran*

Main category: stat.ME

TL;DR: 提出基于样条近似和块经验似然的方法，用于广义部分线性单指标模型的纵向数据分析，无需显式方差估计即可获得参数置信区域。


<details>
  <summary>Details</summary>
Motivation: 广义部分线性单指标模型为纵向数据提供了灵活且可解释的半参数框架，但在重复测量中，由于受试者内相关性引入冗余参数，且半参数设置下方差估计不稳定，使得有效推断具有挑战性。

Method: 采用基于样条近似未知连接函数的轮廓估计方程方法，构建受试者级别的块经验似然，用于参数系数和单指标方向的联合推断。所得BEL比统计量具有Wilks型卡方极限，无需显式三明治方差估计即可获得似然无关的置信区域。

Result: 模拟研究和癫痫纵向研究应用表明该方法在有限样本下表现良好。BEL比统计量具有卡方极限分布，能够有效进行参数推断。

Conclusion: 所提出的块经验似然方法为广义部分线性单指标模型的纵向数据分析提供了有效的推断工具，避免了传统方法中方差估计不稳定的问题，具有实际应用价值。

Abstract: Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance.

</details>


### [5] [Measuring Neural Network Complexity via Effective Degrees of Freedom](https://arxiv.org/abs/2602.13442)
*Jia Zhou,Douglas Landsittel*

Main category: stat.ME

TL;DR: 该研究将广义自由度(GDF)应用于二元结果的前馈神经网络，提供了一种不依赖似然假设的稳健模型复杂度度量方法。


<details>
  <summary>Details</summary>
Motivation: 前馈神经网络因其非线性、层次化结构和大量参数，模型复杂度难以量化。现有方法如基于交叉验证的有效参数数和零自由度都依赖似然假设且存在变异性问题。

Method: 将GDF算法适配到离散响应(二元结果)的神经网络，通过模拟研究和真实数据分析，比较GDF与基于对数似然交叉验证的有效参数数以及Landsittel等人的零自由度方法。

Result: GDF提供了对神经网络模型复杂度的稳健评估，仅依赖于拟合值对观测响应扰动的敏感性，而不依赖似然假设。基于交叉验证的复杂度估计和零自由度方法则依赖假设的似然正确性且表现出较大变异性。

Conclusion: GDF是神经网络统计建模中稳定且广泛适用的模型复杂度度量方法，只有当拟合模型充分代表数据生成机制时，GDF、交叉验证方法和零自由度才会给出相似的复杂度评估。

Abstract: Quantifying the complexity of feed-forward neural networks (FFNNs) remains challenging due to their nonlinear, hierarchical structure and numerous parameters. We apply generalized degrees of freedom (GDF) to measure model complexity in FFNNs with binary outcomes, adapting the algorithm for discrete responses. We compare GDF with both the effective number of parameters derived via log-likelihood cross-validation and the null degrees of freedom of Landsittel et al. Through simulation studies and a real data analysis, we demonstrate that GDF provides a robust assessment of model complexity for neural network models, as it depends only on the sensitivity of fitted values to perturbations in the observed responses rather than on assumptions about the likelihood. In contrast, cross-validation-based estimates of model complexity and the null degrees of freedom rely on the correctness of the assumed likelihood and may exhibit substantial variability. We find that GDF, cross-validation-based measures, and null degrees of freedom yield similar assessments of model complexity only when the fitted model adequately represents the data-generating mechanism. These findings highlight GDF as a stable and broadly applicable measure of model complexity for neural networks in statistical modeling.

</details>


### [6] [A Bayesian Approach to Low-Discrepancy Subset Selection](https://arxiv.org/abs/2602.14607)
*Nathan Kirk*

Main category: stat.ME

TL;DR: 本文证明了基于核差异的子集选择问题是NP难的，并提出了一种使用深度嵌入核的贝叶斯优化方法来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 低差异设计在准蒙特卡洛方法和其他领域（如机器学习、机器人学、计算机图形学）中非常重要。子集选择作为一种低差异构造方法近年来受到广泛关注，但已知该问题的某些版本是NP难的。本文旨在研究基于核差异的子集选择问题的计算复杂性，并为这一难解问题提供有效的解决方案。

Method: 首先证明了基于核差异的子集选择问题是NP难的。然后提出了一种贝叶斯优化（BO）方法，利用深度嵌入核的概念来解决该问题。该方法旨在最小化差异度量，并且框架具有广泛适用性，可用于任何设计标准。

Result: 首次确立了基于核差异的子集选择问题的NP难性。提出的贝叶斯优化算法在最小化差异度量方面表现出良好性能，且该框架具有广泛的适用性。

Conclusion: 基于核差异的子集选择问题是NP难的，这解释了其计算复杂性。提出的贝叶斯优化方法为解决这一难解问题提供了有效的途径，并且该框架可广泛应用于各种设计标准。

Abstract: Low-discrepancy designs play a central role in quasi-Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP-hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP-hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria.

</details>


### [7] [Efficient and Debiased Learning of Average Hazard Under Non-Proportional Hazards](https://arxiv.org/abs/2602.13475)
*Xiang Meng,Lu Tian,Kenneth Kehl,Hajime Uno*

Main category: stat.ME

TL;DR: 本文提出了一种半参数、双重稳健的框架来估计协变量调整后的平均风险（AH），作为Cox比例风险模型中风险比（HR）的替代因果估计量，特别是在风险非比例的情况下。


<details>
  <summary>Details</summary>
Motivation: 当风险非比例时，Cox比例风险模型中的风险比会失去稳定的因果解释，并变得依赖于研究设计，因为它实际上是用随访和删失决定的权重对时变效应进行平均。需要一种在非比例风险情况下仍保持良好定义和可解释的替代因果估计量。

Method: 开发了一个半参数、双重稳健的框架用于协变量调整的平均风险（AH）估计。建立了AH在非参数模型中的路径可微性，推导了其有效影响函数，并构建了交叉拟合、去偏估计器，利用机器学习进行干扰参数估计，同时在温和的产品率条件下保持渐近正态、√n一致的推断。

Result: 模拟研究表明，所提出的估计器在比例和非比例风险设置（包括交叉风险机制）下实现了小偏差和接近名义水平的置信区间覆盖率，而基于Cox的总结在这些情况下可能不稳定。在实际应用中，使用SEER-Medicare关联数据比较了晚期黑色素瘤的免疫治疗方案。

Conclusion: 平均风险（AH）作为风险比（HR）的替代因果估计量，在风险非比例时仍保持稳定解释。提出的半参数、双重稳健框架允许使用机器学习进行灵活的干扰参数估计，同时保持有效的统计推断，为比较有效性研究提供了实用工具。

Abstract: The hazard ratio from the Cox proportional hazards model is a ubiquitous summary of treatment effect. However, when hazards are non-proportional, the hazard ratio can lose a stable causal interpretation and become study-dependent because it effectively averages time-varying effects with weights determined by follow-up and censoring. We consider the average hazard (AH) as an alternative causal estimand: a population-level person-time event rate that remains well-defined and interpretable without assuming proportional hazards. Although AH can be estimated nonparametrically and regression-style adjustments have been proposed, existing approaches do not provide a general framework for flexible, high-dimensional nuisance estimation with valid sqrt{n} inference. We address this gap by developing a semiparametric, doubly robust framework for covariate-adjusted AH. We establish pathwise differentiability of AH in the nonparametric model, derive its efficient influence function, and construct cross-fitted, debiased estimators that leverage machine learning for nuisance estimation while retaining asymptotically normal, sqrt{n}-consistent inference under mild product-rate conditions. Simulations demonstrate that the proposed estimator achieves small bias and near-nominal confidence-interval coverage across proportional and non-proportional hazards settings, including crossing-hazards regimes where Cox-based summaries can be unstable. We illustrate practical utility in comparative effectiveness research by comparing immunotherapy regimens for advanced melanoma using SEER-Medicare linked data.

</details>


### [8] [Towards Semiparametric Bandwidth Selectors for Kernel Density Estimators](https://arxiv.org/abs/2602.13518)
*Nils Lid Hjort*

Main category: stat.ME

TL;DR: 该报告提出了一种基于正态分布Hermite展开的半参数带宽选择方法，作为完全非参数方法和正态参考规则之间的折中方案。


<details>
  <summary>Details</summary>
Motivation: 现有核密度估计的带宽选择方法要么是完全非参数的（不需要任何密度知识），要么是高度参数化的（如基于正态分布的参考规则）。需要一种介于两者之间的半参数方法。

Method: 通过围绕正态分布的Hermite类型展开，开发半参数带宽选择器。这些带宽选择器可以看作是对正态参考规则进行适当修正，只需要从数据中估计少量额外系数。

Result: 报告介绍了基本思想并发展了必要的初始理论，但尚未给出具体程序的精确推荐。需要进一步的分析、数值工作和模拟探索来确定最佳实施方案。

Conclusion: 该研究为带宽选择问题提供了一个有前景的半参数框架，填补了完全非参数和高度参数化方法之间的空白，但具体实施细节需要进一步研究。

Abstract: There is an intense and partly recent literature focussing on the problem of selecting the bandwidth parameter for kernel density estimators. Available methods are largely `very nonparametric', in the sense of not requiring any knowledge about the underlying density, or `very parametric', like the normality-based reference rule. This report aims at widening the scope towards the inclusion of many semiparametric bandwidth selectors, via Hermite type expansions aroundthe normal distribution. The resulting bandwidths may be seen as carrying out suitable corrections on the normal reference rule, requiring a low number of extra coefficients to be estimated from data.
  The present report introduces and discusses some basic ideas and develops the necessary initial theory, but modestly chooses to stop short of giving precise recommendations for specific procedures among the many possible constructions. This will require some further analysis, numerical work, and some simulation-based exploration.

</details>


### [9] [The Role of Measured Covariates in Assessing Sensitivity to Unmeasured Confounding](https://arxiv.org/abs/2602.14414)
*Abhinandan Dalal,Iris Horng,Yang Feng,Dylan S. Small*

Main category: stat.ME

TL;DR: 本文研究了敏感性分析与测量协变量结构之间的相互作用，发现在线性回归中，暴露与测量代理变量之间的强关联会放大对残余混杂的敏感性，并通过吸烟与肺癌的例子展示了社会经济分层如何随时间增加敏感性。


<details>
  <summary>Details</summary>
Motivation: 敏感性分析在观察性研究中广泛用于评估因果结论的稳健性，但其与测量协变量结构的相互作用常被忽视。当潜在混杂因素无法直接调整而使用代理变量控制时，暴露与测量代理变量之间的强关联会放大对残余混杂的敏感性。

Method: 在线性回归框架中形式化这一现象，证明一个简单的比率（涉及暴露模型系数和残余暴露方差）提供了这种敏感性增加的观测度量。将该框架应用于吸烟与肺癌的研究，分析随时间变化的社会经济分层如何影响敏感性。

Result: 研究发现，暴露与测量代理变量之间的强关联确实会放大对未测量混杂的敏感性。在吸烟与肺癌的例子中，随时间增长的社会经济分层导致近期数据对未测量混杂的敏感性显著增加。

Conclusion: 研究结果强调了在基于代理调整的敏感性分析中考虑多重共线性的重要性，暴露与测量代理变量之间的强关联会放大对残余混杂的敏感性，这在解释敏感性分析时需要特别注意。

Abstract: Sensitivity analysis is widely used to assess the robustness of causal conclusions in observational studies, yet its interaction with the structure of measured covariates is often overlooked. When latent confounders cannot be directly adjusted for and are instead controlled using proxy variables, strong associations between exposure and measured proxies can amplify sensitivity to residual confounding. We formalize this phenomenon in linear regression settings by showing that a simple ratio involving the exposure model coefficient and residual exposure variance provides an observable measure of this increased sensitivity. Applying our framework to smoking and lung cancer, we document how growing socioeconomic stratification in smoking behavior over time leads to heightened sensitivity to unmeasured confounding in more recent data. These results highlight the importance of multicollinearity when interpreting sensitivity analyses based on proxy adjustment.

</details>


### [10] [Empirical Bayes data integreation for multi-response regression](https://arxiv.org/abs/2602.13538)
*Antik Chakraborty,Fei Xue*

Main category: stat.ME

TL;DR: 提出一种基于经验贝叶斯的线性收缩估计器，用于整合多源向量值数据，通过收缩数据矩阵的奇异值来估计协方差矩阵，适用于稀疏/密集或低秩/非低秩参数设置。


<details>
  <summary>Details</summary>
Motivation: 组织范围关联研究（TWAS）中需要整合来自不同来源的向量值结果数据，现有稀疏或降秩估计器在参数设置上有限制，需要更灵活且计算可扩展的方法。

Method: 开发经验贝叶斯方法，提出线性收缩估计器收缩数据矩阵的奇异值，扩展到局部线性收缩估计器以增加灵活性，并与特定损失下的协方差矩阵估计相关联。

Result: 方法在稀疏/密集或低秩/非低秩参数设置下均有效，计算上比完全贝叶斯方法更具可扩展性，在数值实验和GTEx项目的真实TWAS数据中表现良好。

Conclusion: 提出的经验贝叶斯方法为TWAS等应用提供了一种灵活、理论可靠且计算高效的数据整合方案，克服了现有方法的参数设置限制。

Abstract: Motivated by applications in tissue-wide association studies (TWAS), we develop a flexible and theoretically grounded empirical Bayes approach for integrating %vector-valued outcomes data obtained from different sources. We propose a linear shrinkage estimator that effectively shrinks singular values of a data matrix. This problem is closely connected to estimating covariance matrices under a specific loss, for which we develop asymptotically optimal estimators. The basic linear shrinkage estimator is then extended to a local linear shrinkage estimator, offering greater flexibility. Crucially, the proposed method works under sparse/dense or low-rank/non low-rank parameter settings unlike well-known sparse or reduced rank estimators in the literature. Furthermore, the empirical Bayes approach offers greater scalability in computation compared to intensive full Bayes procedures. The method is evaluated through an extensive set of numerical experiments, and applied to a real TWAS data obtained from the Genotype-Tissue Expression (GTEx) project.

</details>


### [11] [Backward Smoothing versus Fixed-Lag Smoothing in Particle Filters](https://arxiv.org/abs/2602.13635)
*Genshiro Kitagawa*

Main category: stat.ME

TL;DR: 比较粒子平滑方法在精度-计算成本权衡上的表现，发现FFBS和FFBSm在固定粒子数下优于固定滞后平滑，但固定滞后平滑在相同计算时间下通常精度更高，且高效FFBSm近似对高斯转移有效但对重尾动态效果较差。


<details>
  <summary>Details</summary>
Motivation: 粒子平滑在非线性非高斯状态空间模型中具有重要应用价值，但其高计算成本限制了实际使用。需要研究不同平滑方法在精度和计算效率之间的权衡，特别是针对FFBSm的高效近似方法。

Method: 通过趋势估计示例比较固定滞后平滑、FFBS和FFBSm方法，在Gaussian和重尾（Cauchy型）系统噪声条件下测试，特别关注基于子采样和局部邻域限制的O(m) FFBSm近似方法。

Result: FFBS和FFBSm在固定粒子数下优于固定滞后平滑，但固定滞后平滑在相同计算时间下通常达到更高精度。高效的FFBSm近似对高斯转移有效，但对重尾动态变得不那么有利。

Conclusion: 粒子平滑方法的选择需要在精度和计算效率之间权衡，固定滞后平滑在计算时间受限时可能更优，而FFBSm的高效近似方法对特定噪声类型（高斯）有效但对重尾动态效果有限。

Abstract: Particle smoothing enables state estimation in nonlinear and non-Gaussian state-space models, but its practical use is often limited by high computational cost. Backward smoothing methods such as the Forward Filter Backward Smoother (FFBS) and its marginal form (FFBSm) can achieve high accuracy, yet typically require quadratic computational complexity in the number of particles. This paper examines the accuracy--computational cost trade-offs of particle smoothing methods through a trend-estimation example. Fixed-lag smoothing, FFBS, and FFBSm are compared under Gaussian and heavy-tailed (Cauchy-type) system noise, with particular attention to O(m) approximations of FFBSm based on subsampling and local neighborhood restrictions. The results show that FFBS and FFBSm outperform fixed-lag smoothing at a fixed particle number, while fixed-lag smoothing often achieves higher accuracy under equal computational time. Moreover, efficient FFBSm approximations are effective for Gaussian transitions but become less advantageous for heavy-tailed dynamics.

</details>


### [12] [The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research](https://arxiv.org/abs/2602.14835)
*Evan Hadfield*

Main category: stat.ME

TL;DR: 本文提出了全球代表性指数（GRI），这是一个基于总变差距离的标准化指标，用于量化调查样本在人口统计维度上与目标群体的匹配程度，填补了现有样本质量评估工具的空白。


<details>
  <summary>Details</summary>
Motivation: 全球调查研究对AI治理和跨文化政策的高风险决策日益重要，但缺乏标准化指标来量化样本人口构成与目标群体的匹配程度。现有的响应率和人口配额只能衡量努力程度和覆盖范围，不能评估分布保真度。

Method: 基于总变差距离（Total Variation Distance）构建全球代表性指数（GRI）框架，在[0,1]尺度上对任何调查样本在多个人口统计维度上相对于人口基准进行评分。开发了开源Python库，整合了联合国和皮尤研究中心的人口基准数据。

Result: 验证显示：Global Dialogues调查（7波，N=7,500，60+国家）的精细人口GRI得分仅为0.33-0.36，约为该样本量理论最大值的43%。对世界价值观调查（7波，N=403,000）、Afrobarometer第9轮（N=53,000）和Latinobarometro（N=19,000）的交叉验证表明，即使大型概率调查在精细全球人口统计上的得分也低于0.22。

Conclusion: GRI通过设计效应与经典调查统计连接，建议将GRI和有效样本量作为样本质量的最低总结指标。GRI对称量化人口距离，而有效N捕获代表性不足的不对称推断成本。该框架适用于调查研究、机器学习数据集审计和AI评估基准。

Abstract: Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks.

</details>


### [13] [Predicting fixed-sample test decisions enables anytime-valid inference](https://arxiv.org/abs/2602.13872)
*Chris Holmes,Stephen Walker*

Main category: stat.ME

TL;DR: 提出一种将任意固定样本假设检验转换为任意时间有效检验的简单方法，在控制第一类错误的同时保持接近最优的统计功效，当零假设为假时可显著节省样本量。


<details>
  <summary>Details</summary>
Motivation: 传统统计假设检验使用预设样本量，但数据通常是顺序到达的。中期分析会破坏经典错误保证，而现有序贯方法要么需要严格的测试计划，要么导致统计功效大幅损失。特别是在临床试验等领域，能够安全地提前停止可以确保受试者获得最佳治疗并加速有效疗法的开发。

Method: 该方法将任意固定样本假设检验转换为任意时间有效检验。在每一步，程序预测经典检验在其固定样本量下拒绝零假设的概率，将未来观测值视为零假设下的缺失数据。通过阈值化这个概率来获得任意时间有效的停止规则。

Result: 该方法确保了第一类错误控制，并保持接近最优的统计功效。当零假设为假时，可以显著节省样本量，实现安全提前停止。

Conclusion: 该方法提供了一种简单而有效的框架，用于将传统固定样本检验转换为任意时间有效的序贯检验，在保持统计严谨性的同时，为实际应用（特别是临床试验）提供了灵活性和效率优势。

Abstract: Statistical hypothesis tests typically use prespecified sample sizes, yet data often arrive sequentially. Interim analyses invalidate classical error guarantees, while existing sequential methods require rigid testing preschedules or incur substantial losses in statistical power. We introduce a simple procedure that transforms any fixed-sample hypothesis test into an anytime-valid test while ensuring Type-I error control and near-optimal power with substantial sample savings when the null hypothesis is false. At each step, the procedure predicts the probability that a classical test would reject the null hypothesis at its fixed-sample size, treating future observations as missing data under the null hypothesis. Thresholding this probability yields an anytime-valid stopping rule. In areas such as clinical trials, stopping early and safely can ensure that subjects receive the best treatments and accelerate the development of effective therapies.

</details>


### [14] [Online LLM watermark detection via e-processes](https://arxiv.org/abs/2602.14286)
*Weijie Su,Ruodu Wang,Zinan Zhao*

Main category: stat.ME

TL;DR: 本文提出基于e过程的统一框架用于LLM水印检测，提供在线测试的随时有效性保证，并通过实验验证其竞争力。


<details>
  <summary>Details</summary>
Motivation: LLM水印技术已成为区分AI生成文本与人类写作的有效工具，但现有检测方法缺乏统一的统计框架和在线测试的严格保证。

Method: 开发基于e过程的统一水印检测框架，提出构建经验自适应e过程的方法来增强检测能力，建立理论分析检测方法的功效特性。

Result: 实验表明，提出的框架在检测性能上可与现有水印检测方法竞争，理论分析验证了所提方法的功效特性。

Conclusion: 基于e过程的统一框架为LLM水印检测提供了具有严格统计保证的方法，经验自适应e过程能有效提升检测能力，为在线水印检测提供了可靠解决方案。

Abstract: Watermarking for large language models (LLMs) has emerged as an effective tool for distinguishing AI-generated text from human-written content. Statistically, watermark schemes induce dependence between generated tokens and a pseudo-random sequence, reducing watermark detection to a hypothesis testing problem on independence. We develop a unified framework for LLM watermark detection based on e-processes, providing anytime-valid guarantees for online testing. We propose various methods to construct empirically adaptive e-processes that can enhance the detection power. In addition, theoretical results are established to characterize the power properties of the proposed procedures. Some experiments demonstrate that the proposed framework achieves competitive performance compared to existing watermark detection methods.

</details>


### [15] [A New SMP Transformed Standard Weibull Distribution for Health Data Modelling](https://arxiv.org/abs/2602.14303)
*Isqeel Ogunsola,Nurudeen Ajadi,Gboyega Adepoju*

Main category: stat.ME

TL;DR: 提出了一种通过SMP方法扩展的Weibull分布（SMPtW），推导了其12种统计性质，通过模拟验证了MLE的一致性，并在健康数据集上优于其他五个竞争模型。


<details>
  <summary>Details</summary>
Motivation: 现有SMP方法尚未应用于Weibull分布，需要开发新的扩展分布以更好地建模现实数据，特别是健康数据。

Method: 通过SMP变换技术扩展Weibull分布，推导12种统计性质（可靠性度量、分位数函数、矩、应力强度、平均等待时间等），采用逆变换方法生成随机数，使用最大似然估计进行参数估计。

Result: SMPtW的危险函数具有递减、递增和恒定形状；发现SMPtW与SMP Pareto分布分位数间的关系；模拟显示MLE具有一致性；在健康数据集上，三参数SMPtW优于其他五个竞争模型。

Conclusion: SMPtW是一个有效的Weibull分布扩展，具有良好的统计性质和实际应用性能，特别适合健康数据分析。

Abstract: New methods of extending base distributions are always invoke to increase their adaptability in modeling real life data. Recently, SMP method was introduced but Weibull distribution is yet to be explored through this method. First, we provide updated review on SMP transformed distributions. We then proposed and developed another extended Weibull distribution through this technique named SMPtW. Importantly, twelve of its statistical properties - reliability measures, quantile function, moment, stress-strength, mean waiting time, moment generating function, characteristics function, renyi entropy, order statistics, mean residual life and mode, were derived and studied extensively. The hazard function has a decreasing, increasing and constant shapes. We found a relation between the quantile of SMPtW and that of SMP Pareto distribution despite their difference in density functions. We adopt the inverse transform approach in random number generation and through simulation we evaluate maximum likelihood estimates (MLE) performance of its parameters. The result showed that MLE is consistent all through. The performance of the distribution was then examined using health dataset compared with five similar distributions. The results showed that three parameters SMPtW performed best among the competing models.

</details>


### [16] [Automatic Variance Adjustment for Small Area Estimation](https://arxiv.org/abs/2602.14387)
*Jon Wakefield,Jitong Jiang,Yunhan Wu*

Main category: stat.ME

TL;DR: 本文提出了一种改进的小区域估计方法，通过添加先验样本来解决稀疏数据下方差估计不稳定或未定义的问题，并在R包surveyPrev中实现。


<details>
  <summary>Details</summary>
Motivation: 在低收入和中等收入国家，家庭调查是获取可靠数据的主要来源，小区域估计对于揭示健康和人口指标差异至关重要。然而，在数据稀疏的精细地理分区中，传统的Fay-Herriot区域级模型所需的方差估计可能未定义或不稳定。

Method: 提出了一种基于原则的改进方法：通过添加来自假设调查的先验样本来增强可用数据。这种方法尊重调查设计，易于实施，并在R包surveyPrev中作为自动补救措施实现。

Result: 通过模拟研究验证了该调整方法的经验特性，并使用2018年赞比亚人口与健康调查的消瘦数据进行了实际应用演示。该方法能有效解决稀疏数据下的方差估计问题。

Conclusion: 提出的调整方法为小区域估计提供了一种通用的解决方案，特别适用于数据稀疏的低收入和中等收入国家环境，有助于更可靠地估计健康指标的地理差异。

Abstract: Small area estimation (SAE) is a common endeavor and is used in a variety of disciplines. In low- and middle-income countries (LMICs), in which household surveys provide the most reliable and timely source of data, SAE is vital for highlighting disparities in health and demographic indicators. Weighted estimators are ideal for inference, but for fine geographical partitions in which there are insufficient data, SAE models are required. The most common approach is Fay-Herriot area-level modeling in which the data requirements are a weighted estimate and an associated variance estimate. The latter can be undefined or unstable when data are sparse and so we propose a principled modification which is based on augmenting the available data with a prior sample from a hypothetical survey. This adjustment is generally available, respects the design and is simple to implement. We examine the empirical properties of the adjustment through simulation and illustrate its use with wasting data from a 2018 Zambian Demographic and Health Survey. The modification is implemented as an automatic remedy in the R package surveyPrev, which provides a comprehensive suite of tools for conducing SAE in LMICs.

</details>


### [17] [CAIRO: Decoupling Order from Scale in Regression](https://arxiv.org/abs/2602.14440)
*Harri Vanhems,Yue Zhao,Peng Shi,Archer Y. Yang*

Main category: stat.ME

TL;DR: CAIRO框架将回归分解为两个阶段：首先用尺度不变的排序损失学习评分函数，然后用保序回归恢复目标尺度，从而解耦排序学习和尺度学习，提高对异常值和重尾噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法通常优化单一逐点目标（如均方误差），将排序学习和尺度学习耦合在一起，这使得模型容易受到异常值和重尾噪声的影响。

Method: 提出CAIRO框架，将回归解耦为两个阶段：第一阶段通过最小化尺度不变的排序损失学习评分函数；第二阶段通过保序回归恢复目标尺度。理论分析了"最优排序"目标类别，证明其在温和假设下能恢复真实条件均值的排序。

Result: CAIRO将神经网络的表示学习与基于排序统计的鲁棒性相结合，在表格基准测试中与最先进的树集成方法性能相当，在重尾或异方差噪声情况下显著优于标准回归目标。

Conclusion: 通过解耦排序学习和尺度学习，CAIRO提供了一种更鲁棒的回归方法，特别适用于存在异常值、重尾噪声或异方差性的场景。

Abstract: Standard regression methods typically optimize a single pointwise objective, such as mean squared error, which conflates the learning of ordering with the learning of scale. This coupling renders models vulnerable to outliers and heavy-tailed noise. We propose CAIRO (Calibrate After Initial Rank Ordering), a framework that decouples regression into two distinct stages. In the first stage, we learn a scoring function by minimizing a scale-invariant ranking loss; in the second, we recover the target scale via isotonic regression. We theoretically characterize a class of "Optimal-in-Rank-Order" objectives -- including variants of RankNet and Gini covariance -- and prove that they recover the ordering of the true conditional mean under mild assumptions. We further show that subsequent monotone calibration guarantees recovery of the true regression function. Empirically, CAIRO combines the representation learning of neural networks with the robustness of rank-based statistics. It matches the performance of state-of-the-art tree ensembles on tabular benchmarks and significantly outperforms standard regression objectives in regimes with heavy-tailed or heteroskedastic noise.

</details>


### [18] [The empirical distribution of sequential LS factors in Multi-level Dynamic Factor Models](https://arxiv.org/abs/2602.14813)
*Gian Pietro Bellocca,Ignacio Garrón,Vladimir Rodríguez-Caballero,Esther Ruiz*

Main category: stat.ME

TL;DR: 本文验证了Bai(2003)推导的PC因子渐近分布能否近似ML-DFM中SLS估计量的经验分布，并通过蒙特卡洛实验证实了这种近似效果良好，同时分析了不同MSE估计器的性能。


<details>
  <summary>Details</summary>
Motivation: 在多层次动态因子模型(ML-DFMs)中，需要评估序列最小二乘(SLS)估计器对全局和组别特定因子的估计性能。Bai(2003)推导了动态因子模型中主成分(PC)因子的渐近分布，但该分布是否适用于ML-DFMs中SLS估计量的经验分布近似尚不明确。

Method: 采用蒙特卡洛实验方法，在不同形式的异质性协方差矩阵条件下，比较SLS估计量的有限样本分布与PC因子渐近分布的近似程度。同时分析了几种渐近均方误差(MSE)估计器的性能，包括考虑异质性横截面相关性和因子载荷估计不确定性的估计器。

Result: 蒙特卡洛实验证实，在一般形式的异质性协方差矩阵下，SLS全局和组别特定因子的有限样本分布能够很好地用PC因子的渐近分布来近似。在MSE估计器比较中，允许异质性横截面相关性并考虑因子载荷估计不确定性的MSE估计器表现最佳。

Conclusion: Bai(2003)的PC因子渐近分布能够有效近似ML-DFMs中SLS估计量的经验分布，这为实际应用提供了理论支持。同时，推荐使用考虑异质性相关性和因子载荷估计不确定性的MSE估计器来评估估计精度。

Abstract: The research question we answer in this paper is whether the asymptotic distribution derived by Bai (2003) for Principal Components (PC) factors in dynamic factor models (DFMs) can approximate the empirical distribution of the sequential Least Squares (SLS) estimator of global and group-specific factors in multi-level dynamic factor models (ML-DFMs). Monte Carlo experiments confirm that under general forms of the idiosyncratic covariance matrix, the finite-sample distribution of SLS global and group-specific factors can be well approximated using the asymptotic distribution of PC factors. We also analyse the performance of alternative estimators of the asymptotic mean squared error (MSE) of the SLS factors and show that the MSE estimator that allows for idiosyncratic cross-sectional correlation and accounts for estimation uncertainty of factor loadings is best.

</details>


### [19] [Balanced Stochastic Block Model for Community Detection in Signed Networks](https://arxiv.org/abs/2602.14942)
*Yichao Chen,Weijing Tang,Ji Zhu*

Main category: stat.ME

TL;DR: 提出平衡随机块模型(BSBM)，将结构平衡理论融入网络生成过程，为有符号网络社区检测提供新方法


<details>
  <summary>Details</summary>
Motivation: 有符号网络的社区检测研究不足，传统方法只利用边连接模式，忽略了边符号信息和结构平衡理论的重要性

Method: 提出平衡随机块模型(BSBM)，将结构平衡理论融入网络生成过程，使平衡三角形更可能出现；开发快速轮廓伪似然估计算法，具有可证明的收敛性

Result: 估计器在比仅依赖边连接的二元SBM方法更弱的信号条件下实现强一致性；模拟研究和两个真实有符号网络展示强大经验性能

Conclusion: BSBM成功将有符号网络的结构平衡理论融入社区检测，提供理论保证和实证优势，为有符号网络分析开辟新方向

Abstract: Community detection, discovering the underlying communities within a network from observed connections, is a fundamental problem in network analysis, yet it remains underexplored for signed networks. In signed networks, both edge connection patterns and edge signs are informative, and structural balance theory (e.g., triangles aligned with ``the enemy of my enemy is my friend'' and ``the friend of my friend is my friend'' are more prevalent) provides a global higher-order principle that guides community formation. We propose a Balanced Stochastic Block Model (BSBM), which incorporates balance theory into the network generating process such that balanced triangles are more likely to occur. We develop a fast profile pseudo-likelihood estimation algorithm with provable convergence and establish that our estimator achieves strong consistency under weaker signal conditions than methods for the binary SBM that rely solely on edge connectivity. Extensive simulation studies and two real-world signed networks demonstrate strong empirical performance.

</details>


### [20] [Joint analysis for multivariate longitudinal and event time data with a change point anchored at interval-censored event time](https://arxiv.org/abs/2602.14991)
*Yue Zhan,Cheng Zheng,Ying Zhang*

Main category: stat.ME

TL;DR: 本文提出了一种针对亨廷顿病(HD)的联合模型，用于分析多变量纵向生物标志物与区间删失事件时间的关系，特别关注认知障碍和运动功能障碍在疾病进展中的相互作用。


<details>
  <summary>Details</summary>
Motivation: 亨廷顿病是一种常染色体显性神经退行性疾病，其发病时间受区间删失影响，临床数据通常在离散时间点收集。现有方法难以同时评估纵向生物标志物对事件时间的影响以及事件发生后生物标志物轨迹的变化。

Method: 开发了一个联合模型，用于多变量纵向生物标志物，以区间删失事件时间为锚点的变化点。该模型同时评估纵向生物标志物对事件时间的影响以及事件发生后生物标志物轨迹的变化。

Result: 通过全面的模拟研究验证了所提方法在有限样本下的因果推断性能，并将其应用于PREDICT-HD研究（一项多中心观察性队列研究），以确定认知障碍和运动功能障碍在疾病进展中的相互作用。

Conclusion: 提出的联合模型能够有效处理区间删失数据，为理解亨廷顿病中认知和运动功能障碍的相互作用提供了统计工具，有助于揭示疾病进展机制。

Abstract: Huntington's disease (HD) is an autosomal dominant neurodegenerative disorder characterized by motor dysfunction, psychiatric disturbances, and cognitive decline. The onset of HD is marked by severe motor impairment, which may be predicted by prior cognitive decline and, in turn, exacerbate cognitive deficits. Clinical data, however, are often collected at discrete time points, so the timing of disease onset is subject to interval censoring. To address the challenges posed by such data, we develop a joint model for multivariate longitudinal biomarkers with a change point anchored at an interval-censored event time. The model simultaneously assesses the effects of longitudinal biomarkers on the event time and the changes in biomarker trajectories following the event. We conduct a comprehensive simulation study to demonstrate the finite-sample performance of the proposed method for causal inference. Finally, we apply the method to PREDICT-HD, a multisite observational cohort study of prodromal HD individuals, to ascertain how cognitive impairment and motor dysfunction interact during disease progression.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [21] [MPL-HMC: A Tunable Parameterized Leapfrog Framework for Robust Hamiltonian Monte Carlo](https://arxiv.org/abs/2602.14061)
*Sourabh Bhattacharya*

Main category: stat.CO

TL;DR: MPL-HMC是一种改进的哈密顿蒙特卡洛方法，通过可调参数α和β控制哈密顿动力学扰动，在保持计算效率的同时显著提升采样性能，特别适用于复杂分布。


<details>
  <summary>Details</summary>
Motivation: 标准HMC方法在处理复杂分布（如多峰分布、高维问题）时存在局限性，需要更灵活的动力学扰动机制来改善采样效率和探索能力。

Method: 提出MPL-HMC方法，引入可调积分参数α(δt)和β(δt)来扰动哈密顿动力学，包括阻尼变体、反阻尼变体和用于多峰分布的激进变体，同时保持近似细致平衡。

Result: 阻尼变体在Neal's funnel上有效样本量提升14倍，药代动力学模型效率提升27%；反阻尼变体在贝叶斯神经网络中R-hat从1.981改善到1.026；激进变体能完全探索标准方法失败的多峰分布。

Conclusion: MPL-HMC在保持标准HMC计算效率的同时，通过系统控制阻尼、探索、稳定性和精度，显著扩展了HMC在挑战性领域的适用性，提供了完整的数学基础、实现规范和参数调优策略。

Abstract: This article introduces the Modified Parameterized Leapfrog Hamiltonian Monte Carlo (MPL-HMC) method, a novel extension of HMC addressing key limitations through tunable integration parameters $α(δt)$ and $β(δt)$, enabling controlled perturbations to Hamiltonian dynamics. Theoretical analysis demonstrates MPL-HMC maintains approximate detailed balance. Extensive empirical evaluation reveals systematic performance improvements. The damping variant ($α_2=-0.1$, $β_2=-0.05$) achieves a 14-fold increase in effective sample size for Neal's funnel and 27\% better efficiency for pharmacokinetic models. The anti-damping variant ($α_2=0.1$, $β_2=0.05$) achieves $\hat{R}=1.026$ for Bayesian neural networks versus $\hat{R}=1.981$ for standard HMC. We introduce aggressive MPL-HMC for multimodal distributions, employing extreme parameters ($α_2=8.0$--$15.0$, $β_2=5.0$--$8.0$) with enhanced sampling to achieve full mode exploration where standard methods fail. All variants maintain computational efficiency identical to standard HMC while providing systematic control over damping, exploration, stability, and accuracy. The article provides rigorous mathematical foundations, implementation specifications, parameter tuning strategies, and comprehensive performance comparisons, extending HMC's applicability to previously challenging domains.

</details>


### [22] [Fast Compute for ML Optimization](https://arxiv.org/abs/2602.14280)
*Nick Polson,Vadim Sokolov*

Main category: stat.CO

TL;DR: 提出Scale Mixture EM算法，通过方差-均值尺度混合表示自动确定学习率，无需手动调参，在逻辑回归基准测试中比Adam表现更好


<details>
  <summary>Details</summary>
Motivation: 现有优化算法如Adam需要手动调整学习率和动量参数，这增加了使用复杂性和计算成本。作者希望开发一种能够自动确定这些超参数的算法，减少用户干预

Method: 基于方差-均值尺度混合表示，推导出Scale Mixture EM算法。每个EM迭代都是加权最小二乘更新，其中隐变量决定观测和参数权重，这些权重起到类似Adam二阶矩缩放和AdamW权重衰减的作用，但直接从模型推导而来

Result: 在合成病态逻辑回归基准测试中，SM-EM结合Nesterov加速比网格搜索调优的Adam达到13倍更低的最终损失。对于40点正则化路径，跨惩罚值共享充分统计量使运行时间减少10倍

Conclusion: SM-EM算法消除了用户指定的学习率和动量调度需求，通过模型推导的权重实现自适应优化。基础算法具有EM单调性保证，添加Nesterov外推则用单调性保证换取更快的经验收敛

Abstract: We study optimization for losses that admit a variance-mean scale-mixture representation. Under this representation, each EM iteration is a weighted least squares update in which latent variables determine observation and parameter weights; these play roles analogous to Adam's second-moment scaling and AdamW's weight decay, but are derived from the model. The resulting Scale Mixture EM (SM-EM) algorithm removes user-specified learning-rate and momentum schedules. On synthetic ill-conditioned logistic regression benchmarks with $p \in \{20, \ldots, 500\}$, SM-EM with Nesterov acceleration attains up to $13\times$ lower final loss than Adam tuned by learning-rate grid search. For a 40-point regularization path, sharing sufficient statistics across penalty values yields a $10\times$ runtime reduction relative to the same tuned-Adam protocol. For the base (non-accelerated) algorithm, EM monotonicity guarantees nonincreasing objective values; adding Nesterov extrapolation trades this guarantee for faster empirical convergence.

</details>


### [23] [Higher-Order Hit-&-Run Samplers for Linearly Constrained Densities](https://arxiv.org/abs/2602.14616)
*Richard D. Paul,Anton Stratmann,Johann F. Jadebeck,Martin Beyß,Hanno Scharr,David Rügamer,Katharina Nöh*

Main category: stat.CO

TL;DR: 提出一种结合高阶信息与Hit-&-Run提议的约束采样算法，用于线性约束域上的MCMC采样


<details>
  <summary>Details</summary>
Motivation: 线性约束域上的MCMC采样在自然科学的贝叶斯逆问题中很重要，但现有方法对复杂约束密度处理不足，梯度信息在约束情况下可能无效

Method: 结合目标对数密度的梯度和曲率等高阶信息与Hit-&-Run提议机制，确保生成满足线性约束的可行提议

Result: 在复杂约束密度上相比各种约束和无约束采样器表现出改进的采样效率

Conclusion: 提出的算法有效解决了线性约束域上的MCMC采样问题，通过结合高阶信息和可行提议机制提升了采样性能

Abstract: Markov chain Monte Carlo (MCMC) sampling of densities restricted to linearly constrained domains is an important task arising in Bayesian treatment of inverse problems in the natural sciences. While efficient algorithms for uniform polytope sampling exist, much less work has dealt with more complex constrained densities. In particular, gradient information as used in unconstrained MCMC is not necessarily helpful in the constrained case, where the gradient may push the proposal's density out of the polytope. In this work, we propose a novel constrained sampling algorithm, which combines strengths of higher-order information, like the target's log-density's gradients and curvature, with the Hit-&-Run proposal, a simple mechanism which guarantees the generation of feasible proposals, fulfilling the linear constraints. Our extensive experiments demonstrate improved sampling efficiency on complex constrained densities over various constrained and unconstrained samplers.

</details>


### [24] [Weak Poincaré inequalities for Deterministic-scan Metropolis-within-Gibbs samplers](https://arxiv.org/abs/2602.14692)
*Mengxi Gao,Gareth O. Roberts,Andi Q. Wang*

Main category: stat.CO

TL;DR: 使用弱Poincaré不等式框架分析确定性扫描Metropolis-within-Gibbs采样器的收敛性，适用于不可逆马尔可夫链，通过狄利克雷形式的比较技术获得显式（次几何）收敛界。


<details>
  <summary>Details</summary>
Motivation: 分析确定性扫描Metropolis-within-Gibbs采样器（一类重要的MCMC算法）的收敛特性，特别是针对非可逆马尔可夫链，建立显式的收敛界。

Method: 采用弱Poincaré不等式框架，通过基于狄利克雷形式的新颖比较技术，分析联合链与边际链的收敛行为关系，并建立离散时间马尔可夫链弱Poincaré不等式的基本结果。

Result: 证明联合链继承了边际链的收敛行为，反之亦然；建立了离散时间马尔可夫链弱Poincaré不等式的张量化性质等基本结果；应用于层次回归模型和离散观测数据下扩散模型的贝叶斯推断算法。

Conclusion: 通过弱Poincaré不等式框架成功分析了Metropolis-within-Gibbs采样器的收敛性，为不可逆马尔可夫链提供了显式收敛界，理论结果在贝叶斯推断应用中得到了验证。

Abstract: Using the framework of weak Poincaré inequalities, we analyze the convergence properties of deterministic-scan Metropolis-within-Gibbs samplers, an important class of Markov chain Monte Carlo algorithms. Our analysis applies to nonreversible Markov chains and yields explicit (subgeometric) convergence bounds through novel comparison techniques based on Dirichlet forms. We show that the joint chain inherits the convergence behavior of the marginal chain and conversely. In addition, we establish several fundamental results for weak Poincaré inequalities for discrete-time Markov chains, such as a tensorization property for independent chains. We apply our theoretical results through applications to algorithms for Bayesian inference for a hierarchical regression model and a diffusion model under discretely-observed data.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [25] [Nonparametric Distribution Regression Re-calibration](https://arxiv.org/abs/2602.13362)
*Ádám Jung,Domokos M. Kelen,András A. Benczúr*

Main category: stat.ML

TL;DR: 提出基于条件核均值嵌入的非参数重新校准算法，解决回归预测中校准不足的问题，无需限制性建模假设，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 概率回归中，最小化预测误差常导致模型优先考虑信息性而非校准性，产生过于自信的窄预测区间。在安全关键应用中，可信的不确定性估计比窄区间更有价值。现有后处理方法要么依赖弱校准概念（如PIT均匀性），要么对误差性质施加限制性参数假设。

Method: 提出基于条件核均值嵌入的非参数重新校准算法，能够在不施加限制性建模假设的情况下纠正校准误差。针对实值目标的高效推断，引入了一种新的特征核，可在O(n log n)时间内评估大小为n的经验分布。

Result: 该方法在多样化的回归基准测试和模型类别中，始终优于先前的重新校准方法。

Conclusion: 提出的非参数重新校准算法能够有效解决概率回归中的校准问题，提供更准确的不确定性估计，在安全关键应用中具有重要价值。

Abstract: A key challenge in probabilistic regression is ensuring that predictive distributions accurately reflect true empirical uncertainty. Minimizing overall prediction error often encourages models to prioritize informativeness over calibration, producing narrow but overconfident predictions. However, in safety-critical settings, trustworthy uncertainty estimates are often more valuable than narrow intervals. Realizing the problem, several recent works have focused on post-hoc corrections; however, existing methods either rely on weak notions of calibration (such as PIT uniformity) or impose restrictive parametric assumptions on the nature of the error. To address these limitations, we propose a novel nonparametric re-calibration algorithm based on conditional kernel mean embeddings, capable of correcting calibration error without restrictive modeling assumptions. For efficient inference with real-valued targets, we introduce a novel characteristic kernel over distributions that can be evaluated in $\mathcal{O}(n \log n)$ time for empirical distributions of size $n$. We demonstrate that our method consistently outperforms prior re-calibration approaches across a diverse set of regression benchmarks and model classes.

</details>


### [26] [Metabolic cost of information processing in Poisson variational autoencoders](https://arxiv.org/abs/2602.13421)
*Hadi Vafaii,Jacob L. Yates*

Main category: stat.ML

TL;DR: Poisson变分自编码器通过KL散度项引入代谢成本，将编码率与放电率耦合，实现编码保真度与能量消耗的权衡，为资源受限计算理论提供基础。


<details>
  <summary>Details</summary>
Motivation: 生物系统的计算受能量约束，但传统计算理论将能量视为免费资源。需要建立能量感知的计算理论框架。

Method: 提出Poisson变分自编码器（P-VAE），在Poisson假设下进行变分自由能最小化。KL散度项与模型神经元的先验放电率成正比，产生代谢成本项惩罚高基线活动。

Result: 增加KL权重系数β会单调增加P-VAE的稀疏性并降低平均放电活动，而Grelu-VAE表示保持不变。证明代谢成本结构是Poisson公式特有的，而非非负表示的副产品。

Conclusion: Poisson变分推断为资源受限的计算理论提供了有前景的基础，将抽象信息量（编码率）与具体生物物理变量（放电率）耦合，实现编码保真度与能量消耗的权衡。

Abstract: Computation in biological systems is fundamentally energy-constrained, yet standard theories of computation treat energy as freely available. Here, we argue that variational free energy minimization under a Poisson assumption offers a principled path toward an energy-aware theory of computation. Our key observation is that the Kullback-Leibler (KL) divergence term in the Poisson free energy objective becomes proportional to the prior firing rates of model neurons, yielding an emergent metabolic cost term that penalizes high baseline activity. This structure couples an abstract information-theoretic quantity -- the *coding rate* -- to a concrete biophysical variable -- the *firing rate* -- which enables a trade-off between coding fidelity and energy expenditure. Such a coupling arises naturally in the Poisson variational autoencoder (P-VAE) -- a brain-inspired generative model that encodes inputs as discrete spike counts and recovers a spiking form of *sparse coding* as a special case -- but is absent from standard Gaussian VAEs. To demonstrate that this metabolic cost structure is unique to the Poisson formulation, we compare the P-VAE against Grelu-VAE, a Gaussian VAE with ReLU rectification applied to latent samples, which controls for the non-negativity constraint. Across a systematic sweep of the KL term weighting coefficient $β$ and latent dimensionality, we find that increasing $β$ monotonically increases sparsity and reduces average spiking activity in the P-VAE. In contrast, Grelu-VAE representations remain unchanged, confirming that the effect is specific to Poisson statistics rather than a byproduct of non-negative representations. These results establish Poisson variational inference as a promising foundation for a resource-constrained theory of computation.

</details>


### [27] [Locally Private Parametric Methods for Change-Point Detection](https://arxiv.org/abs/2602.13619)
*Anuj Kumar Yadav,Cemre Cadir,Yanina Shkel,Michael Gastpar*

Main category: stat.ML

TL;DR: 本文研究局部差分隐私下的参数化变点检测问题，推导了非隐私和隐私设置下的理论性能界限，并建立了强数据处理不等式系数的结构结果。


<details>
  <summary>Details</summary>
Motivation: 研究在局部差分隐私约束下如何识别时间序列中的分布变化，分析隐私保护对变点检测性能的影响，并建立相关的理论工具。

Method: 在非隐私设置中使用基于广义对数似然比检验的变点检测算法，通过鞅方法改进有限样本精度保证；在隐私设置中提出基于随机响应和二进制机制的两种局部差分隐私算法。

Result: 推导了检测精度界限并通过实证评估验证结果，量化了局部差分隐私在变点检测中的统计代价，证明了隐私会降低相对于非隐私基准的性能。

Conclusion: 本文建立了局部差分隐私变点检测的理论框架，证明了强数据处理不等式系数由二元输入分布实现，这些结果对统计估计、数据压缩和马尔可夫链混合等应用具有独立价值。

Abstract: We study parametric change-point detection, where the goal is to identify distributional changes in time series, under local differential privacy. In the non-private setting, we derive improved finite-sample accuracy guarantees for a change-point detection algorithm based on the generalized log-likelihood ratio test, via martingale methods. In the private setting, we propose two locally differentially private algorithms based on randomized response and binary mechanisms, and analyze their theoretical performance. We derive bounds on detection accuracy and validate our results through empirical evaluation. Our results characterize the statistical cost of local differential privacy in change-point detection and show how privacy degrades performance relative to a non-private benchmark. As part of this analysis, we establish a structural result for strong data processing inequalities (SDPI), proving that SDPI coefficients for Rényi divergences and their symmetric variants (Jeffreys-Rényi divergences) are achieved by binary input distributions. These results on SDPI coefficients are also of independent interest, with applications to statistical estimation, data compression, and Markov chain mixing.

</details>


### [28] [Quantifying Normality: Convergence Rate to Gaussian Limit for Stochastic Approximation and Unadjusted OU Algorithm](https://arxiv.org/abs/2602.13906)
*Shaan Ul Haque,Zedong Wang,Zixuan Zhang,Siva Theja Maguluri*

Main category: stat.ML

TL;DR: 本文为随机逼近算法建立了非渐近的Wasserstein距离界限，量化了有限时间内高斯近似的精度，并获得了迭代误差的尾部界限。


<details>
  <summary>Details</summary>
Motivation: 随机逼近算法有丰富的渐近正态性文献，但这些渐近结果无法量化有限时间内高斯近似的精度。需要建立明确的非渐近界限来评估实际应用中的性能。

Method: 首先研究由一般噪声驱动的离散Ornstein-Uhlenbeck过程的收敛速率，其平稳分布与缩放SA迭代的极限高斯分布相同。然后通过将Stein方法适应于处理矩阵加权i.i.d.随机变量和，分析缩放SA迭代与离散时间O-U过程之间的误差动态，结合后者的收敛速率获得SA的有限时间界限。

Result: 建立了缩放迭代在时间k的分布与渐近高斯极限之间Wasserstein距离的显式非渐近界限，适用于常数和多项式衰减步长。作为直接推论，获得了任何时间SA迭代误差的尾部界限。

Conclusion: 本文为随机逼近算法提供了有限时间性能的精确量化，通过分析离散O-U过程的收敛速率并适应Stein方法，获得了高斯近似的锐利速率，这对采样文献也有独立意义。

Abstract: Stochastic approximation (SA) is a method for finding the root of an operator perturbed by noise. There is a rich literature establishing the asymptotic normality of rescaled SA iterates under fairly mild conditions. However, these asymptotic results do not quantify the accuracy of the Gaussian approximation in finite time. In this paper, we establish explicit non-asymptotic bounds on the Wasserstein distance between the distribution of the rescaled iterate at time k and the asymptotic Gaussian limit for various choices of step-sizes including constant and polynomially decaying. As an immediate consequence, we obtain tail bounds on the error of SA iterates at any time.
  We obtain the sharp rates by first studying the convergence rate of the discrete Ornstein-Uhlenbeck (O-U) process driven by general noise, whose stationary distribution is identical to the limiting Gaussian distribution of the rescaled SA iterates. We believe that this is of independent interest, given its connection to sampling literature. The analysis involves adapting Stein's method for Gaussian approximation to handle the matrix weighted sum of i.i.d. random variables. The desired finite-time bounds for SA are obtained by characterizing the error dynamics between the rescaled SA iterate and the discrete time O-U process and combining it with the convergence rate of the latter process.

</details>


### [29] [A Theoretical Framework for LLM Fine-tuning Using Early Stopping for Non-random Initialization](https://arxiv.org/abs/2602.13942)
*Zexuan Sun,Garvesh Raskutti*

Main category: stat.ML

TL;DR: 本文提出一个结合早期停止理论和注意力机制神经正切核的统计框架，解释LLM微调中为何只需少量epochs就能获得良好性能，并证明收敛速度与NTK核矩阵特征值衰减率相关。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，微调预训练模型已成为普遍做法，但其理论基础仍不明确。核心问题是：为什么通常只需几个epoch的微调就能在不同任务上获得强大性能？本文旨在通过建立统计框架来回答这个问题。

Method: 开发了一个结合严格早期停止理论和注意力机制神经正切核（NTK）的统计框架。将经典NTK理论扩展到非随机（预训练）初始化，为基于注意力的微调提供收敛保证。框架可用于解释LLMs中多任务的任务向量。

Result: 理论分析表明，收敛速度与样本量密切相关，且与NTK诱导的经验核矩阵的特征值衰减率紧密相关。在真实数据集上的现代语言模型实验为理论见解提供了实证支持。

Conclusion: 本文提出的统计框架为LLM微调实践提供了新的理论见解，解释了为什么少量epochs的微调就足够有效，并为理解任务向量和多任务学习提供了理论基础。

Abstract: In the era of large language models (LLMs), fine-tuning pretrained models has become ubiquitous. Yet the theoretical underpinning remains an open question. A central question is why only a few epochs of fine-tuning are typically sufficient to achieve strong performance on many different tasks. In this work, we approach this question by developing a statistical framework, combining rigorous early stopping theory with the attention-based Neural Tangent Kernel (NTK) for LLMs, offering new theoretical insights on fine-tuning practices. Specifically, we formally extend classical NTK theory [Jacot et al., 2018] to non-random (i.e., pretrained) initializations and provide a convergence guarantee for attention-based fine-tuning. One key insight provided by the theory is that the convergence rate with respect to sample size is closely linked to the eigenvalue decay rate of the empirical kernel matrix induced by the NTK. We also demonstrate how the framework can be used to explain task vectors for multiple tasks in LLMs. Finally, experiments with modern language models on real-world datasets provide empirical evidence supporting our theoretical insights.

</details>


### [30] [Computable Bernstein Certificates for Cross-Fitted Clipped Covariance Estimation](https://arxiv.org/abs/2602.14020)
*Even He,Zaizai Yan*

Main category: stat.ML

TL;DR: 提出一种自适应截断协方差估计方法，通过交叉拟合和可计算的Bernstein型偏差证书实现数据驱动的截断水平选择，在重尾和离群值污染下保持稳健性能。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得范数截断方法需要未知的截断水平，其精度严重依赖该参数的选择。在重尾样本和离群值污染下，需要一种能自适应选择截断水平并提供可计算偏差保证的稳健协方差估计方法。

Method: 提出交叉拟合截断协方差估计器，配备完全可计算的Bernstein型偏差证书。通过MinUpper选择器平衡认证的随机误差和截断偏差的稳健留出代理，实现数据驱动的截断水平自适应选择。

Result: 该方法在温和尾部正则性条件下适应内在复杂性度量（如有效秩），在仅有限四阶矩条件下保持有意义的保证。在污染的尖峰协方差基准测试中表现出稳定性能和竞争性精度。

Conclusion: 提出的自适应截断协方差估计方法通过可计算的偏差证书和MinUpper选择器，解决了传统截断方法依赖未知截断水平的问题，在重尾和离群值污染下实现了稳健且自适应的协方差估计。

Abstract: We study operator-norm covariance estimation from heavy-tailed samples that may include a small fraction of arbitrary outliers. A simple and widely used safeguard is \emph{Euclidean norm clipping}, but its accuracy depends critically on an unknown clipping level. We propose a cross-fitted clipped covariance estimator equipped with \emph{fully computable} Bernstein-type deviation certificates, enabling principled data-driven tuning via a selector (\emph{MinUpper}) that balances certified stochastic error and a robust hold-out proxy for clipping bias. The resulting procedure adapts to intrinsic complexity measures such as effective rank under mild tail regularity and retains meaningful guarantees under only finite fourth moments. Experiments on contaminated spiked-covariance benchmarks illustrate stable performance and competitive accuracy across regimes.

</details>


### [31] [Why Self-Training Helps and Hurts: Denoising vs. Signal Forgetting](https://arxiv.org/abs/2602.14029)
*Mingqi Wu,Archer Y. Yang,Qiang Sun*

Main category: stat.ML

TL;DR: 研究过参数化线性回归中的迭代自训练（自蒸馏），分析其在高维情况下的预测风险动态，发现U形风险曲线和最优早停时间，提出迭代广义交叉验证准则用于数据驱动的早停选择。


<details>
  <summary>Details</summary>
Motivation: 研究迭代自训练（自蒸馏）在过参数化线性回归中的行为，理解其在高维情况下的动态特性，特别是预测风险随迭代的变化规律，以及如何确定最优停止时间。

Method: 在过参数化线性回归框架下，初始估计器在带噪声标签上训练，后续迭代在新鲜协变量上使用前一模型的无噪声伪标签训练。推导高维情况下的确定性等价递归，分析预测风险和有效噪声的动态变化。

Result: 发现预测风险呈现U形曲线，存在最优早停时间。递归分析揭示两个竞争力量：系统性遗忘导致风险增加，去噪效应导致风险减少。在尖峰协方差模型中，迭代起到迭代相关的谱滤波作用，实现隐式软特征选择。

Conclusion: 提出迭代广义交叉验证准则，证明其在整个自训练轨迹上一致估计风险，实现完全数据驱动的停止时间和正则化选择。实验验证了理论预测的去噪-遗忘权衡。

Abstract: Iterative self-training (self-distillation) repeatedly refits a model on pseudo-labels generated by its own predictions. We study this procedure in overparameterized linear regression: an initial estimator is trained on noisy labels, and each subsequent iterate is trained on fresh covariates with noiseless pseudo-labels from the previous model. In the high-dimensional regime, we derive deterministic-equivalent recursions for the prediction risk and effective noise across iterations, and prove that the empirical quantities concentrate sharply around these limits. The recursion separates two competing forces: a systematic component that grows with iteration due to progressive signal forgetting, and a stochastic component that decays due to denoising via repeated data-dependent projections. Their interaction yields a $U$-shaped test-risk curve and an optimal early-stopping time. In spiked covariance models, iteration further acts as an iteration-dependent spectral filter that preserves strong eigendirections while suppressing weaker ones, inducing an implicit form of soft feature selection distinct from ridge regression. Finally, we propose an iterated generalized cross-validation criterion and prove its uniform consistency for estimating the risk along the self-training trajectory, enabling fully data-driven selection of the stopping time and regularization. Experiments on synthetic covariances validate the theory and illustrate the predicted denoising-forgetting trade-off.

</details>


### [32] [Federated Ensemble Learning with Progressive Model Personalization](https://arxiv.org/abs/2602.14244)
*Ala Emrani,Amir Najafi,Abolfazl Motahari*

Main category: stat.ML

TL;DR: 提出一种基于Boosting的个性化联邦学习框架，通过渐进增加个性化组件深度并控制复杂度，平衡共享特征提取与个性化建模的权衡，在异构数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个性化联邦学习(PFL)通过共享特征提取器和客户端特定头部来应对统计异构性，但存在根本性权衡：深度共享组件阻碍个性化，而大型本地头部在有限数据下容易过拟合。现有方法采用刚性浅层头部，无法在原则性方式下平衡这一权衡。

Method: 提出Boosting启发的框架，为每个客户端构建T个模型的集成。在Boosting迭代中，渐进增加个性化组件的深度，同时通过低秩分解或宽度缩减系统控制其有效复杂度。这种方法同时限制过拟合并通过允许更具表达力的个性化来显著减少每个客户端的偏差。

Result: 理论分析建立了泛化边界，对平均本地样本大小和客户端总数具有有利的依赖性。实验在基准和真实数据集（EMNIST、CIFAR-10/100、Sent140）上显示，该框架在异构数据分布下始终优于最先进的PFL方法。

Conclusion: 提出的Boosting框架通过渐进增加个性化深度并控制复杂度，有效平衡了共享特征提取与个性化建模的权衡，在理论保证和实验性能上都优于现有PFL方法。

Abstract: Federated Learning provides a privacy-preserving paradigm for distributed learning, but suffers from statistical heterogeneity across clients. Personalized Federated Learning (PFL) mitigates this issue by considering client-specific models. A widely adopted approach in PFL decomposes neural networks into a shared feature extractor and client-specific heads. While effective, this design induces a fundamental tradeoff: deep or expressive shared components hinder personalization, whereas large local heads exacerbate overfitting under limited per-client data. Most existing methods rely on rigid, shallow heads, and therefore fail to navigate this tradeoff in a principled manner. In this work, we propose a boosting-inspired framework that enables a smooth control of this tradeoff. Instead of training a single personalized model, we construct an ensemble of $T$ models for each client. Across boosting iterations, the depth of the personalized component are progressively increased, while its effective complexity is systematically controlled via low-rank factorization or width shrinkage. This design simultaneously limits overfitting and substantially reduces per-client bias by allowing increasingly expressive personalization. We provide theoretical analysis that establishes generalization bounds with favorable dependence on the average local sample size and the total number of clients. Specifically, we prove that the complexity of the shared layers is effectively suppressed, while the dependence on the boosting horizon $T$ is controlled through parameter reduction. Notably, we provide a novel nonlinear generalization guarantee for decoupled PFL models. Extensive experiments on benchmark and real-world datasets (e.g., EMNIST, CIFAR-10/100, and Sent140) demonstrate that the proposed framework consistently outperforms state-of-the-art PFL methods under heterogeneous data distributions.

</details>


### [33] [Constrained and Composite Sampling via Proximal Sampler](https://arxiv.org/abs/2602.14478)
*Thanh Dang,Jiaming Liang*

Main category: stat.ML

TL;DR: 提出两种对数凹采样方法：约束采样和复合采样。约束采样通过提升变换将约束问题转化为提升空间中的均匀分布采样，使用近端采样器实现；复合采样通过双重提升变换将复合问题转化为约束采样问题。两种方法仅需分离预言机和次梯度预言机，无需投影或障碍函数。


<details>
  <summary>Details</summary>
Motivation: 现有约束采样方法通常依赖投影、反射、障碍函数或镜像映射，这些方法要么计算成本高，要么需要了解约束集的几何结构。本文旨在开发一种仅需最小预言机访问（分离预言机和次梯度预言机）的实用无偏采样器，无需了解约束集的几何结构。

Method: 1. 约束采样：使用epigraph变换将约束采样问题提升到ℝ^{d+1}空间，转化为近似均匀分布采样，然后使用基于切割平面法和拒绝采样的近端采样器实现。2. 复合采样：通过epigraph提升将复合采样问题转化为约束采样问题，形成双重提升到ℝ^{d+2}空间，应用近端采样器，并利用不同的预言机组合（次梯度和近端）构建提升问题的分离预言机。

Result: 开发了两种仅需分离预言机和次梯度预言机的采样算法：约束采样算法和复合采样算法。两种算法都是实用且无偏的，无需了解约束集的几何结构。建立了以Rényi和χ²散度衡量的混合时间界限。

Conclusion: 本文提出了一种新的约束采样和复合采样方法，仅需最小预言机访问即可实现无偏采样，避免了现有方法对投影、反射或障碍函数的依赖。该方法具有实用性和理论保证，为贝叶斯推断等应用提供了新的采样工具。

Abstract: We study two log-concave sampling problems: constrained sampling and composite sampling. First, we consider sampling from a target distribution with density proportional to $\exp(-f(x))$ supported on a convex set $K \subset \mathbb{R}^d$, where $f$ is convex. The main challenge is enforcing feasibility without degrading mixing. Using an epigraph transformation, we reduce this task to sampling from a nearly uniform distribution over a lifted convex set in $\mathbb{R}^{d+1}$. We then solve the lifted problem using a proximal sampler. Assuming only a separation oracle for $K$ and a subgradient oracle for $f$, we develop an implementation of the proximal sampler based on the cutting-plane method and rejection sampling. Unlike existing constrained samplers that rely on projection, reflection, barrier functions, or mirror maps, our approach enforces feasibility using only minimal oracle access, resulting in a practical and unbiased sampler without knowing the geometry of the constraint set.
  Second, we study composite sampling, where the target is proportional to $\exp(-f(x)-h(x))$ with closed and convex $f$ and $h$. This composite structure is standard in Bayesian inference with $f$ modeling data fidelity and $h$ encoding prior information. We reduce composite sampling via an epigraph lifting of $h$ to constrained sampling in $\mathbb{R}^{d+1}$, which allows direct application of the constrained sampling algorithm developed in the first part. This reduction results in a double epigraph lifting formulation in $\mathbb{R}^{d+2}$, on which we apply a proximal sampler. By keeping $f$ and $h$ separate, we further demonstrate how different combinations of oracle access (such as subgradient and proximal) can be leveraged to construct separation oracles for the lifted problem. For both sampling problems, we establish mixing time bounds measured in Rényi and $χ^2$ divergences.

</details>


### [34] [Accelerating Posterior Inference from Pulsar Light Curves via Learned Latent Representations and Local Simulator-Guided Optimization](https://arxiv.org/abs/2602.14520)
*Farhana Taiyebah,Abu Bucker Siddik,Indronil Bhattacharjee,Diane Oyen,Soumi De,Greg Olmschenk,Constantinos Kalapotharakos*

Main category: stat.ML

TL;DR: 提出结合学习潜在表示与局部模拟器引导优化的框架，加速脉冲星光变曲线后验推断，在保持精度的同时将推理时间从24小时减少到12分钟。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法虽然准确但计算成本高，需要加速脉冲星光变曲线的后验推断过程。

Method: 1) 使用掩码U-Net预训练，从部分观测重建完整光变曲线并产生信息丰富的潜在嵌入；2) 在学习的嵌入空间中识别相似模拟光变曲线，获得后验的初始经验近似；3) 使用爬山更新的局部优化程序，在正向模拟器引导下将经验后验逐步移向更高似然参数区域。

Result: 在PSR J0030+0451观测光变曲线上的实验表明，该方法与传统MCMC方法的后验估计结果非常接近，同时将推理时间从24小时减少到12分钟，实现了120倍的加速。

Conclusion: 学习表示与模拟器引导优化的结合为加速后验推断提供了有效方法，在保持精度的同时显著提高了计算效率。

Abstract: Posterior inference from pulsar observations in the form of light curves is commonly performed using Markov chain Monte Carlo methods, which are accurate but computationally expensive. We introduce a framework that accelerates posterior inference while maintaining accuracy by combining learned latent representations with local simulator-guided optimization. A masked U-Net is first pretrained to reconstruct complete light curves from partial observations and to produce informative latent embeddings. Given a query light curve, we identify similar simulated light curves from the simulation bank by measuring similarity in the learned embedding space produced by pretrained U-Net encoder, yielding an initial empirical approximation to the posterior over parameters. This initialization is then refined using a local optimization procedure using hill-climbing updates, guided by a forward simulator, progressively shifting the empirical posterior toward higher-likelihood parameter regions. Experiments on the observed light curve of PSR J0030+0451, captured by NASA's Neutron Star Interior Composition Explorer (NICER), show that our method closely matches posterior estimates obtained using traditional MCMC methods while achieving 120 times reduction in inference time (from 24 hours to 12 minutes), demonstrating the effectiveness of learned representations and simulator-guided optimization for accelerated posterior inference.

</details>


### [35] [GenPANIS: A Latent-Variable Generative Framework for Forward and Inverse PDE Problems in Multiphase Media](https://arxiv.org/abs/2602.14642)
*Matthaios Chatzopoulos,Phaedon-Stelios Koutsourelakis*

Main category: stat.ML

TL;DR: GenPANIS：一种统一的生成式框架，用于多相介质中的逆问题和逆设计，通过连续潜在嵌入保持精确离散微观结构，支持梯度推理，使用单个模型实现双向推断。


<details>
  <summary>Details</summary>
Motivation: 多相介质中的逆问题和逆设计需要处理离散值材料场，导致问题不可微分且与基于梯度的方法不兼容。现有方法要么放松为连续近似（损害物理保真度），要么使用单独的重型模型进行正向和逆向任务。

Method: 提出GenPANIS框架：学习微观结构和PDE解的联合分布，通过连续潜在嵌入保持精确离散微观结构。包含物理感知解码器（集成可微分粗粒度PDE求解器）和可学习归一化流先验，支持双向推断，可使用无标签数据、物理残差和少量标签对进行训练。

Result: 在Darcy流和Helmholtz方程上验证，在具有挑战性的外推场景（包括未见边界条件、体积分数和微观结构形态）中保持准确性，使用稀疏、噪声观测。性能优于最先进方法，参数减少10-100倍，并提供原则性不确定性量化。

Conclusion: GenPANIS提供了一种统一的生成式框架，能够在保持精确离散微观结构的同时实现梯度推理，解决了多相介质逆问题中的关键挑战，为物理感知的逆设计和不确定性量化提供了有效解决方案。

Abstract: Inverse problems and inverse design in multiphase media, i.e., recovering or engineering microstructures to achieve target macroscopic responses, require operating on discrete-valued material fields, rendering the problem non-differentiable and incompatible with gradient-based methods. Existing approaches either relax to continuous approximations, compromising physical fidelity, or employ separate heavyweight models for forward and inverse tasks. We propose GenPANIS, a unified generative framework that preserves exact discrete microstructures while enabling gradient-based inference through continuous latent embeddings. The model learns a joint distribution over microstructures and PDE solutions, supporting bidirectional inference (forward prediction and inverse recovery) within a single architecture. The generative formulation enables training with unlabeled data, physics residuals, and minimal labeled pairs. A physics-aware decoder incorporating a differentiable coarse-grained PDE solver preserves governing equation structure, enabling extrapolation to varying boundary conditions and microstructural statistics. A learnable normalizing flow prior captures complex posterior structure for inverse problems. Demonstrated on Darcy flow and Helmholtz equations, GenPANIS maintains accuracy on challenging extrapolative scenarios - including unseen boundary conditions, volume fractions, and microstructural morphologies, with sparse, noisy observations. It outperforms state-of-the-art methods while using 10 - 100 times fewer parameters and providing principled uncertainty quantification.

</details>


### [36] [The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling](https://arxiv.org/abs/2602.14862)
*Pierre-Alexandre Mattei,Bruno Loureiro*

Main category: stat.ML

TL;DR: 温度缩放是控制概率模型不确定性的简单方法，广泛应用于分类器校准和LLM随机性调节。本文首次系统分析了其理论性质：对分类任务，升温增加不确定性；对LLM，挑战了"升温增加多样性"的普遍观点；并提出两个新特征：几何上是最小信息投影，线性变换中是唯一不改变硬预测的缩放器。


<details>
  <summary>Details</summary>
Motivation: 温度缩放是控制概率模型不确定性最流行的方法，广泛应用于分类器校准和LLM随机性调节，但缺乏严格的理论分析。本文旨在填补这一空白，深入探究温度缩放的理论性质。

Method: 通过理论分析研究温度缩放的性质：1) 分析温度变化对分类模型不确定性的影响；2) 挑战LLM中温度与多样性的关系；3) 从几何角度证明温度缩放是最小信息投影；4) 在线性缩放框架中证明温度缩放是唯一不改变硬预测的缩放器。

Result: 1) 对分类任务，升高温度确实增加模型不确定性（特别是熵）；2) 对LLM，升温不一定增加多样性，挑战了普遍观点；3) 温度缩放可解释为原始模型到给定熵模型集合的信息投影；4) 在线性缩放器中，温度缩放是唯一保持硬预测不变的缩放方法。

Conclusion: 本文首次对温度缩放进行了系统的理论分析，澄清了其在分类和LLM中的不同作用，提出了两个新的理论特征，为理解这一广泛应用但理论未明的技术提供了坚实基础。

Abstract: Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model.

</details>


### [37] [Activation-Space Uncertainty Quantification for Pretrained Networks](https://arxiv.org/abs/2602.14934)
*Richard Bergna,Stefan Depeweg,Sergio Calvo-Ordoñez,Jonathan Plenk,Alvaro Cartea,Jose Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: GAPA是一种后处理方法，将贝叶斯建模从权重转移到激活空间，通过高斯过程激活函数在保持原始预测的同时提供闭式不确定性估计，无需重新训练或采样。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法通常需要重新训练、蒙特卡洛采样或昂贵的二阶计算，并且可能改变预训练模型的预测。需要一种能够保持冻结主干网络预测的同时提供可靠不确定性估计的高效方法。

Method: GAPA将标准非线性激活函数替换为高斯过程激活函数，其后验均值精确匹配原始激活，通过构造保持点预测不变。采用稀疏变分诱导点近似和局部k近邻子集条件化，实现确定性单次前向传播的不确定性量化。

Result: 在回归、分类、图像分割和语言建模任务中，GAPA在校准性和分布外检测方面匹配或优于强后处理基线方法，同时在测试时保持高效。

Conclusion: GAPA提供了一种高效的后处理不确定性量化方法，能够在保持预训练模型预测不变的同时提供可靠的不确定性估计，适用于现代深度学习架构。

Abstract: Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [38] [Zipf-Mandelbrot Scaling in Korean Court Music: Universal Patterns in Music](https://arxiv.org/abs/2602.14198)
*Byeongchan Choi,Junwon You,Myung Ock Kim,Jae-Hun Jung*

Main category: stat.AP

TL;DR: 韩国宫廷音乐也遵循Zipf-Mandelbrot定律，特别是音高-时长组合单元，表明该定律在音乐数据中具有跨文化普遍性。


<details>
  <summary>Details</summary>
Motivation: Zipf定律在自然语言中普遍存在，后来在西方音乐中也观察到类似规律，但其在非西方音乐中的适用性尚未得到充分研究。本研究旨在探究韩国传统宫廷音乐是否也遵循Zipf-Mandelbrot定律。

Method: 分析了43首跨越数个世纪的韩国宫廷音乐作品，这些作品使用传统韩国记谱法Jeongganbo记录，并转录为西方五线谱。提取音高、时长及其组合作为Zipfian单元，检验它们是否符合Zipf-Mandelbrot定律。

Result: 韩国音乐高度符合Zipf-Mandelbrot定律，特别是音高-时长组合单元。韩国音乐的集体演化过程平滑了个体差异，形成了广泛可理解的形式，这可能是导致该定律成立的重要因素。

Conclusion: 研究为Zipf-Mandelbrot定律在音乐数据中的跨文化普遍性提供了额外证据。进一步表明两个独立Zipfian数据集的联合分布也遵循该定律，深化了我们对标度律在组合和交互中行为的理解。

Abstract: Zipf's law, originally discovered in natural language and later generalized to the Zipf-Mandelbrot law, describes a power-law relationship between the frequency of a Zipfian element and its rank. Due to the semantic characteristics of this law, it has also been observed in musical data. However, most such studies have focused on Western music, and its applicability to non-Western music remains not well investigated. We analyzed 43 Korean court music pieces called Jeong-ak, spanning several centuries and written in the traditional Korean musical notation Jeongganbo. These pieces were transcribed into Western staff notation, and musical data such as pitch and duration were extracted. Using pitch, duration, and their paired combinations as Zipfian units, we found that Korean music also fits the Zipf-Mandelbrot law to a high degree, particularly for the paired pitch-duration unit. Korean music has evolved collectively over long periods, smoothing idiosyncratic variations and producing forms that are widely understandable among people. This collective evolution appears to have played a significant role in shaping the characteristics that lead to the satisfaction of Zipf-Mandelbrot law. Our findings provide additional evidence that Zipf-Mandelbrot scaling in musical data is universal across cultures. We further show that the joint distribution of two independent Zipfian data sets follows the Zipf-Mandelbrot law; in this sense, our result does not merely extend Zipf's law but deepens our understanding of how scaling laws behave under composition and interaction, offering a more unified perspective on rank-based statistical regularities.

</details>


### [39] [Evaluating the Impact of COVID-19 on Transportation Infrastructure Funding](https://arxiv.org/abs/2602.14203)
*Lu Gao,Pan Lu,Fengxiang Qiao,Joshua Qiang Li,Yunpeng Zhang,Yihao Ren*

Main category: stat.AP

TL;DR: 评估COVID-19大流行对美国交通基础设施资金的影响，通过分析燃油消耗数据并开发机器学习模型预测未来税收收入


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行导致商业和日常活动减少，燃油消耗下降，进而减少了作为交通基础设施维护主要资金来源的燃油税收入，需要评估这一影响

Method: 整合COVID-19情景、燃油消耗和人口统计数据开发机器学习模型，利用最佳模型预测各州未来燃油消耗

Result: 最佳模型R2分数超过95%，能捕捉疫情期间燃油消耗波动；预测显示某些州的燃油税收入将比疫情前水平低10%-15%，持续至少1-2年

Conclusion: COVID-19大流行对交通基础设施资金有显著负面影响，需要制定应对策略来缓解燃油税收入减少带来的挑战

Abstract: The coronavirus disease 2019 (COVID-19) pandemic has caused a reduction in business and routine activity and resulted in less motor fuel consumption. Thus, the gas tax revenue is reduced which is the major funding resource supporting the rehabilitation and maintenance of transportation infrastructure systems. The focus of this study is to evaluate the impact of the COVID-19 pandemic on transportation infrastructure funds in the United States through analyzing the motor fuel consumption data. Machine learning models were developed by integrating COVID-19 scenarios, fuel consumptions, and demographic data. The best model achieves an R2-score of more than 95% and captures the fluctuations of fuel consumption during the pandemic. Using the developed model, we project future motor gas consumption for each state. For some states, the gas tax revenues are going to be 10%-15% lower than the pre-pandemic level for at least one or two years.

</details>


### [40] [Same Prompt, Different Outcomes: Evaluating the Reproducibility of Data Analysis by LLMs](https://arxiv.org/abs/2602.14349)
*Jiaxin Cui,Rohan Alexander*

Main category: stat.AP

TL;DR: LLM数据分析的复现性评估显示，即使相同配置下，分析结果也存在显著变异，建议多次独立运行并考虑结果分布


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）进行数据分析的复现性，了解在相同任务、数据和设置下，LLM分析结果的稳定性

Method: 系统评估两种提示策略、六个模型、四种温度设置，每个配置执行十次独立运行，共480次尝试，评估完成度、一致性、有效性和稳定性

Result: 即使在相同配置下，分析结果也存在显著变异，类似于人类数据分析，LLM的分析结果也会因运行而异

Conclusion: 使用LLM进行数据分析时，应多次独立运行并考虑结果分布，以确保可靠性和稳定性

Abstract: We systematically evaluate the reproducibility of data analysis conducted by Large Language Models (LLMs). We evaluate two prompting strategies, six models, and four temperature settings, with ten independent executions per configuration, yielding 480 total attempts. We assess the completion, concordance, validity, and consistency of each attempt and find considerable variation in the analytical results even for consistent configurations. This suggests, as with human data analysis, the data analysis conducted by LLMs can vary, even given the same task, data, and settings. Our results mean that if an LLM is being used to conduct data analysis, then it should be run multiple times independently and the distribution of results considered.

</details>


### [41] [When to repeat a biomarker test? Decomposing sources of variation from conditionally repeated measurements](https://arxiv.org/abs/2602.14877)
*Supun Manathunga,Mart P. Janssen,Yu Luo,W. Alton Russell,Mart Pothast*

Main category: stat.AP

TL;DR: 本文开发了贝叶斯分层框架，用于分析条件性重复测量的生物标志物数据，以分解总体变异和测量变异，并评估误分类风险。


<details>
  <summary>Details</summary>
Motivation: 在临床实践中，基于初始结果重复进行不完美的生物标志物测试可能引入偏差并影响误分类风险。例如，在献血场景中，当初始血红蛋白测量低于最低阈值时，会重新测量。需要方法来分解这种条件性重复测量数据中的变异来源。

Method: 首先尝试了两种具有解析解的频率主义方法，但在正态性假设不满足时表现不佳。随后开发了贝叶斯分层框架，允许不同的分布假设，应用于献血者血红蛋白数据集，假设总体血红蛋白呈正态分布，测量变异呈重尾t分布。

Result: 贝叶斯分层模型显示，测量变异占总变异的22%（女性）和25%（男性），女性献血者的总体标准差为1.07 g/dL，男性为1.28 g/dL。

Conclusion: 贝叶斯框架可以利用任何具有条件性重复生物标志物测量的临床过程数据，估计个体在一次或多次噪声连续测量后的误分类风险，并为基于证据的条件性重复测试决策规则提供信息。

Abstract: Repeating an imperfect biomarker test based on an initial result can introduce bias and influence misclassification risk. For example, in some blood donation settings, blood donors' hemoglobin is remeasured when the initial measurement falls below a minimum threshold for donor eligibility. This paper explores methods that use data resulting from processes with conditionally repeated biomarker measurement to decompose the variation in observed measurements of a continuous biomarker into population variability and variability arising from the measurement procedure. We present two frequentist approaches with analytical solutions, but these approaches perform poorly in a dataset of conditionally repeated blood donor hemoglobin measurements where normality assumptions are not met. We then develop a Bayesian hierarchical framework that allows for different distributional assumptions, which we apply to the blood donor hemoglobin dataset. Using a Bayesian hierarchical model that assumes normally distributed population hemoglobin and heavy tailed $t$-distributed measurement variation, we found that the total measurement variation accounted for 22\% of the total variance among females and 25\% among males, with population standard deviations of $1.07\, \rm g/dL$ for female donors and $1.28\, \rm g/dL$ for male donors. Our Bayesian framework can use data resulting from any clinical process with conditionally repeated biomarker measurements to estimate individuals' misclassification risk after one or more noisy continuous measurements and inform evidence-based conditional retesting decision rules.

</details>


### [42] [Hidden Markov Individual-level Models of Infectious Disease Transmission](https://arxiv.org/abs/2602.15007)
*Dirk Douwes-Schultz,Rob Deardon,Alexandra M. Schmidt*

Main category: stat.AP

TL;DR: 提出自回归耦合隐马尔可夫模型，从个体检测时间推断未知感染和移除时间，适用于实际检测场景


<details>
  <summary>Details</summary>
Motivation: 个体层面流行病模型拟合困难，因为通常只知道检测时间而非感染或移除时间，且实际检测场景中个体通常只检测到首次阳性，测试可能不独立

Method: 提出自回归耦合隐马尔可夫模型，假设疾病检测概率依赖于过去观测，可处理单次检测时间、非独立测试等实际场景

Result: 方法能推断未知感染和移除时间及其他参数，应用于番茄斑萎病毒在辣椒植物传播实验和医院护士诺如病毒爆发两个案例

Conclusion: 该方法比传统数据增强方法更灵活，能适应更广泛的实际应用场景，包括非独立测试和有限检测数据

Abstract: Individual-level epidemic models are increasingly being used to help understand the transmission dynamics of various infectious diseases. However, fitting such models to individual-level epidemic data is challenging, as we often only know when an individual's disease status was detected (e.g., when they showed symptoms) and not when they were infected or removed. We propose an autoregressive coupled hidden Markov model to infer unknown infection and removal times, as well as other model parameters, from a single observed detection time for each detected individual. Unlike more traditional data augmentation methods used in epidemic modelling, we do not assume that this detection time corresponds to infection or removal or that infected individuals must at some point be detected. Bayesian coupled hidden Markov models have been used previously for individual-level epidemic data. However, these approaches assumed each individual was continuously tested and that the tests were independent. In practice, individuals are often only tested until their first positive test, and even if they are continuously tested, only the initial detection times may be reported. In addition, multiple tests on the same individual may not be independent. We accommodate these scenarios by assuming that the probability of detecting the disease can depend on past observations, which allows us to fit a much wider range of practical applications. We illustrate the flexibility of our approach by fitting two examples: an experiment on the spread of tomato spot wilt virus in pepper plants and an outbreak of norovirus among nurses in a hospital.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [43] [Benchmarking AI Performance on End-to-End Data Science Projects](https://arxiv.org/abs/2602.14284)
*Evelyn Hughes,Rohan Alexander*

Main category: stat.OT

TL;DR: 研究评估AI模型能否完成端到端数据科学项目，创建了包含40个项目的基准测试和自动评分系统，发现不同模型表现差异大，在常规任务上表现良好但在需要判断的任务上仍有不足


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试主要关注数据科学的组成部分（技术、分析、沟通、伦理技能），但缺乏对端到端数据科学项目能力的评估。研究旨在测试AI模型能否生成完整的端到端数据科学项目。

Method: 创建了包含40个端到端数据科学项目的基准测试，并开发了相关的评分标准。建立了自动评分管道，系统性地评估生成式AI模型产生的数据科学项目质量。

Result: 不同生成式AI模型在完成端到端数据科学项目的能力上差异显著。最新模型在结构化任务上表现良好，但在需要判断的任务上存在明显差异。AI模型能够近似初级数据科学家在常规任务上的表现，但仍需要验证。

Conclusion: 虽然AI模型在常规数据科学任务上表现出色，能够近似初级数据科学家的能力，但在需要判断和决策的任务上仍有不足，实际应用中需要人工验证和指导。

Abstract: Data science is an integrated workflow of technical, analytical, communication, and ethical skills, but current AI benchmarks focus mostly on constituent parts. We test whether AI models can generate end-to-end data science projects. To do this we create a benchmark of 40 end-to-end data science projects with associated rubric evaluations. We use these to build an automated grading pipeline that systematically evaluates the data science projects produced by generative AI models. We find the extent to which generative AI models can complete end-to-end data science projects varies considerably by model. Most recent models did well on structured tasks, but there were considerable differences on tasks that needed judgment. These findings suggest that while AI models could approximate entry-level data scientists on routine tasks, they require verification.

</details>
