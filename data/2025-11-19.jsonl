{"id": "2511.13203", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13203", "abs": "https://arxiv.org/abs/2511.13203", "authors": ["Marco F. De Sanctis", "Eleonora Arnone", "Francesca Ieva", "Laura M. Sangalli"], "title": "Modeling group heterogeneity in spatio-temporal data via physics-informed semiparametric regression", "comment": null, "summary": "In this work we propose a novel approach for modeling spatio-temporal data characterized by group structures. In particular, we extend classical mixed effect regression models by introducing a space-time nonparametric component, regularized through a partial differential equation, to embed the physical dynamics of the underlying process, while random effects capture latent variability associated with the group structure present in the data. We propose a two-step procedure to estimate the fixed and random components of the model, relying on a functional version of the Iterative Reweighted Least Squares algorithm. We investigate the asymptotic properties of both fixed and random components, and we assess the performance of the proposed model through a simulation study, comparing it with state-of-the-art alternatives from the literature. The proposed methodology is finally applied to the study of hourly nitrogen dioxide concentration data in Lombardy (Italy), using random effects to account for measurement heterogeneity across monitoring stations equipped with different sensor technologies."}
{"id": "2511.13337", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.13337", "abs": "https://arxiv.org/abs/2511.13337", "authors": ["Anna van Es", "Eva Cantoni"], "title": "Novel Tau-Informed Initialization for Maximum Likelihood Estimation of Copulas with Discrete Margins", "comment": null, "summary": "We study Gaussian-copula models with discrete margins, with primary emphasis on low-count (Poisson) data. Our goal is exact yet computationally efficient maximum likelihood (ML) estimation in regimes where many observations contain small counts, which imperils both identifiability and numerical stability. We develop three novel Kendall's tau-based approaches for initialization tailored to discrete margins in the low-count regime and embed it within an inference functions for margins (IFM) inspired start. We present three practical initializers (exact, low-intensity approximation, and a transformation-based approach) that substantially reduce the number of ML iterations and improve convergence. For the ML stage, we use an unconstrained reparameterization of the model's parameters using the log and spherical-Cholesky and compute exact rectangle probabilities. Analytical score functions are supplied throughout to stabilize Newton-type optimization. A simulation study across dimensions, dependence levels, and intensity regimes shows that the proposed initialization combined with exact ML achieves lower root-mean-squared error, lower bias and faster computation times than the alternative procedures. The methodology provides a pragmatic path to retain the statistical guarantees of ML (consistency, asymptotic normality, efficiency under correct specification) while remaining tractable for moderate- to high-dimensional discrete data. We conclude with guidance on initializer choice and discuss extensions to alternative correlation structures and different margins."}
{"id": "2511.11833", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.11833", "abs": "https://arxiv.org/abs/2511.11833", "authors": ["Bora Jin", "Bonita D. Salmerón", "David McClosky", "David H. Hagan", "Russell R. Dickerson", "Nicholas J. Spada", "Lauren N. Deanes", "Matthew A. Aubourg", "Laura E. Schmidt", "Gregory G. Sawtell", "Christopher D. Heaney", "Abhirup Datta"], "title": "Use of multi-pollutant air sensor data and geometric non-negative matrix factorization for source apportionment of air pollution burden in Curtis Bay, Baltimore, USA", "comment": null, "summary": "Air sensor networks provide hyperlocal, high temporal resolution data on multiple pollutants that can support credible identification of common pollution sources. Source apportionment using least squares-based non-negative matrix factorization is non-unique and often does not scale. A recent geometric source apportionment framework focuses inference on the source attribution matrix, which is shown to remain identifiable even when the factorization is not. Recognizing that the method scales with and benefits from large data volumes, we use this geometric method to analyze 451,946 one-minute air sensor records from Curtis Bay (Baltimore, USA), collected from October 21, 2022 to June 16, 2023, covering size-resolved particulate matter (PM), black carbon (BC), carbon monoxide (CO), nitric oxide (NO), and nitrogen dioxide (NO2). The analysis identifies three stable sources. Source 1 explains > 70% of fine and coarse PM and ~30% of BC. Source 2 dominates CO and contributes ~70% of BC, NO, and NO2. Source 3 is specific to the larger PM fractions, PM10 to PM40. Regression analyses show Source 1 and Source 3 rise during bulldozer activity at a nearby coal terminal and under winds from the terminal, indicating a direct coal terminal influence, while Source 2 exhibits diurnal patterns consistent with traffic. A case-study on the day with a known bulldozer incident at the coal terminal further confirms the association of terminal activities with Sources 1 and 3. The results are stable under sensitivity analyses. The analysis demonstrates that geometric source apportionment, paired with high temporal resolution data from multi-pollutant air sensor networks, delivers scalable and reliable evidence to inform mitigation strategies."}
{"id": "2511.11776", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.11776", "abs": "https://arxiv.org/abs/2511.11776", "authors": ["Marco Doretti", "Elena Stanghellini", "Alessandro Taraborrelli"], "title": "Handling outcome-dependent missingness with binary responses: A Heckman-like model", "comment": "4 pages, 1 figure", "summary": "In regression models with missing outcomes, selection bias can arise when the missingness mechanism depends on the outcome itself. This proposal focuses on an extension of the Heckman model to a setting where the outcome is binary and both the selection process and the outcome are modeled through logistic regression. A correction term analogous to the inverse Mills' ratio is derived based on relative risks. Under given assumptions, such a strategy provides an effective tool for bias correction in the presence of informative missingness."}
{"id": "2511.12242", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12242", "abs": "https://arxiv.org/abs/2511.12242", "authors": ["Zhuoran Yu", "Armin Schwartzman", "Junting Ren", "Julia Wrobel"], "title": "SCoRES: An R Package for Simultaneous Confidence Region Estimates", "comment": "36 pages, 18 figures", "summary": "The identification of domain sets whose outcomes belong to predefined subsets can address fundamental risk assessment challenges in climatology and medicine. Existing approaches for inverse domain estimates require restrictive assumptions, including domain density and continuity of function near thresholds, and large-sample guarantees, which limit the applicability. Besides, the estimation and coverage depend on setting a fixed threshold level, which is difficult to determine. Recently, Ren et al. (2024) proved that confidence sets of multiple levels can be simultaneously constructed with the desired confidence non-asymptotically through inverting simultaneous confidence bands. Here, we present the SCoRES R package, which implements Ren's approach for both the estimation of the inverse region and the corresponding simultaneous outer and inner confidence regions, along with visualization tools. Besides, the package also provides functions that help construct SCBs for regression data, functional data and geographical data. To illustrate its broad applicability, we present three rigorous examples that demonstrate the SCoRES workflow."}
{"id": "2511.11682", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11682", "abs": "https://arxiv.org/abs/2511.11682", "authors": ["Hayate Toba", "Atsushi Yano", "Takuya Azumi"], "title": "Generalized Inequality-based Approach for Probabilistic WCET Estimation", "comment": null, "summary": "Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions."}
{"id": "2511.12219", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.12219", "abs": "https://arxiv.org/abs/2511.12219", "authors": ["Osafu Augustine Egbon", "Asrat Mekonnen Belachew", "Ezra Gayawan", "Francisco Louzada"], "title": "Violent event-related fatality patterns in Ethiopia: a Bayesian spatiotemporal perspective", "comment": null, "summary": "Fatalities resulting from violence in armed conflict have long been a significant public health issue in Ethiopia. Despite the severity of this problem, more comprehensive quantitative scientific studies need to be conducted to elucidate the sequence and dynamics of these occurrences. In response, this study introduces a spatio-temporal statistical method designed to uncover the patterns of fatalities associated with violent events in Ethiopia. The research employs a two-part zero-inflated Bayesian generalized additive mixed model, which integrates a spatio-temporal component to map the fatality patterns across Ethiopian regions. The dataset utilized originates from the Armed Conflict Location and Event Data Project, covering fatality counts related to violent events from 1997 to 2022. The analysis revealed that nine out of thirteen administrative regions exhibited a probability greater than 0.6 for fatality occurrence due to violent events, with five regions surpassing a 0.7 probability threshold. These five regions include Benishangul Gumz, Gambela, Oromia, Somali, and the South West Ethiopian People's Region. Notably, the Tigray region displayed the highest probability (0.558) of experiencing more than 20 deaths per violent event, followed by the Benishangul Gumz region with a probability of 0.306. Encouragingly, the findings also indicate an average decline in fatalities per violent event over time. Specifically, the probability of more than 20 deaths per event was 0.401 in 2020, which decreased to 0.148 by 2022. These insights are invaluable for the government, policymakers, political leaders, and traditional or religious authorities in Ethiopia, enabling them to make informed, strategic decisions to mitigate and ultimately prevent violence-related fatalities in the country."}
{"id": "2511.11782", "categories": ["stat.ME", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.11782", "abs": "https://arxiv.org/abs/2511.11782", "authors": ["Sascha Desmettre", "Agnes Mallinger", "Amira Meddah", "Irene Tubikanec"], "title": "Approximate Bayesian computation for stochastic hybrid systems with ergodic behaviour", "comment": "30 pages, 19 figures", "summary": "Piecewise diffusion Markov processes (PDifMPs) form a versatile class of stochastic hybrid systems that combine continuous diffusion processes with discrete event-driven dynamics, enabling flexible modelling of complex real-world hybrid phenomena. The practical utility of PDifMP models, however, depends critically on accurate estimation of their underlying parameters. In this work, we present a novel framework for parameter inference in PDifMPs based on approximate Bayesian computation (ABC). Our contributions are threefold. First, we provide detailed simulation algorithms for PDifMP sample paths. Second, we extend existing ABC summary statistics for diffusion processes to account for the hybrid nature of PDifMPs, showing particular effectiveness for ergodic systems. Third, we demonstrate our approach on several representative example PDifMPs that empirically exhibit ergodic behaviour. Our results show that the proposed ABC method reliably recovers model parameters across all examples, even in challenging scenarios where only partial information on jumps and diffusion is available or when parameters appear in state-dependent jump rate functions. These findings highlight the potential of ABC as a practical tool for inference in various complex stochastic hybrid systems."}
{"id": "2511.12257", "categories": ["stat.CO", "cs.CV", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12257", "abs": "https://arxiv.org/abs/2511.12257", "authors": ["Elhadji Cisse Faye", "Mame Diarra Fall", "Nicolas Dobigeon", "Eric Barat"], "title": "Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems", "comment": null, "summary": "This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches."}
{"id": "2511.11817", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11817", "abs": "https://arxiv.org/abs/2511.11817", "authors": ["Zhongde An", "Jinhong You", "Jiyanglin Li", "Yiming Tang", "Wen Li", "Heming Du", "Shouguo Du"], "title": "FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition", "comment": null, "summary": "Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\\%."}
{"id": "2511.12234", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12234", "abs": "https://arxiv.org/abs/2511.12234", "authors": ["Soham Sarkar", "Arnab Hazra"], "title": "A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment", "comment": "40 pages, 3 figures, 3 tables", "summary": "Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching."}
{"id": "2511.11996", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.11996", "abs": "https://arxiv.org/abs/2511.11996", "authors": ["Zitian Wu", "Arkaprava Roy", "Leo L. Duan"], "title": "Graphical Model-based Inference on Persistent Homology", "comment": "29 pages, 11 figures", "summary": "Persistent homology is a cornerstone of topological data analysis, offering a multiscale summary of topology with robustness to nuisance transformations, such as rotations and small deformations. Persistent homology has seen broad use across domains such as computer vision and neuroscience. Most statistical treatments, however, use homology primarily as a feature extractor, relying on statistical distance-based tests or simple time-to-event models for inferential tasks. While these approaches can detect global differences, they rarely localize the source of those differences. We address this gap by taking a graphical model-based approach: we associate each vertex with a population latent position in a conic space and model each bar's key events (birth and death times) using an exponential distribution, whose rate is a transformation of the latent positions according to an event occurring on the graph. The low-dimensional bars have simple graph-event representations, such as the formation of a minimum spanning tree or the triangulation of a loop, and thus enjoy tractable likelihoods. Taking a Bayesian approach, we infer latent positions and enable model extensions such as hierarchical models that allow borrowing strength across groups. Applications to a neuroimaging study of Alzheimer's disease demonstrate that our method localizes sources of difference and provides interpretable, model-based analyses of topological structure in complex data. The code is provided and maintained at https://github.com/zitianwu/graphPH."}
{"id": "2511.12343", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.12343", "abs": "https://arxiv.org/abs/2511.12343", "authors": ["Xueyan Hu", "Jerome P. Reiter"], "title": "Regression Analysis After Bipartite Bayesian Record Linkage", "comment": null, "summary": "In many settings, a data curator links records from two files to produce datasets that are shared with secondary analysts. Analysts use the linked files to estimate models of interest, such as regressions. Such two-stage approaches do not necessarily account for uncertainty in model parameters that results from uncertainty in the linkages. Further, they do not leverage the relationships among the study variables in the two files to help determine the linkages. We propose a multiple imputation framework to address these shortcomings. First, we use a bipartite Bayesian record linkage model to generate multiple plausible linked datasets, disregarding the information in the study variables. Second, we presume each linked file has a mixture of true links and false links. We estimate the mixture model using information from the study variables. Through simulation studies under a regression setting, we demonstrate that estimates of the regression model parameters can be more accurate than those based on an analogous two-stage approach. We illustrate the integrated approach using data from the Survey on Household Income and Wealth, examining a regression involving the persistence of income."}
{"id": "2511.11927", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11927", "abs": "https://arxiv.org/abs/2511.11927", "authors": ["Urte Adomaityte", "Gabriele Sicuro", "Pierpaolo Vivo"], "title": "PCA recovery thresholds in low-rank matrix inference with sparse noise", "comment": "24 pages, 7 figures", "summary": "We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices."}
{"id": "2511.12397", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12397", "abs": "https://arxiv.org/abs/2511.12397", "authors": ["Pedro A. Pury"], "title": "Stochastic Predictive Analytics for Stocks in the Newsvendor Problem", "comment": "21 pages, 4 figures", "summary": "This work addresses a key challenge in inventory management by developing a stochastic model that describes the dynamic distribution of inventory stock over time without assuming a specific demand distribution. Our model provides a flexible and applicable solution for situations with limited historical data and short-term predictions, making it well-suited for the Newsvendor problem. We evaluate our model's performance using real-world data from a large electronic marketplace, demonstrating its effectiveness in a practical forecasting scenario."}
{"id": "2511.12016", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12016", "abs": "https://arxiv.org/abs/2511.12016", "authors": ["Youwu Lin", "Xiaoyu Qian", "Jinru Wu", "Qi Liu", "Pei Wang"], "title": "MMDCP: A Distribution-free Approach to Outlier Detection and Classification with Coverage Guarantees and SCW-FDR Control", "comment": "33 pages", "summary": "We propose the Modified Mahalanobis Distance Conformal Prediction (MMDCP), a unified framework for multi-class classification and outlier detection under label shift, where the training and test distributions may differ. In such settings, many existing methods construct nonconformity scores based on empirical cumulative or density functions combined with data-splitting strategies. However, these approaches are often computationally expensive due to their heavy reliance on resampling procedures and tend to produce overly conservative prediction sets with unstable coverage, especially in small samples. To address these challenges, MMDCP combines class-specific distance measures with full conformal prediction to construct a score function, thereby producing adaptive prediction sets that effectively capture both inlier and outlier structures. Under mild regularity conditions, we establish convergence rates for the resulting sets and provide the first theoretical characterization of the gap between oracle and empirical conformal $p$-values, which ensures valid coverage and effective control of the class-wise false discovery rate (CW-FDR). We further introduce the Summarized Class-Wise FDR (SCW-FDR), a novel global error metric aggregating false discoveries across classes, and show that it can be effectively controlled within the MMDCP framework. Extensive simulations and two real-data applications support our theoretical findings and demonstrate the advantages of the proposed method."}
{"id": "2511.13296", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.13296", "abs": "https://arxiv.org/abs/2511.13296", "authors": ["Michail Tsagris"], "title": "Transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares", "comment": null, "summary": "Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \\cite{fiksel2022} proposed a transformation-free lienar regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions was recently proposed. To effectively estimate the regression coefficients the EM algorithm was employed. We formulate the model as a constrained logistic regression, in the spirit of \\cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. This approach makes the estimation procedure significantly faster."}
{"id": "2511.11983", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11983", "abs": "https://arxiv.org/abs/2511.11983", "authors": ["Debashis Chatterjee"], "title": "Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence", "comment": null, "summary": "Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization.\n  We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective.\n  Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making."}
{"id": "2511.12510", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.12510", "abs": "https://arxiv.org/abs/2511.12510", "authors": ["Yingke He"], "title": "Do Nineteenth-Century Graphics Still Work for Today's Readers?", "comment": "23 pages, 23 figures; includes appendices", "summary": "Do nineteenth-century graphics still work for today's readers? To investigate this question, we conducted a controlled experiment evaluating three canonical historical visualizations- Nightingale's polar area diagram, Playfair's trade balance chart, and Minard's campaign map-against modern redesigns. Fifty-four participants completed structured question-answering tasks, allowing us to measure accuracy, response time, and perceived workload (NASA-TLX). We used mixed-effects regression models to find: Nightingale's diagram remained consistently effective across versions, achieving near-ceiling accuracy and low workload; Playfair's dual-axis redesign underperformed relative to both its historical and alternative versions; and Minard's map showed large accuracy gains under redesign but continued to impose high workload and long response times. These results demonstrate that some nineteenth-century designs remain effective, others degrade under certain modernizations, and some benefit from careful redesign. The findings indicate how perceptual encoding choices, task alignment, and cognitive load determine whether historical charts survive or require adaptation for contemporary use."}
{"id": "2511.12065", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12065", "abs": "https://arxiv.org/abs/2511.12065", "authors": ["Congbin Xu", "Yue Yu", "Haojie Ren", "Zhaojun Wang", "Changliang Zou"], "title": "Aggregating Conformal Prediction Sets via α-Allocation", "comment": null, "summary": "Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage."}
{"id": "2511.13694", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.13694", "abs": "https://arxiv.org/abs/2511.13694", "authors": ["Jay Bartroff", "Asmit Chakraborty"], "title": "Shortest fixed-width confidence intervals for a bounded parameter: The Push algorithm", "comment": null, "summary": "We present a method for computing optimal fixed-width confidence intervals for a single, bounded parameter, extending a method for the binomial due to Asparaouhov and Lorden, who called it the Push algorithm. The method produces the shortest possible non-decreasing confidence interval for a given confidence level, and if the Push interval does not exist for a given width and level, then no such interval exists. The method applies to any bounded parameter that is discrete, or is continuous and has the monotone likelihood ratio property. We demonstrate the method on the binomial, hypergeometric, and normal distributions with our available R package. In each of these distributions the proposed method outperforms the standard ones, and in the latter case even improves upon the $z$-interval. We apply the proposed method to World Health Organization (WHO) data on tobacco use."}
{"id": "2511.12278", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12278", "abs": "https://arxiv.org/abs/2511.12278", "authors": ["Mingqi Wu", "Qiang Sun", "Yi Yang"], "title": "PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning", "comment": "14 pages main, 26 pages appendix", "summary": "High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness."}
{"id": "2511.12625", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.12625", "abs": "https://arxiv.org/abs/2511.12625", "authors": ["Brian O'Donovan", "Andrew Finley", "James Sweeney"], "title": "A spatio-temporal statistical model for property valuation at country-scale with adjustments for regional submarkets", "comment": null, "summary": "Valuing residential property is inherently complex, requiring consideration of numerous environmental, economic, and property-specific factors. These complexities present significant challenges for automated valuation models (AVMs), which are increasingly used to provide objective assessments for property taxation and mortgage financing. The challenge of obtaining accurate and objective valuations for properties at a country level, and not just within major cities, is further compounded by the presence of multiple localised submarkets-spanning urban, suburban, and rural contexts-where property features contribute differently to value. Existing AVMs often struggle in such settings: traditional hedonic regression models lack the flexibility to capture spatial variation, while advanced machine learning approaches demand extensive datasets that are rarely available. In this article, we address these limitations by developing a robust statistical framework for property valuation in the Irish housing market. We segment the country into six submarkets encompassing cities, large towns, and rural areas, and employ a generalized additive model that captures non-linear effects of property characteristics while allowing feature contributions to vary across submarkets. Our approach outperforms both machine learning-based and traditional hedonic regression models, particularly in data-sparse regions. In out-of-sample validation, our model achieves R-squared values of 0.70, 0.84, and 0.83 for rural areas, towns, and Dublin, respectively, compared to 0.52, 0.71, and 0.82 from a random forest benchmark. Furthermore, the temporal dynamics of our model align closely with reported inflation figures for the study period, providing additional validation of its accuracy."}
{"id": "2511.12179", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12179", "abs": "https://arxiv.org/abs/2511.12179", "authors": ["Ismaila A. Jallow", "Samya Tajmouati"], "title": "Determinants of financial and digital inclusion in West and Central Africa: Evidence from binary models with cross-validation", "comment": null, "summary": "This study examines the determinants of financial and digital inclusion in West and Central Africa using the World Bank Findex 2021 data. Unlike prior works that rely solely on traditional logit and probit models, we combine country-by-country analysis with robustness checks including K-fold cross-validation and Vuong test. Three samples were considered : a full sample combin- ing both regions and two separate subsamples for West and Central Africa. The results indicate that gender, educational attainment, income level, and place of residence are significant factors influencing both financial and digital inclusion in the full sample and the West African subsam- ple. In the Central African subsample, gender is not significant; however, age, education, income, and rural residence emerge as key determinants of financial and digital inclusion. Overall, Ghana stands out as the most financially inclusive country, although it trails Senegal in terms of credit access and digital payment use. Nigeria leads in formal account ownership and formal savings but falls considerably behind Ghana in mobile money account ownership and digital payments. Central African countries generally report lower levels of inclusion compared to West Africa, with Cameroon performing comparatively better."}
{"id": "2511.12688", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12688", "abs": "https://arxiv.org/abs/2511.12688", "authors": ["Kaicheng Jin", "Yang Peng", "Jiansheng Yang", "Zhihua Zhang"], "title": "Accelerated Distributional Temporal Difference Learning with Linear Function Approximation", "comment": null, "summary": "In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms."}
{"id": "2511.12703", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12703", "abs": "https://arxiv.org/abs/2511.12703", "authors": ["Qingqing Song", "Shaoliang Xia"], "title": "Change-Point Detection Utilizing Normalized Entropy as a Fundamental Metric", "comment": "4 pages, 2 figures, 2 tables, accepted to the 17th International Conference on Pattern Recognition and Information Processing (PRIP 2025)", "summary": "This paper introduces a concept for change-point detection based on normalized entropy as a fundamental metric, aiming to overcome the dependence of traditional entropy methods on assumptions about data distribution and absolute scales. Normalized entropy maps entropy values to the [0,1] interval through standardization, accurately capturing relative changes in data complexity. By utilizing a sliding window to compute normalized entropy, this approach transforms the challenge of detecting change points in complex time series, arising from variations in scale, distribution, and diversity, into the task of identifying significant features within the normalized entropy sequence, thereby avoiding interference from parametric assumptions and effectively highlighting distributional shifts. Experimental results show that normalized entropy exhibits significant numerical fluctuation characteristics and patterns near change points across various distributions and parameter combinations. The average deviation between fluctuation moments and actual change points is only 2.4% of the sliding window size, demonstrating strong adaptability. This paper provides theoretical support for change-point detection in complex data environments and lays a methodological foundation for precise and automated detection based on normalized entropy as a fundamental metric."}
{"id": "2511.12333", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12333", "abs": "https://arxiv.org/abs/2511.12333", "authors": ["Wei Jin", "Lang Lang", "Amanda B. Spence", "Leah H. Rubin", "Yanxun Xu"], "title": "Bayesian Causal Discovery with Cycles and Latent Confounders", "comment": null, "summary": "Learning causality from observational data has received increasing interest across various scientific fields. However, most existing methods assume the absence of latent confounders and restrict the underlying causal graph to be acyclic, assumptions that are often violated in many real-world applications. In this paper, we address these challenges by proposing a novel framework for causal discovery that accommodates both cycles and latent confounders. By leveraging the identifiability results from noisy independent component analysis and recent advances in factor analysis, we establish the unique causal identifiability under mild conditions. Building on this foundation, we further develop a fully Bayesian approach for causal structure learning, called BayCausal, and evaluate its identifiability, utility, and superior performance against state-of-the-art alternatives through extensive simulation studies. Application to a dataset from the Women's Interagency HIV Study yields interpretable and clinically meaningful insights. To facilitate broader applications, we have implemented BayCausal in an R package, BayCausal, which is the first publicly available software capable of achieving unique causal identification in the presence of both cycles and latent confounders."}
{"id": "2511.12749", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12749", "abs": "https://arxiv.org/abs/2511.12749", "authors": ["Zong-Han Bai", "Po-Yen Chu"], "title": "TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting", "comment": "Preprint. 11 pages, 1 figure, Equal contribution by the two authors", "summary": "Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability.\n  We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing.\n  On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable."}
{"id": "2511.12999", "categories": ["stat.AP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12999", "abs": "https://arxiv.org/abs/2511.12999", "authors": ["Harrison H. Li", "Medhanie Irgau", "Nabil Janmohamed", "Karen Solveig Rieckmann", "David B. Lobell"], "title": "Scalable Vision-Guided Crop Yield Estimation", "comment": "Accepted as a conference paper at AAAI 2026 (oral presentation). This is the extended version, including the technical appendix", "summary": "Precise estimation and uncertainty quantification for average crop yields are critical for agricultural monitoring and decision making. Existing data collection methods, such as crop cuts in randomly sampled fields at harvest time, are relatively time-consuming. Thus, we propose an approach based on prediction-powered inference (PPI) to supplement these crop cuts with less time-consuming field photos. After training a computer vision model to predict the ground truth crop cut yields from the photos, we learn a ``control function\" that recalibrates these predictions with the spatial coordinates of each field. This enables fields with photos but not crop cuts to be leveraged to improve the precision of zone-wide average yield estimates. Our control function is learned by training on a dataset of nearly 20,000 real crop cuts and photos of rice and maize fields in sub-Saharan Africa. To improve precision, we pool training observations across different zones within the same first-level subdivision of each country. Our final PPI-based point estimates of the average yield are provably asymptotically unbiased and cannot increase the asymptotic variance beyond that of the natural baseline estimator -- the sample average of the crop cuts -- as the number of fields grows. We also propose a novel bias-corrected and accelerated (BCa) bootstrap to construct accompanying confidence intervals. Even in zones with as few as 20 fields, the point estimates show significant empirical improvement over the baseline, increasing the effective sample size by as much as 73% for rice and by 12-23% for maize. The confidence intervals are accordingly shorter at minimal cost to empirical finite-sample coverage. This demonstrates the potential for relatively low-cost images to make area-based crop insurance more affordable and thus spur investment into sustainable agricultural practices."}
{"id": "2511.12343", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.12343", "abs": "https://arxiv.org/abs/2511.12343", "authors": ["Xueyan Hu", "Jerome P. Reiter"], "title": "Regression Analysis After Bipartite Bayesian Record Linkage", "comment": null, "summary": "In many settings, a data curator links records from two files to produce datasets that are shared with secondary analysts. Analysts use the linked files to estimate models of interest, such as regressions. Such two-stage approaches do not necessarily account for uncertainty in model parameters that results from uncertainty in the linkages. Further, they do not leverage the relationships among the study variables in the two files to help determine the linkages. We propose a multiple imputation framework to address these shortcomings. First, we use a bipartite Bayesian record linkage model to generate multiple plausible linked datasets, disregarding the information in the study variables. Second, we presume each linked file has a mixture of true links and false links. We estimate the mixture model using information from the study variables. Through simulation studies under a regression setting, we demonstrate that estimates of the regression model parameters can be more accurate than those based on an analogous two-stage approach. We illustrate the integrated approach using data from the Survey on Household Income and Wealth, examining a regression involving the persistence of income."}
{"id": "2511.12783", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12783", "abs": "https://arxiv.org/abs/2511.12783", "authors": ["Jingru Huang", "Haijie Xu", "Manrui Jiang", "Chen Zhang"], "title": "Function-on-Function Bayesian Optimization", "comment": "13 pages, 4 figures, conference", "summary": "Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches."}
{"id": "2511.13326", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13326", "abs": "https://arxiv.org/abs/2511.13326", "authors": ["Siyao Zhao", "Hao Ma", "Zhiqiang Pu", "Jingjing Huang", "Yi Pan", "Shijie Wang", "Zhi Ming"], "title": "TacEleven: generative tactic discovery for football open play", "comment": null, "summary": "Creating offensive advantages during open play is fundamental to football success. However, due to the highly dynamic and long-sequence nature of open play, the potential tactic space grows exponentially as the sequence progresses, making automated tactic discovery extremely challenging. To address this, we propose TacEleven, a generative framework for football open-play tactic discovery developed in close collaboration with domain experts from AJ Auxerre, designed to assist coaches and analysts in tactical decision-making. TacEleven consists of two core components: a language-controlled tactical generator that produces diverse tactical proposals, and a multimodal large language model-based tactical critic that selects the optimal proposal aligned with a high-level stylistic tactical instruction. The two components enables rapid exploration of tactical proposals and discovery of alternative open-play offensive tactics. We evaluate TacEleven across three tasks with progressive tactical complexity: counterfactual exploration, single-step discovery, and multi-step discovery, through both quantitative metrics and a questionnaire-based qualitative assessment. The results show that the TacEleven-discovered tactics exhibit strong realism and tactical creativity, with 52.50% of the multi-step tactical alternatives rated adoptable in real-world elite football scenarios, highlighting the framework's ability to rapidly generate numerous high-quality tactics for complex long-sequence open-play situations. TacEleven demonstrates the potential of creatively leveraging domain data and generative models to advance tactical analysis in sports."}
{"id": "2511.12375", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.12375", "abs": "https://arxiv.org/abs/2511.12375", "authors": ["Yinxiang Wu", "Neil M. Davies", "Ting Ye"], "title": "Group Identification and Variable Selection in Multivariable Mendelian Randomization with Highly-Correlated Exposures", "comment": null, "summary": "Multivariable Mendelian Randomization (MVMR) estimates the direct causal effects of multiple risk factors on an outcome using genetic variants as instruments. The growing availability of summary-level genetic data has created opportunities to apply MVMR in high-dimensional settings with many strongly correlated candidate risk factors. However, existing methods face three major limitations: weak instrument bias, limited interpretability, and the absence of valid post-selection inference. Here we introduce MVMR-PACS, a method that identifies signal-groups -- sets of causal risk factors with high genetic correlation or indistinguishable causal effects -- and estimates the direct effect of each group. MVMR-PACS minimizes a debiased objective function that reduces weak instrument bias while yielding interpretable estimates with theoretical guarantees for variable selection. We adapt a data-thinning strategy to summary-data MVMR to enable valid post-selection inference. In simulations, MVMR-PACS outperforms existing approaches in both estimation accuracy and variable selection. When applied to 27 lipoprotein subfraction traits and coronary artery disease risk, MVMR-PACS identifies biologically meaningful and robust signal-groups with interpretable direct causal effects."}
{"id": "2511.12840", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12840", "abs": "https://arxiv.org/abs/2511.12840", "authors": ["Yuta Kondo"], "title": "Benign Overfitting in Linear Classifiers with a Bias Term", "comment": "17 pages", "summary": "Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of \"homogeneous\" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization."}
{"id": "2511.13603", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13603", "abs": "https://arxiv.org/abs/2511.13603", "authors": ["Bartosz Uniejewski"], "title": "Variance Stabilizing Transformations for Electricity Price Forecasting in Periods of Increased Volatility", "comment": null, "summary": "Accurate day-ahead electricity price forecasts are critical for power system operation and market participation, yet growing renewable penetration and recent crises have caused unprecedented volatility that challenges standard models. This paper revisits variance-stabilizing transformations (VSTs) as a preprocessing tool by introducing a novel parametrization of the asinh transformation, systematically analyzing parameter sensitivity and calibration window size, and explicitly testing performance under volatile market regimes. Using data from Germany, Spain, and France over 2015-2024 with two model classes (NARX and LEAR), we show that VSTs substantially reduce forecast errors, with gains of up to 14.6% for LEAR and 8.7% for NARX relative to untransformed benchmarks. The new parametrized asinh consistently outperforms its standard form, while rolling averaging across transformations delivers the most robust improvements, reducing errors by up to 17.7%. Results demonstrate that VSTs are especially valuable in volatile regimes, making them a powerful tool for enhancing electricity price forecasting in today's power markets."}
{"id": "2511.12435", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12435", "abs": "https://arxiv.org/abs/2511.12435", "authors": ["Bo Fu", "Dandan Jiang"], "title": "Transfer learning for high-dimensional Factor-augmented sparse linear model", "comment": "52 pages, 2 figures", "summary": "In this paper, we study transfer learning for high-dimensional factor-augmented sparse linear models, motivated by applications in economics and finance where strongly correlated predictors and latent factor structures pose major challenges for reliable estimation. Our framework simultaneously mitigates the impact of high correlation and removes the additional contributions of latent factors, thereby reducing potential model misspecification in conventional linear modeling. In such settings, the target dataset is often limited, but multiple heterogeneous auxiliary sources may provide additional information. We develop transfer learning procedures that effectively leverage these auxiliary datasets to improve estimation accuracy, and establish non-asymptotic $\\ell_1$- and $\\ell_2$-error bounds for the proposed estimators. To prevent negative transfer, we introduce a data-driven source detection algorithm capable of identifying informative auxiliary datasets and prove its consistency. In addition, we provide a hypothesis testing framework for assessing the adequacy of the factor model, together with a procedure for constructing simultaneous confidence intervals for the regression coefficients of interest. Numerical studies demonstrate that our methods achieve substantial gains in estimation accuracy and remain robust under heterogeneity across datasets. Overall, our framework offers a theoretical foundation and a practically scalable solution for incorporating heterogeneous auxiliary information in settings with highly correlated features and latent factor structures."}
{"id": "2511.13025", "categories": ["stat.ML", "cs.LG", "math.DG", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.13025", "abs": "https://arxiv.org/abs/2511.13025", "authors": ["Charles Fefferman", "Jonathan Marty", "Kevin Ren"], "title": "Reconstruction of Manifold Distances from Noisy Observations", "comment": "43 pages", "summary": "We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $μ$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\\dots,X_N$ are i.i.d. samples of $μ$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\\varepsilon \\log \\varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \\asymp \\varepsilon^{-2d-2}\\log(1/\\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces."}
{"id": "2511.13672", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13672", "abs": "https://arxiv.org/abs/2511.13672", "authors": ["Tung-Lung Wu"], "title": "Phase I Distribution-Free Control Charts for Individual Observations Using Runs and Patterns", "comment": null, "summary": "Phase I distribution-free runs- and patterns-type control charts are proposed for monitoring the unknown target value (or location parameter) for both continuous and discrete individual observations. Our approach maintains the nominal in-control signal probability at a prescribed level by employing the finite Markov chain imbedding technique combined with random permutation and conditioning arguments. To elucidate the methodology, we examine two popular runs- and patterns-type statistics: the number of success runs and the scan statistic. Numerical results indicate that the performance of our proposed control charts is comparable to that of existing Phase I nonparametric control charts for individual observations."}
{"id": "2511.12459", "categories": ["stat.ME", "cs.CY", "math.PR", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.12459", "abs": "https://arxiv.org/abs/2511.12459", "authors": ["Marco Pollanen"], "title": "The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias", "comment": "24 pages, 1 figure", "summary": "For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.\n  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \\(10^{-35}\\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \\emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.\n  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement."}
{"id": "2511.13221", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13221", "abs": "https://arxiv.org/abs/2511.13221", "authors": ["Mohamed Salem", "Inyoung Kim"], "title": "Likelihood-guided Regularization in Attention Based Models", "comment": null, "summary": "The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques."}
{"id": "2511.12375", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.12375", "abs": "https://arxiv.org/abs/2511.12375", "authors": ["Yinxiang Wu", "Neil M. Davies", "Ting Ye"], "title": "Group Identification and Variable Selection in Multivariable Mendelian Randomization with Highly-Correlated Exposures", "comment": null, "summary": "Multivariable Mendelian Randomization (MVMR) estimates the direct causal effects of multiple risk factors on an outcome using genetic variants as instruments. The growing availability of summary-level genetic data has created opportunities to apply MVMR in high-dimensional settings with many strongly correlated candidate risk factors. However, existing methods face three major limitations: weak instrument bias, limited interpretability, and the absence of valid post-selection inference. Here we introduce MVMR-PACS, a method that identifies signal-groups -- sets of causal risk factors with high genetic correlation or indistinguishable causal effects -- and estimates the direct effect of each group. MVMR-PACS minimizes a debiased objective function that reduces weak instrument bias while yielding interpretable estimates with theoretical guarantees for variable selection. We adapt a data-thinning strategy to summary-data MVMR to enable valid post-selection inference. In simulations, MVMR-PACS outperforms existing approaches in both estimation accuracy and variable selection. When applied to 27 lipoprotein subfraction traits and coronary artery disease risk, MVMR-PACS identifies biologically meaningful and robust signal-groups with interpretable direct causal effects."}
{"id": "2511.12698", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12698", "abs": "https://arxiv.org/abs/2511.12698", "authors": ["Kenichiro McAlinn", "Kosaku Takanashi"], "title": "Determining the K in K-fold cross-validation", "comment": null, "summary": "Cross-validation is a standard technique used across science to test how well a model predicts new data. Data are split into $K$ \"folds,\" where one fold (i.e., hold-out set) is used to evaluate a model's predictive ability. Researchers typically rely on conventions when choosing $K$, commonly $K=5$, or $80{:}20$ split, even though the choice of $K$ can affect inference and model evaluation. Principally, this $K$ should be determined by balancing the predictive accuracy (bias) and the uncertainty of this accuracy (variance), which forms a tradeoff based on the size of the hold-out set. More training data means more accurate models, but fewer testing data lead to uncertain evaluation, and vice versa. The challenge is that this evaluation uncertainty cannot be estimated directly from data. We propose a procedure to determine the optimal $K$ by deriving a finite-sample upper bound on the evaluation uncertainty and adopting a utility-based approach to make this tradeoff explicit. Analyses of real-world datasets using linear regression and random forest demonstrate this procedure in practice, providing insight into implicit assumptions, robustness, and model performance. Critically, the results show that the optimal $K$ depends on both the data and the model, and that conventional choices implicitly make assumptions about the fundamental characteristics of the data. Our framework makes these assumptions explicit and provides a principled, transparent way to select $K$ based on the data and model rather than convention. By replacing a one-size-fits-all choice with context-specific reasoning, it enables more reliable comparisons of predictive performance across scientific domains."}
{"id": "2511.13503", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13503", "abs": "https://arxiv.org/abs/2511.13503", "authors": ["Ioannis Diamantis"], "title": "The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business", "comment": "36 pages, 22 figures", "summary": "Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \\textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics."}
{"id": "2511.12459", "categories": ["stat.ME", "cs.CY", "math.PR", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.12459", "abs": "https://arxiv.org/abs/2511.12459", "authors": ["Marco Pollanen"], "title": "The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias", "comment": "24 pages, 1 figure", "summary": "For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.\n  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \\(10^{-35}\\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \\emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.\n  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement."}
{"id": "2511.12732", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12732", "abs": "https://arxiv.org/abs/2511.12732", "authors": ["Lida Chalangar Jalili Dehkharghani", "Li-Hsiang Lin"], "title": "Scalable and Communication-Efficient Varying Coefficient Mixed Effect Models: Methodology, Theory, and Applications", "comment": "2 Figures", "summary": "Human migration exhibits complex spatiotemporal dependence driven by environmental and socioeconomic forces. Modeling such patterns at scale-often across multiple administrative or institutional boundaries-requires statistically efficient methods that remain robust under limited communication, i.e., when transmitting raw data or large design matrices across distributed nodes is costly or restricted. This paper develops a communication-efficient inference framework for Varying Coefficient Mixed Models (VCMMs) that accommodates many input variables in the mean structure and rich correlation induced by numerous random effects in hierarchical migration data. We show that a penalized spline estimator admit a Bayesian hierarchical representation, which in turn yields sufficient statistics that preserve the full likelihood contribution of each node when communication is unconstrained; aggregating these summaries reproduces the centralized estimator exactly. Under communication constraints, the same summaries define a surrogate likelihood enabling one-step estimation with first-order statistical efficiency. The framework also incorporates an SVD-enhanced implementation to ensure numerical stability and scalability, extending applicability to settings with many random effects, with or without communication limits. Statistical and theoretical guarantees are provided. Extensive simulations confirm the accuracy and robustness of the method. An application to U.S. migration flow data demonstrates its ability to efficiently and precisely uncover dynamic spatial patterns."}
{"id": "2511.12016", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12016", "abs": "https://arxiv.org/abs/2511.12016", "authors": ["Youwu Lin", "Xiaoyu Qian", "Jinru Wu", "Qi Liu", "Pei Wang"], "title": "MMDCP: A Distribution-free Approach to Outlier Detection and Classification with Coverage Guarantees and SCW-FDR Control", "comment": "33 pages", "summary": "We propose the Modified Mahalanobis Distance Conformal Prediction (MMDCP), a unified framework for multi-class classification and outlier detection under label shift, where the training and test distributions may differ. In such settings, many existing methods construct nonconformity scores based on empirical cumulative or density functions combined with data-splitting strategies. However, these approaches are often computationally expensive due to their heavy reliance on resampling procedures and tend to produce overly conservative prediction sets with unstable coverage, especially in small samples. To address these challenges, MMDCP combines class-specific distance measures with full conformal prediction to construct a score function, thereby producing adaptive prediction sets that effectively capture both inlier and outlier structures. Under mild regularity conditions, we establish convergence rates for the resulting sets and provide the first theoretical characterization of the gap between oracle and empirical conformal $p$-values, which ensures valid coverage and effective control of the class-wise false discovery rate (CW-FDR). We further introduce the Summarized Class-Wise FDR (SCW-FDR), a novel global error metric aggregating false discoveries across classes, and show that it can be effectively controlled within the MMDCP framework. Extensive simulations and two real-data applications support our theoretical findings and demonstrate the advantages of the proposed method."}
{"id": "2511.13203", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13203", "abs": "https://arxiv.org/abs/2511.13203", "authors": ["Marco F. De Sanctis", "Eleonora Arnone", "Francesca Ieva", "Laura M. Sangalli"], "title": "Modeling group heterogeneity in spatio-temporal data via physics-informed semiparametric regression", "comment": null, "summary": "In this work we propose a novel approach for modeling spatio-temporal data characterized by group structures. In particular, we extend classical mixed effect regression models by introducing a space-time nonparametric component, regularized through a partial differential equation, to embed the physical dynamics of the underlying process, while random effects capture latent variability associated with the group structure present in the data. We propose a two-step procedure to estimate the fixed and random components of the model, relying on a functional version of the Iterative Reweighted Least Squares algorithm. We investigate the asymptotic properties of both fixed and random components, and we assess the performance of the proposed model through a simulation study, comparing it with state-of-the-art alternatives from the literature. The proposed methodology is finally applied to the study of hourly nitrogen dioxide concentration data in Lombardy (Italy), using random effects to account for measurement heterogeneity across monitoring stations equipped with different sensor technologies."}
{"id": "2511.12825", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12825", "abs": "https://arxiv.org/abs/2511.12825", "authors": ["Yuan Zhong", "Gang Chen", "Paul A. Taylor", "Jian Kang"], "title": "SIMBA: Scalable Image Modeling using a Bayesian Approach, A Consistent Framework for Including Spatial Dependencies in fMRI Studies", "comment": "30 pages, 6 figures, submitted to Imaging Neuroscience", "summary": "Bayesian spatial modeling provides a flexible framework for whole-brain fMRI analysis by explicitly incorporating spatial dependencies, overcoming the limitations of traditional massive univariate approaches that lead to information waste. In this work, we introduce SIMBA, a Scalable Image Modeling using a Bayesian Approach, for group-level fMRI analysis, which places Gaussian process (GP) priors on spatially varying functions to capture smooth and interpretable spatial association patterns across the brain volume. To address the significant computational challenges of GP inference in high-dimensional neuroimaging data, we employ a low-rank kernel approximation that enables projection into a reduced-dimensional subspace. This allows for efficient posterior computation without sacrificing spatial resolution, and we have developed efficient algorithms for this implemented in Python that achieve fully Bayesian inference either within minutes using the Gibbs sampler or within seconds using mean-field variational inference (VI). Through extensive simulation studies, we first show that SIMBA outperforms competing methods in estimation accuracy, activation detection sensitivity, and uncertainty quantification, especially in low signal-to-noise settings. We further demonstrate the scalability and interpretability of SIMBA in large-scale task-based fMRI applications, analyzing both volumetric and cortical surface data from the NARPS and ABCD studies."}
{"id": "2511.12065", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12065", "abs": "https://arxiv.org/abs/2511.12065", "authors": ["Congbin Xu", "Yue Yu", "Haojie Ren", "Zhaojun Wang", "Changliang Zou"], "title": "Aggregating Conformal Prediction Sets via α-Allocation", "comment": null, "summary": "Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage."}
{"id": "2511.13503", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13503", "abs": "https://arxiv.org/abs/2511.13503", "authors": ["Ioannis Diamantis"], "title": "The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business", "comment": "36 pages, 22 figures", "summary": "Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \\textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics."}
{"id": "2511.13000", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.13000", "abs": "https://arxiv.org/abs/2511.13000", "authors": ["Lingxuan Kong", "Yumin Zhang", "Chenkun Wang", "Yaoyuan Vincent Tan"], "title": "Bayesian Variable Selection on Small Sample Trial Data via Adaptive Posterior-Informed Shrinkage Prior", "comment": "19 pages, 7 figures, 3 tables", "summary": "Identifying variables associated with clinical endpoints is of much interest in clinical trials. With the rapid growth of cell and gene therapy (CGT) and therapeutics for ultra-rare diseases, there is an urgent need for statistical methods that can detect meaningful associations under severe sample-size constraints. Motivated by data-borrowing strategies for historical controls, we propose the Adaptive Posterior-Informed Shrinkage Prior (APSP), a Bayesian approach that adaptively borrows information from external sources to improve variable-selection efficiency while preserving robustness across plausible scenarios. APSP builds upon existing Bayesian data borrowing frameworks, incorporating data-driven adaptive information selection, structure of mixture shrinkage informative priors and decision making with empirical null to enhance variable selection performances under small sample size. Extensive simulations show that APSP attains better efficiency relative to traditional and popular data-borrowing and Bayesian variable-selection methods while maintaining robustness under linear relationships. We further applied APSP to identify variables associated with peak C-peptide at Day 75 from the Clinical Islet Transplantation (CIT) Consortium study CIT06 by borrowing information from the study CIT07."}
{"id": "2511.12234", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12234", "abs": "https://arxiv.org/abs/2511.12234", "authors": ["Soham Sarkar", "Arnab Hazra"], "title": "A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment", "comment": "40 pages, 3 figures, 3 tables", "summary": "Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching."}
{"id": "2511.13203", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13203", "abs": "https://arxiv.org/abs/2511.13203", "authors": ["Marco F. De Sanctis", "Eleonora Arnone", "Francesca Ieva", "Laura M. Sangalli"], "title": "Modeling group heterogeneity in spatio-temporal data via physics-informed semiparametric regression", "comment": null, "summary": "In this work we propose a novel approach for modeling spatio-temporal data characterized by group structures. In particular, we extend classical mixed effect regression models by introducing a space-time nonparametric component, regularized through a partial differential equation, to embed the physical dynamics of the underlying process, while random effects capture latent variability associated with the group structure present in the data. We propose a two-step procedure to estimate the fixed and random components of the model, relying on a functional version of the Iterative Reweighted Least Squares algorithm. We investigate the asymptotic properties of both fixed and random components, and we assess the performance of the proposed model through a simulation study, comparing it with state-of-the-art alternatives from the literature. The proposed methodology is finally applied to the study of hourly nitrogen dioxide concentration data in Lombardy (Italy), using random effects to account for measurement heterogeneity across monitoring stations equipped with different sensor technologies."}
{"id": "2511.12257", "categories": ["stat.CO", "cs.CV", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12257", "abs": "https://arxiv.org/abs/2511.12257", "authors": ["Elhadji Cisse Faye", "Mame Diarra Fall", "Nicolas Dobigeon", "Eric Barat"], "title": "Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems", "comment": null, "summary": "This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches."}
{"id": "2511.13296", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.13296", "abs": "https://arxiv.org/abs/2511.13296", "authors": ["Michail Tsagris"], "title": "Transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares", "comment": null, "summary": "Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \\cite{fiksel2022} proposed a transformation-free lienar regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions was recently proposed. To effectively estimate the regression coefficients the EM algorithm was employed. We formulate the model as a constrained logistic regression, in the spirit of \\cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. This approach makes the estimation procedure significantly faster."}
{"id": "2511.13337", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.13337", "abs": "https://arxiv.org/abs/2511.13337", "authors": ["Anna van Es", "Eva Cantoni"], "title": "Novel Tau-Informed Initialization for Maximum Likelihood Estimation of Copulas with Discrete Margins", "comment": null, "summary": "We study Gaussian-copula models with discrete margins, with primary emphasis on low-count (Poisson) data. Our goal is exact yet computationally efficient maximum likelihood (ML) estimation in regimes where many observations contain small counts, which imperils both identifiability and numerical stability. We develop three novel Kendall's tau-based approaches for initialization tailored to discrete margins in the low-count regime and embed it within an inference functions for margins (IFM) inspired start. We present three practical initializers (exact, low-intensity approximation, and a transformation-based approach) that substantially reduce the number of ML iterations and improve convergence. For the ML stage, we use an unconstrained reparameterization of the model's parameters using the log and spherical-Cholesky and compute exact rectangle probabilities. Analytical score functions are supplied throughout to stabilize Newton-type optimization. A simulation study across dimensions, dependence levels, and intensity regimes shows that the proposed initialization combined with exact ML achieves lower root-mean-squared error, lower bias and faster computation times than the alternative procedures. The methodology provides a pragmatic path to retain the statistical guarantees of ML (consistency, asymptotic normality, efficiency under correct specification) while remaining tractable for moderate- to high-dimensional discrete data. We conclude with guidance on initializer choice and discuss extensions to alternative correlation structures and different margins."}
{"id": "2511.13608", "categories": ["stat.ME", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.13608", "abs": "https://arxiv.org/abs/2511.13608", "authors": ["M. Stocker", "W. Małgorzewicz", "M. Fontana", "S. Ben Taieb"], "title": "A Gentle Introduction to Conformal Time Series Forecasting", "comment": null, "summary": "Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions."}
{"id": "2511.13624", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.13624", "abs": "https://arxiv.org/abs/2511.13624", "authors": ["Rajesh Karmakar", "Ruth Heller", "Saharon Rosset"], "title": "The Bottom-Up Approach for Powerful Testing with FWER Control", "comment": null, "summary": "We seek to design novel multiple testing procedures, which take into account a relevant notion of ''power'' or true discovery on the one hand, and allow computationally efficient test design and application on the other. Towards this end we characterize the optimal procedures that strongly control the family-wise error rate, for a range of power objectives measuring the success of multiple testing procedures in making true individual discoveries, and under a reasonable set of assumptions. While we cannot generally find these optimal solutions in practice, we propose the bottom-up approach, which constructs consonant closed testing procedures, while taking into account the overall power objective in designing the tests on every level of the closed testing hierarchy. This leads to a general recipe, yielding novel procedures which are computationally practical and demonstrate substantially improved power in both simulations and a real data study, compared to existing procedures."}
{"id": "2511.13694", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.13694", "abs": "https://arxiv.org/abs/2511.13694", "authors": ["Jay Bartroff", "Asmit Chakraborty"], "title": "Shortest fixed-width confidence intervals for a bounded parameter: The Push algorithm", "comment": null, "summary": "We present a method for computing optimal fixed-width confidence intervals for a single, bounded parameter, extending a method for the binomial due to Asparaouhov and Lorden, who called it the Push algorithm. The method produces the shortest possible non-decreasing confidence interval for a given confidence level, and if the Push interval does not exist for a given width and level, then no such interval exists. The method applies to any bounded parameter that is discrete, or is continuous and has the monotone likelihood ratio property. We demonstrate the method on the binomial, hypergeometric, and normal distributions with our available R package. In each of these distributions the proposed method outperforms the standard ones, and in the latter case even improves upon the $z$-interval. We apply the proposed method to World Health Organization (WHO) data on tobacco use."}
{"id": "2511.12242", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12242", "abs": "https://arxiv.org/abs/2511.12242", "authors": ["Zhuoran Yu", "Armin Schwartzman", "Junting Ren", "Julia Wrobel"], "title": "SCoRES: An R Package for Simultaneous Confidence Region Estimates", "comment": "36 pages, 18 figures", "summary": "The identification of domain sets whose outcomes belong to predefined subsets can address fundamental risk assessment challenges in climatology and medicine. Existing approaches for inverse domain estimates require restrictive assumptions, including domain density and continuity of function near thresholds, and large-sample guarantees, which limit the applicability. Besides, the estimation and coverage depend on setting a fixed threshold level, which is difficult to determine. Recently, Ren et al. (2024) proved that confidence sets of multiple levels can be simultaneously constructed with the desired confidence non-asymptotically through inverting simultaneous confidence bands. Here, we present the SCoRES R package, which implements Ren's approach for both the estimation of the inverse region and the corresponding simultaneous outer and inner confidence regions, along with visualization tools. Besides, the package also provides functions that help construct SCBs for regression data, functional data and geographical data. To illustrate its broad applicability, we present three rigorous examples that demonstrate the SCoRES workflow."}
{"id": "2511.12703", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12703", "abs": "https://arxiv.org/abs/2511.12703", "authors": ["Qingqing Song", "Shaoliang Xia"], "title": "Change-Point Detection Utilizing Normalized Entropy as a Fundamental Metric", "comment": "4 pages, 2 figures, 2 tables, accepted to the 17th International Conference on Pattern Recognition and Information Processing (PRIP 2025)", "summary": "This paper introduces a concept for change-point detection based on normalized entropy as a fundamental metric, aiming to overcome the dependence of traditional entropy methods on assumptions about data distribution and absolute scales. Normalized entropy maps entropy values to the [0,1] interval through standardization, accurately capturing relative changes in data complexity. By utilizing a sliding window to compute normalized entropy, this approach transforms the challenge of detecting change points in complex time series, arising from variations in scale, distribution, and diversity, into the task of identifying significant features within the normalized entropy sequence, thereby avoiding interference from parametric assumptions and effectively highlighting distributional shifts. Experimental results show that normalized entropy exhibits significant numerical fluctuation characteristics and patterns near change points across various distributions and parameter combinations. The average deviation between fluctuation moments and actual change points is only 2.4% of the sliding window size, demonstrating strong adaptability. This paper provides theoretical support for change-point detection in complex data environments and lays a methodological foundation for precise and automated detection based on normalized entropy as a fundamental metric."}
