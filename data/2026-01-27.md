<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 15]
- [stat.ML](#stat.ML) [Total: 3]
- [stat.AP](#stat.AP) [Total: 4]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Faster parallel MCMC: Metropolis adjustment is best served warm](https://arxiv.org/abs/2601.16696)
*Jakob Robnik,Uroš Seljak*

Main category: stat.CO

TL;DR: LAPS是一种并行MCMC采样方法，通过先运行无调整阶段快速收敛到目标分布附近，再开启调整阶段实现精细收敛，显著提升采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统并行MCMC在冷启动时收敛缓慢，理论上可以通过先运行无Metropolis-Hastings调整的MCMC快速收敛到目标分布附近，再开启调整实现精细收敛，但缺乏实用的自动步长选择方案。

Method: LAPS采用基于集成的超参数自适应方法，在每次迭代中估计偏差并转换为合适的步长，实现自动化的无调整阶段步长选择，所有超参数自动选择。

Result: LAPS在多种标准基准问题上一致且显著优于MEADS、ChESS等集成调整方法和Pathfinder优化初始化器，通常比NUTS等顺序算法实现两个数量级更低的时钟时间。

Conclusion: LAPS提供了一种实用有效的并行MCMC采样方案，解决了冷启动收敛慢的问题，显著提升了采样效率，适用于科学计算应用。

Abstract: Despite the enormous success of Hamiltonian Monte Carlo and related Markov Chain Monte Carlo (MCMC) methods, sampling often still represents the computational bottleneck in scientific applications. Availability of parallel resources can significantly speed up MCMC inference by running a large number of chains in parallel, each collecting a single sample. However, the parallel approach converges slowly if the chains are not initialized close to the target distribution (cold start). Theoretically this can be resolved by initially running MCMC without Metropolis-Hastings adjustment to quickly converge to the vicinity of the target distribution and then turn on adjustment to achieve fine convergence. However, no practical scheme uses this strategy, due to the difficulty of automatically selecting the step size during the unadjusted phase. We here develop Late Adjusted Parallel Sampler (LAPS), which is precisely such a scheme and is applicable out of the box, all the hyperparameters are selected automatically. LAPS takes advantage of ensemble-based hyperparameter adaptation to estimate the bias at each iteration and converts it to the appropriate step size. We show that LAPS consistently and significantly outperforms ensemble adjusted methods such as MEADS or ChESS and the optimization-based initializer Pathfinder on a variety of standard benchmark problems. LAPS typically achieves two orders of magnitude lower wall-clock time than the corresponding sequential algorithms such as NUTS.

</details>


### [2] [A Fully Automated DM-BIM-BEM Pipeline Enabling Graph-Based Intelligence, Interoperability, and Performance-Driven Early Design](https://arxiv.org/abs/2601.16813)
*Jun Xiao,Qiong Wang,Yihui Li,Zhexuan Yu,Hao Zhou,Borong Lin*

Main category: stat.CO

TL;DR: 提出一个自动化框架，将无结构的B-rep几何模型转换为知识图谱BIM和可执行的建筑能耗模型，为AI提供结构化基础设施


<details>
  <summary>Details</summary>
Motivation: 早期建筑设计主要使用灵活的B-rep模型，缺乏明确的空间、语义和性能结构，限制了AI在建筑领域的应用，需要将无结构几何转换为结构化表示

Method: 集成自动几何清理、多种自动空间生成策略、基于图的空间和元素拓扑提取、本体对齐的知识建模、以及基于本体的BIM与EnergyPlus能耗模型之间的可逆转换

Result: 在参数化、草图式和真实建筑数据集上验证，展示了高鲁棒性、一致的拓扑重建和可靠的性能模型生成

Conclusion: 该框架通过连接设计模型、BIM和BEM，为AI提供了基础设施，将基于BIM和图表的智能管道扩展到灵活的早期设计几何，支持基于学习的性能驱动设计探索和优化

Abstract: Artificial intelligence in construction increasingly depends on structured representations such as Building Information Models and knowledge graphs, yet early-stage building designs are predominantly created as flexible boundary-representation (B-rep) models that lack explicit spatial, semantic, and performance structure. This paper presents a robust, fully automated framework that transforms unstructured B-rep geometry into knowledge-graph-based Building Information Models and further into executable Building Energy Models. The framework enables artificial intelligence to explicitly interpret building elements, spatial topology, and their associated thermal and performance attributes. It integrates automated geometry cleansing, multiple auto space-generation strategies, graph-based extraction of space and element topology, ontology-aligned knowledge modeling, and reversible transformation between ontology-based BIM and EnergyPlus energy models. Validation on parametric, sketch-based, and real-world building datasets demonstrates high robustness, consistent topological reconstruction, and reliable performance-model generation. By bridging design models, BIM, and BEM, the framework provides an AI-oriented infrastructure that extends BIM- and graph-based intelligence pipelines to flexible early-stage design geometry, enabling performance-driven design exploration and optimization by learning-based methods.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [Orthogonal factorial designs for trials of therapist-delivered interventions: Randomising intervention-therapist combinations to patients](https://arxiv.org/abs/2601.16318)
*Rebecca EA Walwyn,Rosemary A Bailey,Arpan Singh,Neil Corrigan,Steven G Gilmour*

Main category: stat.ME

TL;DR: 论文提出将治疗师和干预视为两个可能交互的治疗因素（一个固定，一个随机），采用正交因子设计来分离治疗师效应和干预效应，并通过模拟研究验证方法性能。


<details>
  <summary>Details</summary>
Motivation: 在评估心理治疗等治疗师提供的干预的个体随机平行组试验中，需要考虑到治疗师相关的聚类效应。传统方法将干预视为治疗因素而治疗师不是，但若要分离治疗师效应和干预效应，需要新的设计方法。

Method: 采用经典的实验设计方法，提出一系列正交因子设计及其相关数据分析方法。具体设计为交叉治疗师-干预设计，每个治疗师提供每种干预，治疗师-干预组合随机分配给患者。使用ANOVA和回归进行数据分析，并通过模拟研究评估方法性能。

Result: 模拟研究探索了所提出的随机化方法在估计干预效应及其标准误、治疗师间方差以及干预效应中治疗师间方差方面的性能。结果表明方法能够有效分离治疗师效应和干预效应。

Conclusion: 更有目的性的试验设计有可能为一系列复杂干预提供更好的证据，并为进一步的方法学研究指明了方向。

Abstract: It is recognised that treatment-related clustering should be allowed for in the sample size and analyses of individually-randomised parallel-group trials that evaluate therapist-delivered interventions such as psychotherapy. Here, interventions are a treatment factor, but therapists are not. If the aim of a trial is to separate effects of therapists from those of interventions, we propose that interventions and therapists should be regarded as two potentially interacting treatment factors (one fixed, one random) with a factorial structure. We consider the specific design where each therapist delivers each intervention (crossed therapist-intervention design), and the resulting therapist-intervention combinations are randomised to patients. We adopt a classical Design of Experiments (DoE) approach to propose a family of orthogonal factorial designs and their associated data analyses, which allow for therapist learning and centre too. We set out the associated data analyses using ANOVA and regression and report the results of a small simulation study conducted to explore the performance of the proposed randomisation methods in estimating the intervention effect and its standard error, the between-therapist variance and the between-therapist variance in the intervention effect. We conclude that more purposeful trial design has the potential to lead to better evidence on a range of complex interventions and outline areas for further methodological research.

</details>


### [4] [Hybrid Partial Least Squares Regression with Multiple Functional and Scalar Predictors](https://arxiv.org/abs/2601.16364)
*Jongmin Mun,Jeong Hoon Jang*

Main category: stat.ME

TL;DR: 提出Hybrid PLS方法，用于处理功能型和标量型预测变量的联合降维与回归，特别适用于多模态相关数据


<details>
  <summary>Details</summary>
Motivation: 肾成像研究中需要同时处理肾图曲线（功能型数据）与药代动力学、人口统计学协变量（标量型数据），且存在跨模态相关性，需要统一框架

Method: 将多种功能型和标量型预测变量嵌入统一的混合希尔伯特空间，扩展NIPALS算法，加入粗糙度惩罚控制平滑度，利用秩一结构获得高效闭式解

Result: 建立了框架的基本几何性质（潜在得分和PLS方向的正交性），在合成数据和肾成像研究中验证了方法在模态间多重共线性下恢复预测结构的能力

Conclusion: Hybrid PLS能有效处理多模态相关数据，提供简洁的低维表示，在肾成像等跨模态研究中具有实用价值

Abstract: Motivated by renal imaging studies that combine renogram curves with pharmacokinetic and demographic covariates, we propose Hybrid partial least squares (Hybrid PLS) for simultaneous supervised dimension reduction and regression in the presence of cross-modality correlations. The proposed approach embeds multiple functional and scalar predictors into a unified hybrid Hilbert space and rigorously extends the nonlinear iterative PLS (NIPALS) algorithm. This theoretical development is complemented by a sample-level algorithm that incorporates roughness penalties to control smoothness. By exploiting the rank-one structure of the resulting optimization problem, the algorithm admits a computationally efficient closed-form solution that requires solving only linear systems at each iteration. We establish fundamental geometric properties of the proposed framework, including orthogonality of the latent scores and PLS directions. Extensive numerical studies on synthetic data, together with an application to a renal imaging study, validate these theoretical results and demonstrate the method's ability to recover predictive structure under intermodal multicollinearity, yielding parsimonious low-dimensional representations.

</details>


### [5] [Inference for competing risks based on area between curves statistics](https://arxiv.org/abs/2601.16368)
*Simon Mack,Marc Ditzhaus,Merle Munko,Markus Pauly*

Main category: stat.ME

TL;DR: 提出一种基于累积发生率函数间面积的新检验统计量，用于处理竞争风险模型中函数交叉的情况，并通过wild bootstrap方法实现渐近有效的两样本检验。


<details>
  <summary>Details</summary>
Motivation: 在竞争风险模型中，通常通过比较累积发生率函数来推断组间差异。然而，当这些函数在感兴趣的时间范围内交叉时，许多现有的推断方法效果不佳。需要解决函数交叉情况下的检验问题。

Method: 提出基于累积发生率函数间面积的检验统计量。由于相应的极限分布依赖于通常未知的量，采用wild bootstrap方法获得可行且渐近有效的两样本检验。

Result: 通过广泛的模拟研究评估了所提方法的有限样本性能，并与现有方法进行了比较。

Conclusion: 该方法为竞争风险模型中累积发生率函数交叉情况下的组间比较提供了有效的检验工具，特别是在传统方法失效的函数交叉场景中表现良好。

Abstract: In competing risks models, cumulative incidence functions are commonly compared to infer differences between groups. Many existing inference methods, however, struggle when these functions cross during the time frame of interest. To address this problem, we investigate a test statistic based on the area between cumulative incidence functions. As the corresponding limiting distribution depends on quantities that are typically unknown, we propose a wild bootstrap approach to obtain a feasible and asymptotically valid two-sample test. The finite sample performance of the proposed method, in comparison with existing methods, is examined in an extensive simulation study.

</details>


### [6] [Spherical Spatial Autoregressive Model for Spherically Embedded Spatial Data](https://arxiv.org/abs/2601.16385)
*Jiazhen Xu,Han Lin Shang*

Main category: stat.ME

TL;DR: 提出一个统一框架处理球面空间数据的建模、推断和不确定性量化，包括球面空间自回归模型、估计量渐近性质检验以及保形预测方法。


<details>
  <summary>Details</summary>
Motivation: 球面嵌入空间数据在多个领域日益普遍，但由于球面的非欧几里得特性，现有统计方法有限，需要专门框架来处理建模、推断和不确定性量化问题。

Method: 1) 提出基于最优传输几何的球面空间自回归模型，可扩展包含外生协变量；2) 建立估计量渐近性质，提出无分布Wald检验和bootstrap方法；3) 开发针对球面空间数据的保形预测方法进行不确定性量化。

Result: 通过模拟研究和实际应用（西班牙地球化学组成和日本死亡年龄分布）验证了方法的实用性，展示了框架在球面空间数据分析中的有效性。

Conclusion: 该研究为球面嵌入空间数据提供了一个全面的统计框架，解决了建模、推断和不确定性量化三个关键挑战，为相关领域的数据分析提供了有力工具。

Abstract: Spherically embedded spatial data are spatially indexed observations whose values naturally reside on or can be equivalently mapped to the unit sphere. Such data are increasingly ubiquitous in fields ranging from geochemistry to demography. However, analysing such data presents unique difficulties due to the intrinsic non-Euclidean nature of the sphere, and rigorous methodologies for statistical modelling, inference, and uncertainty quantification remain limited. This paper introduces a unified framework to address these three limitations for spherically embedded spatial data. We first propose a novel spherical spatial autoregressive model that leverages optimal transport geometry and then extend it to accommodate exogenous covariates. Second, for either scenario with or without covariates, we establish the asymptotic properties of the estimators and derive a distribution-free Wald test for spatial dependence, complemented by a bootstrap procedure to enhance finite-sample performance. Third, we contribute a novel approach to uncertainty quantification by developing a conformal prediction procedure specifically tailored to spherically embedded spatial data. The practical utility of these methodological advances is illustrated through extensive simulations and applications to Spanish geochemical compositions and Japanese age-at-death mortality distributions.

</details>


### [7] [Sequential Experimental Designs for Kriging Model](https://arxiv.org/abs/2601.16431)
*Ruonan Zheng,Min-Qian Liu,Yongdao Zhou,Xuan Chen*

Main category: stat.ME

TL;DR: 提出两种新的单点序贯设计准则和通用批量序贯设计框架，解决现有序贯设计方法在批量场景下的局限性，提高Kriging模型的拟合精度。


<details>
  <summary>Details</summary>
Motivation: 计算机实验已成为复杂物理和工程实验的重要替代方案，Kriging是最广泛使用的代理模型。现有序贯设计方法存在关键限制：基于观测的批量序贯设计研究很少，而单点序贯设计信息利用不足且资源效率低下，需要多次重复观测轮次积累足够点，导致实验周期延长。

Method: 提出两种新的单点序贯设计准则和一个通用的批量序贯设计框架。批量序贯设计框架解决了朴素批量选择中固有的点聚类问题，能够将任何序贯准则高效扩展到批量场景。

Result: 在一些测试函数上的仿真表明，所提出的方法在大多数情况下在拟合精度方面优于现有方法。

Conclusion: 提出的方法有效解决了现有序贯设计方法的局限性，特别是批量场景下的点聚类问题，提高了Kriging模型的拟合精度和实验效率。

Abstract: Computer experiments have become an indispensable alternative to complex physical and engineering experiments. The Kriging model is the most widely used surrogate model, with the core goal of minimizing the discrepancy between the surrogate and true models across the entire experimental domain. However, existing sequential design methods have critical limitations: observation-based batch sequential designs are rarely studied, while one-point sequential designs have insufficient information utilization and suffer from inefficient resource utilization -- they require numerous repeated observation rounds to accumulate sufficient points, leading to prolonged experimental cycles. To address these gaps, this paper proposes two novel one-point sequential design criteria and a general batch sequential design framework. Moreover, the batch sequential design framework solves the inherent point clustering problem in naive batch selection, enabling efficient extension of any sequential criterion to batch scenarios. Simulations on some test functions demonstrate that the proposed methods outperform existing approaches in terms of fitting accuracy in most cases.

</details>


### [8] [Variational Dimension Lifting for Robust Tracking of Nonlinear Stochastic Dynamics](https://arxiv.org/abs/2601.16470)
*Yonatan L. Ashenafi*

Main category: stat.ME

TL;DR: 提出一个框架，通过可逆变换将非线性状态空间模型映射到高维线性高斯SSM，从而应用标准线性高斯推理技术


<details>
  <summary>Details</summary>
Motivation: 非线性随机运动对贝叶斯粒子跟踪带来重大挑战，需要解决传统滤波器在结构不稳定性方面的问题

Method: 构建可逆变换将非线性SSM映射到高维线性高斯SSM，使用伊藤引理和变分法推导变换的必要条件

Result: 在双稳态立方运动模型、径向布朗过程模型和乘性噪声逻辑模型上验证，变换后的线性系统能准确重建非线性动力学，在刚性和奇异性不同机制下跟踪精度与传统滤波器相当

Conclusion: 该方法通过线性化变换有效处理非线性随机运动跟踪问题，避免了传统滤波器的结构不稳定性，在多种非线性模型中表现良好

Abstract: Nonlinear stochastic motion presents significant challenges for Bayesian particle tracking. To address this challenge, this paper proposes a framework to construct an invertible transformation that maps the nonlinear state-space model (SSM) into a higher-dimensional linear Gaussian SSM. This approach allows the application of standard linear-Gaussian inference techniques while maintaining a connection to the dynamics of the original system. The paper derives the necessary conditions for such transformations using Ito's lemma and variational calculus, and illustrates the method on a bistable cubic motion model, radial Brownian process model, and a logistic model with multiplicative noise. Simulations confirm that the transformed linear systems, when projected back, accurately reconstruct the nonlinear dynamics and, in distinct regimes of stiffness and singularity, yield tracking accuracy competitive with conventional filters, while avoiding their structural instabilities.

</details>


### [9] [Variational approximate penalized credible regions for Bayesian grouped regression](https://arxiv.org/abs/2601.16585)
*Weichang Yu,Khue-Dung Dang*

Main category: stat.ME

TL;DR: 提出了一种快速准确的贝叶斯高维线性回归分组惩罚可信区域方法，用于变量选择和预测，解决了现有方法计算成本高或变量选择结果模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯方法存在两个主要问题：1）由于需要长时间运行MCMC导致计算成本高；2）由于输出非稀疏解导致变量选择结果模糊。需要一种既能提供明确分组变量选择，又具有计算可扩展性的方法。

Method: 采用分组惩罚可信区域框架，使用分组全局-局部收缩先验来压缩不重要组的噪声，通过坐标上升变分推断近似后验摘要，并将惩罚可信区域框架重新表述为凸优化问题以实现高效计算。

Result: 理论证明后处理估计器在高维设置下具有参数一致性和变量选择一致性。通过大量模拟实验表明，该方法在分组变量选择、预测和计算时间方面优于现有方法，适用于ANOVA和非参数变系数模型。

Conclusion: 提出的分组惩罚可信区域方法能够同时实现快速计算、准确预测和明确的变量选择，解决了贝叶斯高维回归中的关键挑战，为实际应用提供了实用工具。

Abstract: We develop a fast and accurate grouped penalized credible region approach for variable selection and prediction in Bayesian high-dimensional linear regression. Most existing Bayesian methods either are subject to high computational costs due to long Markov Chain Monte Carlo runs or yield ambiguous variable selection results due to non-sparse solution output. The penalized credible region framework yields sparse post-processed estimates that facilitates unambiguous grouped variable selection. High estimation accuracy is achieved by shrinking noise from unimportant groups using a grouped global-local shrinkage prior. To ensure computational scalability, we approximate posterior summaries using coordinate ascent variational inference and recast the penalized credible region framework as a convex optimization problem that admits efficient computations. We prove that the resultant post-processed estimators are both parameter-consistent and variable selection consistent in high-dimensional settings. Theory is developed to justify running the coordinate ascent algorithm for at least two cycles. Through extensive simulations, we demonstrate that our proposed method outperforms state-of-the-art methods in grouped variable selection, prediction, and computation time for several common models including ANOVA and nonparametric varying coefficient models.

</details>


### [10] [Bayesian Nonparametric Causal Inference for High-Dimensional Nutritional Data via Factor-Based Exposure Mapping](https://arxiv.org/abs/2601.16595)
*Dafne Zorzetto,Zizhao Xie,Julian Stamp,Arman Oganisian,Roberta De Vito*

Main category: stat.ME

TL;DR: 提出一种结合因子分析和贝叶斯因果森林的方法，用于估计高维饮食数据的因果效应，识别潜在饮食模式及其对健康结果的异质性影响


<details>
  <summary>Details</summary>
Motivation: 营养流行病学中的因果推断面临三大挑战：高维相关饮食数据导致大量处理水平；需要识别潜在饮食模式而非单一食物；需要估计这些饮食模式对健康结果的异质性因果效应

Method: 引入暴露映射框架，通过因子分析降维识别饮食模式；扩展贝叶斯因果森林以处理三个有序水平的饮食暴露，捕捉营养数据的复杂结构并估计异质性因果效应

Result: 通过模拟实验验证方法有效性；应用于美国西班牙裔/拉丁裔成人多中心研究，识别出六种饮食模式，发现植物脂质抗氧化、植物性、动物蛋白和乳制品模式与降低BMI和空腹胰岛素水平风险相关

Conclusion: 提出的方法成功解决了营养流行病学中的因果推断挑战，能够识别潜在饮食模式并估计其异质性健康效应，为公共卫生政策和个性化营养策略提供科学依据

Abstract: Diet plays a crucial role in health, and understanding the causal effects of dietary patterns is essential for informing public health policy and personalized nutrition strategies. However, causal inference in nutritional epidemiology faces several challenges: (i) high-dimensional and correlated food/nutrient intake data induce massive treatment levels; (ii) nutritional studies are interested in latent dietary patterns rather than single food items; and (iii) the goal is to estimate heterogeneous causal effects of these dietary patterns on health outcomes. We address these challenges by introducing a sophisticated exposure mapping framework that reduces the high-dimensional treatment space via factor analysis and enables the identification of dietary patterns. We also extend the Bayesian Causal Forest to accommodate three ordered levels of dietary exposure, better capturing the complex structure of nutritional data and enabling estimation of heterogeneous causal effects. We evaluate the proposed method through extensive simulations and apply it to a multi-center epidemiological study of Hispanic/Latino adults residing in the US. Using high-dimensional dietary data, we identify six dietary patterns and estimate their causal link with two key health risk factors: body mass index and fasting insulin levels. Our findings suggest that higher consumption of plant lipid-antioxidant, plant-based, animal protein, and dairy product patterns is associated with reduced risk.

</details>


### [11] [Conformal prediction for full and sparse polynomial chaos expansions](https://arxiv.org/abs/2601.16636)
*A. Hatstatt,X. Zhu,B. Sudret*

Main category: stat.ME

TL;DR: 将保形预测方法（全保形和Jackknife+）整合到多项式混沌展开（PCE）代理模型中，为小训练数据集提供具有统计保证的局部预测区间，优于传统bootstrap方法。


<details>
  <summary>Details</summary>
Motivation: 虽然PCE在代理建模中计算效率高，但缺乏量化局部模型误差的稳健框架。传统bootstrap方法在小型训练数据集上统计保证不足，需要更严格的统计方法来提供可靠的预测区间。

Method: 将两种保形预测方法（全保形和Jackknife+）整合到完整和稀疏PCE中：对于完整PCE，利用回归方法固有结构设计计算捷径；对于稀疏PCE，调整推理策略以处理回归算法的非对称性，确保有效的预测区间。

Result: 开发的方法为完整和稀疏PCE提供了更好的校准预测区间，相比现有方法（如bootstrap）实现了更优的覆盖率，同时保持了适中的计算成本。

Conclusion: 保形预测方法成功应用于PCE代理模型，为小训练数据集提供了具有统计保证的预测区间，优于传统bootstrap方法，在保持计算效率的同时提高了预测可靠性。

Abstract: Polynomial Chaos Expansions (PCEs) are widely recognized for their efficient computational performance in surrogate modeling. Yet, a robust framework to quantify local model errors is still lacking. While the local uncertainty of PCE prediction can be captured using bootstrap resampling, other methods offering more rigorous statistical guarantees are needed, especially in the context of small training datasets. Recently, conformal predictions have demonstrated strong potential in machine learning, providing statistically robust and model-agnostic prediction intervals. Due to its generality and versatility, conformal prediction is especially valuable, as it can be adapted to suit a variety of problems, making it a compelling choice for PCE-based surrogate models. In this contribution, we explore its application to PCE-based surrogate models. More precisely, we present the integration of two conformal prediction methods, namely the full conformal and the Jackknife+ approaches, into both full and sparse PCEs. For full PCEs, we introduce computational shortcuts inspired by the inherent structure of regression methods to optimize the implementation of both conformal methods. For sparse PCEs, we incorporate the two approaches with appropriate modifications to the inference strategy, thereby circumventing the non-symmetrical nature of the regression algorithm and ensuring valid prediction intervals. Our developments yield better-calibrated prediction intervals for both full and sparse PCEs, achieving superior coverage over existing approaches, such as the bootstrap, while maintaining a moderate computational cost.

</details>


### [12] [Comments on "Challenges of cellwise outliers" by Jakob Raymaekers and Peter J. Rousseeuw](https://arxiv.org/abs/2601.16739)
*Claudio Agostinelli*

Main category: stat.ME

TL;DR: 该论文综述了鲁棒统计中处理单元异常值（cellwise outliers）的最新进展，特别关注时间序列中的异常值处理。


<details>
  <summary>Details</summary>
Motivation: 鲁棒统计的主要目标是开发能够处理异常值的方法。近年来，一种新型的"单元异常值"引起了广泛关注，特别是在时间序列分析中，传统方法可能无法有效处理这类异常值。

Method: 论文主要采用文献综述方法，系统总结了Raymaekers和Rousseeuw（2024）提出的处理单元异常值在不同模型中的最新技术，并特别讨论了时间序列中异常值作为单元异常值的处理方法。

Result: 论文呈现了处理单元异常值的最新技术现状，并展示了如何将时间序列中的异常值视为单元异常值进行处理，为这一领域提供了系统性的理论框架。

Conclusion: 单元异常值处理是鲁棒统计的重要发展方向，特别是在时间序列分析中具有重要应用价值。Raymaekers和Rousseeuw（2024）的工作为这一领域提供了重要参考，未来需要进一步研究更有效的单元异常值检测和处理方法。

Abstract: The main aim of robust statistics is the development of methods able to cope with the presence of outliers. A new type of outliers, namely "cellwise", has garnered considerable attention. The state of the art for dealing with cellwise contamination in different models is presented in Raymaekers and Rousseeuw (2024). Outliers in time series can be treated as cellwise outliers, a further discussion on this subject is presented.

</details>


### [13] [From Noisy News Sentiment Scores to Interpretable Temporal Dynamics: A Bayesian State-Space Model](https://arxiv.org/abs/2601.16769)
*Ian Carbó Casals*

Main category: stat.ME

TL;DR: 提出贝叶斯状态空间框架，将聚合新闻情感转化为带不确定性的平滑时间序列，通过信息权重处理数据可用性变化


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的情感指标在周度层面存在噪声，主要原因是相关新闻数量随时间变化，导致某些周度平均值基于大量文章而其他仅基于少量文章，现有方法未明确考虑数据可用性变化来测量不确定性

Method: 提出贝叶斯状态空间框架，将每周情感值视为潜在情感过程的噪声测量，观测不确定性通过有效信息权重n_tj进行缩放：当报道量高时，潜在情感更紧密地锚定在观测聚合值上；当报道量低时，推断更依赖潜在动态且不确定性增加

Result: 使用按类别分组的新闻数据，发现不同类别间潜在动态大致相似，而观测噪声存在较大差异

Conclusion: 该框架适用于描述性监测，可扩展到其他信息可用性随时间变化的文本来源

Abstract: Text-based sentiment indicators are widely used to monitor public and market mood, but weekly sentiment series are noisy by construction. A main reason is that the amount of relevant news changes over time and across categories. As a result, some weekly averages are based on many articles, while others rely on only a few. Existing approaches do not explicitly account for changes in data availability when measuring uncertainty. We present a Bayesian state-space framework that turns aggregated news sentiment into a smoothed time series with uncertainty. The model treats each weekly sentiment value as a noisy measurement of an underlying sentiment process, with observation uncertainty scaled by the effective information weight $n_{tj}$: when coverage is high, latent sentiment is anchored more strongly to the observed aggregate; when coverage is low, inference relies more on the latent dynamics and uncertainty increases. Using news data grouped into multiple categories, we find broadly similar latent dynamics across categories, while larger differences appear in observation noise. The framework is designed for descriptive monitoring and can be extended to other text sources where information availability varies over time.

</details>


### [14] [Finite Population Inference for Factorial Designs and Panel Experiments with Imperfect Compliance](https://arxiv.org/abs/2601.16749)
*Pedro Picchetti*

Main category: stat.ME

TL;DR: 该论文为不完全依从情境下的因果效应分析建立了一个有限总体框架，适用于多处理影响结果的情况，如因子设计和面板实验，并提供了非参数估计量及其有限总体分布。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，当存在不完全依从且多个处理影响结果时（如因子设计和面板实验），需要开发能够捕捉不同处理序列相对有效性的因果效应框架。现有方法可能无法充分处理这些复杂情境。

Method: 开发了一个有限总体框架，定义了捕捉替代处理序列相对有效性的有限总体因果效应。提供了因子和动态因果效应的非参数估计量，并推导了样本量增加时的有限总体分布。

Result: 蒙特卡洛模拟显示了估计量的良好性质。应用因子设计中的因果效应估计量重新分析了一个著名的选民动员实验，研究了电话鼓励对投票率的影响。

Conclusion: 该研究为不完全依从下的多处理因果效应分析提供了理论框架和实用估计方法，能够有效处理因子设计和动态实验中的复杂因果推断问题。

Abstract: This paper develops a finite population framework for analyzing causal effects in settings with imperfect compliance where multiple treatments affect the outcome of interest. Two prominent examples are factorial designs and panel experiments with imperfect compliance. I define finite population causal effects that capture the relative effectiveness of alternative treatment sequences. I provide nonparametric estimators for a rich class of factorial and dynamic causal effects and derive their finite population distributions as the sample size increases. Monte Carlo simulations illustrate the desirable properties of the estimators. Finally, I use the estimator for causal effects in factorial designs to revisit a famous voter mobilization experiment that analyzes the effects of voting encouragement through phone calls on turnout.

</details>


### [15] [Directional-Shift Dirichlet ARMA Models for Compositional Time Series with Structural Break Intervention](https://arxiv.org/abs/2601.16821)
*Harrison Katz*

Main category: stat.ME

TL;DR: 提出贝叶斯Dirichlet ARMA模型，加入方向性转移干预机制，用于处理成分时间序列中的结构性断点，通过方向向量、幅度和逻辑门三个可解释参数捕捉结构变化。


<details>
  <summary>Details</summary>
Motivation: 成分时间序列（比例向量）常因外部冲击、政策变化或市场中断而出现结构性断点。现有方法要么忽略这些断点，要么通过无法外推的临时虚拟变量处理，缺乏系统性方法。

Method: 开发贝叶斯Dirichlet ARMA模型，加入方向性转移干预机制，包含三个参数：单位方向向量（指定哪些成分获得或失去份额）、幅度（控制重新分配的规模）、逻辑门（控制转移的时间和速度）。模型保持成分约束，维持DARMA动态，并产生连贯的概率预测。

Result: 模拟研究（8种场景400次拟合）显示：当正确识别转移方向时（77.5%情况），幅度和时间参数恢复偏差接近零，平均成分的置信区间达到80%名义覆盖率。实证应用于COVID-19期间费用确认前置时间分布，干预模型在滚动预测评估中表现出更优的点准确性（Aitchison距离0.83 vs. 0.90）和校准（87% vs. 71%覆盖率）。

Conclusion: 提出的贝叶斯Dirichlet ARMA模型能有效捕捉和处理成分时间序列中的结构性断点，提供可解释的参数和连贯的预测，在模拟和实证应用中均表现优异。

Abstract: Compositional time series, vectors of proportions summing to unity observed over time, frequently exhibit structural breaks due to external shocks, policy changes, or market disruptions. Standard methods either ignore such breaks or handle them through ad-hoc dummy variables that cannot extrapolate beyond the estimation sample. We develop a Bayesian Dirichlet ARMA model augmented with a directional-shift intervention mechanism that captures structural breaks through three interpretable parameters: a unit direction vector specifying which components gain or lose share, an amplitude controlling the magnitude of redistribution, and a logistic gate governing the timing and speed of transition. The model preserves compositional constraints by construction, maintains innovation-form DARMA dynamics for short-run dependence, and produces coherent probabilistic forecasts during and after structural breaks. We establish that the directional shift corresponds to geodesic motion on the simplex and is invariant to the choice of ILR basis. A comprehensive simulation study with 400 fits across 8 scenarios demonstrates that when the shift direction is correctly identified (77.5% of cases), amplitude and timing parameters are recovered with near-zero bias, and credible intervals for the mean composition achieve nominal 80% coverage; we address the sign identification challenge through a hemisphere constraint. An empirical application to fee recognition lead-time distributions during COVID-19 compares baseline, fixed-effects, and intervention specifications in rolling forecast evaluation, demonstrating the intervention model's superior point accuracy (Aitchison distance 0.83 vs. 0.90) and calibration (87% vs. 71% coverage) during structural transitions.

</details>


### [16] [Directional Asymmetry in Edge BasedSpatial Models via a Skew Normal Prior](https://arxiv.org/abs/2601.16829)
*Danna L. Cruz-Reyes,Renato M. Assunção,Reinaldo B. Arellano-Valle,Rosangela H. Loschi*

Main category: stat.ME

TL;DR: 提出RENeGe-sk模型，在Gaussian RENeGe框架基础上引入偏斜边缘空间先验，通过偏态正态分布捕捉方向性不对称结构


<details>
  <summary>Details</summary>
Motivation: 传统高斯空间先验无法有效捕捉空间过程中的方向性不对称结构，需要开发能够识别边缘对齐方向性模式的方法

Method: 在边缘图上定义偏度并传播到节点空间，使用可识别层次先验和低秩参数化确保可扩展性，基于偏态正态分布的随机表示实现计算

Result: 模拟研究表明RENeGe-sk比对称高斯先验更准确地恢复紧凑的边缘对齐方向结构，在非规则空间模式中保持竞争力；应用于巴西南部癌症发病率数据，获得稳定的区域级估计同时保留局部方向性空间变异

Conclusion: RENeGe-sk模型成功扩展了空间先验框架，能够有效捕捉方向性不对称结构，在保持计算可扩展性的同时提供更准确的空间模式识别

Abstract: We introduce a skewed edge based spatial prior, named RENeGe sk that extends the Gaussian RENeGe framework by incorporating directional asymmetry through a skew normal distribution. Skewness is defined on the edge graph and propagated to the node space, aligning asymmetric behavior with transitions across neighboring regions rather than with marginal node effects. The model is formulated within the skew normal framework and employs identifiable hierarchical priors together with low rank parameterizations to ensure scalability. The skew normal's stochastic representation is considered to facilitate the computational implementation. Simulation studies show that RENeGe sk recovers compact, edge-aligned directional structure more accurately than symmetric Gaussian priors, while remaining competitive under irregular spatial patterns. An application to cancer incidence data in Southern Brazil illustrates how the proposed approach yields stable area-level estimates while preserving localized, directionally driven spatial variation.

</details>


### [17] [Spectral embedding of inhomogeneous Poisson processes on multiplex networks](https://arxiv.org/abs/2601.16784)
*Joshua Corneck,Edward A. K. Cohen,Francesco Sanna Passino*

Main category: stat.ME

TL;DR: 提出了一种用于连续时间观测的多重网络点过程模型，通过谱嵌入方法估计低维潜在位置参数，并建立了理论一致性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多网络的边数据在连续时间演化，且存在异质边类型，这自然催生了基于点过程的表示方法。多重网络点过程需要处理这种连续时间观测的复杂网络数据。

Method: 提出连续时间观测的多重网络模型，将每条边的强度表示为两个低维潜在位置的内积：一个是动态且层无关的，另一个是静态且层依赖的。采用谱嵌入方法进行推断，使用直方图估计器估计网络强度，并应用双重展开邻接谱嵌入方法进行估计。

Result: 建立了谱嵌入估计的无穷范数一致性和渐近正态性理论结果，随着网络规模和时间分辨率的增加而成立。模拟和真实数据分析验证了模型和推断方法的有效性。

Conclusion: 该模型为连续时间观测的多重网络数据提供了有效的表示和推断框架，谱嵌入方法能够准确估计潜在位置参数，理论保证为实际应用提供了可靠性基础。

Abstract: In many real-world networks, data on the edges evolve in continuous time, naturally motivating representations based on point processes. Heterogeneity in edge types further gives rise to multiplex network point processes. In this work, we propose a model for multiplex network data observed in continuous-time. We establish two-to-infinity norm consistency and asymptotic normality for spectral-embedding-based estimation of the model parameters as both network size and time resolution increase. Drawing inspiration from random dot product graph models, each edge intensity is expressed as the inner product of two low-dimensional latent positions: one dynamic and layer-agnostic, the other static and layer-dependent. These latent positions constitute the primary objects of inference, which is conducted via spectral embedding methods. Our theoretical results are established under a histogram estimator of the network intensities and provide justification for applying a doubly unfolded adjacency spectral embedding method for estimation. Simulations and real-data analyses demonstrate the effectiveness of the proposed model and inference procedure.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [18] [Distributional Computational Graphs: Error Bounds](https://arxiv.org/abs/2601.16250)
*Olof Hallqvist Elias,Michael Selby,Phillip Stanley-Marbell*

Main category: stat.ML

TL;DR: 研究分布计算图的离散化误差分析，建立基于Wasserstein-1距离的非渐近误差界


<details>
  <summary>Details</summary>
Motivation: 分布计算图（输入为概率分布而非点值）在实际计算中通常需要离散化近似，需要分析这种近似带来的误差

Method: 建立分布计算图的一般框架，分析使用有限近似（离散表示或经验分布）评估连续概率分布时的误差，使用Wasserstein-1距离作为度量

Result: 建立了非渐近误差界，无需对计算图结构做假设，为分布计算图的离散化提供了理论保证

Conclusion: 该研究为分布计算图的离散化误差提供了理论基础，有助于理解和控制实际计算中的近似误差

Abstract: We study a general framework of distributional computational graphs: computational graphs whose inputs are probability distributions rather than point values. We analyze the discretization error that arises when these graphs are evaluated using finite approximations of continuous probability distributions. Such an approximation might be the result of representing a continuous real-valued distribution using a discrete representation or from constructing an empirical distribution from samples (or might be the output of another distributional computational graph). We establish non-asymptotic error bounds in terms of the Wasserstein-1 distance, without imposing structural assumptions on the computational graph.

</details>


### [19] [Perfect Clustering for Sparse Directed Stochastic Block Models](https://arxiv.org/abs/2601.16427)
*Behzad Aalipur,Yichen Qin*

Main category: stat.ML

TL;DR: 提出了一种针对稀疏有向随机块模型的非谱方法，通过邻域平滑估计概率矩阵，然后进行K-means聚类，实现了在社区数量发散情况下的精确恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏有向网络中的社区检测存在局限：谱方法在非对称、低度区域不稳定，而非谱方法主要关注无向或密集网络。需要开发适用于稀疏有向SBM且能处理社区数量发散的方法。

Method: 两阶段非谱方法：1) 使用针对非对称场景设计的邻域平滑方案估计有向概率矩阵；2) 对估计的行应用K-means聚类，避免在稀疏非对称网络中使用特征值或奇异值分解。

Result: 获得了平滑估计器的统一行向集中界，在温和的稀疏性和分离条件下，实现了所有社区标签的精确恢复（概率趋于1），允许γ_n→0和K_n→∞。

Conclusion: 该方法在高度有向、稀疏和非对称块结构中表现可靠，优于现有有向谱方法和基于分数的方法，是首个在稀疏有向设置中为非谱邻域平滑方法提供精确恢复保证的工作。

Abstract: Exact recovery in stochastic block models (SBMs) is well understood in undirected settings, but remains considerably less developed for directed and sparse networks, particularly when the number of communities diverges. Spectral methods for directed SBMs often lack stability in asymmetric, low-degree regimes, and existing non-spectral approaches focus primarily on undirected or dense settings.
  We propose a fully non-spectral, two-stage procedure for community detection in sparse directed SBMs with potentially growing numbers of communities. The method first estimates the directed probability matrix using a neighborhood-smoothing scheme tailored to the asymmetric setting, and then applies $K$-means clustering to the estimated rows, thereby avoiding the limitations of eigen- or singular value decompositions in sparse, asymmetric networks. Our main theoretical contribution is a uniform row-wise concentration bound for the smoothed estimator, obtained through new arguments that control asymmetric neighborhoods and separate in- and out-degree effects. These results imply the exact recovery of all community labels with probability tending to one, under mild sparsity and separation conditions that allow both $γ_n \to 0$ and $K_n \to \infty$.
  Simulation studies, including highly directed, sparse, and non-symmetric block structures, demonstrate that the proposed procedure performs reliably in regimes where directed spectral and score-based methods deteriorate. To the best of our knowledge, this provides the first exact recovery guarantee for this class of non-spectral, neighborhood-smoothing methods in the sparse, directed setting.

</details>


### [20] [Efficient Learning of Stationary Diffusions with Stein-type Discrepancies](https://arxiv.org/abs/2601.16597)
*Fabian Bleile,Sarah Lumpp,Mathias Drton*

Main category: stat.ML

TL;DR: 提出基于Stein差异的Stein-type KDS（SKDS）方法，用于学习平稳扩散过程，相比原KDS计算成本更低且保持相当精度


<details>
  <summary>Details</summary>
Motivation: 现有KDS方法虽然能通过再生核希尔伯特空间评估扩散生成子来保证平稳性，但计算成本较高，需要更高效的替代方案

Method: 利用KDS与Stein差异之间的联系，提出Stein-type KDS（SKDS）作为替代公式，证明SKDS趋近于零时能保证学习扩散的平稳分布与目标分布对齐

Result: 在广泛参数化下，SKDS是凸的，其经验版本以高概率具有ε-拟凸性；实验表明SKDS达到与KDS相当的精度，同时大幅降低计算成本，优于多数竞争基线

Conclusion: SKDS提供了一种计算效率更高的平稳扩散学习方法，在保持理论保证的同时显著降低计算负担，是KDS的有效替代方案

Abstract: Learning a stationary diffusion amounts to estimating the parameters of a stochastic differential equation whose stationary distribution matches a target distribution. We build on the recently introduced kernel deviation from stationarity (KDS), which enforces stationarity by evaluating expectations of the diffusion's generator in a reproducing kernel Hilbert space. Leveraging the connection between KDS and Stein discrepancies, we introduce the Stein-type KDS (SKDS) as an alternative formulation. We prove that a vanishing SKDS guarantees alignment of the learned diffusion's stationary distribution with the target. Furthermore, under broad parametrizations, SKDS is convex with an empirical version that is $ε$-quasiconvex with high probability. Empirically, learning with SKDS attains comparable accuracy to KDS while substantially reducing computational cost and yields improvements over the majority of competitive baselines.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [21] [Matrix-Response Generalized Linear Mixed Model with Applications to Longitudinal Brain Images](https://arxiv.org/abs/2601.16340)
*Zhentao Yu,Jiaqi Ding,Guorong Wu,Quefeng Li*

Main category: stat.AP

TL;DR: 提出了一种用于纵向脑网络数据的矩阵响应广义线性混合模型，能够识别受外部预测因子影响的连接边，并开发了高效的MCEM算法进行参数估计。


<details>
  <summary>Details</summary>
Motivation: 纵向脑成像数据能够监测个体大脑随时间的结构和功能变化，有助于发现疾病进展的早期生物标志物和评估干预效果。然而，目前缺乏针对高维成像数据衍生的纵向脑网络的统计方法。

Method: 提出了矩阵响应广义线性混合模型，用于分析纵向脑网络数据，识别受外部预测因子影响的连接边。开发了高效的蒙特卡洛期望最大化（MCEM）算法进行参数估计。

Result: 广泛的模拟实验表明，该方法能有效识别协变量相关的网络成分并进行准确的参数估计。在DTI和fMRI数据集上的应用进一步验证了方法的实用性。

Conclusion: 该方法填补了纵向脑网络分析方法的空白，为从高维纵向脑成像数据中提取有意义的网络特征提供了有效的统计工具。

Abstract: Longitudinal brain imaging data facilitate the monitoring of structural and functional alterations in individual brains across time, offering essential understanding of dynamic neurobiological mechanisms. Such data improve sensitivity for detecting early biomarkers of disease progression and enhance the evaluation of intervention effects. While recent matrix-response regression models can relate static brain networks to external predictors, there remain few statistical methods for longitudinal brain networks, especially those derived from high-dimensional imaging data. We introduce a matrix-response generalized linear mixed model that accommodates longitudinal brain networks and identifies edges whose connectivity is influenced by external predictors. An efficient Monte Carlo Expectation-Maximization algorithm is developed for parameter estimation. Extensive simulations demonstrate effective identification of covariate-related network components and accurate parameter estimation. We further demonstrate the usage of the proposed method through applications to diffusion tensor imaging (DTI) and functional MRI (fMRI) datasets.

</details>


### [22] [Long-Term Probabilistic Forecast of Vegetation Conditions Using Climate Attributes in the Four Corners Region](https://arxiv.org/abs/2601.16347)
*Erika McPhillips,Hyeongseong Lee,Xiangyu Xie,Kathy Baylis,Chris Funk,Mengyang Gu*

Main category: stat.AP

TL;DR: 开发两阶段机器学习模型，用于高分辨率网格上一年前预测峰值NDVI，在美国西南部四角地区进行测试，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 天气条件严重影响作物和牧场状态，进而影响全球收入和粮食安全。虽然已有卫星遥感监测植被，但现有机器学习方法只能进行短期（如一个月前）NDVI预测，缺乏长期（如一年前）植被条件预测方法。

Method: 开发两阶段机器学习模型：第一阶段识别关键气候属性（降水和最大蒸汽压亏缺），建立广义并行高斯过程捕捉气候属性与NDVI关系；第二阶段使用至少一年前的历史数据预测这些气候属性，然后作为输入预测每个空间网格的峰值NDVI。

Result: 开发的开源工具在总体NDVI和基于网格的NDVI一年前预测方面均优于替代方法，为农民和牧场主提供可提前一年制定行动计划的信息。

Conclusion: 成功开发了长期植被预测方法，填补了一年提前预测NDVI的空白，为农业和牧场管理提供了实用的决策支持工具。

Abstract: Weather conditions can drastically alter the state of crops and rangelands, and in turn, impact the incomes and food security of individuals worldwide. Satellite-based remote sensing offers an effective way to monitor vegetation and climate variables on regional and global scales. The annual peak Normalized Difference Vegetation Index (NDVI), derived from satellite observations, is closely associated with crop development, rangeland biomass, and vegetation growth. Although various machine learning methods have been developed to forecast NDVI over short time ranges, such as one-month-ahead predictions, long-term forecasting approaches, such as one-year-ahead predictions of vegetation conditions, are not yet available. To fill this gap, we develop a two-phase machine learning model to forecast the one-year-ahead peak NDVI over high-resolution grids, using the Four Corners region of the Southwestern United States as a testbed. In phase one, we identify informative climate attributes, including precipitation and maximum vapor pressure deficit, and develop the generalized parallel Gaussian process that captures the relationship between climate attributes and NDVI. In phase two, we forecast these climate attributes using historical data at least one year before the NDVI prediction month, which then serve as inputs to forecast the peak NDVI at each spatial grid. We developed open-source tools that outperform alternative methods for both gross NDVI and grid-based NDVI one-year forecasts, providing information that can help farmers and ranchers make actionable plans a year in advance.

</details>


### [23] [Spillovers and Co-movements in Multivariate Volatility: A Vector Multiplicative Error Model](https://arxiv.org/abs/2601.16837)
*Edoardo Otranto,Luca Scaffidi Domianello*

Main category: stat.AP

TL;DR: 提出了一种新的多元波动率模型，在MEM框架下同时捕捉溢出效应和共同运动，通过潜在分量和聚类方法降低参数维度，在道琼斯指数29个资产上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 金融市场日益全球化导致资产间波动率存在溢出效应和共同运动，现有多元波动率模型需要同时捕捉这两种交互作用，同时在高维情况下保持计算可行性。

Method: 在MEM框架下引入新的波动率向量模型，通过特定参数化包含溢出效应和共同运动的潜在分量，采用基于模型的聚类方法减少未知系数数量，保持高维情况下的计算可行性。

Result: 在道琼斯工业平均指数29个资产上的实证应用表明，该方法能有效分析波动率溢出和共享市场动态，与替代向量MEM模型相比，在多个评估标准上表现优越或至少相当。

Conclusion: 提出的新MEM模型能有效捕捉多元波动率中的溢出效应和共同运动，通过聚类方法降低参数维度，为高维金融时间序列分析提供了计算可行的解决方案。

Abstract: Recent developments in financial time series focus on modeling volatility across multiple assets or indices in a multivariate framework, accounting for potential interactions such as spillover effects. Furthermore, the increasing integration of global financial markets provides a similar dynamics (referred to as comovement). In this context, we introduce a novel model for volatility vectors within the Multiplicative Error Model (MEM) class. This framework accommodates both spillover and co-movement effects through a distinct latent component. By adopting a specific parameterization, the model remains computationally feasible even for high-dimensional volatility vectors. To reduce the number of unknown coefficients, we propose a simple model-based clustering procedure. We illustrate the effectiveness of the proposed approach through an empirical application to 29 assets of the Dow Jones Industrial Average index, providing insight into volatility spillovers and shared market dynamics. Comparative analysis against alternative vector MEMs, including a fully parameterized version of the proposed model, demonstrates its superior or at least comparable performance across multiple evaluation criteria.

</details>


### [24] [Identifying heat-related diagnoses in emergency department visits among adults in Chicago: a heat-wide association study](https://arxiv.org/abs/2601.16932)
*Hyojung Jang,Peter M. Graffy,Benjamin W. Barrett,Daniel E. Horton,Jennifer L. Chan,Abel N. Kho*

Main category: stat.AP

TL;DR: 该研究通过热广泛关联研究，分析了芝加哥2011-2023年91.7万次急性护理就诊数据，发现极端高温与热病、容量不足、低血压、水肿、急性肾衰竭和多种损伤的就诊增加相关。


<details>
  <summary>Details</summary>
Motivation: 以往研究依赖受限的诊断和诊断类别，会遗漏或错误分类热相关疾病。需要更全面地识别极端高温相关的急性护理诊断。

Method: 采用两阶段分析方法：1) 准泊松回归筛选1803个诊断代码的热相关风险；2) 在时间分层病例交叉设计中应用分布式滞后非线性模型，精炼热相关诊断列表并估计极端高温与参考温度下就诊的当日和短期累积比值比。

Result: 发现极端高温当日就诊增加的热相关疾病包括：热病、容量不足、低血压、水肿、急性肾衰竭和多种损伤。

Conclusion: 通过分析急性护理服务的完整诊断谱，该研究全面描述了热相关发病率，强化并推进了现有文献，为公共卫生应对极端高温提供了更全面的证据基础。

Abstract: Extreme heat is an escalating public health concern. Although prior studies have examined heat-health associations, their reliance on restricted diagnoses and diagnostic categories misses or misclassifies heat-related illness. We conducted a heat-wide association study to identify acute-care diagnoses associated with extreme heat in Chicago, Illinois. Using 916,904 acute-care visits -- including emergency department and urgent care encounters -- among 372,140 adults across five healthcare systems from 2011-2023, we applied a two-stage analytic approach: quasi-Poisson regression to screen 1,803 diagnosis codes for heat-related risks, followed by distributed lag non-linear models in a time-stratified case-crossover design to refine the list of heat-related diagnoses and estimate same-day and short-term cumulative odds ratios of acute-care visits during extreme heat versus reference temperature. We observed same-day increases in visits for heat illness, volume depletion, hypotension, edema, acute kidney failure, and multiple injuries. By analyzing the full diagnostic spectrum of acute-care services, this study comprehensively characterizes heat-associated morbidity, reinforcing and advancing existing literature.

</details>
