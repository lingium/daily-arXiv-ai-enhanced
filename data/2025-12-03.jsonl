{"id": "2512.00220", "categories": ["stat.CO", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.00220", "abs": "https://arxiv.org/abs/2512.00220", "authors": ["Pietari Laitinen", "Matti Vihola"], "title": "Iterated sampling importance resampling with adaptive number of proposals", "comment": null, "summary": "Iterated sampling importance resampling (i-SIR) is a Markov chain Monte Carlo (MCMC) algorithm which is based on $N$ independent proposals. As $N$ grows, its samples become nearly independent, but with an increased computational cost. We discuss a method which finds an approximately optimal number of proposals $N$ in terms of the asymptotic efficiency. The optimal $N$ depends on both the mixing properties of the i-SIR chain and the (parallel) computing costs. Our method for finding an appropriate $N$ is based on an approximate asymptotic variance of the i-SIR, which has similar properties as the i-SIR asymptotic variance, and a generalised i-SIR transition having fractional `number of proposals.' These lead to an adaptive i-SIR algorithm, which tunes the number of proposals automatically during sampling. Our experiments demonstrate that our approximate efficiency and the adaptive i-SIR algorithm have promising empirical behaviour. We also present new theoretical results regarding the i-SIR, such as the convexity of asymptotic variance in the number of proposals, which can be of independent interest."}
{"id": "2512.00232", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.00232", "abs": "https://arxiv.org/abs/2512.00232", "authors": ["Jan de Leeuw"], "title": "Yet Another Smacof -- Square Symmetric Case", "comment": null, "summary": "We rewrite the metric/nonmetric and weighted/unweighted versions of the smacof program for square symmetric data as one monolithic C program. R is used for taking care of the data and parameter setup, the I/O, and of issuing a single call to .C() to start the computations. This makes this new smacofSS() program five to fifty times as fast (for our examples) as the smacofSym() function from the R smacof package. Utilities for various initial configurations and plots are included in the package. Examples are included to compare output and time for the R and C versions and to illustrate the various plots."}
{"id": "2512.01790", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.01790", "abs": "https://arxiv.org/abs/2512.01790", "authors": ["Bernard Bercu", "Luis Fredes", "Eméric Gbaguidi"], "title": "An hybrid stochastic Newton algorithm for logistic regression", "comment": null, "summary": "In this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk."}
{"id": "2512.00627", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.00627", "abs": "https://arxiv.org/abs/2512.00627", "authors": ["Chadi Bsila", "Yiqi Tang", "Kaiwen Wang", "Laurie Heyer"], "title": "Rényi's $α$-divergence variational Bayes for spike-and-slab high-dimensional linear regression", "comment": null, "summary": "Sparse high-dimensional linear regression is a central problem in statistics, where the goal is often variable selection and/or coefficient estimation. We propose a mean-field variational Bayes approximation for sparse regression with spike-and-slab Laplace priors that replaces the standard Kullback-Leibler (KL) divergence objective with the Rényi's $α$ divergence: a one-parameter generalization of KL divergence indexed by $α\\in (0, \\infty) \\setminus \\{1\\}$ that allows flexibility between zero-forcing and mass-covering behavior. We derive coordinate ascent variational inference (CAVI) updates via a second-order delta method and develop a stochastic variational inference algorithm based on a Monte Carlo surrogate Rényi lower bound. In simulations, our two methods perform comparably to state-of-the-art Bayesian variable selection procedures across a range of sparsity configurations and $α$ values for both variable selection and estimation, and our numerical results illustrate how different choices of $α$ can be advantageous in different sparsity configurations."}
{"id": "2512.00220", "categories": ["stat.CO", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.00220", "abs": "https://arxiv.org/abs/2512.00220", "authors": ["Pietari Laitinen", "Matti Vihola"], "title": "Iterated sampling importance resampling with adaptive number of proposals", "comment": null, "summary": "Iterated sampling importance resampling (i-SIR) is a Markov chain Monte Carlo (MCMC) algorithm which is based on $N$ independent proposals. As $N$ grows, its samples become nearly independent, but with an increased computational cost. We discuss a method which finds an approximately optimal number of proposals $N$ in terms of the asymptotic efficiency. The optimal $N$ depends on both the mixing properties of the i-SIR chain and the (parallel) computing costs. Our method for finding an appropriate $N$ is based on an approximate asymptotic variance of the i-SIR, which has similar properties as the i-SIR asymptotic variance, and a generalised i-SIR transition having fractional `number of proposals.' These lead to an adaptive i-SIR algorithm, which tunes the number of proposals automatically during sampling. Our experiments demonstrate that our approximate efficiency and the adaptive i-SIR algorithm have promising empirical behaviour. We also present new theoretical results regarding the i-SIR, such as the convexity of asymptotic variance in the number of proposals, which can be of independent interest."}
{"id": "2512.00203", "categories": ["stat.AP", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00203", "abs": "https://arxiv.org/abs/2512.00203", "authors": ["Jonathan Pipping", "Tianshu Feng", "R. Paul Sabin"], "title": "Beyond Expected Goals: A Probabilistic Framework for Shot Occurrences in Soccer", "comment": "18pp main + 3pp appendix; 8 figures, 12 tables. Submitted to the Journal of Quantitative Analysis in Sports (JQAS). Data proprietary to Gradient Sports; we share derived features & scripts (code under MIT/Apache-2.0). Preprint licensed CC BY 4.0", "summary": "Expected goals (xG) models estimate the probability that a shot results in a goal from its context (e.g., location, pressure), but they operate only on observed shots. We propose xG+, a possession-level framework that first estimates the probability that a shot occurs within the next second and its corresponding xG if it were to occur. We also introduce ways to aggregate this joint probability estimate over the course of a possession. By jointly modeling shot-taking behavior and shot quality, xG+ remedies the conditioning-on-shots limitation of standard xG. We show that this improves predictive accuracy at the team level and produces a more persistent player skill signal than standard xG models."}
{"id": "2512.00175", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00175", "abs": "https://arxiv.org/abs/2512.00175", "authors": ["Helen Guo", "Elizabeth L. Ogburn", "Ilya Shpitser"], "title": "Comparing Two Proxy Methods for Causal Identification", "comment": "10 pages; 6 figures", "summary": "Identifying causal effects in the presence of unmeasured variables is a fundamental challenge in causal inference, for which proxy variable methods have emerged as a powerful solution. We contrast two major approaches in this framework: (1) bridge equation methods, which leverage solutions to integral equations to recover causal targets, and (2) array decomposition methods, which recover latent factors composing counterfactual quantities by exploiting unique determination of eigenspaces. We compare the model restrictions underlying these two approaches and provide insight into implications of the underlying assumptions, clarifying the scope of applicability for each method."}
{"id": "2512.00183", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2512.00183", "abs": "https://arxiv.org/abs/2512.00183", "authors": ["Niki Z. Petrakos", "Erica E. M. Moodie", "Nicolas Savy"], "title": "Incorporating Missingness in a Framework for Generating Realistic Synthetic Randomized Controlled Trial Data", "comment": null, "summary": "The current literature regarding generation of complex, realistic synthetic tabular data, particularly for randomized controlled trials (RCTs), often ignores missing data. However, missing data are common in RCT data and often are not Missing Completely At Random. We bridge the gap of determining how best to generate realistic synthetic data while also accounting for the missingness mechanism. We demonstrate how to generate synthetic missing values while ensuring that synthetic data mimic the targeted real data distribution. We propose and empirically compare several data generation frameworks utilizing various strategies for handling missing data (complete case, inverse probability weighting, and multiple imputation) by quantifying generation performance through a range of metrics. Focusing on the Missing At Random setting, we find that incorporating additional models to account for the missingness always outperformed a complete case approach."}
{"id": "2512.00252", "categories": ["stat.ML", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.00252", "abs": "https://arxiv.org/abs/2512.00252", "authors": ["Martin Andrae", "Erik Larsson", "So Takao", "Tomas Landelius", "Fredrik Lindsten"], "title": "DAISI: Data Assimilation with Inverse Sampling using Stochastic Interpolants", "comment": "41 pages, 24 figures", "summary": "Data assimilation (DA) is a cornerstone of scientific and engineering applications, combining model forecasts with sparse and noisy observations to estimate latent system states. Classical DA methods, such as the ensemble Kalman filter, rely on Gaussian approximations and heuristic tuning (e.g., inflation and localization) to scale to high dimensions. While often successful, these approximations can make the methods unstable or inaccurate when the underlying distributions of states and observations depart significantly from Gaussianity. To address this limitation, we introduce DAISI, a scalable filtering algorithm built on flow-based generative models that enables flexible probabilistic inference using data-driven priors. The core idea is to use a stationary, pre-trained generative prior to assimilate observations via guidance-based conditional sampling while incorporating forecast information through a novel inverse-sampling step. This step maps the forecast ensemble into a latent space to provide initial conditions for the conditional sampling, allowing us to encode model dynamics into the DA pipeline without having to retrain or fine-tune the generative prior at each assimilation step. Experiments on challenging nonlinear systems show that DAISI achieves accurate filtering results in regimes with sparse, noisy, and nonlinear observations where traditional methods struggle."}
{"id": "2512.00895", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.00895", "abs": "https://arxiv.org/abs/2512.00895", "authors": ["Jin Hyung Lee", "Ben Seiyon Lee"], "title": "A Scalable Variational Bayes Approach for Fitting Non-Conjugate Spatial Generalized Linear Mixed Models via Basis Expansions", "comment": "26 pages, 5 figures, 4 tables", "summary": "Large spatial datasets with non-Gaussian responses are increasingly common in environmental monitoring, ecology, and remote sensing, yet scalable Bayesian inference for such data remains challenging. Markov chain Monte Carlo (MCMC) methods are often prohibitive for large datasets, and existing variational Bayes methods rely on conjugacy or strong approximations that limit their applicability and can underestimate posterior variances. We propose a scalable variational framework that incorporates semi-implicit variational inference (SIVI) with basis representations of spatial generalized linear mixed models (SGLMMs), which may not have conjugacy. Our approach accommodates gamma, negative binomial, Poisson, Bernoulli, and Gaussian responses on continuous spatial domains. Across 20 simulation scenarios with 50,000 locations, SIVI achieves predictive accuracy and posterior distributions comparable to Metropolis-Hastings and Hamiltonian Monte Carlo while providing notable computational speedups. Applications to MODIS land surface temperature and Blue Jay abundance further demonstrate the utility of the approach for large non-Gaussian spatial datasets."}
{"id": "2512.00240", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.00240", "abs": "https://arxiv.org/abs/2512.00240", "authors": ["Yingdong Yang"], "title": "Bayesian Analysis of Hotel Booking Cancellations: A Hierarchical Modeling Approach", "comment": null, "summary": "This study presents a comprehensive Bayesian analysis of hotel booking cancellations using PyMC, comparing three model specifications of increasing complexity. We investigate how lead time, special requests, and parking requirements affect cancellation probability, and explore interaction effects with hotel type. Using MCMC sampling (NUTS algorithm) on 5,000 booking records, we find strong evidence that longer lead times increase cancellation probability (posterior mean: 0.600, 95\\% HDI: [0.532, 0.661]), while special requests (posterior mean: -0.642) and parking (posterior mean: -3.879) significantly reduce cancellation risk. Model comparison via WAIC reveals that the full interaction model provides the best predictive performance, suggesting that the effects of booking characteristics vary systematically between city and resort hotels. This Bayesian approach enables full uncertainty quantification and provides actionable insights for revenue management."}
{"id": "2512.00397", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00397", "abs": "https://arxiv.org/abs/2512.00397", "authors": ["Mehdi Dagdoug", "Clement Dombry", "Jean-Jil Duchamps"], "title": "An RKHS Perspective on Tree Ensembles", "comment": "69 pages", "summary": "Random Forests and Gradient Boosting are among the most effective algorithms for supervised learning on tabular data. Both belong to the class of tree-based ensemble methods, where predictions are obtained by aggregating many randomized regression trees. In this paper, we develop a theoretical framework for analyzing such methods through Reproducing Kernel Hilbert Spaces (RKHSs) constructed on tree ensembles -- more precisely, on the random partitions generated by randomized regression trees. We establish fundamental analytical properties of the resulting Random Forest kernel, including boundedness, continuity, and universality, and show that a Random Forest predictor can be characterized as the unique minimizer of a penalized empirical risk functional in this RKHS, providing a variational interpretation of ensemble learning. We further extend this perspective to the continuous-time formulation of Gradient Boosting introduced by Dombry and Duchamps, and demonstrate that it corresponds to a gradient flow on a Hilbert manifold induced by the Random Forest RKHS. A key feature of this framework is that both the kernel and the RKHS geometry are data-dependent, offering a theoretical explanation for the strong empirical performance of tree-based ensembles. Finally, we illustrate the practical potential of this approach by introducing a kernel principal component analysis built on the Random Forest kernel, which enhances the interpretability of ensemble models, as well as GVI, a new geometric variable importance criterion."}
{"id": "2512.00338", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00338", "abs": "https://arxiv.org/abs/2512.00338", "authors": ["Dehao Dai", "Yunyi Zhang"], "title": "On Statistical Inference for High-Dimensional Binary Time Series", "comment": "55 pages, 6 figures", "summary": "The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method."}
{"id": "2512.00277", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.00277", "abs": "https://arxiv.org/abs/2512.00277", "authors": ["Andrew Cooper", "Justin Strait", "Mary Frances Dorn", "Robert B. Gramacy", "Brendon Parsons", "Alessandro Cattaneo"], "title": "Robust Wrapped Gaussian Process Inference for Noisy Angular Data", "comment": null, "summary": "Angular data are commonly encountered in settings with a directional or orientational component. Regressing an angular response on real-valued features requires intrinsically capturing the circular or spherical manifold the data lie on, or using an appropriate extrinsic transformation. A popular example of the latter is the technique of distributional wrapping, in which functions are \"wrapped\" around the unit circle via a modulo-$2π$ transformation. This approach enables flexible, non-linear models like Gaussian processes (GPs) to properly account for circular structure. While straightforward in concept, the need to infer the latent unwrapped distribution along with its wrapping behavior makes inference difficult in noisy response settings, as misspecification of one can severely hinder estimation of the other. However, applications such as radiowave analysis (Shangguan et al., 2015) and biomedical engineering (Kurz and Hanebeck, 2015) encounter radial data where wrapping occurs in only one direction. We therefore propose a novel wrapped GP (WGP) model formulation that recognizes monotonic wrapping behavior for more accurate inference in these situations. This is achieved by estimating the locations where wrapping occurs and partitioning the input space accordingly. We also specify a more robust Student's t response likelihood, and take advantage of an elliptical slice sampling (ESS) algorithm for rejection-free sampling from the latent GP space. We showcase our model's preferable performance on simulated examples compared to existing WGP methodologies. We then apply our method to the problem of localizing radiofrequency identification (RFID) tags, in which we model the relationship between frequency and phase angle to infer how far away an RFID tag is from an antenna."}
{"id": "2512.00517", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.00517", "abs": "https://arxiv.org/abs/2512.00517", "authors": ["Eliabelle Mauduit", "Eloïse Berthier", "Andrea Simonetto"], "title": "No-Regret Gaussian Process Optimization of Time-Varying Functions", "comment": null, "summary": "Sequential optimization of black-box functions from noisy evaluations has been widely studied, with Gaussian Process bandit algorithms such as GP-UCB guaranteeing no-regret in stationary settings. However, for time-varying objectives, it is known that no-regret is unattainable under pure bandit feedback unless strong and often unrealistic assumptions are imposed.\n  In this article, we propose a novel method to optimize time-varying rewards in the frequentist setting, where the objective has bounded RKHS norm. Time variations are captured through uncertainty injection (UI), which enables heteroscedastic GP regression that adapts past observations to the current time step. As no-regret is unattainable in general in the strict bandit setting, we relax the latter allowing additional queries on previously observed points. Building on sparse inference and the effect of UI on regret, we propose \\textbf{W-SparQ-GP-UCB}, an online algorithm that achieves no-regret with only a vanishing number of additional queries per iteration. To assess the theoretical limits of this approach, we establish a lower bound on the number of additional queries required for no-regret, proving the efficiency of our method. Finally, we provide a comprehensive analysis linking the degree of time-variation of the function to achievable regret rates, together with upper and lower bounds on the number of additional queries needed in each regime."}
{"id": "2512.01097", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01097", "abs": "https://arxiv.org/abs/2512.01097", "authors": ["Zachary Terner", "Alexander Petersen", "Yuedong Wang"], "title": "Discriminative classification with generative features: bridging Naive Bayes and logistic regression", "comment": null, "summary": "We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance."}
{"id": "2512.00610", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.00610", "abs": "https://arxiv.org/abs/2512.00610", "authors": ["Bertrand Even", "Luca Ganassali"], "title": "Statistical-computational gap in multiple Gaussian graph alignment", "comment": null, "summary": "We investigate the existence of a statistical-computational gap in multiple Gaussian graph alignment. We first generalize a previously established informational threshold from Vassaux and Massoulié (2025) to regimes where the number of observed graphs $p$ may also grow with the number of nodes $n$: when $p \\leq O(n/\\log(n))$, we recover the results from Vassaux and Massoulié (2025), and $p \\geq Ω(n/\\log(n))$ corresponds to a regime where the problem is as difficult as aligning one single graph with some unknown \"signal\" graph. Moreover, when $\\log p = ω(\\log n)$, the informational thresholds for partial and exact recovery no longer coincide, in contrast to the all-or-nothing phenomenon observed when $\\log p=O(\\log n)$. Then, we provide the first computational barrier in the low-degree framework for (multiple) Gaussian graph alignment. We prove that when the correlation $ρ$ is less than $1$, up to logarithmic terms, low degree non-trivial estimation fails. Our results suggest that the task of aligning $p$ graphs in polynomial time is as hard as the problem of aligning two graphs in polynomial time, up to logarithmic factors. These results characterize the existence of a statistical-computational gap and provide another example in which polynomial-time algorithms cannot handle complex combinatorial bi-dimensional structures."}
{"id": "2512.00312", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.00312", "abs": "https://arxiv.org/abs/2512.00312", "authors": ["Kenny Watts", "Jonathan Pipping"], "title": "Kicking for Goal or Touch? An Expected Points Framework for Penalty Decisions in Rugby Union", "comment": "20 pages; 9 figures, 8 tables. Submitted to the Journal of Quantitative Analysis in Sports (JQAS). Code & replication package: https://github.com/WhartonSABI/rugby-ep (data from a public source; mirrored in the repo with attribution). Preprint licensed CC BY 4.0", "summary": "Following a penalty in rugby union, teams typically choose between attempting a shot at goal or kicking to touch to pursue a try. We develop an Expected Points (EP) framework that quantifies the value of each option as a function of both field location and game context. Using phase-level data from the 2018/19 Premiership Rugby season (35,199 phases across 132 matches) and an angle-distance model of penalty kick success estimated from international records, we construct two surfaces: (i) the expected points of a possession beginning with a lineout, and (ii) the expected points of a kick at goal, taking into account the in-game consequences of made and missed kicks. We then compare these surfaces to produce decision maps that indicate where kicking for goal or kicking to touch maximizes expected return, and we analyze how the boundary shifts with game context and the expected meters gained to touch. Our results provide a unified, data-driven method for evaluating penalty decisions and can be tailored to team-specific kickers and lineout units. This study offers, to our knowledge, the first comprehensive EP-based assessment of penalty strategy in rugby union and outlines extensions to win-probability analysis and richer tracking data."}
{"id": "2512.00316", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.00316", "abs": "https://arxiv.org/abs/2512.00316", "authors": ["Onrina Chandra", "Min-ge Xie"], "title": "Finite-Sample Valid Rank Confidence Sets for a Broad Class of Statistical and Machine Learning Models", "comment": "44 pages, 2 figures, 7 tables. Code available upon request", "summary": "Ranking populations such as institutions based on certain characteristics is often of interest, and these ranks are typically estimated using samples drawn from the populations. Due to sample randomness, it is important to quantify the uncertainty associated with the estimated ranks. This becomes crucial when latent characteristics are poorly separated and where many rank estimates may be incorrectly ordered. Understanding uncertainty can help quantify and mitigate these issues and provide a fuller picture. However, this task is especially challenging because the rank parameters are discrete and the central limit theorem does not apply to the rank estimates. In this article, we propose a Repro Samples Method to address this nontrivial inference problem by developing a confidence set for the true, unobserved population ranks. This method provides finite-sample coverage guarantees and is broadly applicable to ranking problems. The effectiveness of the method is illustrated and compared with several published large sample ranking approaches using simulation studies and real data examples involving samples both from traditional statistical models and modern data science algorithms."}
{"id": "2512.00610", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.00610", "abs": "https://arxiv.org/abs/2512.00610", "authors": ["Bertrand Even", "Luca Ganassali"], "title": "Statistical-computational gap in multiple Gaussian graph alignment", "comment": null, "summary": "We investigate the existence of a statistical-computational gap in multiple Gaussian graph alignment. We first generalize a previously established informational threshold from Vassaux and Massoulié (2025) to regimes where the number of observed graphs $p$ may also grow with the number of nodes $n$: when $p \\leq O(n/\\log(n))$, we recover the results from Vassaux and Massoulié (2025), and $p \\geq Ω(n/\\log(n))$ corresponds to a regime where the problem is as difficult as aligning one single graph with some unknown \"signal\" graph. Moreover, when $\\log p = ω(\\log n)$, the informational thresholds for partial and exact recovery no longer coincide, in contrast to the all-or-nothing phenomenon observed when $\\log p=O(\\log n)$. Then, we provide the first computational barrier in the low-degree framework for (multiple) Gaussian graph alignment. We prove that when the correlation $ρ$ is less than $1$, up to logarithmic terms, low degree non-trivial estimation fails. Our results suggest that the task of aligning $p$ graphs in polynomial time is as hard as the problem of aligning two graphs in polynomial time, up to logarithmic factors. These results characterize the existence of a statistical-computational gap and provide another example in which polynomial-time algorithms cannot handle complex combinatorial bi-dimensional structures."}
{"id": "2512.01667", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.01667", "abs": "https://arxiv.org/abs/2512.01667", "authors": ["Qingyang Liu", "Matthew A. Fisher", "Zheyang Shen", "Katy Tant", "Xuebin Zhao", "Andrew Curtis", "Chris. J. Oates"], "title": "Detecting Model Misspecification in Bayesian Inverse Problems via Variational Gradient Descent", "comment": null, "summary": "Bayesian inference is optimal when the statistical model is well-specified, while outside this setting Bayesian inference can catastrophically fail; accordingly a wealth of post-Bayesian methodologies have been proposed. Predictively oriented (PrO) approaches lift the statistical model $P_θ$ to an (infinite) mixture model $\\int P_θ\\; \\mathrm{d}Q(θ)$ and fit this predictive distribution via minimising an entropy-regularised objective functional. In the well-specified setting one expects the mixing distribution $Q$ to concentrate around the true data-generating parameter in the large data limit, while such singular concentration will typically not be observed if the model is misspecified. Our contribution is to demonstrate that one can empirically detect model misspecification by comparing the standard Bayesian posterior to the PrO `posterior' $Q$. To operationalise this, we present an efficient numerical algorithm based on variational gradient descent. A simulation study, and a more detailed case study involving a Bayesian inverse problem in seismology, confirm that model misspecification can be automatically detected using this framework."}
{"id": "2512.00338", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00338", "abs": "https://arxiv.org/abs/2512.00338", "authors": ["Dehao Dai", "Yunyi Zhang"], "title": "On Statistical Inference for High-Dimensional Binary Time Series", "comment": "55 pages, 6 figures", "summary": "The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method."}
{"id": "2512.00665", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00665", "abs": "https://arxiv.org/abs/2512.00665", "authors": ["Song Liu"], "title": "Self-sufficient Independent Component Analysis via KL Minimizing Flows", "comment": null, "summary": "We study the problem of learning disentangled signals from data using non-linear Independent Component Analysis (ICA). Motivated by advances in self-supervised learning, we propose to learn self-sufficient signals: A recovered signal should be able to reconstruct a missing value of its own from all remaining components without relying on any other signals. We formulate this problem as the minimization of a conditional KL divergence. Compared to traditional maximum likelihood estimation, our algorithm is prior-free and likelihood-free, meaning that we do not need to impose any prior on the original signals or any observational model, which often restricts the model's flexibility. To tackle the KL divergence minimization problem, we propose a sequential algorithm that reduces the KL divergence and learns an optimal de-mixing flow model at each iteration. This approach completely avoids the unstable adversarial training, a common issue in minimizing the KL divergence. Experiments on toy and real-world datasets show the effectiveness of our method."}
{"id": "2512.01927", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.01927", "abs": "https://arxiv.org/abs/2512.01927", "authors": ["Steven D. Barnett", "Robert B. Gramacy", "Lauren J. Beesley", "Dave Osthus", "Yifan Huang", "Fan Guo", "Daniel B. Reisenfeld"], "title": "Bayesian Statistical Inversion for High-Dimensional Computer Model Output and Spatially Distributed Counts", "comment": null, "summary": "Data collected by the Interstellar Boundary Explorer (IBEX) satellite, recording heliospheric energetic neutral atoms (ENAs), exhibit a phenomenon that has caused space scientists to revise hypotheses about the physical processes, and computer simulations under those models, in play at the boundary of our solar system. Evaluating the fit of these computer models involves tuning their parameters to observational data from IBEX. This would be a classic (Bayesian) inverse problem if not for three challenges: (1) the computer simulations are slow, limiting the size of campaigns of runs; so (2) surrogate modeling is essential, but outputs are high-resolution images, thwarting conventional methods; and (3) IBEX observations are counts, whereas most inverse problem techniques assume Gaussian field data. To fill that gap we propose a novel approach to Bayesian inverse problems coupling a Poisson response with a sparse Gaussian process surrogate using the Vecchia approximation. We demonstrate the capabilities of our proposed framework, which compare favorably to alternatives, through multiple simulated examples in terms of recovering \"true\" computer model parameters and accurate out-of-sample prediction. We then apply this new technology to IBEX satellite data and associated computer models developed at Los Alamos National Laboratory."}
{"id": "2512.01790", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.01790", "abs": "https://arxiv.org/abs/2512.01790", "authors": ["Bernard Bercu", "Luis Fredes", "Eméric Gbaguidi"], "title": "An hybrid stochastic Newton algorithm for logistic regression", "comment": null, "summary": "In this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk."}
{"id": "2512.00668", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00668", "abs": "https://arxiv.org/abs/2512.00668", "authors": ["Jungwoo Ho"], "title": "Restricted Block Permutation for Two-Sample Testing", "comment": null, "summary": "We study a structured permutation scheme for two-sample testing that restricts permutations to single cross-swaps between block-selected representatives. Our analysis yields three main results. First, we provide an exact validity construction that applies to any fixed restricted permutation set. Second, for both the difference of sample means and the unbiased $\\widehat{\\mathrm{MMD}}^{2}$ estimator, we derive closed-form one-swap increment identities whose conditional variances scale as $O(h^{2})$, in contrast to the $Θ(h)$ increment variability under full relabeling. This increment-level variance contraction sharpens the Bernstein--Freedman variance proxy and leads to substantially smaller permutation critical values. Third, we obtain explicit, data-dependent expressions for the resulting critical values and statistical power. Together, these results show that block-restricted one-swap permutations can achieve strictly higher power than classical full permutation tests while maintaining exact finite-sample validity, without relying on pessimistic worst-case Lipschitz bounds."}
{"id": "2512.01820", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.01820", "abs": "https://arxiv.org/abs/2512.01820", "authors": ["Valentin de Bortoli", "Romuald Elie", "Anna Kazeykina", "Zhenjie Ren", "Jiacheng Zhang"], "title": "Dimension-free error estimate for diffusion model and optimal scheduling", "comment": null, "summary": "Diffusion generative models have emerged as powerful tools for producing synthetic data from an empirically observed distribution. A common approach involves simulating the time-reversal of an Ornstein-Uhlenbeck (OU) process initialized at the true data distribution. Since the score function associated with the OU process is typically unknown, it is approximated using a trained neural network. This approximation, along with finite time simulation, time discretization and statistical approximation, introduce several sources of error whose impact on the generated samples must be carefully understood. Previous analyses have quantified the error between the generated and the true data distributions in terms of Wasserstein distance or Kullback-Leibler (KL) divergence. However, both metrics present limitations: KL divergence requires absolute continuity between distributions, while Wasserstein distance, though more general, leads to error bounds that scale poorly with dimension, rendering them impractical in high-dimensional settings. In this work, we derive an explicit, dimension-free bound on the discrepancy between the generated and the true data distributions. The bound is expressed in terms of a smooth test functional with bounded first and second derivatives. The key novelty lies in the use of this weaker, functional metric to obtain dimension-independent guarantees, at the cost of higher regularity on the test functions. As an application, we formulate and solve a variational problem to minimize the time-discretization error, leading to the derivation of an optimal time-scheduling strategy for the reverse-time diffusion. Interestingly, this scheduler has appeared previously in the literature in a different context; our analysis provides a new justification for its optimality, now grounded in minimizing the discretization bias in generative sampling."}
{"id": "2512.00678", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.00678", "abs": "https://arxiv.org/abs/2512.00678", "authors": ["Braden Scherting", "Otso Ovaskainen", "Tomas Roslin", "David B. Dunson"], "title": "Model-based indicators for co-clustered environments and species communities", "comment": null, "summary": "Accurate biodiversity monitoring is essential for effective environmental policy, yet current practices often rely on arbitrarily defined ecosystems, communities, and ad-hoc indicator species, limiting cost-efficiency and reproducibility. We present a model-based framework that infers ecological sub-communities and corresponding indicators in terms of habitat and species from species survey data, such as large-scale arthropod abundance data used here as example. Environments and species are co-clustered using Bayesian decoupling for Poisson factorization. Latent, hierarchical regression relates observable habitat features to each subcommunity. Additionally, we propose a novel, model-based ranking of indicator species based on the learned subcommunities, generalizing classical approaches. This integrated approach motivates model-based ecosystem classification and indicator species selection, offering a scalable, reproducible pathway for biodiversity monitoring and informed conservation."}
{"id": "2512.00508", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.00508", "abs": "https://arxiv.org/abs/2512.00508", "authors": ["Lan Li", "Shibo Yu", "Yingzhou Wang", "Guodong Li"], "title": "High-dimensional Autoregressive Modeling for Time Series with Hierarchical Structures", "comment": null, "summary": "High-dimensional time series often exhibit hierarchical structures represented by tensors, while statistical methodologies that can effectively exploit the structural information remain limited. We propose a supervised factor modeling framework that accommodates general hierarchical structures by extracting low-dimensional features sequentially in the mode orders that respect the hierarchical structure. Our method can select a small collection of such orders to allow for impurities in the hierarchical structures, yielding interpretable loading matrices that preserve the hierarchical relationships. A practical estimation procedure is proposed, with a hyperparameter selection scheme that identifies a parsimonious set of action orders and interim ranks, thereby revealing the possibly latent hierarchical structures. Theoretically, non-asymptotic error bounds are derived for the proposed estimators in both regression and autoregressive settings. An application to the IPIP-NEO-120 personality panel illustrates superior forecasting performance and clearer structural interpretation compared with existing methods based on tensor decompositions and hierarchical factor analysis."}
{"id": "2512.00919", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00919", "abs": "https://arxiv.org/abs/2512.00919", "authors": ["Dimitri Meunier", "Jakub Wornbard", "Vladimir R. Kostic", "Antoine Moulin", "Alek Fröhlich", "Karim Lounici", "Massimiliano Pontil", "Arthur Gretton"], "title": "Outcome-Aware Spectral Feature Learning for Instrumental Variable Regression", "comment": null, "summary": "We address the problem of causal effect estimation in the presence of hidden confounders using nonparametric instrumental variable (IV) regression. An established approach is to use estimators based on learned spectral features, that is, features spanning the top singular subspaces of the operator linking treatments to instruments. While powerful, such features are agnostic to the outcome variable. Consequently, the method can fail when the true causal function is poorly represented by these dominant singular functions. To mitigate, we introduce Augmented Spectral Feature Learning, a framework that makes the feature learning process outcome-aware. Our method learns features by minimizing a novel contrastive loss derived from an augmented operator that incorporates information from the outcome. By learning these task-specific features, our approach remains effective even under spectral misalignment. We provide a theoretical analysis of this framework and validate our approach on challenging benchmarks."}
{"id": "2512.01041", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01041", "abs": "https://arxiv.org/abs/2512.01041", "authors": ["Ian Miller", "Ann Hyslop", "Colin Decker"], "title": "A Clinical Instrument to Measure Patient Anecdotes in Clinical Trials", "comment": "18 pages, includes Case Report Form, non-seizure outcome measure", "summary": "Clinical trials assessing neurological treatment are challenging due to the diversity of brain function, and the difficulty in quantifying it. Traditional treatment studies in epilepsy use seizure frequency as the primary outcome measure, which may overlooking meaningful improvements in patients' quality of life. This paper introduces the Clinical Instrument for Measuring Patient Anecdotes in Clinical Trials (Clinical IMPACT), a novel tool designed to capture qualitative non-seizure improvement across neurological domains.\n  The Clinical IMPACT incorporates open-ended inquiries that allow participants or caregivers to identify and select anecdotal evidence of their most significant treatment benefits. A blinded panel of experts ranks these anecdotes, facilitating a rigorous statistical analysis using the Wilcoxon Rank-Sum Test to detect treatment efficacy. The approach is resistant to type 1 error, yet comprehensive in its ability to capture real-world effects on quality of life.\n  The potential of the Clinical IMPACT tool to enhance sensitivity while also providing qualitative insights that can inform patients, healthcare providers, and regulatory bodies about treatment effects makes it important to consider in any neurological trial. We describe how it can be used in epilepsy, and advocate for its inclusion as a key secondary endpoint to provide a perspective on non-seizure outcomes, which have previously been challenging to measure, let alone to interpret, even when the clinical trial is positive."}
{"id": "2512.00583", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.00583", "abs": "https://arxiv.org/abs/2512.00583", "authors": ["Zoe Kristin Lange", "Maryam Farhadizadeh", "Holger Dette", "Nadine Binder"], "title": "Testing similarity of competing risks models by comparing transition probabilities", "comment": null, "summary": "Assessing whether two patient populations exhibit comparable event dynamics is essential for evaluating treatment equivalence, pooling data across cohorts, or comparing clinical pathways across hospitals or strategies. We introduce a statistical framework for formally testing the similarity of competing risks models based on transition probabilities, which represent the cumulative risk of each event over time. Our method defines a maximum-type distance between the transition probability matrices of two multistate processes and employs a novel constrained parametric bootstrap test to evaluate similarity under both administrative and random right censoring. We theoretically establish the asymptotic validity and consistency of the bootstrap test. Through extensive simulation studies, we show that our method reliably controls the type I error and achieves higher statistical power than existing intensity-based approaches. Applying the framework to routine clinical data of prostate cancer patients treated with radical prostatectomy, we identify the smallest similarity threshold at which patients with and without prior in-house fusion biopsy exhibit comparable readmission dynamics. The proposed method provides a robust and interpretable tool for quantifying similarity in event history models."}
{"id": "2512.00930", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00930", "abs": "https://arxiv.org/abs/2512.00930", "authors": ["Somangchan Park", "Heesang Ann", "Min-hwan Oh"], "title": "Thompson Sampling for Multi-Objective Linear Contextual Bandit", "comment": "NeurIPS 2025", "summary": "We study the multi-objective linear contextual bandit problem, where multiple possible conflicting objectives must be optimized simultaneously. We propose \\texttt{MOL-TS}, the \\textit{first} Thompson Sampling algorithm with Pareto regret guarantees for this problem. Unlike standard approaches that compute an empirical Pareto front each round, \\texttt{MOL-TS} samples parameters across objectives and efficiently selects an arm from a novel \\emph{effective Pareto front}, which accounts for repeated selections over time. Our analysis shows that \\texttt{MOL-TS} achieves a worst-case Pareto regret bound of $\\widetilde{O}(d^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature vectors, $T$ is the total number of rounds, matching the best known order for randomized linear bandit algorithms for single objective. Empirical results confirm the benefits of our proposed approach, demonstrating improved regret minimization and strong multi-objective performance."}
{"id": "2512.01057", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.01057", "abs": "https://arxiv.org/abs/2512.01057", "authors": ["Yihao Tan", "Marianthi Markatou", "Saptarshi Chakraborty"], "title": "pvEBayes: An R Package for Empirical Bayes Methods in Pharmacovigilance", "comment": null, "summary": "Monitoring the safety of medical products is a core concern of contemporary pharmacovigilance. To support drug safety assessment, Spontaneous Reporting Systems (SRS) collect reports of suspected adverse events of approved medical products offering a critical resource for identifying potential safety concerns that may not emerge during clinical trials. Modern nonparametric empirical Bayes methods are flexible statistical approaches that can accurately identify and estimate the strength of the association between an adverse event and a drug from SRS data. However, there is currently no comprehensive and easily accessible implementation of these methods. Here, we introduce the R package pvEBayes, which implements a suite of nonparametric empirical Bayes methods for pharmacovigilance, along with post-processing tools and graphical summaries for streamlining the application of these methods. Detailed examples are provided to demonstrate the application of the package through analyses of two real-world SRS datasets curated from the publicly available FDA FAERS database."}
{"id": "2512.00627", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.00627", "abs": "https://arxiv.org/abs/2512.00627", "authors": ["Chadi Bsila", "Yiqi Tang", "Kaiwen Wang", "Laurie Heyer"], "title": "Rényi's $α$-divergence variational Bayes for spike-and-slab high-dimensional linear regression", "comment": null, "summary": "Sparse high-dimensional linear regression is a central problem in statistics, where the goal is often variable selection and/or coefficient estimation. We propose a mean-field variational Bayes approximation for sparse regression with spike-and-slab Laplace priors that replaces the standard Kullback-Leibler (KL) divergence objective with the Rényi's $α$ divergence: a one-parameter generalization of KL divergence indexed by $α\\in (0, \\infty) \\setminus \\{1\\}$ that allows flexibility between zero-forcing and mass-covering behavior. We derive coordinate ascent variational inference (CAVI) updates via a second-order delta method and develop a stochastic variational inference algorithm based on a Monte Carlo surrogate Rényi lower bound. In simulations, our two methods perform comparably to state-of-the-art Bayesian variable selection procedures across a range of sparsity configurations and $α$ values for both variable selection and estimation, and our numerical results illustrate how different choices of $α$ can be advantageous in different sparsity configurations."}
{"id": "2512.00979", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00979", "abs": "https://arxiv.org/abs/2512.00979", "authors": ["Victor Saquicela", "Kenneth Palacio-Baus", "Mario Chifla"], "title": "An Approach to Variable Clustering: K-means in Transposed Data and its Relationship with Principal Component Analysis", "comment": "Presented at conference and to appear in the proceedings of the 2025 IEEE Chilean Conference on Electrical, Electronics Engineering, Information and Communication Technologies (ChileCon)", "summary": "Principal Component Analysis (PCA) and K-means constitute fundamental techniques in multivariate analysis. Although they are frequently applied independently or sequentially to cluster observations, the relationship between them, especially when K-means is used to cluster variables rather than observations, has been scarcely explored. This study seeks to address this gap by proposing an innovative method that analyzes the relationship between clusters of variables obtained by applying K-means on transposed data and the principal components of PCA. Our approach involves applying PCA to the original data and K-means to the transposed data set, where the original variables are converted into observations. The contribution of each variable cluster to each principal component is then quantified using measures based on variable loadings. This process provides a tool to explore and understand the clustering of variables and how such clusters contribute to the principal dimensions of variation identified by PCA."}
{"id": "2512.01074", "categories": ["stat.AP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.01074", "abs": "https://arxiv.org/abs/2512.01074", "authors": ["Faharudeen Alhassan", "Hamed Karami", "Amanda Bleichrodt", "James M. Hyman", "Isaac C. H. Fung", "Ruiyan Luo", "Gerardo Chowell"], "title": "COVID-19 Forecasting from U.S. Wastewater Surveillance Data: A Retrospective Multi-Model Study (2022-2024)", "comment": "38 pages, 21 figures", "summary": "Accurate and reliable forecasting models are critical for guiding public health responses and policy decisions during pandemics such as COVID-19. Retrospective evaluation of model performance is essential for improving epidemic forecasting capabilities. In this study, we used COVID-19 wastewater data from CDC's National Wastewater Surveillance System to generate sequential weekly retrospective forecasts for the United States from March 2022 through September 2024, both at the national level and for four major regions (Northeast, Midwest, South, and West). We produced 133 weekly forecasts using 11 models, including ARIMA, generalized additive models (GAM), simple linear regression (SLR), Prophet, and the n-sub-epidemic framework (top-ranked, weighted-ensemble, and unweighted-ensemble variants). Forecast performance was assessed using mean absolute error (MAE), mean squared error (MSE), weighted interval score (WIS), and 95% prediction interval coverage. The n-sub-epidemic unweighted ensembles outperformed all other models at 3-4-week horizons, particularly at the national level and in the Midwest and West. ARIMA and GAM performed best at 1-2-week horizons in most regions, whereas Prophet and SLR consistently underperformed across regions and horizons. These findings highlight the value of region-specific modeling strategies and demonstrate the utility of the n-sub-epidemic framework for real-time outbreak forecasting using wastewater surveillance data."}
{"id": "2512.00631", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.00631", "abs": "https://arxiv.org/abs/2512.00631", "authors": ["Jonas Andersson", "Dimitris Karlis"], "title": "Maximum Likelihood Estimation of the Vector AutoRegressive To Anything (VARTA) model", "comment": "23 pages, 4 figures", "summary": "The literature on multivariate time series is, largely, limited to either models based on the multivariate Gaussian distribution or models specifically developed for a given application. In this paper we develop a general approach which is based on an underlying, unobserved, Gaussian Vector Autoregressive (VAR) model. Using a transformation, we can capture the time dynamics as well as the distributional properties of a multivariate time series. The model is called the Vector AutoRegressive To Anyting (VARTA) model and was originally presented by Biller and Nelson (2003) who used it for the purpose of simulation. In this paper we derive a maximum likelihood estimator for the model and investigate its performance. We also provide diagnostic analysis and how to compute the predictive distribution. The proposed approach can provide better estimates about the forecasting distributions which can be of every kind not necessarily Gaussian distributions as for the standard VAR models."}
{"id": "2512.01097", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01097", "abs": "https://arxiv.org/abs/2512.01097", "authors": ["Zachary Terner", "Alexander Petersen", "Yuedong Wang"], "title": "Discriminative classification with generative features: bridging Naive Bayes and logistic regression", "comment": null, "summary": "We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance."}
{"id": "2512.01639", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.01639", "abs": "https://arxiv.org/abs/2512.01639", "authors": ["Conor Rosato", "Joshua Murphy", "Siân E. Jenkins", "Paul Horridge", "Alessandro Varsi", "Martyn Bull", "Alessandro Gerada", "Alex Howard", "Veronica Bowman", "Simon Maskell"], "title": "Improved Disease Outbreak Detection from Out-of-sequence measurements Using Markov-switching Fixed-lag Particle Filters", "comment": "23 Pages", "summary": "Particle filters (PFs) have become an essential tool for disease surveillance, as they can estimate hidden epidemic states in nonlinear and non-Gaussian models. In epidemic modelling, population dynamics may be governed by distinct regimes such as endemic or outbreak phases which can be represented using Markov-switching state-space models. In many real-world surveillance systems, data often arrives with delays or in the wrong temporal order, producing out-of-sequence (OOS) measurements that pertain to past time points rather than the current one. While existing PF methods can incorporate OOS measurements through particle reweighting, these approaches are limited in their ability to fully adjust past latent trajectories. To address this, we introduce a Markov-switching fixed-lag particle filter (FL-PF) that resimulates particle trajectories within a user-specified lag window, allowing OOS measurements to retroactively update both state and model estimates. By explicitly reevaluating historical samples, the FL-PF improves the accuracy and timeliness of outbreak detection and reduces false alarms. We also show how to compute the log-likelihood within the FL-PF framework, enabling parameter estimation using Sequential Monte Carlo squared (SMC$^2$). Together, these contributions extend the applicability of PFs to surveillance systems where retrospective data are common, offering a more robust framework for monitoring disease outbreaks and parameter inference."}
{"id": "2512.00688", "categories": ["stat.ME", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00688", "abs": "https://arxiv.org/abs/2512.00688", "authors": ["Weijie Peng", "Nanbing Li", "Jin Luo", "Shuai Wang", "Yihui Li", "Jun Fang", "Yun", "Liang"], "title": "NOVA: Coordinated Test Selection and Bayes-Optimized Constrained Randomization for Accelerated Coverage Closure", "comment": null, "summary": "Functional verification relies on large simulation-based regressions. Traditional test selection relies on static test features and overlooks actual coverage behavior, wasting substantial simulation time, while constrained random stimuli generation depends on manually crafted distributions that are difficult to design and often ineffective. We present NOVA, a framework that coordinates coverage-aware test selection with Bayes-optimized constrained randomization. NOVA extracts fine-grained coverage features to filter redundant tests and modifies the constraint solver to expose parameterized decision strategies whose settings are tuned via Bayesian optimization to maximize coverage growth. Across multiple RTL designs, NOVA achieves up to a 2.82$\\times$ coverage convergence speedup without requiring human-crafted heuristics."}
{"id": "2512.01172", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.01172", "abs": "https://arxiv.org/abs/2512.01172", "authors": ["Jiajia Yu", "Junghwan Lee", "Yao Xie", "Xiuyuan Cheng"], "title": "High-dimensional Mean-Field Games by Particle-based Flow Matching", "comment": null, "summary": "Mean-field games (MFGs) study the Nash equilibrium of systems with a continuum of interacting agents, which can be formulated as the fixed-point of optimal control problems. They provide a unified framework for a variety of applications, including optimal transport (OT) and generative models. Despite their broad applicability, solving high-dimensional MFGs remains a significant challenge due to fundamental computational and analytical obstacles. In this work, we propose a particle-based deep Flow Matching (FM) method to tackle high-dimensional MFG computation. In each iteration of our proximal fixed-point scheme, particles are updated using first-order information, and a flow neural network is trained to match the velocity of the sample trajectories in a simulation-free manner. Theoretically, in the optimal control setting, we prove that our scheme converges to a stationary point sublinearly, and upgrade to linear (exponential) convergence under additional convexity assumptions. Our proof uses FM to induce an Eulerian coordinate (density-based) from a Lagrangian one (particle-based), and this also leads to certain equivalence results between the two formulations for MFGs when the Eulerian solution is sufficiently regular. Our method demonstrates promising performance on non-potential MFGs and high-dimensional OT problems cast as MFGs through a relaxed terminal-cost formulation."}
{"id": "2512.01927", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.01927", "abs": "https://arxiv.org/abs/2512.01927", "authors": ["Steven D. Barnett", "Robert B. Gramacy", "Lauren J. Beesley", "Dave Osthus", "Yifan Huang", "Fan Guo", "Daniel B. Reisenfeld"], "title": "Bayesian Statistical Inversion for High-Dimensional Computer Model Output and Spatially Distributed Counts", "comment": null, "summary": "Data collected by the Interstellar Boundary Explorer (IBEX) satellite, recording heliospheric energetic neutral atoms (ENAs), exhibit a phenomenon that has caused space scientists to revise hypotheses about the physical processes, and computer simulations under those models, in play at the boundary of our solar system. Evaluating the fit of these computer models involves tuning their parameters to observational data from IBEX. This would be a classic (Bayesian) inverse problem if not for three challenges: (1) the computer simulations are slow, limiting the size of campaigns of runs; so (2) surrogate modeling is essential, but outputs are high-resolution images, thwarting conventional methods; and (3) IBEX observations are counts, whereas most inverse problem techniques assume Gaussian field data. To fill that gap we propose a novel approach to Bayesian inverse problems coupling a Poisson response with a sparse Gaussian process surrogate using the Vecchia approximation. We demonstrate the capabilities of our proposed framework, which compare favorably to alternatives, through multiple simulated examples in terms of recovering \"true\" computer model parameters and accurate out-of-sample prediction. We then apply this new technology to IBEX satellite data and associated computer models developed at Los Alamos National Laboratory."}
{"id": "2512.00895", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.00895", "abs": "https://arxiv.org/abs/2512.00895", "authors": ["Jin Hyung Lee", "Ben Seiyon Lee"], "title": "A Scalable Variational Bayes Approach for Fitting Non-Conjugate Spatial Generalized Linear Mixed Models via Basis Expansions", "comment": "26 pages, 5 figures, 4 tables", "summary": "Large spatial datasets with non-Gaussian responses are increasingly common in environmental monitoring, ecology, and remote sensing, yet scalable Bayesian inference for such data remains challenging. Markov chain Monte Carlo (MCMC) methods are often prohibitive for large datasets, and existing variational Bayes methods rely on conjugacy or strong approximations that limit their applicability and can underestimate posterior variances. We propose a scalable variational framework that incorporates semi-implicit variational inference (SIVI) with basis representations of spatial generalized linear mixed models (SGLMMs), which may not have conjugacy. Our approach accommodates gamma, negative binomial, Poisson, Bernoulli, and Gaussian responses on continuous spatial domains. Across 20 simulation scenarios with 50,000 locations, SIVI achieves predictive accuracy and posterior distributions comparable to Metropolis-Hastings and Hamiltonian Monte Carlo while providing notable computational speedups. Applications to MODIS land surface temperature and Blue Jay abundance further demonstrate the utility of the approach for large non-Gaussian spatial datasets."}
{"id": "2512.01231", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01231", "abs": "https://arxiv.org/abs/2512.01231", "authors": ["Samet Demir", "Zafer Dogan"], "title": "Implicitly Normalized Online PCA: A Regularized Algorithm with Exact High-Dimensional Dynamics", "comment": "34 pages 9 figures", "summary": "Many online learning algorithms, including classical online PCA methods, enforce explicit normalization steps that discard the evolving norm of the parameter vector. We show that this norm can in fact encode meaningful information about the underlying statistical structure of the problem, and that exploiting this information leads to improved learning behavior. Motivated by this principle, we introduce Implicitly Normalized Online PCA (INO-PCA), an online PCA algorithm that removes the unit-norm constraint and instead allows the parameter norm to evolve dynamically through a simple regularized update. We prove that in the high-dimensional limit the joint empirical distribution of the estimate and the true component converges to a deterministic measure-valued process governed by a nonlinear PDE. This analysis reveals that the parameter norm obeys a closed-form ODE coupled with the cosine similarity, forming an internal state variable that regulates learning rate, stability, and sensitivity to signal-to-noise ratio (SNR). The resulting dynamics uncover a three-way relationship between the norm, SNR, and optimal step size, and expose a sharp phase transition in steady-state performance. Both theoretically and experimentally, we show that INO-PCA consistently outperforms Oja's algorithm and adapts rapidly in non-stationary environments. Overall, our results demonstrate that relaxing norm constraints can be a principled and effective way to encode and exploit problem-relevant information in online learning algorithms."}
{"id": "2512.01965", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.01965", "abs": "https://arxiv.org/abs/2512.01965", "authors": ["Colin Bobocea", "Yves Atchadé"], "title": "Predicting Onsets and Dry Spells of the West African Monsoon Season Using Machine Learning Methods", "comment": null, "summary": "The beginning of the rainy season and the occurrence of dry spells in West Africa is notoriously difficult to predict, however these are the key indicators farmers use to decide when to plant crops, having a major influence on their overall yield. While many studies have shown correlations between global sea surface temperatures and characteristics of the West African monsoon season, there are few that effectively implementing this information into machine learning (ML) prediction models. In this study we investigated the best ways to define our target variables, onset and dry spell, and produced methods to predict them for upcoming seasons using sea surface temperature teleconnections. Defining our target variables required the use of a combination of two well known definitions of onset. We then applied custom statistical techniques -- like total variation regularization and predictor selection -- to the two models we constructed, the first being a linear model and the other an adaptive-threshold logistic regression model. We found mixed results for onset prediction, with spatial verification showing signs of significant skill, while temporal verification showed little to none. For dry spell though, we found significant accuracy through the analysis of multiple binary classification metrics. These models overcome some limitations that current approaches have, such as being computationally intensive and needing bias correction. We also introduce this study as a framework to use ML methods for targeted prediction of certain weather phenomenon using climatologically relevant variables. As we apply ML techniques to more problems, we see clear benefits for fields like meteorology and lay out a few new directions for further research."}
{"id": "2512.00901", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.00901", "abs": "https://arxiv.org/abs/2512.00901", "authors": ["Mingzhou Deng", "Yan Fu"], "title": "Grouped Competition Test with Unified False Discovery Rate Control", "comment": null, "summary": "This paper discusses several p-value-free multiple hypothesis testing methods proposed in recent years and organizes them by introducing a unified framework termed competition test. Although existing competition tests are effective in controlling the False Discovery Rate (FDR), they struggle with handling data with strong heterogeneity or dependency structures. Based on this framework, the paper proposes a novel approach that applies a corrected competition procedure to group data with certain structure, and then integrates the results from each group. Using the favorable properties of competition test, the paper proposes a theorem demonstrating that this approach controls the global FDR. We further show that although the correction parameters may lead to a slight loss in power, such loss is typically minimal. Through simulation experiments and mass spectrometry data analysis, we illustrate the flexibility and efficacy of our approach."}
{"id": "2512.01546", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01546", "abs": "https://arxiv.org/abs/2512.01546", "authors": ["Yuma Ichikawa", "Yudai Fujimoto", "Akira Sakai"], "title": "LPCD: Unified Framework from Layer-Wise to Submodule Quantization", "comment": "21 pages, 4 figures", "summary": "Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches."}
{"id": "2512.00338", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00338", "abs": "https://arxiv.org/abs/2512.00338", "authors": ["Dehao Dai", "Yunyi Zhang"], "title": "On Statistical Inference for High-Dimensional Binary Time Series", "comment": "55 pages, 6 figures", "summary": "The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method."}
{"id": "2512.00916", "categories": ["stat.ME", "q-fin.RM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00916", "abs": "https://arxiv.org/abs/2512.00916", "authors": ["Sotirios D. Nikolopoulos"], "title": "An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts", "comment": "43 pages, 16 figures, 16 tables. Includes simulations, empirical application, and supplementary appendices", "summary": "Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts."}
{"id": "2512.01708", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01708", "abs": "https://arxiv.org/abs/2512.01708", "authors": ["Ghita Fassy El Fehri", "Aurélien Bellet", "Philippe Bastien"], "title": "Differentially Private and Federated Structure Learning in Bayesian Networks", "comment": null, "summary": "Learning the structure of a Bayesian network from decentralized data poses two major challenges: (i) ensuring rigorous privacy guarantees for participants, and (ii) avoiding communication costs that scale poorly with dimensionality. In this work, we introduce Fed-Sparse-BNSL, a novel federated method for learning linear Gaussian Bayesian network structures that addresses both challenges. By combining differential privacy with greedy updates that target only a few relevant edges per participant, Fed-Sparse-BNSL efficiently uses the privacy budget while keeping communication costs low. Our careful algorithmic design preserves model identifiability and enables accurate structure estimation. Experiments on synthetic and real datasets demonstrate that Fed-Sparse-BNSL achieves utility close to non-private baselines while offering substantially stronger privacy and communication efficiency."}
{"id": "2512.01003", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.01003", "abs": "https://arxiv.org/abs/2512.01003", "authors": ["William H. Press"], "title": "Correlated Confounding Variables Are Not Easily Controlled for in Large Survey Research", "comment": "19 pages, 6 figures", "summary": "Results in epidemiology and social science often require the removal of confounding effects from measurements of the pairwise correlation of variables in survey data. This is typically accomplished by some variant of linear regression (e.g., ``logistic\" or ``Cox proportional\"). But, knowing whether all possible confounders have been identified, or are even visible (not latent), is in general impossible. Here, we exhibit two examples that frame the issue. The first example proposes a highly unlikely hypothesis on drug use, draws data from a large, respected survey, and succeeds in ``proving\" the implausible hypothesis, despite regressing out more than 20 confounding variables. The second constructs a ``metamodel\" in which a single (by hypothesis unmeasurable) latent variable affects many mutually correlated confounders. From simulations, we derive formulas for the magnitude of spurious association that persists even as increasing numbers of confounders are regressed out. The intent of these examples is for them to serve as cautionary tales."}
{"id": "2512.00996", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.00996", "abs": "https://arxiv.org/abs/2512.00996", "authors": ["Raymond J. Hinton,", "Pepa Ramírez Cobo", "Brani Vidakovic"], "title": "The Dual Wavelet Spectra: An Alternative Perspective on Hurst Exponent Estimation with Application to Mammogram Classification", "comment": "20 pages, 6 figures", "summary": "The wavelet spectra is a common starting point for estimating the Hurst exponent of a self-similar signal using wavelet-based techniques. The decay of the $\\log_2$ average energy of the detail wavelet coefficients as a function of the level of signal decomposition can be used to construct estimators for this parameter. In this paper, we expand on previous work which introduced the ``dual\" wavelet spectra, where decomposition levels are instead treated as a function of energy values, and propose a relationship between its slope and the Hurst exponent by inverting the standard wavelet spectra, thereby creating a new estimator. The effectiveness of this estimator and its sensitivity to several settings are demonstrated through a simulation study. Finally, we show how the technique performs as a feature extraction method by applying it to the task of detecting the presence of breast cancer in mammogram images. Dual spectra wavelet features had a statistically significant effect on the log-odds of Cancer."}
{"id": "2512.01716", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.01716", "abs": "https://arxiv.org/abs/2512.01716", "authors": ["Louis Lacoste", "Pierre Barbillon", "Sophie Donnet"], "title": "Common Structure Discovery in Collections of Bipartite Networks: Application to Pollination Systems", "comment": null, "summary": "Bipartite networks are widely used to encode the ecological interactions. Being able to compare the organization of bipartite networks is a first step toward a better understanding of how environmental factors shape community structure and resilience. Yet current methods for structure detection in bipartite networks overlook shared patterns across collections of networks. We introduce the \\emph{colBiSBM}, a family of probabilistic models for collections of bipartite networks that extends the classical Latent Block Model (LBM). The proposed framework assumes that networks are independent realizations of a shared mesoscale structure, encoded through common inter-block connectivity parameters. We establish identifiability conditions for the different variants of \\emph{colBiSBM} and develop a variational EM algorithm for parameter estimation, coupled with an adaptation of the Integrated Classification Likelihood (ICL) criterion for model selection. We demonstrate how our approach can be used to classify networks based on their topology or organization. Simulation studies highlight the ability of \\emph{colBiSBM} to recover common structures, improve clustering performance, and enhance link prediction by borrowing strength across networks. An application to plant--pollinator networks highlights how the method uncovers shared ecological roles and partitions networks into sub-collections with similar connectivity patterns. These results illustrate the methodological and practical advantages of joint modeling over separate network analyses in the study of bipartite systems."}
{"id": "2512.01508", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.01508", "abs": "https://arxiv.org/abs/2512.01508", "authors": ["Álvaro Briz-Redón", "Ana Corberán-Vallet", "Adina Iftimi", "Carmen Íñiguez"], "title": "A mixture of distributed lag non-linear models to account for spatially heterogeneous exposure-lag-response associations", "comment": "22 pages, 8 figures", "summary": "Environmental exposures, such as air pollution and extreme temperatures, have complex effects on human health. These effects are often characterized by non-linear exposure-lag-response relationships and delayed impacts over time. Accurately capturing these dynamics is crucial for informing public health interventions. The Distributed Lag Non-Linear Model (DLNM) is a flexible statistical framework for estimating such effects in epidemiological research. However, standard DLNM implementations typically assume a homogeneous exposure-lag-response association across the study region, overlooking potential spatial heterogeneity, which can lead to biased risk estimates. To address this limitation, we introduce DLNM-Clust: a novel mixture of DLNMs that extends the traditional DLNM. Within a Bayesian framework, DLNM-Clust probabilistically assigns each geographic unit to one of $C$ latent spatial clusters, each of which is defined by a distinct DLNM specification. This approach allows capturing both common patterns and singular deviations in the exposure-lag-response surface. We demonstrate the method using municipality-level time-series data on the relationship between air pollution and the incidence of COVID-19 in Belgium. Our results emphasize the importance of spatially aware modeling strategies in environmental epidemiology, facilitating region-specific risk assessment and supporting the development of targeted public health initiatives."}
{"id": "2512.01003", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.01003", "abs": "https://arxiv.org/abs/2512.01003", "authors": ["William H. Press"], "title": "Correlated Confounding Variables Are Not Easily Controlled for in Large Survey Research", "comment": "19 pages, 6 figures", "summary": "Results in epidemiology and social science often require the removal of confounding effects from measurements of the pairwise correlation of variables in survey data. This is typically accomplished by some variant of linear regression (e.g., ``logistic\" or ``Cox proportional\"). But, knowing whether all possible confounders have been identified, or are even visible (not latent), is in general impossible. Here, we exhibit two examples that frame the issue. The first example proposes a highly unlikely hypothesis on drug use, draws data from a large, respected survey, and succeeds in ``proving\" the implausible hypothesis, despite regressing out more than 20 confounding variables. The second constructs a ``metamodel\" in which a single (by hypothesis unmeasurable) latent variable affects many mutually correlated confounders. From simulations, we derive formulas for the magnitude of spurious association that persists even as increasing numbers of confounders are regressed out. The intent of these examples is for them to serve as cautionary tales."}
{"id": "2512.01819", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01819", "abs": "https://arxiv.org/abs/2512.01819", "authors": ["Cencheng Shen", "Yuexiao Dong", "Carey E. Priebe"], "title": "Decision Tree Embedding by Leaf-Means", "comment": "9 pages", "summary": "Decision trees and random forest remain highly competitive for classification on medium-sized, standard datasets due to their robustness, minimal preprocessing requirements, and interpretability. However, a single tree suffers from high estimation variance, while large ensembles reduce this variance at the cost of substantial computational overhead and diminished interpretability. In this paper, we propose Decision Tree Embedding (DTE), a fast and effective method that leverages the leaf partitions of a trained classification tree to construct an interpretable feature representation. By using the sample means within each leaf region as anchor points, DTE maps inputs into an embedding space defined by the tree's partition structure, effectively circumventing the high variance inherent in decision-tree splitting rules. We further introduce an ensemble extension based on additional bootstrap trees, and pair the resulting embedding with linear discriminant analysis for classification. We establish several population-level theoretical properties of DTE, including its preservation of conditional density under mild conditions and a characterization of the resulting classification error. Empirical studies on synthetic and real datasets demonstrate that DTE strikes a strong balance between accuracy and computational efficiency, outperforming or matching random forest and shallow neural networks while requiring only a fraction of their training time in most cases. Overall, the proposed DTE method can be viewed either as a scalable decision tree classifier that improves upon standard split rules, or as a neural network model whose weights are learned from tree-derived anchor points, achieving an intriguing integration of both paradigms."}
{"id": "2512.01716", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.01716", "abs": "https://arxiv.org/abs/2512.01716", "authors": ["Louis Lacoste", "Pierre Barbillon", "Sophie Donnet"], "title": "Common Structure Discovery in Collections of Bipartite Networks: Application to Pollination Systems", "comment": null, "summary": "Bipartite networks are widely used to encode the ecological interactions. Being able to compare the organization of bipartite networks is a first step toward a better understanding of how environmental factors shape community structure and resilience. Yet current methods for structure detection in bipartite networks overlook shared patterns across collections of networks. We introduce the \\emph{colBiSBM}, a family of probabilistic models for collections of bipartite networks that extends the classical Latent Block Model (LBM). The proposed framework assumes that networks are independent realizations of a shared mesoscale structure, encoded through common inter-block connectivity parameters. We establish identifiability conditions for the different variants of \\emph{colBiSBM} and develop a variational EM algorithm for parameter estimation, coupled with an adaptation of the Integrated Classification Likelihood (ICL) criterion for model selection. We demonstrate how our approach can be used to classify networks based on their topology or organization. Simulation studies highlight the ability of \\emph{colBiSBM} to recover common structures, improve clustering performance, and enhance link prediction by borrowing strength across networks. An application to plant--pollinator networks highlights how the method uncovers shared ecological roles and partitions networks into sub-collections with similar connectivity patterns. These results illustrate the methodological and practical advantages of joint modeling over separate network analyses in the study of bipartite systems."}
{"id": "2512.01157", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01157", "abs": "https://arxiv.org/abs/2512.01157", "authors": ["William Stewart", "Carly L. Brantner", "Elizabeth A. Stuart", "Laine Thomas"], "title": "Weight a Minute: Understanding Variability in PATE Estimates Across Target Populations", "comment": null, "summary": "Clinical study populations often differ meaningfully from the broader populations to which results are intended to generalize. Weighting methods such as inverse probability of sampling weights (IPSW) reweight study participants to resemble a target population, but the accuracy of these estimates depends heavily on how well the chosen population represents the population of substantive interest. We conduct a simulation study grounded in empirical covariate distributions from several real-world data sources spanning a continuum from highly selective to broadly inclusive populations. Using treatment effect scenarios with varying levels of effect modification, we evaluate IPSW estimators of the population average treatment effect (PATE) across multiple candidate target populations. We quantify the bias that arises when the dataset used to operationalize the target population differs from the intended inference population, even when IPSW is correctly specified. Our results show that bias increases systematically as target populations diverge from a well-representative population, and that weighting to a poorly aligned target can introduce more bias than not weighting at all. These findings highlight that selecting an appropriate target population dataset is a critical design choice for valid generalization."}
{"id": "2512.01820", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.01820", "abs": "https://arxiv.org/abs/2512.01820", "authors": ["Valentin de Bortoli", "Romuald Elie", "Anna Kazeykina", "Zhenjie Ren", "Jiacheng Zhang"], "title": "Dimension-free error estimate for diffusion model and optimal scheduling", "comment": null, "summary": "Diffusion generative models have emerged as powerful tools for producing synthetic data from an empirically observed distribution. A common approach involves simulating the time-reversal of an Ornstein-Uhlenbeck (OU) process initialized at the true data distribution. Since the score function associated with the OU process is typically unknown, it is approximated using a trained neural network. This approximation, along with finite time simulation, time discretization and statistical approximation, introduce several sources of error whose impact on the generated samples must be carefully understood. Previous analyses have quantified the error between the generated and the true data distributions in terms of Wasserstein distance or Kullback-Leibler (KL) divergence. However, both metrics present limitations: KL divergence requires absolute continuity between distributions, while Wasserstein distance, though more general, leads to error bounds that scale poorly with dimension, rendering them impractical in high-dimensional settings. In this work, we derive an explicit, dimension-free bound on the discrepancy between the generated and the true data distributions. The bound is expressed in terms of a smooth test functional with bounded first and second derivatives. The key novelty lies in the use of this weaker, functional metric to obtain dimension-independent guarantees, at the cost of higher regularity on the test functions. As an application, we formulate and solve a variational problem to minimize the time-discretization error, leading to the derivation of an optimal time-scheduling strategy for the reverse-time diffusion. Interestingly, this scheduler has appeared previously in the literature in a different context; our analysis provides a new justification for its optimality, now grounded in minimizing the discretization bias in generative sampling."}
{"id": "2512.01162", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01162", "abs": "https://arxiv.org/abs/2512.01162", "authors": ["Genshiro Kitagawa"], "title": "Gaussian Process State-Space Modeling and Particle Filtering for Time Series Decomposition and Nonlinear Signal Extraction", "comment": "18 pages, 5 tables, 16 figures", "summary": "Gaussian-process state-space models (GP-SSMs) provide a flexible nonparametric alternative for modeling time-series dynamics that are nonlinear or difficult to specify parametrically. While the Kalman filter is effective for linear-Gaussian trend and seasonal components, many real-world systems require more expressive representations. GP-SSMs address this need by learning transition functions directly from data, while particle filtering enables Bayesian state estimation even when posterior distributions deviate from Gaussianity. This paper develops a particle-filtering framework for GP-SSM inference and compares its performance with the Kalman filter in trend extraction and seasonal adjustment. We further evaluate nonlinear signal-extraction tasks, demonstrating that GP-SSMs can recover latent states under sharp or asymmetric dynamics. The results highlight the utility of combining GP modeling with sequential Monte Carlo methods for complex time-series analysis."}
{"id": "2512.01920", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01920", "abs": "https://arxiv.org/abs/2512.01920", "authors": ["Miguel A. Mendez"], "title": "Fundamentals of Regression", "comment": "Chapter 2 from Machine Learning for Fluid Dynamics (ISBN 978-2875162090). Based on the VKI-ULB lecture series ''Machine Learning for Fluid Dynamics,'' held in Brussels in February 2022", "summary": "This chapter opens with a review of classic tools for regression, a subset of machine learning that seeks to find relationships between variables. With the advent of scientific machine learning this field has moved from a purely data-driven (statistical) formalism to a constrained or ``physics-informed'' formalism, which integrates physical knowledge and methods from traditional computational engineering. In the first part, we introduce the general concepts and the statistical flavor of regression versus other forms of curve fitting. We then move to an overview of traditional methods from machine learning and their classification and ways to link these to traditional computational science. Finally, we close with a note on methods to combine machine learning and numerical methods for physics"}
{"id": "2512.01279", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01279", "abs": "https://arxiv.org/abs/2512.01279", "authors": ["Yutong Zhang", "Xiao Liu"], "title": "The Dynamical Model Representation of Convolution-Generated Spatio-Temporal Gaussian Processes and Its Applications", "comment": null, "summary": "Convolution-generated space-time models yield an important class of non-separable stationary Gaussian Processes (GP) through a sequence of convolution operations, in both space and time, on spatially correlated Brownian motion with a Gaussian convolution kernel. Because of its solid connection to stochastic partial differential equations, such a modeling approach offers strong physical interpretations when it is applied to scientific and engineering processes. In this paper, we obtain a new dynamical model representation for convolution-generated spatio-temporal GP. In particular, an infinite-dimensional linear state-space representation is firstly obtained where the state transition is governed by a stochastic differential equation (SDE) whose solution has the same space-time covariance as the original convolution-generated process. Then, using the Galerkin's method, a finite-dimension approximation to the infinite-dimensional SDE is obtained, yielding a dynamical model with finite states that facilitates the computation and parameter estimation. The space-time covariance of the approximated dynamical model is obtained, and the error between the approximate and exact covariance matrices is quantified. We investigate the performance of the proposed model through a simulation-based study, and apply the approach to a real case study utilizing the remote-sensing aerosol data during the recent 2025 Los Angeles wildfire. The modeling capability of the proposed approach has been well demonstrated, and the proposed approach is found particularly effective in monitoring the first-order time derivative of the underlying space-time process, making it a good candidate for process modeling, monitoring and anomaly detection problems. Computer code and data have been made publicly available."}
{"id": "2512.00175", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00175", "abs": "https://arxiv.org/abs/2512.00175", "authors": ["Helen Guo", "Elizabeth L. Ogburn", "Ilya Shpitser"], "title": "Comparing Two Proxy Methods for Causal Identification", "comment": "10 pages; 6 figures", "summary": "Identifying causal effects in the presence of unmeasured variables is a fundamental challenge in causal inference, for which proxy variable methods have emerged as a powerful solution. We contrast two major approaches in this framework: (1) bridge equation methods, which leverage solutions to integral equations to recover causal targets, and (2) array decomposition methods, which recover latent factors composing counterfactual quantities by exploiting unique determination of eigenspaces. We compare the model restrictions underlying these two approaches and provide insight into implications of the underlying assumptions, clarifying the scope of applicability for each method."}
{"id": "2512.01301", "categories": ["stat.ME", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.01301", "abs": "https://arxiv.org/abs/2512.01301", "authors": ["Jeshwanth Mohan", "Bharath Ramsundar", "Sandya Subramanian"], "title": "Inferring Dynamic Hidden Graph Structure in Heterogeneous Correlated Time Series", "comment": "4 pages, 1 figure, Presented at BayLearn 2025", "summary": "Modeling heterogeneous correlated time series requires the ability to learn hidden dynamic relationships between component time series with possibly varying periodicities and generative processes. To address this challenge, we formulate and evaluate a windowed variance-correlation metric (WVC) designed to quantify time-varying correlations between signals. This method directly recovers hidden relationships in an specified time interval as a weighted adjacency matrix, consequently inferring hidden dynamic graph structure. On simulated data, our method captures correlations that other methods miss. The proposed method expands the ability to learn dynamic graph structure between significantly different signals within a single cohesive dynamical graph model."}
{"id": "2512.00338", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00338", "abs": "https://arxiv.org/abs/2512.00338", "authors": ["Dehao Dai", "Yunyi Zhang"], "title": "On Statistical Inference for High-Dimensional Binary Time Series", "comment": "55 pages, 6 figures", "summary": "The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method."}
{"id": "2512.01341", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01341", "abs": "https://arxiv.org/abs/2512.01341", "authors": ["Hua Liu", "Boyi Hu", "Jinhong You", "Jiguo Cao"], "title": "Convolution-smoothing based locally sparse estimation for functional quantile regression", "comment": "The main manuscript includes 26 pages, 1 table and 3 figures", "summary": "Motivated by an application to study the impact of temperature, precipitation and irrigation on soybean yield, this article proposes a sparse semi-parametric functional quantile model. The model is called ``sparse'' because the functional coefficients are only nonzero in the local time region where the functional covariates have significant effects on the response under different quantile levels. To tackle the computational and theoretical challenges in optimizing the quantile loss function added with a concave penalty, we develop a novel Convolution-smoothing based Locally Sparse Estimation (CLoSE) method, to do three tasks in one step, including selecting significant functional covariates, identifying the nonzero region of functional coefficients to enhance the interpretability of the model and estimating the functional coefficients. We establish the functional oracle properties and simultaneous confidence bands for the estimated functional coefficients, along with the asymptotic normality for the estimated parameters. In addition, because it is difficult to estimate the conditional density function given the scalar and functional covariates, we propose the split wild bootstrap method to construct the confidence interval of the estimated parameters and simultaneous confidence band for the functional coefficients. We also establish the consistency of the split wild bootstrap method. The finite sample performance of the proposed CLoSE method is assessed with simulation studies. The proposed model and estimation procedure are also illustrated by identifying the active time regions when the daily temperature influences the soybean yield."}
{"id": "2512.00916", "categories": ["stat.ME", "q-fin.RM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00916", "abs": "https://arxiv.org/abs/2512.00916", "authors": ["Sotirios D. Nikolopoulos"], "title": "An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts", "comment": "43 pages, 16 figures, 16 tables. Includes simulations, empirical application, and supplementary appendices", "summary": "Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts."}
{"id": "2512.01423", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01423", "abs": "https://arxiv.org/abs/2512.01423", "authors": ["Qi Kuang", "Bowen Gang", "Yin Xia"], "title": "Active Hypothesis Testing under Computational Budgets with Applications to GWAS and LLM", "comment": null, "summary": "In large-scale hypothesis testing, computing exact $p$-values or $e$-values is often resource-intensive, creating a need for budget-aware inferential methods. We propose a general framework for active hypothesis testing that leverages inexpensive auxiliary statistics to allocate a global computational budget. For each hypothesis, our data-adaptive procedure probabilistically decides whether to compute the exact test statistic or a transformed proxy, guaranteeing a valid $p$-value or $e$-value while satisfying the budget constraint in expectation. Theoretical guarantees are established for our constructions, showing that the procedure achieves optimality for $e$-values and for $p$-values under independence, and admissibility for $p$-values under general dependence. Empirical results from simulations and two real-world applications, including a large-scale genome-wide association study (GWAS) and a clinical prediction task leveraging large language models (LLM), demonstrate that our framework improves statistical efficiency under fixed resource limits."}
{"id": "2512.01790", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.01790", "abs": "https://arxiv.org/abs/2512.01790", "authors": ["Bernard Bercu", "Luis Fredes", "Eméric Gbaguidi"], "title": "An hybrid stochastic Newton algorithm for logistic regression", "comment": null, "summary": "In this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk."}
{"id": "2512.01450", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01450", "abs": "https://arxiv.org/abs/2512.01450", "authors": ["Matteo Mori", "Laura Anderlucci"], "title": "Model-Based Clustering of Functional Data Via Random Projection Ensembles", "comment": null, "summary": "Clustering functional data is a challenging task due to intrinsic infinite-dimensionality and the need for stable, data-adaptive partitioning. In this work, we propose a clustering framework based on Random Projections, which simultaneously performs dimensionality reduction and generates multiple stochastic representations of the original functions. Each projection is clustered independently, and the resulting partitions are then aggregated through an ensemble consensus procedure, enhancing robustness and mitigating the influence of any single projection. To focus on the most informative representations, projections are ranked according to clustering quality criteria, and only a selected subset is retained. In particular, we adopt Gaussian Mixture Models as base clusterers and employ the Kullback-Leibler divergence to order the random projections; these choices enable fast computation and eliminate the need to specify the number of clusters a priori. The performance of the proposed methodology is assessed through an extensive simulation study and two real-data applications, one from spectroscopy data for food authentication and one from log-periodograms of speech recording; the obtained results suggest that the proposal represents an effective tool for the clustering of functional data."}
{"id": "2512.01508", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.01508", "abs": "https://arxiv.org/abs/2512.01508", "authors": ["Álvaro Briz-Redón", "Ana Corberán-Vallet", "Adina Iftimi", "Carmen Íñiguez"], "title": "A mixture of distributed lag non-linear models to account for spatially heterogeneous exposure-lag-response associations", "comment": "22 pages, 8 figures", "summary": "Environmental exposures, such as air pollution and extreme temperatures, have complex effects on human health. These effects are often characterized by non-linear exposure-lag-response relationships and delayed impacts over time. Accurately capturing these dynamics is crucial for informing public health interventions. The Distributed Lag Non-Linear Model (DLNM) is a flexible statistical framework for estimating such effects in epidemiological research. However, standard DLNM implementations typically assume a homogeneous exposure-lag-response association across the study region, overlooking potential spatial heterogeneity, which can lead to biased risk estimates. To address this limitation, we introduce DLNM-Clust: a novel mixture of DLNMs that extends the traditional DLNM. Within a Bayesian framework, DLNM-Clust probabilistically assigns each geographic unit to one of $C$ latent spatial clusters, each of which is defined by a distinct DLNM specification. This approach allows capturing both common patterns and singular deviations in the exposure-lag-response surface. We demonstrate the method using municipality-level time-series data on the relationship between air pollution and the incidence of COVID-19 in Belgium. Our results emphasize the importance of spatially aware modeling strategies in environmental epidemiology, facilitating region-specific risk assessment and supporting the development of targeted public health initiatives."}
{"id": "2512.01513", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01513", "abs": "https://arxiv.org/abs/2512.01513", "authors": ["Hester Huijsdens", "Linda Geerligs", "Max Hinne"], "title": "Dynamic functional brain connectivity results depend on modeling assumptions: comparing frequentist and Bayesian hypothesis tests", "comment": null, "summary": "Understanding the temporal dynamics of functional brain connectivity is important for addressing various questions in network neuroscience, such as how connectivity affects cognition and changes with disease. A fundamental challenge is to evaluate whether connectivity truly exhibits dynamics, or simply is static. The most common frequentist approach uses sliding-window methods to model functional connectivity over time, but this requires defining appropriate sampling distributions and hyperparameters, such as window length, which imposes specific assumptions on the dynamics. Here, we explore how these assumptions influence the detection of dynamic connectivity, and introduce an alternative approach based on Bayesian hypothesis testing with Wishart processes. This framework encodes assumptions through prior distributions, allowing prior knowledge on the time-dependent structure of connectivity to be incorporated into the model. Moreover, this framework provides evidence for both dynamic and static connectivity, offering additional information. Using simulations, we compare the frequentist and Bayesian approaches and demonstrate how different assumptions affect the detection of dynamic connectivity. Finally, by applying both approaches to an fMRI working-memory task, we find that conclusions at the individual level vary with modeling choices, while group-level results are more robust. Our work highlights the importance of carefully considering modeling assumptions when evaluating dynamic connectivity."}
{"id": "2512.01667", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.01667", "abs": "https://arxiv.org/abs/2512.01667", "authors": ["Qingyang Liu", "Matthew A. Fisher", "Zheyang Shen", "Katy Tant", "Xuebin Zhao", "Andrew Curtis", "Chris. J. Oates"], "title": "Detecting Model Misspecification in Bayesian Inverse Problems via Variational Gradient Descent", "comment": null, "summary": "Bayesian inference is optimal when the statistical model is well-specified, while outside this setting Bayesian inference can catastrophically fail; accordingly a wealth of post-Bayesian methodologies have been proposed. Predictively oriented (PrO) approaches lift the statistical model $P_θ$ to an (infinite) mixture model $\\int P_θ\\; \\mathrm{d}Q(θ)$ and fit this predictive distribution via minimising an entropy-regularised objective functional. In the well-specified setting one expects the mixing distribution $Q$ to concentrate around the true data-generating parameter in the large data limit, while such singular concentration will typically not be observed if the model is misspecified. Our contribution is to demonstrate that one can empirically detect model misspecification by comparing the standard Bayesian posterior to the PrO `posterior' $Q$. To operationalise this, we present an efficient numerical algorithm based on variational gradient descent. A simulation study, and a more detailed case study involving a Bayesian inverse problem in seismology, confirm that model misspecification can be automatically detected using this framework."}
{"id": "2512.01823", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01823", "abs": "https://arxiv.org/abs/2512.01823", "authors": ["Jake P. Grainger", "Tuomas A. Rajala", "David J. Murrell", "Sofia C. Olhede"], "title": "The partial K function", "comment": null, "summary": "The K function and its related statistics have been an enduring tool in the analysis of spatial point processes, providing an easy to compute and interpret summary statistic for characterising the interactions between points of one type, or between two different types of points. In this paper, introduce a partial K function, enabling us to account for some of the effects of the other point types when analysing point-point interactions. The partial K function we introduce reduces to the usual K function when the other points are independent of the points of interest and has a similar interpretation. Using examples, we demonstrate how the partial K function can unpick dependence between point types that would otherwise be hidden in the usual K function. We also discuss important bias correction steps and hyperparameter selection. In addition, we discuss an extension to account for other spatial covariates, and demonstrate the methodology on the Lansing Woods dataset."}
{"id": "2512.01847", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01847", "abs": "https://arxiv.org/abs/2512.01847", "authors": ["Davide Fabbrico", "Andi Q. Wang", "Sebastiano Grazzi", "Alice Corbella", "Gareth O. Roberts", "Sylvia Richardson", "Filippo Pagani", "Paul D. W. Kirk"], "title": "A discomfort-informed adaptive Gibbs sampler for finite mixture models", "comment": "40 pages", "summary": "Finite mixture models are frequently used to uncover latent structures in high-dimensional datasets (e.g.\\ identifying clusters of patients in electronic health records). The inference of such structures can be performed in a Bayesian framework, and involves the use of sampling algorithms such as Gibbs samplers aimed at deriving posterior distribution of the probabilities of observations to belong to specific clusters. Unfortunately, traditional implementations of Gibbs samplers in this context often face critical challenges, such as inefficient use of computational resources and unnecessary updates for observations that are highly likely to remain in their current cluster. This paper introduces a new adaptive Gibbs sampler that improves the convergence efficiency over existing methods. In particular, our sampler is guided by a function that, at each iteration, uses the past of the chain to focus the updating on observations potentially misclassified in the current clustering, i.e.\\ those with a low probability of belonging to their current component. Through simulation studies and two real data analyses, we empirically demonstrate that, in terms of convergence time, our method tends to perform more efficiently compared to state-of-the-art approaches."}
{"id": "2512.01041", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01041", "abs": "https://arxiv.org/abs/2512.01041", "authors": ["Ian Miller", "Ann Hyslop", "Colin Decker"], "title": "A Clinical Instrument to Measure Patient Anecdotes in Clinical Trials", "comment": "18 pages, includes Case Report Form, non-seizure outcome measure", "summary": "Clinical trials assessing neurological treatment are challenging due to the diversity of brain function, and the difficulty in quantifying it. Traditional treatment studies in epilepsy use seizure frequency as the primary outcome measure, which may overlooking meaningful improvements in patients' quality of life. This paper introduces the Clinical Instrument for Measuring Patient Anecdotes in Clinical Trials (Clinical IMPACT), a novel tool designed to capture qualitative non-seizure improvement across neurological domains.\n  The Clinical IMPACT incorporates open-ended inquiries that allow participants or caregivers to identify and select anecdotal evidence of their most significant treatment benefits. A blinded panel of experts ranks these anecdotes, facilitating a rigorous statistical analysis using the Wilcoxon Rank-Sum Test to detect treatment efficacy. The approach is resistant to type 1 error, yet comprehensive in its ability to capture real-world effects on quality of life.\n  The potential of the Clinical IMPACT tool to enhance sensitivity while also providing qualitative insights that can inform patients, healthcare providers, and regulatory bodies about treatment effects makes it important to consider in any neurological trial. We describe how it can be used in epilepsy, and advocate for its inclusion as a key secondary endpoint to provide a perspective on non-seizure outcomes, which have previously been challenging to measure, let alone to interpret, even when the clinical trial is positive."}
{"id": "2512.01097", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.01097", "abs": "https://arxiv.org/abs/2512.01097", "authors": ["Zachary Terner", "Alexander Petersen", "Yuedong Wang"], "title": "Discriminative classification with generative features: bridging Naive Bayes and logistic regression", "comment": null, "summary": "We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance."}
