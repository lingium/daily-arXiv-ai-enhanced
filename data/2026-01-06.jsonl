{"id": "2601.00188", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.00188", "abs": "https://arxiv.org/abs/2601.00188", "authors": ["Landon Hurley"], "title": "An exact unbiased semi-parametric L2 quasi-likelihood framework, complete in the presence of ties", "comment": null, "summary": "Maximum likelihood style estimators possesses a number of ideal characteristics, but require prior identification of the distribution of errors to ensure exact unbiasedness. Independent of the focus of the primary statistical analysis, the estimation of a covariance matrix \\(S^{P \\times P}\\approx Σ^{P \\times P}\\) must possess a specific structure and regularity constraints. The need to estimate a linear Gaussian covariance models appear in various applications as a formal precondition for scientific investigation and predictive analytics. In this work, we construct an \\(\\ell_{2}\\)-norm based quasi-likelihood framework, identified by binomial comparisons between all pairs \\(X_{n},Y_{n}, \\forall {n}\\). Our work here focuses upon the quasi-likelihood basis for estimation of an exactly unbiased linear regression Hájek projection, within which the Kemeny metric space is operationalised via Whitney embedding to obtain exact unbiased minimum variance multivariate covariance estimators upon both discrete and continuous random variables (i.e., exact unbiased identification in the presence of ties upon finite samples). While the covariance estimator is inherently useful, expansion of the Wilcoxon rank-sum testing framework to handle multiple covariates with exact unbiasedness upon finite samples is a currently unresolved research problem, as it maintains identification in the presence of linear surjective mappings onto common points: this model space, by definition, expands our likelihood framework into a consistent non-parametric form of the standard general linear model, which we extend to address both unknown heterogeneity and the problem of weak inferential instruments."}
{"id": "2601.00284", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00284", "abs": "https://arxiv.org/abs/2601.00284", "authors": ["Neda Mohammadi", "Soham Sarkar", "Piotr Kokoszka"], "title": "Deep learning estimation of the spectral density of functional time series on large domains", "comment": null, "summary": "We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \\times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \\sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images."}
{"id": "2601.00015", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2601.00015", "abs": "https://arxiv.org/abs/2601.00015", "authors": ["Noé Stauffer", "Hossein Gorji", "Ivan Lunati"], "title": "Surrogate Trajectories Along Probability Flows: Pseudo Markovian Alternative to Mori Zwanzig", "comment": null, "summary": "Model reduction techniques have emerged as a powerful paradigm across different fronts of scientific computing. Despite their success, the provided tools and methodologies remain limited if high-dimensional dynamical systems subject to initial uncertainty and/or stochastic noise are encountered; in particular if rare events are of interest. We address this open challenge by borrowing ideas from Mori-Zwanzig formalism and Chorin's optimal prediction method. The novelty of our work lies on employing time-dependent optimal projection of the dynamic on a desired set of resolved variables. We show several theoretical and numerical properties of our model reduction approach. In particular, we show that the devised surrogate trajectories are consistent with the probability flow of the full-order system. Furthermore, we identify the measure underlying the projection through polynomial chaos expansion technique. This allows us to efficiently compute the projection even for trajectories that are initiated on low probability events. Moreover, we investigate the introduced model-reduction error of the surrogate trajectories on a standard setup, characterizing the convergence behaviour of the scheme. Several numerical results highlight the computational advantages of the proposed scheme in comparison to Monte-Carlo and optimal prediction method. Through this framework, we demonstrate that by tracking the measure along with the consistent projection of the dynamic we are able to access accurate estimates of different statistics including observables conditional on a given initial configuration."}
{"id": "2601.00008", "categories": ["stat.AP", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.00008", "abs": "https://arxiv.org/abs/2601.00008", "authors": ["Mohamadali Berahman", "Madjid Eshaghi Gordji"], "title": "The Dynamics of Trust: A Stochastic Levy Model Capturing Sudden Behavioral Jumps", "comment": null, "summary": "Trust is the invisible glue that holds together the fabric of societies, economic systems, and political institutions. Yet, its dynamics-especially in real-world settings remain unpredictable and difficult to control. While classical trust game models largely rely on discrete frameworks with limited noise, they fall short in capturing sudden behavioral shifts, extreme volatility, or abrupt breakdowns in cooperation.Here, we propose-for the first time a comprehensive stochastic model of trust based on Lévy processes that integrates three fundamental components: Brownian motion (representing everyday fluctuations), Poissonian jump intensity (capturing the frequency of shocks), and random distributions for jump magnitudes. This framework surpasses conventional models by enabling simulations of phenomena such as \"sudden trust collapse,\" \"chaotic volatility,\" and \"nonlinear recoveries\" dynamics often neglected in both theoretical and empirical studies.By implementing four key simulation scenarios and conducting a detailed parameter sensitivity analysis via 3D and contour plots, we demonstrate that the proposed model is not only mathematically more advanced, but also offers a more realistic representation of human dynamics compared to previous approaches. Beyond its technical contributions, this study outlines a conceptual framework for understanding fragile, jump-driven behaviors in social, economic, and geopolitical systems-where trust is not merely a psychological construct, but an inherently unstable and stochastic variable best captured through Lévy based modeling."}
{"id": "2601.00038", "categories": ["stat.ML", "cs.CE", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.00038", "abs": "https://arxiv.org/abs/2601.00038", "authors": ["Shane A. McQuarrie", "Mengwu Guo", "Anirban Chaudhuri"], "title": "Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference", "comment": null, "summary": "This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget."}
{"id": "2601.00147", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00147", "abs": "https://arxiv.org/abs/2601.00147", "authors": ["Debjoy Thakur", "Soumendra N. Lahiri"], "title": "Multi-Resolution Analysis of Variable Selection for Road Safety in St. Louis and Its Neighboring Area", "comment": null, "summary": "Generally, Lasso, Adaptive Lasso, and SCAD are standard approaches in variable selection in the presence of a large number of predictors. In recent years, during intensity function estimation for spatial point processes with a diverging number of predictors, many researchers have considered these penalized methods. But we have discussed a multi-resolution perspective for the variable selection method for spatial point process data. Its advantage is twofold: it not only efficiently selects the predictors but also provides the idea of which points are liable for selecting a predictor at a specific resolution. Actually, our research is motivated by the crime and accident occurrences in St. Louis and its neighborhoods. It is more relevant to select predictors at the local level, and thus we get the idea of which set of predictors is relevant for the occurrences of crime or accident in which parts of St. Louis. We describe the simulation results to justify the accuracy of local-level variable selection during intensity function estimation."}
{"id": "2601.00615", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00615", "abs": "https://arxiv.org/abs/2601.00615", "authors": ["Foo Hui-Mean", "Yuan-chin Ivan Chang"], "title": "Integrating Multi-Armed Bandit, Active Learning, and Distributed Computing for Scalable Optimization", "comment": "36 pages. Submitted to a conference", "summary": "Modern optimization problems in scientific and engineering domains often rely on expensive black-box evaluations, such as those arising in physical simulations or deep learning pipelines, where gradient information is unavailable or unreliable. In these settings, conventional optimization methods quickly become impractical due to prohibitive computational costs and poor scalability. We propose ALMAB-DC, a unified and modular framework for scalable black-box optimization that integrates active learning, multi-armed bandits, and distributed computing, with optional GPU acceleration. The framework leverages surrogate modeling and information-theoretic acquisition functions to guide informative sample selection, while bandit-based controllers dynamically allocate computational resources across candidate evaluations in a statistically principled manner. These decisions are executed asynchronously within a distributed multi-agent system, enabling high-throughput parallel evaluation. We establish theoretical regret bounds for both UCB-based and Thompson-sampling-based variants and develop a scalability analysis grounded in Amdahl's and Gustafson's laws. Empirical results across synthetic benchmarks, reinforcement learning tasks, and scientific simulation problems demonstrate that ALMAB-DC consistently outperforms state-of-the-art black-box optimizers. By design, ALMAB-DC is modular, uncertainty-aware, and extensible, making it particularly well suited for high-dimensional, resource-intensive optimization challenges."}
{"id": "2601.00136", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00136", "abs": "https://arxiv.org/abs/2601.00136", "authors": ["Nan Miles Xi", "Xin Huang", "Lin Wang"], "title": "Subgroup Identification and Individualized Treatment Policies: A Tutorial on the Hybrid Two-Stage Workflow", "comment": null, "summary": "Patients in clinical studies often exhibit heterogeneous treatment effect (HTE). Classical subgroup analyses provide inferential tools to test for effect modification, while modern machine learning methods estimate the Conditional Average Treatment Effect (CATE) to enable individual level prediction. Each paradigm has limitations: inference focused approaches may sacrifice predictive utility, and prediction focused approaches often lack statistical guarantees. We present a hybrid two-stage workflow that integrates these perspectives. Stage 1 applies statistical inference to test whether credible treatment effect heterogeneity exists with the protection against spurious findings. Stage 2 translates heterogeneity evidence into individualized treatment policies, evaluated by cross fitted doubly robust (DR) metrics with Neyman-Pearson (NP) constraints on harm. We illustrate the workflow with working examples based on simulated data and a real ACTG 175 HIV trial. This tutorial provides practical implementation checklists and discusses links to sponsor oriented HTE workflows, offering a transparent and auditable pathway from heterogeneity assessment to individualized treatment policies."}
{"id": "2601.00200", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00200", "abs": "https://arxiv.org/abs/2601.00200", "authors": ["Yikai Chen", "Yunxin Mao", "Chunyuan Zheng", "Hao Zou", "Shanzhi Gu", "Shixuan Liu", "Yang Shi", "Wenjing Yang", "Kun Kuang", "Haotian Wang"], "title": "Detecting Unobserved Confounders: A Kernelized Regression Approach", "comment": null, "summary": "Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency."}
{"id": "2601.00154", "categories": ["stat.ME", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00154", "abs": "https://arxiv.org/abs/2601.00154", "authors": ["Qianqian Qi", "Zhongming Chen", "Peter G. M. van der Heijden"], "title": "Unmixing highly mixed grain size distribution data via maximum volume constrained end member analysis", "comment": null, "summary": "End member analysis (EMA) unmixes grain size distribution (GSD) data into a mixture of end members (EMs), thus helping understand sediment provenance and depositional regimes and processes. In highly mixed data sets, however, many EMA algorithms find EMs which are still a mixture of true EMs. To overcome this, we propose maximum volume constrained EMA (MVC-EMA), which finds EMs as different as possible. We provide a uniqueness theorem and a quadratic programming algorithm for MVC-EMA. Experimental results show that MVC-EMA can effectively find true EMs in highly mixed data sets."}
{"id": "2601.00760", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00760", "abs": "https://arxiv.org/abs/2601.00760", "authors": ["Diksha Bhandari", "Sebastian Reich"], "title": "Gradient-free ensemble transform methods for generalized Bayesian inference in generative models", "comment": null, "summary": "Bayesian inference in complex generative models is often obstructed by the absence of tractable likelihoods and the infeasibility of computing gradients of high-dimensional simulators. Existing likelihood-free methods for generalized Bayesian inference typically rely on gradient-based optimization or reparameterization, which can be computationally expensive and often inapplicable to black-box simulators. To overcome these limitations, we introduce a gradient-free ensemble transform Langevin dynamics method for generalized Bayesian inference using the maximum mean discrepancy. By relying on ensemble-based covariance structures rather than simulator derivatives, the proposed method enables robust posterior approximation without requiring access to gradients of the forward model, making it applicable to a broader class of likelihood-free models. The method is affine invariant, computationally efficient, and robust to model misspecification. Through numerical experiments on well-specified chaotic dynamical systems, and misspecified generative models with contaminated data, we demonstrate that the proposed method achieves comparable or improved accuracy relative to existing gradient-based methods, while substantially reducing computational cost."}
{"id": "2601.00517", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00517", "abs": "https://arxiv.org/abs/2601.00517", "authors": ["George Sun", "Yi-Hui Zhou"], "title": "Generative Conditional Missing Imputation Networks", "comment": null, "summary": "In this study, we introduce a sophisticated generative conditional strategy designed to impute missing values within datasets, an area of considerable importance in statistical analysis. Specifically, we initially elucidate the theoretical underpinnings of the Generative Conditional Missing Imputation Networks (GCMI), demonstrating its robust properties in the context of the Missing Completely at Random (MCAR) and the Missing at Random (MAR) mechanisms. Subsequently, we enhance the robustness and accuracy of GCMI by integrating a multiple imputation framework using a chained equations approach. This innovation serves to bolster model stability and improve imputation performance significantly. Finally, through a series of meticulous simulations and empirical assessments utilizing benchmark datasets, we establish the superior efficacy of our proposed methods when juxtaposed with other leading imputation techniques currently available. This comprehensive evaluation not only underscores the practicality of GCMI but also affirms its potential as a leading-edge tool in the field of statistical data analysis."}
{"id": "2601.00188", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.00188", "abs": "https://arxiv.org/abs/2601.00188", "authors": ["Landon Hurley"], "title": "An exact unbiased semi-parametric L2 quasi-likelihood framework, complete in the presence of ties", "comment": null, "summary": "Maximum likelihood style estimators possesses a number of ideal characteristics, but require prior identification of the distribution of errors to ensure exact unbiasedness. Independent of the focus of the primary statistical analysis, the estimation of a covariance matrix \\(S^{P \\times P}\\approx Σ^{P \\times P}\\) must possess a specific structure and regularity constraints. The need to estimate a linear Gaussian covariance models appear in various applications as a formal precondition for scientific investigation and predictive analytics. In this work, we construct an \\(\\ell_{2}\\)-norm based quasi-likelihood framework, identified by binomial comparisons between all pairs \\(X_{n},Y_{n}, \\forall {n}\\). Our work here focuses upon the quasi-likelihood basis for estimation of an exactly unbiased linear regression Hájek projection, within which the Kemeny metric space is operationalised via Whitney embedding to obtain exact unbiased minimum variance multivariate covariance estimators upon both discrete and continuous random variables (i.e., exact unbiased identification in the presence of ties upon finite samples). While the covariance estimator is inherently useful, expansion of the Wilcoxon rank-sum testing framework to handle multiple covariates with exact unbiasedness upon finite samples is a currently unresolved research problem, as it maintains identification in the presence of linear surjective mappings onto common points: this model space, by definition, expands our likelihood framework into a consistent non-parametric form of the standard general linear model, which we extend to address both unknown heterogeneity and the problem of weak inferential instruments."}
{"id": "2601.00499", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00499", "abs": "https://arxiv.org/abs/2601.00499", "authors": ["Marcio A. Diniz", "Hulya Kocyigit", "Erin Moshier", "Madhu Mazumdar", "Deukwoo Kwon"], "title": "Continuous monitoring of delayed outcomes in basket trials", "comment": "28 pages, 8 figures, 6 tables", "summary": "Precision medicine has led to a paradigm shift allowing the development of targeted drugs that are agnostic to the tumor location. In this context, basket trials aim to identify which tumor types - or baskets - would benefit from the targeted therapy among patients with the same molecular marker or mutation. We propose the implementation of continuous monitoring for basket trials to increase the likelihood of early identification of non-promising baskets. Although the current Bayesian trial designs available in the literature can incorporate more than one interim analysis, most of them have high computational cost, and none of them handle delayed outcomes that are expected for targeted treatments such as immunotherapies. We leverage the Bayesian empirical approach proposed by Fujiwara et al., which has low computational cost. We also extend ideas of Cai et al to address the practical challenge of performing interim analysis with delayed outcomes using multiple imputation. Operating characteristics of four different strategies to handle delayed outcomes in basket trials are compared in an extensive simulation study with the benchmark strategy where trial accrual is put on hold until complete data is observed to make a decision. The optimal handling of missing data at interim analyses is trial-dependent. With slow accrual, missingness is minimal even with continuous monitoring, favoring simpler approaches over computationally intensive methods. Although individual sample-size savings are small, multiple imputation becomes more appealing when sample size savings scale with the number of baskets and agents tested."}
{"id": "2601.00284", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00284", "abs": "https://arxiv.org/abs/2601.00284", "authors": ["Neda Mohammadi", "Soham Sarkar", "Piotr Kokoszka"], "title": "Deep learning estimation of the spectral density of functional time series on large domains", "comment": null, "summary": "We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \\times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \\sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images."}
{"id": "2601.00284", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00284", "abs": "https://arxiv.org/abs/2601.00284", "authors": ["Neda Mohammadi", "Soham Sarkar", "Piotr Kokoszka"], "title": "Deep learning estimation of the spectral density of functional time series on large domains", "comment": null, "summary": "We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \\times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \\sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images."}
{"id": "2601.00287", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00287", "abs": "https://arxiv.org/abs/2601.00287", "authors": ["Kohei Yoshikawa", "Shuichi Kawano"], "title": "Identification and Estimation under Multiple Versions of Treatment: Mixture-of-Experts Approach", "comment": "20 pages, 3 figures", "summary": "The Stable Unit Treatment Value Assumption (SUTVA) includes the condition that there are no multiple versions of treatment in causal inference. Though we could not control the implementation of treatment in observational studies, multiple versions may exist in the treatment. It has been pointed out that ignoring such multiple versions of treatment can lead to biased estimates of causal effects, but a causal inference framework that explicitly deals with the unbiased identification and estimation of version-specific causal effects has not been fully developed yet. Thus, obtaining a deeper understanding for mechanisms of the complex treatments is difficult. In this paper, we introduce the Mixture-of-Experts framework into causal inference and develop a methodology for estimating the causal effects of latent versions. This approach enables explicit estimation of version-specific causal effects even if the versions are not observed. Numerical experiments demonstrate the effectiveness of the proposed method."}
{"id": "2601.00287", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00287", "abs": "https://arxiv.org/abs/2601.00287", "authors": ["Kohei Yoshikawa", "Shuichi Kawano"], "title": "Identification and Estimation under Multiple Versions of Treatment: Mixture-of-Experts Approach", "comment": "20 pages, 3 figures", "summary": "The Stable Unit Treatment Value Assumption (SUTVA) includes the condition that there are no multiple versions of treatment in causal inference. Though we could not control the implementation of treatment in observational studies, multiple versions may exist in the treatment. It has been pointed out that ignoring such multiple versions of treatment can lead to biased estimates of causal effects, but a causal inference framework that explicitly deals with the unbiased identification and estimation of version-specific causal effects has not been fully developed yet. Thus, obtaining a deeper understanding for mechanisms of the complex treatments is difficult. In this paper, we introduce the Mixture-of-Experts framework into causal inference and develop a methodology for estimating the causal effects of latent versions. This approach enables explicit estimation of version-specific causal effects even if the versions are not observed. Numerical experiments demonstrate the effectiveness of the proposed method."}
{"id": "2601.00615", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00615", "abs": "https://arxiv.org/abs/2601.00615", "authors": ["Foo Hui-Mean", "Yuan-chin Ivan Chang"], "title": "Integrating Multi-Armed Bandit, Active Learning, and Distributed Computing for Scalable Optimization", "comment": "36 pages. Submitted to a conference", "summary": "Modern optimization problems in scientific and engineering domains often rely on expensive black-box evaluations, such as those arising in physical simulations or deep learning pipelines, where gradient information is unavailable or unreliable. In these settings, conventional optimization methods quickly become impractical due to prohibitive computational costs and poor scalability. We propose ALMAB-DC, a unified and modular framework for scalable black-box optimization that integrates active learning, multi-armed bandits, and distributed computing, with optional GPU acceleration. The framework leverages surrogate modeling and information-theoretic acquisition functions to guide informative sample selection, while bandit-based controllers dynamically allocate computational resources across candidate evaluations in a statistically principled manner. These decisions are executed asynchronously within a distributed multi-agent system, enabling high-throughput parallel evaluation. We establish theoretical regret bounds for both UCB-based and Thompson-sampling-based variants and develop a scalability analysis grounded in Amdahl's and Gustafson's laws. Empirical results across synthetic benchmarks, reinforcement learning tasks, and scientific simulation problems demonstrate that ALMAB-DC consistently outperforms state-of-the-art black-box optimizers. By design, ALMAB-DC is modular, uncertainty-aware, and extensible, making it particularly well suited for high-dimensional, resource-intensive optimization challenges."}
{"id": "2601.00310", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00310", "abs": "https://arxiv.org/abs/2601.00310", "authors": ["Manganaw N'Daam", "Tchilabalo Abozou Kpanzou", "Edoh Katchekpele"], "title": "Asymptotic distribution of a robust wavelet-based NKK periodogram", "comment": "19 pages, 4 figures", "summary": "This paper investigates the asymptotic distribution of a wavelet-based NKK periodogram constructed from least absolute deviations (LAD) harmonic regression at a fixed resolution level. Using a wavelet representation of the underlying time series, we analyze the probabilistic structure of the resulting periodogram under long-range dependence. It is shown that, under suitable regularity conditions, the NKK periodogram converges in distribution to a nonstandard limit characterized as a quadratic form in a Gaussian random vector, whose covariance structure depends on the memory properties of the process and on the chosen wavelet filters. This result establishes a rigorous theoretical foundation for the use of robust wavelet-based periodograms in the spectral analysis of long-memory time series with heavy-tailed inovations."}
{"id": "2601.00499", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.00499", "abs": "https://arxiv.org/abs/2601.00499", "authors": ["Marcio A. Diniz", "Hulya Kocyigit", "Erin Moshier", "Madhu Mazumdar", "Deukwoo Kwon"], "title": "Continuous monitoring of delayed outcomes in basket trials", "comment": "28 pages, 8 figures, 6 tables", "summary": "Precision medicine has led to a paradigm shift allowing the development of targeted drugs that are agnostic to the tumor location. In this context, basket trials aim to identify which tumor types - or baskets - would benefit from the targeted therapy among patients with the same molecular marker or mutation. We propose the implementation of continuous monitoring for basket trials to increase the likelihood of early identification of non-promising baskets. Although the current Bayesian trial designs available in the literature can incorporate more than one interim analysis, most of them have high computational cost, and none of them handle delayed outcomes that are expected for targeted treatments such as immunotherapies. We leverage the Bayesian empirical approach proposed by Fujiwara et al., which has low computational cost. We also extend ideas of Cai et al to address the practical challenge of performing interim analysis with delayed outcomes using multiple imputation. Operating characteristics of four different strategies to handle delayed outcomes in basket trials are compared in an extensive simulation study with the benchmark strategy where trial accrual is put on hold until complete data is observed to make a decision. The optimal handling of missing data at interim analyses is trial-dependent. With slow accrual, missingness is minimal even with continuous monitoring, favoring simpler approaches over computationally intensive methods. Although individual sample-size savings are small, multiple imputation becomes more appealing when sample size savings scale with the number of baskets and agents tested."}
{"id": "2601.00508", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00508", "abs": "https://arxiv.org/abs/2601.00508", "authors": ["Simon Rudkin", "Wanling Rudkin"], "title": "ballmapper: Applying Topological Data Analysis Ball Mapper in Stata", "comment": "The accompanying GitHub is https://github.com/srudkin12/statabm", "summary": "Topological Data Analysis Ball Mapper (TDABM) offers a model-free visualization of multivariate data which does not necessitate the information loss associated with dimensionality reduction. TDABM Dlotko (2019) produces a cover of a multidimensional point cloud using equal size balls, the radius of the ball is the only parameter. A TDABM visualization retains the full structure of the data. The graphs produced by TDABM can convey coloration according to further variables, model residuals, or variables within the multivariate data. An expanding literature makes use of the power of TDABM across Finance, Economics, Geography, Medicine and Chemistry amongst others. We provide an introduction to TDABM and the \\texttt{ballmapper} package for Stata."}
{"id": "2601.00531", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00531", "abs": "https://arxiv.org/abs/2601.00531", "authors": ["Raphael C. Kim", "Rachel C. Nethery", "Kevin L. Chen", "Falco J. Bargagli-Stoffi"], "title": "Fair Policy Learning under Bipartite Network Interference: Learning Fair and Cost-Effective Environmental Policies", "comment": null, "summary": "Numerous studies have shown the harmful effects of airborne pollutants on human health. Vulnerable groups and communities often bear a disproportionately larger health burden due to exposure to airborne pollutants. Thus, there is a need to design policies that effectively reduce the public health burdens while ensuring cost-effective policy interventions. Designing policies that optimally benefit the population while ensuring equity between groups under cost constraints is a challenging statistical and causal inference problem. In the context of environmental policy this is further complicated by the fact that interventions target emission sources but health impacts occur in potentially distant communities due to atmospheric pollutant transport -- a setting known as bipartite network interference (BNI). To address these issues, we propose a fair policy learning approach under BNI. Our approach allows to learn cost-effective policies under fairness constraints even accounting for complex BNI data structures. We derive asymptotic properties and demonstrate finite sample performance via Monte Carlo simulations. Finally, we apply the proposed method to a real-world dataset linking power plant scrubber installations to Medicare health records for more than 2 million individuals in the U.S. Our method determine fair scrubber allocations to reduce mortality under fairness and cost constraints."}
{"id": "2601.00773", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.00773", "abs": "https://arxiv.org/abs/2601.00773", "authors": ["Sinan Acemoglu", "Christian Kleiber", "Jörg Urban"], "title": "Variable Importance in Generalized Linear Models -- A Unifying View Using Shapley Values", "comment": "33 pages, 3 figures", "summary": "Variable importance in regression analyses is of considerable interest in a variety of fields. There is no unique method for assessing variable importance. However, a substantial share of the available literature employs Shapley values, either explicitly or implicitly, to decompose a suitable goodness-of-fit measure, in the linear regression model typically the classical $R^2$. Beyond linear regression, there is no generally accepted goodness-of-fit measure, only a variety of pseudo-$R^2$s. We formulate and discuss the desirable properties of goodness-of-fit measures that enable Shapley values to be interpreted in terms of relative, and even absolute, importance. We suggest to use a pseudo-$R^2$ based on the Kullback-Leibler divergence, the Kullback-Leibler $R^2$, which has a convenient form for generalized linear models and permits to unify and extend previous work on variable importance for linear and nonlinear models. Several examples are presented, using data from public health and insurance."}
