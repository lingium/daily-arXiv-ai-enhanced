<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 30]
- [stat.AP](#stat.AP) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [A simple and powerful test of vaccine waning](https://arxiv.org/abs/2511.21836)
*Gellért Perényi,Matias Janvin,Mats J. Stensrud*

Main category: stat.ME

TL;DR: 提出一种新的统计检验方法，用于检测疫苗效果是否随时间减弱，相比现有方法具有更高的统计功效，并在BNT162b2疫苗试验中成功检测到效果减弱


<details>
  <summary>Details</summary>
Motivation: 量化疫苗效果减弱对个人和公共卫生决策很重要，但传统方法需要不合理假设，现有因果估计量虽然能在较弱假设下界定，但界限太宽难以得出明确结论

Method: 提出一种正式的统计检验方法，用于检测治疗效果是否随时间恒定，该方法在疫苗试验中合理的假设下有效，提供了三种计算检验统计量的方法，其中两种仅需汇总数据

Result: 新方法相比现有方法具有显著更高的统计功效，在BNT162b2 COVID-19疫苗随机对照试验的重新分析中，成功拒绝了"无减弱"的零假设，而先前分析未能建立减弱证据

Conclusion: 提出的统计检验方法为检测疫苗效果减弱提供了更有效的工具，能够基于合理的假设和现有临床试验数据得出可靠结论，同时提供了新的效果减弱界限结果

Abstract: Determining whether vaccine efficacy wanes is important for individual and public decision making. Yet, quantification of waning is a subtle task. The classical approaches cannot be interpreted as measures of declining efficacy unless we impose unreasonable assumptions. Recently, formal causal estimands designed to quantify vaccine waning have been proposed. These estimands can be bounded under weaker assumptions, but the bounds are often too wide to make claims about the presence of vaccine waning. We propose an alternative approach: a formal test to determine whether a treatment effect is constant over time. This test not only gives a considerable power gain compared to existing approaches but is also valid under plausible assumptions that are expected to hold in vaccine trials. We illustrate the increase in power through real and simulated examples, using three different approaches to compute the test statistics. Two of these approaches are based solely on summary data, accessible from existing clinical trials. Beyond our test, we also give new results that bound the waning effect. We use our methods to reanalyze data from a randomized controlled trial of the BNT162b2 COVID-19 vaccine. While prior analysis did not establish waning, our test rejects the null hypothesis of no waning.

</details>


### [2] [A Non-Bipartite Matching Framework for Difference-in-Differences with General Treatment Types](https://arxiv.org/abs/2511.21973)
*Siyu Heng,Yuan Huang,Hyunseung Kang*

Main category: stat.ME

TL;DR: 提出一种新的非二分匹配框架，将双重差分法扩展到非二元处理（如连续、序数处理），无需参数假设或"停留者"假设，通过优化匹配设计平衡协变量并最大化处理轨迹对比。


<details>
  <summary>Details</summary>
Motivation: 现有DID方法主要针对二元处理，扩展到非二元处理需要强假设（如参数结果模型或"停留者"假设），限制了在连续处理等实际场景中的应用。

Method: 开发最优非二分匹配设计：1）联合平衡基线协变量以减少偏差；2）最大化时间上处理轨迹的对比以提高效率；3）建立设计后随机化条件作为平行趋势假设的设计对应；4）引入样本平均DID比率作为有限总体有效的非参数因果估计量。

Result: 提出的框架能够处理任意处理类型（二元、序数、连续），保留完整的处理剂量信息，避免参数假设，不依赖停留者或准停留者存在，完全在有限总体框架内操作。

Conclusion: 该非二分匹配框架为DID在非二元处理设置中提供了灵活、稳健的设计基础方法，扩展了DID的应用范围，同时保持了设计基础的推理有效性。

Abstract: Difference-in-differences (DID) is one of the most widely used causal inference frameworks in observational studies. However, most existing DID methods are designed for binary treatments and cannot be readily applied to non-binary treatment settings. Although recent work has begun to extend DID to non-binary (e.g., continuous) treatments, these approaches typically require strong additional assumptions, including parametric outcome models or the presence of idealized comparison units with (nearly) static treatment levels over time (commonly called ``stayers'' or ``quasi-stayers''). In this technical note, we introduce a new non-bipartite matching framework for DID that naturally accommodates general treatment types (e.g., binary, ordinal, or continuous). Our framework makes three main contributions. First, we develop an optimal non-bipartite matching design for DID that jointly balances baseline covariates across comparable units (reducing bias) and maximizes contrasts in treatment trajectories over time (improving efficiency). Second, we establish a post-matching randomization condition, the design-based counterpart to the traditional parallel-trends assumption, which enables valid design-based inference. Third, we introduce the sample average DID ratio, a finite-population-valid and fully nonparametric causal estimand applicable to arbitrary treatment types. Our design-based approach that preserves the full treatment-dose information, avoids parametric assumptions, does not rely on the existence of stayers or quasi-stayers, and operates entirely within a finite-population framework, without appealing to hypothetical super-populations or outcome distributions.

</details>


### [3] [The Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors](https://arxiv.org/abs/2511.23144)
*Riko Kelter,Samuel Pawel*

Main category: stat.ME

TL;DR: 提出基于贝叶斯因子的最优两阶段临床试验设计，通过三叉树分支方法替代蒙特卡洛模拟，实现快速校准并最小化零假设下的期望样本量


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯序贯试验设计依赖蒙特卡洛模拟，存在重复复杂模拟、需要蒙特卡洛标准误判断可靠性、缺乏闭式解或数值方法校准等问题

Method: 提出基于贝叶斯因子的最优两阶段设计，采用三叉树分支方法校正引入期中分析后的设计特性，开发校准算法寻找最小化零假设下期望样本量的最优设计

Result: 设计能恢复Simon两阶段最优设计作为特例，改进基于贝叶斯因子的非序贯设计，快速校准仅需标准数值技术而非耗时蒙特卡洛模拟，并能确保零假设有利证据的最小概率

Conclusion: 提出的三叉树分支方法不依赖于终点或贝叶斯因子使用，可推广到其他设置，为贝叶斯序贯试验设计提供了高效校准方案

Abstract: Sequential trial design is an important statistical approach to increase the efficiency of clinical trials. Bayesian sequential trial design relies primarily on conducting a Monte Carlo simulation under the hypotheses of interest and investigating the resulting design characteristics via Monte Carlo estimates. This approach has several drawbacks, namely that replicating the calibration of a Bayesian design requires repeating a possibly complex Monte Carlo simulation. Furthermore, Monte Carlo standard errors are required to judge the reliability of the simulation. All of this is due to a lack of closed-form or numerical approaches to calibrate a Bayesian design which uses Bayes factors. In this paper, we propose the Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors. The optimal two-stage Bayes factor design is a sequential clinical trial design that is built on the idea of trinomial tree branching, a method we propose to correct the resulting design characteristics for introducing a single interim analysis. We build upon this idea to invent a calibration algorithm which yields the optimal Bayesian design that minimizes the expected sample size under the null hypothesis. Examples show that our design recovers Simon's two-stage optimal design as a special case, improves upon non-sequential Bayesian design based on Bayes factors, and can be calibrated quickly, as it makes use only of standard numerical techniques instead of time-consuming Monte Carlo simulations. Furthermore, the design allows to ensure a minimum probability on compelling evidence in favour of the null hypothesis, which is not possible with other designs. As the idea of trinomial tree branching is neither dependent on the endpoint, nor on the use of Bayes factors, the design can therefore be generalized to other settings, too.

</details>


### [4] [Design-based nested instrumental variable analysis](https://arxiv.org/abs/2511.21992)
*Zhe Chen,Xinran Li,Michael O. Harhay,Bo Zhang*

Main category: stat.ME

TL;DR: 本文提出了一种新的嵌套工具变量设计，用于识别两个潜在亚组（始终遵从者和转换者）的因果效应，并应用于癌症筛查试验的数据分析。


<details>
  <summary>Details</summary>
Motivation: 在嵌套工具变量结构中，当两个二元工具变量代表不同强度的治疗鼓励时，传统方法难以识别不同遵从类型亚组的因果效应。需要开发新的设计和方法来估计始终遵从者和转换者的样本平均处理效应。

Method: 提出了一种新的"配对-配对"嵌套工具变量设计，每个匹配层包含四个单位组织成两个配对。开发了基于设计的推断方法来估计ACO-SATE和SW-SATE，并提出了部分有偏随机化方案来处理工具变量强度分配的随机性问题。

Result: 模拟研究验证了方法的有效性。在癌症筛查试验应用中，识别出52.2%的参与者为始终遵从者，26.7%为转换者。在始终遵从者中，柔性乙状结肠镜检查与结直肠癌率下降趋势相关，而在转换者中未检测到效应。

Conclusion: 嵌套工具变量框架提供了对因果效应的更丰富解释，能够解释为什么在遵从率上升后未观察到意向治疗效应的增加。该方法为分析不同遵从类型亚组的异质性处理效应提供了有力工具。

Abstract: Two binary instrumental variables (IVs) are nested if individuals who comply under one binary IV also comply under the other. This situation often arises when the two IVs represent different intensities of encouragement or discouragement to take the treatment--one stronger than the other. In a nested IV structure, treatment effects can be identified for two latent subgroups: always-compliers and switchers. Always-compliers are individuals who comply even under the weaker IV, while switchers are those who do not comply under the weaker IV but do under the stronger IV. We introduce a novel pair-of-pairs nested IV design, where each matched stratum consists of four units organized in two pairs. Under this design, we develop design-based inference for estimating the always-complier sample average treatment effect (ACO-SATE) and switcher sample average treatment effect (SW-SATE). In a nested IV analysis, IV assignment is randomized within each IV pair; however, whether a study unit receives the weaker or stronger IV may not be randomized. To address this complication, we then propose a novel partly biased randomization scheme and study design-based inference under this new scheme. Using extensive simulation studies, we demonstrate the validity of the proposed method and assess its power under different scenarios. Applying the nested IV framework, we estimated that 52.2% (95% CI: 50.4%-53.9%) of participants enrolled at the Henry Ford Health System in the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial were always-compliers, while 26.7% (95% CI: 24.5%-28.9%) were switchers. Among always-compliers, flexible sigmoidoscopy was associated with a trend toward a decreased colorectal cancer rate. No effect was detected among switchers. This offers a richer interpretation of why no increase in the intention-to-treat effect was observed after 1997, even though the compliance rate rose.

</details>


### [5] [Univariate-Guided Sparse Regression for Biobank-Scale High-Dimensional -omics Data](https://arxiv.org/abs/2511.22049)
*Joshua Richland,Tuomo Kiiskinen,William Wang,Sophia Lu,Balasubramanian Narasimhan,Manuel Rivas,Robert Tibshirani*

Main category: stat.ME

TL;DR: 提出基于uniLasso的可扩展多基因风险评分计算框架，在保持预测性能的同时显著减少变异选择数量，提高模型稀疏性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统多基因风险评分方法在高维基因组数据中面临特征选择不稳定、模型可解释性差的问题，需要一种既能保持预测性能又能产生稀疏可解释模型的方法。

Method: 采用两阶段惩罚回归方法uniLasso，利用单变量系数和幅度指导特征选择，并扩展框架以整合外部汇总统计信息，应用于UK Biobank大规模基因组数据。

Result: uniLasso在预测性能上与标准Lasso相当，但选择的变异数量显著减少，产生更稀疏、更可解释的模型；在PRS估计方面优于PRS-CS等竞争方法；整合外部评分进一步提升了预测性能。

Conclusion: uniLasso框架为大规模基因组数据中的多基因风险评分计算提供了一种高效、稀疏且可解释的解决方案，通过整合外部信息可进一步提升预测准确性。

Abstract: We present a scalable framework for computing polygenic risk scores (PRS) in high-dimensional genomic settings using the recently introduced Univariate-Guided Sparse Regression (uniLasso). UniLasso is a two-stage penalized regression procedure that leverages univariate coefficients and magnitudes to stabilize feature selection and enhance interpretability. Building on its theoretical and empirical advantages, we adapt uniLasso for application to the UK Biobank, a population-based repository comprising over one million genetic variants measured on hundreds of thousands of individuals from the United Kingdom. We further extend the framework to incorporate external summary statistics to increase predictive accuracy. Our results demonstrate that the adapted uniLasso attains predictive performance comparable to standard Lasso while selecting substantially fewer variants, yielding sparser and more interpretable models. Moreover, it exhibits superior performance in estimating PRS relative to its competitors, such as PRS-CS. Integrating external scores further improves prediction while maintaining sparsity.

</details>


### [6] [The Bayes Factor Reversal Paradox](https://arxiv.org/abs/2511.22152)
*Miodrag M. Lovric*

Main category: stat.ME

TL;DR: 贝叶斯推断内部存在矛盾：对于正态模型中任意双侧显著的结果，存在不同的先验方差使得贝叶斯因子对同一数据得出相反的结论（支持备择假设vs支持原假设）。


<details>
  <summary>Details</summary>
Motivation: 揭示贝叶斯推断内部存在的矛盾，不同于经典的Jeffreys-Lindley悖论（需要样本量趋于无穷），这种新悖论在现实样本量下就会发生，回应Robert（2016）关于先验尺度对贝叶斯因子影响的担忧。

Method: 在已知方差的正态模型中，证明对于任意在0.05水平上双侧统计显著的结果，存在不同的先验方差选择，使得贝叶斯因子得出相反的结论。

Result: 证明了贝叶斯推断内部存在矛盾：同一数据、检验同一假设，仅因先验方差的选择不同，贝叶斯因子可以同时支持备择假设和原假设，得出完全相反的结论。

Conclusion: 先验选择在贝叶斯推断中具有高度任意性，这构成了贝叶斯推断内部的一个根本性矛盾，不同于传统的频率主义与贝叶斯主义之间的冲突。

Abstract: In 1957, Lindley published "A statistical paradox" in Biometrika, revealing a fundamental conflict between frequentist and Bayesian inference as sample size approaches infinity. We present a new paradox of a different kind: a conflict within Bayesian inference itself. In the normal model with known variance, we prove that for any two-sided statistically significant result at the 0.05 level there exist prior variances such that the Bayes factor indicates evidence for the alternative with one choice while indicating evidence for the null with another. Thus, the same data, testing the same hypothesis, can yield opposite conclusions depending solely on prior choice. This answers Robert's 2016 call to investigate the impact of the prior scale on Bayes factors and formalises his concern that this choice involves arbitrariness to a high degree. Unlike the Jeffreys-Lindley paradox, which requires sample size approaching infinity, the paradox we identify occurs with realistic sample sizes.

</details>


### [7] [Overall marginalized models for longitudinal zero-inflated count data](https://arxiv.org/abs/2511.22223)
*Keunbaik Lee,Eun Jin Jang,Dipak Dey*

Main category: stat.ME

TL;DR: 提出边际化零膨胀泊松（MZIP）和负二项（MZINB）混合效应模型，用于分析纵向零膨胀计数数据，能更清晰地解释协变量的边际效应并处理过度离散问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析纵向零膨胀计数数据时，对协变量效应的解释不够清晰，且难以处理过度离散问题。需要开发能明确捕捉协变量边际效应、同时考虑个体异质性的模型。

Method: 扩展现有模型，提出边际化零膨胀泊松（MZIP）和负二项（MZINB）混合效应模型，包含灵活的随机效应协方差结构。通过模拟研究评估模型性能，并应用于系统性红斑狼疮数据分析。

Result: MZIP和MZINB模型能有效捕捉协变量的边际效应，MZINB模型能更好地处理过度离散问题。模拟研究表明模型在不同随机效应结构下均有良好推断性能。

Conclusion: 提出的边际化零膨胀混合效应模型为纵向零膨胀计数数据分析提供了更清晰的解释框架，能同时处理过度离散和个体异质性，在实际应用中具有良好适用性。

Abstract: To analyze longitudinal zero-inflated count data, we extend existing models by introducing marginalized zero-inflated Poisson (MZIP) models with random effects, which explicitly capture the marginal effect of covariates and address limitations of previous methods. These models provide a clearer interpretation of the overall mean effect of covariates on zero-inflated count data. To further accommodate overdispersion, we develop marginalized zero-inflated negative binomial (MZINB) models. Both models incorporate subject-specific heterogeneity through a flexible random effects covariance structure. Simulation studies are conducted to evaluate the performance of the MZIP and MZINB models, comparing their inference under both homogeneous and heterogeneous random effects. Finally, we illustrate the applicability of the proposed models through an analysis of systemic lupus erythematosus data.

</details>


### [8] [Diagnostic Checking for Wasserstein Autoregression](https://arxiv.org/abs/2511.22274)
*Chenxiao Dai,Feiyu Jiang,Dong Li,Xiaofeng Shao*

Main category: stat.ME

TL;DR: 开发用于评估Wasserstein自回归模型充分性的portmanteau型诊断检验，包括基于McLeod型检验和样本分割方法的两种检验


<details>
  <summary>Details</summary>
Motivation: Wasserstein自回归模型在经济学、金融和气候科学中广泛应用，但缺乏有效的诊断工具来评估模型充分性，需要开发专门的检验方法

Method: 定义Wasserstein空间中模型误差和残差的自相关函数，构建两种相关检验：类似经典McLeod型检验的方法，以及基于Davis和Fernandes(2025)样本分割方法的检验

Result: 在温和正则条件下，相应检验统计量收敛于卡方分布极限；模拟研究和实证应用表明，所提检验能有效检测模型误设

Conclusion: 提出的检验为分布时间序列分析提供了原则性和可靠的诊断工具，能有效评估Wasserstein自回归模型的充分性

Abstract: Wasserstein autoregression provides a robust framework for modeling serial dependence among probability distributions, with wide-ranging applications in economics, finance, and climate science. In this paper, we develop portmanteau-type diagnostic tests for assessing the adequacy of Wasserstein autoregressive models. By defining autocorrelation functions for model errors and residuals in the Wasserstein space, we construct two related tests: one analogous to the classical McLeod type test, and the other based on the sample-splitting approach of Davis and Fernandes(2025). We establish that, under mild regularity conditions, the corresponding test statistics converge in distribution to chi-square limits. Simulation studies and empirical applications demonstrate that the proposed tests effectively detect model mis-specification, offering a principled and reliable diagnostic tool for distributional time series analysis.

</details>


### [9] [Investigating new, signature-based, spatial autoregressive models for functional covariates](https://arxiv.org/abs/2511.22414)
*Camille Frévent*

Main category: stat.ME

TL;DR: 开发了两种新的空间自回归模型替代方法，在模拟研究中表现优于现有方法且计算时间更短，并应用于分析过早死亡率和65岁以上人群死亡率


<details>
  <summary>Details</summary>
Motivation: 现有基于签名的空间自回归模型可能存在计算效率问题，需要开发更高效且性能相当的替代方法

Method: 开发了两种新的空间自回归模型替代方法，通过模拟研究评估性能，并将新模型应用于实际健康数据

Result: 新模型在模拟研究中表现至少与现有方法相当，但计算时间更短，成功应用于分析过早死亡率和65岁以上人群死亡率

Conclusion: 提出的新空间自回归模型替代方法在保持性能的同时提高了计算效率，为空间数据分析提供了更实用的工具

Abstract: We developed two new alternatives to signature-based, spatial autoregressive models. In a simulation study, we found that the new models performed at least as well as existing approaches but presented shorter computation times. We then used the new models to analyze the premature mortality rate and the mortality rate for people aged 65 and over.

</details>


### [10] [A signature-based spatial scan statistic for functional data](https://arxiv.org/abs/2511.22432)
*Camille Frévent*

Main category: stat.ME

TL;DR: 开发了一种基于签名的功能数据空间扫描统计量SigFSS，适用于单变量和多变量功能数据，在模拟中优于现有方法，能更精确地识别地理聚类，并成功应用于法国死亡率异常空间聚类检测。


<details>
  <summary>Details</summary>
Motivation: 现有功能数据空间聚类检测方法存在局限性，需要开发更有效的统计方法来识别功能数据中的空间异常模式，特别是在公共卫生等领域有重要应用价值。

Method: 提出基于签名的功能数据空间扫描统计量SigFSS，该方法将功能数据转换为签名表示，然后应用空间扫描统计框架，可处理单变量和多变量功能数据。

Result: 模拟研究表明SigFSS几乎在所有情况下都优于现有文献方法，能产生更精确的地理聚类；在法国死亡率数据分析中成功检测出异常高或低死亡率的空间聚类。

Conclusion: SigFSS是一种有效的功能数据空间聚类检测方法，在模拟和实际应用中均表现出优越性能，为功能数据空间分析提供了有力工具。

Abstract: We have developed a new signature-based spatial scan statistic for functional data (SigFSS). This scan statistic can be applied to both univariate and multivariate functional data. In a simulation study, SigFSS almost always performed better than the literature approaches and yielded more precise clusters in geographic terms. Lastly, we used SigFSS to search for spatial clusters of abnormally high or abnormally low mortality rates in mainland France.

</details>


### [11] [Improving Spatio-temporal Gaussian Process Modeling with Vecchia Approximation: A Low-Cost Sensor-Driven Approach to Urban Environmental Monitoring](https://arxiv.org/abs/2511.22500)
*Yacine Mohamed Idir,Olivier Orfila,Patrice Chatellier,Vincent Judalet*

Main category: stat.ME

TL;DR: 本文探索了Vecchia似然近似方法，用于建模城市环境中移动和固定低成本传感器感知的物理现象，提出了三层次层次模型，并研究了多种创新的Vecchia近似配置，在模拟和真实空气质量数据上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模时空数据集中的计算挑战，特别是在环境监测中使用低成本传感器网络时，需要处理物理过程和传感器测量误差的复杂建模问题。

Method: 提出三层次层次模型同时考虑物理过程和传感器误差；研究多种Vecchia近似配置，包括排序策略、距离定义和传感器特定条件的变化；使用min-max距离算法和随机排序方法；通过模拟数据和真实空气质量数据进行评估。

Result: min-max距离算法排序效果最好，随机排序方法无需预先定义时空距离也表现良好；这两种排序方案在log Kullback-Leibler散度上平均比其他四种方案好102%；通过渐近相对效率分析提供了参数估计优化建议；在真实空气质量数据应用中展示了污染地图和预测的实用价值。

Conclusion: 本研究推进了Vecchia近似在环境监测中大规模时空数据集高斯模型计算挑战的应用，为低成本传感器网络的数据分析提供了有效的建模框架和实用建议。

Abstract: This paper explores Vecchia likelihood approximation for modeling physical phenomena sensed by mobile and fixed low-cost sensors in urban environments. A three-level hierarchical model is proposed to simultaneously accounts for the physical process of interest and measurement errors inherent in low-cost sensors. Several innovative configurations of Vecchia's approximation are investigated, including variations in ordering strategies, distance definitions, and sensor-specific conditioning. These configurations are evaluated for approximating the likelihood of a spatio-temporal Gaussian process, using simulated data based on real mobile sensor trajectories across Nantes, France. Our findings highlight the effectiveness of the min-max distance algorithm for ordering, reaffirming existing literature. Additionally, we demonstrate the utility of a random ordering approach that doesn't require prior definition of a spatio-temporal distance. These two ordering configurations achieved, on average, 102\% better results in log Kullback-Leibler divergence compared with four other ordering schemes studied. Results are supplemented with Asymptotic Relative Efficiency analysis, offering practical recommendations for optimizing parameter estimation. The proposed model and preferred Vecchia configuration are applied to real-world air quality data collected using mobile and fixed low-cost sensors. This application underscores the model's practical value for pollution mapping and prediction in environmental monitoring. This study advances the use of Vecchia's approximation for addressing computational challenges of Gaussian models in large-scale spatio-temporal datasets from environmental monitoring with low-cost sensor networks.

</details>


### [12] [Design-based theory for causal inference](https://arxiv.org/abs/2511.22518)
*Xin Lu,Wanjia Fu,Hongzi Li,Haoyang Yu,Honghao Zhang,Ke Zhu,Hanzhong Liu*

Main category: stat.ME

TL;DR: 本文系统综述了基于设计的因果推断的最新进展，包括协变量平衡随机化设计、基于设计的统计推断方法及其在高维、不依从和网络干扰场景下的扩展。


<details>
  <summary>Details</summary>
Motivation: 因果推断在医学、经济学、教育和社会科学等多个领域具有核心重要性。基于设计的因果推断从随机实验出发，强调利用已知随机化机制进行统计推断，能够在弱模型依赖下识别和估计因果效应。随着高维数据、个体不依从和网络干扰等复杂场景的出现，该领域需要系统总结最新理论和方法进展。

Method: 本文采用系统性综述方法，重点分析三个关键方面：1) 协变量平衡随机化设计（如分层随机化和再随机化）；2) 基于设计的统计推断方法（包括Fisher随机化检验、Neyman式渐近推断和回归调整）；3) 将这些方法扩展到高维数据、个体不依从和网络干扰等复杂场景。

Result: 综述展示了基于设计的因果推断在理论和方法的显著进展，特别是在复杂设置下的扩展。该范式已发展出多种设计策略和推断方法，能够处理现代数据分析中遇到的挑战性问题。

Conclusion: 基于设计的因果推断在理论和应用方面都取得了重要进展，为处理复杂现实世界问题提供了有力工具。未来需要在理论发展和实际应用两个方向继续推进，特别是在新兴复杂场景下的方法创新。

Abstract: Causal inference, as a major research area in statistics and data science, plays a central role across diverse fields such as medicine, economics, education, and the social sciences. Design-based causal inference begins with randomized experiments and emphasizes conducting statistical inference by leveraging the known randomization mechanism, thereby enabling identification and estimation of causal effects under weak model dependence. Grounded in the seminal works of Fisher and Neyman, this paradigm has evolved to include various design strategies, such as stratified randomization and rerandomization, and analytical methods including Fisher randomization tests, Neyman-style asymptotic inference, and regression adjustment. In recent years, with the emergence of complex settings involving high-dimensional data, individual noncompliance, and network interference, design-based causal inference has witnessed remarkable theoretical and methodological advances. This paper provides a systematic review of recent progress in this field, focusing on covariate-balanced randomization designs, design-based statistical inference methods, and their extensions to high-dimensional, noncompliance, and network interference scenarios. It concludes with a comprehensive perspective on future directions for the theoretical development and practical applications of causal inference.

</details>


### [13] [Bayes Factor Hypothesis Testing in Meta-Analyses: Practical Advantages and Methodological Considerations](https://arxiv.org/abs/2511.22535)
*Joris Mulder,Robbie C. M. van Aert*

Main category: stat.ME

TL;DR: 贝叶斯因子在元分析中提供比p值更优的假设检验方法，支持效应存在与否的量化评估，适合累积性和序贯性分析，并能控制I类错误率。


<details>
  <summary>Details</summary>
Motivation: 元分析具有累积性和序贯性特点，传统p值方法存在局限，而贝叶斯因子能提供更全面的证据量化、支持持续证据监测，并保持长期一致性行为。

Method: 通过贝叶斯因子进行假设检验，利用e值理论控制I类错误率，考虑先验敏感性等方法论问题，并开发开源R包BFpack支持应用。

Result: 论文展示了两个应用案例：语言障碍个体的统计学习和乳腺癌患者术后运动后血清肿发生率，验证了贝叶斯因子在元分析中的实用性。

Conclusion: 贝叶斯因子为元分析提供了理论严谨、方法灵活且实用的替代方案，尽管在元分析文献中应用仍有限，但其优势值得更广泛采用。

Abstract: Bayesian hypothesis testing via Bayes factors offers a principled alternative to classical p-value methods in meta-analysis, particularly suited to its cumulative and sequential nature. Unlike p-values, Bayes factors allow for quantifying support both for and against the existence of an effect, facilitate ongoing evidence monitoring, and maintain coherent long-run behavior as additional studies are incorporated. Recent theoretical developments further show how Bayes factors can flexibly control Type I error rates through connections to e-value theory. Despite these advantages, their use remains limited in the meta-analytic literature. This paper provides a critical overview of their theoretical properties, methodological considerations, such as prior sensitivity, and practical advantages for evidence synthesis. Two illustrative applications are provided: one on statistical learning in individuals with language impairments, and another on seroma incidence following post-operative exercise in breast cancer patients. New tools supporting these methods are available in the open-source R package BFpack.

</details>


### [14] [Bayesian Nonparametric Marked Hawkes Processes for Earthquake Modeling](https://arxiv.org/abs/2511.22538)
*Hyotae Kim,Athanasios Kottas*

Main category: stat.ME

TL;DR: 提出了一种用于标记霍克斯过程的贝叶斯非参数建模方法，特别针对地震发生建模，其中标记变量为地震震级。该方法使用基函数表示和伽马过程先验，实现了完全非参数化的标记霍克斯过程强度函数建模。


<details>
  <summary>Details</summary>
Motivation: 现有自激励点过程模型在地震建模中通常假设余震密度与主震震级无关，这限制了模型的推断能力。需要一种能够捕捉震级依赖的余震动态的灵活建模方法。

Method: 使用基函数表示时间滞后和标记变量，通过伽马过程先验定义基权重，构建标记霍克斯过程激发函数的非参数先验模型。进一步引入时间依赖背景强度的非参数先验，实现完全非参数化的标记霍克斯过程强度函数建模。

Result: 该方法能够估计随主震震级变化的余震密度，显著扩展了现有地震自激励点过程模型的推断范围。通过模型性质研究和合成标记点模式推断验证了方法的不同方面，并在1885-1980年日本地震数据分析中展示了震级依赖余震动态建模的实际效用。

Conclusion: 提出的贝叶斯非参数建模方法为标记霍克斯过程提供了灵活且计算可行的框架，特别适用于地震发生建模，能够捕捉震级依赖的余震动态，为地震风险评估提供了更准确的工具。

Abstract: The Hawkes process is a versatile stochastic model for point patterns that exhibit self-excitation, that is, the property that an event occurrence increases the rate of occurrence for some period of time in the future. We present a Bayesian nonparametric modeling approach for temporal marked Hawkes processes. Our focus is on point process modeling of earthquake occurrences, where the mark variable is given by earthquake magnitude. We develop a nonparametric prior model for the marked Hawkes process excitation function, using a representation with basis components for the time lag and the mark, and basis weights defined through a gamma process prior. We elaborate the model with a nonparametric prior for time-dependent background intensity functions, thus enabling a fully nonparametric approach to modeling the ground process intensity of marked Hawkes processes. The model construction balances computationally tractable inference with flexible forms for marked Hawkes process functionals, including mark-dependent offspring densities. The posterior simulation method provides full inference, without any approximations to the Hawkes process likelihood. In the context of the application, the modeling approach enables estimation of aftershock densities that vary with the magnitude of the main shock, thus significantly expanding the inferential scope of existing self-exciting point process models for earthquake occurrences. We investigate different aspects of the methodology through study of model properties, and with inference results based on synthetic marked point patterns. The practical utility of modeling magnitude-dependent aftershock dynamics is demonstrated with analysis of earthquakes that occurred in Japan from 1885 through 1980.

</details>


### [15] [Mapping Urban Air Quality from Mobile Sensors Using Spatio-Temporal Geostatistics](https://arxiv.org/abs/2511.22544)
*Yacine Mohamed Idir,Olivier Orfila,Vincent Judalet,Benoit Sagot,Patrice Chatellier*

Main category: stat.ME

TL;DR: 该研究评估了三种地统计方法（简单克里金法、普通克里金法和外部漂移克里金法）在移动空气质量监测网络数据插值和外推中的性能，发现地统计方法在插值场景中比反距离加权法有显著改进，但在外推场景中效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着微型环境传感器技术的发展，移动空气质量监测网络成为可能，但如何有效整合这些多源数据以提供精确的空气质量地图成为一个挑战。研究旨在探索时空地统计方法作为解决这一问题的方案。

Method: 研究评估了三种地统计方法：残差简单克里金法（SK）、普通克里金法（OK）和外部漂移克里金法（KED），并与标准的反距离加权法（IDW）进行对比。研究分别在插值场景和外推场景中测试这些方法。

Result: 地统计模型在插值场景中平均比IDW方法降低了26.57%的均方根误差（RMSE），其中KED改进27.94%，OK改进26.05%，SK改进25.71%。但在外推场景中改进较小，地统计模型仅比IDW降低12.22%的RMSE。

Conclusion: 单变量地统计方法适用于空气质量数据的插值分析，但对于未采样区域的外推效果有限，因为该方法无法创造新信息。需要结合其他方法或数据源来提高外推能力。

Abstract: With the advancement of technology and the arrival of miniaturized environmental sensors that offer greater performance, the idea of building mobile network sensing for air quality has quickly emerged to increase our knowledge of air pollution in urban environments. However, with these new techniques, the difficulty of building mathematical models capable of aggregating all these data sources in order to provide precise mapping of air quality arises. In this context, we explore the spatio-temporal geostatistics methods as a solution for such a problem and evaluate three different methods: Simple Kriging (SK) in residuals, Ordinary Kriging (OK), and Kriging with External Drift (KED). On average, geostatistical models showed 26.57% improvement in the Root Mean Squared Error (RMSE) compared to the standard Inverse Distance Weighting (IDW) technique in interpolating scenarios (27.94% for KED, 26.05% for OK, and 25.71% for SK). The results showed less significant scores in extrapolating scenarios (a 12.22% decrease in the RMSE for geostatisical models compared to IDW). We conclude that univariable geostatistics is suitable for interpolating this type of data but is less appropriate for an extrapolation of non-sampled places since it does not create any information.

</details>


### [16] [A Framework for Initial Transient Detection and Statistical Assessment of Convergence in CFD Simulations](https://arxiv.org/abs/2511.22618)
*Leonardo Scandurra,Pavlos Alexias,Eugene de Villiers*

Main category: stat.ME

TL;DR: 提出一种基于反转均方误差和分数滤波的时间序列初始瞬态检测方法，结合自相关校正置信区间，实现稳定状态分析的自动化框架。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常包含初始瞬态期，这给分析和解释带来挑战。现有方法在处理自相关数据和噪声时效果有限，需要一种能自动检测瞬态结束并评估收敛的稳健方法。

Method: 使用反转均方误差（RMSE）作为数据稳定性的度量指标，结合分数滤波技术去除噪声并捕捉底层动态特征。采用自相关校正的置信区间构建自动化瞬态检测和收敛评估框架。

Result: 方法在模拟中验证有效，不受数值参数（如时间步长、欠松弛因子）影响。框架轻量、通用性强，能有效减少自相关数据集中的假阳性。

Conclusion: 提出的方法为时间序列稳态分析提供了统计严谨、自动化的可靠工具，特别适用于处理自相关数据，具有实际应用价值。

Abstract: Time series data often contain initial transient periods before reaching a stable state, posing challenges in analysis and interpretation. In this paper, we propose a novel approach to detect and estimate the end of the initial transient in time series data. Our method leverages the reversal mean standard error (RMSE) as a metric for assessing the stability of the data. Additionally, we employ fractional filtering techniques to enhance the detection accuracy by filtering out noise and capturing essential features of the underlying dynamics.
  Combining with autocorrelation-corrected confidence intervals we provide a robust framework to automate transient detection and convergence assessment. The method ensures statistical rigor by accounting for autocorrelation effects, validated through simulations with varying time steps. Results demonstrate independence from numerical parameters (e.g., time step size, under-relaxation factors), offering a reliable tool for steady-state analysis. The framework is lightweight, generalizable, and mitigates inflated false positives in autocorrelated datasets.

</details>


### [17] [High dimensional Mean Test for Temporal Dependent Data](https://arxiv.org/abs/2511.22762)
*Yuchen Hu,Xiaoyi Wang,Long Feng*

Main category: stat.ME

TL;DR: 提出一种针对时间依赖数据的高维均值检验新方法，具有更宽松的假设条件和更低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现有高维均值检验方法通常依赖高斯分布或M-依赖等限制性假设，且计算复杂度高，难以处理大规模时间依赖数据

Method: 建立不依赖高斯分布或M-依赖假设的检验统计量渐近正态性理论框架，设计计算复杂度显著降低的新检验方法

Result: 模拟研究验证了方法的计算优势（复杂度降低）和性能提升，理论框架可扩展到其他高维时间依赖问题

Conclusion: 提出的方法在保持理论严谨性的同时，显著提高了计算效率，为大规模时间依赖数据的高维统计推断提供了实用解决方案

Abstract: This paper proposes a novel test method for high-dimensional mean testing regard for the temporal dependent data. Comparison to existing methods, we establish the asymptotic normality of the test statistic without relying on restrictive assumptions, such as Gaussian distribution or M-dependence. Importantly, our theoretical framework holds potential for extension to other high-dimensional problems involving temporal dependent data. Additionally, our method offers significantly reduced computational complexity, making it more practical for large-scale applications. Simulation studies further demonstrate the computational advantages and performance improvements of our test.

</details>


### [18] [Gaussian approximations for fast Bayesian inference of partially observed branching processes with applications to epidemiology](https://arxiv.org/abs/2511.22833)
*Angus Lewis,Antonio Parrella,John Maclean,Andrew J. Black*

Main category: stat.ME

TL;DR: 提出了一种用于连续时间多类型分支过程的高斯近似方法，通过卡尔曼滤波算法实现高效推理，解决了大规模种群和长时间序列下的计算难题。


<details>
  <summary>Details</summary>
Motivation: 连续时间多类型分支过程的精确推理（通常使用顺序蒙特卡洛方法）在种群呈指数增长或时间序列较长时计算成本极高，需要更高效的近似方法。

Method: 推导了过程转移函数的高斯近似，基于此开发了卡尔曼滤波算法，其运行时间与种群规模无关；同时为小种群情况设计了混合方法。

Result: 在简单和复杂流行病模型上测试，近似方法能很好地逼近真实后验分布，且在大多数情况下获得显著的计算加速；成功应用于具有时变参数的COVID-19数据集，而精确方法因种群规模过大而无法处理。

Conclusion: 提出的高斯近似和卡尔曼滤波算法为大规模连续时间多类型分支过程提供了一种高效、准确的推理框架，特别适用于流行病学等需要处理大规模种群的应用场景。

Abstract: We consider the problem of inference for the states and parameters of a continuous-time multitype branching process from partially observed time series data. Exact inference for this class of models, typically using sequential Monte Carlo, can be computationally challenging when the populations that are being modelled grow exponentially or the time series is long. Instead, we derive a Gaussian approximation for the transition function of the process that leads to a Kalman filtering algorithm that runs in a time independent of the population sizes. We also develop a hybrid approach for when populations are smaller and the approximation is less applicable. We investigate the performance of our approximation and algorithms to both a simple and a complex epidemic model, finding good adherence to the true posterior distributions in both cases with large computational speed-ups in most cases. We also apply our method to a COVID-19 dataset with time dependent parameters where exact methods are intractable due to the population sizes involved.

</details>


### [19] [Constrained Gaussian Random Fields with Continuous Linear Boundary Restrictions for Physics-informed Modeling of States](https://arxiv.org/abs/2511.22868)
*Yue Ma,Oksana A. Chkrebtii,Stephen R. Niezgoda*

Main category: stat.ME

TL;DR: 提出一种构建边界约束高斯随机场的新框架，用于在多维凸域上精确实施物理边界条件


<details>
  <summary>Details</summary>
Motivation: 物理、环境和工程模型中的边界约束（如固定状态、固定导数或状态-导数关系）在现有高斯随机场中无法精确实施，需要开发能结合已知物理边界机制的先验模型

Method: 开发了一个通用框架，从无约束的高斯随机场构建线性边界约束的高斯随机场，适用于多维凸域

Result: 模拟研究表明，这种物理信息概率模型在概率数值方法、数据驱动的动力系统发现和边界约束状态估计等应用中，相比无约束替代方案具有更好的预测性能和更真实的量化不确定性

Conclusion: 该框架为具有已知边界物理机制的系统状态建模提供了灵活的先验，能有效结合物理约束与概率建模的优势

Abstract: Boundary constraints in physical, environmental and engineering models restrict smooth states such as temperature to follow known physical laws at the edges of their spatio-temporal domain. Examples include fixed-state or fixed-derivative (insulated) boundary conditions, and constraints that relate the state and the derivatives, such as in models of heat transfer. Despite their flexibility as prior models over system states, Gaussian random fields do not in general enable exact enforcement of such constraints. This work develops a new general framework for constructing linearly boundary-constrained Gaussian random fields from unconstrained Gaussian random fields over multi-dimensional, convex domains. This new class of models provides flexible priors for modeling smooth states with known physical mechanisms acting at the domain boundaries. Simulation studies illustrate how such physics-informed probability models yield improved predictive performance and more realistic uncertainty quantification in applications including probabilistic numerics, data-driven discovery of dynamical systems, and boundary-constrained state estimation, as compared to unconstrained alternatives.

</details>


### [20] [Joint Bayesian Inference of Parameter and Discretization Error Uncertainties in ODE Models](https://arxiv.org/abs/2511.23010)
*Shoji Toyota,Yuto Miyatake*

Main category: stat.ME

TL;DR: 提出了一种贝叶斯推断框架，用于常微分方程模型参数估计，显式量化数值求解器引入的离散化误差，将离散化误差建模为随机变量并同时推断ODE参数和离散化误差方差。


<details>
  <summary>Details</summary>
Motivation: 传统ODE参数估计方法（如欧拉法或龙格-库塔法）通常忽略数值求解器引入的离散化误差，这可能导致参数估计不准确且无法量化不确定性。需要一种能够显式处理离散化误差的贝叶斯推断方法。

Method: 将离散化误差建模为随机变量，引入马尔可夫先验来描述离散化误差方差的时间演化，将推断问题转化为状态空间模型。提出一种基于标准离散化误差分析的自然马尔可夫先验形式，该先验依赖于数值求解器的步长。

Result: 数值实验表明，该方法能够同时量化ODE参数和离散化误差的不确定性，通过考虑离散化误差可以产生具有更广泛支持的参数后验分布。

Conclusion: 提出的贝叶斯框架能够显式量化离散化误差，为ODE参数估计提供了更全面的不确定性量化方法，当步长趋近于零时具有渐近性质。

Abstract: We address the problem of Bayesian inference for parameters in ordinary differential equation (ODE) models based on observational data. Conventional approaches in this setting typically rely on numerical solvers such as the Euler or Runge-Kutta methods. However, these methods generally do not account for the discretization error induced by discretizing the ODE model. We propose a Bayesian inference framework for ODE models that explicitly quantifies discretization errors. Our method models discretization error as a random variable and performs Bayesian inference on both ODE parameters and variances of the randomized discretization errors, referred to as the discretization error variance. A key idea of our approach is the introduction of a Markov prior on the temporal evolution of the discretization error variances, enabling the inference problem to be formulated as a state-space model. Furthermore, we propose a specific form of the Markov prior that arises naturally from standard discretization error analysis. This prior depends on the step size in the numerical solver, and we discuss its asymptotic property in the limit as the step size approaches zero. Numerical experiments illustrate that the proposed method can simultaneously quantify uncertainties in both the ODE parameters and the discretization errors, and can produce posterior distributions over the parameters with broader support by accounting for discretization error.

</details>


### [21] [A General Bayesian Nonparametric Approach for Estimating Population-Level and Conditional Causal Effects](https://arxiv.org/abs/2511.23085)
*Yongseok Hur,Joonhyuk Jung,Juhee Lee*

Main category: stat.ME

TL;DR: 提出一种基于贝叶斯非参数方法的因果推断模型，使用依赖非参数混合模型灵活建模结果变量，有效处理混杂偏倚和异质性治疗效应，提供完全概率推断。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法通常依赖参数假设，难以灵活处理混杂变量对条件分布的复杂影响，也无法有效建模治疗效应异质性。需要一种既能灵活调整混杂偏倚，又能提供完全概率推断的方法。

Method: 采用贝叶斯非参数方法，使用依赖非参数混合模型建模结果变量的条件分布，其中原子和权重都随混杂变量变化。通过数据增广实现高效的后验推断。

Result: 模型在广泛评估中表现出色，优于或与多种最新方法（包括BART）竞争，能更准确地估计平均治疗效应和异质性治疗效应，并提供完全概率推断。

Conclusion: 提出的贝叶斯非参数模型为因果推断提供了灵活且强大的框架，既能有效处理混杂偏倚和异质性，又能提供完全概率推断，在理论和实践上都具有重要价值。

Abstract: We propose a Bayesian nonparametric (BNP) approach to causal inference using observational data consisting of outcome, treatment, and a set of confounders. The conditional distribution of the outcome given treatment and confounders is modeled flexibly using a dependent nonparametric mixture model, in which both the atoms and the weights vary with the confounders. The proposed BNP model is well suited for causal inference problems, as it does not rely on parametric assumptions about how the conditional distribution depends on the confounders. In particular, the model effectively adjusts for confounding and improves the modeling of treatment effect heterogeneity, leading to more accurate estimation of both the average treatment effect (ATE) and heterogeneous treatment effects (HTE). Posterior inference under the proposed model is computationally efficient due to the use of data augmentation. Extensive evaluations demonstrate that the proposed model offers competitive or superior performance compared to a wide range of recent methods spanning various statistical approaches, including Bayesian additive regression tree (BART) models, which are well known for their strong empirical performance. More importantly, the model provides fully probabilistic inference on quantities of interest that other methods cannot easily provide, using their posterior distributions.

</details>


### [22] [Inference for quantile-parametrized families via CDF confidence bands](https://arxiv.org/abs/2511.23086)
*Srijan Chattopadhyay,Siddhaarth Sarkar,Arun Kumar Kuchibhotla*

Main category: stat.ME

TL;DR: 提出一种基于分位数函数的分布自由推断框架，用于处理分位数分布族参数估计问题，避免传统似然方法的计算和理论困难。


<details>
  <summary>Details</summary>
Motivation: 分位数分布族参数模型（如Tukey Lambda分布）的CDF和密度函数没有闭式表达式，传统最大似然估计可能产生非√n和非正态渐近性，导致自助法和重采样技术不可靠。

Method: 通过已知分位数函数反转经验CDF的分布自由置信带来构建置信集，提供原则性且假设较少的替代方法，仅需参数模型规范，避免基于似然方法的计算和理论困难。

Result: 在Tukey Lambda和广义Lambda分布上验证了框架的有效性，通过模拟研究评估性能，并在小样本（双胞胎研究）和大样本（西班牙家庭收入）数据集上展示了实际应用价值。

Conclusion: 该推断框架为复杂分位数分布族提供了一种稳健的替代推断方法，克服了传统似然方法的局限性，在实际应用中表现出良好的性能。

Abstract: Quantile-based distribution families are an important subclass of parametric families, capable of exhibiting a wide range of behaviors using very few parameters. These parametric models present significant challenges for classical methods, since the CDF and density do not have a closed-form expression. Furthermore, approximate maximum likelihood estimation and related procedures may yield non-$\sqrt{n}$ and non-normal asymptotics over regions of the parameter space, making bootstrap and resampling techniques unreliable. We develop a novel inference framework that constructs confidence sets by inverting distribution-free confidence bands for the empirical CDF through the known quantile function. Our proposed inference procedure provides a principled and assumption-lean alternative in this setting, requiring no distributional assumptions beyond the parametric model specification and avoiding the computational and theoretical difficulties associated with likelihood-based methods for these complex parametric families. We demonstrate our framework on Tukey Lambda and generalized Lambda distributions, evaluate its performance through simulation studies, and illustrate its practical utility with an application to both a small-sample dataset (Twin Study) and a large-sample dataset (Spanish household incomes).

</details>


### [23] [Machine learning for violence prediction: a systematic review and critical appraisal](https://arxiv.org/abs/2511.23118)
*Stefaniya Kozhevnikova,Denis Yukhnenko,Giulio Scola,Seena Fazel*

Main category: stat.ME

TL;DR: 系统综述发现机器学习暴力行为预测模型存在方法学质量问题，临床实用性有限，但识别高危个体有潜力


<details>
  <summary>Details</summary>
Motivation: 系统评估机器学习模型在预测暴力行为方面的有效性、实用性和性能，为临床实践和研究提供指导

Method: 系统检索9个文献数据库和Google Scholar至2025年9月，纳入暴力行为预测机器学习模型的开发和/或验证研究，通过总结区分度和校准性能统计量合成结果，并通过检查偏倚风险和临床实用性评估研究质量

Result: 纳入38项研究（40个模型），AUC范围0.68-0.99，仅8项研究报告校准性能，3项报告外部验证，31项存在高偏倚风险，整体临床实用性较差（样本小、过拟合、报告不透明、泛化性低）

Conclusion: 黑盒机器学习模型目前临床适用性有限，但识别高危个体有潜力；提出五项关键建议：确保方法学质量、仅对复杂数据使用黑盒算法、纳入动态预测、开发可解释算法、适当应用因果机器学习方法

Abstract: Purpose To conduct a systematic review of machine learning models for predicting violent behaviour by synthesising and appraising their validity, usefulness, and performance.
  Methods We systematically searched nine bibliographic databases and Google Scholar up to September 2025 for development and/or validation studies on machine learning methods for predicting all forms of violent behaviour. We synthesised the results by summarising discrimination and calibration performance statistics and evaluated study quality by examining risk of bias and clinical utility.
  Results We identified 38 studies reporting the development and validation of 40 models. Most studies reported Area Under the Curve (AUC) as the discrimination statistic with a range of 0.68-0.99. Only eight studies reported calibration performance, and three studies reported external validation. 31 studies had a high risk of bias, mainly in the analysis domain, and three studies had low risk of bias. The overall clinical utility of violence prediction models is poor, as indicated by risks of overfitting due to small samples, lack of transparent reporting, and low generalisability.
  Conclusion Although black box machine learning models currently have limited applicability in clinical settings, they may show promise for identifying high-risk individuals. We recommend five key considerations for violence prediction modelling: (i) ensuring methodological quality (e.g. following guidelines) and interdisciplinary collaborations; (ii) using black box algorithms only for highly complex data; (iii) incorporating dynamic predictions to allow for risk monitoring; (iv) developing more trustworthy algorithms using explainable methods; and (v) applying causal machine learning approaches where appropriate.

</details>


### [24] [Goodness-of-fit testing for the error distribution in functional linear models](https://arxiv.org/abs/2511.23137)
*Natalie Neumeyer,Leonie Selk*

Main category: stat.ME

TL;DR: 该论文研究函数线性模型中误差分布的统计推断，包括经验分布函数和特征函数的渐近展开，并将其应用于误差分布的正态性检验等拟合优度检验。


<details>
  <summary>Details</summary>
Motivation: 在函数线性模型中，误差分布的假设检验对于模型诊断和统计推断至关重要。然而，基于估计残差的误差分布检验理论尚未充分发展，特别是在函数协变量背景下。

Method: 讨论了在不同模型假设下，基于估计残差的经验分布函数和经验特征函数的渐近展开。将这些理论结果应用于误差分布的简单和复合拟合优度检验，特别是正态性检验。

Result: 建立了函数线性模型中误差分布检验的渐近理论框架，为基于估计残差的拟合优度检验提供了理论基础，特别关注正态性检验的统计性质。

Conclusion: 该研究为函数线性模型中误差分布的统计检验提供了系统的理论框架，扩展了传统线性模型的诊断工具到函数数据领域，具有重要的理论和应用价值。

Abstract: We consider the error distribution in functional linear models with scalar response and functional covariate. Different asymptotic expansions of the empirical distribution function and the empirical characteristic function based on estimated residuals under different model assumptions are discussed. The results are applied for simple and composite goodness-of-fit testing for the error distribution, in particular testing for normal distribution.

</details>


### [25] [A Design-Based Matching Framework for Staggered Adoption with Time-Varying Confounding](https://arxiv.org/abs/2511.23208)
*Suehyun Kim,Kwonsang Lee*

Main category: stat.ME

TL;DR: 提出基于设计的框架，用于分析面板数据中交错处理采用下的组别-时间特定处理效应，开发反向时间嵌套匹配算法，应用于Netflix-IPTV数据集发现订阅Netflix对IPTV总观看时间无显著影响但减少VoD使用。


<details>
  <summary>Details</summary>
Motivation: 纵向数据中的因果推断面临动态处理采用和时间变化混杂变量的挑战，现有方法要么未能考虑处理采用队列和处理时间间的异质性，要么依赖建模假设。

Method: 开发基于设计的框架，建立识别结果，提出估计量，使用块自助法估计协方差矩阵并检验组别-时间处理效应的同质性，提出反向时间嵌套匹配算法构建匹配层。

Result: 应用于Netflix-IPTV数据集发现：Netflix订阅对IPTV总观看时间无显著影响，但负向影响VoD使用，统计证据表明Netflix订阅的因果效应在处理队列内和跨相同结果及事件时间间存在异质性。

Conclusion: 提出的框架为交错处理采用下面板数据中的因果推断提供了新方法，反向时间嵌套匹配算法确保协变量历史可比性，实证分析揭示了媒体消费行为的复杂因果模式。

Abstract: Causal inference in longitudinal datasets has long been challenging due to dynamic treatment adoption and confounding by time-varying covariates. Prior work either fails to account for heterogeneity across treatment adoption cohorts and treatment timings or relies on modeling assumptions. In this paper, we develop a novel design-based framework for inference on group- and time-specific treatment effects in panel data with staggered treatment adoption. We establish identification results for causal effects under this structure and introduce corresponding estimators, together with a block bootstrap procedure for estimating the covariance matrix and testing the homogeneity of group-time treatment effects. To implement the framework in practice, we propose the Reverse-Time Nested Matching algorithm, which constructs matched strata by pairing units from different adoption cohorts in a way that ensures comparability of covariate histories at each treatment time. Applying the algorithm to the Netflix-IPTV dataset, we find that while Netflix subscription does not significantly affect total IPTV viewing time, it does negatively affect VoD usage. We also provide statistical evidence that the causal effects of Netflix subscription may vary even within the same treatment cohort or across the same outcome and event times.

</details>


### [26] [Comparing Variable Selection and Model Averaging Methods for Logistic Regression](https://arxiv.org/abs/2511.23216)
*Nikola Sekulovski,František Bartoš,Don van den Bergh,Giuseppe Arena,Henrik R. Godmann,Vipasha Goyal,Julius M. Pfadt,Maarten Marsman,Adrian E. Raftery*

Main category: stat.ME

TL;DR: 比较了28种处理逻辑回归模型不确定性的方法，发现无分离时基于g先验的贝叶斯模型平均表现最佳，有分离时LASSO等惩罚似然方法最稳定。


<details>
  <summary>Details</summary>
Motivation: 逻辑回归等二元结果统计模型面临模型不确定性挑战（不清楚应包含哪些预测变量）。虽然已有许多方法，但在实际条件下的相对性能仍不清楚。

Method: 使用11个经验数据集（涵盖不同样本量和预测变量数量），在有无分离两种情况下，对28种变量选择和推断方法进行了预注册的模拟比较。

Result: 无分离时，基于g先验的贝叶斯模型平均方法（特别是g = max(n, p^2)）表现最佳；有分离时，惩罚似然方法（尤其是LASSO）提供最稳定结果；局部经验贝叶斯先验的贝叶斯模型平均在两种情况下都有竞争力。

Conclusion: 为应用研究者提供了处理逻辑回归模型不确定性的实用指导：无分离时推荐基于g先验的贝叶斯模型平均，有分离时推荐LASSO等惩罚似然方法。

Abstract: Model uncertainty is a central challenge in statistical models for binary outcomes such as logistic regression, arising when it is unclear which predictors should be included in the model. Many methods have been proposed to address this issue for logistic regression, but their relative performance under realistic conditions remains poorly understood. We therefore conducted a preregistered, simulation-based comparison of 28 established methods for variable selection and inference under model uncertainty, using 11 empirical datasets spanning a range of sample sizes and numbers of predictors, in cases both with and without separation. We found that Bayesian model averaging methods based on g-priors, particularly with g = max(n, p^2), show the strongest overall performance when separation is absent. When separation occurs, penalized likelihood approaches, especially the LASSO, provide the most stable results, while Bayesian model averaging with the local empirical Bayes (EB-local) prior is competitive in both situations. These findings offer practical guidance for applied researchers on how to effectively address model uncertainty in logistic regression in modern empirical and machine learning research.

</details>


### [27] [Conjugate Generalised Bayesian Inference for Discrete Doubly Intractable Problems](https://arxiv.org/abs/2511.23275)
*William Laplante,Matias Altamirano,Jeremias Knoblauch,Andrew Duncan,François-Xavier Briol*

Main category: stat.ME

TL;DR: 提出一种针对离散数据指数族模型的新型广义贝叶斯后验方法，解决双难处理问题，计算效率比现有方法快10-6000倍


<details>
  <summary>Details</summary>
Motivation: 双难处理问题中似然函数和后验分布都只有未归一化形式，归一化常数计算困难，传统贝叶斯推断需要昂贵专门的MCMC方法，需要更高效的计算替代方案

Method: 提出新型广义贝叶斯后验方法，允许在离散数据指数族模型中进行共轭推断，推导理论保证来刻画广义后验的渐近行为

Result: 方法在多个挑战性难处理指数族模型上评估，包括Conway-Maxwell-Poisson图模型、自回归离散时间序列模型、Ising和Potts等马尔可夫随机场，计算效率显著提升10-6000倍

Conclusion: 该方法为双难处理问题提供了计算高效的贝叶斯推断替代方案，支持在离散数据指数族模型中进行共轭推断，具有理论保证和实际计算优势

Abstract: Doubly intractable problems occur when both the likelihood and the posterior are available only in unnormalised form, with computationally intractable normalisation constants. Bayesian inference then typically requires direct approximation of the posterior through specialised and typically expensive MCMC methods. In this paper, we provide a computationally efficient alternative in the form of a novel generalised Bayesian posterior that allows for conjugate inference within the class of exponential family models for discrete data. We derive theoretical guarantees to characterise the asymptotic behaviour of the generalised posterior, supporting its use for inference. The method is evaluated on a range of challenging intractable exponential family models, including the Conway-Maxwell-Poisson graphical model of multivariate count data, autoregressive discrete time series models, and Markov random fields such as the Ising and Potts models. The computational gains are significant; in our experiments, the method is between 10 and 6000 times faster than state-of-the-art Bayesian computational methods.

</details>


### [28] [Getting it right: Methods for risk ratios and risk differences cluster randomized trials with a small number of clusters](https://arxiv.org/abs/2511.23419)
*Shifeng Sun,Xueqi Wang,Zhuoran Hou,Elizabeth L. Turner*

Main category: stat.ME

TL;DR: 该论文研究在小样本群组随机试验中，针对风险比和风险差的偏倚校正标准误方法的表现评估。


<details>
  <summary>Details</summary>
Motivation: 大多数群组随机试验（CRTs）的总群组数少于30-40个，对于这种"小"样本CRTs，需要适当考虑小样本量的方法。虽然已有几种偏倚校正方法在比值比推断中进行了评估，但它们在风险比和风险差推断中的有限样本表现尚未得到充分研究。

Method: 通过二项分布、泊松分布和高斯模型进行分析，包括对数连接函数（风险比）和恒等连接函数（风险差）。采用"修正"方法处理错误指定的泊松和高斯模型。考虑多种场景：罕见结局、小群组规模、高群组内相关性（ICC）和高群组规模变异系数（CV）。

Result: 论文填补了现有研究空白，系统评估了KC、MD、MBN和AVG等偏倚校正标准误方法在小样本CRTs中风险比和风险差推断的表现。

Conclusion: 对于小样本群组随机试验，需要采用适当的偏倚校正标准误方法，并结合t统计量而非Z统计量进行干预效果推断，以避免I类错误膨胀。该研究为风险比和风险差推断提供了方法学指导。

Abstract: Most cluster randomized trials (CRTs) randomize fewer than 30-40 clusters in total. When performing inference for such ``small'' CRTs, it is important to use methods that appropriately account for the small sample size. When the generalized estimating equations (GEE) approach is used for analysis of ``small'' CRTs, the robust variance estimator from GEE is biased downward and therefore bias-corrected standard errors should be used. Moreover, in order to avoid inflated Type I error, an appropriate bias-corrected standard error should be paired with the t- rather than Z-statistic when making inference about a single-parameter intervention effect. Although several bias-correction methods (including Kauermann and Carroll (KC), Mancl and DeRouen (MD), Morel, Bokossa, and Neerchal (MBN), and the average of KC and MD (AVG)) have been evaluated for inference for odds ratios, their finite-sample behavior in ``small'' CRTs with few clusters has not been thoroughly investigated for risk ratios and risk differences. The current article aims to fill the gap by including analysis via binomial, Poisson and Gaussian models and for a broad spectrum of scenarios. Analysis is via binomial and Poisson models (using log and identity link for risk and differences measures, respectively). We additionally explore the use of Gaussian models with identity link for risk differences and adopt the "modified" approach for analysis with misspecified Poisson and Gaussian models. We consider a broad spectrum of scenarios including for rare outcomes, small cluster sizes, high intracluster correlations (ICCs), and high coefficients of variation (CVs) of cluster size.

</details>


### [29] [Consensus Tree Estimation with False Discovery Rate Control via Partially Ordered Sets](https://arxiv.org/abs/2511.23433)
*Maria Alejandra Valdez Cabrera,Amy D Willis,Armeen Taeb*

Main category: stat.ME

TL;DR: 将共识树估计重构为结构化特征选择问题，在偏序集上定义真/假发现，开发能控制FDR的估计算法，适用于非参数生成模型


<details>
  <summary>Details</summary>
Motivation: 树状结构在多个领域广泛存在，但如何从树集合中提取代表性共识树具有挑战性，传统方法缺乏统计保证

Method: 引入叶标记树的偏序关系，定义候选总结树的真/假发现，开发控制假发现率(FDR)的估计算法，支持不等叶集和非二叉树

Result: 获得首个具有有限样本和模型无关保证的共识树估计器，可量化深度分支顺序的不确定性，应用于真核细胞古菌起源研究

Conclusion: 将共识树构建重构为偏序集上的特征选择，为整合多重检验工具到树估计提供了基础框架

Abstract: Connected acyclic graphs (trees) are data objects that hierarchically organize categories. Collections of trees arise in a diverse variety of fields, including evolutionary biology, public health, machine learning, social sciences and anatomy. Summarizing a collection of trees by a single representative is challenging, in part due to the dimension of both the sample and parameter space. We frame consensus tree estimation as a structured feature-selection problem, where leaves and edges are the features. We introduce a partial order on leaf-labeled trees, use it to define true and false discoveries for a candidate summary tree, and develop an estimation algorithm that controls the false discovery rate at a nominal level for a broad class of non-parametric generative models. Furthermore, using the partial order structure, we assess the stability of each feature in a selected tree. Importantly, our method accommodates unequal leaf sets and non-binary trees, allowing the estimator to reflect uncertainty by collapsing poorly supported structure instead of forcing full resolution. We apply the method to study the archaeal origin of eukaryotic cells and to quantify uncertainty in deep branching orders. While consensus tree construction has historically been viewed as an estimation task, reframing it as feature selection over a partially ordered set allows us to obtain the first estimator with finite-sample and model-free guarantees. More generally, our approach provides a foundation for integrating tools from multiple testing into tree estimation.

</details>


### [30] [The $L$-test: Increasing the Linear Model $F$-test's Power Under Sparsity Without Sacrificing Validity](https://arxiv.org/abs/2511.23466)
*Danielle Paulson,Souhardya Sengupta,Lucas Janson*

Main category: stat.ME

TL;DR: 提出L-test方法，用于检验高斯线性模型中回归系数的显著性，在干扰系数稀疏时比传统F-test更有效，同时保持相同的统计有效性保证。


<details>
  <summary>Details</summary>
Motivation: 传统F-test在检验回归系数显著性时，当干扰系数（nuisance coefficients）稀疏时统计功效不足。需要一种既能保持统计有效性，又在干扰系数稀疏时具有更高功效的检验方法。

Method: 提出L-test方法，基于蒙特卡洛抽样，每个样本的计算主要涉及简单的矩阵-向量乘法，保持计算效率。还提供了无需蒙特卡洛的变体，适用于大规模多重检验应用。

Result: L-test在保持与F-test相同统计有效性保证的同时，在干扰系数稀疏时具有更高功效。通过大量模拟验证了其优势，并在HIV耐药性数据集的实际应用中展示了实用性。

Conclusion: L-test为高斯线性模型中的显著性检验提供了更有效的替代方案，特别是在干扰系数稀疏的情况下。该方法还可推广到更广泛的参数模型类别，只要这些模型具有渐近高斯估计量。

Abstract: We introduce a new procedure for testing the significance of a set of regression coefficients in a Gaussian linear model with $n \geq d$. Our method, the $L$-test, provides the same statistical validity guarantee as the classical $F$-test, while attaining higher power when the nuisance coefficients are sparse. Although the $L$-test requires Monte Carlo sampling, each sample's runtime is dominated by simple matrix-vector multiplications so that the overall test remains computationally efficient. Furthermore, we provide a Monte-Carlo-free variant that can be used for particularly large-scale multiple testing applications. We give intuition for the power of our approach, validate its advantages through extensive simulations, and illustrate its practical utility in both single- and multiple-testing contexts with an application to an HIV drug resistance dataset. In the concluding remarks, we also discuss how our methodology can be applied to a more general class of parametric models that admit asymptotically Gaussian estimators.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [31] [A two-parameter, minimal-data model to predict dengue cases: the 2022-2023 outbreak in Florida, USA](https://arxiv.org/abs/2511.22040)
*Saman Hosseini,Lee W. Cohnstaedt,Caterina Scoglio*

Main category: stat.AP

TL;DR: 提出基于发病率-累计病例曲线的数据简约框架，仅需目标季节的发病率时间序列和两个参数，实现登革热预测，计算成本低且适用于数据稀疏环境。


<details>
  <summary>Details</summary>
Motivation: 现有登革热预测模型通常依赖特定地点的协变量和昆虫学数据，在数据稀疏环境中泛化能力有限。需要开发更简约、通用的预测框架，特别是在缺乏密集监测或长期历史记录的地区。

Method: 提出数据简约框架，基于发病率-累计病例曲线，从基本SIR模型扩展到两群体SEIR模型。仅使用目标季节的发病率时间序列，估计两个参数。采用贝叶斯扩展量化病例报告和拟合不确定性，生成校准的预测区间。

Result: 在佛罗里达州2022-2023年疫情中评估，DP框架表现出有竞争力的预测性能，计算成本显著低于复杂模型。适用于登革热早期检测，特别是在缺乏密集监测或长期历史记录的环境中。

Conclusion: 数据简约框架为登革热预测提供了实用解决方案，在数据稀疏环境中具有优势，能够为针对性病媒控制和临床准备提供可靠、及时的预测，降低疾病负担和医疗系统成本。

Abstract: Reliable and timely dengue predictions provide actionable lead time for targeted vector control and clinical preparedness, reducing preventable diseases and health-system costs in at-risk communities. Dengue forecasting often relies on site-specific covariates and entomological data, limiting generalizability in data-sparse settings. We propose a data-parsimonious (DP) framework based on the incidence versus cumulative cases (ICC) curve, extending it from a basic SIR to a two-population SEIR model for dengue. Our DP model uses only the target season's incidence time series and estimates only two parameters, reducing noise and computational burden. A Bayesian extension quantifies the case reporting and fitting uncertainty to produce calibrated predictive intervals. We evaluated the performance of the DP model in the 2022-2023 outbreaks in Florida, where standardized clinical tests and reporting support accurate case determination. The DP framework demonstrates competitive predictive performance at substantially lower computational cost than more elaborate models, making it suitable for dengue early detection where dense surveillance or long historical records are unavailable.

</details>


### [32] [Spatial Two-Stage Hierarchical Optimization Analysis for Site Selection of Bitcoin Mining in South Korea](https://arxiv.org/abs/2511.21773)
*Yoonseul Choi,Jungsoon Choi*

Main category: stat.AP

TL;DR: 开发两阶段分层优化框架，利用比特币挖矿作为灵活负载来货币化韩国分布式太阳能剩余电力，通过成本效益优化和GIS筛选确定最优选址。


<details>
  <summary>Details</summary>
Motivation: 韩国面临分布式太阳能剩余电力增长和比特币挖矿等高能耗产业的双重挑战，需要一种客观的选址方法来利用挖矿作为灵活负载货币化"净计量剩余"电力。

Method: 采用两阶段分层优化框架：第一阶段（区域级）进行成本效益优化，确定最优区域数量和组合；第二阶段（点位级）在选定区域内使用GIS滑动窗口搜索，应用地形和土地利用约束识别可建设的"单元站点"。

Result: 模型识别出最优的3个区域（龙仁、潭阳、密阳），最大潜在净利润约3.07亿美元。第二阶段筛选显示，最盈利的龙仁区域物理约束最大，87%的站点被过滤掉。

Conclusion: 该研究提供了一个可扩展、客观的能源基础设施选址框架，有效整合多尺度空间数据，为政策制定者和电网运营商提供了数据驱动的策略来货币化被削减的可再生能源并增强电网稳定性。

Abstract: South Korea faces the dual challenge of managing growing distributed solar energy surpluses and the high energy demand of industries like Bitcoin mining. Leveraging mining operations as a flexible load to monetize this `net-metering surplus' presents a viable synergy, but requires a robust site selection methodology. Traditional GIS-based Multi-Criteria Decision Analysis (MCDA) struggles with subjective weighting and integrating heterogeneous spatial data (areal-level and lattice-level). This thesis develops and implements a Two-Stage Hierarchical Optimization framework to overcome these limitations. Stage 1 (Areal-Level) employs a cost-benefit optimization to determine the optimal number ($K^*$) and combination of regions, maximizing a final adjusted net profit by balancing surplus power revenue against detailed land and non-linear infrastructure costs. Stage 2 (Point-Level) then uses a GIS-based sliding window search within these selected regions, applying topographic (slope $< 6.0^\circ$) and land-use constraints at a 30m resolution to identify physically constructible `unit sites'. The model identified an optimal configuration of $K^*=3$ regions (Yongin, Damyang, Miryang) yielding a maximum potential net profit of approximately \$307 million. Crucially, the Stage 2 screening revealed that Yongin, the most profitable region, was also the most physically constrained, 87\% of sites filtered out. This research contributes a scalable, objective framework for energy infrastructure siting that effectively integrates multi-scale spatial data. It provides a data-driven strategy for policymakers and grid operators (like Korea Electric Power Corporation) to monetize curtailed renewables and enhance grid stability.

</details>


### [33] [Survey-Based Estimation of Probe Group Sizes in the Network Scale-up Method: A Case Study from Jordan](https://arxiv.org/abs/2511.21938)
*Ian Laga*

Main category: stat.AP

TL;DR: 本文提出了一种使用直接探测组方法的网络规模放大法，用于估计约旦移民家政工人这一边缘化群体规模，并通过贝叶斯逻辑混合效应模型稳定了小区域估计。


<details>
  <summary>Details</summary>
Motivation: 边缘化群体（如约旦的移民家政工人）由于污名化和法律限制，难以通过普查和行政数据进行规模估计，需要开发可靠的替代方法。

Method: 采用网络规模放大法中的直接探测组方法，从调查受访者自身成员身份估计探测组规模，结合贝叶斯逻辑混合效应模型在省一级稳定小区域估计。

Result: 直接探测组方法能产生可靠的推断，提供了在缺乏已知探测组规模时的实用替代方案，并揭示了社会网络规模和与移民家政工人连接度的区域差异。

Conclusion: 直接探测组方法比依赖官方记录更可能满足无偏估计的条件，为样本量有限的调查中应用网络规模放大法提供了首个系统验证和指导。

Abstract: Estimating the size of marginalized populations is a persistent challenge in survey statistics and public health, especially where stigma and legal restrictions exclude such groups from census and administrative data. Migrant domestic workers in Jordan represent one such population. We employ the Network Scale-up Method using the direct probe group method, estimating probe group sizes from survey respondents' own membership rather than relying on external counts. Using data from a nationally representative household survey in Jordan, we combine the direct probe group method with Bayesian logistic mixed-effects models to stabilize small-area estimates at the Governorate level. We validate the method against census data, demonstrating that direct probe group estimates yield reliable inference and provide a practical alternative where known probe group sizes are unavailable. Our results highlight regional variation in social network size and connectivity to migrant domestic workers. We argue that the direct probe group method is more likely to satisfy the conditions required for unbiased estimation than relying on official record sizes. This work provides the first systematic validation of the direct probe group method in a small-area setting and offers guidance for adapting the Network Scale-up Method to surveys with limited sample sizes.

</details>


### [34] [Spatial constraints improve filtering of measurement noise from animal tracks](https://arxiv.org/abs/2511.22430)
*Alexandre Delporte,Susanne Ditlevsen,Adeline Samson*

Main category: stat.AP

TL;DR: 提出一种基于欠阻尼朗之万SDE的动物运动模型，包含空间约束项，并开发了适用于非高斯误差的扩展卡尔曼滤波和粒子滤波算法。


<details>
  <summary>Details</summary>
Motivation: 随着动物追踪技术的发展，数据量增加需要新的统计工具。GPS和Argos卫星系统获取的位置数据包含复杂的非高斯和重尾测量误差，传统卡尔曼滤波对非高斯误差敏感。

Method: 使用欠阻尼朗之万随机微分方程建立潜在运动模型，加入额外的漂移项确保动物保持在已知空间域内。通过分裂方案求解SDE近似潜在动态，实现扩展卡尔曼滤波和适应非高斯误差分布的粒子滤波。

Result: 将空间约束整合到潜在运动模型中提高了对噪声位置观测的滤波精度。提出的滤波器能更好地处理非高斯误差分布。

Conclusion: 提出的空间约束SDE模型和改进的滤波方法为处理复杂动物运动数据提供了更准确的统计工具，特别适用于水生动物或受限制区域内的陆地动物。

Abstract: Advances in tracking technologies for animal movement require new statistical tools to better exploit the increasing amount of data. Animal positions are usually calculated using the GPS or Argos satellite system and include potentially complex non-Gaussian and heavy-tailed measurement error patterns. Errors are usually handled through a Kalman filter algorithm, which can be sensitive to non-Gaussian error distributions.
  In this paper, we introduce a realistic latent movement model through an underdamped Langevin stochastic differential equation (SDE) that includes an additional drift term to ensure that the animal remains in a known spatial domain of interest. This can be applied to aquatic animals moving in water or terrestrial animals moving in a restricted zone delimited by fences or natural barriers. We demonstrate that the incorporation of these spatial constraints into the latent movement model improves the accuracy of filtering for noisy observations of the positions. We implement an Extended Kalman Filter as well as a particle filter adapted to non-Gaussian error distributions. Our filters are based on solving the SDE through splitting schemes to approximate the latent dynamic.

</details>


### [35] [Mapping urban air quality using mobile and fixed low cost sensors: a model comparison](https://arxiv.org/abs/2511.22550)
*Yacine Mohamed Idir,Olivier Orfila,Patrice Chatellier,Vincent Judalet,Valentin Guaffre*

Main category: stat.AP

TL;DR: 该研究评估了10种统计模型在利用低成本传感器数据（包括车载移动传感器）预测城市空气质量时的性能，发现仅依赖低成本传感器数据会导致模型偏差，而机器学习模型在预测场景中表现更优，建议将低成本传感器数据与固定监测站数据整合校准以提高精度。


<details>
  <summary>Details</summary>
Motivation: 解决在未监测地点确定污染物浓度的关键挑战，利用低成本传感器（特别是车载网络部署的传感器）提供的新型数据集来增强空气质量建模能力。

Method: 对10种统计模型进行综合评估，使用法国南特市的固定和移动低成本传感器数据及辅助变量，采用交叉验证方法（在低成本传感器数据上交叉验证，在固定空气质量监测站上验证），评估模型在时间插值和预测场景中的性能。

Result: 发现仅依赖低成本传感器数据的模型输出存在显著偏差；机器学习模型在预测场景中表现更优；低成本移动传感器数据的准确性不足会损害空气质量模型的可靠性。

Conclusion: 不应仅关注统计建模技术的复杂性，而应重点将低成本传感器数据与固定监测站信息进行整合和校准，这对于实现有效空气质量管理和政策制定所需的精度至关重要。

Abstract: This study addresses the critical challenge of modeling and mapping urban air quality to ascertain pollutant concentrations in unmonitored locations. The advent of low-cost sensors, particularly those deployed in vehicular networks, presents novel datasets that hold the potential to enhance air quality modeling. This research conducts a comprehensive review of ten statistical models drawn from existing literature, using both fixed and mobile low-cost sensor data, alongside ancillary variables, within the urban confines of Nantes, France.
  Employing a methodology that includes cross-validation of data from low-cost sensors and validation on fixed air quality monitoring stations, this paper evaluates the models' performance in scenarios of temporal interpolation and prediction. Our findings reveal a pronounced bias in the model outputs when reliant on low-cost sensor data compared to the verification data obtained from fixed stations. Furthermore, machine learning models demonstrated superior performance in predictive scenarios, suggesting their enhanced suitability for forecasting tasks.
  The study conclusively indicates that reliance solely on data from low-cost mobile sensors compromises the reliability of air quality models, due to significant accuracy deficiencies. Consequently, we advocate for a directed focus towards the integration and calibration of low-cost sensor data with information from fixed monitoring stations. This approach, rather than an exclusive emphasis on the complexity of statistical modeling techniques, is pivotal for achieving the precision required for effective air quality management and policy-making.

</details>


### [36] [The Causal Uncertainty Principle](https://arxiv.org/abs/2511.22649)
*Daniel D. Reidpath*

Main category: stat.AP

TL;DR: 内部效度和外部效度无法同时最大化，因为因果推断中的常规操作（限制、条件化、干预）具有不可交换性，提高因果精确性必然缩小研究结果的适用范围。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解释为什么内部效度和外部效度无法同时最大化，揭示因果推断中常规操作的非交换性如何导致这种权衡。

Method: 引入"证据状态"概念来表示因果推断的可用信息，分析限制、条件化和干预这三种常规操作如何以不可交换的方式转换证据状态，并通过小型模型、观察性和实验性案例进行说明。

Result: 研究发现因果推断操作具有不可交换性，改变操作顺序会产生支持不同因果主张的证据状态，导致确保精确因果识别的步骤会消除推广所需的异质性。

Conclusion: 因果推断存在结构性权衡：提高因果精确性必然缩小研究结果的适用范围，这解释了为什么内部效度和外部效度无法同时最大化。

Abstract: This paper explains why internal and external validity cannot be simultaneously maximised. It introduces "evidential states" to represent the information available for causal inference and shows that routine study operations (restriction, conditioning, and intervention) transform these states in ways that do not commute. Because each operation removes or reorganises information differently, changing their order yields evidential states that support different causal claims. This non-commutativity creates a structural trade-off: the steps that secure precise causal identification also eliminate the heterogeneity required for generalisation. Small model, observational and experimental examples illustrate how familiar failures of transportability arise from this order dependence. The result is a concise structural account of why increasing causal precision necessarily narrows the world to which findings apply.

</details>


### [37] [A novel method to analyze pattern shifts in rainfall using cluster analysis and probability models](https://arxiv.org/abs/2511.23067)
*Abhishek Singh,Aaditya Jadhav,Abha Goyal,Jesma V,Vyshna I C*

Main category: stat.AP

TL;DR: 使用概率模型对印度北方邦瓦拉纳西地区122年月降雨数据进行建模，通过探索性分析和聚类分析揭示气候变化模式


<details>
  <summary>Details</summary>
Motivation: 气候变化对农业造成严重影响，虽然宏观层面的气候变化建模已有大量研究，但州级或地区级的微观层面研究较少。地区级研究有助于制定针对性的本地气候变化缓解计划

Method: 首先使用探索性分析和聚类分析等多元技术揭示122年来的气候变化模式，然后为选定月份拟合概率模型

Result: 论文揭示了瓦拉纳西地区122年来的气候变化模式，并成功为选定月份建立了概率模型

Conclusion: 地区级的气候变化建模对于制定本地化的气候变化缓解策略具有重要意义，概率模型为地区级降雨预测提供了有效工具

Abstract: : One of the prominent challenges being faced by agricultural sciences is the onset of climate change which is adversely affecting every aspect of cropping. Modelling of climate change at macro level have been carried out at large scale and there is ample amount of research publications available for that. But at micro level like at state level or district level there are lesser studies. District level studies can help in preparing specific plans for the mitigation of adverse effects of climate change at local level. An attempt has been made in this paper to model the monthly rainfall of Varanasi district of the state of Uttar Pradesh with the help of probability models. Firstly, the pattern of the climate change over 122 years has been unveiled by using exploratory analysis and using multivariate techniques like cluster analysis and then probability models have been fitted for selected months

</details>


### [38] [Multi-fidelity Bayesian Optimization Framework for CFD-Based Non-Premixed Burner Design](https://arxiv.org/abs/2511.23140)
*Patrick Souza Lima,Paulo Roberto Santana dos Reis,Alex Álisson Bandeira Santos,Ehecatl Antonio del Río Chanona,Idelfonso Bessa dos Reis Nogueira*

Main category: stat.AP

TL;DR: 提出多保真度贝叶斯优化框架，结合CFD模拟与高斯过程代理模型，通过自适应调整网格分辨率平衡精度与计算成本，应用于氢富燃料非预混燃烧器热效率优化。


<details>
  <summary>Details</summary>
Motivation: 传统CFD优化计算成本高昂，网格分辨率直接影响模拟精度与计算时间。需要开发能自适应平衡精度与成本的方法，加速工业燃烧器等反应流系统的设计优化。

Method: 采用多保真度贝叶斯优化框架，设计向量包含高度、长度和网格尺寸，定义连续保真度指数。结合高斯过程代理模型、校准的运行时间模型和带约束的噪声期望改进采样策略，在NOx排放限制下进行优化。

Result: 代理模型显示稳定超参数和物理一致的敏感性：平均温度随反应器长度和保真度增加而升高，与高度呈弱负相关；NOx随温度增加但随长度减少。最佳设计达到约2000K平均温度同时满足NOx限制。相比单保真度方法，总计算时间减少约57%。

Conclusion: 该方法为反应流系统优化提供通用且计算高效的技术路径，能显著加速设计周期并降低资源需求，在工业燃烧器开发等高成本CFD应用中具有重要潜力。

Abstract: We propose a multi-fidelity Bayesian optimization (MF-BO) framework that integrates computational fluid dynamics (CFD) evaluations with Gaussian-process surrogates to efficiently navigate the accuracy-cost trade-off induced by mesh resolution. The design vector x = [h, l, s] (height, length, and mesh element size) defines a continuous fidelity index Z(h, l, s), enabling the optimizer to adaptively combine low- and high-resolution simulations. This framework is applied to a non-premixed burner configuration targeting improved thermal efficiency under hydrogen-enriched fuels. A calibrated runtime model t_hat(h, l, s) penalizes computationally expensive queries, while a constrained noisy expected improvement (qNEI) guides sampling under an emissions cap of 2e-6 for NOx.
  Surrogates trained on CFD data exhibit stable hyperparameters and physically consistent sensitivities: mean temperature increases with reactor length and fidelity and is weakly negative with height; NOx grows with temperature yet tends to decrease with length. The best design achieves T_bar approx 2.0e3 K while satisfying the NOx limit.
  Relative to a hypothetical single-fidelity campaign (Z = 1), the MF-BO achieves comparable convergence with about 57 percent lower total wall time by learning the design landscape through fast low-Z evaluations and reserving high-Z CFD for promising candidates. Overall, the methodology offers a generalizable and computationally affordable path for optimizing reacting-flow systems in which mesh-driven fidelity inherently couples accuracy, cost, and emissions. This highlights its potential to accelerate design cycles and reduce resource requirements in industrial burner development and other high-cost CFD-driven applications.

</details>


### [39] [Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures](https://arxiv.org/abs/2511.23156)
*Sanne M. van Essen,Harleigh C. Seyffert*

Main category: stat.AP

TL;DR: 提出一种新的多保真度筛选方法PAS，用于高效预测非线性波浪载荷的极值，相比传统蒙特卡洛模拟可减少96%以上的高保真模拟时间。


<details>
  <summary>Details</summary>
Motivation: 海洋和海岸结构设计需要极端载荷值，但对于非线性且罕见的事件，传统高保真物理模拟计算成本极高，难以高效获取可靠极值估计。

Method: 提出概率自适应筛选(PAS)方法，采用多保真度策略：低保真阶段使用高效线性势流指标，高保真阶段仅对关键样本进行模拟，通过概率方法实现非线性载荷情况下的有效筛选。

Result: PAS在所有测试案例中都能准确估计短期分布和极值，最可能最大值(MPM)与完整蒙特卡洛模拟结果偏差在10%以内，同时仅需传统方法4%的高保真模拟时间。

Conclusion: PAS方法能可靠再现弱非线性和强非线性极端载荷问题的统计特性，显著降低计算成本，为海洋结构设计提供了高效可靠的极值预测工具。

Abstract: To ensure the safety of marine and coastal structures, extreme (design) values should be known at the design stage. But for such complex systems, estimating the magnitude of events which are both non-linear and rare is extremely challenging, and involves considerable computational cost to capture the high-fidelity physics. To address this challenge, we offer a new multi-fidelity screening method, Probabilistic Adaptive Screening (PAS), which accurately predicts extreme values of strongly non-linear wave-induced loads while minimising the required high-fidelity simulation duration. The method introduces a probabilistic approach to multi-fidelity screening, allowing efficient linear potential flow indicators to be used in the low-fidelity stage, even for strongly non-linear load cases. The method is validated against a range of cases, including non-linear waves, and ship vertical bending moments, green water impact loads, and slamming loads. It can be concluded that PAS accurately estimates both the short-term distributions and extreme values in all test cases, with most probable maximum (MPM) values within 10\% of the available full brute-force Monte-Carlo Simulation (MCS) results. In addition, PAS achieves this performance very efficiently, requiring less than 4\% of the high-fidelity simulation time needed for conventional MCS. These results demonstrate that PAS can reliably reproduce the statistics of both weakly and strongly non-linear extreme load problems, while significantly reducing the associated computational cost. The present study validates the statistical PAS framework; further work should focus on validating the full procedure including CFD load simulations, and on validating it for long-term extremes.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [40] [Convergence of a Sequential Monte Carlo algorithm towards multimodal distributions on Rd](https://arxiv.org/abs/2511.22564)
*Ruiyu Han*

Main category: stat.CO

TL;DR: 提出一种用于在非紧致空间Rd上采样非凸能量函数Gibbs测度的序贯蒙特卡洛算法，时间复杂度约为逆温度的七次方、绝对误差倒数的平方和概率误差倒数的平方


<details>
  <summary>Details</summary>
Motivation: 先前工作证明了该算法在紧致空间（环面）上能以多项式时间复杂度采样Gibbs测度，但该方法在非紧致空间（Rd）上失效。本研究旨在克服这一障碍，将算法扩展到非紧致空间

Method: 使用序贯蒙特卡洛算法来采样具有非凸能量函数的Gibbs测度，特别是在低温和非紧致空间Rd上的情况

Result: 成功将算法扩展到非紧致空间Rd，证明了蒙特卡洛估计器的收敛性，时间复杂度约为β^7·ε^{-2}·δ^{-2}，其中β是逆温度，ε是绝对误差，δ是概率误差

Conclusion: 该研究克服了先前算法在非紧致空间上的限制，为在Rd上采样非凸Gibbs测度提供了理论保证，时间复杂度的多项式特性使其具有实际应用价值

Abstract: We study a sequential Monte Carlo algorithm to sample from the Gibbs measure supported on Rd with a non-convex energy function at a low temperature. In an earlier joint work, we proved that the algorithm samples from Gibbs measures supported on torus with time complexity that is polynomial in the inverse temperature; however, the approach breaks down in the non-compact setting. This work overcomes this obstacle and establishes a similar result for sampling from Gibbs measures supported on Rd. Our main result shows convergence of Monte Carlo estimators with time complexity that, approximately, scales like the seventh power of the inverse temperature, the square of the inverse allowed absolute error and the square of the inverse allowed probability error.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [41] [Sparse Multiple Kernel Learning: Alternating Best Response and Semidefinite Relaxations](https://arxiv.org/abs/2511.21890)
*Dimitris Bertsimas,Caio de Prospero Iglesias,Nicholas A. G. Johnson*

Main category: stat.ML

TL;DR: 提出一种稀疏多核学习方法，通过显式基数约束和l2正则化，使用交替最优响应算法求解，并通过混合整数半定规划验证最优性。


<details>
  <summary>Details</summary>
Motivation: 现有l1正则化方法只能近似稀疏性，需要更直接控制核选择数量的方法，同时保证鲁棒性。

Method: 1) 使用显式基数约束和l2正则化构建非凸极小极大问题；2) 采用交替最优响应算法，alpha子问题用LIBSVM求解，beta子问题用贪心选择器和单纯形投影器算法；3) 将问题重构为混合整数半定规划，推导半定凸松弛层次用于验证最优性和热启动。

Result: 在10个UCI基准测试中，随机初始化方法平均比最优基准方法提升3.34个百分点的样本外预测准确率，同时选择少量核；热启动后平均提升4.05个百分点。凸松弛提供的最优性证书表明算法在多个案例中返回全局最优解。

Conclusion: 提出的稀疏多核学习方法在预测准确率和核选择稀疏性方面优于现有方法，同时通过凸松弛层次提供了最优性验证机制。

Abstract: We study Sparse Multiple Kernel Learning (SMKL), which is the problem of selecting a sparse convex combination of prespecified kernels for support vector binary classification. Unlike prevailing l1 regularized approaches that approximate a sparsifying penalty, we formulate the problem by imposing an explicit cardinality constraint on the kernel weights and add an l2 penalty for robustness. We solve the resulting non-convex minimax problem via an alternating best response algorithm with two subproblems: the alpha subproblem is a standard kernel SVM dual solved via LIBSVM, while the beta subproblem admits an efficient solution via the Greedy Selector and Simplex Projector algorithm. We reformulate SMKL as a mixed integer semidefinite optimization problem and derive a hierarchy of semidefinite convex relaxations which can be used to certify near-optimality of the solutions returned by our best response algorithm and also to warm start it. On ten UCI benchmarks, our method with random initialization outperforms state-of-the-art MKL approaches in out-of-sample prediction accuracy on average by 3.34 percentage points (relative to the best performing benchmark) while selecting a small number of candidate kernels in comparable runtime. With warm starting, our method outperforms the best performing benchmark's out-of-sample prediction accuracy on average by 4.05 percentage points. Our convex relaxations provide a certificate that in several cases, the solution returned by our best response algorithm is the globally optimal solution.

</details>


### [42] [Algorithms and Scientific Software for Quasi-Monte Carlo, Fast Gaussian Process Regression, and Scientific Machine Learning](https://arxiv.org/abs/2511.21915)
*Aleksei G. Sorokin*

Main category: stat.ML

TL;DR: 该论文提出在三个科学计算领域（准蒙特卡洛方法、高斯过程回归、科学机器学习）的统一算法与软件框架，开发了QMCPy、FastGPs等开源工具，并在多个应用场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 科学计算领域需要高效算法和易用软件，但现有工具在准蒙特卡洛高维积分、高斯过程高维插值、科学机器学习求解偏微分方程等方面存在不足，需要开发统一框架和开源实现。

Method: 1) 准蒙特卡洛：开发向量化误差估计算法，构建QMCPy开源Python接口，集成随机低差异序列生成器、自动变量变换、自适应误差估计；2) 高斯过程：推导数字平移不变核函数，开发快速多任务GP算法，实现FastGPs可扩展Python软件；3) 科学机器学习：开发能够以机器精度恢复随机系数PDE的新算法。

Result: 成功开发了QMCPy和FastGPs等开源软件框架，在多个应用场景中验证了方法的有效性，包括：GP用于失效概率估计、多层GP用于达西流方程、神经代理模型用于辐射传输建模、快速GP用于贝叶斯多层QMC。

Conclusion: 该论文在准蒙特卡洛、高斯过程和科学机器学习三个领域提供了统一的算法和软件解决方案，开发的开源工具能够有效支持高维积分、不确定性量化、偏微分方程求解等科学计算任务，具有广泛的应用前景。

Abstract: Most scientific domains elicit the development of efficient algorithms and accessible scientific software. This thesis unifies our developments in three broad domains: Quasi-Monte Carlo (QMC) methods for efficient high-dimensional integration, Gaussian process (GP) regression for high-dimensional interpolation with built-in uncertainty quantification, and scientific machine learning (sciML) for modeling partial differential equations (PDEs) with mesh-free solvers. For QMC, we built new algorithms for vectorized error estimation and developed QMCPy (https://qmcsoftware.github.io/QMCSoftware/): an open-source Python interface to randomized low-discrepancy sequence generators, automatic variable transforms, adaptive error estimation procedures, and diverse use cases. For GPs, we derived new digitally-shift-invariant kernels of higher-order smoothness, developed novel fast multitask GP algorithms, and produced the scalable Python software FastGPs (https://alegresor.github.io/fastgps/). For sciML, we developed a new algorithm capable of machine precision recovery of PDEs with random coefficients. We have also studied a number of applications including GPs for probability of failure estimation, multilevel GPs for the Darcy flow equation, neural surrogates for modeling radiative transfer, and fast GPs for Bayesian multilevel QMC.

</details>


### [43] [A Sensitivity Approach to Causal Inference Under Limited Overlap](https://arxiv.org/abs/2511.22003)
*Yuanzhe Ma,Hongseok Namkoong*

Main category: stat.ML

TL;DR: 提出一个敏感性分析框架，用于评估在治疗组与对照组重叠有限的情况下，研究结果需要多强的异常性才会被推翻，从而量化有限重叠区域的不确定性。


<details>
  <summary>Details</summary>
Motivation: 观察性研究中治疗组与对照组重叠有限是一个关键挑战。标准的修剪重要性权重方法虽然能降低方差，但会引入根本性偏差。需要一种方法来评估在有限重叠情况下研究结果的稳健性。

Method: 提出一个敏感性分析框架，基于最坏情况置信界限来评估标准修剪实践引入的偏差。该方法在明确假设下，从重叠区域外推反事实估计到无重叠区域，评估结果函数需要多异常才能使主要发现无效。

Result: 该敏感性框架能够通过量化有限重叠区域的不确定性来防止虚假发现，为观察性分析中有限重叠问题提供了实用的评估工具。

Conclusion: 提出的敏感性分析框架为观察性研究中有限重叠问题提供了有效的解决方案，能够评估研究结果的稳健性并防止虚假发现，特别是在治疗组与对照组重叠有限的情况下。

Abstract: Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap.

</details>


### [44] [Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests](https://arxiv.org/abs/2511.23212)
*Tomoshige Nakamura,Hiroshi Shiraishi*

Main category: stat.ML

TL;DR: QRF的变量重要性推断面临挑战，本文建立了基于pinball损失风险的变量重要性渐近理论，揭示了子采样率β控制的"相变"现象，发现实践中偏好的大子样本会导致偏差主导，使标准推断失效。


<details>
  <summary>Details</summary>
Motivation: 量化回归森林广泛用于非参数条件分位数估计，但由于损失函数的非光滑性和复杂的偏差-方差权衡，变量重要性度量的统计推断仍然具有挑战性。

Method: 首先通过Knight恒等式处理不可微的pinball损失，建立QRF估计量的渐近正态性；其次分析子采样率β控制的"相变"现象；最后推导渐近偏差的显式解析形式，讨论通过解析偏差校正恢复有效推断的理论可行性。

Result: 证明了在偏差主导机制（β≥1/2，对应实践中偏好的大子样本）下，标准推断失效，估计量收敛到确定性偏差常数而非零均值正态分布；推导了渐近偏差的显式解析形式。

Conclusion: 研究结果突显了预测性能与推断有效性之间的基本权衡，为理解高维设置中随机森林推断的内在局限性提供了理论基础。

Abstract: Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a "phase transition" phenomenon governed by the subsampling rate $β$ (where $s \asymp n^β$). We prove that in the bias-dominated regime ($β\ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings.

</details>


### [45] [On the Effect of Regularization on Nonparametric Mean-Variance Regression](https://arxiv.org/abs/2511.22004)
*Eliot Wong-Toi,Alex Boyd,Vincent Fortuin,Stephan Mandt*

Main category: stat.ML

TL;DR: 该论文研究了过参数化均值-方差回归模型中的信号-噪声模糊性问题，发现正则化水平会引发尖锐的相变现象，提出了统计场论框架来解释这一行为，并显著降低了超参数搜索的计算成本。


<details>
  <summary>Details</summary>
Motivation: 均值-方差回归模型为机器学习中的不确定性量化提供了简单方法，但过参数化模型面临信号-噪声模糊性问题：模型难以决定预测目标应归因于信号（均值）还是噪声（方差），这影响了不确定性量化的可靠性。

Method: 通过经验研究观察不同正则化水平下的相变现象，开发统计场论框架来解释模型行为，将正则化超参数搜索空间从二维降至一维，并在UCI数据集和大规模ClimSim数据集上进行实验验证。

Result: 研究发现正则化水平会引发尖锐的相变现象，模型在完美拟合训练数据（零残差噪声）和提供恒定无信息预测之间转变。统计场论框架成功捕捉了观察到的相变行为，超参数搜索空间减少显著降低了计算成本，实验显示出稳健的校准性能。

Conclusion: 该研究揭示了过参数化均值-方差回归模型中的信号-噪声模糊性问题及其相变行为，提出的统计场论框架不仅解释了这一现象，还通过减少超参数搜索维度显著提升了不确定性量化的计算效率，为决策制定和风险评估提供了更可靠的工具。

Abstract: Uncertainty quantification is vital for decision-making and risk assessment in machine learning. Mean-variance regression models, which predict both a mean and residual noise for each data point, provide a simple approach to uncertainty quantification. However, overparameterized mean-variance models struggle with signal-to-noise ambiguity, deciding whether prediction targets should be attributed to signal (mean) or noise (variance). At one extreme, models fit all training targets perfectly with zero residual noise, while at the other, they provide constant, uninformative predictions and explain the targets as noise. We observe a sharp phase transition between these extremes, driven by model regularization. Empirical studies with varying regularization levels illustrate this transition, revealing substantial variability across repeated runs. To explain this behavior, we develop a statistical field theory framework, which captures the observed phase transition in alignment with experimental results. This analysis reduces the regularization hyperparameter search space from two dimensions to one, significantly lowering computational costs. Experiments on UCI datasets and the large-scale ClimSim dataset demonstrate robust calibration performance, effectively quantifying predictive uncertainty.

</details>


### [46] [Support Vector Machine Classifier with Rescaled Huberized Pinball Loss](https://arxiv.org/abs/2511.22065)
*Shibo Diao*

Main category: stat.ML

TL;DR: 本文提出了一种新的重标度Huberized pinball损失函数，并基于此构建了RHPSVM模型，该模型在分类精度、异常值不敏感性和重采样稳定性方面优于传统SVM。


<details>
  <summary>Details</summary>
Motivation: 传统SVM模型存在对异常值敏感和重采样不稳定的问题，这限制了其在实际应用中的性能。需要开发一种更鲁棒的SVM变体来解决这些问题。

Method: 提出具有非对称、非凸、平滑特性的重标度Huberized pinball损失函数，并构建RHPSVM模型。使用CCCP将非凸优化问题转化为一系列凸子问题，采用ClipDCD算法求解。

Result: 理论分析表明RHPSVM符合贝叶斯规则，具有严格的泛化误差界、有界影响函数和可控最优性条件。实验证明在模拟数据、UCI数据集和小样本作物叶片图像分类任务中，RHPSVM在噪声和无噪声场景下均优于现有SVM模型。

Conclusion: RHPSVM通过创新的损失函数设计，有效解决了传统SVM对异常值敏感和重采样不稳定的问题，特别适合处理高维小样本数据，具有优异的分类性能和鲁棒性。

Abstract: Support vector machines are widely used in machine learning classification tasks, but traditional SVM models suffer from sensitivity to outliers and instability in resampling, which limits their performance in practical applications. To address these issues, this paper proposes a novel rescaled Huberized pinball loss function with asymmetric, non-convex, and smooth properties. Based on this loss function, we develop a corresponding SVM model called RHPSVM (Rescaled Huberized Pinball Loss Support Vector Machine). Theoretical analyses demonstrate that RHPSVM conforms to Bayesian rules, has a strict generalization error bound, a bounded influence function, and controllable optimality conditions, ensuring excellent classification accuracy, outlier insensitivity, and resampling stability. Additionally, RHPSVM can be extended to various advanced SVM variants by adjusting parameters, enhancing its flexibility. We transform the non-convex optimization problem of RHPSVM into a series of convex subproblems using the concave-convex procedure (CCCP) and solve it with the ClipDCD algorithm, which is proven to be convergent. Experimental results on simulated data, UCI datasets, and small-sample crop leaf image classification tasks show that RHPSVM outperforms existing SVM models in both noisy and noise-free scenarios, especially in handling high-dimensional small-sample data.

</details>


### [47] [Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs](https://arxiv.org/abs/2511.22270)
*Zhongjie Shi,Puyu Wang,Chenyang Zhang,Yuan Cao*

Main category: stat.ML

TL;DR: DP-GD（差分隐私梯度下降）在特定学习任务中可能比普通GD具有更好的泛化性能，同时提供隐私保护


<details>
  <summary>Details</summary>
Motivation: 深度学习训练数据可能包含敏感信息，需要开发既能保持良好性能又能保护隐私的训练算法

Method: 研究差分隐私梯度下降（DP-GD）算法的泛化和隐私性能，该算法通过在每次迭代中向梯度添加额外噪声来实现隐私保护

Result: 在特定条件下，当信噪比较小时，GD可能产生测试精度差的模型，而DP-GD可以产生具有良好测试精度和隐私保证的模型

Conclusion: DP-GD在某些学习任务中具有在确保隐私保护的同时提升模型性能的潜力

Abstract: Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration. Moreover, we identify a concrete learning task where DP-GD can achieve superior generalization performance compared to GD in training two-layer Huberized ReLU convolutional neural networks (CNNs). Specifically, we demonstrate that, under mild conditions, a small signal-to-noise ratio can result in GD producing training models with poor test accuracy, whereas DP-GD can yield training models with good test accuracy and privacy guarantees if the signal-to-noise ratio is not too small. This indicates that DP-GD has the potential to enhance model performance while ensuring privacy protection in certain learning tasks. Numerical simulations are further conducted to support our theoretical results.

</details>


### [48] [UCB for Large-Scale Pure Exploration: Beyond Sub-Gaussianity](https://arxiv.org/abs/2511.22273)
*Zaile Li,Weiwei Fan,L. Jeff Hong*

Main category: stat.ML

TL;DR: 本文研究UCB算法在大规模非亚高斯纯探索问题中的性能，证明了在方差有界或高阶矩有界的非亚高斯分布下，meta-UCB算法能达到样本最优性。


<details>
  <summary>Details</summary>
Motivation: 传统纯探索方法主要依赖高斯或亚高斯假设，限制了在非亚高斯（特别是重尾分布）问题中的应用。大规模问题对分布假设特别敏感，需要超越亚高斯性的方法。UCB算法在纯探索中广泛应用，但其在非亚高斯大规模设置下的性能尚未充分研究。

Method: 提出meta-UCB算法框架，将UCB值定义为样本均值加上仅依赖自身样本量的探索奖励。算法停止时选择样本量最大的备选方案作为最优。分析两种非亚高斯场景：1）所有备选方案具有共同的位置-尺度结构且有界方差；2）当这种结构不成立时，每个备选方案具有q>3阶有界绝对矩。

Result: 推导了meta-UCB算法正确选择的分布无关下界。在两种非亚高斯设置下，证明了meta-UCB算法（以及一大类UCB算法）能够达到样本最优性。数值实验支持理论结果，并提供了UCB算法在meta-UCB框架内外的比较行为洞察。

Conclusion: UCB算法适用于解决具有非亚高斯分布的大规模纯探索问题。即使在非亚高斯设置下，通过适当的算法设计和分析，UCB算法仍能保持样本最优性，扩展了传统亚高斯假设下的适用范围。

Abstract: Selecting the best alternative from a finite set represents a broad class of pure exploration problems. Traditional approaches to pure exploration have predominantly relied on Gaussian or sub-Gaussian assumptions on the performance distributions of all alternatives, which limit their applicability to non-sub-Gaussian especially heavy-tailed problems. The need to move beyond sub-Gaussianity may become even more critical in large-scale problems, which tend to be especially sensitive to distributional specifications. In this paper, motivated by the widespread use of upper confidence bound (UCB) algorithms in pure exploration and beyond, we investigate their performance in the large-scale, non-sub-Gaussian settings. We consider the simplest category of UCB algorithms, where the UCB value for each alternative is defined as the sample mean plus an exploration bonus that depends only on its own sample size. We abstract this into a meta-UCB algorithm and propose letting it select the alternative with the largest sample size as the best upon stopping. For this meta-UCB algorithm, we first derive a distribution-free lower bound on the probability of correct selection. Building on this bound, we analyze two general non-sub-Gaussian scenarios: (1) all alternatives follow a common location-scale structure and have bounded variance; and (2) when such a structure does not hold, each alternative has a bounded absolute moment of order $q > 3$. In both settings, we show that the meta-UCB algorithm and therefore a broad class of UCB algorithms can achieve the sample optimality. These results demonstrate the applicability of UCB algorithms for solving large-scale pure exploration problems with non-sub-Gaussian distributions. Numerical experiments support our results and provide additional insights into the comparative behaviors of UCB algorithms within and beyond our meta-UCB framework.

</details>


### [49] [Data-driven informative priors for Bayesian inference with quasi-periodic data](https://arxiv.org/abs/2511.22296)
*Javier Lopez-Santiago,Luca Martino,Joaquin Miguez,Gonzalo Vazquez-Vilar*

Main category: stat.ML

TL;DR: 提出一种基于高斯过程周期核的贝叶斯计算策略，通过自适应重要性采样近似周期超参数的后验分布，并将其作为先验分布用于参数模型的周期推断。


<details>
  <summary>Details</summary>
Motivation: 在具有周期性的模型中，周期参数的后验分布通常高度集中在参数空间的很小区域，导致贝叶斯计算效率低下。需要为推断方法提供更多信息，特别是通过先验分布。

Method: 使用具有周期核的高斯过程拟合数据构建先验分布。采用自适应重要性采样近似高斯过程超参数的后验分布，然后将周期相关超参数的边际后验分布作为参数模型周期的先验分布。这是一种经验贝叶斯方法，实现为高斯过程后验到参数模型的模块化（切割）传递。

Result: 该方法在合成数据和真实数据上都得到了应用。成功近似了高斯过程核周期的后验分布，并将其作为后验即先验进行前向传递（无反馈），分析了其对边际后验分布的影响。

Conclusion: 提出了一种有效的经验贝叶斯工作流程，通过高斯过程周期核构建数据驱动的先验分布，改善了周期性模型中参数推断的计算效率，特别是在周期参数后验高度集中的情况下。

Abstract: Bayesian computational strategies for inference can be inefficient in approximating the posterior distribution in models that exhibit some form of periodicity. This is because the probability mass of the marginal posterior distribution of the parameter representing the period is usually highly concentrated in a very small region of the parameter space. Therefore, it is necessary to provide as much information as possible to the inference method through the parameter prior distribution. We intend to show that it is possible to construct a prior distribution from the data by fitting a Gaussian process (GP) with a periodic kernel. More specifically, we want to show that it is possible to approximate the marginal posterior distribution of the hyperparameter corresponding to the period in the kernel. Subsequently, this distribution can be used as a prior distribution for the inference method. We use an adaptive importance sampling method to approximate the posterior distribution of the hyperparameters of the GP. Then, we use the marginal posterior distribution of the hyperparameter related to the periodicity in order to construct a prior distribution for the period of the parametric model. This workflow is empirical Bayes, implemented as a modular (cut) transfer of a GP posterior for the period to the parametric model. We applied the proposed methodology to both synthetic and real data. We approximated the posterior distribution of the period of the GP kernel and then passed it forward as a posterior-as-prior with no feedback. Finally, we analyzed its impact on the marginal posterior distribution.

</details>


### [50] [A PLS-Integrated LASSO Method with Application in Index Tracking](https://arxiv.org/abs/2511.23205)
*Shiqin Tang,Yining Dong,S. Joe Qin*

Main category: stat.ML

TL;DR: 提出PLS-Lasso方法，将降维直接整合到回归过程中，包含两个版本PLS-Lasso-v1和PLS-Lasso-v2，在金融指数跟踪任务中表现优于Lasso。


<details>
  <summary>Details</summary>
Motivation: 传统多元数据分析中，降维和回归被视为独立任务。虽然PCR和PLS等方法在回归前计算潜在成分，但它们使用不同标准且降维与回归分离。需要一种将降维直接整合到回归过程中的创新方法。

Method: 提出PLS-Lasso方法，包含两个版本：PLS-Lasso-v1和PLS-Lasso-v2。该方法将偏最小二乘（PLS）的降维概念与Lasso回归直接整合，提供了清晰有效的算法确保收敛到全局最优解。

Result: 在金融指数跟踪任务中，PLS-Lasso-v1和PLS-Lasso-v2与Lasso进行比较，显示出有希望的结果，表明整合降维的回归方法具有优势。

Conclusion: PLS-Lasso是一种创新的回归方法，成功将降维直接整合到回归过程中，为多元数据分析提供了新的有效工具，在金融指数跟踪等应用中展现出潜力。

Abstract: In traditional multivariate data analysis, dimension reduction and regression have been treated as distinct endeavors. Established techniques such as principal component regression (PCR) and partial least squares (PLS) regression traditionally compute latent components as intermediary steps -- although with different underlying criteria -- before proceeding with the regression analysis. In this paper, we introduce an innovative regression methodology named PLS-integrated Lasso (PLS-Lasso) that integrates the concept of dimension reduction directly into the regression process. We present two distinct formulations for PLS-Lasso, denoted as PLS-Lasso-v1 and PLS-Lasso-v2, along with clear and effective algorithms that ensure convergence to global optima. PLS-Lasso-v1 and PLS-Lasso-v2 are compared with Lasso on the task of financial index tracking and show promising results.

</details>


### [51] [OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning](https://arxiv.org/abs/2511.23310)
*Zixun Huang,Jiayi Sheng,Zeyu Zheng*

Main category: stat.ML

TL;DR: 本文提出了一个统一的理论框架来分析RL后训练中策略梯度估计器的统计特性，并基于此开发了OBLR-PO算法，通过自适应学习率和基线优化提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的LLM后训练方法大多基于启发式设计，缺乏系统的理论指导，这限制了我们对梯度估计器特性和优化算法的理解，从而制约了训练稳定性和整体性能的提升。

Method: 建立统一的理论框架分析策略梯度估计器的统计特性（无偏性、方差表达式、优化损失上界），证明收敛保证，推导由梯度信噪比控制的自适应学习率调度，提出方差最优基线作为梯度加权估计器，最终开发OBLR-PO算法联合自适应学习率和基线。

Result: 在Qwen3-4B-Base和Qwen3-8B-Base模型上的实验表明，OBLR-PO算法相比现有策略优化方法取得了持续的性能提升，验证了理论贡献在实际大规模后训练中的有效性。

Conclusion: 本文的理论框架为RL后训练提供了系统化的理论指导，OBLR-PO算法通过理论驱动的自适应学习率和基线优化，显著提升了训练稳定性和性能，为大规模语言模型后训练提供了新的优化方向。

Abstract: Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.

</details>
