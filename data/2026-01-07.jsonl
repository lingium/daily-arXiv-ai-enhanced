{"id": "2601.00904", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00904", "abs": "https://arxiv.org/abs/2601.00904", "authors": ["Qiang Li", "Shujian Yu", "Liang Ma", "Chen Ma", "Jingyu Liu", "Tulay Adali", "Vince D. Calhoun"], "title": "Deep Deterministic Nonlinear ICA via Total Correlation Minimization with Matrix-Based Entropy Functional", "comment": "16 pages, 9 figures", "summary": "Blind source separation, particularly through independent component analysis (ICA), is widely utilized across various signal processing domains for disentangling underlying components from observed mixed signals, owing to its fully data-driven nature that minimizes reliance on prior assumptions. However, conventional ICA methods rely on an assumption of linear mixing, limiting their ability to capture complex nonlinear relationships and to maintain robustness in noisy environments. In this work, we present deep deterministic nonlinear independent component analysis (DDICA), a novel deep neural network-based framework designed to address these limitations. DDICA leverages a matrix-based entropy function to directly optimize the independence criterion via stochastic gradient descent, bypassing the need for variational approximations or adversarial schemes. This results in a streamlined training process and improved resilience to noise. We validated the effectiveness and generalizability of DDICA across a range of applications, including simulated signal mixtures, hyperspectral image unmixing, modeling of primary visual receptive fields, and resting-state functional magnetic resonance imaging (fMRI) data analysis. Experimental results demonstrate that DDICA effectively separates independent components with high accuracy across a range of applications. These findings suggest that DDICA offers a robust and versatile solution for blind source separation in diverse signal processing tasks."}
{"id": "2601.01116", "categories": ["stat.ME", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.01116", "abs": "https://arxiv.org/abs/2601.01116", "authors": ["Richik Chakraborty"], "title": "Beyond P-Values: Importing Quantitative Finance's Risk and Regret Metrics for AI in Learning Health Systems", "comment": "Manuscript: 12 pages, 3 mathematical boxes, 1 figure, 1 table, 26 references", "summary": "The increasing deployment of artificial intelligence (AI) in clinical settings challenges foundational assumptions underlying traditional frameworks of medical evidence. Classical statistical approaches, centered on randomized controlled trials, frequentist hypothesis testing, and static confidence intervals, were designed for fixed interventions evaluated under stable conditions. In contrast, AI-driven clinical systems learn continuously, adapt their behavior over time, and operate in non-stationary environments shaped by evolving populations, practices, and feedback effects. In such systems, clinical harm arises less from average error rates than from calibration drift, rare but severe failures, and the accumulation of suboptimal decisions over time.\n  In this perspective, we argue that prevailing notions of statistical significance are insufficient for characterizing evidence and safety in learning health systems. Drawing on risk-theoretic concepts from quantitative finance and online decision theory, we propose reframing medical evidence for adaptive AI systems in terms of time-indexed calibration stability, bounded downside risk, and controlled cumulative regret. We emphasize that this approach does not replace randomized trials or causal inference, but complements them by addressing dimensions of risk and uncertainty that emerge only after deployment. This framework provides a principled mathematical language for evaluating AI-driven clinical systems under continual learning and offers implications for clinical practice, research design, and regulatory oversight."}
{"id": "2601.01163", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01163", "abs": "https://arxiv.org/abs/2601.01163", "authors": ["Satoshi Usami"], "title": "Matrix Decomposition-Based Approach to Estimate the STARTS Model", "comment": null, "summary": "We propose a new estimation method for the Stable Trait, Auto Regressive Trait, and State (STARTS) model, which is well known for its frequent occurrence of improper solutions. The proposed approach is implemented through a two-stage estimation procedure that combines matrix decomposition factor analysis (MDFA) based on eigenvalue decomposition with conventional SEM estimation principles. By reformulating the STARTS model within a factor-analytic framework, this study presents a novel way of applying MDFA in the context of structural equation modeling (SEM). Through a simulation study and an empirical application to ToKyo Teen Cohort data, the proposed method was shown to entail a substantially lower risk of improper solutions than commonly used maximum likelihood, conditional ML, and (unweighted) least squares estimators, while tending to yield solutions similar to those obtained by ML. Compared with Bayesian estimation, the proposed method does not require the specification of appropriate (weakly informative) prior distributions and may effectively mitigate bias issues that arise when the number of time points is small. Applying the proposed method, as well as conducting sensitivity analyses informed by it, will enable researchers to more effectively delineate the range of plausible conclusions from data in estimating the STARTS model and other SEMs."}
{"id": "2601.01190", "categories": ["stat.ME", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.01190", "abs": "https://arxiv.org/abs/2601.01190", "authors": ["Diogenes de Jesus Ramirez", "Anderson Melchor Hernandez", "Isabel Cristina Ramirez", "Luis Raúl Pericchi"], "title": "A Modified Bayesian Criterion for Model Selection in Mixed and Hierarchical Frameworks", "comment": null, "summary": "In this work, we propose a modified Bayesian Information Criterion (BIC) specifically designed for mixture models and hierarchical structures. This criterion incorporates the determinant of the Hessian matrix of the log-likelihood function, thereby refining the classical Bayes Factor by accounting for the curvature of the likelihood surface. Such geometric information introduces a more nuanced penalization for model complexity. The proposed approach improves model selection, particularly under small-sample conditions or in the presence of noise variables. Through theoretical derivations and extensive simulation studies-including both linear and linear mixed models-we show that our criterion consistently outperforms traditional methods such as BIC, Akaike Information Criterion (AIC), and related variants. The results suggest that integrating curvature-based information from the likelihood landscape leads to more robust and accurate model discrimination in complex data environments."}
{"id": "2601.01029", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.01029", "abs": "https://arxiv.org/abs/2601.01029", "authors": ["Zeyu Bian", "Max Biggs", "Ruijiang Gao", "Zhengling Qi"], "title": "Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights", "comment": "74 pages", "summary": "This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies."}
{"id": "2601.01422", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01422", "abs": "https://arxiv.org/abs/2601.01422", "authors": ["Arghya Mukherjee", "Dootika Vats"], "title": "Hamiltonian Monte Carlo for (Physics) Dummies", "comment": "39 pages, 12 figures, 1 table", "summary": "Sampling-based inference has seen a surge of interest in recent years. Hamiltonian Monte Carlo (HMC) has emerged as a powerful algorithm that leverages concepts from Hamiltonian dynamics to efficiently explore complex target distributions. Variants of HMC are available in popular software packages, enabling off-the-shelf implementations that have greatly benefited the statistics and machine learning communities. At the same time, the availability of such black-box implementations has made it challenging for users to understand the inner workings of HMC, especially when they are unfamiliar with the underlying physical principles. We provide a pedagogical overview of HMC that aims to bridge the gap between its theoretical foundations and practical applicability. This review article seeks to make HMC more accessible to applied researchers by highlighting its advantages, limitations, and role in enabling scalable and exact Bayesian inference for complex models."}
{"id": "2601.01029", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.01029", "abs": "https://arxiv.org/abs/2601.01029", "authors": ["Zeyu Bian", "Max Biggs", "Ruijiang Gao", "Zhengling Qi"], "title": "Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights", "comment": "74 pages", "summary": "This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies."}
{"id": "2601.01259", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.01259", "abs": "https://arxiv.org/abs/2601.01259", "authors": ["Guilherme Pumi", "Taiane Schaedler Prass", "Douglas Krauthein Verdum"], "title": "A Novel Multiple Imputation Approach For Parameter Estimation in Observation-Driven Time Series Models With Missing Data", "comment": null, "summary": "Handling missing data in time series is a complex problem due to the presence of temporal dependence. General-purpose imputation methods, while widely used, often distort key statistical properties of the data, such as variance and dependence structure, leading to biased estimation and misleading inference. These issues become more pronounced in models that explicitly rely on capturing serial dependence, as standard imputation techniques fail to preserve the underlying dynamics. This paper proposes a novel multiple imputation method specifically designed for parameter estimation in observation-driven models (ODM). The approach takes advantage of the iterative nature of the systematic component in ODM to propagate the dependence structure through missing data, minimizing its impact on estimation. Unlike traditional imputation techniques, the proposed method accommodates continuous, discrete, and mixed-type data while preserving key distributional and dependence properties. We evaluate its performance through Monte Carlo simulations in the context of GARMA models, considering time series with up to 70\\% missing data. An application to the proportion of stocked energy stored in South Brazil further demonstrates its practical utility."}
{"id": "2601.01216", "categories": ["stat.AP", "math.ST", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2601.01216", "abs": "https://arxiv.org/abs/2601.01216", "authors": ["Alejandro Rodriguez Dominguez"], "title": "Order-Constrained Spectral Causality in Multivariate Time Series", "comment": "72 pages, 18 figures, 10 tables", "summary": "We introduce an operator-theoretic framework for causal analysis in multivariate time series based on order-constrained spectral non-invariance. Directional influence is defined as sensitivity of second-order dependence operators to admissible, order-preserving temporal deformations of a designated source component, yielding an intrinsically multivariate causal notion summarized through orthogonally invariant spectral functionals. Under linear Gaussian assumptions, the criterion coincides with linear Granger causality, while beyond this regime it captures collective and nonlinear directional dependence not reflected in pairwise predictability. We establish existence, uniform consistency, and valid inference for the resulting non-smooth supremum--infimum statistics using shift-based randomization that exploits order-induced group invariance, yielding finite-sample exactness under exact invariance and asymptotic validity under weak dependence without parametric assumptions. Simulations demonstrate correct size and strong power against distributed and bulk-dominated alternatives, including nonlinear dependence missed by linear Granger tests with appropriate feature embeddings. An empirical application to a high-dimensional panel of daily financial return series spanning major asset classes illustrates system-level causal monitoring in practice. Directional organization is episodic and stress-dependent, causal propagation strengthens while remaining multi-channel, dominant causal hubs reallocate rapidly, and statistically robust transmission channels are sparse and horizon-heterogeneous even when aggregate lead--lag asymmetry is weak. The framework provides a scalable and interpretable complement to correlation-, factor-, and pairwise Granger-style analyses for complex systems."}
{"id": "2601.01604", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01604", "abs": "https://arxiv.org/abs/2601.01604", "authors": ["Nikolaos Korfiatis"], "title": "grangersearch: An R Package for Exhaustive Granger Causality Testing with Tidyverse Integration", "comment": "16 pages, 2 figures, R package available at https://github.com/nkorf/grangersearch", "summary": "This paper introduces grangersearch, an R package for performing exhaustive Granger causality searches on multiple time series. The package provides: (1) exhaustive pairwise search across multiple variables, (2) automatic lag order optimization with visualization, (3) tidyverse-compatible syntax with pipe operators and non-standard evaluation, and (4) integration with the broom ecosystem through tidy() and glance() methods. The package wraps the vars infrastructure while providing a simple interface for exploratory causal analysis. We describe the statistical methodology, demonstrate the package through worked examples, and discuss practical considerations for applied researchers."}
{"id": "2601.01055", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01055", "abs": "https://arxiv.org/abs/2601.01055", "authors": ["Ernest Fokoué"], "title": "Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics", "comment": "19 pages", "summary": "This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems."}
{"id": "2601.01052", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01052", "abs": "https://arxiv.org/abs/2601.01052", "authors": ["Shangkun Jiang", "Ruggiero Lovreglio", "Thomas J. Cova", "Sangung Park", "Susu Xu", "Xilei Zhao"], "title": "Wildfire Evacuation Analysis Using Facebook Data: Evidence from Palisades and Eaton Fires", "comment": null, "summary": "The growing frequency and intensity of wildfires pose serious threats to communities in wildland-urban interface regions. Understanding evacuation behavior is critical for effective emergency planning. This study analyzes evacuation during the 2025 Palisades and Eaton Fires using high-resolution Facebook data. We propose a comprehensive framework to derive wildfire evacuation-related metrics, including compliance rate, departure timing, delay, origin-destination flows, travel distance, and destination types. A new metric, Damage-Evacuation Disparity Index (DEDI), identifies areas with severe structural damage but low evacuation compliance. Results reveal spatiotemporal heterogeneity: residents closer to the fire evacuated earlier, whereas late or nighttime orders led to lower compliance and longer delays. Contrasting patterns between East and West Altadena further illustrate this disparity. DEDI-identified communities exhibited higher social vulnerability and fire risk. Most evacuations concluded in residential areas, while longer trips concentrated in hotels and public facilities. These findings showcase the Facebook data's potential for data-driven wildfire evacuation planning."}
{"id": "2601.01344", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01344", "abs": "https://arxiv.org/abs/2601.01344", "authors": ["Shiyin Du", "Yiting Chen", "Wenzhi Yang", "Qiong Li", "Xiaoping Shi"], "title": "Adaptive Kernel Regression for Constrained Route Alignment: Theory and Iterative Data Sharpening", "comment": "31 pages, 15 figures", "summary": "Route alignment design in surveying and transportation engineering frequently involves fixed waypoint constraints, where a path must precisely traverse specific coordinates. While existing literature primarily relies on geometric optimization or control-theoretic spline frameworks, there is a lack of systematic statistical modeling approaches that balance global smoothness with exact point adherence. This paper proposes an Adaptive Nadaraya-Watson (ANW) kernel regression estimator designed to address the fixed waypoint problem. By incorporating waypoint-specific weight tuning parameters, the ANW estimator decouples global smoothing from local constraint satisfaction, avoiding the \"jagged\" artifacts common in naive local bandwidth-shrinking strategies. To further enhance estimation accuracy, we develop an iterative data sharpening algorithm that systematically reduces bias while maintaining the stability of the kernel framework. We establish the theoretical foundation for the ANW estimator by deriving its asymptotic bias and variance and proving its convergence properties under the internal constraint model. Numerical case studies in 1D and 2D trajectory planning demonstrate that the method effectively balances root mean square error (RMSE) and curvature smoothness. Finally, we validate the practical utility of the framework through empirical applications to railway and highway route planning. In sum, this work provides a stable, theoretically grounded, and computationally efficient solution for complex, constrained alignment design problems."}
{"id": "2601.01259", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.01259", "abs": "https://arxiv.org/abs/2601.01259", "authors": ["Guilherme Pumi", "Taiane Schaedler Prass", "Douglas Krauthein Verdum"], "title": "A Novel Multiple Imputation Approach For Parameter Estimation in Observation-Driven Time Series Models With Missing Data", "comment": null, "summary": "Handling missing data in time series is a complex problem due to the presence of temporal dependence. General-purpose imputation methods, while widely used, often distort key statistical properties of the data, such as variance and dependence structure, leading to biased estimation and misleading inference. These issues become more pronounced in models that explicitly rely on capturing serial dependence, as standard imputation techniques fail to preserve the underlying dynamics. This paper proposes a novel multiple imputation method specifically designed for parameter estimation in observation-driven models (ODM). The approach takes advantage of the iterative nature of the systematic component in ODM to propagate the dependence structure through missing data, minimizing its impact on estimation. Unlike traditional imputation techniques, the proposed method accommodates continuous, discrete, and mixed-type data while preserving key distributional and dependence properties. We evaluate its performance through Monte Carlo simulations in the context of GARMA models, considering time series with up to 70\\% missing data. An application to the proportion of stocked energy stored in South Brazil further demonstrates its practical utility."}
{"id": "2601.01642", "categories": ["stat.ME", "q-fin.CP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.01642", "abs": "https://arxiv.org/abs/2601.01642", "authors": ["Dohyun Ahn", "Huiyi Chen", "Lewen Zheng"], "title": "Wasserstein Distributionally Robust Rare-Event Simulation", "comment": null, "summary": "Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks."}
{"id": "2601.01097", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01097", "abs": "https://arxiv.org/abs/2601.01097", "authors": ["Xuan Son Nguyen", "Shuo Yang", "Aymeric Histace"], "title": "Neural Networks on Symmetric Spaces of Noncompact Type", "comment": null, "summary": "Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference."}
{"id": "2601.01117", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01117", "abs": "https://arxiv.org/abs/2601.01117", "authors": ["Shuhan Ai"], "title": "Discussion Network Formation and Evolution in an Online Professional Development Class: Evidence from a MOOC for K-12 Educators", "comment": null, "summary": "Understanding how educators interact and form peer networks in online professional development contexts has become increasingly important as MOOCs for educators (MOOC-Eds) proliferate. This study examines peer discussion network formation and evolution in 'The Digital Learning Transition in K-12 Schools', a MOOC-Ed offered to U.S. and international educators in Spring 2013. Using cross-sectional and temporal exponential random graph models (ERGMs and TERGMs), the study analyzes two network subsamples: the largest connected component (N = 363) and active participants with three or more interactions (N = 227). Results reveal strong reciprocity and transitive closure effects across both networks, with participants six to nine times more likely to reciprocate interactions and over twice as likely to form ties with peers sharing common discussion partners. Assigned discussion group homophily emerged as the strongest predictor of tie formation, while regional homophily and willingness to connect also significantly influenced network structure. Temporal analysis showed discussion activity peaked mid-course before declining sharply, with network structure evolving from broadly distributed participation to concentrated interaction among a tightly connected core. These findings illuminate the mechanisms driving peer-supported learning in online professional development contexts and suggest design implications for fostering sustained educator engagement in MOOC-based learning environments."}
{"id": "2601.01380", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01380", "abs": "https://arxiv.org/abs/2601.01380", "authors": ["Xingyu Li", "Qing Liu", "Tony Jiang", "Hong Amy Xia", "Peng Wei", "Brian P. Hobbs"], "title": "Unsupervised dense random survival forests identify interpretable patient profiles with heterogeneous treatment benefit", "comment": null, "summary": "Precision oncology aims to prescribe the optimal cancer treatment to the right patients, maximizing therapeutic benefits. However, identifying patient subgroups that may benefit more from experimental cancer treatments based on randomized clinical trials presents a significant analytical challenge. To address this, we introduce a novel unsupervised machine learning approach based on very dense random survival forests (up to 100,000 trees), equipped with a new splitting rule that explicitly targets treatment-effect heterogeneity. This method is robust, interpretable, and effectively identifies responsive subgroups. Extensive simulations confirm its ability to detect heterogeneous patient responses and distinguish between datasets with and without heterogeneity, while maintaining a stringent Type I error rate of 1%. We further validate its performance using Phase III randomized clinical trial datasets, demonstrating significant patient heterogeneity in treatment response based on baseline characteristics."}
{"id": "2601.01147", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01147", "abs": "https://arxiv.org/abs/2601.01147", "authors": ["Johan Hallberg Szabadváry"], "title": "Conformal Blindness: A Note on $A$-Cryptic change-points", "comment": "6 pages, 3 figures", "summary": "Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \\emph{conformal blindness}.\n  Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \\emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.\n  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts."}
{"id": "2601.01216", "categories": ["stat.AP", "math.ST", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2601.01216", "abs": "https://arxiv.org/abs/2601.01216", "authors": ["Alejandro Rodriguez Dominguez"], "title": "Order-Constrained Spectral Causality in Multivariate Time Series", "comment": "72 pages, 18 figures, 10 tables", "summary": "We introduce an operator-theoretic framework for causal analysis in multivariate time series based on order-constrained spectral non-invariance. Directional influence is defined as sensitivity of second-order dependence operators to admissible, order-preserving temporal deformations of a designated source component, yielding an intrinsically multivariate causal notion summarized through orthogonally invariant spectral functionals. Under linear Gaussian assumptions, the criterion coincides with linear Granger causality, while beyond this regime it captures collective and nonlinear directional dependence not reflected in pairwise predictability. We establish existence, uniform consistency, and valid inference for the resulting non-smooth supremum--infimum statistics using shift-based randomization that exploits order-induced group invariance, yielding finite-sample exactness under exact invariance and asymptotic validity under weak dependence without parametric assumptions. Simulations demonstrate correct size and strong power against distributed and bulk-dominated alternatives, including nonlinear dependence missed by linear Granger tests with appropriate feature embeddings. An empirical application to a high-dimensional panel of daily financial return series spanning major asset classes illustrates system-level causal monitoring in practice. Directional organization is episodic and stress-dependent, causal propagation strengthens while remaining multi-channel, dominant causal hubs reallocate rapidly, and statistically robust transmission channels are sparse and horizon-heterogeneous even when aggregate lead--lag asymmetry is weak. The framework provides a scalable and interpretable complement to correlation-, factor-, and pairwise Granger-style analyses for complex systems."}
{"id": "2601.01432", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01432", "abs": "https://arxiv.org/abs/2601.01432", "authors": ["Sai Li", "Linjun Zhang"], "title": "Personalizing black-box models for nonparametric regression with minimax optimality", "comment": null, "summary": "Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models."}
{"id": "2601.01238", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01238", "abs": "https://arxiv.org/abs/2601.01238", "authors": ["Kalyaan Rao"], "title": "Evidence Slopes and Effective Dimension in Singular Linear Models", "comment": "Preprint. 10 pages, 6 figures. Under review", "summary": "Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.\n  We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.\n  Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings."}
{"id": "2601.01245", "categories": ["stat.AP", "q-bio.QM", "q-bio.TO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01245", "abs": "https://arxiv.org/abs/2601.01245", "authors": ["Yiyuan Huang", "Ling Zhou", "Min Zhang", "Peter X. K. Song"], "title": "Model-Assisted Causal Inference for the Treatment Effect on Recurrent Events in the Presence of Terminal Events", "comment": null, "summary": "This paper is motivated by evaluating the benefits of patients receiving mechanical circulatory support (MCS) devices in end-stage heart failure management inference, in which hypothesis testing for a treatment effect on the risk of recurrent events is challenged in the presence of terminal events. Existing methods based on cumulative frequency unreasonably disadvantage longer survivors as they tend to experience more recurrent events. The While-Alive-based (WA) test has provided a solution to address this survival-length-bias problem, and it performs well when the recurrent event rate holds constant over time. However, if such a constant-rate assumption is violated, the WA test can exhibit an inflated type I error and inaccurate estimation of treatment effects. To fill this methodological gap, we propose a Proportional Rate Marginal Structural Model-assisted Test (PR-MSMaT) in the causal inference framework of separable treatment effects for recurrent and terminal events. Using the simulation study, we demonstrate that our PR-MSMaT can properly control type I error while gaining power comparable to the WA test under time-varying recurrent event rates. We employ PR-MSMaT to compare different MCS devices with the postoperative risk of gastrointestinal bleeding among patients enrolled in the Interagency Registry of Mechanically Assisted Circulatory Support program."}
{"id": "2601.01510", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01510", "abs": "https://arxiv.org/abs/2601.01510", "authors": ["Qi Lyu", "Xiaoyu Zhang", "Guodong Li", "Di Wang"], "title": "Reduced-Rank Autoregressive Model for High-Dimensional Multivariate Network Time Series", "comment": "123 pages, 10 figures", "summary": "Multivariate network time series are ubiquitous in modern systems, yet existing network autoregressive models typically treat nodes as scalar processes, ignoring cross-variable spillovers. To capture these complex interactions without the curse of dimensionality, we propose the Reduced-Rank Network Autoregressive (RRNAR) model. Our framework introduces a separable bilinear transition structure that couples the known network topology with a learnable low-rank variable subspace. We estimate the model using a novel Scaled Gradient Descent (ScaledGD) algorithm, explicitly designed to bridge the gap between rigid network scalars and flexible factor components. Theoretically, we establish non-asymptotic error bounds under a novel distance metric. A key finding is a network-induced blessing of dimensionality: for sparse networks, the estimation accuracy for network parameters improves as the network size grows. Applications to traffic and server monitoring networks demonstrate that RRNAR significantly outperforms univariate and unstructured benchmarks by identifying latent cross-channel propagation mechanisms."}
{"id": "2601.01442", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01442", "abs": "https://arxiv.org/abs/2601.01442", "authors": ["Dongrong Li", "Tianwei Yu", "Xiaodan Fan"], "title": "Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations", "comment": "45 pages, 2 figures", "summary": "The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS)."}
{"id": "2601.01351", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01351", "abs": "https://arxiv.org/abs/2601.01351", "authors": ["Jingkun Qiu", "Hanyue Chen", "Song Xi Chen"], "title": "Errors-in-variables regression for dependent data with estimated error covariance matrix: To prewhiten or not?", "comment": null, "summary": "We consider statistical inference for errors-in-variables regression models with dependent observations under the high dimensionality of the error covariance matrix. It is tempting to prewhiten the model and data that had led to efficient weighted least squares estimation in the presence of the measurement errors, as being practised in the optimal fingerprinting approach in climate change studies. However, it is unclear to what extent the prewhitened estimator can improve the estimation efficiency of the unprewhitened estimator for errors-in-variables regression. We compare the prewhitening and unprewhitening estimators in terms of their estimation efficiency and computational cost. It shows that while the prewhitening operation does not necessarily improve the estimation efficiency of its unprewhitening counterpart, it demands more on the ensemble size needed in the error-covariance matrix estimation to ensure the asymptotic normality, and hence it would requires much more computationally resource."}
{"id": "2601.01583", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01583", "abs": "https://arxiv.org/abs/2601.01583", "authors": ["Caner Tanış"], "title": "Cubic lower record-based transmuted family of distributions: Theory, Estimation, Applications", "comment": null, "summary": "In this study, a family of distributions called cubic lower record-based transmuted is provided. A special case of this family is proposed as an alternative exponential distribution. Several statistical properties are explored. We utilize nine different methods to estimate the parameters of the suggested distribution. In order to compare the performances of these methods, we consider a comprehensive Monte-Carlo simulation study. As a result of simulation study, we conclude that minimum absolute distance estimator is a valuable alternative to maximum likelihood estimator. Then, we carried out two real-world data examples to evaluate the fits of introduced distribution as well as its potential competitor ones. The findings of real-world data analysis show that the best-fitting distribution for both datasets is our model."}
{"id": "2601.01480", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01480", "abs": "https://arxiv.org/abs/2601.01480", "authors": ["Aman Sunesh", "Allan Ma", "Siddarth Nilol"], "title": "Modeling Information Blackouts in Missing Not-At-Random Time Series Data", "comment": "8 pages, 7 figures, 3 tables", "summary": "Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative."}
{"id": "2601.02011", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02011", "abs": "https://arxiv.org/abs/2601.02011", "authors": ["Andrew Nugent", "Yi Ting Loo", "Jack Buckingham"], "title": "Cyclists Cardiac Conundrum", "comment": null, "summary": "Arrhythmia is an abnormality of the heart's rhythm, caused by problems in the conductive system and resulting in irregular heartbeats. There is increasing evidence that undertaking frequent endurance sports training elevates one's risk of arrhythmia. Arrhythmia is diagnosed using an electrocardiogram (ECG) but this is not typically available to athletes while exercising. Previous research by Crickles investigates the usefulness of commonly available heart rate data in detecting signs of arrhythmia. It is hypothesised that a feature termed 'gappiness', defined by jumps in the heart rate while the athlete is under exertion, may be a characteristic of arrhythmia. A correlation was found between the proportion of 'gappy' activities and survey responses about heart rhythm problems. We develop on this measure by exploring various methods to detect spikes in heart rate data, allowing us to describe the extent of irregularity in an activity via the rate of spikes. We first compare the performance of these methods on simulated data, where we find that smoothing using a moving average and setting a constant threshold on the residuals is most effective. This method was then implemented on real data provided by Crickles from 168 athletes, where no significant correlation was found between the spike rates and survey responses. However, when considering only those spikes that occur above a heart rate of 160 beats per minute (bpm) a significant correlation was found. This supports the hypothesis that jumps at only high heart rates are informative of arrhythmia and indicates the need for further research into better measures to characterise features of heart rate data."}
{"id": "2601.01642", "categories": ["stat.ME", "q-fin.CP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.01642", "abs": "https://arxiv.org/abs/2601.01642", "authors": ["Dohyun Ahn", "Huiyi Chen", "Lewen Zheng"], "title": "Wasserstein Distributionally Robust Rare-Event Simulation", "comment": null, "summary": "Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks."}
{"id": "2601.01594", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01594", "abs": "https://arxiv.org/abs/2601.01594", "authors": ["Alois Duston", "Tan Bui-Thanh"], "title": "Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity", "comment": null, "summary": "We introduce and prove a \\textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems."}
{"id": "2601.02226", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02226", "abs": "https://arxiv.org/abs/2601.02226", "authors": ["Lukas Klein", "Gunter Grieser", "Carl-Ludwig Fischer-Fröhlich", "Axel Rahmel", "Henrik Stahl", "Andreas Wienke", "Antje Jahn-Eimermacher"], "title": "Initial data analysis of the national German transplantation registry with a focus on kidney transplantation", "comment": "28 pages, 9 figures, 1 supplementary document, Submitted to BMC Medical Research Methodology", "summary": "This study presents an Initial Data Analysis (IDA) of the German Transplantation Registry (TxReg) data for a better data understanding and to inform future data analyses. The IDA is focusing on data on first-time kidney-only transplantations in adult recipients from deceased donors between 2006 and 2016 and refers to data from 14,954 recipients and 9,964 donors across 25 tables. Investigated aspects include missing data patterns and structure, data consistency, and availability of event time data. Results show that missing data proportions vary widely, with some tables nearly complete while others have over 50% missing values. Missing data patterns are identified using a decision tree approach. An influx and outflux analysis demonstrates that some variables have high potential for imputing missing data, while others were less suitable for imputation. We identified 168 multi-sourced variables that are reported by multiple data providers in parallel leading to discrepancies for some variables but also providing opportunities for missing data imputation. Our findings on event time data demonstrate the importance of carefully selecting the variables used for event time analyses as results will strongly depend on this selection. In summary, our findings highlight the challenges when utilizing the TxReg data for research and provide recommendations for data preprocessing and analysis in future analyses."}
{"id": "2601.01662", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01662", "abs": "https://arxiv.org/abs/2601.01662", "authors": ["Saku Suorsa", "Aki Vehtari"], "title": "Predictive Assessment and Comparison of Bayesian Survival Models for Cancer Recurrence", "comment": null, "summary": "Complex data features, such as unmodelled censored event times and variables with time-dependent effects, are common in cancer recurrence studies and pose challenges for Bayesian survival modelling. However, current methodologies for predictive model checking and comparison often fail to adequately address these features. This paper bridges that gap by introducing new, targeted recommendations for predictive assessment and comparison of Bayesian survival models for cancer recurrence. Our recommendations cover a variety of different scenarios and models. Accompanying code together with our implementations to open source software help in replicating the results and applying our recommendations in practice."}
{"id": "2601.01619", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01619", "abs": "https://arxiv.org/abs/2601.01619", "authors": ["Maxat Tezekbayev", "Rustem Takhanov", "Arman Bolatov", "Zhenisbek Assylbekov"], "title": "Deep Linear Discriminant Analysis Revisited", "comment": null, "summary": "We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \\emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models."}
{"id": "2601.01480", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01480", "abs": "https://arxiv.org/abs/2601.01480", "authors": ["Aman Sunesh", "Allan Ma", "Siddarth Nilol"], "title": "Modeling Information Blackouts in Missing Not-At-Random Time Series Data", "comment": "8 pages, 7 figures, 3 tables", "summary": "Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative."}
{"id": "2601.01686", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01686", "abs": "https://arxiv.org/abs/2601.01686", "authors": ["Jackie Siaw Tze Wong", "Emiliano A. Valdez"], "title": "Bayesian mortality forecasting with a Conway--Maxwell--Poisson specification", "comment": null, "summary": "This paper presents a novel approach to stochastic mortality modelling by using the Conway--Maxwell--Poisson (CMP) distribution to model death counts. Unlike standard Poisson or negative binomial distributions, the CMP is a more adaptable choice because it can account for different levels of variability in the data, a feature known as dispersion. Specifically, it can handle data that are underdispersed (less variable than expected), equidispersed (as variable as expected), and overdispersed (more variable than expected). We develop a Bayesian formulation that treats the dispersion level as an unknown parameter, using a Gamma prior to enable a robust and coherent integration of the parameter, process, and distributional uncertainty. The model is calibrated using Markov chain Monte Carlo (MCMC) methods, with model performance evaluated using standard statistical criteria such as residual analysis and scoring rules. An empirical study using England and Wales male mortality data shows that our CMP-based models provide a better fit for both existing data and future predictions compared to traditional Poisson and negative binomial models, particularly when the data exhibit overdispersion. Finally, we conduct a sensitivity analysis with respect to prior specification to assess robustness."}
{"id": "2601.01679", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01679", "abs": "https://arxiv.org/abs/2601.01679", "authors": ["Maxat Tezekbayev", "Arman Bolatov", "Zhenisbek Assylbekov"], "title": "Simplex Deep Linear Discriminant Analysis", "comment": null, "summary": "We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections."}
{"id": "2601.01583", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01583", "abs": "https://arxiv.org/abs/2601.01583", "authors": ["Caner Tanış"], "title": "Cubic lower record-based transmuted family of distributions: Theory, Estimation, Applications", "comment": null, "summary": "In this study, a family of distributions called cubic lower record-based transmuted is provided. A special case of this family is proposed as an alternative exponential distribution. Several statistical properties are explored. We utilize nine different methods to estimate the parameters of the suggested distribution. In order to compare the performances of these methods, we consider a comprehensive Monte-Carlo simulation study. As a result of simulation study, we conclude that minimum absolute distance estimator is a valuable alternative to maximum likelihood estimator. Then, we carried out two real-world data examples to evaluate the fits of introduced distribution as well as its potential competitor ones. The findings of real-world data analysis show that the best-fitting distribution for both datasets is our model."}
{"id": "2601.01699", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01699", "abs": "https://arxiv.org/abs/2601.01699", "authors": ["Qicheng Zhao", "Celia M. T. Greenwood", "Qihuang Zhang"], "title": "Varying-Coefficient Mixture of Experts Model", "comment": "63 pages", "summary": "Mixture-of-Experts (MoE) is a flexible framework that combines multiple specialized submodels (``experts''), by assigning covariate-dependent weights (``gating functions'') to each expert, and have been commonly used for analyzing heterogeneous data. Existing statistical MoE formulations typically assume constant coefficients, for covariate effects within the expert or gating models, which can be inadequate for longitudinal, spatial, or other dynamic settings where covariate influences and latent subpopulation structure evolve across a known dimension. We propose a Varying-Coefficient Mixture of Experts (VCMoE) model that allows all coefficient effects in both the gating functions and expert models to vary along an indexing variable. We establish identifiability and consistency of the proposed model, and develop an estimation procedure, label-consistent EM algorithm, for both fully functional and hybrid specifications, along with the corresponding asymptotic distributions of the resulting estimators. For inference, simultaneous confidence bands are constructed using both asymptotic theory for the maximum discrepancy between the estimated functional coefficients and their true counterparts, and with bootstrap methods. In addition, a generalized likelihood ratio test is developed to examine whether a coefficient function is genuinely varying across the index variable. Simulation studies demonstrate good finite-sample performance, with acceptable bias and satisfactory coverage rates. We illustrate the proposed VCMoE model using a dataset of single nucleus gene expression in embryonic mice to characterize the temporal dynamics of the associations between the expression levels of genes Satb2 and Bcl11b across two latent cell subpopulations of neurons, yielding results that are consistent with prior findings."}
{"id": "2601.01757", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01757", "abs": "https://arxiv.org/abs/2601.01757", "authors": ["Jiakun Jiang", "Dewei Xiang", "Chenliang Gu", "Wei Liu", "Binhuan Wang"], "title": "Sparse Convex Biclustering", "comment": null, "summary": "Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets."}
{"id": "2601.01811", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01811", "abs": "https://arxiv.org/abs/2601.01811", "authors": ["Xin Zhang", "Hui Zhang", "Satrajit Roychoudhury"], "title": "On regional treatment effect assessment using robust MAP priors", "comment": null, "summary": "Bayesian dynamic borrowing has become an increasingly important tool for evaluating the consistency of regional treatment effects which is a key requirement for local regulatory approval of a new drug. It helps increase the precision of regional treatment effect estimate when regional and global data are similar, while guarding against potential bias when they differ. In practice, the two-component mixture prior, of which one mixture component utilizes the power prior to incorporate external data, is widely used. It allows convenient prior specification, analytical posterior computation, and fast evaluation of operating characteristics. Though the robust meta-analytical-predictive (MAP) prior is broadly used with multiple external data sources, it remains underutilized for regional treatment effect assessment (typically only one external data source is available) due to its inherit complexity in prior specification and posterior computation. In this article, we illustrate the applicability of the robust MAP prior in the regional treatment effect assessment by developing a closed-form approximation for its posterior distribution while leveraging its relationship with the power prior. The proposed methodology substantially reduces the computational burden of identifying prior parameters for desired operating characteristics. Moreover, we have demonstrated that the MAP prior is an attractive choice to construct the informative component of the mixture prior compared to the power prior. The advantage can be explained through a Bayesian hypothesis testing perspective. Using a real-world example, we illustrate how our proposed method enables efficient and transparent development of a Bayesian dynamic borrowing design to show regional consistency."}
{"id": "2601.01811", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01811", "abs": "https://arxiv.org/abs/2601.01811", "authors": ["Xin Zhang", "Hui Zhang", "Satrajit Roychoudhury"], "title": "On regional treatment effect assessment using robust MAP priors", "comment": null, "summary": "Bayesian dynamic borrowing has become an increasingly important tool for evaluating the consistency of regional treatment effects which is a key requirement for local regulatory approval of a new drug. It helps increase the precision of regional treatment effect estimate when regional and global data are similar, while guarding against potential bias when they differ. In practice, the two-component mixture prior, of which one mixture component utilizes the power prior to incorporate external data, is widely used. It allows convenient prior specification, analytical posterior computation, and fast evaluation of operating characteristics. Though the robust meta-analytical-predictive (MAP) prior is broadly used with multiple external data sources, it remains underutilized for regional treatment effect assessment (typically only one external data source is available) due to its inherit complexity in prior specification and posterior computation. In this article, we illustrate the applicability of the robust MAP prior in the regional treatment effect assessment by developing a closed-form approximation for its posterior distribution while leveraging its relationship with the power prior. The proposed methodology substantially reduces the computational burden of identifying prior parameters for desired operating characteristics. Moreover, we have demonstrated that the MAP prior is an attractive choice to construct the informative component of the mixture prior compared to the power prior. The advantage can be explained through a Bayesian hypothesis testing perspective. Using a real-world example, we illustrate how our proposed method enables efficient and transparent development of a Bayesian dynamic borrowing design to show regional consistency."}
{"id": "2601.01970", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01970", "abs": "https://arxiv.org/abs/2601.01970", "authors": ["Ayomide Afolabi", "Ebere Ogburu", "Symon Kimitei"], "title": "A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk", "comment": null, "summary": "This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem."}
{"id": "2601.01830", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01830", "abs": "https://arxiv.org/abs/2601.01830", "authors": ["Kwangmoon Park", "Hongzhe Li"], "title": "Causal Network Recovery in Perturb-seq Experiments Using Proxy and Instrumental Variables", "comment": null, "summary": "Emerging single-cell technologies that integrate CRISPR-based genetic perturbations with single-cell RNA sequencing, such as Perturb-seq, have substantially advanced our understanding of gene regulation and causal influence of genes. While Perturb-seq data provide valuable causal insights into gene-gene interactions, statistical concerns remain regarding unobserved confounders that may bias inference. These latent factors may arise not only from intrinsic molecular features of regulatory elements encoded in Perturb-seq experiments, but also from unobserved genes arising from cost-constrained experimental designs. Although methods for analyzing largescale Perturb-seq data are rapidly maturing, approaches that explicitly account for such unobserved confounders in learning the causal gene networks are still lacking. Here, we propose a novel method to recover causal gene networks from Perturb-seq experiments with robustness to arbitrarily omitted confounders. Our framework leverages proxy and instrumental variable strategies to exploit the rich information embedded in perturbations, enabling unbiased estimation of the underlying directed acyclic graph (DAG) of gene expressions. Simulation studies and analyses of CRISPR interference experiments of K562 cells demonstrate that our method outperforms baseline approaches that ignore unmeasured confounding, yielding more accurate and biologically relevant recovery of the true gene causal DAGs."}
{"id": "2601.01813", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01813", "abs": "https://arxiv.org/abs/2601.01813", "authors": ["Pratik Nag", "Andrew Zammit-Mangion", "Sumeetpal Singh", "Noel Cressie"], "title": "Spatio-temporal modeling and forecasting with Fourier neural operators", "comment": null, "summary": "Spatio-temporal process models are often used for modeling dynamic physical and biological phenomena that evolve across space and time. These phenomena may exhibit environmental heterogeneity and complex interactions that are difficult to capture using traditional statistical process models such as Gaussian processes. This work proposes the use of Fourier neural operators (FNOs) for constructing statistical dynamical spatio-temporal models for forecasting. An FNO is a flexible mapping of functions that approximates the solution operator of possibly unknown linear or non-linear partial differential equations (PDEs) in a computationally efficient manner. It does so using samples of inputs and their respective outputs, and hence explicit knowledge of the underlying PDE is not required. Through simulations from a nonlinear PDE with known solution, we compare FNO forecasts to those from state-of-the-art statistical spatio-temporal-forecasting methods. Further, using sea surface temperature data over the Atlantic Ocean and precipitation data across Europe, we demonstrate the ability of FNO-based dynamic spatio-temporal (DST) statistical modeling to capture complex real-world spatio-temporal dependencies. Using collections of testing instances, we show that the FNO-DST forecasts are accurate with valid uncertainty quantification."}
{"id": "2601.02241", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02241", "abs": "https://arxiv.org/abs/2601.02241", "authors": ["Svenja Jedhoff", "Elizaveta Semenova", "Aura Raulo", "Anne Meyer", "Paul-Christian Bürkner"], "title": "From Mice to Trains: Amortized Bayesian Inference on Graph Data", "comment": null, "summary": "Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration."}
{"id": "2601.01970", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01970", "abs": "https://arxiv.org/abs/2601.01970", "authors": ["Ayomide Afolabi", "Ebere Ogburu", "Symon Kimitei"], "title": "A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk", "comment": null, "summary": "This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem."}
{"id": "2601.01830", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01830", "abs": "https://arxiv.org/abs/2601.01830", "authors": ["Kwangmoon Park", "Hongzhe Li"], "title": "Causal Network Recovery in Perturb-seq Experiments Using Proxy and Instrumental Variables", "comment": null, "summary": "Emerging single-cell technologies that integrate CRISPR-based genetic perturbations with single-cell RNA sequencing, such as Perturb-seq, have substantially advanced our understanding of gene regulation and causal influence of genes. While Perturb-seq data provide valuable causal insights into gene-gene interactions, statistical concerns remain regarding unobserved confounders that may bias inference. These latent factors may arise not only from intrinsic molecular features of regulatory elements encoded in Perturb-seq experiments, but also from unobserved genes arising from cost-constrained experimental designs. Although methods for analyzing largescale Perturb-seq data are rapidly maturing, approaches that explicitly account for such unobserved confounders in learning the causal gene networks are still lacking. Here, we propose a novel method to recover causal gene networks from Perturb-seq experiments with robustness to arbitrarily omitted confounders. Our framework leverages proxy and instrumental variable strategies to exploit the rich information embedded in perturbations, enabling unbiased estimation of the underlying directed acyclic graph (DAG) of gene expressions. Simulation studies and analyses of CRISPR interference experiments of K562 cells demonstrate that our method outperforms baseline approaches that ignore unmeasured confounding, yielding more accurate and biologically relevant recovery of the true gene causal DAGs."}
{"id": "2601.00904", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00904", "abs": "https://arxiv.org/abs/2601.00904", "authors": ["Qiang Li", "Shujian Yu", "Liang Ma", "Chen Ma", "Jingyu Liu", "Tulay Adali", "Vince D. Calhoun"], "title": "Deep Deterministic Nonlinear ICA via Total Correlation Minimization with Matrix-Based Entropy Functional", "comment": "16 pages, 9 figures", "summary": "Blind source separation, particularly through independent component analysis (ICA), is widely utilized across various signal processing domains for disentangling underlying components from observed mixed signals, owing to its fully data-driven nature that minimizes reliance on prior assumptions. However, conventional ICA methods rely on an assumption of linear mixing, limiting their ability to capture complex nonlinear relationships and to maintain robustness in noisy environments. In this work, we present deep deterministic nonlinear independent component analysis (DDICA), a novel deep neural network-based framework designed to address these limitations. DDICA leverages a matrix-based entropy function to directly optimize the independence criterion via stochastic gradient descent, bypassing the need for variational approximations or adversarial schemes. This results in a streamlined training process and improved resilience to noise. We validated the effectiveness and generalizability of DDICA across a range of applications, including simulated signal mixtures, hyperspectral image unmixing, modeling of primary visual receptive fields, and resting-state functional magnetic resonance imaging (fMRI) data analysis. Experimental results demonstrate that DDICA effectively separates independent components with high accuracy across a range of applications. These findings suggest that DDICA offers a robust and versatile solution for blind source separation in diverse signal processing tasks."}
{"id": "2601.02292", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02292", "abs": "https://arxiv.org/abs/2601.02292", "authors": ["Alessia Mapelli", "Laura Carini", "Francesca Ieva", "Sara Sommariva"], "title": "A neighbour selection approach for identifying differential networks in conditional functional graphical models", "comment": null, "summary": "Estimation of brain functional connectivity from EEG data is of great importance both for medical research and diagnosis. It involves quantifying the conditional dependencies among the activity of different brain areas from the time-varying electric field recorded by sensors placed outside the scalp. These dependencies may vary within and across individuals and be influenced by covariates such as age, mental status, or disease severity. Motivated by this problem, we propose a novel neighbour selection approach based on functional-on-functional regression for the characterization of conditional Gaussian functional graphical models. We provide a fully automated, data-driven procedure for inferring conditional dependence structures among observed functional variables. In particular, pairwise interactions are directly identified and allowed to vary as a function of covariates, enabling covariate-specific modulation of connectivity patterns. Our proposed method accommodates an arbitrary number of continuous and discrete covariates. Moreover, unlike existing methods for direct estimation of differential graphical models, the proposed approach yields directly interpretable coefficients, allowing discrimination between covariate-induced increases and decreases in interaction strength. The methodology is evaluated through extensive simulation studies and an application to experimental EEG data. The results demonstrate clear advantages over existing approaches, including higher estimation accuracy and substantially reduced computational cost, especially in high-dimensional settings."}
{"id": "2601.01854", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01854", "abs": "https://arxiv.org/abs/2601.01854", "authors": ["Lianqiang Qu", "Long Lv", "Liuquan Sun"], "title": "Causal inference for censored data with continuous marks", "comment": "22", "summary": "This paper presents a framework for causal inference in the presence of censored data, where the failure time is marked by a continuous variable known as a mark. The mark can be viewed as an extension of the failure cause in the classical competing risks model where the cause of failure is replaced by a continuous mark only observed at uncensored failure times. Due to the continuous nature of the marks, observations at each specific mark are sparse, making the identification and estimation of causality a challenging task. To address this issue, we define a new mark-specific treatment effect within the potential outcomes framework and characterize its identifying conditions. We then propose a local smoothing causal estimand and establish its asymptotic properties. We evaluate our method using simulation studies as well as a real dataset from the Antibody Mediated Prevention trials."}
{"id": "2601.01432", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01432", "abs": "https://arxiv.org/abs/2601.01432", "authors": ["Sai Li", "Linjun Zhang"], "title": "Personalizing black-box models for nonparametric regression with minimax optimality", "comment": null, "summary": "Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models."}
{"id": "2601.02154", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.02154", "abs": "https://arxiv.org/abs/2601.02154", "authors": ["Nolwenn Le Méhauté", "Jean-François Coeurjolly", "Marie-Hélène Descary"], "title": "Simulation of warping processes with applications to temperature data", "comment": null, "summary": "Curve registration plays a major role in functional data analysis by separating amplitude and phase variation through warping functions and the accurate simulation of warping processes is essential for developing statistical methods that properly account for phase variability in functional data. In this paper, we focus on the simulation of continuous warping processes with a prescribed expectation and a controllable variance. We study and compare three procedures, including two existing methods and a new algorithm based on randomized empirical cumulative distribution functions. For each approach, we provide an operational description and establish theoretical results for the first two moments of the simulated processes. A numerical study illustrates the theoretical findings and highlights the respective merits of the three methods. Finally, we present an application to the analysis of temperature distributions in Montreal based on simulated realizations from a warping process estimated from temperature quantile functions."}
{"id": "2601.01604", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01604", "abs": "https://arxiv.org/abs/2601.01604", "authors": ["Nikolaos Korfiatis"], "title": "grangersearch: An R Package for Exhaustive Granger Causality Testing with Tidyverse Integration", "comment": "16 pages, 2 figures, R package available at https://github.com/nkorf/grangersearch", "summary": "This paper introduces grangersearch, an R package for performing exhaustive Granger causality searches on multiple time series. The package provides: (1) exhaustive pairwise search across multiple variables, (2) automatic lag order optimization with visualization, (3) tidyverse-compatible syntax with pipe operators and non-standard evaluation, and (4) integration with the broom ecosystem through tidy() and glance() methods. The package wraps the vars infrastructure while providing a simple interface for exploratory causal analysis. We describe the statistical methodology, demonstrate the package through worked examples, and discuss practical considerations for applied researchers."}
{"id": "2601.02292", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02292", "abs": "https://arxiv.org/abs/2601.02292", "authors": ["Alessia Mapelli", "Laura Carini", "Francesca Ieva", "Sara Sommariva"], "title": "A neighbour selection approach for identifying differential networks in conditional functional graphical models", "comment": null, "summary": "Estimation of brain functional connectivity from EEG data is of great importance both for medical research and diagnosis. It involves quantifying the conditional dependencies among the activity of different brain areas from the time-varying electric field recorded by sensors placed outside the scalp. These dependencies may vary within and across individuals and be influenced by covariates such as age, mental status, or disease severity. Motivated by this problem, we propose a novel neighbour selection approach based on functional-on-functional regression for the characterization of conditional Gaussian functional graphical models. We provide a fully automated, data-driven procedure for inferring conditional dependence structures among observed functional variables. In particular, pairwise interactions are directly identified and allowed to vary as a function of covariates, enabling covariate-specific modulation of connectivity patterns. Our proposed method accommodates an arbitrary number of continuous and discrete covariates. Moreover, unlike existing methods for direct estimation of differential graphical models, the proposed approach yields directly interpretable coefficients, allowing discrimination between covariate-induced increases and decreases in interaction strength. The methodology is evaluated through extensive simulation studies and an application to experimental EEG data. The results demonstrate clear advantages over existing approaches, including higher estimation accuracy and substantially reduced computational cost, especially in high-dimensional settings."}
{"id": "2601.01699", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01699", "abs": "https://arxiv.org/abs/2601.01699", "authors": ["Qicheng Zhao", "Celia M. T. Greenwood", "Qihuang Zhang"], "title": "Varying-Coefficient Mixture of Experts Model", "comment": "63 pages", "summary": "Mixture-of-Experts (MoE) is a flexible framework that combines multiple specialized submodels (``experts''), by assigning covariate-dependent weights (``gating functions'') to each expert, and have been commonly used for analyzing heterogeneous data. Existing statistical MoE formulations typically assume constant coefficients, for covariate effects within the expert or gating models, which can be inadequate for longitudinal, spatial, or other dynamic settings where covariate influences and latent subpopulation structure evolve across a known dimension. We propose a Varying-Coefficient Mixture of Experts (VCMoE) model that allows all coefficient effects in both the gating functions and expert models to vary along an indexing variable. We establish identifiability and consistency of the proposed model, and develop an estimation procedure, label-consistent EM algorithm, for both fully functional and hybrid specifications, along with the corresponding asymptotic distributions of the resulting estimators. For inference, simultaneous confidence bands are constructed using both asymptotic theory for the maximum discrepancy between the estimated functional coefficients and their true counterparts, and with bootstrap methods. In addition, a generalized likelihood ratio test is developed to examine whether a coefficient function is genuinely varying across the index variable. Simulation studies demonstrate good finite-sample performance, with acceptable bias and satisfactory coverage rates. We illustrate the proposed VCMoE model using a dataset of single nucleus gene expression in embryonic mice to characterize the temporal dynamics of the associations between the expression levels of genes Satb2 and Bcl11b across two latent cell subpopulations of neurons, yielding results that are consistent with prior findings."}
{"id": "2601.02322", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02322", "abs": "https://arxiv.org/abs/2601.02322", "authors": ["Shuozhi Zuo", "Yixin Wang"], "title": "Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction", "comment": null, "summary": "Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts."}
{"id": "2601.01813", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01813", "abs": "https://arxiv.org/abs/2601.01813", "authors": ["Pratik Nag", "Andrew Zammit-Mangion", "Sumeetpal Singh", "Noel Cressie"], "title": "Spatio-temporal modeling and forecasting with Fourier neural operators", "comment": null, "summary": "Spatio-temporal process models are often used for modeling dynamic physical and biological phenomena that evolve across space and time. These phenomena may exhibit environmental heterogeneity and complex interactions that are difficult to capture using traditional statistical process models such as Gaussian processes. This work proposes the use of Fourier neural operators (FNOs) for constructing statistical dynamical spatio-temporal models for forecasting. An FNO is a flexible mapping of functions that approximates the solution operator of possibly unknown linear or non-linear partial differential equations (PDEs) in a computationally efficient manner. It does so using samples of inputs and their respective outputs, and hence explicit knowledge of the underlying PDE is not required. Through simulations from a nonlinear PDE with known solution, we compare FNO forecasts to those from state-of-the-art statistical spatio-temporal-forecasting methods. Further, using sea surface temperature data over the Atlantic Ocean and precipitation data across Europe, we demonstrate the ability of FNO-based dynamic spatio-temporal (DST) statistical modeling to capture complex real-world spatio-temporal dependencies. Using collections of testing instances, we show that the FNO-DST forecasts are accurate with valid uncertainty quantification."}
{"id": "2601.01245", "categories": ["stat.AP", "q-bio.QM", "q-bio.TO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01245", "abs": "https://arxiv.org/abs/2601.01245", "authors": ["Yiyuan Huang", "Ling Zhou", "Min Zhang", "Peter X. K. Song"], "title": "Model-Assisted Causal Inference for the Treatment Effect on Recurrent Events in the Presence of Terminal Events", "comment": null, "summary": "This paper is motivated by evaluating the benefits of patients receiving mechanical circulatory support (MCS) devices in end-stage heart failure management inference, in which hypothesis testing for a treatment effect on the risk of recurrent events is challenged in the presence of terminal events. Existing methods based on cumulative frequency unreasonably disadvantage longer survivors as they tend to experience more recurrent events. The While-Alive-based (WA) test has provided a solution to address this survival-length-bias problem, and it performs well when the recurrent event rate holds constant over time. However, if such a constant-rate assumption is violated, the WA test can exhibit an inflated type I error and inaccurate estimation of treatment effects. To fill this methodological gap, we propose a Proportional Rate Marginal Structural Model-assisted Test (PR-MSMaT) in the causal inference framework of separable treatment effects for recurrent and terminal events. Using the simulation study, we demonstrate that our PR-MSMaT can properly control type I error while gaining power comparable to the WA test under time-varying recurrent event rates. We employ PR-MSMaT to compare different MCS devices with the postoperative risk of gastrointestinal bleeding among patients enrolled in the Interagency Registry of Mechanically Assisted Circulatory Support program."}
{"id": "2601.01422", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01422", "abs": "https://arxiv.org/abs/2601.01422", "authors": ["Arghya Mukherjee", "Dootika Vats"], "title": "Hamiltonian Monte Carlo for (Physics) Dummies", "comment": "39 pages, 12 figures, 1 table", "summary": "Sampling-based inference has seen a surge of interest in recent years. Hamiltonian Monte Carlo (HMC) has emerged as a powerful algorithm that leverages concepts from Hamiltonian dynamics to efficiently explore complex target distributions. Variants of HMC are available in popular software packages, enabling off-the-shelf implementations that have greatly benefited the statistics and machine learning communities. At the same time, the availability of such black-box implementations has made it challenging for users to understand the inner workings of HMC, especially when they are unfamiliar with the underlying physical principles. We provide a pedagogical overview of HMC that aims to bridge the gap between its theoretical foundations and practical applicability. This review article seeks to make HMC more accessible to applied researchers by highlighting its advantages, limitations, and role in enabling scalable and exact Bayesian inference for complex models."}
{"id": "2601.01442", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01442", "abs": "https://arxiv.org/abs/2601.01442", "authors": ["Dongrong Li", "Tianwei Yu", "Xiaodan Fan"], "title": "Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations", "comment": "45 pages, 2 figures", "summary": "The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS)."}
