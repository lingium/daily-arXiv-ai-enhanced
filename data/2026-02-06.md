<div id=toc></div>

# Table of Contents

- [stat.OT](#stat.OT) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.CO](#stat.CO) [Total: 3]
- [stat.ME](#stat.ME) [Total: 15]
- [stat.ML](#stat.ML) [Total: 21]


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [1] [Anyone for chess? Analysing chess ratings above high thresholds](https://arxiv.org/abs/2602.04353)
*Nils Lid Hjort*

Main category: stat.OT

TL;DR: 论文开发了用于分析极端高分尾部分布的统计模型，并将其应用于国际象棋评级数据，发现即使分布均值相近，方差差异也能解释顶尖选手的差距。


<details>
  <summary>Details</summary>
Motivation: 当研究某些"聪明度"参数时，虽然整体分布可能呈高斯分布，但当我们关注极端高分尾部的顶尖少数人时，需要不同的统计模型。特别是当只能获得最高分列表数据时，需要专门的分析工具。

Method: 开发了专门用于分析极端高分尾部分布的统计模型和工具，并将其应用于FIDE 2026年1月公布的活跃棋手数据，包括前100名和2100分以上的男性和女性棋手评级列表。

Result: 研究发现，即使两个或多个分布的期望值或中位数非常接近，方差上的微小差异也能解释顶尖选手之间的差距。这一发现有助于理解为什么在某些领域，即使整体平均水平相似，顶尖表现者之间仍存在明显差异。

Conclusion: 对于极端高分尾部分布的分析需要专门的统计模型，方差差异是解释顶尖表现者差距的关键因素，即使均值相似。这一方法可应用于各种需要分析顶尖表现的领域。

Abstract: Suppose some cleverness score parameter is sufficiently
  interesting to be defined and then measured, perhaps for
  different strata of specialists or for the broader population.
  Such phenomena could have Gaussian distributions,
  when it comes to all players in a stratum, but when interest
  focuses on the very tails, for the top few percent,
  those above certain high thresholds,
  different models are called for, along with the need
  to analyse such based on the listed top scores only.
  In this note I develop such models and tools,
  and apply them to the top-100 and above 2100 points
  lists for regular chess ratings, for the currently active
  14671 men and 753 women,
  as given by the FIDE, January 2026.
  It is argued that even when two or more distributions have
  close to identical expected values, or medians,
  even smaller differences in variance may explain gaps
  for the few very best ones.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [2] [Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment](https://arxiv.org/abs/2602.04092)
*Oana M. Enache,Sherri Rose*

Main category: stat.AP

TL;DR: 该论文将生存分析中的限制平均损失时间（RMTL）方法扩展到医疗保险优势计划中，用于估计编码强度和可能的过度编码问题，并开发了开源R包来模拟真实的标记数据。


<details>
  <summary>Details</summary>
Motivation: 医疗保险优势计划中，由于保险公司按提交的健康状况获得前瞻性支付，存在过度编码（upcoding）的激励，这导致联邦政府每年损失数十亿美元，但缺乏可靠的估计方法。

Method: 将生存分析中的限制平均损失时间（RMTL）方法扩展到医疗保险优势计划，提出多种新颖的编码强度估计器，包括考虑不可靠报告的情况，并在模拟数据中验证性能。

Result: 开发了开源R包来模拟真实的标记过度编码数据，这些数据以前不可用，并在利用NIH All of Us研究的模拟数据中展示了估计器的性能。

Conclusion: 该方法为医疗保险优势计划中的过度编码问题提供了新的估计工具，有助于识别和量化这一成本高昂的问题，为政策制定提供依据。

Abstract: Time-to-event estimation (i.e., survival analysis) is common in health research, most often using methods that assume proportional hazards and no competing risks. Because both assumptions are frequently invalid, estimators more aligned with real-world settings have been proposed. An effect can be estimated as the difference in areas below the cumulative incidence functions of two groups up to a pre-specified time point. This approach, restricted mean time lost (RMTL), can be used in settings with competing risks as well. We extend RMTL estimation for use in an understudied health policy application in Medicare. Medicare currently supports healthcare payment for over 69 million beneficiaries, most of whom are enrolled in Medicare Advantage plans and receive insurance from private insurers. These insurers are prospectively paid by the federal government for each of their beneficiaries' anticipated health needs using an ordinary least squares linear regression algorithm. As all coefficients are positive and predictor variables are largely insurer-submitted health conditions, insurers are incentivized to upcode, or report more diagnoses than may be accurate. Such gaming is projected to cost the federal government $40 billion in 2025 alone without clear benefit to beneficiaries. We propose several novel estimators of coding intensity and possible upcoding in Medicare Advantage, including accounting for unreliable reporting. We demonstrate estimator performance in simulated data leveraging the National Institutes of Health's All of Us study and also develop an open source R package to simulate realistic labeled upcoding data, which were not previously available.

</details>


### [3] [mmcmcBayes:An R Package Implementing a Multistage MCMC Framework for Detecting the Differentially Methylated Regions](https://arxiv.org/abs/2602.04554)
*Zhexuan Yang,Duchwan Ryu,Feng Luan*

Main category: stat.AP

TL;DR: mmcmcBayes是一个R包，通过多阶段马尔可夫链蒙特卡洛方法检测差异甲基化区域，使用α-偏斜广义正态分布建模区域甲基化摘要，并通过贝叶斯因子评估组间差异证据。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过聚合CpG位点水平的测试结果来检测差异甲基化区域，这可能限制了捕捉复杂区域甲基化模式的能力。需要一种更有效的区域水平检测方法。

Method: 采用多阶段马尔可夫链蒙特卡洛程序，使用α-偏斜广义正态分布对样本区域甲基化摘要进行建模，通过贝叶斯因子评估组间差异甲基化证据，并使用多阶段区域分割策略基于统计证据细化候选区域。

Result: 通过模拟研究和Illumina 450K甲基化数据应用展示了该方法的性能。mmcmcBayes包提供了实用的区域水平替代方案，并包含用于总结、比较和可视化检测区域的辅助函数。

Conclusion: mmcmcBayes为现有基于CpG的差异甲基化区域检测方法提供了实用的区域水平替代方案，能够更好地捕捉复杂的区域甲基化模式。

Abstract: Identifying differentially methylated regions is an important task in epigenome-wide association studies, where differential signals often arise across groups of neighboring CpG sites. Many existing methods detect differentially methylated regions by aggregating CpG-level test results, which may limit their ability to capture complex regional methylation patterns. In this paper, we introduce the R package mmcmcBayes, which implements a multistage Markov chain Monte Carlo procedure for region-level detection of differentially methylated regions. The method models sample-wise regional methylation summaries using the alpha-skew generalized normal distribution and evaluates evidence for differential methylation between groups through Bayes factors. We use a multistage region-splitting strategy to refine candidate regions based on statistical evidence. We describe the underlying methodology and software implementation, and illustrate its performance through simulation studies and applications to Illumina 450K methylation data. The mmcmcBayes package provides a practical region-level alternative to existing CpG-based differentially methylated regions detection methods and includes supporting functions for summarizing, comparing, and visualizing detected regions.

</details>


### [4] [Inference for Within- and Between-Partnership Transmission Rates for HIV Infection](https://arxiv.org/abs/2602.04638)
*Irene García Muñoz,Ian Hall,Thomas House*

Main category: stat.AP

TL;DR: 该研究提出了一个随机SI配对模型，用于估计HIV在血清不一致伴侣中的传播参数，考虑了性别差异，使用基于似然的推断方法从观测数据中估计传播参数及其不确定性。


<details>
  <summary>Details</summary>
Motivation: HIV在血清不一致伴侣中的传播仍然是重要的公共卫生挑战，特别是在撒哈拉以南非洲地区。需要准确估计伴侣内传播率和伴侣外引入感染率，以制定有效的预防策略。

Method: 开发了一个随机易感-感染(SI)配对模型，用于估计HIV在伴侣内和伴侣间的传播参数。模型扩展以考虑性别特异性感染动态，采用基于似然的推断方法从观测数据中估计参数及其不确定性。

Result: 成功估计了HIV传播的关键流行病学参数及其不确定性。这些参数可用于指导HIV感染预防策略，并且所提出的方法可以推广到其他流行病学场景。

Conclusion: 该研究提供了一个有效的统计框架来估计HIV在血清不一致伴侣中的传播动态，考虑了性别差异，为制定针对性预防策略提供了科学依据，方法具有广泛适用性。

Abstract: HIV transmission within serodiscordant couples remains a significant public health challenge, particularly in sub-Saharan Africa. Estimating the rate of such infection, alongside the rates of introduction of infection from outside the partnership, is a special case of the more general epidemiological challenge of inferring intensities of within- and between-group intensities of transmission. This study presents a stochastic susceptible-infected (SI) pair model for estimating key epidemiological parameters governing HIV transmission within and between couples, which we further extend to account for gender-specific differences in infection dynamics. Using a likelihood-based inference approach, we estimate transmission parameters and associated uncertainty from observed data. These values can be used to inform infection prevention strategies for HIV, and the methodology proposed can be generalised to other epidemiological settings.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [5] [Bures-Wasserstein Importance-Weighted Evidence Lower Bound: Exposition and Applications](https://arxiv.org/abs/2602.04272)
*Peiwen Jiang,Takuo Matsubara,Minh-Ngoc Tran*

Main category: stat.CO

TL;DR: 该论文提出在Bures-Wasserstein空间中优化重要性加权证据下界(IW-ELBO)，通过Wasserstein梯度解决了传统欧几里得空间中梯度估计器信噪比消失的问题，显著提升了高斯变分推断的性能。


<details>
  <summary>Details</summary>
Motivation: 重要性加权证据下界(IW-ELBO)虽然能收紧标准ELBO并缓解模式寻求行为，但在欧几里得空间中的优化效率低下，其梯度估计器的信噪比会消失。需要一种更稳定的优化方法。

Method: 将IW-ELBO优化问题表述在Bures-Wasserstein空间中，这是一个配备2-Wasserstein度量的高斯分布流形。推导IW-ELBO的Wasserstein梯度，并将其投影到Bures-Wasserstein空间，得到高斯变分推断的可行算法。

Result: 证明了Wasserstein梯度估计器的信噪比以Ω(√K)的有利方式缩放，即使对于大的重要性样本数K也能保持优化效率。实验表明该方法在近似性能上优于其他基线方法。

Conclusion: 在Bures-Wasserstein空间中优化IW-ELBO提供了一种稳定高效的变分推断方法，解决了传统欧几里得空间中梯度估计器信噪比消失的问题，为高斯变分推断提供了理论保证和实际优势。

Abstract: The Importance-Weighted Evidence Lower Bound (IW-ELBO) has emerged as an effective objective for variational inference (VI), tightening the standard ELBO and mitigating the mode-seeking behaviour.
  However, optimizing the IW-ELBO in Euclidean space is often inefficient, as its gradient estimators suffer from a vanishing signal-to-noise ratio (SNR). This paper formulates the optimisation of the IW-ELBO in Bures-Wasserstein space, a manifold of Gaussian distributions equipped with the 2-Wasserstein metric. We derive the Wasserstein gradient of the IW-ELBO and project it onto the Bures-Wasserstein space to yield a tractable algorithm for Gaussian VI.
  A pivotal contribution of our analysis concerns the stability of the gradient estimator. While the SNR of the standard Euclidean gradient estimator is known to vanish as the number of importance samples $K$ increases, we prove that the SNR of the Wasserstein gradient scales favourably as $Ω(\sqrt{K})$, ensuring optimisation efficiency even for large $K$. We further extend this geometric analysis to the Variational Rényi Importance-Weighted Autoencoder bound, establishing analogous stability guarantees. Experiments demonstrate that the proposed framework achieves superior approximation performance compared to other baselines.

</details>


### [6] [LID Framework: A new method for geospatial and exploratory data analysis of potential innovation deter-minants at the neighborhood level](https://arxiv.org/abs/2602.04679)
*Eleni Oikonomaki,Belivanis Dimitris,Kakderi Christina*

Main category: stat.CO

TL;DR: 开发本地创新决定因素（LID）数据库和框架，结合传统政府数据和API公开数据，在邻里尺度分析创新影响因素，发现替代数据源对理解创新动态有重要潜力。


<details>
  <summary>Details</summary>
Motivation: 现有创新地理研究多关注国家和区域尺度，城市和次区域尺度研究较少，且通常依赖有限指标（如企业数据、专利等），缺乏整合城市形态、流动性、设施和人力资本代理的系统框架。

Method: 开发LID数据库和框架，结合传统政府数据和API公开数据，使用探索性大数据和地理空间数据分析以及随机森林模型，分析纽约和马萨诸塞州邻里在四个维度：社会因素、经济特征、土地利用与流动性、形态与环境。

Result: 替代数据源提供了显著但尚未充分开发的潜力，能增强对创新动态的理解。城市政策制定者在设计和实施本地创新战略时应考虑邻里特定的决定因素和特征。

Conclusion: 需要在更精细的空间尺度上研究创新，结合多种数据源和维度，为城市政策制定提供更全面的创新影响因素分析框架。

Abstract: The geography of innovation offers a framework to understand how territorial characteristics shape innovation, often via spatial and cognitive proximity. Empirical research has focused largely on national and regional scales, while urban and sub-regional geographies receive less attention. Local studies typically rely on limited indicators (e.g., firm-level data, patents, basic socioeconomic measures), with few offering a systematic framework integrating urban form, mobility, amenities, and human-capital proxies at the neighborhood scale. Our study investigates innovation at a finer spatial resolution, going beyond proprietary or static indicators. We develop the Local Innovation Determinants (LID) database and framework to identify key enabling factors across regions, combining traditional government data with publicly available data via APIs for a more granular understanding of spatial dynamics shaping innovation capacity. Using exploratory big and geospatial data analytics and random forest models, we examine neighborhoods in New York and Massachusetts across four dimensions: social factors, economic characteristics, land use and mobility, morphology, and environment. Results show that alternative data sources offer significant yet underexplored potential to enhance insights into innovation dynamics. City policymakers should consider neighborhood-specific determinants and characteristics when designing and implementing local innovation strategies.

</details>


### [7] [Multiple Imputation Methods under Extreme Values](https://arxiv.org/abs/2602.04751)
*Enzo Porto Brasil*

Main category: stat.CO

TL;DR: 该研究评估了多种多重插补方法在有无极端值情况下的表现，发现基于线性回归的插补方法整体预测性能最佳，而稀疏模型方法效率较低。


<details>
  <summary>Details</summary>
Motivation: 实证数据库中普遍存在缺失数据，但统计分析通常需要完整数据矩阵。多重插补为解决这一问题提供了原则性方案，但不同方法在不同条件下的表现需要评估。

Method: 使用R语言中的MICE包，通过蒙特卡洛模拟生成包含三个变量的不完整数据集，在回归模型中评估各种多重插补方法的表现。

Result: 基于线性回归的插补方法显示出最佳的整体预测性能（CV-MSE），而稀疏模型方法通常效率较低。极端值的存在、样本大小、缺失比例、极端值出现以及拟合模型类型是影响性能的关键因素。

Conclusion: 研究强调了在选择插补策略时考虑极端值的重要性，并为研究者提供了实用建议：在选择插补方法前，需要检查缺失机制和极端值的出现情况。

Abstract: Missing data are ubiquitous in empirical databases, yet statistical analyses typically require complete data matrices. Multiple imputation offers a principled solution for filling these gaps. This study evaluates the performance of several multiple imputation methods, both in the presence and absence of extreme values, using the MICE package in R. Through Monte Carlo simulations, we generated incomplete data sets with three variables and assessed each imputation method within regression models. The results indicate that the linear regression based imputation method showed the best overall predictive performance (CV-MSE), whereas the sparse model approach was generally less efficient. Our findings underscore the relevance of extreme values when selecting an imputation strategy and highlight sample size, proportion of missingness, presence of extremes, and the type of fitted model as key determinants of performance. Despite its limitations, the study offers practical recommendations for researchers, stressing the need to examine the missingness mechanism and the occurrence of extreme values before choosing an imputation method.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [8] [Doubly-Robust Bayesian Estimation of Optimal Individualized Treatment Rules using Network Meta-Analysis](https://arxiv.org/abs/2602.03985)
*Augustine Wigle,Erica E. M. Moodie*

Main category: stat.ME

TL;DR: 提出BBdWOLS方法改进网络荟萃分析中的个体化治疗规则估计，解决模型误设和缺失数据问题，应用于抑郁症治疗决策


<details>
  <summary>Details</summary>
Motivation: 抑郁症治疗中，现有两阶段ITR-NMA方法存在模型误设脆弱性和无法处理缺失数据的问题，需要更稳健的方法来支持个性化治疗决策

Method: 提出贝叶斯自助动态加权最小二乘法（BBdWOLS），这是一个双重稳健的ITR估计方法，能处理随机缺失结果并量化估计不确定性；同时改进NMA模型以纳入研究特定估计的完整方差-协方差矩阵

Result: 模拟研究表明，完全贝叶斯ITR-NMA方法比现有方法更稳健和高效；应用于三个抑郁症药物治疗研究数据集，展示了如何支持个性化决策

Conclusion: BBdWOLS方法解决了现有ITR-NMA的局限性，为抑郁症等疾病的个性化治疗决策提供了更可靠的工具，能够处理缺失数据并量化不确定性

Abstract: An optimal individualized treatment rule (ITR) is a function that takes a patient's characteristics, such as demographics, biomarkers, and treatment history, and outputs a treatment that is expected to give the best outcome for that patient. Major Depressive Disorder (MDD) is a common and disabling mental health condition for which an optimal ITR is of interest. Unfortunately, the power to detect treatment-covariate interactions in individual studies of MDD treatments is low. Additionally, all treatments of interest are not compared head-to-head in a single study. Network meta-analysis (NMA) is a method of synthesizing data from multiple studies to estimate the relative effects of a set of treatments. Recently, two-stage ITR NMA was proposed as a method to estimate ITRs that has the potential to improve power and simultaneously consider all relevant treatment options. In the first stage, study-specific ITRs are estimated, and in the second stage, they are pooled using a Bayesian NMA model. The existing approach is vulnerable to model misspecification and fails to address missing outcomes, which occur in the MDD data. We overcome these challenges by proposing Bayesian Bootstrap dynamic Weighted Ordinary Least Squares (BBdWOLS), a doubly-robust approach to ITR estimation that accounts for missing at random outcomes and naturally quantifies the uncertainty in estimation. We also propose an improvement to the NMA model that incorporates the full variance-covariance matrix of study-specific estimates. In a simulation study, we show that our fully Bayesian ITR NMA method is more robust and efficient than the existing approach. We apply our method to the motivating dataset consisting of three studies of pharmacological treatments for MDD, and explore how ITR NMA results can support personalized decision making in this context.

</details>


### [9] [Sparse group principal component analysis via double thresholding with application to multi-cellular programs](https://arxiv.org/abs/2602.04178)
*Qi Xu,Jing Lei,Kathryn Roeder*

Main category: stat.ME

TL;DR: 提出SGPCA方法，通过双重阈值算法高效估计多细胞程序，具有线性计算复杂度，在模拟和狼疮研究中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有估计多细胞程序的方法（如稀疏主成分分析或潜在因子模型）存在计算成本高、统计功效有限的问题，需要更高效、可扩展的解决方案。

Method: 提出稀疏组主成分分析（SGPCA），利用多细胞程序的固有组稀疏性和个体稀疏性，设计基于幂迭代的高效双重阈值算法，包括组阈值步骤和个体阈值步骤。

Result: SGPCA具有O(np)的线性计算复杂度，理论保证包括统计一致性和优于竞争方法的收敛速度，在模拟中表现出更优的估计精度和信号检测统计功效，在狼疮研究中发现了区分患者与正常受试者的差异表达多细胞程序。

Conclusion: SGPCA是一种高效、可扩展的多细胞程序估计方法，具有理论保证和实际应用价值，特别适合大规模基因组分析。

Abstract: Multi-cellular programs (MCPs) are coordinated patterns of gene expression across interacting cell types that collectively drive complex biological processes such as tissue development and immune responses. While MCPs are typically estimated from high-dimensional gene expression data using methods like sparse principal component analysis or latent factor models, these approaches often suffer from high computational costs and limited statistical power. In this work, we propose Sparse Group Principal Component Analysis (SGPCA) to estimate MCPs by leveraging their inherent group and individual sparsity. We introduce an efficient double-thresholding algorithm based on power iteration. In each iteration, a group thresholding step first identifies relevant gene groups, followed by an individual thresholding step to select active cell types. This algorithm achieves a linear computational complexity of $O(np)$, making it highly efficient and scalable for large-scale genomic analyses. We establish theoretical guarantees for SGPCA, including statistical consistency and a convergence rate that surpasses competing methods. Through extensive simulations, we demonstrate that SGPCA achieves superior estimation accuracy and improved statistical power for signal detection. Furthermore, We apply SGPCA to a Lupus study, discovering differentially expressed MCPs distinguishing Lupus patients from normal subjects.

</details>


### [10] [Robust Nonparametric Two-Sample Tests via Mutual Information using Extended Bregman Divergence](https://arxiv.org/abs/2602.04010)
*Arijit Pyne*

Main category: stat.ME

TL;DR: 提出基于扩展Bregman散度的广义互信息框架，构建稳健的非参数两样本检验，证明渐近正态性，研究鲁棒性，并通过模拟和实际数据验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于统计距离的最小散度估计器在参数推断中已证明能平衡鲁棒性和渐近效率，但基于此类距离的非参数检验研究相对较少。需要开发既一致又稳健的非参数两样本检验方法。

Method: 引入基于扩展Bregman散度的广义互信息框架，该框架包含广义S-Bregman散度族，统一了S-散度和Bregman指数散度。利用广义互信息构建非参数两样本检验，建立检验统计量在零假设和局部备择下的渐近正态性，通过影响函数和崩溃点研究鲁棒性，并提出数据驱动的调优参数选择方案。

Result: 理论证明：检验统计量具有渐近正态性；鲁棒性分析：广义互信息的稳定性转化为检验的稳定性；模拟研究：超越幂散度族的散度在污染下具有更优的鲁棒性，同时保持高渐近功效；实际应用：方法在真实数据中有效。

Conclusion: 提出的广义互信息框架为构建稳健的非参数两样本检验提供了统一方法，扩展了S-Bregman散度族在非参数检验中的应用，在保持统计功效的同时显著提升了鲁棒性，具有实际应用价值。

Abstract: We introduce a generalized formulation of mutual information (MI) based on the extended Bregman divergence, a framework that subsumes the generalized S-Bregman (GSB) divergence family. The GSB divergence unifies two important classes of statistical distances, namely the S-divergence and the Bregman exponential divergence (BED), thereby encompassing several widely used subfamilies, including the power divergence (PD), density power divergence (DPD), and S-Hellinger distance (S-HD). In parametric inference, minimum divergence estimators are well known to balance robustness with high asymptotic efficiency relative to the maximum likelihood estimator. However, nonparametric tests based on such statistical distances have been relatively less explored. In this paper, we construct a class of consistent and robust nonparametric two-sample tests for the equality of two absolutely continuous distributions using the generalized MI. We establish the asymptotic normality of the proposed test statistics under the null and contiguous alternatives. The robustness properties of the generalized MI are rigorously studied through the influence function and the breakdown point, demonstrating that stability of the generalized MI translates into stability of the associated tests. Extensive simulation studies show that divergences beyond the PD family often yield superior robustness under contamination while retaining high asymptotic power. A data-driven scheme for selecting optimal tuning parameters is also proposed. Finally, the methodology is illustrated with applications to real data.

</details>


### [11] [Species Sensitivity Distribution revisited: a Bayesian nonparametric approach](https://arxiv.org/abs/2602.04788)
*Louise Alamichel,Julyan Arbel,Guillaume Kon Kam King,Igor Prünster*

Main category: stat.ME

TL;DR: 提出将物种敏感度分布方法置于贝叶斯非参数框架中，解决传统参数假设限制，提供更好的小数据集处理、不确定性量化和聚类分析能力。


<details>
  <summary>Details</summary>
Motivation: 传统SSD方法依赖参数假设建模物种变异性，受到批评。生态风险评估中常遇到小数据集或截尾数据，需要更稳健的统计基础。

Method: 采用贝叶斯非参数混合模型，使用具有稳健聚类特性的非参数先验作为混合测度，建立BNP-SSD框架。

Result: 通过模拟研究和真实数据分析，证明BNP-SSD优于经典SSD方法，提供BNP-SSD Shiny应用，并利用混合模型的聚类结构探索物种敏感度模式。

Conclusion: 所提出的贝叶斯非参数方法能有效改进生态风险评估方法学，为生态毒理学界提供更强大的分析工具。

Abstract: We present a novel approach to ecological risk assessment by recasting the Species Sensitivity Distribution (SSD) method within a Bayesian nonparametric (BNP) framework. Widely mandated by environmental regulatory bodies globally, SSD has faced criticism due to its historical reliance on parametric assumptions when modeling species variability. By adopting nonparametric mixture models, we address this limitation, establishing a statistically robust foundation for SSD. Our BNP approach offers several advantages, including its efficacy in handling small datasets or censored data, which are common in ecological risk assessment, and its ability to provide principled uncertainty quantification alongside simultaneous density estimation and clustering. We utilize a specific nonparametric prior as the mixing measure, chosen for its robust clustering properties, a crucial consideration given the lack of strong prior beliefs about the number of components. Through simulation studies and analysis of real datasets, we demonstrate the superiority of our BNP-SSD over classical SSD methods. We also provide a BNP-SSD Shiny application, making our methodology available to the Ecotoxicology community. Moreover, we exploit the inherent clustering structure of the mixture model to explore patterns in species sensitivity. Our findings underscore the effectiveness of the proposed approach in improving ecological risk assessment methodologies.

</details>


### [12] [Privacy Amplification for Synthetic data using Range Restriction](https://arxiv.org/abs/2602.04124)
*Monika Hu,Matthew R. Williams,Terrance D. Savitsky*

Main category: stat.ME

TL;DR: 论文提出了一种新的范围限制形式化数据隐私标准，通过纳入数据所有者对敏感数据范围的先验信念，提供更强的隐私保障（如隐私放大）。该标准仅保护敏感值子集，排除已公开的范围，适用于风险加权伪后验机制生成合成数据。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私标准对所有数据提供统一保护，但实际中数据所有者可能对敏感数据范围有先验信念。通过纳入这些信念信息，可以针对性地保护真正敏感的数据范围，排除已公开信息，从而提供更强的隐私保障。

Method: 提出两种调整风险加权伪似然的方法：1）用概率λ表示数据值落在敏感范围外的信念，只有(1-λ)部分被视为敏感并接受风险降权；2）用概率质量差P(R)编码敏感范围边缘的知识。将调整后的条件伪似然应用于敏感记录，提升其最坏情况尾部值。

Result: 该方法在渐进差分隐私保证下，通过范围限制隐私标准增强了隐私保护强度（实现隐私放大），同时保持了数据效用。比较了PPM在aDP和范围限制隐私标准下的隐私与效用特性。

Conclusion: 通过纳入数据所有者对敏感范围的信念，范围限制形式化隐私标准能够提供比传统差分隐私更强的隐私保障，同时保持数据效用，为合成数据生成提供了更精细的隐私保护框架。

Abstract: We introduce a new class of range restricted formal data privacy standards that condition on owner beliefs about sensitive data ranges. By incorporating this additional information, we can provide a stronger privacy guarantee (e.g. an amplification). The range restricted formal privacy standards protect only a subset (or ball) of data values and exclude ranges (or balls) believed to be already publicly known. The privacy standards are designed for the risk-weighted pseudo posterior (model) mechanism (PPM) used to generate synthetic data under an asymptotic Differential (aDP) privacy guarantee. The PPM downweights the likelihood contribution for each record proportionally to its disclosure risk. The PPM is adapted under inclusion of beliefs by adjusting the risk-weighted pseudo likelihood. We introduce two alternative adjustments. The first expresses data owner knowledge of the sensitive range as a probability, $λ$, that a datum value drawn from the underlying generating distribution lies outside the ball or subspace of values that are sensitive. The portion of each datum likelihood contribution deemed sensitive is then $(1-λ) \leq 1$ and is the only portion of the likelihood subject to risk down-weighting. The second adjustment encodes knowledge as the difference in probability masses $P(R) \leq 1$ between the edges of the sensitive range, $R$. We use the resulting conditional (pseudo) likelihood for a sensitive record, which boosts its worst case tail values away from 0. We compare privacy and utility properties for the PPM under the aDP and range restricted privacy standards.

</details>


### [13] [Accurate and Efficient Approximation of the Null Distribution of Rao's Spacing Test](https://arxiv.org/abs/2602.04318)
*Yoshiki Kinoshita,Aya Shinozaki,Toshinari Kamakura*

Main category: stat.ME

TL;DR: 提出基于高阶矩递归计算和Gram-Charlier展开的方法，近似Rao间距检验的零分布，实现任意样本量下p值的高效计算，摆脱传统临界值表的限制。


<details>
  <summary>Details</summary>
Motivation: Rao间距检验作为广泛使用的圆形均匀性非参数检验方法，在实际应用中受到限制，因为其零分布难以计算。传统方法依赖有限样本量的预计算临界值表，限制了方法的灵活性和通用性。

Method: 通过递归计算Rao间距检验统计量的高阶矩，并采用Gram-Charlier展开来推导其零分布的精确近似。这种方法能够高效直接地计算任意样本量的p值。

Result: 该方法能够准确计算任意样本量的p值，即使在当前表中未包含的大样本量情况下也保持准确有效。与已发表临界值和鞍点近似的比较评估表明，该方法在广泛的样本量范围内实现了高度准确性。

Conclusion: 该方法显著提高了Rao间距检验在理论研究和应用统计分析中的实用性和可用性，消除了对现有临界值表的依赖，为更灵活的应用提供了可能。

Abstract: Rao's spacing test is a widely used nonparametric method for assessing uniformity on the circle. However, its broader applicability in practical settings has been limited because the null distribution is not easily calculated. As a result, practitioners have traditionally depended on pre-tabulated critical values computed for a limited set of sample sizes, which restricts the flexibility and generality of the method. In this paper, we address this limitation by recursively computing higher-order moments of the Rao's spacing test statistic and employing the Gram-Charlier expansion to derive an accurate approximation to its null distribution. This approach allows for the efficient and direct computation of p-values for arbitrary sample sizes, thereby eliminating the dependency on existing critical value tables. Moreover, we confirm that our method remains accurate and effective even for large sample sizes that are not represented in current tables, thus overcoming a significant practical limitation. Comparative evaluations with published critical values and saddlepoint approximations demonstrate that our method achieves a high degree of accuracy across a wide range of sample sizes. These findings greatly improve the practicality and usability of Rao's spacing test in both theoretical investigations and applied statistical analyses.

</details>


### [14] [Validating Causal Message Passing Against Network-Aware Methods on Real Experiments](https://arxiv.org/abs/2602.04230)
*Albert Tan,Sadegh Shirani,James Nordlund,Mohsen Bayati*

Main category: stat.ME

TL;DR: 因果消息传递方法利用结果数据的时间结构而非网络拓扑来估计网络干扰下的总处理效应，在缺乏网络数据时能获得与网络感知方法相当的效果估计。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，网络数据常常不可得、不完整或存在严重测量误差，这限制了传统需要网络拓扑信息的方法的应用。需要一种不依赖网络观测就能估计溢出效应的方法。

Method: 提出因果消息传递方法，该方法不依赖网络拓扑知识，而是利用结果数据的时间结构来估计总处理效应。通过两个大规模现场实验验证，与需要网络知识的二分图方法进行对比。

Result: 因果消息传递在没有访问交互网络的情况下，产生的效应估计在所有指标方向上与网络感知方法一致，在主要决策指标的统计显著性上也匹配。效果估计与网络感知方法相当。

Conclusion: 结果验证了因果消息传递的前提：在估计溢出效应时，结果的时间变化可以有效地替代网络观测。这对实践有重要启示：当网络数据收集成本高、专有或不可靠时，可以利用实验数据的时间动态。

Abstract: Estimating total treatment effects in the presence of network interference typically requires knowledge of the underlying interaction structure. However, in many practical settings, network data is either unavailable, incomplete, or measured with substantial error. We demonstrate that causal message passing, a methodology that leverages temporal structure in outcome data rather than network topology, can recover total treatment effects comparable to network-aware approaches. We apply causal message passing to two large-scale field experiments where a recently developed bipartite graph methodology, which requires network knowledge, serves as a benchmark. Despite having no access to the interaction network, causal message passing produces effect estimates that match the network-aware approach in direction across all metrics and in statistical significance for the primary decision metric. Our findings validate the premise of causal message passing: that temporal variation in outcomes can serve as an effective substitute for network observation when estimating spillover effects. This has important practical implications: practitioners facing settings where network data is costly to collect, proprietary, or unreliable can instead exploit the temporal dynamics of their experimental data.

</details>


### [15] [Unit Shiha Distribution and its Applications to Engineering and Medical Data](https://arxiv.org/abs/2602.04400)
*F. A. Shiha*

Main category: stat.ME

TL;DR: 本文提出了一种新的单位区间分布——单位Shiha(USh)分布，通过逆指数变换从原始Shiha分布推导而来，能够灵活建模单位区间数据，在四个实际数据集上表现优于现有竞争模型。


<details>
  <summary>Details</summary>
Motivation: 随着对单位区间数据建模需求的增长，需要更灵活的统计分布来准确描述这类数据。现有单位分布可能无法充分捕捉数据的各种偏斜形态和失效率模式。

Method: 通过逆指数变换从原始Shiha分布推导出单位Shiha(USh)分布，研究其概率密度函数和风险率函数的特性，使用最大似然法进行参数估计，并通过模拟研究评估性能。

Result: USh分布的概率密度函数能够灵活建模左偏和右偏数据，风险率函数能够捕捉递增、浴盆形和J形等多种失效率模式。在四个实际数据集上的比较表明，USh分布比现有竞争模型拟合效果更好。

Conclusion: 提出的单位Shiha分布是一个灵活的单位区间分布模型，能够有效建模各种偏斜形态和失效率模式，在实际应用中表现出优于现有竞争模型的拟合性能。

Abstract: There is a growing need for flexible statistical distributions that can accurately model data defined on the unit interval. This paper introduces a new unit distribution, termed the unit Shiha (USh) distribution, which is derived from the original Shiha (Sh) distribution through an inverse exponential transformation. The probability density function of the USh distribution is sufficiently flexible to model both left- and right-skewed data, while its hazard rate function is capable of capturing various failure-rate patterns, including increasing, bathtub-shaped, and J-shaped forms. Several statistical properties of the proposed distribution are investigated, including moments and related measures, the quantile function, entropy, and stress-strength reliability. Parameter estimation is carried out using the maximum likelihood method, and its performance is evaluated through a simulation study. The practical usefulness of the USh distribution is demonstrated using four real-life data sets, and its performance is compared with several well-known competing unit distributions. The comparative results indicate that the proposed model fits the data better than the competitive models applied in this study.

</details>


### [16] [Exact Multiple Change-Point Detection Via Smallest Valid Partitioning](https://arxiv.org/abs/2602.04322)
*Vincent Runge,Anica Kostic,Alexandre Combeau,Gaetano Romano*

Main category: stat.ME

TL;DR: SVP是一种基于局部有效性检验的时间序列多变化点检测方法，通过词汇序优化优先考虑简洁性，计算复杂度从线性到立方不等，在保持竞争力的同时显式确保分段有效性。


<details>
  <summary>Details</summary>
Motivation: 现有变化点检测方法可能缺乏对分段有效性的显式保证，需要一种既能检测多个变化点又能确保每个检测到的分段都通过统计有效性检验的方法。

Method: SVP基于局部有效性概念：候选分段仅当通过用户选择的有效性检验（如单变化点检验）时才被保留。通过有效分段的集合，采用词汇序优化问题的聚合程序构建全局分割，优先考虑简洁性。

Result: SVP的计算复杂度从线性到立方时间不等，具体取决于成本函数、有效性函数、数据机制和检测到的变化点数量。与标准最优分割算法相比，SVP在保持竞争力的同时显式确保分段有效性。

Conclusion: SVP提供了一种灵活的多变化点检测框架，通过局部有效性检验确保分段质量，词汇序优化优先简洁性，适用于广泛问题，如通过有效性准则编码实现鲁棒变化点检测。

Abstract: We introduce smallest valid partitioning (SVP), a segmentation method for multiple change-point detection in time-series. SVP relies on a local notion of segment validity: a candidate segment is retained only if it passes a user-chosen validity test (e.g., a single change-point test). From the collection of valid segments, we propose a coherent aggregation procedure that constructs a global segmentation which is the exact solution of an optimization problem. Our main contribution is the use of a lexicographic order for the optimization problem that prioritizes parsimony. We analyze the computational complexity of the resulting procedure, which ranges from linear to cubic time depending on the chosen cost and validity functions, the data regime and the number of detected changes. Finally, we assess the quality of SVP through comparisons with standard optimal partitioning algorithms, showing that SVP yields competitive segmentations while explicitly enforcing segment validity. The flexibility of SVP makes it applicable to a broad class of problems; as an illustration, we demonstrate robust change-point detection by encoding robustness in the validity criterion.

</details>


### [17] [Journey to the Centre of Cluster: Harnessing Interior Nodes for A/B Testing under Network Interference](https://arxiv.org/abs/2602.04457)
*Qianyi Chen,Anpeng Wu,Bo Li,Lu Deng,Yong Wang*

Main category: stat.ME

TL;DR: 提出MII估计器，通过直接平均内部节点来减少方差，再通过预测器调整协变量偏移，在存在网络干扰的A/B测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: A/B测试中的网络干扰问题：个体的结果不仅受自身处理影响，还受网络邻居处理的影响。现有的网络感知估计器虽然能降低偏差，但方差很高，且需要复杂的重新加权。

Method: 1. 提出MII估计器：直接平均内部节点（邻居都在同一集群内的单元），避免复杂加权，降低方差。2. 增强MII估计器：使用在整个网络上训练的反事实预测器，调整内部节点与总体之间的协变量分布偏移。3. 将增强MII估计器重新表述为预测驱动推断框架中的点估计器，采用半监督视角。

Result: 模拟研究表明，增强MII估计器在各种设置下都表现出色，显著降低了方差，同时通过协变量调整减少了偏差。

Conclusion: 增强MII估计器通过直接利用内部节点并调整协变量偏移，在存在网络干扰的A/B测试中实现了低方差和低偏差的平衡，为网络干扰问题提供了有效的解决方案。

Abstract: A/B testing on platforms often faces challenges from network interference, where a unit's outcome depends not only on its own treatment but also on the treatments of its network neighbors. To address this, cluster-level randomization has become standard, enabling the use of network-aware estimators. These estimators typically trim the data to retain only a subset of informative units, achieving low bias under suitable conditions but often suffering from high variance. In this paper, we first demonstrate that the interior nodes - units whose neighbors all lie within the same cluster - constitute the vast majority of the post-trimming subpopulation. In light of this, we propose directly averaging over the interior nodes to construct the mean-in-interior (MII) estimator, which circumvents the delicate reweighting required by existing network-aware estimators and substantially reduces variance in classical settings. However, we show that interior nodes are often not representative of the full population, particularly in terms of network-dependent covariates, leading to notable bias. We then augment the MII estimator with a counterfactual predictor trained on the entire network, allowing us to adjust for covariate distribution shifts between the interior nodes and full population. By rearranging the expression, we reveal that our augmented MII estimator embodies an analytical form of the point estimator within prediction-powered inference framework. This insight motivates a semi-supervised lens, wherein interior nodes are treated as labeled data subject to selection bias. Extensive and challenging simulation studies demonstrate the outstanding performance of our augmented MII estimator across various settings.

</details>


### [18] [Distributed Convoluted Rank Regression for Non-Shareable Data under Non-Additive Losses](https://arxiv.org/abs/2602.04594)
*Wen Zhang,Liping Zhu,Songshan Yang*

Main category: stat.ME

TL;DR: 提出分布式卷积秩回归(DCRR)框架，解决多机环境下非可加U统计量损失函数的分布式学习问题，在保持集中式估计器统计等价性的同时，实现高效通信。


<details>
  <summary>Details</summary>
Motivation: 传统通信高效的替代似然方法依赖于经验损失的可加性，但在卷积秩回归(CRR)中失效，因为其全局损失耦合了所有机器上的样本对。需要解决非可加损失下的分布式学习问题。

Method: 提出DCRR框架，构建与全数据CRR损失具有相同总体最小化器的替代损失。开发两阶段稀疏DCRR程序：迭代ℓ1惩罚阶段+折叠凹细化阶段，并建立DHBIC型准则进行模型选择。

Result: 证明替代损失与集中式CRR统计等价，建立非渐近误差界、分布式强oracle性质。机器数量可发散至M=o(N/(s²log p))，仅需O(log N)通信轮次即可达到集中式oracle速率。

Conclusion: DCRR框架有效解决了非可加损失下的分布式学习问题，在重尾误差下相比朴素分治方法有显著优势，实现了通信效率与统计性能的良好平衡。

Abstract: We study high-dimensional rank regression when data are distributed across multiple machines and the loss is a non-additive U-statistic, as in convoluted rank regression (CRR). Classical communication-efficient surrogate likelihood (CSL) methods crucially rely on the additivity of the empirical loss and therefore break down for CRR, whose global loss couples all sample pairs across machines. We propose a distributed convoluted rank regression (DCRR) framework that constructs a similar surrogate loss and demonstrate its validity under the non-additive losses. We show that this surrogate shares the same population minimizer as the full-data CRR loss and yields estimators that are statistically equivalent to centralized CRR. Building on this, we develop a two-stage sparse DCRR procedure -- an iterative $\ell_1$-penalized stage followed by a folded-concave refinement -- and establish non-asymptotic error bounds, a distributed strong oracle property, and a DHBIC-type criterion for consistent model selection. A scaling result shows that the number of machines may diverge as $M = o({N/(s^2\log p)})$ while achieving centralized oracle rates with only $O(\log N)$ communication rounds. Simulations and a large-scale real data example demonstrate substantial gains over naive divide-and-conquer, particularly under heavy-tailed errors.

</details>


### [19] [Covariate Selection for Joint Latent Space Modeling of Sparse Network Data](https://arxiv.org/abs/2602.04682)
*Emma G Crenshaw,Yuhua Zhang,Jukka-Pekka Onnela*

Main category: stat.ME

TL;DR: 提出一个联合潜在空间建模框架，用于处理稀疏网络和高维二元节点协变量，通过协变量选择和考虑潜在位置估计的不确定性来改进预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏网络和高维节点特征时表现不佳，特别是无法有效利用孤立节点信息、进行协变量选择以及处理潜在位置估计的不确定性。

Method: 基于联合潜在空间模型，引入组套索筛选步骤和测量误差感知的稳定项，通过共享潜在位置耦合边和节点变量，减少使用估计潜在位置作为预测变量时的偏差。

Result: 建立了协变量分量的预测误差率，当潜在位置被视为观测值时为O(log q/n)，当估计有界误差时增加额外项。模拟显示随着协变量稀疏性增加，预测性能保持稳定，而朴素方法性能下降。

Conclusion: 该方法能有效处理稀疏网络中的三个挑战：利用孤立节点信息、高维协变量选择和潜在位置估计不确定性，在印度村庄家庭社交网络的实际应用中支持高效研究设计。

Abstract: Network data are increasingly common in the social sciences and infectious disease epidemiology. Analyses often link network structure to node-level covariates, but existing methods falter with sparse networks and high-dimensional node features. We propose a joint latent space modeling framework for sparse networks with high-dimensional binary node covariates that performs covariate selection while accounting for uncertainty in estimated latent positions. Building on joint latent space models that couple edges and node variables through shared latent positions, we introduce a group lasso screening step and incorporate a measurement-error-aware stabilization term to mitigate bias from using estimated latent positions as predictors. We establish prediction error rates for the covariate component both when latent positions are treated as observed and when they are estimated with bounded error; under uniform control across $q$ covariates and $n$ nodes, the rate is of order $O(\log q / n)$ up to an additional term due to latent position estimation error. Our method addresses three challenges: (1) incorporating information from isolated nodes, which are common in sparse networks but often ignored; (2) selecting relevant covariates from high-dimensional spaces; and (3) accounting for uncertainty in estimated latent positions. Simulations show predictive performance remains stable as covariate sparsity grows, while naive approaches degrade. We illustrate how the method can support efficient study design using household social networks from 75 Indian villages, where an emulated pilot study screens a large covariate battery and substantially reduces required subsequent data collection without sacrificing network predictive accuracy.

</details>


### [20] [Linear Regression: Inference Based on Cluster Estimates](https://arxiv.org/abs/2602.04691)
*Subhodeep Dey,Gopal K. Basak,Samarjit Das*

Main category: stat.ME

TL;DR: 提出针对聚类数据回归系数的新估计器，考虑组内相关性，研究有限和无限聚类规模下的渐近性质，扩展至随机系数模型，开发参数稳定性检验，证明传统POLS估计器不可靠。


<details>
  <summary>Details</summary>
Motivation: 聚类数据中传统估计方法（如POLS）忽略组内相关性可能导致不可靠的估计结果，需要开发能明确考虑组内依赖关系的估计方法，并建立相应的检验框架。

Method: 提出新的回归系数估计器，考虑聚类内相关性；研究有限和无限聚类规模下的渐近性质；扩展至随机系数模型，推导平均参数的渐近结果；开发Wald型检验用于一般线性假设；引入高层级参数稳定性检验。

Result: 证明传统POLS估计器在随机系数框架下不可靠；新估计器具有良好的渐近性质；模拟研究验证了所提检验的有效性；实证应用展示了方法的实际相关性。

Conclusion: 所提出的估计器和检验方法能有效处理聚类数据中的组内相关性，为随机系数模型提供了可靠的推断工具，具有重要的理论和应用价值。

Abstract: This article proposes a novel estimator for regression coefficients in clustered data that explicitly accounts for within-cluster dependence. We study the asymptotic properties of the proposed estimator under both finite and infinite cluster sizes. The analysis is then extended to a standard random coefficient model, where we derive asymptotic results for the average (common) parameters and develop a Wald-type test for general linear hypotheses. We also investigate the performance of the conventional pooled ordinary least squares (POLS) estimator within the random coefficients framework and show that it can be unreliable across a wide range of empirically relevant settings. Furthermore, we introduce a new test for parameter stability at a higher (superblock; Tier 2, Tier 3,...) level, assuming that parameters are stable across clusters within that level. Extensive simulation studies demonstrate the effectiveness of the proposed tests, and an empirical application illustrates their practical relevance.

</details>


### [21] [Score-Based Change-Point Detection and Region Localization for Spatio-Temporal Point Processes](https://arxiv.org/abs/2602.04798)
*Wenbin Zhou,Liyan Xie,Shixiang Zhu*

Main category: stat.ME

TL;DR: 提出了一种用于时空点过程的序贯变点检测框架，不仅能检测变化时间，还能定位变化发生的空间区域，无需参数化假设。


<details>
  <summary>Details</summary>
Motivation: 现有变点检测方法主要关注时间变化，无法明确推断受影响的空间区域，而实际应用中需要同时识别变化时间和空间位置。

Method: 基于无似然、评分的方法，利用局部条件加权Hyvärinen评分量化事件级偏差，通过时空CUSUM统计量在空间区域类上聚合，序贯输出停止时间和估计变化区域。

Result: 建立了误报控制、检测延迟和空间定位精度的理论保证，通过模拟和真实时空事件数据验证了方法的有效性。

Conclusion: 提出的框架实现了具有空间可解释性的实时检测，为时空点过程的变点检测提供了完整的解决方案。

Abstract: We study sequential change-point detection for spatio-temporal point processes, where actionable detection requires not only identifying when a distributional change occurs but also localizing where it manifests in space. While classical quickest change detection methods provide strong guarantees on detection delay and false-alarm rates, existing approaches for point-process data predominantly focus on temporal changes and do not explicitly infer affected spatial regions. We propose a likelihood-free, score-based detection framework that jointly estimates the change time and the change region in continuous space-time without assuming parametric knowledge of the pre- or post-change dynamics. The method leverages a localized and conditionally weighted Hyvärinen score to quantify event-level deviations from nominal behavior and aggregates these scores using a spatio-temporal CUSUM-type statistic over a prescribed class of spatial regions. Operating sequentially, the procedure outputs both a stopping time and an estimated change region, enabling real-time detection with spatial interpretability. We establish theoretical guarantees on false-alarm control, detection delay, and spatial localization accuracy, and demonstrate the effectiveness of the proposed approach through simulations and real-world spatio-temporal event data.

</details>


### [22] [Marginal Likelihood Inference for Fitting Dynamical Survival Analysis Models to Epidemic Count Data](https://arxiv.org/abs/2602.04855)
*Suchismita Roy,Alexander A. Fisher,Jason Xu*

Main category: stat.ME

TL;DR: 该论文提出了一种基于动态生存分析（DSA）的闭式似然方法，用于处理离散观测的发病率计数数据，解决了传统随机区室模型在部分观测数据下似然函数难以计算的问题。


<details>
  <summary>Details</summary>
Motivation: 随机区室模型常用于描述疾病传播，但在许多监测数据下，由于信息缺失导致边际似然函数难以处理，使得推断变得困难。

Method: 采用动态生存分析（DSA）范式，通过大种群极限近似随机种群水平风险，同时保留计数值的随机模型，开发出闭式似然函数用于离散观测的发病率计数数据。

Result: 模拟研究表明，在部分观测设置下，参数估计与最近精确但计算昂贵的基于似然的方法具有竞争力。该方法可推广到具有个体异质性的模型（如脆弱模型），并在埃博拉和COVID-19数据案例中展示了灵活性。

Conclusion: 提出的DSA方法为部分观测的流行病数据提供了计算高效且灵活的推断策略，能够处理复杂模型如网络传播模型和具有易感性分布的模型，具有实际应用价值。

Abstract: Stochastic compartmental models are prevalent tools for describing disease spread, but inference under these models is challenging for many types of surveillance data when the marginal likelihood function becomes intractable due to missing information. To address this, we develop a closed-form likelihood for discretely observed incidence count data under the dynamical survival analysis (DSA) paradigm. The method approximates the stochastic population-level hazard by a large population limit while retaining a count-valued stochastic model, and leads to survival analytic inferential strategies that are both computationally efficient and flexible to model generalizations. Through simulation, we show that parameter estimation is competitive with recent exact but computationally expensive likelihood-based methods in partially observed settings. Previous work has shown that the DSA approximation is generalizable, and we show that the inferential developments here also carry over to models featuring individual heterogeneity, such as frailty models. We consider case studies of both Ebola and COVID-19 data on variants of the model, including a network-based epidemic model and a model with distributions over susceptibility, demonstrating its flexibility and practical utility on real, partially observed datasets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [23] [Transcendental Regularization of Finite Mixtures:Theoretical Guarantees and Practical Limitations](https://arxiv.org/abs/2602.03889)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 论文提出了一种防止有限混合模型退化的正则化方法——超越正则化，通过解析屏障函数防止分量崩溃，同时保持渐近效率。


<details>
  <summary>Details</summary>
Motivation: 有限混合模型在无监督学习中广泛应用，但通过EM算法进行最大似然估计时存在退化问题，即分量会崩溃。现有方法难以同时防止退化并保持统计效率。

Method: 提出了超越正则化框架，使用惩罚似然方法，包含解析屏障函数来防止退化。开发了TAMD算法（Transcendental Algorithm for Mixtures of Distributions），具有可识别性、一致性和鲁棒性等理论保证。

Result: TAMD成功稳定了估计并防止了崩溃，但在高维无监督学习中分类准确率的提升有限，突显了混合模型在无监督学习中的基本限制。

Conclusion: 该工作提供了一个新的理论框架，并对混合模型在高维无监督学习中的实际局限性进行了诚实评估，已实现为开源R包。

Abstract: Finite mixture models are widely used for unsupervised learning, but maximum likelihood estimation via EM suffers from degeneracy as components collapse. We introduce transcendental regularization, a penalized likelihood framework with analytic barrier functions that prevent degeneracy while maintaining asymptotic efficiency. The resulting Transcendental Algorithm for Mixtures of Distributions (TAMD) offers strong theoretical guarantees: identifiability, consistency, and robustness. Empirically, TAMD successfully stabilizes estimation and prevents collapse, yet achieves only modest improvements in classification accuracy-highlighting fundamental limits of mixture models for unsupervised learning in high dimensions. Our work provides both a novel theoretical framework and an honest assessment of practical limitations, implemented in an open-source R package.

</details>


### [24] [Byzantine Machine Learning: MultiKrum and an optimal notion of robustness](https://arxiv.org/abs/2602.03899)
*Gilles Bareilles,Wassim Bouaziz,Julien Fageot,El-Mahdi El-Mhamdi*

Main category: stat.ML

TL;DR: 本文首次证明了MultiKrum聚合规则具有鲁棒性，并引入了最优鲁棒系数κ*来更精确地量化对抗环境下的均值估计准确性，改进了对Krum和MultiKrum鲁棒系数的理论分析。


<details>
  <summary>Details</summary>
Motivation: MultiKrum作为Krum的自然扩展，在实践中因其优越的实证性能而被广泛采用，但一直缺乏理论保证。本文旨在填补这一空白，为MultiKrum提供首个鲁棒性证明。

Method: 引入最优鲁棒系数κ*作为新的鲁棒性度量标准，然后构建MultiKrum鲁棒系数的上下界，同时改进了Krum鲁棒系数的已知最佳界限。

Result: 证明了MultiKrum的鲁棒性，其界限从不差于Krum，在现实场景中表现更好。通过实验验证了下界的质量。

Conclusion: 本文首次为MultiKrum提供了理论保证，证明了其鲁棒性，并引入了更精确的鲁棒系数度量方法，为分布式学习中的对抗性防御提供了更强的理论基础。

Abstract: Aggregation rules are the cornerstone of distributed (or federated) learning in the presence of adversaries, under the so-called Byzantine threat model. They are also interesting mathematical objects from the point of view of robust mean estimation. The Krum aggregation rule has been extensively studied, and endowed with formal robustness and convergence guarantees. Yet, MultiKrum, a natural extension of Krum, is often preferred in practice for its superior empirical performance, even though no theoretical guarantees were available until now. In this work, we provide the first proof that MultiKrum is a robust aggregation rule, and bound its robustness coefficient. To do so, we introduce $κ^\star$, the optimal *robustness coefficient* of an aggregation rule, which quantifies the accuracy of mean estimation in the presence of adversaries in a tighter manner compared with previously adopted notions of robustness. We then construct an upper and a lower bound on MultiKrum's robustness coefficient. As a by-product, we also improve on the best-known bounds on Krum's robustness coefficient. We show that MultiKrum's bounds are never worse than Krum's, and better in realistic regimes. We illustrate this analysis by an experimental investigation on the quality of the lower bound.

</details>


### [25] [A Hitchhiker's Guide to Poisson Gradient Estimation](https://arxiv.org/abs/2602.03896)
*Michael Ibrahim,Hanqi Zhao,Eli Sennesh,Zhi Li,Anqi Wu,Jacob L. Yates,Chengrui Li,Hadi Vafaii*

Main category: stat.ML

TL;DR: 本文系统比较了泊松潜变量模型中的两种梯度估计方法（EAT和GSM），提出改进的EAT方法保证一阶矩无偏并减少二阶矩偏差，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 泊松潜变量模型在计算神经科学中广泛应用，但通过离散随机样本进行微分仍然具有挑战性。需要系统比较现有的两种主要方法（EAT和GSM），并为实践者提供实用指导。

Method: 1. 系统比较EAT（指数到达时间）模拟和Gumbel-SoftMax松弛两种方法；2. 提出改进的EAT方法，理论上保证一阶矩无偏（精确匹配发放率）并减少二阶矩偏差；3. 在分布保真度、梯度质量和两个任务上评估方法：泊松潜变量的变分自编码器和部分可观测广义线性模型。

Result: 改进的EAT方法在所有指标上都表现出更好的整体性能（通常与精确梯度相当），并且对超参数选择具有显著更高的鲁棒性。该方法在分布保真度和梯度质量方面优于GSM方法。

Conclusion: 本文阐明了泊松潜变量模型中不同方法之间的权衡，为实践者提供了具体建议。改进的EAT方法因其优越的性能和鲁棒性成为推荐选择。

Abstract: Poisson-distributed latent variable models are widely used in computational neuroscience, but differentiating through discrete stochastic samples remains challenging. Two approaches address this: Exponential Arrival Time (EAT) simulation and Gumbel-SoftMax (GSM) relaxation. We provide the first systematic comparison of these methods, along with practical guidance for practitioners. Our main technical contribution is a modification to the EAT method that theoretically guarantees an unbiased first moment (exactly matching the firing rate), and reduces second-moment bias. We evaluate these methods on their distributional fidelity, gradient quality, and performance on two tasks: (1) variational autoencoders with Poisson latents, and (2) partially observable generalized linear models, where latent neural connectivity must be inferred from observed spike trains. Across all metrics, our modified EAT method exhibits better overall performance (often comparable to exact gradients), and substantially higher robustness to hyperparameter choices. Together, our results clarify the trade-offs between these methods and offer concrete recommendations for practitioners working with Poisson latent variable models.

</details>


### [26] [Privacy utility trade offs for parameter estimation in degree heterogeneous higher order networks](https://arxiv.org/abs/2602.03948)
*Bibhabasu Mandal,Sagnik Nandy*

Main category: stat.ML

TL;DR: 该论文研究了在关系数据集中保护个体链接信息的问题，针对仅通过节点度汇总的数据，采用β模型，在本地和中心差分隐私约束下建立了参数估计的极小极大最优性理论。


<details>
  <summary>Details</summary>
Motivation: 在涉及关系数据集的敏感应用中，保护个体链接信息免受对抗性查询至关重要。许多情况下，可用数据仅通过网络中的节点度进行汇总，需要研究如何在隐私约束下进行参数估计。

Method: 采用β模型作为聚合关系信息的原型统计模型，在本地和中心差分隐私约束下，建立有限样本极小极大下界，并提出简单估计器，在两种隐私框架下都能达到这些界（最多相差常数和对数因子）。

Result: 建立了精确刻画估计风险与网络规模和隐私参数依赖关系的有限样本极小极大下界，提出的估计器在两种隐私框架下都能达到这些界。这是β模型参数估计隐私效用权衡的第一个全面有限样本表征，涵盖经典图情况并扩展到高阶超图模型。

Conclusion: 该研究为β模型参数估计提供了完整的隐私效用权衡理论框架，在合成数据和真实世界通信网络上的实验验证了方法的有效性，填补了该领域的研究空白。

Abstract: In sensitive applications involving relational datasets, protecting information about individual links from adversarial queries is of paramount importance. In many such settings, the available data are summarized solely through the degrees of the nodes in the network. We adopt the $β$ model, which is the prototypical statistical model adopted for this form of aggregated relational information, and study the problem of minimax-optimal parameter estimation under both local and central differential privacy constraints. We establish finite sample minimax lower bounds that characterize the precise dependence of the estimation risk on the network size and the privacy parameters, and we propose simple estimators that achieve these bounds up to constants and logarithmic factors under both local and central differential privacy frameworks. Our results provide the first comprehensive finite sample characterization of privacy utility trade offs for parameter estimation in $β$ models, addressing the classical graph case and extending the analysis to higher order hypergraph models. We further demonstrate the effectiveness of our methods through experiments on synthetic data and a real world communication network.

</details>


### [27] [Statistical Guarantees for Reasoning Probes on Looped Boolean Circuits](https://arxiv.org/abs/2602.03970)
*Anastasis Kratsios,Giulia Livieri,A. Martina Neuman*

Main category: stat.ML

TL;DR: 研究循环推理中推理探针的统计行为，在部分可观测条件下，使用GCN参数化的推理探针能够达到最优泛化误差率，且与图大小无关。


<details>
  <summary>Details</summary>
Motivation: 研究在循环推理模型中，当推理探针只能访问计算图的部分节点时，如何分析其统计泛化性能。部分可观测性带来了泛化问题，需要理解计算图的结构特性如何影响推理的统计效率。

Method: 使用布尔电路作为循环推理的模型化表示，计算图为完美的ν元树。推理探针通过GCN参数化，查询N个节点来推断每个查询节点执行的布尔门类型。分析结合雪花度量嵌入技术和统计最优传输工具。

Result: 当推理探针使用GCN参数化并查询N个节点时，最坏情况泛化误差达到最优速率O(√log(2/δ)/√N)，概率至少为1-δ。这一最优速率与图大小无关，归因于诱导图度量存在低失真的一维雪花嵌入。

Conclusion: 研究结果表明，计算图的结构特性（特别是存在低失真雪花嵌入）决定了部分访问下推理的统计效率。这为理解循环推理中推理探针的统计行为提供了理论框架。

Abstract: We study the statistical behaviour of reasoning probes in a stylized model of looped reasoning, given by Boolean circuits whose computational graph is a perfect $ν$-ary tree ($ν\ge 2$) and whose output is appended to the input and fed back iteratively for subsequent computation rounds. A reasoning probe has access to a sampled subset of internal computation nodes, possibly without covering the entire graph, and seeks to infer which $ν$-ary Boolean gate is executed at each queried node, representing uncertainty via a probability distribution over a fixed collection of $\mathtt{m}$ admissible $ν$-ary gates. This partial observability induces a generalization problem, which we analyze in a realizable, transductive setting.
  We show that, when the reasoning probe is parameterized by a graph convolutional network (GCN)-based hypothesis class and queries $N$ nodes, the worst-case generalization error attains the optimal rate $\mathcal{O}(\sqrt{\log(2/δ)}/\sqrt{N})$ with probability at least $1-δ$, for $δ\in (0,1)$. Our analysis combines snowflake metric embedding techniques with tools from statistical optimal transport. A key insight is that this optimal rate is achievable independently of graph size, owing to the existence of a low-distortion one-dimensional snowflake embedding of the induced graph metric. As a consequence, our results provide a sharp characterization of how structural properties of the computational graph govern the statistical efficiency of reasoning under partial access.

</details>


### [28] [Learning Multi-type heterogeneous interacting particle systems](https://arxiv.org/abs/2602.03954)
*Quanjun Lang,Xiong Wang,Fei Lu,Mauro Maggioni*

Main category: stat.ML

TL;DR: 提出一个从多轨迹数据中联合推断异质交互粒子系统的网络拓扑、多类型交互核和潜在类型分配的框架，通过三阶段方法解决非凸混合整数优化问题。


<details>
  <summary>Details</summary>
Motivation: 异质交互粒子系统（如生物群体、社交网络）中，不同个体可能具有不同的交互类型和强度，从观测数据中联合推断网络结构、交互核和类型分配是一个具有挑战性的非凸混合整数优化问题。

Method: 三阶段方法：1) 利用智能体交互的共享结构，通过矩阵感知恢复系统参数的低秩嵌入；2) 在学到的嵌入中通过聚类识别离散交互类型；3) 通过矩阵分解和后处理细化恢复网络权重矩阵和核系数。

Result: 在合成数据集（包括异质捕食者-被捕食者系统）上的数值实验表明，该方法能准确重建底层动态，并对噪声具有鲁棒性。理论分析提供了在RIP假设下的估计误差界，并建立了基于聚类可分离性的交互类型精确恢复条件。

Conclusion: 该框架为从多轨迹数据中学习异质交互粒子系统提供了一种有效的三阶段方法，具有理论保证和实际应用价值，特别适用于生物群体动态建模等场景。

Abstract: We propose a framework for the joint inference of network topology, multi-type interaction kernels, and latent type assignments in heterogeneous interacting particle systems from multi-trajectory data. This learning task is a challenging non-convex mixed-integer optimization problem, which we address through a novel three-stage approach. First, we leverage shared structure across agent interactions to recover a low-rank embedding of the system parameters via matrix sensing. Second, we identify discrete interaction types by clustering within the learned embedding. Third, we recover the network weight matrix and kernel coefficients through matrix factorization and a post-processing refinement. We provide theoretical guarantees with estimation error bounds under a Restricted Isometry Property (RIP) assumption and establish conditions for the exact recovery of interaction types based on cluster separability. Numerical experiments on synthetic datasets, including heterogeneous predator-prey systems, demonstrate that our method yields an accurate reconstruction of the underlying dynamics and is robust to noise.

</details>


### [29] [Performative Learning Theory](https://arxiv.org/abs/2602.04402)
*Julian Rodemann,Unai Fischer-Abaigar,James Bailie,Krikamol Muandet*

Main category: stat.ML

TL;DR: 该论文研究在预测会影响实际结果（performative predictions）的情况下，机器学习模型的泛化能力，分析了预测对样本和总体产生不同影响时的泛化界限，并揭示了改变世界与从世界学习之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 研究在预测会影响实际结果（performative predictions）的情况下，机器学习模型的泛化能力。当预测不仅影响样本（如现有用户）还影响总体（如潜在用户）时，如何评估模型的泛化性能？例如，当现有用户和潜在用户都会对应用的预测做出反应时，如何基于现有用户洞察新用户？

Method: 将performative predictions嵌入统计学习理论框架，证明在预测影响样本、总体或两者情况下的泛化界限。将自我否定和自我实现的预测分别建模为Wasserstein空间中的min-max和min-min风险泛函。通过德国失业居民就业培训分配的案例研究（基于1975-2017年德国劳动力市场行政记录）验证理论分析。

Result: 证明了在performative effects下的泛化界限，揭示了模型改变数据与从数据学习之间的基本权衡：模型对数据的影响越大，从数据中学习的能力越弱。同时发现了一个反直觉的见解：通过在performatively扭曲的样本上重新训练可以改善泛化保证。

Conclusion: 在performative predictions场景下，模型的泛化能力受到预测对样本和总体影响的制约。最坏情况下，总体可能否定预测，而样本可能欺骗性地实现预测。研究为理解预测影响现实世界时的学习理论提供了框架，并提出了改善泛化性能的实用策略。

Abstract: Performative predictions influence the very outcomes they aim to forecast. We study performative predictions that affect a sample (e.g., only existing users of an app) and/or the whole population (e.g., all potential app users). This raises the question of how well models generalize under performativity. For example, how well can we draw insights about new app users based on existing users when both of them react to the app's predictions? We address this question by embedding performative predictions into statistical learning theory. We prove generalization bounds under performative effects on the sample, on the population, and on both. A key intuition behind our proofs is that in the worst case, the population negates predictions, while the sample deceptively fulfills them. We cast such self-negating and self-fulfilling predictions as min-max and min-min risk functionals in Wasserstein space, respectively. Our analysis reveals a fundamental trade-off between performatively changing the world and learning from it: the more a model affects data, the less it can learn from it. Moreover, our analysis results in a surprising insight on how to improve generalization guarantees by retraining on performatively distorted samples. We illustrate our bounds in a case study on prediction-informed assignments of unemployed German residents to job trainings, drawing upon administrative labor market records from 1975 to 2017 in Germany.

</details>


### [30] [Fixed Budget is No Harder Than Fixed Confidence in Best-Arm Identification up to Logarithmic Factors](https://arxiv.org/abs/2602.03972)
*Kapilan Balagopalan,Yinan Li,Yao Zhao,Tuan Nguyen,Anton Daitche,Houssam Nassif,Kwang-Sung Jun*

Main category: stat.ML

TL;DR: FB（固定预算）不比FC（固定置信度）难，两者样本复杂度在log因子内匹配。提出了FC2FB元算法，可将FC算法转换为FB算法，且保持样本复杂度相近。


<details>
  <summary>Details</summary>
Motivation: 在最佳臂识别（BAI）问题中，固定预算（FB）和固定置信度（FC）两种设置的样本复杂度已有研究。对于K臂赌博机问题，两者的最优样本复杂度已确定且相差log因子。这引发了一个基本问题：对于更一般的结构化BAI问题，FB和FC哪个更难？本文旨在探究这两种设置之间的根本关系。

Method: 提出了FC2FB（固定置信度转固定预算）元算法。该算法以任意FC算法A作为输入，将其转换为FB算法。通过理论分析证明，转换后的FB算法样本复杂度与原始FC算法在log因子内匹配。

Result: 证明了FB不比FC难（在log因子内），即最优FC样本复杂度是FB样本复杂度的上界（相差log因子）。FC2FB算法与现有最优FC算法结合，可在多个FB问题上获得改进的样本复杂度。

Conclusion: 揭示了FB和FC设置之间的基本关系：FB在log因子内不比FC难。FC2FB元算法不仅建立了理论关系，还具有实际意义，能够利用现有FC算法提升FB问题的性能。

Abstract: The best-arm identification (BAI) problem is one of the most fundamental problems in interactive machine learning, which has two flavors: the fixed-budget setting (FB) and the fixed-confidence setting (FC).
  For $K$-armed bandits with the unique best arm, the optimal sample complexities for both settings have been settled down, and they match up to logarithmic factors.
  This prompts an interesting research question about the generic, potentially structured BAI problems: Is FB harder than FC or the other way around?
  In this paper, we show that FB is no harder than FC up to logarithmic factors.
  We do this constructively: we propose a novel algorithm called FC2FB (fixed confidence to fixed budget), which is a meta algorithm that takes in an FC algorithm $\mathcal{A}$ and turn it into an FB algorithm.
  We prove that this FC2FB enjoys a sample complexity that matches, up to logarithmic factors, that of the sample complexity of $\mathcal{A}$.
  This means that the optimal FC sample complexity is an upper bound of the optimal FB sample complexity up to logarithmic factors.
  Our result not only reveals a fundamental relationship between FB and FC, but also has a significant implication: FC2FB, combined with existing state-of-the-art FC algorithms, leads to improved sample complexity for a number of FB problems.

</details>


### [31] [Efficient Subgroup Analysis via Optimal Trees with Global Parameter Fusion](https://arxiv.org/abs/2602.04077)
*Zhongming Xie,Joseph Giorgio,Jingshen Wang*

Main category: stat.ML

TL;DR: 提出融合最优因果树方法，使用混合整数优化进行全局最优子组识别，通过参数融合约束促进子组间信息共享，提高子组发现准确性和统计效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于树的递归分割方法存在贪婪启发式导致次优分割、局部估计分割在小样本下容易过拟合等问题，限制了子组分析的准确性和效率。

Method: 提出融合最优因果树方法，利用混合整数优化实现全局最优分割，引入参数融合约束促进相关子组间的信息共享。

Result: 理论上建立了样本外风险界限，证明优于经典树方法；实证上在模拟中优于流行基线方法；在HABS-HD数据集案例研究中获得临床有意义的见解。

Conclusion: 融合最优因果树方法通过全局优化和参数融合，显著提高了子组发现的准确性和统计效率，为精准医疗提供了有效的子组分析工具。

Abstract: Identifying and making statistical inferences on differential treatment effects (commonly known as subgroup analysis in clinical research) is central to precision health. Subgroup analysis allows practitioners to pinpoint populations for whom a treatment is especially beneficial or protective, thereby advancing targeted interventions. Tree based recursive partitioning methods are widely used for subgroup analysis due to their interpretability. Nevertheless, these approaches encounter significant limitations, including suboptimal partitions induced by greedy heuristics and overfitting from locally estimated splits, especially under limited sample sizes. To address these limitations, we propose a fused optimal causal tree method that leverages mixed integer optimization (MIO) to facilitate precise subgroup identification. Our approach ensures globally optimal partitions and introduces a parameter fusion constraint to facilitate information sharing across related subgroups. This design substantially improves subgroup discovery accuracy and enhances statistical efficiency. We provide theoretical guarantees by rigorously establishing out of sample risk bounds and comparing them with those of classical tree based methods. Empirically, our method consistently outperforms popular baselines in simulations. Finally, we demonstrate its practical utility through a case study on the Health and Aging Brain Study Health Disparities (HABS-HD) dataset, where our approach yields clinically meaningful insights.

</details>


### [32] [Attack-Resistant Uniform Fairness for Linear and Smooth Contextual Bandits](https://arxiv.org/abs/2602.04125)
*Qingwen Zhang,Wenjia Wang*

Main category: stat.ML

TL;DR: 该论文研究带有公平性约束的上下文赌博机问题，开发了在保持公平性的同时达到最优遗憾的算法，并揭示了此类公平性对策略性操纵的独特脆弱性，进而设计了鲁棒变体来抵御攻击。


<details>
  <summary>Details</summary>
Motivation: 现代系统（如数字平台和服务系统）越来越多地依赖上下文赌博机进行在线决策，但这些部署可能无意中造成不同选项（arms）之间的不公平曝光，损害平台的长期可持续性和供应商信任。需要研究在公平性约束下的决策问题，并特别关注公平性约束对策略性操纵的独特脆弱性。

Method: 提出了新颖的算法，针对线性和平滑奖励函数，在保持强(1-Õ(1/T))-公平性保证的同时，达到（近似）极小极大最优遗憾。为了抵御攻击，设计了鲁棒变体，结合了腐败自适应探索和误差补偿阈值技术。

Result: 算法在保持公平性的同时达到了极小极大最优遗憾，并揭示了基于价值的公平性对信号操纵的独特脆弱性：对手只需Õ(1)的预算就能导致公平性特定的失败，而显性的遗憾度量基本不受影响。鲁棒变体在C-预算攻击下实现了首个极小极大最优遗憾界，同时保持公平性。

Conclusion: 该研究为公平性约束下的上下文赌博机问题提供了理论和算法基础，揭示了公平性对策略性操纵的独特脆弱性，并设计了鲁棒算法来同时保证公平性和效率。数值实验和真实案例验证了算法的有效性。

Abstract: Modern systems, such as digital platforms and service systems, increasingly rely on contextual bandits for online decision-making; however, their deployment can inadvertently create unfair exposure among arms, undermining long-term platform sustainability and supplier trust. This paper studies the contextual bandit problem under a uniform $(1-δ)$-fairness constraint, and addresses its unique vulnerabilities to strategic manipulation. The fairness constraint ensures that preferential treatment is strictly justified by an arm's actual reward across all contexts and time horizons, using uniformity to prevent statistical loopholes. We develop novel algorithms that achieve (nearly) minimax-optimal regret for both linear and smooth reward functions, while maintaining strong $(1-\tilde{O}(1/T))$-fairness guarantees, and further characterize the theoretically inherent yet asymptotically marginal "price of fairness". However, we reveal that such merit-based fairness becomes uniquely susceptible to signal manipulation. We show that an adversary with a minimal $\tilde{O}(1)$ budget can not only degrade overall performance as in traditional attacks, but also selectively induce insidious fairness-specific failures while leaving conspicuous regret measures largely unaffected. To counter this, we design robust variants incorporating corruption-adaptive exploration and error-compensated thresholding. Our approach yields the first minimax-optimal regret bounds under $C$-budgeted attack while preserving $(1-\tilde{O}(1/T))$-fairness. Numerical experiments and a real-world case demonstrate that our algorithms sustain both fairness and efficiency.

</details>


### [33] [Maximin Relative Improvement: Fair Learning as a Bargaining Problem](https://arxiv.org/abs/2602.04155)
*Jiwoo Han,Moulinath Banerjee,Yuekai Sun*

Main category: stat.ML

TL;DR: 将群体公平性视为子群体间的讨价还价问题，提出相对改进方法作为Kalai-Smorodinsky解，相比现有方法具有更好的可比性和公理基础


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒优化方法（如最小化最差群体损失）在部署单一预测器到多个子群体时存在局限性，需要从博弈论角度重新思考群体公平性问题

Method: 将群体公平性建模为子群体间的讨价还价问题，提出相对改进方法（实际风险降低与潜在降低的比值），对应Kalai-Smorodinsky解，具有尺度不变性和个体单调性等公理性质

Result: 建立了在温和条件下的有限样本收敛保证，相对改进方法在不同群体具有不同可预测潜力时更具可比性

Conclusion: 博弈论视角为群体公平性提供了新的理论框架，相对改进方法相比绝对尺度方法具有更好的公理基础和实际适用性

Abstract: When deploying a single predictor across multiple subpopulations, we propose a fundamentally different approach: interpreting group fairness as a bargaining problem among subpopulations. This game-theoretic perspective reveals that existing robust optimization methods such as minimizing worst-group loss or regret correspond to classical bargaining solutions and embody different fairness principles. We propose relative improvement, the ratio of actual risk reduction to potential reduction from a baseline predictor, which recovers the Kalai-Smorodinsky solution. Unlike absolute-scale methods that may not be comparable when groups have different potential predictability, relative improvement provides axiomatic justification including scale invariance and individual monotonicity. We establish finite-sample convergence guarantees under mild conditions.

</details>


### [34] [Provable Target Sample Complexity Improvements as Pre-Trained Models Scale](https://arxiv.org/abs/2602.04233)
*Kazuto Fukuchi,Ryuichiro Hataya,Kota Matsui*

Main category: stat.ML

TL;DR: 本文提出一个名为"caulking"的理论框架，解释预训练模型规模与下游任务性能之间的缩放定律，为经验观察提供理论依据。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究无法解释预训练模型规模扩大能显著降低下游任务样本复杂度的经验现象，需要新的理论框架来填补这一空白。

Method: 提出名为"caulking"的理论框架，灵感来源于参数高效微调方法（如适配器微调、低秩适应、部分微调），通过理论分析建立预训练模型改进与下游任务样本复杂度降低之间的关系。

Result: 理论分析证明改进的预训练模型确实能降低下游任务的样本复杂度，为预训练模型规模与下游性能之间的缩放定律提供了理论支持。

Conclusion: 该研究填补了现有理论无法解释预训练模型缩放定律的空白，为预训练模型的优势提供了理论依据，并建立了与参数高效微调方法的理论联系。

Abstract: Pre-trained models have become indispensable for efficiently building models across a broad spectrum of downstream tasks. The advantages of pre-trained models have been highlighted by empirical studies on scaling laws, which demonstrate that larger pre-trained models can significantly reduce the sample complexity of downstream learning. However, existing theoretical investigations of pre-trained models lack the capability to explain this phenomenon. In this paper, we provide a theoretical investigation by introducing a novel framework, caulking, inspired by parameter-efficient fine-tuning (PEFT) methods such as adapter-based fine-tuning, low-rank adaptation, and partial fine-tuning. Our analysis establishes that improved pre-trained models provably decrease the sample complexity of downstream tasks, thereby offering theoretical justification for the empirically observed scaling laws relating pre-trained model size to downstream performance, a relationship not covered by existing results.

</details>


### [35] [Geometry-Aware Optimal Transport: Fast Intrinsic Dimension and Wasserstein Distance Estimation](https://arxiv.org/abs/2602.04335)
*Ferdinand Genans,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 提出无需最优传输求解器的采样误差估计器和内在维度估计器，解决大规模最优传输中的离散化误差瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大规模最优传输通常依赖采样获得离散问题，但采样误差收敛速度受数据内在维度控制，成为实际瓶颈。需要量化采样误差和控制内在维度。

Method: 提出基于半对偶最优传输函数的无调参采样误差估计器，无需最优传输求解器。从采样误差估计器的多尺度衰减推导快速内在维度估计器。

Result: 该框架实现计算和统计优势：量化离散化误差收敛率、根据数据内在几何校准Sinkhorn散度的熵正则化、提出基于内在维度的Richardson外推估计器显著减少Wasserstein距离估计偏差。

Conclusion: 数值实验表明，几何感知的流程有效缓解离散化误差瓶颈，同时保持计算效率，为大规模最优传输提供实用解决方案。

Abstract: Solving large scale Optimal Transport (OT) in machine learning typically relies on sampling measures to obtain a tractable discrete problem. While the discrete solver's accuracy is controllable, the rate of convergence of the discretization error is governed by the intrinsic dimension of our data. Therefore, the true bottleneck is the knowledge and control of the sampling error. In this work, we tackle this issue by introducing novel estimators for both sampling error and intrinsic dimension. The key finding is a simple, tuning-free estimator of $\text{OT}_c(ρ, \hatρ)$ that utilizes the semi-dual OT functional and, remarkably, requires no OT solver. Furthermore, we derive a fast intrinsic dimension estimator from the multi-scale decay of our sampling error estimator. This framework unlocks significant computational and statistical advantages in practice, enabling us to (i) quantify the convergence rate of the discretization error, (ii) calibrate the entropic regularization of Sinkhorn divergences to the data's intrinsic geometry, and (iii) introduce a novel, intrinsic-dimension-based Richardson extrapolation estimator that strongly debiases Wasserstein distance estimation. Numerical experiments demonstrate that our geometry-aware pipeline effectively mitigates the discretization error bottleneck while maintaining computational efficiency.

</details>


### [36] [A Bandit-Based Approach to Educational Recommender Systems: Contextual Thompson Sampling for Learner Skill Gain Optimization](https://arxiv.org/abs/2602.04347)
*Lukas De Kerpel,Arthur Thuy,Dries F. Benoit*

Main category: stat.ML

TL;DR: 提出一种基于学习者信息和历史表现生成个性化练习序列的方法，通过选择最可能提升目标技能的练习，实现在线教育中的规模化个性化教学。


<details>
  <summary>Details</summary>
Motivation: 随着运筹学、管理科学和分析学教学向数字环境转移，面对大规模多样化学习者群体，传统方法难以提供适应个体需求的个性化练习，需要开发能够规模化提供个性化练习的方法。

Method: 开发了一种个性化练习序列生成方法，在每一步选择最可能提升学习者目标技能的练习。该方法利用学习者信息和历史表现指导选择，通过每次练习前后估计技能水平的变化来衡量学习进展。

Result: 基于在线数学辅导平台数据的实验表明，该方法推荐的练习与更大的技能提升相关，并能有效适应不同学习者的差异。从教学角度看，该框架实现了规模化个性化练习，识别出具有持续强学习价值的练习，并帮助教师识别需要额外支持的学习者。

Conclusion: 该方法为在线教育环境中的规模化个性化教学提供了有效框架，能够根据个体学习者需求动态调整练习，提升学习效果并支持教学决策。

Abstract: In recent years, instructional practices in Operations Research (OR), Management Science (MS), and Analytics have increasingly shifted toward digital environments, where large and diverse groups of learners make it difficult to provide practice that adapts to individual needs. This paper introduces a method that generates personalized sequences of exercises by selecting, at each step, the exercise most likely to advance a learner's understanding of a targeted skill. The method uses information about the learner and their past performance to guide these choices, and learning progress is measured as the change in estimated skill level before and after each exercise. Using data from an online mathematics tutoring platform, we find that the approach recommends exercises associated with greater skill improvement and adapts effectively to differences across learners. From an instructional perspective, the framework enables personalized practice at scale, highlights exercises with consistently strong learning value, and helps instructors identify learners who may benefit from additional support.

</details>


### [37] [Anytime-Valid Conformal Risk Control](https://arxiv.org/abs/2602.04364)
*Bror Hultberg,Dave Zachariah,Antônio H. Ribeiro*

Main category: stat.ML

TL;DR: 论文提出了一种新的预测集框架，能够在累积增长的校准数据集上以高概率保持统计有效性，适用于分布漂移场景。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测和风险控制方法仅在固定大小的校准数据集上平均控制误差，无法保证在累积增长的校准数据上始终保持有效性。本文旨在解决这一局限性。

Method: 使用分位数论证方法，建立能够在任意时间点对累积增长的校准数据集保持高概率有效性的统计保证框架。

Result: 建立了匹配的下界，证明所提保证是渐近紧的。通过模拟和真实世界数值实验验证了方法的实际性能。

Conclusion: 提出的框架扩展了传统共形预测方法，使其能够在累积增长的校准数据集上保持高概率有效性，特别适用于存在分布漂移的场景。

Abstract: Prediction sets provide a means of quantifying the uncertainty in predictive tasks. Using held out calibration data, conformal prediction and risk control can produce prediction sets that exhibit statistically valid error control in a computationally efficient manner. However, in the standard formulations, the error is only controlled on average over many possible calibration datasets of fixed size. In this paper, we extend the control to remain valid with high probability over a cumulatively growing calibration dataset at any time point. We derive such guarantees using quantile-based arguments and illustrate the applicability of the proposed framework to settings involving distribution shift. We further establish a matching lower bound and show that our guarantees are asymptotically tight. Finally, we demonstrate the practical performance of our methods through both simulations and real-world numerical examples.

</details>


### [38] [Bayesian PINNs for uncertainty-aware inverse problems (BPINN-IP)](https://arxiv.org/abs/2602.04459)
*Ali Mohammad-Djafari*

Main category: stat.ML

TL;DR: 本文提出了BPINN-IP，一种用于线性逆问题的分层贝叶斯物理信息神经网络框架，能够结合先验知识并量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理逆问题时缺乏不确定性量化能力，且难以融入先验知识。需要开发一种贝叶斯框架来扩展PINNs，使其能够处理线性逆问题并提供不确定性估计。

Method: 提出分层贝叶斯PINN框架(BPINN-IP)，将先验知识融入神经网络输出和权重。采用变分推理和蒙特卡洛dropout方法计算预测均值和方差，实现不确定性量化。

Result: 在去卷积和超分辨率等线性逆问题上进行了应用验证，展示了框架的实现细节，并给出了初步实验结果，证明了方法在不确定性量化方面的有效性。

Conclusion: BPINN-IP成功将贝叶斯框架引入PINNs，为线性逆问题提供了结合物理约束和先验知识的不确定性量化解决方案，在图像重建等应用中具有潜力。

Abstract: The main contribution of this paper is to develop a hierarchical Bayesian formulation of PINNs for linear inverse problems, which is called BPINN-IP. The proposed methodology extends PINN to account for prior knowledge on the nature of the expected NN output, as well as its weights. Also, as we can have access to the posterior probability distributions, naturally uncertainties can be quantified. Also, variational inference and Monte Carlo dropout are employed to provide predictive means and variances for reconstructed images. Un example of applications to deconvolution and super-resolution is considered, details of the different steps of implementations are given, and some preliminary results are presented.

</details>


### [39] [A principled framework for uncertainty decomposition in TabPFN](https://arxiv.org/abs/2602.04596)
*Sandra Fortini,Kenyon Ng,Sonia Petrone,Judith Rousseau,Susan Wei*

Main category: stat.ML

TL;DR: TabPFN作为表格数据预测的transformer模型缺乏不确定性分解方法，本文通过贝叶斯预测推断理论，在监督学习场景下证明了预测中心极限定理，提出了基于预测更新波动率的方差估计方法，实现了快速计算的可信区间和不确定性分解。


<details>
  <summary>Details</summary>
Motivation: TabPFN在表格监督任务中表现出色，但缺乏不确定性分解方法。由于TabPFN在理想化极限下表现为贝叶斯上下文学习器，因此需要解决其不确定性分解问题，这对于可靠预测和决策至关重要。

Method: 将不确定性分解问题转化为贝叶斯预测推断问题，在监督学习场景下证明了预测中心极限定理（基于拟鞅条件），推导出由上下文预测更新波动率决定的方差估计器。对于分类任务，进一步获得基于熵的不确定性分解方法。

Result: 提出的方法能够快速计算可信区间，针对认知不确定性，实现接近名义频率的覆盖度。对于分类任务，获得了基于熵的不确定性分解，为TabPFN提供了有效的不确定性量化工具。

Conclusion: 本文填补了TabPFN不确定性分解的理论空白，通过贝叶斯预测推断框架，在监督学习场景下建立了预测中心极限定理，为表格数据transformer模型提供了高效可靠的不确定性量化方法。

Abstract: TabPFN is a transformer that achieves state-of-the-art performance on supervised tabular tasks by amortizing Bayesian prediction into a single forward pass. However, there is currently no method for uncertainty decomposition in TabPFN. Because it behaves, in an idealised limit, as a Bayesian in-context learner, we cast the decomposition challenge as a Bayesian predictive inference (BPI) problem. The main computational tool in BPI, predictive Monte Carlo, is challenging to apply here as it requires simulating unmodeled covariates. We therefore pursue the asymptotic alternative, filling a gap in the theory for supervised settings by proving a predictive CLT under quasi-martingale conditions. We derive variance estimators determined by the volatility of predictive updates along the context. The resulting credible bands are fast to compute, target epistemic uncertainty, and achieve near-nominal frequentist coverage. For classification, we further obtain an entropy-based uncertainty decomposition.

</details>


### [40] [Targeted Synthetic Control Method](https://arxiv.org/abs/2602.04611)
*Yuxin Wang,Dennis Frauen,Emil Javurek,Konstantin Hess,Yuchen Ma,Stefan Feuerriegel*

Main category: stat.ML

TL;DR: 提出目标合成控制(TSC)方法，通过两阶段估计直接估计反事实结果，改进传统合成控制方法的权重稳定性和估计准确性


<details>
  <summary>Details</summary>
Motivation: 传统合成控制方法(SCM)在面板数据中估计因果效应时，通过未处理控制单元的加权组合构建反事实结果。现有方法存在权重估计不稳定、反事实估计可能无界等问题，需要更稳健的估计方法

Method: 提出目标合成控制(TSC)方法：1) 通过目标去偏估计器，使用权重倾斜子模型进行一维目标更新，校准权重以减少预治疗拟合带来的偏差；2) 确保最终反事实估计是观测控制结果的凸组合，便于直接解释合成控制权重；3) 方法灵活，可与任意机器学习模型结合

Result: 在广泛的合成和真实世界实验中，TSC相比最先进的SCM基线方法，持续提高了估计准确性，避免了现有方法(如增强SCM)可能产生无界反事实估计的关键缺陷

Conclusion: TSC方法通过目标更新机制改进了合成控制权重估计的稳定性，同时保持权重的可解释性，为面板数据中的因果推断提供了更准确和稳健的估计框架

Abstract: The synthetic control method (SCM) estimates causal effects in panel data with a single-treated unit by constructing a counterfactual outcome as a weighted combination of untreated control units that matches the pre-treatment trajectory. In this paper, we introduce the targeted synthetic control (TSC) method, a new two-stage estimator that directly estimates the counterfactual outcome. Specifically, our TSC method (1) yields a targeted debiasing estimator, in the sense that the targeted updating refines the initial weights to produce more stable weights; and (2) ensures that the final counterfactual estimation is a convex combination of observed control outcomes to enable direct interpretation of the synthetic control weights. TSC is flexible and can be instantiated with arbitrary machine learning models. Methodologically, TSC starts from an initial set of synthetic-control weights via a one-dimensional targeted update through the weight-tilting submodel, which calibrates the weights to reduce bias of weights estimation arising from pre-treatment fit. Furthermore, TSC avoids key shortcomings of existing methods (e.g., the augmented SCM), which can produce unbounded counterfactual estimates. Across extensive synthetic and real-world experiments, TSC consistently improves estimation accuracy over state-of-the-art SCM baselines.

</details>


### [41] [Causal explanations of outliers in systems with lagged time-dependencies](https://arxiv.org/abs/2602.04667)
*Philipp Alexander Schwarz,Johannes Oberpriller,Sven Klaassen*

Main category: stat.ML

TL;DR: 该论文将Budhathoki等人的因果根因分析方法扩展到时间依赖系统，提出两种截断方法处理无限依赖图，并在工厂能源管理的峰值避免问题上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 时间依赖系统（特别是能源系统）的根因分析具有挑战性，因为这些系统同时存在瞬时效应和延迟效应，且如果配备存储则具有记忆性。现有方法难以处理这类系统的因果分析。

Method: 将Budhathoki等人的因果根因分析方法扩展到一般时间依赖系统，提出两种截断方法：一种保持因果机制完整，另一种在起始节点处近似因果机制。使用工厂能源管理中避免功率消耗峰值的数据生成过程进行基准测试。

Result: 当提供足够的滞后项时，扩展方法能够在特征域和时间域中定位根因。同时讨论了机制近似的影响。

Conclusion: 该研究成功将因果根因分析方法扩展到时间依赖系统，提出的截断方法能够有效处理无限依赖图，为能源系统等具有时间依赖性的复杂系统提供了实用的根因分析工具。

Abstract: Root-cause analysis in controlled time dependent systems poses a major challenge in applications. Especially energy systems are difficult to handle as they exhibit instantaneous as well as delayed effects and if equipped with storage, do have a memory. In this paper we adapt the causal root-cause analysis method of Budhathoki et al. [2022] to general time-dependent systems, as it can be regarded as a strictly causal definition of the term "root-cause". Particularly, we discuss two truncation approaches to handle the infinite dependency graphs present in time-dependent systems. While one leaves the causal mechanisms intact, the other approximates the mechanisms at the start nodes. The effectiveness of the different approaches is benchmarked using a challenging data generation process inspired by a problem in factory energy management: the avoidance of peaks in the power consumption. We show that given enough lags our extension is able to localize the root-causes in the feature and time domain. Further the effect of mechanism approximation is discussed.

</details>


### [42] [Conditional Counterfactual Mean Embeddings: Doubly Robust Estimation and Learning Rates](https://arxiv.org/abs/2602.04736)
*Thatchanon Anancharoenkij,Donlapark Ponnoprat*

Main category: stat.ML

TL;DR: 提出条件反事实均值嵌入（CCME）框架，将反事实结果的条件分布嵌入再生核希尔伯特空间，开发了三种具有双重稳健性的估计器，能准确恢复条件反事实分布的多模态结构。


<details>
  <summary>Details</summary>
Motivation: 为了全面理解异质性处理效应，需要刻画潜在结果的完整条件分布。现有方法通常只关注条件均值，而忽略了分布的其他特征。

Method: 提出CCME框架，将条件反事实分布嵌入RKHS。开发两阶段元估计器，可容纳任何RKHS值回归。具体实现三种估计器：岭回归估计器、深度特征估计器（用神经网络参数化特征映射）、神经核估计器（用神经网络参数化RKHS值回归系数）。

Result: 为所有估计器提供了有限样本收敛率，证明它们具有双重稳健性。实验表明估计器能准确恢复条件反事实分布的分布特征，包括多模态结构。

Conclusion: CCME框架为分析异质性处理效应提供了强大的工具，能够捕捉完整的条件分布特征，超越了传统的均值估计方法。

Abstract: A complete understanding of heterogeneous treatment effects involves characterizing the full conditional distribution of potential outcomes. To this end, we propose the Conditional Counterfactual Mean Embeddings (CCME), a framework that embeds conditional distributions of counterfactual outcomes into a reproducing kernel Hilbert space (RKHS). Under this framework, we develop a two-stage meta-estimator for CCME that accommodates any RKHS-valued regression in each stage. Based on this meta-estimator, we develop three practical CCME estimators: (1) Ridge Regression estimator, (2) Deep Feature estimator that parameterizes the feature map by a neural network, and (3) Neural-Kernel estimator that performs RKHS-valued regression, with the coefficients parameterized by a neural network. We provide finite-sample convergence rates for all estimators, establishing that they possess the double robustness property. Our experiments demonstrate that our estimators accurately recover distributional features including multimodal structure of conditional counterfactual distributions.

</details>


### [43] [Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning](https://arxiv.org/abs/2602.04872)
*Nicholas Barnfield,Subhabrata Sen,Pragya Sur*

Main category: stat.ML

TL;DR: 本文研究了多模态数据中上下文学习的理论机制，证明了单层线性自注意力无法实现贝叶斯最优预测，而提出的线性化交叉注意力机制在深度和上下文长度足够大时能够实现贝叶斯最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习理论研究主要关注单模态数据，而多模态数据上下文学习的理论基础仍不清楚。本文旨在建立多模态学习中上下文学习的理论框架，探索transformer架构何时能恢复贝叶斯最优性能。

Method: 提出数学可处理的多模态学习框架，假设观测数据来自潜在因子模型。首先证明单层线性自注意力无法实现贝叶斯最优预测，然后引入线性化交叉注意力机制，研究在交叉注意力层数和上下文长度都很大的情况下的性能。

Result: 1. 单层线性自注意力无法在任务分布上一致恢复贝叶斯最优预测器；2. 提出的线性化交叉注意力机制在使用梯度流优化时，在深度和上下文长度足够大的情况下能够实现贝叶斯最优性能。

Conclusion: 深度对上下文学习有益，交叉注意力对多模态分布具有可证明的效用。研究为多模态上下文学习提供了理论依据，强调了交叉注意力机制在处理多模态数据中的重要性。

Abstract: Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context. To model multi-modal problems, we assume the observed data arises from a latent factor model. Our first result comprises a negative take on expressibility: we prove that single-layer, linear self-attention fails to recover the Bayes-optimal predictor uniformly over the task distribution. To address this limitation, we introduce a novel, linearized cross-attention mechanism, which we study in the regime where both the number of cross-attention layers and the context length are large. We show that this cross-attention mechanism is provably Bayes optimal when optimized using gradient flow. Our results underscore the benefits of depth for in-context learning and establish the provable utility of cross-attention for multi-modal distributions.

</details>
