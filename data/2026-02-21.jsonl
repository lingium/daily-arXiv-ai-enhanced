{"id": "2602.16794", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16794", "abs": "https://arxiv.org/abs/2602.16794", "authors": ["Pengqi Liu", "Zijun Yu", "Mouloud Belbahri", "Arthur Charpentier", "Masoud Asgharian", "Jesse C. Cresswell"], "title": "Beyond Procedure: Substantive Fairness in Conformal Prediction", "comment": null, "summary": "Conformal prediction (CP) offers distribution-free uncertainty quantification for machine learning models, yet its interplay with fairness in downstream decision-making remains underexplored. Moving beyond CP as a standalone operation (procedural fairness), we analyze the holistic decision-making pipeline to evaluate substantive fairness-the equity of downstream outcomes. Theoretically, we derive an upper bound that decomposes prediction-set size disparity into interpretable components, clarifying how label-clustered CP helps control method-driven contributions to unfairness. To facilitate scalable empirical analysis, we introduce an LLM-in-the-loop evaluator that approximates human assessment of substantive fairness across diverse modalities. Our experiments reveal that label-clustered CP variants consistently deliver superior substantive fairness. Finally, we empirically show that equalized set sizes, rather than coverage, strongly correlate with improved substantive fairness, enabling practitioners to design more fair CP systems. Our code is available at https://github.com/layer6ai-labs/llm-in-the-loop-conformal-fairness."}
{"id": "2602.16789", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.16789", "abs": "https://arxiv.org/abs/2602.16789", "authors": ["Herold Dehling", "Daniel Vogel", "Martin Wendler"], "title": "First versus full or first versus last: U-statistic change-point tests under fixed and local alternatives", "comment": "56 pages: 22 pages main document, 34 pages appendices (containing proofs), one reference list at the end", "summary": "The use of U-statistics in the change-point context has received considerable attention in the literature. We compare two approaches of constructing CUSUM-type change-point tests, which we call the first-vs-full and first-vs-last approach. Both have been pursued by different authors. The question naturally arises if the two tests substantially differ and, if so, which of them is better in which data situation. In large samples, both tests are similar: they are asymptotically equivalent under the null hypothesis and under sequences of local alternatives. In small samples, there may be quite noticeable differences, which is in line with a different asymptotic behavior under fixed alternatives. We derive a simple criterion for deciding which test is more powerful. We examine the examples Gini's mean difference, the sample variance, and Kendall's tau in detail. Particularly, when testing for changes in scale by Gini's mean difference, we show that the first-vs-full approach has a higher power if and only if the scale changes from a smaller to a larger value -- regardless of the population distribution or the location of the change. The asymptotic derivations are under weak dependence. The results are illustrated by numerical simulations and data examples."}
{"id": "2602.16789", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.16789", "abs": "https://arxiv.org/abs/2602.16789", "authors": ["Herold Dehling", "Daniel Vogel", "Martin Wendler"], "title": "First versus full or first versus last: U-statistic change-point tests under fixed and local alternatives", "comment": "56 pages: 22 pages main document, 34 pages appendices (containing proofs), one reference list at the end", "summary": "The use of U-statistics in the change-point context has received considerable attention in the literature. We compare two approaches of constructing CUSUM-type change-point tests, which we call the first-vs-full and first-vs-last approach. Both have been pursued by different authors. The question naturally arises if the two tests substantially differ and, if so, which of them is better in which data situation. In large samples, both tests are similar: they are asymptotically equivalent under the null hypothesis and under sequences of local alternatives. In small samples, there may be quite noticeable differences, which is in line with a different asymptotic behavior under fixed alternatives. We derive a simple criterion for deciding which test is more powerful. We examine the examples Gini's mean difference, the sample variance, and Kendall's tau in detail. Particularly, when testing for changes in scale by Gini's mean difference, we show that the first-vs-full approach has a higher power if and only if the scale changes from a smaller to a larger value -- regardless of the population distribution or the location of the change. The asymptotic derivations are under weak dependence. The results are illustrated by numerical simulations and data examples."}
{"id": "2602.16830", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16830", "abs": "https://arxiv.org/abs/2602.16830", "authors": ["Genís Ruiz-Menárguez", "Llorenç Badiella"], "title": "The Impact of Formations on Football Matches Using Double Machine Learning. Is it worth parking the bus?", "comment": "17 pages, 5 figures, 3 tables", "summary": "This study addresses a central tactical dilemma for football coaches: whether to employ a defensive strategy, colloquially known as \"parking the bus\", or a more offensive one. Using an advanced Double Machine Learning (DML) framework, this project provides a robust and interpretable tool to estimate the causal impact of different formations on key match outcomes such as goal difference, possession, corners, and disciplinary actions. Leveraging a dataset of over 22,000 matches from top European leagues, formations were categorized into six representative types based on tactical structure and expert consultation. A major methodological contribution lies in the adaptation of DML to handle categorical treatments, specifically formation combinations, through a novel matrix-based residualization process, allowing for a detailed estimation of formation-versus-formation effects that can inform a coach's tactical decision-making. Results show that while offensive formations like 4-3-3 and 4-2-3-1 offer modest statistical advantages in possession and corners, their impact on goals is limited. Furthermore, no evidence supports the idea that defensive formations, commonly associated with parking the bus, increase a team's winning potential. Additionally, red cards appear unaffected by formation choice, suggesting other behavioral factors dominate. Although this approach does not fully capture all aspects of playing style or team strength, it provides a valuable framework for coaches to analyze tactical efficiency and sets a precedent for future research in sports analytics."}
{"id": "2602.16923", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16923", "abs": "https://arxiv.org/abs/2602.16923", "authors": ["Junhui Cai", "Ran Chen", "Qitao Huang", "Linda Zhao", "Wu Zhu"], "title": "Poisson-MNL Bandit: Nearly Optimal Dynamic Joint Assortment and Pricing with Decision-Dependent Customer Arrivals", "comment": null, "summary": "We study dynamic joint assortment and pricing where a seller updates decisions at regular accounting/operating intervals to maximize the cumulative per-period revenue over a horizon $T$. In many settings, assortment and prices affect not only what an arriving customer buys but also how many customers arrive within the period, whereas classical multinomial logit (MNL) models assume arrivals as fixed, potentially leading to suboptimal decisions. We propose a Poisson-MNL model that couples a contextual MNL choice model with a Poisson arrival model whose rate depends on the offered assortment and prices. Building on this model, we develop an efficient algorithm PMNL based on the idea of upper confidence bound (UCB). We establish its (near) optimality by proving a non-asymptotic regret bound of order $\\sqrt{T\\log{T}}$ and a matching lower bound (up to $\\log T$). Simulation studies underscore the importance of accounting for the dependency of arrival rates on assortment and pricing: PMNL effectively learns customer choice and arrival models and provides joint assortment-pricing decisions that outperform others that assume fixed arrival rates."}
{"id": "2602.16970", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.16970", "abs": "https://arxiv.org/abs/2602.16970", "authors": ["Chen Li", "Thomas W. Hsiao", "Stefanie Ebelt", "Rebecca H. Zhang", "Howard H. Chang"], "title": "Temperature and Respiratory Emergency Department Visits: A Mediation Analysis with Ambient Ozone Exposure", "comment": null, "summary": "High temperatures are associated with adverse respiratory health outcomes and increases in ambient air pollution. Limited research has quantified air pollution's mediating role in the relationship between temperature and respiratory morbidity, such as emergency department (ED) visits. In this study, we conducted a causal mediation analysis to decompose the total effect of daily temperature on respiratory ED visits in Los Angeles from 2005 to 2016. We focused on ambient ozone as a mediator because its precursors and formation are directly driven by sunlight and temperature. We estimated natural direct, indirect, and total effects on the relative risk scale across deciles of temperature exposure compared to the median. We utilized Bayesian additive regression trees (BART) to flexibly characterize the nonlinear relationship between temperature and ozone and quantified uncertainty via posterior prediction and the Bayesian bootstrap. Our results showed that ozone partially mediated the association between high temperatures and respiratory ED visits, particularly at moderately high temperatures. We also validated our modeling approach through simulation studies. This study extends the existing literature by considering acute respiratory morbidity and employing a flexible modeling approach, offering new insights into the mechanisms underlying temperature-related health risks."}
{"id": "2602.17115", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17115", "abs": "https://arxiv.org/abs/2602.17115", "authors": ["Juntong Chen", "Claire Donnat", "Olga Klopp", "Johannes Schmidt-Hieber"], "title": "Semi-Supervised Learning on Graphs using Graph Neural Networks", "comment": "57 pages, 7 figures", "summary": "Graph neural networks (GNNs) work remarkably well in semi-supervised node regression, yet a rigorous theory explaining when and why they succeed remains lacking. To address this gap, we study an aggregate-and-readout model that encompasses several common message passing architectures: node features are first propagated over the graph then mapped to responses via a nonlinear function. For least-squares estimation over GNNs with linear graph convolutions and a deep ReLU readout, we prove a sharp non-asymptotic risk bound that separates approximation, stochastic, and optimization errors. The bound makes explicit how performance scales with the fraction of labeled nodes and graph-induced dependence. Approximation guarantees are further derived for graph-smoothing followed by smooth nonlinear readouts, yielding convergence rates that recover classical nonparametric behavior under full supervision while characterizing performance when labels are scarce. Numerical experiments validate our theory, providing a systematic framework for understanding GNN performance and limitations."}
{"id": "2602.17034", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17034", "abs": "https://arxiv.org/abs/2602.17034", "authors": ["Oluwayomi Akinfenwa", "Niamh Cahill", "Catherine Hurley"], "title": "Using Time Series Measures to Explore Family Planning Survey Data and Model-based Estimates", "comment": null, "summary": "Family planning is a global development priority and a key indicator of reproductive health. Monitoring progress is challenged by gaps in survey data across countries. The United Nations Population Division addresses this with the Family Planning Estimation Model (FPEM), a Bayesian hierarchical time series model producing annual estimates of modern contraceptive use while sharing information across countries and regions. This paper evaluates how well FPEM estimates align with survey data using time series diagnostic indices from the wdiexplorer R package, which account for countries nested within sub-regions. Visualisation of survey data, modelled trajectories, and diagnostics enables assessment of model performance, highlighting where trends align and where discrepancies occur."}
{"id": "2602.16992", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.16992", "abs": "https://arxiv.org/abs/2602.16992", "authors": ["Daniel Suen", "Yen-Chi Chen"], "title": "Modeling Multivariate Missingness with Tree Graphs and Conjugate Odds", "comment": "82 pages, 15 figures", "summary": "In this paper, we analyze a specific class of missing not at random (MNAR) assumptions called tree graphs, extending upon the work of pattern graphs. We build off previous work by introducing the idea of a conjugate odds family in which certain parametric models on the selection odds can preserve the data distribution family across all missing data patterns. Under a conjugate odds family and a tree graph assumption, we are able to model the full data distribution elegantly in the sense that for the observed data, we obtain a model that is conjugate from the complete-data, and for the missing entries, we create a simple imputation model. In addition, we investigate the problem of graph selection, sensitivity analysis, and statistical inference. Using both simulations and real data, we illustrate the applicability of our method."}
{"id": "2602.17043", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17043", "abs": "https://arxiv.org/abs/2602.17043", "authors": ["Paul-Hieu V. Nguyen", "James M. Smoliga", "Benton Lindaman", "Sameer K. Deshpande"], "title": "Quantifying the limits of human athletic performance: A Bayesian analysis of elite decathletes", "comment": null, "summary": "Because the decathlon tests many facets of athleticism, including sprinting, throwing, jumping, and endurance, many consider it to be the ultimate test of athletic ability. On this view, estimating the maximal decathlon score and understanding what it would take to achieve that score provides insight into the upper limits of human athletic potential. To this end, we develop a Bayesian composition model for forecasting how individual athletes perform in each of the 10 decathlon events of time. Besides capturing potential non-linear temporal trends in performance, our model carefully captures the dependence between performance in an event and all preceding events. Using our model, we can simulate and evaluate the distribution of the maximal possible scores and identify profiles of athletes who could realistically attain scores approaching this limit."}
{"id": "2602.17211", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17211", "abs": "https://arxiv.org/abs/2602.17211", "authors": ["Etienne Lempereur", "Nathanaël Cuvelle--Magar", "Florentin Coeurdoux", "Stéphane Mallat", "Eric Vanden-Eijnden"], "title": "MGD: Moment Guided Diffusion for Maximum Entropy Generation", "comment": null, "summary": "Generating samples from limited information is a fundamental problem across scientific domains. Classical maximum entropy methods provide principled uncertainty quantification from moment constraints but require sampling via MCMC or Langevin dynamics, which typically exhibit exponential slowdown in high dimensions. In contrast, generative models based on diffusion and flow matching efficiently transport noise to data but offer limited theoretical guarantees and can overfit when data is scarce. We introduce Moment Guided Diffusion (MGD), which combines elements of both approaches. Building on the stochastic interpolant framework, MGD samples maximum entropy distributions by solving a stochastic differential equation that guides moments toward prescribed values in finite time, thereby avoiding slow mixing in equilibrium-based methods. We formally obtain, in the large-volatility limit, convergence of MGD to the maximum entropy distribution and derive a tractable estimator of the resulting entropy computed directly from the dynamics. Applications to financial time series, turbulent flows, and cosmological fields using wavelet scattering moments yield estimates of negentropy for high-dimensional multiscale processes."}
{"id": "2602.17041", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17041", "abs": "https://arxiv.org/abs/2602.17041", "authors": ["Conor Chandler", "Jack Ishak"], "title": "Reframing Population-Adjusted Indirect Comparisons as a Transportability Problem: An Estimand-Based Perspective and Implications for Health Technology Assessment", "comment": "28 pages (excluding supplement), 7 figures, 1 table", "summary": "Population-adjusted indirect comparisons (PAICs) are widely used to synthesize evidence when randomized controlled trials enroll different patient populations and head-to-head comparisons are unavailable. Although PAICs adjust for observed population differences across trials, adjustment alone does not ensure transportability of estimated effects to decision-relevant populations for health technology assessment (HTA). We examine and formalize transportability in PAICs from an estimand-based perspective. We distinguish conditional and marginal treatment effect estimands and show how transportability depends on effect modification, collapsibility, and alignment between the scale of effect modification and the effect measure. Using illustrative examples, we demonstrate that even when effect modifiers are shared across treatments, marginal effects are generally population-dependent for commonly used non-collapsible measures, including hazard ratios and odds ratios. Conversely, collapsible and conditional effects defined on the linear predictor scale exhibit more favorable transportability properties. We further show that pairwise PAIC approaches typically identify effects defined in the comparator population and that applying these estimates to other populations entails an additional, often implicit, transport step requiring further assumptions. This has direct implications for HTA, where PAIC-derived effects are routinely applied within cost-effectiveness and decision models defined for different target populations. Our results clarify when applying PAIC-derived treatment effects to desired target populations is justified, when doing so requires additional assumptions, and when results should instead be interpreted as population-specific rather than decision-relevant, supporting more transparent and principled use of indirect evidence in HTA and related decision-making contexts."}
{"id": "2602.17640", "categories": ["stat.AP", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17640", "abs": "https://arxiv.org/abs/2602.17640", "authors": ["Thomas Wieland"], "title": "huff: A Python package for Market Area Analysis", "comment": null, "summary": "Market area models, such as the Huff model and its extensions, are widely used to estimate regional market shares and customer flows of retail and service locations. Another, now very common, area of application is the analysis of catchment areas, supply structures and the accessibility of healthcare locations. The huff Python package provides a complete workflow for market area analysis, including data import, construction of origin-destination interaction matrices, basic model analysis, parameter estimation from empirical data, calculation of distance or travel time indicators, and map visualization. Additionally, the package provides several methods of spatial accessibility analysis. The package is modular and object-oriented. It is intended for researchers in economic geography, regional economics, spatial planning, marketing, geoinformation science, and health geography. The software is openly available via the [Python Package Index (PyPI)](https://pypi.org/project/huff/); its development and version history are managed in a public [GitHub Repository](https://github.com/geowieland/huff_official) and archived at [Zenodo](https://doi.org/10.5281/zenodo.18639559)."}
{"id": "2602.17070", "categories": ["stat.ME", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17070", "abs": "https://arxiv.org/abs/2602.17070", "authors": ["Tianyuan Cheng", "Ruirui Mao", "Judea Pearl", "Ang Li"], "title": "General sample size analysis for probabilities of causation: a delta method approach", "comment": null, "summary": "Probabilities of causation (PoCs), such as the probability of necessity and sufficiency (PNS), are important tools for decision making but are generally not point identifiable. Existing work has derived bounds for these quantities using combinations of experimental and observational data. However, there is very limited research on sample size analysis, namely, how many experimental and observational samples are required to achieve a desired margin of error. In this paper, we propose a general sample size framework based on the delta method. Our approach applies to settings in which the target bounds of PoCs can be expressed as finite minima or maxima of linear combinations of experimental and observational probabilities. Through simulation studies, we demonstrate that the proposed sample size calculations lead to stable estimation of these bounds."}
{"id": "2602.16933", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16933", "abs": "https://arxiv.org/abs/2602.16933", "authors": ["Dan M. Kluger", "Stephen Bates"], "title": "M-estimation under Two-Phase Multiwave Sampling with Applications to Prediction-Powered Inference", "comment": null, "summary": "In two-phase multiwave sampling, inexpensive measurements are collected on a large sample and expensive, more informative measurements are adaptively obtained on subsets of units across multiple waves. Adaptively collecting the expensive measurements can increase efficiency but complicates statistical inference. We give valid estimators and confidence intervals for M-estimation under adaptive two-phase multiwave sampling. We focus on the case where proxies for the expensive variables -- such as predictions from pretrained machine learning models -- are available for all units and propose a Multiwave Predict-Then-Debias estimator that combines proxy information with the expensive, higher-quality measurements to improve efficiency while removing bias. We establish asymptotic linearity and normality and propose asymptotically valid confidence intervals. We also develop an approximately greedy sampling strategy that improves efficiency relative to uniform sampling. Data-based simulation studies support the theoretical results and demonstrate efficiency gains."}
{"id": "2602.17603", "categories": ["stat.ML", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17603", "abs": "https://arxiv.org/abs/2602.17603", "authors": ["Roey Yadgar", "Roy R. Lederman", "Yoel Shkolnisky"], "title": "SOLVAR: Fast covariance-based heterogeneity analysis with pose refinement for cryo-EM", "comment": null, "summary": "Cryo-electron microscopy (cryo-EM) has emerged as a powerful technique for resolving the three-dimensional structures of macromolecules. A key challenge in cryo-EM is characterizing continuous heterogeneity, where molecules adopt a continuum of conformational states. Covariance-based methods offer a principled approach to modeling structural variability. However, estimating the covariance matrix efficiently remains a challenging computational task. In this paper, we present SOLVAR (Stochastic Optimization for Low-rank Variability Analysis), which leverages a low-rank assumption on the covariance matrix to provide a tractable estimator for its principal components, despite the apparently prohibitive large size of the covariance matrix. Under this low-rank assumption, our estimator can be formulated as an optimization problem that can be solved quickly and accurately. Moreover, our framework enables refinement of the poses of the input particle images, a capability absent from most heterogeneity-analysis methods, and all covariance-based methods. Numerical experiments on both synthetic and experimental datasets demonstrate that the algorithm accurately captures dominant components of variability while maintaining computational efficiency. SOLVAR achieves state-of-the-art performance across multiple datasets in a recent heterogeneity benchmark. The code of the algorithm is freely available at https://github.com/RoeyYadgar/SOLVAR."}
{"id": "2602.17261", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17261", "abs": "https://arxiv.org/abs/2602.17261", "authors": ["Gudmund Hermansen", "Nils Lid Hjort", "Martin Jullum"], "title": "Parametric or nonparametric: the FIC approach for stationary time series", "comment": "21 pages, 6 figures; Statistical Research Report (Department of Mathematics, University of Oslo), from December 2015, but arXiv'd February 2026; a later modified and extended version might then become a journal paper", "summary": "We seek to narrow the gap between parametric and nonparametric modelling of stationary time series processes. The approach is inspired by recent advances in focused inference and model selection techniques. The paper generalises and extends recent work by developing a new version of the focused information criterion (FIC), directly comparing the performance of parametric time series models with a nonparametric alternative. For a pre-specified focused parameter, for which scrutiny is considered valuable, this is achieved by comparing the mean squared error of the model-based estimators of this quantity. In particular, this yields FIC formulae for covariances or correlations at specified lags, for the probability of reaching a threshold, etc. Suitable weighted average versions, the AFIC, also lead to model selection strategies for finding the best model for the purpose of estimating e.g.~a sequence of correlations."}
{"id": "2602.17503", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17503", "abs": "https://arxiv.org/abs/2602.17503", "authors": ["Emily Gribbin", "Benjamin Davis", "Daniel Rolfe", "Hannah Mitchell"], "title": "An extension to reversible jump Markov chain Monte Carlo for change point problems with heterogeneous temporal dynamics", "comment": null, "summary": "Detecting brief changes in time-series data remains a major challenge in fields where short-lived states carry meaning. In single-molecule localisation microscopy, this problem is particularly acute as fluorescent molecules used to tag protein oligomers display heterogenous photophysical behaviour that can complicate photobleach step analysis; a key step in resolving nanoscale protein organisation. Existing methods often require extensive filtering or prior calibration, and can fail to accurately account for blinking or reversible dark states that may contaminate downstream analysis. In this paper, an extension to RJMCMC is proposed for change point detection with heterogeneous temporal dynamics. This approach is applied to the problem of estimating per-frame active fluorophore counts from one-dimensional integrated intensity traces derived from Fluorescence Localisation Imaging with Photobleaching (FLImP), where compound change point pair moves are introduced to better account for short-lived events known as blinking and dark states. The approach is validated using simulated and experimental data, demonstrating improved accuracy and robustness when compared with current photobleach step analysis methods and with the existing analysis approach for FLImP data. This Compound RJMCMC (CRJMCMC) algorithm performs reliably across a wide range of fluorophore counts and signal-to-noise conditions, with signal-to-noise ratio (SNR) down to 0.001 and counts as high as seventeen fluorophores, while also effectively estimating low counts observed when studying EGFR oligomerisation. Beyond single molecule imaging, this work has applications for a variety of time series change point detection problems with heterogeneous state persistence. For example, electrocorticography brain-state segmentation, fault detection in industrial process monitoring and realised volatility in financial time series."}
{"id": "2602.17592", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17592", "abs": "https://arxiv.org/abs/2602.17592", "authors": ["Di Zhu", "Yong Zang"], "title": "BMW: Bayesian Model-Assisted Adaptive Phase II Clinical Trial Design for Win Ratio Statistic", "comment": "32 pages, 2 figures", "summary": "The win ratio (WR) statistic is increasingly used to evaluate treatment effects based on prioritized composite endpoints, yet existing Bayesian adaptive designs are not directly applicable because the WR is a summary statistic derived from pairwise comparisons and does not correspond to a unique data-generating mechanism. We propose a Bayesian model-assisted adaptive design for randomized phase II clinical trials based on the WR statistic, referred to as the BMW design. The proposed design uses the joint asymptotic distribution of WR test statistics across interim and final analyses to compute posterior probabilities without specifying the underlying outcome distribution. The BMW design allows flexible interim monitoring with early stopping for futility or superiority and is extended to jointly evaluate efficacy and toxicity using a graphical testing procedure that controls the family-wise error rate (FWER). Simulation studies demonstrate that the BMW design maintains valid type I error and FWER control, achieves power comparable to conventional methods, and substantially reduces expected sample size. An R Shiny application is provided to facilitate practical implementation."}
{"id": "2602.17543", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17543", "abs": "https://arxiv.org/abs/2602.17543", "authors": ["Masahiro Kato"], "title": "genriesz: A Python Package for Automatic Debiased Machine Learning with Generalized Riesz Regression", "comment": null, "summary": "Efficient estimation of causal and structural parameters can be automated using the Riesz representation theorem and debiased machine learning (DML). We present genriesz, an open-source Python package that implements automatic DML and generalized Riesz regression, a unified framework for estimating Riesz representers by minimizing empirical Bregman divergences. This framework includes covariate balancing, nearest-neighbor matching, calibrated estimation, and density ratio estimation as special cases. A key design principle of the package is automatic regressor balancing (ARB): given a Bregman generator $g$ and a representer model class, genriesz} automatically constructs a compatible link function so that the generalized Riesz regression estimator satisfies balancing (moment-matching) optimality conditions in a user-chosen basis. The package provides a modulr interface for specifying (i) the target linear functional via a black-box evaluation oracle, (ii) the representer model via basis functions (polynomial, RKHS approximations, random forest leaf encodings, neural embeddings, and a nearest-neighbor catchment basis), and (iii) the Bregman generator, with optional user-supplied derivatives. It returns regression adjustment (RA), Riesz weighting (RW), augmented Riesz weighting (ARW), and TMLE-style estimators with cross-fitting, confidence intervals, and $p$-values. We highlight representative workflows for estimation problems such as the average treatment effect (ATE), ATE on treated (ATT), and average marginal effect estimation. The Python package is available at https://github.com/MasaKat0/genriesz and on PyPI."}
