<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 11]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.AP](#stat.AP) [Total: 6]
- [stat.CO](#stat.CO) [Total: 3]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [PLS-SEM-power: A Shiny App and R package for Computing Required Sample Size and Minimum Detectable Effect Size in PLS-SEMs](https://arxiv.org/abs/2511.14546)
*Alessandro Ansani,Elena Rinallo*

Main category: stat.ME

TL;DR: 介绍了PLS-SEM-power工具，用于计算PLS-SEM研究所需的最小样本量和最小可检测效应大小


<details>
  <summary>Details</summary>
Motivation: 统计功效对PLS-SEM研究规划至关重要，但现有工具不够直观易用

Method: 基于Kock和Hadaya(2018)的逆平方根方法，开发了Shiny应用和R包，实现先验分析和敏感性分析

Result: 创建了具有直观用户界面的工具，支持在80%功效水平下计算最小样本量和最小可检测效应大小

Conclusion: 该工具为不同研究背景下的PLS-SEM分析提供了可重复且易于访问的解决方案

Abstract: Despite its evanescent nature, statistical power is crucial for planning Partial Least Squares Structural Equation Modelling (PLS-SEM) studies. This brief paper introduces PLS-SEM-power, a Shiny Application and R package that implements the inverse square root method by Kock and Hadaya (2018) to calculate both the minimum required sample size (a priori analysis) and the Minimum Detectable Effect Size (MDES, sensitivity analysis), given a chosen significance level (alpha level) at 80% power (1 - beta). The application provides an intuitive user interface, facilitating reproducible and easily accessible analyses in diverse research contexts.

</details>


### [2] [On robust Bayesian causal inference](https://arxiv.org/abs/2511.13895)
*Angelos Alexopoulos,Nikolaos Demiris*

Main category: stat.ME

TL;DR: 本文开发了一个贝叶斯框架，用于从纵向观测数据中进行稳健的因果推断，通过广义贝叶斯推理量化模型误设并调整，同时保持可解释的后验推断。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖结构假设（如因子模型）来调整未观测混杂，但当模型误设时会导致有偏的因果估计量。

Method: 使用广义贝叶斯推理直接估计时间-单位特定因果效应，基于适当评分规则选择学习率ω，联合评估因果估计量的点和区间准确性。

Result: 模拟研究和真实数据应用显示在估计因果效应时改进了校准性、锐度和稳健性。

Conclusion: 该方法为调整模型误设提供了连贯的决策理论基础，在因果推断中实现了更好的校准和稳健性。

Abstract: This paper develops a Bayesian framework for robust causal inference from longitudinal observational data. Many contemporary methods rely on structural assumptions, such as factor models, to adjust for unobserved confounding, but they can lead to biased causal estimands when mis-specified. We focus on directly estimating time--unit--specific causal effects and use generalised Bayesian inference to quantify model mis-specification and adjust for it, while retaining interpretable posterior inference. We select the learning rate~$ω$ based on a proper scoring rule that jointly evaluates point and interval accuracy of the causal estimand, thus providing a coherent, decision-theoretic foundation for tuning~$ω$. Simulation studies and applications to real data demonstrate improved calibration, sharpness, and robustness in estimating causal effects.

</details>


### [3] [High-Dimensional Covariate-Dependent Discrete Graphical Models and Dynamic Ising Models](https://arxiv.org/abs/2511.14123)
*Lyndsay Roach,Qiong Li,Nanwei Wang,Xin Gao*

Main category: stat.ME

TL;DR: 提出了一种协变量依赖的离散图模型，用于捕捉离散随机变量间的动态网络，允许顶点间的依赖结构随协变量变化。该模型包含动态Ising模型作为特例，采用基于似然的方法进行参数估计和统计推断。


<details>
  <summary>Details</summary>
Motivation: 现有网络模型往往假设依赖结构是静态的，无法捕捉网络随协变量变化的动态特性。需要开发能够建模顶点间依赖关系随外部因素变化的动态网络模型。

Method: 使用伪似然方法进行高效参数估计，提出生灭马尔可夫链蒙特卡洛算法进行模型选择，探索模型空间并选择最合适的模型。

Result: 开发了一个能够处理高维设置的协变量依赖离散图模型框架，实现了参数的有效估计和模型选择。

Conclusion: 提出的方法能够有效建模动态网络，捕捉顶点间依赖关系随协变量的变化，为高维动态网络分析提供了实用工具。

Abstract: We propose a covariate-dependent discrete graphical model for capturing dynamic networks among discrete random variables, allowing the dependence structure among vertices to vary with covariates. This discrete dynamic network encompasses the dynamic Ising model as a special case. We formulate a likelihood-based approach for parameter estimation and statistical inference. We achieve efficient parameter estimation in high-dimensional settings through the use of the pseudo-likelihood method. To perform model selection, a birth-and-death Markov chain Monte Carlo algorithm is proposed to explore the model space and select the most suitable model.

</details>


### [4] [State-Space Representation of INGARCH Models and Their Application in Insurance](https://arxiv.org/abs/2511.14091)
*Jae Youn Ahn,Hong Beng Lim,Mario V. Wüthrich*

Main category: stat.ME

TL;DR: 本文提出了边际状态空间模型(M-SSM)框架，将INGARCH模型作为其特例，解决了INGARCH模型在理论解释、协变量纳入和缺失数据处理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: INGARCH模型虽然常用于计数时间序列建模，但缺乏清晰的理论基础，难以解释演化过程，协变量纳入困难，且处理缺失数据计算负担重，限制了其在保险等领域的应用。

Method: 引入边际状态空间模型(M-SSM)，仅通过观测值的边际分布定义，证明INGARCH模型是该框架的特例。在条件均值演化满足特定假设时，M-SSM可转化为观测驱动状态空间模型(O-SSM)。

Result: M-SSM框架为INGARCH模型提供了理论支撑，便于自然纳入协变量和缺失数据机制，并在异质性和缺失数据情况下建立弱平稳性。

Conclusion: M-SSM框架扩展了INGARCH模型的应用范围，特别适用于保险数据的预测分析，通过泊松和负二项INGARCH(1,1)模型验证了方法的有效性。

Abstract: Integer-valued generalized autoregressive conditional heteroskedastic (INGARCH) models are a popular framework for modeling serial dependence in count time-series. While convenient for modeling, prediction, and estimation, INGARCH models lack a clear theoretical justification for the evolution step. This limitation not only makes interpretation difficult and complicates the inclusion of covariates, but can also make the handling of missing data computationally burdensome. Consequently, applying such models in an insurance context, where covariates and missing observations are common, can be challenging. In this paper, we first introduce the marginalized state-space model (M-SSM), defined solely through the marginal distribution of the observations, and show that INGARCH models arise as special cases of this framework. The M-SSM formulation facilitates the natural incorporation of covariates and missing data mechanisms, and this representation in turn provides a coherent way to incorporate these elements within the INGARCH model as well. We then demonstrate that an M-SSM can admit an observation-driven state-space model (O-SSM) representation when suitable assumptions are imposed on the evolution of its conditional mean. This lifting from an M-SSM to an O-SSM provides a natural setting for establishing weak stationarity, even in the presence of heterogeneity and missing observations. The proposed ideas are illustrated through the Poisson and the Negative-Binomial INGARCH(1,1) models, highlighting their applicability in predictive analysis for insurance data.

</details>


### [5] [The Prevalence of Misreporting and Misinterpreting Correlation Coefficients in Biomedical Literature](https://arxiv.org/abs/2511.14092)
*Jiayang Xu,Xintong Chen,Yufeng Liu,Xiaoli Guo,Shanbao Tong*

Main category: stat.ME

TL;DR: 对2022年发表在Science、Nature和Nature Neuroscience的310篇文章中1326个相关性分析记录的系统回顾显示，相关性系数在生物医学研究中存在广泛误用和错误解读，严重影响了科学发现的可信度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 相关性系数在生物医学文献中被广泛使用，但其频繁的误用和错误解读正在削弱科学发现的可信度和可重复性，需要系统评估当前研究中的统计报告质量。

Method: 系统回顾了2022年发表在Science、Nature和Nature Neuroscience期刊上的310篇文章中的1326个相关性分析记录，分析统计报告的完整性和推断方法。

Result: 58.71%的研究未明确报告样本量，98.06%的研究未提供相关性系数的置信区间。在推断相关性强度的177篇文章中，45.25%仅依赖点估计值，53.63%基于零假设显著性检验得出结论。

Conclusion: 相关性分析中普遍存在的遗漏和误用反映了统计素养和编辑标准的系统性差距，建议制定明确的报告指南，要求在相关性分析中必须报告效应量和置信区间，以提高生命科学定量研究的透明度、严谨性和可重复性。

Abstract: Correlation coefficient is widely used in biomedical and biological literature, yet its frequent misuse and misinterpretation undermine the credibility and reproducibility of the scientific findings. We systematically reviewed 1326 records of correlation analyses across 310 articles published in Science, Nature, and Nature Neuroscience in 2022. Our analysis revealed a troubling pattern of poor statistical reporting and inferring: 58.71% (95% CI: [53.23%, 64.19%], 182/310) of studies did not explicitly report sample sizes, and 98.06% (95% CI: [96.53%, 99.60%], 304/310) failed to provide confidence intervals for correlation coefficients. Among 177 articles inferring correlation strength, 45.25% (95% CI: [38.42%, 53.10%], 81/177) relied solely on point estimates, while 53.63% (95% CI: [46.90%, 61.58%], 96/177) drew conclusions based on null hypothesis significance testing. This widespread omission and misuse highlight a systematic gap in both statistic literacy and editorial standards. We advocate clear reporting guidelines mandating effect sizes and confidence intervals in correlation analyses to enhance the transparency, rigor, and reproducibility of quantitative life sciences research.

</details>


### [6] [Covariate Adjustment for the Win Odds: Application to Cardiovascular Outcomes Trials](https://arxiv.org/abs/2511.14292)
*Cyrill Scheidegger,Simon Wandel,Tobias Mütze*

Main category: stat.ME

TL;DR: 本文建立了胜率比与边际概率指数的联系，提出了胜率比的协变量调整方法，可提高估计精度和统计功效，但小样本下可能轻微增加I类错误率。


<details>
  <summary>Details</summary>
Motivation: 协变量调整能提高临床试验的精度和功效，但如何将其应用于胜率比尚不明确，需要建立理论框架。

Method: 通过建立胜率比与边际概率指数的联系，利用成熟的协变量调整理论，开发了胜率比的协变量调整方法，并在基于CANTOS试验特征的合成数据和模拟数据上验证。

Result: 当基线协变量对结局有预后价值时，协变量调整的胜率比确实能提高统计功效，但在小样本下会轻微增加I类错误率。

Conclusion: 协变量调整的胜率比方法可行且有效，能提高临床试验的分析效率，但需注意小样本下的I类错误率控制。

Abstract: Covariate adjustment can enhance precision and power in clinical trials, yet its application to the win odds remains unclear. The win odds is an extension of the win ratio that includes ties. In their original form, both methods rely on comparing each individual from the treatment group to each individual from the control group in pairwise manner, and count the number of wins, losses, and ties from these pairwise comparisons. A priori, it is not clear how covariate adjustment can be implemented for the win odds. To address this, we establish a connection between the win odds and the marginal probabilistic index, a measure for which covariate adjustment theory is well-developed. Using this connection, we show how covariate adjustment for the win odds is possible, leading to potentially more precise estimators and larger power as compared to the unadjusted win odds. We present the underlying theory for covariate adjustment for the win odds in an accessible way and apply the method on synthetic data based on the CANTOS trial (ClinicalTrials.gov identifier: NCT01327846) characteristics and on simulated data to study the operating characteristics of the method. We observe that there is indeed a potential gain in power when the win odds are adjusted for baseline covariates if the baseline covariates are prognostic for the outcome. This comes at the cost of a slight inflation of the type I error rate for small sample sizes.

</details>


### [7] [Consistent detection and estimation of multiple structural changes in functional data: unsupervised and supervised approaches](https://arxiv.org/abs/2511.14353)
*Sourav Chakrabarty,Anirvan Chakraborty,Shyamal K. De*

Main category: stat.ME

TL;DR: 开发基于最大均值差异(MMD)的函数数据变点检测算法，涵盖无监督、监督和半监督三种场景，能够检测一般分布变化。


<details>
  <summary>Details</summary>
Motivation: 解决函数数据中未知数量变点的检测问题，利用MMD度量希尔伯特空间中分布差异，建立变点位置与MMD曲线局部最大值的关系。

Method: 使用MMD度量分布差异，在无监督情况下测试变点显著性，在监督和半监督情况下合并相似组以利用先验信息。

Result: 算法在单变点和多变点设置下具有强一致性，在监督情况下满足顺序保持特性，在多种数据集上优于现有方法。

Conclusion: 提出的MMD-based变点检测算法能够有效检测函数数据中的分布变化，在不同监督水平下均表现良好。

Abstract: We develop algorithms for detecting multiple changepoints in functional data when the number of changepoints is unknown (unsupervised case), when it is specified apriori (supervised case), and when certain bounds are available (semi-supervised case). These algorithms utilize the maximum mean discrepancy (MMD) measure between distributions on Hilbert spaces. We develop an oracle analysis of the changepoint detection problem which reveals an interesting relationship between the true changepoint locations and the local maxima of the oracle MMD curve. The proposed algorithms are shown to detect general distributional changes by exploiting this connection. In the unsupervised case, we test the significance of a potential changepoint and establish its consistency under the single changepoint setting. We investigate the strong consistency of the changepoint estimators in both single and multiple changepoint settings. In both supervised and semi-supervised scenarios, we include a step to merge consecutive groups that are similar to appropriately utilize the prior information about the number of changepoints. In the supervised scenario, the algorithm satisfies an order-preserving property: the estimated changepoints are contained in the true set of changepoints in the underspecified case, while they contain the true set under overspecification. We evaluate the performance of the algorithms on a variety of datasets demonstrating the superiority of the proposed algorithms compared to some of the existing methods.

</details>


### [8] [Teaching Longitudinal Linear Mixed Models End-to-End: A Reproducible Case Study in Mouse Body-Weight Growth](https://arxiv.org/abs/2511.14523)
*Sunday A. Adetunji*

Main category: stat.ME

TL;DR: 本文提供了一个完整的纵向线性混合模型工作流程案例研究，使用小鼠体重实验数据，从数据整理、探索性分析到模型选择、诊断和对比分析。


<details>
  <summary>Details</summary>
Motivation: 线性混合效应模型在分析纵向连续数据中很重要，但学习者通常只能看到分散的公式或软件输出，缺乏连贯的工作流程。需要提供一个可重复的案例研究来连接问题、模型构建、诊断和解释。

Method: 重新分析已发表的小鼠体重实验数据，使用31只小鼠在12周内的体重测量。通过数据重塑、剖面图分析，拟合三种随机截距线性混合模型：共同斜率模型、完全交互模型和简约模型。使用最大似然、AIC、BIC、似然比检验比较模型，并使用线性对比估计组间差异。

Result: 简约模型与完全交互模型拟合效果相当，但明显优于共同斜率模型。结果显示两个组增长相似且较小，第三组增长更陡峭，超重增长的对比具有高度显著性。

Conclusion: 该案例研究提供了一个完整的、可执行的纵向线性混合建模工作流程，从原始数据和探索性图表到模型选择、诊断和针对性对比。通过明确科学问题到模型项和可估计对比的映射，并提供R代码和逐步检查清单，可作为生物统计学、流行病学及相关领域教学和应用工作的实用模板。

Abstract: Background: Linear mixed-effects models are central for analyzing longitudinal continuous data, yet many learners meet them as scattered formulas or software output rather than as a coherent workflow. There is a need for a single, reproducible case study that links questions, model building, diagnostics, and interpretation.
  Methods: We reanalyze a published mouse body-weight experiment with 31 mice in three groups weighed weekly for 12 weeks. After reshaping the data to long format and using profile plots to motivate linear time trends, we fit three random-intercept linear mixed models: a common-slope model, a fully interacted group-by-time model, and a parsimonious model with group-specific intercepts, a shared slope for two groups, and an extra slope for the third. Models are compared using maximum likelihood, AIC, BIC, and likelihood ratio tests, and linear contrasts are used to estimate group differences in weekly means and 12 week gains.
  Results: The parsimonious model fits as well as the fully interacted model and clearly outperforms the common-slope model, revealing small and similar gains in two groups and much steeper growth in the third, with highly significant contrasts for excess weight gain.
  Interpretation: This case study gives a complete, executable workflow for longitudinal linear mixed modeling, from raw data and exploratory plots through model selection, diagnostics, and targeted contrasts. By making explicit the mapping from scientific questions to model terms and estimable contrasts, and by providing R code and a stepwise checklist, it serves as a practical template for teaching and applied work in biostatistics, epidemiology, and related fields

</details>


### [9] [A Bayesian INLA-SPDE Approach to Spatio-Temporal Point-Grid Fusion with Change-of-Support and Misaligned Covariates](https://arxiv.org/abs/2511.14535)
*Weiyue Zheng,Andrew Elliott,Claire Miller,Marian Scott*

Main category: stat.ME

TL;DR: 提出了一个时空数据融合框架，用于融合点数据和网格数据，解决不同空间支持下的变量观测问题，通过潜在高斯场和特定源观测算子实现连续空间表示和变化支持处理。


<details>
  <summary>Details</summary>
Motivation: 解决不同空间分辨率、时间分辨率和测量误差的多种数据源融合问题，特别是处理变化支持和协变量错位的情况。

Method: 使用带有Matérn-SPDE先验的潜在高斯场提供连续空间表示，通过特定源观测算子将观测映射到点测量和网格平均值，结合时间依赖性进行预测。采用INLA和SPDE方法进行推理和预测。

Result: 通过模拟实验验证了方法的稳定性和性能，展示了多源数据融合相对于单源模型的优势。在苏格兰Elliot Water流域的土壤湿度制图应用中成功融合了现场传感器数据、卫星数据和海拔数据。

Conclusion: 该框架能够有效融合多种空间和时间分辨率的数据源，处理协变量错位问题，生成高分辨率地图并提供不确定性量化，在环境监测等领域具有实用价值。

Abstract: We propose a spatio-temporal data-fusion framework for point data and gridded data with variables observed on different spatial supports. A latent Gaussian field with a Matérn-SPDE prior provides a continuous space representation, while source-specific observation operators map observations to both point measurements and gridded averages, addressing change-of-support and covariate misalignment. Additionally incorporating temporal dependence enables prediction at unknown locations and time points. Inference and prediction are performed using the Integrated Nested Laplace Approximation and the Stochastic Partial Differential Equations approach, which delivers fast computation with uncertainty quantification. Our contributions are: a hierarchical model that jointly fuses multiple data sources of the same variable under different spatial and temporal resolutions and measurement errors, and a practical implementation that incorporates misaligned covariates via the same data fusion framework allowing differing covariate supports. We demonstrate the utility of this framework via simulations calibrated to realistic sensor densities and spatial coverage. Using the simulation framework, we explore the stability and performance of the approach with respect to the number of time points and data/covariate availability, demonstrating gains over single-source models through point and gridded data fusion. We apply our framework to soil moisture mapping in the Elliot Water catchment (Angus, Scotland). We fuse in-situ sensor data with aligned and misaligned covariates, satellite data and elevation data to produce daily high resolution maps with uncertainty.

</details>


### [10] [Amalgamations in a hierarchy as a way of variable selection in compositional data analysis](https://arxiv.org/abs/2511.14622)
*Michael Greenacre,Martin Graeve*

Main category: stat.ME

TL;DR: 提出了一种基于领域知识的组合数据变量选择方法，通过构建层次化的合并结构来创建对数比变量，并量化其对总方差的解释能力。


<details>
  <summary>Details</summary>
Motivation: 在组合数据分析中，基于领域知识将成分组合成合并子集，这些子集可以形成自然的分层结构，需要一种系统的方法来创建和评估这些合并变量。

Method: 通过构建层次化的合并结构，将合并转换为对数比，并计算其对总对数比方差的解释百分比来量化其有用性。

Result: 在海洋生物脂肪酸组成的案例中成功应用了该方法，展示了如何创建合并层次结构并评估其作为新变量的有效性。

Conclusion: 该方法为组合数据分析提供了一种替代的变量选择方法，能够基于领域知识系统地创建和评估有意义的合并变量。

Abstract: In certain fields where compositional data are studied, the compositional components, called parts, can be combined into certain subsets, called amalgamations, that are based on domain knowledge. Furthermore, these subsets can form a natural hierarchy of amalgamations subdividing into sub-amalgamations. The authors, a statistician and a biochemist, demonstrate how to create a hierarchy of amalgamations in the context of fatty acid compositions in a sample of marine organisms. Following a tradition in compositional data analysis, these amalgamations are transformed to logratios, and their usefulness as new variables is quantified by the percentage of total logratio variance that they explain. This method is proposed as an alternative method of variable selection in compositional data analysis.

</details>


### [11] [Scalable and Efficient Multiple Imputation for Case-Cohort Studies via Influence Function-Based Supersampling](https://arxiv.org/abs/2511.14692)
*Jooho Kim,Yei Eun Shin*

Main category: stat.ME

TL;DR: 提出了一种基于影响函数的超采样方法，用于解决两阶段抽样设计中大规模队列多重插补计算成本高的问题，通过权重校准实现高效估计。


<details>
  <summary>Details</summary>
Motivation: 两阶段抽样设计在流行病学研究中广泛应用，但大规模队列的多重插补计算成本过高甚至不可行。现有随机超采样方法效率较低，需要更高效的解决方案。

Method: 提出了基于影响函数的超采样方法，结合权重校准技术，仅需小规模超样本即可达到与全队列插补相当的效率。

Result: 模拟研究和NIH-AARP真实数据应用表明，该方法显著降低计算负担，在估计高维昂贵生物标志物的风险比时特别有效。

Conclusion: ISS方法提供了一种计算高效且统计效率高的解决方案，特别适用于包含高维昂贵生物标志物的两阶段抽样研究。

Abstract: Two-phase sampling designs have been widely adopted in epidemiological studies to reduce costs when measuring certain biomarkers is prohibitively expensive. Under these designs, investigators commonly relate survival outcomes to risk factors using the Cox proportional hazards model. To fully utilize covariates collected in phase 1, multiple imputation methods have been developed to impute missing covariates for individuals not included in the phase 2 sample. However, performing multiple imputation on large-scale cohorts can be computationally intensive or even infeasible. To address this issue, Borgan et al. (2023) proposed a random supersampling (RSS) approach that randomly selects a subset of cohort members for imputation, albeit at the cost of reduced efficiency. In this study, we propose an influence function-based supersampling (ISS) approach with weight calibration. The method achieves efficiency comparable to imputing the entire cohort, even with a small supersample, while substantially reducing computational burden. We further demonstrate that the proposed method is particularly advantageous when estimating hazard ratios for high-dimensional expensive biomarkers. Extensive simulation studies are conducted, and a real data application using the National Institutes of Health-American Association of Retired Persons (NIH-AARP) Diet and Health Study is provided to illustrate the effectiveness of the proposed method.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [12] [Empirical Likelihood for Random Forests and Ensembles](https://arxiv.org/abs/2511.13934)
*Harold D. Chiang,Yukitoshi Matsushita,Taisuke Otsu*

Main category: stat.ML

TL;DR: 本文为随机森林等集成方法开发了经验似然框架，通过利用集成预测中的不完全U统计量结构，构建了渐近卡方分布的经验似然统计量，并提出了修正版本以在稀疏子采样情况下保持枢轴性。


<details>
  <summary>Details</summary>
Motivation: 为随机森林及相关集成方法提供基于似然的统计不确定性量化方法，解决现有推断方法的局限性。

Method: 利用集成预测中的不完全U统计量结构构建经验似然统计量，在稀疏子采样情况下通过简单调整提出修正经验似然以恢复枢轴性。

Result: 理论分析和模拟实验表明，修正经验似然相对于现有推断方法实现了准确的覆盖率和实际可靠性。

Conclusion: 该方法保留了经验似然的关键特性，同时保持计算效率，为随机森林提供了有效的统计推断工具。

Abstract: We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods.

</details>


### [13] [Knowledge vs. Experience: Asymptotic Limits of Impatience in Edge Tenants](https://arxiv.org/abs/2511.13763)
*Anthony Kiggundu,Bin Han,Hans D. Schotten*

Main category: stat.ML

TL;DR: 研究两种信息反馈（马尔可夫估计器和在线训练的actor-critic）对双M/M/1系统中顾客放弃和换队行为的影响，发现有限规模下信息价值重要，但渐近状态下不同信息模型收敛到相同结果。


<details>
  <summary>Details</summary>
Motivation: 探讨在双队列系统中，不同类型的信息反馈如何影响顾客的放弃决策和换队行为，为低成本、支持换队的系统设计提供理论指导。

Method: 使用分析性方法研究不等服务率和总时间耐心下的系统行为，并通过实证验证理论结果，比较学习型和分析型信息反馈的差异。

Result: 理论证明当积压趋于无穷时，总等待时间线性增长，放弃不可避免，成功换队概率趋近于零；实证显示在有限规模下不同信息反馈导致不同延迟和放弃率，但渐近收敛。

Conclusion: 信息价值在有限规模下重要，但在渐近状态下不重要，这为轻量级遥测和决策逻辑设计提供了指导。

Abstract: We study how two information feeds, a closed-form Markov estimator of residual sojourn and an online trained actor-critic, affect reneging and jockeying in a dual M/M/1 system. Analytically, for unequal service rates and total-time patience, we show that total wait grows linearly so abandonment is inevitable and the probability of a successful jockey vanishes as the backlog approaches towards infinity. Furthermore, under a mild sub-linear error condition both information models yield the same asymptotic limits (robustness). We empirically validate these limits and quantify finite backlog differences. Our findings show that learned and analytic feeds produce different delays, reneging rates and transient jockeying behavior at practical sizes, but converge to the same asymptotic outcome implied by our theory. The results characterize when value-of-information matters (finite regimes) and when it does not (asymptotics), informing lightweight telemetry and decision-logic design for low-cost, jockeying-aware systems.

</details>


### [14] [Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands](https://arxiv.org/abs/2511.13911)
*Vasiliki Tassopoulou,Charis Stamouli,Haochang Shou,George J. Pappas,Christos Davatzikos*

Main category: stat.ML

TL;DR: 该论文提出了一种用于生物标志物轨迹预测的保形预测方法，通过构建具有统计保证的预测区间来量化不确定性，在阿尔茨海默病神经影像数据上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生物标志物轨迹预测方法存在不确定性风险，限制了临床部署。需要开发能够提供统计保证的预测区间的方法，确保临床决策的安全性。

Method: 扩展保形预测方法到随机时间轨迹场景，提出新的非保形性评分，构建具有用户指定覆盖概率的预测区间，并开发群体条件保形区间来处理群体异质性。

Result: 方法在阿尔茨海默病两种脑生物标志物上验证，预测区间始终达到期望覆盖率且比基线更紧凑。不确定性校准风险评分比标准风险评分多识别17.5%的高风险受试者。

Conclusion: 该方法为生物标志物轨迹预测提供了统计保证的不确定性量化，在临床决策中具有实际价值，能够更可靠地识别疾病进展高风险人群。

Abstract: Despite recent progress in predicting biomarker trajectories from real clinical data, uncertainty in the predictions poses high-stakes risks (e.g., misdiagnosis) that limit their clinical deployment. To enable safe and reliable use of such predictions in healthcare, we introduce a conformal method for uncertainty-calibrated prediction of biomarker trajectories resulting from randomly-timed clinical visits of patients. Our approach extends conformal prediction to the setting of randomly-timed trajectories via a novel nonconformity score that produces prediction bands guaranteed to cover the unknown biomarker trajectories with a user-prescribed probability. We apply our method across a wide range of standard and state-of-the-art predictors for two well-established brain biomarkers of Alzheimer's disease, using neuroimaging data from real clinical studies. We observe that our conformal prediction bands consistently achieve the desired coverage, while also being tighter than baseline prediction bands. To further account for population heterogeneity, we develop group-conditional conformal bands and test their coverage guarantees across various demographic and clinically relevant subpopulations. Moreover, we demonstrate the clinical utility of our conformal bands in identifying subjects at high risk of progression to Alzheimer's disease. Specifically, we introduce an uncertainty-calibrated risk score that enables the identification of 17.5% more high-risk subjects compared to standard risk scores, highlighting the value of uncertainty calibration in real-world clinical decision making. Our code is available at github.com/vatass/ConformalBiomarkerTrajectories.

</details>


### [15] [Splat Regression Models](https://arxiv.org/abs/2511.14042)
*Mara Daniels,Philippe Rigollet*

Main category: stat.ML

TL;DR: 提出了一种称为Splat回归模型的高度表达性函数逼近器，使用异质性和各向异性凸函数的混合作为输出，通过Wasserstein-Fisher-Rao梯度流优化混合度量空间。


<details>
  <summary>Details</summary>
Motivation: 为高斯Splatting等最先进技术提供统一的理论框架，明确区分逆问题、模型和优化算法，实现高可解释性和准确性。

Method: 使用称为splat的异质性和各向异性凸函数混合作为模型输出，每个splat由输出向量加权，通过Wasserstein-Fisher-Rao梯度流优化混合度量空间。

Result: 恢复了流行的高斯Splatting方法作为特例，通过数值实验证明该模型和算法是解决涉及低维数据的各种逼近、估计和逆问题的灵活且有前景的方法。

Conclusion: Splat回归模型提供了一个统一的理论框架，能够实现高可解释性和准确性，是解决低维数据问题的有前景方法。

Abstract: We introduce a highly expressive class of function approximators called Splat Regression Models. Model outputs are mixtures of heterogeneous and anisotropic bump functions, termed splats, each weighted by an output vector. The power of splat modeling lies in its ability to locally adjust the scale and direction of each splat, achieving both high interpretability and accuracy. Fitting splat models reduces to optimization over the space of mixing measures, which can be implemented using Wasserstein-Fisher-Rao gradient flows. As a byproduct, we recover the popular Gaussian Splatting methodology as a special case, providing a unified theoretical framework for this state-of-the-art technique that clearly disambiguates the inverse problem, the model, and the optimization algorithm. Through numerical experiments, we demonstrate that the resulting models and algorithms constitute a flexible and promising approach for solving diverse approximation, estimation, and inverse problems involving low-dimensional data.

</details>


### [16] [SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation](https://arxiv.org/abs/2511.14146)
*Renjie Chen,Viet Anh Nguyen,Huifu Xu*

Main category: stat.ML

TL;DR: 提出一种分布鲁棒的协方差矩阵和精度矩阵联合估计方法，通过最小化最坏情况下的加权损失函数，得到非线性收缩估计量，能够校正经验估计量的谱偏差。


<details>
  <summary>Details</summary>
Motivation: 传统的协方差和精度矩阵估计方法存在谱偏差问题，需要一种能够同时估计这两个矩阵并改善条件数的鲁棒方法。

Method: 使用分布鲁棒优化框架，在凸谱散度定义的模糊集上最小化最坏情况下的Frobenius损失和Stein损失加权和，转化为凸优化问题求解。

Result: 提出的SCOPE估计量是非线性收缩估计量，能够有效校正谱偏差，改善条件数，在合成和真实数据实验中表现优于现有方法。

Conclusion: 该方法提供了一种有效的协方差和精度矩阵联合估计框架，通过参数调整实现渐近最优性能，具有实际应用价值。

Abstract: We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.

</details>


### [17] [Causal Discovery on Higher-Order Interactions](https://arxiv.org/abs/2511.14206)
*Alessio Zanga,Marco Scutari,Fabio Stella*

Main category: stat.ML

TL;DR: 本文提出了一种基于高阶结构的因果发现DAG聚合新方法，在数据稀缺时通过bagging提升置信度评估，相比现有方法在低样本量和高维设置下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有DAG聚合方法仅考虑边级别的置信度，忽略了复杂的高阶边结构信息，导致在数据稀缺时置信度评估不够准确。

Method: 提出了基于高阶结构的理论框架，并开发了新的DAG聚合算法，通过考虑更复杂的边结构关系来改进聚合效果。

Result: 模拟研究表明该方法计算效率高且效果显著，在低样本量和高维设置下优于现有最先进方法。

Conclusion: 基于高阶结构的DAG聚合方法能够更有效地利用bagging结果，在数据稀缺情况下提供更可靠的因果发现置信度评估。

Abstract: Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.

</details>


### [18] [Skewness-Robust Causal Discovery in Location-Scale Noise Models](https://arxiv.org/abs/2511.14441)
*Daniel Klippert,Alexander Marx*

Main category: stat.ML

TL;DR: 提出了SkewD算法，用于在偏态噪声分布下进行双变量因果发现，解决了现有方法在偏态噪声下可靠性下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有位置-尺度噪声模型(LSNM)在因果发现中通常假设对称噪声分布(如正态分布)，但在真实世界数据中噪声往往是偏态的，这导致现有方法的可靠性降低。

Method: SkewD是一种基于似然的方法，将通常的正态分布框架扩展到偏态正态设置，结合启发式搜索和期望条件最大化算法进行参数估计。

Result: 在具有偏态噪声的新合成数据集和已建立基准数据集上的实验表明，SkewD表现出强大性能，与先前工作相比，在高偏度下保持稳健。

Conclusion: SkewD能够可靠地在对称和偏态噪声下进行因果推断，解决了现有方法在偏态噪声场景下的局限性。

Abstract: To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \to Y$ and $Y \to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.

</details>


### [19] [DeepBlip: Estimating Conditional Average Treatment Effects Over Time](https://arxiv.org/abs/2511.14545)
*Haorui Ma,Dennis Frauen,Stefan Feuerriegel*

Main category: stat.ML

TL;DR: DeepBlip是首个用于结构嵌套均值模型(SNMMs)的神经网络框架，通过双重优化技巧实现所有blip函数的同步学习，解决了传统g-估计无法端到端训练的问题。


<details>
  <summary>Details</summary>
Motivation: SNMMs能够将时间序列治疗效应分解为局部化的blip效应，但缺乏神经网络框架，因为其固有的顺序g-估计方案阻碍了端到端的基于梯度的训练。

Method: 提出DeepBlip框架，采用双重优化技巧实现所有blip函数的同步学习，无缝集成LSTM或transformer等序列神经网络来捕捉复杂时间依赖关系，使用Neyman-正交损失函数确保对nuisance模型误设的鲁棒性。

Result: 在多个临床数据集上评估，DeepBlip达到了最先进的性能表现。

Conclusion: DeepBlip是首个神经SNMM框架，能够正确调整时变混杂因素产生无偏估计，其设计确保了鲁棒性和最优治疗策略的高效离线评估。

Abstract: Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.

</details>


### [20] [Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization](https://arxiv.org/abs/2511.14710)
*Zonghao Chen,Atsushi Nitanda,Arthur Gretton,Taiji Suzuki*

Main category: stat.ML

TL;DR: 本文首次建立了神经网络在非参数工具变量回归中两阶段最小二乘法的全局收敛性结果，通过提出一种新的完全一阶算法F²BMLD来解决双层优化问题，并提供了收敛界和泛化界。


<details>
  <summary>Details</summary>
Motivation: 解决非参数工具变量回归中两阶段最小二乘法面临的全局收敛性挑战，特别是在双层优化问题空间中的收敛保证问题。

Method: 采用均值场朗之万动力学的提升视角，结合最近发展的惩罚梯度方法将双层优化转化为拉格朗日问题，提出完全一阶算法F²BMLD。

Result: 建立了首个全局收敛结果，提供了收敛界和泛化界，揭示了拉格朗日乘子在优化和统计保证之间的权衡关系，并在离线强化学习基准上验证了有效性。

Conclusion: 该研究成功解决了NPIV中2SLS的全局收敛问题，提出的F²BMLD算法在理论和实验上都表现出色，为相关领域提供了新的解决方案。

Abstract: We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [21] [Waiting for Dabo:A machine learning model for P4 college football coaching hires](https://arxiv.org/abs/2511.14035)
*Michael Schuckers,Austin Hayes*

Main category: stat.AP

TL;DR: 基于103名P4大学橄榄球教练的招聘数据，构建了预测教练在新学校成功率的统计模型，使用29个变量和交叉验证正则化线性回归，准确率为66%，但预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究大学橄榄球教练招聘的成功因素，为学校提供数据驱动的招聘决策依据。

Method: 收集103名P4大学橄榄球教练的背景数据，使用29个变量构建交叉验证正则化线性回归模型，以Bill Connelly的SP+评分作为成功指标。

Result: 模型识别成功教练的准确率为66%，重要预测因素包括：曾任大学主教练、赢得过联盟冠军、曾任进攻协调员、年龄和招聘学校过去15年的球队质量。

Conclusion: 虽然发现了一些重要预测因素，但这些趋势较弱，无法高度准确地预测教练招聘的成功与否。

Abstract: Using data on 103 recent P4 college football hires, we built a statistical model for predicting a coach's success at their new school. For each hire, we collected data about their background and experiences, the previous success as a head coach or coordinator and their success since hiring. Over 50 variables on these factors were recorded though we used 29 of these in building our predictive model. Our measure of success is based upon Bill Connelly's SP+ team ratings relative to the performance on the same metric of the school in the 15 year prior to their selection as head coach. Using a cross-validated regularized linear regression, we obtain a predictive model for coaching success. Among the important factors for predicting a successful hire are having been a previous college head coach, having won a prior conference championship as a head coach, leaving a job as an Offensive Coordinator, age and quality of the hiring school's team in the previous 15 years. While we do find these factors are important for the prediction of a successful coaching hire, the trends here are weak. With 66% accuracy, the model does identify coaching hires that will outperform team performance in the 15 years before the hire. However, no combination of these factors leads to high predictability of identifying a successful coaching hire. All of the data and code for this paper are available in a Github repository.

</details>


### [22] [Model-Based Clustering of Football Event Sequences: A Marked Spatio-Temporal Point Process Mixture Approach](https://arxiv.org/abs/2511.14297)
*Koffi Amezouwui,Brigitte Gelein,Matthieu Marbac,Anthony Sorel*

Main category: stat.AP

TL;DR: 提出了一种用于足球事件数据的混合模型，通过聚类完整进攻回合来揭示其时间、序列和空间结构，支持战术分析和虚拟训练环境开发。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单个事件，缺乏对完整进攻回合的聚类分析。本文旨在开发能够系统分析足球进攻回合时空结构的模型，为战术分析提供新视角。

Method: 使用混合模型将进攻回合聚类，每个组件建模为标记时空点过程：事件类型遵循有限马尔可夫链，事件时间遵循条件Gamma过程，空间位置通过截断布朗运动演化。采用广义期望最大化算法进行最大似然估计。

Result: 应用于38场法甲联赛数据，成功识别出雷恩队面对的不同防守进攻模式，验证了模型在揭示战术模式方面的有效性。

Conclusion: 该混合模型框架能够对完整进攻回合进行原则性聚类，为足球战术分析提供了新的方法论支持，并为开发逼真的虚拟训练环境奠定了基础。

Abstract: We propose a novel mixture model for football event data that clusters entire possessions to reveal their temporal, sequential, and spatial structure. Each mixture component models possessions as marked spatio-temporal point processes: event types follow a finite Markov chain with an absorbing state for ball loss, event times follow a conditional Gamma process to account for dispersion, and spatial locations evolve via truncated Brownian motion. To aid interpretation, we derive summary indicators from model parameters capturing possession speed, number of events, and spatial dynamics. Parameters are estimated through maximum likelihood via Generalized Expectation-Maximization algorithm. Applied to StatsBomb data from 38 Ligue 1 matches (2020/2021), our approach uncovers distinct defensive possession patterns faced by Stade Rennais. Unlike previous approaches focusing on individual events, our mixture structure enables principled clustering of full possessions, supporting tactical analysis and the future development of realistic virtual training environments.

</details>


### [23] [Nonlinear Coherence for Vector Time Series: Defining Region-to-Region Functional Brain Connectivity](https://arxiv.org/abs/2511.14417)
*Paolo Victor Redondo,Raphaël Huser,Hernando Ombao*

Main category: stat.AP

TL;DR: 提出了一种新的非线性向量相干性(NVC)方法，用于分析脑电图(EEG)中脑区间的非线性相互作用，改进了传统的通道间连接分析方法，能够有效区分阿尔茨海默病和额颞叶痴呆的功能连接模式。


<details>
  <summary>Details</summary>
Motivation: 脑功能连接改变是阿尔茨海默病和额颞叶痴呆等神经退行性疾病的特征。EEG作为一种非侵入性且成本效益高的技术，在早期检测和鉴别诊断方面具有潜力，但传统通道间分析方法存在局限性。

Method: 提出非线性向量相干性(NVC)来捕捉两个多元时间序列之间的非线性相互作用，建立更自然的脑区到脑区连接框架。引入基于秩的推断程序进行快速、无分布估计，以及完全非参数的光谱独立性检验。

Result: 数值实验验证了推断方法的性能。应用于静息态EEG数据显示，NVC能够揭示具有诊断意义的连接模式，有效区分健康个体与AD、FTD患者。

Conclusion: NVC提供了一种研究脑区间依赖关系的新方法，能够识别与AD和FTD相关的功能连接动态改变，为神经退行性疾病的早期检测和鉴别诊断提供了有前景的工具。

Abstract: Alterations in functional brain connectivity characterize neurodegenerative disorders such as Alzheimer's disease (AD) and frontotemporal dementia (FTD). As a non-invasive and cost-effective technique, electroencephalography (EEG) is gaining increasing attention for its potential to identify reliable biomarkers for early detection and differential diagnosis of AD and FTD. Considering the behavioral similarities of signals from adjacent EEG channels, we propose a new spectral dependence measure, the nonlinear vector coherence (NVC), to capture beyond-linear interactions between oscillations of two multivariate time series observed from distinct brain regions. This addresses the limitations of conventional channel-to-channel approaches and defines a more natural region-to-region connectivity framework in the frequency domain. As a result, the NVC measure offers a new approach to investigate dependence between brain regions, which then enables to identify altered functional connectivity dynamics associated with AD and FTD. We further introduce a rank-based inference procedure that enables fast and distribution-free estimation of the proposed measure, as well as a fully nonparametric test for spectral independence. The empirical performance of our proposed inference methodology is demonstrated through extensive numerical experiments. An application to resting-state EEG data reveals that our novel NVC measure uncovers distinct and diagnostically meaningful connectivity patterns which effectively discriminate healthy individuals from those with AD and FTD.

</details>


### [24] [Darts Analysis](https://arxiv.org/abs/2511.14537)
*Ayham Makhamra,Yelyzaveta Satynska,Michael Weselcouch*

Main category: stat.AP

TL;DR: 本文研究了五种数学模型在预测业余飞镖比赛结果方面的有效性，包括空模型、逻辑回归模型、基础模拟模型、时间调整模拟模型和基于当前比分更新的Massey模型变体。


<details>
  <summary>Details</summary>
Motivation: 评估不同数学模型在预测业余飞镖比赛结果时的表现，特别是在比赛进行中根据实时比分更新预测的能力。

Method: 使用包含Roanoke College学生、教职员工业余飞镖比赛的数据集训练和测试五种模型，通过Brier分数和投注游戏两种方法评估模型性能。

Result: 在两种评估方法中，基于比分更新的Massey模型表现最佳。

Conclusion: 基于比分更新的Massey模型框架可以适应其他竞技场景的应用。

Abstract: In this paper we examine the effectiveness of five mathematical models used to predict the outcomes of amateur darts games. These models not only predict the outcomes at the start of the game, but also update their estimations as the game score changes. The models were trained and tested on a dataset consisting of games played by amateur players involving students, faculty, and staff at Roanoke College. The five models are: the null model, which is based only on the live scores, a logistic regression model, a basic simulation model, a time-adjusted simulation model, and a new variation of the Massey model which updates based on the current score. We evaluate these models using two approaches. First, we compare their Brier scores. Second, we conduct head-to-head comparisons in a betting game in which one model sets the betting odds while the other places bets. In both cases, model performance is assessed not only at the start of the game but also at the start of each round. Across both evaluation methods, the score-dependent Massey model performs the best. We conclude by illustrating how this score-dependent Massey model framework can be adapted to other competitive settings beyond darts.

</details>


### [25] [Decoupling Urban Food Accessibility Resilience during Disasters through Time-Series Analysis of Human Mobility and Power Outages](https://arxiv.org/abs/2511.14706)
*Junwei Ma,Bo Li,Xiangpeng Li,Ali Mostafavi*

Main category: stat.AP

TL;DR: 本研究量化了飓风期间电网故障与食品服务可及性之间的时间耦合关系，发现食品可及性在飓风登陆日达到最低点，而停电严重程度在两天后达到峰值，存在一致的2天滞后效应。


<details>
  <summary>Details</summary>
Motivation: 灾害引发的停电会在城市生命线系统中产生级联破坏，但电网故障与基本服务可及性之间的时间耦合关系尚未得到充分量化。

Method: 整合约17.3万条15分钟停电记录和超过125万次对3187个食品设施的访问数据，构建停电特征和食品可及性指标的每日指数，通过滞后相关性分析和DTW k-means聚类识别恢复模式。

Result: 分析揭示了持续的2天滞后效应：食品可及性在7月8日飓风登陆时达到最低点，而停电严重程度在7月10日达到峰值；识别出4种复合类型；道路网络稀疏性比收入更能决定可及性损失的深度和持续性。

Conclusion: 该框架可将动态耦合模式转化为针对性的干预措施，用于阶段敏感型恢复和公平意识型准备，并可推广到其他生命线和灾害类型。

Abstract: Disaster-induced power outages create cascading disruptions across urban lifelines, yet the timed coupling between grid failure and essential service access remains poorly quantified. Focusing on Hurricane Beryl in Houston (2024), this study integrates approximately 173000 15-minute outage records with over 1.25 million visits to 3187 food facilities to quantify how infrastructure performance and human access co-evolve. We construct daily indices for outage characteristics (intensity, duration) and food access metrics (redundancy, frequency, proximity), estimate cross-system lags through lagged correlations over zero to seven days, and identify recovery patterns using DTW k-means clustering. Overlaying these clusters yields compound power-access typologies and enables facility-level criticality screening. The analysis reveals a consistent two-day lag: food access reaches its nadir on July 8 at landfall while outage severity peaks around July 10, with negative correlations strongest at a two-day lag and losing significance by day four. We identify four compound typologies from high/low outage crossed with high/low access disruption levels. Road network sparsity, more than income, determines the depth and persistence of access loss. Through this analysis, we enumerate 294 critical food facilities in the study area requiring targeted continuity measures including backup power, microgrids, and feeder prioritization. The novelty lies in measuring interdependency at daily operational resolution while bridging scales from communities to individual facilities, converting dynamic coupling patterns into actionable interventions for phase-sensitive restoration and equity-aware preparedness. The framework is transferable to other lifelines and hazards, offering a generalizable template for diagnosing and mitigating cascading effects on community access during disaster recovery.

</details>


### [26] [Bayesian Optimal Phase II design with optimised stopping boundaries and response-adaptive randomisation](https://arxiv.org/abs/2511.14714)
*Connor Fitchett,Ayon Mukherjee,Sofía S. Villar,David S. Robertson*

Main category: stat.AP

TL;DR: BOP2框架通过引入贝叶斯响应自适应随机化和最优中期分析位置，改进了治疗效果和样本量效率


<details>
  <summary>Details</summary>
Motivation: BOP2框架虽然灵活，但其等比例随机化和等间隔中期分析设计可能不是最优的，需要改进其操作特性

Method: 在BOP2框架中引入贝叶斯响应自适应随机化和最优中期分析位置，通过模拟研究评估改进效果

Result: 改进后的设计能够提高最佳治疗分配比例并减少预期样本量，同时对统计功效影响很小

Conclusion: 这些改进为实践者提供了实用的实施建议，通过自适应设计优化了BOP2框架的性能

Abstract: The Bayesian Optimal Phase II (BOP2) framework is a flexible trial design that can naturally facilitate complex adaptations due to its Bayesian setting. BOP2 uses equal randomisation and equally placed interim analyses in its design, but it is unclear whether these give the best operating characteristics. By incorporating Bayesian Response-Adaptive Randomisation (BRAR) and optimal interim analysis placement, we show that allocation to the best treatment and expected sample size can be improved with minimal impact on power. We discuss recommendations on implementing these adaptations, using simulation-based evidence, to give practical advice to practitioners. Reproducible code for the simulations is freely provided.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [27] [Uncertainty assessment of spatial dynamic microsimulations](https://arxiv.org/abs/2511.14294)
*Morgane Dumont,Ahmed Alsaloum,Julian Ernst,Jan Weymeirsch,Ralf Münnich*

Main category: stat.CO

TL;DR: 论文通过方差敏感性分析发现，在德国MikroSim模型的就业模块中，定性建模选择比常用的系数和参数不确定性对模拟结果影响更大。


<details>
  <summary>Details</summary>
Motivation: 空间动态微观模拟在预测地理参考单元时存在固有不确定性，但这些不确定性因素很少被系统研究。需要识别影响模拟输出的关键不确定性因素以改进模型设计和结果解释。

Method: 应用基于方差的敏感性分析方法，分析德国MikroSim模型就业模块中的直接和间接效应。

Result: 研究发现定性建模选择比系数和参数不确定性对模拟结果影响更大，简单的汇总指标无法充分捕捉整体模型不确定性。

Conclusion: 模型设计者需要在构建微观模拟时考虑更广泛的不确定性来源，并更谨慎地设计模拟设置和传达结果。

Abstract: Spatial dynamic microsimulations probabilistically project geographically referenced units with individual characteristics over time. Like any projection method, their outcomes are inherently uncertain and sensitive to multiple factors. However, such factors are rarely addressed. Applying variance-based sensitivity analysis to both direct and indirect effects within the employment module of the MikroSim model for Germany, we show that commonly considered sources of uncertainty, namely coefficient and parameter uncertainty, are less influential than qualitative modeling choices. Because dynamic microsimulations are inherently complex and are computationally intensive, it is crucial to consider potential factors of uncertainty and their influence on simulation outputs in order to more carefully design simulation setups and better communicate results. We find, that simple summary measures insufficiently capture overall model uncertainty and urge modelers to account for these broader sources when designing microsimulations and their results.

</details>


### [28] [Spatio-temporal Hawkes point processes: statistical inference and simulation strategies](https://arxiv.org/abs/2511.14509)
*Alba Bernabeu,Jorge Mateu*

Main category: stat.CO

TL;DR: 本文实现了时空Hawkes过程的两种模拟技术和三种统一的推断技术，并评估了这些方法的实际性能，同时提供了可复现的代码。


<details>
  <summary>Details</summary>
Motivation: 尽管时空Hawkes点过程在建模自激励行为方面被广泛使用，但缺乏统一的形式化框架，每篇论文都提出了不同的随机机制观点。

Method: 实现了两种模拟技术和三种统一的、自洽的推断技术，用于时空Hawkes过程的实际建模。

Result: 提供了这些方法的实际性能评估，并提供了有用的代码以确保可复现性。

Conclusion: 通过统一的框架和可复现的代码，促进了时空Hawkes过程建模方法的一致性和实用性。

Abstract: Spatio-temporal Hawkes point processes are a particularly interesting class of stochastic point processes for modeling self-exciting behavior, in which the occurrence of one event increases the probability of other events occurring. These processes are able to handle complex interrelationships between stochastic and deterministic components of spatio-temporal phenomena. However, despite its widespread use in practice, there is no common and unified formalism and every paper proposes different views of these stochastic mechanisms. With this in mind, we implement two simulation techniques and three unified, self-consistent inference techniques, which are widely used in the practical modeling of spatio-temporal Hawkes processes. Furthermore, we provide an evaluation of the practical performance of these methods, while providing useful code for reproducibility.

</details>


### [29] [Estimation of Spatial and Temporal Autoregressive Effects using LASSO - An Example of Hourly Particulate Matter Concentrations](https://arxiv.org/abs/2511.14666)
*Elkanah Nyabuto,Philipp Otto,Yarema Okhrin*

Main category: stat.CO

TL;DR: 提出使用LASSO方法估计时空自回归面板数据模型中的空间和时间效应，通过约束惩罚最大似然估计器同时估计权重矩阵和其他参数，并在巴伐利亚PM10浓度数据上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 需要估计时空面板数据中的空间和时间效应，特别是当空间权重矩阵未知时，传统方法难以同时估计权重矩阵和其他参数，LASSO方法可以解决这一问题并提高模型可解释性。

Method: 使用LASSO约束惩罚最大似然估计器，假设数据来自单变量随机过程并遵循时空自回归过程，包含空间/时间变化的外生回归项、时间自回归项和具有未知权重矩阵的空间自回归项。

Result: 蒙特卡洛模拟显示良好性能，时间点增加时精度提高；LASSO能有效区分空间权重和其他参数中的有意义关系；在巴伐利亚PM10浓度数据应用中，发现一些站点具有高空间依赖性，LASSO产生稀疏权重矩阵提高了可解释性。

Conclusion: LASSO方法能有效估计时空面板数据模型中的空间权重矩阵和其他参数，产生稀疏解提高可解释性，在环境监测数据应用中表现出良好性能。

Abstract: We present an estimation procedure of spatial and temporal effects in spatiotemporal autoregressive panel data models using the Least Absolute Shrinkage and Selection Operator, LASSO (Tibshirani, 1996). We assume that the spatiotemporal panel is drawn from a univariate random process and that the data follows a spatiotemporal autoregressive process which includes a regressive term with space-/ time-varying exogenous regressor, a temporal autoregressive term and a spatial autoregressive term with an unknown weights matrix. The aim is to estimate this weight matrix alongside other parameters using a constraint penalised maximum likelihood estimator. Monte Carlo simulations showed a good performance with the accuracy increasing with an increasing number of time points. The use of the LASSO technique also consistently distinguishes between meaningful relationships (non-zeros) from those that are not (existing zeros) in both the spatial weights and other parameters. This regularised estimation procedure is applied to hourly particulate matter concentrations (PM10) in the Bavaria region, Germany for the years 2005 to 2020. Results show some stations with a high spatial dependency, resulting in a greater influence of PM10 concentrations in neighbouring monitoring stations. The LASSO technique proved to produce a sparse weights matrix by shrinking some weights to zero, hence improving the interpretability of the PM concentration dependencies across measurement stations in Bavaria

</details>
