{"id": "2510.07518", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.07518", "abs": "https://arxiv.org/abs/2510.07518", "authors": ["Blake Hansen", "Dafne Zorzetto", "Valeria Edefonti", "Roberta De Vito"], "title": "Zero-Inflated Bayesian Multi-Study Infinite Non-Negative Matrix Factorization", "comment": null, "summary": "Understanding the association between dietary patterns and health outcomes,\nsuch as the cancer risk, is crucial to inform public health guidelines and\nshaping future dietary interventions. However, dietary intake data present\nseveral statistical challenges: they are high-dimensional, often sparse with\nexcess zeros, and exhibit heterogeneity driven by individual-level covariates.\nNon-Negative Matrix Factorization (NMF), commonly used to estimate patterns in\nhigh-dimensional count data, typically relies on Poisson assumptions and lacks\nthe flexibility to fully address these complexities. Additionally, integrating\ndata across multiple studies, such as case-control studies on cancer risk,\nrequires models that can share information across sources while preserving\nstudy-specific structure.\n  In this paper, we introduce a novel Bayesian NMF model that (i) jointly\nmodels multi-study count data to enable cross-study information sharing, (ii)\nincorporate a mixture component to account for zero inflation, and (iii)\nleverage flexible Bayesian non-parametric priors for characterizing the\nheterogeneity in pattern scores induced by the individual covariates. This\nstructure allows for clustering of individuals based on dietary profiles,\nenabling downstream association analyses with health outcomes. Through\nextensive simulation studies, we demonstrate that our model significantly\nimproves estimation accuracy compared to existing Bayesian NMF methods.\n  We further illustrate its utility through an application to multiple\ncase-control studies on diet and upper aero-digestive tract cancers,\nidentifying nutritionally meaningful dietary patterns. An R package\nimplementing our approach is available at\nhttps://github.com/blhansen/ZIMultiStudyNMF."}
{"id": "2510.07521", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07521", "abs": "https://arxiv.org/abs/2510.07521", "authors": ["Danielle Mccool", "Peter Lugtig", "Bella Struminskaya"], "title": "Integrating smart surveys with traditional surveys", "comment": null, "summary": "Smart surveys are surveys that make use of sensors and machine intelligence\nto reduce respondent burden and increase data quality. Smart surveys have been\ntests as a way to improve diary surveys in official statistics, where data are\ncollected on topics such as travel, time use and household expenditures. There\nare often inherent differences both in measurement and representation between\nsmart surveys and traditional diaries, which makes it difficult to integrate\nboth data sources in producing statistics over time, or within a mixed- or\nmulti-source context. This paper distinguishes two different approaches to\nintegration: the mixed-mode approach, which prioritizes outcome alignment and\nminimizes measurement differences for straightforward data merging, and the\nmultisource approach, which maintains inherent mode differences and integrates\ndata at the modeling stage, allowing exploitation of the strengths of each\nsource. Using travel surveys as an illustrative example, we explore the\nbenefits and drawbacks of each approach, and propose a decision framework to\nguide researchers in selecting the appropriate integration strategy."}
{"id": "2510.07608", "categories": ["stat.ME", "62G07, 62H30"], "pdf": "https://arxiv.org/pdf/2510.07608", "abs": "https://arxiv.org/abs/2510.07608", "authors": ["Jiajin Xie", "Yong Wang", "Eduardo García-Portugués"], "title": "Density estimation for compositional data using nonparametric mixtures", "comment": "26 pages, 12 figures, 11 tables", "summary": "Compositional data, representing proportions constrained to the simplex,\narise in diverse fields such as geosciences, ecology, genomics, and microbiome\nresearch. Existing nonparametric density estimation methods often rely on\ntransformations, which may induce substantial bias near the simplex boundary.\nWe propose a nonparametric mixture-based framework for density estimation on\ncompositions. Nonparametric Dirichlet mixtures are employed to naturally\naccommodate boundary values, thereby avoiding the transformation or\nzero-replacement, while also identifying components supported on the boundary,\nproviding reliable estimates for data with zero or near-zero values. Bandwidth\nselection and initialization schemes are addressed. For comparison,\nnonparametric Gaussian mixtures, coupled with log-ratio transformations, are\nalso considered. Extensive simulations show that the proposed estimators\noutperform existing approaches. Three real data applications, including GDP\ndata analysis, handwritten digit recognition, and skin detection, demonstrate\nthe usefulness of nonparametric Dirichlet mixtures in practice."}
{"id": "2510.07770", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.07770", "abs": "https://arxiv.org/abs/2510.07770", "authors": ["Zhi Yang Tho", "Raymond Chambers", "A. H. Welsh"], "title": "Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data", "comment": null, "summary": "Clustered data arise naturally in many scientific and applied research\nsettings where units are grouped within clusters. They are commonly analyzed\nusing linear mixed models to account for within-cluster correlations. This\narticle focuses on the scenario in which cluster sizes might be highly\nunbalanced and proposes a proportional random effect block bootstrap and a\nmodified random effect block bootstrap, which are applicable in such cases and\naccommodate general distributions of random effects and error terms. These\nmethods generalize the random effect block bootstrap, originally designed for\nthe balanced case, and can be used for inference on parameters of linear mixed\nmodels or functions thereof. Both proposed bootstraps are shown to enjoy Fisher\nconsistency under general cluster sizes, while the original random effect block\nbootstrap is consistent only for balanced clusters. Simulations demonstrate\nstrong finite sample inferential performance of the proposed bootstraps\nrelative to the random effect block bootstrap and other existing bootstrap\nmethods for clustered data. Application to the Oman rainfall enhancement trial\ndataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap\nconfidence intervals using the proposed bootstraps over the random effect block\nbootstrap and a statistically significant effect of the ionization technology\non rainfall."}
{"id": "2510.07501", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07501", "abs": "https://arxiv.org/abs/2510.07501", "authors": ["Sihyung Park", "Wenbin Lu", "Shu Yang"], "title": "Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death", "comment": "30 pages, 5 figures, 6 tables, The Thirty-Ninth Annual Conference on\n  Neural Information Processing Systems", "summary": "Truncation by death, a prevalent challenge in critical care, renders\ntraditional dynamic treatment regime (DTR) evaluation inapplicable due to\nill-defined potential outcomes. We introduce a principal stratification-based\nmethod, focusing on the always-survivor value function. We derive a\nsemiparametrically efficient, multiply robust estimator for multi-stage DTRs,\ndemonstrating its robustness and efficiency. Empirical validation and an\napplication to electronic health records showcase its utility for personalized\ntreatment optimization."}
{"id": "2510.07559", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07559", "abs": "https://arxiv.org/abs/2510.07559", "authors": ["Adrien Corenflos", "Hai-Dang Dau"], "title": "A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo", "comment": "15 pages + 13 pages of appendix comprising mostly proofs. 8 figures", "summary": "A long-standing gap exists between the theoretical analysis of Markov chain\nMonte Carlo convergence, which is often based on statistical divergences, and\nthe diagnostics used in practice. We introduce the first general convergence\ndiagnostics for Markov chain Monte Carlo based on any f-divergence, allowing\nusers to directly monitor, among others, the Kullback--Leibler and the $\\chi^2$\ndivergences as well as the Hellinger and the total variation distances. Our\nfirst key contribution is a coupling-based `weight harmonization' scheme that\nproduces a direct, computable, and consistent weighting of interacting Markov\nchains with respect to their target distribution. The second key contribution\nis to show how such consistent weightings of empirical measures can be used to\nprovide upper bounds to f-divergences in general. We prove that these bounds\nare guaranteed to tighten over time and converge to zero as the chains approach\nstationarity, providing a concrete diagnostic. Numerical experiments\ndemonstrate that our method is a practical and competitive diagnostic tool."}
{"id": "2510.07854", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07854", "abs": "https://arxiv.org/abs/2510.07854", "authors": ["Šárka Hudecová", "Claudia Kirch"], "title": "Detection of mean changes in partially observed functional data", "comment": null, "summary": "We propose a test for a change in the mean for a sequence of functional\nobservations that are only partially observed on subsets of the domain, with no\ninformation available on the complement. The framework accommodates important\nscenarios, including both abrupt and gradual changes. The significance of the\ntest statistic is assessed via a permutation test. In addition to the classical\npermutation approach with a fixed number of permutation samples, we also\ndiscuss a variant with controlled resampling risk that relies on a random\n(data-driven) number of permutation samples. The small sample performance of\nthe proposed methodology is illustrated in a Monte Carlo simulation study and\nan application to real data."}
{"id": "2510.07624", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07624", "abs": "https://arxiv.org/abs/2510.07624", "authors": ["Abdelhakim Benechehab", "Gabriel Singer", "Corentin Léger", "Youssef Attia El Hili", "Giuseppe Paolo", "Albert Thomas", "Maurizio Filippone", "Balázs Kégl"], "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation", "comment": null, "summary": "Generative models form the backbone of modern machine learning, underpinning\nstate-of-the-art systems in text, vision, and multimodal applications. While\nMaximum Likelihood Estimation has traditionally served as the dominant training\nparadigm, recent work have highlighted its limitations, particularly in\ngeneralization and susceptibility to catastrophic forgetting compared to\nReinforcement Learning techniques, such as Policy Gradient methods. However,\nthese approaches depend on explicit reward signals, which are often unavailable\nin practice, leaving open the fundamental problem of how to align generative\nmodels when only high-quality datasets are accessible. In this work, we address\nthis challenge via a Bilevel Optimization framework, where the reward function\nis treated as the optimization variable of an outer-level problem, while a\npolicy gradient objective defines the inner-level. We then conduct a\ntheoretical analysis of this optimization problem in a tractable setting and\nextract insights that, as we demonstrate, generalize to applications such as\ntabular classification and model-based reinforcement learning. We release the\ncode at https://github.com/abenechehab/nll_to_po ."}
{"id": "2510.07732", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.07732", "abs": "https://arxiv.org/abs/2510.07732", "authors": ["Yifan Chen", "Sifan Liu"], "title": "Rotated Mean-Field Variational Inference and Iterative Gaussianization", "comment": null, "summary": "We propose to perform mean-field variational inference (MFVI) in a rotated\ncoordinate system that reduces correlations between variables. The rotation is\ndetermined by principal component analysis (PCA) of a cross-covariance matrix\ninvolving the target's score function. Compared with standard MFVI along the\noriginal axes, MFVI in this rotated system often yields substantially more\naccurate approximations with negligible additional cost.\n  MFVI in a rotated coordinate system defines a rotation and a coordinatewise\nmap that together move the target closer to Gaussian. Iterating this procedure\nyields a sequence of transformations that progressively transforms the target\ntoward Gaussian. The resulting algorithm provides a computationally efficient\nway to construct flow-like transport maps: it requires only MFVI subproblems,\navoids large-scale optimization, and yields transformations that are easy to\ninvert and evaluate. In Bayesian inference tasks, we demonstrate that the\nproposed method achieves higher accuracy than standard MFVI, while maintaining\nmuch lower computational cost than conventional normalizing flows."}
{"id": "2510.07568", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.07568", "abs": "https://arxiv.org/abs/2510.07568", "authors": ["Jaeho Lee", "Eunju Hwang"], "title": "Modeling and forecasting of European Carbon Emission Allowance futures by ARIMA-TX-GARCH models with correlation threshold", "comment": "29 pages, 10 tables and 9 figures", "summary": "We propose an ARIMA-TX-GARCH model and use it to forecast European Carbon\nEmission Allowance futures prices, incorporating Brent crude oil futures prices\nas an exogenous variable."}
{"id": "2510.07559", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07559", "abs": "https://arxiv.org/abs/2510.07559", "authors": ["Adrien Corenflos", "Hai-Dang Dau"], "title": "A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo", "comment": "15 pages + 13 pages of appendix comprising mostly proofs. 8 figures", "summary": "A long-standing gap exists between the theoretical analysis of Markov chain\nMonte Carlo convergence, which is often based on statistical divergences, and\nthe diagnostics used in practice. We introduce the first general convergence\ndiagnostics for Markov chain Monte Carlo based on any f-divergence, allowing\nusers to directly monitor, among others, the Kullback--Leibler and the $\\chi^2$\ndivergences as well as the Hellinger and the total variation distances. Our\nfirst key contribution is a coupling-based `weight harmonization' scheme that\nproduces a direct, computable, and consistent weighting of interacting Markov\nchains with respect to their target distribution. The second key contribution\nis to show how such consistent weightings of empirical measures can be used to\nprovide upper bounds to f-divergences in general. We prove that these bounds\nare guaranteed to tighten over time and converge to zero as the chains approach\nstationarity, providing a concrete diagnostic. Numerical experiments\ndemonstrate that our method is a practical and competitive diagnostic tool."}
{"id": "2510.08204", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.08204", "abs": "https://arxiv.org/abs/2510.08204", "authors": ["Soham Ghosh", "Saloni Bhogale", "Sameer K. Deshpande"], "title": "Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles", "comment": null, "summary": "By allowing the effects of $p$ covariates in a linear regression model to\nvary as functions of $R$ additional effect modifiers, varying-coefficient\nmodels (VCMs) strike a compelling balance between interpretable-but-rigid\nparametric models popular in classical statistics and flexible-but-opaque\nmethods popular in machine learning. But in high-dimensional settings where $p$\nand/or $R$ exceed the number of observations, existing approaches to fitting\nVCMs fail to identify which covariates have a non-zero effect and which effect\nmodifiers drive these effects. We propose sparseVCBART, a fully Bayesian model\nthat approximates each coefficient function in a VCM with a regression tree\nensemble and encourages sparsity with a global--local shrinkage prior on the\nregression tree leaf outputs and a hierarchical prior on the splitting\nprobabilities of each tree. We show that the sparseVCBART posterior contracts\nat a near-minimax optimal rate, automatically adapting to the unknown sparsity\nstructure and smoothness of the true coefficient functions. Compared to\nexisting state-of-the-art methods, sparseVCBART achieved competitive predictive\naccuracy and substantially narrower and better-calibrated uncertainty\nintervals, especially for null covariate effects. We use sparseVCBART to\ninvestigate how the effects of interpersonal conversations on prejudice could\nvary according to the political and demographic characteristics of the\nrespondents."}
{"id": "2510.07649", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07649", "abs": "https://arxiv.org/abs/2510.07649", "authors": ["Tianyu Pan", "Vincent Z. Yu", "Viswanath Devanarayan", "Lu Tian"], "title": "A Honest Cross-Validation Estimator for Prediction Performance", "comment": null, "summary": "Cross-validation is a standard tool for obtaining a honest assessment of the\nperformance of a prediction model. The commonly used version repeatedly splits\ndata, trains the prediction model on the training set, evaluates the model\nperformance on the test set, and averages the model performance across\ndifferent data splits. A well-known criticism is that such cross-validation\nprocedure does not directly estimate the performance of the particular model\nrecommended for future use. In this paper, we propose a new method to estimate\nthe performance of a model trained on a specific (random) training set. A naive\nestimator can be obtained by applying the model to a disjoint testing set.\nSurprisingly, cross-validation estimators computed from other random splits can\nbe used to improve this naive estimator within a random-effects model\nframework. We develop two estimators -- a hierarchical Bayesian estimator and\nan empirical Bayes estimator -- that perform similarly to or better than both\nthe conventional cross-validation estimator and the naive single-split\nestimator. Simulations and a real-data example demonstrate the superior\nperformance of the proposed method."}
{"id": "2510.07653", "categories": ["stat.AP", "cs.DB", "q-bio.GN", "q-bio.TO", "stat.CO", "62P10", "J.3"], "pdf": "https://arxiv.org/pdf/2510.07653", "abs": "https://arxiv.org/abs/2510.07653", "authors": ["Jiawen Chen", "Jinwei Zhang", "Dongshen Peng", "Yutong Song", "Aitong Ruan", "Yun Li", "Didong Li"], "title": "Large-scale spatial variable gene atlas for spatial transcriptomics", "comment": null, "summary": "Spatial variable genes (SVGs) reveal critical information about tissue\narchitecture, cellular interactions, and disease microenvironments. As spatial\ntranscriptomics (ST) technologies proliferate, accurately identifying SVGs\nacross diverse platforms, tissue types, and disease contexts has become both a\nmajor opportunity and a significant computational challenge. Here, we present a\ncomprehensive benchmarking study of 20 state-of-the-art SVG detection methods\nusing human slides from STimage-1K4M, a large-scale resource of ST data\ncomprising 662 slides from more than 18 tissue types. We evaluate each method\nacross a range of biologically and technically meaningful criteria, including\nrecovery of pathologist-annotated domain-specific markers, cross-slide\nreproducibility, scalability to high-resolution data, and robustness to\ntechnical variation. Our results reveal marked differences in performance\ndepending on tissue type, spatial resolution, and study design. Beyond\nbenchmarking, we construct the first cross-tissue atlas of SVGs, enabling\ncomparative analysis of spatial gene programs across cancer and normal tissues.\nWe observe similarities between pairs of tissues that reflect developmental and\nfunctional relationships, such as high overlap between thymus and lymph node,\nand uncover spatial gene programs associated with metastasis, immune\ninfiltration, and tissue-of-origin identity in cancer. Together, our work\ndefines a framework for evaluating and interpreting spatial gene expression and\nestablishes a reference resource for the ST community."}
{"id": "2510.07653", "categories": ["stat.AP", "cs.DB", "q-bio.GN", "q-bio.TO", "stat.CO", "62P10", "J.3"], "pdf": "https://arxiv.org/pdf/2510.07653", "abs": "https://arxiv.org/abs/2510.07653", "authors": ["Jiawen Chen", "Jinwei Zhang", "Dongshen Peng", "Yutong Song", "Aitong Ruan", "Yun Li", "Didong Li"], "title": "Large-scale spatial variable gene atlas for spatial transcriptomics", "comment": null, "summary": "Spatial variable genes (SVGs) reveal critical information about tissue\narchitecture, cellular interactions, and disease microenvironments. As spatial\ntranscriptomics (ST) technologies proliferate, accurately identifying SVGs\nacross diverse platforms, tissue types, and disease contexts has become both a\nmajor opportunity and a significant computational challenge. Here, we present a\ncomprehensive benchmarking study of 20 state-of-the-art SVG detection methods\nusing human slides from STimage-1K4M, a large-scale resource of ST data\ncomprising 662 slides from more than 18 tissue types. We evaluate each method\nacross a range of biologically and technically meaningful criteria, including\nrecovery of pathologist-annotated domain-specific markers, cross-slide\nreproducibility, scalability to high-resolution data, and robustness to\ntechnical variation. Our results reveal marked differences in performance\ndepending on tissue type, spatial resolution, and study design. Beyond\nbenchmarking, we construct the first cross-tissue atlas of SVGs, enabling\ncomparative analysis of spatial gene programs across cancer and normal tissues.\nWe observe similarities between pairs of tissues that reflect developmental and\nfunctional relationships, such as high overlap between thymus and lymph node,\nand uncover spatial gene programs associated with metastasis, immune\ninfiltration, and tissue-of-origin identity in cancer. Together, our work\ndefines a framework for evaluating and interpreting spatial gene expression and\nestablishes a reference resource for the ST community."}
{"id": "2510.07854", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07854", "abs": "https://arxiv.org/abs/2510.07854", "authors": ["Šárka Hudecová", "Claudia Kirch"], "title": "Detection of mean changes in partially observed functional data", "comment": null, "summary": "We propose a test for a change in the mean for a sequence of functional\nobservations that are only partially observed on subsets of the domain, with no\ninformation available on the complement. The framework accommodates important\nscenarios, including both abrupt and gradual changes. The significance of the\ntest statistic is assessed via a permutation test. In addition to the classical\npermutation approach with a fixed number of permutation samples, we also\ndiscuss a variant with controlled resampling risk that relies on a random\n(data-driven) number of permutation samples. The small sample performance of\nthe proposed methodology is illustrated in a Monte Carlo simulation study and\nan application to real data."}
{"id": "2510.08304", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.08304", "abs": "https://arxiv.org/abs/2510.08304", "authors": ["Matteo Amestoy", "Mark van de Wiel", "Jeroen Lakerveld", "Wessel van Wieringen"], "title": "Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data", "comment": null, "summary": "Exposure to diverse non-genetic factors, known as the exposome, is a critical\ndeterminant of health outcomes. However, analyzing the exposome presents\nsignificant methodological challenges, including: high collinearity among\nexposures, the longitudinal nature of repeated measurements, and potential\ncomplex interactions with individual characteristics. In this paper, we address\nthese challenges by proposing a novel statistical framework that extends\nBayesian profile regression. Our method integrates profile regression, which\nhandles collinearity by clustering exposures into latent profiles, into a\nlinear mixed model (LMM), a framework for longitudinal data analysis. This\nprofile-LMM approach effectively accounts for within-person variability over\ntime while also incorporating interactions between the latent exposure clusters\nand individual characteristics. We validate our method using simulated data,\ndemonstrating its ability to accurately identify model parameters and recover\nthe true latent exposure cluster structure. Finally, we apply this approach to\na large longitudinal data set from the Lifelines cohort to identify\ncombinations of exposures that are significantly associated with diastolic\nblood pressure."}
{"id": "2510.07750", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07750", "abs": "https://arxiv.org/abs/2510.07750", "authors": ["Wenbin Zhou", "Shixiang Zhu"], "title": "When Robustness Meets Conservativeness: Conformalized Uncertainty Calibration for Balanced Decision Making", "comment": null, "summary": "Robust optimization safeguards decisions against uncertainty by optimizing\nagainst worst-case scenarios, yet their effectiveness hinges on a prespecified\nrobustness level that is often chosen ad hoc, leading to either insufficient\nprotection or overly conservative and costly solutions. Recent approaches using\nconformal prediction construct data-driven uncertainty sets with finite-sample\ncoverage guarantees, but they still fix coverage targets a priori and offer\nlittle guidance for selecting robustness levels. We propose a new framework\nthat provides distribution-free, finite-sample guarantees on both miscoverage\nand regret for any family of robust predict-then-optimize policies. Our method\nconstructs valid estimators that trace out the miscoverage-regret Pareto\nfrontier, enabling decision-makers to reliably evaluate and calibrate\nrobustness levels according to their cost-risk preferences. The framework is\nsimple to implement, broadly applicable across classical optimization\nformulations, and achieves sharper finite-sample performance than existing\napproaches. These results offer the first principled data-driven methodology\nfor guiding robustness selection and empower practitioners to balance\nrobustness and conservativeness in high-stakes decision-making."}
{"id": "2510.07770", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.07770", "abs": "https://arxiv.org/abs/2510.07770", "authors": ["Zhi Yang Tho", "Raymond Chambers", "A. H. Welsh"], "title": "Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data", "comment": null, "summary": "Clustered data arise naturally in many scientific and applied research\nsettings where units are grouped within clusters. They are commonly analyzed\nusing linear mixed models to account for within-cluster correlations. This\narticle focuses on the scenario in which cluster sizes might be highly\nunbalanced and proposes a proportional random effect block bootstrap and a\nmodified random effect block bootstrap, which are applicable in such cases and\naccommodate general distributions of random effects and error terms. These\nmethods generalize the random effect block bootstrap, originally designed for\nthe balanced case, and can be used for inference on parameters of linear mixed\nmodels or functions thereof. Both proposed bootstraps are shown to enjoy Fisher\nconsistency under general cluster sizes, while the original random effect block\nbootstrap is consistent only for balanced clusters. Simulations demonstrate\nstrong finite sample inferential performance of the proposed bootstraps\nrelative to the random effect block bootstrap and other existing bootstrap\nmethods for clustered data. Application to the Oman rainfall enhancement trial\ndataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap\nconfidence intervals using the proposed bootstraps over the random effect block\nbootstrap and a statistically significant effect of the ionization technology\non rainfall."}
{"id": "2510.08151", "categories": ["stat.AP", "q-bio.PE", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.08151", "abs": "https://arxiv.org/abs/2510.08151", "authors": ["André Luís Luza", "Didier Alard", "Frédéric Barraquand"], "title": "Evaluating multi-season occupancy models with autocorrelation fitted to heterogeneous datasets", "comment": null, "summary": "Predicting species distributions using occupancy models accounting for\nimperfect detection is now commonplace in ecology. Recently, modelling spatial\nand temporal autocorrelation was proposed to alleviate the lack of replication\nin occupancy data, which often prevents model identifiability. However, how\nsuch models perform in highly heterogeneous datasets where missing or\nsingle-visit data dominates remains an open question. Motivated by an\nheterogeneous fine-scale butterfly occupancy dataset, we evaluate the\nperformance of a multi-season occupancy model with spatial and temporal random\neffects to a skewed (Poisson) distribution of the number of surveys per site,\noverlap of covariates between occupancy and detection submodels, and\nspatiotemporal clustering of observations. Results showed that the model is\nrobust to heterogeneous data and covariate overlap. However, when\nspatiotemporal gaps were added, site occupancy was biased towards the average\noccupancy, itself overestimated. Random effects did not correct the influence\nof gaps, due to identifiability issues of variance and autocorrelation\nparameters. Occupancy analysis of two butterfly species further confirmed these\nresults. Overall, multi-season occupancy models with autocorrelation are robust\nto heterogeneous data and covariate overlap, but still present identifiability\nissues and are challenged by severe data gaps, which compromise predictions\neven in data-rich areas."}
{"id": "2510.07867", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07867", "abs": "https://arxiv.org/abs/2510.07867", "authors": ["Xabier de Juan", "Santiago Mazuelas"], "title": "On the Optimality of the Median-of-Means Estimator under Adversarial Contamination", "comment": null, "summary": "The Median-of-Means (MoM) is a robust estimator widely used in machine\nlearning that is known to be (minimax) optimal in scenarios where samples are\ni.i.d. In more grave scenarios, samples are contaminated by an adversary that\ncan inspect and modify the data. Previous work has theoretically shown the\nsuitability of the MoM estimator in certain contaminated settings. However, the\n(minimax) optimality of MoM and its limitations under adversarial contamination\nremain unknown beyond the Gaussian case. In this paper, we present upper and\nlower bounds for the error of MoM under adversarial contamination for multiple\nclasses of distributions. In particular, we show that MoM is (minimax) optimal\nin the class of distributions with finite variance, as well as in the class of\ndistributions with infinite variance and finite absolute $(1+r)$-th moment. We\nalso provide lower bounds for MoM's error that match the order of the presented\nupper bounds, and show that MoM is sub-optimal for light-tailed distributions."}
{"id": "2510.08359", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.08359", "abs": "https://arxiv.org/abs/2510.08359", "authors": ["Jinho Cha", "Eunchan Cha"], "title": "Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials", "comment": "32 pages, 7 figures, planned to submit to Biostatistics", "summary": "Micro-randomized trials (MRTs) are increasingly used to evaluate mobile\nhealth interventions with binary proximal outcomes. Standard inverse\nprobability weighting (IPW) estimators are unbiased but unstable in small\nsamples or under extreme randomization. Estimated mean excursion effect (EMEE)\nimproves efficiency but lacks double robustness. We propose a doubly robust\nEMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision\nIPW and outcome regression. We prove double robustness, asymptotic efficiency,\nand provide finite-sample variance corrections, with extensions to machine\nlearning nuisance estimators. In simulations, DR-EMEE reduces root mean squared\nerror, improves coverage, and achieves up to twofold efficiency gains over IPW\nand five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and\nmHealth datasets confirm stable and efficient inference across both randomized\nand observational settings."}
{"id": "2510.07832", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07832", "abs": "https://arxiv.org/abs/2510.07832", "authors": ["Yuta Shikuri", "Hironori Fujisawa"], "title": "Surrogate Graph Partitioning for Spatial Prediction", "comment": "18 pages, 5 figures, 2 tables", "summary": "Spatial prediction refers to the estimation of unobserved values from\nspatially distributed observations. Although recent advances have improved the\ncapacity to model diverse observation types, adoption in practice remains\nlimited in industries that demand interpretability. To mitigate this gap,\nsurrogate models that explain black-box predictors provide a promising path\ntoward interpretable decision making. In this study, we propose a graph\npartitioning problem to construct spatial segments that minimize the sum of\nwithin-segment variances of individual predictions. The assignment of data\npoints to segments can be formulated as a mixed-integer quadratic programming\nproblem. While this formulation potentially enables the identification of exact\nsegments, its computational complexity becomes prohibitive as the number of\ndata points increases. Motivated by this challenge, we develop an approximation\nscheme that leverages the structural properties of graph partitioning.\nExperimental results demonstrate the computational efficiency of this\napproximation in identifying spatial segments."}
{"id": "2510.08309", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.08309", "abs": "https://arxiv.org/abs/2510.08309", "authors": ["Michael T. Gorczyca", "Jenna D. Li", "Charissa M. Newkirk", "Arjun S. Srivatsa", "Hugo F. M. Milan"], "title": "Two-Stage Trigonometric Regression for Modeling Circadian Rhythms", "comment": null, "summary": "Gene expression levels, hormone secretion, and internal body temperature each\noscillate over an approximately 24-hour cycle, or display circadian rhythms.\nMany circadian biology studies have investigated how these rhythms vary across\ncohorts, uncovering associations between atypical rhythms and diseases such as\ncancer, metabolic syndrome, and sleep disorders. A challenge in analyzing\ncircadian biology data is that the oscillation peak and trough times for a\nphenomenon differ across individuals. If these individual-level differences are\nnot accounted for in trigonometric regression, which is prevalent in circadian\nbiology studies, then estimates of the population-level amplitude parameters\ncan suffer from attenuation bias. This attenuation bias could lead to\ninaccurate study conclusions. To address attenuation bias, we propose a refined\ntwo-stage (RTS) method for trigonometric regression given longitudinal data\nobtained from each individual participating in a study. In the first stage, the\nparameters of individual-level models are estimated. In the second stage,\ntransformations of these individual-level estimates are aggregated to produce\npopulation-level parameter estimates for inference. Simulation studies show\nthat our RTS method mitigates bias in parameter estimation, obtains greater\nstatistical power, and maintains appropriate type I error control when compared\nto the standard two-stage (STS) method, which ignores individual-level\ndifferences in peak and trough times. The only exception for parameter\nestimation and statistical power occurs when the oscillation amplitudes are\nweak relative to random variability in the data and the sample size is small.\nIllustrations with cortisol level data and heart rate data show that our RTS\nmethod obtains larger population-level amplitude parameter estimates and\nsmaller $p$-values for multiple hypothesis tests when compared to the STS\nmethod."}
{"id": "2510.08204", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.08204", "abs": "https://arxiv.org/abs/2510.08204", "authors": ["Soham Ghosh", "Saloni Bhogale", "Sameer K. Deshpande"], "title": "Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles", "comment": null, "summary": "By allowing the effects of $p$ covariates in a linear regression model to\nvary as functions of $R$ additional effect modifiers, varying-coefficient\nmodels (VCMs) strike a compelling balance between interpretable-but-rigid\nparametric models popular in classical statistics and flexible-but-opaque\nmethods popular in machine learning. But in high-dimensional settings where $p$\nand/or $R$ exceed the number of observations, existing approaches to fitting\nVCMs fail to identify which covariates have a non-zero effect and which effect\nmodifiers drive these effects. We propose sparseVCBART, a fully Bayesian model\nthat approximates each coefficient function in a VCM with a regression tree\nensemble and encourages sparsity with a global--local shrinkage prior on the\nregression tree leaf outputs and a hierarchical prior on the splitting\nprobabilities of each tree. We show that the sparseVCBART posterior contracts\nat a near-minimax optimal rate, automatically adapting to the unknown sparsity\nstructure and smoothness of the true coefficient functions. Compared to\nexisting state-of-the-art methods, sparseVCBART achieved competitive predictive\naccuracy and substantially narrower and better-calibrated uncertainty\nintervals, especially for null covariate effects. We use sparseVCBART to\ninvestigate how the effects of interpersonal conversations on prejudice could\nvary according to the political and demographic characteristics of the\nrespondents."}
{"id": "2510.08438", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.08438", "abs": "https://arxiv.org/abs/2510.08438", "authors": ["Xi Fang", "Bingkai Wang", "Liangyuan Hu", "Fan Li"], "title": "Estimands and doubly robust estimation for cluster-randomized trials with survival outcomes", "comment": "1 figure", "summary": "Cluster-randomized trials (CRTs) are experimental designs where groups or\nclusters of participants, rather than the individual participants themselves,\nare randomized to intervention groups. Analyzing CRT requires distinguishing\nbetween treatment effects at the cluster level and the individual level, which\nrequires a clear definition of the estimands under the potential outcomes\nframework. For analyzing survival outcomes, it is common to assess the\ntreatment effect by comparing survival functions or restricted mean survival\ntimes between treatment groups. In this article, we formally characterize\ncluster-level and individual-level treatment effect estimands with\nright-censored survival outcomes in CRTs and propose doubly robust estimators\nfor targeting such estimands. Under covariate-dependent censoring, our\nestimators ensure consistency when either the censoring model or the outcome\nmodel is correctly specified, but not necessarily both. We explore different\nmodeling options for the censoring and outcome models to estimate the censoring\nand survival distributions, and investigate a deletion-based jackknife method\nfor variance and interval estimation. Extensive simulations demonstrate that\nthe proposed methods perform adequately in finite samples. Finally, we\nillustrate our method by analyzing a completed CRT with survival endpoints."}
{"id": "2510.07862", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07862", "abs": "https://arxiv.org/abs/2510.07862", "authors": ["Sanghwa Kim", "Dohyun Ahn", "Seungki Min"], "title": "On the Optimality of Tracking Fisher Information in Adaptive Testing with Stochastic Binary Responses", "comment": null, "summary": "We study the problem of estimating a continuous ability parameter from\nsequential binary responses by actively asking questions with varying\ndifficulties, a setting that arises naturally in adaptive testing and online\npreference learning. Our goal is to certify that the estimate lies within a\ndesired margin of error, using as few queries as possible. We propose a simple\nalgorithm that adaptively selects questions to maximize Fisher information and\nupdates the estimate using a method-of-moments approach, paired with a novel\ntest statistic to decide when the estimate is accurate enough. We prove that\nthis Fisher-tracking strategy achieves optimal performance in both\nfixed-confidence and fixed-budget regimes, which are commonly invested in the\nbest-arm identification literature. Our analysis overcomes a key technical\nchallenge in the fixed-budget setting -- handling the dependence between the\nevolving estimate and the query distribution -- by exploiting a structural\nsymmetry in the model and combining large deviation tools with Ville's\ninequality. Our results provide rigorous theoretical support for simple and\nefficient adaptive testing procedures."}
{"id": "2510.07518", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.07518", "abs": "https://arxiv.org/abs/2510.07518", "authors": ["Blake Hansen", "Dafne Zorzetto", "Valeria Edefonti", "Roberta De Vito"], "title": "Zero-Inflated Bayesian Multi-Study Infinite Non-Negative Matrix Factorization", "comment": null, "summary": "Understanding the association between dietary patterns and health outcomes,\nsuch as the cancer risk, is crucial to inform public health guidelines and\nshaping future dietary interventions. However, dietary intake data present\nseveral statistical challenges: they are high-dimensional, often sparse with\nexcess zeros, and exhibit heterogeneity driven by individual-level covariates.\nNon-Negative Matrix Factorization (NMF), commonly used to estimate patterns in\nhigh-dimensional count data, typically relies on Poisson assumptions and lacks\nthe flexibility to fully address these complexities. Additionally, integrating\ndata across multiple studies, such as case-control studies on cancer risk,\nrequires models that can share information across sources while preserving\nstudy-specific structure.\n  In this paper, we introduce a novel Bayesian NMF model that (i) jointly\nmodels multi-study count data to enable cross-study information sharing, (ii)\nincorporate a mixture component to account for zero inflation, and (iii)\nleverage flexible Bayesian non-parametric priors for characterizing the\nheterogeneity in pattern scores induced by the individual covariates. This\nstructure allows for clustering of individuals based on dietary profiles,\nenabling downstream association analyses with health outcomes. Through\nextensive simulation studies, we demonstrate that our model significantly\nimproves estimation accuracy compared to existing Bayesian NMF methods.\n  We further illustrate its utility through an application to multiple\ncase-control studies on diet and upper aero-digestive tract cancers,\nidentifying nutritionally meaningful dietary patterns. An R package\nimplementing our approach is available at\nhttps://github.com/blhansen/ZIMultiStudyNMF."}
{"id": "2510.08359", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.08359", "abs": "https://arxiv.org/abs/2510.08359", "authors": ["Jinho Cha", "Eunchan Cha"], "title": "Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials", "comment": "32 pages, 7 figures, planned to submit to Biostatistics", "summary": "Micro-randomized trials (MRTs) are increasingly used to evaluate mobile\nhealth interventions with binary proximal outcomes. Standard inverse\nprobability weighting (IPW) estimators are unbiased but unstable in small\nsamples or under extreme randomization. Estimated mean excursion effect (EMEE)\nimproves efficiency but lacks double robustness. We propose a doubly robust\nEMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision\nIPW and outcome regression. We prove double robustness, asymptotic efficiency,\nand provide finite-sample variance corrections, with extensions to machine\nlearning nuisance estimators. In simulations, DR-EMEE reduces root mean squared\nerror, improves coverage, and achieves up to twofold efficiency gains over IPW\nand five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and\nmHealth datasets confirm stable and efficient inference across both randomized\nand observational settings."}
{"id": "2510.07501", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07501", "abs": "https://arxiv.org/abs/2510.07501", "authors": ["Sihyung Park", "Wenbin Lu", "Shu Yang"], "title": "Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death", "comment": "30 pages, 5 figures, 6 tables, The Thirty-Ninth Annual Conference on\n  Neural Information Processing Systems", "summary": "Truncation by death, a prevalent challenge in critical care, renders\ntraditional dynamic treatment regime (DTR) evaluation inapplicable due to\nill-defined potential outcomes. We introduce a principal stratification-based\nmethod, focusing on the always-survivor value function. We derive a\nsemiparametrically efficient, multiply robust estimator for multi-stage DTRs,\ndemonstrating its robustness and efficiency. Empirical validation and an\napplication to electronic health records showcase its utility for personalized\ntreatment optimization."}
{"id": "2510.07867", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07867", "abs": "https://arxiv.org/abs/2510.07867", "authors": ["Xabier de Juan", "Santiago Mazuelas"], "title": "On the Optimality of the Median-of-Means Estimator under Adversarial Contamination", "comment": null, "summary": "The Median-of-Means (MoM) is a robust estimator widely used in machine\nlearning that is known to be (minimax) optimal in scenarios where samples are\ni.i.d. In more grave scenarios, samples are contaminated by an adversary that\ncan inspect and modify the data. Previous work has theoretically shown the\nsuitability of the MoM estimator in certain contaminated settings. However, the\n(minimax) optimality of MoM and its limitations under adversarial contamination\nremain unknown beyond the Gaussian case. In this paper, we present upper and\nlower bounds for the error of MoM under adversarial contamination for multiple\nclasses of distributions. In particular, we show that MoM is (minimax) optimal\nin the class of distributions with finite variance, as well as in the class of\ndistributions with infinite variance and finite absolute $(1+r)$-th moment. We\nalso provide lower bounds for MoM's error that match the order of the presented\nupper bounds, and show that MoM is sub-optimal for light-tailed distributions."}
{"id": "2510.07649", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07649", "abs": "https://arxiv.org/abs/2510.07649", "authors": ["Tianyu Pan", "Vincent Z. Yu", "Viswanath Devanarayan", "Lu Tian"], "title": "A Honest Cross-Validation Estimator for Prediction Performance", "comment": null, "summary": "Cross-validation is a standard tool for obtaining a honest assessment of the\nperformance of a prediction model. The commonly used version repeatedly splits\ndata, trains the prediction model on the training set, evaluates the model\nperformance on the test set, and averages the model performance across\ndifferent data splits. A well-known criticism is that such cross-validation\nprocedure does not directly estimate the performance of the particular model\nrecommended for future use. In this paper, we propose a new method to estimate\nthe performance of a model trained on a specific (random) training set. A naive\nestimator can be obtained by applying the model to a disjoint testing set.\nSurprisingly, cross-validation estimators computed from other random splits can\nbe used to improve this naive estimator within a random-effects model\nframework. We develop two estimators -- a hierarchical Bayesian estimator and\nan empirical Bayes estimator -- that perform similarly to or better than both\nthe conventional cross-validation estimator and the naive single-split\nestimator. Simulations and a real-data example demonstrate the superior\nperformance of the proposed method."}
{"id": "2510.07559", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07559", "abs": "https://arxiv.org/abs/2510.07559", "authors": ["Adrien Corenflos", "Hai-Dang Dau"], "title": "A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo", "comment": "15 pages + 13 pages of appendix comprising mostly proofs. 8 figures", "summary": "A long-standing gap exists between the theoretical analysis of Markov chain\nMonte Carlo convergence, which is often based on statistical divergences, and\nthe diagnostics used in practice. We introduce the first general convergence\ndiagnostics for Markov chain Monte Carlo based on any f-divergence, allowing\nusers to directly monitor, among others, the Kullback--Leibler and the $\\chi^2$\ndivergences as well as the Hellinger and the total variation distances. Our\nfirst key contribution is a coupling-based `weight harmonization' scheme that\nproduces a direct, computable, and consistent weighting of interacting Markov\nchains with respect to their target distribution. The second key contribution\nis to show how such consistent weightings of empirical measures can be used to\nprovide upper bounds to f-divergences in general. We prove that these bounds\nare guaranteed to tighten over time and converge to zero as the chains approach\nstationarity, providing a concrete diagnostic. Numerical experiments\ndemonstrate that our method is a practical and competitive diagnostic tool."}
{"id": "2510.07965", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07965", "abs": "https://arxiv.org/abs/2510.07965", "authors": ["Seungsu Han", "Juyoung Hwang", "Won Chang"], "title": "Stick-Breaking Mixture Normalizing Flows with Component-Wise Tail Adaptation for Variational Inference", "comment": null, "summary": "Normalizing flows with a Gaussian base provide a computationally efficient\nway to approximate posterior distributions in Bayesian inference, but they\noften struggle to capture complex posteriors with multimodality and heavy\ntails. We propose a stick-breaking mixture base with component-wise tail\nadaptation (StiCTAF) for posterior approximation. The method first learns a\nflexible mixture base to mitigate the mode-seeking bias of reverse KL\ndivergence through a weighted average of component-wise ELBOs. It then\nestimates local tail indices of unnormalized densities and finally refines each\nmixture component using a shared backbone combined with component-specific tail\ntransforms calibrated by the estimated indices. This design enables accurate\nmode coverage and anisotropic tail modeling while retaining exact density\nevaluation and stable optimization. Experiments on synthetic posteriors\ndemonstrate improved tail recovery and better coverage of multiple modes\ncompared to benchmark models. We also present a real-data analysis illustrating\nthe practical benefits of our approach for posterior inference."}
{"id": "2510.07649", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07649", "abs": "https://arxiv.org/abs/2510.07649", "authors": ["Tianyu Pan", "Vincent Z. Yu", "Viswanath Devanarayan", "Lu Tian"], "title": "A Honest Cross-Validation Estimator for Prediction Performance", "comment": null, "summary": "Cross-validation is a standard tool for obtaining a honest assessment of the\nperformance of a prediction model. The commonly used version repeatedly splits\ndata, trains the prediction model on the training set, evaluates the model\nperformance on the test set, and averages the model performance across\ndifferent data splits. A well-known criticism is that such cross-validation\nprocedure does not directly estimate the performance of the particular model\nrecommended for future use. In this paper, we propose a new method to estimate\nthe performance of a model trained on a specific (random) training set. A naive\nestimator can be obtained by applying the model to a disjoint testing set.\nSurprisingly, cross-validation estimators computed from other random splits can\nbe used to improve this naive estimator within a random-effects model\nframework. We develop two estimators -- a hierarchical Bayesian estimator and\nan empirical Bayes estimator -- that perform similarly to or better than both\nthe conventional cross-validation estimator and the naive single-split\nestimator. Simulations and a real-data example demonstrate the superior\nperformance of the proposed method."}
{"id": "2510.08095", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08095", "abs": "https://arxiv.org/abs/2510.08095", "authors": ["Amitis Shidani", "Tyler Farghly", "Yang Sun", "Habib Ganjgahi", "George Deligiannidis"], "title": "Beyond Real Data: Synthetic Data through the Lens of Regularization", "comment": null, "summary": "Synthetic data can improve generalization when real data is scarce, but\nexcessive reliance may introduce distributional mismatches that degrade\nperformance. In this paper, we present a learning-theoretic framework to\nquantify the trade-off between synthetic and real data. Our approach leverages\nalgorithmic stability to derive generalization error bounds, characterizing the\noptimal synthetic-to-real data ratio that minimizes expected test error as a\nfunction of the Wasserstein distance between the real and synthetic\ndistributions. We motivate our framework in the setting of kernel ridge\nregression with mixed data, offering a detailed analysis that may be of\nindependent interest. Our theory predicts the existence of an optimal ratio,\nleading to a U-shaped behavior of test error with respect to the proportion of\nsynthetic data. Empirically, we validate this prediction on CIFAR-10 and a\nclinical brain MRI dataset. Our theory extends to the important scenario of\ndomain adaptation, showing that carefully blending synthetic target data with\nlimited source data can mitigate domain shift and enhance generalization. We\nconclude with practical guidance for applying our results to both in-domain and\nout-of-domain scenarios."}
{"id": "2510.08309", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.08309", "abs": "https://arxiv.org/abs/2510.08309", "authors": ["Michael T. Gorczyca", "Jenna D. Li", "Charissa M. Newkirk", "Arjun S. Srivatsa", "Hugo F. M. Milan"], "title": "Two-Stage Trigonometric Regression for Modeling Circadian Rhythms", "comment": null, "summary": "Gene expression levels, hormone secretion, and internal body temperature each\noscillate over an approximately 24-hour cycle, or display circadian rhythms.\nMany circadian biology studies have investigated how these rhythms vary across\ncohorts, uncovering associations between atypical rhythms and diseases such as\ncancer, metabolic syndrome, and sleep disorders. A challenge in analyzing\ncircadian biology data is that the oscillation peak and trough times for a\nphenomenon differ across individuals. If these individual-level differences are\nnot accounted for in trigonometric regression, which is prevalent in circadian\nbiology studies, then estimates of the population-level amplitude parameters\ncan suffer from attenuation bias. This attenuation bias could lead to\ninaccurate study conclusions. To address attenuation bias, we propose a refined\ntwo-stage (RTS) method for trigonometric regression given longitudinal data\nobtained from each individual participating in a study. In the first stage, the\nparameters of individual-level models are estimated. In the second stage,\ntransformations of these individual-level estimates are aggregated to produce\npopulation-level parameter estimates for inference. Simulation studies show\nthat our RTS method mitigates bias in parameter estimation, obtains greater\nstatistical power, and maintains appropriate type I error control when compared\nto the standard two-stage (STS) method, which ignores individual-level\ndifferences in peak and trough times. The only exception for parameter\nestimation and statistical power occurs when the oscillation amplitudes are\nweak relative to random variability in the data and the sample size is small.\nIllustrations with cortisol level data and heart rate data show that our RTS\nmethod obtains larger population-level amplitude parameter estimates and\nsmaller $p$-values for multiple hypothesis tests when compared to the STS\nmethod."}
{"id": "2510.08123", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08123", "abs": "https://arxiv.org/abs/2510.08123", "authors": ["Parham Rezaei", "Filip Kovacevic", "Francesco Locatello", "Marco Mondelli"], "title": "High-dimensional Analysis of Synthetic Data Selection", "comment": null, "summary": "Despite the progress in the development of generative models, their\nusefulness in creating synthetic data that improve prediction performance of\nclassifiers has been put into question. Besides heuristic principles such as\n\"synthetic data should be close to the real data distribution\", it is actually\nnot clear which specific properties affect the generalization error. Our paper\naddresses this question through the lens of high-dimensional regression.\nTheoretically, we show that, for linear models, the covariance shift between\nthe target distribution and the distribution of the synthetic data affects the\ngeneralization error but, surprisingly, the mean shift does not. Furthermore we\nprove that, in some settings, matching the covariance of the target\ndistribution is optimal. Remarkably, the theoretical insights from linear\nmodels carry over to deep neural networks and generative models. We empirically\ndemonstrate that the covariance matching procedure (matching the covariance of\nthe synthetic data with that of the data coming from the target distribution)\nperforms well against several recent approaches for synthetic data selection,\nacross training paradigms, architectures, datasets and generative models used\nfor augmentation."}
{"id": "2510.08335", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08335", "abs": "https://arxiv.org/abs/2510.08335", "authors": ["Ivan Kirev", "Lyuben Baltadzhiev", "Nikola Konstantinov"], "title": "PAC Learnability in the Presence of Performativity", "comment": "21 pages, 3 figures", "summary": "Following the wide-spread adoption of machine learning models in real-world\napplications, the phenomenon of performativity, i.e. model-dependent shifts in\nthe test distribution, becomes increasingly prevalent. Unfortunately, since\nmodels are usually trained solely based on samples from the original\n(unshifted) distribution, this performative shift may lead to decreased\ntest-time performance. In this paper, we study the question of whether and when\nperformative binary classification problems are learnable, via the lens of the\nclassic PAC (Probably Approximately Correct) learning framework. We motivate\nseveral performative scenarios, accounting in particular for linear shifts in\nthe label distribution, as well as for more general changes in both the labels\nand the features. We construct a performative empirical risk function, which\ndepends only on data from the original distribution and on the type\nperformative effect, and is yet an unbiased estimate of the true risk of a\nclassifier on the shifted distribution. Minimizing this notion of performative\nrisk allows us to show that any PAC-learnable hypothesis space in the standard\nbinary classification setting remains PAC-learnable for the considered\nperformative scenarios. We also conduct an extensive experimental evaluation of\nour performative risk minimization method and showcase benefits on synthetic\nand real data."}
{"id": "2510.08409", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08409", "abs": "https://arxiv.org/abs/2510.08409", "authors": ["Yu-Han Wu", "Quentin Berthet", "Gérard Biau", "Claire Boyer", "Romuald Elie", "Pierre Marion"], "title": "Optimal Stopping in Latent Diffusion Models", "comment": null, "summary": "We identify and analyze a surprising phenomenon of Latent Diffusion Models\n(LDMs) where the final steps of the diffusion can degrade sample quality. In\ncontrast to conventional arguments that justify early stopping for numerical\nstability, this phenomenon is intrinsic to the dimensionality reduction in\nLDMs. We provide a principled explanation by analyzing the interaction between\nlatent dimension and stopping time. Under a Gaussian framework with linear\nautoencoders, we characterize the conditions under which early stopping is\nneeded to minimize the distance between generated and target distributions.\nMore precisely, we show that lower-dimensional representations benefit from\nearlier termination, whereas higher-dimensional latent spaces require later\nstopping time. We further establish that the latent dimension interplays with\nother hyperparameters of the problem such as constraints in the parameters of\nscore matching. Experiments on synthetic and real datasets illustrate these\nproperties, underlining that early stopping can improve generative quality.\nTogether, our results offer a theoretical foundation for understanding how the\nlatent dimension influences the sample quality, and highlight stopping time as\na key hyperparameter in LDMs."}
{"id": "2510.08465", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08465", "abs": "https://arxiv.org/abs/2510.08465", "authors": ["Chih-Yu Chang", "Ming-Chung Chang"], "title": "Accelerated Aggregated D-Optimal Designs for Estimating Main Effects in Black-Box Models", "comment": null, "summary": "Recent advances in supervised learning have driven growing interest in\nexplaining black-box models, particularly by estimating the effects of input\nvariables on model predictions. However, existing approaches often face key\nlimitations, including poor scalability, sensitivity to out-of-distribution\nsampling, and instability under correlated features. To address these issues,\nwe propose A2D2E, an $\\textbf{E}$stimator based on $\\textbf{A}$ccelerated\n$\\textbf{A}$ggregated $\\textbf{D}$-Optimal $\\textbf{D}$esigns. Our method\nleverages principled experimental design to improve efficiency and robustness\nin main effect estimation. We establish theoretical guarantees, including\nconvergence and variance reduction, and validate A2D2E through extensive\nsimulations. We further provide the potential of the proposed method with a\ncase study on real data and applications in language models. The code to\nreproduce the results can be found at https://github.com/cchihyu/A2D2E."}
{"id": "2510.08535", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.08535", "abs": "https://arxiv.org/abs/2510.08535", "authors": ["Tassilo Schwarz", "Cai Dieball", "Constantin Kogler", "Kevin Lam", "Renaud Lambiotte", "Arnaud Doucet", "Aljaž Godec", "George Deligiannidis"], "title": "Permutation-Invariant Spectral Learning via Dyson Diffusion", "comment": null, "summary": "Diffusion models are central to generative modeling and have been adapted to\ngraphs by diffusing adjacency matrix representations. The challenge of having\nup to $n!$ such representations for graphs with $n$ nodes is only partially\nmitigated by using permutation-equivariant learning architectures. Despite\ntheir computational efficiency, existing graph diffusion models struggle to\ndistinguish certain graph families, unless graph data are augmented with ad hoc\nfeatures. This shortcoming stems from enforcing the inductive bias within the\nlearning architecture. In this work, we leverage random matrix theory to\nanalytically extract the spectral properties of the diffusion process, allowing\nus to push the inductive bias from the architecture into the dynamics. Building\non this, we introduce the Dyson Diffusion Model, which employs Dyson's Brownian\nMotion to capture the spectral dynamics of an Ornstein-Uhlenbeck process on the\nadjacency matrix while retaining all non-spectral information. We demonstrate\nthat the Dyson Diffusion Model learns graph spectra accurately and outperforms\nexisting graph diffusion models."}
{"id": "2510.07732", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.07732", "abs": "https://arxiv.org/abs/2510.07732", "authors": ["Yifan Chen", "Sifan Liu"], "title": "Rotated Mean-Field Variational Inference and Iterative Gaussianization", "comment": null, "summary": "We propose to perform mean-field variational inference (MFVI) in a rotated\ncoordinate system that reduces correlations between variables. The rotation is\ndetermined by principal component analysis (PCA) of a cross-covariance matrix\ninvolving the target's score function. Compared with standard MFVI along the\noriginal axes, MFVI in this rotated system often yields substantially more\naccurate approximations with negligible additional cost.\n  MFVI in a rotated coordinate system defines a rotation and a coordinatewise\nmap that together move the target closer to Gaussian. Iterating this procedure\nyields a sequence of transformations that progressively transforms the target\ntoward Gaussian. The resulting algorithm provides a computationally efficient\nway to construct flow-like transport maps: it requires only MFVI subproblems,\navoids large-scale optimization, and yields transformations that are easy to\ninvert and evaluate. In Bayesian inference tasks, we demonstrate that the\nproposed method achieves higher accuracy than standard MFVI, while maintaining\nmuch lower computational cost than conventional normalizing flows."}
