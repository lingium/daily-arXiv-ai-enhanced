{"id": "2601.20888", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.20888", "abs": "https://arxiv.org/abs/2601.20888", "authors": ["Youguang Chen", "George Biros"], "title": "Latent-IMH: Efficient Bayesian Inference for Inverse Problems with Approximate Operators", "comment": null, "summary": "We study sampling from posterior distributions in Bayesian linear inverse problems where $A$, the parameters to observables operator, is computationally expensive. In many applications, $A$ can be factored in a manner that facilitates the construction of a cost-effective approximation $\\tilde{A}$. In this framework, we introduce Latent-IMH, a sampling method based on the Metropolis-Hastings independence (IMH) sampler. Latent-IMH first generates intermediate latent variables using the approximate $\\tilde{A}$, and then refines them using the exact $A$. Its primary benefit is that it shifts the computational cost to an offline phase. We theoretically analyze the performance of Latent-IMH using KL divergence and mixing time bounds. Using numerical experiments on several model problems, we show that, under reasonable assumptions, it outperforms state-of-the-art methods such as the No-U-Turn sampler (NUTS) in computational efficiency. In some cases, Latent-IMH can be orders of magnitude faster than existing schemes."}
{"id": "2601.21093", "categories": ["stat.ML", "cs.LG", "math.OC", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.21093", "abs": "https://arxiv.org/abs/2601.21093", "authors": ["Zhou Fan", "Leda Wang"], "title": "High-dimensional learning dynamics of multi-pass Stochastic Gradient Descent in multi-index models", "comment": null, "summary": "We study the learning dynamics of a multi-pass, mini-batch Stochastic Gradient Descent (SGD) procedure for empirical risk minimization in high-dimensional multi-index models with isotropic random data. In an asymptotic regime where the sample size $n$ and data dimension $d$ increase proportionally, for any sub-linear batch size $κ\\asymp n^α$ where $α\\in [0,1)$, and for a commensurate ``critical'' scaling of the learning rate, we provide an asymptotically exact characterization of the coordinate-wise dynamics of SGD. This characterization takes the form of a system of dynamical mean-field equations, driven by a scalar Poisson jump process that represents the asymptotic limit of SGD sampling noise. We develop an analogous characterization of the Stochastic Modified Equation (SME) which provides a Gaussian diffusion approximation to SGD.\n  Our analyses imply that the limiting dynamics for SGD are the same for any batch size scaling $α\\in [0,1)$, and that under a commensurate scaling of the learning rate, dynamics of SGD, SME, and gradient flow are mutually distinct, with those of SGD and SME coinciding in the special case of a linear model. We recover a known dynamical mean-field characterization of gradient flow in a limit of small learning rate, and of one-pass/online SGD in a limit of increasing sample size $n/d \\to \\infty$."}
{"id": "2601.21613", "categories": ["stat.CO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.21613", "abs": "https://arxiv.org/abs/2601.21613", "authors": ["Hugo Morvan", "Jonas Agholme", "Bjorn Eliasson", "Katarina Olofsson", "Ludger Grote", "Fredrik Iredahl", "Oleg Sysoev"], "title": "bigMICE: Multiple Imputation of Big Data", "comment": null, "summary": "Missing data is a prevalent issue in many applications, including large medical registries such as the Swedish Healthcare Quality Registries, potentially leading to biased or inefficient analyses if not handled properly. Multiple Imputation by Chained Equations (MICE) is a popular and versatile method for handling multivariate missing data but traditional implementations face significant challenges when applied to big data sets due to computational time and memory limitations.\n  To address this, the bigMICE package was developed, adapting the MICE framework to big data using Apache Spark MLLib and Spark ML. Our implementation allows for controlling the maximum memory usage during the execution, enabling processing of very large data sets on a hardware with a limited memory, such as ordinary laptops.\n  The developed package was tested on a large Swedish medical registry to measure memory usage, runtime and dependence of the imputation quality on sample size and on missingness proportion in the data. In conclusion, our method is generally more memory efficient and faster on large data sets compared to a commonly used MICE implementation. We also demonstrate that working with very large datasets can result in high quality imputations even when a variable has a large proportion of missing data. This paper also provides guidelines and recommendations on how to install and use our open source package."}
{"id": "2601.20874", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20874", "abs": "https://arxiv.org/abs/2601.20874", "authors": ["Mustafa Cavus", "Przemysław Biecek", "Julian Tejada", "Fernando Marmolejo-Ramos", "Andre Faro"], "title": "Analyzing the Temporal Factors for Anxiety and Depression Symptoms with the Rashomon Perspective", "comment": "19 pages, 2 figures", "summary": "This paper introduces a new modeling perspective in the public mental health domain to provide a robust interpretation of the relations between anxiety and depression, and the demographic and temporal factors. This perspective particularly leverages the Rashomon Effect, where multiple models exhibit similar predictive performance but rely on diverse internal structures. Instead of considering these multiple models, choosing a single best model risks masking alternative narratives embedded in the data. To address this, we employed this perspective in the interpretation of a large-scale psychological dataset, specifically focusing on the Patient Health Questionnaire-4. We use a random forest model combined with partial dependence profiles to rigorously assess the robustness and stability of predictive relationships across the resulting Rashomon set, which consists of multiple models that exhibit similar predictive performance. Our findings confirm that demographic variables \\texttt{age}, \\texttt{sex}, and \\texttt{education} lead to consistent structural shifts in anxiety and depression risk. Crucially, we identify significant temporal effects: risk probability demonstrates clear diurnal and circaseptan fluctuations, peaking during early morning hours. This work demonstrates the necessity of moving beyond the best model to analyze the entire Rashomon set. Our results highlight that the observed variability, particularly due to circadian and circaseptan rhythms, must be meticulously considered for robust interpretation in psychological screening. We advocate for a multiplicity-aware approach to enhance the stability and generalizability of ML-based conclusions in mental health research."}
{"id": "2601.21036", "categories": ["stat.ME", "econ.EM", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21036", "abs": "https://arxiv.org/abs/2601.21036", "authors": ["Chonghuan Wang"], "title": "Experimental Design for Matching", "comment": null, "summary": "Matching mechanisms play a central role in operations management across diverse fields including education, healthcare, and online platforms. However, experimentally comparing a new matching algorithm against a status quo presents some fundamental challenges due to matching interference, where assigning a unit in one matching may preclude its assignment in the other. In this work, we take a design-based perspective to study the design of randomized experiments to compare two predetermined matching plans on a finite population, without imposing outcome or behavioral models. We introduce the notation of a disagreement set, which captures the difference between the two matching plans, and show that it admits a unique decomposition into disjoint alternating paths and cycles with useful structural properties. Based on these properties, we propose the Alternating Path Randomized Design, which sequentially randomizes along these paths and cycles to effectively manage interference. Within a minimax framework, we optimize the conditional randomization probability and show that, for long paths, the optimal choice converges to $\\sqrt{2}-1$, minimizing worst-case variance. We establish the unbiasedness of the Horvitz-Thompson estimator and derive a finite-population Central Limit Theorem that accommodates complex and unstable path and cycle structures as the population grows. Furthermore, we extend the design to many-to-one matchings, where capacity constraints fundamentally alter the structure of the disagreement set. Using graph-theoretic tools, including finding augmenting paths and Euler-tour decomposition on an auxiliary unbalanced directed graph, we construct feasible alternating path and cycle decompositions that allow the design and inference results to carry over."}
{"id": "2601.20888", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.20888", "abs": "https://arxiv.org/abs/2601.20888", "authors": ["Youguang Chen", "George Biros"], "title": "Latent-IMH: Efficient Bayesian Inference for Inverse Problems with Approximate Operators", "comment": null, "summary": "We study sampling from posterior distributions in Bayesian linear inverse problems where $A$, the parameters to observables operator, is computationally expensive. In many applications, $A$ can be factored in a manner that facilitates the construction of a cost-effective approximation $\\tilde{A}$. In this framework, we introduce Latent-IMH, a sampling method based on the Metropolis-Hastings independence (IMH) sampler. Latent-IMH first generates intermediate latent variables using the approximate $\\tilde{A}$, and then refines them using the exact $A$. Its primary benefit is that it shifts the computational cost to an offline phase. We theoretically analyze the performance of Latent-IMH using KL divergence and mixing time bounds. Using numerical experiments on several model problems, we show that, under reasonable assumptions, it outperforms state-of-the-art methods such as the No-U-Turn sampler (NUTS) in computational efficiency. In some cases, Latent-IMH can be orders of magnitude faster than existing schemes."}
{"id": "2601.21765", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21765", "abs": "https://arxiv.org/abs/2601.21765", "authors": ["Augusto Fasano", "Giovanni Rebaudo"], "title": "Mean-field Variational Bayes for Sparse Probit Regression", "comment": null, "summary": "We consider Bayesian variable selection for binary outcomes under a probit link with a spike-and-slab prior on the regression coefficients. Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, and the evidence lower bound is available in closed form. This, in turn, allows the development of an efficient coordinate ascent variational inference algorithm to find the optimal values of the variational parameters. The approach produces posterior inclusion probabilities and parameter estimates, enabling interpretable selection and prediction within a single framework. As shown in both simulated and real data applications, the proposed method successfully identifies the important variables and is orders of magnitude faster than MCMC, while maintaining comparable accuracy."}
{"id": "2601.20875", "categories": ["stat.AP", "cs.LG", "econ.EM", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.20875", "abs": "https://arxiv.org/abs/2601.20875", "authors": ["Md Muhtasim Munif Fahim", "Md Jahid Hasan Imran", "Luknath Debnath", "Tonmoy Shill", "Md. Naim Molla", "Ehsanul Bashar Pranto", "Md Shafin Sanyan Saad", "Md Rezaul Karim"], "title": "Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis", "comment": "Comprehensive Manuscript with Code & Data", "summary": "The achievement of the 2030 Sustainable Development Goals (SDGs) is dependent upon strategic resource distribution. We propose a causal discovery framework using Panel Vector Autoregression, along with both country-specific fixed effects and PCMCI+ conditional independence testing on 168 countries (2000-2025) to develop the first complete causal architecture of SDG dependencies. Utilizing 8 strategically chosen SDGs, we identify a distributed causal network (i.e., no single 'hub' SDG), with 10 statistically significant Granger-causal relationships identified as 11 unique direct effects. Education to Inequality is identified as the most statistically significant direct relationship (r = -0.599; p < 0.05), while effect magnitude significantly varies depending on income levels (e.g., high-income: r = -0.65; lower-middle-income: r = -0.06; non-significant). We also reject the idea that there exists a single 'keystone' SDG. Additionally, we offer a proposed tiered priority framework for the SDGs namely, identifying upstream drivers (Education, Growth), enabling goals (Institutions, Energy), and downstream outcomes (Poverty, Health). Therefore, we conclude that effective SDG acceleration can be accomplished through coordinated multi-dimensional intervention(s), and that single-goal sequential strategies are insufficient."}
{"id": "2601.21106", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.21106", "abs": "https://arxiv.org/abs/2601.21106", "authors": ["Annesh Pal", "Aguirre Mimoun", "Rodolphe Thiébaut", "Boris P. Hejblum"], "title": "Adaptive Dirichlet Process mixture model with unknown concentration parameter and variance: Scaling high dimensional clustering via collapsed variational inference", "comment": "19 pages with 7 figures and 1 table", "summary": "We propose a novel method that performs adaptive clustering with DPMM using collapsed VI, while incorporating weakly-informative priors for DP concentration parameter alpha and base distribution G0. We illustrate the importance of G0 covariance structure and prior choice by considering different parameterisations of the data covariance matrix. On high-dimensional Gaussian simulations, our model demonstrates substantially faster convergence than a state-of-the-art MCMC splice sampler. We further evaluate performances on Negative Binomial simulations and conduct sensitivity analyses to assess robustness on realistic data conditions. Application to a publicly available leukemia transcriptomic data set comprising 72 samples and 2,194 gene expression successfully recovers every known sub-type, all while identifying additional gene expression-based sub-clusters with meaningful biological interpretation."}
{"id": "2601.21014", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.21014", "abs": "https://arxiv.org/abs/2601.21014", "authors": ["Haixiang Sun", "Pengchao Tian", "Zihan Zhou", "Jielei Zhang", "Peiyi Li", "Andrew L. Liu"], "title": "Efficient Causal Structure Learning via Modular Subgraph Integration", "comment": null, "summary": "Learning causal structures from observational data remains a fundamental yet computationally intensive task, particularly in high-dimensional settings where existing methods face challenges such as the super-exponential growth of the search space and increasing computational demands. To address this, we introduce VISTA (Voting-based Integration of Subgraph Topologies for Acyclicity), a modular framework that decomposes the global causal structure learning problem into local subgraphs based on Markov Blankets. The global integration is achieved through a weighted voting mechanism that penalizes low-support edges via exponential decay, filters unreliable ones with an adaptive threshold, and ensures acyclicity using a Feedback Arc Set (FAS) algorithm. The framework is model-agnostic, imposing no assumptions on the inductive biases of base learners, is compatible with arbitrary data settings without requiring specific structural forms, and fully supports parallelization. We also theoretically establish finite-sample error bounds for VISTA, and prove its asymptotic consistency under mild conditions. Extensive experiments on both synthetic and real datasets consistently demonstrate the effectiveness of VISTA, yielding notable improvements in both accuracy and efficiency over a wide range of base learners."}
{"id": "2601.20888", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.20888", "abs": "https://arxiv.org/abs/2601.20888", "authors": ["Youguang Chen", "George Biros"], "title": "Latent-IMH: Efficient Bayesian Inference for Inverse Problems with Approximate Operators", "comment": null, "summary": "We study sampling from posterior distributions in Bayesian linear inverse problems where $A$, the parameters to observables operator, is computationally expensive. In many applications, $A$ can be factored in a manner that facilitates the construction of a cost-effective approximation $\\tilde{A}$. In this framework, we introduce Latent-IMH, a sampling method based on the Metropolis-Hastings independence (IMH) sampler. Latent-IMH first generates intermediate latent variables using the approximate $\\tilde{A}$, and then refines them using the exact $A$. Its primary benefit is that it shifts the computational cost to an offline phase. We theoretically analyze the performance of Latent-IMH using KL divergence and mixing time bounds. Using numerical experiments on several model problems, we show that, under reasonable assumptions, it outperforms state-of-the-art methods such as the No-U-Turn sampler (NUTS) in computational efficiency. In some cases, Latent-IMH can be orders of magnitude faster than existing schemes."}
{"id": "2601.20880", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20880", "abs": "https://arxiv.org/abs/2601.20880", "authors": ["Stefano Maria Iacus", "Haodong Qi", "Devika Jain"], "title": "Spatial Heterogeneity in Climate Risk and Human Flourishing: An Exploration with Generative AI", "comment": null, "summary": "Recent advances in Generative Artificial Intelligence (AI), particularly Large Language Models (LLMs), enable scalable extraction of spatial information from unstructured text and offer new methodological opportunities for studying climate geography. This study develops a spatial framework to examine how cumulative climate risk relates to multidimensional human flourishing across U.S. counties. High-resolution climate hazard indicators are integrated with a Human Flourishing Geographic Index (HFGI), an index derived from classification of 2.6 billion geotagged tweets using fine-tuned open-source Large Language Models (LLMs). These indicators are aggregated to the US county-level and mapped to a structural equation model to infer overall climate risk and human flourishing dimensions, including expressed well-being, meaning and purpose, social connectedness, psychological distress, physical condition, economic stability, religiosity, character and virtue, and institutional trust. The results reveal spatially heterogeneous associations between greater cumulative climate risk and lower levels of expressed human flourishing, with coherent spatial patterns corresponding to recurrent exposure to heat, flooding, wind, drought, and wildfire hazards. The study demonstrates how Generative AI can be combined with latent construct modeling for geographical analysis and for spatial knowledge extraction."}
{"id": "2601.21493", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.21493", "abs": "https://arxiv.org/abs/2601.21493", "authors": ["Silvia Dallari", "Laura Anderlucci", "Nicola Grandi", "Angela Montanari"], "title": "A Poisson Factor Mixture Model for the Analysis of Linguistic Competence in Italian University Students' Writing", "comment": null, "summary": "Public debate on the alleged decline of language skills among younger generations often focuses on university students, the most highly educated segment of the population. Rather than addressing the ill posed question of linguistic decline, this paper examines how formal written Italian is currently used by university students and whether systematic patterns of competence and heterogeneity can be identified. The analysis is based on data from the UniversITA project, which collected formal texts written by a large and nationally representative sample of Italian university students. Texts were annotated for linguistically motivated features covering orthography, lexicon, syntax, morphosyntax, coherence, register, and sentence structure, yielding low frequency multivariate count data. To analyse these data, we propose a novel model-based clustering approach based on a Poisson factor mixture model that accounts for dependence among linguistic features and unobserved population heterogeneity. The results identify two correlated dimensions of writing competence, interpretable as communicative competence and linguistic grammatical competence. When educational and socio demographic information is incorporated, distinct student profiles emerge that are associated with field of study and educational background. These findings provide quantitative evidence on contemporary writing and offer insights relevant for language education and higher education policy."}
{"id": "2601.21025", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21025", "abs": "https://arxiv.org/abs/2601.21025", "authors": ["Louis Grenioux", "RuiKang OuYang", "José Miguel Hernández-Lobato"], "title": "A Diffusive Classification Loss for Learning Energy-based Generative Models", "comment": null, "summary": "Score-based generative models have recently achieved remarkable success. While they are usually parameterized by the score, an alternative way is to use a series of time-dependent energy-based models (EBMs), where the score is obtained from the negative input-gradient of the energy. Crucially, EBMs can be leveraged not only for generation, but also for tasks such as compositional sampling or building Boltzmann Generators via Monte Carlo methods. However, training EBMs remains challenging. Direct maximum likelihood is computationally prohibitive due to the need for nested sampling, while score matching, though efficient, suffers from mode blindness. To address these issues, we introduce the Diffusive Classification (DiffCLF) objective, a simple method that avoids blindness while remaining computationally efficient. DiffCLF reframes EBM learning as a supervised classification problem across noise levels, and can be seamlessly combined with standard score-based objectives. We validate the effectiveness of DiffCLF by comparing the estimated energies against ground truth in analytical Gaussian mixture cases, and by applying the trained models to tasks such as model composition and Boltzmann Generator sampling. Our results show that DiffCLF enables EBMs with higher fidelity and broader applicability than existing approaches."}
{"id": "2601.21217", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.21217", "abs": "https://arxiv.org/abs/2601.21217", "authors": ["Dongyue Xie", "Wanrong Zhu", "Matthew Stephens"], "title": "A Flexible Empirical Bayes Approach to Generalized Linear Models, with Applications to Sparse Logistic Regression", "comment": null, "summary": "We introduce a flexible empirical Bayes approach for fitting Bayesian generalized linear models. Specifically, we adopt a novel mean-field variational inference (VI) method and the prior is estimated within the VI algorithm, making the method tuning-free. Unlike traditional VI methods that optimize the posterior density function, our approach directly optimizes the posterior mean and prior parameters. This formulation reduces the number of parameters to optimize and enables the use of scalable algorithms such as L-BFGS and stochastic gradient descent. Furthermore, our method automatically determines the optimal posterior based on the prior and likelihood, distinguishing it from existing VI methods that often assume a Gaussian variational. Our approach represents a unified framework applicable to a wide range of exponential family distributions, removing the need to develop unique VI methods for each combination of likelihood and prior distributions. We apply the framework to solve sparse logistic regression and demonstrate the superior predictive performance of our method in extensive numerical studies, by comparing it to prevalent sparse logistic regression approaches."}
{"id": "2601.21153", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21153", "abs": "https://arxiv.org/abs/2601.21153", "authors": ["Liang Hong"], "title": "A new strategy for finite-sample valid prediction of future insurance claims in the regression setting", "comment": null, "summary": "The extant insurance literature demonstrates a paucity of finite-sample valid prediction intervals of future insurance claims in the regression setting. To address this challenge, this article proposes a new strategy that converts a predictive method in the unsupervised iid (independent identically distributed) setting to a predictive method in the regression setting. In particular, it enables an actuary to obtain infinitely many finite-sample valid prediction intervals in the regression setting."}
{"id": "2601.21532", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.21532", "abs": "https://arxiv.org/abs/2601.21532", "authors": ["Edoardo Otranto"], "title": "A Matrix-Variate Log-Normal Model for Covariance Matrices", "comment": null, "summary": "We propose a modeling framework for time-varying covariance matrices based on the assumption that the logarithm of a realized covariance matrix follows a matrix-variate oNrmal distribution. By operating in the space of symmetric matrices, the approach guarantees positive definiteness without imposing parameter constraints beyond stationarity. The conditional mean of the logarithmic covariance matrix is specified through a BEKK-type structure that can be rewritten as a diagonal vector representation, yielding a parsimonious specification that mitigates the curse of dimensionality. Estimation is performed by maximum likelihood exploiting properties of matrix-variate Normal distributions and expressing the scale parameter matrix as a function of the location matrix. The covariance matrix is recovered via the matrix exponential. Since this transformation induces an upward bias, an approximate, time-specific bias correction based on a second-order Taylor expansion is proposed. The framework is flexible and applicable to a wide class of problems involving symmetric positive definite matrices."}
{"id": "2601.21026", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21026", "abs": "https://arxiv.org/abs/2601.21026", "authors": ["Louis Grenioux", "Maxence Noble"], "title": "Diffusion-based Annealed Boltzmann Generators : benefits, pitfalls and hopes", "comment": null, "summary": "Sampling configurations at thermodynamic equilibrium is a central challenge in statistical physics. Boltzmann Generators (BGs) tackle it by combining a generative model with a Monte Carlo (MC) correction step to obtain asymptotically unbiased samples from an unnormalized target. Most current BGs use classic MC mechanisms such as importance sampling, which both require tractable likelihoods from the backbone model and scale poorly in high-dimensional, multi-modal targets. We study BGs built on annealed Monte Carlo (aMC), which is designed to overcome these limitations by bridging a simple reference to the target through a sequence of intermediate densities. Diffusion models (DMs) are powerful generative models and have already been incorporated into aMC-based recalibration schemes via the diffusion-induced density path, making them appealing backbones for aMC-BGs. We provide an empirical meta-analysis of DM-based aMC-BGs on controlled multi-modal Gaussian mixtures (varying mode separation, number of modes, and dimension), explicitly disentangling inference effects from learning effects by comparing (i) a perfectly learned DM and (ii) a DM trained from data. Even with a perfect DM, standard integrations using only first-order stochastic denoising kernels fail systematically, whereas second-order denoising kernels can substantially improve performance when covariance information is available. We further propose a deterministic aMC integration based on first-order transport maps derived from DMs, which outperforms the stochastic first-order variant at higher computational cost. Finally, in the learned-DM setting, all DM-aMC variants struggle to produce accurate BGs; we trace the main bottleneck to inaccurate DM log-density estimation."}
{"id": "2601.21951", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.21951", "abs": "https://arxiv.org/abs/2601.21951", "authors": ["James Matthew Young", "Paula Cordero-Encinar", "Sebastian Reich", "Andrew Duncan", "O. Deniz Akyildiz"], "title": "Diffusion Path Samplers via Sequential Monte Carlo", "comment": null, "summary": "We develop a diffusion-based sampler for target distributions known up to a normalising constant. To this end, we rely on the well-known diffusion path that smoothly interpolates between a (simple) base distribution and the target distribution, widely used in diffusion models. Our approach is based on a practical implementation of diffusion-annealed Langevin Monte Carlo, which approximates the diffusion path with convergence guarantees. We tackle the score estimation problem by developing an efficient sequential Monte Carlo sampler that evolves auxiliary variables from conditional distributions along the path, which provides principled score estimates for time-varying distributions. We further develop novel control variate schedules that minimise the variance of these score estimates. Finally, we provide theoretical guarantees and empirically demonstrate the effectiveness of our method on several synthetic and real-world datasets."}
{"id": "2601.21401", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.21401", "abs": "https://arxiv.org/abs/2601.21401", "authors": ["Ferdinand Buchner", "David Jobst", "Annette Möller", "Claudia Czado"], "title": "Bivariate Postprocessing of Wind Vectors", "comment": null, "summary": "To quantify the uncertainty in numerical weather prediction (NWP) forecasts, ensemble prediction systems are utilized. Although NWP forecasts continuously improve, they suffer from systematic bias and dispersion errors. To obtain well calibrated and sharp predictive probability distributions, statistical postprocessing methods are applied to NWP output. Recent developments focus on multivariate postprocessing models incorporating dependencies directly into the model. We introduce three novel bivariate postprocessing approaches, and analyze their performance for joint postprocessing of bivariate wind vector components for 60 stations in Germany. Bivariate vine copula based models, a bivariate gradient boosted version of ensemble model output statistics (EMOS), and a bivariate distributional regression network (DRN) are compared to bivariate EMOS. The case study indicates that the novel bivariate methods improve over the bivariate EMOS approaches. The bivariate DRN and the most flexible version of the bivariate vine copula approach exhibit the best performance in terms of verification scores and calibration."}
{"id": "2601.21696", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21696", "abs": "https://arxiv.org/abs/2601.21696", "authors": ["Alexandre Chaussard", "Anna Bonnet", "Sylvain Le Corff"], "title": "Independent Component Discovery in Temporal Count Data", "comment": "8 pages, 6 figures, Appendix provided", "summary": "Advances in data collection are producing growing volumes of temporal count observations, making adapted modeling increasingly necessary. In this work, we introduce a generative framework for independent component analysis of temporal count data, combining regime-adaptive dynamics with Poisson log-normal emissions. The model identifies disentangled components with regime-dependent contributions, enabling representation learning and perturbations analysis. Notably, we establish the identifiability of the model, supporting principled interpretation. To learn the parameters, we propose an efficient amortized variational inference procedure. Experiments on simulated data evaluate recovery of the mixing function and latent sources across diverse settings, while an in vivo longitudinal gut microbiome study reveals microbial co-variation patterns and regime shifts consistent with clinical perturbations."}
{"id": "2601.21089", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21089", "abs": "https://arxiv.org/abs/2601.21089", "authors": ["Jacob Zhu", "Donald Estep"], "title": "An efficient, accurate, and interpretable machine learning method for computing probability of failure", "comment": null, "summary": "We introduce a novel machine learning method called the Penalized Profile Support Vector Machine based on the Gabriel edited set for the computation of the probability of failure for a complex system as determined by a threshold condition on a computer model of system behavior. The method is designed to minimize the number of evaluations of the computer model while preserving the geometry of the decision boundary that determines the probability. It employs an adaptive sampling strategy designed to strategically allocate points near the boundary determining failure and builds a locally linear surrogate boundary that remains consistent with its geometry by strategic clustering of training points. We prove two convergence results and we compare the performance of the method against a number of state of the art classification methods on four test problems. We also apply the method to determine the probability of survival using the Lotka--Volterra model for competing species."}
{"id": "2601.22003", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.22003", "abs": "https://arxiv.org/abs/2601.22003", "authors": ["James Cuin", "Davide Carbone", "Yanbo Tang", "O. Deniz Akyildiz"], "title": "Efficient Stochastic Optimisation via Sequential Monte Carlo", "comment": null, "summary": "The problem of optimising functions with intractable gradients frequently arise in machine learning and statistics, ranging from maximum marginal likelihood estimation procedures to fine-tuning of generative models. Stochastic approximation methods for this class of problems typically require inner sampling loops to obtain (biased) stochastic gradient estimates, which rapidly becomes computationally expensive. In this work, we develop sequential Monte Carlo (SMC) samplers for optimisation of functions with intractable gradients. Our approach replaces expensive inner sampling methods with efficient SMC approximations, which can result in significant computational gains. We establish convergence results for the basic recursions defined by our methodology which SMC samplers approximate. We demonstrate the effectiveness of our approach on the reward-tuning of energy-based models within various settings."}
{"id": "2601.21495", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.21495", "abs": "https://arxiv.org/abs/2601.21495", "authors": ["Edoardo Otranto"], "title": "Clustering Methods for Identifying and Modelling Areas with Similar Temperature Variations", "comment": null, "summary": "This paper proposes a novel data-driven approach for identifying and modelling areas with similar temperature variations throufigureh clustering and Space-Time AutoRegressive (STAR) models. Using annual temperature data from 168 countries (1901-2022), we apply three clustering methods based on (i) warming rates, (ii) annual temperature variations, and (iii) persistence of variation signs, using Euclidean and Hamming distances. These clusters are then employed to construct alternative spatial weight matrices for STAR models. Empirical results show that distance-based STAR models outperform classical contiguity-based ones, both in-sample and out-of-sample, with the Hamming distance-based STAR model achieving the best predictive accuracy. The study demonstrates that using statistical similarity rather than geographical proximity improves the modelling of global temperature dynamics, suggesting broader applicability to other environmental and socioeconomic datasets."}
{"id": "2601.21732", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.21732", "abs": "https://arxiv.org/abs/2601.21732", "authors": ["Xiaoyu Hu", "Zhenhua Lin"], "title": "Neural Wasserstein Two-Sample Tests", "comment": "49 pages, 3 figures", "summary": "The two-sample homogeneity testing problem is fundamental in statistics and becomes particularly challenging in high dimensions, where classical tests can suffer substantial power loss. We develop a learning-assisted procedure based on the projection 1-Wasserstein distance, which we call the neural Wasserstein test. The method is motivated by the observation that there often exists a low-dimensional projection under which the two high-dimensional distributions differ. In practice, we learn the projection directions via manifold optimization and a witness function using deep neural networks. To adapt to unknown projection dimensions and sparsity levels, we aggregate a collection of candidate statistics through a max-type construction, avoiding explicit tuning while potentially improving power. We establish the validity and consistency of the proposed test and prove a Berry--Esseen type bound for the Gaussian approximation. In particular, under the null hypothesis, the aggregated statistic converges to the absolute maximum of a standard Gaussian vector, yielding an asymptotically pivotal (distribution-free) calibration that bypasses resampling. Simulation studies and a real-data example demonstrate the strong finite-sample performance of the proposed method."}
{"id": "2601.21093", "categories": ["stat.ML", "cs.LG", "math.OC", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.21093", "abs": "https://arxiv.org/abs/2601.21093", "authors": ["Zhou Fan", "Leda Wang"], "title": "High-dimensional learning dynamics of multi-pass Stochastic Gradient Descent in multi-index models", "comment": null, "summary": "We study the learning dynamics of a multi-pass, mini-batch Stochastic Gradient Descent (SGD) procedure for empirical risk minimization in high-dimensional multi-index models with isotropic random data. In an asymptotic regime where the sample size $n$ and data dimension $d$ increase proportionally, for any sub-linear batch size $κ\\asymp n^α$ where $α\\in [0,1)$, and for a commensurate ``critical'' scaling of the learning rate, we provide an asymptotically exact characterization of the coordinate-wise dynamics of SGD. This characterization takes the form of a system of dynamical mean-field equations, driven by a scalar Poisson jump process that represents the asymptotic limit of SGD sampling noise. We develop an analogous characterization of the Stochastic Modified Equation (SME) which provides a Gaussian diffusion approximation to SGD.\n  Our analyses imply that the limiting dynamics for SGD are the same for any batch size scaling $α\\in [0,1)$, and that under a commensurate scaling of the learning rate, dynamics of SGD, SME, and gradient flow are mutually distinct, with those of SGD and SME coinciding in the special case of a linear model. We recover a known dynamical mean-field characterization of gradient flow in a limit of small learning rate, and of one-pass/online SGD in a limit of increasing sample size $n/d \\to \\infty$."}
{"id": "2601.22104", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.22104", "abs": "https://arxiv.org/abs/2601.22104", "authors": ["Paolo Andrich", "Shengjie Lai", "Halim Jun", "Qianwen Duan", "Zhifeng Cheng", "Seth R. Flaxman", "Andrew J. Tatem"], "title": "Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges", "comment": "25 pages, 8 figures", "summary": "Accurate and timely population data are essential for disaster response and humanitarian planning, but traditional censuses often cannot capture rapid demographic changes. Social media data offer a promising alternative for dynamic population monitoring, but their representativeness remains poorly understood and stringent privacy requirements limit their reliability. Here, we address these limitations in the context of the Philippines by calibrating Facebook user counts with the country's 2020 census figures. First, we find that differential privacy techniques commonly applied to social media-based population datasets disproportionately mask low-population areas. To address this, we propose a Bayesian imputation approach to recover missing values, restoring data coverage for $5.5\\%$ of rural areas. Further, using the imputed social media data and leveraging predictors such as urbanisation level, demographic composition, and socio-economic status, we develop a statistical model for the proportion of Facebook users in each municipality, which links observed Facebook user numbers to the true population levels. Out-of-sample validation demonstrates strong result generalisability, with errors as low as ${\\approx}18\\%$ and ${\\approx}24\\%$ for urban and rural Facebook user proportions, respectively. We further demonstrate that accounting for overdispersion and spatial correlations in the data is crucial to obtain accurate estimates and appropriate credible intervals. Crucially, as predictors change over time, the models can be used to regularly update the population predictions, providing a dynamic complement to census-based estimates. These results have direct implications for humanitarian response in disaster-prone regions and offer a general framework for using biased social media signals to generate reliable and timely population data."}
{"id": "2601.21752", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.21752", "abs": "https://arxiv.org/abs/2601.21752", "authors": ["Nina Moutonnet", "Joshua Corneck", "Felipe Tobar", "Danilo Mandic"], "title": "Synthesizing Epileptic Seizures: Gaussian Processes for EEG Generation", "comment": null, "summary": "Reliable seizure detection from electroencephalography (EEG) time series is a high-priority clinical goal, yet the acquisition cost and scarcity of labeled EEG data limit the performance of machine learning methods. This challenge is exacerbated by the long-range, high-dimensional, and non-stationary nature of epileptic EEG recordings, which makes realistic data generation particularly difficult. In this work, we revisit Gaussian processes as a principled and interpretable foundation for modeling EEG dynamics, and propose a novel hierarchical framework, \\textit{GP-EEG}, for generating synthetic epileptic EEG recordings. At its core, our approach decomposes EEG signals into temporal segments modeled via Gaussian process regression, and integrates a domain-adaptation variational autoencoder. We validate the proposed method on two real-world, open-source epileptic EEG datasets. The synthetic EEG recordings generated by our model match real-world epileptic EEG both quantitatively and qualitatively, and can be used to augment training sets."}
{"id": "2601.21104", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21104", "abs": "https://arxiv.org/abs/2601.21104", "authors": ["Aidan Gleich", "Scott C. Schmidler"], "title": "Multilevel and Sequential Monte Carlo for Training-Free Diffusion Guidance", "comment": null, "summary": "We address the problem of accurate, training-free guidance for conditional generation in trained diffusion models. Existing methods typically rely on point-estimates to approximate the posterior score, often resulting in biased approximations that fail to capture multimodality inherent to the reverse process of diffusion models. We propose a sequential Monte Carlo (SMC) framework that constructs an unbiased estimator of $p_θ(y|x_t)$ by integrating over the full denoising distribution via Monte Carlo approximation. To ensure computational tractability, we incorporate variance-reduction schemes based on Multi-Level Monte Carlo (MLMC). Our approach achieves new state-of-the-art results for training-free guidance on CIFAR-10 class-conditional generation, achieving $95.6\\%$ accuracy with $3\\times$ lower cost-per-success than baselines. On ImageNet, our algorithm achieves $1.5\\times$ cost-per-success advantage over existing methods."}
{"id": "2601.21014", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.21014", "abs": "https://arxiv.org/abs/2601.21014", "authors": ["Haixiang Sun", "Pengchao Tian", "Zihan Zhou", "Jielei Zhang", "Peiyi Li", "Andrew L. Liu"], "title": "Efficient Causal Structure Learning via Modular Subgraph Integration", "comment": null, "summary": "Learning causal structures from observational data remains a fundamental yet computationally intensive task, particularly in high-dimensional settings where existing methods face challenges such as the super-exponential growth of the search space and increasing computational demands. To address this, we introduce VISTA (Voting-based Integration of Subgraph Topologies for Acyclicity), a modular framework that decomposes the global causal structure learning problem into local subgraphs based on Markov Blankets. The global integration is achieved through a weighted voting mechanism that penalizes low-support edges via exponential decay, filters unreliable ones with an adaptive threshold, and ensures acyclicity using a Feedback Arc Set (FAS) algorithm. The framework is model-agnostic, imposing no assumptions on the inductive biases of base learners, is compatible with arbitrary data settings without requiring specific structural forms, and fully supports parallelization. We also theoretically establish finite-sample error bounds for VISTA, and prove its asymptotic consistency under mild conditions. Extensive experiments on both synthetic and real datasets consistently demonstrate the effectiveness of VISTA, yielding notable improvements in both accuracy and efficiency over a wide range of base learners."}
{"id": "2601.22103", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.22103", "abs": "https://arxiv.org/abs/2601.22103", "authors": ["Greg Kreider"], "title": "A Spacing Estimator", "comment": "40 pages, 3 figures", "summary": "The distribution of the spacing, or the difference between consecutive order statistics, is known only for uniform and exponential random variates. We add here logistic and Gumbel variates, and present an estimator for distributions with a known inverse cumulative density function. We show the estimator is accurate to the limit of numerical simulations for points near the middle of the order statistics, but degrades by up to 20% in the tails."}
{"id": "2601.21200", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21200", "abs": "https://arxiv.org/abs/2601.21200", "authors": ["Sharan Sahu", "Arisina Banerjee", "Yuchen Wu"], "title": "Provably Reliable Classifier Guidance through Cross-entropy Error Control", "comment": "32 pages, 6 figures", "summary": "Classifier-guided diffusion models generate conditional samples by augmenting the reverse-time score with the gradient of a learned classifier, yet it remains unclear whether standard classifier training procedures yield effective diffusion guidance. We address this gap by showing that, under mild smoothness assumptions on the classifiers, controlling the cross-entropy error at each diffusion step also controls the error of the resulting guidance vectors: classifiers achieving conditional KL divergence $\\varepsilon^2$ from the ground-truth conditional label probabilities induce guidance vectors with mean squared error $\\widetilde{O}(d \\varepsilon )$. Our result yields an upper bound on the sampling error under classifier guidance and bears resemblance to a reverse log-Sobolev-type inequality. Moreover, we show that the classifier smoothness assumption is essential, by constructing simple counterexamples demonstrating that, without it, control of the guidance vector can fail for almost all distributions. To our knowledge, our work establishes the first quantitative link between classifier training and guidance alignment, yielding both a theoretical foundation for classifier guidance and principled guidelines for classifier selection."}
{"id": "2601.21493", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.21493", "abs": "https://arxiv.org/abs/2601.21493", "authors": ["Silvia Dallari", "Laura Anderlucci", "Nicola Grandi", "Angela Montanari"], "title": "A Poisson Factor Mixture Model for the Analysis of Linguistic Competence in Italian University Students' Writing", "comment": null, "summary": "Public debate on the alleged decline of language skills among younger generations often focuses on university students, the most highly educated segment of the population. Rather than addressing the ill posed question of linguistic decline, this paper examines how formal written Italian is currently used by university students and whether systematic patterns of competence and heterogeneity can be identified. The analysis is based on data from the UniversITA project, which collected formal texts written by a large and nationally representative sample of Italian university students. Texts were annotated for linguistically motivated features covering orthography, lexicon, syntax, morphosyntax, coherence, register, and sentence structure, yielding low frequency multivariate count data. To analyse these data, we propose a novel model-based clustering approach based on a Poisson factor mixture model that accounts for dependence among linguistic features and unobserved population heterogeneity. The results identify two correlated dimensions of writing competence, interpretable as communicative competence and linguistic grammatical competence. When educational and socio demographic information is incorporated, distinct student profiles emerge that are associated with field of study and educational background. These findings provide quantitative evidence on contemporary writing and offer insights relevant for language education and higher education policy."}
{"id": "2601.22106", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.22106", "abs": "https://arxiv.org/abs/2601.22106", "authors": ["Harry T. Bond", "Bertrand Gauthier", "Kirstin Strokorb"], "title": "Information-geometry-driven graph sequential growth", "comment": "23 pages, 10 figures", "summary": "We investigate the properties of a class of regularisation-free approaches for Gaussian graphical inference based on the information-geometry-driven sequential growth of initially edgeless graphs. Relating the growth of a graph to a coordinate descent process, we characterise the fully-corrective descents corresponding to information-optimal growths, and propose numerically efficient strategies for their approximation. We demonstrate the ability of the proposed procedures to reliably extract sparse graphical models while limiting the number of false detections, and illustrate how activation ranks can provide insight into the informational relevance of edge sets. The considered approaches are tuning-parameter-free and have complexities akin to coordinate descents."}
{"id": "2601.21217", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.21217", "abs": "https://arxiv.org/abs/2601.21217", "authors": ["Dongyue Xie", "Wanrong Zhu", "Matthew Stephens"], "title": "A Flexible Empirical Bayes Approach to Generalized Linear Models, with Applications to Sparse Logistic Regression", "comment": null, "summary": "We introduce a flexible empirical Bayes approach for fitting Bayesian generalized linear models. Specifically, we adopt a novel mean-field variational inference (VI) method and the prior is estimated within the VI algorithm, making the method tuning-free. Unlike traditional VI methods that optimize the posterior density function, our approach directly optimizes the posterior mean and prior parameters. This formulation reduces the number of parameters to optimize and enables the use of scalable algorithms such as L-BFGS and stochastic gradient descent. Furthermore, our method automatically determines the optimal posterior based on the prior and likelihood, distinguishing it from existing VI methods that often assume a Gaussian variational. Our approach represents a unified framework applicable to a wide range of exponential family distributions, removing the need to develop unique VI methods for each combination of likelihood and prior distributions. We apply the framework to solve sparse logistic regression and demonstrate the superior predictive performance of our method in extensive numerical studies, by comparing it to prevalent sparse logistic regression approaches."}
{"id": "2601.22116", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.22116", "abs": "https://arxiv.org/abs/2601.22116", "authors": ["Greg Kreider"], "title": "Interval Spacing", "comment": "19 pages, 5 figures", "summary": "We define interval spacing as the difference in the order statistics of data over a gap of some width. We derive its density, expected value, and variance for uniform, exponential, and logistic variates. We show that interval spacing is equivalent to running a rectangular low-pass filter over the spacing, which simplifies the expressions for the expected values and introduces correlations between overlapping intervals."}
{"id": "2601.21324", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21324", "abs": "https://arxiv.org/abs/2601.21324", "authors": ["Mengqi Chen", "Thomas B. Berrett", "Theodoros Damoulas", "Michele Caprio"], "title": "Bulk-Calibrated Credal Ambiguity Sets: Fast, Tractable Decision Making under Out-of-Sample Contamination", "comment": null, "summary": "Distributionally robust optimisation (DRO) minimises the worst-case expected loss over an ambiguity set that can capture distributional shifts in out-of-sample environments. While Huber (linear-vacuous) contamination is a classical minimal-assumption model for an $\\varepsilon$-fraction of arbitrary perturbations, including it in an ambiguity set can make the worst-case risk infinite and the DRO objective vacuous unless one imposes strong boundedness or support assumptions. We address these challenges by introducing bulk-calibrated credal ambiguity sets: we learn a high-mass bulk set from data while considering contamination inside the bulk and bounding the remaining tail contribution separately. This leads to a closed-form, finite $\\mathrm{mean}+\\sup$ robust objective and tractable linear or second-order cone programs for common losses and bulk geometries. Through this framework, we highlight and exploit the equivalence between the imprecise probability (IP) notion of upper expectation and the worst-case risk, demonstrating how IP credal sets translate into DRO objectives with interpretable tolerance levels. Experiments on heavy-tailed inventory control, geographically shifted house-price regression, and demographically shifted text classification show competitive robustness-accuracy trade-offs and efficient optimisation times, using Bayesian, frequentist, or empirical reference distributions."}
{"id": "2601.22147", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.22147", "abs": "https://arxiv.org/abs/2601.22147", "authors": ["Melissa Lynne Martin", "Juliette Brook", "Sage Rush", "Theodore D. Satterthwaite", "Ian J. Barnett"], "title": "Variance component score test for multivariate change point detection with applications to mobile health", "comment": "15 pages, 5 figures", "summary": "Multivariate change point detection is the process of identifying distributional shifts in time-ordered data across multiple features. This task is particularly challenging when the number of features is large relative to the number of observations. This problem is often present in mobile health, where behavioral changes in at-risk patients must be detected in real time in order to prompt timely interventions. We propose a variance component score test (VC*) for detecting changes in feature means and/or variances using only pre-change point data to estimate distributional parameters. Through simulation studies, we show that VC* has higher power than existing methods. Moreover, we demonstrate that reducing bias by using only pre-change point days to estimate parameters outweighs the increased estimator variances in most scenarios. Lastly, we apply VC* and competing methods to passively collected smartphone data in adolescents and young adults with affective instability."}
{"id": "2601.21410", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21410", "abs": "https://arxiv.org/abs/2601.21410", "authors": ["Erica Zhang", "Naomi Sagan", "Danny Tse", "Fangzhao Zhang", "Mert Pilanci", "Jose Blanchet"], "title": "Statsformer: Validated Ensemble Learning with LLM-Derived Semantic Priors", "comment": null, "summary": "We introduce Statsformer, a principled framework for integrating large language model (LLM)-derived knowledge into supervised statistical learning. Existing approaches are limited in adaptability and scope: they either inject LLM guidance as an unvalidated heuristic, which is sensitive to LLM hallucination, or embed semantic information within a single fixed learner. Statsformer overcomes both limitations through a guardrailed ensemble architecture. We embed LLM-derived feature priors within an ensemble of linear and nonlinear learners, adaptively calibrating their influence via cross-validation. This design yields a flexible system with an oracle-style guarantee that it performs no worse than any convex combination of its in-library base learners, up to statistical error. Empirically, informative priors yield consistent performance improvements, while uninformative or misspecified LLM guidance is automatically downweighted, mitigating the impact of hallucinations across a diverse range of prediction tasks."}
{"id": "2601.20875", "categories": ["stat.AP", "cs.LG", "econ.EM", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.20875", "abs": "https://arxiv.org/abs/2601.20875", "authors": ["Md Muhtasim Munif Fahim", "Md Jahid Hasan Imran", "Luknath Debnath", "Tonmoy Shill", "Md. Naim Molla", "Ehsanul Bashar Pranto", "Md Shafin Sanyan Saad", "Md Rezaul Karim"], "title": "Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis", "comment": "Comprehensive Manuscript with Code & Data", "summary": "The achievement of the 2030 Sustainable Development Goals (SDGs) is dependent upon strategic resource distribution. We propose a causal discovery framework using Panel Vector Autoregression, along with both country-specific fixed effects and PCMCI+ conditional independence testing on 168 countries (2000-2025) to develop the first complete causal architecture of SDG dependencies. Utilizing 8 strategically chosen SDGs, we identify a distributed causal network (i.e., no single 'hub' SDG), with 10 statistically significant Granger-causal relationships identified as 11 unique direct effects. Education to Inequality is identified as the most statistically significant direct relationship (r = -0.599; p < 0.05), while effect magnitude significantly varies depending on income levels (e.g., high-income: r = -0.65; lower-middle-income: r = -0.06; non-significant). We also reject the idea that there exists a single 'keystone' SDG. Additionally, we offer a proposed tiered priority framework for the SDGs namely, identifying upstream drivers (Education, Growth), enabling goals (Institutions, Energy), and downstream outcomes (Poverty, Health). Therefore, we conclude that effective SDG acceleration can be accomplished through coordinated multi-dimensional intervention(s), and that single-goal sequential strategies are insufficient."}
{"id": "2601.21455", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21455", "abs": "https://arxiv.org/abs/2601.21455", "authors": ["Yizhou Min", "Yizhou Lu", "Lanqi Li", "Zhen Zhang", "Jiaye Teng"], "title": "Questioning the Coverage-Length Metric in Conformal Prediction: When Shorter Intervals Are Not Better", "comment": null, "summary": "Conformal prediction (CP) has become a cornerstone of distribution-free uncertainty quantification, conventionally evaluated by its coverage and interval length. This work critically examines the sufficiency of these standard metrics. We demonstrate that the interval length might be deceptively improved through a counter-intuitive approach termed Prejudicial Trick (PT), while the coverage remains valid. Specifically, for any given test sample, PT probabilistically returns an interval, which is either null or constructed using an adjusted confidence level, thereby preserving marginal coverage. While PT potentially yields a deceptively lower interval length, it introduces practical vulnerabilities: the same input can yield completely different prediction intervals across repeated runs of the algorithm. We formally derive the conditions under which PT achieves these misleading improvements and provides extensive empirical evidence across various regression and classification tasks. Furthermore, we introduce a new metric interval stability which helps detect whether a new CP method implicitly improves the length based on such PT-like techniques."}
{"id": "2601.21217", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.21217", "abs": "https://arxiv.org/abs/2601.21217", "authors": ["Dongyue Xie", "Wanrong Zhu", "Matthew Stephens"], "title": "A Flexible Empirical Bayes Approach to Generalized Linear Models, with Applications to Sparse Logistic Regression", "comment": null, "summary": "We introduce a flexible empirical Bayes approach for fitting Bayesian generalized linear models. Specifically, we adopt a novel mean-field variational inference (VI) method and the prior is estimated within the VI algorithm, making the method tuning-free. Unlike traditional VI methods that optimize the posterior density function, our approach directly optimizes the posterior mean and prior parameters. This formulation reduces the number of parameters to optimize and enables the use of scalable algorithms such as L-BFGS and stochastic gradient descent. Furthermore, our method automatically determines the optimal posterior based on the prior and likelihood, distinguishing it from existing VI methods that often assume a Gaussian variational. Our approach represents a unified framework applicable to a wide range of exponential family distributions, removing the need to develop unique VI methods for each combination of likelihood and prior distributions. We apply the framework to solve sparse logistic regression and demonstrate the superior predictive performance of our method in extensive numerical studies, by comparing it to prevalent sparse logistic regression approaches."}
{"id": "2601.21812", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21812", "abs": "https://arxiv.org/abs/2601.21812", "authors": ["Francisco Caldas", "Sahil Kumar", "Cláudia Soares"], "title": "A Decomposable Forward Process in Diffusion Models for Time-Series Forecasting", "comment": "submitted to ICML'26", "summary": "We introduce a model-agnostic forward diffusion process for time-series forecasting that decomposes signals into spectral components, preserving structured temporal patterns such as seasonality more effectively than standard diffusion. Unlike prior work that modifies the network architecture or diffuses directly in the frequency domain, our proposed method alters only the diffusion process itself, making it compatible with existing diffusion backbones (e.g., DiffWave, TimeGrad, CSDI). By staging noise injection according to component energy, it maintains high signal-to-noise ratios for dominant frequencies throughout the diffusion trajectory, thereby improving the recoverability of long-term patterns. This strategy enables the model to maintain the signal structure for a longer period in the forward process, leading to improved forecast quality. Across standard forecasting benchmarks, we show that applying spectral decomposition strategies, such as the Fourier or Wavelet transform, consistently improves upon diffusion models using the baseline forward process, with negligible computational overhead. The code for this paper is available at https://anonymous.4open.science/r/D-FDP-4A29."}
{"id": "2601.21765", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21765", "abs": "https://arxiv.org/abs/2601.21765", "authors": ["Augusto Fasano", "Giovanni Rebaudo"], "title": "Mean-field Variational Bayes for Sparse Probit Regression", "comment": null, "summary": "We consider Bayesian variable selection for binary outcomes under a probit link with a spike-and-slab prior on the regression coefficients. Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, and the evidence lower bound is available in closed form. This, in turn, allows the development of an efficient coordinate ascent variational inference algorithm to find the optimal values of the variational parameters. The approach produces posterior inclusion probabilities and parameter estimates, enabling interpretable selection and prediction within a single framework. As shown in both simulated and real data applications, the proposed method successfully identifies the important variables and is orders of magnitude faster than MCMC, while maintaining comparable accuracy."}
{"id": "2601.21817", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21817", "abs": "https://arxiv.org/abs/2601.21817", "authors": ["Mingyuan Xu", "Xinzi Tan", "Jiawei Wu", "Doudou Zhou"], "title": "A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth", "comment": null, "summary": "Evaluating large language models (LLMs) on open-ended tasks without ground-truth labels is increasingly done via the LLM-as-a-judge paradigm. A critical but under-modeled issue is that judge LLMs differ substantially in reliability; treating all judges equally can yield biased leaderboards and misleading uncertainty estimates. More data can make evaluation more confidently wrong under misspecified aggregation. We propose a judge-aware ranking framework that extends the Bradley-Terry-Luce model by introducing judge-specific discrimination parameters, jointly estimating latent model quality and judge reliability from pairwise comparisons without reference labels. We establish identifiability up to natural normalizations and prove consistency and asymptotic normality of the maximum likelihood estimator, enabling confidence intervals for score differences and rank comparisons. Across multiple public benchmarks and a newly collected dataset, our method improves agreement with human preferences, achieves higher data efficiency than unweighted baselines, and produces calibrated uncertainty quantification for LLM rankings."}
{"id": "2601.22104", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.22104", "abs": "https://arxiv.org/abs/2601.22104", "authors": ["Paolo Andrich", "Shengjie Lai", "Halim Jun", "Qianwen Duan", "Zhifeng Cheng", "Seth R. Flaxman", "Andrew J. Tatem"], "title": "Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges", "comment": "25 pages, 8 figures", "summary": "Accurate and timely population data are essential for disaster response and humanitarian planning, but traditional censuses often cannot capture rapid demographic changes. Social media data offer a promising alternative for dynamic population monitoring, but their representativeness remains poorly understood and stringent privacy requirements limit their reliability. Here, we address these limitations in the context of the Philippines by calibrating Facebook user counts with the country's 2020 census figures. First, we find that differential privacy techniques commonly applied to social media-based population datasets disproportionately mask low-population areas. To address this, we propose a Bayesian imputation approach to recover missing values, restoring data coverage for $5.5\\%$ of rural areas. Further, using the imputed social media data and leveraging predictors such as urbanisation level, demographic composition, and socio-economic status, we develop a statistical model for the proportion of Facebook users in each municipality, which links observed Facebook user numbers to the true population levels. Out-of-sample validation demonstrates strong result generalisability, with errors as low as ${\\approx}18\\%$ and ${\\approx}24\\%$ for urban and rural Facebook user proportions, respectively. We further demonstrate that accounting for overdispersion and spatial correlations in the data is crucial to obtain accurate estimates and appropriate credible intervals. Crucially, as predictors change over time, the models can be used to regularly update the population predictions, providing a dynamic complement to census-based estimates. These results have direct implications for humanitarian response in disaster-prone regions and offer a general framework for using biased social media signals to generate reliable and timely population data."}
{"id": "2601.21831", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21831", "abs": "https://arxiv.org/abs/2601.21831", "authors": ["Daniel Gonzalez-Alvarado", "Jonas Cassel", "Stefania Petra", "Christoph Schnörr"], "title": "Generative Modeling of Discrete Data Using Geometric Latent Subspaces", "comment": null, "summary": "We introduce the use of latent subspaces in the exponential parameter space of product manifolds of categorial distributions, as a tool for learning generative models of discrete data. The low-dimensional latent space encodes statistical dependencies and removes redundant degrees of freedom among the categorial variables. We equip the parameter domain with a Riemannian geometry such that the spaces and distances are related by isometries which enables consistent flow matching. In particular, geodesics become straight lines which makes model training by flow matching effective. Empirical results demonstrate that reduced latent dimensions suffice to represent data for generative modeling."}
{"id": "2601.21868", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21868", "abs": "https://arxiv.org/abs/2601.21868", "authors": ["Stanislas Strasman", "Gabriel Cardoso", "Sylvain Le Corff", "Vincent Lemaire", "Antonio Ocello"], "title": "On Forgetting and Stability of Score-based Generative models", "comment": null, "summary": "Understanding the stability and long-time behavior of generative models is a fundamental problem in modern machine learning. This paper provides quantitative bounds on the sampling error of score-based generative models by leveraging stability and forgetting properties of the Markov chain associated with the reverse-time dynamics. Under weak assumptions, we provide the two structural properties to ensure the propagation of initialization and discretization errors of the backward process: a Lyapunov drift condition and a Doeblin-type minorization condition. A practical consequence is quantitative stability of the sampling procedure, as the reverse diffusion dynamics induces a contraction mechanism along the sampling trajectory. Our results clarify the role of stochastic dynamics in score-based models and provide a principled framework for analyzing propagation of errors in such approaches."}
{"id": "2601.21942", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21942", "abs": "https://arxiv.org/abs/2601.21942", "authors": ["Lev Fedorov", "Michaël E. Sander", "Romuald Elie", "Pierre Marion", "Mathieu Laurière"], "title": "Clustering in Deep Stochastic Transformers", "comment": "24 pages", "summary": "Transformers have revolutionized deep learning across various domains but understanding the precise token dynamics remains a theoretical challenge. Existing theories of deep Transformers with layer normalization typically predict that tokens cluster to a single point; however, these results rely on deterministic weight assumptions, which fail to capture the standard initialization scheme in Transformers. In this work, we show that accounting for the intrinsic stochasticity of random initialization alters this picture. More precisely, we analyze deep Transformers where noise arises from the random initialization of value matrices. Under diffusion scaling and token-wise RMS normalization, we prove that, as the number of Transformer layers goes to infinity, the discrete token dynamics converge to an interacting-particle system on the sphere where tokens are driven by a \\emph{common} matrix-valued Brownian noise. In this limit, we show that initialization noise prevents the collapse to a single cluster predicted by deterministic models. For two tokens, we prove a phase transition governed by the interaction strength and the token dimension: unlike deterministic attention flows, antipodal configurations become attracting with positive probability. Numerical experiments confirm the predicted transition, reveal that antipodal formations persist for more than two tokens, and demonstrate that suppressing the intrinsic noise degrades accuracy."}
{"id": "2601.21951", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.21951", "abs": "https://arxiv.org/abs/2601.21951", "authors": ["James Matthew Young", "Paula Cordero-Encinar", "Sebastian Reich", "Andrew Duncan", "O. Deniz Akyildiz"], "title": "Diffusion Path Samplers via Sequential Monte Carlo", "comment": null, "summary": "We develop a diffusion-based sampler for target distributions known up to a normalising constant. To this end, we rely on the well-known diffusion path that smoothly interpolates between a (simple) base distribution and the target distribution, widely used in diffusion models. Our approach is based on a practical implementation of diffusion-annealed Langevin Monte Carlo, which approximates the diffusion path with convergence guarantees. We tackle the score estimation problem by developing an efficient sequential Monte Carlo sampler that evolves auxiliary variables from conditional distributions along the path, which provides principled score estimates for time-varying distributions. We further develop novel control variate schedules that minimise the variance of these score estimates. Finally, we provide theoretical guarantees and empirically demonstrate the effectiveness of our method on several synthetic and real-world datasets."}
{"id": "2601.21959", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21959", "abs": "https://arxiv.org/abs/2601.21959", "authors": ["Yu-Wei Chen", "Raghu Pasupathy", "Jordan Awan"], "title": "Near-Optimal Private Tests for Simple and MLR Hypotheses", "comment": null, "summary": "We develop a near-optimal testing procedure under the framework of Gaussian differential privacy for simple as well as one- and two-sided tests under monotone likelihood ratio conditions. Our mechanism is based on a private mean estimator with data-driven clamping bounds, whose population risk matches the private minimax rate up to logarithmic factors. Using this estimator, we construct private test statistics that achieve the same asymptotic relative efficiency as the non-private, most powerful tests while maintaining conservative type I error control. In addition to our theoretical results, our numerical experiments show that our private tests outperform competing DP methods and offer comparable power to the non-private most powerful tests, even at moderately small sample sizes and privacy loss budgets."}
{"id": "2601.22003", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.22003", "abs": "https://arxiv.org/abs/2601.22003", "authors": ["James Cuin", "Davide Carbone", "Yanbo Tang", "O. Deniz Akyildiz"], "title": "Efficient Stochastic Optimisation via Sequential Monte Carlo", "comment": null, "summary": "The problem of optimising functions with intractable gradients frequently arise in machine learning and statistics, ranging from maximum marginal likelihood estimation procedures to fine-tuning of generative models. Stochastic approximation methods for this class of problems typically require inner sampling loops to obtain (biased) stochastic gradient estimates, which rapidly becomes computationally expensive. In this work, we develop sequential Monte Carlo (SMC) samplers for optimisation of functions with intractable gradients. Our approach replaces expensive inner sampling methods with efficient SMC approximations, which can result in significant computational gains. We establish convergence results for the basic recursions defined by our methodology which SMC samplers approximate. We demonstrate the effectiveness of our approach on the reward-tuning of energy-based models within various settings."}
{"id": "2601.20875", "categories": ["stat.AP", "cs.LG", "econ.EM", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.20875", "abs": "https://arxiv.org/abs/2601.20875", "authors": ["Md Muhtasim Munif Fahim", "Md Jahid Hasan Imran", "Luknath Debnath", "Tonmoy Shill", "Md. Naim Molla", "Ehsanul Bashar Pranto", "Md Shafin Sanyan Saad", "Md Rezaul Karim"], "title": "Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis", "comment": "Comprehensive Manuscript with Code & Data", "summary": "The achievement of the 2030 Sustainable Development Goals (SDGs) is dependent upon strategic resource distribution. We propose a causal discovery framework using Panel Vector Autoregression, along with both country-specific fixed effects and PCMCI+ conditional independence testing on 168 countries (2000-2025) to develop the first complete causal architecture of SDG dependencies. Utilizing 8 strategically chosen SDGs, we identify a distributed causal network (i.e., no single 'hub' SDG), with 10 statistically significant Granger-causal relationships identified as 11 unique direct effects. Education to Inequality is identified as the most statistically significant direct relationship (r = -0.599; p < 0.05), while effect magnitude significantly varies depending on income levels (e.g., high-income: r = -0.65; lower-middle-income: r = -0.06; non-significant). We also reject the idea that there exists a single 'keystone' SDG. Additionally, we offer a proposed tiered priority framework for the SDGs namely, identifying upstream drivers (Education, Growth), enabling goals (Institutions, Energy), and downstream outcomes (Poverty, Health). Therefore, we conclude that effective SDG acceleration can be accomplished through coordinated multi-dimensional intervention(s), and that single-goal sequential strategies are insufficient."}
{"id": "2601.21696", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21696", "abs": "https://arxiv.org/abs/2601.21696", "authors": ["Alexandre Chaussard", "Anna Bonnet", "Sylvain Le Corff"], "title": "Independent Component Discovery in Temporal Count Data", "comment": "8 pages, 6 figures, Appendix provided", "summary": "Advances in data collection are producing growing volumes of temporal count observations, making adapted modeling increasingly necessary. In this work, we introduce a generative framework for independent component analysis of temporal count data, combining regime-adaptive dynamics with Poisson log-normal emissions. The model identifies disentangled components with regime-dependent contributions, enabling representation learning and perturbations analysis. Notably, we establish the identifiability of the model, supporting principled interpretation. To learn the parameters, we propose an efficient amortized variational inference procedure. Experiments on simulated data evaluate recovery of the mixing function and latent sources across diverse settings, while an in vivo longitudinal gut microbiome study reveals microbial co-variation patterns and regime shifts consistent with clinical perturbations."}
{"id": "2601.21765", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21765", "abs": "https://arxiv.org/abs/2601.21765", "authors": ["Augusto Fasano", "Giovanni Rebaudo"], "title": "Mean-field Variational Bayes for Sparse Probit Regression", "comment": null, "summary": "We consider Bayesian variable selection for binary outcomes under a probit link with a spike-and-slab prior on the regression coefficients. Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, and the evidence lower bound is available in closed form. This, in turn, allows the development of an efficient coordinate ascent variational inference algorithm to find the optimal values of the variational parameters. The approach produces posterior inclusion probabilities and parameter estimates, enabling interpretable selection and prediction within a single framework. As shown in both simulated and real data applications, the proposed method successfully identifies the important variables and is orders of magnitude faster than MCMC, while maintaining comparable accuracy."}
