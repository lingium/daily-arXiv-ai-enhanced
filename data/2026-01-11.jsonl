{"id": "2601.04499", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.04499", "abs": "https://arxiv.org/abs/2601.04499", "authors": ["Baolin Chen", "Mengfei Ran"], "title": "A Generalized Adaptive Joint Learning Framework for High-Dimensional Time-Varying Models", "comment": null, "summary": "In modern biomedical and econometric studies, longitudinal processes are often characterized by complex time-varying associations and abrupt regime shifts that are shared across correlated outcomes. Standard functional data analysis (FDA) methods, which prioritize smoothness, often fail to capture these dynamic structural features, particularly in high-dimensional settings. This article introduces Adaptive Joint Learning (AJL), a regularization framework designed to simultaneously perform functional variable selection and structural changepoint detection in multivariate time-varying coefficient models. We propose a convex optimization procedure that synergizes adaptive group-wise penalization with fused regularization, effectively borrowing strength across multiple outcomes to enhance estimation efficiency. We provide a rigorous theoretical analysis of the estimator in the ultra-high-dimensional regime (p >> n), establishing non-asymptotic error bounds and proving that AJL achieves the oracle property--performing as well as if the true active set and changepoint locations were known a priori. A key theoretical contribution is the explicit handling of approximation bias via undersmoothing conditions to ensure valid asymptotic inference. The proposed method is validated through comprehensive simulations and an application to Primary Biliary Cirrhosis (PBC) data. The analysis uncovers synchronized phase transitions in disease progression and identifies a parsimonious set of time-varying prognostic markers."}
{"id": "2601.04499", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.04499", "abs": "https://arxiv.org/abs/2601.04499", "authors": ["Baolin Chen", "Mengfei Ran"], "title": "A Generalized Adaptive Joint Learning Framework for High-Dimensional Time-Varying Models", "comment": null, "summary": "In modern biomedical and econometric studies, longitudinal processes are often characterized by complex time-varying associations and abrupt regime shifts that are shared across correlated outcomes. Standard functional data analysis (FDA) methods, which prioritize smoothness, often fail to capture these dynamic structural features, particularly in high-dimensional settings. This article introduces Adaptive Joint Learning (AJL), a regularization framework designed to simultaneously perform functional variable selection and structural changepoint detection in multivariate time-varying coefficient models. We propose a convex optimization procedure that synergizes adaptive group-wise penalization with fused regularization, effectively borrowing strength across multiple outcomes to enhance estimation efficiency. We provide a rigorous theoretical analysis of the estimator in the ultra-high-dimensional regime (p >> n), establishing non-asymptotic error bounds and proving that AJL achieves the oracle property--performing as well as if the true active set and changepoint locations were known a priori. A key theoretical contribution is the explicit handling of approximation bias via undersmoothing conditions to ensure valid asymptotic inference. The proposed method is validated through comprehensive simulations and an application to Primary Biliary Cirrhosis (PBC) data. The analysis uncovers synchronized phase transitions in disease progression and identifies a parsimonious set of time-varying prognostic markers."}
{"id": "2601.04499", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.04499", "abs": "https://arxiv.org/abs/2601.04499", "authors": ["Baolin Chen", "Mengfei Ran"], "title": "A Generalized Adaptive Joint Learning Framework for High-Dimensional Time-Varying Models", "comment": null, "summary": "In modern biomedical and econometric studies, longitudinal processes are often characterized by complex time-varying associations and abrupt regime shifts that are shared across correlated outcomes. Standard functional data analysis (FDA) methods, which prioritize smoothness, often fail to capture these dynamic structural features, particularly in high-dimensional settings. This article introduces Adaptive Joint Learning (AJL), a regularization framework designed to simultaneously perform functional variable selection and structural changepoint detection in multivariate time-varying coefficient models. We propose a convex optimization procedure that synergizes adaptive group-wise penalization with fused regularization, effectively borrowing strength across multiple outcomes to enhance estimation efficiency. We provide a rigorous theoretical analysis of the estimator in the ultra-high-dimensional regime (p >> n), establishing non-asymptotic error bounds and proving that AJL achieves the oracle property--performing as well as if the true active set and changepoint locations were known a priori. A key theoretical contribution is the explicit handling of approximation bias via undersmoothing conditions to ensure valid asymptotic inference. The proposed method is validated through comprehensive simulations and an application to Primary Biliary Cirrhosis (PBC) data. The analysis uncovers synchronized phase transitions in disease progression and identifies a parsimonious set of time-varying prognostic markers."}
{"id": "2601.04538", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.04538", "abs": "https://arxiv.org/abs/2601.04538", "authors": ["Kumar Utkarsh", "Nirmish R. Shah", "Tanvi Banerjee", "Daniel M. Abrams"], "title": "A new method for augmenting short time series, with application to pain events in sickle cell disease", "comment": "15 pages, 9 figures", "summary": "Researchers across different fields, including but not limited to ecology, biology, and healthcare, often face the challenge of sparse data. Such sparsity can lead to uncertainties, estimation difficulties, and potential biases in modeling. Here we introduce a novel data augmentation method that combines multiple sparse time series datasets when they share similar statistical properties, thereby improving parameter estimation and model selection reliability. We demonstrate the effectiveness of this approach through validation studies comparing Hawkes and Poisson processes, followed by application to subjective pain dynamics in patients with sickle cell disease (SCD), a condition affecting millions worldwide, particularly those of African, Mediterranean, Middle Eastern, and Indian descent."}
{"id": "2601.04808", "categories": ["stat.AP", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.04808", "abs": "https://arxiv.org/abs/2601.04808", "authors": ["Muhammad Shoaib", "Zaka Ur Rehman", "Muhammad Qasim"], "title": "Comparison of Maximum Likelihood Classification Before and After Applying Weierstrass Transform", "comment": null, "summary": "The aim of this paper is to use Maximum Likelihood (ML) Classification on multispectral data by means of qualitative and quantitative approaches. Maximum Likelihood is a supervised classification algorithm which is based on the Classical Bayes theorem. It makes use of a discriminant function to assign pixel to the class with the highest likelihood. Class means vector and covariance matrix are the key inputs to the function and can be estimated from training pixels of a particular class. As Maximum Likelihood need some assumptions before it has to be applied on the data. In this paper we will compare the results of Maximum Likelihood Classification (ML) before apply the Weierstrass Transform and apply Weierstrass Transform and will see the difference between the accuracy on training pixels of high resolution Quickbird satellite image. Principle Component analysis (PCA) is also used for dimension reduction and also used to check the variation in bands. The results shows that the separation between mean of the classes in the decision space is to be the main factor that leads to the high classification accuracy of Maximum Likelihood (ML) after using Weierstrass Transform than without using it."}
{"id": "2601.05219", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05219", "abs": "https://arxiv.org/abs/2601.05219", "authors": ["Maja Waldron"], "title": "CAOS: Conformal Aggregation of One-Shot Predictors", "comment": null, "summary": "One-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient in the one-shot setting due to data splitting and reliance on a single predictor. We propose Conformal Aggregation of One-Shot Predictors (CAOS), a conformal framework that adaptively aggregates multiple one-shot predictors and uses a leave-one-out calibration scheme to fully exploit scarce labeled data. Despite violating classical exchangeability assumptions, we prove that CAOS achieves valid marginal coverage using a monotonicity-based argument. Experiments on one-shot facial landmarking and RAFT text classification tasks show that CAOS produces substantially smaller prediction sets than split conformal baselines while maintaining reliable coverage."}
{"id": "2601.04966", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.04966", "abs": "https://arxiv.org/abs/2601.04966", "authors": ["Zixuan Feng", "Qiushi Chen", "Paul Griffin", "Le Bao"], "title": "A Bayesian Multi-State Data Integration Approach for Estimating County-level Prevalence of Opioid Misuse in the United States", "comment": null, "summary": "Drug overdose deaths, including from opioids, remain a significant public health threat to the United States (US). To abate the harms of opioid misuse, understanding its prevalence at the local level is crucial for stakeholders in communities to develop response strategies that effectively use limited resources. Although there exist several state-specific studies that provide county-level prevalence estimates, such estimates are not widely available across the country, as the datasets used in these studies are not always readily available in other states, which, therefore, has limited the wider applications of existing models. To fill this gap, we propose a Bayesian multi-state data integration approach that fully utilizes publicly available data sources to estimate county-level opioid misuse prevalence for all counties in the US. The hierarchical structure jointly models opioid misuse prevalence and overdose death outcomes, leverages existing county-level prevalence estimates in limited states and state-level estimates from national surveys, and accounts for heterogeneity across counties and states with counties' covariates and mixed effects. Furthermore, our parsimonious and generalizable modeling framework employs horseshoe+ prior to flexibly shrink coefficients and prevent overfitting, ensuring adaptability as new county-level prevalence data in additional states become available. Using real-world data, our model shows high estimation accuracy through cross-validation and provides nationwide county-level estimates of opioid misuse for the first time."}
{"id": "2601.05213", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.05213", "abs": "https://arxiv.org/abs/2601.05213", "authors": ["Mellissa Meisels", "Melody Huang", "Tiffany M. Tang"], "title": "Estimating Consensus Ideal Points Using Multi-Source Data", "comment": null, "summary": "In the advent of big data and machine learning, researchers now have a wealth of congressional candidate ideal point estimates at their disposal for theory testing. Weak relationships raise questions about the extent to which they capture a shared quantity -- rather than idiosyncratic, domain-specific factors -- yet different measures are used interchangeably in most substantive analyses. Moreover, questions central to the study of American politics implicate relationships between candidate ideal points and other variables derived from the same data sources, introducing endogeneity. We propose a method, consensus multidimensional scaling (CoMDS), which better aligns with how applied scholars use ideal points in practice. CoMDS captures the shared, stable associations of a set of underlying ideal point estimates and can be interpreted as their common spatial representation. We illustrate the utility of our approach for assessing relationships within domains of existing measures and provide a suite of diagnostic tools to aid in practical usage."}
{"id": "2601.04499", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.04499", "abs": "https://arxiv.org/abs/2601.04499", "authors": ["Baolin Chen", "Mengfei Ran"], "title": "A Generalized Adaptive Joint Learning Framework for High-Dimensional Time-Varying Models", "comment": null, "summary": "In modern biomedical and econometric studies, longitudinal processes are often characterized by complex time-varying associations and abrupt regime shifts that are shared across correlated outcomes. Standard functional data analysis (FDA) methods, which prioritize smoothness, often fail to capture these dynamic structural features, particularly in high-dimensional settings. This article introduces Adaptive Joint Learning (AJL), a regularization framework designed to simultaneously perform functional variable selection and structural changepoint detection in multivariate time-varying coefficient models. We propose a convex optimization procedure that synergizes adaptive group-wise penalization with fused regularization, effectively borrowing strength across multiple outcomes to enhance estimation efficiency. We provide a rigorous theoretical analysis of the estimator in the ultra-high-dimensional regime (p >> n), establishing non-asymptotic error bounds and proving that AJL achieves the oracle property--performing as well as if the true active set and changepoint locations were known a priori. A key theoretical contribution is the explicit handling of approximation bias via undersmoothing conditions to ensure valid asymptotic inference. The proposed method is validated through comprehensive simulations and an application to Primary Biliary Cirrhosis (PBC) data. The analysis uncovers synchronized phase transitions in disease progression and identifies a parsimonious set of time-varying prognostic markers."}
{"id": "2601.04913", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.04913", "abs": "https://arxiv.org/abs/2601.04913", "authors": ["Jan Martin Wenkel", "Michael Stanley Smith", "Nadja Klein"], "title": "Bayesian Additive Regression Tree Copula Processes for Scalable Distributional Prediction", "comment": null, "summary": "We show how to construct the implied copula process of response values from a Bayesian additive regression tree (BART) model with prior on the leaf node variances. This copula process, defined on the covariate space, can be paired with any marginal distribution for the dependent variable to construct a flexible distributional BART model. Bayesian inference is performed via Markov chain Monte Carlo on an augmented posterior, where we show that key sampling steps can be realized as those of Chipman et al. (2010), preserving scalability and computational efficiency even though the copula process is high dimensional. The posterior predictive distribution from the copula process model is derived in closed form as the push-forward of the posterior predictive distribution of the underlying BART model with an optimal transport map. Under suitable conditions, we establish posterior consistency for the regression function and posterior means and prove convergence in distribution of the predictive process and conditional expectation. Simulation studies demonstrate improved accuracy of distributional predictions compared to the original BART model and leading benchmarks. Applications to five real datasets with 506 to 515,345 observations and 8 to 90 covariates further highlight the efficacy and scalability of our proposed BART copula process model."}
{"id": "2601.04538", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.04538", "abs": "https://arxiv.org/abs/2601.04538", "authors": ["Kumar Utkarsh", "Nirmish R. Shah", "Tanvi Banerjee", "Daniel M. Abrams"], "title": "A new method for augmenting short time series, with application to pain events in sickle cell disease", "comment": "15 pages, 9 figures", "summary": "Researchers across different fields, including but not limited to ecology, biology, and healthcare, often face the challenge of sparse data. Such sparsity can lead to uncertainties, estimation difficulties, and potential biases in modeling. Here we introduce a novel data augmentation method that combines multiple sparse time series datasets when they share similar statistical properties, thereby improving parameter estimation and model selection reliability. We demonstrate the effectiveness of this approach through validation studies comparing Hawkes and Poisson processes, followed by application to subjective pain dynamics in patients with sickle cell disease (SCD), a condition affecting millions worldwide, particularly those of African, Mediterranean, Middle Eastern, and Indian descent."}
