<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 3]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.ME](#stat.ME) [Total: 10]
- [stat.AP](#stat.AP) [Total: 1]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Autocovariance and Optimal Design for Random Walk Metropolis-Hastings Algorithm](https://arxiv.org/abs/2511.10967)
*Jingyi Zhang,James C. Spall*

Main category: stat.CO

TL;DR: 本文研究了Metropolis-Hastings算法的协方差结构，在对称单峰目标分布和对称随机游走提议下建立了理论分析，并推导了最优提议设计。同时将高维情况下的协方差矩阵与经典的0.23平均接受率调优准则联系起来。


<details>
  <summary>Details</summary>
Motivation: Metropolis-Hastings算法在估计和模拟文献中已被广泛研究，但大多数先前工作关注收敛行为和渐近理论，而其协方差结构这一对理论和实现都很重要的统计性质仍较少被理解。

Method: 主要针对对称单峰目标分布和对称随机游走提议的标量情况提供新的理论见解，并建立最优提议设计。同时推导了超出此设置的更一般结果。

Result: 在标量情况下建立了协方差结构的理论分析，并确定了最优提议设计。在高维情况下，将协方差矩阵与经典的0.23平均接受率调优准则建立了联系。

Conclusion: 这项工作为Metropolis-Hastings算法的协方差结构提供了新的理论见解，填补了先前研究的空白，并为算法实现提供了理论指导。

Abstract: The Metropolis-Hastings algorithm has been extensively studied in the estimation and simulation literature, with most prior work focusing on convergence behavior and asymptotic theory. However, its covariance structure-an important statistical property for both theory and implementation-remains less understood. In this work, we provide new theoretical insights into the scalar case, focusing primarily on symmetric unimodal target distributions with symmetric random walk proposals, where we also establish an optimal proposal design. In addition, we derive some more general results beyond this setting. For the high-dimensional case, we relate the covariance matrix to the classical 0.23 average acceptance rate tuning criterion.

</details>


### [2] [Influence of Prior Distributions on Gaussian Process Hyperparameter Inference](https://arxiv.org/abs/2511.10950)
*Ayumi Mutoh,Junoh Heo*

Main category: stat.CO

TL;DR: 本文研究了高斯过程中长度尺度参数的不同先验分布和提议分布对预测性能的影响，通过模拟和真实数据实验评估其对预测准确性和不确定性量化的影响。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在工程设计和空间预测中被广泛使用，但其性能对协方差参数的估计很敏感。最大边际似然方法对初始化和优化设置敏感，而完全贝叶斯分层框架虽然能提供更稳健的不确定性量化，但先验分布的设定是一个关键挑战。现有软件包的默认先验对模型行为的影响往往未被充分探索。

Method: 采用分层高斯过程模型，通过模拟和真实数据实验，评估不同类型先验分布和提议分布在长度尺度参数上的影响。

Result: 通过系统评估不同先验和提议分布，揭示了它们对预测准确性和不确定性量化的具体影响。

Conclusion: 先验分布和提议分布的仔细选择对分层高斯过程模型的预测性能有显著影响，需要根据具体应用场景进行适当配置。

Abstract: Gaussian processes (GPs) are widely used metamodels for approximating expensive computer simulations, particularly in engineering design and spatial prediction. However, their performance can deteriorate significantly when covariance parameters are poorly estimated, highlighting the importance of accurate inference. The most common approach involves maximizing the marginal likelihood, yielding point estimates of these parameters. However, this approach is highly sensitive to initialization and optimization settings. An alternative is to adopt a fully Bayesian hierarchical framework, where the posterior distribution over the covariance parameters is inferred. This approach provides more robust uncertainty quantification and reduces sensitivity to parameter selection. Yet, a key challenge lies in the careful specification of prior distributions for these parameters. While many available software packages provide default priors, their influence on model behavior is often underexplored. Additionally, the choice of proposal distributions can also influence sampling efficiency and convergence. In this paper, we examine how different prior and proposal distributions over the lengthscale parameters $θ$ affect predictive performance in a hierarchical GP model, using both simulated and real data experiments. By evaluating various types of priors and proposals, we aim to better understand their influence on predictive accuracy and uncertainty quantification.

</details>


### [3] [Dual Riemannian Newton Method on Statistical Manifolds](https://arxiv.org/abs/2511.11318)
*Derun Zhou,Keisuke Yano,Mahito Sugiyama*

Main category: stat.CO

TL;DR: 提出了对偶黎曼牛顿方法，这是一种在具有度量和一对对偶仿射连接的流形上的牛顿型优化算法，利用信息几何中的对偶结构实现二阶效率。


<details>
  <summary>Details</summary>
Motivation: 现有二阶流形算法通常依赖Levi-Civita连接，忽略了信息几何中核心的对偶连接结构。自然梯度虽然利用Fisher信息度量，但只是一阶方法，在最优解附近收敛缓慢。

Method: 提出对偶黎曼牛顿方法，当回缩（指数映射的局部替代）由一个连接定义时，相关的牛顿方程用其对偶连接来构建，明确了对偶性如何塑造二阶更新。

Result: 建立了局部二次收敛性，并在代表性统计模型上通过实验验证了理论。

Conclusion: 对偶黎曼牛顿方法在保持与信息几何学习和推理基础的对偶结构兼容的同时，提供了二阶效率。

Abstract: In probabilistic modeling, parameter estimation is commonly formulated as a minimization problem on a parameter manifold. Optimization in such spaces requires geometry-aware methods that respect the underlying information structure. While the natural gradient leverages the Fisher information metric as a form of Riemannian gradient descent, it remains a first-order method and often exhibits slow convergence near optimal solutions. Existing second-order manifold algorithms typically rely on the Levi-Civita connection, thus overlooking the dual-connection structure that is central to information geometry. We propose the dual Riemannian Newton method, a Newton-type optimization algorithm on manifolds endowed with a metric and a pair of dual affine connections. The dual Riemannian Newton method explicates how duality shapes second-order updates: when the retraction (a local surrogate of the exponential map) is defined by one connection, the associated Newton equation is posed with its dual. We establish local quadratic convergence and validate the theory with experiments on representative statistical models. Thus, the dual Riemannian Newton method thus delivers second-order efficiency while remaining compatible with the dual structures that underlie modern information-geometric learning and inference.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [4] [Neural Local Wasserstein Regression](https://arxiv.org/abs/2511.10824)
*Inga Girshfeld,Xiaohui Chen*

Main category: stat.ML

TL;DR: 提出神经局部Wasserstein回归方法，通过局部定义的传输映射在Wasserstein空间中建模分布对分布的回归关系，克服了全局最优传输映射和切空间线性化的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局最优传输映射或切空间线性化，在逼近能力和多变量底层域几何方面存在限制，需要更灵活的非参数框架来处理分布对分布的回归问题。

Method: 基于经典核回归类比，使用2-Wasserstein距离的核权重在参考测度周围局部化估计器，同时用神经网络参数化传输算子以适应复杂数据几何。采用DeepSets架构、Sinkhorn近似损失和贪婪参考选择策略。

Result: 在Gaussian和混合模型的合成实验以及MNIST的分布预测任务中，该方法有效捕捉了现有方法无法处理的非线性和高维分布关系。

Conclusion: 神经局部Wasserstein回归提供了一个灵活框架，能够处理复杂的分布回归问题，避免了全局映射假设和线性化结构的限制。

Abstract: We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods.

</details>


### [5] [Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data](https://arxiv.org/abs/2511.10919)
*Jialei Liu,Jun Liao,Kuangnan Fang*

Main category: stat.ML

TL;DR: 提出了一种新的迁移学习框架，通过模型平均整合来自完全标记、半监督和PU数据集的异构信息，无需直接数据共享，用于解决PU学习中的数据稀缺和隐私约束问题。


<details>
  <summary>Details</summary>
Motivation: 解决PU学习中因缺乏明确负样本标记带来的挑战，特别是在欺诈检测和医疗诊断等高风险领域，同时应对数据稀缺和隐私约束问题。

Method: 为每种源域类型构建定制化的逻辑回归模型，通过模型平均将知识迁移到PU目标域，使用交叉验证准则最小化KL散度来确定最优权重组合源模型。

Result: 在广泛的模拟和真实信用风险数据分析中，该方法在预测准确性和鲁棒性方面优于其他比较方法，特别是在有限标记数据和异构环境下表现更佳。

Conclusion: 该框架为PU学习提供了一种有效的迁移学习解决方案，具有理论保证和实际应用价值，特别适用于数据稀缺和隐私敏感的场景。

Abstract: Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments.

</details>


### [6] [Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths](https://arxiv.org/abs/2511.11161)
*Yuzhen Zhao,Yating Liu,Marc Hoffmann*

Main category: stat.ML

TL;DR: 本文提出了一种基于神经网络的漂移函数非参数估计方法，用于时间齐次扩散过程，基于高频离散观测数据。该方法在组合漂移函数情况下建立了显式收敛率，数值实验显示其在高维设置下比B样条方法更有效。


<details>
  <summary>Details</summary>
Motivation: 解决时间齐次扩散过程中漂移函数的非参数估计问题，特别是在高维设置下传统方法（如B样条）难以有效捕捉局部特征和获得良好收敛率。

Method: 基于神经网络构建漂移函数估计器，利用N条独立轨迹的高频离散观测数据，推导非渐近收敛率，包含训练误差、近似误差和扩散相关项。

Result: 对于组合漂移函数建立了显式收敛率，数值实验表明经验收敛率与输入维度d无关，相比B样条方法，神经网络估计器在收敛率和局部特征捕捉方面表现更好。

Conclusion: 神经网络方法在高维扩散过程漂移函数估计中优于传统B样条方法，能有效处理局部波动并实现更好的收敛性能。

Abstract: This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.

</details>


### [7] [Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint](https://arxiv.org/abs/2511.11294)
*Bertille Tierny,Arthur Charpentier,François Hu*

Main category: stat.ML

TL;DR: 提出了一个后处理框架，用于分解线性模型中的预测偏差为直接（敏感属性）和间接（相关特征）成分，无需重新训练即可提供特征级别的公平性解释。


<details>
  <summary>Details</summary>
Motivation: 线性模型在高风险决策中广泛应用，但引入公平性约束（如人口统计均等）后，其对模型系数的影响以及预测偏差在特征间的分布仍然不透明。现有方法要么依赖不现实的假设，要么忽视敏感属性的明确作用。

Method: 扩展前人工作，提出后处理框架，可应用于任何线性模型之上，通过解析方法描述人口统计均等如何重塑每个模型系数（包括敏感和非敏感特征）。

Result: 在合成和真实数据集上的实验表明，该方法能捕捉到先前工作遗漏的公平性动态，为线性模型的责任部署提供了实用且可解释的工具。

Conclusion: 该框架无需重新训练，为模型审计和缓解提供了可操作的见解，实现了对公平性干预的透明、特征级解释，揭示了偏差如何通过相关变量持续存在或转移。

Abstract: Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [8] [Improving Variance and Confidence Interval Estimation in Small-Sample Propensity Score Analyses: Bootstrap vs. Asymptotic Methods](https://arxiv.org/abs/2511.10911)
*Baoshan Zhang,Sean M. O'Brien,Yuan Wu,Laine E. Thomas*

Main category: stat.ME

TL;DR: 在非随机研究中比较倾向得分方法中bootstrap与sandwich方差估计器的性能，特别关注小样本情况下的表现差异


<details>
  <summary>Details</summary>
Motivation: 随着罕见病治疗和外部对照临床试验的发展，小样本研究越来越常见，而sandwich估计器的渐近性质在小样本中可能不成立，bootstrap方法也可能存在准分离问题

Method: 进行蒙特卡洛模拟，比较bootstrap与sandwich方差和置信区间估计器的性能，系统评估将倾向得分视为固定与重新估计的影响，使用IPTW和AIPW估计器，在不同条件下评估性能

Result: sandwich估计器在小样本中表现较差，固定倾向得分方法不一定保守，分层bootstrap避免了准分离问题且表现良好

Conclusion: 在LIMIT-JIA试验中，这些差异足以改变统计结论，推荐在小样本情况下使用分层bootstrap方法

Abstract: Propensity score (PS) methods are widely used to estimate treatment effects in non-randomized studies. Variance is typically estimated using sandwich or bootstrap methods, which can either treat the PS as estimated or fixed. The latter is thought to be conservative. Comparisons between the sandwich and bootstrap estimators have been compared in moderate to large sample sizes, favoring the bootstrap estimator. With the growing interest in treatments for rare disease and externally controlled clinical trials, very small sample sizes are not uncommon and the asymptotic properties of sandwich estimators may not hold. Bootstrap methods that allow for PS re-estimation can also generate problems with quasi-separation in small samples. It is unclear whether it is safe to prefer sandwich estimators or to assume that treating the PS as fixed is conservative. We conducted a Monte Carlo simulation to compare the performance of bootstrap versus sandwich variance and CI estimators for average treatment effects estimated with PS methods. We systematically evaluated the impact of treating the PS as fixed versus re-estimating it. These methodological comparisons were performed using Inverse Probability of Treatment Weighting (IPTW) and Augmented Inverse Probability of Treatment Weighting (AIPW) estimators. Simulations assessed performance under various conditions, including small sample sizes and different outcome and treatment prevalences. We illustrate the differences in our motivating example, the LIMIT-JIA trial. We show that the sandwich estimators can perform quite poorly in small samples, and fixed PS methods are not necessarily conservative. A stratified bootstrap avoids quasi-separation and performs well. Differences were large enough to alter statistical conclusions in our motivating example, LIMIT-JIA.

</details>


### [9] [Extreme-PLS with missing data under weak dependence](https://arxiv.org/abs/2511.11338)
*Stéphane Girard,Cambyse Pakzad*

Main category: stat.ME

TL;DR: 该论文为存在缺失数据和弱时间依赖性的极端偏最小二乘(EPLS)降维开发了理论框架，扩展了EPLS方法以处理序列相关性和缺失数据，在重尾条件下建立了估计量的渐近行为，并通过模拟实验和实际环境数据应用验证了方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩展极端偏最小二乘方法以应对更现实的数据场景，包括序列相关性和缺失数据，特别是在重尾条件下当协变量缺失概率取决于响应变量的极端性时。

Method: 在重尾条件下采用单指标逆回归模型，引入依赖于响应极端性的随机缺失机制，在alpha混合框架下建立估计量的渐近性质。

Result: 在11种依赖方案（包括ARMA、GARCH和非线性ESTAR过程）的蒙特卡洛实验中，该方法在广泛的重尾和依赖场景下表现鲁棒，即使大量数据缺失时仍能有效工作。实际环境数据应用进一步证实了方法恢复有意义尾部方向的能力。

Conclusion: 提出的EPLS方法在处理缺失数据和弱时间依赖性时具有理论保证和实际有效性，能够稳健地估计极端依赖关系，为高维重尾数据分析提供了实用工具。

Abstract: This paper develops a theoretical framework for Extreme Partial Least Squares (EPLS) dimension reduction in the presence of missing data and weak temporal dependence. Building upon the recent EPLS methodology for modeling extremal dependence between a response variable and high-dimensional covariates, we extend the approach to more realistic data settings where both serial correlation and missing-ness occur. Specifically, we consider a single-index inverse regression model under heavy-tailed conditions and introduce a Missing-at-Random (MAR) mechanism acting on the covariates, whose probability depends on the extremeness of the response. The asymptotic behavior of the proposed estimator is established within an alpha-mixing framework, leading to consistency results under regularly varying tails. Extensive Monte-Carlo experiments covering eleven dependence schemes (including ARMA, GARCH, and nonlinear ESTAR processes) demonstrate that the method performs robustly across a wide range of heavy-tailed and dependent scenarios, even when substantial portions of data are missing. A real-world application to environmental data further confirms the method's capacity to recover meaningful tail directions.

</details>


### [10] [Multivariate longitudinal modeling of cross-sectional and lagged associations between a continuous time-varying endogenous covariate and a non-Gaussian outcome](https://arxiv.org/abs/2511.11114)
*Chiara Degan,Bart J. A. Mertens,Jelle Goeman,Nadine A. Ikelaar,Erik H. Niks,Pietro Spitali,Roula Tsonaka*

Main category: stat.ME

TL;DR: 提出了两种多元模型（JMM和JSM）来处理纵向研究中内生性时间变化协变量的问题，通过贝叶斯INLA方法估计，并推导了可解释的关联系数。


<details>
  <summary>Details</summary>
Motivation: 纵向研究中时间变化协变量常具有内生性，违反GLMM假设导致估计偏差，且缺失数据和非同步测量进一步增加分析难度。

Method: 扩展了两种多元模型：联合混合模型（JMM）通过联合建模随机效应诱导关联，联合尺度模型（JSM）使用尺度因子量化关联；采用贝叶斯INLA方法进行估计。

Result: 方法能够提供可解释的横截面关联和滞后关联的群体水平估计，在模拟研究和杜氏肌营养不良自然史研究中得到验证。

Conclusion: 提出的JMM和JSM模型有效解决了内生性协变量问题，提供了清晰的临床解释，但关联解释仍不够直观。

Abstract: In longitudinal studies, time-varying covariates are often endogenous, meaning their values depend on both their own history and that of the outcome variable. This violates key assumptions of Generalized Linear Mixed Effects Models (GLMMs), leading to biased and inconsistent estimates. Additionally, missing data and non-concurrent measurements between covariates and outcomes further complicate analysis, especially in rare or degenerative diseases where data is limited. To address these challenges, we propose an alternative use of two well-known multivariate models, each assuming a different form of the association. One induces the association by jointly modeling the random effects, called Joint Mixed Model (JMM); the other quantifies the association using a scaling factor, called Joint Scaled Model (JSM). We extend these models to accommodate continuous endogenous covariates and a wide range of longitudinal outcome types. A limitation in both cases is that the interpretation of the association is neither straightforward nor easy to communicate to scientists. Hence, we have numerically derived an association coefficient that measures the marginal relation between the outcome and the endogenous covariate. The proposed method provides interpretable, population-level estimates of cross-sectional associations (capturing relationships between covariates and outcomes measured at the same time point) and lagged associations (quantifying how past covariate values influence future outcomes), enabling clearer clinical insights. We fitted the JMM and JSM using a flexible Bayesian estimation approach, known as Integrated Nested Laplace Approximation (INLA), to overcome computation burden problems. These models will be presented along with the results of a simulation study and a natural history study on patients with Duchenne Muscular Dystrophy.

</details>


### [11] [Model Class Selection](https://arxiv.org/abs/2511.11355)
*Ryan Cecil,Lucas Mentch*

Main category: stat.ME

TL;DR: 该论文提出了模型类别选择(MCS)框架，扩展了传统的模型选择和模型集选择方法，旨在识别包含至少一个最优模型的多个模型集合。


<details>
  <summary>Details</summary>
Motivation: 传统模型选择只寻找单个最优模型，模型集选择识别一组近似最优模型，而本文进一步推广到模型类别选择，旨在识别包含最优模型的多个模型类别集合。

Method: 在温和条件下，提出了基于数据分割的方法作为MCS的通用解决方案，能够正式比较简单可解释模型与复杂黑盒机器学习模型的性能。

Result: 通过模拟和真实数据实验验证了所提出方法的有效性。

Conclusion: 模型类别选择框架为比较不同类别统计模型的性能提供了正式方法，特别有助于评估简单可解释模型是否能与复杂黑盒模型相媲美。

Abstract: Classical model selection seeks to find a single model within a particular class that optimizes some pre-specified criteria, such as maximizing a likelihood or minimizing a risk. More recently, there has been an increased interest in model set selection (MSS), where the aim is to identify a (confidence) set of near-optimal models. Here, we generalize the MSS framework further by introducing the idea of model class selection (MCS). In MCS, multiple model collections are evaluated, and all collections that contain at least one optimal model are sought for identification. Under mild conditions, data splitting based approaches are shown to provide general solutions for MCS. As a direct consequence, for particular datasets we are able to investigate formally whether classes of simpler and more interpretable statistical models are able to perform on par with more complex black-box machine learning models. A variety of simulated and real-data experiments are provided.

</details>


### [12] [Estimating the Effects of Heatwaves on Health: A Causal Inference Framework](https://arxiv.org/abs/2511.11433)
*Giulio Grossi,Leo Vanciu,Veronica Ballerini,Danielle Braun,Falco J. Bargagli Stoffi*

Main category: stat.ME

TL;DR: 提出因果推断框架解决热浪健康效应研究中的因果识别问题，引入合成控制方法和空间增强贝叶斯合成控制方法，在考虑空间依赖性和溢出效应的情况下估计热浪对医疗住院的因果效应。


<details>
  <summary>Details</summary>
Motivation: 现有热浪与健康关系研究多为关联性证据，缺乏因果解释，且统计方法依赖的假设在热浪背景下可能失效，需要更透明的因果识别框架。

Method: 提出因果推断框架，首先引入合成控制方法，然后开发空间增强贝叶斯合成控制方法，考虑空间依赖和溢出效应，通过经验蒙特卡洛模拟验证方法性能。

Result: 模拟显示两种方法表现良好，SA-SC方法在存在溢出效应和空间依赖时降低均方根误差并改善后验区间覆盖。应用于美国东北部1375万医疗保险受益人数据，估计2000-2019年热浪对热相关住院的因果效应。

Conclusion: 该因果推断框架提供了空间一致的反事实结果和稳健、可解释、透明的因果估计，明确解决了现有热浪效应文献中未检验的假设问题。

Abstract: The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature.

</details>


### [13] [Knockoffs for low dimensions: changing the nominal level post-hoc to gain power while controlling the FDR](https://arxiv.org/abs/2511.11166)
*Lasse Fischer,Konstantinos Sechidis*

Main category: stat.ME

TL;DR: 本文利用e值对knockoff方法进行后处理，允许在查看数据后调整名义FDR水平，从而在原始knockoff无法做出选择时提高发现能力，在已有发现时提高精度，且结果始终比原始knockoff更优。


<details>
  <summary>Details</summary>
Motivation: 传统knockoff方法在低维和稀疏信号设置下缺乏统计功效，且需要根据名义FDR水平设定最小选择数量，限制了其灵活性。

Method: 利用e值构建后处理knockoff程序，允许在观察数据后动态调整名义FDR水平，并将该方法应用于最近提出的去随机化knockoff程序。

Result: 后处理knockoff方法在不增加成本的情况下，始终提供比原始knockoff方法更具信息量的结果，能够提高发现能力和精度。

Conclusion: 基于e值的后处理knockoff方法显著提升了传统knockoff的灵活性和统计功效，为变量选择提供了更优的解决方案。

Abstract: Knockoffs are a powerful tool for controlled variable selection with false discovery rate (FDR) control. However, while they are frequently used in high-dimensional regressions, they lack power in low-dimensional and sparse signal settings. One of the main reasons is that knockoffs require a minimum number of selections, depending on the nominal FDR level. In this paper, we leverage e-values to allow the nominal level to be switched after looking at the data and applying the knockoff procedure. In this way, we can increase the nominal level in cases where the original knockoff procedure does not make any selections to potentially make discoveries. Also, in cases where the original knockoff procedure makes discoveries, we can often decrease the nominal level to increase the precision. These improvements come without any costs, meaning the results of our post-hoc knockoff procedure are always more informative than the results of the original knockoff procedure. Furthermore, we apply our technique to recently proposed derandomized knockoff procedures.

</details>


### [14] [Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility](https://arxiv.org/abs/2511.11564)
*Albert Tan,Mohsen Bayati,James Nordlund,Roman Istomin*

Main category: stat.ME

TL;DR: 该论文提出了在二分系统随机实验中处理干扰效应的方法，定义了主要和次要总处理效应，并开发了结合暴露映射、广义倾向得分和机器学习的估计器，在模拟和实际实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 研究二分系统随机实验中只有部分处理侧单元有资格接受处理的情况，所有单元继续交互产生干扰效应，需要解决这种干扰带来的估计偏差问题。

Method: 形式化资格约束的二分实验，定义主要总处理效应(PTTE)和次要总处理效应(STTE)，开发干扰感知的集成估计器，结合暴露映射、广义倾向得分和灵活机器学习方法，并引入连接处理和结果层面估计量的投影方法。

Result: 在已知真实值的模拟实验中，提出的估计器能够以低偏差和方差恢复PTTE和STTE，减少忽略干扰时可能产生的偏差。两个实地实验表明该方法修正了干扰偏差的方向，并在一个案例中改变了主要决策指标的符号和显著性。

Conclusion: 该方法有效解决了二分系统随机实验中的干扰问题，能够准确估计主要和次要总处理效应，在实际应用中具有重要价值。

Abstract: We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case.

</details>


### [15] [Online Spectral Density Estimation](https://arxiv.org/abs/2511.11296)
*Shahriar Hasnat Kazi,Niall Adams,Edward A. K. Cohen*

Main category: stat.ME

TL;DR: 开发了首个满足流式推断三大核心要求（固定内存、固定计算复杂度、时间自适应性）的在线谱密度估计算法，基于遗忘因子概念，无需先验知识即可适应数据生成过程的渐变或突变。


<details>
  <summary>Details</summary>
Motivation: 传统谱密度估计方法无法满足流式推断的实时性要求，需要开发能够在内存和计算资源受限条件下自适应跟踪时变谱特性的在线算法。

Method: 基于遗忘因子构建在线遗忘因子周期图，在平稳条件下渐近恢复离线对应方法的性质；进一步构建在线Whittle估计器，并开发自适应在线谱估计器，使用Whittle似然作为损失函数动态调整遗忘因子。

Result: 通过大量模拟研究和海洋漂流器速度数据的应用，证明该方法能够实时跟踪时变谱特性，具有强大的经验性能。

Conclusion: 提出的在线谱密度估计方法成功满足了流式推断的核心要求，能够有效适应数据生成过程的动态变化，为实时时间序列分析提供了实用工具。

Abstract: This paper develops the first online algorithms for estimating the spectral density function -- a fundamental object of interest in time series analysis -- that satisfies the three core requirements of streaming inference: fixed memory, fixed computational complexity, and temporal adaptivity. Our method builds on the concept of forgetting factors, allowing the estimator to adapt to gradual or abrupt changes in the data-generating process without prior knowledge of its dynamics. We introduce a novel online forgetting-factor periodogram and show that, under stationarity, it asymptotically recovers the properties of its offline counterpart. Leveraging this, we construct an online Whittle estimator, and further develop an adaptive online spectral estimator that dynamically tunes its forgetting factor using the Whittle likelihood as a loss. Through extensive simulation studies and an application to ocean drifter velocity data, we demonstrate the method's ability to track time-varying spectral properties in real-time with strong empirical performance.

</details>


### [16] [Interpolated stochastic interventions based on propensity scores, target policies and treatment-specific costs](https://arxiv.org/abs/2511.11353)
*Johan de Aguas*

Main category: stat.ME

TL;DR: 本文提出了离散治疗的随机干预族，将因果建模与成本敏感决策联系起来。通过成本惩罚信息投影获得闭式解，定义了可平滑插值的修改策略，并推导了高效影响函数和一步估计器。


<details>
  <summary>Details</summary>
Motivation: 连接因果建模与成本敏感决策，在现实约束下操作分级科学假设，帮助在投入实验资源前探索可行策略空间。

Method: 使用成本惩罚信息投影方法，从有机倾向得分和用户指定目标的独立乘积出发，得到Boltzmann-Gibbs耦合的闭式解。推导非参数模型下的高效影响函数，构建一步估计器。

Result: 提出的估计器在模拟中相比插件基线提高了稳定性和对错误指定的鲁棒性。框架支持在实验前探索可行策略空间。

Conclusion: 该框架可在现实约束下操作分级科学假设，帮助弥合观察证据与资源感知实验设计之间的差距。

Abstract: We introduce families of stochastic interventions for discrete treatments that connect causal modeling to cost-sensitive decision making. The interventions arise from a cost-penalized information projection of the independent product of the organic propensity and a user-specified target, yielding closed-form Boltzmann-Gibbs couplings. The induced marginals define modified stochastic policies that interpolate smoothly, via a single tilt parameter, from the organic law or from the target distribution toward a product-of-experts limit when all destination costs are strictly positive. One of these families recovers and extends incremental propensity score interventions, retaining identification without global positivity. For inference, we derive efficient influence functions under a nonparametric model for the expected outcomes after these policies and construct one-step estimators with uniform confidence bands. In simulations, the proposed estimators improve stability and robustness to nuisance misspecification relative to plug-in baselines. The framework can operationalize graded scientific hypotheses under realistic constraints: because inputs are modular, analysts can sweep feasible policy spaces, prototype candidates, and align interventions with budgets and logistics before committing experimental resources. This could help close the loop between observational evidence and resource-aware experimental design.

</details>


### [17] [A Recursive Theory of Variational State Estimation: The Dynamic Programming Approach](https://arxiv.org/abs/2511.11497)
*Filip Tronarp*

Main category: stat.ME

TL;DR: 本文从动态规划角度研究变分状态估计，提出了基于前向和后向动态规划的两种值函数递归方法，建立了与贝叶斯状态估计相对应的变分理论框架。


<details>
  <summary>Details</summary>
Motivation: 从动态规划视角重新审视变分状态估计问题，建立与经典贝叶斯状态估计理论相对应的变分理论框架。

Method: 采用前向和后向动态规划方法，分别推导对应的值函数递归公式，并提出了变分双滤波公式和次优变分滤波方法以降低计算复杂度。

Result: 在跳跃高斯-马尔可夫系统中，在特定因子化马尔可夫过程近似下，值函数递归是可处理的，仿真研究表明后验近似具有足够质量。

Conclusion: 变分状态估计可以从动态规划角度统一理解，提出的方法在保持理论一致性的同时提供了计算效率的改进方案。

Abstract: In this article, variational state estimation is examined from the dynamic programming perspective. This leads to two different value functional recursions depending on whether backward or forward dynamic programming is employed. The result is a theory of variational state estimation that corresponds to the classical theory of Bayesian state estimation. More specifically, in the backward method, the value functional corresponds to a likelihood that is upper bounded by the state likelihood from the Bayesian backward recursion. In the forward method, the value functional corresponds to an unnormalized density that is upper bounded by the unnormalized filtering density. Both methods can be combined to arrive at a variational two-filter formula. Additionally, it is noted that optimal variational filtering is generally of quadratic time-complexity in the sequence length. This motivates the notion of sub-optimal variational filtering, which also lower bounds the evidence but is of linear time-complexity. Another problem is the fact that the value functional recursions are generally intractable. This is briefly discussed and a simple approximation is suggested that retrieves the filter proposed by Courts et al. (2021). The methodology is examined in a jump Gauss--Markov system, where it is observed that the value functional recursions are tractable under a certain factored Markov process approximation. A simulation study demonstrates that the posterior approximation is of adequate quality.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [18] [Modeling U.S. Mortality and Suicide Rates by Integrating Mental Health and Socio-Economic Indicators](https://arxiv.org/abs/2511.10719)
*Brianne Weaver,Brigg Trendler,Chris Groendyke,Brian Hartman,Robert Richardson,Davey Erekson*

Main category: stat.AP

TL;DR: 本文开发了贝叶斯分层模型分析美国县级死亡率和自杀率，发现心理健康指标对年轻人群影响最大，社会经济因素与自杀率显著相关，支持时空方法在公共卫生政策中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着心理健康成为人口结果的重要因素，需要开发准确的死亡率模型来支持精算科学和公共卫生决策。

Method: 应用贝叶斯分层模型和条件自回归结构，分析2010-2023年美国县级数据，结合心理健康监测数据和社会经济指标。

Result: 教育水平、房价、婚姻率、种族构成、家庭规模和心理健康天数与自杀率显著相关；心理健康指标对年轻人群的死亡率影响最大；发现明显的区域聚集和时间一致性趋势。

Conclusion: 将心理健康监测数据整合到死亡率模型中能更好识别风险区域和脆弱人群，为公共卫生政策、资源分配和针对性干预提供依据。

Abstract: Accurate mortality modeling is central to actuarial science and public health, especially as mental health emerges as a significant factor in population outcomes. This paper develops and applies a Bayesian hierarchical model to analyze U.S. county-level mortality and suicide rates from 2010 to 2023. Applying a conditional autoregressive (CAR) structure to each combination of sex and age grouping, the model captures spatial and temporal trends while incorporating mental health surveillance data and socio-economic indicators. We first assess socio-economic covariates in predicting suicide. While the results vary considerably by age and sex, we find that the county-wide levels of educational attainment, housing prices, marriage rates, racial composition, household size, and poor mental health days all have significant relationships with suicide rates. We next consider the impact of various mental health indicators on all-cause and suicide-specific mortality and find that the strongest effects are observed in younger populations. The spatial and temporal correlation structures reveal substantial regional clustering and time-consistent trends in both all-cause mortality and suicide rates, supporting the use of spatio-temporal methods. Our findings highlight the value of integrating mental health surveillance data into mortality models to better identify emerging risk areas and vulnerable populations. This approach has the potential to inform public health policy, resource allocation, and targeted interventions aimed at reducing disparities in mortality and suicide across U.S. communities.

</details>
