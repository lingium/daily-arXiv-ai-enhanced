<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 3]
- [stat.AP](#stat.AP) [Total: 4]
- [stat.ME](#stat.ME) [Total: 6]
- [stat.CO](#stat.CO) [Total: 1]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Bayesian Physics-Informed Neural Networks for Inverse Problems (BPINN-IP): Application in Infrared Image Processing](https://arxiv.org/abs/2512.02495)
*Ali Mohammad-Djafari,Ning Chu,Li Wang*

Main category: stat.ML

TL;DR: 提出BPINN-IP框架，将贝叶斯推理与物理信息神经网络结合，用于解决反问题，支持不确定性量化，标准PINN是其最大后验估计特例。


<details>
  <summary>Details</summary>
Motivation: 传统变分正则化和贝叶斯推理方法在高维或复杂物理模型下计算受限，物理信息神经网络(PINNs)虽能嵌入物理约束但缺乏不确定性量化能力。

Method: 提出贝叶斯物理信息神经网络(BPINN-IP)框架，通过贝叶斯先验建模显式纳入训练数据生成、建模和测量不确定性，利用后验分布进行推理。

Result: 在红外图像处理的反问题（去卷积和超分辨率）中验证了框架有效性，展示了模拟和真实工业数据上的结果。

Conclusion: BPINN-IP提供了统一框架，能同时利用物理约束、先验知识和数据驱动推理，并支持不确定性量化，标准PINN是其最大后验估计特例。

Abstract: Inverse problems arise across scientific and engineering domains, where the goal is to infer hidden parameters or physical fields from indirect and noisy observations. Classical approaches, such as variational regularization and Bayesian inference, provide well established theoretical foundations for handling ill posedness. However, these methods often become computationally restrictive in high dimensional settings or when the forward model is governed by complex physics. Physics Informed Neural Networks (PINNs) have recently emerged as a promising framework for solving inverse problems by embedding physical laws directly into the training process of neural networks. In this paper, we introduce a new perspective on the Bayesian Physics Informed Neural Network (BPINN) framework, extending classical PINNs by explicitly incorporating training data generation, modeling and measurement uncertainties through Bayesian prior modeling and doing inference with the posterior laws. Also, as we focus on the inverse problems, we call this method BPINN-IP, and we show that the standard PINN formulation naturally appears as its special case corresponding to the Maximum A Posteriori (MAP) estimate. This unified formulation allows simultaneous exploitation of physical constraints, prior knowledge, and data-driven inference, while enabling uncertainty quantification through posterior distributions. To demonstrate the effectiveness of the proposed framework, we consider inverse problems arising in infrared image processing, including deconvolution and super-resolution, and present results on both simulated and real industrial data.

</details>


### [2] [Revisiting Theory of Contrastive Learning for Domain Generalization](https://arxiv.org/abs/2512.02831)
*Ali Alvandi,Mina Rezaei*

Main category: stat.ML

TL;DR: 该论文提出了针对对比学习的泛化理论分析，考虑了预训练和下游任务之间的两种不匹配：领域偏移和领域泛化，推导了新的泛化边界。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习理论假设下游任务类别分布与预训练阶段相同，但实际应用中下游任务可能出现分布偏移或引入新标签空间，导致领域泛化挑战。

Method: 引入新的泛化边界，分析两种场景：(1) 下游任务从相同潜在类别空间但分布偏移；(2) 下游任务涉及预训练未见的新标签空间，分析表示性能对统计差异的依赖。

Result: 分析揭示了对比学习表示性能如何依赖于预训练和下游分布之间的统计差异，为涉及预训练潜在类别集之外类分布的平均分类任务提供了可证明的性能保证。

Conclusion: 该工作扩展了对比学习的理论视角，为实际应用中常见的领域偏移和领域泛化问题提供了理论分析框架和性能保证。

Abstract: Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.

</details>


### [3] [Laplace Approximation For Tensor Train Kernel Machines In System Identification](https://arxiv.org/abs/2512.02532)
*Albert Saiapin,Kim Batselier*

Main category: stat.ML

TL;DR: 提出贝叶斯张量列车核机器，通过拉普拉斯近似和变分推理解决张量网络GP扩展中的设计挑战，实现高效训练和超参数优化。


<details>
  <summary>Details</summary>
Motivation: 为了解决高斯过程回归的可扩展性限制，张量网络方法被提出，但将其扩展到完全概率公式时面临设计挑战，特别是对于张量列车模型，不清楚哪个TT-core应该以贝叶斯方式处理。

Method: 提出贝叶斯张量列车核机器，对选定的TT-core应用拉普拉斯近似估计后验分布，对精度超参数使用变分推理，避免了交叉验证的需要。

Result: 实验表明核心选择基本独立于TT-秩和特征结构，变分推理替代交叉验证的同时提供高达65倍的训练加速，在逆动力学问题上验证了方法有效性。

Conclusion: 该方法成功解决了张量列车核机器完全贝叶斯化中的设计挑战，实现了高效的概率建模，为大规模高斯过程回归提供了实用解决方案。

Abstract: To address the scalability limitations of Gaussian process (GP) regression, several approximation techniques have been proposed. One such method is based on tensor networks, which utilizes an exponential number of basis functions without incurring exponential computational cost. However, extending this model to a fully probabilistic formulation introduces several design challenges. In particular, for tensor train (TT) models, it is unclear which TT-core should be treated in a Bayesian manner. We introduce a Bayesian tensor train kernel machine that applies Laplace approximation to estimate the posterior distribution over a selected TT-core and employs variational inference (VI) for precision hyperparameters. Experiments show that core selection is largely independent of TT-ranks and feature structure, and that VI replaces cross-validation while offering up to 65x faster training. The method's effectiveness is demonstrated on an inverse dynamics problem.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [4] [From 'What-is' to 'What-if' in Human-Factor Analysis: A Post-Occupancy Evaluation Case](https://arxiv.org/abs/2512.02060)
*Xia Chen,Ruiji Sun,Philipp Geyer,André Borrmann,Stefano Schiavon*

Main category: stat.AP

TL;DR: 论文主张在人为因素分析中明确区分描述性问题和干预性问题，采用因果推断框架来避免方法错配，并通过建筑环境中心入住后评估数据展示因果发现如何揭示传统关联分析遗漏的干预层次和方向关系。


<details>
  <summary>Details</summary>
Motivation: 传统人为因素分析主要使用相关性分析和显著性检验，这些描述性方法虽然能识别变量关联，但无法回答因果性问题。在因果问题中应用这些方法会忽略混杂变量和碰撞变量，可能导致偏差和错误决策。

Method: 提出明确区分描述性问题和干预性问题，应用因果推断框架解决干预性问题。使用建筑环境中心入住后评估数据作为案例，展示因果发现如何揭示干预层次和方向关系。

Result: 因果发现能够揭示传统关联分析遗漏的干预层次和方向关系，系统区分因果关联变量和独立变量，结合干预优先级排序能力，为复杂人本系统提供更有效的分析工具。

Conclusion: 在人为因素分析中明确区分描述性和干预性问题并应用因果推断框架，能够避免方法错配，支持反事实推理，在建筑科学、人机工程学等需要理解干预效果的领域具有广泛应用价值。

Abstract: Human-factor analysis typically employs correlation analysis and significance testing to identify relationships between variables. However, these descriptive ('what-is') methods, while effective for identifying associations, are often insufficient for answering causal ('what-if') questions. Their application in such contexts often overlooks confounding and colliding variables, potentially leading to bias and suboptimal or incorrect decisions.
  We advocate for explicitly distinguishing descriptive from interventional questions in human-factor analysis, and applying causal inference frameworks specifically to these problems to prevent methodological mismatches. This approach disentangles complex variable relationships and enables counterfactual reasoning. Using post-occupancy evaluation (POE) data from the Center for the Built Environment's (CBE) Occupant Survey as a demonstration case, we show how causal discovery reveals intervention hierarchies and directional relationships that traditional associational analysis misses. The systematic distinction between causally associated and independent variables, combined with intervention prioritization capabilities, offers broad applicability to complex human-centric systems, for example, in building science or ergonomics, where understanding intervention effects is critical for optimization and decision-making.

</details>


### [5] [Probabilistic Analysis of Various Squash Shots and Skill Study of Different Levels of Squash Players and Teams](https://arxiv.org/abs/2512.02210)
*Prathamesh Anwekar,Kaushal Kirpekar,Mahesh B,Sainath Bitragunta*

Main category: stat.AP

TL;DR: 提出用于壁球单打和双打比赛的紧凑概率模型，基于得分概率推导实用的技能比较规则，分析职业与业余选手的击球分布差异


<details>
  <summary>Details</summary>
Motivation: 量化壁球比赛中的战略差异，提供简单有效的球员和团队技能比较方法，为体育分析和教练提供可操作的见解

Method: 建立紧凑的概率模型分析两球员和四球员壁球比赛，基于得分概率推导技能比较规则，利用记录的击球类型和场地位置数据对比职业与业余选手的击球分布

Result: 职业选手使用更多样化的击球方式，偏好后场打法以保持控制；业余选手更集中于中场击球，产生更多失误，位置控制能力较弱

Conclusion: 该研究量化了壁球的战略差异，提供了简单的球员和团队技能比较方法，为体育分析和教练提供了有价值的见解

Abstract: We introduce a compact probabilistic model for two-player and two-team (four-player) squash matches, along with a practical skill-comparison rule derived from point-scoring probabilities. Using recorded shot types and court locations, we analyze how shot distributions differ between professional-level and intermediate-level players. Our analysis shows that professional players use a wider variety of shots and favor backcourt play to maintain control, while intermediate players concentrate more on mid-court shots, generate more errors, and exercise less positional control. These results quantify strategic differences in squash, offer a simple method to compare player and team skill, and provide actionable insights for sports analytics and coaching.

</details>


### [6] [Estimating excess mortality during the Covid-19 pandemic in Aotearoa New Zealand: Addendum](https://arxiv.org/abs/2512.02266)
*Michael J. Plank,Pubudu Senanayake,Richard Lyon*

Main category: stat.AP

TL;DR: 更新新西兰2020-2023年超额死亡率估计：基于新人口数据重新计算，超额死亡率从0.7%修正为2.0%


<details>
  <summary>Details</summary>
Motivation: 新西兰统计局发布了更新的人口估计数据，需要基于新数据重新评估之前研究的超额死亡率估计

Method: 将原始模型应用于新的人口数据，重新计算2020-2023年期间新西兰的超额死亡率

Result: 更新后的超额死亡率估计为2.0%（95%置信区间[0.5%, 3.3%]），比原始估计高出1.3个百分点，主要原因是新的人口估计数据更小

Conclusion: 虽然超额死亡率估计有所调整，但原始文章的主要结论仍然适用，新的人口数据导致死亡率估计上升

Abstract: In our previous article, we estimated excess mortality during in Aotearoa New Zealand for 2020 to 2023. Since our work was published, updated population estimates have been released by Statistics NZ. In this short letter, we provide the results of applying our original model to the new population data. Our updated excess mortality estimate of 2.0% (95% CI [0.5%, 3.3%]) is 1.3 percentage points higher than our original estimate because the new population estimates for the period 2020 to 2023 are smaller, but the main conclusions of our original article still apply.

</details>


### [7] [Leveraging ontologies to predict biological activity of chemicals across genes](https://arxiv.org/abs/2512.02327)
*Jennifer N. Kampe,David B. Dunson,Celeste K. Carberry,Julia E. Rager,Daniel Zilber,Kyle P. Messier*

Main category: stat.AP

TL;DR: DART是一种贝叶斯因子模型，利用化学结构特性和基因本体论信息预测化学物质在基因水平上的生物活性，填补高通量筛选数据稀疏性的空白。


<details>
  <summary>Details</summary>
Motivation: 高通量筛选数据稀疏，大多数化学-基因对缺乏剂量-反应曲线数据，且基因本体论虽能描述基因相似性，但无法反映环境污染物对各通路的敏感性差异。

Method: 提出剂量-活性反应追踪方法，在贝叶斯因子模型中整合化学结构特性和基因本体论信息，预测化学物质在基因上的生物活性。

Result: 通过模拟研究和PFAS暴露HepG2细胞的大规模多实验数据集应用，DART能揭示驱动剂量-反应行为的潜在过程，并为缺乏实验数据的化学-基因对预测新的活性谱。

Conclusion: DART为毒理学家提供了适用于不同HTS平台的灵活工具，可用于化学物质优先级排序和推断实验激活的结构与功能机制。

Abstract: High-throughput screening (HTS) is useful for evaluating chemicals for potential human health risks. However, given the extraordinarily large number of genes, assay endpoints, and chemicals of interest, available data are sparse, with dose-response curves missing for the vast majority of chemical-gene pairs. Although gene ontologies characterize similarity among genes with respect to known cellular functions and biological pathways, the sensitivity of various pathways to environmental contaminants remains unclear. We propose a novel Dose-Activity Response Tracking (DART) approach to predict the biological activity of chemicals across genes using information on chemical structural properties and gene ontologies within a Bayesian factor model. Designed to provide toxicologists with a flexible tool applicable across diverse HTS assay platforms, DART reveals the latent processes driving dose-response behavior and predicts new activity profiles for chemical-gene pairs lacking experimental data. We demonstrate the performance of DART through simulation studies and an application to a vast new multi-experiment data set consisting of dose-response observations generated by the exposure of HepG2 cells to per- and polyfluoroalkyl substances (PFAS), where it provides actionable guidance for chemical prioritization and inference on the structural and functional mechanisms underlying assay activation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [8] [Tolerance Intervals Using Dirichlet Processes](https://arxiv.org/abs/2512.02178)
*Seokjun Choi,Tony Pourmohamad,Bruno Sansó*

Main category: stat.ME

TL;DR: 提出基于狄利克雷过程的贝叶斯非参数容忍区间方法，克服传统参数和非参数方法的局限性，在保持计算效率的同时对分布假设具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在非临床药物开发中，容忍区间对确保产品质量至关重要。传统参数方法需要小样本但对分布误设敏感，非参数方法灵活但需要大样本。需要一种能克服这些局限性的新方法。

Method: 提出基于狄利克雷过程的贝叶斯非参数容忍区间方法，开发了基于狄利克雷过程可解析处理的量化过程的计算高效容忍区间构建算法。

Result: 模拟研究表明，新方法对分布假设具有很强鲁棒性，性能与现有容忍区间方法相当。应用于效价数据的容忍区间估计，展示了实际应用效果。

Conclusion: 狄利克雷过程贝叶斯非参数容忍区间方法有效克服了传统方法的局限性，在保持计算效率的同时提供了对分布假设的强鲁棒性，适用于非临床药物开发中的质量保证。

Abstract: In nonclinical pharmaceutical development, tolerance intervals are critical in ensuring product and process quality. They are statistical intervals designed to contain a specified proportion of the population with a given confidence level. Parametric and non-parametric methods have been developed to obtain tolerance intervals. The former work with small samples but can be affected by distribution misspecification. The latter offer larger flexibility but require large sample sizes. As an alternative, we propose Dirichlet process-based Bayesian nonparametric tolerance intervals to overcome the limitations. We develop a computationally efficient tolerance interval construction algorithm based on the analytically tractable quantile process of the Dirichlet process. Simulation studies show that our new approach is very robust to distributional assumptions and performs as efficiently as existing tolerance interval methods. To illustrate how the model works in practice, we apply our method to the tolerance interval estimation for potency data.

</details>


### [9] [Discrete Sequential Barycenter Arrays: Representation, Approximation, and Modeling of Probability Measures](https://arxiv.org/abs/2512.02249)
*Alejandro Jara,Carlos Sing-Long*

Main category: stat.ME

TL;DR: 本文提出了一种基于离散顺序重心数组(SBA)的单变量概率测度新表示方法，能够系统地在保持完全建模灵活性的同时强制实施均值等关键泛函约束。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏在保持完全建模灵活性的同时系统性地强制实施均值等关键泛函约束的工具。需要一种新的概率测度表示方法来克服这一限制。

Method: 引入基于离散顺序重心数组(SBA)的单变量概率测度新表示方法。研究SBA表示的结构特性，建立新的逼近结果，证明其收敛性和精确性。基于此构建能够保持指定均值约束的概率模型。

Result: 证明了SBA离散逼近在弱拓扑和Wasserstein距离下的收敛性，对有限离散支撑分布具有精确表示。识别了一类具有规则分区和细化网格的测度，并证明其在标准概率拓扑中稠密。构建了能够保持指定均值约束的混合模型。

Conclusion: SBA表示提供了一种原则性机制，能够在非参数建模中纳入均值约束，同时保持强大的逼近特性。该方法为构建灵活的概率模型提供了系统工具，在模拟和实际数据中均得到验证。

Abstract: Constructing flexible probability models that respect constraints on key functionals -- such as the mean -- is a fundamental problem in nonparametric statistics. Existing approaches lack systematic tools for enforcing such constraints while retaining full modeling flexibility. This paper introduces a new representation for univariate probability measures based on discrete sequential barycenter arrays (SBA). We study structural properties of SBA representations and establish new approximation results. In particular, we show that for any target distribution, its SBA-based discrete approximations converge in both the weak topology and in Wasserstein distances, and that the representation is exact for all distributions with finite discrete support. We further characterize a broad class of measures whose SBA partitions exhibit regularity and induce increasingly fine meshes, and we prove that this class is dense in standard probabilistic topologies. These theoretical results enable the construction of probability models that preserve prescribed values -- or full distributions -- of the mean while maintaining large support. As an application, we derive a mixture model for density estimation whose induced mixing distribution has a fixed or user-specified mean. The resulting framework provides a principled mechanism for incorporating mean constraints in nonparametric modeling while preserving strong approximation properties. The approach is illustrated using both simulated and real data.

</details>


### [10] [Efficient and Intuitive Two-Phase Validation Across Multiple Models via Principal Components](https://arxiv.org/abs/2512.02182)
*Sarah C. Lotspeich,Cole Manschot*

Main category: stat.ME

TL;DR: 提出基于主成分分析的极端尾抽样方法，用于两阶段抽样设计，平衡多个竞争模型的效率需求


<details>
  <summary>Details</summary>
Motivation: 两阶段抽样中，当存在多个竞争模型时，传统的针对单一模型的抽样策略难以平衡不同分析需求，需要一种能同时优化多个模型效率的抽样方法

Method: 使用主成分分析总结误差易发协变量的内在变异性，然后对第一主成分的极端值（最小或最大）进行抽样验证

Result: 模拟和NHANES数据应用显示，基于第一主成分的极端尾抽样相对于针对单一模型的抽样，能在多个感兴趣模型中同时获得效率提升

Conclusion: 提出的主成分极端尾抽样策略为多模型分析提供了一种直观易用的解决方案，已实现为开源R包auditDesignR

Abstract: Two-phase sampling offers a cost-effective way to validate error-prone measurements in observational databases or randomized trials. Inexpensive or easy-to-obtain information is collected for the entire study in Phase I. Then, a subset of patients undergoes cost-intensive validation to collect more accurate data in Phase II. Critically, any Phase I variables can be used to strategically select the Phase II subset, often enriched for a particular model of interest. However, when balancing primary and secondary analyses in the same study, competing models and priorities can result in poorly defined objectives for the most informative Phase II sampling criterion. We propose an intuitive, easy-to-use solution that balances and prioritizes explaining the largest amount of variability across all models of interest. Using principal components to succinctly summarize the inherent variability of the error-prone covariates for all models. Then, we sample patients with the most "extreme" principal components (i.e., the smallest or largest values) for validation. Through simulations and an application to data from the National Health and Nutrition Examination Survey (NHANES), we show that extreme tail sampling on the first principal component offers simultaneous efficiency gains across multiple models of interest relative to sampling for one specific model. Our proposed sampling strategy is implemented in the open-source R package, auditDesignR.

</details>


### [11] [Implicit score-driven filters for time-varying parameter models](https://arxiv.org/abs/2512.02744)
*Rutger-Jan Lange,Bram van Os,Dick van Dijk*

Main category: stat.ME

TL;DR: 提出了一种基于隐式得分驱动（ISD）更新的观测驱动建模框架，该框架允许模型参数随时间变化，通过最大化观测对数密度并惩罚相对于一步预测参数的加权L2范数来实现。


<details>
  <summary>Details</summary>
Motivation: 现有的显式得分驱动（ESD）模型通过对观测对数密度进行线性近似来更新参数，但这种方法可能无法充分利用完整的密度信息。ISD框架旨在保留完整的密度信息，从而全局化ESD更新的良好局部性质。

Method: 提出隐式得分驱动（ISD）更新方法：通过最大化观测对数密度相对于参数向量的值，同时惩罚相对于一步预测参数的加权L2范数，形成隐式随机梯度更新。该方法保留了完整的观测密度，而非像ESD那样进行线性近似。

Result: 对于对数凹观测密度（无论是否正确设定），ISD滤波器对所有学习率都是稳定的，其更新在均方误差意义下向（伪）真实参数收缩。在金融和宏观经济学的模拟和实证应用中证明了ISD滤波器的有效性。

Conclusion: ISD框架通过保留完整的观测密度，全局化了ESD更新的良好局部性质，提供了更稳定和收缩性的参数更新机制，在时间序列建模中具有实用价值。

Abstract: We propose an observation-driven modeling framework that permits time variation in the model parameters using an implicit score-driven (ISD) update. The ISD update maximizes the logarithmic observation density with respect to the parameter vector, while penalizing the weighted L2 norm relative to a one-step-ahead predicted parameter. This yields an implicit stochastic-gradient update. We show that the popular class of explicit score-driven (ESD) models arises if the observation log density is linearly approximated around the prediction. By preserving the full density, the ISD update globalizes favorable local properties of the ESD update. Namely, for log-concave observation densities, whether correctly specified or not, the ISD filter is stable for all learning rates, while its updates are contractive in mean squared error toward the (pseudo-)true parameter at every time step. We demonstrate the usefulness of ISD filters in simulations and empirical illustrations in finance and macroeconomics.

</details>


### [12] [Analysis of hypothesis tests for multiple uncertain finite populations with applications to normal uncertainty distributions](https://arxiv.org/abs/2512.02832)
*Fan Zhang,Zhiming Li*

Main category: stat.ME

TL;DR: 本文扩展了不确定统计中的假设检验，从单总体扩展到多总体，定义了不确定族错误率来控制整体误差，提出了两总体假设检验，并在此基础上发展了多总体的同质性检验和公共检验。


<details>
  <summary>Details</summary>
Motivation: 假设检验在基于不确定测度的不确定统计中起着关键作用。现有研究主要关注单个不确定总体的参数假设检验，但在实际应用中经常需要处理多个总体的情况。因此，需要将参数假设从单个不确定总体扩展到多个总体，以解决更广泛的统计推断问题。

Method: 1. 定义不确定族错误率来控制同时检验的整体误差
2. 提出两不确定总体的假设检验，推导零假设在显著性水平下的拒绝域
3. 基于此发展多总体的同质性检验，评估未知总体参数是否存在显著差异
4. 当有限总体或子集间参数无显著差异时，使用公共检验判断是否等于固定常数
5. 针对正态不确定总体，在均值未知、标准差未知、两者均未知三种情况下进行同质性检验和公共检验
6. 通过数值模拟验证方法的可行性，并提供实际案例说明有效性

Result: 数值模拟证明了所提方法的可行性和准确性。实际案例展示了这些方法在实践中的有效性。具体来说，成功实现了对多不确定总体的假设检验，包括两总体检验、多总体同质性检验以及公共检验，特别是在正态不确定总体参数推断方面取得了良好效果。

Conclusion: 本文成功将不确定统计中的假设检验从单总体扩展到多总体，建立了完整的多总体假设检验框架。提出的不确定族错误率、两总体检验、同质性检验和公共检验方法为解决实际中的多总体统计推断问题提供了有效工具，特别是在正态不确定总体参数分析中表现出良好的性能。

Abstract: Hypothesis test plays a key role in uncertain statistics based on uncertain measure. This paper extends the parametric hypothesis of a single uncertain population to multiple cases, thereby addressing a broader range of scenarios. First, an uncertain family-wise error rate is defined to control the overall error in simultaneous testing. Subsequently, a hypothesis test of two uncertain populations is proposed, and the rejection region for the null hypothesis at a significance level is derived, laying the foundation for further analysis. Building on this, a homogeneity test for multiple populations is developed to assess whether the unknown population parameters differ significantly. When there is no significant difference in these parameters among finite populations or within a subset, a common test is used to determine whether they equal a fixed constant. Finally, homogeneity and common tests for normal uncertain populations with means and standard deviations are conducted under three cases: only means, only standard deviations, or both are unknown. Numerical simulations demonstrate the feasibility and accuracy of the proposed methods, and a real example is provided to illustrate their effectiveness.

</details>


### [13] [Correcting for sampling variability in maximum likelihood-based one-sample log-rank tests](https://arxiv.org/abs/2512.02878)
*Moritz Fabian Danzer,Rene Schmidt*

Main category: stat.ME

TL;DR: 本文针对单臂研究中单样本对数秩检验的参考曲线估计不确定性问题，提出了考虑历史数据参数估计变异性的方差调整方法。


<details>
  <summary>Details</summary>
Motivation: 在罕见病或儿科药物早期开发中，单臂研究常见，需将观察终点与历史数据参考值比较。对于时间-事件终点，单样本对数秩检验使用参考曲线进行比较，但参考曲线通常从历史对照组通过参数方法估计，存在估计不确定性，而传统检验未考虑此变异性。

Method: 分析参考曲线通过最大似然法参数估计时的估计不确定性，提出调整单样本对数秩检验方差估计的方法以考虑此变异性，通过数据实例说明并利用模拟进行详细分析。

Result: 开发了考虑历史数据参数估计变异性的检验程序，通过模拟分析验证了调整方法的有效性，展示了在实际数据应用中的表现。

Conclusion: 单样本对数秩检验应考虑参考曲线的估计不确定性，提出的方差调整方法能更准确地反映检验的统计特性，提高单臂研究结果的可解释性。

Abstract: Single-arm studies in the early development phases of new treatments are not uncommon in the context of rare diseases or in paediatrics. If an assessment of efficacy is to be made at the end of such a study, the observed endpoints can be compared with reference values that can be derived from historical data. For a time-to-event endpoint, a statistical comparison with a reference curve can be made using the one-sample log-rank test. In order to ensure the interpretability of the results of this test, the role of the reference curve is crucial. This quantity is often estimated from a historical control group using a parametric procedure. Hence, it should be noted that it is subject to estimation uncertainty. However, this aspect is not taken into account in the one-sample log-rank test statistic. We analyse this estimation uncertainty for the common situation that the reference curve is estimated parametrically using the maximum likelihood method, and indicate how the variance estimation of the one-sample log-rank test can be adapted in order to take this variability into account. The resulting test procedures are illustrated using a data example and analysed in more detail using simulations.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [14] [Semiparametric Robust Estimation of Population Location](https://arxiv.org/abs/2512.03021)
*Ananyabrata Barua,Ayanendranath Basu*

Main category: stat.CO

TL;DR: 提出一种半参数方法，仅对主导信号进行参数化建模，将噪声背景完全非参数化处理，通过FFT加速算法实现高效计算


<details>
  <summary>Details</summary>
Motivation: 现实测量数据通常包含主导信号和噪声背景，传统方法存在偏差或效率问题，需要一种既稳健又高效的处理方法

Method: 采用半参数方法：主导成分参数化建模，背景噪声完全非参数化；提出基于FFT加速的近似似然最大化算法

Result: FFT插件算法相比传统加权EM方法实现数量级的速度提升，同时保持统计准确性和大样本性质

Conclusion: 该方法在计算效率和统计稳健性之间取得了良好平衡，为处理含噪声背景的测量数据提供了有效解决方案

Abstract: Real-world measurements often comprise a dominant signal contaminated by a noisy background. Robustly estimating the dominant signal in practice has been a fundamental statistical problem. Classically, mixture models have been used to cluster the heterogeneous population into homogeneous components. Modeling such data with fully parametric models risks bias under misspecification, while fully nonparametric approaches can dissipate power and computational resources. We propose a middle path: a semiparametric method that models only the dominant component parametrically and leaves the background completely nonparametric, yet remains computationally scalable and statistically robust. So instead of outlier downweighting, traditionally done in robust statistics literature, we maximize the observed likelihood such that the noisy background is absorbed by the nonparametric component. Computationally, we propose a new approximate FFT-accelerated likelihood maximization algorithm. Empirically, this FFT plug-in achieves order-of-magnitude speedups over vanilla weighted EM while preserving statistical accuracy and large sample properties.

</details>
