{"id": "2512.10053", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10053", "abs": "https://arxiv.org/abs/2512.10053", "authors": ["Tiago Brogueira", "Mário A. T. Figueiredo"], "title": "LxCIM: a new rank-based binary classifier performance metric invariant to local exchange of classes", "comment": "28 pages, 7 figures", "summary": "Binary classification is one of the oldest, most prevalent, and studied problems in machine learning. However, the metrics used to evaluate model performance have received comparatively little attention. The area under the receiver operating characteristic curve (AUROC) has long been a standard choice for model comparison. Despite its advantages, AUROC is not always ideal, particularly for problems that are invariant to local exchange of classes (LxC), a new form of metric invariance introduced in this work. To address this limitation, we propose LxCIM (LxC-invariant metric), which is not only rank-based and invariant under local exchange of classes, but also intuitive, logically consistent, and always computable, while enabling more detailed analysis through the cumulative accuracy-decision rate curve. Moreover, LxCIM exhibits clear theoretical connections to AUROC, accuracy, and the area under the accuracy-decision rate curve (AUDRC). These relationships allow for multiple complementary interpretations: as a symmetric form of AUROC, a rank-based analogue of accuracy, or a more representative and more interpretable variant of AUDRC. Finally, we demonstrate the direct applicability of LxCIM to the bivariate causal discovery problem (which exhibits invariance to local exchange of classes) and show how it addresses the acknowledged limitations of existing metrics used in this field. All code and implementation details are publicly available at github.com/tiagobrogueira/Causal-Discovery-In-Exchangeable-Data."}
{"id": "2512.10188", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.10188", "abs": "https://arxiv.org/abs/2512.10188", "authors": ["Gabriel Clara", "Yazan Mash'al"], "title": "The Interplay of Statistics and Noisy Optimization: Learning Linear Predictors with Random Data Weights", "comment": null, "summary": "We analyze gradient descent with randomly weighted data points in a linear regression model, under a generic weighting distribution. This includes various forms of stochastic gradient descent, importance sampling, but also extends to weighting distributions with arbitrary continuous values, thereby providing a unified framework to analyze the impact of various kinds of noise on the training trajectory. We characterize the implicit regularization induced through the random weighting, connect it with weighted linear regression, and derive non-asymptotic bounds for convergence in first and second moments. Leveraging geometric moment contraction, we also investigate the stationary distribution induced by the added noise. Based on these results, we discuss how specific choices of weighting distribution influence both the underlying optimization problem and statistical properties of the resulting estimator, as well as some examples for which weightings that lead to fast convergence cause bad statistical performance."}
{"id": "2512.10256", "categories": ["stat.ML", "cs.LG", "math.DS", "math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.10256", "abs": "https://arxiv.org/abs/2512.10256", "authors": ["Quanjun Lang", "Jianfeng Lu"], "title": "Error Analysis of Generalized Langevin Equations with Approximated Memory Kernels", "comment": null, "summary": "We analyze prediction error in stochastic dynamical systems with memory, focusing on generalized Langevin equations (GLEs) formulated as stochastic Volterra equations. We establish that, under a strongly convex potential, trajectory discrepancies decay at a rate determined by the decay of the memory kernel and are quantitatively bounded by the estimation error of the kernel in a weighted norm. Our analysis integrates synchronized noise coupling with a Volterra comparison theorem, encompassing both subexponential and exponential kernel classes. For first-order models, we derive moment and perturbation bounds using resolvent estimates in weighted spaces. For second-order models with confining potentials, we prove contraction and stability under kernel perturbations using a hypocoercive Lyapunov-type distance. This framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction. Numerical examples validate these theoretical findings."}
{"id": "2512.10401", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10401", "abs": "https://arxiv.org/abs/2512.10401", "authors": ["Jennifer Rosina Andersson", "Zheng Zhao"], "title": "Diffusion differentiable resampling", "comment": null, "summary": "This paper is concerned with differentiable resampling in the context of sequential Monte Carlo (e.g., particle filtering). We propose a new informative resampling method that is instantly pathwise differentiable, based on an ensemble score diffusion model. We prove that our diffusion resampling method provides a consistent estimate to the resampling distribution, and we show by experiments that it outperforms the state-of-the-art differentiable resampling methods when used for stochastic filtering and parameter estimation."}
{"id": "2512.10254", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.10254", "abs": "https://arxiv.org/abs/2512.10254", "authors": ["Juan Sosa", "Erika Martínez", "Danna L. Cruz-Reyes"], "title": "Peace Sells, But Whose Songs Connect? Bayesian Multilayer Network Analysis of the Big 4 of Thrash Metal", "comment": "52 pages, 8 figures, 8 tables", "summary": "We propose a Bayesian framework for multilayer song similarity networks and apply it to the complete studio discographies of the \"Big 4\" of thrash metal (Metallica, Slayer, Megadeth, Anthrax). Starting from raw audio, we construct four feature-specific layers (loudness, brightness, tonality, rhythm), augment them with song exogenous information, and represent each layer as a k-nearest neighbor graph. We then fit a family of hierarchical probit models with global and layer-specific baselines, node- and layer-specific sociability effects, dyadic covariates, and alternative forms of latent structure (bilinear, distance-based, and stochastic block communities), comparing increasingly flexible specifications using posterior predictive checks, discrimination and calibration metrics (AUC, Brier score, log-loss), and information criteria (DIC, WAIC). Across all bands, the richest stochastic block specification attains the best predictive performance and posterior predictive fit, while revealing sparse but structured connectivity, interpretable covariate effects (notably album membership and temporal proximity), and latent communities and hubs that cut across albums and eras. Taken together, these results illustrate how Bayesian multilayer network models can help organize high-dimensional audio and text features into coherent, musically meaningful patterns."}
{"id": "2512.10055", "categories": ["stat.ME", "physics.data-an", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.10055", "abs": "https://arxiv.org/abs/2512.10055", "authors": ["Yannick Kuhn", "Masaki Adachi", "Micha Philipp", "David A. Howey", "Birger Horstmann"], "title": "A Primer on Bayesian Parameter Estimation and Model Selection for Battery Simulators", "comment": "22 pages, 19 figures", "summary": "Physics-based battery modelling has emerged to accelerate battery materials discovery and performance assessment. Its success, however, is still hindered by difficulties in aligning models to experimental data. Bayesian approaches are a valuable tool to overcome these challenges, since they enable prior assumptions and observations to be combined in a principled manner that improves numerical conditioning. Here we introduce two new algorithms to the battery community, SOBER and BASQ, that greatly speed up Bayesian inference for parameterisation and model comparison. We showcase how Bayesian model selection allows us to tackle data observability, model identifiability, and data-informed model development together. We propose this approach for the search for battery models of novel materials."}
{"id": "2512.10188", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.10188", "abs": "https://arxiv.org/abs/2512.10188", "authors": ["Gabriel Clara", "Yazan Mash'al"], "title": "The Interplay of Statistics and Noisy Optimization: Learning Linear Predictors with Random Data Weights", "comment": null, "summary": "We analyze gradient descent with randomly weighted data points in a linear regression model, under a generic weighting distribution. This includes various forms of stochastic gradient descent, importance sampling, but also extends to weighting distributions with arbitrary continuous values, thereby providing a unified framework to analyze the impact of various kinds of noise on the training trajectory. We characterize the implicit regularization induced through the random weighting, connect it with weighted linear regression, and derive non-asymptotic bounds for convergence in first and second moments. Leveraging geometric moment contraction, we also investigate the stationary distribution induced by the added noise. Based on these results, we discuss how specific choices of weighting distribution influence both the underlying optimization problem and statistical properties of the resulting estimator, as well as some examples for which weightings that lead to fast convergence cause bad statistical performance."}
{"id": "2512.10066", "categories": ["stat.AP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10066", "abs": "https://arxiv.org/abs/2512.10066", "authors": ["Yongkai Chen", "Samuel WK Wong", "SC Kou"], "title": "Classifying Metamorphic versus Single-Fold Proteins with Statistical Learning and AlphaFold2", "comment": null, "summary": "The remarkable success of AlphaFold2 in providing accurate atomic-level prediction of protein structures from their amino acid sequence has transformed approaches to the protein folding problem. However, its core paradigm of mapping one sequence to one structure may only be appropriate for single-fold proteins with one stable conformation. Metamorphic proteins, which can adopt multiple distinct conformations, have conformational diversity that cannot be adequately modeled by AlphaFold2. Hence, classifying whether a given protein is metamorphic or single-fold remains a critical challenge for both laboratory experiments and computational methods. To address this challenge, we developed a novel classification framework by re-purposing AlphaFold2 to generate conformational ensembles via a multiple sequence alignment sampling method. From these ensembles, we extract a comprehensive set of features characterizing the conformational ensemble's modality and structural dispersion. A random forest classifier trained on a carefully curated benchmark dataset of known metamorphic and single-fold proteins achieves a mean AUC of 0.869 with cross-validation, demonstrating the effectiveness of our integrated approach. Furthermore, by applying our classifier to 600 randomly sampled proteins from the Protein Data Bank, we identified several potential metamorphic protein candidates -- including the 40S ribosomal protein S30, whose conformational change is crucial for its secondary function in antimicrobial defense. By combining AI-driven protein structure prediction with statistical learning, our work provides a powerful new approach for discovering metamorphic proteins and deepens our understanding of their role in their molecular function."}
{"id": "2512.10407", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10407", "abs": "https://arxiv.org/abs/2512.10407", "authors": ["Christian Soize"], "title": "Supervised Learning of Random Neural Architectures Structured by Latent Random Fields on Compact Boundaryless Multiply-Connected Manifolds", "comment": "46 pages, 12 figures", "summary": "This paper introduces a new probabilistic framework for supervised learning in neural systems. It is designed to model complex, uncertain systems whose random outputs are strongly non-Gaussian given deterministic inputs. The architecture itself is a random object stochastically generated by a latent anisotropic Gaussian random field defined on a compact, boundaryless, multiply-connected manifold. The goal is to establish a novel conceptual and mathematical framework in which neural architectures are realizations of a geometry-aware, field-driven generative process. Both the neural topology and synaptic weights emerge jointly from a latent random field. A reduced-order parameterization governs the spatial intensity of an inhomogeneous Poisson process on the manifold, from which neuron locations are sampled. Input and output neurons are identified via extremal evaluations of the latent field, while connectivity is established through geodesic proximity and local field affinity. Synaptic weights are conditionally sampled from the field realization, inducing stochastic output responses even for deterministic inputs. To ensure scalability, the architecture is sparsified via percentile-based diffusion masking, yielding geometry-aware sparse connectivity without ad hoc structural assumptions. Supervised learning is formulated as inference on the generative hyperparameters of the latent field, using a negative log-likelihood loss estimated through Monte Carlo sampling from single-observation-per-input datasets. The paper initiates a mathematical analysis of the model, establishing foundational properties such as well-posedness, measurability, and a preliminary analysis of the expressive variability of the induced stochastic mappings, which support its internal coherence and lay the groundwork for a broader theory of geometry-driven stochastic learning."}
{"id": "2512.10276", "categories": ["stat.AP", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10276", "abs": "https://arxiv.org/abs/2512.10276", "authors": ["Gbenga A. Olalude", "Taiwo A. Ojurongbe", "Olalekan A. Bello", "Kehinde A. Bashiru", "Kazeem A. Alamu"], "title": "Alpha Power Harris-G Family of Distributions: Properties and Application to Burr XII Distribution", "comment": "43 pages, 8 figures, 13 tables", "summary": "This study introduces a new family of probability distributions, termed the alpha power Harris-generalized (APHG) family. The generator arises by incorporating two shape parameters from the Harris-G framework into the alpha power transformation, resulting in a more flexible class for modelling survival and reliability data. A special member of this family, obtained using the two-parameter Burr XII distribution as the baseline, is developed and examined in detail. Several analytical properties of the proposed alpha power Harris Burr XII (APHBXII) model are derived, which include closed-form expressions for its moments, mean and median deviations, Bonferroni and Lorenz curves, order statistics, and Renyi and Tsallis entropies. Parameter estimation is performed via maximum likelihood, and a Monte Carlo simulation study is carried out to assess the finite-sample performance of the estimators. In addition, three real lifetime datasets are analyzed to evaluate the empirical performance of the APHBXII distribution relative to four competing models. The results show that the five-parameter APHBXII model provides superior fit across all datasets, as supported by model-selection criteria and goodness-of-fit statistics."}
{"id": "2512.10069", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10069", "abs": "https://arxiv.org/abs/2512.10069", "authors": ["Chloe Si", "David A. Stephens", "Erica E. M. Moodie"], "title": "Incorporating Partial Adherence for Estimation of Dynamic Treatment Regimes", "comment": null, "summary": "Dynamic Treatment Regimes (DTRs) provide a systematic framework for optimizing sequential decision-making in chronic disease management, where therapies must adapt to patients' evolving clinical profiles. Inverse probability weighting (IPW) is a cornerstone methodology for estimating regime values from observational data due to its intuitive formulation and established theoretical properties, yet standard IPW estimators face significant limitations, including variance instability and data inefficiency. A fundamental but underexplored source of inefficiency lies in the strict binary adherence criterion that fails to account for partial adherence, thereby discarding substantial data from individuals with even minimal deviations from the target regime. We propose two novel methodologies that relax the strict inclusion rule through flexible compatibility mechanisms. Both methods provide computationally tractable alternatives that can be easily integrated into existing IPW workflows, offering more efficient approaches to DTR estimation. Theoretical analysis demonstrates that both estimators preserve consistency while achieving superior finite-sample efficiency compared to standard IPW, and comprehensive simulation studies confirm improved stability. We illustrate the practical utility of our methods through an application to HIV treatment data from the AIDS Clinical Trials Group Study 175 (ACTG175)."}
{"id": "2512.10250", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.10250", "abs": "https://arxiv.org/abs/2512.10250", "authors": ["Sicheng Liu", "Alexander Fengler", "Michael J. Frank", "Matthew T. Harrison"], "title": "Time-Averaged Drift Approximations are Inconsistent for Inference in Drift Diffusion Models", "comment": null, "summary": "Drift diffusion models (DDMs) have found widespread use in computational neuroscience and other fields. They model evidence accumulation in simple decision tasks as a stochastic process drifting towards a decision barrier. In models where the drift rate is both time-varying within a trial and variable across trials, the high computational cost for accurate likelihood evaluation has led to the common use of a computationally convenient surrogate for parameter inference, the time-averaged drift approximation (TADA). In each trial, the TADA assumes that the time-varying drift rate can be replaced by its temporal average throughout the trial. This approach enables fast parameter inference using analytical likelihood formulas for DDMs with constant drift. In this work, we show that such an estimator is inconsistent: it does not converge to the true drift, posing a risk of biasing scientific conclusions drawn from parameter estimates produced by TADA and similar surrogates. We provide an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a Brownian motion with piecewise constant drift hitting a one-sided upper boundary. Furthermore, we conduct numerical examples with an attentional DDM (aDDM) to show that the use of TADA systematically misestimates the effect of attention in decision making."}
{"id": "2512.10276", "categories": ["stat.AP", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10276", "abs": "https://arxiv.org/abs/2512.10276", "authors": ["Gbenga A. Olalude", "Taiwo A. Ojurongbe", "Olalekan A. Bello", "Kehinde A. Bashiru", "Kazeem A. Alamu"], "title": "Alpha Power Harris-G Family of Distributions: Properties and Application to Burr XII Distribution", "comment": "43 pages, 8 figures, 13 tables", "summary": "This study introduces a new family of probability distributions, termed the alpha power Harris-generalized (APHG) family. The generator arises by incorporating two shape parameters from the Harris-G framework into the alpha power transformation, resulting in a more flexible class for modelling survival and reliability data. A special member of this family, obtained using the two-parameter Burr XII distribution as the baseline, is developed and examined in detail. Several analytical properties of the proposed alpha power Harris Burr XII (APHBXII) model are derived, which include closed-form expressions for its moments, mean and median deviations, Bonferroni and Lorenz curves, order statistics, and Renyi and Tsallis entropies. Parameter estimation is performed via maximum likelihood, and a Monte Carlo simulation study is carried out to assess the finite-sample performance of the estimators. In addition, three real lifetime datasets are analyzed to evaluate the empirical performance of the APHBXII distribution relative to four competing models. The results show that the five-parameter APHBXII model provides superior fit across all datasets, as supported by model-selection criteria and goodness-of-fit statistics."}
{"id": "2512.10445", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10445", "abs": "https://arxiv.org/abs/2512.10445", "authors": ["Francesco Freni", "Anya Fries", "Linus Kühne", "Markus Reichstein", "Jonas Peters"], "title": "Maximum Risk Minimization with Random Forests", "comment": "47 pages, 13 figures", "summary": "We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data."}
{"id": "2512.10401", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10401", "abs": "https://arxiv.org/abs/2512.10401", "authors": ["Jennifer Rosina Andersson", "Zheng Zhao"], "title": "Diffusion differentiable resampling", "comment": null, "summary": "This paper is concerned with differentiable resampling in the context of sequential Monte Carlo (e.g., particle filtering). We propose a new informative resampling method that is instantly pathwise differentiable, based on an ensemble score diffusion model. We prove that our diffusion resampling method provides a consistent estimate to the resampling distribution, and we show by experiments that it outperforms the state-of-the-art differentiable resampling methods when used for stochastic filtering and parameter estimation."}
{"id": "2512.10212", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10212", "abs": "https://arxiv.org/abs/2512.10212", "authors": ["Y. Xu", "S. Tu L. Shao", "T. Lin", "X. M. Tu"], "title": "Semiparametric rank-based regression models as robust alternatives to parametric mean-based counterparts for censored responses under detection-limit", "comment": null, "summary": "Detection limits are common in biomedical and environmental studies, where key covariates or outcomes are censored below an assay-specific threshold. Standard approaches such as complete-case analysis, single-value substitution, and parametric Tobit-type models are either inefficient or sensitive to distributional misspecification.\n  We study semiparametric rank-based regression models as robust alternatives to parametric mean-based counterparts for censored responses under detection limits. Our focus is on accelerated failure time (AFT) type formulations, where rank-based estimating equations yield consistent slope estimates without specifying the error distribution. We develop a unifying simulation framework that generates left- and right-censored data under several data-generating mechanisms, including normal, Weibull, and log-normal error structures, with detection limits or administrative censoring calibrated to target censoring rates between 10\\% and 60\\%.\n  Across scenarios, we compare semiparametric AFT estimators with parametric Weibull AFT, Tobit, and Cox proportional hazards models in terms of bias, empirical variability, and relative efficiency. Numerical results show that parametric models perform well only under correct specification, whereas rank-based semiparametric AFT estimators maintain near-unbiased covariate effects and stable precision even under heavy censoring and distributional misspecification. These findings support semiparametric rank-based regression as a practical default for censored regression with detection limits when the error distribution is uncertain.\n  Keywords: Semiparametric models, Estimating equations, Left censoring, Right censoring, Tobit regression, Efficiency"}
{"id": "2512.10537", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.10537", "abs": "https://arxiv.org/abs/2512.10537", "authors": ["Daojiang He", "Suren Xu", "Jing Zhou"], "title": "A Bayesian Two-Sample Mean Test for High-Dimensional Data", "comment": null, "summary": "We propose a two-sample Bayesian mean test based on the Bayes factor with non-informative priors, specifically designed for scenarios where $p$ grows with $n$ with a linear rate $p/n \\to c_1 \\in (0, \\infty)$. We establish the asymptotic normality of the test statistic and the asymptotic power. Through extensive simulations, we demonstrate that the proposed test performs competitively, particularly when the diagonal elements have heterogeneous variances and for small sample sizes. Furthermore, our test remains robust under distribution misspecification. The proposed method not only effectively detects both sparse and non-sparse differences in mean vectors but also maintains a well-controlled type I error rate, even in small-sample scenarios. We also demonstrate the performance of our proposed test using the \\texttt{SRBCTs} dataset."}
{"id": "2512.10055", "categories": ["stat.ME", "physics.data-an", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.10055", "abs": "https://arxiv.org/abs/2512.10055", "authors": ["Yannick Kuhn", "Masaki Adachi", "Micha Philipp", "David A. Howey", "Birger Horstmann"], "title": "A Primer on Bayesian Parameter Estimation and Model Selection for Battery Simulators", "comment": "22 pages, 19 figures", "summary": "Physics-based battery modelling has emerged to accelerate battery materials discovery and performance assessment. Its success, however, is still hindered by difficulties in aligning models to experimental data. Bayesian approaches are a valuable tool to overcome these challenges, since they enable prior assumptions and observations to be combined in a principled manner that improves numerical conditioning. Here we introduce two new algorithms to the battery community, SOBER and BASQ, that greatly speed up Bayesian inference for parameterisation and model comparison. We showcase how Bayesian model selection allows us to tackle data observability, model identifiability, and data-informed model development together. We propose this approach for the search for battery models of novel materials."}
{"id": "2512.10570", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10570", "abs": "https://arxiv.org/abs/2512.10570", "authors": ["Asaf Ben Arie", "Malka Gorfine"], "title": "Flexible Deep Neural Networks for Partially Linear Survival Data", "comment": null, "summary": "We propose a flexible deep neural network (DNN) framework for modeling survival data within a partially linear regression structure. The approach preserves interpretability through a parametric linear component for covariates of primary interest, while a nonparametric DNN component captures complex time-covariate interactions among nuisance variables. We refer to the method as FLEXI-Haz, a flexible hazard model with a partially linear structure. In contrast to existing DNN approaches for partially linear Cox models, FLEXI-Haz does not rely on the proportional hazards assumption. We establish theoretical guarantees: the neural network component attains minimax-optimal convergence rates based on composite Holder classes, and the linear estimator is root-n consistent, asymptotically normal, and semiparametrically efficient. Extensive simulations and real-data analyses demonstrate that FLEXI-Haz provides accurate estimation of the linear effect, offering a principled and interpretable alternative to modern methods based on proportional hazards. Code for implementing FLEXI-Haz, as well as scripts for reproducing data analyses and simulations, is available at: https://github.com/AsafBanana/FLEXI-Haz"}
{"id": "2512.10467", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10467", "abs": "https://arxiv.org/abs/2512.10467", "authors": ["Bufan Li", "Lujia Bai", "Weichi Wu"], "title": "Learning Time-Varying Correlation Networks with FDR Control via Time-Varying P-values", "comment": null, "summary": "This paper presents a systematic framework for controlling false discovery rate in learning time-varying correlation networks from high-dimensional, non-linear, non-Gaussian and non-stationary time series with an increasing number of potential abrupt change points in means. We propose a bootstrap-assisted approach to derive dependent and time-varying P-values from a robust estimate of time-varying correlation functions, which are not sensitive to change points. Our procedure is based on a new high-dimensional Gaussian approximation result for the uniform approximation of P-values across time and different coordinates. Moreover, we establish theoretically guaranteed Benjamini--Hochberg and Benjamini--Yekutieli procedures for the dependent and time-varying P-values, which can achieve uniform false discovery rate control. The proposed methods are supported by rigorous mathematical proofs and simulation studies. We also illustrate the real-world application of our framework using both brain electroencephalogram and financial time series data."}
{"id": "2512.10250", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.10250", "abs": "https://arxiv.org/abs/2512.10250", "authors": ["Sicheng Liu", "Alexander Fengler", "Michael J. Frank", "Matthew T. Harrison"], "title": "Time-Averaged Drift Approximations are Inconsistent for Inference in Drift Diffusion Models", "comment": null, "summary": "Drift diffusion models (DDMs) have found widespread use in computational neuroscience and other fields. They model evidence accumulation in simple decision tasks as a stochastic process drifting towards a decision barrier. In models where the drift rate is both time-varying within a trial and variable across trials, the high computational cost for accurate likelihood evaluation has led to the common use of a computationally convenient surrogate for parameter inference, the time-averaged drift approximation (TADA). In each trial, the TADA assumes that the time-varying drift rate can be replaced by its temporal average throughout the trial. This approach enables fast parameter inference using analytical likelihood formulas for DDMs with constant drift. In this work, we show that such an estimator is inconsistent: it does not converge to the true drift, posing a risk of biasing scientific conclusions drawn from parameter estimates produced by TADA and similar surrogates. We provide an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a Brownian motion with piecewise constant drift hitting a one-sided upper boundary. Furthermore, we conduct numerical examples with an attentional DDM (aDDM) to show that the use of TADA systematically misestimates the effect of attention in decision making."}
{"id": "2512.10250", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.10250", "abs": "https://arxiv.org/abs/2512.10250", "authors": ["Sicheng Liu", "Alexander Fengler", "Michael J. Frank", "Matthew T. Harrison"], "title": "Time-Averaged Drift Approximations are Inconsistent for Inference in Drift Diffusion Models", "comment": null, "summary": "Drift diffusion models (DDMs) have found widespread use in computational neuroscience and other fields. They model evidence accumulation in simple decision tasks as a stochastic process drifting towards a decision barrier. In models where the drift rate is both time-varying within a trial and variable across trials, the high computational cost for accurate likelihood evaluation has led to the common use of a computationally convenient surrogate for parameter inference, the time-averaged drift approximation (TADA). In each trial, the TADA assumes that the time-varying drift rate can be replaced by its temporal average throughout the trial. This approach enables fast parameter inference using analytical likelihood formulas for DDMs with constant drift. In this work, we show that such an estimator is inconsistent: it does not converge to the true drift, posing a risk of biasing scientific conclusions drawn from parameter estimates produced by TADA and similar surrogates. We provide an elementary proof of this inconsistency in what is perhaps the simplest possible setting: a Brownian motion with piecewise constant drift hitting a one-sided upper boundary. Furthermore, we conduct numerical examples with an attentional DDM (aDDM) to show that the use of TADA systematically misestimates the effect of attention in decision making."}
{"id": "2512.10873", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10873", "abs": "https://arxiv.org/abs/2512.10873", "authors": ["Qitian Lu", "Himanshu Sharma", "Michael D. Shields", "Lukáš Novák"], "title": "Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling", "comment": null, "summary": "Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks."}
{"id": "2512.10828", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10828", "abs": "https://arxiv.org/abs/2512.10828", "authors": ["Alexander J. McNeil", "Johanna G. Neslehova", "Andrew D. Smith"], "title": "Measures and Models of Non-Monotonic Dependence", "comment": null, "summary": "A margin-free measure of bivariate association generalizing Spearman's rho to the case of non-monotonic dependence is defined in terms of two square integrable functions on the unit interval. Properties of generalized Spearman correlation are investigated when the functions are piecewise continuous and strictly monotonic, with particular focus on the special cases where the functions are drawn from orthonormal bases defined by Legendre polynomials and cosine functions. For continuous random variables, generalized Spearman correlation is treated as a copula-based measure and shown to depend on a pair of uniform-distribution-preserving (udp) transformations determined by the underlying functions. Bounds for generalized Spearman correlation are derived and a novel technique referred to as stochastic inversion of udp transformations is used to construct singular copulas that attain the bounds and parametric copulas with densities that interpolate between the bounds and model different degrees of non-monotonic dependence. Sample analogues of generalized Spearman correlation are proposed and their asymptotic and small-sample properties are investigated. Potential applications of the theory are demonstrated including: exploratory analyses of the dependence structures of datasets and their symmetries; elicitation of functions maximizing generalized Spearman correlation via expansions in orthonormal basis functions; and construction of tractable probability densities to model a wide variety of non-monotonic dependencies."}
{"id": "2512.10254", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.10254", "abs": "https://arxiv.org/abs/2512.10254", "authors": ["Juan Sosa", "Erika Martínez", "Danna L. Cruz-Reyes"], "title": "Peace Sells, But Whose Songs Connect? Bayesian Multilayer Network Analysis of the Big 4 of Thrash Metal", "comment": "52 pages, 8 figures, 8 tables", "summary": "We propose a Bayesian framework for multilayer song similarity networks and apply it to the complete studio discographies of the \"Big 4\" of thrash metal (Metallica, Slayer, Megadeth, Anthrax). Starting from raw audio, we construct four feature-specific layers (loudness, brightness, tonality, rhythm), augment them with song exogenous information, and represent each layer as a k-nearest neighbor graph. We then fit a family of hierarchical probit models with global and layer-specific baselines, node- and layer-specific sociability effects, dyadic covariates, and alternative forms of latent structure (bilinear, distance-based, and stochastic block communities), comparing increasingly flexible specifications using posterior predictive checks, discrimination and calibration metrics (AUC, Brier score, log-loss), and information criteria (DIC, WAIC). Across all bands, the richest stochastic block specification attains the best predictive performance and posterior predictive fit, while revealing sparse but structured connectivity, interpretable covariate effects (notably album membership and temporal proximity), and latent communities and hubs that cut across albums and eras. Taken together, these results illustrate how Bayesian multilayer network models can help organize high-dimensional audio and text features into coherent, musically meaningful patterns."}
{"id": "2512.10254", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.10254", "abs": "https://arxiv.org/abs/2512.10254", "authors": ["Juan Sosa", "Erika Martínez", "Danna L. Cruz-Reyes"], "title": "Peace Sells, But Whose Songs Connect? Bayesian Multilayer Network Analysis of the Big 4 of Thrash Metal", "comment": "52 pages, 8 figures, 8 tables", "summary": "We propose a Bayesian framework for multilayer song similarity networks and apply it to the complete studio discographies of the \"Big 4\" of thrash metal (Metallica, Slayer, Megadeth, Anthrax). Starting from raw audio, we construct four feature-specific layers (loudness, brightness, tonality, rhythm), augment them with song exogenous information, and represent each layer as a k-nearest neighbor graph. We then fit a family of hierarchical probit models with global and layer-specific baselines, node- and layer-specific sociability effects, dyadic covariates, and alternative forms of latent structure (bilinear, distance-based, and stochastic block communities), comparing increasingly flexible specifications using posterior predictive checks, discrimination and calibration metrics (AUC, Brier score, log-loss), and information criteria (DIC, WAIC). Across all bands, the richest stochastic block specification attains the best predictive performance and posterior predictive fit, while revealing sparse but structured connectivity, interpretable covariate effects (notably album membership and temporal proximity), and latent communities and hubs that cut across albums and eras. Taken together, these results illustrate how Bayesian multilayer network models can help organize high-dimensional audio and text features into coherent, musically meaningful patterns."}
{"id": "2512.10446", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10446", "abs": "https://arxiv.org/abs/2512.10446", "authors": ["Chiara Boetti", "Matthew A. Nunes", "Marina I. Knight"], "title": "Long memory network time series", "comment": null, "summary": "Many scientific areas, from computer science to the environmental sciences and finance, give rise to multivariate time series which exhibit long memory, or loosely put, a slow decay in their autocorrelation structure. Efficient modelling and estimation in such settings is key for a number of analysis tasks, such as accurate prediction. However, traditional approaches for modelling such data, for example long memory vector autoregressive processes, are challenging even in modest dimensions, as the number of parameters grows quadratically with the number of modelled variables. Additionally, in many practical data settings, the observed series is accompanied by a (possibly inferred) network that provides information about the presence or absence of between-component associations via the graph edge topology. This article proposes two new models for capturing the dynamics of long memory time series where a network is accounted for. Our approach not only facilitates the analysis of graph-structured long memory time series, but also improves computational efficiency over traditional multivariate long memory models by leveraging the inherent low-dimensional parameter space by adapting likelihood-based estimation algorithms to the network setting. Simulation studies show that our proposed estimation is more stable than traditional models, and is able to tackle data scenarios where current models fail due to computational challenges. While widely applicable, here we demonstrate the efficacy of our proposed models on datasets arising in environmental science and finance."}
{"id": "2512.10467", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10467", "abs": "https://arxiv.org/abs/2512.10467", "authors": ["Bufan Li", "Lujia Bai", "Weichi Wu"], "title": "Learning Time-Varying Correlation Networks with FDR Control via Time-Varying P-values", "comment": null, "summary": "This paper presents a systematic framework for controlling false discovery rate in learning time-varying correlation networks from high-dimensional, non-linear, non-Gaussian and non-stationary time series with an increasing number of potential abrupt change points in means. We propose a bootstrap-assisted approach to derive dependent and time-varying P-values from a robust estimate of time-varying correlation functions, which are not sensitive to change points. Our procedure is based on a new high-dimensional Gaussian approximation result for the uniform approximation of P-values across time and different coordinates. Moreover, we establish theoretically guaranteed Benjamini--Hochberg and Benjamini--Yekutieli procedures for the dependent and time-varying P-values, which can achieve uniform false discovery rate control. The proposed methods are supported by rigorous mathematical proofs and simulation studies. We also illustrate the real-world application of our framework using both brain electroencephalogram and financial time series data."}
{"id": "2512.10537", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.10537", "abs": "https://arxiv.org/abs/2512.10537", "authors": ["Daojiang He", "Suren Xu", "Jing Zhou"], "title": "A Bayesian Two-Sample Mean Test for High-Dimensional Data", "comment": null, "summary": "We propose a two-sample Bayesian mean test based on the Bayes factor with non-informative priors, specifically designed for scenarios where $p$ grows with $n$ with a linear rate $p/n \\to c_1 \\in (0, \\infty)$. We establish the asymptotic normality of the test statistic and the asymptotic power. Through extensive simulations, we demonstrate that the proposed test performs competitively, particularly when the diagonal elements have heterogeneous variances and for small sample sizes. Furthermore, our test remains robust under distribution misspecification. The proposed method not only effectively detects both sparse and non-sparse differences in mean vectors but also maintains a well-controlled type I error rate, even in small-sample scenarios. We also demonstrate the performance of our proposed test using the \\texttt{SRBCTs} dataset."}
{"id": "2512.10632", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10632", "abs": "https://arxiv.org/abs/2512.10632", "authors": ["Guo Liu"], "title": "Lasso-Ridge Refitting: A Two-Stage Estimator for High-Dimensional Linear Regression", "comment": "20 pages", "summary": "The least absolute shrinkage and selection operator (Lasso) is a popular method for high-dimensional statistics. However, it is known that the Lasso often has estimation bias and prediction error. To address such disadvantages, many alternatives and refitting strategies have been proposed and studied. This work introduces a novel Lasso--Ridge method. Our analysis indicates that the proposed estimator achieves improved prediction performance in a range of settings, including cases where the Lasso is tuned at its theoretical optimal rate \\(\\sqrt{\\log(p)/n}\\). Moreover, the proposed method retains several key advantages of the Lasso, such as prediction consistency and reliable variable selection under mild conditions. Through extensive simulations, we further demonstrate that our estimator outperforms the Lasso in both prediction and estimation accuracy, highlighting its potential as a powerful tool for high-dimensional linear regression."}
{"id": "2512.10697", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10697", "abs": "https://arxiv.org/abs/2512.10697", "authors": ["Simon Bang Kristensen", "Erik Thorlund Parner"], "title": "Revisiting the apparent discrepancy between the frequentist and Bayesian interpretation of an adaptive design", "comment": null, "summary": "It is generally appreciated that a frequentist analysis of a group sequential trial must in order to avoid inflating type I error account for the fact that one or more interim analyses were performed. It is also to a lesser extent realised that it may be necessary to account for the ensuing estimation bias. A group sequential design is an instance of adaptive clinical trials where a study may change its design dynamically as a reaction to the observed data. There is a widespread perception that one may circumvent the statistical issues associated with the analysis of an adaptive clinical trial by performing the analysis under a Bayesian paradigm. The root of the argument is that the Bayesian posterior is perceived as unaltered by the data-driven adaptations. We examine this claim by analysing a simple trial with a single interim analysis. We approach the interpretation of the trial data under both a frequentist and Bayesian paradigm with a focus on estimation. The conventional result is that the interim analysis impacts the estimation procedure under the frequentist paradigm, but not under the Bayesian paradigm, which may be seen as expressing a \"paradox\" between the two paradigms. We argue that this result however relies heavily on what one would define as the universe of relevant trials defined by first samples of the parameters from a prior distribution and then the data from a sampling model given the parameters. In particular, in this set of trials, whether a connection exists between the parameter of interest and design parameters. We show how an alternative interpretation of the trial yields a Bayesian posterior mean that corrects for the interim analysis with a term that closely resembles the frequentist conditional bias. We conclude that the role of auxiliary trial parameters needs to be carefully considered when constructing a prior in an adaptive design."}
{"id": "2512.10717", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10717", "abs": "https://arxiv.org/abs/2512.10717", "authors": ["Antreas Laos", "Xenia Miscouridou", "Francesca Panero"], "title": "Dynamic sparse graphs with overlapping communities", "comment": null, "summary": "Dynamic community detection in networks addresses the challenge of tracking how groups of interconnected nodes evolve, merge, and dissolve within time-evolving networks. Here, we propose a novel statistical framework for sparse networks with power-law degree distribution and dynamic overlapping community structure. Using a Bayesian Nonparametric framework, we build on the idea to represent the graph as an exchangeable point process on the plane. We base the model construction on vectors of completely random measures and a latent Markov process for the time-evolving node affiliations. This construction provides a flexible and interpretable approach to model dynamic communities, naturally generalizing existing overlapping block models to the sparse and scale-free regimes. We provide the asymptotic properties of the model concerning sparsity and power-law behavior and propose inference through an approximate procedure which we validate empirically. We show how the model can uncover interpretable community trajectories in a real-world network."}
{"id": "2512.10804", "categories": ["stat.ME", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.10804", "abs": "https://arxiv.org/abs/2512.10804", "authors": ["Takashi Arai"], "title": "Identifiable factor analysis for mixed continuous and binary variables based on the Gaussian-Grassmann distribution", "comment": "25 pages, 8 figures", "summary": "We develop a factor analysis for mixed continuous and binary observed variables. To this end, we utilized a recently developed multivariate probability distribution for mixed-type random variables, the Gaussian-Grassmann distribution. In the proposed factor analysis, marginalization over latent variables can be performed analytically, yielding an analytical expression for the distribution of the observed variables. This analytical tractability allows model parameters to be estimated using standard gradient-based optimization techniques. We also address improper solutions associated with maximum likelihood factor analysis. We propose a prescription to avoid improper solutions by imposing a constraint that row vectors of the factor loading matrix have the same norm for all features. Then, we prove that the proposed factor analysis is identifiable under the norm constraint. We demonstrate the validity of this norm constraint prescription and numerically verified the model's identifiability using both real and synthetic datasets. We also compare the proposed model with quantification method and found that the proposed model achieves better reproducibility of correlations than the quantification method."}
{"id": "2512.10828", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10828", "abs": "https://arxiv.org/abs/2512.10828", "authors": ["Alexander J. McNeil", "Johanna G. Neslehova", "Andrew D. Smith"], "title": "Measures and Models of Non-Monotonic Dependence", "comment": null, "summary": "A margin-free measure of bivariate association generalizing Spearman's rho to the case of non-monotonic dependence is defined in terms of two square integrable functions on the unit interval. Properties of generalized Spearman correlation are investigated when the functions are piecewise continuous and strictly monotonic, with particular focus on the special cases where the functions are drawn from orthonormal bases defined by Legendre polynomials and cosine functions. For continuous random variables, generalized Spearman correlation is treated as a copula-based measure and shown to depend on a pair of uniform-distribution-preserving (udp) transformations determined by the underlying functions. Bounds for generalized Spearman correlation are derived and a novel technique referred to as stochastic inversion of udp transformations is used to construct singular copulas that attain the bounds and parametric copulas with densities that interpolate between the bounds and model different degrees of non-monotonic dependence. Sample analogues of generalized Spearman correlation are proposed and their asymptotic and small-sample properties are investigated. Potential applications of the theory are demonstrated including: exploratory analyses of the dependence structures of datasets and their symmetries; elicitation of functions maximizing generalized Spearman correlation via expansions in orthonormal basis functions; and construction of tractable probability densities to model a wide variety of non-monotonic dependencies."}
{"id": "2512.10276", "categories": ["stat.AP", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10276", "abs": "https://arxiv.org/abs/2512.10276", "authors": ["Gbenga A. Olalude", "Taiwo A. Ojurongbe", "Olalekan A. Bello", "Kehinde A. Bashiru", "Kazeem A. Alamu"], "title": "Alpha Power Harris-G Family of Distributions: Properties and Application to Burr XII Distribution", "comment": "43 pages, 8 figures, 13 tables", "summary": "This study introduces a new family of probability distributions, termed the alpha power Harris-generalized (APHG) family. The generator arises by incorporating two shape parameters from the Harris-G framework into the alpha power transformation, resulting in a more flexible class for modelling survival and reliability data. A special member of this family, obtained using the two-parameter Burr XII distribution as the baseline, is developed and examined in detail. Several analytical properties of the proposed alpha power Harris Burr XII (APHBXII) model are derived, which include closed-form expressions for its moments, mean and median deviations, Bonferroni and Lorenz curves, order statistics, and Renyi and Tsallis entropies. Parameter estimation is performed via maximum likelihood, and a Monte Carlo simulation study is carried out to assess the finite-sample performance of the estimators. In addition, three real lifetime datasets are analyzed to evaluate the empirical performance of the APHBXII distribution relative to four competing models. The results show that the five-parameter APHBXII model provides superior fit across all datasets, as supported by model-selection criteria and goodness-of-fit statistics."}
{"id": "2512.10445", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.10445", "abs": "https://arxiv.org/abs/2512.10445", "authors": ["Francesco Freni", "Anya Fries", "Linus Kühne", "Markus Reichstein", "Jonas Peters"], "title": "Maximum Risk Minimization with Random Forests", "comment": "47 pages, 13 figures", "summary": "We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data."}
