<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 11]
- [stat.OT](#stat.OT) [Total: 1]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Optimal transport with a density-dependent cost function](https://arxiv.org/abs/2511.02929)
*Zichu Wang,Esteban G. Tabak*

Main category: stat.CO

TL;DR: 提出了一种新的成对成本函数用于最优传输重心问题，该函数采用两点间最小作用量的形式，并考虑了基础概率分布的拉格朗日量。开发了数值求解框架，使用路径依赖的切比雪夫多项式参数化最小作用量路径，并通过对抗性惩罚确保路径端点与给定源和目标分布一致。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输方法可能无法充分反映数据分布的几何结构，特别是在存在低概率区域时。新方法旨在确保两点只有在存在不穿越低概率区域的路径时才能被认为是接近的，从而更好地捕捉数据的底层结构。

Method: 提出基于最小作用量的成对成本函数，使用路径依赖的切比雪夫多项式参数化最小作用量路径，采用对抗性惩罚机制确保路径端点与源/目标分布一致，开发了相应的数值求解框架。

Result: 通过合成示例验证了该方法在聚类和匹配问题中的有效性，展示了新成本函数能够更好地反映数据的几何结构，特别是在处理存在低概率区域的数据分布时。

Conclusion: 所提出的基于最小作用量的最优传输方法能够更准确地捕捉数据的几何结构，为聚类和匹配问题提供了更有效的解决方案，特别是在需要考虑数据分布中低概率区域影响的情况下。

Abstract: A new pairwise cost function is proposed for the optimal transport barycenter
problem, adopting the form of the minimal action between two points, with a
Lagrangian that takes into account an underlying probability distribution.
Under this notion of distance, two points can only be close if there exist
paths joining them that do not traverse areas of small probability. A framework
is proposed and developed for the numerical solution of the corresponding
data-driven optimal transport problem. The procedure parameterizes the paths of
minimal action through path dependent Chebyshev polynomials and enforces the
agreement between the paths' endpoints and the given source and target
distributions through an adversarial penalization. The methodology and its
application to clustering and matching problems is illustrated through
synthetic examples.

</details>


### [2] [Robust Global Fr'echet Regression via Weight Regularization](https://arxiv.org/abs/2511.03694)
*Hao Li,Shonosuke Sugasawa,Shota Katayama*

Main category: stat.CO

TL;DR: 提出了一种鲁棒的全局Fréchet回归方法，通过引入权重参数和弹性网络正则化来处理度量空间中异常对象的影响。


<details>
  <summary>Details</summary>
Motivation: 传统的Fréchet回归对度量空间中的异常对象敏感，当异常对象到回归曲面的距离远大于其他对象时会影响模型性能。

Method: 在目标函数中引入权重参数，并采用弹性网络正则化来获得稀疏的鲁棒参数向量，控制异常对象的影响。提供了迭代估计算法，并证明了线性收敛性。

Result: 通过矩阵和分布响应的数值研究验证了所提方法的有限样本性能。

Conclusion: 该方法能够自适应地提供鲁棒性，并通过贝叶斯信息准则选择正则化参数，有效处理度量空间回归中的异常值问题。

Abstract: The Fr\'echet regression is a useful method for modeling random objects in a
general metric space given Euclidean covariates. However, the conventional
approach could be sensitive to outlying objects in the sense that the distance
from the regression surface is large compared to the other objects. In this
study, we develop a robust version of the global Fr\'echet regression by
incorporating weight parameters into the objective function. We then introduce
the Elastic net regularization, favoring a sparse vector of robust parameters
to control the influence of outlying objects. We provide a computational
algorithm to iteratively estimate the regression function and weight
parameters, with providing a linear convergence property. We also propose the
Bayesian information criterion to select the tuning parameters for
regularization, which gives adaptive robustness along with observed data. The
finite sample performance of the proposed method is demonstrated through
numerical studies on matrix and distribution responses.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [3] [Precise asymptotic analysis of Sobolev training for random feature models](https://arxiv.org/abs/2511.03050)
*Katharine E Fisher,Matthew TC Li,Youssef Marzouk,Timo Schorlepp*

Main category: stat.ML

TL;DR: 本文研究了Sobolev训练（结合函数和梯度数据的回归）对高维过参数化随机特征模型泛化误差的影响，发现在某些情况下添加梯度数据并不能改善预测性能，过参数化程度应指导训练方法的选择。


<details>
  <summary>Details</summary>
Motivation: 梯度信息在实际应用中广泛可用且有用，但理论上对于Sobolev训练在高维过参数化模型中的泛化误差影响了解甚少。

Method: 使用统计物理中的复制方法和算子值自由概率理论中的线性化方法，推导了训练后随机特征模型泛化误差的闭式描述，并通过将梯度数据投影到有限维子空间来模拟实际实现。

Result: 对于单指数模型描述的目标函数，补充函数数据与额外梯度数据并不能普遍提高预测性能，过参数化程度应指导训练方法的选择。

Conclusion: 研究结果确定了模型通过插值噪声函数和梯度数据达到最优性能的设置，过参数化程度是选择训练方法的关键因素。

Abstract: Gradient information is widely useful and available in applications, and is
therefore natural to include in the training of neural networks. Yet little is
known theoretically about the impact of Sobolev training -- regression with
both function and gradient data -- on the generalization error of highly
overparameterized predictive models in high dimensions. In this paper, we
obtain a precise characterization of this training modality for random feature
(RF) models in the limit where the number of trainable parameters, input
dimensions, and training data tend proportionally to infinity. Our model for
Sobolev training reflects practical implementations by sketching gradient data
onto finite dimensional subspaces. By combining the replica method from
statistical physics with linearizations in operator-valued free probability
theory, we derive a closed-form description for the generalization errors of
the trained RF models. For target functions described by single-index models,
we demonstrate that supplementing function data with additional gradient data
does not universally improve predictive performance. Rather, the degree of
overparameterization should inform the choice of training method. More broadly,
our results identify settings where models perform optimally by interpolating
noisy function and gradient data.

</details>


### [4] [Scalable Single-Cell Gene Expression Generation with Latent Diffusion Models](https://arxiv.org/abs/2511.02986)
*Giovanni Palla,Sudarshan Babu,Payam Dibaeinia,James D. Pearce,Donghui Li,Aly A. Khan,Theofanis Karaletsos,Jakub M. Tomczak*

Main category: stat.ML

TL;DR: 提出了scLDM，一种用于单细胞基因表达数据的可扩展潜在扩散模型，该模型尊重数据的可交换性特性，在观察性和扰动性单细胞数据中都表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞基因表达的计算建模对于理解细胞过程至关重要，但由于基因表达数据的计数性质和基因间复杂的潜在依赖性，生成真实的表达谱仍然是一个重大挑战。现有生成模型通常强加人工基因排序或依赖浅层神经网络架构。

Method: 使用具有固定大小潜在变量的VAE，采用统一的Multi-head Cross-Attention Block架构，在编码器中实现置换不变池化，在解码器中实现置换等变解池化。用基于Diffusion Transformers和线性插值的潜在扩散模型替换高斯先验，支持多条件无分类器引导的高质量生成。

Result: 在各种实验中显示出优越性能，包括观察性和扰动性单细胞数据，以及细胞级分类等下游任务。

Conclusion: scLDM模型能够有效处理单细胞基因表达数据的复杂特性，为单细胞分析提供了强大的生成建模工具。

Abstract: Computational modeling of single-cell gene expression is crucial for
understanding cellular processes, but generating realistic expression profiles
remains a major challenge. This difficulty arises from the count nature of gene
expression data and complex latent dependencies among genes. Existing
generative models often impose artificial gene orderings or rely on shallow
neural network architectures. We introduce a scalable latent diffusion model
for single-cell gene expression data, which we refer to as scLDM, that respects
the fundamental exchangeability property of the data. Our VAE uses fixed-size
latent variables leveraging a unified Multi-head Cross-Attention Block (MCAB)
architecture, which serves dual roles: permutation-invariant pooling in the
encoder and permutation-equivariant unpooling in the decoder. We enhance this
framework by replacing the Gaussian prior with a latent diffusion model using
Diffusion Transformers and linear interpolants, enabling high-quality
generation with multi-conditional classifier-free guidance. We show its
superior performance in a variety of experiments for both observational and
perturbational single-cell data, as well as downstream tasks like cell-level
classification.

</details>


### [5] [Unifying Information-Theoretic and Pair-Counting Clustering Similarity](https://arxiv.org/abs/2511.03000)
*Alexander J. Gates*

Main category: stat.ML

TL;DR: 本文提出了一个统一框架，将聚类相似性度量的两个主要家族（对计数和信息论）联系起来，揭示了它们作为观测与期望共现的加权展开的互补视角。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类相似性度量方法产生分歧甚至矛盾的评价结果，且两个主要家族之间的深层分析联系尚未完全理解。

Method: 开发了一个分析框架，将两个家族表达为观测与期望共现的加权展开，其中对计数作为二次低阶近似，信息论度量作为高阶频率加权扩展。同时将对计数推广到k元组一致性，显示信息论度量可视为系统累积超越成对水平的高阶共分配结构。

Result: 该框架阐明了Rand指数和互信息等度量如何从统一视角产生，并展示了两个家族中其他指数作为自然扩展的出现。

Conclusion: 这一统一视图澄清了两个体系何时以及为何产生分歧，将其敏感性直接与加权和近似阶数联系起来，为跨应用选择、解释和扩展聚类相似性度量提供了原则性基础。

Abstract: Comparing clusterings is central to evaluating unsupervised models, yet the
many existing similarity measures can produce widely divergent, sometimes
contradictory, evaluations. Clustering similarity measures are typically
organized into two principal families, pair-counting and information-theoretic,
reflecting whether they quantify agreement through element pairs or aggregate
information across full cluster contingency tables. Prior work has uncovered
parallels between these families and applied empirical normalization or
chance-correction schemes, but their deeper analytical connection remains only
partially understood. Here, we develop an analytical framework that unifies
these families through two complementary perspectives. First, both families are
expressed as weighted expansions of observed versus expected co-occurrences,
with pair-counting arising as a quadratic, low-order approximation and
information-theoretic measures as higher-order, frequency-weighted extensions.
Second, we generalize pair-counting to $k$-tuple agreement and show that
information-theoretic measures can be viewed as systematically accumulating
higher-order co-assignment structure beyond the pairwise level. We illustrate
the approaches analytically for the Rand index and Mutual Information, and show
how other indices in each family emerge as natural extensions. Together, these
views clarify when and why the two regimes diverge, relating their
sensitivities directly to weighting and approximation order, and provide a
principled basis for selecting, interpreting, and extending clustering
similarity measures across applications.

</details>


### [6] [Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity](https://arxiv.org/abs/2511.03606)
*Diego Martinez-Taboada,Tomas Gonzalez,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 本文研究了向量值自归一化过程的浓度界限，超越了传统的高斯假设，适用于更广泛的轻尾分布，并应用于在线线性回归和核化线性赌博机问题。


<details>
  <summary>Details</summary>
Motivation: 虽然标量值自归一化过程的研究已很成熟，但向量值过程在非高斯框架下的研究相对不足，特别是在序列决策和计量经济学等应用中具有重要意义。

Method: 提出了针对轻尾分布（如Bennett或Bernstein界限）的自归一化过程的浓度界限分析方法，超越了传统的次高斯假设。

Result: 建立了适用于向量值自归一化过程的浓度界限理论框架，能够处理更广泛的分布类型。

Conclusion: 该研究扩展了自归一化过程的理论基础，为在线线性回归和核化线性赌博机等应用提供了更通用的理论支持。

Abstract: The study of self-normalized processes plays a crucial role in a wide range
of applications, from sequential decision-making to econometrics. While the
behavior of self-normalized concentration has been widely investigated for
scalar-valued processes, vector-valued processes remain comparatively
underexplored, especially outside of the sub-Gaussian framework. In this
contribution, we provide concentration bounds for self-normalized processes
with light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds).
We illustrate the relevance of our results in the context of online linear
regression, with applications in (kernelized) linear bandits.

</details>


### [7] [Provable Accelerated Bayesian Optimization with Knowledge Transfer](https://arxiv.org/abs/2511.03125)
*Haitao Lin,Boxin Zhao,Mladen Kolar,Chong Liu*

Main category: stat.ML

TL;DR: DeltaBO算法通过构建源任务与目标任务之间的差异函数，在贝叶斯优化中实现知识迁移，显著降低了遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯优化知识迁移方法要么缺乏理论保证，要么在非迁移设置下达到相同的遗憾界，无法充分利用历史知识加速优化过程。

Method: 提出DeltaBO算法，在源任务和目标任务可能属于不同再生核希尔伯特空间的情况下，构建差异函数的不确定性量化方法。

Result: 理论证明DeltaBO的遗憾界为$\tilde{\mathcal{O}}(\sqrt{T (T/N + \gamma_\delta)})$，其中$N \\gg T$，且$\gamma_\delta$通常远小于$\gamma_f$。实证研究表明DeltaBO在真实超参数调优和合成函数上优于基线方法。

Conclusion: DeltaBO通过有效利用源任务知识显著加速贝叶斯优化，在理论和实证上都表现出优越性能。

Abstract: We study how Bayesian optimization (BO) can be accelerated on a target task
with historical knowledge transferred from related source tasks. Existing works
on BO with knowledge transfer either do not have theoretical guarantees or
achieve the same regret as BO in the non-transfer setting,
$\tilde{\mathcal{O}}(\sqrt{T \gamma_f})$, where $T$ is the number of
evaluations of the target function and $\gamma_f$ denotes its information gain.
In this paper, we propose the DeltaBO algorithm, in which a novel
uncertainty-quantification approach is built on the difference function
$\delta$ between the source and target functions, which are allowed to belong
to different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions,
we prove that the regret of DeltaBO is of order $\tilde{\mathcal{O}}(\sqrt{T
(T/N + \gamma_\delta)})$, where $N$ denotes the number of evaluations from
source tasks and typically $N \gg T$. In many applications, source and target
tasks are similar, which implies that $\gamma_\delta$ can be much smaller than
$\gamma_f$. Empirical studies on both real-world hyperparameter tuning tasks
and synthetic functions show that DeltaBO outperforms other baseline methods
and support our theoretical claims.

</details>


### [8] [Provable Separations between Memorization and Generalization in Diffusion Models](https://arxiv.org/abs/2511.03202)
*Zeqi Ye,Qijie Zhu,Molei Tao,Minshuo Chen*

Main category: stat.ML

TL;DR: 本文从统计估计和网络逼近两个角度分析了扩散模型的记忆化问题，提出了双重分离理论，并开发了基于剪枝的方法来减少记忆化同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然取得了显著成功，但存在记忆化问题——复制训练数据而非生成新颖输出，这不仅限制了其创造性潜力，还引发了隐私和安全担忧。目前对记忆化的理论理解有限。

Method: 通过统计估计和网络逼近两个互补视角建立双重分离理论：从估计角度证明真实评分函数不最小化经验去噪损失；从逼近角度证明实现经验评分函数需要网络规模随样本量增长。基于这些洞察开发了基于剪枝的方法。

Result: 建立了扩散模型记忆化的理论框架，揭示了记忆化的根本原因，并提出了有效的剪枝方法在扩散变换器中减少记忆化同时保持生成质量。

Conclusion: 该研究为理解扩散模型记忆化提供了理论基础，提出的双重分离理论和剪枝方法为解决记忆化问题提供了有效途径，有助于提升扩散模型的创造性和安全性。

Abstract: Diffusion models have achieved remarkable success across diverse domains, but
they remain vulnerable to memorization -- reproducing training data rather than
generating novel outputs. This not only limits their creative potential but
also raises concerns about privacy and safety. While empirical studies have
explored mitigation strategies, theoretical understanding of memorization
remains limited. We address this gap through developing a dual-separation
result via two complementary perspectives: statistical estimation and network
approximation. From the estimation side, we show that the ground-truth score
function does not minimize the empirical denoising loss, creating a separation
that drives memorization. From the approximation side, we prove that
implementing the empirical score function requires network size to scale with
sample size, spelling a separation compared to the more compact network
representation of the ground-truth score function. Guided by these insights, we
develop a pruning-based method that reduces memorization while maintaining
generation quality in diffusion transformers.

</details>


### [9] [RKUM: An R Package for Robust Kernel Unsupervised Methods](https://arxiv.org/abs/2511.03216)
*Md Ashad Alam*

Main category: stat.ML

TL;DR: RKUM是一个R包，用于实现基于核的鲁棒无监督方法，包括鲁棒核协方差算子和交叉协方差算子的估计，以及鲁棒核典型相关分析和影响函数分析。


<details>
  <summary>Details</summary>
Motivation: 传统核方法使用二次损失函数，对污染或噪声数据敏感。需要开发鲁棒核方法来处理高维数据中的异常值和噪声问题。

Method: 使用广义损失函数替代传统二次损失来估计核协方差算子和交叉协方差算子，实现鲁棒核典型相关分析，并计算影响函数来检测异常观测。

Result: 实验表明，标准核CCA的影响函数能有效识别异常值，而RKUM实现的鲁棒核方法对数据污染的敏感性显著降低。

Conclusion: RKUM为高维数据应用提供了一个高效且可扩展的鲁棒核分析平台。

Abstract: RKUM is an R package developed for implementing robust kernel-based
unsupervised methods. It provides functions for estimating the robust kernel
covariance operator (CO) and the robust kernel cross-covariance operator (CCO)
using generalized loss functions instead of the conventional quadratic loss.
These operators form the foundation of robust kernel learning and enable
reliable analysis under contaminated or noisy data conditions. The package
includes implementations of robust kernel canonical correlation analysis
(Kernel CCA), as well as the influence function (IF) for both standard and
multiple kernel CCA frameworks. The influence function quantifies sensitivity
and helps detect influential or outlying observations across two-view and
multi-view datasets. Experiments using synthesized two-view and multi-view data
demonstrate that the IF of the standard kernel CCA effectively identifies
outliers, while the robust kernel methods implemented in RKUM exhibit reduced
sensitivity to contamination. Overall, RKUM provides an efficient and
extensible platform for robust kernel-based analysis in high-dimensional data
applications.

</details>


### [10] [Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning](https://arxiv.org/abs/2511.03693)
*Md Ahasanul Arafath,Abhijit Kumar Ghosh,Md Rony Ahmed,Sabrin Afroz,Minhazul Hosen,Md Hasan Moon,Md Tanzim Reza,Md Ashad Alam*

Main category: stat.ML

TL;DR: 提出了一个可扩展的隐私保护联邦学习框架，用于结直肠癌组织病理学分级，通过多尺度特征学习和分布式训练，在保护患者隐私的同时提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌分级存在观察者间变异性和多机构数据共享的隐私限制问题，集中式训练模型违反数据治理法规且忽略了多尺度分析的重要性。

Method: 采用双流ResNetRS50骨干网络同时捕获细粒度核细节和更广泛的组织级上下文，集成到使用FedProx稳定的联邦学习系统中，以减轻异构数据分布下的客户端漂移。

Result: 在CRC-HGD数据集上达到83.5%的整体准确率，优于集中式模型(81.6%)，对最具侵袭性的III级肿瘤召回率达到87.5%，在40倍放大下准确率提升至88.0%。

Conclusion: 该联邦多尺度方法不仅保护患者隐私，还增强了模型性能和泛化能力，为可部署的隐私感知临床AI建立了基础。

Abstract: Colorectal cancer (CRC) grading is a critical prognostic factor but remains
hampered by inter-observer variability and the privacy constraints of
multi-institutional data sharing. While deep learning offers a path to
automation, centralized training models conflict with data governance
regulations and neglect the diagnostic importance of multi-scale analysis. In
this work, we propose a scalable, privacy-preserving federated learning (FL)
framework for CRC histopathological grading that integrates multi-scale feature
learning within a distributed training paradigm. Our approach employs a
dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear
detail and broader tissue-level context. This architecture is integrated into a
robust FL system stabilized using FedProx to mitigate client drift across
heterogeneous data distributions from multiple hospitals. Extensive evaluation
on the CRC-HGD dataset demonstrates that our framework achieves an overall
accuracy of 83.5%, outperforming a comparable centralized model (81.6%).
Crucially, the system excels in identifying the most aggressive Grade III
tumors with a high recall of 87.5%, a key clinical priority to prevent
dangerous false negatives. Performance further improves with higher
magnification, reaching 88.0% accuracy at 40x. These results validate that our
federated multi-scale approach not only preserves patient privacy but also
enhances model performance and generalization. The proposed modular pipeline,
with built-in preprocessing, checkpointing, and error handling, establishes a
foundational step toward deployable, privacy-aware clinical AI for digital
pathology.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [11] [Modeling Headway in Heterogeneous and Mixed Traffic Flow: A Statistical Distribution Based on a General Exponential Function](https://arxiv.org/abs/2511.03154)
*Natchaphon Leungbootnak,Zihao Li,Zihang Wei,Dominique Lord,Yunlong Zhang*

Main category: stat.AP

TL;DR: 提出了一种新的车头时距分布模型，通过修改指数函数使用实数底数而非欧拉数，提高了在异构和混合交通流中的拟合精度。


<details>
  <summary>Details</summary>
Motivation: 现有车头时距分布模型在异构交通（不同车辆类型）和混合交通（人工驾驶与自动驾驶车辆共存）中拟合效果不佳，无法准确反映多样化的驾驶行为和特征。

Method: 修改指数函数，使用实数底数替代欧拉数e，增加建模灵活性。虽然提出的函数不是概率函数，但通过归一化处理计算概率并推导出闭式方程。使用五个公开数据集（highD、exiD、NGSIM、Waymo、Lyft）进行综合实验验证。

Result: 提出的分布不仅能捕捉车头时距分布的基本特征，还提供了具有物理意义的参数来描述观测车头时距的分布形状。在高速公路异构交通流中表现最优，在城市道路中断交通流（包括异构和混合交通）中也取得了良好结果。

Conclusion: 新提出的车头时距分布在异构和混合交通环境中优于现有六种分布，特别是在高速公路不间断交通流条件下表现突出，为交通流建模提供了更准确的工具。

Abstract: The ability of existing headway distributions to accurately reflect the
diverse behaviors and characteristics in heterogeneous traffic (different types
of vehicles) and mixed traffic (human-driven vehicles with autonomous vehicles)
is limited, leading to unsatisfactory goodness of fit. To address these issues,
we modified the exponential function to obtain a novel headway distribution.
Rather than employing Euler's number (e) as the base of the exponential
function, we utilized a real number base to provide greater flexibility in
modeling the observed headway. However, the proposed is not a probability
function. We normalize it to calculate the probability and derive the
closed-form equation. In this study, we utilized a comprehensive experiment
with five open datasets: highD, exiD, NGSIM, Waymo, and Lyft to evaluate the
performance of the proposed distribution and compared its performance with six
existing distributions under mixed and heterogeneous traffic flow. The results
revealed that the proposed distribution not only captures the fundamental
characteristics of headway distribution but also provides physically meaningful
parameters that describe the distribution shape of observed headways. Under
heterogeneous flow on highways (i.e., uninterrupted traffic flow), the proposed
distribution outperforms other candidate distributions. Under urban road
conditions (i.e., interrupted traffic flow), including heterogeneous and mixed
traffic, the proposed distribution still achieves decent results.

</details>


### [12] [Post-2024 U.S. Presidential Election Analysis of Election and Poll Data: Real-life Validation of Prediction via Small Area Estimation and Uncertainty Quantification](https://arxiv.org/abs/2511.03555)
*Zheshi Zheng,Yuanyuan Li,Peter X. K. Song,Jiming Jiang*

Main category: stat.AP

TL;DR: 使用小区域估计(SAE)方法预测2024年美国总统选举，在44个有民调数据的州中完美预测选举人团结果，并提出错误预测概率(PoIP)和保形推理方法来量化预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够准确预测总统选举结果并可靠量化预测不确定性的模型，同时分析民调偏差对摇摆州预测的影响。

Method: 基于小区域估计(SAE)方法构建预测模型，使用选举前一周的民调数据，采用保形推理方法估计错误预测概率(PoIP)来量化不确定性。

Result: 在44个有民调数据的州中完美预测了选举人团结果，发现摇摆州特别容易受到民调偏差的影响。

Conclusion: SAE方法在选举预测中具有高准确性，保形推理能有效量化预测不确定性，摇摆州的预测对民调偏差特别敏感。

Abstract: We carry out a post-election analysis of the 2024 U.S. Presidential Election
(USPE) using a prediction model derived from the Small Area Estimation (SAE)
methodology. With pollster data obtained one week prior to the election day,
retrospectively, our SAE-based prediction model can perfectly predict the
Electoral College election results in all 44 states where polling data were
available. In addition to such desirable prediction accuracy, we introduce the
probability of incorrect prediction (PoIP) to rigorously analyze prediction
uncertainty. Since the standard bootstrap method appears inadequate for
estimating PoIP, we propose a conformal inference method that yields reliable
uncertainty quantification. We further investigate potential pollster biases by
the means of sensitivity analyses and conclude that swing states are
particularly vulnerable to polling bias in the prediction of the 2024 USPE.

</details>


### [13] [Adjusting for Heavy Censoring and Double-Dipping to Compare Risk Stratification Abilities of Existing Models for Time to Diagnosis of Huntington Disease](https://arxiv.org/abs/2511.03596)
*Kyle F. Grosser,Abigail G. Foes,Stellen Li,Vraj Parikh,Tanya P. Garcia,Sarah C. Lotspeich*

Main category: stat.AP

TL;DR: 本文系统比较了四种亨廷顿病诊断时间预测模型，通过外部验证发现MRS模型表现最佳，但CAP和PIN模型在简化实施方面具有优势，同时指出先前临床试验样本量估计不足的问题。


<details>
  <summary>Details</summary>
Motivation: 亨廷顿病是一种遗传性神经退行性疾病，准确预测诊断时间对临床试验设计和治疗规划至关重要。现有模型在方法、假设和准确性上存在差异，可能导致冲突预测，且缺乏系统比较和考虑高右删失率的性能评估。

Method: 讨论了四种常见HD诊断时间模型的理论基础，使用ENROLL-HD研究数据进行外部验证，并采用调整删失率的性能指标评估模型的风险分层能力。

Result: MRS模型（包含最多协变量）表现最佳，但较简单的CAP和PIN模型表现相近且实施更简便。研究还显示先前临床试验样本量估计会导致试验效能不足。

Conclusion: 研究为HD临床试验设计中的模型选择提供指导，建议根据具体需求在模型精度和实施简便性之间权衡，并强调需要重新评估临床试验样本量估计方法。

Abstract: Huntington disease (HD) is a genetically inherited neurodegenerative disease
with progressively worsening symptoms. Accurately modeling time to HD diagnosis
is essential for clinical trial design and treatment planning. Langbehn's
model, the CAG-Age Product (CAP) model, the Prognostic Index Normed (PIN)
model, and the Multivariate Risk Score (MRS) model have all been proposed for
this task. However, differing in methodology, assumptions, and accuracy, these
models may yield conflicting predictions. Few studies have systematically
compared these models' performance, and those that have could be misleading due
to (i) testing the models on the same data used to train them and (ii) failing
to account for high rates of right censoring (80%+) in performance metrics. We
discuss the theoretical foundations of the four most common models of time to
HD diagnosis, offering intuitive comparisons about their practical feasibility.
Further, we externally validate their risk stratification abilities using data
from the ENROLL-HD study and performance metrics that adjust for censoring. Our
findings guide the selection of a model for HD clinical trial design. The MRS
model, which incorporates the most covariates, performed the best. However, the
simpler CAP and PIN models were not far behind and may be logistically simpler
to adopt. We also show how these models can be used to estimate sample sizes
for an HD clinical trial, emphasizing that previous estimates would lead to
underpowered trials.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [14] [Adaptive Orthogonalization for Stable Estimation of the Effects of Time-Varying Treatments](https://arxiv.org/abs/2511.02971)
*Yige Li,María de los Angeles Resa,José R. Zubizarreta*

Main category: stat.ME

TL;DR: 提出一种新的因果效应估计方法，通过平衡正交于历史信息的协变量分量来解决逆概率权重高度可变的问题，在协变量重叠有限的情况下提供稳健的估计。


<details>
  <summary>Details</summary>
Motivation: 在时间变化治疗的因果效应推断中，逆概率权重的高度可变性是一个主要挑战，特别是在协变量重叠有限的情况下。现有方法在平衡观测协变量时可能效率低下。

Method: 基于Imai和Ratkovic(2015)框架，提出一种新估计器，直接针对反事实或潜在协变量的特征。方法平衡与历史正交的协变量分量，而非平衡观测协变量本身。

Result: 证明所得估计器对于平均潜在结果是一致的且渐近正态的，即使在标准逆概率加权失败的情况下。模拟显示该方法达到与g-计算相当的效率，同时对模型误设具有更好的稳健性。

Conclusion: 该方法在智利私立与公立学校教育对大学入学分数影响的纵向研究中表现出稳定性和可解释性，为解决高度可变逆概率权重问题提供了有效方案。

Abstract: Inferring the causal effects of time-varying treatments is often hindered by
highly variable inverse propensity weights, particularly in settings with
limited covariate overlap. Building on the key framework of Imai and Ratkovic
(2015), we establish sufficient balancing conditions for identification in
longitudinal studies of treatment effects and propose a novel estimator that
directly targets features of counterfactual or potential covariates. Instead of
balancing observed covariates, our method balances the components of covariates
that are orthogonal to their history, thereby isolating the new information at
each time point. This strategy directly targets the joint distribution of
potential covariates and prioritizes features that are most relevant to the
outcome. We prove that the resulting estimator for the mean potential outcome
is consistent and asymptotically normal, even in settings where standard
inverse propensity weighting fails. Extensive simulations show that our
estimator attains efficiency comparable to that of g-computation while
providing superior robustness to model misspecification. We apply our method to
a longitudinal study of private versus public schooling in Chile, demonstrating
its stability and interpretability in estimating their effects on university
admission scores.

</details>


### [15] [Detecting Conflicts in Evidence Synthesis Models Using Score Discrepancies](https://arxiv.org/abs/2511.02977)
*Fuming Yang,David J. Nott,Anne M. Presanis*

Main category: stat.ME

TL;DR: 提出了一个基于分数差异的证据合成模型冲突检测通用框架，将先验-数据冲突诊断扩展到层次模型潜在空间中的更一般冲突检查。


<details>
  <summary>Details</summary>
Motivation: 证据合成模型中共享参数可能引发数据间冲突和模型结构冲突，检测和量化这些冲突在模型批评中仍具挑战性。

Method: 基于分数差异的冲突检测框架，在层次模型的潜在空间中进行一般性冲突检查，通过模拟研究和实际应用验证。

Result: 模拟研究表明该方法能有效检测数据间不一致性，在流感严重程度模型中的应用显示其与传统偏差诊断互补。

Conclusion: 该框架为贝叶斯证据合成中的一致性评估提供了灵活且广泛适用的工具。

Abstract: Evidence synthesis models combine multiple data sources to estimate latent
quantities of interest, enabling reliable inference on parameters that are
difficult to measure directly. However, shared parameters across data sources
can induce conflicts both among the data and with the assumed model structure.
Detecting and quantifying such conflicts remains a challenge in model
criticism. Here we propose a general framework for conflict detection in
evidence synthesis models based on score discrepancies, extending prior-data
conflict diagnostics to more general conflict checks in the latent space of
hierarchical models. Simulation studies in an exchangeable model demonstrate
that the proposed approach effectively detects between-data inconsistencies.
Application to an influenza severity model illustrates its use, complementary
to traditional deviance-based diagnostics, in complex real-world hierarchical
settings. The proposed framework thus provides a flexible and broadly
applicable tool for consistency assessment in Bayesian evidence synthesis.

</details>


### [16] [Constructing Large Orthogonal Minimally Aliased Response Surface Designs by Concatenating Two Definitive Screening Designs](https://arxiv.org/abs/2511.02984)
*Alan R. Vazquez,Peter Goos,Eric D. Schoen*

Main category: stat.ME

TL;DR: 提出了一种构建大型OMARS设计的方法，通过拼接两个确定性筛选设计并使用折叠技术和列置换来最小化二阶效应的混淆。


<details>
  <summary>Details</summary>
Motivation: 现有的OMARS设计枚举算法在因子数和试验次数较多时计算量过大，需要更高效的方法来构建大型设计。

Method: 将两个确定性筛选设计拼接，使用折叠技术和列置换算法来最小化二阶效应的混淆。

Result: 新方法构建的OMARS设计改进了父设计的统计特性，并与文献中的替代设计进行了比较。

Conclusion: 提出的构造方法能够高效地构建大型OMARS设计，解决了枚举算法计算量大的问题。

Abstract: Orthogonal minimally aliased response surface (OMARS) designs permit the
study of quantitative factors at three levels using an economical number of
runs. In these designs, the linear effects of the factors are neither aliased
with each other nor with the quadratic effects and the two-factor interactions.
Complete catalogs of OMARS designs with up to five factors have been obtained
using an enumeration algorithm. However, the algorithm is computationally
demanding for designs with many factors and runs. To overcome this issue, we
propose a construction method for large OMARS designs that concatenates two
definitive screening designs and improves the statistical features of its
parent designs. The concatenation employs an algorithm that minimizes the
aliasing among the second-order effects using foldover techniques and column
permutations for one of the parent designs. We study the properties of the new
OMARS designs and compare them with alternative designs in the literature.

</details>


### [17] [New sampling approaches for Shrinkage Inverse-Wishart distribution](https://arxiv.org/abs/2511.03044)
*Yiye Jiang*

Main category: stat.ME

TL;DR: 提出了基于采样重要性重采样(SIR)的新算法来从收缩逆Wishart(SIW)分布中采样，相比现有的嵌套Gibbs采样器更快且具有理论收敛保证。针对SIR方法中重要性权重差异大的问题，引入了剪裁步骤来鲁棒化算法。


<details>
  <summary>Details</summary>
Motivation: SIW分布作为逆Wishart分布的推广，为协方差矩阵提供了灵活的先验且保持与高斯似然的共轭性，但现有采样方法速度慢且缺乏收敛性理论分析。

Method: 基于SIR方法设计新采样算法，通过剪裁步骤处理重要性权重差异大的问题，确保算法鲁棒性。

Result: 新算法显著提高了采样效率，具有理论收敛保证，鲁棒化版本在保持计算效率的同时解决了权重差异问题。

Conclusion: 提出的SIR-based算法为SIW分布提供了高效且理论可靠的采样方法，通过剪裁步骤增强了算法的鲁棒性。

Abstract: In this paper, we propose new sampling approaches for the Shrinkage
Inverse-Wishart (SIW) distribution, a generalized family of the Inverse-Wishart
distribution originally proposed by Berger et al. (2020, Annals of Statistics).
It offers a flexible prior for covariance matrices and remains conjugate to the
Gaussian likelihood, similar to the classical Inverse-Wishart. Despite these
advantages, sampling from SIW remains challenging. The existing algorithm
relies on a nested Gibbs sampler, which is slow and lacks rigorous theoretical
analysis of its convergence. We propose a new algorithm based on the Sampling
Importance Resampling (SIR) method, which is significantly faster and comes
with theoretical guarantees on convergence rates. A known issue with SIR
methods is the large discrepancy in importance weights, which occurs when the
proposal distribution has thinner tails than the target. In the case of SIW,
certain parameter settings can lead to such discrepancies, reducing the
robustness of the output samples. To sample from such SIW distributions, we
robustify the proposed algorithm by including a clipping step to the SIR
framework which transforms large importance weights. We provide theoretical
results on the convergence behavior in terms of the clipping size, and discuss
strategies for choosing this parameter via simulation studies. The robustified
version retains the computational efficiency of the original algorithm.

</details>


### [18] [Beyond Maximum Likelihood: Variational Inequality Estimation for Generalized Linear Models](https://arxiv.org/abs/2511.03087)
*Linglingzhi Zhu,Jonghyeok Lee,Yao Xie*

Main category: stat.ME

TL;DR: 本文提出了一种基于变分不等式(VI)的广义线性模型(GLM)参数估计方法，作为最大似然估计(MLE)的替代方案。VI方法在非标准连接函数、非凸优化等挑战性场景下具有更好的计算效率和数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统MLE方法在标准GLM中表现良好，但在非标准连接函数、非凸优化等复杂场景下存在计算效率低、数值不稳定等问题。需要寻找更稳健的替代估计方法。

Method: 采用变分不等式(VI)方法求解GLM似然方程，扩展了VI理论到更广泛的连接函数类别，包括满足强Minty条件的非单调情况。

Result: VI方法在保持MLE统计效率的同时，显著扩展了其适用范围，在计算速度、稳定性和避免局部最优方面表现更优。理论分析建立了非渐近估计误差界和渐近正态性。

Conclusion: VI框架为GLM参数估计提供了MLE的有效替代方案，特别适用于具有挑战性的建模场景，在计算效率和数值稳定性方面具有明显优势。

Abstract: Generalized linear models (GLMs) are fundamental tools for statistical
modeling, with maximum likelihood estimation (MLE) serving as the classical
method for parameter inference. While MLE performs well in canonical GLMs, it
can become computationally inefficient near the true parameter value. In more
general settings with non-canonical or fully general link functions, the
resulting optimization landscape is often non-convex, non-smooth, and
numerically unstable. To address these challenges, we investigate an
alternative estimator based on solving the variational inequality (VI)
formulation of the GLM likelihood equations, originally proposed by Juditsky
and Nemirovski as an alternative for solving nonlinear least-squares problems.
Unlike their focus on algorithmic convergence in monotone settings, we analyze
the VI approach from a statistical perspective, comparing it systematically
with the MLE. We also extend the theory of VI estimators to a broader class of
link functions, including non-monotone cases satisfying a strong Minty
condition, and show that it admits weaker smoothness requirements than MLE,
enabling faster, more stable, and less locally trapped optimization.
Theoretically, we establish both non-asymptotic estimation error bounds and
asymptotic normality for the VI estimator, and further provide convergence
guarantees for fixed-point and stochastic approximation algorithms. Numerical
experiments show that the VI framework preserves the statistical efficiency of
MLE while substantially extending its applicability to more challenging GLM
settings.

</details>


### [19] [On Ignorability of Preferential Sampling in Geostatistics](https://arxiv.org/abs/2511.03158)
*Changqing Lu,Ganggang Xu,Junho Yang,Yongtao Guan*

Main category: stat.ME

TL;DR: 研究发现：在某些条件下，即使忽略优先抽样机制，现有的非似然方法仍能产生无偏且一致的估计量，无需参数化建模抽样机制。


<details>
  <summary>Details</summary>
Motivation: 传统基于似然的优先抽样校正方法虽然有效但计算成本高且易受模型误设影响，需要探索更简单有效的替代方法。

Method: 研究在Diggle等人(2010)框架下忽略优先抽样的条件，开发无需参数化抽样机制的回归和协方差参数估计方法。

Result: 模拟研究显示该方法具有估计误差减小、置信区间覆盖改善和计算成本显著降低等优势。

Conclusion: 提出的方法在实际应用中具有实用价值，并在热带森林数据集上得到验证。

Abstract: Preferential sampling has attracted considerable attention in geostatistics
since the pioneering work of Diggle et al. (2010). A variety of
likelihood-based approaches have been developed to correct estimation bias by
explicitly modelling the sampling mechanism. While effective in many
applications, these methods are often computationally expensive and can be
susceptible to model misspecification. In this paper, we present a surprising
finding: some existing non-likelihood-based methods that ignore preferential
sampling can still produce unbiased and consistent estimators under the widely
used framework of Diggle et al. (2010) and its extensions. We investigate the
conditions under which preferential sampling can be ignored and develop
relevant estimators for both regression and covariance parameters without
specifying the sampling mechanism parametrically. Simulation studies
demonstrate clear advantages of our approach, including reduced estimation
error, improved confidence interval coverage, and substantially lower
computational cost. To show the practical utility, we further apply it to a
tropical forest data set.

</details>


### [20] [Comment on: "Model uncertainty and missing data: An Objective Bayesian Perspective"](https://arxiv.org/abs/2511.03395)
*Stefan Franssen*

Main category: stat.ME

TL;DR: 对《模型不确定性与缺失数据：客观贝叶斯视角》的讨论，从频率主义角度评论所提出的方法


<details>
  <summary>Details</summary>
Motivation: 从频率主义统计学的角度来讨论和评价该论文中提出的客观贝叶斯方法，提供不同的统计视角

Method: 采用频率主义统计学的理论框架和分析方法，对论文中的客观贝叶斯方法进行批判性讨论和比较

Result: 从频率主义视角指出了客观贝叶斯方法在处理模型不确定性和缺失数据问题时的潜在局限性和优势

Conclusion: 频率主义视角为客观贝叶斯方法提供了有价值的补充和批判性见解，两种方法可以相互借鉴

Abstract: We give a contributed discussion on "Model uncertainty and missing data: An
Objective Bayesian Perspective", where we discuss frequentist perspectives on
the proposed methodology.

</details>


### [21] [Bayesian Causal Effect Estimation for Categorical Data using Staged Tree Models](https://arxiv.org/abs/2511.03399)
*Andrea Cremaschi,Manuele Leonelli,Gherardo Varando*

Main category: stat.ME

TL;DR: 提出了一种基于阶段树模型的完全贝叶斯因果推断方法，用于处理多元分类数据，能够表示非对称和上下文特定的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够处理多元分类数据中非对称和上下文特定依赖关系的因果推断方法，同时考虑结构和参数的不确定性。

Method: 引入了阶段树模型的灵活先验分布族，包括产品划分模型、基于距离的先验以及连续协变量的扩展。使用定制的马尔可夫链蒙特卡洛算法进行后验推断。

Result: 通过两个案例研究（电子胎儿监测与剖腹产、蒽环类药物治疗与乳腺癌心功能障碍）验证了方法的有效性。

Conclusion: 该方法能够从阶段树的后验样本中推导出平均处理效应和不确定性度量，为多元分类数据的因果推断提供了有效的贝叶斯框架。

Abstract: We propose a fully Bayesian approach for causal inference with multivariate
categorical data based on staged tree models, a class of probabilistic
graphical models capable of representing asymmetric and context-specific
dependencies. To account for uncertainty in both structure and parameters, we
introduce a flexible family of prior distributions over staged trees. These
include product partition models to encourage parsimony, a novel distance-based
prior to promote interpretable dependence patterns, and an extension that
incorporates continuous covariates into the learning process. Posterior
inference is achieved via a tailored Markov Chain Monte Carlo algorithm with
split-and-merge moves, yielding posterior samples of staged trees from which
average treatment effects and uncertainty measures are derived. Posterior
summaries and uncertainty measures are obtained via techniques from the
Bayesian nonparametrics literature. Two case studies on electronic fetal
monitoring and cesarean delivery and on anthracycline therapy and cardiac
dysfunction in breast cancer illustrate the methods.

</details>


### [22] [Multi-layer dissolution exponential-family models for weighted signed networks](https://arxiv.org/abs/2511.03420)
*Alberto Caimo,Isabella Gollini*

Main category: stat.ME

TL;DR: 提出了一种多层解构指数随机图模型框架，用于联合分析带符号和权重的网络关系，采用贝叶斯分层方法进行参数估计，并在美国参议院法案赞助数据上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的统计网络分析方法缺乏能够同时严格处理关系符号和强度的模型，这限制了分析社交系统中复杂关系结构的能力。

Method: 采用多层解构指数随机图建模框架，结合完全概率的贝叶斯分层方法，通过自适应近似交换算法进行参数估计，实现跨层信息的部分共享。

Result: 该方法能够揭示传统方法无法捕捉的复杂符号和权重交互模式以及结构平衡效应，在美国参议院法案赞助数据中得到了验证。

Conclusion: 提出的框架为分析带符号和权重的网络提供了灵活且具有解释力的工具，能够更全面地理解复杂社交系统的结构特征。

Abstract: Understanding the structure of weighted signed networks is essential for
analysing social systems in which relationships vary both in sign and strength.
Despite significant advances in statistical network analysis, there is still a
lack of statistical models that can jointly and rigorously account for both the
sign and strength of relationships in networks. We introduce a multi-layer
dissolution exponential random graph modelling framework that jointly captures
the signed and weighted processes, conditional on the observed interaction
structure. The framework enables rigorous assessment of structural balance
effects while fully accounting for edge weights. To enhance inference, we adopt
a fully-probabilistic Bayesian hierarchical approach that partially pools
information across layers, with parameters estimated via an adaptive
approximate exchange algorithm. We demonstrate the flexibility and explanatory
power of the proposed methodology by applying it to bill sponsorship data from
the 108th US Senate, revealing complex patterns of signed and weighted
interactions and structural balance effects that traditional approaches are
unable to capture.

</details>


### [23] [The Bradley-Terry Stochastic Block Model](https://arxiv.org/abs/2511.03467)
*Lapo Santi,Nial Friel*

Main category: stat.ME

TL;DR: 将Bradley-Terry模型嵌入随机块模型，提出BT-SBM方法对项目进行聚类排名，并通过贝叶斯吉布斯采样器进行推断，应用于男子网球数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统Bradley-Terry模型只能产生单一排名，无法识别项目间的聚类结构，需要开发能够同时进行聚类和排名的模型。

Method: 将Bradley-Terry模型与随机块模型结合，使用贝叶斯框架和Thurstonian数据增强技术开发快速吉布斯采样器，联合学习块数、强度和项目分配。

Result: 在男子网球数据分析中，发现前100名球员通常可分为3-4个层级，且最强层级规模在2000年代中期至2018年较小，近年来有所增加，表明竞争加剧。

Conclusion: BT-SBM方法能有效识别项目聚类结构并提供有意义的排名，揭示了男子网球竞争格局的变化趋势。

Abstract: The Bradley-Terry model is widely used for the analysis of pairwise
comparison data and, in essence, produces a ranking of the items under
comparison. We embed the Bradley-Terry model within a stochastic block model,
allowing items to cluster. The resulting Bradley-Terry SBM (BT-SBM) ranks
clusters so that items within a cluster share the same tied rank. We develop a
fully Bayesian specification in which all quantities-the number of blocks,
their strengths, and item assignments-are jointly learned via a fast Gibbs
sampler derived through a Thurstonian data augmentation. Despite its
efficiency, the sampler yields coherent and interpretable posterior summaries
for all model components. Our motivating application analyzes men's tennis
results from ATP tournaments over the seasons 2000-2022. We find that the top
100 players can be broadly partitioned into three or four tiers in most
seasons. Moreover, the size of the strongest tier was small from the mid-2000s
to 2018 and has increased since, providing evidence that men's tennis has
become more competitive in recent years.

</details>


### [24] [Bayesian Topological Analysis of Functional Brain Networks](https://arxiv.org/abs/2511.03605)
*Xukun Zhu,Michael W Lutz,Tananun Songdechakraiwut*

Main category: stat.ME

TL;DR: 提出了一个贝叶斯推理框架，用于脑网络拓扑比较，能够检测传统统计方法难以发现的细微网络拓扑变化。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法难以检测脑网络拓扑的细微变化，需要更敏感的方法来发现早期或微妙的脑网络改变。

Method: 采用贝叶斯推理框架，概率建模组内和组间差异，使用马尔可夫链蒙特卡洛采样估计后验分布和贝叶斯因子。

Result: 模拟实验证实了与置换测试的统计一致性，在阿尔茨海默病fMRI数据中检测到了传统置换测试未能发现的拓扑网络差异。

Conclusion: 该框架对临床神经影像中早期或细微的脑网络改变具有增强的敏感性。

Abstract: Subtle alterations in brain network topology often evade detection by
traditional statistical methods. To address this limitation, we introduce a
Bayesian inference framework for topological comparison of brain networks that
probabilistically models within- and between-group dissimilarities. The
framework employs Markov chain Monte Carlo sampling to estimate posterior
distributions of test statistics and Bayes factors, enabling graded evidence
assessment beyond binary significance testing. Simulations confirmed
statistical consistency to permutation testing. Applied to fMRI data from the
Duke-UNC Alzheimer's Disease Research Center, the framework detected
topology-based network differences that conventional permutation tests failed
to reveal, highlighting its enhanced sensitivity to early or subtle brain
network alterations in clinical neuroimaging.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [25] [From Hume to Jaynes: Induction as the Logic of Plausible Reasoning](https://arxiv.org/abs/2511.02881)
*Tommaso Costa*

Main category: stat.OT

TL;DR: 本文通过将归纳问题重新定义为逻辑一致性问题而非真理问题，论证了归纳问题在贝叶斯框架下得到解决。概率被解释为演绎逻辑在不完整信息下的扩展，贝叶斯定理成为理性信念更新的约束条件。


<details>
  <summary>Details</summary>
Motivation: 解决自休谟以来存在的归纳问题，即重复观察与普遍推断之间的逻辑鸿沟。传统方法在拉普拉斯的概率乐观主义和波普尔的证伪怀疑主义之间摇摆，但都假设归纳必须提供确定性或其否定。

Method: 采用E.T. Jaynes的方法，将概率解释为演绎逻辑在不完整信息下的扩展。贝叶斯定理被视为一致性条件而非经验陈述，归纳作为演绎推理在不确定前提下的特例出现。

Result: 通过分析案例（拉普拉斯日出问题、杰弗里斯混合先验、置信度重构）表明，只有合理推理的逻辑才能无矛盾地统一这些观点。证伪表现为贝叶斯更新的极限形式，贝叶斯因子量化了证据强度的连续谱。

Conclusion: 归纳不是从过去到未来的跳跃，而是保持证据、信念和信息之间一致性的学科。在逻辑一致性的框架下，归纳问题得以消解。

Abstract: The problem of induction has persisted since Hume exposed the logical gap
between repeated observation and universal inference. Traditional attempts to
resolve it have oscillated between two extremes: the probabilistic optimism of
Laplace and Jeffreys, who sought to quantify belief through probability, and
the critical skepticism of Popper, who replaced confirmation with
falsification. Both approaches, however, assume that induction must deliver
certainty or its negation. In this paper, I argue that the problem of induction
dissolves when recast in terms of logical coherence (understood as internal
consistency of credences under updating) rather than truth. Following E. T.
Jaynes, probability is interpreted not as frequency or decision rule but as the
extension of deductive logic to incomplete information. Under this
interpretation, Bayes's theorem is not an empirical statement but a consistency
condition that constrains rational belief updating. Induction thus emerges as
the special case of deductive reasoning applied to uncertain premises.
Falsification appears as the limiting form of Bayesian updating when new data
drive posterior plausibility toward zero, while the Bayes Factor quantifies the
continuous spectrum of evidential strength. Through analytical examples,
including Laplace's sunrise problem, Jeffreys's mixed prior, and
confidence-based reformulations, I show that only the logic of plausible
reasoning unifies these perspectives without contradiction. Induction, properly
understood, is not the leap from past to future but the discipline of
maintaining coherence between evidence, belief, and information.

</details>
