{"id": "2512.03057", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.03057", "abs": "https://arxiv.org/abs/2512.03057", "authors": ["Hao Zeng"], "title": "A note on the impossibility of conditional PAC-efficient reasoning in large language models", "comment": null, "summary": "We prove an impossibility result for conditional Probably Approximately Correct (PAC)-efficient reasoning in large language models. While recent work has established marginal PAC efficiency guarantees for composite models that switch between expensive expert models and cheaper fast models, we show that conditional (pointwise) guarantees are impossible in the distribution-free setting. Specifically, for non-atomic input spaces, any algorithm achieving conditional PAC efficiency must be trivial in the sense that it defers to the expert model with probability at least $1-α$ for almost every input."}
{"id": "2512.03243", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.03243", "abs": "https://arxiv.org/abs/2512.03243", "authors": ["Ioannis Gasteratos", "Antoine Jacquier", "Maud Lemercier", "Terry Lyons", "Cristopher Salvi"], "title": "Novelty detection on path space", "comment": null, "summary": "We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type-$\\mathrm{II}$ error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type-$\\mathrm{I}$ error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data."}
{"id": "2512.03266", "categories": ["stat.ME", "math.ST", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.03266", "abs": "https://arxiv.org/abs/2512.03266", "authors": ["Merlise A Clyde"], "title": "Invited Discussion of \"Model Uncertainty and Missing Data: An Objective Bayesian Perspective\" by Gonzalo García-Donato , María Eugenia Castellanos , Stefano Cabras Alicia Quirós , and Anabel Forte", "comment": null, "summary": "The article by Garc{í}a-Donato and co-authors addresses the dual challenges of accounting for model uncertainty and missing data within the Gaussian regression frameworks from an objective Bayesian perspective. Thru the use of an imputation $g$-prior that replaces $X_γ^TX_γ$ for model $γ$ in the covariance of $β_γ$ with $Σ_{X_γ}$, the authors develop a coherent approach to addressing the missing data problem and model uncertainty simultaneously with random $X_γ$ in the missing at random (MAR) or missing completely at random (MCAR) settings, while still being computationally tractable. I discuss the connection of the imputation $g$-prior to the $g$-prior with imputed $X$, and to model selection for graphical models that provide an alternative justification for the $g$-prior for random $X$s."}
{"id": "2512.04059", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.04059", "abs": "https://arxiv.org/abs/2512.04059", "authors": ["Alden Green", "Jonathan Taylor"], "title": "Inference for location and height of peaks of a standardized field after selection", "comment": null, "summary": "Peak inference concerns the use of local maxima (\"peaks\") of a noisy random field to detect and localize regions where underlying signal is present. We propose a peak inference method that first subjects observed peaks to a significance test of the null hypothesis that no signal is present, and then uses the peaks that are declared significant to construct post-selectively valid confidence regions for the location and height of nearby true peaks. We analyze the performance of this method in a smooth signal plus constant variance noise model under a high-curvature asymptotic assumption, and prove that it asymptotically controls both the number of false discoveries, and the number of confidence regions that do not contain a true peak, relative to the number of points at which inference is conducted. An important intermediate theoretical result uses the Kac-Rice formula to derive a novel approximation to the intensity function of a point process that counts local maxima, which is second-order accurate under the alternative, nearby high-curvature true peaks."}
{"id": "2512.03225", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03225", "abs": "https://arxiv.org/abs/2512.03225", "authors": ["Christophe Andrieu", "Nicolas Chopin", "Ettore Fincato", "Mathieu Gerber"], "title": "Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both", "comment": null, "summary": "We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning."}
{"id": "2512.03116", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03116", "abs": "https://arxiv.org/abs/2512.03116", "authors": ["Joseph de Vilmarest", "Olivier Wintenberger"], "title": "Assessing Extrapolation of Peaks Over Thresholds with Martingale Testing", "comment": null, "summary": "We present the winning strategy for the EVA2025 Data Challenge, which aimed to estimate the probability of extreme precipitation events. These events occurred at most once in the dataset making the challenge fundamentally one of extrapolating extreme values. Given the scarcity of extreme events, we argue that a simple, robust modeling approach is essential. We adopt univariate models instead of multivariate ones and model Peaks Over Thresholds using Extreme Value Theory. Specifically, we fit an exponential distribution to model exceedances of the target variable above a high quantile (after seasonal adjustment). The novelty of our approach lies in using martingale testing to evaluate the extrapolation power of the procedure and to agnostically select the level of the high quantile. While this method has several limitations, we believe that framing extrapolation as a game opens the door to other agnostic approaches in Extreme Value Analysis."}
{"id": "2512.03057", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.03057", "abs": "https://arxiv.org/abs/2512.03057", "authors": ["Hao Zeng"], "title": "A note on the impossibility of conditional PAC-efficient reasoning in large language models", "comment": null, "summary": "We prove an impossibility result for conditional Probably Approximately Correct (PAC)-efficient reasoning in large language models. While recent work has established marginal PAC efficiency guarantees for composite models that switch between expensive expert models and cheaper fast models, we show that conditional (pointwise) guarantees are impossible in the distribution-free setting. Specifically, for non-atomic input spaces, any algorithm achieving conditional PAC efficiency must be trivial in the sense that it defers to the expert model with probability at least $1-α$ for almost every input."}
{"id": "2512.03513", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.03513", "abs": "https://arxiv.org/abs/2512.03513", "authors": ["Jaechoul Lee", "Mintaek Lee", "Thea Sukianto"], "title": "Seasonal trend assessment of US extreme precipitation via changepoint segmentation", "comment": null, "summary": "Most climate trend studies analyze long-term trends as a proxy for climate dynamics. However, when examining seasonal data, it is unrealistic to assume that long-term trends remain consistent across all seasons. Instead, each season likely experiences distinct trends. Additionally, seasonal climate time series, such as seasonal maximum precipitation, often exhibit nonstationarities, including periodicities and location shifts. Failure to rigorously account for these features in modeling may lead to inaccurate trend estimates. This study quantifies seasonal trends in the contiguous United States' seasonal maximum precipitation series while addressing these nonstationarities. To ensure accurate trend estimation, we identify changepoints where the seasonal maximum precipitation shifts due to factors like measurement device changes, observer differences, or location moves. We employ a penalized likelihood method to estimate multiple changepoints, incorporating a generalized extreme value distribution with periodic features. A genetic algorithm based search algorithm efficiently explores the vast space of potential changepoints in both number and timing. Additionally, we compute seasonal return levels for extreme precipitation. Our methods are illustrated using two selected stations, and the results for the US are summarized through maps. We find that seasonal trends vary more when changepoints are considered than in studies that ignore them. Our findings also reveal distinct regional and seasonal patterns, with increasing trends more prevalent during fall in the South and along the East Coast when changepoints are accounted for."}
{"id": "2512.03321", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.03321", "abs": "https://arxiv.org/abs/2512.03321", "authors": ["Kei Hirose"], "title": "Numerical optimization for the compatibility constant of the lasso", "comment": null, "summary": "Compatibility condition and compatibility constant have been commonly used to evaluate the prediction error of the lasso when the number of variables exceeds the number of observations. However, the computation of the compatibility constant is generally difficult because it is a complicated nonlinear optimization problem. In this study, we present a numerical approach to compute the compatibility constant when the zero/nonzero pattern of true regression coefficients is given. We show that the optimization problem reduces to a quadratic program (QP) once the signs of the nonzero coefficients are specified. In this case, the compatibility constant can be obtained by solving QPs for all possible sign combinations. We also formulate a mixed-integer quadratic programming (MIQP) approach that can be applied when the number of true nonzero coefficients is moderately large. We investigate the finite-sample behavior of the compatibility constant for simulated data under a wide variety of parameter settings and compare the mean squared error with its theoretical error bound based on the compatibility constant. The behavior of the compatibility constant in finite samples is also investigated through a real data analysis."}
{"id": "2512.03235", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03235", "abs": "https://arxiv.org/abs/2512.03235", "authors": ["Sijie Zheng"], "title": "Estimation of Semiparametric Factor Models with Missing Data", "comment": null, "summary": "We study semiparametric factor models in high-dimensional panels where the factor loadings consist of a nonparametric component explained by observed covariates and an idiosyncratic component capturing unobserved heterogeneity. A key challenge in empirical applications is the presence of missing observations, which can distort both factor recovery and loading estimation. To address this issue, we develop a projected principal component analysis (PPCA) procedure that accommodates general missing-at-random mechanisms through inverse-probability weighting. We establish consistency and derive the asymptotic distributions of the estimated factors and loading functions, allowing the sieve dimension to diverge and permitting the time dimension to be either fixed or growing. Unlike classical PCA, PPCA achieves consistent factor estimation even when T is fixed, and the limiting distributions under missing data exhibit mixture normality with enlarged asymptotic variances. Theoretical results are supported by simulations and an empirical application. Our findings demonstrate that PPCA provides an effective and robust framework for estimating semiparametric factor models in the presence of missing data."}
{"id": "2512.03208", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03208", "abs": "https://arxiv.org/abs/2512.03208", "authors": ["Pangpang Liu", "Junwei Lu", "Will Wei Sun"], "title": "Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback", "comment": null, "summary": "We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment."}
{"id": "2512.03760", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03760", "abs": "https://arxiv.org/abs/2512.03760", "authors": ["Emanuele Giorgi", "Claudio Fronterre", "Peter J. Diggle"], "title": "A decay-adjusted spatio-temporal model to account for the impact of mass drug administration on neglected tropical disease prevalence", "comment": "Under review", "summary": "Prevalence surveys are routinely used to monitor the effectiveness of mass drug administration (MDA) programmes for controlling neglected tropical diseases (NTDs). We propose a decay-adjusted spatio-temporal (DAST) model that explicitly accounts for the time-varying impact of MDA on NTD prevalence, providing a flexible and interpretable framework for estimating intervention effects from sparse survey data. Using case studies on soil-transmitted helminths and lymphatic filariasis, we show that DAST offers a practical alternative to standard geostatistical models when the objective includes quantifying MDA impact and supporting short-term programmatic forecasting. We also discuss extensions and identifiability challenges, advocating for data-driven parsimony over complexity in settings where the available data are too sparse to support the estimation of highly parameterised models."}
{"id": "2512.03322", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03322", "abs": "https://arxiv.org/abs/2512.03322", "authors": ["Geoffrey J. McLachlan", "Jinran Wu"], "title": "SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models", "comment": "15 pages, 1 figure", "summary": "Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \\textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples."}
{"id": "2512.03254", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03254", "abs": "https://arxiv.org/abs/2512.03254", "authors": ["Philippe A. Boileau", "Hani Zaki", "Gabriele Lileikyte", "Niklas Nielsen", "Patrick R. Lawler", "Mireille E. Schnitzer"], "title": "Assumption-Lean Differential Variance Inference for Heterogeneous Treatment Effect Detection", "comment": null, "summary": "The conditional average treatment effect (CATE) is frequently estimated to refute the homogeneous treatment effect assumption. Under this assumption, all units making up the population under study experience identical benefit from a given treatment. Uncovering heterogeneous treatment effects through inference about the CATE, however, requires that covariates truly modifying the treatment effect be reliably collected at baseline. CATE-based techniques will necessarily fail to detect violations when effect modifiers are omitted from the data due to, for example, resource constraints. Severe measurement error has a similar impact. To address these limitations, we prove that the homogeneous treatment effect assumption can be gauged through inference about contrasts of the potential outcomes' variances. We derive causal machine learning estimators of these contrasts and study their asymptotic properties. We establish that these estimators are doubly robust and asymptotically linear under mild conditions, permitting formal hypothesis testing about the homogeneous treatment effect assumption even when effect modifiers are missing or mismeasured. Numerical experiments demonstrate that these estimators' asymptotic guarantees are approximately achieved in experimental and observational data alike. These inference procedures are then used to detect heterogeneous treatment effects in the re-analysis of randomized controlled trials investigating targeted temperature management in cardiac arrest patients."}
{"id": "2512.03234", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03234", "abs": "https://arxiv.org/abs/2512.03234", "authors": ["Jean Pachebat", "Giovanni Conforti", "Alain Durmus", "Yazid Janati"], "title": "Iterative Tilting for Diffusion Fine-Tuning", "comment": "14 pages", "summary": "We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt $\\exp(λr)$ into $N$ sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form."}
{"id": "2512.03738", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.03738", "abs": "https://arxiv.org/abs/2512.03738", "authors": ["Jaeyoung Shin", "Chi Hyun Lee", "Sangwook Kang"], "title": "Weighted Conformal Prediction for Survival Analysis under Covariate Shift", "comment": "16 pages, 0 figure", "summary": "Reliable uncertainty quantification is essential in survival prediction, particularly in clinical settings where erroneous decisions carry high risk. Conformal prediction has attracted substantial attention as it offers a model-agnostic framework with finite-sample coverage guarantees. Extending it to right-censored outcomes poses nontrivial challenges. Several adaptations of conformal approaches for survival outcomes have been developed, but they either rely on restrictive censoring settings or substantial computation. A recent conformal approach for right-censored data constructs censoring-adjusted p-values and enables prediction intervals in general survival settings. However, the empirical coverage depends sensitively on heuristic tuning choices and its validity is limited to scenarios without covariate shift. In this paper, we establish theoretical justification for its prediction-set construction, providing a principled basis for defining prediction-set bounds, and extend the approach to covariate-shift settings. Simulation studies and a real data application demonstrate that the proposed method achieves robust coverage and coherent interval structure across varying censoring levels and covariate-shift settings."}
{"id": "2512.03255", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03255", "abs": "https://arxiv.org/abs/2512.03255", "authors": ["Federica Spoto", "Alessia Caponera", "Pierpaolo Brutti"], "title": "Change Point Detection for Functional Autoregressive Processes on the Sphere", "comment": null, "summary": "We introduce a novel framework for change point detection in spherical functional autoregressive (SPHAR) processes, enabling the identification of structural breaks in spatio-temporal random fields on the sphere. Our LASSO-regularized estimator, based on penalized dynamic programming in the harmonic domain, operates without knowledge of the number or locations of change points and offers non-asymptotic theoretical guarantees. This approach provides a new tool for analyzing nonstationary phenomena on the sphere, relevant to climate science, cosmology, and beyond."}
{"id": "2512.03243", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.03243", "abs": "https://arxiv.org/abs/2512.03243", "authors": ["Ioannis Gasteratos", "Antoine Jacquier", "Maud Lemercier", "Terry Lyons", "Cristopher Salvi"], "title": "Novelty detection on path space", "comment": null, "summary": "We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type-$\\mathrm{II}$ error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type-$\\mathrm{I}$ error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data."}
{"id": "2512.03761", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.03761", "abs": "https://arxiv.org/abs/2512.03761", "authors": ["Pablo Martinez-Camblor"], "title": "Using functional information for binary classifications", "comment": null, "summary": "The adequate use of information measured in a continuous manner along a period of time represents a methodological challenge. In the last decades, most of traditional statistical procedures have been extended for accommodating these functional data. The binary classification problem, which aims to correctly identify units as positive or negative based on marker values, is not aside of this scenario. The crucial point for making binary classifications based on a marker is to establish an order in the marker values, which is not immediate when these values are presented as functions. Here, we argue that if the marker is related to the characteristic under study, a trajectory from a positive participant should be more similar to trajectories from the positive population than to those drawn from the negative. With this criterion, a classification procedure based on the distance between the involved functions is proposed. Besides, we propose a fully non-parametric estimator for this so-called probability-based criterion, PBC. We explore its asymptotic properties, and its finite-sample behavior from an extensive Monte Carlo study. The observed results suggest that the proposed methodology works adequately, and frequently better than its competitors, for a wide variety of situations when the sample size in both the training and the testing cohorts is adequate. The practical use of the proposal is illustrated from real-world dataset. As online supplementary material, the manuscript includes a document with further simulations and additional comments. An R function which wraps up the implemented routines is also provided."}
{"id": "2512.03266", "categories": ["stat.ME", "math.ST", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.03266", "abs": "https://arxiv.org/abs/2512.03266", "authors": ["Merlise A Clyde"], "title": "Invited Discussion of \"Model Uncertainty and Missing Data: An Objective Bayesian Perspective\" by Gonzalo García-Donato , María Eugenia Castellanos , Stefano Cabras Alicia Quirós , and Anabel Forte", "comment": null, "summary": "The article by Garc{í}a-Donato and co-authors addresses the dual challenges of accounting for model uncertainty and missing data within the Gaussian regression frameworks from an objective Bayesian perspective. Thru the use of an imputation $g$-prior that replaces $X_γ^TX_γ$ for model $γ$ in the covariance of $β_γ$ with $Σ_{X_γ}$, the authors develop a coherent approach to addressing the missing data problem and model uncertainty simultaneously with random $X_γ$ in the missing at random (MAR) or missing completely at random (MCAR) settings, while still being computationally tractable. I discuss the connection of the imputation $g$-prior to the $g$-prior with imputed $X$, and to model selection for graphical models that provide an alternative justification for the $g$-prior for random $X$s."}
{"id": "2512.03727", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03727", "abs": "https://arxiv.org/abs/2512.03727", "authors": ["Lorenzo Marinucci", "Leonardo Di Nino", "Gabriele D'Acunto", "Mario Edoardo Pandolfo", "Paolo Di Lorenzo", "Sergio Barbarossa"], "title": "Colored Markov Random Fields for Probabilistic Topological Modeling", "comment": "Proceeding of 2025 Asilomar Conference on Signals, Systems, and Computers", "summary": "Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior."}
{"id": "2512.03777", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03777", "abs": "https://arxiv.org/abs/2512.03777", "authors": ["Federico P. Cortese", "Luca Rossini"], "title": "A comparison between initialization strategies for the infinite hidden Markov model", "comment": null, "summary": "Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature."}
{"id": "2512.03543", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03543", "abs": "https://arxiv.org/abs/2512.03543", "authors": ["Pavel Krupskii", "Boris Béranger"], "title": "Parsimonious Factor Models for Asymmetric Dependence in Multivariate Extremes", "comment": "51 pages, 8 figures, and 4 tables", "summary": "Modelling multivariate extreme events is essential when extrapolating beyond the range of observed data. Parametric models that are suitable for real-world extremes must be flexible -- particularly in their ability to capture asymmetric dependence structures -- while also remaining parsimonious for interpretability and computationally scalable in high dimensions. Although many models have been proposed, it is rare for any single construction to satisfy all of these requirements. For instance, the popular Hüsler-Reiss model is limited to symmetric dependence structures. In this manuscript, we introduce a class of additive factor models and derive their extreme-value limits. This leads to a broad and tractable family of models characterised by a manageable number of parameters. These models naturally accommodate asymmetric tail dependence and allow for non-stationary behaviour. We present the limiting models from both the componentwise-maxima and Peaks-over-Thresholds perspectives, via the multivariate extreme value and multivariate generalized Pareto distributions, respectively. Simulation studies illustrate identifiability properties based on existing inference methodologies. Finally, applications to summer temperature maxima in Melbourne, Australia, and to weekly negative returns from four major UK banks demonstrate improved fit compared with the Hüsler-Reiss model."}
{"id": "2512.03851", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03851", "abs": "https://arxiv.org/abs/2512.03851", "authors": ["Paul Strasser", "Andreas Pfeffer", "Jakob Weber", "Markus Gurtner", "Andreas Körner"], "title": "Comparison of neural network training strategies for the simulation of dynamical systems", "comment": "submitted to ECC", "summary": "Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems."}
{"id": "2512.03738", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.03738", "abs": "https://arxiv.org/abs/2512.03738", "authors": ["Jaeyoung Shin", "Chi Hyun Lee", "Sangwook Kang"], "title": "Weighted Conformal Prediction for Survival Analysis under Covariate Shift", "comment": "16 pages, 0 figure", "summary": "Reliable uncertainty quantification is essential in survival prediction, particularly in clinical settings where erroneous decisions carry high risk. Conformal prediction has attracted substantial attention as it offers a model-agnostic framework with finite-sample coverage guarantees. Extending it to right-censored outcomes poses nontrivial challenges. Several adaptations of conformal approaches for survival outcomes have been developed, but they either rely on restrictive censoring settings or substantial computation. A recent conformal approach for right-censored data constructs censoring-adjusted p-values and enables prediction intervals in general survival settings. However, the empirical coverage depends sensitively on heuristic tuning choices and its validity is limited to scenarios without covariate shift. In this paper, we establish theoretical justification for its prediction-set construction, providing a principled basis for defining prediction-set bounds, and extend the approach to covariate-shift settings. Simulation studies and a real data application demonstrate that the proposed method achieves robust coverage and coherent interval structure across varying censoring levels and covariate-shift settings."}
{"id": "2512.03116", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03116", "abs": "https://arxiv.org/abs/2512.03116", "authors": ["Joseph de Vilmarest", "Olivier Wintenberger"], "title": "Assessing Extrapolation of Peaks Over Thresholds with Martingale Testing", "comment": null, "summary": "We present the winning strategy for the EVA2025 Data Challenge, which aimed to estimate the probability of extreme precipitation events. These events occurred at most once in the dataset making the challenge fundamentally one of extrapolating extreme values. Given the scarcity of extreme events, we argue that a simple, robust modeling approach is essential. We adopt univariate models instead of multivariate ones and model Peaks Over Thresholds using Extreme Value Theory. Specifically, we fit an exponential distribution to model exceedances of the target variable above a high quantile (after seasonal adjustment). The novelty of our approach lies in using martingale testing to evaluate the extrapolation power of the procedure and to agnostically select the level of the high quantile. While this method has several limitations, we believe that framing extrapolation as a game opens the door to other agnostic approaches in Extreme Value Analysis."}
{"id": "2512.03761", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.03761", "abs": "https://arxiv.org/abs/2512.03761", "authors": ["Pablo Martinez-Camblor"], "title": "Using functional information for binary classifications", "comment": null, "summary": "The adequate use of information measured in a continuous manner along a period of time represents a methodological challenge. In the last decades, most of traditional statistical procedures have been extended for accommodating these functional data. The binary classification problem, which aims to correctly identify units as positive or negative based on marker values, is not aside of this scenario. The crucial point for making binary classifications based on a marker is to establish an order in the marker values, which is not immediate when these values are presented as functions. Here, we argue that if the marker is related to the characteristic under study, a trajectory from a positive participant should be more similar to trajectories from the positive population than to those drawn from the negative. With this criterion, a classification procedure based on the distance between the involved functions is proposed. Besides, we propose a fully non-parametric estimator for this so-called probability-based criterion, PBC. We explore its asymptotic properties, and its finite-sample behavior from an extensive Monte Carlo study. The observed results suggest that the proposed methodology works adequately, and frequently better than its competitors, for a wide variety of situations when the sample size in both the training and the testing cohorts is adequate. The practical use of the proposal is illustrated from real-world dataset. As online supplementary material, the manuscript includes a document with further simulations and additional comments. An R function which wraps up the implemented routines is also provided."}
{"id": "2512.03225", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03225", "abs": "https://arxiv.org/abs/2512.03225", "authors": ["Christophe Andrieu", "Nicolas Chopin", "Ettore Fincato", "Mathieu Gerber"], "title": "Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both", "comment": null, "summary": "We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning."}
{"id": "2512.03777", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03777", "abs": "https://arxiv.org/abs/2512.03777", "authors": ["Federico P. Cortese", "Luca Rossini"], "title": "A comparison between initialization strategies for the infinite hidden Markov model", "comment": null, "summary": "Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature."}
{"id": "2512.03322", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03322", "abs": "https://arxiv.org/abs/2512.03322", "authors": ["Geoffrey J. McLachlan", "Jinran Wu"], "title": "SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models", "comment": "15 pages, 1 figure", "summary": "Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \\textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples."}
{"id": "2512.03859", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03859", "abs": "https://arxiv.org/abs/2512.03859", "authors": ["Kehan Wang", "Wenxuan Song", "Wangli Xu", "Linglong Kong"], "title": "SUP: An Inferable Private Multiple Testing Framework with Super Uniformity", "comment": null, "summary": "Multiple testing is widely applied across scientific fields, particularly in genomic and health data analysis, where protecting sensitive personal information is imperative. However, developing private multiple testing algorithms for super uniform $p$-values remains an open question, as privacy mechanisms introduce intricate dependence among the peeled $p$-values and disrupt their super uniformity, complicating post-selection inference. To address this, we introduce a general Super Uniform Private (SUP) multiple testing framework with three key components. First, we develop a novel \\( p \\)-value transformation that is compatible with diverse privacy regimes while retaining the super uniformity. Next, a reversed peeling algorithm is designed to reduce privacy budgets while facilitating inference. Then, we provide diverse rejection thresholds that are privacy-parameter-free and tailored for different Type-I errors, including the family-wise error rate (FWER) and the false discovery rate (FDR). Building upon these, we advance adaptive techniques to determine the peeling number and boost thresholds. Theoretically, we propose a technique overcoming the post-selection obstacle to Type-I error control, quantify the privacy-induced power loss of SUP relative to its non-private counterpart, and demonstrate that SUP surpasses existing private methods in terms of power. The results of extensive simulations and a real data application validate our theories."}
{"id": "2512.03777", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03777", "abs": "https://arxiv.org/abs/2512.03777", "authors": ["Federico P. Cortese", "Luca Rossini"], "title": "A comparison between initialization strategies for the infinite hidden Markov model", "comment": null, "summary": "Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature."}
{"id": "2512.03912", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03912", "abs": "https://arxiv.org/abs/2512.03912", "authors": ["Yixi Xu", "Yi Zhao"], "title": "Parsimonious Clustering of Covariance Matrices", "comment": null, "summary": "Functional connectivity (FC) derived from functional magnetic resonance imaging (fMRI) data offers vital insights for understanding brain function and neurological and psychiatric disorders. Unsupervised clustering methods are desired to group individuals based on shared features, facilitating clinical diagnosis. In this study, a parsimonious clustering model is proposed, which integrates the Mixture-of-Experts (MoE) and covariance regression framework, to cluster individuals based on FC captured by data covariance matrices in resting-state fMRI studies. The model assumes common linear projections across covariance matrices and a generalized linear model with covariates, allowing for flexible yet interpretable projection-specific clustering solutions. To evaluate the performance of the proposed framework, extensive simulation studies are conducted to assess clustering accuracy and robustness. The approach is applied to resting-state fMRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Subgroups are identified based on brain coherence and simultaneously uncover the association with demographic factors and cognitive functions."}
{"id": "2512.03946", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03946", "abs": "https://arxiv.org/abs/2512.03946", "authors": ["Nate Wiecha", "Emily Griffith", "Brian J. Reich", "Jane A. Hoppin"], "title": "When are novel methods for analyzing complex chemical mixtures in epidemiology beneficial?", "comment": null, "summary": "Estimating the health impacts of exposure to a mixture of chemicals poses many statistical challenges: multiple correlated exposure variables, moderate to high dimensionality, and possible nonlinear and interactive health effects of mixture components. Reviews of chemical mixture methods aim to help researchers select a statistical method suited to their goals and data, but examinations of empirical performance have emphasized novel methods purpose-built for analyzing complex chemical mixtures, or other more advanced methods, over more general methods which are widely used in many application domains. We conducted a broad experimental comparison, across simulated scenarios, of both more general methods (such as generalized linear models) and novel methods (such as Bayesian Kernel Machine Regression) designed to study chemical mixtures. We assessed methods based on their ability to control Type I error rate, maximize power, provide interpretable results, and make accurate predictions. We find that when there is moderate correlation between mixture components and the exposure-response function does not have complicated interactions, or when mixture components have opposite effects, general methods are preferred over novel ones. With highly interactive exposure-response functions or highly correlated exposures, novel methods provide important benefits. We provide a comprehensive summary of when different methods are most suitable."}
{"id": "2512.03983", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03983", "abs": "https://arxiv.org/abs/2512.03983", "authors": ["Maximilian Baum", "Francesco Sanna Passino", "Axel Gandy"], "title": "Statistical hypothesis testing for differences between layers in dynamic multiplex networks", "comment": "11 pages, 2 figures", "summary": "With the emergence of dynamic multiplex networks, corresponding to graphs where multiple types of edges evolve over time, a key inferential task is to determine whether the layers associated with different edge types differ in their connectivity. In this work, we introduce a hypothesis testing framework, under a latent space network model, for assessing whether the layers share a common latent representation. The method we propose extends previous literature related to the problem of pairwise testing for random graphs and enables global testing of differences between layers in multiplex graphs. While we introduce the method as a test for differences between layers, it can easily be adapted to test for differences between time points. We construct a test statistic based on a spectral embedding of an unfolded representation of the graph adjacency matrices and demonstrate its ability to detect differences across layers in the asymptotic regime where the number of nodes in each graph tends to infinity. The finite-sample properties of the test are empirically demonstrated by assessing its performance on both simulated data and a biological dataset describing the neural activity of larval Drosophila."}
{"id": "2512.04059", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.04059", "abs": "https://arxiv.org/abs/2512.04059", "authors": ["Alden Green", "Jonathan Taylor"], "title": "Inference for location and height of peaks of a standardized field after selection", "comment": null, "summary": "Peak inference concerns the use of local maxima (\"peaks\") of a noisy random field to detect and localize regions where underlying signal is present. We propose a peak inference method that first subjects observed peaks to a significance test of the null hypothesis that no signal is present, and then uses the peaks that are declared significant to construct post-selectively valid confidence regions for the location and height of nearby true peaks. We analyze the performance of this method in a smooth signal plus constant variance noise model under a high-curvature asymptotic assumption, and prove that it asymptotically controls both the number of false discoveries, and the number of confidence regions that do not contain a true peak, relative to the number of points at which inference is conducted. An important intermediate theoretical result uses the Kac-Rice formula to derive a novel approximation to the intensity function of a point process that counts local maxima, which is second-order accurate under the alternative, nearby high-curvature true peaks."}
{"id": "2512.03322", "categories": ["stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03322", "abs": "https://arxiv.org/abs/2512.03322", "authors": ["Geoffrey J. McLachlan", "Jinran Wu"], "title": "SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models", "comment": "15 pages, 1 figure", "summary": "Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \\textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples."}
{"id": "2512.03727", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03727", "abs": "https://arxiv.org/abs/2512.03727", "authors": ["Lorenzo Marinucci", "Leonardo Di Nino", "Gabriele D'Acunto", "Mario Edoardo Pandolfo", "Paolo Di Lorenzo", "Sergio Barbarossa"], "title": "Colored Markov Random Fields for Probabilistic Topological Modeling", "comment": "Proceeding of 2025 Asilomar Conference on Signals, Systems, and Computers", "summary": "Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior."}
{"id": "2512.03760", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03760", "abs": "https://arxiv.org/abs/2512.03760", "authors": ["Emanuele Giorgi", "Claudio Fronterre", "Peter J. Diggle"], "title": "A decay-adjusted spatio-temporal model to account for the impact of mass drug administration on neglected tropical disease prevalence", "comment": "Under review", "summary": "Prevalence surveys are routinely used to monitor the effectiveness of mass drug administration (MDA) programmes for controlling neglected tropical diseases (NTDs). We propose a decay-adjusted spatio-temporal (DAST) model that explicitly accounts for the time-varying impact of MDA on NTD prevalence, providing a flexible and interpretable framework for estimating intervention effects from sparse survey data. Using case studies on soil-transmitted helminths and lymphatic filariasis, we show that DAST offers a practical alternative to standard geostatistical models when the objective includes quantifying MDA impact and supporting short-term programmatic forecasting. We also discuss extensions and identifiability challenges, advocating for data-driven parsimony over complexity in settings where the available data are too sparse to support the estimation of highly parameterised models."}
