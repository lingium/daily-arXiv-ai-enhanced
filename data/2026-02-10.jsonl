{"id": "2602.06153", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.06153", "abs": "https://arxiv.org/abs/2602.06153", "authors": ["Anthony Almudevar", "Jacob Almudevar"], "title": "A Compound Logistic Regression Model for Binary Responses", "comment": "33 pages; 6 figures", "summary": "Logistic regression is the most commonly used method for constructing predictive models for binary responses. One significant drawback to this approach, however, is that the asymptotes of the logistic response function are fixed at 0 and 1, and there are many applications for which this constraint is inappropriate. More flexible models have been proposed for this application, most proceeding by supplementing the logistic response function with additional parameters. In this article we extend these models to allow correlated responses and the inclusion of covariates. This is achieved through the \\emph{compound logistic regression model}, for which the mean response is a function of several logistic regression functions. This permits a greater variety of models, while retaining the advantages of logistic regression."}
{"id": "2602.06262", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06262", "abs": "https://arxiv.org/abs/2602.06262", "authors": ["Bronner P. Gonçalves"], "title": "Latent variation in pathogen strain-specific effects under multiple-versions-of-treatment theory", "comment": "9 pages, 1 figure", "summary": "Evidence-informed policy on infections requires estimates of their effects on health. However, pathogenic variation, whereby occurrence of adverse outcomes depends on the infecting strain, might complicate the study of many infectious agents. Here, we consider the interpretation of epidemiologic studies on effects of infections on health when there is heterogeneity in strain-specific effects and information on strain composition is unavailable. We use potential outcomes and causal inference theory for analyses in the presence of multiple versions of treatment to argue that oft-reported quantities in these studies have a causal interpretation that depends on population frequencies of infecting strains. Moreover, as in other contexts where the treatment-variation-irrelevance assumption might be violated, transportability requires additional considerations, beyond those needed for non-compound exposures. This discussion, that considers potential heterogeneity in strain-specific effects, will facilitate interpretation of these studies, and for the reasons mentioned above, also highlights the value of pathogen subtype data."}
{"id": "2602.06267", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.06267", "abs": "https://arxiv.org/abs/2602.06267", "authors": ["Rohan Hore", "Aaditya Ramdas"], "title": "Conformal changepoint localization", "comment": "52 pages, 12 figures", "summary": "We study the problem of offline changepoint localization in a distribution-free setting. One observes a vector of data with a single changepoint, assuming that the data before and after the changepoint are iid (or more generally exchangeable) from arbitrary and unknown distributions. The goal is to produce a finite-sample confidence set for the index at which the change occurs without making any other assumptions. Existing methods often rely on parametric assumptions, tail conditions, or asymptotic approximations, or only produce point estimates. In contrast, our distribution-free algorithm, CONformal CHangepoint localization (CONCH), only leverages exchangeability arguments to construct confidence sets with finite sample coverage. By proving a conformal Neyman-Pearson lemma, we derive principled score functions that yield informative (small) sets. Moreover, with such score functions, the normalized length of the confidence set shrinks to zero under weak assumptions. We also establish a universality result showing that any distribution-free changepoint localization method must be an instance of CONCH. Experiments suggest that CONCH delivers precise confidence sets even in challenging settings involving images or text."}
{"id": "2602.06301", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.06301", "abs": "https://arxiv.org/abs/2602.06301", "authors": ["JoonHo Lee"], "title": "Design-Conditional Prior Elicitation for Dirichlet Process Mixtures: A Unified Framework for Cluster Counts and Weight Control", "comment": null, "summary": "Dirichlet process mixture (DPM) models are widely used for semiparametric Bayesian analysis in educational and behavioral research, yet specifying the concentration parameter remains a critical barrier. Default hyperpriors often impose strong, unintended assumptions about clustering, while existing calibration methods based on cluster counts suffer from computational inefficiency and fail to control the distribution of mixture weights. This article introduces Design-Conditional Elicitation (DCE), a unified framework that translates practitioner beliefs about cluster structure into coherent Gamma hyperpriors for a fixed design size J. DCE makes three contributions. First, it solves the computational bottleneck using Two-Stage Moment Matching (TSMM), which couples a closed-form approximation with an exact Newton refinement to calibrate hyperparameters without grid search. Second, addressing the \"unintended prior\" phenomenon, DCE incorporates a Dual-Anchor protocol to diagnose and optionally constrain the risk of weight dominance while transparently reporting the resulting trade-off against cluster-count fidelity. Third, the complete workflow is implemented in the open-source DPprior R package with reproducible diagnostics and a reporting checklist. Simulation studies demonstrate that common defaults such as Gamma(1, 1) induce posterior collapse rates exceeding 60% regardless of the true cluster structure, while DCE-calibrated priors substantially reduce bias and improve recovery across varying levels of data informativeness."}
{"id": "2602.06301", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.06301", "abs": "https://arxiv.org/abs/2602.06301", "authors": ["JoonHo Lee"], "title": "Design-Conditional Prior Elicitation for Dirichlet Process Mixtures: A Unified Framework for Cluster Counts and Weight Control", "comment": null, "summary": "Dirichlet process mixture (DPM) models are widely used for semiparametric Bayesian analysis in educational and behavioral research, yet specifying the concentration parameter remains a critical barrier. Default hyperpriors often impose strong, unintended assumptions about clustering, while existing calibration methods based on cluster counts suffer from computational inefficiency and fail to control the distribution of mixture weights. This article introduces Design-Conditional Elicitation (DCE), a unified framework that translates practitioner beliefs about cluster structure into coherent Gamma hyperpriors for a fixed design size J. DCE makes three contributions. First, it solves the computational bottleneck using Two-Stage Moment Matching (TSMM), which couples a closed-form approximation with an exact Newton refinement to calibrate hyperparameters without grid search. Second, addressing the \"unintended prior\" phenomenon, DCE incorporates a Dual-Anchor protocol to diagnose and optionally constrain the risk of weight dominance while transparently reporting the resulting trade-off against cluster-count fidelity. Third, the complete workflow is implemented in the open-source DPprior R package with reproducible diagnostics and a reporting checklist. Simulation studies demonstrate that common defaults such as Gamma(1, 1) induce posterior collapse rates exceeding 60% regardless of the true cluster structure, while DCE-calibrated priors substantially reduce bias and improve recovery across varying levels of data informativeness."}
{"id": "2602.06065", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06065", "abs": "https://arxiv.org/abs/2602.06065", "authors": ["Jack T. Parley", "Francesco Cagnetta", "Matthieu Wyart"], "title": "Deep networks learn to parse uniform-depth context-free languages from local statistics", "comment": null, "summary": "Understanding how the structure of language can be learned from sentences alone is a central question in both cognitive science and machine learning. Studies of the internal representations of Large Language Models (LLMs) support their ability to parse text when predicting the next word, while representing semantic notions independently of surface form. Yet, which data statistics make these feats possible, and how much data is required, remain largely unknown. Probabilistic context-free grammars (PCFGs) provide a tractable testbed for studying these questions. However, prior work has focused either on the post-hoc characterization of the parsing-like algorithms used by trained networks; or on the learnability of PCFGs with fixed syntax, where parsing is unnecessary. Here, we (i) introduce a tunable class of PCFGs in which both the degree of ambiguity and the correlation structure across scales can be controlled; (ii) provide a learning mechanism -- an inference algorithm inspired by the structure of deep convolutional networks -- that links learnability and sample complexity to specific language statistics; and (iii) validate our predictions empirically across deep convolutional and transformer-based architectures. Overall, we propose a unifying framework where correlations at different scales lift local ambiguities, enabling the emergence of hierarchical representations of the data."}
{"id": "2602.06379", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06379", "abs": "https://arxiv.org/abs/2602.06379", "authors": ["Alexandra Sokolova", "Vadim Sokolov"], "title": "E-values for Adaptive Clinical Trials: Anytime-Valid Monitoring in Practice", "comment": null, "summary": "Adaptive clinical trials rely on interim analyses, flexible stopping, and data-dependent design modifications that complicate statistical guarantees when fixed-horizon test statistics are repeatedly inspected or reused after adaptations. E-values and e-processes provide anytime-valid tests and confidence sequences that remain valid under optional stopping and optional continuation without requiring a prespecified monitoring schedule.\n  This paper is a methodology guide for practitioners. We develop the betting-martingale construction of e-processes for two-arm randomized controlled trials, show how e-values naturally handle composite null hypotheses and support futility monitoring, and provide guidance on when e-values are appropriate, when established alternatives are preferable, and how to integrate e-value monitoring with group sequential and Bayesian adaptive workflows.\n  A numerical study compares five monitoring rules -- naive and calibrated versions of frequentist, Bayesian, and e-value approaches -- in a two-arm binary-endpoint trial. Naive repeated testing and naive posterior thresholds inflate Type I error substantially under frequent interim looks. Among the valid methods, the calibrated group sequential rule achieves the highest power, the e-value rule provides robust anytime-valid control with moderate power, and the calibrated Bayesian rule is the most conservative.\n  Extended simulations show that the power gap between group sequential and e-value methods depends on the monitoring schedule and reverses under continuous monitoring. The methodology, including futility monitoring, platform trial multiplicity control, and hybrid strategies combining e-values with established methods, is implemented in the open-source R package `evalinger` and situated within the regulatory framework of the January 2026 FDA draft guidance on Bayesian methodology."}
{"id": "2602.06105", "categories": ["stat.ML", "cs.LG", "math.AG"], "pdf": "https://arxiv.org/pdf/2602.06105", "abs": "https://arxiv.org/abs/2602.06105", "authors": ["Yulia Alexandr", "Hao Duan", "Guido Montúfar"], "title": "Algebraic Robustness Verification of Neural Networks", "comment": null, "summary": "We formulate formal robustness verification of neural networks as an algebraic optimization problem. We leverage the Euclidean Distance (ED) degree, which is the generic number of complex critical points of the distance minimization problem to a classifier's decision boundary, as an architecture-dependent measure of the intrinsic complexity of robustness verification. To make this notion operational, we define the associated ED discriminant, which characterizes input points at which the number of real critical points changes, distinguishing test instances that are easier or harder to verify. We provide an explicit algorithm for computing this discriminant. We further introduce the parameter discriminant of a neural network, identifying parameters where the ED degree drops and the decision boundary exhibits reduced algebraic complexity. We derive closed-form expressions for the ED degree for several classes of neural architectures, as well as formulas for the expected number of real critical points in the infinite-width limit. Finally, we present an exact robustness certification algorithm based on numerical homotopy continuation, establishing a concrete link between metric algebraic geometry and neural network verification."}
{"id": "2602.06135", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06135", "abs": "https://arxiv.org/abs/2602.06135", "authors": ["Hannah Craddock", "Joel O. Wertheim", "Eliah Aronoff-Spencer", "Mark Beatty", "David Valentine", "Rishi Graham", "Jade C. Wang", "Lior Rennert", "Seema Shah", "Ravi Goyal", "Natasha K. Martin"], "title": "Early warning of Mpox outbreaks in U.S. jurisdictions using Lasso Vector Autoregression models with cross-jurisdictional lags", "comment": null, "summary": "Mpox is an orthopoxvirus that infects humans and animals and is transmitted primarily through close physical contact. The episodic and spatially heterogeneous dynamics of Mpox transmission underscores the need for timely, area-specific forecasts to support targeted public health responses in the U.S. We develop a Vector Autoregression model with Lasso regularization (VAR-Lasso) to generate rolling two-week-ahead forecasts of weekly Mpox cases for eight high-incidence U.S. jurisdictions using national surveillance data from the Centers for Disease Control and Prevention (CDC). The VAR-Lasso model identifies significant long-lag, cross-jurisdictional predictors. For a case study in San Diego County (SDC), these statistical predictors align with phylogenetic analysis that traces a 2023 cluster in SDC to an outbreak in Illinois six months earlier. As the need for public health action is often greatest when incidence is increasing, our performance evaluation focuses on positive-slope weighted error metrics. Forecast performance of the VAR-Lasso model is compared to a uni-variate Auto-Regressive (AR) Lasso model and a naive moving-average estimate. The models are compared using slope-weighted Root Mean Squared Error (RMSE), slope-weighted Mean Absolute Error (MAE), and slope-weighted bias. Across all observations, the VAR-Lasso model reduces slope-weighted RMSE, MAE, and bias by 12%, 7%, and 66% relative to the AR model, and by 16%, 13%, and 76% relative to the naive benchmark. Our findings highlight the value of sparse multivariate time-series models that leverage cross-jurisdictional case data for early forecasting of Mpox outbreaks. Such forecasting can aid health departments in proactively providing timely resources and messaging to mitigate the risks of a future outbreak."}
{"id": "2602.06435", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.06435", "abs": "https://arxiv.org/abs/2602.06435", "authors": ["Zhongjian Lin", "Zhentao Shi", "Yapeng Zheng"], "title": "Social Interactions Models with Latent Structures", "comment": null, "summary": "This paper studies estimation and inference of heterogeneous peer effects featuring group fixed effects and slope heterogeneity under latent structure. We adapt the Classifier-Lasso algorithm to consistently discover latent structures and determine the number of clusters. To solve the incidental parameter problem in the binary choice model with social interactions, we propose a parametric bootstrap method to debias and establish its asymptotic validity. Monte Carlo simulations confirm strong finite sample performance of our methods. In an application to students' risky behaviors, the algorithm detects two latent clusters and finds that peer effects are significant within one of the clusters, demonstrating the practical applicability in uncovering heterogeneous social interactions."}
{"id": "2602.06245", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06245", "abs": "https://arxiv.org/abs/2602.06245", "authors": ["Nicolas Ewen", "Jairo Diaz-Rodriguez", "Kelly Ramsay"], "title": "Inheritance Between Feedforward and Convolutional Networks via Model Projection", "comment": null, "summary": "Techniques for feedforward networks (FFNs) and convolutional networks (CNNs) are frequently reused across families, but the relationship between the underlying model classes is rarely made explicit. We introduce a unified node-level formalization with tensor-valued activations and show that generalized feedforward networks form a strict subset of generalized convolutional networks. Motivated by the mismatch in per-input parameterization between the two families, we propose model projection, a parameter-efficient transfer learning method for CNNs that freezes pretrained per-input-channel filters and learns a single scalar gate for each (output channel, input channel) contribution. Projection keeps all convolutional layers adaptable to downstream tasks while substantially reducing the number of trained parameters in convolutional layers. We prove that projected nodes take the generalized FFN form, enabling projected CNNs to inherit feedforward techniques that do not rely on homogeneous layer inputs. Experiments across multiple ImageNet-pretrained backbones and several downstream image classification datasets show that model projection is a strong transfer learning baseline under simple training recipes."}
{"id": "2602.06148", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06148", "abs": "https://arxiv.org/abs/2602.06148", "authors": ["Filippo Monti", "Nuno R. Faria", "Xiang Ji", "Philippe Lemey", "Moritz U. G. Kraemer", "Marc A. Suchard"], "title": "Non-Linear Drivers of Population Dynamics: a Nonparametric Coalescent Approach", "comment": null, "summary": "Effective population size (Ne(t)) is a fundamental parameter in population genetics and phylodynamics that quantifies genetic diversity and reveals demographic history. Coalescent-based methods enable the inference of Ne(t) trajectories through time from phylogenies reconstructed from molecular sequence data. Understanding the ecological and environmental drivers of population dynamics requires linking Ne(t) to external covariates. Existing approaches typically impose log-linear relationships between covariates and Ne(t), which may fail to capture complex biological processes and can introduce bias when the true relationship is nonlinear. We present a flexible Bayesian framework that integrates covariates into coalescent models with piecewise-constant Ne(t) through a Gaussian process (GP) prior. The GP, a distribution over functions, naturally accommodates nonlinear covariate effects without restrictive parametric assumptions. This formulation improves estimation of covariate-Ne(t) relationships, mitigates bias under nonlinear associations, and yields interpretable uncertainty quantification that varies across the covariate space. To balance global covariate-driven patterns with local temporal dynamics, we couple the GP prior with a Gaussian Markov random field that enforces smoothness in Ne(t) trajectories. Through simulation studies and three empirical applications - yellow fever virus dynamics in Brazil (2016-2018), late-Quaternary musk ox demography, and HIV-1 CRF02-AG evolution in Cameroon - we demonstrate that our method both confirms linear relationships where appropriate and reveals nonlinear covariate effects that would otherwise be missed or mischaracterized. This framework advances phylodynamic inference by enabling more accurate and biologically realistic modeling of how environmental and epidemiological factors shape population size through time."}
{"id": "2602.06482", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.06482", "abs": "https://arxiv.org/abs/2602.06482", "authors": ["Alfred Kume", "Stephen G. Walker"], "title": "On Stein's Method of Moments and Generalized Score Matching", "comment": "10 pages, 1 figure", "summary": "We show that a special case of method of moment estimator derived from the Stein class coincides with the class of generalized score matching estimator. Choosing a suitable weight function for generalized score matching is not straightforward. However, by placing it within the method of moment framework we can alleviate this problem by extending the Stein class to generalized method of moments. As a consequence we can work with a number of functions and hence derive generalized score matching estimators with optimal properties."}
{"id": "2602.06297", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06297", "abs": "https://arxiv.org/abs/2602.06297", "authors": ["Kayla E. Scharfstein", "Arun Kumar Kuchibhotla"], "title": "Time-uniform conformal and PAC prediction", "comment": null, "summary": "Given that machine learning algorithms are increasingly being deployed to aid in high stakes decision-making, uncertainty quantification methods that wrap around these black box models such as conformal prediction have received much attention in recent years. In sequential settings, where data are observed/generated in a streaming fashion, traditional conformal methods do not provide any guarantee without fixing the sample size. More importantly, traditional conformal methods cannot cope with sequentially updated predictions. As such, we develop an extension of the conformal prediction and related probably approximately correct (PAC) prediction frameworks to sequential settings where the number of data points is not fixed in advance. The resulting prediction sets are anytime-valid in that their expected coverage is at the required level at any time chosen by the analyst even if this choice depends on the data. We present theoretical guarantees for our proposed methods and demonstrate their validity and utility on simulated and real datasets."}
{"id": "2602.06210", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06210", "abs": "https://arxiv.org/abs/2602.06210", "authors": ["Pamela M. Chiroque-Solano", "M Lee Van Horn", "Thomas Jaki"], "title": "Evaluating Predictive Modeling Strategies for Predicting Individual Treatment Effects in Precision Medicine", "comment": null, "summary": "Precision medicine seeks to match patients with treatments that produce the greatest benefit. The Predicted Individual Treatment Effect (PITE)-the difference between predicted outcomes under treatment and control-quantifies this benefit but is difficult to estimate due to unobserved counterfactuals, high dimensionality, and complex interactions. We compared 30+ modeling strategies, including penalized and projection-based methods, flexible learners, and tree-ensembles, using a structured simulation framework varying sample size, dimensionality, multicollinearity, and interaction complexity. Performance was measured using root mean squared error (RMSE) for prediction accuracy and directional accuracy (DIR) for correctly classifying benefit versus harm. Internal validation produced optimistic estimates, whereas external validation with distributional shifts and higher-order interactions more clearly revealed model weaknesses. Penalized and projection-based approaches-ridge, lasso, elastic net, partial least squares (PLS), and principal components regression (PCR)-consistently achieved strong RMSE and DIR performance. Flexible learners excelled only under strong signals and sufficient sample sizes. Results highlight robust linear/projection defaults and the necessity of rigorous external validation."}
{"id": "2602.06579", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.06579", "abs": "https://arxiv.org/abs/2602.06579", "authors": ["Mathis Chagneux", "Mathias Müller", "Pierre Gloaguen", "Sylvain Le Corff", "Jimmy Olsson"], "title": "Efficient Online Variational Estimation via Monte Carlo Sampling", "comment": null, "summary": "This article addresses online variational estimation in parametric state-space models. We propose a new procedure for efficiently computing the evidence lower bound and its gradient in a streaming-data setting, where observations arrive sequentially. The algorithm allows for the simultaneous training of the model parameters and the distribution of the latent states given the observations. It is based on i.i.d. Monte Carlo sampling, coupled with a well-chosen deep architecture, enabling both computational efficiency and flexibility. The performance of the method is illustrated on both synthetic data and real-world air-quality data. The proposed approach is theoretically motivated by the existence of an asymptotic contrast function and the ergodicity of the underlying Markov chain, and applies more generally to the computation of additive expectations under posterior distributions in state-space models."}
{"id": "2602.06320", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06320", "abs": "https://arxiv.org/abs/2602.06320", "authors": ["Sota Nishiyama", "Masaaki Imaizumi"], "title": "High-Dimensional Limit of Stochastic Gradient Flow via Dynamical Mean-Field Theory", "comment": null, "summary": "Modern machine learning models are typically trained via multi-pass stochastic gradient descent (SGD) with small batch sizes, and understanding their dynamics in high dimensions is of great interest. However, an analytical framework for describing the high-dimensional asymptotic behavior of multi-pass SGD with small batch sizes for nonlinear models is currently missing. In this study, we address this gap by analyzing the high-dimensional dynamics of a stochastic differential equation called a \\emph{stochastic gradient flow} (SGF), which approximates multi-pass SGD in this regime. In the limit where the number of data samples $n$ and the dimension $d$ grow proportionally, we derive a closed system of low-dimensional and continuous-time equations and prove that it characterizes the asymptotic distribution of the SGF parameters. Our theory is based on the dynamical mean-field theory (DMFT) and is applicable to a wide range of models encompassing generalized linear models and two-layer neural networks. We further show that the resulting DMFT equations recover several existing high-dimensional descriptions of SGD dynamics as special cases, thereby providing a unifying perspective on prior frameworks such as online SGD and high-dimensional linear regression. Our proof builds on the existing DMFT technique for gradient flow and extends it to handle the stochasticity in SGF using tools from stochastic calculus."}
{"id": "2602.06322", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.06322", "abs": "https://arxiv.org/abs/2602.06322", "authors": ["Dananjani Liyanage", "Mahmudul Bari Hridoy", "Fahad Mostafa"], "title": "Modeling the Hazard Function with Non-linear Systems in Dynamical Survival Analysis", "comment": null, "summary": "Hazard functions play a central role in survival analysis, offering insight into the underlying risk dynamics of time to event data, with broad applications in medicine, epidemiology, and related fields. First order ordinary differential equation (ODE) formulations of the hazard function have been explored as extensions beyond classical parametric models. However, such approaches typically produce monotonic hazard patterns, limiting their ability to represent oscillatory behavior, nonlinear damping, or coupled growth decay dynamics. We propose a new statistical framework for modeling and simulating hazard functions governed by higher-order ODEs, allowing risk to depend on both its current level, its rate of change, and time. This class of models captures complex time dependent risk behaviors relevant to survival analysis and reliability studies. We develop a simulation procedure by reformulating the higher order ODE as a system of nonlinear first order equations solved numerically, with failure times generated via cumulative hazard inversion. Likelihood based inference under right censoring is also developed, and moment generating function analysis is used to characterize tail behavior. The proposed framework is evaluated through simulation studies and illustrated using real world survival data, where oscillatory hazard dynamics capture temporal risk patterns beyond standard monotone models."}
{"id": "2602.06828", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.06828", "abs": "https://arxiv.org/abs/2602.06828", "authors": ["Remi Luschei", "Werner Brannath"], "title": "A prediction interval for the population-wise error rate", "comment": null, "summary": "We construct an asymptotic prediction interval for the population-wise error rate (PWER), which is a multiple type I error criterion for clinical trials with overlapping patient populations. The PWER is the probability that a randomly selected patient will receive an ineffective treatment. It must usually be estimated due to unknown population strata sizes, such that only an estimate can be controlled at the given significance level. We apply the delta method to find a prediction interval for the resulting true PWER, we demonstrate by simulations that the interval has the required coverage probability, and illustrate the approach with real data examples."}
{"id": "2602.06539", "categories": ["stat.ML", "cs.CG"], "pdf": "https://arxiv.org/pdf/2602.06539", "abs": "https://arxiv.org/abs/2602.06539", "authors": ["Marc Janthial", "Théo Lacombe"], "title": "Revisiting the Sliced Wasserstein Kernel for persistence diagrams: a Figalli-Gigli approach", "comment": null, "summary": "The Sliced Wasserstein Kernel (SWK) for persistence diagrams was introduced in (Carri{è}re et al. 2017) as a powerful tool to implicitly embed persistence diagrams in a Hilbert space with reasonable distortion. This kernel is built on the intuition that the Figalli-Gigli distance-that is the partial matching distance routinely used to compare persistence diagrams-resembles the Wasserstein distance used in the optimal transport literature, and that the later could be sliced to define a positive definite kernel on the space of persistence diagrams. This efficient construction nonetheless relies on ad-hoc tweaks on the Wasserstein distance to account for the peculiar geometry of the space of persistence diagrams. In this work, we propose to revisit this idea by directly using the Figalli-Gigli distance instead of the Wasserstein one as the building block of our kernel. On the theoretical side, our sliced Figalli-Gigli kernel (SFGK) shares most of the important properties of the SWK of Carri{è}re et al., including distortion results on the induced embedding and its ease of computation, while being more faithful to the natural geometry of persistence diagrams. In particular, it can be directly used to handle infinite persistence diagrams and persistence measures. On the numerical side, we show that the SFGK performs as well as the SWK on benchmark applications."}
{"id": "2602.06262", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06262", "abs": "https://arxiv.org/abs/2602.06262", "authors": ["Bronner P. Gonçalves"], "title": "Latent variation in pathogen strain-specific effects under multiple-versions-of-treatment theory", "comment": "9 pages, 1 figure", "summary": "Evidence-informed policy on infections requires estimates of their effects on health. However, pathogenic variation, whereby occurrence of adverse outcomes depends on the infecting strain, might complicate the study of many infectious agents. Here, we consider the interpretation of epidemiologic studies on effects of infections on health when there is heterogeneity in strain-specific effects and information on strain composition is unavailable. We use potential outcomes and causal inference theory for analyses in the presence of multiple versions of treatment to argue that oft-reported quantities in these studies have a causal interpretation that depends on population frequencies of infecting strains. Moreover, as in other contexts where the treatment-variation-irrelevance assumption might be violated, transportability requires additional considerations, beyond those needed for non-compound exposures. This discussion, that considers potential heterogeneity in strain-specific effects, will facilitate interpretation of these studies, and for the reasons mentioned above, also highlights the value of pathogen subtype data."}
{"id": "2602.06910", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06910", "abs": "https://arxiv.org/abs/2602.06910", "authors": ["Björn Bornkamp", "Jiarui Lu", "Frank Bretz"], "title": "Assessment of evidence against homogeneity in exhaustive subgroup treatment effect plots", "comment": null, "summary": "Exhaustive subgroup treatment effect plots are constructed by displaying all subgroup treatment effects of interest against subgroup sample size, providing a useful overview of the observed treatment effect heterogeneity in a clinical trial. As in any exploratory subgroup analysis, however, the observed estimates suffer from small sample sizes and multiplicity issues. To facilitate more interpretable exploratory assessments, this paper introduces a computationally efficient method to generate homogeneity regions within exhaustive subgroup treatment effect plots. Using the Doubly Robust (DR) learner, pseudo-outcomes are used to estimate subgroup effects and derive reference distributions, quantifying how surprising observed heterogeneity is under a homogeneous effects model. Explicit formulas are derived for the homogeneity region and different methods for calculation of the critical values are compared. The method is illustrated with a cardiovascular trial and evaluated via simulation, showing well-calibrated inference and improved performance over standard approaches using simple differences of observed group means."}
{"id": "2602.06545", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06545", "abs": "https://arxiv.org/abs/2602.06545", "authors": ["Zhiyu Zhang", "Aaditya Ramdas"], "title": "Operationalizing Stein's Method for Online Linear Optimization: CLT-Based Optimal Tradeoffs", "comment": null, "summary": "Adversarial online linear optimization (OLO) is essentially about making performance tradeoffs with respect to the unknown difficulty of the adversary. In the setting of one-dimensional fixed-time OLO on a bounded domain, it has been observed since Cover (1966) that achievable tradeoffs are governed by probabilistic inequalities, and these descriptive results can be converted into algorithms via dynamic programming, which, however, is not computationally efficient. We address this limitation by showing that Stein's method, a classical framework underlying the proofs of probabilistic limit theorems, can be operationalized as computationally efficient OLO algorithms. The associated regret and total loss upper bounds are \"additively sharp\", meaning that they surpass the conventional big-O optimality and match normal-approximation-based lower bounds by additive lower order terms. Our construction is inspired by the remarkably clean proof of a Wasserstein martingale central limit theorem (CLT) due to Röllin (2018).\n  Several concrete benefits can be obtained from this general technique. First, with the same computational complexity, the proposed algorithm improves upon the total loss upper bounds of online gradient descent (OGD) and multiplicative weight update (MWU). Second, our algorithm can realize a continuum of optimal two-point tradeoffs between the total loss and the maximum regret over comparators, improving upon prior works in parameter-free online learning. Third, by allowing the adversary to randomize on an unbounded support, we achieve sharp in-expectation performance guarantees for OLO with noisy feedback."}
{"id": "2602.06301", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.06301", "abs": "https://arxiv.org/abs/2602.06301", "authors": ["JoonHo Lee"], "title": "Design-Conditional Prior Elicitation for Dirichlet Process Mixtures: A Unified Framework for Cluster Counts and Weight Control", "comment": null, "summary": "Dirichlet process mixture (DPM) models are widely used for semiparametric Bayesian analysis in educational and behavioral research, yet specifying the concentration parameter remains a critical barrier. Default hyperpriors often impose strong, unintended assumptions about clustering, while existing calibration methods based on cluster counts suffer from computational inefficiency and fail to control the distribution of mixture weights. This article introduces Design-Conditional Elicitation (DCE), a unified framework that translates practitioner beliefs about cluster structure into coherent Gamma hyperpriors for a fixed design size J. DCE makes three contributions. First, it solves the computational bottleneck using Two-Stage Moment Matching (TSMM), which couples a closed-form approximation with an exact Newton refinement to calibrate hyperparameters without grid search. Second, addressing the \"unintended prior\" phenomenon, DCE incorporates a Dual-Anchor protocol to diagnose and optionally constrain the risk of weight dominance while transparently reporting the resulting trade-off against cluster-count fidelity. Third, the complete workflow is implemented in the open-source DPprior R package with reproducible diagnostics and a reporting checklist. Simulation studies demonstrate that common defaults such as Gamma(1, 1) induce posterior collapse rates exceeding 60% regardless of the true cluster structure, while DCE-calibrated priors substantially reduce bias and improve recovery across varying levels of data informativeness."}
{"id": "2602.06322", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.06322", "abs": "https://arxiv.org/abs/2602.06322", "authors": ["Dananjani Liyanage", "Mahmudul Bari Hridoy", "Fahad Mostafa"], "title": "Modeling the Hazard Function with Non-linear Systems in Dynamical Survival Analysis", "comment": null, "summary": "Hazard functions play a central role in survival analysis, offering insight into the underlying risk dynamics of time to event data, with broad applications in medicine, epidemiology, and related fields. First order ordinary differential equation (ODE) formulations of the hazard function have been explored as extensions beyond classical parametric models. However, such approaches typically produce monotonic hazard patterns, limiting their ability to represent oscillatory behavior, nonlinear damping, or coupled growth decay dynamics. We propose a new statistical framework for modeling and simulating hazard functions governed by higher-order ODEs, allowing risk to depend on both its current level, its rate of change, and time. This class of models captures complex time dependent risk behaviors relevant to survival analysis and reliability studies. We develop a simulation procedure by reformulating the higher order ODE as a system of nonlinear first order equations solved numerically, with failure times generated via cumulative hazard inversion. Likelihood based inference under right censoring is also developed, and moment generating function analysis is used to characterize tail behavior. The proposed framework is evaluated through simulation studies and illustrated using real world survival data, where oscillatory hazard dynamics capture temporal risk patterns beyond standard monotone models."}
{"id": "2602.06621", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06621", "abs": "https://arxiv.org/abs/2602.06621", "authors": ["Thorben Pieper-Sethmacher", "Daniel Paulin"], "title": "Infinite-dimensional generative diffusions via Doob's h-transform", "comment": null, "summary": "This paper introduces a rigorous framework for defining generative diffusion models in infinite dimensions via Doob's h-transform. Rather than relying on time reversal of a noising process, a reference diffusion is forced towards the target distribution by an exponential change of measure. Compared to existing methodology, this approach readily generalises to the infinite-dimensional setting, hence offering greater flexibility in the diffusion model. The construction is derived rigorously under verifiable conditions, and bounds with respect to the target measure are established. We show that the forced process under the changed measure can be approximated by minimising a score-matching objective and validate our method on both synthetic and real data."}
{"id": "2602.06379", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06379", "abs": "https://arxiv.org/abs/2602.06379", "authors": ["Alexandra Sokolova", "Vadim Sokolov"], "title": "E-values for Adaptive Clinical Trials: Anytime-Valid Monitoring in Practice", "comment": null, "summary": "Adaptive clinical trials rely on interim analyses, flexible stopping, and data-dependent design modifications that complicate statistical guarantees when fixed-horizon test statistics are repeatedly inspected or reused after adaptations. E-values and e-processes provide anytime-valid tests and confidence sequences that remain valid under optional stopping and optional continuation without requiring a prespecified monitoring schedule.\n  This paper is a methodology guide for practitioners. We develop the betting-martingale construction of e-processes for two-arm randomized controlled trials, show how e-values naturally handle composite null hypotheses and support futility monitoring, and provide guidance on when e-values are appropriate, when established alternatives are preferable, and how to integrate e-value monitoring with group sequential and Bayesian adaptive workflows.\n  A numerical study compares five monitoring rules -- naive and calibrated versions of frequentist, Bayesian, and e-value approaches -- in a two-arm binary-endpoint trial. Naive repeated testing and naive posterior thresholds inflate Type I error substantially under frequent interim looks. Among the valid methods, the calibrated group sequential rule achieves the highest power, the e-value rule provides robust anytime-valid control with moderate power, and the calibrated Bayesian rule is the most conservative.\n  Extended simulations show that the power gap between group sequential and e-value methods depends on the monitoring schedule and reverses under continuous monitoring. The methodology, including futility monitoring, platform trial multiplicity control, and hybrid strategies combining e-values with established methods, is implemented in the open-source R package `evalinger` and situated within the regulatory framework of the January 2026 FDA draft guidance on Bayesian methodology."}
{"id": "2602.06713", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06713", "abs": "https://arxiv.org/abs/2602.06713", "authors": ["Luke Shannon", "Song Liu", "Katarzyna Reluga"], "title": "Missing At Random as Covariate Shift: Correcting Bias in Iterative Imputation", "comment": "8 pages, 6 figures", "summary": "Accurate imputation of missing data is critical to downstream machine learning performance. We formulate missing data imputation as a risk minimisation problem, which highlights a covariate shift between the observed and unobserved data distributions. This covariate shift induced bias is not accounted for by popular imputation methods and leads to suboptimal performance. In this paper, we derive theoretically valid importance weights that correct for the induced distributional bias. Furthermore, we propose a novel imputation algorithm that jointly estimates both the importance weights and imputation models, enabling bias correction throughout the imputation process. Empirical results across benchmark datasets show reductions in root mean squared error and Wasserstein distance of up to 7% and 20%, respectively, compared to otherwise identical unweighted methods."}
{"id": "2602.06910", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.06910", "abs": "https://arxiv.org/abs/2602.06910", "authors": ["Björn Bornkamp", "Jiarui Lu", "Frank Bretz"], "title": "Assessment of evidence against homogeneity in exhaustive subgroup treatment effect plots", "comment": null, "summary": "Exhaustive subgroup treatment effect plots are constructed by displaying all subgroup treatment effects of interest against subgroup sample size, providing a useful overview of the observed treatment effect heterogeneity in a clinical trial. As in any exploratory subgroup analysis, however, the observed estimates suffer from small sample sizes and multiplicity issues. To facilitate more interpretable exploratory assessments, this paper introduces a computationally efficient method to generate homogeneity regions within exhaustive subgroup treatment effect plots. Using the Doubly Robust (DR) learner, pseudo-outcomes are used to estimate subgroup effects and derive reference distributions, quantifying how surprising observed heterogeneity is under a homogeneous effects model. Explicit formulas are derived for the homogeneity region and different methods for calculation of the critical values are compared. The method is illustrated with a cardiovascular trial and evaluated via simulation, showing well-calibrated inference and improved performance over standard approaches using simple differences of observed group means."}
{"id": "2602.06797", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06797", "abs": "https://arxiv.org/abs/2602.06797", "authors": ["Binghui Li", "Zilin Wang", "Fengling Chen", "Shiyang Zhao", "Ruiheng Zheng", "Lei Wu"], "title": "Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay", "comment": null, "summary": "We study optimal learning-rate schedules (LRSs) under the functional scaling law (FSL) framework introduced in Li et al. (2025), which accurately models the loss dynamics of both linear regression and large language model (LLM) pre-training. Within FSL, loss dynamics are governed by two exponents: a source exponent $s>0$ controlling the rate of signal learning, and a capacity exponent $β>1$ determining the rate of noise forgetting. Focusing on a fixed training horizon $N$, we derive the optimal LRSs and reveal a sharp phase transition. In the easy-task regime $s \\ge 1 - 1/β$, the optimal schedule follows a power decay to zero, $η^*(z) = η_{\\mathrm{peak}}(1 - z/N)^{2β- 1}$, where the peak learning rate scales as $η_{\\mathrm{peak}} \\eqsim N^{-ν}$ for an explicit exponent $ν= ν(s,β)$. In contrast, in the hard-task regime $s < 1 - 1/β$, the optimal LRS exhibits a warmup-stable-decay (WSD) (Hu et al. (2024)) structure: it maintains the largest admissible learning rate for most of training and decays only near the end, with the decay phase occupying a vanishing fraction of the horizon.\n  We further analyze optimal shape-fixed schedules, where only the peak learning rate is tuned -- a strategy widely adopted in practiceand characterize their strengths and intrinsic limitations. This yields a principled evaluation of commonly used schedules such as cosine and linear decay. Finally, we apply the power-decay LRS to one-pass stochastic gradient descent (SGD) for kernel regression and show the last iterate attains the exact minimax-optimal rate, eliminating the logarithmic suboptimality present in prior analyses. Numerical experiments corroborate our theoretical predictions."}
{"id": "2602.06579", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.06579", "abs": "https://arxiv.org/abs/2602.06579", "authors": ["Mathis Chagneux", "Mathias Müller", "Pierre Gloaguen", "Sylvain Le Corff", "Jimmy Olsson"], "title": "Efficient Online Variational Estimation via Monte Carlo Sampling", "comment": null, "summary": "This article addresses online variational estimation in parametric state-space models. We propose a new procedure for efficiently computing the evidence lower bound and its gradient in a streaming-data setting, where observations arrive sequentially. The algorithm allows for the simultaneous training of the model parameters and the distribution of the latent states given the observations. It is based on i.i.d. Monte Carlo sampling, coupled with a well-chosen deep architecture, enabling both computational efficiency and flexibility. The performance of the method is illustrated on both synthetic data and real-world air-quality data. The proposed approach is theoretically motivated by the existence of an asymptotic contrast function and the ergodicity of the underlying Markov chain, and applies more generally to the computation of additive expectations under posterior distributions in state-space models."}
