<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 3]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 26]
- [stat.ML](#stat.ML) [Total: 15]
- [stat.OT](#stat.OT) [Total: 1]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [StochTree: BART-based modeling in R and Python](https://arxiv.org/abs/2512.12051)
*Andrew Herren,P. Richard Hahn,Jared Murray,Carlos Carvalho*

Main category: stat.CO

TL;DR: stochtree是一个C++库，提供贝叶斯树集成模型（如BART和BCF）的实现，支持R和Python绑定，具有更全面的模型范围和灵活的模型拟合处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有BART包功能有限，缺乏跨平台互操作性，模型范围不够全面，且用户难以自定义模型。stochtree旨在解决这些问题，提供更灵活、全面的贝叶斯树集成建模工具。

Method: 开发C++核心库，提供R和Python绑定实现跨平台互操作性。支持多种贝叶斯树集成模型变体，包括异方差森林、随机效应和树状线性模型。提供灵活的模型拟合处理机制，如保存/加载模型、改进初始化启发式方法。

Result: stochtree成功实现了：1）R和Python的完整互操作性；2）更全面的模型范围；3）灵活的模型拟合处理能力；4）底层功能暴露，允许用户自定义模型而无需修改C++代码。

Conclusion: stochtree是一个功能强大的贝叶斯树集成建模库，通过跨平台支持和灵活的设计，能够满足从标准应用到复杂自定义模型的各种需求，为贝叶斯树集成方法的研究和应用提供了更好的工具。

Abstract: stochtree is a C++ library for Bayesian tree ensemble models such as BART and Bayesian Causal Forests (BCF), as well as user-specified variations. Unlike previous BART packages, stochtree provides bindings to both R and Python for full interoperability. stochtree boasts a more comprehensive range of models relative to previous packages, including heteroskedastic forests, random effects, and treed linear models. Additionally, stochtree offers flexible handling of model fits: the ability to save model fits, reinitialize models from existing fits (facilitating improved model initialization heuristics), and pass fits between R and Python. On both platforms, stochtree exposes lower-level functionality, allowing users to specify models incorporating Bayesian tree ensembles without needing to modify C++ code. We illustrate the use of stochtree in three settings: i) straightfoward applications of existing models such as BART and BCF, ii) models that include more sophisticated components like heteroskedasticity and leaf-wise regression models, and iii) as a component of custom MCMC routines to fit nonstandard tree ensemble models.

</details>


### [2] [Complexity of Markov Chain Monte Carlo for Generalized Linear Models](https://arxiv.org/abs/2512.12748)
*Martin Chak,Giacomo Zanella*

Main category: stat.CO

TL;DR: 该论文比较了MCMC、拉普拉斯近似和变分推断在贝叶斯推断中的计算复杂度，发现在n≳d条件下，MCMC的复杂度与一阶优化算法相当，比BvM定理要求的n≫d²条件更宽松。


<details>
  <summary>Details</summary>
Motivation: 理解MCMC、拉普拉斯近似和变分推断在计算成本与精度之间的权衡差异，特别是在大样本量n和大维度d的情况下，缺乏理论分析。

Method: 针对线性、逻辑和泊松回归模型，分析MCMC算法的计算复杂度，并与拉普拉斯近似、高斯变分推断进行比较。考虑包括Student-t和平坦先验在内的非高斯尾先验，以及非全局凹或梯度Lipschitz的对数后验。

Result: 在n≳d条件下，MCMC达到与一阶优化算法相同的复杂度缩放（至多差亚多项式因子），比BvM定理要求的n≫d²条件更宽松。MCMC在复杂度上与拉普拉斯近似和高斯变分推断具有竞争力。

Conclusion: MCMC在更一般的n与d缩放关系下具有计算竞争力，突破了传统BvM定理的限制条件，适用于更广泛的先验分布和后验形状。

Abstract: Markov Chain Monte Carlo (MCMC), Laplace approximation (LA) and variational inference (VI) methods are popular approaches to Bayesian inference, each with trade-offs between computational cost and accuracy. However, a theoretical understanding of these differences is missing, particularly when both the sample size $n$ and the dimension $d$ are large. LA and Gaussian VI are justified by Bernstein-von Mises (BvM) theorems, and recent work has derived the characteristic condition $n\gg d^2$ for their validity, improving over the condition $n\gg d^3$. In this paper, we show for linear, logistic and Poisson regression that for $n\gtrsim d$, MCMC attains the same complexity scaling in $n$, $d$ as first-order optimization algorithms, up to sub-polynomial factors. Thus MCMC is competitive with LA and Gaussian VI in complexity, under a scaling between $n$ and $d$ more general than BvM regimes. Our complexities apply to appropriately scaled priors that are not necessarily Gaussian-tailed, including Student-$t$ and flat priors, with log-posteriors that are not necessarily globally concave or gradient-Lipschitz.

</details>


### [3] [Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations](https://arxiv.org/abs/2512.12749)
*Sahil Bhola,Karthik Duraisamy*

Main category: stat.CO

TL;DR: 提出一种在无限维函数空间中学习PDE概率代理的方法，通过流匹配将低精度近似映射到高精度解流形，实现任意分辨率推理和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺情况下学习PDE概率代理具有挑战性：神经算子需要大量高精度数据，而生成方法通常牺牲分辨率不变性。需要一种能在有限高精度数据下学习，同时保持分辨率不变性和提供不确定性估计的方法。

Method: 1) 在无限维函数空间中制定流匹配，学习从低精度近似到高精度PDE解流形的概率传输；2) 基于特征线性调制的条件神经算子架构，直接在函数空间中匹配流向量场；3) 将流向量场参数化为线性算子和非线性算子之和，结合轻量线性组件和条件傅里叶神经算子；4) 残差增强学习策略，学习从廉价低精度代理到高精度解的概率修正；5) 推导可处理的训练目标，将条件流匹配扩展到算子设置。

Result: 在1D平流方程、Burgers方程和2D达西流问题上的实验表明，该方法能准确学习不同分辨率和精度下的解算子，即使在有限高精度数据下训练，也能产生适当反映模型置信度的不确定性估计。

Conclusion: 该方法成功解决了数据稀缺情况下学习PDE概率代理的挑战，实现了分辨率不变推理和不确定性量化，为PDE求解提供了高效的概率框架。

Abstract: Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium.
  We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [4] [Estimation of Heterogeneous Causal Mediation Effects in a Hypertension Treatment Trial](https://arxiv.org/abs/2512.12043)
*Yi Zhao,Chengyun Li,Wanzhu Tu*

Main category: stat.AP

TL;DR: 该研究开发了一个新的因果中介分析框架，用于识别高血压治疗中白蛋白尿降低的中介效应异质性，发现SPRINT试验中只有部分患者能从针对白蛋白尿的治疗中获益。


<details>
  <summary>Details</summary>
Motivation: SPRINT试验显示将收缩压目标从140mmHg降至120mmHg可显著降低心血管死亡率，但其机制尚不完全清楚。虽然早期白蛋白尿降低被认为是潜在的中介通路，但传统因果中介分析结果不一致，可能源于个体间中介效应的异质性。

Method: 提出了一个包含协变量-治疗和中介-治疗交互作用的线性结构方程模型新框架。参数化异质性自然直接和间接效应为患者特征的函数，采用改进的协变量方法放松层次约束，并使用广义lasso正则化确保高维设置的简约性。

Result: 模拟研究显示良好的估计和推断性能。SPRINT数据分析揭示了显著的中介效应异质性，识别出能从针对白蛋白尿治疗中获益的患者亚组。

Conclusion: 该研究为理解高血压治疗机制提供了新视角，表明只有特定患者亚群能从白蛋白尿靶向治疗中获益，强调了考虑个体异质性的重要性。

Abstract: Hypertension is a highly prevalent condition and a major risk factor for cardiovascular disease. The landmark Systolic Blood Pressure Intervention Trial (SPRINT) showed that lowering systolic blood pressure (BP) goals from 140 mmHg to 120 mmHg leads to significantly reduced BP, cardiovascular mortality, and morbidity. However, the underlying mechanisms are not yet fully elucidated. In patients with impaired renal function, early reduction of albuminuria has been proposed as a potential mediation pathway. Evidence from the standard causal mediation analysis (CMA), however, yields inconsistent results, possibly due to heterogeneous mediation effects across individuals. To disseminate the heterogeneity, a new framework that incorporates covariate-treatment and mediator-treatment interactions within a linear structural equation modeling system is introduced. Causal assumptions are discussed and heterogeneous natural direct and indirect effects are parameterized as functions of patient characteristics. A modified covariate approach is proposed to relax the hierarchical constraints and the generalized lasso regularization is employed to ensure parsimony in high-dimensional settings. Asymptotic properties are studied. Simulation studies demonstrate good estimation and inference performance. Analysis of the SPRINT data reveals substantial heterogeneity in mediation effects, identifying a subset of patients who stand to gain from therapies targeting albuminuria.

</details>


### [5] [A Real Data-Driven, Robust Survival Analysis on Patients who Underwent Deep Brain Stimulation for Parkinson's Disease by Utilizing Parametric, Non-Parametric, and Semi-Parametric Approaches](https://arxiv.org/abs/2512.12579)
*Malinda Iluppangama,Dilmi Abeywardana,Chris Tsokos*

Main category: stat.AP

TL;DR: 该研究使用非参数、半参数和稳健参数生存分析方法，分析帕金森病深部脑刺激患者的长期生存结果，发现女性患者生存率高于男性，且两性生存时间分布特征不同。


<details>
  <summary>Details</summary>
Motivation: 帕金森病严重影响患者生活质量，深部脑刺激治疗在改善运动症状方面显示出良好效果。研究旨在通过统计方法分析DBS患者的长期生存结果，为临床治疗提供依据。

Method: 采用非参数、半参数和稳健参数生存分析方法，包括COX比例风险模型，分析DBS患者的生存数据，比较性别差异和影响因素。

Result: 女性患者生存率高于男性；女性生存时间符合3参数对数正态分布，男性符合3参数威布尔分布；右侧初始植入对女性预后较差，对男性预后较好；修订次数与初始植入尺寸的交互作用对男性预后有负面影响。

Conclusion: DBS治疗帕金森病患者的生存结果存在显著性别差异，女性预后优于男性。不同性别患者的生存时间分布特征不同，且临床因素对预后的影响存在性别特异性，这为个性化治疗提供了重要参考。

Abstract: Parkinson's Disease (PD) is a devastating neurodegenerative disorder that affects millions of people around the globe. Many researchers are continuously working to understand PD and develop treatments to improve the condition of PD patients, which affects their day-to-day lives. Since the last decades, the treatment, Deep Brain Stimulation (DBS) has given promising results for motor symptoms by improving the quality of daily living of PD patients. In the methodology of the present study, we have utilized sophisticated statistical approaches such as Nonparametric, Semi-parametric, and robust Parametric survival analysis to extract useful and important information about the long-term survival outcomes of the patients who underwent DBS for PD. Finally, we were able to conclude that the probabilistic behavior of the survival time of female patients is statistically different from that of male patients. Furthermore, we have identified that the probabilistic behavior of the survival times of Female patients is characterized by the 3-parameter Lognormal distribution, while that of Male patients is characterized by the 3-parameter Weibull distribution. More importantly, we have found that the Female patients have higher survival compared to the Male patients after conducting a robust parametric survival analysis. Using the semi-parametric COX-PH, we found that the initial implant of the right side leads to a high frequency of events occurring for the female patients with a bad prognostic factor, while for the male patients, a low events occurs with a good prognostic factor. Furthermore, we have found an interaction term between the number of revisions and the initial size of the implant, which increases the frequency of events occurring for the Male patients with a bad prognostic factor.

</details>


### [6] [Beyond Missing Data: Questionnaire Uncertainty Responses as Early Digital Biomarkers of Cognitive Decline and Neurodegenerative Diseases](https://arxiv.org/abs/2512.13346)
*Yukun Lu,Bingjie Li,Zhigang Yao*

Main category: stat.AP

TL;DR: "不知道/记不住"回答频率可作为早期认知脆弱和神经退行性疾病风险的新型数字行为生物标志物


<details>
  <summary>Details</summary>
Motivation: 识别神经退行性疾病的临床前生物标志物是衰老研究的主要挑战。研究旨在探索通常被视为缺失数据的"不知道/记不住"回答是否具有临床意义

Method: 使用502,234名UK Biobank参与者的数据，根据DK回答频率(0-1, 2-4, 5-7, >7)对个体分层，分析其与阿尔茨海默病和血管性痴呆风险的关系，并检测相关生物标志物和代谢组学变化

Result: DK回答频率与阿尔茨海默病(HR=1.64)和血管性痴呆(HR=1.93)风险呈剂量依赖关系增加。高DK回答者显示更高的BMI、更少的体力活动、更高吸烟率和慢性病患病率，以及Abeta40、Abeta42、NFL、pTau-181等早期神经退行性变化标志物升高，同时存在脂质代谢异常

Conclusion: DK回答模式是临床有意义的多维神经生物学改变信号，为人群水平的早期风险识别和预防提供了可扩展、低成本、非侵入性的工具

Abstract: Identifying preclinical biomarkers of neurodegenerative diseases remains a major challenge in aging research. In this study, we demonstrate that frequent "Don't know/can't remember" (DK) responses, often treated as missing data in touchscreen questionnaires, serve as a novel digital behavioral biomarker of early cognitive vulnerability and neurodegenerative disease risk. Using data from 502,234 UK Biobank participants, we stratified individuals based on DK response frequency (0-1, 2-4, 5-7, >7) and observed a robust, dose-dependent association with an increased risk of Alzheimer's disease (HR = 1.64, 95% CI: 1.26-2.14) and vascular dementia (HR = 1.93, 95% CI: 1.37-2.72), independent of established risk factors. As DK response frequency increased, participants exhibited higher BMI, reduced physical activity, higher smoking rates, and a higher prevalence of chronic diseases, particularly hypertension, diabetes, and depression. Further analysis revealed a dose-dependent relationship between DK response frequency and the risk of Alzheimer's disease and vascular dementia, with high DK responders showing early neurodegenerative changes, marked by elevated levels of Abeta40, Abeta42, NFL, and pTau-181. Metabolomic analysis also revealed lipid metabolism abnormalities, which may mediate this relationship. Together, these findings reframe DK response patterns as clinically meaningful signals of multidimensional neurobiological alterations, offering a scalable, low-cost, non-invasive tool for early risk identification and prevention at the population level.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [7] [A fine-grained look at causal effects in causal spaces](https://arxiv.org/abs/2512.11919)
*Junhyung Park,Yuqing Zhou*

Main category: stat.ME

TL;DR: 论文提出在事件层面而非变量层面研究因果效应，为现代数据领域（如图像像素、语言模型标记）提供更细粒度的因果分析框架。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应研究在变量层面进行，但现代数据领域（如像素、语言标记）缺乏语义结构，难以提出有意义的因果问题。需要更细粒度的视角来研究因果效应。

Method: 在因果空间的测度理论框架下，首先引入多个二元定义来判断因果效应是否存在，并证明其在干预测度下与（不）独立性的联系。然后提供量化因果效应强度和性质的测度。

Result: 提出的框架能够捕捉事件层面的因果效应，并且证明常见的处理效应度量可以作为该框架的特殊情况恢复出来。

Conclusion: 事件层面的因果效应分析为现代数据领域提供了更合适的因果推理框架，能够处理传统变量层面方法难以应对的语义结构缺失问题。

Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.

</details>


### [8] [Debiased Inference for High-Dimensional Regression Models Based on Profile M-Estimation](https://arxiv.org/abs/2512.12003)
*Yi Wang,Yuhao Deng,Yu Gu,Yuanjia Wang,Donglin Zeng*

Main category: stat.ME

TL;DR: 提出Debiased Profile M-Estimation (DPME)框架，用于高维回归模型的去偏推断，无需显式构造Neyman正交化或投影，通过数值微分实现，计算成本低且覆盖率高。


<details>
  <summary>Details</summary>
Motivation: 现有高维回归模型的去偏推断方法都需要显式构造对干扰参数空间的投影，这在投影形式不可得时不可行。需要一种更通用的方法，适用于更广泛的模型类别，且不需要模型特定的Neyman正交化推导。

Method: DPME框架：首先通过优化惩罚目标函数获得初始参数估计；为纠正惩罚引入的偏差，使用牛顿-拉弗森更新构造一步估计器，应用于定义在固定感兴趣参数下的最优目标函数的梯度函数；使用数值微分而不需要显式计算梯度。

Result: DPME估计器被证明是渐近线性且正态分布的。通过大量模拟实验，该方法比现有替代方法获得更好的覆盖率，且计算成本大幅降低。在多发性骨髓瘤治疗规则估计的应用中展示了其实用性。

Conclusion: DPME提供了一个通用、计算高效的去偏推断框架，适用于广泛的高维回归模型，无需模型特定的Neyman正交化推导，在实际应用中表现出优越的统计性能和计算效率。

Abstract: Debiased inference for high-dimensional regression models has received substantial recent attention to ensure regularized estimators have valid inference. All existing methods focus on achieving Neyman orthogonality through explicitly constructing projections onto the space of nuisance parameters, which is infeasible when an explicit form of the projection is unavailable. We introduce a general debiasing framework, Debiased Profile M-Estimation (DPME), which applies to a broad class of models and does not require model-specific Neyman orthogonalization or projection derivations as in existing methods. Our approach begins by obtaining an initial estimator of the parameters by optimizing a penalized objective function. To correct for the bias introduced by penalization, we construct a one-step estimator using the Newton-Raphson update, applied to the gradient of a profile function defined as the optimal objective function with the parameter of interest held fixed. We use numerical differentiation without requiring the explicit calculation of the gradients. The resulting DPME estimator is shown to be asymptotically linear and normally distributed. Through extensive simulations, we demonstrate that the proposed method achieves better coverage rates than existing alternatives, with largely reduced computational cost. Finally, we illustrate the utility of our method with an application to estimating a treatment rule for multiple myeloma.

</details>


### [9] [Proximal Causal Inference for Modified Treatment Policies](https://arxiv.org/abs/2512.12038)
*Antonio Olivas-Martinez,Peter B. Gilbert,Andrea Rotnitzky*

Main category: stat.ME

TL;DR: 扩展近端因果推断框架，识别和估计修改治疗机制下的平均结果，解决未测量混杂问题


<details>
  <summary>Details</summary>
Motivation: 现有近端因果推断主要应用于静态固定干预，对于连续暴露的治疗修改机制缺乏方法；在COVID-19疫苗免疫桥接研究中，个体免疫能力是重要的未测量混杂因素

Method: 扩展近端因果推断框架，利用负控制治疗和负控制结果，采用现代去偏机器学习技术估计nuisance函数，避免参数假设

Result: 提出灵活策略，不依赖所有混杂因素已测量的假设，应用于COVID-19疫苗研究数据，并通过模拟研究评估有限样本性能

Conclusion: 成功扩展近端因果推断到修改治疗机制场景，为存在未测量混杂的连续暴露因果效应估计提供新方法

Abstract: The proximal causal inference framework enables the identification and estimation of causal effects in the presence of unmeasured confounding by leveraging two disjoint sets of observed strong proxies: negative control treatments and negative control outcomes. In the point exposure setting, this framework has primarily been applied to estimands comparing counterfactual outcomes under a static fixed intervention or, possibly randomized, regime that depends on baseline covariates. For continuous exposures, alternative hypothetical scenarios can enrich our understanding of causal effects, such as those where each individual receives their observed treatment dose modified in a pre-specified manner - commonly referred to as modified treatment regimes. In this work, we extend the proximal causal inference framework to identify and estimate the mean outcome under a modified treatment regime, addressing this gap in the literature. We propose a flexible strategy that does not rely on the assumption that all confounders have been measured - unlike existing estimators - and leverages modern debiased machine learning techniques using non-parametric estimators of nuisance functions to avoid restrictive parametric assumptions. Our methodology was motivated by immunobridging studies of COVID-19 vaccines aimed at identifying correlates of protection, where the individual's underlying immune capacity is an important unmeasured confounder. We demonstrate its applicability using data from such a study and evaluate its finite-sample performance through simulation studies.

</details>


### [10] [Sparse Bayesian Partially Identified Models for Sequence Count Data](https://arxiv.org/abs/2512.12040)
*Won Gu,Francesca Chiaromonte,Justin D. Silverman*

Main category: stat.ME

TL;DR: 提出稀疏贝叶斯部分识别模型(PIM)，用于处理基因组学中序列计数数据的组成性限制，通过显式建模稀疏性假设的不确定性，显著降低I型和II型错误率。


<details>
  <summary>Details</summary>
Motivation: 基因组学中的差异丰度和表达分析面临组成性数据的限制，现有归一化方法依赖不现实的生物学假设，而稀疏方法又忽略稀疏性假设的不确定性，导致高错误率。

Method: 提出稀疏贝叶斯部分识别模型(PIM)，将尺度依赖推理(SRI)框架扩展到稀疏设置，显式建模稀疏性假设的不确定性，提供在尺度不确定性下的差异分析原则方法。

Result: 建立了所提估计器的理论一致性，通过大量模拟和真实数据分析，相比现有方法显著降低了I型和II型错误率。

Conclusion: 稀疏贝叶斯PIM通过显式建模稀疏性假设的不确定性，有效解决了基因组学差异分析中的组成性数据问题，提供了更可靠的统计推断框架。

Abstract: In genomics, differential abundance and expression analyses are complicated by the compositional nature of sequence count data, which reflect only relative-not absolute-abundances or expression levels. Many existing methods attempt to address this limitation through data normalizations, but we have shown that such approaches imply strong, often biologically implausible assumptions about total microbial load or total gene expression. Even modest violations of these assumptions can inflate Type I and Type II error rates to over 70%. Sparse estimators have been proposed as an alternative, leveraging the assumption that only a small subset of taxa (or genes) change between conditions. However, we show that current sparse methods suffer from similar pathologies because they treat sparsity assumptions as fixed and ignore the uncertainty inherent in these assumptions. We introduce a sparse Bayesian Partially Identified Model (PIM) that addresses this limitation by explicitly modeling uncertainty in sparsity assumptions. Our method extends the Scale-Reliant Inference (SRI) framework to the sparse setting, providing a principled approach to differential analysis under scale uncertainty. We establish theoretical consistency of the proposed estimator and, through extensive simulations and real data analyses, demonstrate substantial reductions in both Type I and Type II errors compared to existing methods.

</details>


### [11] [Scalable Spatial Stream Network (S3N) Models](https://arxiv.org/abs/2512.12398)
*Jessica P. Kunke,Julian D. Olden,Tyler H. McCormick*

Main category: stat.ME

TL;DR: 提出可扩展空间河流网络模型S3N，通过最近邻高斯过程改进计算效率，应用于俄亥俄河流域285种鱼类分布建模


<details>
  <summary>Details</summary>
Motivation: 现有空间河流网络模型计算成本高、时间消耗大，难以应用于区域或大陆尺度，限制了淡水生物多样性管理和保护的有效应用

Method: 开发可扩展空间河流网络模型框架，将最近邻高斯过程扩展到河流网络，纳入生态相关的空间依赖性，显著提高计算效率

Result: S3N模型能准确恢复空间和协方差参数，相比标准SSN实现具有更低的偏差和方差，成功应用于俄亥俄河流域285种鱼类分布建模

Conclusion: S3N模型为实现大规模淡水鱼类分布制图和量化广泛河流网络中环境驱动因素的影响提供了关键进展

Abstract: Understanding how habitats shape species distributions and abundances across spatially complex, dendritic freshwater networks remains a longstanding and fundamental challenge in ecology, with direct implications for effective biodiversity management and conservation. Existing spatial stream network (SSN) models adapt spatial process models to river networks by creating covariance functions that account for stream distance, but preprocessing and estimation with these models is both computationally and time intensive, thus precluding the application of these models to regional or continental scales. This paper introduces a new class of Scalable Spatial Stream Network (S3N) models, which extend nearest-neighbor Gaussian processes to incorporate ecologically relevant spatial dependence while greatly improving computational efficiency. The S3N framework enables scalable modeling of spatial stream networks, demonstrated here for 285 fish species in the Ohio River Basin (>4,000 river km). Validation analyses show that S3N accurately recovers spatial and covariance parameters, even with reduced bias and variance compared to standard SSN implementations. These results represent a key advancement toward large-scale mapping of freshwater fish distributions and quantifying the influence of environmental drivers across extensive river networks.

</details>


### [12] [Meta-analysis of diagnostic test accuracy with multiple disease stages: combining stage-specific and merged-stage data](https://arxiv.org/abs/2512.12065)
*Efthymia Derezea,Nicky J Welton,Gabriel Rogers,Hayley E Jones*

Main category: stat.ME

TL;DR: 提出一种新的元分析方法，能够整合报告合并疾病分期数据的研究，提高疾病分期特异性测试准确性估计的精度。


<details>
  <summary>Details</summary>
Motivation: 临床需要了解测试在不同疾病分期（特别是最易治疗阶段）的检测能力，但现有元分析常因研究仅报告合并分期数据而受限。

Method: 开发了能够同时整合报告分期特异性数据和报告合并分期数据研究的元分析方法，适用于二分类和连续测试结果，通过模拟数据集验证并应用于肝细胞癌筛查测试的元分析。

Result: 新方法能够纳入更多研究，提高估计精度，并在某些情况下纠正因分期特异性数据有限而产生的生物学上不合理结果。

Conclusion: 整合报告合并分期数据的研究能够显著改善疾病分期特异性测试准确性估计，为临床决策提供更可靠证据。

Abstract: For many conditions, it is of clinical importance to know not just the ability of a test to distinguish between those with and without the disease, but also the sensitivity to detect disease at different stages: in particular, the test's ability to detect disease at a stage most amenable to treatment. In a systematic review of test accuracy, pooled stage-specific estimates can be produced using subgroup analysis or meta-regression. However, this requires stage-specific data from each study, which is often not reported. Studies may however report test sensitivity for merged stage categories (e.g. stages I-II) or merged across all stages, together with information on the proportion of patients with disease at each stage. We demonstrate how to incorporate studies reporting merged stage data alongside studies reporting stage-specific data, to allow the inclusion of more studies in the meta-analysis. We consider both meta-analysis of tests with binary results, and meta-analysis of tests with continuous results, where the sensitivity to detect disease of each stage across the whole range of observed thresholds is estimated. The methods are demonstrated using a series of simulated datasets and applied to data from a systematic review of the accuracy of tests used to screen for hepatocellular carcinoma in people with liver cirrhosis. We show that incorporating studies with merged stage data can lead to more precise estimates and, in some cases, corrects biologically implausible results that can arise when the availability of stage-specific data is limited.

</details>


### [13] [Sleep pattern profiling using a finite mixture of contaminated multivariate skew-normal distributions on incomplete data](https://arxiv.org/abs/2512.12464)
*Jason Pillay,Cristina Tortora,Antonio Punzo,Andriette Bekker*

Main category: stat.ME

TL;DR: 提出一种统一模型聚类方法，能同时处理异常值、缺失值和偏斜分布，使用污染多元偏态正态分布建模，无需数据预处理。


<details>
  <summary>Details</summary>
Motivation: 医学数据常包含缺失值、异常值和偏斜特征，传统方法需要预处理（如清洗异常值和缺失值），但这些预处理步骤与聚类阶段的目标函数不同，可能导致次优结果。

Method: 使用污染多元偏态正态分布建模每个簇，该分布是多元偏态正态密度的两成分混合，一个成分代表主要数据（"主体"），另一个捕获潜在异常值。采用EM算法的变体进行最大似然估计。

Result: 模拟研究表明，该方法在聚类准确性和异常值检测方面优于现有方法，适用于低维和高维设置，即使在大量缺失值情况下也表现良好。应用于CCSHS数据集时，无需预处理就识别出5个不同的睡眠者群体。

Conclusion: 提出了一种统一的模型聚类框架，能同时处理医学数据中的典型挑战（异常值、缺失值、偏斜分布），无需单独的数据预处理步骤，在实际应用中表现出色。

Abstract: Medical data often exhibit characteristics that make cluster analysis particularly challenging, such as missing values, outliers, and cluster features like skewness. Typically, such data would need to be preprocessed -- by cleaning outliers and missing values -- before clustering could be performed. However, these preliminary steps rely on objective functions different from those used in the clustering stage. In this paper, we propose a unified model-based clustering approach that simultaneously handles atypical observations, missing values, and cluster-wise skewness within a single framework. Each cluster is modelled using a contaminated multivariate skew-normal distribution -- a convenient two-component mixture of multivariate skew-normal densities -- in which one component represents the main data (the "bulk") and the other captures potential outliers. From an inferential perspective, we implement and use a variant of the EM algorithm to obtain the maximum likelihood estimates of the model parameters. Simulation studies demonstrate that the proposed model outperforms existing approaches in both clustering accuracy and outlier detection, across low- and high-dimensional settings, even in the presence of substantial missingness. The method is further applied to the Cleveland Children's Sleep and Health Study (CCSHS), a dataset characterised by incomplete observations. Without any preprocessing, the proposed approach identifies five distinct groups of sleepers, revealing meaningful differences in sleeper typologies.

</details>


### [14] [Quantile regression with generalized multiquadric loss function](https://arxiv.org/abs/2512.12340)
*Wenwu Gao,Dongyi Zheng,Hanbing Zhu*

Main category: stat.ME

TL;DR: 本文提出了一种基于多二次函数的光滑损失函数来解决分位数回归中的优化问题，使得可以使用梯度下降方法高效求解。


<details>
  <summary>Details</summary>
Motivation: 分位数回归能更全面地分析协变量对响应变量条件分布的影响，但其检验损失函数的不可微性阻碍了基于梯度的优化方法的应用。

Method: 基于多二次函数构造光滑损失函数，将分位数回归转化为全局凸优化问题，可采用Barzilai-Borwein梯度下降法等（随机）梯度下降方法求解。

Result: 在正则性条件下建立了所提估计量的理论结果，并通过蒙特卡洛模拟与其他估计方法进行了比较验证。

Conclusion: 提出的光滑损失函数使得分位数回归能够使用高效的梯度下降方法求解，为解决传统分位数回归优化问题提供了新途径。

Abstract: Quantile regression (QR) is now widely used to analyze the effect of covariates on the conditional distribution of a response variable. It provides a more comprehensive picture of the relationship between a response and covariates compared with classical least squares regression. However, the non-differentiability of the check loss function precludes the use of gradient-based methods to solve the optimization problem in quantile regression estimation. To this end, This paper constructs a smoothed loss function based on multiquadric (MQ) function. The proposed loss function leads to a globally convex optimization problem that can be efficiently solved via (stochastic) gradient descent methods. As an example, we apply the Barzilai-Borwein gradient descent method to obtain the estimation of quantile regression. We establish the theoretical results of the proposed estimator under some regularity conditions, and compare it with other estimation methods using Monte Carlo simulations.

</details>


### [15] [Safe, Always-Valid Alpha-Investing Rules For Doubly Sequential Online Inference](https://arxiv.org/abs/2512.12244)
*Zeyu Yao,Bowen Gang,Wenguang Sun*

Main category: stat.ME

TL;DR: 提出SAVA算法解决动态决策中的在线多重假设检验问题，在数据流和任务流双重连续框架下控制错误选择率并提高统计功效


<details>
  <summary>Details</summary>
Motivation: 在营销、金融、药物研发等快速演化的研究领域中，研究者面临双重连续框架的挑战：高容量数据流持续涌入，新任务间歇出现，需要实时决策。传统方法难以同时处理连续信息处理和新兴任务资源分配

Method: 提出安全且始终有效的Alpha投资规则（SAVA），整合始终有效的p值、e-过程和在线错误发现率方法。该算法跨任务整合信息，缓解alpha死亡问题，在所有决策点控制错误选择率

Result: 通过严格理论分析和大量数值实验验证SAVA框架有效性。结果表明SAVA不仅能有效控制错误选择率，相比传统在线测试方法还显著提高统计功效

Conclusion: SAVA算法为动态决策环境中的在线多重假设检验提供了有效解决方案，能够应对数据流和任务流的双重挑战，在控制错误率的同时提高检测能力

Abstract: Dynamic decision-making in rapidly evolving research domains, including marketing, finance, and pharmaceutical development, presents a significant challenge. Researchers frequently confront the need for real-time action within a doubly sequential framework characterized by the continuous influx of high-volume data streams and the intermittent arrival of novel tasks. This calls for the development and implementation of new online inference protocols capable of handling both the continuous processing of incoming information and the efficient allocation of resources to address emerging priorities. We introduce a novel class of Safe and Always-Valid Alpha-investing (SAVA) rules that leverages powerful tools including always valid p-values, e-processes, and online false discovery rate methods. The SAVA algorithm effectively integrates information across all tasks, mitigates the alpha-death problem, and controls the false selection rate (FSR) at all decision points. We validate the efficacy of the SAVA framework through rigorous theoretical analysis and extensive numerical experiments. Our results demonstrate that SAVA not only offers effective control of the FSR but also significantly improves statistical power compared to traditional online testing approaches.

</details>


### [16] [A Bayesian approach to learning mixtures of nonparametric components](https://arxiv.org/abs/2512.12988)
*Yilei Zhang,Yun Wei,Aritra Guha,XuanLong Nguyen*

Main category: stat.ME

TL;DR: 该论文研究具有非参数混合分量的有限混合模型，采用贝叶斯非参数方法，建立了混合分量可识别性条件，证明了后验收缩率，并开发了高效的MCMC算法。


<details>
  <summary>Details</summary>
Motivation: 传统混合模型通常假设混合分量具有参数核形式，但在许多应用中，对潜在子群体分布做参数假设可能不现实。需要开发能够对混合分量本身进行非参数建模的方法。

Method: 采用贝叶斯非参数建模方法，假设数据总体由有限个潜在分量分布混合生成，每个分量赋予狄利克雷过程混合等贝叶斯非参数先验。建立了混合分量可识别性条件，证明了后验收缩行为，并开发了高效的MCMC算法进行后验推断。

Result: 理论证明分量密度的后验收缩率接近多项式收敛，显著优于通过解卷积估计混合测度的对数收敛率。通过模拟研究和真实数据验证了算法能够高效学习复杂潜在子群体分布。

Conclusion: 该方法能够有效识别和学习复杂非参数混合分量，为异质数据建模提供了更灵活的工具，在理论和实际应用中都取得了显著改进。

Abstract: Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution.

</details>


### [17] [The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework](https://arxiv.org/abs/2512.12394)
*Vladimir Berman*

Main category: stat.ME

TL;DR: 提出基于词素组合结构的单词形成模型，仅通过词素位置槽的概率激活就能解释单词长度分布和齐夫律现象，无需语义或通信效率假设。


<details>
  <summary>Details</summary>
Motivation: 传统解释依赖随机文本或通信效率，但本文试图证明仅通过词素组合的内部结构就能产生语言中的统计规律，为齐夫律等普遍现象提供新的结构解释。

Method: 提出词素组合单词模型：单词由多个位置槽激活形成，每个槽以一定概率激活并从其词素库中选择一个词素。词素被视为稳定的构建块，具有特征性位置。

Result: 模型成功生成真实的单词长度分布（中间集中、长尾薄）和齐夫律排名频率曲线（指数约1.1-1.4），与英语、俄语、罗曼语等真实语言匹配。

Conclusion: 齐夫律行为可以在没有意义、通信压力或优化原则的情况下出现，仅靠形态学的内部结构和位置槽的概率激活就足以产生跨语言的稳健统计模式。

Abstract: We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.

</details>


### [18] [Asymptotic Inference for Constrained Regression](https://arxiv.org/abs/2512.12953)
*Madhav Sankaranarayanan,Yana Hrytsenko,Jerome I. Rotter,Tamar Sofer,Rajarshi Mukherjee*

Main category: stat.ME

TL;DR: 该论文研究高维回归中带仿射约束的统计推断，应用于遗传因素对糖尿病等疾病的效应估计，利用外部蛋白质表达信息作为约束条件。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于利用外部信息（如蛋白质表达水平）来约束遗传因素对疾病（如糖尿病）影响的统计推断。在遗传学研究中，基因对疾病的影响往往通过蛋白质等中介变量实现，因此可以利用基因-蛋白质关系和蛋白质-疾病关系作为先验约束，提高高维回归中的估计精度。

Method: 开发了在参数空间上施加仿射约束的高维回归统计推断方法。讨论了多种候选估计器，在比例渐近框架下研究其理论性质，包括大样本最优性和数值特性。方法利用外部信息（基因-蛋白质关系、蛋白质-疾病关系）作为约束条件。

Result: 建立了带仿射约束的高维回归统计推断的理论框架，证明了估计器的大样本最优性（sharp large sample optimality），并展示了在数值模拟中的良好性能。为利用外部信息约束遗传效应估计提供了严谨的方法论基础。

Conclusion: 该研究为高维回归中利用外部信息（如蛋白质表达）进行约束推断提供了理论和方法基础，特别适用于遗传学中基因对疾病效应的估计问题，能有效提高估计精度和统计推断的可靠性。

Abstract: We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework.

</details>


### [19] [Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture](https://arxiv.org/abs/2512.12289)
*Bingbing Wang,Shengyan Sun,Jiaqi Wang,Yu Tang*

Main category: stat.ME

TL;DR: 提出一个统一的鲁棒回归框架，用于联合检测异常值和概念漂移，通过双通道决策过程和EWMAD-DT检测器区分点异常和漂移类型。


<details>
  <summary>Details</summary>
Motivation: 异常检测和概念漂移检测通常是分开研究的，但在回归问题中，由于输出空间的连续性，区分漂移和异常值具有挑战性，需要联合检测机制。

Method: 提出双通道决策过程：快速响应通道过滤点异常，深度分析通道诊断漂移；开发EWMAD-DT检测器，通过动态阈值自主区分突变漂移和渐进漂移。

Result: 在合成和真实数据集上的综合实验表明，该统一框架在点异常和概念漂移共存时仍表现出优越的检测性能。

Conclusion: 提出的鲁棒回归框架能够有效联合检测异常值和概念漂移，解决了回归中区分这两种现象的挑战，并通过EWMAD-DT检测器增强了漂移类型识别能力。

Abstract: Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist.

</details>


### [20] [Variational Inference for Fully Bayesian Hierarchical Linear Models](https://arxiv.org/abs/2512.12857)
*Cristian Parra-Aldana,Juan Sosa*

Main category: stat.ME

TL;DR: 本文比较了贝叶斯分层线性模型中MCMC与变分推断两种方法，发现变分方法计算快但后验依赖失真，信息准则不稳定，为实际应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯分层线性模型适用于嵌套和聚类数据分析，但传统MCMC方法在高维或大样本场景下计算成本高昂。变分推断和随机变分推断提供了更快的优化替代方案，但在分层结构中，当组间分离较弱时，其准确性存在不确定性。

Method: 本文比较了MCMC和变分推断两种范式，研究了三种模型类别：线性回归模型、分层线性回归模型和聚类分层线性回归模型。通过模拟研究和真实数据应用进行分析。

Result: 变分方法以少量计算时间恢复了全局回归效应和聚类结构，但扭曲了后验依赖性，并产生了不稳定的信息准则值（如WAIC和DIC）。

Conclusion: 研究结果阐明了变分方法何时可以作为MCMC的实际替代方案，何时其局限性使得完全贝叶斯采样成为必要，并为将相同变分框架扩展到广义线性模型和指数族其他成员提供了指导。

Abstract: Bayesian hierarchical linear models provide a natural framework to analyze nested and clustered data. Classical estimation with Markov chain Monte Carlo produces well calibrated posterior distributions but becomes computationally expensive in high dimensional or large sample settings. Variational Inference and Stochastic Variational Inference offer faster optimization based alternatives, but their accuracy in hierarchical structures is uncertain when group separation is weak. This paper compares these two paradigms across three model classes, the Linear Regression Model, the Hierarchical Linear Regression Model, and a Clustered Hierarchical Linear Regression Model. Through simulation studies and an application to real data, the results show that variational methods recover global regression effects and clustering structure with a fraction of the computing time, but distort posterior dependence and yield unstable values of information criteria such as WAIC and DIC. The findings clarify when variational methods can serve as practical surrogates for Markov chain Monte Carlo and when their limitations make full Bayesian sampling necessary, and they provide guidance for extending the same variational framework to generalized linear models and other members of the exponential family.

</details>


### [21] [Asymmetric Laplace distribution regression model for fitting heterogeneous longitudinal response](https://arxiv.org/abs/2512.12362)
*Antoine Barbieri,Angelo Alcaraz,Mouna Abed,Hugues de Courson,Hélène Jacqmin-Gadda*

Main category: stat.ME

TL;DR: 提出基于非对称拉普拉斯分布的混合效应分布回归模型，用于处理纵向数据中的异方差性、异常值和不对称性，并开发新的模型选择准则


<details>
  <summary>Details</summary>
Motivation: 传统混合模型主要关注均值轨迹且假设同方差，但实际纵向数据常存在异方差性、异常值和不对称分布，忽略这些特征会导致建模偏差

Method: 基于非对称拉普拉斯分布的混合效应分布回归模型，采用贝叶斯估计方法，并提出基于分位数的新模型选择准则来比较个体分布与经验分布

Result: 通过模拟研究验证了估计方法和选择准则的有效性，与基于高斯分布的分布回归混合模型和位置-尺度线性分位数混合模型相比表现更好

Conclusion: 该模型能更全面地建模纵向响应变量的分布特征，在ICU患者血压数据分析中展示了实际应用价值

Abstract: The systematic collection of longitudinal data is very common in practice, making mixed models widely used. Most developments around these models focus on modeling the mean trajectory of repeated measurements, typically under the assumption of homoskedasticity. However, as data become increasingly rich through intensive collection over time, these models can become limiting and may introduce biases in analysis. In fact, such data are often heterogeneous, with the presence of outliers, heteroskedasticity, and asymmetry in the distribution of individual measurements. Therefore, ignoring these characteristics can lead to biased modeling results. In this work, we propose a mixed-effect distributional regression model based on the asymmetric Laplace distribution to: (1) address the presence of outliers, heteroskedasticity, and asymmetry in longitudinal measurements; (2) model the entire individual distribution of the heterogeneous longitudinal response over time, rather than just its conditional expectation; and (3) give a more comprehensive evaluation of the impact of covariates on the distribution of the responses through meaningful indicator. A Bayesian estimation procedure is presented. In order to choose between two distributional regression models, we also propose a new model selection criterion for longitudinal data. It measures the proximity between the individual distribution estimated by the model and the empirical individual distribution of the data over time, using a set of quantiles. The estimation procedure and the selection criterion are validated in a simulation study and the proposed model is compared to a distributional regression mixed model based on the Gaussian distribution and a location-scale linear quantile mixed model. Finally, the proposed model is applied to analyze blood pressure over time for hospitalized patients in the intensive care unit.

</details>


### [22] [Automatic Quality Control for Agricultural Field Trials -- Detection of Nonstationarity in Grid-indexed Data](https://arxiv.org/abs/2512.13383)
*Karen Wolf,Pierre Fernique,Hans-Peter Piepho*

Main category: stat.ME

TL;DR: 提出一种自动检验农业田间试验空间平稳性假设的方法，专门针对二维网格结构，能检测均值和方差-协方差的非平稳性


<details>
  <summary>Details</summary>
Motivation: 农业田间试验中常用的空间平稳性假设在实践中常被违反，未考虑的田间效应会导致植物性能估计不准确，从而影响育种者选择最佳品种，延缓育种进展

Method: 基于将田间划分为平稳区域的假设，开发专门针对二维网格结构的自动检验方法，对均值和方差-协方差非平稳性都敏感

Result: 在大量模拟数据集和真实数据上应用，方法能可靠指出存在质量问题的试验，并指示非平稳性的严重程度

Conclusion: 该方法可显著减少手动质量控制时间，提高可靠性，其输出结果可用于改进已进行试验的分析和未来试验的实验设计

Abstract: A common assumption in the spatial analysis of agricultural field trials is stationarity. In practice, however, this assumption is often violated due to unaccounted field effects. For instance, in plant breeding field trials, this can lead to inaccurate estimates of plant performance. Based on such inaccurate estimates, breeders may be impeded in selecting the best performing plant varieties, slowing breeding progress. We propose a method to automatically verify the hypothesis of stationarity. The method is sensitive towards mean as well as variance-covariance nonstationarity. It is specifically developed for the two-dimensional grid-structure of field trials. The method relies on the hypothesis that we can detect nonstationarity by partitioning the field into areas, within which stationarity holds. We applied the method to a large number of simulated datasets and a real-data example. The method reliably points out which trials exhibit quality issues and gives an indication about the severity of nonstationarity. This information can significantly reduce the time spent on manual quality control and enhance its overall reliability. Furthermore, the output of the method can be used to improve the analysis of conducted trials as well as the experimental design of future trials.

</details>


### [23] [A Metadata-Only Feature-Augmented Method Factor for Ex-Post Correction and Attribution of Common Method Variance](https://arxiv.org/abs/2512.13446)
*Murat Yaslioglu*

Main category: stat.ME

TL;DR: 本文提出FAMF-SEM方法，通过问卷元数据构建固定权重的单一方法因子来校正共同方法偏差，无需额外数据或标记变量。


<details>
  <summary>Details</summary>
Motivation: 现有CMV校正方法（如Harman单因子检验、相关独特性、共同潜在因子模型、标记变量法）存在识别能力差、依赖研究者选择、遗漏真实信息或需要特殊标记变量等缺陷，需要更有效的解决方案。

Method: 提出基于元数据的特征增强方法因子（FAMF-SEM）：使用问卷设计特征（反向编码、页面和题目顺序、量表宽度、措辞方向、题目长度）通过岭回归确定固定权重，构建单一方法因子，在模型中保持权重固定。

Result: 开发了AMOS/LISREL友好的无代码Excel工作流程，提供CMV校正结果，并能明确关联问卷设计特征，避免了额外数据需求。

Conclusion: FAMF-SEM方法为CMV问题提供了实用解决方案，克服了现有方法的局限性，建立了问卷设计与方法偏差之间的明确联系。

Abstract: Common Method Variance (CMV) is a recurring problem that reduces survey accuracy. Popular fixes such as the Harman single-factor test, correlated uniquenesses, common latent factor models, and marker variable approaches have well known flaws. These approaches either poorly identify issues, rely too heavily on researchers' choices, omit real information, or require special marker items that many datasets lack. This paper introduces a metadata-only Feature-Augmented Method Factor (FAMF-SEM): a single extra method factor with fixed, item-specific weights based on questionnaire details like reverse coding, page and item order, scale width, wording direction, and item length. These weights are set using ridge regression, based on residual correlations in a basic CFA, and remain fixed in the model. The method avoids the need for additional data or marker variables and provides CMV-adjusted results with clear links to survey design features. An AMOS/LISREL-friendly, no-code Excel workflow demonstrates the method. The paper explains the rationale, provides model details, outlines setup, presents step-by-step instructions, describes checks and reliability tests, and notes limitations.

</details>


### [24] [Design-Based Weighted Regression Estimators for Average and Conditional Spillover Effects](https://arxiv.org/abs/2512.12452)
*Fei Fang,Laura Forastiere*

Main category: stat.ME

TL;DR: 该论文提出了一个统一框架，将各种溢出效应估计量表征为单位间溢出效应的加权平均，并开发了基于设计的加权最小二乘估计器，用于估计平均和条件溢出效应。


<details>
  <summary>Details</summary>
Motivation: 在社交或物理互动中，个体的结果可能受到他人处理的影响（干扰环境）。现有研究缺乏统一的框架来表征各种溢出效应估计量，需要开发适用于此类环境的估计方法。

Method: 提出了基于设计的加权最小二乘（WLS）估计器，引入了三种非参数估计器（二元、发送者、接收者视角），将估计权重分布在结果向量、设计矩阵和权重矩阵中。建立了估计量的渐近性质。

Result: 对于平均型估计量，三种估计器等价于Hajek估计器。对于条件溢出效应，建立了估计量一致性条件。推导了集中不等式、中心极限定理和保守方差估计器。

Conclusion: 该研究提供了一个统一框架来表征和估计干扰环境中的溢出效应，开发了具有良好理论性质的估计方法，适用于集群数量和规模都增长的渐近框架。

Abstract: When individuals engage in social or physical interactions, a unit's outcome may depend on the treatments received by others. In such interference environments, we provide a unified framework characterizing a broad class of spillover estimands as weighted averages of unit-to-unit spillover effects, with estimand-specific weights. We then develop design-based weighted least squares (WLS) estimators for both average and conditional spillover effects. We introduce three nonparametric estimators under the dyadic, sender, and receiver perspectives, which distribute the estimand weights differently across the outcome vector, design matrix, and weight matrix. For the average-type estimands, we show that all three estimators are equivalent to the Hajek estimator. For conditional spillover effects, we establish conditions under which the estimands are consistent for the target conditional spillover effects. We further derive concentration inequalities, a central limit theorem, and conservative variance estimators in an asymptotic regime where both the number of clusters and cluster sizes grow.

</details>


### [25] [Empirical Bayes learning from selectively reported confidence intervals](https://arxiv.org/abs/2512.13622)
*Hunter Chen,Junming Guan,Erik van Zwet,Nikolaos Ignatiadis*

Main category: stat.ME

TL;DR: 开发了一个统计框架，用于从选择性报告的置信区间进行经验贝叶斯学习，应用于解释MEDLINE摘要中的研究结果。该方法处理选择偏差，使用绝对z分数分析信号噪声比分布。


<details>
  <summary>Details</summary>
Motivation: MEDLINE摘要中发表的研究结果存在选择性报告问题（发表偏倚），需要开发统计方法来正确解释这些结果，为个体研究提供背景参考。

Method: 采用选择性倾斜方法扩展经验贝叶斯置信区间到截断抽样机制。使用326,060个MEDLINE摘要的z分数，处理符号不可靠问题，专注于绝对z分数和绝对信号噪声比分布分析。

Result: 框架为包括描述理想化复制的后验估计量和对称后验均值在内的函数提供了覆盖保证，这些估计量在符号等变估计量中是最优的，在诱导相同绝对SNR分布的先验中是极小极大的。

Conclusion: 该统计框架能够处理选择性报告偏差，为解释MEDLINE摘要中的研究结果提供了可靠的经验贝叶斯方法，特别适用于存在发表偏倚的医学文献分析。

Abstract: We develop a statistical framework for empirical Bayes learning from selectively reported confidence intervals, applied here to provide context for interpreting results published in MEDLINE abstracts. A collection of 326,060 z-scores from MEDLINE abstracts (2000-2018) provides context for interpreting individual studies; we formalize this as an empirical Bayes task complicated by selection bias. We address selection bias through a selective tilting approach that extends empirical Bayes confidence intervals to truncated sampling mechanisms. Sign information is unreliable (a positive z-score need not indicate benefit, and investigators may choose contrast directions post hoc), so we work with absolute z-scores and identify only the distribution of absolute signal-to-noise ratios (SNRs). Our framework provides coverage guarantees for functionals including posterior estimands describing idealized replications and the symmetrized posterior mean, which we justify decision-theoretically as optimal among sign-equivariant (odd) estimators and minimax among priors inducing the same absolute SNR distribution.

</details>


### [26] [Robust Variational Bayes by Min-Max Median Aggregation](https://arxiv.org/abs/2512.12676)
*Jiawei Yan,Ju Liu,Weidong Liu,Jiyuan Tu*

Main category: stat.ME

TL;DR: 提出一个鲁棒的变分贝叶斯框架，通过数据分区和稳健聚合处理数据集中的污染和异常值，采用min-max中位数公式增强鲁棒性，并针对局部隐变量存在与否分别处理以保证聚合后验的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理包含污染和异常值的数据集时缺乏鲁棒性，需要开发一个既能有效处理数据污染又能保持统计理论保证的变分贝叶斯框架。

Method: 1) 将数据划分为m个不相交子集；2) 基于稳健聚合原理构建联合优化问题；3) 用min-max中位数公式替代平均KL散度增强鲁棒性；4) 针对有无局部隐变量两种情况分别处理：无局部隐变量时直接聚合，有时采用聚合-重缩放策略；5) 进行两阶段聚合以减少近似误差。

Result: 理论分析表明：两阶段方法比直接聚合m次幂局部后验具有更小的近似误差；提出的后验均值达到近乎最优的统计速率；改进了现有min-max中位数估计器的理论；通过大量模拟研究验证了方法的有效性。

Conclusion: 提出的鲁棒变分贝叶斯框架能有效处理数据污染和异常值，在理论和实际性能上都表现出色，为大规模数据集中的稳健贝叶斯推断提供了新方法。

Abstract: We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies.

</details>


### [27] [Robust tests for parameter change in conditionally heteroscedastic time series models](https://arxiv.org/abs/2512.12946)
*Junmo Song*

Main category: stat.ME

TL;DR: 提出一种针对条件异方差时间序列模型的稳健参数变化检验方法，通过两步法（稳健估计+残差截断）处理异常值，开发了稳健CUSUM检验及其自标准化版本。


<details>
  <summary>Details</summary>
Motivation: 结构变化和异常值经常同时存在，使得统计推断变得复杂。条件异方差时间序列模型中的参数变化检验在异常值存在时面临挑战，需要开发稳健的检验方法。

Method: 采用两步法：1) 稳健估计模型参数；2) 对残差进行截断处理以减轻异常值影响。基于此提出残差稳健CUSUM检验及其自标准化版本。

Result: 推导了所提稳健检验的极限零分布，建立了检验的一致性。模拟结果显示检验对异常值具有很强的稳健性。比特币数据分析展示了实际应用效果。

Conclusion: 提出的稳健检验方法能有效处理条件异方差时间序列模型中的结构变化检测问题，特别是在异常值存在的情况下，具有实际应用价值。

Abstract: Structural changes and outliers often coexist, complicating statistical inference. This paper addresses the problem of testing for parameter changes in conditionally heteroscedastic time series models, particularly in the presence of outliers. To mitigate the impact of outliers, we introduce a two-step procedure comprising robust estimation and residual truncation. Based on this procedure, we propose a residual-based robust CUSUM test and its self-normalized counterpart. We derive the limiting null distributions of the proposed robust tests and establish their consistency. Simulation results demonstrate the strong robustness of the tests against outliers. To illustrate the practical application, we analyze Bitcoin data.

</details>


### [28] [Machine learning to optimize precision in the analysis of randomized trials: A journey in pre-specified, yet data-adaptive learning](https://arxiv.org/abs/2512.13610)
*Laura B. Balzer,Mark J. van der Laan,Maya L. Petersen*

Main category: stat.ME

TL;DR: 论文提出了一种基于机器学习的协变量调整方法（TMLE with Adaptive Pre-specification），用于提高临床试验分析的精确度，并在8个已发表试验中成功应用。


<details>
  <summary>Details</summary>
Motivation: 受SEARCH通用HIV检测与治疗试验（2013-2017）的启发，需要开发一种能够提高试验分析精确度的协变量调整方法。传统方法存在实际应用中的局限性，需要一种能够自适应选择最优估计器组合的方法来最大化经验效率。

Method: 采用目标机器学习估计（TMLE）与自适应预指定相结合的方法。通过样本分割，数据自适应地选择结果回归（给定试验组和协变量的结果条件期望）和已知倾向得分（给定协变量的干预分配条件概率）的估计器组合，以最小化交叉验证方差估计，从而最大化经验效率。

Result: 该方法已在8个最近发表的试验（2022-2024）的主要预指定分析中成功应用，证明了其在实际临床试验中的可行性和有效性。

Conclusion: 论文提供了实用的实施建议，并邀请研究者在未来的试验中采用这种方法进行主要分析。该方法通过机器学习优化协变量调整，能够显著提高临床试验分析的精确度和效率。

Abstract: Covariate adjustment is an approach to improve the precision of trial analyses by adjusting for baseline variables that are prognostic of the primary endpoint. Motivated by the SEARCH Universal HIV Test-and-Treat Trial (2013-2017), we tell our story of developing, evaluating, and implementing a machine learning-based approach for covariate adjustment. We provide the rationale for as well as the practical concerns with such an approach for estimating marginal effects. Using schematics, we illustrate our procedure: targeted machine learning estimation (TMLE) with Adaptive Pre-specification. Briefly, sample-splitting is used to data-adaptively select the combination of estimators of the outcome regression (i.e., the conditional expectation of the outcome given the trial arm and covariates) and known propensity score (i.e., the conditional probability of being randomized to the intervention given the covariates) that minimizes the cross-validated variance estimate and, thereby, maximizes empirical efficiency. We discuss our approach for evaluating finite sample performance with parametric and plasmode simulations, pre-specifying the Statistical Analysis Plan, and unblinding in real-time on video conference with our colleagues from around the world. We present the results from applying our approach in the primary, pre-specified analysis of 8 recently published trials (2022-2024). We conclude with practical recommendations and an invitation to implement our approach in the primary analysis of your next trial.

</details>


### [29] [Clinical transfusion-outcomes research: A practical guide](https://arxiv.org/abs/2512.13155)
*Sarah J Valk,Camila Caram-Deelder,Rolf. H. H. Groenwold,Johanna G van der Bom*

Main category: stat.ME

TL;DR: 比较不同统计方法在输血研究中的表现，发现传统方法可能产生偏倚，而考虑治疗-混杂反馈的加权方法能提供更准确的因果推断。


<details>
  <summary>Details</summary>
Motivation: 临床输血研究面临独特的方法学挑战，包括患者多次输血、每单位血来自不同供者、输血特征受外部因素影响等，这些复杂性使观察性研究中的因果推断变得困难。

Method: 使用男性输血受者的大规模观察数据集，比较限制性分析、时变Cox回归和逆概率加权边际结构模型三种方法，以供者性别和妊娠史与受者死亡率的关系为例。

Result: 限制性分析和传统时变方法显示曾妊娠女性供者红细胞输血与男性供者相比，死亡风险增加（HR 1.22和1.21），而考虑治疗-混杂反馈的逆概率加权分析显示无关联（HR 1.01）。

Conclusion: 传统方法在处理复杂纵向结构时可能产生偏倚估计，需要采用g-方法等先进统计技术来支持临床输血研究中的有效因果推断。

Abstract: Clinical transfusion-outcomes research faces unique methodological challenges compared with other areas of clinical research. These challenges arise because patients frequently receive multiple transfusions, each unit originates from a different donor, and the probability of receiving specific blood product characteristics is influenced by external, often uncontrollable, factors. These complexities complicate causal inference in observational studies of transfusion effectiveness and safety. This guide addresses key challenges in observational transfusion research, with a focus on time-varying exposure, time-varying confounding, and treatment-confounder feedback. Using the example of donor sex and pregnancy history in relation to recipient mortality, we illustrate the strengths and limitations of commonly used analytical approaches. We compare restriction-based analyses, time-varying Cox regression, and inverse probability weighted marginal structural models using a large observational dataset of male transfusion recipients. In the applied example, restriction and conventional time-varying approaches suggested an increased mortality risk associated with transfusion of red blood cells from ever-pregnant female donors compared with male-only donors (hazard ratio [HR] 1.22; 95% CI 1.05-1.42 and HR 1.21; 95% CI 1.04-1.41, respectively). In contrast, inverse probability of treatment and censoring weighted analyses, which account for treatment-confounder feedback, showed no evidence of an association (HR 1.01; 95% CI 0.85-1.20). These findings demonstrate how conventional methods can yield biased estimates when complex longitudinal structures are not adequately handled. We provide practical guidance on study design, target trial emulation, and the use of g-methods, including a reproducible tutorial and example dataset, to support valid causal inference in clinical transfusion research.

</details>


### [30] [Parsimonious Ultrametric Manly Mixture Models](https://arxiv.org/abs/2512.13473)
*Alexa A. Sochaniwsky,Paul D. McNicholas*

Main category: stat.ME

TL;DR: 提出了一种结合Manly变换的简约超度量混合模型家族，用于处理高维偏态数据聚类，通过超度量协方差结构减少参数并揭示变量间的层次关系，同时提出了两步模型选择方法。


<details>
  <summary>Details</summary>
Motivation: 高斯混合模型虽然能处理高维数据，但难以应对常见的偏态分布，且现有方法虽然减少了参数数量，但对聚类结构和可解释性的洞察有限。

Method: 开发了结合Manly变换的简约超度量混合模型家族，利用超度量协方差结构减少自由参数并识别变量间的潜在层次关系，同时提出了两步模型选择程序。

Result: 通过模拟研究和真实数据分析，证明了所提出的两步模型选择方法的有效性，以及该模型家族在聚类性能上的优越表现。

Conclusion: 提出的超度量Manly混合模型家族能够有效处理高维偏态数据，通过超度量协方差结构提高聚类可解释性，两步模型选择方法改进了模型选择效果。

Abstract: A family of parsimonious ultrametric mixture models with the Manly transformation is developed for clustering high-dimensional and asymmetric data. Advances in Gaussian mixture modeling sufficiently handle high-dimensional data but struggle with the common presence of skewness. While these advances reduce the number of free parameters, they often provide limited insight into the structure and interpretation of the clusters. To address this shortcoming, this research implements the extended ultrametric covariance structure and the Manly transformation resulting in the parsimonious ultrametric Manly mixture model family. The ultrametric covariance structure reduces the number of free parameters while identifying latent hierarchical relationships between and within groups of variables. This phenomenon allows the visualization of hierarchical relationships within individual clusters, improving cluster interpretability. Additionally, as with many classes of mixture models, model selection remains a fundamental challenge; a two-step model selection procedure is proposed herein. With simulation studies and real data analyses, we demonstrate improved model selection via the proposed two-step method, and the effective clustering performance for the proposed family.

</details>


### [31] [Actively Learning Joint Contours of Multiple Computer Experiments](https://arxiv.org/abs/2512.13530)
*Shih-Ni Prim,Kevin R. Quinlan,Paul Hawkins,Jagadeesh Movva,Annie S. Booth*

Main category: stat.ME

TL;DR: 提出联合等高线定位(jCL)方法，用于同时定位多个独立计算机实验的预定义响应值交点，比单响应等高线定位方法更高效


<details>
  <summary>Details</summary>
Motivation: 受飞行器旋转扭矩计算机实验启发，需要同时找到多个响应为零的稳定飞行条件，即定位多个独立实验的联合等高线交点

Method: 提出联合等高线定位(jCL)方案，在探索多个响应表面和利用等高线交点学习之间取得平衡；使用浅层和深层高斯过程代理模型，但方法适用于任何能提供后验预测分布的代理

Result: jCL设计显著优于现有的单响应等高线定位策略，能高效定位动机计算机实验的联合等高线

Conclusion: 联合等高线定位方法有效解决了多响应计算机实验的联合等高线定位问题，在探索和利用之间取得了良好平衡

Abstract: Contour location$\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a "joint contour location" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.

</details>


### [32] [A comparative overview of win ratio and joint frailty models for recurrent event endpoints with applications in oncology and cardiology](https://arxiv.org/abs/2512.13629)
*Adrien Orué,Derek Dinart,Laurent Billot,Carine Bellera,Virginie Rondeau*

Main category: stat.ME

TL;DR: 比较两种处理复合终点（复发事件+死亡）的统计方法：联合脆弱模型（JFM）和末次事件辅助复发事件胜率（LWR）。JFM提供分量特异性估计，LWR提供总体治疗效果摘要。JFM在统计功效和样本量估计方面更可靠。


<details>
  <summary>Details</summary>
Motivation: 传统的时间到首次事件分析可能掩盖临床相关信息，需要更合适的统计方法来处理结合复发非致命事件和终点事件的复合终点。

Method: 比较两种统计框架：1）联合脆弱模型（JFM）- 通过共享脆弱性连接复发事件和终点事件的比例风险模型；2）末次事件辅助复发事件胜率（LWR）- 非参数优先配对比较方法。通过模拟研究和实际临床应用评估两种方法。

Result: 两种方法提供互补见解：JFM提供分量特异性估计，LWR提供总体治疗效果摘要。JFM在统计功效方面系统性地优于LWR，是推断和样本量估计最可靠的方法。

Conclusion: JFM是处理复合终点的更可靠方法，特别适合统计推断和样本量估计。LWR需要进一步扩展以适当处理删失和形式化因果估计，这是未来研究的有前景方向。

Abstract: Composite endpoints that combine recurrent non-fatal events with a terminal event are increasingly used in randomized clinical trials, yet conventional time-to-first event analyses may obscure clinically relevant information. We compared two statistical frameworks tailored to such endpoints: the joint frailty model (JFM) and the last-event assisted recurrent-event win ratio (LWR). The JFM specifies proportional hazards for the recurrent and terminal events linked through a shared frailty, yielding covariate-adjusted, component-specific hazard ratios that account for informative recurrences and dependence with death. The LWR is a nonparametric, prioritized pairwise comparison that incorporates all observed events over follow-up and summarizes a population-level benefit of treatment while respecting a pre-specified hierarchy between death and recurrences. We first assessed the performance of the methods using simulations that varied both the gamma-frailty variance and the event rates. Next, we investigated these two frameworks using practical clinical applications, to assess the performance of the methods and to estimate the sample size required to achieve adequate power. These two approaches delivered complementary insights. The JFM provided component-specific estimates, while the LWR led to a summary measure of treatment effect with direction. Power was systematically improved with JFM, which thus appeared as the most reliable approach for inference and sample size estimation. Methodological extensions of the LWR to appropriately handle censoring and to formalize causal estimands remain a promising direction for future research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [33] [Interval Fisher's Discriminant Analysis and Visualisation](https://arxiv.org/abs/2512.11945)
*Diogo Pinheiro,M. Rosário Oliveira,Igor Kravchenko,Lina Oliveira*

Main category: stat.ML

TL;DR: 该论文提出了一种针对区间值数据的多类Fisher判别分析方法，使用Moore区间算术和Mallows距离，并开发了可视化工具来评估分类器性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据分析通常使用单值测量表示实体，而符号数据分析可以处理更复杂的结构如区间和直方图。现有方法需要扩展到区间值数据，以表达内部变异性，并提供更好的分类结果解释工具。

Method: 扩展多类Fisher判别分析到区间值数据，使用Moore区间算术和Mallows距离。将Fisher目标函数推广到同时考虑区间中心和范围，通过数值最大化得到判别方向。开发了类图、轮廓图和堆叠马赛克图等可视化工具来评估分类器性能。

Result: 提出的方法能够有效分类区间值观测，可视化工具提供了对分类器性能和类别成员强度的深入洞察。在实际数据集上的应用证明了该方法的有效性。

Conclusion: 该方法成功将Fisher判别分析扩展到区间值数据，提供了一套完整的分类和可视化框架，增强了区间值数据分类结果的可解释性，在符号数据分析领域具有实用价值。

Abstract: In Data Science, entities are typically represented by single valued measurements. Symbolic Data Analysis extends this framework to more complex structures, such as intervals and histograms, that express internal variability. We propose an extension of multiclass Fisher's Discriminant Analysis to interval-valued data, using Moore's interval arithmetic and the Mallows' distance. Fisher's objective function is generalised to consider simultaneously the contributions of the centres and the ranges of intervals and is numerically maximised. The resulting discriminant directions are then used to classify interval-valued observations.To support visual assessment, we adapt the class map, originally introduced for conventional data, to classifiers that assign labels through minimum distance rules. We also extend the silhouette plot to this setting and use stacked mosaic plots to complement the visual display of class assignments. Together, these graphical tools provide insight into classifier performance and the strength of class membership. Applications to real datasets illustrate the proposed methodology and demonstrate its value in interpreting classification results for interval-valued data.

</details>


### [34] [Hellinger loss function for Generative Adversarial Networks](https://arxiv.org/abs/2512.12267)
*Giovanni Saraceno,Anand N. Vidyashankar,Claudio Agostinelli*

Main category: stat.ML

TL;DR: 提出基于Hellinger距离的GAN损失函数，具有有界性、对称性和鲁棒性，建立了估计量的统计性质，并通过实验验证了其在数据污染下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统GAN训练存在不稳定性和对数据污染的敏感性。Hellinger距离具有有界性、对称性和鲁棒性，适合用于改进GAN训练。

Method: 提出基于Hellinger散度的对抗性目标函数，在一般参数框架下研究其统计性质。建立了估计量的存在性、唯一性、一致性和联合渐近正态性。实现了两种Hellinger型损失函数。

Result: 理论分析表明Hellinger-GAN估计量具有良好的统计性质。实验显示，在数据污染增加的情况下，两种Hellinger损失函数相比传统最大似然型GAN损失，都能提供更好的估计精度和鲁棒性。

Conclusion: Hellinger型损失函数为GAN训练提供了理论保证和实际优势，特别是在数据存在污染的情况下表现出更好的鲁棒性和估计性能。

Abstract: We propose Hellinger-type loss functions for training Generative Adversarial Networks (GANs), motivated by the boundedness, symmetry, and robustness properties of the Hellinger distance. We define an adversarial objective based on this divergence and study its statistical properties within a general parametric framework. We establish the existence, uniqueness, consistency, and joint asymptotic normality of the estimators obtained from the adversarial training procedure. In particular, we analyze the joint estimation of both generator and discriminator parameters, offering a comprehensive asymptotic characterization of the resulting estimators. We introduce two implementations of the Hellinger-type loss and we evaluate their empirical behavior in comparison with the classic (Maximum Likelihood-type) GAN loss. Through a controlled simulation study, we demonstrate that both proposed losses yield improved estimation accuracy and robustness under increasing levels of data contamination.

</details>


### [35] [Towards a pretrained deep learning estimator of the Linfoot informational correlation](https://arxiv.org/abs/2512.12358)
*Stéphanie M. van den Berg,Ulrich Halekoh,Sören Möller,Andreas Kryger Jensen,Jacob von Bornemann Hjelmborg*

Main category: stat.ML

TL;DR: 本文提出了一种基于监督深度学习的连续随机变量互信息估计方法，使用Linfoot信息相关性作为标签，在Gaussian和Clayton copulas上训练，相比传统方法具有更低偏差和方差。


<details>
  <summary>Details</summary>
Motivation: 需要开发更准确、更稳定的互信息估计方法。传统方法如核密度估计、k近邻和神经网络估计器存在偏差和方差问题，特别是在连续随机变量情况下。

Method: 采用监督深度学习框架，使用Linfoot信息相关性（互信息的变换形式）作为训练标签。在Gaussian和Clayton copulas上生成带真实标签的训练数据，构建深度学习模型来估计互信息。

Result: 与核密度估计、k近邻和神经网络估计器相比，该方法表现出更低的偏差和更低的方差，证明了监督深度学习在互信息估计中的有效性。

Conclusion: 监督深度学习是估计互信息的有效方法，未来研究可以扩展到更多copula类型的数据集，以进一步提高模型的泛化能力。

Abstract: We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available.

</details>


### [36] [Co-Hub Node Based Multiview Graph Learning with Theoretical Guarantees](https://arxiv.org/abs/2512.12435)
*Bisakh Banerjee,Mohammad Alwardat,Tapabrata Maiti,Selin Aviyente*

Main category: stat.ML

TL;DR: 提出了一种多视图图学习的协同枢纽节点模型，假设不同视图共享共同的枢纽节点组，通过结构化稀疏性约束来学习多个相关图结构。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设数据均匀且只学习单一图结构，但现实应用中存在多视图异构数据，这些数据包含多个密切相关但不同的图。现有多视图图学习方法主要关注边相似性，忽略了节点层面的共性，特别是不同视图可能共享共同的枢纽节点结构。

Method: 提出协同枢纽节点模型，假设不同视图共享共同的枢纽节点组。通过在这些协同枢纽节点的连接上施加结构化稀疏性约束来构建优化框架。该方法能够同时学习多个相关图结构，并利用节点层面的共性提高学习精度。

Result: 提供了理论分析，包括层可识别性分析和估计误差界确定。在合成图数据和多个受试者的fMRI时间序列数据上验证了方法的有效性，能够准确识别多个密切相关图。

Conclusion: 协同枢纽节点模型能够有效利用多视图数据中节点层面的共性，提高图结构学习的精度和可解释性，为多视图图学习提供了新的理论框架和实用方法。

Abstract: Identifying the graphical structure underlying the observed multivariate data is essential in numerous applications. Current methodologies are predominantly confined to deducing a singular graph under the presumption that the observed data are uniform. However, many contexts involve heterogeneous datasets that feature multiple closely related graphs, typically referred to as multiview graphs. Previous research on multiview graph learning promotes edge-based similarity across layers using pairwise or consensus-based regularizers. However, multiview graphs frequently exhibit a shared node-based architecture across different views, such as common hub nodes. Such commonalities can enhance the precision of learning and provide interpretive insight. In this paper, we propose a co-hub node model, positing that different views share a common group of hub nodes. The associated optimization framework is developed by enforcing structured sparsity on the connections of these co-hub nodes. Moreover, we present a theoretical examination of layer identifiability and determine bounds on estimation error. The proposed methodology is validated using both synthetic graph data and fMRI time series data from multiple subjects to discern several closely related graphs.

</details>


### [37] [Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data](https://arxiv.org/abs/2512.12442)
*Haoyu Li,Isaac J Michaud,Ayan Biswas,Han-Wei Shen*

Main category: stat.ML

TL;DR: 提出一种基于高斯过程回归模型的分层自适应方法，加速水平交叉概率计算，通过区域划分和概率上界估计减少计算复杂度


<details>
  <summary>Details</summary>
Motivation: 科学数据普遍存在不确定性，高斯过程回归模型能有效建模这类数据并减少存储需求，但重建计算复杂，传统不确定性可视化方法（如概率行进立方体）计算成本高，尤其对高分辨率数据

Method: 将数据空间划分为分层数据结构，仅在具有非零概率的区域自适应重建；利用已知GPR核和保存的数据观测，提出新方法高效计算区域内水平交叉概率的上界，基于上界做出细分和重建决策

Result: 实验证明，该方法在不同数据集上计算水平交叉概率场时，值出现概率估计准确且计算成本低

Conclusion: 提出的分层自适应方法能有效加速GPR模型上的水平交叉概率计算，通过概率上界估计实现高效的区域细分和重建决策

Abstract: Almost all scientific data have uncertainties originating from different sources. Gaussian process regression (GPR) models are a natural way to model data with Gaussian-distributed uncertainties. GPR also has the benefit of reducing I/O bandwidth and storage requirements for large scientific simulations. However, the reconstruction from the GPR models suffers from high computation complexity. To make the situation worse, classic approaches for visualizing the data uncertainties, like probabilistic marching cubes, are also computationally very expensive, especially for data of high resolutions. In this paper, we accelerate the level-crossing probability calculation efficiency on GPR models by subdividing the data spatially into a hierarchical data structure and only reconstructing values adaptively in the regions that have a non-zero probability. For each region, leveraging the known GPR kernel and the saved data observations, we propose a novel approach to efficiently calculate an upper bound for the level-crossing probability inside the region and use this upper bound to make the subdivision and reconstruction decisions. We demonstrate that our value occurrence probability estimation is accurate with a low computation cost by experiments that calculate the level-crossing probability fields on different datasets.

</details>


### [38] [Understanding Overparametrization in Survival Models through Double-Descent](https://arxiv.org/abs/2512.12463)
*Yin Liu,Jianwen Cai,Didong Li*

Main category: stat.ML

TL;DR: 该研究首次在生存分析中探索双下降现象，分析了四种代表性生存模型，发现基于似然的损失函数和模型实现共同决定了插值的可行性，且过拟合对生存模型并非良性。


<details>
  <summary>Details</summary>
Motivation: 经典统计学习理论预测测试损失与模型容量呈U型关系（偏差-方差权衡），但现代机器学习发现了更复杂的双下降模式。然而，这种模式在生存分析中尚未被探索，本研究旨在填补这一空白。

Method: 研究四种代表性生存模型：DeepSurv、PC-Hazard、Nnet-Survival和N-MTLR。严格定义插值和有限范数插值这两个关键特征，分析这些模型是否存在（有限范数）插值，并通过数值实验验证理论结果。

Result: 发现基于似然的损失函数和模型实现共同决定了插值的可行性，澄清了生存模型中双下降现象的存在与否，并证明过拟合对生存模型不应被视为良性。

Conclusion: 这是首次在生存分析中系统研究双下降现象，揭示了生存模型独特的泛化行为，为理解生存模型的容量-性能关系提供了新视角。

Abstract: Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models.

</details>


### [39] [Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization](https://arxiv.org/abs/2512.12550)
*Jie Wang*

Main category: stat.ML

TL;DR: 该论文从原始视角研究基于Sinkhorn散度的分布鲁棒优化，将其重构为双层规划问题，提出双循环和单循环采样算法，可同时获得最优鲁棒决策和最坏情况分布。


<details>
  <summary>Details</summary>
Motivation: 现有Sinkhorn DRO研究主要从对偶视角出发，依赖损失函数有界性假设，且难以直接获得最坏情况分布。论文旨在从原始视角解决这些问题，同时获得鲁棒决策和最坏分布，这在压力测试场景生成和鲁棒学习算法设计等实际应用中很有价值。

Method: 将Sinkhorn DRO重构为概率空间上的双层规划问题，包含多个无限维下层子问题。基于此提出双循环和单循环采样算法，具有理论保证。

Result: 提出的算法能够同时获得最优鲁棒决策和最坏情况分布，在对抗性分类的数值研究中验证了方法的有效性。

Conclusion: 从原始视角研究Sinkhorn DRO提供了同时获取鲁棒决策和最坏分布的新途径，提出的采样算法具有理论保证，在对抗性分类等实际应用中表现有效。

Abstract: Distributionally robust optimization (DRO) has emerged as a powerful paradigm for reliable decision-making under uncertainty. This paper focuses on DRO with ambiguity sets defined via the Sinkhorn discrepancy: an entropy-regularized Wasserstein distance, referred to as Sinkhorn DRO. Existing work primarily addresses Sinkhorn DRO from a dual perspective, leveraging its formulation as a conditional stochastic optimization problem, for which many stochastic gradient methods are applicable. However, the theoretical analyses of such methods often rely on the boundedness of the loss function, and it is indirect to obtain the worst-case distribution associated with Sinkhorn DRO. In contrast, we study Sinkhorn DRO from the primal perspective, by reformulating it as a bilevel program with several infinite-dimensional lower-level subproblems over probability space. This formulation enables us to simultaneously obtain the optimal robust decision and the worst-case distribution, which is valuable in practical settings, such as generating stress-test scenarios or designing robust learning algorithms. We propose both double-loop and single-loop sampling-based algorithms with theoretical guarantees to solve this bilevel program. Finally, we demonstrate the effectiveness of our approach through a numerical study on adversarial classification.

</details>


### [40] [Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities](https://arxiv.org/abs/2512.12574)
*Isaac Adjetey,Yiyuan She*

Main category: stat.ML

TL;DR: 提出RLGP模型，通过自适应最近邻选择和稀疏鲁棒机制改进非平稳高斯过程，在间断响应面建模中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在平稳性假设下难以处理响应面的突变和不连续性，现有非平稳扩展方法在边界一致性、异常值敏感性和高维可扩展性方面存在问题

Method: 提出鲁棒局部高斯过程(RLGP)模型，结合自适应最近邻选择和稀疏驱动的鲁棒化机制，采用基于优化的均值偏移调整和多变量视角变换，结合局部邻域建模来减轻异常值影响

Result: 在真实数据集上，RLGP在预测精度和计算效率方面表现优异，特别是在突变和复杂响应结构场景中，高维可扩展性测试也证实了其稳定性和可靠性

Conclusion: RLGP为建模非平稳和不连续响应面提供了一种有效实用的解决方案，适用于广泛的应用领域

Abstract: Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.
  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.
  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications.

</details>


### [41] [Limits To (Machine) Learning](https://arxiv.org/abs/2512.12735)
*Zhimin Chen,Bryan Kelly,Semyon Malamud*

Main category: stat.ML

TL;DR: 机器学习方法在有限样本下存在不可避免的学习极限差距(LLG)，导致模型经验拟合与总体基准之间存在差异，需要修正观察到的预测性能


<details>
  <summary>Details</summary>
Motivation: 机器学习方法虽然灵活，但在有限样本下逼近真实数据生成过程的能力受到根本性约束，需要量化这种不可避免的差距

Method: 提出"学习极限差距"(LLG)作为通用下界，量化模型经验拟合与总体基准之间的不可避免差异，应用于超额收益、收益率、信用利差和估值比率等多种变量

Result: 发现金融数据中隐含的LLG很大，表明标准ML方法可能严重低估真实可预测性；推导了基于LLG的Hansen-Jagannathan边界改进，并展示了LLG如何自然产生超额波动

Conclusion: LLG为理解机器学习在金融应用中的局限性提供了重要框架，揭示了有限样本下不可避免的预测性能差距及其对资产定价和一般均衡设置的影响

Abstract: Machine learning (ML) methods are highly flexible, but their ability to approximate the true data-generating process is fundamentally constrained by finite samples. We characterize a universal lower bound, the Limits-to-Learning Gap (LLG), quantifying the unavoidable discrepancy between a model's empirical fit and the population benchmark. Recovering the true population $R^2$, therefore, requires correcting observed predictive performance by this bound. Using a broad set of variables, including excess returns, yields, credit spreads, and valuation ratios, we find that the implied LLGs are large. This indicates that standard ML approaches can substantially understate true predictability in financial data. We also derive LLG-based refinements to the classic Hansen and Jagannathan (1991) bounds, analyze implications for parameter learning in general-equilibrium settings, and show that the LLG provides a natural mechanism for generating excess volatility.

</details>


### [42] [Transport Reversible Jump Markov Chain Monte Carlo with proposals generated by Variational Inference with Normalizing Flows](https://arxiv.org/abs/2512.12742)
*Pingping Yin,Xiyun Jiao*

Main category: stat.ML

TL;DR: 提出使用变分推断与归一化流（VI-NFs）为可逆跳转MCMC（RJMCMC）生成提案的框架，通过最小化反向KL散度避免昂贵的目标分布采样，实现高效的跨维度贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 传统基于传输的可逆跳转方法需要前向KL最小化和先导MCMC采样，成本高昂。本文旨在开发一种更高效的RJMCMC提案生成方法，避免对目标分布进行采样，同时提供准确的边际似然估计。

Method: 使用基于RealNVP的归一化流学习模型特定的传输映射，最小化反向KL散度（仅需基分布样本）。构建跨模型和模型内提案，通过变分近似提供准确的边际似然估计，支持模型比较和提案自适应。

Result: 在示例问题、因子分析和线性回归变量选择任务上的实验表明，VI-NFs设计的TRJ相比现有基线方法具有更快的混合速度和更高效的模型空间探索能力。

Conclusion: 提出的VI-NFs框架为RJMCMC提供了高效的提案生成方法，避免了昂贵的目标采样，可扩展到条件流实现跨模型的摊销变分推断，代码已开源。

Abstract: We present a framework using variational inference with normalizing flows (VI-NFs) to generate proposals of reversible jump Markov chain Monte Carlo (RJMCMC) for efficient trans-dimensional Bayesian inference. Unlike transport reversible jump methods relying on forward KL minimization with pilot MCMC samples, our approach minimizes the reverse KL divergence which requires only samples from a base distribution, eliminating costly target sampling. The method employs RealNVP-based flows to learn model-specific transport maps, enabling construction of both between-model and within-model proposals. Our framework provides accurate marginal likelihood estimates from the variational approximation. This facilitates efficient model comparison and proposal adaptation in RJMCMC. Experiments on illustrative example, factor analysis and variable selection tasks in linear regression show that TRJ designed by VI-NFs achieves faster mixing and more efficient model space exploration compared to existing baselines. The proposed algorithm can be extended to conditional flows for amortized vairiational inference across models. Code is available at https://github.com/YinPingping111/TRJ_VINFs.

</details>


### [43] [PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders](https://arxiv.org/abs/2512.12905)
*Ruixin Guo,Ruoming Jin,Xinyu Li,Yang Zhou*

Main category: stat.ML

TL;DR: 该论文为线性自编码器（LAEs）在推荐系统中的泛化性能提供了理论分析，提出了多元线性回归的PAC-Bayes边界，并将其扩展到LAEs，通过实验验证了边界的紧致性和与实际排序指标的相关性。


<details>
  <summary>Details</summary>
Motivation: 线性自编码器在推荐系统中表现出色，但其成功主要基于经验观察，缺乏理论理解。作者旨在从统计学习角度研究多元线性回归和线性自编码器的泛化性能，填补理论空白。

Method: 首先将Shalaeva等人的单输出线性回归PAC-Bayes边界扩展到多元线性回归，建立收敛条件。然后将线性自编码器解释为有界数据上的约束多元线性回归模型，使边界适应LAEs。最后开发理论方法提高优化LAE边界的计算效率。

Result: 提出的边界紧致且与Recall@K和NDCG@K等实际排序指标相关性良好。实验证明该方法能在大型模型和真实数据集上进行实际评估。

Conclusion: 该研究为线性自编码器在推荐系统中的泛化性能提供了理论保证，建立了多元线性回归的PAC-Bayes边界框架，并通过计算效率优化使其具有实际应用价值。

Abstract: Linear Autoencoders (LAEs) have shown strong performance in state-of-the-art recommender systems. However, this success remains largely empirical, with limited theoretical understanding. In this paper, we investigate the generalizability -- a theoretical measure of model performance in statistical learning -- of multivariate linear regression and LAEs. We first propose a PAC-Bayes bound for multivariate linear regression, extending the earlier bound for single-output linear regression by Shalaeva et al., and establish sufficient conditions for its convergence. We then show that LAEs, when evaluated under a relaxed mean squared error, can be interpreted as constrained multivariate linear regression models on bounded data, to which our bound adapts. Furthermore, we develop theoretical methods to improve the computational efficiency of optimizing the LAE bound, enabling its practical evaluation on large models and real-world datasets. Experimental results demonstrate that our bound is tight and correlates well with practical ranking metrics such as Recall@K and NDCG@K.

</details>


### [44] [Universality of high-dimensional scaling limits of stochastic gradient descent](https://arxiv.org/abs/2512.13634)
*Reza Gheissari,Aukosh Jagannath*

Main category: stat.ML

TL;DR: 本文研究了高维统计任务中随机梯度下降动态的普适性，证明了在满足特定条件时，SGD动态的ODE极限具有普适性，但也展示了某些情况下的非普适性现象。


<details>
  <summary>Details</summary>
Motivation: 研究高维统计任务中随机梯度下降动态的普适性问题。当数据来自各向同性高斯混合分布时，已知SGD动态会收敛到自治ODE。但需要探究这种ODE极限是否具有普适性，即是否适用于更广泛的分布类别。

Method: 采用理论分析方法，研究高维统计任务中SGD动态的收敛性。通过分析有限族摘要统计量在SGD下的演化，证明当维度、样本量趋于无穷且步长相应趋于0时，动态收敛到自治ODE。主要考察数据来自满足特定矩条件的乘积测度混合分布的情况。

Result: 主要结果：证明了ODE极限的普适性——当数据来自满足前两阶矩匹配相应高斯分布、且初始化和真实向量充分坐标去局域化的乘积测度混合分布时，SGD动态仍收敛到相同的ODE极限。同时证明了两个非普适性结果：1）当初始化坐标对齐时，ODE极限可能非普适；2）围绕ODE固定点的随机微分方程极限不具有普适性。

Conclusion: 高维统计任务中SGD动态的ODE极限在一定条件下具有普适性，但普适性依赖于初始化和真实向量的坐标去局域化条件。同时，更高阶的SDE极限不具有普适性，表明普适性现象存在层次性。

Abstract: We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.

</details>


### [45] [Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory](https://arxiv.org/abs/2512.12911)
*Kohei Nishikawa,Koki Shimizu,Hashiguchi Hiroki*

Main category: stat.ML

TL;DR: 评估SVD低秩近似中奇异值去除阈值，提出基于信号与原始权重矩阵奇异向量余弦相似度的评估指标，比较两种阈值估计方法


<details>
  <summary>Details</summary>
Motivation: 深度学习神经网络权重矩阵的SVD低秩近似中，需要确定合适的阈值来去除噪声相关的奇异值，但现有方法缺乏有效的评估标准来验证阈值选择的合理性

Method: 将权重矩阵建模为信号矩阵和噪声矩阵之和，使用随机矩阵理论确定阈值去除噪声奇异值，提出基于信号矩阵与原始权重矩阵奇异向量余弦相似度的评估指标，通过数值实验比较两种阈值估计方法

Result: 提出了有效的评估指标来验证SVD低秩近似中奇异值去除阈值的合理性，并通过实验比较了不同阈值估计方法的性能

Conclusion: 基于奇异向量余弦相似度的评估指标能够有效评估SVD低秩近似中阈值选择的合理性，为深度学习模型压缩和噪声去除提供了理论依据和实用工具

Abstract: This study evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices. Each weight matrix is modeled as the sum of signal and noise matrices. The low-rank approximation is obtained by removing noise-related singular values using a threshold based on random matrix theory. To assess the adequacy of this threshold, we propose an evaluation metric based on the cosine similarity between the singular vectors of the signal and original weight matrices. The proposed metric is used in numerical experiments to compare two threshold estimation methods.

</details>


### [46] [General OOD Detection via Model-aware and Subspace-aware Variable Priority](https://arxiv.org/abs/2512.13003)
*Min Lu,Hemant Ishwaran*

Main category: stat.ML

TL;DR: 提出一个同时具备模型感知和子空间感知的OOD检测框架，适用于回归和生存分析，通过局部邻域构建和特征优先级嵌入实现有效检测


<details>
  <summary>Details</summary>
Motivation: 分类任务的OOD检测已广泛研究，但回归和生存分析的OOD检测仍有限，主要因为缺乏离散标签和量化预测不确定性的挑战

Method: 使用拟合的预测器为每个测试案例构建局部邻域，强调驱动模型学习关系的特征，降权与预测相关性较低的方向；不依赖全局距离度量或全特征密度估计；使用随机森林实现，其规则结构提供透明邻域和有效评分

Result: 在合成和真实数据基准测试中，针对功能偏移设计的实验显示相比现有方法有持续改进；在食管癌生存研究中，与淋巴结清扫相关的分布偏移识别出与手术指南相关的模式

Conclusion: 该框架适用于多种结果类型，通过模型感知和子空间感知的方法，将变量优先级直接嵌入检测步骤，为回归和生存分析的OOD检测提供了有效解决方案

Abstract: Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.

</details>


### [47] [A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees](https://arxiv.org/abs/2512.13565)
*Junye Du,Zhenghao Li,Zhutong Gu,Long Feng*

Main category: stat.ML

TL;DR: 提出一种基于神经网络和Stein公式的特征选择方法，能在非线性、高维稀疏场景下保证特征选择一致性，无需梯度下降优化。


<details>
  <summary>Details</summary>
Motivation: 解决非线性回归中特征选择的挑战性问题：响应变量y的条件期望是未知非线性函数G(x_S0)，其中S0是相关特征集，G满足温和的光滑性条件。传统方法难以处理这种复杂的非线性特征交互。

Method: 1. 将神经网络重新表述为索引模型；2. 使用二阶Stein公式估计相关特征集S0，避免梯度下降优化；3. 针对高维场景引入筛选-选择机制；4. 在选定特征上重新训练神经网络进行预测。

Result: 1. 保证特征选择一致性，样本量要求n=Ω(p²)；2. 高维稀疏场景下实现非线性选择一致性，样本量要求n=Ω(s log p)；3. 在松弛的稀疏性假设下建立预测性能保证；4. 模拟和真实数据分析显示方法在复杂特征交互下表现优异。

Conclusion: 该方法为非线性特征选择提供了理论保证和实用框架，特别适用于高维稀疏数据和复杂特征交互场景，通过避免梯度下降优化简化了实现并保证了理论性质。

Abstract: This paper tackles the problem of feature selection in a highly challenging setting: $\mathbb{E}(y | \boldsymbol{x}) = G(\boldsymbol{x}_{\mathcal{S}_0})$, where $\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Ω(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Ω(s \log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [48] [Data-driven inverse uncertainty quantification: application to the Chemical Vapor Deposition Reactor Modeling](https://arxiv.org/abs/2512.13354)
*Geremy Loachamín,Eleni D. Koronaki,Dimitrios G. Giovanis,Martin Kathrein,Christoph Czettl,Andreas G. Boudouvis,Stéphane P. A. Bordas*

Main category: stat.OT

TL;DR: 提出贝叶斯框架用于化学气相沉积涂层工艺的逆不确定性量化和参数估计，使用XGBoost代理模型减少采样成本，处理混合数据类型，通过贝叶斯模型选择建立先验分布，采用加权近似贝叶斯计算进行逆不确定性量化，并结合聚类方法聚焦同质生产组分析。


<details>
  <summary>Details</summary>
Motivation: 开发一个基于生产数据的框架，用于化学气相沉积涂层工艺的不确定性量化和参数估计，以改进工业过程控制，同时减少昂贵的采样成本。

Method: 1) 开发XGBoost代理模型将反应器设置参数映射到涂层厚度测量；2) 处理连续、离散整数、二进制和编码分类变量的混合数据；3) 通过贝叶斯模型选择建立参数先验分布；4) 使用带汇总统计的加权近似贝叶斯计算进行逆不确定性量化；5) 采用基于几何嵌入的聚类方法聚焦同质生产组分析。

Result: 建立了一个经过验证的工具，能够提供稳健的参数可信区间，同时过滤多个反应器位置的测量噪声，为改进工业过程控制提供支持。

Conclusion: 该集成方法为在不确定性条件下改进工业过程控制提供了一个有效的工具，通过代理模型降低采样成本，结合贝叶斯分析和聚类技术，实现了对化学气相沉积涂层工艺的逆不确定性量化和参数估计。

Abstract: This study presents a Bayesian framework for (inverse) uncertainty quantification and parameter estimation in a two-step Chemical Vapor Deposition coating process using production data. We develop an XGBoost surrogate model that maps reactor setup parameters to coating thickness measurements, enabling efficient Bayesian analysis while reducing sampling costs. The methodology handles a mixture of data including continuous, discrete integer, binary, and encoded categorical variables. We establish parameter prior distributions through Bayesian Model Selection and perform Inverse Uncertainty Quantification via weighted Approximate Bayesian Computation with summary statistics, providing robust parameter credible intervals while filtering measurement noise across multiple reactor locations. Furthermore, we employ clustering methods guided by geometry embeddings to focus analysis within homogeneous production groups. This integrated approach provides a validated tool for improving industrial process control under uncertainty.

</details>
