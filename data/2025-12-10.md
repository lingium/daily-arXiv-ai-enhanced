<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 16]
- [stat.AP](#stat.AP) [Total: 13]
- [stat.OT](#stat.OT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 14]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions](https://arxiv.org/abs/2512.06270)
*Nifei Lin,Heng Luo,L. Jeff Hong*

Main category: stat.ML

TL;DR: 研究上下文强凸模拟优化，采用"先优化后预测"方法进行实时决策，分析模拟优化算法的不精确性对最优性差距的影响，建立了统一分析框架，并验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了模拟优化算法生成解的不精确性对最优性差距的影响，需要开发一个能同时考虑解偏差和方差的统一分析框架。

Method: 采用"先优化后预测"方法：离线阶段在协变量集上进行模拟优化以近似最优解函数；在线阶段通过评估该近似值获得决策。使用Polyak-Ruppert平均SGD作为模拟优化算法，分析四种平滑技术下的最优性差距。

Result: 建立了收敛速率，推导了计算预算在协变量数量和每个协变量模拟努力之间的最优分配，证明在适当的平滑技术和样本分配规则下，收敛速率可近似达到Γ^{-1}。

Conclusion: 通过数值研究验证了理论发现，证明了所提方法的有效性和实用价值，为上下文模拟优化提供了理论分析框架和实际指导。

Abstract: In this work, we study contextual strongly convex simulation optimization and adopt an "optimize then predict" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $Γ$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $Γ^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.

</details>


### [2] [Statistical analysis of Inverse Entropy-regularized Reinforcement Learning](https://arxiv.org/abs/2512.06956)
*Denis Belomestny,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

TL;DR: 本文提出了一种统计框架解决逆强化学习中的奖励函数非唯一性问题，通过熵正则化和软贝尔曼残差的最小二乘重构，得到唯一确定的最小二乘奖励函数。


<details>
  <summary>Details</summary>
Motivation: 传统逆强化学习存在奖励函数非唯一性问题——许多不同的奖励函数可以诱导出相同的最优策略，这使得逆问题不适定。需要一种统计框架来消除这种模糊性。

Method: 结合熵正则化和软贝尔曼残差的最小二乘重构，得到唯一的最小二乘奖励函数。将专家演示建模为马尔可夫链，通过惩罚最大似然估计在动作空间的条件分布类中估计策略。

Result: 建立了估计策略与专家策略之间超额KL散度的高概率界，通过策略类的覆盖数考虑统计复杂性。得到了最小二乘奖励函数的非渐近极小极大最优收敛率，揭示了平滑性（熵正则化）、模型复杂性和样本量之间的相互作用。

Conclusion: 该框架解决了逆强化学习的奖励函数非唯一性问题，建立了行为克隆、逆强化学习和现代统计学习理论之间的桥梁，提供了统计保证和收敛率分析。

Abstract: Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $π^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.

</details>


### [3] [Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders](https://arxiv.org/abs/2512.06348)
*Xiaoyu Ma,Likun Zhang,Christopher K. Wikle*

Main category: stat.ML

TL;DR: 提出条件变分自编码器(cXVAE)模型，整合气候指数来建模时空极端事件，通过卷积神经网络在解码器中结合气候变量与空间依赖性，用于分析极端天气事件的时空共现模式。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件在农业、生态和气象等领域广泛研究，其时空共现在气候变化条件下可能增强或减弱。需要一种能够整合气候指数、建模时空极端事件依赖结构的方法，以分析气候条件变化对极端事件的影响。

Method: 提出条件变分自编码器(cXVAE)模型，在解码器中嵌入卷积神经网络(CNN)，将气候指数与潜在空间中的空间依赖性进行卷积，使解码器依赖于气候变量。该方法能够模拟空间场、恢复时空变化的极端依赖性。

Result: 通过模拟验证cXVAE能准确模拟空间场并以极低计算成本恢复时空极端依赖性；提供检测条件驱动变化的方法；当依赖性对条件敏感时，支持反事实实验，通过干预气候协变量来量化联合尾部风险、共现范围和重现期指标的差异。

Conclusion: 将cXVAE应用于2014-2024年澳大利亚东部月最大火灾天气指数(FWI)分析，以ENSO指数为条件，展示了模型在实际场景中的实用性和性能。该方法为分析气候条件变化对极端事件时空共现的影响提供了有效工具。

Abstract: Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index.

</details>


### [4] [Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals](https://arxiv.org/abs/2512.06435)
*Mara Sherlin Talento,Jordan Richards,Raphael Huser,Hernando Ombao*

Main category: stat.ML

TL;DR: 提出了一种基于尾部极值依赖的脑信号分析方法，通过尾部连接性特征改进癫痫发作识别和风险管理的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统尾部依赖建模方法无法识别驱动最大尾部依赖的具体通道，而这对癫痫患者脑电图分析很重要，因为特定通道负责癫痫发作。

Method: 将传统信号处理中的典型相关分析扩展到尾部，开发尾部典型依赖度量，通过尾部成对依赖矩阵（TPDM）构建计算高效的估计器。

Result: 尾部连接性提供了额外的判别能力，能够更准确地识别极端相关事件，改进癫痫发作风险管理，并成功用于新生儿基于频率的软聚类分析。

Conclusion: 尾部依赖分析揭示了极端事件（如癫痫发作）的独特特征，尾部典型依赖度量能够可视化极端通道贡献，为癫痫诊断和风险管理提供了有效工具。

Abstract: We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.

</details>


### [5] [Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions](https://arxiv.org/abs/2512.06615)
*Kaichen Shen,Wei Zhu*

Main category: stat.ML

TL;DR: 提出LNDSM方法，将非线性前向动力学与基于VAE的潜在SGM框架结合，通过欧拉-丸山方案近似高斯转移重构交叉熵项，实现更快的合成和更好的分布学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的生成模型（SGM）在潜在空间中通常使用线性前向动力学，这限制了其对结构化分布的学习能力。需要一种方法能够结合非线性动力学，同时保持数值稳定性。

Method: 提出潜在非线性去噪分数匹配（LNDSM），将非线性前向动力学整合到基于VAE的潜在SGM框架中。通过欧拉-丸山方案诱导的近似高斯转移来重构交叉熵项，并识别和移除两个零均值但方差爆炸的项以确保数值稳定性。

Result: 在MNIST数据集变体上的实验表明，该方法实现了更快的合成速度，并增强了对固有结构化分布的学习能力。与基准的结构不可知潜在SGM相比，LNDSM始终获得更优的样本质量和多样性。

Conclusion: LNDSM成功地将非线性前向动力学整合到潜在SGM框架中，通过数值稳定的训练目标实现了更好的生成性能，为结构化分布学习提供了有效解决方案。

Abstract: We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.

</details>


### [6] [ADAM Optimization with Adaptive Batch Selection](https://arxiv.org/abs/2512.06795)
*Gyu Yeol Kim,Min-hwan Oh*

Main category: stat.ML

TL;DR: 提出AdamCB，一种将组合多臂老虎机采样集成到Adam优化器中的方法，通过自适应选择有影响力的样本提高收敛效率


<details>
  <summary>Details</summary>
Motivation: 传统Adam优化器对所有样本平等对待，但不同样本对模型更新的影响程度不同，导致收敛效率低下。现有的老虎机采样方法理论保证有限，需要改进

Method: 提出AdamCB（Adam with Combinatorial Bandit Sampling），将组合多臂老虎机技术集成到Adam优化器中，能够同时利用多个样本的反馈信息

Result: 理论分析显示AdamCB比包括先前老虎机变体在内的Adam方法收敛更快；数值实验证明AdamCB在性能上持续优于现有方法

Conclusion: AdamCB通过组合老虎机采样有效解决了传统Adam优化器的收敛效率问题，在理论和实践上都表现出优越性能

Abstract: Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.

</details>


### [7] [Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets](https://arxiv.org/abs/2512.06945)
*Nabil Alami,Jad Zakharia,Souhaib Ben Taieb*

Main category: stat.ML

TL;DR: SACP是一种新颖的对称聚合共形预测方法，通过将多个预测模型的非共形分数转换为e值并使用对称聚合函数组合，生成更精确的预测集。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，针对同一任务训练多个预测模型越来越普遍，但如何聚合这些模型的预测不确定性以产生可靠且高效的量化仍然是一个未充分探索的挑战。虽然共形预测方法可以从每个模型生成单独的预测集，但将它们组合成单个更信息丰富的集合仍然是一个难题。

Method: 提出SACP方法，将多个预测器的非共形分数转换为e值，然后使用任何对称聚合函数组合这些e值。这种灵活设计支持数据驱动的聚合策略选择，以获得更锐利的预测集。

Result: 在多样化数据集上的广泛实验表明，SACP在效率上持续改进，并且通常优于最先进的模型聚合基线方法。

Conclusion: SACP为多模型不确定性聚合提供了一个稳健的数据驱动框架，通过理论分析和实证验证展示了其在生成更精确预测集方面的有效性。

Abstract: Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.

</details>


### [8] [PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios](https://arxiv.org/abs/2512.06950)
*Enrico Camporeale*

Main category: stat.ML

TL;DR: PARIS提出了一种基于表示定理的数据集剪枝框架，通过计算闭式表示器删除残差来优化训练集，有效解决不平衡回归问题，在空间天气预测中减少75%训练数据的同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统经验风险最小化(ERM)在不平衡回归中偏向高频数据区域，导致对罕见但重要的"尾部"事件预测性能严重下降。现有方法如损失重加权或合成过采样会引入噪声、扭曲数据分布或增加算法复杂度。

Method: PARIS利用神经网络表示定理计算闭式表示器删除残差，量化单个训练点移除对验证损失的精确影响而无需重新训练。结合高效的Cholesky秩一降阶方案，实现快速迭代剪枝，消除无信息或性能下降的样本。

Result: 在真实空间天气预测任务中，PARIS能够减少高达75%的训练集，同时保持或改善整体RMSE性能，优于重加权、合成过采样和提升基准方法。

Conclusion: 表示器引导的数据集剪枝是一种强大、可解释且计算高效的方法，能够有效处理罕见事件回归问题，为不平衡回归提供了新的解决方案。

Abstract: The challenge of \textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.
  We introduce \textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.
  We use a real-world space weather example, where PARIS reduces the training set by up to 75\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.

</details>


### [9] [Learning Conditional Independence Differential Graphs From Time-Dependent Data](https://arxiv.org/abs/2512.06960)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: 提出一种基于频率域的惩罚D-trace损失函数方法，用于估计两个时间序列高斯图模型的条件独立图差异，考虑了数据的时间依赖性，优于现有的i.i.d.建模方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注独立同分布数据的精度矩阵差异估计，但实际应用中数据往往具有时间依赖性。本文旨在估计两个时间序列高斯图模型的逆功率谱密度差异，以更准确地刻画时间依赖数据中条件依赖关系的变化。

Method: 使用惩罚D-trace损失函数方法在频率域进行差分图学习，采用Wirtinger微积分。考虑凸（群lasso）和非凸（log-sum和SCAD群惩罚）惩罚函数，提出ADMM算法优化目标函数。

Result: 在高维设置下建立了逆功率谱密度在Frobenius范数下收敛到真值的充分条件和图恢复条件。合成数据实验显示，log-sum惩罚的差分时间序列图估计器显著优于lasso方法，而lasso方法又显著优于现有的i.i.d.建模方法（以F1分数为评价指标）。

Conclusion: 提出的方法能够有效估计时间序列数据的条件独立图差异，考虑了数据的时间依赖性，在合成和真实数据中都表现出良好性能，特别是非凸惩罚方法优于凸惩罚方法。

Abstract: Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.

</details>


### [10] [Exact Synthetic Populations for Scalable Societal and Market Modeling](https://arxiv.org/abs/2512.07306)
*Thierry Petit,Arnault Pachot*

Main category: stat.ML

TL;DR: 提出一个基于约束规划的合成人口生成框架，能够精确复制目标统计特征并保持个体一致性，无需微观数据


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法需要从样本推断分布，无法精确控制人口统计特征。需要一种能够直接编码聚合统计和结构关系的方法，在不使用个人数据的情况下生成高质量合成人口

Method: 采用约束规划框架，直接编码目标统计数据和结构关系，确保生成的合成人口完全符合预设的统计特征，同时保持个体层面的一致性

Result: 在官方人口统计数据上验证了方法的有效性，能够高精度复制目标统计特征，并研究了分布偏差对下游分析的影响

Conclusion: 该方法为无需个人数据的决策支持提供了新途径，通过合成人口可以模拟社会行为、探索市场政策场景，为可重复的决策级洞察提供基础

Abstract: We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.

</details>


### [11] [Machine learning in an expectation-maximisation framework for nowcasting](https://arxiv.org/abs/2512.07335)
*Paul Wilsens,Katrien Antonio,Gerda Claeskens*

Main category: stat.ML

TL;DR: 提出一个基于EM框架的nowcasting方法，使用机器学习技术建模事件发生和报告过程，支持高维协变量，在非线性场景下优于传统GLM方法。


<details>
  <summary>Details</summary>
Motivation: 决策常面临信息不完整问题，导致风险估计偏差。实践中信息不完整常由报告或观察延迟引起，需要利用可观测信息推断完整信息（nowcasting）。现有方法在处理高维协变量和非线性效应时存在局限。

Method: 提出基于期望最大化（EM）的nowcasting框架，使用神经网络和XGBoost等机器学习技术建模事件发生和报告过程。支持包含事件发生期和报告期的协变量信息以及实体特征。定制化EM迭代中的最大化步骤和信息流以利用机器学习模型的预测能力。

Result: 模拟实验显示能有效建模高维协变量下的事件发生和报告过程。在非线性效应存在时，该方法优于使用广义线性模型（GLM）的现有EM-based nowcasting框架。在阿根廷新冠病例报告应用中，XGBoost方法表现最佳。

Conclusion: 提出的EM框架结合机器学习技术能有效处理nowcasting问题，特别是在高维协变量和非线性效应场景下，XGBoost方法表现优异，为实际应用提供了更准确的预测工具。

Abstract: Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.

</details>


### [12] [High-Dimensional Change Point Detection using Graph Spanning Ratio](https://arxiv.org/abs/2512.07541)
*Youngwen Sun,Katerina Papagiannouli,Vladimir Spokoiny*

Main category: stat.ML

TL;DR: 提出一种基于图的新颖图跨越算法，用于检测低维到高维数据的离线/在线变化，适用于未知分布的欧几里得和图结构数据，能控制错误概率，在变化幅度超过最小最大分离率下界时具有高检测能力。


<details>
  <summary>Details</summary>
Motivation: 需要一种适用于多种数据类型（欧几里得和图结构）且分布未知的通用变化检测方法，特别是在在线环境中需要及时准确检测变化，同时要控制错误概率。

Method: 基于图的方法论，提出新颖的图跨越算法，能够处理低维到高维数据，适用于离线（批量）和在线（流式）场景，对数据分布没有特定假设。

Result: 算法在变化幅度超过最小最大分离率下界（量级为√(nd)）时具有高检测能力，在Gaussian和非Gaussian数据上都比其他方法更准确，即使在小观测窗口下也能保持强检测能力。

Conclusion: 该图跨越算法是一种通用、高效的变化检测方法，特别适合在线环境，能够在保持错误概率控制的同时实现及时准确的变化检测。

Abstract: Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.

</details>


### [13] [On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series](https://arxiv.org/abs/2512.07557)
*Jitendra K. Tugnait*

Main category: stat.ML

TL;DR: 提出了一种基于频域惩罚对数似然的高维多属性时间序列条件独立图估计方法，支持凸和非凸惩罚函数，建立了无需不可表示性条件的理论保证


<details>
  <summary>Details</summary>
Motivation: 现有图估计方法主要针对单属性时间序列（每个节点对应一个标量时间序列），而多属性图模型中每个节点代表一个随机向量或向量时间序列，需要新的方法来处理这种更复杂的数据结构

Method: 基于频域惩罚对数似然方法，使用时域数据的离散傅里叶变换，考虑凸惩罚（稀疏组lasso）和非凸惩罚（log-sum和SCAD组惩罚），通过贝叶斯信息准则选择调优参数

Result: 在高维设置下建立了充分条件，证明了逆功率谱密度在Frobenius范数下的收敛性、非凸惩罚时的局部凸性以及图恢复能力，且无需不可表示性条件

Conclusion: 提出的多属性图学习方法在理论和实证上均表现良好，能够有效处理高维依赖时间序列的图结构估计问题

Abstract: Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.

</details>


### [14] [$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations](https://arxiv.org/abs/2512.07578)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: stat.ML

TL;DR: φ-test是一种结合Shapley归因与选择性推断的全局特征选择和显著性检验方法，用于黑盒预测模型的特征重要性分析。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够为黑盒预测模型提供全局特征重要性分析的方法，将Shapley重要性总结与经典统计推断联系起来，同时保持模型的预测能力。

Method: 结合SHAP引导的特征筛选和选择性推断：1) 使用SHAP进行特征筛选；2) 通过具有可处理选择性推断形式的选择规则在筛选特征上拟合线性代理模型；3) 为每个保留特征输出Shapley全局分数、代理系数、后选择p值和置信区间。

Result: 在真实表格回归任务中，φ-test能够保留原始模型的大部分预测能力，同时仅使用少量特征，且特征集在不同重采样和骨干模型类别之间保持相对稳定。

Conclusion: φ-test作为一种实用的全局解释层，成功地将Shapley重要性总结与经典统计推断联系起来，为黑盒模型提供了可靠的特征重要性分析框架。

Abstract: We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.

</details>


### [15] [Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion](https://arxiv.org/abs/2512.07755)
*Brenda Anague,Bamdad Hosseini,Issa Karambal,Jean Medard Ngnotchouye*

Main category: stat.ML

TL;DR: 提出一种基于物理信息神经网络（PINNs）的加权自适应方法，用于解决二维和三维对流扩散方程中的源反演和参数估计问题，能够同时恢复解、源项及未知参数。


<details>
  <summary>Details</summary>
Motivation: 在大气科学和环境监测领域，从稀缺数据中同时估计排放源位置以及控制速度剖面和扩散参数的多个模型参数是一个困难任务。传统方法难以处理这种高度不适定问题。

Method: 基于神经正切核的加权自适应方法，将PINNs扩展到源反演和参数估计问题。通过将底层偏微分方程作为约束条件，联合恢复解、源项和未知参数，更有效地利用测量中的有限信息。

Result: 在各种数值实验中，使用不同类型的测量数据（模拟实际工程系统）表明，该方法成功且对测量中的额外噪声具有鲁棒性。

Conclusion: 该方法成功解决了高度不适定的源反演和参数估计问题，展示了PINNs在解决复杂工程和科学计算问题中的灵活性和通用性。

Abstract: Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.

</details>


### [16] [Distribution-informed Online Conformal Prediction](https://arxiv.org/abs/2512.07770)
*Dongjian Hu,Junxi Wu,Shu-Tao Xia,Changliang Zou*

Main category: stat.ML

TL;DR: COP是一种在线共形预测算法，通过整合数据模式来生成更紧凑的预测集，同时保持有效覆盖率保证


<details>
  <summary>Details</summary>
Motivation: 现有在线共形预测方法在完全对抗环境中处理数据分布偏移时会产生过于保守的预测集，需要一种能利用数据模式来生成更紧凑预测集的方法

Method: 提出Conformal Optimistic Prediction (COP)算法，通过非一致性分数的累积分布函数估计，将底层数据模式整合到更新规则中

Result: COP在存在可预测模式时能产生更紧凑的预测集，即使估计不准确也能保持有效覆盖率保证；实验显示COP能实现有效覆盖率并构建比其他基线更短的预测区间

Conclusion: COP算法成功整合数据模式到在线共形预测中，在保持覆盖率保证的同时生成更紧凑的预测集，为不确定性量化提供了更高效的解决方案

Abstract: Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [17] [Spatial Analysis for AI-segmented Histopathology Images: Methods and Implementation](https://arxiv.org/abs/2512.06116)
*Y. Park,F. Wu,X. Feng,S. Yang,E. H. Wang,B. Yao,C. Moon,G. Xiao,Q. Li*

Main category: stat.AP

TL;DR: SASHIMI是一个基于浏览器的工具，用于实时分析AI分割的组织病理学图像中的细胞空间组织，通过多种空间统计和拓扑特征量化细胞分布模式，并在口腔癌前病变和肺癌数据集中识别出与患者生存显著相关的空间特征。


<details>
  <summary>Details</summary>
Motivation: 量化细胞空间组织及其相互作用对于理解癌症进展和免疫反应至关重要。虽然AI技术已能大规模分割和分类细胞核，但用于分析这种复杂空间组织的工具仍然有限。

Method: 首先回顾了27种传统空间统计方法，然后开发了SASHIMI工具，计算包括空间统计、邻近度度量、网格级相似性指数、空间自相关度量和拓扑描述符在内的综合特征集，用于量化细胞丰度和细胞间相互作用。

Result: 在口腔癌前病变和非小细胞肺癌两个癌症数据集中，SASHIMI识别出多个与患者生存结果显著相关的空间特征，验证了其在肿瘤形态结构分析中的有效性。

Conclusion: SASHIMI为肿瘤形态结构的单细胞水平空间分析提供了一个可访问且可重复的平台，为跨癌症类型的组织组织定量探索提供了稳健框架。

Abstract: Quantitatively characterizing the spatial organization of cells and their interaction is essential for understanding cancer progression and immune response. Recent advances in machine intelligence have enabled large-scale segmentation and classification of cell nuclei from digitized histopathology slides, generating massive point pattern and marked point pattern datasets. However, accessible tools for quantitative analysis of such complex cellular spatial organization remain limited. In this paper, we first review 27 traditional spatial summary statistics, areal indices, and topological features applicable to point pattern data. Then, we introduce SASHIMI (Spatial Analysis for Segmented Histopathology Images using Machine Intelligence), a browser-based tool for real-time spatial analysis of artificial intelligence (AI)-segmented histopathology images. SASHIMI computes a comprehensive suite of mathematically grounded descriptors, including spatial statistics, proximity-based measures, grid-level similarity indices, spatial autocorrelation measures, and topological descriptors, to quantify cellular abundance and cell-cell interaction. Applied to two cancer datasets, oral potentially malignant disorders (OPMD) and non-small-cell lung cancer (NSCLC), SASHIMI identified multiple spatial features significantly associated with patient survival outcomes. SASHIMI provides an accessible and reproducible platform for single-cell-level spatial profiling of tumor morphological architecture, offering a robust framework for quantitative exploration of tissue organization across cancer types.

</details>


### [18] [Mode Choice Heterogeneity Among Zero-Vehicle Households: A Latent Class Cluster Approach](https://arxiv.org/abs/2512.06127)
*Nancy Kasamala,Arthur Mukwaya,Nana Kankam Gyimah,Judith Mwakalonge,Gurcan Comert,Saidi Siuhi,Akinbobola Jegede*

Main category: stat.AP

TL;DR: 该研究使用加权潜在类别聚类分析识别了零车辆家庭中的三种不同出行模式类别，打破了将零车辆家庭视为同质群体的传统假设。


<details>
  <summary>Details</summary>
Motivation: 传统交通规划将零车辆家庭视为同质群体，假设他们主要依赖步行或公共交通，忽视了不同出行需求和人口特征下的多样化出行策略。

Method: 使用2022年全国家庭出行调查数据，应用加权潜在类别聚类分析，以出行模式和出行目的为指标，以人口、经济和建成环境变量为协变量。

Result: 识别出三个潜在类别：共享出行通勤者（36.3%），主要使用公交和网约车通勤和进行必要活动；依赖汽车购物者（29.9%），依赖非正式车辆进行较长距离的自由出行；活跃出行购物者（33.8%），依赖步行或骑行进行短距离本地购物出行。

Conclusion: 这些行为发现使政策制定者能够根据不同地理和人口背景下零车辆家庭各细分群体的具体需求，制定差异化的规划解决方案。

Abstract: In transportation planning, Zero-Vehicle Households (ZVHs) are often treated as a uniform group with limited mobility options and assumed to rely heavily on walking or public transit. However, such assumptions overlook the diverse travel strategies ZVHs employ in response to varying trip needs and sociodemographic factors. This study addresses this gap by applying a weighted Latent Class Cluster Analysis (LCCA) to data from the 2022 National Household Travel Survey (NHTS) to uncover distinct mobility patterns within the ZVH population. Using travel mode and trip purpose as indicators and demographic, economic, and built environment variables as covariates, we identified three latent classes :Shared mobility errand workers (36.3%), who primarily use transit and ridehailing for commuting and essential activities; car based shoppers (29.9%), who depend on informal vehicle access for longer discretionary trips and active travel Shoppers (33.8%), who rely on walking or cycling for short, local shopping oriented travel. These behavioral findings enable policymakers to develop differentiated planning solutions to the specific needs of each segment among the ZVHs population across varied geographic and demographic settings.

</details>


### [19] [Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict](https://arxiv.org/abs/2512.06210)
*Daniel Mittermaier,Tobias Bohne,Martin Hofer,Daniel Racek*

Main category: stat.AP

TL;DR: 提出一个量化冲突预测不确定性的方法，从点预测转向完整预测分布，在PRIO-GRID月度数据上评估，性能优于历史基准但受零膨胀数据特性影响


<details>
  <summary>Details</summary>
Motivation: 暴力冲突死亡预测存在高度不确定性，限制了实际应用价值。需要量化预测不确定性，改进传统点预测方法

Method: 开发量化冲突预测不确定性的策略，比较和组合多种基于树的分类器和分布回归器，采用自定义auto-ML设置，为每个网格单元单独估计分布，并测试空间集成中的区域模型集成

Result: 模型在提前一年的预测中持续优于基于冲突历史的基准套件，性能在观察到冲突的区域表现更好。区域模型集成虽未提升预测性能但也不降低性能

Conclusion: 需要理解指标在特定预测问题中的行为（如极端零膨胀特性），区域模型集成为未来整合空间覆盖较少的数据源提供了可能性

Abstract: Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future.

</details>


### [20] [Spatio-temporal Shared-Field Modeling of Beluga and Bowhead Whale Sightings Using a Joint Marked Log-Gaussian Cox Process](https://arxiv.org/abs/2512.06450)
*Mauli Pant,Linda Fernandez,Indranil Sahoo*

Main category: stat.AP

TL;DR: 开发了一个多物种对数高斯Cox过程模型，结合共享潜在空间场和物种特定协变量，用于联合分析2010-2019年美国北极地区白鲸和弓头鲸的时空分布和群体大小。


<details>
  <summary>Details</summary>
Motivation: 在数据稀疏且调查环境异质的北极地区，需要开发能够同时建模多个鲸鱼物种分布的方法，以更好地理解它们的空间格局和生态关系。

Method: 使用多物种对数高斯Cox过程模型，通过共享潜在空间高斯场连接物种特定强度表面；采用SPDE方法和各向异性Matern协方差在海洋约束三角网格上表示潜在场；通过标记点过程扩展结合物种特定的负二项分布标记来建模群体大小；使用INLA进行高效推理。

Result: 模型成功识别了白鲸和弓头鲸的持久性多物种热点区域，并揭示了每个物种独特的环境关联，证明了共享场LGCP在数据稀疏环境中的有效性。

Conclusion: 共享潜在场LGCP框架为在异质调查环境中进行联合物种分布建模提供了有力工具，特别适用于数据稀疏的北极海洋生态系统研究。

Abstract: We analyze a decade of aerial survey whale sighting data (2010-2019) to model the spatio-temporal distributions and group sizes of beluga (Delphinapterus leucas) and bowhead (Balaena mysticetus) whales in the United States Arctic. To jointly model these species, we develop a multi-species Log-Gaussian Cox Process (LGCP) in which species specific intensity surfaces are linked through a shared latent spatial Gaussian field. This structure allows the model to capture broad spatial patterns common to both species while still accommodating species level responses to environmental covariates and seasonal variation. The latent field is represented using the Stochastic Partial Differential Equation (SPDE) approach with an anisotropic Matern covariance, implemented on an ocean constrained triangulated mesh so that spatial dependence aligns with marine geography. Whale group size is incorporated through a marked point process extension with species specific negative binomial marks, allowing occurrence and group sizes to be jointly analyzed within a unified framework. Inference is carried out using the Integrated Nested Laplace Approximation (INLA), enabling efficient model fitting over a decade of survey effort. The results highlight persistent multi-species hotspots and distinct environmental associations for each species, demonstrating the value of shared field LGCPs for joint species distribution modeling in data sparse and heterogeneous survey settings.

</details>


### [21] [Big shells, bigger data: cohort analysis of Chesapeake Bay Crassostrea virginica reefs](https://arxiv.org/abs/2512.07080)
*Madison D. Griffin,Grace S. Chiu,Roger L. Mann,Melissa J. Southworth,John K. Thomas*

Main category: stat.AP

TL;DR: 开发基于高斯混合模型的新方法，利用壳长数据识别牡蛎年龄组和估计寿命，发现弗吉尼亚切萨皮克湾牡蛎在2010年代中后期比2000年代初期表现出更强的恢复力。


<details>
  <summary>Details</summary>
Motivation: 弗吉尼亚切萨皮克湾牡蛎礁存在"年龄截断"现象，可能由过度捕捞、疾病爆发、环境退化和气候变化共同导致。虽然研究表明牡蛎对环境压力具有恢复力，但这些证据基于当前对牡蛎寿命的有限理解。VOSARA数据集尚未在恢复力背景下进行全面分析。

Method: 开发基于高斯混合模型的新方法：1) 对每个年份、每个礁石拟合单变量GMM，估计每个年龄组的平均壳长和标准差以及混合比例；2) 开发机制算法跨时间连接年龄组推断年龄队列，防止壳长随时间缩小；3) 使用222个礁石2003-2023年的壳长数据，筛选出64个数据充足的礁石进行分析。

Result: 方法在仅使用壳长数据识别牡蛎队列和估计寿命方面显示出潜力。结果显示几乎所有河流系统都表现出恢复力信号：与2000年代初期相比，牡蛎队列在2010年代中后期寿命更长、生长更大。

Conclusion: 开发的新方法能够有效利用壳长数据识别牡蛎年龄队列和估计寿命，为评估牡蛎恢复力提供了新工具。研究结果表明弗吉尼亚切萨皮克湾牡蛎种群在2010年代中后期表现出积极的恢复趋势。

Abstract: Oysters in Virginia Chesapeake Bay oyster reefs are "age-truncated", possibly due to a combination of historical overfishing, disease epizootics, environmental degradation, and climate change. Research has suggested that oysters exhibit resilience to environmental stressors; however, that evidence is based on the current limited understanding of oyster lifespan. Until this paper, the Virginia Oyster Stock Assessment and Replenishment Archive (VOSARA), a spatially and temporally expansive dataset (222 reefs across 2003-2023) of shell lengths (SL, mm), had yet to be examined comprehensively in the context of resilience. We develop a novel method using Gaussian mixture modeling (GMM) to identify the age groups in each reef using yearly SL data and then link those age groups over time to identify cohorts and estimate their lifespan. Sixty-four reefs (29%) are deemed to have sufficient data (at least 300 oysters sampled for a minimum of 8 consecutive years) for this analysis. We fit univariate GMMs for each year ($t$) and reef ($r$) for each of the seven river strata ($R$) to estimate 1) the mean and standard deviation of SL for each $a_{Rrt}$th age group, and 2) the mixture percentage of each $a_{Rrt}$th age group. We link age groups across time to infer age cohorts by developing a mechanistic algorithm that prevents the shrinking of shell length when an $a_{Rrt}$th group becomes an ($a_{R,r,t+1}$)th group. Our method shows promise in identifying oyster cohorts and estimating lifespan solely using SL data. Our results show signals of resiliency in almost all river systems: oyster cohorts live longer and grow larger in the mid-to-late 2010s compared to the early 2000s.

</details>


### [22] [A Latent Variable Framework for Scaling Laws in Large Language Models](https://arxiv.org/abs/2512.06553)
*Peiyao Cai,Chengyu Cui,Felipe Maia Polo,Seamus Somerstep,Leshem Choshen,Mikhail Yurochkin,Moulinath Banerjee,Yuekai Sun,Kean Ming Tan,Gongjun Xu*

Main category: stat.AP

TL;DR: 提出基于潜变量建模的统计框架，用于分析大语言模型的缩放定律，解决不同模型架构和基准测试的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 随着新的大语言模型家族不断涌现，每个家族都有独特的架构和训练策略，同时评估基准也在增加。这种异质性使得单一的全局缩放曲线无法准确捕捉不同模型家族和基准测试间的性能变化。

Method: 提出潜变量建模框架：每个LLM家族关联一个潜变量，捕捉该家族的共同底层特征；模型在不同基准上的性能由其潜变量技能驱动，这些技能由潜变量和模型自身可观测特征共同决定。开发了该模型的估计程序并建立了统计性质，设计了支持估计和各种下游任务的高效数值算法。

Result: 在Open LLM Leaderboard（v1/v2）的12个广泛使用的基准测试上进行了实证评估。

Conclusion: 该框架能够更好地理解和预测不同大语言模型家族在各种基准测试上的性能表现，解决了现有单一缩放曲线方法的局限性。

Abstract: We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2).

</details>


### [23] [Disentangling the Mediation Pathways of Depression in Asian Students and Workers](https://arxiv.org/abs/2512.06654)
*Zhaojin Nan,Ran Chen*

Main category: stat.AP

TL;DR: 该研究比较了印度、马来西亚和中国学生与工作者群体的抑郁预测因素，发现压力是主要预测因子，年龄对学生和工作者有相反影响，模型在跨国泛化性有限。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是全球主要心理健康问题，受文化、人口和职业因素影响。研究旨在比较学生和工作者群体的抑郁预测因素，了解不同人群的抑郁风险机制。

Method: 使用印度、马来西亚和中国数据集，将印度数据分为学生和工作者组，马来西亚只有学生数据，中国只有工作者数据。采用逻辑回归、随机森林和因果森林模型识别关键预测因素，并进行因果中介分析评估变量是否通过感知压力等中介因素起作用。

Result: 学生群体中压力、年龄、工作量、财务压力、心理健康史和满意度是显著预测因素；工作者群体也有类似因素。年龄对学生和工作者有相反影响：年轻学生抑郁风险更高，而年长工作者风险更高。随机森林表现优于逻辑回归，但模型跨国泛化性有限。因果森林显示压力效应异质性有限，中介分析表明压力不中介年龄效应但更直接影响抑郁，满意度部分通过压力影响抑郁。

Conclusion: 压力是抑郁的最强预测因子，针对学业和职业压力的干预可能有助于减轻抑郁症状。研究强调了考虑人口和职业差异的重要性，以及开发针对特定群体的干预策略的必要性。

Abstract: Depression is a major global mental health issue shaped by cultural, demographic, and occupational factors. This study compares predictors of depression across student and worker populations using datasets from India, Malaysia, and China. The India dataset was split into student and worker groups, while the Malaysia dataset includes only students and the China (CHARLS) dataset includes only workers. After harmonizing variables, we applied logistic regression, random forest, and causal forest models to identify key predictors and subgroup-specific effects, and conducted causal mediation analysis (CMA) to assess whether variables operate through intermediaries such as perceived pressure. Among students, pressure, age, workload, financial stress, mental health history, and satisfaction were significant predictors; similar factors emerged for workers. Notably, age showed opposite effects across groups: younger students were more likely to experience depression, whereas older workers showed higher risk. Model performance showed moderate internal accuracy but weaker external generalizability across countries, with random forest outperforming logistic regression. Causal forest results indicated limited heterogeneity in the effect of pressure, while CMA showed that pressure does not mediate the effect of age but operates more directly, and satisfaction influences depression partly through pressure. Overall, pressure consistently emerged as the strongest predictor, suggesting that interventions targeting academic and occupational stress may help reduce depressive symptoms.

</details>


### [24] [Partially Observable Markov Decision Process Framework for Operating Condition Optimization Using Real-Time Degradation Signals](https://arxiv.org/abs/2512.06682)
*Boyang Xu,Yunyi Kang,Xinyu Zhao,Hao Yan,Feng Ju*

Main category: stat.AP

TL;DR: 提出一个基于部分可观测马尔可夫决策过程（POMDP）的系统决策框架，用于联合优化运行控制和预测性维护，利用多传感器实时退化信号提高制造系统性能。


<details>
  <summary>Details</summary>
Motivation: 在工程系统中，预测性维护和运行控制对提高效率和可靠性、降低维护成本至关重要。然而，多传感器同时监测系统状态的分析非常困难，需要系统性的决策方法来优化预测性维护。

Method: 提出部分可观测马尔可夫决策过程（POMDP）模型，考虑系统状态观测不完美的情况，生成最优容量和预测性维护策略。该方法联合控制运行条件和预防性维护，利用实时机器退化信号，并纳入退化约束和不可观测状态。

Result: 将该技术应用于轴承退化数据和NASA飞机涡轮风扇发动机数据集，证明了所提方法的有效性。

Conclusion: 该工作提供了一个系统性方法，专注于利用实时机器退化信号联合控制运行条件和预防性维护，为制造实践中的系统性能改进提供了有效框架。

Abstract: In many engineering systems, proper predictive maintenance and operational control are essential to increase efficiency and reliability while reducing maintenance costs. However, one of the major challenges is that many sensors are used for system monitoring. Analyzing these sensors simultaneously for better predictive maintenance optimization is often very challenging. In this paper, we propose a systematic decision-making framework to improve the system performance in manufacturing practice, considering the real-time degradation signals generated by multiple sensors. Specifically, we propose a partially observed Markov decision process (POMDP) model to generate the optimal capacity and predictive maintenance policies, given the fact that the observation of the system state is imperfect. Such work provides a systematic approach that focuses on jointly controlling the operating conditions and preventive maintenance utilizing the real-time machine deterioration signals by incorporating the degradation constraint and non-observable states. We apply this technique to the bearing degradation data and NASA aircraft turbofan engine dataset, demonstrating the effectiveness of the proposed method.

</details>


### [25] [Machine Learning-based Unfolding for Cross Section Measurements in the Presence of Nuisance Parameters](https://arxiv.org/abs/2512.07074)
*Huanbiao Zhu,Krish Desai,Mikael Kuusela,Vinicius Mikuni,Benjamin Nachman,Larry Wasserman*

Main category: stat.AP

TL;DR: 提出Profile OmniFold算法，将OmniFold展开方法扩展到包含nuisance参数，以处理前向模型不确定性


<details>
  <summary>Details</summary>
Motivation: 在粒子物理实验中，探测器效应校正（展开）通常基于模拟，但模拟模型存在不确定性（nuisance参数）。现有OmniFold方法未考虑这种不确定性，需要扩展以包含nuisance参数处理。

Method: 基于OmniFold算法（分类器期望最大化方法），提出Profile OmniFold算法，通过nuisance参数剖面化技术将机器学习展开扩展到包含nuisance参数的情况。

Result: 使用高斯示例和CMS实验模拟数据进行验证，展示了Profile OmniFold算法能够有效处理前向模型不确定性。

Conclusion: Profile OmniFold成功扩展了机器学习展开方法以包含nuisance参数，为处理探测器模拟不确定性提供了实用工具。

Abstract: Statistically correcting measured cross sections for detector effects is an important step across many applications. In particle physics, this inverse problem is known as \textit{unfolding}. In cases with complex instruments, the distortions they introduce are often known only implicitly through simulations of the detector. Modern machine learning has enabled efficient simulation-based approaches for unfolding high-dimensional data. Among these, one of the first methods successfully deployed on experimental data is the \textsc{OmniFold} algorithm, a classifier-based Expectation-Maximization procedure. In practice, however, the forward model is only approximately specified, and the corresponding uncertainty is encoded through nuisance parameters. Building on the well-studied \textsc{OmniFold} algorithm, we show how to extend machine learning-based unfolding to incorporate nuisance parameters. Our new algorithm, called Profile \textsc{OmniFold}, is demonstrated using a Gaussian example as well as a particle physics case study using simulated data from the CMS Experiment at the Large Hadron Collider.

</details>


### [26] [Facilitating Conditions as an Enabler, Not a Direct Motivator: A Robustness and Mediation Analysis of E-Learning Adoption](https://arxiv.org/abs/2512.07185)
*Jaka Nugraha,Noyyn Sun,Xinlin Zhao,Vindi Kusuma Wardani,Inna Koblianska,Jiunn-Woei Lian*

Main category: stat.AP

TL;DR: 研究发现：在UTAUT框架中，便利条件（FC）对行为意向（BI）没有直接影响，而是通过增强绩效期望和努力期望间接发挥作用，解释了e-learning投资与学生参与度之间的悖论。


<details>
  <summary>Details</summary>
Motivation: 尽管机构在e-learning基础设施上投入了大量资金，但学生参与度往往达不到预期，这与UTAUT框架中便利条件（FC）直接影响行为意向（BI）的传统假设相矛盾，形成了理论悖论。

Method: 对470名印尼大学生进行实证研究，采用多阶段分析方法，首先验证了绩效期望、努力期望、社会影响和感知享受对行为意向的直接影响，然后通过中介模型分析便利条件的间接作用。

Result: 绩效期望（β=0.190）、努力期望（β=0.198）、社会影响（β=0.151）和感知享受（β=0.472）显著影响行为意向，行为意向又强预测使用行为（β=0.666）。便利条件对行为意向的直接效应不显著（β=-0.085），但通过增强绩效期望（β=0.556）和努力期望（β=0.419）间接发挥作用。

Conclusion: 技术基础设施的价值不在于其存在本身，而在于其动态能力——能够促进学习和优化用户体验。研究提出了"赋能路径"理论框架，指导管理者将技术投资重点从单纯提供工具转向战略性地设计学习体验。

Abstract: Despite substantial institutional investment in e-learning infrastructure, student engagement often fails to meet expectations--a persistent paradox that challenges the established direct relationship between Facilitating Conditions (FC) and behavioral intention within the classic UTAUT framework. To resolve this theoretical puzzle, we reconceptualized the role of FC through an empirical study of 470 Indonesian university students. Our robust, multi-stage analytical approach first confirmed the significant influence of established drivers--Performance Expectancy (beta=0.190), Effort Expectancy (beta=0.198), Social Influence (beta=0.151), and Perceived Enjoyment (beta=0.472)--on Behavioral Intention (BI), which in turn strongly predicted Use Behavior (beta=0.666). Crucially, however, the direct effect of FC on BI proved non-significant (beta=-0.085). A subsequent mediation model revealed FC's true function as a foundational enabling construct that operates indirectly by powerfully enhancing both Performance Expectancy (beta=0.556) and Effort Expectancy (beta=0.419). Our findings demonstrate that the value of technological infrastructure lies not in its mere presence, but in its dynamic capacity to enable learning and optimize user experience. This research advances a refined "enabling pathway" theoretical framework, guiding administrators to shift the focus of technological investment from merely providing tools to strategically crafting learning experiences.

</details>


### [27] [Bridging CORDEX and CMIP6: Machine Learning Downscaling for Wind and Solar Energy Droughts in Central Europe](https://arxiv.org/abs/2512.07429)
*Nina Effenberger,Maxim Samarin,Maybritt Schillinger,Reto Knutti*

Main category: stat.AP

TL;DR: 该研究开发了一个机器学习模拟器，能够将全球气候模型输出降尺度到区域分辨率，用于评估气候变化对可再生能源的影响，特别是风能和太阳能干旱事件。


<details>
  <summary>Details</summary>
Motivation: 传统区域气候模型（如CORDEX）计算成本高且组织困难，需要更高效的方法来获取高分辨率区域气候信息，以评估气候变化影响和可再生能源规划。

Method: 开发机器学习模拟器，在CMIP5和CORDEX模拟数据上训练，学习全球与区域气候场之间的映射关系，然后应用于未见过的CMIP6模拟。

Result: 模拟器能够准确再现区域气候模型数据，在未见过的CMIP6模拟上也产生现实结果。分析发现未来低风速和低太阳辐射同时发生的能源干旱天数可能减少。

Conclusion: 机器学习降尺度模拟器为CORDEX等传统方法提供了高效补充，能够为影响评估提供所需的高分辨率信息。

Abstract: Reliable regional climate information is essential for assessing the impacts of climate change and for planning in sectors such as renewable energy; yet, producing high-resolution projections through coordinated initiatives like CORDEX that run multiple physical regional climate models is both computationally demanding and difficult to organize. Machine learning emulators that learn the mapping between global and regional climate fields offer a promising way to address these limitations. Here we introduce the application of such an emulator: trained on CMIP5 and CORDEX simulations, it reproduces regional climate model data with sufficient accuracy. When applied to CMIP6 simulations not seen during training, it also produces realistic results, indicating stable performance. Using CORDEX data, CMIP5 and CMIP6 simulations, as well as regional data generated by two machine learning models, we analyze the co-occurrence of low wind speed and low solar radiation and find indications that the number of such energy drought days is likely to decrease in the future. Our results highlight that downscaling with machine learning emulators provides an efficient complement to efforts such as CORDEX, supplying the higher-resolution information required for impact assessments.

</details>


### [28] [Permanent and transitory crime risk in variable-density hot spot analysis](https://arxiv.org/abs/2512.07467)
*Ben Moews*

Main category: stat.AP

TL;DR: 对芝加哥2001-2022年犯罪数据进行变密度聚类分析，研究不同密度热点区域的犯罪类型构成变化，特别关注COVID-19疫情的影响及城市功能区差异。


<details>
  <summary>Details</summary>
Motivation: 为有效配置公共安全资源，需要基于时空数据的实证分析。现有研究在犯罪类型构成随时间演变、疫情影响及城市功能区差异方面存在不足，本研究旨在填补这些空白。

Method: 采用变密度聚类分析方法，对芝加哥市2001-2022年的犯罪事件报告进行时空分析，研究不同密度热点区域的犯罪类型份额变化，并分析空间自相关性。

Result: 发现COVID-19疫情及其社交隔离措施对犯罪类型构成有显著影响，且这种影响因城市功能区而异；不同犯罪类型的治理难度存在差异；空间自相关分析显示不同距离半径下热点与异常区域的犯罪事件均匀性不同。

Conclusion: 研究强调了运筹学与刑事司法的交叉应用价值，为热点警务和公共安全优化提供实证依据，同时指出犯罪学应用中常被忽视的数据偏差问题及其带来的挑战和风险。

Abstract: Crime prevention measures, aiming for the effective and efficient spending of public resources, rely on the empirical analysis of spatial and temporal data for public safety outcomes. We perform a variable-density cluster analysis on crime incident reports in the City of Chicago for the years 2001--2022 to investigate changes in crime share composition for hot spots of different densities. Contributing to and going beyond the existing wealth of research on criminological applications in the operational research literature, we study the evolution of crime type shares in clusters over the course of two decades and demonstrate particularly notable impacts of the COVID-19 pandemic and its associated social contact avoidance measures, as well as a dependence of these effects on the primary function of city areas. Our results also indicate differences in the relative difficulty to address specific crime types, and an analysis of spatial autocorrelations further shows variations in incident uniformity between clusters and outlier areas at different distance radii. We discuss our findings in the context of the interplay between operational research and criminal justice, the practice of hot spot policing and public safety optimization, and the factors contributing to, and challenges and risks due to, data biases as an often neglected factor in criminological applications.

</details>


### [29] [Meta-analyses of dietary exposures must consider energy adjustment: recommendations from a meta-scientific review](https://arxiv.org/abs/2512.07531)
*Natalia Ortega,Peter WG Tennant,Darren C Greenwood,Octavio Pano,Christina C Dahm,Russell J de Souza,Daniel B Ibsen,Conor J MacDonald,Deirdre K Tobias,Georgia D Tomova*

Main category: stat.AP

TL;DR: 该论文发现营养流行病学meta分析中普遍忽视能量调整策略，导致合并了不可比较的效应估计，影响证据质量和营养建议


<details>
  <summary>Details</summary>
Motivation: 在观察性饮食研究中，能量调整策略对估计效应有重要影响，但meta分析往往忽视这一点，可能合并了不可比较的效应估计，影响证据解释和营养建议质量

Method: 选取饱和脂肪和鱼类摄入与心血管疾病关系的meta分析，筛选出8篇最新和最常被引用的meta分析，分析其纳入的82项原始研究中的144个模型，评估能量调整策略的考虑情况

Result: 仅1篇meta分析明确考虑原始研究的能量调整策略用于替代效应亚组分析；没有meta分析承认他们在合并不同效应的估计；82%的原始研究模型隐含估计替代效应，但大多数研究目的、解释或结论中未明确说明

Conclusion: meta分析很少考虑原始研究的能量调整策略，导致合并估计反映的是定义不清、解释不明的量。作者提出改进未来meta分析和营养建议证据质量的建议

Abstract: In observational studies of dietary exposures, the energy adjustment strategy has a critical impact on the effect being estimated. Adjusting for total energy intake or expressing the exposure as a percentage of total energy, leads to a substitution effect being estimated. This impacts the interpretation of primary studies and meta-analyses. Unless energy adjustment strategies are considered, meta-analyses may end up pooling estimates for incomparable effects. This meta-scientific review aimed to investigate the extent to which meta-analyses of dietary exposures may be pooling incomparable effects by reviewing the energy adjustment strategies. We identified all meta-analyses examining the relationship between saturated fat and fish and cardiovascular disease. The two most recent and two most cited reviews for each exposure were examined, along with all primary studies. Information on the study aims, targeted effects, and interpretations were summarized. The eight meta-analyses summarised results from 82 primary studies including 144 unique models. Only one meta-analysis explicitly considered the energy adjustment strategy of the primary studies to determine eligibility for a substitution subgroup analysis. None of the meta-analyses acknowledged that they were pooling estimates for different effects. 82% of the models from the primary studies were implicitly estimating substitution effects but this was not explicitly stated in most study aims, interpretation or conclusions. Our meta-scientific review found little evidence that the energy adjustment strategies of the primary studies were being considered in the synthesis or interpretation of evidence. Consequently, the pooled estimates reflect ill-defined quantities with unclear interpretations. We offer recommendations to improve the conduct of future meta-analyses and the quality of evidence that informs nutritional recommendations.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [30] [Evidence and Elimination: A Bayesian Interpretation of Falsification in Scientific Practice](https://arxiv.org/abs/2512.06777)
*Tommaso Costa*

Main category: stat.OT

TL;DR: 将波普尔的证伪主义重新解释为贝叶斯模型比较过程，理论因数据概率趋零而被淘汰而非逻辑矛盾，解决了迪昂-蒯因问题并解释特设性修改为何降低理论可信度。


<details>
  <summary>Details</summary>
Motivation: 传统证伪主义认为理论与观察矛盾即被反驳，但这与科学实践不符。波普尔的淘汰性理论检验与贝叶斯模型比较之间的关系尚未充分阐明，需要建立统一框架。

Method: 将证伪重新解释为贝叶斯模型淘汰过程：理论被淘汰是因为相对于替代模型，它赋予数据的积分概率趋近于零。通过海王星发现和水星近日点进动两个经典案例进行说明。

Result: 海王星案例中，牛顿引力内部辅助假设大幅提高理论边际似然，使其免于被证伪；水星案例中，牛顿模型无法通过允许的辅助修改挽救，而广义相对论无需可调参数即赋予异常高概率。

Conclusion: 贝叶斯模型比较提供了波普尔哲学缺乏的数学结构，将科学理论变革理解为竞争模型空间中连续淘汰的过程，为科学理论变革提供了连贯解释。

Abstract: The classical conception of falsification presents scientific theories as entities that are decisively refuted when their predictions fail. This picture has long been challenged by both philosophical analysis and scientific practice, yet the relationship between Popper's eliminative view of theory testing and Bayesian model comparison remains insufficiently articulated. This paper develops a unified account in which falsification is reinterpreted as a Bayesian process of model elimination. A theory is not rejected because it contradicts an observation in a logical sense; it is eliminated because it assigns vanishing integrated probability to the data in comparison with an alternative model. This reinterpretation resolves the difficulties raised by the Duhem-Quine thesis, clarifies the status of auxiliary hypotheses, and explains why ad hoc modifications reduce rather than increase theoretical credibility. The analysis is illustrated through two classical episodes in celestial mechanics, the discovery of Neptune and the anomalous precession of Mercury. In the Neptune case, an auxiliary hypothesis internal to Newtonian gravity dramatically increases the marginal likelihood of the theory, preserving it from apparent refutation. In the Mercury case, no permissible auxiliary modification can rescue the Newtonian model, while general relativity assigns high probability to the anomaly without adjustable parameters. The resulting posterior collapse provides a quantitative realisation of Popper's eliminative criterion. Bayesian model comparison therefore supplies the mathematical structure that Popper's philosophy lacked and offers a coherent account of scientific theory change as a process of successive eliminations within a space of competing models.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [31] [Hierarchical Clustering With Confidence](https://arxiv.org/abs/2512.06522)
*Di Wu,Jacob Bien,Snigdha Panigrahi*

Main category: stat.ME

TL;DR: 提出一种随机化层次聚类方法，为聚类树每个节点构造有效的p值，量化贪婪合并的证据，并开发自适应α-spending过程估计聚类数量


<details>
  <summary>Details</summary>
Motivation: 层次聚类对数据微小扰动敏感，难以区分真实结构与虚假模式，需要更稳健的统计推断方法

Method: 提出简单随机化方案，为层次聚类树每个节点构造有效p值，控制I类错误率；开发自适应α-spending过程估计聚类数量

Result: 方法比现有选择性推断方法更强大，实验显示在模拟和真实数据上能提供强大的聚类结果，并可评估聚类稳定性

Conclusion: 随机化层次聚类不仅可用于测量稳定性，还能设计有效的假设检验程序，为聚类结果提供统计保证

Abstract: Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.
  We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $α$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm.

</details>


### [32] [The Bag-and-Whisker Plot: A New Bagplot for Bivariate Data](https://arxiv.org/abs/2512.06314)
*Shenghao Qin,Bowen Gang,Tiejun Tong,Hengjian Cui*

Main category: stat.ME

TL;DR: 提出改进的"袋须图"，通过多重检验方法自适应检测异常值，并采用更稳定的可视化方式替代凸包


<details>
  <summary>Details</summary>
Motivation: 传统袋图存在两个关键限制：异常值检测的固定膨胀因子不随样本量自适应调整，以及用于可视化围栏的凸包不稳定，这阻碍了其实际应用

Method: 将异常值检测重构为多重检验问题，生成数据自适应的围栏以控制统计错误率；放弃凸包，直接渲染统计围栏，并添加细粒度须线展示数据分布

Result: 广泛的模拟和真实数据分析表明，新袋图相比现有标准具有更好的自适应性和鲁棒性

Conclusion: 新提出的袋须图解决了传统袋图的局限性，在异常值检测可靠性和可视化稳定性方面表现优异，值得在实际应用中推荐使用

Abstract: The bagplot, also known as the "bag-and-bolster plot", is a notable extension of the boxplot from univariate to bivariate data. Although widely used, its practical application is hindered by two key limitations: the fixed inflation factor for outlier detection that does not adapt to the sample size, and the unstable convex hull used to visualize its fence. In this paper, we propose a new bagplot, namely the "bag-and-whisker plot'', as an improvement method to address these limitations. Our framework recasts outlier detection as a multiple testing problem, yielding a data-adaptive fence that controls statistical error rates and enhances the reliability of outlier identification. To further resolve graphical instability, we introduce a refined visualization that abandons the convex hull (the bolster) with a direct rendering of the statistical fence, complemented by granular whiskers that effectively illustrate the data's spread. Extensive simulations and real-world data analyses demonstrate that our new bagplot exhibits superior adaptivity and robustness compared to the existing standard, and thus can be highly recommended for practical use.

</details>


### [33] [Goodness-of-fit Tests for Heavy-tailed Random Fields](https://arxiv.org/abs/2512.06412)
*Ying Niu,Zhao Chen,Christina Dan Wang,Yuwei Zhao*

Main category: stat.ME

TL;DR: 提出基于傅里叶变换的极值指示器统计量，用于检验重尾空间数据的极值稳定随机场拟合优度，采用空间平稳自助法逼近临界值。


<details>
  <summary>Details</summary>
Motivation: 重尾空间数据（如极端天气、污染浓度）常用极值稳定随机场建模，但缺乏有效的拟合优度检验方法评估模型是否合适。

Method: 基于重尾空间数据极值指示器的傅里叶变换构造检验统计量，其渐近分布为高斯随机场。由于协方差结构无显式表达式，提出空间平稳自助法逼近临界值。

Result: 模拟研究验证了理论分布结果，PM2.5和温度数据的应用展示了该方法在实际模型评估中的实用性。

Conclusion: 该方法为极值稳定随机场提供了有效的拟合优度检验工具，适用于重尾空间数据的模型诊断。

Abstract: We develop goodness-of-fit tests for max-stable random fields, which are used to model heavy-tailed spatial data. The test statistics are constructed based on the Fourier transforms of the indicators of extreme values in the heavy-tailed spatial data, whose asymptotic distribution is a Gaussian random field under a hypothesized max-stable random field. Since the covariance structure of the limiting Gaussian random field lacks an explicit expression, we propose a stationary bootstrap procedure for spatial fields to approximate critical values. Simulation studies confirm the theoretical distributional results, and applications to PM2.5 and temperature data illustrate the practical utility of the proposed method for model assessment.

</details>


### [34] [Community detection in heterogeneous signed networks](https://arxiv.org/abs/2512.06428)
*Yuwen Wang,Shiwen Ye,Jingnan Zhang,Junhui Wang*

Main category: stat.ME

TL;DR: 提出了一种用于有符号网络（包含正负边）的符号块β模型，能够同时建模强平衡和弱平衡网络，并建立了模型的可识别性、高效算法和渐近一致性。


<details>
  <summary>Details</summary>
Motivation: 现有网络分析方法主要关注无符号网络，但在实践中经常遇到包含正负边的有符号网络，这方面的研究相对较少。需要开发能够同时处理强平衡和弱平衡有符号网络的模型。

Method: 提出了符号块β模型，正式定义了有符号网络中的强平衡和弱平衡概念。利用二分图性质建立模型的可识别性，开发了交替更新算法来优化对数似然函数。

Result: 模型在概率估计和社区检测方面都建立了渐近一致性。通过广泛的数值实验和实际国际关系网络应用证明了其优势。

Conclusion: 提出的符号块β模型能够有效建模有符号网络，解决了现有方法主要关注无符号网络的局限性，为有符号网络分析提供了理论保证和实用工具。

Abstract: Network data has attracted growing interest across scientific domains, prompting the development of various network models. Existing network analysis methods mainly focus on unsigned networks, whereas signed networks, consisting of both positive and negative edges, have been frequently encountered in practice but much less investigated. In this paper, we formally define strong and weak balance in signed networks, and propose a signed block $β$-model, which is capable of modeling strong- and weak-balanced signed networks simultaneously. We establish the identifiability of the proposed model by leveraging properties of bipartite graphs, and develop an efficient alternating updating algorithm to optimize the resulting log-likelihood function. More importantly, we establish the asymptotic consistencies of the proposed model in terms of both probability estimation and community detection. Its advantages are also demonstrated through extensive numerical experiments and the application to a real-world international relationship network.

</details>


### [35] [Simultaneous Heterogeneity and Reduced-rank Learning for Multivariate Response Regression](https://arxiv.org/abs/2512.06514)
*Jie Wu,Bo Zhang,Daoji Li,Zemin Zheng*

Main category: stat.ME

TL;DR: 提出联合异质性和降秩学习框架，用于多变量响应回归中的子组检测和协变量效应估计


<details>
  <summary>Details</summary>
Motivation: 现有子组检测方法主要关注单变量响应设置，而实际应用中多变量响应数据普遍存在，需要同时识别子组结构和估计协变量效应

Method: 使用秩约束的成对融合惩罚方法，通过ADMM算法实现，无需先验子组成员信息，并提出预测信息准则选择系数矩阵的秩

Result: 建立了估计量的渐近性质，通过模拟研究和实际数据应用验证了方法的有效性

Conclusion: 提出的联合学习框架能够有效处理异质多变量响应回归问题，同时识别子组结构和估计协变量效应

Abstract: Heterogeneous data are now ubiquitous in many applications in which correctly identifying the subgroups from a heterogeneous population is critical. Although there is an increasing body of literature on subgroup detection, existing methods mainly focus on the univariate response setting. In this paper, we propose a joint heterogeneity and reduced-rank learning framework to simultaneously identify the subgroup structure and estimate the covariate effects for heterogeneous multivariate response regression. In particular, our approach uses rank-constrained pairwise fusion penalization and conducts the subgroup analysis without requiring prior knowledge regarding the individual subgroup memberships. We implement the proposed approach by an alternating direction method of multipliers (ADMM) algorithm and show its convergence. We also establish the asymptotic properties for the resulting estimators under mild and interpretable conditions. A predictive information criterion is proposed to select the rank of the coefficient matrix with theoretical support. The effectiveness of the proposed approach is demonstrated through simulation studies and a real data application.

</details>


### [36] [Controlling the False Discovery Proportion in Matched Observational Studies](https://arxiv.org/abs/2512.06601)
*Mengqi Lin,Colin Fogarty*

Main category: stat.ME

TL;DR: 提出一种匹配观察研究中探索性数据分析的方法，用于处理单一干预和多个终点的情况，同时考虑错误发现和未测量混杂偏倚的影响。


<details>
  <summary>Details</summary>
Motivation: 在匹配观察研究中，研究者需要探索多个终点变量的实际处理效应证据，但面临两个挑战：1) 需要控制错误发现率；2) 未测量混杂偏倚可能影响结果。现有方法难以同时处理这两个问题。

Method: 为任何候选假设子集提供敏感性集合，表示该子集中真实假设的比例。这些敏感性陈述在所有可能的拒绝集选择上同时有效，允许研究者寻找即使在存在隐藏偏倚时仍能保持较大真实发现比例的假设子集。通过整数规划序列和筛选步骤实现高效计算。

Result: 方法在模拟研究和儿童虐待长期影响的实际数据示例中展示了实用价值。结果表明，关于某些比例的结果受处理影响的结论比关于任何特定结果受影响的发现对未测量混杂偏倚具有更大的稳健性。

Conclusion: 该方法为匹配观察研究中的探索性数据分析提供了一种有效框架，能够同时控制错误发现和未测量混杂偏倚的影响，特别适用于需要探索多个终点变量的研究场景。

Abstract: We provide an approach to exploratory data analysis in matched observational studies with a single intervention and multiple endpoints. In such settings, the researcher would like to explore evidence for actual treatment effects among these variables while accounting not only for the possibility of false discoveries, but also for the potential impact of unmeasured confounding. For any candidate subset of hypotheses about these outcomes, we provide sensitivity sets for the proportion of the hypotheses within the subset which are actually true. The resulting sensitivity statements are valid simultaneously over all possible choices for the rejected set, allowing the researcher to search for promising subsets of hypotheses that maintain a large estimated fraction of true discoveries even if hidden bias is present. The approach is well suited to sensitivity analysis, as conclusions that some fraction of outcomes are affected by the treatment exhibit larger robustness to unmeasured confounding than findings that any particular outcome is affected. We show how a sequence of integer programs, in tandem with screening steps, facilitate the efficient computation of the required sensitivity sets. We illustrate the practical utility of our method through both simulation studies and a data example on the long-term impacts of childhood abuse.

</details>


### [37] [Monotone data augmentation algorithm for longitudinal continuous, binary and ordinal outcomes: a unifying approach](https://arxiv.org/abs/2512.06621)
*Yongqiang Tang*

Main category: stat.ME

TL;DR: 将单调数据增强算法扩展到多元probit模型，用于处理纵向二元和有序分类数据，通过参数扩展策略加速马尔可夫链收敛


<details>
  <summary>Details</summary>
Motivation: MDA算法在纵向连续数据缺失值填补中表现良好，但需要扩展到处理纵向二元和有序分类数据，同时需要解决多元probit模型中的计算效率问题

Method: 将MDA算法扩展到多元probit模型，采用参数扩展策略促进后验采样，使用独立Metropolis-Hasting采样器处理复杂先验，统一处理连续、二元和有序分类数据

Result: 开发了统一的框架，能够以相同方式采样回归系数和协方差矩阵，加速马尔可夫链收敛，并评估了不同先验对参数估计和缺失数据填补的影响

Conclusion: 扩展的MDA算法为纵向二元和有序分类数据提供了有效的缺失数据填补方法，参数扩展策略提高了计算效率，统一的采样框架简化了算法理解和代码实现

Abstract: The monotone data augmentation (MDA) algorithm has been widely used to impute missing data for longitudinal continuous outcomes. Compared to a full data augmentation approach, the MDA scheme accelerates the mixing of the Markov chain, reduces computational costs per iteration, and aids in missing data imputation under nonignorable dropouts. We extend the MDA algorithm to the multivariate probit (MVP) model for longitudinal binary and ordinal outcomes. The MVP model assumes the categorical outcomes are discretized versions of underlying longitudinal latent Gaussian outcomes modeled by a mixed effects model for repeated measures. A parameter expansion strategy is employed to facilitate the posterior sampling, and expedite the convergence of the Markov chain in MVP. The method enables the sampling of the regression coefficients and covariance matrix for longitudinal continuous, binary and ordinal outcomes in a unified manner. This property aids in understanding the algorithm and developing computer codes for MVP. We also introduce independent Metropolis-Hasting samplers to handle complex priors, and evaluate how the choice between flat and diffuse normal priors for regression coefficients influences parameter estimation and missing data imputation. Numerical examples are used to illustrate the methodology.

</details>


### [38] [SLOACI: Surrogate-Leveraged Online Adaptive Causal Inference](https://arxiv.org/abs/2512.06872)
*Yingying Fan,Zihan Wang,Waverly Wei*

Main category: stat.ME

TL;DR: 提出SLOACI框架，将预测性替代结果整合到自适应设计中以提高效率，构建自适应增强逆概率加权估计器，在替代结果有噪声或弱的情况下保持稳健，达到半参数效率界。


<details>
  <summary>Details</summary>
Motivation: 自适应实验设计在各个领域受到越来越多的关注，但现有方法在效率方面仍有提升空间。本文旨在通过整合预测性替代结果来增强自适应设计的效率，特别是在替代结果有噪声或弱的情况下保持稳健性。

Method: 提出SLOACI（surrogate-leveraged online adaptive causal inference）框架，将预测性替代结果整合到自适应设计中。构建自适应增强逆概率加权估计器用于平均处理效应的下游分析。开发了适应渐近和非渐近两种情况的序列检验程序工具箱。

Result: 理论分析表明，在渐近情况下，所提出的估计器达到半参数效率界；从非渐近角度推导了遗憾界。通过大量模拟和合成案例研究展示了该方法在有限样本下的优越性能。

Conclusion: SLOACI框架成功地将预测性替代结果整合到自适应设计中，提高了因果推断的效率，即使在替代结果有噪声或弱的情况下也能保持稳健。该方法为实验者提供了灵活的工具箱，可以根据实际需求选择渐近或非渐近视角。

Abstract: Adaptive experimental designs have gained increasing attention across a range of domains. In this paper, we propose a new methodological framework, surrogate-leveraged online adaptive causal inference (SLOACI), which integrates predictive surrogate outcomes into adaptive designs to enhance efficiency. For downstream analysis, we construct the adaptive augmented inverse probability weighting estimator for the average treatment effect using collected data. Our procedure remains robust even when surrogates are noisy or weak. We provide a comprehensive theoretical foundation for SLOACI. Under the asymptotic regime, we show that the proposed estimator attains the semiparametric efficiency bound. From a non-asymptotic perspective, we derive a regret bound to provide practical insights. We also develop a toolbox of sequential testing procedures that accommodates both asymptotic and non-asymptotic regimes, allowing experimenters to choose the perspective that best aligns with their practical needs. Extensive simulations and a synthetic case study are conducted to showcase the superior finite-sample performance of our method.

</details>


### [39] [Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length](https://arxiv.org/abs/2512.07019)
*Zhiyu Xu,Jia Liu,Yixin Wang,Yuqi Gu*

Main category: stat.ME

TL;DR: 提出Latency-Response Theory (LaRT)模型，联合建模LLMs的响应准确率和思维链长度，通过相关性参数连接潜在能力和潜在速度，相比传统IRT模型在多个评估指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs评估方法主要基于响应准确率，但思维链长度也是推理能力的重要指标。需要开发能够同时利用准确率和思维链长度信息的评估框架，以更全面、高效地评估LLMs。

Method: 提出LaRT模型，引入潜在能力和潜在速度之间的相关性参数，联合建模响应准确率和思维链长度。开发了高效的随机近似EM算法进行参数估计，并建立了参数可识别性的严格理论保证。

Result: 理论分析和模拟研究显示LaRT相比IRT具有更优的估计精度和更短的置信区间。在实际LLMs评估中，LaRT产生了与IRT不同的模型排名，并在预测能力、项目效率、排名有效性和评估效率等多个指标上优于IRT。

Conclusion: LaRT模型通过联合利用响应准确率和思维链长度信息，为LLMs评估提供了更全面、高效的框架，能够产生更可靠的模型排名和评估结果，优于传统的IRT方法。

Abstract: The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.

</details>


### [40] [Easy-to-Implement Two-Way Effect Decomposition for Any Outcome Variable with Endogenous Mediator](https://arxiv.org/abs/2512.07058)
*Bora Kim,Myoung-jae Lee*

Main category: stat.ME

TL;DR: 该论文提出了一种处理中介变量内生性的因果简化形式方法，允许中介变量M内生而处理变量D外生，使用二元工具变量Z解决内生性问题，通过OLS或IV估计直接和间接效应。


<details>
  <summary>Details</summary>
Motivation: 传统的中介分析通常假设处理变量D和中介变量M都是外生的，但在实际应用中，中介变量M可能存在内生性问题。当D是随机分配时，D的外生性可以得到保证，但M的内生性仍然需要解决。现有方法如匹配或逆概率加权在寻找渐近分布或处理接近零分母时存在困难。

Method: 提出"因果简化形式(CRF)"方法，允许中介变量M内生而保持处理变量D外生。使用二元工具变量Z解决M的内生性问题。推导出以(D,Z,DZ)或(D,M,DZ)为回归元的Y的非参数CRF，然后可以使用普通最小二乘法或工具变量估计器轻松估计直接和间接效应。

Result: 该方法不仅实现简单，而且适用于任何类型的Y（二元、计数、连续等）。通过模拟和实证研究验证了该方法的有效性，相比传统方法更容易实现且避免了渐近分布问题和分母接近零的困难。

Conclusion: 该论文提出的CRF方法为处理中介变量内生性问题提供了一种简单有效的解决方案，特别适用于处理变量外生而中介变量内生的情境，扩展了中介分析的应用范围并提高了估计的可行性。

Abstract: Given a binary treatment D and a binary mediator M, mediation analysis decomposes the total effect of D on an outcome Y into the direct and indirect effects. Typically, both D and M are assumed to be exogenous, but this paper allows M to be endogenous while maintaining the exogeneity of D, which holds certainly if D is randomized. The endogeneity problem of M is then overcome using a binary instrumental variable Z. We derive a nonparametric "causal reduced form (CRF)" for Y with either (D,Z,DZ) or (D,M,DZ) as the regressors. The CRF enables estimating the direct and indirect effects easily with ordinary least squares or instrumental variable estimator, instead of matching or inverse probability weighting that have difficulties in finding the asymptotic distribution or in dealing with near-zero denominators. Not just this ease in implementation, our approach is applicable to any Y (binary, count, continuous, etc.). Simulation and empirical studies illustrate our approach.

</details>


### [41] [Finite-Sample Failures and Condition-Number Diagnostics in Double Machine Learning](https://arxiv.org/abs/2512.07083)
*Gabriel Saco*

Main category: stat.ME

TL;DR: DML置信区间在得分方程病态时存在有限样本覆盖失真，论文提出用条件数κ_DML作为诊断指标，推导了覆盖误差界，并通过模拟验证κ_DML能预测DML性能


<details>
  <summary>Details</summary>
Motivation: 标准DML置信区间在得分方程病态时即使使用先进方法估计nuisance函数，仍存在显著的有限样本覆盖失真问题，需要诊断工具来评估推断可靠性

Method: 针对部分线性回归模型，提出正交得分条件数κ_DML作为诊断指标，推导非渐近Berry-Esseen型覆盖误差界和精化线性化分析，通过蒙特卡洛实验验证κ_DML的预测能力

Result: κ_DML能有效预测DML性能：κ_DML<1时覆盖接近名义水平且区间短，严重病态时即使使用灵活nuisance拟合，覆盖可能降至40%（名义95%），κ_DML与方差和偏差都成比例

Conclusion: 建议将κ_DML作为常规诊断指标报告，类似于IV中的条件数检查和弱工具诊断，以评估DML推断的可靠性，特别是当κ_DML=o_p(√n)且κ_DML*r_n→0时才能获得有意义的收缩置信集

Abstract: Standard Double Machine Learning (DML; Chernozhukov et al., 2018) confidence intervals can exhibit substantial finite-sample coverage distortions when the underlying score equations are ill-conditioned, even if nuisance functions are estimated with state-of-the-art methods. Focusing on the partially linear regression (PLR) model, we show that a simple, easily computed condition number for the orthogonal score, denoted kappa_DML := 1 / |J_theta|, largely determines when DML inference is reliable. Our first result derives a nonasymptotic, Berry-Esseen-type bound showing that the coverage error of the usual DML t-statistic is of order n^{-1/2} + sqrt(n) * r_n, where r_n is the standard DML remainder term summarizing nuisance estimation error. Our second result provides a refined linearization in which both estimation error and confidence interval length scale as kappa_DML / sqrt(n) + kappa_DML * r_n, so that ill-conditioning directly inflates both variance and bias. These expansions yield three conditioning regimes - well-conditioned, moderately ill-conditioned, and severely ill-conditioned - and imply that informative, shrinking confidence sets require kappa_DML = o_p(sqrt(n)) and kappa_DML * r_n -> 0. We conduct Monte Carlo experiments across overlap levels, nuisance learners (OLS, Lasso, random forests), and both low- and high-dimensional (p > n) designs. Across these designs, kappa_DML is highly predictive of finite-sample performance: well-conditioned designs with kappa_DML < 1 deliver near-nominal coverage with short intervals, whereas severely ill-conditioned designs can exhibit large bias and coverage around 40% for nominal 95% intervals, despite flexible nuisance fitting. We propose reporting kappa_DML alongside DML estimates as a routine diagnostic of score conditioning, in direct analogy to condition-number checks and weak-instrument diagnostics in IV settings.

</details>


### [42] [Symmetric Vaccine Efficacy](https://arxiv.org/abs/2512.07739)
*Lucy D'Agostino McGowan,Sarah C. Lotspeich,Michael G. Hudgens*

Main category: stat.ME

TL;DR: 提出对称疫苗效力(SVE)作为传统疫苗效力(VE)的替代指标，解决VE的不对称性问题，提供[-1,1]范围内的对称解释尺度。


<details>
  <summary>Details</summary>
Motivation: 传统疫苗效力(VE)指标存在不对称性：上限为1但下限无界，导致VE估计值和置信区间可能远低于零，难以区分是真实危害还是统计不确定性。

Method: 提出对称疫苗效力(SVE)作为替代指标，通过对感染风险进行对称变换，将取值范围限制在[-1,1]之间，为有益和有害的疫苗效应提供统一尺度。

Result: 建立了SVE与传统VE的关系，考虑了SVE的统计推断方法，并通过重新分析HIV候选疫苗随机试验数据展示了SVE的实用性。

Conclusion: SVE解决了传统VE的不对称性问题，提供了更易解释的疫苗效果度量，并开发了R包sve用于计算SVE估计值和置信区间。

Abstract: Traditional measures of vaccine efficacy (VE) are inherently asymmetric, constrained above by $1$ but unbounded below. As a result, VE estimates and corresponding confidence intervals can extend far below zero, making interpretation difficult and potentially obscuring whether the apparent effect reflects true harm or simply statistical uncertainty. The proposed symmetric vaccine efficacy (SVE) is a bounded and interpretable alternative to VE that maintains desirable statistical properties while resolving these asymmetries. SVE is defined as a symmetric transformation of infection risks, with possible values within $[-1, 1]$, providing a common scale for both beneficial and harmful vaccine effects. This paper describes the relationship between SVE and traditional VE, considers inference about SVE, and illustrates the utility of the proposed measure by reanalyzing data from a randomized trial of a candidate HIV vaccine. Open-source tools for computing estimates of SVE and corresponding confidence intervals are available in R through the sve package.

</details>


### [43] [Asymptotic theory and statistical inference for the samples problems with heavy-tailed data using the functional empirical process](https://arxiv.org/abs/2512.07088)
*Abdoulaye Camara,Saliou Diouf,Moumouni Diallo,Gane Samb Lo*

Main category: stat.ME

TL;DR: TFEP（修剪函数经验过程）为处理重尾或偏态分布提供稳健推断框架，通过修剪极端顺序统计量恢复渐近高斯性，解决传统FEP在无限方差分布中的失效问题。


<details>
  <summary>Details</summary>
Motivation: 经典函数经验过程（FEP）在处理重尾或偏态分布（如帕累托、柯西、低自由度t分布）时失效，因为这些分布的均值或方差可能无限或未定义，需要有限方差假设来保证渐近收敛。

Method: 提出修剪函数经验过程（TFEP），通过控制比例修剪极端顺序统计量来稳定经验过程，恢复渐近高斯行为。建立TFEP在温和正则条件下的弱收敛性，推导单样本和双样本问题的渐近分布。

Result: TFEP为重尾分布提供了截断均值、方差及其差异或比率的稳健置信区间。蒙特卡洛实验和塞内加尔收入数据的实证应用表明，TFEP在传统方法失效的情况下仍能提供准确推断。

Conclusion: TFEP为处理重尾和非标准环境中的统计推断提供了强大灵活的工具，在传统高斯方法和经典FEP失效的情况下仍能保持准确性和可靠性。

Abstract: This paper introduces the Trimmed Functional Empirical Process (TFEP) as a robust framework for statistical inference when dealing with heavy-tailed or skewed distributions, where classical moments such as the mean or variance may be infinite or undefined. Standard approaches including the classical Functional Empirical Process (FEP), break down under such conditions, especially for distributions like Pareto, Cauchy, low degree of freedom Student-t, due to their reliance on finite-variance assumptions to guarantee asymptotic convergence. The TFEP approach addresses these limitations by trimming a controlled proportion of extreme order statistics, thereby stabilizing the empirical process and restoring asymptotic Gaussian behavior. We establish the weak convergence of the TFEP under mild regularity conditions and derive new asymptotic distributions for one-sample and twosample problems. These theoretical developments lead to robust confidence intervals for truncated means, variances, and their differences or ratios. The efficiency and reliability of the TFEP are supported by extensive Monte Carlo experiments and an empirical application to Senegalese income data. In all scenarios, the TFEP provides accurate inference where both Gaussian-based methods and the classical FEP break down. The methodology thus offers a powerful and flexible tool for statistical analysis in heavy-tailed and non-standard environments.

</details>


### [44] [Surprisingly-early bias in forecasts for unscheduled events](https://arxiv.org/abs/2512.07575)
*Niklas V. Lehmann*

Main category: stat.ME

TL;DR: 当数据集中包含未计划事件（如自然灾害）的预测时，由于某些事件尚未发生，结果可能被审查或"隐藏"，这会导致影响预测准确性和校准感知的选择偏差。


<details>
  <summary>Details</summary>
Motivation: 研究当数据集中包含未计划事件的预测时，由于某些事件结果尚未发生（被审查），导致的选择偏差问题，这种偏差会影响对预测准确性和校准的评估。

Method: 通过排除那些结果验证时间异常早的预测来消除选择偏差。

Result: 发现未计划事件预测中的审查问题会导致选择偏差，影响预测准确性和校准的感知，但可以通过排除验证时间异常早的预测来消除这种偏差。

Conclusion: 在评估未计划事件的预测时，需要考虑结果审查导致的选择偏差，通过适当的方法（如排除验证时间异常早的预测）可以消除这种偏差，从而获得更准确的预测评估。

Abstract: When a dataset contains forecasts on unscheduled events, such as natural catastrophes, outcomes may be censored or ``hidden'' since some events have not yet occurred. This article finds that this can lead to a selection bias which affects the perceived accuracy and calibration of forecasts. This selection bias can be eliminated by excluding forecasts on outcomes which have been verified surprisingly early.

</details>
