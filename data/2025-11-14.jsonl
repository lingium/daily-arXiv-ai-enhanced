{"id": "2511.08717", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08717", "abs": "https://arxiv.org/abs/2511.08717", "authors": ["Yuxin Bai", "Aranyak Acharyya", "Ashwin De Silva", "Zeyu Shen", "James Hassett", "Joshua T. Vogelstein"], "title": "Optimal Control of the Future via Prospective Foraging", "comment": null, "summary": "Optimal control of the future is the next frontier for AI. Current approaches to this problem are typically rooted in either reinforcement learning or online learning. While powerful, these frameworks for learning are mathematically distinct from Probably Approximately Correct (PAC) learning, which has been the workhorse for the recent technological achievements in AI. We therefore build on the prior work of prospective learning, an extension of PAC learning (without control) in non-stationary environments (De Silva et al., 2023; Silva et al., 2024; Bai et al., 2026). Here, we further extend the PAC learning framework to address learning and control in non-stationary environments. Using this framework, called ''Prospective Control'', we prove that under certain fairly general assumptions, empirical risk minimization (ERM) asymptotically achieves the Bayes optimal policy. We then consider a specific instance of prospective control, foraging, which is a canonical task for any mobile agent, be it natural or artificial. We illustrate that existing reinforcement learning algorithms fail to learn in these non-stationary environments, and even with modifications, they are orders of magnitude less efficient than our prospective foraging agents. Code is available at: https://github.com/neurodata/ProspectiveLearningwithControl."}
{"id": "2511.08791", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08791", "abs": "https://arxiv.org/abs/2511.08791", "authors": ["Rocco A. Servedio"], "title": "The Probably Approximately Correct Learning Model in Computational Learning Theory", "comment": "45 pages, 4 figures", "summary": "This survey paper gives an overview of various known results on learning classes of Boolean functions in Valiant's Probably Approximately Correct (PAC) learning model and its commonly studied variants."}
{"id": "2511.08808", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08808", "abs": "https://arxiv.org/abs/2511.08808", "authors": ["Matheus Vinícius Barreto de Farias", "Mario de Castro"], "title": "Effects of label noise on the classification of outlier observations", "comment": "10 pages, 8 figures", "summary": "This study investigates the impact of adding noise to the training set classes in classification tasks using the BCOPS algorithm (Balanced and Conformal Optimized Prediction Sets), proposed by Guan & Tibshirani (2022). The BCOPS algorithm is an application of conformal prediction combined with a machine learning method to construct prediction sets such that the probability of the true class being included in the prediction set for a test observation meets a specified coverage guarantee. An observation is considered an outlier if its true class is not present in the training set. The study employs both synthetic and real datasets and conducts experiments to evaluate the prediction abstention rate for outlier observations and the model's robustness in this previously untested scenario. The results indicate that the addition of noise, even in small amounts, can have a significant effect on model performance."}
{"id": "2511.08991", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08991", "abs": "https://arxiv.org/abs/2511.08991", "authors": ["Puheng Li", "Tijana Zrnic", "Emmanuel Candès"], "title": "Robust Sampling for Active Statistical Inference", "comment": "NeurIPS 2025", "summary": "Active statistical inference is a new method for inference with AI-assisted data collection. Given a budget on the number of labeled data points that can be collected and assuming access to an AI predictive model, the basic idea is to improve estimation accuracy by prioritizing the collection of labels where the model is most uncertain. The drawback, however, is that inaccurate uncertainty estimates can make active sampling produce highly noisy results, potentially worse than those from naive uniform sampling. In this work, we present robust sampling strategies for active statistical inference. Robust sampling ensures that the resulting estimator is never worse than the estimator using uniform sampling. Furthermore, with reliable uncertainty estimates, the estimator usually outperforms standard active inference. This is achieved by optimally interpolating between uniform and active sampling, depending on the quality of the uncertainty scores, and by using ideas from robust optimization. We demonstrate the utility of the method on a series of real datasets from computational social science and survey research."}
{"id": "2511.08772", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.08772", "abs": "https://arxiv.org/abs/2511.08772", "authors": ["Myeonghun Yu", "Kean Ming Tan", "Huixia Judy Wang", "Wen-Xin Zhou"], "title": "Deep neural expected shortfall regression with tail-robustness", "comment": null, "summary": "Expected shortfall (ES), also known as conditional value-at-risk, is a widely recognized risk measure that complements value-at-risk by capturing tail-related risks more effectively. Compared with quantile regression, which has been extensively developed and applied across disciplines, ES regression remains in its early stage, partly because the traditional empirical risk minimization framework is not directly applicable. In this paper, we develop a nonparametric framework for expected shortfall regression based on a two-step approach that treats the conditional quantile function as a nuisance parameter. Leveraging the representational power of deep neural networks, we construct a two-step ES estimator using feedforward ReLU networks, which can alleviate the curse of dimensionality when the underlying functions possess hierarchical composition structures. However, ES estimation is inherently sensitive to heavy-tailed response or error distributions. To address this challenge, we integrate a properly tuned Huber loss into the neural network training, yielding a robust deep ES estimator that is provably resistant to heavy-tailedness in a non-asymptotic sense and first-order insensitive to quantile estimation errors in the first stage. Comprehensive simulation studies and an empirical analysis of the effect of El Niño on extreme precipitation illustrate the accuracy and robustness of the proposed method."}
{"id": "2511.08802", "categories": ["stat.AP", "q-bio.PE", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.08802", "abs": "https://arxiv.org/abs/2511.08802", "authors": ["Maxime Fajgenblat", "Marc Herremans", "Pieter Vanormelingen", "Kristijn Swinnen", "Dirk Maes", "Robby Stoks", "Luc De Meester", "Christel Faes", "Thomas Neyens"], "title": "Backcasting biodiversity at high spatiotemporal resolution using flexible site-occupancy models for opportunistically sampled citizen science data", "comment": null, "summary": "For many taxonomic groups, online biodiversity portals used by naturalists and citizen scientists constitute the primary source of distributional information. Over the last decade, site-occupancy models have been advanced as a promising framework to analyse such loosely structured, opportunistically collected datasets. Current approaches often ignore important aspects of the detection process and do not fully capitalise on the information present in these datasets, leaving opportunities for fine-grained spatiotemporal backcasting untouched. We propose a flexible Bayesian spatiotemporal site-occupancy model that aims to mimic the data-generating process that underlies common citizen science datasets sourced from public biodiversity portals, and yields rich biological output. We illustrate the use of the model to a dataset containing over 3M butterfly records in Belgium, collected through the citizen science data portal Observations.be. We show that the proposed approach enables retrospective predictions on the occupancy of species through time and space at high resolution, as well as inference on inter-annual distributional trends, range dynamics, habitat preferences, phenological patterns, detection patterns and observer heterogeneity. The proposed model can be used to increase the value of opportunistically collected data by naturalists and citizen scientists, and can aid the understanding of spatiotemporal dynamics of species for which rigorously collected data are absent or too costly to collect."}
{"id": "2511.08719", "categories": ["stat.ME", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.08719", "abs": "https://arxiv.org/abs/2511.08719", "authors": ["Rachel T Gonzalez", "Madeline R Abbott", "Brahmajee Nallamothu", "Scott Hummel", "Michael Dorsch", "Walter Dempsey"], "title": "Practical considerations when designing an online learning algorithm for an app-based mHealth intervention", "comment": null, "summary": "The ubiquitous nature of mobile health (mHealth) technology has expanded opportunities for the integration of reinforcement learning into traditional clinical trial designs, allowing researchers to learn individualized treatment policies during the study. LowSalt4Life 2 (LS4L2) is a recent trial aimed at reducing sodium intake among hypertensive individuals through an app-based intervention. A reinforcement learning algorithm, which was deployed in one of the trial arms, was designed to send reminder notifications to promote app engagement in contexts where the notification would be effective, i.e., when a participant is likely to open the app in the next 30-minute and not when prior data suggested reduced effectiveness. Such an algorithm can improve app-based mHealth interventions by reducing participant burden and more effectively promoting behavior change. We encountered various challenges during the implementation of the learning algorithm, which we present as a template to solving challenges in future trials that deploy reinforcement learning algorithms. We provide template solutions based on LS4L2 for solving the key challenges of (i) defining a relevant reward, (ii) determining a meaningful timescale for optimization, (iii) specifying a robust statistical model that allows for automation, (iv) balancing model flexibility with computational cost, and (v) addressing missing values in gradually collected data."}
{"id": "2511.09002", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09002", "abs": "https://arxiv.org/abs/2511.09002", "authors": ["Hongru Zhao", "Jinwen Fu", "Tuan Pham"], "title": "Convergence and Stability Analysis of Self-Consuming Generative Models with Heterogeneous Human Curation", "comment": "42 pages, 2 tables", "summary": "Self-consuming generative models have received significant attention over the last few years. In this paper, we study a self-consuming generative model with heterogeneous preferences that is a generalization of the model in Ferbach et al. (2024). The model is retrained round by round using real data and its previous-round synthetic outputs. The asymptotic behavior of the retraining dynamics is investigated across four regimes using different techniques including the nonlinear Perron--Frobenius theory. Our analyses improve upon that of Ferbach et al. (2024) and provide convergence results in settings where the well-known Banach contraction mapping arguments do not apply. Stability and non-stability results regarding the retraining dynamics are also given."}
{"id": "2511.08979", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.08979", "abs": "https://arxiv.org/abs/2511.08979", "authors": ["Feridun Tasdan", "Rukiye Dagalp"], "title": "Enhanced Rank-Based Correlation Estimation Using Smoothed Wilcoxon Rank Scores", "comment": null, "summary": "This article proposes an improved version of the Spearman rank correlation based on using Wilcoxon rank score function. A smoothed empirical cumulative distribution function (ecdf)computes the smoothed ranks and replaces the regular ranks in the Wilcoxon rank score function. The smoothed Wilcoxon rank scores are then used for estimation of the Spearman's correlation. The proposed approach is similar to the Spearman's rho estimator which uses ranks of the random samples of X and Y but the proposed method improves Spearman's approach such as handling ties and gaining higher efficiency under monotone associations. A Wald type hypothesis test has been proposed for the new estimator and the asymptotic properties are shown."}
{"id": "2511.08994", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08994", "abs": "https://arxiv.org/abs/2511.08994", "authors": ["Daijiro Kabata", "Mari Ito", "Tokito Koga", "Kazuma Yunoki"], "title": "Generalisable prediction model of surgical case duration: multicentre development and temporal validation", "comment": "20 pages, 2 tables, and 5 figures", "summary": "Background: Accurate prediction of surgical case duration underpins operating room (OR) scheduling, yet existing models often depend on site- or surgeon-specific inputs and rarely undergo external validation, limiting generalisability.\n  Methods: We undertook a retrospective multicentre study using routinely collected perioperative data from two general hospitals in Japan (development: 1 January 2021-31 December 2023; temporal test: 1 January-31 December 2024). Elective weekday procedures with American Society of Anesthesiologists (ASA) Physical Status 1-4 were included. Pre-specified preoperative predictors comprised surgical context (year, month, weekday, scheduled duration, general anaesthesia indicator, body position) and patient factors (sex, age, body mass index, allergy, infection, comorbidity, ASA). Missing data were addressed by multiple imputation by chained equations. Four learners (elastic-net, generalised additive models, random forest, gradient-boosted trees) were tuned within internal-external cross-validation (IECV; leave-one-cluster-out by centre-year) and combined by stacked generalisation to predict log-transformed duration.\n  Results: We analysed 63,206 procedures (development 45,647; temporal test 17,559). Cluster-specific and pooled errors and calibrations from IECV are provided with consistent performance across centres and years. In the 2024 temporal test cohort, calibration was good (intercept 0.423, 95%CI 0.372 to 0.474; slope 0.921, 95%CI 0.911 to 0.932).\n  Conclusions: A stacked machine-learning model using only widely available preoperative variables achieved accurate, well-calibrated predictions in temporal external validation, supporting transportability across sites and over time. Such general-purpose tools may improve OR scheduling without relying on idiosyncratic inputs."}
{"id": "2511.08772", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.08772", "abs": "https://arxiv.org/abs/2511.08772", "authors": ["Myeonghun Yu", "Kean Ming Tan", "Huixia Judy Wang", "Wen-Xin Zhou"], "title": "Deep neural expected shortfall regression with tail-robustness", "comment": null, "summary": "Expected shortfall (ES), also known as conditional value-at-risk, is a widely recognized risk measure that complements value-at-risk by capturing tail-related risks more effectively. Compared with quantile regression, which has been extensively developed and applied across disciplines, ES regression remains in its early stage, partly because the traditional empirical risk minimization framework is not directly applicable. In this paper, we develop a nonparametric framework for expected shortfall regression based on a two-step approach that treats the conditional quantile function as a nuisance parameter. Leveraging the representational power of deep neural networks, we construct a two-step ES estimator using feedforward ReLU networks, which can alleviate the curse of dimensionality when the underlying functions possess hierarchical composition structures. However, ES estimation is inherently sensitive to heavy-tailed response or error distributions. To address this challenge, we integrate a properly tuned Huber loss into the neural network training, yielding a robust deep ES estimator that is provably resistant to heavy-tailedness in a non-asymptotic sense and first-order insensitive to quantile estimation errors in the first stage. Comprehensive simulation studies and an empirical analysis of the effect of El Niño on extreme precipitation illustrate the accuracy and robustness of the proposed method."}
{"id": "2511.09118", "categories": ["stat.ML", "cs.LG", "hep-ex", "hep-ph"], "pdf": "https://arxiv.org/pdf/2511.09118", "abs": "https://arxiv.org/abs/2511.09118", "authors": ["Pietro Cappelli", "Gaia Grosso", "Marco Letizia", "Humberto Reyes-González", "Marco Zanetti"], "title": "Learning to Validate Generative Models: a Goodness-of-Fit Approach", "comment": "16 pages, 6 figures", "summary": "Generative models are increasingly central to scientific workflows, yet their systematic use and interpretation require a proper understanding of their limitations through rigorous validation. Classic approaches struggle with scalability, statistical power, or interpretability when applied to high-dimensional data, making it difficult to certify the reliability of these models in realistic, high-dimensional scientific settings. Here, we propose the use of the New Physics Learning Machine (NPLM), a learning based approach to goodness-of-fit testing inspired by the Neyman-Pearson construction, to test generative networks trained on high-dimensional scientific data. We demonstrate the performance of NPLM for validation in two benchmark cases: generative models trained on mixtures of Gaussian models with increasing dimensionality, and a public end-to-end generator for the Large Hadron Collider called FlashSim, trained on jet data, typical in the field of high-energy physics. We demonstrate that the NPLM can serve as a powerful validation method while also providing a means to diagnose sub-optimally modeled regions of the data."}
{"id": "2511.09305", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.09305", "abs": "https://arxiv.org/abs/2511.09305", "authors": ["Ryan Martin", "Naomi Singer", "Jonathan Williams"], "title": "Valid and efficient possibilistic structure learning in Gaussian linear regression", "comment": null, "summary": "A crucial step in fitting a regression model to data is determining the model's structure, i.e., the subset of explanatory variables to be included. However, the uncertainty in this step is often overlooked due to a lack of satisfactory methods. Frequentists have no broadly applicable confidence set constructions for a model's structure, and Bayesian posterior credible sets do not achieve the desired finite-sample coverage. In this paper, we propose an extension of the possibility-theoretic inferential model (IM) framework that offers reliable, data-driven uncertainty quantification about the unknown model structure. This particular extension allows for the inclusion of incomplete prior information about the unknown structure that facilitates regularization. We prove that this new, regularized, possibilistic IM's uncertainty quantification is suitably calibrated relative to the set of joint distributions compatible with the data-generating process and assumed partial prior knowledge about the structure. This implies, among other things, that the derived confidence sets for the unknown model structure attain the nominal coverage probability in finite samples. We provide background and guidance on quantifying prior knowledge in this new context and analyze two benchmark data sets, comparing our results to those obtained by existing methods."}
{"id": "2511.09023", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09023", "abs": "https://arxiv.org/abs/2511.09023", "authors": ["Ye Jin Choi", "Sebastian Kurtek", "Simeng Zhu", "Karthik Bharath"], "title": "Second-order spatial analysis of shapes of tumor cell nuclei", "comment": null, "summary": "Intra-tumor heterogeneity driving disease progression is characterized by distinct growth and spatial proliferation patterns of cells and their nuclei within tumor and non-tumor tissues. A widely accepted hypothesis is that these spatial patterns are correlated with morphology of the cells and their nuclei. Nevertheless, tools to quantify the correlation, with uncertainty, are scarce, and the state-of-the-art is based on low-dimensional numerical summaries of the shapes that are inadequate to fully encode shape information. To this end, we propose a marked point process framework to assess spatial correlation among shapes of planar closed curves, which represent cell or nuclei outlines. With shapes of curves as marks, the framework is based on a mark-weighted $K$ function, a second-order spatial statistic that accounts for the marks' variation by using test functions that capture only the shapes of cells and their nuclei. We then develop local and global hypothesis tests for spatial dependence between the marks using the $K$ function. The framework is brought to bear on the cell nuclei extracted from histopathology images of breast cancer, where we uncover distinct correlation patterns that are consistent with clinical expectations."}
{"id": "2511.08952", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.08952", "abs": "https://arxiv.org/abs/2511.08952", "authors": ["Shibo Diao"], "title": "A new approach to reliability assessment based on Exploratory factor analysis", "comment": null, "summary": "We need to collect data in any science and reliability is a fundamental problem for measurement in all of science. Reliability means calculation the variance ratio. Reliability was defined as the fraction of an observed score variance that was not error. here are a lot of methods to estimated reliability. All of these indicators of dependability and stability are in contradiction to the long held belief that a problem with test-retest reliability is that it introduces memory effects of learning and practice. As a result, Kuder and Richardson developed a method named KR20 before advances in computational speed made it trivial to find the factor structure of tests, and were based upon test and item variances. These procedures were essentially short cuts for estimating reliability. Exploratory factor analysis is also a Traditional method to calculate the reliability. It focus on only one variable in the liner model, a statistical method that can be used to collect an important type of validity evidence. but in reality, we need to focus on many more variables. So we will introduce a novel method following."}
{"id": "2511.09465", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09465", "abs": "https://arxiv.org/abs/2511.09465", "authors": ["Hedwig Nora Nordlinder", "Lukas Billera", "Jack Collier Ryder", "Anton Oresten", "Aron Stålmarck", "Theodor Mosetti Björk", "Ben Murrell"], "title": "Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions", "comment": "30 pages, 10 figures", "summary": "Diffusion and flow matching approaches to generative modeling have shown promise in domains where the state space is continuous, such as image generation or protein folding & design, and discrete, exemplified by diffusion large language models. They offer a natural fit when the number of elements in a state is fixed in advance (e.g. images), but require ad hoc solutions when, for example, the length of a response from a large language model, or the number of amino acids in a protein chain is not known a priori.\n  Here we propose Branching Flows, a generative modeling framework that, like diffusion and flow matching approaches, transports a simple distribution to the data distribution. But in Branching Flows, the elements in the state evolve over a forest of binary trees, branching and dying stochastically with rates that are learned by the model. This allows the model to control, during generation, the number of elements in the sequence. We also show that Branching Flows can compose with any flow matching base process on discrete sets, continuous Euclidean spaces, smooth manifolds, and `multimodal' product spaces that mix these components. We demonstrate this in three domains: small molecule generation (multimodal), antibody sequence generation (discrete), and protein backbone generation (multimodal), and show that Branching Flows is a capable distribution learner with a stable learning objective, and that it enables new capabilities."}
{"id": "2511.09500", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.09500", "abs": "https://arxiv.org/abs/2511.09500", "authors": ["Tengyuan Liang"], "title": "Distributional Shrinkage I: Universal Denoisers in Multi-Dimensions", "comment": "26 pages, 5 figures", "summary": "We revisit the problem of denoising from noisy measurements where only the noise level is known, not the noise distribution. In multi-dimensions, independent noise $Z$ corrupts the signal $X$, resulting in the noisy measurement $Y = X + σZ$, where $σ\\in (0, 1)$ is a known noise level. Our goal is to recover the underlying signal distribution $P_X$ from denoising $P_Y$. We propose and analyze universal denoisers that are agnostic to a wide range of signal and noise distributions. Our distributional denoisers offer order-of-magnitude improvements over the Bayes-optimal denoiser derived from Tweedie's formula, if the focus is on the entire distribution $P_X$ rather than on individual realizations of $X$. Our denoisers shrink $P_Y$ toward $P_X$ optimally, achieving $O(σ^4)$ and $O(σ^6)$ accuracy in matching generalized moments and density functions. Inspired by optimal transport theory, the proposed denoisers are optimal in approximating the Monge-Ampère equation with higher-order accuracy, and can be implemented efficiently via score matching.\n  Let $q$ represent the density of $P_Y$; for optimal distributional denoising, we recommend replacing the Bayes-optimal denoiser, \\[ \\mathbf{T}^*(y) = y + σ^2 \\nabla \\log q(y), \\] with denoisers exhibiting less aggressive distributional shrinkage, \\[ \\mathbf{T}_1(y) = y + \\frac{σ^2}{2} \\nabla \\log q(y), \\] \\[ \\mathbf{T}_2(y) = y + \\frac{σ^2}{2} \\nabla \\log q(y) - \\frac{σ^4}{8} \\nabla \\left( \\frac{1}{2} \\| \\nabla \\log q(y) \\|^2 + \\nabla \\cdot \\nabla \\log q(y) \\right) . \\]"}
{"id": "2511.09237", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09237", "abs": "https://arxiv.org/abs/2511.09237", "authors": ["Bing Liu", "Yuan Liao", "Sonia Yeh", "Oded Cats", "Kristian S. Nielsen", "Zhenning Dong", "Yong Wang", "Yi Li", "Yanli Liu", "Zirui Ni", "Xiaolei Ma"], "title": "Scaling behavioral incentives for low-carbon mobility through digital platforms", "comment": "18 pages, 5 Figures", "summary": "Meeting global carbon reduction targets requires large-scale behavioral shifts in everyday travel. Yet, real-world evidence on how to motivate such large-scale behavioral change remains scarce. We evaluate a carbon incentive program embedded in a MaaS platform in Beijing, China, using data from 3.9 million participants and 4.8 billion multimodal trips over 395 days. The program increased reported public transport and bike travel by 20.3% per month and reduced gasoline car use by 1.8% per day, yielding an annual carbon reduction of ~94,000 tons, or 5.7% of certified reductions in Beijing's carbon market. Although effects diminished over time, participants still made 12.8% more green trips per month after eight months, indicating persistence. These results provide the first large-scale empirical evidence of carbon incentives in MaaS and highlight their potential to inform targeted, city-specific interventions that can scale to support global low-carbon mobility transitions."}
{"id": "2511.08957", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.08957", "abs": "https://arxiv.org/abs/2511.08957", "authors": ["Thu Nguyen", "Lam Si Tung Ho"], "title": "rfBLT: Random Feature Bayesian Lasso Takens Model for time series forecasting", "comment": null, "summary": "Time series prediction is challenging due to our limited understanding of the underlying dynamics. Conventional models such as ARIMA and Holt's linear trend model experience difficulty in identifying nonlinear patterns in time series. In contrast, machine learning models excel at learning complex patterns and handling high-dimensional data; however, they are unable to quantify the uncertainty associated with their predictions, as statistical models do. To overcome these drawbacks, we propose Random Feature Bayesian Lasso Takens (rfBLT) for forecasting time series data. This non-parametric model captures the underlying system via the Takens' theorem and measures the degree of uncertainty with credible intervals. This is achieved by projecting delay embeddings into a higher-dimensional space via random features and applying regularization within the Bayesian framework to identify relevant terms. Our results demonstrate that the rfBLT method is comparable to traditional statistical models on simulated data, while significantly outperforming both conventional and machine learning models when evaluated on real-world data. The proposed algorithm is implemented in an R package, rfBLT."}
{"id": "2511.09486", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09486", "abs": "https://arxiv.org/abs/2511.09486", "authors": ["Antonio Di Noia", "Federico Ravenda", "Antonietta Mira"], "title": "A general framework for adaptive nonparametric dimensionality reduction", "comment": null, "summary": "Dimensionality reduction is a fundamental task in modern data science. Several projection methods specifically tailored to take into account the non-linearity of the data via local embeddings have been proposed. Such methods are often based on local neighbourhood structures and require tuning the number of neighbours that define this local structure, and the dimensionality of the lower-dimensional space onto which the data are projected. Such choices critically influence the quality of the resulting embedding. In this paper, we exploit a recently proposed intrinsic dimension estimator which also returns the optimal locally adaptive neighbourhood sizes according to some desirable criteria. In principle, this adaptive framework can be employed to perform an optimal hyper-parameter tuning of any dimensionality reduction algorithm that relies on local neighbourhood structures. Numerical experiments on both real-world and simulated datasets show that the proposed method can be used to significantly improve well-known projection methods when employed for various learning tasks, with improvements measurable through both quantitative metrics and the quality of low-dimensional visualizations."}
{"id": "2511.09457", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09457", "abs": "https://arxiv.org/abs/2511.09457", "authors": ["Koen W. van Arem", "Jakob Söhl", "Mirjam Bruinsma", "Geurt Jongbloed"], "title": "The trade-off between model flexibility and accuracy of the Expected Threat model in football", "comment": "In Conference Proceedings MathSport International 2025 (pp. 150-155)", "summary": "With an average football (soccer) match recording over 3,000 on-ball events, effective use of this event data is essential for practitioners at football clubs to obtain meaningful insights. Models can extract more information from this data, and explainable methods can make them more accessible to practitioners. The Expected Threat model has been praised for its explainability and offers an accessible option. However, selecting the grid size is a challenging key design choice that has to be made when applying the Expected Threat model. Using a finer grid leads to a more flexible model that can better distinguish between different situations, but the accuracy of the estimates deteriorates with a more flexible model. Consequently, practitioners face challenges in balancing the trade-off between model flexibility and model accuracy. In this study, the Expected Threat model is analyzed from a theoretical perspective and simulations are performed based on the Markov chain of the model to examine its behavior in practice. Our theoretical results establish an upper bound on the error of the Expected Threat model for different flexibilities. Based on the simulations, a more accurate characterization of the model's error is provided, improving over the theoretical bound. Finally, these insights are converted into a practical rule of thumb to help practitioners choose the right balance between the model flexibility and the desired accuracy of the Expected Threat model."}
{"id": "2511.08979", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.08979", "abs": "https://arxiv.org/abs/2511.08979", "authors": ["Feridun Tasdan", "Rukiye Dagalp"], "title": "Enhanced Rank-Based Correlation Estimation Using Smoothed Wilcoxon Rank Scores", "comment": null, "summary": "This article proposes an improved version of the Spearman rank correlation based on using Wilcoxon rank score function. A smoothed empirical cumulative distribution function (ecdf)computes the smoothed ranks and replaces the regular ranks in the Wilcoxon rank score function. The smoothed Wilcoxon rank scores are then used for estimation of the Spearman's correlation. The proposed approach is similar to the Spearman's rho estimator which uses ranks of the random samples of X and Y but the proposed method improves Spearman's approach such as handling ties and gaining higher efficiency under monotone associations. A Wald type hypothesis test has been proposed for the new estimator and the asymptotic properties are shown."}
{"id": "2511.09500", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.09500", "abs": "https://arxiv.org/abs/2511.09500", "authors": ["Tengyuan Liang"], "title": "Distributional Shrinkage I: Universal Denoisers in Multi-Dimensions", "comment": "26 pages, 5 figures", "summary": "We revisit the problem of denoising from noisy measurements where only the noise level is known, not the noise distribution. In multi-dimensions, independent noise $Z$ corrupts the signal $X$, resulting in the noisy measurement $Y = X + σZ$, where $σ\\in (0, 1)$ is a known noise level. Our goal is to recover the underlying signal distribution $P_X$ from denoising $P_Y$. We propose and analyze universal denoisers that are agnostic to a wide range of signal and noise distributions. Our distributional denoisers offer order-of-magnitude improvements over the Bayes-optimal denoiser derived from Tweedie's formula, if the focus is on the entire distribution $P_X$ rather than on individual realizations of $X$. Our denoisers shrink $P_Y$ toward $P_X$ optimally, achieving $O(σ^4)$ and $O(σ^6)$ accuracy in matching generalized moments and density functions. Inspired by optimal transport theory, the proposed denoisers are optimal in approximating the Monge-Ampère equation with higher-order accuracy, and can be implemented efficiently via score matching.\n  Let $q$ represent the density of $P_Y$; for optimal distributional denoising, we recommend replacing the Bayes-optimal denoiser, \\[ \\mathbf{T}^*(y) = y + σ^2 \\nabla \\log q(y), \\] with denoisers exhibiting less aggressive distributional shrinkage, \\[ \\mathbf{T}_1(y) = y + \\frac{σ^2}{2} \\nabla \\log q(y), \\] \\[ \\mathbf{T}_2(y) = y + \\frac{σ^2}{2} \\nabla \\log q(y) - \\frac{σ^4}{8} \\nabla \\left( \\frac{1}{2} \\| \\nabla \\log q(y) \\|^2 + \\nabla \\cdot \\nabla \\log q(y) \\right) . \\]"}
{"id": "2511.08719", "categories": ["stat.ME", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.08719", "abs": "https://arxiv.org/abs/2511.08719", "authors": ["Rachel T Gonzalez", "Madeline R Abbott", "Brahmajee Nallamothu", "Scott Hummel", "Michael Dorsch", "Walter Dempsey"], "title": "Practical considerations when designing an online learning algorithm for an app-based mHealth intervention", "comment": null, "summary": "The ubiquitous nature of mobile health (mHealth) technology has expanded opportunities for the integration of reinforcement learning into traditional clinical trial designs, allowing researchers to learn individualized treatment policies during the study. LowSalt4Life 2 (LS4L2) is a recent trial aimed at reducing sodium intake among hypertensive individuals through an app-based intervention. A reinforcement learning algorithm, which was deployed in one of the trial arms, was designed to send reminder notifications to promote app engagement in contexts where the notification would be effective, i.e., when a participant is likely to open the app in the next 30-minute and not when prior data suggested reduced effectiveness. Such an algorithm can improve app-based mHealth interventions by reducing participant burden and more effectively promoting behavior change. We encountered various challenges during the implementation of the learning algorithm, which we present as a template to solving challenges in future trials that deploy reinforcement learning algorithms. We provide template solutions based on LS4L2 for solving the key challenges of (i) defining a relevant reward, (ii) determining a meaningful timescale for optimization, (iii) specifying a robust statistical model that allows for automation, (iv) balancing model flexibility with computational cost, and (v) addressing missing values in gradually collected data."}
{"id": "2511.09024", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09024", "abs": "https://arxiv.org/abs/2511.09024", "authors": ["Simon Kuang", "Xinfan Lin"], "title": "Instrumental variables system identification with $L^p$ consistency", "comment": null, "summary": "Instrumental variables (eliminate the bias that afflicts least-squares identification of dynamical systems through noisy data, yet traditionally relies on external instruments that are seldom available for nonlinear time series data. We propose an IV estimator that synthesizes instruments from the data. We establish finite-sample $L^{p}$ consistency for all $p \\ge 1$ in both discrete- and continuous-time models, recovering a nonparametric $\\sqrt{n}$-convergence rate. On a forced Lorenz system our estimator reduces parameter bias by 200x (continuous-time) and 500x (discrete-time) relative to least squares and reduces RMSE by up to tenfold. Because the method only assumes that the model is linear in the unknown parameters, it is broadly applicable to modern sparsity-promoting dynamics learning models."}
{"id": "2511.08719", "categories": ["stat.ME", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.08719", "abs": "https://arxiv.org/abs/2511.08719", "authors": ["Rachel T Gonzalez", "Madeline R Abbott", "Brahmajee Nallamothu", "Scott Hummel", "Michael Dorsch", "Walter Dempsey"], "title": "Practical considerations when designing an online learning algorithm for an app-based mHealth intervention", "comment": null, "summary": "The ubiquitous nature of mobile health (mHealth) technology has expanded opportunities for the integration of reinforcement learning into traditional clinical trial designs, allowing researchers to learn individualized treatment policies during the study. LowSalt4Life 2 (LS4L2) is a recent trial aimed at reducing sodium intake among hypertensive individuals through an app-based intervention. A reinforcement learning algorithm, which was deployed in one of the trial arms, was designed to send reminder notifications to promote app engagement in contexts where the notification would be effective, i.e., when a participant is likely to open the app in the next 30-minute and not when prior data suggested reduced effectiveness. Such an algorithm can improve app-based mHealth interventions by reducing participant burden and more effectively promoting behavior change. We encountered various challenges during the implementation of the learning algorithm, which we present as a template to solving challenges in future trials that deploy reinforcement learning algorithms. We provide template solutions based on LS4L2 for solving the key challenges of (i) defining a relevant reward, (ii) determining a meaningful timescale for optimization, (iii) specifying a robust statistical model that allows for automation, (iv) balancing model flexibility with computational cost, and (v) addressing missing values in gradually collected data."}
{"id": "2511.09353", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09353", "abs": "https://arxiv.org/abs/2511.09353", "authors": ["Yujing Gao", "Xiang Zhang", "Shu Yang"], "title": "Designing Efficient Hybrid and Single-Arm Trials: External Control Borrowing and Sample Size Calculation", "comment": null, "summary": "External controls (ECs) from historical clinical trials or real-world data have gained increasing attention as a way to augment hybrid and single-arm trials, especially when balanced randomization is infeasible. While most existing work has focused on post-trial inference using ECs, their role in prospective trial design remains less explored. We propose a unified experimental design framework that encompasses standard randomized controlled trials (RCTs), hybrid trials, and single-arm trials, focusing on sample size determination and power analysis. Building on estimators derived from the efficient influence function, we develop hybrid and single-arm design strategies that leverage comparable EC data to reduce the required sample size of the current study. We derive asymptotic variance expressions for these estimators in terms of interpretable, population-level quantities and introduce a pre-experimental variance estimation procedure to guide sample size calculation, ensuring prespecified type I error and power for the relevant hypothesis test. Simulation studies demonstrate that the proposed hybrid and single-arm designs maintain valid type I error and achieve target power across diverse scenarios while requiring substantially fewer subjects in the current study than RCT designs. A real data application further illustrates the practical utility and advantages of the proposed hybrid and single-arm designs."}
{"id": "2511.09215", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09215", "abs": "https://arxiv.org/abs/2511.09215", "authors": ["Zhichao Jiang", "Peng Ding"], "title": "Principled analysis of crossover designs: causal effects, efficient estimation, and robust inference", "comment": null, "summary": "Crossover designs randomly assign each unit to receive a sequence of treatments. By comparing outcomes within the same unit, these designs can effectively eliminate between-unit variation and facilitate the identification of both instantaneous effects of current treatments and carryover effects from past treatments. They are widely used in traditional biomedical studies and are increasingly adopted in modern digital platforms. However, standard analyses of crossover designs often rely on strong parametric models, making inference vulnerable to model misspecification. This paper adopts a design-based framework to analyze general crossover designs. We make two main contributions. First, we use potential outcomes to formally define the causal estimands and assumptions on the data-generating process. For any given type of crossover design and assumptions on potential outcomes, we outline a procedure for identification and estimation, emphasizing the central role of the treatment assignment mechanism in design-based inference. Second, we unify the analysis of crossover designs using least squares, with restrictions on the coefficients and weights on the units. Based on the theory, we recommend the specification of the regression function, weighting scheme, and coefficient restrictions to assess identifiability, construct efficient estimators, and estimate variances in a unified fashion. Crucially, the least squares procedure is simple to implement, and yields not only consistent and efficient point estimates but also valid variance estimates even when the working regression model is misspecified."}
{"id": "2511.09398", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09398", "abs": "https://arxiv.org/abs/2511.09398", "authors": ["James Hugh McVittie", "Archer Gong Zhang"], "title": "Density ratio model for multiple types of survival data with empirical likelihood", "comment": null, "summary": "The density ratio model (DRM) is a semiparametric model that relates the distributions from multiple samples to a nonparametrically defined reference distribution via exponential tilting, with finite-dimensional parameters governing their differences in shape. When multiple types of partially observed (censored/truncated) failure time data are collected in an observational study, the DRM can be utilized to conduct a single unified analysis of the combined data. In this paper, we extend the methodology for censored length-biased/truncated data to the DRM framework and formulate the inference using empirical likelihood. We develop an EM algorithm to compute the DRM-based maximum empirical likelihood estimators of the model parameters and survival function, and assess its performance through extensive simulations under correct model specification, overspecification, and misspecification, across a range of failure-time distributions and censoring proportions. We also illustrate the efficacy of our method by analyzing the duration of time spent from admission to discharge in a Montreal-area hospital in Canada. The R code that implements our method is available on GitHub at \\href{https://github.com/gozhang/DRM-combined-survival}{DRM-combined-survival}."}
{"id": "2511.09305", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.09305", "abs": "https://arxiv.org/abs/2511.09305", "authors": ["Ryan Martin", "Naomi Singer", "Jonathan Williams"], "title": "Valid and efficient possibilistic structure learning in Gaussian linear regression", "comment": null, "summary": "A crucial step in fitting a regression model to data is determining the model's structure, i.e., the subset of explanatory variables to be included. However, the uncertainty in this step is often overlooked due to a lack of satisfactory methods. Frequentists have no broadly applicable confidence set constructions for a model's structure, and Bayesian posterior credible sets do not achieve the desired finite-sample coverage. In this paper, we propose an extension of the possibility-theoretic inferential model (IM) framework that offers reliable, data-driven uncertainty quantification about the unknown model structure. This particular extension allows for the inclusion of incomplete prior information about the unknown structure that facilitates regularization. We prove that this new, regularized, possibilistic IM's uncertainty quantification is suitably calibrated relative to the set of joint distributions compatible with the data-generating process and assumed partial prior knowledge about the structure. This implies, among other things, that the derived confidence sets for the unknown model structure attain the nominal coverage probability in finite samples. We provide background and guidance on quantifying prior knowledge in this new context and analyze two benchmark data sets, comparing our results to those obtained by existing methods."}
{"id": "2511.09307", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09307", "abs": "https://arxiv.org/abs/2511.09307", "authors": ["Christophe Biscio", "Frédéric Lavancier"], "title": "Nonparametric intensity estimation of spatial point processes by random forests", "comment": null, "summary": "We propose a random forest estimator for the intensity of spatial point processes, applicable with or without covariates. It retains the well-known advantages of a random forest approach, including the ability to handle a large number of covariates, out-of-bag cross-validation, and variable importance assessment. Importantly, even in the absence of covariates, it requires no border correction and adapts naturally to irregularly shaped domains and manifolds. Consistency and convergence rates are established under various asymptotic regimes, revealing the benefit of using covariates when available. Numerical experiments illustrate the methodology and demonstrate that it performs competitively with state-of-the-art methods."}
{"id": "2511.09353", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09353", "abs": "https://arxiv.org/abs/2511.09353", "authors": ["Yujing Gao", "Xiang Zhang", "Shu Yang"], "title": "Designing Efficient Hybrid and Single-Arm Trials: External Control Borrowing and Sample Size Calculation", "comment": null, "summary": "External controls (ECs) from historical clinical trials or real-world data have gained increasing attention as a way to augment hybrid and single-arm trials, especially when balanced randomization is infeasible. While most existing work has focused on post-trial inference using ECs, their role in prospective trial design remains less explored. We propose a unified experimental design framework that encompasses standard randomized controlled trials (RCTs), hybrid trials, and single-arm trials, focusing on sample size determination and power analysis. Building on estimators derived from the efficient influence function, we develop hybrid and single-arm design strategies that leverage comparable EC data to reduce the required sample size of the current study. We derive asymptotic variance expressions for these estimators in terms of interpretable, population-level quantities and introduce a pre-experimental variance estimation procedure to guide sample size calculation, ensuring prespecified type I error and power for the relevant hypothesis test. Simulation studies demonstrate that the proposed hybrid and single-arm designs maintain valid type I error and achieve target power across diverse scenarios while requiring substantially fewer subjects in the current study than RCT designs. A real data application further illustrates the practical utility and advantages of the proposed hybrid and single-arm designs."}
{"id": "2511.09398", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09398", "abs": "https://arxiv.org/abs/2511.09398", "authors": ["James Hugh McVittie", "Archer Gong Zhang"], "title": "Density ratio model for multiple types of survival data with empirical likelihood", "comment": null, "summary": "The density ratio model (DRM) is a semiparametric model that relates the distributions from multiple samples to a nonparametrically defined reference distribution via exponential tilting, with finite-dimensional parameters governing their differences in shape. When multiple types of partially observed (censored/truncated) failure time data are collected in an observational study, the DRM can be utilized to conduct a single unified analysis of the combined data. In this paper, we extend the methodology for censored length-biased/truncated data to the DRM framework and formulate the inference using empirical likelihood. We develop an EM algorithm to compute the DRM-based maximum empirical likelihood estimators of the model parameters and survival function, and assess its performance through extensive simulations under correct model specification, overspecification, and misspecification, across a range of failure-time distributions and censoring proportions. We also illustrate the efficacy of our method by analyzing the duration of time spent from admission to discharge in a Montreal-area hospital in Canada. The R code that implements our method is available on GitHub at \\href{https://github.com/gozhang/DRM-combined-survival}{DRM-combined-survival}."}
{"id": "2511.09449", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09449", "abs": "https://arxiv.org/abs/2511.09449", "authors": ["Remi Luschei", "Werner Brannath"], "title": "Family-wise error rate control in clinical trials with overlapping populations", "comment": null, "summary": "We consider clinical trials with multiple, overlapping patient populations, that test multiple treatment policies specifically tailored to these populations. Such designs may lead to multiplicity issues, as false statements will affect several populations. For type I error control, often the family-wise error rate (FWER) is controlled, which is the probability to reject at least one true null hypothesis. If the joint distribution of the test statistics is known, the FWER level can be exhausted by determining critical values or adjusted $α$-levels. The adjustment is typically done under the common ANOVA assumptions. However, the performed tests are then only valid under the rather strong assumption of homogeneous null effects, i.e., when the null hypothesis applies to all subpopulations and their intersections. We show that under cancelling null effects, when heterogeneous effects cancel out in some or all subpopulations, this procedure does not provide FWER control. We also suggest different alternatives and compare them in terms of FWER control and their power."}
{"id": "2511.09542", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09542", "abs": "https://arxiv.org/abs/2511.09542", "authors": ["Jingyang Li", "Yang Chen"], "title": "Local Interaction Autoregressive Model for High Dimension Time Series Data", "comment": null, "summary": "High-dimensional matrix and tensor time series often exhibit local dependency, where each entry interacts mainly with a small neighborhood. Accounting for local interactions in a prediction model can greatly reduce the dimensionality of the parameter space, leading to more efficient inference and more accurate predictions. We propose a Local Interaction Autoregressive (LIAR) framework and study Separable LIAR, a variant with shared row and column components, for high-dimensional matrix/tensor time series forecasting problems. We derive a scalable parameter estimation algorithm via parallel least squares with a BIC-type neighborhood selector. Theoretically, we show consistency of neighborhood selection and derive error bounds for kernel and auto-covariance estimation. Numerical simulations show that the BIC selector recovers the true neighborhood with high success rates, the LIAR achieves small estimation errors, and the forecasts outperform matrix time-series baselines. In real data applications, a Total Electron Content (TEC) case study shows the model can identify localized spatio-temporal propagation and improved prediction as compared with non-local time series prediction models."}
{"id": "2511.09457", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09457", "abs": "https://arxiv.org/abs/2511.09457", "authors": ["Koen W. van Arem", "Jakob Söhl", "Mirjam Bruinsma", "Geurt Jongbloed"], "title": "The trade-off between model flexibility and accuracy of the Expected Threat model in football", "comment": "In Conference Proceedings MathSport International 2025 (pp. 150-155)", "summary": "With an average football (soccer) match recording over 3,000 on-ball events, effective use of this event data is essential for practitioners at football clubs to obtain meaningful insights. Models can extract more information from this data, and explainable methods can make them more accessible to practitioners. The Expected Threat model has been praised for its explainability and offers an accessible option. However, selecting the grid size is a challenging key design choice that has to be made when applying the Expected Threat model. Using a finer grid leads to a more flexible model that can better distinguish between different situations, but the accuracy of the estimates deteriorates with a more flexible model. Consequently, practitioners face challenges in balancing the trade-off between model flexibility and model accuracy. In this study, the Expected Threat model is analyzed from a theoretical perspective and simulations are performed based on the Markov chain of the model to examine its behavior in practice. Our theoretical results establish an upper bound on the error of the Expected Threat model for different flexibilities. Based on the simulations, a more accurate characterization of the model's error is provided, improving over the theoretical bound. Finally, these insights are converted into a practical rule of thumb to help practitioners choose the right balance between the model flexibility and the desired accuracy of the Expected Threat model."}
{"id": "2511.09486", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09486", "abs": "https://arxiv.org/abs/2511.09486", "authors": ["Antonio Di Noia", "Federico Ravenda", "Antonietta Mira"], "title": "A general framework for adaptive nonparametric dimensionality reduction", "comment": null, "summary": "Dimensionality reduction is a fundamental task in modern data science. Several projection methods specifically tailored to take into account the non-linearity of the data via local embeddings have been proposed. Such methods are often based on local neighbourhood structures and require tuning the number of neighbours that define this local structure, and the dimensionality of the lower-dimensional space onto which the data are projected. Such choices critically influence the quality of the resulting embedding. In this paper, we exploit a recently proposed intrinsic dimension estimator which also returns the optimal locally adaptive neighbourhood sizes according to some desirable criteria. In principle, this adaptive framework can be employed to perform an optimal hyper-parameter tuning of any dimensionality reduction algorithm that relies on local neighbourhood structures. Numerical experiments on both real-world and simulated datasets show that the proposed method can be used to significantly improve well-known projection methods when employed for various learning tasks, with improvements measurable through both quantitative metrics and the quality of low-dimensional visualizations."}
