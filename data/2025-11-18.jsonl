{"id": "2511.10824", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10824", "abs": "https://arxiv.org/abs/2511.10824", "authors": ["Inga Girshfeld", "Xiaohui Chen"], "title": "Neural Local Wasserstein Regression", "comment": "Accepted to TAG-DS 2025. 11 pages, 3 figures", "summary": "We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \\emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods."}
{"id": "2511.10919", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10919", "abs": "https://arxiv.org/abs/2511.10919", "authors": ["Jialei Liu", "Jun Liao", "Kuangnan Fang"], "title": "Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data", "comment": null, "summary": "Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments."}
{"id": "2511.11161", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.11161", "abs": "https://arxiv.org/abs/2511.11161", "authors": ["Yuzhen Zhao", "Yating Liu", "Marc Hoffmann"], "title": "Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths", "comment": null, "summary": "This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings."}
{"id": "2511.10967", "categories": ["stat.CO", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.10967", "abs": "https://arxiv.org/abs/2511.10967", "authors": ["Jingyi Zhang", "James C. Spall"], "title": "Autocovariance and Optimal Design for Random Walk Metropolis-Hastings Algorithm", "comment": null, "summary": "The Metropolis-Hastings algorithm has been extensively studied in the estimation and simulation literature, with most prior work focusing on convergence behavior and asymptotic theory. However, its covariance structure-an important statistical property for both theory and implementation-remains less understood. In this work, we provide new theoretical insights into the scalar case, focusing primarily on symmetric unimodal target distributions with symmetric random walk proposals, where we also establish an optimal proposal design. In addition, we derive some more general results beyond this setting. For the high-dimensional case, we relate the covariance matrix to the classical 0.23 average acceptance rate tuning criterion."}
{"id": "2511.11294", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11294", "abs": "https://arxiv.org/abs/2511.11294", "authors": ["Bertille Tierny", "Arthur Charpentier", "François Hu"], "title": "Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint", "comment": null, "summary": "Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models."}
{"id": "2511.11161", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.11161", "abs": "https://arxiv.org/abs/2511.11161", "authors": ["Yuzhen Zhao", "Yating Liu", "Marc Hoffmann"], "title": "Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths", "comment": null, "summary": "This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings."}
{"id": "2511.10719", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.10719", "abs": "https://arxiv.org/abs/2511.10719", "authors": ["Brianne Weaver", "Brigg Trendler", "Chris Groendyke", "Brian Hartman", "Robert Richardson", "Davey Erekson"], "title": "Modeling U.S. Mortality and Suicide Rates by Integrating Mental Health and Socio-Economic Indicators", "comment": "29 pages, 15 figures", "summary": "Accurate mortality modeling is central to actuarial science and public health, especially as mental health emerges as a significant factor in population outcomes. This paper develops and applies a Bayesian hierarchical model to analyze U.S. county-level mortality and suicide rates from 2010 to 2023. Applying a conditional autoregressive (CAR) structure to each combination of sex and age grouping, the model captures spatial and temporal trends while incorporating mental health surveillance data and socio-economic indicators. We first assess socio-economic covariates in predicting suicide. While the results vary considerably by age and sex, we find that the county-wide levels of educational attainment, housing prices, marriage rates, racial composition, household size, and poor mental health days all have significant relationships with suicide rates. We next consider the impact of various mental health indicators on all-cause and suicide-specific mortality and find that the strongest effects are observed in younger populations. The spatial and temporal correlation structures reveal substantial regional clustering and time-consistent trends in both all-cause mortality and suicide rates, supporting the use of spatio-temporal methods. Our findings highlight the value of integrating mental health surveillance data into mortality models to better identify emerging risk areas and vulnerable populations. This approach has the potential to inform public health policy, resource allocation, and targeted interventions aimed at reducing disparities in mortality and suicide across U.S. communities."}
{"id": "2511.10911", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.10911", "abs": "https://arxiv.org/abs/2511.10911", "authors": ["Baoshan Zhang", "Sean M. O'Brien", "Yuan Wu", "Laine E. Thomas"], "title": "Improving Variance and Confidence Interval Estimation in Small-Sample Propensity Score Analyses: Bootstrap vs. Asymptotic Methods", "comment": null, "summary": "Propensity score (PS) methods are widely used to estimate treatment effects in non-randomized studies. Variance is typically estimated using sandwich or bootstrap methods, which can either treat the PS as estimated or fixed. The latter is thought to be conservative. Comparisons between the sandwich and bootstrap estimators have been compared in moderate to large sample sizes, favoring the bootstrap estimator. With the growing interest in treatments for rare disease and externally controlled clinical trials, very small sample sizes are not uncommon and the asymptotic properties of sandwich estimators may not hold. Bootstrap methods that allow for PS re-estimation can also generate problems with quasi-separation in small samples. It is unclear whether it is safe to prefer sandwich estimators or to assume that treating the PS as fixed is conservative. We conducted a Monte Carlo simulation to compare the performance of bootstrap versus sandwich variance and CI estimators for average treatment effects estimated with PS methods. We systematically evaluated the impact of treating the PS as fixed versus re-estimating it. These methodological comparisons were performed using Inverse Probability of Treatment Weighting (IPTW) and Augmented Inverse Probability of Treatment Weighting (AIPW) estimators. Simulations assessed performance under various conditions, including small sample sizes and different outcome and treatment prevalences. We illustrate the differences in our motivating example, the LIMIT-JIA trial. We show that the sandwich estimators can perform quite poorly in small samples, and fixed PS methods are not necessarily conservative. A stratified bootstrap avoids quasi-separation and performs well. Differences were large enough to alter statistical conclusions in our motivating example, LIMIT-JIA."}
{"id": "2511.10950", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.10950", "abs": "https://arxiv.org/abs/2511.10950", "authors": ["Ayumi Mutoh", "Junoh Heo"], "title": "Influence of Prior Distributions on Gaussian Process Hyperparameter Inference", "comment": "26 pages, 5 figures", "summary": "Gaussian processes (GPs) are widely used metamodels for approximating expensive computer simulations, particularly in engineering design and spatial prediction. However, their performance can deteriorate significantly when covariance parameters are poorly estimated, highlighting the importance of accurate inference. The most common approach involves maximizing the marginal likelihood, yielding point estimates of these parameters. However, this approach is highly sensitive to initialization and optimization settings. An alternative is to adopt a fully Bayesian hierarchical framework, where the posterior distribution over the covariance parameters is inferred. This approach provides more robust uncertainty quantification and reduces sensitivity to parameter selection. Yet, a key challenge lies in the careful specification of prior distributions for these parameters. While many available software packages provide default priors, their influence on model behavior is often underexplored. Additionally, the choice of proposal distributions can also influence sampling efficiency and convergence. In this paper, we examine how different prior and proposal distributions over the lengthscale parameters $θ$ affect predictive performance in a hierarchical GP model, using both simulated and real data experiments. By evaluating various types of priors and proposals, we aim to better understand their influence on predictive accuracy and uncertainty quantification."}
{"id": "2511.11318", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11318", "abs": "https://arxiv.org/abs/2511.11318", "authors": ["Derun Zhou", "Keisuke Yano", "Mahito Sugiyama"], "title": "Dual Riemannian Newton Method on Statistical Manifolds", "comment": null, "summary": "In probabilistic modeling, parameter estimation is commonly formulated as a minimization problem on a parameter manifold. Optimization in such spaces requires geometry-aware methods that respect the underlying information structure. While the natural gradient leverages the Fisher information metric as a form of Riemannian gradient descent, it remains a first-order method and often exhibits slow convergence near optimal solutions. Existing second-order manifold algorithms typically rely on the Levi-Civita connection, thus overlooking the dual-connection structure that is central to information geometry. We propose the dual Riemannian Newton method, a Newton-type optimization algorithm on manifolds endowed with a metric and a pair of dual affine connections. The dual Riemannian Newton method explicates how duality shapes second-order updates: when the retraction (a local surrogate of the exponential map) is defined by one connection, the associated Newton equation is posed with its dual. We establish local quadratic convergence and validate the theory with experiments on representative statistical models. Thus, the dual Riemannian Newton method thus delivers second-order efficiency while remaining compatible with the dual structures that underlie modern information-geometric learning and inference."}
{"id": "2511.11338", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.11338", "abs": "https://arxiv.org/abs/2511.11338", "authors": ["Stéphane Girard", "Cambyse Pakzad"], "title": "Extreme-PLS with missing data under weak dependence", "comment": "45 pages, 14 figures", "summary": "This paper develops a theoretical framework for Extreme Partial Least Squares (EPLS) dimension reduction in the presence of missing data and weak temporal dependence. Building upon the recent EPLS methodology for modeling extremal dependence between a response variable and high-dimensional covariates, we extend the approach to more realistic data settings where both serial correlation and missing-ness occur. Specifically, we consider a single-index inverse regression model under heavy-tailed conditions and introduce a Missing-at-Random (MAR) mechanism acting on the covariates, whose probability depends on the extremeness of the response. The asymptotic behavior of the proposed estimator is established within an alpha-mixing framework, leading to consistency results under regularly varying tails. Extensive Monte-Carlo experiments covering eleven dependence schemes (including ARMA, GARCH, and nonlinear ESTAR processes) demonstrate that the method performs robustly across a wide range of heavy-tailed and dependent scenarios, even when substantial portions of data are missing. A real-world application to environmental data further confirms the method's capacity to recover meaningful tail directions."}
{"id": "2511.11114", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.11114", "abs": "https://arxiv.org/abs/2511.11114", "authors": ["Chiara Degan", "Bart J. A. Mertens", "Jelle Goeman", "Nadine A. Ikelaar", "Erik H. Niks", "Pietro Spitali", "Roula Tsonaka"], "title": "Multivariate longitudinal modeling of cross-sectional and lagged associations between a continuous time-varying endogenous covariate and a non-Gaussian outcome", "comment": null, "summary": "In longitudinal studies, time-varying covariates are often endogenous, meaning their values depend on both their own history and that of the outcome variable. This violates key assumptions of Generalized Linear Mixed Effects Models (GLMMs), leading to biased and inconsistent estimates. Additionally, missing data and non-concurrent measurements between covariates and outcomes further complicate analysis, especially in rare or degenerative diseases where data is limited. To address these challenges, we propose an alternative use of two well-known multivariate models, each assuming a different form of the association. One induces the association by jointly modeling the random effects, called Joint Mixed Model (JMM); the other quantifies the association using a scaling factor, called Joint Scaled Model (JSM). We extend these models to accommodate continuous endogenous covariates and a wide range of longitudinal outcome types. A limitation in both cases is that the interpretation of the association is neither straightforward nor easy to communicate to scientists. Hence, we have numerically derived an association coefficient that measures the marginal relation between the outcome and the endogenous covariate. The proposed method provides interpretable, population-level estimates of cross-sectional associations (capturing relationships between covariates and outcomes measured at the same time point) and lagged associations (quantifying how past covariate values influence future outcomes), enabling clearer clinical insights. We fitted the JMM and JSM using a flexible Bayesian estimation approach, known as Integrated Nested Laplace Approximation (INLA), to overcome computation burden problems. These models will be presented along with the results of a simulation study and a natural history study on patients with Duchenne Muscular Dystrophy."}
{"id": "2511.11114", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.11114", "abs": "https://arxiv.org/abs/2511.11114", "authors": ["Chiara Degan", "Bart J. A. Mertens", "Jelle Goeman", "Nadine A. Ikelaar", "Erik H. Niks", "Pietro Spitali", "Roula Tsonaka"], "title": "Multivariate longitudinal modeling of cross-sectional and lagged associations between a continuous time-varying endogenous covariate and a non-Gaussian outcome", "comment": null, "summary": "In longitudinal studies, time-varying covariates are often endogenous, meaning their values depend on both their own history and that of the outcome variable. This violates key assumptions of Generalized Linear Mixed Effects Models (GLMMs), leading to biased and inconsistent estimates. Additionally, missing data and non-concurrent measurements between covariates and outcomes further complicate analysis, especially in rare or degenerative diseases where data is limited. To address these challenges, we propose an alternative use of two well-known multivariate models, each assuming a different form of the association. One induces the association by jointly modeling the random effects, called Joint Mixed Model (JMM); the other quantifies the association using a scaling factor, called Joint Scaled Model (JSM). We extend these models to accommodate continuous endogenous covariates and a wide range of longitudinal outcome types. A limitation in both cases is that the interpretation of the association is neither straightforward nor easy to communicate to scientists. Hence, we have numerically derived an association coefficient that measures the marginal relation between the outcome and the endogenous covariate. The proposed method provides interpretable, population-level estimates of cross-sectional associations (capturing relationships between covariates and outcomes measured at the same time point) and lagged associations (quantifying how past covariate values influence future outcomes), enabling clearer clinical insights. We fitted the JMM and JSM using a flexible Bayesian estimation approach, known as Integrated Nested Laplace Approximation (INLA), to overcome computation burden problems. These models will be presented along with the results of a simulation study and a natural history study on patients with Duchenne Muscular Dystrophy."}
{"id": "2511.10967", "categories": ["stat.CO", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.10967", "abs": "https://arxiv.org/abs/2511.10967", "authors": ["Jingyi Zhang", "James C. Spall"], "title": "Autocovariance and Optimal Design for Random Walk Metropolis-Hastings Algorithm", "comment": null, "summary": "The Metropolis-Hastings algorithm has been extensively studied in the estimation and simulation literature, with most prior work focusing on convergence behavior and asymptotic theory. However, its covariance structure-an important statistical property for both theory and implementation-remains less understood. In this work, we provide new theoretical insights into the scalar case, focusing primarily on symmetric unimodal target distributions with symmetric random walk proposals, where we also establish an optimal proposal design. In addition, we derive some more general results beyond this setting. For the high-dimensional case, we relate the covariance matrix to the classical 0.23 average acceptance rate tuning criterion."}
{"id": "2511.11355", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11355", "abs": "https://arxiv.org/abs/2511.11355", "authors": ["Ryan Cecil", "Lucas Mentch"], "title": "Model Class Selection", "comment": null, "summary": "Classical model selection seeks to find a single model within a particular class that optimizes some pre-specified criteria, such as maximizing a likelihood or minimizing a risk. More recently, there has been an increased interest in model set selection (MSS), where the aim is to identify a (confidence) set of near-optimal models. Here, we generalize the MSS framework further by introducing the idea of model class selection (MCS). In MCS, multiple model collections are evaluated, and all collections that contain at least one optimal model are sought for identification. Under mild conditions, data splitting based approaches are shown to provide general solutions for MCS. As a direct consequence, for particular datasets we are able to investigate formally whether classes of simpler and more interpretable statistical models are able to perform on par with more complex black-box machine learning models. A variety of simulated and real-data experiments are provided."}
{"id": "2511.11433", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.11433", "abs": "https://arxiv.org/abs/2511.11433", "authors": ["Giulio Grossi", "Leo Vanciu", "Veronica Ballerini", "Danielle Braun", "Falco J. Bargagli Stoffi"], "title": "Estimating the Effects of Heatwaves on Health: A Causal Inference Framework", "comment": null, "summary": "The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature."}
{"id": "2511.11166", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.11166", "abs": "https://arxiv.org/abs/2511.11166", "authors": ["Lasse Fischer", "Konstantinos Sechidis"], "title": "Knockoffs for low dimensions: changing the nominal level post-hoc to gain power while controlling the FDR", "comment": null, "summary": "Knockoffs are a powerful tool for controlled variable selection with false discovery rate (FDR) control. However, while they are frequently used in high-dimensional regressions, they lack power in low-dimensional and sparse signal settings. One of the main reasons is that knockoffs require a minimum number of selections, depending on the nominal FDR level. In this paper, we leverage e-values to allow the nominal level to be switched after looking at the data and applying the knockoff procedure. In this way, we can increase the nominal level in cases where the original knockoff procedure does not make any selections to potentially make discoveries. Also, in cases where the original knockoff procedure makes discoveries, we can often decrease the nominal level to increase the precision. These improvements come without any costs, meaning the results of our post-hoc knockoff procedure are always more informative than the results of the original knockoff procedure. Furthermore, we apply our technique to recently proposed derandomized knockoff procedures."}
{"id": "2511.11318", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11318", "abs": "https://arxiv.org/abs/2511.11318", "authors": ["Derun Zhou", "Keisuke Yano", "Mahito Sugiyama"], "title": "Dual Riemannian Newton Method on Statistical Manifolds", "comment": null, "summary": "In probabilistic modeling, parameter estimation is commonly formulated as a minimization problem on a parameter manifold. Optimization in such spaces requires geometry-aware methods that respect the underlying information structure. While the natural gradient leverages the Fisher information metric as a form of Riemannian gradient descent, it remains a first-order method and often exhibits slow convergence near optimal solutions. Existing second-order manifold algorithms typically rely on the Levi-Civita connection, thus overlooking the dual-connection structure that is central to information geometry. We propose the dual Riemannian Newton method, a Newton-type optimization algorithm on manifolds endowed with a metric and a pair of dual affine connections. The dual Riemannian Newton method explicates how duality shapes second-order updates: when the retraction (a local surrogate of the exponential map) is defined by one connection, the associated Newton equation is posed with its dual. We establish local quadratic convergence and validate the theory with experiments on representative statistical models. Thus, the dual Riemannian Newton method thus delivers second-order efficiency while remaining compatible with the dual structures that underlie modern information-geometric learning and inference."}
{"id": "2511.11564", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11564", "abs": "https://arxiv.org/abs/2511.11564", "authors": ["Albert Tan", "Mohsen Bayati", "James Nordlund", "Roman Istomin"], "title": "Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility", "comment": "21 pages, 6 figures, Appeared as Oral Presentation in 2025 Conference on Digital Experimentation (CODE) at MIT", "summary": "We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case."}
{"id": "2511.11296", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.11296", "abs": "https://arxiv.org/abs/2511.11296", "authors": ["Shahriar Hasnat Kazi", "Niall Adams", "Edward A. K. Cohen"], "title": "Online Spectral Density Estimation", "comment": null, "summary": "This paper develops the first online algorithms for estimating the spectral density function -- a fundamental object of interest in time series analysis -- that satisfies the three core requirements of streaming inference: fixed memory, fixed computational complexity, and temporal adaptivity. Our method builds on the concept of forgetting factors, allowing the estimator to adapt to gradual or abrupt changes in the data-generating process without prior knowledge of its dynamics. We introduce a novel online forgetting-factor periodogram and show that, under stationarity, it asymptotically recovers the properties of its offline counterpart. Leveraging this, we construct an online Whittle estimator, and further develop an adaptive online spectral estimator that dynamically tunes its forgetting factor using the Whittle likelihood as a loss. Through extensive simulation studies and an application to ocean drifter velocity data, we demonstrate the method's ability to track time-varying spectral properties in real-time with strong empirical performance."}
{"id": "2511.11338", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.11338", "abs": "https://arxiv.org/abs/2511.11338", "authors": ["Stéphane Girard", "Cambyse Pakzad"], "title": "Extreme-PLS with missing data under weak dependence", "comment": "45 pages, 14 figures", "summary": "This paper develops a theoretical framework for Extreme Partial Least Squares (EPLS) dimension reduction in the presence of missing data and weak temporal dependence. Building upon the recent EPLS methodology for modeling extremal dependence between a response variable and high-dimensional covariates, we extend the approach to more realistic data settings where both serial correlation and missing-ness occur. Specifically, we consider a single-index inverse regression model under heavy-tailed conditions and introduce a Missing-at-Random (MAR) mechanism acting on the covariates, whose probability depends on the extremeness of the response. The asymptotic behavior of the proposed estimator is established within an alpha-mixing framework, leading to consistency results under regularly varying tails. Extensive Monte-Carlo experiments covering eleven dependence schemes (including ARMA, GARCH, and nonlinear ESTAR processes) demonstrate that the method performs robustly across a wide range of heavy-tailed and dependent scenarios, even when substantial portions of data are missing. A real-world application to environmental data further confirms the method's capacity to recover meaningful tail directions."}
{"id": "2511.11353", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.11353", "abs": "https://arxiv.org/abs/2511.11353", "authors": ["Johan de Aguas"], "title": "Interpolated stochastic interventions based on propensity scores, target policies and treatment-specific costs", "comment": null, "summary": "We introduce families of stochastic interventions for discrete treatments that connect causal modeling to cost-sensitive decision making. The interventions arise from a cost-penalized information projection of the independent product of the organic propensity and a user-specified target, yielding closed-form Boltzmann-Gibbs couplings. The induced marginals define modified stochastic policies that interpolate smoothly, via a single tilt parameter, from the organic law or from the target distribution toward a product-of-experts limit when all destination costs are strictly positive. One of these families recovers and extends incremental propensity score interventions, retaining identification without global positivity. For inference, we derive efficient influence functions under a nonparametric model for the expected outcomes after these policies and construct one-step estimators with uniform confidence bands. In simulations, the proposed estimators improve stability and robustness to nuisance misspecification relative to plug-in baselines. The framework can operationalize graded scientific hypotheses under realistic constraints: because inputs are modular, analysts can sweep feasible policy spaces, prototype candidates, and align interventions with budgets and logistics before committing experimental resources. This could help close the loop between observational evidence and resource-aware experimental design."}
{"id": "2511.11355", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11355", "abs": "https://arxiv.org/abs/2511.11355", "authors": ["Ryan Cecil", "Lucas Mentch"], "title": "Model Class Selection", "comment": null, "summary": "Classical model selection seeks to find a single model within a particular class that optimizes some pre-specified criteria, such as maximizing a likelihood or minimizing a risk. More recently, there has been an increased interest in model set selection (MSS), where the aim is to identify a (confidence) set of near-optimal models. Here, we generalize the MSS framework further by introducing the idea of model class selection (MCS). In MCS, multiple model collections are evaluated, and all collections that contain at least one optimal model are sought for identification. Under mild conditions, data splitting based approaches are shown to provide general solutions for MCS. As a direct consequence, for particular datasets we are able to investigate formally whether classes of simpler and more interpretable statistical models are able to perform on par with more complex black-box machine learning models. A variety of simulated and real-data experiments are provided."}
{"id": "2511.11433", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.11433", "abs": "https://arxiv.org/abs/2511.11433", "authors": ["Giulio Grossi", "Leo Vanciu", "Veronica Ballerini", "Danielle Braun", "Falco J. Bargagli Stoffi"], "title": "Estimating the Effects of Heatwaves on Health: A Causal Inference Framework", "comment": null, "summary": "The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature."}
{"id": "2511.11497", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.11497", "abs": "https://arxiv.org/abs/2511.11497", "authors": ["Filip Tronarp"], "title": "A Recursive Theory of Variational State Estimation: The Dynamic Programming Approach", "comment": null, "summary": "In this article, variational state estimation is examined from the dynamic programming perspective. This leads to two different value functional recursions depending on whether backward or forward dynamic programming is employed. The result is a theory of variational state estimation that corresponds to the classical theory of Bayesian state estimation. More specifically, in the backward method, the value functional corresponds to a likelihood that is upper bounded by the state likelihood from the Bayesian backward recursion. In the forward method, the value functional corresponds to an unnormalized density that is upper bounded by the unnormalized filtering density. Both methods can be combined to arrive at a variational two-filter formula. Additionally, it is noted that optimal variational filtering is generally of quadratic time-complexity in the sequence length. This motivates the notion of sub-optimal variational filtering, which also lower bounds the evidence but is of linear time-complexity. Another problem is the fact that the value functional recursions are generally intractable. This is briefly discussed and a simple approximation is suggested that retrieves the filter proposed by Courts et al. (2021). The methodology is examined in a jump Gauss--Markov system, where it is observed that the value functional recursions are tractable under a certain factored Markov process approximation. A simulation study demonstrates that the posterior approximation is of adequate quality."}
{"id": "2511.11564", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11564", "abs": "https://arxiv.org/abs/2511.11564", "authors": ["Albert Tan", "Mohsen Bayati", "James Nordlund", "Roman Istomin"], "title": "Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility", "comment": "21 pages, 6 figures, Appeared as Oral Presentation in 2025 Conference on Digital Experimentation (CODE) at MIT", "summary": "We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case."}
