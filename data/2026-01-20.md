<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 4]
- [stat.ME](#stat.ME) [Total: 7]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.CO](#stat.CO) [Total: 3]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Mass Distribution versus Density Distribution in the Context of Clustering](https://arxiv.org/abs/2601.10759)
*Kai Ming Ting,Ye Zhu,Hang Zhang,Tianrun Liang*

Main category: stat.ML

TL;DR: 该论文提出了一种基于质量分布而非密度分布的新聚类方法MMC，解决了传统密度聚类算法的高密度偏差问题，能够发现任意形状、大小和密度的簇。


<details>
  <summary>Details</summary>
Motivation: 传统密度分布作为数据描述符存在根本性限制——高密度偏差，无论使用何种聚类算法。现有的密度聚类算法虽然通过不同技术手段部分缓解了这一问题，但密度分布的根本限制仍然是发现任意形状、大小和密度簇的障碍。

Method: 提出基于质量分布的新聚类算法MMC（质量最大化聚类），最大化所有簇的总质量。该方法可以轻松改为最大化总密度，以便比较密度分布与质量分布的根本差异。MMC的关键优势在于最大化过程不会偏向密集簇。

Result: MMC算法相比密度最大化聚类具有显著优势，能够在没有高密度偏差的情况下进行聚类，从而能够发现任意形状、大小和密度的簇。

Conclusion: 质量分布比密度分布更适合作为聚类的基础描述符，MMC算法通过最大化簇的总质量而非密度，有效克服了传统密度聚类的高密度偏差问题，为发现更广泛类型的簇提供了新途径。

Abstract: This paper investigates two fundamental descriptors of data, i.e., density distribution versus mass distribution, in the context of clustering. Density distribution has been the de facto descriptor of data distribution since the introduction of statistics. We show that density distribution has its fundamental limitation -- high-density bias, irrespective of the algorithms used to perform clustering. Existing density-based clustering algorithms have employed different algorithmic means to counter the effect of the high-density bias with some success, but the fundamental limitation of using density distribution remains an obstacle to discovering clusters of arbitrary shapes, sizes and densities. Using the mass distribution as a better foundation, we propose a new algorithm which maximizes the total mass of all clusters, called mass-maximization clustering (MMC). The algorithm can be easily changed to maximize the total density of all clusters in order to examine the fundamental limitation of using density distribution versus mass distribution. The key advantage of the MMC over the density-maximization clustering is that the maximization is conducted without a bias towards dense clusters.

</details>


### [2] [Memorize Early, Then Query: Inlier-Memorization-Guided Active Outlier Detection](https://arxiv.org/abs/2601.10993)
*Minseo Kang,Seunghwan Park,Dongha Kim*

Main category: stat.ML

TL;DR: IMBoost：结合主动学习增强内点记忆效应，提升无监督异常检测性能的新框架


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖内点记忆效应的无监督异常检测方法在异常与正常数据分离不明显或异常形成密集簇时效果不佳，需要更有效的解决方案

Method: 提出两阶段框架：1）预热阶段诱导和促进内点记忆效应；2）极化阶段使用主动查询的样本最大化内点和异常得分差异，包括新颖的查询策略和定制损失函数

Result: 在多个基准数据集上显著优于最先进的主动异常检测方法，同时计算成本大幅降低

Conclusion: IMBoost通过主动学习增强内点记忆效应，有效解决了异常检测中的挑战，为无监督异常检测提供了高效实用的解决方案

Abstract: Outlier detection (OD) aims to identify abnormal instances, known as outliers or anomalies, by learning typical patterns of normal data, or inliers. Performing OD under an unsupervised regime-without any information about anomalous instances in the training data-is challenging. A recently observed phenomenon, known as the inlier-memorization (IM) effect, where deep generative models (DGMs) tend to memorize inlier patterns during early training, provides a promising signal for distinguishing outliers. However, existing unsupervised approaches that rely solely on the IM effect still struggle when inliers and outliers are not well-separated or when outliers form dense clusters. To address these limitations, we incorporate active learning to selectively acquire informative labels, and propose IMBoost, a novel framework that explicitly reinforces the IM effect to improve outlier detection. Our method consists of two stages: 1) a warm-up phase that induces and promotes the IM effect, and 2) a polarization phase in which actively queried samples are used to maximize the discrepancy between inlier and outlier scores. In particular, we propose a novel query strategy and tailored loss function in the polarization phase to effectively identify informative samples and fully leverage the limited labeling budget. We provide a theoretical analysis showing that the IMBoost consistently decreases inlier risk while increasing outlier risk throughout training, thereby amplifying their separation. Extensive experiments on diverse benchmark datasets demonstrate that IMBoost not only significantly outperforms state-of-the-art active OD methods but also requires substantially less computational cost.

</details>


### [3] [Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach](https://arxiv.org/abs/2601.11016)
*Fenglin Zhang,Jie Wang*

Main category: stat.ML

TL;DR: 提出因果Sinkhorn分布鲁棒优化框架，结合因果结构、连续传输和可解释决策规则，通过Soft Regression Forest和随机组合梯度算法实现高效求解。


<details>
  <summary>Details</summary>
Motivation: 现有分布鲁棒优化方法通常忽略数据的因果结构和连续性，导致决策规则缺乏可解释性。需要开发既考虑因果一致性又保持连续传输的DRO框架，同时提供可解释的决策规则。

Method: 1) 提出因果Sinkhorn散度(CSD)，结合熵正则化和因果一致性；2) 构建基于CSD的上下文DRO模型(Causal-SDRO)；3) 设计Soft Regression Forest决策规则，保持树的可解释性同时参数化可微；4) 开发随机组合梯度算法，收敛率O(ε⁻⁴)。

Result: 理论推导了Causal-SDRO的强对偶形式，最坏分布表现为Gibbs分布的混合。SRF决策规则在任意可测函数空间中近似最优策略。算法收敛率与标准SGD匹配。实验在合成和真实数据集上验证了方法的优越性能和可解释性。

Conclusion: 提出的Causal-SDRO框架成功整合了因果结构、连续传输和可解释决策规则，通过理论分析和实验验证展示了其在分布鲁棒优化中的有效性和实用性。

Abstract: In this paper, we introduce a framework for contextual distributionally robust optimization (DRO) that considers the causal and continuous structure of the underlying distribution by developing interpretable and tractable decision rules that prescribe decisions using covariates. We first introduce the causal Sinkhorn discrepancy (CSD), an entropy-regularized causal Wasserstein distance that encourages continuous transport plans while preserving the causal consistency. We then formulate a contextual DRO model with a CSD-based ambiguity set, termed Causal Sinkhorn DRO (Causal-SDRO), and derive its strong dual reformulation where the worst-case distribution is characterized as a mixture of Gibbs distributions. To solve the corresponding infinite-dimensional policy optimization, we propose the Soft Regression Forest (SRF) decision rule, which approximates optimal policies within arbitrary measurable function spaces. The SRF preserves the interpretability of classical decision trees while being fully parametric, differentiable, and Lipschitz smooth, enabling intrinsic interpretation from both global and local perspectives. To solve the Causal-SDRO with parametric decision rules, we develop an efficient stochastic compositional gradient algorithm that converges to an $\varepsilon$-stationary point at a rate of $O(\varepsilon^{-4})$, matching the convergence rate of standard stochastic gradient descent. Finally, we validate our method through numerical experiments on synthetic and real-world datasets, demonstrating its superior performance and interpretability.

</details>


### [4] [Split-and-Conquer: Distributed Factor Modeling for High-Dimensional Matrix-Variate Time Series](https://arxiv.org/abs/2601.11091)
*Hangjin Jiang,Yuzhou Li,Zhaoxing Gao*

Main category: stat.ML

TL;DR: 提出分布式框架处理高维大规模异构矩阵时间序列数据，通过因子模型降维，保持矩阵结构提高计算效率


<details>
  <summary>Details</summary>
Motivation: 处理高维、大规模、异构矩阵时间序列数据的降维问题，现有分布式方法未能充分利用矩阵结构信息

Method: 数据按列（或行）分区分配到节点服务器，各节点通过二维张量PCA估计行（或列）加载矩阵，中心服务器聚合后进行最终PCA得到全局加载矩阵估计，进而计算因子矩阵

Result: 推导了数据维度发散和样本量T下的渐近性质，模拟实验验证了计算效率和估计精度，实际数据应用验证了预测性能

Conclusion: 提出的分布式框架能有效处理大规模矩阵时间序列数据，保持矩阵结构提高计算效率和信息利用率，适用于未知分组和单位根非平稳情况

Abstract: In this paper, we propose a distributed framework for reducing the dimensionality of high-dimensional, large-scale, heterogeneous matrix-variate time series data using a factor model. The data are first partitioned column-wise (or row-wise) and allocated to node servers, where each node estimates the row (or column) loading matrix via two-dimensional tensor PCA. These local estimates are then transmitted to a central server and aggregated, followed by a final PCA step to obtain the global row (or column) loading matrix estimator. Given the estimated loading matrices, the corresponding factor matrices are subsequently computed. Unlike existing distributed approaches, our framework preserves the latent matrix structure, thereby improving computational efficiency and enhancing information utilization. We also discuss row- and column-wise clustering procedures for settings in which the group memberships are unknown. Furthermore, we extend the analysis to unit-root nonstationary matrix-variate time series. Asymptotic properties of the proposed method are derived for the diverging dimension of the data in each computing unit and the sample size $T$. Simulation results assess the computational efficiency and estimation accuracy of the proposed framework, and real data applications further validate its predictive performance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [5] [Locally sparse varying coefficient mixed model with application to longitudinal microbiome differential abundance](https://arxiv.org/abs/2601.10872)
*Simon Fontaine,Nisha J. D'Silva,Marcell Costa de Medeiros,Grace Y. Chen,Ji Zhu,Gen Li*

Main category: stat.ME

TL;DR: 提出用于纵向微生物组研究的差异丰度分析方法，通过变系数混合效应模型和局部稀疏性识别时间区间上的组间差异，考虑时间依赖性和不规则采样。


<details>
  <summary>Details</summary>
Motivation: 当前差异丰度分析方法主要适用于横断面数据，但许多微生物组研究采用纵向设计以更好地理解微生物动态变化。需要开发能够处理纵向数据、考虑时间依赖性和不规则采样的方法。

Method: 提出一种新颖的变系数混合效应模型，具有局部稀疏性。采用惩罚核平滑方法进行参数估计，包含随机效应以考虑序列相关性。该方法能有效处理受试者间采样时间不一致的情况，适应不规则采样和缺失观测。

Result: 模拟研究表明建模依赖性对于精确估计和支持恢复是必要的。将方法应用于小鼠口腔微生物组在癌症发展过程中的纵向研究，揭示了通过横断面分析无法发现的显著科学见解。

Conclusion: 该方法为纵向微生物组研究中的差异丰度分析提供了有效工具，能够识别显著组间差异的时间区间，同时考虑时间依赖性和不规则采样，补充了现有横断面分析方法的不足。

Abstract: Differential abundance (DA) analysis in microbiome studies has recently been used to uncover a plethora of associations between microbial composition and various health conditions. While current approaches to DA typically apply only to cross-sectional data, many studies feature a longitudinal design to better understand the underlying microbial dynamics. To study DA in longitudinal microbial studies, we introduce a novel varying coefficient mixed-effects model with local sparsity. The proposed method can identify time intervals of significant group differences while accounting for temporal dependence. Specifically, we exploit a penalized kernel smoothing approach for parameter estimation and include a random effect to account for serial correlation. In particular, our method operates effectively regardless of whether sampling times are shared across subjects, accommodating irregular sampling and missing observations. Simulation studies demonstrate the necessity of modeling dependence for precise estimation and support recovery. The application of our method to a longitudinal study of mice oral microbiome during cancer development revealed significant scientific insights that were otherwise not discernible through cross-sectional analyses. An R implementation is available at https://github.com/fontaine618/LSVCMM.

</details>


### [6] [Robust $M$-Estimation of Scatter Matrices via Precision Structure Shrinkage](https://arxiv.org/abs/2601.11099)
*Soma Nikai,Yuichi Goto,Koji Tsukuda*

Main category: stat.ME

TL;DR: 提出一种收缩估计器，将估计的精度矩阵向单位矩阵收缩，以解决高维情况下M估计器对聚类异常值敏感的问题


<details>
  <summary>Details</summary>
Motivation: Maronna和Tyler的M估计器是广泛使用的稳健散度矩阵估计器，但在高维情况下，特别是在存在聚类异常值时，其性能会显著下降

Method: 提出一种收缩估计器，将估计的精度矩阵向单位矩阵收缩，推导其存在的充分条件，讨论统计解释，并建立其崩溃点的上下界

Result: 数值实验证实了所提方法的稳健性

Conclusion: 通过向单位矩阵收缩的精度矩阵估计方法，有效提高了高维情况下对聚类异常值的稳健性

Abstract: Maronna's and Tyler's $M$-estimators are among the most widely used robust estimators for scatter matrices. However, when the dimension of observations is relatively high, their performance can substantially deteriorate in certain situations, particularly in the presence of clustered outliers. To address this issue, we propose an estimator that shrinks the estimated precision matrix toward the identity matrix. We derive a sufficient condition for its existence, discuss its statistical interpretation, and establish upper and lower bounds for its breakdown point. Numerical experiments confirm robustness of the proposed method.

</details>


### [7] [On the use of cross-fitting in causal machine learning with correlated units](https://arxiv.org/abs/2601.10899)
*Salvador V. Balkus,Hasan Laith,Nima S. Hejazi*

Main category: stat.ME

TL;DR: 交叉拟合在相关数据中无需特殊处理：即使忽略相关性，标准交叉拟合仍能消除关键偏差项


<details>
  <summary>Details</summary>
Motivation: 在因果机器学习中，当研究单元存在相关性（如空间、聚类或时间序列数据）时，研究者通常设计专门的交叉拟合方法来最小化折叠间的相关性。但本文质疑这种做法的必要性。

Method: 通过理论证明和模拟实验，比较标准交叉拟合（假设单元独立）与专门设计的消除相关性交叉拟合方法在不同相关结构下的表现。

Result: 模拟实验显示，忽略相关性的标准交叉拟合通常能获得相同或更好的偏差和精度表现，相比专门消除折叠间相关性的技术。

Conclusion: 在处理相关数据时，无需设计复杂的交叉拟合方法来消除折叠间的相关性，标准交叉拟合方法通常已足够有效。

Abstract: In causal machine learning, the fitting and evaluation of nuisance models are typically performed on separate partitions, or folds, of the observed data. This technique, called cross-fitting, eliminates bias introduced by the use of black-box predictive algorithms. When study units may be correlated, such as in spatial, clustered, or time-series data, investigators often design bespoke forms of cross-fitting to minimize correlation between folds. We prove that, perhaps contrary to popular belief, this is typically unnecessary: performing cross-fitting as if study units were independent usually still eliminates key bias terms even when units may be correlated. In simulation experiments with various correlation structures, we show that causal machine learning estimators typically have the same or improved bias and precision under cross-fitting that ignores correlation compared to techniques striving to eliminate correlation between folds.

</details>


### [8] [Generalized Heterogeneous Functional Model with Applications to Large-scale Mobile Health Data](https://arxiv.org/abs/2601.10994)
*Xiaojing Sun,Bingxin Zhao,Fei Xue*

Main category: stat.ME

TL;DR: 提出广义异质函数方法，在广义函数回归框架下同时估计函数效应和识别亚组，用于分析体力活动与疾病关系的异质性，在UK Biobank数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 体力活动对健康至关重要，大规模移动健康数据发现体力活动与多种疾病存在强关联，但准确捕捉这种复杂关系具有挑战性，因为这种关系可能在不同亚组中存在差异，特别是在大规模数据集中。

Method: 提出广义异质函数方法，在广义函数回归框架下同时估计函数效应和识别亚组；开发预聚类方法提高大规模数据计算效率；引入检验程序评估不同亚组是否具有不同的函数效应。

Result: 在UK Biobank数据集（96,433名参与者）中应用，分析体力活动对痴呆风险的影响，方法在预测准确性上优于现有方法，识别出三个具有科学解释的亚组，并证明了理论一致性。

Conclusion: 该方法能有效捕捉体力活动与疾病关系的亚组特异性函数关系，提供更细致的理解，适用于大规模移动健康数据分析，代码已开源。

Abstract: Physical activity is crucial for human health. With the increasing availability of large-scale mobile health data, strong associations have been found between physical activity and various diseases. However, accurately capturing this complex relationship is challenging, possibly because it varies across different subgroups of subjects, especially in large-scale datasets. To fill this gap, we propose a generalized heterogeneous functional method which simultaneously estimates functional effects and identifies subgroups within the generalized functional regression framework. The proposed method captures subgroup-specific functional relationships between physical activity and diseases, providing a more nuanced understanding of these associations. Additionally, we develop a pre-clustering method that enhances computational efficiency for large-scale data through a finer partition of subjects compared to true subgroups. We further introduce a testing procedure to assess whether the different subgroups exhibit distinct functional effects. In the real data application, we examine the impact of physical activity on the risk of dementia using the UK Biobank dataset, which includes over 96,433 participants. Our proposed method outperforms existing methods in future-day prediction accuracy, identifying three distinct subgroups, with detailed scientific interpretations for each subgroup. We also demonstrate the theoretical consistency of our methods. Codes implementing the proposed method are available at: https://github.com/xiaojing777/GHFM.

</details>


### [9] [TSQCA: Threshold-Sweep Qualitative Comparative Analysis in R](https://arxiv.org/abs/2601.11229)
*Yuki Toyoda*

Main category: stat.ME

TL;DR: TSQCA是一个R包，用于自动化QCA中的阈值敏感性分析，通过将阈值作为显式分析变量，提供四种扫描函数来探索不同阈值组合对结果的影响。


<details>
  <summary>Details</summary>
Motivation: QCA分析中校准和二分阈值的选择会显著影响真值表、最小化和最终解决方案，但阈值敏感性分析通常只是临时进行，因为重复分析耗时且容易出错。

Method: 开发TSQCA R包，提供四种扫描函数：otSweep（结果阈值扫描）、ctSweepS（单条件阈值扫描）、ctSweepM（多条件阈值网格扫描）、dtSweep（联合结果-条件阈值空间扫描），并与QCA包集成进行真值表构建和布尔最小化。

Result: TSQCA包能够自动化阈值扫描分析，返回结构化的S3对象，支持一致的打印/摘要方法，可选详细结果，并支持自动Markdown报告生成和配置图表输出，便于跨阈值结果的可重复文档记录。

Conclusion: TSQCA通过自动化阈值敏感性分析，解决了QCA研究中阈值选择依赖性的问题，提高了分析效率和可重复性，为研究人员提供了系统探索阈值影响的工具。

Abstract: Qualitative Comparative Analysis (QCA) requires researchers to choose calibration and dichotomization thresholds, and these choices can substantially affect truth tables, minimization, and resulting solution formulas. Despite this dependency, threshold sensitivity is often examined only in an ad hoc manner because repeated analyses are time-intensive and error-prone. We present TSQCA, an R package that automates threshold-sweep analyses by treating thresholds as explicit analytical variables. It provides four sweep functions (otSweep, ctSweepS, ctSweepM, dtSweep) to explore outcome thresholds, single-condition thresholds, multi-condition threshold grids, and joint outcome-condition threshold spaces, respectively. TSQCA integrates with the established CRAN package QCA for truth table construction and Boolean minimization, while returning structured S3 objects with consistent print/summary methods and optional detailed results. The package also supports automated Markdown report generation and configuration-chart output to facilitate reproducible documentation of cross-threshold results.

</details>


### [10] [Estimation of time series by Maximum Mean Discrepancy](https://arxiv.org/abs/2601.11233)
*Pierre Alquier,Jean-David Fermanian,Benjamin Poignard*

Main category: stat.ME

TL;DR: 提出两种基于近似最大均值差异的最小距离估计器，用于处理依赖数据，通过模拟近似难以处理的模型分布，适用于含隐变量的动态过程。


<details>
  <summary>Details</summary>
Motivation: 针对依赖数据（特别是含隐变量的动态过程）的参数估计问题，当模型分布难以解析处理时，需要开发能够通过模拟近似模型分布的稳健估计方法。

Method: 定义两种最小距离估计器：通过最小化真实经验分布与假设参数模型分布之间的近似最大均值差异距离。当模型分布难以处理时，通过模拟进行近似，从而适应含隐变量的动态过程。

Result: 在绝对正则/β混合随机元素的背景下，推导了估计器的非渐近和大样本性质。模拟实验表明，与标准估计方法相比，所提方法对模型误设具有更强的稳健性。

Conclusion: 提出的基于近似最大均值差异的最小距离估计器能够有效处理依赖数据和含隐变量的复杂模型，在模型误设情况下表现出优越的稳健性，为依赖数据的参数估计提供了新的有效工具。

Abstract: We define two minimum distance estimators for dependent data by minimizing some approximated Maximum Mean Discrepancy distances between the true empirical distribution of observations and their assumed (parametric) model distribution. When the latter one is intractable, it is approximated by simulation, allowing to accommodate most dynamic processes with latent variables. We derive the non-asymptotic and the large sample properties of our estimators in the context of absolutely regular/beta-mixing random elements. Our simulation experiments illustrate the robustness of our procedures to model misspecification, particularly in comparison with alternative standard estimation methods.

</details>


### [11] [Deriving Complete Constraints in Hidden Variable Models](https://arxiv.org/abs/2601.11242)
*Michael C. Sachs,Erin E. Gabriel,Robin J. Evans,Arvid Sjölander*

Main category: stat.ME

TL;DR: 提出系统方法推导隐变量图模型中可观测约束的完整集合，适用于分类观测变量和线性响应函数模型


<details>
  <summary>Details</summary>
Motivation: 隐变量图模型可能产生比简单条件独立更复杂的可观测约束，这些约束可以检验模型假设并提高统计效率，但完整约束集难以确定

Method: 针对分类观测变量和线性响应函数变量完全表征联合分布的情况，开发系统方法推导可观测约束的完整集合

Result: 方法在多个新场景中得到验证，包括同时蕴含不等式和等式约束的情况

Conclusion: 提出的系统方法能够有效确定隐变量图模型中的完整可观测约束集，为模型检验和统计估计提供有力工具

Abstract: Hidden variable graphical models can sometimes imply constraints on the observable distribution that are more complex than simple conditional independence relations. These observable constraints can falsify assumptions of the model that would otherwise be untestable due to the unobserved variables and can be used to constrain estimation procedures to improve statistical efficiency. Knowing the complete set of observable constraints is thus ideal, but this can be difficult to determine in many settings. In models with categorical observed variables and a joint distribution that is completely characterized by linear relations to the unobservable response function variables, we develop a systematic method for deriving the complete set of observable constraints. We illustrate the method in several new settings, including ones that imply both inequality and equality constraints.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [12] [A Note on Harmonic Underspecification in Log-Normal Trigonometric Regression](https://arxiv.org/abs/2601.10919)
*Michael T. Gorczyca*

Main category: stat.AP

TL;DR: 比较两种生物节律数据分析方法：对数变换后最小二乘三角回归与广义线性模型三角回归。当振荡谐波数量不足时，对数正态三角回归能产生无偏参数估计，而GLM需要正确指定模型。


<details>
  <summary>Details</summary>
Motivation: 生物节律数据分析通常使用最小二乘三角回归，但当响应变量不服从正态分布时，研究者要么先变换响应变量再应用最小二乘回归，要么将三角回归扩展到广义线性模型框架。本文旨在比较这两种方法在振荡谐波数量不足时的表现。

Method: 假设数据在等间距实验设计下采样，且对数连接函数适用于GLM。研究当响应变量服从广义伽马分布时，比较两种方法：1）对数变换后最小二乘三角回归（对数正态三角回归），2）广义线性模型三角回归。分析它们在振荡谐波数量不足时的参数估计特性。

Result: 当响应变量服从广义伽马分布时，即使振荡谐波数量不足，对数正态三角回归也能产生无偏的参数估计。而GLM需要正确指定模型才能产生无偏估计。在皮质醇水平数据分析中，只有对数正态三角回归产生的参数估计对指定的振荡谐波数量保持不变。当指定足够多的振荡谐波时，两种方法产生相同的参数估计。

Conclusion: 对于生物节律数据分析，当振荡谐波数量可能不足时，对数变换后最小二乘三角回归比广义线性模型三角回归更具鲁棒性，因为它能在模型错误指定时仍产生无偏的参数估计。

Abstract: Analysis of biological rhythm data often involves performing least squares trigonometric regression, which models the oscillations of a response over time as a sum of sinusoidal components. When the response is not normally distributed, an investigator will either transform the response before applying least squares trigonometric regression or extend trigonometric regression to a generalized linear model (GLM) framework. In this note, we compare these two approaches when the number of oscillation harmonics is underspecified. We assume data are sampled under an equispaced experimental design and that a log link function would be appropriate for a GLM. We show that when the response follows a generalized gamma distribution, least squares trigonometric regression with a log-transformed response, or log-normal trigonometric regression, produces unbiased parameter estimates for the oscillation harmonics, even when the number of oscillation harmonics is underspecified. In contrast, GLMs require correct specification to produce unbiased parameter estimates. We apply both methods to cortisol level data and find that only log-normal trigonometric regression produces parameter estimates that are invariant to the number of specified oscillation harmonics. Additionally, when a sufficiently large number of oscillation harmonics is specified, both methods produce identical parameter estimates for the oscillation harmonics.

</details>


### [13] [Analyzing Residential Speeding Using Connected Vehicle Data: A Case Study in Charlottesville, VA Area](https://arxiv.org/abs/2601.10974)
*Shi Feng,B. Brian Park,Andrew Mondschein*

Main category: stat.AP

TL;DR: 利用联网车辆数据分析住宅道路超速行为，发现超速分布高度偏斜，夜间超速是白天的27倍，特定路段同时存在攻击性和鲁莽超速，需要空间和行为干预。


<details>
  <summary>Details</summary>
Motivation: 传统执法和规划工具在监测住宅道路超速行为方面存在局限，需要利用联网车辆数据提供更全面、可扩展的分析框架，以支持交通安全政策和规划。

Method: 开发可扩展的数据处理管道，处理轨迹数据并补充缺失的限速信息，在OpenStreetMap的way ID级别生成汇总分析，以弗吉尼亚州夏洛茨维尔市的联网车辆数据为案例研究。

Result: 住宅道路超速分布高度偏斜：38%的路段至少有一次攻击性超速，20%的路段至少有一次鲁莽超速；夜间超速发生率是白天的27倍；特定路段同时出现在攻击性和鲁莽超速的前10名，表明存在高风险住宅道路。

Conclusion: 该框架为交通安全分析奠定了基础，展示了远程信息处理数据在创建更安全、更宜居社区方面的潜力，分析结果为政策和规划提供了丰富基础，是对传统执法和规划工具的有价值补充。

Abstract: This study uses connected vehicle data to analyze speeding behavior on residential roads. A scalable pipeline processes trajectory data and supplements missing speed limits to generate summaries at OpenStreetMap's way ID level. The findings reveal a highly skewed distribution of both aggressive and reckless speeding. Based on a case study of Charlottesville, VA's connected vehicle data on residential roads, we found that 38% of segments had at least one instance of aggressive speeding, and 20% had at least one instance of reckless speeding. In addition, night time speeding is 27 times more prevalent than day time, and extreme violations on specific road segments highlight how severe the issue can be. Several segments rank among the top 10 for both aggressive and reckless speedings, indicating that there exist high-risk residential roads. These findings support the need for both spatial and behavioral interventions. The analysis provides a rich foundation for policy and planning, offering a valuable complement to traditional enforcement and planning tools. In conclusion, this framework sets the foundation for future applications in traffic safety analytics, demonstrating the growing potential of telematics data to inform safer, more livable communities.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [14] [Sub-Cauchy Sampling: Escaping the Dark Side of the Moon](https://arxiv.org/abs/2601.11066)
*Sebastiano Grazzi,Sifan Liu,Gareth O. Roberts,Jun Yang*

Main category: stat.CO

TL;DR: 提出基于亚柯西投影的MCMC算法，将欧氏空间映射到超球面的球冠区域，适用于亚柯西目标分布（尾部至多与多维柯西分布一样重）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理重尾分布时不可靠，特别是在高维贝叶斯建模和计算中，需要一种简单且广泛适用的算法来处理亚柯西目标分布。

Method: 基于亚柯西投影的MCMC算法，这是一种几何变换，将欧氏空间映射到超球面的球冠区域（称为"月亮暗面"的补集）。

Result: 理论证明该方法对亚柯西目标分布是一致遍历的，经验验证在高维挑战性问题中表现良好。

Conclusion: 该方法简单且适用广泛，为重尾分布的贝叶斯建模和计算开辟了新机会，特别是在现有方法不可靠的场景中。

Abstract: We introduce a Markov chain Monte Carlo algorithm based on Sub-Cauchy Projection, a geometric transformation that generalizes stereographic projection by mapping Euclidean space into a spherical cap of a hyper-sphere, referred to as the complement of the dark side of the moon. We prove that our proposed method is uniformly ergodic for sub-Cauchy targets, namely targets whose tails are at most as heavy as a multidimensional Cauchy distribution, and show empirically its performance for challenging high-dimensional problems. The simplicity and broad applicability of our approach open new opportunities for Bayesian modeling and computation with heavy-tailed distributions in settings where most existing methods are unreliable.

</details>


### [15] [Smooth SCAD: A Raised Cosine SCAD Type Thresholding Rule for Wavelet Denoising](https://arxiv.org/abs/2601.11461)
*Radhika Kulkarni,Aluisio Pinheiro,Brani Vidakovic,Abdourrahmane M. Atto*

Main category: stat.CO

TL;DR: 提出平滑SCAD阈值规则用于小波去噪，用余弦函数替换分段线性过渡，保持SCAD的稀疏性和低偏置特性，同时获得连续可微性，便于风险估计和阈值选择。


<details>
  <summary>Details</summary>
Motivation: 标准SCAD阈值规则在过渡区域是分段线性的，这导致其不可导，限制了Stein无偏风险估计(SURE)的应用。需要一种既保持SCAD优良特性（稀疏性和低偏置）又具有足够平滑性的改进方法，以便进行理论分析和实际应用。

Method: 用余弦函数替换SCAD阈值规则中的分段线性过渡区域，创建平滑的SCAD阈值函数。该函数是奇函数，在实数域上连续，在主阈值外连续可微。推导了对应的非凸先验分布，使得后验众数与估计量一致。给出了显式的SURE风险表达式，讨论了最优阈值的oracle尺度，并设计了全局和层依赖的自适应版本。

Result: 获得了平滑的SCAD阈值规则，保持了SCAD的核心特性：对小系数的稀疏性（精确为零）和对大系数的近似无偏性。平滑性使得该规则属于连续阈值类，Stein无偏风险估计有效，可以进行无偏风险计算、稳定的数据驱动阈值选择，并应用Kudryavtsev和Shestakov的渐近理论。

Conclusion: 平滑SCAD规则提供了SCAD的一个易处理改进，在小波收缩中结合了低偏置、精确稀疏性和分析便利性。通过平滑化实现了理论分析的可行性，同时保持了原始SCAD的优良统计特性。

Abstract: We introduce a smooth variant of the SCAD thresholding rule for wavelet denoising by replacing its piecewise linear transition with a raised cosine. The resulting shrinkage function is odd, continuous on R, and continuously differentiable away from the main threshold, yet retains the hallmark SCAD properties of sparsity for small coefficients and near unbiasedness for large ones. This smoothness places the rule within the continuous thresholding class for which Stein's unbiased risk estimate is valid. As a result, unbiased risk computation, stable data-driven threshold selection, and the asymptotic theory of Kudryavtsev and Shestakov apply.
  A corresponding nonconvex prior is obtained whose posterior mode coincides with the estimator, yielding a transparent Bayesian interpretation. We give an explicit SURE risk expression, discuss the oracle scale of the optimal threshold, and describe both global and level-dependent adaptive versions. The smooth SCAD rule therefore offers a tractable refinement of SCAD, combining low bias, exact sparsity, and analytical convenience in a single wavelet shrinkage procedure.

</details>


### [16] [Fisher Scoring for Exact Matérn Covariance Estimation through Stable Smoothness Optimization](https://arxiv.org/abs/2601.11437)
*Yiping Hong,Sameh Abdulah,Marc G. Genton,Ying Sun*

Main category: stat.CO

TL;DR: 提出Fisher-BT方法，结合Fisher评分算法和回溯线搜索，用于高效估计Matérn协方差函数中难以估计的光滑参数，解决了传统MLE在数值不稳定性和计算效率方面的问题。


<details>
  <summary>Details</summary>
Motivation: Matérn协方差函数的GRF模型在空间过程建模中很强大，但光滑参数难以用MLE估计，存在数值不稳定问题，且对于大规模空间数据集计算成本过高。

Method: 提出Fisher-BT方法：整合Fisher评分算法和回溯线搜索策略，采用修正贝塞尔函数的级数近似，利用ExaGeoStat高性能计算框架实现高效MLE估计。

Result: Fisher-BT方法相比无导数优化方法（如BOBYQA和Nelder-Mead）减少了迭代次数、加速收敛，提高了光滑参数估计的数值稳定性，在计算效率和数值稳定性方面显著优于现有方法。

Conclusion: Fisher-BT方法为大规模空间数据集提供了准确、高效且数值稳定的光滑参数估计方案，在模拟和真实土壤湿度数据应用中验证了其优越性。

Abstract: Gaussian Random Fields (GRFs) with Matérn covariance functions have emerged as a powerful framework for modeling spatial processes due to their flexibility in capturing different features of the spatial field. However, the smoothness parameter is challenging to estimate using maximum likelihood estimation (MLE), which involves evaluating the likelihood based on the full covariance matrix of the GRF, due to numerical instability. Moreover, MLE remains computationally prohibitive for large spatial datasets. To address this challenge, we propose the Fisher-BackTracking (Fisher-BT) method, which integrates the Fisher scoring algorithm with a backtracking line search strategy and adopts a series approximation for the modified Bessel function. This method enables an efficient MLE estimation for spatial datasets using the ExaGeoStat high-performance computing framework. Our proposed method not only reduces the number of iterations and accelerates convergence compared to derivative-free optimization methods but also improves the numerical stability of the smoothness parameter estimation. Through simulations and real-data analysis using a soil moisture dataset covering the Mississippi River Basin, we show that the proposed Fisher-BT method achieves accuracy comparable to existing approaches while significantly outperforming derivative-free algorithms such as BOBYQA and Nelder-Mead in terms of computational efficiency and numerical stability.

</details>
