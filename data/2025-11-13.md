<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 2]
- [stat.ME](#stat.ME) [Total: 12]
- [stat.CO](#stat.CO) [Total: 4]
- [stat.ML](#stat.ML) [Total: 8]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Parameter Estimation and Seasonal Modification of the Fractional Poisson Process with Application to Vorticity Extremes over the North Atlantic](https://arxiv.org/abs/2511.08081)
*Merle Mendel,Roland Fried*

Main category: stat.AP

TL;DR: 提出了一种基于分位数距离最小化的分数泊松过程参数估计方法，并引入了季节性建模方法，应用于北大西洋-欧洲地区相对涡度极值的重现时间分析。


<details>
  <summary>Details</summary>
Motivation: 分数泊松过程在气象学和物理学等领域有广泛应用，但现有参数估计方法存在局限性，需要更灵活和准确的估计方法，特别是在考虑季节性的实际应用中。

Method: 基于经验分布与理论分布在选定分位数处的距离最小化来估计参数，并提出了通过距离加权方法将季节性纳入分数泊松过程建模。

Result: 通过广泛的模拟研究评估了新估计方法的优势和局限性，并与多种竞争估计器进行比较，验证了方法的有效性。

Conclusion: 提出的参数估计方法和季节性建模方法增强了分数泊松过程在实际应用中的适用性，特别是在气象学领域的重现时间分析中。

Abstract: The fractional Poisson process (FPP) generalizes the standard Poisson process by replacing exponentially distributed return times with Mittag-Leffler distributed ones with an extra tail parameter, allowing for greater flexibility. The FPP has been applied in various fields, such as modeling occurrences of extratropical cyclones in meteorology and solar flares in physics. We propose a new estimation method for the parameters of the FPP, based on minimizing the distance between the empirical and the theoretical distribution at selected quantiles. We conduct an extensive simulation study to evaluate the advantages and limitations of the new estimation method and to compare it with several competing estimators, some of which have not yet been examined in the Mittag-Leffler setting. To enhance the applicability of the FPP in real-world scenarios, particularly in meteorology, we propose a method for incorporating seasonality into the FPP through distance-based weighting. We then analyze the return times of relative vorticity extremes in the North Atlantic-European region using our seasonal modeling approach.

</details>


### [2] [The impact of Women's empowerment on childhood vaccination coverage in Nigeria: a spatio-temporal analysis](https://arxiv.org/abs/2511.08262)
*Ezra Gayawan,Osafu Augustine Egbon,Edson Utazi,Jamila Abubakar Umar,Caroline Trotter*

Main category: stat.AP

TL;DR: 该研究分析了尼日利亚女性赋权与疫苗接种覆盖率之间的关系，发现女性在家庭决策和医疗自主方面的赋权通常能提高疫苗接种率，但效果存在地理差异。


<details>
  <summary>Details</summary>
Motivation: 性别不平等和女性赋权不足阻碍了低收入和中等收入国家的疫苗接种服务获取，尼日利亚的社会规范和文化价值观限制了女性在医疗决策中的自主权，导致免疫接种存在空间差异。

Method: 使用尼日利亚人口与健康调查的四轮数据，构建了两个赋权指数（家庭决策参与和医疗自主权），应用结构化时空统计模型评估女性赋权对疫苗接种差异的影响，并预测第三行政级别的疫苗接种结果。

Result: 研究发现，与家庭参与和医疗自主相关的赋权总体上提高了疫苗接种率，但效果大小因地区而异，特别是在高度赋权的女性中。模型验证显示预测结果与第二行政级别的经验估计一致。

Conclusion: 尽管国家持续努力缩小免疫接种差距，但研究强调需要制定针对具体背景的策略，增强女性的决策权和社区参与，以减少地区差异并提高整体疫苗接种覆盖率。

Abstract: Immunization remains one of the most effective public health interventions, substantially reducing childhood morbidity and mortality worldwide. Yet, gender disparity and women's disempowerment continue to hinder access to vaccination services in low- and middle-income countries. In Nigeria, variations in social norms and cultural values shape gender roles, limiting women's autonomy in healthcare decisions and household participation. These constraints contribute to spatial differences in immunization uptake. Using data from four waves of the Nigeria Demographic and Health Survey, we developed two empowerment indices capturing women's participation in household decision-making and their ability to decide on personal healthcare needs. A structured spatiotemporal statistical model was applied to assess how much of the observed vaccination disparities could be attributed to women's empowerment and to predict vaccination outcomes at the third administrative level. We examined five indicators: Bacillus Calmette-Guerin (BCG), zero-dose, complete DPT, MCV-1 (first dose of measles-containing vaccine), and all-basic vaccination coverage. Model validation involved comparing empirical estimates with projections at the second administrative level. Results indicate that empowerment related to household participation and healthcare autonomy generally increases vaccination uptake, though the magnitude of effects varies geographically, particularly among highly empowered women. Despite ongoing national efforts to close immunization gaps, the study highlights the need for context-specific strategies that enhance women's decision-making power and community engagement to reduce regional disparities and improve overall vaccination coverage.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [ANOVATS: A subsampling-based test to detect differences among short time series in marine studies](https://arxiv.org/abs/2511.08070)
*Yuichi Goto,Hiroko Kato Solvang,Masanobu Taniguchi,Tone Falkenhaug*

Main category: stat.ME

TL;DR: 提出了ANOVATS方法，用于检测小样本时间序列数据中的区域差异，无需依赖专家知识或大量时间点数据


<details>
  <summary>Details</summary>
Motivation: 海洋生态系统评估中，传统方法依赖专家知识和可视化，且时间序列数据通常只有几十年，限制了经典时间序列方法的应用

Method: 开发了基于子采样的ANOVATS方法，避免频谱密度估计，并设计了ANOVATS事后检验程序对区域进行分组

Result: 通过分析北海不同层位的浮游动物生物量数据，验证了该方法能够量化地理区域间的物种差异

Conclusion: ANOVATS方法能够有效检测小样本时间序列数据中的区域差异，无需先验生物或地理知识

Abstract: Assessing marine ecosystems is important for understanding the impacts of climate change and human activity, as well as for maintaining healthy oceans and ecosystems. In marine science, it is common for biologists and geologists to identify regional differences based on expert knowledge, frequently through data visualization. However, time series data collected through surveys in marine studies typically span only a few decades, limiting the applicability of classical time series methods. Additionally, without expert knowledge, detecting significant differences becomes challenging. To address these issues, we introduce ANOVATS (ANOVA for small-sample time series data), a subsampling-based method to detect regional differences in small-sample time series data with a fixed number of groups. This method bypasses the need for spectral density estimation, which requires a large number of time points in the data. Furthermore, after detecting differences in homogeneity across all areas using the ANOVATS procedure, we devised a simple ANOVATS post hoc procedure to group the areas. Finally, we demonstrate the effectiveness of our method by analyzing zooplankton biomass data collected in different strata of the North Sea, showing its ability to quantify differences in species between geographical areas without relying on prior biological or geographical knowledge.

</details>


### [4] [Weighted Asymptotically Optimal Sequential Testing](https://arxiv.org/abs/2511.07588)
*Soumyabrata Bose,Jay Bartroff*

Main category: stat.ME

TL;DR: 本文开发了一个在保持渐近最优性的同时将先验信息纳入序贯多重检验程序的框架，提出了加权对数似然比(WLLR)和两种新的序贯检验方法：加权间隙和加权间隙-交集程序。


<details>
  <summary>Details</summary>
Motivation: 在序贯多重检验中有效利用先验信息，同时保持统计检验的渐近最优性。

Method: 定义加权对数似然比(WLLR)作为标准LLR的加性修改，构建加权间隙和加权间隙-交集两种序贯检验程序。

Result: 证明两种程序都能强控制族错误率，且在误差概率趋于零时，期望停止时间达到理论下界，具有渐近最优性。

Conclusion: 加权序贯检验程序在满足权重分布的可解释条件下，在高维场景和随机权重设置中保持鲁棒的渐近最优性。

Abstract: This paper develops a framework for incorporating prior information into sequential multiple testing procedures while maintaining asymptotic optimality. We define a weighted log-likelihood ratio (WLLR) as an additive modification of the standard LLR and use it to construct two new sequential tests: the Weighted Gap and Weighted Gap-Intersection procedures. We prove that both procedures provide strong control of the family-wise error rate. Our main theoretical contribution is to show that these weighted procedures are asymptotically optimal; their expected stopping times achieve the theoretical lower bound as the error probabilities vanish. This first-order optimality is shown to be robust, holding in high-dimensional regimes where the number of null hypotheses grows and in settings with random weights, provided that mild, interpretable conditions on the weight distribution are met.

</details>


### [5] [Asymmetric Space-Time Covariance Functions via Hierarchical Mixtures](https://arxiv.org/abs/2511.07959)
*Pulong Ma*

Main category: stat.ME

TL;DR: 提出了一种通过分层混合方法构建时空协方差函数的新框架，能够生成具有封闭形式的非对称协方差函数，并统一了现有多种时空协方差模型的构建方法。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉复杂的时空依赖结构，需要构建更灵活、具有封闭形式的非对称时空协方差函数，同时统一现有模型的构建框架。

Method: 采用分层混合方法，通过在不同层次上分解模型规范复杂度，使用简单的混合测度来指定简约的协方差模型，获得灵活性质和封闭形式推导。

Result: 构建了新的非对称时空协方差函数类，提供了表征定理说明混合测度如何决定协方差函数的统计性质，并在爱尔兰风速数据和美国气温数据上验证了新模型的优越性能。

Conclusion: 分层混合方法提供了一个统一的建模框架，能够构建具有灵活性质的时空协方差函数，填补了拉格朗日参考框架的理论空白，并在实际应用中表现出优于现有模型的性能。

Abstract: This work is focused on constructing space-time covariance functions through a hierarchical mixture approach that can serve as building blocks for capturing complex dependency structures. This hierarchical mixture approach provides a unified modeling framework that not only constructs a new class of asymmetric space-time covariance functions with closed-form expressions, but also provides corresponding space-time process representations, which further unify constructions for many existing space-time covariance models. This hierarchical mixture framework decomposes the complexity of model specification at different levels of hierarchy, for which parsimonious covariance models can be specified with simple mixing measures to yield flexible properties and closed-form derivation. A characterization theorem is provided for the hierarchical mixture approach on how the mixing measures determine the statistical properties of covariance functions. Several new covariance models resulting from this hierarchical mixture approach are discussed in terms of their practical usefulness. A theorem is also provided to construct a general class of valid asymmetric space-time covariance functions with arbitrary and possibly different degrees of smoothness in space and in time and flexible long-range dependence. The proposed covariance class also bridges a theoretical gap in using the Lagrangian reference framework. The superior performance of several new parsimonious covariance models over existing models is verified with the well-known Irish wind data and the U.S. air temperature data.

</details>


### [6] [Inference on multiple quantiles in regression models by a rank-score approach](https://arxiv.org/abs/2511.07999)
*Riccardo De Santis,Anna Vesely,Angela Andreella*

Main category: stat.ME

TL;DR: 提出了一种多变量秩得分检验的扩展方法，结合封闭测试程序来解决多分位数回归中的多重检验问题，有效控制族错误率并提高统计功效。


<details>
  <summary>Details</summary>
Motivation: 解决多分位数回归中不同分位数水平下的多重检验问题，以及实践中常被忽视的族错误率控制问题。

Method: 将多变量秩得分检验扩展到多分位数回归场景，并嵌入封闭测试程序来高效处理多重检验。

Result: 理论分析和模拟研究显示，该方法能有效控制族错误率，且比传统校正方法（如Bonferroni）具有更高的统计功效。

Conclusion: 所提出的方法为多分位数回归中的多重检验问题提供了有效的解决方案，在控制错误率的同时提升了检测能力。

Abstract: This paper tackles the challenge of performing multiple quantile regressions across different quantile levels and the associated problem of controlling the familywise error rate, an issue that is generally overlooked in practice. We propose a multivariate extension of the rank-score test and embed it within a closed-testing procedure to efficiently account for multiple testing. Theoretical foundations and simulation studies demonstrate that our method effectively controls the familywise error rate while achieving higher power than traditional corrections, such as Bonferroni.

</details>


### [7] [Who's Afraid of the Wallenius Distribution?](https://arxiv.org/abs/2511.08088)
*Linda M. Haines*

Main category: stat.ME

TL;DR: 本文探讨了使用Wallenius非中心超几何分布分析具有固定行边际和样本量的2×2及多组列联表的方法，提出了基于似然和贝叶斯推断的分析框架，并开发了新的球面行走Metropolis算法。


<details>
  <summary>Details</summary>
Motivation: 传统列联表分析方法在处理固定边际和样本量的多组分类数据时存在局限性，需要更合适的统计分布模型来准确分析这类数据。

Method: 采用Wallenius非中心超几何分布模型，将参数视为权重（定义在正则单纯形上），通过权重变换实现无约束优化，并对极小似然值进行适当缩放。开发了球面行走Metropolis MCMC算法，在笛卡尔坐标和重心坐标间无缝转换。

Result: 成功实现了对2×2列联表的直接分析（权重直接转换为所需比值和比值比），对多组列联表的分析虽然更复杂但可行，数值计算中的细微问题得到解决。

Conclusion: Wallenius分布为固定边际列联表分析提供了有效框架，2×2表分析简单直接，多组表分析需要更精细的数值处理，新开发的MCMC算法在贝叶斯推断中表现良好。

Abstract: This paper is about the use of the Wallenius noncentral hypergeometric distribution for analysing contingency tables with two or more groups and two categories and with row margins and sample size, that is both margins, fixed. The parameters of the distribution are taken to be weights which are positive and sum to one and are thus defined on a regular simplex. The approach to analysis is presented for likelihood-based and Bayesian inference and is illustrated by example, with datasets taken from the literature and, in one case, used to generate semi-synthetic data. The analysis of two-by-two contingency tables using the univariate Wallenius distribution is shown to be straightforward, with the parameter a single weight which translates immediately to the requisite odds and the odds ratio. The analysis of contingency tables with more than two groups based on the multivariate Wallenius distribution was however more nuanced than that of the two-group tables. Specifically, some numerical subtleties were required in order to implement the necessary calculations. In particular, optimisation with respect to the weights was performed by transforming the weights to yield an unconstrained optimisation problem and likelihoods which are extremely small were scaled by an appropriate multiplying factor without compromising the elements of inference. Furthermore, a novel Markov chain Monte Carlo algorithm for Bayesian inference, termed the sphere walk Metropolis, was constructed. The proposal is implemented in Cartesian coordinates on the reference simplex and the Metropolis filter in barycentric coordinates on the regular simplex, with the transition between barycentric and Cartesian coordinates effected seamlessly.

</details>


### [8] [Simulation-Based Fitting of Intractable Models via Sequential Sampling and Local Smoothing](https://arxiv.org/abs/2511.08180)
*Guido Masarotto*

Main category: stat.ME

TL;DR: 提出一种用于拟合似然函数、矩和其他推断量难以解析或数值计算的生成模型的综合算法，结合全局搜索和局部搜索阶段，性能优于替代方法。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中似然函数、矩等推断量难以解析或数值计算的问题，提供一种仅需有限先验信息的通用解决方案。

Method: 结合全局搜索阶段（识别解区域）和局部搜索阶段（模拟Fisher评分算法的信任区域版本计算拟似然估计量）。

Result: 与替代方法比较显示该算法具有强大性能，已在CRAN上提供R包实现。

Conclusion: 该算法为难以解析的生成模型拟合提供了有效的通用解决方案，性能优越且易于使用。

Abstract: This paper presents a comprehensive algorithm for fitting generative models whose likelihood, moments, and other quantities typically used for inference are not analytically or numerically tractable. The proposed method aims to provide a general solution that requires only limited prior information on the model parameters. The algorithm combines a global search phase, aimed at identifying the region of the solution, with a local search phase that mimics a trust region version of the Fisher scoring algorithm for computing a quasi-likelihood estimator. Comparisons with alternative methods demonstrate the strong performance of the proposed approach. An R package implementing the algorithm is available on CRAN.

</details>


### [9] [Reclustering: A New Method to Test the Appropriate Level of Clustering](https://arxiv.org/abs/2511.08184)
*Kentaro Fukumoto*

Main category: stat.ME

TL;DR: 提出了一种名为"reclustering"的新方法，用于在有限样本中确定聚类稳健标准误(CRSEs)的最佳聚类级别，通过随机重复将细聚类重新分组为粗聚类来检验聚类独立性假设。


<details>
  <summary>Details</summary>
Motivation: 当学者怀疑数据中存在聚类依赖结构但不确定具体聚类级别时（如家庭、城市、县或州），需要一种在有限样本中确定最佳聚类级别的方法，而现有方法主要基于渐近理论。

Method: 提出reclustering方法：随机重复将细聚类重新分组为新的粗聚类，计算CRSEs等统计量。如果原始聚类下的统计量在reclustering诱导的统计量分布中是显著异常值，则拒绝细聚类相互独立的原假设，采用粗聚类。

Result: 通过蒙特卡洛模拟和应用研究，比较了reclustering方法与先前几种检验方法的性能表现。

Conclusion: reclustering方法为在有限样本中确定CRSEs的适当聚类级别提供了一种有效的新途径，当原始聚类统计量在reclustering分布中显著异常时，应采用更粗的聚类级别。

Abstract: When scholars suspect units are dependent on each other within clusters but independent of each other across clusters, they employ cluster-robust standard errors (CRSEs). Nevertheless, what to cluster over is sometimes unknown. For instance, in the case of cross-sectional survey samples, clusters may be households, municipalities, counties, or states. A few approaches have been proposed, although they are based on asymptotics. I propose a new method to address this issue that works in a finite sample: reclustering. That is, we randomly and repeatedly group fine clusters into new gross clusters and calculate a statistic such as CRSEs. Under the null hypothesis that fine clusters are independent of each other, how they are grouped into gross clusters should not matter for any cluster-sensitive statistic. Thus, if the statistic based on the original clustering is a significant outlier against the distributions of the statistics induced by reclustering, it is reasonable to reject the null hypothesis and employ gross clusters. I compare the performance of reclustering with that of a few previous tests using Monte Carlo simulation and application.

</details>


### [10] [Geometric modelling of spatial extremes](https://arxiv.org/abs/2511.08192)
*Lydia Kakampakou,Jennifer L. Wadsworth*

Main category: stat.ME

TL;DR: 该论文将极值统计的几何方法应用于空间建模，通过构建空间参数化的标度函数和角度分布模型来推断极值依赖性和进行外推，并与经典空间极值分析方法进行比较。


<details>
  <summary>Details</summary>
Motivation: 将极值统计中的几何方法扩展到空间建模领域，探索其在推断极值依赖性和外推方面的有效性，以改进空间极值分析。

Method: 基于Wadsworth和Campbell(2024)的框架，使用截断伽马分布对极值半径建模，并提出两种空间角度分布候选模型。通过结合径向和角度模型的抽样来估计极端事件概率。

Result: 与两种经典空间极值分析框架相比，该方法通常能够提供无偏推断，尽管不确定性更大。方法应用于空间天气数据集中的地磁场日波动分析。

Conclusion: 几何方法为空间极值分析提供了有效的建模工具，能够进行无偏推断，但在不确定性方面需要权衡。

Abstract: Recent developments in extreme value statistics have established the so-called geometric approach as a powerful modelling tool for multivariate extremes. We tailor these methods to the case of spatial modelling and examine their efficacy at inferring extremal dependence and performing extrapolation. The geometric approach is based around a limit set described by a gauge function, which is a key target for inference. We consider a variety of spatially-parameterised gauge functions and perform inference on them by building on the framework of Wadsworth and Campbell (2024), where extreme radii are modelled via a truncated gamma distribution. We also consider spatial modelling of the angular distribution, for which we propose two candidate models. Estimation of extreme event probabilities is possible by combining draws from the radial and angular models respectively. We compare our method with two other established frameworks for spatial extreme value analysis and show that our approach generally allows for unbiased, albeit more uncertain, inference compared to the more classical models. We apply the methodology to a space weather dataset of daily geomagnetic field fluctuations.

</details>


### [11] [Clinicians' Interpretation and Preferences for Survival Data Visualisation: A Pre-Post Study Comparing Kaplan-Meier and Mean Residual Life Plots](https://arxiv.org/abs/2511.08332)
*Victor Pacifique Rwandarwacu*

Main category: stat.ME

TL;DR: 该研究评估了医学专业人士对四种生存图类型（KM、生存差异、MRL和MRL差异）的解读能力，发现简短学习后解读准确率从50%提升到81.2%，MRL图改善最大。KM图在临床使用中最受青睐，而MRL图在患者沟通中更有价值。


<details>
  <summary>Details</summary>
Motivation: 虽然Kaplan-Meier图广泛使用，但Mean Residual Life图可能提供更直观的预后显示，但临床医生对这些替代方法的了解和偏好尚不明确。

Method: 采用前后测试的横断面调查，32名医学生和医生在学习前后解读四种生存图类型，分析解读准确性、学习收益和偏好排名。

Result: 总体准确率从学习前的50.0%提高到学习后的81.2%，MRL图的改善最大（+37.5个百分点）。KM图在临床易用性方面最受偏好（59%），而MRL图在患者沟通方面更有价值（9%）。自评生存知识较低的参与者学习收益最大。

Conclusion: 通过简短指导，临床医生能够像解读KM图一样有效地解读MRL图。将MRL可视化纳入临床仪表盘和医学教育可以改善对生存结果的理解和以患者为中心的沟通。

Abstract: Effective visualization of survival data is essential for clinician interpretation and patient communication. While Kaplan-Meier (KM) plots are widely used, Mean Residual Life (MRL) plots may offer a more intuitive display of prognosis over time. However, little is known about clinicians' knowledge and preferences regarding these alternatives. This pre-post pilot cross-sectional survey assessed 32 medical students and doctors who interpreted four survival plot types (KM, survival difference, MRL, and MRL difference) before and after a brief learning section. Interpretation accuracy, learning gain, and ranking preferences were analyzed. Overall accuracy improved from 50.0 percent pre-learning to 81.2 percent post-learning (p = 0.002), with the largest improvement for MRL plots (+37.5 percentage points). KM plots remained the most preferred for ease of clinical use (59 percent), while MRL plots were valued for patient communication (9 percent). Participants with lower self-rated survival knowledge showed the greatest learning gains. These findings suggest that with minimal instruction, clinicians can interpret MRL plots as effectively as KM plots. Incorporating MRL visualizations into clinical dashboards and medical education could improve understanding of survival outcomes and patient-centered communication.

</details>


### [12] [Principal nested spheres for high-dimensional data](https://arxiv.org/abs/2511.08398)
*Mymuna Monem,Ian L. Dryden,Florence George*

Main category: stat.ME

TL;DR: 本文改进了主嵌套球面方法，提出了新的模型选择方法、快速PNS算法和PNS双标图可视化技术，并在癌症研究数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 原始PNS方法在高维球面数据分析中存在计算速度慢、缺乏有效模型选择和可视化工具的局限性，需要改进以适应高维生物医学数据的分析需求。

Method: 1) 基于Kolmogorov-Smirnov检验、方差检验和似然比检验的模型选择方法；2) 结合主成分分析的快速PNS算法；3) 新的PNS双标图可视化方法，支持从PNS得分回退到原始变量的分析。

Result: 在黑色素瘤蛋白质组学数据（500变量×205患者）和泛癌症数据集（12,478基因×300患者）上的应用表明，PNS双标图能有效选择变量进行分类分析。

Conclusion: 改进的PNS方法为高维球面数据分析提供了更高效的计算工具和可视化手段，在生物医学数据分类中具有实用价值。

Abstract: The method of Principal Nested Spheres (PNS) is a non-linear dimension reduction technique for spherical data. The method is a backwards fitting procedure, starting with fitting a high-dimensional sphere and then successively reducing dimension at each stage. After reviewing the PNS method in detail, we introduce some new methods for model selection at each stage between great and small subspheres, based on the Kolmogorov-Smirnov test, a variance test and a likelihood ratio test. The current PNS fitting method is slow for high-dimensional spherical data, and so we introduce a fast PNS method which involves an initial principal components analysis decomposition to select a basis for lower dimensional PNS. A new visual method called the PNS biplot is introduced for examining the effects of the original variables on the PNS, and this involves procedures for back-fitting from the PNS scores back to the original variables. The methodology is illustrated with two high-dimensional datasets from cancer research: Melanoma proteomics data with 500 variables and 205 patients, and a Pan Cancer dataset with 12,478 genes and 300 patients. In both applications the PNS biplot is used to select variables for effective classification.

</details>


### [13] [Multi-level Latent Variable Models for Coheritability Analysis in Electronic Health Records](https://arxiv.org/abs/2511.08532)
*Yinjun Zhao,Nicholas Tatonetti,Yuanjia Wang*

Main category: stat.ME

TL;DR: 提出了一个用于电子健康记录家族研究的统计框架，联合估计连续和二元表型的遗传率和遗传相关性，解决了现有方法在家族相关结构、表型异质性和计算可扩展性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录与家族关系数据结合为研究复杂表型的遗传结构提供了独特机会，但现有方法无法充分处理家族相关结构的复杂性、表型类型的异质性以及计算可扩展性问题。

Method: 基于多层次潜变量模型，将表型协方差分解为可解释的遗传和环境成分，结合家族内和家族间变异，采用基于广义方程估计的迭代算法进行估计。

Result: 模拟研究显示估计量在不同参数设置下具有一致性且推断有效，真实世界应用揭示了心理健康状况与内分泌/代谢表型之间存在显著遗传相关性。

Conclusion: 该工作为高维电子健康记录数据中的共遗传性分析提供了可扩展且严谨的框架，有助于识别复杂疾病网络中的共享遗传影响。

Abstract: Electronic health records (EHRs) linked with familial relationship data offer a unique opportunity to investigate the genetic architecture of complex phenotypes at scale. However, existing heritability and coheritability estimation methods often fail to account for the intricacies of familial correlation structures, heterogeneity across phenotype types, and computational scalability. We propose a robust and flexible statistical framework for jointly estimating heritability and genetic correlation among continuous and binary phenotypes in EHR-based family studies. Our approach builds on multi-level latent variable models to decompose phenotypic covariance into interpretable genetic and environmental components, incorporating both within- and between-family variations. We derive iteration algorithms based on generalized equation estimations (GEE) for estimation. Simulation studies under various parameter configurations demonstrate that our estimators are consistent and yield valid inference across a range of realistic settings. Applying our methods to real-world EHR data from a large, urban health system, we identify significant genetic correlations between mental health conditions and endocrine/metabolic phenotypes, supporting hypotheses of shared etiology. This work provides a scalable and rigorous framework for coheritability analysis in high-dimensional EHR data and facilitates the identification of shared genetic influences in complex disease networks.

</details>


### [14] [Reluctant Transfer Learning in Penalized Regressions for Individualized Treatment Rules under Effect Heterogeneity](https://arxiv.org/abs/2511.08559)
*Eun Jeong Oh,Min Qian*

Main category: stat.ME

TL;DR: 提出了一种不情愿迁移学习框架，用于在治疗-协变量关系发生变化时更新个性化治疗规则模型，通过选择性迁移源模型组件来适应目标数据。


<details>
  <summary>Details</summary>
Motivation: 在精准医疗实践中，基于源数据训练的治疗决策模型需要适应新的目标数据集，这些数据集可能表现出治疗效应的变化，但现有研究缺乏针对这种场景的模型更新方法。

Method: 采用不情愿迁移学习框架，选择性地从源数据向目标数据迁移必要的模型组件（如回归系数），仅在能提升目标数据集性能时进行模型调整，支持多臂治疗设置并执行变量选择。

Result: 通过模拟研究和BestAIR试验的真实数据应用，证明RTL方法优于现有替代方案，提供了理论上的价值收敛保证。

Conclusion: 该框架为在治疗效应条件不断变化的情况下进行自适应治疗决策提供了一种高效且实际可行的解决方案。

Abstract: Estimating individualized treatment rules (ITRs) is fundamental to precision medicine, where the goal is to tailor treatment decisions to individual patient characteristics. While numerous methods have been developed for ITR estimation, there is limited research on model updating that accounts for shifted treatment-covariate relationships in the ITR setting. In real-world practice, models trained on source data must be updated for new (target) datasets that exhibit shifts in treatment effects. To address this challenge, we propose a Reluctant Transfer Learning (RTL) framework that enables efficient model adaptation by selectively transferring essential model components (e.g., regression coefficients) from source to target data, without requiring access to individual-level source data. Leveraging the principle of reluctant modeling, the RTL approach incorporates model adjustments only when they improve performance on the target dataset, thereby controlling complexity and enhancing generalizability. Our method supports multi-armed treatment settings, performs variable selection for interpretability, and provides theoretical guarantees for the value convergence. Through simulation studies and an application to a real data example from the Best Apnea Interventions for Research (BestAIR) trial, we demonstrate that RTL outperforms existing alternatives. The proposed framework offers an efficient, practically feasible approach to adaptive treatment decision-making under evolving treatment effect conditions.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [15] [gemlib: Probabilistic programming for epidemic models](https://arxiv.org/abs/2511.08124)
*Alin Morariu,Jess Bridgen,Chris Jewell*

Main category: stat.CO

TL;DR: gemlib是一个Python库，用于定义、模拟和校准马尔可夫状态转移模型，特别针对流行病学模型，利用JAX和TensorFlow Probability框架实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 随机模型在疫情应对中计算密集，难以实用，尽管比确定性模型解释性更好。需要工具来简化随机模型的实现和校准。

Method: 将状态转移模型分解为三个关键要素，使用Gillespie算法进行连续时间模拟和Tau-leaping算法进行离散时间模拟，与MCMC采样器无缝集成。

Result: 开发了gemlib库，能够利用现代硬件加速计算，让建模者专注于模型结构和假设的测试。

Conclusion: gemlib使用户能够快速实现和校准随机流行病模型，为新兴疫情决策提供所需的灵活性和鲁棒性。

Abstract: gemlib is a Python library for defining, simulating, and calibrating Markov state-transition models. Stochastic models are often computationally intensive, making them impractical to use in pandemic response efforts despite their favourable interpretations compared to their deterministic counterparts. gemlib decomposes state-transition models into three key ingredients which succinctly encapsulate the model and are sufficient for executing the subsequent computational routines. Simulation is performed using implementations of Gillespie's algorithm for continuous-time models and a generic Tau-leaping algorithm for discrete time models. gemlib models integrate seamlessly with Markov Chain Monte Carlo samplers as they provide a target distribution for the inference algorithm. Algorithms are implemented using the machine learning computational frameworks JAX and TensorFlow Probability, thus taking advantage of modern hardware to accelerate computation. This abstracts away computational concerns from modellers, allowing them to focus on developing and testing different model structures or assumptions. The gemlib library enables users to rapidly implement and calibrate stochastic epidemic models with the flexibility and robustness required to support decision during an emerging outbreak.

</details>


### [16] [Improved Bounds for Context-Dependent Evolutionary Models Using Sequential Monte Carlo](https://arxiv.org/abs/2511.07736)
*Joseph Mathews,Scott C. Schmidler*

Main category: stat.CO

TL;DR: 提出了一种用于近似依赖位点模型下边际序列似然的顺序蒙特卡罗算法，该算法比现有的重要性采样方法有显著改进，并提供了多项式混合时间界限和有限样本近似误差界限。


<details>
  <summary>Details</summary>
Motivation: 进化模型中位点依赖的统计推断是系统发育学和计算生物学中长期存在的挑战，需要开发高效的方法来近似边际序列似然。

Method: 使用马尔可夫链蒙特卡罗算法采样潜在样本路径的条件分布，并引入顺序蒙特卡罗算法来近似边际似然，结合重要性采样和有限样本SMC结果。

Result: 证明了MCMC算法在热启动初始化时具有多项式混合时间界限，SMC算法为许多实际问题提供了高效随机化近似方案。

Conclusion: 该方法结合了MCMC和SMC采样器界限的最新进展，可能适用于其他近似边际似然和贝叶斯因子的问题。

Abstract: Statistical inference in evolutionary models with site-dependence is a long-standing challenge in phylogenetics and computational biology. We consider the problem of approximating marginal sequence likelihoods under dependent-site models of biological sequence evolution. We prove a polynomial mixing time bound for a Markov chain Monte Carlo algorithm that samples the conditional distribution over latent sample paths, when the chain is initialized with a warm start. We then introduce a sequential Monte Carlo (SMC) algorithm for approximating the marginal likelihood, and show that our mixing time bound can be combined with recent importance sampling and finite-sample SMC results to obtain bounds on the finite sample approximation error of the resulting estimator. Our results show that the proposed SMC algorithm yields an efficient randomized approximation scheme for many practical problems of interest, and offers a significant improvement over a recently developed importance sampler for this problem. Our approach combines recent innovations in obtaining bounds for MCMC and SMC samplers, and may prove applicable to other problems of approximating marginal likelihoods and Bayes factors.

</details>


### [17] [A Closed-Form Diffusion Model for Learnring Dynamics from Marginal Observations](https://arxiv.org/abs/2511.07786)
*Hanwen Huang*

Main category: stat.CO

TL;DR: 提出了一个封闭形式的薛定谔桥框架，统一并扩展了已知的封闭形式解，包括Schrödinger Föllmer过程和Gaussian SB。开发了无需模拟的算法，直接从源分布和目标分布的样本推断SB动态。


<details>
  <summary>Details</summary>
Motivation: 基于分数的生成模型学习从简单高斯分布到复杂数据分布的变换。为了推广这些变换到任意分布之间，需要解决薛定谔桥问题，但现有方法依赖迭代随机模拟，不稳定且成本高。

Method: 引入封闭形式的框架学习SB动态，统一并扩展已知封闭形式解。基于此开发模拟自由算法，直接从源和目标分布的样本推断SB动态。

Result: 在单细胞发育轨迹建模和图像修复任务（如修复和去模糊）中验证了方法的有效性。

Conclusion: 提出的封闭形式框架为学习薛定谔桥动态提供了更稳定和高效的解决方案，统一了现有方法并扩展了应用范围。

Abstract: Score-based generative models learn transformations from a simple Gaussian to complex data distributions. To generalize these transformations between arbitrary distributions, recent work has focused on the Schrödinger Bridge (SB) problem. However, SB solutions are rarely available in closed form, and existing methods rely on iterative stochastic simulations that are often unstable and costly. We introduce a closed-form framework for learning SB dynamics that unifies and extends previously known closed-form solutions, including the Schrödinger Föllmer process and the Gaussian SB. Notably, the classical Gaussian SB solution arises as an immediate corollary of our formulation. Based on this result, we develop a simulation-free algorithm that directly infers SB dynamics from samples of the source and target distributions. We demonstrate the approach in modeling single-cell developmental trajectories and in image restoration tasks such as inpainting and deblurring.

</details>


### [18] [A Fast and Accurate Approach for Covariance Matrix Construction](https://arxiv.org/abs/2511.08223)
*Felix Reichel*

Main category: stat.CO

TL;DR: 论文扩展了Bariance概念到协方差矩阵，提出了无需显式中心化的高效计算方法，在Python中相比numpy.cov显示出运行时间优势。


<details>
  <summary>Details</summary>
Motivation: 将Reichel(2025)提出的Bariance概念从标量扩展到矩阵，寻找比传统协方差计算更高效的替代方法。

Method: 使用代数恒等式将协方差矩阵表示为$\mathrm{Cov}(X)=\frac{1}{n-1}\left(X^\top X-\frac{1}{n}ss^\top\right)$，其中$s=X^\top \mathbf{1}_n$，避免了显式中心化操作。

Result: 计算简化为单个$p\times p$外积矩阵和一次减法，在非BLAS优化设置下比numpy.cov有明显运行时间优势。使用RXTX等快速Gram例程可进一步降低总成本。

Conclusion: 提出的协方差矩阵计算方法在代数上等同于成对差分形式，但计算效率更高，特别适合大规模数据处理。

Abstract: Reichel (2025) defined the Bariance as $\mathrm{Bariance}(x)=\frac{1}{n(n-1)}\sum_{i<j}(x_i-x_j)^2$, which admits an $O(n)$ reformulation using scalar sums. We extend this to the covariance matrix by showing that $\mathrm{Cov}(X)=\frac{1}{n-1}\!\left(X^\top X-\frac{1}{n}\,s\,s^\top\right)$ with $s=X^\top \mathbf{1}_n$ is algebraically identical to the pairwise-difference form yet avoids explicit centering. Computation reduces to a single $p\times p$ outer matrix product and one subtraction. Empirical benchmarks in Python show clear runtime gains over numpy.cov in non-BLAS-tuned settings. Faster Gram routines such as RXTX (Rybin et. al) for $XX^\top$ further reduce total cost.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [19] [Tractable Instances of Bilinear Maximization: Implementing LinUCB on Ellipsoids](https://arxiv.org/abs/2511.07504)
*Raymond Zhang,Hédi Hadiji,Richard Combes*

Main category: stat.ML

TL;DR: 该论文研究了在凸集X和椭球集Θ上最大化x⊤θ的问题，这是线性赌博机中的核心问题。作者证明了对于某些集合（如ℓp球，p>2），除非P=NP，否则不存在高效算法。然后提出了两种新算法，当X是中心椭球时能高效解决该问题。


<details>
  <summary>Details</summary>
Motivation: 线性赌博机中的乐观算法需要在每个时间步解决x⊤θ在凸集X和椭球Θ上的最大化问题，但现有方法在高维情况下缺乏高效实现。

Method: 提出了两种新算法：第一种利用椭球特性将问题转化为可高效求解的形式；第二种通过数学变换处理中心椭球情况。

Result: 证明了对于ℓp球（p>2）等集合，该问题是NP难的；但针对中心椭球情况，提出的算法能够高效求解。

Conclusion: 这是首个已知的实现高维线性赌博机乐观算法的方法，为相关领域提供了理论基础和实用工具。

Abstract: We consider the maximization of $x^\top θ$ over $(x,θ) \in \mathcal{X} \times Θ$, with $\mathcal{X} \subset \mathbb{R}^d$ convex and $Θ\subset \mathbb{R}^d$ an ellipsoid. This problem is fundamental in linear bandits, as the learner must solve it at every time step using optimistic algorithms. We first show that for some sets $\mathcal{X}$ e.g. $\ell_p$ balls with $p>2$, no efficient algorithms exist unless $\mathcal{P} = \mathcal{NP}$. We then provide two novel algorithms solving this problem efficiently when $\mathcal{X}$ is a centered ellipsoid. Our findings provide the first known method to implement optimistic algorithms for linear bandits in high dimensions.

</details>


### [20] [Infinite-Dimensional Operator/Block Kaczmarz Algorithms: Regret Bounds and $λ$-Effectiveness](https://arxiv.org/abs/2511.07604)
*Halyun Jeong,Palle E. T. Jorgensen,Hyun-Kyoung Kwon,Myung-Sin Song*

Main category: stat.ML

TL;DR: 本文研究了基于投影的线性回归算法，特别是广义Kaczmarz算法中的松弛参数作用，建立了带明确λ依赖性的先验遗憾界，并分析了松弛参数的影响。


<details>
  <summary>Details</summary>
Motivation: 研究现代机器学习模型中的投影线性回归算法，特别是量化算法性能与最优性能的偏离程度，并针对有界算子和噪声数据等实际机器学习场景建立理论框架。

Method: 使用广义Kaczmarz算法框架，分析松弛参数的作用，建立先验遗憾界，并将方法扩展到无限维希尔伯特空间中的有界算子，采用（块）Kaczmarz算法更新。

Result: 获得了明确的遗憾界估计，包括Kaczmarz算法模型、非正交傅里叶展开以及噪声Kaczmarz算法的遗憾界，提供了松弛参数的详细分析。

Conclusion: 提出了一个更广泛的框架来处理无限维希尔伯特空间中的有界算子，通过（块）Kaczmarz算法更新，得到了新颖且多功能的结果，适用于现代机器学习实践。

Abstract: We present a variety of projection-based linear regression algorithms with a focus on modern machine-learning models and their algorithmic performance. We study the role of the relaxation parameter in generalized Kaczmarz algorithms and establish a priori regret bounds with explicit $λ$-dependence to quantify how much an algorithm's performance deviates from its optimal performance. A detailed analysis of relaxation parameter is also provided. Applications include: explicit regret bounds for the framework of Kaczmarz algorithm models, non-orthogonal Fourier expansions, and the use of regret estimates in modern machine learning models, including for noisy data, i.e., regret bounds for the noisy Kaczmarz algorithms. Motivated by machine-learning practice, our wider framework treats bounded operators (on infinite-dimensional Hilbert spaces), with updates realized as (block) Kaczmarz algorithms, leading to new and versatile results.

</details>


### [21] [Semi-Supervised Treatment Effect Estimation with Unlabeled Covariates via Generalized Riesz Regression](https://arxiv.org/abs/2511.08303)
*Masahiro Kato*

Main category: stat.ML

TL;DR: 本文研究了半监督设置下的治疗效果估计问题，通过引入未标记的辅助协变量来降低效率界限并提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 在治疗效果估计中，传统方法只使用协变量、治疗指标和结果的标准三元组。本研究旨在利用未标记的辅助协变量来改进估计效率。

Method: 提出了两种数据生成过程：单样本设置（审查设置）和双样本设置（病例对照设置）。开发了效率界限和高效估计器，其渐近方差与效率界限一致。

Result: 研究发现，通过整合辅助协变量，可以降低效率界限，获得比不使用辅助协变量时渐近方差更小的估计器。

Conclusion: 在半监督设置下，利用未标记的辅助协变量能够显著提高治疗效果估计的效率，为因果推断提供了新的改进途径。

Abstract: This study investigates treatment effect estimation in the semi-supervised setting, where we can use not only the standard triple of covariates, treatment indicator, and outcome, but also unlabeled auxiliary covariates. For this problem, we develop efficiency bounds and efficient estimators whose asymptotic variance aligns with the efficiency bound. In the analysis, we introduce two different data-generating processes: the one-sample setting and the two-sample setting. The one-sample setting considers the case where we can observe treatment indicators and outcomes for a part of the dataset, which is also called the censoring setting. In contrast, the two-sample setting considers two independent datasets with labeled and unlabeled data, which is also called the case-control setting or the stratified setting. In both settings, we find that by incorporating auxiliary covariates, we can lower the efficiency bound and obtain an estimator with an asymptotic variance smaller than that without such auxiliary covariates.

</details>


### [22] [Robust Experimental Design via Generalised Bayesian Inference](https://arxiv.org/abs/2511.07671)
*Yasir Zubayr Barlas,Sabina J. Sloman,Samuel Kaski*

Main category: stat.ML

TL;DR: 提出了广义贝叶斯最优实验设计（GBOED），通过用损失函数替代似然函数来增强对模型误设的鲁棒性，并推导了新的Gibbs期望信息增益采集函数。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯最优实验设计依赖模型正确设定的假设，当模型误设时会导致推断不佳和信息增益估计不准确。需要更鲁棒的方法来处理异常值和噪声分布假设错误的情况。

Method: 将广义贝叶斯（Gibbs）推断扩展到实验设计设置，用合适的损失函数替代贝叶斯更新中的似然函数，推导出Gibbs期望信息增益（Gibbs EIG）采集函数。

Result: 实证结果表明，GBOED增强了对异常值和结果噪声分布错误假设的鲁棒性。

Conclusion: GBOED框架在实验设计中实现了设计和推断的双重鲁棒性，为模型误设情况下的实验设计提供了更可靠的解决方案。

Abstract: Bayesian optimal experimental design is a principled framework for conducting experiments that leverages Bayesian inference to quantify how much information one can expect to gain from selecting a certain design. However, accurate Bayesian inference relies on the assumption that one's statistical model of the data-generating process is correctly specified. If this assumption is violated, Bayesian methods can lead to poor inference and estimates of information gain. Generalised Bayesian (or Gibbs) inference is a more robust probabilistic inference framework that replaces the likelihood in the Bayesian update by a suitable loss function. In this work, we present Generalised Bayesian Optimal Experimental Design (GBOED), an extension of Gibbs inference to the experimental design setting which achieves robustness in both design and inference. Using an extended information-theoretic framework, we derive a new acquisition function, the Gibbs expected information gain (Gibbs EIG). Our empirical results demonstrate that GBOED enhances robustness to outliers and incorrect assumptions about the outcome noise distribution.

</details>


### [23] [Source-Optimal Training is Transfer-Suboptimal](https://arxiv.org/abs/2511.08401)
*C. Evans Hedges*

Main category: stat.ML

TL;DR: 论文证明了迁移学习中存在基本错位：最小化源任务风险的源正则化几乎从不与最大化迁移收益的正则化一致。通过L2-SP岭回归的尖锐相边界，刻画了迁移最优的源惩罚项τ₀*，并显示其可预测地偏离任务最优值，在高信噪比下需要更强的正则化，在低信噪比下需要更弱的正则化。


<details>
  <summary>Details</summary>
Motivation: 研究迁移学习中源任务正则化与迁移性能之间的基本关系，揭示传统任务最优正则化与迁移最优正则化之间的系统性差异。

Method: 使用L2-SP岭回归模型，通过理论分析建立尖锐相边界，刻画迁移最优的源惩罚项τ₀*，并在CIFAR-10和MNIST数据集上进行非线性网络实验验证。

Result: 发现迁移最优的源正则化与任务最优正则化存在系统性偏离：高信噪比下需要更强的正则化，低信噪比下需要更弱的正则化。在各项同性设置中，迁移决策与目标样本量和噪声无关，仅取决于任务对齐度和源特征。

Conclusion: 迁移学习存在基本错位，源任务的最优正则化策略与最大化迁移收益的正则化策略不一致，这一反直觉模式在非线性网络中依然存在，对迁移学习实践具有重要指导意义。

Abstract: We prove a fundamental misalignment in transfer learning: the source regularization that minimizes source risk almost never coincides with the regularization maximizing transfer benefit. Through sharp phase boundaries for L2-SP ridge regression, we characterize the transfer-optimal source penalty $τ_0^*$ and show it diverges predictably from task-optimal values, requiring stronger regularization in high-SNR regimes and weaker regularization in low-SNR regimes. Additionally, in isotropic settings the decision to transfer is remarkably independent of target sample size and noise, depending only on task alignment and source characteristics. CIFAR-10 and MNIST experiments confirm this counterintuitive pattern persists in non-linear networks.

</details>


### [24] [Distributionally Robust Online Markov Game with Linear Function Approximation](https://arxiv.org/abs/2511.07831)
*Zewu Zheng,Yuanyuan Lin*

Main category: stat.ML

TL;DR: 提出首个样本高效的鲁棒粗相关均衡算法DR-CCE-LSI，解决多智能体强化学习中的sim-to-real差距问题，在满足特征映射条件下实现O(dH√K)的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中sim-to-real差距问题，即模拟器训练的策略在真实环境中性能显著下降的挑战，特别是在多智能体设置下学习鲁棒策略。

Method: 采用分布鲁棒RL框架，假设环境动态转移满足d-矩形性，提出DR-CCE-LSI算法——一种带有为多智能体设计的探索奖励的最小二乘值迭代算法。

Result: 当特征映射满足特定性质时，算法能以O{dHmin{H,1/min{σ_i}}√K}的遗憾界实现ε-近似粗相关均衡，其中K是交互轮数，H是时间步长，d是特征维度，σ_i是玩家i的不确定性水平。

Conclusion: 该工作是该设置下首个样本高效算法，与单智能体设置的最佳结果匹配，在特征维度d上达到极小极大最优样本复杂度，并通过仿真验证了算法的有效性。

Abstract: The sim-to-real gap, where agents trained in a simulator face significant performance degradation during testing, is a fundamental challenge in reinforcement learning. Extansive works adopt the framework of distributionally robust RL, to learn a policy that acts robustly under worst case environment shift. Within this framework, our objective is to devise algorithms that are sample efficient with interactive data collection and large state spaces. By assuming d-rectangularity of environment dynamic shift, we identify a fundamental hardness result for learning in online Markov game, and address it by adopting minimum value assumption. Then, a novel least square value iteration type algorithm, DR-CCE-LSI, with exploration bonus devised specifically for multiple agents, is proposed to find an \episilon-approximate robust Coarse Correlated Equilibrium(CCE). To obtain sample efficient learning, we find that: when the feature mapping function satisfies certain properties, our algorithm, DR-CCE-LSI, is able to achieve ε-approximate CCE with a regret bound of O{dHmin{H,1/min{σ_i}}\sqrt{K}}, where K is the number of interacting episodes, H is the horizon length, d is the feature dimension, and \simga_i represents the uncertainty level of player i. Our work introduces the first sample-efficient algorithm for this setting, matches the best result so far in single agent setting, and achieves minimax optimalsample complexity in terms of the feature dimension d. Meanwhile, we also conduct simulation study to validate the efficacy of our algorithm in learning a robust equilibrium.

</details>


### [25] [PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure](https://arxiv.org/abs/2511.07997)
*Ke Jia,Yuheng Ma,Yang Li,Feifei Wang*

Main category: stat.ML

TL;DR: 提出PrAda-GAN方法，结合GAN和边际方法的优势，通过序列生成器架构捕获变量间复杂依赖关系，并自适应正则化贝叶斯网络结构以促进稀疏性。


<details>
  <summary>Details</summary>
Motivation: 解决基于边际的差分隐私合成数据生成方法的核心局限性，需要更好地捕获变量间的复杂依赖关系。

Method: 采用序列生成器架构，自适应正则化学习到的贝叶斯网络结构以促进稀疏性，结合GAN和边际方法的优势。

Result: 理论分析显示利用依赖稀疏性显著提高了收敛速率；实证实验表明在隐私-效用权衡方面优于现有表格数据合成方法。

Conclusion: PrAda-GAN在差分隐私合成数据生成中实现了更好的隐私-效用权衡，特别是在捕获复杂变量依赖关系方面表现优异。

Abstract: We revisit the problem of generating synthetic data under differential privacy. To address the core limitations of marginal-based methods, we propose the Private Adaptive Generative Adversarial Network with Bayes Network Structure (PrAda-GAN), which integrates the strengths of both GAN-based and marginal-based approaches. Our method adopts a sequential generator architecture to capture complex dependencies among variables, while adaptively regularizing the learned structure to promote sparsity in the underlying Bayes network. Theoretically, we establish diminishing bounds on the parameter distance, variable selection error, and Wasserstein distance. Our analysis shows that leveraging dependency sparsity leads to significant improvements in convergence rates. Empirically, experiments on both synthetic and real-world datasets demonstrate that PrAda-GAN outperforms existing tabular data synthesis methods in terms of the privacy-utility trade-off.

</details>


### [26] [Concentration bounds on response-based vector embeddings of black-box generative models](https://arxiv.org/abs/2511.08307)
*Aranyak Acharyya,Joshua Agterberg,Youngser Park,Carey E. Priebe*

Main category: stat.ML

TL;DR: 本文为数据核视角空间嵌入方法建立了高概率浓度界限，确定了近似总体水平向量嵌入所需的样本响应数量。


<details>
  <summary>Details</summary>
Motivation: 生成模型的响应式向量嵌入便于统计分析，但需要知道需要多少样本响应才能以所需精度近似总体水平向量嵌入。

Method: 使用数据核视角空间嵌入方法，在适当的正则条件下建立样本向量嵌入的高概率浓度界限。

Result: 建立了浓度界限，确定了近似总体水平向量嵌入所需的样本响应数量。

Conclusion: 所建立的代数工具还可用于建立经典多维尺度嵌入的浓度界限，当相似度测量存在噪声时。

Abstract: Generative models, such as large language models or text-to-image diffusion models, can generate relevant responses to user-given queries. Response-based vector embeddings of generative models facilitate statistical analysis and inference on a given collection of black-box generative models. The Data Kernel Perspective Space embedding is one particular method of obtaining response-based vector embeddings for a given set of generative models, already discussed in the literature. In this paper, under appropriate regularity conditions, we establish high probability concentration bounds on the sample vector embeddings for a given set of generative models, obtained through the method of Data Kernel Perspective Space embedding. Our results tell us the required number of sample responses needed in order to approximate the population-level vector embeddings with a desired level of accuracy. The algebraic tools used to establish our results can be used further for establishing concentration bounds on Classical Multidimensional Scaling embeddings in general, when the dissimilarities are observed with noise.

</details>
