<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 10]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.AP](#stat.AP) [Total: 4]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [On Meta-Evaluation](https://arxiv.org/abs/2601.14262)
*Hongxiao Li,Chenxi Wang,Fanda Fan,Zihan Wang,Wanling Gao,Lei Wang,Jianfeng Zhan*

Main category: stat.ME

TL;DR: 该论文提出了一个元评估框架AxiaBench，首次大规模比较了10种评估方法在8个应用领域的效果，发现现有方法无法同时保证准确性和效率，而分层抽样方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估是实证科学的基础，但元评估（对评估本身的评估）发展严重不足。虽然观察研究、实验设计、随机对照试验等方法塑造了现代科学实践，但缺乏对其跨领域有效性和实用性的系统研究。

Method: 提出了一个形式化的元评估框架，定义了评估空间及其结构化表示，并创建了名为AxiaBench的基准。使用该基准对10种广泛使用的评估方法在8个代表性应用领域进行了首次大规模定量比较。

Result: 分析揭示了一个根本性局限：现有方法无法在多样化场景中同时实现准确性和效率，特别是实验设计和观察研究设计与真实世界基准存在显著偏差。评估了先前评估学研究中的全空间分层抽样统一方法，结果显示该方法在所有测试领域中始终优于先前方法。

Conclusion: 这些结果确立了元评估作为一个独立的科学对象，为推进计算和实验研究中可信评估提供了概念基础和实用工具集。

Abstract: Evaluation is the foundation of empirical science, yet the evaluation of evaluation itself -- so-called meta-evaluation -- remains strikingly underdeveloped. While methods such as observational studies, design of experiments (DoE), and randomized controlled trials (RCTs) have shaped modern scientific practice, there has been little systematic inquiry into their comparative validity and utility across domains. Here we introduce a formal framework for meta-evaluation by defining the evaluation space, its structured representation, and a benchmark we call AxiaBench. AxiaBench enables the first large-scale, quantitative comparison of ten widely used evaluation methods across eight representative application domains. Our analysis reveals a fundamental limitation: no existing method simultaneously achieves accuracy and efficiency across diverse scenarios, with DoE and observational designs in particular showing significant deviations from real-world ground truth. We further evaluate a unified method of entire-space stratified sampling from previous evaluatology research, and the results report that it consistently outperforms prior approaches across all tested domains. These results establish meta-evaluation as a scientific object in its own right and provide both a conceptual foundation and a pragmatic tool set for advancing trustworthy evaluation in computational and experimental research.

</details>


### [2] [Geostatistics from Elliptic Boundary-Value Problems: Green Operators, Transmission Conditions, and Schur Complements](https://arxiv.org/abs/2601.14937)
*Juan J. Segura*

Main category: stat.ME

TL;DR: 该论文提出了一种基于算子的高斯空间随机场建模框架，将边界条件和传输条件作为统计模型的显式组成部分，建立了精度-协方差的对应关系，并展示了变差函数如何依赖于边界条件和域几何。


<details>
  <summary>Details</summary>
Motivation: 传统地统计学在欧几里得域上通过变差函数或协方差核编码空间依赖性，而SPDE-GMRF范式通过椭圆精度算子指定高斯场。本文旨在开发一个算子基础的框架，在具有内部界面的有界域和流形上建模高斯空间随机场，将边界和传输条件作为统计模型的显式组成部分。

Method: 从强制二次能量泛函出发，利用变分理论建立精度-协方差的精确对应关系，展示变差函数是格林算子的导出二次泛函。通过表面惩罚项建模界面，变分产生通量跳跃传输条件。使用Dirichlet-to-Neumann算子和Schur补进行边界驱动的预测和域约简。

Result: 建立了边界条件和域几何对协方差建模的显式影响，展示了硬约束可以通过精确插值约束或分布源项等价表示，界面建模导致跨界面协方差的受控衰减，提供了上尺度化、支持度变化和子域到边界映射的算子语言。

Conclusion: 该算子框架将空间统计和椭圆PDE理论的标准工具结合，使边界和界面效应在协方差建模和预测中保持显式，为具有复杂几何和边界条件的高斯空间随机场提供了统一的建模方法。

Abstract: Classical geostatistics encodes spatial dependence by prescribing variograms or covariance kernels on Euclidean domains, whereas the SPDE--GMRF paradigm specifies Gaussian fields through an elliptic precision operator whose inverse is the corresponding Green operator. We develop an operator-based formulation of Gaussian spatial random fields on bounded domains and manifolds with internal interfaces, treating boundary and transmission conditions as explicit components of the statistical model. Starting from coercive quadratic energy functionals, variational theory yields a precise precision--covariance correspondence and shows that variograms are derived quadratic functionals of the Green operator, hence depend on boundary conditions and domain geometry. Conditioning and kriging follow from standard Gaussian update identities in both covariance and precision form, with hard constraints represented equivalently by exact interpolation constraints or by distributional source terms. Interfaces are modelled via surface penalty terms; taking variations produces flux-jump transmission conditions and induces controlled attenuation of cross-interface covariance. Finally, boundary-driven prediction and domain reduction are formulated through Dirichlet-to-Neumann operators and Schur complements, providing an operator language for upscaling, change of support, and subdomain-to-boundary mappings. Throughout, we use tools standard in spatial statistics and elliptic PDE theory to keep boundary and interface effects explicit in covariance modeling and prediction.

</details>


### [3] [A Bayesian framework for cost-effectiveness analysis with time-varying treatment decisions](https://arxiv.org/abs/2601.14309)
*Esteban Fernández-Morales,Emily M. Ko,Nandita Mitra,Youjin Lee,Arman Oganisian*

Main category: stat.ME

TL;DR: 提出一个贝叶斯框架，用于在连续时间中联合建模成本和事件时间，以解决观察性索赔数据中的非随机治疗分配、行政审查和不规则医疗访问问题，应用于高风险早期子宫内膜癌的辅助放射治疗成本效益分析。


<details>
  <summary>Details</summary>
Motivation: 现有成本效益分析方法在处理观察性索赔数据时面临挑战：点治疗模型无法处理时变混杂，离散时间模型需要将连续时间分箱并扩展为人-期格式，可能导致零膨胀问题。在高风险早期子宫内膜癌中，辅助放射治疗在子宫切除术后特定时间启动，导致治疗与结果之间的混杂随时间演变。

Method: 提出贝叶斯框架，在连续时间中联合建模成本和事件时间，考虑行政审查，支持动态治疗策略，使用贝叶斯g-计算估计因果可解释的成本效益指标（包括净货币效益），并通过后验对比比较不同策略。

Result: 通过模拟评估了该方法在不同审查水平下的有限样本性能，并与离散时间和完全参数化替代方法进行比较。然后使用SEER-Medicare数据评估高风险早期子宫内膜癌患者在子宫切除术后6个月内启动辅助放射治疗的成本效益。

Conclusion: 该方法为观察性索赔数据中的成本效益分析提供了一个灵活的贝叶斯框架，能够处理连续时间建模、时变混杂和行政审查，特别适用于高风险早期子宫内膜癌等需要动态治疗决策的临床场景。

Abstract: Cost-effectiveness analyses (CEAs) compare the costs and health outcomes of treatment regimes to inform medical decisions. With observational claims data, CEAs must address nonrandom treatment assignment, administrative censoring, and irregularly spaced medical visits that reflect the continuous timing of care and treatment initiation. In high-risk, early-stage endometrial cancer (HR-EC), adjuvant radiation is initiated at patient-specific times following hysterectomy, causing confounding between treatment and outcomes that can evolve with post-surgical recovery and clinical course. Most existing CEA methods use point-treatment or discrete-time models. However, point-treatment approaches break down with time-varying confounding, while discrete-time models bin continuous time, expand the data into a person-period format, and can induce zero-inflation by creating many intervals with no cost-accruing events. We propose a Bayesian framework for CEAs with sequential decision-making that jointly models costs and event times in continuous time, accounts for administrative censoring, and supports dynamic treatment regimes with minimal parametric assumptions. We use Bayesian g-computation to estimate causally interpretable cost-effectiveness measures, including net monetary benefit, and to compare regimes through posterior contrasts. We evaluate the finite-sample performance of the proposed method in simulations across censoring levels and compare it against discrete-time and fully parametric alternatives. We then use SEER-Medicare data to assess the cost-effectiveness of initiating adjuvant radiation therapy within six months following hysterectomy among HR-EC patients.

</details>


### [4] [Doubly robust estimators of the restricted mean time in favor estimands in individual- and cluster-randomized trials](https://arxiv.org/abs/2601.14431)
*Xi Fang,Bingkai Wang,Guangyu Tong,Liangyuan Hu,Shuangge Ma,Fan Li*

Main category: stat.ME

TL;DR: 提出针对多状态生存数据的双重稳健RMT-IF估计方法，支持个体随机化和整群随机化试验


<details>
  <summary>Details</summary>
Motivation: 现有RMT-IF方法存在局限性：非参数方法效率低，假设协变量独立删失，无法处理整群随机化试验，限制了应用范围和效率

Method: 开发基于增强逆概率加权的双重稳健估计框架，结合阶段特异性结局回归和组特异性删失模型；扩展至整群随机化试验，定义集群水平和个体水平平均RMT-IF估计量，构建考虑簇内相关性的双重稳健估计量

Result: 模拟研究验证了有限样本性能；通过两个随机试验实例展示方法应用；在个体随机化和整群随机化设置中均采用模型无关的刀切法方差估计

Conclusion: 提出的双重稳健RMT-IF估计方法解决了现有方法的局限性，提高了效率和适用性，特别适用于整群随机化试验中的多状态生存数据分析

Abstract: Progressive multi-state survival outcomes are common in trials with recurrent or sequential events and require treatment effect estimands that remain interpretable without proportional intensity or Markov assumptions. The restricted mean time in favor of treatment (RMT-IF) extends the restricted mean survival time to ordered multi-state processes and provides such an interpretable estimand. However, existing RMT-IF methods are nonparametric, assume covariate-independent censoring for independent observations, and do not accommodate cluster-randomized trials (CRTs), limiting both efficiency and applicability. We develop a class of doubly robust estimators for RMT-IF under right censoring using an augmented inverse-probability weighting framework that combines stage-specific outcome regression with arm-specific censoring models, yielding consistency when either nuisance model is correctly specified. We further extend the framework to CRTs by formalizing both cluster-level and individual-level average RMT-IF estimands to address informative cluster size and by constructing corresponding doubly robust estimators that account for within-cluster correlation. For inference, we employ model-agnostic jackknife variance estimators in both individually randomized and cluster-randomized settings. Extensive simulation studies demonstrate finite-sample performance, and the methods are illustrated using two randomized trial examples.

</details>


### [5] [The RobinCar Family: R Tools for Robust Covariate Adjustment in Randomized Clinical Trials](https://arxiv.org/abs/2601.14498)
*Marlena Bannick,Yuanyuan Bian,Gregory Chen,Liming Li,Yuhan Qian,Daniel Sabanés Bové,Dong Xi,Ting Ye,Yanyao Yi*

Main category: stat.ME

TL;DR: 开发了RobinCar系列R包，为临床试验提供符合FDA指南的协变量调整分析工具，支持连续、离散和生存时间结局的多种调整方法。


<details>
  <summary>Details</summary>
Motivation: 虽然协变量调整是提高临床试验效率的重要统计技术，且FDA已发布相关指南，但现有统计文献与易于使用且符合最佳实践的软件之间存在差距。

Method: 开发了RobinCar和RobinCar2两个R包，支持连续和离散结局的传统ANCOVA、ANHECOVA、G-computation（含GLM和机器学习模型）、PROCOVA(TM)的超协变量调整，以及生存时间结局的协变量调整log-rank检验、分层协变量调整log-rank检验和边际协变量调整风险比。

Result: 提供了协变量调整统计方法的易用概述，描述了在RobinCar和RobinCar2中的实现方式，并为临床试验从业者提供了重要的使用说明。

Conclusion: 通过分析AIDS临床试验组研究175的数据，证明RobinCar和RobinCar2功能直接且用户友好，便于实际应用。

Abstract: Purpose: Covariate adjustment is a powerful statistical technique that can increase efficiency in clinical trials. Recent guidance from the U.S. FDA provided recommendations and best practices for using covariate adjustment. However, there has existed a gap between the extensive statistical literature on covariate adjustment and software that is easy to use and abides by these best practices.
  Methods: We have developed the RobinCar Family, which is comprised of RobinCar and RobinCar2. These two R packages enable covariate-adjusted analyses for continuous, discrete, and time-to-event outcomes that follow best practices. For continuous and discrete outcomes, the functions in the RobinCar Family facilitate traditional forms of covariate adjustment such as ANCOVA as well as more recent approaches like ANHECOVA, G-computation with generalized linear models and machine learning models, and adjustment for a super-covariate (as in PROCOVA(TM)). Functions for time-to-event outcomes implement the covariate-adjusted log-rank test, the stratified covariate-adjusted log-rank test, and the marginal covariate-adjusted hazard ratio. The RobinCar Family is supported by the ASA Biopharmaceutical Section Covariate Adjustment Scientific Working Group.
  Results: We provide an accessible overview of the covariate-adjusted statistical methods, and describe how they are implemented in RobinCar and RobinCar2. We highlight important usage notes for clinical trial practitioners.
  Conclusion: We apply RobinCar and RobinCar2 functions by analyzing data from the AIDS Clinical Trials Group Study 175, demonstrating that they are straightforward and user-friendly.

</details>


### [6] [Recent advances in the Bradley--Terry Model: theory, algorithms, and applications](https://arxiv.org/abs/2601.14727)
*Shuxing Fang,Ruijian Han,Yuanhang Luo,Yiming Xu*

Main category: stat.ME

TL;DR: 本文综述了Bradley-Terry模型及其扩展的最新进展，重点关注统计和计算方面，特别关注对象数量和比较量都趋于无穷大的大规模应用场景。


<details>
  <summary>Details</summary>
Motivation: 随着大规模比较数据应用的增加（如推荐系统、机器学习偏好对齐等），需要系统性地理解Bradley-Terry模型在对象数量和比较量都趋于无穷大时的统计和计算特性。

Method: 采用文献综述方法，系统梳理Bradley-Terry模型及其扩展的最新研究进展，重点关注统计估计和推断的渐近理论，以及相关算法设计。

Result: 总结了Bradley-Terry模型在大规模场景下的统计理论进展、高效算法设计，以及在机器学习偏好对齐等领域的应用成果。

Conclusion: Bradley-Terry模型在大规模比较数据分析中具有重要价值，但仍面临若干挑战，需要未来研究进一步探索新的理论框架和计算方法。

Abstract: This article surveys recent progress in the Bradley-Terry (BT) model and its extensions. We focus on the statistical and computational aspects, with emphasis on the regime in which both the number of objects and the volume of comparisons tend to infinity, a setting relevant to large-scale applications. The main topics include asymptotic theory for statistical estimation and inference, along with the associated algorithms. We also discuss applications of these models, including recent work on preference alignment in machine learning. Finally, we discuss several key challenges and outline directions for future research.

</details>


### [7] [Global-local shrinkage priors for modeling random effects in multivariate spatial small area estimation](https://arxiv.org/abs/2601.14752)
*Shushi Nishina,Takahiro Onizuka,Shintaro Hashimoto*

Main category: stat.ME

TL;DR: 提出了一种贝叶斯扩展的多变量Fay-Herriot模型，通过全局-局部先验实现随机效应的分量特异性收缩，并加入空间相关性，提高小区域估计的准确性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 多变量Fay-Herriot模型在小区域估计中广泛应用，但现有方法在随机效应收缩方面不够灵活，可能对强信号区域过度收缩。需要一种能够自适应调整每个随机效应分量收缩程度的方法，同时考虑空间相关性以提高估计效率。

Method: 提出贝叶斯扩展的多变量Fay-Herriot模型，采用三明治混合表示的全局-局部先验，实现随机效应的分量特异性自适应收缩。同时将空间依赖性纳入模型，构建空间多变量框架，同时利用跨变量关系和空间结构。

Result: 通过模拟研究和真实调查数据的实证应用证明，该方法比传统方法具有更好的稳健性，能防止对强信号区域的过度收缩，同时通过利用空间相关性提高了估计效率。

Conclusion: 提出的贝叶斯空间多变量Fay-Herriot模型为小区域估计提供了更灵活、更稳健的框架，通过分量特异性收缩和空间相关性整合，显著提高了估计质量。

Abstract: Small area estimation (SAE) plays a central role in survey statistics and epidemiology, providing reliable estimates for domains with limited sample sizes. The multivariate Fay-Herriot model has been extensively used for this purpose, because it enhances estimation accuracy by borrowing strength across multiple correlated variables. In this paper, we develop a Bayesian extension of the multivariate Fay-Herriot model that enables flexible, component-specific shrinkage of the random effects. The proposed approach employs global-local priors formulated through a sandwich mixture representation, allowing adaptive regularization of each element of the random-effect vectors. This construction yields greater robustness and prevents excessive shrinkage in areas exhibiting strong underlying signals. In addition, we incorporate spatial dependence into the model to account for geographical correlation across small areas. The resulting spatial multivariate framework simultaneously exploits cross-variable relationships and spatial structure, yielding improved estimation efficiency. The utility of the proposed method is demonstrated through simulation studies and an empirical application to real survey data.

</details>


### [8] [Graphical model-based clustering of categorical data](https://arxiv.org/abs/2601.14849)
*Laura Ferrini,Federico Castelletti*

Main category: stat.ME

TL;DR: 提出基于图模型的多元分类数据聚类方法，使用狄利克雷过程混合分类图模型，将个体聚类到具有相似依赖结构和参数的组中。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型的聚类方法通常假设变量在给定聚类分配时相互独立，忽略了组间可能的依赖结构差异。对于多元分类数据，需要能够直接考虑变量间依赖关系的聚类方法。

Method: 使用狄利克雷过程混合分类图模型进行聚类，将图模型作为编码变量间依赖关系的工具。开发了完整的贝叶斯推断框架和马尔可夫链蒙特卡洛方案进行后验分析。

Result: 通过模拟和真实案例研究（包括基因组数据和投票记录分析）评估方法，结果显示基于图模型的聚类方法优于不考虑变量间依赖关系的传统方法。

Conclusion: 基于图模型的聚类方法能够有效处理多元分类数据中的依赖结构，在聚类性能上优于忽略变量间依赖关系的传统方法，具有实际应用价值。

Abstract: Clustering multivariate data is a pervasive task in many applied problems, particularly in social studies and life science. Model-based approaches to clustering rely on mixture models, where each mixture component corresponds to the kernel of a distribution characterizing a latent sub-group. Current methods developed within this framework employ multivariate distributions built under the assumption of independence among variables given the cluster allocation. Accordingly, possible dependence structures characterizing differences across groups are not directly accounted for during the clustering process. In this paper we consider multivariate categorical data, and introduce a model-based clustering method which employs graphical models as a tool to encode dependencies between variables. Specifically, we consider a Dirichlet Process mixture of categorical graphical models, which clusters individuals into groups that are homogeneous in terms of dependence (graphical) structure and allied parameters. We provide full Bayesian inference for the model and develop a Markov chain Monte Carlo scheme for posterior analysis. Our method is evaluated through simulations and applied to real case studies, including the analysis of genomic data and voting records. Results reveal the merits of a graphical model-based clustering, in comparison with approaches that do not explicitly account for dependencies in the multivariate distribution of variables.

</details>


### [9] [Consistency of Honest Decision Trees and Random Forests](https://arxiv.org/abs/2601.14991)
*Martin Bladt,Rasmus Frigaard Lemvig*

Main category: stat.ME

TL;DR: 本文研究了回归设置中诚实决策树和随机森林的各种一致性类型，使用经典平滑方法论证，在温和条件下建立了弱收敛、几乎必然收敛和一致收敛结果。


<details>
  <summary>Details</summary>
Motivation: 现有文献对树基方法一致性的分析较为复杂，本文旨在提供更基础、更易理解的证明方法，澄清数据自适应划分与核型方法之间的密切关系。

Method: 采用经典平滑方法的论证框架，在回归函数和数据分布的温和正则条件下，分析诚实树和诚实森林平均值的收敛性质。框架自然容纳了基于子采样的集成变体和两阶段自助采样方案。

Result: 建立了诚实树和诚实森林平均值向真实回归函数的弱收敛和几乎必然收敛，并在紧致协变量域上获得了一致收敛。该框架简化了现有分析，将多个结果作为特例恢复。

Conclusion: 本文通过基础论证阐明了树基方法的渐近行为，揭示了数据自适应划分与核型方法之间的紧密联系，为理解树基方法提供了更易理解的途径。

Abstract: We study various types of consistency of honest decision trees and random forests in the regression setting. In contrast to related literature, our proofs are elementary and follow the classical arguments used for smoothing methods. Under mild regularity conditions on the regression function and data distribution, we establish weak and almost sure convergence of honest trees and honest forest averages to the true regression function, and moreover we obtain uniform convergence over compact covariate domains. The framework naturally accommodates ensemble variants based on subsampling and also a two-stage bootstrap sampling scheme. Our treatment synthesizes and simplifies existing analyses, in particular recovering several results as special cases. The elementary nature of the arguments clarifies the close relationship between data-adaptive partitioning and kernel-type methods, providing an accessible approach to understanding the asymptotic behavior of tree-based methods.

</details>


### [10] [Efficient prior sensitivity analysis for Bayesian model comparison](https://arxiv.org/abs/2601.15132)
*Zixiao Hu,Jason D. McEwen*

Main category: stat.ME

TL;DR: 提出一种基于学习谐波均值估计器(LHME)的贝叶斯模型比较先验敏感性分析方法，通过解耦采样和证据计算，大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 贝叶斯模型比较对先验分布敏感，需要评估不同先验的影响，但传统方法计算成本高昂，需要重复模型拟合或专门采样方案

Method: 利用学习谐波均值估计器(LHME)解耦采样和证据计算，使重采样的后验样本可直接用于计算证据，无需额外似然评估

Result: 在玩具问题和宇宙学案例研究中验证了方法有效性，与完整MCMC采样和嵌套采样重拟合结果一致，在宇宙学案例中计算成本降低高达6000倍

Conclusion: 该方法为贝叶斯模型比较的先验敏感性分析提供了一种计算高效、与采样方法无关的替代方案，显著降低了计算负担

Abstract: Bayesian model comparison implements Occam's razor through its sensitivity to the prior. However, prior-dependence makes it important to assess the influence of plausible alternative priors. Such prior sensitivity analyses for the Bayesian evidence are expensive, either requiring repeated, costly model re-fits or specialised sampling schemes. By exploiting the learned harmonic mean estimator (LHME) for evidence calculation we decouple sampling and evidence calculation, allowing resampled posterior draws to be used directly to calculate the evidence without further likelihood evaluations. This provides an alternative approach to prior sensitivity analysis for Bayesian model comparison that dramatically alleviates the computational cost and is agnostic to the method used to generate posterior samples. We validate our method on toy problems and a cosmological case study, reproducing estimates obtained by full Markov chain Monte Carlo (MCMC) sampling and nested sampling re-fits. For the cosmological example considered our approach achieves up to $6000\times$ lower computational cost.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [11] [Semi-Supervised Mixture Models under the Concept of Missing at Radom with Margin Confidence and Aranda Ordaz Function](https://arxiv.org/abs/2601.14631)
*Jinyang Liao,Ziyang Lyu*

Main category: stat.ML

TL;DR: 提出了一种基于MAR机制的半监督高斯混合模型框架，通过建模缺失概率与分类不确定性的关系来减少偏差


<details>
  <summary>Details</summary>
Motivation: 在现实半监督学习场景中，标签缺失通常不是随机的（MAR机制），忽略缺失机制会导致估计偏差。需要开发能够显式建模缺失机制的半监督学习方法

Method: 1) 将缺失概率参数化为分类不确定性的函数；2) 引入边界置信度量化不确定性；3) 使用Aranda Ordaz链接函数灵活捕捉不确定性与缺失概率的非对称关系；4) 开发ECM算法联合估计GMM和缺失机制参数；5) 基于拟合的混合模型使用贝叶斯分类器填补缺失标签

Result: 该方法有效减轻了忽略缺失机制导致的偏差，增强了半监督学习的鲁棒性，在标签大量缺失的现实MAR场景中提供了可靠的分类性能

Conclusion: 提出的不确定性感知框架通过显式建模缺失机制，为MAR机制下的半监督高斯混合模型提供了有效的解决方案，能够处理现实世界中大量标签缺失的情况

Abstract: This paper presents a semi-supervised learning framework for Gaussian mixture modelling under a Missing at Random (MAR) mechanism. The method explicitly parameterizes the missingness mechanism by modelling the probability of missingness as a function of classification uncertainty. To quantify classification uncertainty, we introduce margin confidence and incorporate the Aranda Ordaz (AO) link function to flexibly capture the asymmetric relationships between uncertainty and missing probability. Based on this formulation, we develop an efficient Expectation Conditional Maximization (ECM) algorithm that jointly estimates all parameters appearing in both the Gaussian mixture model (GMM) and the missingness mechanism, and subsequently imputes the missing labels by a Bayesian classifier derived from the fitted mixture model. This method effectively alleviates the bias induced by ignoring the missingness mechanism while enhancing the robustness of semi-supervised learning. The resulting uncertainty-aware framework delivers reliable classification performance in realistic MAR scenarios with substantial proportions of missing labels.

</details>


### [12] [Meta Flow Maps enable scalable reward alignment](https://arxiv.org/abs/2601.14430)
*Peter Potaptchik,Adhi Saravanan,Abbas Mammadov,Alvaro Prat,Michael S. Albergo,Yee Whye Teh*

Main category: stat.ML

TL;DR: 提出Meta Flow Maps (MFMs)框架，通过随机一步后验采样实现高效生成模型控制，避免昂贵的轨迹模拟，在推理时引导和微调中显著提升效率


<details>
  <summary>Details</summary>
Motivation: 当前生成模型控制方法计算成本高昂，因为需要估计价值函数，这通常需要访问条件后验分布p_{1|t}(x_1|x_t)，迫使方法采用昂贵的轨迹模拟。需要解决这个计算瓶颈

Method: 引入Meta Flow Maps (MFMs)框架，将一致性模型和流映射扩展到随机机制。MFMs训练用于执行随机一步后验采样，从任何中间状态生成任意多个独立同分布的干净数据样本，提供可微分重参数化以实现高效价值函数估计

Result: 单粒子引导的MFM采样器在ImageNet上优于Best-of-1000基线，在多个奖励函数上以少量计算成本取得更好性能

Conclusion: MFMs通过随机一步后验采样解决了生成模型控制中的计算瓶颈，实现了无需内部滚动的推理时引导和针对通用奖励的无偏、离策略微调，显著提升了效率

Abstract: Controlling generative models is computationally expensive. This is because optimal alignment with a reward function--whether via inference-time steering or fine-tuning--requires estimating the value function. This task demands access to the conditional posterior $p_{1|t}(x_1|x_t)$, the distribution of clean data $x_1$ consistent with an intermediate state $x_t$, a requirement that typically compels methods to resort to costly trajectory simulations. To address this bottleneck, we introduce Meta Flow Maps (MFMs), a framework extending consistency models and flow maps into the stochastic regime. MFMs are trained to perform stochastic one-step posterior sampling, generating arbitrarily many i.i.d. draws of clean data $x_1$ from any intermediate state. Crucially, these samples provide a differentiable reparametrization that unlocks efficient value function estimation. We leverage this capability to solve bottlenecks in both paradigms: enabling inference-time steering without inner rollouts, and facilitating unbiased, off-policy fine-tuning to general rewards. Empirically, our single-particle steered-MFM sampler outperforms a Best-of-1000 baseline on ImageNet across multiple rewards at a fraction of the compute.

</details>


### [13] [Large Data Limits of Laplace Learning for Gaussian Measure Data in Infinite Dimensions](https://arxiv.org/abs/2601.14515)
*Zhengang Zhong,Yury Korolev,Matthew Thorpe*

Main category: stat.ML

TL;DR: 该论文分析了在高斯测度生成数据的无限维希尔伯特空间中，拉普拉斯学习的图狄利克雷能量的逐点收敛性。


<details>
  <summary>Details</summary>
Motivation: 拉普拉斯学习在半监督学习中用于从未标记数据中推断缺失标签，但现有理论主要针对有限维数据。当数据生成于无限维空间（如高斯测度生成的希尔伯特空间）时，缺乏勒贝格测度使得传统分析失效，需要重新思考收敛性分析。

Method: 研究高斯测度在希尔伯特空间中生成数据的情况，分析图狄利克雷能量在无限维空间中的渐近行为，证明从离散图设置到连续设置的逐点收敛性。

Result: 证明了在无限维希尔伯特空间中，当数据由高斯测度生成时，图狄利克雷能量能够逐点收敛到相应的连续泛函，这是无限维空间中拉普拉斯学习收敛性的首个理论结果。

Conclusion: 该研究为无限维空间中的半监督学习提供了理论基础，填补了拉普拉斯学习在无限维数据分析中的理论空白，为后续研究无限维机器学习算法奠定了基础。

Abstract: Laplace learning is a semi-supervised method, a solution for finding missing labels from a partially labeled dataset utilizing the geometry given by the unlabeled data points. The method minimizes a Dirichlet energy defined on a (discrete) graph constructed from the full dataset. In finite dimensions the asymptotics in the large (unlabeled) data limit are well understood with convergence from the graph setting to a continuum Sobolev semi-norm weighted by the Lebesgue density of the data-generating measure. The lack of the Lebesgue measure on infinite-dimensional spaces requires rethinking the analysis if the data aren't finite-dimensional. In this paper we make a first step in this direction by analyzing the setting when the data are generated by a Gaussian measure on a Hilbert space and proving pointwise convergence of the graph Dirichlet energy.

</details>


### [14] [Efficient and Minimax-optimal In-context Nonparametric Regression with Transformers](https://arxiv.org/abs/2601.15014)
*Michelle Ching,Ioana Popescu,Nico Smith,Tianyi Ma,William G. Underwood,Richard J. Samworth*

Main category: stat.ML

TL;DR: 本文研究了非参数回归中的上下文学习，证明了预训练的Transformer模型能够以较少的参数和训练序列达到最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在上下文学习中的能力，特别是在非参数回归任务中，探索如何用更少的模型参数和训练数据达到理论最优性能。

Method: 使用α-Hölder光滑回归函数，通过预训练的Transformer实现局部多项式估计器，具体实现核加权多项式基并运行梯度下降。

Result: 证明了仅需Θ(log n)个参数和Ω(n^{2α/(2α+d)}log^3 n)个预训练序列，就能达到最小最大最优收敛速率O(n^{-2α/(2α+d)})，显著优于现有结果。

Conclusion: Transformer能够高效近似局部多项式估计器，在上下文学习中用较少的资源达到理论最优性能，为理解Transformer的上下文学习能力提供了理论保证。

Abstract: We study in-context learning for nonparametric regression with $α$-Hölder smooth regression functions, for some $α>0$. We prove that, with $n$ in-context examples and $d$-dimensional regression covariates, a pretrained transformer with $Θ(\log n)$ parameters and $Ω\bigl(n^{2α/(2α+d)}\log^3 n\bigr)$ pretraining sequences can achieve the minimax-optimal rate of convergence $O\bigl(n^{-2α/(2α+d)}\bigr)$ in mean squared error. Our result requires substantially fewer transformer parameters and pretraining sequences than previous results in the literature. This is achieved by showing that transformers are able to approximate local polynomial estimators efficiently by implementing a kernel-weighted polynomial basis and then running gradient descent.

</details>


### [15] [Communication-Efficient Federated Risk Difference Estimation for Time-to-Event Clinical Outcomes](https://arxiv.org/abs/2601.14609)
*Ziwen Wang,Siqi Li,Marcus Eng Hock Ong,Nan Liu*

Main category: stat.ML

TL;DR: FedRD：一种通信高效的联邦风险差异估计框架，用于分布式生存数据分析，无需持续服务器连接，提供临床可解释的绝对风险评估。


<details>
  <summary>Details</summary>
Motivation: 医疗研究中隐私保护模型协同训练面临两大挑战：1）依赖服务器的架构与医院受保护数据系统不兼容；2）现有方法主要关注相对效应指标（风险比），缺乏临床可解释的绝对生存风险评估。

Method: 提出FedRD框架，采用服务器无关架构，通信成本极低：分层模型只需一轮汇总统计交换，非分层模型只需三轮。提供有效的置信区间和假设检验功能，并建立了理论保证，证明FedRD（非分层）在渐近意义上等价于集中式个体水平分析。

Result: 模拟研究和跨国真实世界临床应用表明，FedRD在估计准确性和预测性能上均优于本地和联邦基线方法，为隐私受限的多中心临床研究提供了架构可行的绝对风险评估解决方案。

Conclusion: FedRD为分布式生存数据分析提供了一种通信高效、服务器无关的解决方案，能够提供临床可解释的绝对风险差异估计，填补了现有联邦学习方法在置信区间和假设检验方面的空白。

Abstract: Privacy-preserving model co-training in medical research is often hindered by server-dependent architectures incompatible with protected hospital data systems and by the predominant focus on relative effect measures (hazard ratios) which lack clinical interpretability for absolute survival risk assessment. We propose FedRD, a communication-efficient framework for federated risk difference estimation in distributed survival data. Unlike typical federated learning frameworks (e.g., FedAvg) that require persistent server connections and extensive iterative communication, FedRD is server-independent with minimal communication: one round of summary statistics exchange for the stratified model and three rounds for the unstratified model. Crucially, FedRD provides valid confidence intervals and hypothesis testing--capabilities absent in FedAvg-based frameworks. We provide theoretical guarantees by establishing the asymptotic properties of FedRD and prove that FedRD (unstratified) is asymptotically equivalent to pooled individual-level analysis. Simulation studies and real-world clinical applications across different countries demonstrate that FedRD outperforms local and federated baselines in both estimation accuracy and prediction performance, providing an architecturally feasible solution for absolute risk assessment in privacy-restricted, multi-site clinical studies.

</details>


### [16] [Multi-context principal component analysis](https://arxiv.org/abs/2601.15239)
*Kexin Wang,Salil Bhate,João M. Pereira,Joe Kileel,Matylda Figlerowicz,Anna Seigal*

Main category: stat.ML

TL;DR: MCPCA是多上下文主成分分析框架，能识别跨数据子集的共享变异因素，在基因表达和词嵌入分析中发现了传统PCA无法检测的模式。


<details>
  <summary>Details</summary>
Motivation: 当前数据常在不同上下文（如不同疾病个体、不同细胞类型、不同文本）中收集，虽然变异因素可能跨上下文共享，但现有工具无法系统性地恢复这些共享因素。

Method: 开发了多上下文主成分分析（MCPCA）的理论和算法框架，将数据分解为跨上下文子集共享的因素。

Result: 在基因表达分析中，MCPCA发现了跨癌症类型子集共享的变异轴，以及一个与肺癌进展相关的肿瘤细胞变异轴；在词嵌入分析中，MCPCA映射了关于人性辩论的阶段，揭示了科学与小说之间数十年的讨论。

Conclusion: MCPCA是PCA的原则性泛化，能解决跨上下文数据理解的根本挑战，这些轴无法通过跨上下文组合数据或限制于单个上下文来发现。

Abstract: Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematically recover such factors. We develop multi-context principal component analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors shared across subsets of contexts. Applied to gene expression, MCPCA reveals axes of variation shared across subsets of cancer types and an axis whose variability in tumor cells, but not mean, is associated with lung cancer progression. Applied to contextualized word embeddings from language models, MCPCA maps stages of a debate on human nature, revealing a discussion between science and fiction over decades. These axes are not found by combining data across contexts or by restricting to individual contexts. MCPCA is a principled generalization of PCA to address the challenge of understanding factors underlying data across contexts.

</details>


### [17] [Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?](https://arxiv.org/abs/2601.15254)
*Felix Schur,Niklas Pfister,Peng Ding,Sach Mukherjee,Jonas Peters*

Main category: stat.ML

TL;DR: 提出一种针对未配对数据（X和Y在不同环境下分别观测）的因果效应估计方法，使用环境作为工具变量，通过交叉折叠样本分割的GMM估计器解决每环境样本量少的问题。


<details>
  <summary>Details</summary>
Motivation: 在隐藏混淆变量的情况下，当X和Y在不同实验条件下分别观测（未配对数据），且每个环境只有少量观测时，传统的两样本工具变量估计器会失效，需要新的估计方法。

Method: 将环境作为（可能高维的）工具变量，提出基于交叉折叠样本分割的GMM型估计器，并扩展到稀疏因果效应情况，使用ℓ1正则化估计和后选择重新拟合。

Result: 证明了当环境数量增长但每个环境的样本量保持不变时，该估计器是一致的，能够有效处理每环境样本量少的未配对数据问题。

Conclusion: 该方法为隐藏混淆变量下的未配对数据因果推断提供了有效的解决方案，特别适用于环境多但每环境观测少的情况，扩展了工具变量方法的应用范围。

Abstract: We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\ell_1$-regularized estimation and post-selection refitting.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [18] [Implementing Substance Over Form: A Novel Metric for Taxing E-commerce to Address Deterritorialization](https://arxiv.org/abs/2601.14616)
*Li Tuobang*

Main category: stat.AP

TL;DR: 论文提出基于"实质重于形式"原则，以"配送费+保费"为税基，通过"货值转换"修正的税率计算方法，使电商终端税负与社区零售实质匹配，解决电商去地域化问题。


<details>
  <summary>Details</summary>
Motivation: 电商重构消费模式，末端配送站承担社区零售功能，但现行税制仅对配送费征收低税率劳务税，导致大量流通商品价值的税收贡献远低于同等规模零售超市，造成地方税基侵蚀和不公平竞争。

Method: 基于"实质重于形式"原则，提出以"配送费加保险费"为税基，通过"商品价值转换"进行修正的税率计算方法，使电商末端税负与社区零售实质匹配。

Result: 该方法能够使电商终端阶段的实质税负与社区零售对齐，通过财政工具有效内化配送站的高负外部性，解决电商去地域化问题。

Conclusion: 提出的税基修正方法能够解决电商末端配送站与社区零售之间的税收不公平问题，防止地方税基侵蚀，促进公平竞争，应对电商去地域化挑战。

Abstract: Against the backdrop of e-commerce restructuring consumption patterns, last-mile delivery stations have substantially fulfilled the function of community retail distribution. However, the current tax system only levies a low labor service tax on delivery fees, resulting in a tax contribution from the massive circulating goods value that is significantly lower than that of retail supermarkets of equivalent scale. This disparity not only triggers local tax base erosion but also fosters unfair competition. Based on the "substance over form" principle, this paper proposes a tax rate calculation method using "delivery fee plus insurance premium" as the base, corrected through "goods value conversion." This method aims to align the substantive tax burden of e-commerce with that of community retail at the terminal stage, effectively internalizing the high negative externalities of delivery stations through fiscal instruments, addressing E-commerce Deterritorialization.

</details>


### [19] [Regulatory Expectations for Bayesian Methods in Drug and Biologic Clinical Trials: A Practical Perspective on FDA's 2026 Draft Guidance](https://arxiv.org/abs/2601.14701)
*Yuan Ji,Ph. D*

Main category: stat.AP

TL;DR: FDA发布贝叶斯方法用于临床试验的指导草案，强调贝叶斯设计需通过明确成功标准、合理先验、前瞻性操作特性评估和计算透明度来论证，而非简单允许使用。


<details>
  <summary>Details</summary>
Motivation: FDA发布了关于在药物和生物制品临床试验中使用贝叶斯方法支持主要推断的指导草案，需要为申办方提供实用的监管导向解读，明确贝叶斯设计在监管审查中的具体要求。

Method: 提供对FDA指导草案的实践性、监管导向的综合解读，强调贝叶斯设计如何与传统频率主义错误率目标校准，以及在申办方-FDA同意下使用替代贝叶斯操作指标的适用情况。

Result: 通过指导草案中讨论的示例（如平台试验、外部/非并发对照、儿科外推）说明监管期望，并为规划文件和提交包提供可操作的检查清单。

Conclusion: 贝叶斯设计在临床试验中的应用需要系统论证和透明计算，FDA指导草案为申办方提供了明确的监管框架，通过适当校准和协议，贝叶斯方法可以与传统频率主义方法协调使用。

Abstract: The U.S. Food and Drug Administration (FDA) released a landmark draft guidance in January 2026 on the use of Bayesian methodology to support primary inference in clinical trials of drugs and biological products. For sponsors, the central message is not merely that ``Bayes is allowed,'' but that Bayesian designs should be justified through explicit success criteria, thoughtful priors (especially when borrowing external information), prospective operating-characteristic evaluation (often via simulation when simulation is used), and computational transparency suitable for regulatory review. This paper provides a practical, regulatory-oriented synthesis of the draft guidance, highlighting where Bayesian designs can be calibrated to traditional frequentist error-rate targets and where, with sponsor--FDA agreement, alternative Bayesian operating metrics may be appropriate. We illustrate expectations through examples discussed in the guidance (e.g., platform trials, external/nonconcurrent controls, pediatric extrapolation) and conclude with an actionable checklist for planning documents and submission packages.

</details>


### [20] [A Practical Guide to Modern Imputation](https://arxiv.org/abs/2601.14796)
*Jeffrey Näf*

Main category: stat.AP

TL;DR: 这是一篇关于缺失值插补常见陷阱的指南性论文，旨在帮助研究人员避免在缺失值处理中犯常见错误。


<details>
  <summary>Details</summary>
Motivation: 缺失值插补是数据分析中的关键步骤，但许多研究人员在处理缺失值时容易陷入常见陷阱，导致分析结果出现偏差或错误。本文旨在提供基于最新研究的实用指南。

Method: 基于近期相关研究文献，系统总结和整理了缺失值插补中的常见问题和错误做法，提供避免这些陷阱的具体建议和最佳实践。

Result: 提供了全面的缺失值插补指南，涵盖了从数据理解、插补方法选择到结果验证的完整流程，帮助研究人员做出更明智的决策。

Conclusion: 通过遵循基于最新研究的指南，研究人员可以显著提高缺失值处理的准确性和可靠性，从而获得更有效的分析结果。

Abstract: This guide based on recent papers should help researchers avoid some of the most common pitfalls of missing value imputation imputation.

</details>


### [21] [Zero-inflated binary Tree Pólya splitting regression for multivariate count data](https://arxiv.org/abs/2601.14815)
*Fabrice Moudjieu,Jean Peyhardi,Maxime Réjou-Méchain,Patrice Soh Takam,Frédéric Mortier*

Main category: stat.AP

TL;DR: 本文提出了零膨胀树Pólya分裂（Z-TPS）分布，用于解决多变量物种分布模型中零膨胀、计算复杂性和可解释性问题，并在刚果盆地热带雨林数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统物种分布模型忽略物种间依赖关系，而现有多变量模型（如MPLN）存在计算复杂、维度灾难和可解释性差的问题。Pólya分裂分布虽然具有生态过程解释性，但缺乏建模相关结构的灵活性。树Pólya分裂通过引入层次结构（如系统发育树）解决了这一问题，但仍需处理零膨胀问题。

Method: 扩展树Pólya分裂（TPS）分布，引入零膨胀机制，形成零膨胀树Pólya分裂（Z-TPS）分布族。详细阐述了其统计特性，展示了如何使用标准软件实现高效推断，并利用刚果盆地热带雨林中180多个属的树木丰度数据进行验证。

Result: Z-TPS分布能够有效处理多变量物种分布数据中的零膨胀问题，同时保持计算效率、可解释性和对相关结构的建模能力。在刚果盆地热带雨林数据上的应用证明了该方法的生态相关性。

Conclusion: Z-TPS分布为多变量物种分布建模提供了一个灵活、可解释且计算高效的新框架，特别适合处理具有零膨胀特征的生态数据，能够更好地捕捉物种间的依赖关系和生态过程。

Abstract: Species distribution models (SDMs) are widely used to assess the effects of environmental factors on species distributions. However, classical SDMs ignore inter-species dependencies. Multivariate SDMs (MSDMs), especially those based on latent Gaussian fields such as the multivariate Poisson log-normal (MPLN), address this limitation but face challenges related to computation, dimensionality, and interpretability. Pólya-splitting (PS) distributions offer an alternative, combining a model for total abundance with a multivariate allocation structure, and have natural interpretations from ecological process models. Yet, they lack flexibility in modeling correlation structures. Tree Pólya-splitting (TPS) distributions overcome this by introducing hierarchical structure such as a phylogenetic tree. In this paper, we extend TPS to account for zero-inflation, leading to the zero-inflated tree Pólya-splitting (Z-TPS) family. We detail its statistical properties, show how standard software enables efficient inference, and illustrate its ecological relevance using tree abundance data from over 180 genera across the Congo Basin tropical rainforest.

</details>
