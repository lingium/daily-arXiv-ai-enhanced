<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 11]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 6]
- [stat.ME](#stat.ME) [Total: 12]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Non-Asymptotic Analysis of Data Augmentation for Precision Matrix Estimation](https://arxiv.org/abs/2510.02119)
*Lucas Morisset,Adrien Hardy,Alain Durmus*

Main category: stat.ML

TL;DR: 本文研究了高维设置下的逆协方差矩阵估计问题，重点关注线性收缩估计器和数据增强估计器，并提供了它们的二次误差集中界。


<details>
  <summary>Details</summary>
Motivation: 在高维统计中，逆协方差矩阵估计是一个重要问题，但传统方法在样本量小于维度时表现不佳。数据增强作为一种常用技术，但其理论性质尚未得到充分研究。

Method: 使用随机矩阵理论工具，引入广义解析矩阵的新确定性等价，分析线性收缩估计器和数据增强估计器的性能。

Result: 推导了两种估计器的集中界，允许进行方法比较和超参数调优，并通过数值实验验证了理论结果。

Conclusion: 提出的理论框架为高维逆协方差估计提供了新的分析工具，特别适用于数据增强场景下的性能评估和参数选择。

Abstract: This paper addresses the problem of inverse covariance (also known as
precision matrix) estimation in high-dimensional settings. Specifically, we
focus on two classes of estimators: linear shrinkage estimators with a target
proportional to the identity matrix, and estimators derived from data
augmentation (DA). Here, DA refers to the common practice of enriching a
dataset with artificial samples--typically generated via a generative model or
through random transformations of the original data--prior to model fitting.
For both classes of estimators, we derive estimators and provide concentration
bounds for their quadratic error. This allows for both method comparison and
hyperparameter tuning, such as selecting the optimal proportion of artificial
samples. On the technical side, our analysis relies on tools from random matrix
theory. We introduce a novel deterministic equivalent for generalized resolvent
matrices, accommodating dependent samples with specific structure. We support
our theoretical results with numerical experiments.

</details>


### [2] [Private Realizable-to-Agnostic Transformation with Near-Optimal Sample Complexity](https://arxiv.org/abs/2510.01291)
*Bo Li,Wei Wang,Peng Ye*

Main category: stat.ML

TL;DR: 本文改进了从可实现到不可知设置的私有学习转换，消除了对隐私参数ε的依赖，实现了近乎最优的额外样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的可实现到不可知转换在任意ε下需要应用隐私放大技术，导致样本复杂度中出现1/ε因子，这是次优的。

Method: 提出了一种改进的构造方法，消除了对隐私参数ε的依赖，通过新技术实现更高效的转换。

Result: 实现了近乎最优的额外样本复杂度O~(VC(C)/α²)，适用于任意ε≤1，解决了Dwork等人的开放问题。

Conclusion: 在私有不可知学习中，隐私成本主要影响可实现部分，改进的转换方法达到了近乎最优的样本复杂度。

Abstract: The realizable-to-agnostic transformation (Beimel et al., 2015; Alon et al.,
2020) provides a general mechanism to convert a private learner in the
realizable setting (where the examples are labeled by some function in the
concept class) to a private learner in the agnostic setting (where no
assumptions are imposed on the data). Specifically, for any concept class
$\mathcal{C}$ and error parameter $\alpha$, a private realizable learner for
$\mathcal{C}$ can be transformed into a private agnostic learner while only
increasing the sample complexity by
$\widetilde{O}(\mathrm{VC}(\mathcal{C})/\alpha^2)$, which is essentially tight
assuming a constant privacy parameter $\varepsilon = \Theta(1)$. However, when
$\varepsilon$ can be arbitrary, one has to apply the standard
privacy-amplification-by-subsampling technique (Kasiviswanathan et al., 2011),
resulting in a suboptimal extra sample complexity of
$\widetilde{O}(\mathrm{VC}(\mathcal{C})/\alpha^2\varepsilon)$ that involves a
$1/\varepsilon$ factor.
  In this work, we give an improved construction that eliminates the dependence
on $\varepsilon$, thereby achieving a near-optimal extra sample complexity of
$\widetilde{O}(\mathrm{VC}(\mathcal{C})/\alpha^2)$ for any $\varepsilon\le 1$.
Moreover, our result reveals that in private agnostic learning, the privacy
cost is only significant for the realizable part. We also leverage our
technique to obtain a nearly tight sample complexity bound for the private
prediction problem, resolving an open question posed by Dwork and Feldman
(2018) and Dagan and Feldman (2020).

</details>


### [3] [Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling](https://arxiv.org/abs/2510.01329)
*Huangjie Zheng,Shansan Gong,Ruixiang Zhang,Tianrong Chen,Jiatao Gu,Mingyuan Zhou,Navdeep Jaitly,Yizhe Zhang*

Main category: stat.ML

TL;DR: 提出连续增强离散扩散(CADD)框架，通过在连续潜在空间中配对扩散来增强离散状态空间，解决传统离散扩散模型中信息丢失问题，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统离散扩散模型将所有未观测状态统一映射为吸收[MASK]标记，造成'信息空洞'问题，导致语义信息在去噪步骤间丢失。

Method: 在离散状态空间中引入连续潜在空间中的配对扩散，将掩码标记表示为有噪声但信息丰富的潜在向量而非信息空洞，利用连续潜在作为语义提示指导离散去噪。

Result: 在文本生成、图像合成和代码建模任务中，CADD在定性和定量指标上均优于基于掩码的扩散方法，相比强离散基线获得一致提升。

Conclusion: CADD框架通过连续潜在空间增强有效解决了离散扩散中的信息丢失问题，实现了模式覆盖和模式寻求行为之间的可控权衡，显著提升了生成质量。

Abstract: Standard discrete diffusion models treat all unobserved states identically by
mapping them to an absorbing [MASK] token. This creates an 'information void'
where semantic information that could be inferred from unmasked tokens is lost
between denoising steps. We introduce Continuously Augmented Discrete Diffusion
(CADD), a framework that augments the discrete state space with a paired
diffusion in a continuous latent space. This yields graded, gradually corrupted
states in which masked tokens are represented by noisy yet informative latent
vectors rather than collapsed 'information voids'. At each reverse step, CADD
may leverage the continuous latent as a semantic hint to guide discrete
denoising. The design is clean and compatible with existing discrete diffusion
training. At sampling time, the strength and choice of estimator for the
continuous latent vector enables a controlled trade-off between mode-coverage
(generating diverse outputs) and mode-seeking (generating contextually precise
outputs) behaviors. Empirically, we demonstrate CADD improves generative
quality over mask-based diffusion across text generation, image synthesis, and
code modeling, with consistent gains on both qualitative and quantitative
metrics against strong discrete baselines.

</details>


### [4] [Risk Phase Transitions in Spiked Regression: Alignment Driven Benign and Catastrophic Overfitting](https://arxiv.org/abs/2510.01414)
*Jiping Li,Rishi Sonthalia*

Main category: stat.ML

TL;DR: 本文分析了线性回归中最小范数插值解在尖峰协方差数据模型下的泛化误差，揭示了尖峰强度、目标-尖峰对齐如何影响风险，特别是在过参数化设置中。


<details>
  <summary>Details</summary>
Motivation: 研究过参数化线性回归中最小范数插值解的泛化性能，特别关注尖峰协方差结构和目标对齐对风险的影响。

Method: 使用尖峰协方差数据模型，推导出泛化误差的精确表达式，分析尖峰强度、维度-样本比c=d/n（特别是c→∞时）和目标对齐的影响。

Result: 建立了良性、温和和灾难性过拟合的完整分类体系。发现在对齐良好的问题中，增加尖峰强度可能先导致灾难性过拟合，然后才达到良性过拟合。目标-尖峰对齐并不总是有益的，在某些情况下反而有害。

Conclusion: 目标与尖峰的对齐在过参数化线性回归中具有复杂影响，可能在某些条件下产生反直觉的有害效果，这种现象在非线性模型中也持续存在。

Abstract: This paper analyzes the generalization error of minimum-norm interpolating
solutions in linear regression using spiked covariance data models. The paper
characterizes how varying spike strengths and target-spike alignments can
affect risk, especially in overparameterized settings. The study presents an
exact expression for the generalization error, leading to a comprehensive
classification of benign, tempered, and catastrophic overfitting regimes based
on spike strength, the aspect ratio $c=d/n$ (particularly as $c \to \infty$),
and target alignment. Notably, in well-specified aligned problems, increasing
spike strength can surprisingly induce catastrophic overfitting before
achieving benign overfitting. The paper also reveals that target-spike
alignment is not always advantageous, identifying specific, sometimes
counterintuitive, conditions for its benefit or detriment. Alignment with the
spike being detrimental is empirically demonstrated to persist in nonlinear
models.

</details>


### [5] [AI Foundation Model for Time Series with Innovations Representation](https://arxiv.org/abs/2510.01560)
*Lang Tong,Xinyi Wang*

Main category: stat.ML

TL;DR: 提出了TS-GPT，一个基于创新表示理论的时间序列基础模型，专门用于工程监控和控制应用，并在电力市场价格预测中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于工程时间序列受物理规律而非语言规律支配，基于大语言模型的AI基础模型可能无效或低效，需要专门针对工程应用的时间序列基础模型。

Method: 基于Wiener、Kallianpur和Rosenblatt的创新表示理论，提出了TS-GPT——一个基于创新表示的生成预训练变换器，用于工程监控和控制。

Result: 在概率生成预测任务中，TS-GPT能够根据历史实现生成未来时间序列样本，并在美国独立系统运营商的实时位置边际价格预测中表现出有效性。

Conclusion: TS-GPT作为专门针对工程时间序列的基础模型，在需要因果操作的实时监控和控制应用中具有实用价值，特别是在电力市场价格预测方面。

Abstract: This paper introduces an Artificial Intelligence (AI) foundation model for
time series in engineering applications, where causal operations are required
for real-time monitoring and control. Since engineering time series are
governed by physical, rather than linguistic, laws, large-language-model-based
AI foundation models may be ineffective or inefficient. Building on the
classical innovations representation theory of Wiener, Kallianpur, and
Rosenblatt, we propose Time Series GPT (TS-GPT) -- an
innovations-representation-based Generative Pre-trained Transformer for
engineering monitoring and control. As an example of foundation model
adaptation, we consider Probabilistic Generative Forecasting, which produces
future time series samples from conditional probability distributions given
past realizations. We demonstrate the effectiveness of TS-GPT in forecasting
real-time locational marginal prices using historical data from U.S.
independent system operators.

</details>


### [6] [A reproducible comparative study of categorical kernels for Gaussian process regression, with new clustering-based nested kernels](https://arxiv.org/abs/2510.01840)
*Raphaël Carpintero Perez,Sébastien Da Veiga,Josselin Garnier*

Main category: stat.ML

TL;DR: 本文对现有分类核方法进行了可复现的比较研究，提出新的评估指标，发现在具有分组结构的分类输入中嵌套核方法表现最佳，并提出基于聚类的目标编码策略来应对未知分组结构的情况。


<details>
  <summary>Details</summary>
Motivation: 设计分类核是高斯过程回归中处理连续和分类输入的挑战，现有研究难以确定首选方法，因为评估指标、优化过程或数据集在不同研究中各不相同，且缺乏可复现代码。

Method: 对所有现有分类核方法进行可复现比较研究，提出受优化社区启发的新评估指标，在具有分组结构的分类输入数据集上测试，并提出基于聚类的目标编码策略处理未知分组结构。

Result: 在具有分组结构的分类输入数据集中，嵌套核方法明显优于所有竞争对手；对于未知分组结构的情况，基于聚类的目标编码策略在大量数据集上仍优于其他方法，同时保持低计算成本。

Conclusion: 嵌套核方法在已知分组结构时表现最佳，而基于聚类的目标编码策略是处理未知分组结构情况的有效方法，在广泛数据集上保持优越性能且计算成本低。

Abstract: Designing categorical kernels is a major challenge for Gaussian process
regression with continuous and categorical inputs. Despite previous studies, it
is difficult to identify a preferred method, either because the evaluation
metrics, the optimization procedure, or the datasets change depending on the
study. In particular, reproducible code is rarely available. The aim of this
paper is to provide a reproducible comparative study of all existing
categorical kernels on many of the test cases investigated so far. We also
propose new evaluation metrics inspired by the optimization community, which
provide quantitative rankings of the methods across several tasks. From our
results on datasets which exhibit a group structure on the levels of
categorical inputs, it appears that nested kernels methods clearly outperform
all competitors. When the group structure is unknown or when there is no prior
knowledge of such a structure, we propose a new clustering-based strategy using
target encodings of categorical variables. We show that on a large panel of
datasets, which do not necessarily have a known group structure, this
estimation strategy still outperforms other approaches while maintaining low
computational cost.

</details>


### [7] [Deep Hedging Under Non-Convexity: Limitations and a Case for AlphaZero](https://arxiv.org/abs/2510.01874)
*Matteo Maggiolo,Giuseppe Nuti,Miroslav Štrupl,Oleg Szehr*

Main category: stat.ML

TL;DR: 本文研究了不完全市场中的复制投资组合构建问题，提出了一种基于AlphaZero的方法，并与业界广泛使用的深度对冲方法进行比较，发现在非凸环境下AlphaZero表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决不完全市场中复制投资组合构建的关键问题，该问题在定价、对冲、资产负债表管理和能源存储规划中具有重要应用价值。

Method: 将问题建模为投资者与市场之间的双人博弈，引入基于AlphaZero的系统，并与基于梯度下降的深度对冲方法进行对比。

Result: 深度对冲在Q函数不受凸性约束的环境中（如涉及非凸交易成本、资本约束或监管限制）会陷入局部最优，而AlphaZero始终能找到接近最优的复制策略，且样本效率更高。

Conclusion: AlphaZero在不完全市场的复制投资组合构建中优于深度对冲，特别是在非凸环境下，同时建立了深度对冲与凸优化之间的联系，表明其有效性依赖于凸性假设。

Abstract: This paper examines replication portfolio construction in incomplete markets
- a key problem in financial engineering with applications in pricing, hedging,
balance sheet management, and energy storage planning. We model this as a
two-player game between an investor and the market, where the investor makes
strategic bets on future states while the market reveals outcomes. Inspired by
the success of Monte Carlo Tree Search in stochastic games, we introduce an
AlphaZero-based system and compare its performance to deep hedging - a widely
used industry method based on gradient descent. Through theoretical analysis
and experiments, we show that deep hedging struggles in environments where the
$Q$-function is not subject to convexity constraints - such as those involving
non-convex transaction costs, capital constraints, or regulatory limitations -
converging to local optima. We construct specific market environments to
highlight these limitations and demonstrate that AlphaZero consistently finds
near-optimal replication strategies. On the theoretical side, we establish a
connection between deep hedging and convex optimization, suggesting that its
effectiveness is contingent on convexity assumptions. Our experiments further
suggest that AlphaZero is more sample-efficient - an important advantage in
data-scarce, overfitting-prone derivative markets.

</details>


### [8] [Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by Dynamical Mean-Field Theory](https://arxiv.org/abs/2510.01930)
*Sota Nishiyama,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 使用动力学平均场理论(DMFT)分析对角线性网络(DLNs)的梯度流动力学，推导出高维渐近动力学的低维有效过程，统一解释多种现象如初始化依赖解和增量学习。


<details>
  <summary>Details</summary>
Motivation: 对角线性网络虽然可解析处理，但现有研究通常孤立分析各种现象，缺乏对整体动力学的统一理解。

Method: 应用动力学平均场理论推导高维梯度流动力学的低维有效过程，分析该有效过程来研究DLN动力学。

Result: 获得了关于损失收敛速率及其与泛化权衡的新见解，系统重现了先前观察到的多种现象。

Conclusion: 加深了对DLNs的理解，证明了DMFT方法在分析神经网络高维学习动力学中的有效性。

Abstract: Diagonal linear networks (DLNs) are a tractable model that captures several
nontrivial behaviors in neural network training, such as
initialization-dependent solutions and incremental learning. These phenomena
are typically studied in isolation, leaving the overall dynamics insufficiently
understood. In this work, we present a unified analysis of various phenomena in
the gradient flow dynamics of DLNs. Using Dynamical Mean-Field Theory (DMFT),
we derive a low-dimensional effective process that captures the asymptotic
gradient flow dynamics in high dimensions. Analyzing this effective process
yields new insights into DLN dynamics, including loss convergence rates and
their trade-off with generalization, and systematically reproduces many of the
previously observed phenomena. These findings deepen our understanding of DLNs
and demonstrate the effectiveness of the DMFT approach in analyzing
high-dimensional learning dynamics of neural networks.

</details>


### [9] [Uniform-in-time convergence bounds for Persistent Contrastive Divergence Algorithms](https://arxiv.org/abs/2510.01944)
*Paul Felix Valsecchi Oliva,O. Deniz Akyildiz,Andrew Duncan*

Main category: stat.ML

TL;DR: 提出了一种基于连续时间公式的持久对比散度方法，用于未归一化密度的最大似然估计，通过耦合的多尺度随机微分方程系统同时进行参数优化和采样。


<details>
  <summary>Details</summary>
Motivation: 为能量基模型训练提供具有显式误差保证的新方法，解决传统PCD方法缺乏理论误差界限的问题。

Method: 将PCD表达为耦合的多尺度SDE系统，使用随机正交Runge-Kutta Chebyshev（S-ROCK）积分器进行高效实现。

Result: 推导了PCD迭代与MLE解之间的误差界限，获得了多尺度系统与平均状态之间矩差的一致时间界限。

Conclusion: 该方法为训练能量基模型提供了一种具有显式误差保证的新颖方法，在长时间范围内具有明确的误差估计。

Abstract: We propose a continuous-time formulation of persistent contrastive divergence
(PCD) for maximum likelihood estimation (MLE) of unnormalised densities. Our
approach expresses PCD as a coupled, multiscale system of stochastic
differential equations (SDEs), which perform optimisation of the parameter and
sampling of the associated parametrised density, simultaneously.
  From this novel formulation, we are able to derive explicit bounds for the
error between the PCD iterates and the MLE solution for the model parameter.
This is made possible by deriving uniform-in-time (UiT) bounds for the
difference in moments between the multiscale system and the averaged regime. An
efficient implementation of the continuous-time scheme is introduced,
leveraging a class of explicit, stable intregators, stochastic orthogonal
Runge-Kutta Chebyshev (S-ROCK), for which we provide explicit error estimates
in the long-time regime. This leads to a novel method for training energy-based
models (EBMs) with explicit error guarantees.

</details>


### [10] [Adaptive Kernel Selection for Stein Variational Gradient Descent](https://arxiv.org/abs/2510.02067)
*Moritz Melcher,Simon Weissmann,Ashia C. Wilson,Jakob Zech*

Main category: stat.ML

TL;DR: 提出自适应SVGD方法，通过交替更新粒子和自适应调整核带宽来改进Stein变分梯度下降的性能


<details>
  <summary>Details</summary>
Motivation: SVGD方法对核函数选择高度敏感，常用的中位数启发式方法在高维设置中表现不佳，需要更灵活的自适应核参数选择策略

Method: Ad-SVGD方法交替进行粒子更新和核带宽自适应调整，通过梯度上升优化核化Stein差异来调整核参数

Result: Ad-SVGD在各种任务中一致优于标准启发式方法，经验结果验证了方法的有效性

Conclusion: 自适应核参数选择策略能够显著提升SVGD的性能，特别是在高维贝叶斯推理问题中

Abstract: A central challenge in Bayesian inference is efficiently approximating
posterior distributions. Stein Variational Gradient Descent (SVGD) is a popular
variational inference method which transports a set of particles to approximate
a target distribution. The SVGD dynamics are governed by a reproducing kernel
Hilbert space (RKHS) and are highly sensitive to the choice of the kernel
function, which directly influences both convergence and approximation quality.
The commonly used median heuristic offers a simple approach for setting kernel
bandwidths but lacks flexibility and often performs poorly, particularly in
high-dimensional settings. In this work, we propose an alternative strategy for
adaptively choosing kernel parameters over an abstract family of kernels.
Recent convergence analyses based on the kernelized Stein discrepancy (KSD)
suggest that optimizing the kernel parameters by maximizing the KSD can improve
performance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a
method that alternates between updating the particles via SVGD and adaptively
tuning kernel bandwidths through gradient ascent on the KSD. We provide a
simplified theoretical analysis that extends existing results on minimizing the
KSD for fixed kernels to our adaptive setting, showing convergence properties
for the maximal KSD over our kernel class. Our empirical results further
support this intuition: Ad-SVGD consistently outperforms standard heuristics in
a variety of tasks.

</details>


### [11] [Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure Risk at Record 2.9-Million Observation Scale](https://arxiv.org/abs/2510.02189)
*Boris Kriuk*

Main category: stat.ML

TL;DR: 提出了一个混合物理-机器学习框架，用于评估北极永久冻土融化对基础设施的风险，结合290万个观测数据和物理模型，在RCP8.5情景下预测永久冻土平均减少20.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 北极变暖威胁着北方地区超过1000亿美元的基础设施，但现有风险评估框架缺乏时空验证、不确定性量化和运营决策支持能力。

Method: 使用混合物理-机器学习框架，整合290万个观测数据，采用堆叠集成模型（随机森林+直方图梯度提升+弹性网络），结合学习的气候-永久冻土关系（60%）和物理敏感性模型（40%）。

Result: 模型达到R²=0.980（RMSE=5.01 pp），在RCP8.5情景下预测永久冻土平均减少20.3个百分点，识别出15%的高风险区和25%的中风险区。

Conclusion: 该框架是全球最大的验证永久冻土机器学习数据集，提供了首个用于北极基础设施的混合物理-ML预测系统，方法可推广到其他永久冻土区域。

Abstract: Arctic warming threatens over 100 billion in permafrost-dependent
infrastructure across Northern territories, yet existing risk assessment
frameworks lack spatiotemporal validation, uncertainty quantification, and
operational decision-support capabilities. We present a hybrid physics-machine
learning framework integrating 2.9 million observations from 171,605 locations
(2005-2021) combining permafrost fraction data with climate reanalysis. Our
stacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic
Net) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal
cross-validation preventing data leakage. To address machine learning
limitations in extrapolative climate scenarios, we develop a hybrid approach
combining learned climate-permafrost relationships (60%) with physical
permafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over
10 years), we project mean permafrost fraction decline of -20.3 pp (median:
-20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point
loss. Infrastructure risk classification identifies 15% high-risk zones (25%
medium-risk) with spatially explicit uncertainty maps. Our framework represents
the largest validated permafrost ML dataset globally, provides the first
operational hybrid physics-ML forecasting system for Arctic infrastructure, and
delivers open-source tools enabling probabilistic permafrost projections for
engineering design codes and climate adaptation planning. The methodology is
generalizable to other permafrost regions and demonstrates how hybrid
approaches can overcome pure data-driven limitations in climate change
applications.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [12] [Efficient Probabilistic Visualization of Local Divergence of 2D Vector Fields with Independent Gaussian Uncertainty](https://arxiv.org/abs/2510.01190)
*Timbwaoga A. J. Ouermi,Eric Li,Kenneth Moreland,Dave Pugmire,Chris R. Johnson,Tushar M. Athawale*

Main category: stat.CO

TL;DR: 提出一种高效可视化二维向量场局部发散不确定性的方法，通过封闭形式解替代蒙特卡洛采样，显著提升计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 向量场数据中的不确定性会导致发散计算错误，影响下游分析。传统蒙特卡洛方法收敛慢且可扩展性差，需要更高效的解决方案。

Method: 1) 推导封闭形式方法，假设向量不确定性独立高斯分布；2) 集成到Viskores并行库中加速不确定性可视化。

Result: 串行分析算法速度提升1946倍，并行Viskores算法速度提升19698倍，在风预报和海洋模拟数据集上验证了准确性和效率。

Conclusion: 该方法显著提高了发散不确定性可视化的效率和准确性，相比传统方法有质的改进。

Abstract: This work focuses on visualizing uncertainty of local divergence of
two-dimensional vector fields. Divergence is one of the fundamental attributes
of fluid flows, as it can help domain scientists analyze potential positions of
sources (positive divergence) and sinks (negative divergence) in the flow.
However, uncertainty inherent in vector field data can lead to erroneous
divergence computations, adversely impacting downstream analysis. While Monte
Carlo (MC) sampling is a classical approach for estimating divergence
uncertainty, it suffers from slow convergence and poor scalability with
increasing data size and sample counts. Thus, we present a two-fold
contribution that tackles the challenges of slow convergence and limited
scalability of the MC approach. (1) We derive a closed-form approach for highly
efficient and accurate uncertainty visualization of local divergence, assuming
independently Gaussian-distributed vector uncertainties. (2) We further
integrate our approach into Viskores, a platform-portable parallel library, to
accelerate uncertainty visualization. In our results, we demonstrate
significantly enhanced efficiency and accuracy of our serial analytical
(speed-up up to 1946X) and parallel Viskores (speed-up up to 19698X) algorithms
over the classical serial MC approach. We also demonstrate qualitative
improvements of our probabilistic divergence visualizations over traditional
mean-field visualization, which disregards uncertainty. We validate the
accuracy and efficiency of our methods on wind forecast and ocean simulation
datasets.

</details>


### [13] [Knots and variance ordering of sequential Monte Carlo algorithms](https://arxiv.org/abs/2510.01901)
*Joshua J Bon,Anthony Lee*

Main category: stat.CO

TL;DR: 提出了一种新的方差减少技术——knot算子，通过将势函数信息整合到转移核中来提高粒子滤波器的效率，并建立了Feynman-Kac模型的偏序关系来指导算法设计。


<details>
  <summary>Details</summary>
Motivation: 粒子滤波器在贝叶斯推断和状态空间模型中广泛用于近似难解积分，但需要提高其效率。现有方法如模型边缘化等策略可以设计高效粒子滤波器，但需要更一般的理论框架来统一和扩展这些技术。

Method: 引入knot算子作为方差减少技术，将势函数信息整合到转移核中。该算子诱导了Feynman-Kac模型的偏序关系，从而建立了粒子滤波器渐近方差的排序，为算法设计提供了新方法。

Result: 理论推广了现有技术如模型边缘化，并提供了定量的渐近方差排序结果。重新审视了完全自适应（辅助）粒子滤波器，通过knot理论的小修改保证了所有相关测试函数的渐近方差排序。

Conclusion: knot算子为粒子滤波器设计提供了新的理论框架，能够系统性地提高算法效率，并统一和扩展了现有的方差减少技术。

Abstract: Sequential Monte Carlo algorithms, or particle filters, are widely used for
approximating intractable integrals, particularly those arising in Bayesian
inference and state-space models. We introduce a new variance reduction
technique, the knot operator, which improves the efficiency of particle filters
by incorporating potential function information into part, or all, of a
transition kernel. The knot operator induces a partial ordering of Feynman-Kac
models that implies an order on the asymptotic variance of particle filters,
offering a new approach to algorithm design. We discuss connections to existing
strategies for designing efficient particle filters, including model
marginalisation. Our theory generalises such techniques and provides
quantitative asymptotic variance ordering results. We revisit the fully-adapted
(auxiliary) particle filter using our theory of knots to show how a small
modification guarantees an asymptotic variance ordering for all relevant test
functions.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [14] [Lung Cancer Survival Prediction Using Machine Learning and Statistical Methods](https://arxiv.org/abs/2510.01267)
*Varun Vishwanathan Nair,Victor Miranda Soberanis*

Main category: stat.AP

TL;DR: 该研究整合基线特征和术后变量（如无进展生存期和残留肿瘤状态）构建肺癌生存预测模型，Cox模型C-index达0.90，RSF模型达0.86，优于以往研究。


<details>
  <summary>Details</summary>
Motivation: 现有肺癌生存模型仅依赖基线因素，忽视了反映疾病进展的术后变量，无法提供临床意义更强的生存预测。

Method: 应用Cox比例风险模型和随机生存森林，整合基线特征与术后预测因子（无进展生存期、残留肿瘤状态）。

Result: Cox模型C-index为0.90，随机生存森林模型为0.86，均显著优于先前研究。术后变量的整合提供了更具临床意义的生存估计。

Conclusion: 常规收集的临床变量可转化为可操作的生存预测，有助于改善治疗计划、个性化患者咨询和随访策略。

Abstract: Lung cancer remains one of the leading causes of cancer-related mortality,
yet most survival models rely only on baseline factors and overlook
posttreatment variables that reflect disease progression. To address this gap,
we applied Cox Proportional Hazards and Random Survival Forests, integrating
baseline features with post-treatment predictors such as progression-free
interval (PFI.time) and residual tumor status. The Cox model achieved a
concordance index (C-index) of 0.90, while the RSF model reached 0.86, both
outperforming previous studies. Beyond statistical gains, the integration of
post-treatment variables provides oncologists with more clinically meaningful
and reliable survival estimates. This enables improved treatment planning, more
personalized patient counseling, and better-informed follow-up strategies. From
a practical standpoint, these results demonstrate how routinely collected
clinical variables can be transformed into actionable survival predictions.

</details>


### [15] [Neural Tangent Kernels for Complex Genetic Risk Prediction: Bridging Deep Learning and Kernel Methods in Genomics](https://arxiv.org/abs/2510.01426)
*Heng Ge,Qing Lu*

Main category: stat.AP

TL;DR: 开发了基于神经正切核(NTK)的框架，将核方法整合到深度神经网络中用于遗传风险预测，在阿尔茨海默病相关数据分析中表现出优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 遗传风险预测的复杂性需要能够有效捕捉复杂基因型-表型关系（如非线性关系）的新方法，同时保持统计可解释性和计算可行性。

Method: 提出了两种NTK方法：NTK-LMM（将经验NTK嵌入线性混合模型，通过MINQUE估计方差分量）和NTK-KRR（使用交叉验证正则化的核岭回归）。

Result: 模拟研究表明NTK模型优于传统神经网络和线性混合模型。在ADNI数据应用中，NTK在海马体积和内嗅皮层厚度预测中达到更高准确度。

Conclusion: 通过整合深度神经网络和核方法的优势，NTK在遗传风险预测分析中提供了有竞争力的性能，同时具有可解释性和计算效率的优势。

Abstract: Given the complexity of genetic risk prediction, there is a critical need for
the development of novel methodologies that can effectively capture intricate
genotype--phenotype relationships (e.g., nonlinear) while remaining
statistically interpretable and computationally tractable. We develop a Neural
Tangent Kernel (NTK) framework to integrate kernel methods into deep neural
networks for genetic risk prediction analysis. We consider two approaches:
NTK-LMM, which embeds the empirical NTK in a linear mixed model with variance
components estimated via minimum quadratic unbiased estimator (MINQUE), and
NTK-KRR, which performs kernel ridge regression with cross-validated
regularization. Through simulation studies, we show that NTK-based models
outperform the traditional neural network models and linear mixed models. By
applying NTK to endophenotypes (e.g., hippocampal volume) and AD-related genes
(e.g., APOE) from Alzheimer's Disease Neuroimaging Initiative (ADNI), we found
that NTK achieved higher accuracy than existing methods for hippocampal volume
and entorhinal cortex thickness. In addition to its accuracy performance, NTK
has favorable optimization properties (i.e., having a closed-form or convex
training) and generates interpretable results due to its connection to variance
components and heritability. Overall, our results indicate that by integrating
the strengths of both deep neural networks and kernel methods, NTK offers
competitive performance for genetic risk prediction analysis while having the
advantages of interpretability and computational efficiency.

</details>


### [16] [The Perceived Influences of Environment on Health in Italy: a Penalized Ordinal Regression Approach](https://arxiv.org/abs/2510.01803)
*Mattia Stival,Angela Andreella,Gaia Bertarelli,Catarina Midões,Stefano Federico Tonellato,Enrica De Cian,Stefano Campostrini*

Main category: stat.AP

TL;DR: 该研究通过整合意大利全国健康监测系统PASSI的环境模块与市级背景信息，使用惩罚性半并行累积有序回归模型分析环境感知的个人和地域决定因素，揭示了环境感知的显著地域异质性和PM2.5等危险因素与较差环境感知的关联。


<details>
  <summary>Details</summary>
Motivation: 理解个体如何感知生活环境是一个复杂任务，因为它反映了个人和背景决定因素。研究旨在分析环境感知如何受到个人和地域因素的双重影响。

Method: 采用惩罚性半并行累积有序回归模型，整合PASSI健康监测系统的环境模块与市级社会经济指标、污染暴露等背景信息，平衡灵活性和可解释性，同时处理多重共线性和分离问题。

Result: 结果显示意大利各地存在显著异质性，地方特征强烈影响环境感知；个人因素与背景影响相互作用塑造感知；PM2.5等危险环境因素与较差环境感知相关，表明受访者能够识别特定环境问题。

Conclusion: 该方法在应用方面展现出强大潜力，为环境政策规划提供了有用的见解，证明了整合个人和背景因素分析环境感知的有效性。

Abstract: Understanding how individuals perceive their living environment is a complex
task, as it reflects both personal and contextual determinants. In this paper,
we address this task by analyzing the environmental module of the Italian
nationwide health surveillance system PASSI (Progressi delle Aziende Sanitarie
per la Salute in Italia), integrating it with contextual information at the
municipal level, including socio-economic indicators, pollution exposure, and
other geographical characteristics. Methodologically, we adopt a penalized
semi-parallel cumulative ordinal regression model to analyze how subjective
perceptions are shaped by both personal and territorial determinants. The
approach balances flexibility and interpretability by allowing both parallel
and non-parallel effects while regularizing estimates to address
multicollinearity and separation issues. We use the model as an analytical tool
to uncover the determinants of positivity and neutrality in environmental
perceptions, defined as factors that contribute the most to improving
perception or increasing the sense of neutrality. The results are diverse.
First, results reveal significant heterogeneity across Italian territories,
indicating that local characteristics strongly shape environmental perception.
Second, various individual factors interact with contextual influences to shape
perceptions. Third, hazardous environmental factors, such as higher PM2.5
levels, appear to be associated with poorer environmental perception,
suggesting a tendency among respondents to recognize specific environmental
issues. Overall, the approach demonstrates strong potential for application and
provides useful insights for environmental policy planning.

</details>


### [17] [Dependent stochastic block models for age-indexed sequences of directed causes-of-death networks](https://arxiv.org/abs/2510.01806)
*Giovanni Romanò,Cristian Castiglione,Daniele Durante*

Main category: stat.AP

TL;DR: 提出了一种新的贝叶斯随机块模型，用于分析死亡证书中潜在原因和促成原因之间的年龄特异性相互作用网络，揭示了美国死亡数据中隐藏的群体结构和演化模式。


<details>
  <summary>Details</summary>
Motivation: 死亡事件通常由相互关联的原因复杂交互引起，但目前缺乏合适的随机块模型来分析年龄索引的有向网络序列，以学习潜在原因和促成原因之间的年龄特异性群体相互作用。

Method: 开发了一种新颖的贝叶斯公式，关键地学习了潜在原因和促成原因的两个独立群体结构，同时通过依赖随机分区先验允许这些结构在年龄间平滑变化。

Result: 在模拟中该模型优于现有解决方案，应用于美国死亡率数据时揭示了先前研究中隐藏的死亡原因群体组成、演化和模块化相互作用结构。

Conclusion: 该研究为理解美国近期'绝望死亡'现象提供了新见解，可能具有相关政策意义，并改进了对死亡原因复杂相互作用的理解。

Abstract: Death events commonly arise from complex interactions among interrelated
causes, formally classified in reporting practices as underlying and
contributing. Leveraging information from death certificates, these
interactions can be naturally represented through a sequence of directed
networks encoding co-occurrence strengths between pairs of underlying and
contributing causes across ages. Although this perspective opens the avenues to
learn informative age-specific block interactions among endogenous groups of
underlying and contributing causes displaying similar co-occurrence patterns,
there has been limited research along this direction in mortality modeling.
This is mainly due to the lack of suitable stochastic block models for
age-indexed sequences of directed networks. We cover this gap through a novel
Bayesian formulation which crucially learns two separate group structures for
underlying and contributing causes, while allowing both structures to change
smoothly across ages via dependent random partition priors. As illustrated in
simulations, this formulation outperforms state-of-the-art solutions that could
be adapted to our motivating application. Moreover, when applied to USA
mortality data, it unveils structures in the composition, evolution, and
modular interactions among causes-of-death groups that were hidden to previous
studies. Such findings could have relevant policy implications and contribute
to an improved understanding of the recent "death of despair" phenomena in USA.

</details>


### [18] [Multidata Causal Discovery for Statistical Hurricane Intensity Forecasting](https://arxiv.org/abs/2510.02050)
*Saranya Ganesh S.,Frederick Iat-Hin Tam,Milton S. Gomez,Marie McGraw,Mark DeMaria,Kate Musgrave,Jakob Runge,Tom Beucler*

Main category: stat.AP

TL;DR: 本文提出使用因果发现框架改进大西洋飓风强度预测，通过识别与飓风强度变化因果相关的预测因子，构建扩展预测集SHIPS+，在短期预测中显著提升预测技能。


<details>
  <summary>Details</summary>
Motivation: 传统飓风强度统计预测方法受限于复杂非线性相互作用和难以识别相关预测因子，往往忽视混杂变量，导致对未见热带风暴的泛化能力有限。

Method: 采用多数据因果发现框架，基于ERA5再分析数据的SHIPS数据集，识别因果相关的预测因子，训练多元线性回归模型，并与无选择、相关性和随机森林特征重要性方法进行比较。

Result: 因果特征选择在未见测试案例中表现最优，特别是在3天以内的短时预测。构建的SHIPS+预测集在24、48和72小时预测中提升了预测技能，使用多层感知器进一步扩展了长时预测能力。

Conclusion: 因果发现显著改善了飓风强度预测，三个新增因果发现预测因子在业务测试中提升了预测效果，为更经验性的预测铺平了道路。

Abstract: Improving statistical forecasts of Atlantic hurricane intensity is limited by
complex nonlinear interactions and difficulty in identifying relevant
predictors. Conventional methods prioritize correlation or fit, often
overlooking confounding variables and limiting generalizability to unseen
tropical storms. To address this, we leverage a multidata causal discovery
framework with a replicated dataset based on Statistical Hurricane Intensity
Prediction Scheme (SHIPS) using ERA5 meteorological reanalysis. We conduct
multiple experiments to identify and select predictors causally linked to
hurricane intensity changes. We train multiple linear regression models to
compare causal feature selection with no selection, correlation, and random
forest feature importance across five forecast lead times from 1 to 5 days (24
to 120 hours). Causal feature selection consistently outperforms on unseen test
cases, especially for lead times shorter than 3 days. The causal features
primarily include vertical shear, mid-tropospheric potential vorticity and
surface moisture conditions, which are physically significant yet often
underutilized in hurricane intensity predictions. Further, we build an extended
predictor set (SHIPS+) by adding selected features to the standard SHIPS
predictors. SHIPS+ yields increased short-term predictive skill at lead times
of 24, 48, and 72 hours. Adding nonlinearity using multilayer perceptron
further extends skill to longer lead times, despite our framework being purely
regional and not requiring global forecast data. Operational SHIPS tests
confirm that three of the six added causally discovered predictors improve
forecasts, with the largest gains at longer lead times. Our results demonstrate
that causal discovery improves hurricane intensity prediction and pave the way
toward more empirical forecasts.

</details>


### [19] [How to Find Fantastic Papers: Self-Rankings as a Powerful Predictor of Scientific Impact Beyond Peer Review](https://arxiv.org/abs/2510.02143)
*Buxin Su,Natalie Collina,Garrett Wen,Didong Li,Kyunghyun Cho,Jianqing Fan,Bingxin Zhao,Weijie Su*

Main category: stat.AP

TL;DR: 作者自我排名能有效识别高影响力AI研究，比同行评审更能预测论文引用量，可作为同行评审的有价值补充。


<details>
  <summary>Details</summary>
Motivation: 在AI等快速发展的领域，同行评审难以识别具有高科学潜力的研究。作者对自身工作的独特理解可能为识别高影响力研究提供新视角。

Method: 在顶级AI会议上进行大规模实验，让1,342名研究人员对其2,592篇投稿进行自我质量排名，并追踪一年多的引用结果。

Result: 作者排名最高的论文引用量是最低排名论文的两倍；自我排名在识别高引用论文（超过150次引用）方面特别有效，且优于同行评审分数。

Conclusion: 作者自我排名为识别和提升AI领域高影响力研究提供了可靠且有价值的补充方法。

Abstract: Peer review in academic research aims not only to ensure factual correctness
but also to identify work of high scientific potential that can shape future
research directions. This task is especially critical in fast-moving fields
such as artificial intelligence (AI), yet it has become increasingly difficult
given the rapid growth of submissions. In this paper, we investigate an
underexplored measure for identifying high-impact research: authors' own
rankings of their multiple submissions to the same AI conference. Grounded in
game-theoretic reasoning, we hypothesize that self-rankings are informative
because authors possess unique understanding of their work's conceptual depth
and long-term promise. To test this hypothesis, we conducted a large-scale
experiment at a leading AI conference, where 1,342 researchers self-ranked
their 2,592 submissions by perceived quality. Tracking outcomes over more than
a year, we found that papers ranked highest by their authors received twice as
many citations as their lowest-ranked counterparts; self-rankings were
especially effective at identifying highly cited papers (those with over 150
citations). Moreover, we showed that self-rankings outperformed peer review
scores in predicting future citation counts. Our results remained robust after
accounting for confounders such as preprint posting time and self-citations.
Together, these findings demonstrate that authors' self-rankings provide a
reliable and valuable complement to peer review for identifying and elevating
high-impact research in AI.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [20] [DiffKnock: Diffusion-based Knockoff Statistics for Neural Networks Inference](https://arxiv.org/abs/2510.01418)
*Heng Ge,Qing Lu*

Main category: stat.ME

TL;DR: DiffKnock是一种基于扩散模型的knockoff框架，用于高维特征选择，具有有限样本错误发现率控制功能。


<details>
  <summary>Details</summary>
Motivation: 解决现有knockoff方法在保持复杂特征依赖性和检测非线性关联方面的局限性。

Method: 训练扩散模型生成有效knockoff，使用基于神经网络的梯度和过滤统计量构建反对称特征重要性度量。

Result: 在模拟中，DiffKnock比基于自编码器的knockoff方法具有更高的检测能力，同时保持目标FDR。在小鼠单细胞RNA-seq数据分析中成功识别了NF-κB靶基因和调控因子。

Conclusion: DiffKnock通过结合深度生成模型的灵活性和严格的统计保证，成为分析单细胞RNA-seq数据及其他高维结构化数据的强大可靠工具。

Abstract: We introduce DiffKnock, a diffusion-based knockoff framework for
high-dimensional feature selection with finite-sample false discovery rate
(FDR) control. DiffKnock addresses two key limitations of existing knockoff
methods: preserving complex feature dependencies and detecting non-linear
associations. Our approach trains diffusion models to generate valid knockoffs
and uses neural network--based gradient and filter statistics to construct
antisymmetric feature importance measures. Through simulations, we showed that
DiffKnock achieved higher power than autoencoder-based knockoffs while
maintaining target FDR, indicating its superior performance in scenarios
involving complex non-linear architectures. Applied to murine single-cell
RNA-seq data of LPS-stimulated macrophages, DiffKnock identifies canonical
NF-$\kappa$B target genes (Ccl3, Hmox1) and regulators (Fosb, Pdgfb). These
results highlight that, by combining the flexibility of deep generative models
with rigorous statistical guarantees, DiffKnock is a powerful and reliable tool
for analyzing single-cell RNA-seq data, as well as high-dimensional and
structured data in other domains.

</details>


### [21] [Repro Samples Method for Model-Free Inference in High-Dimensional Binary Classification](https://arxiv.org/abs/2510.01468)
*Xiaotian Hou,Peng Wang,Minge Xie,Linjun Zhang*

Main category: stat.ME

TL;DR: 提出一种基于repro samples框架的高维二元模型统计推断方法，利用稀疏约束的工作GLM进行推断，不依赖特定模型假设，支持模型选择、案例概率和回归系数推断。


<details>
  <summary>Details</summary>
Motivation: 解决高维二元模型中统计推断的挑战，特别是当模型结构未知或工作模型可能错误设定时，需要一种不依赖特定模型假设的稳健推断方法。

Method: 基于repro samples框架生成模拟数据生成过程的人工样本，利用稀疏约束的工作GLM进行推断，涵盖模型支持、案例概率和oracle回归系数等目标。

Result: 模拟结果显示该方法能产生有效且小的模型候选集，在真实模型为工作模型时比现有去偏方法有更好的覆盖性能。在单细胞RNA-seq数据分析中发现了新的显著基因。

Conclusion: 该方法为高维二元模型提供了一种模型无关的稳健推断框架，在模型选择和回归系数推断方面表现优异，并具有实际应用价值。

Abstract: This paper presents a novel method for statistical inference in
high-dimensional binary models with unspecified structure, where we leverage a
(potentially misspecified) sparsity-constrained working generalized linear
model (GLM) to facilitate the inference process. Our method is based on the
repro samples framework, which generates artificial samples that mimic the
actual data-generating process. Our inference targets include the model
support, case probabilities, and the oracle regression coefficients defined in
the working GLM. The proposed method has three major advantages. First, this
approach is model-free, that is, it does not rely on specific model assumptions
such as logistic or probit regression, nor does it require sparsity assumptions
on the underlying model. Second, for model support, we construct a model
candidate set for the most influential covariates that achieves guaranteed
coverage under a weak signal strength assumption. Third, for oracle regression
coefficients, we establish confidence sets for any group of linear combinations
of regression coefficients. Simulation results demonstrate that the proposed
method produces valid and small model candidate sets. It also achieves better
coverage for regression coefficients than the state-of-the-art debiasing
methods when the working model is the actual model that generates the sample
data. Additionally, we analyze single-cell RNA-seq data on the immune response.
Besides identifying genes previously proven as relevant in the literature, our
method also discovers a significant gene that has not been studied before,
revealing a potential new direction in understanding cellular immune response
mechanisms.

</details>


### [22] [Scalable Asynchronous Federated Modeling for Spatial Data](https://arxiv.org/abs/2510.01771)
*Jianwei Shi,Sameh Abdulah,Ying Sun,Marc G. Genton*

Main category: stat.ME

TL;DR: 提出了一种基于低秩高斯过程近似的异步联邦建模框架，用于处理空间数据，解决了现有方法忽略空间依赖性或同步更新在异构环境中性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 空间数据通常分布在设备上，受隐私和通信限制无法直接共享。联邦建模能保护隐私并实现全局建模，但现有方法要么忽略空间依赖性，要么依赖同步更新在异构环境中存在性能问题。

Method: 基于低秩高斯过程近似，采用分块优化策略，引入梯度校正、自适应聚合和稳定更新方法，建立具有明确陈旧度依赖的线性收敛理论。

Result: 异步算法在平衡资源分配下达到同步性能，在异构设置中显著优于同步方法，展现出更好的鲁棒性和可扩展性。

Conclusion: 该异步联邦建模框架为空间数据提供了一种高效、鲁棒的解决方案，在保持隐私的同时实现了高性能的分布式建模。

Abstract: Spatial data are central to applications such as environmental monitoring and
urban planning, but are often distributed across devices where privacy and
communication constraints limit direct sharing. Federated modeling offers a
practical solution that preserves data privacy while enabling global modeling
across distributed data sources. For instance, environmental sensor networks
are privacy- and bandwidth-constrained, motivating federated spatial modeling
that shares only privacy-preserving summaries to produce timely,
high-resolution pollution maps without centralizing raw data. However, existing
federated modeling approaches either ignore spatial dependence or rely on
synchronous updates that suffer from stragglers in heterogeneous environments.
This work proposes an asynchronous federated modeling framework for spatial
data based on low-rank Gaussian process approximations. The method employs
block-wise optimization and introduces strategies for gradient correction,
adaptive aggregation, and stabilized updates. We establish linear convergence
with explicit dependence on staleness, a result of standalone theoretical
significance. Moreover, numerical experiments demonstrate that the asynchronous
algorithm achieves synchronous performance under balanced resource allocation
and significantly outperforms it in heterogeneous settings, showcasing superior
robustness and scalability.

</details>


### [23] [SLOPE and Designing Robust Studies for Generalization](https://arxiv.org/abs/2510.01577)
*Xinran Miao,Jiwei Zhao,Hyunseung Kang*

Main category: stat.ME

TL;DR: 提出了SLOPE方法，用于量化条件可交换性局部违反的敏感性，帮助研究者选择稳健的源/目标群体或估计量。


<details>
  <summary>Details</summary>
Motivation: 在泛化任务中，条件可交换性假设经常因源和目标群体间的不可观测差异而难以满足，且无法用数据验证，需要稳健的数据收集和研究设计。

Method: 结合因果推断中的敏感性分析和Hampel(1974)的基于导数的稳健性度量，提出SLOPE方法，并与影响函数建立解析关系。

Result: SLOPE能帮助选择稳健的源/目标群体或估计量，并通过多国随机实验的重新分析展示了其在指导稳健研究设计中的作用。

Conclusion: SLOPE为处理条件可交换性违反提供了一种简单直观的敏感性度量工具，有助于改进泛化研究的稳健性。

Abstract: A popular task in generalization is to learn about a new, target population
based on data from an existing, source population. This task relies on
conditional exchangeability, which asserts that differences between the source
and target populations are fully captured by observable characteristics of the
two populations. Unfortunately, this assumption is often untenable in practice
due to unobservable differences between the source and target populations.
Worse, the assumption cannot be verified with data, warranting the need for
robust data collection processes and study designs that are inherently less
sensitive to violation of the assumption. In this paper, we propose SLOPE
(Sensitivity of LOcal Perturbations from Exchangeability), a simple, intuitive,
and novel measure that quantifies the sensitivity to local violation of
conditional exchangeability. SLOPE combines ideas from sensitivity analysis in
causal inference and derivative-based measure of robustness from Hampel (1974).
Among other properties, SLOPE can help investigators to choose (a) a robust
source or target population or (b) a robust estimand. Also, we show an analytic
relationship between SLOPE and influence functions (IFs), which investigators
can use to derive SLOPE given an IF. We conclude with a re-analysis of a
multi-national randomized experiment and illustrate the role of SLOPE in
informing robust study designs for generalization.

</details>


### [24] [Compressed Bayesian Tensor Regression](https://arxiv.org/abs/2510.01861)
*Roberto Casarin,Radu Craiu,Qing Wang*

Main category: stat.ME

TL;DR: 提出了一种广义张量随机投影方法，通过将高维张量协变量嵌入低维子空间来降低维度，同时最小化响应信息损失。该方法结合贝叶斯推断框架，提供理论支持并开发了高效的Gibbs采样器。


<details>
  <summary>Details</summary>
Motivation: 解决张量回归中常见的高维问题，在保持响应信息的同时显著降低计算成本。

Method: 使用广义张量随机投影方法进行降维，结合贝叶斯层次先验分布和参数的低秩表示，开发Gibbs采样器进行推断，并采用贝叶斯模型平均来减轻随机投影的敏感性。

Result: 模拟研究和实际数据应用表明，压缩贝叶斯张量回归相比标准方法在保持更好样本外预测性能的同时显著降低计算成本。

Conclusion: 该方法为高维张量回归提供了一种计算高效且预测性能良好的解决方案，具有理论和实际应用价值。

Abstract: To address the common problem of high dimensionality in tensor regressions,
we introduce a generalized tensor random projection method that embeds
high-dimensional tensor-valued covariates into low-dimensional subspaces with
minimal loss of information about the responses. The method is flexible,
allowing for tensor-wise, mode-wise, or combined random projections as special
cases. A Bayesian inference framework is provided featuring the use of a
hierarchical prior distribution and a low-rank representation of the parameter.
Strong theoretical support is provided for the concentration properties of the
random projection and posterior consistency of the Bayesian inference. An
efficient Gibbs sampler is developed to perform inference on the compressed
data. To mitigate the sensitivity introduced by random projections, Bayesian
model averaging is employed, with normalising constants estimated using reverse
logistic regression. An extensive simulation study is conducted to examine the
effects of different tuning parameters. Simulations indicate, and the real data
application confirms, that compressed Bayesian tensor regression can achieve
better out-of-sample prediction while significantly reducing computational cost
compared to standard Bayesian tensor regression.

</details>


### [25] [Forecasting intraday particle number size distribution: A functional time series approach](https://arxiv.org/abs/2510.01692)
*Han Lin Shang,Israel Martinez Hernandez*

Main category: stat.ME

TL;DR: 提出了一种多级功能时间序列框架，结合功能因子模型进行颗粒物数据的一日预测，并开发了校准方法和分位数预测方法来构建预测区间，同时采用动态更新技术提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 颗粒物数据包含多种粒径，形成高维功能时间序列，高维度既带来统计挑战也提供丰富信息，需要有效建模方法来分析时间变化并进行预测。

Method: 使用多级功能时间序列框架，结合功能因子模型进行预测，开发了校准方法和分位数预测方法构建预测区间，并采用动态更新技术。

Result: 方法在伦敦51种粒径颗粒物每小时测量数据的实证应用中得到了验证。

Conclusion: 所提出的方法能够有效处理高维功能时间序列数据，提供准确的点预测和区间预测，并通过动态更新技术提高预测精度。

Abstract: Particulate matter data now include various particle sizes, which often
manifest as a collection of curves observed sequentially over time. When
considering 51 distinct particle sizes, these curves form a high-dimensional
functional time series observed over equally spaced and densely sampled grids.
While high dimensionality poses statistical challenges due to the curse of
dimensionality, it also offers a rich source of information that enables
detailed analysis of temporal variation across short time intervals for all
particle sizes. To model this complexity, we propose a multilevel functional
time series framework incorporating a functional factor model to facilitate
one-day-ahead forecasting. To quantify forecast uncertainty, we develop a
calibration approach and a split conformal prediction approach to construct
prediction intervals. Both approaches are designed to minimise the absolute
difference between empirical and nominal coverage probabilities using a
validation dataset. Furthermore, to improve forecast accuracy as new intraday
data become available, we implement dynamic updating techniques for point and
interval forecasts. The proposed methods are validated through an empirical
application to hourly measurements of particulate matter in 51 size categories
in London.

</details>


### [26] [Stabilizing Thompson Sampling with Point Null Bayesian Response-Adaptive Randomization](https://arxiv.org/abs/2510.01734)
*Samuel Pawel,Leonhard Held*

Main category: stat.ME

TL;DR: 提出一种基于贝叶斯假设检验的响应自适应随机化方法，通过引入等效性零假设来平衡Thompson采样的高变异性与静态等概率随机化。


<details>
  <summary>Details</summary>
Motivation: Thompson采样在试验早期的高变异性会增加患者被分配到劣效治疗的风险，需要一种更稳健的方法来平衡探索与利用。

Method: 引入点零假设（假设各治疗等效），通过控制零假设的先验概率来调节向等概率随机化的收缩程度，Thompson采样和等概率随机化是该方法的特例。

Result: 模拟和真实世界案例显示该方法能平衡Thompson采样的高变异性与静态等概率随机化，统计特性与常用的Thompson采样改进方法相当。

Conclusion: 提出的点零贝叶斯RAR方法能有效缓解普通Thompson采样的问题，已在R包brar中实现，便于实验者使用。

Abstract: Response-adaptive randomization (RAR) methods use accumulated data to adapt
randomization probabilities, aiming to increase the probability of allocating
patients to effective treatments. A popular RAR method is Thompson sampling,
which randomizes patients proportionally to the Bayesian posterior probability
that each treatment is the most effective. However, its high variability early
in a trial can also increase the risk of assigning patients to inferior
treatments. We propose a principled method based on Bayesian hypothesis testing
to mitigate this issue. Specifically, we introduce a point null hypothesis that
postulates equal effectiveness of treatments. This induces shrinkage toward
equal randomization probabilities, with the degree of shrinkage controlled by
the prior probability of the null hypothesis. Equal randomization and Thompson
sampling arise as special cases when the prior probability is set to one or
zero, respectively. Simulated and real-world examples illustrate that the
proposed method balances highly variable Thompson sampling with static equal
randomization. A simulation study demonstrates that the method can mitigate
issues with ordinary Thompson sampling and has comparable statistical
properties to Thompson sampling with common ad hoc modifications such as power
transformation and probability capping. We implement the method in the
open-source R package brar, enabling experimenters to easily perform point null
Bayesian RAR and support more effective randomization of patients.

</details>


### [27] [Optimal smoothing parameter in Eilers-Wittaker smoother](https://arxiv.org/abs/2510.01798)
*Roberto Bernal-Arencibia,Karel Garcia Medina,Ernesto Estevez-Rams,Beatriz Aragon-Fernandez*

Main category: stat.ME

TL;DR: 提出基于残差谱熵的新方法来自动选择Eilers-Whittaker平滑方法的正则化参数，通过S曲线找到最优参数，在序列相关噪声情况下优于交叉验证。


<details>
  <summary>Details</summary>
Motivation: Eilers-Whittaker平滑方法的效果依赖于正则化参数的选择，传统方法如留一交叉验证在存在序列相关噪声时表现不佳，需要更稳健的自动选择方法。

Method: 定义S曲线：在残差谱熵与平滑信号谱熵的图中计算欧几里得距离，选择S曲线绝对最大值对应的正则化参数作为最优参数。

Result: 使用模拟数据验证，新方法在序列相关噪声情况下优于交叉验证和V曲线方法，并在多种实验数据上进行了验证。

Conclusion: 这种基于谱熵的稳健且简单的方法是对Eilers平滑器现有选择方法的有价值补充。

Abstract: The Eilers-Whittaker method for data smoothing effectiveness depends on the
choice of the regularisation parameter, and automatic selection is a necessity
for large datasets. Common methods, such as leave-one-out cross-validation, can
perform poorly when serially correlated noise is present. We propose a novel
procedure for selecting the control parameter based on the spectral entropy of
the residuals. We define an S-curve from the Euclidean distance between points
in a plot of the spectral entropy of the residuals versus that of the smoothed
signal. The regularisation parameter corresponding to the absolute maximum of
this S-curve is chosen as the optimal parameter. Using simulated data, we
benchmarked our method against cross-validation and the V-curve. Validation was
also performed on diverse experimental data. This robust and straightforward
procedure can be a valuable addition to the available selection methods for the
Eilers smoother.

</details>


### [28] [Multivariate distributional modeling of low, moderate, and large intensities without threshold selection steps](https://arxiv.org/abs/2510.02152)
*Carlo Gaetan,Philippe Naveau*

Main category: stat.ME

TL;DR: 提出了一种基于扩展广义帕累托分布(EGPD)的统一、无阈值框架，用于建模正数据的完整分布，特别适用于水文和气候学中的多元数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常将分布分割成不同区域，这引入了主观性并限制了连贯性，特别是在处理多元数据时。需要一种能够建模完整分布范围（从低值到极值）的统一方法。

Method: 将多元数据分解为径向和角度分量。径向分量使用半参数EGPD建模，角度分布允许条件变化。结合经典最大似然估计和基于Bernstein多项式的半参数方法估计径向分布，使用多元回归技术估计角度分量参数。

Result: 通过合成模拟和水文数据集评估，证明该方法能够捕捉重尾边缘分布和复杂的多元依赖关系，无需指定阈值。

Conclusion: 该框架提供了一种灵活、连贯的方法来建模多元正数据的完整分布，避免了传统分割方法的主观性限制。

Abstract: In fields such as hydrology and climatology, modelling the entire
distribution of positive data is essential, as stakeholders require insights
into the full range of values, from low to extreme. Traditional approaches
often segment the distribution into separate regions, which introduces
subjectivity and limits coherence. This is especially true when dealing with
multivariate data.
  In line with multivariate extreme value theory, this paper presents a
unified, threshold-free framework for modelling marginal behaviours and
dependence structures based on an extended generalized Pareto distribution
(EGPD). We propose decomposing multivariate data into radial and angular
components. The radial component is modelled using a semi-parametric EGPD and
the angular distribution is permitted to vary conditionally. This approach
allows for sufficiently flexible dependence modelling.
  The hierarchical structure of the model facilitates the inference process.
First, we combine classical maximum likelihood estimation (MLE) methods with
semi-parametric approaches based on Bernstein polynomials to estimate the
distribution of the radial component. Then, we use multivariate regression
techniques to estimate the angular component's parameters.
  The model is evaluated through synthetic simulations and applied to
hydrological datasets to exemplify its capacity to capture heavy-tailed
marginals and complex multivariate dependencies without threshold
specification.

</details>


### [29] [Predictively Oriented Posteriors](https://arxiv.org/abs/2510.01915)
*Yann McLatchie,Badr-Eddine Cherief-Abdellatif,David T. Frazier,Jeremias Knoblauch*

Main category: stat.ME

TL;DR: 提出预测导向（PrO）后验，结合参数推断和密度估计的优点，在模型错误设定时仍能保持预测最优性，收敛速度达n^{-1/2}。


<details>
  <summary>Details</summary>
Motivation: 结合参数推断和密度估计的最优特性，解决传统贝叶斯方法在模型错误设定时预测性能下降的问题。

Method: 基于预测能力的预测导向后验，通过平均场朗之万动力学进行粒子演化采样。

Result: PrO后验在模型正确设定时以n^{1/2}速率收敛，在模型错误设定时稳定于预测最优后验，其不确定性可解释为模型错误设定程度。

Conclusion: PrO后验提供了一种新的统计推断框架，在模型错误设定时仍能保持预测优势，且不确定性具有可解释性。

Abstract: We advocate for a new statistical principle that combines the most desirable
aspects of both parameter inference and density estimation. This leads us to
the predictively oriented (PrO) posterior, which expresses uncertainty as a
consequence of predictive ability. Doing so leads to inferences which
predictively dominate both classical and generalised Bayes posterior predictive
distributions: up to logarithmic factors, PrO posteriors converge to the
predictively optimal model average at rate $n^{-1/2}$. Whereas classical and
generalised Bayes posteriors only achieve this rate if the model can recover
the data-generating process, PrO posteriors adapt to the level of model
misspecification. This means that they concentrate around the true model at
rate $n^{1/2}$ in the same way as Bayes and Gibbs posteriors if the model can
recover the data-generating distribution, but do \textit{not} concentrate in
the presence of non-trivial forms of model misspecification. Instead, they
stabilise towards a predictively optimal posterior whose degree of irreducible
uncertainty admits an interpretation as the degree of model misspecification --
a sharp contrast to how Bayesian uncertainty and its existing extensions
behave. Lastly, we show that PrO posteriors can be sampled from by evolving
particles based on mean field Langevin dynamics, and verify the practical
significance of our theoretical developments on a number of numerical examples.

</details>


### [30] [Identifying Subgroup and Context Effects in Conjoint Experiments](https://arxiv.org/abs/2510.02123)
*Steven Wang,Isys Johnson,Jessica Grogan,Lalit Jain,Atri Rudra,Kyle Hunt,Kenneth Joseph*

Main category: stat.ME

TL;DR: 本文系统评估了联合实验中的主要效应和交互效应推断方法，并提出了一种基于机器学习的新型黑盒推断框架，用于在统计功效受限的情况下可靠地检测异质性效应。


<details>
  <summary>Details</summary>
Motivation: 联合实验在政治学等领域日益重要，但研究者面临在统计功效受限情况下检测和解释交互效应的挑战，需要更可靠的方法来推断主要效应和交互效应。

Method: 系统评估了事后修正、稀疏回归方法和贝叶斯模型等主流方法，并提出了一种基于机器学习的黑盒推断框架，在不同稀疏度、噪声和数据可用性的模拟环境中进行测试。

Result: 提出的机器学习框架在计算效率和准确性之间取得了平衡，为研究异质性效应提供了实用工具。

Conclusion: 该黑盒推断框架能够可靠地恢复联合实验中的主要效应和交互效应，解决了统计功效约束下的异质性分析挑战。

Abstract: Conjoint experiments have become central to survey research in political
science and related fields because they allow researchers to study preferences
across multiple attributes simultaneously. Beyond estimating main effects,
scholars increasingly analyze heterogeneity through subgroup analysis and
contextual variables, raising methodological challenges in detecting and
interpreting interaction effects. Statistical power constraints, common in
survey experiments, further complicate this task. This paper addresses the
question: how can both main and interaction effects be reliably inferred in
conjoint studies? We contribute in two ways. First, we conduct a systematic
evaluation of leading approaches, including post-hoc corrections, sparse
regression methods, and Bayesian models, across simulation regimes that vary
sparsity, noise, and data availability. Second, we propose a novel black-box
inference framework that leverages machine learning to recover main and
interaction effects in conjoint experiments. Our approach balances
computational efficiency with accuracy, providing a practical tool for
researchers studying heterogeneous effects.

</details>


### [31] [Improving Survival Models in Healthcare by Balancing Imbalanced Cohorts: A Novel Approach](https://arxiv.org/abs/2510.02137)
*Catherine Ning,Dimitris Bertsimas,Johan Gagnière,Stefan Buettner,Per Eystein Loenning,Hideo Baba,Itaru Endo,Georgios Stasinos,Richard Burkhart,Federico N. Auecio,Felix Balzer,Cornelis Verhoef,Martin E. Kreis,Georgios Antonios Margonis*

Main category: stat.ME

TL;DR: 提出一种风险分层采样方法，通过重新平衡训练数据在预后风险亚组中的分布来改进生存模型在代表性不足的高风险和低风险亚组中的性能。


<details>
  <summary>Details</summary>
Motivation: 在预后谱中代表性不足的高风险和低风险亚组中，临床决策最为关键，但现有模型在这些区域的性能可能受限。

Method: 基于基线预后风险对患者进行分层，在每个风险层内应用匹配来平衡风险分布中的代表性，使用Cox比例风险模型。

Result: 在内部验证中，基于风险平衡队列训练的Cox模型相比完整数据集训练的模型表现一致改善，在外部队列的代表性不足高风险和低风险亚组中显著提高了分层C指数值。

Conclusion: 通过有针对性地重新平衡训练数据在预后风险亚组中的分布，可以显著改善观察性肿瘤学队列中生存模型的性能，为现有方法提供了一个实用且模型无关的补充。

Abstract: We explore whether survival model performance in underrepresented high- and
low-risk subgroups - regions of the prognostic spectrum where clinical
decisions are most consequential - can be improved through targeted
restructuring of the training dataset. Rather than modifying model
architecture, we propose a novel risk-stratified sampling method that addresses
imbalances in prognostic subgroup density to support more reliable learning in
underrepresented tail strata. We introduce a novel methodology that partitions
patients by baseline prognostic risk and applies matching within each stratum
to equalize representation across the risk distribution. We implement this
framework on a cohort of 1,799 patients with resected colorectal liver
metastases (CRLM), including 1,197 who received adjuvant chemotherapy and 602
who did not. All models used in this study are Cox proportional hazards models
trained on the same set of selected variables. Model performance is assessed
via Harrell's C index, time-dependent AUC, and Integrated Calibration Index
(ICI), with internal validation using Efron's bias-corrected bootstrapping.
External validation is conducted on two independent CRLM datasets. Cox models
trained on risk-balanced cohorts showed consistent improvements in internal
validation compared to models trained on the full dataset while noticeably
enhancing stratified C-index values in underrepresented high- and low-risk
strata of the external cohorts. Our findings suggest that survival model
performance in observational oncology cohorts can be meaningfully improved
through targeted rebalancing of the training data across prognostic risk
strata. This approach offers a practical and model-agnostic complement to
existing methods, especially in applications where predictive reliability
across the full risk continuum is critical to downstream clinical decisions.

</details>
