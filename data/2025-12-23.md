<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 7]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 9]
- [stat.CO](#stat.CO) [Total: 2]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Disentangled representations via score-based variational autoencoders](https://arxiv.org/abs/2512.17127)
*Benjamin S. H. Lyo,Eero P. Simoncelli,Cristina Savin*

Main category: stat.ML

TL;DR: SAMI是一种结合扩散模型和VAE的无监督表示学习方法，通过统一两者的证据下界，利用扩散过程的分数指导学习有意义的表示结构。


<details>
  <summary>Details</summary>
Motivation: 扩散模型中隐含的结构信息可以通过与变分自编码器的协同组合变得显式和可解释。现有的无监督表示学习方法在捕获数据中的语义结构方面存在局限，需要一种能够自动学习有意义表示的方法。

Method: SAMI统一了扩散模型和VAE的证据下界，形成一个原则性目标函数，通过底层扩散过程的分数指导来学习表示。该方法可以从预训练扩散模型中提取有用表示，且只需极少额外训练。

Result: 在合成数据集中恢复真实生成因子；从复杂自然图像中学习因子化、语义化的潜在维度；将视频序列编码为比替代编码器更直的潜在轨迹（尽管仅使用静态图像训练）；能够从预训练扩散模型中提取有用表示。

Conclusion: 扩散模型中隐含的结构信息可以通过与变分自编码器的协同组合变得显式和可解释。SAMI的数学精确性允许对学习表示的性质做出形式化陈述，为无监督语义轴识别提供了新方法。

Abstract: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.

</details>


### [2] [Sharp Structure-Agnostic Lower Bounds for General Functional Estimation](https://arxiv.org/abs/2512.17341)
*Jikai Jin,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 本文系统研究了结构不可知估计器的最优误差率，证明了双重稳健学习在ATE估计中的最优性，并将分析扩展到一般函数类，确立了去偏/双重机器学习(DML)的结构不可知最优性。


<details>
  <summary>Details</summary>
Motivation: 传统最优估计方法依赖强结构假设，在实际中可能被错误设定且部署复杂。这引发了对结构不可知方法的兴趣——这些方法在不施加结构先验的情况下对黑盒干扰估计进行去偏。理解这些方法的基本极限至关重要。

Method: 首先证明双重稳健学习在ATE估计中达到最优结构不可知误差率，然后将分析扩展到依赖未知干扰函数的一般函数类，区分双重稳健可达到和不可达到两种机制，展示DML在两种机制中的最优性，并通过推导显式最优率来实例化一般下界。

Result: 证明了双重稳健学习在ATE估计中的结构不可知最优性，确立了DML在一般函数类估计中的最优性，区分了两种机制下的不同最优率，推导了显式最优率并恢复了现有结果。

Conclusion: 结果为广泛使用的一阶去偏方法提供了理论验证，并为在缺乏结构假设情况下寻求最优方法的实践者提供了指导。本文概括并包含了作者先前在ATE下界方面的工作。

Abstract: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.

</details>


### [3] [Generative modeling of conditional probability distributions on the level-sets of collective variables](https://arxiv.org/abs/2512.17374)
*Fatima-Zahrae Akhyar,Wei Zhang,Gabriel Stoltz,Christof Schütte*

Main category: stat.ML

TL;DR: 提出一种高效学习条件概率分布生成模型的方法，能够在集体变量不同能级集上同时学习，并利用增强采样技术提升低概率区域的学习质量。


<details>
  <summary>Details</summary>
Motivation: 在生物物理等领域的分子系统生成建模中，需要学习概率分布在集体变量能级集上的条件分布，特别是在低概率区域的学习质量需要提升。

Method: 提出通用高效的学习方法，能够在集体变量不同能级集上同时学习生成模型，并利用增强采样技术进行数据增强以改善低概率区域的学习。

Result: 通过具体数值示例证明了所提出学习方法的有效性，特别是在不同能级集上的生成建模表现良好。

Conclusion: 该方法在生物物理分子系统生成建模中具有潜在应用价值，能够有效处理集体变量能级集上的条件分布学习问题。

Abstract: Given a probability distribution $μ$ in $\mathbb{R}^d$ represented by data, we study in this paper the generative modeling of its conditional probability distributions on the level-sets of a collective variable $ξ: \mathbb{R}^d \rightarrow \mathbb{R}^k$, where $1 \le k<d$. We propose a general and effcient learning approach that is able to learn generative models on different level-sets of $ξ$ simultaneously. To improve the learning quality on level-sets in low-probability regions, we also propose a strategy for data enrichment by utilizing data from enhanced sampling techniques. We demonstrate the effectiveness of our proposed learning approach through concrete numerical examples. The proposed approach is potentially useful for the generative modeling of molecular systems in biophysics, for instance.

</details>


### [4] [Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing](https://arxiv.org/abs/2512.17426)
*Xiaosi Gu,Ayaka Sakata,Tomoyuki Obuchi*

Main category: stat.ML

TL;DR: 该论文提出了基于一阶副本对称破缺(1RSB)的近似消息传递算法(1RSB-AMP)，用于SCAD惩罚项的稀疏信号重建，相比传统RS-AMP提高了重建性能。


<details>
  <summary>Details</summary>
Motivation: 传统副本对称(RS)的AMP算法在某些参数区域会发散，且无法达到贝叶斯最优性能。需要开发更先进的1RSB-AMP算法来克服这些限制，提高稀疏信号重建的性能。

Method: 从1RSB信念传播出发，推导出1RSB-AMP的显式更新规则和对应的状态演化(1RSB-SE)方程。提出最小化发散区域的新准则来确定Parisi参数，并结合非凸控制协议。

Result: 1RSB-AMP与1RSB-SE在宏观层面高度一致。相比RS-AMP，1RSB-AMP提高了完美重建的算法极限，虽然增益有限且仍略低于贝叶斯最优阈值。成功分析了1RSB相的热力学量。

Conclusion: 1RSB-AMP为SCAD惩罚项的稀疏信号重建提供了改进的算法框架，通过1RSB扩展克服了RS-AMP的发散问题，并在实践中实现了性能提升，尽管仍存在进一步优化的空间。

Abstract: We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.

</details>


### [5] [Fast and Robust: Computationally Efficient Covariance Estimation for Sub-Weibull Vectors](https://arxiv.org/abs/2512.17632)
*Even He*

Main category: stat.ML

TL;DR: 提出一种计算高效的协方差估计方法，针对Sub-Weibull分布（拉伸指数尾分布），使用交叉拟合范数截断估计器，在保持谱几何的同时达到O(Nd²)计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 高维协方差估计对异常值敏感，现有统计最优估计器（如半定规划或迭代M估计）计算成本高（O(d³)），需要针对特定分布设计更高效的算法。

Method: 提出交叉拟合范数截断估计器，不同于逐元素截断，该方法保持谱几何特性，计算复杂度为O(Nd²)，利用加权Hanson-Wright不等式进行理论分析。

Result: 在Sub-Weibull分布类中，证明该估计器以高概率达到最优次高斯速率$\tilde{O}(\sqrt{r(Σ)/N})$，指数尾衰减补偿了各向异性数据中球形截断的几何次优性。

Conclusion: 为高维数据提供了一种可扩展的协方差估计解决方案，适用于尾部比高斯分布重但比多项式衰减轻的分布，在计算效率和统计性能之间取得平衡。

Abstract: High-dimensional covariance estimation is notoriously sensitive to outliers. While statistically optimal estimators exist for general heavy-tailed distributions, they often rely on computationally expensive techniques like semidefinite programming or iterative M-estimation ($O(d^3)$). In this work, we target the specific regime of \textbf{Sub-Weibull distributions} (characterized by stretched exponential tails $\exp(-t^α)$). We investigate a computationally efficient alternative: the \textbf{Cross-Fitted Norm-Truncated Estimator}. Unlike element-wise truncation, our approach preserves the spectral geometry while requiring $O(Nd^2)$ operations, which represents the theoretical lower bound for constructing a full covariance matrix. Although spherical truncation is geometrically suboptimal for anisotropic data, we prove that within the Sub-Weibull class, the exponential tail decay compensates for this mismatch. Leveraging weighted Hanson-Wright inequalities, we derive non-asymptotic error bounds showing that our estimator recovers the optimal sub-Gaussian rate $\tilde{O}(\sqrt{r(Σ)/N})$ with high probability. This provides a scalable solution for high-dimensional data that exhibits tails heavier than Gaussian but lighter than polynomial decay.

</details>


### [6] [Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design](https://arxiv.org/abs/2512.17659)
*Madhav R. Muthyala,Farshud Sorourifar,Tianhong Tan,You Peng,Joel A. Paulson*

Main category: stat.ML

TL;DR: 提出一种模块化的"生成-优化"框架用于多目标分子设计，结合生成模型构建候选分子池，使用新型采集函数qPMHI进行批量选择，在可持续能源存储应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 分子设计需要同时满足多个相互冲突的目标，化学空间巨大且高保真模拟成本高。现有方法通常依赖连续潜在空间，存在架构纠缠和可扩展性挑战。

Method: 提出模块化"生成-优化"框架：1) 使用生成模型构建大规模多样化的候选分子池；2) 采用新型采集函数qPMHI（多点最大超体积改进概率）进行批量选择；3) qPMHI具有可加性分解特性，可通过简单的概率排序实现精确、可扩展的批量选择。

Result: 在合成基准测试和应用驱动任务中显著优于最先进的潜在空间和离散分子优化方法。在可持续能源存储案例研究中，快速发现了新颖、多样化且高性能的有机（醌基）阴极材料用于水系氧化还原液流电池应用。

Conclusion: 提出的模块化"生成-优化"框架为多目标分子设计提供了一种有效的替代方案，解决了现有方法的架构纠缠和可扩展性问题，在真实应用中展示了优越性能。

Abstract: Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular "generate-then-optimize" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.

</details>


### [7] [Imputation Uncertainty in Interpretable Machine Learning Methods](https://arxiv.org/abs/2512.17689)
*Pegah Golchian,Marvin N. Wright*

Main category: stat.ML

TL;DR: 比较不同插补方法对可解释机器学习方法置信区间覆盖率的影响，发现单一插补会低估方差，而多重插补在大多数情况下接近名义覆盖率


<details>
  <summary>Details</summary>
Motivation: 现实数据中经常出现缺失值，这会影响可解释机器学习方法的解释。现有研究关注偏差问题，显示不同插补方法可能导致模型解释不同，但忽略了额外的插补不确定性及其对方差和置信区间的影响。

Method: 比较不同插补方法对三种可解释机器学习方法置信区间覆盖率的影响：排列特征重要性、部分依赖图和Shapley值。研究单一插补和多重插补的效果。

Result: 单一插补会导致方差低估，在大多数情况下，只有多重插补能接近名义覆盖率。

Conclusion: 在处理缺失值时，需要考虑插补不确定性对可解释机器学习方法置信区间的影响，多重插补比单一插补更能提供准确的统计推断。

Abstract: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [8] [Do Generalized=Gamma Scale Mixtures of Normals Fit Large Image Data-Sets?](https://arxiv.org/abs/2512.17038)
*Brandon Marks,Yash Dave,Zixun Wang,Hannah Chung,Riya Patwa,Simon Cha,Michael Murphy,Alexander Strang*

Main category: stat.AP

TL;DR: 广义伽马尺度混合正态分布作为贝叶斯逆成像问题的先验模型，在多个大型成像数据集上首次验证了其现实性，相比高斯、拉普拉斯等标准先验具有显著更好的拟合效果。


<details>
  <summary>Details</summary>
Motivation: 广义伽马尺度混合正态分布作为参数先验在贝叶斯逆成像问题中具有吸引力，但需要验证其在真实成像数据上的现实性，并确定最适合的参数区域。

Method: 从遥感、医学成像和图像分类应用中收集大型成像数据集，研究该先验在傅里叶变换、小波变换（Haar和Gabor）以及AlexNet第一层卷积滤波器系数上的适用性，采用数据增强和改进可交换系数识别方法。

Result: 该先验模型在多个数据集上比包含的高斯、拉普拉斯、ℓ_p和学生t分布等标准先验提供显著更好的拟合效果，确定了比计算工作中主要关注区域更广泛的合适参数区域。

Conclusion: 广义伽马尺度混合正态分布是现实且有效的成像先验模型，但需要识别其不适用的情况，并理解导致模型拟合不佳的图像特征。

Abstract: A scale mixture of normals is a distribution formed by mixing a collection of normal distributions with fixed mean but different variances. A generalized gamma scale mixture draws the variances from a generalized gamma distribution. Generalized gamma scale mixtures of normals have been proposed as an attractive class of parametric priors for Bayesian inference in inverse imaging problems. Generalized gamma scale mixtures have two shape parameters, one that controls the behavior of the distribution about its mode, and the other that controls its tail decay. In this paper, we provide the first demonstration that the prior model is realistic for multiple large imaging data sets. We draw data from remote sensing, medical imaging, and image classification applications. We study the realism of the prior when applied to Fourier and wavelet (Haar and Gabor) transformations of the images, as well as to the coefficients produced by convolving the images against the filters used in the first layer of AlexNet, a popular convolutional neural network trained for image classification. We discuss data augmentation procedures that improve the fit of the model, procedures for identifying approximately exchangeable coefficients, and characterize the parameter regions that best describe the observed data sets. These regions are significantly broader than the region of primary focus in computational work. We show that this prior family provides a substantially better fit to each data set than any of the standard priors it contains. These include Gaussian, Laplace, $\ell_p$, and Student's $t$ priors. Finally, we identify cases where the prior is unrealistic and highlight characteristic features of images that suggest the model will fit poorly.

</details>


### [9] [Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models](https://arxiv.org/abs/2512.17119)
*Laura Pardo,Juan Sosa*

Main category: stat.AP

TL;DR: 该研究开发了一个贝叶斯分层框架来分析哥伦比亚2022年第二学期Saber 11考试学术表现，结合多层次回归与空间随机效应，使用Ridge和Lasso正则化先验比较社会人口协变量的贡献。


<details>
  <summary>Details</summary>
Motivation: 分析哥伦比亚教育表现的空间分布模式，识别影响学术成绩的关键社会人口因素，为教育政策的区域针对性提供依据。

Method: 采用贝叶斯分层框架，结合多层次回归与市、省级空间随机效应，使用Ridge和Lasso正则化先验，通过MCMC方法进行推断，并通过合成数据评估模型行为。

Result: Ridge在参数恢复、预测准确性和采样效率方面表现最均衡；Lasso拟合和稳定性较差，但在强多重共线性下预测准确性有提升。应用结果显示成绩高度集中化，中部省份分数较高，边缘地区较低。学生生活条件、母亲教育程度、教育资源获取、性别和种族背景是最强相关因素。

Conclusion: 研究揭示了与结构性不平等一致的多尺度地域模式，为教育政策的区域针对性提供了信息。基于K-means的混合贝叶斯分割将后验不确定性传播到聚类分析中，有助于理解教育表现的空间分布。

Abstract: This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy.

</details>


### [10] [Day-Ahead Electricity Price Forecasting Using Merit-Order Curves Time Series](https://arxiv.org/abs/2512.17758)
*Guillaume Koechlin,Filippo Bovera,Piercesare Secchi*

Main category: stat.AP

TL;DR: 提出基于供需曲线预测的电力市场价格预测框架，相比直接预测价格的方法，在点预测和概率预测方面均有显著改进


<details>
  <summary>Details</summary>
Motivation: 传统电力市场价格预测方法直接预测出清价格，但忽略了供需曲线的动态变化信息。本文旨在通过预测供需曲线来更准确地预测电力市场价格，特别是考虑可再生能源高渗透和需求低谷时的价格波动

Method: 使用函数主成分分析将供需曲线对表示为低维向量空间，然后采用正则化向量自回归模型进行预测，从预测的供需曲线推导出价格预测

Result: 在意大利日前市场2023-2024年数据上的实证比较显示，曲线基方法相比价格基方法平均提升约5%的预测精度，中午时段（可再生能源高发电、低需求导致价格下降时）提升可达10%

Conclusion: 通过预测供需曲线而非直接预测价格的方法，能够显著提高电力市场日前价格的预测精度，特别是在可再生能源渗透率高的时段，为电力市场参与者提供了更可靠的价格预测工具

Abstract: We introduce a general, simple, and computationally efficient framework for predicting day-ahead supply and demand merit-order curves, from which both point and probabilistic electricity price forecasts can be derived. Specifically, we leverage functional principal component analysis to efficiently represent a pair of supply and demand curves in a low-dimensional vector space and employ regularized vector autoregressive models for their prediction. We conduct a rigorous empirical comparison of price forecasting performance between the proposed curve-based model, i.e., derived from predicted merit-order curves, and state-of-the-art price-based models that directly forecast the clearing price, using data from the Italian day-ahead market over the 2023-2024 period. Our results show that the proposed curve-based approach significantly improves both point and probabilistic price forecasting accuracy relative to price-based approaches, with average gains of approximately 5%, and improvements of up to 10% during mid-day hours, when prices occasionally drop due to high renewable generation and low demand.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [11] [Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease](https://arxiv.org/abs/2512.17340)
*Carter H. Nakamoto,Lucia Lushi Chen,Agata Foryciarz,Sherri Rose*

Main category: stat.ME

TL;DR: 提出一个针对多群体公平回归的通用框架，通过不公平性惩罚和成本敏感分类实现，在慢性肾病预测中显著提升多种族群体的公平性而不损失整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有公平回归方法主要关注单一群体，缺乏处理多个群体同时存在社会偏见的解决方案，特别是在医疗保健领域需要同时考虑多个种族和族裔群体的公平性。

Method: 提出通用回归框架，包含多群体不公平性惩罚机制，针对二分类结果使用真阳性率差异惩罚，通过转化为成本敏感分类问题实现高效计算，并引入新的评分函数自动选择惩罚权重。

Result: 模拟实验显示该方法在公平性-准确性边界上优于现有方法；在慢性肾病终末期肾病预测应用中，显著改善了多个种族和族裔群体的公平性，且没有明显损失整体拟合效果。

Conclusion: 该方法为处理医疗保健中多群体社会偏见提供了有效的公平回归框架，能够在保持预测准确性的同时显著提升多个受偏见影响群体的公平性。

Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.

</details>


### [12] [A systematic assessment of Large Language Models for constructing two-level fractional factorial designs](https://arxiv.org/abs/2512.17113)
*Alan R. Vazquez,Kilian M. Rother,Marco V. Charles-Gonzalez*

Main category: stat.ME

TL;DR: 评估GPT和Gemini等大语言模型在构建8、16、32次试验的二水平部分因子设计方面的能力，发现它们能有效构建最多8个因子的最优设计。


<details>
  <summary>Details</summary>
Motivation: 传统上二水平部分因子设计通过标准教科书或统计软件获得，而现代大语言模型虽然能生成这类设计，但其质量尚未经过系统评估。

Method: 使用提示工程技术开发高质量的设计构建任务，系统评估GPT和Gemini模型构建8、16、32次试验、4-26个因子的二水平部分因子设计的能力。

Result: 大语言模型能有效构建最多8个因子的最优8、16、32次试验设计，但在更多因子情况下表现有限。

Conclusion: 大语言模型在构建中小规模的部分因子设计方面具有实用价值，但需要进一步改进以处理更复杂的设计需求。

Abstract: Two-level fractional factorial designs permit the study multiple factors using a limited number of runs. Traditionally, these designs are obtained from catalogs available in standard textbooks or statistical software. However, modern Large Language Models (LLMs) can now produce two-level fractional factorial designs, but the quality of these designs has not been previously assessed. In this paper, we perform a systematic evaluation of two popular classes of LLMs, namely GPT and Gemini models, to construct two-level fractional factorial designs with 8, 16, and 32 runs, and 4 to 26 factors. To this end, we use prompting techniques to develop a high-quality set of design construction tasks for the LLMs. We compare the designs obtained by the LLMs with the best-known designs in terms of resolution and minimum aberration criteria. We show that the LLMs can effectively construct optimal 8-, 16-, and 32-run designs with up to eight factors.

</details>


### [13] [A Synthetic Instrumental Variable Method: Using the Dual Tendency Condition for Coplanar Instruments](https://arxiv.org/abs/2512.17301)
*Ratbek Dzhumashev,Ainura Tursunalieva*

Main category: stat.ME

TL;DR: 提出合成工具变量(SIV)方法，仅用现有数据构建有效工具变量，无需外部数据，解决传统IV方法对弱/无效工具变量依赖的问题


<details>
  <summary>Details</summary>
Motivation: 传统工具变量方法存在两个主要问题：1) 对弱工具变量或无效工具变量敏感；2) 严重依赖外部数据。这些限制在实际应用中常常导致因果推断困难，特别是在经济学、流行病学等领域

Method: 提出合成工具变量(SIV)方法，基于数据驱动的双重倾向(DT)条件来识别有效工具变量。该方法仅使用现有数据，无需外部变量，能处理异方差性，并能确定内生回归变量与误差项之间相关性的真实符号

Result: 通过模拟实验和实际应用验证，SIV方法能改善因果推断效果，缓解传统IV方法的常见限制，减少对稀缺工具变量的依赖。该方法对异方差性具有鲁棒性

Conclusion: SIV方法为经济学、流行病学和政策评估等领域的因果推断提供了更稳健的工具，减少了对外部数据和强工具变量的依赖，具有广泛的应用前景

Abstract: Traditional instrumental variable (IV) methods often struggle with weak or invalid instruments and rely heavily on external data. We introduce a Synthetic Instrumental Variable (SIV) approach that constructs valid instruments using only existing data. Our method leverages a data-driven dual tendency (DT) condition to identify valid instruments without requiring external variables. SIV is robust to heteroscedasticity and can determine the true sign of the correlation between endogenous regressors and errors--an assumption typically imposed in empirical work. Through simulations and real-world applications, we show that SIV improves causal inference by mitigating common IV limitations and reducing dependence on scarce instruments. This approach has broad implications for economics, epidemiology, and policy evaluation.

</details>


### [14] [Estimation and model errors in Gaussian-process-based Sensitivity Analysis of functional outputs](https://arxiv.org/abs/2512.17635)
*Yuri Taglieri Sáo,Olivier Roustant,Geraldo de Freitas Maciel*

Main category: stat.ME

TL;DR: 提出一种基于高斯过程的全局敏感性分析高效算法，通过基展开减少计算维度，结合向量化Pick-Freeze估计，相比逐维处理提速15倍


<details>
  <summary>Details</summary>
Motivation: 功能输出模型的全局敏感性分析通常结合基展开、元建模和采样估计，存在元建模误差和采样估计误差。现有方法计算效率低，需要处理高维输出

Method: 利用基展开将高维输出转换为低维系数，用高斯过程拟合这些系数，采样多个条件GP轨迹，采用向量化Pick-Freeze估计加速Sobol指数和广义敏感性指数的计算

Result: 在分析测试案例和非牛顿流体力学（理想化溃坝流）应用中验证了方法有效性，相比Le Gratiet等人算法逐维处理，计算时间缩短15倍

Conclusion: 提出的算法能高效估计全局敏感性分析中的误差，通过基展开降维和向量化估计显著提升计算效率，适用于功能输出模型

Abstract: Global sensitivity analysis (GSA) of functional-output models is usually performed by combining statistical techniques, such as basis expansions, metamodeling and sampling based estimation of sensitivity indices. By neglecting truncation error from basis expansion, two main sources of errors propagate to the final sensitivity indices: the metamodeling related error and the sampling-based, or pick-freeze (PF), estimation error. This work provides an efficient algorithm to estimate these errors in the frame of Gaussian processes (GP), based on the approach of Le Gratiet et al. [16]. The proposed algorithm takes advantage of the fact that the number of basis coefficients of expanded model outputs is significantly smaller than output dimensions. Basis coefficients are fitted by GP models and multiple conditional GP trajectories are sampled. Then, vector-valued PF estimation is used to speed-up the estimation of Sobol indices and generalized sensitivity indices (GSI). We illustrate the methodology on an analytical test case and on an application in non-Newtonian hydraulics, modelling an idealized dam-break flow. Numerical tests show an improvement of 15 times in the computational time when compared to the application of Le Gratiet et al. [16] algorithm separately over each output dimension.

</details>


### [15] [A General Stability Approach to False Discovery Rate Control](https://arxiv.org/abs/2512.17401)
*Jiajun Sun,Zhanrui Cai,Wei Zhong*

Main category: stat.ME

TL;DR: 提出FDR Stabilizer方法，通过聚合多次运行基础FDR控制程序的特征重要性统计量，构建稳定的松弛e值，提高特征选择和多重检验中FDR控制的稳定性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 现有FDR控制方法（如Model-X knockoff和数据分割方法）由于算法固有的随机性，会产生不稳定的结果。在统计方法应用中，稳定性和可重复性至关重要，需要解决这一问题。

Method: 首先将基础FDR控制程序多次运行得到的特征重要性统计量聚合成共识排序，然后为每个特征构建稳定的松弛e值，最后应用e-BH程序到这些稳定的e值上得到最终选择集。

Result: 理论上推导了方法的FDR和功效的有限样本界，证明方法能渐近控制FDR且不损失功效。建立了方法的稳定性，显示随着重复次数增加，稳定选择集收敛到确定性极限。数值实验和真实数据集应用表明方法优于现有替代方案。

Conclusion: 提出的FDR Stabilizer方法能有效提高FDR控制的稳定性和可重复性，在保持统计性能的同时解决现有方法因随机性导致的结果不稳定问题。

Abstract: Stability and reproducibility are essential considerations in various applications of statistical methods. False Discovery Rate (FDR) control methods are able to control false signals in scientific discoveries. However, many FDR control methods, such as Model-X knockoff and data-splitting approaches, yield unstable results due to the inherent randomness of the algorithms. To enhance the stability and reproducibility of statistical outcomes, we propose a general stability approach for FDR control in feature selection and multiple testing problems, named FDR Stabilizer. Taking feature selection as an example, our method first aggregates feature importance statistics obtained by multiple runs of the base FDR control procedure into a consensus ranking. Then, we construct a stabilized relaxed e-value for each feature and apply the e-BH procedure to these stabilized e-values to obtain the final selection set. We theoretically derive the finite-sample bounds for the FDR and the power of our method, and show that our method asymptotically controls the FDR without power loss. Moreover, we establish the stability of the proposed method, showing that the stabilized selection set converges to a deterministic limit as the number of repetitions increases. Extensive numerical experiments and applications to real datasets demonstrate that the proposed method generally outperforms existing alternatives.

</details>


### [16] [Bayesian Markov-Switching Partial Reduced-Rank Regression](https://arxiv.org/abs/2512.17471)
*Maria F. Pintado,Matteo Iacopini,Luca Rossini,Alexander Y. Shestopaloff*

Main category: stat.ME

TL;DR: 提出贝叶斯马尔可夫切换部分降秩回归模型，通过马尔可夫过程捕捉响应变量分组结构和模型参数随时间变化，同时估计未知的分组分配和秩参数。


<details>
  <summary>Details</summary>
Motivation: 传统降秩回归假设整个系数矩阵具有低秩结构，忽略了响应变量可能存在的分组配置，且时间序列中预测变量与响应变量关系随时间变化，可能产生时变的分组结构。

Method: 提出贝叶斯马尔可夫切换部分降秩回归模型，将响应向量分为简单组（低秩线性回归）和复杂组（高斯过程非参数回归），分组分配和秩作为未知参数进行估计，通过马尔可夫切换过程捕捉分组结构和参数随时间变化。

Result: 在宏观经济和大宗商品数据应用中，模型显示出时变分组结构以及跨状态和状态内不同程度的复杂性证据。

Conclusion: MS-PRR模型能够有效捕捉时间序列中响应变量关系的时变分组结构和复杂性差异，为不确定性量化提供了完整贝叶斯推断框架。

Abstract: Reduced-Rank (RR) regression is a powerful dimensionality reduction technique but it overlooks any possible group configuration among the responses by assuming a low-rank structure on the entire coefficient matrix. Moreover, the temporal change of the relations between predictors and responses in time series induce a possibly time-varying grouping structure in the responses. To address these limitations, a Bayesian Markov-switching partial RR (MS-PRR) model is proposed, where the response vector is partitioned in two groups to reflect different complexity of the relationship. A \textit{simple} group assumes a low-rank linear regression, while a \textit{complex} group exploits nonparametric regression via a Gaussian Process. Differently from traditional approaches, group assignments and rank are treated as unknown parameters to be estimated. Then temporal persistence in the regression function is accounted for by a Markov-switching process that drives the changes in the grouping structure and model parameters over time. Full Bayesian inference is preformed via a partially collapsed Gibbs sampler, which allows uncertainty quantification without the need for trans-dimensional moves. Applications to two real-world macroeconomic and commodity data demonstrate the evidence of time-varying grouping and different degrees of complexity both across states and within each state.

</details>


### [17] [Inference on state occupancy in covariate-driven hidden Markov models](https://arxiv.org/abs/2512.17496)
*Maya N. Vienken,Jan-Ole Koslik,Roland Langrock*

Main category: stat.ME

TL;DR: 针对动物行为研究中隐马尔可夫模型(HMM)的状态占用分布估计问题，本文指出传统静态分布近似在协变量变化较快时存在严重偏差，提出了两种新方法：基于协变量过程重采样的方法和基于经验状态概率回归分析的方法。


<details>
  <summary>Details</summary>
Motivation: 在动物行为研究中，隐马尔可夫模型常用于分析动物决策过程与内外驱动因素的关系。当协变量随机变化且持续性较低时，传统基于固定协变量值的静态分布近似会产生严重偏差，可能使生态推断无效。需要开发更准确的状态占用分布估计方法。

Method: 提出了两种新方法：1）基于协变量过程重采样的方法，通过重采样技术考虑协变量的随机变化；2）基于经验状态概率回归分析的方法，通过回归分析直接估计状态占用分布与协变量的关系。在模拟数据和加拉帕戈斯象龟运动数据中验证了方法的实用性。

Result: 研究表明传统静态分布近似在协变量变化较快时存在严重偏差。新提出的两种方法能够提供无偏的状态占用分布估计，在模拟和实际案例中都表现出更好的性能，能够更准确地揭示动物行为与协变量之间的关系。

Conclusion: 本文开发的方法使研究者能够对动物行为与各种类型协变量之间的关系进行无偏推断，有助于揭示影响动物行为决策的因素。这些方法解决了传统HMM分析中的关键局限性，为生态行为研究提供了更可靠的分析工具。

Abstract: Hidden Markov models (HMMs) are popular tools for analysing animal behaviour based on movement, acceleration and other sensor data. In particular, these models allow to infer how the animal's decision-making process interacts with internal and external drivers, by relating the probabilities of switching between distinct behavioural states to covariates. A key challenge arising in the statistical analysis of behavioural data using covariate-driven HMMs is the models' interpretation, especially when there are more than two states, as then several functional relationships between state-switching probabilities and covariates need to be jointly interpreted. The model-implied probabilities of occupying the different states, as a function of a covariate of interest, constitute a much simpler summary statistic. A pragmatic approximation of the state occupancy distribution, namely the hypothetical stationary distribution of the model's underlying Markov chain for fixed covariate values, has in fact routinely been reported in HMM-based analyses of ecological data. However, for stochastically varying covariates with relatively little persistence, we show that this approximation can be severely biased, potentially invalidating ecological inference. We develop two alternative approaches for obtaining the state occupancy distribution as a function of a covariate of interest - one based on resampling of the covariate process, the other obtained by regression analysis of the empirical state probabilities. The practical application of these approaches is demonstrated in simulations and a case study on Galápagos tortoise (Chelonoidis niger) movement data. Our methods enable practitioners to conduct unbiased inference on the relationship between animal behaviour and general types of covariates, thus allowing to uncover the factors influencing behavioural decisions made by animals.

</details>


### [18] [A Dependent Feature Allocation Model Based on Random Fields](https://arxiv.org/abs/2512.17701)
*Bernardo Flores,Yang Ni,Yanxun Xu,Peter Müller*

Main category: stat.ME

TL;DR: 提出一个用于建模依赖特征分配的灵活框架，通过直接建模特征paintbox的logit概率表面，结合协变量和复杂依赖结构，使用GMRF分解潜在场，并扩展到动态时空过程。


<details>
  <summary>Details</summary>
Motivation: 传统非参数方法在建模依赖特征分配方面存在局限性，无法有效纳入协变量和复杂依赖结构。需要一种能够直接建模特征分配概率表面、结合协变量信息并处理高维数据的灵活框架。

Method: 使用高斯马尔可夫随机场(GMRF)作为核心，分解潜在场为基于基线协变量的结构成分和内在非结构化异质性。采用从数据潜在几何导出的稀疏k近邻图而非刚性网格，确保高维可处理性。扩展到动态时空过程，通过Ornstein-Uhlenbeck过程使项目效应演化。使用低秩分解捕获特征相关性。

Result: 将模型应用于多药使用数据集，成功从患者药物配置文件中推断潜在健康状况，展示了模型的实际效用。

Conclusion: 提出的框架为建模依赖特征分配提供了灵活且可扩展的方法，能够有效结合协变量信息、处理复杂依赖结构，并在高维设置中保持可处理性，在医疗数据分析等实际应用中表现出良好性能。

Abstract: We introduce a flexible framework for modeling dependent feature allocations. Our approach addresses limitations in traditional nonparametric methods by directly modeling the logit-probability surface of the feature paintbox, enabling the explicit incorporation of covariates and complex but tractable dependence structures. The core of our model is a Gaussian Markov Random Field (GMRF), which we use to robustly decompose the latent field, separating a structural component based on the baseline covariates from intrinsic, unstructured heterogeneity. This structure is not a rigid grid but a sparse k-nearest neighbors graph derived from the latent geometry in the data, ensuring high-dimensional tractability. We extend this framework to a dynamic spatio-temporal process, allowing item effects to evolve via an Ornstein-Uhlenbeck process. Feature correlations are captured using a low-rank factorization of their joint prior. We demonstrate our model's utility by applying it to a polypharmacy dataset, successfully inferring latent health conditions from patient drug profiles.

</details>


### [19] [Recursive state estimation via approximate modal paths](https://arxiv.org/abs/2512.17737)
*Filip Tronarp*

Main category: stat.ME

TL;DR: 提出一种递归计算近似模态路径的方法，结合前向和后向动态规划形成"双滤波"公式，通过二次近似值函数解决计算难题，性能优于现代算法


<details>
  <summary>Details</summary>
Motivation: 模态路径计算在一般情况下的值函数递归是难以处理的，需要开发有效的近似方法来克服这一计算难题

Method: 结合前向和后向动态规划形成"双滤波"公式，在前向动态规划框架中对值函数进行二次近似，从而得到滤波和平滑方法

Result: 仿真实验验证了该方法的有效性，性能达到或优于其他现代算法

Conclusion: 提出的递归近似模态路径计算方法通过二次近似值函数解决了计算难题，在性能上具有竞争力

Abstract: In this paper, a method for recursively computing approximate modal paths is developed. A recursive formulation of the modal path can be obtained either by backward or forward dynamic programming. By combining both methods, a ``two-filter'' formula is demonstrated. Both method involves a recursion over a so-called value function, which is intractable in general. This problem is overcome by quadratic approximation of the value function in the forward dynamic programming paradigm, resulting in both a filtering and smoothing method. The merit of the approach is verified in a simulation experiments, where it is shown to be on par or better than other modern algorithms.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [20] [Inference for high dimensional repeated measure designs with the R package hdrm](https://arxiv.org/abs/2512.17478)
*Paavo Sattler,Nils Hichert*

Main category: stat.CO

TL;DR: 介绍高维重复测量设计的R包hdrm，提供参数化方法分析高维数据，无需对协方差结构或d与N的渐近关系做限制性假设


<details>
  <summary>Details</summary>
Motivation: 现代数据收集常产生维度d接近或超过样本量N的观测向量，这带来了统计推断的挑战。重复测量设计在医学、心理学等领域广泛应用，但高维数据下的分析方法有限。

Method: 基于多元正态性的参数化方法，避免对协方差结构或d与N渐近关系的限制性假设。R包hdrm实现了多种假设检验，包括单组和多组设置，支持同质或异质协方差矩阵。

Result: hdrm包提供了分析高维重复测量设计的工具，通过高效估计器和子采样策略解决大d带来的计算挑战，显著减少计算时间同时保持统计有效性。

Conclusion: hdrm包为高维重复测量设计提供了一个灵活实用的分析框架，适用于实际的高维数据场景，解决了传统方法在高维情况下的局限性。

Abstract: Repeated-measure designs allow comparisons within a group as well as between groups, and are commonly referred to as split-plot designs. While originating in agricultural experiments, they are now widely used in medical research, psychology, and the life sciences, where repeated observations on the same subject are essential.
  Modern data collection often produces observation vectors with dimension $d$ comparable to or exceeding the sample size $N$. Although this can be advantageous in terms of cost efficiency, ethical considerations, and the study of rare diseases, it poses substantial challenges for statistical inference.
  Parametric methods based on multivariate normality provide a flexible framework that avoids restrictive assumptions on covariance structures or on the asymptotic relationship between $d$ and $N$. Within this framework, the freely available R-package hdrm enables the analysis of a wide range of hypotheses concerning expectation vectors in high-dimensional repeated-measure designs, covering both single-group and multi-group settings with homogeneous or heterogeneous covariance matrices.
  This paper describes the implemented tests, demonstrates their use through examples, and discusses their applicability in practical high-dimensional data scenarios. To address computational challenges arising for large $d$, the package incorporates efficient estimators and subsampling strategies that substantially reduce computation time while preserving statistical validity.

</details>


### [21] [Delayed Acceptance Slice Sampling](https://arxiv.org/abs/2512.17868)
*Kevin Bitterlich,Daniel Rudolf,Björn Sprungk*

Main category: stat.CO

TL;DR: 提出了一种基于延迟接受的混合切片采样方法，利用廉价的目标密度确定性近似来减少计算成本，特别适用于贝叶斯推断中似然函数计算昂贵的情况。


<details>
  <summary>Details</summary>
Motivation: 切片采样是MCMC中常用的采样方法，但通常需要多次评估目标密度，计算成本较高，特别是在贝叶斯推断中似然函数计算昂贵的情况下。需要一种方法来减少计算开销。

Method: 提出了延迟接受的混合切片采样方法，利用相对廉价的目标密度确定性近似来预筛选候选状态，减少对昂贵目标密度的评估次数。

Result: 证明了所提方法的遍历性，展示了延迟接受切片采样相对于延迟接受Metropolis-Hastings算法的优越性，并通过数值实验验证了计算效率的提升。

Conclusion: 延迟接受切片采样方法能够有效减少计算成本，特别适用于目标密度评估昂贵的场景，为贝叶斯推断等应用提供了更高效的采样工具。

Abstract: Slice sampling is a well-established Markov chain Monte Carlo method for (approximate) sampling of target distributions which are only known up to a normalizing constant. The method is based on choosing a new state on a slice, i.e., a superlevel set of the given unnormalized target density (with respect to a reference measure). However, slice sampling algorithms usually require per step multiple evaluations of the target density, and thus can become computationally expensive. This is particularly the case for Bayesian inference with costly likelihoods. In this paper, we exploit deterministic approximations of the target density, which are relatively cheap to evaluate, and propose delayed acceptance versions of hybrid slice samplers. We show ergodicity of the resulting slice sampling methods, discuss the superiority of delayed acceptance (ideal) slice sampling over delayed acceptance Metropolis-Hastings algorithms, and illustrate the benefits of our novel approach in terms improved computational efficiency in several numerical experiments.

</details>
