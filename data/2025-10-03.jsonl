{"id": "2510.02119", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02119", "abs": "https://arxiv.org/abs/2510.02119", "authors": ["Lucas Morisset", "Adrien Hardy", "Alain Durmus"], "title": "Non-Asymptotic Analysis of Data Augmentation for Precision Matrix Estimation", "comment": "Conference paper at NeurIPS 2025 (Spotlight)", "summary": "This paper addresses the problem of inverse covariance (also known as\nprecision matrix) estimation in high-dimensional settings. Specifically, we\nfocus on two classes of estimators: linear shrinkage estimators with a target\nproportional to the identity matrix, and estimators derived from data\naugmentation (DA). Here, DA refers to the common practice of enriching a\ndataset with artificial samples--typically generated via a generative model or\nthrough random transformations of the original data--prior to model fitting.\nFor both classes of estimators, we derive estimators and provide concentration\nbounds for their quadratic error. This allows for both method comparison and\nhyperparameter tuning, such as selecting the optimal proportion of artificial\nsamples. On the technical side, our analysis relies on tools from random matrix\ntheory. We introduce a novel deterministic equivalent for generalized resolvent\nmatrices, accommodating dependent samples with specific structure. We support\nour theoretical results with numerical experiments."}
{"id": "2510.01291", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01291", "abs": "https://arxiv.org/abs/2510.01291", "authors": ["Bo Li", "Wei Wang", "Peng Ye"], "title": "Private Realizable-to-Agnostic Transformation with Near-Optimal Sample Complexity", "comment": null, "summary": "The realizable-to-agnostic transformation (Beimel et al., 2015; Alon et al.,\n2020) provides a general mechanism to convert a private learner in the\nrealizable setting (where the examples are labeled by some function in the\nconcept class) to a private learner in the agnostic setting (where no\nassumptions are imposed on the data). Specifically, for any concept class\n$\\mathcal{C}$ and error parameter $\\alpha$, a private realizable learner for\n$\\mathcal{C}$ can be transformed into a private agnostic learner while only\nincreasing the sample complexity by\n$\\widetilde{O}(\\mathrm{VC}(\\mathcal{C})/\\alpha^2)$, which is essentially tight\nassuming a constant privacy parameter $\\varepsilon = \\Theta(1)$. However, when\n$\\varepsilon$ can be arbitrary, one has to apply the standard\nprivacy-amplification-by-subsampling technique (Kasiviswanathan et al., 2011),\nresulting in a suboptimal extra sample complexity of\n$\\widetilde{O}(\\mathrm{VC}(\\mathcal{C})/\\alpha^2\\varepsilon)$ that involves a\n$1/\\varepsilon$ factor.\n  In this work, we give an improved construction that eliminates the dependence\non $\\varepsilon$, thereby achieving a near-optimal extra sample complexity of\n$\\widetilde{O}(\\mathrm{VC}(\\mathcal{C})/\\alpha^2)$ for any $\\varepsilon\\le 1$.\nMoreover, our result reveals that in private agnostic learning, the privacy\ncost is only significant for the realizable part. We also leverage our\ntechnique to obtain a nearly tight sample complexity bound for the private\nprediction problem, resolving an open question posed by Dwork and Feldman\n(2018) and Dagan and Feldman (2020)."}
{"id": "2510.01329", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01329", "abs": "https://arxiv.org/abs/2510.01329", "authors": ["Huangjie Zheng", "Shansan Gong", "Ruixiang Zhang", "Tianrong Chen", "Jiatao Gu", "Mingyuan Zhou", "Navdeep Jaitly", "Yizhe Zhang"], "title": "Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling", "comment": null, "summary": "Standard discrete diffusion models treat all unobserved states identically by\nmapping them to an absorbing [MASK] token. This creates an 'information void'\nwhere semantic information that could be inferred from unmasked tokens is lost\nbetween denoising steps. We introduce Continuously Augmented Discrete Diffusion\n(CADD), a framework that augments the discrete state space with a paired\ndiffusion in a continuous latent space. This yields graded, gradually corrupted\nstates in which masked tokens are represented by noisy yet informative latent\nvectors rather than collapsed 'information voids'. At each reverse step, CADD\nmay leverage the continuous latent as a semantic hint to guide discrete\ndenoising. The design is clean and compatible with existing discrete diffusion\ntraining. At sampling time, the strength and choice of estimator for the\ncontinuous latent vector enables a controlled trade-off between mode-coverage\n(generating diverse outputs) and mode-seeking (generating contextually precise\noutputs) behaviors. Empirically, we demonstrate CADD improves generative\nquality over mask-based diffusion across text generation, image synthesis, and\ncode modeling, with consistent gains on both qualitative and quantitative\nmetrics against strong discrete baselines."}
{"id": "2510.01414", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01414", "abs": "https://arxiv.org/abs/2510.01414", "authors": ["Jiping Li", "Rishi Sonthalia"], "title": "Risk Phase Transitions in Spiked Regression: Alignment Driven Benign and Catastrophic Overfitting", "comment": null, "summary": "This paper analyzes the generalization error of minimum-norm interpolating\nsolutions in linear regression using spiked covariance data models. The paper\ncharacterizes how varying spike strengths and target-spike alignments can\naffect risk, especially in overparameterized settings. The study presents an\nexact expression for the generalization error, leading to a comprehensive\nclassification of benign, tempered, and catastrophic overfitting regimes based\non spike strength, the aspect ratio $c=d/n$ (particularly as $c \\to \\infty$),\nand target alignment. Notably, in well-specified aligned problems, increasing\nspike strength can surprisingly induce catastrophic overfitting before\nachieving benign overfitting. The paper also reveals that target-spike\nalignment is not always advantageous, identifying specific, sometimes\ncounterintuitive, conditions for its benefit or detriment. Alignment with the\nspike being detrimental is empirically demonstrated to persist in nonlinear\nmodels."}
{"id": "2510.01560", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01560", "abs": "https://arxiv.org/abs/2510.01560", "authors": ["Lang Tong", "Xinyi Wang"], "title": "AI Foundation Model for Time Series with Innovations Representation", "comment": null, "summary": "This paper introduces an Artificial Intelligence (AI) foundation model for\ntime series in engineering applications, where causal operations are required\nfor real-time monitoring and control. Since engineering time series are\ngoverned by physical, rather than linguistic, laws, large-language-model-based\nAI foundation models may be ineffective or inefficient. Building on the\nclassical innovations representation theory of Wiener, Kallianpur, and\nRosenblatt, we propose Time Series GPT (TS-GPT) -- an\ninnovations-representation-based Generative Pre-trained Transformer for\nengineering monitoring and control. As an example of foundation model\nadaptation, we consider Probabilistic Generative Forecasting, which produces\nfuture time series samples from conditional probability distributions given\npast realizations. We demonstrate the effectiveness of TS-GPT in forecasting\nreal-time locational marginal prices using historical data from U.S.\nindependent system operators."}
{"id": "2510.01190", "categories": ["stat.CO", "cs.HC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01190", "abs": "https://arxiv.org/abs/2510.01190", "authors": ["Timbwaoga A. J. Ouermi", "Eric Li", "Kenneth Moreland", "Dave Pugmire", "Chris R. Johnson", "Tushar M. Athawale"], "title": "Efficient Probabilistic Visualization of Local Divergence of 2D Vector Fields with Independent Gaussian Uncertainty", "comment": null, "summary": "This work focuses on visualizing uncertainty of local divergence of\ntwo-dimensional vector fields. Divergence is one of the fundamental attributes\nof fluid flows, as it can help domain scientists analyze potential positions of\nsources (positive divergence) and sinks (negative divergence) in the flow.\nHowever, uncertainty inherent in vector field data can lead to erroneous\ndivergence computations, adversely impacting downstream analysis. While Monte\nCarlo (MC) sampling is a classical approach for estimating divergence\nuncertainty, it suffers from slow convergence and poor scalability with\nincreasing data size and sample counts. Thus, we present a two-fold\ncontribution that tackles the challenges of slow convergence and limited\nscalability of the MC approach. (1) We derive a closed-form approach for highly\nefficient and accurate uncertainty visualization of local divergence, assuming\nindependently Gaussian-distributed vector uncertainties. (2) We further\nintegrate our approach into Viskores, a platform-portable parallel library, to\naccelerate uncertainty visualization. In our results, we demonstrate\nsignificantly enhanced efficiency and accuracy of our serial analytical\n(speed-up up to 1946X) and parallel Viskores (speed-up up to 19698X) algorithms\nover the classical serial MC approach. We also demonstrate qualitative\nimprovements of our probabilistic divergence visualizations over traditional\nmean-field visualization, which disregards uncertainty. We validate the\naccuracy and efficiency of our methods on wind forecast and ocean simulation\ndatasets."}
{"id": "2510.01418", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01418", "abs": "https://arxiv.org/abs/2510.01418", "authors": ["Heng Ge", "Qing Lu"], "title": "DiffKnock: Diffusion-based Knockoff Statistics for Neural Networks Inference", "comment": null, "summary": "We introduce DiffKnock, a diffusion-based knockoff framework for\nhigh-dimensional feature selection with finite-sample false discovery rate\n(FDR) control. DiffKnock addresses two key limitations of existing knockoff\nmethods: preserving complex feature dependencies and detecting non-linear\nassociations. Our approach trains diffusion models to generate valid knockoffs\nand uses neural network--based gradient and filter statistics to construct\nantisymmetric feature importance measures. Through simulations, we showed that\nDiffKnock achieved higher power than autoencoder-based knockoffs while\nmaintaining target FDR, indicating its superior performance in scenarios\ninvolving complex non-linear architectures. Applied to murine single-cell\nRNA-seq data of LPS-stimulated macrophages, DiffKnock identifies canonical\nNF-$\\kappa$B target genes (Ccl3, Hmox1) and regulators (Fosb, Pdgfb). These\nresults highlight that, by combining the flexibility of deep generative models\nwith rigorous statistical guarantees, DiffKnock is a powerful and reliable tool\nfor analyzing single-cell RNA-seq data, as well as high-dimensional and\nstructured data in other domains."}
{"id": "2510.01267", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.01267", "abs": "https://arxiv.org/abs/2510.01267", "authors": ["Varun Vishwanathan Nair", "Victor Miranda Soberanis"], "title": "Lung Cancer Survival Prediction Using Machine Learning and Statistical Methods", "comment": "12 pages, 5 figures", "summary": "Lung cancer remains one of the leading causes of cancer-related mortality,\nyet most survival models rely only on baseline factors and overlook\nposttreatment variables that reflect disease progression. To address this gap,\nwe applied Cox Proportional Hazards and Random Survival Forests, integrating\nbaseline features with post-treatment predictors such as progression-free\ninterval (PFI.time) and residual tumor status. The Cox model achieved a\nconcordance index (C-index) of 0.90, while the RSF model reached 0.86, both\noutperforming previous studies. Beyond statistical gains, the integration of\npost-treatment variables provides oncologists with more clinically meaningful\nand reliable survival estimates. This enables improved treatment planning, more\npersonalized patient counseling, and better-informed follow-up strategies. From\na practical standpoint, these results demonstrate how routinely collected\nclinical variables can be transformed into actionable survival predictions."}
{"id": "2510.01840", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01840", "abs": "https://arxiv.org/abs/2510.01840", "authors": ["Raphaël Carpintero Perez", "Sébastien Da Veiga", "Josselin Garnier"], "title": "A reproducible comparative study of categorical kernels for Gaussian process regression, with new clustering-based nested kernels", "comment": null, "summary": "Designing categorical kernels is a major challenge for Gaussian process\nregression with continuous and categorical inputs. Despite previous studies, it\nis difficult to identify a preferred method, either because the evaluation\nmetrics, the optimization procedure, or the datasets change depending on the\nstudy. In particular, reproducible code is rarely available. The aim of this\npaper is to provide a reproducible comparative study of all existing\ncategorical kernels on many of the test cases investigated so far. We also\npropose new evaluation metrics inspired by the optimization community, which\nprovide quantitative rankings of the methods across several tasks. From our\nresults on datasets which exhibit a group structure on the levels of\ncategorical inputs, it appears that nested kernels methods clearly outperform\nall competitors. When the group structure is unknown or when there is no prior\nknowledge of such a structure, we propose a new clustering-based strategy using\ntarget encodings of categorical variables. We show that on a large panel of\ndatasets, which do not necessarily have a known group structure, this\nestimation strategy still outperforms other approaches while maintaining low\ncomputational cost."}
{"id": "2510.01901", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01901", "abs": "https://arxiv.org/abs/2510.01901", "authors": ["Joshua J Bon", "Anthony Lee"], "title": "Knots and variance ordering of sequential Monte Carlo algorithms", "comment": "Working paper in advance of journal submission. 49 pages, 5 figures", "summary": "Sequential Monte Carlo algorithms, or particle filters, are widely used for\napproximating intractable integrals, particularly those arising in Bayesian\ninference and state-space models. We introduce a new variance reduction\ntechnique, the knot operator, which improves the efficiency of particle filters\nby incorporating potential function information into part, or all, of a\ntransition kernel. The knot operator induces a partial ordering of Feynman-Kac\nmodels that implies an order on the asymptotic variance of particle filters,\noffering a new approach to algorithm design. We discuss connections to existing\nstrategies for designing efficient particle filters, including model\nmarginalisation. Our theory generalises such techniques and provides\nquantitative asymptotic variance ordering results. We revisit the fully-adapted\n(auxiliary) particle filter using our theory of knots to show how a small\nmodification guarantees an asymptotic variance ordering for all relevant test\nfunctions."}
{"id": "2510.01468", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01468", "abs": "https://arxiv.org/abs/2510.01468", "authors": ["Xiaotian Hou", "Peng Wang", "Minge Xie", "Linjun Zhang"], "title": "Repro Samples Method for Model-Free Inference in High-Dimensional Binary Classification", "comment": "arXiv admin note: substantial text overlap with arXiv:2403.09984", "summary": "This paper presents a novel method for statistical inference in\nhigh-dimensional binary models with unspecified structure, where we leverage a\n(potentially misspecified) sparsity-constrained working generalized linear\nmodel (GLM) to facilitate the inference process. Our method is based on the\nrepro samples framework, which generates artificial samples that mimic the\nactual data-generating process. Our inference targets include the model\nsupport, case probabilities, and the oracle regression coefficients defined in\nthe working GLM. The proposed method has three major advantages. First, this\napproach is model-free, that is, it does not rely on specific model assumptions\nsuch as logistic or probit regression, nor does it require sparsity assumptions\non the underlying model. Second, for model support, we construct a model\ncandidate set for the most influential covariates that achieves guaranteed\ncoverage under a weak signal strength assumption. Third, for oracle regression\ncoefficients, we establish confidence sets for any group of linear combinations\nof regression coefficients. Simulation results demonstrate that the proposed\nmethod produces valid and small model candidate sets. It also achieves better\ncoverage for regression coefficients than the state-of-the-art debiasing\nmethods when the working model is the actual model that generates the sample\ndata. Additionally, we analyze single-cell RNA-seq data on the immune response.\nBesides identifying genes previously proven as relevant in the literature, our\nmethod also discovers a significant gene that has not been studied before,\nrevealing a potential new direction in understanding cellular immune response\nmechanisms."}
{"id": "2510.01426", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.01426", "abs": "https://arxiv.org/abs/2510.01426", "authors": ["Heng Ge", "Qing Lu"], "title": "Neural Tangent Kernels for Complex Genetic Risk Prediction: Bridging Deep Learning and Kernel Methods in Genomics", "comment": null, "summary": "Given the complexity of genetic risk prediction, there is a critical need for\nthe development of novel methodologies that can effectively capture intricate\ngenotype--phenotype relationships (e.g., nonlinear) while remaining\nstatistically interpretable and computationally tractable. We develop a Neural\nTangent Kernel (NTK) framework to integrate kernel methods into deep neural\nnetworks for genetic risk prediction analysis. We consider two approaches:\nNTK-LMM, which embeds the empirical NTK in a linear mixed model with variance\ncomponents estimated via minimum quadratic unbiased estimator (MINQUE), and\nNTK-KRR, which performs kernel ridge regression with cross-validated\nregularization. Through simulation studies, we show that NTK-based models\noutperform the traditional neural network models and linear mixed models. By\napplying NTK to endophenotypes (e.g., hippocampal volume) and AD-related genes\n(e.g., APOE) from Alzheimer's Disease Neuroimaging Initiative (ADNI), we found\nthat NTK achieved higher accuracy than existing methods for hippocampal volume\nand entorhinal cortex thickness. In addition to its accuracy performance, NTK\nhas favorable optimization properties (i.e., having a closed-form or convex\ntraining) and generates interpretable results due to its connection to variance\ncomponents and heritability. Overall, our results indicate that by integrating\nthe strengths of both deep neural networks and kernel methods, NTK offers\ncompetitive performance for genetic risk prediction analysis while having the\nadvantages of interpretability and computational efficiency."}
{"id": "2510.01874", "categories": ["stat.ML", "cs.LG", "68T07", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.01874", "abs": "https://arxiv.org/abs/2510.01874", "authors": ["Matteo Maggiolo", "Giuseppe Nuti", "Miroslav Štrupl", "Oleg Szehr"], "title": "Deep Hedging Under Non-Convexity: Limitations and a Case for AlphaZero", "comment": "15 pages in main text + 18 pages of references and appendices", "summary": "This paper examines replication portfolio construction in incomplete markets\n- a key problem in financial engineering with applications in pricing, hedging,\nbalance sheet management, and energy storage planning. We model this as a\ntwo-player game between an investor and the market, where the investor makes\nstrategic bets on future states while the market reveals outcomes. Inspired by\nthe success of Monte Carlo Tree Search in stochastic games, we introduce an\nAlphaZero-based system and compare its performance to deep hedging - a widely\nused industry method based on gradient descent. Through theoretical analysis\nand experiments, we show that deep hedging struggles in environments where the\n$Q$-function is not subject to convexity constraints - such as those involving\nnon-convex transaction costs, capital constraints, or regulatory limitations -\nconverging to local optima. We construct specific market environments to\nhighlight these limitations and demonstrate that AlphaZero consistently finds\nnear-optimal replication strategies. On the theoretical side, we establish a\nconnection between deep hedging and convex optimization, suggesting that its\neffectiveness is contingent on convexity assumptions. Our experiments further\nsuggest that AlphaZero is more sample-efficient - an important advantage in\ndata-scarce, overfitting-prone derivative markets."}
{"id": "2510.01771", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01771", "abs": "https://arxiv.org/abs/2510.01771", "authors": ["Jianwei Shi", "Sameh Abdulah", "Ying Sun", "Marc G. Genton"], "title": "Scalable Asynchronous Federated Modeling for Spatial Data", "comment": null, "summary": "Spatial data are central to applications such as environmental monitoring and\nurban planning, but are often distributed across devices where privacy and\ncommunication constraints limit direct sharing. Federated modeling offers a\npractical solution that preserves data privacy while enabling global modeling\nacross distributed data sources. For instance, environmental sensor networks\nare privacy- and bandwidth-constrained, motivating federated spatial modeling\nthat shares only privacy-preserving summaries to produce timely,\nhigh-resolution pollution maps without centralizing raw data. However, existing\nfederated modeling approaches either ignore spatial dependence or rely on\nsynchronous updates that suffer from stragglers in heterogeneous environments.\nThis work proposes an asynchronous federated modeling framework for spatial\ndata based on low-rank Gaussian process approximations. The method employs\nblock-wise optimization and introduces strategies for gradient correction,\nadaptive aggregation, and stabilized updates. We establish linear convergence\nwith explicit dependence on staleness, a result of standalone theoretical\nsignificance. Moreover, numerical experiments demonstrate that the asynchronous\nalgorithm achieves synchronous performance under balanced resource allocation\nand significantly outperforms it in heterogeneous settings, showcasing superior\nrobustness and scalability."}
{"id": "2510.01577", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01577", "abs": "https://arxiv.org/abs/2510.01577", "authors": ["Xinran Miao", "Jiwei Zhao", "Hyunseung Kang"], "title": "SLOPE and Designing Robust Studies for Generalization", "comment": null, "summary": "A popular task in generalization is to learn about a new, target population\nbased on data from an existing, source population. This task relies on\nconditional exchangeability, which asserts that differences between the source\nand target populations are fully captured by observable characteristics of the\ntwo populations. Unfortunately, this assumption is often untenable in practice\ndue to unobservable differences between the source and target populations.\nWorse, the assumption cannot be verified with data, warranting the need for\nrobust data collection processes and study designs that are inherently less\nsensitive to violation of the assumption. In this paper, we propose SLOPE\n(Sensitivity of LOcal Perturbations from Exchangeability), a simple, intuitive,\nand novel measure that quantifies the sensitivity to local violation of\nconditional exchangeability. SLOPE combines ideas from sensitivity analysis in\ncausal inference and derivative-based measure of robustness from Hampel (1974).\nAmong other properties, SLOPE can help investigators to choose (a) a robust\nsource or target population or (b) a robust estimand. Also, we show an analytic\nrelationship between SLOPE and influence functions (IFs), which investigators\ncan use to derive SLOPE given an IF. We conclude with a re-analysis of a\nmulti-national randomized experiment and illustrate the role of SLOPE in\ninforming robust study designs for generalization."}
{"id": "2510.01803", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.01803", "abs": "https://arxiv.org/abs/2510.01803", "authors": ["Mattia Stival", "Angela Andreella", "Gaia Bertarelli", "Catarina Midões", "Stefano Federico Tonellato", "Enrica De Cian", "Stefano Campostrini"], "title": "The Perceived Influences of Environment on Health in Italy: a Penalized Ordinal Regression Approach", "comment": null, "summary": "Understanding how individuals perceive their living environment is a complex\ntask, as it reflects both personal and contextual determinants. In this paper,\nwe address this task by analyzing the environmental module of the Italian\nnationwide health surveillance system PASSI (Progressi delle Aziende Sanitarie\nper la Salute in Italia), integrating it with contextual information at the\nmunicipal level, including socio-economic indicators, pollution exposure, and\nother geographical characteristics. Methodologically, we adopt a penalized\nsemi-parallel cumulative ordinal regression model to analyze how subjective\nperceptions are shaped by both personal and territorial determinants. The\napproach balances flexibility and interpretability by allowing both parallel\nand non-parallel effects while regularizing estimates to address\nmulticollinearity and separation issues. We use the model as an analytical tool\nto uncover the determinants of positivity and neutrality in environmental\nperceptions, defined as factors that contribute the most to improving\nperception or increasing the sense of neutrality. The results are diverse.\nFirst, results reveal significant heterogeneity across Italian territories,\nindicating that local characteristics strongly shape environmental perception.\nSecond, various individual factors interact with contextual influences to shape\nperceptions. Third, hazardous environmental factors, such as higher PM2.5\nlevels, appear to be associated with poorer environmental perception,\nsuggesting a tendency among respondents to recognize specific environmental\nissues. Overall, the approach demonstrates strong potential for application and\nprovides useful insights for environmental policy planning."}
{"id": "2510.01930", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01930", "abs": "https://arxiv.org/abs/2510.01930", "authors": ["Sota Nishiyama", "Masaaki Imaizumi"], "title": "Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by Dynamical Mean-Field Theory", "comment": "54 pages", "summary": "Diagonal linear networks (DLNs) are a tractable model that captures several\nnontrivial behaviors in neural network training, such as\ninitialization-dependent solutions and incremental learning. These phenomena\nare typically studied in isolation, leaving the overall dynamics insufficiently\nunderstood. In this work, we present a unified analysis of various phenomena in\nthe gradient flow dynamics of DLNs. Using Dynamical Mean-Field Theory (DMFT),\nwe derive a low-dimensional effective process that captures the asymptotic\ngradient flow dynamics in high dimensions. Analyzing this effective process\nyields new insights into DLN dynamics, including loss convergence rates and\ntheir trade-off with generalization, and systematically reproduces many of the\npreviously observed phenomena. These findings deepen our understanding of DLNs\nand demonstrate the effectiveness of the DMFT approach in analyzing\nhigh-dimensional learning dynamics of neural networks."}
{"id": "2510.01861", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.01861", "abs": "https://arxiv.org/abs/2510.01861", "authors": ["Roberto Casarin", "Radu Craiu", "Qing Wang"], "title": "Compressed Bayesian Tensor Regression", "comment": null, "summary": "To address the common problem of high dimensionality in tensor regressions,\nwe introduce a generalized tensor random projection method that embeds\nhigh-dimensional tensor-valued covariates into low-dimensional subspaces with\nminimal loss of information about the responses. The method is flexible,\nallowing for tensor-wise, mode-wise, or combined random projections as special\ncases. A Bayesian inference framework is provided featuring the use of a\nhierarchical prior distribution and a low-rank representation of the parameter.\nStrong theoretical support is provided for the concentration properties of the\nrandom projection and posterior consistency of the Bayesian inference. An\nefficient Gibbs sampler is developed to perform inference on the compressed\ndata. To mitigate the sensitivity introduced by random projections, Bayesian\nmodel averaging is employed, with normalising constants estimated using reverse\nlogistic regression. An extensive simulation study is conducted to examine the\neffects of different tuning parameters. Simulations indicate, and the real data\napplication confirms, that compressed Bayesian tensor regression can achieve\nbetter out-of-sample prediction while significantly reducing computational cost\ncompared to standard Bayesian tensor regression."}
{"id": "2510.01692", "categories": ["stat.ME", "62R10"], "pdf": "https://arxiv.org/pdf/2510.01692", "abs": "https://arxiv.org/abs/2510.01692", "authors": ["Han Lin Shang", "Israel Martinez Hernandez"], "title": "Forecasting intraday particle number size distribution: A functional time series approach", "comment": "31 pages, 11 figures, 2 tables", "summary": "Particulate matter data now include various particle sizes, which often\nmanifest as a collection of curves observed sequentially over time. When\nconsidering 51 distinct particle sizes, these curves form a high-dimensional\nfunctional time series observed over equally spaced and densely sampled grids.\nWhile high dimensionality poses statistical challenges due to the curse of\ndimensionality, it also offers a rich source of information that enables\ndetailed analysis of temporal variation across short time intervals for all\nparticle sizes. To model this complexity, we propose a multilevel functional\ntime series framework incorporating a functional factor model to facilitate\none-day-ahead forecasting. To quantify forecast uncertainty, we develop a\ncalibration approach and a split conformal prediction approach to construct\nprediction intervals. Both approaches are designed to minimise the absolute\ndifference between empirical and nominal coverage probabilities using a\nvalidation dataset. Furthermore, to improve forecast accuracy as new intraday\ndata become available, we implement dynamic updating techniques for point and\ninterval forecasts. The proposed methods are validated through an empirical\napplication to hourly measurements of particulate matter in 51 size categories\nin London."}
{"id": "2510.01806", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01806", "abs": "https://arxiv.org/abs/2510.01806", "authors": ["Giovanni Romanò", "Cristian Castiglione", "Daniele Durante"], "title": "Dependent stochastic block models for age-indexed sequences of directed causes-of-death networks", "comment": null, "summary": "Death events commonly arise from complex interactions among interrelated\ncauses, formally classified in reporting practices as underlying and\ncontributing. Leveraging information from death certificates, these\ninteractions can be naturally represented through a sequence of directed\nnetworks encoding co-occurrence strengths between pairs of underlying and\ncontributing causes across ages. Although this perspective opens the avenues to\nlearn informative age-specific block interactions among endogenous groups of\nunderlying and contributing causes displaying similar co-occurrence patterns,\nthere has been limited research along this direction in mortality modeling.\nThis is mainly due to the lack of suitable stochastic block models for\nage-indexed sequences of directed networks. We cover this gap through a novel\nBayesian formulation which crucially learns two separate group structures for\nunderlying and contributing causes, while allowing both structures to change\nsmoothly across ages via dependent random partition priors. As illustrated in\nsimulations, this formulation outperforms state-of-the-art solutions that could\nbe adapted to our motivating application. Moreover, when applied to USA\nmortality data, it unveils structures in the composition, evolution, and\nmodular interactions among causes-of-death groups that were hidden to previous\nstudies. Such findings could have relevant policy implications and contribute\nto an improved understanding of the recent \"death of despair\" phenomena in USA."}
{"id": "2510.01944", "categories": ["stat.ML", "cs.LG", "68T07, 60J60, 62M05, 60H35"], "pdf": "https://arxiv.org/pdf/2510.01944", "abs": "https://arxiv.org/abs/2510.01944", "authors": ["Paul Felix Valsecchi Oliva", "O. Deniz Akyildiz", "Andrew Duncan"], "title": "Uniform-in-time convergence bounds for Persistent Contrastive Divergence Algorithms", "comment": null, "summary": "We propose a continuous-time formulation of persistent contrastive divergence\n(PCD) for maximum likelihood estimation (MLE) of unnormalised densities. Our\napproach expresses PCD as a coupled, multiscale system of stochastic\ndifferential equations (SDEs), which perform optimisation of the parameter and\nsampling of the associated parametrised density, simultaneously.\n  From this novel formulation, we are able to derive explicit bounds for the\nerror between the PCD iterates and the MLE solution for the model parameter.\nThis is made possible by deriving uniform-in-time (UiT) bounds for the\ndifference in moments between the multiscale system and the averaged regime. An\nefficient implementation of the continuous-time scheme is introduced,\nleveraging a class of explicit, stable intregators, stochastic orthogonal\nRunge-Kutta Chebyshev (S-ROCK), for which we provide explicit error estimates\nin the long-time regime. This leads to a novel method for training energy-based\nmodels (EBMs) with explicit error guarantees."}
{"id": "2510.01734", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01734", "abs": "https://arxiv.org/abs/2510.01734", "authors": ["Samuel Pawel", "Leonhard Held"], "title": "Stabilizing Thompson Sampling with Point Null Bayesian Response-Adaptive Randomization", "comment": null, "summary": "Response-adaptive randomization (RAR) methods use accumulated data to adapt\nrandomization probabilities, aiming to increase the probability of allocating\npatients to effective treatments. A popular RAR method is Thompson sampling,\nwhich randomizes patients proportionally to the Bayesian posterior probability\nthat each treatment is the most effective. However, its high variability early\nin a trial can also increase the risk of assigning patients to inferior\ntreatments. We propose a principled method based on Bayesian hypothesis testing\nto mitigate this issue. Specifically, we introduce a point null hypothesis that\npostulates equal effectiveness of treatments. This induces shrinkage toward\nequal randomization probabilities, with the degree of shrinkage controlled by\nthe prior probability of the null hypothesis. Equal randomization and Thompson\nsampling arise as special cases when the prior probability is set to one or\nzero, respectively. Simulated and real-world examples illustrate that the\nproposed method balances highly variable Thompson sampling with static equal\nrandomization. A simulation study demonstrates that the method can mitigate\nissues with ordinary Thompson sampling and has comparable statistical\nproperties to Thompson sampling with common ad hoc modifications such as power\ntransformation and probability capping. We implement the method in the\nopen-source R package brar, enabling experimenters to easily perform point null\nBayesian RAR and support more effective randomization of patients."}
{"id": "2510.02050", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02050", "abs": "https://arxiv.org/abs/2510.02050", "authors": ["Saranya Ganesh S.", "Frederick Iat-Hin Tam", "Milton S. Gomez", "Marie McGraw", "Mark DeMaria", "Kate Musgrave", "Jakob Runge", "Tom Beucler"], "title": "Multidata Causal Discovery for Statistical Hurricane Intensity Forecasting", "comment": "19 pages, 7 Figures, 1 Table, SI", "summary": "Improving statistical forecasts of Atlantic hurricane intensity is limited by\ncomplex nonlinear interactions and difficulty in identifying relevant\npredictors. Conventional methods prioritize correlation or fit, often\noverlooking confounding variables and limiting generalizability to unseen\ntropical storms. To address this, we leverage a multidata causal discovery\nframework with a replicated dataset based on Statistical Hurricane Intensity\nPrediction Scheme (SHIPS) using ERA5 meteorological reanalysis. We conduct\nmultiple experiments to identify and select predictors causally linked to\nhurricane intensity changes. We train multiple linear regression models to\ncompare causal feature selection with no selection, correlation, and random\nforest feature importance across five forecast lead times from 1 to 5 days (24\nto 120 hours). Causal feature selection consistently outperforms on unseen test\ncases, especially for lead times shorter than 3 days. The causal features\nprimarily include vertical shear, mid-tropospheric potential vorticity and\nsurface moisture conditions, which are physically significant yet often\nunderutilized in hurricane intensity predictions. Further, we build an extended\npredictor set (SHIPS+) by adding selected features to the standard SHIPS\npredictors. SHIPS+ yields increased short-term predictive skill at lead times\nof 24, 48, and 72 hours. Adding nonlinearity using multilayer perceptron\nfurther extends skill to longer lead times, despite our framework being purely\nregional and not requiring global forecast data. Operational SHIPS tests\nconfirm that three of the six added causally discovered predictors improve\nforecasts, with the largest gains at longer lead times. Our results demonstrate\nthat causal discovery improves hurricane intensity prediction and pave the way\ntoward more empirical forecasts."}
{"id": "2510.02067", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02067", "abs": "https://arxiv.org/abs/2510.02067", "authors": ["Moritz Melcher", "Simon Weissmann", "Ashia C. Wilson", "Jakob Zech"], "title": "Adaptive Kernel Selection for Stein Variational Gradient Descent", "comment": null, "summary": "A central challenge in Bayesian inference is efficiently approximating\nposterior distributions. Stein Variational Gradient Descent (SVGD) is a popular\nvariational inference method which transports a set of particles to approximate\na target distribution. The SVGD dynamics are governed by a reproducing kernel\nHilbert space (RKHS) and are highly sensitive to the choice of the kernel\nfunction, which directly influences both convergence and approximation quality.\nThe commonly used median heuristic offers a simple approach for setting kernel\nbandwidths but lacks flexibility and often performs poorly, particularly in\nhigh-dimensional settings. In this work, we propose an alternative strategy for\nadaptively choosing kernel parameters over an abstract family of kernels.\nRecent convergence analyses based on the kernelized Stein discrepancy (KSD)\nsuggest that optimizing the kernel parameters by maximizing the KSD can improve\nperformance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a\nmethod that alternates between updating the particles via SVGD and adaptively\ntuning kernel bandwidths through gradient ascent on the KSD. We provide a\nsimplified theoretical analysis that extends existing results on minimizing the\nKSD for fixed kernels to our adaptive setting, showing convergence properties\nfor the maximal KSD over our kernel class. Our empirical results further\nsupport this intuition: Ad-SVGD consistently outperforms standard heuristics in\na variety of tasks."}
{"id": "2510.01771", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01771", "abs": "https://arxiv.org/abs/2510.01771", "authors": ["Jianwei Shi", "Sameh Abdulah", "Ying Sun", "Marc G. Genton"], "title": "Scalable Asynchronous Federated Modeling for Spatial Data", "comment": null, "summary": "Spatial data are central to applications such as environmental monitoring and\nurban planning, but are often distributed across devices where privacy and\ncommunication constraints limit direct sharing. Federated modeling offers a\npractical solution that preserves data privacy while enabling global modeling\nacross distributed data sources. For instance, environmental sensor networks\nare privacy- and bandwidth-constrained, motivating federated spatial modeling\nthat shares only privacy-preserving summaries to produce timely,\nhigh-resolution pollution maps without centralizing raw data. However, existing\nfederated modeling approaches either ignore spatial dependence or rely on\nsynchronous updates that suffer from stragglers in heterogeneous environments.\nThis work proposes an asynchronous federated modeling framework for spatial\ndata based on low-rank Gaussian process approximations. The method employs\nblock-wise optimization and introduces strategies for gradient correction,\nadaptive aggregation, and stabilized updates. We establish linear convergence\nwith explicit dependence on staleness, a result of standalone theoretical\nsignificance. Moreover, numerical experiments demonstrate that the asynchronous\nalgorithm achieves synchronous performance under balanced resource allocation\nand significantly outperforms it in heterogeneous settings, showcasing superior\nrobustness and scalability."}
{"id": "2510.02143", "categories": ["stat.AP", "cs.AI", "cs.DL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02143", "abs": "https://arxiv.org/abs/2510.02143", "authors": ["Buxin Su", "Natalie Collina", "Garrett Wen", "Didong Li", "Kyunghyun Cho", "Jianqing Fan", "Bingxin Zhao", "Weijie Su"], "title": "How to Find Fantastic Papers: Self-Rankings as a Powerful Predictor of Scientific Impact Beyond Peer Review", "comment": null, "summary": "Peer review in academic research aims not only to ensure factual correctness\nbut also to identify work of high scientific potential that can shape future\nresearch directions. This task is especially critical in fast-moving fields\nsuch as artificial intelligence (AI), yet it has become increasingly difficult\ngiven the rapid growth of submissions. In this paper, we investigate an\nunderexplored measure for identifying high-impact research: authors' own\nrankings of their multiple submissions to the same AI conference. Grounded in\ngame-theoretic reasoning, we hypothesize that self-rankings are informative\nbecause authors possess unique understanding of their work's conceptual depth\nand long-term promise. To test this hypothesis, we conducted a large-scale\nexperiment at a leading AI conference, where 1,342 researchers self-ranked\ntheir 2,592 submissions by perceived quality. Tracking outcomes over more than\na year, we found that papers ranked highest by their authors received twice as\nmany citations as their lowest-ranked counterparts; self-rankings were\nespecially effective at identifying highly cited papers (those with over 150\ncitations). Moreover, we showed that self-rankings outperformed peer review\nscores in predicting future citation counts. Our results remained robust after\naccounting for confounders such as preprint posting time and self-citations.\nTogether, these findings demonstrate that authors' self-rankings provide a\nreliable and valuable complement to peer review for identifying and elevating\nhigh-impact research in AI."}
{"id": "2510.02119", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02119", "abs": "https://arxiv.org/abs/2510.02119", "authors": ["Lucas Morisset", "Adrien Hardy", "Alain Durmus"], "title": "Non-Asymptotic Analysis of Data Augmentation for Precision Matrix Estimation", "comment": "Conference paper at NeurIPS 2025 (Spotlight)", "summary": "This paper addresses the problem of inverse covariance (also known as\nprecision matrix) estimation in high-dimensional settings. Specifically, we\nfocus on two classes of estimators: linear shrinkage estimators with a target\nproportional to the identity matrix, and estimators derived from data\naugmentation (DA). Here, DA refers to the common practice of enriching a\ndataset with artificial samples--typically generated via a generative model or\nthrough random transformations of the original data--prior to model fitting.\nFor both classes of estimators, we derive estimators and provide concentration\nbounds for their quadratic error. This allows for both method comparison and\nhyperparameter tuning, such as selecting the optimal proportion of artificial\nsamples. On the technical side, our analysis relies on tools from random matrix\ntheory. We introduce a novel deterministic equivalent for generalized resolvent\nmatrices, accommodating dependent samples with specific structure. We support\nour theoretical results with numerical experiments."}
{"id": "2510.01798", "categories": ["stat.ME", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.01798", "abs": "https://arxiv.org/abs/2510.01798", "authors": ["Roberto Bernal-Arencibia", "Karel Garcia Medina", "Ernesto Estevez-Rams", "Beatriz Aragon-Fernandez"], "title": "Optimal smoothing parameter in Eilers-Wittaker smoother", "comment": null, "summary": "The Eilers-Whittaker method for data smoothing effectiveness depends on the\nchoice of the regularisation parameter, and automatic selection is a necessity\nfor large datasets. Common methods, such as leave-one-out cross-validation, can\nperform poorly when serially correlated noise is present. We propose a novel\nprocedure for selecting the control parameter based on the spectral entropy of\nthe residuals. We define an S-curve from the Euclidean distance between points\nin a plot of the spectral entropy of the residuals versus that of the smoothed\nsignal. The regularisation parameter corresponding to the absolute maximum of\nthis S-curve is chosen as the optimal parameter. Using simulated data, we\nbenchmarked our method against cross-validation and the V-curve. Validation was\nalso performed on diverse experimental data. This robust and straightforward\nprocedure can be a valuable addition to the available selection methods for the\nEilers smoother."}
{"id": "2510.01418", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01418", "abs": "https://arxiv.org/abs/2510.01418", "authors": ["Heng Ge", "Qing Lu"], "title": "DiffKnock: Diffusion-based Knockoff Statistics for Neural Networks Inference", "comment": null, "summary": "We introduce DiffKnock, a diffusion-based knockoff framework for\nhigh-dimensional feature selection with finite-sample false discovery rate\n(FDR) control. DiffKnock addresses two key limitations of existing knockoff\nmethods: preserving complex feature dependencies and detecting non-linear\nassociations. Our approach trains diffusion models to generate valid knockoffs\nand uses neural network--based gradient and filter statistics to construct\nantisymmetric feature importance measures. Through simulations, we showed that\nDiffKnock achieved higher power than autoencoder-based knockoffs while\nmaintaining target FDR, indicating its superior performance in scenarios\ninvolving complex non-linear architectures. Applied to murine single-cell\nRNA-seq data of LPS-stimulated macrophages, DiffKnock identifies canonical\nNF-$\\kappa$B target genes (Ccl3, Hmox1) and regulators (Fosb, Pdgfb). These\nresults highlight that, by combining the flexibility of deep generative models\nwith rigorous statistical guarantees, DiffKnock is a powerful and reliable tool\nfor analyzing single-cell RNA-seq data, as well as high-dimensional and\nstructured data in other domains."}
{"id": "2510.02189", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02189", "abs": "https://arxiv.org/abs/2510.02189", "authors": ["Boris Kriuk"], "title": "Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure Risk at Record 2.9-Million Observation Scale", "comment": "14 pages, 9 figures", "summary": "Arctic warming threatens over 100 billion in permafrost-dependent\ninfrastructure across Northern territories, yet existing risk assessment\nframeworks lack spatiotemporal validation, uncertainty quantification, and\noperational decision-support capabilities. We present a hybrid physics-machine\nlearning framework integrating 2.9 million observations from 171,605 locations\n(2005-2021) combining permafrost fraction data with climate reanalysis. Our\nstacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic\nNet) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal\ncross-validation preventing data leakage. To address machine learning\nlimitations in extrapolative climate scenarios, we develop a hybrid approach\ncombining learned climate-permafrost relationships (60%) with physical\npermafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over\n10 years), we project mean permafrost fraction decline of -20.3 pp (median:\n-20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point\nloss. Infrastructure risk classification identifies 15% high-risk zones (25%\nmedium-risk) with spatially explicit uncertainty maps. Our framework represents\nthe largest validated permafrost ML dataset globally, provides the first\noperational hybrid physics-ML forecasting system for Arctic infrastructure, and\ndelivers open-source tools enabling probabilistic permafrost projections for\nengineering design codes and climate adaptation planning. The methodology is\ngeneralizable to other permafrost regions and demonstrates how hybrid\napproaches can overcome pure data-driven limitations in climate change\napplications."}
{"id": "2510.01861", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.01861", "abs": "https://arxiv.org/abs/2510.01861", "authors": ["Roberto Casarin", "Radu Craiu", "Qing Wang"], "title": "Compressed Bayesian Tensor Regression", "comment": null, "summary": "To address the common problem of high dimensionality in tensor regressions,\nwe introduce a generalized tensor random projection method that embeds\nhigh-dimensional tensor-valued covariates into low-dimensional subspaces with\nminimal loss of information about the responses. The method is flexible,\nallowing for tensor-wise, mode-wise, or combined random projections as special\ncases. A Bayesian inference framework is provided featuring the use of a\nhierarchical prior distribution and a low-rank representation of the parameter.\nStrong theoretical support is provided for the concentration properties of the\nrandom projection and posterior consistency of the Bayesian inference. An\nefficient Gibbs sampler is developed to perform inference on the compressed\ndata. To mitigate the sensitivity introduced by random projections, Bayesian\nmodel averaging is employed, with normalising constants estimated using reverse\nlogistic regression. An extensive simulation study is conducted to examine the\neffects of different tuning parameters. Simulations indicate, and the real data\napplication confirms, that compressed Bayesian tensor regression can achieve\nbetter out-of-sample prediction while significantly reducing computational cost\ncompared to standard Bayesian tensor regression."}
{"id": "2510.02152", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.02152", "abs": "https://arxiv.org/abs/2510.02152", "authors": ["Carlo Gaetan", "Philippe Naveau"], "title": "Multivariate distributional modeling of low, moderate, and large intensities without threshold selection steps", "comment": null, "summary": "In fields such as hydrology and climatology, modelling the entire\ndistribution of positive data is essential, as stakeholders require insights\ninto the full range of values, from low to extreme. Traditional approaches\noften segment the distribution into separate regions, which introduces\nsubjectivity and limits coherence. This is especially true when dealing with\nmultivariate data.\n  In line with multivariate extreme value theory, this paper presents a\nunified, threshold-free framework for modelling marginal behaviours and\ndependence structures based on an extended generalized Pareto distribution\n(EGPD). We propose decomposing multivariate data into radial and angular\ncomponents. The radial component is modelled using a semi-parametric EGPD and\nthe angular distribution is permitted to vary conditionally. This approach\nallows for sufficiently flexible dependence modelling.\n  The hierarchical structure of the model facilitates the inference process.\nFirst, we combine classical maximum likelihood estimation (MLE) methods with\nsemi-parametric approaches based on Bernstein polynomials to estimate the\ndistribution of the radial component. Then, we use multivariate regression\ntechniques to estimate the angular component's parameters.\n  The model is evaluated through synthetic simulations and applied to\nhydrological datasets to exemplify its capacity to capture heavy-tailed\nmarginals and complex multivariate dependencies without threshold\nspecification."}
{"id": "2510.01190", "categories": ["stat.CO", "cs.HC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01190", "abs": "https://arxiv.org/abs/2510.01190", "authors": ["Timbwaoga A. J. Ouermi", "Eric Li", "Kenneth Moreland", "Dave Pugmire", "Chris R. Johnson", "Tushar M. Athawale"], "title": "Efficient Probabilistic Visualization of Local Divergence of 2D Vector Fields with Independent Gaussian Uncertainty", "comment": null, "summary": "This work focuses on visualizing uncertainty of local divergence of\ntwo-dimensional vector fields. Divergence is one of the fundamental attributes\nof fluid flows, as it can help domain scientists analyze potential positions of\nsources (positive divergence) and sinks (negative divergence) in the flow.\nHowever, uncertainty inherent in vector field data can lead to erroneous\ndivergence computations, adversely impacting downstream analysis. While Monte\nCarlo (MC) sampling is a classical approach for estimating divergence\nuncertainty, it suffers from slow convergence and poor scalability with\nincreasing data size and sample counts. Thus, we present a two-fold\ncontribution that tackles the challenges of slow convergence and limited\nscalability of the MC approach. (1) We derive a closed-form approach for highly\nefficient and accurate uncertainty visualization of local divergence, assuming\nindependently Gaussian-distributed vector uncertainties. (2) We further\nintegrate our approach into Viskores, a platform-portable parallel library, to\naccelerate uncertainty visualization. In our results, we demonstrate\nsignificantly enhanced efficiency and accuracy of our serial analytical\n(speed-up up to 1946X) and parallel Viskores (speed-up up to 19698X) algorithms\nover the classical serial MC approach. We also demonstrate qualitative\nimprovements of our probabilistic divergence visualizations over traditional\nmean-field visualization, which disregards uncertainty. We validate the\naccuracy and efficiency of our methods on wind forecast and ocean simulation\ndatasets."}
{"id": "2510.01915", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01915", "abs": "https://arxiv.org/abs/2510.01915", "authors": ["Yann McLatchie", "Badr-Eddine Cherief-Abdellatif", "David T. Frazier", "Jeremias Knoblauch"], "title": "Predictively Oriented Posteriors", "comment": "40 pages (84 with references and appendix), 7 Figures", "summary": "We advocate for a new statistical principle that combines the most desirable\naspects of both parameter inference and density estimation. This leads us to\nthe predictively oriented (PrO) posterior, which expresses uncertainty as a\nconsequence of predictive ability. Doing so leads to inferences which\npredictively dominate both classical and generalised Bayes posterior predictive\ndistributions: up to logarithmic factors, PrO posteriors converge to the\npredictively optimal model average at rate $n^{-1/2}$. Whereas classical and\ngeneralised Bayes posteriors only achieve this rate if the model can recover\nthe data-generating process, PrO posteriors adapt to the level of model\nmisspecification. This means that they concentrate around the true model at\nrate $n^{1/2}$ in the same way as Bayes and Gibbs posteriors if the model can\nrecover the data-generating distribution, but do \\textit{not} concentrate in\nthe presence of non-trivial forms of model misspecification. Instead, they\nstabilise towards a predictively optimal posterior whose degree of irreducible\nuncertainty admits an interpretation as the degree of model misspecification --\na sharp contrast to how Bayesian uncertainty and its existing extensions\nbehave. Lastly, we show that PrO posteriors can be sampled from by evolving\nparticles based on mean field Langevin dynamics, and verify the practical\nsignificance of our theoretical developments on a number of numerical examples."}
{"id": "2510.01418", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01418", "abs": "https://arxiv.org/abs/2510.01418", "authors": ["Heng Ge", "Qing Lu"], "title": "DiffKnock: Diffusion-based Knockoff Statistics for Neural Networks Inference", "comment": null, "summary": "We introduce DiffKnock, a diffusion-based knockoff framework for\nhigh-dimensional feature selection with finite-sample false discovery rate\n(FDR) control. DiffKnock addresses two key limitations of existing knockoff\nmethods: preserving complex feature dependencies and detecting non-linear\nassociations. Our approach trains diffusion models to generate valid knockoffs\nand uses neural network--based gradient and filter statistics to construct\nantisymmetric feature importance measures. Through simulations, we showed that\nDiffKnock achieved higher power than autoencoder-based knockoffs while\nmaintaining target FDR, indicating its superior performance in scenarios\ninvolving complex non-linear architectures. Applied to murine single-cell\nRNA-seq data of LPS-stimulated macrophages, DiffKnock identifies canonical\nNF-$\\kappa$B target genes (Ccl3, Hmox1) and regulators (Fosb, Pdgfb). These\nresults highlight that, by combining the flexibility of deep generative models\nwith rigorous statistical guarantees, DiffKnock is a powerful and reliable tool\nfor analyzing single-cell RNA-seq data, as well as high-dimensional and\nstructured data in other domains."}
{"id": "2510.02123", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.02123", "abs": "https://arxiv.org/abs/2510.02123", "authors": ["Steven Wang", "Isys Johnson", "Jessica Grogan", "Lalit Jain", "Atri Rudra", "Kyle Hunt", "Kenneth Joseph"], "title": "Identifying Subgroup and Context Effects in Conjoint Experiments", "comment": null, "summary": "Conjoint experiments have become central to survey research in political\nscience and related fields because they allow researchers to study preferences\nacross multiple attributes simultaneously. Beyond estimating main effects,\nscholars increasingly analyze heterogeneity through subgroup analysis and\ncontextual variables, raising methodological challenges in detecting and\ninterpreting interaction effects. Statistical power constraints, common in\nsurvey experiments, further complicate this task. This paper addresses the\nquestion: how can both main and interaction effects be reliably inferred in\nconjoint studies? We contribute in two ways. First, we conduct a systematic\nevaluation of leading approaches, including post-hoc corrections, sparse\nregression methods, and Bayesian models, across simulation regimes that vary\nsparsity, noise, and data availability. Second, we propose a novel black-box\ninference framework that leverages machine learning to recover main and\ninteraction effects in conjoint experiments. Our approach balances\ncomputational efficiency with accuracy, providing a practical tool for\nresearchers studying heterogeneous effects."}
{"id": "2510.01771", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01771", "abs": "https://arxiv.org/abs/2510.01771", "authors": ["Jianwei Shi", "Sameh Abdulah", "Ying Sun", "Marc G. Genton"], "title": "Scalable Asynchronous Federated Modeling for Spatial Data", "comment": null, "summary": "Spatial data are central to applications such as environmental monitoring and\nurban planning, but are often distributed across devices where privacy and\ncommunication constraints limit direct sharing. Federated modeling offers a\npractical solution that preserves data privacy while enabling global modeling\nacross distributed data sources. For instance, environmental sensor networks\nare privacy- and bandwidth-constrained, motivating federated spatial modeling\nthat shares only privacy-preserving summaries to produce timely,\nhigh-resolution pollution maps without centralizing raw data. However, existing\nfederated modeling approaches either ignore spatial dependence or rely on\nsynchronous updates that suffer from stragglers in heterogeneous environments.\nThis work proposes an asynchronous federated modeling framework for spatial\ndata based on low-rank Gaussian process approximations. The method employs\nblock-wise optimization and introduces strategies for gradient correction,\nadaptive aggregation, and stabilized updates. We establish linear convergence\nwith explicit dependence on staleness, a result of standalone theoretical\nsignificance. Moreover, numerical experiments demonstrate that the asynchronous\nalgorithm achieves synchronous performance under balanced resource allocation\nand significantly outperforms it in heterogeneous settings, showcasing superior\nrobustness and scalability."}
{"id": "2510.02137", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.02137", "abs": "https://arxiv.org/abs/2510.02137", "authors": ["Catherine Ning", "Dimitris Bertsimas", "Johan Gagnière", "Stefan Buettner", "Per Eystein Loenning", "Hideo Baba", "Itaru Endo", "Georgios Stasinos", "Richard Burkhart", "Federico N. Auecio", "Felix Balzer", "Cornelis Verhoef", "Martin E. Kreis", "Georgios Antonios Margonis"], "title": "Improving Survival Models in Healthcare by Balancing Imbalanced Cohorts: A Novel Approach", "comment": "14 pages, 3 figures, under revision at JCO Clinical Cancer\n  Informatics", "summary": "We explore whether survival model performance in underrepresented high- and\nlow-risk subgroups - regions of the prognostic spectrum where clinical\ndecisions are most consequential - can be improved through targeted\nrestructuring of the training dataset. Rather than modifying model\narchitecture, we propose a novel risk-stratified sampling method that addresses\nimbalances in prognostic subgroup density to support more reliable learning in\nunderrepresented tail strata. We introduce a novel methodology that partitions\npatients by baseline prognostic risk and applies matching within each stratum\nto equalize representation across the risk distribution. We implement this\nframework on a cohort of 1,799 patients with resected colorectal liver\nmetastases (CRLM), including 1,197 who received adjuvant chemotherapy and 602\nwho did not. All models used in this study are Cox proportional hazards models\ntrained on the same set of selected variables. Model performance is assessed\nvia Harrell's C index, time-dependent AUC, and Integrated Calibration Index\n(ICI), with internal validation using Efron's bias-corrected bootstrapping.\nExternal validation is conducted on two independent CRLM datasets. Cox models\ntrained on risk-balanced cohorts showed consistent improvements in internal\nvalidation compared to models trained on the full dataset while noticeably\nenhancing stratified C-index values in underrepresented high- and low-risk\nstrata of the external cohorts. Our findings suggest that survival model\nperformance in observational oncology cohorts can be meaningfully improved\nthrough targeted rebalancing of the training data across prognostic risk\nstrata. This approach offers a practical and model-agnostic complement to\nexisting methods, especially in applications where predictive reliability\nacross the full risk continuum is critical to downstream clinical decisions."}
{"id": "2510.01915", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01915", "abs": "https://arxiv.org/abs/2510.01915", "authors": ["Yann McLatchie", "Badr-Eddine Cherief-Abdellatif", "David T. Frazier", "Jeremias Knoblauch"], "title": "Predictively Oriented Posteriors", "comment": "40 pages (84 with references and appendix), 7 Figures", "summary": "We advocate for a new statistical principle that combines the most desirable\naspects of both parameter inference and density estimation. This leads us to\nthe predictively oriented (PrO) posterior, which expresses uncertainty as a\nconsequence of predictive ability. Doing so leads to inferences which\npredictively dominate both classical and generalised Bayes posterior predictive\ndistributions: up to logarithmic factors, PrO posteriors converge to the\npredictively optimal model average at rate $n^{-1/2}$. Whereas classical and\ngeneralised Bayes posteriors only achieve this rate if the model can recover\nthe data-generating process, PrO posteriors adapt to the level of model\nmisspecification. This means that they concentrate around the true model at\nrate $n^{1/2}$ in the same way as Bayes and Gibbs posteriors if the model can\nrecover the data-generating distribution, but do \\textit{not} concentrate in\nthe presence of non-trivial forms of model misspecification. Instead, they\nstabilise towards a predictively optimal posterior whose degree of irreducible\nuncertainty admits an interpretation as the degree of model misspecification --\na sharp contrast to how Bayesian uncertainty and its existing extensions\nbehave. Lastly, we show that PrO posteriors can be sampled from by evolving\nparticles based on mean field Langevin dynamics, and verify the practical\nsignificance of our theoretical developments on a number of numerical examples."}
{"id": "2510.02152", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.02152", "abs": "https://arxiv.org/abs/2510.02152", "authors": ["Carlo Gaetan", "Philippe Naveau"], "title": "Multivariate distributional modeling of low, moderate, and large intensities without threshold selection steps", "comment": null, "summary": "In fields such as hydrology and climatology, modelling the entire\ndistribution of positive data is essential, as stakeholders require insights\ninto the full range of values, from low to extreme. Traditional approaches\noften segment the distribution into separate regions, which introduces\nsubjectivity and limits coherence. This is especially true when dealing with\nmultivariate data.\n  In line with multivariate extreme value theory, this paper presents a\nunified, threshold-free framework for modelling marginal behaviours and\ndependence structures based on an extended generalized Pareto distribution\n(EGPD). We propose decomposing multivariate data into radial and angular\ncomponents. The radial component is modelled using a semi-parametric EGPD and\nthe angular distribution is permitted to vary conditionally. This approach\nallows for sufficiently flexible dependence modelling.\n  The hierarchical structure of the model facilitates the inference process.\nFirst, we combine classical maximum likelihood estimation (MLE) methods with\nsemi-parametric approaches based on Bernstein polynomials to estimate the\ndistribution of the radial component. Then, we use multivariate regression\ntechniques to estimate the angular component's parameters.\n  The model is evaluated through synthetic simulations and applied to\nhydrological datasets to exemplify its capacity to capture heavy-tailed\nmarginals and complex multivariate dependencies without threshold\nspecification."}
{"id": "2510.01806", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01806", "abs": "https://arxiv.org/abs/2510.01806", "authors": ["Giovanni Romanò", "Cristian Castiglione", "Daniele Durante"], "title": "Dependent stochastic block models for age-indexed sequences of directed causes-of-death networks", "comment": null, "summary": "Death events commonly arise from complex interactions among interrelated\ncauses, formally classified in reporting practices as underlying and\ncontributing. Leveraging information from death certificates, these\ninteractions can be naturally represented through a sequence of directed\nnetworks encoding co-occurrence strengths between pairs of underlying and\ncontributing causes across ages. Although this perspective opens the avenues to\nlearn informative age-specific block interactions among endogenous groups of\nunderlying and contributing causes displaying similar co-occurrence patterns,\nthere has been limited research along this direction in mortality modeling.\nThis is mainly due to the lack of suitable stochastic block models for\nage-indexed sequences of directed networks. We cover this gap through a novel\nBayesian formulation which crucially learns two separate group structures for\nunderlying and contributing causes, while allowing both structures to change\nsmoothly across ages via dependent random partition priors. As illustrated in\nsimulations, this formulation outperforms state-of-the-art solutions that could\nbe adapted to our motivating application. Moreover, when applied to USA\nmortality data, it unveils structures in the composition, evolution, and\nmodular interactions among causes-of-death groups that were hidden to previous\nstudies. Such findings could have relevant policy implications and contribute\nto an improved understanding of the recent \"death of despair\" phenomena in USA."}
{"id": "2510.01901", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01901", "abs": "https://arxiv.org/abs/2510.01901", "authors": ["Joshua J Bon", "Anthony Lee"], "title": "Knots and variance ordering of sequential Monte Carlo algorithms", "comment": "Working paper in advance of journal submission. 49 pages, 5 figures", "summary": "Sequential Monte Carlo algorithms, or particle filters, are widely used for\napproximating intractable integrals, particularly those arising in Bayesian\ninference and state-space models. We introduce a new variance reduction\ntechnique, the knot operator, which improves the efficiency of particle filters\nby incorporating potential function information into part, or all, of a\ntransition kernel. The knot operator induces a partial ordering of Feynman-Kac\nmodels that implies an order on the asymptotic variance of particle filters,\noffering a new approach to algorithm design. We discuss connections to existing\nstrategies for designing efficient particle filters, including model\nmarginalisation. Our theory generalises such techniques and provides\nquantitative asymptotic variance ordering results. We revisit the fully-adapted\n(auxiliary) particle filter using our theory of knots to show how a small\nmodification guarantees an asymptotic variance ordering for all relevant test\nfunctions."}
