{"id": "2511.14784", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14784", "abs": "https://arxiv.org/abs/2511.14784", "authors": ["Sourav De", "Koustav Chowdhury", "Bibhabasu Mandal", "Sagar Ghosh", "Swagatam Das", "Debolina Paul", "Saptarshi Chakraborty"], "title": "Convex Clustering Redefined: Robust Learning with the Median of Means Estimator", "comment": "Accepted in AAAI 2026", "summary": "Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches."}
{"id": "2511.14827", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.AP"], "pdf": "https://arxiv.org/pdf/2511.14827", "abs": "https://arxiv.org/abs/2511.14827", "authors": ["Peter Halmos", "Boris Hanin"], "title": "Implicit Bias of the JKO Scheme", "comment": null, "summary": "Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \\emph{modified} energy \\[ J^η(ρ) = J(ρ) - \\fracη{4}\\int_M \\Big\\lVert \\nabla_g \\frac{δJ}{δρ} (ρ) \\Big\\rVert_{2}^{2} \\,ρ(dx), \\] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \\textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \\emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D."}
{"id": "2511.15010", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15010", "abs": "https://arxiv.org/abs/2511.15010", "authors": ["Katie Rainey", "Erin Hausmann", "Donald Waagen", "David Gray", "Donald Hulsey"], "title": "Latent space analysis and generalization to out-of-distribution data", "comment": null, "summary": "Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \\textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability."}
{"id": "2511.15120", "categories": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.15120", "abs": "https://arxiv.org/abs/2511.15120", "authors": ["Bohan Zhang", "Zihao Wang", "Hengyu Fu", "Jason D. Lee"], "title": "Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit", "comment": "86 pages, 2 figures. The order of the first two authors was determined by a coin flip", "summary": "In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\\boldsymbol{x})=g(\\boldsymbol{U}\\boldsymbol{x})$ with hidden subspace $\\boldsymbol{U}\\in \\mathbb{R}^{r\\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\\widetilde{\\mathcal{O}}(d)$ samples and $\\widetilde{\\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency."}
{"id": "2511.14893", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14893", "abs": "https://arxiv.org/abs/2511.14893", "authors": ["Changjun Li", "Heather Allore", "Michael O. Harhay", "Fan Li", "Guangyu Tong"], "title": "Uncovering Treatment Effect Heterogeneity in Pragmatic Gerontology Trials", "comment": null, "summary": "Detecting heterogeneity in treatment response enriches the interpretation of gerontologic trials. In aging research, estimating the effect of the intervention on clinically meaningful outcomes faces analytical challenges when it is truncated by death. For example, in the Whole Systems Demonstrator trial, a large cluster-randomized study evaluating telecare among older adults, the overall effect of the intervention on quality of life was found to be null. However, this marginal intervention estimate obscures potential heterogeneity of individuals responding to the intervention, particularly among those who survive to the end of follow-up. To explore this heterogeneity, we adopt a causal framework grounded in principal stratification, targeting the Survivor Average Causal Effect (SACE)-the treatment effect among \"always-survivors,\" or those who would survive regardless of treatment assignment. We extend this framework using Bayesian Additive Regression Trees (BART), a nonparametric machine learning method, to flexibly model both latent principal strata and stratum-specific potential outcomes. This enables the estimation of the Conditional SACE (CSACE), allowing us to uncover variation in treatment effects across subgroups defined by baseline characteristics. Our analysis reveals that despite the null average effect, some subgroups experience distinct quality of life benefits (or lack thereof) from telecare, highlighting opportunities for more personalized intervention strategies. This study demonstrates how embedding machine learning methods, such as BART, within a principled causal inference framework can offer deeper insights into trial data with complex features including truncation by death and clustering-key considerations in analyzing pragmatic gerontology trials."}
{"id": "2511.14784", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14784", "abs": "https://arxiv.org/abs/2511.14784", "authors": ["Sourav De", "Koustav Chowdhury", "Bibhabasu Mandal", "Sagar Ghosh", "Swagatam Das", "Debolina Paul", "Saptarshi Chakraborty"], "title": "Convex Clustering Redefined: Robust Learning with the Median of Means Estimator", "comment": "Accepted in AAAI 2026", "summary": "Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches."}
{"id": "2511.14815", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14815", "abs": "https://arxiv.org/abs/2511.14815", "authors": ["Musab Alamoudi", "Robert L. Paige", "Vic Patrangenaru"], "title": "Extrinsic Total-Variance and Coplanarity via Oriented and Classical Projective Shape Analysis", "comment": "15 pages, 5 .eps files", "summary": "Projective shape analysis provides a geometric framework for studying digital images acquired by pinhole digital cameras. In the classical projective shape (PS) method, landmark configurations are represented in $(\\RP^2)^{k-4}$, where $k$ is the number of landmarks observed. This representation is invariant under the action of the full projective group on this space and is sign-blind, so opposite directions in $\\R^{3}$ determine the same projective point and front--back orientation of a surface is not recorded. Oriented projective shape ($\\OPS$) restores this information by working on a product of $k-4$ spheres $\\SP^2$ instead of projective space and restricting attention to the orientation-preserving subgroup of projective transformations. In this paper we introduce an extrinsic total-variance index for OPS, resulting in the extrinsic Fréchet framework for the m dimensional case from the inclusion $\\jdir:(\\SP^m)^q\\hookrightarrow(\\R^{m+1})^q,q=k-m-2$. In the planar pentad case ($m=2$, $q=1$) the sample total extrinsic variance has a closed form in terms of the mean of a random sample of size $n$ of oriented projective coordinates in $S^2$. As an illustration, using an oriented projective frame, we analyze the Sope Creek stone data set, a benchmark and nearly planar example with $41$ images and $5$ landmarks. Using a delta-method applied to a large sample and a generalized Slutsky theorem argument, for an OPS leave-two-out diagnostic, one identifies coplanarity at the $5\\%$ level, confirming the concentrated data coplanarity PS result in Patrangenaru(2001)\\cite{Patrangenaru2001}."}
{"id": "2511.15146", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.15146", "abs": "https://arxiv.org/abs/2511.15146", "authors": ["Eugene Ndiaye"], "title": "Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings", "comment": null, "summary": "Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure."}
{"id": "2511.14930", "categories": ["stat.AP", "cs.AI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.14930", "abs": "https://arxiv.org/abs/2511.14930", "authors": ["Robert Kubinec", "Aseem Mahajan"], "title": "Fifty Shades of Greenwashing: The Political Economy of Climate Change Advertising on Social Media", "comment": "Supplementary information can be downloaded at https://www.icloud.com/iclouddrive/00eqjqGpFLQ86sPSGGWRPuuhw#mahajan%5Fkubinec%5FSI", "summary": "In this paper, we provide a novel measure for greenwashing -- i.e., climate-related misinformation -- that shows how polluting companies can use social media advertising related to climate change to redirect criticism. To do so, we identify greenwashing content in 11 million social-political ads in Meta's Ad Targeting Datset with a measurement technique that combines large language models, human coders, and advances in Bayesian item response theory. We show that what is called greenwashing has diverse actors and components, but we also identify a very pernicious form, which we call political greenwashing, that appears to be promoted by fossil fuel companies and related interest groups. Based on ad targeting data, we show that much of this advertising happens via organizations with undisclosed links to the fossil fuel industry. Furthermore, we show that greenwashing ad content is being micro-targeted at left-leaning communities with fossil fuel assets, though we also find comparatively little evidence of ad targeting aimed at influencing public opinion at the national level."}
{"id": "2511.15120", "categories": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.15120", "abs": "https://arxiv.org/abs/2511.15120", "authors": ["Bohan Zhang", "Zihao Wang", "Hengyu Fu", "Jason D. Lee"], "title": "Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit", "comment": "86 pages, 2 figures. The order of the first two authors was determined by a coin flip", "summary": "In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\\boldsymbol{x})=g(\\boldsymbol{U}\\boldsymbol{x})$ with hidden subspace $\\boldsymbol{U}\\in \\mathbb{R}^{r\\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\\widetilde{\\mathcal{O}}(d)$ samples and $\\widetilde{\\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency."}
{"id": "2511.14992", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14992", "abs": "https://arxiv.org/abs/2511.14992", "authors": ["Jiajun Liu", "Guangcai Mao", "Xiaofei Wang"], "title": "An Estimand-Focused Approach for AUC Estimation, Generalization, and Comparison: From Non-representative Samples to Target Population", "comment": null, "summary": "The area under the ROC curve (AUC) is the standard measure of a biomarker's discriminatory accuracy; however, naive AUC estimates can be misleading when validation cohorts differ from the intended target population. Such covariate shifts commonly arise under biased or non-random sampling, distorting AUC estimations and thus impeding both generalization and cross-study comparison of AUC. We develop an estimand-focused framework for valid AUC estimation and benchmarking under covariate shift. Leveraging balancing ideas from causal inference, we extend calibration weighting to the U-statistic framework for AUC estimation and introduce a family of estimators that accommodate both summary-level and patient-level information; in certain specifications, some of these estimators attain double robustness. Furthermore, we establish asymptotic properties and study their performances across a spectrum of covariate shift severities and calibration choices in comprehensive simulations. Finally, we demonstrate practical utility in the POWER trials by evaluating how baseline stair-climb power (SCP) predicts 6-month survival among advanced non-small-cell lung cancer (NSCLC) patients. Together, the results provide a principled toolkit for anchoring biomarker AUCs to clinically relevant target populations and for comparing them fairly across studies despite distributional differences."}
{"id": "2511.15469", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.15469", "abs": "https://arxiv.org/abs/2511.15469", "authors": ["Xiangxin Kong", "Hang Wang", "Yutong Li", "Yanghao Chen", "Zudi Lu"], "title": "Computation for Epidemic Prediction with Graph Neural Network by Model Combination", "comment": "37pages, 24 figures", "summary": "Modelling epidemic events such as COVID-19 cases in both time and space dimensions is an important but challenging task. Building on in-depth review and assessment of two popular graph neural network (GNN)-based regional epidemic forecasting models of \\textbf{EpiGNN} and \\textbf{ColaGNN}, we propose a novel hybrid graph neural network model, \\textbf{EpiHybridGNN}, which integrates the strengths of both EpiGNN and \\textbf{ColaGNN}. In the EpiGNN, through its transmission risk encoding module and Region-Aware Graph Learner (RAGL), both multi-scale convolutions and Graph Convolutional Networks (GCNs) are combined, aiming to effectively capture spatio-temporal propagation dynamics between regions and support the integration of external resources to enhance forecasting performance. While, in the ColaGNN, a cross-location attention mechanism, multi-scale dilated convolutions, and graph message passing are utilized to address the challenges of long-term forecasting through dynamic graph structures and spatio-temporal feature fusion. Both enjoy respective advantages but also share mutual shortcomings. Our EpiHybridGNN is therefore designed to combine the advantages of both EpiGNN, in its risk encoding and RAGL, and ColaGNN, in its long-term forecasting capabilities and dynamic attention mechanisms. This helps to form a more comprehensive and robust prediction of spatio-temporal epidemic propagation. The computational architecture, core formulas and their interpretations of our proposed EpiHybridGNN are provided. Multiple numerical real data experiments validate that our EpiHybridGNN significantly outperforms both EpiGNN and ColaGNN in epidemic forecasting with comprehensive insights and references offered."}
{"id": "2511.15196", "categories": ["stat.ML", "cs.LG", "hep-lat"], "pdf": "https://arxiv.org/pdf/2511.15196", "abs": "https://arxiv.org/abs/2511.15196", "authors": ["David Yallup"], "title": "Particle Monte Carlo methods for Lattice Field Theory", "comment": "To appear in the NeurIPS 2025 workshop, Frontiers in Probabilistic Inference: Sampling Meets Learning", "summary": "High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost."}
{"id": "2511.14951", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14951", "abs": "https://arxiv.org/abs/2511.14951", "authors": ["Sam Fisher", "Dmitry Lesnik", "Tobias Schäfer"], "title": "Principled Frequentist Estimation of Racial Disparity in Credit Approval under Unobserved Race", "comment": "30 pages, 10 figures", "summary": "Estimating racial disparities in loan-approval probabilities when race is unobserved is routinely required for fair lending compliance. In such cases, race probabilities-typically from Bayesian Improved Surname Geocoding (BISG)-stand in for true race. Prior work shows that common heuristic approaches, including the Threshold and Weighting estimators, are inconsistent under valid identification assumptions, compromising internal validity. A recent Bayesian approach demonstrates consistency under assumptions reasonable in many fair lending contexts. This approach hinges on the insight that identification requires the race predictors to be exogenous with respect to loan approval, essentially an instrumental-variables design. We present a frequentist counterpart to this solution via Ordinary Least Squares (OLS) and Maximum Likelihood Estimation (MLE) under a similar exogeneity assumption. To satisfy these assumptions in practice, we introduce (i) a surname-only proxy analogous to BISG and (ii) an income-stratified prior for race probabilities. Monte Carlo simulations and an application to 2023 Los Angeles HMDA data confirm superior performance: this method reduces RMSE in the LA Black/White adverse-impact ratio by 79.7% (from 10.639pp to 2.158pp) compared to a Weighting estimator with the standard prior."}
{"id": "2511.15146", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.15146", "abs": "https://arxiv.org/abs/2511.15146", "authors": ["Eugene Ndiaye"], "title": "Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings", "comment": null, "summary": "Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure."}
{"id": "2511.14996", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14996", "abs": "https://arxiv.org/abs/2511.14996", "authors": ["Jonas M. Mikhaeil", "Donald P. Green", "David Blei"], "title": "The Sequential Nature of Science: Quantifying Learning from a Sequence of Studies", "comment": null, "summary": "Scientific progress is inherently sequential: collective knowledge is updated as new studies enter the literature. We propose the sequential meta-analysis research trace (SMART), which quantifies the influence of each study at the time it enters the literature. In contrast to classical meta-analysis, our method can capture how new studies may cast doubt on previously held beliefs, increasing collective uncertainty. For example, a new study may present a methodological critique of prior work and propose a superior method. Even small studies, which may not materially affect a retrospective meta-analysis, can be influential at the time they appeared. To contrast SMART with classical meta-analysis, we re-analyze two meta-analysis datasets, from psychology and labor economics. One assembles studies using a single methodology; the other contains studies that predate or follow an important methodological innovation. Our formalization of sequential learning highlights the importance of methodological innovation that might otherwise be overlooked by classical meta-analysis."}
{"id": "2511.14784", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14784", "abs": "https://arxiv.org/abs/2511.14784", "authors": ["Sourav De", "Koustav Chowdhury", "Bibhabasu Mandal", "Sagar Ghosh", "Swagatam Das", "Debolina Paul", "Saptarshi Chakraborty"], "title": "Convex Clustering Redefined: Robust Learning with the Median of Means Estimator", "comment": "Accepted in AAAI 2026", "summary": "Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches."}
{"id": "2511.15315", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15315", "abs": "https://arxiv.org/abs/2511.15315", "authors": ["Abdelhamid Ezzerg", "Ilija Bogunovic", "Jeremias Knoblauch"], "title": "Robust Bayesian Optimisation with Unbounded Corruptions", "comment": null, "summary": "Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm."}
{"id": "2511.15003", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15003", "abs": "https://arxiv.org/abs/2511.15003", "authors": ["Reza Mirjalili", "Behrad Braghi", "Shahram Shadrokh Sikari"], "title": "Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks", "comment": "52 pages, 12 figures", "summary": "Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies.\n  We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions."}
{"id": "2511.15561", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.15561", "abs": "https://arxiv.org/abs/2511.15561", "authors": ["Louison Bocquet-Nouaille", "Jérôme Morio", "Benjamin Bobbia"], "title": "Variance-reduced extreme value index estimators using control variates in a semi-supervised setting", "comment": null, "summary": "The estimation of the Extreme Value Index (EVI) is fundamental in extreme value analysis but suffers from high variance due to reliance on only a few extreme observations. We propose a control variates based transfer learning approach in a semi-supervised framework, where a small set of coupled target and source observations is combined with abundant unpaired source data. By expressing the Hill estimator of the target EVI as a ratio of means, we apply approximate control variates to both numerator and denominator, with jointly optimized coefficients that guarantee variance reduction without introducing bias. We show theoretically and through simulations that the asymptotic relative variance reduction of the transferred Hill estimator is proportional to the tail dependence between the target and source variables and independent of their EVI values. Thus, substantial variance reduction can be achieved even without similarity in tail heaviness of the target and source distributions. The proposed approach can be extended to other EVI estimators expressed with ratio of means, as demonstrated on the moment estimator. The practical value of the proposed method is illustrated on multi-fidelity water surge and ice accretion datasets."}
{"id": "2511.15045", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15045", "abs": "https://arxiv.org/abs/2511.15045", "authors": ["Kirsten E. Landsiedel", "Rachael V. Phillips", "Maya L. Petersen", "Mark J. van der Laan"], "title": "Hazard-Based Targeted Maximum Likelihood Estimation for Survival in Resampling Designs", "comment": "28 pages, 5 figures", "summary": "Survival is a key metric for evaluating standards of care for people living with HIV. In resource-limited settings, high rates of loss to follow-up (LTFU) often result in underestimation of mortality when only observed deaths are considered. Resampling, which tracks a subset of LTFU patients to ascertain their outcomes, mitigates bias and improves survival estimates. However, common estimators for survival in resampling designs, such as weighted Kaplan-Meier (KM), fail to leverage covariate information collected during repeated clinic visits, even though this information is highly predictive of survival. We propose a Targeted Maximum Likelihood Estimator (TMLE) for survival in resampling designs, which addresses these limitations by leveraging baseline and longitudinal covariates to achieve greater efficiency. Our TMLE is a plug-in estimator and is robust to misspecification of the initial model for the conditional hazard of death, guaranteeing consistency of our estimator due to known resampling probabilities. We present: (1) a fully efficient TMLE for data from resampling studies with fixed follow-up time for all participants and (2) an inverse probability of censoring weighted (IPCW) TMLE that accounts for varied follow-up times by stratifying on patients with sufficient follow-up to evaluate survival. This IPCW-TMLE can be made highly efficient through nonparametric or targeted estimation of the follow-up censoring mechanism. In simulations, our TMLE reduced variance by up to 55% compared with the commonly used weighted KM estimator while preserving nominal confidence interval coverage. These findings demonstrate the potential of our TMLE to improve survival estimation in resampling designs, offering a robust and resource-efficient framework for HIV research. Keywords: Resampling designs, Survival analysis, Targeted Maximum Likelihood Estimation, Inverse probability weighting"}
{"id": "2511.15332", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15332", "abs": "https://arxiv.org/abs/2511.15332", "authors": ["The Tien Mai"], "title": "Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss", "comment": null, "summary": "In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.\n  Our method is implemented in the \\texttt{R} package \\texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso"}
{"id": "2511.14815", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14815", "abs": "https://arxiv.org/abs/2511.14815", "authors": ["Musab Alamoudi", "Robert L. Paige", "Vic Patrangenaru"], "title": "Extrinsic Total-Variance and Coplanarity via Oriented and Classical Projective Shape Analysis", "comment": "15 pages, 5 .eps files", "summary": "Projective shape analysis provides a geometric framework for studying digital images acquired by pinhole digital cameras. In the classical projective shape (PS) method, landmark configurations are represented in $(\\RP^2)^{k-4}$, where $k$ is the number of landmarks observed. This representation is invariant under the action of the full projective group on this space and is sign-blind, so opposite directions in $\\R^{3}$ determine the same projective point and front--back orientation of a surface is not recorded. Oriented projective shape ($\\OPS$) restores this information by working on a product of $k-4$ spheres $\\SP^2$ instead of projective space and restricting attention to the orientation-preserving subgroup of projective transformations. In this paper we introduce an extrinsic total-variance index for OPS, resulting in the extrinsic Fréchet framework for the m dimensional case from the inclusion $\\jdir:(\\SP^m)^q\\hookrightarrow(\\R^{m+1})^q,q=k-m-2$. In the planar pentad case ($m=2$, $q=1$) the sample total extrinsic variance has a closed form in terms of the mean of a random sample of size $n$ of oriented projective coordinates in $S^2$. As an illustration, using an oriented projective frame, we analyze the Sope Creek stone data set, a benchmark and nearly planar example with $41$ images and $5$ landmarks. Using a delta-method applied to a large sample and a generalized Slutsky theorem argument, for an OPS leave-two-out diagnostic, one identifies coplanarity at the $5\\%$ level, confirming the concentrated data coplanarity PS result in Patrangenaru(2001)\\cite{Patrangenaru2001}."}
{"id": "2511.15068", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15068", "abs": "https://arxiv.org/abs/2511.15068", "authors": ["Soham Bakshi", "Snigdha Panigrahi"], "title": "Classification Trees with Valid Inference via the Exponential Mechanism", "comment": null, "summary": "Decision trees are widely used for non-linear modeling, as they capture interactions between predictors while producing inherently interpretable models. Despite their popularity, performing inference on the non-linear fit remains largely unaddressed. This paper focuses on classification trees and makes two key contributions. First, we introduce a novel tree-fitting method that replaces the greedy splitting of the predictor space in standard tree algorithms with a probabilistic approach. Each split in our approach is selected according to sampling probabilities defined by an exponential mechanism, with a temperature parameter controlling its deviation from the deterministic choice given data. Second, while our approach can fit a tree that, with high probability, approximates the fit produced by standard tree algorithms at high temperatures, it is not merely predictive- unlike standard algorithms, it enables valid inference by taking into account the highly adaptive tree structure. Our method produces pivots directly from the sampling probabilities in the exponential mechanism. In theory, our pivots allow asymptotically valid inference on the parameters in the predictive fit, and in practice, our method delivers powerful inference without sacrificing predictive accuracy, in contrast to data splitting methods."}
{"id": "2511.15446", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15446", "abs": "https://arxiv.org/abs/2511.15446", "authors": ["Alexej Brauer", "Mario V. Wüthrich"], "title": "Gini Score under Ties and Case Weights", "comment": null, "summary": "The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights."}
{"id": "2511.15075", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15075", "abs": "https://arxiv.org/abs/2511.15075", "authors": ["Max Sampson", "Kung-Sik Chan"], "title": "Individualized Prediction Bands in Causal Inference with Continuous Treatments", "comment": null, "summary": "Individualized treatments are crucial for optimal decision making and treatment allocation, specifically in personalized medicine based on the estimation of an individual's dose-response curve across a continuum of treatment levels, e.g., drug dosage. Current works focus on conditional mean and median estimates, which are useful but do not provide the full picture. We propose viewing causal inference with a continuous treatment as a covariate shift. This allows us to leverage existing weighted conformal prediction methods with both quantile and point estimates to compute individualized uncertainty quantification for dose-response curves. Our method, individualized prediction bands (IPB), is demonstrated via simulations and a real data analysis, which demonstrates the additional medical expenditure caused by continued smoking for selected individuals. The results demonstrate that IPB provides an effective solution to a gap in individual dose-response uncertainty quantification literature."}
{"id": "2511.15075", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15075", "abs": "https://arxiv.org/abs/2511.15075", "authors": ["Max Sampson", "Kung-Sik Chan"], "title": "Individualized Prediction Bands in Causal Inference with Continuous Treatments", "comment": null, "summary": "Individualized treatments are crucial for optimal decision making and treatment allocation, specifically in personalized medicine based on the estimation of an individual's dose-response curve across a continuum of treatment levels, e.g., drug dosage. Current works focus on conditional mean and median estimates, which are useful but do not provide the full picture. We propose viewing causal inference with a continuous treatment as a covariate shift. This allows us to leverage existing weighted conformal prediction methods with both quantile and point estimates to compute individualized uncertainty quantification for dose-response curves. Our method, individualized prediction bands (IPB), is demonstrated via simulations and a real data analysis, which demonstrates the additional medical expenditure caused by continued smoking for selected individuals. The results demonstrate that IPB provides an effective solution to a gap in individual dose-response uncertainty quantification literature."}
{"id": "2511.15543", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15543", "abs": "https://arxiv.org/abs/2511.15543", "authors": ["Georgios Venianakis", "Constantinos Theodoropoulos", "Michail Kavousanakis"], "title": "A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation", "comment": null, "summary": "Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions."}
{"id": "2511.15099", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15099", "abs": "https://arxiv.org/abs/2511.15099", "authors": ["Ethan Ashby", "Dean Follmann", "Holly Janes", "Peter B. Gilbert", "Ting Ye", "Lindsey R. Baden", "Hana M. El Sahly", "Bo Zhang"], "title": "Debiasing hazard-based, time-varying vaccine effects using vaccine-irrelevant infections: An observational extension of a pivotal Phase 3 COVID-19 vaccine efficacy trial", "comment": "4 figures", "summary": "Understanding how vaccine effectiveness (VE) changes over time can provide evidence-based guidance for public health decision making. While commonly reported by practitioners, time-varying VE estimates obtained using Cox regression are vul- nerable to hidden biases. To address these limitations, we describe how to leverage vaccine-irrelevant infections to identify hazard-based, time-varying VE in the pres- ence of unmeasured confounding and selection bias. We articulate assumptions under which our approach identifies a causal effect of an intervention deferring vaccination and interaction with the community in which infections circulate. We develop sieve and efficient influence curve-based estimators and discuss imposing monotone shape constraints and estimating VE against multiple variants. As a case study, we examine the observational booster phase of the Coronavirus Vaccine Efficacy (COVE) trial of the Moderna mRNA-1273 COVID-19 vaccine which used symptom-triggered multi- plex PCR testing to identify acute respiratory illnesses (ARIs) caused by SARS-CoV-2 and 20 off-target pathogens previously identified as compelling negative controls for COVID-19. Accounting for vaccine-irrelevant ARIs supported that the mRNA-1273 booster was more effective and durable against Omicron COVID-19 than suggested by Cox regression. Our work offers an approach to mitigate bias in hazard-based, time- varying treatment effects in randomized and non-randomized studies using negative controls."}
{"id": "2511.15099", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15099", "abs": "https://arxiv.org/abs/2511.15099", "authors": ["Ethan Ashby", "Dean Follmann", "Holly Janes", "Peter B. Gilbert", "Ting Ye", "Lindsey R. Baden", "Hana M. El Sahly", "Bo Zhang"], "title": "Debiasing hazard-based, time-varying vaccine effects using vaccine-irrelevant infections: An observational extension of a pivotal Phase 3 COVID-19 vaccine efficacy trial", "comment": "4 figures", "summary": "Understanding how vaccine effectiveness (VE) changes over time can provide evidence-based guidance for public health decision making. While commonly reported by practitioners, time-varying VE estimates obtained using Cox regression are vul- nerable to hidden biases. To address these limitations, we describe how to leverage vaccine-irrelevant infections to identify hazard-based, time-varying VE in the pres- ence of unmeasured confounding and selection bias. We articulate assumptions under which our approach identifies a causal effect of an intervention deferring vaccination and interaction with the community in which infections circulate. We develop sieve and efficient influence curve-based estimators and discuss imposing monotone shape constraints and estimating VE against multiple variants. As a case study, we examine the observational booster phase of the Coronavirus Vaccine Efficacy (COVE) trial of the Moderna mRNA-1273 COVID-19 vaccine which used symptom-triggered multi- plex PCR testing to identify acute respiratory illnesses (ARIs) caused by SARS-CoV-2 and 20 off-target pathogens previously identified as compelling negative controls for COVID-19. Accounting for vaccine-irrelevant ARIs supported that the mRNA-1273 booster was more effective and durable against Omicron COVID-19 than suggested by Cox regression. Our work offers an approach to mitigate bias in hazard-based, time- varying treatment effects in randomized and non-randomized studies using negative controls."}
{"id": "2511.15615", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15615", "abs": "https://arxiv.org/abs/2511.15615", "authors": ["Gábor Balázs"], "title": "Near-optimal delta-convex estimation of Lipschitz functions", "comment": "41 pages, 7 figures", "summary": "This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors."}
{"id": "2511.15446", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.15446", "abs": "https://arxiv.org/abs/2511.15446", "authors": ["Alexej Brauer", "Mario V. Wüthrich"], "title": "Gini Score under Ties and Case Weights", "comment": null, "summary": "The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights."}
{"id": "2511.15155", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15155", "abs": "https://arxiv.org/abs/2511.15155", "authors": ["Rajan Shankar", "Ines Wilms", "Jakob Raymaekers", "Garth Tarr"], "title": "Robust outlier-adjusted mean-shift estimation of state-space models", "comment": "30 pages, 7 figures, 4 tables", "summary": "State-space models (SSMs) provide a flexible framework for modelling time series data, but their reliance on Gaussian error assumptions makes them highly sensitive to outliers. We propose a robust estimation method, ROAMS, that mitigates the influence of additive outliers by introducing shift parameters at each timepoint in the observation equation of the SSM. These parameters allow the model to attribute non-zero shifts to outliers while leaving clean observations unaffected. ROAMS then enables automatic outlier detection, through the addition of a penalty term on the number of flagged outlying timepoints in the objective function, and simultaneous estimation of model parameters. We apply the method to robustly estimate SSMs on both simulated data and real-world animal location-tracking data, demonstrating its ability to produce more reliable parameter estimates than classical methods and other benchmark methods. In addition to improved robustness, ROAMS offers practical diagnostic tools, including BIC curves for selecting tuning parameters and visualising outlier structure. These features make our approach broadly useful for researchers and practitioners working with contaminated time series data."}
{"id": "2511.15634", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15634", "abs": "https://arxiv.org/abs/2511.15634", "authors": ["Benjamin Dupuis", "Mert Gürbüzbalaban", "Umut Şimşekli", "Jian Wang", "Sinan Yildirim", "Lingjiong Zhu"], "title": "Rényi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincaré Inequalities", "comment": null, "summary": "Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,δ)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established Rényi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new Rényi flow computations and the use of well-established fractional Poincaré inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art."}
{"id": "2511.15236", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15236", "abs": "https://arxiv.org/abs/2511.15236", "authors": ["Xu Liu"], "title": "Testing relevant difference in high-dimensional linear regression with applications to detect transferability", "comment": null, "summary": "Most of researchers on testing a significance of coefficient $\\ubeta$ in high-dimensional linear regression models consider the classical hypothesis testing problem $H_0^{c}: \\ubeta=\\uzero \\mbox{ versus } H_1^{c}: \\ubeta \\neq \\uzero$. We take a different perspective and study the testing problem with the null hypothesis of no relevant difference between $\\ubeta$ and $\\uzero$, that is, $H_0: \\|\\ubeta\\|\\leq δ_0 \\mbox{ versus } H_1: \\|\\ubeta\\|> δ_0$, where $δ_0$ is a prespecified small constant. This testing problem is motivated by the urgent requirement to detect the transferability of source data in the transfer learning framework. We propose a novel test procedure incorporating the estimation of the largest eigenvalue of a high-dimensional covariance matrix with the assistance of the random matrix theory. In the more challenging setting in the presence of high-dimensional nuisance parameters, we establish the asymptotic normality for the proposed test statistics under both the null and alternative hypotheses. By applying the proposed test approaches to detect the transferability of source data, the unified transfer learning models simultaneously achieve lower estimation and prediction errors with comparison to existing methods. We study the finite-sample properties of the new test by means of simulation studies and illustrate its performance by analyzing the GTEx data."}
{"id": "2511.15679", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15679", "abs": "https://arxiv.org/abs/2511.15679", "authors": ["Jianqiao Mao", "Max A. Little"], "title": "Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion", "comment": "16 pages, 3 figures", "summary": "Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\\left(\\boldsymbol{X}^{*},\\boldsymbol{Y}^{*},\\boldsymbol{M}^{*}\\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs."}
{"id": "2511.15320", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15320", "abs": "https://arxiv.org/abs/2511.15320", "authors": ["Shu Tamano", "Yui Tomo"], "title": "Location--Scale Calibration for Generalized Posterior", "comment": null, "summary": "General Bayesian updating replaces the likelihood with a loss scaled by a learning rate, but posterior uncertainty can depend sharply on that scale. We propose a simple post-processing that aligns generalized posterior draws with their asymptotic target, yielding uncertainty quantification that is invariant to the learning rate. We prove total-variation convergence for generalized posteriors with an effective sample size, allowing sample-size-dependent priors, non-i.i.d. observations, and convex penalties under model misspecification. Within this framework, we justify and extend the open-faced sandwich adjustment (Shaby, 2014), provide general theoretical guarantees for its use within generalized Bayes, and extend it from covariance rescaling to a location--scale calibration whose draws converge in total variation to the target for any learning rate. In our empirical illustration, calibrated draws maintain stable coverage, interval width, and bias over orders of magnitude in the learning rate and closely track frequentist benchmarks, whereas uncalibrated posteriors vary markedly."}
{"id": "2511.15330", "categories": ["stat.ME", "q-bio.GN", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15330", "abs": "https://arxiv.org/abs/2511.15330", "authors": ["Marta S. Lemanczyk", "Lucas Kock", "Johanna Schlimme", "Nadja Klein", "Bernhard Y. Renard"], "title": "BaGGLS: A Bayesian Shrinkage Framework for Interpretable Modeling of Interactions in High-Dimensional Biological Data", "comment": null, "summary": "Biological data sets are often high-dimensional, noisy, and governed by complex interactions among sparse signals. This poses major challenges for interpretability and reliable feature selection. Tasks such as identifying motif interactions in genomics exemplify these difficulties, as only a small subset of biologically relevant features (e.g., motifs) are typically active, and their effects are often non-linear and context-dependent. While statistical approaches often result in more interpretable models, deep learning models have proven effective in modeling complex interactions and prediction accuracy, yet their black-box nature limits interpretability. We introduce BaGGLS, a flexible and interpretable probabilistic binary regression model designed for high-dimensional biological inference involving feature interactions. BaGGLS incorporates a Bayesian group global-local shrinkage prior, aligned with the group structure introduced by interaction terms. This prior encourages sparsity while retaining interpretability, helping to isolate meaningful signals and suppress noise. To enable scalable inference, we employ a partially factorized variational approximation that captures posterior skewness and supports efficient learning even in large feature spaces. In extensive simulations, we can show that BaGGLS outperforms the other methods with regard to interaction detection and is many times faster than MCMC sampling under the horseshoe prior. We also demonstrate the usefulness of BaGGLS in the context of interaction discovery from motif scanner outputs and noisy attribution scores from deep learning models. This shows that BaGGLS is a promising approach for uncovering biologically relevant interaction patterns, with potential applicability across a range of high-dimensional tasks in computational biology."}
{"id": "2511.15330", "categories": ["stat.ME", "q-bio.GN", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.15330", "abs": "https://arxiv.org/abs/2511.15330", "authors": ["Marta S. Lemanczyk", "Lucas Kock", "Johanna Schlimme", "Nadja Klein", "Bernhard Y. Renard"], "title": "BaGGLS: A Bayesian Shrinkage Framework for Interpretable Modeling of Interactions in High-Dimensional Biological Data", "comment": null, "summary": "Biological data sets are often high-dimensional, noisy, and governed by complex interactions among sparse signals. This poses major challenges for interpretability and reliable feature selection. Tasks such as identifying motif interactions in genomics exemplify these difficulties, as only a small subset of biologically relevant features (e.g., motifs) are typically active, and their effects are often non-linear and context-dependent. While statistical approaches often result in more interpretable models, deep learning models have proven effective in modeling complex interactions and prediction accuracy, yet their black-box nature limits interpretability. We introduce BaGGLS, a flexible and interpretable probabilistic binary regression model designed for high-dimensional biological inference involving feature interactions. BaGGLS incorporates a Bayesian group global-local shrinkage prior, aligned with the group structure introduced by interaction terms. This prior encourages sparsity while retaining interpretability, helping to isolate meaningful signals and suppress noise. To enable scalable inference, we employ a partially factorized variational approximation that captures posterior skewness and supports efficient learning even in large feature spaces. In extensive simulations, we can show that BaGGLS outperforms the other methods with regard to interaction detection and is many times faster than MCMC sampling under the horseshoe prior. We also demonstrate the usefulness of BaGGLS in the context of interaction discovery from motif scanner outputs and noisy attribution scores from deep learning models. This shows that BaGGLS is a promising approach for uncovering biologically relevant interaction patterns, with potential applicability across a range of high-dimensional tasks in computational biology."}
{"id": "2511.15366", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15366", "abs": "https://arxiv.org/abs/2511.15366", "authors": ["Ao Huang", "Christian Röver", "Tim Friede"], "title": "Utilizing subgroup information in random-effects meta-analysis of few studies", "comment": "30 pages, 6 figures", "summary": "Random-effects meta-analyses are widely used for evidence synthesis in medical research. However, conventional methods based on large-sample approximations often exhibit poor performance in case of very few studies (e.g., 2 to 4), which is very common in practice. Existing methods aiming to improve small-sample performance either still suffer from poor estimates of heterogeneity or result in very wide confidence intervals. Motivated by meta-analyses evaluating surrogate outcomes, where units nested within a trial are often exploited when the number of trials is small, we propose an inference approach based on a common-effect estimator synthesizing data from the subgroup-level instead of the study-level. Two DerSimonian-Laird type heterogeneity estimators are derived using the subgroup-level data, and are incorporated into the Henmi-Copas type variance to adequately reflect variance components. We considered t-quantile based intervals to account for small-sample properties and used flexible degrees of freedom to reduce interval lengths. A comprehensive simulation is conducted to study the performance of our methods depending on various magnitudes of subgroup effects as well as subgroup prevalences. Some general recommendations are provided on how to select the subgroups, and methods are illustrated using two example applications."}
{"id": "2511.15453", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15453", "abs": "https://arxiv.org/abs/2511.15453", "authors": ["Ryunosuke Miyazaki", "Yoshimasa Uematsu"], "title": "Testing Conditional Independence via the Spectral Generalized Covariance Measure: Beyond Euclidean Data", "comment": null, "summary": "We propose a conditional independence (CI) test based on a new measure, the \\emph{spectral generalized covariance measure} (SGCM). The SGCM is constructed by approximating the basis expansion of the squared norm of the conditional cross-covariance operator, using data-dependent bases obtained via spectral decompositions of empirical covariance operators. This construction avoids direct estimation of conditional mean embeddings and reduces the problem to scalar-valued regressions, resulting in robust finite-sample size control. Theoretically, we derive the limiting distribution of the SGCM statistic, establish the validity of a wild bootstrap for inference, and obtain uniform asymptotic size control under doubly robust conditions. As an additional contribution, we show that exponential kernels induced by continuous semimetrics of negative type are characteristic on general Polish spaces -- with extensions to finite tensor products -- thereby providing a foundation for applying our test and other kernel methods to complex objects such as distribution-valued data and curves on metric spaces. Extensive simulations indicate that the SGCM-based CI test attains near-nominal size and exhibits power competitive with or superior to state-of-the-art alternatives across a range of challenging scenarios."}
{"id": "2511.15495", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15495", "abs": "https://arxiv.org/abs/2511.15495", "authors": ["Taehyoung Kim", "Seohwa Hwang", "Junyong Park"], "title": "FDR Control via Neural Networks under Covariate-Dependent Symmetric Nulls", "comment": "35 pages, 9 figures. Code available on GitHub", "summary": "In modern multiple hypothesis testing, the availability of covariate information alongside the primary test statistics has motivated the development of more powerful and adaptive inference methods. However, most existing approaches rely on p-values that are precomputed under the assumption that their null distributions are independent of the covariates. In this paper, we propose a framework that derives covariate-adaptive p-values from the assumption of a symmetric null distribution of the primary variable given the covariates, without imposing any parametric assumptions. Building on these data-driven p-values, we employ a neural network model to learn a covariate-adaptive rejection threshold via the mirror estimation principle, optimizing the number of discoveries while maintaining valid false discovery rate control. Furthermore, our estimation of the conditional null distribution enables the computation of p-values directly from the raw data. The proposed method provides a principled way to derive covariate-adjusted p-values from raw data and allows seamless integration with previously established p-value based procedures. Simulation studies show that the proposed method outperforms existing approaches in terms of power. We further illustrate its applicability through two real data analyses: age-specific blood pressure data and U.S. air pollution data."}
{"id": "2511.15561", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.15561", "abs": "https://arxiv.org/abs/2511.15561", "authors": ["Louison Bocquet-Nouaille", "Jérôme Morio", "Benjamin Bobbia"], "title": "Variance-reduced extreme value index estimators using control variates in a semi-supervised setting", "comment": null, "summary": "The estimation of the Extreme Value Index (EVI) is fundamental in extreme value analysis but suffers from high variance due to reliance on only a few extreme observations. We propose a control variates based transfer learning approach in a semi-supervised framework, where a small set of coupled target and source observations is combined with abundant unpaired source data. By expressing the Hill estimator of the target EVI as a ratio of means, we apply approximate control variates to both numerator and denominator, with jointly optimized coefficients that guarantee variance reduction without introducing bias. We show theoretically and through simulations that the asymptotic relative variance reduction of the transferred Hill estimator is proportional to the tail dependence between the target and source variables and independent of their EVI values. Thus, substantial variance reduction can be achieved even without similarity in tail heaviness of the target and source distributions. The proposed approach can be extended to other EVI estimators expressed with ratio of means, as demonstrated on the moment estimator. The practical value of the proposed method is illustrated on multi-fidelity water surge and ice accretion datasets."}
{"id": "2511.14784", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14784", "abs": "https://arxiv.org/abs/2511.14784", "authors": ["Sourav De", "Koustav Chowdhury", "Bibhabasu Mandal", "Sagar Ghosh", "Swagatam Das", "Debolina Paul", "Saptarshi Chakraborty"], "title": "Convex Clustering Redefined: Robust Learning with the Median of Means Estimator", "comment": "Accepted in AAAI 2026", "summary": "Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches."}
{"id": "2511.14893", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14893", "abs": "https://arxiv.org/abs/2511.14893", "authors": ["Changjun Li", "Heather Allore", "Michael O. Harhay", "Fan Li", "Guangyu Tong"], "title": "Uncovering Treatment Effect Heterogeneity in Pragmatic Gerontology Trials", "comment": null, "summary": "Detecting heterogeneity in treatment response enriches the interpretation of gerontologic trials. In aging research, estimating the effect of the intervention on clinically meaningful outcomes faces analytical challenges when it is truncated by death. For example, in the Whole Systems Demonstrator trial, a large cluster-randomized study evaluating telecare among older adults, the overall effect of the intervention on quality of life was found to be null. However, this marginal intervention estimate obscures potential heterogeneity of individuals responding to the intervention, particularly among those who survive to the end of follow-up. To explore this heterogeneity, we adopt a causal framework grounded in principal stratification, targeting the Survivor Average Causal Effect (SACE)-the treatment effect among \"always-survivors,\" or those who would survive regardless of treatment assignment. We extend this framework using Bayesian Additive Regression Trees (BART), a nonparametric machine learning method, to flexibly model both latent principal strata and stratum-specific potential outcomes. This enables the estimation of the Conditional SACE (CSACE), allowing us to uncover variation in treatment effects across subgroups defined by baseline characteristics. Our analysis reveals that despite the null average effect, some subgroups experience distinct quality of life benefits (or lack thereof) from telecare, highlighting opportunities for more personalized intervention strategies. This study demonstrates how embedding machine learning methods, such as BART, within a principled causal inference framework can offer deeper insights into trial data with complex features including truncation by death and clustering-key considerations in analyzing pragmatic gerontology trials."}
{"id": "2511.15332", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.15332", "abs": "https://arxiv.org/abs/2511.15332", "authors": ["The Tien Mai"], "title": "Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss", "comment": null, "summary": "In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.\n  Our method is implemented in the \\texttt{R} package \\texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso"}
