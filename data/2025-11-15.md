<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 14]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Heuristic Solutions for the Best Secretary Problem](https://arxiv.org/abs/2511.10206)
*Eugene Seong*

Main category: stat.AP

TL;DR: 提出了一个启发式框架来解决最佳秘书问题，包含五个数据响应规则：期望记录阈值、自适应偏差校正、概率早期接受规则、两阶段松弛和局部动态规划近似。这些规则在模拟中显示与传统最优规则相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 扩展经典的固定截止方法，开发能够根据积累信息动态调整阈值的数据响应规则，以适应动态决策环境。

Method: 开发了五种启发式规则：期望记录阈值、自适应偏差校正、概率早期接受规则、两阶段松弛和局部动态规划近似，这些规则能够顺序调整阈值。

Result: 模拟实验表明，这些启发式规则在稳定性和效率上匹配或超过传统最优规则，其中期望记录规则表现强劲，自适应校正规则在非对称情况下表现良好，自适应和概率规则减少了平均停止时间。

Conclusion: 少数直观参数就能实现接近最优的结果，证明数据响应启发式方法可以有效扩展基于排名的最优停止问题到动态决策环境。

Abstract: This paper introduces a heuristic framework for the Best Secretary Problem, where one item must be selected using rank information only. We develop five data-responsive rules extending classical fixed-cutoff methods: an expected-record threshold, an adaptive deviation correction, a probabilistic early-accept rule, a two-phase relaxation, and a local dynamic programming approximation. These rules adjust thresholds sequentially as information accumulates. Simulations across diverse sample sizes, distributions, and autocorrelated settings show that the heuristics match or exceed traditional optimal rules in stability and efficiency. The expected-record rule remains strong despite its simplicity, the adaptive correction performs well under asymmetry, and the adaptive and probabilistic rules reduce average stopping times. An ensemble combining multiple rules yields the most stable performance. Overall, a few intuitive parameters achieve near-optimal results, demonstrating that data-responsive heuristics can effectively extend rank-based optimal stopping to dynamic decision environments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [2] [Siegel Neural Networks](https://arxiv.org/abs/2511.09577)
*Xuan Son Nguyen,Aymeric Histace,Nistor Grozavu*

Main category: stat.ML

TL;DR: 提出了一种在Siegel空间上构建判别性神经网络的新方法，包括多类逻辑回归和全连接层，并在雷达杂波分类和节点分类任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: Riemannian对称空间如双曲空间和对称正定流形已成为表示学习的流行空间，但Siegel空间作为RSS家族在机器学习任务中尚未充分探索。

Method: 基于这些空间的商结构和RSS上的向量值距离概念，构建Siegel神经网络的多类逻辑回归和全连接层。

Result: 在雷达杂波分类和节点分类两个应用中，该方法在所有数据集上都取得了最先进的性能。

Conclusion: 该方法成功地将神经网络扩展到Siegel空间，为这一未被充分探索的Riemannian对称空间家族提供了有效的机器学习工具。

Abstract: Riemannian symmetric spaces (RSS) such as hyperbolic spaces and symmetric positive definite (SPD) manifolds have become popular spaces for representation learning. In this paper, we propose a novel approach for building discriminative neural networks on Siegel spaces, a family of RSS that is largely unexplored in machine learning tasks. For classification applications, one focus of recent works is the construction of multiclass logistic regression (MLR) and fully-connected (FC) layers for hyperbolic and SPD neural networks. Here we show how to build such layers for Siegel neural networks. Our approach relies on the quotient structure of those spaces and the notation of vector-valued distance on RSS. We demonstrate the relevance of our approach on two applications, i.e., radar clutter classification and node classification. Our results successfully demonstrate state-of-the-art performance across all datasets.

</details>


### [3] [Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling](https://arxiv.org/abs/2511.09722)
*Sujay Nair,Evan Coleman,Sherrie Wang,Elsa Olivetti*

Main category: stat.ML

TL;DR: 开发了一种基于生成建模的学习方法，通过掩码和填充地理空间地图来推断矿物位置，可整合辅助数据源提升预测性能


<details>
  <summary>Details</summary>
Motivation: 地下矿物资源表征成本高且具有挑战性，但对脱碳所需的先进能源技术至关重要

Method: 使用掩码和填充技术处理地理空间资源可用性地图，训练生成模型来推断矿物位置，并可整合地球物理和土壤调查等辅助数据

Result: 在美国本土48州矿物数据上，最佳模型在1平方英里空间分辨率下达到Dice系数0.31±0.01和召回率0.22±0.02

Conclusion: 该方法能有效整合辅助数据源提升矿物位置推断性能，并可在无记录矿物区域进行模型评估

Abstract: Minerals play a critical role in the advanced energy technologies necessary for decarbonization, but characterizing mineral deposits hidden underground remains costly and challenging. Inspired by recent progress in generative modeling, we develop a learning method which infers the locations of minerals by masking and infilling geospatial maps of resource availability. We demonstrate this technique using mineral data for the conterminous United States, and train performant models, with the best achieving Dice coefficients of $0.31 \pm 0.01$ and recalls of $0.22 \pm 0.02$ on test data at 1$\times$1 mi$^2$ spatial resolution. One major advantage of our approach is that it can easily incorporate auxiliary data sources for prediction which may be more abundant than mineral data. We highlight the capabilities of our model by adding input layers derived from geophysical sources, along with a nation-wide ground survey of soils originally intended for agronomic purposes. We find that employing such auxiliary features can improve inference performance, while also enabling model evaluation in regions with no recorded minerals.

</details>


### [4] [Generalized infinite dimensional Alpha-Procrustes based geometries](https://arxiv.org/abs/2511.09801)
*Salvish Goomanee,Andi Han,Pratik Jawanpuria,Bamdev Mishra*

Main category: stat.ML

TL;DR: 本文扩展了Alpha-Procrustes黎曼度量框架，引入了广义Bures-Wasserstein、Log-Euclidean和Wasserstein距离的无限维版本，通过单位化Hilbert-Schmidt算子和扩展Mahalanobis范数构建了更稳健的度量方法。


<details>
  <summary>Details</summary>
Motivation: Alpha-Procrustes框架虽然统一了许多经典度量，但缺乏实现广义距离形式的结构组件，特别是在无限维场景下需要更稳健的几何方法。

Method: 基于单位化Hilbert-Schmidt算子和扩展Mahalanobis范数的形式化方法，构建了广义GBW和Log-Hilbert-Schmidt距离，并引入了可学习的正则化参数以增强高维比较的几何稳定性。

Result: 初步实验重现文献基准表明，广义度量在涉及不同维度和尺度数据集比较的场景中表现出改进的性能。

Conclusion: 这项工作为机器学习、统计推断和函数数据分析中稳健几何方法的发展奠定了理论和计算基础。

Abstract: This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.

</details>


### [5] [Theory and computation for structured variational inference](https://arxiv.org/abs/2511.09897)
*Shunan Sheng,Bohan Wu,Bennett Zhu,Sinho Chewi,Aram-Alexandre Pooladian*

Main category: stat.ML

TL;DR: 本文研究了星型结构变分推断的理论性质，证明了变分近似的存在性、唯一性和自洽性，并推导了后验近似误差的定量界限。


<details>
  <summary>Details</summary>
Motivation: 结构化变分推断是现代统计应用中的核心方法，与均值场变分推断不同，它假设近似后验具有相互依赖的结构。本文关注星型结构变分推断的自然设置，其中一个根变量影响所有其他变量。

Method: 开发了基于梯度且具有可证明保证的算法来计算变分近似，使用了最优传输理论的思想。分析了高斯测度和层次贝叶斯模型的应用，包括具有位置族先验的广义线性模型和具有一维去偏的尖峰-平板先验。

Result: 证明了变分近似的存在性、唯一性和自洽性，推导了后验近似误差的定量界限，将先前均值场设置的工作扩展到星型结构设置。

Conclusion: 星型结构变分推断为结构化变分推断提供了坚实的理论基础，所开发的新算法具有理论保证，对高斯测度和层次贝叶斯模型具有重要应用价值。分析中还发展了星可分离传输映射的新稳定性结果，可能具有独立意义。

Abstract: Structured variational inference constitutes a core methodology in modern statistical applications. Unlike mean-field variational inference, the approximate posterior is assumed to have interdependent structure. We consider the natural setting of star-structured variational inference, where a root variable impacts all the other ones. We prove the first results for existence, uniqueness, and self-consistency of the variational approximation. In turn, we derive quantitative approximation error bounds for the variational approximation to the posterior, extending prior work from the mean-field setting to the star-structured setting. We also develop a gradient-based algorithm with provable guarantees for computing the variational approximation using ideas from optimal transport theory. We explore the implications of our results for Gaussian measures and hierarchical Bayesian models, including generalized linear models with location family priors and spike-and-slab priors with one-dimensional debiasing. As a by-product of our analysis, we develop new stability results for star-separable transport maps which might be of independent interest.

</details>


### [6] [Operator Models for Continuous-Time Offline Reinforcement Learning](https://arxiv.org/abs/2511.10383)
*Nicolas Hoischen,Petar Bevanda,Max Beier,Stefan Sosnowski,Boris Houska,Sandra Hirche*

Main category: stat.ML

TL;DR: 本文提出了基于算子理论的离线强化学习方法，通过连接强化学习与Hamilton-Jacobi-Bellman方程，在再生核希尔伯特空间中学习受控扩散过程的无穷小生成元，为连续时间最优控制提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 在医疗、自动驾驶和工业控制等领域，直接与环境交互通常不安全或不切实际，因此需要从历史数据中进行离线强化学习。然而，当前对从离线数据集中学习策略所固有的近似误差缺乏统计理解。

Method: 将强化学习与Hamilton-Jacobi-Bellman方程联系起来，提出基于简单动态规划递归的算子理论算法。在再生核希尔伯特空间中表示受控扩散过程的无穷小生成元作为世界模型，结合统计学习方法和算子理论。

Result: 建立了价值函数的全局收敛性，并推导了有限样本保证，其边界与系统的平滑性和稳定性等属性相关。理论和数值结果表明基于算子的方法在解决连续时间最优控制的离线强化学习问题中具有潜力。

Conclusion: 算子方法在利用连续时间最优控制解决离线强化学习问题方面展现出前景，为离线强化学习提供了理论基础和性能保证。

Abstract: Continuous-time stochastic processes underlie many natural and engineered systems. In healthcare, autonomous driving, and industrial control, direct interaction with the environment is often unsafe or impractical, motivating offline reinforcement learning from historical data. However, there is limited statistical understanding of the approximation errors inherent in learning policies from offline datasets. We address this by linking reinforcement learning to the Hamilton-Jacobi-Bellman equation and proposing an operator-theoretic algorithm based on a simple dynamic programming recursion. Specifically, we represent our world model in terms of the infinitesimal generator of controlled diffusion processes learned in a reproducing kernel Hilbert space. By integrating statistical learning methods and operator theory, we establish global convergence of the value function and derive finite-sample guarantees with bounds tied to system properties such as smoothness and stability. Our theoretical and numerical results indicate that operator-based approaches may hold promise in solving offline reinforcement learning using continuous-time optimal control.

</details>


### [7] [Continuum Dropout for Neural Differential Equations](https://arxiv.org/abs/2511.10446)
*Jonghun Lee,YongKyung Oh,Sungil Kim,Dong-Young Lim*

Main category: stat.ML

TL;DR: 提出了Continuum Dropout，一种基于交替更新过程的通用正则化方法，用于解决神经微分方程无法有效应用dropout的问题，提升模型泛化能力和不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 神经微分方程在建模连续时间动态方面表现出色，但难以应用dropout这一深度学习核心正则化技术，容易过拟合，需要一种专门的正则化方法。

Method: 基于交替更新过程理论，将dropout的开-关机制建模为在连续时间中交替处于活跃（演化）和非活跃（暂停）状态的随机过程。

Result: 在多个时间序列和图像分类任务上，Continuum Dropout优于现有NDE正则化方法，获得更好的校准概率估计和更可信的不确定性建模。

Conclusion: Continuum Dropout为神经微分方程提供了一种原则性的正则化框架，有效防止过拟合并增强泛化能力，同时支持通过蒙特卡洛采样进行预测不确定性量化。

Abstract: Neural Differential Equations (NDEs) excel at modeling continuous-time dynamics, effectively handling challenges such as irregular observations, missing values, and noise. Despite their advantages, NDEs face a fundamental challenge in adopting dropout, a cornerstone of deep learning regularization, making them susceptible to overfitting. To address this research gap, we introduce Continuum Dropout, a universally applicable regularization technique for NDEs built upon the theory of alternating renewal processes. Continuum Dropout formulates the on-off mechanism of dropout as a stochastic process that alternates between active (evolution) and inactive (paused) states in continuous time. This provides a principled approach to prevent overfitting and enhance the generalization capabilities of NDEs. Moreover, Continuum Dropout offers a structured framework to quantify predictive uncertainty via Monte Carlo sampling at test time. Through extensive experiments, we demonstrate that Continuum Dropout outperforms existing regularization methods for NDEs, achieving superior performance on various time series and image classification tasks. It also yields better-calibrated and more trustworthy probability estimates, highlighting its effectiveness for uncertainty-aware modeling.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [8] [Diagnostics for Semiparametric Accelerated Failure Time Models with R Package afttest](https://arxiv.org/abs/2511.09823)
*Woojung Bae,Dongrak Choi,Jun Yan,Sangwook Kang*

Main category: stat.CO

TL;DR: 本文介绍了afttest R包，用于对半参数加速失效时间模型进行诊断检验，包括模型假设评估、链接函数和协变量函数形式检验，支持秩基和最小二乘方法拟合的模型。


<details>
  <summary>Details</summary>
Motivation: 半参数加速失效时间模型是Cox比例风险模型的有用替代，其回归系数更易解释，但诊断方法研究较少。

Method: 使用Kolmogorov型检验统计量，基于变换后的聚合鞅残差过程，通过高效乘数自助法计算p值，并提供图形工具比较观测过程与近似实现。

Result: 开发了afttest R包，实现了半参数AFT模型的诊断工具，并应用于梅奥诊所原发性胆汁性肝硬化研究。

Conclusion: afttest包为半参数AFT模型提供了全面的诊断程序，填补了该领域诊断工具的空缺。

Abstract: The semiparametric accelerated failure time (AFT) model is a useful alternative to the widely used Cox proportional hazard model, which directly links the logarithm of the failure time to the covariates, yielding more interpretable regression coefficients. However, diagnostic procedures for the semiparametric AFT model have received relatively little attention. This paper introduces afttest, an R package that implements recently developed diagnostic tools for the semiparametric AFT model. The package supports diagnostic procedures for models fitted with either rank-based or least-squares methods. It provides functions to assess model assumptions, including the overall adequacy, the link function, and functional form of each covariate. The test statistics are of Kolmogorov-type suprema of transformed aggregated martingale residual processes. The p-values are obtained by approximating the null distribution with an efficient multiplier bootstrap procedure. Additionally, the package offers graphical tools to compare the observed stochastic processes with a number of approximated realizations. Applications of the package to the well-known Mayo clinic primary biliary cirrhosis study are presented.

</details>


### [9] [On The Performance of Prefix-Sum Parallel Kalman Filters and Smoothers on GPUs](https://arxiv.org/abs/2511.10363)
*Simo Särkkä,Ángel F. García-Fernández*

Main category: stat.CO

TL;DR: 本文对基于GPU的并行时间卡尔曼滤波器和平滑器进行了实验评估，比较了不同的并行扫描算法，并提出了一种新的并行时间双滤波器平滑器。


<details>
  <summary>Details</summary>
Motivation: 评估并行时间卡尔曼滤波器和平滑器在GPU上的性能，探索时间并行化算法的效率。

Method: 通过仿真计算所需操作数，并在真实GPU硬件上测量算法运行时间；提出并评估新的并行时间双滤波器平滑器。

Result: 提供了Metal和CUDA实现的Julia代码，并公开可用。

Conclusion: 并行时间卡尔曼滤波器和平滑器在GPU上具有实际应用价值，新的双滤波器平滑器表现出良好的性能。

Abstract: This paper presents an experimental evaluation of parallel-in-time Kalman filters and smoothers using graphics processing units (GPUs). In particular, the paper evaluates different all-prefix-sum algorithms, that is, parallel scan algorithms for temporal parallelization of Kalman filters and smoothers in two ways: by calculating the required number of operations via simulation, and by measuring the actual run time of the algorithms on real GPU hardware. In addition, a novel parallel-in-time two-filter smoother is proposed and experimentally evaluated. Julia code for Metal and CUDA implementations of all the algorithms is made publicly available.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [10] [Distributional Treatment Effect Estimation across Heterogeneous Sites via Optimal Transport](https://arxiv.org/abs/2511.09759)
*Borna Bateni,Yubai Yuan,Qi Xu,Annie Qu*

Main category: stat.ME

TL;DR: 提出一个新颖框架，通过整合源站点的完整治疗组和对照组数据以及目标站点的对照组数据，合成目标站点的反事实治疗组数据。该方法从分布因果推断角度建模，将跨站点异质性形式化为将联合特征-结果分布从源站点映射到目标站点的前推变换。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要关注平均治疗效果估计，但实际应用中需要理解治疗效果的完整分布特性。当目标站点缺乏治疗组数据时，如何准确合成反事实治疗组数据是一个重要挑战。

Method: 采用最优传输方法学习源站点和目标站点对照组分布之间的对齐变换，然后将该变换应用于源站点的治疗组，生成合成的目标治疗分布。

Result: 在一般正则条件下建立了合成治疗组数据与真实目标分布一致性和渐近收敛的理论保证。模拟研究和真实世界患者来源异种移植数据应用表明，该框架能稳健地恢复治疗效果的完整分布特性。

Conclusion: 该框架为跨站点治疗效果分布推断提供了有效方法，特别适用于目标站点缺乏治疗组数据的情况，在理论和实证上都表现出良好性能。

Abstract: We propose a novel framework for synthesizing counterfactual treatment group data in a target site by integrating full treatment and control group data from a source site with control group data from the target. Departing from conventional average treatment effect estimation, our approach adopts a distributional causal inference perspective by modeling treatment and control as distinct probability measures on the source and target sites. We formalize the cross-site heterogeneity (effect modification) as a push-forward transformation that maps the joint feature-outcome distribution from the source to the target site. This transformation is learned by aligning the control group distributions between sites using an Optimal Transport-based procedure, and subsequently applied to the source treatment group to generate the synthetic target treatment distribution. Under general regularity conditions, we establish theoretical guarantees for the consistency and asymptotic convergence of the synthetic treatment group data to the true target distribution. Simulation studies across multiple data-generating scenarios and a real-world application to patient-derived xenograft data demonstrate that our framework robustly recovers the full distributional properties of treatment effects.

</details>


### [11] [Masking criteria for selecting an imputation model](https://arxiv.org/abs/2511.10048)
*Yanjiao Yang,Daniel Suen,Yen-Chi Chen*

Main category: stat.ME

TL;DR: 本文研究了掩蔽一个观测值(MOO)程序在评估插补模型时的局限性，提出了三种改进的MOO标准，并建立了基于似然方法的统计学习理论框架。


<details>
  <summary>Details</summary>
Motivation: MOO程序虽然常用于比较插补模型，但它主要衡量预测准确性，不适合评估考虑数据随机性的插补模型。

Method: 提出了基于秩变换、能量距离和似然原理的三种改进MOO标准，建立了基于似然方法的统计学习理论框架，并引入了预测-插补图进行可视化比较。

Result: 推导了统计和计算学习理论以及BIC模型选择的一致性，展示了MOO与随机缺失假设的关系。

Conclusion: 改进的MOO标准能更好地评估考虑数据随机性的插补模型，似然方法提供了一个优雅的学习框架，预测-插补图为模型比较提供了直观工具。

Abstract: The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models.

</details>


### [12] [Coalescent Inference for Epidemics with Latent Periods](https://arxiv.org/abs/2511.09686)
*Isaac H. Goldstein,Julia A. Palacios*

Main category: stat.ME

TL;DR: 该论文提出了一个用于暴露-感染人群动态的溯祖模型，可以从样本谱系推断感染人数和有效再生数随时间的变化，并开发了基于Phase-type分布的贝叶斯推断框架。


<details>
  <summary>Details</summary>
Motivation: 传统的溯祖模型参数（如有效种群大小）对病原体传播动态的解释能力有限，需要开发能够直接推断流行病学参数的模型。

Method: 提出了一个两deme的溯祖模型，将人群分为暴露和感染两类，限制不同deme个体间的溯祖事件，并开发了基于Phase-type分布的数据增强贝叶斯推断框架。

Result: 在模拟研究中验证了方法的性能，并将其应用于重新分析2014年利比里亚埃博拉疫情。

Conclusion: 该模型能够从分子序列数据直接推断关键的流行病学参数，为病原体传播动态研究提供了更直观的推断工具。

Abstract: Coalescent models are used to study the transmission dynamics of rapidly evolving pathogens from molecular sequence data obtained from infected individuals. However coalescent parameters, such as effective population size, offer limited interpretability for transmission dynamics. In this work, we derive a coalescent model for exposed-infected population dynamics that allows us to infer the number of infected individuals and the effective reproduction number over time from the sample genealogy. The model can be interpreted as a two-deme model in which coalescence is restricted to individuals from different demes (exposed and infected). We propose a new data-augmentation framework with Phase-type distribution for Bayesian inference of epidemiological parameters. We study the performance of our approach on simulations and apply it to re-analyze the 2014 Ebola outbreak in Liberia.

</details>


### [13] [State Space Modeling of Mortgage Default Rates under Natural Hazard Shocks](https://arxiv.org/abs/2511.09698)
*Samuel J. Eschker,Antik Chakraborty,Melanie Gall,Peter Jevtic,Jianxi Su*

Main category: stat.ME

TL;DR: 本文研究了自然灾害经济损失与抵押贷款违约率之间的关系，采用状态空间模型来分离自然灾害损失对违约率的影响，并验证了这种关系的显著性。


<details>
  <summary>Details</summary>
Motivation: 抵押贷款违约率既是衡量经济健康状况的指标，也是保险公司资产负债管理中的关键风险因素。自然灾害造成的过度保险损失可能导致抵押贷款违约激增，给保险公司带来复合挑战。

Method: 应用状态空间建模方法，通过包含潜在变量来控制其他经济决定因素，从而分离自然灾害损失对抵押贷款违约率的影响。还考虑了经典状态空间模型的切片变体，以捕捉仅在自然灾害损失足够高时才出现的微妙关系。

Result: 模型验证了自然灾害损失与抵押贷款违约率之间关系的显著性，并提供了关于自然灾害损失如何表现为抵押贷款违约率增加的见解。

Conclusion: 研究证实了自然灾害经济损失与抵押贷款违约率之间存在显著关系，这对保险行业的风险评估和决策制定具有重要意义。

Abstract: Mortgage default rates, on the one hand, serve as a measure of economic health to support decision-making by insurance companies, and on the other hand, is a key risk factor in the asset-liability management (ALM) practice, as mortgage related assets constitute a significant proportion of insurers' investment portfolios. This paper studies the relationship between economic losses due to natural hazards and mortgage default rates. The topic is greatly relevant to the insurance industry, as excessive insurance losses from natural hazards can lead to a surge in mortgage defaults, creating compounded challenges for insurers. To this end, we apply a state-space modeling approach to decouple the effect of natural hazard losses on mortgage default rates after controlling for other economic determinants through the inclusion of latent variables. Moreover, we consider a sliced variant of the classical SSM to capture the subtle relationship that only emerges when natural hazard losses are sufficiently high. Our model verifies the significance of this relationship and provides insights into how natural hazard losses manifest as increased mortgage default rates.

</details>


### [14] [Modelos Empiricos de Pos-Dupla Selecao por LASSO: Discussoes para Estudos do Transporte Aereo](https://arxiv.org/abs/2511.09767)
*Alessandro V. M. Oliveira*

Main category: stat.ME

TL;DR: 本文介绍并讨论了使用LASSO方法进行正则化回归和模型选择的形式，包括其在计量经济学中的应用、相关概念问题以及lassopack软件包的使用，最后探讨了在航空运输研究中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 处理高维计量经济学中的大容量数据和多重相关控制变量问题，LASSO作为主要的监督学习方法能够有效应对这些挑战。

Method: 使用LASSO方法进行正则化回归和模型选择，包括后双重选择和后正则化模型，以及应用于工具变量模型的变体，使用lassopack软件包实现HD、HDS和IV-HDS模型。

Result: 提出了基于稀疏性原则的正则化程序，能够有效处理高维数据，并展示了在航空运输研究中应用LASSO方法的潜力。

Conclusion: LASSO方法为高维计量经济学提供了有效的工具，特别适用于处理大容量数据和多重相关控制变量，在航空运输等领域的实证研究中具有重要应用价值。

Abstract: This paper presents and discusses forms of estimation by regularized regression and model selection using the LASSO method - Least Absolute Shrinkage and Selection Operator. LASSO is recognized as one of the main supervised learning methods applied to high-dimensional econometrics, allowing work with large volumes of data and multiple correlated controls. Conceptual issues related to the consequences of high dimensionality in modern econometrics and the principle of sparsity, which underpins regularization procedures, are addressed. The study examines the main post-double selection and post-regularization models, including variations applied to instrumental variable models. A brief description of the lassopack routine package, its syntaxes, and examples of HD, HDS (High-Dimension Sparse), and IV-HDS models, with combinations involving fixed effects estimators, is also presented. Finally, the potential application of the approach in research focused on air transport is discussed, with emphasis on an empirical study on the operational efficiency of airlines and aircraft fuel consumption.

</details>


### [15] [A Clustering Approach for Basket Trials Based on Treatment Response Trajectories](https://arxiv.org/abs/2511.09890)
*Masahiro Kojima,Keisuke Hanada,Atsuya Sato*

Main category: stat.ME

TL;DR: 提出了一种基于治疗反应轨迹的无模型聚类框架，用于篮子试验中篮子的分组，通过自动确定聚类数量并采用分层贝叶斯模型分析，提高疗效终点估计精度和统计功效。


<details>
  <summary>Details</summary>
Motivation: 篮子试验中经常观察到篮子间的疗效异质性，传统方法仅依赖单一疗效终点可能无法充分捕捉这种异质性。

Method: 基于治疗反应轨迹的转移概率对篮子进行聚类，聚类数量通过数据驱动自动确定，同一聚类内的篮子使用分层贝叶斯模型进行分析。

Result: 模拟研究显示该方法能准确识别异质设置下的聚类结构，在维持I类错误率的同时提高统计功效。

Conclusion: 所提出的方法能有效处理篮子试验中的异质性，改善疗效估计精度和统计功效，同时控制I类错误率。

Abstract: Heterogeneity in efficacy is sometimes observed across baskets in basket trials. In this study, we propose a model-free clustering framework that groups baskets based on transition probabilities derived from the trajectories of treatment response, rather than relying solely on a single efficacy endpoint such as the objective response rate. The number of clusters is not predetermined but is automatically determined in a data-driven manner based on the similarity structure among baskets. After clustering, baskets within the same cluster are analyzed using a hierarchical Bayesian model. This framework aims to improve the estimation precision of efficacy endpoints and enhance statistical power while maintaining the type~I error rate at the nominal level. The performance of the proposed method was evaluated through simulation studies. The results demonstrated that the proposed method can accurately identify cluster structures in heterogeneous settings and, even under such conditions, maintain the type~I error rate at the nominal level while improving statistical power.

</details>


### [16] [Zeroes and Extrema of Functions via Random Measures](https://arxiv.org/abs/2511.10293)
*Athanasios Christou Micheas*

Main category: stat.ME

TL;DR: 提出了无需微分的函数零点和极值点定位方法，基于点过程理论描述其位置、数量和分布，并提供实现算法和示例验证。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要微分来寻找函数零点和极值点，本文旨在开发不依赖微分的替代方法。

Method: 使用点过程理论，通过算法实现零点和极值点的定位、计数和分布分析。

Result: 方法在实函数和复函数上通过多个示例验证有效，能够准确描述零点和极值点的特性。

Conclusion: 提出的无微分方法成功实现了函数零点和极值点的分析，为相关研究提供了新工具。

Abstract: We present methods that provide all zeroes and extrema of a function that do not require differentiation. Using point process theory, we are able to describe the locations of zeroes or maxima, their number, as well as their distribution over a given window of observation. The algorithms in order to accomplish the theoretical development are also provided, and they are exemplified using many illustrative examples, for real and complex functions.

</details>


### [17] [A tutorial for propensity score weighting methods under violations of the positivity assumption](https://arxiv.org/abs/2511.10077)
*Yi Liu,Yuan Wang,Ying Gao,Tonia Poteat,Roland A. Matsouaka*

Main category: stat.ME

TL;DR: 本教程综述了倾向得分加权方法的最新进展，提供了选择目标估计量、实施相应估计器以及进行加权后诊断评估的实践指导，并开发了R包ChiPS。


<details>
  <summary>Details</summary>
Motivation: 当违背正性假设时，传统的因果估计量（如ATE、ATT、ATC）可能无法识别，需要转向其加权替代版本（WATE、WATT、WATC）来保持内部有效性。

Method: 通过倾向得分加权方法，包括选择主要目标估计量、实施相应的PS加权估计器、进行加权后诊断评估，并开发了用户友好的R包ChiPS。

Result: 通过广泛的模拟研究验证了各种估计器的适用性，并在两个真实案例（吸烟对血铅水平的影响、性工作史对南非跨性别女性HIV状态的影响）中展示了方法流程。

Conclusion: 加权因果估计量为在违背正性假设情况下进行因果推断提供了有价值的替代方案，本教程提供了完整的实践指导框架。

Abstract: Violations of the positivity assumption can render conventional causal estimands unidentifiable, including the average treatment effect (ATE), the average treatment effect on the treated (ATT), and the average treatment effect on the controls (ATC). Shifting the inferential focus to their alternative counterparts -- the weighted ATE (WATE), the weighted ATT (WATT), and the weighted ATC (WATC) -- offers valuable insights into treatment effects while preserving internal validity. In this tutorial, we provide a comprehensive review of recent advances in propensity score (PS) weighting methods, along with practical guidance on how to select a primary target estimand (while other estimands serve as supplementary analyses), implement the corresponding PS-weighted estimators, and conduct post-weighting diagnostic assessments. The tutorial is accompanied by a user-friendly R package, ChiPS. We demonstrate the pertinence of various estimators through extensive simulation studies. We illustrate the flow of the tutorial on two real-world case studies: (i) Effect of smoking on blood lead level using data from the 2007-2008 National Health and Nutrition Examination Survey (NHANES); and (ii) Impact of history of sex work on HIV status among transgender women in South Africa.

</details>


### [18] [Multiple Treatments Causal Effects Estimation with Task Embeddings and Balanced Representation Learning](https://arxiv.org/abs/2511.09814)
*Yuki Murakami,Takumi Hattori,Kohsuke Kubota*

Main category: stat.ME

TL;DR: 提出了一种结合任务嵌入网络和表示学习网络的新深度学习框架，用于估计多治疗场景下的单治疗效应和交互治疗效应，通过参数共享和平衡惩罚提高估计准确性。


<details>
  <summary>Details</summary>
Motivation: 多治疗同时应用在医疗和营销等领域日益普遍，需要准确估计单治疗效应和交互治疗效应。现有方法存在参数共享不足或不必要潜变量估计的问题。

Method: 结合任务嵌入网络（实现相关治疗模式间的参数共享）和表示学习网络（通过平衡惩罚减少不同治疗模式间表示分布的差异），非参数学习观测协变量的表示。

Result: 模拟研究显示该方法优于现有基线方法，在真实营销数据集上的应用证实了其实际意义和实用性。

Conclusion: 所提出的框架通过参数共享和平衡表示学习，有效解决了多治疗效应估计中的选择偏差和模型误设问题，具有较好的性能和实用价值。

Abstract: The simultaneous application of multiple treatments is increasingly common in many fields, such as healthcare and marketing. In such scenarios, it is important to estimate the single treatment effects and the interaction treatment effects that arise from treatment combinations. Previous studies have proposed using independent outcome networks with subnetworks for interactions, or combining task embedding networks that capture treatment similarity with variational autoencoders. However, these methods suffer from the lack of parameter sharing among related treatments, or the estimation of unnecessary latent variables reduces the accuracy of causal effect estimation. To address these issues, we propose a novel deep learning framework that incorporates a task embedding network and a representation learning network with the balancing penalty. The task embedding network enables parameter sharing across related treatment patterns because it encodes elements common to single effects and contributions specific to interaction effects. The representation learning network with the balancing penalty learns representations nonparametrically from observed covariates while reducing distances in representation distributions across different treatment patterns. This process mitigates selection bias and avoids model misspecification. Simulation studies demonstrate that the proposed method outperforms existing baselines, and application to real-world marketing datasets confirms the practical implications and utility of our framework.

</details>


### [19] [Goodness-of-fit Test for Generalized Functional Linear Models via Projection Averaging](https://arxiv.org/abs/2511.09886)
*Feifei Chen,Kaiming Zhang,Yanni Zhang,Hua Liang*

Main category: stat.ME

TL;DR: 本文提出了一种新的广义函数线性模型(GFLM)拟合优度检验方法，通过Cramér-von-Mises度量和一维投影平均策略构建U统计量，有效缓解维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 广义函数线性模型在标量响应与函数预测变量建模中广泛应用，但目前缺乏正式的拟合优度检验程序，现有文献对此主题的研究有限。

Method: 构建基于Cramér-von-Mises度量的U统计量，通过函数预测变量所有一维投影的积分平均来缓解维度灾难，并建立了渐近正态性和一致性理论。

Result: 模拟研究证实该检验方法在各种设置下具有良好的功效表现，优于现有方法。

Conclusion: 提出的检验方法为GFLM提供了有效的拟合优度评估工具，通过投影平均策略成功解决了高维问题，并提供了实用的bootstrap重抽样方法来计算p值。

Abstract: Assessing model adequacy is a crucial step in regression analysis, ensuring the validity of statistical inferences. For Generalized Functional Linear Models (GFLMs), which are widely used for modeling relationships between scalar responses and functional predictors, there is a recognized need for formal goodness-of-fit testing procedures. Current literature on this specific topic remains limited. This paper introduces a novel goodness-of-fit test for GFLMs. The test statistic is formulated as a U-statistic derived from a Cramér-von-Mises metric integrated over all one-dimensional projections of the functional predictor. This projection averaging strategy is designed to effectively mitigate the curse of dimensionality. We establish the asymptotic normality of the test statistic under the null hypothesis and prove the consistency under the alternatives. As the asymptotic variance of the limiting null distribution can be complex for practical use, we also propose practical bootstrap resampling methods for both continuous and discrete responses to compute p-values. Simulation studies confirm that the proposed test demonstrates good power performance across various settings, showing advantages over existing methods.

</details>


### [20] [Leader-Follower Identification Methodology for Non-Lane Disciplined Heterogeneous Traffic Using Steady State Features](https://arxiv.org/abs/2511.09946)
*Susan Eldhose,Bhargava Rama Chilukuri,Chandrasekharan Rajendran*

Main category: stat.ME

TL;DR: 提出了一种用于混合交通条件下领导者-跟随者对识别的自适应方法，通过速度-间隙一致性、相对速度变化和小波变换分析来过滤虚假配对，提高车辆跟驰行为建模的准确性。


<details>
  <summary>Details</summary>
Motivation: 发展中国家混合交通条件下的车辆跟驰行为识别面临挑战，传统基于固定阈值的方法容易误分类非跟随实例为有效领导者-跟随者对，影响模型性能。

Method: 采用三阶段过滤：速度-间隙一致性检查、通过相对速度符号变化和间隙范围的接近/分离检测、使用墨西哥帽小波变换分析速度相关性以确认领导者对跟随者的影响。

Result: 框架有效过滤了与超车、尾随和不一致间隙动态相关的领导者-跟随者对，保留了具有一致跟驰行为的配对。分析显示对称配对具有更高的可预测性，而不对称配对表现出更大的变异性。

Conclusion: 该框架为交通建模和行为分析提供了稳健基础，能够准确识别混合交通条件下的车辆跟驰行为。

Abstract: Road traffic in developing countries, such as India, features a heterogeneous mix of vehicles operating under weak lane discipline (HWLD), encompassing both motorised and non-motorised modes with diverse sizes and manoeuvrability. These conditions lead to complex driver interactions, complicating the reliable identification of vehicle-following (VF) behaviour and leader-follower (LF) pairs. Traditional identification methods based on fixed thresholds for longitudinal and lateral proximity often misclassify non-following instances as valid LF pairs, degrading model performance. This study presents a refined and adaptive method for LF identification in HWLD traffic. It employs vehicle-type- and speed-specific desirable gap thresholds derived from the fundamental density-speed diagram to eliminate false-positive pairs. Additionally, Mexican Hat Wavelet Transform (MWT) is employed to analyse LV and SV speed profiles, verifying LV-SV interaction for LF pair identification. The three-stage filtering includes: (i) speed-gap consistency, (ii) approach/diverge detection via relative velocity sign changes and gap range, and (iii) wavelet-based speed correlation using MWT to confirm LV influence on SV. The framework effectively filters out LF pairs associated with overtaking, tailgating, and inconsistent gap dynamics, retaining only those with consistent VF behaviour and improving model accuracy. Analysis across thirteen LF combinations shows that VF dynamics depend on both SV and LV types. Symmetric pairs (e.g., CAR-CAR, AUTO-CAR) exhibit higher predictability and lower errors, while asymmetric pairs with heavy vehicles or two-wheelers show greater variability. The framework offers a robust foundation for traffic modelling and behaviour analysis.

</details>


### [21] [Addressing zero-inflated and mis-measured functional predictors in scalar-on-function regression model](https://arxiv.org/abs/2511.09972)
*Heyang Ji,Lan Xue,Ufuk Beyaztas,Roger S. Zoh,Jeff Goldsmith,Mark E. Benden,Carmen D. Tekwe*

Main category: stat.ME

TL;DR: 提出半连续建模方法，用于调整标量对函数回归模型中零膨胀和测量误差引起的偏差


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备监测身体活动时会产生过量零值（久坐行为或未佩戴时间），且测量精度受活动强度和佩戴时间影响，现有研究较少同时考虑零膨胀和测量误差对多级标量对函数回归模型的影响

Method: 开发半连续建模方法，提供理论证明，并通过大量模拟验证有限样本性质

Result: 方法在模拟中表现出良好的有限样本性质，并成功应用于学校干预研究，分析学日身体活动与年龄性别调整BMI的关联

Conclusion: 提出的方法能有效调整零膨胀和测量误差带来的偏差，为可穿戴设备数据在多级标量对函数回归模型中的应用提供了可靠工具

Abstract: Wearable devices are often used in clinical and epidemiological studies to monitor physical activity behavior and its influence on health outcomes. These devices are worn over multiple days to record activity patterns, such as step counts recorded at the minute level, resulting in multi-level, longitudinal, high-dimensional, or functional data. When monitoring patterns of step counts over multiple days, devices may record excess zeros during periods of sedentary behavior or non-wear times. Additionally, it has been demonstrated that the accuracy of wearable devices in monitoring true physical activity patterns depends on the intensity of the activities and wear times. While work on adjusting for biases due to measurement errors in functional data is a growing field, relatively less work has been done to study the occurrence of excess zeros along with measurement errors and their combined influence on estimation and inference in multi-level scalar-on-function regression models. We propose semi-continuous modeling approaches to adjust for biases due to zero inflation and measurement errors in scalar-on-function regression models. We provide theoretical justifications for our proposed methods and, through extensive simulations, we demonstrated their finite sample properties. Finally, the developed methods are applied to a school-based intervention study examining the association between school day physical activity with age- and sex-adjusted body mass index among elementary school-aged children.

</details>


### [22] [Outlier-robust copula regression for bivariate continuous proportions: an application to cushion plant vitality](https://arxiv.org/abs/2511.10016)
*Divan A. Burger,Janet van Niekerk,Peter C. le Roux,Morgan J. Raath-Krüger*

Main category: stat.ME

TL;DR: 提出一种贝叶斯copula建模方法，结合矩形-贝塔边缘分布和单参数copula来处理连续比例数据中的内部异常值和残差依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 连续比例数据常面临两个挑战：内部异常值导致方差超出贝塔分布上限，以及残差依赖性使独立边缘模型失效。

Method: 使用矩形-贝塔边缘分布来缓解内部异常值，结合高斯、Gumbel和Clayton copula族来捕捉一致性，通过桥采样获得对数边际似然进行模型选择。

Result: 应用于亚南极马里恩岛的Azorella selago垫状植物调查数据，copula模型在解释死亡茎覆盖百分比方面优于独立基线模型。考虑年际依赖性揭示了正向的西坡效应并减弱了垫状大小效应。

Conclusion: 该方法与JAGS无缝集成，为生态学和其他领域中具有边界结果和偶尔异常值的成对比例数据提供了稳健的默认解决方案。

Abstract: Continuous proportions measured on the same experimental unit often pose two challenges: interior outliers that inflate variance beyond the beta ceiling and residual dependence that invalidates independent-margin models. We introduce a Bayesian copula modeling approach that combines rectangular-beta margins, which temper interior outliers by reallocating mass from the peak to a uniform component, with a single-parameter copula to capture concordance. Gaussian, Gumbel, and Clayton copula families are fitted, and log marginal likelihoods are obtained via bridge sampling to guide model selection. Applied to a 13-year survey (2003-2016) of Azorella selago cushion plants on sub-Antarctic Marion Island, the copula models outperform independence baselines in explaining percent dead stem cover. Accounting for between-year dependence uncovers a positive west-slope effect and weakens the cushion size effect. Simulation results show negligible bias and near-nominal 95% highest posterior density coverage across a range of tail weight and dependence scenarios, confirming good frequentist properties. The method integrates readily with JAGS and provides a robust default for paired proportion data in ecology and other disciplines where bounded outcomes and occasional outliers coincide.

</details>


### [23] [Modelling toroidal and cylindrical data via the trivariate wrapped Cauchy copula with non-uniform marginals](https://arxiv.org/abs/2511.10336)
*Sophia Loizidou,Christophe Ley,Shogo Kato,Kanti V. Mardia*

Main category: stat.ME

TL;DR: 提出一个新的灵活分布族，用于处理包含角度和线性分量的三变量数据，通过将三变量缠绕柯西copula与非均匀边缘分布结合，并开发参数估计方法。


<details>
  <summary>Details</summary>
Motivation: 受到蛋白质生物信息学中的构象角度数据和气候科学中浮标数据的启发，需要处理包含角度和线性分量的三变量数据。

Method: 将三变量缠绕柯西copula与非均匀边缘分布结合，开发参数估计程序。

Result: 与主要竞争模型相比，新模型显示出优势，并在蛋白质构象角度和海洋气候数据中得到验证。

Conclusion: 新提出的分布族为处理包含角度和线性分量的三变量数据提供了灵活有效的工具。

Abstract: In this paper, we propose a new flexible family of distributions for data that consist of three angles, two angles and one linear component, or one angle and two linear components. To achieve this, we equip the recently proposed trivariate wrapped Cauchy copula with non-uniform marginals and develop a parameter estimation procedure. We compare our model to its main competitors for analyzing trivariate data and provide some evidence of its advantages. We illustrate our new model using toroidal data from protein bioinformatics of conformational angles, and cylindrical data from climate science related to buoy in the Adriatic Sea. The paper is motivated by these real trivariate datasets.

</details>
