{"id": "2511.14294", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14294", "abs": "https://arxiv.org/abs/2511.14294", "authors": ["Morgane Dumont", "Ahmed Alsaloum", "Julian Ernst", "Jan Weymeirsch", "Ralf Münnich"], "title": "Uncertainty assessment of spatial dynamic microsimulations", "comment": null, "summary": "Spatial dynamic microsimulations probabilistically project geographically referenced units with individual characteristics over time. Like any projection method, their outcomes are inherently uncertain and sensitive to multiple factors. However, such factors are rarely addressed. Applying variance-based sensitivity analysis to both direct and indirect effects within the employment module of the MikroSim model for Germany, we show that commonly considered sources of uncertainty, namely coefficient and parameter uncertainty, are less influential than qualitative modeling choices. Because dynamic microsimulations are inherently complex and are computationally intensive, it is crucial to consider potential factors of uncertainty and their influence on simulation outputs in order to more carefully design simulation setups and better communicate results. We find, that simple summary measures insufficiently capture overall model uncertainty and urge modelers to account for these broader sources when designing microsimulations and their results."}
{"id": "2511.14509", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.14509", "abs": "https://arxiv.org/abs/2511.14509", "authors": ["Alba Bernabeu", "Jorge Mateu"], "title": "Spatio-temporal Hawkes point processes: statistical inference and simulation strategies", "comment": null, "summary": "Spatio-temporal Hawkes point processes are a particularly interesting class of stochastic point processes for modeling self-exciting behavior, in which the occurrence of one event increases the probability of other events occurring. These processes are able to handle complex interrelationships between stochastic and deterministic components of spatio-temporal phenomena. However, despite its widespread use in practice, there is no common and unified formalism and every paper proposes different views of these stochastic mechanisms. With this in mind, we implement two simulation techniques and three unified, self-consistent inference techniques, which are widely used in the practical modeling of spatio-temporal Hawkes processes. Furthermore, we provide an evaluation of the practical performance of these methods, while providing useful code for reproducibility."}
{"id": "2511.14666", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14666", "abs": "https://arxiv.org/abs/2511.14666", "authors": ["Elkanah Nyabuto", "Philipp Otto", "Yarema Okhrin"], "title": "Estimation of Spatial and Temporal Autoregressive Effects using LASSO - An Example of Hourly Particulate Matter Concentrations", "comment": "27 pages, 11 figures, 4 tables. Under revision at Environmetrics", "summary": "We present an estimation procedure of spatial and temporal effects in spatiotemporal autoregressive panel data models using the Least Absolute Shrinkage and Selection Operator, LASSO (Tibshirani, 1996). We assume that the spatiotemporal panel is drawn from a univariate random process and that the data follows a spatiotemporal autoregressive process which includes a regressive term with space-/ time-varying exogenous regressor, a temporal autoregressive term and a spatial autoregressive term with an unknown weights matrix. The aim is to estimate this weight matrix alongside other parameters using a constraint penalised maximum likelihood estimator. Monte Carlo simulations showed a good performance with the accuracy increasing with an increasing number of time points. The use of the LASSO technique also consistently distinguishes between meaningful relationships (non-zeros) from those that are not (existing zeros) in both the spatial weights and other parameters. This regularised estimation procedure is applied to hourly particulate matter concentrations (PM10) in the Bavaria region, Germany for the years 2005 to 2020. Results show some stations with a high spatial dependency, resulting in a greater influence of PM10 concentrations in neighbouring monitoring stations. The LASSO technique proved to produce a sparse weights matrix by shrinking some weights to zero, hence improving the interpretability of the PM concentration dependencies across measurement stations in Bavaria"}
{"id": "2511.13934", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.13934", "abs": "https://arxiv.org/abs/2511.13934", "authors": ["Harold D. Chiang", "Yukitoshi Matsushita", "Taisuke Otsu"], "title": "Empirical Likelihood for Random Forests and Ensembles", "comment": "34 pages, 1 figure", "summary": "We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods."}
{"id": "2511.13763", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13763", "abs": "https://arxiv.org/abs/2511.13763", "authors": ["Anthony Kiggundu", "Bin Han", "Hans D. Schotten"], "title": "Knowledge vs. Experience: Asymptotic Limits of Impatience in Edge Tenants", "comment": "Submitted to IEEE ICC 2026", "summary": "We study how two information feeds, a closed-form Markov estimator of residual sojourn and an online trained actor-critic, affect reneging and jockeying in a dual M/M/1 system. Analytically, for unequal service rates and total-time patience, we show that total wait grows linearly so abandonment is inevitable and the probability of a successful jockey vanishes as the backlog approaches towards infinity. Furthermore, under a mild sub-linear error condition both information models yield the same asymptotic limits (robustness). We empirically validate these limits and quantify finite backlog differences. Our findings show that learned and analytic feeds produce different delays, reneging rates and transient jockeying behavior at practical sizes, but converge to the same asymptotic outcome implied by our theory. The results characterize when value-of-information matters (finite regimes) and when it does not (asymptotics), informing lightweight telemetry and decision-logic design for low-cost, jockeying-aware systems."}
{"id": "2511.14546", "categories": ["stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.14546", "abs": "https://arxiv.org/abs/2511.14546", "authors": ["Alessandro Ansani", "Elena Rinallo"], "title": "PLS-SEM-power: A Shiny App and R package for Computing Required Sample Size and Minimum Detectable Effect Size in PLS-SEMs", "comment": "for the associated Shiny App, see https://aleansani.shinyapps.io/pls-sem-power; for the user guide and code, see https://github.com/AleAnsani/plssempower", "summary": "Despite its evanescent nature, statistical power is crucial for planning Partial Least Squares Structural Equation Modelling (PLS-SEM) studies. This brief paper introduces PLS-SEM-power, a Shiny Application and R package that implements the inverse square root method by Kock and Hadaya (2018) to calculate both the minimum required sample size (a priori analysis) and the Minimum Detectable Effect Size (MDES, sensitivity analysis), given a chosen significance level (alpha level) at 80% power (1 - beta). The application provides an intuitive user interface, facilitating reproducible and easily accessible analyses in diverse research contexts."}
{"id": "2511.14035", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14035", "abs": "https://arxiv.org/abs/2511.14035", "authors": ["Michael Schuckers", "Austin Hayes"], "title": "Waiting for Dabo:A machine learning model for P4 college football coaching hires", "comment": "Version 2025.11.17", "summary": "Using data on 103 recent P4 college football hires, we built a statistical model for predicting a coach's success at their new school. For each hire, we collected data about their background and experiences, the previous success as a head coach or coordinator and their success since hiring. Over 50 variables on these factors were recorded though we used 29 of these in building our predictive model. Our measure of success is based upon Bill Connelly's SP+ team ratings relative to the performance on the same metric of the school in the 15 year prior to their selection as head coach. Using a cross-validated regularized linear regression, we obtain a predictive model for coaching success. Among the important factors for predicting a successful hire are having been a previous college head coach, having won a prior conference championship as a head coach, leaving a job as an Offensive Coordinator, age and quality of the hiring school's team in the previous 15 years. While we do find these factors are important for the prediction of a successful coaching hire, the trends here are weak. With 66% accuracy, the model does identify coaching hires that will outperform team performance in the 15 years before the hire. However, no combination of these factors leads to high predictability of identifying a successful coaching hire. All of the data and code for this paper are available in a Github repository."}
{"id": "2511.13895", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.13895", "abs": "https://arxiv.org/abs/2511.13895", "authors": ["Angelos Alexopoulos", "Nikolaos Demiris"], "title": "On robust Bayesian causal inference", "comment": "23 pages, 15 Figures", "summary": "This paper develops a Bayesian framework for robust causal inference from longitudinal observational data. Many contemporary methods rely on structural assumptions, such as factor models, to adjust for unobserved confounding, but they can lead to biased causal estimands when mis-specified. We focus on directly estimating time--unit--specific causal effects and use generalised Bayesian inference to quantify model mis-specification and adjust for it, while retaining interpretable posterior inference. We select the learning rate~$ω$ based on a proper scoring rule that jointly evaluates point and interval accuracy of the causal estimand, thus providing a coherent, decision-theoretic foundation for tuning~$ω$. Simulation studies and applications to real data demonstrate improved calibration, sharpness, and robustness in estimating causal effects."}
{"id": "2511.14123", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.14123", "abs": "https://arxiv.org/abs/2511.14123", "authors": ["Lyndsay Roach", "Qiong Li", "Nanwei Wang", "Xin Gao"], "title": "High-Dimensional Covariate-Dependent Discrete Graphical Models and Dynamic Ising Models", "comment": null, "summary": "We propose a covariate-dependent discrete graphical model for capturing dynamic networks among discrete random variables, allowing the dependence structure among vertices to vary with covariates. This discrete dynamic network encompasses the dynamic Ising model as a special case. We formulate a likelihood-based approach for parameter estimation and statistical inference. We achieve efficient parameter estimation in high-dimensional settings through the use of the pseudo-likelihood method. To perform model selection, a birth-and-death Markov chain Monte Carlo algorithm is proposed to explore the model space and select the most suitable model."}
{"id": "2511.13911", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13911", "abs": "https://arxiv.org/abs/2511.13911", "authors": ["Vasiliki Tassopoulou", "Charis Stamouli", "Haochang Shou", "George J. Pappas", "Christos Davatzikos"], "title": "Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands", "comment": null, "summary": "Despite recent progress in predicting biomarker trajectories from real clinical data, uncertainty in the predictions poses high-stakes risks (e.g., misdiagnosis) that limit their clinical deployment. To enable safe and reliable use of such predictions in healthcare, we introduce a conformal method for uncertainty-calibrated prediction of biomarker trajectories resulting from randomly-timed clinical visits of patients. Our approach extends conformal prediction to the setting of randomly-timed trajectories via a novel nonconformity score that produces prediction bands guaranteed to cover the unknown biomarker trajectories with a user-prescribed probability. We apply our method across a wide range of standard and state-of-the-art predictors for two well-established brain biomarkers of Alzheimer's disease, using neuroimaging data from real clinical studies. We observe that our conformal prediction bands consistently achieve the desired coverage, while also being tighter than baseline prediction bands. To further account for population heterogeneity, we develop group-conditional conformal bands and test their coverage guarantees across various demographic and clinically relevant subpopulations. Moreover, we demonstrate the clinical utility of our conformal bands in identifying subjects at high risk of progression to Alzheimer's disease. Specifically, we introduce an uncertainty-calibrated risk score that enables the identification of 17.5% more high-risk subjects compared to standard risk scores, highlighting the value of uncertainty calibration in real-world clinical decision making. Our code is available at github.com/vatass/ConformalBiomarkerTrajectories."}
{"id": "2511.14297", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14297", "abs": "https://arxiv.org/abs/2511.14297", "authors": ["Koffi Amezouwui", "Brigitte Gelein", "Matthieu Marbac", "Anthony Sorel"], "title": "Model-Based Clustering of Football Event Sequences: A Marked Spatio-Temporal Point Process Mixture Approach", "comment": null, "summary": "We propose a novel mixture model for football event data that clusters entire possessions to reveal their temporal, sequential, and spatial structure. Each mixture component models possessions as marked spatio-temporal point processes: event types follow a finite Markov chain with an absorbing state for ball loss, event times follow a conditional Gamma process to account for dispersion, and spatial locations evolve via truncated Brownian motion. To aid interpretation, we derive summary indicators from model parameters capturing possession speed, number of events, and spatial dynamics. Parameters are estimated through maximum likelihood via Generalized Expectation-Maximization algorithm. Applied to StatsBomb data from 38 Ligue 1 matches (2020/2021), our approach uncovers distinct defensive possession patterns faced by Stade Rennais. Unlike previous approaches focusing on individual events, our mixture structure enables principled clustering of full possessions, supporting tactical analysis and the future development of realistic virtual training environments."}
{"id": "2511.14091", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14091", "abs": "https://arxiv.org/abs/2511.14091", "authors": ["Jae Youn Ahn", "Hong Beng Lim", "Mario V. Wüthrich"], "title": "State-Space Representation of INGARCH Models and Their Application in Insurance", "comment": null, "summary": "Integer-valued generalized autoregressive conditional heteroskedastic (INGARCH) models are a popular framework for modeling serial dependence in count time-series. While convenient for modeling, prediction, and estimation, INGARCH models lack a clear theoretical justification for the evolution step. This limitation not only makes interpretation difficult and complicates the inclusion of covariates, but can also make the handling of missing data computationally burdensome. Consequently, applying such models in an insurance context, where covariates and missing observations are common, can be challenging. In this paper, we first introduce the marginalized state-space model (M-SSM), defined solely through the marginal distribution of the observations, and show that INGARCH models arise as special cases of this framework. The M-SSM formulation facilitates the natural incorporation of covariates and missing data mechanisms, and this representation in turn provides a coherent way to incorporate these elements within the INGARCH model as well. We then demonstrate that an M-SSM can admit an observation-driven state-space model (O-SSM) representation when suitable assumptions are imposed on the evolution of its conditional mean. This lifting from an M-SSM to an O-SSM provides a natural setting for establishing weak stationarity, even in the presence of heterogeneity and missing observations. The proposed ideas are illustrated through the Poisson and the Negative-Binomial INGARCH(1,1) models, highlighting their applicability in predictive analysis for insurance data."}
{"id": "2511.13934", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.13934", "abs": "https://arxiv.org/abs/2511.13934", "authors": ["Harold D. Chiang", "Yukitoshi Matsushita", "Taisuke Otsu"], "title": "Empirical Likelihood for Random Forests and Ensembles", "comment": "34 pages, 1 figure", "summary": "We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods."}
{"id": "2511.14417", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14417", "abs": "https://arxiv.org/abs/2511.14417", "authors": ["Paolo Victor Redondo", "Raphaël Huser", "Hernando Ombao"], "title": "Nonlinear Coherence for Vector Time Series: Defining Region-to-Region Functional Brain Connectivity", "comment": null, "summary": "Alterations in functional brain connectivity characterize neurodegenerative disorders such as Alzheimer's disease (AD) and frontotemporal dementia (FTD). As a non-invasive and cost-effective technique, electroencephalography (EEG) is gaining increasing attention for its potential to identify reliable biomarkers for early detection and differential diagnosis of AD and FTD. Considering the behavioral similarities of signals from adjacent EEG channels, we propose a new spectral dependence measure, the nonlinear vector coherence (NVC), to capture beyond-linear interactions between oscillations of two multivariate time series observed from distinct brain regions. This addresses the limitations of conventional channel-to-channel approaches and defines a more natural region-to-region connectivity framework in the frequency domain. As a result, the NVC measure offers a new approach to investigate dependence between brain regions, which then enables to identify altered functional connectivity dynamics associated with AD and FTD. We further introduce a rank-based inference procedure that enables fast and distribution-free estimation of the proposed measure, as well as a fully nonparametric test for spectral independence. The empirical performance of our proposed inference methodology is demonstrated through extensive numerical experiments. An application to resting-state EEG data reveals that our novel NVC measure uncovers distinct and diagnostically meaningful connectivity patterns which effectively discriminate healthy individuals from those with AD and FTD."}
{"id": "2511.14092", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14092", "abs": "https://arxiv.org/abs/2511.14092", "authors": ["Jiayang Xu", "Xintong Chen", "Yufeng Liu", "Xiaoli Guo", "Shanbao Tong"], "title": "The Prevalence of Misreporting and Misinterpreting Correlation Coefficients in Biomedical Literature", "comment": "14 Pages, 3 figures", "summary": "Correlation coefficient is widely used in biomedical and biological literature, yet its frequent misuse and misinterpretation undermine the credibility and reproducibility of the scientific findings. We systematically reviewed 1326 records of correlation analyses across 310 articles published in Science, Nature, and Nature Neuroscience in 2022. Our analysis revealed a troubling pattern of poor statistical reporting and inferring: 58.71% (95% CI: [53.23%, 64.19%], 182/310) of studies did not explicitly report sample sizes, and 98.06% (95% CI: [96.53%, 99.60%], 304/310) failed to provide confidence intervals for correlation coefficients. Among 177 articles inferring correlation strength, 45.25% (95% CI: [38.42%, 53.10%], 81/177) relied solely on point estimates, while 53.63% (95% CI: [46.90%, 61.58%], 96/177) drew conclusions based on null hypothesis significance testing. This widespread omission and misuse highlight a systematic gap in both statistic literacy and editorial standards. We advocate clear reporting guidelines mandating effect sizes and confidence intervals in correlation analyses to enhance the transparency, rigor, and reproducibility of quantitative life sciences research."}
{"id": "2511.14042", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.14042", "abs": "https://arxiv.org/abs/2511.14042", "authors": ["Mara Daniels", "Philippe Rigollet"], "title": "Splat Regression Models", "comment": null, "summary": "We introduce a highly expressive class of function approximators called Splat Regression Models. Model outputs are mixtures of heterogeneous and anisotropic bump functions, termed splats, each weighted by an output vector. The power of splat modeling lies in its ability to locally adjust the scale and direction of each splat, achieving both high interpretability and accuracy. Fitting splat models reduces to optimization over the space of mixing measures, which can be implemented using Wasserstein-Fisher-Rao gradient flows. As a byproduct, we recover the popular Gaussian Splatting methodology as a special case, providing a unified theoretical framework for this state-of-the-art technique that clearly disambiguates the inverse problem, the model, and the optimization algorithm. Through numerical experiments, we demonstrate that the resulting models and algorithms constitute a flexible and promising approach for solving diverse approximation, estimation, and inverse problems involving low-dimensional data."}
{"id": "2511.14537", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14537", "abs": "https://arxiv.org/abs/2511.14537", "authors": ["Ayham Makhamra", "Yelyzaveta Satynska", "Michael Weselcouch"], "title": "Darts Analysis", "comment": "16 pages, 8 figures", "summary": "In this paper we examine the effectiveness of five mathematical models used to predict the outcomes of amateur darts games. These models not only predict the outcomes at the start of the game, but also update their estimations as the game score changes. The models were trained and tested on a dataset consisting of games played by amateur players involving students, faculty, and staff at Roanoke College. The five models are: the null model, which is based only on the live scores, a logistic regression model, a basic simulation model, a time-adjusted simulation model, and a new variation of the Massey model which updates based on the current score. We evaluate these models using two approaches. First, we compare their Brier scores. Second, we conduct head-to-head comparisons in a betting game in which one model sets the betting odds while the other places bets. In both cases, model performance is assessed not only at the start of the game but also at the start of each round. Across both evaluation methods, the score-dependent Massey model performs the best. We conclude by illustrating how this score-dependent Massey model framework can be adapted to other competitive settings beyond darts."}
{"id": "2511.14123", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.14123", "abs": "https://arxiv.org/abs/2511.14123", "authors": ["Lyndsay Roach", "Qiong Li", "Nanwei Wang", "Xin Gao"], "title": "High-Dimensional Covariate-Dependent Discrete Graphical Models and Dynamic Ising Models", "comment": null, "summary": "We propose a covariate-dependent discrete graphical model for capturing dynamic networks among discrete random variables, allowing the dependence structure among vertices to vary with covariates. This discrete dynamic network encompasses the dynamic Ising model as a special case. We formulate a likelihood-based approach for parameter estimation and statistical inference. We achieve efficient parameter estimation in high-dimensional settings through the use of the pseudo-likelihood method. To perform model selection, a birth-and-death Markov chain Monte Carlo algorithm is proposed to explore the model space and select the most suitable model."}
{"id": "2511.14146", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.14146", "abs": "https://arxiv.org/abs/2511.14146", "authors": ["Renjie Chen", "Viet Anh Nguyen", "Huifu Xu"], "title": "SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation", "comment": null, "summary": "We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications."}
{"id": "2511.14706", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14706", "abs": "https://arxiv.org/abs/2511.14706", "authors": ["Junwei Ma", "Bo Li", "Xiangpeng Li", "Ali Mostafavi"], "title": "Decoupling Urban Food Accessibility Resilience during Disasters through Time-Series Analysis of Human Mobility and Power Outages", "comment": null, "summary": "Disaster-induced power outages create cascading disruptions across urban lifelines, yet the timed coupling between grid failure and essential service access remains poorly quantified. Focusing on Hurricane Beryl in Houston (2024), this study integrates approximately 173000 15-minute outage records with over 1.25 million visits to 3187 food facilities to quantify how infrastructure performance and human access co-evolve. We construct daily indices for outage characteristics (intensity, duration) and food access metrics (redundancy, frequency, proximity), estimate cross-system lags through lagged correlations over zero to seven days, and identify recovery patterns using DTW k-means clustering. Overlaying these clusters yields compound power-access typologies and enables facility-level criticality screening. The analysis reveals a consistent two-day lag: food access reaches its nadir on July 8 at landfall while outage severity peaks around July 10, with negative correlations strongest at a two-day lag and losing significance by day four. We identify four compound typologies from high/low outage crossed with high/low access disruption levels. Road network sparsity, more than income, determines the depth and persistence of access loss. Through this analysis, we enumerate 294 critical food facilities in the study area requiring targeted continuity measures including backup power, microgrids, and feeder prioritization. The novelty lies in measuring interdependency at daily operational resolution while bridging scales from communities to individual facilities, converting dynamic coupling patterns into actionable interventions for phase-sensitive restoration and equity-aware preparedness. The framework is transferable to other lifelines and hazards, offering a generalizable template for diagnosing and mitigating cascading effects on community access during disaster recovery."}
{"id": "2511.14292", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14292", "abs": "https://arxiv.org/abs/2511.14292", "authors": ["Cyrill Scheidegger", "Simon Wandel", "Tobias Mütze"], "title": "Covariate Adjustment for the Win Odds: Application to Cardiovascular Outcomes Trials", "comment": null, "summary": "Covariate adjustment can enhance precision and power in clinical trials, yet its application to the win odds remains unclear. The win odds is an extension of the win ratio that includes ties. In their original form, both methods rely on comparing each individual from the treatment group to each individual from the control group in pairwise manner, and count the number of wins, losses, and ties from these pairwise comparisons. A priori, it is not clear how covariate adjustment can be implemented for the win odds. To address this, we establish a connection between the win odds and the marginal probabilistic index, a measure for which covariate adjustment theory is well-developed. Using this connection, we show how covariate adjustment for the win odds is possible, leading to potentially more precise estimators and larger power as compared to the unadjusted win odds. We present the underlying theory for covariate adjustment for the win odds in an accessible way and apply the method on synthetic data based on the CANTOS trial (ClinicalTrials.gov identifier: NCT01327846) characteristics and on simulated data to study the operating characteristics of the method. We observe that there is indeed a potential gain in power when the win odds are adjusted for baseline covariates if the baseline covariates are prognostic for the outcome. This comes at the cost of a slight inflation of the type I error rate for small sample sizes."}
{"id": "2511.14206", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14206", "abs": "https://arxiv.org/abs/2511.14206", "authors": ["Alessio Zanga", "Marco Scutari", "Fabio Stella"], "title": "Causal Discovery on Higher-Order Interactions", "comment": "16 pages, 2 figures", "summary": "Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings."}
{"id": "2511.14714", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14714", "abs": "https://arxiv.org/abs/2511.14714", "authors": ["Connor Fitchett", "Ayon Mukherjee", "Sofía S. Villar", "David S. Robertson"], "title": "Bayesian Optimal Phase II design with optimised stopping boundaries and response-adaptive randomisation", "comment": "27 pages, 11 figures", "summary": "The Bayesian Optimal Phase II (BOP2) framework is a flexible trial design that can naturally facilitate complex adaptations due to its Bayesian setting. BOP2 uses equal randomisation and equally placed interim analyses in its design, but it is unclear whether these give the best operating characteristics. By incorporating Bayesian Response-Adaptive Randomisation (BRAR) and optimal interim analysis placement, we show that allocation to the best treatment and expected sample size can be improved with minimal impact on power. We discuss recommendations on implementing these adaptations, using simulation-based evidence, to give practical advice to practitioners. Reproducible code for the simulations is freely provided."}
{"id": "2511.14353", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14353", "abs": "https://arxiv.org/abs/2511.14353", "authors": ["Sourav Chakrabarty", "Anirvan Chakraborty", "Shyamal K. De"], "title": "Consistent detection and estimation of multiple structural changes in functional data: unsupervised and supervised approaches", "comment": "58 pages, 6 figures, 9 tables", "summary": "We develop algorithms for detecting multiple changepoints in functional data when the number of changepoints is unknown (unsupervised case), when it is specified apriori (supervised case), and when certain bounds are available (semi-supervised case). These algorithms utilize the maximum mean discrepancy (MMD) measure between distributions on Hilbert spaces. We develop an oracle analysis of the changepoint detection problem which reveals an interesting relationship between the true changepoint locations and the local maxima of the oracle MMD curve. The proposed algorithms are shown to detect general distributional changes by exploiting this connection. In the unsupervised case, we test the significance of a potential changepoint and establish its consistency under the single changepoint setting. We investigate the strong consistency of the changepoint estimators in both single and multiple changepoint settings. In both supervised and semi-supervised scenarios, we include a step to merge consecutive groups that are similar to appropriately utilize the prior information about the number of changepoints. In the supervised scenario, the algorithm satisfies an order-preserving property: the estimated changepoints are contained in the true set of changepoints in the underspecified case, while they contain the true set under overspecification. We evaluate the performance of the algorithms on a variety of datasets demonstrating the superiority of the proposed algorithms compared to some of the existing methods."}
{"id": "2511.14441", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14441", "abs": "https://arxiv.org/abs/2511.14441", "authors": ["Daniel Klippert", "Alexander Marx"], "title": "Skewness-Robust Causal Discovery in Location-Scale Noise Models", "comment": null, "summary": "To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \\to Y$ and $Y \\to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness."}
{"id": "2511.13911", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13911", "abs": "https://arxiv.org/abs/2511.13911", "authors": ["Vasiliki Tassopoulou", "Charis Stamouli", "Haochang Shou", "George J. Pappas", "Christos Davatzikos"], "title": "Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands", "comment": null, "summary": "Despite recent progress in predicting biomarker trajectories from real clinical data, uncertainty in the predictions poses high-stakes risks (e.g., misdiagnosis) that limit their clinical deployment. To enable safe and reliable use of such predictions in healthcare, we introduce a conformal method for uncertainty-calibrated prediction of biomarker trajectories resulting from randomly-timed clinical visits of patients. Our approach extends conformal prediction to the setting of randomly-timed trajectories via a novel nonconformity score that produces prediction bands guaranteed to cover the unknown biomarker trajectories with a user-prescribed probability. We apply our method across a wide range of standard and state-of-the-art predictors for two well-established brain biomarkers of Alzheimer's disease, using neuroimaging data from real clinical studies. We observe that our conformal prediction bands consistently achieve the desired coverage, while also being tighter than baseline prediction bands. To further account for population heterogeneity, we develop group-conditional conformal bands and test their coverage guarantees across various demographic and clinically relevant subpopulations. Moreover, we demonstrate the clinical utility of our conformal bands in identifying subjects at high risk of progression to Alzheimer's disease. Specifically, we introduce an uncertainty-calibrated risk score that enables the identification of 17.5% more high-risk subjects compared to standard risk scores, highlighting the value of uncertainty calibration in real-world clinical decision making. Our code is available at github.com/vatass/ConformalBiomarkerTrajectories."}
{"id": "2511.14523", "categories": ["stat.ME", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2511.14523", "abs": "https://arxiv.org/abs/2511.14523", "authors": ["Sunday A. Adetunji"], "title": "Teaching Longitudinal Linear Mixed Models End-to-End: A Reproducible Case Study in Mouse Body-Weight Growth", "comment": "42 pages, 5 figures, 7 tables. Includes fully reproducible R code and teaching materials for longitudinal linear mixed models using a mouse body-weight case study", "summary": "Background: Linear mixed-effects models are central for analyzing longitudinal continuous data, yet many learners meet them as scattered formulas or software output rather than as a coherent workflow. There is a need for a single, reproducible case study that links questions, model building, diagnostics, and interpretation.\n  Methods: We reanalyze a published mouse body-weight experiment with 31 mice in three groups weighed weekly for 12 weeks. After reshaping the data to long format and using profile plots to motivate linear time trends, we fit three random-intercept linear mixed models: a common-slope model, a fully interacted group-by-time model, and a parsimonious model with group-specific intercepts, a shared slope for two groups, and an extra slope for the third. Models are compared using maximum likelihood, AIC, BIC, and likelihood ratio tests, and linear contrasts are used to estimate group differences in weekly means and 12 week gains.\n  Results: The parsimonious model fits as well as the fully interacted model and clearly outperforms the common-slope model, revealing small and similar gains in two groups and much steeper growth in the third, with highly significant contrasts for excess weight gain.\n  Interpretation: This case study gives a complete, executable workflow for longitudinal linear mixed modeling, from raw data and exploratory plots through model selection, diagnostics, and targeted contrasts. By making explicit the mapping from scientific questions to model terms and estimable contrasts, and by providing R code and a stepwise checklist, it serves as a practical template for teaching and applied work in biostatistics, epidemiology, and related fields"}
{"id": "2511.14545", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14545", "abs": "https://arxiv.org/abs/2511.14545", "authors": ["Haorui Ma", "Dennis Frauen", "Stefan Feuerriegel"], "title": "DeepBlip: Estimating Conditional Average Treatment Effects Over Time", "comment": "42 pages", "summary": "Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance."}
{"id": "2511.14091", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14091", "abs": "https://arxiv.org/abs/2511.14091", "authors": ["Jae Youn Ahn", "Hong Beng Lim", "Mario V. Wüthrich"], "title": "State-Space Representation of INGARCH Models and Their Application in Insurance", "comment": null, "summary": "Integer-valued generalized autoregressive conditional heteroskedastic (INGARCH) models are a popular framework for modeling serial dependence in count time-series. While convenient for modeling, prediction, and estimation, INGARCH models lack a clear theoretical justification for the evolution step. This limitation not only makes interpretation difficult and complicates the inclusion of covariates, but can also make the handling of missing data computationally burdensome. Consequently, applying such models in an insurance context, where covariates and missing observations are common, can be challenging. In this paper, we first introduce the marginalized state-space model (M-SSM), defined solely through the marginal distribution of the observations, and show that INGARCH models arise as special cases of this framework. The M-SSM formulation facilitates the natural incorporation of covariates and missing data mechanisms, and this representation in turn provides a coherent way to incorporate these elements within the INGARCH model as well. We then demonstrate that an M-SSM can admit an observation-driven state-space model (O-SSM) representation when suitable assumptions are imposed on the evolution of its conditional mean. This lifting from an M-SSM to an O-SSM provides a natural setting for establishing weak stationarity, even in the presence of heterogeneity and missing observations. The proposed ideas are illustrated through the Poisson and the Negative-Binomial INGARCH(1,1) models, highlighting their applicability in predictive analysis for insurance data."}
{"id": "2511.14535", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14535", "abs": "https://arxiv.org/abs/2511.14535", "authors": ["Weiyue Zheng", "Andrew Elliott", "Claire Miller", "Marian Scott"], "title": "A Bayesian INLA-SPDE Approach to Spatio-Temporal Point-Grid Fusion with Change-of-Support and Misaligned Covariates", "comment": null, "summary": "We propose a spatio-temporal data-fusion framework for point data and gridded data with variables observed on different spatial supports. A latent Gaussian field with a Matérn-SPDE prior provides a continuous space representation, while source-specific observation operators map observations to both point measurements and gridded averages, addressing change-of-support and covariate misalignment. Additionally incorporating temporal dependence enables prediction at unknown locations and time points. Inference and prediction are performed using the Integrated Nested Laplace Approximation and the Stochastic Partial Differential Equations approach, which delivers fast computation with uncertainty quantification. Our contributions are: a hierarchical model that jointly fuses multiple data sources of the same variable under different spatial and temporal resolutions and measurement errors, and a practical implementation that incorporates misaligned covariates via the same data fusion framework allowing differing covariate supports. We demonstrate the utility of this framework via simulations calibrated to realistic sensor densities and spatial coverage. Using the simulation framework, we explore the stability and performance of the approach with respect to the number of time points and data/covariate availability, demonstrating gains over single-source models through point and gridded data fusion. We apply our framework to soil moisture mapping in the Elliot Water catchment (Angus, Scotland). We fuse in-situ sensor data with aligned and misaligned covariates, satellite data and elevation data to produce daily high resolution maps with uncertainty."}
{"id": "2511.14710", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14710", "abs": "https://arxiv.org/abs/2511.14710", "authors": ["Zonghao Chen", "Atsushi Nitanda", "Arthur Gretton", "Taiji Suzuki"], "title": "Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization", "comment": null, "summary": "We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \\emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \\texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark."}
{"id": "2511.14292", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14292", "abs": "https://arxiv.org/abs/2511.14292", "authors": ["Cyrill Scheidegger", "Simon Wandel", "Tobias Mütze"], "title": "Covariate Adjustment for the Win Odds: Application to Cardiovascular Outcomes Trials", "comment": null, "summary": "Covariate adjustment can enhance precision and power in clinical trials, yet its application to the win odds remains unclear. The win odds is an extension of the win ratio that includes ties. In their original form, both methods rely on comparing each individual from the treatment group to each individual from the control group in pairwise manner, and count the number of wins, losses, and ties from these pairwise comparisons. A priori, it is not clear how covariate adjustment can be implemented for the win odds. To address this, we establish a connection between the win odds and the marginal probabilistic index, a measure for which covariate adjustment theory is well-developed. Using this connection, we show how covariate adjustment for the win odds is possible, leading to potentially more precise estimators and larger power as compared to the unadjusted win odds. We present the underlying theory for covariate adjustment for the win odds in an accessible way and apply the method on synthetic data based on the CANTOS trial (ClinicalTrials.gov identifier: NCT01327846) characteristics and on simulated data to study the operating characteristics of the method. We observe that there is indeed a potential gain in power when the win odds are adjusted for baseline covariates if the baseline covariates are prognostic for the outcome. This comes at the cost of a slight inflation of the type I error rate for small sample sizes."}
{"id": "2511.14546", "categories": ["stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.14546", "abs": "https://arxiv.org/abs/2511.14546", "authors": ["Alessandro Ansani", "Elena Rinallo"], "title": "PLS-SEM-power: A Shiny App and R package for Computing Required Sample Size and Minimum Detectable Effect Size in PLS-SEMs", "comment": "for the associated Shiny App, see https://aleansani.shinyapps.io/pls-sem-power; for the user guide and code, see https://github.com/AleAnsani/plssempower", "summary": "Despite its evanescent nature, statistical power is crucial for planning Partial Least Squares Structural Equation Modelling (PLS-SEM) studies. This brief paper introduces PLS-SEM-power, a Shiny Application and R package that implements the inverse square root method by Kock and Hadaya (2018) to calculate both the minimum required sample size (a priori analysis) and the Minimum Detectable Effect Size (MDES, sensitivity analysis), given a chosen significance level (alpha level) at 80% power (1 - beta). The application provides an intuitive user interface, facilitating reproducible and easily accessible analyses in diverse research contexts."}
{"id": "2511.14294", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14294", "abs": "https://arxiv.org/abs/2511.14294", "authors": ["Morgane Dumont", "Ahmed Alsaloum", "Julian Ernst", "Jan Weymeirsch", "Ralf Münnich"], "title": "Uncertainty assessment of spatial dynamic microsimulations", "comment": null, "summary": "Spatial dynamic microsimulations probabilistically project geographically referenced units with individual characteristics over time. Like any projection method, their outcomes are inherently uncertain and sensitive to multiple factors. However, such factors are rarely addressed. Applying variance-based sensitivity analysis to both direct and indirect effects within the employment module of the MikroSim model for Germany, we show that commonly considered sources of uncertainty, namely coefficient and parameter uncertainty, are less influential than qualitative modeling choices. Because dynamic microsimulations are inherently complex and are computationally intensive, it is crucial to consider potential factors of uncertainty and their influence on simulation outputs in order to more carefully design simulation setups and better communicate results. We find, that simple summary measures insufficiently capture overall model uncertainty and urge modelers to account for these broader sources when designing microsimulations and their results."}
{"id": "2511.14622", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14622", "abs": "https://arxiv.org/abs/2511.14622", "authors": ["Michael Greenacre", "Martin Graeve"], "title": "Amalgamations in a hierarchy as a way of variable selection in compositional data analysis", "comment": "10 pages 4 figures", "summary": "In certain fields where compositional data are studied, the compositional components, called parts, can be combined into certain subsets, called amalgamations, that are based on domain knowledge. Furthermore, these subsets can form a natural hierarchy of amalgamations subdividing into sub-amalgamations. The authors, a statistician and a biochemist, demonstrate how to create a hierarchy of amalgamations in the context of fatty acid compositions in a sample of marine organisms. Following a tradition in compositional data analysis, these amalgamations are transformed to logratios, and their usefulness as new variables is quantified by the percentage of total logratio variance that they explain. This method is proposed as an alternative method of variable selection in compositional data analysis."}
{"id": "2511.14666", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14666", "abs": "https://arxiv.org/abs/2511.14666", "authors": ["Elkanah Nyabuto", "Philipp Otto", "Yarema Okhrin"], "title": "Estimation of Spatial and Temporal Autoregressive Effects using LASSO - An Example of Hourly Particulate Matter Concentrations", "comment": "27 pages, 11 figures, 4 tables. Under revision at Environmetrics", "summary": "We present an estimation procedure of spatial and temporal effects in spatiotemporal autoregressive panel data models using the Least Absolute Shrinkage and Selection Operator, LASSO (Tibshirani, 1996). We assume that the spatiotemporal panel is drawn from a univariate random process and that the data follows a spatiotemporal autoregressive process which includes a regressive term with space-/ time-varying exogenous regressor, a temporal autoregressive term and a spatial autoregressive term with an unknown weights matrix. The aim is to estimate this weight matrix alongside other parameters using a constraint penalised maximum likelihood estimator. Monte Carlo simulations showed a good performance with the accuracy increasing with an increasing number of time points. The use of the LASSO technique also consistently distinguishes between meaningful relationships (non-zeros) from those that are not (existing zeros) in both the spatial weights and other parameters. This regularised estimation procedure is applied to hourly particulate matter concentrations (PM10) in the Bavaria region, Germany for the years 2005 to 2020. Results show some stations with a high spatial dependency, resulting in a greater influence of PM10 concentrations in neighbouring monitoring stations. The LASSO technique proved to produce a sparse weights matrix by shrinking some weights to zero, hence improving the interpretability of the PM concentration dependencies across measurement stations in Bavaria"}
{"id": "2511.14692", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14692", "abs": "https://arxiv.org/abs/2511.14692", "authors": ["Jooho Kim", "Yei Eun Shin"], "title": "Scalable and Efficient Multiple Imputation for Case-Cohort Studies via Influence Function-Based Supersampling", "comment": "32 pages, 3 figures, 6 tables (3 in appendix)", "summary": "Two-phase sampling designs have been widely adopted in epidemiological studies to reduce costs when measuring certain biomarkers is prohibitively expensive. Under these designs, investigators commonly relate survival outcomes to risk factors using the Cox proportional hazards model. To fully utilize covariates collected in phase 1, multiple imputation methods have been developed to impute missing covariates for individuals not included in the phase 2 sample. However, performing multiple imputation on large-scale cohorts can be computationally intensive or even infeasible. To address this issue, Borgan et al. (2023) proposed a random supersampling (RSS) approach that randomly selects a subset of cohort members for imputation, albeit at the cost of reduced efficiency. In this study, we propose an influence function-based supersampling (ISS) approach with weight calibration. The method achieves efficiency comparable to imputing the entire cohort, even with a small supersample, while substantially reducing computational burden. We further demonstrate that the proposed method is particularly advantageous when estimating hazard ratios for high-dimensional expensive biomarkers. Extensive simulation studies are conducted, and a real data application using the National Institutes of Health-American Association of Retired Persons (NIH-AARP) Diet and Health Study is provided to illustrate the effectiveness of the proposed method."}
{"id": "2511.14545", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.14545", "abs": "https://arxiv.org/abs/2511.14545", "authors": ["Haorui Ma", "Dennis Frauen", "Stefan Feuerriegel"], "title": "DeepBlip: Estimating Conditional Average Treatment Effects Over Time", "comment": "42 pages", "summary": "Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance."}
