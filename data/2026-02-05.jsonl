{"id": "2602.02874", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2602.02874", "abs": "https://arxiv.org/abs/2602.02874", "authors": ["Tiffany A. Timbers", "Mine Çetinkaya-Rundel"], "title": "Ten simple rules for teaching data science", "comment": null, "summary": "Teaching data science presents unique challenges and opportunities that cannot be fully addressed by simply borrowing pedagogical strategies from its parent disciplines of statistics and computer science. Here, we present ten simple rules for teaching data science, developed and refined by leading educators in the community and successfully applied in our own data science classrooms."}
{"id": "2602.03274", "categories": ["stat.OT", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.03274", "abs": "https://arxiv.org/abs/2602.03274", "authors": ["Nils Lid Hjort"], "title": "Six-Minute Man Sander Eitrem 5:58.52 -- first man below the 6:00.00 barrier", "comment": null, "summary": "In Calgary, November 2005, Chad Hedrick was the first to skate the 5,000 m below 6:10. His world record time 6:09.68 was then beaten a week later, in Salt Lake City, by Sven Kramer's 6:08.78. Further top races and world records followed over the ensuing seasons; up to and including the 2024-2025 season, a total of 126 races have been below 6:10, with Nils van der Poel's 2021 world record being 6:01.56. The appropriately hyped-up canonical question for the friends and followers and aficionados of speedskating has then been when (and by whom  we for the first time would witness a below 6:00.00 race. In this note I first use extreme value statistics modelling to assess the state of affairs, as per the end of the 2024-2025 season, with predictions and probabilities for the 2025-2026 season. Under natural modelling assumptions the probability of seeing a new world record during this new season is shown to be about ten percent. We were indeed excited but in reality merely modestly surprised that a race better than van der Poel's record was clocked, by Timothy Loubineaud, in Salt Lake City, November 14, 2025. But Six-Minute Man Sander Eitrem's outstanding 5:58.52 in Inzell, on January 24, 2026, is truly beamonesquely shocking. I also use the modelling machinery to analyse the post-Eitrem situation, and suggest answers to the question of how fast the 5,000 m ever can be skated."}
{"id": "2602.02887", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2602.02887", "abs": "https://arxiv.org/abs/2602.02887", "authors": ["Yue Sun", "Ryan Weightman", "Yang Yang", "Anye Shi", "Timur Dogan", "Samitha Samaranayake"], "title": "From Accessibility to Allocation: An Integrated Workflow for Land-Use Assignment and FAR Estimation", "comment": null, "summary": "Urban land use and building intensity are often planned without a direct, auditable link to network accessibility, limiting ex-ante policy evaluation. This study asks whether multi-radius street centralities can be elevated from diagnosis to design lever to allocate land use and floor area in a transparent, optimization-ready workflow. We introduce a three-stage pipeline that connects configuration to program and intensity. First, multi-radius accessibility is computed on the street network and translated to blocks to provide scale-legible measures of reach. Second, these measures structure nested service basins that guide a rule-based placement of land uses with explicit priorities and minimum parcel footprints, ensuring reproducibility. Third, within each use, floor-area ratio (FAR) is assigned by an accessibility-weighted linear model that satisfies global construction totals while anchoring the average FAR, thereby tilting height toward better-connected blocks without pathological extremes. The framework supports multi-objective policy search via sampling and Pareto screening. Applied to a real urban district, the workflow reproduces corridor-biased commercial siting and industrial belts while concentrating intensity on highly connected blocks. Policy sampling via multi-objective screening yields Pareto-efficient plans that reconcile accessibility gains with deviations from target land-share and construction-share structures. The contribution is twofold: methodologically, it translates familiar space-syntax measures into cluster-aware, rule-governed land-use and FAR assignment with explicit guarantees (scale-legible radii, parcel minima, and an average-FAR anchor). Practically, it offers planners a transparent instrument for counterfactual testing and negotiated trade-offs at neighborhood/district/city scales."}
{"id": "2602.02753", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02753", "abs": "https://arxiv.org/abs/2602.02753", "authors": ["Youngjin Cho", "Meimei Liu"], "title": "Effect-Wise Inference for Smoothing Spline ANOVA on Tensor-Product Sobolev Space", "comment": null, "summary": "Functional ANOVA provides a nonparametric modeling framework for multivariate covariates, enabling flexible estimation and interpretation of effect functions such as main effects and interaction effects. However, effect-wise inference in such models remains challenging. Existing methods focus primarily on inference for entire functions rather than individual effects. Methods addressing effect-wise inference face substantial limitations: the inability to accommodate interactions, a lack of rigorous theoretical foundations, or restriction to pointwise inference. To address these limitations, we develop a unified framework for effect-wise inference in smoothing spline ANOVA on a subspace of tensor product Sobolev space. For each effect function, we establish rates of convergence, pointwise confidence intervals, and a Wald-type test for whether the effect is zero, with power achieving the minimax distinguishable rate up to a logarithmic factor. Main effects achieve the optimal univariate rates, and interactions achieve optimal rates up to logarithmic factors. The theoretical foundation relies on an orthogonality decomposition of effect subspaces, which enables the extension of the functional Bahadur representation framework to effect-wise inference in smoothing spline ANOVA with interactions. Simulation studies and real-data application to the Colorado temperature dataset demonstrate superior performance compared to existing methods."}
{"id": "2602.02703", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02703", "abs": "https://arxiv.org/abs/2602.02703", "authors": ["Chenxi Li", "Ke Zhu", "Shu Yang", "Xiaofei Wang"], "title": "Selective Information Borrowing for Region-Specific Treatment Effect Inference under Covariate Mismatch in Multi-Regional Clinical Trials", "comment": null, "summary": "Multi-regional clinical trials (MRCTs) are central to global drug development, enabling evaluation of treatment effects across diverse populations. A key challenge is valid and efficient inference for a region-specific estimand when the target region is small and differs from auxiliary regions in baseline covariates or unmeasured factors. We adopt an estimand-based framework and focus on the region-specific average treatment effect (RSATE) in a prespecified target region, which is directly relevant to local regulatory decision-making. Cross-region differences can induce covariate shift, covariate mismatch, and outcome drift, potentially biasing information borrowing and invalidating RSATE inference. To address these issues, we develop a unified causal inference framework with selective information borrowing. First, we introduce an inverse-variance weighting estimator that combines a \"small-sample, rich-covariate\" target-only estimator with a \"large-sample, limited-covariate\" full-borrowing doubly robust estimator, maximizing efficiency under no outcome drift. Second, to accommodate outcome drift, we apply conformal prediction to assess patient-level comparability and adaptively select auxiliary-region patients for borrowing. Third, to ensure rigorous finite-sample inference, we employ a conditional randomization test with exact, model-free, selection-aware type I error control. Simulation studies show the proposed estimator improves efficiency, yielding 10-50% reductions in mean squared error and higher power relative to no-borrowing and full-borrowing approaches, while maintaining valid inference across diverse scenarios. An application to the POWER trial further demonstrates improved precision for RSATE estimation."}
{"id": "2602.02577", "categories": ["stat.ML", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02577", "abs": "https://arxiv.org/abs/2602.02577", "authors": ["Shiji Xiao", "Yufeng Zhang", "Chubo Liu", "Yan Ding", "Keqin Li", "Kenli Li"], "title": "Relaxed Triangle Inequality for Kullback-Leibler Divergence Between Multivariate Gaussian Distributions", "comment": null, "summary": "The Kullback-Leibler (KL) divergence is not a proper distance metric and does not satisfy the triangle inequality, posing theoretical challenges in certain practical applications. Existing work has demonstrated that KL divergence between multivariate Gaussian distributions follows a relaxed triangle inequality. Given any three multivariate Gaussian distributions $\\mathcal{N}_1, \\mathcal{N}_2$, and $\\mathcal{N}_3$, if $KL(\\mathcal{N}_1, \\mathcal{N}_2)\\leq ε_1$ and $KL(\\mathcal{N}_2, \\mathcal{N}_3)\\leq ε_2$, then $KL(\\mathcal{N}_1, \\mathcal{N}_3)< 3ε_1+3ε_2+2\\sqrt{ε_1ε_2}+o(ε_1)+o(ε_2)$. However, the supremum of $KL(\\mathcal{N}_1, \\mathcal{N}_3)$ is still unknown. In this paper, we investigate the relaxed triangle inequality for the KL divergence between multivariate Gaussian distributions and give the supremum of $KL(\\mathcal{N}_1, \\mathcal{N}_3)$ as well as the conditions when the supremum can be attained. When $ε_1$ and $ε_2$ are small, the supremum is $ε_1+ε_2+\\sqrt{ε_1ε_2}+o(ε_1)+o(ε_2)$. Finally, we demonstrate several applications of our results in out-of-distribution detection with flow-based generative models and safe reinforcement learning."}
{"id": "2602.02806", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.02806", "abs": "https://arxiv.org/abs/2602.02806", "authors": ["Dongqing Li", "Zheqiao Cheng", "Geoff K. Nicholls", "Quyu Kong"], "title": "De-Linearizing Agent Traces: Bayesian Inference of Latent Partial Orders for Efficient Execution", "comment": null, "summary": "AI agents increasingly execute procedural workflows as sequential action traces, which obscures latent concurrency and induces repeated step-by-step reasoning. We introduce BPOP, a Bayesianframework that infers a latent dependency partial order from noisy linearized traces. BPOP models traces as stochastic linear extensions of an underlying graph and performs efficient MCMC inference via a tractable frontier-softmax likelihood that avoids #P-hard marginalization over linear extensions. We evaluate on our open-sourced Cloud-IaC-6, a suite of cloud provisioning tasks with heterogeneous LLM-generated traces, and WFCommons scientific workflows. BPOP recover dependency structure more accurately than trace-only and process-mining baselines, and the inferred graphs support a compiled executor that prunes irrelevant context, yielding substantial reductions in token usage and execution time."}
{"id": "2602.02945", "categories": ["stat.CO", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.02945", "abs": "https://arxiv.org/abs/2602.02945", "authors": ["Nicholas Polson", "Vadim Sokolov"], "title": "Bayesian Methods for the Navier-Stokes Equations", "comment": null, "summary": "We develop a Bayesian methodology for numerical solution of the incompressible Navier--Stokes equations with quantified uncertainty. The central idea is to treat discretized Navier--Stokes dynamics as a state-space model and to view numerical solution as posterior computation: priors encode physical structure and modeling error, and the solver outputs a distribution over states and quantities of interest rather than a single trajectory. In two dimensions, stochastic representations (Feynman--Kac and stochastic characteristics for linear advection--diffusion with prescribed drift) motivate Monte Carlo solvers and provide intuition for uncertainty propagation. In three dimensions, we formulate stochastic Navier--Stokes models and describe particle-based and ensemble-based Bayesian workflows for uncertainty propagation in spectral discretizations. A key computational advantage is that parameter learning can be performed stably via particle learning: marginalization and resample--propagate (one-step smoothing) constructions avoid the weight-collapse that plagues naive sequential importance sampling on static parameters. When partial observations are available, the same machinery supports sequential observational updating as an additional capability. We also discuss non-Gaussian (heavy-tailed) error models based on normal variance-mean mixtures, which yield conditionally Gaussian updates via latent scale augmentation."}
{"id": "2602.02791", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02791", "abs": "https://arxiv.org/abs/2602.02791", "authors": ["Yuzhen Zhao", "Jiarong Fan", "Yating Liu"], "title": "Plug-In Classification of Drift Functions in Diffusion Processes Using Neural Networks", "comment": null, "summary": "We study a supervised multiclass classification problem for diffusion processes, where each class is characterized by a distinct drift function and trajectories are observed at discrete times. Extending the one-dimensional multiclass framework of Denis et al. (2024) to multidimensional diffusions, we propose a neural network-based plug-in classifier that estimates the drift functions for each class from independent sample paths and assigns labels based on a Bayes-type decision rule. Under standard regularity assumptions, we establish convergence rates for the excess misclassification risk, explicitly capturing the effects of drift estimation error and time discretization. Numerical experiments demonstrate that the proposed method achieves faster convergence and improved classification performance compared to Denis et al. (2024) in the one-dimensional setting, remains effective in higher dimensions when the underlying drift functions admit a compositional structure, and consistently outperforms direct neural network classifiers trained end-to-end on trajectories without exploiting the diffusion model structure."}
{"id": "2602.02753", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02753", "abs": "https://arxiv.org/abs/2602.02753", "authors": ["Youngjin Cho", "Meimei Liu"], "title": "Effect-Wise Inference for Smoothing Spline ANOVA on Tensor-Product Sobolev Space", "comment": null, "summary": "Functional ANOVA provides a nonparametric modeling framework for multivariate covariates, enabling flexible estimation and interpretation of effect functions such as main effects and interaction effects. However, effect-wise inference in such models remains challenging. Existing methods focus primarily on inference for entire functions rather than individual effects. Methods addressing effect-wise inference face substantial limitations: the inability to accommodate interactions, a lack of rigorous theoretical foundations, or restriction to pointwise inference. To address these limitations, we develop a unified framework for effect-wise inference in smoothing spline ANOVA on a subspace of tensor product Sobolev space. For each effect function, we establish rates of convergence, pointwise confidence intervals, and a Wald-type test for whether the effect is zero, with power achieving the minimax distinguishable rate up to a logarithmic factor. Main effects achieve the optimal univariate rates, and interactions achieve optimal rates up to logarithmic factors. The theoretical foundation relies on an orthogonality decomposition of effect subspaces, which enables the extension of the functional Bahadur representation framework to effect-wise inference in smoothing spline ANOVA with interactions. Simulation studies and real-data application to the Colorado temperature dataset demonstrate superior performance compared to existing methods."}
{"id": "2602.02633", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02633", "abs": "https://arxiv.org/abs/2602.02633", "authors": ["Tahir Qasim Syed", "Behraj Khan"], "title": "Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation", "comment": null, "summary": "Often, constraints arise in deployment settings where even lightweight parameter updates e.g. parameter-efficient fine-tuning could induce model shift or tuning instability. We study test-time adaptation of foundation models for few-shot classification under a completely frozen-model regime, where additionally, no upstream data are accessible. We propose arguably the first training-free inference method that adapts predictions to the new task by performing a change of measure over the latent embedding distribution induced by the encoder. Using task-similarity scores derived from a small labeled support set, exponential tilting reweights latent distributions in a KL-optimal manner without modifying model parameters. Empirically, the method consistently competes with parameter-update-based methods across multiple benchmarks and shot regimes, while operating under strictly and universally stronger constraints. These results demonstrate the viability of inference-level distributional correction for test-time adaptation even with a fully-frozen model pipeline."}
{"id": "2602.02813", "categories": ["stat.AP", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02813", "abs": "https://arxiv.org/abs/2602.02813", "authors": ["Sanjit Dandapanthula", "Margaret Johnson", "Madeleine Pascolini-Campbell", "Glynn Hulley", "Mikael Kuusela"], "title": "Downscaling land surface temperature data using edge detection and block-diagonal Gaussian process regression", "comment": null, "summary": "Accurate and high-resolution estimation of land surface temperature (LST) is crucial in estimating evapotranspiration, a measure of plant water use and a central quantity in agricultural applications. In this work, we develop a novel statistical method for downscaling LST data obtained from NASA's ECOSTRESS mission, using high-resolution data from the Landsat 8 mission as a proxy for modeling agricultural field structure. Using the Landsat data, we identify the boundaries of agricultural fields through edge detection techniques, allowing us to capture the inherent block structure present in the spatial domain. We propose a block-diagonal Gaussian process (BDGP) model that captures the spatial structure of the agricultural fields, leverages independence of LST across fields for computational tractability, and accounts for the change of support present in ECOSTRESS observations. We use the resulting BDGP model to perform Gaussian process regression and obtain high-resolution estimates of LST from ECOSTRESS data, along with uncertainty quantification. Our results demonstrate the practicality of the proposed method in producing reliable high-resolution LST estimates, with potential applications in agriculture, urban planning, and climate studies."}
{"id": "2602.03343", "categories": ["stat.CO", "q-bio.GN", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.03343", "abs": "https://arxiv.org/abs/2602.03343", "authors": ["Georgy Meshcheryakov", "Andrey I. Buyan"], "title": "MARADONER: Motif Activity Response Analysis Done Right", "comment": null, "summary": "Inferring the activities of transcription factors from high-throughput transcriptomic or open chromatin profiling, such as RNA-/CAGE-/ATAC-Seq, is a long-standing challenge in systems biology. Identification of highly active master regulators enables mechanistic interpretation of differential gene expression, chromatin state changes, or perturbation responses across conditions, cell types, and diseases. Here, we describe MARADONER, a statistical framework and its software implementation for motif activity response analysis (MARA), utilizing the sequence-level features obtained with pattern matching (motif scanning) of individual promoters and promoter- or gene-level activity or expression estimates. Compared to the classic MARA, MARADONER (MARA-done-right) employs an unbiased variance parameter estimation and a bias-adjusted likelihood estimation of fixed effects, thereby enhancing goodness-of-fit and the accuracy of activity estimation. Further, MARADONER is capable of accounting for heteroscedasticity of motif scores and activity estimates."}
{"id": "2602.02875", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02875", "abs": "https://arxiv.org/abs/2602.02875", "authors": ["F. A. Shiha"], "title": "Shiha Distribution: Statistical Properties and Applications to Reliability Engineering and Environmental Data", "comment": null, "summary": "This paper introduces a new two-parameter distribution, referred to as the Shiha distribution, which provides a flexible model for skewed lifetime data with either heavy or light tails. The proposed distribution is applicable to various fields, including reliability engineering, environmental studies, and related areas. We derive its main statistical properties, including the moment generating function, moments, hazard rate function, quantile function, and entropy. The stress--strength reliability parameter is also derived in closed form. A simulation study is conducted to evaluate its performance. Applications to several real data sets demonstrate that the Shiha distribution consistently provides a superior fit compared with established competing models, confirming its practical effectiveness for lifetime data analysis."}
{"id": "2602.02771", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02771", "abs": "https://arxiv.org/abs/2602.02771", "authors": ["J. Brandon Carter", "Catherine A. Calder"], "title": "Markov Random Fields: Structural Properties, Phase Transition, and Response Function Analysis", "comment": null, "summary": "This paper presents a focused review of Markov random fields (MRFs)--commonly used probabilistic representations of spatial dependence in discrete spatial domains--for categorical data, with an emphasis on models for binary-valued observations or latent variables. We examine core structural properties of these models, including clique factorization, conditional independence, and the role of neighborhood structures. We also discuss the phenomenon of phase transition and its implications for statistical model specification and inference. A central contribution of this review is the use of response functions, a unifying tool we introduce for prior analysis that provides insight into how different formulations of MRFs influence implied marginal and joint distributions. We illustrate these concepts through a case study of direct-data MRF models with covariates, highlighting how different formulations encode dependence. While our focus is on binary fields, the principles outlined here extend naturally to more complex categorical MRFs and we draw connections to these higher-dimensional modeling scenarios. This review provides both theoretical grounding and practical tools for interpreting and extending MRF-based models."}
{"id": "2602.02759", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02759", "abs": "https://arxiv.org/abs/2602.02759", "authors": ["John Hood", "Aaron Schein"], "title": "Near-Universal Multiplicative Updates for Nonnegative Einsum Factorization", "comment": "26 pages, 5 figures", "summary": "Despite the ubiquity of multiway data across scientific domains, there are few user-friendly tools that fit tailored nonnegative tensor factorizations. Researchers may use gradient-based automatic differentiation (which often struggles in nonnegative settings), choose between a limited set of methods with mature implementations, or implement their own model from scratch. As an alternative, we introduce NNEinFact, an einsum-based multiplicative update algorithm that fits any nonnegative tensor factorization expressible as a tensor contraction by minimizing one of many user-specified loss functions (including the $(α,β)$-divergence). To use NNEinFact, the researcher simply specifies their model with a string. NNEinFact converges to a local minimum of the loss, supports missing data, and fits to tensors with hundreds of millions of entries in seconds. Empirically, NNEinFact fits custom models which outperform standard ones in heldout prediction tasks on real-world tensor data by over $37\\%$ and attains less than half the test loss of gradient-based methods while converging up to 90 times faster."}
{"id": "2602.02825", "categories": ["stat.AP", "q-bio.QM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02825", "abs": "https://arxiv.org/abs/2602.02825", "authors": ["Jiayu Su", "Jun Hou Fung", "Haoyu Wang", "Dian Yang", "David A. Knowles", "Raul Rabadan"], "title": "On the consistent and scalable detection of spatial patterns", "comment": null, "summary": "Detecting spatial patterns is fundamental to scientific discovery, yet current methods lack statistical consensus and face computational barriers when applied to large-scale spatial omics datasets. We unify major approaches through a single quadratic form and derive general consistency conditions. We reveal that several widely used methods, including Moran's I, are inconsistent, and propose scalable corrections. The resulting test enables robust pattern detection across millions of spatial locations and single-cell lineage-tracing datasets."}
{"id": "2602.03413", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2602.03413", "abs": "https://arxiv.org/abs/2602.03413", "authors": ["Van Chien Ta", "Thi Mai Hong Chu", "Minh-Ngoc Tran"], "title": "On the Convergence of Wasserstein Gradient Descent for Sampling", "comment": null, "summary": "This paper studies the optimization of the KL functional on the Wasserstein space of probability measures, and develops a sampling framework based on Wasserstein gradient descent (WGD). We identify two important subclasses of the Wasserstein space for which the WGD scheme is guaranteed to converge, thereby providing new theoretical foundations for optimization-based sampling methods on measure spaces. For practical implementation, we construct a particle-based WGD algorithm in which the score function is estimated via score matching. Through a series of numerical experiments, we demonstrate that WGD can provide good approximation to a variety of complex target distributions, including those that pose substantial challenges for standard MCMC and parametric variational Bayes methods. These results suggest that WGD offers a promising and flexible alternative for scalable Bayesian inference in high-dimensional or multimodal settings."}
{"id": "2602.03049", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.03049", "abs": "https://arxiv.org/abs/2602.03049", "authors": ["Zhixian Zhang", "Xiaotian Hou", "Linjun Zhang"], "title": "Unified Inference Framework for Single and Multi-Player Performative Prediction: Method and Asymptotic Optimality", "comment": null, "summary": "Performative prediction characterizes environments where predictive models alter the very data distributions they aim to forecast, triggering complex feedback loops. While prior research treats single-agent and multi-agent performativity as distinct phenomena, this paper introduces a unified statistical inference framework that bridges these contexts, treating the former as a special case of the latter. Our contribution is two-fold. First, we put forward the Repeated Risk Minimization (RRM) procedure for estimating the performative stability, and establish a rigorous inferential theory for admitting its asymptotic normality and confirming its asymptotic efficiency. Second, for the performative optimality, we introduce a novel two-step plug-in estimator that integrates the idea of Recalibrated Prediction Powered Inference (RePPI) with Importance Sampling, and further provide formal derivations for the Central Limit Theorems of both the underlying distributional parameters and the plug-in results. The theoretical analysis demonstrates that our estimator achieves the semiparametric efficiency bound and maintains robustness under mild distributional misspecification. This work provides a principled toolkit for reliable estimation and decision-making in dynamic, performative environments."}
{"id": "2602.02777", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02777", "abs": "https://arxiv.org/abs/2602.02777", "authors": ["Isqeel Ogunsola", "Olatunji Johnson"], "title": "Disentangling spatial interference and spatial confounding biases in causal inference", "comment": null, "summary": "Spatial interference and spatial confounding are two major issues inhibiting precise causal estimates when dealing with observational spatial data. Moreover, the definition and interpretation of spatial confounding remain arguable in the literature. In this paper, our goal is to provide clarity in a novel way on misconception and issues around spatial confounding from Directed Acyclic Graph (DAG) perspective and to disentangle both direct, indirect spatial confounding and spatial interference based on bias induced on causal estimates. Also, existing analyses of spatial confounding bias typically rely on Normality assumptions for treatments and confounders, assumptions that are often violated in practice. Relaxing these assumptions, we derive analytical expressions for spatial confounding bias under more general distributional settings using Poisson as example . We showed that the choice of spatial weights, the distribution of the treatment, and the magnitude of interference critically determine the extent of bias due to spatial interference. We further demonstrate that direct and indirect spatial confounding can be disentangled, with both the weight matrix and the nature of exposure playing central roles in determining the magnitude of indirect bias. Theoretical results are supported by simulation studies and an application to real-world spatial data. In future, parametric frameworks for concomitantly adjusting for spatial interference, direct and indirect spatial confounding for both direct and mediated effects estimation will be developed."}
{"id": "2602.02791", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02791", "abs": "https://arxiv.org/abs/2602.02791", "authors": ["Yuzhen Zhao", "Jiarong Fan", "Yating Liu"], "title": "Plug-In Classification of Drift Functions in Diffusion Processes Using Neural Networks", "comment": null, "summary": "We study a supervised multiclass classification problem for diffusion processes, where each class is characterized by a distinct drift function and trajectories are observed at discrete times. Extending the one-dimensional multiclass framework of Denis et al. (2024) to multidimensional diffusions, we propose a neural network-based plug-in classifier that estimates the drift functions for each class from independent sample paths and assigns labels based on a Bayes-type decision rule. Under standard regularity assumptions, we establish convergence rates for the excess misclassification risk, explicitly capturing the effects of drift estimation error and time discretization. Numerical experiments demonstrate that the proposed method achieves faster convergence and improved classification performance compared to Denis et al. (2024) in the one-dimensional setting, remains effective in higher dimensions when the underlying drift functions admit a compositional structure, and consistently outperforms direct neural network classifiers trained end-to-end on trajectories without exploiting the diffusion model structure."}
{"id": "2602.03609", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.03609", "abs": "https://arxiv.org/abs/2602.03609", "authors": ["Tim Gyger", "Reinhard Furrer", "Fabio Sigrist"], "title": "Scalable non-separable spatio-temporal Gaussian process models for large-scale short-term weather prediction", "comment": null, "summary": "Monitoring daily weather fields is critical for climate science, agriculture, and environmental planning, yet fully probabilistic spatio-temporal models become computationally prohibitive at continental scale. We present a case study on short-term forecasting of daily maximum temperature and precipitation across the conterminous United States using novel scalable spatio-temporal Gaussian process methodology. Building on three approximation families - inducing-point methods (FITC), Vecchia approximations, and a hybrid Vecchia-inducing-point full-scale approach (VIF) - we introduce three extensions that address key bottlenecks in large space-time settings: (i) a scalable correlation-based neighbor selection strategy for Vecchia approximations with point-referenced data, enabling accurate conditioning under complex dependence structures, (ii) a space-time kMeans++ inducing-point selection algorithm, and (iii) GPU-accelerated implementations of computationally expensive operations, including matrix operations and neighbor searches. Using both synthetic experiments and a large NOAA station dataset containing approximately 1.7 million space-time observations, we analyze the models with respect to predictive performance, parameter estimation, and computational efficiency. Our results demonstrate that scalable Gaussian process models can yield accurate continental-scale forecasts while remaining computationally feasible, offering practical tools for weather applications."}
{"id": "2602.02813", "categories": ["stat.AP", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02813", "abs": "https://arxiv.org/abs/2602.02813", "authors": ["Sanjit Dandapanthula", "Margaret Johnson", "Madeleine Pascolini-Campbell", "Glynn Hulley", "Mikael Kuusela"], "title": "Downscaling land surface temperature data using edge detection and block-diagonal Gaussian process regression", "comment": null, "summary": "Accurate and high-resolution estimation of land surface temperature (LST) is crucial in estimating evapotranspiration, a measure of plant water use and a central quantity in agricultural applications. In this work, we develop a novel statistical method for downscaling LST data obtained from NASA's ECOSTRESS mission, using high-resolution data from the Landsat 8 mission as a proxy for modeling agricultural field structure. Using the Landsat data, we identify the boundaries of agricultural fields through edge detection techniques, allowing us to capture the inherent block structure present in the spatial domain. We propose a block-diagonal Gaussian process (BDGP) model that captures the spatial structure of the agricultural fields, leverages independence of LST across fields for computational tractability, and accounts for the change of support present in ECOSTRESS observations. We use the resulting BDGP model to perform Gaussian process regression and obtain high-resolution estimates of LST from ECOSTRESS data, along with uncertainty quantification. Our results demonstrate the practicality of the proposed method in producing reliable high-resolution LST estimates, with potential applications in agriculture, urban planning, and climate studies."}
{"id": "2602.02809", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02809", "abs": "https://arxiv.org/abs/2602.02809", "authors": ["Zhiwei Zhang", "Peisong Han", "Wei Zhang"], "title": "A Model-Robust G-Computation Method for Analyzing Hybrid Control Studies Without Assuming Exchangeability", "comment": null, "summary": "There is growing interest in a hybrid control design for treatment evaluation, where a randomized controlled trial is augmented with external control data from a previous trial or a real world data source. The hybrid control design has the potential to improve efficiency but also carries the risk of introducing bias. The potential bias in a hybrid control study can be mitigated by adjusting for baseline covariates that are related to the control outcome. Existing methods that serve this purpose commonly assume that the internal and external control outcomes are exchangeable upon conditioning on a set of measured covariates. Possible violations of the exchangeability assumption can be addressed using a g-computation method with variable selection under a correctly specified outcome regression model. In this article, we note that a particular version of this g-computation method is protected against misspecification of the outcome regression model. This observation leads to a model-robust g-computation method that is remarkably simple and easy to implement, consistent and asymptotically normal under minimal assumptions, and able to improve efficiency by exploiting similarities between the internal and external control groups. The method is evaluated in a simulation study and illustrated using real data from HIV treatment trials."}
{"id": "2602.02927", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02927", "abs": "https://arxiv.org/abs/2602.02927", "authors": ["Yidong Ouyang", "Panwen Hu", "Zhengyan Wan", "Zhe Wang", "Liyan Xie", "Dmitriy Bespalov", "Ying Nian Wu", "Guang Cheng", "Hongyuan Zha", "Qiang Sun"], "title": "Training-Free Self-Correction for Multimodal Masked Diffusion Models", "comment": null, "summary": "Masked diffusion models have emerged as a powerful framework for text and multimodal generation. However, their sampling procedure updates multiple tokens simultaneously and treats generated tokens as immutable, which may lead to error accumulation when early mistakes cannot be revised. In this work, we revisit existing self-correction methods and identify limitations stemming from additional training requirements or reliance on misaligned likelihood estimates. We propose a training-free self-correction framework that exploits the inductive biases of pre-trained masked diffusion models. Without modifying model parameters or introducing auxiliary evaluators, our method significantly improves generation quality on text-to-image generation and multimodal understanding tasks with reduced sampling steps. Moreover, the proposed framework generalizes across different masked diffusion architectures, highlighting its robustness and practical applicability. Code can be found in https://github.com/huge123/FreeCorrection."}
{"id": "2602.03077", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.03077", "abs": "https://arxiv.org/abs/2602.03077", "authors": ["Ziang Zhang", "Peter Carbonetto", "Matthew Stephens"], "title": "Empirical Bayes Shrinkage of Functional Effects, with Application to Analysis of Dynamic eQTLs", "comment": null, "summary": "We introduce functional adaptive shrinkage (FASH), an empirical Bayes method for joint analysis of observation units in which each unit estimates an effect function at several values of a continuous condition variable. The ideas in this paper are motivated by dynamic expression quantitative trait locus (eQTL) studies, which aim to characterize how genetic effects on gene expression vary with time or another continuous condition. FASH integrates a broad family of Gaussian processes defined through linear differential operators into an empirical Bayes shrinkage framework, enabling adaptive smoothing and borrowing of information across units. This provides improved estimation of effect functions and principled hypothesis testing, allowing straightforward computation of significance measures such as local false discovery and false sign rates. To encourage conservative inferences, we propose a simple prior- adjustment method that has theoretical guarantees and can be more broadly used with other empirical Bayes methods. We illustrate the benefits of FASH by reanalyzing dynamic eQTL data on cardiomyocyte differentiation from induced pluripotent stem cells. FASH identified novel dynamic eQTLs, revealed diverse temporal effect patterns, and provided improved power compared with the original analysis. More broadly, FASH offers a flexible statistical framework for joint analysis of functional data, with applications extending beyond genomics. To facilitate use of FASH in dynamic eQTL studies and other settings, we provide an accompanying R package at https: //github.com/stephenslab/fashr."}
{"id": "2602.03483", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.03483", "abs": "https://arxiv.org/abs/2602.03483", "authors": ["Francisco Cuevas-Pacheco", "Jonathan Acosta"], "title": "Kriging for large datasets via penalized neighbor selection", "comment": "Submitted for Journal publication", "summary": "Kriging is a fundamental tool for spatial prediction, but its computational complexity of $O(N^3)$ becomes prohibitive for large datasets. While local kriging using $K$-nearest neighbors addresses this issue, the selection of $K$ typically relies on ad-hoc criteria that fail to account for spatial correlation structure. We propose a penalized kriging framework that incorporates LASSO-type penalties directly into the kriging equations to achieve automatic, data-driven neighbor selection. We further extend this to adaptive LASSO, using data-driven penalty weights that account for the spatial correlation structure. Our method determines which observations contribute non-zero weights through $\\ell_1$ regularization, with the penalty parameter selected via a novel criterion based on effective sample size that balances prediction accuracy against information redundancy. Numerical experiments demonstrate that penalized kriging automatically adapts neighborhood structure to the underlying spatial correlation, selecting fewer neighbors for smoother processes and more for highly variable fields, while maintaining prediction accuracy comparable to global kriging at substantially reduced computational cost."}
{"id": "2602.02860", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02860", "abs": "https://arxiv.org/abs/2602.02860", "authors": ["Ruiyan Luo", "Xin Qi"], "title": "Functional regression with multivariate responses", "comment": null, "summary": "We consider the functional regression model with multivariate response and functional predictors. Compared to fitting each individual response variable separately, taking advantage of the correlation between the response variables can improve the estimation and prediction accuracy. Using information in both functional predictors and multivariate response, we identify the optimal decomposition of the coefficient functions for prediction in population level. Then we propose methods to estimate this decomposition and fit the regression model for the situations of a small and a large number $p$ of functional predictors separately. For a large $p$, we propose a simultaneous smooth-sparse penalty which can both make curve selection and improve estimation and prediction accuracy. We provide the asymptotic results when both the sample size and the number of functional predictors go to infinity. Our method can be applied to models with thousands of functional predictors and has been implemented in the R package FRegSigCom."}
{"id": "2602.03049", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.03049", "abs": "https://arxiv.org/abs/2602.03049", "authors": ["Zhixian Zhang", "Xiaotian Hou", "Linjun Zhang"], "title": "Unified Inference Framework for Single and Multi-Player Performative Prediction: Method and Asymptotic Optimality", "comment": null, "summary": "Performative prediction characterizes environments where predictive models alter the very data distributions they aim to forecast, triggering complex feedback loops. While prior research treats single-agent and multi-agent performativity as distinct phenomena, this paper introduces a unified statistical inference framework that bridges these contexts, treating the former as a special case of the latter. Our contribution is two-fold. First, we put forward the Repeated Risk Minimization (RRM) procedure for estimating the performative stability, and establish a rigorous inferential theory for admitting its asymptotic normality and confirming its asymptotic efficiency. Second, for the performative optimality, we introduce a novel two-step plug-in estimator that integrates the idea of Recalibrated Prediction Powered Inference (RePPI) with Importance Sampling, and further provide formal derivations for the Central Limit Theorems of both the underlying distributional parameters and the plug-in results. The theoretical analysis demonstrates that our estimator achieves the semiparametric efficiency bound and maintains robustness under mild distributional misspecification. This work provides a principled toolkit for reliable estimation and decision-making in dynamic, performative environments."}
{"id": "2602.03218", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.03218", "abs": "https://arxiv.org/abs/2602.03218", "authors": ["Hirotada Maeda", "Satoshi Hattori", "Tim Friede"], "title": "Blinded sample size re-estimation accounting for uncertainty in mid-trial estimation", "comment": null, "summary": "For randomized controlled trials to be conclusive, it is important to set the target sample size accurately at the design stage. Comparing two normal populations, the sample size calculation requires specification of the variance other than the treatment effect and misspecification can lead to underpowered studies. Blinded sample size re-estimation is an approach to minimize the risk of inconclusive studies. Existing methods proposed to use the total (one-sample) variance that is estimable from blinded data without knowledge of the treatment allocation. We demonstrate that, since the expectation of this estimator is greater than or equal to the true variance, the one-sample variance approach can be regarded as providing an upper bound of the variance in blind reviews. This worst-case evaluation can likely reduce a risk of underpowered studies. However, blinded reviews of small sample size may still lead to underpowered studies. We propose a refined method accounting for estimation error in blind reviews using an upper confidence limit of the variance. A similar idea had been proposed in the setting of external pilot studies. Furthermore, we developed a method to select an appropriate confidence level so that the re-estimated sample size attains the target power. Numerical studies showed that our method works well and outperforms existing methods. The proposed procedure is motivated and illustrated by recent randomized clinical trials."}
{"id": "2602.02875", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02875", "abs": "https://arxiv.org/abs/2602.02875", "authors": ["F. A. Shiha"], "title": "Shiha Distribution: Statistical Properties and Applications to Reliability Engineering and Environmental Data", "comment": null, "summary": "This paper introduces a new two-parameter distribution, referred to as the Shiha distribution, which provides a flexible model for skewed lifetime data with either heavy or light tails. The proposed distribution is applicable to various fields, including reliability engineering, environmental studies, and related areas. We derive its main statistical properties, including the moment generating function, moments, hazard rate function, quantile function, and entropy. The stress--strength reliability parameter is also derived in closed form. A simulation study is conducted to evaluate its performance. Applications to several real data sets demonstrate that the Shiha distribution consistently provides a superior fit compared with established competing models, confirming its practical effectiveness for lifetime data analysis."}
{"id": "2602.03168", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03168", "abs": "https://arxiv.org/abs/2602.03168", "authors": ["Tuo Liu", "Edgar Dobriban", "Francesco Orabona"], "title": "Online Conformal Prediction via Universal Portfolio Algorithms", "comment": null, "summary": "Online conformal prediction (OCP) seeks prediction intervals that achieve long-run $1-α$ coverage for arbitrary (possibly adversarial) data streams, while remaining as informative as possible. Existing OCP methods often require manual learning-rate tuning to work well, and may also require algorithm-specific analyses. Here, we develop a general regret-to-coverage theory for interval-valued OCP based on the $(1-α)$-pinball loss. Our first contribution is to identify \\emph{linearized regret} as a key notion, showing that controlling it implies coverage bounds for any online algorithm. This relies on a black-box reduction that depends only on the Fenchel conjugate of an upper bound on the linearized regret. Building on this theory, we propose UP-OCP, a parameter-free method for OCP, via a reduction to a two-asset portfolio selection problem, leveraging universal portfolio algorithms. We show strong finite-time bounds on the miscoverage of UP-OCP, even for polynomially growing predictions. Extensive experiments support that UP-OCP delivers consistently better size/coverage trade-offs than prior online conformal baselines."}
{"id": "2602.03449", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.03449", "abs": "https://arxiv.org/abs/2602.03449", "authors": ["Fabian Schneider", "Meghdoot Mozumder", "Konstantin Tamarov", "Leila Taghizadeh", "Tanja Tarvainen", "Tapio Helin", "Duc-Lam Duong"], "title": "Score-based diffusion models for diffuse optical tomography with uncertainty quantification", "comment": null, "summary": "Score-based diffusion models are a recently developed framework for posterior sampling in Bayesian inverse problems with a state-of-the-art performance for severely ill-posed problems by leveraging a powerful prior distribution learned from empirical data. Despite generating significant interest especially in the machine-learning community, a thorough study of realistic inverse problems in the presence of modelling error and utilization of physical measurement data is still outstanding. In this work, the framework of unconditional representation for the conditional score function (UCoS) is evaluated for linearized difference imaging in diffuse optical tomography (DOT). DOT uses boundary measurements of near-infrared light to estimate the spatial distribution of absorption and scattering parameters in biological tissues. The problem is highly ill-posed and thus sensitive to noise and modelling errors. We introduce a novel regularization approach that prevents overfitting of the score function by constructing a mixed score composed of a learned and a model-based component. Validation of this approach is done using both simulated and experimental measurement data. The experiments demonstrate that a data-driven prior distribution results in posterior samples with low variance, compared to classical model-based estimation, and centred around the ground truth, even in the context of a highly ill-posed problem and in the presence of modelling errors."}
{"id": "2602.02931", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02931", "abs": "https://arxiv.org/abs/2602.02931", "authors": ["Kevin McCoy", "Zachary Wooten", "Katarzyna Tomczak", "Christine B. Peterson"], "title": "Weighted Sum-of-Trees Model for Clustered Data", "comment": "14 pages, 8 figures, 3 tables", "summary": "Clustered data, which arise when observations are nested within groups, are incredibly common in clinical, education, and social science research. Traditionally, a linear mixed model, which includes random effects to account for within-group correlation, would be used to model the observed data and make new predictions on unseen data. Some work has been done to extend the mixed model approach beyond linear regression into more complex and non-parametric models, such as decision trees and random forests. However, existing methods are limited to using the global fixed effects for prediction on data from out-of-sample groups, effectively assuming that all clusters share a common outcome model. We propose a lightweight sum-of-trees model in which we learn a decision tree for each sample group. We combine the predictions from these trees using weights so that out-of-sample group predictions are more closely aligned with the most similar groups in the training data. This strategy also allows for inference on the similarity across groups in the outcome prediction model, as the unique tree structures and variable importances for each group can be directly compared. We show our model outperforms traditional decision trees and random forests in a variety of simulation settings. Finally, we showcase our method on real-world data from the sarcoma cohort of The Cancer Genome Atlas, where patient samples are grouped by sarcoma subtype."}
{"id": "2602.03169", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03169", "abs": "https://arxiv.org/abs/2602.03169", "authors": ["Xinyang Xiong", "Siyuan jiang", "Pengcheng Zeng"], "title": "NeuralFLoC: Neural Flow-Based Joint Registration and Clustering of Functional Data", "comment": null, "summary": "Clustering functional data in the presence of phase variation is challenging, as temporal misalignment can obscure intrinsic shape differences and degrade clustering performance. Most existing approaches treat registration and clustering as separate tasks or rely on restrictive parametric assumptions. We present \\textbf{NeuralFLoC}, a fully unsupervised, end-to-end deep learning framework for joint functional registration and clustering based on Neural ODE-driven diffeomorphic flows and spectral clustering. The proposed model learns smooth, invertible warping functions and cluster-specific templates simultaneously, effectively disentangling phase and amplitude variation. We establish universal approximation guarantees and asymptotic consistency for the proposed framework. Experiments on functional benchmarks show state-of-the-art performance in both registration and clustering, with robustness to missing data, irregular sampling, and noise, while maintaining scalability. Code is available at https://anonymous.4open.science/r/NeuralFLoC-FEC8."}
{"id": "2602.03077", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.03077", "abs": "https://arxiv.org/abs/2602.03077", "authors": ["Ziang Zhang", "Peter Carbonetto", "Matthew Stephens"], "title": "Empirical Bayes Shrinkage of Functional Effects, with Application to Analysis of Dynamic eQTLs", "comment": null, "summary": "We introduce functional adaptive shrinkage (FASH), an empirical Bayes method for joint analysis of observation units in which each unit estimates an effect function at several values of a continuous condition variable. The ideas in this paper are motivated by dynamic expression quantitative trait locus (eQTL) studies, which aim to characterize how genetic effects on gene expression vary with time or another continuous condition. FASH integrates a broad family of Gaussian processes defined through linear differential operators into an empirical Bayes shrinkage framework, enabling adaptive smoothing and borrowing of information across units. This provides improved estimation of effect functions and principled hypothesis testing, allowing straightforward computation of significance measures such as local false discovery and false sign rates. To encourage conservative inferences, we propose a simple prior- adjustment method that has theoretical guarantees and can be more broadly used with other empirical Bayes methods. We illustrate the benefits of FASH by reanalyzing dynamic eQTL data on cardiomyocyte differentiation from induced pluripotent stem cells. FASH identified novel dynamic eQTLs, revealed diverse temporal effect patterns, and provided improved power compared with the original analysis. More broadly, FASH offers a flexible statistical framework for joint analysis of functional data, with applications extending beyond genomics. To facilitate use of FASH in dynamic eQTL studies and other settings, we provide an accompanying R package at https: //github.com/stephenslab/fashr."}
{"id": "2602.03215", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03215", "abs": "https://arxiv.org/abs/2602.03215", "authors": ["Benjamin Maurel", "Agathe Guilloux", "Sarah Zohar", "Moreno Ursino", "Jean-Baptiste Woillard"], "title": "Latent Neural-ODE for Model-Informed Precision Dosing: Overcoming Structural Assumptions in Pharmacokinetics", "comment": null, "summary": "Accurate estimation of tacrolimus exposure, quantified by the area under the concentration-time curve (AUC), is essential for precision dosing after renal transplantation. Current practice relies on population pharmacokinetic (PopPK) models based on nonlinear mixed-effects (NLME) methods. However, these models depend on rigid, pre-specified assumptions and may struggle to capture complex, patient-specific dynamics, leading to model misspecification.\n  In this study, we introduce a novel data-driven alternative based on Latent Ordinary Differential Equations (Latent ODEs) for tacrolimus AUC prediction. This deep learning approach learns individualized pharmacokinetic dynamics directly from sparse clinical data, enabling greater flexibility in modeling complex biological behavior. The model was evaluated through extensive simulations across multiple scenarios and benchmarked against two standard approaches: NLME-based estimation and the iterative two-stage Bayesian (it2B) method. We further performed a rigorous clinical validation using a development dataset (n = 178) and a completely independent external dataset (n = 75).\n  In simulation, the Latent ODE model demonstrated superior robustness, maintaining high accuracy even when underlying biological mechanisms deviated from standard assumptions. Regarding experiments on clinical datasets, in internal validation, it achieved significantly higher precision with a mean RMSPE of 7.99% compared with 9.24% for it2B (p < 0.001). On the external cohort, it achieved an RMSPE of 10.82%, comparable to the two standard estimators (11.48% and 11.54%).\n  These results establish the Latent ODE as a powerful and reliable tool for AUC prediction. Its flexible architecture provides a promising foundation for next-generation, multi-modal models in personalized medicine."}
{"id": "2602.03165", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03165", "abs": "https://arxiv.org/abs/2602.03165", "authors": ["Anas Cherradi", "Yazid Janati", "Alain Durmus", "Sylvain Le Corff", "Yohan Petetin", "Julien Stoehr"], "title": "Entropic Mirror Monte Carlo", "comment": null, "summary": "Importance sampling is a Monte Carlo method which designs estimators of expectations under a target distribution using weighted samples from a proposal distribution. When the target distribution is complex, such as multimodal distributions in highdimensional spaces, the efficiency of importance sampling critically depends on the choice of the proposal distribution. In this paper, we propose a novel adaptive scheme for the construction of efficient proposal distributions. Our algorithm promotes efficient exploration of the target distribution by combining global sampling mechanisms with a delayed weighting procedure. The proposed weighting mechanism plays a key role by enabling rapid resampling in regions where the proposal distribution is poorly adapted to the target. Our sampling algorithm is shown to be geometrically convergent under mild assumptions and is illustrated through various numerical experiments."}
{"id": "2602.03258", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03258", "abs": "https://arxiv.org/abs/2602.03258", "authors": ["Rémi Khellaf", "Erwan Scornet", "Aurélien Bellet", "Julie Josse"], "title": "Principled Federated Random Forests for Heterogeneous Data", "comment": null, "summary": "Random Forests (RF) are among the most powerful and widely used predictive models for centralized tabular data, yet few methods exist to adapt them to the federated learning setting. Unlike most federated learning approaches, the piecewise-constant nature of RF prevents exact gradient-based optimization. As a result, existing federated RF implementations rely on unprincipled heuristics: for instance, aggregating decision trees trained independently on clients fails to optimize the global impurity criterion, even under simple distribution shifts. We propose FedForest, a new federated RF algorithm for horizontally partitioned data that naturally accommodates diverse forms of client data heterogeneity, from covariate shift to more complex outcome shift mechanisms. We prove that our splitting procedure, based on aggregating carefully chosen client statistics, closely approximates the split selected by a centralized algorithm. Moreover, FedForest allows splits on client indicators, enabling a non-parametric form of personalization that is absent from prior federated random forest methods. Empirically, we demonstrate that the resulting federated forests closely match centralized performance across heterogeneous benchmarks while remaining communication-efficient."}
{"id": "2602.03218", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.03218", "abs": "https://arxiv.org/abs/2602.03218", "authors": ["Hirotada Maeda", "Satoshi Hattori", "Tim Friede"], "title": "Blinded sample size re-estimation accounting for uncertainty in mid-trial estimation", "comment": null, "summary": "For randomized controlled trials to be conclusive, it is important to set the target sample size accurately at the design stage. Comparing two normal populations, the sample size calculation requires specification of the variance other than the treatment effect and misspecification can lead to underpowered studies. Blinded sample size re-estimation is an approach to minimize the risk of inconclusive studies. Existing methods proposed to use the total (one-sample) variance that is estimable from blinded data without knowledge of the treatment allocation. We demonstrate that, since the expectation of this estimator is greater than or equal to the true variance, the one-sample variance approach can be regarded as providing an upper bound of the variance in blind reviews. This worst-case evaluation can likely reduce a risk of underpowered studies. However, blinded reviews of small sample size may still lead to underpowered studies. We propose a refined method accounting for estimation error in blind reviews using an upper confidence limit of the variance. A similar idea had been proposed in the setting of external pilot studies. Furthermore, we developed a method to select an appropriate confidence level so that the re-estimated sample size attains the target power. Numerical studies showed that our method works well and outperforms existing methods. The proposed procedure is motivated and illustrated by recent randomized clinical trials."}
{"id": "2602.03317", "categories": ["stat.ML", "cs.AI", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.03317", "abs": "https://arxiv.org/abs/2602.03317", "authors": ["Alex Finkelstein", "Ron Moneta", "Or Zohar", "Michal Rivlin", "Moritz Zaiss", "Dinora Friedmann Morvinski", "Or Perlman"], "title": "Multiparameter Uncertainty Mapping in Quantitative Molecular MRI using a Physics-Structured Variational Autoencoder (PS-VAE)", "comment": "Submitted to IEEE Transactions on Medical Imaging. This project was funded by the European Union (ERC, BabyMagnet, project no. 101115639). Views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them", "summary": "Quantitative imaging methods, such as magnetic resonance fingerprinting (MRF), aim to extract interpretable pathology biomarkers by estimating biophysical tissue parameters from signal evolutions. However, the pattern-matching algorithms or neural networks used in such inverse problems often lack principled uncertainty quantification, which limits the trustworthiness and transparency, required for clinical acceptance. Here, we describe a physics-structured variational autoencoder (PS-VAE) designed for rapid extraction of voxelwise multi-parameter posterior distributions. Our approach integrates a differentiable spin physics simulator with self-supervised learning, and provides a full covariance that captures the inter-parameter correlations of the latent biophysical space. The method was validated in a multi-proton pool chemical exchange saturation transfer (CEST) and semisolid magnetization transfer (MT) molecular MRF study, across in-vitro phantoms, tumor-bearing mice, healthy human volunteers, and a subject with glioblastoma. The resulting multi-parametric posteriors are in good agreement with those calculated using a brute-force Bayesian analysis, while providing an orders-of-magnitude acceleration in whole brain quantification. In addition, we demonstrate how monitoring the multi-parameter posterior dynamics across progressively acquired signals provides practical insights for protocol optimization and may facilitate real-time adaptive acquisition."}
{"id": "2602.03483", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.03483", "abs": "https://arxiv.org/abs/2602.03483", "authors": ["Francisco Cuevas-Pacheco", "Jonathan Acosta"], "title": "Kriging for large datasets via penalized neighbor selection", "comment": "Submitted for Journal publication", "summary": "Kriging is a fundamental tool for spatial prediction, but its computational complexity of $O(N^3)$ becomes prohibitive for large datasets. While local kriging using $K$-nearest neighbors addresses this issue, the selection of $K$ typically relies on ad-hoc criteria that fail to account for spatial correlation structure. We propose a penalized kriging framework that incorporates LASSO-type penalties directly into the kriging equations to achieve automatic, data-driven neighbor selection. We further extend this to adaptive LASSO, using data-driven penalty weights that account for the spatial correlation structure. Our method determines which observations contribute non-zero weights through $\\ell_1$ regularization, with the penalty parameter selected via a novel criterion based on effective sample size that balances prediction accuracy against information redundancy. Numerical experiments demonstrate that penalized kriging automatically adapts neighborhood structure to the underlying spatial correlation, selecting fewer neighbors for smoother processes and more for highly variable fields, while maintaining prediction accuracy comparable to global kriging at substantially reduced computational cost."}
{"id": "2602.03394", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03394", "abs": "https://arxiv.org/abs/2602.03394", "authors": ["Pedro Jiménez", "Luis A. Ortega", "Pablo Morales-Álvarez", "Daniel Hernández-Lobato"], "title": "Improving the Linearized Laplace Approximation via Quadratic Approximations", "comment": "6 pages, 1 table. Accepted at European Symposium on Artificial Neural Networks (ESANN 2026) as poster presentation", "summary": "Deep neural networks (DNNs) often produce overconfident out-of-distribution predictions, motivating Bayesian uncertainty quantification. The Linearized Laplace Approximation (LLA) achieves this by linearizing the DNN and applying Laplace inference to the resulting model. Importantly, the linear model is also used for prediction. We argue this linearization in the posterior may degrade fidelity to the true Laplace approximation. To alleviate this problem, without increasing significantly the computational cost, we propose the Quadratic Laplace Approximation (QLA). QLA approximates each second order factor in the approximate Laplace log-posterior using a rank-one factor obtained via efficient power iterations. QLA is expected to yield a posterior precision closer to that of the full Laplace without forming the full Hessian, which is typically intractable. For prediction, QLA also uses the linearized model. Empirically, QLA yields modest yet consistent uncertainty estimation improvements over LLA on five regression datasets."}
{"id": "2602.03613", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03613", "abs": "https://arxiv.org/abs/2602.03613", "authors": ["Arya Farahi", "Jonah Rose", "Paul Torrey"], "title": "Simulation-Based Inference via Regression Projection and Batched Discrepancies", "comment": "comments are welcome,", "summary": "We analyze a lightweight simulation-based inference method that infers simulator parameters using only a regression-based projection of the observed data. After fitting a surrogate linear regression once, the procedure simulates small batches at the proposed parameter values and assigns kernel weights based on the resulting batch-residual discrepancy, producing a self-normalized pseudo-posterior that is simple, parallelizable, and requires access only to the fitted regression coefficients rather than raw observations. We formalize the construction as an importance-sampling approximation to a population target that averages over simulator randomness, prove consistency as the number of parameter draws grows, and establish stability in estimating the surrogate regression from finite samples. We then characterize the asymptotic concentration as the batch size increases and the bandwidth shrinks, showing that the pseudo-posterior concentrates on an identified set determined by the chosen projection, thereby clarifying when the method yields point versus set identification. Experiments on a tractable nonlinear model and on a cosmological calibration task using the DREAMS simulation suite illustrate the computational advantages of regression-based projections and the identifiability limitations arising from low-information summaries."}
{"id": "2602.03449", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.03449", "abs": "https://arxiv.org/abs/2602.03449", "authors": ["Fabian Schneider", "Meghdoot Mozumder", "Konstantin Tamarov", "Leila Taghizadeh", "Tanja Tarvainen", "Tapio Helin", "Duc-Lam Duong"], "title": "Score-based diffusion models for diffuse optical tomography with uncertainty quantification", "comment": null, "summary": "Score-based diffusion models are a recently developed framework for posterior sampling in Bayesian inverse problems with a state-of-the-art performance for severely ill-posed problems by leveraging a powerful prior distribution learned from empirical data. Despite generating significant interest especially in the machine-learning community, a thorough study of realistic inverse problems in the presence of modelling error and utilization of physical measurement data is still outstanding. In this work, the framework of unconditional representation for the conditional score function (UCoS) is evaluated for linearized difference imaging in diffuse optical tomography (DOT). DOT uses boundary measurements of near-infrared light to estimate the spatial distribution of absorption and scattering parameters in biological tissues. The problem is highly ill-posed and thus sensitive to noise and modelling errors. We introduce a novel regularization approach that prevents overfitting of the score function by constructing a mixed score composed of a learned and a model-based component. Validation of this approach is done using both simulated and experimental measurement data. The experiments demonstrate that a data-driven prior distribution results in posterior samples with low variance, compared to classical model-based estimation, and centred around the ground truth, even in the context of a highly ill-posed problem and in the presence of modelling errors."}
{"id": "2602.03756", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.03756", "abs": "https://arxiv.org/abs/2602.03756", "authors": ["Yulong Chen", "Jim Griffin", "Francisco Javier Rubio"], "title": "Bayesian variable and hazard structure selection in the General Hazard model", "comment": null, "summary": "The proportional hazards (PH) and accelerated failure time (AFT) models are the most widely used hazard structures for analysing time-to-event data. When the goal is to identify variables associated with event times, variable selection is typically performed within a single hazard structure, imposing strong assumptions on how covariates affect the hazard function. To allow simultaneous selection of relevant variables and the hazard structure itself, we develop a Bayesian variable selection approach within the general hazard (GH) model, which includes the PH, AFT, and other structures as special cases. We propose two types of g-priors for the regression coefficients that enable tractable computation and show that both lead to consistent model selection. We also introduce a hierarchical prior on the model space that accounts for multiplicity and penalises model complexity. To efficiently explore the GH model space, we extend the Add-Delete-Swap algorithm to jointly sample variable inclusion indicators and hazard structures. Simulation studies show accurate recovery of both the true hazard structure and active variables across different sample sizes and censoring levels. Two real-data applications are presented to illustrate the use of the proposed methodology and to compare it with existing variable selection methods."}
{"id": "2602.03612", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03612", "abs": "https://arxiv.org/abs/2602.03612", "authors": ["Anthony Stephenson", "Ian Gallagher", "Christopher Nemeth"], "title": "Generator-based Graph Generation via Heat Diffusion", "comment": "Submitted to ICML; 8+15 pages; 20 figures", "summary": "Graph generative modelling has become an essential task due to the wide range of applications in chemistry, biology, social networks, and knowledge representation. In this work, we propose a novel framework for generating graphs by adapting the Generator Matching (arXiv:2410.20587) paradigm to graph-structured data. We leverage the graph Laplacian and its associated heat kernel to define a continous-time diffusion on each graph. The Laplacian serves as the infinitesimal generator of this diffusion, and its heat kernel provides a family of conditional perturbations of the initial graph. A neural network is trained to match this generator by minimising a Bregman divergence between the true generator and a learnable surrogate. Once trained, the surrogate generator is used to simulate a time-reversed diffusion process to sample new graph structures. Our framework unifies and generalises existing diffusion-based graph generative models, injecting domain-specific inductive bias via the Laplacian, while retaining the flexibility of neural approximators. Experimental studies demonstrate that our approach captures structural properties of real and synthetic graphs effectively."}
{"id": "2602.02813", "categories": ["stat.AP", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02813", "abs": "https://arxiv.org/abs/2602.02813", "authors": ["Sanjit Dandapanthula", "Margaret Johnson", "Madeleine Pascolini-Campbell", "Glynn Hulley", "Mikael Kuusela"], "title": "Downscaling land surface temperature data using edge detection and block-diagonal Gaussian process regression", "comment": null, "summary": "Accurate and high-resolution estimation of land surface temperature (LST) is crucial in estimating evapotranspiration, a measure of plant water use and a central quantity in agricultural applications. In this work, we develop a novel statistical method for downscaling LST data obtained from NASA's ECOSTRESS mission, using high-resolution data from the Landsat 8 mission as a proxy for modeling agricultural field structure. Using the Landsat data, we identify the boundaries of agricultural fields through edge detection techniques, allowing us to capture the inherent block structure present in the spatial domain. We propose a block-diagonal Gaussian process (BDGP) model that captures the spatial structure of the agricultural fields, leverages independence of LST across fields for computational tractability, and accounts for the change of support present in ECOSTRESS observations. We use the resulting BDGP model to perform Gaussian process regression and obtain high-resolution estimates of LST from ECOSTRESS data, along with uncertainty quantification. Our results demonstrate the practicality of the proposed method in producing reliable high-resolution LST estimates, with potential applications in agriculture, urban planning, and climate studies."}
{"id": "2602.03682", "categories": ["stat.ML", "cs.DC", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.03682", "abs": "https://arxiv.org/abs/2602.03682", "authors": ["Pierre Aguié", "Mathieu Even", "Laurent Massoulié"], "title": "Improved Analysis of the Accelerated Noisy Power Method with Applications to Decentralized PCA", "comment": null, "summary": "We analyze the Accelerated Noisy Power Method, an algorithm for Principal Component Analysis in the setting where only inexact matrix-vector products are available, which can arise for instance in decentralized PCA. While previous works have established that acceleration can improve convergence rates compared to the standard Noisy Power Method, these guarantees require overly restrictive upper bounds on the magnitude of the perturbations, limiting their practical applicability. We provide an improved analysis of this algorithm, which preserves the accelerated convergence rate under much milder conditions on the perturbations. We show that our new analysis is worst-case optimal, in the sense that the convergence rate cannot be improved, and that the noise conditions we derive cannot be relaxed without sacrificing convergence guarantees. We demonstrate the practical relevance of our results by deriving an accelerated algorithm for decentralized PCA, which has similar communication costs to non-accelerated methods. To our knowledge, this is the first decentralized algorithm for PCA with provably accelerated convergence."}
{"id": "2602.02825", "categories": ["stat.AP", "q-bio.QM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.02825", "abs": "https://arxiv.org/abs/2602.02825", "authors": ["Jiayu Su", "Jun Hou Fung", "Haoyu Wang", "Dian Yang", "David A. Knowles", "Raul Rabadan"], "title": "On the consistent and scalable detection of spatial patterns", "comment": null, "summary": "Detecting spatial patterns is fundamental to scientific discovery, yet current methods lack statistical consensus and face computational barriers when applied to large-scale spatial omics datasets. We unify major approaches through a single quadratic form and derive general consistency conditions. We reveal that several widely used methods, including Moran's I, are inconsistent, and propose scalable corrections. The resulting test enables robust pattern detection across millions of spatial locations and single-cell lineage-tracing datasets."}
{"id": "2602.03730", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03730", "abs": "https://arxiv.org/abs/2602.03730", "authors": ["Luke Solo", "Matthew B. A. McDermott", "William F. Parker", "Bashar Ramadan", "Michael C. Burkhart", "Brett K. Beaulieu-Jones"], "title": "Efficient Variance-reduced Estimation from Generative EHR Models: The SCOPE and REACH Estimators", "comment": "10 pages, 2 figures", "summary": "Generative models trained using self-supervision of tokenized electronic health record (EHR) timelines show promise for clinical outcome prediction. This is typically done using Monte Carlo simulation for future patient trajectories. However, existing approaches suffer from three key limitations: sparse estimate distributions that poorly differentiate patient risk levels, extreme computational costs, and high sampling variance. We propose two new estimators: the Sum of Conditional Outcome Probability Estimator (SCOPE) and Risk Estimation from Anticipated Conditional Hazards (REACH), that leverage next-token probability distributions discarded by standard Monte Carlo. We prove both estimators are unbiased and that REACH guarantees variance reduction over Monte Carlo sampling for any model and outcome. Empirically, on hospital mortality prediction in MIMIC-IV using the ETHOS-ARES framework, SCOPE and REACH match 100-sample Monte Carlo performance using only 10-11 samples (95% CI: [9,11]), representing a ~10x reduction in inference cost without degrading calibration. For ICU admission prediction, efficiency gains are more modest (~1.2x), which we attribute to the outcome's lower \"spontaneity,\" a property we characterize theoretically and empirically. These methods substantially improve the feasibility of deploying generative EHR models in resource-constrained clinical settings."}
{"id": "2602.03049", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.03049", "abs": "https://arxiv.org/abs/2602.03049", "authors": ["Zhixian Zhang", "Xiaotian Hou", "Linjun Zhang"], "title": "Unified Inference Framework for Single and Multi-Player Performative Prediction: Method and Asymptotic Optimality", "comment": null, "summary": "Performative prediction characterizes environments where predictive models alter the very data distributions they aim to forecast, triggering complex feedback loops. While prior research treats single-agent and multi-agent performativity as distinct phenomena, this paper introduces a unified statistical inference framework that bridges these contexts, treating the former as a special case of the latter. Our contribution is two-fold. First, we put forward the Repeated Risk Minimization (RRM) procedure for estimating the performative stability, and establish a rigorous inferential theory for admitting its asymptotic normality and confirming its asymptotic efficiency. Second, for the performative optimality, we introduce a novel two-step plug-in estimator that integrates the idea of Recalibrated Prediction Powered Inference (RePPI) with Importance Sampling, and further provide formal derivations for the Central Limit Theorems of both the underlying distributional parameters and the plug-in results. The theoretical analysis demonstrates that our estimator achieves the semiparametric efficiency bound and maintains robustness under mild distributional misspecification. This work provides a principled toolkit for reliable estimation and decision-making in dynamic, performative environments."}
{"id": "2602.03789", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03789", "abs": "https://arxiv.org/abs/2602.03789", "authors": ["Gabriel Damsholt", "Jes Frellsen", "Susanne Ditlevsen"], "title": "Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants", "comment": null, "summary": "Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A primary hyperparameter in these methods is the interpolation schedule that determines how to bridge a standard Gaussian base measure to an arbitrary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule into the unique sample path under another arbitrary schedule and diffusion coefficient. We then extend the stochastic interpolant framework to admit a larger class of point mass schedules in which the Gaussian base measure collapses to a point mass measure. Under the assumption of Gaussian data, we identify lazy schedule families that make the drift identically zero and show that with deterministic sampling one gets a variance-preserving schedule commonly used in diffusion models, whereas with statistically optimal SDE sampling one gets our point mass schedule. Finally, to demonstrate the usefulness of our theoretical results on realistic highly non-Gaussian data, we apply our lazy schedule conversion to a state-of-the-art pretrained flow model and show that this allows for generating images in fewer steps without retraining the model."}
{"id": "2602.03823", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03823", "abs": "https://arxiv.org/abs/2602.03823", "authors": ["Dovid Parnas", "Mathieu Even", "Julie Josse", "Uri Shalit"], "title": "Preference-based Conditional Treatment Effects and Policy Learning", "comment": "Accepted to AISTATS 2026; 10 pages + appendix", "summary": "We introduce a new preference-based framework for conditional treatment effect estimation and policy learning, built on the Conditional Preference-based Treatment Effect (CPTE). CPTE requires only that outcomes be ranked under a preference rule, unlocking flexible modeling of heterogeneous effects with multivariate, ordinal, or preference-driven outcomes. This unifies applications such as conditional probability of necessity and sufficiency, conditional Win Ratio, and Generalized Pairwise Comparisons. Despite the intrinsic non-identifiability of comparison-based estimands, CPTE provides interpretable targets and delivers new identifiability conditions for previous unidentifiable estimands. We present estimation strategies via matching, quantile, and distributional regression, and further design efficient influence-function estimators to correct plug-in bias and maximize policy value. Synthetic and semi-synthetic experiments demonstrate clear performance gains and practical impact."}
{"id": "2602.03165", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03165", "abs": "https://arxiv.org/abs/2602.03165", "authors": ["Anas Cherradi", "Yazid Janati", "Alain Durmus", "Sylvain Le Corff", "Yohan Petetin", "Julien Stoehr"], "title": "Entropic Mirror Monte Carlo", "comment": null, "summary": "Importance sampling is a Monte Carlo method which designs estimators of expectations under a target distribution using weighted samples from a proposal distribution. When the target distribution is complex, such as multimodal distributions in highdimensional spaces, the efficiency of importance sampling critically depends on the choice of the proposal distribution. In this paper, we propose a novel adaptive scheme for the construction of efficient proposal distributions. Our algorithm promotes efficient exploration of the target distribution by combining global sampling mechanisms with a delayed weighting procedure. The proposed weighting mechanism plays a key role by enabling rapid resampling in regions where the proposal distribution is poorly adapted to the target. Our sampling algorithm is shown to be geometrically convergent under mild assumptions and is illustrated through various numerical experiments."}
{"id": "2602.03613", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03613", "abs": "https://arxiv.org/abs/2602.03613", "authors": ["Arya Farahi", "Jonah Rose", "Paul Torrey"], "title": "Simulation-Based Inference via Regression Projection and Batched Discrepancies", "comment": "comments are welcome,", "summary": "We analyze a lightweight simulation-based inference method that infers simulator parameters using only a regression-based projection of the observed data. After fitting a surrogate linear regression once, the procedure simulates small batches at the proposed parameter values and assigns kernel weights based on the resulting batch-residual discrepancy, producing a self-normalized pseudo-posterior that is simple, parallelizable, and requires access only to the fitted regression coefficients rather than raw observations. We formalize the construction as an importance-sampling approximation to a population target that averages over simulator randomness, prove consistency as the number of parameter draws grows, and establish stability in estimating the surrogate regression from finite samples. We then characterize the asymptotic concentration as the batch size increases and the bandwidth shrinks, showing that the pseudo-posterior concentrates on an identified set determined by the chosen projection, thereby clarifying when the method yields point versus set identification. Experiments on a tractable nonlinear model and on a cosmological calibration task using the DREAMS simulation suite illustrate the computational advantages of regression-based projections and the identifiability limitations arising from low-information summaries."}
