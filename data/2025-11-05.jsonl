{"id": "2511.00006", "categories": ["stat.ME", "math.PR", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.00006", "abs": "https://arxiv.org/abs/2511.00006", "authors": ["Xingyu Ren", "Michael C. Fu", "Pierre L'Ecuyer"], "title": "Stochastic Derivative Estimation for Discontinuous Sample Performances: A Leibniz Integration Perspective", "comment": null, "summary": "We develop a novel stochastic derivative estimation framework for sample\nperformance functions that are discontinuous in the parameter of interest,\nbased on the multidimensional Leibniz integral rule. When discontinuities arise\nfrom indicator functions, we embed the indicator functions into the sample\nspace, yielding a continuous performance function over a parameter-dependent\ndomain. Applying the Leibniz integral rule in this case produces a single-run,\nunbiased derivative estimator. For general discontinuous functions, we apply a\nchange of variables to shift parameter dependence into the sample space and the\nunderlying probability measure. Applying the Leibniz integral rule leads to two\nterms: a standard likelihood ratio (LR) term from differentiating the\nunderlying probability measure and a surface integral from differentiating the\nboundary of the domain. Evaluating the surface integral may require simulating\nmultiple sample paths. Our proposed Leibniz integration framework generalizes\nthe generalized LR (GLR) method and provides intuition as to when the surface\nintegral vanishes, thereby enabling single-run, easily implementable\nestimators. Numerical experiments demonstrate the effectiveness and robustness\nof our methods."}
{"id": "2511.00395", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.00395", "abs": "https://arxiv.org/abs/2511.00395", "authors": ["Chuanji Gao", "Gang Chen", "Svetlana V. Shinkareva", "Rutvik H. Desai"], "title": "Is Representational Similarity Analysis Reliable? A Comparison with Regression", "comment": null, "summary": "Representational Similarity Analysis (RSA) is a popular method for analyzing\nneuroimaging and behavioral data. Here we evaluate the accuracy and reliability\nof RSA in the context of model selection, and compare it to that of regression.\nAlthough RSA offers flexibility in handling high-dimensional, cross-modal, and\ncross-species data, its reliance on a transformation of raw data into\nsimilarity structures may result in the loss of critical stimulus-response\ninformation. Across extensive simulation studies and empirical analyses, we\nshow that RSA leads to lower model selection accuracy, regardless of sample\nsize, noise level, feature dimensionality, or multicollinearity, relative to\nregression. While principal component analysis and feature reweighting mitigate\nRSA's deficits driven by multicollinearity, regression remains superior in\naccurately distinguishing between models. Empirical data and a follow-up fMRI\nsimulation further support these conclusions. Our findings suggest that\nresearchers should carefully consider which approach to use: RSA is less\neffective than linear regression for model selection and fitting when direct\nstimulus-response mappings are available."}
{"id": "2511.00455", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.00455", "abs": "https://arxiv.org/abs/2511.00455", "authors": ["Andrea Cremaschi", "Maria De Iorio", "Garritt Page", "Ajay Jasra"], "title": "Latent Modularity in Multi-View Data", "comment": null, "summary": "In this article, we consider the problem of clustering multi-view data, that\nis, information associated to individuals that form heterogeneous data sources\n(the views). We adopt a Bayesian model and in the prior structure we assume\nthat each individual belongs to a baseline cluster and conditionally allow each\nindividual in each view to potentially belong to different clusters than the\nbaseline. We call such a structure ''latent modularity''. Then for each\ncluster, in each view we have a specific statistical model with an associated\nprior. We derive expressions for the marginal priors on the view-specific\ncluster labels and the associated partitions, giving several insights into our\nchosen prior structure. Using simple Markov chain Monte Carlo algorithms, we\nconsider our model in a simulation study, along with a more detailed case study\nthat requires several modeling innovations."}
{"id": "2511.00501", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.00501", "abs": "https://arxiv.org/abs/2511.00501", "authors": ["Nihan Acar-Denizli", "Pedro Delicado"], "title": "Modeling continuous monitoring glucose curves by Beta generalized non-parametric models", "comment": null, "summary": "We present a functional data analysis approach for studying time-dependent,\ncontinuous glucose monitoring data with repeated measures for each individual\nin an experiment. After scaling the glucose concentration curves to the\ninterval [0, 1], we model them by using a Beta distribution with two\ntime-varying parameters. In this context, we develop a local linear maximum\nlikelihood smoothing procedure that is valid when more than one parameter\ndepends on time. Our approach requires much fewer observations than previous\nfunctional methods for this setting and is also applicable when only one\nindividual (or a few) is available. We evaluate the performance of our\nestimator in terms of computation time and model fit using a synthetic dataset\nas well as a large, real clinical trial dataset. We also compare our method\nwith existing methods in the literature. From a methodological point of view,\nwe contribute to extend local likelihood estimation from one to two\ntime-varying parameters by developing theoretical expressions for estimation\nand for approximating the leave-one-out cross-validation. Moreover, we show\nthat this kernel-based approach competes with spline-based estimation methods,\nthe dominant line of functional regression models today."}
{"id": "2511.00217", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.00217", "abs": "https://arxiv.org/abs/2511.00217", "authors": ["Mitchell L. Prevett", "Francis K. C. Hui", "Zhi Yang Tho", "A. H. Welsh", "Anton H. Westveld"], "title": "Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data", "comment": null, "summary": "Linear mixed models are widely used for clustered data, but their reliance on\nparametric forms limits flexibility in complex and high-dimensional settings.\nIn contrast, gradient boosting methods achieve high predictive accuracy through\nnonparametric estimation, but do not accommodate clustered data structures or\nprovide uncertainty quantification.\n  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and\nalgorithm that extends boosting to jointly estimate mean and variance\ncomponents via likelihood-based gradients. In addition to nonparametric mean\nestimation, the method models both random effects and residual variances as\npotentially covariate-dependent functions using flexible base learners such as\nregression trees or splines, enabling nonparametric estimation while\nmaintaining interpretability.\n  Simulations and real-world applications demonstrate accurate recovery of\nvariance components, calibrated prediction intervals, and improved predictive\naccuracy relative to standard linear mixed models and nonparametric methods.\nGBMixed provides heteroscedastic uncertainty quantification and introduces\nboosting for heterogeneous random effects. This enables covariate-dependent\nshrinkage for cluster-specific predictions to adapt between population and\ncluster-level data. Under standard causal assumptions, the framework enables\nestimation of heterogeneous treatment effects with reliable uncertainty\nquantification."}
{"id": "2511.00132", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.00132", "abs": "https://arxiv.org/abs/2511.00132", "authors": ["Felipe E. Sanchez", "Thomas A. Lake", "Jason A. Galvis", "Chris Jones", "Gustavo Machado"], "title": "Predicting the spatial distribution and demographics of commercial swine farms in the United States", "comment": null, "summary": "Data on livestock farm locations and demographics are essential for disease\nmonitoring, risk assessment, and developing spatially explicit epidemiological\nmodels. Our semantic segmentation model achieved an F2 score of 92 % and a mean\nIntersection over Union of 76 %. An initial total of 194,474 swine barn\ncandidates were identified in the Southeast (North Carolina = 111,135, South\nCarolina = 37,264 Virginia = 46,075) and 524,962 in the Midwest (Iowa = 168,866\nMinnesota = 165,714 Ohio = 190,382). The post processing Random Forest\nclassifier reduced false positives by 82 % in the Southeast and 88 % in the\nMidwest, resulting in 45,580 confirmed barn polygons. These were grouped into\n16,976 predicted farms and classified into one of the four production types.\nPopulation sizes were then estimated using the Random Forest regression model,\nwith prediction accuracy varying by production type. Across all farms, 87 % of\npredictions for operations with 1,000 2,000 pigs were within 500 pigs of the\nreference value, with nursery farms showing the highest agreement (R2= 0.82),\nfollowed by finisher farms (R2 = 0.77) and sow farms (R2 = 0.56). Our results\nrevealed substantial gaps in the existing spatial and demographic data on U.S.\nswine production."}
{"id": "2511.00820", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.00820", "abs": "https://arxiv.org/abs/2511.00820", "authors": ["Isaac Gibbs", "John J. Cherian", "Emmanuel J. Candès"], "title": "Correcting the Coverage Bias of Quantile Regression", "comment": null, "summary": "We develop a collection of methods for adjusting the predictions of quantile\nregression to ensure coverage. Our methods are model agnostic and can be used\nto correct for high-dimensional overfitting bias with only minimal assumptions.\nTheoretical results show that the estimates we develop are consistent and\nfacilitate accurate calibration in the proportional asymptotic regime where the\nratio of the dimension of the data and the sample size converges to a constant.\nThis is further confirmed by experiments on both simulated and real data. A key\ncomponent of our work is a new connection between the leave-one-out coverage\nand the fitted values of variables appearing in a dual formulation of the\nquantile regression problem. This facilitates the use of cross-validation in a\nvariety of settings at significantly reduced computational costs."}
{"id": "2511.00708", "categories": ["stat.CO", "math.PR", "stat.ML", "60J20, 65C05, 65C40, 68Q25"], "pdf": "https://arxiv.org/pdf/2511.00708", "abs": "https://arxiv.org/abs/2511.00708", "authors": ["Quan Zhou"], "title": "Polynomial Mixing Times of Simulated Tempering for Mixture Targets by Conductance Decomposition", "comment": "37 pages", "summary": "We study the theoretical complexity of simulated tempering for sampling from\nmixtures of log-concave components differing only by location shifts. The main\nresult establishes the first polynomial-time guarantee for simulated tempering\ncombined with the Metropolis-adjusted Langevin algorithm (MALA) with respect to\nthe problem dimension $d$, maximum mode displacement $D$, and logarithmic\naccuracy $\\log \\epsilon^{-1}$. The proof builds on a general state\ndecomposition theorem for $s$-conductance, applied to an auxiliary Markov chain\nconstructed on an augmented space. We also obtain an improved complexity\nestimate for simulated tempering combined with random-walk Metropolis. Our\nbounds assume an inverse-temperature ladder with smallest value $\\beta_1 =\nO(D^{-2})$ and spacing $\\beta_{i+1}/\\beta_i = 1 + O( d^{-1/2} )$, both of which\nare shown to be asymptotically optimal up to logarithmic factors."}
{"id": "2511.00982", "categories": ["stat.OT", "62G35"], "pdf": "https://arxiv.org/pdf/2511.00982", "abs": "https://arxiv.org/abs/2511.00982", "authors": ["Thomas F. Heston"], "title": "The Neutrality Boundary Framework: Quantifying Statistical Robustness Geometrically", "comment": "8 pages, no figures", "summary": "We introduce the Neutrality Boundary Framework (NBF), a set of geometric\nmetrics for quantifying statistical robustness and fragility as the normalized\ndistance from the neutrality boundary, the manifold where the effect equals\nzero. The neutrality boundary value nb in [0,1) provides a threshold-free,\nsample-size invariant measure of stability that complements traditional effect\nsizes and p-values. We derive the general form nb = |Delta - Delta_0| / (|Delta\n- Delta_0| + S), where S>0 is a scale parameter for normalization; we prove\nboundedness and monotonicity, and provide domain-specific implementations: Risk\nQuotient (binary outcomes), partial eta^2 (ANOVA), and Fisher z-based measures\n(correlation). Unlike threshold-dependent fragility indices, NBF quantifies\nrobustness geometrically across arbitrary significance levels and statistical\ncontexts."}
{"id": "2511.00676", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.00676", "abs": "https://arxiv.org/abs/2511.00676", "authors": ["Easton Huch", "Fred Feinberg", "Walter Dempsey"], "title": "Robust Bayesian Inference of Causal Effects via Randomization Distributions", "comment": null, "summary": "We present a general framework for Bayesian inference of causal effects that\ndelivers provably robust inferences founded on design-based randomization of\ntreatments. The framework involves fixing the observed potential outcomes and\nforming a likelihood based on the randomization distribution of a statistic.\nThe method requires specification of a treatment effect model; in many cases,\nhowever, it does not require specification of marginal outcome distributions,\nresulting in weaker assumptions compared to Bayesian superpopulation-based\nmethods. We show that the framework is compatible with posterior model checking\nin the form of posterior-averaged randomization tests. We prove several\ntheoretical properties for the method, including a Bernstein-von Mises theorem\nand large-sample properties of posterior expectations. In particular, we show\nthat the posterior mean is asymptotically equivalent to Hodges-Lehmann\nestimators, which provides a bridge to many classical estimators in causal\ninference, including inverse-probability-weighted estimators and H\\'ajek\nestimators. We evaluate the theory and utility of the framework in simulation\nand a case study involving a nutrition experiment. In the latter, our framework\nuncovers strong evidence of effect heterogeneity despite a lack of evidence for\nmoderation effects. The basic framework allows numerous extensions, including\nthe use of covariates, sensitivity analysis, estimation of assignment\nmechanisms, and generalization to nonbinary treatments."}
{"id": "2511.00366", "categories": ["stat.ML", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00366", "abs": "https://arxiv.org/abs/2511.00366", "authors": ["Krishna Prasath Logakannan", "Shridhar Vashishtha", "Jacob Hochhalter", "Shandian Zhe", "Robert M. Kirby"], "title": "A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications", "comment": null, "summary": "Digital twins are developed to model the behavior of a specific physical\nasset (or twin), and they can consist of high-fidelity physics-based models or\nsurrogates. A highly accurate surrogate is often preferred over multi-physics\nmodels as they enable forecasting the physical twin future state in real-time.\nTo adapt to a specific physical twin, the digital twin model must be updated\nusing in-service data from that physical twin. Here, we extend Gaussian process\n(GP) models to include derivative data, for improved accuracy, with dynamic\nupdating to ingest physical twin data during service. Including derivative\ndata, however, comes at a prohibitive cost of increased covariance matrix\ndimension. We circumvent this issue by using a sparse GP approximation, for\nwhich we develop extensions to incorporate derivatives. Numerical experiments\ndemonstrate that the prediction accuracy of the derivative-enhanced sparse GP\nmethod produces improved models upon dynamic data additions. Lastly, we apply\nthe developed algorithm within a DT framework to model fatigue crack growth in\nan aerospace vehicle."}
{"id": "2511.00422", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.00422", "abs": "https://arxiv.org/abs/2511.00422", "authors": ["Isabella Habereder", "Thomas Kneib", "Isao Echizen", "Timo Spinde"], "title": "A Systematic Review of Spatio-Temporal Statistical Models: Theory, Structure, and Applications", "comment": null, "summary": "Data with spatial-temporal attributes are prevalent across many research\nfields, and statistical models for analyzing spatio-temporal relationships are\nwidely used. Existing reviews focus either on specific domains or model types,\ncreating a gap in comprehensive, cross-disciplinary overviews. To address this,\nwe conducted a systematic literature review following the PRISMA guidelines,\nsearched two databases for the years 2021-2025, and identified 83 publications\nthat met our criteria. We propose a classification scheme for spatio-temporal\nmodel structures and highlight their application in the most common fields:\nepidemiology, ecology, public health, economics, and criminology. Although\ntasks vary by domain, many models share similarities. We found that\nhierarchical models are the most frequently used, and most models incorporate\nadditive components to account for spatial-temporal dependencies. The preferred\nmodel structures differ among fields of application. We also observe that\nresearch efforts are concentrated in only a few specific disciplines, despite\nthe broader relevance of spatio-temporal data. Furthermore, we notice that\nreproducibility remains limited. Our review, therefore, not only offers\ninspiration for comparing model structures in an interdisciplinary manner but\nalso highlights opportunities for greater transparency, accessibility, and\ncross-domain knowledge transfer."}
{"id": "2511.01151", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.01151", "abs": "https://arxiv.org/abs/2511.01151", "authors": ["Unnati Nigam", "Radhendushka Srivastava", "Faezeh Marzbanrad", "Michael Burke"], "title": "A structural equation formulation for general quasi-periodic Gaussian processes", "comment": null, "summary": "This paper introduces a structural equation formulation that gives rise to a\nnew family of quasi-periodic Gaussian processes, useful to process a broad\nclass of natural and physiological signals. The proposed formulation simplifies\ngeneration and forecasting, and provides hyperparameter estimates, which we\nexploit in a convergent and consistent iterative estimation algorithm. A\nbootstrap approach for standard error estimation and confidence intervals is\nalso provided. We demonstrate the computational and scaling benefits of the\nproposed approach on a broad class of problems, including water level tidal\nanalysis, CO$_{2}$ emission data, and sunspot numbers data. By leveraging the\nstructural equations, our method reduces the cost of likelihood evaluations and\npredictions from $\\mathcal{O}(k^2 p^2)$ to $\\mathcal{O}(p^2)$, significantly\nimproving scalability."}
{"id": "2511.01281", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.01281", "abs": "https://arxiv.org/abs/2511.01281", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Particle Filter Made Simple: A Step-by-Step Beginner-friendly Guide", "comment": "26 pages", "summary": "The particle filter is a powerful framework for estimating hidden states in\ndynamic systems where uncertainty, noise, and nonlinearity dominate. This\nmini-book offers a clear and structured introduction to the core ideas behind\nparticle filters-how they represent uncertainty through random samples, update\nbeliefs using observations, and maintain robustness where linear or Gaussian\nassumptions fail. Starting from the limitations of the Kalman filter, the book\ndevelops the intuition that drives the particle filter: belief as a cloud of\nweighted hypotheses that evolve through prediction, measurement, and\nresampling. Step by step, it connects these ideas to their mathematical\nfoundations, showing how probability distributions can be approximated by a\nfinite set of particles and how Bayesian reasoning unfolds in sampled form.\nIllustrated examples, numerical walk-throughs, and Python code bring each\nconcept to life, bridging the gap between theory and implementation. By the\nend, readers will not only understand the algorithmic flow of the particle\nfilter but also develop an intuitive grasp of how randomness and structure\ntogether enable systems to infer, adapt, and make sense of noisy observations\nin real time."}
{"id": "2511.01040", "categories": ["stat.OT", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.01040", "abs": "https://arxiv.org/abs/2511.01040", "authors": ["Junjie Ma", "Xiaoya Zhang", "Guangye He", "Yuting Han", "Ting Ge", "Feng Ji"], "title": "From Path Coefficients to Targeted Estimands: A Comparison of Structural Equation Models (SEM) and Targeted Maximum Likelihood Estimation (TMLE)", "comment": null, "summary": "Structural Equation Modeling (SEM) has gained popularity in the social\nsciences and causal inference due to its flexibility in modeling complex\nrelationships between variables and its availability in modern statistical\nsoftware. To move beyond the parametric assumptions of SEM, this paper reviews\ntargeted maximum likelihood estimation (TMLE), a doubly robust, machine\nlearning-based approach that builds on nonparametric SEM. We demonstrate that\nboth TMLE and SEM can be used to estimate standard causal effects and show that\nTMLE is robust to model misspecification. We conducted simulation studies under\nboth correct and misspecified model conditions, implementing SEM and TMLE to\nestimate these causal effects. The simulations confirm that TMLE consistently\noutperforms SEM under misspecification in terms of bias, mean squared error,\nand the validity of confidence intervals. We applied both approaches to a\nreal-world dataset to analyze the mediation effects of poverty on access to\nhigh school, revealing that the direct effect is no longer significant under\nTMLE, whereas SEM indicates significance. We conclude with practical guidance\non using SEM and TMLE in light of recent developments in targeted learning for\ncausal inference."}
{"id": "2511.00820", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.00820", "abs": "https://arxiv.org/abs/2511.00820", "authors": ["Isaac Gibbs", "John J. Cherian", "Emmanuel J. Candès"], "title": "Correcting the Coverage Bias of Quantile Regression", "comment": null, "summary": "We develop a collection of methods for adjusting the predictions of quantile\nregression to ensure coverage. Our methods are model agnostic and can be used\nto correct for high-dimensional overfitting bias with only minimal assumptions.\nTheoretical results show that the estimates we develop are consistent and\nfacilitate accurate calibration in the proportional asymptotic regime where the\nratio of the dimension of the data and the sample size converges to a constant.\nThis is further confirmed by experiments on both simulated and real data. A key\ncomponent of our work is a new connection between the leave-one-out coverage\nand the fitted values of variables appearing in a dual formulation of the\nquantile regression problem. This facilitates the use of cross-validation in a\nvariety of settings at significantly reduced computational costs."}
{"id": "2511.00490", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.00490", "abs": "https://arxiv.org/abs/2511.00490", "authors": ["Gero Junike", "Marco Oesting"], "title": "Accuracy estimation of neural networks by extreme value theory", "comment": null, "summary": "Neural networks are able to approximate any continuous function on a compact\nset. However, it is not obvious how to quantify the error of the neural\nnetwork, i.e., the remaining bias between the function and the neural network.\nHere, we propose the application of extreme value theory to quantify large\nvalues of the error, which are typically relevant in applications. The\ndistribution of the error beyond some threshold is approximately generalized\nPareto distributed. We provide a new estimator of the shape parameter of the\nPareto distribution suitable to describe the error of neural networks.\nNumerical experiments are provided."}
{"id": "2511.00867", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.00867", "abs": "https://arxiv.org/abs/2511.00867", "authors": ["Yassin Tesfaw Abebe", "Abdu Mohammed Seid", "Lassi Roininen", "Mohammed Seid Ali"], "title": "Spatiotemporal Dynamics of Conflict Occurrence and Fatalities in Ethiopia: A Bayesian Model and Predictive Insights Using Event-level Data (1997--2024)", "comment": null, "summary": "This study presents a spatiotemporal dual Bayesian model that examines both\nthe occurrence and number of conflict fatalities using event-level data from\nEthiopia (1997-2024), sourced from the Armed Conflict Location and Event Data\n(ACLED) project. Fatalities are treated as two linked outcomes: the binary\noccurrence of deaths and the count of deaths when they occur. The model\ncombines additive fixed effects for covariates with random effects capturing\nspatiotemporal influences, allowing for outcome-specific effects. Covariates\ninclude event type and season as categorical variables, proximity to cities and\nborders as nonlinear effects, and population as an offset term in the count\nmodel. A latent spatiotemporal process accounts for shared spatial and temporal\ndependence, with the spatial structure modeled using a Mat\\'ern field prior and\ninference via Integrated Nested Laplace Approximation (INLA). Results show\nstrong spatial clustering and temporal variation in fatality risk, emphasizing\nthe importance of modeling both dimensions for better understanding and\nprediction. Airstrikes, shelling, and attacks show the highest fatality\nlikelihood and counts, while communal and rebel actors cause the most deaths.\nMultiple fatalities are more likely in summer, and proximity to borders drives\nintense violence, whereas remoteness from urban centers is linked to\nlower-intensity events. These results provide insight for planning, policy, and\nresource allocation to protect vulnerable communities."}
{"id": "2511.00006", "categories": ["stat.ME", "math.PR", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.00006", "abs": "https://arxiv.org/abs/2511.00006", "authors": ["Xingyu Ren", "Michael C. Fu", "Pierre L'Ecuyer"], "title": "Stochastic Derivative Estimation for Discontinuous Sample Performances: A Leibniz Integration Perspective", "comment": null, "summary": "We develop a novel stochastic derivative estimation framework for sample\nperformance functions that are discontinuous in the parameter of interest,\nbased on the multidimensional Leibniz integral rule. When discontinuities arise\nfrom indicator functions, we embed the indicator functions into the sample\nspace, yielding a continuous performance function over a parameter-dependent\ndomain. Applying the Leibniz integral rule in this case produces a single-run,\nunbiased derivative estimator. For general discontinuous functions, we apply a\nchange of variables to shift parameter dependence into the sample space and the\nunderlying probability measure. Applying the Leibniz integral rule leads to two\nterms: a standard likelihood ratio (LR) term from differentiating the\nunderlying probability measure and a surface integral from differentiating the\nboundary of the domain. Evaluating the surface integral may require simulating\nmultiple sample paths. Our proposed Leibniz integration framework generalizes\nthe generalized LR (GLR) method and provides intuition as to when the surface\nintegral vanishes, thereby enabling single-run, easily implementable\nestimators. Numerical experiments demonstrate the effectiveness and robustness\nof our methods."}
{"id": "2511.01732", "categories": ["stat.AP", "q-bio.QM", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.01732", "abs": "https://arxiv.org/abs/2511.01732", "authors": ["Liangkang Wang", "Akhil Ambekar", "Ani Eloyan"], "title": "Geometric Modeling of Hippocampal Tau Deposition: A Surface-Based Framework for Covariate Analysis and Off-Target Contamination Detection", "comment": null, "summary": "We introduce a framework combining geometric modeling with disease\nprogression analysis to investigate tau deposition in Alzheimer's disease (AD)\nusing positron emission tomography (PET) data. Focusing on the hippocampus, we\nconstruct a principal surface that captures the spatial distribution and\nmorphological changes of tau pathology. By projecting voxels onto this surface,\nwe quantify tau coverage, intensity, and thickness through bidirectional\nprojection distances and interpolated standardized uptake value ratios (SUVR).\nThis low-dimensional embedding preserves spatial specificity while mitigating\nmultiple comparison issues. Covariate effects are analyzed using a two-stage\nregression model with inverse probability weighting to adjust for signal\nsparsity and selection bias. Using the SuStaIn model, we identify subtypes and\nstages of AD, revealing distinct tau dynamics: the limbic-predominant subtype\nshows age-related nonlinear accumulation in coverage and thickness, whereas the\nposterior subtype exhibits uniform SUVR increases across disease progression.\nModel-based predictions show that hippocampal tau deposition follows a\nstructured spatial trajectory expanding bidirectionally with increasing\nthickness, while subtype differences highlight posterior hippocampal\ninvolvement consistent with whole-brain patterns. Finally, directional signal\npatterns on the principal surface reveal contamination from the choroid plexus,\ndemonstrating the broader applicability of the proposed framework across\nmodalities including amyloid PET."}
{"id": "2511.00870", "categories": ["stat.ME", "cs.DC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00870", "abs": "https://arxiv.org/abs/2511.00870", "authors": ["Maxime Bouton", "Pierre-Antoine Thouvenin", "Audrey Repetti", "Pierre Chainais"], "title": "A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems", "comment": null, "summary": "Markov Chain Monte Carlo (MCMC) algorithms are standard approaches to solve\nimaging inverse problems and quantify estimation uncertainties, a key\nrequirement in absence of ground-truth data. To improve estimation quality,\nPlug-and-Play MCMC algorithms, such as PnP-ULA, have been recently developed to\naccommodate priors encoded by a denoising neural network. Designing scalable\nsamplers for high-dimensional imaging inverse problems remains a challenge:\ndrawing and storing high-dimensional samples can be prohibitive, especially for\nhigh-resolution images. To address this issue, this work proposes a distributed\nsampler based on approximate data augmentation and PnP-ULA to solve very large\nproblems. The proposed sampler uses lightweight denoising convolutional neural\nnetwork, to efficiently exploit multiple GPUs on a Single Program Multiple Data\narchitecture. Reconstruction performance and scalability are evaluated on\nseveral imaging problems. Communication and computation overheads due to the\ndenoiser are carefully discussed. The proposed distributed approach noticeably\ncombines three very precious qualities: it is scalable, enables uncertainty\nquantification, for a reconstruction performance comparable to other PnP\nmethods."}
{"id": "2511.00685", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00685", "abs": "https://arxiv.org/abs/2511.00685", "authors": ["Haoting Zhang", "Haoxian Chen", "Donglin Zhan", "Hanyang Zhao", "Henry Lam", "Wenpin Tang", "David Yao", "Zeyu Zheng"], "title": "SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations", "comment": null, "summary": "The field of simulation optimization (SO) encompasses various methods\ndeveloped to optimize complex, expensive-to-sample stochastic systems.\nEstablished methods include, but are not limited to, ranking-and-selection for\nfinite alternatives and surrogate-based methods for continuous domains, with\nbroad applications in engineering and operations management. The recent advent\nof large language models (LLMs) offers a new paradigm for exploiting system\nstructure and automating the strategic selection and composition of these\nestablished SO methods into a tailored optimization procedure. This work\nintroduces SOCRATES (Simulation Optimization with Correlated Replicas and\nAdaptive Trajectory Evaluations), a novel two-stage procedure that leverages\nLLMs to automate the design of tailored SO algorithms. The first stage\nconstructs an ensemble of digital replicas of the real system. An LLM is\nemployed to implement causal discovery from a textual description of the\nsystem, generating a structural `skeleton' that guides the sample-efficient\nlearning of the replicas. In the second stage, this replica ensemble is used as\nan inexpensive testbed to evaluate a set of baseline SO algorithms. An LLM then\nacts as a meta-optimizer, analyzing the performance trajectories of these\nalgorithms to iteratively revise and compose a final, hybrid optimization\nschedule. This schedule is designed to be adaptive, with the ability to be\nupdated during the final execution on the real system when the optimization\nperformance deviates from expectations. By integrating LLM-driven reasoning\nwith LLM-assisted trajectory-aware meta-optimization, SOCRATES creates an\neffective and sample-efficient solution for complex SO optimization problems."}
{"id": "2511.01607", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.01607", "abs": "https://arxiv.org/abs/2511.01607", "authors": ["Rolando Gonzales Martinez", "Hinke Haisma"], "title": "The Multidimensional Index of Child Growth (MICG) of the Task Force \"Towards a Multidimensional Approach for Child Growth\" of the International Union for Nutrition Sciences", "comment": null, "summary": "Children's growth extends beyond height and weight. This paper introduces the\nMultidimensional Index of Child Growth (MICG), developed by the IUNS Task Force\n\"Towards a Multidimensional Approach for Child Growth.\" The IUNS-MICG applies a\ncapability- and rights-based framework covering 14 dimensions of child\nwellbeing, including health, care, mental wellbeing, participation, autonomy,\nmobility, and safety. Using data from the Young Lives Study in Ethiopia, India,\nPeru, and Vietnam, we tested the framework with 29 indicators. Comparisons of\ndifferent weighting methods show that equal weights provide robust and\npolicy-relevant results. MICG uncovers deprivations hidden by physical measures\nalone; for instance, rural girls in Peru face educational and mental wellbeing\ndisadvantages despite similar physical growth. Further analyses show that\ncommunity participation in WASH programs is linked to higher multidimensional\noutcomes, especially for the most deprived. We also extend MICG with a Bayesian\napproach to estimate children's unrealized opportunities and propose a\nspiderweb growth chart for visualizing multidimensional progress. MICG offers a\npractical, equity-focused tool to monitor, evaluate, and strengthen\ninterventions that support the Sustainable Development Goals and ensure no\nchild is left behind."}
{"id": "2511.00217", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.00217", "abs": "https://arxiv.org/abs/2511.00217", "authors": ["Mitchell L. Prevett", "Francis K. C. Hui", "Zhi Yang Tho", "A. H. Welsh", "Anton H. Westveld"], "title": "Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data", "comment": null, "summary": "Linear mixed models are widely used for clustered data, but their reliance on\nparametric forms limits flexibility in complex and high-dimensional settings.\nIn contrast, gradient boosting methods achieve high predictive accuracy through\nnonparametric estimation, but do not accommodate clustered data structures or\nprovide uncertainty quantification.\n  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and\nalgorithm that extends boosting to jointly estimate mean and variance\ncomponents via likelihood-based gradients. In addition to nonparametric mean\nestimation, the method models both random effects and residual variances as\npotentially covariate-dependent functions using flexible base learners such as\nregression trees or splines, enabling nonparametric estimation while\nmaintaining interpretability.\n  Simulations and real-world applications demonstrate accurate recovery of\nvariance components, calibrated prediction intervals, and improved predictive\naccuracy relative to standard linear mixed models and nonparametric methods.\nGBMixed provides heteroscedastic uncertainty quantification and introduces\nboosting for heterogeneous random effects. This enables covariate-dependent\nshrinkage for cluster-specific predictions to adapt between population and\ncluster-level data. Under standard causal assumptions, the framework enables\nestimation of heterogeneous treatment effects with reliable uncertainty\nquantification."}
{"id": "2511.00937", "categories": ["stat.ME", "62H30, 60D05"], "pdf": "https://arxiv.org/pdf/2511.00937", "abs": "https://arxiv.org/abs/2511.00937", "authors": ["Bogdan Radović", "Vesna Gotovac Đogaš", "Kateřina Helisová"], "title": "Classification of realisations of random sets", "comment": "76 pages, 57 figures", "summary": "In this paper, the classification task for a family of sets representing the\nrealisation of some random set models is solved. Both unsupervised and\nsupervised classification methods are utilised using the similarity measure\nbetween two realisations derived as empirical estimates of $\\mathcal\nN$-distances quantified based on geometric characteristics of the realisations,\nnamely the boundary curvature and the perimeter over area ratios of obtained\nsamples of connected components from the realisations. To justify the proposed\nmethodology, a simulation study is performed using random set models. The\nmethods are used further for classifying histological images of mastopathy and\nmammary cancer tissue."}
{"id": "2511.00849", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00849", "abs": "https://arxiv.org/abs/2511.00849", "authors": ["Zhexiao Huang", "Weihao He", "Shutao Deng", "Junzhe Chen", "Chao Yuan", "Hongxin Wang", "Changsheng Zhou"], "title": "Perturbations in the Orthogonal Complement Subspace for Efficient Out-of-Distribution Detection", "comment": null, "summary": "Out-of-distribution (OOD) detection is essential for deploying deep learning\nmodels in open-world environments. Existing approaches, such as energy-based\nscoring and gradient-projection methods, typically rely on high-dimensional\nrepresentations to separate in-distribution (ID) and OOD samples. We introduce\nP-OCS (Perturbations in the Orthogonal Complement Subspace), a lightweight and\ntheoretically grounded method that operates in the orthogonal complement of the\nprincipal subspace defined by ID features. P-OCS applies a single projected\nperturbation restricted to this complementary subspace, enhancing subtle ID-OOD\ndistinctions while preserving the geometry of ID representations. We show that\na one-step update is sufficient in the small-perturbation regime and provide\nconvergence guarantees for the resulting detection score. Experiments across\nmultiple architectures and datasets demonstrate that P-OCS achieves\nstate-of-the-art OOD detection with negligible computational cost and without\nrequiring model retraining, access to OOD data, or changes to model\narchitecture."}
{"id": "2511.01732", "categories": ["stat.AP", "q-bio.QM", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.01732", "abs": "https://arxiv.org/abs/2511.01732", "authors": ["Liangkang Wang", "Akhil Ambekar", "Ani Eloyan"], "title": "Geometric Modeling of Hippocampal Tau Deposition: A Surface-Based Framework for Covariate Analysis and Off-Target Contamination Detection", "comment": null, "summary": "We introduce a framework combining geometric modeling with disease\nprogression analysis to investigate tau deposition in Alzheimer's disease (AD)\nusing positron emission tomography (PET) data. Focusing on the hippocampus, we\nconstruct a principal surface that captures the spatial distribution and\nmorphological changes of tau pathology. By projecting voxels onto this surface,\nwe quantify tau coverage, intensity, and thickness through bidirectional\nprojection distances and interpolated standardized uptake value ratios (SUVR).\nThis low-dimensional embedding preserves spatial specificity while mitigating\nmultiple comparison issues. Covariate effects are analyzed using a two-stage\nregression model with inverse probability weighting to adjust for signal\nsparsity and selection bias. Using the SuStaIn model, we identify subtypes and\nstages of AD, revealing distinct tau dynamics: the limbic-predominant subtype\nshows age-related nonlinear accumulation in coverage and thickness, whereas the\nposterior subtype exhibits uniform SUVR increases across disease progression.\nModel-based predictions show that hippocampal tau deposition follows a\nstructured spatial trajectory expanding bidirectionally with increasing\nthickness, while subtype differences highlight posterior hippocampal\ninvolvement consistent with whole-brain patterns. Finally, directional signal\npatterns on the principal surface reveal contamination from the choroid plexus,\ndemonstrating the broader applicability of the proposed framework across\nmodalities including amyloid PET."}
{"id": "2511.00395", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.00395", "abs": "https://arxiv.org/abs/2511.00395", "authors": ["Chuanji Gao", "Gang Chen", "Svetlana V. Shinkareva", "Rutvik H. Desai"], "title": "Is Representational Similarity Analysis Reliable? A Comparison with Regression", "comment": null, "summary": "Representational Similarity Analysis (RSA) is a popular method for analyzing\nneuroimaging and behavioral data. Here we evaluate the accuracy and reliability\nof RSA in the context of model selection, and compare it to that of regression.\nAlthough RSA offers flexibility in handling high-dimensional, cross-modal, and\ncross-species data, its reliance on a transformation of raw data into\nsimilarity structures may result in the loss of critical stimulus-response\ninformation. Across extensive simulation studies and empirical analyses, we\nshow that RSA leads to lower model selection accuracy, regardless of sample\nsize, noise level, feature dimensionality, or multicollinearity, relative to\nregression. While principal component analysis and feature reweighting mitigate\nRSA's deficits driven by multicollinearity, regression remains superior in\naccurately distinguishing between models. Empirical data and a follow-up fMRI\nsimulation further support these conclusions. Our findings suggest that\nresearchers should carefully consider which approach to use: RSA is less\neffective than linear regression for model selection and fitting when direct\nstimulus-response mappings are available."}
{"id": "2511.00944", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.00944", "abs": "https://arxiv.org/abs/2511.00944", "authors": ["Qiang Liu", "Zhi Liu", "Wang Zhou"], "title": "On the estimation of leverage effect and volatility of volatility in the presence of jumps", "comment": null, "summary": "We study the estimation of leverage effect and volatility of volatility by\nusing high-frequency data with the presence of jumps. We first construct spot\nvolatility estimator by using the empirical characteristic function of the\nhigh-frequency increments to deal with the effect of jumps, based on which the\nestimators of leverage effect and volatility of volatility are proposed.\nCompared with existing estimators, our method is valid under more general\njumps, making it a better alternative for empirical applications. Under some\nmild conditions, the asymptotic normality of the estimators is established and\nconsistent estimators of the limiting variances are proposed based on the\nestimation of volatility functionals. We conduct extensive simulation study to\nverify the theoretical results. The results demonstrate that our estimators\nhave relative better performance than the existing ones, especially when the\njump is of infinite variation. Besides, we apply our estimators to a real\nhigh-frequency dataset, which reveals nonzero leverage effect and volatility of\nvolatility in the market."}
{"id": "2511.01037", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.01037", "abs": "https://arxiv.org/abs/2511.01037", "authors": ["Mihailo Stojnic"], "title": "Binary perceptron computational gap -- a parametric fl RDT view", "comment": null, "summary": "Recent studies suggest that asymmetric binary perceptron (ABP) likely\nexhibits the so-called statistical-computational gap characterized with the\nappearance of two phase transitioning constraint density thresholds:\n\\textbf{\\emph{(i)}} the \\emph{satisfiability threshold} $\\alpha_c$, below/above\nwhich ABP succeeds/fails to operate as a storage memory; and\n\\textbf{\\emph{(ii)}} \\emph{algorithmic threshold} $\\alpha_a$, below/above which\none can/cannot efficiently determine ABP's weight so that it operates as a\nstorage memory.\n  We consider a particular parametric utilization of \\emph{fully lifted random\nduality theory} (fl RDT) [85] and study its potential ABP's algorithmic\nimplications. A remarkable structural parametric change is uncovered as one\nprogresses through fl RDT lifting levels. On the first two levels, the\nso-called $\\c$ sequence -- a key parametric fl RDT component -- is of the\n(natural) decreasing type. A change of such phenomenology on higher levels is\nthen connected to the $\\alpha_c$ -- $\\alpha_a$ threshold change. Namely, on the\nsecond level concrete numerical values give for the critical constraint density\n$\\alpha=\\alpha_c\\approx 0.8331$. While progressing through higher levels\ndecreases this estimate, already on the fifth level we observe a satisfactory\nlevel of convergence and obtain $\\alpha\\approx 0.7764$. This allows to draw two\nstriking parallels: \\textbf{\\emph{(i)}} the obtained constraint density\nestimate is in a remarkable agrement with range $\\alpha\\in (0.77,0.78)$ of\nclustering defragmentation (believed to be responsible for failure of locally\nimproving algorithms) [17,88]; and \\textbf{\\emph{(ii)}} the observed change of\n$\\c$ sequence phenomenology closely matches the one of the negative Hopfield\nmodel for which the existence of efficient algorithms that closely approach\nsimilar type of threshold has been demonstrated recently [87]."}
{"id": "2511.01778", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.01778", "abs": "https://arxiv.org/abs/2511.01778", "authors": ["Richard Evans", "Max Felland", "Susanna Evans", "Lindsey Sloan"], "title": "Large Language Model-Derived Priors Can Improve Bayesian Survival Analyses: A Glioblastoma Application", "comment": "Presented at the 2nd Annual Southeast Wisconsin Data Science\n  (SEAWINDS) Research Symposium, Milwaukee, WI", "summary": "This report describes an application of artificial intelligence (AI) to the\nBayesian analysis of glioblastoma survival data. It has been suggested that AI\ncan be used to construct prior distributions for parameters in Bayesian models\nrather than using the difficult, unreliable, and time-consuming process of\neliciting expert opinion from radiation oncologists. Here, we show how\ngenerative AI can quickly propose sensible prior distributions of the hazard\nratio comparing two glioblastoma therapies, for a standard Bayesian survival\nmodel on real data. Three Chatbots generated two alternative priors each which\nwere evaluated by a radiation oncologist and then used in a sensitivity\nanalysis to assess posterior stability. The results suggest that, for this\ncancer survival analysis, priors from generative AI are a preferred alternative\nmethod to expert elicitation."}
{"id": "2511.01040", "categories": ["stat.OT", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.01040", "abs": "https://arxiv.org/abs/2511.01040", "authors": ["Junjie Ma", "Xiaoya Zhang", "Guangye He", "Yuting Han", "Ting Ge", "Feng Ji"], "title": "From Path Coefficients to Targeted Estimands: A Comparison of Structural Equation Models (SEM) and Targeted Maximum Likelihood Estimation (TMLE)", "comment": null, "summary": "Structural Equation Modeling (SEM) has gained popularity in the social\nsciences and causal inference due to its flexibility in modeling complex\nrelationships between variables and its availability in modern statistical\nsoftware. To move beyond the parametric assumptions of SEM, this paper reviews\ntargeted maximum likelihood estimation (TMLE), a doubly robust, machine\nlearning-based approach that builds on nonparametric SEM. We demonstrate that\nboth TMLE and SEM can be used to estimate standard causal effects and show that\nTMLE is robust to model misspecification. We conducted simulation studies under\nboth correct and misspecified model conditions, implementing SEM and TMLE to\nestimate these causal effects. The simulations confirm that TMLE consistently\noutperforms SEM under misspecification in terms of bias, mean squared error,\nand the validity of confidence intervals. We applied both approaches to a\nreal-world dataset to analyze the mediation effects of poverty on access to\nhigh school, revealing that the direct effect is no longer significant under\nTMLE, whereas SEM indicates significance. We conclude with practical guidance\non using SEM and TMLE in light of recent developments in targeted learning for\ncausal inference."}
{"id": "2511.01110", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01110", "abs": "https://arxiv.org/abs/2511.01110", "authors": ["Zhiwei Zhang", "Yongwu Shao", "Zhishen Ye"], "title": "Variance Estimation for the Inverse Probability of Treatment Weighted Kaplan Meier Estimator", "comment": null, "summary": "In a widely cited paper, Xie and Liu (henceforth XL) proposed to use inverse\nprobability of treatment weighting (IPTW) to account for possible confounding\nin observational studies with survival endpoints subject to right censoring.\nTheir proposal includes an IPTW Kaplan-Meier (KM) estimator for the survival\nfunction of a treatment-specific potential failure time, which can be used to\nevaluate the causal effect of one treatment versus another. The IPTW KM\nestimator is remarkably simple and highly effective for confounding bias\ncorrection. The method has been implemented in SAS's popular procedure LIFETEST\nfor analyzing survival data and has seen widespread use.\n  This letter is concerned with variance estimation for the IPTW KM estimator.\nThe variance estimator provided by XL does not account for the variability of\nthe IPTW weight when the propensity score is estimated from data, as is usually\nthe case in observational studies. In this letter, we provide a rigorous\nasymptotic analysis of the IPTW KM estimator based on an estimated propensity\nscore. Our analysis indicates that estimating the propensity score does tend to\nresult in a smaller asymptotic variance, which can be estimated consistently\nusing a plug-in variance estimator. We also present a simulation study\ncomparing the variance estimator we propose with the XL variance estimator. Our\nsimulation results confirm that the proposed variance estimator is more\naccurate than the XL variance estimator, which tends to over-estimate the\nsampling variance of the IPTW KM estimator."}
{"id": "2511.01064", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.01064", "abs": "https://arxiv.org/abs/2511.01064", "authors": ["Charles C. Margossian", "Lawrence K. Saul"], "title": "Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry", "comment": null, "summary": "We extend several recent results providing symmetry-based guarantees for\nvariational inference (VI) with location-scale families. VI approximates a\ntarget density~$p$ by the best match $q^*$ in a family $Q$ of tractable\ndistributions that in general does not contain $p$. It is known that VI can\nrecover key properties of $p$, such as its mean and correlation matrix, when\n$p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the\nreverse Kullback-Leibler divergence. We extend these guarantees in two\nimportant directions. First, we provide symmetry-based guarantees for a broader\nfamily of divergences, highlighting the properties of variational objectives\nunder which VI provably recovers the mean and correlation matrix. Second, we\nobtain further guarantees for VI when the target density $p$ exhibits even and\nelliptical symmetries in some but not all of its coordinates. These partial\nsymmetries arise naturally in Bayesian hierarchical models, where the prior\ninduces a challenging geometry but still possesses axes of symmetry. We\nillustrate these theoretical results in a number of experimental settings."}
{"id": "2511.01040", "categories": ["stat.OT", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.01040", "abs": "https://arxiv.org/abs/2511.01040", "authors": ["Junjie Ma", "Xiaoya Zhang", "Guangye He", "Yuting Han", "Ting Ge", "Feng Ji"], "title": "From Path Coefficients to Targeted Estimands: A Comparison of Structural Equation Models (SEM) and Targeted Maximum Likelihood Estimation (TMLE)", "comment": null, "summary": "Structural Equation Modeling (SEM) has gained popularity in the social\nsciences and causal inference due to its flexibility in modeling complex\nrelationships between variables and its availability in modern statistical\nsoftware. To move beyond the parametric assumptions of SEM, this paper reviews\ntargeted maximum likelihood estimation (TMLE), a doubly robust, machine\nlearning-based approach that builds on nonparametric SEM. We demonstrate that\nboth TMLE and SEM can be used to estimate standard causal effects and show that\nTMLE is robust to model misspecification. We conducted simulation studies under\nboth correct and misspecified model conditions, implementing SEM and TMLE to\nestimate these causal effects. The simulations confirm that TMLE consistently\noutperforms SEM under misspecification in terms of bias, mean squared error,\nand the validity of confidence intervals. We applied both approaches to a\nreal-world dataset to analyze the mediation effects of poverty on access to\nhigh school, revealing that the direct effect is no longer significant under\nTMLE, whereas SEM indicates significance. We conclude with practical guidance\non using SEM and TMLE in light of recent developments in targeted learning for\ncausal inference."}
{"id": "2511.01064", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.01064", "abs": "https://arxiv.org/abs/2511.01064", "authors": ["Charles C. Margossian", "Lawrence K. Saul"], "title": "Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry", "comment": null, "summary": "We extend several recent results providing symmetry-based guarantees for\nvariational inference (VI) with location-scale families. VI approximates a\ntarget density~$p$ by the best match $q^*$ in a family $Q$ of tractable\ndistributions that in general does not contain $p$. It is known that VI can\nrecover key properties of $p$, such as its mean and correlation matrix, when\n$p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the\nreverse Kullback-Leibler divergence. We extend these guarantees in two\nimportant directions. First, we provide symmetry-based guarantees for a broader\nfamily of divergences, highlighting the properties of variational objectives\nunder which VI provably recovers the mean and correlation matrix. Second, we\nobtain further guarantees for VI when the target density $p$ exhibits even and\nelliptical symmetries in some but not all of its coordinates. These partial\nsymmetries arise naturally in Bayesian hierarchical models, where the prior\ninduces a challenging geometry but still possesses axes of symmetry. We\nillustrate these theoretical results in a number of experimental settings."}
{"id": "2511.01151", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.01151", "abs": "https://arxiv.org/abs/2511.01151", "authors": ["Unnati Nigam", "Radhendushka Srivastava", "Faezeh Marzbanrad", "Michael Burke"], "title": "A structural equation formulation for general quasi-periodic Gaussian processes", "comment": null, "summary": "This paper introduces a structural equation formulation that gives rise to a\nnew family of quasi-periodic Gaussian processes, useful to process a broad\nclass of natural and physiological signals. The proposed formulation simplifies\ngeneration and forecasting, and provides hyperparameter estimates, which we\nexploit in a convergent and consistent iterative estimation algorithm. A\nbootstrap approach for standard error estimation and confidence intervals is\nalso provided. We demonstrate the computational and scaling benefits of the\nproposed approach on a broad class of problems, including water level tidal\nanalysis, CO$_{2}$ emission data, and sunspot numbers data. By leveraging the\nstructural equations, our method reduces the cost of likelihood evaluations and\npredictions from $\\mathcal{O}(k^2 p^2)$ to $\\mathcal{O}(p^2)$, significantly\nimproving scalability."}
{"id": "2511.01096", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01096", "abs": "https://arxiv.org/abs/2511.01096", "authors": ["Alex Boyd", "Andrew Warrington", "Taha Kass-Hout", "Parminder Bhatia", "Danica Xiao"], "title": "Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes", "comment": null, "summary": "Foundational marked temporal point process (MTPP) models, such as the Hawkes\nprocess, often use inexpressive model families in order to offer interpretable\nparameterizations of event data. On the other hand, neural MTPPs models forego\nthis interpretability in favor of absolute predictive performance. In this\nwork, we present a new family MTPP models: the hyper Hawkes process (HHP),\nwhich aims to be as flexible and performant as neural MTPPs, while retaining\ninterpretable aspects. To achieve this, the HHP extends the classical Hawkes\nprocess to increase its expressivity by first expanding the dimension of the\nprocess into a latent space, and then introducing a hypernetwork to allow time-\nand data-dependent dynamics. These extensions define a highly performant MTPP\nfamily, achieving state-of-the-art performance across a range of benchmark\ntasks and metrics. Furthermore, by retaining the linearity of the recurrence,\nalbeit now piecewise and conditionally linear, the HHP also retains much of the\nstructure of the original Hawkes process, which we exploit to create direct\nprobes into how the model creates predictions. HHP models therefore offer both\nstate-of-the-art predictions, while also providing an opportunity to ``open the\nbox'' and inspect how predictions were generated."}
{"id": "2511.01151", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.01151", "abs": "https://arxiv.org/abs/2511.01151", "authors": ["Unnati Nigam", "Radhendushka Srivastava", "Faezeh Marzbanrad", "Michael Burke"], "title": "A structural equation formulation for general quasi-periodic Gaussian processes", "comment": null, "summary": "This paper introduces a structural equation formulation that gives rise to a\nnew family of quasi-periodic Gaussian processes, useful to process a broad\nclass of natural and physiological signals. The proposed formulation simplifies\ngeneration and forecasting, and provides hyperparameter estimates, which we\nexploit in a convergent and consistent iterative estimation algorithm. A\nbootstrap approach for standard error estimation and confidence intervals is\nalso provided. We demonstrate the computational and scaling benefits of the\nproposed approach on a broad class of problems, including water level tidal\nanalysis, CO$_{2}$ emission data, and sunspot numbers data. By leveraging the\nstructural equations, our method reduces the cost of likelihood evaluations and\npredictions from $\\mathcal{O}(k^2 p^2)$ to $\\mathcal{O}(p^2)$, significantly\nimproving scalability."}
{"id": "2511.01222", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01222", "abs": "https://arxiv.org/abs/2511.01222", "authors": ["Mengchu Zheng", "Matteo Bonvini", "Zijian Guo"], "title": "Perturbed Double Machine Learning: Nonstandard Inference Beyond the Parametric Length", "comment": null, "summary": "We study inference on a low dimensional functional $\\beta$ in the presence of\npossibly infinite dimensional nuisance parameters. Classical inferential\nmethods are typically based on the Wald interval, whose large sample validity\nrests on the asymptotic negligibility of the nuisance error; for example,\nestimators based on the influence curve of the parameter (Double/Debiased\nMachine Learning DML estimators) are asymptotically Gaussian when the nuisance\nestimators converge at rates faster than $n^{-1/4}$. Although, under suitable\nconditions, such negligibility can hold even in nonparametric classes, it can\nbe restrictive. To relax this requirement, we propose Perturbed Double Machine\nLearning (Perturbed DML) to ensure valid inference even when nuisance\nestimators converge at rates slower than $n^{-1/4}$. Our proposal is to 1)\ninject randomness into the nuisance estimation step to generate a collection of\nperturbed nuisance models, each yielding an estimate of $\\beta$ and a\ncorresponding Wald interval, and 2) filter out perturbations whose deviations\nfrom the original DML estimate exceed a threshold. For Lasso nuisance learners,\nwe show that, with high probability, at least one perturbation produces\nnuisance estimates sufficiently close to the truth, so that the associated\nestimator of $\\beta$ is close to an oracle estimator with knowledge of the true\nnuisances. Taking the union of the retained intervals delivers valid coverage\neven when the DML estimator converges more slowly than $n^{-1/2}$. The\nframework extends to general machine learning nuisance learners, and\nsimulations show that Perturbed DML can have coverage when state of the art\nmethods fail."}
{"id": "2511.01140", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.01140", "abs": "https://arxiv.org/abs/2511.01140", "authors": ["Md Talha Mohsin", "Ismail Abdulrashid"], "title": "Few-Shot Multimodal Medical Imaging: A Theoretical Framework", "comment": "6 Pages", "summary": "Medical imaging relies heavily on large, labeled datasets. But,\nunfortunately, they are not always easily accessible in clinical settings.\nAdditionally, many practitioners often face various structural obstacles like\nlimited data availability, fragmented data systems, and unbalanced datasets.\nThese barriers often lead to the increased diagnostic uncertainty,\nunderrepresentation of certain conditions, reduced model robustness, and biased\ndiagnostic decisions. In response to these challenges, approaches such as\ntransfer learning, meta-learning, and multimodal fusion have made great\nstrides. However, they still need a solid theoretical justification for why\nthey succeed or fail in situations where data is scarce. To address this gap,\nwe propose a unified theoretical framework that characterizes learning and\ninference under low-resource medical imaging conditions. We first formalize the\nlearning objective under few-shot conditions and compute sample complexity\nconstraints to estimate the smallest quantity of data needed to achieve\nclinically reliable accuracy. Then based on ideas from PAC-learning and\nPAC-Bayesian theory, we explain how multimodal integration encourages\ngeneralization and quantifies uncertainty under sparse supervision. We further\npropose a formal metric for explanation stability, offering interpretability\nguarantees under low-data conditions. Taken together, the proposed framework\nestablishes a principled foundation for constructing dependable, data-efficient\ndiagnostic systems by jointly characterizing sample efficiency, uncertainty\nquantification, and interpretability in a unified theoretical setting."}
{"id": "2511.01705", "categories": ["stat.ME", "cs.SI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.01705", "abs": "https://arxiv.org/abs/2511.01705", "authors": ["Edoardo Di Martino", "Matteo Cinelli", "Roy Cerqueti"], "title": "Z-Dip: a validated generalization of the Dip Test", "comment": null, "summary": "Detecting multimodality in empirical distributions is a fundamental problem\nin statistics and data analysis, with applications ranging from clustering to\nsocial science. Hartigan's Dip Test is a classical nonparametric procedure for\ntesting unimodality versus multimodality, but its interpretation is hindered by\nstrong dependence on sample size and the need for lookup tables. We introduce\nthe Z-Dip, a standardized extension of the Dip Test that removes sample-size\ndependence by comparing observed Dip values to simulated null distributions. We\ncalibrate a universal decision threshold for Z-Dip via simulation and bootstrap\nresampling, providing a unified criterion for multimodality detection. In the\nfinal section, we also propose a downsampling-based approach to further\nmitigate residual sample-size effects in very large datasets. Lookup tables and\nsoftware implementations are made available for efficient use in practice."}
{"id": "2511.01290", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01290", "abs": "https://arxiv.org/abs/2511.01290", "authors": ["Kana Makino", "Natsumi Makigusa", "Masahiro Kojima"], "title": "Seamless Phase I--II Cancer Clinical Trials Using Kernel-Based Covariate Similarity", "comment": null, "summary": "In response to the U.S.\\ Food and Drug Administration's (FDA) Project\nOptimus, a paradigm shift is underway in the design of early-phase oncology\ntrials. To accelerate drug development, seamless Phase I/II designs have gained\nincreasing attention, along with growing interest in the efficient reuse of\nPhase I data. We propose a nonparametric information-borrowing method that\nadaptively discounts Phase I observations according to the similarity of\ncovariate distributions between Phase I and Phase II. Similarity is quantified\nusing a kernel-based maximum mean discrepancy (MMD) and transformed into a\ndose-specific weight incorporated into a power-prior framework for Phase II\nefficacy evaluation, such as for the objective response rate (ORR). Considering\nthe small sample sizes typical of early-phase oncology studies, we analytically\nderive a confidence interval for the weight, enabling assessment of borrowing\nprecision without resampling procedures. Simulation studies under four toxicity\nscenarios and five baseline-covariate settings showed that the proposed method\nimproved the probability that the lower bound of the 95\\% credible interval for\nORR exceeded a prespecified threshold at efficacious doses, while avoiding\nfalse threshold crossings at weakly efficacious doses. A case study based on a\nmetastatic pancreatic ductal adenocarcinoma trial illustrates the resulting\nborrowing weights and posterior estimates."}
{"id": "2511.01196", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01196", "abs": "https://arxiv.org/abs/2511.01196", "authors": ["Jicong Fan"], "title": "An Interdisciplinary and Cross-Task Review on Missing Data Imputation", "comment": null, "summary": "Missing data is a fundamental challenge in data science, significantly\nhindering analysis and decision-making across a wide range of disciplines,\nincluding healthcare, bioinformatics, social science, e-commerce, and\nindustrial monitoring. Despite decades of research and numerous imputation\nmethods, the literature remains fragmented across fields, creating a critical\nneed for a comprehensive synthesis that connects statistical foundations with\nmodern machine learning advances. This work systematically reviews core\nconcepts-including missingness mechanisms, single versus multiple imputation,\nand different imputation goals-and examines problem characteristics across\nvarious domains. It provides a thorough categorization of imputation methods,\nspanning classical techniques (e.g., regression, the EM algorithm) to modern\napproaches like low-rank and high-rank matrix completion, deep learning models\n(autoencoders, GANs, diffusion models, graph neural networks), and large\nlanguage models. Special attention is given to methods for complex data types,\nsuch as tensors, time series, streaming data, graph-structured data,\ncategorical data, and multimodal data. Beyond methodology, we investigate the\ncrucial integration of imputation with downstream tasks like classification,\nclustering, and anomaly detection, examining both sequential pipelines and\njoint optimization frameworks. The review also assesses theoretical guarantees,\nbenchmarking resources, and evaluation metrics. Finally, we identify critical\nchallenges and future directions, emphasizing model selection and\nhyperparameter optimization, the growing importance of privacy-preserving\nimputation via federated learning, and the pursuit of generalizable models that\ncan adapt across domains and data types, thereby outlining a roadmap for future\nresearch."}
{"id": "2511.01412", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01412", "abs": "https://arxiv.org/abs/2511.01412", "authors": ["Rui Hu", "Ted Westling"], "title": "Nonparametric Sensitivity Analysis for Unobserved Confounding with Survival Outcomes", "comment": null, "summary": "In observational studies, the observed association between an exposure and\noutcome of interest may be distorted by unobserved confounding. Causal\nsensitivity analysis can be used to assess the robustness of observed\nassociations to potential unobserved confounding. For time-to-event outcomes,\nexisting sensitivity analysis methods rely on parametric assumptions on the\nstructure of the unobserved confounders and Cox proportional hazards models for\nthe outcome regression. If these assumptions fail to hold, it is unclear\nwhether the conclusions of the sensitivity analysis remain valid. Additionally,\ncausal interpretation of the hazard ratio is challenging. To address these\nlimitations, in this paper we develop a nonparametric sensitivity analysis\nframework for time-to-event data. Specifically, we derive nonparametric bounds\nfor the difference between the observed and counterfactual survival curves and\npropose estimators and inference for these bounds using semiparametric\nefficiency theory. We also provide nonparametric bounds and inference for the\ndifference between the observed and counterfactual restricted mean survival\ntimes. We demonstrate the performance of our proposed methods using numerical\nstudies and an analysis of the causal effect of elective neck dissection on\nmortality in patients with high-grade parotid carcinoma."}
{"id": "2511.01292", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01292", "abs": "https://arxiv.org/abs/2511.01292", "authors": ["Samet Demir", "Zafer Dogan"], "title": "Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift", "comment": "26 pages, 6 figures", "summary": "Pretrained Transformers excel at in-context learning (ICL), inferring new\ntasks from only a handful of examples. Yet, their ICL performance can degrade\nsharply under distribution shift between pretraining and test data, a regime\nincreasingly common in real-world deployments. While recent empirical work\nhints that adjusting the attention temperature in the softmax can enhance\nTransformer performance, the attention temperature's role in ICL under\ndistribution shift remains unexplored. This paper provides the first\ntheoretical and empirical study of attention temperature for ICL under\ndistribution shift. Using a simplified but expressive \"linearized softmax\"\nframework, we derive closed-form generalization error expressions and prove\nthat shifts in input covariance or label noise substantially impair ICL, but\nthat an optimal attention temperature exists which minimizes this error. We\nthen validate our predictions through extensive simulations on linear\nregression tasks and large-scale experiments with GPT-2 and LLaMA2-7B on\nquestion-answering benchmarks. Our results establish attention temperature as a\nprincipled and powerful mechanism for improving the robustness of ICL in\npretrained Transformers, advancing theoretical understanding and providing\nactionable guidance for selecting attention temperature in practice."}
{"id": "2511.01487", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01487", "abs": "https://arxiv.org/abs/2511.01487", "authors": ["Xiaoyi Wang", "Jixuan Liu", "Long Feng"], "title": "Adaptive Change Point Inference for High Dimensional Time Series with Temporal Dependence", "comment": null, "summary": "This paper investigates change point inference in high-dimensional time\nseries. We begin by introducing a max-$L_2$-norm based test procedure, which\ndemonstrates strong performance under dense alternatives. We then establish the\nasymptotic independence between our proposed statistic and the two\nmax-$L_\\infty$-based statistics introduced by Wang and Feng (2023). Building on\nthis result, we develop an adaptive inference approach by applying the Cauchy\ncombination method to integrate these tests. This combined procedure exhibits\nrobust performance across varying levels of sparsity. Extensive simulation\nstudies and real data analysis further confirm the superior effectiveness of\nour proposed methods in the high-dimensional setting."}
{"id": "2511.01628", "categories": ["stat.ML", "cs.LG", "stat.ME", "62G99"], "pdf": "https://arxiv.org/pdf/2511.01628", "abs": "https://arxiv.org/abs/2511.01628", "authors": ["Arran Carter", "Torben Sell"], "title": "Partial Trace-Class Bayesian Neural Networks", "comment": "10 pages, 4 figures", "summary": "Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in\ndeep learning, but often come at a prohibitive computational cost. We propose\nthree different innovative architectures of partial trace-class Bayesian neural\nnetworks (PaTraC BNNs) that enable uncertainty quantification comparable to\nstandard BNNs but use significantly fewer Bayesian parameters. These PaTraC\nBNNs have computational and statistical advantages over standard Bayesian\nneural networks in terms of speed and memory requirements. Our proposed\nmethodology therefore facilitates reliable, robust, and scalable uncertainty\nquantification in neural networks. The three architectures build on trace-class\nneural network priors which induce an ordering of the neural network\nparameters, and are thus a natural choice in our framework. In a numerical\nsimulation study, we verify the claimed benefits, and further illustrate the\nperformance of our proposed methodology on a real-world dataset."}
{"id": "2511.01705", "categories": ["stat.ME", "cs.SI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.01705", "abs": "https://arxiv.org/abs/2511.01705", "authors": ["Edoardo Di Martino", "Matteo Cinelli", "Roy Cerqueti"], "title": "Z-Dip: a validated generalization of the Dip Test", "comment": null, "summary": "Detecting multimodality in empirical distributions is a fundamental problem\nin statistics and data analysis, with applications ranging from clustering to\nsocial science. Hartigan's Dip Test is a classical nonparametric procedure for\ntesting unimodality versus multimodality, but its interpretation is hindered by\nstrong dependence on sample size and the need for lookup tables. We introduce\nthe Z-Dip, a standardized extension of the Dip Test that removes sample-size\ndependence by comparing observed Dip values to simulated null distributions. We\ncalibrate a universal decision threshold for Z-Dip via simulation and bootstrap\nresampling, providing a unified criterion for multimodality detection. In the\nfinal section, we also propose a downsampling-based approach to further\nmitigate residual sample-size effects in very large datasets. Lookup tables and\nsoftware implementations are made available for efficient use in practice."}
{"id": "2511.01734", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01734", "abs": "https://arxiv.org/abs/2511.01734", "authors": ["Soufiane Hayou"], "title": "A Proof of Learning Rate Transfer under $μ$P", "comment": "23 pages", "summary": "We provide the first proof of learning rate transfer with width in a linear\nmulti-layer perceptron (MLP) parametrized with $\\mu$P, a neural network\nparameterization designed to ``maximize'' feature learning in the\ninfinite-width limit. We show that under $\\mu P$, the optimal learning rate\nconverges to a \\emph{non-zero constant} as width goes to infinity, providing a\ntheoretical explanation to learning rate transfer. In contrast, we show that\nthis property fails to hold under alternative parametrizations such as Standard\nParametrization (SP) and Neural Tangent Parametrization (NTP). We provide\nintuitive proofs and support the theoretical findings with extensive empirical\nresults."}
{"id": "2511.01785", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01785", "abs": "https://arxiv.org/abs/2511.01785", "authors": ["Lang Lang", "Yao Zhao", "Qiuxin Gao", "Yanxun Xu"], "title": "RESOLVE-IPD: High-Fidelity Individual Patient Data Reconstruction and Uncertainty-Aware Subgroup Meta-Analysis", "comment": null, "summary": "Individual patient data (IPD) from oncology trials are essential for reliable\nevidence synthesis but are rarely publicly available, necessitating\nreconstruction from published Kaplan-Meier (KM) curves. Existing reconstruction\nmethods suffer from digitization errors, unrealistic uniform censoring\nassumptions, and the inability to recover subgroup-level IPD when only\naggregate statistics are available. We developed RESOLVE-IPD, a unified\ncomputational framework that enables high-fidelity IPD reconstruction and\nuncertainty-aware subgroup meta-analysis to address these limitations.\nRESOLVE-IPD comprises two components. The first component, High-Fidelity IPD\nReconstruction, integrates the VEC-KM and CEN-KM modules: VEC-KM extracts\nprecise KM coordinates and explicit censoring marks from vectorized figures,\nminimizing digitization error, while CEN-KM corrects overlapping censor symbols\nand eliminates the uniform censoring assumption. The second component,\nUncertainty-Aware Subgroup Recovery, employs the MAPLE (Marginal Assignment of\nPlausible Labels and Evidence Propagation) algorithm to infer patient-level\nsubgroup labels consistent with published summary statistics (e.g., hazard\nratio, median overall survival) when subgroup KM curves are unavailable. MAPLE\ngenerates ensembles of mathematically valid labelings, facilitating a\npropagating meta-analysis that quantifies and reflects uncertainty from\nsubgroup reconstruction. RESOLVE-IPD was validated through a subgroup\nmeta-analysis of four trials in advanced esophageal squamous cell carcinoma,\nfocusing on the programmed death ligand 1 (PD-L1)-low population. RESOLVE-IPD\nenables accurate IPD reconstruction and robust, uncertainty-aware subgroup\nmeta-analyses, strengthening the reliability and transparency of secondary\nevidence synthesis in precision oncology."}
{"id": "2511.00708", "categories": ["stat.CO", "math.PR", "stat.ML", "60J20, 65C05, 65C40, 68Q25"], "pdf": "https://arxiv.org/pdf/2511.00708", "abs": "https://arxiv.org/abs/2511.00708", "authors": ["Quan Zhou"], "title": "Polynomial Mixing Times of Simulated Tempering for Mixture Targets by Conductance Decomposition", "comment": "37 pages", "summary": "We study the theoretical complexity of simulated tempering for sampling from\nmixtures of log-concave components differing only by location shifts. The main\nresult establishes the first polynomial-time guarantee for simulated tempering\ncombined with the Metropolis-adjusted Langevin algorithm (MALA) with respect to\nthe problem dimension $d$, maximum mode displacement $D$, and logarithmic\naccuracy $\\log \\epsilon^{-1}$. The proof builds on a general state\ndecomposition theorem for $s$-conductance, applied to an auxiliary Markov chain\nconstructed on an augmented space. We also obtain an improved complexity\nestimate for simulated tempering combined with random-walk Metropolis. Our\nbounds assume an inverse-temperature ladder with smallest value $\\beta_1 =\nO(D^{-2})$ and spacing $\\beta_{i+1}/\\beta_i = 1 + O( d^{-1/2} )$, both of which\nare shown to be asymptotically optimal up to logarithmic factors."}
{"id": "2511.00217", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.00217", "abs": "https://arxiv.org/abs/2511.00217", "authors": ["Mitchell L. Prevett", "Francis K. C. Hui", "Zhi Yang Tho", "A. H. Welsh", "Anton H. Westveld"], "title": "Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data", "comment": null, "summary": "Linear mixed models are widely used for clustered data, but their reliance on\nparametric forms limits flexibility in complex and high-dimensional settings.\nIn contrast, gradient boosting methods achieve high predictive accuracy through\nnonparametric estimation, but do not accommodate clustered data structures or\nprovide uncertainty quantification.\n  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and\nalgorithm that extends boosting to jointly estimate mean and variance\ncomponents via likelihood-based gradients. In addition to nonparametric mean\nestimation, the method models both random effects and residual variances as\npotentially covariate-dependent functions using flexible base learners such as\nregression trees or splines, enabling nonparametric estimation while\nmaintaining interpretability.\n  Simulations and real-world applications demonstrate accurate recovery of\nvariance components, calibrated prediction intervals, and improved predictive\naccuracy relative to standard linear mixed models and nonparametric methods.\nGBMixed provides heteroscedastic uncertainty quantification and introduces\nboosting for heterogeneous random effects. This enables covariate-dependent\nshrinkage for cluster-specific predictions to adapt between population and\ncluster-level data. Under standard causal assumptions, the framework enables\nestimation of heterogeneous treatment effects with reliable uncertainty\nquantification."}
{"id": "2511.01628", "categories": ["stat.ML", "cs.LG", "stat.ME", "62G99"], "pdf": "https://arxiv.org/pdf/2511.01628", "abs": "https://arxiv.org/abs/2511.01628", "authors": ["Arran Carter", "Torben Sell"], "title": "Partial Trace-Class Bayesian Neural Networks", "comment": "10 pages, 4 figures", "summary": "Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in\ndeep learning, but often come at a prohibitive computational cost. We propose\nthree different innovative architectures of partial trace-class Bayesian neural\nnetworks (PaTraC BNNs) that enable uncertainty quantification comparable to\nstandard BNNs but use significantly fewer Bayesian parameters. These PaTraC\nBNNs have computational and statistical advantages over standard Bayesian\nneural networks in terms of speed and memory requirements. Our proposed\nmethodology therefore facilitates reliable, robust, and scalable uncertainty\nquantification in neural networks. The three architectures build on trace-class\nneural network priors which induce an ordering of the neural network\nparameters, and are thus a natural choice in our framework. In a numerical\nsimulation study, we verify the claimed benefits, and further illustrate the\nperformance of our proposed methodology on a real-world dataset."}
