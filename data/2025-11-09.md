<div id=toc></div>

# Table of Contents

- [stat.OT](#stat.OT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 12]
- [stat.AP](#stat.AP) [Total: 4]
- [stat.ME](#stat.ME) [Total: 12]


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [1] [Can we trust LLMs as a tutor for our students? Evaluating the Quality of LLM-generated Feedback in Statistics Exams](https://arxiv.org/abs/2511.04213)
*Markus Herklotz,Niklas Ippisch,Anna-Carolina Haensch*

Main category: stat.OT

TL;DR: 本研究探索了在慕尼黑大学统计学导论课程中使用LLM进行自动反馈的效果，发现约7%的反馈存在错误，且反馈主要关注答案对错解释，缺乏深度概念洞察和学习策略建议。


<details>
  <summary>Details</summary>
Motivation: 解决大规模课程中教师难以提供个性化反馈的问题，探索利用大型语言模型作为可扩展反馈工具的潜力。

Method: 在统计学导论课程中进行LLM批改模拟考试，使用连接GPT-4的在线平台，学生完成练习后获得基于教师提供正确答案的个性化反馈。

Result: 在2389个反馈实例中，约7%存在错误，从轻微技术不准确到概念误导性解释不等；反馈主要集中于解释答案对错，较少提供深度概念洞察、学习策略或自我调节建议。

Conclusion: LLM作为高等教育中的可扩展反馈工具具有潜力但也存在局限，需要仔细的质量监控和提示设计以最大化其教学价值。

Abstract: One of the central challenges for instructors is offering meaningful
individual feedback, especially in large courses. Faced with limited time and
resources, educators are often forced to rely on generalized feedback, even
when more personalized support would be pedagogically valuable. To overcome
this limitation, one potential technical solution is to utilize large language
models (LLMs). For an exploratory study using a new platform connected with
LLMs, we conducted a LLM-corrected mock exam during the "Introduction to
Statistics" lecture at the University of Munich (Germany). The online platform
allows instructors to upload exercises along with the correct solutions.
Students complete these exercises and receive overall feedback on their
results, as well as individualized feedback generated by GPT-4 based on the
correct answers provided by the lecturers. The resulting dataset comprised
task-level information for all participating students, including individual
responses and the corresponding LLM-generated feedback. Our systematic analysis
revealed that approximately 7 \% of the 2,389 feedback instances contained
errors, ranging from minor technical inaccuracies to conceptually misleading
explanations. Further, using a combined feedback framework approach, we found
that the feedback predominantly focused on explaining why an answer was correct
or incorrect, with fewer instances providing deeper conceptual insights,
learning strategies or self-regulatory advice. These findings highlight both
the potential and the limitations of deploying LLMs as scalable feedback tools
in higher education, emphasizing the need for careful quality monitoring and
prompt design to maximize their pedagogical value.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [2] [Friction on Demand: A Generative Framework for the Inverse Design of Metainterfaces](https://arxiv.org/abs/2511.03735)
*Valentin Mouton,Adrien Mélot*

Main category: stat.ML

TL;DR: 使用变分自编码器(VAEs)生成满足目标摩擦定律的表面形貌的生成建模框架，解决了摩擦界面设计的逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖低维参数化的启发式搜索，难以处理复杂非线性摩擦定律，且接触模拟计算成本高。需要一种能高效生成多样化表面形貌的方法。

Method: 基于参数化接触力学模型构建2亿样本的合成数据集，训练VAE模型，实现无需模拟的候选表面形貌生成。

Result: 该方法能够高效生成候选表面形貌，在准确性、吞吐量和多样性之间取得平衡，为实时控制摩擦行为提供可能。

Conclusion: 生成建模为摩擦界面逆设计提供了新途径，但需要在准确性、吞吐量和多样性之间权衡，为通过定制表面形貌实现近实时摩擦控制铺平了道路。

Abstract: Designing frictional interfaces to exhibit prescribed macroscopic behavior is
a challenging inverse problem, made difficult by the non-uniqueness of
solutions and the computational cost of contact simulations. Traditional
approaches rely on heuristic search over low-dimensional parameterizations,
which limits their applicability to more complex or nonlinear friction laws. We
introduce a generative modeling framework using Variational Autoencoders (VAEs)
to infer surface topographies from target friction laws. Trained on a synthetic
dataset composed of 200 million samples constructed from a parameterized
contact mechanics model, the proposed method enables efficient, simulation-free
generation of candidate topographies. We examine the potential and limitations
of generative modeling for this inverse design task, focusing on balancing
accuracy, throughput, and diversity in the generated solutions. Our results
highlight trade-offs and outline practical considerations when balancing these
objectives. This approach paves the way for near-real-time control of
frictional behavior through tailored surface topographies.

</details>


### [3] [Bifidelity Karhunen-Loève Expansion Surrogate with Active Learning for Random Fields](https://arxiv.org/abs/2511.03756)
*Aniket Jivani,Cosmin Safta,Beckett Y. Zhou,Xun Huan*

Main category: stat.ML

TL;DR: 提出了一种双保真度Karhunen-Loève展开(KLE)代理模型，用于处理不确定输入下的场值感兴趣量(QoIs)，结合主动学习策略提高计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统单保真度方法计算成本高，需要开发能结合低保真度模拟的效率和低保真度模拟的精度的方法，以构建准确且计算负担小的代理模型。

Method: 将KLE的光谱效率与多项式混沌展开(PCEs)结合，耦合低保真度模拟捕捉主要响应趋势和有限数量的低保真度模拟校正系统偏差，并采用基于泛化误差的主动学习策略自适应选择新低保真度评估。

Result: 在三个复杂度递增的案例中(一维分析基准、二维对流扩散系统、三维湍流圆射流模拟)，该方法相比单保真度和随机采样方法在预测精度和样本效率方面均取得一致改进。

Conclusion: BF-KLE-AL框架能够有效构建准确且计算负担小的场值QoIs代理模型，在保持精度的同时显著降低计算成本。

Abstract: We present a bifidelity Karhunen-Lo\`eve expansion (KLE) surrogate model for
field-valued quantities of interest (QoIs) under uncertain inputs. The approach
combines the spectral efficiency of the KLE with polynomial chaos expansions
(PCEs) to preserve an explicit mapping between input uncertainties and output
fields. By coupling inexpensive low-fidelity (LF) simulations that capture
dominant response trends with a limited number of high-fidelity (HF)
simulations that correct for systematic bias, the proposed method enables
accurate and computationally affordable surrogate construction. To further
improve surrogate accuracy, we form an active learning strategy that adaptively
selects new HF evaluations based on the surrogate's generalization error,
estimated via cross-validation and modeled using Gaussian process regression.
New HF samples are then acquired by maximizing an expected improvement
criterion, targeting regions of high surrogate error. The resulting BF-KLE-AL
framework is demonstrated on three examples of increasing complexity: a
one-dimensional analytical benchmark, a two-dimensional convection-diffusion
system, and a three-dimensional turbulent round jet simulation based on
Reynolds-averaged Navier--Stokes (RANS) and enhanced delayed detached-eddy
simulations (EDDES). Across these cases, the method achieves consistent
improvements in predictive accuracy and sample efficiency relative to
single-fidelity and random-sampling approaches.

</details>


### [4] [Learning Paths for Dynamic Measure Transport: A Control Perspective](https://arxiv.org/abs/2511.03797)
*Aimee Maurais,Bamdad Hosseini,Youssef Marzouk*

Main category: stat.ML

TL;DR: 本文从控制角度提出了一种用于动态测度传输采样的测度路径识别方法，通过连接现有方法与均值场博弈，构建了优化问题族来寻找倾斜测度路径，并开发了基于高斯过程的数值算法。


<details>
  <summary>Details</summary>
Motivation: 现有动态测度传输中常用的测度路径可能效果不佳，需要寻找更优的路径选择方法。

Method: 提出基于均值场博弈连接的优化问题族，使用鼓励速度平滑性的目标项，开发基于高斯过程求解偏微分方程的数值算法。

Result: 相比使用未倾斜参考路径的方法，本文方法能够恢复更高效且平滑的传输模型。

Conclusion: 从控制视角提出的倾斜测度路径优化方法在动态测度传输中表现优于传统路径选择，提供了更有效的采样解决方案。

Abstract: We bring a control perspective to the problem of identifying paths of
measures for sampling via dynamic measure transport (DMT). We highlight the
fact that commonly used paths may be poor choices for DMT and connect existing
methods for learning alternate paths to mean-field games. Based on these
connections we pose a flexible family of optimization problems for identifying
tilted paths of measures for DMT and advocate for the use of objective terms
which encourage smoothness of the corresponding velocities. We present a
numerical algorithm for solving these problems based on recent Gaussian process
methods for solution of partial differential equations and demonstrate the
ability of our method to recover more efficient and smooth transport models
compared to those which use an untilted reference path.

</details>


### [5] [A general technique for approximating high-dimensional empirical kernel matrices](https://arxiv.org/abs/2511.03892)
*Chiraag Kaushik,Justin Romberg,Vidya Muthukumar*

Main category: stat.ML

TL;DR: 提出了随机核矩阵期望算子范数的简单用户友好边界，使用U统计量的解耦和非交换Khintchine不等式，得到仅依赖于核函数标量统计量和相关核矩阵的上下界。应用于高维内积核矩阵，获得比现有方法更紧的近似，并给出各向异性高斯数据的新结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖矩方法和组合论证，较为复杂。需要更简单、用户友好的方法来估计随机核矩阵的算子范数，特别是在高维数据和各向异性高斯数据场景下。

Method: 使用U统计量的解耦结果和非交换Khintchine不等式，构建仅依赖于核函数标量统计量和相关核矩阵的上下界。

Result: 获得了比现有方法更紧的近似结果，简化了现有结果的证明，并为各向异性高斯数据提供了新的近似结果。还得到了核回归偏差的更紧下界。

Conclusion: 该方法提供了一种简单有效的工具来分析随机核矩阵的算子范数，特别适用于高维数据和各向异性高斯数据场景，简化了证明过程并获得了更精确的结果。

Abstract: We present simple, user-friendly bounds for the expected operator norm of a
random kernel matrix under general conditions on the kernel function
$k(\cdot,\cdot)$. Our approach uses decoupling results for U-statistics and the
non-commutative Khintchine inequality to obtain upper and lower bounds
depending only on scalar statistics of the kernel function and a ``correlation
kernel'' matrix corresponding to $k(\cdot,\cdot)$. We then apply our method to
provide new, tighter approximations for inner-product kernel matrices on
general high-dimensional data, where the sample size and data dimension are
polynomially related. Our method obtains simplified proofs of existing results
that rely on the moment method and combinatorial arguments while also providing
novel approximation results for the case of anisotropic Gaussian data. Finally,
using similar techniques to our approximation result, we show a tighter lower
bound on the bias of kernel regression with anisotropic Gaussian data.

</details>


### [6] [High-dimensional limit theorems for SGD: Momentum and Adaptive Step-sizes](https://arxiv.org/abs/2511.03952)
*Aukosh Jagannath,Taj Jones-McCormick,Varnan Sarangian*

Main category: stat.ML

TL;DR: 该论文为带Polyak动量的随机梯度下降(SGD-M)开发了高维缩放极限，并与在线SGD进行了严格比较，发现在特定时间重缩放和步长选择下两者极限相同，但相同步长时SGD-M会放大高维效应。


<details>
  <summary>Details</summary>
Motivation: 提供严格框架来比较在线SGD及其变体，解释早期预处理器如何在高维设置中稳定和改进动态性能。

Method: 开发SGD-M的高维缩放极限理论框架，在尖峰张量PCA和单指标模型上验证，并分析基于归一化梯度的自适应步长算法。

Result: SGD-M在特定条件下与在线SGD极限相同，但相同步长时会放大高维效应；自适应步长算法能产生更接近总体最小值的固定点，并扩大收敛步长范围。

Conclusion: 该研究为早期预处理器在高维设置中的稳定性和改进效果提供了严格的理论解释，与经验动机一致。

Abstract: We develop a high-dimensional scaling limit for Stochastic Gradient Descent
with Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework
to rigourously compare online SGD with some of its popular variants. We show
that the scaling limits of SGD-M coincide with those of online SGD after an
appropriate time rescaling and a specific choice of step-size. However, if the
step-size is kept the same between the two algorithms, SGD-M will amplify
high-dimensional effects, potentially degrading performance relative to online
SGD. We demonstrate our framework on two popular learning problems: Spiked
Tensor PCA and Single Index Models. In both cases, we also examine online SGD
with an adaptive step-size based on normalized gradients. In the
high-dimensional regime, this algorithm yields multiple benefits: its dynamics
admit fixed points closer to the population minimum and widens the range of
admissible step-sizes for which the iterates converge to such solutions. These
examples provide a rigorous account, aligning with empirical motivation, of how
early preconditioners can stabilize and improve dynamics in settings where
online SGD fails.

</details>


### [7] [Robust inference using density-powered Stein operators](https://arxiv.org/abs/2511.03963)
*Shinto Eguchi*

Main category: stat.ML

TL;DR: 提出了一种基于密度幂加权的γ-Stein算子，用于构建非归一化概率模型的鲁棒推断方法，并开发了γ-核化Stein差异和γ-Stein变分梯度下降两个关键应用。


<details>
  <summary>Details</summary>
Motivation: 现有Stein算子对异常值敏感，需要开发对异常值具有鲁棒性的推断方法，同时保持对模型归一化常数的不依赖性。

Method: 通过将模型密度提升到正幂γ进行加权，构建γ-Stein算子，从而内在降低异常值的影响。基于此开发了鲁棒的得分匹配方法、γ-核化Stein差异和γ-Stein变分梯度下降。

Result: 在受污染的高斯模型和四次势能模型上的实证结果表明，该方法在鲁棒性和统计效率方面显著优于标准基线方法。

Conclusion: γ-Stein算子提供了一种原则性的鲁棒机制，为构建对异常值不敏感的推断方法提供了新框架，在保持统计效率的同时显著提升了鲁棒性。

Abstract: We introduce a density-power weighted variant for the Stein operator, called
the $\gamma$-Stein operator. This is a novel class of operators derived from
the $\gamma$-divergence, designed to build robust inference methods for
unnormalized probability models. The operator's construction (weighting by the
model density raised to a positive power $\gamma$ inherently down-weights the
influence of outliers, providing a principled mechanism for robustness.
Applying this operator yields a robust generalization of score matching that
retains the crucial property of being independent of the model's normalizing
constant. We extend this framework to develop two key applications: the
$\gamma$-kernelized Stein discrepancy for robust goodness-of-fit testing, and
$\gamma$-Stein variational gradient descent for robust Bayesian posterior
approximation. Empirical results on contaminated Gaussian and quartic potential
models show our methods significantly outperform standard baselines in both
robustness and statistical efficiency.

</details>


### [8] [Online Bayesian Experimental Design for Partially Observed Dynamical Systems](https://arxiv.org/abs/2511.04403)
*Sara Pérez-Vieites,Sahel Iqbal,Simo Särkkä,Dominik Baumann*

Main category: stat.ML

TL;DR: 提出了一个用于部分可观测动态系统中贝叶斯实验设计的新框架，通过嵌套粒子滤波估计期望信息增益及其梯度，解决了非线性状态空间模型中的在线优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯实验设计方法无法处理具有部分可观测性的动态系统，这些系统通常建模为状态空间模型，其中潜状态使得似然函数和信息论目标（如期望信息增益）难以处理，且需要在线算法进行高效计算。

Method: 推导了新的期望信息增益及其梯度估计器，通过显式边缘化潜状态，在非线性状态空间模型中实现可扩展的随机优化。利用具有收敛保证的嵌套粒子滤波进行高效的在线推断。

Result: 在现实模型（如SIR传染病模型和移动源定位任务）上的应用表明，该框架成功处理了部分可观测性和在线计算问题。

Conclusion: 该研究为部分可观测动态系统中的贝叶斯实验设计提供了一个有效且可扩展的解决方案，能够处理复杂的非线性状态空间模型并实现高效的在线优化。

Abstract: Bayesian experimental design (BED) provides a principled framework for
optimizing data collection, but existing approaches do not apply to crucial
real-world settings such as dynamical systems with partial observability, where
only noisy and incomplete observations are available. These systems are
naturally modeled as state-space models (SSMs), where latent states mediate the
link between parameters and data, making the likelihood -- and thus
information-theoretic objectives like the expected information gain (EIG) --
intractable. In addition, the dynamical nature of the system requires online
algorithms that update posterior distributions and select designs sequentially
in a computationally efficient manner. We address these challenges by deriving
new estimators of the EIG and its gradient that explicitly marginalize latent
states, enabling scalable stochastic optimization in nonlinear SSMs. Our
approach leverages nested particle filters (NPFs) for efficient online
inference with convergence guarantees. Applications to realistic models, such
as the susceptible-infected-recovered (SIR) and a moving source location task,
show that our framework successfully handles both partial observability and
online computation.

</details>


### [9] [Online Conformal Inference with Retrospective Adjustment for Faster Adaptation to Distribution Shift](https://arxiv.org/abs/2511.04275)
*Jungbin Jun,Ilsang Ohn*

Main category: stat.ML

TL;DR: 提出了一种具有回顾性调整的在线保形推理方法，通过回归方法和留一更新公式来调整过去预测，以更快适应分布变化。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在在线环境中假设数据可交换性，但现实中数据分布会随时间变化，现有方法只能前向更新预测，适应分布变化较慢。

Method: 使用回归方法和高效的留一更新公式，在新数据到达时回顾性调整过去预测，使所有预测与最新数据分布对齐。

Result: 在合成和真实数据集上的实验表明，该方法比现有在线保形预测方法能更快重新校准覆盖度，并提高统计效率。

Conclusion: 所提出的回顾性调整方法能有效应对在线环境中的分布漂移问题，实现更快的适应性和更好的性能。

Abstract: Conformal prediction has emerged as a powerful framework for constructing
distribution-free prediction sets with guaranteed coverage assuming only the
exchangeability assumption. However, this assumption is often violated in
online environments where data distributions evolve over time. Several recent
approaches have been proposed to address this limitation, but, typically, they
slowly adapt to distribution shifts because they update predictions only in a
forward manner, that is, they generate a prediction for a newly observed data
point while previously computed predictions are not updated. In this paper, we
propose a novel online conformal inference method with retrospective
adjustment, which is designed to achieve faster adaptation to distributional
shifts. Our method leverages regression approaches with efficient leave-one-out
update formulas to retroactively adjust past predictions when new data arrive,
thereby aligning the entire set of predictions with the most recent data
distribution. Through extensive numerical studies performed on both synthetic
and real-world data sets, we show that the proposed approach achieves faster
coverage recalibration and improved statistical efficiency compared to existing
online conformal prediction methods.

</details>


### [10] [Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition](https://arxiv.org/abs/2511.04291)
*Giovanni Barbarino,Nicolas Gillis,Subhayan Saha*

Main category: stat.ML

TL;DR: 本文证明了最小体积非负矩阵分解在噪声存在下能够识别真实因子，前提是数据点在基向量生成的潜在单纯形中充分分散。


<details>
  <summary>Details</summary>
Motivation: 最小体积NMF已在多个领域成功应用，但其对噪声的鲁棒性一直是一个未解决的问题。

Method: 使用扩展充分分散条件，要求数据点在基向量生成的潜在单纯形中充分分散。

Result: 证明了在扩展充分分散条件下，最小体积NMF能够在噪声存在时识别出真实因子。

Conclusion: 最小体积NMF在满足特定分散条件时对噪声具有鲁棒性，解决了长期存在的开放性问题。

Abstract: Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used
successfully in many applications, such as hyperspectral imaging, chemical
kinetics, spectroscopy, topic modeling, and audio source separation. However,
its robustness to noise has been a long-standing open problem. In this paper,
we prove that min-vol NMF identifies the groundtruth factors in the presence of
noise under a condition referred to as the expanded sufficiently scattered
condition which requires the data points to be sufficiently well scattered in
the latent simplex generated by the basis vectors.

</details>


### [11] [Riesz Regression As Direct Density Ratio Estimation](https://arxiv.org/abs/2511.04568)
*Masahiro Kato*

Main category: stat.ML

TL;DR: Riesz回归与直接密度比估计密切相关，在平均处理效应估计等场景中，其思想与目标与最小二乘重要性拟合方法一致。


<details>
  <summary>Details</summary>
Motivation: 研究Riesz回归与直接密度比估计之间的关系，以促进两个领域方法的相互借鉴和应用扩展。

Method: 通过理论分析建立Riesz回归与直接密度比估计的等价关系，并探讨方法间的相互借鉴。

Result: 证明了Riesz回归在特定情况下与直接密度比估计等价，使得两个领域的方法可以相互导入和扩展应用。

Conclusion: Riesz回归与直接密度比估计的等价关系为两个领域提供了方法互通的桥梁，促进了理论结果和技术的共享应用。

Abstract: Riesz regression has garnered attention as a tool in debiased machine
learning for causal and structural parameter estimation (Chernozhukov et al.,
2021). This study shows that Riesz regression is closely related to direct
density-ratio estimation (DRE) in important cases, including average treat-
ment effect (ATE) estimation. Specifically, the idea and objective in Riesz
regression coincide with the one in least-squares importance fitting (LSIF,
Kanamori et al., 2009) in direct density-ratio estimation. While Riesz
regression is general in the sense that it can be applied to Riesz representer
estimation in a wide class of problems, the equivalence with DRE allows us to
directly import exist- ing results in specific cases, including
convergence-rate analyses, the selection of loss functions via
Bregman-divergence minimization, and regularization techniques for flexible
models, such as neural networks. Conversely, insights about the Riesz
representer in debiased machine learning broaden the applications of direct
density-ratio estimation methods. This paper consolidates our prior results in
Kato (2025a) and Kato (2025b).

</details>


### [12] [Simultaneous Optimization of Geodesics and Fréchet Means](https://arxiv.org/abs/2511.04301)
*Frederik Möbius Rygaard,Søren Hauberg,Steen Markvorsen*

Main category: stat.ML

TL;DR: 提出了GEORCE-FM算法，用于在黎曼流形上同时计算Fréchet均值和黎曼距离，比现有方法更快，并扩展到Finsler流形和自适应扩展以处理大数据集。


<details>
  <summary>Details</summary>
Motivation: Fréchet均值是几何统计中的核心概念，但现有计算方法需要在每次迭代中解决嵌入优化问题，计算效率低。

Method: 开发GEORCE-FM算法，在局部图表中同时计算Fréchet均值和黎曼距离，避免重复优化，并扩展到Finsler流形和自适应扩展。

Result: 理论证明GEORCE-FM具有全局收敛性和局部二次收敛性，自适应扩展在期望上收敛到Fréchet均值。实证显示在准确性和运行时间上优于基线方法。

Conclusion: GEORCE-FM算法为计算Fréchet均值提供了更高效的方法，具有理论保证和实际性能优势。

Abstract: A central part of geometric statistics is to compute the Fr\'echet mean. This
is a well-known intrinsic mean on a Riemannian manifold that minimizes the sum
of squared Riemannian distances from the mean point to all other data points.
The Fr\'echet mean is simple to define and generalizes the Euclidean mean, but
for most manifolds even minimizing the Riemannian distance involves solving an
optimization problem. Therefore, numerical computations of the Fr\'echet mean
require solving an embedded optimization problem in each iteration. We
introduce the GEORCE-FM algorithm to simultaneously compute the Fr\'echet mean
and Riemannian distances in each iteration in a local chart, making it faster
than previous methods. We extend the algorithm to Finsler manifolds and
introduce an adaptive extension such that GEORCE-FM scales to a large number of
data points. Theoretically, we show that GEORCE-FM has global convergence and
local quadratic convergence and prove that the adaptive extension converges in
expectation to the Fr\'echet mean. We further empirically demonstrate that
GEORCE-FM outperforms existing baseline methods to estimate the Fr\'echet mean
in terms of both accuracy and runtime.

</details>


### [13] [Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis](https://arxiv.org/abs/2511.04576)
*Zhuo Zhang,Xiong Xiong,Sen Zhang,Yuan Zhao,Xi Yang*

Main category: stat.ML

TL;DR: 该论文系统分析了基于机器学习的参数化偏微分方程求解方法，重点关注物理信息神经网络(PINNs)和神经算子两大范式，比较了它们在多查询场景下的计算效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在求解参数化PDE时需要为每个参数重新求解，计算成本高昂。机器学习方法特别是PINNs和神经算子能够学习解算子，在参数空间中实现泛化，显著降低多查询场景的计算成本。

Method: 批判性分析两种主要范式：(1) PINNs：将物理定律作为软约束嵌入神经网络，擅长处理稀疏数据的反问题；(2) 神经算子(如DeepONet、傅里叶神经算子)：学习无限维函数空间之间的映射，实现前所未有的泛化能力。

Result: 在流体动力学、固体力学、热传导和电磁学等领域的比较表明，神经算子相比传统求解器可以实现10^3到10^5倍的计算加速，同时保持相当的精度。

Conclusion: 建立了基于算子学习的参数化PDE求解器的统一框架，为这一快速发展的领域提供了全面的参考资源，并指出了高维参数、复杂几何和分布外泛化等关键挑战。

Abstract: PDEs arise ubiquitously in science and engineering, where solutions depend on
parameters (physical properties, boundary conditions, geometry). Traditional
numerical methods require re-solving the PDE for each parameter, making
parameter space exploration prohibitively expensive. Recent machine learning
advances, particularly physics-informed neural networks (PINNs) and neural
operators, have revolutionized parametric PDE solving by learning solution
operators that generalize across parameter spaces. We critically analyze two
main paradigms: (1) PINNs, which embed physical laws as soft constraints and
excel at inverse problems with sparse data, and (2) neural operators (e.g.,
DeepONet, Fourier Neural Operator), which learn mappings between
infinite-dimensional function spaces and achieve unprecedented generalization.
Through comparisons across fluid dynamics, solid mechanics, heat transfer, and
electromagnetics, we show neural operators can achieve computational speedups
of $10^3$ to $10^5$ times faster than traditional solvers for multi-query
scenarios, while maintaining comparable accuracy. We provide practical guidance
for method selection, discuss theoretical foundations (universal approximation,
convergence), and identify critical open challenges: high-dimensional
parameters, complex geometries, and out-of-distribution generalization. This
work establishes a unified framework for understanding parametric PDE solvers
via operator learning, offering a comprehensive, incrementally updated resource
for this rapidly evolving field

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [14] [Centralized Health and Exposomic Resource (C-HER): Analytic and AI-Ready Data for External Exposomic Research](https://arxiv.org/abs/2511.03750)
*Heidi A. Hanson,Joemy Ramsay,Josh Grant,Maggie Davis,Janet O. Agbaje,Dakotah Maguire,Jeremy Logan,Marissa Taddie,Chad Melton,Midgie MacFarland,James VanDerslice*

Main category: stat.AP

TL;DR: C-HER项目开发了一个数据工程解决方案，整合了30多个外部暴露组数据集，创建了分析就绪和AI就绪数据(AAIRD)，以克服暴露组研究中时空链接的挑战。


<details>
  <summary>Details</summary>
Motivation: 暴露组是一个概念框架，用于研究环境和遗传因素共同塑造人类健康的复杂关系。由于暴露数据的高维度、多模态数据源和不同时空尺度，现有的暴露组综合测量很少。

Method: 开发数据工程解决方案，识别、分析、空间索引和存储外部暴露组数据集，创建AAIRD。

Result: 成功整合了30多个外部暴露组数据集，提供了如何将环境数据结合来表征区域、模拟空气污染或为癌症研究提供指标的例子。

Conclusion: AAIRD的开发将使未来研究能够使用机器学习和深度学习方法生成空间和上下文暴露数据用于疾病预测。

Abstract: The Centralized Health and Exposomic Resource (C-HER) project has identified,
profiled, spatially indexed, and stored over 30 external exposomic datasets.
The resulting analytic and AI-ready data (AAIRD) provides a significant
opportunity to develop an integrated picture of the exposome for health
research. The exposome is a conceptual framework designed to guide the study of
the complex environmental and genetic factors that together shape human health.
Few composite measures of the exposome exist due to the high dimensionality of
exposure data, multimodal data sources, and varying spatiotemporal scales. We
develop a data engineering solution that overcomes the challenges of
spatio-temporal linkage in this field. We provide examples of how environmental
data can be combined to characterize a region, model air pollution, or provide
indicators for cancer research. The development of AAIRD will allow future
studies to use ML and deep learning methods to generate spatial and contextual
exposure data for disease prediction.

</details>


### [15] [Transportability of Prognostic Markers: Rethinking Common Practices through a Sufficient-Component-Cause Perspective](https://arxiv.org/abs/2511.04065)
*Mohsen Sadatsafavi,Gavin Pereira,Wenjia Chen*

Main category: stat.AP

TL;DR: 本文通过充分病因成分框架重新审视预后标志物的可迁移性，挑战了传统迁移方法的假设，提出了基于病因分布变化的透明迁移方法。


<details>
  <summary>Details</summary>
Motivation: 预后标志物在不同人群中性能变化显著，传统迁移方法（如患病率调整）依赖强假设，需要更透明的迁移框架。

Method: 使用充分病因成分框架分解风险预测的因果成分，分析不同迁移方法对病因分布稳定性的假设依赖。

Result: 数值实验表明不同可迁移性假设导致不同程度的信息损失，取决于人群间病因分布的差异程度。

Conclusion: SCC框架挑战了标志物可迁移性的常见假设和实践，提出了反映病因分布变化知识的迁移算法。

Abstract: Transportability, the ability to maintain performance across populations, is
a desirable property of of markers of clinical outcomes. However, empirical
findings indicate that markers often exhibit varying performances across
populations. For prognostic markers whose results are used to quantify of the
risk of an outcome, oftentimes a form of updating is required when the marker
is transported to populations with different disease prevalences. Here, we
revisit transportability of prognostic markers through the lens of the
foundational framework of sufficient component causes (SCC). We argue that
transporting a marker "as is" implicitly assumes predictive values are
transportable, whereas conventional prevalence-adjustment shifts the locus of
transportability to accuracy metrics (sensitivity and specificity). Using a
minimalist SCC framework that decomposes risk prediction into its causal
constituents, we show that both approaches rely on strong assumptions about the
stability of cause distributions across populations. A SCC framework instead
invites making transparent assumptions about how different causes vary across
populations, leading to different transportation methods. For example, in the
absence of any external information other than disease prevalence, a
cause-neutral perspective can assume all causes are responsible for change in
prevalence, leading to a new form of marker transportation. Numerical
experiments demonstrate that different transportability assumptions lead to
varying degrees of information loss, depending on how population differ from
each other in the distribution of causes. A SCC perspective challenges common
assumptions and practices for marker transportability, and proposes
transportation algorithms that reflect our knowledge or assumptions about how
causes vary across populations.

</details>


### [16] [Nonparametric Safety Stock Dimensioning: A Data-Driven Approach for Supply Chains of Hardware OEMs](https://arxiv.org/abs/2511.04616)
*Elvis Agbenyega,Cody Quick*

Main category: stat.AP

TL;DR: 提出了一种基于核密度估计的数据驱动安全库存计算方法，该方法放宽了需求正态分布的假设，并考虑了预测需求变异性，在模拟环境中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统安全库存计算方法假设需求呈正态分布且忽略未来需求变异性，在制造业中需求通常是非正态、间歇性和高度偏态的情况下适用性有限。

Method: 使用核密度估计分析确定每个库存项目的需求分布，将分析从历史需求变异性扩展到预测需求变异性，并通过线性优化模型确定最优安全库存配置。

Result: 在近真实库存补货模拟中，数据驱动方法优于传统方法，在较低的安全库存水平下实现了期望的服务水平。

Conclusion: 数据驱动方法能够更有效地确定安全库存，提高供应链弹性，特别适用于需求分布非正态的制造环境。

Abstract: Resilient supply chains are critical, especially for Original Equipment
Manufacturers (OEMs) that power today's digital economy. Safety Stock
dimensioning-the computation of the appropriate safety stock quantity-is one of
several mechanisms to ensure supply chain resiliency, as it protects the supply
chain against demand and supply uncertainties. Unfortunately, the major
approaches to dimensioning safety stock heavily assume that demand is normally
distributed and ignore future demand variability, limiting their applicability
in manufacturing contexts where demand is non-normal, intermittent, and highly
skewed. In this paper, we propose a data-driven approach that relaxes the
assumption of normality, enabling the demand distribution of each inventory
item to be analytically determined using Kernel Density Estimation. Also, we
extended the analysis from historical demand variability to forecasted demand
variability. We evaluated the proposed approach against a normal distribution
model in a near-world inventory replenishment simulation. Afterwards, we used a
linear optimization model to determine the optimal safety stock configuration.
The results from the simulation and linear optimization models showed that the
data-driven approach outperformed traditional approaches. In particular, the
data-driven approach achieved the desired service levels at lower safety stock
levels than the conventional approaches.

</details>


### [17] [Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling](https://arxiv.org/abs/2511.04619)
*Natalia Glazman,Jyoti Mangal,Pedro Borges,Sebastien Ourselin,M. Jorge Cardoso*

Main category: stat.AP

TL;DR: 提出了一种基于潜在变量模型的阿尔茨海默病因果发现框架，通过推断疾病伪时间轨迹来学习动态因果关系的演化。


<details>
  <summary>Details</summary>
Motivation: 大多数因果发现方法的静态图假设无法捕捉阿尔茨海默病等疾病的动态病理生理学演化过程，需要能够适应潜在疾病伪时间调制的模型。

Method: 应用现有潜在变量模型到真实AD数据，推断独立于实际年龄的疾病伪时间轨迹，然后学习因果关系的动态演化，并整合最小化的疾病无关背景知识。

Result: 伪时间在预测诊断方面优于年龄（AUC 0.82 vs 0.59），整合背景知识显著提高了图结构的准确性和方向性，揭示了新型标志物与已建立AD标志物之间的动态相互作用。

Conclusion: 该框架能够在假设被违反的情况下实现实用的因果发现，为理解阿尔茨海默病的动态病理机制提供了有效工具。

Abstract: The application of causal discovery to diseases like Alzheimer's (AD) is
limited by the static graph assumptions of most methods; such models cannot
account for an evolving pathophysiology, modulated by a latent disease
pseudotime. We propose to apply an existing latent variable model to real-world
AD data, inferring a pseudotime that orders patients along a data-driven
disease trajectory independent of chronological age, then learning how causal
relationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC
0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge
substantially improved graph accuracy and orientation. Our framework reveals
dynamic interactions between novel (NfL, GFAP) and established AD markers,
enabling practical causal discovery despite violated assumptions.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [18] [Adaptive Geometric Regression for High-Dimensional Structured Data](https://arxiv.org/abs/2511.03817)
*Pawel Gajer,Jacques Ravel*

Main category: stat.ME

TL;DR: 提出了一种用于高维结构化数据回归的几何框架，通过热扩散和响应一致性调制迭代演化几何结构，在响应平滑区域集中质量，在响应快速变化区域创建扩散屏障。


<details>
  <summary>Details</summary>
Motivation: 解决高环境维度但低内在维度数据集（如微生物组组成）分析中的基本挑战，传统方法无法捕捉底层几何结构。

Method: 从特征空间的k近邻覆盖开始，通过热扩散和响应一致性调制迭代演化几何结构，在响应平滑区域集中质量，在响应快速变化区域创建扩散屏障。

Result: 产生尊重特征空间内在几何和响应结构的条件期望估计。

Conclusion: 该方法能够有效处理高维结构化数据，同时捕捉数据的几何特性和响应模式。

Abstract: We present a geometric framework for regression on structured
high-dimensional
  data that shifts the analysis from the ambient space to a geometric object
  capturing the data's intrinsic structure. The method addresses a fundamental
  challenge in analyzing datasets with high ambient dimension but low intrinsic
  dimension, such as microbiome compositions, where traditional approaches fail
  to capture the underlying geometric structure. Starting from a k-nearest
  neighbor covering of the feature space, the geometry evolves iteratively
  through heat diffusion and response-coherence modulation, concentrating mass
  within regions where the response varies smoothly while creating diffusion
  barriers where the response changes rapidly. This iterative refinement
  produces conditional expectation estimates that respect both the intrinsic
  geometry of the feature space and the structure of the response.

</details>


### [19] [Nonparametric Modeling of Continuous-Time Markov Chains](https://arxiv.org/abs/2511.03954)
*Filippo Monti,Xiang Ji,Marc A. Suchard*

Main category: stat.ME

TL;DR: 提出了一种新的贝叶斯框架，使用高斯过程建模连续时间马尔可夫链的转移率，通过协变量改进推断并理解外部驱动因素，采用可扩展的哈密顿蒙特卡洛采样器进行高效推理。


<details>
  <summary>Details</summary>
Motivation: 连续时间马尔可夫链的转移率推断面临三个挑战：状态空间扩大时转移率数量呈二次增长、转移率间强相关性、许多转移信息不完整。现有方法仅限于线性协变量效应，无法捕捉复杂非线性关系。

Method: 使用高斯过程将协变量纳入CTMC转移率建模，采用可扩展的哈密顿蒙特卡洛采样器，结合可扩展梯度近似将计算复杂度从O(K^5)降低到O(K^2)。

Result: 在贝叶斯系统地理学推断中验证了方法的有效性，在合成和真实数据集上都表现出色，能够更充分地利用协变量信息并更准确地表征其影响。

Conclusion: 该方法能够灵活建模CTMC转移率，捕捉协变量与转移率之间的复杂非线性关系，显著提高了推断效率，为理解CTMC随机行为提供了新视角。

Abstract: Inferring the infinitesimal rates of continuous-time Markov chains (CTMCs) is
a central challenge in many scientific domains. This task is hindered by three
factors: quadratic growth in the number of rates as the CTMC state space
expands, strong dependencies among rates, and incomplete information for many
transitions. We introduce a new Bayesian framework that flexibly models the
CTMC rates by incorporating covariates through Gaussian processes (GPs). This
approach improves inference by integrating new information and contributes to
the understanding of the CTMC stochastic behavior by shedding light on
potential external drivers. Unlike previous approaches limited to linear
covariate effects, our method captures complex non-linear relationships,
enabling fuller use of covariate information and more accurate characterization
of their influence. To perform efficient inference, we employ a scalable
Hamiltonian Monte Carlo (HMC) sampler. We address the prohibitive cost of
computing the exact likelihood gradient by integrating the HMC trajectories
with a scalable gradient approximation, reducing the computational complexity
from $O(K^5)$ to $O(K^2)$, where $K$ is the number of CTMC states. Finally, we
demonstrate our method on Bayesian phylogeography inference -- a domain where
CTMCs are central -- showing effectiveness on both synthetic and real datasets.

</details>


### [20] [A Pragmatic Framework for Bayesian Utility Magnitude-Based Decisions](https://arxiv.org/abs/2511.03932)
*Will G. Hopkins*

Main category: stat.ME

TL;DR: 本文提出了一个实用的决策框架，通过结合贝叶斯后验概率和实际价值点来为干预措施计算期望效用得分，从而做出基于效用的正式决策。


<details>
  <summary>Details</summary>
Motivation: 旨在解决统计推断到实际决策的转化问题，提供一个统一、非任意的点标度来量化不同效应大小的实际价值，并允许系统性地考虑损失厌恶、副作用和实施成本等因素。

Method: 使用贝叶斯后验概率与1-9点标度（小到极大效应）相结合计算期望效用得分，包含个体响应标准差来表征个体差异，并通过敏感性分析检验系统偏差和主观输入的影响。

Result: 框架产生单一的期望效用得分，通过与用户定义的最小重要净效益比较来做出初始决策，同时提供个体受益、无效应和受害的比例来为实施决策提供背景。

Conclusion: 该框架是一个开发中的实用工具，用于基于可用统计证据进行实际决策，并结构化思考结果价值，但尚未经过实证验证。

Abstract: This article presents a pragmatic framework for making formal, utility-based
decisions from statistical inferences. The method calculates an expected
utility score for an intervention by combining Bayesian posterior probabilities
of different effect magnitudes with points representing their practical value.
A key innovation is a unified, non-arbitrary points scale (1-9 for small to
extremely large) derived from a principle linking tangible outcomes across
different effect types. This tangible scale enables a principled "trade-off"
method for including values for loss aversion, side effects, and implementation
cost. The framework produces a single, definitive expected utility score, and
the initial decision is made by comparing the magnitude of this single score to
a user-defined smallest important net benefit, a direct and intuitive
comparison made possible by the scale's tangible nature. This expected utility
decision is interpreted alongside clinical magnitude-based decision
probabilities or credible interval coverage to assess evidence strength.
Inclusion of a standard deviation representing individual responses to an
intervention (or differences between settings with meta-analytic data) allows
characterization of differences between individuals (or settings) in the
utility score expressed as proportions expected to experience benefit, a
negligible effect, and harm. These proportions provide context for the final
decision about implementation. Users must perform sensitivity analyses to
investigate the effects of systematic bias and of the subjective inputs on the
final decision. This framework, implemented in an accessible spreadsheet, has
not been empirically validated. It represents a tool in development, designed
for practical decision-making from available statistical evidence and
structured thinking about values of outcomes.

</details>


### [21] [Generative Bayesian Filtering and Parameter Learning](https://arxiv.org/abs/2511.04552)
*Edoardo Marcelli,Sean O'Hagan,Veronika Rockova*

Main category: stat.ME

TL;DR: 提出了生成贝叶斯滤波(GBF)框架，用于复杂非线性非高斯状态空间模型的后验推断，无需显式密度评估，特别适用于观测或转移分布解析不可处理的情况。


<details>
  <summary>Details</summary>
Motivation: 解决复杂非线性非高斯状态空间模型中的后验推断问题，特别是在观测或转移分布解析不可处理时，现有方法效果有限。

Method: 将生成贝叶斯计算扩展到动态设置，使用深度神经网络驱动的基于模拟的方法进行递归后验推断。引入生成-吉布斯采样器进行参数学习。

Result: 在模拟和实证研究中，包括α-稳定随机波动模型的估计，GBF在精度和鲁棒性上显著优于现有的无似然方法。

Conclusion: GBF为处理不可处理状态空间模型提供了一种强大而灵活的方法，在复杂动态系统的贝叶斯推断中表现出优越性能。

Abstract: Generative Bayesian Filtering (GBF) provides a powerful and flexible
framework for performing posterior inference in complex nonlinear and
non-Gaussian state-space models. Our approach extends Generative Bayesian
Computation (GBC) to dynamic settings, enabling recursive posterior inference
using simulation-based methods powered by deep neural networks. GBF does not
require explicit density evaluations, making it particularly effective when
observation or transition distributions are analytically intractable. To
address parameter learning, we introduce the Generative-Gibbs sampler, which
bypasses explicit density evaluation by iteratively sampling each variable from
its implicit full conditional distribution. Such technique is broadly
applicable and enables inference in hierarchical Bayesian models with
intractable densities, including state-space models. We assess the performance
of the proposed methodologies through both simulated and empirical studies,
including the estimation of $\alpha$-stable stochastic volatility models. Our
findings indicate that GBF significantly outperforms existing likelihood-free
approaches in accuracy and robustness when dealing with intractable state-space
models.

</details>


### [22] [Assessing Replicability Across Dependent Studies: A Framework for Testing Partial Conjunction Hypotheses with Application to GWAS](https://arxiv.org/abs/2511.04130)
*Monitirtha Dey,Trambak Banerjee,Prajamitra Bhuyan,Arunabha Majumdar*

Main category: stat.ME

TL;DR: 提出e-Filter方法，用于处理具有样本重叠的研究中的部分联合假设检验，在未知研究依赖性的情况下控制FWER和FDR，并在GWAS应用中显示出优越的统计功效。


<details>
  <summary>Details</summary>
Motivation: 现有部分联合假设检验方法假设研究独立，但许多现代应用（如具有样本重叠的GWAS）违反这一假设，导致I类错误膨胀。

Method: 基于e值理论，包含过滤步骤（保留最有希望的部分联合假设）和选择步骤（当e值超过选择阈值时标记为发现）。

Result: 模拟研究显示e-Filter相比竞争方法具有优异的功效提升；在LDL-C GWAS应用中，复现信号在生物学相关通路上获得更强的统计富集。

Conclusion: e-Filter是处理具有未知依赖性的研究部分联合假设检验的有效工具，在GWAS复现性研究中表现出色。

Abstract: Replicability is central to scientific progress, and the partial conjunction
(PC) hypothesis testing framework provides an objective tool to quantify it
across disciplines. Existing PC methods assume independent studies. Yet many
modern applications, such as genome-wide association studies (GWAS) with sample
overlap, violate this assumption, leading to dependence among study-specific
summary statistics. Failure to account for this dependence can drastically
inflate type I errors when combining inferences. We propose e-Filter, a
powerful procedure grounded on the theory of e-values. It involves a filtering
step that retains a set of the most promising PC hypotheses, and a selection
step where PC hypotheses from the filtering step are marked as discoveries
whenever their e-values exceed a selection threshold. We establish the validity
of e-Filter for FWER and FDR control under unknown study dependence. A
comprehensive simulation study demonstrates its excellent power gains over
competing methods. We apply e-Filter to a GWAS replicability study to identify
consistent genetic signals for low-density lipoprotein cholesterol (LDL-C).
Here, the participating studies exhibit varying levels of sample overlap,
rendering existing methods unsuitable for combining inferences. A subsequent
pathway enrichment analysis shows that e-Filter replicated signals achieve
stronger statistical enrichment on biologically relevant LDL-C pathways than
competing approaches.

</details>


### [23] [Estimation of Independent Component Analysis Systems](https://arxiv.org/abs/2511.04273)
*Vincent Starck*

Main category: stat.ME

TL;DR: 提出了一种基于特征函数的独立成分分析最优估计器，避免了数值积分和参数选择问题，具有计算可行性和渐近效率。


<details>
  <summary>Details</summary>
Motivation: 虽然基于特征函数的ICA方法理论优雅，但存在数值积分和参数选择的实现挑战，需要开发更实用的方法。

Method: 扩展先前目标函数，利用Carrasco和Florens(2000)的连续广义矩方法结果，推导出可处理形式的最优估计器。

Result: 模拟显示该方法优于高效GMM、JADE和FastICA，并在结构向量自回归估计应用中表现良好。

Conclusion: 该方法结合了特征函数方法的优势（无需高阶矩存在或参数限制），同时保持计算可行性和渐近效率，并提供有用的规范检验。

Abstract: Although approaches to Independent Component Analysis (ICA) based on
characteristic function seem theoretically elegant, they may suffer from
implementational challenges because of numerical integration steps or selection
of tuning parameters. Extending previously considered objective functions and
leveraging results from the continuum Generalized Method of Moments of Carrasco
and Florens (2000), I derive an optimal estimator that can take a tractable
form and thus bypass these concerns. The method shares advantages with
characteristic function approaches -- it does not require the existence of
higher-order moments or parametric restrictions -- while retaining
computational feasibility and asymptotic efficiency. The results are adapted to
handle a possible first step that delivers estimated sensors. Finally, a
by-product of the approach is a specification test that is valuable in many ICA
applications. The method's effectiveness is illustrated through simulations,
where the estimator outperforms efficient GMM, JADE, or FastICA, and an
application to the estimation of Structural Vector Autoregressions (SVAR), a
workhorse of the macroeconometric time series literature.

</details>


### [24] [Matrix-Variate Regression Model for Multivariate Spatio-Temporal Data](https://arxiv.org/abs/2511.04331)
*Carlos A. Ribeiro Diniz,Victor E. Lachos Olivares,Victor H. Lachos Davila*

Main category: stat.ME

TL;DR: 提出了一个矩阵变量回归模型，用于分析跨空间位置和时间观测的多变量数据，包含均值结构和可分离协方差结构，通过最大似然估计进行参数估计，并在模拟和实际农业数据中验证了模型效果。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够有效分析跨空间位置和时间观测的多变量数据的统计模型，特别是能够同时处理空间和时间依赖性的方法。

Method: 使用矩阵变量回归模型，包含均值结构（连接协变量和响应矩阵）和基于Kronecker积的可分离协方差结构，采用最大似然估计方法估计所有模型参数。

Result: 模拟研究验证了模型在不同空间分辨率下参数恢复的有效性，实际应用展示了模型在揭示农业和畜牧业生产数据的结构化时空变异模式和协变量效应方面的实用性。

Conclusion: 该矩阵变量回归模型能够有效捕捉多变量数据的时空依赖性，在模拟和实际应用中均表现出良好的性能，为分析复杂时空数据提供了实用工具。

Abstract: This paper introduces a matrix-variate regression model for analyzing
multivariate data observed across spatial locations and over time. The model's
design incorporates a mean structure that links covariates to the response
matrix and a separable covariance structure, based on a Kronecker product, to
capture spatial and temporal dependencies efficiently. We derive maximum
likelihood estimators for all model parameters. A simulation study validates
the model, showing its effectiveness in parameter recovery across different
spatial resolutions. Finally, an application to real-world data on agricultural
and livestock production from Brazilian municipalities showcases the model's
practical utility in revealing structured spatio-temporal patterns of variation
and covariate effects.

</details>


### [25] [Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures](https://arxiv.org/abs/2511.04599)
*Pawel Gajer,Jacques Ravel*

Main category: stat.ME

TL;DR: 开发了一个几何分解框架，通过两种策略在高维数据中识别子群体特定的特征-结果关联模式：梯度流分解和共单调性分解。


<details>
  <summary>Details</summary>
Motivation: 标准方法假设全局关联会错过上下文依赖模式，降低统计功效和可解释性。需要识别子群体中变化的特征-结果关联。

Method: 1) 梯度流分解：使用路径单调性验证的离散Morse理论将样本划分为结果呈现单调行为的盆地；2) 共单调性分解：基于关联结构定义样本在关联空间中的嵌入，通过双聚类识别共单调性单元和特征模块。

Result: 框架提供了在黎曼图上进行区域分析的两种策略，可独立或联合应用，并通过贝叶斯后验采样提供可信区间。

Conclusion: 该几何分解框架能够有效识别高维数据中子群体特定的特征-结果关联模式，提高统计功效和可解释性，并支持多模态数据整合。

Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns,
reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into basins where outcomes exhibit monotonic behavior.
  Co-monotonicity decomposition leverages association structure: vertex-level
  coefficients measuring directional concordance between outcome and features,
  or between feature pairs, define embeddings of samples into association
space.
  These embeddings induce Riemannian k-NN graphs on which biclustering
  identifies co-monotonicity cells (coherent regions) and feature modules. This
  extends naturally to multi-modal integration across multiple feature sets.
  Both strategies apply independently or jointly, with Bayesian posterior
  sampling providing credible intervals.

</details>


### [26] [Nonparametric Robust Comparison of Solutions under Input Uncertainty](https://arxiv.org/abs/2511.04457)
*Jaime Gonzalez-Hodar,Johannes Milz,Eunhye Song*

Main category: stat.ME

TL;DR: 提出了NIOU-C方法，在无法收集额外数据的情况下处理输入不确定性下的排序和选择问题，构建包含最优解的置信集。


<details>
  <summary>Details</summary>
Motivation: 在无法收集额外数据的情况下，处理输入不确定性对排序和选择问题的影响，确保找到最优解。

Method: 使用经验似然构建输入分布的模糊集，通过线性函数表示近似每个解的平均性能，在模糊集内评估最坏情况下的成对均值差异来构建置信集。

Result: NIOU-C比参数化方法提供更小的置信集且更频繁地包含最优解，扩展版本NIOU-C:E进一步减小了保守性。

Conclusion: NIOU-C方法在输入不确定性下有效构建置信集，扩展版本进一步提升了性能，适用于无法收集额外数据的情况。

Abstract: We study ranking and selection under input uncertainty in settings where
additional data cannot be collected. We propose the Nonparametric Input-Output
Uncertainty Comparisons (NIOU-C) procedure to construct a confidence set that
includes the optimal solution with a user-specified probability. We construct
an ambiguity set of input distributions using empirical likelihood and
approximate the mean performance of each solution using a linear functional
representation of the input distributions. By solving optimization problems
evaluating worst-case pairwise mean differences within the ambiguity set, we
build a confidence set of solutions indistinguishable from the optimum. We
characterize sample size requirements for NIOU-C to achieve the asymptotic
validity under mild conditions. Moreover, we propose an extension to NIOU-C,
NIOU-C:E, that mitigates conservatism and yields a smaller confidence set. In
numerical experiments, NIOU-C provides a smaller confidence set that includes
the optimum more frequently than a parametric procedure that takes advantage of
the parametric distribution families.

</details>


### [27] [Conditional Selective Inference for the Selected Groups in Panel Data](https://arxiv.org/abs/2511.04466)
*Chuang Wan,Jiajun Sun,Xingbai Xu*

Main category: stat.ME

TL;DR: 提出了一种针对面板数据中通过k-means聚类选择的组别斜率差异进行选择性推断的方法，解决了传统Wald检验在聚类后直接检验导致的I类错误膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 在面板数据中，当使用相同数据集进行k-means聚类识别组结构并构建检验统计量时，传统Wald检验会产生严重膨胀的I类错误概率，因为选择阶段和推断阶段之间存在依赖关系。

Method: 提出选择性推断方法，在给定选择事件的条件下进行有效推断，考虑了选择效应的影响。该方法可以扩展到检验单个协变量系数的差异，并能整合到GMM估计框架中。

Result: 模拟研究表明该方法在有限样本下具有满意的性能。应用于经济增长与CO₂排放关系的异质性分析，发现了一些新发现。

Conclusion: 该方法为面板数据中的选择性推断提供了有效解决方案，并开发了相应的R包TestHomoPanel来实现所提出的框架。

Abstract: We consider the problem of testing for differences in group-specific slopes
between the selected groups in panel data identified via k-means clustering. In
this setting, the classical Wald-type test statistic is problematic because it
produces an extremely inflated type I error probability. The underlying reason
is that the same dataset is used to identify the group structure and construct
the test statistic, simultaneously. This creates dependence between the
selection and inference stages. To address this issue, we propose a valid
selective inference approach conditional on the selection event to account for
the selection effect. We formally define the selective type I error and
describe how to efficiently compute the correct p-values for clusters obtained
using k-means clustering. Furthermore, the same idea can be extended to test
for differences in coefficients due to a single covariate and can be
incorporated into the GMM estimation framework. Simulation studies show that
our method has satisfactory finite sample performance. We apply this method to
explore the heterogeneous relationships between economic growth and the $CO_2$
emission across countries for which some new findings are discovered. An R
package TestHomoPanel is provided to implement the proposed selective inference
framework for panel data.

</details>


### [28] [A General Approach for Calibration Weighting under Missing at Random](https://arxiv.org/abs/2511.04496)
*Yonghyun Kwon,Jae Kwang Kim,Yumou Qiu*

Main category: stat.ME

TL;DR: 提出基于加权广义熵的统一校准加权方法GEC，用于处理MAR缺失数据，具有改进的稳定性和效率。该方法通过凸优化统一熵方法和广义回归加权，实现双重稳健性。


<details>
  <summary>Details</summary>
Motivation: 处理缺失数据时，现有校准加权方法在稳定性和效率方面存在局限，需要统一框架来整合熵方法和回归加权方法，并提供双重稳健性保证。

Method: 广义熵校准(GEC)将权重构建表述为凸优化问题，结合协变量平衡、倾向得分模型的去偏约束和Neyman正交约束。通过Bregman投影和几何表征实现权重优化。

Result: GEC方法在正确指定结果回归模型时可获得最优校准估计量。模拟研究表明该方法在稳定性和效率上优于现有方法。

Conclusion: GEC提供了一个统一的校准加权框架，具有双重稳健性和几何表征，能有效处理MAR缺失数据问题，并可通过高维扩展保持推断的稳健性。

Abstract: We propose a unified class of calibration weighting methods based on weighted
generalized entropy to handle missing at random (MAR) data with improved
stability and efficiency. The proposed generalized entropy calibration (GEC)
formulates weight construction as a convex optimization program that unifies
entropy-based approaches and generalized regression weighting. Double
robustness is achieved by augmenting standard covariate balancing with a
debiasing constraint tied to the propensity score model and a Neyman-orthogonal
constraint that removes first-order sensitivity to nuisance estimation.
Selection of the weights on the entropy function can lead to the optimal
calibration estimator under a correctly specified outcome regression model. The
proposed GEC weighting ha a nice geometric characterization: the GEC solution
is the Bregman projection of the initial weights onto a constraint set, which
yields a generalized Pythagorean identity and a nested decomposition that
quantifies the incremental distance paid for additional constraints. We also
develop a high-dimensional extension with soft calibration and a projection
calibration constraint that preserves doubly robust inference. Two simulation
studies are presented to compare the performance of the proposed method with
the existing methods.

</details>


### [29] [Where to Experiment? Site Selection Under Distribution Shift via Optimal Transport and Wasserstein DRO](https://arxiv.org/abs/2511.04658)
*Adam Bouyamourn*

Main category: stat.ME

TL;DR: 研究者提出基于最优传输理论的实验地点选择方法，通过最小化总体与样本协变量分布的Wasserstein距离来减少下游估计误差，并开发了对抗分布偏移的鲁棒选择策略。


<details>
  <summary>Details</summary>
Motivation: 当部署总体与观测数据存在差异时，如何选择实验地点以最小化估计误差是一个重要问题。

Method: 将实验地点选择建模为最优传输问题，开发Wasserstein距离最小化方法，并扩展为Wasserstein分布鲁棒优化来处理协变量信息的对抗扰动。

Result: 模拟和摩洛哥微贷实验重新分析显示，当协变量预测R²>0.5时，该方法优于随机和分层抽样，在中等至大规模问题、协变量对处理效应有中等预测力以及存在分布偏移时表现更佳。

Conclusion: 基于Wasserstein距离的实验地点选择方法能有效减少估计误差，特别是在协变量信息丰富和存在分布偏移的情况下。

Abstract: How should researchers select experimental sites when the deployment
population differs from observed data? I formulate the problem of experimental
site selection as an optimal transport problem, developing methods to minimize
downstream estimation error by choosing sites that minimize the Wasserstein
distance between population and sample covariate distributions. I develop new
theoretical upper bounds on PATE and CATE estimation errors, and show that
these different objectives lead to different site selection strategies. I
extend this approach by using Wasserstein Distributionally Robust Optimization
to develop a site selection procedure robust to adversarial perturbations of
covariate information: a specific model of distribution shift. I also propose a
novel data-driven procedure for selecting the uncertainty radius the
Wasserstein DRO problem, which allows the user to benchmark robustness levels
against observed variation in their data. Simulation evidence, and a reanalysis
of a randomized microcredit experiment in Morocco (Cr\'epon et al.), show that
these methods outperform random and stratified sampling of sites when
covariates have prognostic R-squared > .5, and alternative optimization methods
i) for moderate-to-large size problem instances ii) when covariates are
moderately informative about treatment effects, and iii) under induced
distribution shift.

</details>
