{"id": "2511.17725", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17725", "abs": "https://arxiv.org/abs/2511.17725", "authors": ["Jose A. Ordoñez", "Tsung-I Lin", "Victor H. Lachos", "Luis M. Castro"], "title": "A Unified Spatiotemporal Framework for Modeling Censored and Missing Areal Responses", "comment": null, "summary": "We propose a new Bayesian approach for spatiotemporal areal data with censored and missing observations. The method introduces a flexible random effect that combines the spatial dependence structures of the Simultaneous Autoregressive (SAR) and Directed Acyclic Graph Autoregressive (DAGAR) models with a temporal autoregressive component. We demonstrate that this formulation extends both spatial models into a unified spatiotemporal framework, expressing them as Gaussian Markov random fields in their innovation form. The resulting model captures spatial, temporal, and joint spatiotemporal correlations in an interpretable way. Simulation studies show that the proposed model outperforms common ad hoc imputation strategies, such as replacing censored values with the limit of detection (LOD) or imputing missing data by the sample mean. We further apply the method to carbon monoxide (CO) concentration data from Beijing's air quality network, comparing the proposed DAGAR-AR model with the traditional Conditional Autoregressive (CAR) approach. The results indicate that while the CAR model achieves slightly better predictive performance, the DAGAR-AR specification offers clearer interpretability and a more coherent representation of the spatiotemporal dependence structure."}
{"id": "2511.17870", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17870", "abs": "https://arxiv.org/abs/2511.17870", "authors": ["Robert Lund", "Xueheng Shi"], "title": "Single Changepoint Procedures", "comment": null, "summary": "Single changepoint tests have become a staple check for homogeneity of a climate time series, suggesting how climate has changed should non-homogeneity be declared. This paper summarizes the most prominent single changepoint tests used in today's climate literature, relating them to one and other and unifying their presentations. Asymptotic quantiles for the individual tests are presented. Derivations of the quantiles are given, enabling the reader to tackle cases not considered within. Our work here studies both mean and trend shifts, covering the most common settings arising in climatology. SOI and global temperature series are analyzed within to illustrate the techniques."}
{"id": "2511.17907", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17907", "abs": "https://arxiv.org/abs/2511.17907", "authors": ["Hao Wu", "Lucy Shao", "Toni Gui", "Tsungchin Wu", "Zhuochao Huang", "Shengjia Tu", "Xin Tu", "Jinyuan Liu", "Tuo Lin"], "title": "Why Is the Double-Robust Estimator for Causal Inference Not Doubly Robust for Variance Estimation?", "comment": "Hao Wu, Lucy Shao: These authors contributed equally to this work. Corresponding author: Jinyuan Liu (jinyuan.liu@vumc.org)", "summary": "Doubly robust estimators (DRE) are widely used in causal inference because they yield consistent estimators of average causal effect when at least one of the nuisance models, the propensity for treatment (exposure) or the outcome regression, is correct. However, double robustness does not extend to variance estimation; the influence-function (IF)-based variance estimator is consistent only when both nuisance parameters are correct. This raises concerns about applying DRE in practice, where model misspecification is inevitable. The recent paper by Shook-Sa et al. (2025, Biometrics, 81(2), ujaf054) demonstrated through Monte Carlo simulations that the IF-based variance estimator is biased. However, the paper's findings are empirical. The key question remains: why does the variance estimator fail in double robustness, and under what conditions do alternatives succeed, such as the ones demonstrated in Shook-Sa et al. 2025. In this paper, we develop a formal theory to clarify the efficiency properties of DRE that underlie these empirical findings. We also introduce alternative strategies, including a mixture-based framework underlying the sample-splitting and crossfitting approaches, to achieve valid inference with misspecified nuisance parameters. Our considerations are illustrated with simulation and real study data."}
{"id": "2511.17942", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.17942", "abs": "https://arxiv.org/abs/2511.17942", "authors": ["Xueheng Shi", "Robert Lund"], "title": "The Asymptotic Distribution for a Single Joinpoint Changepoint Model", "comment": null, "summary": "A single joinpoint changepoint model partitions a time series into two segments, joined at the changepoint time by constraining the estimated piecewise linear regression responses to be continuous. This manuscript derives the exact asymptotic distribution of the changepoint existence test statistic gauging whether or not a second segment is necessary. The identified asymptotic distribution, a supremum of a Gaussian process over the unit interval, is rather unwieldy. The work presented here provides the result and its derivation; quantiles of the asymptotic distribution are presented for the user. This addresses a subtle gap in the changepoint literature."}
{"id": "2511.17816", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17816", "abs": "https://arxiv.org/abs/2511.17816", "authors": ["Jose R. Palacio", "Katherine B. Ensor", "Sallie A. Keller", "Rebecca Schneider", "Kaavya Domakonda", "Loren Hopkins", "Lauren B. Stadler"], "title": "Inferring Transmission Dynamics of Respiratory Syncytial Virus from Houston Wastewater", "comment": "17 pages (including Appendix, pp. 15-18), 6 figures", "summary": "Wastewater-based epidemiology (WBE) is an effective tool for tracking community circulation of respiratory viruses. We address estimating the effective reproduction number ($R_t$) and the relative number of infections from wastewater viral load. Using weekly Houston data on respiratory syncytial virus (RSV), we implement a parsimonious Bayesian renewal model that links latent infections to measured viral load through biologically motivated generation and shedding kernels. The framework yields estimates of $R_t$ and relative infections, enabling a coherent interpretation of transmission timing and phase. We compare two input strategies-(i) raw viral-load measurements with a log-scale standard deviation, and (ii) state-space-filtered load estimates with time-varying variances-and find no practically meaningful differences in inferred trajectories or peak timing. Given this equivalence, we report the filtered input as a pragmatic default because it embeds week-specific variances while leaving epidemiological conclusions unchanged."}
{"id": "2511.17698", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17698", "abs": "https://arxiv.org/abs/2511.17698", "authors": ["Nawfel Mechiche-Alami", "Eduardo Rodriguez", "Jose M. Cardemil", "Enrique Lopez Droguett"], "title": "Quantum Fourier Transform Based Kernel for Solar Irrandiance Forecasting", "comment": "33 pages, 13 figures", "summary": "This study proposes a Quantum Fourier Transform (QFT)-enhanced quantum kernel for short-term time-series forecasting. Each signal is windowed, amplitude-encoded, transformed by a QFT, then passed through a protective rotation layer to avoid the QFT/QFT adjoint cancellation; the resulting kernel is used in kernel ridge regression (KRR). Exogenous predictors are incorporated by convexly fusing feature-specific kernels. On multi-station solar irradiance data across Koppen climate classes, the proposed kernel consistently improves median R2 and nRMSE over reference classical RBF and polynomials kernels, while also reducing bias (nMBE); complementary MAE/ERMAX analyses indicate tighter average errors with remaining headroom under sharp transients. For both quantum and classical models, the only tuned quantities are the feature-mixing weights and the KRR ridge alpha; classical hyperparameters (gamma, r, d) are fixed, with the same validation set size for all models. Experiments are conducted on a noiseless simulator (5 qubits; window length L=32). Limitations and ablations are discussed, and paths toward NISQ execution are outlined."}
{"id": "2511.19397", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.19397", "abs": "https://arxiv.org/abs/2511.19397", "authors": ["Jan de Leeuw"], "title": "Metric/Nonmetric Elastic MDS", "comment": null, "summary": "We present R and C implementations for metric (ratio) and non-metric (ordinal) versions of Elastic MDS, the multidimensional scaling technique proposed by McGee (1966). The R and C versions are compared for speed, with the C version anywhere from 15 to 100 times as fast as the R version."}
{"id": "2511.18030", "categories": ["stat.ME", "cs.AI", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.18030", "abs": "https://arxiv.org/abs/2511.18030", "authors": ["O. Debeaupuis"], "title": "Hierarchical biomarker thresholding: a model-agnostic framework for stability", "comment": null, "summary": "Many biomarker pipelines require patient-level decisions aggregated from instance-level (cell/patch) scores. Thresholds tuned on pooled instances often fail across sites due to hierarchical dependence, prevalence shift, and score-scale mismatch. We present a selection-honest framework for hierarchical thresholding that makes patient-level decisions reproducible and more defensible. At its core is a risk decomposition theorem for selection-honest thresholds. The theorem separates contributions from (i) internal fit and patient-level generalization, (ii) operating-point shift reflecting prevalence and shape changes, and (iii) a stability term that penalizes sensitivity to threshold perturbations. The stability component is computable via patient-block bootstraps mapped through a monotone modulus of risk. This framework is model-agnostic, reconciles heterogeneous decision rules on a quantile scale, and yields monotone-invariant ensembles and reportable diagnostics (e.g. flip-rate, operating-point shift)."}
{"id": "2511.18030", "categories": ["stat.ME", "cs.AI", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.18030", "abs": "https://arxiv.org/abs/2511.18030", "authors": ["O. Debeaupuis"], "title": "Hierarchical biomarker thresholding: a model-agnostic framework for stability", "comment": null, "summary": "Many biomarker pipelines require patient-level decisions aggregated from instance-level (cell/patch) scores. Thresholds tuned on pooled instances often fail across sites due to hierarchical dependence, prevalence shift, and score-scale mismatch. We present a selection-honest framework for hierarchical thresholding that makes patient-level decisions reproducible and more defensible. At its core is a risk decomposition theorem for selection-honest thresholds. The theorem separates contributions from (i) internal fit and patient-level generalization, (ii) operating-point shift reflecting prevalence and shape changes, and (iii) a stability term that penalizes sensitivity to threshold perturbations. The stability component is computable via patient-block bootstraps mapped through a monotone modulus of risk. This framework is model-agnostic, reconciles heterogeneous decision rules on a quantile scale, and yields monotone-invariant ensembles and reportable diagnostics (e.g. flip-rate, operating-point shift)."}
{"id": "2511.18844", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.18844", "abs": "https://arxiv.org/abs/2511.18844", "authors": ["Iona Ann Sebastian", "S. M. Sunoj"], "title": "Fractional cumulative Residual Inaccuracy in the Quantile Framework and its Appications", "comment": null, "summary": "Fractional cumulative residual inaccuracy (FCRI) measure allows to determine regions of discrepancy between systems, depending on their respective fractional and chaotic map parameters. Most of the theoretical results and applications related to the FCRI of the lifetime random variable are based on the distribution function approach. However, there are situations in which the distribution function may not be available in explicit form but has a closed-form quantile function (QF), an alternative method of representing a probability distribution. Motivated by these, the present study is devoted to introduce a quantile-based FCRI and study its various properties. We also deal with non-parametric estimation of quantile-based FCRI and examine its validity using simulation studies and illustrate its usefulness in measuring the discrepancy between chaotic systems and in measuring the discrepancy in two different time regimes using Nifty 50 dataset."}
{"id": "2511.17721", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17721", "abs": "https://arxiv.org/abs/2511.17721", "authors": ["Shreya Sinha-Roy", "Richard G. Everitt", "Christian P. Robert", "Ritabrata Dutta"], "title": "Prequential posteriors", "comment": null, "summary": "Data assimilation is a fundamental task in updating forecasting models upon observing new data, with applications ranging from weather prediction to online reinforcement learning. Deep generative forecasting models (DGFMs) have shown excellent performance in these areas, but assimilating data into such models is challenging due to their intractable likelihood functions. This limitation restricts the use of standard Bayesian data assimilation methodologies for DGFMs. To overcome this, we introduce prequential posteriors, based upon a predictive-sequential (prequential) loss function; an approach naturally suited for temporally dependent data which is the focus of forecasting tasks. Since the true data-generating process often lies outside the assumed model class, we adopt an alternative notion of consistency and prove that, under mild conditions, both the prequential loss minimizer and the prequential posterior concentrate around parameters with optimal predictive performance. For scalable inference, we employ easily parallelizable wastefree sequential Monte Carlo (SMC) samplers with preconditioned gradient-based kernels, enabling efficient exploration of high-dimensional parameter spaces such as those in DGFMs. We validate our method on both a synthetic multi-dimensional time series and a real-world meteorological dataset; highlighting its practical utility for data assimilation for complex dynamical systems."}
{"id": "2511.18035", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18035", "abs": "https://arxiv.org/abs/2511.18035", "authors": ["Giacomo Iannucci", "Petros Barmpounakis", "Alexandros Beskos", "Nikolaos Demiris"], "title": "On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19", "comment": "Submitted to Statistics and Computing. Approx. 26 pages, 10 figures", "summary": "This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies."}
{"id": "2511.18583", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.18583", "abs": "https://arxiv.org/abs/2511.18583", "authors": ["Valentin Roth", "Marco Avella-Medina"], "title": "Differential privacy with dependent data", "comment": null, "summary": "Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \\textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \\iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\\textit{item-level}) and \\textit{user-level} DP estimation of a mean $μ\\in \\R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \\iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \\textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data."}
{"id": "2511.18035", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18035", "abs": "https://arxiv.org/abs/2511.18035", "authors": ["Giacomo Iannucci", "Petros Barmpounakis", "Alexandros Beskos", "Nikolaos Demiris"], "title": "On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19", "comment": "Submitted to Statistics and Computing. Approx. 26 pages, 10 figures", "summary": "This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies."}
{"id": "2511.19039", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19039", "abs": "https://arxiv.org/abs/2511.19039", "authors": ["Cassandra C. Chou", "Scott L. Zeger", "Benjamin Q. Huynh"], "title": "Validity in machine learning for extreme event attribution", "comment": null, "summary": "Extreme event attribution (EEA), an approach for assessing the extent to which disasters are caused by climate change, is crucial for informing climate policy and legal proceedings. Machine learning is increasingly used for EEA by modeling rare weather events otherwise too complex or computationally intensive to model using traditional simulation methods. However, the validity of using machine learning in this context remains unclear, particularly as high-stakes machine learning applications in general are criticized for inherent bias and lack of robustness. Here we use machine learning and simulation analyses to evaluate EEA in the context of California wildfire data from 2003-2020. We identify three major threats to validity: (1) individual event attribution estimates are highly sensitive to algorithmic design choices; (2) common performance metrics like area under the ROC curve or Brier score are not strongly correlated with attribution error, facilitating suboptimal model selection; and (3) distribution shift -- changes in temperature across climate scenarios -- substantially degrades predictive performance. To address these challenges, we propose a more valid and robust attribution analysis based on aggregate machine learning estimates, using an additional metric -- mean calibration error -- to assess model performance, and using subgroup and propensity diagnostics to assess distribution shift."}
{"id": "2511.17783", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17783", "abs": "https://arxiv.org/abs/2511.17783", "authors": ["Jony Karki", "Dongzhou Huang", "Yunpeng Zhao"], "title": "Variational Estimators for Node Popularity Models", "comment": null, "summary": "Node popularity is recognized as a key factor in modeling real-world networks, capturing heterogeneity in connectivity across communities. This concept is equally important in bipartite networks, where nodes in different partitions may exhibit varying popularity patterns, motivating models such as the Two-Way Node Popularity Model (TNPM). Existing methods, such as the Two-Stage Divided Cosine (TSDC) algorithm, provide a scalable estimation approach but may have limitations in terms of accuracy or applicability across different types of networks. In this paper, we develop a computationally efficient and theoretically justified variational expectation-maximization (VEM) framework for the TNPM. We establish label consistency for the estimated community assignments produced by the proposed variational estimator in bipartite networks. Through extensive simulation studies, we show that our method achieves superior estimation accuracy across a range of bipartite as well as undirected networks compared to existing algorithms. Finally, we evaluate our method on real-world bipartite and undirected networks, further demonstrating its practical effectiveness and robustness."}
{"id": "2511.18750", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.18750", "abs": "https://arxiv.org/abs/2511.18750", "authors": ["Samya Praharaj", "Koulik Khamaru"], "title": "On Instability of Minimax Optimal Optimism-Based Bandit Algorithms", "comment": null, "summary": "Statistical inference from data generated by multi-armed bandit (MAB) algorithms is challenging due to their adaptive, non-i.i.d. nature. A classical manifestation is that sample averages of arm rewards under bandit sampling may fail to satisfy a central limit theorem. Lai and Wei's stability condition provides a sufficient, and essentially necessary criterion, for asymptotic normality in bandit problems. While the celebrated Upper Confidence Bound (UCB) algorithm satisfies this stability condition, it is not minimax optimal, raising the question of whether minimax optimality and statistical stability can be achieved simultaneously. In this paper, we analyze the stability properties of a broad class of bandit algorithms that are based on the optimism principle. We establish general structural conditions under which such algorithms violate the Lai-Wei stability criterion. As a consequence, we show that widely used minimax-optimal UCB-style algorithms, including MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH, are unstable. We further complement our theoretical results with numerical simulations demonstrating that, in all these cases, the sample means fail to exhibit asymptotic normality.\n  Overall, our findings suggest a fundamental tension between stability and minimax optimal regret, raising the question of whether it is possible to design bandit algorithms that achieve both. Understanding whether such simultaneously stable and minimax optimal strategies exist remains an important open direction."}
{"id": "2511.18065", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18065", "abs": "https://arxiv.org/abs/2511.18065", "authors": ["Cheng Peng"], "title": "Sequential Bootstrap for Out-of-Bag Error Estimation: A Simulation-Based Replication and Stability-Oriented Refinement", "comment": "25 pages, 0 figures, 16 tables, R code available upon request", "summary": "Bootstrap resampling is the foundation of many ensemble learning methods, and out-of-bag (OOB) error estimation is the most widely used internal measure of generalization performance. In the standard multinomial bootstrap, the number of distinct observations in each resample is random. Although this source of variability exists, it has rarely been studied in isolation to understand how much it affects OOB-based quantities. To address this gap, we investigate Sequential Bootstrap, a resampling method that forces every bootstrap replicate to contain the same number of distinct observations, and treat it as a controlled modification of the classical bootstrap within the OOB framework. We reproduce Breiman's five original OOB experiments on both synthetic and real-world datasets, repeating all analyses across many different random seeds. Our results show that switching from the classical bootstrap to Sequential Bootstrap leaves accuracy-related metrics essentially unchanged, but yields measurable and data-dependent reductions in several variance-related measures. Therefore, Sequential Bootstrap should not be viewed as a new method for improving predictive performance, but rather as a tool for understanding how randomness in the number of distinct samples contributes to the variance of OOB estimators. This work provides a reproducible setting for studying the statistical properties of resampling-based ensemble estimators and offers empirical evidence that may support future theoretical work on variance decomposition in bootstrap-based systems."}
{"id": "2511.19151", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19151", "abs": "https://arxiv.org/abs/2511.19151", "authors": ["Jacob Martin", "Carlo Giovanni Camarda"], "title": "Modeling smooth and localized mortality patterns across age, time, and space to uncover small-area inequalities", "comment": null, "summary": "Small-area mortality estimation is inherently difficult, as random fluctuations from low death counts can obscure real geographic differences. We introduce a flexible model that borrows strength across age, space, and time to estimate mortality schedules and trends in very small populations. The approach ensures smooth patterns across these dimensions while allowing localized breaks from the spatial structure, capturing broad trajectories as well as sharp local contrasts. We implement our model within a Penalized Spline framework and estimate it using Generalized Linear Array Model techniques, resulting in a computationally fast, interpretable, and parsimonious method. Crucially, it can readily incorporate sudden mortality shocks, such as the Covid-19 pandemic, making it highly versatile for real-world demographic and epidemiological challenges. We demonstrate its application by estimating life expectancy and age-specific mortality inequalities in over 4,800 small areas across the Greater London Authority from 2002 to 2024."}
{"id": "2511.18060", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18060", "abs": "https://arxiv.org/abs/2511.18060", "authors": ["Francesca Romana Crucinio", "Sahani Pathiraja"], "title": "An operator splitting analysis of Wasserstein--Fisher--Rao gradient flows", "comment": null, "summary": "Wasserstein-Fisher-Rao (WFR) gradient flows have been recently proposed as a powerful sampling tool that combines the advantages of pure Wasserstein (W) and pure Fisher-Rao (FR) gradient flows. Existing algorithmic developments implicitly make use of operator splitting techniques to numerically approximate the WFR partial differential equation, whereby the W flow is evaluated over a given step size and then the FR flow (or vice versa). This works investigates the impact of the order in which the W and FR operator are evaluated and aims to provide a quantitative analysis. Somewhat surprisingly, we show that with a judicious choice of step size and operator ordering, the split scheme can converge to the target distribution faster than the exact WFR flow (in terms of model time). We obtain variational formulae describing the evolution over one time step of both sequential splitting schemes and investigate in which settings the W-FR split should be preferred to the FR-W split. As a step towards this goal we show that the WFR gradient flow preserves log-concavity and obtain the first sharp decay bound for WFR."}
{"id": "2511.19375", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.19375", "abs": "https://arxiv.org/abs/2511.19375", "authors": ["Chifeng Shen", "Yuejiao Fu", "Xiaoping Shi", "Michael Chen"], "title": "Product Depth for Temporal Point Processes Observed Only Up to the First k Events", "comment": "27 pages, 12 figures", "summary": "Temporal point processes (TPPs) model the timing of discrete events along a timeline and are widely used in fields such as neuroscience and fi- nance. Statistical depth functions are powerful tools for analyzing centrality and ranking in multivariate and functional data, yet existing depth notions for TPPs remain limited. In this paper, we propose a novel product depth specifically designed for TPPs observed only up to the first k events. Our depth function comprises two key components: a normalized marginal depth, which captures the temporal distribution of the final event, and a conditional depth, which characterizes the joint distribution of the preceding events. We establish its key theoretical properties and demonstrate its practical utility through simulation studies and real data applications."}
{"id": "2511.18094", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.18094", "abs": "https://arxiv.org/abs/2511.18094", "authors": ["Daijiro Kabata", "Takumi Imai"], "title": "A sensitivity analysis for non-inferiority studies with non-randomised data", "comment": "22 pages, 2 figures, 1 table", "summary": "Background: Non-inferiority studies based on non-randomised data are increasingly used in clinical research but remain prone to unmeasured confounding. The classical E-value offers a simple way to quantify such bias but has been applied almost exclusively with respect to the statistical null. We reformulated the E-value framework to make explicit its applicability to predefined clinical margins, thereby extending its utility to non-inferiority analyses.\n  Development: Using the bias-factor formulation by Ding and VanderWeele, we defined the non-inferiority E-value as the minimum strength of association that an unmeasured confounder would need with both treatment and outcome, on the risk-ratio scale, to move the 95% confidence-limit estimate to the prespecified non-inferiority margin.\n  Application: This approach was applied to three observational studies and one single-arm trial with external controls to illustrate interpretation and range. The resulting non-inferiority E-values for the confidence limits varied from about one to three, depending on design and findings. In the single-arm trial, a large gap between the confidence-limit and point-estimate NIEs reflected small sample size and wide confidence intervals, highlighting that both should be reported for a balanced assessment of robustness.\n  Conclusion: This study reformulates the E-value to focus on clinically meaningful margins rather than the statistical null, enabling its application to non-inferiority analyses. Although the non-inferiority E-value inherits the limitations of the original method and cannot address all bias sources, it offers a transparent framework for interpreting non-randomised evidence and for generating insights that inform the design of future, more definitive randomised controlled trials."}
{"id": "2511.17870", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.17870", "abs": "https://arxiv.org/abs/2511.17870", "authors": ["Robert Lund", "Xueheng Shi"], "title": "Single Changepoint Procedures", "comment": null, "summary": "Single changepoint tests have become a staple check for homogeneity of a climate time series, suggesting how climate has changed should non-homogeneity be declared. This paper summarizes the most prominent single changepoint tests used in today's climate literature, relating them to one and other and unifying their presentations. Asymptotic quantiles for the individual tests are presented. Derivations of the quantiles are given, enabling the reader to tackle cases not considered within. Our work here studies both mean and trend shifts, covering the most common settings arising in climatology. SOI and global temperature series are analyzed within to illustrate the techniques."}
{"id": "2511.18141", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18141", "abs": "https://arxiv.org/abs/2511.18141", "authors": ["Lucas P. Amaral", "Luben M. C. Cabezas", "Thiago R. Ramos", "Gustavo H. G. A. Pereira"], "title": "Conformal Prediction for Compositional Data", "comment": "18 pages, 4 figures", "summary": "In this work, we propose a set of conformal prediction procedures tailored to compositional responses, where outcomes are proportions that must be positive and sum to one. Building on Dirichlet regression, we introduce a split conformal approach based on quantile residuals and a highest-density region strategy that combines a fast coordinate-floor approximation with an internal grid refinement to restore sharpness. Both constructions are model-agnostic at the conformal layer and guarantee finite-sample marginal coverage under exchangeability, while respecting the geometry of the simplex. A comprehensive Monte Carlo study spanning homoscedastic and heteroscedastic designs shows that the quantile residual and grid-refined HDR methods achieve empirical coverage close to the nominal 90\\% level and produce substantially narrower regions than the coordinate-floor approximation, which tends to be conservative. We further demonstrate the methods on household budget shares from the BudgetItaly dataset, using standardized socioeconomic and price covariates with a train, calibration, and test split. In this application, the grid-refined HDR attains coverage closest to the target with the smallest average widths, closely followed by the quantile residual approach, while the simple triangular HDR yields wider, less informative sets. Overall, the results indicate that conformal prediction on the simplex can be both calibrated and efficient, providing practical uncertainty quantification for compositional prediction tasks."}
{"id": "2511.19404", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.19404", "abs": "https://arxiv.org/abs/2511.19404", "authors": ["Zikai Shen", "Zonghao Chen", "Dimitri Meunier", "Ingo Steinwart", "Arthur Gretton", "Zhu Li"], "title": "Nonparametric Instrumental Variable Regression with Observed Covariates", "comment": null, "summary": "We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O."}
{"id": "2511.18106", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18106", "abs": "https://arxiv.org/abs/2511.18106", "authors": ["Hou Jian", "Meng Tan", "Tian Maozai"], "title": "Sparse-Smooth Spatially Varying Coefficient Quantile Regression", "comment": "33 pages, 6 figures, 6 tables", "summary": "We develop a convex framework for spatially varying coefficient quantile regression that, for each predictor, separates a location-invariant \\emph{global} effect from a \\emph{spatial deviation}. An adaptive group penalty selects whether a predictor varies over space, while a graph\\textendash Laplacian quadratic promotes spatial continuity of the deviations on irregular networks. The formulation is identifiable via degree-weighted centering and scales with sparse linear algebra. We provide two practical solvers\\textemdash an ADMM algorithm with closed-form proximal maps for the check loss and a smoothed proximal-gradient scheme based on the Moreau envelope\\textemdash together with implementation guidance (projection for identifiability, stopping diagnostics, and preconditioning). Under mild conditions on the sampling design, covariates, error density, and graph geometry, we establish selection consistency for the deviation groups, mean-squared error bounds that balance Laplacian bias and stochastic variability, and root-\\(n\\) asymptotic normality for the global coefficients with an oracle property. Simulations mimicking air-pollution applications demonstrate accurate recovery of global vs.\\ local effects and competitive predictive performance under heteroskedastic, heavy-tailed noise. We discuss graph construction, spatially blocked cross-validation (to prevent leakage), and options for robust standard errors under spatial dependence."}
{"id": "2511.18094", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.18094", "abs": "https://arxiv.org/abs/2511.18094", "authors": ["Daijiro Kabata", "Takumi Imai"], "title": "A sensitivity analysis for non-inferiority studies with non-randomised data", "comment": "22 pages, 2 figures, 1 table", "summary": "Background: Non-inferiority studies based on non-randomised data are increasingly used in clinical research but remain prone to unmeasured confounding. The classical E-value offers a simple way to quantify such bias but has been applied almost exclusively with respect to the statistical null. We reformulated the E-value framework to make explicit its applicability to predefined clinical margins, thereby extending its utility to non-inferiority analyses.\n  Development: Using the bias-factor formulation by Ding and VanderWeele, we defined the non-inferiority E-value as the minimum strength of association that an unmeasured confounder would need with both treatment and outcome, on the risk-ratio scale, to move the 95% confidence-limit estimate to the prespecified non-inferiority margin.\n  Application: This approach was applied to three observational studies and one single-arm trial with external controls to illustrate interpretation and range. The resulting non-inferiority E-values for the confidence limits varied from about one to three, depending on design and findings. In the single-arm trial, a large gap between the confidence-limit and point-estimate NIEs reflected small sample size and wide confidence intervals, highlighting that both should be reported for a balanced assessment of robustness.\n  Conclusion: This study reformulates the E-value to focus on clinically meaningful margins rather than the statistical null, enabling its application to non-inferiority analyses. Although the non-inferiority E-value inherits the limitations of the original method and cannot address all bias sources, it offers a transparent framework for interpreting non-randomised evidence and for generating insights that inform the design of future, more definitive randomised controlled trials."}
{"id": "2511.18167", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.18167", "abs": "https://arxiv.org/abs/2511.18167", "authors": ["Tianqi Qiao", "Marie Maros"], "title": "Sparse Polyak with optimal thresholding operators for high-dimensional M-estimation", "comment": null, "summary": "We propose and analyze a variant of Sparse Polyak for high dimensional M-estimation problems. Sparse Polyak proposes a novel adaptive step-size rule tailored to suitably estimate the problem's curvature in the high-dimensional setting, guaranteeing that the algorithm's performance does not deteriorate when the ambient dimension increases. However, convergence guarantees can only be obtained by sacrificing solution sparsity and statistical accuracy. In this work, we introduce a variant of Sparse Polyak that retains its desirable scaling properties with respect to the ambient dimension while obtaining sparser and more accurate solutions."}
{"id": "2511.18111", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18111", "abs": "https://arxiv.org/abs/2511.18111", "authors": ["Ayumi Mutoh", "Annie S. Booth", "Jonathan W. Stallrich"], "title": "Revisiting Penalized Likelihood Estimation for Gaussian Processes", "comment": "14 pages, 8 figures, 3 tables", "summary": "Gaussian processes (GPs) are popular as nonlinear regression models for expensive computer simulations, yet GP performance relies heavily on estimation of unknown covariance parameters. Maximum likelihood estimation (MLE) is common, but it can be plagued by numerical issues in small data settings. The addition of a nugget helps but is not a cure-all. Penalized likelihood methods may improve upon traditional MLE, but their success depends on tuning parameter selection. We introduce a new cross-validation (CV) metric called ``decorrelated prediction error'' (DPE), within the penalized likelihood framework for GPs. Inspired by the Mahalanobis distance, DPE provides more consistent and reliable tuning parameter selection than traditional metrics like prediction error, particularly for $K$-fold CV. Our proposed metric performs comparably to standard MLE when penalization is unnecessary and outperforms traditional tuning parameter selection metrics in scenarios where regularization is beneficial, especially under the one-standard error rule."}
{"id": "2511.18948", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.18948", "abs": "https://arxiv.org/abs/2511.18948", "authors": ["Ruilin Bai", "Bo Chen"], "title": "X-chromosome Multilocus Association Studies for Common and Rare Variants", "comment": null, "summary": "X-chromosome association study has specific model uncertainty challenges, such as unknown X-chromosome inactivation status and baseline allele, and considering nonadditive and gene-sex interaction effects in the analysis or not. Although these challenges have been answered for single-locus X-chromosome variants, it remains unclear how to properly perform multilocus association studies when above uncertainties are present. We first carefully investigate the inferential consequences of these uncertainties on existing multilocus association analysis methods, and then propose a theoretically justified framework to analyze multilocus X-chromosome variants while all the uncertainty issues are addressed. We provide separate solutions for common and rare variants, and simulation results show that our solutions are overall more powerful than existing multilocus methods which were proposed to analyze autosomal variants. We finally provide supporting evidences of our approach by revisiting some published X-chromosome association studies."}
{"id": "2511.18199", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18199", "abs": "https://arxiv.org/abs/2511.18199", "authors": ["Genesis Hang", "Annie Chen", "Hope Neveux", "Matthew K. Nock", "Yaniv Yacoby"], "title": "Improving Forecasts of Suicide Attempts for Patients with Little Data", "comment": "Accepted at the TS4H Workshop at NeurIPS 2025", "summary": "Ecological Momentary Assessment provides real-time data on suicidal thoughts and behaviors, but predicting suicide attempts remains challenging due to their rarity and patient heterogeneity. We show that single models fit to all patients perform poorly, while individualized models improve performance but still overfit to patients with limited data. To address this, we introduce Latent Similarity Gaussian Processes (LSGPs) to capture patient heterogeneity, enabling those with little data to leverage similar patients' trends. Preliminary results show promise: even without kernel-design, we outperform all but one baseline while offering a new understanding of patient similarity."}
{"id": "2511.18201", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18201", "abs": "https://arxiv.org/abs/2511.18201", "authors": ["Rodrigo de Souza Bulhões", "Marina Silva Paez", "Dani Gamerman"], "title": "Spatial deformation in a Bayesian spatiotemporal model for incomplete matrix-variate responses", "comment": "Submitted to Environmental and Ecological Statistics", "summary": "In this paper, we propose a flexible matrix-variate spatiotemporal model for analyzing multiple response variables observed at spatially distributed locations over time. Our approach relaxes the restrictive assumption of spatial isotropy, which is often unrealistic in environmental and ecological processes. We adopt a deformation-based method that allows the covariance structure to adapt to directional patterns and nonstationary behavior in space. Temporal dynamics are incorporated through dynamic linear models within a fully Bayesian framework, ensuring coherent uncertainty propagation and efficient state-space inference. Additionally, we introduce a strategy for handling missing observations across different variables, preserving the joint data structure without discarding entire time points or stations. Through a simulation study and an application to real-world air quality monitoring data, we demonstrate that incorporating spatial deformation substantially improves interpolation accuracy in anisotropic scenarios while maintaining competitive performance under near-isotropy. The proposed methodology provides a general and computationally tractable framework for multivariate spatiotemporal modeling with incomplete data."}
{"id": "2511.19075", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19075", "abs": "https://arxiv.org/abs/2511.19075", "authors": ["Emanuele Pardini", "Katerina Papagiannouli"], "title": "Structured Matching via Cost-Regularized Unbalanced Optimal Transport", "comment": null, "summary": "Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches."}
{"id": "2511.18464", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18464", "abs": "https://arxiv.org/abs/2511.18464", "authors": ["Jiayi Guo", "Zijun Gao"], "title": "Reliable Selection of Heterogeneous Treatment Effect Estimators", "comment": null, "summary": "We study the problem of selecting the best heterogeneous treatment effect (HTE) estimator from a collection of candidates in settings where the treatment effect is fundamentally unobserved. We cast estimator selection as a multiple testing problem and introduce a ground-truth-free procedure based on a cross-fitted, exponentially weighted test statistic. A key component of our method is a two-way sample splitting scheme that decouples nuisance estimation from weight learning and ensures the stability required for valid inference. Leveraging a stability-based central limit theorem, we establish asymptotic familywise error rate control under mild regularity conditions. Empirically, our procedure provides reliable error control while substantially reducing false selections compared with commonly used methods across ACIC 2016, IHDP, and Twins benchmarks, demonstrating that our method is feasible and powerful even without ground-truth treatment effects."}
{"id": "2511.18237", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18237", "abs": "https://arxiv.org/abs/2511.18237", "authors": ["Sijie Zheng", "Fandong Meng", "Jie Zhou"], "title": "Efficient Covariance Estimation for Sparsified Functional Data", "comment": null, "summary": "Motivated by recent work involving the analysis of leveraging spatial correlations in sparsified mean estimation, we present a novel procedure for constructing covariance estimator. The proposed Random-knots (Random-knots-Spatial) and B-spline (Bspline-Spatial) estimators of the covariance function are computationally efficient. Asymptotic pointwise of the covariance are obtained for sparsified individual trajectories under some regularity conditions. Our proposed nonparametric method well perform the functional principal components analysis for the case of sparsified data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. Theoretical results are illustrated with Monte Carlo simulation experiments. Finally, we cluster multi-domain data by replacing the covariance function with our proposed covariance estimator during PCA."}
{"id": "2511.19234", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19234", "abs": "https://arxiv.org/abs/2511.19234", "authors": ["Claudia Collarin", "Matteo Fasiolo", "Yannig Goude", "Simon Wood"], "title": "Integrating Complex Covariate Transformations in Generalized Additive Models", "comment": null, "summary": "Transformations of covariates are widely used in applied statistics to improve interpretability and to satisfy assumptions required for valid inference. More broadly, feature engineering encompasses a wider set of practices aimed at enhancing predictive performance, and is typically performed as part of a data pre-processing step. In contrast, this paper integrates a substantial component of the feature engineering process directly into the modelling stage. This is achieved by introducing a novel general framework for embedding interpretable covariate transformations within multi-parameter Generalised Additive Models (GAMs). Our framework accommodates any sufficiently differentiable scalar-valued transformation of potentially high-dimensional and complex covariates. These transformations are treated as integral model components, with their parameters estimated jointly with regression coefficients via maximum a posteriori (MAP) methods, and joint uncertainty quantified via approximate Bayesian techniques. Smoothing parameters are selected in an empirical Bayes framework using a Laplace approximation to the marginal likelihood, supported by efficient computation based on implicit differentiation methods. We demonstrate the flexibility and practical value of the proposed methodology through applications to forecasting electricity net-demand in Great Britain and to modelling house prices in London. The proposed methods are implemented by the gamFactory R package, available at https://github.com/mfasiolo/gamFactory."}
{"id": "2511.18530", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18530", "abs": "https://arxiv.org/abs/2511.18530", "authors": ["Alexander G. Reisach", "Olivier Collier", "Alex Luedtke", "Antoine Chambaz"], "title": "Transforming Conditional Density Estimation Into a Single Nonparametric Regression Task", "comment": null, "summary": "We propose a way of transforming the problem of conditional density estimation into a single nonparametric regression task via the introduction of auxiliary samples. This allows leveraging regression methods that work well in high dimensions, such as neural networks and decision trees. Our main theoretical result characterizes and establishes the convergence of our estimator to the true conditional density in the data limit. We develop condensité, a method that implements this approach. We demonstrate the benefit of the auxiliary samples on synthetic data and showcase that condensité can achieve good out-of-the-box results. We evaluate our method on a large population survey dataset and on a satellite imaging dataset. In both cases, we find that condensité matches or outperforms the state of the art and yields conditional densities in line with established findings in the literature on each dataset. Our contribution opens up new possibilities for regression-based conditional density estimation and the empirical results indicate strong promise for applied research."}
{"id": "2511.18432", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18432", "abs": "https://arxiv.org/abs/2511.18432", "authors": ["Serim Han", "Jingru Zhang", "Hoseung Song"], "title": "Change-Point Detection With Multivariate Repeated Measures", "comment": null, "summary": "Graph-based methods have shown particular strengths in change-point detection (CPD) tasks for high-dimensional nonparametric settings. However, existing CPD research has rarely addressed data with repeated measurements or local group structures. A common treatment is to average repeated measurements, which can result in the loss of important within-individual information. In this paper, we propose a new graph-based method for detecting change-points in data with repeated measurements or local structures by incorporating both within-individual and between-individual information. Analytical approximations to the significance of the proposed statistics are derived, enabling efficient computation of p-values for the combined test statistic. The proposed method effectively detects change-points across a wide range of alternatives, particularly when within-individual differences are present. The new method is illustrated through an analysis of the New York City taxi dataset."}
{"id": "2511.19305", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19305", "abs": "https://arxiv.org/abs/2511.19305", "authors": ["Sara Geremia", "Domenico De Stefano", "Michael Fop"], "title": "Community-level core-periphery structures in collaboration networks", "comment": null, "summary": "Uncovering structural patterns in collaboration networks is key for understanding how knowledge flows and innovation emerges. These networks often exhibit a rich interplay of meso-scale structures, such as communities, core-periphery organization, and influential hubs, which shape the complexity of scientific collaboration. The coexistence of such structures challenges traditional approaches, which typically isolate specific network patterns at the node level. We introduce a novel framework for detecting core-periphery structures at the community level. Given a reference grouping of the nodes, the method optimizes an objective function that assigns core or peripheral roles to communities by accounting for the density and strength of their inter-community connections. The node-level partition may correspond to either inferred communities or to a node-attribute classification, such as discipline or location, enabling direct interpretation of how different social or organizational groups occupy central positions in the network. The method is motivated by an application to a co-authorship network of Italian academics in three different disciplines, where it reveals a hierarchical core-periphery structure associated with institutional role, regional location, and research topics."}
{"id": "2511.18562", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18562", "abs": "https://arxiv.org/abs/2511.18562", "authors": ["Xunlei Qian", "Yue Xing"], "title": "Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks", "comment": "Submitted to AISTATS 2026", "summary": "Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness."}
{"id": "2511.18543", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18543", "abs": "https://arxiv.org/abs/2511.18543", "authors": ["Veronica Poda", "Veronica Vinciotti", "Ernst C. Wit"], "title": "Hyperevent network modelling of partially observed gossip data", "comment": null, "summary": "Gossiping is a widespread social phenomenon that shapes relationships and information flow in communities. From a network theoretic point of view, gossiping can be seen as a higher-order interaction, as it involves at least two persons talking about a non-present third. The mechanism of gossiping is complex: it is most likely dynamic, as its intensity changes over time, and possibly viral, if a gossiping event induces future gossiping, such as a repetition or retaliation. We define covariates of interest for these effects and propose a relational hyperevent model to study and quantify these complex dynamics. We consider survey data collected yearly from 44 secondary schools in Hungary. No information is available about the exact timing of the events nor about the aggregate number of events within the yearly time interval. What is measured is whether at least one gossiping event has occurred in a given time interval. We extend inference for relational hyperevent models to the case of rightcensored interval-time data and show how flexible and efficient generalized additive models can be used for estimation of effects of interest. Our analysis on the school data illustrates how a model that accounts for linear, smooth and random effects can identify the social drivers of gossiping, while revealing complex temporal dynamics."}
{"id": "2511.19406", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19406", "abs": "https://arxiv.org/abs/2511.19406", "authors": ["Rebecca Lee", "Alexander Coulter", "Greg J. Siegle", "Scott A. Bruce", "Anirban Bhattacharya"], "title": "Hierarchical Bayesian spectral analysis of multiple stationary time series", "comment": "32 pages, 8 figures", "summary": "The power spectrum of biomedical time series provides important indirect measurements of physiological processes underlying health and biological functions. However, simultaneously characterizing power spectra for multiple time series remains challenging due to extra spectral variability and varying time series lengths. We propose a method for hierarchical Bayesian estimation of stationary time series (HBEST) that provides an interpretable framework for efficiently modeling multiple power spectra. HBEST models log power spectra using a truncated cosine basis expansion with a novel global-local coefficient decomposition, enabling simultaneous estimation of population-level and individual-level power spectra and accommodating time series of varying lengths. The fully Bayesian framework provides shrinkage priors for regularized estimation and efficient information sharing. Simulations demonstrate HBEST's advantages over competing methods in computational efficiency and estimation accuracy. An application to heart rate variability time series demonstrates HBEST's ability to accurately characterize power spectra and capture associations with traditional cardiovascular risk factors."}
{"id": "2511.18583", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.18583", "abs": "https://arxiv.org/abs/2511.18583", "authors": ["Valentin Roth", "Marco Avella-Medina"], "title": "Differential privacy with dependent data", "comment": null, "summary": "Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \\textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \\iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\\textit{item-level}) and \\textit{user-level} DP estimation of a mean $μ\\in \\R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \\iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \\textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data."}
{"id": "2511.18555", "categories": ["stat.ME", "cs.LG", "math.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18555", "abs": "https://arxiv.org/abs/2511.18555", "authors": ["Alexander W. Hsu", "Ike W. Griss Salas", "Jacob M. Stevens-Haas", "J. Nathan Kutz", "Aleksandr Aravkin", "Bamdad Hosseini"], "title": "A joint optimization approach to identifying sparse dynamics using least squares kernel collocation", "comment": null, "summary": "We develop an all-at-once modeling framework for learning systems of ordinary differential equations (ODE) from scarce, partial, and noisy observations of the states. The proposed methodology amounts to a combination of sparse recovery strategies for the ODE over a function library combined with techniques from reproducing kernel Hilbert space (RKHS) theory for estimating the state and discretizing the ODE. Our numerical experiments reveal that the proposed strategy leads to significant gains in terms of accuracy, sample efficiency, and robustness to noise, both in terms of learning the equation and estimating the unknown states. This work demonstrates capabilities well beyond existing and widely used algorithms while extending the modeling flexibility of other recent developments in equation discovery."}
{"id": "2511.18661", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18661", "abs": "https://arxiv.org/abs/2511.18661", "authors": ["Guillaume Braun", "Bruno Loureiro", "Ha Quang Minh", "Masaaki Imaizumi"], "title": "Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data", "comment": null, "summary": "Scaling laws describe how learning performance improves with data, compute, or training time, and have become a central theme in modern deep learning. We study this phenomenon in a canonical nonlinear model: phase retrieval with anisotropic Gaussian inputs whose covariance spectrum follows a power law. Unlike the isotropic case, where dynamics collapse to a two-dimensional system, anisotropy yields a qualitatively new regime in which an infinite hierarchy of coupled equations governs the evolution of the summary statistics. We develop a tractable reduction that reveals a three-phase trajectory: (i) fast escape from low alignment, (ii) slow convergence of the summary statistics, and (iii) spectral-tail learning in low-variance directions. From this decomposition, we derive explicit scaling laws for the mean-squared error, showing how spectral decay dictates convergence times and error curves. Experiments confirm the predicted phases and exponents. These results provide the first rigorous characterization of scaling laws in nonlinear regression with anisotropic data, highlighting how anisotropy reshapes learning dynamics."}
{"id": "2511.18731", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18731", "abs": "https://arxiv.org/abs/2511.18731", "authors": ["Hao Wang", "Guangyu Tong", "Heather Allore", "Monica Taljaard", "Fan Li"], "title": "Can discrete-time analyses be trusted for stepped wedge trials with continuous recruitment?", "comment": null, "summary": "In stepped wedge cluster randomized trials (SW-CRTs), interventions are sequentially rolled out to clusters over multiple periods. It is common practice to analyze SW-CRTs using discrete-time linear mixed models, in which measurements are considered to be taken at discrete time points. However, a recent systematic review found that 95.1\\% of cross-sectional SW-CRTs recruit individuals continuously over time. Despite the high prevalence of designs with continuous recruitment, there has been limited guidance on how to draw model-robust inference when analyzing such SW-CRTs. In this article, we investigate through simulations the implications of using discrete-time linear mixed models in the case of continuous recruitment designs with a continuous outcome. First, in the data-generating process, we characterize continuous recruitment with a continuous-time exponential decay correlation structure in the presence or absence of a continuous period effect, addressing scenarios both with and without a random or exposure-time-dependent intervention effect. Then, we analyze the simulated data under three popular discrete-time working correlation structures: simple exchangeable, nested exchangeable, and discrete-time exponential decay, with a robust sandwich variance estimator. Our results demonstrate that discrete-time analysis often yields minimum bias, and the robust variance estimator with the Mancl and DeRouen correction consistently achieves nominal coverage and type I error rate. One important exception occurs when recruitment patterns vary systematically between control and intervention periods, where discrete-time analysis leads to slightly biased estimates. Finally, we illustrate these findings by reanalyzing a concluded SW-CRT."}
{"id": "2511.18750", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.18750", "abs": "https://arxiv.org/abs/2511.18750", "authors": ["Samya Praharaj", "Koulik Khamaru"], "title": "On Instability of Minimax Optimal Optimism-Based Bandit Algorithms", "comment": null, "summary": "Statistical inference from data generated by multi-armed bandit (MAB) algorithms is challenging due to their adaptive, non-i.i.d. nature. A classical manifestation is that sample averages of arm rewards under bandit sampling may fail to satisfy a central limit theorem. Lai and Wei's stability condition provides a sufficient, and essentially necessary criterion, for asymptotic normality in bandit problems. While the celebrated Upper Confidence Bound (UCB) algorithm satisfies this stability condition, it is not minimax optimal, raising the question of whether minimax optimality and statistical stability can be achieved simultaneously. In this paper, we analyze the stability properties of a broad class of bandit algorithms that are based on the optimism principle. We establish general structural conditions under which such algorithms violate the Lai-Wei stability criterion. As a consequence, we show that widely used minimax-optimal UCB-style algorithms, including MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH, are unstable. We further complement our theoretical results with numerical simulations demonstrating that, in all these cases, the sample means fail to exhibit asymptotic normality.\n  Overall, our findings suggest a fundamental tension between stability and minimax optimal regret, raising the question of whether it is possible to design bandit algorithms that achieve both. Understanding whether such simultaneously stable and minimax optimal strategies exist remains an important open direction."}
{"id": "2511.18797", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18797", "abs": "https://arxiv.org/abs/2511.18797", "authors": ["Jessalyn N. Sebastian", "Volodymyr M. Minin"], "title": "Gaussian process priors with Markov properties for effective reproduction number inference", "comment": "19 pages, 5 figures, 2 tables in the main text", "summary": "Many quantities characterizing infectious disease outbreaks - like the effective reproduction number ($R_t$), defined as the average number of secondary infections a newly infected individual will cause over the course of their infection - need to be modeled as time-varying parameters. It is common practice to use Gaussian random walks as priors for estimating such functions in Bayesian analyses of pathogen surveillance data. In this setting, however, the random walk prior may be too permissive, as it fails to capture prior scientific knowledge about the estimand and results in high posterior variance. We propose several Gaussian Markov process priors for $R_t$ inference, including the Integrated Brownian Motion (IBM), which can be represented as a Markov process when augmented with its corresponding Brownian Motion component, and is therefore computationally efficient and simple to implement and tune. We use simulated outbreak data to compare the performance of these proposed priors with the Gaussian random walk prior and another state-of-the-art Gaussian process prior based on an approximation to a Matérn covariance function. We find that IBM can match or exceed the performance of other priors, and we show that it produces epidemiologically reasonable and precise results when applied to county-level SARS-CoV-2 data."}
{"id": "2511.18813", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18813", "abs": "https://arxiv.org/abs/2511.18813", "authors": ["Sing-Yuan Yeh", "Chun-Hao Yang"], "title": "Uncertainty of Network Topology with Applications to Out-of-Distribution Detection", "comment": "Submitted for journal publication", "summary": "Persistent homology (PH) is a crucial concept in computational topology, providing a multiscale topological description of a space. It is particularly significant in topological data analysis, which aims to make statistical inference from a topological perspective. In this work, we introduce a new topological summary for Bayesian neural networks, termed the predictive topological uncertainty (pTU). The proposed pTU measures the uncertainty in the interaction between the model and the inputs. It provides insights from the model perspective: if two samples interact with a model in a similar way, then they are considered identically distributed. We also show that the pTU is insensitive to the model architecture. As an application, pTU is used to solve the out-of-distribution (OOD) detection problem, which is critical to ensure model reliability. Failure to detect OOD input can lead to incorrect and unreliable predictions. To address this issue, we propose a significance test for OOD based on the pTU, providing a statistical framework for this issue. The effectiveness of the framework is validated through various experiments, in terms of its statistical power, sensitivity, and robustness."}
{"id": "2511.18948", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.18948", "abs": "https://arxiv.org/abs/2511.18948", "authors": ["Ruilin Bai", "Bo Chen"], "title": "X-chromosome Multilocus Association Studies for Common and Rare Variants", "comment": null, "summary": "X-chromosome association study has specific model uncertainty challenges, such as unknown X-chromosome inactivation status and baseline allele, and considering nonadditive and gene-sex interaction effects in the analysis or not. Although these challenges have been answered for single-locus X-chromosome variants, it remains unclear how to properly perform multilocus association studies when above uncertainties are present. We first carefully investigate the inferential consequences of these uncertainties on existing multilocus association analysis methods, and then propose a theoretically justified framework to analyze multilocus X-chromosome variants while all the uncertainty issues are addressed. We provide separate solutions for common and rare variants, and simulation results show that our solutions are overall more powerful than existing multilocus methods which were proposed to analyze autosomal variants. We finally provide supporting evidences of our approach by revisiting some published X-chromosome association studies."}
{"id": "2511.18876", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18876", "abs": "https://arxiv.org/abs/2511.18876", "authors": ["Lilian Say", "Christophe Denis", "Rafael Pinot"], "title": "Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification", "comment": null, "summary": "The increasing use of machine learning in sensitive applications demands algorithms that simultaneously preserve data privacy and ensure fairness across potentially sensitive sub-populations. While privacy and fairness have each been extensively studied, their joint treatment remains poorly understood. Existing research often frames them as conflicting objectives, with multiple studies suggesting that strong privacy notions such as differential privacy inevitably compromise fairness. In this work, we challenge that perspective by showing that differential privacy can be integrated into a fairness-enhancing pipeline with minimal impact on fairness guarantees. We design a postprocessing algorithm, called DP2DP, that enforces both demographic parity and differential privacy. Our analysis reveals that our algorithm converges towards its demographic parity objective at essentially the same rate (up logarithmic factor) as the best non-private methods from the literature. Experiments on both synthetic and real datasets confirm our theoretical results, showing that the proposed algorithm achieves state-of-the-art accuracy/fairness/privacy trade-offs."}
{"id": "2511.18963", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18963", "abs": "https://arxiv.org/abs/2511.18963", "authors": ["Adéla Hladká", "Patrícia Martinková"], "title": "A novel nonparametric framework for DIF detection using kernel-smoothed item response curves", "comment": null, "summary": "This study introduces a novel nonparametric approach for detecting Differential Item Functioning (DIF) in binary items through direct comparison of Item Response Curves (IRCs). Building on prior work on nonparametric comparison of regression curves, we extend the methodology to accommodate binary response data, which is typical in psychometric applications. The proposed approach includes a new estimator of the asymptotic variance of the test statistic and derives optimal weight functions that maximise local power. Because the asymptotic distribution of the resulting test statistic is unknown, a wild bootstrap procedure is applied for inference. A Monte Carlo simulation study demonstrates that the nonparametric approach effectively controls Type I error and achieves power comparable to the traditional logistic regression method, outperforming it in cases with multiple intersections of the underlying IRCs. The impact of bandwidth and weight specification is explored. Application to a verbal aggression dataset further illustrates the method's ability to detect subtle DIF patterns missed by parametric models. Overall, the proposed nonparametric framework provides a flexible and powerful alternative for detecting DIF, particularly in complex scenarios where traditional model-based assumptions may not be applicable."}
{"id": "2511.18992", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18992", "abs": "https://arxiv.org/abs/2511.18992", "authors": ["Zineddine Tighidet", "Lazhar Labiod", "Mohamed Nadif"], "title": "Classification EM-PCA for clustering and embedding", "comment": "Accepted at the IEEE conference on Big Data (Special Session on Machine Learning)", "summary": "The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches."}
{"id": "2511.19096", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19096", "abs": "https://arxiv.org/abs/2511.19096", "authors": ["Eni Musta", "Joris Mooij"], "title": "Can we detect treatment effect waning from time-to-event data?", "comment": null, "summary": "Understanding how the causal effect of a treatment evolves over time, including the potential for waning, is important for informed decisions on treatment discontinuation or repetition. For example, waning vaccine protection influences booster dose recommendations, while cost-effectiveness analyses require accounting for long-term efficacy of treatments. However, there is no consensus on the methodology to assess and account for treatment effect waning. Even in randomized controlled trials, the common naïve comparison of hazard functions can lead to misleading causal conclusions due to inherent selection bias. Although comparing survival curves is sometimes recommended as a safer measure of causal effect, it only represents a cumulative effect over time and does not address treatment effect waning. We also explore recent formulations of causal hazard ratios, based on the principal stratification approach or the controlled direct effect. These causal hazard ratios cannot be identified without strong modeling assumptions, but bounds can be derived accounting for unobserved heterogeneity and one could try to use them to detect treatment effect waning. However, we illustrate that an increase in causal hazard ratios towards one does not necessarily mean that the protective effect of the treatment is fading. Furthermore, the same survival functions may correspond to both scenarios with and without waning, which shows that treatment effect waning cannot be identified from standard time-to-event data without strong untestable modeling assumptions."}
{"id": "2511.19075", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19075", "abs": "https://arxiv.org/abs/2511.19075", "authors": ["Emanuele Pardini", "Katerina Papagiannouli"], "title": "Structured Matching via Cost-Regularized Unbalanced Optimal Transport", "comment": null, "summary": "Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches."}
{"id": "2511.19234", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19234", "abs": "https://arxiv.org/abs/2511.19234", "authors": ["Claudia Collarin", "Matteo Fasiolo", "Yannig Goude", "Simon Wood"], "title": "Integrating Complex Covariate Transformations in Generalized Additive Models", "comment": null, "summary": "Transformations of covariates are widely used in applied statistics to improve interpretability and to satisfy assumptions required for valid inference. More broadly, feature engineering encompasses a wider set of practices aimed at enhancing predictive performance, and is typically performed as part of a data pre-processing step. In contrast, this paper integrates a substantial component of the feature engineering process directly into the modelling stage. This is achieved by introducing a novel general framework for embedding interpretable covariate transformations within multi-parameter Generalised Additive Models (GAMs). Our framework accommodates any sufficiently differentiable scalar-valued transformation of potentially high-dimensional and complex covariates. These transformations are treated as integral model components, with their parameters estimated jointly with regression coefficients via maximum a posteriori (MAP) methods, and joint uncertainty quantified via approximate Bayesian techniques. Smoothing parameters are selected in an empirical Bayes framework using a Laplace approximation to the marginal likelihood, supported by efficient computation based on implicit differentiation methods. We demonstrate the flexibility and practical value of the proposed methodology through applications to forecasting electricity net-demand in Great Britain and to modelling house prices in London. The proposed methods are implemented by the gamFactory R package, available at https://github.com/mfasiolo/gamFactory."}
{"id": "2511.19157", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19157", "abs": "https://arxiv.org/abs/2511.19157", "authors": ["Weitao Liu"], "title": "A Robust State Filter Against Unmodeled Process And Measurement Noise", "comment": null, "summary": "This paper introduces a novel Kalman filter framework designed to achieve robust state estimation under both process and measurement noise. Inspired by the Weighted Observation Likelihood Filter (WoLF), which provides robustness against measurement outliers, we applied generalized Bayesian approach to build a framework considering both process and measurement noise outliers."}
{"id": "2511.19305", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19305", "abs": "https://arxiv.org/abs/2511.19305", "authors": ["Sara Geremia", "Domenico De Stefano", "Michael Fop"], "title": "Community-level core-periphery structures in collaboration networks", "comment": null, "summary": "Uncovering structural patterns in collaboration networks is key for understanding how knowledge flows and innovation emerges. These networks often exhibit a rich interplay of meso-scale structures, such as communities, core-periphery organization, and influential hubs, which shape the complexity of scientific collaboration. The coexistence of such structures challenges traditional approaches, which typically isolate specific network patterns at the node level. We introduce a novel framework for detecting core-periphery structures at the community level. Given a reference grouping of the nodes, the method optimizes an objective function that assigns core or peripheral roles to communities by accounting for the density and strength of their inter-community connections. The node-level partition may correspond to either inferred communities or to a node-attribute classification, such as discipline or location, enabling direct interpretation of how different social or organizational groups occupy central positions in the network. The method is motivated by an application to a co-authorship network of Italian academics in three different disciplines, where it reveals a hierarchical core-periphery structure associated with institutional role, regional location, and research topics."}
{"id": "2511.19284", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19284", "abs": "https://arxiv.org/abs/2511.19284", "authors": ["Eichi Uehara"], "title": "The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility", "comment": "10 pages, 1 table", "summary": "This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a \"Gatekeeper\" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes."}
{"id": "2511.19375", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.19375", "abs": "https://arxiv.org/abs/2511.19375", "authors": ["Chifeng Shen", "Yuejiao Fu", "Xiaoping Shi", "Michael Chen"], "title": "Product Depth for Temporal Point Processes Observed Only Up to the First k Events", "comment": "27 pages, 12 figures", "summary": "Temporal point processes (TPPs) model the timing of discrete events along a timeline and are widely used in fields such as neuroscience and fi- nance. Statistical depth functions are powerful tools for analyzing centrality and ranking in multivariate and functional data, yet existing depth notions for TPPs remain limited. In this paper, we propose a novel product depth specifically designed for TPPs observed only up to the first k events. Our depth function comprises two key components: a normalized marginal depth, which captures the temporal distribution of the final event, and a conditional depth, which characterizes the joint distribution of the preceding events. We establish its key theoretical properties and demonstrate its practical utility through simulation studies and real data applications."}
{"id": "2511.19404", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.19404", "abs": "https://arxiv.org/abs/2511.19404", "authors": ["Zikai Shen", "Zonghao Chen", "Dimitri Meunier", "Ingo Steinwart", "Arthur Gretton", "Zhu Li"], "title": "Nonparametric Instrumental Variable Regression with Observed Covariates", "comment": null, "summary": "We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O."}
{"id": "2511.19381", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19381", "abs": "https://arxiv.org/abs/2511.19381", "authors": ["Chifeng Shen", "Yuejiao Fu", "Michael Chen", "Xiaoping Shi"], "title": "Asymptotic linear dependence and ellipse statistics for multivariate two-sample homogeneity test", "comment": "19 pages; 9 figures", "summary": "Statistical depth, which measures the center-outward rank of a given sample with respect to its underlying distribution, has become a popular and powerful tool in nonparametric inference. In this paper, we investigate the use of statistical depth in multivariate two-sample problems. We propose a new depth-based nonparametric two-sample test, which has the Chi-square(1) asymptotic distribution under the null hypothesis. Simulations and real-data applications highlight the efficacy and practical value of the proposed test."}
{"id": "2511.18035", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18035", "abs": "https://arxiv.org/abs/2511.18035", "authors": ["Giacomo Iannucci", "Petros Barmpounakis", "Alexandros Beskos", "Nikolaos Demiris"], "title": "On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19", "comment": "Submitted to Statistics and Computing. Approx. 26 pages, 10 figures", "summary": "This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies."}
{"id": "2511.19406", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19406", "abs": "https://arxiv.org/abs/2511.19406", "authors": ["Rebecca Lee", "Alexander Coulter", "Greg J. Siegle", "Scott A. Bruce", "Anirban Bhattacharya"], "title": "Hierarchical Bayesian spectral analysis of multiple stationary time series", "comment": "32 pages, 8 figures", "summary": "The power spectrum of biomedical time series provides important indirect measurements of physiological processes underlying health and biological functions. However, simultaneously characterizing power spectra for multiple time series remains challenging due to extra spectral variability and varying time series lengths. We propose a method for hierarchical Bayesian estimation of stationary time series (HBEST) that provides an interpretable framework for efficiently modeling multiple power spectra. HBEST models log power spectra using a truncated cosine basis expansion with a novel global-local coefficient decomposition, enabling simultaneous estimation of population-level and individual-level power spectra and accommodating time series of varying lengths. The fully Bayesian framework provides shrinkage priors for regularized estimation and efficient information sharing. Simulations demonstrate HBEST's advantages over competing methods in computational efficiency and estimation accuracy. An application to heart rate variability time series demonstrates HBEST's ability to accurately characterize power spectra and capture associations with traditional cardiovascular risk factors."}
{"id": "2511.18237", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18237", "abs": "https://arxiv.org/abs/2511.18237", "authors": ["Sijie Zheng", "Fandong Meng", "Jie Zhou"], "title": "Efficient Covariance Estimation for Sparsified Functional Data", "comment": null, "summary": "Motivated by recent work involving the analysis of leveraging spatial correlations in sparsified mean estimation, we present a novel procedure for constructing covariance estimator. The proposed Random-knots (Random-knots-Spatial) and B-spline (Bspline-Spatial) estimators of the covariance function are computationally efficient. Asymptotic pointwise of the covariance are obtained for sparsified individual trajectories under some regularity conditions. Our proposed nonparametric method well perform the functional principal components analysis for the case of sparsified data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. Theoretical results are illustrated with Monte Carlo simulation experiments. Finally, we cluster multi-domain data by replacing the covariance function with our proposed covariance estimator during PCA."}
{"id": "2511.18060", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18060", "abs": "https://arxiv.org/abs/2511.18060", "authors": ["Francesca Romana Crucinio", "Sahani Pathiraja"], "title": "An operator splitting analysis of Wasserstein--Fisher--Rao gradient flows", "comment": null, "summary": "Wasserstein-Fisher-Rao (WFR) gradient flows have been recently proposed as a powerful sampling tool that combines the advantages of pure Wasserstein (W) and pure Fisher-Rao (FR) gradient flows. Existing algorithmic developments implicitly make use of operator splitting techniques to numerically approximate the WFR partial differential equation, whereby the W flow is evaluated over a given step size and then the FR flow (or vice versa). This works investigates the impact of the order in which the W and FR operator are evaluated and aims to provide a quantitative analysis. Somewhat surprisingly, we show that with a judicious choice of step size and operator ordering, the split scheme can converge to the target distribution faster than the exact WFR flow (in terms of model time). We obtain variational formulae describing the evolution over one time step of both sequential splitting schemes and investigate in which settings the W-FR split should be preferred to the FR-W split. As a step towards this goal we show that the WFR gradient flow preserves log-concavity and obtain the first sharp decay bound for WFR."}
{"id": "2511.18555", "categories": ["stat.ME", "cs.LG", "math.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18555", "abs": "https://arxiv.org/abs/2511.18555", "authors": ["Alexander W. Hsu", "Ike W. Griss Salas", "Jacob M. Stevens-Haas", "J. Nathan Kutz", "Aleksandr Aravkin", "Bamdad Hosseini"], "title": "A joint optimization approach to identifying sparse dynamics using least squares kernel collocation", "comment": null, "summary": "We develop an all-at-once modeling framework for learning systems of ordinary differential equations (ODE) from scarce, partial, and noisy observations of the states. The proposed methodology amounts to a combination of sparse recovery strategies for the ODE over a function library combined with techniques from reproducing kernel Hilbert space (RKHS) theory for estimating the state and discretizing the ODE. Our numerical experiments reveal that the proposed strategy leads to significant gains in terms of accuracy, sample efficiency, and robustness to noise, both in terms of learning the equation and estimating the unknown states. This work demonstrates capabilities well beyond existing and widely used algorithms while extending the modeling flexibility of other recent developments in equation discovery."}
{"id": "2511.18813", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.18813", "abs": "https://arxiv.org/abs/2511.18813", "authors": ["Sing-Yuan Yeh", "Chun-Hao Yang"], "title": "Uncertainty of Network Topology with Applications to Out-of-Distribution Detection", "comment": "Submitted for journal publication", "summary": "Persistent homology (PH) is a crucial concept in computational topology, providing a multiscale topological description of a space. It is particularly significant in topological data analysis, which aims to make statistical inference from a topological perspective. In this work, we introduce a new topological summary for Bayesian neural networks, termed the predictive topological uncertainty (pTU). The proposed pTU measures the uncertainty in the interaction between the model and the inputs. It provides insights from the model perspective: if two samples interact with a model in a similar way, then they are considered identically distributed. We also show that the pTU is insensitive to the model architecture. As an application, pTU is used to solve the out-of-distribution (OOD) detection problem, which is critical to ensure model reliability. Failure to detect OOD input can lead to incorrect and unreliable predictions. To address this issue, we propose a significance test for OOD based on the pTU, providing a statistical framework for this issue. The effectiveness of the framework is validated through various experiments, in terms of its statistical power, sensitivity, and robustness."}
{"id": "2511.19284", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19284", "abs": "https://arxiv.org/abs/2511.19284", "authors": ["Eichi Uehara"], "title": "The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility", "comment": "10 pages, 1 table", "summary": "This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a \"Gatekeeper\" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes."}
