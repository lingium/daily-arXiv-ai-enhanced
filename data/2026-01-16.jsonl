{"id": "2601.09019", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09019", "abs": "https://arxiv.org/abs/2601.09019", "authors": ["Nawaf Bou-Rabee", "Siddharth Mitra", "Andre Wibisono"], "title": "Tail-Sensitive KL and Rényi Convergence of Unadjusted Hamiltonian Monte Carlo via One-Shot Couplings", "comment": "64 pages", "summary": "Hamiltonian Monte Carlo (HMC) algorithms are among the most widely used sampling methods in high dimensional settings, yet their convergence properties are poorly understood in divergences that quantify relative density mismatch, such as Kullback-Leibler (KL) and Rényi divergences. These divergences naturally govern acceptance probabilities and warm-start requirements for Metropolis-adjusted Markov chains. In this work, we develop a framework for upgrading Wasserstein convergence guarantees for unadjusted Hamiltonian Monte Carlo (uHMC) to guarantees in tail-sensitive KL and Rényi divergences. Our approach is based on one-shot couplings, which we use to establish a regularization property of the uHMC transition kernel. This regularization allows Wasserstein-2 mixing-time and asymptotic bias bounds to be lifted to KL divergence, and analogous Orlicz-Wasserstein bounds to be lifted to Rényi divergence, paralleling earlier work of Bou-Rabee and Eberle (2023) that upgrade Wasserstein-1 bounds to total variation distance via kernel smoothing. As a consequence, our results provide quantitative control of relative density mismatch, clarify the role of discretization bias in strong divergences, and yield principled guarantees relevant both for unadjusted sampling and for generating warm starts for Metropolis-adjusted Markov chains."}
{"id": "2601.09677", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09677", "abs": "https://arxiv.org/abs/2601.09677", "authors": ["Guillermina Senn", "Håkon Tjelmeland", "Nathan Glatt-Holtz", "Matt Walker", "Andrew Holbrook"], "title": "Bayesian Semi-Blind Deconvolution at Scale", "comment": null, "summary": "Blind image deconvolution refers to the problem of simultaneously estimating the blur kernel and the true image from a set of observations when both the blur kernel and the true image are unknown. Sometimes, additional image and/or blur information is available and the term semi-blind deconvolution (SBD) is used. We consider a recently introduced Bayesian conjugate hierarchical model for SBD, formulated on an extended cyclic lattice to allow a computationally scalable Gibbs sampler. In this article, we extend this model to the general SBD problem, rewrite the previously proposed Gibbs sampler so that operations are performed in the Fourier domain whenever possible, and introduce a new marginal Hamiltonian Monte Carlo (HMC) blur update, obtained by analytically integrating the blur-image joint conditional over the image. The cyclic formulation combined with non-trivial linear algebra manipulations allows a Fourier-based, scalable HMC update, otherwise complicated by the rigid constraints of the SBD problem. Having determined the padding size in the cyclic embedding through a numerical experiment, we compare the mixing and exploration behaviour of the Gibbs and HMC blur updates on simulated data and on a real geophysical seismic imaging problem where we invert a grid with $300\\times50$ nodes, corresponding to a posterior with approximately $80,000$ parameters."}
{"id": "2601.08964", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.08964", "abs": "https://arxiv.org/abs/2601.08964", "authors": ["Jiahao Tian", "Hugh Chipman", "Thomas Loughin"], "title": "MLCBART: Multilabel Classification with Bayesian Additive Regression Trees", "comment": null, "summary": "Multilabel Classification (MLC) deals with the simultaneous classification of multiple binary labels. The task is challenging because, not only may there be arbitrarily different and complex relationships between predictor variables and each label, but associations among labels may exist even after accounting for effects of predictor variables. In this paper, we present a Bayesian additive regression tree (BART) framework to model the problem. BART is a nonparametric and flexible model structure capable of uncovering complex relationships within the data. Our adaptation, MLCBART, assumes that labels arise from thresholding an underlying numeric scale, where a multivariate normal model allows explicit estimation of the correlation structure among labels. This enables the discovery of complicated relationships in various forms and improves MLC predictive performance. Our Bayesian framework not only enables uncertainty quantification for each predicted label, but our MCMC draws produce an estimated conditional probability distribution of label combinations for any predictor values. Simulation experiments demonstrate the effectiveness of the proposed model by comparing its performance with a set of models, including the oracle model with the correct functional form. Results show that our model predicts vectors of labels more accurately than other contenders and its performance is close to the oracle model. An example highlights how the method's ability to produce measures of uncertainty on predictions provides nuanced understanding of classification results."}
{"id": "2601.09019", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09019", "abs": "https://arxiv.org/abs/2601.09019", "authors": ["Nawaf Bou-Rabee", "Siddharth Mitra", "Andre Wibisono"], "title": "Tail-Sensitive KL and Rényi Convergence of Unadjusted Hamiltonian Monte Carlo via One-Shot Couplings", "comment": "64 pages", "summary": "Hamiltonian Monte Carlo (HMC) algorithms are among the most widely used sampling methods in high dimensional settings, yet their convergence properties are poorly understood in divergences that quantify relative density mismatch, such as Kullback-Leibler (KL) and Rényi divergences. These divergences naturally govern acceptance probabilities and warm-start requirements for Metropolis-adjusted Markov chains. In this work, we develop a framework for upgrading Wasserstein convergence guarantees for unadjusted Hamiltonian Monte Carlo (uHMC) to guarantees in tail-sensitive KL and Rényi divergences. Our approach is based on one-shot couplings, which we use to establish a regularization property of the uHMC transition kernel. This regularization allows Wasserstein-2 mixing-time and asymptotic bias bounds to be lifted to KL divergence, and analogous Orlicz-Wasserstein bounds to be lifted to Rényi divergence, paralleling earlier work of Bou-Rabee and Eberle (2023) that upgrade Wasserstein-1 bounds to total variation distance via kernel smoothing. As a consequence, our results provide quantitative control of relative density mismatch, clarify the role of discretization bias in strong divergences, and yield principled guarantees relevant both for unadjusted sampling and for generating warm starts for Metropolis-adjusted Markov chains."}
{"id": "2601.09294", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.09294", "abs": "https://arxiv.org/abs/2601.09294", "authors": ["Guodong Xu", "Juan Du", "Hui Yang"], "title": "3D-SONAR: Self-Organizing Network for 3D Anomaly Ranking", "comment": "28 pages, 12 figures", "summary": "Surface anomaly detection using 3D point cloud data has gained increasing attention in industrial inspection. However, most existing methods rely on deep learning techniques that are highly dependent on large-scale datasets for training, which are difficult and expensive to acquire in real-world applications. To address this challenge, we propose a novel method based on self-organizing network for 3D anomaly ranking, also named 3D-SONAR. The core idea is to model the 3D point cloud as a dynamic system, where the points are represented as an undirected graph and interact via attractive and repulsive forces. The energy distribution induced by these forces can reveal surface anomalies. Experimental results show that our method achieves superior anomaly detection performance in both open surface and closed surface without training. This work provides a new perspective on unsupervised inspection and highlights the potential of physics-inspired models in industrial anomaly detection tasks with limited data."}
{"id": "2601.09019", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09019", "abs": "https://arxiv.org/abs/2601.09019", "authors": ["Nawaf Bou-Rabee", "Siddharth Mitra", "Andre Wibisono"], "title": "Tail-Sensitive KL and Rényi Convergence of Unadjusted Hamiltonian Monte Carlo via One-Shot Couplings", "comment": "64 pages", "summary": "Hamiltonian Monte Carlo (HMC) algorithms are among the most widely used sampling methods in high dimensional settings, yet their convergence properties are poorly understood in divergences that quantify relative density mismatch, such as Kullback-Leibler (KL) and Rényi divergences. These divergences naturally govern acceptance probabilities and warm-start requirements for Metropolis-adjusted Markov chains. In this work, we develop a framework for upgrading Wasserstein convergence guarantees for unadjusted Hamiltonian Monte Carlo (uHMC) to guarantees in tail-sensitive KL and Rényi divergences. Our approach is based on one-shot couplings, which we use to establish a regularization property of the uHMC transition kernel. This regularization allows Wasserstein-2 mixing-time and asymptotic bias bounds to be lifted to KL divergence, and analogous Orlicz-Wasserstein bounds to be lifted to Rényi divergence, paralleling earlier work of Bou-Rabee and Eberle (2023) that upgrade Wasserstein-1 bounds to total variation distance via kernel smoothing. As a consequence, our results provide quantitative control of relative density mismatch, clarify the role of discretization bias in strong divergences, and yield principled guarantees relevant both for unadjusted sampling and for generating warm starts for Metropolis-adjusted Markov chains."}
{"id": "2601.08981", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.08981", "abs": "https://arxiv.org/abs/2601.08981", "authors": ["Fredrik Lohne Aanes"], "title": "Approximate Shapley value estimation using sampling without replacement and variance estimation via the new Symmetric bootstrap and the Doubled half bootstrap", "comment": "19 pages, 8 figures", "summary": "In this paper I consider improving the KernelSHAP algorithm. I suggest to use the Wallenius' noncentral hypergeometric distribution for sampling the number of coalitions and perform sampling without replacement, so that the KernelSHAP estimation framework is improved further. I also introduce the Symmetric bootstrap to calculate the standard deviations and also use the Doubled half bootstrap method to compare the performance. The new bootstrap algorithm performs better or equally well in the two simulation studies performed in this paper. The new KernelSHAP algorithm performs similarly as the improved KernelSHAP method in the state-of-the-art R-package shapr, which samples coalitions with replacement in one of the options"}
{"id": "2601.09043", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09043", "abs": "https://arxiv.org/abs/2601.09043", "authors": ["Nick Polson", "Vadim Sokolov"], "title": "Horseshoe Mixtures-of-Experts (HS-MoE)", "comment": null, "summary": "Horseshoe mixtures-of-experts (HS-MoE) models provide a Bayesian framework for sparse expert selection in mixture-of-experts architectures. We combine the horseshoe prior's adaptive global-local shrinkage with input-dependent gating, yielding data-adaptive sparsity in expert usage. Our primary methodological contribution is a particle learning algorithm for sequential inference, in which the filter is propagated forward in time while tracking only sufficient statistics. We also discuss how HS-MoE relates to modern mixture-of-experts layers in large language models, which are deployed under extreme sparsity constraints (e.g., activating a small number of experts per token out of a large pool)."}
{"id": "2601.09525", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09525", "abs": "https://arxiv.org/abs/2601.09525", "authors": ["Rongqian Zhang", "Elena Tuzhilina", "Jun Young Park"], "title": "Sparse covariate-driven factorization of high-dimensional brain connectivity with application to site effect correction", "comment": null, "summary": "Large-scale neuroimaging studies often collect data from multiple scanners across different sites, where variations in scanners, scanning procedures, and other conditions across sites can introduce artificial site effects. These effects may bias brain connectivity measures, such as functional connectivity (FC), which quantify functional network organization derived from functional magnetic resonance imaging (fMRI). How to leverage high-dimensional network structures to effectively mitigate site effects has yet to be addressed. In this paper, we propose SLACC (Sparse LAtent Covariate-driven Connectome) factorization, a multivariate method that explicitly parameterizes covariate effects in latent subject scores corresponding to sparse rank-1 latent patterns derived from brain connectivity. The proposed method identifies localized site-driven variability within and across brain networks, enabling targeted correction. We develop a penalized Expectation-Maximization (EM) algorithm for parameter estimation, incorporating the Bayesian Information Criterion (BIC) to guide optimization. Extensive simulations validate SLACC's robustness in recovering the true parameters and underlying connectivity patterns. Applied to the Autism Brain Imaging Data Exchange (ABIDE) dataset, SLACC demonstrates its ability to reduce site effects. The R package to implement our method is publicly available."}
{"id": "2601.09525", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09525", "abs": "https://arxiv.org/abs/2601.09525", "authors": ["Rongqian Zhang", "Elena Tuzhilina", "Jun Young Park"], "title": "Sparse covariate-driven factorization of high-dimensional brain connectivity with application to site effect correction", "comment": null, "summary": "Large-scale neuroimaging studies often collect data from multiple scanners across different sites, where variations in scanners, scanning procedures, and other conditions across sites can introduce artificial site effects. These effects may bias brain connectivity measures, such as functional connectivity (FC), which quantify functional network organization derived from functional magnetic resonance imaging (fMRI). How to leverage high-dimensional network structures to effectively mitigate site effects has yet to be addressed. In this paper, we propose SLACC (Sparse LAtent Covariate-driven Connectome) factorization, a multivariate method that explicitly parameterizes covariate effects in latent subject scores corresponding to sparse rank-1 latent patterns derived from brain connectivity. The proposed method identifies localized site-driven variability within and across brain networks, enabling targeted correction. We develop a penalized Expectation-Maximization (EM) algorithm for parameter estimation, incorporating the Bayesian Information Criterion (BIC) to guide optimization. Extensive simulations validate SLACC's robustness in recovering the true parameters and underlying connectivity patterns. Applied to the Autism Brain Imaging Data Exchange (ABIDE) dataset, SLACC demonstrates its ability to reduce site effects. The R package to implement our method is publicly available."}
{"id": "2601.08996", "categories": ["stat.ME", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.08996", "abs": "https://arxiv.org/abs/2601.08996", "authors": ["Andrea Toloba", "Klaus Langohr", "Guadalupe Gómez Melis"], "title": "Semiparametric estimation of GLMs with interval-censored covariates via an augmented Turnbull estimator", "comment": null, "summary": "Interval-censored covariates are frequently encountered in biomedical studies, particularly in time-to-event data or when measurements are subject to detection or quantification limits. Yet, the estimation of regression models with interval-censored covariates remains methodologically underdeveloped. In this article, we address the estimation of generalized linear models when one covariate is subject to interval censoring. We propose a likelihood-based approach, GELc, that builds upon an augmented version of Turnbull's nonparametric estimator for interval-censored data. We prove that the GELc estimator is consistent and asymptotically normal under mild regularity conditions, with available standard errors. Simulation studies demonstrate favorable finite-sample performance of the estimator and satisfactory coverage of the confidence intervals. Finally, we illustrate the method using two real-world applications: the AIDS Clinical Trials Group Study 359 and an observational nutrition study on circulating carotenoids. The proposed methodology is available as an R package at github.com/atoloba/ICenCov."}
{"id": "2601.08964", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.08964", "abs": "https://arxiv.org/abs/2601.08964", "authors": ["Jiahao Tian", "Hugh Chipman", "Thomas Loughin"], "title": "MLCBART: Multilabel Classification with Bayesian Additive Regression Trees", "comment": null, "summary": "Multilabel Classification (MLC) deals with the simultaneous classification of multiple binary labels. The task is challenging because, not only may there be arbitrarily different and complex relationships between predictor variables and each label, but associations among labels may exist even after accounting for effects of predictor variables. In this paper, we present a Bayesian additive regression tree (BART) framework to model the problem. BART is a nonparametric and flexible model structure capable of uncovering complex relationships within the data. Our adaptation, MLCBART, assumes that labels arise from thresholding an underlying numeric scale, where a multivariate normal model allows explicit estimation of the correlation structure among labels. This enables the discovery of complicated relationships in various forms and improves MLC predictive performance. Our Bayesian framework not only enables uncertainty quantification for each predicted label, but our MCMC draws produce an estimated conditional probability distribution of label combinations for any predictor values. Simulation experiments demonstrate the effectiveness of the proposed model by comparing its performance with a set of models, including the oracle model with the correct functional form. Results show that our model predicts vectors of labels more accurately than other contenders and its performance is close to the oracle model. An example highlights how the method's ability to produce measures of uncertainty on predictions provides nuanced understanding of classification results."}
{"id": "2601.09686", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.09686", "abs": "https://arxiv.org/abs/2601.09686", "authors": ["Ha Nguyen", "Sumanta Basu"], "title": "LARGE: A Locally Adaptive Regularization Approach for Estimating Gaussian Graphical Models", "comment": "21 pages, 5 figures, 2 tables", "summary": "The graphical Lasso (GLASSO) is a widely used algorithm for learning high-dimensional undirected Gaussian graphical models (GGM). Given i.i.d. observations from a multivariate normal distribution, GLASSO estimates the precision matrix by maximizing the log-likelihood with an \\ell_1-penalty on the off-diagonal entries. However, selecting an optimal regularization parameter λin this unsupervised setting remains a significant challenge. A well-known issue is that existing methods, such as out-of-sample likelihood maximization, select a single global λand do not account for heterogeneity in variable scaling or partial variances. Standardizing the data to unit variances, although a common workaround, has been shown to negatively affect graph recovery. Addressing the problem of nodewise adaptive tuning in graph estimation is crucial for applications like computational neuroscience, where brain networks are constructed from highly heterogeneous, region-specific fMRI data.\n  In this work, we develop Locally Adaptive Regularization for Graph Estimation (LARGE), an approach to adaptively learn nodewise tuning parameters to improve graph estimation and selection. In each block coordinate descent step of GLASSO, we augment the nodewise Lasso regression to jointly estimate the regression coefficients and error variance, which in turn guides the adaptive learning of nodewise penalties. In simulations, LARGE consistently outperforms benchmark methods in graph recovery, demonstrates greater stability across replications, and achieves the best estimation accuracy in the most difficult simulation settings. We demonstrate the practical utility of our method by estimating brain functional connectivity from a real fMRI data set."}
{"id": "2601.09011", "categories": ["stat.ME", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2601.09011", "abs": "https://arxiv.org/abs/2601.09011", "authors": ["Steven A. Frank"], "title": "Fisher's fundamental theorem and regression in causal analysis", "comment": null, "summary": "Fisher's fundamental theorem describes the change caused by natural selection as the change in gene frequencies multiplied by the partial regression coefficients for the average effects of genes on fitness. Fisher's result has generated extensive controversy in biology. I show that the theorem is a simple example of a general partition for change in regression predictions across altered contexts. By that rule, the total change in a mean response is the sum of two terms. The first ascribes change to the difference in predictor variables, holding constant the regression coefficients. The second ascribes change to altered context, captured by shifts in the regression coefficients. This general result follows immediately from the product rule for finite differences applied to a regression equation. Economics widely applies this same partition, the Oaxaca-Blinder decomposition, as a fundamental tool that can in proper situations be used for causal analysis. Recognizing the underlying mathematical generality clarifies Fisher's theorem, provides a useful tool for causal analysis, and reveals connections across disciplines."}
{"id": "2601.09686", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.09686", "abs": "https://arxiv.org/abs/2601.09686", "authors": ["Ha Nguyen", "Sumanta Basu"], "title": "LARGE: A Locally Adaptive Regularization Approach for Estimating Gaussian Graphical Models", "comment": "21 pages, 5 figures, 2 tables", "summary": "The graphical Lasso (GLASSO) is a widely used algorithm for learning high-dimensional undirected Gaussian graphical models (GGM). Given i.i.d. observations from a multivariate normal distribution, GLASSO estimates the precision matrix by maximizing the log-likelihood with an \\ell_1-penalty on the off-diagonal entries. However, selecting an optimal regularization parameter λin this unsupervised setting remains a significant challenge. A well-known issue is that existing methods, such as out-of-sample likelihood maximization, select a single global λand do not account for heterogeneity in variable scaling or partial variances. Standardizing the data to unit variances, although a common workaround, has been shown to negatively affect graph recovery. Addressing the problem of nodewise adaptive tuning in graph estimation is crucial for applications like computational neuroscience, where brain networks are constructed from highly heterogeneous, region-specific fMRI data.\n  In this work, we develop Locally Adaptive Regularization for Graph Estimation (LARGE), an approach to adaptively learn nodewise tuning parameters to improve graph estimation and selection. In each block coordinate descent step of GLASSO, we augment the nodewise Lasso regression to jointly estimate the regression coefficients and error variance, which in turn guides the adaptive learning of nodewise penalties. In simulations, LARGE consistently outperforms benchmark methods in graph recovery, demonstrates greater stability across replications, and achieves the best estimation accuracy in the most difficult simulation settings. We demonstrate the practical utility of our method by estimating brain functional connectivity from a real fMRI data set."}
{"id": "2601.09038", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.09038", "abs": "https://arxiv.org/abs/2601.09038", "authors": ["Kyusoon Kim", "Hee-Seok Oh"], "title": "Graph Canonical Coherence Analysis", "comment": "32 pages, 6 figures", "summary": "We propose graph canonical coherence analysis (gCChA), a novel framework that extends canonical correlation analysis to multivariate graph signals in the graph frequency domain. The proposed method addresses challenges posed by the inherent features of graphs: discreteness, finiteness, and irregularity. It identifies pairs of canonical graph signals that maximize their coherence, enabling the exploration of relationships between two sets of graph signals from a spectral perspective. This framework shows how these relationships change across different structural scales of the graph. We demonstrate the usefulness of this method through applications to economic and energy datasets of G20 countries and the USPS handwritten digit dataset."}
{"id": "2601.09126", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.09126", "abs": "https://arxiv.org/abs/2601.09126", "authors": ["Pratim Guha Niyogi", "Muraleetharan Sanjayan", "Kathryn C. Fitzgerald", "Ellen M. Mowry", "Vadim Zipunnikov"], "title": "Scalar-on-distribution regression via generalized odds with applications to accelerometry-assessed disability in multiple sclerosis", "comment": null, "summary": "Distributional representations of data collected using digital health technologies have been shown to outperform scalar summaries for clinical prediction, with carefully quantified tail-behavior often driving the gains. Motivated by these findings, we propose a unified generalized odds (GO) framework that represents subject-specific distributions through ratios of probabilities over arbitrary regions of the sample space, subsuming hazard, survival, and residual life representations as special cases. We develop a scale-on-odds regression model using spline-based functional representations with penalization for efficient estimation. Applied to wrist-worn accelerometry data from the HEAL-MS study, generalized odds models yield improved prediction of Expanded Disability Status Scale (EDSS) scores compared to classical scalar and survival-based approaches, demonstrating the value of odds-based distributional covariates for modeling DHT data."}
{"id": "2601.09161", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.09161", "abs": "https://arxiv.org/abs/2601.09161", "authors": ["Dapeng Shi", "Haoran Zhang", "Tiandong Wang", "Junhui Wang"], "title": "A Multilayer Probit Network Model for Community Detection with Dependent Edges and Layers", "comment": "31 pages, 4 figures", "summary": "Community detection in multilayer networks, which aims to identify groups of nodes exhibiting similar connectivity patterns across multiple network layers, has attracted considerable attention in recent years. Most existing methods are based on the assumption that different layers are either independent or follow specific dependence structures, and edges within the same layer are independent. In this article, we propose a novel method for community detection in multilayer networks that accounts for a broad range of inter-layer and intra-layer dependence structures. The proposed method integrates the multilayer stochastic block model for community detection with a multivariate probit model to capture the structures of inter-layer dependence, which also allows intra-layer dependence. To facilitate parameter estimation, we develop a constrained pairwise likelihood method coupled with an efficient alternating updating algorithm. The asymptotic properties of the proposed method are also established, with a focus on examining the influence of inter-layer and intra-layer dependences on the accuracy of both parameter estimation and community detection. The theoretical results are supported by extensive numerical experiments on both simulated networks and a real-world multilayer trade network."}
{"id": "2601.09371", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.09371", "abs": "https://arxiv.org/abs/2601.09371", "authors": ["Ángel López-Oriona", "Ying Sun", "Hanlin Shang"], "title": "White noise testing for functional time series via functional quantile autocorrelation", "comment": null, "summary": "We introduce a novel class of nonlinear tests for serial dependence in functional time series, grounded in the functional quantile autocorrelation framework. Unlike traditional approaches based on the classical autocovariance kernel, the functional quantile autocorrelation framework leverages quantile-based excursion sets to robustly capture temporal dependence within infinite-dimensional functional data, accommodating potential outliers and complex nonlinear dependencies. We propose omnibus test statistics and study their asymptotic properties under both known and estimated quantile curves, establishing their asymptotic distribution and consistency under mild assumptions. In particular, no moment conditions are required for the validity of the tests. Extensive simulations and an application to high-frequency financial functional time series demonstrate the methodology's effectiveness, reliably detecting complex serial dependence with superior power relative to several existing tests. This work expands the toolkit for functional time series, providing a robust framework for inference in settings where traditional methods may fail."}
{"id": "2601.09442", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.09442", "abs": "https://arxiv.org/abs/2601.09442", "authors": ["Joanna Hindley", "Charlotte Hartley", "Jennifer Hellier", "Kate Sturgeon", "Sophie Greenwood", "Ian Newsome", "Katherine Barrett", "Debs Smith", "Tra My Pham", "Dongquan Bi", "Beatriz Goulao", "Suzie Cro", "Brennan C Kahan"], "title": "Tools to help patients and other stakeholders' input into choice of estimand and intercurrent event strategy in randomised trials", "comment": null, "summary": "Estimands can help to clarify the research questions being addressed in randomised trials. Because the choice of estimand can affect how relevant trial results are to patients and other stakeholders, such as clinicians or policymakers, it is important for them to be involved in these decisions. However, there are barriers to having these conversations. For instance, discussions around how intercurrent events should be addressed in the estimand definition typically involve complex concepts as well as technical language. We three tools to facilitate conversations between researchers and patients and other stakeholders about the choice of estimand and intercurrent event strategy: (i) a video explaining the concept of an estimand and the five different ways that intercurrent events can be incorporated into the estimand definition; (ii) an infographic outlining these five strategies; and (iii) an editable PowerPoint slide which can be completed with trial-specific details to facilitate conversations around choice of estimand for a particular trial. These resources can help to start conversations between the trial team and patients and other stakeholders about the best choice of estimand and intercurrent event strategies for a randomised trial."}
{"id": "2601.09525", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09525", "abs": "https://arxiv.org/abs/2601.09525", "authors": ["Rongqian Zhang", "Elena Tuzhilina", "Jun Young Park"], "title": "Sparse covariate-driven factorization of high-dimensional brain connectivity with application to site effect correction", "comment": null, "summary": "Large-scale neuroimaging studies often collect data from multiple scanners across different sites, where variations in scanners, scanning procedures, and other conditions across sites can introduce artificial site effects. These effects may bias brain connectivity measures, such as functional connectivity (FC), which quantify functional network organization derived from functional magnetic resonance imaging (fMRI). How to leverage high-dimensional network structures to effectively mitigate site effects has yet to be addressed. In this paper, we propose SLACC (Sparse LAtent Covariate-driven Connectome) factorization, a multivariate method that explicitly parameterizes covariate effects in latent subject scores corresponding to sparse rank-1 latent patterns derived from brain connectivity. The proposed method identifies localized site-driven variability within and across brain networks, enabling targeted correction. We develop a penalized Expectation-Maximization (EM) algorithm for parameter estimation, incorporating the Bayesian Information Criterion (BIC) to guide optimization. Extensive simulations validate SLACC's robustness in recovering the true parameters and underlying connectivity patterns. Applied to the Autism Brain Imaging Data Exchange (ABIDE) dataset, SLACC demonstrates its ability to reduce site effects. The R package to implement our method is publicly available."}
{"id": "2601.09571", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.09571", "abs": "https://arxiv.org/abs/2601.09571", "authors": ["Jonathan W. Bartlett", "Dominic Magirr", "Tim P. Morris"], "title": "How to interpret hazard ratios", "comment": "14 pages, 2 figures", "summary": "The hazard ratio, typically estimated using Cox's famous proportional hazards model, is the most common effect measure used to describe the association or effect of a covariate on a time-to-event outcome. In recent years the hazard ratio has been argued by some to lack a causal interpretation, even in randomised trials, and even if the proportional hazards assumption holds. This is concerning, not least due to the ubiquity of hazard ratios in analyses of time-to-event data. We review these criticisms, describe how we think hazard ratios should be interpreted, and argue that they retain a valid causal interpretation. Nevertheless, alternative measures may be preferable to describe effects of exposures or treatments on time-to-event outcomes."}
{"id": "2601.09576", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.09576", "abs": "https://arxiv.org/abs/2601.09576", "authors": ["David Bamio", "Jacobo de Uña-Álvarez"], "title": "Smoothing spline density estimation from doubly truncated data", "comment": null, "summary": "In Astronomy, Survival Analysis and Epidemiology, among many other fields, doubly truncated data often appear. Double truncation generally induces a sampling bias, so ordinary estimators may be inconsistent. In this paper, smoothing spline density estimation from doubly truncated data is investigated. For this purpose, an appropriate correction of the penalized likelihood that accounts for the sampling bias is considered. The theoretical properties of the estimator are discussed, and its practical performance is evaluated through simulations. Two real datasets are analyzed using the proposed method for illustrative purposes. Comparison to kernel density smoothing is included."}
{"id": "2601.09686", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.09686", "abs": "https://arxiv.org/abs/2601.09686", "authors": ["Ha Nguyen", "Sumanta Basu"], "title": "LARGE: A Locally Adaptive Regularization Approach for Estimating Gaussian Graphical Models", "comment": "21 pages, 5 figures, 2 tables", "summary": "The graphical Lasso (GLASSO) is a widely used algorithm for learning high-dimensional undirected Gaussian graphical models (GGM). Given i.i.d. observations from a multivariate normal distribution, GLASSO estimates the precision matrix by maximizing the log-likelihood with an \\ell_1-penalty on the off-diagonal entries. However, selecting an optimal regularization parameter λin this unsupervised setting remains a significant challenge. A well-known issue is that existing methods, such as out-of-sample likelihood maximization, select a single global λand do not account for heterogeneity in variable scaling or partial variances. Standardizing the data to unit variances, although a common workaround, has been shown to negatively affect graph recovery. Addressing the problem of nodewise adaptive tuning in graph estimation is crucial for applications like computational neuroscience, where brain networks are constructed from highly heterogeneous, region-specific fMRI data.\n  In this work, we develop Locally Adaptive Regularization for Graph Estimation (LARGE), an approach to adaptively learn nodewise tuning parameters to improve graph estimation and selection. In each block coordinate descent step of GLASSO, we augment the nodewise Lasso regression to jointly estimate the regression coefficients and error variance, which in turn guides the adaptive learning of nodewise penalties. In simulations, LARGE consistently outperforms benchmark methods in graph recovery, demonstrates greater stability across replications, and achieves the best estimation accuracy in the most difficult simulation settings. We demonstrate the practical utility of our method by estimating brain functional connectivity from a real fMRI data set."}
