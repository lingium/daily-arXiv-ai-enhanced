<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 15]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 11]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Subspace Ordering for Maximum Response Preservation in Sufficient Dimension Reduction](https://arxiv.org/abs/2510.27593)
*Derik T. Boonstra,Rakheon Kim,Dean M. Young*

Main category: stat.ME

TL;DR: 本文提出了一种新的子空间排序标准，替代传统基于特征值的排序方法，通过直接评估子空间的预测相关性来提高分类准确性和子空间估计效果。


<details>
  <summary>Details</summary>
Motivation: 传统充分降维方法使用特征值作为子空间重要性的衡量标准，但作者认为这不是最优的，需要更直接评估子空间预测相关性的排序标准。

Method: 对于二元响应变量，提出了基于独立学生t统计量绝对值的子空间排序标准；同时使用F统计量构建统一框架，将分类和连续响应统一在单一子空间标准下。

Result: 通过大量模拟研究和实际数据应用评估，证明重新排序子空间能显著提高分类准确性和子空间估计效果，优于基于特征值的排序方法。

Conclusion: 提出的新排序标准在理论上能识别达到局部最小贝叶斯错误率的子空间，在实践中能有效改进充分降维方法的性能。

Abstract: Sufficient dimension reduction (SDR) methods aim to identify a dimension
reduction subspace (DRS) that preserves all the information about the
conditional distribution of a response given its predictor. Traditional SDR
methods determine the DRS by solving a method-specific generalized eigenvalue
problem and selecting the eigenvectors corresponding to the largest
eigenvalues. In this article, we argue against the long-standing convention of
using eigenvalues as the measure of subspace importance and propose alternative
ordering criteria that directly assess the predictive relevance of each
subspace. For a binary response, we introduce a subspace ordering criterion
based on the absolute value of the independent Student's t-statistic.
Theoretically, our criterion identifies subspaces that achieve the local
minimum Bayes' error rate and yields consistent ordering of directions under
mild regularity conditions. Additionally, we employ an F-statistic to provide a
framework that unifies categorical and continuous responses under a single
subspace criterion. We evaluate our proposed criteria within multiple SDR
methods through extensive simulation studies and applications to real data. Our
empirical results demonstrate the efficacy of reordering subspaces using our
proposed criteria, which generally improves classification accuracy and
subspace estimation compared to ordering by eigenvalues.

</details>


### [2] [Impact of clinical decision support systems (cdss) on clinical outcomes and healthcare delivery in low- and middle-income countries: protocol for a systematic review and meta-analysis](https://arxiv.org/abs/2510.26812)
*Garima Jain,Anand Bodade,Sanghamitra Pati*

Main category: stat.ME

TL;DR: 该协议旨在通过系统评价和荟萃分析，量化临床决策支持系统（CDSS）在低收入和中等收入国家（LMICs）中对患者和医疗服务结果的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管临床决策支持系统被用于改善临床和服务结果，但在低收入和中等收入国家的证据分散且缺乏系统性评估。

Method: 采用系统评价方法，纳入比较性定量研究设计（随机试验、前后对照、中断时间序列、比较队列），检索多个数据库和灰色文献，双人筛选和提取数据，使用RoB 2和ROBINS-I评估偏倚风险，进行随机效应荟萃分析或结构化叙述性综合。

Result: 该研究将提供关于CDSS在LMICs中有效性的综合证据，包括对患者结果和医疗服务交付的影响。

Conclusion: 这项系统评价将为CDSS在资源有限环境中的实施效果提供重要证据基础，有助于指导未来的临床决策支持系统部署。

Abstract: Clinical decision support systems (CDSS) are used to improve clinical and
service outcomes, yet evidence from low- and middle-income countries (LMICs) is
dispersed. This protocol outlines methods to quantify the impact of CDSS on
patient and healthcare delivery outcomes in LMICs. We will include comparative
quantitative designs (randomized trials, controlled before-after, interrupted
time series, comparative cohorts) evaluating CDSS in World Bank-defined LMICs.
Standalone qualitative studies are excluded; mixed-methods studies are eligible
only if they report comparative quantitative outcomes, for which we will
extract the quantitative component. Searches (from inception to 30 September
2024) will cover MEDLINE, Embase, CINAHL, CENTRAL, Web of Science, Global
Health, Scopus, IEEE Xplore, LILACS, African Index Medicus, and IndMED, plus
grey sources. Screening and extraction will be performed in duplicate. Risk of
bias will be assessed with RoB 2 (randomized trials) and ROBINS-I
(non-randomized). Random-effects meta-analysis will be performed where outcomes
are conceptually or statistically comparable; otherwise, a structured narrative
synthesis will be presented. Heterogeneity will be explored using relative and
absolute metrics and a priori subgroups or meta-regression (condition area,
care level, CDSS type, readiness proxies, study design).

</details>


### [3] [Residual Distribution Predictive Systems](https://arxiv.org/abs/2510.26914)
*Sam Allen,Enrico Pescara,Johanna Ziegel*

Main category: stat.ME

TL;DR: 本文提出了一种称为残差分布预测系统的新方法，用于构建具有样本外校准保证的预测系统，该方法在完整共形设置中比传统共形预测系统具有优势。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测系统依赖于满足严格要求的符合性度量，限制了其实用性。本文旨在开发一种更灵活的方法，能够与任何点值回归方法结合使用。

Method: 提出了残差分布预测系统方法，在分割共形设置中嵌套了传统共形预测系统，但在完整共形设置中提供了更灵活的实现方式。

Result: 在模拟数据上的实证评估表明，新方法的性能与传统共形预测系统相当，但提供了更广泛的回归方法实现可能性。

Conclusion: 残差分布预测系统为构建具有校准保证的预测系统提供了一种有前景的替代方法，特别是在完整共形设置中具有显著优势。

Abstract: Conformal predictive systems are sets of predictive distributions with
theoretical out-of-sample calibration guarantees. The calibration guarantees
are typically that the set of predictions contains a forecast distribution
whose prediction intervals exhibit the correct marginal coverage at all levels.
Conformal predictive systems are constructed using conformity measures that
quantify how well possible outcomes conform with historical data. However,
alternative methods have been proposed to construct predictive systems with
more appealing theoretical properties. We study an approach to construct
predictive systems that we term Residual Distribution Predictive Systems. In
the split conformal setting, this approach nests conformal predictive systems
with a popular class of conformity measures, providing an alternative
perspective on the classical approach. In the full conformal setting, the two
approaches differ, and the new approach has the advantage that it does not rely
on a conformity measure satisfying fairly stringent requirements to ensure that
the predictive system is well-defined; it can readily be implemented alongside
any point-valued regression method to yield predictive systems with
out-of-sample calibration guarantees. The empirical performance of this
approach is assessed using simulated data, where it is found to perform
competitively with conformal predictive systems. However, the new approach
offers considerable scope for implementation with alternative regression
methods.

</details>


### [4] [A novel generalized additive scalar-on-function regression model for partially observed multidimensional functional data: An application to air quality classification](https://arxiv.org/abs/2510.26917)
*Pavel Hernández-Amaro,Maria Durban,M. Carmen Aguilera-Morillo*

Main category: stat.ME

TL;DR: 提出了一种广义加性函数回归模型，用于处理部分观测的函数数据，无需对缺失观测进行插补，能够处理不同维度的函数预测变量。


<details>
  <summary>Details</summary>
Motivation: 处理部分观测的函数数据时，传统方法通常需要插补缺失值，这可能导致偏差。本文旨在开发一种能够直接处理不完整函数数据的回归模型。

Method: 使用基函数展开表示函数系数和协变量（本研究使用B样条），通过惩罚似然估计模型系数，利用惩罚样条的混合模型表示进行高效计算和平滑参数估计。

Result: 通过两个模拟研究验证方法性能：一个涉及两个一维函数协变量，另一个使用二维函数协变量。在印度迪马普尔空气污染分类应用中，模型能有效处理不完整函数数据并准确区分污染水平。

Conclusion: 提出的方法能够有效处理部分观测的函数数据，无需插补缺失值，在模拟和实际应用中均表现出良好的性能，特别适用于处理图像等二维函数变量的分类问题。

Abstract: In this work we propose a generalized additive functional regression model
for partially observed functional data. Our approach accommodates functional
predictors of varying dimensions without requiring imputation of missing
observations. Both the functional coefficients and covariates are represented
using basis function expansions, with B-splines used in this study, though the
method is not restricted to any specific basis choice. Model coefficients are
estimated via penalized likelihood, leveraging the mixed model representation
of penalized splines for efficient computation and smoothing parameter
estimation.The performance of the proposed approach is assessed through two
simulation studies: one involving two one-dimensional functional covariates,
and another using a two-dimensional functional covariate. Finally, we
demonstrate the practical utility of our method in an application to
air-pollution classification in Dimapur, India, where images are treated as
observations of a two-dimensional functional variable. This case study
highlights the models ability to effectively handle incomplete functional data
and to accurately discriminate between pollution levels.

</details>


### [5] [The Interplay between Bayesian Inference and Conformal Prediction](https://arxiv.org/abs/2510.26930)
*Nina Deliu,Brunero Liseo*

Main category: stat.ME

TL;DR: 本文探讨了共形预测与贝叶斯统计的结合，提出贝叶斯共形推断框架，旨在平衡有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 共形预测提供有限样本频率覆盖保证，而贝叶斯方法常缺乏频率保证。两者结合可以相互增强：共形预测可校准贝叶斯可信集，贝叶斯方法可提升共形预测的信息量和决策理论最优性。

Method: 形式化贝叶斯共形推断框架，涵盖统计效率和计算复杂性等挑战性方面，并调查现有思想。

Result: 建立了连接频率主义和贝叶斯方法的桥梁，为两者在有效性和效率之间的原则性平衡提供了理论基础。

Conclusion: 共形预测和贝叶斯统计可以联合使用，实现有效性和效率的平衡，贝叶斯程序有潜力增强共形预测的信息量和决策理论性能。

Abstract: Conformal prediction has emerged as a cutting-edge methodology in statistics
and machine learning, providing prediction intervals with finite-sample
frequentist coverage guarantees. Yet, its interplay with Bayesian statistics,
often criticised for lacking frequentist guarantees, remains underexplored.
Recent work has suggested that conformal prediction can serve to "calibrate"
Bayesian credible sets, thereby imparting frequentist validity and motivating
deeper investigation into frequentist-Bayesian hybrids. We further argue that
Bayesian procedures have the potential to enhance conformal prediction, not
only in terms of more informative intervals, but also for achieving nearly
optimal solutions under a decision-theoretic framework. Thus, the two paradigms
can be jointly used for a principled balance between validity and efficiency.
This work provides a basis for bridging this gap. After surveying existing
ideas, we formalise the Bayesian conformal inference framework, covering
challenging aspects such as statistical efficiency and computational
complexity.

</details>


### [6] [Finite Sample MIMO System Identification with Multisine Excitation: Nonparametric, Direct, and Two-step Parametric Estimators](https://arxiv.org/abs/2510.26929)
*Rodrigo A. González,Koen Classens,Cristian R. Rojas,Tom Oomen,Håkan Hjalmarsson*

Main category: stat.ME

TL;DR: 该论文开发了一个有限样本统计框架，用于分析多正弦激励下频率响应函数的最小二乘估计及其对参数建模的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管多正弦激励广泛用于多输入多输出系统识别，但其在频域估计器下的有限样本统计特性，包括非参数和参数设置，仍然不够清楚。

Method: 首先推导了FRF估计器的精确分布和协方差特性，明确考虑了慢采样机制下的混叠效应；其次证明了FRF估计在高斯噪声下是任何参数模型的充分统计量。

Result: 建立了最优两阶段频域方法与时域预测误差和最大似然估计之间的精确等价性，得到了参数最大似然估计器的有限样本集中界，实现了严格的不确定性量化。

Conclusion: 该理论框架为多正弦激励下的系统识别提供了严格的统计基础，并在代表性案例研究中得到了验证。

Abstract: Multisine excitations are widely used for identifying multi-input
multi-output systems due to their periodicity, data compression properties, and
control over the input spectrum. Despite their popularity, the finite sample
statistical properties of frequency-domain estimators under multisine
excitation, for both nonparametric and parametric settings, remain
insufficiently understood. This paper develops a finite-sample statistical
framework for least-squares estimation of the frequency response function (FRF)
and its implications for parametric modeling. First, we derive exact
distributional and covariance properties of the FRF estimator, explicitly
accounting for aliasing effects under slow sampling regimes, and establish
conditions for unbiasedness, uncorrelatedness, and consistency across multiple
experiments. Second, we show that the FRF estimate is a sufficient statistic
for any parametric model under Gaussian noise, leading to an exact equivalence
between optimal two stage frequency-domain methods and time-domain prediction
error and maximum likelihood estimation. This equivalence is shown to yield
finite-sample concentration bounds for parametric maximum likelihood
estimators, enabling rigorous uncertainty quantification, and closed-form
prediction error method estimators without iterative optimization. The
theoretical results are demonstrated in a representative case study.

</details>


### [7] [LLMs are Overconfident: Evaluating Confidence Interval Calibration with FermiEval](https://arxiv.org/abs/2510.26995)
*Elliot L. Epstein,John Winnicki,Thanawat Sornwanee,Rajat Dwaraknath*

Main category: stat.ME

TL;DR: LLMs在数值估计方面表现出色，但在量化不确定性方面存在系统性过度自信问题。研究者开发了FermiEval基准测试，发现模型构建的99%置信区间实际覆盖率仅为65%。通过保形预测等方法调整后，覆盖率提升至99%，并提出了感知隧道理论解释这种过度自信现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数值估计任务中表现出色，但无法正确量化自身答案的不确定性，存在系统性过度自信问题。这限制了模型在需要可靠不确定性估计的实际应用中的可信度。

Method: 1. 引入FermiEval基准测试，包含费米式估计问题，采用严格的置信区间覆盖率和锐度评分规则；2. 使用保形预测方法调整置信区间；3. 提出直接对数概率诱导和分位数调整方法；4. 开发感知隧道理论解释过度自信现象。

Result: 1. 多个现代模型构建的99%置信区间平均实际覆盖率仅为65%；2. 经保形预测调整后，观测覆盖率准确达到99%，Winkler区间得分降低54%；3. 直接对数概率诱导和分位数调整方法进一步减少了高置信水平下的过度自信。

Conclusion: LLMs在不确定性量化方面存在系统性过度自信，但可以通过保形预测等方法有效校正。感知隧道理论解释了这种现象：模型在不确定性推理时，仿佛从其推断分布的截断区域采样，忽略了分布的尾部。

Abstract: Large language models (LLMs) excel at numerical estimation but struggle to
correctly quantify uncertainty. We study how well LLMs construct confidence
intervals around their own answers and find that they are systematically
overconfident. To evaluate this behavior, we introduce FermiEval, a benchmark
of Fermi-style estimation questions with a rigorous scoring rule for confidence
interval coverage and sharpness. Across several modern models, nominal 99\%
intervals cover the true answer only 65\% of the time on average. With a
conformal prediction based approach that adjusts the intervals, we obtain
accurate 99\% observed coverage, and the Winkler interval score decreases by
54\%. We also propose direct log-probability elicitation and quantile
adjustment methods, which further reduce overconfidence at high confidence
levels. Finally, we develop a perception-tunnel theory explaining why LLMs
exhibit overconfidence: when reasoning under uncertainty, they act as if
sampling from a truncated region of their inferred distribution, neglecting its
tails.

</details>


### [8] [Generalized Maximum Entropy: When and Why you need it](https://arxiv.org/abs/2510.27006)
*Giuseppe M. Ferro,Edwin T. Pos,Andrea Somazzi*

Main category: stat.ME

TL;DR: 论文指出经典的最大熵原理基于强系统独立性假设，这在现实世界的强相关系统中常被违反。作者建议在系统独立性不成立时，应使用Uffink-Jizba-Korbel广义熵族来替代香农熵。


<details>
  <summary>Details</summary>
Motivation: 香农熵基于的Shore-Johnson公理假设强系统独立性，但现实世界中的强相关系统经常违反这一假设，因此需要更通用的熵函数来处理这类情况。

Method: 回顾Shore-Johnson公理，介绍常用的熵函数并将其定位在UJK广义熵族中，通过经济学和生态学的两个应用案例展示广义熵的必要性，并提供一个详细的数学模型来展示广义最大熵方法在处理强相关系统时的能力。

Result: 研究表明当系统独立性假设不成立时，使用UJK广义熵族能够更准确地构建概率分布，广义最大熵方法在处理强相关系统时表现出更强的能力。

Conclusion: 提出了选择熵测度的实用指南和结果报告规范，以确保分析的透明性和可重复性，建议在系统独立性不成立时使用广义熵方法。

Abstract: The classical Maximum-Entropy Principle (MEP) based on Shannon entropy is
widely used to construct least-biased probability distributions from partial
information. However, the Shore-Johnson axioms that single out the Shannon
functional hinge on strong system independence, an assumption often violated in
real-world, strongly correlated systems. We provide a self-contained guide to
when and why practitioners should abandon the Shannon form in favour of the
one-parameter Uffink-Jizba-Korbel (UJK) family of generalized entropies. After
reviewing the Shore and Johnson axioms from an applied perspective, we recall
the most commonly used entropy functionals and locate them within the UJK
family. The need for generalized entropies is made clear with two applications,
one rooted in economics and the other in ecology. A simple mathematical model
worked out in detail shows the power of generalized maximum entropy approaches
in dealing with cases where strong system independence does not hold. We
conclude with practical guidelines for choosing an entropy measure and
reporting results so that analyses remain transparent and reproducible.

</details>


### [9] [Inconsistency thresholds revisited: The effect of the graph associated with incomplete pairwise comparisons](https://arxiv.org/abs/2510.27011)
*Kolos Csaba Ágoston,László Csató*

Main category: stat.ME

TL;DR: 本文修订了不完全成对比较矩阵的不一致性阈值，发现这些阈值不仅取决于矩阵大小和缺失条目数量，还取决于表示已知比较的图结构。


<details>
  <summary>Details</summary>
Motivation: 现有的10%不一致性阈值规则在应用于不完全成对比较矩阵时存在局限性，需要更精确的阈值来更好地解释不一致性。

Method: 通过分析表示已知比较的图结构，研究不一致性阈值与图谱半径之间的关联，提出精确的阈值计算方法。

Result: 发现不一致性阈值强烈依赖于图的谱半径，新的阈值可以集成到软件中实时监控不一致性。

Conclusion: 提出的精确不一致性阈值对于具有特定填充模式的大量矩阵尤为重要，能够帮助在收集成对比较时立即检测潜在错误。

Abstract: The inconsistency of pairwise comparisons remains difficult to interpret in
the absence of acceptability thresholds. The popular 10% cut-off rule proposed
by Saaty has recently been applied to incomplete pairwise comparison matrices,
which contain some unknown comparisons. This paper revises these inconsistency
thresholds: we uncover that they depend not only on the size of the matrix and
the number of missing entries, but also on the undirected graph whose edges
represent the known pairwise comparisons. Therefore, using our exact thresholds
is especially important if the filling in patterns coincide for a large number
of matrices, as has been recommended in the literature. The strong association
between the new threshold values and the spectral radius of the representing
graph is also demonstrated. Our results can be integrated into software to
continuously monitor inconsistency during the collection of pairwise
comparisons and immediately detect potential errors.

</details>


### [10] [Calibrating Bayesian Inference](https://arxiv.org/abs/2510.27144)
*Yang Liu,Youjin Sung,Jonathan P. Williams,Jan Hannig*

Main category: stat.ME

TL;DR: 贝叶斯统计在有限样本中可能不可靠，特别是当先验分布与真实参数生成过程不匹配时。本文提出通过校准贝叶斯可信区间来实现频率有效性，确保无论参数生成机制如何都能保持有效性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯统计在心理学研究中很受欢迎，但其在有限样本中的性能可能不可靠。当分析者选择的先验分布与真实参数生成过程不匹配时，贝叶斯推断在长期运行中可能产生误导性结果。

Method: 提出校准贝叶斯可信区间以实现频率有效性，并开发了一种新的随机逼近算法来解决校准问题。通过蒙特卡洛实验验证方法。

Result: 实验观察到未校准的贝叶斯推断在某些参数生成场景下可能过于宽松，而校准后的解决方案始终能够保持有效性。

Conclusion: 通过校准贝叶斯可信区间可以实现频率有效性，这为贝叶斯推断提供了更强的保证，无论底层参数生成机制如何都能保持有效性。

Abstract: While Bayesian statistics is popular in psychological research for its
intuitive uncertainty quantification and flexible decision-making, its
performance in finite samples can be unreliable. In this paper, we demonstrate
a key vulnerability: When analysts' chosen prior distribution mismatches the
true parameter-generating process, Bayesian inference can be misleading in the
long run. Given that this true process is rarely known in practice, we propose
a safer alternative: calibrating Bayesian credible regions to achieve
frequentist validity. This latter criterion is stronger and guarantees validity
of Bayesian inference regardless of the underlying parameter-generating
mechanism. To solve the calibration problem in practice, we propose a novel
stochastic approximation algorithm. A Monte Carlo experiment is conducted and
reported, in which we observe that uncalibrated Bayesian inference can be
liberal under certain parameter-generating scenarios, whereas our calibrated
solution is always able to maintain validity.

</details>


### [11] [Change-in-velocity detection for multidimensional data](https://arxiv.org/abs/2510.27150)
*Linh Do,Dat Do,Keisha J. Cook,Scott A. McKinley*

Main category: stat.ME

TL;DR: CPLASS算法用于检测多维数据中的速度变化点，通过随机搜索进行连续分段线性逼近，特别适用于细胞内运输数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统变化点检测方法主要关注均值变化，而检测速度变化需要专门方法，因为连续性约束和参数依赖性使得二进制分割和动态规划等流行算法失效。

Method: 使用专门的惩罚函数平衡模型复杂度的似然改进，采用基于MCMC的方法和定制化提议机制进行高效参数探索，并引入速度惩罚确保生物物理真实性。

Result: 开发了累积速度分配统计量，对变化点检测的个体差异具有鲁棒性，同时保持区分生物物理不同群体的能力。

Conclusion: CPLASS为多维速度变化检测提供了一种有效方法，特别适用于分析分子马达驱动的细胞内运输轨迹数据。

Abstract: In this work, we introduce CPLASS (Continuous Piecewise-Linear Approximation
via Stochastic Search), an algorithm for detecting changes in velocity within
multidimensional data. The one-dimensional version of this problem is known as
the change-in-slope problem (see Fearnhead & Grose (2022), Baranowski et al.
(2019)). Unlike traditional changepoint detection methods that focus on changes
in mean, detecting changes in velocity requires a specialized approach due to
continuity constraints and parameter dependencies, which frustrate popular
algorithms like binary segmentation and dynamic programming. To overcome these
difficulties, we introduce a specialized penalty function to balance
improvements in likelihood due to model complexity, and a Markov Chain Monte
Carlo (MCMC)-based approach with tailored proposal mechanisms for efficient
parameter exploration. Our method is particularly suited for analyzing
intracellular transport data, where the multidimensional trajectories of
microscale cargo are driven by teams of molecular motors that undergo complex
biophysical transitions. To ensure biophysical realism in the results, we
introduce a speed penalty that discourages overfitted of short noisy segments
while maintaining consistency in the large-sample limit. Additionally, we
introduce a summary statistic called the Cumulative Speed Allocation, which is
robust with respect to idiosyncracies of changepoint detection while
maintaining the ability to discriminate between biophysically distinct
populations.

</details>


### [12] [Flexible model for varying levels of zeros and outliers in count data](https://arxiv.org/abs/2510.27365)
*Touqeer Ahmad,Abid Hussain*

Main category: stat.ME

TL;DR: 提出了一种基于广义帕累托分布的离散模型作为零膨胀负二项式模型的稳健替代方案，特别适用于处理具有异常值、过度离散和大量零值的计数数据。


<details>
  <summary>Details</summary>
Motivation: 传统方法如零膨胀负二项式模型在处理具有异常值、过度离散和大量零值的计数数据时，特别是在尾部区域，往往无法提供满意的拟合效果。

Method: 通过将广义帕累托分布及其零膨胀版本扩展到离散领域，构建了一个灵活的重尾离散模型框架，有效处理过度离散和零膨胀问题。

Result: 通过模拟研究和实际应用验证，提出的模型在拟合优度方面始终优于传统的负二项式和零膨胀负二项式回归，特别适用于具有大量零值和异常值的数据集。

Conclusion: 该框架为建模复杂计数数据提供了一个强大而灵活的选择，特别适用于处理重尾、过度离散和零膨胀的数据特征。

Abstract: Count regression models are necessary for examining discrete dependent
variables alongside covariates. Nonetheless, when data display outliers,
overdispersion, and an abundance of zeros, traditional methods like the
zero-inflated negative binomial (ZINB) model sometimes do not yield a
satisfactory fit, especially in the tail regions. This research presents a
versatile, heavy-tailed discrete model as a resilient substitute for the ZINB
model. The suggested framework is built by extending the generalized Pareto
distribution and its zero-inflated version to the discrete domain. This
formulation efficiently addresses both overdispersion and zero inflation,
providing increased flexibility for heavy-tailed count data. Through intensive
simulation studies and real-world implementations, the proposed models are
thoroughly tested to see how well they work. The results show that our models
always do better than classic negative binomial and zero-inflated negative
binomial regressions when it comes to goodness-of-fit. This is especially true
for datasets with a lot of zeros and outliers. These results highlight the
proposed framework's potential as a strong and flexible option for modeling
complicated count data.

</details>


### [13] [Mastering an Accurate and Generalizable Simulation-Based Method to Obtain Bias-corrected Point Estimates and Sampling Variance for Any Effect Sizes](https://arxiv.org/abs/2510.27519)
*Shinichi Nakagawa,Ayumi Mizuno,Coralie Williams,Santiago Ortega,Szymon M. Drobniak,Malgorzata Lagisz,Yefeng Yang,Alistair M. Senior,Daniel W. A. Noble,Erick Lundgren*

Main category: stat.ME

TL;DR: SAFE bootstrap是一种简单易用的模拟方法，用于估计效应量的抽样方差，替代了传统复杂的delta方法推导过程。


<details>
  <summary>Details</summary>
Motivation: 传统通过泰勒展开（delta方法）推导效应量抽样方差公式对非统计学家来说很困难，需要一种更直观的方法。

Method: 通过四个简单步骤：拟合模型、从简单抽样分布中抽取数据、转换为效应量、汇总模拟结果，得到偏差校正的点估计和标准误。

Result: SAFE bootstrap能够为任何效应量统计量（包括标准化均值差、对数优势比等）提供偏差校正的估计和抽样方差，适用于连续和离散结果。

Conclusion: SAFE bootstrap不仅增强了人们对效应量抽样方差的理解，还展示了基于模拟方法的强大能力，可用于推导任何效应量及其抽样方差。

Abstract: Meta-analyses require an effect-size estimate and its corresponding sampling
variance from primary studies. In some cases, estimators for the sampling
variance of a given effect size statistic may not exist, necessitating the
derivation of a new formula for sampling variance. Traditionally, sampling
variance formulas are obtained via hand-derived Taylor expansions (the delta
method), though this procedure can be challenging for non-statisticians.
Building on the idea of single-fit parametric resampling, we introduce SAFE
bootstrap: a Single-fit, Accurate, Fast, and Easy simulation recipe that
replaces potentially complex algebra with four intuitive steps: fit, draw,
transform, and summarise. In a unified framework, the SAFE bootstrap yields
bias-corrected point estimates and standard errors for any effect size
statistic, regardless of whether the outcome is continuous or discrete. SAFE
bootstrapping works by drawing once from a simple sampling model (normal,
binomial, etc.), converting each replicate into any effect size of interest and
then calculating the bias and sampling variance from simulated data. We
demonstrate how to implement the SAFE bootstrap for a simple example first, and
then for common effect sizes, such as the standardised mean difference and log
odds ratio, as well as for less common effect sizes. With some additional
coding, SAFE can also handle zero values and small sample sizes. Our tutorial,
with R code supplements, should not only enhance understanding of sampling
variance for effect sizes, but also serve as an introduction to the power of
simulation-based methods for deriving any effect size with bias correction and
its associated sampling variance.

</details>


### [14] [Refining capture-recapture methods to estimate case counts in a finite population setting](https://arxiv.org/abs/2510.27580)
*Michael Doerfler,Wenhao Mao,Lin Ge,Yuzi Zhang,Timothy L. Lash,Kevin C. Ward,Lance A. Waller,Robert H. Lyles*

Main category: stat.ME

TL;DR: 本文提出了一种改进的疾病监测策略，通过将非代表性数据流与随机样本（锚定流）结合，使用捕获-再捕获估计器进行有限封闭人群的疾病监测，并引入了有限总体校正方法以提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 针对有限封闭人群中疾病监测的挑战，特别是在第一个数据流仅记录阳性结果而锚定流记录阳性和阴性的常见情况下，需要解决非代表性数据流和有限总体效应的问题。

Method: 提出了两种有限总体校正方法用于推断，包括FPC调整的贝叶斯可信区间，并与现有方法进行模拟比较。

Result: 模拟结果显示有限总体校正方法能显著提高估计精度，并在乳腺癌复发数据的实际应用中验证了方法的有效性。

Conclusion: 有限总体校正方法在疾病监测中具有重要价值，能够提高非代表性数据流监测的精确度，特别是在有限封闭人群的背景下。

Abstract: In this paper, we expand upon and refine a monitoring strategy proposed for
surveillance of diseases in finite, closed populations. This monitoring
strategy consists of augmenting an arbitrarily non-representative data stream
(such as a voluntary flu testing program) with a random sample (referred to as
an "anchor stream"). This design allows for the use of traditional
capture-recapture (CRC) estimators, as well as recently proposed anchor stream
estimators that more efficiently utilize the data. Here, we focus on a
particularly common situation in which the first data stream only records
positive test results, while the anchor stream documents both positives and
negatives. Due to the non-representative nature of the first data stream, along
with the fact that inference is being performed on a finite, closed population,
there are standard and non-standard finite population effects at play. Here, we
propose two methods of incorporating finite population corrections (FPCs) for
inference, along with an FPC-adjusted Bayesian credible interval. We compare
these approaches with existing methods through simulation and demonstrate that
the FPC adjustments can lead to considerable gains in precision. Finally, we
provide a real data example by applying these methods to estimating the breast
cancer recurrence count among Metro Atlanta-area patients in the Georgia Cancer
Registry-based Cancer Recurrence Information and Surveillance Program (CRISP)
database.

</details>


### [15] [Testing Inequalities Linear in Nuisance Parameters](https://arxiv.org/abs/2510.27633)
*Gregory Fletcher Cox,Xiaoxia Shi,Yuya Shimizu*

Main category: stat.ME

TL;DR: 提出一种针对部分识别干扰参数线性不等式的新检验方法，使用两步检验统计量和数据依赖自由度的卡方临界值，无需调参，具有实用价值。


<details>
  <summary>Details</summary>
Motivation: 解决线性无条件矩（不）等式模型中的子向量推断、模型设定检验以及线性规划约束参数推断等问题中的假设检验需求。

Method: 采用两步检验统计量，使用数据依赖自由度的卡方临界值，通过基本公式计算，无需调参。

Result: 建立了检验的均匀渐近有效性，通过模拟验证了有限样本下的检验水平和功效，并在女性劳动供给对福利政策改革的实证应用中进行了说明。

Conclusion: 该检验方法结构简单、实现方便，在理论和实证应用中均表现出良好性能，适合实际应用。

Abstract: This paper proposes a new test for inequalities that are linear in possibly
partially identified nuisance parameters. This type of hypothesis arises in a
broad set of problems, including subvector inference for linear unconditional
moment (in)equality models, specification testing of such models, and inference
for parameters bounded by linear programs. The new test uses a two-step test
statistic and a chi-squared critical value with data-dependent degrees of
freedom that can be calculated by an elementary formula. Its simple structure
and tuning-parameter-free implementation make it attractive for practical use.
We establish uniform asymptotic validity of the test, demonstrate its
finite-sample size and power in simulations, and illustrate its use in an
empirical application that analyzes women's labor supply in response to a
welfare policy reform.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [16] [Minimax-Optimal Two-Sample Test with Sliced Wasserstein](https://arxiv.org/abs/2510.27498)
*Binh Thuan Tran,Nicolas Schreuder*

Main category: stat.ML

TL;DR: 提出了基于切片Wasserstein距离的置换检验方法，建立了有限样本理论保证，在保持计算效率的同时达到最小最大分离率。


<details>
  <summary>Details</summary>
Motivation: 切片Wasserstein距离在统计保证和计算效率之间提供了良好平衡，但其在假设检验中的理论基础仍有限，需要填补这一空白。

Method: 使用置换检验框架结合切片Wasserstein距离，分析投影数量与统计功效之间的权衡关系。

Result: 检验方法继承了置换原理的有限样本第一类错误控制，建立了非渐近功效边界，在多项分布和有界支撑备择下达到最小最大分离率n^{-1/2}。

Conclusion: 该检验方法结合了有限样本有效性、竞争性功效和可扩展性，且无需核调优，在所有考虑场景中表现一致良好。

Abstract: We study the problem of nonparametric two-sample testing using the sliced
Wasserstein (SW) distance. While prior theoretical and empirical work indicates
that the SW distance offers a promising balance between strong statistical
guarantees and computational efficiency, its theoretical foundations for
hypothesis testing remain limited. We address this gap by proposing a
permutation-based SW test and analyzing its performance. The test inherits
finite-sample Type I error control from the permutation principle. Moreover, we
establish non-asymptotic power bounds and show that the procedure achieves the
minimax separation rate $n^{-1/2}$ over multinomial and bounded-support
alternatives, matching the optimal guarantees of kernel-based tests while
building on the geometric foundations of Wasserstein distances. Our analysis
further quantifies the trade-off between the number of projections and
statistical power. Finally, numerical experiments demonstrate that the test
combines finite-sample validity with competitive power and scalability, and --
unlike kernel-based tests, which require careful kernel tuning -- it performs
consistently well across all scenarios we consider.

</details>


### [17] [Overspecified Mixture Discriminant Analysis: Exponential Convergence, Statistical Guarantees, and Remote Sensing Applications](https://arxiv.org/abs/2510.27056)
*Arman Bolatov,Alan Legg,Igor Melnykov,Amantay Nurlanuly,Maxat Tezekbayev,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 该研究分析了混合判别分析(MDA)在过拟合情况下的分类误差，证明了在适当初始化下，EM算法能快速收敛到贝叶斯风险，并在有限样本下达到n^{-1/2}的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究MDA在混合组件数量超过实际数据分布时的分类误差，这在图像和文本分类等复杂数据场景中经常被经验性地使用，但缺乏理论分析。

Method: 使用两类高斯混合模型拟合单高斯生成的数据，分析EM算法的收敛性和统计分类误差，并在遥感数据集上进行实验验证。

Result: 在适当初始化下，EM算法在总体水平上以指数速度收敛到贝叶斯风险；在有限样本下，分类误差以n^{-1/2}的速率收敛到贝叶斯风险。

Conclusion: 为理解过拟合MDA的性能提供了严格的理论框架，验证了其在复杂数据设置中的有效性。

Abstract: This study explores the classification error of Mixture Discriminant Analysis
(MDA) in scenarios where the number of mixture components exceeds those present
in the actual data distribution, a condition known as overspecification. We use
a two-component Gaussian mixture model within each class to fit data generated
from a single Gaussian, analyzing both the algorithmic convergence of the
Expectation-Maximization (EM) algorithm and the statistical classification
error. We demonstrate that, with suitable initialization, the EM algorithm
converges exponentially fast to the Bayes risk at the population level.
Further, we extend our results to finite samples, showing that the
classification error converges to Bayes risk with a rate $n^{-1/2}$ under mild
conditions on the initial parameter estimates and sample size. This work
provides a rigorous theoretical framework for understanding the performance of
overspecified MDA, which is often used empirically in complex data settings,
such as image and text classification. To validate our theory, we conduct
experiments on remote sensing datasets.

</details>


### [18] [Optimal Convergence Analysis of DDPM for General Distributions](https://arxiv.org/abs/2510.27562)
*Yuchen Jiao,Yuchen Zhou,Gen Li*

Main category: stat.ML

TL;DR: 本文对DDPM采样器进行了改进的收敛性分析，在一般分布假设下建立了近乎最优的收敛速率，显著改进了已知的d²/T²速率，并揭示了DDPM和DDIM在维度依赖上的相似性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于分数的扩散模型在生成高质量样本方面取得了显著经验成功，但DDPM的理论理解，特别是其收敛性质，仍然有限。需要对其收敛性进行更严格的理论分析。

Method: 引入了一个由常数L参数化的松弛平滑条件，该条件对许多实际分布（如高斯混合模型）来说较小。在准确分数估计的情况下分析DDPM采样器的收敛性能。

Result: 证明DDPM采样器在Kullback-Leibler散度下实现了Õ(d min{d,L²}/T²)的收敛速率，当L < √d时显著优于已知的d²/T²速率。通过建立匹配下界，表明该分析对广泛目标分布是紧致的。

Conclusion: DDPM和DDIM在维度依赖上具有相同的特性，这引发了为什么DDIM在经验上通常更快的有趣问题。该收敛分析为理解扩散模型的性能提供了理论基础。

Abstract: Score-based diffusion models have achieved remarkable empirical success in
generating high-quality samples from target data distributions. Among them, the
Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used
samplers, generating samples via estimated score functions. Despite its
empirical success, a tight theoretical understanding of DDPM -- especially its
convergence properties -- remains limited.
  In this paper, we provide a refined convergence analysis of the DDPM sampler
and establish near-optimal convergence rates under general distributional
assumptions. Specifically, we introduce a relaxed smoothness condition
parameterized by a constant $L$, which is small for many practical
distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler
with accurate score estimates achieves a convergence rate of
$$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)~\text{in
Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the
number of iterations, and $\widetilde{O}$ hides polylogarithmic factors in $T$.
This result substantially improves upon the best-known $d^2/T^2$ rate when $L <
\sqrt{d}$. By establishing a matching lower bound, we show that our convergence
analysis is tight for a wide array of target distributions. Moreover, it
reveals that DDPM and DDIM share the same dependence on $d$, raising an
interesting question of why DDIM often appears empirically faster.

</details>


### [19] [Decreasing Entropic Regularization Averaged Gradient for Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.27340)
*Ferdinand Genans,Antoine Godichon-Baggioni,François-Xavier Vialard,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 提出了DRAG算法，通过自适应降低熵正则化来加速半离散最优传输问题的求解，相比固定正则化方法获得更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 熵正则化虽然能加速最优传输问题的求解，但会引入偏差。为了在保持加速效果的同时减少偏差，需要自适应降低正则化参数。

Method: 提出DRAG（递减熵正则化平均梯度）算法，在随机梯度下降过程中随着优化步数增加而降低正则化参数。

Result: 理论分析表明DRAG相比固定正则化方案具有加速效果，在OT成本和势函数估计上达到无偏的O(1/t)样本和迭代复杂度，在OT映射上达到O(1/√t)收敛率。

Conclusion: 数值实验验证了DRAG的有效性，展示了其在实践中的优势，为自适应正则化方法提供了理论保证。

Abstract: Adding entropic regularization to Optimal Transport (OT) problems has become
a standard approach for designing efficient and scalable solvers. However,
regularization introduces a bias from the true solution. To mitigate this bias
while still benefiting from the acceleration provided by regularization, a
natural solver would adaptively decrease the regularization as it approaches
the solution. Although some algorithms heuristically implement this idea, their
theoretical guarantees and the extent of their acceleration compared to using a
fixed regularization remain largely open. In the setting of semi-discrete OT,
where the source measure is continuous and the target is discrete, we prove
that decreasing the regularization can indeed accelerate convergence. To this
end, we introduce DRAG: Decreasing (entropic) Regularization Averaged Gradient,
a stochastic gradient descent algorithm where the regularization decreases with
the number of optimization steps. We provide a theoretical analysis showing
that DRAG benefits from decreasing regularization compared to a fixed scheme,
achieving an unbiased $\mathcal{O}(1/t)$ sample and iteration complexity for
both the OT cost and the potential estimation, and a $\mathcal{O}(1/\sqrt{t})$
rate for the OT map. Our theoretical findings are supported by numerical
experiments that validate the effectiveness of DRAG and highlight its practical
advantages.

</details>


### [20] [On the Equivalence of Optimal Transport Problem and Action Matching with Optimal Vector Fields](https://arxiv.org/abs/2510.27385)
*Nikita Kornilov,Alexander Korotin*

Main category: stat.ML

TL;DR: 本文比较了流匹配(FM)和动作匹配(AM)两种生成建模方法，指出仅考虑最优向量场可以在动作匹配方法中实现最优输运(OT)。


<details>
  <summary>Details</summary>
Motivation: 研究如何在生成建模中实现最优输运，比较流匹配和动作匹配两种方法的差异和联系。

Method: 分析流匹配方法中仅考虑最优向量场的情况，探讨其在动作匹配方法中实现最优输运的机制。

Result: 发现仅考虑最优向量场可以在动作匹配方法中实现最优输运，这与流匹配方法不同，后者需要手动选择分布间的插值。

Conclusion: 动作匹配方法通过直接学习给定分布序列的向量场，能够更自然地实现最优输运，而不需要手动选择插值路径。

Abstract: Flow Matching (FM) method in generative modeling maps arbitrary probability
distributions by constructing an interpolation between them and then learning
the vector field that defines ODE for this interpolation. Recently, it was
shown that FM can be modified to map distributions optimally in terms of the
quadratic cost function for any initial interpolation. To achieve this, only
specific optimal vector fields, which are typical for solutions of Optimal
Transport (OT) problems, need to be considered during FM loss minimization. In
this note, we show that considering only optimal vector fields can lead to OT
in another approach: Action Matching (AM). Unlike FM, which learns a vector
field for a manually chosen interpolation between given distributions, AM
learns the vector field that defines ODE for an entire given sequence of
distributions.

</details>


### [21] [Interpretable Model-Aware Counterfactual Explanations for Random Forest](https://arxiv.org/abs/2510.27397)
*Joshua S. Harvey,Guanchao Feng,Sai Anusha Meesala,Tina Zhao,Dhagash Mehta*

Main category: stat.ML

TL;DR: 提出了一种基于随机森林的因果反事实解释方法，通过相似性学习寻找反事实案例，并利用随机森林分区计算特征重要性，相比Shapley值能生成更稀疏和有用的解释。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在金融等受监管行业应用受限，因为现有解释方法如Shapley值与因果解释需求不匹配。反事实案例解释更直观和可操作，但寻找合适反事实案例和解释特征重要性仍是挑战。

Method: 将反事实搜索和解释问题转化为相似性学习，利用随机森林模型学习到的表示。找到反事实后，通过计算从原始实例到达反事实需要跨越的随机森林分区来确定特征重要性。

Result: 在MNIST手写数字数据集和德国信用数据集上的实验表明，该方法生成的解释比Shapley值更稀疏和有用。

Conclusion: 基于随机森林的反事实解释方法能够提供更符合实际需求的因果解释，在受监管行业具有应用潜力。

Abstract: Despite their enormous predictive power, machine learning models are often
unsuitable for applications in regulated industries such as finance, due to
their limited capacity to provide explanations. While model-agnostic frameworks
such as Shapley values have proved to be convenient and popular, they rarely
align with the kinds of causal explanations that are typically sought after.
Counterfactual case-based explanations, where an individual is informed of
which circumstances would need to be different to cause a change in outcome,
may be more intuitive and actionable. However, finding appropriate
counterfactual cases is an open challenge, as is interpreting which features
are most critical for the change in outcome. Here, we pose the question of
counterfactual search and interpretation in terms of similarity learning,
exploiting the representation learned by the random forest predictive model
itself. Once a counterfactual is found, the feature importance of the
explanation is computed as a function of which random forest partitions are
crossed in order to reach it from the original instance. We demonstrate this
method on both the MNIST hand-drawn digit dataset and the German credit
dataset, finding that it generates explanations that are sparser and more
useful than Shapley values.

</details>


### [22] [Bayesian Optimization on Networks](https://arxiv.org/abs/2510.27643)
*Wenwen Li,Daniel Sanz-Alonso,Ruiyi Yang*

Main category: stat.ML

TL;DR: 该论文开发了在度量图上进行贝叶斯优化的算法，使用Whittle-Matérn高斯过程先验模型，通过随机偏微分方程定义，适用于网络几何结构。


<details>
  <summary>Details</summary>
Motivation: 研究在度量图上进行优化的问题，适用于目标函数评估成本高或仅作为黑盒可用的应用场景。

Method: 采用Whittle-Matérn高斯过程先验模型，通过随机偏微分方程在度量图上定义，使用有限元表示，并建立贝叶斯优化算法来顺序更新高斯过程代理模型。

Result: 为充分平滑的目标函数建立了遗憾界，并在平滑度未知的情况下进行了分析。数值结果表明该算法在合成度量图和电信网络上的贝叶斯反演中有效。

Conclusion: 提出的贝叶斯优化算法在度量图上表现出有效性，特别适用于目标函数评估昂贵或作为黑盒的应用场景。

Abstract: This paper studies optimization on networks modeled as metric graphs.
Motivated by applications where the objective function is expensive to evaluate
or only available as a black box, we develop Bayesian optimization algorithms
that sequentially update a Gaussian process surrogate model of the objective to
guide the acquisition of query points. To ensure that the surrogates are
tailored to the network's geometry, we adopt Whittle-Mat\'ern Gaussian process
prior models defined via stochastic partial differential equations on metric
graphs. In addition to establishing regret bounds for optimizing sufficiently
smooth objective functions, we analyze the practical case in which the
smoothness of the objective is unknown and the Whittle-Mat\'ern prior is
represented using finite elements. Numerical results demonstrate the
effectiveness of our algorithms for optimizing benchmark objective functions on
a synthetic metric graph and for Bayesian inversion via maximum a posteriori
estimation on a telecommunication network.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [23] [Robust fuzzy clustering for high-dimensional multivariate time series with outlier detection](https://arxiv.org/abs/2510.26982)
*Ziling Ma,Ángel López-Oriona,Hernando Ombao,Ying Sun*

Main category: stat.CO

TL;DR: RFCPCA是一种针对多元时间序列的鲁棒模糊子空间聚类方法，能够处理时间依赖性、不等序列长度、高维度和噪声污染等问题，在驾驶员困倦EEG数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列中的状态边界通常是模糊的，现有算法大多针对静态低维数据，难以处理时间依赖性、不等序列长度、高维度和噪声污染等挑战。

Method: 使用鲁棒模糊子空间聚类方法，通过学习成员感知子空间、适应不等长度和中等高维度、通过修剪、指数重加权和专用噪声簇实现鲁棒性，并自动选择所有必需的超参数。

Result: 在驾驶员困倦EEG数据上，RFCPCA相比相关方法提高了聚类准确性，并提供了更可靠的成员不确定性和异常值结构表征。

Conclusion: RFCPCA能够捕捉潜在时间结构，提供校准的成员不确定性，标记序列级异常值，并在污染条件下保持稳定，是首个同时具备这些特性的多元时间序列聚类方法。

Abstract: Fuzzy clustering provides a natural framework for modeling partial
memberships, particularly important in multivariate time series (MTS) where
state boundaries are often ambiguous. For example, in EEG monitoring of driver
alertness, neural activity evolves along a continuum (from unconscious to fully
alert, with many intermediate levels of drowsiness) so crisp labels are
unrealistic and partial memberships are essential. However, most existing
algorithms are developed for static, low-dimensional data and struggle with
temporal dependence, unequal sequence lengths, high dimensionality, and
contamination by noise or artifacts. To address these challenges, we introduce
RFCPCA, a robust fuzzy subspace-clustering method explicitly tailored to MTS
that, to the best of our knowledge, is the first of its kind to simultaneously:
(i) learn membership-informed subspaces, (ii) accommodate unequal lengths and
moderately high dimensions, (iii) achieve robustness through trimming,
exponential reweighting, and a dedicated noise cluster, and (iv) automatically
select all required hyperparameters. These components enable RFCPCA to capture
latent temporal structure, provide calibrated membership uncertainty, and flag
series-level outliers while remaining stable under contamination. On driver
drowsiness EEG, RFCPCA improves clustering accuracy over related methods and
yields a more reliable characterization of uncertainty and outlier structure in
MTS.

</details>


### [24] [Differential Set Selection via Confidence-Guided Entropy Minimization](https://arxiv.org/abs/2510.27479)
*María del Carmen Romero,Mariana del Fresno,Alejandro Clausse*

Main category: stat.CO

TL;DR: 提出一种高效迭代方法，通过顺序选择变量来识别最小预测子集，使用置信区间处理有限样本不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决识别离散独立变量中最小子集来最佳预测二元分类的挑战，这是一个NP完全问题。

Method: 基于统计显著性的条件熵减少，使用置信区间考虑有限样本不确定性，顺序选择变量。

Result: 在模拟数据测试中，该方法能正确识别有影响力的变量，同时最小化虚假选择，即使在样本量较小的情况下也有效。

Conclusion: 该方法为这个NP完全问题提供了一个计算上可行的解决方案。

Abstract: This paper addresses the challenge of identifying a minimal subset of
discrete, independent variables that best predicts a binary class. We propose
an efficient iterative method that sequentially selects variables based on
which one provides the most statistically significant reduction in conditional
entropy, using confidence bounds to account for finite-sample uncertainty.
Tests on simulated data demonstrate the method's ability to correctly identify
influential variables while minimizing spurious selections, even with small
sample sizes, offering a computationally tractable solution to this NP-complete
problem.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [25] [Diabetes Lifestyle Medicine Treatment Assistance Using Reinforcement Learning](https://arxiv.org/abs/2510.26807)
*Yuhan Tang*

Main category: stat.AP

TL;DR: 提出一种基于离线上下文老虎机的方法，从NHANES数据中学习个性化生活方式处方，用于2型糖尿病预防和治疗


<details>
  <summary>Details</summary>
Motivation: 个性化生活方式处方对2型糖尿病防治有益，但受限于专业医生短缺和专业知识差异

Method: 使用离线上下文老虎机方法，通过混合动作Soft Actor-Critic算法从119,555名NHANES参与者数据中学习，最小化Magni血糖风险-奖励函数

Result: 模型生成的生活方式处方与湘雅医院三位认证医师的处方进行了验证

Conclusion: 离线混合动作SAC能够从横断面NHANES数据生成风险感知的生活方式处方，值得进行前瞻性临床验证

Abstract: Type 2 diabetes prevention and treatment can benefit from personalized
lifestyle prescriptions. However, the delivery of personalized lifestyle
medicine prescriptions is limited by the shortage of trained professionals and
the variability in physicians' expertise. We propose an offline contextual
bandit approach that learns individualized lifestyle prescriptions from the
aggregated NHANES profiles of 119,555 participants by minimizing the Magni
glucose risk-reward function. The model encodes patient status and generates
lifestyle medicine prescriptions, which are trained using a mixed-action Soft
Actor-Critic algorithm. The task is treated as a single-step contextual bandit.
The model is validated against lifestyle medicine prescriptions issued by three
certified physicians from Xiangya Hospital. These results demonstrate that
offline mixed-action SAC can generate risk-aware lifestyle medicine
prescriptions from cross-sectional NHANES data, warranting prospective clinical
validation.

</details>


### [26] [A Machine Learning-Based Framework to Shorten the Questionnaire for Assessing Autism Intervention](https://arxiv.org/abs/2510.26808)
*Audrey Dong,Claire Xu,Samuel R. Guo,Kevin Yang,Xue-Jun Kong*

Main category: stat.AP

TL;DR: 开发了一个通用机器学习框架来缩短自闭症治疗评估量表(ATEC)，从77项减少到16项用于治疗追踪，13项用于严重程度评估，同时保持评估准确性。


<details>
  <summary>Details</summary>
Motivation: 照顾者发现77项的ATEC评估量表负担过重，限制了常规监测的使用，需要开发更简短的评估工具。

Method: 使用60名自闭症儿童的纵向ATEC数据，应用特征选择和交叉验证技术，识别最具预测性的项目，基于子集优化、模型可解释性和统计严谨性的方法。

Result: 治疗追踪方面识别出16个项目(原问卷的21%)，保持与总分变化的强相关性；严重程度评估使用13个项目(原问卷的17%)达到80%以上的分类准确率。

Conclusion: 该框架可应用于其他高维心理测量工具，使评估更易获取、频繁和可扩展，为神经发育和精神疾病背景下的AI支持干预提供数据驱动方法。

Abstract: Caregivers of individuals with autism spectrum disorder (ASD) often find the
77-item Autism Treatment Evaluation Checklist (ATEC) burdensome, limiting its
use for routine monitoring. This study introduces a generalizable machine
learning framework that seeks to shorten assessments while maintaining
evaluative accuracy. Using longitudinal ATEC data from 60 autistic children
receiving therapy, we applied feature selection and cross-validation techniques
to identify the most predictive items across two assessment goals: longitudinal
therapy tracking and point-in-time severity estimation. For progress
monitoring, the framework identified 16 items (21% of the original
questionnaire) that retained strong correlation with total score change and
full subdomain coverage. We also generated smaller subsets (1-7 items) for
efficient approximations. For point-in-time severity assessment, our model
achieved over 80% classification accuracy using just 13 items (17% of the
original set). While demonstrated on ATEC, the methodology-based on subset
optimization, model interpretability, and statistical rigor-is broadly
applicable to other high-dimensional psychometric tools. The resulting
framework could potentially enable more accessible, frequent, and scalable
assessments and offer a data-driven approach for AI-supported interventions
across neurodevelopmental and psychiatric contexts.

</details>


### [27] [A generalisation of the signal-to-noise ratio using proper scoring rules](https://arxiv.org/abs/2510.26809)
*Jochen Bröcker,Eviatar Bach*

Main category: stat.AP

TL;DR: 本文基于适当的评分规则，提出了信号噪声比（或可预测成分比RPC）的广义概念，适用于任何可评分的预测类型，并通过合成数据和北大西洋涛动指数季节集合预报进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统信号噪声比概念适用范围有限，需要一种能够适用于各种预测类型（如集合预报、概率预报）的广义定义，以更全面地分析预测系统的信号噪声特性。

Method: 基于适当的评分规则构建广义RPC定义，分别针对连续排序概率评分（CRPS）的集合预报和对数评分的二元事件概率预报进行验证，使用具有预设信号噪声比的合成数据和北大西洋涛动指数的季节集合后报数据。

Result: 对于合成数据，RPC统计量和基于评分规则的统计量在识别异常信号噪声比方面一致，但方差不同，表明统计特性存在差异；对于NAO数据，不同统计量之间的结果更加不确定。

Conclusion: 基于评分规则的广义RPC定义能够有效扩展信号噪声比概念的应用范围，但在实际应用中不同统计量可能给出不一致的结果，需要谨慎解释。

Abstract: A generalised concept of the signal-to-noise ratio (or equivalently the ratio
of predictable components, or RPC) is provided, based on proper scoring rules.
This definition is the natural generalisation of the classical RPC, yet it
allows one to define and analyse the signal-to-noise properties of any type of
forecast that is amenable to scoring, thus drastically widening the
applicability of these concepts. The methodology is illustrated for ensemble
forecasts, scored using the continuous ranked probability score (CRPS), and for
probability forecasts of a binary event, scored using the logarithmic score.
Numerical examples are demonstrated using synthetic data with prescribed
signal-to-noise ratios as well as seasonal ensemble hindcasts of the North
Atlantic Oscillation (NAO) index. For the synthetic data, the RPC statistic as
well as the scoring rule--based ones agree regarding which data sets exhibit
anomalous signal-to-noise ratios, but exhibit different variance, indicating
different statistical properties. For the NAO data, on the other hand, the
results among the different statistics are more equivocal.

</details>


### [28] [Emergent Dynamical Spatial Boundaries in Emergency Medical Services: A Navier-Stokes Framework from First Principles](https://arxiv.org/abs/2510.26810)
*Tatsuru Kikuchi*

Main category: stat.AP

TL;DR: 本文基于流体动力学原理推导出急救响应的连续空间边界，发现响应效果随时间呈指数衰减，并识别出老年人、农村和低收入社区面临系统性更长的响应时间。


<details>
  <summary>Details</summary>
Motivation: 现有急救医疗服务空间覆盖分析方法依赖离散距离缓冲区或临时GIS等时线，缺乏理论基础，需要建立基于第一原理的连续空间边界模型。

Method: 使用流体动力学（纳维-斯托克斯方程）推导连续空间边界，利用10,000个模拟紧急事件估计衰减参数，计算响应效果低于政策相关阈值的临界边界，并通过非参数核回归和传统差异分析进行验证。

Result: 响应效果呈指数衰减：τ(t) = τ₀exp(-κt)；老年人群（85+）平均响应时间8.40分钟，年轻成人（18-44）7.83分钟；33.6%的访问不良事件影响老年人群，尽管他们仅占样本5.2%；指数衰减模型验证有效（MSE比参数模型小8-12倍）。

Conclusion: 该框架识别出面临系统性更长响应时间的脆弱人群，为优化EMS站点布局和资源分配以减少健康差异提供了信息。

Abstract: Emergency medical services (EMS) response times are critical determinants of
patient survival, yet existing approaches to spatial coverage analysis rely on
discrete distance buffers or ad-hoc geographic information system (GIS)
isochrones without theoretical foundation. This paper derives continuous
spatial boundaries for emergency response from first principles using fluid
dynamics (Navier-Stokes equations), demonstrating that response effectiveness
decays exponentially with time: $\tau(t) = \tau_0 \exp(-\kappa t)$, where
$\tau_0$ is baseline effectiveness and $\kappa$ is the temporal decay rate.
Using 10,000 simulated emergency incidents from the National Emergency Medical
Services Information System (NEMSIS), I estimate decay parameters and calculate
critical boundaries $d^*$ where response effectiveness falls below
policy-relevant thresholds. The framework reveals substantial demographic
heterogeneity: elderly populations (85+) experience 8.40-minute average
response times versus 7.83 minutes for younger adults (18-44), with 33.6\% of
poor-access incidents affecting elderly populations despite representing 5.2\%
of the sample. Non-parametric kernel regression validation confirms exponential
decay is appropriate (mean squared error 8-12 times smaller than parametric),
while traditional difference-in-differences analysis validates treatment effect
existence (DiD coefficient = -1.35 minutes, $p < 0.001$). The analysis
identifies vulnerable populations--elderly, rural, and low-income
communities--facing systematically longer response times, informing optimal EMS
station placement and resource allocation to reduce health disparities.

</details>


### [29] [Proxy Variable in OECD Database: Application of Parametric Quantile Regression and Median Based Unit Rayleigh Distribution](https://arxiv.org/abs/2510.26811)
*Iman Mohamed Attia*

Main category: stat.AP

TL;DR: 本文深入探讨了作者先前提出的基于中位数的单位瑞利分布，该新方法专门用于分位数回归分析，帮助研究人员从实际数据中获得有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 开发MBUR分布的目的是为了将先进的统计理论与实际数据分析结果联系起来，特别是在分位数回归分析中提供可行的优势。

Method: 作者使用OECD数据，采用参数化MBUR分位数回归方法，其中响应变量服从MBUR分布。

Result: 研究有效证明了MBUR分布在分位数回归分析中的可行优势，能够为实际数据应用提供有价值的见解。

Conclusion: MBUR分布作为一种创新的统计工具，成功连接了统计理论与实际数据分析，在分位数回归应用中展现出良好的潜力。

Abstract: This paper presents an in-depth exploration of the innovative Median-based
unit Rayleigh (MBUR) distribution, previously introduced by the author. This
new approach is specifically designed for conducting quantile regression
analysis, enabling researchers to gain valuable insights into real-world data
applications. The author effectively demonstrates the feasible advantage of the
MBUR distribution, highlighting its potential to connect advanced statistical
theory with meaningful results in data analysis. The author utilized OECD data
in employing the parametric MBUR quantile regression using the response
variables which are distributed as MBUR.

</details>


### [30] [Towards Gaussian processes modelling to study the late effects of radiotherapy in children and young adults with brain tumours](https://arxiv.org/abs/2510.26814)
*Angela Davey,Arthur Leroy,Eliana Vasquez Osorio,Kate Vaughan,Peter Clayton,Marcel van Herk,Mauricio A Alvarez,Martin McCabe,Marianne Aznar*

Main category: stat.AP

TL;DR: 使用高斯过程模型对儿童癌症幸存者的胰岛素样生长因子1（IGF-1）测量值进行纵向预测，以解决常规监测数据稀疏和不规则的问题。


<details>
  <summary>Details</summary>
Motivation: 儿童癌症幸存者需要终身监测放疗副作用，但常规监测数据通常稀疏、不规则且存在不准确性，现有方法往往孤立分析测量值或使用简单线性关系填补缺失时间点。

Method: 采用高斯过程（GP）建模方法，使用23名患者的训练数据（中位数4个时间点，范围1-16）进行群体和个体预测，以IGF-1测量值作为测试案例。

Result: 识别出与文献报道值范围一致的趋势，在8个测试案例中，两种方法的个体预测平均均方根误差分别为31.9 ng/ml（范围10.1-62.3）和27.4 ng/ml（范围0.02-66.1）。

Conclusion: 高斯过程建模可能克服常规纵向数据的局限性，促进放疗晚期效应的分析。

Abstract: Survivors of childhood cancer need lifelong monitoring for side effects from
radiotherapy. However, longitudinal data from routine monitoring is often
infrequently and irregularly sampled, and subject to inaccuracies. Due to this,
measurements are often studied in isolation, or simple relationships (e.g.,
linear) are used to impute missing timepoints. In this study, we investigated
the potential role of Gaussian Processes (GP) modelling to make
population-based and individual predictions, using insulin-like growth factor 1
(IGF-1) measurements as a test case. With training data of 23 patients with a
median (range) of 4 (1-16) timepoints we identified a trend within the range of
literature reported values. In addition, with 8 test cases, individual
predictions were made with an average root mean squared error of 31.9 (10.1 -
62.3) ng/ml and 27.4 (0.02 - 66.1) ng/ml for two approaches. GP modelling may
overcome limitations of routine longitudinal data and facilitate analysis of
late effects of radiotherapy.

</details>


### [31] [Toward precision soil health: A regional framework for site-specific management across Missouri](https://arxiv.org/abs/2510.26815)
*Dipal Shah,Jordon Wade,Timothy Haithcoat,Robert Myers,Kelly Wilson*

Main category: stat.AP

TL;DR: 该研究开发了一个基于数据驱动的土壤聚类框架，将密苏里州划分为10个土壤健康管理区，为精准土壤管理提供依据。


<details>
  <summary>Details</summary>
Motivation: 密苏里州多样的地形限制了通用管理建议的有效性，现有土壤分组系统缺乏分辨率，需要基于数据的特定地点洞察来指导定制化干预措施。

Method: 利用高分辨率SSURGO数据集，处理0-30cm根区的土壤特性，采用变分自编码器和K-means聚类进行多变量分析，将具有相似特性的土壤分组。

Result: 确定了10个不同的土壤健康管理区，这些区域由黏土含量、有机质、pH值和可用水容量的独特组合定义，根系深度限制和饱和导水率是驱动土壤分化的主要变量。

Conclusion: 该框架将复杂数据分析与可操作的、针对特定地点的建议相结合，使保护规划师和农学家能够优化管理实践，提高全州的资源利用效率。

Abstract: Effective soil health management is crucial for sustaining agriculture,
adopting ecosystem resilience, and preserving water quality. However,
Missouri's diverse landscapes limit the effectiveness of broad generalized
management recommendations. The lack of resolution in existing soil grouping
systems necessitates data driven, site specific insights to guide tailored
interventions. To address these critical challenges, a regional soil clustering
framework designed to support precision soil health management strategies
across the state. The methodology leveraged high resolution SSURGO dataset,
explicitly processing soil properties aggregated across the 0 to 30 cm root
zone. Multivariate analysis incorporating a variational autoencoder and KMeans
clustering was used to group soils with similar properties. The derived
clusters were validated using statistical metrics, including silhouette scores
and checks against existing taxonomic units, to confirm their spatial
coherence. This approach enabled us to delineate soil groups that capture
textures, hydraulic properties, chemical fertility, and biological indicators
unique to Missouri's diverse agroecological regions. The clustering map
identified ten distinct soil health management zones. This alignment of 10
clusters was selected as optimal because it was sufficiently large to capture
inherited soil patterns while remaining manageable for practical statewide
application. Rooting depth limitation and saturated hydraulic conductivity
emerged as principal variables driving soil differentiation. Each management
zone is defined by a unique combination of clay, organic matter, pH, and
available water capacity. This framework bridges sophisticated data analysis
with actionable, site targeted recommendations, enabling conservation planners,
and agronomists to optimize management practices and enhance resource
efficiency statewide.

</details>


### [32] [Systematic Absence of Low-Confidence Nighttime Fire Detections in VIIRS Active Fire Product: Evidence of Undocumented Algorithmic Filtering](https://arxiv.org/abs/2510.26816)
*Rohit Rajendra Dhage*

Main category: stat.AP

TL;DR: VIIRS活跃火产品在夜间观测中完全缺失低置信度分类，这是算法约束而非地球物理现象，影响了27.9%的火点检测数据。


<details>
  <summary>Details</summary>
Motivation: 发现VIIRS火点产品的置信度分类方案存在未记录的系统性模式，影响全球火点监测的准确性。

Method: 分析了21,540,921个火点检测数据，使用机器学习逆向工程、自助法模拟和时空分析验证模式。

Result: 在6,007,831个夜间火点中，零个被分类为低置信度，而统计独立预期应为696,908个。夜间约295K以下的火点可能被完全排除而非标记为低置信度。

Conclusion: 建议在VIIRS用户指南中明确记录此算法约束，并为受影响的分析制定重处理策略。

Abstract: The Visible Infrared Imaging Radiometer Suite (VIIRS) active fire product is
widely used for global fire monitoring, yet its confidence classification
scheme exhibits an undocumented systematic pattern. Through analysis of
21,540,921 fire detections spanning one year (January 2023 - January 2024), I
demonstrate a complete absence of low-confidence classifications during
nighttime observations. Of 6,007,831 nighttime fires, zero were classified as
low confidence, compared to an expected 696,908 under statistical independence
(chi-squared = 1,474,795, p < 10^-15, Z = -833). This pattern persists globally
across all months, latitude bands, and both NOAA-20 and Suomi-NPP satellites.
Machine learning reverse-engineering (88.9% accuracy), bootstrap simulation
(1,000 iterations), and spatial-temporal analysis confirm this is an
algorithmic constraint rather than a geophysical phenomenon. Brightness
temperature analysis reveals nighttime fires below approximately 295K are
likely excluded entirely rather than flagged as low-confidence, while daytime
fires show normal confidence distributions. This undocumented behavior affects
27.9% of all VIIRS fire detections and has significant implications for fire
risk assessment, day-night detection comparisons, confidence-weighted analyses,
and any research treating confidence levels as uncertainty metrics. I recommend
explicit documentation of this algorithmic constraint in VIIRS user guides and
reprocessing strategies for affected analyses.

</details>


### [33] [Functional Analysis of Loss-development Patterns in P&C Insurance](https://arxiv.org/abs/2510.27204)
*Arthur Charpentier,Qiheng Guo,Mike Ludkovski*

Main category: stat.AP

TL;DR: 使用功能数据分析方法分析NAIC Schedule P损失三角形中的损失发展模式，提出基于偏最小二乘回归的功能模型来预测部分发展的损失曲线，并通过功能自助法量化未来不确定性。


<details>
  <summary>Details</summary>
Motivation: 分析工人赔偿保险线的损失发展模式，识别异常曲线，并开发能够提供准确功能预测区间的概率预测框架。

Method: 采用功能数据深度分析发展模式相似性和差异，提出基于PCA得分的偏最小二乘回归功能模型，结合功能自助法进行不确定性量化。

Result: 该方法相比传统链梯法具有更好的概率评分，能够提供准确的功能预测区间。

Conclusion: 功能数据分析方法在保险损失预测中表现优异，能够有效处理曲线数据并提供可靠的不确定性量化。

Abstract: We analyze loss development in NAIC Schedule P loss triangles using
functional data analysis methods. Adopting the functional viewpoint, our
dataset comprises 3300+ curves of incremental loss ratios (ILR) of workers'
compensation lines over 24 accident years. Relying on functional data depth, we
first study similarities and differences in development patterns based on
company-specific covariates, as well as identify anomalous ILR curves.
  The exploratory findings motivate the probabilistic forecasting framework
developed in the second half of the paper. We propose a functional model to
complete partially developed ILR curves based on partial least squares
regression of PCA scores. Coupling the above with functional bootstrapping
allows us to quantify future ILR uncertainty jointly across all future lags. We
demonstrate that our method has much better probabilistic scores relative to
Chain Ladder and in particular can provide accurate functional predictive
intervals.

</details>


### [34] [Bias correction of satellite and reanalysis products for daily rainfall occurrence and intensity](https://arxiv.org/abs/2510.27456)
*John Bagiliko,David Stern,Denis Ndanguza,Francis Feehi Torgbor,Danny Parsons,Samuel Owusu Ansah*

Main category: stat.AP

TL;DR: 本研究评估了多种偏差校正方法在卫星和再分析降雨产品中的应用效果，发现统计方法表现最佳，但所有方法在检测强降雨事件方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 卫星和再分析降雨产品在数据稀疏地区可作为补充或替代数据源，但其显著偏差需要校正，以提升降雨检测和强度估计的准确性。

Method: 评估了统计方法（LOCI、QM）、机器学习（SVR、GPR）和混合技术（LOCI-GPR、QM-GPR）等偏差校正方法，应用于加纳和赞比亚38个站点的7种卫星降雨产品。

Result: ENACTS产品最易校正，在赞比亚70%以上站点成功减少日降雨量平均误差；统计方法（QM和LOCI）整体表现最优，但QM倾向于高估降雨值；所有校正产品在检测干旱日方面表现良好，但在检测强降雨事件方面效果不佳。

Conclusion: 统计偏差校正方法在降雨产品校正中表现最佳，但在强降雨事件检测方面仍需改进，这是未来研究的关键方向。

Abstract: Satellite and reanalysis rainfall products (SREs) can serve as valuable
complements or alternatives in data-sparse regions, but their significant
biases necessitate correction. This study rigorously evaluates a suite of bias
correction (BC) methods, including statistical approaches (LOCI, QM), machine
learning (SVR, GPR), and hybrid techniques (LOCI-GPR, QM-GPR), applied to seven
SREs across 38 stations in Ghana and Zambia, aimed at assessing their
performance in rainfall detection and intensity estimation. Results indicate
that the ENACTS product, which uniquely integrates a large number of station
records, was the most corrigible SRE; in Zambia, nearly all BC methods
successfully reduced the mean error on daily rainfall amounts at over 70% of
stations. However, this performance requires further validation at independent
stations not incorporated into the ENACTS product. Overall, the statistical
methods (QM and LOCI) generally outperformed other techniques, although QM
exhibited a tendency to inflate rainfall values. All corrected SREs
demonstrated a high capability for detecting dry days (POD $\ge$ 0.80),
suggesting their potential utility for drought applications. A critical
limitation persisted, however, as most SREs and BC methods consistently failed
to improve the detection of heavy and violent rainfall events (POD $\leq$ 0.2),
highlighting a crucial area for future research.

</details>


### [35] [Bayesian Source Apportionment of Spatio-temporal air pollution data](https://arxiv.org/abs/2510.27551)
*Michela Frigeri,Veronica Berrocal,Alessandra Guglielmi*

Main category: stat.AP

TL;DR: 提出了一种贝叶斯源解析模型，能够估计污染源数量并考虑污染物浓度的时空依赖性，应用于加州PM2.5数据识别出3个主要污染源。


<details>
  <summary>Details</summary>
Motivation: 确定细颗粒物(PM2.5)的来源对于设计有针对性的空气污染缓解策略至关重要，需要改进现有源解析方法以估计污染源数量并考虑时空依赖性。

Method: 采用贝叶斯潜在函数因子模型，将六种污染物的对数浓度表示为未知数量污染源的时空变化排放的线性组合，通过引入源特异性收缩参数来估计污染源数量。

Result: 模拟数据验证了模型能够准确恢复真实污染源数量并可靠估计潜在因子，应用于加州PM2.5数据识别出3个主要污染源。

Conclusion: 该模型在源解析方面取得了进展，能够有效估计污染源数量并考虑时空依赖性，为空气污染治理提供了有力工具。

Abstract: Understanding the sources that contribute to fine particulate matter
(PM$_{2.5}$) is of crucial importance for designing and implementing targeted
air pollution mitigation strategies. Determining what factors contribute to a
pollutant's concentration goes under the name of source apportionment and it is
a problem long studied by atmospheric scientists and statisticians alike. In
this paper, we propose a Bayesian model for source apportionment, that advances
the literature on source apportionment by allowing estimation of the number of
sources and accounting for spatial and temporal dependence in the observed
pollutants' concentrations. Taking as example observations of six species of
fine particulate matter observed over the course of a year, we present a latent
functional factor model that expresses the space-time varying observations of
log concentrations of the six pollutant as a linear combination of space-time
varying emissions produced by an unknown number of sources each multiplied by
the corresponding source's relative contribution to the pollutant. Estimation
of the number of sources is achieved by introducing source-specific shrinkage
parameters. Application of the model to simulated data showcases its ability to
retrieve the true number of sources and to reliably estimate the functional
latent factors, whereas application to PM$_{2.5}$ speciation data in California
identifies 3 major sources for the six PM$_{2.5}$ species.

</details>
