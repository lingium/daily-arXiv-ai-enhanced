{"id": "2511.02971", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.02971", "abs": "https://arxiv.org/abs/2511.02971", "authors": ["Yige Li", "María de los Angeles Resa", "José R. Zubizarreta"], "title": "Adaptive Orthogonalization for Stable Estimation of the Effects of Time-Varying Treatments", "comment": null, "summary": "Inferring the causal effects of time-varying treatments is often hindered by\nhighly variable inverse propensity weights, particularly in settings with\nlimited covariate overlap. Building on the key framework of Imai and Ratkovic\n(2015), we establish sufficient balancing conditions for identification in\nlongitudinal studies of treatment effects and propose a novel estimator that\ndirectly targets features of counterfactual or potential covariates. Instead of\nbalancing observed covariates, our method balances the components of covariates\nthat are orthogonal to their history, thereby isolating the new information at\neach time point. This strategy directly targets the joint distribution of\npotential covariates and prioritizes features that are most relevant to the\noutcome. We prove that the resulting estimator for the mean potential outcome\nis consistent and asymptotically normal, even in settings where standard\ninverse propensity weighting fails. Extensive simulations show that our\nestimator attains efficiency comparable to that of g-computation while\nproviding superior robustness to model misspecification. We apply our method to\na longitudinal study of private versus public schooling in Chile, demonstrating\nits stability and interpretability in estimating their effects on university\nadmission scores."}
{"id": "2511.02977", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.02977", "abs": "https://arxiv.org/abs/2511.02977", "authors": ["Fuming Yang", "David J. Nott", "Anne M. Presanis"], "title": "Detecting Conflicts in Evidence Synthesis Models Using Score Discrepancies", "comment": null, "summary": "Evidence synthesis models combine multiple data sources to estimate latent\nquantities of interest, enabling reliable inference on parameters that are\ndifficult to measure directly. However, shared parameters across data sources\ncan induce conflicts both among the data and with the assumed model structure.\nDetecting and quantifying such conflicts remains a challenge in model\ncriticism. Here we propose a general framework for conflict detection in\nevidence synthesis models based on score discrepancies, extending prior-data\nconflict diagnostics to more general conflict checks in the latent space of\nhierarchical models. Simulation studies in an exchangeable model demonstrate\nthat the proposed approach effectively detects between-data inconsistencies.\nApplication to an influenza severity model illustrates its use, complementary\nto traditional deviance-based diagnostics, in complex real-world hierarchical\nsettings. The proposed framework thus provides a flexible and broadly\napplicable tool for consistency assessment in Bayesian evidence synthesis."}
{"id": "2511.02984", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.02984", "abs": "https://arxiv.org/abs/2511.02984", "authors": ["Alan R. Vazquez", "Peter Goos", "Eric D. Schoen"], "title": "Constructing Large Orthogonal Minimally Aliased Response Surface Designs by Concatenating Two Definitive Screening Designs", "comment": "35 pages, 2 figures, 7 tables", "summary": "Orthogonal minimally aliased response surface (OMARS) designs permit the\nstudy of quantitative factors at three levels using an economical number of\nruns. In these designs, the linear effects of the factors are neither aliased\nwith each other nor with the quadratic effects and the two-factor interactions.\nComplete catalogs of OMARS designs with up to five factors have been obtained\nusing an enumeration algorithm. However, the algorithm is computationally\ndemanding for designs with many factors and runs. To overcome this issue, we\npropose a construction method for large OMARS designs that concatenates two\ndefinitive screening designs and improves the statistical features of its\nparent designs. The concatenation employs an algorithm that minimizes the\naliasing among the second-order effects using foldover techniques and column\npermutations for one of the parent designs. We study the properties of the new\nOMARS designs and compare them with alternative designs in the literature."}
{"id": "2511.03044", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03044", "abs": "https://arxiv.org/abs/2511.03044", "authors": ["Yiye Jiang"], "title": "New sampling approaches for Shrinkage Inverse-Wishart distribution", "comment": null, "summary": "In this paper, we propose new sampling approaches for the Shrinkage\nInverse-Wishart (SIW) distribution, a generalized family of the Inverse-Wishart\ndistribution originally proposed by Berger et al. (2020, Annals of Statistics).\nIt offers a flexible prior for covariance matrices and remains conjugate to the\nGaussian likelihood, similar to the classical Inverse-Wishart. Despite these\nadvantages, sampling from SIW remains challenging. The existing algorithm\nrelies on a nested Gibbs sampler, which is slow and lacks rigorous theoretical\nanalysis of its convergence. We propose a new algorithm based on the Sampling\nImportance Resampling (SIR) method, which is significantly faster and comes\nwith theoretical guarantees on convergence rates. A known issue with SIR\nmethods is the large discrepancy in importance weights, which occurs when the\nproposal distribution has thinner tails than the target. In the case of SIW,\ncertain parameter settings can lead to such discrepancies, reducing the\nrobustness of the output samples. To sample from such SIW distributions, we\nrobustify the proposed algorithm by including a clipping step to the SIR\nframework which transforms large importance weights. We provide theoretical\nresults on the convergence behavior in terms of the clipping size, and discuss\nstrategies for choosing this parameter via simulation studies. The robustified\nversion retains the computational efficiency of the original algorithm."}
{"id": "2511.03050", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG", "math.PR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03050", "abs": "https://arxiv.org/abs/2511.03050", "authors": ["Katharine E Fisher", "Matthew TC Li", "Youssef Marzouk", "Timo Schorlepp"], "title": "Precise asymptotic analysis of Sobolev training for random feature models", "comment": "23(+49) pages, 7(+16) figures main text(+appendix)", "summary": "Gradient information is widely useful and available in applications, and is\ntherefore natural to include in the training of neural networks. Yet little is\nknown theoretically about the impact of Sobolev training -- regression with\nboth function and gradient data -- on the generalization error of highly\noverparameterized predictive models in high dimensions. In this paper, we\nobtain a precise characterization of this training modality for random feature\n(RF) models in the limit where the number of trainable parameters, input\ndimensions, and training data tend proportionally to infinity. Our model for\nSobolev training reflects practical implementations by sketching gradient data\nonto finite dimensional subspaces. By combining the replica method from\nstatistical physics with linearizations in operator-valued free probability\ntheory, we derive a closed-form description for the generalization errors of\nthe trained RF models. For target functions described by single-index models,\nwe demonstrate that supplementing function data with additional gradient data\ndoes not universally improve predictive performance. Rather, the degree of\noverparameterization should inform the choice of training method. More broadly,\nour results identify settings where models perform optimally by interpolating\nnoisy function and gradient data."}
{"id": "2511.03154", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03154", "abs": "https://arxiv.org/abs/2511.03154", "authors": ["Natchaphon Leungbootnak", "Zihao Li", "Zihang Wei", "Dominique Lord", "Yunlong Zhang"], "title": "Modeling Headway in Heterogeneous and Mixed Traffic Flow: A Statistical Distribution Based on a General Exponential Function", "comment": null, "summary": "The ability of existing headway distributions to accurately reflect the\ndiverse behaviors and characteristics in heterogeneous traffic (different types\nof vehicles) and mixed traffic (human-driven vehicles with autonomous vehicles)\nis limited, leading to unsatisfactory goodness of fit. To address these issues,\nwe modified the exponential function to obtain a novel headway distribution.\nRather than employing Euler's number (e) as the base of the exponential\nfunction, we utilized a real number base to provide greater flexibility in\nmodeling the observed headway. However, the proposed is not a probability\nfunction. We normalize it to calculate the probability and derive the\nclosed-form equation. In this study, we utilized a comprehensive experiment\nwith five open datasets: highD, exiD, NGSIM, Waymo, and Lyft to evaluate the\nperformance of the proposed distribution and compared its performance with six\nexisting distributions under mixed and heterogeneous traffic flow. The results\nrevealed that the proposed distribution not only captures the fundamental\ncharacteristics of headway distribution but also provides physically meaningful\nparameters that describe the distribution shape of observed headways. Under\nheterogeneous flow on highways (i.e., uninterrupted traffic flow), the proposed\ndistribution outperforms other candidate distributions. Under urban road\nconditions (i.e., interrupted traffic flow), including heterogeneous and mixed\ntraffic, the proposed distribution still achieves decent results."}
{"id": "2511.02986", "categories": ["stat.ML", "cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.02986", "abs": "https://arxiv.org/abs/2511.02986", "authors": ["Giovanni Palla", "Sudarshan Babu", "Payam Dibaeinia", "James D. Pearce", "Donghui Li", "Aly A. Khan", "Theofanis Karaletsos", "Jakub M. Tomczak"], "title": "Scalable Single-Cell Gene Expression Generation with Latent Diffusion Models", "comment": "Github: https://github.com/czi-ai/scldm/", "summary": "Computational modeling of single-cell gene expression is crucial for\nunderstanding cellular processes, but generating realistic expression profiles\nremains a major challenge. This difficulty arises from the count nature of gene\nexpression data and complex latent dependencies among genes. Existing\ngenerative models often impose artificial gene orderings or rely on shallow\nneural network architectures. We introduce a scalable latent diffusion model\nfor single-cell gene expression data, which we refer to as scLDM, that respects\nthe fundamental exchangeability property of the data. Our VAE uses fixed-size\nlatent variables leveraging a unified Multi-head Cross-Attention Block (MCAB)\narchitecture, which serves dual roles: permutation-invariant pooling in the\nencoder and permutation-equivariant unpooling in the decoder. We enhance this\nframework by replacing the Gaussian prior with a latent diffusion model using\nDiffusion Transformers and linear interpolants, enabling high-quality\ngeneration with multi-conditional classifier-free guidance. We show its\nsuperior performance in a variety of experiments for both observational and\nperturbational single-cell data, as well as downstream tasks like cell-level\nclassification."}
{"id": "2511.02929", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.02929", "abs": "https://arxiv.org/abs/2511.02929", "authors": ["Zichu Wang", "Esteban G. Tabak"], "title": "Optimal transport with a density-dependent cost function", "comment": null, "summary": "A new pairwise cost function is proposed for the optimal transport barycenter\nproblem, adopting the form of the minimal action between two points, with a\nLagrangian that takes into account an underlying probability distribution.\nUnder this notion of distance, two points can only be close if there exist\npaths joining them that do not traverse areas of small probability. A framework\nis proposed and developed for the numerical solution of the corresponding\ndata-driven optimal transport problem. The procedure parameterizes the paths of\nminimal action through path dependent Chebyshev polynomials and enforces the\nagreement between the paths' endpoints and the given source and target\ndistributions through an adversarial penalization. The methodology and its\napplication to clustering and matching problems is illustrated through\nsynthetic examples."}
{"id": "2511.02881", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2511.02881", "abs": "https://arxiv.org/abs/2511.02881", "authors": ["Tommaso Costa"], "title": "From Hume to Jaynes: Induction as the Logic of Plausible Reasoning", "comment": null, "summary": "The problem of induction has persisted since Hume exposed the logical gap\nbetween repeated observation and universal inference. Traditional attempts to\nresolve it have oscillated between two extremes: the probabilistic optimism of\nLaplace and Jeffreys, who sought to quantify belief through probability, and\nthe critical skepticism of Popper, who replaced confirmation with\nfalsification. Both approaches, however, assume that induction must deliver\ncertainty or its negation. In this paper, I argue that the problem of induction\ndissolves when recast in terms of logical coherence (understood as internal\nconsistency of credences under updating) rather than truth. Following E. T.\nJaynes, probability is interpreted not as frequency or decision rule but as the\nextension of deductive logic to incomplete information. Under this\ninterpretation, Bayes's theorem is not an empirical statement but a consistency\ncondition that constrains rational belief updating. Induction thus emerges as\nthe special case of deductive reasoning applied to uncertain premises.\nFalsification appears as the limiting form of Bayesian updating when new data\ndrive posterior plausibility toward zero, while the Bayes Factor quantifies the\ncontinuous spectrum of evidential strength. Through analytical examples,\nincluding Laplace's sunrise problem, Jeffreys's mixed prior, and\nconfidence-based reformulations, I show that only the logic of plausible\nreasoning unifies these perspectives without contradiction. Induction, properly\nunderstood, is not the leap from past to future but the discipline of\nmaintaining coherence between evidence, belief, and information."}
{"id": "2511.03087", "categories": ["stat.ME", "math.OC", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03087", "abs": "https://arxiv.org/abs/2511.03087", "authors": ["Linglingzhi Zhu", "Jonghyeok Lee", "Yao Xie"], "title": "Beyond Maximum Likelihood: Variational Inequality Estimation for Generalized Linear Models", "comment": null, "summary": "Generalized linear models (GLMs) are fundamental tools for statistical\nmodeling, with maximum likelihood estimation (MLE) serving as the classical\nmethod for parameter inference. While MLE performs well in canonical GLMs, it\ncan become computationally inefficient near the true parameter value. In more\ngeneral settings with non-canonical or fully general link functions, the\nresulting optimization landscape is often non-convex, non-smooth, and\nnumerically unstable. To address these challenges, we investigate an\nalternative estimator based on solving the variational inequality (VI)\nformulation of the GLM likelihood equations, originally proposed by Juditsky\nand Nemirovski as an alternative for solving nonlinear least-squares problems.\nUnlike their focus on algorithmic convergence in monotone settings, we analyze\nthe VI approach from a statistical perspective, comparing it systematically\nwith the MLE. We also extend the theory of VI estimators to a broader class of\nlink functions, including non-monotone cases satisfying a strong Minty\ncondition, and show that it admits weaker smoothness requirements than MLE,\nenabling faster, more stable, and less locally trapped optimization.\nTheoretically, we establish both non-asymptotic estimation error bounds and\nasymptotic normality for the VI estimator, and further provide convergence\nguarantees for fixed-point and stochastic approximation algorithms. Numerical\nexperiments show that the VI framework preserves the statistical efficiency of\nMLE while substantially extending its applicability to more challenging GLM\nsettings."}
{"id": "2511.03087", "categories": ["stat.ME", "math.OC", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03087", "abs": "https://arxiv.org/abs/2511.03087", "authors": ["Linglingzhi Zhu", "Jonghyeok Lee", "Yao Xie"], "title": "Beyond Maximum Likelihood: Variational Inequality Estimation for Generalized Linear Models", "comment": null, "summary": "Generalized linear models (GLMs) are fundamental tools for statistical\nmodeling, with maximum likelihood estimation (MLE) serving as the classical\nmethod for parameter inference. While MLE performs well in canonical GLMs, it\ncan become computationally inefficient near the true parameter value. In more\ngeneral settings with non-canonical or fully general link functions, the\nresulting optimization landscape is often non-convex, non-smooth, and\nnumerically unstable. To address these challenges, we investigate an\nalternative estimator based on solving the variational inequality (VI)\nformulation of the GLM likelihood equations, originally proposed by Juditsky\nand Nemirovski as an alternative for solving nonlinear least-squares problems.\nUnlike their focus on algorithmic convergence in monotone settings, we analyze\nthe VI approach from a statistical perspective, comparing it systematically\nwith the MLE. We also extend the theory of VI estimators to a broader class of\nlink functions, including non-monotone cases satisfying a strong Minty\ncondition, and show that it admits weaker smoothness requirements than MLE,\nenabling faster, more stable, and less locally trapped optimization.\nTheoretically, we establish both non-asymptotic estimation error bounds and\nasymptotic normality for the VI estimator, and further provide convergence\nguarantees for fixed-point and stochastic approximation algorithms. Numerical\nexperiments show that the VI framework preserves the statistical efficiency of\nMLE while substantially extending its applicability to more challenging GLM\nsettings."}
{"id": "2511.03555", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.03555", "abs": "https://arxiv.org/abs/2511.03555", "authors": ["Zheshi Zheng", "Yuanyuan Li", "Peter X. K. Song", "Jiming Jiang"], "title": "Post-2024 U.S. Presidential Election Analysis of Election and Poll Data: Real-life Validation of Prediction via Small Area Estimation and Uncertainty Quantification", "comment": null, "summary": "We carry out a post-election analysis of the 2024 U.S. Presidential Election\n(USPE) using a prediction model derived from the Small Area Estimation (SAE)\nmethodology. With pollster data obtained one week prior to the election day,\nretrospectively, our SAE-based prediction model can perfectly predict the\nElectoral College election results in all 44 states where polling data were\navailable. In addition to such desirable prediction accuracy, we introduce the\nprobability of incorrect prediction (PoIP) to rigorously analyze prediction\nuncertainty. Since the standard bootstrap method appears inadequate for\nestimating PoIP, we propose a conformal inference method that yields reliable\nuncertainty quantification. We further investigate potential pollster biases by\nthe means of sensitivity analyses and conclude that swing states are\nparticularly vulnerable to polling bias in the prediction of the 2024 USPE."}
{"id": "2511.03000", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.03000", "abs": "https://arxiv.org/abs/2511.03000", "authors": ["Alexander J. Gates"], "title": "Unifying Information-Theoretic and Pair-Counting Clustering Similarity", "comment": "28 pages, 2 figures", "summary": "Comparing clusterings is central to evaluating unsupervised models, yet the\nmany existing similarity measures can produce widely divergent, sometimes\ncontradictory, evaluations. Clustering similarity measures are typically\norganized into two principal families, pair-counting and information-theoretic,\nreflecting whether they quantify agreement through element pairs or aggregate\ninformation across full cluster contingency tables. Prior work has uncovered\nparallels between these families and applied empirical normalization or\nchance-correction schemes, but their deeper analytical connection remains only\npartially understood. Here, we develop an analytical framework that unifies\nthese families through two complementary perspectives. First, both families are\nexpressed as weighted expansions of observed versus expected co-occurrences,\nwith pair-counting arising as a quadratic, low-order approximation and\ninformation-theoretic measures as higher-order, frequency-weighted extensions.\nSecond, we generalize pair-counting to $k$-tuple agreement and show that\ninformation-theoretic measures can be viewed as systematically accumulating\nhigher-order co-assignment structure beyond the pairwise level. We illustrate\nthe approaches analytically for the Rand index and Mutual Information, and show\nhow other indices in each family emerge as natural extensions. Together, these\nviews clarify when and why the two regimes diverge, relating their\nsensitivities directly to weighting and approximation order, and provide a\nprincipled basis for selecting, interpreting, and extending clustering\nsimilarity measures across applications."}
{"id": "2511.03694", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03694", "abs": "https://arxiv.org/abs/2511.03694", "authors": ["Hao Li", "Shonosuke Sugasawa", "Shota Katayama"], "title": "Robust Global Fr'echet Regression via Weight Regularization", "comment": null, "summary": "The Fr\\'echet regression is a useful method for modeling random objects in a\ngeneral metric space given Euclidean covariates. However, the conventional\napproach could be sensitive to outlying objects in the sense that the distance\nfrom the regression surface is large compared to the other objects. In this\nstudy, we develop a robust version of the global Fr\\'echet regression by\nincorporating weight parameters into the objective function. We then introduce\nthe Elastic net regularization, favoring a sparse vector of robust parameters\nto control the influence of outlying objects. We provide a computational\nalgorithm to iteratively estimate the regression function and weight\nparameters, with providing a linear convergence property. We also propose the\nBayesian information criterion to select the tuning parameters for\nregularization, which gives adaptive robustness along with observed data. The\nfinite sample performance of the proposed method is demonstrated through\nnumerical studies on matrix and distribution responses."}
{"id": "2511.03158", "categories": ["stat.ME", "60G55, 62M30"], "pdf": "https://arxiv.org/pdf/2511.03158", "abs": "https://arxiv.org/abs/2511.03158", "authors": ["Changqing Lu", "Ganggang Xu", "Junho Yang", "Yongtao Guan"], "title": "On Ignorability of Preferential Sampling in Geostatistics", "comment": "36 pages, 6 figures", "summary": "Preferential sampling has attracted considerable attention in geostatistics\nsince the pioneering work of Diggle et al. (2010). A variety of\nlikelihood-based approaches have been developed to correct estimation bias by\nexplicitly modelling the sampling mechanism. While effective in many\napplications, these methods are often computationally expensive and can be\nsusceptible to model misspecification. In this paper, we present a surprising\nfinding: some existing non-likelihood-based methods that ignore preferential\nsampling can still produce unbiased and consistent estimators under the widely\nused framework of Diggle et al. (2010) and its extensions. We investigate the\nconditions under which preferential sampling can be ignored and develop\nrelevant estimators for both regression and covariance parameters without\nspecifying the sampling mechanism parametrically. Simulation studies\ndemonstrate clear advantages of our approach, including reduced estimation\nerror, improved confidence interval coverage, and substantially lower\ncomputational cost. To show the practical utility, we further apply it to a\ntropical forest data set."}
{"id": "2511.03606", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03606", "abs": "https://arxiv.org/abs/2511.03606", "authors": ["Diego Martinez-Taboada", "Tomas Gonzalez", "Aaditya Ramdas"], "title": "Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity", "comment": null, "summary": "The study of self-normalized processes plays a crucial role in a wide range\nof applications, from sequential decision-making to econometrics. While the\nbehavior of self-normalized concentration has been widely investigated for\nscalar-valued processes, vector-valued processes remain comparatively\nunderexplored, especially outside of the sub-Gaussian framework. In this\ncontribution, we provide concentration bounds for self-normalized processes\nwith light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds).\nWe illustrate the relevance of our results in the context of online linear\nregression, with applications in (kernelized) linear bandits."}
{"id": "2511.03596", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.03596", "abs": "https://arxiv.org/abs/2511.03596", "authors": ["Kyle F. Grosser", "Abigail G. Foes", "Stellen Li", "Vraj Parikh", "Tanya P. Garcia", "Sarah C. Lotspeich"], "title": "Adjusting for Heavy Censoring and Double-Dipping to Compare Risk Stratification Abilities of Existing Models for Time to Diagnosis of Huntington Disease", "comment": "14 pages, 5 tables, 2 figures", "summary": "Huntington disease (HD) is a genetically inherited neurodegenerative disease\nwith progressively worsening symptoms. Accurately modeling time to HD diagnosis\nis essential for clinical trial design and treatment planning. Langbehn's\nmodel, the CAG-Age Product (CAP) model, the Prognostic Index Normed (PIN)\nmodel, and the Multivariate Risk Score (MRS) model have all been proposed for\nthis task. However, differing in methodology, assumptions, and accuracy, these\nmodels may yield conflicting predictions. Few studies have systematically\ncompared these models' performance, and those that have could be misleading due\nto (i) testing the models on the same data used to train them and (ii) failing\nto account for high rates of right censoring (80%+) in performance metrics. We\ndiscuss the theoretical foundations of the four most common models of time to\nHD diagnosis, offering intuitive comparisons about their practical feasibility.\nFurther, we externally validate their risk stratification abilities using data\nfrom the ENROLL-HD study and performance metrics that adjust for censoring. Our\nfindings guide the selection of a model for HD clinical trial design. The MRS\nmodel, which incorporates the most covariates, performed the best. However, the\nsimpler CAP and PIN models were not far behind and may be logistically simpler\nto adopt. We also show how these models can be used to estimate sample sizes\nfor an HD clinical trial, emphasizing that previous estimates would lead to\nunderpowered trials."}
{"id": "2511.03050", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG", "math.PR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03050", "abs": "https://arxiv.org/abs/2511.03050", "authors": ["Katharine E Fisher", "Matthew TC Li", "Youssef Marzouk", "Timo Schorlepp"], "title": "Precise asymptotic analysis of Sobolev training for random feature models", "comment": "23(+49) pages, 7(+16) figures main text(+appendix)", "summary": "Gradient information is widely useful and available in applications, and is\ntherefore natural to include in the training of neural networks. Yet little is\nknown theoretically about the impact of Sobolev training -- regression with\nboth function and gradient data -- on the generalization error of highly\noverparameterized predictive models in high dimensions. In this paper, we\nobtain a precise characterization of this training modality for random feature\n(RF) models in the limit where the number of trainable parameters, input\ndimensions, and training data tend proportionally to infinity. Our model for\nSobolev training reflects practical implementations by sketching gradient data\nonto finite dimensional subspaces. By combining the replica method from\nstatistical physics with linearizations in operator-valued free probability\ntheory, we derive a closed-form description for the generalization errors of\nthe trained RF models. For target functions described by single-index models,\nwe demonstrate that supplementing function data with additional gradient data\ndoes not universally improve predictive performance. Rather, the degree of\noverparameterization should inform the choice of training method. More broadly,\nour results identify settings where models perform optimally by interpolating\nnoisy function and gradient data."}
{"id": "2511.02977", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.02977", "abs": "https://arxiv.org/abs/2511.02977", "authors": ["Fuming Yang", "David J. Nott", "Anne M. Presanis"], "title": "Detecting Conflicts in Evidence Synthesis Models Using Score Discrepancies", "comment": null, "summary": "Evidence synthesis models combine multiple data sources to estimate latent\nquantities of interest, enabling reliable inference on parameters that are\ndifficult to measure directly. However, shared parameters across data sources\ncan induce conflicts both among the data and with the assumed model structure.\nDetecting and quantifying such conflicts remains a challenge in model\ncriticism. Here we propose a general framework for conflict detection in\nevidence synthesis models based on score discrepancies, extending prior-data\nconflict diagnostics to more general conflict checks in the latent space of\nhierarchical models. Simulation studies in an exchangeable model demonstrate\nthat the proposed approach effectively detects between-data inconsistencies.\nApplication to an influenza severity model illustrates its use, complementary\nto traditional deviance-based diagnostics, in complex real-world hierarchical\nsettings. The proposed framework thus provides a flexible and broadly\napplicable tool for consistency assessment in Bayesian evidence synthesis."}
{"id": "2511.03395", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.03395", "abs": "https://arxiv.org/abs/2511.03395", "authors": ["Stefan Franssen"], "title": "Comment on: \"Model uncertainty and missing data: An Objective Bayesian Perspective\"", "comment": "Comment on arxiv:2410.05893", "summary": "We give a contributed discussion on \"Model uncertainty and missing data: An\nObjective Bayesian Perspective\", where we discuss frequentist perspectives on\nthe proposed methodology."}
{"id": "2511.03125", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03125", "abs": "https://arxiv.org/abs/2511.03125", "authors": ["Haitao Lin", "Boxin Zhao", "Mladen Kolar", "Chong Liu"], "title": "Provable Accelerated Bayesian Optimization with Knowledge Transfer", "comment": null, "summary": "We study how Bayesian optimization (BO) can be accelerated on a target task\nwith historical knowledge transferred from related source tasks. Existing works\non BO with knowledge transfer either do not have theoretical guarantees or\nachieve the same regret as BO in the non-transfer setting,\n$\\tilde{\\mathcal{O}}(\\sqrt{T \\gamma_f})$, where $T$ is the number of\nevaluations of the target function and $\\gamma_f$ denotes its information gain.\nIn this paper, we propose the DeltaBO algorithm, in which a novel\nuncertainty-quantification approach is built on the difference function\n$\\delta$ between the source and target functions, which are allowed to belong\nto different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions,\nwe prove that the regret of DeltaBO is of order $\\tilde{\\mathcal{O}}(\\sqrt{T\n(T/N + \\gamma_\\delta)})$, where $N$ denotes the number of evaluations from\nsource tasks and typically $N \\gg T$. In many applications, source and target\ntasks are similar, which implies that $\\gamma_\\delta$ can be much smaller than\n$\\gamma_f$. Empirical studies on both real-world hyperparameter tuning tasks\nand synthetic functions show that DeltaBO outperforms other baseline methods\nand support our theoretical claims."}
{"id": "2511.03044", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03044", "abs": "https://arxiv.org/abs/2511.03044", "authors": ["Yiye Jiang"], "title": "New sampling approaches for Shrinkage Inverse-Wishart distribution", "comment": null, "summary": "In this paper, we propose new sampling approaches for the Shrinkage\nInverse-Wishart (SIW) distribution, a generalized family of the Inverse-Wishart\ndistribution originally proposed by Berger et al. (2020, Annals of Statistics).\nIt offers a flexible prior for covariance matrices and remains conjugate to the\nGaussian likelihood, similar to the classical Inverse-Wishart. Despite these\nadvantages, sampling from SIW remains challenging. The existing algorithm\nrelies on a nested Gibbs sampler, which is slow and lacks rigorous theoretical\nanalysis of its convergence. We propose a new algorithm based on the Sampling\nImportance Resampling (SIR) method, which is significantly faster and comes\nwith theoretical guarantees on convergence rates. A known issue with SIR\nmethods is the large discrepancy in importance weights, which occurs when the\nproposal distribution has thinner tails than the target. In the case of SIW,\ncertain parameter settings can lead to such discrepancies, reducing the\nrobustness of the output samples. To sample from such SIW distributions, we\nrobustify the proposed algorithm by including a clipping step to the SIR\nframework which transforms large importance weights. We provide theoretical\nresults on the convergence behavior in terms of the clipping size, and discuss\nstrategies for choosing this parameter via simulation studies. The robustified\nversion retains the computational efficiency of the original algorithm."}
{"id": "2511.03399", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.03399", "abs": "https://arxiv.org/abs/2511.03399", "authors": ["Andrea Cremaschi", "Manuele Leonelli", "Gherardo Varando"], "title": "Bayesian Causal Effect Estimation for Categorical Data using Staged Tree Models", "comment": null, "summary": "We propose a fully Bayesian approach for causal inference with multivariate\ncategorical data based on staged tree models, a class of probabilistic\ngraphical models capable of representing asymmetric and context-specific\ndependencies. To account for uncertainty in both structure and parameters, we\nintroduce a flexible family of prior distributions over staged trees. These\ninclude product partition models to encourage parsimony, a novel distance-based\nprior to promote interpretable dependence patterns, and an extension that\nincorporates continuous covariates into the learning process. Posterior\ninference is achieved via a tailored Markov Chain Monte Carlo algorithm with\nsplit-and-merge moves, yielding posterior samples of staged trees from which\naverage treatment effects and uncertainty measures are derived. Posterior\nsummaries and uncertainty measures are obtained via techniques from the\nBayesian nonparametrics literature. Two case studies on electronic fetal\nmonitoring and cesarean delivery and on anthracycline therapy and cardiac\ndysfunction in breast cancer illustrate the methods."}
{"id": "2511.03202", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03202", "abs": "https://arxiv.org/abs/2511.03202", "authors": ["Zeqi Ye", "Qijie Zhu", "Molei Tao", "Minshuo Chen"], "title": "Provable Separations between Memorization and Generalization in Diffusion Models", "comment": "51 pages, 4 figures", "summary": "Diffusion models have achieved remarkable success across diverse domains, but\nthey remain vulnerable to memorization -- reproducing training data rather than\ngenerating novel outputs. This not only limits their creative potential but\nalso raises concerns about privacy and safety. While empirical studies have\nexplored mitigation strategies, theoretical understanding of memorization\nremains limited. We address this gap through developing a dual-separation\nresult via two complementary perspectives: statistical estimation and network\napproximation. From the estimation side, we show that the ground-truth score\nfunction does not minimize the empirical denoising loss, creating a separation\nthat drives memorization. From the approximation side, we prove that\nimplementing the empirical score function requires network size to scale with\nsample size, spelling a separation compared to the more compact network\nrepresentation of the ground-truth score function. Guided by these insights, we\ndevelop a pruning-based method that reduces memorization while maintaining\ngeneration quality in diffusion transformers."}
{"id": "2511.03420", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.03420", "abs": "https://arxiv.org/abs/2511.03420", "authors": ["Alberto Caimo", "Isabella Gollini"], "title": "Multi-layer dissolution exponential-family models for weighted signed networks", "comment": "27 pages, 11 figures", "summary": "Understanding the structure of weighted signed networks is essential for\nanalysing social systems in which relationships vary both in sign and strength.\nDespite significant advances in statistical network analysis, there is still a\nlack of statistical models that can jointly and rigorously account for both the\nsign and strength of relationships in networks. We introduce a multi-layer\ndissolution exponential random graph modelling framework that jointly captures\nthe signed and weighted processes, conditional on the observed interaction\nstructure. The framework enables rigorous assessment of structural balance\neffects while fully accounting for edge weights. To enhance inference, we adopt\na fully-probabilistic Bayesian hierarchical approach that partially pools\ninformation across layers, with parameters estimated via an adaptive\napproximate exchange algorithm. We demonstrate the flexibility and explanatory\npower of the proposed methodology by applying it to bill sponsorship data from\nthe 108th US Senate, revealing complex patterns of signed and weighted\ninteractions and structural balance effects that traditional approaches are\nunable to capture."}
{"id": "2511.03216", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03216", "abs": "https://arxiv.org/abs/2511.03216", "authors": ["Md Ashad Alam"], "title": "RKUM: An R Package for Robust Kernel Unsupervised Methods", "comment": "26, 2 figures", "summary": "RKUM is an R package developed for implementing robust kernel-based\nunsupervised methods. It provides functions for estimating the robust kernel\ncovariance operator (CO) and the robust kernel cross-covariance operator (CCO)\nusing generalized loss functions instead of the conventional quadratic loss.\nThese operators form the foundation of robust kernel learning and enable\nreliable analysis under contaminated or noisy data conditions. The package\nincludes implementations of robust kernel canonical correlation analysis\n(Kernel CCA), as well as the influence function (IF) for both standard and\nmultiple kernel CCA frameworks. The influence function quantifies sensitivity\nand helps detect influential or outlying observations across two-view and\nmulti-view datasets. Experiments using synthesized two-view and multi-view data\ndemonstrate that the IF of the standard kernel CCA effectively identifies\noutliers, while the robust kernel methods implemented in RKUM exhibit reduced\nsensitivity to contamination. Overall, RKUM provides an efficient and\nextensible platform for robust kernel-based analysis in high-dimensional data\napplications."}
{"id": "2511.03467", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.03467", "abs": "https://arxiv.org/abs/2511.03467", "authors": ["Lapo Santi", "Nial Friel"], "title": "The Bradley-Terry Stochastic Block Model", "comment": null, "summary": "The Bradley-Terry model is widely used for the analysis of pairwise\ncomparison data and, in essence, produces a ranking of the items under\ncomparison. We embed the Bradley-Terry model within a stochastic block model,\nallowing items to cluster. The resulting Bradley-Terry SBM (BT-SBM) ranks\nclusters so that items within a cluster share the same tied rank. We develop a\nfully Bayesian specification in which all quantities-the number of blocks,\ntheir strengths, and item assignments-are jointly learned via a fast Gibbs\nsampler derived through a Thurstonian data augmentation. Despite its\nefficiency, the sampler yields coherent and interpretable posterior summaries\nfor all model components. Our motivating application analyzes men's tennis\nresults from ATP tournaments over the seasons 2000-2022. We find that the top\n100 players can be broadly partitioned into three or four tiers in most\nseasons. Moreover, the size of the strongest tier was small from the mid-2000s\nto 2018 and has increased since, providing evidence that men's tennis has\nbecome more competitive in recent years."}
{"id": "2511.03606", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03606", "abs": "https://arxiv.org/abs/2511.03606", "authors": ["Diego Martinez-Taboada", "Tomas Gonzalez", "Aaditya Ramdas"], "title": "Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity", "comment": null, "summary": "The study of self-normalized processes plays a crucial role in a wide range\nof applications, from sequential decision-making to econometrics. While the\nbehavior of self-normalized concentration has been widely investigated for\nscalar-valued processes, vector-valued processes remain comparatively\nunderexplored, especially outside of the sub-Gaussian framework. In this\ncontribution, we provide concentration bounds for self-normalized processes\nwith light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds).\nWe illustrate the relevance of our results in the context of online linear\nregression, with applications in (kernelized) linear bandits."}
{"id": "2511.03605", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.03605", "abs": "https://arxiv.org/abs/2511.03605", "authors": ["Xukun Zhu", "Michael W Lutz", "Tananun Songdechakraiwut"], "title": "Bayesian Topological Analysis of Functional Brain Networks", "comment": null, "summary": "Subtle alterations in brain network topology often evade detection by\ntraditional statistical methods. To address this limitation, we introduce a\nBayesian inference framework for topological comparison of brain networks that\nprobabilistically models within- and between-group dissimilarities. The\nframework employs Markov chain Monte Carlo sampling to estimate posterior\ndistributions of test statistics and Bayes factors, enabling graded evidence\nassessment beyond binary significance testing. Simulations confirmed\nstatistical consistency to permutation testing. Applied to fMRI data from the\nDuke-UNC Alzheimer's Disease Research Center, the framework detected\ntopology-based network differences that conventional permutation tests failed\nto reveal, highlighting its enhanced sensitivity to early or subtle brain\nnetwork alterations in clinical neuroimaging."}
{"id": "2511.03693", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03693", "abs": "https://arxiv.org/abs/2511.03693", "authors": ["Md Ahasanul Arafath", "Abhijit Kumar Ghosh", "Md Rony Ahmed", "Sabrin Afroz", "Minhazul Hosen", "Md Hasan Moon", "Md Tanzim Reza", "Md Ashad Alam"], "title": "Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning", "comment": "15 pages and 7 figures", "summary": "Colorectal cancer (CRC) grading is a critical prognostic factor but remains\nhampered by inter-observer variability and the privacy constraints of\nmulti-institutional data sharing. While deep learning offers a path to\nautomation, centralized training models conflict with data governance\nregulations and neglect the diagnostic importance of multi-scale analysis. In\nthis work, we propose a scalable, privacy-preserving federated learning (FL)\nframework for CRC histopathological grading that integrates multi-scale feature\nlearning within a distributed training paradigm. Our approach employs a\ndual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear\ndetail and broader tissue-level context. This architecture is integrated into a\nrobust FL system stabilized using FedProx to mitigate client drift across\nheterogeneous data distributions from multiple hospitals. Extensive evaluation\non the CRC-HGD dataset demonstrates that our framework achieves an overall\naccuracy of 83.5%, outperforming a comparable centralized model (81.6%).\nCrucially, the system excels in identifying the most aggressive Grade III\ntumors with a high recall of 87.5%, a key clinical priority to prevent\ndangerous false negatives. Performance further improves with higher\nmagnification, reaching 88.0% accuracy at 40x. These results validate that our\nfederated multi-scale approach not only preserves patient privacy but also\nenhances model performance and generalization. The proposed modular pipeline,\nwith built-in preprocessing, checkpointing, and error handling, establishes a\nfoundational step toward deployable, privacy-aware clinical AI for digital\npathology."}
{"id": "2511.03596", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.03596", "abs": "https://arxiv.org/abs/2511.03596", "authors": ["Kyle F. Grosser", "Abigail G. Foes", "Stellen Li", "Vraj Parikh", "Tanya P. Garcia", "Sarah C. Lotspeich"], "title": "Adjusting for Heavy Censoring and Double-Dipping to Compare Risk Stratification Abilities of Existing Models for Time to Diagnosis of Huntington Disease", "comment": "14 pages, 5 tables, 2 figures", "summary": "Huntington disease (HD) is a genetically inherited neurodegenerative disease\nwith progressively worsening symptoms. Accurately modeling time to HD diagnosis\nis essential for clinical trial design and treatment planning. Langbehn's\nmodel, the CAG-Age Product (CAP) model, the Prognostic Index Normed (PIN)\nmodel, and the Multivariate Risk Score (MRS) model have all been proposed for\nthis task. However, differing in methodology, assumptions, and accuracy, these\nmodels may yield conflicting predictions. Few studies have systematically\ncompared these models' performance, and those that have could be misleading due\nto (i) testing the models on the same data used to train them and (ii) failing\nto account for high rates of right censoring (80%+) in performance metrics. We\ndiscuss the theoretical foundations of the four most common models of time to\nHD diagnosis, offering intuitive comparisons about their practical feasibility.\nFurther, we externally validate their risk stratification abilities using data\nfrom the ENROLL-HD study and performance metrics that adjust for censoring. Our\nfindings guide the selection of a model for HD clinical trial design. The MRS\nmodel, which incorporates the most covariates, performed the best. However, the\nsimpler CAP and PIN models were not far behind and may be logistically simpler\nto adopt. We also show how these models can be used to estimate sample sizes\nfor an HD clinical trial, emphasizing that previous estimates would lead to\nunderpowered trials."}
{"id": "2511.03087", "categories": ["stat.ME", "math.OC", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03087", "abs": "https://arxiv.org/abs/2511.03087", "authors": ["Linglingzhi Zhu", "Jonghyeok Lee", "Yao Xie"], "title": "Beyond Maximum Likelihood: Variational Inequality Estimation for Generalized Linear Models", "comment": null, "summary": "Generalized linear models (GLMs) are fundamental tools for statistical\nmodeling, with maximum likelihood estimation (MLE) serving as the classical\nmethod for parameter inference. While MLE performs well in canonical GLMs, it\ncan become computationally inefficient near the true parameter value. In more\ngeneral settings with non-canonical or fully general link functions, the\nresulting optimization landscape is often non-convex, non-smooth, and\nnumerically unstable. To address these challenges, we investigate an\nalternative estimator based on solving the variational inequality (VI)\nformulation of the GLM likelihood equations, originally proposed by Juditsky\nand Nemirovski as an alternative for solving nonlinear least-squares problems.\nUnlike their focus on algorithmic convergence in monotone settings, we analyze\nthe VI approach from a statistical perspective, comparing it systematically\nwith the MLE. We also extend the theory of VI estimators to a broader class of\nlink functions, including non-monotone cases satisfying a strong Minty\ncondition, and show that it admits weaker smoothness requirements than MLE,\nenabling faster, more stable, and less locally trapped optimization.\nTheoretically, we establish both non-asymptotic estimation error bounds and\nasymptotic normality for the VI estimator, and further provide convergence\nguarantees for fixed-point and stochastic approximation algorithms. Numerical\nexperiments show that the VI framework preserves the statistical efficiency of\nMLE while substantially extending its applicability to more challenging GLM\nsettings."}
