{"id": "2512.03021", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.03021", "abs": "https://arxiv.org/abs/2512.03021", "authors": ["Ananyabrata Barua", "Ayanendranath Basu"], "title": "Semiparametric Robust Estimation of Population Location", "comment": null, "summary": "Real-world measurements often comprise a dominant signal contaminated by a noisy background. Robustly estimating the dominant signal in practice has been a fundamental statistical problem. Classically, mixture models have been used to cluster the heterogeneous population into homogeneous components. Modeling such data with fully parametric models risks bias under misspecification, while fully nonparametric approaches can dissipate power and computational resources. We propose a middle path: a semiparametric method that models only the dominant component parametrically and leaves the background completely nonparametric, yet remains computationally scalable and statistically robust. So instead of outlier downweighting, traditionally done in robust statistics literature, we maximize the observed likelihood such that the noisy background is absorbed by the nonparametric component. Computationally, we propose a new approximate FFT-accelerated likelihood maximization algorithm. Empirically, this FFT plug-in achieves order-of-magnitude speedups over vanilla weighted EM while preserving statistical accuracy and large sample properties."}
{"id": "2512.02178", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.02178", "abs": "https://arxiv.org/abs/2512.02178", "authors": ["Seokjun Choi", "Tony Pourmohamad", "Bruno Sansó"], "title": "Tolerance Intervals Using Dirichlet Processes", "comment": "42 pages, 11 Tables", "summary": "In nonclinical pharmaceutical development, tolerance intervals are critical in ensuring product and process quality. They are statistical intervals designed to contain a specified proportion of the population with a given confidence level. Parametric and non-parametric methods have been developed to obtain tolerance intervals. The former work with small samples but can be affected by distribution misspecification. The latter offer larger flexibility but require large sample sizes. As an alternative, we propose Dirichlet process-based Bayesian nonparametric tolerance intervals to overcome the limitations. We develop a computationally efficient tolerance interval construction algorithm based on the analytically tractable quantile process of the Dirichlet process. Simulation studies show that our new approach is very robust to distributional assumptions and performs as efficiently as existing tolerance interval methods. To illustrate how the model works in practice, we apply our method to the tolerance interval estimation for potency data."}
{"id": "2512.02060", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02060", "abs": "https://arxiv.org/abs/2512.02060", "authors": ["Xia Chen", "Ruiji Sun", "Philipp Geyer", "André Borrmann", "Stefano Schiavon"], "title": "From 'What-is' to 'What-if' in Human-Factor Analysis: A Post-Occupancy Evaluation Case", "comment": "17 pages, 5 figures", "summary": "Human-factor analysis typically employs correlation analysis and significance testing to identify relationships between variables. However, these descriptive ('what-is') methods, while effective for identifying associations, are often insufficient for answering causal ('what-if') questions. Their application in such contexts often overlooks confounding and colliding variables, potentially leading to bias and suboptimal or incorrect decisions.\n  We advocate for explicitly distinguishing descriptive from interventional questions in human-factor analysis, and applying causal inference frameworks specifically to these problems to prevent methodological mismatches. This approach disentangles complex variable relationships and enables counterfactual reasoning. Using post-occupancy evaluation (POE) data from the Center for the Built Environment's (CBE) Occupant Survey as a demonstration case, we show how causal discovery reveals intervention hierarchies and directional relationships that traditional associational analysis misses. The systematic distinction between causally associated and independent variables, combined with intervention prioritization capabilities, offers broad applicability to complex human-centric systems, for example, in building science or ergonomics, where understanding intervention effects is critical for optimization and decision-making."}
{"id": "2512.02249", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.02249", "abs": "https://arxiv.org/abs/2512.02249", "authors": ["Alejandro Jara", "Carlos Sing-Long"], "title": "Discrete Sequential Barycenter Arrays: Representation, Approximation, and Modeling of Probability Measures", "comment": "105 pages, 4 figures", "summary": "Constructing flexible probability models that respect constraints on key functionals -- such as the mean -- is a fundamental problem in nonparametric statistics. Existing approaches lack systematic tools for enforcing such constraints while retaining full modeling flexibility. This paper introduces a new representation for univariate probability measures based on discrete sequential barycenter arrays (SBA). We study structural properties of SBA representations and establish new approximation results. In particular, we show that for any target distribution, its SBA-based discrete approximations converge in both the weak topology and in Wasserstein distances, and that the representation is exact for all distributions with finite discrete support. We further characterize a broad class of measures whose SBA partitions exhibit regularity and induce increasingly fine meshes, and we prove that this class is dense in standard probabilistic topologies. These theoretical results enable the construction of probability models that preserve prescribed values -- or full distributions -- of the mean while maintaining large support. As an application, we derive a mixture model for density estimation whose induced mixing distribution has a fixed or user-specified mean. The resulting framework provides a principled mechanism for incorporating mean constraints in nonparametric modeling while preserving strong approximation properties. The approach is illustrated using both simulated and real data."}
{"id": "2512.02495", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02495", "abs": "https://arxiv.org/abs/2512.02495", "authors": ["Ali Mohammad-Djafari", "Ning Chu", "Li Wang"], "title": "Bayesian Physics-Informed Neural Networks for Inverse Problems (BPINN-IP): Application in Infrared Image Processing", "comment": "31 page, paper in revision, submitted in Journal of the Franklin Institute, 2025", "summary": "Inverse problems arise across scientific and engineering domains, where the goal is to infer hidden parameters or physical fields from indirect and noisy observations. Classical approaches, such as variational regularization and Bayesian inference, provide well established theoretical foundations for handling ill posedness. However, these methods often become computationally restrictive in high dimensional settings or when the forward model is governed by complex physics. Physics Informed Neural Networks (PINNs) have recently emerged as a promising framework for solving inverse problems by embedding physical laws directly into the training process of neural networks. In this paper, we introduce a new perspective on the Bayesian Physics Informed Neural Network (BPINN) framework, extending classical PINNs by explicitly incorporating training data generation, modeling and measurement uncertainties through Bayesian prior modeling and doing inference with the posterior laws. Also, as we focus on the inverse problems, we call this method BPINN-IP, and we show that the standard PINN formulation naturally appears as its special case corresponding to the Maximum A Posteriori (MAP) estimate. This unified formulation allows simultaneous exploitation of physical constraints, prior knowledge, and data-driven inference, while enabling uncertainty quantification through posterior distributions. To demonstrate the effectiveness of the proposed framework, we consider inverse problems arising in infrared image processing, including deconvolution and super-resolution, and present results on both simulated and real industrial data."}
{"id": "2512.02178", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.02178", "abs": "https://arxiv.org/abs/2512.02178", "authors": ["Seokjun Choi", "Tony Pourmohamad", "Bruno Sansó"], "title": "Tolerance Intervals Using Dirichlet Processes", "comment": "42 pages, 11 Tables", "summary": "In nonclinical pharmaceutical development, tolerance intervals are critical in ensuring product and process quality. They are statistical intervals designed to contain a specified proportion of the population with a given confidence level. Parametric and non-parametric methods have been developed to obtain tolerance intervals. The former work with small samples but can be affected by distribution misspecification. The latter offer larger flexibility but require large sample sizes. As an alternative, we propose Dirichlet process-based Bayesian nonparametric tolerance intervals to overcome the limitations. We develop a computationally efficient tolerance interval construction algorithm based on the analytically tractable quantile process of the Dirichlet process. Simulation studies show that our new approach is very robust to distributional assumptions and performs as efficiently as existing tolerance interval methods. To illustrate how the model works in practice, we apply our method to the tolerance interval estimation for potency data."}
{"id": "2512.02210", "categories": ["stat.AP", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.02210", "abs": "https://arxiv.org/abs/2512.02210", "authors": ["Prathamesh Anwekar", "Kaushal Kirpekar", "Mahesh B", "Sainath Bitragunta"], "title": "Probabilistic Analysis of Various Squash Shots and Skill Study of Different Levels of Squash Players and Teams", "comment": "13 pages, 6 figures, Presented at 3rd International Conference on Sports Engineering, held at BITS Pilani, Pilani Campus", "summary": "We introduce a compact probabilistic model for two-player and two-team (four-player) squash matches, along with a practical skill-comparison rule derived from point-scoring probabilities. Using recorded shot types and court locations, we analyze how shot distributions differ between professional-level and intermediate-level players. Our analysis shows that professional players use a wider variety of shots and favor backcourt play to maintain control, while intermediate players concentrate more on mid-court shots, generate more errors, and exercise less positional control. These results quantify strategic differences in squash, offer a simple method to compare player and team skill, and provide actionable insights for sports analytics and coaching."}
{"id": "2512.02831", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.02831", "abs": "https://arxiv.org/abs/2512.02831", "authors": ["Ali Alvandi", "Mina Rezaei"], "title": "Revisiting Theory of Contrastive Learning for Domain Generalization", "comment": "19 pages", "summary": "Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set."}
{"id": "2512.02532", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02532", "abs": "https://arxiv.org/abs/2512.02532", "authors": ["Albert Saiapin", "Kim Batselier"], "title": "Laplace Approximation For Tensor Train Kernel Machines In System Identification", "comment": "6 pages, 2 figures, 4 tables. Submitted to IFAC 2026. Code available at: https://github.com/AlbMLpy/laplace-ttkm", "summary": "To address the scalability limitations of Gaussian process (GP) regression, several approximation techniques have been proposed. One such method is based on tensor networks, which utilizes an exponential number of basis functions without incurring exponential computational cost. However, extending this model to a fully probabilistic formulation introduces several design challenges. In particular, for tensor train (TT) models, it is unclear which TT-core should be treated in a Bayesian manner. We introduce a Bayesian tensor train kernel machine that applies Laplace approximation to estimate the posterior distribution over a selected TT-core and employs variational inference (VI) for precision hyperparameters. Experiments show that core selection is largely independent of TT-ranks and feature structure, and that VI replaces cross-validation while offering up to 65x faster training. The method's effectiveness is demonstrated on an inverse dynamics problem."}
{"id": "2512.02182", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.02182", "abs": "https://arxiv.org/abs/2512.02182", "authors": ["Sarah C. Lotspeich", "Cole Manschot"], "title": "Efficient and Intuitive Two-Phase Validation Across Multiple Models via Principal Components", "comment": "14 pages, 3 figures, 1 table, GitHub repositories with R package and simulation/analysis code", "summary": "Two-phase sampling offers a cost-effective way to validate error-prone measurements in observational databases or randomized trials. Inexpensive or easy-to-obtain information is collected for the entire study in Phase I. Then, a subset of patients undergoes cost-intensive validation to collect more accurate data in Phase II. Critically, any Phase I variables can be used to strategically select the Phase II subset, often enriched for a particular model of interest. However, when balancing primary and secondary analyses in the same study, competing models and priorities can result in poorly defined objectives for the most informative Phase II sampling criterion. We propose an intuitive, easy-to-use solution that balances and prioritizes explaining the largest amount of variability across all models of interest. Using principal components to succinctly summarize the inherent variability of the error-prone covariates for all models. Then, we sample patients with the most \"extreme\" principal components (i.e., the smallest or largest values) for validation. Through simulations and an application to data from the National Health and Nutrition Examination Survey (NHANES), we show that extreme tail sampling on the first principal component offers simultaneous efficiency gains across multiple models of interest relative to sampling for one specific model. Our proposed sampling strategy is implemented in the open-source R package, auditDesignR."}
{"id": "2512.02266", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.02266", "abs": "https://arxiv.org/abs/2512.02266", "authors": ["Michael J. Plank", "Pubudu Senanayake", "Richard Lyon"], "title": "Estimating excess mortality during the Covid-19 pandemic in Aotearoa New Zealand: Addendum", "comment": null, "summary": "In our previous article, we estimated excess mortality during in Aotearoa New Zealand for 2020 to 2023. Since our work was published, updated population estimates have been released by Statistics NZ. In this short letter, we provide the results of applying our original model to the new population data. Our updated excess mortality estimate of 2.0% (95% CI [0.5%, 3.3%]) is 1.3 percentage points higher than our original estimate because the new population estimates for the period 2020 to 2023 are smaller, but the main conclusions of our original article still apply."}
{"id": "2512.02831", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.02831", "abs": "https://arxiv.org/abs/2512.02831", "authors": ["Ali Alvandi", "Mina Rezaei"], "title": "Revisiting Theory of Contrastive Learning for Domain Generalization", "comment": "19 pages", "summary": "Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set."}
{"id": "2512.02249", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.02249", "abs": "https://arxiv.org/abs/2512.02249", "authors": ["Alejandro Jara", "Carlos Sing-Long"], "title": "Discrete Sequential Barycenter Arrays: Representation, Approximation, and Modeling of Probability Measures", "comment": "105 pages, 4 figures", "summary": "Constructing flexible probability models that respect constraints on key functionals -- such as the mean -- is a fundamental problem in nonparametric statistics. Existing approaches lack systematic tools for enforcing such constraints while retaining full modeling flexibility. This paper introduces a new representation for univariate probability measures based on discrete sequential barycenter arrays (SBA). We study structural properties of SBA representations and establish new approximation results. In particular, we show that for any target distribution, its SBA-based discrete approximations converge in both the weak topology and in Wasserstein distances, and that the representation is exact for all distributions with finite discrete support. We further characterize a broad class of measures whose SBA partitions exhibit regularity and induce increasingly fine meshes, and we prove that this class is dense in standard probabilistic topologies. These theoretical results enable the construction of probability models that preserve prescribed values -- or full distributions -- of the mean while maintaining large support. As an application, we derive a mixture model for density estimation whose induced mixing distribution has a fixed or user-specified mean. The resulting framework provides a principled mechanism for incorporating mean constraints in nonparametric modeling while preserving strong approximation properties. The approach is illustrated using both simulated and real data."}
{"id": "2512.02327", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.02327", "abs": "https://arxiv.org/abs/2512.02327", "authors": ["Jennifer N. Kampe", "David B. Dunson", "Celeste K. Carberry", "Julia E. Rager", "Daniel Zilber", "Kyle P. Messier"], "title": "Leveraging ontologies to predict biological activity of chemicals across genes", "comment": null, "summary": "High-throughput screening (HTS) is useful for evaluating chemicals for potential human health risks. However, given the extraordinarily large number of genes, assay endpoints, and chemicals of interest, available data are sparse, with dose-response curves missing for the vast majority of chemical-gene pairs. Although gene ontologies characterize similarity among genes with respect to known cellular functions and biological pathways, the sensitivity of various pathways to environmental contaminants remains unclear. We propose a novel Dose-Activity Response Tracking (DART) approach to predict the biological activity of chemicals across genes using information on chemical structural properties and gene ontologies within a Bayesian factor model. Designed to provide toxicologists with a flexible tool applicable across diverse HTS assay platforms, DART reveals the latent processes driving dose-response behavior and predicts new activity profiles for chemical-gene pairs lacking experimental data. We demonstrate the performance of DART through simulation studies and an application to a vast new multi-experiment data set consisting of dose-response observations generated by the exposure of HepG2 cells to per- and polyfluoroalkyl substances (PFAS), where it provides actionable guidance for chemical prioritization and inference on the structural and functional mechanisms underlying assay activation."}
{"id": "2512.02744", "categories": ["stat.ME", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.02744", "abs": "https://arxiv.org/abs/2512.02744", "authors": ["Rutger-Jan Lange", "Bram van Os", "Dick van Dijk"], "title": "Implicit score-driven filters for time-varying parameter models", "comment": "67 pages", "summary": "We propose an observation-driven modeling framework that permits time variation in the model parameters using an implicit score-driven (ISD) update. The ISD update maximizes the logarithmic observation density with respect to the parameter vector, while penalizing the weighted L2 norm relative to a one-step-ahead predicted parameter. This yields an implicit stochastic-gradient update. We show that the popular class of explicit score-driven (ESD) models arises if the observation log density is linearly approximated around the prediction. By preserving the full density, the ISD update globalizes favorable local properties of the ESD update. Namely, for log-concave observation densities, whether correctly specified or not, the ISD filter is stable for all learning rates, while its updates are contractive in mean squared error toward the (pseudo-)true parameter at every time step. We demonstrate the usefulness of ISD filters in simulations and empirical illustrations in finance and macroeconomics."}
{"id": "2512.02182", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.02182", "abs": "https://arxiv.org/abs/2512.02182", "authors": ["Sarah C. Lotspeich", "Cole Manschot"], "title": "Efficient and Intuitive Two-Phase Validation Across Multiple Models via Principal Components", "comment": "14 pages, 3 figures, 1 table, GitHub repositories with R package and simulation/analysis code", "summary": "Two-phase sampling offers a cost-effective way to validate error-prone measurements in observational databases or randomized trials. Inexpensive or easy-to-obtain information is collected for the entire study in Phase I. Then, a subset of patients undergoes cost-intensive validation to collect more accurate data in Phase II. Critically, any Phase I variables can be used to strategically select the Phase II subset, often enriched for a particular model of interest. However, when balancing primary and secondary analyses in the same study, competing models and priorities can result in poorly defined objectives for the most informative Phase II sampling criterion. We propose an intuitive, easy-to-use solution that balances and prioritizes explaining the largest amount of variability across all models of interest. Using principal components to succinctly summarize the inherent variability of the error-prone covariates for all models. Then, we sample patients with the most \"extreme\" principal components (i.e., the smallest or largest values) for validation. Through simulations and an application to data from the National Health and Nutrition Examination Survey (NHANES), we show that extreme tail sampling on the first principal component offers simultaneous efficiency gains across multiple models of interest relative to sampling for one specific model. Our proposed sampling strategy is implemented in the open-source R package, auditDesignR."}
{"id": "2512.02832", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.02832", "abs": "https://arxiv.org/abs/2512.02832", "authors": ["Fan Zhang", "Zhiming Li"], "title": "Analysis of hypothesis tests for multiple uncertain finite populations with applications to normal uncertainty distributions", "comment": "18 pages, 3 figures", "summary": "Hypothesis test plays a key role in uncertain statistics based on uncertain measure. This paper extends the parametric hypothesis of a single uncertain population to multiple cases, thereby addressing a broader range of scenarios. First, an uncertain family-wise error rate is defined to control the overall error in simultaneous testing. Subsequently, a hypothesis test of two uncertain populations is proposed, and the rejection region for the null hypothesis at a significance level is derived, laying the foundation for further analysis. Building on this, a homogeneity test for multiple populations is developed to assess whether the unknown population parameters differ significantly. When there is no significant difference in these parameters among finite populations or within a subset, a common test is used to determine whether they equal a fixed constant. Finally, homogeneity and common tests for normal uncertain populations with means and standard deviations are conducted under three cases: only means, only standard deviations, or both are unknown. Numerical simulations demonstrate the feasibility and accuracy of the proposed methods, and a real example is provided to illustrate their effectiveness."}
{"id": "2512.02744", "categories": ["stat.ME", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.02744", "abs": "https://arxiv.org/abs/2512.02744", "authors": ["Rutger-Jan Lange", "Bram van Os", "Dick van Dijk"], "title": "Implicit score-driven filters for time-varying parameter models", "comment": "67 pages", "summary": "We propose an observation-driven modeling framework that permits time variation in the model parameters using an implicit score-driven (ISD) update. The ISD update maximizes the logarithmic observation density with respect to the parameter vector, while penalizing the weighted L2 norm relative to a one-step-ahead predicted parameter. This yields an implicit stochastic-gradient update. We show that the popular class of explicit score-driven (ESD) models arises if the observation log density is linearly approximated around the prediction. By preserving the full density, the ISD update globalizes favorable local properties of the ESD update. Namely, for log-concave observation densities, whether correctly specified or not, the ISD filter is stable for all learning rates, while its updates are contractive in mean squared error toward the (pseudo-)true parameter at every time step. We demonstrate the usefulness of ISD filters in simulations and empirical illustrations in finance and macroeconomics."}
{"id": "2512.02878", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.02878", "abs": "https://arxiv.org/abs/2512.02878", "authors": ["Moritz Fabian Danzer", "Rene Schmidt"], "title": "Correcting for sampling variability in maximum likelihood-based one-sample log-rank tests", "comment": "Main manuscript: 10 pages, 3 figures, 2 tables Supplementary Material: 9 pages, 3 figures with multiple subfigures", "summary": "Single-arm studies in the early development phases of new treatments are not uncommon in the context of rare diseases or in paediatrics. If an assessment of efficacy is to be made at the end of such a study, the observed endpoints can be compared with reference values that can be derived from historical data. For a time-to-event endpoint, a statistical comparison with a reference curve can be made using the one-sample log-rank test. In order to ensure the interpretability of the results of this test, the role of the reference curve is crucial. This quantity is often estimated from a historical control group using a parametric procedure. Hence, it should be noted that it is subject to estimation uncertainty. However, this aspect is not taken into account in the one-sample log-rank test statistic. We analyse this estimation uncertainty for the common situation that the reference curve is estimated parametrically using the maximum likelihood method, and indicate how the variance estimation of the one-sample log-rank test can be adapted in order to take this variability into account. The resulting test procedures are illustrated using a data example and analysed in more detail using simulations."}
