{"id": "2512.17968", "categories": ["stat.CO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.17968", "abs": "https://arxiv.org/abs/2512.17968", "authors": ["Ravi Prasad"], "title": "A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework", "comment": null, "summary": "Monte Carlo algorithms are a foundational pillar of modern computational science, yet their effective application hinges on a deep understanding of their performance trade offs. This paper presents a critical analysis of the evolution of Monte Carlo algorithms, focusing on the persistent tension between statistical efficiency and computational cost. We describe the historical development from the foundational Metropolis Hastings algorithm to contemporary methods like Hamiltonian Monte Carlo. A central emphasis of this survey is the rigorous discussion of time and space complexity, including upper, lower, and asymptotic tight bounds for each major algorithm class. We examine the specific motivations for developing these methods and the key theoretical and practical observations such as the introduction of gradient information and adaptive tuning in HMC that led to successively better solutions. Furthermore, we provide a justification framework that discusses explicit situations in which using one algorithm is demonstrably superior to another for the same problem. The paper concludes by assessing the profound significance and impact of these algorithms and detailing major current research challenges."}
{"id": "2512.18884", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.18884", "abs": "https://arxiv.org/abs/2512.18884", "authors": ["Moreno Bevilacqua", "Xavier Emery", "Francisco Cuevas-Pacheco"], "title": "Fast simulation of Gaussian random fields with flexible correlation models in Euclidean spaces", "comment": null, "summary": "The efficient simulation of Gaussian random fields with flexible correlation structures is fundamental in spatial statistics, machine learning, and uncertainty quantification. In this work, we revisit the \\emph{spectral turning-bands} (STB) method as a versatile and scalable framework for simulating isotropic Gaussian random fields with a broad range of covariance models. Beyond the classical Matérn family, we show that the STB approach can be extended to two recent and flexible correlation classes that generalize the Matérn model: the Bummer-Tricomi model, which allows for polynomially decaying correlations and long-range dependence, and the Gauss-Hypergeometric model, which admits compactly supported correlations, including the Generalized Wendland family as a special case. We derive exact stochastic representations for both families: a Beta-prime mixture formulation for the Kummer-Tricomi model and complementary Beta- and Gasper-mixture representations for the Gauss-Hypergeometric model. These formulations enable exact, numerically stable, and computationally efficient simulation with linear complexity in the number of spectral components. Numerical experiments confirm the accuracy and computational stability of the proposed algorithms across a wide range of parameter configurations, demonstrating their practical viability for large-scale spatial modeling. As an application, we use the proposed STB simulators to perform parametric bootstrap for standard error estimation and model selection under weighted pairwise composite likelihood in the analysis of a large climate dataset."}
{"id": "2512.19589", "categories": ["stat.CO", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.19589", "abs": "https://arxiv.org/abs/2512.19589", "authors": ["Charles Shaw"], "title": "srvar-toolkit: A Python Implementation of Shadow-Rate Vector Autoregressions with Stochastic Volatility", "comment": null, "summary": "We introduce srvar-toolkit, an open-source Python package for Bayesian vector autoregression with shadow-rate constraints and stochastic volatility. The toolkit implements the methodology of Grammatikopoulos (2025, Journal of Forecasting) for forecasting macroeconomic variables when interest rates hit the effective lower bound. We provide conjugate Normal-Inverse-Wishart priors with Minnesota-style shrinkage, latent shadow-rate data augmentation via Gibbs sampling, diagonal stochastic volatility using the Kim-Shephard-Chib mixture approximation, and stochastic search variable selection. Core dependencies are NumPy, SciPy, and Pandas, with optional extras for plotting and a configuration-driven command-line interface. We release the software under the MIT licence at https://github.com/shawcharles/srvar-toolkit."}
{"id": "2512.17977", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.17977", "abs": "https://arxiv.org/abs/2512.17977", "authors": ["Holden Lee", "Matheau Santana-Gijzen"], "title": "Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler", "comment": null, "summary": "Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. In light of hardness results for sampling -- classical MCMC methods, even with tempering, can suffer from exponential mixing times -- a natural question is how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes. To address this, we introduce Reweighted ALPS (Re-ALPS), a modified version of the Annealed Leap-Point Sampler (ALPS) that dispenses with the Gaussian approximation assumption. We prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. Similarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and at the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is crucial in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance compared to ALPS on a mixture of heavy-tailed distributions."}
{"id": "2512.18013", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18013", "abs": "https://arxiv.org/abs/2512.18013", "authors": ["Shirsa Maitra", "Tathagata Banerjee", "Anushka De", "Diganta Mukherjee", "Tridib Mukherjee"], "title": "Empirical parameterization of the Elo Rating System", "comment": "15 pages, 3 figures", "summary": "This study aims to provide a data-driven approach for empirically tuning and validating rating systems, focusing on the Elo system. Well-known rating frameworks, such as Elo, Glicko, TrueSkill systems, rely on parameters that are usually chosen based on probabilistic assumptions or conventions, and do not utilize game-specific data. To address this issue, we propose a methodology that learns optimal parameter values by maximizing the predictive accuracy of match outcomes. The proposed parameter-tuning framework is a generalizable method that can be extended to any rating system, even for multiplayer setups, through suitable modification of the parameter space. Implementation of the rating system on real and simulated gameplay data demonstrates the suitability of the data-driven rating system in modeling player performance."}
{"id": "2512.17977", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.17977", "abs": "https://arxiv.org/abs/2512.17977", "authors": ["Holden Lee", "Matheau Santana-Gijzen"], "title": "Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler", "comment": null, "summary": "Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. In light of hardness results for sampling -- classical MCMC methods, even with tempering, can suffer from exponential mixing times -- a natural question is how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes. To address this, we introduce Reweighted ALPS (Re-ALPS), a modified version of the Annealed Leap-Point Sampler (ALPS) that dispenses with the Gaussian approximation assumption. We prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. Similarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and at the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is crucial in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance compared to ALPS on a mixture of heavy-tailed distributions."}
{"id": "2512.17977", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.17977", "abs": "https://arxiv.org/abs/2512.17977", "authors": ["Holden Lee", "Matheau Santana-Gijzen"], "title": "Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler", "comment": null, "summary": "Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. In light of hardness results for sampling -- classical MCMC methods, even with tempering, can suffer from exponential mixing times -- a natural question is how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes. To address this, we introduce Reweighted ALPS (Re-ALPS), a modified version of the Annealed Leap-Point Sampler (ALPS) that dispenses with the Gaussian approximation assumption. We prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. Similarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and at the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is crucial in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance compared to ALPS on a mixture of heavy-tailed distributions."}
{"id": "2512.18066", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18066", "abs": "https://arxiv.org/abs/2512.18066", "authors": ["Annie S. Booth"], "title": "Deep Gaussian Processes with Gradients", "comment": "16 pages, 8 figures", "summary": "Deep Gaussian processes (DGPs) are popular surrogate models for complex nonstationary computer experiments. DGPs use one or more latent Gaussian processes (GPs) to warp the input space into a plausibly stationary regime, then use typical GP regression on the warped domain. While this composition of GPs is conceptually straightforward, the functional nature of the multi-dimensional latent warping makes Bayesian posterior inference challenging. Traditional GPs with smooth kernels are naturally suited for the integration of gradient information, but the integration of gradients within a DGP presents new challenges and has yet to be explored. We propose a novel and comprehensive Bayesian framework for DGPs with gradients that facilitates both gradient-enhancement and gradient posterior predictive distributions. We provide open-source software in the \"deepgp\" package on CRAN, with optional Vecchia approximation to circumvent cubic computational bottlenecks. We benchmark our DGPs with gradients on a variety of nonstationary simulations, showing improvement over both GPs with gradients and conventional DGPs."}
{"id": "2512.18166", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.18166", "abs": "https://arxiv.org/abs/2512.18166", "authors": ["Jayani P. Gamage", "Dianne Cook", "Paul Harrison", "Michael Lydeamore", "Thiyanga S. Talagala"], "title": "quollr: An R Package for Visualizing 2-D Models from Nonlinear Dimension Reductions in High-Dimensional Space", "comment": null, "summary": "Nonlinear dimension reduction methods provide a low-dimensional representation of high-dimensional data by applying a Nonlinear transformation. However, the complexity of the transformations and data structures can create wildly different representations depending on the method and hyper-parameter choices. It is difficult to determine whether any of these representations are accurate, which one is the best, or whether they have missed important structures. The R package quollr has been developed as a new visual tool to determine which method and which hyper-parameter choices provide the most accurate representation of high-dimensional data. The scurve data from the package is used to illustrate the algorithm. Single-cell RNA sequencing (scRNA-seq) data from mouse limb muscles are used to demonstrate the usability of the package."}
{"id": "2512.18118", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18118", "abs": "https://arxiv.org/abs/2512.18118", "authors": ["Matteo Sesia", "Vladimir Svetnik"], "title": "Distribution-Free Selection of Low-Risk Oncology Patients for Survival Beyond a Time Horizon", "comment": null, "summary": "We study the problem of selecting a subset of patients who are unlikely to experience an event within a specified time horizon, by calibrating a screening rule based on the output of a black-box survival model. This statistics problem has many applications in medicine, including identifying candidates for treatment de-escalation and prioritizing the allocation of limited medical resources. In this paper, we compare two families of methods that can provide different types of distribution-free guarantees for this task: (i) high-probability risk control and (ii) expectation-based false discovery rate control using conformal $p$-values. We clarify the relation between these two frameworks, which have important conceptual differences, and explain how each can be adapted to analyze time-to-event data using inverse probability of censoring weighting. Through experiments on semi-synthetic and real oncology data from the Flatiron Health Research Database, we find that both approaches often achieve the desired survival rate among selected patients, but with distinct efficiency profiles. The conformal method tends to be more powerful, whereas high-probability risk control offers stronger guarantees at the cost of some additional conservativeness. Finally, we provide practical guidance on implementation and parameter tuning."}
{"id": "2512.18168", "categories": ["stat.ME", "cs.IT", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.18168", "abs": "https://arxiv.org/abs/2512.18168", "authors": ["Jian Ma"], "title": "Copula Entropy: Theory and Applications", "comment": null, "summary": "This is the monograph on the theory and applications of copula entropy (CE). This book first introduces the theory of CE, including its background, definition, theorems, properties, and estimation methods. The theoretical applications of CE to structure learning, association discovery, variable selection, causal discovery, system identification, time lag estimation, domain adaptation, multivariate normality test, copula hypothesis test, two-sample test, change point detection, and symmetry test are reviewed. The relationships between the theoretical applications and their connections to correlation and causality are discussed. The framework based on CE for measuring statistical independence and conditional independence is compared to the other similar ones. The advantages of CE based methodologies over the other comparable ones are evaluated with simulations. The mathematical generalizations of CE are reviewed. The real applications of CE to every branch of science and engineering are briefly introduced."}
{"id": "2512.18083", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18083", "abs": "https://arxiv.org/abs/2512.18083", "authors": ["Ashley Zhang"], "title": "Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty", "comment": null, "summary": "Standard approaches to causal inference, such as Outcome Regression and Inverse Probability Weighted Regression Adjustment (IPWRA), are typically derived through the lens of missing data imputation and identification theory. In this work, we unify these methods from a Machine Learning perspective, reframing ATE estimation as a \\textit{domain adaptation problem under distribution shift}. We demonstrate that the canonical Hajek estimator is a special case of IPWRA restricted to a constant hypothesis class, and that IPWRA itself is fundamentally Importance-Weighted Empirical Risk Minimization designed to correct for the covariate shift between the treated sub-population and the target population.\n  Leveraging this unified framework, we critically examine the optimization objectives of Doubly Robust estimators. We argue that standard methods enforce \\textit{sufficient but not necessary} conditions for consistency by requiring outcome models to be individually unbiased. We define the true \"ATE Risk Function\" and show that minimizing it requires only that the biases of the treated and control models structurally cancel out. Exploiting this insight, we propose the \\textbf{Joint Robust Estimator (JRE)}. Instead of treating propensity estimation and outcome modeling as independent stages, JRE utilizes bootstrap-based uncertainty quantification of the propensity score to train outcome models jointly. By optimizing for the expected ATE risk over the distribution of propensity scores, JRE leverages model degrees of freedom to achieve robustness against propensity misspecification. Simulation studies demonstrate that JRE achieves up to a 15\\% reduction in MSE compared to standard IPWRA in finite-sample regimes with misspecified outcome models."}
{"id": "2512.18069", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18069", "abs": "https://arxiv.org/abs/2512.18069", "authors": ["Simion De", "Jared D. Huling"], "title": "Data adaptive covariate balancing for causal effect estimation for high dimensional data", "comment": null, "summary": "A key challenge in estimating causal effects from observational data is handling confounding and is commonly achieved through weighting methods that balance distribution of covariates between treatment and control groups. Weighting approaches can be classified by whether weights are estimated using parametric or nonparametric methods, and by whether the model relies on modeling and inverting the propensity score or directly estimates weights to achieve distributional balance by minimizing a measure of dissimilarity between groups. Parametric methods, both for propensity score modeling and direct balancing, are prone to model misspecification. In addition, balancing approaches often suffer from the curse of dimensionality, as they assign equal importance to all covariates, thus potentially de-emphasizing true confounders. Several methods, such as the outcome adaptive lasso, attempt to mitigate this issue through variable selection, but are parametric and focus on propensity score estimation rather than direct balancing. In this paper, we propose a nonparametric direct balancing approach that uses random forests to adaptively emphasize confounders. Our method jointly models treatment and outcome using random forests, allowing the data to identify covariates that influence both processes. We construct a similarity measure, defined by the proportion of trees in which two observations fall into the same leaf node, yielding a distance between treatment and control distributions that is sensitive to relevant covariates and captures the structure of confounding. Under suitable assumptions, we show that the resulting weights converge to normalized inverse propensity scores in the L2 norm and provide consistent treatment effect estimates. We demonstrate the effectiveness of our approach through extensive simulations and an application to a real dataset."}
{"id": "2512.18627", "categories": ["stat.ME", "econ.EM", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.18627", "abs": "https://arxiv.org/abs/2512.18627", "authors": ["Shunsuke Imai"], "title": "Accuracy of Uniform Inference on Fine Grid Points", "comment": null, "summary": "Uniform confidence bands for functions are widely used in empirical analysis. A variety of simple implementation methods (most notably multiplier bootstrap) have been proposed and theoretically justified. However, an implementation over a literally continuous index set is generally computationally infeasible, and practitioners therefore compute the critical value by evaluating the statistic on a finite evaluation grid. This paper quantifies how fine the evaluation grid must be for a multiplier bootstrap procedure over finite grid points to deliver valid uniform confidence bands. We derive an explicit bound on the resulting coverage error that separates discretization effects from the intrinsic high-dimensional bootstrap approximation error on the grid. The bound yields a transparent workflow for choosing the grid size in practice, and we illustrate the implementation through an example of kernel density estimation."}
{"id": "2512.18467", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18467", "abs": "https://arxiv.org/abs/2512.18467", "authors": ["Supratim Das", "Sarthak Sarkar", "Subhamoy Maitra", "Tridib Mukherjee"], "title": "Analysing Skill Predominance in Generalized Fantasy Cricket", "comment": null, "summary": "In fantasy sports, strategic thinking-not mere luck-often defines who wins and who falls short. As fantasy cricket grows in popularity across India, understanding whether success stems from skill or chance has become both an analytical and regulatory question. This study introduces a new limited-selection contest framework in which participants choose from four expert-designed teams and share prizes based on the highest cumulative score. By combining simulation experiments with real performance data from the 2024 Indian Premier League (IPL), we evaluate whether measurable skill emerges within this structure. Results reveal that strategic and informed team selection consistently outperforms random choice, underscoring a clear skill advantage that persists despite stochastic variability. The analysis quantifies how team composition, inter-team correlation, and participant behaviour jointly influence winning probabilities, highlighting configurations where skill becomes statistically dominant. These findings provide actionable insights for players seeking to maximise returns through strategy and for platform designers aiming to develop fair, transparent, and engaging skill-based gaming ecosystems that balance competition with regulatory compliance."}
{"id": "2512.18584", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.18584", "abs": "https://arxiv.org/abs/2512.18584", "authors": ["Marios Papamichalis", "Regina Ruane", "Theofanis Papamichalis"], "title": "State-Space Modeling of Time-Varying Spillovers on Networks", "comment": "While under review, the material in this paper may be reorganized to meet journal requirements", "summary": "Many modern time series arise on networks, where each component is attached to a node and interactions follow observed edges. Classical time-varying parameter VARs (TVP-VARs) treat all series symmetrically and ignore this structure, while network autoregressive models exploit a given graph but usually impose constant parameters and stationarity. We develop network state-space models in which a low-dimensional latent state controls time-varying network spillovers, own-lag persistence and nodal covariate effects. A key special case is a network time-varying parameter VAR (NTVP-VAR) that constrains each lag matrix to be a linear combination of known network operators, such as a row-normalised adjacency and the identity, and lets the associated coefficients evolve stochastically in time. The framework nests Gaussian and Poisson network autoregressions, network ARIMA models with graph differencing, and dynamic edge models driven by multivariate logistic regression. We give conditions ensuring that NTVP-VARs are well-defined in second moments despite nonstationary states, describe network versions of stability and local stationarity, and discuss shrinkage, thresholding and low-rank tensor structures for high-dimensional graphs. Conceptually, network state-space models separate where interactions may occur (the graph) from how strong they are at each time (the latent state), providing an interpretable alternative to both unstructured TVP-VARs and existing network time-series models."}
{"id": "2512.18720", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18720", "abs": "https://arxiv.org/abs/2512.18720", "authors": ["Feng Yu", "MD Saifur Rahman Mazumder", "Ying Su", "Oscar Contreras Velasco"], "title": "Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning", "comment": null, "summary": "Effective feature selection is essential for high-dimensional data analysis and machine learning. Unsupervised feature selection (UFS) aims to simultaneously cluster data and identify the most discriminative features. Most existing UFS methods linearly project features into a pseudo-label space for clustering, but they suffer from two critical limitations: (1) an oversimplified linear mapping that fails to capture complex feature relationships, and (2) an assumption of uniform cluster distributions, ignoring outliers prevalent in real-world data. To address these issues, we propose the Robust Autoencoder-based Unsupervised Feature Selection (RAEUFS) model, which leverages a deep autoencoder to learn nonlinear feature representations while inherently improving robustness to outliers. We further develop an efficient optimization algorithm for RAEUFS. Extensive experiments demonstrate that our method outperforms state-of-the-art UFS approaches in both clean and outlier-contaminated data settings."}
{"id": "2512.18119", "categories": ["stat.ME", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18119", "abs": "https://arxiv.org/abs/2512.18119", "authors": ["Kohei Watanabe"], "title": "Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences", "comment": "34 pages", "summary": "Social scientists employ latent Dirichlet allocation (LDA) to find highly specific topics in large corpora, but they often struggle in this task because (1) LDA, in general, takes a significant amount of time to fit on large corpora; (2) unsupervised LDA fragments topics into sub-topics in short documents; (3) semi-supervised LDA fails to identify specific topics defined using seed words. To solve these problems, I have developed a new topic model called distributed asymmetric allocation (DAA) that integrates multiple algorithms for efficiently identifying sentences about important topics in large corpora. I evaluate the ability of DAA to identify politically important topics by fitting it to the transcripts of speeches at the United Nations General Assembly between 1991 and 2017. The results show that DAA can classify sentences significantly more accurately and quickly than LDA thanks to the new algorithms. More generally, the results demonstrate that it is important for social scientists to optimize Dirichlet priors of LDA to perform content analysis accurately."}
{"id": "2512.19398", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.19398", "abs": "https://arxiv.org/abs/2512.19398", "authors": ["Jiahua Jiang", "Joseph Marsh", "Rowland G Seymour"], "title": "A Reduced Basis Decomposition Approach to Efficient Data Collection in Pairwise Comparison Studies", "comment": "24 pages, 2 tables, 3 figures", "summary": "Comparative judgement studies elicit quality assessments through pairwise comparisons, typically analysed using the Bradley-Terry model. A challenge in these studies is experimental design, specifically, determining the optimal pairs to compare to maximize statistical efficiency. Constructing static experimental designs for these studies requires spectral decomposition of a covariance matrix over pairs of pairs, which becomes computationally infeasible for studies with more than approximately 150 objects. We propose a scalable method based on reduced basis decomposition that bypasses explicit construction of this matrix, achieving computational savings of two to three orders of magnitude. We establish eigenvalue bounds guaranteeing approximation quality and characterise the rank structure of the design matrix. Simulations demonstrate speedup factors exceeding 100 for studies with 64 or more objects, with negligible approximation error. We apply the method to construct designs for a 452-region spatial study in under 7 minutes and enable real-time design updates for classroom peer assessment, reducing computation time from 15 minutes to 15 seconds."}
{"id": "2512.18760", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18760", "abs": "https://arxiv.org/abs/2512.18760", "authors": ["Maria Laura Battagliola", "Laura J. Benoit", "Sarah Canetta", "Shizhe Zhang", "R. Todd Ogden"], "title": "Functional Modeling of Learning and Memory Dynamics in Cognitive Disorders", "comment": null, "summary": "Deficits in working memory, which includes both the ability to learn and to retain information short-term, are a hallmark of many cognitive disorders. Our study analyzes data from a neuroscience experiment on animal subjects, where performance on a working memory task was recorded as repeated binary success or failure data. We estimate continuous probability of success curves from this binary data in the context of functional data analysis, which is largely used in biological processes that are intrinsically continuous. We then register these curves to decompose each function into its amplitude, representing overall performance, and its phase, representing the speed of learning or response. Because we are able to separate speed from performance, we can address the crucial question of whether a cognitive disorder impacts not only how well subjects can learn and remember, but also how fast. This allows us to analyze the components jointly to uncover how speed and performance co-vary, and to compare them separately to pinpoint whether group differences stem from a deficit in peak performance or a change in speed."}
{"id": "2512.18924", "categories": ["stat.ME", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.18924", "abs": "https://arxiv.org/abs/2512.18924", "authors": ["Jonquil Z. Liao", "Joshua Cape"], "title": "Testing for latent structure via the Wilcoxon--Wigner random matrix of normalized rank statistics", "comment": "17 pages main text, 2 figures, 3 tables, 43 pages supplementary material", "summary": "This paper considers the problem of testing for latent structure in large symmetric data matrices. The goal here is to develop statistically principled methodology that is flexible in its applicability, computationally efficient, and insensitive to extreme data variation, thereby overcoming limitations facing existing approaches. To do so, we introduce and systematically study certain symmetric matrices, called Wilcoxon--Wigner random matrices, whose entries are normalized rank statistics derived from an underlying independent and identically distributed sample of absolutely continuous random variables. These matrices naturally arise as the matricization of one-sample problems in statistics and conceptually lie at the interface of nonparametrics, multivariate analysis, and data reduction. Among our results, we establish that the leading eigenvalue and corresponding eigenvector of Wilcoxon--Wigner random matrices admit asymptotically Gaussian fluctuations with explicit centering and scaling terms. These asymptotic results enable rigorous parameter-free and distribution-free spectral methodology for addressing two hypothesis testing problems, namely community detection and principal submatrix detection. Numerical examples illustrate the performance of the proposed approach. Throughout, our findings are juxtaposed with existing results based on the spectral properties of independent entry symmetric random matrices in signal-plus-noise data settings."}
{"id": "2512.18971", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18971", "abs": "https://arxiv.org/abs/2512.18971", "authors": ["Shuntuo Xu", "Zhou Yu", "Jian Huang"], "title": "On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction", "comment": null, "summary": "Identifying low-dimensional sufficient structures in nonlinear sufficient dimension reduction (SDR) has long been a fundamental yet challenging problem. Most existing methods lack theoretical guarantees of exhaustiveness in identifying lower dimensional structures, either at the population level or at the sample level. We tackle this issue by proposing a new method, generative sufficient dimension reduction (GenSDR), which leverages modern generative models. We show that GenSDR is able to fully recover the information contained in the central $σ$-field at both the population and sample levels. In particular, at the sample level, we establish a consistency property for the GenSDR estimator from the perspective of conditional distributions, capitalizing on the distributional learning capabilities of deep generative models. Moreover, by incorporating an ensemble technique, we extend GenSDR to accommodate scenarios with non-Euclidean responses, thereby substantially broadening its applicability. Extensive numerical results demonstrate the outstanding empirical performance of GenSDR and highlight its strong potential for addressing a wide range of complex, real-world tasks."}
{"id": "2512.18143", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18143", "abs": "https://arxiv.org/abs/2512.18143", "authors": ["Konstantin Larin", "Daniel R. Kowal"], "title": "Efficient Bayesian inference for two-stage models in environmental epidemiology", "comment": "35 pages (main text only), 9 Figures", "summary": "Statistical models often require inputs that are not completely known. This can occur when inputs are measured with error, indirectly, or when they are predicted using another model. In environmental epidemiology, air pollution exposure is a key determinant of health, yet typically must be estimated for each observational unit by a complex model. Bayesian two-stage models combine this stage-one model with a stage-two model for the health outcome given the exposure. However, analysts usually only have access to the stage-one model output without all of its specifications or input data, making joint Bayesian inference apparently intractable. We show that two prominent workarounds-using a point estimate or using the posterior from the stage-one model without feedback from the stage-two model-lead to miscalibrated inference. Instead, we propose efficient algorithms to facilitate joint Bayesian inference and provide more accurate estimates and well-calibrated uncertainties. Comparing different approaches, we investigate the association between PM2.5 exposure and county-level mortality rates in the South-Central USA."}
{"id": "2512.19035", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19035", "abs": "https://arxiv.org/abs/2512.19035", "authors": ["Michael R. Schwob", "Nicholas M. Calzada", "Justin J. Van Ee", "Diana Gamba", "Rebecca A. Nelson", "Megan L. Vahsen", "Peter B. Adler", "Jesse R. Lasky", "Mevin B. Hooten"], "title": "Dyadic Flow Models for Nonstationary Gene Flow in Landscape Genomics", "comment": "43 pages, 7 figures, 5 appendices", "summary": "The field of landscape genomics aims to infer how landscape features affect gene flow across space. Most landscape genomic frameworks assume the isolation-by-distance and isolation-by-resistance hypotheses, which propose that genetic dissimilarity increases as a function of distance and as a function of cumulative landscape resistance, respectively. While these hypotheses are valid in certain settings, other mechanisms may affect gene flow. For example, the gene flow of invasive species may depend on founder effects and multiple introductions. Such mechanisms are not considered in modern landscape genomic models. We extend dyadic models to allow for mechanisms that range-shifting and/or invasive species may experience by introducing dyadic spatially-varying coefficients (DSVCs) defined on source-destination pairs. The DSVCs allow the effects of landscape on gene flow to vary across space, capturing nonstationary and asymmetric connectivity. Additionally, we incorporate explicit landscape features as connectivity covariates, which are localized to specific regions of the spatial domain and may function as barriers or corridors to gene flow. Such covariates are central to colonization and invasion, where spread accelerates along corridors and slows across landscape barriers. The proposed framework accommodates colonization-specific processes while retaining the ability to assess landscape influences on gene flow. Our case study of the highly invasive cheatgrass (Bromus tectorum) demonstrates the necessity of accounting for nonstationarity gene flow in range-shifting species."}
{"id": "2512.19373", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19373", "abs": "https://arxiv.org/abs/2512.19373", "authors": ["Xin Huang", "Jia Li", "Jun Yu"], "title": "Cluster-Based Generalized Additive Models Informed by Random Fourier Features", "comment": "25 pages, 13 figures, 4 tables", "summary": "Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling."}
{"id": "2512.18149", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18149", "abs": "https://arxiv.org/abs/2512.18149", "authors": ["Kento Okuyama", "Tim Fabian Schaffland", "Pascal Kilian", "Holger Brandt", "Augustin Kelava"], "title": "Frequentist forecasting in regime-switching models with extended Hamilton filter", "comment": "31 pages, 3 figures, 11 tables", "summary": "Psychological change processes, such as university student dropout in math, often exhibit discrete latent state transitions and can be studied using regime-switching models with intensive longitudinal data (ILD). Recently, regime-switching state-space (RSSS) models have been extended to allow for latent variables and their autoregressive effects. Despite this progress, estimation methods for handling both intra-individual changes and inter-individual differences as predictors of regime-switches need further exploration. Specifically, there's a need for frequentist estimation methods in dynamic latent variable frameworks that allow real-time inferences and forecasts of latent or observed variables during ongoing data collection. Building on Chow and Zhang's (2013) extended Kim filter, we introduce a first frequentist filter for RSSS models which allows hidden Markov(-switching) models to depend on both latent within- and between-individual characteristics. As a counterpart of Kelava et al.'s (2022) Bayesian forecasting filter for nonlinear dynamic latent class structural equation models (NDLC-SEM), our proposed method is the first frequentist approach within this general class of models. In an empirical study, the filter is applied to forecast emotions and behavior related to student dropout in math. Parameter recovery and prediction of regime and dynamic latent variables are evaluated through simulation study."}
{"id": "2512.19064", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.19064", "abs": "https://arxiv.org/abs/2512.19064", "authors": ["Nicole Fontana", "Francesca Ieva", "Luisa Zuccolo", "Emanuele Di Angelantonio", "Piercesare Secchi"], "title": "Unraveling time-varying causal effects of multiple exposures: integrating Functional Data Analysis with Multivariable Mendelian Randomization", "comment": null, "summary": "Mendelian Randomization is a widely used instrumental variable method for assessing causal effects of lifelong exposures on health outcomes. Many exposures, however, have causal effects that vary across the life course and often influence outcomes jointly with other exposures or indirectly through mediating pathways. Existing approaches to multivariable Mendelian Randomization assume constant effects over time and therefore fail to capture these dynamic relationships. We introduce Multivariable Functional Mendelian Randomization (MV-FMR), a new framework that extends functional Mendelian Randomization to simultaneously model multiple time-varying exposures. The method combines functional principal component analysis with a data-driven cross-validation strategy for basis selection and accounts for overlapping instruments and mediation effects. Through extensive simulations, we assessed MV-FMR's ability to recover time-varying causal effects under a range of data-generating scenarios and compared the performance of joint versus separate exposure effect estimation strategies. Across scenarios involving nonlinear effects, horizontal pleiotropy, mediation, and sparse data, MV-FMR consistently recovered the true causal functions and outperformed univariable approaches. To demonstrate its practical value, we applied MV-FMR to UK Biobank data to investigate the time-varying causal effects of systolic blood pressure and body mass index on coronary artery disease. MV-FMR provides a flexible and interpretable framework for disentangling complex time-dependent causal processes and offers new opportunities for identifying life-course critical periods and actionable drivers relevant to disease prevention."}
{"id": "2512.18924", "categories": ["stat.ME", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.18924", "abs": "https://arxiv.org/abs/2512.18924", "authors": ["Jonquil Z. Liao", "Joshua Cape"], "title": "Testing for latent structure via the Wilcoxon--Wigner random matrix of normalized rank statistics", "comment": "17 pages main text, 2 figures, 3 tables, 43 pages supplementary material", "summary": "This paper considers the problem of testing for latent structure in large symmetric data matrices. The goal here is to develop statistically principled methodology that is flexible in its applicability, computationally efficient, and insensitive to extreme data variation, thereby overcoming limitations facing existing approaches. To do so, we introduce and systematically study certain symmetric matrices, called Wilcoxon--Wigner random matrices, whose entries are normalized rank statistics derived from an underlying independent and identically distributed sample of absolutely continuous random variables. These matrices naturally arise as the matricization of one-sample problems in statistics and conceptually lie at the interface of nonparametrics, multivariate analysis, and data reduction. Among our results, we establish that the leading eigenvalue and corresponding eigenvector of Wilcoxon--Wigner random matrices admit asymptotically Gaussian fluctuations with explicit centering and scaling terms. These asymptotic results enable rigorous parameter-free and distribution-free spectral methodology for addressing two hypothesis testing problems, namely community detection and principal submatrix detection. Numerical examples illustrate the performance of the proposed approach. Throughout, our findings are juxtaposed with existing results based on the spectral properties of independent entry symmetric random matrices in signal-plus-noise data settings."}
{"id": "2512.18166", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.18166", "abs": "https://arxiv.org/abs/2512.18166", "authors": ["Jayani P. Gamage", "Dianne Cook", "Paul Harrison", "Michael Lydeamore", "Thiyanga S. Talagala"], "title": "quollr: An R Package for Visualizing 2-D Models from Nonlinear Dimension Reductions in High-Dimensional Space", "comment": null, "summary": "Nonlinear dimension reduction methods provide a low-dimensional representation of high-dimensional data by applying a Nonlinear transformation. However, the complexity of the transformations and data structures can create wildly different representations depending on the method and hyper-parameter choices. It is difficult to determine whether any of these representations are accurate, which one is the best, or whether they have missed important structures. The R package quollr has been developed as a new visual tool to determine which method and which hyper-parameter choices provide the most accurate representation of high-dimensional data. The scurve data from the package is used to illustrate the algorithm. Single-cell RNA sequencing (scRNA-seq) data from mouse limb muscles are used to demonstrate the usability of the package."}
{"id": "2512.19627", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.19627", "abs": "https://arxiv.org/abs/2512.19627", "authors": ["Elliot Fisher", "Robin Smith"], "title": "Ant Colony Optimisation applied to the Travelling Santa Problem", "comment": "Submitted to Frontiers in Applied Mathematics and Statistics", "summary": "The hypothetical global delivery schedule of Santa Claus must follow strict rolling night-time windows that vary with the Earth's rotation and obey an energy budget that depends on payload size and cruising speed. To design this schedule, the Travelling-Santa Ant-Colony Optimisation framework (TSaP-ACO) was developed. This heuristic framework constructs potential routes via a population of artificial ants that iteratively extend partial paths. Ants make their decisions much like they do in nature, following pheromones left by other ants, but with a degree of permitted exploration. This approach: (i) embeds local darkness feasibility directly into the pheromone heuristic, (ii) seeks to minimise aerodynamic work via a shrinking sleigh cross sectional area, (iii) uses a low-cost \"rogue-ant\" reversal to capture direction-sensitive time-zones, and (iv) tunes leg-specific cruise speeds on the fly. On benchmark sets of 15 and 30 capital cities, the TSaP-ACO eliminates all daylight violations and reduces total work by up to 10% compared to a distance-only ACO. In a 40-capital-city stress test, it cuts energy use by 88%, and shortens tour length by around 67%. Population-first routing emerges naturally from work minimisation (50% served by leg 11 of 40). These results demonstrate that rolling-window, energy-aware ACO has potential applications more realistic global delivery scenarios."}
{"id": "2512.18168", "categories": ["stat.ME", "cs.IT", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.18168", "abs": "https://arxiv.org/abs/2512.18168", "authors": ["Jian Ma"], "title": "Copula Entropy: Theory and Applications", "comment": null, "summary": "This is the monograph on the theory and applications of copula entropy (CE). This book first introduces the theory of CE, including its background, definition, theorems, properties, and estimation methods. The theoretical applications of CE to structure learning, association discovery, variable selection, causal discovery, system identification, time lag estimation, domain adaptation, multivariate normality test, copula hypothesis test, two-sample test, change point detection, and symmetry test are reviewed. The relationships between the theoretical applications and their connections to correlation and causality are discussed. The framework based on CE for measuring statistical independence and conditional independence is compared to the other similar ones. The advantages of CE based methodologies over the other comparable ones are evaluated with simulations. The mathematical generalizations of CE are reviewed. The real applications of CE to every branch of science and engineering are briefly introduced."}
{"id": "2512.19681", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.19681", "abs": "https://arxiv.org/abs/2512.19681", "authors": ["Elizabeth B. Amona", "Indranil Sahoo", "David Chan", "Marianne B. Lund", "Miriam Kuttikat"], "title": "An Adaptive Graphical Lasso Approach to Modeling Symptom Networks of Common Mental Disorders in Eritrean Refugee Population", "comment": "34 pages, 7 figures", "summary": "Despite the significant public health burden of common mental disorders (CMDs) among refugee populations, their underlying symptom structures remain underexplored. This study uses Gaussian graphical modeling to examine the symptom network of post-traumatic stress disorder (PTSD), depression, anxiety, and somatic distress among Eritrean refugees in the Greater Washington, DC area. Given the small sample size (n) and high-dimensional symptom space (p), we propose a novel extension of the standard graphical LASSO by incorporating adaptive penalization, which improves sparsity selection and network estimation stability under n < p conditions. To evaluate the reliability of the network, we apply bootstrap resampling and use centrality measures to identify the most influential symptoms. Our analysis identifies six distinct symptom clusters, with somatic-anxiety symptoms forming the most interconnected group. Notably, symptoms such as nausea and reliving past experiences emerge as central symptoms linking PTSD, anxiety, depression, and somatic distress. Additionally, we identify symptoms like feeling fearful, sleep problems, and loss of interest in activities as key symptoms, either being closely positioned to many others or acting as important bridges that help maintain the overall network connectivity, thereby highlighting their potential importance as possible intervention targets."}
{"id": "2512.18172", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18172", "abs": "https://arxiv.org/abs/2512.18172", "authors": ["Jayani P. Gamage", "Dianne Cook", "Paul Harrison", "Michael Lydeamore", "Thiyanga S. Talagala"], "title": "cardinalR: Generating Interesting High-Dimensional Data Structures", "comment": null, "summary": "Simulated high-dimensional data is useful for testing, validating, and improving algorithms used in dimension reduction, supervised and unsupervised learning. High-dimensional data is characterized by multiple variables that are dependent or associated in some way, such as linear, nonlinear, clustering or anomalies. Here we provide new methods for generating a variety of high-dimensional structures using mathematical functions and statistical distributions organized into the R package cardinalR. Several example data sets are also provided. These will be useful for researchers to better understand how different analytical methods work and can be improved, with a special focus on nonlinear dimension reduction methods. This package enriches the existing toolset of benchmark datasets for evaluating algorithms."}
{"id": "2512.18172", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18172", "abs": "https://arxiv.org/abs/2512.18172", "authors": ["Jayani P. Gamage", "Dianne Cook", "Paul Harrison", "Michael Lydeamore", "Thiyanga S. Talagala"], "title": "cardinalR: Generating Interesting High-Dimensional Data Structures", "comment": null, "summary": "Simulated high-dimensional data is useful for testing, validating, and improving algorithms used in dimension reduction, supervised and unsupervised learning. High-dimensional data is characterized by multiple variables that are dependent or associated in some way, such as linear, nonlinear, clustering or anomalies. Here we provide new methods for generating a variety of high-dimensional structures using mathematical functions and statistical distributions organized into the R package cardinalR. Several example data sets are also provided. These will be useful for researchers to better understand how different analytical methods work and can be improved, with a special focus on nonlinear dimension reduction methods. This package enriches the existing toolset of benchmark datasets for evaluating algorithms."}
{"id": "2512.18250", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18250", "abs": "https://arxiv.org/abs/2512.18250", "authors": ["Kenichi Satoh"], "title": "Applying non-negative matrix factorization with covariates to structural equation modeling for blind input-output analysis", "comment": "16 pages, 3 figures", "summary": "Structural equation modeling (SEM) describes directed dependence and feedback, whereas non-negative matrix factorization (NMF) provides interpretable, parts-based representations for non-negative data. We propose NMF-SEM, a unified non-negative framework that embeds NMF within a simultaneous-equation structure, enabling latent feedback loops and a reduced-form input-output mapping when intermediate flows are unobserved. The mapping separates direct effects from cumulative propagation effects and summarizes reinforcement using an amplification ratio.\n  We develop regularized multiplicative-update estimation with orthogonality and sparsity penalties, and introduce structural evaluation metrics for input-output fidelity, second-moment (covariance-like) agreement, and feedback strength. Applications show that NMF-SEM recovers the classical three-factor structure in the Holzinger-Swineford data, identifies climate- and pollutant-driven mortality pathways with negligible feedback in the Los Angeles system, and separates deprivation, general morbidity, and deaths-of-despair components with weak feedback in Mississippi health outcomes."}
{"id": "2512.18785", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18785", "abs": "https://arxiv.org/abs/2512.18785", "authors": ["Renato Panaro", "Christian Röver", "Tim Friede"], "title": "Consistent Bayesian meta-analysis on subgroup specific effects and interactions", "comment": null, "summary": "Commonly, clinical trials report effects not only for the full study population but also for patient subgroups. Meta-analyses of subgroup-specific effects and treatment-by-subgroup interactions may be inconsistent, especially when trials apply different subgroup weightings. We show that meta-regression can, in principle, with a contribution adjustment, recover the same interaction inference regardless of whether interaction data or subgroup data are used. Our Bayesian framework for subgroup-data interaction meta-analysis inherently (i) adjusts for varying relative subgroup contribution, quantified by the information fraction (IF) within a trial; (ii) is robust to prevalence imbalance and variation; (iii) provides a self-contained, model-based approach; and (iv) can be used to incorporate prior information into interaction meta-analyses with few studies.The method is demonstrated using an example with as few as seven trials of disease-modifying therapies in relapsing-remitting multiple sclerosis. The Bayesian Contribution-adjusted Meta-analysis by Subgroup (CAMS) indicates a stronger treatment-by-disability interaction (relapse rate reduction) in patients with lower disability (EDSS <= 3.5) compared with the unadjusted model, while results for younger patients (age < 40 years) are unchanged.By controlling subgroup contribution while retaining subgroup interpretability, this approach enables reliable interaction decision-making when published subgroup data are available.Although the proposed CAMS approach is presented in a Bayesian context, it can also be implemented in frequentist or likelihood frameworks."}
{"id": "2512.18315", "categories": ["stat.ME", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18315", "abs": "https://arxiv.org/abs/2512.18315", "authors": ["Isabela Belciug", "Simon Ferreira", "Charles K. Assaad"], "title": "On Efficient Adjustment for Micro Causal Effects in Summary Causal Graphs", "comment": null, "summary": "Observational studies in fields such as epidemiology often rely on covariate adjustment to estimate causal effects. Classical graphical criteria, like the back-door criterion and the generalized adjustment criterion, are powerful tools for identifying valid adjustment sets in directed acyclic graphs (DAGs). However, these criteria are not directly applicable to summary causal graphs (SCGs), which are abstractions of DAGs commonly used in dynamic systems. In SCGs, each node typically represents an entire time series and may involve cycles, making classical criteria inapplicable for identifying causal effects. Recent work established complete conditions for determining whether the micro causal effect of a treatment or an exposure $X_{t-γ}$ on an outcome $Y_t$ is identifiable via covariate adjustment in SCGs, under the assumption of no hidden confounding. However, these identifiability conditions have two main limitations. First, they are complex, relying on cumbersome definitions and requiring the enumeration of multiple paths in the SCG, which can be computationally expensive. Second, when these conditions are satisfied, they only provide two valid adjustment sets, limiting flexibility in practical applications. In this paper, we propose an equivalent but simpler formulation of those identifiability conditions and introduce a new criterion that identifies a broader class of valid adjustment sets in SCGs. Additionally, we characterize the quasi-optimal adjustment set among these, i.e., the one that minimizes the asymptotic variance of the causal effect estimator. Our contributions offer both theoretical advancement and practical tools for more flexible and efficient causal inference in abstracted causal graphs."}
{"id": "2512.18997", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18997", "abs": "https://arxiv.org/abs/2512.18997", "authors": ["Jianan Zhu", "Tianruo Zhang", "Diana Silver", "Ellicott Matthay", "Omar El-Shahawy", "Hyunseung Kang", "Siyu Heng"], "title": "A Universal Framework for Factorial Matched Observational Studies with General Treatment Types: Design, Analysis, and Applications", "comment": null, "summary": "Matching is one of the most widely used causal inference frameworks in observational studies. However, all the existing matching-based causal inference methods are designed for either a single treatment with general treatment types (e.g., binary, ordinal, or continuous) or factorial (multiple) treatments with binary treatments only. To our knowledge, no existing matching-based causal methods can handle factorial treatments with general treatment types. This critical gap substantially hinders the applicability of matching in many real-world problems, in which there are often multiple, potentially non-binary (e.g., continuous) treatment components. To address this critical gap, this work develops a universal framework for the design and analysis of factorial matched observational studies with general treatment types (e.g., binary, ordinal, or continuous). We first propose a two-stage non-bipartite matching algorithm that constructs matched sets of units with similar covariates but distinct combinations of treatment doses, thereby enabling valid estimation of both main and interaction effects. We then introduce a new class of generalized factorial Neyman-type estimands that provide model-free, finite-population-valid definitions of marginal and interaction causal effects under factorial treatments with general treatment types. Randomization-based Fisher-type and Neyman-type inference procedures are developed, including unbiased estimators, asymptotically valid variance estimators, and variance adjustments incorporating covariate information for improved efficiency. Finally, we illustrate the proposed framework through a county-level application that evaluates the causal impacts of work- and non-work-trip reductions (social distancing practices) on COVID-19-related and drug-related outcomes during the COVID-19 pandemic in the United States."}
{"id": "2512.18403", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18403", "abs": "https://arxiv.org/abs/2512.18403", "authors": ["Zijing Li", "Chenhao Zeng", "Shufei Ge"], "title": "Bayesian Brain Edge-Based Connectivity (BBeC): a Bayesian model for brain edge-based connectivity inference", "comment": null, "summary": "Brain connectivity analysis based on magnetic resonance imaging is crucial for understanding neurological mechanisms. However, edge-based connectivity inference faces significant challenges, particularly the curse of dimensionality when estimating high-dimensional covariance matrices. Existing methods often struggle to account for the unknown latent topological structure among brain edges, leading to inaccurate parameter estimation and unstable inference. To address these issues, this study proposes a Bayesian model based on a finite-dimensional Dirichlet distribution. Unlike non-parametric approaches, our method utilizes a finite-dimensional Dirichlet distribution to model the topological structure of brain networks, ensuring constant parameter dimensionality and improving algorithmic stability. We reformulate the covariance matrix structure to guarantee positive definiteness and employ a Metropolis-Hastings algorithm to simultaneously infer network topology and correlation parameters. Simulations validated the recovery of both network topology and correlation parameters. When applied to the Alzheimer's Disease Neuroimaging Initiative dataset, the model successfully identified structural subnetworks. The identified clusters were not only validated by composite anatomical metrics but also consistent with established findings in the literature, collectively demonstrating the model's reliability. The estimated covariance matrix also revealed that intragroup connection strength is stronger than intergroup connection strength. This study introduces a Bayesian framework for inferring brain network topology and high-dimensional covariance structures. The model configuration reduces parameter dimensionality while ensuring the positive definiteness of covariance matrices. As a result, it offers a reliable tool for investigating intrinsic brain connectivity in large-scale neuroimaging studies."}
{"id": "2512.19635", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.19635", "abs": "https://arxiv.org/abs/2512.19635", "authors": ["Lyza Iamrache", "Kamel Rekab", "Majid Bani-Yagoub", "Julia Pluta", "Abdelghani Mehailia"], "title": "A Markov Chain Modeling Approach for Predicting Relative Risks of Spatial Clusters in Public Health", "comment": null, "summary": "Predicting relative risk (RR) of spatial clusters is a complex task in public health that can be achieved through various statistical and machine-learning methods for different time intervals. However, high-resolution longitudinal data is often unavailable to successfully apply such methods. The goal of the present study is to further develop and test a new methodology proposed in our previous work for accurate sequential RR predictions in the case of limited lon gitudinal data. In particular, we first use a well-known likelihood ratio test to identify significant spatial clusters over user-defined time intervals. Then we apply a Markov chain modeling ap approach to predict RR values for each time interval. Our findings demonstrate that the proposed approach yields better performance with COVID-19 morbidity data compared to the previous study on mortality data. Additionally, increasing the number of time intervals enhances the accuracy of the proposed Markov chain modeling method."}
{"id": "2512.18479", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18479", "abs": "https://arxiv.org/abs/2512.18479", "authors": ["Rayleigh Lei", "Yajuan Si"], "title": "Calibrating hierarchical Bayesian domain inference for a proportion", "comment": null, "summary": "Small area estimation (SAE) improves estimates for local communities or groups, such as counties, neighborhoods, or demographic subgroups, when data are insufficient for each area. This is important for targeting local resources and policies, especially when national-level or large-area data mask variation at a more granular level. Researchers often fit hierarchical Bayesian models to stabilize SAE when data are sparse. Ideally, Bayesian procedures also exhibit good frequentist properties, as demonstrated by calibrated Bayes metrics. However, hierarchical Bayesian models tend to shrink domain estimates toward the overall mean and may produce credible intervals that do not maintain nominal coverage. Hoff et al. developed the Frequentist, but Assisted by Bayes (FAB) intervals for subgroup estimates with normally distributed outcomes. However, non-normally distributed data present new challenges, and multiple types of intervals have been proposed for estimating proportions. We examine domain inference with binary outcomes and extend FAB intervals to improve nominal coverage. We describe how to numerically compute FAB intervals for a proportion and evaluate their performance through repeated simulation studies. Leveraging multilevel regression and poststratification (MRP), we further refine SAE to correct for sample selection bias, construct the FAB intervals for MRP estimates and assess their repeated sampling properties. Finally, we apply the proposed inference methods to estimate COVID-19 infection rates across geographic and demographic subgroups. We find that the FAB intervals improve nominal coverage, at the cost of wider intervals."}
{"id": "2512.18492", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18492", "abs": "https://arxiv.org/abs/2512.18492", "authors": ["Annan Deng", "Carole Siegel", "Hyung G. Park"], "title": "A Bayesian likely responder approach for the analysis of randomized controlled trials", "comment": null, "summary": "An important goal of precision medicine is to personalize medical treatment by identifying individuals who are most likely to benefit from a specific treatment. The Likely Responder (LR) framework, which identifies a subpopulation where treatment response is expected to exceed a certain clinical threshold, plays a role in this effort. However, the LR framework, and more generally, data-driven subgroup analyses, often fail to account for uncertainty in the estimation of model-based data-driven subgrouping. We propose a simple two-stage approach that integrates subgroup identification with subsequent subgroup-specific inference on treatment effects. We incorporate model estimation uncertainty from the first stage into subgroup-specific treatment effect estimation in the second stage, by utilizing Bayesian posterior distributions from the first stage. We evaluate our method through simulations, demonstrating that the proposed Bayesian two-stage model produces better calibrated confidence intervals than naïve approaches. We apply our method to an international COVID-19 treatment trial, which shows substantial variation in treatment effects across data-driven subgroups."}
{"id": "2512.18508", "categories": ["stat.ME", "cs.AI", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.18508", "abs": "https://arxiv.org/abs/2512.18508", "authors": ["Barak Or"], "title": "The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics", "comment": "8 pages, preprint", "summary": "Validation gating is a fundamental component of classical Kalman-based tracking systems. Only measurements whose normalized innovation squared (NIS) falls below a prescribed threshold are considered for state update. While this procedure is statistically motivated by the chi-square distribution, it implicitly replaces the unconditional innovation process with a conditionally observed one, restricted to the validation event. This paper shows that innovation statistics computed after gating converge to gate-conditioned rather than nominal quantities. Under classical linear--Gaussian assumptions, we derive exact expressions for the first- and second-order moments of the innovation conditioned on ellipsoidal gating, and show that gating induces a deterministic, dimension-dependent contraction of the innovation covariance. The analysis is extended to NN association, which is shown to act as an additional statistical selection operator. We prove that selecting the minimum-norm innovation among multiple in-gate measurements introduces an unavoidable energy contraction, implying that nominal innovation statistics cannot be preserved under nontrivial gating and association. Closed-form results in the two-dimensional case quantify the combined effects and illustrate their practical significance."}
{"id": "2512.18584", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.18584", "abs": "https://arxiv.org/abs/2512.18584", "authors": ["Marios Papamichalis", "Regina Ruane", "Theofanis Papamichalis"], "title": "State-Space Modeling of Time-Varying Spillovers on Networks", "comment": "While under review, the material in this paper may be reorganized to meet journal requirements", "summary": "Many modern time series arise on networks, where each component is attached to a node and interactions follow observed edges. Classical time-varying parameter VARs (TVP-VARs) treat all series symmetrically and ignore this structure, while network autoregressive models exploit a given graph but usually impose constant parameters and stationarity. We develop network state-space models in which a low-dimensional latent state controls time-varying network spillovers, own-lag persistence and nodal covariate effects. A key special case is a network time-varying parameter VAR (NTVP-VAR) that constrains each lag matrix to be a linear combination of known network operators, such as a row-normalised adjacency and the identity, and lets the associated coefficients evolve stochastically in time. The framework nests Gaussian and Poisson network autoregressions, network ARIMA models with graph differencing, and dynamic edge models driven by multivariate logistic regression. We give conditions ensuring that NTVP-VARs are well-defined in second moments despite nonstationary states, describe network versions of stability and local stationarity, and discuss shrinkage, thresholding and low-rank tensor structures for high-dimensional graphs. Conceptually, network state-space models separate where interactions may occur (the graph) from how strong they are at each time (the latent state), providing an interpretable alternative to both unstructured TVP-VARs and existing network time-series models."}
{"id": "2512.18627", "categories": ["stat.ME", "econ.EM", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.18627", "abs": "https://arxiv.org/abs/2512.18627", "authors": ["Shunsuke Imai"], "title": "Accuracy of Uniform Inference on Fine Grid Points", "comment": null, "summary": "Uniform confidence bands for functions are widely used in empirical analysis. A variety of simple implementation methods (most notably multiplier bootstrap) have been proposed and theoretically justified. However, an implementation over a literally continuous index set is generally computationally infeasible, and practitioners therefore compute the critical value by evaluating the statistic on a finite evaluation grid. This paper quantifies how fine the evaluation grid must be for a multiplier bootstrap procedure over finite grid points to deliver valid uniform confidence bands. We derive an explicit bound on the resulting coverage error that separates discretization effects from the intrinsic high-dimensional bootstrap approximation error on the grid. The bound yields a transparent workflow for choosing the grid size in practice, and we illustrate the implementation through an example of kernel density estimation."}
{"id": "2512.18768", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18768", "abs": "https://arxiv.org/abs/2512.18768", "authors": ["Elling Svee", "Geir-Arne Fuglstad"], "title": "Non-stationary Spatial Modeling Using Fractional SPDEs", "comment": null, "summary": "We construct a Gaussian random field (GRF) that combines fractional smoothness with spatially varying anisotropy. The GRF is defined through a stochastic partial differential equation (SPDE), where the range, marginal variance, and anisotropy vary spatially according to a spectral parametrization of the SPDE coefficients. Priors are constructed to reduce overfitting in this flexible covariance model, and parameter estimation is done with an efficient gradient-based optimization approach that combines automatic differentiation with sparse matrix operations. In a simulation study, we investigate how many observations are required to reliably estimate fractional smoothness and non-stationarity, and find that one realization containing 500 observations or more is needed in the scenario considered. We also find that the proposed penalization prevents overfitting across varying numbers of observation locations. Two case studies demonstrate that the relative importance of fractional smoothness and non-stationarity is application dependent. Non-stationarity improves predictions in an application to ocean salinity, whereas fractional smoothness improves predictions in an application to precipitation. Predictive ability is assessed using mean squared error and the continuous ranked probability score. In addition to prediction, the proposed approach can be used as a tool to explore the presence of fractional smoothness and non-stationarity."}
{"id": "2512.18785", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18785", "abs": "https://arxiv.org/abs/2512.18785", "authors": ["Renato Panaro", "Christian Röver", "Tim Friede"], "title": "Consistent Bayesian meta-analysis on subgroup specific effects and interactions", "comment": null, "summary": "Commonly, clinical trials report effects not only for the full study population but also for patient subgroups. Meta-analyses of subgroup-specific effects and treatment-by-subgroup interactions may be inconsistent, especially when trials apply different subgroup weightings. We show that meta-regression can, in principle, with a contribution adjustment, recover the same interaction inference regardless of whether interaction data or subgroup data are used. Our Bayesian framework for subgroup-data interaction meta-analysis inherently (i) adjusts for varying relative subgroup contribution, quantified by the information fraction (IF) within a trial; (ii) is robust to prevalence imbalance and variation; (iii) provides a self-contained, model-based approach; and (iv) can be used to incorporate prior information into interaction meta-analyses with few studies.The method is demonstrated using an example with as few as seven trials of disease-modifying therapies in relapsing-remitting multiple sclerosis. The Bayesian Contribution-adjusted Meta-analysis by Subgroup (CAMS) indicates a stronger treatment-by-disability interaction (relapse rate reduction) in patients with lower disability (EDSS <= 3.5) compared with the unadjusted model, while results for younger patients (age < 40 years) are unchanged.By controlling subgroup contribution while retaining subgroup interpretability, this approach enables reliable interaction decision-making when published subgroup data are available.Although the proposed CAMS approach is presented in a Bayesian context, it can also be implemented in frequentist or likelihood frameworks."}
{"id": "2512.18860", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18860", "abs": "https://arxiv.org/abs/2512.18860", "authors": ["Merle Munko", "Simon Mack", "Marc Ditzhaus", "Stefan Fröhling", "Dennis Dobler", "Dominic Edelmann"], "title": "Effect measures for comparing paired event times", "comment": null, "summary": "The progression-free survival ratio (PFSr) is a widely used measure in personalized oncology trials. It evaluates the effectiveness of treatment by comparing two consecutive event times - one under standard therapy and one under an experimental treatment. However, most proposed tests based on the PFSr cannot control the nominal type I error rate, even under mild assumptions such as random right-censoring. Consequently the results of these tests are often unreliable.\n  As a remedy, we propose to estimate the relevant probabilities related to the PFSr by adapting recently developed methodology for the relative treatment effect between paired event times. As an additional alternative, we develop inference procedures based on differences and ratios of restricted mean survival times.\n  An extensive simulation study confirms that the proposed novel methodology provides reliable inference, whereas previously proposed techniques break down in many realistic settings. The utility of our methods is further illustrated through an analysis of real data from a molecularly aided tumor trial."}
{"id": "2512.18924", "categories": ["stat.ME", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.18924", "abs": "https://arxiv.org/abs/2512.18924", "authors": ["Jonquil Z. Liao", "Joshua Cape"], "title": "Testing for latent structure via the Wilcoxon--Wigner random matrix of normalized rank statistics", "comment": "17 pages main text, 2 figures, 3 tables, 43 pages supplementary material", "summary": "This paper considers the problem of testing for latent structure in large symmetric data matrices. The goal here is to develop statistically principled methodology that is flexible in its applicability, computationally efficient, and insensitive to extreme data variation, thereby overcoming limitations facing existing approaches. To do so, we introduce and systematically study certain symmetric matrices, called Wilcoxon--Wigner random matrices, whose entries are normalized rank statistics derived from an underlying independent and identically distributed sample of absolutely continuous random variables. These matrices naturally arise as the matricization of one-sample problems in statistics and conceptually lie at the interface of nonparametrics, multivariate analysis, and data reduction. Among our results, we establish that the leading eigenvalue and corresponding eigenvector of Wilcoxon--Wigner random matrices admit asymptotically Gaussian fluctuations with explicit centering and scaling terms. These asymptotic results enable rigorous parameter-free and distribution-free spectral methodology for addressing two hypothesis testing problems, namely community detection and principal submatrix detection. Numerical examples illustrate the performance of the proposed approach. Throughout, our findings are juxtaposed with existing results based on the spectral properties of independent entry symmetric random matrices in signal-plus-noise data settings."}
{"id": "2512.18946", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18946", "abs": "https://arxiv.org/abs/2512.18946", "authors": ["Yunhan Mou", "Scott Hummel", "Yuan Huang"], "title": "Integrating Prioritized and Non-Prioritized Structures in Win Statistics", "comment": null, "summary": "Composite endpoints are frequently used as primary or secondary analyses in cardiovascular clinical trials to increase clinical relevance and statistical efficiency. Alternatively, the Win Ratio (WR) and other Win Statistics (WS) analyses rely on a strict hierarchical ordering of endpoints, assigning higher priority to clinically important endpoints. However, determining a definitive endpoint hierarchy can be challenging and may not adequately reflect situations where endpoints have comparable importance. In this study, we discuss the challenges of endpoint prioritization, underscore its critical role in WS analyses, and propose Rotation WR (RWR), a hybrid prioritization framework that integrates both prioritized and non-prioritized structures. By permitting blocks of equally-prioritized endpoints, RWR accommodates endpoints of equal or near equal clinical importance, recurrent events, and contexts requiring individualized shared decision making. Statistical inference for RWR is developed using U-statistics theory, including the hypothesis testing procedure and confidence interval construction. Extensions to two additional WS measures, Rotation Net Benefit and Rotation Win Odds, are also provided. Through extensive simulation studies involving multiple time-to-event endpoints, including recurrent events, we demonstrate that RWR achieves valid type I error control, desirable statistical power, and accurate confidence interval coverage. We illustrate both the methodological and practical insights of our work in a case study on endpoint prioritization with the SPRINT clinical trial, highlighting its implications for real-world clinical trial studies."}
{"id": "2512.18997", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18997", "abs": "https://arxiv.org/abs/2512.18997", "authors": ["Jianan Zhu", "Tianruo Zhang", "Diana Silver", "Ellicott Matthay", "Omar El-Shahawy", "Hyunseung Kang", "Siyu Heng"], "title": "A Universal Framework for Factorial Matched Observational Studies with General Treatment Types: Design, Analysis, and Applications", "comment": null, "summary": "Matching is one of the most widely used causal inference frameworks in observational studies. However, all the existing matching-based causal inference methods are designed for either a single treatment with general treatment types (e.g., binary, ordinal, or continuous) or factorial (multiple) treatments with binary treatments only. To our knowledge, no existing matching-based causal methods can handle factorial treatments with general treatment types. This critical gap substantially hinders the applicability of matching in many real-world problems, in which there are often multiple, potentially non-binary (e.g., continuous) treatment components. To address this critical gap, this work develops a universal framework for the design and analysis of factorial matched observational studies with general treatment types (e.g., binary, ordinal, or continuous). We first propose a two-stage non-bipartite matching algorithm that constructs matched sets of units with similar covariates but distinct combinations of treatment doses, thereby enabling valid estimation of both main and interaction effects. We then introduce a new class of generalized factorial Neyman-type estimands that provide model-free, finite-population-valid definitions of marginal and interaction causal effects under factorial treatments with general treatment types. Randomization-based Fisher-type and Neyman-type inference procedures are developed, including unbiased estimators, asymptotically valid variance estimators, and variance adjustments incorporating covariate information for improved efficiency. Finally, we illustrate the proposed framework through a county-level application that evaluates the causal impacts of work- and non-work-trip reductions (social distancing practices) on COVID-19-related and drug-related outcomes during the COVID-19 pandemic in the United States."}
{"id": "2512.19187", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19187", "abs": "https://arxiv.org/abs/2512.19187", "authors": ["Saïd Maanan", "Azzouz Dermoune", "Ahmed El Ghini"], "title": "Smoothed Quantile Estimation: A Unified Framework Interpolating to the Mean", "comment": null, "summary": "This paper develops and analyzes three families of estimators that continuously interpolate between classical quantiles and the sample mean. The construction begins with a smoothed version of the $L_{1}$ loss, indexed by a location parameter $z$ and a smoothing parameter $h \\ge 0$, whose minimizer $\\hat q(z,h)$ yields a unified M-estimation framework. Depending on how $(z, h)$ is specified, this framework generates three distinct classes of estimators: fixed-parameter smoothed quantile estimators, plug-in estimators of fixed quantiles, and a new continuum of mean-estimating procedures. For all three families we establish consistency and asymptotic normality via a uniform asymptotic equicontinuity argument. The limiting variances admit closed forms, allowing a transparent comparison of efficiency across families and smoothing levels. A geometric decomposition of the parameter space shows that, for fixed quantile level $τ$, admissible pairs $(z, h)$ lie on straight lines along which the estimator targets the same population quantile while its asymptotic variance evolves. The theoretical analysis reveals two efficiency regimes. Under light-tailed distributions (e.g., Gaussian), smoothing yields a monotone variance reduction. Under heavy-tailed distributions (e.g., Laplace), a finite smoothing parameter $h^{*}(τ) > 0$ strictly improves efficiency for quantile estimation. Numerical experiments -- based on simulated data and real financial returns -- validate these conclusions and show that, both asymptotically and in finite samples, the mean-estimating family does not improve upon the sample mean."}
{"id": "2512.19273", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19273", "abs": "https://arxiv.org/abs/2512.19273", "authors": ["Xiaoyu Zhang", "Zhiyun Fan", "Wenyang Zhang", "Di Wang"], "title": "Scale-Invariant Robust Estimation of High-Dimensional Kronecker-Structured Matrices", "comment": "85 pages, 11 figues", "summary": "High-dimensional Kronecker-structured estimation faces a conflict between non-convex scaling ambiguities and statistical robustness. The arbitrary factor scaling distorts gradient magnitudes, rendering standard fixed-threshold robust methods ineffective. We resolve this via Scaled Robust Gradient Descent (SRGD), which stabilizes optimization by de-scaling gradients before truncation. To further enforce interpretability, we introduce Scaled Hard Thresholding (SHT) for invariant variable selection. A two-step estimation procedure, built upon robust initialization and SRGD--SHT iterative updates, is proposed for canonical matrix problems, such as trace regression, matrix GLMs, and bilinear models. The convergence rates are established for heavy-tailed predictors and noise, identifying a phase transition where optimal convergence rates recover under finite noise variance and degrade optimally for heavier tails. Experiments on simulated data and two real-world applications confirm superior robustness and efficiency of the proposed procedure."}
{"id": "2512.19325", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19325", "abs": "https://arxiv.org/abs/2512.19325", "authors": ["Xinyue Xu", "Huifang Ma", "Hongfei Wang", "Long Feng"], "title": "High dimensional matrix estimation through elliptical factor models", "comment": null, "summary": "Elliptical factor models play a central role in modern high-dimensional data analysis, particularly due to their ability to capture heavy-tailed and heterogeneous dependence structures. Within this framework, Tyler's M-estimator (Tyler, 1987a) enjoys several optimality properties and robustness advantages. In this paper, we develop high-dimensional scatter matrix, covariance matrix and precision matrix estimators grounded in Tyler's M-estimation. We first adapt the Principal Orthogonal complEment Thresholding (POET) framework (Fan et al., 2013) by incorporating the spatial-sign covariance matrix as an effective initial estimator. Building on this idea, we further propose a direct extension of POET tailored for Tyler's M-estimation, referred to as the POET-TME method. We establish the consistency rates for the resulting estimators under elliptical factor models. Comprehensive simulation studies and a real data application illustrate the superior performance of POET-TME, especially in the presence of heavy-tailed distributions, demonstrating the practical value of our methodological contributions."}
{"id": "2512.19398", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.19398", "abs": "https://arxiv.org/abs/2512.19398", "authors": ["Jiahua Jiang", "Joseph Marsh", "Rowland G Seymour"], "title": "A Reduced Basis Decomposition Approach to Efficient Data Collection in Pairwise Comparison Studies", "comment": "24 pages, 2 tables, 3 figures", "summary": "Comparative judgement studies elicit quality assessments through pairwise comparisons, typically analysed using the Bradley-Terry model. A challenge in these studies is experimental design, specifically, determining the optimal pairs to compare to maximize statistical efficiency. Constructing static experimental designs for these studies requires spectral decomposition of a covariance matrix over pairs of pairs, which becomes computationally infeasible for studies with more than approximately 150 objects. We propose a scalable method based on reduced basis decomposition that bypasses explicit construction of this matrix, achieving computational savings of two to three orders of magnitude. We establish eigenvalue bounds guaranteeing approximation quality and characterise the rank structure of the design matrix. Simulations demonstrate speedup factors exceeding 100 for studies with 64 or more objects, with negligible approximation error. We apply the method to construct designs for a 452-region spatial study in under 7 minutes and enable real-time design updates for classroom peer assessment, reducing computation time from 15 minutes to 15 seconds."}
{"id": "2512.19553", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19553", "abs": "https://arxiv.org/abs/2512.19553", "authors": ["Luke Benz", "Rajarshi Mukherjee", "Rui Wang", "David Arterburn", "Heidi Fischer", "Catherine Lee", "Susan M. Shortreed", "Alexander W. Levis", "Sebastien Haneuse"], "title": "A Statistical Framework for Understanding Causal Effects that Vary by Treatment Initiation Time in EHR-based Studies", "comment": null, "summary": "Comparative effectiveness studies using electronic health records (EHR) consider data from patients who could ``enter'' the study cohort at any point during an interval that spans many years in calendar time. Unlike treatments in tightly controlled trials, real-world treatments can evolve over calendar time, especially if comparators include standard of care, or procedures where techniques may improve. Efforts to assess whether treatment efficacy itself is changing are complicated by changing patient populations, with potential covariate shift in key effect modifiers. In this work, we propose a statistical framework to estimate calendar-time specific average treatment effects and describe both how and why effects vary across treatment initiation time in EHR-based studies. Our approach projects doubly robust, time-specific treatment effect estimates onto candidate marginal structural models and uses a model selection procedure to best describe how effects vary by treatment initiation time. We further introduce a novel summary metric, based on standardization analysis, to quantify the role of covariate shift in explaining observed effect changes and disentangle changes in treatment effects from changes in the patient population receiving treatment. Extensive simulations using EHR data from Kaiser Permanente are used to validate the utility of the framework, which we apply to study changes in relative weight loss following two bariatric surgical interventions versus no surgery among patients with severe obesity between 2005-2011."}
{"id": "2512.19588", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19588", "abs": "https://arxiv.org/abs/2512.19588", "authors": ["Yaohui Lin"], "title": "Possibilistic Inferential Models for Post-Selection Inference in High-Dimensional Linear Regression", "comment": null, "summary": "Valid uncertainty quantification after model selection remains challenging in high-dimensional linear regression, especially within the possibilistic inferential model (PIM) framework. We develop possibilistic inferential models for post-selection inference based on a regularized split possibilistic construction (RSPIM) that combines generic high-dimensional selectors with PIM validification through sample splitting. A first subsample is used to select a sparse model; ordinary least-squares refits on an independent inference subsample yield classical t/F pivots, which are then turned into consonant plausibility contours. In Gaussian linear models this leads to coor-dinatewise intervals with exact finite-sample strong validity conditional on the split and selected model, uniformly over all selectors that use only the selection data. We further analyze RSPIM in a sparse p >> n regime under high-level screening conditions, develop orthogonalized and bootstrap-based extensions for low-dimensional targets with high-dimensional nuisance, and study a maxitive multi-split aggregation that stabilizes inference across random splits while preserving strong validity. Simulations and a riboflavin gene-expression example show that calibrated RSPIM intervals are well behaved under both Gaussian and heteroskedastic errors and are competitive with state-of-the-art post-selection methods, while plausibility contours provide transparent diagnostics of post-selection uncertainty."}
{"id": "2512.19635", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.19635", "abs": "https://arxiv.org/abs/2512.19635", "authors": ["Lyza Iamrache", "Kamel Rekab", "Majid Bani-Yagoub", "Julia Pluta", "Abdelghani Mehailia"], "title": "A Markov Chain Modeling Approach for Predicting Relative Risks of Spatial Clusters in Public Health", "comment": null, "summary": "Predicting relative risk (RR) of spatial clusters is a complex task in public health that can be achieved through various statistical and machine-learning methods for different time intervals. However, high-resolution longitudinal data is often unavailable to successfully apply such methods. The goal of the present study is to further develop and test a new methodology proposed in our previous work for accurate sequential RR predictions in the case of limited lon gitudinal data. In particular, we first use a well-known likelihood ratio test to identify significant spatial clusters over user-defined time intervals. Then we apply a Markov chain modeling ap approach to predict RR values for each time interval. Our findings demonstrate that the proposed approach yields better performance with COVID-19 morbidity data compared to the previous study on mortality data. Additionally, increasing the number of time intervals enhances the accuracy of the proposed Markov chain modeling method."}
{"id": "2512.19641", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19641", "abs": "https://arxiv.org/abs/2512.19641", "authors": ["John H. J. Einmahl", "Denis Kojevnikov", "Bas J. M. Werker"], "title": "Testing for Conditional Independence in Binary Single-Index Models", "comment": null, "summary": "We wish to test whether a real-valued variable $Z$ has explanatory power, in addition to a multivariate variable $X$, for a binary variable $Y$. Thus, we are interested in testing the hypothesis $\\mathbb{P}(Y=1\\, | \\, X,Z)=\\mathbb{P}(Y=1\\, | \\, X)$, based on $n$ i.i.d.\\ copies of $(X,Y,Z)$. In order to avoid the curse of dimensionality, we follow the common approach of assuming that the dependence of both $Y$ and $Z$ on $X$ is through a single-index $X^\\topβ$ only. Splitting the sample on both $Y$-values, we construct a two-sample empirical process of transformed $Z$-variables, after splitting the $X$-space into parallel strips. Studying this two-sample empirical process is challenging: it does not converge weakly to a standard Brownian bridge, but after an appropriate normalization it does. We use this result to construct distribution-free tests."}
{"id": "2512.18118", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.18118", "abs": "https://arxiv.org/abs/2512.18118", "authors": ["Matteo Sesia", "Vladimir Svetnik"], "title": "Distribution-Free Selection of Low-Risk Oncology Patients for Survival Beyond a Time Horizon", "comment": null, "summary": "We study the problem of selecting a subset of patients who are unlikely to experience an event within a specified time horizon, by calibrating a screening rule based on the output of a black-box survival model. This statistics problem has many applications in medicine, including identifying candidates for treatment de-escalation and prioritizing the allocation of limited medical resources. In this paper, we compare two families of methods that can provide different types of distribution-free guarantees for this task: (i) high-probability risk control and (ii) expectation-based false discovery rate control using conformal $p$-values. We clarify the relation between these two frameworks, which have important conceptual differences, and explain how each can be adapted to analyze time-to-event data using inverse probability of censoring weighting. Through experiments on semi-synthetic and real oncology data from the Flatiron Health Research Database, we find that both approaches often achieve the desired survival rate among selected patients, but with distinct efficiency profiles. The conformal method tends to be more powerful, whereas high-probability risk control offers stronger guarantees at the cost of some additional conservativeness. Finally, we provide practical guidance on implementation and parameter tuning."}
{"id": "2512.19035", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19035", "abs": "https://arxiv.org/abs/2512.19035", "authors": ["Michael R. Schwob", "Nicholas M. Calzada", "Justin J. Van Ee", "Diana Gamba", "Rebecca A. Nelson", "Megan L. Vahsen", "Peter B. Adler", "Jesse R. Lasky", "Mevin B. Hooten"], "title": "Dyadic Flow Models for Nonstationary Gene Flow in Landscape Genomics", "comment": "43 pages, 7 figures, 5 appendices", "summary": "The field of landscape genomics aims to infer how landscape features affect gene flow across space. Most landscape genomic frameworks assume the isolation-by-distance and isolation-by-resistance hypotheses, which propose that genetic dissimilarity increases as a function of distance and as a function of cumulative landscape resistance, respectively. While these hypotheses are valid in certain settings, other mechanisms may affect gene flow. For example, the gene flow of invasive species may depend on founder effects and multiple introductions. Such mechanisms are not considered in modern landscape genomic models. We extend dyadic models to allow for mechanisms that range-shifting and/or invasive species may experience by introducing dyadic spatially-varying coefficients (DSVCs) defined on source-destination pairs. The DSVCs allow the effects of landscape on gene flow to vary across space, capturing nonstationary and asymmetric connectivity. Additionally, we incorporate explicit landscape features as connectivity covariates, which are localized to specific regions of the spatial domain and may function as barriers or corridors to gene flow. Such covariates are central to colonization and invasion, where spread accelerates along corridors and slows across landscape barriers. The proposed framework accommodates colonization-specific processes while retaining the ability to assess landscape influences on gene flow. Our case study of the highly invasive cheatgrass (Bromus tectorum) demonstrates the necessity of accounting for nonstationarity gene flow in range-shifting species."}
