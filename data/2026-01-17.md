<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 4]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.ME](#stat.ME) [Total: 13]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Estimation of Parameters of the Truncated Normal Distribution with Unknown Bounds](https://arxiv.org/abs/2601.09857)
*Dylan Borchert,Semhar Michael,Christopher Saunders*

Main category: stat.CO

TL;DR: 提出一种基于期望-解(ES)框架的新算法，用于同时估计截断正态分布的截断边界和位置尺度参数，该算法结合了最佳线性无偏估计和无偏最小方差估计理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注已知截断区域的参数估计，或已知父分布时估计未知边界，但缺乏同时估计截断边界和正态分布参数的统一方法。

Method: 基于期望-解(ES)框架的迭代算法，结合位置尺度分布族的最佳线性无偏估计理论和截断区域的无偏最小方差估计，同时估计截断边界和正态分布参数。

Result: 讨论了算法在固定样本量下收敛到估计方程解的条件，利用经验过程理论中的M-和Z-估计结果分析了估计量的渐近性质，并通过蒙特卡洛模拟与已知截断边界方法进行比较。

Conclusion: 提出的ES框架算法能够有效同时估计截断正态分布的截断边界和位置尺度参数，为未知截断区域的参数估计问题提供了系统解决方案。

Abstract: Estimators of parameters of truncated distributions, namely the truncated normal distribution, have been widely studied for a known truncation region. There is also literature for estimating the unknown bounds for known parent distributions. In this work, we develop a novel algorithm under the expectation-solution (ES) framework, which is an iterative method of solving nonlinear estimating equations, to estimate both the bounds and the location and scale parameters of the parent normal distribution utilizing the theory of best linear unbiased estimates from location-scale families of distribution and unbiased minimum variance estimation of truncation regions. The conditions for the algorithm to converge to the solution of the estimating equations for a fixed sample size are discussed, and the asymptotic properties of the estimators are characterized using results on M- and Z-estimation from empirical process theory. The proposed method is then compared to methods utilizing the known truncation bounds via Monte Carlo simulation.

</details>


### [2] [Mesh Denoising](https://arxiv.org/abs/2601.10487)
*Constantin Vaillant Tenzer*

Main category: stat.CO

TL;DR: 比较四种网格去噪方法：线性滤波、热扩散、Sobolev正则化和Sinkhorn重心法，发现Sobolev最快但效果稍差，热扩散在小网格上参数合适时可提升质量，所有方法在大网格上表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究不同网格去噪方法的性能比较，评估它们在处理不同规模网格时的效果差异，为实际应用提供方法选择指导。

Method: 比较了四种网格去噪方法：线性滤波、热扩散方法、Sobolev正则化以及基于Sinkhorn算法的重心方法。通过实验评估这些方法在简单图像去噪任务中的表现。

Result: 1. 朴素Gibbs核选择会导致不理想结果；2. Sobolev正则化最快但去噪效果略差于最佳迭代滤波或热扩散；3. 热扩散方法在大网格上较慢且效果不优于滤波，但在小网格上适当参数可提升质量；4. 所有三种网格方法在大网格上表现明显优于小网格。

Conclusion: 网格去噪方法的选择应考虑网格规模：Sobolev正则化适合速度优先场景，热扩散在小网格上参数调优后可获得更好质量，所有方法在大网格上表现更佳，实际应用需权衡速度与质量。

Abstract: In this paper, we study four mesh denoising methods: linear filtering, a heat diffusion method, Sobolev regularization, and, to a lesser extent, a barycentric approach based on the Sinkhorn algorithm. We illustrate that, for a simple image denoising task, a naive choice of a Gibbs kernel can lead to unsatisfactory results. We demonstrate that while Sobolev regularization is the fastest method in our implementation, it produces slightly less faithful denoised meshes than the best results obtained with iterative filtering or heat diffusion. We empirically show that, for the large mesh considered, the heat diffusion method is slower and not more effective than filtering, whereas on a small mesh an appropriate choice of diffusion parameters can improve the quality. Finally, we observe that all three mesh-based methods perform markedly better on the large mesh than on the small one.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [3] [Forecasting Seasonal Peaks of Pediatric Respiratory Infections Using an Alert-Based Model Combining SIR Dynamics and Historical Trends in Santiago, Chile](https://arxiv.org/abs/2601.09821)
*Gloria Henríquez,Jhoan Báez,Víctor Riquelme,Pedro Gajardo,Michel Royer,Héctor Ramírez*

Main category: stat.AP

TL;DR: 开发基于警报的预测模型，通过季节性SIR模型结合历史移动预测因子，提前预测儿童急性呼吸道感染住院高峰的时间和规模，为医院规划提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 智利儿童急性呼吸道感染（ARI）是儿科住院的主要原因，冬季需求激增给医院规划带来挑战。需要提前预测住院高峰的时间和规模，以帮助医院做好应对准备。

Method: 整合季节性SIR模型与历史移动预测因子，采用基于导数的警报系统检测早期疫情增长。使用15天移动平均和Savitzky-Golay滤波平滑每日住院数据，通过惩罚损失函数估计参数以减少噪声敏感性。

Result: 回顾性评估和2023-2024年圣地亚哥主要儿科医院的实地实施显示：高峰日期可提前约一个月预测，两周前预测准确度高；高峰规模约十天前开始提供信息，一周前稳定。模型为医院准备提供实用可解释工具。

Conclusion: 该基于警报的预测模型能够有效预测儿童急性呼吸道感染住院高峰的时间和规模，为医院应对冬季需求激增提供了实用的规划工具，有助于改善医疗资源分配和应急准备。

Abstract: Acute respiratory infections (ARI) are a major cause of pediatric hospitalization in Chile, producing marked winter increases in demand that challenge hospital planning. This study presents an alert-based forecasting model to predict the timing and magnitude of ARI hospitalization peaks in Santiago. The approach integrates a seasonal SIR model with a historical mobile predictor, activated by a derivative-based alert system that detects early epidemic growth. Daily hospitalization data from DEIS were smoothed using a 15-day moving average and Savitzky-Golay filtering, and parameters were estimated using a penalized loss function to reduce sensitivity to noise. Retrospective evaluation and real-world implementation in major Santiago pediatric hospitals during 2023 and 2024 show that peak date can be anticipated about one month before the event and predicted with high accuracy two weeks in advance. Peak magnitude becomes informative roughly ten days before the peak and stabilizes one week prior. The model provides a practical and interpretable tool for hospital preparedness.

</details>


### [4] [The Knowable Future: Mapping the Decay of Past-Future Mutual Information Across Forecast Horizons](https://arxiv.org/abs/2601.10006)
*Peter Maurice Catt*

Main category: stat.AP

TL;DR: 该研究提出使用滞后h的自互信息（AMI）作为时间序列可预测性的度量，通过k近邻估计器从训练数据中估计AMI，并在M4数据集的1,350个时间序列上验证其与样本外预测误差（sMAPE）的相关性。


<details>
  <summary>Details</summary>
Motivation: 在预测实践中，能够事前评估时间序列是否可能被准确预测非常重要，因为这决定了需要投入多少建模努力。当前缺乏一种仅基于训练数据就能评估序列可预测性的方法。

Method: 将可预测性定义为时间序列的属性，使用滞后h的自互信息（AMI）作为特定预测时域可预测性的度量。通过k近邻估计器从训练数据中估计AMI，并在M4数据集的1,350个时间序列（6种采样频率）上评估AMI与样本外预测误差（sMAPE）的相关性。使用Seasonal Naive、ETS和N-BEATS作为样本外预测性能的探针。

Result: 对于小时、周、季度和年度序列，AMI与sMAPE呈现一致的负秩相关，表明训练数据的AMI可以有效诊断预测难度。在N-BEATS下，小时序列（ρ=-0.52）和周序列（ρ=-0.51）相关性最强，季度（ρ=-0.42）和年度（ρ=-0.36）也显著。月度序列相关性依赖于探针方法，日序列的AMI-sMAPE相关性较弱。

Conclusion: 研究支持基于可测量的信号内容在预测前进行频率内分类和努力分配，而不是进行频率间的难度比较。训练数据的AMI可以作为频率条件性的预测难度诊断工具，有助于在实际预测前评估序列的可预测性。

Abstract: The ability to assess ex-ante whether a time series is likely to be accurately forecast is important for forecasting practice because it informs the degree of modelling effort warranted. We define forecastability as a property of a time series (given a declared information set), and measure horizon-specific forecastability as the reduction in uncertainty provided by the past, using auto-mutual information (AMI) at lag h. AMI is estimated from training data using a k-nearest-neighbour estimator and evaluated against out-of-sample forecast error (sMAPE) on a filtered, balanced sample of 1,350 M4 series across six sampling frequencies. Seasonal Naive, ETS, and N-BEATS are used as probes of out-of-sample forecast performance. Training-only AMI provides a frequency-conditional diagnostic for forecast difficulty: for Hourly, Weekly, Quarterly, and Yearly series, AMI exhibits consistently negative rank correlation with sMAPE across probes. Under N-BEATS, the correlation is strongest for Hourly (p= -0.52) and Weekly (p= -0.51), with Quarterly (p= -0.42) and Yearly (p = -0.36) also substantial. Monthly is probe-dependent (Seasonal Naive p= -0.12; ETS p = -0.26; N-BEATS p = -0.24). Daily shows notably weaker AMI-sMAPE correlation under this protocol, suggesting limited ability to discriminate between series despite the presence of temporal dependence. The findings support within-frequency triage and effort allocation based on measurable signal content prior to forecasting, rather than between-frequency comparisons of difficulty.

</details>


### [5] [Modeling mental health trajectories during the COVID-19 pandemic using UK-wide data in the presence of sociodemographic variables](https://arxiv.org/abs/2601.10445)
*Glenna Nightingale,Karthik Mohan,Eloi Ribe,Valentin Popov,Shakes Wang,Clara Calia,Luciana Brondi,Sohan Seth*

Main category: stat.AP

TL;DR: 该研究利用英国理解社会COVID-19研究数据，通过广义加性模型分析疫情期间心理健康轨迹的社会人口学决定因素。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行对人群心理健康和福祉的负面影响是重要的公共卫生问题，需要了解疫情期间心理健康轨迹的潜在影响因素。

Method: 使用理解社会COVID-19研究数据，以GHQ36评分为结果变量，采用广义加性模型分析时间趋势和社会人口学变量（年龄、性别、种族、居住国家、就业状况、家庭收入、伴侣状况、有无16岁以下子女、长期疾病）对心理健康变化的影响。

Result: 心理健康在年龄、性别、种族、居住国家、就业状况、家庭收入、伴侣状况、有无16岁以下子女、长期疾病等方面存在显著差异。女性GHQ36评分比男性高1.260分；无伴侣者比有伴侣者高1.050分；16-64岁各年龄段比65岁以上者评分更高；低收入家庭心理健康状况更差。

Conclusion: 研究确定了英国COVID-19大流行期间影响心理健康轨迹的关键人口学决定因素。减少心理健康不平等的政策应针对女性、年轻人、无伴侣者、有16岁以下子女者、长期疾病患者和低收入家庭。

Abstract: Background: The negative effects of the COVID-19 pandemic on the mental health and well-being of populations are an important public health issue. Our study aims to determine the underlying factors shaping mental health trajectories during the COVID-19 pandemic in the UK. Methods: Data from the Understanding Society COVID-19 Study were utilized and the core analysis focussed on GHQ36 scores as the outcome variable. We used GAMs to evaluate trends over time and the role of sociodemographic variables, i.e., age, sex, ethnicity, country of residence (in UK), job status (employment), household income, living with a partner, living with children under age 16, and living with a long-term illness, on the variation of mental health during the study period. Results: Statistically significant differences in mental health were observed for age, sex,ethnicity, country of residence (in UK), job status (employment), household income, living with a partner, living with children under age 16, and living with a long-term illness. Women experienced higher GHQ36 scores relative to men with the GHQ36 score expected to increase by 1.260 (95%CI: 1.176, 1.345). Individuals living without a partner were expected to have higher GHQ36 scores, of 1.050 (95%CI: 0.949, 1.148) more than those living with a partner, and age groups 16-34, 35-44, 45-54, 55-64 experienced higher GHQ36 scores relative to those who were 65+. Individuals with relatively lower household income were likely to have poorer mental health relative to those who were more well off. Conclusion: This study identifies key demographic determinants shaping mental health trajectories during the COVID-19 pandemic in the UK. Policies aiming to reduce mental health inequalities should target women, youth, individuals living without a partner, individuals living with children under 16, individuals with a long-term illness, and lower income families.

</details>


### [6] [MitoFREQ: A Novel Approach for Mitogenome Frequency Estimation from Top-level Haplogroups and Single Nucleotide Variants](https://arxiv.org/abs/2601.10464)
*Mikkel Meyer Andersen,Nicole Huber,Kimberly S Andreaggi,Tóra Oluffa Stenberg Olsen,Walther Parson,Charla Marshall*

Main category: stat.AP

TL;DR: 开发了MitoFREQ方法，利用HelixMTdb和gnomAD数据库估算线粒体基因组频率，通过顶级单倍群分类和稀有SNV频率加权，为法医DNA证据提供统计评估


<details>
  <summary>Details</summary>
Motivation: 高质量全线粒体DNA基因组序列（mitogenomes）的群体数据有限，需要一种方法来估算mitogenomes的群体频率，以增强法医mtDNA证据的统计评估能力

Method: 提出MitoFREQ方法：1) 将给定mitogenome分类到30个"顶级单倍群"（TLHG）中；2) 使用该TLHG中最稀有单核苷酸变异（SNV）的频率，并加权TLHG频率；3) 仅需227个特定位置即可完成顶级单倍群分类，适用于低质量样本

Result: 方法在高质量法医参考数据集和GenBank多样化mitogenomes集合上测试，证明对两种数据都稳健。产生似然比在100-100,000范围内，显著增强法医mtDNA证据统计评估能力

Conclusion: MitoFREQ方法为估算mitogenomes群体频率提供了有效工具，开发了开源R包`mitofreq`（含Shiny应用），可加强法医mtDNA证据的统计评估

Abstract: Lineage marker population frequencies can serve as one way to express evidential value in forensic genetics. However, for high-quality whole mitochondrial DNA genome sequences (mitogenomes), population data remain limited. In this paper, we offer a new method, MitoFREQ, for estimating the population frequencies of mitogenomes. MitoFREQ uses the mitogenome resources HelixMTdb and gnomAD, harbouring information from 195,983 and 56,406 mitogenomes, respectively. Neither HelixMTdb nor gnomAD can be queried directly for individual mitogenome frequencies, but offers single nucleotide variant (SNV) allele frequencies for each of 30 "top-level" haplogroups (TLHG). We propose using the HelixMTdb and gnomAD resources by classifying a given mitogenome within the TLHG scheme and subsequently using the frequency of its rarest SNV within that TLHG weighted by the TLHG frequency. We show that this method is guaranteed to provide a higher population frequency estimate than if a refined haplogroup and its SNV frequencies were used. Further, we show that top-level haplogrouping can be achieved by using only 227 specific positions for 99.9% of the tested mitogenomes, potentially making the method available for low-quality samples. The method was tested on two types of datasets: high-quality forensic reference datasets and a diverse collection of scrutinised mitogenomes from GenBank. This dual evaluation demonstrated that the approach is robust across both curated forensic data and broader population-level sequences. This method produced likelihood ratios in the range of 100-100,000, demonstrating its potential to strengthen the statistical evaluation of forensic mtDNA evidence. We have developed an open-source R package `mitofreq` that implements our method, including a Shiny app where custom TLHG frequencies can be supplied.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [7] [Accelerated Regularized Wasserstein Proximal Sampling Algorithms](https://arxiv.org/abs/2601.09848)
*Hong Ye Tan,Stanley Osher,Wuchen Li*

Main category: stat.ML

TL;DR: 提出ARWP方法，通过二阶得分ODE加速粒子采样，相比传统方法在混合速率和尾部探索方面表现更好


<details>
  <summary>Details</summary>
Motivation: 传统基于布朗运动的采样方法效率有限，需要开发更快的采样算法来加速从Gibbs分布的采样过程

Method: 使用二阶得分ODE（类似Nesterov加速）替代布朗运动，结合正则化Wasserstein近端方法，提出ARWP算法

Result: ARWP在渐近时间区域比动能Langevin采样有更高的收缩率，在多模态高斯混合和病态Rosenbrock分布等实验中表现优异

Conclusion: ARWP方法在采样效率、粒子收敛性和尾部探索方面优于传统方法，对非对数凹贝叶斯神经网络任务也有更好的泛化性能

Abstract: We consider sampling from a Gibbs distribution by evolving a finite number of particles using a particular score estimator rather than Brownian motion. To accelerate the particles, we consider a second-order score-based ODE, similar to Nesterov acceleration. In contrast to traditional kernel density score estimation, we use the recently proposed regularized Wasserstein proximal method, yielding the Accelerated Regularized Wasserstein Proximal method (ARWP). We provide a detailed analysis of continuous- and discrete-time non-asymptotic and asymptotic mixing rates for Gaussian initial and target distributions, using techniques from Euclidean acceleration and accelerated information gradients. Compared with the kinetic Langevin sampling algorithm, the proposed algorithm exhibits a higher contraction rate in the asymptotic time regime. Numerical experiments are conducted across various low-dimensional experiments, including multi-modal Gaussian mixtures and ill-conditioned Rosenbrock distributions. ARWP exhibits structured and convergent particles, accelerated discrete-time mixing, and faster tail exploration than the non-accelerated regularized Wasserstein proximal method and kinetic Langevin methods. Additionally, ARWP particles exhibit better generalization properties for some non-log-concave Bayesian neural network tasks.

</details>


### [8] [CROCS: A Two-Stage Clustering Framework for Behaviour-Centric Consumer Segmentation with Smart Meter Data](https://arxiv.org/abs/2601.10494)
*Luke W. Yerbury,Ricardo J. G. B. Campello,G. C. Livingston,Mark Goldsworthy,Lachlan O'Neil*

Main category: stat.ML

TL;DR: 提出CROCS两阶段聚类框架，通过代表性负荷集和加权最小距离和度量，解决传统方法在消费者行为多样性、时间对齐、异常数据和大规模部署方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源集成和电气化推进，需求侧管理（特别是需求响应）成为平衡电力系统的关键。智能电表数据使基于实际使用行为的消费者细分成为可能，但现有聚类方法无法充分反映消费者行为多样性，存在时间对齐僵化、对异常和缺失数据敏感、难以大规模部署等问题。

Method: 提出CROCS两阶段聚类框架：第一阶段对每个消费者的日负荷曲线独立聚类，形成代表性负荷集（RLS），压缩总结典型日消费行为；第二阶段使用加权最小距离和（WSMD）这一新颖的集合间度量来比较RLS，考虑行为的普遍性和相似性；最后在WSMD诱导图上进行社区检测，揭示定义消费者群体的共享日行为高阶原型。

Result: 在合成和澳大利亚真实智能电表数据集上的广泛实验表明，CROCS能够捕捉消费者内部变异性，发现同步和异步行为相似性，对异常和缺失数据保持鲁棒性，并通过自然并行化实现高效扩展。

Conclusion: CROCS框架有效解决了现有消费者细分方法的局限性，为设计更有效的需求侧管理和需求响应项目提供了更好的行为洞察，具有实际应用价值。

Abstract: With grid operators confronting rising uncertainty from renewable integration and a broader push toward electrification, Demand-Side Management (DSM) -- particularly Demand Response (DR) -- has attracted significant attention as a cost-effective mechanism for balancing modern electricity systems. Unprecedented volumes of consumption data from a continuing global deployment of smart meters enable consumer segmentation based on real usage behaviours, promising to inform the design of more effective DSM and DR programs. However, existing clustering-based segmentation methods insufficiently reflect the behavioural diversity of consumers, often relying on rigid temporal alignment, and faltering in the presence of anomalies, missing data, or large-scale deployments.
  To address these challenges, we propose a novel two-stage clustering framework -- Clustered Representations Optimising Consumer Segmentation (CROCS). In the first stage, each consumer's daily load profiles are clustered independently to form a Representative Load Set (RLS), providing a compact summary of their typical diurnal consumption behaviours. In the second stage, consumers are clustered using the Weighted Sum of Minimum Distances (WSMD), a novel set-to-set measure that compares RLSs by accounting for both the prevalence and similarity of those behaviours. Finally, community detection on the WSMD-induced graph reveals higher-order prototypes that embody the shared diurnal behaviours defining consumer groups, enhancing the interpretability of the resulting clusters.
  Extensive experiments on both synthetic and real Australian smart meter datasets demonstrate that CROCS captures intra-consumer variability, uncovers both synchronous and asynchronous behavioural similarities, and remains robust to anomalies and missing data, while scaling efficiently through natural parallelisation. These results...

</details>


### [9] [Coarsening Causal DAG Models](https://arxiv.org/abs/2601.10531)
*Francisco Madaleno,Pratik Misra,Alex Markham*

Main category: stat.ML

TL;DR: 提出新的因果抽象学习框架，包含图可识别性理论、高效算法和搜索空间结构分析，用于从干预数据直接学习抽象因果图


<details>
  <summary>Details</summary>
Motivation: DAG模型能有效表示因果关系，但在实际应用中，以给定特征的粒度估计因果模型并不总是可行或理想，需要因果抽象方法来处理这类问题

Method: 提供新的图可识别性结果，提出高效且可证明一致的算法直接从干预数据学习抽象因果图（干预目标未知），揭示搜索空间的格结构理论洞察

Result: 在合成和真实数据集（包括已知地面真实值的受控物理系统测量数据）上验证了算法的有效性

Conclusion: 该研究为因果抽象学习提供了理论框架和实用算法，揭示了搜索空间的结构特性，对因果发现领域有更广泛的启示

Abstract: Directed acyclic graphical (DAG) models are a powerful tool for representing causal relationships among jointly distributed random variables, especially concerning data from across different experimental settings. However, it is not always practical or desirable to estimate a causal model at the granularity of given features in a particular dataset. There is a growing body of research on causal abstraction to address such problems. We contribute to this line of research by (i) providing novel graphical identifiability results for practically-relevant interventional settings, (ii) proposing an efficient, provably consistent algorithm for directly learning abstract causal graphs from interventional data with unknown intervention targets, and (iii) uncovering theoretical insights about the lattice structure of the underlying search space, with connections to the field of causal discovery more generally. As proof of concept, we apply our algorithm on synthetic and real datasets with known ground truths, including measurements from a controlled physical system with interacting light intensity and polarization.

</details>


### [10] [Parametric RDT approach to computational gap of symmetric binary perceptron](https://arxiv.org/abs/2601.10628)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 使用完全提升随机对偶理论分析对称二元感知机的统计-计算间隙，发现二阶提升对应可满足性阈值，高阶提升对应算法阈值，存在约0.2的间隙。


<details>
  <summary>Details</summary>
Motivation: 研究对称二元感知机中是否存在统计-计算间隙，即理论可满足性阈值与算法可实现阈值之间的差异。

Method: 采用完全提升随机对偶理论，通过分析c序列的结构变化，在不同提升层次上估计可满足性阈值和算法阈值。

Result: 对于标准对称二元感知机，二阶提升得到可满足性阈值α_c≈1.8159，七阶提升得到算法阈值α_a≈1.6021（趋向于约1.59），存在约0.2的计算间隙。

Conclusion: 对称二元感知机中存在显著的统计-计算间隙，fl-RDT方法能有效预测阈值，结果与局部熵、重叠间隙性质等文献预测一致。

Abstract: We study potential presence of statistical-computational gaps (SCG) in symmetric binary perceptrons (SBP) via a parametric utilization of \emph{fully lifted random duality theory} (fl-RDT) [96]. A structural change from decreasingly to arbitrarily ordered $c$-sequence (a key fl-RDT parametric component) is observed on the second lifting level and associated with \emph{satisfiability} ($α_c$) -- \emph{algorithmic} ($α_a$) constraints density threshold change thereby suggesting a potential existence of a nonzero computational gap $SCG=α_c-α_a$. The second level estimate is shown to match the theoretical $α_c$ whereas the $r\rightarrow \infty$ level one is proposed to correspond to $α_a$. For example, for the canonical SBP ($κ=1$ margin) we obtain $α_c\approx 1.8159$ on the second and $α_a\approx 1.6021$ (with converging tendency towards $\sim 1.59$ range) on the seventh level. Our propositions remarkably well concur with recent literature: (i) in [20] local entropy replica approach predicts $α_{LE}\approx 1.58$ as the onset of clustering defragmentation (presumed driving force behind locally improving algorithms failures); (ii) in $α\rightarrow 0$ regime we obtain on the third lifting level $κ\approx 1.2385\sqrt{\frac{α_a}{-\log\left ( α_a \right ) }}$ which qualitatively matches overlap gap property (OGP) based predictions of [43] and identically matches local entropy based predictions of [24]; (iii) $c$-sequence ordering change phenomenology mirrors the one observed in asymmetric binary perceptron (ABP) in [98] and the negative Hopfield model in [100]; and (iv) as in [98,100], we here design a CLuP based algorithm whose practical performance closely matches proposed theoretical predictions.

</details>


### [11] [Classification Imbalance as Transfer Learning](https://arxiv.org/abs/2601.10630)
*Eric Xia,Jason M. Klusowski*

Main category: stat.ML

TL;DR: 该研究将类别不平衡问题视为标签偏移下的迁移学习，分析了过采样方法（如SMOTE）的理论性能，发现自助法在中等高维下通常优于SMOTE。


<details>
  <summary>Details</summary>
Motivation: 解决分类任务中的类别不平衡问题，现有过采样方法（如SMOTE）缺乏理论分析，需要从迁移学习角度理解不同过采样策略的性能差异。

Method: 将不平衡分类框架化为标签偏移下的迁移学习问题，分析过采样方法的超额风险分解为平衡训练可达到的风险和迁移成本，比较SMOTE与自助法的理论性能。

Result: 超额风险可分解为平衡训练风险加迁移成本；在中等高维情况下，SMOTE的迁移成本主导自助法，表明自助法通常优于SMOTE；实验验证了理论发现。

Conclusion: 从迁移学习角度为不平衡分类的增强策略选择提供理论指导，自助法在多数情况下是比SMOTE更优的选择，特别是在中等高维设置中。

Abstract: Classification imbalance arises when one class is much rarer than the other. We frame this setting as transfer learning under label (prior) shift between an imbalanced source distribution induced by the observed data and a balanced target distribution under which performance is evaluated. Within this framework, we study a family of oversampling procedures that augment the training data by generating synthetic samples from an estimated minority-class distribution to roughly balance the classes, among which the celebrated SMOTE algorithm is a canonical example. We show that the excess risk decomposes into the rate achievable under balanced training (as if the data had been drawn from the balanced target distribution) and an additional term, the cost of transfer, which quantifies the discrepancy between the estimated and true minority-class distributions. In particular, we show that the cost of transfer for SMOTE dominates that of bootstrapping (random oversampling) in moderately high dimensions, suggesting that we should expect bootstrapping to have better performance than SMOTE in general. We corroborate these findings with experimental evidence. More broadly, our results provide guidance for choosing among augmentation strategies for imbalanced classification.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [12] [Model selection by cross-validation in an expectile linear regression](https://arxiv.org/abs/2601.09874)
*Bilel Bousselmi,Gabriela Ciuperca*

Main category: stat.ME

TL;DR: 本文研究线性模型在非对称误差下的变量选择问题，提出基于交叉验证的期望分位数LASSO方法，证明了模型选择的一致性，并通过模拟和实证验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 针对线性模型中可能存在的非对称误差问题，需要开发有效的变量选择方法。传统的交叉验证方法在非对称误差情况下可能不够有效，因此需要结合期望分位数回归和LASSO惩罚来改进变量选择。

Method: 将数据分为训练集和验证集，验证集样本量远大于训练集。在训练集上计算模型系数的期望分位数或自适应LASSO期望分位数估计量，然后在验证集上计算交叉验证平均得分(CVS)，选择最小化CVS的模型。

Result: 理论证明：在解释变量数量固定或随样本量变化的情况下，最小化CVS的模型选择是一致的。蒙特卡洛模拟验证了理论结果，并显示该方法优于文献中的其他两种方法。实证应用展示了该方法的实用性。

Conclusion: 提出的基于交叉验证的期望分位数模型选择技术在非对称误差的线性模型中具有一致性和优越性，为变量选择提供了有效的工具。

Abstract: For linear models that may have asymmetric errors, we study variable selection by cross-validation. The data are split into training and validation sets, with the number of observations in the validation set much larger than in the training set. For the model coefficients, the expectile or adaptive LASSO expectile estimators are calculated on the training set. These estimators will be used to calculate the cross-validation mean score (CVS) on the validation set. We show that the model that minimizes CVS is consistent in two cases: when the number of explanatory variables is fixed or when it depends on the number of observations. Monte Carlo simulations confirm the theoretical results and demonstrate the superiority of our estimation method compared to two others in the literature. The usefulness of the CV expectile model selection technique is illustrated by applying it to real data sets.

</details>


### [13] [Derivations for the Cumulative Standardized Binomial EWMA (CSB-EWMA) Control Chart](https://arxiv.org/abs/2601.09968)
*Faruk Muritala,Austin Brown,Dhrubajyoti Ghosh,Sherry Ni*

Main category: stat.ME

TL;DR: 本文推导了在多流过程中应用EWMA统计量监测二项比例时的精确均值和方差性质，提出了基于精确时变方差计算的自适应控制限的CSB-EWMA方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在早期监测阶段使用渐近近似存在局限性，需要精确的数学推导来建立二项比例监测的理论基础，以支持制造、医疗、网络安全等领域的统计过程控制。

Method: 推导了EWMA统计量在多流过程中应用于二项比例监测的精确数学性质，提出了累积标准化二项EWMA(CSB-EWMA)公式，使用精确的时变方差计算自适应控制限，并通过蒙特卡洛模拟进行验证。

Result: 理论预测与经验结果显示出显著的一致性，验证了推导的正确性，为并行数据流中二元结果的分布无关监测建立了理论基础。

Conclusion: 该工作克服了早期监测阶段渐近近似的局限性，为跨领域的统计过程控制提供了精确的数学框架，具有广泛的应用价值。

Abstract: This paper presents the exact mathematical derivation of the mean and variance properties for the Exponentially Weighted Moving Average (EWMA) statistic applied to binomial proportion monitoring in Multiple Stream Processes (MSPs). We develop a Cumulative Standardized Binomial EWMA (CSB-EWMA) formulation that provides adaptive control limits based on exact time-varying variance calculations, overcoming the limitations of asymptotic approximations during early-phase monitoring. The derivations are rigorously validated through Monte Carlo simulations, demonstrating remarkable agreement between theoretical predictions and empirical results. This work establishes a theoretical foundation for distribution-free monitoring of binary outcomes across parallel data streams, with applications in statistical process control across diverse domains including manufacturing, healthcare, and cybersecurity.

</details>


### [14] [Model-Agnostic and Uncertainty-Aware Dimensionality Reduction in Supervised Learning](https://arxiv.org/abs/2601.10357)
*Yue Yu,Guanghui Wang,Liu Liu,Changliang Zou*

Main category: stat.ME

TL;DR: 提出POD框架，通过直接评估样本外预测能力来确定最小预测充分维度，统一了降维与预测性能


<details>
  <summary>Details</summary>
Motivation: 传统降维方法在估计内在维度时过于关注模型特定的结构假设，而忽略了预测效用。需要一种模型无关的框架，能够直接基于预测性能来确定维度

Method: 提出预测顺序确定(POD)框架，通过直接评估样本外预测能力来确定最小预测充分维度。使用误差界限量化过估计和欠估计的不确定性，在温和条件下实现一致性

Result: 模拟和真实数据分析表明，POD能够提供准确且具有不确定性意识的维度估计，使其成为预测中心化流程的通用组件

Conclusion: POD通过统一降维与预测性能，为各种降维任务和监督学习器提供了灵活的维度确定框架，具有实际应用价值

Abstract: Dimension reduction is a fundamental tool for analyzing high-dimensional data in supervised learning. Traditional methods for estimating intrinsic order often prioritize model-specific structural assumptions over predictive utility. This paper introduces predictive order determination (POD), a model-agnostic framework that determines the minimal predictively sufficient dimension by directly evaluating out-of-sample predictiveness. POD quantifies uncertainty via error bounds for over- and underestimation and achieves consistency under mild conditions. By unifying dimension reduction with predictive performance, POD applies flexibly across diverse reduction tasks and supervised learners. Simulations and real-data analyses show that POD delivers accurate, uncertainty-aware order estimates, making it a versatile component for prediction-centric pipelines.

</details>


### [15] [High Dimensional Gaussian and Bootstrap Approximations in Generalized Linear Models](https://arxiv.org/abs/2601.09925)
*Mayukh Choudhury,Debraj Das*

Main category: stat.ME

TL;DR: 本文研究高维广义线性模型估计量的渐近性质，建立参数维度随样本量增长时的分布近似理论，并针对超高维情况提出扰动自助法


<details>
  <summary>Details</summary>
Motivation: 研究广义线性模型在参数维度随样本量增长时的渐近行为，特别是在高维和超高维情况下的统计推断问题

Method: 使用Fang和Koike的高维结果处理主要项，He和Shao的方法控制余项，Nazarov的高斯等周不等式；针对超高维情况提出扰动自助法

Result: 获得凸集上d=o(n^{2/5})和欧几里得球上d=o(n^{1/2})的高斯近似最优速率；证明Lasso在变量选择一致性下无法获得高斯近似，但扰动自助法有效

Conclusion: 建立了高维GLM估计量的最优分布近似理论，针对超高维情况提出的扰动自助法在理论和模拟中表现良好

Abstract: Generalized Linear Models (GLMs) extend ordinary linear regression by linking the mean of the response variable to covariates through appropriate link functions. This paper investigates the asymptotic behavior of GLM estimators when the parameter dimension $d$ grows with the sample size $n$. In the first part, we establish Gaussian approximation results for the distribution of a properly centered and scaled GLM estimator uniformly over class of convex sets and Euclidean balls. Using high-dimensional results from Fang and Koike (2024) for the leading Bahadur term, bounding remainder terms as in He and Shao (2000), and applying Nazarov's (2003) Gaussian isoperimetric inequality, we show that Gaussian approximation holds when $d = o(n^{2/5})$ for convex sets and $d = o(n^{1/2})$ for Euclidean balls-the best possible rates matching those for high-dimensional sample means. We further extend these results to the bootstrap approximation when the covariance matrix is unknown. In the second part, when $d>>n$, a natural question is to answer whether all covariates are equally important. To answer that, we employ sparsity in GLM through the Lasso estimator. While Lasso is widely used for variable selection, it cannot achieve both Variable Selection Consistency (VSC) and $n^{1/2}$-consistency simultaneously (Lahiri, 2021). Under the regime ensuring VSC, we show that Gaussian approximation for the Lasso estimator fails. To overcome this, we propose a Perturbation Bootstrap (PB) approach and establish a Berry-Esseen type bound for its approximation uniformly over class of convex sets. Simulation studies confirm the strong finite-sample performance of the proposed method.

</details>


### [16] [A Bayesian Discrete Framework for Enhancing Decision-Making Processes in Clinical Trial Designs and Evaluations](https://arxiv.org/abs/2601.10615)
*Paramahansa Pramanik,Arnab Kumar Maity,Anjan Mandal,Haley Kate Robinson*

Main category: stat.ME

TL;DR: 贝叶斯方法在临床试验中的应用研究，探讨其相比传统频率方法的优势，包括整合先验知识、适应性和决策支持，并分析二项、泊松、负二项分布等离散概率模型在临床终点建模中的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统频率方法在临床试验中缺乏整合先验知识的灵活性，限制了在适应性试验环境中的有效性。临床研究面临重复性困难和统计结果误解等挑战，需要更稳健的分析方法。

Method: 采用贝叶斯方法进行统计分析，探索二项分布、泊松分布和负二项分布等离散概率模型在临床终点建模中的应用。使用贝叶斯网络和贝叶斯估计技术，并与最大似然估计进行对比评估。

Result: 贝叶斯方法能够通过积累证据持续优化统计推断，支持更明智的决策制定，提高试验结果的可靠性。特别适用于二元响应或过度分散数据的临床试验。

Conclusion: 贝叶斯策略为增强临床研究的分析稳健性提供了可行路径，特别是在整合先验知识和适应复杂数据模式方面具有显著优势，有助于解决当前临床试验面临的挑战。

Abstract: This study examines the application of Bayesian approach in the context of clinical trials, emphasizing their increasing importance in contemporary biomedical research. While conventional frequentist approach provides a foundational basis for analysis, it often lacks the flexibility to integrate prior knowledge, which can constrain its effectiveness in adaptive settings. In contrast, Bayesian methods enable continual refinement of statistical inferences through the assimilation of accumulating evidence, thereby supporting more informed decision-making and improving the reliability of trial findings. This paper also considers persistent challenges in clinical investigations, including replication difficulties and the misinterpretation of statistical results, suggesting that Bayesian strategies may offer a path toward enhanced analytical robustness. Moreover, discrete probability models, specifically the Binomial, Poisson, and Negative Binomial distributions are explored for their suitability in modeling clinical endpoints, particularly in trials involving binary responses or data with overdispersion. The discussion further incorporates Bayesian networks and Bayesian estimation techniques, with a comparative evaluation against maximum likelihood estimation to elucidate differences in inferential behavior and practical implementation.

</details>


### [17] [Adjusted Similarity Measures and a Violation of Expectations](https://arxiv.org/abs/2601.10641)
*William L. Lippitt,Edward J. Bedrick,Nichole E. Carlson*

Main category: stat.ME

TL;DR: 本文研究调整相似度测量方法，探讨在不同零模型下调整算子的性质，并识别保证调整后测量具有期望性质（零期望、最大值为1）的充分条件。


<details>
  <summary>Details</summary>
Motivation: 调整相似度测量（如Cohen's kappa和调整Rand指数）是评估离散标记相似性的重要工具，传统上基于置换分布进行调整。当前研究关注更合适的零模型（如允许随机聚类数量的聚类集成），需要系统研究调整算子在一般零模型下的性质。

Method: 将调整算子的研究推广到一般零模型和更一般的过程（包括统计标准化作为特例），识别调整算子产生预期性质（零期望、最大值为1）的充分条件，这些条件与观测数据如何纳入零分布相关。

Result: 建立了调整算子在一般零模型下的理论框架，确定了保证调整后测量具有期望性质的充分条件。当这些条件被违反时，可能导致严重问题：传统调整可能产生非正测量而非零期望，统计标准化可能产生确定为零的测量。

Conclusion: 调整相似度测量的性质依赖于零模型如何纳入观测数据。研究为选择合适的零模型和调整方法提供了理论指导，确保调整后测量具有可解释的性质（零期望、最大值为1）。

Abstract: Adjusted similarity measures, such as Cohen's kappa for inter-rater reliability and the adjusted Rand index used to compare clustering algorithms, are a vital tool for comparing discrete labellings. These measures are intended to have the property of 0 expectation under a null distribution and maximum value 1 under maximal similarity to aid in interpretation. Measures are frequently adjusted with respect to the permutation distribution for historic and analytic reasons. There is currently renewed interest in considering other null models more appropriate for context, such as clustering ensembles permitting a random number of identified clusters. The purpose of this work is two -- fold: (1) to generalize the study of the adjustment operator to general null models and to a more general procedure which includes statistical standardization as a special case and (2) to identify sufficient conditions for the adjustment operator to produce the intended properties, where sufficient conditions are related to whether and how observed data are incorporated into null distributions. We demonstrate how violations of the sufficient conditions may lead to substantial breakdown, such as by producing a non-positive measure under traditional adjustment rather than one with mean 0, or by producing a measure which is deterministically 0 under statistical standardization.

</details>


### [18] [Tree Estimation and Saddlepoint-Based Diagnostics for the Nested Dirichlet Distribution: Application to Compositional Behavioral Data](https://arxiv.org/abs/2601.09941)
*Jacob A. Turner,Monnie McGee,Bianca A. Luedeker*

Main category: stat.ME

TL;DR: 本文针对嵌套狄利克雷分布(NDD)在实际应用中的两大限制——需要预定义树结构和缺乏模型拟合诊断工具——提出了解决方案：数据驱动的贪婪树搜索算法和基于鞍点近似的伪残差诊断方法。


<details>
  <summary>Details</summary>
Motivation: NDD虽然理论上比传统狄利克雷分布更灵活，能通过层次树结构放宽成分方差和相关性约束，但在实践中使用不足，主要因为需要预定义树结构和缺乏模型拟合诊断工具。

Method: 1. 提出数据驱动的贪婪树搜索算法，从观测数据中识别合理的NDD树结构；2. 开发基于鞍点近似的伪残差诊断工具和似然位移度量，用于检测有影响的观测值，即使在边缘分布解析不可解时也能提供计算可行的模型评估。

Result: 通过模拟研究和Morris水迷宫实验数据应用，证明该方法能产生可解释的结构并在现实成分数据设置中改进模型评估。实验目标是检测认知受损和未受损小鼠空间学习策略的差异。

Conclusion: 本文解决了NDD在实际应用中的关键限制，提供了数据驱动的树结构识别方法和有效的诊断工具，使NDD更适用于实际成分数据分析。配套R包支持方法的可重复性和新数据集应用。

Abstract: The Nested Dirichlet Distribution (NDD) provides a flexible alternative to the Dirichlet distribution for modeling compositional data, relaxing constraints on component variances and correlations through a hierarchical tree structure. While theoretically appealing, the NDD is underused in practice due to two main limitations: the need to predefine the tree structure and the lack of diagnostics for evaluating model fit. This paper addresses both issues. First, we introduce a data-driven, greedy tree-finding algorithm that identifies plausible NDD tree structures from observed data. Second, we propose novel diagnostic tools, including pseudo-residuals based on a saddlepoint approximation to the marginal distributions and a likelihood displacement measure to detect influential observations. These tools provide accurate and computationally tractable assessments of model fit, even when marginal distributions are analytically intractable. We demonstrate our approach through simulation studies and apply it to data from a Morris water maze experiment, where the goal is to detect differences in spatial learning strategies among cognitively impaired and unimpaired mice. Our methods yield interpretable structures and improved model evaluation in a realistic compositional setting. An accompanying R package is provided to support reproducibility and application to new datasets.

</details>


### [19] [Estimating the effect of lymphovascular invasion on 2-year survival probability under endogeneity: a recursive copula-based approach](https://arxiv.org/abs/2601.09984)
*Yang Ou,Lan Xue,Carmen Tekwe,Kedir N. Turi,Roger S. Zoh*

Main category: stat.ME

TL;DR: 提出半参数递归copula框架处理头颈鳞癌中淋巴血管侵犯(LVI)的内生性问题，无需强工具变量，在模拟和应用中显示优于传统方法


<details>
  <summary>Details</summary>
Motivation: LVI是头颈鳞癌的重要预后标志物，但其对生存的真实影响可能因未测量的混杂因素导致的内生性而扭曲。传统的一阶段条件模型和基于工具变量的两阶段估计器在内生性下容易产生偏倚，且实践中往往缺乏足够强的工具变量。

Method: 提出半参数递归copula框架，联合指定LVI（作为内生暴露变量）和二元2年生存结局的边际模型，通过灵活copula连接两者以考虑潜在混杂并适应删失数据，无需强工具变量。

Result: 在模拟研究中，该copula框架相比一阶段和两阶段方法显示出更小的偏倚和更好的区间覆盖，对中等程度的模型误设保持稳健。应用于TCGA数据（n=215）发现LVI显著降低2年生存概率约47%（95% CI: -0.61至-0.29），正依赖参数表明衰减由LVI和生存未观测成分间的残差依赖驱动。

Conclusion: 提出的copula框架在没有强工具变量的情况下为生存结局提供更可信的效应估计，减轻了内生性和删失导致的偏倚，加强了头颈鳞癌研究的定量证据。

Abstract: Lymphovascular invasion (LVI) is an important prognostic marker for head and neck squamous cell carcinoma (HNSC), but the true effect of LVI on survival may be distorted by endogeneity arising from unmeasured confounding. Conventional one-stage conditional models and instrument-based two-stage estimators are prone to bias under endogeneity, and sufficiently strong instruments are often unavailable in practice. To address these challenges, we propose a semiparametric recursive copula framework that jointly specifies marginal models for both LVI, treated as an endogenous exposure, and a binary 2-year survival outcome, and links them through a flexible copula to account for latent confounding and accommodate censoring without requiring strong instruments. In two simulation studies, we systematically varied sample sizes, censoring rates from 0% to 60%, and endogeneity strengths, and assessed robustness under moderate model misspecification. The proposed copula framework exhibited reduced bias and improved interval coverage compared with both one-stage and two-stage approaches while maintaining robustness to moderate misspecification. We applied the method to HNSC cases with associated clinical and microRNA data from The Cancer Genome Atlas (n = 215), and found that LVI significantly reduced 2-year survival probability by approximately 47%, with a 95% confidence interval of -0.61 to -0.29 on the probability scale. The estimated positive dependence parameter indicates that the attenuation is driven by residual dependence between unobserved components of LVI and survival. Overall, the proposed copula framework yields more credible effect estimates for survival outcomes in the absence of strong instruments, mitigating biases due to endogeneity and censoring and strengthening quantitative evidence for HNSC research.

</details>


### [20] [Weighted least squares estimation by multivariate-dependent weights for linear regression models](https://arxiv.org/abs/2601.10049)
*Lei Huang,Chengyue Liu,Li Wang*

Main category: stat.ME

TL;DR: 提出一种多元依赖加权最小二乘估计方法，通过构建解释变量的线性组合来刻画异方差性，提高多元线性回归模型的准确性和预测能力。


<details>
  <summary>Details</summary>
Motivation: 多元线性回归模型常面临由多个解释变量引起的异方差问题，传统的单变量依赖权重加权最小二乘估计在构建权重函数方面存在局限性。

Method: 通过构建解释变量的线性组合，最大化其与绝对残差值的Spearman秩相关系数，结合最大似然方法刻画异方差性，全面反映随机误差的方差变化趋势。

Result: 证明了算法得到的异方差波动最优线性组合指数估计量具有一致性和渐近正态性。模拟实验和实际数据应用（中国消费支出和波士顿房价数据）显示，该方法在参数估计、模型预测、区间估计和泛化能力方面优于单变量依赖权重方法。

Conclusion: 该方法为处理多元线性回归中的异方差问题提供了一种有效的新方法，特别在拟合波动较大的数据时表现出相对优势。

Abstract: Multivariate linear regression models often face the problem of heteroscedasticity caused by multiple explanatory variables. The weighted least squares estimation with univariate-dependent weights has limitations in constructing weight functions. Therefore, this paper proposes a multivariate dependent weighted least squares estimation method. By constructing a linear combination of explanatory variables and maximizing their Spearman rank correlation coefficient with the absolute residual value, combined with maximum likelihood method to depict heteroscedasticity, it can comprehensively reflect the trend of variance changes in the random error and improve the accuracy of the model. This paper demonstrates that the optimal linear combination exponent estimator for heteroscedastic volatility obtained by our algorithm possesses consistency and asymptotic normality. In the simulation experiment, three scenarios of heteroscedasticity were designed, and the comparison showed that the proposed method was superior to the univariate-dependent weighting method in parameter estimation and model prediction. In the real data applications, the proposed method was applied to two real-world datasets about consumer spending in China and housing prices in Boston. From the perspectives of MAE, RSE, cross-validation, and fitting performance, its accuracy and stability were verified in terms of model prediction, interval estimation, and generalization ability. Additionally, the proposed method demonstrated relative advantages in fitting data with large fluctuations. This study provides an effective new approach for dealing with heteroscedasticity in multivariate linear regression.

</details>


### [21] [Fair Regression under Demographic Parity: A Unified Framework](https://arxiv.org/abs/2601.10623)
*Yongzhen Feng,Weiwei Wang,Raymond K. W. Wong,Xianyang Zhang*

Main category: stat.ME

TL;DR: 提出一个统一的公平回归框架，通过风险最小化结合人口统计均等约束，适用于多种回归任务和损失函数，并提供高效的计算方法和理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有公平回归方法通常局限于特定损失函数或依赖于复杂的非凸优化，缺乏一个统一的框架来处理广泛的回归任务，需要一种既通用又计算高效的方法。

Method: 提出一个统一的公平回归框架，将问题表述为风险最小化问题，并加入人口统计均等约束。推导出公平风险最小化器的新特征，为一般损失函数提供计算高效的估计程序。

Result: 方法在多种回归设置中都能有效最小化风险同时满足公平约束，包括线性回归、二分类、分位数回归和鲁棒回归等。理论分析证明了估计量的渐近一致性和收敛速率。

Conclusion: 该框架为公平回归提供了一个统一、通用且计算高效的方法，适用于广泛的损失函数和回归任务，具有良好的理论保证和实际性能。

Abstract: We propose a unified framework for fair regression tasks formulated as risk minimization problems subject to a demographic parity constraint. Unlike many existing approaches that are limited to specific loss functions or rely on challenging non-convex optimization, our framework is applicable to a broad spectrum of regression tasks. Examples include linear regression with squared loss, binary classification with cross-entropy loss, quantile regression with pinball loss, and robust regression with Huber loss. We derive a novel characterization of the fair risk minimizer, which yields a computationally efficient estimation procedure for general loss functions. Theoretically, we establish the asymptotic consistency of the proposed estimator and derive its convergence rates under mild assumptions. We illustrate the method's versatility through detailed discussions of several common loss functions. Numerical results demonstrate that our approach effectively minimizes risk while satisfying fairness constraints across various regression settings.

</details>


### [22] [Asymptotic Theory of Tail Dependence Measures for Checkerboard Copula and the Validity of Multiplier Bootstrap](https://arxiv.org/abs/2601.10252)
*Mayukh Choudhury,Debraj Das,Sujit Ghosh*

Main category: stat.ME

TL;DR: 提出基于棋盘平滑的尾部连接函数非参数估计方法，解决未知边际分布下的尾部依赖推断问题，并开发直接乘数自助法进行可行推断


<details>
  <summary>Details</summary>
Motivation: 在未知边际分布情况下，经验尾部连接函数存在离散性和边界不规则问题，需要开发稳健的估计和推断方法

Method: 引入基于局部双线性插值的棋盘平滑尾部连接函数估计器，建立其一致性和弱收敛性，并开发直接乘数自助法进行推断

Result: 建立了经验棋盘尾部连接函数过程的一致性和弱收敛性，证明了自助法过程条件弱收敛到相同高斯极限，提供了尾部依赖系数的渐近有效推断

Conclusion: 该方法为尾部依赖分析提供了完全可行的置信区域和假设检验框架，无需显式估计极限协方差结构，模拟研究验证了有限样本性能

Abstract: Nonparametric estimation and inference for lower and upper tail copulas under unknown marginal distributions are considered. To mitigate the inherent discreteness and boundary irregularities of the empirical tail copula, a checkerboard smoothed tail copula estimator based on local bilinear interpolation is introduced. Almost sure uniform consistency and weak convergence of the centered and scaled empirical checkerboard tail copula process are established in the space of bounded functions. The resulting Gaussian limit differs from its known-marginal counterpart and incorporates additional correction terms that account for first-order stochastic errors arising from marginal estimation. Since the limiting covariance structure depends on the unknown tail copula and its partial derivatives, direct asymptotic inference is generally infeasible. To address this challenge, a direct multiplier bootstrap procedure tailored to the checkerboard tail copula is developed. By combining multiplier reweighting with checkerboard smoothing, the bootstrap preserves the extremal dependence structure of the data and consistently captures both joint tail variability and the effects of marginal estimation. Conditional weak convergence of the bootstrap process to the same Gaussian limit as the original estimator is established, yielding asymptotically valid inference for smooth functionals of the tail copula, including the lower and upper tail dependence coefficient. The proposed approach provides a fully feasible framework for confidence regions and hypothesis testing in tail dependence analysis without requiring explicit estimation of the limiting covariance structure. A simulation study illustrates the finite-sample performance of the proposed estimator and demonstrates the accuracy and reliability of the bootstrap confidence intervals under various dependence structures and tuning parameter choices.

</details>


### [23] [A Propagation Framework for Network Regression](https://arxiv.org/abs/2601.10533)
*Yingying Ma,Chenlei Leng*

Main category: stat.ME

TL;DR: 提出网络传播回归(NPR)框架，统一处理网络数据回归问题，通过普通最小二乘法估计，捕捉直接和间接网络效应。


<details>
  <summary>Details</summary>
Motivation: 现有网络数据回归模型需要专门的估计程序或施加限制性衰减假设，缺乏统一且计算高效的框架。

Method: 网络传播回归(NPR)将结果建模为通过网络连接传播的协变量函数，捕捉直接和间接效应，可通过普通最小二乘法估计连续结果，标准程序处理二元、分类和时间事件数据。

Result: 在弱条件下建立一致性和渐近正态性，开发有效的网络影响阶数假设检验。模拟研究表明NPR优于线性均值模型和网络凝聚回归，尤其在模型误设情况下。

Conclusion: NPR提供了一个统一、计算高效且可解释的框架，适用于各种网络数据回归问题，在社交媒体情感分析等实际应用中表现出实用性和鲁棒性。

Abstract: We introduce a unified and computationally efficient framework for regression on network data, addressing limitations of existing models that require specialized estimation procedures or impose restrictive decay assumptions. Our Network Propagation Regression (NPR) models outcomes as functions of covariates propagated through network connections, capturing both direct and indirect effects. NPR is estimable via ordinary least squares for continuous outcomes and standard routines for binary, categorical, and time-to-event data, all within a single interpretable framework. We establish consistency and asymptotic normality under weak conditions and develop valid hypothesis tests for the order of network influence. Simulation studies demonstrate that NPR consistently outperforms established approaches, such as the linear-in-means model and regression with network cohesion, especially under model misspecification. An application to social media sentiment analysis highlights the practical utility and robustness of NPR in real-world settings.

</details>


### [24] [From aggressive to conservative early stopping in Bayesian group sequential designs](https://arxiv.org/abs/2601.10590)
*Zhangyi He,Feng Yu,Suzie Cro,Laurent Billot*

Main category: stat.ME

TL;DR: 该论文提出了两种改进贝叶斯序贯设计的方法，通过引入两阶段后验概率阈值和预测概率标准，恢复早期停止的保守性，使其更符合O'Brien-Fleming型边界的行为。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯序贯设计使用固定的后验概率阈值，虽然能控制整体I类错误率，但缺乏对早期分析的惩罚，导致过早停止的风险增加，可能产生过早结论和治疗效应估计的膨胀，这在确证性试验中存在问题。

Method: 提出了两种改进策略：1) 两阶段后验概率阈值结构，在试验早期应用更严格的标准，后期放宽以保持功效；2) 用预测概率标准替代后验概率监测，自然考虑未来数据的不确定性，从而抑制过早停止。两种策略都只需要一个额外的调优参数，并能高效校准。

Result: 在HYPRESS设置中，两种方法都比传统贝叶斯设计获得更高的功效，同时产生的α消耗曲线在早期分析中与O'Brien-Fleming型行为紧密对齐。

Conclusion: 这些改进为贝叶斯序贯设计提供了原则性和可操作的方法，使其与公认的频率学派实践和监管期望保持一致，支持其在确证性试验中的稳健应用。

Abstract: Group sequential designs (GSDs) are widely used in confirmatory trials to allow interim monitoring while preserving control of the type I error rate. In the frequentist framework, O'Brien-Fleming-type stopping boundaries dominate practice because they impose highly conservative early stopping while allowing more liberal decisions as information accumulates. Bayesian GSDs, in contrast, are most often implemented using fixed posterior probability thresholds applied uniformly at all analyses. While such designs can be calibrated to control the overall type I error rate, they do not penalise early analyses and can therefore lead to substantially more aggressive early stopping. Such behaviour can risk premature conclusions and inflation of treatment effect estimates, raising concerns for confirmatory trials. We introduce two practically implementable refinements that restore conservative early stopping in Bayesian GSDs. The first introduces a two-phase structure for posterior probability thresholds, applying more stringent criteria in the early phase of the trial and relaxing them later to preserve power. The second replaces posterior probability monitoring at interim looks with predictive probability criteria, which naturally account for uncertainty in future data and therefore suppress premature stopping. Both strategies require only one additional tuning parameter and can be efficiently calibrated. In the HYPRESS setting, both approaches achieve higher power than the conventional Bayesian design while producing alpha-spending profiles closely aligned with O'Brien-Fleming-type behaviour at early looks. These refinements provide a principled and tractable way to align Bayesian GSDs with accepted frequentist practice and regulatory expectations, supporting their robust application in confirmatory trials.

</details>
