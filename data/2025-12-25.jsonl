{"id": "2512.21193", "categories": ["stat.CO", "cs.CC", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.21193", "abs": "https://arxiv.org/abs/2512.21193", "authors": ["Brani Vidakovic"], "title": "Adjusted Kolmogorov Complexity of Binary Words with Empirical Entropy Normalization", "comment": "28 pages, 2 figures", "summary": "Kolmogorov complexity of a finite binary word reflects both algorithmic structure and the empirical distribution of symbols appearing in the word. Words with symbol frequencies far from one half have smaller combinatorial richness and therefore appear less complex under the standard definition. In this paper an entropy-normalized complexity measure is introduced that divides the Kolmogorov complexity of a word by the empirical entropy of its observed distribution of zeros and ones. This adjustment isolates intrinsic descriptive complexity from the purely combinatorial effect of symbol imbalance. For Martin Löf random sequences under constructive exchangeable measures, the adjusted complexity grows linearly and converges to one. A pathological construction shows that regularity of the underlying measure is essential. The proposed framework connects Kolmogorov complexity, empirical entropy, and randomness in a natural manner and suggests applications in randomness testing and in the analysis of structured binary data."}
{"id": "2512.20753", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.20753", "abs": "https://arxiv.org/abs/2512.20753", "authors": ["Madison Coots", "Robert Bartlett", "Julian Nyarko", "Sharad Goel"], "title": "A Profit-Based Measure of Lending Discrimination", "comment": null, "summary": "Algorithmic lending has transformed the consumer credit landscape, with complex machine learning models now commonly used to make or assist underwriting decisions. To comply with fair lending laws, these algorithms typically exclude legally protected characteristics, such as race and gender. Yet algorithmic underwriting can still inadvertently favor certain groups, prompting new questions about how to audit lending algorithms for potentially discriminatory behavior. Building on prior theoretical work, we introduce a profit-based measure of lending discrimination in loan pricing. Applying our approach to approximately 80,000 personal loans from a major U.S. fintech platform, we find that loans made to men and Black borrowers yielded lower profits than loans to other groups, indicating that men and Black applicants benefited from relatively favorable lending decisions. We trace these disparities to miscalibration in the platform's underwriting model, which underestimates credit risk for Black borrowers and overestimates risk for women. We show that one could correct this miscalibration -- and the corresponding lending disparities -- by explicitly including race and gender in underwriting models, illustrating a tension between competing notions of fairness."}
{"id": "2512.20810", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.20810", "abs": "https://arxiv.org/abs/2512.20810", "authors": ["Jakub J. Pypkowski", "Adam M. Sykulski", "James S. Martin", "Ben P. Marchant"], "title": "The Whittle likelihood for mixed models with application to groundwater level time series", "comment": "29 pages, 8 figures, 2 tables, 1 appendix", "summary": "Understanding the processes that influence groundwater levels is crucial for forecasting and responding to hazards such as groundwater droughts. Mixed models, which combine a fixed mean, expressed using independent predictors, with autocorrelated random errors, are used for inference, forecasting and filling in missing values in groundwater level time series. Estimating parameters of mixed models using maximum likelihood has high computational complexity. For large datasets, this leads to restrictive simplifying assumptions such as fixing certain free parameters in practical implementations. In this paper, we propose a method to jointly estimate all parameters of mixed models using the Whittle likelihood, a frequency-domain quasi-likelihood. Our method is robust to missing and non-Gaussian data and can handle much larger data sizes. We demonstrate the utility of our method both in a simulation study and with real-world data, comparing against maximum likelihood and an alternative two-stage approach that estimates fixed and random effect parameters separately."}
{"id": "2512.21136", "categories": ["stat.ME", "math.PR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.21136", "abs": "https://arxiv.org/abs/2512.21136", "authors": ["Ankita Sharma", "Partha Chakroborty", "Pranamesh Chakraborty"], "title": "Modeling gap acceptance behavior allowing for perceptual distortions and exogenous influences", "comment": null, "summary": "This work on gap acceptance is based on the premise that the decision to accept/reject a gap happens in a person's mind and therefore must be based on the perceived gap and not the measured gap. The critical gap must also exist in a person's mind and hence, together with the perceived gap, is a latent variable. Finally, it is also proposed that the critical gap is influenced by various exogenous variables such as subject and opposing vehicle types, and perceived waiting time. Mathematical models that (i) incorporate systematic and random distortions during the perception process and (ii) account for the effect of the various influencing variables are developed. The parameters of these models are estimated for two different gap acceptance data sets using the maximum likelihood technique. The data is collected as part of this study. The estimated parameters throw valuable insights into how these influencing variables affect the critical gap. The results corroborate the initial predictions on the nature of influence these variables must exert and give strength to the gap acceptance decision-making construct proposed here. This work also proposes a methodology to estimate a measurable/observable world emulator of the latent variable critical gap. The use of the emulator critical gap provides improved estimates of derived quantities like the average waiting time of subject vehicles. Finally, studies are also conducted to show that the number of rejected gaps can work as a reasonable surrogate for the influencing variable, waiting time."}
{"id": "2512.20682", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.20682", "abs": "https://arxiv.org/abs/2512.20682", "authors": ["Stefan Volz", "Martin Storath", "Andreas Weinmann"], "title": "Fast and Exact Least Absolute Deviations Line Fitting via Piecewise Affine Lower-Bounding", "comment": "Submitted to IEEE Transactions on Signal Processing", "summary": "Least-absolute-deviations (LAD) line fitting is robust to outliers but computationally more involved than least squares regression. Although the literature includes linear and near-linear time algorithms for the LAD line fitting problem, these methods are difficult to implement and, to our knowledge, lack maintained public implementations. As a result, practitioners often resort to linear programming (LP) based methods such as the simplex-based Barrodale-Roberts method and interior-point methods, or on iteratively reweighted least squares (IRLS) approximation which does not guarantee exact solutions. To close this gap, we propose the Piecewise Affine Lower-Bounding (PALB) method, an exact algorithm for LAD line fitting. PALB uses supporting lines derived from subgradients to build piecewise-affine lower bounds, and employs a subdivision scheme involving minima of these lower bounds. We prove correctness and provide bounds on the number of iterations. On synthetic datasets with varied signal types and noise including heavy-tailed outliers as well as a real dataset from the NOAA's Integrated Surface Database, PALB exhibits empirical log-linear scaling. It is consistently faster than publicly available implementations of LP based and IRLS based solvers. We provide a reference implementation written in Rust with a Python API."}
{"id": "2512.20685", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.20685", "abs": "https://arxiv.org/abs/2512.20685", "authors": ["Jonas Arruda", "Niels Bracher", "Ullrich Köthe", "Jan Hasenauer", "Stefan T. Radev"], "title": "Diffusion Models in Simulation-Based Inference: A Tutorial Review", "comment": null, "summary": "Diffusion models have recently emerged as powerful learners for simulation-based inference (SBI), enabling fast and accurate estimation of latent parameters from simulated and real data. Their score-based formulation offers a flexible way to learn conditional or joint distributions over parameters and observations, thereby providing a versatile solution to various modeling problems. In this tutorial review, we synthesize recent developments on diffusion models for SBI, covering design choices for training, inference, and evaluation. We highlight opportunities created by various concepts such as guidance, score composition, flow matching, consistency models, and joint modeling. Furthermore, we discuss how efficiency and statistical accuracy are affected by noise schedules, parameterizations, and samplers. Finally, we illustrate these concepts with case studies across parameter dimensionalities, simulation budgets, and model types, and outline open questions for future research."}
{"id": "2512.20810", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.20810", "abs": "https://arxiv.org/abs/2512.20810", "authors": ["Jakub J. Pypkowski", "Adam M. Sykulski", "James S. Martin", "Ben P. Marchant"], "title": "The Whittle likelihood for mixed models with application to groundwater level time series", "comment": "29 pages, 8 figures, 2 tables, 1 appendix", "summary": "Understanding the processes that influence groundwater levels is crucial for forecasting and responding to hazards such as groundwater droughts. Mixed models, which combine a fixed mean, expressed using independent predictors, with autocorrelated random errors, are used for inference, forecasting and filling in missing values in groundwater level time series. Estimating parameters of mixed models using maximum likelihood has high computational complexity. For large datasets, this leads to restrictive simplifying assumptions such as fixing certain free parameters in practical implementations. In this paper, we propose a method to jointly estimate all parameters of mixed models using the Whittle likelihood, a frequency-domain quasi-likelihood. Our method is robust to missing and non-Gaussian data and can handle much larger data sizes. We demonstrate the utility of our method both in a simulation study and with real-world data, comparing against maximum likelihood and an alternative two-stage approach that estimates fixed and random effect parameters separately."}
{"id": "2512.20811", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20811", "abs": "https://arxiv.org/abs/2512.20811", "authors": ["Rommel Cortez", "Bala Krishnamoorthy"], "title": "Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights", "comment": null, "summary": "Several performance measures are used to evaluate binary and multiclass classification tasks.\n  But individual observations may often have distinct weights, and none of these measures are sensitive to such varying weights.\n  We propose a new weighted Pearson-Matthews Correlation Coefficient (MCC) for binary classification as well as weighted versions of related multiclass measures. The weighted MCC varies between $-1$ and $1$. But crucially, the weighted MCC values are higher for classifiers that perform better on highly weighted observations, and hence is able to distinguish them from classifiers that have a similar overall performance and ones that perform better on the lowly weighted observations.\n  Furthermore, we prove that the weighted measures are robust with respect to the choice of weights in a precise manner:\n  if the weights are changed by at most $ε$, the value of the weighted measure changes at most by a factor of $ε$ in the binary case\n  and by a factor of $ε^2$ in the multiclass case.\n  Our computations demonstrate that the weighted measures clearly identify classifiers that perform better on higher weighted observations, while the unweighted measures remain completely indifferent to the choices of weights."}
{"id": "2512.20837", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.20837", "abs": "https://arxiv.org/abs/2512.20837", "authors": ["Jasper B. Yang", "Thomas Lumley", "Bryan E. Shepherd", "Pamela A. Shaw"], "title": "Improving optimal subsampling through stratification", "comment": "29 pages (20 main, 9 supplemental); 4 figures", "summary": "Recent works have proposed optimal subsampling algorithms to improve computational efficiency in large datasets and to design validation studies in the presence of measurement error. Existing approaches generally fall into two categories: (i) designs that optimize individualized sampling rules, where unit-specific probabilities are assigned and applied independently, and (ii) designs based on stratified sampling with simple random sampling within strata. Focusing on the logistic regression setting, we derive the asymptotic variances of estimators under both approaches and compare them numerically through extensive simulations and an application to data from the Vanderbilt Comprehensive Care Clinic cohort. Our results reinforce that stratified sampling is not merely an approximation to individualized sampling, showing instead that optimal stratified designs are often more efficient than optimal individualized designs through their elimination of between-stratum contributions to variance. These findings suggest that optimizing over the class of individualized sampling rules overlooks highly efficient sampling designs and highlight the often underappreciated advantages of stratified sampling."}
{"id": "2512.21005", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.21005", "abs": "https://arxiv.org/abs/2512.21005", "authors": ["Edwin Fong", "Lancelot F. James", "Juho Lee"], "title": "Learning from Neighbors with PHIBP: Predicting Infectious Disease Dynamics in Data-Sparse Environments", "comment": "Draft Book chapter on AMMI methods -- Application of PHIBP arXiv:2502.01919 to Infectious Disease Detection with suggested extensions using the developments in arXiv:2508.18668", "summary": "Modeling sparse count data, which arise across numerous scientific fields, presents significant statistical challenges. This chapter addresses these challenges in the context of infectious disease prediction, with a focus on predicting outbreaks in geographic regions that have historically reported zero cases. To this end, we present the detailed computational framework and experimental application of the Poisson Hierarchical Indian Buffet Process (PHIBP), with demonstrated success in handling sparse count data in microbiome and ecological studies. The PHIBP's architecture, grounded in the concept of absolute abundance, systematically borrows statistical strength from related regions and circumvents the known sensitivities of relative-rate methods to zero counts. Through a series of experiments on infectious disease data, we show that this principled approach provides a robust foundation for generating coherent predictive distributions and for the effective use of comparative measures such as alpha and beta diversity. The chapter's emphasis on algorithmic implementation and experimental results confirms that this unified framework delivers both accurate outbreak predictions and meaningful epidemiological insights in data-sparse settings."}
{"id": "2512.20922", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.20922", "abs": "https://arxiv.org/abs/2512.20922", "authors": ["Jiarui Sun", "Kaiyuan Liu", "Xiao-Hua Zhou"], "title": "A Unified Inference Method for FROC-type Curves and Related Summary Indices", "comment": null, "summary": "Free-response observer performance studies are of great importance for accuracy evaluation and comparison in tasks related to the detection and localization of multiple targets or signals. The free-response receiver operating characteristic (FROC) curve and many similar curves based on the free-response observer performance assessment data are important tools to display the accuracy of detection under different thresholds. The true positive rate at a fixed false positive rate and summary indices such as the area under the FROC curve are also commonly used as the figures of merit in the statistical evaluation of these studies. Motivated by a free-response observer performance assessment research of a Software as a Medical Device (SaMD), we propose a unified method based on the initial-detection-and-candidate model to simultaneously estimate a smooth curve and derive confidence intervals for summary indices and the true positive rate at a fixed false positive rate. A maximum likelihood estimator is proposed and its asymptotic normality property is derived. Confidence intervals are constructed based on the asymptotic normality of our maximum likelihood estimator. Simulation studies are conducted to evaluate the finite sample performance of the proposed method. We apply the proposed method to evaluate the diagnostic performance of the SaMD for detecting pulmonary lesions."}
{"id": "2512.21020", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21020", "abs": "https://arxiv.org/abs/2512.21020", "authors": ["Li Cunzhi", "Louis Kang", "Hideaki Shimazaki"], "title": "Enhancing diffusion models with Gaussianization preprocessing", "comment": "17 pages, 9 figures", "summary": "Diffusion models are a class of generative models that have demonstrated remarkable success in tasks such as image generation. However, one of the bottlenecks of these models is slow sampling due to the delay before the onset of trajectory bifurcation, at which point substantial reconstruction begins. This issue degrades generation quality, especially in the early stages. Our primary objective is to mitigate bifurcation-related issues by preprocessing the training data to enhance reconstruction quality, particularly for small-scale network architectures. Specifically, we propose applying Gaussianization preprocessing to the training data to make the target distribution more closely resemble an independent Gaussian distribution, which serves as the initial density of the reconstruction process. This preprocessing step simplifies the model's task of learning the target distribution, thereby improving generation quality even in the early stages of reconstruction with small networks. The proposed method is, in principle, applicable to a broad range of generative tasks, enabling more stable and efficient sampling processes."}
{"id": "2512.21060", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.21060", "abs": "https://arxiv.org/abs/2512.21060", "authors": ["Mohammed Saif Ismail Hameed", "Eric D. Schoen", "Jose Nunez Ares", "Peter Goos"], "title": "Two-level D- and A-optimal designs of Ehlich type with run sizes three more than a multiple of four", "comment": null, "summary": "For the majority of run sizes N where N <= 20, the literature reports the best D- and A-optimal designs for the main-effects model which sequentially minimizes the aliasing between main effects and interaction effects and among interaction effects. The only series of run sizes for which all the minimally aliased D- and A-optimal main-effects designs remain unknown are those with run sizes three more than a multiple of four. To address this, in our paper, we propose an algorithm to generate all non-isomorphic D- and A-optimal main-effects designs for run sizes three more than a multiple of four. We enumerate all such designs for run sizes up to 19, report the numbers of designs we obtained, and identify those that minimize the aliasing between main effects and interaction effects and among interaction effects."}
{"id": "2512.21211", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21211", "abs": "https://arxiv.org/abs/2512.21211", "authors": ["Georgios Filippou", "Boi Mai Quach", "Diana Lenghel", "Arthur White", "Ashish Kumar Jha"], "title": "Causal-driven attribution (CDA): Estimating channel influence without user-level data", "comment": "42 pages, 8 figures, submitted initially to the journal of the academy of marketing science on 24th Dec 2025", "summary": "Attribution modelling lies at the heart of marketing effectiveness, yet most existing approaches depend on user-level path data, which are increasingly inaccessible due to privacy regulations and platform restrictions. This paper introduces a Causal-Driven Attribution (CDA) framework that infers channel influence using only aggregated impression-level data, avoiding any reliance on user identifiers or click-path tracking. CDA integrates temporal causal discovery (using PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions to conversions. Using large-scale synthetic data designed to replicate real marketing dynamics, we show that CDA achieves an average relative RMSE of 9.50% when given the true causal graph, and 24.23% when using the predicted graph, demonstrating strong accuracy under correct structure and meaningful signal recovery even under structural uncertainty. CDA captures cross-channel interdependencies while providing interpretable, privacy-preserving attribution insights, offering a scalable and future-proof alternative to traditional path-based models."}
{"id": "2512.21124", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.21124", "abs": "https://arxiv.org/abs/2512.21124", "authors": ["Jingyu Zhu", "Daniel W. Apley"], "title": "Measuring Variable Importance via Accumulated Local Effects", "comment": "32 pages, 8 figures", "summary": "A shortcoming of black-box supervised learning models is their lack of interpretability or transparency. To facilitate interpretation, post-hoc global variable importance measures (VIMs) are widely used to assign to each predictor or input variable a numerical score that represents the extent to which that predictor impacts the fitted model's response predictions across the training data. It is well known that the most common existing VIMs, namely marginal Shapley and marginal permutation-based methods, can produce unreliable results if the predictors are highly correlated, because they require extrapolation of the response at predictor values that fall far outside the training data. Conditional versions of Shapley and permutation VIMs avoid or reduce the extrapolation but can substantially deflate the importance of correlated predictors. For the related goal of visualizing the effects of each predictor when strong predictor correlation is present, accumulated local effects (ALE) plots were recently introduced and have been widely adopted. This paper presents a new VIM approach based on ALE concepts that avoids both the extrapolation and the VIM deflation problems when predictors are correlated. We contrast, both theoretically and numerically, ALE VIMs with Shapley and permutation VIMs. Our results indicate that ALE VIMs produce similar variable importance rankings as Shapley and permutation VIMs when predictor correlations are mild and more reliable rankings when correlations are strong. An additional advantage is that ALE VIMs are far less computationally expensive."}
{"id": "2512.21136", "categories": ["stat.ME", "math.PR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.21136", "abs": "https://arxiv.org/abs/2512.21136", "authors": ["Ankita Sharma", "Partha Chakroborty", "Pranamesh Chakraborty"], "title": "Modeling gap acceptance behavior allowing for perceptual distortions and exogenous influences", "comment": null, "summary": "This work on gap acceptance is based on the premise that the decision to accept/reject a gap happens in a person's mind and therefore must be based on the perceived gap and not the measured gap. The critical gap must also exist in a person's mind and hence, together with the perceived gap, is a latent variable. Finally, it is also proposed that the critical gap is influenced by various exogenous variables such as subject and opposing vehicle types, and perceived waiting time. Mathematical models that (i) incorporate systematic and random distortions during the perception process and (ii) account for the effect of the various influencing variables are developed. The parameters of these models are estimated for two different gap acceptance data sets using the maximum likelihood technique. The data is collected as part of this study. The estimated parameters throw valuable insights into how these influencing variables affect the critical gap. The results corroborate the initial predictions on the nature of influence these variables must exert and give strength to the gap acceptance decision-making construct proposed here. This work also proposes a methodology to estimate a measurable/observable world emulator of the latent variable critical gap. The use of the emulator critical gap provides improved estimates of derived quantities like the average waiting time of subject vehicles. Finally, studies are also conducted to show that the number of rejected gaps can work as a reasonable surrogate for the influencing variable, waiting time."}
{"id": "2512.21283", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.21283", "abs": "https://arxiv.org/abs/2512.21283", "authors": ["Yuyao Wang", "Andrew Ying", "Ronghui Xu"], "title": "Proximal Survival Analysis for Dependent Left Truncation", "comment": null, "summary": "In prevalent cohort studies with delayed entry, time-to-event outcomes are often subject to left truncation where only subjects that have not experienced the event at study entry are included, leading to selection bias. Existing methods for handling left truncation mostly rely on the (quasi-)independence assumption or the weaker conditional (quasi-)independence assumption which assumes that conditional on observed covariates, the left truncation time and the event time are independent on the observed region. In practice, however, our analysis of the Honolulu Asia Aging Study (HAAS) suggests that the conditional quasi-independence assumption may fail because measured covariates often serve only as imperfect proxies for the underlying mechanisms, such as latent health status, that induce dependence between truncation and event times. To address this gap, we propose a proximal weighting identification framework that admits the dependence-inducing factors may not be fully observed. We then construct an estimator based on the framework and study its asymptotic properties. We examine the finite sample performance of the proposed estimator by comprehensive simulations, and apply it to analyzing the cognitive impairment-free survival probabilities using data from the Honolulu Asia Aging Study."}
{"id": "2512.20685", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.20685", "abs": "https://arxiv.org/abs/2512.20685", "authors": ["Jonas Arruda", "Niels Bracher", "Ullrich Köthe", "Jan Hasenauer", "Stefan T. Radev"], "title": "Diffusion Models in Simulation-Based Inference: A Tutorial Review", "comment": null, "summary": "Diffusion models have recently emerged as powerful learners for simulation-based inference (SBI), enabling fast and accurate estimation of latent parameters from simulated and real data. Their score-based formulation offers a flexible way to learn conditional or joint distributions over parameters and observations, thereby providing a versatile solution to various modeling problems. In this tutorial review, we synthesize recent developments on diffusion models for SBI, covering design choices for training, inference, and evaluation. We highlight opportunities created by various concepts such as guidance, score composition, flow matching, consistency models, and joint modeling. Furthermore, we discuss how efficiency and statistical accuracy are affected by noise schedules, parameterizations, and samplers. Finally, we illustrate these concepts with case studies across parameter dimensionalities, simulation budgets, and model types, and outline open questions for future research."}
