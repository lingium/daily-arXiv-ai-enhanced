<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 14]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 8]
- [stat.ML](#stat.ML) [Total: 6]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Dependence-Aware False Discovery Rate Control in Two-Sided Gaussian Mean Testing](https://arxiv.org/abs/2511.19960)
*Deepra Ghosh,Sanat K. Sarkar*

Main category: stat.ME

TL;DR: 提出了一个控制高斯均值双边检验中错误发现率(FDR)的通用框架，扩展了BH方法在相关双边检验中的有效性，并引入了广义移位BH方法。


<details>
  <summary>Details</summary>
Motivation: BH方法在独立或特定单边依赖结构下能有效控制FDR，但在相关双边检验中的有效性一直是个未解决的问题，需要开发新的理论框架和方法。

Method: 引入了零假设下的正左尾依赖(PLTDN)概念，扩展了经典依赖假设到双边设置，并提出了广义移位BH(GSBH)方法，通过简单的p值调整来整合相关性信息。

Result: 模拟结果显示在各种依赖结构下都能可靠控制FDR并提高检验功效，HIV基因表达数据集的应用证明了该方法的实际有效性。

Conclusion: 该框架为相关双边检验中的FDR控制提供了理论基础和实用方法，解决了BH方法在这一重要场景中的有效性难题。

Abstract: This paper develops a general framework for controlling the false discovery rate (FDR) in multiple testing of Gaussian means against two-sided alternatives. The widely used Benjamini-Hochberg (BH) procedure provides exact FDR control under independence or conservative control under specific one-sided dependence structures, but its validity for correlated two-sided tests has remained an open question. We introduce the notion of positive left-tail dependence under the null (PLTDN), extending classical dependence assumptions to two-sided settings, and show that it ensures valid FDR control for BH-type procedures. Building on this framework, we propose a family of generalized shifted BH (GSBH) methods that incorporate correlation information through simple p-value adjustments. Simulation results demonstrate reliable FDR control and improved power across a range of dependence structures, while an application to an HIV gene expression dataset illustrates the practical effectiveness of the proposed approach.

</details>


### [2] [Clarifying identification and estimation of treatment effects in the Sequential Parallel Comparison Design](https://arxiv.org/abs/2511.19677)
*Benjamin Stockton,Michele Santacatterina,Soutrik Mandal,Charles M. Cleland,Erinn M. Hade,Nicholas Illenberger,Sharon Meropol,Andrea B. Troxel,Eva Petkova,Chang Yu,Thaddeus Tarpey*

Main category: stat.ME

TL;DR: 本文使用因果推理工具分析SPCD临床试验，澄清了在何种假设下可以识别治疗效果，以及传统估计量在SPCD试验各阶段的目标效果，发现传统SPCD估计量不针对有意义的治疗效果。


<details>
  <summary>Details</summary>
Motivation: SPCD临床试验旨在通过调整安慰剂反应来最小化安慰剂反应者对活性治疗效应估计的影响，但需要澄清在何种假设下可以识别治疗效果以及传统估计量的目标效果。

Method: 使用因果推理工具分析SPCD试验，研究治疗效果识别的假设条件，评估传统估计量在各阶段的目标效果，并说明安慰剂反应错误分类对第二阶段估计的严重影响。

Result: 发现传统SPCD估计量不针对有意义的治疗效果，安慰剂反应错误分类对第二阶段估计具有高度影响。

Conclusion: 传统SPCD估计量不针对有意义的治疗效果，需要重新考虑SPCD试验的设计和估计方法。

Abstract: Sequential parallel comparison design (SPCD) clinical trials aim to adjust active treatment effect estimates for placebo response to minimize the impact of placebo responders on the estimates. This is potentially accomplished using a two stage design by measuring treatment effects among all participants during the first stage, then classifying some placebo arm participants as placebo non-responders who will be re-randomized in the second stage. In this paper, we use causal inference tools to clarify under what assumptions treatment effects can be identified in SPCD trials and what effects the conventional estimators target at each stage of the SPCD trial. We further illustrate the highly influential impact of placebo response misclassification on the second stage estimate. We conclude that the conventional SPCD estimators do not target meaningful treatment effects.

</details>


### [3] [Integrating RCTs, RWD, AI/ML and Statistics: Next-Generation Evidence Synthesis](https://arxiv.org/abs/2511.19735)
*Shu Yang,Margaret Gamalo,Haoda Fu*

Main category: stat.ME

TL;DR: 本文主张将随机对照试验(RCT)与真实世界数据(RWD)、传统统计与人工智能/机器学习(AI/ML)进行原则性整合，提出因果路线图来明确推断目标、假设和权衡，推动证据生成的未来发展。


<details>
  <summary>Details</summary>
Motivation: RCT成本高、周期长、入组标准严格限制了其效能和外部有效性，而RWD虽在建立因果关系上可靠性较低但能生成重要真实世界证据。AI/ML在药物开发中应用增加但面临可解释性和严谨性挑战。

Method: 提出因果路线图方法，明确推断目标、使假设显性化、确保透明度。包括整合证据合成的关键目标：将RCT结果推广到更广泛人群、在RCT中嵌入AI辅助分析、设计混合对照试验、用长期RWD扩展短期RCT。

Result: 整合方法能够产生稳健、透明且政策相关的证据，成为现代监管科学的关键组成部分。

Conclusion: 通过将统计严谨性与AI/ML创新相结合，整合方法可以产生稳健、透明且政策相关的证据，成为现代监管科学的关键组成部分。未来方向包括隐私保护分析、不确定性量化和小样本方法。

Abstract: Randomized controlled trials (RCTs) have been the cornerstone of clinical evidence; however, their cost, duration, and restrictive eligibility criteria limit power and external validity. Studies using real-world data (RWD), historically considered less reliable for establishing causality, are now recognized to be important for generating real-world evidence (RWE). In parallel, artificial intelligence and machine learning (AI/ML) are being increasingly used throughout the drug development process, providing scalability and flexibility but also presenting challenges in interpretability and rigor that traditional statistics do not face. This Perspective argues that the future of evidence generation will not depend on RCTs versus RWD, or statistics versus AI/ML, but on their principled integration. To this end, a causal roadmap is needed to clarify inferential goals, make assumptions explicit, and ensure transparency about tradeoffs. We highlight key objectives of integrative evidence synthesis, including transporting RCT results to broader populations, embedding AI-assisted analyses within RCTs, designing hybrid controlled trials, and extending short-term RCTs with long-term RWD. We also outline future directions in privacy-preserving analytics, uncertainty quantification, and small-sample methods. By uniting statistical rigor with AI/ML innovation, integrative approaches can produce robust, transparent, and policy-relevant evidence, making them a key component of modern regulatory science.

</details>


### [4] [Order Selection in Vector Autoregression by Mean Square Information Criterion](https://arxiv.org/abs/2511.19761)
*Michael Hellstern,Ali Shojaie*

Main category: stat.ME

TL;DR: 提出了一种新的VAR模型阶数选择方法——均方信息准则(MIC)，基于拟合阶数达到或超过真实阶数时预期平方误差损失保持平坦的观察。MIC在相对温和条件下能一致估计过程阶数，在模型误设情况下优于AIC、BIC和HQ准则。


<details>
  <summary>Details</summary>
Motivation: VAR过程在经济学、金融学和生物学中广泛应用，但现有的阶数选择方法都存在缺陷。AIC最小化方法对小维度过程会一致高估真实阶数，而基于BIC或Hannan-Quinn准则的方法需要大样本才能准确估计较大维度过程的阶数。

Method: 基于预期平方误差损失在拟合阶数达到或超过真实阶数时保持平坦的观察，提出了均方信息准则(MIC)。该方法在R包micvar中实现。

Result: 模拟结果显示，在模型误设情况下，MIC相对于AIC、BIC和HQ准则表现更好。在纽约市COVID-19结果预测中进一步证实了这一优势。

Conclusion: MIC是一种有效的VAR模型阶数选择方法，在相对温和条件下能一致估计过程阶数，并在实际应用中表现出优于传统准则的性能。

Abstract: Vector autoregressive (VAR) processes are ubiquitously used in economics, finance, and biology. Order selection is an essential step in fitting VAR models. While many order selection methods exist, all come with weaknesses. Order selection by minimizing AIC is a popular approach but is known to consistently overestimate the true order for processes of small dimension. On the other hand, methods based on BIC or the Hannan-Quinn (HQ) criteria are shown to require large sample sizes in order to accurately estimate the order for larger-dimensional processes. We propose the mean square information criterion (MIC) based on the observation that the expected squared error loss is flat once the fitted order reaches or exceeds the true order. MIC is shown to consistently estimate the order of the process under relatively mild conditions. Our simulation results show that MIC offers better performance relative to AIC, BIC, and HQ under misspecification. This advantage is corroborated when forecasting COVID-19 outcomes in New York City. Order selection by MIC is implemented in the micvar R package available on CRAN.

</details>


### [5] [A Generalized Additive Partial-Mastery Cognitive Diagnosis Model](https://arxiv.org/abs/2511.20191)
*Camilo Cárdenas-Hurtado,Yunxiao Chen,Irini Moustaki*

Main category: stat.ME

TL;DR: 本文提出了广义加性部分掌握认知诊断模型(GaPM-CDM)，通过非参数单调函数建模来放松传统PM-CDMs的强参数假设，提高模型灵活性并降低误设风险。


<details>
  <summary>Details</summary>
Motivation: 传统的部分掌握认知诊断模型(PM-CDMs)虽然比经典CDMs更灵活，但仍继承了关于项目反应函数的强参数假设，存在显著的模型误设风险。

Method: 将每个项目反应函数建模为属性非参数单调函数的混合，结合边际最大似然估计与筛法逼近非参数函数，适用于验证性和探索性两种设置。

Result: 新模型在教育测试和医疗研究两个测量问题中应用，并通过广泛的模拟研究评估，与PM-CDMs相比表现更好。

Conclusion: GaPM-CDM显著放松了PM-CDMs的参数假设，在保持模型简洁性和可解释性的同时，提供了更灵活的建模框架。

Abstract: Cognitive diagnosis models (CDMs) are restricted latent class models widely used for measuring attributes of interest in diagnostic assessments in education, psychology, biomedical sciences, and related fields. Partial-mastery CDMs (PM-CDMs) are an important extension of CDMs. They model individuals' status for each attribute to be continuous for measuring the partial mastery level, which relaxes the restrictive discrete-attribute assumption of classical CDMs. As a result, PM-CDMs often yield better fits for real-world data and refined measurement of the substantive attributes of interest. However, these models inherit some strong parametric assumptions from the traditional CDMs about the item response functions and, thus, still suffer from a significant risk of model misspecification. This paper proposes a generalized additive PM-CDM (GaPM-CDM) that substantially relaxes the parametric assumptions of PM-CDMs. This proposal leverages model parsimony and interpretability by modeling each item response function as a mixture of nonparametric monotone functions of attributes. A method for the estimation of GaPM-CDM is developed, which combines the marginal maximum likelihood estimator with a sieve approximation of the nonparametric functions. The new model is applicable under both confirmatory and exploratory settings, depending on whether prior knowledge is available about the relationship between observed variables and attributes. The proposed method is applied to two measurement problems from educational testing and healthcare research, respectively, and further evaluated and compared with PM-CDMs through extensive simulation studies.

</details>


### [6] [Differentially Private Computation of the Gini Index for Income Inequality](https://arxiv.org/abs/2511.19771)
*Wenjie Lan,Jerome P. Reiter*

Main category: stat.ME

TL;DR: 提出一种基于差分隐私的基尼系数发布方法，通过分析局部敏感度和平滑上界来减少噪声注入误差，相比基于全局敏感度的方法能显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 基尼系数是衡量收入不平等的重要指标，但计算所用的基础数据通常是保密的。发布基尼系数可能泄露底层数据信息，需要保护数据隐私。

Method: 分析单个观测值变化对基尼系数的影响（局部敏感度），推导局部敏感度的平滑上界，基于此定义添加噪声的机制以满足差分隐私要求。

Result: 在模拟和真实收入数据上的实验表明，该方法相比基于全局敏感度的差分隐私算法能显著减少噪声注入误差，在某些场景下能提供高精度估计。

Conclusion: 提出的平滑敏感度方法在特定场景下能提供准确的基尼系数估计，但也存在噪声方差过大导致结果不可靠的情况，同时提供了贝叶斯后处理步骤来改进区间估计。

Abstract: The Gini index is a widely reported measure of income inequality. In some settings, the underlying data used to compute the Gini index are confidential. The organization charged with reporting the Gini index may be concerned that its release could leak information about the underlying data. We present an approach for bounding this information leakage by releasing a differentially private version of the Gini index. In doing so, we analyze how adding, deleting, or altering a single observation in any specific dataset can affect the computation of the Gini index; this is known as the local sensitivity. We then derive a smooth upper bound on the local sensitivity. Using this bound, we define a mechanism that adds noise to the Gini index, thereby satisfying differential privacy. Using simulated and genuine income data, we show that the mechanism can reduce the errors from noise injection substantially relative to differentially private algorithms that rely on the global sensitivity, that is, the maximum of the local sensitivities over all possible datasets. We characterize settings where using smooth sensitivity can provide highly accurate estimates, as well as settings where the noise variance is simply too large to provide reliably useful results. We also present a Bayesian post-processing step that provides interval estimates about the value of the Gini index computed with the confidential data.

</details>


### [7] [Threshold Tensor Factor Model in CP Form](https://arxiv.org/abs/2511.19796)
*Stevenson Bolivar,Rong Chen,Yuefeng Han*

Main category: stat.ME

TL;DR: 提出了一种新的阈值张量因子模型（CP形式），用于张量时间序列分析，通过集成阈值自回归结构来捕捉潜在因子过程中的状态切换动态。


<details>
  <summary>Details</summary>
Motivation: 现有的张量因子模型缺乏对潜在因子过程中状态切换动态的建模能力，需要一种既能保持低秩张量表示的简洁性和可解释性，又能捕捉复杂动态变化的方法。

Method: 将阈值自回归结构集成到CP形式的张量因子模型中，开发了相应的估计程序，并建立了估计量的理论性质。

Result: 数值实验和实际数据应用验证了所提出框架的实用性能和有效性。

Conclusion: 该模型成功地在保持张量因子模型优势的同时，有效捕捉了潜在因子过程中的状态切换动态，具有重要的理论和应用价值。

Abstract: This paper proposes a new Threshold Tensor Factor Model in Canonical Polyadic (CP) form for tensor time series. By integrating a thresholding autoregressive structure for the latent factor process into the tensor factor model in CP form, the model captures regime-switching dynamics in the latent factor processes while retaining the parsimony and interpretability of low-rank tensor representations. We develop estimation procedures for the model and establish the theoretical properties of the resulting estimators. Numerical experiments and a real-data application illustrate the practical performance and usefulness of the proposed framework.

</details>


### [8] [Hierarchical Causal Structure Learning](https://arxiv.org/abs/2511.20021)
*Sjoerd Hermes,Joost van Heerwaarden,Fred van Eeuwijk,Pariya Behrouzi*

Main category: stat.ME

TL;DR: 提出了一种处理多层次数据结构中因果推断的方法，适用于同时存在单元级和组级变量的情况，并开发了相应的R包HSCM。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法主要关注变量间的关联，但许多科学和实际问题需要因果方法。现有方法无法处理层次化或多层次组织的数据，这在农业等领域很常见。

Method: 基于非线性结构因果模型（加性噪声模型），提出了一种方法，能够处理未观察到的混杂因素以及组特定的因果函数。

Result: 开发了R包HSCM来实现该方法，可在CRAN上获取。

Conclusion: 该方法填补了多层次数据结构中因果推断方法的空白，为农业等领域的因果分析提供了实用工具。

Abstract: Traditional statistical approaches primarily aim to model associations between variables, but many scientific and practical questions require causal methods instead. These approaches rely on assumptions about an underlying structure, often represented by a directed acyclic graph (DAG). When all variables are measured at the same level, causal structures can be learned using existing techniques. However, no suitable methods exist when data are organized hierarchically or across multiple levels. This paper addresses such cases, where both unit-level and group-level variables are present. These multi-level structures frequently arise in fields such as agriculture, where plants (units) grow within different environments (groups). Building on nonlinear structural causal models, or additive noise models, we propose a method that accommodates unobserved confounders as well as group-specific causal functions. The approach is implemented in the R package HSCM, available at https://CRAN.R-project.org/package=HSCM.

</details>


### [9] [Heckman Selection Contaminated Normal Model](https://arxiv.org/abs/2409.12348)
*Heeju Lim,Jose Alejandro Ordonez,Victor H. Lachos,Antonio Punzo*

Main category: stat.ME

TL;DR: 本文提出了一种基于二元污染正态分布的新型Heckman选择模型，通过ECM算法进行参数估计，解决了传统模型对正态分布假设的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 传统Heckman选择模型假设误差项服从二元正态分布，但实际数据常存在厚尾和异常观测，导致模型表现不佳。需要更灵活的分布假设来改进模型。

Method: 使用二元污染正态分布作为误差项分布，开发了基于截断多元正态分布公式的ECM算法进行参数估计，并讨论了模型的可识别性。

Result: 通过模拟研究比较了正态分布、t分布和污染正态分布模型，发现新模型在有限样本性质和缺失率变化方面表现更好。两个真实数据分析验证了模型的有效性。

Conclusion: 提出的基于污染正态分布的Heckman选择模型能够更好地处理实际数据中的厚尾和异常观测问题，R包HeckmanEM实现了相关算法。

Abstract: The Heckman selection model is one of the most well-renounced econometric models in the analysis of data with sample selection. This model is designed to rectify sample selection biases based on the assumption of bivariate normal error terms. However, real data diverge from this assumption in the presence of heavy tails and/or atypical observations. Recently, this assumption has been relaxed via a more flexible Student's t-distribution, which has appealing statistical properties. This paper introduces a novel Heckman selection model using a bivariate contaminated normal distribution for the error terms. We present an efficient ECM algorithm for parameter estimation with closed-form expressions at the E-step based on truncated multinormal distribution formulas. The identifiability of the proposed model is also discussed, and its properties have been examined. Through simulation studies, we compare our proposed model with the normal and Student's t counterparts and investigate the finite-sample properties and the variation in missing rate. Results obtained from two real data analyses showcase the usefulness and effectiveness of our model. The proposed algorithms are implemented in the R package HeckmanEM.

</details>


### [10] [Rectangular augmented row-column designs generated from contractions](https://arxiv.org/abs/2511.20052)
*Hans-Peter Piepho,Emlyn Williams*

Main category: stat.ME

TL;DR: 本文提出使用辅助设计（收缩设计）来生成增强的行列设计，通过计算机搜索高效收缩设计来间接获得高效增强设计，显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 在植物育种等应用中，种子数量有限导致每个处理只能安排一个实验单元，需要增强设计来控制行列误差源并估计误差方差。

Method: 利用收缩设计与增强设计效率因子紧密关联的特性，通过计算机搜索高效收缩设计来间接生成高效增强设计。

Result: 提出的方法能够以更高的计算速度找到高效的增强行列设计，并通过两个示例进行了验证。

Conclusion: 使用收缩设计生成增强行列设计的方法计算效率高，适用于种子有限的实验场景，能够有效控制行列误差源。

Abstract: Row-column designs play an important role in applications where two orthogonal sources of error need to be controlled for by blocking. Field or greenhouse experiments, in which experimental units are arranged as a rectangular array of experimental units are a prominent example. In plant breeding, the amount of seed available for the treatments to be tested may be so limited that only one experimental unit per treatment can be accommodated. In such settings, augmented designs become an interesting option, where a small set of treatments, for which sufficient seed is available, are replicated across the rectangular layout so that row and column effects, as well as the error variance can be estimated. Here, we consider the use of an auxiliary design, also known as a contraction, to generate an augmented row-column design. We make use of the fact that the efficiency factors of the contraction and the associated augmented design are closely interlinked. A major advantage of this approach is that an efficient contraction can be found by computer search at much higher computational speed than is required for direct search for an efficient augmented design. Two examples are used to illustrate the proposed method.

</details>


### [11] [Wilcoxon-Mann-Whitney Test of No Group Discrimination](https://arxiv.org/abs/2511.20308)
*Marian Grendar*

Main category: stat.ME

TL;DR: 本文重新审视了Wilcoxon-Mann-Whitney (WMW) 检验的零假设和备择假设，指出传统理解存在错误。WMW实际上检验的是AUC=0.5，而非F=G。推导了标准化U统计量在正确零假设下的渐近分布，并提供了有限样本偏差校正。


<details>
  <summary>Details</summary>
Motivation: 传统对WMW检验零假设的理解过于宽泛，将H0:F=G作为零假设是错误的。需要澄清WMW检验实际检验的是AUC=0.5，并建立正确的理论框架。

Method: 推导标准化U统计量（即经验AUC）在正确零假设下的渐近分布，提供有限样本偏差校正方法，重新定义WMW检验的零假设和备择假设。

Result: 证明了WMW检验实际上检验H0:AUC=0.5，而非传统的H0:F=G。传统理解的备择假设（随机占优）过于狭窄，WMW检验对H1:AUC≠0.5是一致的，这一结果由Van Dantzig在1951年建立。

Conclusion: 需要修正对WMW检验的传统误解，明确其实际检验的是AUC=0.5，并采用正确的零假设和更广泛的备择假设框架。

Abstract: The traditional WMW null hypothesis $H_0: F = G$ is erroneously too broad. WMW actually tests narrower $H_0: AUC = 0.5$. Asymptotic distribution of the standardized $U$ statistic (i.e., the empirical AUC) under the correct $H_0$ is derived along with finite sample bias corrections. The traditional alternative hypothesis of stochastic dominance is too narrow. WMW is consistent against $H_1: AUC \neq 0.5$, as established by Van Dantzig in 1951.

</details>


### [12] [Pseudo-strata learning via maximizing misclassification reward](https://arxiv.org/abs/2511.20318)
*Shanshan Luo,Peng Wu,Zhi Geng*

Main category: stat.ME

TL;DR: 提出了一种基于伪分层的方法来优化在线广告投放，通过利用购买后的收入信息来学习用户伪分层，从而最大化广告收入。


<details>
  <summary>Details</summary>
Motivation: 在线广告中用户对广告曝光的响应存在异质性，但用户的真实响应类型无法直接观测，这限制了广告投放策略的优化效果。

Method: 构建伪分层来分类用户，引入误分类奖励来量化基于伪分层的策略相对于真实分层的预期收入增益，在贝叶斯分类框架下通过优化预期收入来学习伪分层。

Result: 模拟研究显示该方法比基线方法实现了更准确的分层分类和显著更高的收入，并在Criteo预测搜索平台的大规模工业数据集上得到验证。

Conclusion: 该方法能够有效处理用户响应异质性，通过伪分层学习显著提升广告收入，为在线广告优化提供了实用解决方案。

Abstract: Online advertising aims to increase user engagement and maximize revenue, but users respond heterogeneously to ad exposure. Some users purchase only when exposed to ads, while others purchase regardless of exposure, and still others never purchase. This heterogeneity can be characterized by latent response types, commonly referred to as principal strata, defined by users' joint potential outcomes under exposure and non-exposure. However, users' true strata are unobserved, making direct analysis infeasible. In this article, instead of learning the true strata, we propose a novel approach that learns users' pseudo-strata by leveraging information from an outcome (revenue) observed after the response (purchase). We construct pseudo-strata to classify users and introduce misclassification rewards to quantify the expected revenue gain of pseudo-strata-based policies relative to true strata. Within a Bayesian classification framework, we learn the pseudo-strata by optimizing the expected revenue. To implement these procedures, we introduce identification assumptions and estimation methods, and establish their large-sample properties. Simulation studies show that the proposed method achieves more accurate strata classification and substantially higher revenue than baselines. We further illustrate the method using a large-scale industrial dataset from the Criteo Predictive Search Platform.

</details>


### [13] [A novel multi-exposure-to-multi-mediator mediation model for imaging genetic study of brain disorders](https://arxiv.org/abs/2511.20412)
*Neng Wang,Eric V. Slud,Tianzhou Ma*

Main category: stat.ME

TL;DR: 提出了一种新的多暴露-多中介中介模型，整合遗传、神经影像和表型数据来研究"基因-神经影像-脑疾病"的中介通路。该方法通过联合降维和稀疏加载来提高可解释性，并在UK Biobank数据中识别出通过改变特定脑区功能连接影响尼古丁依赖的关键基因。


<details>
  <summary>Details</summary>
Motivation: 常见的心理和脑疾病具有高度遗传性，但遗传因素如何通过改变大脑结构和功能导致这些疾病的机制尚不清楚。传统中介分析在处理大量基因暴露和神经影像中介变量时面临挑战。

Method: 提出多暴露-多中介中介模型，联合将暴露和中介变量降维到低维聚合器中以最大化中介效应。引入稀疏加载提高可解释性，使用交替方向乘子法求解双凸优化问题。

Result: 通过广泛模拟证明，该方法在恢复真实加载和中介比例方面优于其他竞争方法。在UK Biobank数据分析中成功识别出通过改变特定脑区功能连接影响尼古丁依赖的关键基因。

Conclusion: 该方法为整合多组学数据研究复杂脑疾病的中介机制提供了有效工具，能够识别基因通过神经影像中介影响行为表型的关键通路。

Abstract: Common psychiatric and brain disorders are highly heritable and affected by a number of genetic risk factors, yet the mechanism by which these genetic factors contribute to the disorders through alterations in brain structure and function remain poorly understood. Contemporary imaging genetic studies integrate genetic and neuroimaging data to investigate how genetic variation contributes to brain disorders via intermediate neuroimaging endophenotypes. However, the large number of potential exposures (genes) and mediators (neuroimaging features) pose new challenges to the traditional mediation analysis. In this paper, we propose a novel multi-exposure-to-multi-mediator mediation model that integrates genetic, neuroimaging and phenotypic data to investigate the "geneneuroimaging-brain disorder" mediation pathway. Our method jointly reduces the dimensions of exposures and mediators into low-dimensional aggregators where the mediation effect is maximized. We further introduce sparsity into the loadings to improve the interpretability. To target the bi-convex optimization problem, we implement an efficient alternating direction method of multipliers algorithm with block coordinate updates. We provide theoretical guarantees for the convergence of our algorithm and establish the asymptotic properties of the resulting estimators. Through extensive simulations, we demonstrate that our method outperforms other competing methods in recovering true loadings and true mediation proportions across a wide range of signal strengths, noise levels, and correlation structures. We further illustrate the utility of the method through a mediation analysis that integrates genetic, brain functional connectivity and smoking behavior data from UK Biobank, and identifies critical genes that impact nicotine dependence via changing the functional connectivity in specific brain regions.

</details>


### [14] [Extrapolating into the Extremes with Minimum Distance Estimation](https://arxiv.org/abs/2511.20466)
*Alexis Boulin,Erik Haufs*

Main category: stat.ME

TL;DR: 提出一种简化方法，将多元超阈值问题投影到单变量峰值超阈值问题，通过最小化经验分布函数与理论分布之间的L2距离来估计参数，在环境时空极值建模中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决环境时空极值建模中复杂依赖关系和观测外推的关键挑战，简化多元极值分析问题。

Method: 将多元超阈值问题投影到单变量峰值超阈值问题，通过最小化经验分布函数与理论分布之间的L2距离来估计参数，并推导了估计量的渐近性质。

Result: 在EVA 2025会议数据挑战赛中，该方法在六个竞争类别中的两个获得前三名，并在与十个竞争团队的初步挑战中获胜。

Conclusion: 该方法在环境时空极值建模中表现出色，能够有效处理复杂依赖关系并进行观测外推。

Abstract: Understanding complex dependencies and extrapolating beyond observations are key challenges in modeling environmental space-time extremes. To address this, we introduce a simplifying approach that projects a wide range of multivariate exceedance problems onto a univariate peaks-over-threshold problem. In this framework, an estimator is computed by minimizing the $L_2$-distance between the empirical distribution function of the data and the theoretical distribution of the model. Asymptotic properties of this estimator are derived and validated in a simulation study. We evaluated our estimator in the EVA (2025) conference Data Challenge as part of Team Bochum's submission. The challenge provided precipitation data from four runs of LENS2, an ensemble of long-term weather simulations, on a $5 \times 5$ grid of locations centered at the grid point closest to Asheville, NC. Our estimator achieved a top-three rank in two of six competitive categories and won the overall preliminary challenge against ten competing teams.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [15] [An Infinite BART model](https://arxiv.org/abs/2511.20087)
*Marco Battiston,Yu Luo*

Main category: stat.CO

TL;DR: 本文提出了无限BART模型，通过引入二元权重矩阵和印度自助餐过程先验，自动选择决策树数量并允许不同观测簇使用不同的回归函数子集。


<details>
  <summary>Details</summary>
Motivation: 传统BART模型需要预先指定决策树数量，且所有观测使用相同的决策树集合。本文旨在解决这两个限制，实现自动树数量选择和观测特定的回归函数。

Method: 在BART框架中引入二元权重矩阵，使用印度自助餐过程先验，通过MCMC采样器同时估计权重矩阵和其他BART参数。

Result: 在模拟和真实数据集上的比较表明，无限BART模型在变量重要性、偏依赖性和因果估计方面表现优于经典BART。

Conclusion: 无限BART模型成功扩展了传统BART，实现了自动树数量选择和观测特定的回归函数，为回归和分类分析提供了更灵活的框架。

Abstract: Bayesian additive regression trees (BART) are popular Bayesian ensemble models used in regression and classification analysis. Under this modeling framework, the regression function is approximated by an ensemble of decision trees, interpreted as weak learners that capture different features of the data. In this work, we propose a generalization of the BART model that has two main features: first, it automatically selects the number of decision trees using the given data; second, the model allows clusters of observations to have different regression functions since each data point can only use a selection of weak learners, instead of all of them. This model generalization is accomplished by including a binary weight matrix in the conditional distribution of the response variable, which activates only a specific subset of decision trees for each observation. Such a matrix is endowed with an Indian Buffet process prior, and sampled within the MCMC sampler, together with the other BART parameters. We then compare the Infinite BART model with the classic one on simulated and real datasets. Specifically, we provide examples illustrating variable importance, partial dependence and causal estimation.

</details>


### [16] [$MC^2$ Mixed Integer and Linear Programming](https://arxiv.org/abs/2511.20575)
*Nick Polson,Vadim Sokolov*

Main category: stat.CO

TL;DR: 提出MC²算法，将混合整数和线性规划问题重新表述为蒙特卡洛优化问题，通过模拟截断分布（多元指数和高斯分布）来求解约束优化问题。


<details>
  <summary>Details</summary>
Motivation: 将约束优化问题重新表述为从玻尔兹曼分布的模拟问题，从而将整数和线性规划转化为蒙特卡洛优化问题。

Method: 使用MC²算法，通过模拟截断的多元指数和高斯分布来求解优化问题，利用Kent和Davis的算法实现高效模拟。

Result: 在投资组合优化和随机规划中的经典农民问题上验证了该方法的有效性。

Conclusion: 该方法为整数和线性规划提供了新的蒙特卡洛求解思路，并指出了未来研究方向。

Abstract: In this paper, we design $MC^2$ algorithms for Mixed Integer and Linear Programming. By expressing a constrained optimisation as one of simulation from a Boltzmann distribution, we reformulate integer and linear programming as Monte Carlo optimisation problems. The key insight is that solving these optimisation problems requires the ability to simulate from truncated distributions, namely multivariate exponentials and Gaussians. Efficient simulation can be achieved using the algorithms of Kent and Davis. We demonstrate our methodology on portfolio optimisation and the classical farmer problem from stochastic programming. Finally, we conclude with directions for future research.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [17] [Beyond the ACE Score: Replicable Combinations of Adverse Childhood Experiences That Worsen Depression Risk](https://arxiv.org/abs/2511.19574)
*Ruizhe Zhang,Jooyoung Kong,Dylan S. Small,William Bekerman*

Main category: stat.AP

TL;DR: 本文提出了一种数据轮换框架，结合等渗子组选择方法，识别可复制的儿童不良经历组合模式，用于更准确地筛查成人抑郁症高风险人群。


<details>
  <summary>Details</summary>
Motivation: 传统的儿童不良经历单一累加评分方法在个体层面识别能力较差，需要找到可复制的ACEs组合模式来更有效地识别抑郁症高风险人群。

Method: 采用数据轮换框架，结合等渗子组选择方法，在保持家族错误率控制的同时提高统计功效；使用频率编码保留ACEs强度信息；预设风险阈值τ（约2倍抑郁风险增加）。

Result: 在BRFSS 2022验证集上，与ACE评分截断法相比，在相同特异性水平（0.95）下，使用可复制子组作为筛查规则可将敏感性提高26%。

Conclusion: 该方法能够识别出可复制的高风险子组，提供具体可行的筛查触发条件，有助于将有限的临床筛查资源更有效地分配给真正的高风险人群。

Abstract: Adverse childhood experiences (ACEs) are categories of childhood abuse, neglect, and household dysfunction. Screening by a single additive ACE score (e.g., a $\ge 4$ cutoff) has poor individual-level discrimination. We instead identify replicable combinations of ACEs that elevate adult depression risk. Our data turnover framework enables a single research team to explore, confirm, and replicate within one observational dataset while controlling the family-wise error rate. We integrate isotonic subgroup selection (ISS) to estimate a higher-risk subgroup under a monotonicity assumption -- additional ACE exposure or higher intensity cannot reduce depression risk. We pre-specify a risk threshold $τ$ corresponding to roughly a two-fold increase in the odds of depression relative to the no-ACE baseline. Within data turnover, the prespecified component improves power while maintaining FWER control, as demonstrated in simulations. Guided by EDA, we adopt frequency coding for ACE items, retaining intensity information that reduces false positives relative to binary or score codings. The result is a replicable, pattern-based higher-risk subgroup. On held-out BRFSS 2022, we show that, at the same level of specificity (0.95), using our replicable subgroup as the screening rule increases sensitivity by 26\% compared with an ACE-score cutoff, yielding concrete triggers that are straightforward to implement and help target scarce clinical screening resources toward truly higher-risk profiles.

</details>


### [18] [A Win-Expectancy Framework for Contextualizing Runs Batted In: Introducing ARBI and CRBI](https://arxiv.org/abs/2511.19642)
*Wuhuan Deng*

Main category: stat.AP

TL;DR: 本文提出了两个新的上下文感知指标ARBI和CRBI，通过结合胜率期望来改进传统RBI统计的局限性，更准确地衡量击球员的进攻贡献。


<details>
  <summary>Details</summary>
Motivation: 传统RBI统计将所有打点得分视为同等重要，忽略了比赛情境（如杠杆率、比分状态和得分对胜率的影响），无法反映得分对比赛结果的实际影响。

Method: ARBI根据得分事件前后胜率期望的变化来重新调整每个RBI的价值，为在关键时刻得分赋予更高权重；CRBI进一步考虑事件结束时的终末胜率期望，区分相同胜率变化但竞争影响不同的RBI。

Result: ARBI和CRBI提供了经过校准的、上下文敏感的进攻贡献度量，能更准确地反映得分生产的真实价值。

Conclusion: ARBI和CRBI现代化了RBI的解释，在球员评估、预测、合同评估和棒球分析决策中具有广泛应用前景。

Abstract: Runs Batted IN (RBI) records the number of runs a hitter directly drives in during their plate appearances and reflects a batter's ability to convert opportunities into scoring. Because producing runs determines game outcomes, RBI has long served as a central statistic in evaluating offensive performance. However, traditional RBI treats all batted-in runs equally and ignores th game context in which they occur, such as leverage, score state, and the actual impact of a run on a team's chance of winning. In this paper, we introduce two new context-aware metrics-Adjusted RBI (ARBI) and Contextual RBI (CRBI)-that address the fundamental limitations of RBI by incorporating Win Expectancy (WE). ARBI rescales each RBI according to the change in WE before and after the scoring event, assigning more value to runs that meaningfully shift the likelihood of winning and less to runs scored in low-leverage situations. We then extend this framework to CRBI, which further differentiates RBIs with the same WE change by accounting for the terminal WE at the end of the event. This refinement captures the idea that an RBI increasing WE from, for example, 0.45 to 0.65 has a larger competitive impact than one increasing WE from 0.05 to 0.25, even though both represent a 20% increase. Together, ARBI and CRBI provide calibrated, context-sensitive measures of offensive contribution that more accurately reflect the true value of run production. These metrics modernize the interpretation of RBI and have broad applications in player evaluation, forecasting, contract evaluation, and decision-making in baseball analytics.

</details>


### [19] [Introducing Discipline Score Based on League Overall Swinging Probability](https://arxiv.org/abs/2511.19672)
*Wuhuan Deng,Scott Nestler*

Main category: stat.AP

TL;DR: 提出了两个新的击球手纪律性评估指标：纪律分数（DS）和调整后纪律分数（ADS），用于衡量击球手在面对坏球时的纪律性表现。


<details>
  <summary>Details</summary>
Motivation: 现有指标大多无法准确捕捉击球手放弃坏球的能力，而识别好球和放弃坏球是击球手成功的重要特征。

Method: 通过比较击球手在面对坏球时的表现与联盟所有击球手的预测趋势，开发了DS和ADS两个新指标。

Result: 引入了能够更准确评估击球手纪律性的新量化方法。

Conclusion: DS和ADS指标能够更好地评估击球手在面对坏球时的纪律性，弥补了现有指标的不足。

Abstract: Plate discipline is an important feature of a hitter's success. Hitter who are able to recognize good pitches to swing at and balls to take are generally recognized as disciplined hitters. Although there are some metrics that can provide insight into the patience of a hitter, most do not capture the ability of a batter to take balls. In this research, we introduce two new metrics, Discipline Score (DS) and Adjusted Discipline Score (ADS), which evaluate batters' discipline when the pitch is a ball compared with the predicted tendencies of all batters in the league.

</details>


### [20] [Anchoring Convenience Survey Samples to a Baseline Census for Vaccine Coverage Monitoring in Global Health](https://arxiv.org/abs/2511.19742)
*Nathaniel Dyrkton,Shomoita Alam,Susan Shepherd,Ibrahim Sana,Kevin Phelan,Jay JH Park*

Main category: stat.AP

TL;DR: 该研究评估了使用混合锚定调查方法（将非概率随访调查与概率基线普查相结合）来估计麻疹疫苗第一剂接种覆盖率的可行性，通过模拟研究分析了不同选择偏差程度、村庄抽样比例和调查响应率对估计性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统概率调查在评估疫苗覆盖率时面临实施挑战，特别是在全球卫生领域需要更经济实用的替代方案。研究受乍得和尼日尔农村地区儿童疫苗监测项目的启发，旨在开发更便捷的疫苗覆盖率评估方法。

Method: 采用模拟研究方法，评估基于校准加权设计的方法和基于逻辑回归的插补估计方法，这些方法使用混合方法将非概率随访调查锚定到概率基线普查，以解决选择偏差问题。研究探索了不同选择偏差程度、村庄抽样比例和调查响应率的影响。

Result: 在最具挑战性的情景下（选择偏差OR=1.5，25%村庄抽样，50%调查响应率），两种方法都显示出2.1%或更低的经验偏差，低于95%的覆盖率，以及较低的等效容忍度。在更现实的情景下（OR≤1.2），两种方法都表现出高性能，除了在最低的村庄抽样和参与率情况下。

Conclusion: 模拟研究表明，混合锚定调查方法是疫苗监测的可行调查选项，特别是在选择偏差较小的情况下表现良好。

Abstract: While conducting probabilistic surveys is the gold standard for assessing vaccine coverage, implementing these surveys poses challenges for global health. There is a need for more convenient option that is more affordable and practical. Motivated by childhood vaccine monitoring programs in rural areas of Chad and Niger, we conducted a simulation study to evaluate calibration-weighted design-based and logistic regression-based imputation estimators of the finite-population proportion of MCV1 coverage. These estimators use a hybrid approach that anchors non-probabilistic follow-up survey to probabilistic baseline census to account for selection bias. We explored varying degrees of non-ignorable selection bias (odds ratios from 1.0-1.5), percentage of villages sampled (25-75%), and village-level survey response rate to the follow-up survey (50-80%). Our performance metrics included bias, coverage, and proportion of simulated 95% confidence intervals falling within equivalence margins of 5% and 7.5% (equivalence tolerance). For both adjustment methods, the performance worsened with higher selection bias and lower response rate and generally improved as a larger proportion of villages was sampled. Under the worst scenario with 1.5 OR, 25% village sampled, and 50% survey response rate, both methods showed empirical biases of 2.1% or less, below 95% coverage, and low equivalence tolerances. In more realistic scenarios, the performance of our estimators showed lower biases and close to 95% coverage. For example, at OR$\leq$1.2, both methods showed high performance, except at the lowest village sampling and participation rates. Our simulations show that a hybrid anchoring survey approach is a feasible survey option for vaccine monitoring.

</details>


### [21] [Non-stationarities in extreme hourly precipitation over the Piave Basin, northern Italy](https://arxiv.org/abs/2511.20069)
*Dáire Healy,Ilaria Prosdocimi,Isadora Antoniano-Villalobos*

Main category: stat.AP

TL;DR: 该研究分析了意大利皮亚韦河流域极端小时降水的时空特征，发现极端降水的边际和依赖结构存在季节性模式，且空间依赖随事件极端程度增加而减弱。研究比较了多种协变量依赖模型，发现仅对边际建模无法完全捕捉极端降水变异性，需要同时考虑极值依赖的季节变化。


<details>
  <summary>Details</summary>
Motivation: 准确描述极端降水过程的时空特征对于更好估计相关风险至关重要。现有研究在捕捉极端降水的边际和依赖结构季节性变化方面存在不足。

Method: 使用观测小时降雨数据，识别不同时空尺度的气候协变量，比较新近提出的协变量依赖模型在边际和依赖结构建模中的性能，采用灵活的max-id模型分析极端水平降雨过程的时空变异性。

Result: 发现极端降水的边际和依赖结构均呈现季节性模式，空间依赖随事件极端程度增加而减弱。仅对边际建模无法完全捕捉极端降水变异性，必须同时考虑极值依赖的季节变化。

Conclusion: 为提供极端降水过程的现实描述，需要同时建模边际和依赖结构的非平稳性，特别是捕捉极值依赖的季节变化，这对准确估计极端降水风险至关重要。

Abstract: We study the spatio-temporal features of extremal sub-daily precipitation data over the Piave river basin in northeast Italy using a rich database of observed hourly rainfall. Empirical evidence suggests that both the marginal and dependence structures for extreme precipitation in the area exhibit seasonal patterns, and spatial dependence appears to weaken as events become more extreme. We investigate factors affecting the marginal distributions, the spatial dependence and the interplay between them. Capturing these features is essential to provide a realistic description of extreme precipitation processes in order to better estimate their associated risks. With this aim, we identify various climatic covariates at different spatio-temporal scales and explore their usefulness. We go beyond existing literature by investigating and comparing the performance of recently proposed covariate-dependent models for both the marginal and dependence structures of extremes. Furthermore, a flexible max-id model, which encompasses both asymptotic dependence and independence, is used to learn about the spatio-temporal variability of rainfall processes at extreme levels. We find that modelling non-stationarity only at the marginal level does not fully capture the variability of precipitation extremes, and that it is important to also capture the seasonal variation of extremal dependence.

</details>


### [22] [Efficient multi-fidelity Gaussian process regression for noisy outputs and non-nested experimental designs](https://arxiv.org/abs/2511.20183)
*Nils Baillie,Baptiste Kerleguer,Cyril Feau,Josselin Garnier*

Main category: stat.AP

TL;DR: 提出了一种多保真度高斯过程代理建模方法，适用于噪声和非嵌套数据，通过EM算法估计参数，并推导了缩放因子为参数线性预测函数时的闭式更新公式。


<details>
  <summary>Details</summary>
Motivation: 解决高保真度和低保真度数据集存在噪声且不一定嵌套时的多保真度建模问题，提高参数估计的效率和可扩展性。

Method: 使用多保真度高斯过程代理建模，推广自回归模型的递归公式，通过EM算法估计高保真度参数，并推导缩放因子为参数线性预测函数时的闭式更新公式。

Result: 提出了一种解耦优化策略，比直接最大似然最大化更高效和可扩展，并在不同复杂度的应用案例中进行了基准测试。

Conclusion: 该方法在多保真度建模中表现出高效性和可扩展性，适用于噪声和非嵌套数据场景。

Abstract: This paper presents a multi-fidelity Gaussian process surrogate modeling that generalizes the recursive formulation of the auto-regressive model when the high-fidelity and low-fidelity data sets are noisy and not necessarily nested. The estimation of high-fidelity parameters by the EM (expectation-maximization) algorithm is shown to be still possible in this context and a closed-form update formula is derived when the scaling factor is a parametric linear predictor function. This yields a decoupled optimization strategy for the parameter selection that is more efficient and scalable than the direct maximum likelihood maximization. The proposed approach is compared to other multi-fidelity models, and benchmarks for different application cases of increasing complexity are provided.

</details>


### [23] [Investigating access to support centers for Violence Against Women in Apulia: A Spatial analysis over multiple years](https://arxiv.org/abs/2511.20481)
*Leonardo Cefalo,Crescenza Calculli,Alessio Pollice*

Main category: stat.AP

TL;DR: 本文提出贝叶斯时空泊松回归模型分析意大利南部地区针对女性的暴力空间变异性，发现支持服务可达性、教育水平和经济发展对暴力报告和发生率有显著影响。


<details>
  <summary>Details</summary>
Motivation: 解决意大利南部地区针对女性暴力的空间变异性建模挑战，研究市政层面社会经济特征和当地脆弱性对性别暴力发生率和报告的影响。

Method: 使用贝叶斯时空泊松回归模型，在INLA框架内比较四种空间模型，评估模型拟合度、先验假设、空间混淆效应和推断意义。

Result: 支持服务可达性随距离增加而降低，教育水平较低地区存在报告不足，经济发展水平较高地区报告暴力发生率较低。

Conclusion: 空间建模在捕捉报告动态和为政策干预提供信息方面具有关键作用。

Abstract: In this study, we address the challenge of modelling the spatial variability in violence against women across municipalities in a Southern Italian region by proposing a Bayesian spatio-temporal Poisson regression model. Using data on access to Local Anti-Violence Centers in the Apulia region from 2021 to 2024, we investigate the impact of municipality-level socioeconomic characteristics and local vulnerabilities on both the incidence and reporting of gender-based violence. To explicitly account for spatial dependence, we compare four spatial models within the Integrated Nested Laplace Approximation framework for Bayesian model estimation. We assess the relative fit of the competing models, discussing their prior assumptions, spatial confounding effects, and inferential implications. Our findings indicate that access to support services decreases with distance from the residential municipality, highlighting spatial constraints in reporting and the strategic importance of support center location. Furthermore, lower education levels appear to contribute to under-reporting in disadvantaged areas, while higher economic development may be associated with a lower incidence of reported violence. This study emphasises the critical role of spatial modelling in capturing reporting dynamics and informing policy interventions.

</details>


### [24] [Discovering Spatial Patterns of Readmission Risk Using a Bayesian Competing Risks Model with Spatially Varying Coefficients](https://arxiv.org/abs/2511.20616)
*Yueming Shen,Christian Pean,David Dunson,Samuel Berchuck*

Main category: stat.AP

TL;DR: 提出了一种贝叶斯竞争风险比例风险模型，引入点参考空间效应，使用高斯过程先验和Hilbert空间低秩近似来提高计算效率，并应用基于损失的聚类方法识别高风险区域。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录研究中需要考虑患者地理位置的社会健康决定因素，现有方法缺乏对空间效应的有效建模。

Method: 使用高斯过程先验建模空间变化的截距和斜率，采用Hilbert空间低秩近似处理大量空间位置，基线风险曲线建模为分段常数，引入新型乘法伽马过程先验进行收缩和平滑。

Result: 通过模拟和杜克医院老年上肢骨折患者再入院风险的真实数据分析，证明该方法提高了推断效率并为下游政策决策提供了有价值见解。

Conclusion: 所提出的方法在竞争风险模型中有效整合了空间效应，为基于位置的健康风险分析提供了实用工具。

Abstract: Time-to-event models are commonly used to study associations between risk factors and disease outcomes in the setting of electronic health records (EHR). In recent years, focus has intensified on social determinants of health, highlighting the need for methods that account for patients' locations. We propose a Bayesian approach for introducing point-referenced spatial effects into a competing risks proportional hazards model. Our method leverages Gaussian process (GP) priors for spatially varying intercept and slope. To improve computational efficiency under a large number of spatial locations, we implemented a Hilbert space low-rank approximation of the GP. We modeled the baseline hazard curves as piecewise constant, and introduced a novel multiplicative gamma process prior to induce shrinkage and smoothing. A loss-based clustering method was then used on the spatial random effects to identify high-risk regions. We demonstrate the utility of this method through simulation and a real-world analysis of EHR data from Duke Hospital to study readmission risk of elderly patients with upper extremity fractures. Our results showed that the proposed method improved inference efficiency and provided valuable insights for downstream policy decisions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [25] [FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection](https://arxiv.org/abs/2511.19476)
*Jin Cui,Boran Zhao,Jiajun Xu,Jiaqi Guo,Shuo Guan,Pengju Ren*

Main category: stat.ML

TL;DR: FAST是一个基于谱图理论和特征函数距离的无DNN分布匹配核心集选择框架，通过渐进式采样策略显著提升性能，在准确率、能耗和速度方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法要么依赖DNN参数引入架构偏差，要么依赖缺乏理论保证的启发式方法，且都无法显式约束分布等价性，主流度量指标也无法准确捕捉高阶矩差异。

Method: 将核心集选择建模为图约束优化问题，使用特征函数距离在频域捕获完整分布信息，提出衰减相位解耦CFD解决梯度消失问题，并设计渐进式差异感知采样策略从低频到高频调度。

Result: 在所有基准测试中显著优于现有方法，平均准确率提升9.12%，功耗降低96.57%，速度提升2.2倍。

Conclusion: FAST框架通过频域分布匹配和渐进采样策略，实现了高效、节能的核心集选择，为大规模深度学习训练提供了实用的压缩解决方案。

Abstract: Coreset selection compresses large datasets into compact, representative subsets, reducing the energy and computational burden of training deep neural networks. Existing methods are either: (i) DNN-based, which are tied to model-specific parameters and introduce architectural bias; or (ii) DNN-free, which rely on heuristics lacking theoretical guarantees. Neither approach explicitly constrains distributional equivalence, largely because continuous distribution matching is considered inapplicable to discrete sampling. Moreover, prevalent metrics (e.g., MSE, KL, MMD, CE) cannot accurately capture higher-order moment discrepancies, leading to suboptimal coresets. In this work, we propose FAST, the first DNN-free distribution-matching coreset selection framework that formulates the coreset selection task as a graph-constrained optimization problem grounded in spectral graph theory and employs the Characteristic Function Distance (CFD) to capture full distributional information in the frequency domain. We further discover that naive CFD suffers from a "vanishing phase gradient" issue in medium and high-frequency regions; to address this, we introduce an Attenuated Phase-Decoupled CFD. Furthermore, for better convergence, we design a Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high, preserving global structure before refining local details and enabling accurate matching with fewer frequencies while avoiding overfitting. Extensive experiments demonstrate that FAST significantly outperforms state-of-the-art coreset selection methods across all evaluated benchmarks, achieving an average accuracy gain of 9.12%. Compared to other baseline coreset methods, it reduces power consumption by 96.57% and achieves a 2.2x average speedup, underscoring its high performance and energy efficiency.

</details>


### [26] [Optimization and Regularization Under Arbitrary Objectives](https://arxiv.org/abs/2511.19628)
*Jared N. Lakhani,Etienne Pienaar*

Main category: stat.ML

TL;DR: 该研究探讨了MCMC方法在任意目标函数应用中的局限性，重点关注两区块MCMC框架的性能对似然函数锐度的依赖性，并通过强化学习任务验证了理论分析。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示MCMC方法在任意目标函数应用中的局限性，特别是两区块MCMC框架的性能如何受似然函数锐度的影响，这对于理解数据驱动正则化的有效性至关重要。

Method: 采用两区块MCMC框架，交替使用Metropolis-Hastings和Gibbs采样，引入锐度参数探索不同似然函数形式，并在导航问题和井字棋等强化学习任务中进行实证分析。

Result: 研究表明似然曲率同时影响样本内性能和训练数据推断的正则化程度，在极端锐度情况下，后验质量会坍缩到单一主导模式。

Conclusion: MCMC方法在任意目标函数上的应用效果严重依赖于似然函数的锐度，过度锐化的似然会导致后验坍缩，而混合方法能达到与原MCMC框架相似的性能。

Abstract: This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.

</details>


### [27] [Clustering Approaches for Mixed-Type Data: A Comparative Study](https://arxiv.org/abs/2511.19755)
*Badih Ghattas,Alvaro Sanchez San-Benito*

Main category: stat.ML

TL;DR: 该研究比较了混合类型数据聚类的多种方法，包括距离基方法（k-prototypes、PDQ、凸k-means）和概率方法（KAMILA、MBNs、LCM），通过模拟实验评估它们在各种场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 混合类型数据的聚类仍然是一个挑战，因为很少有现有方法适合此任务。本研究旨在提供对不同方法在各种场景下行为的见解。

Method: 使用各种模拟模型比较混合类型数据聚类方法，通过改变实验因素如聚类数量、聚类重叠度、样本大小、维度、连续变量比例和聚类分布来评估性能。

Result: 聚类重叠度、连续变量比例和样本大小对性能有显著影响。当变量间存在强交互作用且明确依赖聚类成员关系时，所有评估方法都表现不佳。KAMILA、LCM和k-prototypes在调整兰德指数方面表现最佳。

Conclusion: KAMILA、LCM和k-prototypes在混合类型数据聚类中表现最好，但所有方法在变量间存在强交互作用且依赖聚类成员关系时都难以获得满意结果。所有方法都可在R语言中使用。

Abstract: Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R.

</details>


### [28] [A Fully Probabilistic Tensor Network for Regularized Volterra System Identification](https://arxiv.org/abs/2511.20457)
*Afra Kilic,Kim Batselier*

Main category: stat.ML

TL;DR: BTN-V是一种贝叶斯张量网络Volterra核机，通过规范多分解表示Volterra核，将模型复杂度从O(I^D)降低到O(DIR)，并提供预测不确定性估计。


<details>
  <summary>Details</summary>
Motivation: Volterra级数建模非线性系统时，核系数数量随模型阶数呈指数增长，这带来了计算挑战。

Method: 使用规范多分解表示Volterra核，将所有张量分量和超参数视为随机变量，采用稀疏诱导分层先验实现自动秩确定和衰落记忆行为学习。

Result: 实证结果显示了竞争力的准确性、增强的不确定性量化和降低的计算成本。

Conclusion: BTN-V框架在保持准确性的同时，显著降低了模型复杂度，并提供了更好的可解释性和过拟合预防能力。

Abstract: Modeling nonlinear systems with Volterra series is challenging because the number of kernel coefficients grows exponentially with the model order. This work introduces Bayesian Tensor Network Volterra kernel machines (BTN-V), extending the Bayesian Tensor Network framework to Volterra system identification. BTN-V represents Volterra kernels using canonical polyadic decomposition, reducing model complexity from O(I^D) to O(DIR). By treating all tensor components and hyperparameters as random variables, BTN-V provides predictive uncertainty estimation at no additional computational cost. Sparsity-inducing hierarchical priors enable automatic rank determination and the learning of fading-memory behavior directly from data, improving interpretability and preventing overfitting. Empirical results demonstrate competitive accuracy, enhanced uncertainty quantification, and reduced computational cost.

</details>


### [29] [Generative Modeling with Manifold Percolation](https://arxiv.org/abs/2511.20503)
*Rui Tong*

Main category: stat.ML

TL;DR: 该论文提出使用连续渗流理论分析生成模型的几何支撑，建立随机几何图的拓扑相变与数据流形之间的同构关系，并开发可微损失函数防止流形收缩。


<details>
  <summary>Details</summary>
Motivation: 从观察者视角，生成建模任务表现为从概率分布中解耦几何支撑，需要新的方法来分析生成模型的结构病理问题。

Method: 利用连续渗流理论将高维密度估计投影为支撑上的几何计数问题，建立渗流偏移度量，并将其转化为可微损失函数指导训练。

Result: 实验证明该方法不仅防止流形收缩，还推动模型达到"超泛化"状态，具有良好的保真度和验证的拓扑扩展。

Conclusion: 连续渗流为生成模型的支撑分析提供了独特视角，渗流偏移度量能捕捉统计度量无法检测的结构病理，可微损失函数能有效指导训练过程。

Abstract: Generative modeling is typically framed as learning mapping rules, but from an observer's perspective without access to these rules, the task manifests as disentangling the geometric support from the probability distribution. We propose that Continuum Percolation is uniquely suited for this support analysis, as the sampling process effectively projects high-dimensional density estimation onto a geometric counting problem on the support. In this work, we establish a rigorous isomorphism between the topological phase transitions of Random Geometric Graphs and the underlying data manifold in high-dimensional space. By analyzing the relationship between our proposed Percolation Shift metric and FID, we demonstrate that our metric captures structural pathologies (such as implicit mode collapse) where statistical metrics fail. Finally, we translate this topological phenomenon into a differentiable loss function to guide training. Experimental results confirm that this approach not only prevents manifold shrinkage but drives the model toward a state of "Hyper-Generalization," achieving good fidelity and verified topological expansion.

</details>


### [30] [Spatio-Temporal Hierarchical Causal Models](https://arxiv.org/abs/2511.20558)
*Xintong Li,Haoran Zhang,Xiao Zhou*

Main category: stat.ML

TL;DR: 提出了时空分层因果模型（ST-HCMs），这是一个新的图模型框架，用于处理时空数据中的因果推断问题，特别是在存在未观测的单元级混杂因素时。


<details>
  <summary>Details</summary>
Motivation: 现有的时空因果推断方法通常假设所有混杂因素都被观测到，但在实践中这一假设经常被违反。特别是存在未观测的、随时间不变的单元级混杂因素时，标准非分层模型会失效。

Method: 引入了时空分层因果模型（ST-HCMs），扩展了分层因果建模到时空领域。核心是时空折叠定理，表明随着子单元数据量的增加，复杂的ST-HCM会收敛到更简单的平面因果模型。

Result: 在合成和真实世界数据集上验证了该框架的有效性，证明了其在复杂动态系统中进行稳健因果推断的潜力。

Conclusion: ST-HCMs能够恢复因果效应，即使在存在未观测的、随时间不变的单元级混杂因素的情况下，而标准的非分层模型在这种情况下会失败。

Abstract: The abundance of fine-grained spatio-temporal data, such as traffic sensor networks, offers vast opportunities for scientific discovery. However, inferring causal relationships from such observational data remains challenging, particularly due to unobserved confounders that are specific to units (e.g., geographical locations) yet influence outcomes over time. Most existing methods for spatio-temporal causal inference assume that all confounders are observed, an assumption that is often violated in practice. In this paper, we introduce Spatio-Temporal Hierarchical Causal Models (ST-HCMs), a novel graphical framework that extends hierarchical causal modeling to the spatio-temporal domain. At the core of our approach is the Spatio-Temporal Collapse Theorem, which shows that a complex ST-HCM converges to a simpler flat causal model as the amount of subunit data increases. This theoretical result enables a general procedure for causal identification, allowing ST-HCMs to recover causal effects even in the presence of unobserved, time-invariant unit-level confounders, a scenario where standard non-hierarchical models fail. We validate the effectiveness of our framework on both synthetic and real-world datasets, demonstrating its potential for robust causal inference in complex dynamic systems.

</details>
