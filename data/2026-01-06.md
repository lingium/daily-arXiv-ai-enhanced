<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 3]
- [stat.ME](#stat.ME) [Total: 10]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 3]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference](https://arxiv.org/abs/2601.00038)
*Shane A. McQuarrie,Mengwu Guo,Anirban Chaudhuri*

Main category: stat.ML

TL;DR: 本文提出了一种用于参数化动力系统数据驱动降阶模型（ROM）的主动学习框架，通过贝叶斯线性回归建立概率模型，利用预测不确定性设计自适应采样策略，相比随机采样能获得更稳定准确的ROM。


<details>
  <summary>Details</summary>
Motivation: 数据驱动降阶模型作为数字孪生的虚拟资产基础，其质量受限于训练数据。由于ROM对训练数据质量敏感，需要识别能产生最佳参数化ROM的训练参数，以提升模型在参数域内的全局稳定性和准确性。

Method: 采用算子推断方法，建立参数化算子推断的概率版本，将学习问题转化为贝叶斯线性回归。利用概率ROM解产生的预测不确定性，设计顺序自适应采样方案来选择新的训练参数向量，促进ROM在参数域内的全局稳定性和准确性。

Result: 对多个非线性参数化偏微分方程系统进行数值实验，结果表明在相同计算预算下，提出的自适应采样策略相比随机采样能持续产生更稳定、更准确的ROM。

Conclusion: 该主动学习框架通过智能丰富数据驱动降阶模型的训练数据，有效提升了参数化ROM的质量，为数字孪生应用提供了更可靠的虚拟资产基础。

Abstract: This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget.

</details>


### [2] [Detecting Unobserved Confounders: A Kernelized Regression Approach](https://arxiv.org/abs/2601.00200)
*Yikai Chen,Yunxin Mao,Chunyuan Zheng,Hao Zou,Shanzhi Gu,Shixuan Liu,Yang Shi,Wenjing Yang,Kun Kuang,Haotian Wang*

Main category: stat.ML

TL;DR: 提出KRCD方法，用于检测非线性单环境观测数据中的未观测混杂因素，通过比较标准和高阶核回归来识别未观测混杂


<details>
  <summary>Details</summary>
Motivation: 现有方法需要线性假设或多个异质环境，限制了在非线性单环境设置中的应用，需要填补这一空白

Method: 利用再生核希尔伯特空间建模复杂依赖关系，通过比较标准和高阶核回归推导检验统计量，显著偏离零值表示存在未观测混杂

Result: 理论证明：无限样本中回归系数相等当且仅当无未观测混杂；有限样本差异收敛到零均值高斯分布。实验表明KRCD优于现有基线且计算效率更高

Conclusion: KRCD是检测非线性单环境观测数据中未观测混杂的有效方法，具有理论保证和实际优势

Abstract: Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency.

</details>


### [3] [Generative Conditional Missing Imputation Networks](https://arxiv.org/abs/2601.00517)
*George Sun,Yi-Hui Zhou*

Main category: stat.ML

TL;DR: 提出生成式条件缺失值插补网络(GCMI)，通过链式方程多重插补增强鲁棒性，在MCAR和MAR机制下表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 数据集中的缺失值问题是统计分析中的重要挑战，需要开发更有效的插补方法来提高数据质量和分析准确性

Method: 1. 提出生成式条件缺失值插补网络(GCMI)的理论框架；2. 通过链式方程多重插补方法增强GCMI的鲁棒性和准确性；3. 在MCAR和MAR缺失机制下验证方法有效性

Result: 通过模拟实验和基准数据集评估，证明GCMI方法在缺失值插补方面优于现有领先技术，具有更好的稳定性和性能表现

Conclusion: GCMI方法不仅具有实际应用价值，而且有潜力成为统计数据分析领域的先进工具，为缺失值处理提供有效解决方案

Abstract: In this study, we introduce a sophisticated generative conditional strategy designed to impute missing values within datasets, an area of considerable importance in statistical analysis. Specifically, we initially elucidate the theoretical underpinnings of the Generative Conditional Missing Imputation Networks (GCMI), demonstrating its robust properties in the context of the Missing Completely at Random (MCAR) and the Missing at Random (MAR) mechanisms. Subsequently, we enhance the robustness and accuracy of GCMI by integrating a multiple imputation framework using a chained equations approach. This innovation serves to bolster model stability and improve imputation performance significantly. Finally, through a series of meticulous simulations and empirical assessments utilizing benchmark datasets, we establish the superior efficacy of our proposed methods when juxtaposed with other leading imputation techniques currently available. This comprehensive evaluation not only underscores the practicality of GCMI but also affirms its potential as a leading-edge tool in the field of statistical data analysis.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [4] [An exact unbiased semi-parametric L2 quasi-likelihood framework, complete in the presence of ties](https://arxiv.org/abs/2601.00188)
*Landon Hurley*

Main category: stat.ME

TL;DR: 提出基于ℓ₂范数的拟似然框架，通过二项比较构建精确无偏的线性回归Hájek投影估计，扩展Wilcoxon秩和检验以处理多协变量，解决有限样本下的精确无偏性问题。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然估计需要先验确定误差分布才能保证精确无偏性，而协方差矩阵估计需要特定结构和正则约束。现有Wilcoxon秩和检验框架在处理多协变量时无法保证有限样本下的精确无偏性，特别是在存在线性满射映射到共同点的情况下。

Method: 构建ℓ₂范数拟似然框架，通过所有对(Xₙ,Yₙ)的二项比较进行识别。利用Kemeny度量空间和Whitney嵌入实现精确无偏最小方差多元协方差估计，扩展Wilcoxon秩和检验框架处理多协变量，形成一致的非参数广义线性模型形式。

Result: 开发了在离散和连续随机变量上都能实现精确无偏最小方差多元协方差估计的方法，解决了有限样本下存在结(ties)时的精确无偏识别问题，扩展了Wilcoxon秩和检验以处理多协变量。

Conclusion: 该框架将拟似然方法扩展到一致的非参数广义线性模型形式，能够处理未知异质性和弱推断工具问题，为科学研究和预测分析提供了更稳健的协方差估计方法。

Abstract: Maximum likelihood style estimators possesses a number of ideal characteristics, but require prior identification of the distribution of errors to ensure exact unbiasedness. Independent of the focus of the primary statistical analysis, the estimation of a covariance matrix \(S^{P \times P}\approx Σ^{P \times P}\) must possess a specific structure and regularity constraints. The need to estimate a linear Gaussian covariance models appear in various applications as a formal precondition for scientific investigation and predictive analytics. In this work, we construct an \(\ell_{2}\)-norm based quasi-likelihood framework, identified by binomial comparisons between all pairs \(X_{n},Y_{n}, \forall {n}\). Our work here focuses upon the quasi-likelihood basis for estimation of an exactly unbiased linear regression Hájek projection, within which the Kemeny metric space is operationalised via Whitney embedding to obtain exact unbiased minimum variance multivariate covariance estimators upon both discrete and continuous random variables (i.e., exact unbiased identification in the presence of ties upon finite samples). While the covariance estimator is inherently useful, expansion of the Wilcoxon rank-sum testing framework to handle multiple covariates with exact unbiasedness upon finite samples is a currently unresolved research problem, as it maintains identification in the presence of linear surjective mappings onto common points: this model space, by definition, expands our likelihood framework into a consistent non-parametric form of the standard general linear model, which we extend to address both unknown heterogeneity and the problem of weak inferential instruments.

</details>


### [5] [Deep learning estimation of the spectral density of functional time series on large domains](https://arxiv.org/abs/2601.00284)
*Neda Mohammadi,Soham Sarkar,Piotr Kokoszka*

Main category: stat.ME

TL;DR: 提出一种基于多层感知机神经网络的功能时间序列谱密度估计器，解决了传统方法在大规模网格（如气候模型和医学扫描）上计算成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有功能时间序列谱密度估计器需要计算大型G×G自协方差核矩阵，当函数定义在2D/3D域且网格点G~10^5时，计算成本过高甚至不可行，这在气候计算模型和医学扫描等应用中尤为突出。

Method: 利用谱功能主成分理论，构建基于多层感知机神经网络的深度学习方法。该方法无需计算自协方差核，可并行化训练，在一般假设下被证明是谱密度的通用逼近器。

Result: 新估计器在计算效率上显著优于传统方法，能够处理大规模网格数据。通过模拟实验和fMRI图像应用验证了其性能。

Conclusion: 提出的深度学习估计器为大规模功能时间序列谱密度估计提供了高效可行的解决方案，特别适用于高维网格数据的应用场景。

Abstract: We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images.

</details>


### [6] [Multi-Resolution Analysis of Variable Selection for Road Safety in St. Louis and Its Neighboring Area](https://arxiv.org/abs/2601.00147)
*Debjoy Thakur,Soumendra N. Lahiri*

Main category: stat.ME

TL;DR: 提出基于多分辨率视角的空间点过程变量选择方法，相比传统Lasso等方法，不仅能有效选择预测变量，还能识别在特定分辨率下哪些空间区域对变量选择起关键作用。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于圣路易斯市犯罪和事故发生的空间数据分析需求。传统变量选择方法在处理空间点过程数据时，无法识别预测变量在局部空间尺度上的相关性，而实际应用中需要了解哪些预测变量在城市的哪些区域对犯罪或事故发生有影响。

Method: 采用多分辨率视角的变量选择方法，该方法不仅进行变量选择，还能提供在特定分辨率下哪些空间点对选择某个预测变量起关键作用的信息。通过模拟研究验证了该方法在局部水平变量选择中的准确性。

Result: 模拟结果表明，该方法在强度函数估计过程中能够准确进行局部水平的变量选择，相比传统方法（Lasso、Adaptive Lasso、SCAD）具有双重优势：既能高效选择预测变量，又能识别在特定分辨率下哪些空间区域对变量选择起关键作用。

Conclusion: 多分辨率视角的变量选择方法为空间点过程数据分析提供了更精细的工具，特别适用于需要理解预测变量在局部空间尺度上影响的现实问题，如城市犯罪和事故分析。

Abstract: Generally, Lasso, Adaptive Lasso, and SCAD are standard approaches in variable selection in the presence of a large number of predictors. In recent years, during intensity function estimation for spatial point processes with a diverging number of predictors, many researchers have considered these penalized methods. But we have discussed a multi-resolution perspective for the variable selection method for spatial point process data. Its advantage is twofold: it not only efficiently selects the predictors but also provides the idea of which points are liable for selecting a predictor at a specific resolution. Actually, our research is motivated by the crime and accident occurrences in St. Louis and its neighborhoods. It is more relevant to select predictors at the local level, and thus we get the idea of which set of predictors is relevant for the occurrences of crime or accident in which parts of St. Louis. We describe the simulation results to justify the accuracy of local-level variable selection during intensity function estimation.

</details>


### [7] [Unmixing highly mixed grain size distribution data via maximum volume constrained end member analysis](https://arxiv.org/abs/2601.00154)
*Qianqian Qi,Zhongming Chen,Peter G. M. van der Heijden*

Main category: stat.ME

TL;DR: 提出MVC-EMA方法，通过最大体积约束解决高混合数据集中端元分析难以找到真实端元的问题


<details>
  <summary>Details</summary>
Motivation: 传统端元分析(EMA)在处理高度混合的数据集时，找到的端元仍然是真实端元的混合物，无法准确识别真实的沉积物来源和沉积过程

Method: 提出最大体积约束端元分析(MVC-EMA)，通过寻找尽可能不同的端元来克服混合问题，提供唯一性定理和二次规划算法

Result: 实验结果表明MVC-EMA能够在高度混合的数据集中有效找到真实端元

Conclusion: MVC-EMA方法通过最大体积约束改进了传统端元分析，能够更准确地解混高度混合的粒度分布数据，有助于更好地理解沉积物来源和沉积过程

Abstract: End member analysis (EMA) unmixes grain size distribution (GSD) data into a mixture of end members (EMs), thus helping understand sediment provenance and depositional regimes and processes. In highly mixed data sets, however, many EMA algorithms find EMs which are still a mixture of true EMs. To overcome this, we propose maximum volume constrained EMA (MVC-EMA), which finds EMs as different as possible. We provide a uniqueness theorem and a quadratic programming algorithm for MVC-EMA. Experimental results show that MVC-EMA can effectively find true EMs in highly mixed data sets.

</details>


### [8] [Continuous monitoring of delayed outcomes in basket trials](https://arxiv.org/abs/2601.00499)
*Marcio A. Diniz,Hulya Kocyigit,Erin Moshier,Madhu Mazumdar,Deukwoo Kwon*

Main category: stat.ME

TL;DR: 本文提出在篮子试验中实施连续监测，以早期识别无前景的治疗篮子，并解决延迟结局数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 精准医疗背景下，篮子试验需要识别哪些肿瘤类型能从靶向治疗中获益。现有贝叶斯设计存在计算成本高、无法处理延迟结局数据的问题，特别是免疫疗法等靶向治疗常出现延迟结局。

Method: 采用Fujiwara等人的低计算成本贝叶斯经验方法，并扩展Cai等人的思路，使用多重插补处理延迟结局数据。比较了四种处理延迟结局的策略与基准策略（暂停入组直到获得完整数据）。

Result: 通过广泛模拟研究比较了不同策略的操作特性。发现处理缺失数据的最佳方法取决于具体试验：在入组缓慢时，缺失数据很少，简单方法优于计算密集型方法；多重插补在样本量节省随篮子数量和测试药物数量增加时更具吸引力。

Conclusion: 连续监测能提高早期识别无前景篮子的可能性。处理延迟结局的最优策略是试验依赖的，多重插补在样本量节省规模化时更有价值。

Abstract: Precision medicine has led to a paradigm shift allowing the development of targeted drugs that are agnostic to the tumor location. In this context, basket trials aim to identify which tumor types - or baskets - would benefit from the targeted therapy among patients with the same molecular marker or mutation. We propose the implementation of continuous monitoring for basket trials to increase the likelihood of early identification of non-promising baskets. Although the current Bayesian trial designs available in the literature can incorporate more than one interim analysis, most of them have high computational cost, and none of them handle delayed outcomes that are expected for targeted treatments such as immunotherapies. We leverage the Bayesian empirical approach proposed by Fujiwara et al., which has low computational cost. We also extend ideas of Cai et al to address the practical challenge of performing interim analysis with delayed outcomes using multiple imputation. Operating characteristics of four different strategies to handle delayed outcomes in basket trials are compared in an extensive simulation study with the benchmark strategy where trial accrual is put on hold until complete data is observed to make a decision. The optimal handling of missing data at interim analyses is trial-dependent. With slow accrual, missingness is minimal even with continuous monitoring, favoring simpler approaches over computationally intensive methods. Although individual sample-size savings are small, multiple imputation becomes more appealing when sample size savings scale with the number of baskets and agents tested.

</details>


### [9] [Identification and Estimation under Multiple Versions of Treatment: Mixture-of-Experts Approach](https://arxiv.org/abs/2601.00287)
*Kohei Yoshikawa,Shuichi Kawano*

Main category: stat.ME

TL;DR: 将混合专家框架引入因果推断，开发了估计潜在版本因果效应的方法，即使版本未被观测也能显式估计版本特异性因果效应。


<details>
  <summary>Details</summary>
Motivation: 在观察性研究中，由于无法控制治疗实施，可能存在治疗的多个版本。忽略这些多个版本会导致因果效应估计偏差，但目前缺乏明确处理版本特异性因果效应的无偏识别和估计框架，难以深入理解复杂治疗机制。

Method: 将混合专家框架引入因果推断，开发了估计潜在版本因果效应的方法。该方法能够显式估计版本特异性因果效应，即使治疗版本未被观测。

Result: 数值实验证明了所提出方法的有效性。

Conclusion: 通过引入混合专家框架，开发了能够处理潜在治疗版本的因果推断方法，为理解复杂治疗机制提供了新工具。

Abstract: The Stable Unit Treatment Value Assumption (SUTVA) includes the condition that there are no multiple versions of treatment in causal inference. Though we could not control the implementation of treatment in observational studies, multiple versions may exist in the treatment. It has been pointed out that ignoring such multiple versions of treatment can lead to biased estimates of causal effects, but a causal inference framework that explicitly deals with the unbiased identification and estimation of version-specific causal effects has not been fully developed yet. Thus, obtaining a deeper understanding for mechanisms of the complex treatments is difficult. In this paper, we introduce the Mixture-of-Experts framework into causal inference and develop a methodology for estimating the causal effects of latent versions. This approach enables explicit estimation of version-specific causal effects even if the versions are not observed. Numerical experiments demonstrate the effectiveness of the proposed method.

</details>


### [10] [Asymptotic distribution of a robust wavelet-based NKK periodogram](https://arxiv.org/abs/2601.00310)
*Manganaw N'Daam,Tchilabalo Abozou Kpanzou,Edoh Katchekpele*

Main category: stat.ME

TL;DR: 研究基于小波的NKK周期图在长记忆过程中的渐近分布，该周期图通过LAD谐波回归构建，收敛于高斯随机向量二次形式的非标准极限。


<details>
  <summary>Details</summary>
Motivation: 为具有重尾创新的长记忆时间序列的谱分析提供稳健的小波基周期图的理论基础。

Method: 使用小波表示时间序列，分析基于最小绝对偏差(LAD)谐波回归构建的NKK周期图在固定分辨率水平下的概率结构。

Result: 在适当正则条件下，NKK周期图依分布收敛于非标准极限，该极限表征为高斯随机向量的二次形式，其协方差结构取决于过程记忆特性和所选小波滤波器。

Conclusion: 为长记忆时间序列谱分析中稳健小波基周期图的使用建立了严格的理论基础。

Abstract: This paper investigates the asymptotic distribution of a wavelet-based NKK periodogram constructed from least absolute deviations (LAD) harmonic regression at a fixed resolution level. Using a wavelet representation of the underlying time series, we analyze the probabilistic structure of the resulting periodogram under long-range dependence. It is shown that, under suitable regularity conditions, the NKK periodogram converges in distribution to a nonstandard limit characterized as a quadratic form in a Gaussian random vector, whose covariance structure depends on the memory properties of the process and on the chosen wavelet filters. This result establishes a rigorous theoretical foundation for the use of robust wavelet-based periodograms in the spectral analysis of long-memory time series with heavy-tailed inovations.

</details>


### [11] [ballmapper: Applying Topological Data Analysis Ball Mapper in Stata](https://arxiv.org/abs/2601.00508)
*Simon Rudkin,Wanling Rudkin*

Main category: stat.ME

TL;DR: TDABM是一种无需降维的多变量数据可视化方法，通过等半径球覆盖点云，保留完整数据结构，可通过颜色编码展示额外变量或模型残差。


<details>
  <summary>Details</summary>
Motivation: 传统降维方法会导致信息损失，TDABM旨在提供一种模型无关的可视化方法，能够完整保留多维数据的结构，同时便于展示额外变量信息。

Method: 使用等半径球覆盖多维点云，球半径是唯一参数，生成保留数据完整结构的图，可通过颜色编码展示额外变量、模型残差等。

Result: TDABM已在金融、经济、地理、医学和化学等多个领域得到应用，并开发了Stata的ballmapper包实现该方法。

Conclusion: TDABM提供了一种强大的多维数据可视化工具，避免了降维的信息损失，具有广泛的应用前景，并通过Stata包降低了使用门槛。

Abstract: Topological Data Analysis Ball Mapper (TDABM) offers a model-free visualization of multivariate data which does not necessitate the information loss associated with dimensionality reduction. TDABM Dlotko (2019) produces a cover of a multidimensional point cloud using equal size balls, the radius of the ball is the only parameter. A TDABM visualization retains the full structure of the data. The graphs produced by TDABM can convey coloration according to further variables, model residuals, or variables within the multivariate data. An expanding literature makes use of the power of TDABM across Finance, Economics, Geography, Medicine and Chemistry amongst others. We provide an introduction to TDABM and the \texttt{ballmapper} package for Stata.

</details>


### [12] [Fair Policy Learning under Bipartite Network Interference: Learning Fair and Cost-Effective Environmental Policies](https://arxiv.org/abs/2601.00531)
*Raphael C. Kim,Rachel C. Nethery,Kevin L. Chen,Falco J. Bargagli-Stoffi*

Main category: stat.ME

TL;DR: 提出一种在二分网络干扰（BNI）设置下的公平政策学习方法，用于设计在公平性和成本约束下减少空气污染物健康影响的政策。


<details>
  <summary>Details</summary>
Motivation: 空气污染物对健康有害，弱势群体承受不成比例的健康负担。需要设计既能有效减少公共卫生负担，又能在成本约束下确保群体间公平的政策。在环境政策中，干预针对排放源，但健康影响发生在可能遥远的社区，形成二分网络干扰的复杂设置。

Method: 提出一种公平政策学习方法，能够处理二分网络干扰数据结构。该方法允许在公平约束下学习成本有效的政策，即使面对复杂的BNI数据结构。推导了渐近性质，并通过蒙特卡洛模拟验证了有限样本性能。

Result: 通过蒙特卡洛模拟展示了有限样本性能。将方法应用于真实世界数据集，连接美国发电厂洗涤器安装与超过200万医疗保险记录，确定了在公平和成本约束下减少死亡率的公平洗涤器分配方案。

Conclusion: 该方法能够解决在二分网络干扰设置下的公平政策学习问题，为设计既有效又公平的环境政策提供了统计和因果推断框架，特别适用于排放源干预与健康影响分离的复杂环境政策场景。

Abstract: Numerous studies have shown the harmful effects of airborne pollutants on human health. Vulnerable groups and communities often bear a disproportionately larger health burden due to exposure to airborne pollutants. Thus, there is a need to design policies that effectively reduce the public health burdens while ensuring cost-effective policy interventions. Designing policies that optimally benefit the population while ensuring equity between groups under cost constraints is a challenging statistical and causal inference problem. In the context of environmental policy this is further complicated by the fact that interventions target emission sources but health impacts occur in potentially distant communities due to atmospheric pollutant transport -- a setting known as bipartite network interference (BNI). To address these issues, we propose a fair policy learning approach under BNI. Our approach allows to learn cost-effective policies under fairness constraints even accounting for complex BNI data structures. We derive asymptotic properties and demonstrate finite sample performance via Monte Carlo simulations. Finally, we apply the proposed method to a real-world dataset linking power plant scrubber installations to Medicare health records for more than 2 million individuals in the U.S. Our method determine fair scrubber allocations to reduce mortality under fairness and cost constraints.

</details>


### [13] [Variable Importance in Generalized Linear Models -- A Unifying View Using Shapley Values](https://arxiv.org/abs/2601.00773)
*Sinan Acemoglu,Christian Kleiber,Jörg Urban*

Main category: stat.ME

TL;DR: 该论文提出在回归分析中使用基于Kullback-Leibler散度的伪R²来评估变量重要性，统一了线性和非线性模型的Shapley值分解方法。


<details>
  <summary>Details</summary>
Motivation: 回归分析中的变量重要性评估在多个领域都很重要，但目前缺乏统一的方法。虽然Shapley值常用于分解拟合优度度量（在线性回归中通常用R²），但在非线性回归中缺乏普遍接受的拟合优度度量，只有各种伪R²。

Method: 提出使用基于Kullback-Leibler散度的伪R²（KL R²），该度量具有便于Shapley值解释相对和绝对重要性的理想性质。该方法特别适用于广义线性模型，能够统一和扩展先前关于线性和非线性模型变量重要性的工作。

Result: 通过公共卫生和保险数据的多个示例展示了该方法的应用，证明了KL R²能够有效评估变量重要性，并为线性和非线性模型提供了统一的框架。

Conclusion: 基于Kullback-Leibler散度的伪R²为回归分析中的变量重要性评估提供了统一的理论框架，特别适用于广义线性模型，能够将Shapley值分解方法从线性模型扩展到非线性模型。

Abstract: Variable importance in regression analyses is of considerable interest in a variety of fields. There is no unique method for assessing variable importance. However, a substantial share of the available literature employs Shapley values, either explicitly or implicitly, to decompose a suitable goodness-of-fit measure, in the linear regression model typically the classical $R^2$. Beyond linear regression, there is no generally accepted goodness-of-fit measure, only a variety of pseudo-$R^2$s. We formulate and discuss the desirable properties of goodness-of-fit measures that enable Shapley values to be interpreted in terms of relative, and even absolute, importance. We suggest to use a pseudo-$R^2$ based on the Kullback-Leibler divergence, the Kullback-Leibler $R^2$, which has a convenient form for generalized linear models and permits to unify and extend previous work on variable importance for linear and nonlinear models. Several examples are presented, using data from public health and insurance.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [14] [Surrogate Trajectories Along Probability Flows: Pseudo Markovian Alternative to Mori Zwanzig](https://arxiv.org/abs/2601.00015)
*Noé Stauffer,Hossein Gorji,Ivan Lunati*

Main category: stat.CO

TL;DR: 提出一种基于Mori-Zwanzig形式主义和Chorin最优预测方法的时间依赖最优投影模型降阶方法，用于处理高维随机动力系统中的罕见事件，通过多项式混沌展开识别投影测度，保证降阶轨迹与全阶系统概率流的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模型降阶技术在处理具有初始不确定性和随机噪声的高维动力系统时存在局限，特别是对罕见事件的分析能力不足。需要开发能够有效处理这类复杂随机系统的新方法。

Method: 基于Mori-Zwanzig形式主义和Chorin最优预测方法，采用时间依赖的最优投影将动态系统映射到一组选定的解析变量上。通过多项式混沌展开技术识别投影测度，即使在低概率事件初始条件下也能高效计算投影。

Result: 所提方法生成的降阶轨迹与全阶系统概率流保持一致，能够准确估计包括给定初始配置条件下的可观测量在内的各种统计量。数值实验表明该方法在计算效率上优于蒙特卡洛方法和最优预测方法。

Conclusion: 通过结合动态的一致投影和测度跟踪，该方法能够有效处理高维随机动力系统中的罕见事件问题，为复杂随机系统的模型降阶提供了新的理论框架和计算工具。

Abstract: Model reduction techniques have emerged as a powerful paradigm across different fronts of scientific computing. Despite their success, the provided tools and methodologies remain limited if high-dimensional dynamical systems subject to initial uncertainty and/or stochastic noise are encountered; in particular if rare events are of interest. We address this open challenge by borrowing ideas from Mori-Zwanzig formalism and Chorin's optimal prediction method. The novelty of our work lies on employing time-dependent optimal projection of the dynamic on a desired set of resolved variables. We show several theoretical and numerical properties of our model reduction approach. In particular, we show that the devised surrogate trajectories are consistent with the probability flow of the full-order system. Furthermore, we identify the measure underlying the projection through polynomial chaos expansion technique. This allows us to efficiently compute the projection even for trajectories that are initiated on low probability events. Moreover, we investigate the introduced model-reduction error of the surrogate trajectories on a standard setup, characterizing the convergence behaviour of the scheme. Several numerical results highlight the computational advantages of the proposed scheme in comparison to Monte-Carlo and optimal prediction method. Through this framework, we demonstrate that by tracking the measure along with the consistent projection of the dynamic we are able to access accurate estimates of different statistics including observables conditional on a given initial configuration.

</details>


### [15] [Integrating Multi-Armed Bandit, Active Learning, and Distributed Computing for Scalable Optimization](https://arxiv.org/abs/2601.00615)
*Foo Hui-Mean,Yuan-chin Ivan Chang*

Main category: stat.CO

TL;DR: ALMAB-DC是一个用于可扩展黑盒优化的统一框架，结合了主动学习、多臂老虎机和分布式计算，通过代理模型和信息论采集函数指导采样选择，在分布式系统中实现并行评估。


<details>
  <summary>Details</summary>
Motivation: 科学和工程领域的现代优化问题通常依赖于昂贵的黑盒评估（如物理模拟或深度学习流水线），其中梯度信息不可用或不可靠。传统优化方法由于计算成本过高和可扩展性差而变得不实用。

Method: 提出ALMAB-DC框架，集成主动学习、多臂老虎机和分布式计算（可选GPU加速）。利用代理模型和信息论采集函数指导信息丰富的样本选择，基于老虎机的控制器以统计原则方式动态分配计算资源，在分布式多智能体系统中异步执行决策。

Result: 为基于UCB和Thompson采样的变体建立了理论遗憾界限，基于Amdahl和Gustafson定律进行了可扩展性分析。在合成基准测试、强化学习任务和科学模拟问题上的实证结果表明，ALMAB-DC始终优于最先进的黑盒优化器。

Conclusion: ALMAB-DC是模块化、不确定性感知和可扩展的，特别适合高维、资源密集型的优化挑战。

Abstract: Modern optimization problems in scientific and engineering domains often rely on expensive black-box evaluations, such as those arising in physical simulations or deep learning pipelines, where gradient information is unavailable or unreliable. In these settings, conventional optimization methods quickly become impractical due to prohibitive computational costs and poor scalability. We propose ALMAB-DC, a unified and modular framework for scalable black-box optimization that integrates active learning, multi-armed bandits, and distributed computing, with optional GPU acceleration. The framework leverages surrogate modeling and information-theoretic acquisition functions to guide informative sample selection, while bandit-based controllers dynamically allocate computational resources across candidate evaluations in a statistically principled manner. These decisions are executed asynchronously within a distributed multi-agent system, enabling high-throughput parallel evaluation. We establish theoretical regret bounds for both UCB-based and Thompson-sampling-based variants and develop a scalability analysis grounded in Amdahl's and Gustafson's laws. Empirical results across synthetic benchmarks, reinforcement learning tasks, and scientific simulation problems demonstrate that ALMAB-DC consistently outperforms state-of-the-art black-box optimizers. By design, ALMAB-DC is modular, uncertainty-aware, and extensible, making it particularly well suited for high-dimensional, resource-intensive optimization challenges.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [16] [The Dynamics of Trust: A Stochastic Levy Model Capturing Sudden Behavioral Jumps](https://arxiv.org/abs/2601.00008)
*Mohamadali Berahman,Madjid Eshaghi Gordji*

Main category: stat.AP

TL;DR: 提出首个基于Lévy过程的综合随机信任模型，整合布朗运动、泊松跳跃强度和随机跳跃幅度，能模拟信任突然崩溃、混沌波动等现实现象，超越传统离散模型。


<details>
  <summary>Details</summary>
Motivation: 传统信任博弈模型主要依赖离散框架且噪声有限，无法捕捉现实世界中突然的行为转变、极端波动或合作突然崩溃等现象。信任作为社会、经济和政治系统的"隐形粘合剂"，其动态在现实环境中难以预测和控制。

Method: 提出基于Lévy过程的综合随机信任模型，整合三个核心组件：布朗运动（代表日常波动）、泊松跳跃强度（捕捉冲击频率）和跳跃幅度的随机分布。通过四个关键模拟场景和详细的参数敏感性分析（使用3D和等高线图）验证模型。

Result: 该模型不仅能模拟"信任突然崩溃"、"混沌波动"和"非线性恢复"等常被忽视的现象，而且数学上更先进，比先前方法更真实地反映人类动态。参数敏感性分析展示了模型对不同场景的适应性。

Conclusion: 该研究不仅提供了技术贡献，还建立了一个概念框架，用于理解社会、经济和地缘政治系统中脆弱、跳跃驱动的行为。信任不仅是心理建构，更是一个本质上不稳定且随机的变量，最适合通过Lévy建模来捕捉。

Abstract: Trust is the invisible glue that holds together the fabric of societies, economic systems, and political institutions. Yet, its dynamics-especially in real-world settings remain unpredictable and difficult to control. While classical trust game models largely rely on discrete frameworks with limited noise, they fall short in capturing sudden behavioral shifts, extreme volatility, or abrupt breakdowns in cooperation.Here, we propose-for the first time a comprehensive stochastic model of trust based on Lévy processes that integrates three fundamental components: Brownian motion (representing everyday fluctuations), Poissonian jump intensity (capturing the frequency of shocks), and random distributions for jump magnitudes. This framework surpasses conventional models by enabling simulations of phenomena such as "sudden trust collapse," "chaotic volatility," and "nonlinear recoveries" dynamics often neglected in both theoretical and empirical studies.By implementing four key simulation scenarios and conducting a detailed parameter sensitivity analysis via 3D and contour plots, we demonstrate that the proposed model is not only mathematically more advanced, but also offers a more realistic representation of human dynamics compared to previous approaches. Beyond its technical contributions, this study outlines a conceptual framework for understanding fragile, jump-driven behaviors in social, economic, and geopolitical systems-where trust is not merely a psychological construct, but an inherently unstable and stochastic variable best captured through Lévy based modeling.

</details>


### [17] [Subgroup Identification and Individualized Treatment Policies: A Tutorial on the Hybrid Two-Stage Workflow](https://arxiv.org/abs/2601.00136)
*Nan Miles Xi,Xin Huang,Lin Wang*

Main category: stat.AP

TL;DR: 该论文提出一个两阶段混合工作流，将统计推断与机器学习预测相结合，用于处理临床研究中的异质性治疗效应，既保证统计可靠性又实现个体化治疗策略。


<details>
  <summary>Details</summary>
Motivation: 临床研究中患者常表现出异质性治疗效应，传统亚组分析侧重于统计推断但可能牺牲预测效用，而现代机器学习方法估计条件平均治疗效应但缺乏统计保证。需要结合两者优势。

Method: 提出两阶段混合工作流：第一阶段应用统计推断检验是否存在可信的治疗效应异质性，防止虚假发现；第二阶段将异质性证据转化为个体化治疗策略，使用交叉拟合双重稳健指标评估，并施加Neyman-Pearson约束控制危害。

Result: 通过模拟数据和真实ACTG 175 HIV试验的实例展示了该工作流的应用，提供了实用实施检查清单，并讨论了与赞助方导向的HTE工作流的联系。

Conclusion: 该工作流为从异质性评估到个体化治疗策略提供了透明且可审计的路径，整合了统计推断和预测建模的优势，平衡了统计保证与预测效用。

Abstract: Patients in clinical studies often exhibit heterogeneous treatment effect (HTE). Classical subgroup analyses provide inferential tools to test for effect modification, while modern machine learning methods estimate the Conditional Average Treatment Effect (CATE) to enable individual level prediction. Each paradigm has limitations: inference focused approaches may sacrifice predictive utility, and prediction focused approaches often lack statistical guarantees. We present a hybrid two-stage workflow that integrates these perspectives. Stage 1 applies statistical inference to test whether credible treatment effect heterogeneity exists with the protection against spurious findings. Stage 2 translates heterogeneity evidence into individualized treatment policies, evaluated by cross fitted doubly robust (DR) metrics with Neyman-Pearson (NP) constraints on harm. We illustrate the workflow with working examples based on simulated data and a real ACTG 175 HIV trial. This tutorial provides practical implementation checklists and discusses links to sponsor oriented HTE workflows, offering a transparent and auditable pathway from heterogeneity assessment to individualized treatment policies.

</details>


### [18] [Gradient-free ensemble transform methods for generalized Bayesian inference in generative models](https://arxiv.org/abs/2601.00760)
*Diksha Bhandari,Sebastian Reich*

Main category: stat.AP

TL;DR: 提出一种无需梯度的集成变换朗之万动力学方法，用于基于最大平均差异的广义贝叶斯推断，适用于黑盒模拟器


<details>
  <summary>Details</summary>
Motivation: 复杂生成模型中的贝叶斯推断常因缺乏可处理的似然函数和高维模拟器梯度计算不可行而受阻。现有无似然方法通常依赖梯度优化或重参数化，计算昂贵且常不适用于黑盒模拟器

Method: 引入梯度自由的集成变换朗之万动力学方法，利用最大平均差异进行广义贝叶斯推断。该方法基于集成协方差结构而非模拟器导数，无需前向模型梯度，适用于更广泛的无似然模型

Result: 方法具有仿射不变性、计算高效且对模型误设鲁棒。在混沌动力系统和带污染数据的误设生成模型上的数值实验表明，该方法达到或优于现有梯度方法精度，同时显著降低计算成本

Conclusion: 提出的梯度自由方法为复杂黑盒模拟器的广义贝叶斯推断提供了高效、鲁棒的解决方案，扩展了无似然推断的应用范围

Abstract: Bayesian inference in complex generative models is often obstructed by the absence of tractable likelihoods and the infeasibility of computing gradients of high-dimensional simulators. Existing likelihood-free methods for generalized Bayesian inference typically rely on gradient-based optimization or reparameterization, which can be computationally expensive and often inapplicable to black-box simulators. To overcome these limitations, we introduce a gradient-free ensemble transform Langevin dynamics method for generalized Bayesian inference using the maximum mean discrepancy. By relying on ensemble-based covariance structures rather than simulator derivatives, the proposed method enables robust posterior approximation without requiring access to gradients of the forward model, making it applicable to a broader class of likelihood-free models. The method is affine invariant, computationally efficient, and robust to model misspecification. Through numerical experiments on well-specified chaotic dynamical systems, and misspecified generative models with contaminated data, we demonstrate that the proposed method achieves comparable or improved accuracy relative to existing gradient-based methods, while substantially reducing computational cost.

</details>
