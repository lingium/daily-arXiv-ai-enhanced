<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Improve Power of Knockoffs with Annotation Information of Covariates](https://arxiv.org/abs/2601.02583)
*Xiangyu Zhang,Lijun Wang,Changjun Li,Chen Lin,Hongyu Zhao*

Main category: stat.ME

TL;DR: AnnoKn是一种基于knockoff的方法，通过整合功能注释信息进行变量选择，同时严格控制错误发现率，适用于GWAS数据。


<details>
  <summary>Details</summary>
Motivation: GWAS研究中存在大量遗传变异与性状的关联信号，功能注释提供了有价值的先验信息，但变异间的高度相关性使得在利用先验知识的同时严格控制错误发现率成为一个关键挑战。

Method: AnnoKn将knockoff程序与自适应Lasso回归相结合，在统一的贝叶斯框架中评估多个协变量的重要性并整合功能注释信息。该方法还扩展到适用于汇总统计数据的版本，以解决个体水平数据不可访问的实际问题。

Result: 通过模拟实验和对GTEx及GWAS数据集的实际应用，AnnoKn在检测因果遗传变异方面比现有的注释信息变量选择方法具有更高的统计功效，同时保持对错误发现的严格控制。

Conclusion: AnnoKn提供了一种有效整合功能注释信息进行变量选择的方法，在保持错误发现率控制的同时提高了检测因果变异的能力，特别适用于GWAS研究。

Abstract: Genome-wide association studies (GWAS) often find association signals between many genetic variants and traits of interest in a genomic region. Functional annotations of these variants provide valuable prior information that helps prioritize biologically relevant variants and enhances the power to detect causal variants. However, due to substantial correlations among these variants, a critical question is how to rigorously control the false discovery rate while effectively leveraging prior knowledge. We introduce annotation-informed knockoffs (AnnoKn), a knockoff-based method that performs annotation-informed variable selection with strict control of the false discovery rate. AnnoKn integrates the knockoff procedure with adaptive Lasso regression to evaluate the importance of multiple covariates while incorporating functional annotation information within a unified Bayesian framework. To facilitate real-world applications where individual-level data are not accessible, we further extend AnnoKn to operate on summary statistics. Through simulations and real-world applications to GTEx and GWAS datasets, we show that AnnoKn achieves superior power in detecting causal genetic variants compared with existing annotation-informed variable selection methods, while maintaining valid control over false discoveries.

</details>


### [2] [Beyond Point Estimates: Toward Proper Statistical Inferencing and Reporting of Intraclass Correlation Coefficients](https://arxiv.org/abs/2601.02765)
*Yufeng Liu,Xiangfei Hong,Shanbao Tong*

Main category: stat.ME

TL;DR: 该研究系统回顾了2022-2024年NeuroImage期刊中使用ICC的报告情况，发现大多数研究未能正确进行统计推断，仅依赖点估计可能导致不可靠结论，并提供了ICC统计推断的实用指南和在线工具。


<details>
  <summary>Details</summary>
Motivation: 神经影像学及其他生物医学研究中，使用组内相关系数(ICC)报告重测信度越来越普遍，但许多研究仅比较ICC点估计值，未能进行统计差异检验或报告置信区间，这可能导致对群体水平差异的错误推断，影响结论的可靠性。

Method: 系统回顾2022-2024年NeuroImage期刊中ICC的使用情况，分析ICC报告中的错误和误用；重新计算已发表文章的置信区间并进行正式统计检验；开发在线应用程序用于ICC的统计检验和样本量估计；提供ICC统计推断的实用指南。

Result: 在回顾的11篇文章中，仅2篇提供了基于ICC的有效统计推断，2篇完全未能提供有效推断；仅依赖ICC点估计可能导致不可靠甚至误导性结论；重新计算置信区间和统计检验后，部分原始推断需要重新评估。

Conclusion: 神经影像学领域ICC报告存在严重问题，大多数研究缺乏适当的统计推断框架；迫切需要更严谨的ICC报告和解释方法；提供的指南和在线工具可帮助研究者进行正确的ICC统计推断。

Abstract: Reporting test-retest reliability using the intraclass correlation coefficient (ICC) has received increasing attention due to the criticisms of poor transparency and replicability in neuroimaging research, as well as many other biomedical studies. Numerous studies have thus evaluated the reliability of their findings by comparing ICCs, however, they often failed to test statistical differences between ICCs or report confidence intervals. Relying solely on point estimates may preclude valid inference about population-level differences and compromise the reliability of conclusions. To address this issue, this study systematically reviewed the use of ICC in articles published in NeuroImage from 2022 to 2024, highlighting the prevalence of misreporting and misuse of ICCs. We further provide practical guidelines for conducting appropriate statistical inference on ICCs. For practitioners in this area, we introduce an online application for statistical testing and sample size estimation when utilizing ICCs. We recalculated confidence intervals and formally tested ICC values reported in the reviewed articles, thereby reassessing the original inferences. Our results demonstrate that exclusive reliance on point estimates could lead to unreliable or even misleading conclusions. Specifically, only two of the eleven reviewed articles provided unequivocally valid statistical inferences based on ICCs, whereas two articles failed to yield any valid inference at all, raising serious concerns about the replicability of findings in this field. These results underscore the urgent need for rigorous inferential frameworks when reporting and interpreting ICCs.

</details>


### [3] [Scalable Ultra-High-Dimensional Quantile Regression with Genomic Applications](https://arxiv.org/abs/2601.02826)
*Hanqing Wu,Jonas Wallin,Iuliana Ionita-Laza*

Main category: stat.ME

TL;DR: 提出FS-QRPPA算法，用于高维惩罚分位数回归，通过特征分割和并行计算解决大规模数据集计算挑战


<details>
  <summary>Details</summary>
Motivation: 现代数据集（如社交媒体、基因组学、生物医学信息学）通常是异质性和超高维的，传统分位数回归方法在大规模设置下面临严重的内存和计算限制，特别是在p>>n的高维情况下，现有方法研究不足

Method: 提出特征分割近端点算法FS-QRPPA，用于高维惩罚分位数回归，利用变分分析的最新进展，通过并行计算实现可扩展性

Result: FS-QRPPA具有Q线性收敛率，在UK Biobank的大规模基因组应用中表现出优越的可扩展性，比现有方法产生更准确的系数估计和更好的预测区间覆盖

Conclusion: FS-QRPPA使惩罚分位数回归在大规模数据集上变得可行，提供了R包fsQRPPA的并行实现

Abstract: Modern datasets arising from social media, genomics, and biomedical informatics are often heterogeneous and (ultra) high-dimensional, creating substantial challenges for conventional modeling techniques. Quantile regression (QR) not only offers a flexible way to capture heterogeneous effects across the conditional distribution of an outcome, but also naturally produces prediction intervals that help quantify uncertainty in future predictions. However, classical QR methods can face serious memory and computational constraints in large-scale settings. These limitations motivate the use of parallel computing to maintain tractability. While extensive work has examined sample-splitting strategies in settings where the number of observations $n$ greatly exceeds the number of features $p$, the equally important (ultra) high-dimensional regime ($p >> n$) has been comparatively underexplored. To address this gap, we introduce a feature-splitting proximal point algorithm, FS-QRPPA, for penalized QR in high-dimensional regime. Leveraging recent developments in variational analysis, we establish a Q-linear convergence rate for FS-QRPPA and demonstrate its superior scalability in large-scale genomic applications from the UK Biobank relative to existing methods. Moreover, FS-QRPPA yields more accurate coefficient estimates and better coverage for prediction intervals than current approaches. We provide a parallel implementation in the R package fsQRPPA, making penalized QR tractable on large-scale datasets.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [4] [Computationally Efficient Estimation of Localized Treatment Effects in High-Dimensional Design Spaces using Gaussian Process Regression](https://arxiv.org/abs/2601.03105)
*Abdulrahman A. Ahmed,M. Amin Rahimian,Qiushi Chen,Praveen Kumar*

Main category: stat.AP

TL;DR: 开发了一个元模型框架，通过高斯过程回归学习响应函数系数，高效估计阿片类药物流行病干预措施的效果，仅需1/10的模拟运行就能达到约5%的平均相对误差。


<details>
  <summary>Details</summary>
Motivation: 基于人口的阿片类药物流行病模拟有助于评估干预策略，但全面模拟所有治疗条件和所有社区的计算成本过高，因为可能的治疗数量随干预措施和级别呈指数增长。

Method: 开发了一个元模型框架，通过高斯过程回归学习响应函数系数，利用空间相关性和后验不确定性，顺序采样信息量最大的县和治疗条件，而不是使用传统的实验设计方法。

Result: 在宾夕法尼亚县应用该框架，获得了所有治疗条件对每10万人口过量死亡的治疗效果估计，仅需全面评估所需模拟运行次数的1/10，就达到了约5%的平均相对误差。

Conclusion: 该双层框架为政策制定者提供了计算高效的决策支持方法，能够快速评估替代资源分配策略以缓解当地社区的阿片类药物流行病，并可应用于指导其他流行病场景的精准公共卫生干预。

Abstract: Population-scale agent-based simulations of the opioid epidemic help evaluate intervention strategies and overdose outcomes in heterogeneous communities and provide estimates of localized treatment effects, which support the design of locally-tailored policies for precision public health. However, it is prohibitively costly to run simulations of all treatment conditions in all communities because the number of possible treatments grows exponentially with the number of interventions and levels at which they are applied. To address this need efficiently, we develop a metamodel framework, whereby treatment outcomes are modeled using a response function whose coefficients are learned through Gaussian process regression (GPR) on locally-contextualized covariates. We apply this framework to efficiently estimate treatment effects on overdose deaths in Pennsylvania counties. In contrast to classical designs such as fractional factorial design or Latin hypercube sampling, our approach leverages spatial correlations and posterior uncertainty to sequentially sample the most informative counties and treatment conditions. Using a calibrated agent-based opioid epidemic model, informed by county-level overdose mortality and baseline dispensing rate data for different treatments, we obtained county-level estimates of treatment effects on overdose deaths per 100,000 population for all treatment conditions in Pennsylvania, achieving approximately 5% average relative error using one-tenth the number of simulation runs required for exhaustive evaluation. Our bi-level framework provides a computationally efficient approach to decision support for policy makers, enabling rapid evaluation of alternative resource-allocation strategies to mitigate the opioid epidemic in local communities. The same analytical framework can be applied to guide precision public health interventions in other epidemic settings.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [5] [Mitigating Long-Tailed Anomaly Score Distributions with Importance-Weighted Loss](https://arxiv.org/abs/2601.02440)
*Jungi Lee,Jungkwon Kim,Chi Zhang,Sangmin Kim,Kwangsun Yoo,Seok-Joo Byun*

Main category: stat.ML

TL;DR: 提出了一种针对异常检测中长尾分布问题的重要性加权损失函数，无需正常数据类别先验知识，通过重要性采样将异常分数分布对齐到目标高斯分布，在多个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测模型在单类正常数据上训练，但现实世界中正常数据呈现多样化模式，导致类别不平衡和长尾异常分数分布(LTD)。这种不平衡会扭曲模型训练，特别是对少数实例的检测性能下降。

Method: 提出了一种专门针对异常检测的重要性加权损失函数。与之前分类中处理LTD的方法不同，本方法不需要正常数据类别的先验知识。通过引入加权损失函数，结合重要性采样，将异常分数分布与目标高斯分布对齐，确保正常数据的平衡表示。

Result: 在三个基准图像数据集和三个真实世界高光谱成像数据集上进行了广泛实验，证明了该方法在缓解LTD引起的偏差方面的鲁棒性。方法将异常检测性能提高了0.043，突显了其在现实应用中的有效性。

Conclusion: 提出的重要性加权损失函数有效解决了异常检测中的长尾分布问题，无需类别先验知识，通过分布对齐改善了模型训练平衡性，在实际应用中表现出显著性能提升。

Abstract: Anomaly detection is crucial in industrial applications for identifying rare and unseen patterns to ensure system reliability. Traditional models, trained on a single class of normal data, struggle with real-world distributions where normal data exhibit diverse patterns, leading to class imbalance and long-tailed anomaly score distributions (LTD). This imbalance skews model training and degrades detection performance, especially for minority instances. To address this issue, we propose a novel importance-weighted loss designed specifically for anomaly detection. Compared to the previous method for LTD in classification, our method does not require prior knowledge of normal data classes. Instead, we introduce a weighted loss function that incorporates importance sampling to align the distribution of anomaly scores with a target Gaussian, ensuring a balanced representation of normal data. Extensive experiments on three benchmark image datasets and three real-world hyperspectral imaging datasets demonstrate the robustness of our approach in mitigating LTD-induced bias. Our method improves anomaly detection performance by 0.043, highlighting its effectiveness in real-world applications.

</details>


### [6] [Fast Conformal Prediction using Conditional Interquantile Intervals](https://arxiv.org/abs/2601.02769)
*Naixin Guo,Rui Luo,Zhixin Zhou*

Main category: stat.ML

TL;DR: CIR和CIR+是两种保形回归方法，通过利用黑盒机器学习模型估计结果分布，构建近乎最小的预测区间并保证覆盖率，在计算效率和预测准确性之间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有分布保形预测方法存在局限性：保形分位数回归(CQR)对偏态分布处理效果不佳，保形直方图回归(CHR)计算效率低且需要构建直方图。需要开发既能有效处理偏态分布又具有高计算效率的方法。

Method: CIR利用黑盒机器学习模型通过分位数间范围估计结果分布，将这些估计转化为紧凑的预测区间并实现近似条件覆盖率。CIR+在CIR基础上引入基于宽度的分位数区间选择规则，进一步缩小预测区间宽度。

Result: 两种方法在合成和真实数据集上的实验表明，相比现有方法，CIR和CIR+在预测准确性和计算效率之间达到最佳平衡。CIR+能产生更窄的预测区间但计算时间略有增加。

Conclusion: CIR和CIR+解决了现有分布保形预测方法的关键限制，能更有效地处理偏态分布，计算效率显著高于CHR，同时保持可比的覆盖率，为实际应用提供了高效的预测区间构建方案。

Abstract: We introduce Conformal Interquantile Regression (CIR), a conformal regression method that efficiently constructs near-minimal prediction intervals with guaranteed coverage. CIR leverages black-box machine learning models to estimate outcome distributions through interquantile ranges, transforming these estimates into compact prediction intervals while achieving approximate conditional coverage. We further propose CIR+ (Conditional Interquantile Regression with More Comparison), which enhances CIR by incorporating a width-based selection rule for interquantile intervals. This refinement yields narrower prediction intervals while maintaining comparable coverage, though at the cost of slightly increased computational time. Both methods address key limitations of existing distributional conformal prediction approaches: they handle skewed distributions more effectively than Conformalized Quantile Regression, and they achieve substantially higher computational efficiency than Conformal Histogram Regression by eliminating the need for histogram construction. Extensive experiments on synthetic and real-world datasets demonstrate that our methods optimally balance predictive accuracy and computational efficiency compared to existing approaches.

</details>


### [7] [Self-Supervised Learning from Noisy and Incomplete Data](https://arxiv.org/abs/2601.03244)
*Julián Tachella,Mike Davies*

Main category: stat.ML

TL;DR: 本文综述了逆问题中的自监督学习方法，特别关注其理论基础和在成像逆问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程问题需要从噪声或不完整观测中推断信号，传统方法依赖手工设计的正则化，而数据驱动方法需要大量真实参考数据，这在许多实际应用中难以获取。

Method: 系统总结不同的自监督学习方法，特别强调其理论基础，包括如何仅从测量数据学习求解器而无需真实参考数据。

Result: 提供了逆问题自监督学习的全面综述，涵盖不同方法的理论分析和实际应用，特别在成像逆问题中展示了实用价值。

Conclusion: 自监督学习为逆问题提供了有前景的替代方案，能够在无需真实参考数据的情况下获得有效求解器，在成像等实际应用中具有重要价值。

Abstract: Many important problems in science and engineering involve inferring a signal from noisy and/or incomplete observations, where the observation process is known. Historically, this problem has been tackled using hand-crafted regularization (e.g., sparsity, total-variation) to obtain meaningful estimates. Recent data-driven methods often offer better solutions by directly learning a solver from examples of ground-truth signals and associated observations. However, in many real-world applications, obtaining ground-truth references for training is expensive or impossible. Self-supervised learning methods offer a promising alternative by learning a solver from measurement data alone, bypassing the need for ground-truth references. This manuscript provides a comprehensive summary of different self-supervised methods for inverse problems, with a special emphasis on their theoretical underpinnings, and presents practical applications in imaging inverse problems.

</details>
