<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 8]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ML](#stat.ML) [Total: 10]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Maximum softly penalised likelihood in factor analysis](https://arxiv.org/abs/2510.06465)
*Philipp Sterzinger,Ioannis Kosmids,Irini Moustaki*

Main category: stat.ME

TL;DR: 本文提出了最大软惩罚似然框架，通过向对数似然函数添加适当惩罚项来解决探索性因子分析中的Heywood案例问题，确保参数估计在参数空间内部且具有渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 探索性因子分析中经常出现Heywood案例（参数估计位于参数空间边界），这会导致数值优化问题、收敛失败，并在因子得分和模型选择方面产生误导性推断。

Method: 推导了模型和惩罚函数的充分条件，保证最大惩罚似然估计存在于参数空间内部，并确保估计量具有一致性、渐近正态性等优良渐近性质。引入了最大软惩罚似然框架来适当缩放惩罚项。

Result: 证明了Akaike(1987)和Hirose等人(2011)的惩罚项满足存在性条件，能处理Heywood案例。通过模拟研究和真实数据分析验证了最大软惩罚似然估计量具有良好的有限样本性质。

Conclusion: 最大软惩罚似然框架能有效解决Heywood案例问题，确保估计和推断过程的渐近最优性，同时保持优良的有限样本性能。

Abstract: Estimation in exploratory factor analysis often yields estimates on the
boundary of the parameter space. Such occurrences, known as Heywood cases, are
characterised by non-positive variance estimates and can cause issues in
numerical optimisation procedures or convergence failures, which, in turn, can
lead to misleading inferences, particularly regarding factor scores and model
selection. We derive sufficient conditions on the model and a penalty to the
log-likelihood function that i) guarantee the existence of maximum penalised
likelihood estimates in the interior of the parameter space, and ii) ensure
that the corresponding estimators possess the desirable asymptotic properties
expected by the maximum likelihood estimator, namely consistency and asymptotic
normality. Consistency and asymptotic normality are achieved when the
penalisation is soft enough, in a way that adapts to the information
accumulation about the model parameters. We formally show, for the first time,
that the penalties of Akaike (1987) and Hirose et al. (2011) to the
log-likelihood of the normal linear factor model satisfy the conditions for
existence, and, hence, deal with Heywood cases. Their vanilla versions, though,
can result in questionable finite-sample properties in estimation, inference,
and model selection. The maximum softly-penalised likelihood framework we
introduce enables the careful scaling of those penalties to ensure that the
resulting estimation and inference procedures are asymptotically optimal.
Through comprehensive simulation studies and the analysis of real data sets, we
illustrate the desirable finite-sample properties of the maximum softly
penalised likelihood estimators and associated procedures.

</details>


### [2] [Inference in pseudo-observation-based regression using (biased) covariance estimation and naive bootstrapping](https://arxiv.org/abs/2510.06815)
*Simon Mack,Morten Overgaard,Dennis Dobler*

Main category: stat.ME

TL;DR: 本文证明在伪观测回归方法中，常用的Huber-White估计量对于参数估计的极限协方差矩阵不是一致的。作者提出使用插件估计量替代，并验证了在伪观测模型中，朴素自助法不能用于协方差估计，但通过适当的学生化可以用于假设检验。


<details>
  <summary>Details</summary>
Motivation: 现有伪观测回归方法中使用的Huber-White估计量存在不一致性问题，需要开发更可靠的协方差估计和假设检验方法。

Method: 提出使用插件估计量替代Huber-White估计量，并研究自助法在伪观测模型中的适用性，同时开发了U-和V-统计量的通用一致大数定律。

Result: 模拟研究表明所提方法在多种场景下表现良好，插件估计量能提供渐近精确且一致的检验，学生化自助法可用于假设检验。

Conclusion: 插件估计量是伪观测回归中协方差估计的可靠替代方案，自助法经过适当调整后可用于假设检验，为相关统计推断提供了理论支持。

Abstract: We demonstrate that the usual Huber-White estimator is not consistent for the
limiting covariance of parameter estimates in pseudo-observation regression
approaches. By confirming that a plug-in estimator can be used instead, we
obtain asymptotically exact and consistent tests for general linear hypotheses
in the parameters of the model. Additionally, we confirm that naive
bootstrapping can not be used for covariance estimation in the
pseudo-observation model either. However, it can be used for hypothesis testing
by applying a suitable studentization. Simulations illustrate the good
performance of our proposed methods in many scenarios. Finally, we obtain a
general uniform law of large numbers for U- and V-statistics, as such
statistics are central in the mathematical analysis of the inference procedures
developed in this work.

</details>


### [3] [Likelihood-based inference for the Gompertz model with Poisson errors](https://arxiv.org/abs/2510.06787)
*Paolo Onorati,Sofia Ruiz-Suarez,Radu Craiu*

Main category: stat.ME

TL;DR: 开发了基于完整似然的高效计算工具，用于Gompertz模型中的统计推断，解决了考虑抽样误差时的计算难题。


<details>
  <summary>Details</summary>
Motivation: 人口动态模型在精算科学、人口学和生态学中很重要，但统计推断困难，因为除了过程固有的随机性外，还需要考虑抽样误差，忽略后者会导致估计偏差和错误结论。

Method: 开发了基于完整似然的高效计算工具，在贝叶斯和频率主义两种范式下进行统计推断。

Result: 通过模拟和数据分析验证了方法的性能，能够有效处理Gompertz模型中的统计推断问题。

Conclusion: 该方法填补了Gompertz模型在考虑抽样误差时完整似然方法计算上的空白，为人口动态研究提供了实用的统计工具。

Abstract: Population dynamics models play an important role in a number of fields, such
as actuarial science, demography, and ecology. Statistical inference for these
models can be difficult when, in addition to the process' inherent
stochasticity, one also needs to account for sampling error. Ignoring the
latter can lead to biases in the estimation, which in turn can produce
erroneous conclusions about the system's behavior. The Gompertz model is widely
used to infer population size dynamics, but a full likelihood approach can be
computationally prohibitive when sampling error is accounted for. We close this
gap by developing efficient computational tools for statistical inference in
the Gompertz model based on the full likelihood. The approach is illustrated in
both the Bayesian and frequentist paradigms. Performance is illustrated with
simulations and data analysis.

</details>


### [4] [Rank Aggregation under Weak Stochastic Transitivity via a Maximum Score Estimator](https://arxiv.org/abs/2510.06789)
*Haoran Zhang,Yunxiao Chen*

Main category: stat.ME

TL;DR: 提出了一种基于弱随机传递性假设的最大得分估计器，用于聚合基于成对比较数据的排名，适用于稀疏比较场景，并证明了其一致性和最优性。


<details>
  <summary>Details</summary>
Motivation: 现有模型采用强随机传递性假设，对成对比较概率施加了严格的单调性约束，这在现实应用中往往不切实际。本文旨在放宽这一假设，仅要求最弱的随机传递性假设。

Method: 引入最大得分估计器，仅需弱随机传递性假设，允许许多成对比较缺失且缺失概率可能不均匀的稀疏设置。

Result: 证明了所提估计器是一致的，随着玩家数量增加，不一致对的比例以概率收敛到零。同时证明了该估计器在基于Kendall's tau距离的损失函数收敛方面接近极小极大最优。

Conclusion: 通过模拟研究和网球运动员排名应用验证了所提方法的有效性，为基于成对比较数据的排名聚合提供了一种更灵活且理论保证的解决方案。

Abstract: Stochastic transitivity is central for rank aggregation based on pairwise
comparison data. The existing models, including the Thurstone, Bradley-Terry
(BT), and nonparametric BT models, adopt a strong notion of stochastic
transitivity, known as strong stochastic transitivity (SST). This assumption
imposes restrictive monotonicity constraints on the pairwise comparison
probabilities, which is often unrealistic for real-world applications. This
paper introduces a maximum score estimator for aggregating ranks, which only
requires the assumption of weak stochastic transitivity (WST), the weakest
assumption needed for the existence of a global ranking. The proposed estimator
allows for sparse settings where the comparisons between many pairs are missing
with possibly nonuniform missingness probabilities. We show that the proposed
estimator is consistent, in the sense that the proportion of discordant pairs
converges to zero in probability as the number of players diverges. We also
establish that the proposed estimator is nearly minimax optimal for the
convergence of a loss function based on Kendall's tau distance. The power of
the proposed method is shown via a simulation study and an application to rank
professional tennis players.

</details>


### [5] [Confidence Regions for Multiple Outcomes, Effect Modifiers, and Other Multiple Comparisons](https://arxiv.org/abs/2510.07076)
*Paul N Zivich,Stephen R Cole,Noah Greifer,Lina M Montoya,Michael R Kosorok,Jessie K Edwards*

Main category: stat.ME

TL;DR: 本文讨论了流行病学中多重比较校正的必要性，提出当关注多个参数时，应使用置信带来代替标准置信区间，以确保同时覆盖多个参数的不确定性。


<details>
  <summary>Details</summary>
Motivation: 流行病学研究中常涉及多个参数的估计，但标准置信区间无法保证对多个参数的同时覆盖，会低估随机误差带来的不确定性。

Method: 使用置信带作为置信区间对参数向量的扩展，通过三个案例研究（多重因果效应估计、二元变量效应修正、连续变量效应修正）展示置信带的应用，并提供SAS、R和Python代码。

Result: 置信带能够有效解决多重参数估计中的不确定性低估问题，特别是在多重因果效应、效应修正等场景下表现良好。

Conclusion: 流行病学家应根据科学兴趣是单个参数还是参数集来选择置信区域类型，对于参数集的情况，sup-t置信带因其统计特性、计算简单性和易于展示而更受推荐。

Abstract: In epidemiology, some have argued that multiple comparison corrections are
not necessary as there is rarely interest in the universal null hypothesis.
From a parameter estimation perspective, epidemiologists may still be
interested in multiple parameters. In this context, standard confidence
intervals are not guaranteed to provide simultaneous coverage of more than one
parameter. In other words, use of confidence intervals in these cases will
understate the uncertainty due to random error. To address this challenge, one
can use confidence bands, an extension of confidence intervals to parameter
vectors. We illustrate the use of confidence bands in three case studies:
estimation of multiple causal effects, effect measure modification by a binary
variable, and effect measure modification by a continuous variable. Each
example uses publicly available data is accompanied by SAS, R, and Python code.
The type of confidence region reported by epidemiologists should depend on
whether scientific interest is in a single parameter or a set of parameters.
For sets of parameters, like in cases where multiple actions or outcomes,
effect measure modification, dose-response, or other functions are of interest,
sup-t confidence bands are preferred due to their statistical properties,
computational simplicity, and ease of presentation.

</details>


### [6] [On Assessing Overall Survival (OS) in Oncology Studies](https://arxiv.org/abs/2510.07122)
*Jason C. Hsu*

Main category: stat.ME

TL;DR: 本文论证了在肿瘤学研究中，时间比率(TR)是逻辑尊重的疗效指标，而风险比(HR)不是。建议采用时间比率，并提出了平滑过渡策略。


<details>
  <summary>Details</summary>
Motivation: 在评估肿瘤学研究的总体生存期(OS)时，需要确保疗效指标是逻辑尊重的，否则可能导致患者被错误地靶向治疗。

Method: 通过逻辑性分析比较时间比率(TR)和风险比(HR)的特性，提出采用时间比率作为疗效评估指标，并建议平滑过渡策略。

Result: 时间比率是逻辑尊重的疗效指标，而风险比不是。子群混合估计(SME)能够确保总体人群的疗效评估在子群最小和最大疗效范围内。

Conclusion: 逻辑性要求无论结果如何测量，无论选择哪种逻辑尊重的疗效指标，无论子群如何分层，都应获得相同的疗效评估。子群混合估计(SME)能够实现这一目标。

Abstract: In assessing Overall Survival (OS) in oncology studies, it is essential for
the efficacy measure to be Logic-respecting, for otherwise patients may be
incorrectly targeted. This paper explains, while Time Ratio (TR) is
Logic-respecting, Hazard Ratio (HR) is not Logic-respecting. With Time Ratio
(TR) being recommended, a smooth transitioning strategy is suggested. The
conclusion states: Logicality requires, and Subgroup Mixable Estimation (SME)
delivers, an efficacy assessment for the overall population within the range of
minimum and maximum efficacy in the subgroups, no matter how outcome is
measured, whichever logic-respecting efficacy measure is chosen, the same
efficacy assessment regardless of how subgroups are stratified.

</details>


### [7] [jmstate, a Flexible Python Package for Multi-State Joint Modeling](https://arxiv.org/abs/2510.07128)
*Félix Laplante,Christophe Ambroise,Estelle Kuhn,Sarah Lemler*

Main category: stat.ME

TL;DR: 本文提出了一个统一纵向生物标志物建模与多状态事件过程的通用框架，支持马尔可夫和半马尔可夫转移结构，并开发了基于随机梯度下降的可扩展推断方法。


<details>
  <summary>Details</summary>
Motivation: 传统的联合建模方法通常只能捕捉有限的事件动态，而多状态联合模型通过表示完整的事件历史网络提供了更灵活的替代方案。

Method: 将非线性混合效应纵向子模型与多状态生存过程通过共享潜在结构耦合，推导完整似然函数并开发基于随机梯度下降的推断程序。

Result: 模拟实验和生物医学案例研究证明了该框架在表示复杂纵向和多状态事件动态方面的灵活性和性能。

Conclusion: 该框架为复杂纵向和多状态事件过程提供了统一的建模方法，并提供了开源Python库jmstate实现该方法。

Abstract: Classical joint modeling approaches often rely on competing risks or
recurrent event formulations to account for complex real-world processes
involving evolving longitudinal markers and discrete event occurrences.
However, these frameworks typically capture only limited aspects of the
underlying event dynamics.
  Multi-state joint models offer a more flexible alternative by representing
full event histories through a network of possible transitions, including
recurrent cycles and terminal absorptions, all potentially influenced by
longitudinal covariates.
  In this paper, we propose a general framework that unifies longitudinal
biomarker modeling with multi-state event processes defined on arbitrary
directed graphs. Our approach accommodates both Markovian and semi-Markovian
transition structures, and extends classical joint models by coupling nonlinear
mixed-effects longitudinal submodels with multi-state survival processes via
shared latent structures.
  We derive the full likelihood and develop scalable inference procedures based
on stochastic gradient descent. Furthermore, we introduce a dynamic prediction
framework, enabling individualized risk assessments along complex
state-transition trajectories.
  To facilitate reproducibility and dissemination, we provide an open-source
Python library \texttt{jmstate} implementing the proposed methodology,
available on \href{https://pypi.org/project/jmstate/}{PyPI}. Simulation
experiments and a biomedical case study demonstrate the flexibility and
performance of the framework in representing complex longitudinal and
multi-state event dynamics. The full Python notebooks used to reproduce the
experiments as well as the source code of this paper are available on
\href{https://gitlab.com/felixlaplante0/jmstate-paper/}{GitLab}.

</details>


### [8] [Randomization Restrictions: Their Impact on Type I Error When Experimenting with Finite Populations](https://arxiv.org/abs/2510.07153)
*Jonathan J. Chipman,Oleksandr Sverdlov,Diane Uschner*

Main category: stat.ME

TL;DR: 本文探讨了临床试验中随机化限制对随机化推断(RBI)和方差分析(ANOVA)一致性的影响，发现随机化限制会显著影响ANOVA的I类错误，即使在大样本试验中也是如此。


<details>
  <summary>Details</summary>
Motivation: 临床试验参与者通常被视为独特的有限总体，但统计分析常假设他们是从更大总体中随机抽样而来。序列入组试验采用限制性随机化方案来减少时间性治疗不平衡，但这些限制对RBI和ANOVA一致性的影响尚不清楚。

Method: 通过真实世界参考框架（如罕见病和超罕见病）回顾有限总体的完全抽样与随机抽样，并实证评估在使用ANOVA后随机化限制下的有限总体I类错误。对区块随机化进行了校正，但MTI设计的校正方法仍待解决。

Result: 随机化限制强烈影响ANOVA的I类错误，即使在有1000名参与者的试验中也是如此。适当调整限制可以校正I类错误。RBI能够考虑随机化限制，同时确保正确的有限总体I类错误。

Conclusion: 研究加深了对区块和MTI限制下RBI和ANOVA一致性的理解和校正，并使用有限总体来估计I类错误收敛到名义速率。讨论了指定估计量总体并与抽样试验参与者协调的挑战。

Abstract: Participants in clinical trials are often viewed as a unique, finite
population. Yet, statistical analyses often assume that participants were
randomly sampled from a larger population. Under Complete Randomization,
Randomization-Based Inference (RBI; a finite population inference) and Analysis
of Variance (ANOVA; a random sampling inference) provide asymptotically
equivalent difference-in-means tests. However, sequentially-enrolling trials
typically employ restricted randomization schemes, such as block or Maximum
Tolerable Imbalance (MTI) designs, to reduce the chance of chronological
treatment imbalances. The impact of these restrictions on RBI and ANOVA
concordance is not well understood. With real-world frames of reference, such
as rare and ultra-rare diseases, we review full versus random sampling of
finite populations and empirically evaluate finite population Type I error when
using ANOVA following randomization restrictions. Randomization restrictions
strongly impacted ANOVA Type I error, even for trials with 1,000 participants.
Properly adjusting for restrictions corrected Type I error. We corrected for
block randomization, yet leave open how to correct for MTI designs. More
directly, RBI accounts for randomization restrictions while ensuring correct
finite population Type I error. Novel contributions are: 1) deepening the
understanding and correction of RBI and ANOVA concordance under block and MTI
restrictions and 2) using finite populations to estimate the convergence of
Type I error to a nominal rate. We discuss the challenge of specifying an
estimand's population and reconciling with sampled trial participants.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [9] [A Mixed-Methods Analysis of Repression and Mobilization in Bangladesh's July Revolution Using Machine Learning and Statistical Modeling](https://arxiv.org/abs/2510.06264)
*Md. Saiful Bari Siddiqui,Anupam Debashis Roy*

Main category: stat.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The 2024 July Revolution in Bangladesh represents a landmark event in the
study of civil resistance. This study investigates the central paradox of the
success of this student-led civilian uprising: how state violence, intended to
quell dissent, ultimately fueled the movement's victory. We employ a
mixed-methods approach. First, we develop a qualitative narrative of the
conflict's timeline to generate specific, testable hypotheses. Then, using a
disaggregated, event-level dataset, we employ a multi-method quantitative
analysis to dissect the complex relationship between repression and
mobilisation. We provide a framework to analyse explosive modern uprisings like
the July Revolution. Initial pooled regression models highlight the crucial
role of protest momentum in sustaining the movement. To isolate causal effects,
we specify a Two-Way Fixed Effects panel model, which provides robust evidence
for a direct and statistically significant local suppression backfire effect.
Our Vector Autoregression (VAR) analysis provides clear visual evidence of an
immediate, nationwide mobilisation in response to increased lethal violence. We
further demonstrate that this effect was non-linear. A structural break
analysis reveals that the backfire dynamic was statistically insignificant in
the conflict's early phase but was triggered by the catalytic moral shock of
the first wave of lethal violence, and its visuals circulated around July 16th.
A complementary machine learning analysis (XGBoost, out-of-sample R$^{2}$=0.65)
corroborates this from a predictive standpoint, identifying "excessive force
against protesters" as the single most dominant predictor of nationwide
escalation. We conclude that the July Revolution was driven by a contingent,
non-linear backfire, triggered by specific catalytic moral shocks and
accelerated by the viral reaction to the visual spectacle of state brutality.

</details>


### [10] [Estimating temporary emigration from capture-recapture data in the presence of latent identification](https://arxiv.org/abs/2510.06755)
*Katarina Skopalova,Jafet Osuna,Wei Zhang*

Main category: stat.AP

TL;DR: 提出了一种新的潜在多项式临时迁出建模框架，用于分析具有潜在个体识别的捕获-再捕获数据，解决了传统模型假设个体不迁出或永久迁出的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统捕获-再捕获模型假设个体不迁出或永久迁出采样区域，但现实中个体可能暂时离开后返回，这会导致推断偏差。现有临时迁出模型要求个体被唯一正确识别，但缺乏处理潜在个体识别情况的方法。

Method: 开发了潜在多项式临时迁出建模框架，适用于封闭和开放种群问题，可处理有或无个体识别的数据，并灵活整合不同迁出过程（完全随机和马尔可夫迁出）。

Result: 通过模拟验证了模型在各种迁出情景下能可靠估计参数。应用于金曼特拉蛙的实际数据集，结果显示考虑临时迁出比不考虑的模型能更好地拟合数据。

Conclusion: 该框架为处理具有潜在个体识别的捕获-再捕获数据中的临时迁出问题提供了有效解决方案，提高了模型拟合度和参数估计的准确性。

Abstract: Most capture-recapture models assume that individuals either do not emigrate
or emigrate permanently from the sampling area during the sampling period. This
assumption is violated when individuals temporarily leave the sampling area and
return during later capture occasions, which can result in biased or less
precise inferences under normal capture-recapture models. Existing temporary
emigration models require that individuals are uniquely and correctly
identified. To our knowledge, no studies to date have addressed temporary
emigration in the presence of latent individual identification, which can arise
in many scenarios such as misidentification, data integration, and batch
marking. In this paper, we propose a new latent multinomial temporary
emigration modelling framework for analysing capture-recapture data with latent
identification. The framework is applicable to both closed- and open-population
problems, accommodates data with or without individual identification, and
flexibly incorporates different emigration processes, including the completely
random and Markovian emigration. Through simulations, we demonstrate that model
parameters can be reliably estimated in various emigration scenarios. We apply
the proposed framework to a real dataset on golden mantella collected using
batch marks under Pollock's robust design. The results show that accounting for
temporary emigration provides a better fit to the data compared to the previous
model without temporary emigration.

</details>


### [11] [Estimating Real Demand Using a Flipped Queueing Model: A Case of Shared Micro-Mobility Services](https://arxiv.org/abs/2510.07194)
*Binyu Yang,Jinxiao Du,Junlin He,Shi An,Wei Ma*

Main category: stat.AP

TL;DR: 提出了一种通过广义车辆生存时间(GVST)和翻转排队模型(FQM)来估计共享微出行服务中真实需求的方法，解决了供需时空不平衡导致的观测需求截断问题。


<details>
  <summary>Details</summary>
Motivation: 共享微出行服务中供需的时空不平衡导致观测需求被截断，无法反映真实需求，这影响了需求预测、车队管理等下游应用的可靠性。现有研究未能很好地解决这一问题。

Method: 提出了广义车辆生存时间(GVST)这一可观测变量，通过翻转排队模型(FQM)捕捉共享微出行服务的运营动态，将真实需求估计问题转化为逆排队问题。开发了单边估计(闭式解)和双边估计(方程组)两种方法。

Result: 在合成数据和真实世界数据集(共享单车和电动滑板车)上的实验表明，两种方法均优于基准模型。单边方法提供闭式解，精度可接受，可作为需求相关分析和决策的实用经验法则。

Conclusion: 该方法能够准确估计共享微出行服务的真实需求，为需求预测、车队管理和微出行规划等应用提供了可靠的基础。

Abstract: The spatial-temporal imbalance between supply and demand in shared
micro-mobility services often leads to observed demand being censored,
resulting in incomplete records of the underlying real demand. This phenomenon
undermines the reliability of the collected demand data and hampers downstream
applications such as demand forecasting, fleet management, and micro-mobility
planning. How to accurately estimate the real demand is challenging and has not
been well explored in existing studies. In view of this, we contribute to real
demand estimation for shared micro-mobility services by proposing an analytical
method that rigorously derives the real demand under appropriate assumptions.
Rather than directly modeling the intractable relationship between observed
demand and real demand, we propose a novel random variable, Generalized Vehicle
Survival Time (GVST), which is observable from trip records. The relationship
between GVST and real demand is characterized by introducing a flipped queueing
model (FQM) that captures the operational dynamics of shared micro-mobility
services. Specifically, the distribution of GVST is derived within the FQM,
which allows the real demand estimation problem to be transformed into an
inverse queueing problem. We analytically derive the real demand in closed form
using a one-sided estimation method, and solve the problem by a system of
equations in a two-sided estimation method. We validate the proposed methods
using synthetic data and conduct empirical analyses using real-world datasets
from bike-sharing and shared e-scooter systems. The experimental results show
that both the two-sided and one-sided methods outperform benchmark models. In
particular, the one-sided approach provides a closed-form solution that
delivers acceptable accuracy, constituting a practical rule of thumb for
demand-related analytics and decision-making processes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [12] [A General Constructive Upper Bound on Shallow Neural Nets Complexity](https://arxiv.org/abs/2510.06372)
*Frantisek Hakl,Vit Fojtik*

Main category: stat.ML

TL;DR: 本文提供了一个浅层神经网络中所需神经元数量的上界，用于在紧凑集上以给定精度逼近连续函数。该方法受Stone-Weierstrass定理特定证明的启发，具有构造性，并且比以往类似界限更通用，适用于任何紧凑集上的任何连续函数。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络逼近连续函数时所需的最小神经元数量，为神经网络的理论分析提供更通用和构造性的上界估计方法。

Method: 受Stone-Weierstrass定理特定证明启发，提出了一种构造性方法，通过分析浅层神经网络在紧凑集上逼近连续函数的能力，推导出神经元数量的上界。

Result: 得到了一个适用于任何紧凑集上任何连续函数的神经元数量上界，该界限比以往类似结果更具通用性和构造性。

Conclusion: 提出的方法为神经网络逼近理论提供了更通用和实用的上界估计，扩展了现有理论结果的应用范围。

Abstract: We provide an upper bound on the number of neurons required in a shallow
  neural network to approximate a continuous function on a compact set with a
  given accuracy. This method, inspired by a specific proof of the
  Stone-Weierstrass theorem, is constructive and more general than previous
  bounds of this character, as it applies to any continuous function on any
  compact set.

</details>


### [13] [Online Matching via Reinforcement Learning: An Expert Policy Orchestration Strategy](https://arxiv.org/abs/2510.06515)
*Chiara Mignacco,Matthieu Jonckheere,Gilles Stoltz*

Main category: stat.ML

TL;DR: 提出了一种基于强化学习的方法，通过协调多个专家策略来提升在线匹配系统的性能，该方法具有理论保证并在实验中表现出优于单个专家和传统RL基准的效果。


<details>
  <summary>Details</summary>
Motivation: 在线匹配问题（如云服务、在线市场和器官交换网络）需要及时且原则性的决策来维持高性能。传统启发式方法虽然简单可解释，但通常针对特定操作条件设计，在条件变化时可能导致效率低下。

Method: 基于Adv2框架，提出强化学习方法协调一组专家策略，利用优势函数进行权重更新。引入神经演员-评论家架构支持大规模状态空间，同时保持可解释性。建立了期望和高概率遗憾保证，推导了时间差分学习的有限时间偏差界限。

Result: 在随机匹配模型（包括器官交换场景）的模拟中，协调策略比单个专家和传统RL基准收敛更快，系统级效率更高。

Conclusion: 结构化自适应学习可以改进复杂资源分配和决策过程的建模与管理。

Abstract: Online matching problems arise in many complex systems, from cloud services
and online marketplaces to organ exchange networks, where timely, principled
decisions are critical for maintaining high system performance. Traditional
heuristics in these settings are simple and interpretable but typically
tailored to specific operating regimes, which can lead to inefficiencies when
conditions change. We propose a reinforcement learning (RL) approach that
learns to orchestrate a set of such expert policies, leveraging their
complementary strengths in a data-driven, adaptive manner. Building on the Adv2
framework (Jonckheere et al., 2024), our method combines expert decisions
through advantage-based weight updates and extends naturally to settings where
only estimated value functions are available. We establish both expectation and
high-probability regret guarantees and derive a novel finite-time bias bound
for temporal-difference learning, enabling reliable advantage estimation even
under constant step size and non-stationary dynamics. To support scalability,
we introduce a neural actor-critic architecture that generalizes across large
state spaces while preserving interpretability. Simulations on stochastic
matching models, including an organ exchange scenario, show that the
orchestrated policy converges faster and yields higher system level efficiency
than both individual experts and conventional RL baselines. Our results
highlight how structured, adaptive learning can improve the modeling and
management of complex resource allocation and decision-making processes.

</details>


### [14] [Q-Learning with Fine-Grained Gap-Dependent Regret](https://arxiv.org/abs/2510.06647)
*Haochen Zhang,Zhong Zheng,Lingzhou Xue*

Main category: stat.ML

TL;DR: 本文研究了表格马尔可夫决策过程中无模型强化学习的细粒度间隙依赖遗憾界，改进了UCB和非UCB算法的遗憾分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有无模型算法虽然达到了极小极大最坏情况遗憾，但其间隙依赖边界仍然粗糙，未能充分捕捉次优间隙的结构特征。

Method: 在UCB设置下开发了新的分析框架，明确分离最优和次优状态-动作对的分析；在非UCB设置下重新审视AMB算法，修正了Q值更新中的截断问题和集中性论证中的鞅差条件违反问题。

Result: 为UCB-Hoeffding算法建立了首个细粒度遗憾上界；提出的ULCB-Hoeffding算法在实验中优于AMB；改进的AMB版本实现了首个非UCB方法的严格细粒度间隙依赖遗憾保证。

Conclusion: 通过分离最优和次优状态-动作对的分析，以及修正现有算法的设计缺陷，成功建立了无模型强化学习的细粒度间隙依赖遗憾边界。

Abstract: We study fine-grained gap-dependent regret bounds for model-free
reinforcement learning in episodic tabular Markov Decision Processes. Existing
model-free algorithms achieve minimax worst-case regret, but their
gap-dependent bounds remain coarse and fail to fully capture the structure of
suboptimality gaps. We address this limitation by establishing fine-grained
gap-dependent regret bounds for both UCB-based and non-UCB-based algorithms. In
the UCB-based setting, we develop a novel analytical framework that explicitly
separates the analysis of optimal and suboptimal state-action pairs, yielding
the first fine-grained regret upper bound for UCB-Hoeffding (Jin et al., 2018).
To highlight the generality of this framework, we introduce ULCB-Hoeffding, a
new UCB-based algorithm inspired by AMB (Xu et al.,2021) but with a simplified
structure, which enjoys fine-grained regret guarantees and empirically
outperforms AMB. In the non-UCB-based setting, we revisit the only known
algorithm AMB, and identify two key issues in its algorithm design and
analysis: improper truncation in the $Q$-updates and violation of the
martingale difference condition in its concentration argument. We propose a
refined version of AMB that addresses these issues, establishing the first
rigorous fine-grained gap-dependent regret for a non-UCB-based method, with
experiments demonstrating improved performance over AMB.

</details>


### [15] [Gaussian Equivalence for Self-Attention: Asymptotic Spectral Analysis of Attention Matrix](https://arxiv.org/abs/2510.06685)
*Tomohiro Hayase,Benoît Collins,Ryo Karakida*

Main category: stat.ML

TL;DR: 本文首次建立了注意力机制的高斯等价性，证明在逆温度保持常数阶的自然机制下，注意力矩阵的奇异值分布可由线性模型渐近表征，且偏离了之前认为的Marchenko-Pastur分布。


<details>
  <summary>Details</summary>
Motivation: 自注意力层已成为现代深度神经网络的基本构建模块，但其理论理解仍然有限，特别是从随机矩阵理论的角度来看。

Method: 通过精确控制归一化项的波动和利用指数函数有利泰勒展开的精细化线性化方法，建立了注意力矩阵奇异值谱的严格分析。

Result: 证明了注意力矩阵奇异值分布偏离Marchenko-Pastur定律，并识别了线性化的阈值，阐明了注意力机制为何能在此机制下获得严格的高斯等价性。

Conclusion: 这项工作为注意力机制提供了首个高斯等价性结果，深化了对自注意力层理论性质的理解，特别是在随机矩阵理论框架下的分析。

Abstract: Self-attention layers have become fundamental building blocks of modern deep
neural networks, yet their theoretical understanding remains limited,
particularly from the perspective of random matrix theory. In this work, we
provide a rigorous analysis of the singular value spectrum of the attention
matrix and establish the first Gaussian equivalence result for attention. In a
natural regime where the inverse temperature remains of constant order, we show
that the singular value distribution of the attention matrix is asymptotically
characterized by a tractable linear model. We further demonstrate that the
distribution of squared singular values deviates from the Marchenko-Pastur law,
which has been believed in previous work. Our proof relies on two key
ingredients: precise control of fluctuations in the normalization term and a
refined linearization that leverages favorable Taylor expansions of the
exponential. This analysis also identifies a threshold for linearization and
elucidates why attention, despite not being an entrywise operation, admits a
rigorous Gaussian equivalence in this regime.

</details>


### [16] [Bayesian Nonparametric Dynamical Clustering of Time Series](https://arxiv.org/abs/2510.06919)
*Adrián Pérez-Herrero,Paulo Félix,Jesús Presedo,Carl Henrik Ek*

Main category: stat.ML

TL;DR: 提出了一种基于贝叶斯非参数方法的时序聚类模型，使用层次狄利克雷过程作为切换线性动态系统的先验，结合高斯过程建模振幅和时间对齐的统计变化，能够处理无界数量的时序聚类和未知数量的动态机制切换。


<details>
  <summary>Details</summary>
Motivation: 传统时序聚类方法难以处理无界数量的聚类和动态机制切换问题，需要一种能够避免不必要聚类扩散的建模方法。

Method: 使用层次狄利克雷过程作为切换线性动态系统的先验，结合高斯过程建模振幅和时间对齐的统计变化，通过变分下界进行离线和在线推理。

Result: 在多个公开心电数据库的案例研究中验证了方法的有效性和通用性。

Conclusion: 该方法能够以原则性方式建模时序模式的演化，避免不必要的聚类扩散，在时序分析中表现出良好的性能。

Abstract: We present a method that models the evolution of an unbounded number of time
series clusters by switching among an unknown number of regimes with linear
dynamics. We develop a Bayesian non-parametric approach using a hierarchical
Dirichlet process as a prior on the parameters of a Switching Linear Dynamical
System and a Gaussian process prior to model the statistical variations in
amplitude and temporal alignment within each cluster. By modeling the evolution
of time series patterns, the method avoids unnecessary proliferation of
clusters in a principled manner. We perform inference by formulating a
variational lower bound for off-line and on-line scenarios, enabling efficient
learning through optimization. We illustrate the versatility and effectiveness
of the approach through several case studies of electrocardiogram analysis
using publicly available databases.

</details>


### [17] [PyCFRL: A Python library for counterfactually fair offline reinforcement learning via sequential data preprocessing](https://arxiv.org/abs/2510.06935)
*Jianhan Zhang,Jitao Wang,Chengchun Shi,John D. Piette,Donglin Zeng,Zhenke Wu*

Main category: stat.ML

TL;DR: PyCFRL是一个Python库，用于在离线强化学习中确保反事实公平性，通过数据预处理算法学习公平策略并提供评估工具。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法在最大化整体利益的同时，可能会对少数群体或社会经济弱势群体造成不利影响，需要解决这种不公平问题。

Method: 开发了PyCFRL库，实现了一种新颖的数据预处理算法，从离线数据集中学习反事实公平的强化学习策略，并提供评估工具。

Result: PyCFRL库已公开发布在PyPI和GitHub上，提供了详细的使用教程和文档。

Conclusion: PyCFRL为强化学习中的公平性问题提供了实用的解决方案，有助于确保算法决策不会对特定群体产生不公平影响。

Abstract: Reinforcement learning (RL) aims to learn and evaluate a sequential decision
rule, often referred to as a "policy", that maximizes the population-level
benefit in an environment across possibly infinitely many time steps. However,
the sequential decisions made by an RL algorithm, while optimized to maximize
overall population benefits, may disadvantage certain individuals who are in
minority or socioeconomically disadvantaged groups. To address this problem, we
introduce PyCFRL, a Python library for ensuring counterfactual fairness in
offline RL. PyCFRL implements a novel data preprocessing algorithm for learning
counterfactually fair RL policies from offline datasets and provides tools to
evaluate the values and counterfactual unfairness levels of RL policies. We
describe the high-level functionalities of PyCFRL and demonstrate one of its
major use cases through a data example. The library is publicly available on
PyPI and Github (https://github.com/JianhanZhang/PyCFRL), and detailed
tutorials can be found in the PyCFRL documentation
(https://pycfrl-documentation.netlify.app).

</details>


### [18] [Root Cause Analysis of Outliers in Unknown Cyclic Graphs](https://arxiv.org/abs/2510.06995)
*Daniela Schkoda,Dominik Janzing*

Main category: stat.ML

TL;DR: 本文研究线性结构方程循环因果图中异常值的传播，提出了一种无需先验因果图知识即可识别潜在根本原因节点的方法。


<details>
  <summary>Details</summary>
Motivation: 研究循环因果图中异常值的传播机制，旨在开发一种能够追溯异常到根本原因节点的技术，即使在没有完整因果图知识的情况下。

Method: 基于线性结构方程，分析异常值在循环因果图中的传播路径，通过观察扰动传播模式来识别潜在的根本原因节点。

Result: 研究表明，只要扰动足够强且按正常模式的结构方程传播，就能识别出一个包含真实根本原因及其在循环中父节点的短列表。

Conclusion: 该方法能够在无需因果图先验知识的情况下，有效识别循环因果图中的根本原因节点，为异常诊断提供了实用工具。

Abstract: We study the propagation of outliers in cyclic causal graphs with linear
structural equations, tracing them back to one or several "root cause" nodes.
We show that it is possible to identify a short list of potential root causes
provided that the perturbation is sufficiently strong and propagates according
to the same structural equations as in the normal mode. This shortlist consists
of the true root causes together with those of its parents lying on a cycle
with the root cause. Notably, our method does not require prior knowledge of
the causal graph.

</details>


### [19] [Explaining Models under Multivariate Bernoulli Distribution via Hoeffding Decomposition](https://arxiv.org/abs/2510.07088)
*Baptiste Ferrere,Nicolas Bousquet,Fabrice Gamboa,Jean-Michel Loubes,Joseph Muré*

Main category: stat.ML

TL;DR: 本文针对伯努利分布的输入变量，提出了一个完整的广义Hoeffding分解框架，使得预测模型的可解释性分析变得明确和可行。


<details>
  <summary>Details</summary>
Motivation: 为了解决在相关随机输入变量情况下预测模型的行为解释问题，特别是在输入变量为伯努利分布时，需要建立明确的子模型分解方法。

Method: 基于L2子空间的斜投影概念，构建了广义Hoeffding分解，证明了在伯努利输入情况下L2子空间是一维的，并且函数分解是显式的。

Result: 获得了完整的可解释性框架，能够显式推导输入对输出预测的影响指标（如Sobol指数和Shapley效应），并通过数值实验验证了该方法在决策支持问题中的有效性。

Conclusion: 该方法为基于二进制决策图、布尔网络或二进制神经网络的问题提供了有用的分析工具，并为探索高维设置和有限可数输入模型的扩展提供了前景。

Abstract: Explaining the behavior of predictive models with random inputs can be
achieved through sub-models decomposition, where such sub-models have easier
interpretable features. Arising from the uncertainty quantification community,
recent results have demonstrated the existence and uniqueness of a generalized
Hoeffding decomposition for such predictive models when the stochastic input
variables are correlated, based on concepts of oblique projection onto L 2
subspaces. This article focuses on the case where the input variables have
Bernoulli distributions and provides a complete description of this
decomposition. We show that in this case the underlying L 2 subspaces are
one-dimensional and that the functional decomposition is explicit. This leads
to a complete interpretability framework and theoretically allows reverse
engineering. Explicit indicators of the influence of inputs on the output
prediction (exemplified by Sobol' indices and Shapley effects) can be
explicitly derived. Illustrated by numerical experiments, this type of analysis
proves useful for addressing decision-support problems, based on binary
decision diagrams, Boolean networks or binary neural networks. The article
outlines perspectives for exploring high-dimensional settings and, beyond the
case of binary inputs, extending these findings to models with finite countable
inputs.

</details>


### [20] [Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios](https://arxiv.org/abs/2510.07099)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: stat.ML

TL;DR: 提出DARL框架，结合去噪扩散概率模型和深度强化学习进行投资组合管理，通过生成市场崩溃情景增强训练数据鲁棒性，在风险调整收益和危机抵御方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法难以捕捉市场复杂动态和满足多样化投资者偏好，需要更强大的方法来应对市场不确定性。

Method: DARL框架将DDPMs与DRL结合，使用DDPMs生成不同压力强度下的合成市场崩溃情景，增强训练数据的鲁棒性。

Result: 实证评估显示DARL优于传统基准方法，在风险调整收益和抵御突发事件（如2025年关税危机）方面表现更优。

Conclusion: 该工作为DRL驱动的金融应用提供了增强压力弹性的稳健实用方法。

Abstract: In the ever-changing and intricate landscape of financial markets, portfolio
optimisation remains a formidable challenge for investors and asset managers.
Conventional methods often struggle to capture the complex dynamics of market
behaviour and align with diverse investor preferences. To address this, we
propose an innovative framework, termed Diffusion-Augmented Reinforcement
Learning (DARL), which synergistically integrates Denoising Diffusion
Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for
portfolio management. By leveraging DDPMs to generate synthetic market crash
scenarios conditioned on varying stress intensities, our approach significantly
enhances the robustness of training data. Empirical evaluations demonstrate
that DARL outperforms traditional baselines, delivering superior risk-adjusted
returns and resilience against unforeseen crises, such as the 2025 Tariff
Crisis. This work offers a robust and practical methodology to bolster stress
resilience in DRL-driven financial applications.

</details>


### [21] [Split Conformal Classification with Unsupervised Calibration](https://arxiv.org/abs/2510.07185)
*Santiago Mazuelas*

Main category: stat.ML

TL;DR: 提出了一种用于分类任务的无监督校准分割共形预测方法，使用无监督校准样本和已有的监督训练样本来构建集合预测规则，避免了需要额外标注样本进行校准的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分割共形预测方法需要使用与训练样本不同的标注样本进行校准，这既浪费了可用标注数据，又可能需要额外获取标注，使用不便。

Method: 使用无监督校准样本结合已有的监督训练样本来构建集合预测规则，实现无监督校准的分割共形预测。

Result: 理论和实验结果表明，该方法可以达到与监督校准相当的性能，但性能保证和计算效率会有适度下降。

Conclusion: 提出的无监督校准方法为分割共形预测提供了实用的替代方案，在保持性能的同时解决了标注数据利用效率问题。

Abstract: Methods for split conformal prediction leverage calibration samples to
transform any prediction rule into a set-prediction rule that complies with a
target coverage probability. Existing methods provide remarkably strong
performance guarantees with minimal computational costs. However, they require
to use calibration samples composed by labeled examples different to those used
for training. This requirement can be highly inconvenient, as it prevents the
use of all labeled examples for training and may require acquiring additional
labels solely for calibration. This paper presents an effective methodology for
split conformal prediction with unsupervised calibration for classification
tasks. In the proposed approach, set-prediction rules are obtained using
unsupervised calibration samples together with supervised training samples
previously used to learn the classification rule. Theoretical and experimental
results show that the presented methods can achieve performance comparable to
that with supervised calibration, at the expenses of a moderate degradation in
performance guarantees and computational efficiency.

</details>
