<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 15]
- [stat.AP](#stat.AP) [Total: 7]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Beyond Normality: Reliable A/B Testing with Non-Gaussian Data](https://arxiv.org/abs/2510.23666)
*Junpeng Gong,Chunkai Wang,Hao Li,Jinyong Ma,Haoxuan Li,Xu He*

Main category: stat.ML

TL;DR: 本文量化了偏态分布和不等样本分配对A/B测试中t检验误差率的影响，推导了确保t检验有效的最小样本量公式，并提出基于Edgeworth的校正方法来提高有限样本下的测试可靠性。


<details>
  <summary>Details</summary>
Motivation: 现实A/B测试中，当数据分布偏离正态性或处理组与对照组样本量不相等时，常用的配对t检验不再可靠，可能导致错误的决策。需要量化这些因素对误差率的影响并找到解决方案。

Method: 推导了t检验在偏态分布和不等样本分配情况下的最小样本量要求公式，并提出了基于Edgeworth展开的p值校正方法。

Result: 发现许多在线反馈指标需要数亿样本才能确保可靠的A/B测试。提出的校正方法在真实A/B测试平台上显著提高了测试的可靠性。

Conclusion: 传统t检验在现实条件下可能不可靠，需要大量样本或使用校正方法。提出的最小样本量阈值和校正方法为实际A/B测试提供了更可靠的工具。

Abstract: A/B testing has become the cornerstone of decision-making in online markets,
guiding how platforms launch new features, optimize pricing strategies, and
improve user experience. In practice, we typically employ the pairwise $t$-test
to compare outcomes between the treatment and control groups, thereby assessing
the effectiveness of a given strategy. To be trustworthy, these experiments
must keep Type I error (i.e., false positive rate) under control; otherwise, we
may launch harmful strategies. However, in real-world applications, we find
that A/B testing often fails to deliver reliable results. When the data
distribution departs from normality or when the treatment and control groups
differ in sample size, the commonly used pairwise $t$-test is no longer
trustworthy. In this paper, we quantify how skewed, long tailed data and
unequal allocation distort error rates and derive explicit formulas for the
minimum sample size required for the $t$-test to remain valid. We find that
many online feedback metrics require hundreds of millions samples to ensure
reliable A/B testing. Thus we introduce an Edgeworth-based correction that
provides more accurate $p$-values when the available sample size is limited.
Offline experiments on a leading A/B testing platform corroborate the practical
value of our theoretical minimum sample size thresholds and demonstrate that
the corrected method substantially improves the reliability of A/B testing in
real-world conditions.

</details>


### [2] [VIKING: Deep variational inference with stochastic projections](https://arxiv.org/abs/2510.23684)
*Samuel G. Fadel,Hrittik Roy,Nicholas Krämer,Yevgen Zainchkovskyy,Stas Syrota,Alejandro Valverde Mahou,Carl Henrik Ek,Søren Hauberg*

Main category: stat.ML

TL;DR: 提出一种新的变分平均场近似方法，通过考虑参数空间中的两个独立线性子空间来改进深度神经网络的贝叶斯推断，在多个任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统的变分平均场近似在处理过参数化的深度神经网络时表现不佳，导致训练不稳定、预测能力差和校准效果差。

Method: 构建一个简单的变分族，考虑参数空间中的两个独立线性子空间，分别表示训练数据支持范围内外的功能变化，从而建立完全相关的近似后验分布。

Result: 在多个任务、模型和数据集上观察到相比广泛基线方法的最先进性能。

Conclusion: 当构建反映重参数化几何特性的推断机制时，应用于深度神经网络的近似贝叶斯推断远非无望。

Abstract: Variational mean field approximations tend to struggle with contemporary
overparametrized deep neural networks. Where a Bayesian treatment is usually
associated with high-quality predictions and uncertainties, the practical
reality has been the opposite, with unstable training, poor predictive power,
and subpar calibration. Building upon recent work on reparametrizations of
neural networks, we propose a simple variational family that considers two
independent linear subspaces of the parameter space. These represent functional
changes inside and outside the support of training data. This allows us to
build a fully-correlated approximate posterior reflecting the
overparametrization that tunes easy-to-interpret hyperparameters. We develop
scalable numerical routines that maximize the associated evidence lower bound
(ELBO) and sample from the approximate posterior. Empirically, we observe
state-of-the-art performance across tasks, models, and datasets compared to a
wide array of baseline methods. Our results show that approximate Bayesian
inference applied to deep neural networks is far from a lost cause when
constructing inference mechanisms that reflect the geometry of
reparametrizations.

</details>


### [3] [Bayesian neural networks with interpretable priors from Mercer kernels](https://arxiv.org/abs/2510.23745)
*Alex Alberts,Ilias Bilionis*

Main category: stat.ML

TL;DR: 提出了一种新的贝叶斯神经网络先验方法——Mercer先验，使得BNN的样本能够近似指定的高斯过程，从而结合BNN的可扩展性和GP的可解释性优势。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络在不确定性量化中很重要，但其先验通常难以解释；而高斯过程虽然可解释性强，但难以扩展到大数据集。需要结合两者的优势。

Method: 通过协方差核的Mercer表示，直接在网络参数上定义先验，使得BNN的样本能够近似指定的高斯过程，不依赖特定的网络结构。

Result: 提出的Mercer先验方法能够使BNN产生近似高斯过程的样本，从而在保持可扩展性的同时获得有意义的贝叶斯解释。

Conclusion: Mercer先验为BNN提供了有意义的先验分布，结合了BNN的可扩展性和GP的可解释性，为科学和工程应用中的不确定性量化提供了更好的工具。

Abstract: Quantifying the uncertainty in the output of a neural network is essential
for deployment in scientific or engineering applications where decisions must
be made under limited or noisy data. Bayesian neural networks (BNNs) provide a
framework for this purpose by constructing a Bayesian posterior distribution
over the network parameters. However, the prior, which is of key importance in
any Bayesian setting, is rarely meaningful for BNNs. This is because the
complexity of the input-to-output map of a BNN makes it difficult to understand
how certain distributions enforce any interpretable constraint on the output
space. Gaussian processes (GPs), on the other hand, are often preferred in
uncertainty quantification tasks due to their interpretability. The drawback is
that GPs are limited to small datasets without advanced techniques, which often
rely on the covariance kernel having a specific structure. To address these
challenges, we introduce a new class of priors for BNNs, called Mercer priors,
such that the resulting BNN has samples which approximate that of a specified
GP. The method works by defining a prior directly over the network parameters
from the Mercer representation of the covariance kernel, and does not rely on
the network having a specific structure. In doing so, we can exploit the
scalability of BNNs in a meaningful Bayesian way.

</details>


### [4] [Understanding Fairness and Prediction Error through Subspace Decomposition and Influence Analysis](https://arxiv.org/abs/2510.23935)
*Enze Shi,Pankaj Bhagwat,Zhixian Yang,Linglong Kong,Bei Jiang*

Main category: stat.ML

TL;DR: 提出了一种基于充分降维的公平机器学习框架，通过分解特征空间来控制公平性与预测性能的权衡


<details>
  <summary>Details</summary>
Motivation: 传统公平方法只在预测层面施加约束，未能解决数据表示中的潜在偏见问题

Method: 使用充分降维将特征空间分解为目标相关、敏感和共享组件，通过选择性移除敏感信息来控制公平性-效用权衡

Result: 在合成和真实数据集上的实验验证了理论分析，表明该方法能有效提升公平性同时保持预测性能

Conclusion: 该方法提供了一种原则性的框架来调整数据表示，在平衡公平性和预测效用方面具有优势

Abstract: Machine learning models have achieved widespread success but often inherit
and amplify historical biases, resulting in unfair outcomes. Traditional
fairness methods typically impose constraints at the prediction level, without
addressing underlying biases in data representations. In this work, we propose
a principled framework that adjusts data representations to balance predictive
utility and fairness. Using sufficient dimension reduction, we decompose the
feature space into target-relevant, sensitive, and shared components, and
control the fairness-utility trade-off by selectively removing sensitive
information. We provide a theoretical analysis of how prediction error and
fairness gaps evolve as shared subspaces are added, and employ influence
functions to quantify their effects on the asymptotic behavior of parameter
estimates. Experiments on both synthetic and real-world datasets validate our
theoretical insights and show that the proposed method effectively improves
fairness while preserving predictive performance.

</details>


### [5] [Score-based constrained generative modeling via Langevin diffusions with boundary conditions](https://arxiv.org/abs/2510.23985)
*Adam Nordenhög,Akash Sharma*

Main category: stat.ML

TL;DR: 提出基于带有速度镜面反射的动力学朗之万扩散的约束生成模型，以及改进现有基于反射SDE的约束生成模型，提供高效数值采样器并进行模型比较。


<details>
  <summary>Details</summary>
Motivation: 基于SDE的得分生成模型在采样未知分布方面表现出色，但往往无法满足底层约束条件。

Method: 使用带有边界速度镜面反射的动力学朗之万扩散构建约束生成模型，提出高效数值采样器，并与基于局部时间的反射扩散模型进行比较。

Result: 开发了具有最优离散化步长收敛速率的数值采样器，对两种约束生成模型进行了全面比较。

Conclusion: 通过镜面反射和局部时间两种方法有效解决了约束生成问题，提供了实用的数值实现方案。

Abstract: Score-based generative models based on stochastic differential equations
(SDEs) achieve impressive performance in sampling from unknown distributions,
but often fail to satisfy underlying constraints. We propose a constrained
generative model using kinetic (underdamped) Langevin dynamics with specular
reflection of velocity on the boundary defining constraints. This results in
piecewise continuously differentiable noising and denoising process where the
latter is characterized by a time-reversed dynamics restricted to a domain with
boundary due to specular boundary condition. In addition, we also contribute to
existing reflected SDEs based constrained generative models, where the
stochastic dynamics is restricted through an abstract local time term. By
presenting efficient numerical samplers which converge with optimal rate in
terms of discretizations step, we provide a comprehensive comparison of models
based on confined (specularly reflected kinetic) Langevin diffusion with models
based on reflected diffusion with local time.

</details>


### [6] [Copula-Stein Discrepancy: A Generator-Based Stein Operator for Archimedean Dependence](https://arxiv.org/abs/2510.24056)
*Agnideep Aich,Ashit Baran Aich*

Main category: stat.ML

TL;DR: 提出了Copula-Stein Discrepancy (CSD)，一种专门针对统计依赖结构的新型差异度量，解决了传统Kernel Stein Discrepancies在检测高阶依赖结构（如尾部依赖）方面的不足。


<details>
  <summary>Details</summary>
Motivation: 标准KSD通常对高阶依赖结构不敏感，而这些结构在科学和金融领域至关重要。需要一种能够专门检测依赖结构不匹配的差异度量方法。

Method: 通过在copula密度上直接定义Stein算子，利用依赖的生成结构而非联合密度的得分函数。对于Archimedean copulas，该方法从标量生成函数推导出闭式Stein核。

Result: 理论分析证明CSD能够度量copula分布的弱收敛，确保检测任何依赖不匹配；经验估计器以最优速率收敛；对尾部依赖系数差异具有可证明的敏感性。计算上，精确CSD核评估在维度上线性扩展，随机特征近似将n依赖从二次降低到近线性。

Conclusion: CSD为依赖感知推断提供了一个实用且理论上有原则的工具，特别适用于检测尾部依赖等复杂依赖结构。

Abstract: Kernel Stein discrepancies (KSDs) have become a principal tool for
goodness-of-fit testing, but standard KSDs are often insensitive to
higher-order dependency structures, such as tail dependence, which are critical
in many scientific and financial domains. We address this gap by introducing
the Copula-Stein Discrepancy (CSD), a novel class of discrepancies tailored to
the geometry of statistical dependence. By defining a Stein operator directly
on the copula density, CSD leverages the generative structure of dependence,
rather than relying on the joint density's score function. For the broad class
of Archimedean copulas, this approach yields a closed-form Stein kernel derived
from the scalar generator function. We provide a comprehensive theoretical
analysis, proving that CSD (i) metrizes weak convergence of copula
distributions, ensuring it detects any mismatch in dependence; (ii) has an
empirical estimator that converges at the minimax optimal rate of
$O_P(n^{-1/2})$; and (iii) is provably sensitive to differences in tail
dependence coefficients. The framework is extended to general non-Archimedean
copulas, including elliptical and vine copulas. Computationally, the exact CSD
kernel evaluation scales linearly in dimension, while a novel random feature
approximation reduces the $n$-dependence from quadratic $O(n^2)$ to near-linear
$\tilde{O}(n)$, making CSD a practical and theoretically principled tool for
dependence-aware inference.

</details>


### [7] [Self-Concordant Perturbations for Linear Bandits](https://arxiv.org/abs/2510.24187)
*Lucas Lévy,Jean-Lou Valeau,Arya Akhavan,Patrick Rebeschini*

Main category: stat.ML

TL;DR: 提出了一个统一的算法框架，将FTRL和FTPL方法联系起来，并引入自协调扰动来设计新的FTPL算法，在超立方体和欧几里得球上实现了O(d√(n ln n))的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性线性老虎机问题，旨在统一FTRL和FTPL方法，并改进现有算法在超立方体上的性能。

Method: 引入自协调扰动概率分布，结合自协调正则化和高效随机探索，设计基于FTPL的新算法。

Result: 在d维超立方体和欧几里得球上均实现了O(d√(n ln n))的遗憾界。在超立方体上比现有自协调FTRL方法改进了√d倍，在欧几里得球上达到相同性能。

Conclusion: 成功建立了FTRL和FTPL方法的统一框架，通过自协调扰动设计的新算法在超立方体上达到了最优遗憾界（对数因子内）。

Abstract: We study the adversarial linear bandits problem and present a unified
algorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and
Follow-the-Perturbed-Leader (FTPL) methods, extending the known connection
between them from the full-information setting. Within this framework, we
introduce self-concordant perturbations, a family of probability distributions
that mirror the role of self-concordant barriers previously employed in the
FTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based
algorithm that combines self-concordant regularization with efficient
stochastic exploration. Our approach achieves a regret of $O(d\sqrt{n \ln n})$
on both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean
ball, this matches the rate attained by existing self-concordant FTRL methods.
For the hypercube, this represents a $\sqrt{d}$ improvement over these methods
and matches the optimal bound up to logarithmic factors.

</details>


### [8] [Comparison of generalised additive models and neural networks in applications: A systematic review](https://arxiv.org/abs/2510.24601)
*Jessica Doohan,Lucas Kook,Kevin Burke*

Main category: stat.ML

TL;DR: 通过对143篇论文430个数据集的系统回顾发现，广义可加模型(GAMs)和神经网络在表格数据上没有一致的性能优势，两者应被视为互补方法而非竞争对手。


<details>
  <summary>Details</summary>
Motivation: 比较神经网络和广义可加模型在真实世界表格数据上的性能表现，分析影响性能差异的因素，为模型选择提供实证依据。

Method: 遵循PRISMA指南进行系统文献回顾，提取论文和数据集层面的关键属性，使用混合效应模型分析报告的性能指标。

Result: 在常用指标(RMSE、R²、AUC)上没有发现GAMs或神经网络的持续优势。神经网络在大型数据集和更多预测变量的情况下表现更好，但优势随时间减弱。GAMs在小数据场景中保持竞争力且具有可解释性。

Conclusion: GAMs和神经网络应被视为互补方法。对于许多表格应用，性能差异不大，而可解释性可能更倾向于GAMs。文献中数据集特征和神经网络复杂度的报告不完整，限制了透明度和可重复性。

Abstract: Neural networks have become a popular tool in predictive modelling, more
commonly associated with machine learning and artificial intelligence than with
statistics. Generalised Additive Models (GAMs) are flexible non-linear
statistical models that retain interpretability. Both are state-of-the-art in
their own right, with their respective advantages and disadvantages. This paper
analyses how these two model classes have performed on real-world tabular data.
Following PRISMA guidelines, we conducted a systematic review of papers that
performed empirical comparisons of GAMs and neural networks. Eligible papers
were identified, yielding 143 papers, with 430 datasets. Key attributes at both
paper and dataset levels were extracted and reported. Beyond summarising
comparisons, we analyse reported performance metrics using mixed-effects
modelling to investigate potential characteristics that can explain and
quantify observed differences, including application area, study year, sample
size, number of predictors, and neural network complexity. Across datasets, no
consistent evidence of superiority was found for either GAMs or neural networks
when considering the most frequently reported metrics (RMSE, $R^2$, and AUC).
Neural networks tended to outperform in larger datasets and in those with more
predictors, but this advantage narrowed over time. Conversely, GAMs remained
competitive, particularly in smaller data settings, while retaining
interpretability. Reporting of dataset characteristics and neural network
complexity was incomplete in much of the literature, limiting transparency and
reproducibility. This review highlights that GAMs and neural networks should be
viewed as complementary approaches rather than competitors. For many tabular
applications, the performance trade-off is modest, and interpretability may
favour GAMs.

</details>


### [9] [Statistical physics of deep learning: Optimal learning of a multi-layer perceptron near interpolation](https://arxiv.org/abs/2510.24616)
*Jean Barbier,Francesco Camilli,Minh-Toan Nguyen,Mauro Pastore,Rudy Skerk*

Main category: stat.ML

TL;DR: 该论文通过统计物理方法分析多层感知机的监督学习，研究了特征学习机制，揭示了深度神经网络在插值区域的学习极限和相变现象。


<details>
  <summary>Details</summary>
Motivation: 解决统计物理框架能否分析深度学习中丰富的特征学习效应，超越之前研究的窄网络或核方法，探索深度神经网络在特征学习机制下的基本学习极限。

Method: 采用匹配的师生设置，研究多层感知机在宽度与输入维度成比例、训练参数与数据量相当的插值区域下的监督学习，使用贝叶斯最优方法分析。

Result: 发现了丰富的学习相变现象：模型通过向目标"专业化"实现最优性能，但这种专业化在不同层和神经元中不均匀传播，深层目标更难学习，训练算法可能被理论预测的次优解吸引。

Conclusion: 贝叶斯最优设置提供了关于深度、非线性和有限宽度如何影响特征学习机制的重要见解，这些见解可能具有超越该设置的广泛相关性。

Abstract: For three decades statistical physics has been providing a framework to
analyse neural networks. A long-standing question remained on its capacity to
tackle deep learning models capturing rich feature learning effects, thus going
beyond the narrow networks or kernel methods analysed until now. We positively
answer through the study of the supervised learning of a multi-layer
perceptron. Importantly, (i) its width scales as the input dimension, making it
more prone to feature learning than ultra wide networks, and more expressive
than narrow ones or with fixed embedding layers; and (ii) we focus on the
challenging interpolation regime where the number of trainable parameters and
data are comparable, which forces the model to adapt to the task. We consider
the matched teacher-student setting. It provides the fundamental limits of
learning random deep neural network targets and helps in identifying the
sufficient statistics describing what is learnt by an optimally trained network
as the data budget increases. A rich phenomenology emerges with various
learning transitions. With enough data optimal performance is attained through
model's "specialisation" towards the target, but it can be hard to reach for
training algorithms which get attracted by sub-optimal solutions predicted by
the theory. Specialisation occurs inhomogeneously across layers, propagating
from shallow towards deep ones, but also across neurons in each layer.
Furthermore, deeper targets are harder to learn. Despite its simplicity, the
Bayesian-optimal setting provides insights on how the depth, non-linearity and
finite (proportional) width influence neural networks in the feature learning
regime that are potentially relevant way beyond it.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [10] [Multi-Dimensional Wasserstein Distance Implementation in Scipy](https://arxiv.org/abs/2510.23651)
*Zehao Lu*

Main category: stat.CO

TL;DR: 本文在Scipy中实现了基于线性规划的多维Wasserstein距离计算功能，扩展了原有的一维距离计算能力。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离是衡量概率分布相似性的重要指标，但Scipy库仅支持一维情况，需要扩展到多维场景以满足更广泛的应用需求。

Method: 将多维Wasserstein距离计算问题转化为线性规划问题，利用Scipy的线性规划求解器进行有效求解。

Result: 开发了包含完整文档和测试用例的实现，该功能将被合并到Scipy主开发分支并在未来版本中发布。

Conclusion: 该实现显著增强了Scipy在多维统计分析领域的能力，为研究人员提供了便利的多维Wasserstein距离计算工具。

Abstract: The Wasserstein distance, also known as the Earth mover distance or optimal
transport distance, is a widely used measure of similarity between probability
distributions. This paper presents an linear programming based implementation
of the multi-dimensional Wasserstein distance function in Scipy, a powerful
scientific computing package in Python. Building upon the existing
one-dimensional scipy.stats.wasserstein_distance function, our work extends its
capabilities to handle multi-dimensional distributions. To compute the
multi-dimensional Wasserstein distance, we developed an implementation that
transforms the problem into a linear programming problem. We utilized the scipy
linear programming solver to effectively solve this transformed problem. The
proposed implementation includes thorough documentation and comprehensive test
cases to ensure accuracy and reliability. The resulting feature is set to be
merged into the main Scipy development branch and will be included in the
upcoming release, further enhancing the capabilities of Scipy in the field of
multi-dimensional statistical analysis.

</details>


### [11] [Fast Bayesian Multilevel Quasi-Monte Carlo](https://arxiv.org/abs/2510.24604)
*Aleksei G. Sorokin,Pieterjan Robbe,Gianluca Geraci,Michael S. Eldred,Fred J. Hickernell*

Main category: stat.CO

TL;DR: 提出了一种基于贝叶斯求积框架的多层准蒙特卡洛方法，使用单一低差异序列并通过高斯过程后验方差量化数值误差，相比传统方法更高效。


<details>
  <summary>Details</summary>
Motivation: 传统多层准蒙特卡洛方法使用多个独立随机化的低差异序列来估计统计误差，这种方法效率较低。单一低差异序列虽然更高效，但无法在现有框架下估计统计误差。

Method: 将MLQMC问题重新构建在贝叶斯求积框架中，使用单一低差异序列和高斯过程模型。提出了新的贝叶斯效用函数和自适应平滑度的数字移位不变核，计算复杂度为O(n log n)。

Result: 数值实验表明，该方法在单层和多层问题中表现良好，使用数字网获得的贝叶斯误差估计可靠，尽管在某些情况下略显保守。

Conclusion: 提出的快速贝叶斯MLQMC方法提供了一种更高效的多层准蒙特卡洛计算框架，能够可靠地估计数值误差，同时保持较低的计算复杂度。

Abstract: Existing multilevel quasi-Monte Carlo (MLQMC) methods often rely on multiple
independent randomizations of a low-discrepancy (LD) sequence to estimate
statistical errors on each level. While this approach is standard, it can be
less efficient than simply increasing the number of points from a single LD
sequence. However, a single LD sequence does not permit statistical error
estimates in the current framework. We propose to recast the MLQMC problem in a
Bayesian cubature framework, which uses a single LD sequence and quantifies
numerical error through the posterior variance of a Gaussian process (GP)
model. When paired with certain LD sequences, GP regression and hyperparameter
optimization can be carried out at only $\mathcal{O}(n \log n)$ cost, where $n$
is the number of samples. Building on the adaptive sample allocation used in
traditional MLQMC, where the number of samples is doubled on the level with the
greatest expected benefit, we introduce a new Bayesian utility function that
balances the computational cost of doubling against the anticipated reduction
in posterior uncertainty. We also propose a new digitally-shift-invariant (DSI)
kernel of adaptive smoothness, which combines multiple higher-order DSI kernels
through a weighted sum of smoothness parameters, for use with fast digital net
GPs. A series of numerical experiments illustrate the performance of our fast
Bayesian MLQMC method and error estimates for both single-level problems and
multilevel problems with a fixed number of levels. The Bayesian error estimates
obtained using digital nets are found to be reliable, although, in some cases,
mildly conservative.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [12] [A Frequency-Domain NonStationarity Test for dependent data](https://arxiv.org/abs/2510.24319)
*Mohamedou Ould Haye,Anne Philippe*

Main category: stat.ME

TL;DR: 提出了一种基于多时期周期图评估的无参数检验方法，用于区分长记忆行为和非平稳性。


<details>
  <summary>Details</summary>
Motivation: 现有的平稳性检验要么无法处理长记忆过程，要么在平稳性和非平稳性边界附近表现出较差的经验尺度。长记忆行为和非平稳性都会产生缓慢衰减的样本自协方差，难以区分。

Method: 基于多时期周期图评估的无参数检验程序，推导了在平稳性和非平稳性假设下的极限分布，这些分布可解析表示为加权独立χ²随机变量的有限和。

Result: 模拟研究表明，与现有方法相比，所提出的方法表现良好。

Conclusion: 该方法为区分长记忆行为和非平稳性提供了一种有效的无参数检验工具，在边界情况下表现优于现有方法。

Abstract: Distinguishing long-memory behaviour from nonstationarity is challenging, as
both produce slowly decaying sample autocovariances. Existing stationarity
tests either fail to account for long-memory processes or exhibit poor
empirical size, particularly near the boundary between stationarity and
nonstationarity. We propose a new, parameter-free testing procedure based on
the evaluation of periodograms across multiple epochs. The limiting
distributions derived here are obtained under stationarity and nonstationarity
assumptions and analytically tractable, expressed as finite sums of weighted
independent $\chi^2$ random variables. Simulation studies indicate that the
proposed method performs favorably compared to existing approaches.

</details>


### [13] [Testing-driven Variable Selection in Bayesian Modal Regression](https://arxiv.org/abs/2510.23831)
*Jiasong Duan,Hongmei Zhang,Xianzheng Huang*

Main category: stat.ME

TL;DR: 提出了一种用于重尾响应的贝叶斯变量选择方法，基于模态回归框架，通过EM算法加速参数估计，并构建检验统计量来区分重要和不重要的协变量。


<details>
  <summary>Details</summary>
Motivation: 针对重尾响应数据和非高斯模型误差的情况，需要开发有效的变量选择方法来识别重要协变量。

Method: 在模态回归框架下采用贝叶斯变量选择方法，使用期望最大化（EM）算法进行参数估计，并构建基于模型误差分布形状的检验统计量。

Result: 通过模拟研究验证了该方法在非高斯模型误差存在时识别重要协变量的有效性，并在遗传和表观遗传研究数据集上进行了应用。

Conclusion: 该方法能够有效处理重尾响应数据，在非高斯误差条件下准确识别重要变量，适用于遗传和表观遗传研究等实际应用场景。

Abstract: We propose a Bayesian variable selection method in the framework of modal
regression for heavy-tailed responses. An efficient expectation-maximization
algorithm is employed to expedite parameter estimation. A test statistic is
constructed to exploit the shape of the model error distribution to effectively
separate informative covariates from unimportant ones. Through simulations, we
demonstrate and evaluate the efficacy of the proposed method in identifying
important covariates in the presence of non-Gaussian model errors. Finally, we
apply the proposed method to analyze two datasets arising in genetic and
epigenetic studies.

</details>


### [14] [A Random Forest Inverse Probability Weighted Pseudo-Observation Framework for Alternating Recurrent Events](https://arxiv.org/abs/2510.23764)
*Abigail Loe,Susan Murray,Zhenke Wu*

Main category: stat.ME

TL;DR: 提出了一种用于分析交替复发事件的回归框架，使用随机森林逆概率加权策略处理因交替状态导致的缺失风险期问题。


<details>
  <summary>Details</summary>
Motivation: 在医疗、社会和行为研究中，交替复发事件很常见，其中主要事件触发后进入治疗恢复期（次要事件）。数据删失和主要事件期间缺失风险期的问题需要特殊处理。

Method: 采用随机森林逆概率加权策略，避免因交替次要状态导致的缺失信息偏倚，估计τ限制的主要事件平均时间。

Result: 模拟研究表明该方法在交替事件时间独立或相关时表现良好。应用于移动健康研究数据，评估自我关怀推送通知对脑外伤患者照顾者心理状态的影响。

Conclusion: 提出的框架能够有效处理删失交替复发事件的分析，解决了交替状态导致的缺失风险期问题，在真实数据应用中展示了实用性。

Abstract: Alternating recurrent events, where subjects experience two potentially
correlated event types over time, are common in healthcare, social, and
behavioral studies. Often there is a primary event of interest that, when
triggered, initiates a period of treatment and recovery measured via a
secondary time-to-event. For example, cancer patients can experience repeated
blood clotting emergencies that require hospitalization followed by discharge,
people with alcohol use disorder can have periods of addiction and sobriety, or
care partners can experience periods of depression and recovery. Potential
censoring of the data requires special handling. Overlaying this are the
missing at-risk periods for the primary event type when individuals have
initiated the primary event but not reached the subsequent secondary event. In
this paper, we develop a framework for regression analysis of censored
alternating recurrent events that uses a random forest inverse probability
weighting strategy to avoid bias in the analysis of the time to the primary
event due to informative missingness from the alternate secondary state. The
proposed regression model estimates $\tau$-restricted mean time to the primary
event of interest while taking into account complexities of censored.
Simulations show good performance of our method when the alternate
times-to-event are either independent or correlated. We analyze a mobile health
study data to evaluate the impact of self-care push notifications on the mental
state of caregivers of traumatic brain injury patients.

</details>


### [15] [Self-Normalized Quantile Empirical Saddlepoint Approximation](https://arxiv.org/abs/2510.24352)
*Hou Jian,Meng Tan,Tian Maozai*

Main category: stat.ME

TL;DR: 提出了一种无需密度估计的频数推断方法SNQESA，用于总体分位数推断，通过自归一化枢轴量和约束经验鞍点近似获得高精度尾部概率。


<details>
  <summary>Details</summary>
Motivation: 解决基于核的Wald/Hall-Sheather区间存在的带宽选择和边界问题，为偏态、重尾分布和极端分位数提供可靠的推断方法。

Method: 构建自归一化枢轴量，使用约束经验鞍点近似计算尾部概率，通过反演获得置信区间和检验，无需估计目标分位数的未知密度。

Result: 在轻度局部正则条件下，获得高阶尾部精度和二阶覆盖精度；蒙特卡洛实验显示在小到中等样本中提供稳定覆盖和竞争性区间长度，计算速度比大B重采样方案快几个数量级。

Conclusion: SNQESA为无分布分位数推断提供了一个实用、解析透明的替代方案，可扩展到两样本分位数差异和回归类型设置。

Abstract: We propose a density-free method for frequentist inference on population
quantiles, termed Self-Normalized Quantile Empirical Saddlepoint Approximation
(SNQESA). The approach builds a self-normalized pivot from the indicator score
for a fixed quantile threshold and then employs a constrained empirical
saddlepoint approximation to obtain highly accurate tail probabilities.
Inverting these tail areas yields confidence intervals and tests without
estimating the unknown density at the target quantile, thereby eliminating
bandwidth selection and the boundary issues that affect kernel-based
Wald/Hall-Sheather intervals. Under mild local regularity, the resulting
procedures attain higher-order tail accuracy and second-order coverage after
inversion. Because the pivot is anchored in a bounded Bernoulli reduction, the
method remains reliable for skewed and heavy-tailed distributions and for
extreme quantiles. Extensive Monte Carlo experiments across light, heavy, and
multimodal distributions demonstrate that SNQESA delivers stable coverage and
competitive interval lengths in small to moderate samples while being orders of
magnitude faster than large-B resampling schemes. An empirical study on
Value-at-Risk with rolling windows further highlights the gains in tail
performance and computational efficiency. The framework naturally extends to
two-sample quantile differences and to regression-type settings, offering a
practical, analytically transparent alternative to kernel, bootstrap, and
empirical-likelihood methods for distribution-free quantile inference.

</details>


### [16] [ETZ: A Modeling Principle for Confirmability of Drug-Development Studies](https://arxiv.org/abs/2510.23799)
*Yujia Sun,Yang Han,Xingya Wang,Szu-Yu Tang,Yushi Liu,Jason C. Hsu*

Main category: stat.ME

TL;DR: 提出了一种改进药物开发从二期到三期过渡确认性的方法框架，用置信集推断替代多重假设检验，用置信有界分位数框架替代传统功效和样本量计算。


<details>
  <summary>Details</summary>
Motivation: 药物开发从二期到三期过渡的成功率只有约40%，而三期到批准的成功率仅约50%，需要提高确认性。

Method: 采用基于分区原则的定向置信集推断，结合了枢轴化和Neyman置信集构建的优点；提出置信有界分位数框架，遵循正确有用推断原则；使用ETZ建模原则量化三个变异性成分对确认性的影响。

Result: 该方法框架能够更可靠地决定是否进入确认性研究以及确定研究终点，消除了传统功效计算中非正式折扣的需要。

Conclusion: 提出的方法框架通过置信集推断和置信有界分位数，显著提高了药物开发过渡阶段的确认性，为赞助商提供了有针对性的变异性减少投资决策支持。

Abstract: Transitioning from Phase 2 to Phase 3 in drug development, at a rate of
$\approx$40%, is the most stringent among phase transitions (Hay et al.
(2014)). Yet, success rate at Phase 3 leading to approval is only $\approx$50%
(Arrowsmith (2011b)). To improve Confirmability, we propose a methodological
shift: replacing multiple hypothesis testing with inference based on confidence
sets, and substituting conventional power and sample size calculations with a
Confidently Bounded Quantile (CBQ) framework.
  Our confidence set inferences to answer the questions of whether to
transition to a Confirmatory study as well as what to designate as the endpoint
in that study. Construction of our directed confidence sets follows the
Partitioning Principle, taking the best of each of Pivoting and Neyman
Confidence Set Construction.
  Rooted in Tukey's Confidently Bounded Allowance (CBA) (Tukey (1994a)), our
proposed CBQ makes the transitioning decision following the Correct and Useful
Inference principle in Hsu (1996). CBQ removes from "power" the probability of
rejecting for wrong reasons, eliminating the need for informal discounting in
power calculation that has existed in the biopharmaceutical industry.
  ETZ, the modeling principle proposed in Wang et al. (2025), quantifies the
impact of three variability components on confirmability. In repeated-measures
RCTs, it separates within-subject and between-subject variability, further
dividing the latter into baseline and trajectory components. This enables
informed investment decisions for the sponsors on targeting variability
reduction to improve confirmability. A Shiny-based Confirmability App supports
all computations.

</details>


### [17] [From Stochasticity to Signal: A Bayesian Latent State Model for Reliable Measurement with LLMs](https://arxiv.org/abs/2510.23874)
*Yichi Zhang,Ignacio Martinez*

Main category: stat.ME

TL;DR: 提出贝叶斯潜在状态模型解决LLM分类任务中的随机性测量误差问题，通过将LLM变异性视为统计测量误差来准确估计分类参数。


<details>
  <summary>Details</summary>
Motivation: LLM在商业分类任务中的随机性会产生显著测量误差，传统单次输出或多数投票方法无法量化不确定性且会产生有偏估计。

Method: 将真实分类视为未观测潜在变量，多个LLM评级作为该状态的噪声测量，建立贝叶斯潜在状态模型同时估计错误率、基础率、真实状态后验概率和因果影响。

Result: 模拟研究表明该模型能准确恢复真实参数，而朴素方法会失败。

Conclusion: 该方法为将LLM的噪声概率输出转化为科学和商业应用中准确可操作的洞察提供了通用可靠框架。

Abstract: Large Language Models (LLMs) are increasingly used to automate classification
tasks in business, such as analyzing customer satisfaction from text. However,
the inherent stochasticity of LLMs, in terms of their tendency to produce
different outputs for the same input, creates a significant measurement error
problem that is often neglected with a single round of output, or addressed
with ad-hoc methods like majority voting. Such naive approaches fail to
quantify uncertainty and can produce biased estimates of population-level
metrics. In this paper, we propose a principled solution by reframing LLM
variability as a statistical measurement error problem and introducing a
Bayesian latent state model to address it. Our model treats the true
classification (e.g., customer dissatisfaction) as an unobserved latent
variable and the multiple LLM ratings as noisy measurements of this state. This
framework allows for the simultaneous estimation of the LLM's false positive
and false negative error rates, the underlying base rate of the phenomenon in
the population, the posterior probability of the true state for each individual
observation, and the causal impact of a business intervention, if any, on the
latent state. Through simulation studies, we demonstrate that our model
accurately recovers true parameters where naive methods fail. We conclude that
this methodology provides a general and reliable framework for converting
noisy, probabilistic outputs from LLMs into accurate and actionable insights
for scientific and business applications.

</details>


### [18] [Machine-Learning-Assisted Comparison of Regression Functions](https://arxiv.org/abs/2510.24714)
*Jian Yan,Zhuoxi Li,Yang Ning,Yong Chen*

Main category: stat.ME

TL;DR: 提出了两种基于核条件均值依赖性的新检验方法，用于比较回归函数，克服了维度诅咒问题，适用于固定和高维场景。


<details>
  <summary>Details</summary>
Motivation: 重新审视比较回归函数的经典问题，现有方法受维度诅咒限制，需要开发更灵活且适用性更广的检验方法。

Method: 基于广义核条件均值依赖性重新表述原假设，利用现代机器学习方法进行灵活估计，构建两种新检验统计量。

Result: 建立了检验统计量的渐近性质，在固定和高维情况下均有效，仅需温和矩条件，数值研究验证了方法的有效性。

Conclusion: 提出的检验方法克服了维度诅咒，放宽了分布假设，为数据集成、迁移学习和因果推断等应用提供了有效的回归函数比较工具。

Abstract: We revisit the classical problem of comparing regression functions, a
fundamental question in statistical inference with broad relevance to modern
applications such as data integration, transfer learning, and causal inference.
Existing approaches typically rely on smoothing techniques and are thus
hindered by the curse of dimensionality. We propose a generalized notion of
kernel-based conditional mean dependence that provides a new characterization
of the null hypothesis of equal regression functions. Building on this
reformulation, we develop two novel tests that leverage modern machine learning
methods for flexible estimation. We establish the asymptotic properties of the
test statistics, which hold under both fixed- and high-dimensional regimes.
Unlike existing methods that often require restrictive distributional
assumptions, our framework only imposes mild moment conditions. The efficacy of
the proposed tests is demonstrated through extensive numerical studies.

</details>


### [19] [Nonparametric Identification and Estimation of Ratios of Multi-Category Means under Preferential Sampling](https://arxiv.org/abs/2510.23920)
*Grant Hopkins,Sarah Teichman,Ellen Graham,Amy D Willis*

Main category: stat.ME

TL;DR: 本文提出了一个非参数框架，用于估计多类别数据中类别特定均值的比率，解决了优先抽样下的可识别性问题，并开发了双稳健的靶向最小损失估计器。


<details>
  <summary>Details</summary>
Motivation: 多类别数据在多个领域广泛存在，但现有方法通常依赖参数分布或条件均值限制。本文旨在在完全非参数设置下解决类别特定均值比率的估计问题，允许观测单元和类别被优先抽样。

Method: 通过独立性假设或类别约束（如参考类别或中心化函数）实现可识别性，开发了高效的、双稳健的靶向最小损失估计器，特别适用于大量不频繁观测类别的情况。

Result: 该方法在有限样本下表现优异，通过模拟对比验证了其性能，并成功应用于识别腹泻病例与对照组中差异丰富的细菌。

Conclusion: 本文提供了一个无需数据分布参数假设的通用框架，用于研究组合数据设置中的参数可识别性问题。

Abstract: Multi-category data arise in diverse fields including marketing, chemistry,
public policy, genomics, political science, and ecology. We consider the
problem of estimating ratios of category-specific means in a fully
nonparametric setting, allowing for both observational units and categories to
be preferentially sampled. We consider covariate-adjusted and unadjusted
estimands that are non-parametrically defined and straightforward to interpret.
While identifiability for related models has been established through
parametric distributions or restrictions on the conditional mean (e.g.,
log-linearity), we show that identifiability can be obtained through an
independence assumption or a category constraint, such as a reference category
or a centering function. We develop an efficient, doubly-robust targeted
minimum loss based estimator with excellent finite-sample performance,
including in the setting of a large number of infrequently observed categories.
We contrast the performance of our method with related approaches via
simulation, and apply it to identify bacteria that are differentially abundant
in diarrheal cases compared to controls. Our work provides a general framework
for studying parameter identifiability in compositional data settings without
requiring parametric assumptions on the data distribution.

</details>


### [20] [Illustrating implications of misaligned causal questions and statistics in settings with competing events and interest in treatment mechanisms](https://arxiv.org/abs/2510.24018)
*Takuya Kawahara,Sean McGrath,Jessica G Young*

Main category: stat.ME

TL;DR: 本文探讨了在存在竞争事件的情况下，错误应用针对受控直接效应的估计器来估计可分离直接效应的问题，并比较了这两种效应在不同数据生成场景下的差异。


<details>
  <summary>Details</summary>
Motivation: 在竞争事件存在时，研究者通常关注对目标事件的直接治疗效果，但经典生存分析方法将竞争事件视为删失事件，只能估计受控直接效应，这在临床上往往难以想象且不相关。可分离直接效应可能更好地符合研究者的因果问题。

Method: 通过分析错误应用受控直接效应估计器（即"竞争事件删失"）来估计可分离直接效应的后果，比较两种效应在不同数据生成场景下的取值差异，并使用逆概率删失加权与专门为可分离效应设计的加权估计器进行实证比较。

Result: 研究发现受控直接效应和可分离直接效应可能取不同值，甚至符号相反，且这两种效应在各自识别条件违反或接近违反时的受影响程度不同。

Conclusion: 在存在竞争事件的情况下，应根据研究者的具体因果问题选择合适的直接效应定义和相应的估计方法，避免错误应用估计器导致有偏估计。

Abstract: In the presence of competing events, many investigators are interested in a
direct treatment effect on the event of interest that does not capture
treatment effects on competing events. Classical survival analysis methods that
treat competing events like censoring events, at best, target a controlled
direct effect: the effect of the treatment under a difficult to imagine and
typically clinically irrelevant scenario where competing events are somehow
eliminated. A separable direct effect, quantifying the effect of a future
modified version of the treatment, is an alternative direct effect notion that
may better align with an investigator's underlying causal question. In this
paper, we provide insights into the implications of naively applying an
estimator constructed for a controlled direct effect (i.e., "censoring by
competing events") when the actual causal effect of interest is a separable
direct effect. We illustrate the degree to which controlled and separable
direct effects may take different values, possibly even different signs, and
the degree to which these two different effects may be differentially impacted
by violation and/or near violation of their respective identifying conditions
under a range of data generating scenarios. Finally, we provide an empirical
comparison of inverse probability of censoring weighting to an alternative
weighted estimator specifically structured for a separable effect using data
from a randomized trial of estrogen therapy and prostate cancer mortality.

</details>


### [21] [Quantifying inconsistency in one-stage individual participant data meta-analyses of treatment-covariate interactions: a simulation study](https://arxiv.org/abs/2510.24130)
*Myra B. McGuinness,Joanne E. McKenzie,Andrew Forbes,Flora Hui,Keith R. Martin,Robert J. Casson,Amalia Karahalios*

Main category: stat.ME

TL;DR: 本文扩展了在个体参与者数据荟萃分析中估计I²统计量的方法，特别是在处理治疗-协变量交互作用时，适用于参与者数量不等的亚组或连续协变量情况。


<details>
  <summary>Details</summary>
Motivation: 现有方法在量化研究间异质性时，主要适用于两阶段IPD-MA或参与者数量近似相等的一阶段IPD-MA，但在处理治疗-协变量交互作用时存在局限性。

Method: 扩展了估计I²的公式，适用于参与者数量不等的亚组和连续协变量情况，并通过模拟研究比较一阶段和两阶段模型的I²估计结果。

Result: 一阶段和两阶段模型的I²估计偏差和精度相似，平均差异在-1.0到0.0个百分点之间，但在小样本数据中差异可达19.4个百分点。

Conclusion: 扩展方法得到的I²估计值可以按照现有两阶段模型的公式进行类似解释。

Abstract: It is recommended that measures of between-study effect heterogeneity be
reported when conducting individual-participant data meta-analyses (IPD-MA).
Methods exist to quantify inconsistency between trials via I^2 (the percentage
of variation in the treatment effect due to between-study heterogeneity) when
conducting two-stage IPD-MA, and when conducting one-stage IPD-MA with
approximately equal numbers of treatment and control group participants. We
extend formulae to estimate I^2 when investigating treatment-covariate
interactions with unequal numbers of participants across subgroups and/or
continuous covariates. A simulation study was conducted to assess the agreement
in values of I^2 between those derived from two-stage models using traditional
methods and those derived from equivalent one-stage models. Fourteen scenarios
differed by the magnitude of between-trial heterogeneity, the number of trials,
and the average number of participants in each trial. Bias and precision of I^2
were similar between the one- and two-stage models. The mean difference in I^2
between equivalent models ranged between -1.0 and 0.0 percentage points across
scenarios. However, disparities were larger in simulated datasets with smaller
samples sizes with up to 19.4 percentage points difference between models.
Thus, the estimates of I^2 derived from these extended methods can be
interpreted similarly to those from existing formulae for two-stage models.

</details>


### [22] [Intelligent n-Means Spatial Sampling](https://arxiv.org/abs/2510.24183)
*Bardia Panahbehagh,Mehdi Mohebbi,Amir Mohammad HosseiniNasab*

Main category: stat.ME

TL;DR: 本文提出了一个集成方法框架来在人口空间坐标上分布样本，包括新的空间平衡指数、平衡聚类方法和基于图抽样的高效抽样方案，显著提升了样本的空间分布质量。


<details>
  <summary>Details</summary>
Motivation: 在目标变量具有空间结构时，分布良好的样本能够改善估计效果，因此需要开发系统的方法来优化样本在空间上的分布。

Method: 提出了平移不变的空间平衡指数；开发了基于辅助变量的平衡聚类方法；设计了基于图抽样的智能搜索层抽样方案，能够自适应人口空间结构和包含概率。

Result: 在各种空间模式和等概率/不等概率情况下，该方法在分散度指标上持续优于其他空间导向设计，空间平衡指数保持信息性，聚类步骤提高了代表性。

Conclusion: 该集成框架通过智能耦合空间结构和包含概率，为每个特定人口定制设计，最大化样本的空间分布效果，在空间抽样中具有重要应用价值。

Abstract: Well-spread samples are desirable in many disciplines because they improve
estimation when target variables exhibit spatial structure. This paper
introduces an integrated methodological framework for spreading samples over
the population's spatial coordinates. First, we propose a new,
translation-invariant spreadness index that quantifies spatial balance with a
clear interpretation. Second, we develop a clustering method that balances
clusters with respect to an auxiliary variable; when the auxiliary variable is
the inclusion probability, the procedure yields clusters whose totals are one,
so that a single draw per cluster is, in principle, representative and produces
units optimally spread along the population coordinates, an attractive feature
for finite population sampling. Third, building on the graphical sampling
framework, we design an efficient sampling scheme that further enhances spatial
balance. At its core lies an intelligent, computationally efficient search
layer that adapts to the population's spatial structure and inclusion
probabilities, tailoring a design to each specific population to maximize
spread. Across diverse spatial patterns and both equal- and unequal-probability
regimes, this intelligent coupling consistently outperformed all rival
spread-oriented designs on dispersion metrics, while the spreadness index
remained informative and the clustering step improved representativeness.

</details>


### [23] [Pseudo-Bayesian Optimal Designs for Fitting Fractional Polynomial Response Surface Models](https://arxiv.org/abs/2510.24349)
*Luzia A. Trinca,Steven G. Gilmour*

Main category: stat.ME

TL;DR: 本文使用贝叶斯最优精确设计方法为多个分数多项式模型寻找最优实验设计，并与响应曲面问题中的各种标准设计进行比较。


<details>
  <summary>Details</summary>
Motivation: 分数多项式模型在响应曲面研究中具有潜在价值，但随着统计软件中非线性模型拟合程序的普及，需要选择能够高效估计模型参数的实验设计。

Method: 采用Gilmour和Trinca（2012b）提出的方法，利用现代计算设施生成精确设计，为多个分数多项式模型寻找贝叶斯最优精确设计。

Result: 获得了多个分数多项式模型的贝叶斯最优精确设计，并与响应曲面问题中的标准设计进行了比较。

Conclusion: 该方法能够为分数多项式模型产生有效的实验设计，解决了非线性模型设计中的困难，并展示了与标准设计相比的优势。

Abstract: Fractional polynomial models are potentially useful for response surfaces
investigations. With the availability of routines for fitting nonlinear models
in statistical packages they are increasingly being used. However, as in all
experiments the design should be chosen such that the model parameters are
estimated as efficiently as possible. The design choice for such models
involves the known nonlinear models' design difficulties but
\cite{gilmour_trinca_2012b} proposed a methodology capable of producing exact
designs that makes use of the computing facilities available today. In this
paper, we use this methodology to find Bayesian optimal exact designs for
several fractional polynomial models. The optimum designs are compared to
various standard designs in response surface problems.

</details>


### [24] [Comparison of Estimators for Multi-State Models in Potentially Non-Markov Processes](https://arxiv.org/abs/2510.24453)
*Carolin Drenda,Dennis Dobler,Merle Munko,Andrew Titman*

Main category: stat.ME

TL;DR: 提出了一种新的混合Aalen-Johansen估计器，使用Cox模型替代log-rank检验来验证马尔可夫假设，并在各种马尔可夫、半马尔可夫和非马尔可夫设置下比较了四种估计器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多状态模型转移概率估计器各有局限性：Aalen-Johansen估计器仅在马尔可夫假设下一致，地标Aalen-Johansen估计器能处理非马尔可夫模型但方差较大，现有混合方法使用log-rank检验。需要开发更优的混合估计器。

Method: 提出新混合Aalen-Johansen估计器，第一步使用Cox模型检验马尔可夫假设，仅在拒绝假设时应用地标方法。通过模拟研究比较四种估计器在马尔可夫、半马尔可夫和非马尔可夫设置下的性能。

Result: 模拟研究表明混合Aalen-Johansen估计器在各种测量指标和设置下都表现优异，分析了非马尔可夫行为形式、程度、不同转移和起始时间等因素对估计器的影响。

Conclusion: 混合Aalen-Johansen估计器在多状态模型转移概率估计中具有优越性能，是处理马尔可夫和非马尔可夫情况的良好折中方案。

Abstract: Various estimators for modelling the transition probabilities in multi-state
models have been proposed, e.g., the Aalen-Johansen estimator, the landmark
Aalen-Johansen estimator, and a hybrid Aalen-Johansen estimator. While the
Aalen-Johansen estimator is generally only consistent under the rather
restrictive Markov assumption, the landmark Aalen-Johansen estimator can handle
non-Markov multi-state models. However, the landmark Aalen-Johansen estimator
leads to a strict data reduction and, thus, to an increased variance. The
hybrid Aalen-Johansen estimator serves as a compromise by, firstly, checking
with a log-rank-based test whether the Markov assumption is satisfied.
Secondly, landmarking is only applied if the Markov assumption is rejected. In
this work, we propose a new hybrid Aalen-Johansen estimator which uses a Cox
model instead of the log-rank-based test to check the Markov assumption in the
first step. Furthermore, we compare the four estimators in an extensive
simulation study across Markov, semi-Markov, and distinct non-Markov settings.
In order to get deep insights into the performance of the estimators, we
consider four different measures: bias, variance, root mean squared error, and
coverage rate. Additionally, further influential factors on the estimators such
as the form and degree of non-Markov behaviour, the different transitions, and
the starting time are analysed. The main result of the simulation study is that
the hybrid Aalen-Johansen estimators yield favourable results across various
measures and settings.

</details>


### [25] [Bayesian nonparametric modeling of multivariate count data with an unknown number of traits](https://arxiv.org/abs/2510.24526)
*Lorenzo Ghilotti,Federico Camerlenghi,Tommaso Rigon,Michele Guindani*

Main category: stat.ME

TL;DR: 提出了一种用于部分可交换特征分配模型的贝叶斯非参数先验，允许特征数量随机变化，并开发了能够从数据推断组分区结构的混合模型。


<details>
  <summary>Details</summary>
Motivation: 现有特征分配模型通常假设数据完全可交换，这在具有异质但相关组的设置中可能过于限制。需要处理部分可交换性和随机特征数量的方法。

Method: 基于完全随机向量构建贝叶斯非参数先验，提供边际和后验分布的闭式表达式，在二元和泊松分布特征情况下展示可处理性。开发了推断组分区结构的混合模型。

Result: 模型能够建模和估计未观察到的特征，避免了固定特征数量时的系统性过聚类问题。在'Ndrangheta犯罪网络案例中提供了对非法活动组织的洞察。

Conclusion: 该方法为部分可交换特征分配提供了通用且可处理的框架，扩展了贝叶斯非参数潜在类别模型，在实际应用中显示出实用性。

Abstract: Feature and trait allocation models are fundamental objects in Bayesian
nonparametrics and play a prominent role in several applications. Existing
approaches, however, typically assume full exchangeability of the data, which
may be restrictive in settings characterized by heterogeneous but related
groups. In this paper, we introduce a general and tractable class of Bayesian
nonparametric priors for partially exchangeable trait allocation models,
relying on completely random vectors. We provide a comprehensive theoretical
analysis, including closed-form expressions for marginal and posterior
distributions, and illustrate the tractability of our framework in the cases of
binary and Poisson-distributed traits. A distinctive aspect of our approach is
that the number of traits is a random quantity, thereby allowing us to model
and estimate unobserved traits. Building on these results, we also develop a
novel mixture model that infers the group partition structure from the data,
effectively clustering trait allocations. This extension generalizes Bayesian
nonparametric latent class models and avoids the systematic overclustering that
arises when the number of traits is assumed to be fixed. We demonstrate the
practical usefulness of our methodology through an application to the
`Ndrangheta criminal network from the Operazione Infinito investigation, where
our model provides insights into the organization of illicit activities.

</details>


### [26] [Unbiased likelihood estimation of the Langevin diffusion for animal movement modelling](https://arxiv.org/abs/2510.24539)
*Ron Ronald Togunov,Simen Knutsen Furset,Martin Emil Pettersen,Robert Brian O'Hara*

Main category: stat.ME

TL;DR: 使用重要性采样中的布朗桥改进Langevin扩散模型的似然近似，有效消除参数估计偏差，使模型在较低采样率下表现更好


<details>
  <summary>Details</summary>
Motivation: 当前Langevin扩散模型在处理时间间隔不规则的追踪数据时，随着观测间隔增加，参数估计偏差会增大

Method: 在重要性采样方案中使用布朗桥来改进Langevin扩散模型的似然近似

Result: 通过模拟研究证明该方法能有效消除多种场景下的偏差，且在较低采样率但较长持续时间下表现优于高采样频率短持续时间

Conclusion: 这项研究扩展了Langevin扩散模型在较粗分辨率遥测数据中的适用性

Abstract: The resource selection function provides a model for describing habitat
suitability, which can be used to predict the spatial utilisation distribution
of a species. Tracking data can be modelled as a point process, but this is
made complicated by the presence of temporally irregular autocorrelation. One
proposed model to handle this is the continuous-time Langevin diffusion.
However, current estimation techniques obtain increasingly biased parameter
estimates as the intervals between observations increase. In this paper, we
address this issue using Brownian bridges in an importance sampling scheme to
improve the likelihood approximation of the Langevin diffusion model. We show
using a series of simulation studies that this approach effectively removes the
bias in many scenarios. Furthermore, we show that the model actually performs
better at lower sampling rates over a longer duration than shorter duration at
a higher sampling frequency. This research broadens the applicability of
Langevin diffusion models to telemetry data at coarser resolutions.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [27] [A modified particle filter that reduces weight collapse](https://arxiv.org/abs/2510.23740)
*Shay Gilpin,Michael Herty*

Main category: stat.AP

TL;DR: 提出一种基于能量多样性度量的粒子滤波器改进方法，通过最小化二体能量势来调整粒子权重，防止权重崩溃问题，提高状态估计精度。


<details>
  <summary>Details</summary>
Motivation: 解决粒子滤波器中的权重崩溃问题，即单个权重接近1而其他权重接近0，导致估计分布崩溃的现象。

Method: 引入基于能量多样性度量的改进方法，通过最小化二体能量势来调整粒子权重，促进权重分布的平衡。

Result: 在线性和非线性动力学模型的数值实验中，相比经典粒子滤波器，新方法改善了权重分布并提高了状态估计精度。

Conclusion: 提出的基于能量多样性度量的粒子滤波器改进方法能有效缓解权重崩溃问题，提升状态估计性能。

Abstract: Particle filters are a widely used Monte Carlo based data assimilation
technique that estimates the probability distribution of a system's state
conditioned on observations through a collection of weights and particles. A
known problem for particle filters is weight collapse, or degeneracy, where a
single weight attains a value of one while all others are close to zero,
thereby collapsing the estimated distribution. We address this issue by
introducing a novel modification to the particle filter that is simple to
implement and inspired by energy-based diversity measures. Our approach adjusts
particle weights to minimize a two-body energy potential, promoting balanced
weight distributions and mitigating collapse. We demonstrate the performance of
this modified particle filter in a series of numerical experiments with linear
and nonlinear dynamical models, where we compare with the classical particle
filter and ensemble Kalman filters in the nonlinear case. We find that our new
approach improves weight distributions compared to the classical particle
filter and thereby improve state estimates.

</details>


### [28] [A web-based user interface for Fam3PRO, a multi-gene, multi-cancer risk prediction model for families with cancer history](https://arxiv.org/abs/2510.23805)
*Xueying Chen,Jianfeng Ke,Lauren Flynn,Giovanni Parmigiani,Danielle Braun*

Main category: stat.AP

TL;DR: 开发了F3PI网络界面，为Fam3PRO遗传性癌症风险预测模型提供用户友好的交互界面，使临床医生能更便捷地评估患者遗传性癌症风险和未来癌症发病风险。


<details>
  <summary>Details</summary>
Motivation: Fam3PRO模型虽然功能强大，但缺乏用户界面限制了其在临床环境中的实际应用，因此需要开发易于使用的网络界面来扩大其使用范围。

Method: 使用R Shiny构建网络界面，通过pedigreejs交互式可视化和修改家谱数据，后端Fam3PRO模型处理输入数据生成携带者概率和未来癌症风险。

Result: F3PI能够在一分钟内提供个性化的18种癌症风险和22个遗传性癌症基因的致病性变异概率，结果支持交互查看和下载。

Conclusion: F3PI是一个易于使用的交互式网络应用程序，使癌症和遗传风险信息对医疗服务提供者及其患者更加可及。

Abstract: Purpose: Hereditary cancer risk is key to guiding screening and prevention
strategies. Cancer risks can vary by individual due to the presence or absence
of high- and moderate-risk pathogenic variants (PV) in cancer-associated genes,
in addition to sex, age, and other risk factors. We previously developed
Fam3PRO, a flexible multi-gene, multi-cancer Mendelian risk prediction model
that estimates a patient's risk of carrying a PV in hereditary cancer genes and
their future risk of developing several types of cancer. The Fam3PRO R package
includes 22 genes with 18 associated cancers, allowing users to build
customized sub-models from any gene-cancer set. However, the current R package
lacks a user interface (UI), limiting its practical use in clinical settings.
Therefore, we aim to develop a web-based UI for broader use of the Fam3PRO
functionalities.
  Methods: The Fam3PRO UI (F3PI), built with R Shiny, collects and formats
inputs including family health history, genetic test results, and other risk
factors. Pedigree data are interactively visualized and modified via
pedigreejs, while the backend Fam3PRO model takes all the inputs to generate
carrier probabilities and future cancer risks, presented through an interactive
UI.
  Results: F3PI streamlines the collection of patient and family history data,
which is analyzed by the Fam3PRO models to provide personalized cancer risks
for each proband across 18 cancers, as well as probabilities that a proband has
a PV in up to 22 hereditary cancer genes. These results are returned to the
user, within one minute on average and are available in both interactive and
downloadable formats.
  Conclusion: We have developed F3PI, an easy-to-use, interactive web
application that makes cancer and genetic risk information more accessible to
providers and their patients.

</details>


### [29] [Universal Inference for Testing Calibration of Mean Estimates within the Exponential Dispersion Family](https://arxiv.org/abs/2510.23821)
*Łukasz Delong,Mario Wüthrich*

Main category: stat.AP

TL;DR: 开发了一种在指数离散族中的子采样分割似然比检验方法，用于验证预测的均值校准，该方法提供有限样本保证和通用有效临界值。


<details>
  <summary>Details</summary>
Motivation: 在金融和精算决策等领域，预测的均值校准验证至关重要，需要开发具有有限样本保证的校准测试方法。

Method: 在指数离散族框架下，开发了子采样分割似然比检验，并基于此提出了新的测试统计量来提升校准测试性能。

Result: 数值分析表明，该方法在检测错误校准方面具有高功效，是传统似然比检验的有吸引力的替代方案。

Conclusion: 子采样分割似然比检验为均值校准验证提供了具有有限样本保证和通用有效临界值的有效方法，在检测错误校准方面表现出色。

Abstract: Calibration of mean estimates for predictions is a crucial property in many
applications, particularly in the fields of financial and actuarial
decision-making. In this paper, we first review classical approaches for
validating mean-calibration, and we discuss the Likelihood Ratio Test (LRT)
within the Exponential Dispersion Family (EDF). Then, we investigate the
framework of universal inference to test for mean-calibration. We develop a
sub-sampled split LRT within the EDF that provides finite sample guarantees
with universally valid critical values. We investigate type I error, power and
e-power of this sub-sampled split LRT, we compare it to the classical LRT, and
we propose a novel test statistics based on the sub-sampled split LRT to
enhance the performance of the calibration test. A numerical analysis verifies
that our proposal is an attractive alternative to the classical LRT achieving a
high power in detecting miscalibration.

</details>


### [30] [Forecasting Melting Points in Svalbard, Norway Using Quantile Gradient Boosting and Adaptive Conformal Prediction Region](https://arxiv.org/abs/2510.23976)
*Richard Berk*

Main category: stat.AP

TL;DR: 使用分位数梯度提升方法预测挪威斯瓦尔巴群岛2023年日温度，采用14天滞后气象指标，提供两周提前期的温度预测和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 为北极地区提供准确的温度预测，支持当地利益相关者的决策制定和北极适应政策的讨论。

Method: 应用分位数梯度提升（小AI）方法，使用14天滞后的5个常规气象指标作为预测因子，0.60分位数损失函数对低估赋予更高权重，并采用保形预测区域量化预测不确定性。

Result: 成功生成了斯瓦尔巴群岛2023年日温度的两周提前期预测，并提供了具有统计保证覆盖率的预测不确定性量化。

Conclusion: 该方法为北极地区提供了可靠的温度预测工具，对当地利益相关者的决策和北极适应政策制定具有重要价值。

Abstract: Using data from the Longyearbyen weather station, quantile gradient boosting
(``small AI'') is applied to forecast daily 2023 temperatures in Svalbard,
Norway. The 0.60 quantile loss weights underestimates about 1.5 times more than
overestimates. Predictors include five routinely collected indicators of
weather conditions, each lagged by 14~days, yielding temperature forecasts with
a two-week lead time. Conformal prediction regions quantify forecasting
uncertainty with provably valid coverage. Forecast accuracy is evaluated with
attention to local stakeholder concerns, and implications for Arctic adaptation
policy are discussed.

</details>


### [31] [Machine Learning for the Production of Official Statistics: Density Ratio Estimation using Biased Transaction Data for Japanese labor statistics](https://arxiv.org/abs/2510.24153)
*Yuya Takada,Kiyoshi Izumi*

Main category: stat.AP

TL;DR: 本文展示了如何利用有偏见的交易数据通过机器学习方法快速生成官方统计数据，以日本私营就业机构数据为例，实现了劳动力市场指标的及时发布。


<details>
  <summary>Details</summary>
Motivation: 传统数据源在官方统计中发布延迟严重，而私营公司积累的大量交易数据虽存在选择偏差，但若能解决偏差问题，可显著提升统计数据的时效性和决策价值。

Method: 采用机器学习中的密度比估计和协变量偏移下的监督学习方法，处理非传统数据源中的选择偏差问题。

Result: 成功利用日本私营就业机构的偏见数据，实现了劳动力市场指标的及时发布，将原本可能延迟一年的数据提前可用。

Conclusion: 即使存在偏差的非传统数据源，通过适当的机器学习方法处理，也能成为官方统计的有价值补充，显著提升统计数据的时效性和决策支持能力。

Abstract: National statistical institutes are beginning to use non-traditional data
sources to produce official statistics. These sources, originally collected for
non-statistical purposes, include point-of-sales(POS) data and mobile phone
global positioning system(GPS) data. Such data have the potential to
significantly enhance the usefulness of official statistics. In the era of big
data, many private companies are accumulating vast amounts of transaction data.
Exploring how to leverage these data for official statistics is increasingly
important. However, progress has been slower than expected, mainly because such
data are not collected through sample-based survey methods and therefore
exhibit substantial selection bias. If this bias can be properly addressed,
these data could become a valuable resource for official statistics,
substantially expanding their scope and improving the quality of
decision-making, including economic policy. This paper demonstrates that even
biased transaction data can be useful for producing official statistics for
prompt release, by drawing on the concepts of density ratio estimation and
supervised learning under covariate shift, both developed in the field of
machine learning. As a case study, we show that preliminary statistics can be
produced in a timely manner using biased data from a Japanese private
employment agency. This approach enables the early release of a key labor
market indicator that would otherwise be delayed by up to a year, thereby
making it unavailable for timely decision-making.

</details>


### [32] [Streamlining business functions in official statistical production with Machine Learning](https://arxiv.org/abs/2510.24394)
*Sandra Barragán,Adrián Pérez-Bote,Carlos Sáez,David Salgado,Luis Sanguiao-Sande*

Main category: stat.AP

TL;DR: 本文描述了在官方统计生产过程中使用统计学习模型优化业务功能的试点和生产经验，目标是提高准确性、成本效益、及时性、粒度、减轻响应负担和频率。


<details>
  <summary>Details</summary>
Motivation: 寻求通过统计学习模型改进官方统计生产过程的质量，包括准确性、成本效益、及时性等多个维度。

Method: 在西班牙统计局(INE)的真实调查数据上进行试点实验，采用统计学习模型来优化业务功能。

Result: 获得了在真实统计调查环境中应用统计学习模型的试点经验。

Conclusion: 统计学习模型在官方统计生产过程中具有改善业务功能的潜力，试点经验为后续生产应用提供了基础。

Abstract: We provide a description of pilot and production experiences to streamline
some business functions in the official statistical production process using
statistical learning models. Our approach is quality-oriented searching for an
improvement on accuracy, cost-efficiency, timeliness, granularity, response
burden reduction, and frequency. Pilot experiences have been conducted with
data from real surveys in Statistics Spain (INE).

</details>


### [33] [GNAR-HARX Models for Realised Volatility: Incorporating Exogenous Predictors and Network Effects](https://arxiv.org/abs/2510.24443)
*Tom Ó Nualláin*

Main category: stat.AP

TL;DR: GNAR-HARX模型结合网络自回归结构和HAR动态，用于预测已实现波动率，在16年样本外测试中显示优于单变量基准模型。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时捕捉金融市场时间持续性和横截面溢出效应的波动率预测模型，改进传统单变量方法的局限性。

Method: 将广义网络自回归(GNAR)结构与异质自回归(HAR)动态及外生变量(如隐含波动率)相结合，应用于10个国际股指的日度已实现方差数据，采用滚动窗口进行一步预测。

Result: 基于QLIKE损失的最佳模型是不含外生变量的局部GNAR-HAR，而最低MSE由含隐含波动率的标准GNAR-HARX实现。全连接网络始终优于动态估计的图套索网络。

Conclusion: 局部和标准GNAR-HAR(X)模型提供最强预测性能，尽管参数更多，但显著优于单变量基准。隐含波动率和隔夜收益是最有用的外生预测变量。

Abstract: This project introduces the GNAR-HARX model, which combines Generalised
Network Autoregressive (GNAR) structure with Heterogeneous Autoregressive (HAR)
dynamics and exogenous predictors such as implied volatility. The model is
designed for forecasting realised volatility by capturing both temporal
persistence and cross-sectional spillovers in financial markets. We apply it to
daily realised variance data for ten international stock indices, generating
one-step-ahead forecasts in a rolling window over an out-of-sample period of
approximately 16 years (2005-2020).
  Forecast accuracy is evaluated using the Quasi-Likelihood (QLIKE) loss and
mean squared error (MSE), and we compare global, standard, and local variants
across different network structures and exogenous specifications. The best
model found by QLIKE is a local GNAR-HAR without exogenous variables, while the
lowest MSE is achieved by a standard GNAR-HARX with implied volatility. Fully
connected networks consistently outperform dynamically estimated graphical
lasso networks.
  Overall, local and standard GNAR-HAR(X) models deliver the strongest
forecasts, though at the cost of more parameters than the parsimonious global
variant, which nevertheless remains competitive. Across all cases, GNAR-HAR(X)
models outperform univariate HAR(X) benchmarks, which often require more
parameters than the GNAR-based specifications. While the top model found by
QLIKE does not use exogenous variables, implied volatility and overnight
returns emerge as the most useful predictors when included.

</details>
