<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.ML](#stat.ML) [Total: 11]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [A L-infinity Norm Synthetic Control Approach](https://arxiv.org/abs/2510.26053)
*Le Wang,Xin Xing,Youhui Ye*

Main category: stat.ME

TL;DR: 本文从加权哲学角度重新解释合成控制框架，提出L-无穷正则化合成控制方法，结合了传统SC的稀疏加权和DID的密集加权的优点。


<details>
  <summary>Details</summary>
Motivation: 传统合成控制(SC)和双重差分(DID)反映了两种不同的建模思维：稀疏加权与密集加权方案。作者认为不应将稀疏性视为固有优势，而是作为一种建模选择，可能简单但脆弱。

Method: 提出L-无穷正则化SC方法，采用更密集的加权方案，在控制单元间更均匀地分配权重。开发了高效计算的内点算法，并在弱依赖条件下推导渐近理论。

Result: 通过模拟和实际应用展示了强有限样本性能，该方法增强了稳健性，减少了对少数控制单元的过度依赖，同时保持了灵活性和可解释性。

Conclusion: L-无穷正则化SC方法成功结合了SC和DID的优点，在保持数据驱动和可解释性的同时，提高了估计的稳健性。

Abstract: This paper reinterprets the Synthetic Control (SC) framework through the lens
of weighting philosophy, arguing that the contrast between traditional SC and
Difference-in-Differences (DID) reflects two distinct modeling mindsets: sparse
versus dense weighting schemes. Rather than viewing sparsity as inherently
superior, we treat it as a modeling choice simple but potentially fragile. We
propose an L-infinity-regularized SC method that combines the strengths of both
approaches. Like DID, it employs a denser weighting scheme that distributes
weights more evenly across control units, enhancing robustness and reducing
overreliance on a few control units. Like traditional SC, it remains flexible
and data-driven, increasing the likelihood of satisfying the parallel trends
assumption while preserving interpretability. We develop an interior point
algorithm for efficient computation, derive asymptotic theory under weak
dependence, and demonstrate strong finite-sample performance through
simulations and real-world applications.

</details>


### [2] [Poisson process factorization for mutational signature analysis with genomic covariates](https://arxiv.org/abs/2510.26090)
*Alessandro Zito,Giovanni Parmigiani,Jeffrey W. Miller*

Main category: stat.ME

TL;DR: 本文提出了一种基于泊松点过程的突变特征分析方法，通过考虑基因组位点特异性协变量来改进传统的非负矩阵分解方法，能够更准确地推断突变特征及其在基因组区域中的活动变化。


<details>
  <summary>Details</summary>
Motivation: 传统的突变特征分析方法使用非负矩阵分解处理聚合突变计数，但忽略了突变在基因组中出现的非均匀模式和组织特异性特征。现有方法缺乏直接利用位点特异性协变量的适当方法。

Method: 提出泊松过程因子分解(PPF)模型，使用协变量依赖的因子化强度函数，通过稀疏诱导分层先验自动推断活跃潜在因子数量，并提供最大后验估计和MCMC不确定性量化。

Result: 在模拟数据和乳腺癌真实数据上的测试表明，该方法能够揭示表观遗传标记对潜在过程的联合效应，并实现高分辨率分析。

Conclusion: PPF方法成功克服了传统NMF的局限性，能够整合基因组特征对突变率的影响，为突变特征分析提供了更精确的框架。

Abstract: Mutational signatures are powerful summaries of the mutational processes
altering the DNA of cancer cells and are increasingly relevant as biomarkers in
personalized treatments. The widespread approach to mutational signature
analysis consists of decomposing the matrix of mutation counts from a sample of
patients via non-negative matrix factorization (NMF) algorithms. However, by
working with aggregate counts, this procedure ignores the non-homogeneous
patterns of occurrence of somatic mutations along the genome, as well as the
tissue-specific characteristics that notoriously influence their rate of
appearance. This gap is primarily due to a lack of adequate methodologies to
leverage locus-specific covariates directly in the factorization. In this
paper, we address these limitations by introducing a model based on Poisson
point processes to infer mutational signatures and their activities as they
vary across genomic regions. Using covariate-dependent factorized intensity
functions, our Poisson process factorization (PPF) generalizes the baseline NMF
model to include regression coefficients that capture the effect of commonly
known genomic features on the mutation rates from each latent process.
Furthermore, our method relies on sparsity-inducing hierarchical priors to
automatically infer the number of active latent factors in the data, avoiding
the need to fit multiple models for a range of plausible ranks. We present
algorithms to obtain maximum a posteriori estimates and uncertainty
quantification via Markov chain Monte Carlo. We test the method on simulated
data and on real data from breast cancer, using covariates on alterations in
chromosomal copies, histone modifications, cell replication timing, nucleosome
positioning, and DNA methylation. Our results shed light on the joint effect
that epigenetic marks have on the latent processes at high resolution.

</details>


### [3] [Variable selection in spatial lag models using the focussed information criterion](https://arxiv.org/abs/2510.26177)
*Sagar Pandhare,Divya Kappara,Siuli Mukhopadhyay*

Main category: stat.ME

TL;DR: 该论文提出在线性空间滞后模型中使用聚焦信息准则(FIC)进行变量选择，通过最小化焦点函数估计的渐近风险来处理模型误设问题，并开发了考虑空间特征的实用焦点函数和平均FIC方法。


<details>
  <summary>Details</summary>
Motivation: 在空间回归模型中，选择重要的外生预测变量对理解空间自相关响应变量的影响因素至关重要。传统方法在处理模型误设时存在局限，需要开发更稳健的变量选择方法。

Method: 基于聚焦信息准则(FIC)，在局部扰动相互邻接概率模型序列下研究最大似然估计的关键渐近性质，推导估计焦点的偏差和方差表达式，开发考虑均值响应、估计变异性和空间溢出效应的焦点函数，并提出平均FIC方法。

Result: 通过模拟和真实数据分析验证了所提方法的经验性能，证明了FIC在空间滞后模型中变量选择的有效性。

Conclusion: FIC方法为线性空间滞后模型提供了有效的变量选择框架，能够处理模型误设问题并考虑各种空间特征，具有实际应用价值。

Abstract: Spatial regression models have a variety of applications in several fields
ranging from economics to public health. Typically, it is of interest to select
important exogenous predictors of the spatially autocorrelated response
variable. In this paper, we propose variable selection in linear spatial lag
models by means of the focussed information criterion (FIC). The FIC-based
variable selection involves the minimization of the asymptotic risk in the
estimation of a certain parametric focus function of interest under potential
model misspecification. We systematically investigate the key asymptotics of
the maximum likelihood estimators under the sequence of locally perturbed
mutually contiguous probability models. Using these results, we obtain the
expressions for the bias and the variance of the estimated focus leading to the
desired FIC formula. We provide practically useful focus functions that account
for various spatial characteristics such as mean response, variability in the
estimation and spatial spillover effects. Furthermore, we develop an averaged
version of the FIC that incorporates varying covariate levels while evaluating
the models. The empirical performance of the proposed methodology is
demonstrated through simulations and real data analysis.

</details>


### [4] [Estimating heritability of survival traits using censored multiple variance component model](https://arxiv.org/abs/2510.26226)
*Do Hyun Kim,Hua Zhou,Brendon Chau,Aubrey Jensen,Judong Shen,Devan Mehrotra,Gang Li,Jin J. Zhou*

Main category: stat.ME

TL;DR: 提出了一种用于右删失生存性状遗传力分析的删失多方差分量模型，能够在大规模生物样本库中准确估计生存性状的总遗传力。


<details>
  <summary>Details</summary>
Motivation: 生存性状（如疾病发病年龄）的遗传基础研究对于风险分层和疾病机制理解至关重要，但现有方法无法处理右删失数据且难以扩展到大规模生物样本库。

Method: 开发了删失多方差分量模型，能够处理右删失的生存数据，并在大规模数据集上高效计算遗传方差分量。

Result: 模拟显示该方法在高达80%删失率下仍能准确估计遗传力，在百万样本和百万SNPs的数据集上计算时间少于9小时。在UK Biobank中成功应用于四种疾病发病年龄性状的遗传力估计。

Conclusion: 建立了一个可扩展且稳健的框架，用于大规模遗传研究中右删失生存性状的遗传力分析。

Abstract: Characterizing the genetic basis of survival traits, such as age at disease
onset, is critical for risk stratification, early intervention, and elucidating
biological mechanisms that can inform therapeutic development. However,
time-to-event outcomes in human cohorts are frequently right-censored,
complicating both the estimation and partitioning of total heritability. Modern
biobanks linked to electronic health records offer the unprecedented power to
dissect the genetic basis of age-at-diagnosis traits at large scale. Yet, few
methods exist for estimating and partitioning the total heritability of
censored survival traits. Existing methods impose restrictive distributional
assumptions on genetic and environmental effects and are not scalable to large
biobanks with a million subjects. We introduce a censored multiple variance
component model to robustly estimate the total heritability of survival traits
under right-censoring. We demonstrate through extensive simulations that the
method provides accurate total heritability estimates of right-censored traits
at censoring rates up to 80% given sufficient sample size. The method is
computationally efficient in estimating one hundred genetic variance components
of a survival trait using large-scale biobank genotype data consisting of a
million subjects and a million SNPs in under nine hours, including uncertainty
quantification. We apply our method to estimate the total heritability of four
age-at-diagnosis traits from the UK Biobank study. Our results establish a
scalable and robust framework for heritability analysis of right-censored
survival traits in large-scale genetic studies.

</details>


### [5] [Smoothed Quantile Estimation via Interpolation to the Mean](https://arxiv.org/abs/2510.26447)
*Saïd Maanan,Azzouz Dermoune,Ahmed El Ghini*

Main category: stat.ME

TL;DR: 提出一个统一的平滑分位数估计器家族，在经典经验分位数和样本均值之间连续插值，通过平滑参数h权衡效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统分位数估计在重尾分布下效率较低，而样本均值虽然效率高但缺乏鲁棒性。需要一种能平衡效率与鲁棒性的统一估计框架。

Method: 定义正则化目标函数的最小化解q(z,h)，其中z是位置参数，h是平滑参数。当h=0时还原为经验分位数，h→∞时收敛到样本均值。

Result: 建立了估计器的一致性、渐近正态性和显式方差表达式。发现在轻尾分布下平滑带来渐近方差减少但无有限最优解；在重尾分布下存在有限最优平滑水平h*能严格改进效率。

Conclusion: 平滑分位数估计器提供了一个统一的框架，能够根据分布特性自适应地平衡效率与鲁棒性，特别在重尾分布下能显著提升估计效率。

Abstract: This paper introduces a unified family of smoothed quantile estimators that
continuously interpolate between classical empirical quantiles and the sample
mean. The estimators q(z, h) are defined as minimizers of a regularized
objective function depending on two parameters: a smoothing parameter h $\ge$ 0
and a location parameter z $\in$ R. When h = 0 and z $\in$ (-1, 1), the
estimator reduces to the empirical quantile of order $\tau$ = (1z)/2; as h
$\rightarrow$ $\infty$, it converges to the sample mean for any fixed z. We
establish consistency, asymptotic normality, and an explicit variance
expression characterizing the efficiency-robustness trade-off induced by h. A
key geometric insight shows that for each fixed quantile level $\tau$ , the
admissible parameter pairs (z, h) lie on a straight line in the parameter
space, along which the population quantile remains constant while asymptotic
efficiency varies. The analysis reveals two regimes: under light-tailed
distributions (e.g., Gaussian), smoothing yields a monotonic but asymptotic
variance reduction with no finite optimum; under heavy-tailed distributions
(e.g., Laplace), a finite smoothing level h * ($\tau$ ) > 0 achieves strict
efficiency improvement over the classical empirical quantile. Numerical
illustrations confirm these theoretical predictions and highlight how smoothing
balances robustness and efficiency across quantile levels.

</details>


### [6] [In Defense of the Pre-Test: Valid Inference when Testing Violations of Parallel Trends for Difference-in-Differences](https://arxiv.org/abs/2510.26470)
*Jonas M. Mikhaeil,Christopher Harshaw*

Main category: stat.ME

TL;DR: 本文为DID研究设计中的预检验辩护，提出了条件外推假设，要求通过预检验确定前处理期平行趋势违反程度是否低于可接受水平，从而为后处理期外推提供依据。


<details>
  <summary>Details</summary>
Motivation: 针对当前对DID研究中预检验的批评，作者认为预检验在确定平行趋势违反程度是否可接受方面具有重要作用，需要重新评估预检验的价值。

Method: 提出条件外推假设，要求进行预检验来评估前处理期平行趋势违反的严重程度；在温和假设下提供一致的预检验方法，以及基于检验结果的有效置信区间。

Result: 开发的条件置信区间克服了DID研究中预检验的常见批评，通过真实数据和数值模拟验证了所提方法的性能。

Conclusion: 预检验在DID研究设计中可以且应该发挥重要作用，条件外推假设为合理使用预检验提供了理论基础和方法支持。

Abstract: The difference-in-differences (DID) research design is a key identification
strategy which allows researchers to estimate causal effects under the parallel
trends assumption. While the parallel trends assumption is counterfactual and
cannot be tested directly, researchers often examine pre-treatment periods to
check whether the time trends are parallel before treatment is administered.
Recently, researchers have been cautioned against using preliminary tests which
aim to detect violations of parallel trends in the pre-treatment period. In
this paper, we argue that preliminary testing can -- and should -- play an
important role within the DID research design. We propose a new and more
substantively appropriate conditional extrapolation assumption, which requires
an analyst to conduct a preliminary test to determine whether the severity of
pre-treatment parallel trend violations falls below an acceptable level before
extrapolation to the post-treatment period is justified. This stands in
contrast to prior work which can be interpreted as either setting the
acceptable level to be exactly zero (in which case preliminary tests lack
power) or assuming that extrapolation is always justified (in which case
preliminary tests are not required). Under mild assumptions on how close the
actual violation is to the acceptable level, we provide a consistent
preliminary test as well confidence intervals which are valid when conditioned
on the result of the test. The conditional coverage of these intervals
overcomes a common critique made against the use of preliminary testing within
the DID research design. We use real data as well as numerical simulations to
illustrate the performance of the proposed methods.

</details>


### [7] [Statistical Inference for Matching Decisions via Matrix Completion under Dependent Missingness](https://arxiv.org/abs/2510.26478)
*Congyuan Duan,Wanteng Ma,Dong Xia,Kan Xu*

Main category: stat.ME

TL;DR: 本文通过矩阵补全方法研究双边匹配市场的决策和统计推断，针对匹配容量约束导致的观测数据依赖性，提出了基于Grassmannian梯度下降的非凸算法和去偏投影框架。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵补全方法假设独立采样，但匹配市场中的观测数据受匹配容量约束，这种匹配诱导的依赖性给估计和推断带来了新挑战。

Method: 提出基于Grassmannian梯度下降的非凸算法，并开发了适用于任意线性形式的奖励矩阵的通用去偏投影框架。

Result: 为三种典型匹配机制建立了近乎最优的逐项收敛率，并在匹配诱导的依赖采样下推导了具有有限样本保证的渐近正态性。

Conclusion: 实证实验表明，所提方法能提供准确的估计、有效的置信区间和匹配策略的高效评估。

Abstract: This paper studies decision-making and statistical inference for two-sided
matching markets via matrix completion. In contrast to the independent sampling
assumed in classical matrix completion literature, the observed entries, which
arise from past matching data, are constrained by matching capacity. This
matching-induced dependence poses new challenges for both estimation and
inference in the matrix completion framework. We propose a non-convex algorithm
based on Grassmannian gradient descent and establish near-optimal entrywise
convergence rates for three canonical mechanisms, i.e., one-to-one matching,
one-to-many matching with one-sided random arrival, and two-sided random
arrival. To facilitate valid uncertainty quantification and hypothesis testing
on matching decisions, we further develop a general debiasing and projection
framework for arbitrary linear forms of the reward matrix, deriving asymptotic
normality with finite-sample guarantees under matching-induced dependent
sampling. Our empirical experiments demonstrate that the proposed approach
provides accurate estimation, valid confidence intervals, and efficient
evaluation of matching policies.

</details>


### [8] [Discovering Causal Relationships Between Time Series With Spatial Structure](https://arxiv.org/abs/2510.26485)
*Rebecca F. Supple,Hannah Worthington,Ben Swallow*

Main category: stat.ME

TL;DR: 提出一个将时间序列因果发现扩展到具有空间结构的系统的框架，以解决现有方法在处理时空数据时忽略空间自相关和维度扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 随着因果发现在生态学、公共卫生和环境科学等领域的应用增加，需要开发能够处理时空数据中自相关和复杂混杂因素的方法。现有方法在总结跨位置、忽略空间自相关和维度扩展性方面存在不足。

Method: 基于跨上下文因果发现和空间混杂处理方法，扩展时间序列因果发现框架以处理空间结构。

Result: 提出了一个正在开发中的框架，能够更好地处理时空数据中的因果发现问题。

Conclusion: 该框架为时空因果发现提供了新的方向，但文献中仍存在空白，需要未来研究进一步探索。

Abstract: Causal discovery is the subfield of causal inference concerned with
estimating the structure of cause-and-effect relationships in a system of
interrelated variables, as opposed to quantifying the strength of causal
effects. As interest in causal discovery builds in fields such as ecology,
public health, and environmental sciences where data is regularly collected
with spatial and temporal structures, approaches must evolve to manage
autocorrelation and complex confounding. As it stands, the few proposed causal
discovery algorithms for spatiotemporal data require summarizing across
locations, ignore spatial autocorrelation, and/or scale poorly to high
dimensions. Here, we introduce our developing framework that extends
time-series causal discovery to systems with spatial structure, building upon
work on causal discovery across contexts and methods for handling spatial
confounding in causal effect estimation. We close by outlining remaining gaps
in the literature and directions for future research.

</details>


### [9] [A KL-divergence based test for elliptical distribution](https://arxiv.org/abs/2510.26775)
*Yin Tang,Yanyuan Ma,Bing Li*

Main category: stat.ME

TL;DR: 提出基于KL散度的椭圆分布检验方法，同时考虑长度与方向的独立性以及方向的均匀分布特性，使用k近邻方法构建检验统计量，并针对均值和协方差矩阵已知/未知两种情况进行分析。


<details>
  <summary>Details</summary>
Motivation: 现有椭圆分布检验方法可能未能同时考虑椭圆分布的两个关键特性：长度与方向的独立性，以及方向的均匀分布。需要开发更全面、性能更好的检验方法。

Method: 基于KL散度构建检验程序，使用k近邻方法构造检验统计量。通过样本分割、截断处理以及欧几里得空间与单位球面之间的变换来建立统计量的渐近性质，避免假设任何泛函的Fréchet可微性。

Result: 数值实现表明该方法在尺寸和功效方面优于现有最优方法，通过去偏和方差膨胀处理解决了影响函数退化问题。

Conclusion: 提出的KL散度检验方法能够有效检验椭圆分布，在理论和数值上都表现出优越性能，为椭圆分布检验提供了新的有效工具。

Abstract: We conduct a KL-divergence based procedure for testing elliptical
distributions. The procedure simultaneously takes into account the two defining
properties of an elliptically distributed random vector: independence between
length and direction, and uniform distribution of the direction. The test
statistic is constructed based on the $k$ nearest neighbors ($k$NN) method, and
two cases are considered where the mean vector and covariance matrix are known
and unknown. First-order asymptotic properties of the test statistic are
rigorously established by creatively utilizing sample splitting, truncation and
transformation between Euclidean space and unit sphere, while avoiding assuming
Fr\'echet differentiability of any functionals. Debiasing and variance
inflation are further proposed to treat the degeneration of the influence
function. Numerical implementations suggest better size and power performance
than the state of the art procedures.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [10] [Phantom types for robust hierarchical models with typegeist](https://arxiv.org/abs/2510.26726)
*Daniel O'Hanlon*

Main category: stat.CO

TL;DR: typegeist是一个用于Python的类型系统，通过静态类型分析来确保贝叶斯分层模型中数据与参数索引对应关系的正确性，避免潜在的错误。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯分层模型中，组级参数数组需要映射到观测轴，通常使用显式索引。在复杂模型中，由于索引错误通常静默失败，这引入了潜在的bug风险。

Method: 开发typegeist类型系统，利用静态类型分析来指定和执行数据-参数索引对应关系，可与常见概率编程框架结合使用。

Result: 该方法能够帮助保证模型正确性，同时具有最小的运行时开销。

Conclusion: typegeist类型系统为贝叶斯分层模型提供了一种有效的静态验证方法，能够显著减少索引相关的编程错误。

Abstract: In Bayesian hierarchical models, group-level parameter arrays must be mapped
to the observation axis, often using explicit indexing. In complex models with
numerous incompatible data and parameter sets, this introduces the potential
for bugs, as indexing with the incorrect indices typically fails silently. Here
we present typegeist, a type system for Python that uses static type analysis
to enable specification and enforcement of data-parameter-index
correspondences. We show how this can be used with common probabilistic
programming frameworks to help guarantee model correctness with minimal
run-time overhead.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [11] [Tractable Algorithms for Changepoint Detection in Player Performance Metrics](https://arxiv.org/abs/2510.25961)
*Amanda Glazer*

Main category: stat.AP

TL;DR: 提出了可处理的球员表现指标变化检测方法，应用于2023-2024年MLB数据，包括统计可靠性基准和结合似然法与分样本推断的变点检测算法，在棒球应用中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 需要检测球员表现指标的变化，为棒球数据分析提供统计上可靠的检测方法，并控制误报率。

Method: 首先基于分布假设和集中不等式建立统计可靠性基准，然后提出结合似然法和分样本推断的变点检测算法，使用非参数检验或适合数据分布的检验，包含可指定最小变化幅度的偏移参数。

Result: 在棒球应用中成功检测击球员击球纪律指标变化和投手快速球速度变化，在从救援投手转为先发投手的真实案例中91%检测到有意义变化，某些指标超过60%的变化发生在赛季中。

Conclusion: 提出的框架不仅适用于棒球，还可广泛应用于任何涉及个体表现随时间监测的场景。

Abstract: We present tractable methods for detecting changes in player performance
metrics and apply these methods to Major League Baseball (MLB) batting and
pitching data from the 2023 and 2024 seasons. First, we derive principled
benchmarks for when performance metrics can be considered statistically
reliable, assuming no underlying change, using distributional assumptions and
standard concentration inequalities. We then propose a changepoint detection
algorithm that combines a likelihood-based approach with split-sample inference
to control false positives, using either nonparametric tests or tests
appropriate to the underlying data distribution. These tests incorporate a
shift parameter, allowing users to specify the minimum magnitude of change to
detect. We demonstrate the utility of this approach across several baseball
applications: detecting changes in batter plate discipline metrics (e.g., chase
and whiff rate), identifying velocity changes in pitcher fastballs, and
validating velocity changepoints against a curated ground-truth dataset of
pitchers who transitioned from relief to starting roles. Our method flags
meaningful changes in 91% of these `ground-truth' cases and reveals that, for
some metrics, more than 60% of detected changes occur in-season. While
developed for baseball, the proposed framework is broadly applicable to any
setting involving monitoring of individual performance over time.

</details>


### [12] [Variational System Identification of Aircraft](https://arxiv.org/abs/2510.26496)
*Dimas Abreu Archanjo Dutra*

Main category: stat.AP

TL;DR: 提出变分系统辨识作为最大似然估计的新方法，用于估计受过程噪声和测量噪声影响的动态系统参数，在飞机湍流飞行等场景中表现优于传统滤波误差方法。


<details>
  <summary>Details</summary>
Motivation: 传统滤波误差方法需要求解Riccati方程且存在不稳定预测器问题，需要一种更稳健的参数估计方法。

Method: 采用变分系统辨识方法，避免求解Riccati方程，处理过程噪声和测量噪声，使用真实飞行测试数据进行参数估计。

Result: 在实际应用中，该方法比滤波误差方法具有更好的收敛性，即使所有参数和决策变量使用零初始猜测也能达到最优。

Conclusion: 变分系统辨识是一种有效的替代方法，具有更好的收敛性和稳定性，适用于实际工程应用。

Abstract: Variational system identification is a new formulation of maximum likelihood
for estimation of parameters of dynamical systems subject to process and
measurement noise, such as aircraft flying in turbulence. This formulation is
an alternative to the filter-error method that circumvents the solution of a
Riccati equation and does not have problems with unstable predictors. In this
paper, variational system identification is demonstrated for estimating
aircraft parameters from real flight-test data. The results show that, in real
applications of practical interest, it has better convergence properties than
the filter-error method, reaching the optimum even when null initial guesses
are used for all parameters and decision variables. This paper also presents
the theory behind the method and practical recommendations for its use.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [13] [Multimodal Bandits: Regret Lower Bounds and Optimal Algorithms](https://arxiv.org/abs/2510.25811)
*William Réveillard,Richard Combes*

Main category: stat.ML

TL;DR: 提出了一种针对多峰期望奖励函数的随机多臂老虎机问题的计算可行算法，能够求解Graves-Lai优化问题并实现渐近最优性能


<details>
  <summary>Details</summary>
Motivation: 解决具有多峰期望奖励函数的随机多臂老虎机问题，这类问题在传统方法中难以处理，需要开发计算可行的渐近最优算法

Method: 开发了首个计算可行的算法来求解Graves-Lai优化问题，该算法能够处理最多m个模式的多峰奖励函数，并基于此实现渐近最优的bandit算法

Result: 成功实现了渐近最优的多峰bandit算法，相关代码已在GitHub上公开可用

Conclusion: 该方法为多峰奖励函数的bandit问题提供了首个计算可行的渐近最优解决方案，具有重要的理论和实践价值

Abstract: We consider a stochastic multi-armed bandit problem with i.i.d. rewards where
the expected reward function is multimodal with at most m modes. We propose the
first known computationally tractable algorithm for computing the solution to
the Graves-Lai optimization problem, which in turn enables the implementation
of asymptotically optimal algorithms for this bandit problem. The code for the
proposed algorithms is publicly available at
https://github.com/wilrev/MultimodalBandits

</details>


### [14] [Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning](https://arxiv.org/abs/2510.26723)
*Masahiro Kato*

Main category: stat.ML

TL;DR: 该研究揭示了策略学习中经验福利最大化(EWM)和插件方法之间的等价关系，提出了基于这种等价性的新型正则化方法，实现了凸优化和计算高效的训练过程。


<details>
  <summary>Details</summary>
Motivation: 策略学习中的EWM方法和插件方法被认为是两种不同的方法，本研究旨在弥合这两种方法之间的差距，揭示它们本质上的联系。

Method: 通过证明EWM与策略类重新参数化后的最小二乘法的精确等价性，并基于这种等价性提出新的正则化方法。

Result: 证明了两种方法的等价性，提出了避免NP-hard组合优化步骤的凸优化训练程序。

Conclusion: EWM和插件方法本质上是等价的，可以相互替换，并共享相同的理论保证，新的正则化方法提供了计算效率更高的训练方案。

Abstract: The goal of policy learning is to train a policy function that recommends a
treatment given covariates to maximize population welfare. There are two major
approaches in policy learning: the empirical welfare maximization (EWM)
approach and the plug-in approach. The EWM approach is analogous to a
classification problem, where one first builds an estimator of the population
welfare, which is a functional of policy functions, and then trains a policy by
maximizing the estimated welfare. In contrast, the plug-in approach is based on
regression, where one first estimates the conditional average treatment effect
(CATE) and then recommends the treatment with the highest estimated outcome.
This study bridges the gap between the two approaches by showing that both are
based on essentially the same optimization problem. In particular, we prove an
exact equivalence between EWM and least squares over a reparameterization of
the policy class. As a consequence, the two approaches are interchangeable in
several respects and share the same theoretical guarantees under common
conditions. Leveraging this equivalence, we propose a novel regularization
method for policy learning. Our findings yield a convex and computationally
efficient training procedure that avoids the NP-hard combinatorial step
typically required in EWM.

</details>


### [15] [A Unified Theory for Causal Inference: Direct Debiased Machine Learning via Bregman-Riesz Regression](https://arxiv.org/abs/2510.26783)
*Masahiro Kato*

Main category: stat.ML

TL;DR: 本文提出了一个统一的因果推断理论，整合了Riesz回归、协变量平衡、密度比估计、目标最大似然估计和匹配估计器在平均处理效应估计中的应用。


<details>
  <summary>Details</summary>
Motivation: 在平均处理效应估计中，平衡权重和结果回归函数起着重要作用，但现有方法缺乏统一的理论框架来整合这些不同的估计方法。

Method: 通过建立Riesz回归、协变量平衡、密度比估计、目标最大似然估计和匹配估计器之间的理论联系，构建统一的因果推断框架。

Result: 发现Riesz回归与密度比估计在ATE背景下本质等价，匹配估计器是密度比估计的特例，密度比估计与协变量平衡存在对偶关系，目标最大似然估计用于构造使主要偏差项为零的回归函数估计器。

Conclusion: 最近邻匹配等价于最小二乘密度比估计和Riesz回归，这些方法在统一的因果推断理论框架下相互关联。

Abstract: This note introduces a unified theory for causal inference that integrates
Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted
maximum likelihood estimation (TMLE), and the matching estimator in average
treatment effect (ATE) estimation. In ATE estimation, the balancing weights and
the regression functions of the outcome play important roles, where the
balancing weights are referred to as the Riesz representer, bias-correction
term, and clever covariates, depending on the context. Riesz regression,
covariate balancing, DRE, and the matching estimator are methods for estimating
the balancing weights, where Riesz regression is essentially equivalent to DRE
in the ATE context, the matching estimator is a special case of DRE, and DRE is
in a dual relationship with covariate balancing. TMLE is a method for
constructing regression function estimators such that the leading bias term
becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density
Ratio Estimation and Riesz Regression.

</details>


### [16] [Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation](https://arxiv.org/abs/2510.26026)
*Feichen Gan,Youcun Lu,Yingying Zhang,Yukun Liu*

Main category: stat.ML

TL;DR: 提出一个统一的共形预测框架，用于无限时域策略评估，构建无分布预测区间，适用于在线和离线策略设置。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境中，可靠的强化学习不确定性量化至关重要。

Method: 结合分布强化学习与共形校准，提出基于截断滚动的模块化伪回报构造和时间感知校准策略，使用经验回放和加权子采样。

Result: 在合成环境和基准环境（如Mountain Car）中的实验表明，该方法显著提高了覆盖率和可靠性。

Conclusion: 该方法能够处理未观测回报、时间依赖性和分布偏移等挑战，即使在策略偏移下也能实现不确定性量化。

Abstract: Reliable uncertainty quantification is crucial for reinforcement learning
(RL) in high-stakes settings. We propose a unified conformal prediction
framework for infinite-horizon policy evaluation that constructs
distribution-free prediction intervals {for returns} in both on-policy and
off-policy settings. Our method integrates distributional RL with conformal
calibration, addressing challenges such as unobserved returns, temporal
dependencies, and distributional shifts. We propose a modular pseudo-return
construction based on truncated rollouts and a time-aware calibration strategy
using experience replay and weighted subsampling. These innovations mitigate
model bias and restore approximate exchangeability, enabling uncertainty
quantification even under policy shifts. Our theoretical analysis provides
coverage guarantees that account for model misspecification and importance
weight estimation. Empirical results, including experiments in synthetic and
benchmark environments like Mountain Car, show that our method significantly
improves coverage and reliability over standard distributional RL baselines.

</details>


### [17] [$L_1$-norm Regularized Indefinite Kernel Logistic Regression](https://arxiv.org/abs/2510.26043)
*Shaoxin Wang,Hanjing Yao*

Main category: stat.ML

TL;DR: 提出了一种新的L1正则化不定核逻辑回归模型(RIKLR)，通过引入L1范数惩罚来增强稀疏性、可解释性和泛化能力，并开发了高效的近端线性化算法来解决非光滑和非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，不定核比正定核能捕获更多领域特定的结构信息。现有的不定核逻辑回归(IKLR)框架缺乏稀疏性，限制了模型的可解释性和泛化能力。

Method: 提出L1范数正则化的不定核逻辑回归模型，引入稀疏性惩罚；开发了理论基础的近端线性化算法来处理非光滑和非凸优化问题。

Result: 在多个基准数据集上的实验结果表明，所提方法在准确性和稀疏性方面均表现出优越性能。

Conclusion: RIKLR模型通过L1正则化成功增强了不定核逻辑回归的稀疏性和可解释性，同时保持了良好的分类性能，为解决非光滑非凸优化问题提供了有效算法。

Abstract: Kernel logistic regression (KLR) is a powerful classification method widely
applied across diverse domains. In many real-world scenarios, indefinite
kernels capture more domain-specific structural information than positive
definite kernels. This paper proposes a novel $L_1$-norm regularized indefinite
kernel logistic regression (RIKLR) model, which extends the existing IKLR
framework by introducing sparsity via an $L_1$-norm penalty. The introduction
of this regularization enhances interpretability and generalization while
introducing nonsmoothness and nonconvexity into the optimization landscape. To
address these challenges, a theoretically grounded and computationally
efficient proximal linearized algorithm is developed. Experimental results on
multiple benchmark datasets demonstrate the superior performance of the
proposed method in terms of both accuracy and sparsity.

</details>


### [18] [Bias-Corrected Data Synthesis for Imbalanced Learning](https://arxiv.org/abs/2510.26046)
*Pengfei Lyu,Zhengchi Ma,Linjun Zhang,Anru R. Zhang*

Main category: stat.ML

TL;DR: 本文提出了一种纠正合成数据偏差的方法，通过从多数类中借用信息来提供一致的偏差估计，从而提高不平衡数据分类的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 不平衡数据中，合成数据依赖于观测数据且无法准确复制原始数据分布，当将合成数据简单视为真实数据时会降低预测准确性。

Method: 提出偏差校正程序，通过从多数类借用信息来估计合成数据引入的偏差，并将其扩展到不平衡多任务学习和因果推断等更广泛场景。

Result: 理论分析提供了偏差估计误差的界限和预测准确性的改进，在手写数字数据集上的仿真和数据分析证明了方法的有效性。

Conclusion: 该方法能有效减轻合成数据的不利影响，提高预测准确性同时避免过拟合，适用于不平衡数据分类的多种场景。

Abstract: Imbalanced data, where the positive samples represent only a small proportion
compared to the negative samples, makes it challenging for classification
problems to balance the false positive and false negative rates. A common
approach to addressing the challenge involves generating synthetic data for the
minority group and then training classification models with both observed and
synthetic data. However, since the synthetic data depends on the observed data
and fails to replicate the original data distribution accurately, prediction
accuracy is reduced when the synthetic data is naively treated as the true
data. In this paper, we address the bias introduced by synthetic data and
provide consistent estimators for this bias by borrowing information from the
majority group. We propose a bias correction procedure to mitigate the adverse
effects of synthetic data, enhancing prediction accuracy while avoiding
overfitting. This procedure is extended to broader scenarios with imbalanced
data, such as imbalanced multi-task learning and causal inference. Theoretical
properties, including bounds on bias estimation errors and improvements in
prediction accuracy, are provided. Simulation results and data analysis on
handwritten digit datasets demonstrate the effectiveness of our method.

</details>


### [19] [Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems](https://arxiv.org/abs/2510.26061)
*Tomoharu Iwata,Futoshi Futami*

Main category: stat.ML

TL;DR: 提出基于图神经网络的变量降维框架，通过实例特定投影高效求解高维二次规划问题，在保证解质量的同时显著减少计算时间


<details>
  <summary>Details</summary>
Motivation: 传统二次规划求解器在高维问题中计算成本高昂，需要开发能针对每个QP实例生成定制化投影的数据驱动方法，以降低问题维度并提高求解效率

Method: 使用图神经网络生成实例特定的投影矩阵，将高维QP投影到低维空间；采用双层优化框架训练模型，内层用QP求解器求解投影后问题，外层更新模型参数；开发了不通过求解器反向传播的高效梯度计算算法

Result: 实验结果表明该方法能为未见过的QP问题生成高质量可行解，计算时间显著减少，性能优于现有方法

Conclusion: 该数据驱动框架通过神经网络生成的实例特定投影，有效解决了高维二次规划问题的计算效率问题，具有良好的泛化能力

Abstract: We propose a data-driven framework for efficiently solving quadratic
programming (QP) problems by reducing the number of variables in
high-dimensional QPs using instance-specific projection. A graph neural
network-based model is designed to generate projections tailored to each QP
instance, enabling us to produce high-quality solutions even for previously
unseen problems. The model is trained on heterogeneous QPs to minimize the
expected objective value evaluated on the projected solutions. This is
formulated as a bilevel optimization problem; the inner optimization solves the
QP under a given projection using a QP solver, while the outer optimization
updates the model parameters. We develop an efficient algorithm to solve this
bilevel optimization problem, which computes parameter gradients without
backpropagating through the solver. We provide a theoretical analysis of the
generalization ability of solving QPs with projection matrices generated by
neural networks. Experimental results demonstrate that our method produces
high-quality feasible solutions with reduced computation time, outperforming
existing methods.

</details>


### [20] [Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning](https://arxiv.org/abs/2510.26121)
*Mara Daniels,Liam Hodgkinson,Michael Mahoney*

Main category: stat.ML

TL;DR: 提出了物理信息对数证据（PILE）评分，作为物理信息机器学习模型的单一不确定性感知度量，用于解决多目标优化中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习（PIML）通常同时包含数据和物理约束的多目标优化，这导致模型质量评估存在模糊性，与认知不确定性理解不足相关，即使统计指标显示良好拟合也可能出现意外失败模式。

Method: 在高斯过程回归框架内引入PILE评分，绕过测试损失的模糊性，为PIML模型超参数提供选择原则。PILE最小化可优化各种模型参数，包括核带宽、最小二乘正则化权重和核函数选择。

Result: PILE最小化能为多种模型参数提供优秀选择，即使在数据获取前，特殊的'无数据'PILE评分也能识别先验核选择，使其'良好适应'给定偏微分方程。

Conclusion: PILE评分可以扩展到更广泛的PIML应用，并提出了相应的扩展方法。

Abstract: Physics-informed machine learning (PIML) integrates prior physical
information, often in the form of differential equation constraints, into the
process of fitting machine learning models to physical data. Popular PIML
approaches, including neural operators, physics-informed neural networks,
neural ordinary differential equations, and neural discrete equilibria, are
typically fit to objectives that simultaneously include both data and physical
constraints. However, the multi-objective nature of this approach creates
ambiguity in the measurement of model quality. This is related to a poor
understanding of epistemic uncertainty, and it can lead to surprising failure
modes, even when existing statistical metrics suggest strong fits. Working
within a Gaussian process regression framework, we introduce the
Physics-Informed Log Evidence (PILE) score. Bypassing the ambiguities of test
losses, the PILE score is a single, uncertainty-aware metric that provides a
selection principle for hyperparameters of a PIML model. We show that PILE
minimization yields excellent choices for a wide variety of model parameters,
including kernel bandwidth, least squares regularization weights, and even
kernel function selection. We also show that, even prior to data acquisition, a
special 'data-free' case of the PILE score identifies a priori kernel choices
that are 'well-adapted' to a given PDE. Beyond the kernel setting, we
anticipate that the PILE score can be extended to PIML at large, and we outline
approaches to do so.

</details>


### [21] [Multi-Output Robust and Conjugate Gaussian Processes](https://arxiv.org/abs/2510.26401)
*Joshua Rooijakkers,Leiv Rønneberg,François-Xavier Briol,Jeremias Knoblauch,Matias Altamirano*

Main category: stat.ML

TL;DR: 提出了多输出鲁棒共轭高斯过程（MO-RCGP），扩展了RCGP框架以处理多输出高斯过程中的模型误设和异常值问题，同时保持共轭性和输出间相关性建模能力。


<details>
  <summary>Details</summary>
Motivation: 多输出高斯过程对模型误设和异常值敏感，特别是当多个异常响应变量存在时，误差会通过输出间相关性传播，需要更鲁棒的方法。

Method: 扩展和推广了Altamirano等人（2024）提出的鲁棒共轭高斯过程（RCGP）框架，构建了多输出RCGP（MO-RCGP），能够联合捕捉输出间相关性。

Result: MO-RCGP被证明是鲁棒的多输出高斯过程，具有共轭性，并在金融和癌症研究应用中进行了全面评估。

Conclusion: MO-RCGP成功解决了多输出高斯过程中的鲁棒性问题，为处理相关输出中的异常值提供了有效框架。

Abstract: Multi-output Gaussian process (MOGP) regression allows modelling dependencies
among multiple correlated response variables. Similarly to standard Gaussian
processes, MOGPs are sensitive to model misspecification and outliers, which
can distort predictions within individual outputs. This situation can be
further exacerbated by multiple anomalous response variables whose errors
propagate due to correlations between outputs. To handle this situation, we
extend and generalise the robust and conjugate Gaussian process (RCGP)
framework introduced by Altamirano et al. (2024). This results in the
multi-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and
jointly captures correlations across outputs. We thoroughly evaluate our
approach through applications in finance and cancer research.

</details>


### [22] [Action-Driven Processes for Continuous-Time Control](https://arxiv.org/abs/2510.26672)
*Ruimin He,Shaowei Lin*

Main category: stat.ML

TL;DR: 本文通过动作驱动过程统一了随机过程和强化学习的视角，并将其应用于脉冲神经网络。基于控制即推理的思想，证明了最小化策略驱动的真实分布与奖励驱动的模型分布之间的KL散度等价于最大熵强化学习。


<details>
  <summary>Details</summary>
Motivation: 动作在强化学习和随机过程建模中都是基础概念，它们触发状态转换并促进复杂系统中的信息流动。本文旨在统一这两个领域的视角。

Method: 提出动作驱动过程的概念，利用控制即推理的思想，通过最小化策略驱动的真实分布与奖励驱动的模型分布之间的KL散度。

Result: 证明了最小化KL散度等价于最大熵强化学习，为脉冲神经网络等应用提供了理论基础。

Conclusion: 成功统一了随机过程和强化学习的视角，为复杂系统的建模和控制提供了新的理论框架。

Abstract: At the heart of reinforcement learning are actions - decisions made in
response to observations of the environment. Actions are equally fundamental in
the modeling of stochastic processes, as they trigger discontinuous state
transitions and enable the flow of information through large, complex systems.
In this paper, we unify the perspectives of stochastic processes and
reinforcement learning through action- driven processes, and illustrate their
application to spiking neural networks. Leveraging ideas from
control-as-inference, we show that minimizing the Kullback-Leibler divergence
between a policy-driven true distribution and a reward-driven model
distribution for a suitably defined action-driven process is equivalent to
maximum entropy reinforcement learning.

</details>


### [23] [Assessment of the conditional exchangeability assumption in causal machine learning models: a simulation study](https://arxiv.org/abs/2510.26700)
*Gerard T. Portela,Jason B. Gibbons,Sebastian Schneeweiss,Rishi J. Desai*

Main category: stat.ML

TL;DR: 该研究评估了因果机器学习模型在条件可交换性假设被违反时的表现，以及负控制结果作为诊断工具的有效性。研究发现当条件可交换性被违反时，因果森林和X-learner模型无法准确估计个体化治疗效果，而负控制结果能有效识别未测量混杂因素。


<details>
  <summary>Details</summary>
Motivation: 观察性研究中开发预测个体化治疗效果的因果机器学习模型很少评估条件可交换性假设。本研究旨在评估这些模型在条件可交换性假设被违反时的表现，以及负控制结果作为诊断工具的实用性。

Method: 通过模拟研究，在不同条件下（包括是否存在真实异质性）检查因果森林和X-learner模型生成的个体化治疗效果估计中的混杂偏倚。模拟数据反映了现实世界场景，包括不同程度的混杂、样本量和负控制结果混杂结构。

Result: 当条件可交换性被违反时，因果森林和X-learner模型无法恢复真实的治疗效果异质性，有时甚至在没有异质性的情况下错误地指示异质性。负控制结果成功识别了受未测量混杂影响的亚组。即使负控制结果不完全满足理想假设，它仍然具有信息性，能够标记亚组水平估计中的潜在偏倚。

Conclusion: 条件可交换性的违反严重限制了常规收集的观察性数据中因果机器学习模型个体化治疗效果估计的有效性。负控制结果作为检测亚组特异性未测量混杂的经验诊断工具很有用，应纳入因果机器学习工作流程以支持个体化推断的可信度。

Abstract: Observational studies developing causal machine learning (ML) models for the
prediction of individualized treatment effects (ITEs) seldom conduct empirical
evaluations to assess the conditional exchangeability assumption. We aimed to
evaluate the performance of these models under conditional exchangeability
violations and the utility of negative control outcomes (NCOs) as a diagnostic.
We conducted a simulation study to examine confounding bias in ITE estimates
generated by causal forest and X-learner models under varying conditions,
including the presence or absence of true heterogeneity. We simulated data to
reflect real-world scenarios with differing levels of confounding, sample size,
and NCO confounding structures. We then estimated and compared subgroup-level
treatment effects on the primary outcome and NCOs across settings with and
without unmeasured confounding. When conditional exchangeability was violated,
causal forest and X-learner models failed to recover true treatment effect
heterogeneity and, in some cases, falsely indicated heterogeneity when there
was none. NCOs successfully identified subgroups affected by unmeasured
confounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it
remained informative, flagging potential bias in subgroup level estimates,
though not always pinpointing the subgroup with the largest confounding.
Violations of conditional exchangeability substantially limit the validity of
ITE estimates from causal ML models in routinely collected observational data.
NCOs serve a useful empirical diagnostic tool for detecting subgroup-specific
unmeasured confounding and should be incorporated into causal ML workflows to
support the credibility of individualized inference.

</details>
