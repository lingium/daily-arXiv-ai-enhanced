{"id": "2602.15091", "categories": ["stat.ML", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15091", "abs": "https://arxiv.org/abs/2602.15091", "authors": ["Ali Khalesi", "Mohammad Reza Deylam Salehi"], "title": "Mixture-of-Experts under Finite-Rate Gating: Communication--Generalization Trade-offs", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures decompose prediction tasks into specialized expert sub-networks selected by a gating mechanism. This letter adopts a communication-theoretic view of MoE gating, modeling the gate as a stochastic channel operating under a finite information rate. Within an information-theoretic learning framework, we specialize a mutual-information generalization bound and develop a rate-distortion characterization $D(R_g)$ of finite-rate gating, where $R_g:=I(X; T)$, yielding (under a standard empirical rate-distortion optimality condition) $\\mathbb{E}[R(W)] \\le D(R_g)+δ_m+\\sqrt{(2/m)\\, I(S; W)}$. The analysis yields capacity-aware limits for communication-constrained MoE systems, and numerical simulations on synthetic multi-expert models empirically confirm the predicted trade-offs between gating rate, expressivity, and generalization."}
{"id": "2602.15136", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15136", "abs": "https://arxiv.org/abs/2602.15136", "authors": ["Nick Cannella", "Anzo Teh", "Yanjun Han", "Yury Polyanskiy"], "title": "Universal priors: solving empirical Bayes via Bayesian inference and pretraining", "comment": "40 pages, 5 figures", "summary": "We theoretically justify the recent empirical finding of [Teh et al., 2025] that a transformer pretrained on synthetically generated data achieves strong performance on empirical Bayes (EB) problems. We take an indirect approach to this question: rather than analyzing the model architecture or training dynamics, we ask why a pretrained Bayes estimator, trained under a prespecified training distribution, can adapt to arbitrary test distributions. Focusing on Poisson EB problems, we identify the existence of universal priors such that training under these priors yields a near-optimal regret bound of $\\widetilde{O}(\\frac{1}{n})$ uniformly over all test distributions. Our analysis leverages the classical phenomenon of posterior contraction in Bayesian statistics, showing that the pretrained transformer adapts to unknown test distributions precisely through posterior contraction. This perspective also explains the phenomenon of length generalization, in which the test sequence length exceeds the training length, as the model performs Bayesian inference using a generalized posterior."}
{"id": "2602.15306", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15306", "abs": "https://arxiv.org/abs/2602.15306", "authors": ["Kentaro Kanamori", "Hirofumi Suzuki", "Takuya Takagi"], "title": "Sparse Additive Model Pruning for Order-Based Causal Structure Learning", "comment": "15 pages, 12 figures, to appear in the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Causal structure learning, also known as causal discovery, aims to estimate causal relationships between variables as a form of a causal directed acyclic graph (DAG) from observational data. One of the major frameworks is the order-based approach that first estimates a topological order of the underlying DAG and then prunes spurious edges from the fully-connected DAG induced by the estimated topological order. Previous studies often focus on the former ordering step because it can dramatically reduce the search space of DAGs. In practice, the latter pruning step is equally crucial for ensuring both computational efficiency and estimation accuracy. Most existing methods employ a pruning technique based on generalized additive models and hypothesis testing, commonly known as CAM-pruning. However, this approach can be a computational bottleneck as it requires repeatedly fitting additive models for all variables. Furthermore, it may harm estimation quality due to multiple testing. To address these issues, we introduce a new pruning method based on sparse additive models, which enables direct pruning of redundant edges without relying on hypothesis testing. We propose an efficient algorithm for learning sparse additive models by combining the randomized tree embedding technique with group-wise sparse regression. Experimental results on both synthetic and real datasets demonstrated that our method is significantly faster than existing pruning methods while maintaining comparable or superior accuracy."}
{"id": "2602.15538", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.15538", "abs": "https://arxiv.org/abs/2602.15538", "authors": ["Kessang Flamand", "Victor-Emmanuel Brunel"], "title": "Functional Central Limit Theorem for Stochastic Gradient Descent", "comment": null, "summary": "We study the asymptotic shape of the trajectory of the stochastic gradient descent algorithm applied to a convex objective function. Under mild regularity assumptions, we prove a functional central limit theorem for the properly rescaled trajectory. Our result characterizes the long-term fluctuations of the algorithm around the minimizer by providing a diffusion limit for the trajectory. In contrast with classical central limit theorems for the last iterate or Polyak-Ruppert averages, this functional result captures the temporal structure of the fluctuations and applies to non-smooth settings such as robust location estimation, including the geometric median."}
{"id": "2602.15095", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15095", "abs": "https://arxiv.org/abs/2602.15095", "authors": ["Bronner P. Gonçalves", "Piero L. Olliaro", "Sheena G. Sullivan", "Benjamin J. Cowling"], "title": "Natural direct effects of vaccines and post-vaccination behaviour", "comment": null, "summary": "Knowledge of the protection afforded by vaccines might, in some circumstances, modify a vaccinated individual's behaviour, potentially increasing exposure to pathogens and hindering effectiveness. Although vaccine studies typically do not explicitly account for this possibility in their analyses, we argue that natural direct effects might represent appropriate causal estimands when an objective is to quantify the effect of vaccination on disease while blocking its influence on behaviour. There are, however, complications of a practical nature for the estimation of natural direct effects in this context. Here, we discuss some of these issues, including exposure-outcome and mediator-outcome confounding by healthcare seeking behaviour, and possible approaches to facilitate estimates of these effects. This work highlights the importance of data collection on behaviour, of assessing whether vaccination induces riskier behaviour, and of understanding the potential effects of interventions on vaccination that could turn off vaccine's influence on behaviour."}
{"id": "2602.15385", "categories": ["stat.AP", "q-fin.RM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15385", "abs": "https://arxiv.org/abs/2602.15385", "authors": ["Ronald Richman", "Mario V. Wüthrich"], "title": "From Chain-Ladder to Individual Claims Reserving", "comment": null, "summary": "The chain-ladder (CL) method is the most widely used claims reserving technique in non-life insurance. This manuscript introduces a novel approach to computing the CL reserves based on a fundamental restructuring of the data utilization for the CL prediction procedure. Instead of rolling forward the cumulative claims with estimated CL factors, we estimate multi-period factors that project the latest observations directly to the ultimate claims. This alternative perspective on CL reserving creates a natural pathway for the application of machine learning techniques to individual claims reserving. As a proof of concept, we present a small-scale real data application employing neural networks for individual claims reserving."}
{"id": "2602.15559", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15559", "abs": "https://arxiv.org/abs/2602.15559", "authors": ["Gabriel Saco"], "title": "Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities", "comment": "32 pages. Comments welcome", "summary": "Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails."}
{"id": "2602.15150", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.15150", "abs": "https://arxiv.org/abs/2602.15150", "authors": ["Daniel K. Sewell", "Alan T. Arakkal"], "title": "bayesics: Core Statistical Methods via Bayesian Inference in R", "comment": null, "summary": "Bayesian statistics is an integral part of contemporary applied science. bayesics provides a single framework, unified in syntax and output, for performing the most commonly used statistical procedures, ranging from one- and two-sample inference to general mediation analysis. bayesics leans hard away from the requirement that users be familiar with sampling algorithms by using closed-form solutions whenever possible, and automatically selecting the number of posterior samples required for accurate inference when such solutions are not possible. bayesics} focuses on providing key inferential quantities: point estimates, credible intervals, probability of direction, region of practical equivalance (ROPE), and, when applicable, Bayes factors. While algorithmic assessment is not required in bayesics, model assessment is still critical; towards that, bayesics provides diagnostic plots for parametric inference, including Bayesian p-values. Finally, bayesics provides extensions to models implemented in alternative R packages and, in the case of mediation analysis, correction to existing implementations."}
{"id": "2602.15385", "categories": ["stat.AP", "q-fin.RM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15385", "abs": "https://arxiv.org/abs/2602.15385", "authors": ["Ronald Richman", "Mario V. Wüthrich"], "title": "From Chain-Ladder to Individual Claims Reserving", "comment": null, "summary": "The chain-ladder (CL) method is the most widely used claims reserving technique in non-life insurance. This manuscript introduces a novel approach to computing the CL reserves based on a fundamental restructuring of the data utilization for the CL prediction procedure. Instead of rolling forward the cumulative claims with estimated CL factors, we estimate multi-period factors that project the latest observations directly to the ultimate claims. This alternative perspective on CL reserving creates a natural pathway for the application of machine learning techniques to individual claims reserving. As a proof of concept, we present a small-scale real data application employing neural networks for individual claims reserving."}
{"id": "2602.15150", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.15150", "abs": "https://arxiv.org/abs/2602.15150", "authors": ["Daniel K. Sewell", "Alan T. Arakkal"], "title": "bayesics: Core Statistical Methods via Bayesian Inference in R", "comment": null, "summary": "Bayesian statistics is an integral part of contemporary applied science. bayesics provides a single framework, unified in syntax and output, for performing the most commonly used statistical procedures, ranging from one- and two-sample inference to general mediation analysis. bayesics leans hard away from the requirement that users be familiar with sampling algorithms by using closed-form solutions whenever possible, and automatically selecting the number of posterior samples required for accurate inference when such solutions are not possible. bayesics} focuses on providing key inferential quantities: point estimates, credible intervals, probability of direction, region of practical equivalance (ROPE), and, when applicable, Bayes factors. While algorithmic assessment is not required in bayesics, model assessment is still critical; towards that, bayesics provides diagnostic plots for parametric inference, including Bayesian p-values. Finally, bayesics provides extensions to models implemented in alternative R packages and, in the case of mediation analysis, correction to existing implementations."}
{"id": "2602.15429", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15429", "abs": "https://arxiv.org/abs/2602.15429", "authors": ["Marios Papamichalis", "Nikolaos Nakis", "Nicholas A. Christakis"], "title": "Deep description of static and dynamic network ties in Honduran villages", "comment": "This is the first draft of the paper. It is under review at a statistics journal", "summary": "We examine static and dynamic social network structure in 176 villages within the Copan Department of Honduras across two data waves (2016, 2019), using detailed data on multiplex networks for 20,232 individuals enrolled in a longitudinal survey. These networks capture friendship, health advice, financial help, and adversarial relationships, allowing us to show how cooperation and conflict jointly shape social structure. Using node-level network measures derived from near-census sociocentric village networks, we leverage mixed-effects zero-inflated negative binomial models to assess the influence of individual attributes, such as gender, marital status, education, religion, and indigenous status, and of village characteristics, on the dynamics of social networks over time. We complement these node-level models with dyadic assortativity (odds-ratio-based homophily) and community-level measures to describe how sorting by key attributes differs across network types and between waves. Our results demonstrate significant assortativity based on gender and religion, particularly within health and financial networks. Across networks, gender and religion exhibit the most consistent assortative mixing. Additionally, community-level assortativity metrics indicate that educational and financial factors increasingly influence social ties over time. Our findings provide insights into how personal attributes and community dynamics interact to shape network formation and socio-economic relationships in rural settings over time."}
{"id": "2602.15559", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15559", "abs": "https://arxiv.org/abs/2602.15559", "authors": ["Gabriel Saco"], "title": "Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities", "comment": "32 pages. Comments welcome", "summary": "Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails."}
{"id": "2602.15247", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15247", "abs": "https://arxiv.org/abs/2602.15247", "authors": ["Yuan Bian", "Shelley B. Bull"], "title": "Sample size and power determination for assessing overall SNP effects in joint modeling of longitudinal and time-to-event data", "comment": null, "summary": "Longitudinal biomarkers are frequently collected in clinical studies due to their strong association with time-to-event outcomes. While considerable progress has been made in methods for jointly modeling longitudinal and survival data, comparatively little attention has been paid to statistical design considerations, particularly sample size and power calculations, in genetic studies. Yet, appropriate sample size estimation is essential for ensuring adequate power and valid inference. Genetic variants may influence event risk through both direct effects and indirect effects mediated by longitudinal biomarkers. In this paper, we derive a closed-form sample size formula for testing the overall effect of a single nucleotide polymorphism within a joint modeling framework. Simulation studies demonstrate that the proposed formula yields accurate and robust performance in finite samples. We illustrate the practical utility of our method using data from the Diabetes Control and Complications Trial."}
{"id": "2602.15697", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15697", "abs": "https://arxiv.org/abs/2602.15697", "authors": ["Anthony Almudevar", "Jacob Almudevar"], "title": "Reproducibility and Statistical Methodology", "comment": "34 pages; 4 tables; 7 figures", "summary": "In 2015 the Open Science Collaboration (OSC) (Nosek et al 2015) published a highly influential paper which claimed that a large fraction of published results in the psychological sciences were not reproducible. In this article we review this claim from several points of view. We first offer an extended analysis of the methods used in that study. We show that the OSC methodology induces a bias that is able by itself to explain the discrepancy between the OSC estimates of reproducibility and other more optimistic estimates made by similar studies.\n  The article also offers a more general literature review and discussion of reproducibility in experimental science. We argue, for both scientific and ethical reasons, that a considered balance of false positive and false negative rates is preferable to a single-minded concentration on false positive rates alone."}
{"id": "2602.15568", "categories": ["stat.ME", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15568", "abs": "https://arxiv.org/abs/2602.15568", "authors": ["Algo Carè", "Marco C. Campi", "Simone Garatti"], "title": "Scenario Approach with Post-Design Certification of User-Specified Properties", "comment": null, "summary": "The scenario approach is an established data-driven design framework that comes equipped with a powerful theory linking design complexity to generalization properties. In this approach, data are simultaneously used both for design and for certifying the design's reliability, without resorting to a separate test dataset. This paper takes a step further by guaranteeing additional properties, useful in post-design usage but not considered during the design phase. To this end, we introduce a two-level framework of appropriateness: baseline appropriateness, which guides the design process, and post-design appropriateness, which serves as a criterion for a posteriori evaluation. We provide distribution-free upper bounds on the risk of failing to meet the post-design appropriateness; these bounds are computable without using any additional test data. Under additional assumptions, lower bounds are also derived. As part of an effort to demonstrate the usefulness of the proposed methodology, the paper presents two practical examples in H2 and pole-placement problems. Moreover, a method is provided to infer comprehensive distributional knowledge of relevant performance indexes from the available dataset."}
{"id": "2602.15291", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.15291", "abs": "https://arxiv.org/abs/2602.15291", "authors": ["Takuma Yoshida", "Koki Momoki", "Shuichi Kawano"], "title": "Structural grouping of extreme value models via graph fused lasso", "comment": "40 pages, 14 figures", "summary": "The generalized Pareto distribution (GPD) is a fundamental model for analyzing the tail behavior of a distribution. In particular, the shape parameter of the GPD characterizes the extremal properties of the distribution. As described in this paper, we propose a method for grouping shape parameters in the GPD for clustered data via graph fused lasso. The proposed method simultaneously estimates the model parameters and identifies which clusters can be grouped together. We establish the asymptotic theory of the proposed estimator and demonstrate that its variance is lower than that of the cluster-wise estimator. This variance reduction not only enhances estimation stability but also provides a principled basis for identifying homogeneity and heterogeneity among clusters in terms of their tail behavior. We assess the performance of the proposed estimator through Monte Carlo simulations. As an illustrative example, our method is applied to rainfall data from 996 clustered sites across Japan."}
{"id": "2602.15809", "categories": ["stat.AP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15809", "abs": "https://arxiv.org/abs/2602.15809", "authors": ["Yuqi Tian", "Robert Paine", "Attila Dobi", "Kevin O'Sullivan", "Aravindh Manickavasagam", "Faisal Farooq"], "title": "Decision Quality Evaluation Framework at Pinterest", "comment": null, "summary": "Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demonstrate the framework's practical application in several key areas: benchmarking the cost-performance trade-offs of various LLM agents, establishing a rigorous methodology for data-driven prompt optimization, managing complex policy evolution, and ensuring the integrity of policy content prevalence metrics via continuous validation. The framework enables a shift from subjective assessments to a data-driven and quantitative practice for managing content safety systems."}
{"id": "2602.15319", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15319", "abs": "https://arxiv.org/abs/2602.15319", "authors": ["Agnideep Aich", "Md. Monzur Murshed", "Sameera Hewage", "Ashit Baran Aich"], "title": "Bayesian Inference for Joint Tail Risk in Paired Biomarkers via Archimedean Copulas with Restricted Jeffreys Priors", "comment": null, "summary": "We propose a Bayesian copula-based framework to quantify clinically interpretable joint tail risks from paired continuous biomarkers. After converting each biomarker margin to rank-based pseudo-observations, we model dependence using one-parameter Archimedean copulas and focus on three probability-scale summaries at tail level $α$: the lower-tail joint risk $R_L(θ)=C_θ(α,α)$, the upper-tail joint risk $R_U(θ)=2α-1+C_θ(1-α,1-α)$, and the conditional lower-tail risk $R_C(θ)=R_L(θ)/α$. Uncertainty is quantified via a restricted Jeffreys prior on the copula parameter and grid-based posterior approximation, which induces an exact posterior for each tail-risk functional. In simulations from Clayton and Gumbel copulas across multiple dependence strengths, posterior credible intervals achieve near-nominal coverage for $R_L$, $R_U$, and $R_C$. We then analyze NHANES 2017--2018 fasting glucose (GLU) and HbA1c (GHB) ($n=2887$) at $α=0.05$, obtaining tight posterior credible intervals for both the dependence parameter and induced tail risks. The results reveal markedly elevated extremal co-movement relative to independence; under the Gumbel model, the posterior mean joint upper-tail risk is $R_U(α)=0.0286$, approximately $11.46\\times$ the independence benchmark $α^2=0.0025$. Overall, the proposed approach provides a principled, dependence-aware method for reporting joint and conditional extremal-risk summaries with Bayesian uncertainty quantification in biomedical applications."}
{"id": "2602.15095", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15095", "abs": "https://arxiv.org/abs/2602.15095", "authors": ["Bronner P. Gonçalves", "Piero L. Olliaro", "Sheena G. Sullivan", "Benjamin J. Cowling"], "title": "Natural direct effects of vaccines and post-vaccination behaviour", "comment": null, "summary": "Knowledge of the protection afforded by vaccines might, in some circumstances, modify a vaccinated individual's behaviour, potentially increasing exposure to pathogens and hindering effectiveness. Although vaccine studies typically do not explicitly account for this possibility in their analyses, we argue that natural direct effects might represent appropriate causal estimands when an objective is to quantify the effect of vaccination on disease while blocking its influence on behaviour. There are, however, complications of a practical nature for the estimation of natural direct effects in this context. Here, we discuss some of these issues, including exposure-outcome and mediator-outcome confounding by healthcare seeking behaviour, and possible approaches to facilitate estimates of these effects. This work highlights the importance of data collection on behaviour, of assessing whether vaccination induces riskier behaviour, and of understanding the potential effects of interventions on vaccination that could turn off vaccine's influence on behaviour."}
{"id": "2602.15374", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15374", "abs": "https://arxiv.org/abs/2602.15374", "authors": ["Cheng-Han Yang", "Xu Shi", "Bhramar Mukherjee"], "title": "Joint Modeling of Longitudinal EHR Data with Shared Random Effects for Informative Visiting and Observation Processes", "comment": "37 pages, 8 figures, 6 tables; with 30-page supplementary material (total 67 pages)", "summary": "Longitudinal electronic health record (EHR) data offer opportunities to study biomarker trajectories; however, association estimates-the primary inferential target-from standard models designed for regular observation times may be biased by a two-stage hierarchical missingness mechanism. The first stage is the visiting process (informative presence), where encounters occur at irregular times driven by patient health status; the second is the observation process (informative observation), where biomarkers are selectively measured during visits. To address these mechanisms, we propose a unified semiparametric joint modeling framework that simultaneously characterizes the visiting, biomarker observation, and longitudinal outcome processes. Central to this framework is a shared subject-specific Gaussian latent variable that captures unmeasured frailty and induces dependence across all components. We develop a three-stage estimation procedure and establish the consistency and asymptotic normality of our estimators. We also introduce a sequential procedure that imputes missing biomarkers prior to adjusting for irregular visiting and examine its performance. Simulation results demonstrate that our method yields unbiased estimates under this mechanism, whereas existing approaches can be substantially biased; notably, methods adjusting only for irregular visiting may exhibit even greater bias than those ignoring both mechanisms. We apply our framework to data from the All of Us Research Program to investigate associations between neighborhood-level socioeconomic status indicators and six blood-based biomarker trajectories, providing a robust tool for outpatient settings where irregular monitoring and selective measurement are prevalent."}
{"id": "2602.15247", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15247", "abs": "https://arxiv.org/abs/2602.15247", "authors": ["Yuan Bian", "Shelley B. Bull"], "title": "Sample size and power determination for assessing overall SNP effects in joint modeling of longitudinal and time-to-event data", "comment": null, "summary": "Longitudinal biomarkers are frequently collected in clinical studies due to their strong association with time-to-event outcomes. While considerable progress has been made in methods for jointly modeling longitudinal and survival data, comparatively little attention has been paid to statistical design considerations, particularly sample size and power calculations, in genetic studies. Yet, appropriate sample size estimation is essential for ensuring adequate power and valid inference. Genetic variants may influence event risk through both direct effects and indirect effects mediated by longitudinal biomarkers. In this paper, we derive a closed-form sample size formula for testing the overall effect of a single nucleotide polymorphism within a joint modeling framework. Simulation studies demonstrate that the proposed formula yields accurate and robust performance in finite samples. We illustrate the practical utility of our method using data from the Diabetes Control and Complications Trial."}
{"id": "2602.15387", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15387", "abs": "https://arxiv.org/abs/2602.15387", "authors": ["Durba Bhattacharya", "Sourabh Bhattacharya"], "title": "Bayesian Nonparametrics for Gene-Gene and Gene-Environment Interactions in Case-Control Studies: A Synthesis and Extension", "comment": "Feedback welcome", "summary": "Gene-gene and gene-environment interactions are widely believed to play significant roles in explaining the variability of complex traits. While substantial research exists in this area, a comprehensive statistical framework that addresses multiple sources of uncertainty simultaneously remains lacking. In this article, we synthesize and propose extension of a novel class of Bayesian nonparametric approaches that account for interactions among genes, loci, and environmental factors while accommodating uncertainty about population substructure. Our contribution is threefold: (1) We provide a unified exposition of hierarchical Bayesian models driven by Dirichlet processes for genetic interactions, clarifying their conceptual advantages over traditional regression approaches; (2) We shed light on new computational strategies that combine transformation-based MCMC with parallel processing for scalable inference; and (3) We present enhanced hypothesis testing procedures for identifying disease-predisposing loci.Through applications to myocardial infarction data, we demonstrate how these methods offer biological insights not readily obtainable from standard approaches. Our synthesis highlights the advantages of Bayesian nonparametric thinking in genetic epidemiology while providing practical guidance for implementation."}
{"id": "2602.15319", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15319", "abs": "https://arxiv.org/abs/2602.15319", "authors": ["Agnideep Aich", "Md. Monzur Murshed", "Sameera Hewage", "Ashit Baran Aich"], "title": "Bayesian Inference for Joint Tail Risk in Paired Biomarkers via Archimedean Copulas with Restricted Jeffreys Priors", "comment": null, "summary": "We propose a Bayesian copula-based framework to quantify clinically interpretable joint tail risks from paired continuous biomarkers. After converting each biomarker margin to rank-based pseudo-observations, we model dependence using one-parameter Archimedean copulas and focus on three probability-scale summaries at tail level $α$: the lower-tail joint risk $R_L(θ)=C_θ(α,α)$, the upper-tail joint risk $R_U(θ)=2α-1+C_θ(1-α,1-α)$, and the conditional lower-tail risk $R_C(θ)=R_L(θ)/α$. Uncertainty is quantified via a restricted Jeffreys prior on the copula parameter and grid-based posterior approximation, which induces an exact posterior for each tail-risk functional. In simulations from Clayton and Gumbel copulas across multiple dependence strengths, posterior credible intervals achieve near-nominal coverage for $R_L$, $R_U$, and $R_C$. We then analyze NHANES 2017--2018 fasting glucose (GLU) and HbA1c (GHB) ($n=2887$) at $α=0.05$, obtaining tight posterior credible intervals for both the dependence parameter and induced tail risks. The results reveal markedly elevated extremal co-movement relative to independence; under the Gumbel model, the posterior mean joint upper-tail risk is $R_U(α)=0.0286$, approximately $11.46\\times$ the independence benchmark $α^2=0.0025$. Overall, the proposed approach provides a principled, dependence-aware method for reporting joint and conditional extremal-risk summaries with Bayesian uncertainty quantification in biomedical applications."}
{"id": "2602.15390", "categories": ["stat.ME", "math.NA", "math.NT"], "pdf": "https://arxiv.org/pdf/2602.15390", "abs": "https://arxiv.org/abs/2602.15390", "authors": ["Naoki Sakai", "Takashi Goda"], "title": "Space-filling lattice designs for computer experiments", "comment": "24 pages, 5 figures", "summary": "This paper investigates the construction of space-filling designs for computer experiments. The space-filling property is characterized by the covering and separation radii of a design, which are integrated through the unified criterion of quasi-uniformity. We focus on a special class of designs, known as quasi-Monte Carlo (QMC) lattice point sets, and propose two construction algorithms. The first algorithm generates rank-1 lattice point sets as an approximation of quasi-uniform Kronecker sequences, where the generating vector is determined explicitly. As a byproduct of our analysis, we prove that this explicit point set achieves an isotropic discrepancy of $O(N^{-1/d})$. The second algorithm utilizes Korobov lattice point sets, employing the Lenstra--Lenstra--Lovász (LLL) basis reduction algorithm to identify the generating vector that ensures quasi-uniformity. Numerical experiments are provided to validate our theoretical claims regarding quasi-uniformity. Furthermore, we conduct empirical comparisons between various QMC point sets in the context of Gaussian process regression, showcasing the efficacy of the proposed designs for computer experiments."}
{"id": "2602.15374", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15374", "abs": "https://arxiv.org/abs/2602.15374", "authors": ["Cheng-Han Yang", "Xu Shi", "Bhramar Mukherjee"], "title": "Joint Modeling of Longitudinal EHR Data with Shared Random Effects for Informative Visiting and Observation Processes", "comment": "37 pages, 8 figures, 6 tables; with 30-page supplementary material (total 67 pages)", "summary": "Longitudinal electronic health record (EHR) data offer opportunities to study biomarker trajectories; however, association estimates-the primary inferential target-from standard models designed for regular observation times may be biased by a two-stage hierarchical missingness mechanism. The first stage is the visiting process (informative presence), where encounters occur at irregular times driven by patient health status; the second is the observation process (informative observation), where biomarkers are selectively measured during visits. To address these mechanisms, we propose a unified semiparametric joint modeling framework that simultaneously characterizes the visiting, biomarker observation, and longitudinal outcome processes. Central to this framework is a shared subject-specific Gaussian latent variable that captures unmeasured frailty and induces dependence across all components. We develop a three-stage estimation procedure and establish the consistency and asymptotic normality of our estimators. We also introduce a sequential procedure that imputes missing biomarkers prior to adjusting for irregular visiting and examine its performance. Simulation results demonstrate that our method yields unbiased estimates under this mechanism, whereas existing approaches can be substantially biased; notably, methods adjusting only for irregular visiting may exhibit even greater bias than those ignoring both mechanisms. We apply our framework to data from the All of Us Research Program to investigate associations between neighborhood-level socioeconomic status indicators and six blood-based biomarker trajectories, providing a robust tool for outpatient settings where irregular monitoring and selective measurement are prevalent."}
{"id": "2602.15496", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.15496", "abs": "https://arxiv.org/abs/2602.15496", "authors": ["Céline Cunen", "Nils Lid Hjort"], "title": "Confidence Distributions for FIC scores", "comment": "26 pages, 9 figures, 2020 version, later published in essentially this form, Econometrics 2020, volume 8, number27, www.mdpi.com/2225-1146/8/3/27", "summary": "When using the Focused Information Criterion (FIC) for assessing and ranking candidate models with respect to how well they do for a given estimation task, it is customary to produce a so-called FIC plot. This plot has the different point estimates along the y-axis and the root-FIC scores on the x-axis, these being the estimated root-mean-square scores. In this paper we address the estimation uncertainty involved in each of the points of such a FIC plot. This needs careful assessment of each of the estimators from the candidate models, taking also modelling bias into account, along with the relative precision of the associated estimated mean squared error quantities. We use confidence distributions for these endeavours. This leads to fruitful CD-FIC plots, helping the statistician to judge to what extent the seemingly best models really are better than other models, etc. These efforts also lead to two further developments. The first is a new tool for model selection, which we call the quantile FIC, which helps overcome certain difficulties associated with the usual FIC procedures, related to somewhat arbitrary schemes for handling estimated squared biases. A particular case is the median-FIC. The second development is to form model averaged estimators with fruitful weights determined by the relative sizes of the median- and quantile-FIC scores. And Mrs. Jones is pregnant."}
{"id": "2602.15387", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.15387", "abs": "https://arxiv.org/abs/2602.15387", "authors": ["Durba Bhattacharya", "Sourabh Bhattacharya"], "title": "Bayesian Nonparametrics for Gene-Gene and Gene-Environment Interactions in Case-Control Studies: A Synthesis and Extension", "comment": "Feedback welcome", "summary": "Gene-gene and gene-environment interactions are widely believed to play significant roles in explaining the variability of complex traits. While substantial research exists in this area, a comprehensive statistical framework that addresses multiple sources of uncertainty simultaneously remains lacking. In this article, we synthesize and propose extension of a novel class of Bayesian nonparametric approaches that account for interactions among genes, loci, and environmental factors while accommodating uncertainty about population substructure. Our contribution is threefold: (1) We provide a unified exposition of hierarchical Bayesian models driven by Dirichlet processes for genetic interactions, clarifying their conceptual advantages over traditional regression approaches; (2) We shed light on new computational strategies that combine transformation-based MCMC with parallel processing for scalable inference; and (3) We present enhanced hypothesis testing procedures for identifying disease-predisposing loci.Through applications to myocardial infarction data, we demonstrate how these methods offer biological insights not readily obtainable from standard approaches. Our synthesis highlights the advantages of Bayesian nonparametric thinking in genetic epidemiology while providing practical guidance for implementation."}
{"id": "2602.15559", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15559", "abs": "https://arxiv.org/abs/2602.15559", "authors": ["Gabriel Saco"], "title": "Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities", "comment": "32 pages. Comments welcome", "summary": "Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails."}
{"id": "2602.15568", "categories": ["stat.ME", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15568", "abs": "https://arxiv.org/abs/2602.15568", "authors": ["Algo Carè", "Marco C. Campi", "Simone Garatti"], "title": "Scenario Approach with Post-Design Certification of User-Specified Properties", "comment": null, "summary": "The scenario approach is an established data-driven design framework that comes equipped with a powerful theory linking design complexity to generalization properties. In this approach, data are simultaneously used both for design and for certifying the design's reliability, without resorting to a separate test dataset. This paper takes a step further by guaranteeing additional properties, useful in post-design usage but not considered during the design phase. To this end, we introduce a two-level framework of appropriateness: baseline appropriateness, which guides the design process, and post-design appropriateness, which serves as a criterion for a posteriori evaluation. We provide distribution-free upper bounds on the risk of failing to meet the post-design appropriateness; these bounds are computable without using any additional test data. Under additional assumptions, lower bounds are also derived. As part of an effort to demonstrate the usefulness of the proposed methodology, the paper presents two practical examples in H2 and pole-placement problems. Moreover, a method is provided to infer comprehensive distributional knowledge of relevant performance indexes from the available dataset."}
{"id": "2602.15673", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.15673", "abs": "https://arxiv.org/abs/2602.15673", "authors": ["Sheikh Badar Ud Din Tahir", "Leonardo Egidi", "Nicola Torelli"], "title": "Leicester's Tale: Another Perspective on the EPL 2015/16 Through Expected Goals (xG) Modelling", "comment": null, "summary": "Probabilistic modeling is an effective tool for evaluating team performance and predicting outcomes in sports. However, an important question that hasn't been fully explored is whether these models can reliably reflect actual performance while assigning meaningful probabilities to rare results that differ greatly from expectations. In this study, we create an inference-based probabilistic framework built on expected goals (xG). This framework converts shot-level event data into season-level simulations of points, rankings, and outcome probabilities. Using the English Premier League 2015/16 season as a data, we demonstrate that the framework captures the overall structure of the league table. It correctly identifies the top-four contenders and relegation candidates while explaining a significant portion of the variance in final points and ranks. In a full-season evaluation, the model assigns a low probability to extreme outcomes, particularly Leicester City's historic title win, which stands out as a statistical anomaly. We then look at the ex ante inferential and early-diagnostic role of xG by only using mid-season information. With first-half data, we simulate the rest of the season and show that teams with stronger mid-season xG profiles tend to earn more points in the second half, even after considering their current league position. In this mid-season assessment, Leicester City ranks among the top teams by xG and is given a small but noteworthy chance of winning the league. This suggests that their ultimate success was unlikely but not entirely detached from their actual performance. Our analysis indicates that expected goals models work best as probabilistic baselines for analysis and early-warning diagnostics, rather than as certain predictors of rare season outcomes."}
{"id": "2602.15679", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.15679", "abs": "https://arxiv.org/abs/2602.15679", "authors": ["Ori Davidov"], "title": "Safe hypotheses testing with application to order restricted inference", "comment": null, "summary": "Hypothesis tests under order restrictions arise in a wide range of scientific applications. By exploiting inequality constraints, such tests can achieve substantial gains in power and interpretability. However, these gains come at a cost: when the imposed constraints are misspecified, the resulting inferences may be misleading or even invalid, and Type III errors may occur, i.e., the null hypothesis may be rejected when neither the null nor the alternative is true. To address this problem, this paper introduces safe tests. Heuristically, a safe test is a testing procedure that is asymptotically free of Type III errors. The proposed test is accompanied by a certificate of validity, a pre--test that assesses whether the original hypotheses are consistent with the data, thereby ensuring that the null hypothesis is rejected only when warranted, enabling principled inference without risk of systematic error. Although the development in this paper focus on testing problems in order--restricted inference, the underlying ideas are more broadly applicable. The proposed methodology is evaluated through simulation studies and the analysis of well--known illustrative data examples, demonstrating strong protection against Type III errors while maintaining power comparable to standard procedures."}
{"id": "2602.15731", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.15731", "abs": "https://arxiv.org/abs/2602.15731", "authors": ["Laura M. Craig", "Wagner Barreto-Souza"], "title": "Generalised Exponential Kernels for Nonparametric Density Estimation", "comment": "Paper submitted for publication", "summary": "This paper introduces a novel kernel density estimator (KDE) based on the generalised exponential (GE) distribution, designed specifically for positive continuous data. The proposed GE KDE offers a mathematically tractable form that avoids the use of special functions, for instance, distinguishing it from the widely used gamma KDE, which relies on the gamma function. Despite its simpler form, the GE KDE maintains similar flexibility and shape characteristics, aligning with distributions such as the gamma, which are known for their effectiveness in modelling positive data. We derive the asymptotic bias and variance of the proposed kernel density estimator, and formally demonstrate the order of magnitude of the remaining terms in these expressions. We also propose a second GE KDE, for which we are able to show that it achieves the optimal mean integrated squared error, something that is difficult to establish for the former. Through numerical experiments involving simulated and real data sets, we show that GE KDEs can be an important alternative and competitive to existing KDEs."}
