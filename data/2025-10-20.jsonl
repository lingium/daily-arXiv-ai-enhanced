{"id": "2510.15618", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.15618", "abs": "https://arxiv.org/abs/2510.15618", "authors": ["Abdul-Nasah Soale", "Adewale Lukman"], "title": "Adaptive Influence Diagnostics in High-Dimensional Regression", "comment": null, "summary": "An adaptive Cook's distance (ACD) for diagnosing influential observations in\nhigh-dimensional single-index models with multicollinearity and outlier\ncontamination is proposed. ACD is a model-free technique built on sparse local\nlinear gradients to temper leverage effects. In simulations spanning low- and\nhigh-dimensional design settings with strong correlation, ACD based on LASSO\n(ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative\nto classical Cook's distance and local influence as well as the DF-Model and\nCase-Weight adjusted solution for LASSO. Trimming points flagged by ACD\nstabilizes variable selection while preserving core signals. Applications to\ntwo datasets--the 1960 US cities pollution study and a high-dimensional\nriboflavin genomics experiment show consistent gains in selection stability and\ninterpretability."}
{"id": "2510.15003", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15003", "abs": "https://arxiv.org/abs/2510.15003", "authors": ["Mingao Yuan"], "title": "Asymptotic distribution of the global clustering coefficient in a random annulus graph", "comment": null, "summary": "The global clustering coefficient is an effective measure for analyzing and\ncomparing the structures of complex networks. The random annulus graph is a\nmodified version of the well-known Erd\\H{o}s-R\\'{e}nyi random graph. It has\nbeen recently proposed in modeling network communities. This paper investigates\nthe asymptotic distribution of the global clustering coefficient in a random\nannulus graph. It is demonstrated that the standardized global clustering\ncoefficient converges in law to the standard normal distribution. The result is\nestablished using the asymptotic theory of degenerate U-statistics with a\nsample-size dependent kernel. As far as we know, this method is different from\nestablished approaches for deriving asymptotic distributions of network\nstatistics. Moreover, we get the explicit expression of the limit of the global\nclustering coefficient."}
{"id": "2510.15000", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15000", "abs": "https://arxiv.org/abs/2510.15000", "authors": ["Yixin Fang", "Man Jin"], "title": "Estimand framework and intercurrent events handling for clinical trials with time-to-event outcomes", "comment": null, "summary": "The ICH E9(R1) guideline presents a framework of estimand for clinical\ntrials, proposes five strategies for handling intercurrent events (ICEs), and\nprovides a comprehensive discussion and many real-life clinical examples for\nquantitative outcomes and categorical outcomes. However, in ICH E9(R1) the\ndiscussion is lacking for time-to-event (TTE) outcomes. In this paper, we\ndiscuss how to define estimands and how to handle ICEs for clinical trials with\nTTE outcomes. Specifically, we discuss six ICE handling strategies, including\nthose five strategies proposed by ICH E9(R1) and a new strategy, the\ncompeting-risk strategy. Compared with ICH E9(R1), the novelty of this paper is\nthree-fold: (1) the estimands are defined in terms of potential outcomes, (2)\nthe methods can utilize time-dependent covariates straightforwardly, and (3)\nthe efficient estimators are discussed accordingly."}
{"id": "2510.15011", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15011", "abs": "https://arxiv.org/abs/2510.15011", "authors": ["Tomasz Serafin", "Weronika Nitka"], "title": "Data-driven Calibration Sample Selection and Forecast Combination in Electricity Price Forecasting: An Application of the ARHNN Method", "comment": null, "summary": "Calibration sample selection and forecast combination are two simple yet\npowerful tools used in forecasting. They can be combined with a variety of\nmodels to significantly improve prediction accuracy, at the same time offering\neasy implementation and low computational complexity. While their effectiveness\nhas been repeatedly confirmed in prior scientific literature, the topic is\nstill underexplored in the field of electricity price forecasting. In this\nresearch article we apply the Autoregressive Hybrid Nearest Neighbors (ARHNN)\nmethod to three long-term time series describing the German, Spanish and New\nEngland electricity markets. We show that it outperforms popular literature\nbenchmarks in terms of forecast accuracy by up to 10%. We also propose two\nsimplified variants of the method, granting a vast decrease in computation time\nwith only minor loss of prediction accuracy. Finally, we compare the forecasts'\nperformance in a battery storage system trading case study. We find that using\na forecast-driven strategy can achieve up to 80% of theoretical maximum profits\nwhile trading, demonstrating business value in practical applications."}
{"id": "2510.15012", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15012", "abs": "https://arxiv.org/abs/2510.15012", "authors": ["Yi-Shan Chu", "Yueh-Cheng Kuo"], "title": "From Universal Approximation Theorem to Tropical Geometry of Multi-Layer Perceptrons", "comment": null, "summary": "We revisit the Universal Approximation Theorem(UAT) through the lens of the\ntropical geometry of neural networks and introduce a constructive,\ngeometry-aware initialization for sigmoidal multi-layer perceptrons (MLPs).\nTropical geometry shows that Rectified Linear Unit (ReLU) networks admit\ndecision functions with a combinatorial structure often described as a tropical\nrational, namely a difference of tropical polynomials. Focusing on planar\nbinary classification, we design purely sigmoidal MLPs that adhere to the\nfinite-sum format of UAT: a finite linear combination of shifted and scaled\nsigmoids of affine functions. The resulting models yield decision boundaries\nthat already align with prescribed shapes at initialization and can be refined\nby standard training if desired. This provides a practical bridge between the\ntropical perspective and smooth MLPs, enabling interpretable, shape-driven\ninitialization without resorting to ReLU architectures. We focus on the\nconstruction and empirical demonstrations in two dimensions; theoretical\nanalysis and higher-dimensional extensions are left for future work."}
{"id": "2510.15020", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15020", "abs": "https://arxiv.org/abs/2510.15020", "authors": ["Fan Chen", "Audrey Huang", "Noah Golowich", "Sadhika Malladi", "Adam Block", "Jordan T. Ash", "Akshay Krishnamurthy", "Dylan J. Foster"], "title": "The Coverage Principle: How Pre-training Enables Post-Training", "comment": null, "summary": "Language models demonstrate remarkable abilities when pre-trained on large\ntext corpora and fine-tuned for specific tasks, but how and why pre-training\nshapes the success of the final model remains poorly understood. Notably,\nalthough pre-training success is often quantified by cross entropy loss,\ncross-entropy can be a poor predictor of downstream performance. Instead, we\nprovide a theoretical perspective on this relationship through the lens of\n\\emph{coverage}, which quantifies the probability mass the pre-trained model\nplaces on high-quality responses and which is necessary and sufficient for\npost-training and test-time scaling methods such as Best-of-N to succeed. Our\nmain results develop an understanding of \\emph{the coverage principle}, a\nphenomenon whereby next-token prediction implicitly optimizes toward a model\nwith good coverage. In particular, we uncover a mechanism that explains the\npower of coverage in predicting downstream performance: \\emph{coverage\ngeneralizes faster than cross entropy}, avoiding spurious dependence on\nproblem-dependent parameters such as the sequence length. We also study\npractical algorithmic interventions with provable benefits for improving\ncoverage, including (i) model/checkpoint selection procedures, (ii) gradient\nnormalization schemes, and (iii) test-time decoding strategies."}
{"id": "2510.15003", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15003", "abs": "https://arxiv.org/abs/2510.15003", "authors": ["Mingao Yuan"], "title": "Asymptotic distribution of the global clustering coefficient in a random annulus graph", "comment": null, "summary": "The global clustering coefficient is an effective measure for analyzing and\ncomparing the structures of complex networks. The random annulus graph is a\nmodified version of the well-known Erd\\H{o}s-R\\'{e}nyi random graph. It has\nbeen recently proposed in modeling network communities. This paper investigates\nthe asymptotic distribution of the global clustering coefficient in a random\nannulus graph. It is demonstrated that the standardized global clustering\ncoefficient converges in law to the standard normal distribution. The result is\nestablished using the asymptotic theory of degenerate U-statistics with a\nsample-size dependent kernel. As far as we know, this method is different from\nestablished approaches for deriving asymptotic distributions of network\nstatistics. Moreover, we get the explicit expression of the limit of the global\nclustering coefficient."}
{"id": "2510.15105", "categories": ["stat.AP", "Primary 62P12, Secondary 62P10, 62H25, 62J07"], "pdf": "https://arxiv.org/pdf/2510.15105", "abs": "https://arxiv.org/abs/2510.15105", "authors": ["Mengxiang Zhu", "Riccardo Rastelli"], "title": "Bayesian Additive Regression Trees (BART) in Food Authenticity: A Classification Approach to Food Fraud Detection", "comment": "20 pages, including 12 figures and 4 tables. Preprint under review.\n  Not published", "summary": "Feature engineering plays a critical role in handling hyperspectral data and\nis essential for identifying key wavelengths in food fraud detection. This\nstudy employs Bayesian Additive Regression Trees (BART), a flexible machine\nlearning approach, to discriminate and classify samples of olive oil based on\ntheir level of purity. Leveraging its built-in variable selection mechanism, we\nemploy BART to effectively identify the most representative spectral features\nand to capture the complex interactions among variables. We use network\nrepresentation to illustrate our findings, highlighting the competitiveness of\nour proposed methodology. Results demonstrate that when principal component\nanalysis is used for dimensionality reduction, BART outperforms\nstate-of-the-art models, achieving a classification accuracy of 96.8\\% under\ndefault settings, which further improves to 97.2\\% after hyperparameter tuning.\nIf we leverage a variable selection procedure within BART, the model achieves\nperfect classification performance on this dataset, improving upon previous\noptimal results both in terms of accuracy and interpretability. Our results\ndemonstrate that three key wavelengths, 1160.71 nm, 1328.57 nm, and 1389.29 nm,\nplay a central role in discriminating the olive oil samples, thus highlighting\nan application of our methodology in the context of food quality. Further\nanalysis reveals that these variables do not function independently but rather\ninteract synergistically to achieve accurate classification, and improved\ndetection speed."}
{"id": "2510.15013", "categories": ["stat.ML", "cs.LG", "physics.data-an", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15013", "abs": "https://arxiv.org/abs/2510.15013", "authors": ["Magnus Neuman", "Jelena Smiljanić", "Martin Rosvall"], "title": "Reliable data clustering with Bayesian community detection", "comment": null, "summary": "From neuroscience and genomics to systems biology and ecology, researchers\nrely on clustering similarity data to uncover modular structure. Yet widely\nused clustering methods, such as hierarchical clustering, k-means, and WGCNA,\nlack principled model selection, leaving them susceptible to noise. A common\nworkaround sparsifies a correlation matrix representation to remove noise\nbefore clustering, but this extra step introduces arbitrary thresholds that can\ndistort the structure and lead to unreliable results. To detect reliable\nclusters, we capitalize on recent advances in network science to unite\nsparsification and clustering with principled model selection. We test two\nBayesian community detection methods, the Degree-Corrected Stochastic Block\nModel and the Regularized Map Equation, both grounded in the Minimum\nDescription Length principle for model selection. In synthetic data, they\noutperform traditional approaches, detecting planted clusters under high-noise\nconditions and with fewer samples. Compared to WGCNA on gene co-expression\ndata, the Regularized Map Equation identifies more robust and functionally\ncoherent gene modules. Our results establish Bayesian community detection as a\nprincipled and noise-resistant framework for uncovering modular structure in\nhigh-dimensional data across fields."}
{"id": "2510.15058", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62C20 (Primary) 46E22, 62B10 (Secondary)", "G.3; H.1.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.15058", "abs": "https://arxiv.org/abs/2510.15058", "authors": ["Jose Cribeiro-Ramallo", "Agnideep Aich", "Florian Kalinke", "Ashit Baran Aich", "Zoltán Szabó"], "title": "The Minimax Lower Bound of Kernel Stein Discrepancy Estimation", "comment": null, "summary": "Kernel Stein discrepancies (KSDs) have emerged as a powerful tool for\nquantifying goodness-of-fit over the last decade, featuring numerous successful\napplications. To the best of our knowledge, all existing KSD estimators with\nknown rate achieve $\\sqrt n$-convergence. In this work, we present two\ncomplementary results (with different proof strategies), establishing that the\nminimax lower bound of KSD estimation is $n^{-1/2}$ and settling the optimality\nof these estimators. Our first result focuses on KSD estimation on $\\mathbb\nR^d$ with the Langevin-Stein operator; our explicit constant for the Gaussian\nkernel indicates that the difficulty of KSD estimation may increase\nexponentially with the dimensionality $d$. Our second result settles the\nminimax lower bound for KSD estimation on general domains."}
{"id": "2510.15203", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15203", "abs": "https://arxiv.org/abs/2510.15203", "authors": ["Mauricio Tejo", "Cristian Meza", "Fernando Marmolejo-Ramos"], "title": "Conditional GLMMs for reaction times in choice tasks", "comment": null, "summary": "This study connects two methods for modeling reaction times (RTs) in choice\ntasks: (1) the first-hitting time of a simple diffusion model with a single\nbarrier, representing the cognitive process leading to a response, and (2)\nGeneralized Linear Mixed Models (GLMMs). We achieve this by analyzing RT\ndistributions conditioned on each response alternative. Because certain\ndiffusion model variants yield Inverse Gaussian (IG) and Gamma distributions\nfor first-hitting times, we can justify using these distributions in RT models.\nConversely, employing IG and Gamma distributions within GLMMs allows us to\ninfer the underlying cognitive processes. We demonstrate this concept through\nsimulations and apply it to previously published real-world data. Finally, we\ndiscuss the scope and potential extensions of our approach."}
{"id": "2510.15487", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15487", "abs": "https://arxiv.org/abs/2510.15487", "authors": ["Manit Mishra"], "title": "AI and analytics in sports: Leveraging BERTopic to map the past and chart the future", "comment": "32 pages, 5 figures, 1 table, accepted for presentation at Australia\n  and New Zealand Marketing Academy (ANZMAC) - 2025 Conference", "summary": "Purpose: The purpose of this study is to map the body of scholarly literature\nat the intersection of artificial intelligence (AI), analytics and sports and\nthereafter, leverage the insights generated to chart guideposts for future\nresearch. Design/methodology/approach: The study carries out systematic\nliterature review (SLR). Preferred Reporting Items for Systematic Reviews and\nMeta-Analysis (PRISMA) protocol is leveraged to identify 204 journal articles\npertaining to utilization of AI and analytics in sports published during 2002\nto 2024. We follow it up with extraction of the latent topics from sampled\narticles by leveraging the topic modelling technique of BERTopic. Findings: The\nstudy identifies the following as predominant areas of extant research on usage\nof AI and analytics in sports: performance modelling, physical and mental\nhealth, social media sentiment analysis, and tactical tracking. Each extracted\ntopic is further examined in terms of its relative prominence, representative\nstudies, and key term associations. Drawing on these insights, the study\ndelineates promising avenues for future inquiry. Research\nlimitations/implications: The study offers insights to academicians and sports\nadministrators on transformational impact of AI and analytics in sports.\nOriginality/value: The study introduces BERTopic as a novel approach for\nextracting latent structures in sports research, thereby advancing both\nscholarly understanding and the methodological toolkit of the field."}
{"id": "2510.15014", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.15014", "abs": "https://arxiv.org/abs/2510.15014", "authors": ["Jack Kendrick"], "title": "The Tree-SNE Tree Exists", "comment": null, "summary": "The clustering and visualisation of high-dimensional data is a ubiquitous\ntask in modern data science. Popular techniques include nonlinear\ndimensionality reduction methods like t-SNE or UMAP. These methods face the\n`scale-problem' of clustering: when dealing with the MNIST dataset, do we want\nto distinguish different digits or do we want to distinguish different ways of\nwriting the digits? The answer is task dependent and depends on scale. We\nrevisit an idea of Robinson & Pierce-Hoffman that exploits an underlying\nscaling symmetry in t-SNE to replace 2-dimensional with (2+1)-dimensional\nembeddings where the additional parameter accounts for scale. This gives rise\nto the t-SNE tree (short: tree-SNE). We prove that the optimal embedding\ndepends continuously on the scaling parameter for all initial conditions\noutside a set of measure 0: the tree-SNE tree exists. This idea conceivably\nextends to other attraction-repulsion methods and is illustrated on several\nexamples."}
{"id": "2510.15273", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15273", "abs": "https://arxiv.org/abs/2510.15273", "authors": ["Liner Xiang", "Jiayi Wang", "Hengrui Cai"], "title": "Foresighted Online Policy Optimization with Interference", "comment": null, "summary": "Contextual bandits, which leverage the baseline features of sequentially\narriving individuals to optimize cumulative rewards while balancing exploration\nand exploitation, are critical for online decision-making. Existing approaches\ntypically assume no interference, where each individual's action affects only\ntheir own reward. Yet, such an assumption can be violated in many practical\nscenarios, and the oversight of interference can lead to short-sighted policies\nthat focus solely on maximizing the immediate outcomes for individuals, which\nfurther results in suboptimal decisions and potentially increased regret over\ntime. To address this significant gap, we introduce the foresighted online\npolicy with interference (FRONT) that innovatively considers the long-term\nimpact of the current decision on subsequent decisions and rewards. The\nproposed FRONT method employs a sequence of exploratory and exploitative\nstrategies to manage the intricacies of interference, ensuring robust parameter\ninference and regret minimization. Theoretically, we establish a tail bound for\nthe online estimator and derive the asymptotic distribution of the parameters\nof interest under suitable conditions on the interference network. We further\nshow that FRONT attains sublinear regret under two distinct definitions,\ncapturing both the immediate and consequential impacts of decisions, and we\nestablish these results with and without statistical inference. The\neffectiveness of FRONT is further demonstrated through extensive simulations\nand a real-world application to urban hotel profits."}
{"id": "2510.15272", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15272", "abs": "https://arxiv.org/abs/2510.15272", "authors": ["Atsushi Senda", "Yuki Takatsu", "Ryokan Ikebe", "Hiroshi Suginaka", "Koji Morishita", "Akira Endo"], "title": "Bayesian Sequential Modeling of Time-to-Urination for Dynamic ED Triage", "comment": null, "summary": "Triage tools in routine emergency care are largely static, failing to exploit\nsimple behavioral cues clinicians notice in real time. Here, we developed a\nBayesian, sequentially updating framework that integrates incoming cues to\nproduce calibrated, time-consistent risk. Using a prospective single-center\ncohort of ambulance arrivals in Japan (February-August 2025; n=2,221), we\nevaluated time to first urination (TTU) as a proof-of-concept bedside cue for\npredicting hospital admission. Population-level fit to the cumulative admission\ncurve was excellent (integrated squared error 0.002; RMSE 0.003;\nKolmogorov-Smirnov 0.008; coverage 0.98). At the patient level, performance\nimproved markedly with age/sex adjustment (AUC[t] 0.70 vs. 0.50 unadjusted),\nwith lower Brier scores and positive calibration slopes. Platt recalibration\nrefined probability scaling without altering discrimination, and decision-curve\nanalysis showed small, favorable net benefit at common thresholds. This\nframework is readily extensible to multimodal inputs and external validation\nand is designed to complement, not replace, existing triage systems."}
{"id": "2510.15572", "categories": ["stat.AP", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.15572", "abs": "https://arxiv.org/abs/2510.15572", "authors": ["Kamel Lahssini", "Guerric le Maire", "Nicolas Baghdadi", "Ibrahim Fayad"], "title": "Residual Kriging for Regional-Scale Canopy Height Mapping: Insights into GEDI-Induced Anisotropies and Sparse Sampling", "comment": "22 pages, 12 figures", "summary": "Quantifying aboveground biomass (AGB) is essential in the context of global\nclimate change. Canopy height, which is related to AGB, can be mapped using\nmachine learning models trained with multi-source spatial data and GEDI\nmeasurements. In this study, a comparative analysis of canopy height estimates\nderived from two models is presented: a U-Net deep learning model (CHNET) and a\nRandom Forest algorithm (RFH). Both models were trained using GEDI lidar data\nand utilized multi-source inputs, including optical, radar, and environmental\ndata. While CHNET can leverage its convolutional architecture to account for\nspatial correlations, we observed that it does not fully incorporate all the\nspatial autocorrelation present in GEDI canopy height measurements. By\nconducting a spatial analysis of the models' residuals, we also identified that\nGEDI data acquisition parameters, particularly the variability in laser beam\nenergy combined with the azimuthal directions of the observation tracks,\nintroduce spatial inconsistencies in the measurements in the form of periodic\npatterns. To address these anisotropies, we considered exclusively GEDI power\nbeams, and we conducted our spatial autocorrelation analysis in the GEDI track\nazimuthal direction. Next, we employed the residual kriging (RK) spatial\ninterpolation technique to account for the spatial autocorrelation of canopy\nheights and improve the accuracies of CHNET and RFH estimates. Adding RK\ncorrections improved the performance of both CHNET and RFH, with more\nsubstantial gains observed for RFH. The corrections appeared to be localized\naround the GEDI sample points and the density of usable GEDI information is\ntherefore an important factor in the effectiveness of spatial interpolation.\nFurthermore, our findings reveal that a Random Forest model combined with\nspatial interpolation can deliver performance comparable to that of a U-Net\nmodel alone."}
{"id": "2510.15020", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15020", "abs": "https://arxiv.org/abs/2510.15020", "authors": ["Fan Chen", "Audrey Huang", "Noah Golowich", "Sadhika Malladi", "Adam Block", "Jordan T. Ash", "Akshay Krishnamurthy", "Dylan J. Foster"], "title": "The Coverage Principle: How Pre-training Enables Post-Training", "comment": null, "summary": "Language models demonstrate remarkable abilities when pre-trained on large\ntext corpora and fine-tuned for specific tasks, but how and why pre-training\nshapes the success of the final model remains poorly understood. Notably,\nalthough pre-training success is often quantified by cross entropy loss,\ncross-entropy can be a poor predictor of downstream performance. Instead, we\nprovide a theoretical perspective on this relationship through the lens of\n\\emph{coverage}, which quantifies the probability mass the pre-trained model\nplaces on high-quality responses and which is necessary and sufficient for\npost-training and test-time scaling methods such as Best-of-N to succeed. Our\nmain results develop an understanding of \\emph{the coverage principle}, a\nphenomenon whereby next-token prediction implicitly optimizes toward a model\nwith good coverage. In particular, we uncover a mechanism that explains the\npower of coverage in predicting downstream performance: \\emph{coverage\ngeneralizes faster than cross entropy}, avoiding spurious dependence on\nproblem-dependent parameters such as the sequence length. We also study\npractical algorithmic interventions with provable benefits for improving\ncoverage, including (i) model/checkpoint selection procedures, (ii) gradient\nnormalization schemes, and (iii) test-time decoding strategies."}
{"id": "2510.15337", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15337", "abs": "https://arxiv.org/abs/2510.15337", "authors": ["Yeichan Kim", "Ilmun Kim", "Seyoung Park"], "title": "Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression", "comment": "42 pages, 4 figures, 2 tables, 1 algorithm; camera-ready version\n  accepted at NeurIPS 2025 (Spotlight)", "summary": "Transfer learning is a key component of modern machine learning, enhancing\nthe performance of target tasks by leveraging diverse data sources.\nSimultaneously, overparameterized models such as the minimum-$\\ell_2$-norm\ninterpolator (MNI) in high-dimensional linear regression have garnered\nsignificant attention for their remarkable generalization capabilities, a\nproperty known as benign overfitting. Despite their individual importance, the\nintersection of transfer learning and MNI remains largely unexplored. Our\nresearch bridges this gap by proposing a novel two-step Transfer MNI approach\nand analyzing its trade-offs. We characterize its non-asymptotic excess risk\nand identify conditions under which it outperforms the target-only MNI. Our\nanalysis reveals free-lunch covariate shift regimes, where leveraging\nheterogeneous data yields the benefit of knowledge transfer at limited cost. To\noperationalize our findings, we develop a data-driven procedure to detect\ninformative sources and introduce an ensemble method incorporating multiple\ninformative Transfer MNIs. Finite-sample experiments demonstrate the robustness\nof our methods to model and data heterogeneity, confirming their advantage."}
{"id": "2510.15381", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15381", "abs": "https://arxiv.org/abs/2510.15381", "authors": ["Christian H. Weiß", "Philipp Adämmer"], "title": "Nonparametric Testing of Spatial Dependence in 2D and 3D Random Fields", "comment": null, "summary": "We propose a flexible and robust nonparametric framework for testing spatial\ndependence in two- and three-dimensional random fields. Our approach involves\nconverting spatial data into one-dimensional time series using space-filling\nHilbert curves. We then apply ordinal pattern-based tests for serial dependence\nto this series. Because Hilbert curves preserve spatial locality, spatial\ndependence in the original field manifests as serial dependence in the\ntransformed sequence. The approach is easy to implement, accommodates arbitrary\ngrid sizes through generalized Hilbert (``gilbert'') curves, and naturally\nextends beyond three dimensions. This provides a practical and general\nalternative to existing methods based on spatial ordinal patterns, which are\ntypically limited to two-dimensional settings."}
{"id": "2510.15580", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15580", "abs": "https://arxiv.org/abs/2510.15580", "authors": ["Kyle Stanley", "Nicole Lazar", "Matthew Reimherr"], "title": "Temporal Functional Factor Analysis of Brain Connectivity", "comment": null, "summary": "Many analyses of functional magnetic resonance imaging (fMRI) examine\nfunctional connectivity (FC), or the statistical dependencies among distant\nbrain regions. These analyses are typically exploratory, guiding future\nconfirmatory research. In this work, we present an approach based on factor\nanalysis (FA) that is well-suited to studying FC. FA is appealing in this\ncontext because its flexible model assumptions permit a guided investigation of\nits target subspace consistent with the exploratory role of connectivity\nanalyses. However, applying FA to fMRI data poses three problems: (1) its\ntarget subspace captures short-range spatial dependencies that should be\ntreated as noise, (2) it requires factorization of a massive spatial\ncovariance, and (3) it overlooks temporal dependencies in the data. To address\nthese limitations, we develop a factor model within the framework of functional\ndata analysis--a field which views certain data as arising from smooth\nunderlying curves. The proposed approach (1) uses matrix completion techniques\nto filter short-range spatial dependencies out of its target subspace, (2)\nemploys a distributed algorithm for factorizing large-scale covariance\nmatrices, and (3) leverages functional regression to exploit temporal dynamics.\nTogether, these innovations yield a comprehensive and scalable method for\nstudying FC."}
{"id": "2510.15058", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62C20 (Primary) 46E22, 62B10 (Secondary)", "G.3; H.1.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.15058", "abs": "https://arxiv.org/abs/2510.15058", "authors": ["Jose Cribeiro-Ramallo", "Agnideep Aich", "Florian Kalinke", "Ashit Baran Aich", "Zoltán Szabó"], "title": "The Minimax Lower Bound of Kernel Stein Discrepancy Estimation", "comment": null, "summary": "Kernel Stein discrepancies (KSDs) have emerged as a powerful tool for\nquantifying goodness-of-fit over the last decade, featuring numerous successful\napplications. To the best of our knowledge, all existing KSD estimators with\nknown rate achieve $\\sqrt n$-convergence. In this work, we present two\ncomplementary results (with different proof strategies), establishing that the\nminimax lower bound of KSD estimation is $n^{-1/2}$ and settling the optimality\nof these estimators. Our first result focuses on KSD estimation on $\\mathbb\nR^d$ with the Langevin-Stein operator; our explicit constant for the Gaussian\nkernel indicates that the difficulty of KSD estimation may increase\nexponentially with the dimensionality $d$. Our second result settles the\nminimax lower bound for KSD estimation on general domains."}
{"id": "2510.15632", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15632", "abs": "https://arxiv.org/abs/2510.15632", "authors": ["Max Welz"], "title": "Robust Estimation of Polyserial Correlation", "comment": "64 pages (30 main text), 16 figures and 5 tables in total", "summary": "The association between a continuous and an ordinal variable is commonly\nmodeled through the polyserial correlation model. However, this model, which is\nbased on a partially-latent normality assumption, may be misspecified in\npractice, due to, for example (but not limited to), outliers or careless\nresponses. We demonstrate that the typically used maximum likelihood (ML)\nestimator is highly susceptible to such misspecification: One single\nobservation not generated by partially-latent normality can suffice to produce\narbitrarily poor estimates. As a remedy, we propose a novel estimator of the\npolyserial correlation model designed to be robust against the adverse effects\nof observations discrepant to that model. The estimator achieves robustness by\nimplicitly downweighting such observations; the ensuing weights constitute a\nuseful tool for pinpointing potential sources of model misspecification. We\nshow that the proposed estimator generalizes ML and is consistent as well as\nasymptotically Gaussian. As price for robustness, some efficiency must be\nsacrificed, but substantial robustness can be gained while maintaining more\nthan 98% of ML efficiency. We demonstrate our estimator's robustness and\npractical usefulness in simulation experiments and an empirical application in\npersonality psychology where our estimator helps identify outliers. Finally,\nthe proposed methodology is implemented in free open-source software."}
{"id": "2510.15618", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.15618", "abs": "https://arxiv.org/abs/2510.15618", "authors": ["Abdul-Nasah Soale", "Adewale Lukman"], "title": "Adaptive Influence Diagnostics in High-Dimensional Regression", "comment": null, "summary": "An adaptive Cook's distance (ACD) for diagnosing influential observations in\nhigh-dimensional single-index models with multicollinearity and outlier\ncontamination is proposed. ACD is a model-free technique built on sparse local\nlinear gradients to temper leverage effects. In simulations spanning low- and\nhigh-dimensional design settings with strong correlation, ACD based on LASSO\n(ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative\nto classical Cook's distance and local influence as well as the DF-Model and\nCase-Weight adjusted solution for LASSO. Trimming points flagged by ACD\nstabilizes variable selection while preserving core signals. Applications to\ntwo datasets--the 1960 US cities pollution study and a high-dimensional\nriboflavin genomics experiment show consistent gains in selection stability and\ninterpretability."}
{"id": "2510.15667", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15667", "abs": "https://arxiv.org/abs/2510.15667", "authors": ["Davi Oliveira Chaves", "Chang Chiann", "Pedro Alberto Morettin"], "title": "A nonstationary seasonal Dynamic Factor Model: an application to temperature time series from the state of Minas Gerais", "comment": "Paper presented on the XVII MGEST (Lavras, Brazil - 2025)", "summary": "In many scientific fields, such as agriculture, temperature time series are\nof interest both as explanatory variables and as objects of study in their own\nright. However, at the state level, incorporating information from all possible\nlocations in an analysis can be overwhelming, while using a summary measure,\nsuch as the state-wide average temperature, can result in significant\ninformation loss. In this context, using Dynamic Factor Models (DFMs) provides\na compelling alternative for analyzing such multivariate time series, as they\nallow for the extraction of a small number of common factors that capture the\nmajority of the variability in the data. Given that temperature series are\ntypically seasonal, this study applies a nonstationary seasonal DFM to analyze\na multivariate temperature time series from the state of Minas Gerais. The\nresults show that the data can be effectively represented by two seasonal\nfactors: the first captures the general seasonal pattern of the state, while\nthe second contrasts the months of highest annual temperatures between two\ndistinct regions."}
{"id": "2510.15141", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15141", "abs": "https://arxiv.org/abs/2510.15141", "authors": ["Zelong Bi", "Pierre Lafaye de Micheaux"], "title": "Beyond PCA: Manifold Dimension Estimation via Local Graph Structure", "comment": null, "summary": "Local principal component analysis (Local PCA) has proven to be an effective\ntool for estimating the intrinsic dimension of a manifold. More recently,\ncurvature-adjusted PCA (CA-PCA) has improved upon this approach by explicitly\naccounting for the curvature of the underlying manifold, rather than assuming\nlocal flatness. Building on these insights, we propose a general framework for\nmanifold dimension estimation that captures the manifold's local graph\nstructure by integrating PCA with regression-based techniques. Within this\nframework, we introduce two representative estimators: quadratic embedding (QE)\nand total least squares (TLS). Experiments on both synthetic and real-world\ndatasets demonstrate that these methods perform competitively with, and often\noutperform, state-of-the-art alternatives."}
{"id": "2510.15632", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15632", "abs": "https://arxiv.org/abs/2510.15632", "authors": ["Max Welz"], "title": "Robust Estimation of Polyserial Correlation", "comment": "64 pages (30 main text), 16 figures and 5 tables in total", "summary": "The association between a continuous and an ordinal variable is commonly\nmodeled through the polyserial correlation model. However, this model, which is\nbased on a partially-latent normality assumption, may be misspecified in\npractice, due to, for example (but not limited to), outliers or careless\nresponses. We demonstrate that the typically used maximum likelihood (ML)\nestimator is highly susceptible to such misspecification: One single\nobservation not generated by partially-latent normality can suffice to produce\narbitrarily poor estimates. As a remedy, we propose a novel estimator of the\npolyserial correlation model designed to be robust against the adverse effects\nof observations discrepant to that model. The estimator achieves robustness by\nimplicitly downweighting such observations; the ensuing weights constitute a\nuseful tool for pinpointing potential sources of model misspecification. We\nshow that the proposed estimator generalizes ML and is consistent as well as\nasymptotically Gaussian. As price for robustness, some efficiency must be\nsacrificed, but substantial robustness can be gained while maintaining more\nthan 98% of ML efficiency. We demonstrate our estimator's robustness and\npractical usefulness in simulation experiments and an empirical application in\npersonality psychology where our estimator helps identify outliers. Finally,\nthe proposed methodology is implemented in free open-source software."}
{"id": "2510.15762", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15762", "abs": "https://arxiv.org/abs/2510.15762", "authors": ["Antonio Remiro-Azócar", "Pepa Polavieja", "Emmanuelle Boutmy", "Alessandro Ghiretti", "Lise Lotte Nystrup Husemoen", "Khadija Rerhou Rantell", "Tatsiana Vaitsiakhovich", "David M. Phillippo", "Jay J. H. Park", "Helle Lynggaard", "Robert Bauer", "Antonia Morga"], "title": "Incorporating estimands into meta-analyses of clinical trials", "comment": "27 pages, 6 figures, 6 tables. Submitted to Research Synthesis\n  Methods", "summary": "The estimand framework is increasingly established to pose research questions\nin confirmatory clinical trials. In evidence synthesis, the uptake of estimands\nhas been modest, and the PICO (Population, Intervention, Comparator, Outcome)\nframework is more often applied. While PICOs and estimands have overlapping\nelements, the estimand framework explicitly considers different strategies for\nintercurrent events. We propose a pragmatic framework for the use of estimands\nin meta-analyses of clinical trials, highlighting the value of estimands to\nsystematically identify and mitigate key sources of quantitative heterogeneity,\nand to enhance the applicability or external validity of pooled estimates.\nFocus is placed on the role of strategies for intercurrent events, within the\nspecific context of meta-analyses for health technology assessment. We apply\nthe estimand framework to a network meta-analysis of clinical trials, comparing\nthe efficacy of semaglutide versus dulaglutide in type 2 diabetes. We explore\nthe impact of a treatment policy strategy for treatment discontinuation or\ninitiation of rescue medication versus a hypothetical strategy for the\ncorresponding intercurrent events. The specification of different target\nestimands at the meta-analytical level allows us to be explicit about the\nsource of heterogeneity, the intercurrent event strategy, driving any potential\ndifferences in results. We advocate for the integration of estimands into the\nplanning of meta-analyses, while acknowledging that potential challenges exist\nin the absence of subject-level data. Estimands can complement PICOs to\nstrengthen communication between stakeholders about what evidence syntheses\nseek to demonstrate, and to ensure that the generated evidence is maximally\nrelevant to healthcare decision-makers."}
{"id": "2510.15273", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15273", "abs": "https://arxiv.org/abs/2510.15273", "authors": ["Liner Xiang", "Jiayi Wang", "Hengrui Cai"], "title": "Foresighted Online Policy Optimization with Interference", "comment": null, "summary": "Contextual bandits, which leverage the baseline features of sequentially\narriving individuals to optimize cumulative rewards while balancing exploration\nand exploitation, are critical for online decision-making. Existing approaches\ntypically assume no interference, where each individual's action affects only\ntheir own reward. Yet, such an assumption can be violated in many practical\nscenarios, and the oversight of interference can lead to short-sighted policies\nthat focus solely on maximizing the immediate outcomes for individuals, which\nfurther results in suboptimal decisions and potentially increased regret over\ntime. To address this significant gap, we introduce the foresighted online\npolicy with interference (FRONT) that innovatively considers the long-term\nimpact of the current decision on subsequent decisions and rewards. The\nproposed FRONT method employs a sequence of exploratory and exploitative\nstrategies to manage the intricacies of interference, ensuring robust parameter\ninference and regret minimization. Theoretically, we establish a tail bound for\nthe online estimator and derive the asymptotic distribution of the parameters\nof interest under suitable conditions on the interference network. We further\nshow that FRONT attains sublinear regret under two distinct definitions,\ncapturing both the immediate and consequential impacts of decisions, and we\nestablish these results with and without statistical inference. The\neffectiveness of FRONT is further demonstrated through extensive simulations\nand a real-world application to urban hotel profits."}
{"id": "2510.15664", "categories": ["stat.ME", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.15664", "abs": "https://arxiv.org/abs/2510.15664", "authors": ["Lucas Amoudruz", "Sergey Litvinov", "Costas Papadimitriou", "Petros Koumoutsakos"], "title": "Bayesian Inference for PDE-based Inverse Problems using the Optimization of a Discrete Loss", "comment": null, "summary": "Inverse problems are crucial for many applications in science, engineering\nand medicine that involve data assimilation, design, and imaging. Their\nsolution infers the parameters or latent states of a complex system from noisy\ndata and partially observable processes. When measurements are an incomplete or\nindirect view of the system, additional knowledge is required to accurately\nsolve the inverse problem. Adopting a physical model of the system in the form\nof partial differential equations (PDEs) is a potent method to close this gap.\nIn particular, the method of optimizing a discrete loss (ODIL) has shown great\npotential in terms of robustness and computational cost. In this work, we\nintroduce B-ODIL, a Bayesian extension of ODIL, that integrates the PDE loss of\nODIL as prior knowledge and combines it with a likelihood describing the data.\nB-ODIL employs a Bayesian formulation of PDE-based inverse problems to infer\nsolutions with quantified uncertainties. We demonstrate the capabilities of\nB-ODIL in a series of synthetic benchmarks involving PDEs in one, two, and\nthree dimensions. We showcase the application of B-ODIL in estimating tumor\nconcentration and its uncertainty in a patient's brain from MRI scans using a\nthree-dimensional tumor growth model."}
{"id": "2510.15780", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15780", "abs": "https://arxiv.org/abs/2510.15780", "authors": ["Alireza Moradi", "Mathieu Tanneau", "Reza Zandehshahvar", "Pascal Van Hentenryck"], "title": "Enhanced Renewable Energy Forecasting using Context-Aware Conformal Prediction", "comment": null, "summary": "Accurate forecasting is critical for reliable power grid operations,\nparticularly as the share of renewable generation, such as wind and solar,\ncontinues to grow. Given the inherent uncertainty and variability in renewable\ngeneration, probabilistic forecasts have become essential for informed\noperational decisions. However, such forecasts frequently suffer from\ncalibration issues, potentially degrading decision-making performance. Building\non recent advances in Conformal Predictions, this paper introduces a tailored\ncalibration framework that constructs context-aware calibration sets using a\nnovel weighting scheme. The proposed framework improves the quality of\nprobabilistic forecasts at the site and fleet levels, as demonstrated by\nnumerical experiments on large-scale datasets covering several systems in the\nUnited States. The results demonstrate that the proposed approach achieves\nhigher forecast reliability and robustness for renewable energy applications\ncompared to existing baselines."}
{"id": "2510.15337", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15337", "abs": "https://arxiv.org/abs/2510.15337", "authors": ["Yeichan Kim", "Ilmun Kim", "Seyoung Park"], "title": "Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression", "comment": "42 pages, 4 figures, 2 tables, 1 algorithm; camera-ready version\n  accepted at NeurIPS 2025 (Spotlight)", "summary": "Transfer learning is a key component of modern machine learning, enhancing\nthe performance of target tasks by leveraging diverse data sources.\nSimultaneously, overparameterized models such as the minimum-$\\ell_2$-norm\ninterpolator (MNI) in high-dimensional linear regression have garnered\nsignificant attention for their remarkable generalization capabilities, a\nproperty known as benign overfitting. Despite their individual importance, the\nintersection of transfer learning and MNI remains largely unexplored. Our\nresearch bridges this gap by proposing a novel two-step Transfer MNI approach\nand analyzing its trade-offs. We characterize its non-asymptotic excess risk\nand identify conditions under which it outperforms the target-only MNI. Our\nanalysis reveals free-lunch covariate shift regimes, where leveraging\nheterogeneous data yields the benefit of knowledge transfer at limited cost. To\noperationalize our findings, we develop a data-driven procedure to detect\ninformative sources and introduce an ensemble method incorporating multiple\ninformative Transfer MNIs. Finite-sample experiments demonstrate the robustness\nof our methods to model and data heterogeneity, confirming their advantage."}
{"id": "2510.15670", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15670", "abs": "https://arxiv.org/abs/2510.15670", "authors": ["Paolo Giudici", "Rosa C. Rosciano", "Johanna Schrader", "Delf-Magnus Kummerfeld"], "title": "A Multiclass ROC Curve", "comment": null, "summary": "This paper introduces a novel methodology for constructing multiclass ROC\ncurves using the multidimensional Gini index. The proposed methodology\nleverages the established relationship between the Gini coefficient and the ROC\nCurve and extends it to multiclass settings through the multidimensional Gini\nindex. The framework is validated by means of two comprehensive case studies in\nhealth care and finance. The paper provides a theoretically grounded solution\nto multiclass performance evaluation, particularly valuable for imbalanced\ndatasets, for which a prudential assessment should take precedence over class\nfrequency considerations."}
{"id": "2510.15141", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15141", "abs": "https://arxiv.org/abs/2510.15141", "authors": ["Zelong Bi", "Pierre Lafaye de Micheaux"], "title": "Beyond PCA: Manifold Dimension Estimation via Local Graph Structure", "comment": null, "summary": "Local principal component analysis (Local PCA) has proven to be an effective\ntool for estimating the intrinsic dimension of a manifold. More recently,\ncurvature-adjusted PCA (CA-PCA) has improved upon this approach by explicitly\naccounting for the curvature of the underlying manifold, rather than assuming\nlocal flatness. Building on these insights, we propose a general framework for\nmanifold dimension estimation that captures the manifold's local graph\nstructure by integrating PCA with regression-based techniques. Within this\nframework, we introduce two representative estimators: quadratic embedding (QE)\nand total least squares (TLS). Experiments on both synthetic and real-world\ndatasets demonstrate that these methods perform competitively with, and often\noutperform, state-of-the-art alternatives."}
{"id": "2510.15362", "categories": ["stat.ML", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15362", "abs": "https://arxiv.org/abs/2510.15362", "authors": ["Zixun Wang", "Ben Dai"], "title": "RankSEG-RMA: An Efficient Segmentation Algorithm via Reciprocal Moment Approximation", "comment": null, "summary": "Semantic segmentation labels each pixel in an image with its corresponding\nclass, and is typically evaluated using the Intersection over Union (IoU) and\nDice metrics to quantify the overlap between predicted and ground-truth\nsegmentation masks. In the literature, most existing methods estimate\npixel-wise class probabilities, then apply argmax or thresholding to obtain the\nfinal prediction. These methods have been shown to generally lead to\ninconsistent or suboptimal results, as they do not directly maximize\nsegmentation metrics. To address this issue, a novel consistent segmentation\nframework, RankSEG, has been proposed, which includes RankDice and RankIoU\nspecifically designed to optimize the Dice and IoU metrics, respectively.\nAlthough RankSEG almost guarantees improved performance, it suffers from two\nmajor drawbacks. First, it is its computational expense-RankDice has a\ncomplexity of O(d log d) with a substantial constant factor (where d represents\nthe number of pixels), while RankIoU exhibits even higher complexity O(d^2),\nthus limiting its practical application. For instance, in LiTS, prediction with\nRankSEG takes 16.33 seconds compared to just 0.01 seconds with the argmax rule.\nSecond, RankSEG is only applicable to overlapping segmentation settings, where\nmultiple classes can occupy the same pixel, which contrasts with standard\nbenchmarks that typically assume non-overlapping segmentation. In this paper,\nwe overcome these two drawbacks via a reciprocal moment approximation (RMA) of\nRankSEG with the following contributions: (i) we improve RankSEG using RMA,\nnamely RankSEG-RMA, reduces the complexity of both algorithms to O(d) while\nmaintaining comparable performance; (ii) inspired by RMA, we develop a\npixel-wise score function that allows efficient implementation for\nnon-overlapping segmentation settings."}
{"id": "2510.15013", "categories": ["stat.ML", "cs.LG", "physics.data-an", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15013", "abs": "https://arxiv.org/abs/2510.15013", "authors": ["Magnus Neuman", "Jelena Smiljanić", "Martin Rosvall"], "title": "Reliable data clustering with Bayesian community detection", "comment": null, "summary": "From neuroscience and genomics to systems biology and ecology, researchers\nrely on clustering similarity data to uncover modular structure. Yet widely\nused clustering methods, such as hierarchical clustering, k-means, and WGCNA,\nlack principled model selection, leaving them susceptible to noise. A common\nworkaround sparsifies a correlation matrix representation to remove noise\nbefore clustering, but this extra step introduces arbitrary thresholds that can\ndistort the structure and lead to unreliable results. To detect reliable\nclusters, we capitalize on recent advances in network science to unite\nsparsification and clustering with principled model selection. We test two\nBayesian community detection methods, the Degree-Corrected Stochastic Block\nModel and the Regularized Map Equation, both grounded in the Minimum\nDescription Length principle for model selection. In synthetic data, they\noutperform traditional approaches, detecting planted clusters under high-noise\nconditions and with fewer samples. Compared to WGCNA on gene co-expression\ndata, the Regularized Map Equation identifies more robust and functionally\ncoherent gene modules. Our results establish Bayesian community detection as a\nprincipled and noise-resistant framework for uncovering modular structure in\nhigh-dimensional data across fields."}
{"id": "2510.15618", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.15618", "abs": "https://arxiv.org/abs/2510.15618", "authors": ["Abdul-Nasah Soale", "Adewale Lukman"], "title": "Adaptive Influence Diagnostics in High-Dimensional Regression", "comment": null, "summary": "An adaptive Cook's distance (ACD) for diagnosing influential observations in\nhigh-dimensional single-index models with multicollinearity and outlier\ncontamination is proposed. ACD is a model-free technique built on sparse local\nlinear gradients to temper leverage effects. In simulations spanning low- and\nhigh-dimensional design settings with strong correlation, ACD based on LASSO\n(ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative\nto classical Cook's distance and local influence as well as the DF-Model and\nCase-Weight adjusted solution for LASSO. Trimming points flagged by ACD\nstabilizes variable selection while preserving core signals. Applications to\ntwo datasets--the 1960 US cities pollution study and a high-dimensional\nriboflavin genomics experiment show consistent gains in selection stability and\ninterpretability."}
{"id": "2510.15363", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15363", "abs": "https://arxiv.org/abs/2510.15363", "authors": ["Dechen Zhang", "Zhenmei Shi", "Yi Zhang", "Yingyu Liang", "Difan Zou"], "title": "Kernel Regression in Structured Non-IID Settings: Theory and Implications for Denoising Score Learning", "comment": null, "summary": "Kernel ridge regression (KRR) is a foundational tool in machine learning,\nwith recent work emphasizing its connections to neural networks. However,\nexisting theory primarily addresses the i.i.d. setting, while real-world data\noften exhibits structured dependencies - particularly in applications like\ndenoising score learning where multiple noisy observations derive from shared\nunderlying signals. We present the first systematic study of KRR generalization\nfor non-i.i.d. data with signal-noise causal structure, where observations\nrepresent different noisy views of common signals. By developing a novel\nblockwise decomposition method that enables precise concentration analysis for\ndependent data, we derive excess risk bounds for KRR that explicitly depend on:\n(1) the kernel spectrum, (2) causal structure parameters, and (3) sampling\nmechanisms (including relative sample sizes for signals and noises). We further\napply our results to denoising score learning, establishing generalization\nguarantees and providing principled guidance for sampling noisy data points.\nThis work advances KRR theory while providing practical tools for analyzing\ndependent data in modern machine learning applications."}
{"id": "2510.15273", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15273", "abs": "https://arxiv.org/abs/2510.15273", "authors": ["Liner Xiang", "Jiayi Wang", "Hengrui Cai"], "title": "Foresighted Online Policy Optimization with Interference", "comment": null, "summary": "Contextual bandits, which leverage the baseline features of sequentially\narriving individuals to optimize cumulative rewards while balancing exploration\nand exploitation, are critical for online decision-making. Existing approaches\ntypically assume no interference, where each individual's action affects only\ntheir own reward. Yet, such an assumption can be violated in many practical\nscenarios, and the oversight of interference can lead to short-sighted policies\nthat focus solely on maximizing the immediate outcomes for individuals, which\nfurther results in suboptimal decisions and potentially increased regret over\ntime. To address this significant gap, we introduce the foresighted online\npolicy with interference (FRONT) that innovatively considers the long-term\nimpact of the current decision on subsequent decisions and rewards. The\nproposed FRONT method employs a sequence of exploratory and exploitative\nstrategies to manage the intricacies of interference, ensuring robust parameter\ninference and regret minimization. Theoretically, we establish a tail bound for\nthe online estimator and derive the asymptotic distribution of the parameters\nof interest under suitable conditions on the interference network. We further\nshow that FRONT attains sublinear regret under two distinct definitions,\ncapturing both the immediate and consequential impacts of decisions, and we\nestablish these results with and without statistical inference. The\neffectiveness of FRONT is further demonstrated through extensive simulations\nand a real-world application to urban hotel profits."}
{"id": "2510.15632", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15632", "abs": "https://arxiv.org/abs/2510.15632", "authors": ["Max Welz"], "title": "Robust Estimation of Polyserial Correlation", "comment": "64 pages (30 main text), 16 figures and 5 tables in total", "summary": "The association between a continuous and an ordinal variable is commonly\nmodeled through the polyserial correlation model. However, this model, which is\nbased on a partially-latent normality assumption, may be misspecified in\npractice, due to, for example (but not limited to), outliers or careless\nresponses. We demonstrate that the typically used maximum likelihood (ML)\nestimator is highly susceptible to such misspecification: One single\nobservation not generated by partially-latent normality can suffice to produce\narbitrarily poor estimates. As a remedy, we propose a novel estimator of the\npolyserial correlation model designed to be robust against the adverse effects\nof observations discrepant to that model. The estimator achieves robustness by\nimplicitly downweighting such observations; the ensuing weights constitute a\nuseful tool for pinpointing potential sources of model misspecification. We\nshow that the proposed estimator generalizes ML and is consistent as well as\nasymptotically Gaussian. As price for robustness, some efficiency must be\nsacrificed, but substantial robustness can be gained while maintaining more\nthan 98% of ML efficiency. We demonstrate our estimator's robustness and\npractical usefulness in simulation experiments and an empirical application in\npersonality psychology where our estimator helps identify outliers. Finally,\nthe proposed methodology is implemented in free open-source software."}
{"id": "2510.15390", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15390", "abs": "https://arxiv.org/abs/2510.15390", "authors": ["Tengjie Zheng", "Jilan Mei", "Di Wu", "Lin Cheng", "Shengping Gong"], "title": "Recursive Inference for Heterogeneous Multi-Output GP State-Space Models with Arbitrary Moment Matching", "comment": null, "summary": "Accurate learning of system dynamics is becoming increasingly crucial for\nadvanced control and decision-making in engineering. However, real-world\nsystems often exhibit multiple channels and highly nonlinear transition\ndynamics, challenging traditional modeling methods. To enable online learning\nfor these systems, this paper formulates the system as Gaussian process\nstate-space models (GPSSMs) and develops a recursive learning method. The main\ncontributions are threefold. First, a heterogeneous multi-output kernel is\ndesigned, allowing each output dimension to adopt distinct kernel types,\nhyperparameters, and input variables, improving expressiveness in\nmulti-dimensional dynamics learning. Second, an inducing-point management\nalgorithm enhances computational efficiency through independent selection and\npruning for each output dimension. Third, a unified recursive inference\nframework for GPSSMs is derived, supporting general moment matching approaches,\nincluding the extended Kalman filter (EKF), unscented Kalman filter (UKF), and\nassumed density filtering (ADF), enabling accurate learning under strong\nnonlinearity and significant noise. Experiments on synthetic and real-world\ndatasets show that the proposed method matches the accuracy of SOTA offline\nGPSSMs with only 1/100 of the runtime, and surpasses SOTA online GPSSMs by\naround 70% in accuracy under heavy noise while using only 1/20 of the runtime."}
{"id": "2510.15580", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15580", "abs": "https://arxiv.org/abs/2510.15580", "authors": ["Kyle Stanley", "Nicole Lazar", "Matthew Reimherr"], "title": "Temporal Functional Factor Analysis of Brain Connectivity", "comment": null, "summary": "Many analyses of functional magnetic resonance imaging (fMRI) examine\nfunctional connectivity (FC), or the statistical dependencies among distant\nbrain regions. These analyses are typically exploratory, guiding future\nconfirmatory research. In this work, we present an approach based on factor\nanalysis (FA) that is well-suited to studying FC. FA is appealing in this\ncontext because its flexible model assumptions permit a guided investigation of\nits target subspace consistent with the exploratory role of connectivity\nanalyses. However, applying FA to fMRI data poses three problems: (1) its\ntarget subspace captures short-range spatial dependencies that should be\ntreated as noise, (2) it requires factorization of a massive spatial\ncovariance, and (3) it overlooks temporal dependencies in the data. To address\nthese limitations, we develop a factor model within the framework of functional\ndata analysis--a field which views certain data as arising from smooth\nunderlying curves. The proposed approach (1) uses matrix completion techniques\nto filter short-range spatial dependencies out of its target subspace, (2)\nemploys a distributed algorithm for factorizing large-scale covariance\nmatrices, and (3) leverages functional regression to exploit temporal dynamics.\nTogether, these innovations yield a comprehensive and scalable method for\nstudying FC."}
{"id": "2510.15422", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15422", "abs": "https://arxiv.org/abs/2510.15422", "authors": ["Lin Wang"], "title": "Information Theory in Open-world Machine Learning Foundations, Frameworks, and Future Direction", "comment": null, "summary": "Open world Machine Learning (OWML) aims to develop intelligent systems\ncapable of recognizing known categories, rejecting unknown samples, and\ncontinually learning from novel information. Despite significant progress in\nopen set recognition, novelty detection, and continual learning, the field\nstill lacks a unified theoretical foundation that can quantify uncertainty,\ncharacterize information transfer, and explain learning adaptability in\ndynamic, nonstationary environments. This paper presents a comprehensive review\nof information theoretic approaches in open world machine learning, emphasizing\nhow core concepts such as entropy, mutual information, and Kullback Leibler\ndivergence provide a mathematical language for describing knowledge\nacquisition, uncertainty suppression, and risk control under open world\nconditions. We synthesize recent studies into three major research axes:\ninformation theoretic open set recognition enabling safe rejection of unknowns,\ninformation driven novelty discovery guiding new concept formation, and\ninformation retentive continual learning ensuring stable long term adaptation.\nFurthermore, we discuss theoretical connections between information theory and\nprovable learning frameworks, including PAC Bayes bounds, open-space risk\ntheory, and causal information flow, to establish a pathway toward provable and\ntrustworthy open world intelligence. Finally, the review identifies key open\nproblems and future research directions, such as the quantification of\ninformation risk, development of dynamic mutual information bounds, multimodal\ninformation fusion, and integration of information theory with causal reasoning\nand world model learning."}
{"id": "2510.15458", "categories": ["stat.ML", "cs.AI", "cs.LG", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2510.15458", "abs": "https://arxiv.org/abs/2510.15458", "authors": ["Gabriele Visentin", "Patrick Cheridito"], "title": "Robust Optimization in Causal Models and G-Causal Normalizing Flows", "comment": null, "summary": "In this paper, we show that interventionally robust optimization problems in\ncausal models are continuous under the $G$-causal Wasserstein distance, but may\nbe discontinuous under the standard Wasserstein distance. This highlights the\nimportance of using generative models that respect the causal structure when\naugmenting data for such tasks. To this end, we propose a new normalizing flow\narchitecture that satisfies a universal approximation property for causal\nstructural models and can be efficiently trained to minimize the $G$-causal\nWasserstein distance. Empirically, we demonstrate that our model outperforms\nstandard (non-causal) generative models in data augmentation for causal\nregression and mean-variance portfolio optimization in causal factor models."}
{"id": "2510.15483", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15483", "abs": "https://arxiv.org/abs/2510.15483", "authors": ["Samuel Girard", "Aurélien Bibaut", "Houssam Zenati"], "title": "Online Policy Learning via a Self-Normalized Maximal Inequality", "comment": null, "summary": "Adaptive experiments produce dependent data that break i.i.d. assumptions\nthat underlie classical concentration bounds and invalidate standard learning\nguarantees. In this paper, we develop a self-normalized maximal inequality for\nmartingale empirical processes. Building on this, we first propose an adaptive\nsample-variance penalization procedure which balances empirical loss and sample\nvariance, valid for general dependent data. Next, this allows us to derive a\nnew variance-regularized pessimistic off-policy learning objective, for which\nwe establish excess-risk guarantees. Subsequently, we show that, when combined\nwith sequential updates and under standard complexity and margin conditions,\nthe resulting estimator achieves fast convergence rates in both parametric and\nnonparametric regimes, improving over the usual $1/\\sqrt{n}$\n  baseline. We complement our theoretical findings with numerical simulations\nthat illustrate the practical gains of our approach."}
{"id": "2510.15548", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15548", "abs": "https://arxiv.org/abs/2510.15548", "authors": ["Sushil Bohara", "Amedeo Roberto Esposito"], "title": "Geometric Convergence Analysis of Variational Inference via Bregman Divergences", "comment": "14 pages, 4 figures", "summary": "Variational Inference (VI) provides a scalable framework for Bayesian\ninference by optimizing the Evidence Lower Bound (ELBO), but convergence\nanalysis remains challenging due to the objective's non-convexity and\nnon-smoothness in Euclidean space. We establish a novel theoretical framework\nfor analyzing VI convergence by exploiting the exponential family structure of\ndistributions. We express negative ELBO as a Bregman divergence with respect to\nthe log-partition function, enabling a geometric analysis of the optimization\nlandscape. We show that this Bregman representation admits a weak monotonicity\nproperty that, while weaker than convexity, provides sufficient structure for\nrigorous convergence analysis. By deriving bounds on the objective function\nalong rays in parameter space, we establish properties governed by the spectral\ncharacteristics of the Fisher information matrix. Under this geometric\nframework, we prove non-asymptotic convergence rates for gradient descent\nalgorithms with both constant and diminishing step sizes."}
{"id": "2510.15601", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15601", "abs": "https://arxiv.org/abs/2510.15601", "authors": ["Pierre Glaser", "Steffanie Paul", "Alissa M. Hummer", "Charlotte M. Deane", "Debora S. Marks", "Alan N. Amin"], "title": "Kernel-Based Evaluation of Conditional Biological Sequence Models", "comment": "29 pages", "summary": "We propose a set of kernel-based tools to evaluate the designs and tune the\nhyperparameters of conditional sequence models, with a focus on problems in\ncomputational biology. The backbone of our tools is a new measure of\ndiscrepancy between the true conditional distribution and the model's estimate,\ncalled the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided\nthat the model can be sampled from, the ACMMD can be estimated unbiasedly from\ndata to quantify absolute model fit, integrated within hypothesis tests, and\nused to evaluate model reliability. We demonstrate the utility of our approach\nby analyzing a popular protein design model, ProteinMPNN. We are able to reject\nthe hypothesis that ProteinMPNN fits its data for various protein families, and\ntune the model's temperature hyperparameter to achieve a better fit."}
{"id": "2510.15669", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15669", "abs": "https://arxiv.org/abs/2510.15669", "authors": ["Veranika Boukun", "Jörg Lücke"], "title": "Disentanglement of Sources in a Multi-Stream Variational Autoencoder", "comment": null, "summary": "Variational autoencoders (VAEs) are a leading approach to address the problem\nof learning disentangled representations. Typically a single VAE is used and\ndisentangled representations are sought in its continuous latent space. Here we\nexplore a different approach by using discrete latents to combine\nVAE-representations of individual sources. The combination is done based on an\nexplicit model for source combination, and we here use a linear combination\nmodel which is well suited, e.g., for acoustic data. We formally define such a\nmulti-stream VAE (MS-VAE) approach, derive its inference and learning\nequations, and we numerically investigate its principled functionality. The\nMS-VAE is domain-agnostic, and we here explore its ability to separate sources\ninto different streams using superimposed hand-written digits, and mixed\nacoustic sources in a speaker diarization task. We observe a clear separation\nof digits, and on speaker diarization we observe an especially low rate of\nmissed speakers. Numerical experiments further highlight the flexibility of the\napproach across varying amounts of supervision and training data."}
{"id": "2510.15814", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15814", "abs": "https://arxiv.org/abs/2510.15814", "authors": ["Marco Pacini", "Mircea Petrache", "Bruno Lepri", "Shubhendu Trivedi", "Robin Walters"], "title": "On Universality of Deep Equivariant Networks", "comment": "Preprint. 22 pages", "summary": "Universality results for equivariant neural networks remain rare. Those that\ndo exist typically hold only in restrictive settings: either they rely on\nregular or higher-order tensor representations, leading to impractically\nhigh-dimensional hidden spaces, or they target specialized architectures, often\nconfined to the invariant setting. This work develops a more general account.\nFor invariant networks, we establish a universality theorem under separation\nconstraints, showing that the addition of a fully connected readout layer\nsecures approximation within the class of separation-constrained continuous\nfunctions. For equivariant networks, where results are even scarcer, we\ndemonstrate that standard separability notions are inadequate and introduce the\nsharper criterion of $\\textit{entry-wise separability}$. We show that with\nsufficient depth or with the addition of appropriate readout layers,\nequivariant networks attain universality within the entry-wise separable\nregime. Together with prior results showing the failure of universality for\nshallow models, our findings identify depth and readout layers as a decisive\nmechanism for universality, additionally offering a unified perspective that\nsubsumes and extends earlier specialized results."}
{"id": "2510.15817", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15817", "abs": "https://arxiv.org/abs/2510.15817", "authors": ["Camille Touron", "Gabriel V. Cardoso", "Julyan Arbel", "Pedro L. C. Rodrigues"], "title": "Error analysis of a compositional score-based algorithm for simulation-based inference", "comment": null, "summary": "Simulation-based inference (SBI) has become a widely used framework in\napplied sciences for estimating the parameters of stochastic models that best\nexplain experimental observations. A central question in this setting is how to\neffectively combine multiple observations in order to improve parameter\ninference and obtain sharper posterior distributions. Recent advances in\nscore-based diffusion methods address this problem by constructing a\ncompositional score, obtained by aggregating individual posterior scores within\nthe diffusion process. While it is natural to suspect that the accumulation of\nindividual errors may significantly degrade sampling quality as the number of\nobservations grows, this important theoretical issue has so far remained\nunexplored. In this paper, we study the compositional score produced by the\nGAUSS algorithm of Linhart et al. (2024) and establish an upper bound on its\nmean squared error in terms of both the individual score errors and the number\nof observations. We illustrate our theoretical findings on a Gaussian example,\nwhere all analytical expressions can be derived in a closed form."}
{"id": "2510.15824", "categories": ["stat.ML", "cs.LG", "91A26 62G08"], "pdf": "https://arxiv.org/pdf/2510.15824", "abs": "https://arxiv.org/abs/2510.15824", "authors": ["Guillaume Principato", "Gilles Stoltz"], "title": "Blackwell's Approachability for Sequential Conformal Inference", "comment": "25 pages, 0 figures", "summary": "We study conformal inference in non-exchangeable environments through the\nlens of Blackwell's theory of approachability. We first recast adaptive\nconformal inference (ACI, Gibbs and Cand\\`es, 2021) as a repeated two-player\nvector-valued finite game and characterize attainable coverage--efficiency\ntradeoffs. We then construct coverage and efficiency objectives under potential\nrestrictions on the adversary's play, and design a calibration-based\napproachability strategy to achieve these goals. The resulting algorithm enjoys\nstrong theoretical guarantees and provides practical insights, though its\ncomputational burden may limit deployment in practice."}
{"id": "2510.15000", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15000", "abs": "https://arxiv.org/abs/2510.15000", "authors": ["Yixin Fang", "Man Jin"], "title": "Estimand framework and intercurrent events handling for clinical trials with time-to-event outcomes", "comment": null, "summary": "The ICH E9(R1) guideline presents a framework of estimand for clinical\ntrials, proposes five strategies for handling intercurrent events (ICEs), and\nprovides a comprehensive discussion and many real-life clinical examples for\nquantitative outcomes and categorical outcomes. However, in ICH E9(R1) the\ndiscussion is lacking for time-to-event (TTE) outcomes. In this paper, we\ndiscuss how to define estimands and how to handle ICEs for clinical trials with\nTTE outcomes. Specifically, we discuss six ICE handling strategies, including\nthose five strategies proposed by ICH E9(R1) and a new strategy, the\ncompeting-risk strategy. Compared with ICH E9(R1), the novelty of this paper is\nthree-fold: (1) the estimands are defined in terms of potential outcomes, (2)\nthe methods can utilize time-dependent covariates straightforwardly, and (3)\nthe efficient estimators are discussed accordingly."}
{"id": "2510.15011", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15011", "abs": "https://arxiv.org/abs/2510.15011", "authors": ["Tomasz Serafin", "Weronika Nitka"], "title": "Data-driven Calibration Sample Selection and Forecast Combination in Electricity Price Forecasting: An Application of the ARHNN Method", "comment": null, "summary": "Calibration sample selection and forecast combination are two simple yet\npowerful tools used in forecasting. They can be combined with a variety of\nmodels to significantly improve prediction accuracy, at the same time offering\neasy implementation and low computational complexity. While their effectiveness\nhas been repeatedly confirmed in prior scientific literature, the topic is\nstill underexplored in the field of electricity price forecasting. In this\nresearch article we apply the Autoregressive Hybrid Nearest Neighbors (ARHNN)\nmethod to three long-term time series describing the German, Spanish and New\nEngland electricity markets. We show that it outperforms popular literature\nbenchmarks in terms of forecast accuracy by up to 10%. We also propose two\nsimplified variants of the method, granting a vast decrease in computation time\nwith only minor loss of prediction accuracy. Finally, we compare the forecasts'\nperformance in a battery storage system trading case study. We find that using\na forecast-driven strategy can achieve up to 80% of theoretical maximum profits\nwhile trading, demonstrating business value in practical applications."}
{"id": "2510.15670", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15670", "abs": "https://arxiv.org/abs/2510.15670", "authors": ["Paolo Giudici", "Rosa C. Rosciano", "Johanna Schrader", "Delf-Magnus Kummerfeld"], "title": "A Multiclass ROC Curve", "comment": null, "summary": "This paper introduces a novel methodology for constructing multiclass ROC\ncurves using the multidimensional Gini index. The proposed methodology\nleverages the established relationship between the Gini coefficient and the ROC\nCurve and extends it to multiclass settings through the multidimensional Gini\nindex. The framework is validated by means of two comprehensive case studies in\nhealth care and finance. The paper provides a theoretically grounded solution\nto multiclass performance evaluation, particularly valuable for imbalanced\ndatasets, for which a prudential assessment should take precedence over class\nfrequency considerations."}
