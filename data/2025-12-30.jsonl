{"id": "2512.21379", "categories": ["stat.ME", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.21379", "abs": "https://arxiv.org/abs/2512.21379", "authors": ["Brian Knaeble", "Qinyun Lin", "Erich Kummerfeld", "Kenneth A. Frank"], "title": "Sensitivity Analysis of the Consistency Assumption", "comment": null, "summary": "Sensitivity analysis informs causal inference by assessing the sensitivity of conclusions to departures from assumptions. The consistency assumption states that there are no hidden versions of treatment and that the outcome arising naturally equals the outcome arising from intervention. When reasoning about the possibility of consistency violations, it can be helpful to distinguish between covariates and versions of treatment. In the context of surgery, for example, genomic variables are covariates and the skill of a particular surgeon is a version of treatment. There may be hidden versions of treatment, and this paper addresses that concern with a new kind of sensitivity analysis. Whereas many methods for sensitivity analysis are focused on confounding by unmeasured covariates, the methodology of this paper is focused on confounding by hidden versions of treatment. In this paper, new mathematical notation is introduced to support the novel method, and example applications are described."}
{"id": "2512.21451", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21451", "abs": "https://arxiv.org/abs/2512.21451", "authors": ["Bing Cheng", "Howell Tong"], "title": "An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry", "comment": null, "summary": "Being infinite dimensional, non-parametric information geometry has long faced an \"intractability barrier\" due to the fact that the Fisher-Rao metric is now a functional incurring difficulties in defining its inverse. This paper introduces a novel framework to resolve the intractability with an Orthogonal Decomposition of the Tangent Space ($T_fM=S \\oplus S^{\\perp}$), where S represents an observable covariate subspace. Through the decomposition, we derive the Covariate Fisher Information Matrix (cFIM), denoted as $G_f$, which is a finite-dimensional and computable representative of information extractable from the manifold's geometry. Indeed, by proving the Trace Theorem: $H_G(f)=\\text{Tr}(G_f)$, we establish a rigorous foundation for the G-entropy previously introduced by us, thereby identifying it not merely as a gradient-based regularizer, but also as a fundamental geometric invariant representing the total explainable statistical information captured by the probability distribution associated with the model. Furthermore, we establish a link between $G_f$ and the second-order derivative (i.e. the curvature) of the KL-divergence, leading to the notion of Covariate Cramér-Rao Lower Bound(CRLB). We demonstrate that $G_f$ is congruent to the Efficient Fisher Information Matrix, thereby providing fundamental limits of variance for semi-parametric estimators. Finally, we apply our geometric framework to the Manifold Hypothesis, lifting the latter from a heuristic assumption into a testable condition of rank-deficiency within the cFIM. By defining the Information Capture Ratio, we provide a rigorous method for estimating intrinsic dimensionality in high-dimensional data. In short, our work bridges the gap between abstract information geometry and the demand of explainable AI, by providing a tractable path for revealing the statistical coverage and the efficiency of non-parametric models."}
{"id": "2512.21399", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.21399", "abs": "https://arxiv.org/abs/2512.21399", "authors": ["Mark Dominique Dalipe Muñoz"], "title": "Standardized Descriptive Index for Measuring Deviation and Uncertainty in Psychometric Indicators", "comment": "21 pages, 4 figures, 1 table", "summary": "The use of descriptive statistics in pilot testing procedures requires objective, standard diagnostic tools that are feasible for small sample sizes. While current psychometric practices report item-level statistics, they often report these raw descriptives separately rather than consolidating both mean and standard deviation into a single diagnostic tool to directly measure item quality. By leveraging the analytical properties of Cohen's d, this article repurposes its use in scale development as a standardized item deviation index. This measures the extent of an item's raw deviation relative to its scale midpoint while accounting for its own uncertainty. Analytical properties such as boundedness, scale invariance, and bias are explored to further understand how the index values behave, which will aid future efforts to establish empirical thresholds that characterize redundancy among formative indicators and consistency among reflective indicators."}
{"id": "2512.21593", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21593", "abs": "https://arxiv.org/abs/2512.21593", "authors": ["Takuro Kutsuna"], "title": "Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models", "comment": "40 pages", "summary": "Diffusion models have become a central tool in deep generative modeling, but standard formulations rely on a single network and a single diffusion schedule to transform a simple prior, typically a standard normal distribution, into the target data distribution. As a result, the model must simultaneously represent the global structure of the distribution and its fine-scale local variations, which becomes difficult when these scales are strongly mismatched. This issue arises both in natural images, where coarse manifold-level structure and fine textures coexist, and in low-dimensional distributions with highly concentrated local structure. To address this issue, we propose Residual Prior Diffusion (RPD), a two-stage framework in which a coarse prior model first captures the large-scale structure of the data distribution, and a diffusion model is then trained to represent the residual between the prior and the target data distribution. We formulate RPD as an explicit probabilistic model with a tractable evidence lower bound, whose optimization reduces to the familiar objectives of noise prediction or velocity prediction. We further introduce auxiliary variables that leverage information from the prior model and theoretically analyze how they reduce the difficulty of the prediction problem in RPD. Experiments on synthetic datasets with fine-grained local structure show that standard diffusion models fail to capture local details, whereas RPD accurately captures fine-scale detail while preserving the large-scale structure of the distribution. On natural image generation tasks, RPD achieved generation quality that matched or exceeded that of representative diffusion-based baselines and it maintained strong performance even with a small number of inference steps."}
{"id": "2512.21541", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.21541", "abs": "https://arxiv.org/abs/2512.21541", "authors": ["Ping Zhao", "Zhenyu Liu", "Dan Zhuang"], "title": "Adaptive Test for High Dimensional Quantile Regression", "comment": null, "summary": "Testing high-dimensional quantile regression coefficients is crucial, as tail quantiles often reveal more than the mean in many practical applications. Nevertheless, the sparsity pattern of the alternative hypothesis is typically unknown in practice, posing a major challenge. To address this, we propose an adaptive test that remains powerful across both sparse and dense alternatives.We first establish the asymptotic independence between the max-type test statistic proposed by \\citet{tang2022conditional} and the sum-type test statistic introduced by \\citet{chen2024hypothesis}. Building on this result, we propose a Cauchy combination test that effectively integrates the strengths of both statistics and achieves robust performance across a wide range of sparsity levels. Simulation studies and real data applications demonstrate that our proposed procedure outperforms existing methods in terms of both size control and power."}
{"id": "2512.21829", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21829", "abs": "https://arxiv.org/abs/2512.21829", "authors": ["Peter Potaptchik", "Cheuk-Kit Lee", "Michael S. Albergo"], "title": "Tilt Matching for Scalable Sampling and Fine-Tuning", "comment": null, "summary": "We propose a simple, scalable algorithm for using stochastic interpolants to sample from unnormalized densities and for fine-tuning generative models. The approach, Tilt Matching, arises from a dynamical equation relating the flow matching velocity to one targeting the same distribution tilted by a reward, implicitly solving a stochastic optimal control problem. The new velocity inherits the regularity of stochastic interpolant transports while also being the minimizer of an objective with strictly lower variance than flow matching itself. The update to the velocity field can be interpreted as the sum of all joint cumulants of the stochastic interpolant and copies of the reward, and to first order is their covariance. The algorithms do not require any access to gradients of the reward or backpropagating through trajectories of the flow or diffusion. We empirically verify that the approach is efficient and highly scalable, providing state-of-the-art results on sampling under Lennard-Jones potentials and is competitive on fine-tuning Stable Diffusion, without requiring reward multipliers. It can also be straightforwardly applied to tilting few-step flow map models."}
{"id": "2512.21689", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.21689", "abs": "https://arxiv.org/abs/2512.21689", "authors": ["Jiancheng Jiang", "Xuejun Jiang", "Hongxia Jin"], "title": "Cross-Semantic Transfer Learning for High-Dimensional Linear Regression", "comment": null, "summary": "Current transfer learning methods for high-dimensional linear regression assume feature alignment across domains, restricting their applicability to semantically matched features. In many real-world scenarios, however, distinct features in the target and source domains can play similar predictive roles, creating a form of cross-semantic similarity. To leverage this broader transferability, we propose the Cross-Semantic Transfer Learning (CSTL) framework. It captures potential relationships by comparing each target coefficient with all source coefficients through a weighted fusion penalty. The weights are derived from the derivative of the SCAD penalty, effectively approximating an ideal weighting scheme that preserves transferable signals while filtering out source-specific noise. For computational efficiency, we implement CSTL using the Alternating Direction Method of Multipliers (ADMM). Theoretically, we establish that under mild conditions, CSTL achieves the oracle estimator with overwhelming probability. Empirical results from simulations and a real-data application confirm that CSTL outperforms existing methods in both cross-semantic and partial signal similarity settings."}
{"id": "2512.21435", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21435", "abs": "https://arxiv.org/abs/2512.21435", "authors": ["Stefano M. Iacus", "Haodong Qi", "Marcello Carammia", "Thomas Juneau"], "title": "Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)", "comment": null, "summary": "Forecasting conflict-related fatalities remains a central challenge in political science and policy analysis due to the sparse, bursty, and highly non-stationary nature of violence data. We introduce DynAttn, an interpretable dynamic-attention forecasting framework for high-dimensional spatio-temporal count processes. DynAttn combines rolling-window estimation, shared elastic-net feature gating, a compact weight-tied self-attention encoder, and a zero-inflated negative binomial (ZINB) likelihood. This architecture produces calibrated multi-horizon forecasts of expected casualties and exceedance probabilities, while retaining transparent diagnostics through feature gates, ablation analysis, and elasticity measures.\n  We evaluate DynAttn using global country-level and high-resolution PRIO-grid-level conflict data from the VIEWS forecasting system, benchmarking it against established statistical and machine-learning approaches, including DynENet, LSTM, Prophet, PatchTST, and the official VIEWS baseline. Across forecast horizons from one to twelve months, DynAttn consistently achieves substantially higher predictive accuracy, with particularly large gains in sparse grid-level settings where competing models often become unstable or degrade sharply.\n  Beyond predictive performance, DynAttn enables structured interpretation of regional conflict dynamics. In our application, cross-regional analyses show that short-run conflict persistence and spatial diffusion form the core predictive backbone, while climate stress acts either as a conditional amplifier or a primary driver depending on the conflict theater."}
{"id": "2512.22098", "categories": ["stat.ME", "math.PR", "math.ST", "q-bio.PE", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.22098", "abs": "https://arxiv.org/abs/2512.22098", "authors": ["Marco Dalla Pria", "Matteo Ruggiero", "Dario Spanò"], "title": "Exact inference via quasi-conjugacy in two-parameter Poisson-Dirichlet hidden Markov models", "comment": null, "summary": "We introduce a nonparametric model for time-evolving, unobserved probability distributions from discrete-time data consisting of unlabelled partitions. The latent process is a two-parameter Poisson-Dirichlet diffusion, and observations arise via exchangeable sampling. Applications include social and genetic data where only aggregate clustering summaries are observed. To address the intractable likelihood, we develop a tractable inferential framework that avoids label enumeration and direct simulation of the latent state. We exploit a duality between the diffusion and a pure-death process on partitions, together with coagulation operators that encode the effect of new data. These yield closed-form, recursive updates for forward and backward inference. We compute exact posterior distributions of the latent state at arbitrary times and predictive distributions of future or interpolated partitions. This enables online and offline inference and forecasting with full uncertainty quantification, bypassing MCMC and sequential Monte Carlo. Compared to particle filtering, our method achieves higher accuracy, lower variance, and substantial computational gains. We illustrate the methodology with synthetic experiments and a social network application, recovering interpretable patterns in time-varying heterozygosity."}
{"id": "2512.22098", "categories": ["stat.ME", "math.PR", "math.ST", "q-bio.PE", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.22098", "abs": "https://arxiv.org/abs/2512.22098", "authors": ["Marco Dalla Pria", "Matteo Ruggiero", "Dario Spanò"], "title": "Exact inference via quasi-conjugacy in two-parameter Poisson-Dirichlet hidden Markov models", "comment": null, "summary": "We introduce a nonparametric model for time-evolving, unobserved probability distributions from discrete-time data consisting of unlabelled partitions. The latent process is a two-parameter Poisson-Dirichlet diffusion, and observations arise via exchangeable sampling. Applications include social and genetic data where only aggregate clustering summaries are observed. To address the intractable likelihood, we develop a tractable inferential framework that avoids label enumeration and direct simulation of the latent state. We exploit a duality between the diffusion and a pure-death process on partitions, together with coagulation operators that encode the effect of new data. These yield closed-form, recursive updates for forward and backward inference. We compute exact posterior distributions of the latent state at arbitrary times and predictive distributions of future or interpolated partitions. This enables online and offline inference and forecasting with full uncertainty quantification, bypassing MCMC and sequential Monte Carlo. Compared to particle filtering, our method achieves higher accuracy, lower variance, and substantial computational gains. We illustrate the methodology with synthetic experiments and a social network application, recovering interpretable patterns in time-varying heterozygosity."}
{"id": "2512.21826", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.21826", "abs": "https://arxiv.org/abs/2512.21826", "authors": ["Jianmin Chen", "Huiyuan Wang", "Thomas Lumley", "Xiaowu Dai", "Yong Chen"], "title": "Surrogate-Powered Inference: Regularization and Adaptivity", "comment": null, "summary": "High-quality labeled data are essential for reliable statistical inference, but are often limited by validation costs. While surrogate labels provide cost-effective alternatives, their noise can introduce non-negligible bias. To address this challenge, we propose the surrogate-powered inference (SPI) toolbox, a unified framework that leverages both the validity of high-quality labels and the abundance of surrogates to enable reliable statistical inference. SPI comprises three progressively enhanced versions. Base-SPI integrates validated labels and surrogates through augmentation to improve estimation efficiency. SPI+ incorporates regularized regression to safely handle multiple surrogates, preventing performance degradation due to error accumulation. SPI++ further optimizes efficiency under limited validation budgets through an adaptive, multiwave labeling procedure that prioritizes informative subjects for labeling. Compared to traditional methods, SPI substantially reduces the estimation error and increases the power in risk factor identification. These results demonstrate the value of SPI in improving the reproducibility. Theoretical guarantees and extensive simulation studies further illustrate the properties of our approach."}
{"id": "2512.21399", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.21399", "abs": "https://arxiv.org/abs/2512.21399", "authors": ["Mark Dominique Dalipe Muñoz"], "title": "Standardized Descriptive Index for Measuring Deviation and Uncertainty in Psychometric Indicators", "comment": "21 pages, 4 figures, 1 table", "summary": "The use of descriptive statistics in pilot testing procedures requires objective, standard diagnostic tools that are feasible for small sample sizes. While current psychometric practices report item-level statistics, they often report these raw descriptives separately rather than consolidating both mean and standard deviation into a single diagnostic tool to directly measure item quality. By leveraging the analytical properties of Cohen's d, this article repurposes its use in scale development as a standardized item deviation index. This measures the extent of an item's raw deviation relative to its scale midpoint while accounting for its own uncertainty. Analytical properties such as boundedness, scale invariance, and bias are explored to further understand how the index values behave, which will aid future efforts to establish empirical thresholds that characterize redundancy among formative indicators and consistency among reflective indicators."}
{"id": "2512.21840", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.21840", "abs": "https://arxiv.org/abs/2512.21840", "authors": ["Xiaokang Liu", "Jie Hu", "Naimin Jing", "Yang Ning", "Cheng Yong Tang", "Runze Li", "Yong Chen"], "title": "Targeted learning via probabilistic subpopulation matching", "comment": null, "summary": "In biomedical research, to obtain more accurate prediction results from a target study, leveraging information from multiple similar source studies is proved to be useful. However, in many biomedical applications based on real-world data, populations under consideration in different studies, e.g., clinical sites, can be heterogeneous, leading to challenges in properly borrowing information towards the target study. The state of art methods are typically based on study-level matching to identify source studies that are similar to the target study, whilst samples from source studies that significantly differ from the target study will all be dropped at the study level, which can lead to substantial loss of information. We consider a general situation where all studies are sampled from a super-population composed of distinct subpopulations, and propose a novel framework of targeted learning via subpopulation matching. In contrast to the existing study-level matching methods, measuring similarities between subpopulations can effectively decompose both within- and between-study heterogeneity, allowing incorporation of information from all source studies without dropping any samples as in the existing methods. We devise the proposed framework as a two-step procedure, where a finite mixture model is first fitted jointly across all studies to provide subject-wise probabilistic subpopulation information, followed by a step of within-subpopulation information transferring from source studies to the target study for each identified subpopulation. We establish the non-asymptotic properties of our estimator and demonstrate the ability of our method to improve prediction at the target study via simulation studies."}
{"id": "2512.21879", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.21879", "abs": "https://arxiv.org/abs/2512.21879", "authors": ["Xiaokang Liu", "Yuchen Yang", "Yifei Sun", "Jiang Bian", "Yanyuan Ma", "Raymond J. Carroll", "Yong Chen"], "title": "A Communication-Efficient Distributed Algorithm for Learning with Heterogeneous and Structurally Incomplete Multi-Site Data", "comment": null, "summary": "In multicenter biomedical research, integrating data from multiple decentralized sites provides more robust and generalizable findings due to its larger sample size and the ability to account for the between-site heterogeneity. However, sharing individual-level data across sites is often difficult due to patient privacy concerns and regulatory restrictions. To overcome this challenge, many distributed algorithms, that fit a global model by only communicating aggregated information across sites, have been proposed. A major challenge in applying existing distributed algorithms to real-world data is that their validity often relies on the assumption that data across sites are independently and identically distributed, which is frequently violated in practice. In biomedical applications, data distributions across clinical sites can be heterogeneous. Additionally, the set of covariates available at each site may vary due to different data collection protocols. We propose a distributed inference framework for data integration in the presence of both distribution heterogeneity and data structural heterogeneity. By modeling heterogeneous and structurally missing data using density-tilted generalized method of moments, we developed a general aggregated data-based distributed algorithm that is communication-efficient and heterogeneity-aware. We establish the asymptotic properties of our estimator and demonstrate the validity of our method via simulation studies."}
{"id": "2512.21960", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21960", "abs": "https://arxiv.org/abs/2512.21960", "authors": ["Frédéric Cazals", "Antoine Commaret", "Louis Goldenberg"], "title": "Modeling high dimensional point clouds with the spherical cluster model", "comment": "Main text: 4 figures, 15 pages", "summary": "A parametric cluster model is a statistical model providing geometric insights onto the points defining a cluster. The {\\em spherical cluster model} (SC) approximates a finite point set $P\\subset \\mathbb{R}^d$ by a sphere $S(c,r)$ as follows. Taking $r$ as a fraction $η\\in(0,1)$ (hyper-parameter) of the std deviation of distances between the center $c$ and the data points, the cost of the SC model is the sum over all data points lying outside the sphere $S$ of their power distance with respect to $S$. The center $c$ of the SC model is the point minimizing this cost. Note that $η=0$ yields the celebrated center of mass used in KMeans clustering. We make three contributions.\n  First, we show fitting a spherical cluster yields a strictly convex but not smooth combinatorial optimization problem. Second, we present an exact solver using the Clarke gradient on a suitable stratified cell complex defined from an arrangement of hyper-spheres. Finally, we present experiments on a variety of datasets ranging in dimension from $d=9$ to $d=10,000$, with two main observations. First, the exact algorithm is orders of magnitude faster than BFGS based heuristics for datasets of small/intermediate dimension and small values of $η$, and for high dimensional datasets (say $d>100$) whatever the value of $η$. Second, the center of the SC model behave as a parameterized high-dimensional median.\n  The SC model is of direct interest for high dimensional multivariate data analysis, and the application to the design of mixtures of SC will be reported in a companion paper."}
{"id": "2512.22018", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.22018", "abs": "https://arxiv.org/abs/2512.22018", "authors": ["Silvia Novo", "César Sánchez-Sellero"], "title": "Prediction intervals for quantile autoregression", "comment": "30 pages, 4 figures, 6 tables", "summary": "This paper introduces new methods for constructing prediction intervals using quantile-based techniques. The procedures are developed for both classical (homoscedastic) autoregressive models and modern quantile autoregressive models. They combine quantile estimation with multiplier bootstrap schemes to approximate the sampling variability of coefficient estimates, together with bootstrap replications of future observations. We consider both percentile-based and predictive-root-based constructions. Theoretical results establish the validity and pertinence of the proposed methods. Simulation experiments evaluate their finite-sample performance and show that the proposed methods yield improved coverage properties and computational efficiency relative to existing approaches in the literature. The empirical usefulness of the methods is illustrated through applications to U.S. unemployment rate data and retail gasoline prices."}
{"id": "2512.22098", "categories": ["stat.ME", "math.PR", "math.ST", "q-bio.PE", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.22098", "abs": "https://arxiv.org/abs/2512.22098", "authors": ["Marco Dalla Pria", "Matteo Ruggiero", "Dario Spanò"], "title": "Exact inference via quasi-conjugacy in two-parameter Poisson-Dirichlet hidden Markov models", "comment": null, "summary": "We introduce a nonparametric model for time-evolving, unobserved probability distributions from discrete-time data consisting of unlabelled partitions. The latent process is a two-parameter Poisson-Dirichlet diffusion, and observations arise via exchangeable sampling. Applications include social and genetic data where only aggregate clustering summaries are observed. To address the intractable likelihood, we develop a tractable inferential framework that avoids label enumeration and direct simulation of the latent state. We exploit a duality between the diffusion and a pure-death process on partitions, together with coagulation operators that encode the effect of new data. These yield closed-form, recursive updates for forward and backward inference. We compute exact posterior distributions of the latent state at arbitrary times and predictive distributions of future or interpolated partitions. This enables online and offline inference and forecasting with full uncertainty quantification, bypassing MCMC and sequential Monte Carlo. Compared to particle filtering, our method achieves higher accuracy, lower variance, and substantial computational gains. We illustrate the methodology with synthetic experiments and a social network application, recovering interpretable patterns in time-varying heterozygosity."}
