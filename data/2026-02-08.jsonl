{"id": "2602.05032", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2602.05032", "abs": "https://arxiv.org/abs/2602.05032", "authors": ["Sarah Polson", "Vadim Sokolov"], "title": "Fast Compute via MC Boosting", "comment": null, "summary": "Modern training and inference pipelines in statistical learning and deep learning repeatedly invoke linear-system solves as inner loops, yet high-accuracy deterministic solvers can be prohibitively expensive when solves must be repeated many times or when only partial information (selected components or linear functionals) is required. We position \\emph{Monte Carlo boosting} as a practical alternative in this regime, surveying random-walk estimators and sequential residual correction in a unified notation (Neumann-series representation, forward/adjoint estimators, and Halton-style sequential correction), with extensions to overdetermined/least-squares problems and connections to IRLS-style updates in data augmentation and EM/ECM algorithms. Empirically, we compare Jacobi and Gauss--Seidel iterations with plain Monte Carlo, exact sequential Monte Carlo, and a subsampled sequential variant, illustrating scaling regimes that motivate when Monte Carlo boosting can be an enabling compute primitive for modern statistical learning workflows."}
{"id": "2602.05377", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.05377", "abs": "https://arxiv.org/abs/2602.05377", "authors": ["Sandip Barui", "Shovan Chowdhury"], "title": "Optimal Accelerated Life Testing Sampling Plan Design with Piecewise Linear Function based Modeling of Lifetime Characteristics", "comment": null, "summary": "Researchers have widely used accelerated life tests to determine an optimal inspection plan for lot acceptance. All such plans are proposed by assuming a known relationship between the lifetime characteristic(s) and the accelerating stress factor(s) under a parametric framework of the product lifetime distribution. As the true relationship is rarely known in practical scenarios, the assumption itself may produce biased estimates that may lead to an inefficient sampling plan. To this endeavor, an optimal accelerating life test plan is designed under a Type-I censoring scheme with a generalized link structure similar to a spline regression, to capture the nonlinear relationship between the lifetime characteristics and the stress levels. Product lifetime is assumed to follow Weibull distribution with non-identical scale and shape parameters linked with the stress factor through a piecewise linear function. The elements of the Fisher information matrix are computed in detail to formulate the acceptability criterion for the conforming lots. The decision variables of the sampling plan including sample size, stress factors, and others are determined using a constrained aggregated cost minimization approach and variance minimization approach. A simulated case study demonstrates that the nonlinear link-based piecewise linear approximation model outperforms the linear link-based model."}
{"id": "2602.05226", "categories": ["stat.AP", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.05226", "abs": "https://arxiv.org/abs/2602.05226", "authors": ["Matthew C. Johnson", "Matteo Luciani", "Minzhengxiong Zhang", "Kenichiro McAlinn"], "title": "Predictive Synthesis under Sporadic Participation: Evidence from Inflation Density Surveys", "comment": null, "summary": "Central banks rely on density forecasts from professional surveys to assess inflation risks and communicate uncertainty. A central challenge in using these surveys is irregular participation: forecasters enter and exit, skip rounds, and reappear after long gaps. In the European Central Bank's Survey of Professional Forecasters, turnover and missingness vary substantially over time, causing the set of submitted predictions to change from quarter to quarter. Standard aggregation rules -- such as equal-weight pooling, renormalization after dropping missing forecasters, or ad hoc imputation -- can generate artificial jumps in combined predictions driven by panel composition rather than economic information, complicating real-time interpretation and obscuring forecaster performance. We develop coherent Bayesian updating rules for forecast combination under sporadic participation that maintain a well-defined latent predictive state for each forecaster even when their forecast is unobserved. Rather than relying on renormalization or imputation, the combined predictive distribution is updated through the implied conditional structure of the panel. This approach isolates genuine performance differences from mechanical participation effects and yields interpretable dynamics in forecaster influence. In the ECB survey, it improves predictive accuracy relative to equal-weight benchmarks and delivers smoother and better-calibrated inflation density forecasts, particularly during periods of high turnover."}
{"id": "2602.05022", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.05022", "abs": "https://arxiv.org/abs/2602.05022", "authors": ["Yuqi Li", "Quinn Lanners", "Matthew M. Engelhard"], "title": "Double Variable Importance Matching to Estimate Distinct Causal Effects on Event Probability and Timing", "comment": null, "summary": "In many clinical contexts, estimating effects of treatment in time-to-event data is complicated not only by confounding, censoring, and heterogeneity, but also by the presence of a cured subpopulation in which the event of interest never occurs. In such settings, treatment may have distinct effects on (1) the probability of being cured and (2) the event timing among non-cured individuals. Standard survival analysis and causal inference methods typically do not separate cured from non-cured individuals, obscuring distinct treatment mechanisms on cure probability and event timing. To address these challenges, we propose a matching-based framework that constructs distinct match groups to estimate heterogeneous treatment effects (HTE) on cure probability and event timing, respectively. We use mixture cure models to identify feature importance for both estimands, which in turn informs weighted distance metrics for matching in high-dimensional spaces. Within matched groups, Kaplan-Meier estimators provide estimates of cure probability and expected time to event, from which individual-level treatment effects are derived. We provide theoretical guarantees for estimator consistency and distance metric optimality under an equal-scale constraint. We further decompose estimation error into contributions from censoring, model fitting, and irreducible noise. Simulations and real-world data analyses demonstrate that our approach delivers interpretable and robust HTE estimates in time-to-event settings."}
{"id": "2602.05379", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05379", "abs": "https://arxiv.org/abs/2602.05379", "authors": ["Hua Zheng", "Wei Xie", "M. Ben Feng", "Keilung Choy"], "title": "Variance Reduction Based Experience Replay for Policy Optimization", "comment": "24 pages, 4 figures. arXiv admin note: text overlap with arXiv:2208.12341", "summary": "Effective reinforcement learning (RL) for complex stochastic systems requires leveraging historical data collected in previous iterations to accelerate policy optimization. Classical experience replay treats all past observations uniformly and fails to account for their varying contributions to learning. To overcome this limitation, we propose Variance Reduction Experience Replay (VRER), a principled framework that selectively reuses informative samples to reduce variance in policy gradient estimation. VRER is algorithm-agnostic and integrates seamlessly with existing policy optimization methods, forming the basis of our sample-efficient off-policy algorithm, Policy Gradient with VRER (PG-VRER). Motivated by the lack of rigorous theoretical analysis of experience replay, we develop a novel framework that explicitly captures dependencies introduced by Markovian dynamics and behavior-policy interactions. Using this framework, we establish finite-time convergence guarantees for PG-VRER and reveal a fundamental bias-variance trade-off: reusing older experience increases bias but simultaneously reduces gradient variance. Extensive empirical experiments demonstrate that VRER consistently accelerates policy learning and improves performance over state-of-the-art policy optimization algorithms."}
{"id": "2602.05351", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.05351", "abs": "https://arxiv.org/abs/2602.05351", "authors": ["Shivshankar Nila", "Ishapathik Das", "N. Balakrishna"], "title": "A Flexible Modeling of Extremes in the Presence of Inliers", "comment": null, "summary": "Many random phenomena, including life-testing and environmental data, show positive values and excess zeros, which pose modeling challenges. In life testing, immediate failures result in zero lifetimes, often due to defects or poor quality, especially in electronics and clinical trials. These failures, called inliers at zero, are difficult to model using standard approaches. The presence and proportion of inliers may influence the accuracy of extreme value analysis, bias parameter estimates, or even lead to severe events or extreme effects, such as drought or crop failure. In such scenarios, a key issue in extreme value analysis is determining a suitable threshold to capture tail behaviour accurately. Although some extreme value mixture models address threshold and tail estimation, they often inadequately handle inliers, resulting in suboptimal results. Bulk model misspecification can affect the threshold, extreme value estimates, and, in particular, the tail proportion. There is no unified framework for defining extreme value mixture models, especially the tail proportion. This paper proposes a flexible model that handles extremes, inliers, and the tail proportion. Parameters are estimated using maximum likelihood estimation. Compared the proposed model estimates with the classical mean excess plot, parameter stability plot, and Pickands plot estimates. Theoretical results are established, and the proposed model outperforms traditional methods in both simulation studies and real data analysis."}
{"id": "2602.05246", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.05246", "abs": "https://arxiv.org/abs/2602.05246", "authors": ["Menglin Kong", "Chengyuan Zhang", "Lijun Sun"], "title": "Active Simulation-Based Inference for Scalable Car-Following Model Calibration", "comment": null, "summary": "Credible microscopic traffic simulation requires car-following models that capture both the average response and the substantial variability observed across drivers and situations. However, most data-driven calibrations remain deterministic, producing a single best-fit parameter vector and offering limited guidance for uncertainty-aware prediction, risk-sensitive evaluation, and population-level simulation. Bayesian calibration addresses this gap by inferring a posterior distribution over parameters, but per-trajectory sampling methods such as Markov chain Monte Carlo (MCMC) are computationally infeasible for modern large-scale naturalistic driving datasets. This paper proposes an active simulation-based inference framework for scalable car-following model calibration. The approach combines (i) a residual-augmented car-following simulator with two alternatives for the residual process and (ii) an amortized conditional density estimator that maps an observed leader--follower trajectory directly to a driver-specific posterior over model parameters with a single forward pass at test time. To reduce simulation cost during training, we introduce a joint active design strategy that selects informative parameter proposals together with representative driving contexts, focusing simulations where the current inference model is most uncertain while maintaining realism. Experiments on the HighD dataset show improved predictive accuracy and closer agreement between simulated and observed trajectory distributions relative to Bayesian calibration baselines, with convergence and ablation studies supporting the robustness of the proposed design choices. The framework enables scalable, uncertainty-aware driver population modeling for traffic flow simulation and risk-sensitive transportation analysis."}
{"id": "2602.05030", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.05030", "abs": "https://arxiv.org/abs/2602.05030", "authors": ["Tianyu", "Wang", "Matthew C. Johnson", "Steven Klee", "Matthew L. Malloy"], "title": "Billions-Scale Forecast Reconciliation", "comment": null, "summary": "The problem of combining multiple forecasts of related quantities that obey expected equality and additivity constraints, often referred to a hierarchical forecast reconciliation, is naturally stated as a simple optimization problem. In this paper we explore optimization-based point forecast reconciliation at scales faced by large retailers. We implement and benchmark several algorithms to solve the forecast reconciliation problem, showing efficacy when the dimension of the problem exceeds four billion forecasted values. To the best of our knowledge, this is the largest forecast reconciliation problem, and perhaps on-par with the largest constrained least-squares-problem ever solved. We also make several theoretical contributions. We show that for a restricted class of problems and when the loss function is weighted appropriately, least-squares forecast reconciliation is equivalent to share-based forecast reconciliation. This formalizes how the optimization based approach can be thought of as a generalization of share-based reconciliation, applicable to multiple, overlapping data hierarchies."}
{"id": "2602.05716", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.05716", "abs": "https://arxiv.org/abs/2602.05716", "authors": ["Maria De Martino", "Federico Triolo", "Adrien Perigord", "Alice Margherita Ornago", "Davide Liborio Vetrano", "Caterina Gregorio"], "title": "MixMashNet: An R Package for Single and Multilayer Networks", "comment": null, "summary": "The R package MixMashNet provides an integrated framework for estimating and analyzing single and multilayer networks using Mixed Graphical Models (MGMs), accommodating continuous, count, and categorical variables. In the multilayer setting, layers may comprise different types and numbers of variables, and users can explicitly impose a predefined multilayer topology. Bootstrap procedures are implemented to derive confidence intervals for edge weights and node-level centrality indices. In addition, the package includes tools to assess the stability of node community membership and to compute community scores that summarize the latent dimensions identified through network clustering. MixMashNet also offers interactive Shiny applications to support exploration, visualization, and interpretation of the estimated networks."}
{"id": "2602.05041", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.05041", "abs": "https://arxiv.org/abs/2602.05041", "authors": ["Eli Ben-Michael", "Avi Feller", "Luke Keele"], "title": "A Weighting Framework for Clusters as Confounders in Observational Studies", "comment": null, "summary": "When units in observational studies are clustered in groups, such as students in schools or patients in hospitals, researchers often address confounding by adjusting for cluster-level covariates or cluster membership. In this paper, we develop a unified weighting framework that clarifies how different estimation methods control two distinct sources of imbalance: global balance (differences between treated and control units across clusters) and local balance (differences within clusters). We show that inverse propensity score weighting (IPW) with a random effects propensity score model -- the current standard in the literature -- targets only global balance and constant level shifts across clusters, but imposes no constraints on local balance. We then present two approaches that target both forms of balance. First, hierarchical balancing weights directly control global and local balance through a constrained optimization problem. Second, building on the recently proposed Generalized Mundlak approach, we develop a novel Mundlak balancing weights estimator that adjusts for cluster-level sufficient statistics rather than cluster indicators; this approach can accommodate small clusters where all units are treated or untreated. Critically, these approaches rest on different assumptions: hierarchical balancing weights require only that treatment is ignorable given covariates and cluster membership, while Mundlak methods additionally require an exponential family structure. We then compare these methods in a simulation study and in two applications in education and health services research that exhibit very different cluster structures."}
{"id": "2602.05239", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.05239", "abs": "https://arxiv.org/abs/2602.05239", "authors": ["Jihao You", "Dan Tulpan", "Jiaojiao Diao", "Jennifer L. Ellis"], "title": "Impact Range Assessment (IRA): An Interpretable Sensitivity Measure for Regression Modelling", "comment": "17 pages, 3 figures. This manuscript was first submitted to MethodsX on February 4, 2026", "summary": "While regression models capture the relationship between predictors and the response variable, they often lack intuitive accompanying methods to understand the influence of predictors on the outcome. To address this, we introduce an interpretability method called Impact Range Assessment (IRA), which quantifies the maximal influence of each predictor by measuring the total potential change in the response variable, across the predictor range. Validation using synthetic linear and nonlinear datasets demonstrates that relevant predictors produced higher IRA values than irrelevant ones. Moreover, repeated evaluations produced results closely aligned with those from the single-execution analysis, confirming the robustness of the method. A case study using a model that predicts pellet quality demonstrated that the IRA provides a simple and intuitive approach to interpret and rank predictor influence, thereby improving model transparency and reliability."}
{"id": "2602.05846", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05846", "abs": "https://arxiv.org/abs/2602.05846", "authors": ["Leonardo Defilippis", "Florent Krzakala", "Bruno Loureiro", "Antoine Maillard"], "title": "Optimal scaling laws in learning hierarchical multi-index models", "comment": null, "summary": "In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. We derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. We further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. Once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. As a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets."}
{"id": "2602.05377", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.05377", "abs": "https://arxiv.org/abs/2602.05377", "authors": ["Sandip Barui", "Shovan Chowdhury"], "title": "Optimal Accelerated Life Testing Sampling Plan Design with Piecewise Linear Function based Modeling of Lifetime Characteristics", "comment": null, "summary": "Researchers have widely used accelerated life tests to determine an optimal inspection plan for lot acceptance. All such plans are proposed by assuming a known relationship between the lifetime characteristic(s) and the accelerating stress factor(s) under a parametric framework of the product lifetime distribution. As the true relationship is rarely known in practical scenarios, the assumption itself may produce biased estimates that may lead to an inefficient sampling plan. To this endeavor, an optimal accelerating life test plan is designed under a Type-I censoring scheme with a generalized link structure similar to a spline regression, to capture the nonlinear relationship between the lifetime characteristics and the stress levels. Product lifetime is assumed to follow Weibull distribution with non-identical scale and shape parameters linked with the stress factor through a piecewise linear function. The elements of the Fisher information matrix are computed in detail to formulate the acceptability criterion for the conforming lots. The decision variables of the sampling plan including sample size, stress factors, and others are determined using a constrained aggregated cost minimization approach and variance minimization approach. A simulated case study demonstrates that the nonlinear link-based piecewise linear approximation model outperforms the linear link-based model."}
{"id": "2602.05862", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05862", "abs": "https://arxiv.org/abs/2602.05862", "authors": ["Rohan Hore", "Rina Foygel Barber"], "title": "Distribution-free two-sample testing with blurred total variation distance", "comment": "47 pages, 4 figures", "summary": "Two-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. In particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (TV) distance between the distributions, is impossible to achieve in a distribution-free regime. In this work, we examine the blurred TV distance, a relaxation of TV distance that enables us to perform inference without assumptions on the distributions. We provide theoretical guarantees for distribution-free upper and lower bounds on the blurred TV distance, and examine its properties in high dimensions."}
{"id": "2602.05351", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.05351", "abs": "https://arxiv.org/abs/2602.05351", "authors": ["Shivshankar Nila", "Ishapathik Das", "N. Balakrishna"], "title": "A Flexible Modeling of Extremes in the Presence of Inliers", "comment": null, "summary": "Many random phenomena, including life-testing and environmental data, show positive values and excess zeros, which pose modeling challenges. In life testing, immediate failures result in zero lifetimes, often due to defects or poor quality, especially in electronics and clinical trials. These failures, called inliers at zero, are difficult to model using standard approaches. The presence and proportion of inliers may influence the accuracy of extreme value analysis, bias parameter estimates, or even lead to severe events or extreme effects, such as drought or crop failure. In such scenarios, a key issue in extreme value analysis is determining a suitable threshold to capture tail behaviour accurately. Although some extreme value mixture models address threshold and tail estimation, they often inadequately handle inliers, resulting in suboptimal results. Bulk model misspecification can affect the threshold, extreme value estimates, and, in particular, the tail proportion. There is no unified framework for defining extreme value mixture models, especially the tail proportion. This paper proposes a flexible model that handles extremes, inliers, and the tail proportion. Parameters are estimated using maximum likelihood estimation. Compared the proposed model estimates with the classical mean excess plot, parameter stability plot, and Pickands plot estimates. Theoretical results are established, and the proposed model outperforms traditional methods in both simulation studies and real data analysis."}
{"id": "2602.05927", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05927", "abs": "https://arxiv.org/abs/2602.05927", "authors": ["Siquan Li", "Yao Tong", "Haonan Wang", "Tianyang Hu"], "title": "Transformers Are Born Biased: Structural Inductive Biases at Random Initialization and Their Practical Consequences", "comment": null, "summary": "Transformers underpin modern large language models (LLMs) and are commonly assumed to be behaviorally unstructured at random initialization, with all meaningful preferences emerging only through large-scale training. We challenge this assumption by showing that randomly initialized transformers already exhibit strong and systematic structural biases. In particular, untrained models display extreme token preferences: across random input sequences, certain tokens are predicted with probabilities orders of magnitude larger.\n  We provide a mechanistic explanation for this phenomenon by dissecting the transformer architecture at initialization. We show that extreme token preference arises from a contraction of token representations along a random seed-dependent direction. This contraction is driven by two interacting forces: (i) asymmetric nonlinear activations in MLP sublayers induce global (inter-sequence) representation concentration, and (ii) self-attention further amplifies this effect through local (intra-sequence) aggregation. Together, these mechanisms align hidden representations along a direction determined solely by the random initialization, producing highly non-uniform next-token predictions.\n  Beyond mechanistic insight, we demonstrate that these initialization-induced biases persist throughout training, forming a stable and intrinsic model identity. Leveraging this property, we introduce SeedPrint, a fingerprinting method that can reliably distinguish models that differ only in their random initialization, even after extensive training and under substantial distribution shift. Finally, we identify a fundamental positional discrepancy inherent to the attention mechanism's intra-sequence contraction that is causally linked to the attention-sink phenomenon. This discovery provides a principled explanation for the emergence of sinks and offers a pathway for their control."}
{"id": "2602.05938", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.05938", "abs": "https://arxiv.org/abs/2602.05938", "authors": ["Juho Pelto", "Kari Auranen", "Janne V. Kujala", "Leo Lahti"], "title": "A Bayesian approach to differential prevalence analysis with applications in microbiome studies", "comment": null, "summary": "Recent evidence suggests that analyzing the presence/absence of taxonomic features can offer a compelling alternative to differential abundance analysis in microbiome studies. However, standard approaches face challenges with boundary cases and multiple testing. To address these challenges, we developed DiPPER (Differential Prevalence via Probabilistic Estimation in R), a method based on Bayesian hierarchical modeling. We benchmarked our method against existing differential prevalence and abundance methods using data from 67 publicly available human gut microbiome studies. We observed considerable variation in performance across methods, with DiPPER outperforming alternatives by combining high sensitivity with effective error control. DiPPER also demonstrated superior replication of findings across independent studies. Furthermore, DiPPER provides differential prevalence estimates and uncertainty intervals that are inherently adjusted for multiple testing."}
{"id": "2602.05611", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.05611", "abs": "https://arxiv.org/abs/2602.05611", "authors": ["Nils Lid Hjort"], "title": "The stochastic view used in climate sciences: (some) perspectives from (some of) mathematical statistics", "comment": "17 pages, 18 figures", "summary": "Climate statistics is of course a very broad field, along with the many connections and impacts for yet other areas, with a history as long as mankind has been recording temperatures, describing drastic weather events, etc. The important work of Klaus Hasselmann, with crucial contributions to the field, along with various other connected strands of work,is being reviewed and discussed in other chapters. The aim of the present chapter is to point to a few statistical methodology themes of relevance for and joint interest with climate statistics. These themes, presented from a statistical methods perspective, include (i) more careful modelling and model selection strategies for meteorological type time series; (ii) methods for prediction, not only for future values of a time series, but for assessing when a trend might be crossing a barrier, along with relevant measures of uncertainty for these; (iii) climatic influence on marine biology; (iv) monitoring processes to assess whether and then to what extent models and their parameters have stayed reasonably constant over time; (v) combination of outputs from different information sources; and (vi) analysing probabilities and their uncertainties related to extreme events."}
{"id": "2602.05716", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.05716", "abs": "https://arxiv.org/abs/2602.05716", "authors": ["Maria De Martino", "Federico Triolo", "Adrien Perigord", "Alice Margherita Ornago", "Davide Liborio Vetrano", "Caterina Gregorio"], "title": "MixMashNet: An R Package for Single and Multilayer Networks", "comment": null, "summary": "The R package MixMashNet provides an integrated framework for estimating and analyzing single and multilayer networks using Mixed Graphical Models (MGMs), accommodating continuous, count, and categorical variables. In the multilayer setting, layers may comprise different types and numbers of variables, and users can explicitly impose a predefined multilayer topology. Bootstrap procedures are implemented to derive confidence intervals for edge weights and node-level centrality indices. In addition, the package includes tools to assess the stability of node community membership and to compute community scores that summarize the latent dimensions identified through network clustering. MixMashNet also offers interactive Shiny applications to support exploration, visualization, and interpretation of the estimated networks."}
{"id": "2602.05784", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.05784", "abs": "https://arxiv.org/abs/2602.05784", "authors": ["Caihong Qin", "Lan Xue", "Ufuk Beyaztas", "Roger S. Zoh", "Mark Benden", "Jeff Goldsmith", "Carmen D. Tekwe"], "title": "Correcting Measurement Error and Zero Inflation in Functional Covariates for Scalar-on-Function Quantile Regression", "comment": null, "summary": "Wearable devices collect time-varying biobehavioral data, offering opportunities to investigate how behaviors influence health outcomes. However, these data often contain measurement error and excess zeros (due to nonwear, sedentary behavior, or connectivity issues), each characterized by subject-specific distributions. Current statistical methods fail to address these issues simultaneously. We introduce a novel modeling framework for zero-inflated and error-prone functional data by incorporating a subject-specific time-varying validity indicator that explicitly distinguishes structural zeros from intrinsic values. We iteratively estimate the latent functional covariates and zero-inflation probabilities via maximum likelihood, using basis expansions and linear mixed models to adjust for measurement error. To assess the effects of the recovered latent covariates, we apply joint quantile regression across multiple quantile levels. Through extensive simulations, we demonstrate that our approach significantly improves estimation accuracy over methods that only address measurement error, and joint estimation yields substantial improvements compared with fitting separate quantile regressions. Applied to a childhood obesity study, our approach effectively corrects for zero inflation and measurement error in step counts, yielding results that closely align with energy expenditure and supporting their use as a proxy for physical activity."}
{"id": "2602.05798", "categories": ["stat.ME", "cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05798", "abs": "https://arxiv.org/abs/2602.05798", "authors": ["Arnau Vilella", "Jasin Machkour", "Michael Muma", "Daniel P. Palomar"], "title": "Learning False Discovery Rate Control via Model-Based Neural Networks", "comment": "Accepted to IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Controlling the false discovery rate (FDR) in high-dimensional variable selection requires balancing rigorous error control with statistical power. Existing methods with provable guarantees are often overly conservative, creating a persistent gap between the realized false discovery proportion (FDP) and the target FDR level. We introduce a learning-augmented enhancement of the T-Rex Selector framework that narrows this gap. Our approach replaces the analytical FDP estimator with a neural network trained solely on diverse synthetic datasets, enabling a substantially tighter and more accurate approximation of the FDP. This refinement allows the procedure to operate much closer to the desired FDR level, thereby increasing discovery power while maintaining effective approximate control. Through extensive simulations and a challenging synthetic genome-wide association study (GWAS), we demonstrate that our method achieves superior detection of true variables compared to existing approaches."}
{"id": "2602.05798", "categories": ["stat.ME", "cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05798", "abs": "https://arxiv.org/abs/2602.05798", "authors": ["Arnau Vilella", "Jasin Machkour", "Michael Muma", "Daniel P. Palomar"], "title": "Learning False Discovery Rate Control via Model-Based Neural Networks", "comment": "Accepted to IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Controlling the false discovery rate (FDR) in high-dimensional variable selection requires balancing rigorous error control with statistical power. Existing methods with provable guarantees are often overly conservative, creating a persistent gap between the realized false discovery proportion (FDP) and the target FDR level. We introduce a learning-augmented enhancement of the T-Rex Selector framework that narrows this gap. Our approach replaces the analytical FDP estimator with a neural network trained solely on diverse synthetic datasets, enabling a substantially tighter and more accurate approximation of the FDP. This refinement allows the procedure to operate much closer to the desired FDR level, thereby increasing discovery power while maintaining effective approximate control. Through extensive simulations and a challenging synthetic genome-wide association study (GWAS), we demonstrate that our method achieves superior detection of true variables compared to existing approaches."}
{"id": "2602.05938", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.05938", "abs": "https://arxiv.org/abs/2602.05938", "authors": ["Juho Pelto", "Kari Auranen", "Janne V. Kujala", "Leo Lahti"], "title": "A Bayesian approach to differential prevalence analysis with applications in microbiome studies", "comment": null, "summary": "Recent evidence suggests that analyzing the presence/absence of taxonomic features can offer a compelling alternative to differential abundance analysis in microbiome studies. However, standard approaches face challenges with boundary cases and multiple testing. To address these challenges, we developed DiPPER (Differential Prevalence via Probabilistic Estimation in R), a method based on Bayesian hierarchical modeling. We benchmarked our method against existing differential prevalence and abundance methods using data from 67 publicly available human gut microbiome studies. We observed considerable variation in performance across methods, with DiPPER outperforming alternatives by combining high sensitivity with effective error control. DiPPER also demonstrated superior replication of findings across independent studies. Furthermore, DiPPER provides differential prevalence estimates and uncertainty intervals that are inherently adjusted for multiple testing."}
{"id": "2602.05226", "categories": ["stat.AP", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.05226", "abs": "https://arxiv.org/abs/2602.05226", "authors": ["Matthew C. Johnson", "Matteo Luciani", "Minzhengxiong Zhang", "Kenichiro McAlinn"], "title": "Predictive Synthesis under Sporadic Participation: Evidence from Inflation Density Surveys", "comment": null, "summary": "Central banks rely on density forecasts from professional surveys to assess inflation risks and communicate uncertainty. A central challenge in using these surveys is irregular participation: forecasters enter and exit, skip rounds, and reappear after long gaps. In the European Central Bank's Survey of Professional Forecasters, turnover and missingness vary substantially over time, causing the set of submitted predictions to change from quarter to quarter. Standard aggregation rules -- such as equal-weight pooling, renormalization after dropping missing forecasters, or ad hoc imputation -- can generate artificial jumps in combined predictions driven by panel composition rather than economic information, complicating real-time interpretation and obscuring forecaster performance. We develop coherent Bayesian updating rules for forecast combination under sporadic participation that maintain a well-defined latent predictive state for each forecaster even when their forecast is unobserved. Rather than relying on renormalization or imputation, the combined predictive distribution is updated through the implied conditional structure of the panel. This approach isolates genuine performance differences from mechanical participation effects and yields interpretable dynamics in forecaster influence. In the ECB survey, it improves predictive accuracy relative to equal-weight benchmarks and delivers smoother and better-calibrated inflation density forecasts, particularly during periods of high turnover."}
