<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 3]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 27]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Bayesian Multiple Testing for Suicide Risk in Pharmacoepidemiology: Leveraging Co-Prescription Patterns](https://arxiv.org/abs/2601.17985)
*Soumya Sahu,Kwan Hur,Dulal K. Bhaumik,Robert Gibbons*

Main category: stat.AP

TL;DR: 本文提出了一种统一的贝叶斯尖峰-平板框架，用于分析922种处方药与自杀风险的关系，通过结构化先验和贝叶斯错误发现率控制，改进了传统药物警戒方法。


<details>
  <summary>Details</summary>
Motivation: 自杀是美国第十大死因，但药物相关风险或保护作用的证据仍然有限。现有的上市后研究通常一次只检查一类药物，或依赖经验贝叶斯收缩和保守的多重性校正，牺牲了检测临床有意义信号的能力。

Method: 引入统一的贝叶斯尖峰-平板框架，利用美国商业索赔数据（2003-2014年，1.5亿患者）中真实的联合处方模式，构建协方差先验，在药理学相关药物间自适应地借用信息。该方法将结构化先验与贝叶斯错误发现率控制相结合。

Result: 相对于Gibbons等人的经验贝叶斯分析，该方法不仅重新确认了关键的有害信号（如阿普唑仑、氢可酮）和保护信号（如米氮平、叶酸），还揭示了新的关联，如高风险阿片类药物组合和几个具有潜在预防作用的叶酸相关药物。对18种抗抑郁药的重新分析显示了不同联合处方指标如何调节效应估计。

Conclusion: 该研究为临床医生和监管机构提供了可操作的假设，展示了结构化贝叶斯建模在药物警戒中的价值，能够改进高维罕见事件监测中的网络引导变量选择。

Abstract: Suicide is the tenth leading cause of death in the United States, yet evidence on medication-related risk or protection remains limited. Most post-marketing studies examine one drug class at a time or rely on empirical-Bayes shrinkage with conservative multiplicity corrections, sacrificing power to detect clinically meaningful signals. We introduce a unified Bayesian spike-and-slab framework that advances both applied suicide research and statistical methodology. Substantively, we screen 922 prescription drugs across 150 million patients in U.S. commercial claims (2003 to 2014), leveraging real-world co-prescription patterns to inform a covariance prior that adaptively borrows strength across pharmacologically related agents. Statistically, the model couples this structured prior with Bayesian false-discovery-rate control, illustrating how network-guided variable selection can improve rare-event surveillance in high dimensions. Relative to the seminal empirical-Bayes analysis of Gibbons et al. (2019), our approach reconfirms the key harmful (e.g., alprazolam, hydrocodone) and protective (e.g., mirtazapine, folic acid) signals while revealing additional associations, such as a high-risk opioid combination and several folate-linked agents with potential preventive benefit that had been overlooked. A focused re-analysis of 18 antidepressants shows how alternative co-prescription metrics modulate effect estimates, shedding light on competitive versus complementary prescribing. These findings generate actionable hypotheses for clinicians and regulators and showcase the value of structured Bayesian modeling in pharmacovigilance.

</details>


### [2] [Uncertainty Quantification in Coupled Multiphysics Systems via Gaussian Process Surrogates: Application to Fuel Assembly Bow](https://arxiv.org/abs/2601.18480)
*Ali Abboud,Josselin Garnier,Bertrand Leturcq,Stanislas de Lambert*

Main category: stat.AP

TL;DR: 提出一个基于高斯过程代理模型的耦合多物理场系统数学框架，用于压水堆燃料组件弯曲预测中的不确定性量化，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 压水堆燃料组件弯曲预测需要解决紧密耦合的流固相互作用问题，直接模拟计算成本过高，使得大规模不确定性量化非常困难。

Method: 引入一个通用的数学框架，将代表不同物理求解器的高斯过程代理模型耦合起来，理论分析证明在温和的正则性和稳定性假设下，耦合GP系统的预测方差保持有界。

Result: 该方法应用于燃料组件弯曲的耦合水力-结构模拟，以直接代码耦合的一小部分计算成本实现了全局敏感性分析和完整的不确定性量化，结果展示了准确的不确定性传播和稳定的预测。

Conclusion: 为大规模多物理场模拟中基于代理模型的耦合建立了坚实的数学基础，能够实现严格的不确定性量化。

Abstract: Predicting fuel assembly bow in pressurized water reactors requires solving tightly coupled fluid-structure interaction problems, whose direct simulations can be computationally prohibitive, making large-scale uncertainty quantification (UQ) very challenging. This work introduces a general mathematical framework for coupling Gaussian process (GP) surrogate models representing distinct physical solvers, aimed at enabling rigorous UQ in coupled multiphysics systems. A theoretical analysis establishes that the predictive variance of the coupled GP system remains bounded under mild regularity and stability assumptions, ensuring that uncertainty does not grow uncontrollably through the iterative coupling process. The methodology is then applied to the coupled hydraulic-structural simulation of fuel assembly bow, enabling global sensitivity analysis and full UQ at a fraction of the computational cost of direct code coupling. The results demonstrate accurate uncertainty propagation and stable predictions, establishing a solid mathematical basis for surrogate-based coupling in large-scale multiphysics simulations.

</details>


### [3] [A varying-coefficient model for characterizing duration-driven heterogeneity in flood-related health impacts](https://arxiv.org/abs/2601.18656)
*Sarika Aggarwal,Phillip B. Nicol,Brent A. Coull,Rachel C. Nethery*

Main category: stat.AP

TL;DR: 提出EDVCM框架，用于估计洪水持续时间对健康影响的异质性，通过贝叶斯高斯过程建模，在医疗保险数据中分析洪水对肌肉骨骼疾病住院的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然揭示了洪水暴露与不良健康结果之间的关联，但洪水事件具有高度异质性，特别是持续时间差异巨大（如突发性洪水与缓慢移动洪水）。然而，很少有研究将暴露持续时间纳入洪水相关健康影响的建模中，或研究持续时间驱动的效应异质性。

Method: 提出暴露持续时间变系数建模（EDVCM）框架，在区域级自匹配研究设计中开发，消除时间不变混杂因素，然后使用条件泊松回归建模估计暴露效应并调整时间变化混杂因素。在贝叶斯框架下，引入持续时间和暴露天数特定的暴露系数，并赋予二维高斯过程先验，允许跨持续时间和暴露天数共享信息。

Result: 通过模拟证明，EDVCM在效应估计和不确定性量化方面优于传统方法。应用于全国性、多十年的医疗保险索赔数据与高分辨率洪水暴露测量数据，研究了洪水对肌肉骨骼系统疾病住院影响的持续时间驱动异质性。

Conclusion: EDVCM框架能够提供高分辨率的持续时间驱动效应异质性洞察，同时通过信息共享确保模型稳定性，为理解洪水暴露持续时间对健康影响的复杂模式提供了新方法。

Abstract: Previous work revealed associations between flood exposure and adverse health outcomes during and in the aftermath of flood events. Floods are highly heterogeneous events, largely owing to vast differences in flood durations, i.e., flash-floods versus slow-moving floods. However, little to no work has incorporated exposure duration into the modeling of flood-related health impacts or has investigated duration-driven effect heterogeneity. To address this gap, we propose an exposure duration varying coefficient modeling (EDVCM) framework for estimating exposure day-specific health effects of consecutive-day environmental exposures that vary in duration. We develop the EDVCM within an area-level self-matched study design to eliminate time-invariant confounding followed by conditional Poisson regression modeling for exposure effect estimation and adjustment of time-varying confounders. Using a Bayesian framework, we introduce duration- and exposure day-specific exposure coefficients within the conditional Poisson model and assign them a two-dimensional Gaussian process prior to allow for sharing of information across both duration and exposure day. This approach enables highly-resolved insights into duration-driven effect heterogeneity while ensuring model stability through information sharing. Through simulations, we demonstrate that the EDVCM out-performs conventional approaches in terms of both effect estimation and uncertainty quantification. We apply the EDVCM to nationwide, multi-decade Medicare claims data linked with high-resolution flood exposure measures to investigate duration-driven heterogeneity in flood effects on musculoskeletal system disease hospitalizations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [4] [Data-Driven Information-Theoretic Causal Bounds under Unmeasured Confounding](https://arxiv.org/abs/2601.17160)
*Yonghan Jung,Bogyeong Kang*

Main category: stat.ML

TL;DR: 提出基于信息论的因果效应部分识别框架，无需未测量混杂的外部参数或辅助变量，直接利用观测数据获得尖锐的因果边界


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法存在四大局限：1) 需要限制性假设（如结果变量有界或离散）；2) 依赖外部输入（工具变量、代理变量或用户指定的敏感性参数）；3) 需要完整的结构因果模型；4) 仅关注总体平均效应而忽略协变量条件处理效应

Method: 建立信息论驱动的数据驱动散度边界，证明观测分布P(Y|A=a,X=x)与干预分布P(Y|do(A=a),X=x)之间的f-散度仅由倾向得分函数上界约束。开发满足Neyman正交性的半参数估计器，即使使用灵活机器学习方法估计nuisance函数也能保证√n一致推断

Result: 理论证明条件因果效应可实现尖锐部分识别，无需外部敏感性参数、辅助变量、完整结构规范或结果有界假设。模拟研究和实际数据应用表明该框架在各种数据生成过程中提供紧密且有效的因果边界

Conclusion: 该信息论框架同时克服了现有方法的四大局限，实现了直接从观测数据进行条件因果效应的尖锐部分识别，为未测量混杂下的因果推断提供了强大且实用的工具

Abstract: We develop a data-driven information-theoretic framework for sharp partial identification of causal effects under unmeasured confounding. Existing approaches often rely on restrictive assumptions, such as bounded or discrete outcomes; require external inputs (for example, instrumental variables, proxies, or user-specified sensitivity parameters); necessitate full structural causal model specifications; or focus solely on population-level averages while neglecting covariate-conditional treatment effects. We overcome all four limitations simultaneously by establishing novel information-theoretic, data-driven divergence bounds. Our key theoretical contribution shows that the f-divergence between the observational distribution P(Y | A = a, X = x) and the interventional distribution P(Y | do(A = a), X = x) is upper bounded by a function of the propensity score alone. This result enables sharp partial identification of conditional causal effects directly from observational data, without requiring external sensitivity parameters, auxiliary variables, full structural specifications, or outcome boundedness assumptions. For practical implementation, we develop a semiparametric estimator satisfying Neyman orthogonality (Chernozhukov et al., 2018), which ensures square-root-n consistent inference even when nuisance functions are estimated using flexible machine learning methods. Simulation studies and real-world data applications, implemented in the GitHub repository (https://github.com/yonghanjung/Information-Theretic-Bounds), demonstrate that our framework provides tight and valid causal bounds across a wide range of data-generating processes.

</details>


### [5] [Error Analysis of Bayesian Inverse Problems with Generative Priors](https://arxiv.org/abs/2601.17374)
*Bamdad Hosseini,Ziqi Huang*

Main category: stat.ML

TL;DR: 该论文分析了使用生成模型作为先验的逆问题求解方法，给出了基于Wasserstein-2距离的最小生成模型的定量误差界，并展示了后验误差如何继承先验的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于数据驱动的逆问题求解方法因机器学习技术的发展而日益流行。其中一种流行方法是训练生成模型来学习特定问题的先验分布，但缺乏对这些方法误差的理论分析。

Method: 使用Wasserstein-2距离的最小生成模型作为先验，分析其在逆问题中的表现。通过理论推导得到定量误差界，并在椭圆PDE逆问题等基准测试中进行数值实验验证。

Result: 在特定假设下，证明后验误差会继承先验相对于Wasserstein-1距离的收敛速率。数值实验验证了误差分析的关键方面，特别是在椭圆PDE逆问题中生成先验对非平稳场的建模效果。

Conclusion: 该研究为使用生成模型作为先验的逆问题求解方法提供了理论保证，表明在适当条件下，后验误差能够保持与先验相同的收敛速率，为这类数据驱动方法提供了理论基础。

Abstract: Data-driven methods for the solution of inverse problems have become widely popular in recent years thanks to the rise of machine learning techniques. A popular approach concerns the training of a generative model on additional data to learn a bespoke prior for the problem at hand. In this article we present an analysis for such problems by presenting quantitative error bounds for minimum Wasserstein-2 generative models for the prior. We show that under some assumptions, the error in the posterior due to the generative prior will inherit the same rate as the prior with respect to the Wasserstein-1 distance. We further present numerical experiments that verify that aspects of our error analysis manifests in some benchmarks followed by an elliptic PDE inverse problem where a generative prior is used to model a non-stationary field.

</details>


### [6] [Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes](https://arxiv.org/abs/2601.18145)
*Heguang Lin,Binhao Chen,Mengze Li,Daniel Pimentel-Alarcón,Matthew L. Malloy*

Main category: stat.ML

TL;DR: 本文提出了一种认证算法，用于判断两个观察到的多项分布结果的最小体积置信集(MVCs)是否相交，解决了A/B测试中的核心决策问题。


<details>
  <summary>Details</summary>
Motivation: 最小体积置信集(MVCs)在多项分布参数中是统计最优的，但其几何形状复杂且难以直接计算。本文关注一个实际决策问题：给定两个观察结果，能否认证它们的MVCs是否相交，这对于A/B测试至关重要。

Method: 利用似然排序在对数几率坐标中产生半空间约束，通过自适应几何划分参数空间，并在每个单元上计算p值的上下界。对于三维类别，该方法提供了高效且可证明正确的算法，能够认证相交、不相交或在指定容差范围内返回不确定结果。

Result: 该方法为三维类别提供了高效的认证算法，并可扩展到更高维度。结果表明，尽管MVCs几何形状不规则，但针对A/B测试中的核心任务，它们允许可靠的认证决策过程。

Conclusion: 尽管最小体积置信集几何复杂，但本文提出的认证算法能够可靠地解决A/B测试中的核心决策问题，为实际应用提供了可行的解决方案。

Abstract: Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.

</details>


### [7] ["Rebuilding" Statistics in the Age of AI: A Town Hall Discussion on Culture, Infrastructure, and Training](https://arxiv.org/abs/2601.17510)
*David L. Donoho,Jian Kang,Xihong Lin,Bhramar Mukherjee,Dan Nettleton,Rebecca Nugent,Abel Rodriguez,Eric P. Xing,Tian Zheng,Hongtu Zhu*

Main category: stat.ML

TL;DR: 2024年JSM关于"AI时代的统计学"圆桌会议完整记录，讨论了统计学领域如何应对AI、基础模型和大规模实证建模的挑战


<details>
  <summary>Details</summary>
Motivation: 记录和保存关于统计学在AI时代演变的公开讨论，促进透明度、社区反思和持续对话

Method: 采用开放小组讨论和广泛观众问答的形式，收集经验驱动的观点而非正式演讲，围绕五个核心问题组织对话

Result: 提供了关于学科文化、数据管理、现代实证建模、AI应用培训以及与AI利益相关者合作等问题的深入讨论记录

Conclusion: 该记录支持统计学在数据和AI为中心的未来中不断演变的角色讨论，促进社区反思和持续对话

Abstract: This article presents the full, original record of the 2024 Joint Statistical Meetings (JSM) town hall, "Statistics in the Age of AI," which convened leading statisticians to discuss how the field is evolving in response to advances in artificial intelligence, foundation models, large-scale empirical modeling, and data-intensive infrastructures. The town hall was structured around open panel discussion and extensive audience Q&A, with the aim of eliciting candid, experience-driven perspectives rather than formal presentations or prepared statements. This document preserves the extended exchanges among panelists and audience members, with minimal editorial intervention, and organizes the conversation around five recurring questions concerning disciplinary culture and practices, data curation and "data work," engagement with modern empirical modeling, training for large-scale AI applications, and partnerships with key AI stakeholders. By providing an archival record of this discussion, the preprint aims to support transparency, community reflection, and ongoing dialogue about the evolving role of statistics in the data- and AI-centric future.

</details>


### [8] [Boosting methods for interval-censored data with regression and classification](https://arxiv.org/abs/2601.17973)
*Yuan Bian,Grace Y. Yi,Wenqing He*

Main category: stat.ML

TL;DR: 提出针对区间删失数据的非参数提升方法，通过删失无偏变换调整损失函数，在回归和分类任务中提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统提升算法针对完全观测数据设计，难以处理区间删失数据（常见于生存分析、医学研究等领域），需要开发专门方法。

Method: 使用删失无偏变换调整损失函数，通过变换响应值插补，采用函数梯度下降实现，确保可扩展性和适应性。

Result: 建立了理论性质（最优性和均方误差权衡），实证研究显示在各种有限样本场景下具有稳健性能。

Conclusion: 提出的方法为区间删失数据提供了稳健的预测框架，扩展了现有提升技术的应用范围，具有实际应用价值。

Abstract: Boosting has garnered significant interest across both machine learning and statistical communities. Traditional boosting algorithms, designed for fully observed random samples, often struggle with real-world problems, particularly with interval-censored data. This type of data is common in survival analysis and time-to-event studies where exact event times are unobserved but fall within known intervals. Effective handling of such data is crucial in fields like medical research, reliability engineering, and social sciences. In this work, we introduce novel nonparametric boosting methods for regression and classification tasks with interval-censored data. Our approaches leverage censoring unbiased transformations to adjust loss functions and impute transformed responses while maintaining model accuracy. Implemented via functional gradient descent, these methods ensure scalability and adaptability. We rigorously establish their theoretical properties, including optimality and mean squared error trade-offs. Our proposed methods not only offer a robust framework for enhancing predictive accuracy in domains where interval-censored data are common but also complement existing work, expanding the applicability of existing boosting techniques. Empirical studies demonstrate robust performance across various finite-sample scenarios, highlighting the practical utility of our approaches.

</details>


### [9] [A Cherry-Picking Approach to Large Load Shaping for More Effective Carbon Reduction](https://arxiv.org/abs/2601.17990)
*Bokan Chen,Raiden Hasegawa,Adriaan Hilbers,Ross Koningstein,Ana Radovanović,Utkarsh Shah,Gabriela Volpato,Mohamed Ahmed,Tim Cary,Rod Frowd*

Main category: stat.ML

TL;DR: 该研究通过ERCOT电网模拟分析不同负荷整形策略对碳排放和电力成本的影响，发现基于节点边际价格的策略效果最好，并提出基于电网信号和历史数据的"择优选择"方法。


<details>
  <summary>Details</summary>
Motivation: 数据中心等大型负荷的整形会影响电网调度，进而影响系统碳排放和能源成本，但缺乏详细的反事实数据来准确评估现有策略的有效性。

Method: 使用校准的ERCOT日前直流最优潮流模拟进行反事实分析，评估多种负荷整形策略对电网碳排放和电力成本的影响。

Result: 基于节点边际价格的负荷整形在年度碳排放减少方面优于其他常见策略，但仍有改进空间。研究发现基于电网信号和历史数据的"择优选择"方法更有效。

Conclusion: 提出"择优选择"的负荷整形方法，可根据可观测的电网信号和历史数据选择每日最佳策略，适用于数据中心、分布式能源和虚拟电厂等大型灵活负荷。

Abstract: Shaping multi-megawatt loads, such as data centers, impacts generator dispatch on the electric grid, which in turn affects system CO2 emissions and energy cost. Substantiating the effectiveness of prevalent load shaping strategies, such as those based on grid-level average carbon intensity, locational marginal price, or marginal emissions, is challenging due to the lack of detailed counterfactual data required for accurate attribution. This study uses a series of calibrated granular ERCOT day-ahead direct current optimal power flow (DC-OPF) simulations for counterfactual analysis of a broad set of load shaping strategies on grid CO2 emissions and cost of electricity. In terms of annual grid level CO2 emissions reductions, LMP-based shaping outperforms other common strategies, but can be significantly improved upon. Examining the performance of practicable strategies under different grid conditions motivates a more effective load shaping approach: one that "cherry-picks" a daily strategy based on observable grid signals and historical data. The cherry-picking approach to power load shaping is applicable to any large flexible consumer on the electricity grid, such as data centers, distributed energy resources and Virtual Power Plants (VPPs).

</details>


### [10] [Nonlinear multi-study factor analysis](https://arxiv.org/abs/2601.18128)
*Gemma E. Moran,Anandi Krishnan*

Main category: stat.ML

TL;DR: 提出多研究稀疏变分自编码器，用于从多研究高维数据中识别共享和特定因子，应用于血小板基因表达数据分析。


<details>
  <summary>Details</summary>
Motivation: 高维多研究数据中，需要区分哪些潜在因子是所有研究共享的，哪些是特定研究特有的。以血小板基因表达数据为例，需要识别跨疾病共享的基因簇和疾病特异的基因簇。

Method: 提出非线性多研究因子模型，使用多研究稀疏变分自编码器进行拟合。模型具有稀疏性：每个观测特征仅依赖于少量潜在因子；并隐含惩罚潜在因子数量，有助于区分共享和特定因子。

Result: 证明了潜在因子的可识别性，并在血小板基因表达数据中展示了方法能够恢复有意义的因子。

Conclusion: 该方法能有效从多研究高维数据中识别共享和特定因子，在基因组学等应用中具有实用价值。

Abstract: High-dimensional data often exhibit variation that can be captured by lower dimensional factors. For high-dimensional data from multiple studies or environments, one goal is to understand which underlying factors are common to all studies, and which factors are study or environment-specific. As a particular example, we consider platelet gene expression data from patients in different disease groups. In this data, factors correspond to clusters of genes which are co-expressed; we may expect some clusters (or biological pathways) to be active for all diseases, while some clusters are only active for a specific disease. To learn these factors, we consider a nonlinear multi-study factor model, which allows for both shared and specific factors. To fit this model, we propose a multi-study sparse variational autoencoder. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. In the genomics example, this means each gene is active in only a few biological processes. Further, the model implicitly induces a penalty on the number of latent factors, which helps separate the shared factors from the group-specific factors. We prove that the latent factors are identified, and demonstrate our method recovers meaningful factors in the platelet gene expression data.

</details>


### [11] [Out-of-Distribution Radar Detection with Complex VAEs: Theory, Whitening, and ANMF Fusion](https://arxiv.org/abs/2601.18677)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: stat.ML

TL;DR: 提出基于复数变分自编码器(CVAE)的海杂波中弱复值信号检测方法，通过异常检测和与ANMF融合，在非高斯海杂波中实现更高检测概率


<details>
  <summary>Details</summary>
Motivation: 解决非高斯、距离变化的海杂波中弱复值信号检测问题，特别是在海事雷达场景中，传统检测器性能受限

Method: 使用复数变分自编码器(CVAE)进行异常检测，直接在I/Q样本上操作，保留相位和多普勒结构；评估两种配置：未处理距离剖面和局部白化后；最后与ANMF通过加权对数概率融合规则集成

Result: CVAE在两种配置下都获得比经典检测器(MF、NMF等)更高的检测概率，白化后改进最显著；融合的CVAE-ANMF方案在强非高斯杂波中具有更强鲁棒性，并能实现经验校准的虚警控制

Conclusion: 统计归一化与复数生成模型结合显著改善了实际海杂波条件下的检测性能，融合的CVAE-ANMF方案构成了模型基检测器的有竞争力替代方案

Abstract: We investigate the detection of weak complex-valued signals immersed in non-Gaussian, range-varying interference, with emphasis on maritime radar scenarios. The proposed methodology exploits a Complex-valued Variational AutoEncoder (CVAE) trained exclusively on clutter-plus-noise to perform Out-Of-Distribution detection. By operating directly on in-phase / quadrature samples, the CVAE preserves phase and Doppler structure and is assessed in two configurations: (i) using unprocessed range profiles and (ii) after local whitening, where per-range covariance estimates are obtained from neighboring profiles. Using extensive simulations together with real sea-clutter data from the CSIR maritime dataset, we benchmark performance against classical and adaptive detectors (MF, NMF, AMF-SCM, ANMF-SCM, ANMF-Tyler). In both configurations, the CVAE yields a higher detection probability Pd at matched false-alarm rate Pfa, with the most notable improvements observed under whitening. We further integrate the CVAE with the ANMF through a weighted log-p fusion rule at the decision level, attaining enhanced robustness in strongly non-Gaussian clutter and enabling empirically calibrated Pfa control under H0. Overall, the results demonstrate that statistical normalization combined with complex-valued generative modeling substantively improves detection in realistic sea-clutter conditions, and that the fused CVAE-ANMF scheme constitutes a competitive alternative to established model-based detectors.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [12] [The generalised balanced power diagram: flat sections, affine transformations and an improved rendering algorithm](https://arxiv.org/abs/2601.18593)
*Felix Ballani*

Main category: stat.CO

TL;DR: 论文研究了广义平衡功率图(GBPD)作为描述具有弯曲晶界的多晶微观结构的几何模型，分析了其仿射变换和平截面的性质，并扩展了功率图的数字图像生成算法到GBPD上，提高了效率。


<details>
  <summary>Details</summary>
Motivation: GBPD被认为是描述具有弯曲晶界多晶微观结构的合适几何模型，但需要更深入地理解其数学性质，并开发更高效的算法来生成数字图像表示。

Method: 1. 分析GBPD在仿射变换和平截面方面的数学性质；2. 将功率图的高效数字图像生成算法扩展到GBPD上，替代传统的暴力方法。

Result: 1. 系统整理了GBPD的仿射变换和平截面性质；2. 成功将高效算法扩展到GBPD，相比传统暴力方法显著提高了数字图像生成效率。

Conclusion: GBPD作为多晶微观结构建模的有效工具，其数学性质得到了系统分析，并且通过算法扩展实现了更高效的数字图像生成，为材料科学中的微观结构分析提供了更好的计算工具。

Abstract: The generalised balanced power diagram (GBPD) is regarded in the literature as a suitable geometric model for describing polycrystalline microstructures with curved grain boundaries. This article compiles properties of GBPDs with regard to affine transformations and flat sections. Furthermore, it extends an algorithm known for power diagrams for generating digital images, which is more efficient than the usual brute force approach, on GBPDs.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [13] [Evaluating Aggregated Relational Data Models with Simple Diagnostics](https://arxiv.org/abs/2601.17153)
*Ian Laga,Benjamin Vogel,Jieyun Wang,Anna Smith,Owen Ward*

Main category: stat.ME

TL;DR: 提出一个用于评估聚合关系数据（ARD）模型的诊断框架，帮助研究者系统评估模型拟合度并选择合适的模型


<details>
  <summary>Details</summary>
Motivation: 聚合关系数据广泛用于估计社交网络特征和人群规模，但现有ARD估计器缺乏评估模型拟合度的指导方法，研究者需要系统化的诊断工具来判断模型是否充分拟合数据

Method: 提出基于点估计（最大似然或最大后验优化）的诊断框架，系统评估协变量结构、分布假设和相关性，无需重复贝叶斯模型拟合即可快速评估

Result: 通过模拟研究和大型ARD数据集应用，证明该工作流程能有效识别常见的模型失配源，帮助研究者选择能充分解释数据的合适模型

Conclusion: 该诊断框架为ARD模型评估提供了系统化、可重复的流程，解决了实践中缺乏模型拟合度评估指导的问题，有助于提高ARD分析的质量和可靠性

Abstract: Aggregated Relational Data (ARD) contain summary information about individual social networks and are widely used to estimate social network characteristics and the size of populations of interest. Although a variety of ARD estimators exist, practitioners currently lack guidance on how to evaluate whether a selected model adequately fits the data. We introduce a diagnostic framework for ARD models that provides a systematic, reproducible process for assessing covariate structure, distributional assumptions, and correlation. The diagnostics are based on point estimates, using either maximum likelihood or maximum a posteriori optimization, which allows quick evaluation without requiring repeated Bayesian model fitting. Through simulation studies and applications to large ARD datasets, we show that the proposed workflow identifies common sources of model misfit and helps researchers select an appropriate model that adequately explains the data.

</details>


### [14] [Varying coefficient model for longitudinal data with informative observation times](https://arxiv.org/abs/2601.17141)
*Yu Gu,Yangjianchen Xu,Peijun Sang*

Main category: stat.ME

TL;DR: 提出了一种考虑信息性观测时间的变系数模型估计方法，通过逆强度加权解决观测时间与纵向结果相关导致的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有变系数模型都假设观测时间与纵向结果独立，但实际研究中常存在结果驱动或其他信息性访视安排，这会导致现有方法产生偏差和无效推断。

Method: 将观测时间过程建模为比例强度模型下的通用计数过程，采用逆强度加权结合筛估计框架，通过加权最小二乘法得到闭式系数函数估计。

Result: 建立了估计量的一致性、收敛速度和渐近正态性，构建了系数函数的逐点置信区间。模拟研究表明当观测时间具有信息性时，加权方法显著优于传统未加权方法。

Conclusion: 该方法能有效处理信息性观测时间问题，在阿尔茨海默病神经影像学倡议研究中得到成功应用，为存在信息性访视安排的纵向研究提供了可靠的统计推断工具。

Abstract: Varying coefficient models are widely used to characterize dynamic associations between longitudinal outcomes and covariates. Existing work on varying coefficient models, however, all assumes that observation times are independent of the longitudinal outcomes, which is often violated in real-world studies with outcome-driven or otherwise informative visit schedules. Such informative observation times can lead to biased estimation and invalid inference using existing methods. In this article, we develop estimation and inference procedures for varying coefficient models that account for informative observation times. We model the observation time process as a general counting process under a proportional intensity model, with time-varying covariates summarizing the observed history. To address potential bias, we incorporate inverse intensity weighting into a sieve estimation framework, yielding closed-form coefficient function estimators via weighted least squares. We establish consistency, convergence rates, and asymptotic normality of the proposed estimators, and construct pointwise confidence intervals for the coefficient functions. Extensive simulation studies demonstrate that the proposed weighted method substantially outperforms the conventional unweighted method when observation times are informative. Finally, we provide an application of our method to the Alzheimer's Disease Neuroimaging Initiative study.

</details>


### [15] [Optimal Design under Interference, Homophily, and Robustness Trade-offs](https://arxiv.org/abs/2601.17145)
*Vydhourie Thiyageswaran,Alex Kokot,Jennifer Brennan,Marina Meila,Christina Lee Yu,Maryam Fazel*

Main category: stat.ME

TL;DR: 该论文提出了一种在存在网络干扰和同质性的情况下，优化全局平均处理效应估计均方误差的随机化设计方法，使用半正定规划和Gram-Schmidt Walk两种算法。


<details>
  <summary>Details</summary>
Motivation: 在社交网络等存在同质性的场景中，传统的集群随机化设计反而会增加GATE估计的均方误差，需要开发能够同时考虑干扰、同质性和异质性变化的新设计方法。

Method: 1) 建立包含干扰、同质性和异质性变化的潜在结果模型；2) 在Horvitz-Thompson估计器下，构建最坏情况均方误差优化框架；3) 使用半正定规划和Gaussian rounding算法；4) 采用Gram-Schmidt Walk向量平衡算法。

Result: 通过模拟网络数据和真实村庄网络数据集的实验，验证了所提设计方法的性能优于传统集群随机化设计。

Conclusion: 该研究为存在网络干扰和同质性的GATE估计提供了有效的随机化设计框架，两种互补算法都能显著改善估计精度，具有实际应用价值。

Abstract: To minimize the mean squared error (MSE) in global average treatment effect (GATE) estimation under network interference, a popular approach is to use a cluster-randomized design. However, in the presence of homophily, which is common in social networks, cluster randomization can instead increase the MSE. We develop a novel potential outcomes model that accounts for interference, homophily, and heterogeneous variation. In this setting, we establish a framework for optimizing designs for worst-case MSE under the Horvitz-Thompson estimator. This leads to an optimization problem over the covariance matrices of the treatment assignment, trading off interference, homophily, and robustness. We frame and solve this problem using two complementary approaches. The first involves formulating a semidefinite program (SDP) and employing Gaussian rounding, in the spirit of the Goemans-Williamson approximation algorithm for MAXCUT. The second is an adaptation of the Gram-Schmidt Walk, a vector-balancing algorithm which has recently received much attention. Finally, we evaluate the performance of our designs through various experiments on simulated network data and a real village network dataset.

</details>


### [16] [An Empirical Method for Analyzing Count Data](https://arxiv.org/abs/2601.17233)
*Jiren Sun,Linda Amoafo,Yongming Qu*

Main category: stat.ME

TL;DR: 提出一种经验方法替代负二项回归，用于比较临床试验中的计数终点事件率，在事件稀疏时更稳定


<details>
  <summary>Details</summary>
Motivation: 负二项回归在事件稀疏时数值不稳定，且基线协变量调整的效率增益对模型误设敏感，需要更稳健的方法

Method: 提出经验方法，针对与负二项回归相同的边际估计目标（边际事件率比），避免对计数结果的分布假设

Result: 模拟研究显示经验方法在各种场景下保持适当的I型错误控制，达到与负二项回归相当的效能，基线协变量调整带来一致的效率增益

Conclusion: 经验方法为负二项回归提供了稳健且数值稳定的替代方案，特别适用于事件数量少或数值稳定性受关注的情况

Abstract: Count endpoints are common in clinical trials, particularly for recurrent events such as hypoglycemia. When interest centers on comparing overall event rates between treatment groups, negative binomial (NB) regression is widely used because it accommodates overdispersion and requires only event counts and exposure times. However, NB regression can be numerically unstable when events are sparse, and the efficiency gains from baseline covariate adjustment may be sensitive to model misspecification. We propose an empirical method that targets the same marginal estimand as NB regression -- the ratio of marginal event rates -- while avoiding distributional assumptions on the count outcome. Simulation studies show that the empirical method maintains appropriate Type I error control across diverse scenarios, including extreme overdispersion and zero inflation, achieves power comparable to NB regression, and yields consistent efficiency gains from baseline covariate adjustment. We illustrate the approach using severe hypoglycemia data from the QWINT-5 trial comparing insulin efsitora alfa with insulin degludec in adults with type 1 diabetes. In this sparse-event setting, the empirical method produced stable marginal rate estimates and rate ratios closely aligned with observed rates, while NB regression exhibited greater sensitivity and larger deviations from the observed rates in the sparsest intervals. The proposed empirical method provides a robust and numerically stable alternative to NB regression, particularly when the number of events is low or when numerical stability is a concern.

</details>


### [17] [The effect of collinearity and sample size on linear regression results: a simulation study](https://arxiv.org/abs/2601.18072)
*Stephanie CC van der Lubbe,Jose M Valderas,Evangelos Kontopantelis*

Main category: stat.ME

TL;DR: 论文通过模拟研究发现，多重共线性对OLS回归的影响高度依赖于样本量，不应机械应用固定VIF阈值。在小样本中，即使轻度共线性也会显著降低精度和功效，而在大样本中，即使高共线性也能保持稳健估计。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常对VIF应用固定阈值（如VIF>5或10）来诊断多重共线性问题，但这种做法忽略了样本量的影响。多重共线性本质上是一个有限样本问题，其影响应与样本量结合考虑，但缺乏系统研究量化这种联合效应。

Method: 通过模拟研究，在样本量N=100-100,000和VIF=1-50的范围内生成数据。每个场景生成1,000个数据集，拟合OLS模型，评估覆盖率、平均绝对误差(MAE)、偏差、传统功效（置信区间不包含0）和精度保证（95% CI落在真实效应预设范围内的概率）。还评估了有偏误设定（遗漏相关预测变量）下的偏差放大效应。

Result: 在正确设定下，共线性不影响名义覆盖率和系统性偏差，但在小样本中降低精度：N=100时，即使轻度共线性(VIF<2)也会增加MAE并显著降低两种功效指标；而N>=50,000时，即使VIF=50估计也保持稳健。在误设定下，共线性强烈放大偏差，增加误差，降低覆盖率，即使低VIF也会严重损害精度保证和传统功效。

Conclusion: VIF阈值不应机械应用。共线性必须结合样本量和潜在偏差源来解释；仅为了降低VIF而移除预测变量可能通过遗漏变量偏差恶化推断。附带的heatmap为不同研究规模和建模假设提供了实用参考。

Abstract: Background: Multicollinearity inflates the variance of OLS coefficients, widening confidence intervals and reducing inferential reliability. Yet fixed variance inflation factor (VIF) cut-offs are often applied uniformly across studies with very different sample sizes, even though collinearity is a finite-sample problem. We quantify how collinearity and sample size jointly affect linear regression performance and provide practical guidance for interpreting VIFs.
  Methods: We simulated data across sample sizes N=100-100,000 and collinearity levels VIF=1-50. For each scenario we generated 1,000 datasets, fitted OLS models, and assessed coverage, mean absolute error (MAE), bias, traditional power (CI excludes 0), and precision assurance (probability the 95% CI lies within a prespecified margin around the true effect). We also evaluated a biased, misspecified setting by omitting a relevant predictor to study bias amplification.
  Results: Under correct specification, collinearity did not materially affect nominal coverage and did not introduce systematic bias, but it reduced precision in small samples: at N=100, even mild collinearity (VIF<2) inflated MAE and markedly reduced both power metrics, whereas at N>=50,000 estimates were robust even at VIF=50. Under misspecification, collinearity strongly amplified bias, increasing errors, reducing coverage, and sharply degrading both precision assurance and traditional power even at low VIF.
  Conclusion: VIF thresholds should not be applied mechanically. Collinearity must be interpreted in relation to sample size and potential sources of bias; removing predictors solely to reduce VIF can worsen inference via omitted-variable bias. The accompanying heatmaps provide a practical reference across study sizes and modelling assumptions.

</details>


### [18] [Falsifying Predictive Algorithm](https://arxiv.org/abs/2601.17146)
*Amanda Coston*

Main category: stat.ME

TL;DR: 提出一个统计检验框架，用于验证算法是否对预期结果比不允许的代理变量有更好的预测能力，从而评估判别效度。


<details>
  <summary>Details</summary>
Motivation: 实际应用中经常发现算法预测的是非预期结果，需要在部署前识别算法是否预测了不应预测的量，避免在重要场景中产生意外后果。

Method: 基于因果推断、计量经济学和心理测量学的证伪实践，提出一个证伪框架，通过比较校准预测损失来评估算法相对于指定不允许代理变量的判别效度，使用非参数假设检验方法。

Result: 在招生场景中，框架成功建立了相对于性别的判别效度，但未能建立相对于种族的判别效度；在刑事司法场景中，框架显示出局限性，需要补充方法来评估其他效度方面。

Conclusion: 该证伪框架可作为部署前的早期效度检查，但需要与其他方法结合来全面评估构念效度和外部效度。

Abstract: Empirical investigations into unintended model behavior often show that the algorithm is predicting another outcome than what was intended. These exposes highlight the need to identify when algorithms predict unintended quantities - ideally before deploying them into consequential settings. We propose a falsification framework that provides a principled statistical test for discriminant validity: the requirement that an algorithm predict intended outcomes better than impermissible ones. Drawing on falsification practices from causal inference, econometrics, and psychometrics, our framework compares calibrated prediction losses across outcomes to assess whether the algorithm exhibits discriminant validity with respect to a specified impermissible proxy. In settings where the target outcome is difficult to observe, multiple permissible proxy outcomes may be available; our framework accommodates both this setting and the case with a single permissible proxy. Throughout we use nonparametric hypothesis testing methods that make minimal assumptions on the data-generating process. We illustrate the method in an admissions setting, where the framework establishes discriminant validity with respect to gender but fails to establish discriminant validity with respect to race. This demonstrates how falsification can serve as an early validity check, prior to fairness or robustness analyses. We also provide analysis in a criminal justice setting, where we highlight the limitations of our framework and emphasize the need for complementary approaches to assess other aspects of construct validity and external validity.

</details>


### [19] [Vaccine Efficacy Estimands Implied by Common Estimators Used in Individual Randomized Field Trials](https://arxiv.org/abs/2601.18587)
*Michael P. Fay,Dean Follmann,Bruce J. Swihart,Lauren E. Dang*

Main category: stat.ME

TL;DR: 该论文回顾了自然暴露随机试验中的疫苗效力估计量，探讨了不同效应测量比率的近似质量，区分了完全免疫估计量和局部估计量，并讨论了局部估计量的解释困难


<details>
  <summary>Details</summary>
Motivation: 在自然暴露的随机试验中，疫苗效力估计量存在多种定义和计算方法，需要系统回顾这些估计量的性质、近似质量、因果解释以及局部时间点估计量的潜在偏倚问题

Method: 采用非参数化公式分析不同比率效应测量（发病率比、累积发病率比、风险比、比值比）的近似质量；区分完全免疫估计量和意向治疗人群；使用脆弱模型作为敏感性分析工具

Result: 在低对照事件率下，不同比率效应测量近似相等；完全免疫估计量需要排除疫苗起效前发生事件的人群；局部疫苗效力估计量存在易感者耗竭偏倚等解释困难

Conclusion: 疫苗效力估计量需要根据研究设计和目标仔细选择，完全免疫估计量适用于主要终点，局部估计量需要谨慎解释，脆弱模型可作为敏感性分析工具评估个体水平因果效应

Abstract: We review vaccine efficacy (VE) estimands for susceptibility in individual randomized trials with natural (unmeasured) exposure, where individual responses are measured as time from vaccination until an event (e.g., disease from the infectious agent). Common VE estimands are written as $1-θ$, where $θ$ is some ratio effect measure (e.g., ratio of incidence rates, cumulative incidences, hazards, or odds) comparing outcomes under vaccination versus control. Although the ratio effects are approximately equal with low control event rates, we explore the quality of that approximation using a nonparametric formulation. Traditionally, the primary endpoint VE estimands are full immunization (or biological) estimands that represent a subset of the intent-to-treat population, excluding those that have the event before the vaccine has been able to ramp-up to its full effect, requiring care for proper causal interpretation. Besides these primary VE estimands that summarize an effect of the vaccine over the full course of the study, we also consider local VE estimands that measure the effect at particular time points. We discuss interpretational difficulties of local VE estimands (e.g., depletion of susceptibles bias), and using frailty models as sensitivity analyses for the individual-level causal effects over time.

</details>


### [20] [Goodness-of-Fit Checks for Joint Models](https://arxiv.org/abs/2601.18598)
*Dimitris Rizopoulos,Jeremy M. G. Taylor,Isabella Kardys*

Main category: stat.ME

TL;DR: 提出贝叶斯后验预测检查框架，用于评估纵向与生存联合模型的拟合优度，能识别传统信息准则无法检测的模型误设


<details>
  <summary>Details</summary>
Motivation: 现有联合模型比较标准无法判断模型是否充分拟合数据或哪些组件存在误设，需要更全面的评估框架

Method: 贝叶斯后验预测检查框架，支持多种设置：现有受试者、仅有协变量的新受试者、中间随访时间的动态预测、交叉验证评估。纵向部分评估均值、方差和相关结构，生存部分使用经验累积分布和概率积分变换，关联部分使用时变一致性统计量

Result: 应用于Bio-SHiFT心力衰竭研究，模拟研究表明该方法能识别标准信息准则无法检测的模型误设

Conclusion: 提出的贝叶斯后验预测检查框架为联合模型提供了全面的拟合优度评估工具，已实现于R包JMbayes2中

Abstract: Joint models for longitudinal and time-to-event data are widely used in many disciplines. Nonetheless, existing model comparison criteria do not indicate whether a model adequately fits the data or which components may be misspecified. We introduce a Bayesian posterior predictive checks framework for assessing a joint model's fit to the longitudinal and survival processes and their association. The framework supports multiple settings, including existing subjects, new subjects with only covariates, dynamic prediction at intermediate follow-up times, and cross-validated assessment. For the longitudinal component, goodness-of-fit is assessed through the mean, variance, and correlation structure, while the survival component is evaluated using empirical cumulative distributions and probability integral transforms. The association between processes is examined using time-dependent concordance statistics. We apply these checks to the Bio-SHiFT heart failure study, and a simulation study demonstrates that they can identify model misspecification that standard information criteria fail to detect. The proposed methodology is implemented in the freely available R package JMbayes2.

</details>


### [21] [Bayesian Inference for Discrete Markov Random Fields Through Coordinate Rescaling](https://arxiv.org/abs/2601.17205)
*Giuseppe Arena,Maarten Marsman*

Main category: stat.ME

TL;DR: 提出坐标重缩放采样方法，将伪似然空间参数映射到目标后验，解决离散马尔可夫随机场中精确后验推断的计算挑战


<details>
  <summary>Details</summary>
Motivation: 离散马尔可夫随机场中精确后验推断因配分函数难以处理而计算困难，现有方法要么难以扩展到大型系统，要么低估目标后验分布的变异性

Method: 提出坐标重缩放采样方法，将模型参数从伪似然空间映射到目标后验空间，在保持计算效率的同时改善后验推断

Result: 模拟研究表明，坐标重缩放采样比现有方法能更准确地估计后验变异性

Conclusion: 坐标重缩放采样为离散马尔可夫随机场的贝叶斯推断提供了可扩展且稳健的解决方案

Abstract: Discrete Markov random fields (MRFs) represent a class of undirected graphical models that capture complex conditional dependencies between discrete variables. Conducting exact posterior inference in these models is computationally challenging due to the intractable partition function, which depends on the model parameters and sums over all possible state configurations in the system. As a result, using the exact likelihood function is infeasible and existing methods, such as Double Metropolis-Hastings or pseudo-likelihood approximations, either scale poorly to large systems or underestimate the variability of the target posterior distribution. To address both computational burden and efficiency loss, we propose a new class of coordinate-rescaling sampling methods, which map the model parameters from the pseudo-likelihood space to the target posterior, preserving computational efficiency while improving posterior inference. Finally, in simulation studies, we compare the proposed method to existing approaches and illustrate that coordinate-rescaling sampling provides more accurate estimates of posterior variability, offering a scalable and robust solution for Bayesian inference in discrete MRFs.

</details>


### [22] [Transfer learning for scalar-on-function regression via control variates](https://arxiv.org/abs/2601.17217)
*Yuping Yang,Zhiyang Zhou*

Main category: stat.ME

TL;DR: 提出一种基于控制变量法的迁移学习框架，用于标量对函数回归，仅使用数据集级统计量，无需共享个体数据，适用于隐私受限场景。


<details>
  <summary>Details</summary>
Motivation: 迁移学习能通过利用相关数据集信息提升估计和预测性能。现有方法在隐私受限或去中心化环境中面临挑战，需要共享个体级数据。本文旨在开发一种仅使用数据集级统计量的迁移学习框架，适用于隐私受限场景。

Method: 将控制变量法重新用于标量对函数回归的迁移学习。框架仅依赖数据集特定的汇总统计量，避免汇集个体级数据。建立了多种现有迁移学习策略的理论联系，推导了基于控制变量法提案的收敛速率。

Result: 理论分析揭示了协方差函数相似性对收敛行为的影响，并考虑了通常被忽略的平滑误差。数值研究支持理论发现，表明所提方法在估计和预测性能上与现有替代方法具有竞争力。

Conclusion: 提出的基于控制变量法的迁移学习框架为标量对函数回归提供了一种有效的隐私保护解决方案，仅需数据集级统计量，在隐私受限环境中具有实际应用价值。

Abstract: Transfer learning (TL) has emerged as a powerful tool for improving estimation and prediction performance by leveraging information from related datasets. In this paper, we repurpose the control-variates (CVS) method for TL in the context of scalar-on-function regression. Our proposed framework relies exclusively on dataset-specific summary statistics, avoiding the need to pool subject-level data and thus remaining applicable in privacy-restricted or decentralized settings. We establish theoretical connections among several existing TL strategies and derive convergence rates for our CVS-based proposals. These rates explicitly account for the typically overlooked smoothing error and reveal how the similarity among covariance functions across datasets influences convergence behavior. Numerical studies support the theoretical findings and demonstrate that the proposed methods achieve competitive estimation and prediction performance compared with existing alternatives.

</details>


### [23] [Capturing Cumulative Disease Burden in Chronic Kidney Disease Outcome Trials: Area Under the Curve and Restricted Mean Time in Favor of Treatment Beyond Conventional Time-to-First Analysis](https://arxiv.org/abs/2601.17241)
*Jiren Sun,Tuo Wang,Yu Du*

Main category: stat.ME

TL;DR: 针对慢性肾病临床试验中传统时间-首次事件分析的局限性，提出了两种新方法：AUC（曲线下面积）和RMT-IF（治疗有利限制平均时间），以更好地量化累积疾病负担和治疗效果。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病是渐进性疾病，传统Cox模型分析存在两个主要问题：1）对复合终点各组分赋予相同权重，忽略了临床重要性差异（如eGFR下降与ESRD或死亡）；2）只关注首次事件，忽略了后续疾病进展。需要能全面评估累积疾病负担的方法。

Method: 提出两种互补方法：1）AUC方法：为不同疾病状态分配序数严重性评分，计算平均累积评分曲线下面积，量化总无事件时间损失；2）RMT-IF方法：将限制平均生存时间扩展到多状态过程，测量治疗组患者处于更有利状态的平均时间。

Result: 这些方法能更好地捕捉慢性肾病的渐进性特征，治疗效果不仅体现在首次事件延迟，还体现在整体疾病轨迹的改善。通过区分不同临床重要性的事件并量化完整疾病过程，提供了更全面的评估框架。

Conclusion: AUC和RMT-IF方法为肾脏保护疗法提供了替代评估框架，能更好地区分事件临床重要性，量化完整疾病过程，可能提高未来慢性肾病结局试验的效率和可解释性。

Abstract: Chronic kidney disease (CKD) affects millions worldwide and progresses irreversibly through stages culminating in end-stage renal disease (ESRD) and death. Outcome trials in CKD traditionally employ time-to-first-event analyses using the Cox models. However, this approach has fundamental limitations for progressive diseases: it assigns equal weight to each composite endpoint component despite clear clinical hierarchy: an eGFR decline threshold receives the same weight as ESRD or death in the analysis, and it captures only the first occurrence while ignoring subsequent progression. Given CKD's gradual evolution over years, comprehensive treatment evaluation requires quantifying cumulative disease burden: integrating both event severity and time spent in each disease state. We propose two complementary approaches to better characterize treatment benefits by incorporating event severity and state occupancy: area under the curve (AUC) and restricted mean time in favor of treatment (RMT-IF). The AUC method assigns ordinal severity scores to disease states and calculates the area under the mean cumulative score curve, quantifying total event-free time lost. Treatment effects are expressed as AUC ratios or differences. The RMT-IF extends restricted mean survival time to multistate processes, measuring average time patients in the treatment arm spend in more favorable states versus the comparator. These methods better capture CKD's progressive nature where treatment benefits extend beyond first-event delay to overall disease trajectory modification. By discriminating between events of differing clinical importance and quantifying the complete disease course, these estimands offer alternative assessment frameworks for kidney-protective therapies, potentially improving efficiency and interpretability of future CKD outcome trials.

</details>


### [24] [Covariate-assisted Grade of Membership Models via Shared Latent Geometry](https://arxiv.org/abs/2601.17265)
*Zhiyu Xu,Yuqi Gu*

Main category: stat.ME

TL;DR: 提出协变量辅助的隶属度模型，通过谱估计方法结合响应和协变量信息，无需联合分布建模，在理论和实际中均显示协变量能提升潜在结构恢复效果


<details>
  <summary>Details</summary>
Motivation: 传统方法通过完全指定的联合似然函数整合协变量信息，计算量大且对模型设定错误敏感。需要一种更稳健高效的方法来利用协变量信息改进潜在结构恢复。

Method: 提出协变量辅助隶属度模型，利用响应和协变量共享的低秩单纯形几何结构，而非建模联合分布。采用无似然谱估计方法，通过平衡参数控制数据源贡献。使用异方差主成分分析处理高维异方差噪声，再进行单纯形几何恢复。

Result: 理论分析表明比无协变量模型需要更弱的可识别性条件，并推导了混合隶属度得分和项目参数的有限样本逐项误差界。证明协变量能提高潜在结构恢复，在高维情况下获得更快收敛速度。模拟研究和教育评估数据应用验证了计算效率、统计准确性和可解释性优势。

Conclusion: 协变量辅助隶属度模型提供了一种计算高效、统计稳健的方法来整合多源信息，协变量能显著改进潜在结构恢复，代码开源便于复现。

Abstract: The grade of membership model is a flexible latent variable model for analyzing multivariate categorical data through individual-level mixed membership scores. In many modern applications, auxiliary covariates are collected alongside responses and encode information about the same latent structure. Traditional approaches to incorporating such covariates typically rely on fully specified joint likelihoods, which are computationally intensive and sensitive to misspecification. We introduce a covariate-assisted grade of membership model that integrates response and covariate information by exploiting their shared low-rank simplex geometry, rather than modeling their joint distribution. We propose a likelihood-free spectral estimation procedure that combines heterogeneous data sources through a balance parameter controlling their relative contribution. To accommodate high-dimensional and heteroskedastic noise, we employ heteroskedastic principal component analysis before performing simplex-based geometric recovery. Our theoretical analysis establishes weaker identifiability conditions than those required in the covariate-free model, and further derives finite-sample, entrywise error bounds for both mixed membership scores and item parameters. These results demonstrate that auxiliary covariates can provably improve latent structure recovery, yielding faster convergence rates in high-dimensional regimes. Simulation studies and an application to educational assessment data illustrate the computational efficiency, statistical accuracy, and interpretability gains of the proposed method. The code for reproducing these results is open-source and available at \texttt{https://github.com/Toby-X/Covariate-Assisted-GoM}

</details>


### [25] [Statistical process control via $p$-values](https://arxiv.org/abs/2601.17319)
*Hien Duy Nguyen,Dan Wang*

Main category: stat.ME

TL;DR: 该论文提出了一种基于p值图表的统计过程控制方法，在非参数和两阶段设计中无需过程参数建模，通过p值阈值化和合并函数构建EWMA-like方案，并提供多元SPC的模块化局部化方法。


<details>
  <summary>Details</summary>
Motivation: 传统统计过程控制通常需要参数建模，限制了在非参数和两阶段设计中的应用。作者希望开发一种基于p值的通用框架，能够在无需参数建模的情况下进行过程监控，同时处理时间依赖性和多元情况。

Method: 1) 使用p值图表进行SPC，要求序列在控制状态下满足超均匀性；2) 分析Shewhart规则，推导ARL和k-ARL的通用下界；3) 利用依赖p值的合并函数构建EWMA-like方案，输出每个时间点的有效p值；4) 研究均匀EWMA过程，给出显式分布公式；5) 通过封闭测试提出多元SPC的模块化局部化方法，控制警报时的族错误率。

Result: 在超均匀性条件下，推导出ARL和k-ARL的通用下界；在条件超均匀性下，这些界限锐化为熟悉的α^{-1}和kα^{-1}速率，为p值图表提供简单、分布无关的校准。提出的EWMA-like方案能够平滑监控而无需临时控制限，多元局部化方法能有效控制族错误率。

Conclusion: 该研究建立了一个基于p值的统计过程控制通用框架，适用于非参数和两阶段设计，无需参数建模。提出的方法具有分布无关校准、处理时间依赖性的能力，以及多元情况下的有效局部化，为SPC提供了灵活且理论保证的工具。

Abstract: We study statistical process control (SPC) through charting of $p$-values. When in control (IC), any valid sequence $(P_{t})_{t}$ is super-uniform, a requirement that can hold in nonparametric and two-phase designs without parametric modelling of the monitored process. Within this framework, we analyse the Shewhart rule that signals when $P_{t}\leα$. Under super-uniformity alone, and with no assumptions on temporal dependence, we derive universal IC lower bounds for the average run length (ARL) and for the expected time to the $k$th false alarm ($k$-ARL). When conditional super-uniformity holds, these bounds sharpen to the familiar $α^{-1}$ and $kα^{-1}$ rates, giving simple, distribution-free calibration for $p$-value charts. Beyond thresholding, we use merging functions for dependent $p$-values to build EWMA-like schemes that output, at each time $t$, a valid $p$-value for the hypothesis that the process has remained IC up to $t$, enabling smoothing without ad hoc control limits. We also study uniform EWMA processes, giving explicit distribution formulas and left-tail guarantees. Finally, we propose a modular approach to directional and coordinate localisation in multivariate SPC via closed testing, controlling the family-wise error rate at the time of alarm. Numerical examples illustrate the utility and variety of our approach.

</details>


### [26] [Variational autoencoder for inference of nonlinear mixed effect models based on ordinary differential equations](https://arxiv.org/abs/2601.17400)
*Zhe Li,Mélanie Prague,Rodolphe Thiébaut,Quentin Clairon*

Main category: stat.ME

TL;DR: 提出一种基于变分自编码器（VAE）的方法，用于非线性混合效应常微分方程模型的参数估计，通过共享编码器摊销个体特异性随机效应的推断，避免逐个体优化和MCMC采样。


<details>
  <summary>Details</summary>
Motivation: 传统基于随机近似EM算法（SAEM）的似然推断在中等维度下广泛使用，但依赖MCMC近似个体特异性后验。当模型复杂度增加或个体观测稀疏不规则时，复杂多峰的似然面可能导致MCMC收敛困难，性能下降。

Method: 使用变分自编码器（VAE）最大化证据下界（ELBO），作为边缘似然的正则化替代。VAE通过共享编码器摊销个体特异性随机效应的推断，避免逐个体优化和MCMC使用。除了点估计，还使用基于观测信息的方差估计器量化参数不确定性。

Result: 在三个模拟案例研究（药代动力学、疫苗接种体液反应、哮喘气道TGF-β激活动力学）和一个真实世界抗体动力学数据集上评估方法，并与SAEM基线进行比较。

Conclusion: VAE方法能够有效估计非线性混合效应ODE模型的参数，通过摊销推断避免MCMC收敛问题，同时保持模型参数的可识别性，在复杂模型和稀疏数据场景下表现良好。

Abstract: We propose a variational autoencoder (VAE) approach for parameter estimation in nonlinear mixed-effects models based on ordinary differential equations (NLME-ODEs) using longitudinal data from multiple subjects. In moderate dimensions, likelihood-based inference via the stochastic approximation EM algorithm (SAEM) is widely used, but it relies on Markov Chain Monte-Carlo (MCMC) to approximate subject-specific posteriors. As model complexity increases or observations per subject are sparse and irregular, performance often deteriorates due to a complex, multimodal likelihood surface which may lead to MCMC convergence difficulties. We instead estimate parameters by maximizing the evidence lower bound (ELBO), a regularized surrogate for the marginal likelihood. A VAE with a shared encoder amortizes inference of subject-specific random effects by avoiding per-subject optimization and the use of MCMC. Beyond pointwise estimation, we quantify parameter uncertainty using observed-information-based variance estimator and verify that practical identifiability of the model parameters is not compromised by nuisance parameters introduced in the encoder. We evaluate the method in three simulation case studies (pharmacokinetics, humoral response to vaccination, and TGF-$β$ activation dynamics in asthmatic airways) and on a real-world antibody kinetics dataset, comparing against SAEM baselines.

</details>


### [27] [A Hybrid Latent-Class Item Response Model for Detecting Measurement Non-Invariance in Ordinal Scales](https://arxiv.org/abs/2601.17612)
*Gabriel Wallin,Qi Huang*

Main category: stat.ME

TL;DR: 提出一种无需已知组标签或锚项目的统计框架，用于检测顺序量表（如人格测试）中的差异项目功能（DIF），通过潜在类别模型和L1惩罚实现。


<details>
  <summary>Details</summary>
Motivation: 测量不变性问题（如DIF）会损害组间比较的有效性，但现有方法通常需要已知组标签或锚项目，限制了在组别未知或定义不清情况下的应用。

Method: 提出混合潜在类别项目反应模型，采用比例优势模型处理顺序数据，通过潜在类别捕获DIF（类别特定的项目截距和斜率变化），使用L1惩罚边际似然函数识别DIF效应，并设计专门的EM算法进行模型估计。

Result: 模拟研究显示项目参数和两种DIF类型（均匀和非均匀）的恢复效果良好；在人格测试的实证应用中，发现了具有不同反应模式的潜在子组，并识别出可能导致组比较偏差的项目。

Conclusion: 该框架为评估顺序量表的测量不变性提供了灵活方法，特别适用于比较组未被观察或定义不清的情况，有助于识别测量偏差并提高心理测量的有效性。

Abstract: Measurement non-invariance arises when the psychometric properties of a scale differ across subgroups, undermining the validity of group comparisons. At the item level, such non-invariance manifests as differential item functioning (DIF), which occurs when the conditional distribution of an item response differs across groups after controlling for the latent trait. This paper introduces a statistical framework for detecting DIF in ordinal scales without requiring known group labels or anchor items. We propose a hybrid latent-class item response model to ordinal data using a proportional-odds formulation, assigning individuals probabilistically to latent classes. DIF is captured through class-specific shifts in item intercepts and slopes, allowing for both uniform and non-uniform DIF. The identification of DIF effects is achieved via an $L_1$-penalised marginal likelihood function under a sparsity assumption, and model estimation is implemented using a tailored EM algorithm. Simulation studies demonstrate strong recovery of item parameters and both uniform and non-uniform types of DIF. An empirical application to a personality test reveals latent subgroups with distinct response patterns and identifies items that may bias group comparisons. The proposed framework provides a flexible approach to assessing measurement invariance in ordinal scales when comparison groups are unobserved or poorly defined.

</details>


### [28] [Contrasting Global and Patient-Specific Regression Models via a Neural Network Representation](https://arxiv.org/abs/2601.18658)
*Max Behrens,Daiana Stolz,Eleni Papakonstantinou,Janis M. Nolde,Gabriele Bellerino,Angelika Rohde,Moritz Hess,Harald Binder*

Main category: stat.ME

TL;DR: 提出一种诊断工具，用于对比全局回归模型和患者特异性（局部）回归模型，识别全局模型不适用的患者亚组，并利用自编码器降维处理高维预测变量。


<details>
  <summary>Details</summary>
Motivation: 临床预测模型开发中面临平衡全局模型（适用于所有患者）和个性化模型（针对个体或未知亚组）的挑战。需要工具来识别全局模型在何处、对哪些患者可能不适用。

Method: 提出局部回归方法识别预测变量空间中全局模型不适用的区域。为处理高维预测变量，使用自编码器学习潜在表示，该表示同时优化数据重构和揭示适合稳健局部回归的局部结果相关关联。

Result: 在慢性阻塞性肺疾病临床研究中，全局模型对大多数患者适用，但特定亚组确实受益于个性化模型。能够将这些亚组模型映射回原始预测变量，解释全局模型在这些组中失败的原因。

Conclusion: 该工具的主要应用和诊断价值在于识别和表征其结局关联偏离全局模型的患者或亚组，为临床预测模型的个性化选择提供决策支持。

Abstract: When developing clinical prediction models, it can be challenging to balance between global models that are valid for all patients and personalized models tailored to individuals or potentially unknown subgroups. To aid such decisions, we propose a diagnostic tool for contrasting global regression models and patient-specific (local) regression models. The core utility of this tool is to identify where and for whom a global model may be inadequate. We focus on regression models and specifically suggest a localized regression approach that identifies regions in the predictor space where patients are not well represented by the global model. As localization becomes challenging when dealing with many predictors, we propose modeling in a dimension-reduced latent representation obtained from an autoencoder. Using such a neural network architecture for dimension reduction enables learning a latent representation simultaneously optimized for both good data reconstruction and for revealing local outcome-related associations suitable for robust localized regression. We illustrate the proposed approach with a clinical study involving patients with chronic obstructive pulmonary disease. Our findings indicate that the global model is adequate for most patients but that indeed specific subgroups benefit from personalized models. We also demonstrate how to map these subgroup models back to the original predictors, providing insight into why the global model falls short for these groups. Thus, the principal application and diagnostic yield of our tool is the identification and characterization of patients or subgroups whose outcome associations deviate from the global model.

</details>


### [29] [Two-stage Estimation of Latent Variable Regression Models: A General, Root-N Consistent Solution](https://arxiv.org/abs/2601.17618)
*Yang Liu,Xiaohui Luo,Jieyuan Dong,Youjin Sung,Yueqin Hu,Hongyun Liu,Daniel J. Bauer*

Main category: stat.ME

TL;DR: 提出一个通用的偏差校正框架，用于潜变量模型的两阶段因子分数回归估计，避免传统方法的偏差问题


<details>
  <summary>Details</summary>
Motivation: 潜变量模型在心理学研究中广泛应用，但整体单阶段估计有时困难。两阶段因子分数回归（FSR）是方便的替代方法，但传统FSR会产生结构参数的有偏估计。现有偏差校正方法适用范围有限，需要特定类型的因子分数计算。

Method: 开发了一个通用的偏差校正框架，适用于参数统计模型的两阶段估计，特别针对FSR。该方法适用于更广泛的潜变量模型类别，不要求计算特定类型的因子分数。提出了随机逼近算法进行点估计，以及基于蒙特卡洛的方差估计程序。

Result: 在温和的正则条件下建立了偏差校正两阶段估计量的根n一致性。通过蒙特卡洛实验证明，偏差校正的FSR估计量与"黄金标准"的单阶段最大似然估计量表现相当。

Conclusion: 该方法为估计潜变量模型提供了一个简单而有效的替代方案，具有广泛的适用性，且不依赖于复杂的解析推导。

Abstract: Latent variable (LV) models are widely used in psychological research to investigate relationships among unobservable constructs. When one-stage estimation of the overall LV model is challenging, two-stage factor score regression (FSR) serves as a convenient alternative: the measurement model is fitted to obtain factor scores in the first stage, which are then used to fit the structural model in the subsequent stage. However, naive application of FSR is known to yield biased estimates of structural parameters. In this paper, we develop a generic bias-correction framework for two-stage estimation of parametric statistical models and tailor it specifically to FSR. Unlike existing bias-corrected FSR solutions, the proposed method applies to a broader class of LV models and does not require computing specific types of factor scores. We establish the root-n consistency of the proposed bias-corrected two-stage estimator under mild regularity conditions. To ensure broad applicability and minimize reliance on complex analytical derivations, we introduce a stochastic approximation algorithm for point estimation and a Monte Carlo-based procedure for variance estimation. In a sequence of Monte Carlo experiments, we demonstrate that the bias-corrected FSR estimator performs comparably to the ``gold standard'' one-stage maximum likelihood estimator. These results suggest that our approach offers a straightforward yet effective alternative for estimating LV models.

</details>


### [30] [Non-parametric finite-sample credible intervals with one-dimensional priors: a middle ground between Bayesian and frequentist intervals](https://arxiv.org/abs/2601.17621)
*Tim Ritmeester*

Main category: stat.ME

TL;DR: 提出一种新的统计区间，通过弱化p%可信区间的定义：观察到区间（而非完整数据集）后，我们应至少对其赋予p%的置信度。这种区间在决策理论视角下处于频率学派和完全贝叶斯统计区间之间的中间地带。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯可信区间在观察到完整数据集后仍保持p%的置信度，而频率学派区间通常只在看到数据或区间之前有p%的置信度。作者希望找到一种介于两者之间的统计区间，既具有贝叶斯方法的实践优势，又避免高维先验分配的复杂性。

Method: 通过弱化p%可信区间的定义，提出新的区间构造方法。具体实现了两个案例：1) 估计分布低于某个值的比例（即CDF）；2) 估计有界支撑分布的平均值。这些方法虽然是完全非参数的，但只需要一维先验。

Result: 提出的方法具有贝叶斯方法的许多实践优势，同时完全避免了高维先验分配的复杂性。渐近分析表明，第一种情况下的区间与完全贝叶斯方法等价，第二种情况下的区间则略宽。

Conclusion: 这种新型统计区间在决策理论视角下处于频率学派和贝叶斯方法之间的中间地带，为统计推断提供了有前景的新方向，特别是在需要平衡实践复杂性和理论严谨性的应用中。

Abstract: We propose a new type of statistical interval obtained by weakening the definition of a p% credible interval: Having observed the interval (rather than the full dataset) we should put at least a p% belief in it. From a decision-theoretical point of view the resulting intervals occupy a middle ground between frequentist and fully Bayesian statistical intervals, both practically and philosophically: To a p% Bayesian credible interval we should assign (at least a) p% belief also after seeing the full dataset, while p% frequentist intervals we in general only assign a p% belief before seeing either the data or the interval.
  We derive concrete implementations for two cases: estimation of the fraction of a distribution that falls below a certain value (i.e., the CDF), and of the mean of a distribution with bounded support. Even though the problems are fully non parametric, these methods require only one-dimensional priors. They share many of the practical advantages of Bayesian methods while avoiding the complexity of assigning high-dimensional priors altogether. Asymptotically they give intervals equivalent to the fully Bayesian approach and somewhat wider intervals, respectively. We discuss promising directions where the proposed type of interval may provide significant advantages.

</details>


### [31] [Bidirectional causal inference for binary outcomes in the presence of unmeasured confounding](https://arxiv.org/abs/2601.17695)
*Yafang Deng,Kang Shuai,Shanshan Luo*

Main category: stat.ME

TL;DR: 提出一种结构方程模型方法，用于识别和估计二元变量之间的双向因果效应，解决了现有方法无法处理二元变量的限制。


<details>
  <summary>Details</summary>
Motivation: 在生物医学、计量经济学和社会科学中，变量之间的双向因果关系很常见，但当存在未观测因素且关键变量为二元时，现有方法无法处理。现有基于两个工具变量的方法仅适用于连续变量的线性模型。

Method: 提出结构方程模型方法，通过约束映射将观测的二元变量与连续潜变量联系起来。使用一对工具变量建立双向因果效应的识别结果，并开发相应的因果参数估计方法。还进行了识别条件被违反时的敏感性分析。

Result: 建立了二元变量双向因果效应的识别结果，开发了相应的估计方法，并进行了敏感性分析。将方法应用于研究心脏病和糖尿病之间的双向因果关系，展示了在生物医学研究中的实际应用价值。

Conclusion: 该方法成功解决了二元变量双向因果效应的识别和估计问题，为生物医学等领域的研究提供了实用工具，特别是在处理心脏病和糖尿病等二元健康结局的复杂关系时。

Abstract: Bidirectional causal relationships arising from mutual interactions between variables are commonly observed within biomedical, econometrical, and social science contexts. When such relationships are further complicated by unobserved factors, identifying causal effects in both directions becomes especially challenging. For continuous variables, methods that utilize two instrumental variables from both directions have been proposed to explore bidirectional causal effects in linear models. However, the existing techniques are not applicable when the key variables of interest are binary. To address these issues, we propose a structural equation modeling approach that links observed binary variables to continuous latent variables through a constrained mapping. We further establish identification results for bidirectional causal effects using a pair of instrumental variables. Additionally, we develop an estimation method for the corresponding causal parameters. We also conduct sensitivity analysis under scenarios where certain identification conditions are violated. Finally, we apply our approach to investigate the bidirectional causal relationship between heart disease and diabetes, demonstrating its practical utility in biomedical research.

</details>


### [32] [Group Permutation Testing in Linear Model: Sharp Validity, Power Improvement, and Extension Beyond Exchangeability](https://arxiv.org/abs/2601.17734)
*Zonghan Li,Hongyi Zhou,Zhiheng Zhang*

Main category: stat.ME

TL;DR: 本文提出了一种基于群置换的线性模型回归系数推断框架，统一了置换增强回归测试，在可交换误差下控制I类错误，优化II类错误，并扩展到非可交换情形，提供定量鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对固定设计线性模型中误差项可能存在复杂依赖性或异质性的情况，需要开发有限样本推断方法。现有方法在误差可交换性假设下有效，但缺乏统一的理论框架，且在非可交换误差下缺乏定量鲁棒性保证。

Method: 提出群置换框架，将置换增强回归测试置于群论设置中。开发分组版PALMRT，证明其在任何置换群下控制I类错误；将II类错误与设计相关的几何分离联系起来，并构造优化置换策略的算法；通过连接秩基随机化参数与共形推断，将方法扩展到非可交换情形。

Result: 分组版PALMRT在可交换误差下控制I类错误最多为2α，且因子2是最优的；提出的构造算法在模拟中显示比独立同分布置换有显著功效提升，尤其在重尾设计下；加权群测试在非可交换情形下满足有限样本I类误差界，当误差与置换版本的总变差距离消失时恢复精确有效性。

Conclusion: 群置换视角为从精确随机化有效性到设计自适应功效提供了原则性桥梁，在近似对称性下提供定量鲁棒性，统一了线性模型测试的随机化结构。

Abstract: We consider finite-sample inference for a single regression coefficient in the fixed-design linear model $Y = Zβ+ bX + \varepsilon$, where $\varepsilon\in\mathbb{R}^n$ may exhibit complex dependence or heterogeneity. We develop a group permutation framework, yielding a unified and analyzable randomization structure for linear-model testing. Under exchangeable errors, we place permutation-augmented regression tests within this group-theoretic setting and show that a grouped version of PALMRT controls Type I error at level at most $2α$ for any permutation group; moreover, we provide an worst-case construction demonstrating that the factor $2$ is sharp and cannot be improved without additional assumptions. Second, we relate the Type II error to a design-dependent geometric separation. We formulate it as a combinatorial optimization problem over permutation groups and bound it under additional mild sub-Gaussian assumptions. For the Type II error upper bound control, we propose a constructive algorithm for the permutation strategy that is better (at least no worse) than the i.i.d. permutation, with simulations empirically indicating substantial power gains, especially under heavy-tailed designs. Finally, we extend group-based CPT and PALMRT beyond exchangeability by connecting rank-based randomization arguments to conformal inference. The resulting weighted group tests satisfy finite-sample Type I error bounds that degrade gracefully with a weighted average of total variation distances between $\varepsilon$ and its group-permuted versions, recovering exact validity when these discrepancies vanish and yielding quantitative robustness otherwise. Taken together, the group-permutation viewpoint provides a principled bridge from exact randomization validity to design-adaptive power and quantitative robustness under approximate symmetries.

</details>


### [33] [Sensitivity analysis for incremental effects, with application to a study of victimization & offending](https://arxiv.org/abs/2601.17779)
*Shuying Shen,Valerio Bacak,Edward H. Kennedy*

Main category: stat.ME

TL;DR: 本文针对未测量混杂下的增量倾向得分干预，提出了敏感性分析方法，包括单时间点的稳健估计器和时间变化治疗的边际敏感性模型，并应用于青少年犯罪研究。


<details>
  <summary>Details</summary>
Motivation: 在存在未观测混杂因素的情况下，增量倾向得分干预的敏感性分析相对不发达。增量干预通过乘以治疗几率定义随机治疗机制，为因果效应估计提供了灵活框架。

Method: 1. 单时间点设置：采用Rosenbaum敏感性模型，提出双重稳健估计器用于效应边界估计；2. 时间变化治疗：考虑边际敏感性模型，讨论可识别性问题；3. 应用研究：使用国家青少年到成人健康纵向研究数据，分析受害对后续犯罪的影响。

Result: 1. 边界估计器在温和条件下具有渐近正态性；2. 增量效应边界可能比平均潜在结果边界更窄或更宽；3. 边界必须位于E(Y^0|X)和E(Y^1|X)条件边界的期望最小值和最大值之间；4. 时间变化治疗下，增量效应的尖锐边界可从纵向数据中识别，但实际估计器尚未建立。

Conclusion: 本文发展了未测量混杂下增量倾向得分干预的敏感性分析方法，提供了理论框架和实用估计器，并通过实证应用展示了方法的稳健性，为时间变化治疗的敏感性分析提供了部分进展。

Abstract: Sensitivity analysis for unmeasured confounding under incremental propensity score interventions remains relatively underdeveloped. Incremental interventions define stochastic treatment regimes by multiplying the odds of treatment, offering a flexible framework for causal effect estimation. To study incremental effects when there are unobserved confounders, we adopt Rosenbaum's sensitivity model in single time point settings, and propose a doubly robust estimator for the resulting effect bounds. The bound estimators are asymptotically normal under mild conditions on nuisance function estimation. We show that incremental effect bounds can be narrower or wider than those for mean potential outcomes, and that the bounds must lie between the expected minimum and maximum of the conditional bounds on E(Y^0|X) and E(Y^1|X). For time-varying treatments, we consider the marginal sensitivity model. Although sharp bounds for incremental effects are identifiable from longitudinal data under this model, practical estimators have not yet been established; we discuss this challenge and provide partial results toward implementation. Finally, we apply our methods to study the effect of victimization on subsequent offending using data from the National Longitudinal Study of Adolescent to Adult Health (Add Health), illustrating the robustness of our findings in an empirical setting.

</details>


### [34] [Transportability of Regression Calibration with External Validation Studies for Measurement Error Correction](https://arxiv.org/abs/2601.17961)
*Zexiang Li,Donna Spiegelman,Molin Wang,Zuoheng Wang,Xin Zhou*

Main category: stat.ME

TL;DR: 该研究探讨了在营养和环境流行病学中，当使用外部验证研究进行回归校准以校正测量误差时，估计量的无偏性条件和可迁移性假设。


<details>
  <summary>Details</summary>
Motivation: 在营养和环境流行病学中，准确测量暴露因素通常不切实际，而实际测量方法往往存在显著测量误差。回归校准是最常用的测量误差校正方法之一，但使用外部研究评估测量误差过程可能引入估计偏差。虽然实践中通常假设回归校准具有可迁移性，但这一假设尚未得到充分研究。

Method: 在混合了Berkson-like和classical-like误差的测量误差过程中，研究回归校准估计量在何种条件下对暴露与健康结果之间的关联是无偏的。进一步检验可迁移性假设的偏离情况，理论上证明在大多数情况下，回归校准估计量比朴素方法具有更低的偏差。

Result: 推导出了回归校准估计量无偏的条件，并通过模拟研究验证了这些条件。在大多数情况下，即使可迁移性假设不完全成立，回归校准估计量也比朴素方法具有更低的偏差。这些发现在一个研究心血管疾病风险与适度体力活动关联的健康专业人员随访研究实例中得到了进一步验证。

Conclusion: 该研究为在营养和环境流行病学中使用外部验证研究进行回归校准提供了理论依据，明确了估计量无偏的条件，并证明了在可迁移性假设不完全满足时，回归校准通常仍优于朴素方法，为实际应用提供了指导。

Abstract: In nutritional and environmental epidemiology, exposures are impractical to measure accurately, while practical measures for these exposures are often subject to substantial measurement error. Regression calibration is among the most used measurement error correction methods with external validation studies. The use of external studies to assess the measurement error process always carries the risk of introducing estimation bias into the main study analysis. Although the transportability of regression calibration is usually assumed for practical epidemiology studies, it has not been well studied. In this work, under the measurement error process with a mixture of Berkson-like and classical-like errors, we investigate conditions under which the effect estimate from regression calibration with an external validation study is unbiased for the association between exposure and health outcome. We further examine departures from the transportability assumption, under which the regression calibration estimator is itself biased. However, we theoretically prove that, in most cases, it yields lower bias than the naive method. The derived conditions are confirmed through simulation studies and further verified in an example investigating the association between the risk of cardiovascular disease and moderate physical activity in the health professional follow-up study.

</details>


### [35] [Examining the Efficacy of Coarsen Exact Matching as an Alternative to Propensity Score Matching](https://arxiv.org/abs/2601.18013)
*Fei Wan*

Main category: stat.ME

TL;DR: 本文质疑CEM优于PSM的说法，指出CEM存在残差混杂、模型依赖、维度诅咒等问题，而PSM在减少不平衡、处理多维协变量和模型稳健性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 针对学界普遍认为CEM优于PSM的观点提出质疑，澄清CEM的实际局限性和PSM的优势，为研究者提供更准确的方法选择依据。

Method: 通过理论分析和模拟研究，比较CEM与PSM在减少不平衡、处理残差混杂、模型依赖性和高维协变量等方面的表现，使用多变量标准化均值差等指标进行评估。

Result: PSM在减少不平衡方面优于CEM；CEM存在残差混杂问题，需要准确建模才能减少偏倚；CEM面临维度诅咒问题，协变量增多时数据损失严重；PSM对模型误设更稳健。

Conclusion: CEM并非PSM的可行替代方案，特别是在匹配大量协变量时。PSM在减少不平衡、处理高维数据和模型稳健性方面表现更优，研究者应根据实际情况选择适当方法。

Abstract: Coarsened exact matching (CEM) is often promoted as a superior alternative to propensity score matching (PSM) for addressing imbalance, model dependence, bias, and efficiency. However, this recommendation remains uncertain. First, CEM is commonly mischaracterized as exact matching, despite relying on coarsened rather than original variables. This inexactness in matching introduces residual confounding, which necessitates accurate modeling of the outcome-confounder relationship post-matching to mitigate bias, thereby increasing vulnerability to model misspecification. Second, prior studies overlook that any imbalance between treated and untreated subjects matched on the same propensity score is attributable to random variation. Thus, claims that CEM outperforms PSM in reducing imbalance are unfounded, particularly when using metrics like Mahalanobis distance, which do not account for chance imbalance in PSM. Our simulations show that PSM reduces imbalance more effectively than CEM when evaluated with multivariate standardized mean differences (SMD), and unadjusted analyses indicate greater bias with CEM. While adjusted analyses in both CEM with autocoarsening and PSM may perform similarly when matching on few variables, CEM suffers from the curse of dimensionality as the number of factors increases, resulting in substantial data loss and unstable estimates. Increasing the level of coarsening may mitigate data loss but exacerbates residual confounding and model dependence. In contrast, both analytical results and simulations demonstrate that PSM is more robust to model misspecification and thus less model-dependent. Therefore, CEM is not a viable alternative to PSM when matching on a large number of covariates.

</details>


### [36] [BASTION: A Bayesian Framework for Trend and Seasonality Decomposition](https://arxiv.org/abs/2601.18052)
*Jason B. Cho,David S. Matteson*

Main category: stat.ME

TL;DR: BASTION是一个贝叶斯自适应季节性和趋势分解框架，通过惩罚非参数回归将时间序列分解为趋势和多个季节性成分，提供唯一可识别性、对突变变化的准确估计、对异常值和时变波动性的鲁棒性，以及可靠的区间量化。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分解方法在处理趋势和季节性成分的唯一可识别性方面缺乏形式化处理，且难以应对突变变化、异常值和时变波动性等挑战。

Method: 将分解问题转化为惩罚非参数回归，建立形式化条件确保趋势和季节性成分的唯一可识别性，采用贝叶斯自适应框架处理复杂动态。

Result: 在模拟和真实数据集上评估表明，BASTION相比TBATS、STR、MSTL等方法能更有效地捕捉复杂动态，处理异常值和异方差性，提供更细致和可解释的分解结果。

Conclusion: BASTION提供了一个灵活且理论严谨的时间序列分解框架，在准确估计、鲁棒性和不确定性量化方面优于现有方法，已作为R包开源供研究和应用使用。

Abstract: We introduce BASTION (Bayesian Adaptive Seasonality and Trend DecompositION), a flexible Bayesian framework for decomposing time series into trend and multiple seasonality components. We cast the decomposition as a penalized nonparametric regression and establish formal conditions under which the trend and seasonal components are uniquely identifiable, an issue only treated informally in the existing literature. BASTION offers three key advantages over existing decomposition methods: (1) accurate estimation of trend and seasonality amidst abrupt changes, (2) enhanced robustness against outliers and time-varying volatility, and (3) robust uncertainty quantification. We evaluate BASTION against established methods, including TBATS, STR, and MSTL, using both simulated and real-world datasets. By effectively capturing complex dynamics while accounting for irregular components such as outliers and heteroskedasticity, BASTION delivers a more nuanced and interpretable decomposition. To support further research and practical applications, BASTION is available as an R package at https://github.com/Jasoncho0914/BASTION

</details>


### [37] [Maximum-Variance-Reduction Stratification for Improved Subsampling](https://arxiv.org/abs/2601.18075)
*Dingyi Wang,Haiying Wang,Qingpei Hu*

Main category: stat.ME

TL;DR: 提出了一种新的分层机制MVRS，可与现有子抽样方法结合，通过选择分层变量和区间边界来减少渐近方差，显著提高估计效率，计算成本仅线性增加。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集带来计算挑战，子抽样是有效方法。现有非均匀概率抽样方案已取得进展，但需要进一步提高估计效率。

Method: 提出最大方差减少分层(MVRS)机制，可与现有子抽样设计结合。建立估计量的渐近正态性，量化效率增益，提供选择分层变量和区间边界的原则性程序，以降低渐近方差。

Result: MVRS在仅增加线性计算成本的情况下，显著提高估计效率。适用于非均匀和均匀子抽样方法。模拟和真实数据集实验表明，相比现有方法，MVRS显著降低估计方差并提高准确性。

Conclusion: MVRS是一种有效的分层机制，可与现有子抽样方法结合，在计算成本有限增加的情况下，显著提高估计效率，为大规模数据分析提供更优解决方案。

Abstract: Subsampling is a widely used and effective approach for addressing the computational challenges posed by massive datasets. Substantial progress has been made in developing non-uniform, probability-based subsampling schemes that prioritize more informative observations. We propose a novel stratification mechanism that can be combined with existing subsampling designs to further improve estimation efficiency. We establish the estimator's asymptotic normality and quantify the resulting efficiency gains, which enables a principled procedure for selecting stratification variables and interval boundaries that target reductions in asymptotic variance. The resulting algorithm, Maximum-Variance-Reduction Stratification (MVRS), achieves significant improvements in estimation efficiency while incurring only linear additional computational cost. MVRS is applicable to both non-uniform and uniform subsampling methods. Experiments on simulated and real datasets confirm that MVRS markedly reduces estimator variance and improves accuracy compared with existing subsampling methods.

</details>


### [38] [Preference-based Centrality and Ranking in General Metric Spaces](https://arxiv.org/abs/2601.18412)
*Lingfeng Lyu,Doudou Zhou*

Main category: stat.ME

TL;DR: 提出基于偏好比较的中心性度量框架，将数据深度推广到任意度量空间，通过Bradley-Terry-Luce投影获得一维表示，开发了两种有限样本估计器


<details>
  <summary>Details</summary>
Motivation: 在多元或非欧几里得空间中评估中心性或排序观测值具有挑战性，因为这类数据缺乏内在顺序，且许多经典深度概念在高维或结构化设置中会失去分辨率

Method: 提出基于偏好的框架，通过总体成对邻近比较定义中心性：如果一个点比另一个点更接近典型分布样本，则该点更中心。研究Bradley-Terry-Luce投影获得一维表示，开发基于凸M估计和谱聚合的两种有限样本估计器

Result: 得到的程序具有一致性、可扩展性，适用于高维和非欧几里得数据，在一系列示例中表现出稳定的排序行为和相对于经典深度方法的改进分辨率

Conclusion: 该偏好框架为度量空间中的中心性评估提供了通用方法，克服了传统深度方法在高维和非欧几里得设置中的局限性

Abstract: Assessing centrality or ranking observations in multivariate or non-Euclidean spaces is challenging because such data lack an intrinsic order and many classical depth notions lose resolution in high-dimensional or structured settings. We propose a preference-based framework that defines centrality through population pairwise proximity comparisons: a point is central if a typical draw from the underlying distribution tends to lie closer to it than to another. This perspective yields a well-defined statistical functional that generalizes data depth to arbitrary metric spaces. To obtain a coherent one-dimensional representation, we study a Bradley-Terry-Luce projection of the induced preferences and develop two finite-sample estimators based on convex M-estimation and spectral aggregation. The resulting procedures are consistent, scalable, and applicable to high-dimensional and non-Euclidean data, and across a range of examples they exhibit stable ranking behavior and improved resolution relative to classical depth-based methods.

</details>


### [39] [Learned harmonic mean estimation of the marginal likelihood for multimodal posteriors with flow matching](https://arxiv.org/abs/2601.18683)
*Alicja Polanska,Jason D. McEwen*

Main category: stat.ME

TL;DR: 提出使用流匹配连续归一化流作为学习调和平均估计器的内部密度估计架构，以处理高度多峰后验分布，提升边缘似然计算的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 学习调和平均估计器虽然能准确估计边缘似然，但先前考虑的内部密度估计器在处理高度多峰后验分布时存在困难，需要更强大的架构来解决这一挑战。

Method: 引入基于流匹配的连续归一化流作为学习调和平均估计器的内部密度估计架构，该架构能够处理复杂的多峰后验分布，无需对基分布进行微调或启发式修改。

Result: 该方法能够处理具有挑战性的多峰后验分布，包括20个参数维度的示例，展示了处理复杂后验分布的能力。

Conclusion: 流匹配连续归一化流为学习调和平均估计器提供了强大的内部密度估计架构，能够有效处理高度多峰后验分布，提升边缘似然计算的适用性和准确性。

Abstract: The marginal likelihood, or Bayesian evidence, is a crucial quantity for Bayesian model comparison but its computation can be challenging for complex models, even in parameters space of moderate dimension. The learned harmonic mean estimator has been shown to provide accurate and robust estimates of the marginal likelihood simply using posterior samples. It is agnostic to the sampling strategy, meaning that the samples can be obtained using any method. This enables marginal likelihood calculation and model comparison with whatever sampling is most suitable for the task. However, the internal density estimators considered previously for the learned harmonic mean can struggle with highly multimodal posteriors. In this work we introduce flow matching-based continuous normalizing flows as a powerful architecture for the internal density estimation of the learned harmonic mean. We demonstrate the ability to handle challenging multimodal posteriors, including an example in 20 parameter dimensions, showcasing the method's ability to handle complex posteriors without the need for fine-tuning or heuristic modifications to the base distribution.

</details>
