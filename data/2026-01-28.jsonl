{"id": "2601.17985", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.17985", "abs": "https://arxiv.org/abs/2601.17985", "authors": ["Soumya Sahu", "Kwan Hur", "Dulal K. Bhaumik", "Robert Gibbons"], "title": "Bayesian Multiple Testing for Suicide Risk in Pharmacoepidemiology: Leveraging Co-Prescription Patterns", "comment": null, "summary": "Suicide is the tenth leading cause of death in the United States, yet evidence on medication-related risk or protection remains limited. Most post-marketing studies examine one drug class at a time or rely on empirical-Bayes shrinkage with conservative multiplicity corrections, sacrificing power to detect clinically meaningful signals. We introduce a unified Bayesian spike-and-slab framework that advances both applied suicide research and statistical methodology. Substantively, we screen 922 prescription drugs across 150 million patients in U.S. commercial claims (2003 to 2014), leveraging real-world co-prescription patterns to inform a covariance prior that adaptively borrows strength across pharmacologically related agents. Statistically, the model couples this structured prior with Bayesian false-discovery-rate control, illustrating how network-guided variable selection can improve rare-event surveillance in high dimensions. Relative to the seminal empirical-Bayes analysis of Gibbons et al. (2019), our approach reconfirms the key harmful (e.g., alprazolam, hydrocodone) and protective (e.g., mirtazapine, folic acid) signals while revealing additional associations, such as a high-risk opioid combination and several folate-linked agents with potential preventive benefit that had been overlooked. A focused re-analysis of 18 antidepressants shows how alternative co-prescription metrics modulate effect estimates, shedding light on competitive versus complementary prescribing. These findings generate actionable hypotheses for clinicians and regulators and showcase the value of structured Bayesian modeling in pharmacovigilance."}
{"id": "2601.18480", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.18480", "abs": "https://arxiv.org/abs/2601.18480", "authors": ["Ali Abboud", "Josselin Garnier", "Bertrand Leturcq", "Stanislas de Lambert"], "title": "Uncertainty Quantification in Coupled Multiphysics Systems via Gaussian Process Surrogates: Application to Fuel Assembly Bow", "comment": null, "summary": "Predicting fuel assembly bow in pressurized water reactors requires solving tightly coupled fluid-structure interaction problems, whose direct simulations can be computationally prohibitive, making large-scale uncertainty quantification (UQ) very challenging. This work introduces a general mathematical framework for coupling Gaussian process (GP) surrogate models representing distinct physical solvers, aimed at enabling rigorous UQ in coupled multiphysics systems. A theoretical analysis establishes that the predictive variance of the coupled GP system remains bounded under mild regularity and stability assumptions, ensuring that uncertainty does not grow uncontrollably through the iterative coupling process. The methodology is then applied to the coupled hydraulic-structural simulation of fuel assembly bow, enabling global sensitivity analysis and full UQ at a fraction of the computational cost of direct code coupling. The results demonstrate accurate uncertainty propagation and stable predictions, establishing a solid mathematical basis for surrogate-based coupling in large-scale multiphysics simulations."}
{"id": "2601.18656", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.18656", "abs": "https://arxiv.org/abs/2601.18656", "authors": ["Sarika Aggarwal", "Phillip B. Nicol", "Brent A. Coull", "Rachel C. Nethery"], "title": "A varying-coefficient model for characterizing duration-driven heterogeneity in flood-related health impacts", "comment": null, "summary": "Previous work revealed associations between flood exposure and adverse health outcomes during and in the aftermath of flood events. Floods are highly heterogeneous events, largely owing to vast differences in flood durations, i.e., flash-floods versus slow-moving floods. However, little to no work has incorporated exposure duration into the modeling of flood-related health impacts or has investigated duration-driven effect heterogeneity. To address this gap, we propose an exposure duration varying coefficient modeling (EDVCM) framework for estimating exposure day-specific health effects of consecutive-day environmental exposures that vary in duration. We develop the EDVCM within an area-level self-matched study design to eliminate time-invariant confounding followed by conditional Poisson regression modeling for exposure effect estimation and adjustment of time-varying confounders. Using a Bayesian framework, we introduce duration- and exposure day-specific exposure coefficients within the conditional Poisson model and assign them a two-dimensional Gaussian process prior to allow for sharing of information across both duration and exposure day. This approach enables highly-resolved insights into duration-driven effect heterogeneity while ensuring model stability through information sharing. Through simulations, we demonstrate that the EDVCM out-performs conventional approaches in terms of both effect estimation and uncertainty quantification. We apply the EDVCM to nationwide, multi-decade Medicare claims data linked with high-resolution flood exposure measures to investigate duration-driven heterogeneity in flood effects on musculoskeletal system disease hospitalizations."}
{"id": "2601.17153", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.17153", "abs": "https://arxiv.org/abs/2601.17153", "authors": ["Ian Laga", "Benjamin Vogel", "Jieyun Wang", "Anna Smith", "Owen Ward"], "title": "Evaluating Aggregated Relational Data Models with Simple Diagnostics", "comment": null, "summary": "Aggregated Relational Data (ARD) contain summary information about individual social networks and are widely used to estimate social network characteristics and the size of populations of interest. Although a variety of ARD estimators exist, practitioners currently lack guidance on how to evaluate whether a selected model adequately fits the data. We introduce a diagnostic framework for ARD models that provides a systematic, reproducible process for assessing covariate structure, distributional assumptions, and correlation. The diagnostics are based on point estimates, using either maximum likelihood or maximum a posteriori optimization, which allows quick evaluation without requiring repeated Bayesian model fitting. Through simulation studies and applications to large ARD datasets, we show that the proposed workflow identifies common sources of model misfit and helps researchers select an appropriate model that adequately explains the data."}
{"id": "2601.18593", "categories": ["stat.CO", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.18593", "abs": "https://arxiv.org/abs/2601.18593", "authors": ["Felix Ballani"], "title": "The generalised balanced power diagram: flat sections, affine transformations and an improved rendering algorithm", "comment": "10 pages, 2 figures", "summary": "The generalised balanced power diagram (GBPD) is regarded in the literature as a suitable geometric model for describing polycrystalline microstructures with curved grain boundaries. This article compiles properties of GBPDs with regard to affine transformations and flat sections. Furthermore, it extends an algorithm known for power diagrams for generating digital images, which is more efficient than the usual brute force approach, on GBPDs."}
{"id": "2601.17141", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17141", "abs": "https://arxiv.org/abs/2601.17141", "authors": ["Yu Gu", "Yangjianchen Xu", "Peijun Sang"], "title": "Varying coefficient model for longitudinal data with informative observation times", "comment": null, "summary": "Varying coefficient models are widely used to characterize dynamic associations between longitudinal outcomes and covariates. Existing work on varying coefficient models, however, all assumes that observation times are independent of the longitudinal outcomes, which is often violated in real-world studies with outcome-driven or otherwise informative visit schedules. Such informative observation times can lead to biased estimation and invalid inference using existing methods. In this article, we develop estimation and inference procedures for varying coefficient models that account for informative observation times. We model the observation time process as a general counting process under a proportional intensity model, with time-varying covariates summarizing the observed history. To address potential bias, we incorporate inverse intensity weighting into a sieve estimation framework, yielding closed-form coefficient function estimators via weighted least squares. We establish consistency, convergence rates, and asymptotic normality of the proposed estimators, and construct pointwise confidence intervals for the coefficient functions. Extensive simulation studies demonstrate that the proposed weighted method substantially outperforms the conventional unweighted method when observation times are informative. Finally, we provide an application of our method to the Alzheimer's Disease Neuroimaging Initiative study."}
{"id": "2601.17145", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.17145", "abs": "https://arxiv.org/abs/2601.17145", "authors": ["Vydhourie Thiyageswaran", "Alex Kokot", "Jennifer Brennan", "Marina Meila", "Christina Lee Yu", "Maryam Fazel"], "title": "Optimal Design under Interference, Homophily, and Robustness Trade-offs", "comment": null, "summary": "To minimize the mean squared error (MSE) in global average treatment effect (GATE) estimation under network interference, a popular approach is to use a cluster-randomized design. However, in the presence of homophily, which is common in social networks, cluster randomization can instead increase the MSE. We develop a novel potential outcomes model that accounts for interference, homophily, and heterogeneous variation. In this setting, we establish a framework for optimizing designs for worst-case MSE under the Horvitz-Thompson estimator. This leads to an optimization problem over the covariance matrices of the treatment assignment, trading off interference, homophily, and robustness. We frame and solve this problem using two complementary approaches. The first involves formulating a semidefinite program (SDP) and employing Gaussian rounding, in the spirit of the Goemans-Williamson approximation algorithm for MAXCUT. The second is an adaptation of the Gram-Schmidt Walk, a vector-balancing algorithm which has recently received much attention. Finally, we evaluate the performance of our designs through various experiments on simulated network data and a real village network dataset."}
{"id": "2601.17160", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17160", "abs": "https://arxiv.org/abs/2601.17160", "authors": ["Yonghan Jung", "Bogyeong Kang"], "title": "Data-Driven Information-Theoretic Causal Bounds under Unmeasured Confounding", "comment": null, "summary": "We develop a data-driven information-theoretic framework for sharp partial identification of causal effects under unmeasured confounding. Existing approaches often rely on restrictive assumptions, such as bounded or discrete outcomes; require external inputs (for example, instrumental variables, proxies, or user-specified sensitivity parameters); necessitate full structural causal model specifications; or focus solely on population-level averages while neglecting covariate-conditional treatment effects. We overcome all four limitations simultaneously by establishing novel information-theoretic, data-driven divergence bounds. Our key theoretical contribution shows that the f-divergence between the observational distribution P(Y | A = a, X = x) and the interventional distribution P(Y | do(A = a), X = x) is upper bounded by a function of the propensity score alone. This result enables sharp partial identification of conditional causal effects directly from observational data, without requiring external sensitivity parameters, auxiliary variables, full structural specifications, or outcome boundedness assumptions. For practical implementation, we develop a semiparametric estimator satisfying Neyman orthogonality (Chernozhukov et al., 2018), which ensures square-root-n consistent inference even when nuisance functions are estimated using flexible machine learning methods. Simulation studies and real-world data applications, implemented in the GitHub repository (https://github.com/yonghanjung/Information-Theretic-Bounds), demonstrate that our framework provides tight and valid causal bounds across a wide range of data-generating processes."}
{"id": "2601.17233", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.17233", "abs": "https://arxiv.org/abs/2601.17233", "authors": ["Jiren Sun", "Linda Amoafo", "Yongming Qu"], "title": "An Empirical Method for Analyzing Count Data", "comment": null, "summary": "Count endpoints are common in clinical trials, particularly for recurrent events such as hypoglycemia. When interest centers on comparing overall event rates between treatment groups, negative binomial (NB) regression is widely used because it accommodates overdispersion and requires only event counts and exposure times. However, NB regression can be numerically unstable when events are sparse, and the efficiency gains from baseline covariate adjustment may be sensitive to model misspecification. We propose an empirical method that targets the same marginal estimand as NB regression -- the ratio of marginal event rates -- while avoiding distributional assumptions on the count outcome. Simulation studies show that the empirical method maintains appropriate Type I error control across diverse scenarios, including extreme overdispersion and zero inflation, achieves power comparable to NB regression, and yields consistent efficiency gains from baseline covariate adjustment. We illustrate the approach using severe hypoglycemia data from the QWINT-5 trial comparing insulin efsitora alfa with insulin degludec in adults with type 1 diabetes. In this sparse-event setting, the empirical method produced stable marginal rate estimates and rate ratios closely aligned with observed rates, while NB regression exhibited greater sensitivity and larger deviations from the observed rates in the sparsest intervals. The proposed empirical method provides a robust and numerically stable alternative to NB regression, particularly when the number of events is low or when numerical stability is a concern."}
{"id": "2601.18072", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.18072", "abs": "https://arxiv.org/abs/2601.18072", "authors": ["Stephanie CC van der Lubbe", "Jose M Valderas", "Evangelos Kontopantelis"], "title": "The effect of collinearity and sample size on linear regression results: a simulation study", "comment": "17 pages, 5 figures", "summary": "Background: Multicollinearity inflates the variance of OLS coefficients, widening confidence intervals and reducing inferential reliability. Yet fixed variance inflation factor (VIF) cut-offs are often applied uniformly across studies with very different sample sizes, even though collinearity is a finite-sample problem. We quantify how collinearity and sample size jointly affect linear regression performance and provide practical guidance for interpreting VIFs.\n  Methods: We simulated data across sample sizes N=100-100,000 and collinearity levels VIF=1-50. For each scenario we generated 1,000 datasets, fitted OLS models, and assessed coverage, mean absolute error (MAE), bias, traditional power (CI excludes 0), and precision assurance (probability the 95% CI lies within a prespecified margin around the true effect). We also evaluated a biased, misspecified setting by omitting a relevant predictor to study bias amplification.\n  Results: Under correct specification, collinearity did not materially affect nominal coverage and did not introduce systematic bias, but it reduced precision in small samples: at N=100, even mild collinearity (VIF<2) inflated MAE and markedly reduced both power metrics, whereas at N>=50,000 estimates were robust even at VIF=50. Under misspecification, collinearity strongly amplified bias, increasing errors, reducing coverage, and sharply degrading both precision assurance and traditional power even at low VIF.\n  Conclusion: VIF thresholds should not be applied mechanically. Collinearity must be interpreted in relation to sample size and potential sources of bias; removing predictors solely to reduce VIF can worsen inference via omitted-variable bias. The accompanying heatmaps provide a practical reference across study sizes and modelling assumptions."}
{"id": "2601.17145", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.17145", "abs": "https://arxiv.org/abs/2601.17145", "authors": ["Vydhourie Thiyageswaran", "Alex Kokot", "Jennifer Brennan", "Marina Meila", "Christina Lee Yu", "Maryam Fazel"], "title": "Optimal Design under Interference, Homophily, and Robustness Trade-offs", "comment": null, "summary": "To minimize the mean squared error (MSE) in global average treatment effect (GATE) estimation under network interference, a popular approach is to use a cluster-randomized design. However, in the presence of homophily, which is common in social networks, cluster randomization can instead increase the MSE. We develop a novel potential outcomes model that accounts for interference, homophily, and heterogeneous variation. In this setting, we establish a framework for optimizing designs for worst-case MSE under the Horvitz-Thompson estimator. This leads to an optimization problem over the covariance matrices of the treatment assignment, trading off interference, homophily, and robustness. We frame and solve this problem using two complementary approaches. The first involves formulating a semidefinite program (SDP) and employing Gaussian rounding, in the spirit of the Goemans-Williamson approximation algorithm for MAXCUT. The second is an adaptation of the Gram-Schmidt Walk, a vector-balancing algorithm which has recently received much attention. Finally, we evaluate the performance of our designs through various experiments on simulated network data and a real village network dataset."}
{"id": "2601.17374", "categories": ["stat.ML", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.17374", "abs": "https://arxiv.org/abs/2601.17374", "authors": ["Bamdad Hosseini", "Ziqi Huang"], "title": "Error Analysis of Bayesian Inverse Problems with Generative Priors", "comment": "30 pages, 8 figures", "summary": "Data-driven methods for the solution of inverse problems have become widely popular in recent years thanks to the rise of machine learning techniques. A popular approach concerns the training of a generative model on additional data to learn a bespoke prior for the problem at hand. In this article we present an analysis for such problems by presenting quantitative error bounds for minimum Wasserstein-2 generative models for the prior. We show that under some assumptions, the error in the posterior due to the generative prior will inherit the same rate as the prior with respect to the Wasserstein-1 distance. We further present numerical experiments that verify that aspects of our error analysis manifests in some benchmarks followed by an elliptic PDE inverse problem where a generative prior is used to model a non-stationary field."}
{"id": "2601.18072", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.18072", "abs": "https://arxiv.org/abs/2601.18072", "authors": ["Stephanie CC van der Lubbe", "Jose M Valderas", "Evangelos Kontopantelis"], "title": "The effect of collinearity and sample size on linear regression results: a simulation study", "comment": "17 pages, 5 figures", "summary": "Background: Multicollinearity inflates the variance of OLS coefficients, widening confidence intervals and reducing inferential reliability. Yet fixed variance inflation factor (VIF) cut-offs are often applied uniformly across studies with very different sample sizes, even though collinearity is a finite-sample problem. We quantify how collinearity and sample size jointly affect linear regression performance and provide practical guidance for interpreting VIFs.\n  Methods: We simulated data across sample sizes N=100-100,000 and collinearity levels VIF=1-50. For each scenario we generated 1,000 datasets, fitted OLS models, and assessed coverage, mean absolute error (MAE), bias, traditional power (CI excludes 0), and precision assurance (probability the 95% CI lies within a prespecified margin around the true effect). We also evaluated a biased, misspecified setting by omitting a relevant predictor to study bias amplification.\n  Results: Under correct specification, collinearity did not materially affect nominal coverage and did not introduce systematic bias, but it reduced precision in small samples: at N=100, even mild collinearity (VIF<2) inflated MAE and markedly reduced both power metrics, whereas at N>=50,000 estimates were robust even at VIF=50. Under misspecification, collinearity strongly amplified bias, increasing errors, reducing coverage, and sharply degrading both precision assurance and traditional power even at low VIF.\n  Conclusion: VIF thresholds should not be applied mechanically. Collinearity must be interpreted in relation to sample size and potential sources of bias; removing predictors solely to reduce VIF can worsen inference via omitted-variable bias. The accompanying heatmaps provide a practical reference across study sizes and modelling assumptions."}
{"id": "2601.18145", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.18145", "abs": "https://arxiv.org/abs/2601.18145", "authors": ["Heguang Lin", "Binhao Chen", "Mengze Li", "Daniel Pimentel-Alarcón", "Matthew L. Malloy"], "title": "Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes", "comment": "15 pages, 1 figure", "summary": "Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing."}
{"id": "2601.17146", "categories": ["stat.ME", "cs.CY", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17146", "abs": "https://arxiv.org/abs/2601.17146", "authors": ["Amanda Coston"], "title": "Falsifying Predictive Algorithm", "comment": null, "summary": "Empirical investigations into unintended model behavior often show that the algorithm is predicting another outcome than what was intended. These exposes highlight the need to identify when algorithms predict unintended quantities - ideally before deploying them into consequential settings. We propose a falsification framework that provides a principled statistical test for discriminant validity: the requirement that an algorithm predict intended outcomes better than impermissible ones. Drawing on falsification practices from causal inference, econometrics, and psychometrics, our framework compares calibrated prediction losses across outcomes to assess whether the algorithm exhibits discriminant validity with respect to a specified impermissible proxy. In settings where the target outcome is difficult to observe, multiple permissible proxy outcomes may be available; our framework accommodates both this setting and the case with a single permissible proxy. Throughout we use nonparametric hypothesis testing methods that make minimal assumptions on the data-generating process. We illustrate the method in an admissions setting, where the framework establishes discriminant validity with respect to gender but fails to establish discriminant validity with respect to race. This demonstrates how falsification can serve as an early validity check, prior to fairness or robustness analyses. We also provide analysis in a criminal justice setting, where we highlight the limitations of our framework and emphasize the need for complementary approaches to assess other aspects of construct validity and external validity."}
{"id": "2601.17510", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17510", "abs": "https://arxiv.org/abs/2601.17510", "authors": ["David L. Donoho", "Jian Kang", "Xihong Lin", "Bhramar Mukherjee", "Dan Nettleton", "Rebecca Nugent", "Abel Rodriguez", "Eric P. Xing", "Tian Zheng", "Hongtu Zhu"], "title": "\"Rebuilding\" Statistics in the Age of AI: A Town Hall Discussion on Culture, Infrastructure, and Training", "comment": "35 pages, 3 figures,", "summary": "This article presents the full, original record of the 2024 Joint Statistical Meetings (JSM) town hall, \"Statistics in the Age of AI,\" which convened leading statisticians to discuss how the field is evolving in response to advances in artificial intelligence, foundation models, large-scale empirical modeling, and data-intensive infrastructures. The town hall was structured around open panel discussion and extensive audience Q&A, with the aim of eliciting candid, experience-driven perspectives rather than formal presentations or prepared statements. This document preserves the extended exchanges among panelists and audience members, with minimal editorial intervention, and organizes the conversation around five recurring questions concerning disciplinary culture and practices, data curation and \"data work,\" engagement with modern empirical modeling, training for large-scale AI applications, and partnerships with key AI stakeholders. By providing an archival record of this discussion, the preprint aims to support transparency, community reflection, and ongoing dialogue about the evolving role of statistics in the data- and AI-centric future."}
{"id": "2601.18587", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.18587", "abs": "https://arxiv.org/abs/2601.18587", "authors": ["Michael P. Fay", "Dean Follmann", "Bruce J. Swihart", "Lauren E. Dang"], "title": "Vaccine Efficacy Estimands Implied by Common Estimators Used in Individual Randomized Field Trials", "comment": "28 pages, 6 figures", "summary": "We review vaccine efficacy (VE) estimands for susceptibility in individual randomized trials with natural (unmeasured) exposure, where individual responses are measured as time from vaccination until an event (e.g., disease from the infectious agent). Common VE estimands are written as $1-θ$, where $θ$ is some ratio effect measure (e.g., ratio of incidence rates, cumulative incidences, hazards, or odds) comparing outcomes under vaccination versus control. Although the ratio effects are approximately equal with low control event rates, we explore the quality of that approximation using a nonparametric formulation. Traditionally, the primary endpoint VE estimands are full immunization (or biological) estimands that represent a subset of the intent-to-treat population, excluding those that have the event before the vaccine has been able to ramp-up to its full effect, requiring care for proper causal interpretation. Besides these primary VE estimands that summarize an effect of the vaccine over the full course of the study, we also consider local VE estimands that measure the effect at particular time points. We discuss interpretational difficulties of local VE estimands (e.g., depletion of susceptibles bias), and using frailty models as sensitivity analyses for the individual-level causal effects over time."}
{"id": "2601.17153", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.17153", "abs": "https://arxiv.org/abs/2601.17153", "authors": ["Ian Laga", "Benjamin Vogel", "Jieyun Wang", "Anna Smith", "Owen Ward"], "title": "Evaluating Aggregated Relational Data Models with Simple Diagnostics", "comment": null, "summary": "Aggregated Relational Data (ARD) contain summary information about individual social networks and are widely used to estimate social network characteristics and the size of populations of interest. Although a variety of ARD estimators exist, practitioners currently lack guidance on how to evaluate whether a selected model adequately fits the data. We introduce a diagnostic framework for ARD models that provides a systematic, reproducible process for assessing covariate structure, distributional assumptions, and correlation. The diagnostics are based on point estimates, using either maximum likelihood or maximum a posteriori optimization, which allows quick evaluation without requiring repeated Bayesian model fitting. Through simulation studies and applications to large ARD datasets, we show that the proposed workflow identifies common sources of model misfit and helps researchers select an appropriate model that adequately explains the data."}
{"id": "2601.17973", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17973", "abs": "https://arxiv.org/abs/2601.17973", "authors": ["Yuan Bian", "Grace Y. Yi", "Wenqing He"], "title": "Boosting methods for interval-censored data with regression and classification", "comment": "In The 13th International Conference on Learning Representations (2025)", "summary": "Boosting has garnered significant interest across both machine learning and statistical communities. Traditional boosting algorithms, designed for fully observed random samples, often struggle with real-world problems, particularly with interval-censored data. This type of data is common in survival analysis and time-to-event studies where exact event times are unobserved but fall within known intervals. Effective handling of such data is crucial in fields like medical research, reliability engineering, and social sciences. In this work, we introduce novel nonparametric boosting methods for regression and classification tasks with interval-censored data. Our approaches leverage censoring unbiased transformations to adjust loss functions and impute transformed responses while maintaining model accuracy. Implemented via functional gradient descent, these methods ensure scalability and adaptability. We rigorously establish their theoretical properties, including optimality and mean squared error trade-offs. Our proposed methods not only offer a robust framework for enhancing predictive accuracy in domains where interval-censored data are common but also complement existing work, expanding the applicability of existing boosting techniques. Empirical studies demonstrate robust performance across various finite-sample scenarios, highlighting the practical utility of our approaches."}
{"id": "2601.18598", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.18598", "abs": "https://arxiv.org/abs/2601.18598", "authors": ["Dimitris Rizopoulos", "Jeremy M. G. Taylor", "Isabella Kardys"], "title": "Goodness-of-Fit Checks for Joint Models", "comment": null, "summary": "Joint models for longitudinal and time-to-event data are widely used in many disciplines. Nonetheless, existing model comparison criteria do not indicate whether a model adequately fits the data or which components may be misspecified. We introduce a Bayesian posterior predictive checks framework for assessing a joint model's fit to the longitudinal and survival processes and their association. The framework supports multiple settings, including existing subjects, new subjects with only covariates, dynamic prediction at intermediate follow-up times, and cross-validated assessment. For the longitudinal component, goodness-of-fit is assessed through the mean, variance, and correlation structure, while the survival component is evaluated using empirical cumulative distributions and probability integral transforms. The association between processes is examined using time-dependent concordance statistics. We apply these checks to the Bio-SHiFT heart failure study, and a simulation study demonstrates that they can identify model misspecification that standard information criteria fail to detect. The proposed methodology is implemented in the freely available R package JMbayes2."}
{"id": "2601.17205", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17205", "abs": "https://arxiv.org/abs/2601.17205", "authors": ["Giuseppe Arena", "Maarten Marsman"], "title": "Bayesian Inference for Discrete Markov Random Fields Through Coordinate Rescaling", "comment": null, "summary": "Discrete Markov random fields (MRFs) represent a class of undirected graphical models that capture complex conditional dependencies between discrete variables. Conducting exact posterior inference in these models is computationally challenging due to the intractable partition function, which depends on the model parameters and sums over all possible state configurations in the system. As a result, using the exact likelihood function is infeasible and existing methods, such as Double Metropolis-Hastings or pseudo-likelihood approximations, either scale poorly to large systems or underestimate the variability of the target posterior distribution. To address both computational burden and efficiency loss, we propose a new class of coordinate-rescaling sampling methods, which map the model parameters from the pseudo-likelihood space to the target posterior, preserving computational efficiency while improving posterior inference. Finally, in simulation studies, we compare the proposed method to existing approaches and illustrate that coordinate-rescaling sampling provides more accurate estimates of posterior variability, offering a scalable and robust solution for Bayesian inference in discrete MRFs."}
{"id": "2601.17990", "categories": ["stat.ML", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17990", "abs": "https://arxiv.org/abs/2601.17990", "authors": ["Bokan Chen", "Raiden Hasegawa", "Adriaan Hilbers", "Ross Koningstein", "Ana Radovanović", "Utkarsh Shah", "Gabriela Volpato", "Mohamed Ahmed", "Tim Cary", "Rod Frowd"], "title": "A Cherry-Picking Approach to Large Load Shaping for More Effective Carbon Reduction", "comment": null, "summary": "Shaping multi-megawatt loads, such as data centers, impacts generator dispatch on the electric grid, which in turn affects system CO2 emissions and energy cost. Substantiating the effectiveness of prevalent load shaping strategies, such as those based on grid-level average carbon intensity, locational marginal price, or marginal emissions, is challenging due to the lack of detailed counterfactual data required for accurate attribution. This study uses a series of calibrated granular ERCOT day-ahead direct current optimal power flow (DC-OPF) simulations for counterfactual analysis of a broad set of load shaping strategies on grid CO2 emissions and cost of electricity. In terms of annual grid level CO2 emissions reductions, LMP-based shaping outperforms other common strategies, but can be significantly improved upon. Examining the performance of practicable strategies under different grid conditions motivates a more effective load shaping approach: one that \"cherry-picks\" a daily strategy based on observable grid signals and historical data. The cherry-picking approach to power load shaping is applicable to any large flexible consumer on the electricity grid, such as data centers, distributed energy resources and Virtual Power Plants (VPPs)."}
{"id": "2601.17217", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17217", "abs": "https://arxiv.org/abs/2601.17217", "authors": ["Yuping Yang", "Zhiyang Zhou"], "title": "Transfer learning for scalar-on-function regression via control variates", "comment": "45 pages, 2 figures", "summary": "Transfer learning (TL) has emerged as a powerful tool for improving estimation and prediction performance by leveraging information from related datasets. In this paper, we repurpose the control-variates (CVS) method for TL in the context of scalar-on-function regression. Our proposed framework relies exclusively on dataset-specific summary statistics, avoiding the need to pool subject-level data and thus remaining applicable in privacy-restricted or decentralized settings. We establish theoretical connections among several existing TL strategies and derive convergence rates for our CVS-based proposals. These rates explicitly account for the typically overlooked smoothing error and reveal how the similarity among covariance functions across datasets influences convergence behavior. Numerical studies support the theoretical findings and demonstrate that the proposed methods achieve competitive estimation and prediction performance compared with existing alternatives."}
{"id": "2601.18128", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18128", "abs": "https://arxiv.org/abs/2601.18128", "authors": ["Gemma E. Moran", "Anandi Krishnan"], "title": "Nonlinear multi-study factor analysis", "comment": null, "summary": "High-dimensional data often exhibit variation that can be captured by lower dimensional factors. For high-dimensional data from multiple studies or environments, one goal is to understand which underlying factors are common to all studies, and which factors are study or environment-specific. As a particular example, we consider platelet gene expression data from patients in different disease groups. In this data, factors correspond to clusters of genes which are co-expressed; we may expect some clusters (or biological pathways) to be active for all diseases, while some clusters are only active for a specific disease. To learn these factors, we consider a nonlinear multi-study factor model, which allows for both shared and specific factors. To fit this model, we propose a multi-study sparse variational autoencoder. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. In the genomics example, this means each gene is active in only a few biological processes. Further, the model implicitly induces a penalty on the number of latent factors, which helps separate the shared factors from the group-specific factors. We prove that the latent factors are identified, and demonstrate our method recovers meaningful factors in the platelet gene expression data."}
{"id": "2601.17233", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.17233", "abs": "https://arxiv.org/abs/2601.17233", "authors": ["Jiren Sun", "Linda Amoafo", "Yongming Qu"], "title": "An Empirical Method for Analyzing Count Data", "comment": null, "summary": "Count endpoints are common in clinical trials, particularly for recurrent events such as hypoglycemia. When interest centers on comparing overall event rates between treatment groups, negative binomial (NB) regression is widely used because it accommodates overdispersion and requires only event counts and exposure times. However, NB regression can be numerically unstable when events are sparse, and the efficiency gains from baseline covariate adjustment may be sensitive to model misspecification. We propose an empirical method that targets the same marginal estimand as NB regression -- the ratio of marginal event rates -- while avoiding distributional assumptions on the count outcome. Simulation studies show that the empirical method maintains appropriate Type I error control across diverse scenarios, including extreme overdispersion and zero inflation, achieves power comparable to NB regression, and yields consistent efficiency gains from baseline covariate adjustment. We illustrate the approach using severe hypoglycemia data from the QWINT-5 trial comparing insulin efsitora alfa with insulin degludec in adults with type 1 diabetes. In this sparse-event setting, the empirical method produced stable marginal rate estimates and rate ratios closely aligned with observed rates, while NB regression exhibited greater sensitivity and larger deviations from the observed rates in the sparsest intervals. The proposed empirical method provides a robust and numerically stable alternative to NB regression, particularly when the number of events is low or when numerical stability is a concern."}
{"id": "2601.18145", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.18145", "abs": "https://arxiv.org/abs/2601.18145", "authors": ["Heguang Lin", "Binhao Chen", "Mengze Li", "Daniel Pimentel-Alarcón", "Matthew L. Malloy"], "title": "Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes", "comment": "15 pages, 1 figure", "summary": "Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing."}
{"id": "2601.17241", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17241", "abs": "https://arxiv.org/abs/2601.17241", "authors": ["Jiren Sun", "Tuo Wang", "Yu Du"], "title": "Capturing Cumulative Disease Burden in Chronic Kidney Disease Outcome Trials: Area Under the Curve and Restricted Mean Time in Favor of Treatment Beyond Conventional Time-to-First Analysis", "comment": null, "summary": "Chronic kidney disease (CKD) affects millions worldwide and progresses irreversibly through stages culminating in end-stage renal disease (ESRD) and death. Outcome trials in CKD traditionally employ time-to-first-event analyses using the Cox models. However, this approach has fundamental limitations for progressive diseases: it assigns equal weight to each composite endpoint component despite clear clinical hierarchy: an eGFR decline threshold receives the same weight as ESRD or death in the analysis, and it captures only the first occurrence while ignoring subsequent progression. Given CKD's gradual evolution over years, comprehensive treatment evaluation requires quantifying cumulative disease burden: integrating both event severity and time spent in each disease state. We propose two complementary approaches to better characterize treatment benefits by incorporating event severity and state occupancy: area under the curve (AUC) and restricted mean time in favor of treatment (RMT-IF). The AUC method assigns ordinal severity scores to disease states and calculates the area under the mean cumulative score curve, quantifying total event-free time lost. Treatment effects are expressed as AUC ratios or differences. The RMT-IF extends restricted mean survival time to multistate processes, measuring average time patients in the treatment arm spend in more favorable states versus the comparator. These methods better capture CKD's progressive nature where treatment benefits extend beyond first-event delay to overall disease trajectory modification. By discriminating between events of differing clinical importance and quantifying the complete disease course, these estimands offer alternative assessment frameworks for kidney-protective therapies, potentially improving efficiency and interpretability of future CKD outcome trials."}
{"id": "2601.18677", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18677", "abs": "https://arxiv.org/abs/2601.18677", "authors": ["Yadang Alexis Rouzoumka", "Jean Pinsolle", "Eugénie Terreaux", "Christèle Morisseau", "Jean-Philippe Ovarlez", "Chengfang Ren"], "title": "Out-of-Distribution Radar Detection with Complex VAEs: Theory, Whitening, and ANMF Fusion", "comment": "13 pages, 12 figures, submitted to IEEE Transactions on Signal Processing", "summary": "We investigate the detection of weak complex-valued signals immersed in non-Gaussian, range-varying interference, with emphasis on maritime radar scenarios. The proposed methodology exploits a Complex-valued Variational AutoEncoder (CVAE) trained exclusively on clutter-plus-noise to perform Out-Of-Distribution detection. By operating directly on in-phase / quadrature samples, the CVAE preserves phase and Doppler structure and is assessed in two configurations: (i) using unprocessed range profiles and (ii) after local whitening, where per-range covariance estimates are obtained from neighboring profiles. Using extensive simulations together with real sea-clutter data from the CSIR maritime dataset, we benchmark performance against classical and adaptive detectors (MF, NMF, AMF-SCM, ANMF-SCM, ANMF-Tyler). In both configurations, the CVAE yields a higher detection probability Pd at matched false-alarm rate Pfa, with the most notable improvements observed under whitening. We further integrate the CVAE with the ANMF through a weighted log-p fusion rule at the decision level, attaining enhanced robustness in strongly non-Gaussian clutter and enabling empirically calibrated Pfa control under H0. Overall, the results demonstrate that statistical normalization combined with complex-valued generative modeling substantively improves detection in realistic sea-clutter conditions, and that the fused CVAE-ANMF scheme constitutes a competitive alternative to established model-based detectors."}
{"id": "2601.17265", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17265", "abs": "https://arxiv.org/abs/2601.17265", "authors": ["Zhiyu Xu", "Yuqi Gu"], "title": "Covariate-assisted Grade of Membership Models via Shared Latent Geometry", "comment": null, "summary": "The grade of membership model is a flexible latent variable model for analyzing multivariate categorical data through individual-level mixed membership scores. In many modern applications, auxiliary covariates are collected alongside responses and encode information about the same latent structure. Traditional approaches to incorporating such covariates typically rely on fully specified joint likelihoods, which are computationally intensive and sensitive to misspecification. We introduce a covariate-assisted grade of membership model that integrates response and covariate information by exploiting their shared low-rank simplex geometry, rather than modeling their joint distribution. We propose a likelihood-free spectral estimation procedure that combines heterogeneous data sources through a balance parameter controlling their relative contribution. To accommodate high-dimensional and heteroskedastic noise, we employ heteroskedastic principal component analysis before performing simplex-based geometric recovery. Our theoretical analysis establishes weaker identifiability conditions than those required in the covariate-free model, and further derives finite-sample, entrywise error bounds for both mixed membership scores and item parameters. These results demonstrate that auxiliary covariates can provably improve latent structure recovery, yielding faster convergence rates in high-dimensional regimes. Simulation studies and an application to educational assessment data illustrate the computational efficiency, statistical accuracy, and interpretability gains of the proposed method. The code for reproducing these results is open-source and available at \\texttt{https://github.com/Toby-X/Covariate-Assisted-GoM}"}
{"id": "2601.17146", "categories": ["stat.ME", "cs.CY", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17146", "abs": "https://arxiv.org/abs/2601.17146", "authors": ["Amanda Coston"], "title": "Falsifying Predictive Algorithm", "comment": null, "summary": "Empirical investigations into unintended model behavior often show that the algorithm is predicting another outcome than what was intended. These exposes highlight the need to identify when algorithms predict unintended quantities - ideally before deploying them into consequential settings. We propose a falsification framework that provides a principled statistical test for discriminant validity: the requirement that an algorithm predict intended outcomes better than impermissible ones. Drawing on falsification practices from causal inference, econometrics, and psychometrics, our framework compares calibrated prediction losses across outcomes to assess whether the algorithm exhibits discriminant validity with respect to a specified impermissible proxy. In settings where the target outcome is difficult to observe, multiple permissible proxy outcomes may be available; our framework accommodates both this setting and the case with a single permissible proxy. Throughout we use nonparametric hypothesis testing methods that make minimal assumptions on the data-generating process. We illustrate the method in an admissions setting, where the framework establishes discriminant validity with respect to gender but fails to establish discriminant validity with respect to race. This demonstrates how falsification can serve as an early validity check, prior to fairness or robustness analyses. We also provide analysis in a criminal justice setting, where we highlight the limitations of our framework and emphasize the need for complementary approaches to assess other aspects of construct validity and external validity."}
{"id": "2601.17319", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17319", "abs": "https://arxiv.org/abs/2601.17319", "authors": ["Hien Duy Nguyen", "Dan Wang"], "title": "Statistical process control via $p$-values", "comment": null, "summary": "We study statistical process control (SPC) through charting of $p$-values. When in control (IC), any valid sequence $(P_{t})_{t}$ is super-uniform, a requirement that can hold in nonparametric and two-phase designs without parametric modelling of the monitored process. Within this framework, we analyse the Shewhart rule that signals when $P_{t}\\leα$. Under super-uniformity alone, and with no assumptions on temporal dependence, we derive universal IC lower bounds for the average run length (ARL) and for the expected time to the $k$th false alarm ($k$-ARL). When conditional super-uniformity holds, these bounds sharpen to the familiar $α^{-1}$ and $kα^{-1}$ rates, giving simple, distribution-free calibration for $p$-value charts. Beyond thresholding, we use merging functions for dependent $p$-values to build EWMA-like schemes that output, at each time $t$, a valid $p$-value for the hypothesis that the process has remained IC up to $t$, enabling smoothing without ad hoc control limits. We also study uniform EWMA processes, giving explicit distribution formulas and left-tail guarantees. Finally, we propose a modular approach to directional and coordinate localisation in multivariate SPC via closed testing, controlling the family-wise error rate at the time of alarm. Numerical examples illustrate the utility and variety of our approach."}
{"id": "2601.17217", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17217", "abs": "https://arxiv.org/abs/2601.17217", "authors": ["Yuping Yang", "Zhiyang Zhou"], "title": "Transfer learning for scalar-on-function regression via control variates", "comment": "45 pages, 2 figures", "summary": "Transfer learning (TL) has emerged as a powerful tool for improving estimation and prediction performance by leveraging information from related datasets. In this paper, we repurpose the control-variates (CVS) method for TL in the context of scalar-on-function regression. Our proposed framework relies exclusively on dataset-specific summary statistics, avoiding the need to pool subject-level data and thus remaining applicable in privacy-restricted or decentralized settings. We establish theoretical connections among several existing TL strategies and derive convergence rates for our CVS-based proposals. These rates explicitly account for the typically overlooked smoothing error and reveal how the similarity among covariance functions across datasets influences convergence behavior. Numerical studies support the theoretical findings and demonstrate that the proposed methods achieve competitive estimation and prediction performance compared with existing alternatives."}
{"id": "2601.17400", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17400", "abs": "https://arxiv.org/abs/2601.17400", "authors": ["Zhe Li", "Mélanie Prague", "Rodolphe Thiébaut", "Quentin Clairon"], "title": "Variational autoencoder for inference of nonlinear mixed effect models based on ordinary differential equations", "comment": null, "summary": "We propose a variational autoencoder (VAE) approach for parameter estimation in nonlinear mixed-effects models based on ordinary differential equations (NLME-ODEs) using longitudinal data from multiple subjects. In moderate dimensions, likelihood-based inference via the stochastic approximation EM algorithm (SAEM) is widely used, but it relies on Markov Chain Monte-Carlo (MCMC) to approximate subject-specific posteriors. As model complexity increases or observations per subject are sparse and irregular, performance often deteriorates due to a complex, multimodal likelihood surface which may lead to MCMC convergence difficulties. We instead estimate parameters by maximizing the evidence lower bound (ELBO), a regularized surrogate for the marginal likelihood. A VAE with a shared encoder amortizes inference of subject-specific random effects by avoiding per-subject optimization and the use of MCMC. Beyond pointwise estimation, we quantify parameter uncertainty using observed-information-based variance estimator and verify that practical identifiability of the model parameters is not compromised by nuisance parameters introduced in the encoder. We evaluate the method in three simulation case studies (pharmacokinetics, humoral response to vaccination, and TGF-$β$ activation dynamics in asthmatic airways) and on a real-world antibody kinetics dataset, comparing against SAEM baselines."}
{"id": "2601.17265", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17265", "abs": "https://arxiv.org/abs/2601.17265", "authors": ["Zhiyu Xu", "Yuqi Gu"], "title": "Covariate-assisted Grade of Membership Models via Shared Latent Geometry", "comment": null, "summary": "The grade of membership model is a flexible latent variable model for analyzing multivariate categorical data through individual-level mixed membership scores. In many modern applications, auxiliary covariates are collected alongside responses and encode information about the same latent structure. Traditional approaches to incorporating such covariates typically rely on fully specified joint likelihoods, which are computationally intensive and sensitive to misspecification. We introduce a covariate-assisted grade of membership model that integrates response and covariate information by exploiting their shared low-rank simplex geometry, rather than modeling their joint distribution. We propose a likelihood-free spectral estimation procedure that combines heterogeneous data sources through a balance parameter controlling their relative contribution. To accommodate high-dimensional and heteroskedastic noise, we employ heteroskedastic principal component analysis before performing simplex-based geometric recovery. Our theoretical analysis establishes weaker identifiability conditions than those required in the covariate-free model, and further derives finite-sample, entrywise error bounds for both mixed membership scores and item parameters. These results demonstrate that auxiliary covariates can provably improve latent structure recovery, yielding faster convergence rates in high-dimensional regimes. Simulation studies and an application to educational assessment data illustrate the computational efficiency, statistical accuracy, and interpretability gains of the proposed method. The code for reproducing these results is open-source and available at \\texttt{https://github.com/Toby-X/Covariate-Assisted-GoM}"}
{"id": "2601.17612", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17612", "abs": "https://arxiv.org/abs/2601.17612", "authors": ["Gabriel Wallin", "Qi Huang"], "title": "A Hybrid Latent-Class Item Response Model for Detecting Measurement Non-Invariance in Ordinal Scales", "comment": null, "summary": "Measurement non-invariance arises when the psychometric properties of a scale differ across subgroups, undermining the validity of group comparisons. At the item level, such non-invariance manifests as differential item functioning (DIF), which occurs when the conditional distribution of an item response differs across groups after controlling for the latent trait. This paper introduces a statistical framework for detecting DIF in ordinal scales without requiring known group labels or anchor items. We propose a hybrid latent-class item response model to ordinal data using a proportional-odds formulation, assigning individuals probabilistically to latent classes. DIF is captured through class-specific shifts in item intercepts and slopes, allowing for both uniform and non-uniform DIF. The identification of DIF effects is achieved via an $L_1$-penalised marginal likelihood function under a sparsity assumption, and model estimation is implemented using a tailored EM algorithm. Simulation studies demonstrate strong recovery of item parameters and both uniform and non-uniform types of DIF. An empirical application to a personality test reveals latent subgroups with distinct response patterns and identifies items that may bias group comparisons. The proposed framework provides a flexible approach to assessing measurement invariance in ordinal scales when comparison groups are unobserved or poorly defined."}
{"id": "2601.18658", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18658", "abs": "https://arxiv.org/abs/2601.18658", "authors": ["Max Behrens", "Daiana Stolz", "Eleni Papakonstantinou", "Janis M. Nolde", "Gabriele Bellerino", "Angelika Rohde", "Moritz Hess", "Harald Binder"], "title": "Contrasting Global and Patient-Specific Regression Models via a Neural Network Representation", "comment": null, "summary": "When developing clinical prediction models, it can be challenging to balance between global models that are valid for all patients and personalized models tailored to individuals or potentially unknown subgroups. To aid such decisions, we propose a diagnostic tool for contrasting global regression models and patient-specific (local) regression models. The core utility of this tool is to identify where and for whom a global model may be inadequate. We focus on regression models and specifically suggest a localized regression approach that identifies regions in the predictor space where patients are not well represented by the global model. As localization becomes challenging when dealing with many predictors, we propose modeling in a dimension-reduced latent representation obtained from an autoencoder. Using such a neural network architecture for dimension reduction enables learning a latent representation simultaneously optimized for both good data reconstruction and for revealing local outcome-related associations suitable for robust localized regression. We illustrate the proposed approach with a clinical study involving patients with chronic obstructive pulmonary disease. Our findings indicate that the global model is adequate for most patients but that indeed specific subgroups benefit from personalized models. We also demonstrate how to map these subgroup models back to the original predictors, providing insight into why the global model falls short for these groups. Thus, the principal application and diagnostic yield of our tool is the identification and characterization of patients or subgroups whose outcome associations deviate from the global model."}
{"id": "2601.17618", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17618", "abs": "https://arxiv.org/abs/2601.17618", "authors": ["Yang Liu", "Xiaohui Luo", "Jieyuan Dong", "Youjin Sung", "Yueqin Hu", "Hongyun Liu", "Daniel J. Bauer"], "title": "Two-stage Estimation of Latent Variable Regression Models: A General, Root-N Consistent Solution", "comment": null, "summary": "Latent variable (LV) models are widely used in psychological research to investigate relationships among unobservable constructs. When one-stage estimation of the overall LV model is challenging, two-stage factor score regression (FSR) serves as a convenient alternative: the measurement model is fitted to obtain factor scores in the first stage, which are then used to fit the structural model in the subsequent stage. However, naive application of FSR is known to yield biased estimates of structural parameters. In this paper, we develop a generic bias-correction framework for two-stage estimation of parametric statistical models and tailor it specifically to FSR. Unlike existing bias-corrected FSR solutions, the proposed method applies to a broader class of LV models and does not require computing specific types of factor scores. We establish the root-n consistency of the proposed bias-corrected two-stage estimator under mild regularity conditions. To ensure broad applicability and minimize reliance on complex analytical derivations, we introduce a stochastic approximation algorithm for point estimation and a Monte Carlo-based procedure for variance estimation. In a sequence of Monte Carlo experiments, we demonstrate that the bias-corrected FSR estimator performs comparably to the ``gold standard'' one-stage maximum likelihood estimator. These results suggest that our approach offers a straightforward yet effective alternative for estimating LV models."}
{"id": "2601.17621", "categories": ["stat.ME", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.17621", "abs": "https://arxiv.org/abs/2601.17621", "authors": ["Tim Ritmeester"], "title": "Non-parametric finite-sample credible intervals with one-dimensional priors: a middle ground between Bayesian and frequentist intervals", "comment": null, "summary": "We propose a new type of statistical interval obtained by weakening the definition of a p% credible interval: Having observed the interval (rather than the full dataset) we should put at least a p% belief in it. From a decision-theoretical point of view the resulting intervals occupy a middle ground between frequentist and fully Bayesian statistical intervals, both practically and philosophically: To a p% Bayesian credible interval we should assign (at least a) p% belief also after seeing the full dataset, while p% frequentist intervals we in general only assign a p% belief before seeing either the data or the interval.\n  We derive concrete implementations for two cases: estimation of the fraction of a distribution that falls below a certain value (i.e., the CDF), and of the mean of a distribution with bounded support. Even though the problems are fully non parametric, these methods require only one-dimensional priors. They share many of the practical advantages of Bayesian methods while avoiding the complexity of assigning high-dimensional priors altogether. Asymptotically they give intervals equivalent to the fully Bayesian approach and somewhat wider intervals, respectively. We discuss promising directions where the proposed type of interval may provide significant advantages."}
{"id": "2601.17695", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17695", "abs": "https://arxiv.org/abs/2601.17695", "authors": ["Yafang Deng", "Kang Shuai", "Shanshan Luo"], "title": "Bidirectional causal inference for binary outcomes in the presence of unmeasured confounding", "comment": "21 pages, 8 figures", "summary": "Bidirectional causal relationships arising from mutual interactions between variables are commonly observed within biomedical, econometrical, and social science contexts. When such relationships are further complicated by unobserved factors, identifying causal effects in both directions becomes especially challenging. For continuous variables, methods that utilize two instrumental variables from both directions have been proposed to explore bidirectional causal effects in linear models. However, the existing techniques are not applicable when the key variables of interest are binary. To address these issues, we propose a structural equation modeling approach that links observed binary variables to continuous latent variables through a constrained mapping. We further establish identification results for bidirectional causal effects using a pair of instrumental variables. Additionally, we develop an estimation method for the corresponding causal parameters. We also conduct sensitivity analysis under scenarios where certain identification conditions are violated. Finally, we apply our approach to investigate the bidirectional causal relationship between heart disease and diabetes, demonstrating its practical utility in biomedical research."}
{"id": "2601.17734", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17734", "abs": "https://arxiv.org/abs/2601.17734", "authors": ["Zonghan Li", "Hongyi Zhou", "Zhiheng Zhang"], "title": "Group Permutation Testing in Linear Model: Sharp Validity, Power Improvement, and Extension Beyond Exchangeability", "comment": "74 pages, 3 figures. Includes supplementary material", "summary": "We consider finite-sample inference for a single regression coefficient in the fixed-design linear model $Y = Zβ+ bX + \\varepsilon$, where $\\varepsilon\\in\\mathbb{R}^n$ may exhibit complex dependence or heterogeneity. We develop a group permutation framework, yielding a unified and analyzable randomization structure for linear-model testing. Under exchangeable errors, we place permutation-augmented regression tests within this group-theoretic setting and show that a grouped version of PALMRT controls Type I error at level at most $2α$ for any permutation group; moreover, we provide an worst-case construction demonstrating that the factor $2$ is sharp and cannot be improved without additional assumptions. Second, we relate the Type II error to a design-dependent geometric separation. We formulate it as a combinatorial optimization problem over permutation groups and bound it under additional mild sub-Gaussian assumptions. For the Type II error upper bound control, we propose a constructive algorithm for the permutation strategy that is better (at least no worse) than the i.i.d. permutation, with simulations empirically indicating substantial power gains, especially under heavy-tailed designs. Finally, we extend group-based CPT and PALMRT beyond exchangeability by connecting rank-based randomization arguments to conformal inference. The resulting weighted group tests satisfy finite-sample Type I error bounds that degrade gracefully with a weighted average of total variation distances between $\\varepsilon$ and its group-permuted versions, recovering exact validity when these discrepancies vanish and yielding quantitative robustness otherwise. Taken together, the group-permutation viewpoint provides a principled bridge from exact randomization validity to design-adaptive power and quantitative robustness under approximate symmetries."}
{"id": "2601.17779", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17779", "abs": "https://arxiv.org/abs/2601.17779", "authors": ["Shuying Shen", "Valerio Bacak", "Edward H. Kennedy"], "title": "Sensitivity analysis for incremental effects, with application to a study of victimization & offending", "comment": null, "summary": "Sensitivity analysis for unmeasured confounding under incremental propensity score interventions remains relatively underdeveloped. Incremental interventions define stochastic treatment regimes by multiplying the odds of treatment, offering a flexible framework for causal effect estimation. To study incremental effects when there are unobserved confounders, we adopt Rosenbaum's sensitivity model in single time point settings, and propose a doubly robust estimator for the resulting effect bounds. The bound estimators are asymptotically normal under mild conditions on nuisance function estimation. We show that incremental effect bounds can be narrower or wider than those for mean potential outcomes, and that the bounds must lie between the expected minimum and maximum of the conditional bounds on E(Y^0|X) and E(Y^1|X). For time-varying treatments, we consider the marginal sensitivity model. Although sharp bounds for incremental effects are identifiable from longitudinal data under this model, practical estimators have not yet been established; we discuss this challenge and provide partial results toward implementation. Finally, we apply our methods to study the effect of victimization on subsequent offending using data from the National Longitudinal Study of Adolescent to Adult Health (Add Health), illustrating the robustness of our findings in an empirical setting."}
{"id": "2601.17961", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17961", "abs": "https://arxiv.org/abs/2601.17961", "authors": ["Zexiang Li", "Donna Spiegelman", "Molin Wang", "Zuoheng Wang", "Xin Zhou"], "title": "Transportability of Regression Calibration with External Validation Studies for Measurement Error Correction", "comment": null, "summary": "In nutritional and environmental epidemiology, exposures are impractical to measure accurately, while practical measures for these exposures are often subject to substantial measurement error. Regression calibration is among the most used measurement error correction methods with external validation studies. The use of external studies to assess the measurement error process always carries the risk of introducing estimation bias into the main study analysis. Although the transportability of regression calibration is usually assumed for practical epidemiology studies, it has not been well studied. In this work, under the measurement error process with a mixture of Berkson-like and classical-like errors, we investigate conditions under which the effect estimate from regression calibration with an external validation study is unbiased for the association between exposure and health outcome. We further examine departures from the transportability assumption, under which the regression calibration estimator is itself biased. However, we theoretically prove that, in most cases, it yields lower bias than the naive method. The derived conditions are confirmed through simulation studies and further verified in an example investigating the association between the risk of cardiovascular disease and moderate physical activity in the health professional follow-up study."}
{"id": "2601.18013", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.18013", "abs": "https://arxiv.org/abs/2601.18013", "authors": ["Fei Wan"], "title": "Examining the Efficacy of Coarsen Exact Matching as an Alternative to Propensity Score Matching", "comment": null, "summary": "Coarsened exact matching (CEM) is often promoted as a superior alternative to propensity score matching (PSM) for addressing imbalance, model dependence, bias, and efficiency. However, this recommendation remains uncertain. First, CEM is commonly mischaracterized as exact matching, despite relying on coarsened rather than original variables. This inexactness in matching introduces residual confounding, which necessitates accurate modeling of the outcome-confounder relationship post-matching to mitigate bias, thereby increasing vulnerability to model misspecification. Second, prior studies overlook that any imbalance between treated and untreated subjects matched on the same propensity score is attributable to random variation. Thus, claims that CEM outperforms PSM in reducing imbalance are unfounded, particularly when using metrics like Mahalanobis distance, which do not account for chance imbalance in PSM. Our simulations show that PSM reduces imbalance more effectively than CEM when evaluated with multivariate standardized mean differences (SMD), and unadjusted analyses indicate greater bias with CEM. While adjusted analyses in both CEM with autocoarsening and PSM may perform similarly when matching on few variables, CEM suffers from the curse of dimensionality as the number of factors increases, resulting in substantial data loss and unstable estimates. Increasing the level of coarsening may mitigate data loss but exacerbates residual confounding and model dependence. In contrast, both analytical results and simulations demonstrate that PSM is more robust to model misspecification and thus less model-dependent. Therefore, CEM is not a viable alternative to PSM when matching on a large number of covariates."}
{"id": "2601.18052", "categories": ["stat.ME", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.18052", "abs": "https://arxiv.org/abs/2601.18052", "authors": ["Jason B. Cho", "David S. Matteson"], "title": "BASTION: A Bayesian Framework for Trend and Seasonality Decomposition", "comment": null, "summary": "We introduce BASTION (Bayesian Adaptive Seasonality and Trend DecompositION), a flexible Bayesian framework for decomposing time series into trend and multiple seasonality components. We cast the decomposition as a penalized nonparametric regression and establish formal conditions under which the trend and seasonal components are uniquely identifiable, an issue only treated informally in the existing literature. BASTION offers three key advantages over existing decomposition methods: (1) accurate estimation of trend and seasonality amidst abrupt changes, (2) enhanced robustness against outliers and time-varying volatility, and (3) robust uncertainty quantification. We evaluate BASTION against established methods, including TBATS, STR, and MSTL, using both simulated and real-world datasets. By effectively capturing complex dynamics while accounting for irregular components such as outliers and heteroskedasticity, BASTION delivers a more nuanced and interpretable decomposition. To support further research and practical applications, BASTION is available as an R package at https://github.com/Jasoncho0914/BASTION"}
{"id": "2601.18072", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.18072", "abs": "https://arxiv.org/abs/2601.18072", "authors": ["Stephanie CC van der Lubbe", "Jose M Valderas", "Evangelos Kontopantelis"], "title": "The effect of collinearity and sample size on linear regression results: a simulation study", "comment": "17 pages, 5 figures", "summary": "Background: Multicollinearity inflates the variance of OLS coefficients, widening confidence intervals and reducing inferential reliability. Yet fixed variance inflation factor (VIF) cut-offs are often applied uniformly across studies with very different sample sizes, even though collinearity is a finite-sample problem. We quantify how collinearity and sample size jointly affect linear regression performance and provide practical guidance for interpreting VIFs.\n  Methods: We simulated data across sample sizes N=100-100,000 and collinearity levels VIF=1-50. For each scenario we generated 1,000 datasets, fitted OLS models, and assessed coverage, mean absolute error (MAE), bias, traditional power (CI excludes 0), and precision assurance (probability the 95% CI lies within a prespecified margin around the true effect). We also evaluated a biased, misspecified setting by omitting a relevant predictor to study bias amplification.\n  Results: Under correct specification, collinearity did not materially affect nominal coverage and did not introduce systematic bias, but it reduced precision in small samples: at N=100, even mild collinearity (VIF<2) inflated MAE and markedly reduced both power metrics, whereas at N>=50,000 estimates were robust even at VIF=50. Under misspecification, collinearity strongly amplified bias, increasing errors, reducing coverage, and sharply degrading both precision assurance and traditional power even at low VIF.\n  Conclusion: VIF thresholds should not be applied mechanically. Collinearity must be interpreted in relation to sample size and potential sources of bias; removing predictors solely to reduce VIF can worsen inference via omitted-variable bias. The accompanying heatmaps provide a practical reference across study sizes and modelling assumptions."}
{"id": "2601.18075", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.18075", "abs": "https://arxiv.org/abs/2601.18075", "authors": ["Dingyi Wang", "Haiying Wang", "Qingpei Hu"], "title": "Maximum-Variance-Reduction Stratification for Improved Subsampling", "comment": null, "summary": "Subsampling is a widely used and effective approach for addressing the computational challenges posed by massive datasets. Substantial progress has been made in developing non-uniform, probability-based subsampling schemes that prioritize more informative observations. We propose a novel stratification mechanism that can be combined with existing subsampling designs to further improve estimation efficiency. We establish the estimator's asymptotic normality and quantify the resulting efficiency gains, which enables a principled procedure for selecting stratification variables and interval boundaries that target reductions in asymptotic variance. The resulting algorithm, Maximum-Variance-Reduction Stratification (MVRS), achieves significant improvements in estimation efficiency while incurring only linear additional computational cost. MVRS is applicable to both non-uniform and uniform subsampling methods. Experiments on simulated and real datasets confirm that MVRS markedly reduces estimator variance and improves accuracy compared with existing subsampling methods."}
{"id": "2601.18412", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.18412", "abs": "https://arxiv.org/abs/2601.18412", "authors": ["Lingfeng Lyu", "Doudou Zhou"], "title": "Preference-based Centrality and Ranking in General Metric Spaces", "comment": null, "summary": "Assessing centrality or ranking observations in multivariate or non-Euclidean spaces is challenging because such data lack an intrinsic order and many classical depth notions lose resolution in high-dimensional or structured settings. We propose a preference-based framework that defines centrality through population pairwise proximity comparisons: a point is central if a typical draw from the underlying distribution tends to lie closer to it than to another. This perspective yields a well-defined statistical functional that generalizes data depth to arbitrary metric spaces. To obtain a coherent one-dimensional representation, we study a Bradley-Terry-Luce projection of the induced preferences and develop two finite-sample estimators based on convex M-estimation and spectral aggregation. The resulting procedures are consistent, scalable, and applicable to high-dimensional and non-Euclidean data, and across a range of examples they exhibit stable ranking behavior and improved resolution relative to classical depth-based methods."}
{"id": "2601.18587", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.18587", "abs": "https://arxiv.org/abs/2601.18587", "authors": ["Michael P. Fay", "Dean Follmann", "Bruce J. Swihart", "Lauren E. Dang"], "title": "Vaccine Efficacy Estimands Implied by Common Estimators Used in Individual Randomized Field Trials", "comment": "28 pages, 6 figures", "summary": "We review vaccine efficacy (VE) estimands for susceptibility in individual randomized trials with natural (unmeasured) exposure, where individual responses are measured as time from vaccination until an event (e.g., disease from the infectious agent). Common VE estimands are written as $1-θ$, where $θ$ is some ratio effect measure (e.g., ratio of incidence rates, cumulative incidences, hazards, or odds) comparing outcomes under vaccination versus control. Although the ratio effects are approximately equal with low control event rates, we explore the quality of that approximation using a nonparametric formulation. Traditionally, the primary endpoint VE estimands are full immunization (or biological) estimands that represent a subset of the intent-to-treat population, excluding those that have the event before the vaccine has been able to ramp-up to its full effect, requiring care for proper causal interpretation. Besides these primary VE estimands that summarize an effect of the vaccine over the full course of the study, we also consider local VE estimands that measure the effect at particular time points. We discuss interpretational difficulties of local VE estimands (e.g., depletion of susceptibles bias), and using frailty models as sensitivity analyses for the individual-level causal effects over time."}
{"id": "2601.18598", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.18598", "abs": "https://arxiv.org/abs/2601.18598", "authors": ["Dimitris Rizopoulos", "Jeremy M. G. Taylor", "Isabella Kardys"], "title": "Goodness-of-Fit Checks for Joint Models", "comment": null, "summary": "Joint models for longitudinal and time-to-event data are widely used in many disciplines. Nonetheless, existing model comparison criteria do not indicate whether a model adequately fits the data or which components may be misspecified. We introduce a Bayesian posterior predictive checks framework for assessing a joint model's fit to the longitudinal and survival processes and their association. The framework supports multiple settings, including existing subjects, new subjects with only covariates, dynamic prediction at intermediate follow-up times, and cross-validated assessment. For the longitudinal component, goodness-of-fit is assessed through the mean, variance, and correlation structure, while the survival component is evaluated using empirical cumulative distributions and probability integral transforms. The association between processes is examined using time-dependent concordance statistics. We apply these checks to the Bio-SHiFT heart failure study, and a simulation study demonstrates that they can identify model misspecification that standard information criteria fail to detect. The proposed methodology is implemented in the freely available R package JMbayes2."}
{"id": "2601.18658", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18658", "abs": "https://arxiv.org/abs/2601.18658", "authors": ["Max Behrens", "Daiana Stolz", "Eleni Papakonstantinou", "Janis M. Nolde", "Gabriele Bellerino", "Angelika Rohde", "Moritz Hess", "Harald Binder"], "title": "Contrasting Global and Patient-Specific Regression Models via a Neural Network Representation", "comment": null, "summary": "When developing clinical prediction models, it can be challenging to balance between global models that are valid for all patients and personalized models tailored to individuals or potentially unknown subgroups. To aid such decisions, we propose a diagnostic tool for contrasting global regression models and patient-specific (local) regression models. The core utility of this tool is to identify where and for whom a global model may be inadequate. We focus on regression models and specifically suggest a localized regression approach that identifies regions in the predictor space where patients are not well represented by the global model. As localization becomes challenging when dealing with many predictors, we propose modeling in a dimension-reduced latent representation obtained from an autoencoder. Using such a neural network architecture for dimension reduction enables learning a latent representation simultaneously optimized for both good data reconstruction and for revealing local outcome-related associations suitable for robust localized regression. We illustrate the proposed approach with a clinical study involving patients with chronic obstructive pulmonary disease. Our findings indicate that the global model is adequate for most patients but that indeed specific subgroups benefit from personalized models. We also demonstrate how to map these subgroup models back to the original predictors, providing insight into why the global model falls short for these groups. Thus, the principal application and diagnostic yield of our tool is the identification and characterization of patients or subgroups whose outcome associations deviate from the global model."}
{"id": "2601.18683", "categories": ["stat.ME", "astro-ph.IM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18683", "abs": "https://arxiv.org/abs/2601.18683", "authors": ["Alicja Polanska", "Jason D. McEwen"], "title": "Learned harmonic mean estimation of the marginal likelihood for multimodal posteriors with flow matching", "comment": "Submitted to 44th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering", "summary": "The marginal likelihood, or Bayesian evidence, is a crucial quantity for Bayesian model comparison but its computation can be challenging for complex models, even in parameters space of moderate dimension. The learned harmonic mean estimator has been shown to provide accurate and robust estimates of the marginal likelihood simply using posterior samples. It is agnostic to the sampling strategy, meaning that the samples can be obtained using any method. This enables marginal likelihood calculation and model comparison with whatever sampling is most suitable for the task. However, the internal density estimators considered previously for the learned harmonic mean can struggle with highly multimodal posteriors. In this work we introduce flow matching-based continuous normalizing flows as a powerful architecture for the internal density estimation of the learned harmonic mean. We demonstrate the ability to handle challenging multimodal posteriors, including an example in 20 parameter dimensions, showcasing the method's ability to handle complex posteriors without the need for fine-tuning or heuristic modifications to the base distribution."}
{"id": "2601.17160", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17160", "abs": "https://arxiv.org/abs/2601.17160", "authors": ["Yonghan Jung", "Bogyeong Kang"], "title": "Data-Driven Information-Theoretic Causal Bounds under Unmeasured Confounding", "comment": null, "summary": "We develop a data-driven information-theoretic framework for sharp partial identification of causal effects under unmeasured confounding. Existing approaches often rely on restrictive assumptions, such as bounded or discrete outcomes; require external inputs (for example, instrumental variables, proxies, or user-specified sensitivity parameters); necessitate full structural causal model specifications; or focus solely on population-level averages while neglecting covariate-conditional treatment effects. We overcome all four limitations simultaneously by establishing novel information-theoretic, data-driven divergence bounds. Our key theoretical contribution shows that the f-divergence between the observational distribution P(Y | A = a, X = x) and the interventional distribution P(Y | do(A = a), X = x) is upper bounded by a function of the propensity score alone. This result enables sharp partial identification of conditional causal effects directly from observational data, without requiring external sensitivity parameters, auxiliary variables, full structural specifications, or outcome boundedness assumptions. For practical implementation, we develop a semiparametric estimator satisfying Neyman orthogonality (Chernozhukov et al., 2018), which ensures square-root-n consistent inference even when nuisance functions are estimated using flexible machine learning methods. Simulation studies and real-world data applications, implemented in the GitHub repository (https://github.com/yonghanjung/Information-Theretic-Bounds), demonstrate that our framework provides tight and valid causal bounds across a wide range of data-generating processes."}
{"id": "2601.18656", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.18656", "abs": "https://arxiv.org/abs/2601.18656", "authors": ["Sarika Aggarwal", "Phillip B. Nicol", "Brent A. Coull", "Rachel C. Nethery"], "title": "A varying-coefficient model for characterizing duration-driven heterogeneity in flood-related health impacts", "comment": null, "summary": "Previous work revealed associations between flood exposure and adverse health outcomes during and in the aftermath of flood events. Floods are highly heterogeneous events, largely owing to vast differences in flood durations, i.e., flash-floods versus slow-moving floods. However, little to no work has incorporated exposure duration into the modeling of flood-related health impacts or has investigated duration-driven effect heterogeneity. To address this gap, we propose an exposure duration varying coefficient modeling (EDVCM) framework for estimating exposure day-specific health effects of consecutive-day environmental exposures that vary in duration. We develop the EDVCM within an area-level self-matched study design to eliminate time-invariant confounding followed by conditional Poisson regression modeling for exposure effect estimation and adjustment of time-varying confounders. Using a Bayesian framework, we introduce duration- and exposure day-specific exposure coefficients within the conditional Poisson model and assign them a two-dimensional Gaussian process prior to allow for sharing of information across both duration and exposure day. This approach enables highly-resolved insights into duration-driven effect heterogeneity while ensuring model stability through information sharing. Through simulations, we demonstrate that the EDVCM out-performs conventional approaches in terms of both effect estimation and uncertainty quantification. We apply the EDVCM to nationwide, multi-decade Medicare claims data linked with high-resolution flood exposure measures to investigate duration-driven heterogeneity in flood effects on musculoskeletal system disease hospitalizations."}
