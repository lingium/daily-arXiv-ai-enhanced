<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 9]
- [stat.AP](#stat.AP) [Total: 6]
- [stat.ME](#stat.ME) [Total: 10]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Efficient Inference for Coupled Hidden Markov Models in Continuous Time and Discrete Space](https://arxiv.org/abs/2510.12916)
*Giosue Migliorini,Padhraic Smyth*

Main category: stat.ML

TL;DR: 提出了Latent Interacting Particle Systems模型，通过参数化马尔可夫链生成器，结合前瞻函数估计和扭曲序贯蒙特卡罗采样，解决高维连续时间马尔可夫链系统的后验推断难题。


<details>
  <summary>Details</summary>
Motivation: 高维交互连续时间马尔可夫链系统的推断通常难以处理，离散时间观测信息需要通过Doob's h-变换整合，但会产生难以处理的后验过程。

Method: 参数化每个马尔可夫链的生成器，引入高效参数化的前瞻函数（扭曲势能）估计，并将其整合到扭曲序贯蒙特卡罗采样方案中。

Result: 在图的潜在SIRS模型后验推断任务和基于真实数据的野火传播动态神经网络模型上验证了方法的有效性。

Conclusion: 所提出的方法能够有效处理高维交互连续时间马尔可夫链系统的后验推断问题，在复杂任务中表现出良好性能。

Abstract: Systems of interacting continuous-time Markov chains are a powerful model
class, but inference is typically intractable in high dimensional settings.
Auxiliary information, such as noisy observations, is typically only available
at discrete times, and incorporating it via a Doob's $h-$transform gives rise
to an intractable posterior process that requires approximation. We introduce
Latent Interacting Particle Systems, a model class parameterizing the generator
of each Markov chain in the system. Our inference method involves estimating
look-ahead functions (twist potentials) that anticipate future information, for
which we introduce an efficient parameterization. We incorporate this
approximation in a twisted Sequential Monte Carlo sampling scheme. We
demonstrate the effectiveness of our approach on a challenging posterior
inference task for a latent SIRS model on a graph, and on a neural model for
wildfire spread dynamics trained on real data.

</details>


### [2] [Simplicial Gaussian Models: Representation and Inference](https://arxiv.org/abs/2510.12983)
*Lorenzo Marinucci,Gabriele D'Acunto,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: stat.ML

TL;DR: 提出了单纯形高斯模型(SGM)，将高斯概率图模型扩展到单纯复形，能够联合建模顶点、边和三角形上的随机变量，克服了传统PGM只能处理成对交互的限制。


<details>
  <summary>Details</summary>
Motivation: 传统概率图模型(PGMs)只能处理成对交互，无法捕捉更高阶的统计依赖关系。为了在复杂系统中建模顶点、边和三角形等多层次拓扑结构之间的依赖关系，需要扩展PGM到单纯复形。

Method: 基于离散霍奇理论，构建了单纯形高斯模型，在单个参数化高斯分布中联合建模顶点、边和三角形上的随机变量。开发了最大似然推断算法来恢复完整SGM的参数和条件依赖结构。

Result: 在不同规模和稀疏度的合成单纯复形上的数值实验验证了算法的有效性，能够成功恢复模型的参数和条件依赖结构。

Conclusion: SGM成功将高斯概率图模型扩展到单纯复形，为建模高阶统计依赖提供了有效框架，最大似然推断算法在实际应用中表现良好。

Abstract: Probabilistic graphical models (PGMs) are powerful tools for representing
statistical dependencies through graphs in high-dimensional systems. However,
they are limited to pairwise interactions. In this work, we propose the
simplicial Gaussian model (SGM), which extends Gaussian PGM to simplicial
complexes. SGM jointly models random variables supported on vertices, edges,
and triangles, within a single parametrized Gaussian distribution. Our model
builds upon discrete Hodge theory and incorporates uncertainty at every
topological level through independent random components. Motivated by
applications, we focus on the marginal edge-level distribution while treating
node- and triangle-level variables as latent. We then develop a
maximum-likelihood inference algorithm to recover the parameters of the full
SGM and the induced conditional dependence structure. Numerical experiments on
synthetic simplicial complexes with varying size and sparsity confirm the
effectiveness of our algorithm.

</details>


### [3] [Conformal Inference for Open-Set and Imbalanced Classification](https://arxiv.org/abs/2510.13037)
*Tianmin Xie,Yanfei Zhou,Ziyi Liang,Stefano Favaro,Matteo Sesia*

Main category: stat.ML

TL;DR: 提出一种用于高度不平衡和开放集分类的保形预测方法，能够处理测试时出现新类别的情况，并通过选择性样本分割提高预测效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要已知有限的标签空间，无法处理测试时出现新类别的情况，且在类别不平衡时预测过于保守。

Method: 计算新的保形p值来测试数据点是否属于未见类别，开发基于标签频率的选择性样本分割算法，通过重新加权保持有限样本保证。

Result: 在模拟和真实数据中验证，该方法在无限可能标签的开放集场景下仍能保持有效覆盖，在极端类别不平衡时产生更有信息的预测。

Conclusion: 该方法成功解决了开放集和高度不平衡分类问题，提供了有效的预测集覆盖，并建立了与Good-Turing估计器的理论联系。

Abstract: This paper presents a conformal prediction method for classification in
highly imbalanced and open-set settings, where there are many possible classes
and not all may be represented in the data. Existing approaches require a
finite, known label space and typically involve random sample splitting, which
works well when there is a sufficient number of observations from each class.
Consequently, they have two limitations: (i) they fail to provide adequate
coverage when encountering new labels at test time, and (ii) they may become
overly conservative when predicting previously seen labels. To obtain valid
prediction sets in the presence of unseen labels, we compute and integrate into
our predictions a new family of conformal p-values that can test whether a new
data point belongs to a previously unseen class. We study these p-values
theoretically, establishing their optimality, and uncover an intriguing
connection with the classical Good--Turing estimator for the probability of
observing a new species. To make more efficient use of imbalanced data, we also
develop a selective sample splitting algorithm that partitions training and
calibration data based on label frequency, leading to more informative
predictions. Despite breaking exchangeability, this allows maintaining
finite-sample guarantees through suitable re-weighting. With both simulated and
real data, we demonstrate our method leads to prediction sets with valid
coverage even in challenging open-set scenarios with infinite numbers of
possible labels, and produces more informative predictions under extreme class
imbalance.

</details>


### [4] [A Multi-dimensional Semantic Surprise Framework Based on Low-Entropy Semantic Manifolds for Fine-Grained Out-of-Distribution Detection](https://arxiv.org/abs/2510.13093)
*Ningkang Peng,Yuzhe Mao,Yuhao Zhang,Linjin Qian,Qianfeng Yu,Yanhui Gu,Yi Chen,Li Kong*

Main category: stat.ML

TL;DR: 提出了从传统概率视角转向信息论框架的OOD检测新范式，将OOD检测从二元分类扩展为三元分类（ID vs. Near-OOD vs. Far-OOD），引入语义惊喜向量来量化样本的语义意外程度。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法将问题简化为二元分类，无法区分语义相近（Near-OOD）和语义遥远（Far-OOD）的未知风险，这在需要细粒度风险分层的应用中构成安全瓶颈。

Method: 基于低熵语义流形的理论概念，设计分层原型网络构建反映数据内在语义层次的结构，引入语义惊喜向量（SSV）将样本的总惊喜分解为三个可解释维度：一致性、新颖性和模糊性。

Result: 在三元分类任务上达到新SOTA，其鲁棒表示在传统二元基准测试中也取得顶尖结果，在LSUN等数据集上将误报率降低超过60%。

Conclusion: 信息论框架为OOD检测提供了更细粒度和可解释的风险评估，能够有效区分不同语义距离的未知风险，显著提升AI系统在开放世界中的安全性。

Abstract: Out-of-Distribution (OOD) detection is a cornerstone for the safe deployment
of AI systems in the open world. However, existing methods treat OOD detection
as a binary classification problem, a cognitive flattening that fails to
distinguish between semantically close (Near-OOD) and distant (Far-OOD) unknown
risks. This limitation poses a significant safety bottleneck in applications
requiring fine-grained risk stratification. To address this, we propose a
paradigm shift from a conventional probabilistic view to a principled
information-theoretic framework. We formalize the core task as quantifying the
Semantic Surprise of a new sample and introduce a novel ternary classification
challenge: In-Distribution (ID) vs. Near-OOD vs. Far-OOD. The theoretical
foundation of our work is the concept of Low-Entropy Semantic Manifolds, which
are explicitly structured to reflect the data's intrinsic semantic hierarchy.
To construct these manifolds, we design a Hierarchical Prototypical Network. We
then introduce the Semantic Surprise Vector (SSV), a universal probe that
decomposes a sample's total surprise into three complementary and interpretable
dimensions: conformity, novelty, and ambiguity. To evaluate performance on this
new task, we propose the Normalized Semantic Risk (nSR), a cost-sensitive
metric. Experiments demonstrate that our framework not only establishes a new
state-of-the-art (sota) on the challenging ternary task, but its robust
representations also achieve top results on conventional binary benchmarks,
reducing the False Positive Rate by over 60% on datasets like LSUN.

</details>


### [5] [Gaussian Certified Unlearning in High Dimensions: A Hypothesis Testing Approach](https://arxiv.org/abs/2510.13094)
*Aaradhya Pandey,Arnab Auddy,Haolin Zou,Arian Maleki,Sanjeev Kulkarni*

Main category: stat.ML

TL;DR: 本文提出ε-高斯可认证性概念，证明在单步牛顿法后添加适当高斯噪声即可在高维场景下实现机器遗忘的隐私和准确性，与之前需要至少两步的方法形成对比。


<details>
  <summary>Details</summary>
Motivation: 解决高维场景下机器遗忘的理论挑战，传统优化假设在高维比例机制中很少同时成立，需要新的认证概念来捕获噪声添加机制。

Method: 引入ε-高斯可认证性概念，分析基于单步牛顿法的遗忘算法，配合校准的高斯噪声。

Result: 证明单步牛顿法加高斯噪声足以在高维设置中同时保证隐私和准确性。

Conclusion: ε-高斯可认证性优于ε-可认证性，能够最优地克服噪声添加机制的不兼容性，解释了与先前工作所需步骤数的差异。

Abstract: Machine unlearning seeks to efficiently remove the influence of selected data
while preserving generalization. Significant progress has been made in low
dimensions $(p \ll n)$, but high dimensions pose serious theoretical challenges
as standard optimization assumptions of $\Omega(1)$ strong convexity and $O(1)$
smoothness of the per-example loss $f$ rarely hold simultaneously in
proportional regimes $(p\sim n)$. In this work, we introduce
$\varepsilon$-Gaussian certifiability, a canonical and robust notion
well-suited to high-dimensional regimes, that optimally captures a broad class
of noise adding mechanisms. Then we theoretically analyze the performance of a
widely used unlearning algorithm based on one step of the Newton method in the
high-dimensional setting described above. Our analysis shows that a single
Newton step, followed by a well-calibrated Gaussian noise, is sufficient to
achieve both privacy and accuracy in this setting. This result stands in sharp
contrast to the only prior work that analyzes machine unlearning in high
dimensions \citet{zou2025certified}, which relaxes some of the standard
optimization assumptions for high-dimensional applicability, but operates under
the notion of $\varepsilon$-certifiability. That work concludes %that a single
Newton step is insufficient even for removing a single data point, and that at
least two steps are required to ensure both privacy and accuracy. Our result
leads us to conclude that the discrepancy in the number of steps arises because
of the sub optimality of the notion of $\varepsilon$-certifiability and its
incompatibility with noise adding mechanisms, which $\varepsilon$-Gaussian
certifiability is able to overcome optimally.

</details>


### [6] [Near-Optimality of Contrastive Divergence Algorithms](https://arxiv.org/abs/2510.13438)
*Pierre Glaser,Kevin Han Huang,Arthur Gretton*

Main category: stat.ML

TL;DR: 本文对对比散度算法进行了非渐近分析，证明了在正则性假设下，CD算法可以达到参数化速率O(n^{-1/2})，优于之前建立的O(n^{-1/3})渐近收敛率。


<details>
  <summary>Details</summary>
Motivation: 对比散度算法是训练非归一化模型的重要方法，但之前的分析只建立了O(n^{-1/3})的渐近收敛率，本文旨在研究CD算法是否可以达到更快的参数化收敛速率。

Method: 采用非渐近分析方法，在各种数据批处理方案下（包括完全在线和小批量）分析CD算法的收敛性能，并与Cramér-Rao下界进行比较。

Result: 在正则性假设下，CD算法可以达到参数化速率O(n^{-1/2})，且其渐近方差接近Cramér-Rao下界，表明CD算法是接近最优的。

Conclusion: CD算法在适当条件下可以达到参数化收敛速率，并且其统计效率接近理论最优值，这为CD算法的实际应用提供了理论保证。

Abstract: We perform a non-asymptotic analysis of the contrastive divergence (CD)
algorithm, a training method for unnormalized models. While prior work has
established that (for exponential family distributions) the CD iterates
asymptotically converge at an $O(n^{-1 / 3})$ rate to the true parameter of the
data distribution, we show, under some regularity assumptions, that CD can
achieve the parametric rate $O(n^{-1 / 2})$. Our analysis provides results for
various data batching schemes, including the fully online and minibatch ones.
We additionally show that CD can be near-optimal, in the sense that its
asymptotic variance is close to the Cram\'er-Rao lower bound.

</details>


### [7] [Robust Minimax Boosting with Performance Guarantees](https://arxiv.org/abs/2510.13445)
*Santiago Mazuelas,Veronica Alvarez*

Main category: stat.ML

TL;DR: 提出了鲁棒极小极大提升方法(RMBoost)，通过最小化最坏情况错误概率来应对标签噪声，并提供有限样本性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒提升方法虽然对某些类型的标签噪声有理论鲁棒性保证，但在实际噪声类型和有限训练样本下表现不佳，且在无噪声情况下准确率也不理想。

Method: 开发了鲁棒极小极大提升方法，最小化最坏情况错误概率，对一般类型的标签噪声具有鲁棒性。

Result: 实验结果表明RMBoost不仅对标签噪声具有弹性，还能提供强大的分类准确率。

Conclusion: RMBoost方法在应对标签噪声方面表现出色，同时保持了良好的分类性能，为实际应用提供了有效的解决方案。

Abstract: Boosting methods often achieve excellent classification accuracy, but can
experience notable performance degradation in the presence of label noise.
Existing robust methods for boosting provide theoretical robustness guarantees
for certain types of label noise, and can exhibit only moderate performance
degradation. However, previous theoretical results do not account for realistic
types of noise and finite training sizes, and existing robust methods can
provide unsatisfactory accuracies, even without noise. This paper presents
methods for robust minimax boosting (RMBoost) that minimize worst-case error
probabilities and are robust to general types of label noise. In addition, we
provide finite-sample performance guarantees for RMBoost with respect to the
error obtained without noise and with respect to the best possible error (Bayes
risk). The experimental results corroborate that RMBoost is not only resilient
to label noise but can also provide strong classification accuracy.

</details>


### [8] [On the identifiability of causal graphs with multiple environments](https://arxiv.org/abs/2510.13583)
*Francesco Montagna*

Main category: stat.ML

TL;DR: 该论文证明，通过访问结构因果模型的分布和来自两个噪声统计充分不同环境的数据，可以唯一识别因果图。这是首个保证用恒定数量环境和任意非线性机制恢复整个因果图的结果。


<details>
  <summary>Details</summary>
Motivation: 从独立同分布观测数据进行因果发现通常是不适定的问题。本文旨在探索在获得结构因果模型分布和来自两个不同噪声环境的数据时，因果图的唯一可识别性。

Method: 利用结构因果模型的分布和来自两个噪声统计充分不同环境的数据，扩展了独立成分分析(ICA)与因果发现之间的对偶关系。约束条件是噪声项的高斯性，但提出了放松此要求的方法。

Result: 证明了在仅有两个充分不同的噪声环境数据的情况下，可以唯一识别整个因果图，这是首个用恒定数量环境和任意非线性机制保证因果图恢复的结果。

Conclusion: 该研究显著减少了因果发现所需的辅助信息量，相比非线性ICA需要与源数量相同的环境，本文方法仅需两个环境即可实现因果发现。

Abstract: Causal discovery from i.i.d. observational data is known to be generally
ill-posed. We demonstrate that if we have access to the distribution of a
structural causal model, and additional data from only two environments that
sufficiently differ in the noise statistics, the unique causal graph is
identifiable. Notably, this is the first result in the literature that
guarantees the entire causal graph recovery with a constant number of
environments and arbitrary nonlinear mechanisms. Our only constraint is the
Gaussianity of the noise terms; however, we propose potential ways to relax
this requirement. Of interest on its own, we expand on the well-known duality
between independent component analysis (ICA) and causal discovery; recent
advancements have shown that nonlinear ICA can be solved from multiple
environments, at least as many as the number of sources: we show that the same
can be achieved for causal discovery while having access to much less auxiliary
information.

</details>


### [9] [PriorGuide: Test-Time Prior Adaptation for Simulation-Based Inference](https://arxiv.org/abs/2510.13763)
*Yang Yang,Severi Rissanen,Paul E. Chang,Nasrulloh Loka,Daolang Huang,Arno Solin,Markus Heinonen,Luigi Acerbi*

Main category: stat.ML

TL;DR: 提出了PriorGuide技术，用于扩散模型为基础的摊销模拟器推理方法，能够在测试时灵活适应新的先验分布而无需重新训练


<details>
  <summary>Details</summary>
Motivation: 现有的摊销模拟器推理方法受限于训练阶段使用的先验分布，无法灵活适应新的先验信息

Method: 基于扩散模型的摊销推理方法，引入新的指导近似技术，在测试时调整模型以适应新的先验分布

Result: PriorGuide技术成功实现了在不重新训练的情况下，让预训练推理模型适应新的先验分布

Conclusion: PriorGuide增强了预训练推理模型的灵活性，允许用户在训练后整合更新的信息或专家知识

Abstract: Amortized simulator-based inference offers a powerful framework for tackling
Bayesian inference in computational fields such as engineering or neuroscience,
increasingly leveraging modern generative methods like diffusion models to map
observed data to model parameters or future predictions. These approaches yield
posterior or posterior-predictive samples for new datasets without requiring
further simulator calls after training on simulated parameter-data pairs.
However, their applicability is often limited by the prior distribution(s) used
to generate model parameters during this training phase. To overcome this
constraint, we introduce PriorGuide, a technique specifically designed for
diffusion-based amortized inference methods. PriorGuide leverages a novel
guidance approximation that enables flexible adaptation of the trained
diffusion model to new priors at test time, crucially without costly
retraining. This allows users to readily incorporate updated information or
expert knowledge post-training, enhancing the versatility of pre-trained
inference models.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [10] [Trajectory-based real-time pedestrian crash prediction at intersections: A novel non-linear link function for block maxima led Bayesian GEV framework addressing heterogeneous traffic condition](https://arxiv.org/abs/2510.12963)
*Parvez Anowar,Nazmul Haque,Md Asif Raihan,Md Hadiuzzaman*

Main category: stat.AP

TL;DR: 开发了一个实时框架，用于在异质、非车道化交通条件下估计信号交叉口的行人碰撞风险，采用非线性链接函数的贝叶斯广义极值模型，并引入行为归一化的修正碰撞风险公式。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设协变量与参数之间存在线性关系，过度简化了不同道路使用者之间复杂的非单调交互作用，无法准确捕捉交通变异性。

Method: 在贝叶斯广义极值结构中引入非线性链接函数，使用后侵入时间作为替代安全指标，采用块最大值方法的极值理论，通过马尔可夫链蒙特卡罗模拟估计包含线性和非线性链接函数的分层贝叶斯模型。

Result: 采用位置和尺度参数非线性链接函数的模型显著优于线性对应模型，行人速度与碰撞风险呈负相关，机动车流量和速度、行人流量以及非机动车冲突速度呈正相关，修正碰撞风险公式减少了高估，提供了93%置信度的碰撞预测。

Conclusion: 非线性链接函数的集成增强了模型灵活性，捕捉了交通极值的非线性特性，提出的修正碰撞风险指标使碰撞风险估计与混合交通环境中的实际行人行为保持一致，为交通工程师和规划者提供了实用的分析工具。

Abstract: This study develops a real-time framework for estimating pedestrian crash
risk at signalized intersections under heterogeneous, non-lane-based traffic.
Existing approaches often assume linear relationships between covariates and
parameters, oversimplifying the complex, non-monotonic interactions among
different road users. To overcome this, the framework introduces a non-linear
link function within a Bayesian generalized extreme value (GEV) structure to
capture traffic variability more accurately. The framework applies extreme
value theory through the block maxima approach using post-encroachment time as
a surrogate safety measure. A hierarchical Bayesian model incorporating both
linear and non-linear link functions into GEV parameters is estimated using
Markov Chain Monte Carlo simulation. It also introduces a behavior-normalized
Modified Crash Risk (MRC) formula to account for pedestrians' habitual
risk-taking behavior. Seven Bayesian hierarchical models were developed and
compared using deviance information criterion. Models employing non-linear link
functions for the location and scale parameters significantly outperformed
their linear counterparts. The results revealed that pedestrian speed has a
negative relationship with crash risk, while flow and speed of motorized
vehicles, pedestrian flow, and non-motorized vehicles conflicting speed
contribute positively. The MRC formulation reduced overestimation and provided
crash predictions with 93% confidence. The integration of non-linear link
functions enhances model flexibility, capturing the non-linear nature of
traffic extremes. The proposed MRC metric aligns crash risk estimates with
real-world pedestrian behavior in mixed-traffic environments. This framework
offers a practical analytical tool for traffic engineers and planners to design
adaptive signal control and pedestrian safety interventions before crashes
occur.

</details>


### [11] [Surrogate Models to Predict Wave Hydrodynamics on Evolving Landscapes](https://arxiv.org/abs/2510.12986)
*Mohammad Ahmadi Gharehtoragh,David R Johnson*

Main category: stat.AP

TL;DR: 该研究开发了基于深度学习的代理模型来预测沿海风暴中的峰值有效波高，以替代计算昂贵的数值波浪模型，为海岸规划中的洪水风险评估提供高效工具。


<details>
  <summary>Details</summary>
Motivation: 海岸规划需要评估大量热带气旋情景下的洪水风险，但数值波浪模型计算成本高昂，限制了预算受限的规划工作。需要开发高效的代理模型来预测波浪高度。

Method: 使用深度学习构建代理模型，结合景观形态要素（高程、粗糙度、冠层）、全球边界条件（海平面）和热带气旋特征作为预测特征，评估了多种使用场景下的性能。

Result: 在路易斯安那州2023年海岸总体规划案例中，代理模型表现出适合规划级研究的准确性，约89%的网格单元和景观中与数值模型无显著差异，平均均方根误差为0.05-0.06米。

Conclusion: 深度学习代理模型能够准确预测波浪高度，为海岸规划中的概率风险评估提供计算效率高的替代方案，性能因景观和模型类型而异。

Abstract: Coastal planners using probabilistic risk assessments to evaluate structural
flood risk reduction projects may wish to simulate the hydrodynamics associated
with large suites of tropical cyclones in large ensembles of landscapes: with
and without projects' implementation; over decades of their useful lifetimes;
and under multiple scenarios reflecting uncertainty about sea level rise, land
subsidence, and other factors. Wave action can be a substantial contributor to
flood losses and overtopping of structural features like levees and floodwalls,
but numerical methods solving for wave dynamics are computationally expensive,
potentially limiting budget-constrained planning efforts. In this study, we
present and evaluate the performance of deep learning-based surrogate models
for predicting peak significant wave heights under a variety of relevant use
cases: predicting waves with or without modeled peak storm surge as a feature,
predicting wave heights while simultaneously predicting peak storm surge, or
using storm surge predicted by another surrogate model as an input feature. All
models incorporate landscape morphological elements (e.g., elevation,
roughness, canopy) and global boundary conditions (e.g., sea level) in addition
to tropical cyclone characteristics as predictive features to improve accuracy
as landscapes evolve over time. Using simulations from Louisiana's 2023 Coastal
Master Plan as a case study, we demonstrate suitable accuracy of surrogate
models for planning-level studies, with a two-sided Kolmogorov-Smirnov test
indicating no significant difference between significant wave heights generated
by the Simulating Waves Nearshore model and those predicted by our surrogate
models in approximately 89% of grid cells and landscapes evaluated in the
study, with performance varying by landscape and model. On average, the models
produced a root mean squared error of 0.05-0.06 m.

</details>


### [12] [The Impact of Renewable Energy Communities in the Italian Day-Ahead Electricity Market: A Scenario Analysis](https://arxiv.org/abs/2510.13517)
*Maksym Koltunov,Filippo Beltrami,Luigi Grossi,Nicola Blasuttigh*

Main category: stat.AP

TL;DR: 评估意大利可再生能源社区对电力批发市场的经济影响，发现REC部署能增加白天均衡电量、降低批发电价，并减轻配电系统压力。


<details>
  <summary>Details</summary>
Motivation: 研究可再生能源社区对意大利电力批发市场的经济影响，为政策制定提供实证依据。

Method: 采用自下而上的工程方法结合短期经济影响评估，通过映射现有REC特征建立代表性配置模型，生成能源注入和自消费曲线，并运用合成反事实方法分析不同情景。

Result: REC部署在大多数时间增加白天均衡电量，在寒冷月份减少均衡电量，可能降低批发电价，并通过提高自消费减轻配电系统压力。

Conclusion: 可再生能源社区的推广对电力市场有积极影响，既能降低电价又能改善电网运行效率。

Abstract: This paper evaluates the economic impact of Renewable Energy Communities
(RECs) on the Italian wholesale power market. Combining a bottom-up engineering
approach with a short-run economic impact assessment, the study begins by
mapping existing and emerging RECs in Italy. We identify key characteristics of
RECs, such as average installed capacity, institutional profiles of members,
types of renewable systems used, and distribution across Italy's electricity
market zones. This mapping yields representative REC configurations, which are
employed within a bottom-up engineering model to generate energy injection and
self-consumption profiles for different REC prosumer and producer categories
(residential, public, small and medium enterprise, non-profit organization, and
standalone installation), considering the different levels of solar irradiance
in Italy based on latitude. These zonal results, aggregated on an hourly basis,
inform the implementation of the synthetic counterfactual approach, which
develops alternative scenarios (e.g., 5 GW target for REC-driven capacity set
by Italian policy for 2027) to assess the impact of REC-driven injection and
self-consumption on the Italian day-ahead power market. The findings suggest
that REC deployment can increase equilibrium quantities during daylight in most
of the time, while decreasing equilibrium quantities mostly during the cold
months, as electrified heating drives greater self-consumption and offsets
lower grid injections. Both positive and negative effects on equilibrium
quantities suggest that REC deployment also has a potential to reduce wholesale
electricity prices. Moreover, by reducing grid exchanges through higher
self-consumption, REC proliferation can alleviate pressure on the distribution
system.

</details>


### [13] [Model-assisted estimation for MRV: How to boost the economics of SOC sequestration projects without compromising on scientific integrity](https://arxiv.org/abs/2510.13609)
*Ahmad Awad,Erik Scharwächter*

Main category: stat.AP

TL;DR: 评估简单回归估计器(SRE)在土壤有机碳监测中的表现，发现当样本量大于40时，该模型辅助估计方法具有可忽略的偏差和有效的置信区间覆盖，在存在相关辅助变量时可实现30%的精度提升。


<details>
  <summary>Details</summary>
Motivation: 土壤有机碳封存项目需要无偏、精确且成本效益高的MRV系统，需要平衡采样成本与监管框架要求的不确定性减少。模型辅助估计结合了模型预测和概率采样，在保持设计无偏性的同时提高精度。

Method: 通过广泛的模拟研究评估简单回归估计器(SRE)，模拟涵盖不同的SOC库存方差、样本量和模型性能，评估经验偏差、经验置信区间覆盖和相对于Horvitz-Thompson估计器的精度增益。

Result: 当n>40时，无论SOC库存方差如何，SRE都显示出可忽略的偏差和有效的覆盖概率。存在相关辅助变量(r²=0.3)时，SRE比HTE实现30%的精度增益；无相关变量时无增益，但n≥40时性能收敛于HTE。

Conclusion: 模型辅助估计可以在不损害科学严谨性的前提下增强项目经济性。监管机构应允许此类估计器，同时规定最小样本量阈值。项目方应在存在相关辅助变量时常规使用此类估计器。

Abstract: Soil organic carbon (SOC) sequestration projects require unbiased, precise
and cost-effective Monitoring, Reporting, and Verification (MRV) systems that
balance sampling costs against uncertainty deductions imposed by regulatory
frameworks. Design-based estimators guarantee unbiasedness but cannot exploit
auxiliary data. Model-based approaches (VCS Methodology VT0014 v1.0 (2025)) can
improve precision but require independent validation for each project.
Model-assisted estimation offers a robust compromise, combining model
predictions with probability sampling to retain design-based guarantees while
improving precision. We evaluate the scientific integrity and efficiency of the
simple regression estimator (SRE), a well-known model-assisted estimator, via
an extensive simulation study. Our simulations span diverse SOC stock
variances, sample sizes, and model performances. We assess three core
properties: empirical bias, empirical confidence interval coverage, and
precision gain relative to the design-based Horvitz-Thompson estimator (HTE).
Results show negligible bias and valid coverage probabilities for n > 40,
regardless of SOC stock variance. Below this threshold, variance approximations
and normality assumptions yield unreliable uncertainty estimates. With
correlated ancillary variables (r^2 = 0.3), SRE achieves 30% precision gains
over HTE. With uncorrelated variables, no gains are observed, but performance
converges to HTE for n >= 40. Model-assisted estimation can enhance project
economics without compromising scientific rigor. Regulators should permit such
estimators while mandating minimum sample size thresholds. Project proponents
should routinely employ such estimators when correlated ancillary variables
exist. The industry should prioritize the retrieval of high-quality,
project-specific covariates to maximize precision gains and thereby the project
economics.

</details>


### [14] [Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping](https://arxiv.org/abs/2510.13672)
*Marcílio Ferreira dos Santos,Andreza dos Santos Rodrigues de Melo*

Main category: stat.AP

TL;DR: 本研究使用贝叶斯层次时空模型分析巴西累西腓市2015-2024年登革热病例，发现人口密度、家庭规模、滞后降水增加风险，而收入和排水渠道具有保护作用，识别出北部和西部持续高风险区域。


<details>
  <summary>Details</summary>
Motivation: 登革热是巴西主要流行病挑战，存在显著的城市内部不平等，受气候和社会环境因素影响，需要理解其时空分布模式以支持公共卫生决策。

Method: 采用R-INLA实现的贝叶斯层次时空模型，结合BYM2空间结构和RW1时间成分，纳入人口密度、家庭规模、收入、排水渠道、滞后降水和平均温度等协变量。

Result: 模型拟合良好（DIC=65817；WAIC=64506），人口密度和家庭规模对登革热风险有正向影响，收入和渠道存在具有保护作用，滞后降水增加风险，高温呈负相关，识别出北部和西部持续高风险集群。

Conclusion: 贝叶斯模型能够重现历史模式并支持概率预测和早期预警系统，相比传统模型能明确整合不确定性和时空依赖性，为城市健康管理提供可信区间推断。

Abstract: Dengue remains one of Brazil's major epidemiological challenges, marked by
strong intra-urban inequalities and the influence of climatic and
socio-environmental factors. This study analyzed confirmed dengue cases in
Recife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model
implemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal
component. Covariates included population density, household size, income,
drainage channels, lagged precipitation, and mean temperature. Population
density and household size had positive effects on dengue risk, while income
and channel presence were protective. Lagged precipitation increased risk, and
higher temperatures showed an inverse association, suggesting thermal
thresholds for vector activity. The model achieved good fit (DIC=65817;
WAIC=64506) and stable convergence, with moderate residual spatial
autocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019.
Spatio-temporal estimates revealed persistent high-risk clusters in northern
and western Recife, overlapping with areas of higher density and social
vulnerability. Beyond reproducing historical patterns, the Bayesian model
supports probabilistic forecasting and early warning systems. Compared with
classical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty
and spatial-temporal dependence, offering credible interval inference for
decision-making in urban health management.

</details>


### [15] [Macro-Level Correlational Analysis of Mental Disorders: Economy, Education, Society, and Technology Development](https://arxiv.org/abs/2510.13780)
*Yingzhi Tao,Chang Yang*

Main category: stat.AP

TL;DR: 该研究量化了1990-2021年全球27个地区四种精神障碍的年龄分层负担，发现20-39岁年轻人存在显著时空异质性，教育对应低负担，失业对应高负担，经济和技术因素影响时间较短而教育因素影响时间较长。


<details>
  <summary>Details</summary>
Motivation: 通过全球疾病负担数据研究精神障碍与经济社会发展指标的关系，为验证计算心理健康模型和制定干预措施提供人口层面的参考依据。

Method: 使用GBD 2021数据，将年龄标准化残疾调整生命年与18个世界发展指标关联，采用Pearson相关、互信息、Granger因果和最大信息系数等方法评估线性、非线性和滞后依赖关系。

Result: 发现20-39岁年轻人精神障碍负担存在显著时空异质性，教育水平与低负担相关，失业率与高负担相关，经济和技术因素影响时间较短（1-2年），教育因素影响时间较长（3-4年）。

Conclusion: 研究揭示了不同时间尺度上起作用的宏观决定因素，为特定地区和年龄群体的干预措施提供了重要参考，强调了教育在长期心理健康促进中的关键作用。

Abstract: This paper quantifies the age-stratified global burden of four mental
disorders in 27 regions from 1990 to 2021 using GBD 2021. To put it in detail,
it links the age-standardized years of disability adjustment with 18 world
development indicators across economic, educational, social and information
technology sectors. Then, by means of Pearson correlation, mutual information,
Granger causality and maximum information coefficient and other methods, the
linear, nonlinear and lagged dependency relationships were evaluated. After
research, it was found that there is a very prominent spatio-temporal
heterogeneity among young people aged 20 to 39, and the coupling relationship
is stronger. From the overall situation, education corresponds to a low burden.
Unemployment corresponds to a high burden. Through lag analysis, it can be
known that the influence time of economic and technological factors is
relatively short, while that of educational factors is relatively long. These
results highlight the macro determinants that play a role at different time
scales and also provide population-level references for verifying computational
mental health models and for intervention measures in specific regions and for
specific ages.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [16] [The $φ$-PCA Framework: A Unified and Efficiency-Preserving Approach with Robust Variants](https://arxiv.org/abs/2510.13159)
*Hung Hung,Zhi-Yu Jou,Su-Yun Huang,Shinto Eguchi*

Main category: stat.ME

TL;DR: 提出了φ-PCA框架，统一处理鲁棒和分布式PCA问题，通过φ函数聚合局部估计提高排序鲁棒性，其中HM-PCA达到最优鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统PCA对异常值敏感且在分布式环境中受限，无法有效处理现代大规模应用中的挑战。

Method: 使用φ-PCA框架，通过适当的φ函数聚合多个局部估计，增强排序鲁棒性，保持标准PCA的渐近效率。

Result: φ-PCA方法在污染条件下能更准确估计特征子空间，特别是HM-PCA达到最优排序鲁棒性，且鲁棒性随分区数量增加而增强。

Conclusion: φ-PCA的分区聚合原则为开发鲁棒且保持效率的方法提供了通用策略，适用于鲁棒和分布式数据分析。

Abstract: Principal component analysis (PCA) is a fundamental tool in multivariate
statistics, yet its sensitivity to outliers and limitations in distributed
environments restrict its effectiveness in modern large-scale applications. To
address these challenges, we introduce the $\phi$-PCA framework which provides
a unified formulation of robust and distributed PCA. The class of $\phi$-PCA
methods retains the asymptotic efficiency of standard PCA, while aggregating
multiple local estimates using a proper $\phi$ function enhances
ordering-robustness, leading to more accurate eigensubspace estimation under
contamination. Notably, the harmonic mean PCA (HM-PCA), corresponding to the
choice $\phi(u)=u^{-1}$, achieves optimal ordering-robustness and is
recommended for practical use. Theoretical results further show that robustness
increases with the number of partitions, a phenomenon seldom explored in the
literature on robust or distributed PCA. Altogether, the partition-aggregation
principle underlying $\phi$-PCA offers a general strategy for developing robust
and efficiency-preserving methodologies applicable to both robust and
distributed data analysis.

</details>


### [17] [Non-asymptotic goodness-of-fit tests and model selection in valued stochastic blockmodels](https://arxiv.org/abs/2510.13636)
*Félix Almendra-Hernández,Miles Bakenhus,Vishesh Karwa,Mitsunori Ogawa,Sonja Petrović*

Main category: stat.ME

TL;DR: 该论文提出了用于测试非伯努利随机块模型（SBM）拟合优度的方法，特别是针对有限样本测试，并研究了标记SBM的渐近行为。


<details>
  <summary>Details</summary>
Motivation: 传统SBM主要关注伯努利分布，而现实网络数据往往涉及计数或标记的边观测，需要开发适用于非伯努利SBM的拟合优度测试方法。

Method: 推导显式马尔可夫基移动来生成参考分布样本，定义拟合优度统计量，研究标记SBM的渐近行为，并在模拟数据上验证功效和I类错误率。

Result: 方法在模拟数据上表现出良好的性能，应用于生态宿主-寄生虫网络时，在块数选择上得出了与文献不同的结论。

Conclusion: 提出的方法为测试非伯努利SBM的拟合优度提供了有效工具，特别适用于确定节点块成员资格是否影响网络形成，并在实际应用中提供了新的见解。

Abstract: A valued stochastic blockmodel (SBM) is a general way to view networked data
in which nodes are grouped into blocks and links between them are measured by
counts or labels. This family allows for varying dyad sampling schemes, thereby
including the classical, Poisson, and labeled SBMs, as well as those in which
some edge observations are censored. This paper addresses the question of
testing goodness-of-fit of such non-Bernoulli SBMs, focusing in particular on
finite-sample tests. We derive explicit Markov bases moves necessary to
generate samples from reference distributions and define goodness-of-fit
statistics for determining model fit, comparable to those in the literature for
related model families.
  For the labeled SBM, which includes in particular the censored-edge model, we
study the asymptotic behavior of said statistics. One of the main purposes of
testing goodness-of-fit of an SBM is to determine whether block membership of
the nodes influences network formation. Power and Type 1 error rates are
verified on simulated data. Additionally, we discuss the use of asymptotic
results in selecting the number of blocks under the latent-block modeling
assumption. The method derived for Poisson SBM is applied to ecological
networks of host-parasite interactions. Our data analysis conclusions differ in
selecting the number of blocks for the species from previous results in the
literature.

</details>


### [18] [Understanding and Using the Relative Importance Measures Based on Orthonormality Transformation](https://arxiv.org/abs/2510.13389)
*Tien-En Chang,Argon Chen*

Main category: stat.ME

TL;DR: 本文提出了一个广义框架来分析正交变换方法(OTMs)，将其分解为正交化和重新分配两个功能步骤，并通过蒙特卡洛模拟评估各步骤对OTMs性能的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然基于正交变换的相对重要性度量方法(OTMs)能有效近似广义优势指数(GD)，但OTMs的理论基础仍不清楚，需要进一步理解其工作机制。

Method: 提出了将OTM分解为正交化和重新分配两个功能步骤的广义框架，通过蒙特卡洛模拟在不同预测变量相关结构和响应变量分布下评估各步骤的影响。

Result: 发现Johnson的最小变换始终优于其他常见正交化方法，并总结了在不同预测变量相关结构下重新分配方法的性能表现。

Conclusion: 研究提供了对OTMs更深入的理解，并为从业者在不同建模情境中准确衡量变量重要性提供了有价值的指导。

Abstract: A class of relative importance measures based on orthonormality
transformation (OTMs), has been found to effectively approximate the General
Dominance index (GD). In particular, Johnson's Relative Weight (RW) has been
deemed the most successful OTM in the literature. Nevertheless, the theoretical
foundation of the OTMs remains unclear. To further understand the OTMs, we
provide a generalized framework that breaks down the OTM into two functional
steps: orthogonalization and reallocation. To assess the impact of each step on
the performance of OTMs, we conduct extensive Monte Carlo simulations under
various predictors' correlation structures and response variable distributions.
Our findings reveal that Johnson's minimal transformation consistently
outperforms other common orthogonalization methods. We also summarize the
performance of reallocation methods under four scenarios of predictors'
correlation structures in terms of the first principal component and the
variance inflation factor (VIF). This analysis provides guidelines for
selecting appropriate reallocation methods in different scenarios, illustrated
with real-world dataset examples. Our research offers a deeper understanding of
OTMs and provides valuable insights for practitioners seeking to accurately
measure variable importance in various modeling contexts.

</details>


### [19] [Escaping Neal's Funnel: a multi-stage sampling method for hierarchical models](https://arxiv.org/abs/2510.12917)
*Aiden Gundersen,Neil J. Cornish*

Main category: stat.ME

TL;DR: 提出一种分层采样方法来解决Neal漏斗问题，通过多阶段采样避免传统MCMC在贝叶斯层次模型中的采样困难


<details>
  <summary>Details</summary>
Motivation: 贝叶斯层次模型中的Neal漏斗现象导致概率密度呈指数锥形，传统MCMC方法难以高效采样。重参数化或解析边缘化虽然常用但计算成本高

Method: 采用分层采样策略：第一阶段采样广义层次模型以减轻漏斗锐度，第二阶段在约束条件下采样以恢复原始模型的超参数边际分布，可使用归一化流表示第一阶段分布

Result: 该方法能够有效避免Neal漏斗带来的采样挑战，特别适用于重参数化计算昂贵或已有易采样广义模型的情况

Conclusion: 分层执行层次分析可以有效解决Neal漏斗问题，提供了一种替代传统重参数化方法的有效采样策略

Abstract: Neal's funnel refers to an exponential tapering in probability densities
common to Bayesian hierarchical models. Usual sampling methods, such as Markov
Chain Monte Carlo, struggle to efficiently sample the funnel. Reparameterizing
the model or analytically marginalizing local parameters are common techniques
to remedy sampling pathologies in distributions exhibiting Neal's funnel. In
this paper, we show that the challenges of Neal's funnel can be avoided by
performing the hierarchical analysis, well, hierarchically. That is, instead of
sampling all parameters of the hierarchical model jointly, we break the
sampling into multiple stages. The first stage samples a generalized
(higher-dimensional) hierarchical model which is parameterized to lessen the
sharpness of the funnel. The next stage samples from the estimated density of
the first stage, but under a constraint which restricts the sampling to recover
the marginal distributions on the hyper-parameters of the original
(lower-dimensional) hierarchical model. A normalizing flow can be used to
represent the distribution from the first stage, such that it can easily be
sampled from for the second stage of the analysis. This technique is useful
when effective reparameterizations are computationally expensive to calculate,
or a generalized hierarchical model already exists from which it is easy to
sample.

</details>


### [20] [Scalable Bayesian inference for high-dimensional mixed-type multivariate spatial data](https://arxiv.org/abs/2510.13233)
*Arghya Mukherjee,Arnab Hazra,Dootika Vats*

Main category: stat.ME

TL;DR: 提出了一种基于多元高斯过程的贝叶斯空间方法，用于联合建模混合类型的多元空间响应数据，并采用Vecchia近似提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现代技术常产生向量值的混合类型空间响应数据（如二元、计数、连续型组合），但现有方法很少能联合建模此类数据。

Method: 使用潜在多元高斯过程构建贝叶斯空间模型，采用Vecchia近似加速后验推断，使用基于椭圆切片采样的MCMC方法进行推断。

Result: 建立了模型的理论性质（可识别性和协方差结构），通过模拟研究和美国野火数据应用验证了方法的有效性。

Conclusion: 该方法能有效联合建模混合类型的多元空间响应数据，且计算效率高，适用于大规模空间数据分析。

Abstract: Spatial generalized linear mixed-effects methods are popularly used to model
spatially indexed univariate responses. However, with modern technology, it is
common to observe vector-valued mixed-type responses, e.g., a combination of
binary, count, or continuous types, at each location. Methods that allow joint
modeling of such mixed-type multivariate spatial responses are rare. Using
latent multivariate Gaussian processes (GPs), we present a class of Bayesian
spatial methods that can be employed for any combination of exponential family
responses. Since multivariate GP-based methods can suffer from computational
bottlenecks when the number of spatial locations is high, we further employ a
computationally efficient Vecchia approximation for fast posterior inference
and prediction. Key theoretical properties of the proposed model, such as
identifiability and the structure of the induced covariance, are established.
Our approach employs a Markov chain Monte Carlo-based inference method that
utilizes elliptical slice sampling in a blocked Metropolis-within-Gibbs
sampling framework. We illustrate the efficacy of the proposed method through
simulation studies and a real-data application on joint modeling of wildfire
counts and burnt areas across the United States.

</details>


### [21] [Learning Shared and Source-specific Subspaces across Multiple Data Sources for Functional Data](https://arxiv.org/abs/2510.13010)
*Chi Zhang,Peijun Sang,Yingli Qin*

Main category: stat.ME

TL;DR: 提出一种基于投影的数据集成方法，明确分离共享和源特定子空间，用于多源功能数据分析。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，整合多源功能数据提取共享子空间很重要。实践中数据收集遵循源特定协议，直接平均样本协方差算子假设同质性，可能偏差共享和源特定变异模式的恢复。

Method: 首先通过平滑估计源特定投影算子以适应功能数据的非参数性质，然后通过检查所有源的平均投影算子的特征值来隔离共享子空间。对于源特定子空间，将相关源特定协方差估计量重新投影到与估计共享子空间正交的子空间上。

Result: 建立了共享和源特定子空间估计量的渐近性质。广泛的模拟研究证明了该方法在各种设置下的有效性。

Conclusion: 该方法能有效分离共享和源特定子空间，并在空气污染物数据应用中展示了实际效用。

Abstract: In the era of big data, integrating multi-source functional data to extract a
subspace that captures the shared subspace across sources has attracted
considerable attention. In practice, data collection procedures often follow
source-specific protocols. Directly averaging sample covariance operators
across sources implicitly assumes homogeneity, which may bias the recovery of
both shared and source-specific variation patterns. To address this issue, we
propose a projection-based data integration method that explicitly separates
the shared and source-specific subspaces. The method first estimates
source-specific projection operators via smoothing to accommodate the
nonparametric nature of functional data. The shared subspace is then isolated
by examining the eigenvalues of the averaged projection operator across all
sources. If a source-specific subspace is of interest, we re-project the
associated source-specific covariance estimator onto the subspace orthogonal to
the estimated shared subspace, and estimate the source-specific subspace from
the resulting projection. We further establish the asymptotic properties of
both the shared and source-specific subspace estimators. Extensive simulation
studies demonstrate the effectiveness of the proposed method across a wide
range of settings. Finally, we illustrate its practical utility with an example
of air pollutant data.

</details>


### [22] [Edgington's Method for Random-Effects Meta-Analysis Part II: Prediction](https://arxiv.org/abs/2510.13216)
*David Kronthaler,Leonhard Held*

Main category: stat.ME

TL;DR: 本文提出了一种基于置信分布的预测分布方法，用于随机效应元分析中的异质性量化，通过考虑异质性估计的不确定性，提高了预测区间的覆盖准确性。


<details>
  <summary>Details</summary>
Motivation: 传统随机效应元分析在存在显著研究间异质性时，对平均效应的统计推断被认为不足。预测分布能在效应尺度上解释异质性，并提供临床相关的未来事件信息。

Method: 使用Edgington的p值组合方法和广义异质性统计量，通过置信分布构建预测分布，充分考虑异质性估计的不确定性。

Result: 模拟结果显示，当有超过3个研究时，95%预测区间通常能达到名义覆盖水平；在20个或更少研究的场景中，能有效反映效应估计的偏斜性。忽略异质性估计不确定性的方法通常无法达到正确覆盖。

Conclusion: 在随机效应元分析中，考虑异质性估计不确定性的调整是必要的，所提出的预测分布方法能更好地量化异质性并提供准确的预测区间。

Abstract: Statistical inference about the average effect in random-effects
meta-analysis has been considered insufficient in the presence of substantial
between-study heterogeneity. Predictive distributions are well-suited for
quantifying heterogeneity since they are interpretable on the effect scale and
provide clinically relevant information about future events. We construct
predictive distributions accounting for uncertainty through confidence
distributions from Edgington's $p$-value combination method and the generalized
heterogeneity statistic. Simulation results suggest that 95% prediction
intervals typically achieve nominal coverage when more than three studies are
available and effectively reflect skewness in effect estimates in scenarios
with 20 or less studies. Formulations that ignore uncertainty in heterogeneity
estimation typically fail to achieve correct coverage, underscoring the need
for this adjustment in random-effects meta-analysis.

</details>


### [23] [postcard: An R Package for Marginal Effect Estimation with or without Prognostic Score Adjustment](https://arxiv.org/abs/2510.13347)
*Mathias Lerbech Jeppesen,Emilie Højbjerre-Frandsen*

Main category: stat.ME

TL;DR: 开发了一个名为postcard的R包，实现了GLM插件方法，支持预后协变量调整，用于随机临床试验中的边际效应估计和统计功效近似计算。


<details>
  <summary>Details</summary>
Motivation: 在随机临床试验中，通过调整预测性基线协变量来提高治疗效果估计的效率，减少方差，增强统计精度和研究功效。

Method: 基于GLM插件程序，结合预后评分作为调整协变量，实现半参数效率。提供功效近似公式，即使在模型错误设定下也有效。

Result: 开发了postcard R包，具有边际效应估计（带或不带预后调整）和统计功效近似两大核心功能，集成了离散超级学习器构建预后评分和模拟能力。

Conclusion: postcard为统计学家提供了一个实用的工具包，通过示例和模拟展示了其在实践中的应用价值。

Abstract: Covariate adjustment is a widely used technique in randomized clinical trials
(RCTs) for improving the efficiency of treatment effect estimators. By
adjusting for predictive baseline covariates, variance can be reduced,
enhancing statistical precision and study power. Rosenblum and van der Laan
[2010] use the framework of generalized linear models (GLMs) in a plug-in
analysis to show efficiency gains using covariate adjustment for marginal
effect estimation. Recently the use of prognostic scores as adjustment
covariates has gained popularity. Schuler et al. [2022] introduce and validate
the method for continuous endpoints using linear models. Building on this work
H{\o}jbjerre-Frandsen et al. [2025] extends the method proposed by Schuler et
al. [2022] to be used in combination with the GLM plug-in procedure [Rosenblum
and van der Laan, 2010]. This method achieves semi-parametric efficiency under
assumptions of additive treatment effects on the link scale. Additionally,
H{\o}jbjerre-Frandsen et al. [2025] provide a formula for power approximation
which is valid even under model misspecification, enabling realistic sample
size estimation. This article introduces an R package, which implements the GLM
plug-in method with or without PrOgnoSTic CovARiate aDjustment, postcard. The
package has two core features: (1) estimating marginal effects and the variance
hereof (with or without prognostic adjustment) and (2) approximating
statistical power. Functionalities also include integration of the Discrete
Super Learner for constructing prognostic scores and simulation capabilities
for exploring the methods in practice. Through examples and simulations, we
demonstrate postcard as a practical toolkit for statisticians.

</details>


### [24] [A Flexible Partially Linear Single Index Proportional Hazards Regression Model for Multivariate Survival Data](https://arxiv.org/abs/2510.13377)
*Na Lei,Mark A. Wolters,Wenqing He*

Main category: stat.ME

TL;DR: 提出了一种具有多元响应和非线性协变量效应的生存回归模型，扩展了比例风险模型，采用分段常数基线风险函数、copula关联建模和样条单指数结构处理非线性效应。


<details>
  <summary>Details</summary>
Motivation: 解决多元响应和非线性协变量效应的生存回归建模问题，传统比例风险模型无法处理这些复杂情况。

Method: 扩展比例风险模型，使用分段常数基线风险函数、copula建模关联结构，样条单指数结构处理非线性协变量效应，采用全似然方法进行推断。

Result: 模拟研究和Busselton健康研究数据应用表明，该方法能很好地捕捉非线性协变量效应，建模响应间关联具有明显优势。

Conclusion: 所提出的方法在多元响应生存回归中有效处理非线性效应和关联结构，为个体水平生存函数估计提供了可行方案。

Abstract: We address the problem of survival regression modelling with multivariate
responses and nonlinear covariate effects. Our model extends the proportional
hazards model by introducing several weakly-parametric elements: the marginal
baseline hazard functions are expressed as piecewise constants, association is
modelled with copulas, and nonlinear covariate effects are handled by a
single-index structure using a spline. The model permits a full likelihood
approach to inference, making it possible to obtain individual-level survival
or hazard function estimates. Performance of the new model is evaluated through
simulation studies and application to the Busselton health study data. The
results suggest that the proposed method can capture nonlinear covariate
effects well, and that there is benefit to modeling the association between the
correlated responses.

</details>


### [25] [Exact Coordinate Descent for High-Dimensional Regularized Huber Regression](https://arxiv.org/abs/2510.13715)
*Younghoon Kim,Po-Ling Loh,Sumanta Basu*

Main category: stat.ME

TL;DR: 开发了一种用于高维正则化Huber回归的精确坐标下降算法，相比现有方法能更好地处理稀疏模型和病态Hessian矩阵问题，并提出了变量筛选规则加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有复合梯度下降方法未能充分利用稀疏模型优势，而二阶近似方法在协变量高度相关时因Hessian矩阵病态而失效，需要开发更稳健高效的算法。

Method: 基于坐标下降策略，利用内点观测计算边际增量，在偏残差构建的网格上保持导数单调性，并引入变量筛选规则选择性更新变量。

Result: 算法在重尾和高度相关预测变量下表现优异，模拟研究和实际数据应用验证了方法的实用效率和计算增强效果。

Conclusion: 这是首个用于惩罚Huber损失最小化的一阶坐标下降算法，扩展了Lasso的收敛率分析，为高维稳健回归提供了有效解决方案。

Abstract: We develop an exact coordinate descent algorithm for high-dimensional
regularized Huber regression. In contrast to composite gradient descent
methods, our algorithm fully exploits the advantages of coordinate descent when
the underlying model is sparse. Moreover, unlike existing second-order
approximation methods previously introduced in the literature, it remains
effective even when the Hessian becomes ill-conditioned due to high
correlations among covariates drawn from heavy-tailed distributions. The key
idea is that, for each coordinate, marginal increments arise only from inlier
observations, while the derivatives remain monotonically increasing over a grid
constructed from the partial residuals. Building on conventional coordinate
descent strategies, we further propose variable screening rules that
selectively determine which variables to update at each iteration, thereby
accelerating convergence. To the best of our knowledge, this is the first work
to develop a first-order coordinate descent algorithm for penalized Huber loss
minimization. We bound the nonasymptotic convergence rate of the proposed
algorithm by extending arguments developed for the Lasso and formally
characterize the operation of the proposed screening rule. Extensive simulation
studies under heavy-tailed and highly-correlated predictors, together with a
real data application, demonstrate both the practical efficiency of the method
and the benefits of the computational enhancements.

</details>
