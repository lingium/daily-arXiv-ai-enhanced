{"id": "2509.25648", "categories": ["stat.AP", "62P20"], "pdf": "https://arxiv.org/pdf/2509.25648", "abs": "https://arxiv.org/abs/2509.25648", "authors": ["Adel Daoud", "Cindy Conlin", "Connor T. Jerzak"], "title": "Chinese vs. World Bank Development Projects: Insights from Earth Observation and Computer Vision on Wealth Gains in Africa, 2002-2013", "comment": "44 pages", "summary": "Debates about whether development projects improve living conditions persist,\npartly because observational estimates can be biased by incomplete adjustment\nand because reliable outcome data are scarce at the neighborhood level. We\naddress both issues in a continent-scale, sector-specific evaluation of Chinese\nand World Bank projects across 9,899 neighborhoods in 36 African countries\n(2002 to 2013), representative of 88% of the population. First, we use a recent\ndataset that measures living conditions with a machine-learned wealth index\nderived from contemporaneous satellite imagery, yielding a consistent panel of\n6.7 km square mosaics. Second, to strengthen identification, we proxy\nofficials' map-based placement criteria using pre-treatment daytime satellite\nimages and fuse these with rich tabular covariates to estimate funder- and\nsector-specific ATEs via inverse-probability weighting. Incorporating imagery\nsystematically shrinks effects relative to tabular-only models, indicating\nprior work likely overstated benefits. On average, both donors raise wealth,\nwith larger gains for China; sector extremes in our sample include Trade and\nTourism for the World Bank (+6.27 IWI points), and Emergency Response for China\n(+14.32). Assignment-mechanism analyses show World Bank placement is generally\nmore predictable from imagery alone, as well as from tabular covariates. This\nsuggests that Chinese project placements are more driven by non-visible,\npolitical, or event-driven factors than World Bank placements. To probe\nresidual concerns about selection on observables, we also estimate\nwithin-neighborhood (unit) fixed-effects models at a spatial resolution about\n450 times finer than prior fixed effects analyses, leveraging the\ncomputer-vision-imputed IWI panels; these deliver smaller but directionally\nconsistent effects."}
{"id": "2509.26112", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2509.26112", "abs": "https://arxiv.org/abs/2509.26112", "authors": ["Mikkel Meyer Andersen"], "title": "DNA shotgun sequencing evidence: sample-specific and unknown genotyping error probabilities", "comment": null, "summary": "DNA shotgun sequencing evidence is starting to gain a lot of attraction in\nforensic genetics. Methods to correctly interpret such evidence, including\nproperly accounting for sequencing errors, are needed. This paper extends the\nwgsLR model by Andersen et. al. (2025) from only allowing for the same, known\ngenotyping error probability for the two samples (trace sample from unknown\ndonor and reference sample from person of interest), to allowing for different\ngenotyping error probabilities (e.g., from trace hair sample and buccal swab\nreference sample). The model was also extended to be able to integrate out\nunknown genotyping error probabilities if only a prior probability is known.\nThe sensitivity of the model against overdispersion was also investigated and\nit was found that it is very robust against overdispersion in estimating the\ngenotyping error probability. It was also found that integrating out unknown\ngenotyping error probability of the trace sample gave concordant weight of\nevidence under both the hypotheses (first hypothesis that the same individual\nwas the donor of both trace and reference sample as well as the second\nhypothesis that two different individuals were the donors for the trace and\nreference sample). It was found that it is more consevative to use prior\ndistributions with a too small mean rather than a too high mean. The extension\nof the model is implemented in the R package wgsLR."}
{"id": "2509.26348", "categories": ["stat.AP", "62-07", "G.3"], "pdf": "https://arxiv.org/pdf/2509.26348", "abs": "https://arxiv.org/abs/2509.26348", "authors": ["Lizzie Neumann", "Philipp Wittenberg", "Jan Gertheiss"], "title": "Confidence Intervals for Conditional Covariances of Natural Frequencies", "comment": "9 pages, 4 figures, 1 table, url:\n  https://iomac2025.sciencesconf.org/591249/document", "summary": "In structural health monitoring (SHM), sensor measurements are collected, and\ndamage-sensitive features such as natural frequencies are extracted for damage\ndetection. However, these features depend not only on damage but are also\ninfluenced by various confounding factors, including environmental conditions\nand operational parameters. These factors must be identified, and their effects\nmust be removed before further analysis. However, it has been shown that\nconfounding variables may influence the mean and the covariance of the\nextracted features. This is particularly significant since the covariance is an\nessential building block in many damage detection tools. To account for the\ncomplex relationships resulting from the confounding factors, a nonparametric\nkernel approach can be used to estimate a conditional covariance matrix. By\ndoing so, the covariance matrix is allowed to change depending on the\nidentified confounding factor, thus providing a clearer understanding of how,\nfor example, temperature influences the extracted features. This paper presents\ntwo bootstrap-based methods for obtaining confidence intervals for the\nconditional covariances, providing a way to quantify the uncertainty associated\nwith the conditional covariance estimator. A proof-of-concept Monte Carlo study\ncompares the two bootstrap versions proposed and evaluates their effectiveness.\nFinally, the methods are applied to the natural frequency data of the KW51\nrailway bridge near Leuven, Belgium. This real-world application highlights the\npractical implications of the findings. It underscores the importance of\naccurately accounting for confounding factors to generate more reliable\ndiagnostic values with fewer false alarms."}
{"id": "2509.25548", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.25548", "abs": "https://arxiv.org/abs/2509.25548", "authors": ["Dries Reynders", "Doranne Thomassen", "Satrajit Roychoudhury", "Cecilie Delphin Amdal", "Jammbe Z. Musoro", "Willi Sauerbrei", "Saskia le Cessie", "Els Goetghebeur"], "title": "Evaluating treatment effects on longitudinal outcomes with attrition due to death: Methods for a two-dimentional estimand with a case study in Quality of Life", "comment": null, "summary": "When longitudinal outcomes are evaluated in mortal populations, their\nnon-existence after death complicates the analysis and its causal\ninterpretation. Where popular methods often merge longitudinal outcome and\nsurvival into one scale or otherwise try to circumvent the problem of\nmortality, some highly relevant questions require survival to be acknowledged\nas a unique condition. \"\\textit{What are my chances of survival}\" and\n\"\\textit{What can I expect for my condition while still alive}\" reflect the\nintrinsically two-dimensional outcome of survival and longitudinal outcome\nwhile-alive. We define a two-dimensional causal while-alive estimand for a\npoint exposure and compare two methods for estimation in an observational\nsetting. Regression-Standardization models survival and the observed\nlongitudinal outcome before standardizing the latter to a target population\nweighted by its estimated survival. Alternatively, Inverse Probability of\nTreatment and Censoring Weighting weights the observed outcomes twice, to\naccount for censoring and differences in baseline-case-mix. Both approaches\nrely on the same causal identification assumptions, but require different\nmodels to be correctly specified. With its potential to extrapolate,\nRegression-Standardization is more efficient when all assumptions are met. We\nshow finite sample performance in a simulation study and apply the methods to a\ncase study on quality of life in oncology."}
{"id": "2509.25507", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.25507", "abs": "https://arxiv.org/abs/2509.25507", "authors": ["Anirban Chatterjee", "Sayantan Choudhury", "Rohan Hore"], "title": "One-shot Conditional Sampling: MMD meets Nearest Neighbors", "comment": "53 pages, 14 figures, 1 table", "summary": "How can we generate samples from a conditional distribution that we never\nfully observe? This question arises across a broad range of applications in\nboth modern machine learning and classical statistics, including image\npost-processing in computer vision, approximate posterior sampling in\nsimulation-based inference, and conditional distribution modeling in complex\ndata settings. In such settings, compared with unconditional sampling,\nadditional feature information can be leveraged to enable more adaptive and\nefficient sampling. Building on this, we introduce Conditional Generator using\nMMD (CGMMD), a novel framework for conditional sampling. Unlike many\ncontemporary approaches, our method frames the training objective as a simple,\nadversary-free direct minimization problem. A key feature of CGMMD is its\nability to produce conditional samples in a single forward pass of the\ngenerator, enabling practical one-shot sampling with low test-time complexity.\nWe establish rigorous theoretical bounds on the loss incurred when sampling\nfrom the CGMMD sampler, and prove convergence of the estimated distribution to\nthe true conditional distribution. In the process, we also develop a uniform\nconcentration result for nearest-neighbor based functionals, which may be of\nindependent interest. Finally, we show that CGMMD performs competitively on\nsynthetic tasks involving complex conditional densities, as well as on\npractical applications such as image denoising and image super-resolution."}
{"id": "2509.25444", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25444", "abs": "https://arxiv.org/abs/2509.25444", "authors": ["Vladimir Kondratyev", "Alexander Fishkov", "Nikita Kotelevskii", "Mahmoud Hegazy", "Remi Flamary", "Maxim Panov", "Eric Moulines"], "title": "Neural Optimal Transport Meets Multivariate Conformal Prediction", "comment": null, "summary": "We propose a framework for conditional vector quantile regression (CVQR) that\ncombines neural optimal transport with amortized optimization, and apply it to\nmultivariate conformal prediction. Classical quantile regression does not\nextend naturally to multivariate responses, while existing approaches often\nignore the geometry of joint distributions. Our method parametrizes the\nconditional vector quantile function as the gradient of a convex potential\nimplemented by an input-convex neural network, ensuring monotonicity and\nuniform ranks. To reduce the cost of solving high-dimensional variational\nproblems, we introduced amortized optimization of the dual potentials, yielding\nefficient training and faster inference. We then exploit the induced\nmultivariate ranks for conformal prediction, constructing distribution-free\npredictive regions with finite-sample validity. Unlike coordinatewise methods,\nour approach adapts to the geometry of the conditional distribution, producing\ntighter and more informative regions. Experiments on benchmark datasets show\nimproved coverage-efficiency trade-offs compared to baselines, highlighting the\nbenefits of integrating neural optimal transport with conformal prediction."}
{"id": "2509.25688", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.25688", "abs": "https://arxiv.org/abs/2509.25688", "authors": ["Shixuan Wang", "Jing Zhang", "Emily L. Kang", "Bin Zhang"], "title": "PPD-CPP: Pointwise predictive density calibrated-power prior in dynamically borrowing historical information", "comment": null, "summary": "Incorporating historical or real-world data into analyses of treatment\neffects for rare diseases has become increasingly popular. A major challenge,\nhowever, lies in determining the appropriate degree of congruence between\nhistorical and current data. In this study, we devote ourselves to the capacity\nof historical data in replicating the current data, and propose a new\ncongruence measure/estimand $p_{CM}$. $p_{CM}$ quantifies the heterogeneity\nbetween two datasets following the idea of the marginal posterior predictive\n$p$-value, and its asymptotic properties were derived. Building upon $p_{CM}$,\nwe develop the pointwise predictive density calibrated-power prior (PPD-CPP) to\ndynamically leverage historical information. PPD-CPP achieves the borrowing\nconsistency and allows modeling the power parameter either as a fixed scalar or\ncase-specific quantity informed by covariates. Simulation studies were\nconducted to demonstrate the performance of these methods and the methodology\nwas illustrated using the Mother's Gift study and \\textit{Ceriodaphnia dubia}\ntoxicity test."}
{"id": "2509.26175", "categories": ["stat.ML", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.26175", "abs": "https://arxiv.org/abs/2509.26175", "authors": ["Cecilia Secchi", "Giacomo Zanella"], "title": "Spectral gap of Metropolis-within-Gibbs under log-concavity", "comment": "20 pages, 1 figure", "summary": "The Metropolis-within-Gibbs (MwG) algorithm is a widely used Markov Chain\nMonte Carlo method for sampling from high-dimensional distributions when exact\nconditional sampling is intractable. We study MwG with Random Walk Metropolis\n(RWM) updates, using proposal variances tuned to match the target's conditional\nvariances. Assuming the target $\\pi$ is a $d$-dimensional log-concave\ndistribution with condition number $\\kappa$, we establish a spectral gap lower\nbound of order $\\mathcal{O}(1/\\kappa d)$ for the random-scan version of MwG,\nimproving on the previously available $\\mathcal{O}(1/\\kappa^2 d)$ bound. This\nis obtained by developing sharp estimates of the conductance of one-dimensional\nRWM kernels, which can be of independent interest. The result shows that MwG\ncan mix substantially faster with variance-adaptive proposals and that its\nmixing performance is just a constant factor worse than that of the exact Gibbs\nsampler, thus providing theoretical support to previously observed empirical\nbehavior."}
{"id": "2509.25481", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25481", "abs": "https://arxiv.org/abs/2509.25481", "authors": ["Kevin Jiang", "Edgar Dobriban"], "title": "Fair Classification by Direct Intervention on Operating Characteristics", "comment": null, "summary": "We develop new classifiers under group fairness in the attribute-aware\nsetting for binary classification with multiple group fairness constraints\n(e.g., demographic parity (DP), equalized odds (EO), and predictive parity\n(PP)). We propose a novel approach, applicable to linear fractional\nconstraints, based on directly intervening on the operating characteristics of\na pre-trained base classifier, by (i) identifying optimal operating\ncharacteristics using the base classifier's group-wise ROC convex hulls and\n(ii) post-processing the base classifier to match those targets. As practical\npost-processors, we consider randomizing a mixture of group-wise thresholding\nrules subject to minimizing the expected number of interventions. We further\nextend our approach to handle multiple protected attributes and multiple linear\nfractional constraints. On standard datasets (COMPAS and ACSIncome), our\nmethods simultaneously satisfy approximate DP, EO, and PP with few\ninterventions and a near-oracle drop in accuracy; comparing favorably to\nprevious methods."}
{"id": "2509.25419", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2509.25419", "abs": "https://arxiv.org/abs/2509.25419", "authors": ["Haziq Jamil", "Yves Rosseel", "Oliver Kemp", "Ioannis Kosmidis"], "title": "Bias-Reduced Estimation of Structural Equation Models", "comment": null, "summary": "Finite-sample bias is a pervasive challenge in the estimation of structural\nequation models (SEMs), especially when sample sizes are small or measurement\nreliability is low. A range of methods have been proposed to improve\nfinite-sample bias in the SEM literature, ranging from analytic bias\ncorrections to resampling-based techniques, with each carrying trade-offs in\nscope, computational burden, and statistical performance. We apply the\nreduced-bias M-estimation framework (RBM, Kosmidis & Lunardon, 2024, J. R.\nStat. Soc. Series B Stat. Methodol.) to SEMs. The RBM framework is attractive\nas it requires only first- and second-order derivatives of the log-likelihood,\nwhich renders it both straightforward to implement, and computationally more\nefficient compared to resampling-based alternatives such as bootstrap and\njackknife. It is also robust to departures from modelling assumptions. Through\nextensive simulations studies under a range of experimental conditions, we\nillustrate that RBM estimators consistently reduce mean bias in the estimation\nof SEMs without inflating mean squared error. They also deliver improvements in\nboth median bias and inference relative to maximum likelihood estimators, while\nmaintaining robustness under non-normality. Our findings suggest that RBM\noffers a promising, practical, and broadly applicable tool for mitigating bias\nin the estimation of SEMs, particularly in small-sample research contexts."}
{"id": "2509.25708", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.25708", "abs": "https://arxiv.org/abs/2509.25708", "authors": ["Saskia Comess", "Daniel E Ho", "Joshua L Warren"], "title": "Modeling Spatial Heterogeneity in Exposure Buffers and Risk: A Hierarchical Bayesian Approach", "comment": "Submitted to the Journal of the Royal Statistical Society, Series C", "summary": "Place-based epidemiology studies often rely on circular buffers to define\n\"exposure\" to spatially distributed risk factors, where the buffer radius\nrepresents a threshold beyond which exposure does not influence the outcome of\ninterest. This approach is popular due to its simplicity and alignment with\npublic health policies. However, buffer radii are often chosen relatively\narbitrarily and assumed constant across the spatial domain. This may result in\nsuboptimal statistical inference if these modeling choices are incorrect. To\naddress this, we develop SVBR (Spatially-Varying Buffer Radii), a flexible\nhierarchical Bayesian spatial change points approach that treats buffer radii\nas unknown parameters and allows both radii and exposure effects to vary\nspatially. Through simulations, we find that SVBR improves estimation and\ninference for key model parameters compared to traditional methods. We also\napply SVBR to study healthcare access in Madagascar, finding that proximity to\nhealthcare facilities generally increases antenatal care usage, with clear\nspatial variation in this relationship. By relaxing rigid assumptions about\nbuffer characteristics, our method offers a flexible, data-driven approach to\naccurately defining exposure and quantifying its impact. The newly developed\nmethods are available in the R package EpiBuffer."}
{"id": "2509.25507", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.25507", "abs": "https://arxiv.org/abs/2509.25507", "authors": ["Anirban Chatterjee", "Sayantan Choudhury", "Rohan Hore"], "title": "One-shot Conditional Sampling: MMD meets Nearest Neighbors", "comment": "53 pages, 14 figures, 1 table", "summary": "How can we generate samples from a conditional distribution that we never\nfully observe? This question arises across a broad range of applications in\nboth modern machine learning and classical statistics, including image\npost-processing in computer vision, approximate posterior sampling in\nsimulation-based inference, and conditional distribution modeling in complex\ndata settings. In such settings, compared with unconditional sampling,\nadditional feature information can be leveraged to enable more adaptive and\nefficient sampling. Building on this, we introduce Conditional Generator using\nMMD (CGMMD), a novel framework for conditional sampling. Unlike many\ncontemporary approaches, our method frames the training objective as a simple,\nadversary-free direct minimization problem. A key feature of CGMMD is its\nability to produce conditional samples in a single forward pass of the\ngenerator, enabling practical one-shot sampling with low test-time complexity.\nWe establish rigorous theoretical bounds on the loss incurred when sampling\nfrom the CGMMD sampler, and prove convergence of the estimated distribution to\nthe true conditional distribution. In the process, we also develop a uniform\nconcentration result for nearest-neighbor based functionals, which may be of\nindependent interest. Finally, we show that CGMMD performs competitively on\nsynthetic tasks involving complex conditional densities, as well as on\npractical applications such as image denoising and image super-resolution."}
{"id": "2509.26385", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2509.26385", "abs": "https://arxiv.org/abs/2509.26385", "authors": ["Zejin Gao", "Ksheera Sagar", "Anindya Bhadra"], "title": "An Order of Magnitude Time Complexity Reduction for Gaussian Graphical Model Posterior Sampling Using a Reverse Telescoping Block Decomposition", "comment": null, "summary": "We consider the problem of fully Bayesian posterior estimation and\nuncertainty quantification in undirected Gaussian graphical models via Markov\nchain Monte Carlo (MCMC) under recently-developed element-wise graphical\npriors, such as the graphical horseshoe. Unlike the conjugate Wishart family,\nthese priors are non-conjugate; but have the advantage that they naturally\nallow one to encode a prior belief of sparsity in the off-diagonal elements of\nthe precision matrix, without imposing a structure on the entire matrix.\nUnfortunately, for a graph with $p$ nodes and with $n$ samples, the\nstate-of-the-art MCMC approaches for the element-wise priors achieve a per\niteration complexity of $O(p^4),$ which is prohibitive when $p\\gg n$. In this\nregime, we develop a suitably reparameterized MCMC with per iteration\ncomplexity of $O(p^3)$, providing a one-order of magnitude improvement, and\nconsequently bringing the computational cost at par with the conjugate Wishart\nfamily, which is also $O(p^3)$ due to a use of the classical Bartlett\ndecomposition, but this decomposition does not apply outside the Wishart\nfamily. Importantly, the proposed benefit is obtained solely due to our\nreparameterization in an MCMC scheme targeting the true posterior, that\nreverses the recently developed telescoping block decomposition of Bhadra et\nal. (2024), in a suitable sense. There is no variational or any other\napproximate Bayesian computation scheme considered in this paper that\ncompromises targeting the true posterior. Simulations and the analysis of a\nbreast cancer data set confirm both the correctness and better algorithmic\nscaling of the proposed reverse telescoping sampler."}
{"id": "2509.25295", "categories": ["stat.ME", "62G15, 62G08, 62C12, 62H30, 62G20", "G.3; I.5.1; I.5.2"], "pdf": "https://arxiv.org/pdf/2509.25295", "abs": "https://arxiv.org/abs/2509.25295", "authors": ["Faruk Alpay", "Taylan Alpay"], "title": "Calibrated Counterfactual Conformal Fairness ($C^3F$): Post-hoc, Shift-Aware Coverage Parity via Conformal Prediction and Counterfactual Regularization", "comment": "5 pages", "summary": "We present Calibrated Counterfactual Conformal Fairness ($C^3F$), a post-hoc\nprocedure that targets group-conditional coverage parity under covariate shift.\n$C^3F$ combines importance-weighted conformal calibration with a counterfactual\nregularizer based on path-specific effects in a structural causal model. The\nmethod estimates group-specific nonconformity quantiles using likelihood-ratio\nweights so that coverage degrades gracefully with the second moment of the\nweights. We derive finite-sample lower bounds on group-wise coverage and a\nbound on the equalized conditional coverage gap, and we show first-order\ncontrol of a counterfactual coverage-parity surrogate via smooth threshold\nregularization. The approach is model-agnostic, label-efficient, and deployable\nwithout retraining. Empirical evaluations on standard classification benchmarks\ndemonstrate improved group-conditional coverage and competitive efficiency\nrelative to shift-aware and fairness-oriented conformal baselines. We discuss\npractical considerations, including partial availability of sensitive\nattributes and robustness to structural causal misspecification."}
{"id": "2509.26162", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.26162", "abs": "https://arxiv.org/abs/2509.26162", "authors": ["Prithul Chaturvedi", "Himanshu Pokhriyal"], "title": "Parameter estimation of the four-parameter Harris extended Weibull distribution with applications to real-life data", "comment": null, "summary": "This paper explores the extension of the classical two-parameter Weibull\ndistribution to a four-parameter Harris extended Weibull (HEW) distribution.\nThe flexibility of this probability distribution is illustrated by the varying\nshapes of HEW density function. Estimation of HEW parameters is explored using\nestimation methods such as the least-squares, maximum product of spacings, and\nminimum distance method. We provide Bayesian inference on the random parameters\nof the HEW distribution using Metropolis-Hastings algorithm to sample from the\njoint posterior distribution. Performance of the estimation methods is assessed\nusing extensive simulations. The applicability of the distribution is\ndemonstrated against three variants of the Weibull distribution on three\nreal-life datasets."}
{"id": "2509.25588", "categories": ["stat.ML", "cs.LG", "stat.ME", "62H30, 62G05, 62P10", "I.5.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.25588", "abs": "https://arxiv.org/abs/2509.25588", "authors": ["Yishu Wei", "Wen-Yee Lee", "George Ekow Quaye", "Xiaogang Su"], "title": "Conservative Decisions with Risk Scores", "comment": "22 pages plus a supplement with 3 pages", "summary": "In binary classification applications, conservative decision-making that\nallows for abstention can be advantageous. To this end, we introduce a novel\napproach that determines the optimal cutoff interval for risk scores, which can\nbe directly available or derived from fitted models. Within this interval, the\nalgorithm refrains from making decisions, while outside the interval,\nclassification accuracy is maximized. Our approach is inspired by support\nvector machines (SVM), but differs in that it minimizes the classification\nmargin rather than maximizing it. We provide the theoretical optimal solution\nto this problem, which holds important practical implications. Our proposed\nmethod not only supports conservative decision-making but also inherently\nresults in a risk-coverage curve. Together with the area under the curve (AUC),\nthis curve can serve as a comprehensive performance metric for evaluating and\ncomparing classifiers, akin to the receiver operating characteristic (ROC)\ncurve. To investigate and illustrate our approach, we conduct both simulation\nstudies and a real-world case study in the context of diagnosing prostate\ncancer."}
{"id": "2509.25419", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2509.25419", "abs": "https://arxiv.org/abs/2509.25419", "authors": ["Haziq Jamil", "Yves Rosseel", "Oliver Kemp", "Ioannis Kosmidis"], "title": "Bias-Reduced Estimation of Structural Equation Models", "comment": null, "summary": "Finite-sample bias is a pervasive challenge in the estimation of structural\nequation models (SEMs), especially when sample sizes are small or measurement\nreliability is low. A range of methods have been proposed to improve\nfinite-sample bias in the SEM literature, ranging from analytic bias\ncorrections to resampling-based techniques, with each carrying trade-offs in\nscope, computational burden, and statistical performance. We apply the\nreduced-bias M-estimation framework (RBM, Kosmidis & Lunardon, 2024, J. R.\nStat. Soc. Series B Stat. Methodol.) to SEMs. The RBM framework is attractive\nas it requires only first- and second-order derivatives of the log-likelihood,\nwhich renders it both straightforward to implement, and computationally more\nefficient compared to resampling-based alternatives such as bootstrap and\njackknife. It is also robust to departures from modelling assumptions. Through\nextensive simulations studies under a range of experimental conditions, we\nillustrate that RBM estimators consistently reduce mean bias in the estimation\nof SEMs without inflating mean squared error. They also deliver improvements in\nboth median bias and inference relative to maximum likelihood estimators, while\nmaintaining robustness under non-normality. Our findings suggest that RBM\noffers a promising, practical, and broadly applicable tool for mitigating bias\nin the estimation of SEMs, particularly in small-sample research contexts."}
{"id": "2509.25599", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25599", "abs": "https://arxiv.org/abs/2509.25599", "authors": ["Ruolin Meng", "Ming-Yu Chung", "Dhanajit Brahma", "Ricardo Henao", "Lawrence Carin"], "title": "Coupling Generative Modeling and an Autoencoder with the Causal Bridge", "comment": "Accepted to NeurIPS 2025", "summary": "We consider inferring the causal effect of a treatment (intervention) on an\noutcome of interest in situations where there is potentially an unobserved\nconfounder influencing both the treatment and the outcome. This is achievable\nby assuming access to two separate sets of control (proxy) measurements\nassociated with treatment and outcomes, which are used to estimate treatment\neffects through a function termed the em causal bridge (CB). We present a new\ntheoretical perspective, associated assumptions for when estimating treatment\neffects with the CB is feasible, and a bound on the average error of the\ntreatment effect when the CB assumptions are violated. From this new\nperspective, we then demonstrate how coupling the CB with an autoencoder\narchitecture allows for the sharing of statistical strength between observed\nquantities (proxies, treatment, and outcomes), thus improving the quality of\nthe CB estimates. Experiments on synthetic and real-world data demonstrate the\neffectiveness of the proposed approach in relation to the state-of-the-art\nmethodology for proxy measurements."}
{"id": "2509.25527", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25527", "abs": "https://arxiv.org/abs/2509.25527", "authors": ["Hanying Jiang", "Kris Sankaran", "Yinqiu He"], "title": "Joint Adaptive Penalty for Unbalanced Mediation Pathways", "comment": null, "summary": "Mediation analysis has been widely used to investigate how a treatment\ninfluences an outcome through intermediate variables, known as mediators.\nAnalyzing a mediation mechanism typically requires assessing multiple model\nparameters that characterize distinct pathwise effects. Classical methods that\nestimate these parameters individually can be inefficient, particularly when\nthe underlying pathwise effects exhibit substantial imbalance. To address this\nchallenge, this work proposes a new joint adaptive penalty that integrates\ninformation across entire mediation mechanisms, thereby enhancing both\nparameter estimation and pathway selection. We establish theoretical guarantees\nfor the proposed method under an asymptotic framework and conduct extensive\nnumerical studies to demonstrate its superior performance in scenarios with\nunbalanced mediation pathways."}
{"id": "2509.25630", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.25630", "abs": "https://arxiv.org/abs/2509.25630", "authors": ["Xiaojie Wang", "Bin Yang"], "title": "When Langevin Monte Carlo Meets Randomization: Non-asymptotic Error Bounds beyond Log-Concavity and Gradient Lipschitzness", "comment": null, "summary": "Efficient sampling from complex and high dimensional target distributions\nturns out to be a fundamental task in diverse disciplines such as scientific\ncomputing, statistics and machine learning. In this paper, we revisit the\nrandomized Langevin Monte Carlo (RLMC) for sampling from high dimensional\ndistributions without log-concavity. Under the gradient Lipschitz condition and\nthe log-Sobolev inequality, we prove a uniform-in-time error bound in\n$\\mathcal{W}_2$-distance of order $O(\\sqrt{d}h)$ for the RLMC sampling\nalgorithm, which matches the best one in the literature under the log-concavity\ncondition. Moreover, when the gradient of the potential $U$ is non-globally\nLipschitz with superlinear growth, modified RLMC algorithms are proposed and\nanalyzed, with non-asymptotic error bounds established. To the best of our\nknowledge, the modified RLMC algorithms and their non-asymptotic error bounds\nare new in the non-globally Lipschitz setting."}
{"id": "2509.25548", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.25548", "abs": "https://arxiv.org/abs/2509.25548", "authors": ["Dries Reynders", "Doranne Thomassen", "Satrajit Roychoudhury", "Cecilie Delphin Amdal", "Jammbe Z. Musoro", "Willi Sauerbrei", "Saskia le Cessie", "Els Goetghebeur"], "title": "Evaluating treatment effects on longitudinal outcomes with attrition due to death: Methods for a two-dimentional estimand with a case study in Quality of Life", "comment": null, "summary": "When longitudinal outcomes are evaluated in mortal populations, their\nnon-existence after death complicates the analysis and its causal\ninterpretation. Where popular methods often merge longitudinal outcome and\nsurvival into one scale or otherwise try to circumvent the problem of\nmortality, some highly relevant questions require survival to be acknowledged\nas a unique condition. \"\\textit{What are my chances of survival}\" and\n\"\\textit{What can I expect for my condition while still alive}\" reflect the\nintrinsically two-dimensional outcome of survival and longitudinal outcome\nwhile-alive. We define a two-dimensional causal while-alive estimand for a\npoint exposure and compare two methods for estimation in an observational\nsetting. Regression-Standardization models survival and the observed\nlongitudinal outcome before standardizing the latter to a target population\nweighted by its estimated survival. Alternatively, Inverse Probability of\nTreatment and Censoring Weighting weights the observed outcomes twice, to\naccount for censoring and differences in baseline-case-mix. Both approaches\nrely on the same causal identification assumptions, but require different\nmodels to be correctly specified. With its potential to extrapolate,\nRegression-Standardization is more efficient when all assumptions are met. We\nshow finite sample performance in a simulation study and apply the methods to a\ncase study on quality of life in oncology."}
{"id": "2509.25741", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25741", "abs": "https://arxiv.org/abs/2509.25741", "authors": ["Kento Kuwataka", "Taiji Suzuki"], "title": "Test time training enhances in-context learning of nonlinear functions", "comment": "Under review at ICLR 2026. 36 pages, 2 figures, appendix included", "summary": "Test-time training (TTT) enhances model performance by explicitly updating\ndesignated parameters prior to each prediction to adapt to the test data. While\nTTT has demonstrated considerable empirical success, its theoretical\nunderpinnings remain limited, particularly for nonlinear models. In this paper,\nwe investigate the combination of TTT with in-context learning (ICL), where the\nmodel is given a few examples from the target distribution at inference time.\nWe analyze this framework in the setting of single-index models\n$y=\\sigma_*(\\langle \\beta, \\mathbf{x} \\rangle)$, where the feature vector\n$\\beta$ is drawn from a hidden low-dimensional subspace. For single-layer\ntransformers trained with gradient-based algorithms and adopting TTT, we\nestablish an upper bound on the prediction risk. Our theory reveals that TTT\nenables the single-layer transformers to adapt to both the feature vector\n$\\beta$ and the link function $\\sigma_*$, which vary across tasks. This creates\na sharp contrast with ICL alone, which is theoretically difficult to adapt to\nshifts in the link function. Moreover, we provide the convergence rate with\nrespect to the data length, showing the predictive error can be driven\narbitrarily close to the noise level as the context size and the network width\ngrow."}
{"id": "2509.25688", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.25688", "abs": "https://arxiv.org/abs/2509.25688", "authors": ["Shixuan Wang", "Jing Zhang", "Emily L. Kang", "Bin Zhang"], "title": "PPD-CPP: Pointwise predictive density calibrated-power prior in dynamically borrowing historical information", "comment": null, "summary": "Incorporating historical or real-world data into analyses of treatment\neffects for rare diseases has become increasingly popular. A major challenge,\nhowever, lies in determining the appropriate degree of congruence between\nhistorical and current data. In this study, we devote ourselves to the capacity\nof historical data in replicating the current data, and propose a new\ncongruence measure/estimand $p_{CM}$. $p_{CM}$ quantifies the heterogeneity\nbetween two datasets following the idea of the marginal posterior predictive\n$p$-value, and its asymptotic properties were derived. Building upon $p_{CM}$,\nwe develop the pointwise predictive density calibrated-power prior (PPD-CPP) to\ndynamically leverage historical information. PPD-CPP achieves the borrowing\nconsistency and allows modeling the power parameter either as a fixed scalar or\ncase-specific quantity informed by covariates. Simulation studies were\nconducted to demonstrate the performance of these methods and the methodology\nwas illustrated using the Mother's Gift study and \\textit{Ceriodaphnia dubia}\ntoxicity test."}
{"id": "2509.25783", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.25783", "abs": "https://arxiv.org/abs/2509.25783", "authors": ["Anil Kamber", "Rahul Parhi"], "title": "Sharpness of Minima in Deep Matrix Factorization: Exact Expressions", "comment": "18 pages, 7 figures", "summary": "Understanding the geometry of the loss landscape near a minimum is key to\nexplaining the implicit bias of gradient-based methods in non-convex\noptimization problems such as deep neural network training and deep matrix\nfactorization. A central quantity to characterize this geometry is the maximum\neigenvalue of the Hessian of the loss, which measures the sharpness of the\nlandscape. Currently, its precise role has been obfuscated because no exact\nexpressions for this sharpness measure were known in general settings. In this\npaper, we present the first exact expression for the maximum eigenvalue of the\nHessian of the squared-error loss at any minimizer in general overparameterized\ndeep matrix factorization (i.e., deep linear neural network training) problems,\nresolving an open question posed by Mulayoff & Michaeli (2020). To complement\nour theory, we empirically investigate an escape phenomenon observed during\ngradient-based training near a minimum that crucially relies on our exact\nexpression of the sharpness."}
{"id": "2509.25708", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.25708", "abs": "https://arxiv.org/abs/2509.25708", "authors": ["Saskia Comess", "Daniel E Ho", "Joshua L Warren"], "title": "Modeling Spatial Heterogeneity in Exposure Buffers and Risk: A Hierarchical Bayesian Approach", "comment": "Submitted to the Journal of the Royal Statistical Society, Series C", "summary": "Place-based epidemiology studies often rely on circular buffers to define\n\"exposure\" to spatially distributed risk factors, where the buffer radius\nrepresents a threshold beyond which exposure does not influence the outcome of\ninterest. This approach is popular due to its simplicity and alignment with\npublic health policies. However, buffer radii are often chosen relatively\narbitrarily and assumed constant across the spatial domain. This may result in\nsuboptimal statistical inference if these modeling choices are incorrect. To\naddress this, we develop SVBR (Spatially-Varying Buffer Radii), a flexible\nhierarchical Bayesian spatial change points approach that treats buffer radii\nas unknown parameters and allows both radii and exposure effects to vary\nspatially. Through simulations, we find that SVBR improves estimation and\ninference for key model parameters compared to traditional methods. We also\napply SVBR to study healthcare access in Madagascar, finding that proximity to\nhealthcare facilities generally increases antenatal care usage, with clear\nspatial variation in this relationship. By relaxing rigid assumptions about\nbuffer characteristics, our method offers a flexible, data-driven approach to\naccurately defining exposure and quantifying its impact. The newly developed\nmethods are available in the R package EpiBuffer."}
{"id": "2509.25802", "categories": ["stat.ML", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25802", "abs": "https://arxiv.org/abs/2509.25802", "authors": ["Yanan Zhao", "Feng Ji", "Xingchao Jian", "Wee Peng Tay"], "title": "Graph Distribution-valued Signals: A Wasserstein Space Perspective", "comment": "Submitted to ICASSP 2026", "summary": "We introduce a novel framework for graph signal processing (GSP) that models\nsignals as graph distribution-valued signals (GDSs), which are probability\ndistributions in the Wasserstein space. This approach overcomes key limitations\nof classical vector-based GSP, including the assumption of synchronous\nobservations over vertices, the inability to capture uncertainty, and the\nrequirement for strict correspondence in graph filtering. By representing\nsignals as distributions, GDSs naturally encode uncertainty and stochasticity,\nwhile strictly generalizing traditional graph signals. We establish a\nsystematic dictionary mapping core GSP concepts to their GDS counterparts,\ndemonstrating that classical definitions are recovered as special cases. The\neffectiveness of the framework is validated through graph filter learning for\nprediction tasks, supported by experimental results."}
{"id": "2509.25860", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25860", "abs": "https://arxiv.org/abs/2509.25860", "authors": ["Alexander Mozdzen", "Timothy Wertz", "Maria De Iorio", "Andrea Cremaschi", "Gregor Kastner", "Johan Eriksson"], "title": "Repulsive mixtures via the sparsity-inducing partition prior", "comment": null, "summary": "We introduce a novel prior distribution for modelling the weights in mixture\nmodels based on a generalisation of the Dirichlet distribution, the Selberg\nDirichlet distribution. This distribution contains a repulsive term, which\nnaturally penalises values that lie close to each other on the simplex, thus\nencouraging few dominating clusters. The repulsive behaviour induces additional\nsparsity on the number of components. We refer to this construction as\nsparsity-inducing partition (SIP) prior. By highlighting differences with the\nconventional Dirichlet distribution, we present relevant properties of the SIP\nprior and demonstrate their implications across a variety of mixture models,\nincluding finite mixtures with a fixed or random number of components, as well\nas repulsive mixtures. We propose an efficient posterior sampling algorithm and\nvalidate our model through an extensive simulation study as well as an\napplication to a biomedical dataset describing children's Body Mass Index and\neating behaviour."}
{"id": "2509.26005", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26005", "abs": "https://arxiv.org/abs/2509.26005", "authors": ["Rui-Yang Zhang", "Henry B. Moss", "Lachlan Astfalck", "Edward Cripps", "David S. Leslie"], "title": "BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields", "comment": null, "summary": "We introduce a formal active learning methodology for guiding the placement\nof Lagrangian observers to infer time-dependent vector fields -- a key task in\noceanography, marine science, and ocean engineering -- using a physics-informed\nspatio-temporal Gaussian process surrogate model. The majority of existing\nplacement campaigns either follow standard `space-filling' designs or\nrelatively ad-hoc expert opinions. A key challenge to applying principled\nactive learning in this setting is that Lagrangian observers are continuously\nadvected through the vector field, so they make measurements at different\nlocations and times. It is, therefore, important to consider the likely future\ntrajectories of placed observers to account for the utility of candidate\nplacement locations. To this end, we present BALLAST: Bayesian Active Learning\nwith Look-ahead Amendment for Sea-drifter Trajectories. We observe noticeable\nbenefits of BALLAST-aided sequential observer placement strategies on both\nsynthetic and high-fidelity ocean current models."}
{"id": "2509.25957", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25957", "abs": "https://arxiv.org/abs/2509.25957", "authors": ["Wenhui Wu", "Changchun Shang", "Jianhua Zhao", "Xuan Ma", "Yue Wang"], "title": "Highly robust factored principal component analysis for matrix-valued outlier accommodation and explainable detection via matrix minimum covariance determinant", "comment": null, "summary": "Principal component analysis (PCA) is a classical and widely used method for\ndimensionality reduction, with applications in data compression, computer\nvision, pattern recognition, and signal processing. However, PCA is designed\nfor vector-valued data and encounters two major challenges when applied to\nmatrix-valued data with heavy-tailed distributions or outliers: (1)\nvectorization disrupts the intrinsic matrix structure, leading to information\nloss and the curse of dimensionality, and (2) PCA is highly sensitive to\noutliers. Factored PCA (FPCA) addresses the first issue through probabilistic\nmodeling, using a matrix normal distribution that explicitly represents row and\ncolumn covariances via a separable covariance structure, thereby preserving the\ntwo-way dependency and matrix form of the data. Building on FPCA, we propose\nhighly robust FPCA (HRFPCA), a robust extension that replaces maximum\nlikelihood estimators with the matrix minimum covariance determinant (MMCD)\nestimators. This modification enables HRFPCA to retain FPCA's ability to model\nmatrix-valued data while achieving a breakdown point close to 50\\%,\nsubstantially improving resistance to outliers. Furthermore, HRFPCA produces\nthe score--orthogonal distance analysis (SODA) plot, which effectively\nvisualizes and classifies matrix-valued outliers. Extensive simulations and\nreal-data analyses demonstrate that HRFPCA consistently outperforms competing\nmethods in robustness and outlier detection, underscoring its effectiveness and\nbroad applicability."}
{"id": "2509.26149", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26149", "abs": "https://arxiv.org/abs/2509.26149", "authors": ["Damien Rouchouse", "Antoine Gonon", "Rémi Gribonval", "Benjamin Guedj"], "title": "Non-Vacuous Generalization Bounds: Can Rescaling Invariances Help?", "comment": null, "summary": "A central challenge in understanding generalization is to obtain non-vacuous\nguarantees that go beyond worst-case complexity over data or weight space.\nAmong existing approaches, PAC-Bayes bounds stand out as they can provide\ntight, data-dependent guarantees even for large networks. However, in ReLU\nnetworks, rescaling invariances mean that different weight distributions can\nrepresent the same function while leading to arbitrarily different PAC-Bayes\ncomplexities. We propose to study PAC-Bayes bounds in an invariant, lifted\nrepresentation that resolves this discrepancy. This paper explores both the\nguarantees provided by this approach (invariance, tighter bounds via data\nprocessing) and the algorithmic aspects of KL-based rescaling-invariant\nPAC-Bayes bounds."}
{"id": "2509.26029", "categories": ["stat.ME", "37M10, 62H30, 62L86, 62P05, 62P35"], "pdf": "https://arxiv.org/pdf/2509.26029", "abs": "https://arxiv.org/abs/2509.26029", "authors": ["Federico P. Cortese", "Antonio Pievatolo", "Elisa Maria Alessi"], "title": "Fuzzy Jump Models for Soft and Hard Clustering of Multivariate Time Series Data", "comment": null, "summary": "Statistical jump models have been recently introduced to detect persistent\nregimes by clustering temporal features and discouraging frequent regime\nchanges. However, they are limited to hard clustering and thereby do not\naccount for uncertainty in state assignments. This work presents an extension\nof the statistical jump model that incorporates uncertainty estimation in\ncluster membership. Leveraging the similarities between statistical jump models\nand the fuzzy c-means framework, our fuzzy jump model sequentially estimates\ntime-varying state probabilities. Our approach offers high flexibility, as it\nsupports both soft and hard clustering through the tuning of a fuzziness\nparameter, and it naturally accommodates multivariate time series data of mixed\ntypes. Through a simulation study, we evaluate the ability of the proposed\nmodel to accurately estimate the true latent-state distribution, demonstrating\nthat it outperforms competing approaches under high cluster assignment\nuncertainty. We further demonstrate its utility on two empirical applications:\nfirst, by automatically identifying co-orbital regimes in the three-body\nproblem, a novel application with important implications for understanding\nasteroid behavior and designing interplanetary mission trajectories; and\nsecond, on a financial dataset of five assets representing distinct market\nsectors (equities, bonds, foreign exchange, cryptocurrencies, and utilities),\nwhere the model accurately tracks both bull and bear market phases."}
{"id": "2509.26175", "categories": ["stat.ML", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.26175", "abs": "https://arxiv.org/abs/2509.26175", "authors": ["Cecilia Secchi", "Giacomo Zanella"], "title": "Spectral gap of Metropolis-within-Gibbs under log-concavity", "comment": "20 pages, 1 figure", "summary": "The Metropolis-within-Gibbs (MwG) algorithm is a widely used Markov Chain\nMonte Carlo method for sampling from high-dimensional distributions when exact\nconditional sampling is intractable. We study MwG with Random Walk Metropolis\n(RWM) updates, using proposal variances tuned to match the target's conditional\nvariances. Assuming the target $\\pi$ is a $d$-dimensional log-concave\ndistribution with condition number $\\kappa$, we establish a spectral gap lower\nbound of order $\\mathcal{O}(1/\\kappa d)$ for the random-scan version of MwG,\nimproving on the previously available $\\mathcal{O}(1/\\kappa^2 d)$ bound. This\nis obtained by developing sharp estimates of the conductance of one-dimensional\nRWM kernels, which can be of independent interest. The result shows that MwG\ncan mix substantially faster with variance-adaptive proposals and that its\nmixing performance is just a constant factor worse than that of the exact Gibbs\nsampler, thus providing theoretical support to previously observed empirical\nbehavior."}
{"id": "2509.26162", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.26162", "abs": "https://arxiv.org/abs/2509.26162", "authors": ["Prithul Chaturvedi", "Himanshu Pokhriyal"], "title": "Parameter estimation of the four-parameter Harris extended Weibull distribution with applications to real-life data", "comment": null, "summary": "This paper explores the extension of the classical two-parameter Weibull\ndistribution to a four-parameter Harris extended Weibull (HEW) distribution.\nThe flexibility of this probability distribution is illustrated by the varying\nshapes of HEW density function. Estimation of HEW parameters is explored using\nestimation methods such as the least-squares, maximum product of spacings, and\nminimum distance method. We provide Bayesian inference on the random parameters\nof the HEW distribution using Metropolis-Hastings algorithm to sample from the\njoint posterior distribution. Performance of the estimation methods is assessed\nusing extensive simulations. The applicability of the distribution is\ndemonstrated against three variants of the Weibull distribution on three\nreal-life datasets."}
{"id": "2509.26429", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26429", "abs": "https://arxiv.org/abs/2509.26429", "authors": ["Emil Javurek", "Valentyn Melnychuk", "Jonas Schweisthal", "Konstantin Hess", "Dennis Frauen", "Stefan Feuerriegel"], "title": "An Orthogonal Learner for Individualized Outcomes in Markov Decision Processes", "comment": "Preprint", "summary": "Predicting individualized potential outcomes in sequential decision-making is\ncentral for optimizing therapeutic decisions in personalized medicine (e.g.,\nwhich dosing sequence to give to a cancer patient). However, predicting\npotential outcomes over long horizons is notoriously difficult. Existing\nmethods that break the curse of the horizon typically lack strong theoretical\nguarantees such as orthogonality and quasi-oracle efficiency. In this paper, we\nrevisit the problem of predicting individualized potential outcomes in\nsequential decision-making (i.e., estimating Q-functions in Markov decision\nprocesses with observational data) through a causal inference lens. In\nparticular, we develop a comprehensive theoretical foundation for meta-learners\nin this setting with a focus on beneficial theoretical properties. As a result,\nwe yield a novel meta-learner called DRQ-learner and establish that it is: (1)\ndoubly robust (i.e., valid inference under the misspecification of one of the\nnuisances), (2) Neyman-orthogonal (i.e., insensitive to first-order estimation\nerrors in the nuisance functions), and (3) achieves quasi-oracle efficiency\n(i.e., behaves asymptotically as if the ground-truth nuisance functions were\nknown). Our DRQ-learner is applicable to settings with both discrete and\ncontinuous state spaces. Further, our DRQ-learner is flexible and can be used\ntogether with arbitrary machine learning models (e.g., neural networks). We\nvalidate our theoretical results through numerical experiments, thereby showing\nthat our meta-learner outperforms state-of-the-art baselines."}
{"id": "2509.26265", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.26265", "abs": "https://arxiv.org/abs/2509.26265", "authors": ["Gherardo Varando", "Manuele Leonelli", "Jordi Cerdà-Bautista", "Vasileios Sitokonstantinou", "Gustau Camps-Valls"], "title": "Staged Event Trees for Transparent Treatment Effect Estimation", "comment": null, "summary": "Average and conditional treatment effects are fundamental causal quantities\nused to evaluate the effectiveness of treatments in various critical\napplications, including clinical settings and policy-making. Beyond the\ngold-standard estimators from randomized trials, numerous methods have been\nproposed to estimate treatment effects using observational data. In this paper,\nwe provide a novel characterization of widely used causal inference techniques\nwithin the framework of staged event trees, demonstrating their capacity to\nenhance treatment effect estimation. These models offer a distinct advantage\ndue to their interpretability, making them particularly valuable for practical\napplications. We implement classical estimators within the framework of staged\nevent trees and illustrate their capabilities through both simulation studies\nand real-world applications. Furthermore, we showcase how staged event trees\nexplicitly and visually describe when standard causal assumptions, such as\npositivity, hold, further enhancing their practical utility."}
{"id": "2509.26551", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26551", "abs": "https://arxiv.org/abs/2509.26551", "authors": ["Mary I. Letey", "Jacob A. Zavatone-Veth", "Yue M. Lu", "Cengiz Pehlevan"], "title": "Pretrain-Test Task Alignment Governs Generalization in In-Context Learning", "comment": null, "summary": "In-context learning (ICL) is a central capability of Transformer models, but\nthe structures in data that enable its emergence and govern its robustness\nremain poorly understood. In this work, we study how the structure of\npretraining tasks governs generalization in ICL. Using a solvable model for ICL\nof linear regression by linear attention, we derive an exact expression for ICL\ngeneralization error in high dimensions under arbitrary pretraining-testing\ntask covariance mismatch. This leads to a new alignment measure that quantifies\nhow much information about the pretraining task distribution is useful for\ninference at test time. We show that this measure directly predicts ICL\nperformance not only in the solvable model but also in nonlinear Transformers.\nOur analysis further reveals a tradeoff between specialization and\ngeneralization in ICL: depending on task distribution alignment, increasing\npretraining task diversity can either improve or harm test performance.\nTogether, these results identify train-test task alignment as a key determinant\nof generalization in ICL."}
{"id": "2509.26280", "categories": ["stat.ME", "math.PR", "62H99, 60E05, 62E15, 62H20"], "pdf": "https://arxiv.org/pdf/2509.26280", "abs": "https://arxiv.org/abs/2509.26280", "authors": ["Marius Hofert", "Zhiyuan Pang"], "title": "W-transforms: Uniformity-preserving transformations and induced dependence structures", "comment": "46 pages, 14 figures", "summary": "W-transforms are introduced as uniformity-preserving univariate\ntransformations on the unit interval induced by distribution functions and\npiecewise strictly monotone functions, and their properties are investigated.\nWhen applied componentwise to random vectors with standard uniform univariate\nmargins, W-transforms naturally serve as copula-to-copula transformations.\nProperties of the resulting W-transformed copulas, including their analytical\nform, density, measures of concordance, tail dependence and symmetries, are\nderived. A flexible parametric family of W-transforms is proposed as a special\ncase to further enhance tractability. Illustrative examples highlight the\nintroduced concepts, and improved dependence modelling is demonstrated in terms\nof a real-life dataset."}
{"id": "2509.26560", "categories": ["stat.ML", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.26560", "abs": "https://arxiv.org/abs/2509.26560", "authors": ["Chanwoo Chun", "Abdulkadir Canatar", "SueYeon Chung", "Daniel Lee"], "title": "Estimating Dimensionality of Neural Representations from Finite Samples", "comment": null, "summary": "The global dimensionality of a neural representation manifold provides rich\ninsight into the computational process underlying both artificial and\nbiological neural networks. However, all existing measures of global\ndimensionality are sensitive to the number of samples, i.e., the number of rows\nand columns of the sample matrix. We show that, in particular, the\nparticipation ratio of eigenvalues, a popular measure of global dimensionality,\nis highly biased with small sample sizes, and propose a bias-corrected\nestimator that is more accurate with finite samples and with noise. On\nsynthetic data examples, we demonstrate that our estimator can recover the true\nknown dimensionality. We apply our estimator to neural brain recordings,\nincluding calcium imaging, electrophysiological recordings, and fMRI data, and\nto the neural activations in a large language model and show our estimator is\ninvariant to the sample size. Finally, our estimators can additionally be used\nto measure the local dimensionalities of curved neural manifolds by weighting\nthe finite samples appropriately."}
{"id": "2509.26385", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2509.26385", "abs": "https://arxiv.org/abs/2509.26385", "authors": ["Zejin Gao", "Ksheera Sagar", "Anindya Bhadra"], "title": "An Order of Magnitude Time Complexity Reduction for Gaussian Graphical Model Posterior Sampling Using a Reverse Telescoping Block Decomposition", "comment": null, "summary": "We consider the problem of fully Bayesian posterior estimation and\nuncertainty quantification in undirected Gaussian graphical models via Markov\nchain Monte Carlo (MCMC) under recently-developed element-wise graphical\npriors, such as the graphical horseshoe. Unlike the conjugate Wishart family,\nthese priors are non-conjugate; but have the advantage that they naturally\nallow one to encode a prior belief of sparsity in the off-diagonal elements of\nthe precision matrix, without imposing a structure on the entire matrix.\nUnfortunately, for a graph with $p$ nodes and with $n$ samples, the\nstate-of-the-art MCMC approaches for the element-wise priors achieve a per\niteration complexity of $O(p^4),$ which is prohibitive when $p\\gg n$. In this\nregime, we develop a suitably reparameterized MCMC with per iteration\ncomplexity of $O(p^3)$, providing a one-order of magnitude improvement, and\nconsequently bringing the computational cost at par with the conjugate Wishart\nfamily, which is also $O(p^3)$ due to a use of the classical Bartlett\ndecomposition, but this decomposition does not apply outside the Wishart\nfamily. Importantly, the proposed benefit is obtained solely due to our\nreparameterization in an MCMC scheme targeting the true posterior, that\nreverses the recently developed telescoping block decomposition of Bhadra et\nal. (2024), in a suitable sense. There is no variational or any other\napproximate Bayesian computation scheme considered in this paper that\ncompromises targeting the true posterior. Simulations and the analysis of a\nbreast cancer data set confirm both the correctness and better algorithmic\nscaling of the proposed reverse telescoping sampler."}
{"id": "2509.26265", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.26265", "abs": "https://arxiv.org/abs/2509.26265", "authors": ["Gherardo Varando", "Manuele Leonelli", "Jordi Cerdà-Bautista", "Vasileios Sitokonstantinou", "Gustau Camps-Valls"], "title": "Staged Event Trees for Transparent Treatment Effect Estimation", "comment": null, "summary": "Average and conditional treatment effects are fundamental causal quantities\nused to evaluate the effectiveness of treatments in various critical\napplications, including clinical settings and policy-making. Beyond the\ngold-standard estimators from randomized trials, numerous methods have been\nproposed to estimate treatment effects using observational data. In this paper,\nwe provide a novel characterization of widely used causal inference techniques\nwithin the framework of staged event trees, demonstrating their capacity to\nenhance treatment effect estimation. These models offer a distinct advantage\ndue to their interpretability, making them particularly valuable for practical\napplications. We implement classical estimators within the framework of staged\nevent trees and illustrate their capabilities through both simulation studies\nand real-world applications. Furthermore, we showcase how staged event trees\nexplicitly and visually describe when standard causal assumptions, such as\npositivity, hold, further enhancing their practical utility."}
{"id": "2509.26451", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.26451", "abs": "https://arxiv.org/abs/2509.26451", "authors": ["Nisrine Madhar", "Juliette Legrand", "Maud Thomas"], "title": "Spectral Bootstrap for Non-Parametric Simulation of Multivariate Extreme Events", "comment": null, "summary": "Inference in extreme value theory relies on a limited number of extreme\nobservations, making estimation challenging. To address this limitation, we\npropose a non-parametric bootstrap procedure, the multivariate extreme spectral\nbootstrap procedure, relying on the spectral representation of multivariate\ngeneralized Paretodistributed random vectors. Unlike standard bootstrap\nmethods, our approach preserves the joint tail behaviour of the data and\ngenerates additional synthetic extreme data, thereby improving the reliability\nof inference. We demonstrate the effectiveness of our procedure for the\nestimation of tail risk metrics, under both simulated and real data. The\nresults highlight the potential of this method for enhancing risk assessment in\nhigh-dimensional extreme scenarios."}
{"id": "2509.26554", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.26554", "abs": "https://arxiv.org/abs/2509.26554", "authors": ["Herbert P. Susmann", "Nicholas T. Williams", "Richard Liu", "Jessica G. Young", "Iván Díaz"], "title": "Computationally and statistically efficient estimation of time-smoothed counterfactual curves", "comment": "29 pages, 3 figures", "summary": "Longitudinal causal inference is concerned with defining, identifying, and\nestimating the effect of a time-varying intervention on a time-varying outcome\nthat is indexed by a follow-up time. In an observational study, Robins's\ngeneralized g-formula can identify causal effects induced by a broad class of\ntime-varying interventions. Various methods for estimating the generalized\ng-formula have been posed for different outcome types, such as a failure event\nindicator by a specified time (e.g. mortality by 5 year follow-up), as well as\ncontinuous or dichotomous/multi-valued outcomes measures at a specified time\n(e.g. blood pressure in mm/hg or an indicator of high blood pressure at 5-year\nfollow-up). Multiply-robust, data-adaptive estimators leverage flexible\nnonparametric estimation algorithms while allowing for statistical inference.\nHowever, extant methods do not accommodate time-smoothing when multiple\noutcomes are measured over time, which can lead to substantial loss of\nprecision. We propose a novel multiply-robust estimator of the generalized\ng-formula that accommodates time-smoothing over numerous available outcome\nmeasures. Our approach accommodates any intervention that can be described as a\nLongitudinal Modified Treatment Policy, a flexible class suitable for binary,\nmulti-valued, and continuous longitudinal treatments. Our method produces an\nestimate of the effect curve: the causal effect of the intervention on the\noutcome at each measurement time, taking into account censoring and\nnon-monotonic outcome missingness patterns. In simulations we find that the\nproposed algorithm outperforms extant multiply-robust approaches for effect\ncurve estimation in scenarios with high degrees of outcome missingness and when\nthere is strong confounding. We apply the method to study longitudinal effects\nof union membership on wages."}
{"id": "2509.26577", "categories": ["stat.ME", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.26577", "abs": "https://arxiv.org/abs/2509.26577", "authors": ["Chiara Mattamira", "Olivia Prosper Feldman"], "title": "Stochasticity and Practical Identifiability in Epidemic Models: A Monte Carlo Perspective", "comment": null, "summary": "Assessing the practical identifiability of epidemic models is essential for\ndetermining whether parameters can be meaningfully estimated from observed\ndata. Monte Carlo (MC) methods provide an accessible and intuitive framework;\nhowever, their standard implementation - perturbing deterministic trajectories\nwith independent Gaussian noise - rests on assumptions poorly suited to\nepidemic processes, which are inherently stochastic, temporally correlated, and\nhighly variable, especially in small populations or under slow transmission. In\nthis study, we investigate the structure of stochastic variability in the\nclassic Susceptible-Infected-Recovered (SIR) model across a range of\nepidemiological regimes, and assess whether it can be represented within the\nindependent Gaussian noise framework. We show that continuous-time Markov chain\n(CTMC) trajectories consistently exhibit super-Poissonian variability and\nstrong temporal dependence. Through coverage analysis, we further demonstrate\nthat independent Gaussian noise systematically underestimates the variability\nof the underlying stochastic process, leading to overly optimistic conclusions\nabout parameter identifiability. In addition, we propose a hybrid simulation\napproach that introduces time- and amplitude-dependent variability into\ndeterministic ODE trajectories, preserving computational efficiency while\ncapturing key features of epidemic stochasticity. Our findings highlight the\nlimitations of the standard MC algorithm and provide a pathway for\nincorporating more realistic noise structures into epidemic inference."}
{"id": "2509.25507", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.25507", "abs": "https://arxiv.org/abs/2509.25507", "authors": ["Anirban Chatterjee", "Sayantan Choudhury", "Rohan Hore"], "title": "One-shot Conditional Sampling: MMD meets Nearest Neighbors", "comment": "53 pages, 14 figures, 1 table", "summary": "How can we generate samples from a conditional distribution that we never\nfully observe? This question arises across a broad range of applications in\nboth modern machine learning and classical statistics, including image\npost-processing in computer vision, approximate posterior sampling in\nsimulation-based inference, and conditional distribution modeling in complex\ndata settings. In such settings, compared with unconditional sampling,\nadditional feature information can be leveraged to enable more adaptive and\nefficient sampling. Building on this, we introduce Conditional Generator using\nMMD (CGMMD), a novel framework for conditional sampling. Unlike many\ncontemporary approaches, our method frames the training objective as a simple,\nadversary-free direct minimization problem. A key feature of CGMMD is its\nability to produce conditional samples in a single forward pass of the\ngenerator, enabling practical one-shot sampling with low test-time complexity.\nWe establish rigorous theoretical bounds on the loss incurred when sampling\nfrom the CGMMD sampler, and prove convergence of the estimated distribution to\nthe true conditional distribution. In the process, we also develop a uniform\nconcentration result for nearest-neighbor based functionals, which may be of\nindependent interest. Finally, we show that CGMMD performs competitively on\nsynthetic tasks involving complex conditional densities, as well as on\npractical applications such as image denoising and image super-resolution."}
{"id": "2509.25588", "categories": ["stat.ML", "cs.LG", "stat.ME", "62H30, 62G05, 62P10", "I.5.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.25588", "abs": "https://arxiv.org/abs/2509.25588", "authors": ["Yishu Wei", "Wen-Yee Lee", "George Ekow Quaye", "Xiaogang Su"], "title": "Conservative Decisions with Risk Scores", "comment": "22 pages plus a supplement with 3 pages", "summary": "In binary classification applications, conservative decision-making that\nallows for abstention can be advantageous. To this end, we introduce a novel\napproach that determines the optimal cutoff interval for risk scores, which can\nbe directly available or derived from fitted models. Within this interval, the\nalgorithm refrains from making decisions, while outside the interval,\nclassification accuracy is maximized. Our approach is inspired by support\nvector machines (SVM), but differs in that it minimizes the classification\nmargin rather than maximizing it. We provide the theoretical optimal solution\nto this problem, which holds important practical implications. Our proposed\nmethod not only supports conservative decision-making but also inherently\nresults in a risk-coverage curve. Together with the area under the curve (AUC),\nthis curve can serve as a comprehensive performance metric for evaluating and\ncomparing classifiers, akin to the receiver operating characteristic (ROC)\ncurve. To investigate and illustrate our approach, we conduct both simulation\nstudies and a real-world case study in the context of diagnosing prostate\ncancer."}
{"id": "2509.25599", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25599", "abs": "https://arxiv.org/abs/2509.25599", "authors": ["Ruolin Meng", "Ming-Yu Chung", "Dhanajit Brahma", "Ricardo Henao", "Lawrence Carin"], "title": "Coupling Generative Modeling and an Autoencoder with the Causal Bridge", "comment": "Accepted to NeurIPS 2025", "summary": "We consider inferring the causal effect of a treatment (intervention) on an\noutcome of interest in situations where there is potentially an unobserved\nconfounder influencing both the treatment and the outcome. This is achievable\nby assuming access to two separate sets of control (proxy) measurements\nassociated with treatment and outcomes, which are used to estimate treatment\neffects through a function termed the em causal bridge (CB). We present a new\ntheoretical perspective, associated assumptions for when estimating treatment\neffects with the CB is feasible, and a bound on the average error of the\ntreatment effect when the CB assumptions are violated. From this new\nperspective, we then demonstrate how coupling the CB with an autoencoder\narchitecture allows for the sharing of statistical strength between observed\nquantities (proxies, treatment, and outcomes), thus improving the quality of\nthe CB estimates. Experiments on synthetic and real-world data demonstrate the\neffectiveness of the proposed approach in relation to the state-of-the-art\nmethodology for proxy measurements."}
{"id": "2509.26175", "categories": ["stat.ML", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.26175", "abs": "https://arxiv.org/abs/2509.26175", "authors": ["Cecilia Secchi", "Giacomo Zanella"], "title": "Spectral gap of Metropolis-within-Gibbs under log-concavity", "comment": "20 pages, 1 figure", "summary": "The Metropolis-within-Gibbs (MwG) algorithm is a widely used Markov Chain\nMonte Carlo method for sampling from high-dimensional distributions when exact\nconditional sampling is intractable. We study MwG with Random Walk Metropolis\n(RWM) updates, using proposal variances tuned to match the target's conditional\nvariances. Assuming the target $\\pi$ is a $d$-dimensional log-concave\ndistribution with condition number $\\kappa$, we establish a spectral gap lower\nbound of order $\\mathcal{O}(1/\\kappa d)$ for the random-scan version of MwG,\nimproving on the previously available $\\mathcal{O}(1/\\kappa^2 d)$ bound. This\nis obtained by developing sharp estimates of the conductance of one-dimensional\nRWM kernels, which can be of independent interest. The result shows that MwG\ncan mix substantially faster with variance-adaptive proposals and that its\nmixing performance is just a constant factor worse than that of the exact Gibbs\nsampler, thus providing theoretical support to previously observed empirical\nbehavior."}
