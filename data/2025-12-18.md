<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 14]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.ML](#stat.ML) [Total: 8]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [A variational Bayes latent class approach for EHR-based patient phenotyping in R](https://arxiv.org/abs/2512.14272)
*Brian Buckley,Adrian O'Hagan,Marie Galligan*

Main category: stat.CO

TL;DR: VBphenoR是一个R包，使用变分贝叶斯方法进行电子健康记录的患者表型分析，结合高斯混合模型和逻辑回归来识别表型类别并评估预测性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据包含大量患者信息，但需要有效的方法来识别患者表型。传统方法可能计算复杂或不够准确，需要开发一种封闭形式的变分贝叶斯方法来高效处理EHR数据中的患者表型分析。

Method: 1. 使用封闭形式坐标上升变分推断（CAVI）实现变分贝叶斯高斯混合模型（GMM）算法，确定患者表型的潜在类别。2. 实现变分贝叶斯逻辑回归，利用GMM步骤得到的潜在类别信息，计算表型概率、生物标志物变化，并评估临床代码和药物代码的预测性能。

Result: 开发了VBphenoR R包，提供了一个完整的封闭形式变分贝叶斯框架，能够从EHR数据中识别患者表型，计算表型概率，分析生物标志物差异，并评估临床特征的预测能力。

Conclusion: VBphenoR提供了一个有效的变分贝叶斯工具，用于EHR数据的患者表型分析，结合GMM和逻辑回归方法，能够高效地识别表型类别并评估预测因素，为临床研究和精准医疗提供支持。

Abstract: The VBphenoR package for R provides a closed-form variational Bayes approach to patient phenotyping using Electronic Health Records (EHR) data. We implement a variational Bayes Gaussian Mixture Model (GMM) algorithm using closed-form coordinate ascent variational inference (CAVI) to determine the patient phenotype latent class. We then implement a variational Bayes logistic regression, where we determine the probability of the phenotype in the supplied EHR cohort, the shift in biomarkers for patients with the phenotype of interest versus a healthy population and evaluate predictive performance of binary indicator clinical codes and medication codes. The logistic model likelihood applies the latent class from the GMM step to inform the conditional.

</details>


### [2] [Two Bayesian Approaches to Dynamic Gaussian Bayesian Networks with Intra- and Inter-Slice Edges](https://arxiv.org/abs/2512.14512)
*Kezhuo Li,Marco Grzegorczyk*

Main category: stat.CO

TL;DR: 对比高斯动态贝叶斯网络中的两种贝叶斯建模方法：均值调整BGe（mBGe）和扩展BGe（eBGe），发现它们诱导出不同的网络结构等价类，其中eBGe模型产生非标准等价类。


<details>
  <summary>Details</summary>
Motivation: 高斯动态贝叶斯网络（GDBNs）是学习连续时间序列数据网络结构的常用工具。现有两种基于高斯BGe分数的贝叶斯建模方法（mBGe和eBGe），需要对比它们的性能并理解其理论差异。

Method: 对比分析mBGe和eBGe两种模型，通过实证研究比较它们的性能。特别关注两种模型诱导的网络结构等价类的差异，针对eBGe模型产生的非标准等价类，提出了一种新的DAG-to-CPDAG算法来识别这些等价类。

Result: 研究发现两种模型诱导出不同的网络结构等价类。eBGe模型产生非标准等价类，这是之前文献中未报道的新发现。为此提出了新的DAG-to-CPDAG算法来识别这些非标准等价类。

Conclusion: mBGe和eBGe模型在理论性质上存在重要差异，特别是它们诱导的等价类不同。eBGe模型产生非标准等价类，需要专门的算法来识别，这对高斯动态贝叶斯网络的结构学习有重要影响。

Abstract: Gaussian Dynamic Bayesian Networks (GDBNs) are a widely used tool for learning network structures from continuous time-series data. To capture both time-lagged and contemporaneous dependencies, advanced GDBNs allow for dynamic inter-slice edges as well as static intra-slice edges. In the literature, two Bayesian modeling approaches have been developed for GDBNs. Both build on and extend the well-known Gaussian BGe score. We refer to them as the mean-adjusted BGe (mBGe) and the extended BGe (eBGe) models. In this paper, we contrast the two models and compare their performance empirically. The main finding of our study is that the two models induce different equivalence classes of network structures. In particular, the equivalence classes implied by the eBGe model are non-standard, and we propose a new variant of the DAG-to-CPDAG algorithm to identify them. To the best of our knowledge, these non-standard equivalence classes have not been previously reported.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [Joint Models with Multiple Markers and Multiple Time-to-event Outcomes Using Variational Approximations](https://arxiv.org/abs/2512.13962)
*Benjamin Christoffersen,Keith Humphreys,Alessandro Gasparini,Birzhan Akynkozhayev,Hedvig Kjellström,Mark Clements*

Main category: stat.ME

TL;DR: 提出基于高斯变分近似的联合模型全似然方法，支持多标记、多生存结局、延迟进入和大规模数据，提供开源实现


<details>
  <summary>Details</summary>
Motivation: 现有联合模型难以同时处理多标记、多生存结局（包括终点事件、竞争事件和复发事件）、延迟进入和大规模数据，需要更灵活和可扩展的方法

Method: 基于高斯变分近似的全似然方法，为纵向标记和生存结局提供灵活的模型集，并开发了开源实现

Result: 变分近似的下界接近全似然，方法快速且可扩展；应用于乳腺密度和脂肪组织纵向测量与首次乳腺癌诊断时间的联合模型

Conclusion: 变分近似为扩展当前联合模型提供了有前景的途径，能够处理复杂的数据结构和大规模分析

Abstract: Joint models are well suited to modelling linked data from laboratories and health registers. However, there are few examples of joint models that allow for (a) multiple markers, (b) multiple survival outcomes (including terminal events, competing events, and recurrent events), (c) delayed entry and (d) scalability. We propose a full likelihood approach for joint models based on a Gaussian variational approximation to satisfy criteria (a)-(d). We provide an open-source implementation for this approach, allowing for flexible sets of models for the longitudinal markers and survival outcomes. Through simulations, we find that the lower bound for the variational approximation is close to the full likelihood. We also find that our approach and implementation are fast and scalable. We provide an application with a joint model for longitudinal measurements of dense and fatty breast tissue and time to first breast cancer diagnosis. The use of variational approximations provides a promising approach for extending current joint models.

</details>


### [4] [Parameter Estimation for Partially Observed Stable Continuous-State Branching Processes](https://arxiv.org/abs/2512.13841)
*Eduardo Gutiérrez-Peña,Carlos Octavio Pérez-Mendoza,Alan Riva Palacio,Arno Siri-Jégousse*

Main category: stat.ME

TL;DR: 提出基于从属表示的连续状态分支过程参数估计新框架，通过将随机动态转移到相关从属过程实现参数估计，无需额外假设，并通过拉普拉斯变换反演高效恢复似然函数。


<details>
  <summary>Details</summary>
Motivation: 连续状态分支过程（CSBPs）的参数估计通常面临挑战，特别是在缺乏闭式转移密度的情况下。现有方法可能需要额外假设或计算复杂，需要一种更灵活高效的估计框架。

Method: 利用CSBPs的从属表示，将估计问题重新表述为相关从属过程的参数估计。通过拉普拉斯变换反演数值恢复似然函数，即使在没有闭式转移密度的模型中也能实现。同时提出基于相同从属结构的动态模拟框架来生成离散时间轨迹。

Result: 开发了一个灵活的参数估计框架，能够在没有闭式转移密度的情况下高效恢复似然函数。提出的方法不需要额外假设，并提供了动态模拟能力来生成CSBPs的离散时间轨迹。

Conclusion: 基于从属表示的CSBPs参数估计框架提供了一种有效且灵活的方法，克服了传统估计方法的局限性，特别是在缺乏闭式转移密度的情况下，同时支持动态模拟应用。

Abstract: In this article, we present a novel inference framework for estimating the parameters of Continuous-State Branching Processes (CSBPs). We do so by leveraging their subordinator representation. Our method reformulates the estimation problem by shifting the stochastic dynamics to the associated subordinator, enabling a parametric estimation procedure without requiring additional assumptions. This reformulation allows for efficient numerical recovery of the likelihood function via Laplace transform inversion, even in models where closed-form transition densities are unavailable. In addition to offering a flexible approach to parameter estimation, we propose a dynamic simulation framework that generates discrete-time trajectories of CSBPs using the same subordinator-based structure.

</details>


### [5] [Trunc-Opt vine building algorithms](https://arxiv.org/abs/2512.14399)
*Dániel Pfeifer,Edith Alice Kovács*

Main category: stat.ME

TL;DR: 本文提出了一种新的截断藤copula构建方法，通过引入"截断藤权重"评分，从截断层后的第一棵树开始构建，利用条件独立性，相比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 藤copula模型虽然灵活，但参数空间维度高。截断藤copula通过引入条件独立性来减少参数，但现有构建方法从最低层树开始，未能充分利用截断结构的特点。

Method: 提出"截断藤权重"作为拟合评分，开发新的截断藤构建方法：从截断层后的第一棵树开始构建，利用条件独立性，并给出相应的算法实现。

Result: 在真实数据集上测试，与R包中的知名方法比较，新方法通常表现更优。通过定理证明验证了方法的理论合理性。

Conclusion: 新方法通过从截断层后第一棵树开始构建，充分利用条件独立性，提供了一种更有效的截断藤copula构建方法，在实际应用中表现良好。

Abstract: Vine copula models have become highly popular and practical tools for modelling multivariate probability distributions due to their flexibility in modelling different kinds of dependences between the random variables involved. However, their flexibility comes with the drawback of a high-dimensional parameter space. To tackle this problem, truncated vine copulas were introduced by Kurowicka (2010) (Gaussian case) and Brechmann and Czado (2013) (general case). Truncated vine copulas contain conditionally independent pair copulas after the truncation level. So far, in the general case, truncated vine constructing algorithms started from the lowest tree in order to encode the largest dependences in the lower trees. The novelty of this paper starts from the observation that a truncated vine is determined by the first tree after the truncation level (see Kovács and Szántai (2017)). This paper introduces a new score for fitting truncated vines to given data, called the Weight of the truncated vine. Then we propose a completely new methodology for constructing truncated vines. We prove theorems which motivate this new approach. While earlier algorithms did not use conditional independences, we give algorithms for constructing and encoding truncated vines which do exploit them. Finally, we illustrate the algorithms on real datasets and compare the results with well-known methods included in R packages. Our method generally compare favorably to previously known methods.

</details>


### [6] [Bayesian Global-Local Regularization](https://arxiv.org/abs/2512.13992)
*Jyotishka Datta,Nick Polson,Vadim Sokolov*

Main category: stat.ME

TL;DR: 提出统一全局-局部正则化框架，连接经典技术（如岭回归、非负套索）与现代贝叶斯分层建模，通过边际似然估计局部正则化强度，实现高维自适应收缩


<details>
  <summary>Details</summary>
Motivation: 弥合经典正则化技术与现代贝叶斯分层建模之间的差距，为高维统计推断提供原则性的自适应收缩方法

Method: 通过边际似然在顺序约束下估计局部正则化强度，推广Stein正部估计器，构建保序经验贝叶斯估计器

Result: 该估计器在稀疏有序模型类上达到接近极小极大风险（至多对数因子），在正交多项式回归中展示方法灵活性

Conclusion: 统一框架连接经验贝叶斯、形状约束估计和自由度调整，为高维统计推断提供重要进展

Abstract: We propose a unified framework for global-local regularization that bridges the gap between classical techniques -- such as ridge regression and the nonnegative garotte -- and modern Bayesian hierarchical modeling. By estimating local regularization strengths via marginal likelihood under order constraints, our approach generalizes Stein's positive-part estimator and provides a principled mechanism for adaptive shrinkage in high-dimensional settings. We establish that this isotonic empirical Bayes estimator achieves near-minimax risk (up to logarithmic factors) over sparse ordered model classes, constituting a significant advance in high-dimensional statistical inference. Applications to orthogonal polynomial regression demonstrate the methodology's flexibility, while our theoretical results clarify the connections between empirical Bayes, shape-constrained estimation, and degrees-of-freedom adjustments.

</details>


### [7] [Bond strength uncertainty quantification via confidence intervals for nondestructive evaluation of bonded composites](https://arxiv.org/abs/2512.13875)
*Michael C. Stanley,Peter W. Spaeth,James E. Warner,Matthew R. Webster*

Main category: stat.ME

TL;DR: 提出一种基于优化的有限样本置信区间方法，用于量化粘接复合材料超声无损评估中粘接强度估计的不确定性。


<details>
  <summary>Details</summary>
Motivation: 航空航天应用中粘接复合材料需要认证其物理特性（如强度）以确保安全性能。无损评估（NDE）可以验证粘接特性，但对NDE估计（特别是粘接强度）进行不确定性量化（UQ）对于理解风险至关重要。

Method: 采用基于优化的方法计算有限样本置信区间，使用统计逆模型从扫频超声相位观测计算界面刚度置信区间，然后通过已知的界面刚度回归关系传播到粘接强度。创新性地处理非线性前向模型和未知方差，并开发校准方法确保最终区间达到期望的覆盖水平。

Result: 在模拟测量数据上验证方法，涵盖从低到高的噪声设置和两种典型参数配置。相比基线方法，在高噪声设置和干扰参数接近约束边界时，本方法实现了更好的覆盖率和更小的区间。

Conclusion: 提出的优化方法为粘接强度NDE估计提供了有效的UQ框架，在具有挑战性的条件下（高噪声、参数边界情况）优于传统方法，有助于航空航天应用中粘接复合材料的安全认证。

Abstract: As bonded composite materials are used more frequently for aerospace applications, it is necessary to certify that parts achieve desired levels of certain physical characteristics (e.g., strength) for safety and performance. Nondestructive evaluation (NDE) of adhesively bonded structures enables verification of bond physical characteristics, but uncertainty quantification (UQ) of NDE estimates is crucial for understanding risks, especially for NDE estimates like bond strength. To address the critical need for NDE UQ for adhesive bond strength estimates, we propose an optimization--based approach to computing finite--sample confidence intervals showing the range of bond strengths that could feasibly be produced by the observed data. A statistical inverse model approach is used to compute a confidence interval of specimen interfacial stiffness from swept--frequency ultrasonic phase observations and a method for propagating the interval to bond strength via a known interfacial stiffness regression is proposed. This approach requires innovating the optimization--based confidence interval to handle both a nonlinear forward model and unknown variance and developing a calibration approach to ensure that the final bond strength interval achieves at least the desired coverage level. Using model assumptions in line with current literature, we demonstrate our approach on simulated measurement data using a variety of low to high noise settings under two prototypical parameter settings. Relative to a baseline approach, we show that our method achieves better coverage and smaller intervals in high--noise settings and when a nuisance parameter is near the constraint boundary.

</details>


### [8] [A latent variable model for identifying and characterizing food adulteration](https://arxiv.org/abs/2512.13939)
*Alessandro Casa,Thomas Brendan Murphy,Michael Fop*

Main category: stat.ME

TL;DR: 提出一种针对食品掺假检测的潜变量模型，能够估计掺假水平并识别受影响的谱区，应用于蜂蜜中红外光谱数据


<details>
  <summary>Details</summary>
Motivation: 消费者对食品质量和可持续性的关注增加，需要有效的食品认证方法。振动光谱技术能收集大量数据检测食品掺假，但光谱数据在统计上存在挑战，需要更复杂的建模策略

Method: 提出一种专门针对食品掺假检测的潜变量模型，适应光谱数据特征，不仅能识别掺假样本，还能估计掺假水平并检测受掺假影响最大的光谱区域

Result: 该方法应用于合成和真实的蜂蜜中红外光谱数据，能够精确估计掺假水平，并准确识别受掺假影响最大的光谱区域

Conclusion: 该方法比现有方法提供更细粒度的分析，提供更深入的见解，有助于开发便携快速仪器，促进食品真实性研究中的数据收集效率

Abstract: Recently, growing consumer awareness of food quality and sustainability has led to a rising demand for effective food authentication methods. Vibrational spectroscopy techniques have emerged as a promising tool for collecting large volumes of data to detect food adulteration. However, spectroscopic data pose significant challenges from a statistical viewpoint, highlighting the need for more sophisticated modeling strategies. To address these challenges, in this work we propose a latent variable model specifically tailored for food adulterant detection, while accommodating the features of spectral data. Our proposal offers greater granularity with respect to existing approaches, since it does not only identify adulterated samples but also estimates the level of adulteration, and detects the spectral regions most affected by the adulterant. Consequently, the methodology offers deeper insights, and could facilitate the development of portable and faster instruments for efficient data collection in food authenticity studies. The method is applied to both synthetic and real honey mid-infrared spectroscopy data, delivering precise estimates of the adulteration level and accurately identifying which portions of the spectra are most impacted by the adulterant.

</details>


### [9] [Low-rank Covariate Balancing Estimators under Interference](https://arxiv.org/abs/2512.13944)
*Souhardya Sengupta,Kosuke Imai,Georgia Papadogeorgou*

Main category: stat.ME

TL;DR: 论文提出了一种处理观测研究中干扰效应的统计框架，引入低秩结构假设，构建了无需真实倾向得分的稳健因果效应估计器。


<details>
  <summary>Details</summary>
Motivation: 观测研究中存在两个关键方法学挑战：1) 每个单元的结果可能依赖于许多其他单元的处理；2) 处理分配可能在单元间存在复杂依赖关系。现有方法难以同时应对这些挑战。

Method: 提出低秩潜在结果结构作为干扰假设的广义框架，涵盖匿名、最近邻、加性干扰等常见假设。在此框架下构建了无需真实倾向得分的无偏加权估计器，并开发了数据驱动的低秩结构选择程序。

Result: 理论证明：在已知倾向得分时，标准IPW估计器是唯一一致无偏的；未知倾向得分时，无估计器具有此性质。提出的加权估计器在未知倾向得分时仍能保持无偏性，已知倾向得分时比IPW更高效。

Conclusion: 低秩结构框架为处理观测研究中的干扰效应提供了灵活且稳健的方法，通过模拟和实证研究验证了方法的有效性，为复杂依赖下的因果推断提供了新工具。

Abstract: A key methodological challenge in observational studies with interference between units is twofold: (1) each unit's outcome may depend on many others' treatments, and (2) treatment assignments may exhibit complex dependencies across units. We develop a general statistical framework for constructing robust causal effect estimators to address these challenges. We first show that, without restricting the patterns of interference, the standard inverse probability weighting (IPW) estimator is the only uniformly unbiased estimator when the propensity score is known. In contrast, no estimator has such a property if the propensity score is unknown. We then introduce a \emph{low-rank structure} of potential outcomes as a broad class of structural assumptions about interference. This framework encompasses common assumptions such as anonymous, nearest-neighbor, and additive interference, while flexibly allowing for more complex study-specific interference assumptions. Under this low-rank assumption, we show how to construct an unbiased weighting estimator for a large class of causal estimands. The proposed weighting estimator does not require knowledge of true propensity scores and is therefore robust to unknown treatment assignment dependencies that often exist in observational studies. If the true propensity score is known, we can obtain an unbiased estimator that is more efficient than the IPW estimator by leveraging a low-rank structure. We establish the finite sample and asymptotic properties of the proposed weighting estimator, develop a data-driven procedure to select among candidate low-rank structures, and validate our approach through simulation and empirical studies.

</details>


### [10] [Signature-Informed Selection Detection: A Novel Method for Multi-Locus Temporal Population Genetic Model with Recombination](https://arxiv.org/abs/2512.14353)
*Ritabrata Dutta,Yuehao Xu,Sherman Khoo,Francesca Basini,Andreas Futschik*

Main category: stat.ME

TL;DR: 提出一种基于签名核评分规则的广义贝叶斯框架，用于推断多连锁位点的选择系数，通过伪边际MCMC算法从后验分布中采样，在模拟和真实数据中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在群体遗传学中，推断选择系数是一个重要但具有挑战性的任务，特别是当考虑多个连锁选择位点时，传统的推断方法变得更加困难。需要开发能够处理高维时间序列数据（等位基因频率轨迹）的新方法。

Method: 提出广义贝叶斯框架，使用签名核评分规则（一种基于路径迭代积分的核评分规则）处理等位基因频率时间序列数据。通过模型模拟获得签名核评分的无偏估计，采用伪边际MCMC算法从选择系数的签名核评分规则后验中采样。

Result: 通过模拟研究，在标准Wright-Fisher模型（含重组和选择）下，该方法在2-3个选择位点场景中相比现有基准方法表现出更好的推断效果。在负频率依赖选择模型和联合推断选择系数与初始单倍型频率的场景中也表现良好。在酵母和果蝇实验数据中成功应用。

Conclusion: 提出的基于签名核评分规则的广义贝叶斯框架能够有效推断多连锁位点的选择系数，为处理复杂群体遗传学模型中的选择推断问题提供了新工具。

Abstract: In population genetics, there is often interest in inferring selection coefficients. This task becomes more challenging if multiple linked selected loci are considered simultaneously. For such a situation, we propose a novel generalized Bayesian framework where we compute a scoring rule posterior for the selection coefficients in multi-locus temporal population genetics models. As we consider trajectories of allele frequencies over time as our data, we choose to use a signature kernel scoring rule - a kernel scoring rule defined for high-dimensional time-series data using iterated path integrals of a path (called signatures). We can compute an unbiased estimate of the signature kernel score using model simulations. This enables us to sample asymptotically from the signature kernel scoring rule posterior of the selection coefficients using pseudo-marginal MCMC-type algorithms. Through a simulation study, we were able to show the inferential efficacy of our method compared to existing benchmark methods for two and three selected locus scenarios under the standard Wright-Fisher model with recombination and selection. We also consider a negative frequency-dependent selection model for one and two locus scenarios, and also joint inference of selection coefficients and initial haplotype frequencies under the standard Wright-Fisher model. Finally, we illustrate the application of our inferential method for two real-life dataset. More specifically, we consider a data set on Yeast, as well as data from an Evolve and Resequence (E\&R) experiment on {\em Drosophila simulans}.

</details>


### [11] [Most Powerful Test with Exact Family-Wise Error Rate Control: Necessary Conditions and a Path to Fast Computing](https://arxiv.org/abs/2512.14131)
*Prasanjit Dubey,Xiaoming Huo*

Main category: stat.ME

TL;DR: 提出一种高效坐标下降算法，用于计算多重假设检验中最强大检验的优化对偶解，具有线性收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多重假设检验中最强大检验问题建模为约束优化问题，虽然建立了具有强对偶性的对偶问题，但缺乏构造性求解方法，存在显著计算缺口。

Method: 推导了对偶优化的新颖必要最优性条件，基于此设计了高效的坐标下降算法来计算最优对偶解，进而获得原始问题的最强大检验。

Result: 算法具有线性收敛性，计算复杂度与目标误差倒数的对数成正比。模拟研究显示方法具有优越的检验功效，在临床和金融数据应用中识别出新的显著发现。

Conclusion: 填补了多重假设检验中最强大检验计算方法的空白，首次提出了快速高效算法，为统计方法论提供了重要的计算工具。

Abstract: Identifying the most powerful test in multiple hypothesis testing under strong family-wise error rate (FWER) control is a fundamental problem in statistical methodology. State-of-the-art approaches formulate this as a constrained optimisation problem, for which a dual problem with strong duality has been established in a general sense. However, a constructive method for solving the dual problem is lacking, leaving a significant computational gap. This paper fills this gap by deriving novel, necessary optimality conditions for the dual optimisation. We show that these conditions motivate an efficient coordinate-wise algorithm for computing the optimal dual solution, which, in turn, provides the most powerful test for the primal problem. We prove the linear convergence of our algorithm, i.e., the computational complexity of our proposed algorithm is proportional to the logarithm of the reciprocal of the target error. To the best of our knowledge, this is the first time such a fast and computationally efficient algorithm has been proposed for finding the most powerful test with family-wise error rate control. The method's superior power is demonstrated through simulation studies, and its practical utility is shown by identifying new, significant findings in both clinical and financial data applications.

</details>


### [12] [On the E(s^2)-optimality of two-level supersaturated designs constructed using Wu's method of partially aliased interactions on certain two-level orthogonal arrays](https://arxiv.org/abs/2512.14378)
*Emmanouil Androulakis,Kashinath Chatterjee,Haralambos Evangelaras*

Main category: stat.ME

TL;DR: 本文扩展了Wu提出的超饱和设计构造方法，证明了当起始设计为具有n-1、n-2或n-3列的任意正交阵列时，只要其主效应和两列交互作用与两列交互作用部分别名，所构造的超饱和设计具有E(s²)最优性。


<details>
  <summary>Details</summary>
Motivation: Wu提出的基于Hadamard设计构造超饱和设计的方法已被证明在特定条件下能产生E(s²)最优设计。本文旨在扩展这一结果，证明当起始设计为更广泛的正交阵列时，该方法仍能保持E(s²)最优性。

Method: 扩展Wu的方法：使用具有n次试验和n-1、n-2或n-3列的任意正交阵列作为起始设计，补充两列交互作用列（只要它们是部分别名的），构造超饱和设计。

Result: 证明了当起始设计为满足条件（主效应和两列交互作用与两列交互作用部分别名）的正交阵列时，通过Wu方法构造的超饱和设计具有E(s²)最优性。

Conclusion: 本文成功扩展了Wu和Bulutoglu与Cheng的结果，为使用更广泛的正交阵列作为起始设计构造E(s²)最优超饱和设计提供了理论保证。

Abstract: Wu [10] proposed a method for constructing two-level supersaturated designs by using a Hadamard design with n runs and n-1 columns as a staring design and by supplementing it with two-column interactions, as long as they are partially aliased. Bulutoglu and Cheng [2] proved that this method results in E(s^2)-optimal supersaturated designs when certain interaction columns are selected. In this paper, we extend these results and prove E(s^2)-optimality for supersaturated designs that are constructed using Wu's method when the starting design is any orthogonal array with n runs and n-1, n-2 or n-3 columns, as long as its main effects and two-column interactions are partially aliased with two-column interactions.

</details>


### [13] [Univariate-Guided Interaction Modeling](https://arxiv.org/abs/2512.14413)
*Aymen Echarghaoui,Robert Tibshirani*

Main category: stat.ME

TL;DR: 提出了一种用于稀疏回归与成对交互作用的新方法，通过扩展单变量引导稀疏回归(UniLasso)框架，引入单变量交互作用概念，开发了两种算法，在稀疏性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理成对交互作用的稀疏回归问题时，可能产生不够稀疏或难以解释的模型。需要开发既能保持稀疏性又能提高可解释性的新方法。

Method: 扩展UniLasso方法，引入"单变量交互作用"的新概念，提出了两种算法：uniPairs和uniPairs-2stage。这些方法通过引导机制选择重要的交互作用项。

Result: 与Glinternet和Sprinter等现有方法相比，提出的框架能产生更稀疏、更可解释的模型。在适当条件下证明了支持恢复的理论结果。

Conclusion: 提出的uniPairs框架在成对交互作用的稀疏回归问题上表现出色，既能获得稀疏模型，又能提高交互作用的可解释性，具有理论和实际价值。

Abstract: We propose a procedure for sparse regression with pairwise interactions, by generalizing the Univariate Guided Sparse Regression (UniLasso) methodology. A central contribution is our introduction of a concept of univariate (or marginal) interactions. Using this concept, we propose two algorithms -- uniPairs and uniPairs-2stage -- , and evaluate their performance against established methods, including Glinternet and Sprinter. We show that our framework yields sparser models with more interpretable interactions. We also prove support recovery results for our proposal under suitable conditions.

</details>


### [14] [Causal Secondary Analysis of Linked Data in the Presence of Mismatch Error](https://arxiv.org/abs/2512.14492)
*Martin Slawski*

Main category: stat.ME

TL;DR: 提出一种基于估计方程和EM算法的因果推断方法，用于处理记录链接不确定性下的平均处理效应估计问题


<details>
  <summary>Details</summary>
Motivation: 观测数据增多和多源数据整合需求增加，但记录链接中的错误（如记录不匹配）会阻碍数据分析，特别是在二次分析场景中，分析者缺乏链接质量信息，导致因果推断存在偏差

Method: 基于估计方程的方法，将未知的匹配状态指标视为缺失数据，采用EM算法变体，基于两成分混合模型进行指标插补，支持渐近推断

Result: 模拟研究和案例研究表明，考虑链接不确定性至关重要，所提方法能有效减少偏差，提高平均处理效应估计的准确性

Conclusion: 提出的基于估计方程和EM算法的方法能有效处理记录链接不确定性，在缺乏链接质量信息的二次分析场景中，为因果推断提供可靠的解决方案

Abstract: The increased prevalence of observational data and the need to integrate information from multiple sources are critical challenges in contemporary data analysis. Record linkage is a widely used tool for combining datasets in the absence of unique identifiers. The presence of linkage errors such as mismatched records, however, often hampers the analysis of data sets obtained in this way. This issue is more difficult to address in secondary analysis settings, where linkage and subsequent analysis are performed separately, and analysts have limited information about linkage quality. In this paper, we investigate the estimation of average treatment effects in the conventional potential outcome-based causal inference framework under linkage uncertainty. To mitigate the bias that would be incurred with naive analyses, we propose an approach based on estimating equations that treats the unknown match status indicators as missing data. Leveraging a variant of the Expectation-Maximization algorithm, these indicators are imputed based on a corresponding two-component mixture model. The approach is amenable to asymptotic inference. Simulation studies and a case study highlight the importance of accounting for linkage uncertainty and demonstrate the effectiveness of the proposed approach.

</details>


### [15] [A flexible class of latent variable models for the analysis of antibody response data](https://arxiv.org/abs/2512.14504)
*Emanuele Giorgi,Jonas Wallin*

Main category: stat.ME

TL;DR: 提出基于连续潜变量而非二分混合模型的抗体浓度建模框架，将免疫状态表示为从最小到强免疫激活的连续谱，能更好捕捉年龄相关抗体分布变化并保留定量测量信息。


<details>
  <summary>Details</summary>
Motivation: 现有抗体浓度建模主要基于有限混合模型，假设个体可分为血清阴性和阳性两个离散组别。这种二分法假设可能过于简化，无法充分捕捉免疫状态的连续性变化。

Method: 提出潜变量建模框架，将个体免疫状态表示为连续潜变量（血清反应性），范围从最小到强免疫激活。该框架可容纳多种模型形式，包括机制性和回归模型，有限混合模型是其特例。

Result: 使用疟疾血清学数据展示该方法的优势，能够进行跨年龄的联合分析并考虑传播模式变化。框架保留了定量测量的全部信息，能更好捕捉年龄相关抗体分布变化。

Conclusion: 该连续潜变量建模框架比传统二分模型更灵活，能更好描述免疫状态谱系，并可扩展到其他组学应用领域。

Abstract: Existing approaches to modelling antibody concentration data are mostly based on finite mixture models that rely on the assumption that individuals can be divided into two distinct groups: seronegative and seropositive. Here, we challenge this dichotomous modelling assumption and propose a latent variable modelling framework in which the immune status of each individual is represented along a continuum of latent seroreactivity, ranging from minimal to strong immune activation. This formulation provides greater flexibility in capturing age-related changes in antibody distributions while preserving the full information content of quantitative measurements. We show that the proposed class of models can accommodate a great variety of model formulations, both mechanistic and regression-based, and also includes finite mixture models as a special case. We demonstrate the advantages of this approach using malaria serology data and its ability to develop joint analyses across all ages that account for changes in transmission patterns. We conclude by outlining extensions of the proposed modelling framework and its relevance to other omics applications.

</details>


### [16] [Asymptotic Inference for Rank Correlations](https://arxiv.org/abs/2512.14609)
*Marc-Oliver Pohle,Jan-Lukas Wermuth,Christian H. Weiß*

Main category: stat.ME

TL;DR: 该论文为Kendall's tau、Spearman's rho等经典秩相关系数提供了全面的渐近推断理论，包括离散随机变量和时间序列情况，并提出了方差估计方法，从而能够构建置信区间和检验。


<details>
  <summary>Details</summary>
Motivation: 尽管Kendall's tau和Spearman's rho被广泛用于测量依赖性，但在渐近推断方面存在重要空白：特别是对于离散随机变量和时间序列情况，以及一般的方差估计问题。这导致无法构建渐近置信区间。

Method: 利用U-统计量的渐近理论，推导了iid数据和时间序列数据的渐近分布，并引入了一致的方差估计器。方法适用于多种秩相关系数，包括Kendall's tau、Spearman's rho、Goodman-Kruskal's gamma、Kendall's tau-b和grade correlation。

Result: 建立了秩相关系数的渐近分布理论，提出了有效的方差估计方法，从而能够构建置信区间和独立性检验。该方法推广了连续随机变量的经典结果，并修正了广泛使用的独立性检验。

Conclusion: 该研究填补了秩相关系数渐近推断的重要理论空白，为离散数据和时间序列提供了实用的推断工具，并通过模拟研究和案例应用验证了方法的有限样本性能。

Abstract: Kendall's tau and Spearman's rho are widely used tools for measuring dependence. Surprisingly, when it comes to asymptotic inference for these rank correlations, some fundamental results and methods have not yet been developed, in particular for discrete random variables and in the time series case, and concerning variance estimation in general. Consequently, asymptotic confidence intervals are not available. We provide a comprehensive treatment of asymptotic inference for classical rank correlations, including Kendall's tau, Spearman's rho, Goodman-Kruskal's gamma, Kendall's tau-b, and grade correlation. We derive asymptotic distributions for both iid and time series data, resorting to asymptotic results for U-statistics, and introduce consistent variance estimators. This enables the construction of confidence intervals and tests, generalizes classical results for continuous random variables and leads to corrected versions of widely used tests of independence. We analyze the finite-sample performance of our variance estimators, confidence intervals, and tests in simulations and illustrate their use in case studies.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [17] [Understanding statistics for biomedical research through the lens of replication](https://arxiv.org/abs/2512.13763)
*Huw Llewelyn*

Main category: stat.AP

TL;DR: 论文探讨了重复研究的概率问题，指出当P=0.025时，重复研究获得相同显著性结果的概率仅为28.3%，远低于直觉预期，并提出了基于方差和的改进方法。


<details>
  <summary>Details</summary>
Motivation: 传统研究过于关注结果是否能够被重复，但实际重复概率远低于预期。当前基于单方差功率计算的方法低估了获得稳定重复结果所需的样本量，需要更准确的概率框架来评估研究的可重复性。

Method: 通过分析原始研究和重复研究效应估计的方差和来计算重复概率。将连续分布转换为离散化尺度和概率质量，避免模糊性和不当的平坦先验。比较了不同样本量下的重复概率，特别是当重复研究样本量无限大时的极限情况。

Result: 当P=0.025（单侧）且重复研究具有相同样本量和方差时，获得相同显著性结果的概率仅为28.3%，与当前观察到的适度重复率一致。要获得更高的重复概率，需要比当前单方差功率计算得出的更大的样本量。当重复研究样本量无限大时，估计均值符号相同的概率为97.5%。

Conclusion: 研究可重复性的概率评估需要同时考虑原始研究和重复研究的方差，而不仅仅是传统功率计算。离散化方法提供了更清晰的推理框架，这一视角与频率主义和贝叶斯解释一致，对科学假设检验和决策制定有重要启示。

Abstract: Clinicians and scientists have traditionally focussed on whether their findings will be replicated and are very familiar with the concept. The probability that a replication study yields an effect with the same sign, or the same statistical significance as an original study depends on the sum of the variances of the effect estimates. On this basis, when P equals 0.025 one-sided and the replication study has the same sample size and variance as the original study, the probability of achieving a one-sided P is less than or equal to 0.025 a second time is only about 0.283, consistent with currently observed modest replication rates. A higher replication probability would require a larger sample size than that derived from current single variance power calculations. However, if the replication study is based on an infinitely large sample size and thus has negligible variance then the probability that its estimated mean is same sign is 1 - P = 0.975. The reasoning is made clearer by changing continuous distributions to discretised scales and probability masses, thus avoiding ambiguity and improper flat priors. This perspective is consistent with Frequentist and Bayesian interpretations and also requires further reasoning when testing scientific hypotheses and making decisions.

</details>


### [18] [A two-stage approach to heat-mortality risk assessment comparing multiple exposure-to-temperature models: the case study in Lazio, Italy](https://arxiv.org/abs/2512.14292)
*Emiliano Ceccarelli,Jorge Castillo-Mateo,Sandra Gudžiūnaitė,Giada Minelli,Giovanna Jona Lasinio,Marta Blangiardo*

Main category: stat.AP

TL;DR: 比较三种温度建模方法对意大利拉齐奥地区热相关死亡率估计的影响，发现不同方法导致最小风险温度显著差异，但都显示高温下相对风险明显增加，女性和老年人风险更高。


<details>
  <summary>Details</summary>
Motivation: 研究不同时空温度模型对热相关死亡率估计的影响，为流行病学研究和早期预警系统提供参考，帮助制定气候健康适应策略。

Method: 1. 比较三种市镇级日最高温度重建方法：贝叶斯分位数回归+空间插值、贝叶斯高斯回归、ERA5-Land再分析数据；2. 使用个体心血管和呼吸系统死亡数据，通过贝叶斯条件泊松模型在病例交叉设计中估计温度-死亡率关联；3. 定义暴露为前三天平均最高温度，并纳入不同阈值和持续时间的热浪定义。

Result: 贝叶斯模型比ERA5-Land显示更高且空间变异更大的温度；所有模型都显示高温下相对风险显著增加，但最小风险温度在不同方法间差异显著；分层分析显示女性和80岁以上老年人相对风险增加更高；热浪效应取决于定义方式，但所有方法都捕捉到长期热暴露与死亡率风险增加相关。

Conclusion: 温度模型选择对流行病学研究至关重要，研究结果为早期预警系统和气候健康适应策略提供了重要见解，强调了在热相关健康风险评估中考虑温度模型不确定性的重要性。

Abstract: This study investigates how different spatiotemporal temperature models affect the estimation of heat-related mortality in Lazio, Italy (2008--2022). First, we compare three methods to reconstruct daily maximum temperature at the municipality level: 1. a Bayesian quantile regression model with spatial interpolation, 2. a Bayesian Gaussian regression model, 3. the gridded reanalysis data from ERA5-Land. Both Bayesian models are station-based and exhibit higher and more spatially variable temperatures compared to ERA5-Land. Then, using individual mortality data for cardiovascular and respiratory causes, we estimate temperature-mortality associations through Bayesian conditional Poisson models in a case-crossover design. Exposure is defined as the mean maximum temperature over the previous three days. Additional models include heatwave definitions combining different thresholds and durations. All models exhibit a marked increase in relative risk at high temperatures; however, the temperature of minimum risk varies significantly across methods. Stratified analyses reveal higher relative risk increases in females and the elderly (80+). Heatwave effects depend on the definitions used, but all methods capture an increased mortality risk associated with prolonged heat exposure. Results confirm the importance of temperature model choice in epidemiology and provide insights for early warning systems and climate-health adaptation strategies.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [19] [Improving the Accuracy of Amortized Model Comparison with Self-Consistency](https://arxiv.org/abs/2512.14308)
*Šimon Kucharský,Aayush Mishra,Daniel Habermann,Stefan T. Radev,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: ABI方法在模型错误设定下不稳定，自洽性训练可提升鲁棒性，基于参数后验的模型比较方法优于直接近似模型证据的方法。


<details>
  <summary>Details</summary>
Motivation: 摊销贝叶斯推断（ABI）通过训练神经代理在模拟数据上实现快速后验近似，但对模型错误设定高度敏感。在模型比较场景中，多个统计模型中至少部分存在错误设定，这给ABI应用带来挑战。

Method: 研究自洽性（SC）如何改进四种不同概念的摊销模型比较方法。通过两个合成案例和两个真实案例研究，比较基于参数后验估计边际似然的方法与直接近似模型证据或后验模型概率的方法。

Result: 基于参数后验估计边际似然的方法始终优于直接近似模型证据的方法。当似然函数可用时，SC训练即使在严重模型错误设定下也能提高鲁棒性；但对于无法访问解析似然的方法，SC的益处有限且不一致。

Conclusion: 建议实用的摊销贝叶斯模型比较指南：优先选择基于参数后验的方法，并在经验数据集上使用SC训练来减轻模型错误设定下的外推偏差。

Abstract: Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.

</details>


### [20] [One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing](https://arxiv.org/abs/2512.13892)
*Albert Dorador*

Main category: stat.ML

TL;DR: 提出一种确定性最优置换方法替代传统随机置换，用于计算特征重要性，提高计算效率和稳定性，并引入系统性变量重要性用于模型压力测试。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型特征贡献的可靠估计对于信任、透明度和监管合规至关重要，特别是在模型为专有或黑盒的情况下。传统置换方法依赖重复随机置换，存在计算开销大和随机不稳定的问题。

Method: 用单个确定性最优置换替代多个随机置换，保留置换重要性核心原理的同时实现非随机、更快、更稳定的方法。引入系统性变量重要性，专门考虑特征相关性，用于模型压力测试。

Result: 在近200个场景（包括真实世界家庭金融和信用风险应用）中验证，在样本量小、高维、低信噪比等挑战性场景下表现出改进的偏差-方差权衡和准确性。系统性变量重要性能够揭示标准方法遗漏的依赖关系。

Conclusion: 确定性置换方法提供了一种更高效稳定的特征重要性评估方案，系统性变量重要性框架为模型压力测试提供了透明方法，能够量化相关特征间的冲击传播，有助于审计模型对受保护属性的隐藏依赖，评估公平性和系统性风险。

Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.

</details>


### [21] [Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics](https://arxiv.org/abs/2512.13997)
*Aaron Wei,Milad Jalali,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 该论文解决了MMD双样本检验中样本量不等时的理论限制，通过扩展广义U统计量理论，提出了新的渐近分布特征和检验功效优化准则，避免了数据丢弃问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于MMD的双样本检验方法通常假设两个分布具有相等的样本量，在实际应用中需要丢弃有价值的数据，这会降低检验功效。需要解决这一长期存在的限制。

Method: 扩展广义U统计量理论，将其应用于常规MMD估计量，推导出样本量不等时MMD估计量的渐近分布特征，并提出了新的检验功效优化准则。

Result: 获得了样本量不等时MMD估计量的新渐近分布特征，提出了保持所有可用数据的检验方法，增强了检验准确性和实际应用性。同时揭示了MMD估计量方差的新特征。

Conclusion: 该研究解决了MMD检验中样本量不等的重要理论问题，提供了更实用的检验框架，避免了数据丢弃，提高了检验功效，对实际应用有重要意义。

Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.

</details>


### [22] [On the Hardness of Conditional Independence Testing In Practice](https://arxiv.org/abs/2512.14000)
*Zheng He,Roman Pogodin,Yazhe Li,Namrata Deka,Arthur Gretton,Danica J. Sutherland*

Main category: stat.ML

TL;DR: KCI测试在实践中失败的原因：条件均值嵌入估计误差影响I类错误，条件核选择不当影响检验功效但会膨胀I类错误


<details>
  <summary>Details</summary>
Motivation: 虽然Shah和Peters(2020)证明了条件独立性检验不存在普遍有限样本有效检验，但这不能解释实践中KCI等检验的频繁失败。需要深入分析KCI测试在实际应用中的行为机制。

Method: 研究基于核的条件独立性(KCI)测试，分析其与广义协方差度量的关系，识别影响其实际性能的关键因素：条件均值嵌入估计误差对I类错误的影响，以及条件核选择对检验功效和I类错误的影响。

Result: 发现KCI测试中条件均值嵌入估计误差是影响I类错误的主要因素，而条件核的选择对检验功效至关重要但会膨胀I类错误。广义协方差度量是KCI测试的一个近似特例。

Conclusion: KCI测试的实践失败不仅源于理论上的不可能性定理，更源于具体实现中的技术问题：条件均值嵌入估计误差和条件核选择的权衡，这为改进条件独立性检验提供了具体方向。

Abstract: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on "hiding" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.

</details>


### [23] [Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms](https://arxiv.org/abs/2512.14221)
*Jiarong Fan,Juhyun Park. Thi Phuong Thuy Vo,Nicolas Brunel*

Main category: stat.ML

TL;DR: 提出一种处理缺失协变量的保形预测方法，通过预插补-掩码-校正框架，在保证边际覆盖的同时实现掩码条件有效性，显著减少预测区间宽度。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在缺失协变量时无法保证覆盖性，而掩码条件有效性比边际覆盖更理想。需要开发能处理各种缺失模式并保证覆盖性的方法。

Method: 提出预插补-掩码-校正框架：先对校准数据集进行分布插补（多重插补），然后通过重加权保形预测程序校正预测集。开发两种算法，与标准插补流程兼容。

Result: 方法在合成和真实数据集上验证，相比标准MCV方法显著减少预测区间宽度，同时保持目标保证（近似边际有效和MCV）。

Conclusion: 该方法为处理缺失协变量的保形预测提供了有效解决方案，在保证覆盖性的同时提高了预测效率，与现有插补流程兼容。

Abstract: Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.

</details>


### [24] [Continual Learning at the Edge: An Agnostic IIoT Architecture](https://arxiv.org/abs/2512.14311)
*Pablo García-Santaclara,Bruno Fernández-Castro,Rebeca P. Díaz-Redondo,Carlos Calvo-Moa,Henar Mariño-Bodelón*

Main category: stat.ML

TL;DR: 提出了一种将增量学习应用于工业边缘计算场景的新方法，用于制造系统的实时质量控制


<details>
  <summary>Details</summary>
Motivation: 物联网设备指数增长给传统集中式计算系统带来延迟和带宽挑战，边缘计算将计算靠近数据源以解决这些问题。传统机器学习算法不适合边缘计算系统，因为数据通常以动态连续方式到达，而增量学习为此提供了良好解决方案。

Method: 在工业边缘计算场景中应用增量学习理念，采用持续学习方法减少灾难性遗忘的影响，为制造系统提供实时质量控制解决方案。

Result: 该方法提供了高效有效的解决方案，通过持续学习减少灾难性遗忘的影响，适应边缘计算环境中数据的动态连续特性。

Conclusion: 将增量学习应用于工业边缘计算场景是解决制造系统实时质量控制的有效方法，能够应对物联网设备增长带来的计算挑战，并为动态数据环境提供适应性强的解决方案。

Abstract: The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.

</details>


### [25] [From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification](https://arxiv.org/abs/2512.14404)
*Hangjun Cho,Fabio V. G. Amaral,Andrei A. Klishin,Cassio M. Oishi,Steven L. Brunton*

Main category: stat.ML

TL;DR: 提出了一种基于分数引导的字典选择方法，用于改进SINDy类型算法的稀疏回归，通过理论分析和数值实验验证了该方法在动力系统识别中的有效性。


<details>
  <summary>Details</summary>
Motivation: 重新审视基于字典的稀疏回归方法（特别是顺序阈值最小二乘法STLS），为数据驱动建模提供实用指导，重点改进SINDy类型算法。现有方法在字典选择和稀疏性处理方面存在改进空间，需要更系统的方法来增强算法的准确性和可解释性。

Method: 1. 重新审视STLS算法，该算法通过分裂方法高效解决最小二乘部分，同时通过近端方法处理稀疏项；2. 提出基于分数的字典选择策略，利用投影重构误差（分数）和字典项之间的互相关性来指导字典选择；3. 在原始和弱SINDy两种机制下进行理论分析；4. 在常微分方程和偏微分方程上进行数值实验验证。

Result: 数值实验表明，基于分数的筛选方法显著提高了动力系统识别的准确性和可解释性。该方法能够更精确地细化字典，在某些情况下帮助SINDy用户增强数据驱动发现控制方程的鲁棒性。

Conclusion: 分数引导的字典选择方法为SINDy类型算法提供了实用的改进策略，通过理论分析和实验验证了其在稀疏回归和动力系统识别中的有效性，有助于提高数据驱动建模的准确性和鲁棒性。

Abstract: In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.

</details>


### [26] [LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts](https://arxiv.org/abs/2512.14604)
*Prasanjit Dubey,Aritra Guha,Zhengyi Zhou,Qiong Wu,Xiaoming Huo,Paromita Dubey*

Main category: stat.ML

TL;DR: 提出LLmFPCA-detect框架，结合LLM文本嵌入与功能数据分析，用于稀疏纵向文本数据的聚类和异常检测


<details>
  <summary>Details</summary>
Motivation: 稀疏纵向文本数据（如客户评论、社交媒体帖子、电子病历）具有巨大潜力，但缺乏专门方法，且数据噪声大、异质性强、易出现异常，难以检测关键模式

Method: 1. 使用LLM提示将文本嵌入到应用特定的数值空间；2. 在数值空间中进行稀疏多元功能主成分分析(mFPCA)恢复主要群体特征；3. 结合静态协变量进行数据分割、无监督异常检测和推断；4. 利用LLM进行动态关键词分析

Result: 在亚马逊客户评论轨迹和维基百科讨论页评论流两个公开数据集上验证了方法的有效性，优于现有基线方法，且能提升预测性能

Conclusion: LLmFPCA-detect是一个灵活框架，能够有效处理稀疏纵向文本数据，发现数据中的聚类和异常，支持下游任务，并在跨领域应用中表现出色

Abstract: Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.

</details>
