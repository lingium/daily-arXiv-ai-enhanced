{"id": "2601.03325", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03325", "abs": "https://arxiv.org/abs/2601.03325", "authors": ["Carles Balsells-Rodas", "Toshiko Matsui", "Pedro A. M. Mediano", "Yixin Wang", "Yingzhen Li"], "title": "On the Identifiability of Regime-Switching Models with Multi-Lag Dependencies", "comment": "See https://github.com/charlio23/identifiable-SDS for code", "summary": "Identifiability is central to the interpretability of deep latent variable models, ensuring parameterisations are uniquely determined by the data-generating distribution. However, it remains underexplored for deep regime-switching time series. We develop a general theoretical framework for multi-lag Regime-Switching Models (RSMs), encompassing Markov Switching Models (MSMs) and Switching Dynamical Systems (SDSs). For MSMs, we formulate the model as a temporally structured finite mixture and prove identifiability of both the number of regimes and the multi-lag transitions in a nonlinear-Gaussian setting. For SDSs, we establish identifiability of the latent variables up to permutation and scaling via temporal structure, which in turn yields conditions for identifiability of regime-dependent latent causal graphs (up to regime/node permutations). Our results hold in a fully unsupervised setting through architectural and noise assumptions that are directly enforceable via neural network design. We complement the theory with a flexible variational estimator that satisfies the assumptions and validate the results on synthetic benchmarks. Across real-world datasets from neuroscience, finance, and climate, identifiability leads to more trustworthy interpretability analysis, which is crucial for scientific discovery."}
{"id": "2601.03451", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03451", "abs": "https://arxiv.org/abs/2601.03451", "authors": ["Nassim Helou"], "title": "Microeconomic Foundations of Multi-Agent Learning", "comment": null, "summary": "Modern AI systems increasingly operate inside markets and institutions where data, behavior, and incentives are endogenous. This paper develops an economic foundation for multi-agent learning by studying a principal-agent interaction in a Markov decision process with strategic externalities, where both the principal and the agent learn over time. We propose a two-phase incentive mechanism that first estimates implementable transfers and then uses them to steer long-run dynamics; under mild regret-based rationality and exploration conditions, the mechanism achieves sublinear social-welfare regret and thus asymptotically optimal welfare. Simulations illustrate how even coarse incentives can correct inefficient learning under stateful externalities, highlighting the necessity of incentive-aware design for safe and welfare-aligned AI in markets and insurance."}
{"id": "2601.03533", "categories": ["stat.ML", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03533", "abs": "https://arxiv.org/abs/2601.03533", "authors": ["Vladimir Braverman", "Sumegha Garg", "Chen Wang", "David P. Woodruff", "Samson Zhou"], "title": "Online Learning with Limited Information in the Sliding Window Model", "comment": "SODA 2026", "summary": "Motivated by recent work on the experts problem in the streaming model, we consider the experts problem in the sliding window model. The sliding window model is a well-studied model that captures applications such as traffic monitoring, epidemic tracking, and automated trading, where recent information is more valuable than older data. Formally, we have $n$ experts, $T$ days, the ability to query the predictions of $q$ experts on each day, a limited amount of memory, and should achieve the (near-)optimal regret $\\sqrt{nW}\\text{polylog}(nT)$ regret over any window of the last $W$ days. While it is impossible to achieve such regret with $1$ query, we show that with $2$ queries we can achieve such regret and with only $\\text{polylog}(nT)$ bits of memory. Not only are our algorithms optimal for sliding windows, but we also show for every interval $\\mathcal{I}$ of days that we achieve $\\sqrt{n|\\mathcal{I}|}\\text{polylog}(nT)$ regret with $2$ queries and only $\\text{polylog}(nT)$ bits of memory, providing an exponential improvement on the memory of previous interval regret algorithms. Building upon these techniques, we address the bandit problem in data streams, where $q=1$, achieving $n T^{2/3}\\text{polylog}(T)$ regret with $\\text{polylog}(nT)$ memory, which is the first sublinear regret in the streaming model in the bandit setting with polylogarithmic memory; this can be further improved to the optimal $\\mathcal{O}(\\sqrt{nT})$ regret if the best expert's losses are in a random order."}
{"id": "2601.03994", "categories": ["stat.AP", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03994", "abs": "https://arxiv.org/abs/2601.03994", "authors": ["David Randahl", "Anders Hjort", "Jonathan P. Williams"], "title": "pintervals: an R package for model-agnostic prediction intervals", "comment": null, "summary": "The \\pkg{pintervals} package aims to provide a unified framework for constructing prediction intervals and calibrating predictions in a model-agnostic setting using set-aside calibration data. It comprises routines to construct conformal as well as parametric and bootstrapped prediction intervals from any model that outputs point predictions. Several R packages and functions already exist for constructing prediction intervals, but they often focus on specific modeling frameworks or types of predictions, or require manual customization for different models or applications. By providing a consistent interface for a variety of prediction interval construction approaches (all model-agnostic), \\pkg{pintervals} allows researchers to apply and compare them across different modeling frameworks and applications."}
{"id": "2601.04149", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04149", "abs": "https://arxiv.org/abs/2601.04149", "authors": ["Rose Yvette Bandolo Essomba", "Ernest Fokoué"], "title": "A Theoretical and Empirical Taxonomy of Imbalance in Binary Classification", "comment": "24 pages, 10 figures", "summary": "Class imbalance significantly degrades classification performance, yet its effects are rarely analyzed from a unified theoretical perspective. We propose a principled framework based on three fundamental scales: the imbalance coefficient $η$, the sample--dimension ratio $κ$, and the intrinsic separability $Δ$. Starting from the Gaussian Bayes classifier, we derive closed-form Bayes errors and show how imbalance shifts the discriminant boundary, yielding a deterioration slope that predicts four regimes: Normal, Mild, Extreme, and Catastrophic. Using a balanced high-dimensional genomic dataset, we vary only $η$ while keeping $κ$ and $Δ$ fixed. Across parametric and non-parametric models, empirical degradation closely follows theoretical predictions: minority Recall collapses once $\\log(η)$ exceeds $Δ\\sqrtκ$, Precision increases asymmetrically, and F1-score and PR-AUC decline in line with the predicted regimes. These results show that the triplet $(η,κ,Δ)$ provides a model-agnostic, geometrically grounded explanation of imbalance-induced deterioration."}
{"id": "2601.03299", "categories": ["stat.ME", "math.ST", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.03299", "abs": "https://arxiv.org/abs/2601.03299", "authors": ["Richik Chakraborty"], "title": "Progressive Bayesian Confidence Architectures for Cold-Start Personal Health Analytics: Formalizing Early Insight Through Posterior Contraction and Risk-Aware Interpretation", "comment": null, "summary": "Personal health analytics systems face a persistent cold-start dilemma: users expect meaningful insights early in data collection, while conventional statistical inference requires data volumes that often exceed engagement horizons. Existing approaches either delay inference until fixed statistical thresholds are met -- leading to user disengagement -- or surface heuristic insights without formal uncertainty quantification, risking false confidence. We propose a progressive Bayesian confidence architecture that formalizes early-stage inference through phased interpretation of posterior uncertainty. Drawing on Bayesian updating and epistemic strategies from financial risk modeling under sparse observations, we map posterior contraction to interpretable tiers of insight, ranging from exploratory directional evidence to robust associative inference. We demonstrate the framework's performance through controlled experimentation with synthetic N-of-1 health data, showing that calibrated early insights can be generated within 5--7 days while maintaining explicit epistemic humility. Compared to fixed-threshold baselines requiring 30+ days of data, the proposed approach yields earlier directional signals (mean: 5.3 vs 31.7 days, p<0.001) while controlling false discovery rates below 6% (5.9% at day 30) despite 26-day earlier detection, compared to 0% FDR for fixed-threshold baselines that delay insights by 30 days. In addition, we show strong uncertainty calibration (76% credible interval coverage for ground-truth correlations at day 90). This work contributes a methodological framework for uncertainty-aware early inference in personalized health analytics that bridges the gap between user engagement requirements and statistical rigor."}
{"id": "2601.03299", "categories": ["stat.ME", "math.ST", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.03299", "abs": "https://arxiv.org/abs/2601.03299", "authors": ["Richik Chakraborty"], "title": "Progressive Bayesian Confidence Architectures for Cold-Start Personal Health Analytics: Formalizing Early Insight Through Posterior Contraction and Risk-Aware Interpretation", "comment": null, "summary": "Personal health analytics systems face a persistent cold-start dilemma: users expect meaningful insights early in data collection, while conventional statistical inference requires data volumes that often exceed engagement horizons. Existing approaches either delay inference until fixed statistical thresholds are met -- leading to user disengagement -- or surface heuristic insights without formal uncertainty quantification, risking false confidence. We propose a progressive Bayesian confidence architecture that formalizes early-stage inference through phased interpretation of posterior uncertainty. Drawing on Bayesian updating and epistemic strategies from financial risk modeling under sparse observations, we map posterior contraction to interpretable tiers of insight, ranging from exploratory directional evidence to robust associative inference. We demonstrate the framework's performance through controlled experimentation with synthetic N-of-1 health data, showing that calibrated early insights can be generated within 5--7 days while maintaining explicit epistemic humility. Compared to fixed-threshold baselines requiring 30+ days of data, the proposed approach yields earlier directional signals (mean: 5.3 vs 31.7 days, p<0.001) while controlling false discovery rates below 6% (5.9% at day 30) despite 26-day earlier detection, compared to 0% FDR for fixed-threshold baselines that delay insights by 30 days. In addition, we show strong uncertainty calibration (76% credible interval coverage for ground-truth correlations at day 90). This work contributes a methodological framework for uncertainty-aware early inference in personalized health analytics that bridges the gap between user engagement requirements and statistical rigor."}
{"id": "2601.04138", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2601.04138", "abs": "https://arxiv.org/abs/2601.04138", "authors": ["Peilun He", "Han Lin Shang", "Nan Zou"], "title": "On the Distributed Estimation for Scalar-on-Function Regression Models", "comment": "33 pages, 1 figure", "summary": "This paper proposes distributed estimation procedures for three scalar-on-function regression models: the functional linear model (FLM), the functional non-parametric model (FNPM), and the functional partial linear model (FPLM). The framework addresses two key challenges in functional data analysis, namely the high computational cost of large samples and limitations on sharing raw data across institutions. Monte Carlo simulations show that the distributed estimators substantially reduce computation time while preserving high estimation and prediction accuracy for all three models. When block sizes become too small, the FPLM exhibits overfitting, leading to narrower prediction intervals and reduced empirical coverage probability. An example of an empirical study using the \\textit{tecator} dataset further supports these findings."}
{"id": "2601.03377", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03377", "abs": "https://arxiv.org/abs/2601.03377", "authors": ["Edoardo Efrem Gervasoni", "Liesbet De Bus", "Stijn Vansteelandt", "Oliver Dukes"], "title": "On estimands in target trial emulation", "comment": "38 pages, 11 figures", "summary": "The target trial framework enables causal inference from longitudinal observational data by emulating randomized trials initiated at multiple time points. Precision is often improved by pooling information across trials, with standard models typically assuming - among other things - a time-constant treatment effect. However, this obscures interpretation when the true treatment effect varies, which we argue to be likely as a result of relying on noncollapsible estimands. To address these challenges, this paper introduces a model-free strategy for target trial analysis, centered around the choice of the estimand, rather than model specification. This ensures that treatment effects remain clearly interpretable for well-defined populations even under model misspecification. We propose estimands suitable for different study designs, and develop accompanying G-computation and inverse probability weighted estimators. Applications on simulations and real data on antimicrobial de-escalation in an intensive care unit setting demonstrate the greater clarity and reliability of the proposed methodology over traditional techniques."}
{"id": "2601.03532", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.03532", "abs": "https://arxiv.org/abs/2601.03532", "authors": ["Andrew Gerard Roberts", "Michael Dietze", "Jonathan H. Huggins"], "title": "Propagating Surrogate Uncertainty in Bayesian Inverse Problems", "comment": null, "summary": "Standard Bayesian inference schemes are infeasible for inverse problems with computationally expensive forward models. A common solution is to replace the model with a cheaper surrogate. To avoid overconfident conclusions, it is essential to acknowledge the surrogate approximation by propagating its uncertainty. At present, a variety of distinct uncertainty propagation methods have been suggested, with little understanding of how they vary. To fill this gap, we propose a mixture distribution termed the expected posterior (EP) as a general baseline for uncertainty-aware posterior approximation, justified by decision theoretic and modular Bayesian inference arguments. We then investigate the expected unnormalized posterior (EUP), a popular heuristic alternative, analyzing when it may deviate from the EP baseline. Our results show that this heuristic can break down when the surrogate uncertainty is highly non-uniform over the design space, as can be the case when the log-likelihood is emulated by a Gaussian process. Finally, we present the random kernel preconditioned Crank-Nicolson (RKpCN) algorithm, an approximate Markov chain Monte Carlo scheme that provides practical EP approximation in the challenging setting involving infinite-dimensional Gaussian process surrogates."}
{"id": "2601.03453", "categories": ["stat.ME", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03453", "abs": "https://arxiv.org/abs/2601.03453", "authors": ["Ioannis Ivrissimtzis", "Shauna Concannon", "Matthew Houliston", "Graham Roberts"], "title": "Measures of classification bias derived from sample size analysis", "comment": "9 pages, 3 figures", "summary": "We propose the use of a simple intuitive principle for measuring algorithmic classification bias: the significance of the differences in a classifier's error rates across the various demographics is inversely commensurate with the sample size required to statistically detect them. That is, if large sample sizes are required to statistically establish biased behavior, the algorithm is less biased, and vice versa. In a simple setting, we assume two distinct demographics, and non-parametric estimates of the error rates on them, e1 and e2, respectively. We use a well-known approximate formula for the sample size of the chi-squared test, and verify some basic desirable properties of the proposed measure. Next, we compare the proposed measure with two other commonly used statistics, the difference e2-e1 and the ratio e2/e1 of the error rates. We establish that the proposed measure is essentially different in that it can rank algorithms for bias differently, and we discuss some of its advantages over the other two measures. Finally, we briefly discuss how some of the desirable properties of the proposed measure emanate from fundamental characteristics of the method, rather than the approximate sample size formula we used, and thus, are expected to hold in more complex settings with more than two demographics."}
{"id": "2601.03994", "categories": ["stat.AP", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03994", "abs": "https://arxiv.org/abs/2601.03994", "authors": ["David Randahl", "Anders Hjort", "Jonathan P. Williams"], "title": "pintervals: an R package for model-agnostic prediction intervals", "comment": null, "summary": "The \\pkg{pintervals} package aims to provide a unified framework for constructing prediction intervals and calibrating predictions in a model-agnostic setting using set-aside calibration data. It comprises routines to construct conformal as well as parametric and bootstrapped prediction intervals from any model that outputs point predictions. Several R packages and functions already exist for constructing prediction intervals, but they often focus on specific modeling frameworks or types of predictions, or require manual customization for different models or applications. By providing a consistent interface for a variety of prediction interval construction approaches (all model-agnostic), \\pkg{pintervals} allows researchers to apply and compare them across different modeling frameworks and applications."}
{"id": "2601.03480", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03480", "abs": "https://arxiv.org/abs/2601.03480", "authors": ["Apu Chandra Das", "Sakib Salam", "Aninda Roy", "Rakhi Chowdhury", "Antar Chandra Das", "Ashim Chandra Das"], "title": "Improving operating characteristics of clinical trials by augmenting control arm using propensity score-weighted borrowing-by-parts power prior", "comment": "25 pages, 1 figure, 7 tables", "summary": "Borrowing external data can improve estimation efficiency but may introduce bias when populations differ in covariate distributions or outcome variability. A proper balance needs to be maintained between the two datasets to justify the borrowing. We propose a propensity score weighting borrowing-by-parts power prior (PSW-BPP) that integrates causal covariate adjustment through propensity score weighting with a flexible Bayesian borrowing approach to address these challenges in a unified framework. The proposed approach first applies propensity score weighting to align the covariate distribution of the external data with that of the current study, thereby targeting a common estimand and reducing confounding due to population heterogeneity. The weighted external likelihood is then incorporated into a Bayesian model through a borrowing-by-parts power prior, which allows distinct power parameters for the mean and variance components of the likelihood, enabling differential and calibrated information borrowing. Additionally, we adopt the idea of the minimal plausibility index (mPI) to calculate the power parameters. This separate borrowing provides greater robustness to prior-data conflict compared with traditional power prior methods that impose a single borrowing parameter. We study the operating characteristics of PSW-BPP through extensive simulation and a real data example. Simulation studies demonstrate that PSW-BPP yields more efficient and stable estimation than no borrowing and fixed borrowing, particularly under moderate covariate imbalance and outcome heterogeneity. The proposed framework offers a principled and extensible methodological contribution for Bayesian inference with external data in observational and hybrid study designs."}
{"id": "2601.03497", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03497", "abs": "https://arxiv.org/abs/2601.03497", "authors": ["Shuo Wang", "Joseph Feldman", "Jerome P. Reiter"], "title": "Differentially Private Bayesian Inference for Gaussian Copula Correlations", "comment": null, "summary": "Gaussian copulas are widely used to estimate multivariate distributions and relationships. We present algorithms for estimating Gaussian copula correlations that ensure differential privacy. We first convert data values into sets of two-way tables of counts above and below marginal medians. We then add noise to these counts to satisfy differential privacy. We utilize the one-to-one correspondence between the true counts and the copula correlation to estimate a posterior distribution of the copula correlation given the noisy counts, marginalizing over the distribution of the underlying true counts using a composite likelihood. We also present an alternative, maximum likelihood approach for point estimation. Using simulation studies, we compare these methods to extant methods in the literature for computing differentially private copula correlations."}
{"id": "2601.03532", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.03532", "abs": "https://arxiv.org/abs/2601.03532", "authors": ["Andrew Gerard Roberts", "Michael Dietze", "Jonathan H. Huggins"], "title": "Propagating Surrogate Uncertainty in Bayesian Inverse Problems", "comment": null, "summary": "Standard Bayesian inference schemes are infeasible for inverse problems with computationally expensive forward models. A common solution is to replace the model with a cheaper surrogate. To avoid overconfident conclusions, it is essential to acknowledge the surrogate approximation by propagating its uncertainty. At present, a variety of distinct uncertainty propagation methods have been suggested, with little understanding of how they vary. To fill this gap, we propose a mixture distribution termed the expected posterior (EP) as a general baseline for uncertainty-aware posterior approximation, justified by decision theoretic and modular Bayesian inference arguments. We then investigate the expected unnormalized posterior (EUP), a popular heuristic alternative, analyzing when it may deviate from the EP baseline. Our results show that this heuristic can break down when the surrogate uncertainty is highly non-uniform over the design space, as can be the case when the log-likelihood is emulated by a Gaussian process. Finally, we present the random kernel preconditioned Crank-Nicolson (RKpCN) algorithm, an approximate Markov chain Monte Carlo scheme that provides practical EP approximation in the challenging setting involving infinite-dimensional Gaussian process surrogates."}
{"id": "2601.03647", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03647", "abs": "https://arxiv.org/abs/2601.03647", "authors": ["Koki Momoki", "Takuma Yoshida"], "title": "Small area estimation of dependent extreme value indices", "comment": "32 pages, 9 figures", "summary": "In extreme value analysis, tail behavior of a heavy-tailed data distribution is modeled by a Pareto-type distribution in which the so-called extreme value index (EVI) controls the tail behavior. For heavy-tailed data obtained from multiple population subgroups, or areas, this study efficiently predicts the EVIs of all areas using information among areas. For this purpose, we propose a mixed effects model, which is a useful approach in small area estimation. In this model, we represent differences among areas in the EVIs by latent variables called random effects. Using correlated random effects across areas, we incorporate the relations among areas into the model. The obtained model achieves simultaneous prediction of EVIs of all areas. Herein, we describe parameter estimation and random effect prediction in the model, and clarify theoretical properties of the estimator. Additionally, numerical experiments are presented to demonstrate the effectiveness of the proposed method. As an application of our model, we provide a risk assessment of heavy rainfall in Japan."}
{"id": "2601.03674", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03674", "abs": "https://arxiv.org/abs/2601.03674", "authors": ["Yuanying Chen", "Tongyu Li", "Yang Bai", "Zhenhua Lin"], "title": "Multi-transport Distributional Regression", "comment": null, "summary": "We study distribution-on-distribution regression problems in which a response distribution depends on multiple distributional predictors. Such settings arise naturally in applications where the outcome distribution is driven by several heterogeneous distributional sources, yet remain challenging due to the nonlinear geometry of the Wasserstein space. We propose an intrinsic regression framework that aggregates predictor-specific transported distributions through a weighted Fréchet mean in the Wasserstein space. The resulting model admits multiple distributional predictors, assigns interpretable weights quantifying their relative contributions, and defines a flexible regression operator that is invariant to auxiliary construction choices, such as the selection of a reference distribution. From a theoretical perspective, we establish identifiability of the induced regression operator and derive asymptotic guarantees for its estimation under a predictive Wasserstein semi-norm, which directly characterizes convergence of the composite prediction map. Extensive simulation studies and a real data application demonstrate the improved predictive performance and interpretability of the proposed approach compared with existing Wasserstein regression methods."}
{"id": "2601.03675", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03675", "abs": "https://arxiv.org/abs/2601.03675", "authors": ["Fangyong Zheng", "Pengfei Li", "Tao Yu"], "title": "Maximum smoothed likelihood method for the combination of multiple diagnostic tests, with application to the ROC estimation", "comment": null, "summary": "In medical diagnostics, leveraging multiple biomarkers can significantly improve classification accuracy compared to using a single biomarker. While existing methods based on exponential tilting or density ratio models have shown promise, their assumptions may be overly restrictive in practice. In this paper, we adopt a flexible semiparametric model that relates the density ratio of diseased to healthy subjects through an unknown monotone transformation of a linear combination of biomarkers. To enhance estimation efficiency, we propose a smoothed likelihood framework that exploits the smoothness in the underlying densities and transformation function. Building on the maximum smoothed likelihood methodology, we construct estimators for the model parameters and the associated probability density functions. We develop an effective computational algorithm for implementation, derive asymptotic properties of the proposed estimators, and establish procedures for estimating the receiver operating characteristic (ROC) curve and the area under the curve (AUC). Through simulation studies and a real-data application, we demonstrate that the proposed method yields more accurate and efficient estimates than existing approaches."}
{"id": "2601.03760", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03760", "abs": "https://arxiv.org/abs/2601.03760", "authors": ["Katharina Ammann", "Timo Adam", "Jan-Ole Koslik"], "title": "Non-Homogeneous Markov-Switching Generalized Additive Models for Location, Scale, and Shape", "comment": null, "summary": "We propose an extension of Markov-switching generalized additive models for location, scale, and shape (MS-GAMLSS) that allows covariates to influence not only the parameters of the state-dependent distributions but also the state transition probabilities. Traditional MS-GAMLSS, which combine distributional regression with hidden Markov models, typically assume time-homogeneous (i.e., constant) transition probabilities, thereby preventing regime shifts from responding to covariate-driven changes. Our approach overcomes this limitation by modeling the transition probabilities as smooth functions of covariates, enabling a flexible, data-driven characterization of covariate-dependent regime dynamics. Estimation is carried out within a penalized likelihood framework, where automatic smoothness selection controls model complexity and guards against overfitting. We evaluate the proposed methodology through simulations and applications to daily Lufthansa stock prices and Spanish energy prices. Our results show that incorporating macroeconomic indicators into the transition probabilities yields additional insights into market dynamics. Data and R code to reproduce the results are available online."}
{"id": "2601.03777", "categories": ["stat.ME", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03777", "abs": "https://arxiv.org/abs/2601.03777", "authors": ["Md Nafees Fuad Rafi", "Zhaomiao Guo"], "title": "Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems", "comment": null, "summary": "While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits. This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems. We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing. Such a model allows analyses of the impact of decentralized decision-making on multimodal mobility efficiencies. The proposed formulation can be further convexified to efficiently compute the equilibrium ride-sourcing prices. We conduct numerical experiments on different settings of transportation networks to gain policy insights. We find that travelers prefer ride-sourcing and multimodal transportation more than the driving option when they are more sensitive to prices. We also find that travelers may need to be subsidized to use multimodal transportation when there is fewer transit hubs in the network or, ride-sourcing drivers become too sensitive to the prices. However, we find that more transit hubs in the network increases the total empty VMT of ride-sourcing drivers by increasing the total relocation time. The proposed model can be used by policymakers and platform operators to design pricing and subsidy schemes that align individual decision-making with system-level efficiency and evaluate the trade-offs between accessibility and environmental impacts in multimodal transportation networks."}
{"id": "2601.03815", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03815", "abs": "https://arxiv.org/abs/2601.03815", "authors": ["Shizhe Hong", "Weiming Li", "Guangming Pan"], "title": "High-Dimensional Precision Matrix Quadratic Forms: Estimation Framework for $p > n$", "comment": null, "summary": "We propose a novel estimation framework for quadratic functionals of precision matrices in high-dimensional settings, particularly in regimes where the feature dimension $p$ exceeds the sample size $n$. Traditional moment-based estimators with bias correction remain consistent when $p<n$ (i.e., $p/n \\to c <1$). However, they break down entirely once $p>n$, highlighting a fundamental distinction between the two regimes due to rank deficiency and high-dimensional complexity. Our approach resolves these issues by combining a spectral-moment representation with constrained optimization, resulting in consistent estimation under mild moment conditions.\n  The proposed framework provides a unified approach for inference on a broad class of high-dimensional statistical measures. We illustrate its utility through two representative examples: the optimal Sharpe ratio in portfolio optimization and the multiple correlation coefficient in regression analysis. Simulation studies demonstrate that the proposed estimator effectively overcomes the fundamental $p>n$ barrier where conventional methods fail."}
{"id": "2601.03909", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03909", "abs": "https://arxiv.org/abs/2601.03909", "authors": ["Clara Bertinelli Salucci"], "title": "Asymptotic distribution of the likelihood ratio test statistic with inequality-constrained nuisance parameters", "comment": "23 pages, 8 figures", "summary": "The asymptotic distribution of the likelihood-ratio statistic for testing parameters on the boundary is well known to be a chi-squared mixture. The mixture weights have been shown to correspond to the intrinsic volumes of an associated tangent cone, unifying a wide range of previously isolated special cases. While the weights are fully understood for an arbitrary number of parameters of interest on the boundary, much less is known when nuisance parameters are also constrained to the boundary, a situation that frequently arises in applications. We provide the first general characterization of the asymptotic distribution of the likelihood-ratio test statistic when both the number of parameters of interest and the number of nuisance parameters on the boundary are arbitrary. We analyze how the cone geometry changes when moving from a problem with K parameters of interest on the boundary to one with K-m parameters of interest and m nuisances. In the orthogonal case we show that the resulting change in the chi-bar weights admits a closed-form difference pattern that redistributes probability mass across adjacent degrees of freedom, and that this pattern remains the dominant component of the weight shift under arbitrary covariance structures when the nuisance vector is one-dimensional. For a generic number of nuisance parameters, we introduce a new rank-based aggregation of intrinsic volumes that yields an accurate approximation of the mixture weights. Comprehensive simulations support the theory and demonstrate the accuracy of the proposed approximation."}
{"id": "2601.03957", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03957", "abs": "https://arxiv.org/abs/2601.03957", "authors": ["Paul Guillot", "Antoine Godichon-Baggioni", "Stéphane Robin", "Laure Sansonnet"], "title": "Online robust covariance matrix estimation and outlier detection", "comment": null, "summary": "Robust estimation of the covariance matrix and detection of outliers remain major challenges in statistical data analysis, particularly when the proportion of contaminated observations increases with the size of the dataset. Outliers can severely bias parameter estimates and induce a masking effect, whereby some outliers conceal the presence of other outliers, further complicating their detection. Although many approaches have been proposed for covariance estimation and outlier detection, to our knowledge, none of these methods have been implemented in an online setting. In this paper, we focus on online covariance matrix estimation and outlier detection. Specifically, we propose a new method for simultaneously and online estimating the geometric median and variance, which allows us to calculate the Mahalanobis distance for each incoming data point before deciding whether it should be considered an outlier. To mitigate the masking effect, robust estimation techniques for the mean and variance are required. Our approach uses the geometric median for robust estimation of the location and the median covariance matrix for robust estimation of the dispersion parameters. The new online methods proposed for parameter estimation and outlier detection allow real-time identification of outliers as data are observed sequentially. The performance of our methods is demonstrated on simulated datasets."}
{"id": "2601.04066", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.04066", "abs": "https://arxiv.org/abs/2601.04066", "authors": ["Tomeu López-Nieto-Veitch", "Rossella De Sabbata", "Ryung Kim", "Sven Ove Samuelsen", "Nathalie C. Støer", "Vivian Viallon"], "title": "On the estimation of inclusion probabilities for weighted analyses of nested case control studies", "comment": null, "summary": "Nested case-control (NCC) studies are a widely adopted design in epidemiology to investigate exposure-disease relationships. This paper examines weighted analyses in NCC studies, focusing on two prominent weighting methods: Kaplan-Meier (KM) weights and Generalized Additive Model (GAM) weights. We consider three target estimands: log-hazard ratios, conditional survival, and associations between exposures. While KM- and GAM-weights are generally robust, we identify specific scenarios where they can lead to biased estimates. We demonstrate that KM-weights can lead to biased estimates when a proportion of the originating cohort is effectively ineligible for NCC selection, particularly with small case proportions or numerous matching factors. Instead, GAM-weights can yield biased results if interactions between matching factors influence disease risk and are not adequately incorporated into weight calculation. Using Directed Acyclic Graphs (DAGs), we develop a framework to systematically determine which variables should be included in weight calculations. We show that the optimal set of variables depends on the target estimand and the causal relationships between matching factors, exposures, and disease risk. We illustrate our findings with both synthetic and real data from the European Prospective Investigation into Cancer and nutrition (EPIC) study. Additionally, we extend the application of GAM-weights to \"untypical\" NCC studies, where only a subset of cases are included. Our work provides crucial insights for conducting accurate and robust weighted analyses in NCC studies."}
{"id": "2601.04192", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.04192", "abs": "https://arxiv.org/abs/2601.04192", "authors": ["Edoardo Ratti", "Federico L. Perlino", "Stefania Galimberti", "Maria G. Valsecchi"], "title": "Prediction Intervals for Interim Events in Randomized Clinical Trials with Time-to-Event Endpoints", "comment": "35 pages, 18 figures", "summary": "Time-to-event endpoints are central to evaluate treatment efficacy across many disease areas. Many trial protocols include interim analyses within group-sequential designs that control type I error via spending functions or boundary methods. The corresponding operating characteristics depend on the number of looks and the information accrued. Planning interim analyses with time-to-event endpoints is challenging because statistical information depends on the number of observed events. Ensuring adequate follow-up to accrue the required events is therefore critical, making interim prediction of information at scheduled looks and at the final analysis essential. While several methods have been developed to predict the calendar time required to reach a target number of events, to the best of our knowledge there is no established framework that addresses the prediction of the number of events at a future date with corresponding prediction intervals. Starting from an prediction interval approach originally developed in reliability engineering for the number of future component failures, we reformulated and extended it to the context of interim monitoring in clinical trials. This adaptation yields a general framework for event-count prediction intervals in the clinical setting, taking the patient as the unit of analysis and accommodating a range of parametric survival models, patient-level covariates, stagged entry and possible dependence between entry dates and lost to follow-up. Prediction intervals are obtained in a frequentist framework from a bootstrap estimator of the conditional distribution of future events. The performance of the proposed approach is investigated via simulation studies and illustrated by analyzing a real-world phase III trial in childhood acute lymphoblastic leukaemia."}
{"id": "2601.03994", "categories": ["stat.AP", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.03994", "abs": "https://arxiv.org/abs/2601.03994", "authors": ["David Randahl", "Anders Hjort", "Jonathan P. Williams"], "title": "pintervals: an R package for model-agnostic prediction intervals", "comment": null, "summary": "The \\pkg{pintervals} package aims to provide a unified framework for constructing prediction intervals and calibrating predictions in a model-agnostic setting using set-aside calibration data. It comprises routines to construct conformal as well as parametric and bootstrapped prediction intervals from any model that outputs point predictions. Several R packages and functions already exist for constructing prediction intervals, but they often focus on specific modeling frameworks or types of predictions, or require manual customization for different models or applications. By providing a consistent interface for a variety of prediction interval construction approaches (all model-agnostic), \\pkg{pintervals} allows researchers to apply and compare them across different modeling frameworks and applications."}
