<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 13]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.OT](#stat.OT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [An index of effective number of variables for uncertainty and reliability analysis in model selection problems](https://arxiv.org/abs/2602.21403)
*Luca Martino,Eduardo Morgado,Roberto San Millán-Castillo*

Main category: stat.ME

TL;DR: 提出ENV指数用于嵌套模型选择，基于最大AUC思想，改进现有肘部检测方法，提供置信度度量，可与AIC/BIC等准则结合使用。


<details>
  <summary>Details</summary>
Motivation: 在多项式阶数选择、聚类数量确定、特征选择等嵌套模型选择问题中，现有肘部检测方法存在不足，需要更可靠的选择指标和置信度评估。

Method: 引入有效变量数(ENV)指数，基于最大曲线下面积(AUC)思想，提供不同置信度度量，可与AIC、BIC等信息准则结合使用。

Result: ENV指数改进了现有肘部检测方法的缺点，在真实数据集实验中与经典和最新方案比较表现出优势，并提供相关Matlab代码。

Conclusion: ENV指数为嵌套模型选择提供了有效工具，改进了肘部检测方法，提供置信度评估，可与现有信息准则结合使用，具有实用价值。

Abstract: An index of an effective number of variables (ENV) is introduced for model selection in nested models. This is the case, for instance, when we have to decide the order of a polynomial function or the number of bases in a nonlinear regression, choose the number of clusters in a clustering problem, or the number of features in a variable selection application (to name few examples). It is inspired by the idea of the maximum area under the curve (AUC). The interpretation of the ENV index is identical to the effective sample size (ESS) indices concerning a set of samples. The ENV index improves {drawbacks of} the elbow detectors described in the literature and introduces different confidence measures of the proposed solution. These novel measures can be also employed jointly with the use of different information criteria, such as the well-known AIC and BIC, or any other model selection procedures. Comparisons with classical and recent schemes are provided in different experiments involving real datasets. Related Matlab code is given.

</details>


### [2] [Adaptive Penalized Doubly Robust Regression for Longitudinal Data](https://arxiv.org/abs/2602.21711)
*Yuyao Wang,Yu Lu,Tianni Zhang,Mengfei Ran*

Main category: stat.ME

TL;DR: 提出DAR-R方法，用于纵向线性混合效应模型，结合稳健性、自适应权重和高维变量选择，处理异质性、稀疏信号和异常值问题。


<details>
  <summary>Details</summary>
Motivation: 纵向数据常存在异质性、稀疏信号、响应异常值和高杠杆观测污染等问题。现有方法要么只关注惩罚混合效应建模而缺乏稳健性，要么只关注稳健混合效应估计而缺乏高维变量选择，无法全面解决这些问题。

Method: 提出双重自适应稳健回归(DAR-R)框架：1) 稳健初始拟合；2) 对残差异常值和高杠杆点的双重自适应观测权重；3) 固定效应选择的折叠凹惩罚；4) 随机效应和方差分量的加权更新。开发迭代重加权算法。

Result: 建立了估计和预测误差界、支持恢复一致性、oracle型渐近正态性。模拟显示DAR-R在垂直异常值和坏杠杆污染下提高了估计精度、假阳性控制和协方差估计。在阿尔茨海默病应用中，DAR-R实现了ADAS13的准确稳定预测，选择了临床有意义的预测因子，具有强重采样稳定性。

Conclusion: DAR-R框架有效解决了纵向数据中的异质性、稀疏信号和污染问题，结合了稳健性、自适应权重和高维变量选择，在理论和实际应用中均表现出优越性能。

Abstract: Longitudinal data often involve heterogeneity, sparse signals, and contamination from response outliers or high-leverage observations especially in biomedical science. Existing methods usually address only part of this problem, either emphasizing penalized mixed effects modeling without robustness or robust mixed effects estimation without high-dimensional variable selection. We propose a doubly adaptive robust regression (DAR-R) framework for longitudinal linear mixed effects models. It combines a robust pilot fit, doubly adaptive observation weights for residual outliers and leverage points, and folded concave penalization for fixed effect selection, together with weighted updates of random effects and variance components. We develop an iterative reweighting algorithm and establish estimation and prediction error bounds, support recovery consistency, and oracle-type asymptotic normality. Simulations show that DAR-R improves estimation accuracy, false-positive control, and covariance estimation under both vertical outliers and bad leverage contamination. In the TADPOLE/ADNI Alzheimer's disease application, DAR-R achieves accurate and stable prediction of ADAS13 while selecting clinically meaningful predictors with strong resampling stability.

</details>


### [3] [Discussion of "Matrix Completion When Missing Is Not at Random and Its Applications in Causal Panel Data Models"](https://arxiv.org/abs/2602.21314)
*Eli Ben-Michael,Avi Feller*

Main category: stat.ME

TL;DR: 本文是对Choi和Yuan(2025)关于矩阵补全在面板数据因果效应估计中应用方法的讨论性分析，从"分割-应用-组合"策略和统计"最后一公里问题"两个角度进行评述。


<details>
  <summary>Details</summary>
Motivation: 讨论Choi和Yuan(2025)提出的将矩阵补全应用于面板数据因果效应估计的新方法，分析该方法在存在结构化缺失数据（即处理选择）情况下的有效性，特别是当处理观测数量相对于控制观测数量较小时。

Method: 从两个互补视角进行分析：1) 将该方法定位为现代面板数据估计器中常见的"分割-应用-组合"策略的实例；2) 讨论理论与实践之间的统计"最后一公里问题"，并提出部分解决方案建议。

Result: 通过将矩阵补全方法应用于持枪权法律对暴力犯罪影响的研究案例，展示了该方法在估计公共政策影响时的应用潜力，同时指出了面板数据政策评估面临的挑战。

Conclusion: 矩阵补全方法为面板数据因果推断提供了有前景的新工具，但需要解决理论与实践之间的差距问题。作者建议通过更细致的实证策略来部分解决"最后一公里问题"，并强调在公共政策评估中谨慎应用的重要性。

Abstract: Choi and Yuan (2025) propose a novel approach to applying matrix completion to the problem of estimating causal effects in panel data. The key insight is that even in the presence of structured patterns of missing data -- i.e. selection into treatment -- matrix completion can be effective if the number of treated observations is small relative to the number of control observations. We applaud the authors for their insightful and interesting paper. We discuss this proposal from two complementary perspectives. First, we situate their proposal as an example of a "split-apply-combine" strategy that underlies many modern panel data estimators, including difference-in-differences and synthetic control approaches. Second, we discuss the issue of the statistical "last mile problem" -- the gap between theory and practice -- and offer suggestions on how to partially address it. We conclude by considering the challenges of estimating the impacts of public policies using panel data and apply the approach to a study on the effect of right to carry laws on violent crime.

</details>


### [4] [Evaluating time-varying treatment effects in hybrid SMART-MRT designs](https://arxiv.org/abs/2602.21383)
*Mengbing Li,Inbal Nahum-Shani,Walter Dempsey*

Main category: stat.ME

TL;DR: 本文提出了一种在混合SMART-MRT设计中形式化定义和评估数字与人工干预协同效应的方法，并应用于减少大学生酗酒的实际研究。


<details>
  <summary>Details</summary>
Motivation: 混合实验设计(HED)整合了人工干预和数字干预，但缺乏形式化方法来评估这两种干预在不同时间尺度上的协同效应，特别是在混合SMART-MRT设计中。

Method: 在混合SMART-MRT设计中正式定义和评估协同效应，包括近端和远端结果，并将方法应用于M-Bridge研究（旨在减少大一学生酗酒的混合SMART-MRT）。

Result: 开发了形式化框架来量化数字与人工干预的协同效应，并通过M-Bridge研究的实际分析展示了方法的实用性。

Conclusion: 本文为混合SMART-MRT设计提供了评估数字与人工干预协同效应的形式化方法，填补了该领域的方法学空白，具有实际应用价值。

Abstract: Recently a new experimental approach, the hybrid experimental design (HED), was introduced to enable investigators to answer scientific questions about building behavioral interventions in which human-delivered and digital components are integrated and adapted on multiple timescales: slow (e.g., every few weeks) and fast (e.g., every few hours), respectively. An increasingly common HED involves the integration of the sequential, multiple assignment, randomized trial (SMART) with the micro-randomized trial (MRT), allowing investigators to answer scientific questions about potential synergistic effects of digital and human-delivered interventions. Approaches to formalize these questions in terms of causal estimands and associated data analytic methods are limited. In this paper, we formally define and assess these synergistic effects in hybrid SMART-MRTs on both proximal and distal outcomes. Practical utility is shown through the analysis of M-Bridge, a hybrid SMART-MRT aimed at reducing binge drinking among first-year college students.

</details>


### [5] [Identifying the potential of sample overlap in evidence synthesis of observational studies](https://arxiv.org/abs/2602.21410)
*Zhentian Zhang,Tim Friede,Tim Mathes*

Main category: stat.ME

TL;DR: 开发了一种基于集合论和样本特征编码的方法，用于评估基于现有数据的研究在证据合成中的样本重叠程度，无需个体参与者数据。


<details>
  <summary>Details</summary>
Motivation: 医学研究中，特别是在使用注册表等现有数据库的观察性研究中，样本重叠是证据合成的常见问题。由于通常无法获取每个观察的唯一标识符，解决样本重叠问题一直很复杂，可能导致证据合成结果偏差并削弱其可信度。

Method: 基于集合论开发了一种方法，通过编码几个精心选择的样本特征范围来构建样本重叠程度的指标。该方法基于样本特征进行推断，而非个体参与者数据，提供了一种实用的解决方案。

Result: 该方法能够推导出有用信息，如证据合成中具有最大样本量的无重叠样本集。在多个真实世界证据合成中应用了该模型，证明了其有效性和灵活性。

Conclusion: 研究结果强调了在证据合成中解决样本重叠问题的重要性日益增长，特别是在数据二次使用越来越相关的背景下，这是一个目前研究不足的领域。

Abstract: Sample overlap is a common issue in evidence synthesis in the field of medical research, particularly when integrating findings from observational studies utilizing existing databases such as registries. Due to the general inaccessibility of unique identifiers for each observation, addressing sample overlap has been a complex problem, potentially biasing evidence synthesis outcomes and undermining their credibility. We developed a method to construct indicators for the degree of sample overlap in evidence synthesis of studies based on existing data. Our method is rooted in set theory and is based on the coding of the ranges of several well selected sample characteristics, offers a practical solution by focusing on making inference based on sample characteristics rather than on individual participant data. Useful information, such as the overlap-free sample set with the largest sample size in an evidence synthesis, can be derived from this method. We applied our model to several real-world evidence syntheses, demonstrating its effectiveness and flexibility. Our findings highlight the growing importance of addressing sample overlap in evidence synthesis, especially with the increasing relevance of secondary use of data, an area currently under-explored in research.

</details>


### [6] [Connection Probabilities Estimation in Multi-layer Networks via Iterative Neighborhood Smoothing](https://arxiv.org/abs/2602.21490)
*Dingzi Guo,Diqing Li,Jingyi Wang,Wen-Xin Zhou*

Main category: stat.ME

TL;DR: 提出MICE方法用于多层网络连接概率估计，通过迭代框架联合优化层间和层内相似性，理论证明具有一致性，在模拟和真实数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多层网络的结构机制分析对理解复杂系统至关重要，但现有方法在估计多层网络连接概率方面存在不足，需要更准确且计算高效的方法。

Method: 提出MICE方法：采用迭代框架，通过动态更新基于当前概率估计的距离度量，联合优化层间和层内相似性集合，利用层级和节点级邻域信息。

Result: 理论分析证明估计量具有一致性，在温和正则条件下达到与oracle估计量相当的最优收敛率。模拟和实证研究（ADHD脑网络、全球食品贸易网络）显示MICE在连接预测任务中优于现有方法。

Conclusion: MICE为多层网络系统的概率建模和推断提供了一个理论可靠、实际可扩展的框架，在复杂网络分析中具有重要应用价值。

Abstract: Understanding the structural mechanisms of multi-layer networks is essential for analyzing complex systems characterized by multiple interacting layers. This work studies the problem of estimating connection probabilities in multi-layer networks and introduces a new Multi-layer Iterative Connection Probability Estimation (MICE) method. The proposed approach employs an iterative framework that jointly refines inter-layer and intra-layer similarity sets by dynamically updating distance metrics derived from current probability estimates. By leveraging both layer-level and node-level neighborhood information, MICE improves estimation accuracy while preserving computational efficiency. Theoretical analysis establishes the consistency of the estimator and shows that, under mild regularity conditions, the proposed method achieves an optimal convergence rate comparable to that of an oracle estimator. Extensive simulation studies across diverse graphon structures demonstrate the superior performance of MICE relative to existing methods. Empirical evaluations using brain network data from patients with Attention-Deficit/Hyperactivity Disorder (ADHD) and global food and agricultural trade network data further illustrate the robustness and effectiveness of the method in link prediction tasks. Overall, this work provides a theoretically grounded and practically scalable framework for probabilistic modeling and inference in multi-layer network systems.

</details>


### [7] [Robust Model Selection for Discovery of Latent Mechanistic Processes](https://arxiv.org/abs/2602.22062)
*Jiawei Li,Nguyen Nguyen,Meng Lai,Ioannis Ch. Paschalidis,Jonathan H. Huggins*

Main category: stat.ME

TL;DR: 提出ACDC准则，结合似然方法的敏感性和非参数方法的鲁棒性，用于在模型误设时稳健地选择有机制意义的潜在过程数量。


<details>
  <summary>Details</summary>
Motivation: 基于模型的方法学习可解释潜在结构时，即使与建模假设有微小偏差，也会导致推断结果缺乏机制意义。现有方法存在矛盾：似然方法会高估潜在过程数量，而非参数方法又过于保守。

Method: 提出累积截断差异准则(ACDC)，利用组件级差异的插件估计，基于捕获模型机制结构的组件级差异度量。为包括概率矩阵分解和混合模型在内的潜变量模型开发了有机制意义的组件级差异度量。

Result: 理论证明ACDC在应用于无监督矩阵分解和混合模型时具有鲁棒一致性。数值实验表明，该方法在多种应用中能可靠识别有机制意义的潜在过程数量，优于现有方法。

Conclusion: ACDC成功结合了似然方法的敏感性和非参数方法的鲁棒性，能够在模型误设情况下稳健地选择有机制意义的潜在过程数量，为潜变量模型选择提供了有效解决方案。

Abstract: When learning interpretable latent structures using model-based approaches, even small deviations from modeling assumptions can lead to inferential results that are not mechanistically meaningful. In this work, we consider latent structures that consist of $K_o$ mechanistic processes, where $K_o$ is unknown. When the model is misspecified, likelihood-based model selection methods can substantially overestimate $K_o$ while more robust nonparametric methods can be overly conservative. Hence, there is a need for approaches that combine the sensitivity of likelihood-based methods with the robustness of nonparametric ones. We formalize this objective in terms of a robust model selection consistency property, which is based on a component-level discrepancy measure that captures the mechanistic structure of the model. We then propose the accumulated cutoff discrepancy criterion (ACDC), which leverages plug-in estimates of component-level discrepancies. To apply ACDC, we develop mechanistically meaningful component-level discrepancies for a general class of latent variable models that includes unsupervised and supervised variants of probabilistic matrix factorization and mixture modeling. We show that ACDC is robustly consistent when applied to unsupervised matrix factorization and mixture models. Numerical results demonstrate that in practice our approach reliably identifies a mechanistically meaningful number of latent processes in numerous illustrative applications, outperforming existing methods.

</details>


### [8] [Asymptotically Optimal Sequential Confidence Interval for the Gini Index Under Complex Household Survey Design with Sub-Stratification](https://arxiv.org/abs/2602.21579)
*Shivam,Bhargab Chattopadhyay,Nil Kamal Hazra*

Main category: stat.ME

TL;DR: 本文研究了复杂抽样设计下基尼系数估计量的最优性，提出了纯序贯和两阶段两种方法，证明了估计量的均匀连续性和渐近效率，并通过模拟和印度国家抽样调查数据验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: Darku等人(2020)仅考虑了分层和整群抽样，且缺乏理论保证。本研究旨在解决这些局限，在包含分层、整群和子分层的复杂抽样设计下，建立基尼系数估计量的最优性理论框架。

Method: 提出了两种序贯抽样方法：纯序贯方法和两阶段方法。在适当的正则条件下，建立了估计量的均匀连续性概率性质，为序贯抽样框架下的随机中心极限定理发展做出贡献。

Result: 证明了所提方法满足渐近一阶效率和渐近一致性。模拟结果显示，在不同设置下都能达到期望的最优性。通过印度国家抽样调查数据的实证应用进一步展示了方法的实用性。

Conclusion: 本研究扩展了复杂抽样设计下基尼系数估计的理论基础，提出的序贯方法具有理论保证和实际应用价值，为社会经济不平等测量提供了更可靠的统计工具。

Abstract: We examine the optimality properties of the Gini index estimator under complex survey design involving stratification, clustering, and sub-stratification. While Darku et al. (Econometrics, 26, 2020) considered only stratification and clustering and did not provide theoretical guarantees, this study addresses these limitations by proposing two procedures - a purely sequential method and a two-stage method. Under suitable regularity conditions, we establish uniform continuity in probability for the proposed estimator, thereby contributing to the development of random central limit theorems under sequential sampling frameworks. Furthermore, we show that the resulting procedures satisfy both asymptotic first-order efficiency and asymptotic consistency. Simulation results demonstrate that the proposed procedures achieve the desired optimality properties across diverse settings. The practical utility of the methodology is further illustrated through an empirical application using data collected by the National Sample Survey agency of India

</details>


### [9] [Estimation, inference and model selection for jump regression models](https://arxiv.org/abs/2602.21663)
*Steffen Grønneberg,Gudmund Hermansen,Nils Lid Hjort*

Main category: stat.ME

TL;DR: 论文研究了具有跳跃点的回归模型，提出了最小二乘估计和贝叶斯估计方法，并构建了用于选择跳跃点数量的信息准则AJIC和BJIC。


<details>
  <summary>Details</summary>
Motivation: 研究具有未知跳跃点和水平的分段常数回归模型，这类模型在信号处理、计量经济学等领域有广泛应用，需要有效的参数估计和模型选择方法。

Method: 采用最小二乘估计和贝叶斯估计方法，分析大样本性质，发现跳跃点参数和水平参数分别具有n-速率和√n-速率精度。基于AIC和BIC的思路，构建了适用于跳跃点模型的AJIC和BJIC信息准则。

Result: 最小二乘估计中，跳跃点参数估计精度为n-速率，水平参数估计精度为√n-速率。贝叶斯估计方法表现更优。构建的AJIC和BJIC信息准则能够有效选择正确的跳跃点数量。

Conclusion: 论文为具有跳跃点的回归模型提供了完整的统计推断框架，包括参数估计、大样本理论和模型选择准则，其中贝叶斯方法和新构建的信息准则具有更好的性能。

Abstract: We consider regression models with data of the type $y_i=m(x_i)+\varepsilon_i$, where the $m(x)$ curve is taken locally constant, with unknown levels and jump points. We investigate the large-sample properties of the minimum least squares estimators, finding in particular that jump point parameters and level parameters are estimated with respectively $n$-rate precision and $\sqrt{n}$-rate precision, where $n$ is sample size. Bayes solutions are investigated as well and found to be superior. We then construct jump information criteria, respectively AJIC and BJIC, for selecting the right number of jump points from data. This is done by following the line of arguments that lead to the Akaike and Bayesian information criteria AIC and BIC, but which here lead to different formulae due to the different type of large-sample approximations involved.

</details>


### [10] [Multi-Parameter Estimation of Prevalence (MPEP): A Bayesian modelling approach to estimate the prevalence of opioid dependence](https://arxiv.org/abs/2602.21713)
*Andreas Markoulidakis,Matthew Hickman,Nicky J Welton,Loukia Meligkotsidou,Hayley E Jones*

Main category: stat.ME

TL;DR: MPEP是一种贝叶斯建模方法，利用常规收集的行政数据来估计隐藏或边缘化人群的规模，通过多个事件类型验证一致性，并在苏格兰阿片类药物依赖案例研究中得到应用。


<details>
  <summary>Details</summary>
Motivation: 准确估计隐藏或边缘化人群（如阿片类或可卡因依赖者）的数量对于政策制定和减少伤害服务至关重要。传统方法如捕获-再捕获通常假设条件难以满足，在特定应用中不可行。

Method: 提出多参数患病率估计（MPEP）贝叶斯建模方法：从目标人群的大规模基线队列开始，链接多个事件类型数据，通过贝叶斯框架估计总人口规模。最新版本使用STAN实现以提高计算效率，并包含多个扩展模型来检验敏感性。

Result: MPEP方法能够利用常规行政数据估计隐藏人群规模，通过多个事件类型验证结果一致性，并在不一致时纳入额外证据。案例研究成功应用于估计苏格兰2014-2022年阿片类药物依赖患病率。

Conclusion: MPEP为估计隐藏人群规模提供了灵活、稳健的贝叶斯框架，能够处理传统方法难以满足的假设问题，并通过多源数据验证提高了估计的可靠性，在公共卫生政策制定中具有重要应用价值。

Abstract: Estimating the number of the number of people from hidden and/or marginalised populations - such as people dependent on opioids or cocaine - is important to guide policy decisions and provision of harm reduction services. Methods such as capture-recapture are widely used, but rely on assumptions that are often violated and not feasible in specific applications. We describe a Bayesian modelling approach called Multi-Parameter Estimation of Prevalence (MPEP). The MPEP approach leverages routinely collected administrative data, starting from a large baseline cohort of individuals from the population of interest and linked events, to estimate the full size of the target population. When multiple event types are included, the approach enables checking of the consistency of evidence about prevalence from different event types. Additional evidence can be incorporated where inconsistencies are identified. In this article, we summarize the general framework of MPEP, with focus on the most recent version, with improved computational efficiency (implemented in STAN). We also explore several extensions to the model that help us understand the sensitivity of the results to modelling assumptions or identify potential sources of bias. We demonstrate the MPEP approach through a case study estimating the prevalence of opioid dependence in Scotland each year from 2014 to 2022.

</details>


### [11] [Estimation of the complexity of a network under a Gaussian graphical model](https://arxiv.org/abs/2602.21969)
*Nabaneet Das,Thorsten Dickhaus*

Main category: stat.ME

TL;DR: 提出一种结合FDR控制下p值和Storey估计器的方法来估计高斯图模型中边的比例，建立了弱依赖条件下的理论保证，并分析了Schweder-Spjøtvoll估计器的渐近偏差。


<details>
  <summary>Details</summary>
Motivation: 高斯图模型（GGM）中边的比例表征了其条件依赖结构的复杂性。由于边的存在对应于精度矩阵的非零项，估计这个比例可以表述为一个大规模多重检验问题。需要开发在弱依赖条件下有效的估计方法。

Method: 提出一个结合FDR控制下同时边检验的p值和Storey真实零假设比例估计器的估计器。建立了精度矩阵的弱依赖条件，保证p值的经验累积分布函数收敛到其总体对应物。

Result: 在弱依赖条件下，p值的经验累积分布函数收敛到总体分布。分析了Schweder-Spjøtvoll估计器的渐近偏差，显示它是向上偏倚的，因此会轻微低估真实的边比例。模拟研究证实了图复杂性的准确恢复。

Conclusion: 该方法能够准确估计高斯图模型中边的比例，即使在弱依赖的高维情况下也有效，特别适用于遗传关联研究等场景。理论分析揭示了现有估计器的偏差特性。

Abstract: The proportion of edges in a Gaussian graphical model (GGM) characterizes the complexity of its conditional dependence structure. Since edge presence corresponds to a nonzero entry of the precision matrix, estimation of this proportion can be formulated as a large-scale multiple testing problem. We propose an estimator that combines p-values from simultaneous edge-wise tests, conducted under false discovery rate control, with Storey's estimator of the proportion of true null hypotheses. We establish weak dependence conditions on the precision matrix under which the empirical cumulative distribution function of the p-values converges to its population counterpart. These conditions cover high-dimensional regimes, including those arising in genetic association studies. Under such dependence, we characterize the asymptotic bias of the Schweder--Spjøtvoll estimator, showing that it is upward biased and thus slightly underestimates the true edge proportion. Simulation studies across a variety of models confirm accurate recovery of graph complexity.

</details>


### [12] [Design-based theory for causal inference from adaptive experiments](https://arxiv.org/abs/2602.21998)
*Xinran Li,Anqi Zhao*

Main category: stat.ME

TL;DR: 本文扩展了自适应实验的因果推断理论至有限总体框架，建立了基于设计的推断理论，并提出了自适应协变量调整方法。


<details>
  <summary>Details</summary>
Motivation: 现有自适应实验的因果推断理论主要基于超总体框架，假设独立同分布单位，不适用于随时间演变的单位分布。需要扩展到有限总体框架以处理不可交换单位。

Method: 1. 扩展至有限总体框架，建立基于设计的推断理论，使用IPW和AIPW估计器；2. 提出自适应协变量调整方法，利用鞅结构实现有效推断。

Result: 理论框架能处理不可交换单位、非收敛和消失的处理概率、非收敛结果估计器，并引入协方差估计器缓解方差估计的保守性。方法适用于多臂老虎机、协变量自适应随机化等设计。

Conclusion: 本文建立了有限总体框架下的自适应实验因果推断理论，提出了自适应协变量调整方法，为使用机器学习模型进行因果推断提供了理论支持，推进了基于设计的因果推断理论。

Abstract: Adaptive designs dynamically update treatment probabilities using information accumulated during the experiment. Existing theory for causal inference from adaptive experiments primarily assumes the superpopulation framework with independent and identically distributed units, and may not apply when the distribution of units evolves over time. This paper makes two contributions. First, we extend the literature to the finite-population framework, which allows for possibly nonexchangeable units, and establish the design-based theory for causal inference under general adaptive designs using inverse-propensity-weighted (IPW) and augmented IPW (AIPW) estimators. Our theory accommodates nonexchangeable units, both nonconverging and vanishing treatment probabilities, and nonconverging outcome estimators, thereby justifying inference using AIPW estimators with black-box outcome models that integrate advances from machine learning methods. To alleviate the conservativeness inherent in variance estimation under finite-population inference, we also introduce a covariance estimator for the AIPW estimator that becomes sharp when the residuals from the adaptive regression of potential outcomes on covariates are additive across units. Our framework encompasses widely used adaptive designs, such as multi-armed bandits, covariate-adaptive randomization, and sequential rerandomization, advancing the design-based theory for causal inference in these specific settings. Second, as a methodological contribution, we propose an adaptive covariate adjustment approach for analyzing even nonadaptive designs. The martingale structure induced by adaptive adjustment enables valid inference with black-box outcome estimators that would otherwise require strong assumptions under standard nonadaptive analysis.

</details>


### [13] [Budgeted Active Experimentation for Treatment Effect Estimation from Observational and Randomized Data](https://arxiv.org/abs/2602.22021)
*Jiacan Gao,Xinyan Su,Mingyuan Ma,Yiyan Huang,Xiao Xu,Xinrui Wan,Tianqi Gu,Enyun Yu,Jiecheng Guo,Zhiheng Zhang*

Main category: stat.ME

TL;DR: 提出预算主动实验框架，利用观察数据先验，通过主动采样选择信息量最大的单元进行随机实验，以在有限RCT预算下提升异质处理效应估计效果


<details>
  <summary>Details</summary>
Motivation: 工业应用中面临随机对照试验预算有限与大量但有偏的观察数据之间的根本矛盾。观察数据虽规模大但存在严重的策略诱导不平衡和重叠违反问题，导致独立估计不可靠

Method: 预算主动实验框架，迭代增强因果效应估计的模型训练。利用观察先验，开发针对提升估计不确定性、重叠缺陷和领域差异的获取函数，选择信息量最大的单元进行随机实验

Result: 建立了有限样本偏差界限、通过鞅中心极限定理的渐近正态性、极小极大下界，证明了信息论最优性。工业数据集实验显示在成本受限设置中显著优于标准随机基线

Conclusion: 提出的预算主动实验框架能有效解决RCT预算有限与观察数据有偏的矛盾，通过主动采样选择信息量最大的实验单元，在成本约束下实现更优的异质处理效应估计

Abstract: Estimating heterogeneous treatment effects is central to data-driven decision-making, yet industrial applications often face a fundamental tension between limited randomized controlled trial (RCT) budgets and abundant but biased observational data collected under historical targeting policies. Although observational logs offer the advantage of scale, they inherently suffer from severe policyinduced imbalance and overlap violations, rendering standalone estimation unreliable. We propose a budgeted active experimentation framework that iteratively enhances model training for causal effect estimation via active sampling. By leveraging observational priors, we develop an acquisition function targeting uplift estimation uncertainty, overlap deficits, and domain discrepancy to select the most informative units for randomized experiments. We establish finite-sample deviation bounds, asymptotic normality via martingale Central Limit Theorems (CLTs), and minimax lower bounds to prove information-theoretic optimality. Extensive experiments on industrial datasets demonstrate that our approach significantly outperforms standard randomized baselines in cost-constrained settings.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [14] [Adaptive Importance Tempering: A flexible approach to improve computational efficiency of Metropolis Coupled Markov Chain Monte Carlo algorithms on binary spaces](https://arxiv.org/abs/2602.21356)
*Alexander Valencia-Sanchez,Jeffrey S. Rosenthal,Yasuhiro Watanabe,Hirotaka Tamura,Ali Sheikholeslami*

Main category: stat.CO

TL;DR: 提出基于IIT的自适应有界平衡函数算法，解决高维空间中并行回火结合拒绝自由MCMC的效率问题，在具有多模态的高维二元空间中比现有方法更高效地识别高概率状态。


<details>
  <summary>Details</summary>
Motivation: 在Li等人(2023)提出的IIT算法基础上，针对高维空间中并行回火结合拒绝自由MCMC算法效率低下的问题，提出自适应算法来克服这些计算效率问题。

Method: 提出自适应有界平衡函数算法，包含两个等效版本(A-IIT和SS-IIT)，两者具有相同的极限分布，都适用于并行回火框架。算法基于IIT算法改进，使用自适应边界平衡函数。

Result: 通过模拟实验将自适应IIT与IIT、拒绝自由Metropolis-Hastings(RF-MH)以及使用多重列表的RF-MH进行比较，结果表明在具有多模态的高维二元空间中，自适应IIT比这些竞争算法更高效地识别高概率状态。

Conclusion: 提出的自适应IIT算法解决了高维空间中并行回火结合拒绝自由MCMC的效率问题，在复杂高维二元空间中表现出优越的性能，为多模态分布采样提供了更有效的解决方案。

Abstract: Based on the algorithm Informed Importance Tempering (IIT) proposed by Li et al. (2023) we propose an algorithm that uses an adaptive bounded balancing function. We argue why implementing parallel tempering where each replica uses a rejection free MCMC algorithm can be inefficient in high dimensional spaces and show how the proposed adaptive algorithm can overcome these computational inefficiencies. We present two equivalent versions of the adaptive algorithm (A-IIT and SS-IIT) and establish that both have the same limiting distribution, making either suitable for use within a parallel tempering framework. To evaluate performance, we benchmark the adaptive algorithm against several MCMC methods: IIT, Rejection free Metropolis-Hastings (RF-MH) and RF-MH using a multiplicity list. Simulation results demonstrate that Adaptive IIT identifies high-probability states more efficiently than these competing algorithms in high-dimensional binary spaces with multiple modes.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [15] [Evaluation of Minimal Residual Disease as a Surrogate for Progression-Free Survival in Hematology Oncology Trials: A Meta-Analytic Review](https://arxiv.org/abs/2602.21370)
*Jane She,Xiaofei Chen,Malini Iyengar,Judy Li*

Main category: stat.AP

TL;DR: 评估微小残留病（MRD）作为血液肿瘤药物加速审批替代终点的潜力，重点关注滤泡性淋巴瘤


<details>
  <summary>Details</summary>
Motivation: FDA加速审批途径允许基于替代终点批准严重疾病疗法，MRD作为血液肿瘤治疗后残留癌细胞的敏感测量指标，具有作为替代终点的潜力，需要评估其在加速药物审批过程中的作用

Method: 使用已发表研究数据，通过个体水平和试验水平相关性分析，评估MRD在多种血液肿瘤适应症中的替代终点潜力，特别关注滤泡性淋巴瘤

Result: 通过分析已发表研究的相关性数据，阐明MRD在血液肿瘤临床试验中加速药物审批过程的潜在作用

Conclusion: MRD作为血液肿瘤治疗的敏感测量指标，具有作为加速审批替代终点的潜力，需要进一步评估其在药物审批流程中的作用

Abstract: Traditional health authority approval for oncology drugs is based on a clinical benefit endpoint, or a valid surrogate. In 1992 the FDA created the Accelerated Approval pathway to allow for earlier approval of therapies in serious conditions with an unmet medical need. This is accomplished typically by granting accelerated approval based on a surrogate endpoint that can be measured earlier than a traditional approval endpoint. Minimal residual disease (MRD) is a sensitive measure of residual cancer cells in hematology oncology after treatment, and is increasingly considered as a secondary or exploratory endpoint due to its prognostic potential for traditional clinical trial endpoints such as progression-free survival (PFS) and overall survival (OS). This work aims to evaluate MRD's surrogacy potential across several hematologic cancer indications while keeping the focus on follicular lymphoma (FL), using data from published studies. We examine individual-level and trial-level correlations extracted from previously published studies to elucidate the potential role of MRD in accelerating the drug approval process in hematology oncology trials.

</details>


### [16] [Comparative Evaluation of Machine Learning Models for Predicting Donor Kidney Discard](https://arxiv.org/abs/2602.21876)
*Peer Schliephacke,Hannah Schult,Leon Mizera,Judith Würfel,Gunter Grieser,Axel Rahmel,Carl-Ludwig Fischer-Fröhlich,Antje Jahn-Eimermacher*

Main category: stat.AP

TL;DR: 该研究系统比较了五种机器学习模型预测供体肾脏丢弃率，发现统一的数据预处理和特征工程比算法选择更重要，集成模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 提高终末期肾衰竭患者的肾脏移植率，减少供体肾脏丢弃。目前机器学习模型在该领域的应用存在数据集异质性、特征工程和评估策略不一致的问题，导致结果难以比较。

Method: 在德国4,080例脑死亡供体数据上，训练了逻辑回归、决策树、随机森林、梯度提升、深度学习五种常用ML模型及集成模型。采用统一基准框架，包括标准化特征工程与选择、贝叶斯超参数优化。

Result: 集成模型获得最高判别性能（MCC=0.76, AUC=0.87, F1=0.90）。逻辑回归、随机森林和深度学习表现相当且优于决策树。Platt缩放改善了树基和神经网络模型的校准。SHAP分析一致显示供体年龄和肾脏标志物是主要预测因子。

Conclusion: 一致的数据预处理、特征选择和评估策略对预测成功的影响比ML算法选择更重要。研究为肾脏丢弃预测提供了可复现的基准比较框架。

Abstract: A kidney transplant can improve the life expectancy and quality of life of patients with end-stage renal failure. Even more patients could be helped with a transplant if the rate of kidneys that are discarded and not transplanted could be reduced. Machine learning (ML) can support decision-making in this context by early identification of donor organs at high risk of discard, for instance to enable timely interventions to improve organ utilization such as rescue allocation. Although various ML models have been applied, their results are difficult to compare due to heterogenous datasets and differences in feature engineering and evaluation strategies. This study aims to provide a systematic and reproducible comparison of ML models for donor kidney discard prediction. We trained five commonly used ML models: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, and Deep Learning along with an ensemble model on data from 4,080 deceased donors (death determined by neurologic criteria) in Germany. A unified benchmarking framework was implemented, including standardized feature engineering and selection, and Bayesian hyperparameter optimization. Model performance was assessed for discrimination (MCC, AUC, F1), calibration (Brier score), and explainability (SHAP). The ensemble achieved the highest discrimination performance (MCC=0.76, AUC=0.87, F1=0.90), while individual models such as Logistic Regression, Random Forest, and Deep Learning performed comparably and better than Decision Trees. Platt scaling improved calibration for tree-and neural network-based models. SHAP consistently identified donor age and renal markers as dominant predictors across models, reflecting clinical plausibility. This study demonstrates that consistent data preprocessing, feature selection, and evaluation can be more decisive for predictive success than the choice of the ML algorithm.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [17] [p-Hacking Inflates Type I Error Rates in the Error Statistical Approach but not in the Formal Inference Approach](https://arxiv.org/abs/2602.21792)
*Mark Rubin*

Main category: stat.OT

TL;DR: 论文探讨p-hacking在两种显著性检验哲学中的不同影响：在错误统计方法中会膨胀I类错误率，但在形式推断方法中不会。


<details>
  <summary>Details</summary>
Motivation: 研究动机是澄清p-hacking在不同显著性检验哲学框架下的理论影响。作者发现现有文献对p-hacking的理解存在混淆，特别是在不同统计哲学背景下，需要区分错误统计方法和形式推断方法对p-hacking的不同解读。

Method: 采用哲学分析方法，比较两种显著性检验哲学：错误统计方法（关注实际错误率）和形式推断方法（关注名义错误率）。通过理论分析探讨p-hacking在这两种框架下的不同含义和影响。

Result: 分析表明：在错误统计方法中，p-hacking确实会膨胀I类错误率，因为实际族错误率（如两个检验时为0.098）高于名义错误率（0.05）。但在形式推断方法中，p-hacking不会膨胀I类错误率，因为研究者不报告相应的交集零假设推断，且实际族错误率与报告的单假设推断无关。

Conclusion: 结论是p-hacking的影响取决于所采用的显著性检验哲学。这一区分对理解、证明和减少p-hacking具有重要意义，需要在不同统计哲学框架下重新审视p-hacking的概念和应对策略。

Abstract: p-hacking occurs when researchers conduct multiple significance tests (e.g., p1;H0,1 and p2;H0,2) and then selectively report tests that yield desirable (usually significant) results (e.g., p2 < 0.05;H0,2) without correcting for multiple testing (e.g., 0.05/2 = 0.025). In the present article, I consider p-hacking in the context of two philosophies of significance testing - the error statistical approach and the formal inference approach. I argue that although p-hacking inflates Type I error rates in the error statistical approach, it does not inflate them in the formal inference approach. Specifically, in the error statistical approach, the "actual" familywise error rate (e.g., 1 - [1 - 0.05]2 = 0.098 for two tests) is relevant because it covers both the selectively reported and unreported tests in the "actual" test procedure (i.e., p1;H0,1 and p2;H0,2). In this approach, Type I error rate inflation occurs because the "actual" error rate (0.098) is higher than the nominal error rate (0.05). In contrast, in the formal inference approach, the "actual" familywise error rate is irrelevant because (a) the researcher does not report a statistical inference about the corresponding intersection null hypothesis (i.e., H0,1 intersect H0,2), and (b) the "actual" familywise error rate does not license inferences about the reported individual hypotheses (i.e., H0,2). Instead, in the formal inference approach, only the nominal error rate is relevant, and a comparison with the "actual" error rate is inappropriate. Implications for conceptualizing, demonstrating, and reducing p-hacking are discussed.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [18] [Counterdiabatic Hamiltonian Monte Carlo](https://arxiv.org/abs/2602.21272)
*Reuben Cohn-Gordon,Uroš Seljak,Dries Sels*

Main category: stat.ML

TL;DR: 提出Counterdiabatic Hamiltonian Monte Carlo (CHMC)，通过添加反绝热项改进HMC在复杂多模态分布中的采样效率


<details>
  <summary>Details</summary>
Motivation: 传统Hamiltonian Monte Carlo (HMC)在处理多模态分布时收敛缓慢，现有基于时间变化哈密顿量的方法虽然能解决此问题但效率低下

Method: 受量子计算中反绝热项启发，提出CHMC方法，在哈密顿量中添加学习的反绝热项，可视为更高效的Sequential Monte Carlo采样器

Result: 建立了CHMC与现有加速梯度采样的学习漂移项方法的联系，并在简单基准问题上进行了验证

Conclusion: CHMC通过反绝热项显著提高了HMC在多模态分布采样中的效率，是SMC采样器的一种高效实现

Abstract: Hamiltonian Monte Carlo (HMC) is a state of the art method for sampling from distributions with differentiable densities, but can converge slowly when applied to challenging multimodal problems. Running HMC with a time varying Hamiltonian, in order to interpolate from an initial tractable distribution to the target of interest, can address this problem. In conjunction with a weighting scheme to eliminate bias, this can be viewed as a special case of Sequential Monte Carlo (SMC) sampling \cite{doucet2001introduction}. However, this approach can be inefficient, since it requires slow change between the initial and final distribution. Inspired by \cite{sels2017minimizing}, where a learned \emph{counterdiabatic} term added to the Hamiltonian allows for efficient quantum state preparation, we propose \emph{Counterdiabatic Hamiltonian Monte Carlo} (CHMC), which can be viewed as an SMC sampler with a more efficient kernel. We establish its relationship to recent proposals for accelerating gradient-based sampling with learned drift terms, and demonstrate on simple benchmark problems.

</details>


### [19] [Efficient Inference after Directionally Stable Adaptive Experiments](https://arxiv.org/abs/2602.21478)
*Zikai Shen,Houssam Zenati,Nathan Kallus,Arthur Gretton,Koulik Khamaru,Aurélien Bibaut*

Main category: stat.ML

TL;DR: 提出方向稳定性条件，证明在自适应数据收集下（如bandit算法），满足该条件的估计量仍能保持渐近正态性和半参数效率


<details>
  <summary>Details</summary>
Motivation: 研究在自适应数据收集（如bandit算法）后，对路径可微标量目标的推断问题。现有方法通常要求目标无关的稳定性条件，这限制了应用范围。

Method: 引入新的目标特定条件——方向稳定性，该条件比之前的目标无关稳定性条件更弱。在方向稳定性下，证明在i.i.d.数据下有效的估计量在自适应收集的轨迹中仍能保持渐近正态性和半参数效率。

Result: 方向稳定性保证了典型梯度的可预测二次变差的稳定性，从而实现高维渐近正态性。通过自适应数据设置的卷积定理刻画效率，给出一步估计量达到效率界的条件。验证了LinUCB满足方向稳定性，首次为LinUCB采样下的正则标量目标提供了半参数效率保证。

Conclusion: 方向稳定性为自适应数据收集下的统计推断提供了更弱的条件，使得在bandit算法等自适应设置中，传统有效估计量仍能保持渐近正态性和效率，扩展了自适应推断的理论基础。

Abstract: We study inference on scalar-valued pathwise differentiable targets after adaptive data collection, such as a bandit algorithm. We introduce a novel target-specific condition, directional stability, which is strictly weaker than previously imposed target-agnostic stability conditions. Under directional stability, we show that estimators that would have been efficient under i.i.d. data remain asymptotically normal and semiparametrically efficient when computed from adaptively collected trajectories. The canonical gradient has a martingale form, and directional stability guarantees stabilization of its predictable quadratic variation, enabling high-dimensional asymptotic normality. We characterize efficiency using a convolution theorem for the adaptive-data setting, and give a condition under which the one-step estimator attains the efficiency bound. We verify directional stability for LinUCB, yielding the first semiparametric efficiency guarantee for a regular scalar target under LinUCB sampling.

</details>


### [20] [Conditional neural control variates for variance reduction in Bayesian inverse problems](https://arxiv.org/abs/2602.21357)
*Ali Siahkoohi,Hyunwoo Oh*

Main category: stat.ML

TL;DR: 提出条件神经控制变量方法，通过训练神经网络控制变量来降低贝叶斯逆问题中蒙特卡洛估计的方差，支持高维问题并利用Stein恒等式设计架构。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯逆问题中的蒙特卡洛估计需要大量样本才能获得准确结果，对于偏微分方程约束的问题计算成本过高，需要降低方差的方法。

Method: 提出条件神经控制变量方法，使用基于Stein恒等式的层次耦合层架构，通过联合模型-数据样本训练摊销控制变量，支持后验评分函数来自物理模型、神经算子或生成模型。

Result: 在风格化和Darcy流偏微分方程约束的逆问题上验证，即使使用学习替代评分函数也能实现显著的方差降低。

Conclusion: 条件神经控制变量是一种模块化方法，能够有效降低蒙特卡洛估计方差，训练后可在不同观测间泛化，适用于高维逆问题。

Abstract: Bayesian inference for inverse problems involves computing expectations under posterior distributions -- e.g., posterior means, variances, or predictive quantities -- typically via Monte Carlo (MC) estimation. When the quantity of interest varies significantly under the posterior, accurate estimates demand many samples -- a cost often prohibitive for partial differential equation-constrained problems. To address this challenge, we introduce conditional neural control variates, a modular method that learns amortized control variates from joint model-data samples to reduce the variance of MC estimators. To scale to high-dimensional problems, we leverage Stein's identity to design an architecture based on an ensemble of hierarchical coupling layers with tractable Jacobian trace computation. Training requires: (i) samples from the joint distribution of unknown parameters and observed data; and (ii) the posterior score function, which can be computed from physics-based likelihood evaluations, neural operator surrogates, or learned generative models such as conditional normalizing flows. Once trained, the control variates generalize across observations without retraining. We validate our approach on stylized and partial differential equation-constrained Darcy flow inverse problems, demonstrating substantial variance reduction, even when the analytical score is replaced by a learned surrogate.

</details>


### [21] [A Researcher's Guide to Empirical Risk Minimization](https://arxiv.org/abs/2602.21501)
*Lars van der Laan*

Main category: stat.ML

TL;DR: 本文为经验风险最小化(ERM)开发了高概率遗憾界，提出了模块化分析框架，包括三步骤方法：基本不等式、局部一致集中界和不动点论证。同时处理了带干扰分量的ERM问题，展示了如何将遗憾分解为统计误差和干扰估计误差。


<details>
  <summary>Details</summary>
Motivation: 为经验风险最小化(ERM)提供系统化的高概率遗憾界分析框架，统一处理各种损失函数和函数类，并扩展到包含干扰分量的复杂场景（如因果推断、缺失数据和领域适应）。

Method: 采用模块化方法：1）基于基本不等式、局部一致集中界和不动点论证的三步骤框架；2）通过局部Rademacher复杂度定义临界半径；3）使用局部极大不等式和度量熵积分上界临界半径；4）对于带干扰分量的ERM，采用正交学习框架，通过样本分割或交叉拟合分离统计误差和干扰估计误差。

Result: 建立了统一的ERM遗憾界理论框架，恢复了VC子图、Sobolev/Hölder和有界变差函数类的经典收敛率。对于带干扰分量的ERM，获得了遗憾转移界，将总遗憾分解为统计误差和干扰估计误差两部分。

Conclusion: 本文提供了一个系统化的ERM分析工具箱，通过模块化框架统一了多种场景下的遗憾界推导，特别强调了正交学习框架在处理带干扰分量问题中的有效性，为因果推断等应用提供了理论基础。

Abstract: This guide develops high-probability regret bounds for empirical risk minimization (ERM). The presentation is modular: we state broadly applicable guarantees under high-level conditions and give tools for verifying them for specific losses and function classes. We emphasize that many ERM rate derivations can be organized around a three-step recipe -- a basic inequality, a uniform local concentration bound, and a fixed-point argument -- which yields regret bounds in terms of a critical radius, defined via localized Rademacher complexity, under a mild Bernstein-type variance--risk condition. To make these bounds concrete, we upper bound the critical radius using local maximal inequalities and metric-entropy integrals, recovering familiar rates for VC-subgraph, Sobolev/Hölder, and bounded-variation classes.
  We also review ERM with nuisance components -- including weighted ERM and Neyman-orthogonal losses -- as they arise in causal inference, missing data, and domain adaptation. Following the orthogonal learning framework, we highlight that these problems often admit regret-transfer bounds linking regret under an estimated loss to population regret under the target loss. These bounds typically decompose regret into (i) statistical error under the estimated (optimized) loss and (ii) approximation error due to nuisance estimation. Under sample splitting or cross-fitting, the first term can be controlled using standard fixed-loss ERM regret bounds, while the second term depends only on nuisance-estimation accuracy. We also treat the in-sample regime, where nuisances and the ERM are fit on the same data, deriving regret bounds and giving sufficient conditions for fast rates.

</details>


### [22] [Fair Model-based Clustering](https://arxiv.org/abs/2602.21509)
*Jinwon Park,Kunwoong Kim,Jihu Lee,Yongdai Kim*

Main category: stat.ML

TL;DR: 提出基于有限混合模型的公平聚类算法FMC，参数数量与样本规模无关，支持小批量学习和非度量数据


<details>
  <summary>Details</summary>
Motivation: 现有公平聚类算法参数数量与样本规模成正比，难以扩展到大规模数据集，需要更高效的公平聚类方法

Method: 基于有限混合模型，通过概率建模实现公平聚类，参数数量固定，支持小批量学习，可处理非度量数据

Result: FMC算法参数数量与样本规模无关，可高效扩展到大规模数据集，支持近似公平聚类，理论分析和实证验证了其优越性

Conclusion: FMC是一种高效、可扩展的公平聚类算法，解决了现有方法参数过多的问题，适用于大规模和非度量数据场景

Abstract: The goal of fair clustering is to find clusters such that the proportion of sensitive attributes (e.g., gender, race, etc.) in each cluster is similar to that of the entire dataset. Various fair clustering algorithms have been proposed that modify standard K-means clustering to satisfy a given fairness constraint. A critical limitation of several existing fair clustering algorithms is that the number of parameters to be learned is proportional to the sample size because the cluster assignment of each datum should be optimized simultaneously with the cluster center, and thus scaling up the algorithms is difficult. In this paper, we propose a new fair clustering algorithm based on a finite mixture model, called Fair Model-based Clustering (FMC). A main advantage of FMC is that the number of learnable parameters is independent of the sample size and thus can be scaled up easily. In particular, mini-batch learning is possible to obtain clusters that are approximately fair. Moreover, FMC can be applied to non-metric data (e.g., categorical data) as long as the likelihood is well-defined. Theoretical and empirical justifications for the superiority of the proposed algorithm are provided.

</details>


### [23] [Efficient Uncoupled Learning Dynamics with $\tilde{O}\!\left(T^{-1/4}\right)$ Last-Iterate Convergence in Bilinear Saddle-Point Problems over Convex Sets under Bandit Feedback](https://arxiv.org/abs/2602.21436)
*Arnab Maiti,Claire Jie Zhang,Kevin Jamieson,Jamie Heather Morgenstern,Ioannis Panageas,Lillian J. Ratliff*

Main category: stat.ML

TL;DR: 提出一种在双线性鞍点问题中保证最终迭代收敛的算法，适用于紧致凸集和强盗反馈设置，收敛率为$\tilde{O}(T^{-1/4})$，仅需线性优化预言机。


<details>
  <summary>Details</summary>
Motivation: 研究双线性鞍点问题中学习算法的最终迭代收敛性，这种收敛概念能更好地捕捉学习动态的日常行为。关注具有挑战性的设置：玩家从紧致凸集中选择动作且仅接收强盗反馈。

Method: 结合实验设计和经典FTRL框架技术，为每个学习者的动作集几何特性精心设计正则化函数，构建无需耦合的学习算法，仅需高效的线性优化预言机。

Result: 设计出保证以高概率最终迭代收敛到纳什均衡的算法，收敛率为$\tilde{O}(T^{-1/4})$（忽略问题参数的多项式因子），算法计算高效。

Conclusion: 成功解决了紧致凸集和强盗反馈设置下的最终迭代收敛问题，通过实验设计与FTRL框架结合，为每个玩家量身定制正则化函数，实现了高效收敛。

Abstract: In this paper, we study last-iterate convergence of learning algorithms in bilinear saddle-point problems, a preferable notion of convergence that captures the day-to-day behavior of learning dynamics. We focus on the challenging setting where players select actions from compact convex sets and receive only bandit feedback. Our main contribution is the design of an uncoupled learning algorithm that guarantees last-iterate convergence to the Nash equilibrium with high probability. We establish a convergence rate of $\tilde{O}(T^{-1/4})$ up to polynomial factors in problem parameters. Crucially, our proposed algorithm is computationally efficient, requiring only an efficient linear optimization oracle over the players' compact action sets. The algorithm is obtained by combining techniques from experimental design and the classic Follow-The-Regularized-Leader (FTRL) framework, with a carefully chosen regularizer function tailored to the geometry of the action set of each learner.

</details>


### [24] [ConformalHDC: Uncertainty-Aware Hyperdimensional Computing with Application to Neural Decoding](https://arxiv.org/abs/2602.21446)
*Ziyi Liang,Hamed Poursiami,Zhishun Yang,Keiland Cooper,Akhilesh Jaiswal,Maryam Parsa,Norbert Fortin,Babak Shahbaba*

Main category: stat.ML

TL;DR: ConformalHDC：将保形预测的统计保证与超维计算的高效性相结合，为神经形态学习提供具有不确定性量化的统一框架。


<details>
  <summary>Details</summary>
Motivation: 超维计算虽然计算高效，但缺乏严格的不确定性量化，导致决策边界开放，容易受到异常值、对抗性扰动和分布外输入的影响。

Method: 提出ConformalHDC框架，包含两种互补变体：1) 集合值公式化提供有限样本、分布无关的覆盖保证，使用精心设计的符合度分数形成封闭决策边界；2) 点值公式化利用相同符合度分数在需要时产生单一预测，通过考虑类别交互可能提高传统HDC的准确性。

Result: 在多个真实世界数据集上验证了框架的广泛适用性，特别是在从海马神经元尖峰活动中解码非空间刺激信息的挑战性问题上，不仅准确解码神经活动数据中的刺激信息，还提供严格的不确定性估计，并在面对其他行为状态数据时正确弃权。

Conclusion: ConformalHDC为神经形态计算提供了一个可靠、具有不确定性感知的基础框架，结合了统计保证和计算效率的优势。

Abstract: Hyperdimensional Computing (HDC) offers a computationally efficient paradigm for neuromorphic learning. Yet, it lacks rigorous uncertainty quantification, leading to open decision boundaries and, consequently, vulnerability to outliers, adversarial perturbations, and out-of-distribution inputs. To address these limitations, we introduce ConformalHDC, a unified framework that combines the statistical guarantees of conformal prediction with the computational efficiency of HDC. For this framework, we propose two complementary variations. First, the set-valued formulation provides finite-sample, distribution-free coverage guarantees. Using carefully designed conformity scores, it forms enclosed decision boundaries that improve robustness to non-conforming inputs. Second, the point-valued formulation leverages the same conformity scores to produce a single prediction when desired, potentially improving accuracy over traditional HDC by accounting for class interactions. We demonstrate the broad applicability of the proposed framework through evaluations on multiple real-world datasets. In particular, we apply our method to the challenging problem of decoding non-spatial stimulus information from the spiking activity of hippocampal neurons recorded as subjects performed a sequence memory task. Our results show that ConformalHDC not only accurately decodes the stimulus information represented in the neural activity data, but also provides rigorous uncertainty estimates and correctly abstains when presented with data from other behavioral states. Overall, these capabilities position the framework as a reliable, uncertainty-aware foundation for neuromorphic computing.

</details>


### [25] [Scalable Kernel-Based Distances for Statistical Inference and Integration](https://arxiv.org/abs/2602.21846)
*Masha Naslidnyk*

Main category: stat.ML

TL;DR: 该论文系统研究了基于核的距离度量，重点改进MMD估计并引入新的核分位数差异方法


<details>
  <summary>Details</summary>
Motivation: 概率分布的表征、比较和距离度量是计算统计和机器学习的关键任务。核方法提供了灵活丰富的分布表示，特别是最大均值差异（MMD）因其计算可处理性受到关注，但需要改进估计方法和解决其局限性

Method: 论文分为两部分：第一部分聚焦MMD改进估计，包括模拟推理中的MMD估计器、条件期望的MMD估计器以及积分任务中的校准问题；第二部分引入新的核分位数差异方法，解决MMD的缺陷

Result: 提出了理论上可靠的改进MMD估计器，开发了基于MMD的条件期望估计器，研究了MMD在积分任务中的校准问题，并引入核分位数差异作为MMD的竞争性替代方案

Conclusion: 核基距离度量在分布比较中具有重要价值，改进的MMD估计和新的核分位数差异方法为实践提供了更好的工具，未来工作将继续探索这些方法的扩展和应用

Abstract: Representing, comparing, and measuring the distance between probability distributions is a key task in computational statistics and machine learning. The choice of representation and the associated distance determine properties of the methods in which they are used: for example, certain distances can allow one to encode robustness or smoothness of the problem. Kernel methods offer flexible and rich Hilbert space representations of distributions that allow the modeller to enforce properties through the choice of kernel, and estimate associated distances at efficient nonparametric rates. In particular, the maximum mean discrepancy (MMD), a kernel-based distance constructed by comparing Hilbert space mean functions, has received significant attention due to its computational tractability and is favoured by practitioners.
  In this thesis, we conduct a thorough study of kernel-based distances with a focus on efficient computation, with core contributions in Chapters 3 to 6. Part I of the thesis is focused on the MMD, specifically on improved MMD estimation. In Chapter 3 we propose a theoretically sound, improved estimator for MMD in simulation-based inference. Then, in Chapter 4, we propose an MMD-based estimator for conditional expectations, a ubiquitous task in statistical computation. Closing Part I, in Chapter 5 we study the problem of calibration when MMD is applied to the task of integration.
  In Part II, motivated by the recent developments in kernel embeddings beyond the mean, we introduce a family of novel kernel-based discrepancies: kernel quantile discrepancies. These address some of the pitfalls of MMD, and are shown through both theoretical results and an empirical study to offer a competitive alternative to MMD and its fast approximations. We conclude with a discussion on broader lessons and future work emerging from the thesis.

</details>


### [26] [Global Sequential Testing for Multi-Stream Auditing](https://arxiv.org/abs/2602.21479)
*Beepul Bharti,Ambar Pal,Jeremias Sulam*

Main category: stat.ML

TL;DR: 本文提出新的序列测试方法，通过合并不同权衡的测试鞅，在稀疏和密集备择假设下改进期望停止时间，相比标准Bonferroni方法有更好性能。


<details>
  <summary>Details</summary>
Motivation: 在风险敏感领域，需要持续审计机器学习系统性能并快速检测异常行为。这可以建模为具有k个数据流的序列假设检验问题，全局零假设断言系统在所有流上正常工作。标准全局测试使用Bonferroni校正，在大k和小显著性水平α时期望停止时间为O(ln(k/α))，需要改进。

Method: 通过合并具有不同权衡的测试鞅来构建新的序列测试。提出平衡测试，在稀疏备择假设下匹配Bonferroni性能，在密集备择假设下实现O((1/k)ln(1/α))的期望停止时间。

Result: 新测试在稀疏设置下达到与Bonferroni相同的期望停止时间界限，但在密集备择假设下显著改进为O((1/k)ln(1/α))。在合成和真实数据上的实证结果证明了所提测试的有效性。

Conclusion: 提出的新序列测试方法通过合并不同权衡的测试鞅，在稀疏和密集备择假设下改进了期望停止时间性能，为机器学习系统性能审计提供了更有效的检测工具。

Abstract: Across many risk-sensitive areas, it is critical to continuously audit the performance of machine learning systems and detect any unusual behavior quickly. This can be modeled as a sequential hypothesis testing problem with $k$ incoming streams of data and a global null hypothesis that asserts that the system is working as expected across all $k$ streams. The standard global test employs a Bonferroni correction and has an expected stopping time bound of $O\left(\ln\frac{k}α\right)$ when $k$ is large and the significance level of the test, $α$, is small. In this work, we construct new sequential tests by using ideas of merging test martingales with different trade-offs in expected stopping times under different, sparse or dense alternative hypotheses. We further derive a new, balanced test that achieves an improved expected stopping time bound that matches Bonferroni's in the sparse setting but that naturally results in $O\left(\frac{1}{k}\ln\frac{1}α\right)$ under a dense alternative. We empirically demonstrate the effectiveness of our proposed tests on synthetic and real-world data.

</details>


### [27] [Goodness-of-Fit Tests for Latent Class Models with Ordinal Categorical Data](https://arxiv.org/abs/2602.21572)
*Huan Qing*

Main category: stat.ML

TL;DR: 提出一种基于归一化残差矩阵最大奇异值的检验统计量，用于确定序数分类数据的潜在类别数，该统计量在正确模型下收敛于零，在欠拟合模型下超过正常数，从而构建一致性估计的序贯检验算法。


<details>
  <summary>Details</summary>
Motivation: 序数分类数据在心理学、教育学等社会科学中广泛存在，潜在类别模型能有效揭示未观测异质性，但确定潜在类别数量是一个关键挑战，现有方法存在局限性，需要更可靠的统计检验方法。

Method: 提出基于归一化残差矩阵最大奇异值的检验统计量，通过简单的样本量调整进行中心化处理。在零假设（候选类别数正确）下，统计量上界以概率收敛于零；在欠拟合备择假设下，统计量以概率趋近于1超过固定正常数。基于这种二分行为构建两种序贯检验算法。

Result: 理论分析表明检验统计量具有理想的渐近性质，在正确模型和欠拟合模型下表现出截然不同的行为。大量实验研究证实了理论发现，证明该方法在确定潜在类别数量方面具有准确性和可靠性。

Conclusion: 提出的检验统计量和序贯检验算法能够一致地估计潜在类别的真实数量，为序数分类数据的潜在类别模型选择提供了有效工具，具有理论和实践价值。

Abstract: Ordinal categorical data are widely collected in psychology, education, and other social sciences, appearing commonly in questionnaires, assessments, and surveys. Latent class models provide a flexible framework for uncovering unobserved heterogeneity by grouping individuals into homogeneous classes based on their response patterns. A fundamental challenge in applying these models is determining the number of latent classes, which is unknown and must be inferred from data. In this paper, we propose one test statistic for this problem. The test statistic centers the largest singular value of a normalized residual matrix by a simple sample-size adjustment. Under the null hypothesis that the candidate number of latent classes is correct, its upper bound converges to zero in probability. Under an under-fitted alternative, the statistic itself exceeds a fixed positive constant with probability approaching one. This sharp dichotomous behavior of the test statistic yields two sequential testing algorithms that consistently estimate the true number of latent classes. Extensive experimental studies confirm the theoretical findings and demonstrate their accuracy and reliability in determining the number of latent classes.

</details>


### [28] [Probing the Geometry of Diffusion Models with the String Method](https://arxiv.org/abs/2602.22122)
*Elio Moreau,Florentin Coeurdoux,Grégoire Ferre,Eric Vanden-Eijnden*

Main category: stat.ML

TL;DR: 本文提出基于弦方法的框架，用于计算扩散模型中样本间的连续路径，探索学习分布的几何结构，包括纯生成传输、梯度主导动力学和有限温度弦动力学三种机制。


<details>
  <summary>Details</summary>
Motivation: 理解学习分布的几何结构对于改进和解释扩散模型至关重要，但目前缺乏系统工具来探索其景观。标准的潜在空间插值方法无法尊重学习分布的结构，通常会穿越低密度区域。

Method: 基于弦方法的框架，在预训练模型上无需重新训练，通过演化曲线在学习的得分函数下计算样本间的连续路径。包括三种机制：纯生成传输、梯度主导动力学（恢复最小能量路径）和有限温度弦动力学（计算主曲线）。

Result: 在图像扩散模型中，最小能量路径包含高似然但不真实的"卡通"图像，而主曲线产生真实的变形序列尽管似然较低。在蛋白质结构预测中，该方法直接从静态结构训练的模型中计算亚稳态构象间的过渡路径，产生具有物理合理中间体的路径。

Conclusion: 弦方法成为探索扩散模型模态结构的原则性工具，能够识别模态、表征障碍，并在复杂学习分布中映射连通性。

Abstract: Understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. Standard latent-space interpolations fail to respect the structure of the learned distribution, often traversing low-density regions. We introduce a framework based on the string method that computes continuous paths between samples by evolving curves under the learned score function. Operating on pretrained models without retraining, our approach interpolates between three regimes: pure generative transport, which yields continuous sample paths; gradient-dominated dynamics, which recover minimum energy paths (MEPs); and finite-temperature string dynamics, which compute principal curves -- self-consistent paths that balance energy and entropy. We demonstrate that the choice of regime matters in practice. For image diffusion models, MEPs contain high-likelihood but unrealistic ''cartoon'' images, confirming prior observations that likelihood maxima appear unrealistic; principal curves instead yield realistic morphing sequences despite lower likelihood. For protein structure prediction, our method computes transition pathways between metastable conformers directly from models trained on static structures, yielding paths with physically plausible intermediates. Together, these results establish the string method as a principled tool for probing the modal structure of diffusion models -- identifying modes, characterizing barriers, and mapping connectivity in complex learned distributions.

</details>
