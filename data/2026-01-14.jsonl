{"id": "2601.06264", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06264", "abs": "https://arxiv.org/abs/2601.06264", "authors": ["Florian Brück", "Sebastian Engelke", "Stanislav Volgushev"], "title": "Graph structure learning for stable processes", "comment": null, "summary": "We introduce Ising-Hüsler-Reiss processes, a new class of multivariate Lévy processes that allows for sparse modeling of the path-wise conditional independence structure between marginal stable processes with different stability indices. The underlying conditional independence graph is encoded as zeroes in a suitable precision matrix. An Ising-type parametrization of the weights for each orthant of the Lévy measure allows for data-driven modeling of asymmetry of the jumps while retaining an arbitrary sparse graph. We develop consistent estimators for the graphical structure and asymmetry parameters, relying on a new uniform small-time approximation for Lévy processes. The methodology is illustrated in simulations and a real data application to modeling dependence of stock returns."}
{"id": "2601.06296", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.06296", "abs": "https://arxiv.org/abs/2601.06296", "authors": ["Man Jin", "Yixin Fang"], "title": "A Framework for Estimating Restricted Mean Survival Time Difference using Pseudo-observations", "comment": null, "summary": "A targeted learning (TL) framework is developed to estimate the difference in the restricted mean survival time (RMST) for a clinical trial with time-to-event outcomes. The approach starts by defining the target estimand as the RMST difference between investigational and control treatments. Next, an efficient estimation method is introduced: a targeted minimum loss estimator (TMLE) utilizing pseudo-observations. Moreover, a version of the copy reference (CR) approach is developed to perform a sensitivity analysis for right-censoring. The proposed TL framework is demonstrated using a real data application."}
{"id": "2601.06390", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.06390", "abs": "https://arxiv.org/abs/2601.06390", "authors": ["Qianqian Yao"], "title": "Empirical Likelihood Test for Common Invariant Subspace of Multilayer Networks based on Monte Carlo Approximation", "comment": null, "summary": "Multilayer (or multiple) networks are widely used to represent diverse patterns of relationships among objects in increasingly complex real-world systems. Identifying a common invariant subspace across network layers has become an active area of research, as such a subspace can filter out layer-specific noise, facilitate cross-network comparisons, reduce dimensionality, and extract shared structural features of scientific interest. One statistical approach to detecting a common subspace is hypothesis testing, which evaluates whether the observed networks share a common latent structure. In this paper, we propose an empirical likelihood (EL) based test for this purpose. The null hypothesis states that all network layers share the same invariant subspace, whereas under the alternative hypothesis at least two layers differ in their subspaces. We study the asymptotic behavior of the proposed test via Monte Carlo approximation and assess its finite-sample performance through extensive simulations. The simulation results demonstrate that the proposed method achieves satisfactory size and power, and its practical utility is further illustrated with a real-data application."}
{"id": "2601.06481", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06481", "abs": "https://arxiv.org/abs/2601.06481", "authors": ["Qunqiang Feng", "Yaru Tian", "Ting Yan"], "title": "Triple-dyad ratio estimation for the $p_1$ model", "comment": "19 pages,3 figures, 3 tables", "summary": "Although the $p_1$ model was proposed 40 years ago, little progress has been made to address asymptotic theories in this model, that is, neither consistency of the maximum likelihood estimator (MLE) nor other parameter estimation with statistical guarantees is understood. This problem has been acknowledged as a long-standing open problem. To address it, we propose a novel parametric estimation method based on the ratios of the sum of a sequence of triple-dyad indicators to another one, where a triple-dyad indicator means the product of three dyad indicators. Our proposed estimators, called \\emph{triple-dyad ratio estimator}, have explicit expressions and can be scaled to very large networks with millions of nodes. We establish the consistency and asymptotic normality of the triple-dyad ratio estimator when the number of nodes reaches infinity. Based on the asymptotic results, we develop a test statistic for evaluating whether is a reciprocity effect in directed networks. The estimators for the density and reciprocity parameters contain bias terms, where analytical bias correction formulas are proposed to make valid inference. Numerical studies demonstrate the findings of our theories and show that the estimator is comparable to the MLE in large networks."}
{"id": "2601.06462", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06462", "abs": "https://arxiv.org/abs/2601.06462", "authors": ["Tianming Bai", "Jiannan Yang"], "title": "Physics-informed Gaussian Process Regression in Solving Eigenvalue Problem of Linear Operators", "comment": "17 pages, 7 figures", "summary": "Applying Physics-Informed Gaussian Process Regression to the eigenvalue problem $(\\mathcal{L}-λ)u = 0$ poses a fundamental challenge, where the null source term results in a trivial predictive mean and a degenerate marginal likelihood. Drawing inspiration from system identification, we construct a transfer function-type indicator for the unknown eigenvalue/eigenfunction using the physics-informed Gaussian Process posterior. We demonstrate that the posterior covariance is only non-trivial when $λ$ corresponds to an eigenvalue of the partial differential operator $\\mathcal{L}$, reflecting the existence of a non-trivial eigenspace, and any sample from the posterior lies in the eigenspace of the linear operator. We demonstrate the effectiveness of the proposed approach through several numerical examples with both linear and non-linear eigenvalue problems."}
{"id": "2601.06514", "categories": ["stat.ML", "cs.LG", "math.NA", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.06514", "abs": "https://arxiv.org/abs/2601.06514", "authors": ["Jinyuan Chang", "Chenguang Duan", "Yuling Jiao", "Yi Xu", "Jerry Zhijian Yang"], "title": "Inference-Time Alignment for Diffusion Models via Doob's Matching", "comment": null, "summary": "Inference-time alignment for diffusion models aims to adapt a pre-trained diffusion model toward a target distribution without retraining the base score network, thereby preserving the generative capacity of the base model while enforcing desired properties at the inference time. A central mechanism for achieving such alignment is guidance, which modifies the sampling dynamics through an additional drift term. In this work, we introduce Doob's matching, a novel framework for guidance estimation grounded in Doob's $h$-transform. Our approach formulates guidance as the gradient of logarithm of an underlying Doob's $h$-function and employs gradient-penalized regression to simultaneously estimate both the $h$-function and its gradient, resulting in a consistent estimator of the guidance. Theoretically, we establish non-asymptotic convergence rates for the estimated guidance. Moreover, we analyze the resulting controllable diffusion processes and prove non-asymptotic convergence guarantees for the generated distributions in the 2-Wasserstein distance."}
{"id": "2601.07534", "categories": ["stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2601.07534", "abs": "https://arxiv.org/abs/2601.07534", "authors": ["Lampis Tzai", "Ioannis Ntzoufras", "Silvia Bozza"], "title": "Bayesian Handwriting Evidence Evaluation using MANOVA via Fourier-Based Extracted Features", "comment": null, "summary": "This paper proposes a novel statistical approach that aims at the identification of valid and useful patterns in handwriting examination via Bayesian modeling. Starting from a sample of characters selected among 13 French native writers, an accurate loop reconstruction can be achieved through Fourier analysis. The contour shape of handwritten characters can be described by the first four pairs of Fourier coefficients and by the surface size. Six Bayesian models are considered for such handwritten features. These models arise from two likelihood structures: (a) a multivariate Normal model, and (b) a MANOVA model that accounts for character-level variability. For each likelihood, three different prior formulations are examined, resulting in distinct Bayesian models: (i) a conjugate Normal-Inverse-Wishart prior, (ii) a hierarchical Normal-Inverse-Wishart prior, and (iii) a Normal-LogNormal-LKJ prior specification. The hierarchical prior formulations are of primary interest because they can incorporate the between-writers variability, a distinguishing element that sets writers apart. These approaches do not allow calculation of the marginal likelihood in a closed-form expression. Therefore, bridge sampling is used to estimate it. The Bayes factor is estimated to compare the performance of the proposed models and to evaluate their efficiency for discriminating purposes. Bayesian MANOVA with Normal-LogNormal-LKJ prior showed an overall better performance, in terms of discriminatory capacity and model fitting. Finally, a sensitivity analysis for the elicitation of the prior distribution parameters is performed."}
{"id": "2601.07299", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07299", "abs": "https://arxiv.org/abs/2601.07299", "authors": ["Zhengdao Li", "Penggao Yan", "Weisong Wen", "Li-Ta Hsu"], "title": "Cauchy-Gaussian Overbound for Heavy-tailed GNSS Measurement Errors", "comment": "Accepted in NAVIGATION: Journal of the Institute of Navigation", "summary": "Overbounds of heavy-tailed measurement errors are essential to meet stringent navigation requirements in integrity monitoring applications. This paper proposes to leverage the bounding sharpness of the Cauchy distribution in the core and the Gaussian distribution in the tails to tightly bound heavy-tailed GNSS measurement errors. We develop a procedure to determine the overbounding parameters for both symmetric unimodal (s.u.) and not symmetric unimodal (n.s.u.) heavy-tailed errors and prove that the overbounding property is preserved through convolution. The experiment results on both simulated and real-world datasets reveal that our method can sharply bound heavy-tailed errors at both core and tail regions. In the position domain, the proposed method reduces the average vertical protection level by 15% for s.u. heavy-tailed errors compared to the single-CDF Gaussian overbound, and by 21% to 47% for n.s.u. heavy-tailed errors compared to the Navigation Discrete ENvelope and two-step Gaussian overbounds."}
{"id": "2601.06545", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06545", "abs": "https://arxiv.org/abs/2601.06545", "authors": ["Genshiro Kitagawa"], "title": "Bayesian Optimization of Noisy Log-Likelihoods Evaluated by Particle Filters -- One Parameter Case --", "comment": "15 pages, 2 tables, 4 figures", "summary": "Likelihood functions evaluated using particle filters are typically noisy, computationally expensive, and non-differentiable due to Monte Carlo variability. These characteristics make conventional optimization methods difficult to apply directly or potentially unreliable. This paper investigates the use of Bayesian optimization for maximizing log-likelihood functions estimated by particle filters. By modeling the noisy log-likelihood surface with a Gaussian process surrogate and employing an acquisition function that balances exploration and exploitation, the proposed approach identifies the maximizer using a limited number of likelihood evaluations. Through numerical experiments, we demonstrate that Bayesian optimization provides robust and stable estimation in the presence of observation noise. The results suggest that Bayesian optimization is a promising alternative for likelihood maximization problems where exhaustive search or gradient-based methods are impractical. The estimation accuracy is quantitatively assessed using mean squared error metrics by comparison with the exact maximum likelihood solution obtained via the Kalman filter."}
{"id": "2601.06514", "categories": ["stat.ML", "cs.LG", "math.NA", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.06514", "abs": "https://arxiv.org/abs/2601.06514", "authors": ["Jinyuan Chang", "Chenguang Duan", "Yuling Jiao", "Yi Xu", "Jerry Zhijian Yang"], "title": "Inference-Time Alignment for Diffusion Models via Doob's Matching", "comment": null, "summary": "Inference-time alignment for diffusion models aims to adapt a pre-trained diffusion model toward a target distribution without retraining the base score network, thereby preserving the generative capacity of the base model while enforcing desired properties at the inference time. A central mechanism for achieving such alignment is guidance, which modifies the sampling dynamics through an additional drift term. In this work, we introduce Doob's matching, a novel framework for guidance estimation grounded in Doob's $h$-transform. Our approach formulates guidance as the gradient of logarithm of an underlying Doob's $h$-function and employs gradient-penalized regression to simultaneously estimate both the $h$-function and its gradient, resulting in a consistent estimator of the guidance. Theoretically, we establish non-asymptotic convergence rates for the estimated guidance. Moreover, we analyze the resulting controllable diffusion processes and prove non-asymptotic convergence guarantees for the generated distributions in the 2-Wasserstein distance."}
{"id": "2601.06671", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.06671", "abs": "https://arxiv.org/abs/2601.06671", "authors": ["The Tien Mai", "Sayantan Banerjee"], "title": "Censored Graphical Horseshoe: Bayesian sparse precision matrix estimation with censored and missing data", "comment": null, "summary": "Gaussian graphical models provide a powerful framework for studying conditional dependencies in multivariate data, with widespread applications spanning biomedical, environmental sciences, and other data-rich scientific domains. While the Graphical Horseshoe (GHS) method has emerged as a state-of-the-art Bayesian method for sparse precision matrix estimation, existing approaches assume fully observed data and thus fail in the presence of censoring or missingness, which are pervasive in real-world studies. In this paper, we develop the Censored Graphical Horseshoe (CGHS), a novel Bayesian framework that extends the GHS to censored and arbitrarily missing Gaussian data. By introducing a latent-variable representation, CGHS accommodates incomplete observations while retaining the adaptive global-local shrinkage properties of the Horseshoe prior. We derive efficient Gibbs samplers for posterior computation and establish new theoretical results on posterior behavior under censoring and missingness, filling a gap not addressed by frequentist Lasso-based methods. Through extensive simulations, we demonstrate that CGHS consistently improves estimation accuracy compared to penalized likelihood approaches. Our methods are implemented in the package GHScenmis available on Github: https://github.com/tienmt/ghscenmis ."}
{"id": "2601.07534", "categories": ["stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2601.07534", "abs": "https://arxiv.org/abs/2601.07534", "authors": ["Lampis Tzai", "Ioannis Ntzoufras", "Silvia Bozza"], "title": "Bayesian Handwriting Evidence Evaluation using MANOVA via Fourier-Based Extracted Features", "comment": null, "summary": "This paper proposes a novel statistical approach that aims at the identification of valid and useful patterns in handwriting examination via Bayesian modeling. Starting from a sample of characters selected among 13 French native writers, an accurate loop reconstruction can be achieved through Fourier analysis. The contour shape of handwritten characters can be described by the first four pairs of Fourier coefficients and by the surface size. Six Bayesian models are considered for such handwritten features. These models arise from two likelihood structures: (a) a multivariate Normal model, and (b) a MANOVA model that accounts for character-level variability. For each likelihood, three different prior formulations are examined, resulting in distinct Bayesian models: (i) a conjugate Normal-Inverse-Wishart prior, (ii) a hierarchical Normal-Inverse-Wishart prior, and (iii) a Normal-LogNormal-LKJ prior specification. The hierarchical prior formulations are of primary interest because they can incorporate the between-writers variability, a distinguishing element that sets writers apart. These approaches do not allow calculation of the marginal likelihood in a closed-form expression. Therefore, bridge sampling is used to estimate it. The Bayes factor is estimated to compare the performance of the proposed models and to evaluate their efficiency for discriminating purposes. Bayesian MANOVA with Normal-LogNormal-LKJ prior showed an overall better performance, in terms of discriminatory capacity and model fitting. Finally, a sensitivity analysis for the elicitation of the prior distribution parameters is performed."}
{"id": "2601.06375", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.06375", "abs": "https://arxiv.org/abs/2601.06375", "authors": ["Foo Hui-Mean", "Yuan-chin Ivan Chang"], "title": "Efficient Data Reduction Via PCA-Guided Quantile Based Sampling", "comment": "34", "summary": "In large-scale statistical modeling, reducing data size through subsampling is essential for balancing computational efficiency and statistical accuracy. We propose a new method, Principal Component Analysis guided Quantile Sampling (PCA-QS), which projects data onto principal components and applies quantile-based sampling to retain representative and diverse subsets. Compared with uniform random sampling, leverage score sampling, and coreset methods, PCA-QS consistently achieves lower mean squared error and better preservation of key data characteristics, while also being computationally efficient. This approach is adaptable to a variety of data scenarios and shows strong potential for broad applications in statistical computing."}
{"id": "2601.06610", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06610", "abs": "https://arxiv.org/abs/2601.06610", "authors": ["Monika S. Dhull"], "title": "Mittag Leffler Distributions Estimation and Autoregressive Framework", "comment": null, "summary": "This work deals with the estimation of parameters of Mittag-Leffler (ML($α, σ$)) distribution. We estimate the parameters of ML($α, σ$) using empirical Laplace transform method. The simulation study indicates that the proposed method provides satisfactory results. The real life application of ML($α, σ$) distribution on high frequency trading data is also demonstrated. We also provide the estimation of three-parameter Mittag-Leffler distribution using empirical Laplace transform. Additionally, we establish an autoregressive model of order 1, incorporating the Mittag-Leffler distribution as marginals in one scenario and as innovation terms in another. We apply empirical Laplace transform method to estimate the model parameters and provide the simulation study for the same."}
{"id": "2601.06782", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06782", "abs": "https://arxiv.org/abs/2601.06782", "authors": ["Sungtaek Son", "Eardi Lila", "Kwun Chuen Gary Chan"], "title": "Dimension-reduced outcome-weighted learning for estimating individualized treatment regimes in observational studies", "comment": "54 pages, 9 figures", "summary": "Individualized treatment regimes (ITRs) aim to improve clinical outcomes by assigning treatment based on patient-specific characteristics. However, existing methods often struggle with high-dimensional covariates, limiting accuracy, interpretability, and real-world applicability. We propose a novel sufficient dimension reduction approach that directly targets the contrast between potential outcomes and identifies a low-dimensional subspace of the covariates capturing treatment effect heterogeneity. This reduced representation enables more accurate estimation of optimal ITRs through outcome-weighted learning. To accommodate observational data, our method incorporates kernel-based covariate balancing, allowing treatment assignment to depend on the full covariate set and avoiding the restrictive assumption that the subspace sufficient for modeling heterogeneous treatment effects is also sufficient for confounding adjustment. We show that the proposed method achieves universal consistency, i.e., its risk converges to the Bayes risk, under mild regularity conditions. We demonstrate its finite sample performance through simulations and an analysis of intensive care unit sepsis patient data to determine who should receive transthoracic echocardiography."}
{"id": "2601.06745", "categories": ["stat.CO", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.06745", "abs": "https://arxiv.org/abs/2601.06745", "authors": ["Xavier Mak", "James P. Hobert"], "title": "Extensions of the solidarity principle of the spectral gap for Gibbs samplers to their blocked and collapsed variants", "comment": "32 pages", "summary": "Connections of a spectral nature are formed between Gibbs samplers and their blocked and collapsed variants. The solidarity principle of the spectral gap for full Gibbs samplers is generalized to different cycles and mixtures of Gibbs steps. This generalized solidarity principle is employed to establish that every cycle and mixture of Gibbs steps, which includes blocked Gibbs samplers and collapsed Gibbs samplers, inherits a spectral gap from a full Gibbs sampler. Exact relations between the spectra corresponding to blocked and collapsed variants of a Gibbs sampler are also established. An example is given to show that a blocked or collapsed Gibbs sampler does not in general inherit geometric ergodicity or a spectral gap from another blocked or collapsed Gibbs sampler."}
{"id": "2601.07594", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07594", "abs": "https://arxiv.org/abs/2601.07594", "authors": ["Gianna Gavriel", "Maria Pregnolato", "Francesca Pianosi", "Theo Tryfonas", "Paul Vardanega"], "title": "An evaluation of empirical equations for assessing local scour around bridge piers using global sensitivity analysis", "comment": null, "summary": "Bridge scour is a complex phenomenon combining hydrological, geotechnical and structural processes. Bridge scour is the leading cause of bridge collapse, which can bring catastrophic consequences including the loss of life. Estimating scour on bridges is an important task for engineers assessing bridge system performance. Overestimation of scour depths during design may lead to excess spendings on construction whereas underestimation can lead to the collapse of a bridge. Many empirical equations have been developed over the years to assess scour depth at bridge piers. These equations have only been calibrated with laboratory data or very few field data. This paper compares eight equations including the UK CIRIA C742 approach to establish their accuracy using the open access USGS pier-scour database for both field and laboratory conditions. A one-at-the-time sensitivity assessment and a global sensitivity analysis were then applied to identify the most significant parameters in the eight scour equations. The paper shows that using a global approach, i.e. one where all parameters are varied simultaneously, provides more insights than a traditional one-at-the-time approach. The main findings are that the CIRIA and Froehlich equations are the most accurate equations for field conditions, and that angle of attack, pier shape and the approach flow depth are the most influential parameters. Efforts to reduce uncertainty of these three parameters would maximise increase of scour estimate precision."}
{"id": "2601.06745", "categories": ["stat.CO", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.06745", "abs": "https://arxiv.org/abs/2601.06745", "authors": ["Xavier Mak", "James P. Hobert"], "title": "Extensions of the solidarity principle of the spectral gap for Gibbs samplers to their blocked and collapsed variants", "comment": "32 pages", "summary": "Connections of a spectral nature are formed between Gibbs samplers and their blocked and collapsed variants. The solidarity principle of the spectral gap for full Gibbs samplers is generalized to different cycles and mixtures of Gibbs steps. This generalized solidarity principle is employed to establish that every cycle and mixture of Gibbs steps, which includes blocked Gibbs samplers and collapsed Gibbs samplers, inherits a spectral gap from a full Gibbs sampler. Exact relations between the spectra corresponding to blocked and collapsed variants of a Gibbs sampler are also established. An example is given to show that a blocked or collapsed Gibbs sampler does not in general inherit geometric ergodicity or a spectral gap from another blocked or collapsed Gibbs sampler."}
{"id": "2601.06671", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.06671", "abs": "https://arxiv.org/abs/2601.06671", "authors": ["The Tien Mai", "Sayantan Banerjee"], "title": "Censored Graphical Horseshoe: Bayesian sparse precision matrix estimation with censored and missing data", "comment": null, "summary": "Gaussian graphical models provide a powerful framework for studying conditional dependencies in multivariate data, with widespread applications spanning biomedical, environmental sciences, and other data-rich scientific domains. While the Graphical Horseshoe (GHS) method has emerged as a state-of-the-art Bayesian method for sparse precision matrix estimation, existing approaches assume fully observed data and thus fail in the presence of censoring or missingness, which are pervasive in real-world studies. In this paper, we develop the Censored Graphical Horseshoe (CGHS), a novel Bayesian framework that extends the GHS to censored and arbitrarily missing Gaussian data. By introducing a latent-variable representation, CGHS accommodates incomplete observations while retaining the adaptive global-local shrinkage properties of the Horseshoe prior. We derive efficient Gibbs samplers for posterior computation and establish new theoretical results on posterior behavior under censoring and missingness, filling a gap not addressed by frequentist Lasso-based methods. Through extensive simulations, we demonstrate that CGHS consistently improves estimation accuracy compared to penalized likelihood approaches. Our methods are implemented in the package GHScenmis available on Github: https://github.com/tienmt/ghscenmis ."}
{"id": "2601.06830", "categories": ["stat.ML", "cs.LG", "math.NA", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.06830", "abs": "https://arxiv.org/abs/2601.06830", "authors": ["Yinan Hu", "Estaban Tabak"], "title": "Constrained Density Estimation via Optimal Transport", "comment": null, "summary": "A novel framework for density estimation under expectation constraints is proposed. The framework minimizes the Wasserstein distance between the estimated density and a prior, subject to the constraints that the expected value of a set of functions adopts or exceeds given values. The framework is generalized to include regularization inequalities to mitigate the artifacts in the target measure. An annealing-like algorithm is developed to address non-smooth constraints, with its effectiveness demonstrated through both synthetic and proof-of-concept real world examples in finance."}
{"id": "2601.07074", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07074", "abs": "https://arxiv.org/abs/2601.07074", "authors": ["Pedro Abdalla", "Junren Chen"], "title": "Robust Mean Estimation under Quantization", "comment": null, "summary": "We consider the problem of mean estimation under quantization and adversarial corruption. We construct multivariate robust estimators that are optimal up to logarithmic factors in two different settings. The first is a one-bit setting, where each bit depends only on a single sample, and the second is a partial quantization setting, in which the estimator may use a small fraction of unquantized data."}
{"id": "2601.07668", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07668", "abs": "https://arxiv.org/abs/2601.07668", "authors": ["Shiro Kuriwaki", "Cory McCartan"], "title": "The Role of Confounders and Linearity in Ecological Inference: A Reassessment", "comment": "37 pages", "summary": "Estimating conditional means using only the marginal means available from aggregate data is commonly known as the ecological inference problem (EI). We provide a reassessment of EI, including a new formalization of identification conditions and a demonstration of how these conditions fail to hold in common cases. The identification conditions reveal that, similar to causal inference, credible ecological inference requires controlling for confounders. The aggregation process itself creates additional structure to assist in estimation by restricting the conditional expectation function to be linear in the predictor variable. A linear model perspective also clarifies the differences between the EI methods commonly used in the literature, and when they lead to ecological fallacies. We provide an overview of new methodology which builds on both the identification and linearity results to flexibly control for confounders and yield improved ecological inferences. Finally, using datasets for common EI problems in which the ground truth is fortuitously observed, we show that, while covariates can help, all methods are prone to overestimating both racial polarization and nationalized partisan voting."}
{"id": "2601.07065", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2601.07065", "abs": "https://arxiv.org/abs/2601.07065", "authors": ["Eric Feltham"], "title": "FormulaCompiler.jl and Margins.jl: Efficient Marginal Effects in Julia", "comment": "25 pages, 4 figures, 8 tables", "summary": "Marginal effects analysis is fundamental to interpreting statistical models, yet existing implementations face computational constraints that limit analysis at scale. We introduce two Julia packages that address this gap. Margins.jl provides a clean two-function API organizing analysis around a 2-by-2 framework: evaluation context (population vs profile) by analytical target (effects vs predictions). The package supports interaction analysis through second differences, elasticity measures, categorical mixtures for representative profiles, and robust standard errors. FormulaCompiler.jl provides the computational foundation, transforming statistical formulas into zero-allocation, type-specialized evaluators that enable O(p) per-row computation independent of dataset size. Together, these packages achieve 622x average speedup and 460x memory reduction compared to R's marginaleffects package, with successful computation of average marginal effects and delta-method standard errors on 500,000 observations where R fails due to memory exhaustion, providing the first comprehensive and efficient marginal effects implementation for Julia's statistical ecosystem."}
{"id": "2601.06685", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.06685", "abs": "https://arxiv.org/abs/2601.06685", "authors": ["Glen A. Satten", "Mo Li", "Ni Zhao", "Robert L. Strawderman"], "title": "R-Estimation with Right-Censored Data", "comment": null, "summary": "This paper considers the problem of directly generalizing the R-estimator under a linear model formulation with right-censored outcomes. We propose a natural generalization of the rank and corresponding estimating equation for the R-estimator in the case of the Wilcoxon (i.e., linear-in-ranks) score function, and show how it can respectively be exactly represented as members of the classes of estimating equations proposed in Ritov (1990) and Tsiatis (1990). We then establish analogous results for a large class of bounded nonlinear-in-ranks score functions. Asymptotics and variance estimation are obtained as straightforward consequences of these representation results. The self-consistent estimator of the residual distribution function, and the mid-cumulative distribution function (and, where needed, a generalization of it), play critical roles in these developments."}
{"id": "2601.06961", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06961", "abs": "https://arxiv.org/abs/2601.06961", "authors": ["Taishi Watanabe", "Ryo Karakida", "Jun-nosuke Teramae"], "title": "The Impact of Anisotropic Covariance Structure on the Training Dynamics and Generalization Error of Linear Networks", "comment": "18 pages", "summary": "The success of deep neural networks largely depends on the statistical structure of the training data. While learning dynamics and generalization on isotropic data are well-established, the impact of pronounced anisotropy on these crucial aspects is not yet fully understood. We examine the impact of data anisotropy, represented by a spiked covariance structure, a canonical yet tractable model, on the learning dynamics and generalization error of a two-layer linear network in a linear regression setting. Our analysis reveals that the learning dynamics proceed in two distinct phases, governed initially by the input-output correlation and subsequently by other principal directions of the data structure. Furthermore, we derive an analytical expression for the generalization error, quantifying how the alignment of the spike structure of the data with the learning task improves performance. Our findings offer deep theoretical insights into how data anisotropy shapes the learning trajectory and final performance, providing a foundation for understanding complex interactions in more advanced network architectures."}
{"id": "2601.07144", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07144", "abs": "https://arxiv.org/abs/2601.07144", "authors": ["Linus Bleistein", "Mathieu Dagréou", "Francisco Andrade", "Thomas Boudou", "Aurélien Bellet"], "title": "Optimal Transport under Group Fairness Constraints", "comment": null, "summary": "Ensuring fairness in matching algorithms is a key challenge in allocating scarce resources and positions. Focusing on Optimal Transport (OT), we introduce a novel notion of group fairness requiring that the probability of matching two individuals from any two given groups in the OT plan satisfies a predefined target. We first propose \\texttt{FairSinkhorn}, a modified Sinkhorn algorithm to compute perfectly fair transport plans efficiently. Since exact fairness can significantly degrade matching quality in practice, we then develop two relaxation strategies. The first one involves solving a penalised OT problem, for which we derive novel finite-sample complexity guarantees. This result is of independent interest as it can be generalized to arbitrary convex penalties. Our second strategy leverages bilevel optimization to learn a ground cost that induces a fair OT solution, and we establish a bound guaranteeing that the learned cost yields fair matchings on unseen data. Finally, we present empirical results that illustrate the trade-offs between fairness and performance."}
{"id": "2601.06375", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.06375", "abs": "https://arxiv.org/abs/2601.06375", "authors": ["Foo Hui-Mean", "Yuan-chin Ivan Chang"], "title": "Efficient Data Reduction Via PCA-Guided Quantile Based Sampling", "comment": "34", "summary": "In large-scale statistical modeling, reducing data size through subsampling is essential for balancing computational efficiency and statistical accuracy. We propose a new method, Principal Component Analysis guided Quantile Sampling (PCA-QS), which projects data onto principal components and applies quantile-based sampling to retain representative and diverse subsets. Compared with uniform random sampling, leverage score sampling, and coreset methods, PCA-QS consistently achieves lower mean squared error and better preservation of key data characteristics, while also being computationally efficient. This approach is adaptable to a variety of data scenarios and shows strong potential for broad applications in statistical computing."}
{"id": "2601.07446", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07446", "abs": "https://arxiv.org/abs/2601.07446", "authors": ["Alessandra Ragni", "Lara Cavinato", "Francesca Ieva"], "title": "Penalized Likelihood Optimization for Adaptive Neighborhood Clustering in Time-to-Event Data with Group-Level Heterogeneity", "comment": null, "summary": "The identification of patient subgroups with comparable event-risk dynamics plays a key role in supporting informed decision-making in clinical research. In such settings, it is important to account for the inherent dependence that arises when individuals are nested within higher-level units, such as hospitals. Existing survival models account for group-level heterogeneity through frailty terms but do not uncover latent patient subgroups, while most clustering methods ignore hierarchical structure and are not estimated jointly with survival outcomes. In this work, we introduce a new framework that simultaneously performs patient clustering and shared-frailty survival modeling through a penalized likelihood approach. The proposed methodology adaptively learns a patient-to-patient similarity matrix via a modified version of spectral clustering, enabling cluster formation directly from estimated risk profiles while accounting for group membership. A simulation study highlights the proposed model's ability to recover latent clusters and to correctly estimate hazard parameters. We apply our method to a large cohort of heart-failure patients hospitalized with COVID-19 between 2020 and 2021 in the Lombardy region (Italy), identifying clinically meaningful subgroups characterized by distinct risk profiles and highlighting the role of respiratory comorbidities and hospital-level variability in shaping mortality outcomes. This framework provides a flexible and interpretable tool for risk-based patient stratification in hierarchical data settings."}
{"id": "2601.06695", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06695", "abs": "https://arxiv.org/abs/2601.06695", "authors": ["Sphiwe B. Skhosana", "Weixin Yao"], "title": "Nonparametric contaminated Gaussian mixture of regressions", "comment": null, "summary": "Semi- and non-parametric mixture of regressions are a very useful flexible class of mixture of regressions in which some or all of the parameters are non-parametric functions of the covariates. These models are, however, based on the Gaussian assumption of the component error distributions. Thus, their estimation is sensitive to outliers and heavy-tailed error distributions. In this paper, we propose semi- and non-parametric contaminated Gaussian mixture of regressions to robustly estimate the parametric and/or non-parametric terms of the models in the presence of mild outliers. The virtue of using a contaminated Gaussian error distribution is that we can simultaneously perform model-based clustering of observations and model-based outlier detection. We propose two algorithms, an expectation-maximization (EM)-type algorithm and an expectation-conditional-maximization (ECM)-type algorithm, to perform maximum likelihood and local-likelihood kernel estimation of the parametric and non-parametric of the proposed models, respectively. The robustness of the proposed models is examined using an extensive simulation study. The practical utility of the proposed models is demonstrated using real data."}
{"id": "2601.06982", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06982", "abs": "https://arxiv.org/abs/2601.06982", "authors": ["Zhiyuan Tang", "Wanning Chen", "Kan Xu"], "title": "Match Made with Matrix Completion: Efficient Learning under Matching Interference", "comment": null, "summary": "Matching markets face increasing needs to learn the matching qualities between demand and supply for effective design of matching policies. In practice, the matching rewards are high-dimensional due to the growing diversity of participants. We leverage a natural low-rank matrix structure of the matching rewards in these two-sided markets, and propose to utilize matrix completion to accelerate reward learning with limited offline data. A unique property for matrix completion in this setting is that the entries of the reward matrix are observed with matching interference -- i.e., the entries are not observed independently but dependently due to matching or budget constraints. Such matching dependence renders unique technical challenges, such as sub-optimality or inapplicability of the existing analytical tools in the matrix completion literature, since they typically rely on sample independence. In this paper, we first show that standard nuclear norm regularization remains theoretically effective under matching interference. We provide a near-optimal Frobenius norm guarantee in this setting, coupled with a new analytical technique. Next, to guide certain matching decisions, we develop a novel ``double-enhanced'' estimator, based off the nuclear norm estimator, with a near-optimal entry-wise guarantee. Our double-enhancement procedure can apply to broader sampling schemes even with dependence, which may be of independent interest. Additionally, we extend our approach to online learning settings with matching constraints such as optimal matching and stable matching, and present improved regret bounds in matrix dimensions. Finally, we demonstrate the practical value of our methods using both synthetic data and real data of labor markets."}
{"id": "2601.07247", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07247", "abs": "https://arxiv.org/abs/2601.07247", "authors": ["Yiran Jia"], "title": "Multi-environment Invariance Learning with Missing Data", "comment": null, "summary": "Learning models that can handle distribution shifts is a key challenge in domain generalization. Invariance learning, an approach that focuses on identifying features invariant across environments, improves model generalization by capturing stable relationships, which may represent causal effects when the data distribution is encoded within a structural equation model (SEM) and satisfies modularity conditions. This has led to a growing body of work that builds on invariance learning, leveraging the inherent heterogeneity across environments to develop methods that provide causal explanations while enhancing robust prediction. However, in many practical scenarios, obtaining complete outcome data from each environment is challenging due to the high cost or complexity of data collection. This limitation in available data hinders the development of models that fully leverage environmental heterogeneity, making it crucial to address missing outcomes to improve both causal insights and robust prediction. In this work, we derive an estimator from the invariance objective under missing outcomes. We establish non-asymptotic guarantees on variable selection property and $\\ell_2$ error convergence rates, which are influenced by the proportion of missing data and the quality of imputation models across environments. We evaluate the performance of the new estimator through extensive simulations and demonstrate its application using the UCI Bike Sharing dataset to predict the count of bike rentals. The results show that despite relying on a biased imputation model, the estimator is efficient and achieves lower prediction error, provided the bias is within a reasonable range."}
{"id": "2601.06685", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.06685", "abs": "https://arxiv.org/abs/2601.06685", "authors": ["Glen A. Satten", "Mo Li", "Ni Zhao", "Robert L. Strawderman"], "title": "R-Estimation with Right-Censored Data", "comment": null, "summary": "This paper considers the problem of directly generalizing the R-estimator under a linear model formulation with right-censored outcomes. We propose a natural generalization of the rank and corresponding estimating equation for the R-estimator in the case of the Wilcoxon (i.e., linear-in-ranks) score function, and show how it can respectively be exactly represented as members of the classes of estimating equations proposed in Ritov (1990) and Tsiatis (1990). We then establish analogous results for a large class of bounded nonlinear-in-ranks score functions. Asymptotics and variance estimation are obtained as straightforward consequences of these representation results. The self-consistent estimator of the residual distribution function, and the mid-cumulative distribution function (and, where needed, a generalization of it), play critical roles in these developments."}
{"id": "2601.07532", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2601.07532", "abs": "https://arxiv.org/abs/2601.07532", "authors": ["Nathan Green", "Antonio Remiro-Azocar"], "title": "Population-Adjusted Indirect Treatment Comparison with the outstandR Package in R", "comment": "35 pages", "summary": "Indirect treatment comparisons (ITCs) are essential in Health Technology Assessment (HTA) when head-to-head clinical trials are absent. A common challenge arises when attempting to compare a treatment with available individual patient data (IPD) against a competitor with only reported aggregate-level data (ALD), particularly when trial populations differ in effect modifiers. While methods such as Matching-Adjusted Indirect Comparison (MAIC) and Simulated Treatment Comparison (STC) exist to adjust for these cross-trial differences, software implementations have often been fragmented or limited in scope. This article introduces outstandR, an R package designed to provide a comprehensive and unified framework for population-adjusted indirect comparison (PAIC). Beyond standard weighting and regression approaches, outstandR implements advanced G-computation methods within both maximum likelihood and Bayesian frameworks, and Multiple Imputation Marginalization (MIM) to address non-collapsibility and missing data. By streamlining the workflow of covariate simulation, model standardization, and contrast estimation, outstandR enables robust and compatible evidence synthesis in complex decision-making scenarios."}
{"id": "2601.06807", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06807", "abs": "https://arxiv.org/abs/2601.06807", "authors": ["Yiling Xie"], "title": "Adversarially Perturbed Precision Matrix Estimation", "comment": null, "summary": "Precision matrix estimation is a fundamental topic in multivariate statistics and modern machine learning. This paper proposes an adversarially perturbed precision matrix estimation framework, motivated by recent developments in adversarial training. The proposed framework is versatile for the precision matrix problem since, by adapting to different perturbation geometries, the proposed framework can not only recover the existing distributionally robust method but also inspire a novel moment-adaptive approach to precision matrix estimation, proven capable of sparsity recovery and adversarial robustness. Notably, the proposed perturbed precision matrix framework is proven to be asymptotically equivalent to regularized precision matrix estimation, and the asymptotic normality can be established accordingly. The resulting asymptotic distribution highlights the asymptotic bias introduced by perturbation and identifies conditions under which the perturbed estimation can be unbiased in the asymptotic sense. Numerical experiments on both synthetic and real data demonstrate the desirable performance of the proposed adversarially perturbed approach in practice."}
{"id": "2601.07013", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07013", "abs": "https://arxiv.org/abs/2601.07013", "authors": ["Luke S. Lagunowich", "Guoxiang Grayson Tong", "Daniele E. Schiavazzi"], "title": "Conditional Normalizing Flows for Forward and Backward Joint State and Parameter Estimation", "comment": null, "summary": "Traditional filtering algorithms for state estimation -- such as classical Kalman filtering, unscented Kalman filtering, and particle filters - show performance degradation when applied to nonlinear systems whose uncertainty follows arbitrary non-Gaussian, and potentially multi-modal distributions. This study reviews recent approaches to state estimation via nonlinear filtering based on conditional normalizing flows, where the conditional embedding is generated by standard MLP architectures, transformers or selective state-space models (like Mamba-SSM). In addition, we test the effectiveness of an optimal-transport-inspired kinetic loss term in mitigating overparameterization in flows consisting of a large collection of transformations. We investigate the performance of these approaches on applications relevant to autonomous driving and patient population dynamics, paying special attention to how they handle time inversion and chained predictions. Finally, we assess the performance of various conditioning strategies for an application to real-world COVID-19 joint SIR system forecasting and parameter estimation."}
{"id": "2601.07325", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07325", "abs": "https://arxiv.org/abs/2601.07325", "authors": ["EL Mahdi Khribch", "Pierre Alquier"], "title": "Variational Approximations for Robust Bayesian Inference via Rho-Posteriors", "comment": "53 pages including the proofs in appendices, 16 figures", "summary": "The $ρ$-posterior framework provides universal Bayesian estimation with explicit contamination rates and optimal convergence guarantees, but has remained computationally difficult due to an optimization over reference distributions that precludes intractable posterior computation. We develop a PAC-Bayesian framework that recovers these theoretical guarantees through temperature-dependent Gibbs posteriors, deriving finite-sample oracle inequalities with explicit rates and introducing tractable variational approximations that inherit the robustness properties of exact $ρ$-posteriors. Numerical experiments demonstrate that this approach achieves theoretical contamination rates while remaining computationally feasible, providing the first practical implementation of $ρ$-posterior inference with rigorous finite-sample guarantees."}
{"id": "2601.07249", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07249", "abs": "https://arxiv.org/abs/2601.07249", "authors": ["Suchismita Das", "Akul Ameya", "Cahyani Karunia Putri"], "title": "Compounded Linear Failure Rate Distribution: Properties, Simulation and Analysis", "comment": null, "summary": "This paper proposes a new extension of the linear failure rate (LFR) model to better capture real-world lifetime data. The model incorporates an additional shape parameter to increase flexibility. It helps model the minimum survival time from a set of LFR distributed variables. We define the model, derive certain statistical properties such as the mean residual life, the mean inactivity time, moments, quantile, order statistics and also discuss the results on stochastic orders of the proposed distribution. The proposed model has increasing, bathtub shaped and inverse bathtub shaped hazard rate function. We use the method of maximum likelihood estimation to estimate the unknown parameters. We conduct simulation studies to examine the behavior of the estimators. We also use three real datasets to evaluate the model, which turns out superior compared to classical alternatives."}
{"id": "2601.06390", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.06390", "abs": "https://arxiv.org/abs/2601.06390", "authors": ["Qianqian Yao"], "title": "Empirical Likelihood Test for Common Invariant Subspace of Multilayer Networks based on Monte Carlo Approximation", "comment": null, "summary": "Multilayer (or multiple) networks are widely used to represent diverse patterns of relationships among objects in increasingly complex real-world systems. Identifying a common invariant subspace across network layers has become an active area of research, as such a subspace can filter out layer-specific noise, facilitate cross-network comparisons, reduce dimensionality, and extract shared structural features of scientific interest. One statistical approach to detecting a common subspace is hypothesis testing, which evaluates whether the observed networks share a common latent structure. In this paper, we propose an empirical likelihood (EL) based test for this purpose. The null hypothesis states that all network layers share the same invariant subspace, whereas under the alternative hypothesis at least two layers differ in their subspaces. We study the asymptotic behavior of the proposed test via Monte Carlo approximation and assess its finite-sample performance through extensive simulations. The simulation results demonstrate that the proposed method achieves satisfactory size and power, and its practical utility is further illustrated with a real-data application."}
{"id": "2601.06890", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06890", "abs": "https://arxiv.org/abs/2601.06890", "authors": ["Rahul Konar", "Ramnivas Jat", "Neeraj Joshi", "Raghu Nandan Sengupta"], "title": "Likelihood-Based Regression for Weibull Accelerated Life Testing Model Under Censored Data", "comment": null, "summary": "In this paper, we investigate accelerated life testing (ALT) models based on the Weibull distribution with stress-dependent shape and scale parameters. Temperature and voltage are treated as stress variables influencing the lifetime distribution. Data are assumed to be collected under Progressive Hybrid Censoring (PHC) and Adaptive Progressive Hybrid Censoring (APHC). A two-step estimation framework is developed. First, the Weibull parameters are estimated via maximum likelihood, and the consistency and asymptotic normality of the estimators are established under both censoring schemes. Second, the resulting parameter estimates are linked to the stress variables through a regression model to quantify the stress-lifetime relationship. Extensive simulations are conducted to examine finite-sample performance under a range of parameter settings, and a data illustration is also presented to showcase practical relevance. The proposed framework provides a flexible approach for modeling stress-dependent reliability behavior in ALT studies under complex censoring schemes."}
{"id": "2601.07061", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07061", "abs": "https://arxiv.org/abs/2601.07061", "authors": ["Alex Kokot", "Anand Hemmady", "Vydhourie Thiyageswaran", "Marina Meila"], "title": "Local EGOP for Continuous Index Learning", "comment": null, "summary": "We introduce the setting of continuous index learning, in which a function of many variables varies only along a small number of directions at each point. For efficient estimation, it is beneficial for a learning algorithm to adapt, near each point $x$, to the subspace that captures the local variability of the function $f$. We pose this task as kernel adaptation along a manifold with noise, and introduce Local EGOP learning, a recursive algorithm that utilizes the Expected Gradient Outer Product (EGOP) quadratic form as both a metric and inverse-covariance of our target distribution. We prove that Local EGOP learning adapts to the regularity of the function of interest, showing that under a supervised noisy manifold hypothesis, intrinsic dimensional learning rates are achieved for arbitrarily high-dimensional noise. Empirically, we compare our algorithm to the feature learning capabilities of deep learning. Additionally, we demonstrate improved regression quality compared to two-layer neural networks in the continuous single-index setting."}
{"id": "2601.07369", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07369", "abs": "https://arxiv.org/abs/2601.07369", "authors": ["Roberto Fontana", "Elisa Perrone", "Fabio Rapallo"], "title": "Characterization of multi-way binary tables with uniform margins and fixed correlations", "comment": "21 pages", "summary": "In many applications involving binary variables, only pairwise dependence measures, such as correlations, are available. However, for multi-way tables involving more than two variables, these quantities do not uniquely determine the joint distribution, but instead define a family of admissible distributions that share the same pairwise dependence while potentially differing in higher-order interactions. In this paper, we introduce a geometric framework to describe the entire feasible set of such joint distributions with uniform margins. We show that this admissible set forms a convex polytope, analyze its symmetry properties, and characterize its extreme rays. These extremal distributions provide fundamental insights into how higher-order dependence structures may vary while preserving the prescribed pairwise information. Unlike traditional methods for table generation, which return a single table, our framework makes it possible to explore and understand the full admissible space of dependence structures, enabling more flexible choices for modeling and simulation. We illustrate the usefulness of our theoretical results through examples and a real case study on rater agreement."}
{"id": "2601.07267", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07267", "abs": "https://arxiv.org/abs/2601.07267", "authors": ["Shuli Chen", "Jie Hu", "Zhichao Jiang"], "title": "Connections as treatment: causal inference with edge interventions in networks", "comment": null, "summary": "Causal inference has traditionally focused on interventions at the unit level. In many applications, however, the central question concerns the causal effects of connections between units, such as transportation links, social relationships, or collaborative ties. We develop a causal framework for edge interventions in networks, where treatments correspond to the presence or absence of edges. Our framework defines causal estimands under stochastic interventions on the network structure and introduces an inverse probability weighting estimator under an unconfoundedness assumption on edge assignment. We estimate edge probabilities using exponential random graph models, a widely used class of network models. We establish consistency and asymptotic normality of the proposed estimator. Finally, we apply our methodology to China's transportation network to estimate the causal impact of railroad connections on regional economic development."}
{"id": "2601.06671", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.06671", "abs": "https://arxiv.org/abs/2601.06671", "authors": ["The Tien Mai", "Sayantan Banerjee"], "title": "Censored Graphical Horseshoe: Bayesian sparse precision matrix estimation with censored and missing data", "comment": null, "summary": "Gaussian graphical models provide a powerful framework for studying conditional dependencies in multivariate data, with widespread applications spanning biomedical, environmental sciences, and other data-rich scientific domains. While the Graphical Horseshoe (GHS) method has emerged as a state-of-the-art Bayesian method for sparse precision matrix estimation, existing approaches assume fully observed data and thus fail in the presence of censoring or missingness, which are pervasive in real-world studies. In this paper, we develop the Censored Graphical Horseshoe (CGHS), a novel Bayesian framework that extends the GHS to censored and arbitrarily missing Gaussian data. By introducing a latent-variable representation, CGHS accommodates incomplete observations while retaining the adaptive global-local shrinkage properties of the Horseshoe prior. We derive efficient Gibbs samplers for posterior computation and establish new theoretical results on posterior behavior under censoring and missingness, filling a gap not addressed by frequentist Lasso-based methods. Through extensive simulations, we demonstrate that CGHS consistently improves estimation accuracy compared to penalized likelihood approaches. Our methods are implemented in the package GHScenmis available on Github: https://github.com/tienmt/ghscenmis ."}
{"id": "2601.06900", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06900", "abs": "https://arxiv.org/abs/2601.06900", "authors": ["Issey Sukeda", "Tomonari Sei"], "title": "Minimum information Markov model", "comment": "27 pages, 6 figures", "summary": "The analysis of high-dimensional time series data has become increasingly important across a wide range of fields. Recently, a method for constructing the minimum information Markov kernel on finite state spaces was established. In this study, we propose a statistical model based on a parametrization of its dependence function, which we call the \\textit{Minimum Information Markov Model}. We show that its parametrization induces an orthogonal structure between the stationary distribution and the dependence function, and that the model arises as the optimal solution to a divergence rate minimization problem. In particular, for the Gaussian autoregressive case, we establish the existence of the optimal solution to this minimization problem, a nontrivial result requiring a rigorous proof. For parameter estimation, our approach exploits the conditional independence structure inherent in the model, which is supported by the orthogonality. Specifically, we develop several estimators, including conditional likelihood and pseudo likelihood estimators, for the minimum information Markov model in both univariate and multivariate settings. We demonstrate their practical performance through simulation studies and applications to real-world time series data."}
{"id": "2601.07074", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07074", "abs": "https://arxiv.org/abs/2601.07074", "authors": ["Pedro Abdalla", "Junren Chen"], "title": "Robust Mean Estimation under Quantization", "comment": null, "summary": "We consider the problem of mean estimation under quantization and adversarial corruption. We construct multivariate robust estimators that are optimal up to logarithmic factors in two different settings. The first is a one-bit setting, where each bit depends only on a single sample, and the second is a partial quantization setting, in which the estimator may use a small fraction of unquantized data."}
{"id": "2601.07446", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07446", "abs": "https://arxiv.org/abs/2601.07446", "authors": ["Alessandra Ragni", "Lara Cavinato", "Francesca Ieva"], "title": "Penalized Likelihood Optimization for Adaptive Neighborhood Clustering in Time-to-Event Data with Group-Level Heterogeneity", "comment": null, "summary": "The identification of patient subgroups with comparable event-risk dynamics plays a key role in supporting informed decision-making in clinical research. In such settings, it is important to account for the inherent dependence that arises when individuals are nested within higher-level units, such as hospitals. Existing survival models account for group-level heterogeneity through frailty terms but do not uncover latent patient subgroups, while most clustering methods ignore hierarchical structure and are not estimated jointly with survival outcomes. In this work, we introduce a new framework that simultaneously performs patient clustering and shared-frailty survival modeling through a penalized likelihood approach. The proposed methodology adaptively learns a patient-to-patient similarity matrix via a modified version of spectral clustering, enabling cluster formation directly from estimated risk profiles while accounting for group membership. A simulation study highlights the proposed model's ability to recover latent clusters and to correctly estimate hazard parameters. We apply our method to a large cohort of heart-failure patients hospitalized with COVID-19 between 2020 and 2021 in the Lombardy region (Italy), identifying clinically meaningful subgroups characterized by distinct risk profiles and highlighting the role of respiratory comorbidities and hospital-level variability in shaping mortality outcomes. This framework provides a flexible and interpretable tool for risk-based patient stratification in hierarchical data settings."}
{"id": "2601.07003", "categories": ["stat.ME", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.07003", "abs": "https://arxiv.org/abs/2601.07003", "authors": ["Roman Hornung", "Alexander Hapfelmeier"], "title": "Unity Forests: Improving Interaction Modelling and Interpretability in Random Forests", "comment": "33 pages, 12 figures", "summary": "Random forests (RFs) are widely used for prediction and variable importance analysis and are often believed to capture any types of interactions via recursive splitting. However, since the splits are chosen locally, interactions are only reliably captured when at least one involved covariate has a marginal effect. We introduce unity forests (UFOs), an RF variant designed to better exploit interactions involving covariates without marginal effects. In UFOs, the first few splits of each tree are optimized jointly across a random covariate subset to form a \"tree root\" capturing such interactions; the remainder is grown conventionally. We further propose the unity variable importance measure (VIM), which is based on out-of-bag split criterion values from the tree roots. Here, only a small fraction of tree root splits with the highest in-bag criterion values are considered per covariate, reflecting that covariates with purely interaction-based effects are discriminative only if a split in an interacting covariate occurred earlier in the tree. Finally, we introduce covariate-representative tree roots (CRTRs), which select representative tree roots per covariate and provide interpretable insight into the conditions - marginal or interactive - under which each covariate has its strongest effects. In a simulation study, the unity VIM reliably identified interacting covariates without marginal effects, unlike conventional RF-based VIMs. In a large-scale real-data comparison, UFOs achieved higher discrimination and predictive accuracy than standard RFs, with comparable calibration. The CRTRs reproduced the covariates' true effect types reliably in simulated data and provided interesting insights in a real data analysis."}
{"id": "2601.06989", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06989", "abs": "https://arxiv.org/abs/2601.06989", "authors": ["Hao-Xuan Sun", "Song Xi Chen", "Yumou Qiu"], "title": "Localization Estimator for High Dimensional Tensor Covariance Matrices", "comment": "25 pages, 7 figures, 4 tables", "summary": "This paper considers covariance matrix estimation of tensor data under high dimensionality. A multi-bandable covariance class is established to accommodate the need for complex covariance structures of multi-layer lattices and general covariance decay patterns. We propose a high dimensional covariance localization estimator for tensor data, which regulates the sample covariance matrix through a localization function. The statistical properties of the proposed estimator are studied by deriving the minimax rates of convergence under the spectral and the Frobenius norms. Numerical experiments and real data analysis on ocean eddy data are carried out to illustrate the utility of the proposed method in practice."}
{"id": "2601.07144", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07144", "abs": "https://arxiv.org/abs/2601.07144", "authors": ["Linus Bleistein", "Mathieu Dagréou", "Francisco Andrade", "Thomas Boudou", "Aurélien Bellet"], "title": "Optimal Transport under Group Fairness Constraints", "comment": null, "summary": "Ensuring fairness in matching algorithms is a key challenge in allocating scarce resources and positions. Focusing on Optimal Transport (OT), we introduce a novel notion of group fairness requiring that the probability of matching two individuals from any two given groups in the OT plan satisfies a predefined target. We first propose \\texttt{FairSinkhorn}, a modified Sinkhorn algorithm to compute perfectly fair transport plans efficiently. Since exact fairness can significantly degrade matching quality in practice, we then develop two relaxation strategies. The first one involves solving a penalised OT problem, for which we derive novel finite-sample complexity guarantees. This result is of independent interest as it can be generalized to arbitrary convex penalties. Our second strategy leverages bilevel optimization to learn a ground cost that induces a fair OT solution, and we establish a bound guaranteeing that the learned cost yields fair matchings on unseen data. Finally, we present empirical results that illustrate the trade-offs between fairness and performance."}
{"id": "2601.07003", "categories": ["stat.ME", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.07003", "abs": "https://arxiv.org/abs/2601.07003", "authors": ["Roman Hornung", "Alexander Hapfelmeier"], "title": "Unity Forests: Improving Interaction Modelling and Interpretability in Random Forests", "comment": "33 pages, 12 figures", "summary": "Random forests (RFs) are widely used for prediction and variable importance analysis and are often believed to capture any types of interactions via recursive splitting. However, since the splits are chosen locally, interactions are only reliably captured when at least one involved covariate has a marginal effect. We introduce unity forests (UFOs), an RF variant designed to better exploit interactions involving covariates without marginal effects. In UFOs, the first few splits of each tree are optimized jointly across a random covariate subset to form a \"tree root\" capturing such interactions; the remainder is grown conventionally. We further propose the unity variable importance measure (VIM), which is based on out-of-bag split criterion values from the tree roots. Here, only a small fraction of tree root splits with the highest in-bag criterion values are considered per covariate, reflecting that covariates with purely interaction-based effects are discriminative only if a split in an interacting covariate occurred earlier in the tree. Finally, we introduce covariate-representative tree roots (CRTRs), which select representative tree roots per covariate and provide interpretable insight into the conditions - marginal or interactive - under which each covariate has its strongest effects. In a simulation study, the unity VIM reliably identified interacting covariates without marginal effects, unlike conventional RF-based VIMs. In a large-scale real-data comparison, UFOs achieved higher discrimination and predictive accuracy than standard RFs, with comparable calibration. The CRTRs reproduced the covariates' true effect types reliably in simulated data and provided interesting insights in a real data analysis."}
{"id": "2601.07247", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07247", "abs": "https://arxiv.org/abs/2601.07247", "authors": ["Yiran Jia"], "title": "Multi-environment Invariance Learning with Missing Data", "comment": null, "summary": "Learning models that can handle distribution shifts is a key challenge in domain generalization. Invariance learning, an approach that focuses on identifying features invariant across environments, improves model generalization by capturing stable relationships, which may represent causal effects when the data distribution is encoded within a structural equation model (SEM) and satisfies modularity conditions. This has led to a growing body of work that builds on invariance learning, leveraging the inherent heterogeneity across environments to develop methods that provide causal explanations while enhancing robust prediction. However, in many practical scenarios, obtaining complete outcome data from each environment is challenging due to the high cost or complexity of data collection. This limitation in available data hinders the development of models that fully leverage environmental heterogeneity, making it crucial to address missing outcomes to improve both causal insights and robust prediction. In this work, we derive an estimator from the invariance objective under missing outcomes. We establish non-asymptotic guarantees on variable selection property and $\\ell_2$ error convergence rates, which are influenced by the proportion of missing data and the quality of imputation models across environments. We evaluate the performance of the new estimator through extensive simulations and demonstrate its application using the UCI Bike Sharing dataset to predict the count of bike rentals. The results show that despite relying on a biased imputation model, the estimator is efficient and achieves lower prediction error, provided the bias is within a reasonable range."}
{"id": "2601.07044", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07044", "abs": "https://arxiv.org/abs/2601.07044", "authors": ["Yuhao Deng", "Donglin Zeng", "Yuanjia Wang"], "title": "Semiparametric Analysis of Interval-Censored Data Subject to Inaccurate Diagnoses with A Terminal Event", "comment": "This paper has been accepted by the Annals of Applied Statistics", "summary": "Interval-censoring frequently occurs in studies of chronic diseases where disease status is inferred from intermittently collected biomarkers. Although many methods have been developed to analyze such data, they typically assume perfect disease diagnosis, which often does not hold in practice due to the inherent imperfect clinical diagnosis of cognitive functions or measurement errors of biomarkers such as cerebrospinal fluid. In this work, we introduce a semiparametric modeling framework using the Cox proportional hazards model to address interval-censored data in the presence of inaccurate disease diagnosis. Our model incorporates sensitivity and specificity of the diagnosis to account for uncertainty in whether the interval truly contains the disease onset. Furthermore, the framework accommodates scenarios involving a terminal event and when diagnosis is accurate, such as through postmortem analysis. We propose a nonparametric maximum likelihood estimation method for inference and develop an efficient EM algorithm to ensure computational feasibility. The regression coefficient estimators are shown to be asymptotically normal, achieving semiparametric efficiency bounds. We further validate our approach through extensive simulation studies and an application assessing Alzheimer's disease (AD) risk. We find that amyloid-beta is significantly associated with AD, but Tau is predictive of both AD and mortality."}
{"id": "2601.07281", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07281", "abs": "https://arxiv.org/abs/2601.07281", "authors": ["Likun Zhang", "Wei Ma"], "title": "Covariance-Driven Regression Trees: Reducing Overfitting in CART", "comment": null, "summary": "Decision trees are powerful machine learning algorithms, widely used in fields such as economics and medicine for their simplicity and interpretability. However, decision trees such as CART are prone to overfitting, especially when grown deep or the sample size is small. Conventional methods to reduce overfitting include pre-pruning and post-pruning, which constrain the growth of uninformative branches. In this paper, we propose a complementary approach by introducing a covariance-driven splitting criterion for regression trees (CovRT). This method is more robust to overfitting than the empirical risk minimization criterion used in CART, as it produces more balanced and stable splits and more effectively identifies covariates with true signals. We establish an oracle inequality of CovRT and prove that its predictive accuracy is comparable to that of CART in high-dimensional settings. We find that CovRT achieves superior prediction accuracy compared to CART in both simulations and real-world tasks."}
{"id": "2601.07094", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07094", "abs": "https://arxiv.org/abs/2601.07094", "authors": ["Jiguang Li", "Hengrui Luo"], "title": "Robust Bayesian Optimization via Tempered Posteriors", "comment": null, "summary": "Bayesian optimization (BO) iteratively fits a Gaussian process (GP) surrogate to accumulated evaluations and selects new queries via an acquisition function such as expected improvement (EI). In practice, BO often concentrates evaluations near the current incumbent, causing the surrogate to become overconfident and to understate predictive uncertainty in the region guiding subsequent decisions. We develop a robust GP-based BO via tempered posterior updates, which downweight the likelihood by a power $α\\in (0,1]$ to mitigate overconfidence under local misspecification. We establish cumulative regret bounds for tempered BO under a family of generalized improvement rules, including EI, and show that tempering yields strictly sharper worst-case regret guarantees than the standard posterior $(α=1)$, with the most favorable guarantees occurring near the classical EI choice.\n  Motivated by our theoretic findings, we propose a prequential procedure for selecting $α$ online: it decreases $α$ when realized prediction errors exceed model-implied uncertainty and returns $α$ toward one as calibration improves. Empirical results demonstrate that tempering provides a practical yet theoretically grounded tool for stabilizing BO surrogates under localized sampling."}
{"id": "2601.07325", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07325", "abs": "https://arxiv.org/abs/2601.07325", "authors": ["EL Mahdi Khribch", "Pierre Alquier"], "title": "Variational Approximations for Robust Bayesian Inference via Rho-Posteriors", "comment": "53 pages including the proofs in appendices, 16 figures", "summary": "The $ρ$-posterior framework provides universal Bayesian estimation with explicit contamination rates and optimal convergence guarantees, but has remained computationally difficult due to an optimization over reference distributions that precludes intractable posterior computation. We develop a PAC-Bayesian framework that recovers these theoretical guarantees through temperature-dependent Gibbs posteriors, deriving finite-sample oracle inequalities with explicit rates and introducing tractable variational approximations that inherit the robustness properties of exact $ρ$-posteriors. Numerical experiments demonstrate that this approach achieves theoretical contamination rates while remaining computationally feasible, providing the first practical implementation of $ρ$-posterior inference with rigorous finite-sample guarantees."}
{"id": "2601.07158", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07158", "abs": "https://arxiv.org/abs/2601.07158", "authors": ["Hisaya Okahara", "Tomoyuki Nakagawa", "Shonosuke Sugasawa"], "title": "The Bayesian Intransitive Bradley-Terry Model via Combinatorial Hodge Theory", "comment": null, "summary": "Pairwise comparison data are widely used to infer latent rankings in areas such as sports, social choice, and machine learning. The Bradley-Terry model provides a foundational probabilistic framework but inherently assumes transitive preferences, explaining all comparisons solely through subject-specific parameters. In many competitive networks, however, cycle-induced effects are intrinsic, and ignoring them can distort both estimation and uncertainty quantification. To address this limitation, we propose a Bayesian extension of the Bradley-Terry model that explicitly separates the transitive and intransitive components. The proposed Bayesian Intransitive Bradley-Terry model embeds combinatorial Hodge theory into a logistic framework, decomposing paired relationships into a gradient flow representing transitive strength and a curl flow capturing cycle-induced structure. We impose global-local shrinkage priors on the curl component, enabling data-adaptive regularization and ensuring a natural reduction to the classical Bradley-Terry model when intransitivity is absent. Posterior inference is performed using an efficient Gibbs sampler, providing scalable computation and full Bayesian uncertainty quantification. Simulation studies demonstrate improved estimation accuracy, well-calibrated uncertainty, and substantial computational advantages over existing Bayesian models for intransitivity. The proposed framework enables uncertainty-aware quantification of intransitivity at both the global and triad levels, while also characterizing cycle-induced competitive advantages among teams."}
{"id": "2601.07419", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07419", "abs": "https://arxiv.org/abs/2601.07419", "authors": ["Niklas Kormann", "Benjamin Doerr", "Johannes F. Lutzeyer"], "title": "Position: Don't be Afraid of Over-Smoothing And Over-Squashing", "comment": "Preprint. Copyright 2026 by the authors", "summary": "Over-smoothing and over-squashing have been extensively studied in the literature on Graph Neural Networks (GNNs) over the past years. We challenge this prevailing focus in GNN research, arguing that these phenomena are less critical for practical applications than assumed. We suggest that performance decreases often stem from uninformative receptive fields rather than over-smoothing. We support this position with extensive experiments on several standard benchmark datasets, demonstrating that accuracy and over-smoothing are mostly uncorrelated and that optimal model depths remain small even with mitigation techniques, thus highlighting the negligible role of over-smoothing. Similarly, we challenge that over-squashing is always detrimental in practical applications. Instead, we posit that the distribution of relevant information over the graph frequently factorises and is often localised within a small k-hop neighbourhood, questioning the necessity of jointly observing entire receptive fields or engaging in an extensive search for long-range interactions. The results of our experiments show that architectural interventions designed to mitigate over-squashing fail to yield significant performance gains. This position paper advocates for a paradigm shift in theoretical research, urging a diligent analysis of learning tasks and datasets using statistics that measure the underlying distribution of label-relevant information to better understand their localisation and factorisation."}
{"id": "2601.07202", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07202", "abs": "https://arxiv.org/abs/2601.07202", "authors": ["Kanji Goto", "Shintaro Yuki", "Kensuke Tanioka", "Hiroshi Yadohisa"], "title": "Principal component-guided sparse reduced-rank regression", "comment": null, "summary": "Reduced-rank regression estimates regression coefficients by imposing a low-rank constraint on the matrix of regression coefficients, thereby accounting for correlations among response variables. To further improve predictive accuracy and model interpretability, several regularized reduced-rank regression methods have been proposed. However, these existing methods cannot bias the regression coefficients toward the leading principal component directions while accounting for the correlation structure among explanatory variables. In addition, when the explanatory variables exhibit a group structure, the correlation structure within each group cannot be adequately incorporated. To overcome these limitations, we propose a new method that introduces pcLasso into the reduced-rank regression framework. The proposed method improves predictive accuracy by accounting for the correlation among response variables while strongly biasing the matrix of regression coefficients toward principal component directions with large variance. Furthermore, even in settings where the explanatory variables possess a group structure, the proposed method is capable of explicitly incorporating this structure into the estimation process. Finally, we illustrate the effectiveness of the proposed method through numerical simulations and real data application."}
{"id": "2601.07535", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07535", "abs": "https://arxiv.org/abs/2601.07535", "authors": ["Victor Thuot", "Sebastian Vogt", "Debarghya Ghoshdastidar", "Nicolas Verzelen"], "title": "Nonparametric Kernel Clustering with Bandit Feedback", "comment": null, "summary": "Clustering with bandit feedback refers to the problem of partitioning a set of items, where the clustering algorithm can sequentially query the items to receive noisy observations. The problem is formally posed as the task of partitioning the arms of an N-armed stochastic bandit according to their underlying distributions, grouping two arms together if and only if they share the same distribution, using samples collected sequentially and adaptively. This setting has gained attention in recent years due to its applicability in recommendation systems and crowdsourcing. Existing works on clustering with bandit feedback rely on a strong assumption that the underlying distributions are sub-Gaussian. As a consequence, the existing methods mainly cover settings with linearly-separable clusters, which has little practical relevance. We introduce a framework of ``nonparametric clustering with bandit feedback'', where the underlying arm distributions are not constrained to any parametric, and hence, it is applicable for active clustering of real-world datasets. We adopt a kernel-based approach, which allows us to reformulate the nonparametric problem as the task of clustering the arms according to their kernel mean embeddings in a reproducing kernel Hilbert space (RKHS). Building on this formulation, we introduce the KABC algorithm with theoretical correctness guarantees and analyze its sampling budget. We introduce a notion of signal-to-noise ratio for this problem that depends on the maximum mean discrepancy (MMD) between the arm distributions and on their variance in the RKHS. Our algorithm is adaptive to this unknown quantity: it does not require it as an input yet achieves instance-dependent guarantees."}
{"id": "2601.07249", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07249", "abs": "https://arxiv.org/abs/2601.07249", "authors": ["Suchismita Das", "Akul Ameya", "Cahyani Karunia Putri"], "title": "Compounded Linear Failure Rate Distribution: Properties, Simulation and Analysis", "comment": null, "summary": "This paper proposes a new extension of the linear failure rate (LFR) model to better capture real-world lifetime data. The model incorporates an additional shape parameter to increase flexibility. It helps model the minimum survival time from a set of LFR distributed variables. We define the model, derive certain statistical properties such as the mean residual life, the mean inactivity time, moments, quantile, order statistics and also discuss the results on stochastic orders of the proposed distribution. The proposed model has increasing, bathtub shaped and inverse bathtub shaped hazard rate function. We use the method of maximum likelihood estimation to estimate the unknown parameters. We conduct simulation studies to examine the behavior of the estimators. We also use three real datasets to evaluate the model, which turns out superior compared to classical alternatives."}
{"id": "2601.07640", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07640", "abs": "https://arxiv.org/abs/2601.07640", "authors": ["Mahdi Nasiri", "Johanna Kortelainen", "Simo Särkkä"], "title": "Dual-Level Models for Physics-Informed Multi-Step Time Series Forecasting", "comment": null, "summary": "This paper develops an approach for multi-step forecasting of dynamical systems by integrating probabilistic input forecasting with physics-informed output prediction. Accurate multi-step forecasting of time series systems is important for the automatic control and optimization of physical processes, enabling more precise decision-making. While mechanistic-based and data-driven machine learning (ML) approaches have been employed for time series forecasting, they face significant limitations. Incomplete knowledge of process mathematical models limits mechanistic-based direct employment, while purely data-driven ML models struggle with dynamic environments, leading to poor generalization. To address these limitations, this paper proposes a dual-level strategy for physics-informed forecasting of dynamical systems. On the first level, input variables are forecast using a hybrid method that integrates a long short-term memory (LSTM) network into probabilistic state transition models (STMs). On the second level, these stochastically predicted inputs are sequentially fed into a physics-informed neural network (PINN) to generate multi-step output predictions. The experimental results of the paper demonstrate that the hybrid input forecasting models achieve a higher log-likelihood and lower mean squared errors (MSE) compared to conventional STMs. Furthermore, the PINNs driven by the input forecasting models outperform their purely data-driven counterparts in terms of MSE and log-likelihood, exhibiting stronger generalization and forecasting performance across multiple test cases."}
{"id": "2601.07267", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07267", "abs": "https://arxiv.org/abs/2601.07267", "authors": ["Shuli Chen", "Jie Hu", "Zhichao Jiang"], "title": "Connections as treatment: causal inference with edge interventions in networks", "comment": null, "summary": "Causal inference has traditionally focused on interventions at the unit level. In many applications, however, the central question concerns the causal effects of connections between units, such as transportation links, social relationships, or collaborative ties. We develop a causal framework for edge interventions in networks, where treatments correspond to the presence or absence of edges. Our framework defines causal estimands under stochastic interventions on the network structure and introduces an inverse probability weighting estimator under an unconfoundedness assumption on edge assignment. We estimate edge probabilities using exponential random graph models, a widely used class of network models. We establish consistency and asymptotic normality of the proposed estimator. Finally, we apply our methodology to China's transportation network to estimate the causal impact of railroad connections on regional economic development."}
{"id": "2601.06296", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.06296", "abs": "https://arxiv.org/abs/2601.06296", "authors": ["Man Jin", "Yixin Fang"], "title": "A Framework for Estimating Restricted Mean Survival Time Difference using Pseudo-observations", "comment": null, "summary": "A targeted learning (TL) framework is developed to estimate the difference in the restricted mean survival time (RMST) for a clinical trial with time-to-event outcomes. The approach starts by defining the target estimand as the RMST difference between investigational and control treatments. Next, an efficient estimation method is introduced: a targeted minimum loss estimator (TMLE) utilizing pseudo-observations. Moreover, a version of the copy reference (CR) approach is developed to perform a sensitivity analysis for right-censoring. The proposed TL framework is demonstrated using a real data application."}
{"id": "2601.07282", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.07282", "abs": "https://arxiv.org/abs/2601.07282", "authors": ["Junjun Lang", "Qiong Zhang", "Yukun Liu"], "title": "Minimum Wasserstein distance estimator under covariate shift: closed-form, super-efficiency and irregularity", "comment": null, "summary": "Covariate shift arises when covariate distributions differ between source and target populations while the conditional distribution of the response remains invariant, and it underlies problems in missing data and causal inference. We propose a minimum Wasserstein distance estimation framework for inference under covariate shift that avoids explicit modeling of outcome regressions or importance weights. The resulting W-estimator admits a closed-form expression and is numerically equivalent to the classical 1-nearest neighbor estimator, yielding a new optimal transport interpretation of nearest neighbor methods. We establish root-$n$ asymptotic normality and show that the estimator is not asymptotically linear, leading to super-efficiency relative to the semiparametric efficient estimator under covariate shift in certain regimes, and uniformly in missing data problems. Numerical simulations, along with an analysis of a rainfall dataset, underscore the exceptional performance of our W-estimator."}
{"id": "2601.07282", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.07282", "abs": "https://arxiv.org/abs/2601.07282", "authors": ["Junjun Lang", "Qiong Zhang", "Yukun Liu"], "title": "Minimum Wasserstein distance estimator under covariate shift: closed-form, super-efficiency and irregularity", "comment": null, "summary": "Covariate shift arises when covariate distributions differ between source and target populations while the conditional distribution of the response remains invariant, and it underlies problems in missing data and causal inference. We propose a minimum Wasserstein distance estimation framework for inference under covariate shift that avoids explicit modeling of outcome regressions or importance weights. The resulting W-estimator admits a closed-form expression and is numerically equivalent to the classical 1-nearest neighbor estimator, yielding a new optimal transport interpretation of nearest neighbor methods. We establish root-$n$ asymptotic normality and show that the estimator is not asymptotically linear, leading to super-efficiency relative to the semiparametric efficient estimator under covariate shift in certain regimes, and uniformly in missing data problems. Numerical simulations, along with an analysis of a rainfall dataset, underscore the exceptional performance of our W-estimator."}
{"id": "2601.07369", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07369", "abs": "https://arxiv.org/abs/2601.07369", "authors": ["Roberto Fontana", "Elisa Perrone", "Fabio Rapallo"], "title": "Characterization of multi-way binary tables with uniform margins and fixed correlations", "comment": "21 pages", "summary": "In many applications involving binary variables, only pairwise dependence measures, such as correlations, are available. However, for multi-way tables involving more than two variables, these quantities do not uniquely determine the joint distribution, but instead define a family of admissible distributions that share the same pairwise dependence while potentially differing in higher-order interactions. In this paper, we introduce a geometric framework to describe the entire feasible set of such joint distributions with uniform margins. We show that this admissible set forms a convex polytope, analyze its symmetry properties, and characterize its extreme rays. These extremal distributions provide fundamental insights into how higher-order dependence structures may vary while preserving the prescribed pairwise information. Unlike traditional methods for table generation, which return a single table, our framework makes it possible to explore and understand the full admissible space of dependence structures, enabling more flexible choices for modeling and simulation. We illustrate the usefulness of our theoretical results through examples and a real case study on rater agreement."}
{"id": "2601.07400", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.07400", "abs": "https://arxiv.org/abs/2601.07400", "authors": ["Wai Leong Ng", "Xinyi Tang", "Mun Lau Cheung", "Jiacheng Gao", "Chun Yip Yau", "Holger Dette"], "title": "Inference for Multiple Change-points in Piecewise Locally Stationary Time Series", "comment": null, "summary": "Change-point detection and locally stationary time series modeling are two major approaches for the analysis of non-stationary data. The former aims to identify stationary phases by detecting abrupt changes in the dynamics of a time series model, while the latter employs (locally) time-varying models to describe smooth changes in dependence structure of a time series. However, in some applications, abrupt and smooth changes can co-exist, and neither of the two approaches alone can model the data adequately. In this paper, we propose a novel likelihood-based procedure for the inference of multiple change-points in locally stationary time series. In contrast to traditional change-point analysis where an abrupt change occurs in a real-valued parameter, a change in locally stationary time series occurs in a parameter curve, and can be classified as a jump or a kink depending on whether the curve is discontinuous or not. We show that the proposed method can consistently estimate the number, locations, and the types of change-points. Two different asymptotic distributions corresponding respectively to jump and kink estimators are also established.Extensive simulation studies and a real data application to financial time series are provided."}
{"id": "2601.07490", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07490", "abs": "https://arxiv.org/abs/2601.07490", "authors": ["Miguel Martinez Herrera", "Felix Cheysson"], "title": "Ridge-penalised spectral least-squares estimation for point processes", "comment": null, "summary": "Penalised estimation methods for point processes usually rely on a large amount of independent repetitions for cross-validation purposes. However, in the case of a single realisation of the process, existing cross-validation methods may be impractical depending on the chosen model. To overcome this issue, this paper presents a Ridge-penalised spectral least-squares estimation method for second-order stationary point processes. This is achieved through two novel approaches: a p-thinning-based cross-validation method to tune the penalisation parameter, relying on the spectral representation of the process; and the introduction of a spectral least-squares contrast based around the asymptotic properties of the periodogram of the sample. The proposed method is then illustrated by a simulation study on linear Hawkes processes in the context of parametric estimation, highlighting its performances against more traditional approaches, specifically when working with short observation windows."}
{"id": "2601.07539", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07539", "abs": "https://arxiv.org/abs/2601.07539", "authors": ["Ryo Okano", "Daisuke Kurisu"], "title": "Functional Synthetic Control Methods for Metric Space-Valued Outcomes", "comment": null, "summary": "The synthetic control method (SCM) is a widely used tool for evaluating causal effects of policy changes in panel data settings. Recent studies have extended its framework to accommodate complex outcomes that take values in metric spaces, such as distributions, functions, networks, covariance matrices, and compositional data. However, due to the lack of linear structure in general metric spaces, theoretical guarantees for estimation and inference within these extended frameworks remain underdeveloped. In this study, we propose the functional synthetic control (FSC) method as an extension of the SCM for metric space-valued outcomes. To address challenges arising from the nonlinearlity of metric spaces, we leverage isometric embeddings into Hilbert spaces. Building on this approach, we develop the FSC and augmented FSC estimators for counterfactual outcomes, with the latter being a bias-corrected version of the former. We then derive their finite-sample error bounds to establish theoretical guarantees for estimation, and construct prediction sets based on these estimators to conduct inference on causal effects. We demonstrate the usefulness of the proposed framework through simulation studies and three empirical applications."}
{"id": "2601.07609", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07609", "abs": "https://arxiv.org/abs/2601.07609", "authors": ["Marco Alfo'", "Robrto Rocci"], "title": "Omitted covariates bias and finite mixtures of regression models for longitudinal responses", "comment": null, "summary": "Individual-specific, time-constant, random effects are often used to model dependence and/or to account for omitted covariates in regression models for longitudinal responses. Longitudinal studies have known a huge and widespread use in the last few years as they allow to distinguish between so-called age and cohort effects; these relate to differences that can be observed at the beginning of the study and stay persistent through time, and changes in the response that are due to the temporal dynamics in the observed covariates. While there is a clear and general agreement on this purpose, the random effect approach has been frequently criticized for not being robust to the presence of correlation between the observed (i.e. covariates) and the unobserved (i.e. random effects) heterogeneity. Starting from the so-called correlated effect approach, we argue that the random effect approach may be parametrized to account for potential correlation between observables and unobservables. Specifically, when the random effect distribution is estimated non-parametrically using a discrete distribution on finite number of locations, a further, more general, solution is developed. This is illustrated via a large scale simulation study and the analysis of a benchmark dataset."}
{"id": "2601.07693", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07693", "abs": "https://arxiv.org/abs/2601.07693", "authors": ["Joseph Lam", "Mario Cortina-Borja", "Rob Aldridge", "Ruth Blackburn", "Katie Harron"], "title": "Cluster-based name embeddings reduce ethnic disparities in record linkage quality under realistic name corruption: evidence from the North Carolina Voter Registry", "comment": null, "summary": "Differential ethnic-based record linkage errors can bias epidemiologic estimates. Prior evidence often conflates heterogeneity in error mechanisms with unequal exposure to error. Using snapshots of the North Carolina Voter Registry (Oct 2011-Oct 2022), we derived empirical name-discrepancy profiles to parameterise realistic corruptions. From an Oct 2022 extract (n=848,566), we generated five replicate corrupted datasets under three settings that separately varied mechanism heterogeneity and exposure inequality, and linked records back to originals using unadjusted Jaro-Winkler, Term Frequency (TF)-adjusted Jaro-Winkler, and a cluster-based forename-embedding comparator combined with TF-adjusted surname comparison. We evaluated false match rate (FMR), missed match rate (MMR) and white-centric disparities. At a fixed MMR near 0.20, overall error rates and ethnic disparities diverged substantially by model under disproportionate exposure to corruption. Term-frequency (TF)-adjusted Jaro-Winkler achieved very low overall FMR (0.55% (95% CI 0.54-0.57)) at overall MMR 20.34% (20.30-20.39), but large white-centric under-linkage disparities persisted: Hispanic voters had 36.3% (36.1-36.6) and Non-Hispanic Black voters 8.6% (8.6-8.7) higher FMRs compared to Non-Hispanic White groups. Relative to unadjusted string similarity, TF adjustment reduced these disparities (Hispanic: +60.4% (60.1-60.7) to +36.3%; Black: +13.1% (13.0-13.2) to +8.6%). The cluster-based forename-embedding model reduced missed-match disparities further (Hispanic: +10.2% (9.8-10.3); Black: +0.6% (0.4-0.7)), but at a cost of increasing overall FMR (4.28% (4.22-4.35)) at the same threshold. Unequal exposure to identifier error drove substantially larger disparities than mechanism heterogeneity alone; cluster-based embeddings markedly narrowed under-linkage disparities beyond TF adjustment."}
{"id": "2601.06782", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.06782", "abs": "https://arxiv.org/abs/2601.06782", "authors": ["Sungtaek Son", "Eardi Lila", "Kwun Chuen Gary Chan"], "title": "Dimension-reduced outcome-weighted learning for estimating individualized treatment regimes in observational studies", "comment": "54 pages, 9 figures", "summary": "Individualized treatment regimes (ITRs) aim to improve clinical outcomes by assigning treatment based on patient-specific characteristics. However, existing methods often struggle with high-dimensional covariates, limiting accuracy, interpretability, and real-world applicability. We propose a novel sufficient dimension reduction approach that directly targets the contrast between potential outcomes and identifies a low-dimensional subspace of the covariates capturing treatment effect heterogeneity. This reduced representation enables more accurate estimation of optimal ITRs through outcome-weighted learning. To accommodate observational data, our method incorporates kernel-based covariate balancing, allowing treatment assignment to depend on the full covariate set and avoiding the restrictive assumption that the subspace sufficient for modeling heterogeneous treatment effects is also sufficient for confounding adjustment. We show that the proposed method achieves universal consistency, i.e., its risk converges to the Bayes risk, under mild regularity conditions. We demonstrate its finite sample performance through simulations and an analysis of intensive care unit sepsis patient data to determine who should receive transthoracic echocardiography."}
{"id": "2601.07247", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07247", "abs": "https://arxiv.org/abs/2601.07247", "authors": ["Yiran Jia"], "title": "Multi-environment Invariance Learning with Missing Data", "comment": null, "summary": "Learning models that can handle distribution shifts is a key challenge in domain generalization. Invariance learning, an approach that focuses on identifying features invariant across environments, improves model generalization by capturing stable relationships, which may represent causal effects when the data distribution is encoded within a structural equation model (SEM) and satisfies modularity conditions. This has led to a growing body of work that builds on invariance learning, leveraging the inherent heterogeneity across environments to develop methods that provide causal explanations while enhancing robust prediction. However, in many practical scenarios, obtaining complete outcome data from each environment is challenging due to the high cost or complexity of data collection. This limitation in available data hinders the development of models that fully leverage environmental heterogeneity, making it crucial to address missing outcomes to improve both causal insights and robust prediction. In this work, we derive an estimator from the invariance objective under missing outcomes. We establish non-asymptotic guarantees on variable selection property and $\\ell_2$ error convergence rates, which are influenced by the proportion of missing data and the quality of imputation models across environments. We evaluate the performance of the new estimator through extensive simulations and demonstrate its application using the UCI Bike Sharing dataset to predict the count of bike rentals. The results show that despite relying on a biased imputation model, the estimator is efficient and achieves lower prediction error, provided the bias is within a reasonable range."}
{"id": "2601.07281", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.07281", "abs": "https://arxiv.org/abs/2601.07281", "authors": ["Likun Zhang", "Wei Ma"], "title": "Covariance-Driven Regression Trees: Reducing Overfitting in CART", "comment": null, "summary": "Decision trees are powerful machine learning algorithms, widely used in fields such as economics and medicine for their simplicity and interpretability. However, decision trees such as CART are prone to overfitting, especially when grown deep or the sample size is small. Conventional methods to reduce overfitting include pre-pruning and post-pruning, which constrain the growth of uninformative branches. In this paper, we propose a complementary approach by introducing a covariance-driven splitting criterion for regression trees (CovRT). This method is more robust to overfitting than the empirical risk minimization criterion used in CART, as it produces more balanced and stable splits and more effectively identifies covariates with true signals. We establish an oracle inequality of CovRT and prove that its predictive accuracy is comparable to that of CART in high-dimensional settings. We find that CovRT achieves superior prediction accuracy compared to CART in both simulations and real-world tasks."}
