<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 15]
- [stat.AP](#stat.AP) [Total: 4]
- [stat.ML](#stat.ML) [Total: 13]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Computation for Epidemic Prediction with Graph Neural Network by Model Combination](https://arxiv.org/abs/2511.15469)
*Xiangxin Kong,Hang Wang,Yutong Li,Yanghao Chen,Zudi Lu*

Main category: stat.CO

TL;DR: 提出了一种新颖的混合图神经网络模型EpiHybridGNN，结合了EpiGNN和ColaGNN的优势，用于时空流行病预测。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络模型在流行病时空预测方面各有优势但存在不足，需要整合不同模型的优点来构建更全面和鲁棒的预测方法。

Method: 通过整合EpiGNN的传播风险编码模块、区域感知图学习器(RAGL)以及ColaGNN的跨位置注意力机制、多尺度扩张卷积和动态图结构，构建混合模型。

Result: 多个真实数据实验验证EpiHybridGNN在流行病预测方面显著优于EpiGNN和ColaGNN。

Conclusion: EpiHybridGNN成功整合了两种模型的优势，提供了更全面和鲁棒的时空流行病传播预测能力。

Abstract: Modelling epidemic events such as COVID-19 cases in both time and space dimensions is an important but challenging task. Building on in-depth review and assessment of two popular graph neural network (GNN)-based regional epidemic forecasting models of \textbf{EpiGNN} and \textbf{ColaGNN}, we propose a novel hybrid graph neural network model, \textbf{EpiHybridGNN}, which integrates the strengths of both EpiGNN and \textbf{ColaGNN}. In the EpiGNN, through its transmission risk encoding module and Region-Aware Graph Learner (RAGL), both multi-scale convolutions and Graph Convolutional Networks (GCNs) are combined, aiming to effectively capture spatio-temporal propagation dynamics between regions and support the integration of external resources to enhance forecasting performance. While, in the ColaGNN, a cross-location attention mechanism, multi-scale dilated convolutions, and graph message passing are utilized to address the challenges of long-term forecasting through dynamic graph structures and spatio-temporal feature fusion. Both enjoy respective advantages but also share mutual shortcomings. Our EpiHybridGNN is therefore designed to combine the advantages of both EpiGNN, in its risk encoding and RAGL, and ColaGNN, in its long-term forecasting capabilities and dynamic attention mechanisms. This helps to form a more comprehensive and robust prediction of spatio-temporal epidemic propagation. The computational architecture, core formulas and their interpretations of our proposed EpiHybridGNN are provided. Multiple numerical real data experiments validate that our EpiHybridGNN significantly outperforms both EpiGNN and ColaGNN in epidemic forecasting with comprehensive insights and references offered.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [2] [Extrinsic Total-Variance and Coplanarity via Oriented and Classical Projective Shape Analysis](https://arxiv.org/abs/2511.14815)
*Musab Alamoudi,Robert L. Paige,Vic Patrangenaru*

Main category: stat.ME

TL;DR: 本文介绍了面向投影形状的外在总方差指数，建立了外在Fréchet框架，并在平面五元组情况下给出了样本总外在方差的闭式解。通过Sope Creek石头数据集分析，验证了数据共面性。


<details>
  <summary>Details</summary>
Motivation: 经典投影形状方法在表示地标配置时丢失了方向信息，面向投影形状通过使用球面乘积表示恢复了这一信息，需要建立相应的方差分析框架。

Method: 引入了面向投影形状的外在总方差指数，构建了外在Fréchet框架，在平面五元组情况下推导了样本总外在方差的闭式表达式。

Result: 在Sope Creek石头数据集分析中，使用面向投影框架和留二诊断方法，在5%显著性水平上识别出共面性，验证了数据共面性结果。

Conclusion: 面向投影形状的外在方差分析框架有效恢复了方向信息，为投影形状分析提供了更完整的几何工具，能够准确识别数据的共面性特征。

Abstract: Projective shape analysis provides a geometric framework for studying digital images acquired by pinhole digital cameras. In the classical projective shape (PS) method, landmark configurations are represented in $(\RP^2)^{k-4}$, where $k$ is the number of landmarks observed. This representation is invariant under the action of the full projective group on this space and is sign-blind, so opposite directions in $\R^{3}$ determine the same projective point and front--back orientation of a surface is not recorded. Oriented projective shape ($\OPS$) restores this information by working on a product of $k-4$ spheres $\SP^2$ instead of projective space and restricting attention to the orientation-preserving subgroup of projective transformations. In this paper we introduce an extrinsic total-variance index for OPS, resulting in the extrinsic Fréchet framework for the m dimensional case from the inclusion $\jdir:(\SP^m)^q\hookrightarrow(\R^{m+1})^q,q=k-m-2$. In the planar pentad case ($m=2$, $q=1$) the sample total extrinsic variance has a closed form in terms of the mean of a random sample of size $n$ of oriented projective coordinates in $S^2$. As an illustration, using an oriented projective frame, we analyze the Sope Creek stone data set, a benchmark and nearly planar example with $41$ images and $5$ landmarks. Using a delta-method applied to a large sample and a generalized Slutsky theorem argument, for an OPS leave-two-out diagnostic, one identifies coplanarity at the $5\%$ level, confirming the concentrated data coplanarity PS result in Patrangenaru(2001)\cite{Patrangenaru2001}.

</details>


### [3] [An Estimand-Focused Approach for AUC Estimation, Generalization, and Comparison: From Non-representative Samples to Target Population](https://arxiv.org/abs/2511.14992)
*Jiajun Liu,Guangcai Mao,Xiaofei Wang*

Main category: stat.ME

TL;DR: 开发了一个在协变量偏移下进行有效AUC估计和基准测试的框架，通过校准加权方法扩展了AUC的U统计量估计，并建立了具有双稳健性的估计器。


<details>
  <summary>Details</summary>
Motivation: 当验证队列与目标人群存在差异时，传统的AUC估计会产生误导，这种协变量偏移在偏倚或非随机抽样中常见，会扭曲AUC估计，阻碍AUC的泛化和跨研究比较。

Method: 利用因果推断中的平衡思想，将校准加权扩展到AUC估计的U统计量框架中，引入了一系列估计器，这些估计器可以适应汇总级和患者级信息，某些规格的估计器具有双稳健性。

Result: 建立了渐近性质，并在综合模拟中研究了它们在不同协变量偏移严重程度和校准选择下的性能。在POWER试验中展示了实用性，评估了基线爬楼梯功率(SCP)对晚期非小细胞肺癌患者6个月生存期的预测能力。

Conclusion: 该研究提供了一个原则性工具包，用于将生物标志物AUC锚定到临床相关的目标人群，并在存在分布差异的情况下公平地比较不同研究中的AUC。

Abstract: The area under the ROC curve (AUC) is the standard measure of a biomarker's discriminatory accuracy; however, naive AUC estimates can be misleading when validation cohorts differ from the intended target population. Such covariate shifts commonly arise under biased or non-random sampling, distorting AUC estimations and thus impeding both generalization and cross-study comparison of AUC. We develop an estimand-focused framework for valid AUC estimation and benchmarking under covariate shift. Leveraging balancing ideas from causal inference, we extend calibration weighting to the U-statistic framework for AUC estimation and introduce a family of estimators that accommodate both summary-level and patient-level information; in certain specifications, some of these estimators attain double robustness. Furthermore, we establish asymptotic properties and study their performances across a spectrum of covariate shift severities and calibration choices in comprehensive simulations. Finally, we demonstrate practical utility in the POWER trials by evaluating how baseline stair-climb power (SCP) predicts 6-month survival among advanced non-small-cell lung cancer (NSCLC) patients. Together, the results provide a principled toolkit for anchoring biomarker AUCs to clinically relevant target populations and for comparing them fairly across studies despite distributional differences.

</details>


### [4] [The Sequential Nature of Science: Quantifying Learning from a Sequence of Studies](https://arxiv.org/abs/2511.14996)
*Jonas M. Mikhaeil,Donald P. Green,David Blei*

Main category: stat.ME

TL;DR: 提出SMART方法量化研究在发表时的影响力，相比传统元分析更能捕捉新研究对先前信念的质疑作用，即使小研究也能在发表时产生重要影响。


<details>
  <summary>Details</summary>
Motivation: 科学进步本质上是顺序性的，集体知识随着新研究进入文献而更新。传统元分析无法充分捕捉新研究可能对先前信念产生质疑的情况。

Method: 提出顺序元分析研究轨迹(SMART)方法，量化每个研究在进入文献时的影响力。通过重新分析心理学和劳动经济学的两个元分析数据集进行对比验证。

Result: SMART能够识别传统元分析可能忽略的方法论创新重要性，即使小型研究也能在发表时显著影响集体不确定性。

Conclusion: 顺序学习的形式化强调了方法论创新的重要性，这在传统元分析中可能被忽视。

Abstract: Scientific progress is inherently sequential: collective knowledge is updated as new studies enter the literature. We propose the sequential meta-analysis research trace (SMART), which quantifies the influence of each study at the time it enters the literature. In contrast to classical meta-analysis, our method can capture how new studies may cast doubt on previously held beliefs, increasing collective uncertainty. For example, a new study may present a methodological critique of prior work and propose a superior method. Even small studies, which may not materially affect a retrospective meta-analysis, can be influential at the time they appeared. To contrast SMART with classical meta-analysis, we re-analyze two meta-analysis datasets, from psychology and labor economics. One assembles studies using a single methodology; the other contains studies that predate or follow an important methodological innovation. Our formalization of sequential learning highlights the importance of methodological innovation that might otherwise be overlooked by classical meta-analysis.

</details>


### [5] [Variance-reduced extreme value index estimators using control variates in a semi-supervised setting](https://arxiv.org/abs/2511.15561)
*Louison Bocquet-Nouaille,Jérôme Morio,Benjamin Bobbia*

Main category: stat.ME

TL;DR: 提出基于控制变量的迁移学习方法，在半监督框架下结合少量配对的目标-源观测数据和大量未配对的源数据，通过近似控制变量技术降低极端值指数估计的方差。


<details>
  <summary>Details</summary>
Motivation: 极端值指数估计因仅依赖少量极端观测而存在高方差问题，需要开发能够有效降低方差的方法。

Method: 将Hill估计器表示为均值比形式，对分子和分母分别应用近似控制变量技术，使用联合优化的系数保证方差降低且不引入偏差。

Result: 理论分析和模拟显示，转移Hill估计器的渐近相对方差降低与目标-源变量间的尾部依赖成正比，与它们的极端值指数值无关。

Conclusion: 即使目标分布和源分布的尾部厚重程度不同，也能实现显著的方差降低，该方法可扩展到其他以均值比形式表示的极端值指数估计器。

Abstract: The estimation of the Extreme Value Index (EVI) is fundamental in extreme value analysis but suffers from high variance due to reliance on only a few extreme observations. We propose a control variates based transfer learning approach in a semi-supervised framework, where a small set of coupled target and source observations is combined with abundant unpaired source data. By expressing the Hill estimator of the target EVI as a ratio of means, we apply approximate control variates to both numerator and denominator, with jointly optimized coefficients that guarantee variance reduction without introducing bias. We show theoretically and through simulations that the asymptotic relative variance reduction of the transferred Hill estimator is proportional to the tail dependence between the target and source variables and independent of their EVI values. Thus, substantial variance reduction can be achieved even without similarity in tail heaviness of the target and source distributions. The proposed approach can be extended to other EVI estimators expressed with ratio of means, as demonstrated on the moment estimator. The practical value of the proposed method is illustrated on multi-fidelity water surge and ice accretion datasets.

</details>


### [6] [Hazard-Based Targeted Maximum Likelihood Estimation for Survival in Resampling Designs](https://arxiv.org/abs/2511.15045)
*Kirsten E. Landsiedel,Rachael V. Phillips,Maya L. Petersen,Mark J. van der Laan*

Main category: stat.ME

TL;DR: 提出了一种用于重抽样设计的靶向最大似然估计器(TMLE)，通过利用基线和纵向协变量来提高生存估计的效率，相比常用的加权Kaplan-Meier估计器可减少高达55%的方差。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的环境中，HIV患者的高失访率导致仅考虑观察到的死亡时会低估死亡率。现有的重抽样设计生存估计器未能利用在重复临床访视中收集的协变量信息，而这些信息对生存具有高度预测性。

Method: 开发了两种TMLE方法：(1)针对所有参与者固定随访时间的完全有效TMLE；(2)考虑不同随访时间的IPCW-TMLE，通过对有足够随访时间的患者进行分层来评估生存。

Result: 在模拟中，TMLE相比常用的加权Kaplan-Meier估计器减少了高达55%的方差，同时保持了名义置信区间覆盖率。

Conclusion: 该TMLE为重抽样设计中的生存估计提供了稳健且资源高效的框架，有望改善HIV研究中的生存评估。

Abstract: Survival is a key metric for evaluating standards of care for people living with HIV. In resource-limited settings, high rates of loss to follow-up (LTFU) often result in underestimation of mortality when only observed deaths are considered. Resampling, which tracks a subset of LTFU patients to ascertain their outcomes, mitigates bias and improves survival estimates. However, common estimators for survival in resampling designs, such as weighted Kaplan-Meier (KM), fail to leverage covariate information collected during repeated clinic visits, even though this information is highly predictive of survival. We propose a Targeted Maximum Likelihood Estimator (TMLE) for survival in resampling designs, which addresses these limitations by leveraging baseline and longitudinal covariates to achieve greater efficiency. Our TMLE is a plug-in estimator and is robust to misspecification of the initial model for the conditional hazard of death, guaranteeing consistency of our estimator due to known resampling probabilities. We present: (1) a fully efficient TMLE for data from resampling studies with fixed follow-up time for all participants and (2) an inverse probability of censoring weighted (IPCW) TMLE that accounts for varied follow-up times by stratifying on patients with sufficient follow-up to evaluate survival. This IPCW-TMLE can be made highly efficient through nonparametric or targeted estimation of the follow-up censoring mechanism. In simulations, our TMLE reduced variance by up to 55% compared with the commonly used weighted KM estimator while preserving nominal confidence interval coverage. These findings demonstrate the potential of our TMLE to improve survival estimation in resampling designs, offering a robust and resource-efficient framework for HIV research. Keywords: Resampling designs, Survival analysis, Targeted Maximum Likelihood Estimation, Inverse probability weighting

</details>


### [7] [Classification Trees with Valid Inference via the Exponential Mechanism](https://arxiv.org/abs/2511.15068)
*Soham Bakshi,Snigdha Panigrahi*

Main category: stat.ME

TL;DR: 提出了一种概率决策树拟合方法，通过指数机制选择分裂点，支持对非线性拟合进行有效推断，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 决策树虽然广泛用于非线性建模，但对其拟合结果进行统计推断的方法仍然缺乏。标准决策树算法采用贪婪分裂策略，无法提供有效的统计推断。

Method: 用概率方法替代贪婪分裂策略，通过指数机制定义分裂点的采样概率，温度参数控制与确定性选择的偏离程度。从指数机制的采样概率直接生成枢轴统计量。

Result: 该方法在高温度下能近似标准算法拟合效果，同时提供有效的统计推断能力，相比数据分割方法在不牺牲预测准确性的前提下提供更强大的推断。

Conclusion: 提出的概率决策树方法既能保持预测性能，又能实现有效的统计推断，解决了标准决策树算法无法进行可靠推断的问题。

Abstract: Decision trees are widely used for non-linear modeling, as they capture interactions between predictors while producing inherently interpretable models. Despite their popularity, performing inference on the non-linear fit remains largely unaddressed. This paper focuses on classification trees and makes two key contributions. First, we introduce a novel tree-fitting method that replaces the greedy splitting of the predictor space in standard tree algorithms with a probabilistic approach. Each split in our approach is selected according to sampling probabilities defined by an exponential mechanism, with a temperature parameter controlling its deviation from the deterministic choice given data. Second, while our approach can fit a tree that, with high probability, approximates the fit produced by standard tree algorithms at high temperatures, it is not merely predictive- unlike standard algorithms, it enables valid inference by taking into account the highly adaptive tree structure. Our method produces pivots directly from the sampling probabilities in the exponential mechanism. In theory, our pivots allow asymptotically valid inference on the parameters in the predictive fit, and in practice, our method delivers powerful inference without sacrificing predictive accuracy, in contrast to data splitting methods.

</details>


### [8] [Individualized Prediction Bands in Causal Inference with Continuous Treatments](https://arxiv.org/abs/2511.15075)
*Max Sampson,Kung-Sik Chan*

Main category: stat.ME

TL;DR: 本文提出了一种个体化预测带（IPB）方法，将连续治疗的因果推断视为协变量偏移问题，利用加权保形预测为个体剂量-反应曲线提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 个体化治疗对于优化决策和治疗分配至关重要，特别是在个性化医学中。现有方法主要关注条件均值和条件中位数估计，但无法提供完整的个体剂量-反应曲线不确定性信息。

Method: 将连续治疗的因果推断视为协变量偏移问题，利用现有的加权保形预测方法，结合分位数和点估计来计算个体剂量-反应曲线的不确定性量化。

Result: 通过模拟和真实数据分析验证了IPB方法的有效性，展示了持续吸烟对选定个体造成的额外医疗支出。

Conclusion: IPB方法填补了个体剂量-反应不确定性量化文献的空白，为个性化医学提供了有效的解决方案。

Abstract: Individualized treatments are crucial for optimal decision making and treatment allocation, specifically in personalized medicine based on the estimation of an individual's dose-response curve across a continuum of treatment levels, e.g., drug dosage. Current works focus on conditional mean and median estimates, which are useful but do not provide the full picture. We propose viewing causal inference with a continuous treatment as a covariate shift. This allows us to leverage existing weighted conformal prediction methods with both quantile and point estimates to compute individualized uncertainty quantification for dose-response curves. Our method, individualized prediction bands (IPB), is demonstrated via simulations and a real data analysis, which demonstrates the additional medical expenditure caused by continued smoking for selected individuals. The results demonstrate that IPB provides an effective solution to a gap in individual dose-response uncertainty quantification literature.

</details>


### [9] [Debiasing hazard-based, time-varying vaccine effects using vaccine-irrelevant infections: An observational extension of a pivotal Phase 3 COVID-19 vaccine efficacy trial](https://arxiv.org/abs/2511.15099)
*Ethan Ashby,Dean Follmann,Holly Janes,Peter B. Gilbert,Ting Ye,Lindsey R. Baden,Hana M. El Sahly,Bo Zhang*

Main category: stat.ME

TL;DR: 利用疫苗无关感染来识别存在未测量混杂和选择偏倚时的基于风险的时变疫苗有效性，开发了筛分和有效影响曲线估计器，并在COVID-19疫苗研究中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统Cox回归获得的时变疫苗有效性估计容易受到隐藏偏倚的影响，需要一种能够处理未测量混杂和选择偏倚的方法来准确评估疫苗效果。

Method: 利用疫苗无关感染作为阴性对照，开发筛分和有效影响曲线估计器，并讨论施加单调形状约束和估计针对多种变体的疫苗有效性。

Result: 在Moderna mRNA-1273 COVID-19疫苗的观察性加强针阶段研究中，考虑疫苗无关急性呼吸道感染后，发现该加强针对Omicron COVID-19比Cox回归建议的更有效和持久。

Conclusion: 该方法为在随机和非随机研究中使用阴性对照来减轻基于风险的时变治疗效果偏倚提供了一种有效途径。

Abstract: Understanding how vaccine effectiveness (VE) changes over time can provide evidence-based guidance for public health decision making. While commonly reported by practitioners, time-varying VE estimates obtained using Cox regression are vul- nerable to hidden biases. To address these limitations, we describe how to leverage vaccine-irrelevant infections to identify hazard-based, time-varying VE in the pres- ence of unmeasured confounding and selection bias. We articulate assumptions under which our approach identifies a causal effect of an intervention deferring vaccination and interaction with the community in which infections circulate. We develop sieve and efficient influence curve-based estimators and discuss imposing monotone shape constraints and estimating VE against multiple variants. As a case study, we examine the observational booster phase of the Coronavirus Vaccine Efficacy (COVE) trial of the Moderna mRNA-1273 COVID-19 vaccine which used symptom-triggered multi- plex PCR testing to identify acute respiratory illnesses (ARIs) caused by SARS-CoV-2 and 20 off-target pathogens previously identified as compelling negative controls for COVID-19. Accounting for vaccine-irrelevant ARIs supported that the mRNA-1273 booster was more effective and durable against Omicron COVID-19 than suggested by Cox regression. Our work offers an approach to mitigate bias in hazard-based, time- varying treatment effects in randomized and non-randomized studies using negative controls.

</details>


### [10] [Robust outlier-adjusted mean-shift estimation of state-space models](https://arxiv.org/abs/2511.15155)
*Rajan Shankar,Ines Wilms,Jakob Raymaekers,Garth Tarr*

Main category: stat.ME

TL;DR: ROAMS是一种用于状态空间模型的稳健估计方法，通过引入移位参数来减轻异常值的影响，实现自动异常检测和模型参数的同时估计。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型对高斯误差假设的依赖使其对异常值高度敏感，需要一种能够减轻异常值影响的稳健估计方法。

Method: 在状态空间模型的观测方程中为每个时间点引入移位参数，通过惩罚项自动检测异常时间点，同时估计模型参数。

Result: 在模拟数据和真实动物位置追踪数据上的应用表明，ROAMS比经典方法和其他基准方法能产生更可靠的参数估计。

Conclusion: ROAMS提供了改进的稳健性和实用的诊断工具，包括用于选择调优参数的BIC曲线和可视化异常值结构，使其在处理受污染时间序列数据时具有广泛应用价值。

Abstract: State-space models (SSMs) provide a flexible framework for modelling time series data, but their reliance on Gaussian error assumptions makes them highly sensitive to outliers. We propose a robust estimation method, ROAMS, that mitigates the influence of additive outliers by introducing shift parameters at each timepoint in the observation equation of the SSM. These parameters allow the model to attribute non-zero shifts to outliers while leaving clean observations unaffected. ROAMS then enables automatic outlier detection, through the addition of a penalty term on the number of flagged outlying timepoints in the objective function, and simultaneous estimation of model parameters. We apply the method to robustly estimate SSMs on both simulated data and real-world animal location-tracking data, demonstrating its ability to produce more reliable parameter estimates than classical methods and other benchmark methods. In addition to improved robustness, ROAMS offers practical diagnostic tools, including BIC curves for selecting tuning parameters and visualising outlier structure. These features make our approach broadly useful for researchers and practitioners working with contaminated time series data.

</details>


### [11] [Testing relevant difference in high-dimensional linear regression with applications to detect transferability](https://arxiv.org/abs/2511.15236)
*Xu Liu*

Main category: stat.ME

TL;DR: 该论文提出了一种新的高维线性回归模型假设检验方法，将传统的零系数检验改为无相关差异检验，用于检测迁移学习框架中源数据的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 传统的高维线性回归系数显著性检验关注系数是否为零，但实际应用中需要检测系数是否足够小（无相关差异）。这源于迁移学习框架中检测源数据可迁移性的迫切需求。

Method: 提出了一种新的检验程序，结合随机矩阵理论估计高维协方差矩阵的最大特征值。在存在高维干扰参数的更具挑战性设置下，建立了检验统计量在零假设和备择假设下的渐近正态性。

Result: 将提出的检验方法应用于检测源数据的可迁移性，统一的迁移学习模型相比现有方法同时实现了更低的估计和预测误差。

Conclusion: 通过模拟研究和GTEx数据分析验证了新检验方法的有限样本性质，证明了其在检测迁移学习可迁移性方面的优越性能。

Abstract: Most of researchers on testing a significance of coefficient $\ubeta$ in high-dimensional linear regression models consider the classical hypothesis testing problem $H_0^{c}: \ubeta=\uzero \mbox{ versus } H_1^{c}: \ubeta \neq \uzero$. We take a different perspective and study the testing problem with the null hypothesis of no relevant difference between $\ubeta$ and $\uzero$, that is, $H_0: \|\ubeta\|\leq δ_0 \mbox{ versus } H_1: \|\ubeta\|> δ_0$, where $δ_0$ is a prespecified small constant. This testing problem is motivated by the urgent requirement to detect the transferability of source data in the transfer learning framework. We propose a novel test procedure incorporating the estimation of the largest eigenvalue of a high-dimensional covariance matrix with the assistance of the random matrix theory. In the more challenging setting in the presence of high-dimensional nuisance parameters, we establish the asymptotic normality for the proposed test statistics under both the null and alternative hypotheses. By applying the proposed test approaches to detect the transferability of source data, the unified transfer learning models simultaneously achieve lower estimation and prediction errors with comparison to existing methods. We study the finite-sample properties of the new test by means of simulation studies and illustrate its performance by analyzing the GTEx data.

</details>


### [12] [Location--Scale Calibration for Generalized Posterior](https://arxiv.org/abs/2511.15320)
*Shu Tamano,Yui Tomo*

Main category: stat.ME

TL;DR: 提出了一种简单的后处理方法，用于校准广义贝叶斯后验分布，使其与渐近目标对齐，从而获得不依赖于学习率的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 广义贝叶斯更新用损失函数替代似然函数，但后验不确定性对学习率尺度敏感。需要一种方法使不确定性量化对学习率不变。

Method: 提出后处理校准方法，将广义后验抽样与渐近目标对齐。扩展了开放三明治调整方法，从协方差重标定扩展到位置-尺度校准。

Result: 证明了广义后验在总变差意义下的收敛性，允许样本量相关先验、非独立同分布观测和模型错误设定下的凸惩罚。经验研究表明校准后的抽样保持稳定的覆盖、区间宽度和偏差。

Conclusion: 校准后的抽样在广泛的学习率范围内保持稳定性能，紧密跟踪频率基准，而未校准的后验则变化显著。该方法为广义贝叶斯提供了可靠的不确定性量化。

Abstract: General Bayesian updating replaces the likelihood with a loss scaled by a learning rate, but posterior uncertainty can depend sharply on that scale. We propose a simple post-processing that aligns generalized posterior draws with their asymptotic target, yielding uncertainty quantification that is invariant to the learning rate. We prove total-variation convergence for generalized posteriors with an effective sample size, allowing sample-size-dependent priors, non-i.i.d. observations, and convex penalties under model misspecification. Within this framework, we justify and extend the open-faced sandwich adjustment (Shaby, 2014), provide general theoretical guarantees for its use within generalized Bayes, and extend it from covariance rescaling to a location--scale calibration whose draws converge in total variation to the target for any learning rate. In our empirical illustration, calibrated draws maintain stable coverage, interval width, and bias over orders of magnitude in the learning rate and closely track frequentist benchmarks, whereas uncalibrated posteriors vary markedly.

</details>


### [13] [BaGGLS: A Bayesian Shrinkage Framework for Interpretable Modeling of Interactions in High-Dimensional Biological Data](https://arxiv.org/abs/2511.15330)
*Marta S. Lemanczyk,Lucas Kock,Johanna Schlimme,Nadja Klein,Bernhard Y. Renard*

Main category: stat.ME

TL;DR: BaGGLS是一个用于高维生物数据特征交互分析的灵活可解释概率二元回归模型，通过贝叶斯分组全局-局部收缩先验实现稀疏性和可解释性，在交互检测方面优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 生物数据集通常具有高维、噪声大、稀疏信号复杂交互的特点，传统统计方法可解释性好但建模复杂交互能力有限，深度学习模型预测准确但缺乏可解释性。

Method: 引入贝叶斯分组全局-局部收缩先验，与交互项引入的分组结构对齐，采用部分因子化变分近似捕获后验偏斜度，支持大规模特征空间的高效学习。

Result: 在广泛模拟中，BaGGLS在交互检测方面优于其他方法，比基于马蹄先验的MCMC采样快数倍，在基序扫描器输出和深度学习模型噪声归因评分中有效发现交互作用。

Conclusion: BaGGLS是揭示生物相关交互模式的有前景方法，在计算生物学的高维任务中具有广泛应用潜力。

Abstract: Biological data sets are often high-dimensional, noisy, and governed by complex interactions among sparse signals. This poses major challenges for interpretability and reliable feature selection. Tasks such as identifying motif interactions in genomics exemplify these difficulties, as only a small subset of biologically relevant features (e.g., motifs) are typically active, and their effects are often non-linear and context-dependent. While statistical approaches often result in more interpretable models, deep learning models have proven effective in modeling complex interactions and prediction accuracy, yet their black-box nature limits interpretability. We introduce BaGGLS, a flexible and interpretable probabilistic binary regression model designed for high-dimensional biological inference involving feature interactions. BaGGLS incorporates a Bayesian group global-local shrinkage prior, aligned with the group structure introduced by interaction terms. This prior encourages sparsity while retaining interpretability, helping to isolate meaningful signals and suppress noise. To enable scalable inference, we employ a partially factorized variational approximation that captures posterior skewness and supports efficient learning even in large feature spaces. In extensive simulations, we can show that BaGGLS outperforms the other methods with regard to interaction detection and is many times faster than MCMC sampling under the horseshoe prior. We also demonstrate the usefulness of BaGGLS in the context of interaction discovery from motif scanner outputs and noisy attribution scores from deep learning models. This shows that BaGGLS is a promising approach for uncovering biologically relevant interaction patterns, with potential applicability across a range of high-dimensional tasks in computational biology.

</details>


### [14] [Utilizing subgroup information in random-effects meta-analysis of few studies](https://arxiv.org/abs/2511.15366)
*Ao Huang,Christian Röver,Tim Friede*

Main category: stat.ME

TL;DR: 针对随机效应元分析在少量研究（2-4个）时性能不佳的问题，提出基于亚组数据的推断方法，通过亚组级数据估计异质性并改进置信区间。


<details>
  <summary>Details</summary>
Motivation: 传统基于大样本近似的随机效应元分析方法在仅有少量研究时表现不佳，现有方法要么异质性估计不准，要么置信区间过宽。

Method: 使用亚组级数据的共同效应估计器，推导两种DerSimonian-Laird型异质性估计量，结合Henmi-Copas型方差，采用t分位数区间和灵活自由度。

Result: 通过全面模拟研究在不同亚组效应大小和流行度下的方法性能，提供了亚组选择的一般建议，并用两个实例应用说明方法。

Conclusion: 提出的基于亚组数据的推断方法能够改善少量研究时的元分析性能，特别是在评估替代结局的元分析中。

Abstract: Random-effects meta-analyses are widely used for evidence synthesis in medical research. However, conventional methods based on large-sample approximations often exhibit poor performance in case of very few studies (e.g., 2 to 4), which is very common in practice. Existing methods aiming to improve small-sample performance either still suffer from poor estimates of heterogeneity or result in very wide confidence intervals. Motivated by meta-analyses evaluating surrogate outcomes, where units nested within a trial are often exploited when the number of trials is small, we propose an inference approach based on a common-effect estimator synthesizing data from the subgroup-level instead of the study-level. Two DerSimonian-Laird type heterogeneity estimators are derived using the subgroup-level data, and are incorporated into the Henmi-Copas type variance to adequately reflect variance components. We considered t-quantile based intervals to account for small-sample properties and used flexible degrees of freedom to reduce interval lengths. A comprehensive simulation is conducted to study the performance of our methods depending on various magnitudes of subgroup effects as well as subgroup prevalences. Some general recommendations are provided on how to select the subgroups, and methods are illustrated using two example applications.

</details>


### [15] [Testing Conditional Independence via the Spectral Generalized Covariance Measure: Beyond Euclidean Data](https://arxiv.org/abs/2511.15453)
*Ryunosuke Miyazaki,Yoshimasa Uematsu*

Main category: stat.ME

TL;DR: 提出了一种基于谱广义协方差度量(SGCM)的条件独立性检验方法，通过谱分解构造数据依赖基函数，避免了直接估计条件均值嵌入，实现了稳健的有限样本控制。


<details>
  <summary>Details</summary>
Motivation: 现有条件独立性检验方法在复杂数据结构和有限样本情况下存在性能不足，需要开发更稳健、适用范围更广的检验方法。

Method: 使用谱分解获得数据依赖基函数来近似条件交叉协方差算子的平方范数，将问题简化为标量回归问题，并采用wild bootstrap进行推断。

Result: SGCM检验在广泛模拟中达到接近名义水平的大小控制，并在各种挑战性场景中表现出与最先进方法相当或更优的检验功效。

Conclusion: SGCM提供了一种稳健有效的条件独立性检验框架，适用于复杂数据类型，包括分布值数据和度量空间上的曲线。

Abstract: We propose a conditional independence (CI) test based on a new measure, the \emph{spectral generalized covariance measure} (SGCM). The SGCM is constructed by approximating the basis expansion of the squared norm of the conditional cross-covariance operator, using data-dependent bases obtained via spectral decompositions of empirical covariance operators. This construction avoids direct estimation of conditional mean embeddings and reduces the problem to scalar-valued regressions, resulting in robust finite-sample size control. Theoretically, we derive the limiting distribution of the SGCM statistic, establish the validity of a wild bootstrap for inference, and obtain uniform asymptotic size control under doubly robust conditions. As an additional contribution, we show that exponential kernels induced by continuous semimetrics of negative type are characteristic on general Polish spaces -- with extensions to finite tensor products -- thereby providing a foundation for applying our test and other kernel methods to complex objects such as distribution-valued data and curves on metric spaces. Extensive simulations indicate that the SGCM-based CI test attains near-nominal size and exhibits power competitive with or superior to state-of-the-art alternatives across a range of challenging scenarios.

</details>


### [16] [FDR Control via Neural Networks under Covariate-Dependent Symmetric Nulls](https://arxiv.org/abs/2511.15495)
*Taehyoung Kim,Seohwa Hwang,Junyong Park*

Main category: stat.ME

TL;DR: 提出了一个基于协变量自适应p值的多重假设检验框架，通过神经网络学习拒绝阈值，在控制错误发现率的同时优化发现数量。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖预计算的p值，且假设零分布与协变量无关。本文旨在开发更强大、自适应的推断方法，直接从原始数据计算协变量调整的p值。

Method: 基于对称零分布假设推导协变量自适应p值，使用神经网络通过镜像估计原理学习拒绝阈值，并估计条件零分布直接从原始数据计算p值。

Result: 模拟研究表明该方法在功效方面优于现有方法，并在血压数据和空气污染数据的实际应用中验证了其有效性。

Conclusion: 该方法提供了从原始数据推导协变量调整p值的原理性途径，并能与现有p值方法无缝集成，显著提升了多重假设检验的效能。

Abstract: In modern multiple hypothesis testing, the availability of covariate information alongside the primary test statistics has motivated the development of more powerful and adaptive inference methods. However, most existing approaches rely on p-values that are precomputed under the assumption that their null distributions are independent of the covariates. In this paper, we propose a framework that derives covariate-adaptive p-values from the assumption of a symmetric null distribution of the primary variable given the covariates, without imposing any parametric assumptions. Building on these data-driven p-values, we employ a neural network model to learn a covariate-adaptive rejection threshold via the mirror estimation principle, optimizing the number of discoveries while maintaining valid false discovery rate control. Furthermore, our estimation of the conditional null distribution enables the computation of p-values directly from the raw data. The proposed method provides a principled way to derive covariate-adjusted p-values from raw data and allows seamless integration with previously established p-value based procedures. Simulation studies show that the proposed method outperforms existing approaches in terms of power. We further illustrate its applicability through two real data analyses: age-specific blood pressure data and U.S. air pollution data.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [17] [Uncovering Treatment Effect Heterogeneity in Pragmatic Gerontology Trials](https://arxiv.org/abs/2511.14893)
*Changjun Li,Heather Allore,Michael O. Harhay,Fan Li,Guangyu Tong*

Main category: stat.AP

TL;DR: 该研究使用贝叶斯加性回归树(BART)结合主分层因果框架，分析老年护理试验中因死亡截断导致的治疗异质性，发现尽管整体效果为零，但某些亚组在远程护理中获得了生活质量改善。


<details>
  <summary>Details</summary>
Motivation: 在老年学试验中，治疗反应存在异质性，但由于死亡截断问题，传统的边际干预估计可能掩盖了生存者的潜在治疗效果差异。

Method: 采用基于主分层的因果框架，结合贝叶斯加性回归树(BART)非参数机器学习方法，灵活建模潜在主分层和分层特异性潜在结果，估计条件幸存者平均因果效应(CSACE)。

Result: 分析显示，尽管平均治疗效果为零，但基于基线特征定义的某些亚组在远程护理中获得了不同的生活质量改善，突显了个性化干预策略的机会。

Conclusion: 将机器学习方法嵌入原则性因果推断框架中，可以为具有复杂特征（包括死亡截断和聚类）的试验数据提供更深入的见解，这对分析实用的老年学试验至关重要。

Abstract: Detecting heterogeneity in treatment response enriches the interpretation of gerontologic trials. In aging research, estimating the effect of the intervention on clinically meaningful outcomes faces analytical challenges when it is truncated by death. For example, in the Whole Systems Demonstrator trial, a large cluster-randomized study evaluating telecare among older adults, the overall effect of the intervention on quality of life was found to be null. However, this marginal intervention estimate obscures potential heterogeneity of individuals responding to the intervention, particularly among those who survive to the end of follow-up. To explore this heterogeneity, we adopt a causal framework grounded in principal stratification, targeting the Survivor Average Causal Effect (SACE)-the treatment effect among "always-survivors," or those who would survive regardless of treatment assignment. We extend this framework using Bayesian Additive Regression Trees (BART), a nonparametric machine learning method, to flexibly model both latent principal strata and stratum-specific potential outcomes. This enables the estimation of the Conditional SACE (CSACE), allowing us to uncover variation in treatment effects across subgroups defined by baseline characteristics. Our analysis reveals that despite the null average effect, some subgroups experience distinct quality of life benefits (or lack thereof) from telecare, highlighting opportunities for more personalized intervention strategies. This study demonstrates how embedding machine learning methods, such as BART, within a principled causal inference framework can offer deeper insights into trial data with complex features including truncation by death and clustering-key considerations in analyzing pragmatic gerontology trials.

</details>


### [18] [Fifty Shades of Greenwashing: The Political Economy of Climate Change Advertising on Social Media](https://arxiv.org/abs/2511.14930)
*Robert Kubinec,Aseem Mahajan*

Main category: stat.AP

TL;DR: 提出了一种新的漂绿衡量方法，通过分析Meta广告定向数据，发现化石燃料公司使用社交媒体广告进行政治漂绿，主要针对左倾社区进行微定向。


<details>
  <summary>Details</summary>
Motivation: 需要衡量漂绿现象，特别是化石燃料公司如何利用社交媒体广告来转移批评，进行气候相关的误导宣传。

Method: 结合大型语言模型、人工编码和贝叶斯项目反应理论，分析1100万条社会政治广告，识别漂绿内容。

Result: 发现漂绿具有多样化的行为主体和组成部分，化石燃料公司通过未公开关联的组织进行政治漂绿广告，主要针对左倾社区进行微定向，但很少针对国家层面的公众舆论。

Conclusion: 漂绿现象复杂多样，化石燃料公司通过社交媒体广告进行政治漂绿，主要针对特定社区而非全国公众，这为监管和政策制定提供了重要见解。

Abstract: In this paper, we provide a novel measure for greenwashing -- i.e., climate-related misinformation -- that shows how polluting companies can use social media advertising related to climate change to redirect criticism. To do so, we identify greenwashing content in 11 million social-political ads in Meta's Ad Targeting Datset with a measurement technique that combines large language models, human coders, and advances in Bayesian item response theory. We show that what is called greenwashing has diverse actors and components, but we also identify a very pernicious form, which we call political greenwashing, that appears to be promoted by fossil fuel companies and related interest groups. Based on ad targeting data, we show that much of this advertising happens via organizations with undisclosed links to the fossil fuel industry. Furthermore, we show that greenwashing ad content is being micro-targeted at left-leaning communities with fossil fuel assets, though we also find comparatively little evidence of ad targeting aimed at influencing public opinion at the national level.

</details>


### [19] [Principled Frequentist Estimation of Racial Disparity in Credit Approval under Unobserved Race](https://arxiv.org/abs/2511.14951)
*Sam Fisher,Dmitry Lesnik,Tobias Schäfer*

Main category: stat.AP

TL;DR: 提出了一种新的频率主义方法来估计贷款审批中的种族差异，当种族信息不可观测时，通过OLS和MLE方法在种族预测变量外生性假设下实现一致性估计。


<details>
  <summary>Details</summary>
Motivation: 在公平贷款合规中需要估计种族差异，但种族信息通常不可观测，现有启发式方法（如阈值和加权估计器）在有效识别假设下不一致，影响内部有效性。

Method: 提出频率主义对应方法，使用OLS和最大似然估计，在种族预测变量外生性假设下工作。引入仅姓氏代理和收入分层先验来满足实践中的假设要求。

Result: 蒙特卡洛模拟和2023年洛杉矶HMDA数据应用显示，该方法相比标准先验的加权估计器，在洛杉矶黑人/白人不利影响比率中将RMSE降低了79.7%（从10.639pp降至2.158pp）。

Conclusion: 该方法在种族信息不可观测的情况下，显著提高了贷款审批种族差异估计的准确性和一致性，为公平贷款合规提供了更可靠的统计工具。

Abstract: Estimating racial disparities in loan-approval probabilities when race is unobserved is routinely required for fair lending compliance. In such cases, race probabilities-typically from Bayesian Improved Surname Geocoding (BISG)-stand in for true race. Prior work shows that common heuristic approaches, including the Threshold and Weighting estimators, are inconsistent under valid identification assumptions, compromising internal validity. A recent Bayesian approach demonstrates consistency under assumptions reasonable in many fair lending contexts. This approach hinges on the insight that identification requires the race predictors to be exogenous with respect to loan approval, essentially an instrumental-variables design. We present a frequentist counterpart to this solution via Ordinary Least Squares (OLS) and Maximum Likelihood Estimation (MLE) under a similar exogeneity assumption. To satisfy these assumptions in practice, we introduce (i) a surname-only proxy analogous to BISG and (ii) an income-stratified prior for race probabilities. Monte Carlo simulations and an application to 2023 Los Angeles HMDA data confirm superior performance: this method reduces RMSE in the LA Black/White adverse-impact ratio by 79.7% (from 10.639pp to 2.158pp) compared to a Weighting estimator with the standard prior.

</details>


### [20] [Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks](https://arxiv.org/abs/2511.15003)
*Reza Mirjalili,Behrad Braghi,Shahram Shadrokh Sikari*

Main category: stat.AP

TL;DR: 提出基于图神经网络的项目工期和成本预测框架，相比传统方法显著提升预测精度，并提供可解释的资源瓶颈分析


<details>
  <summary>Details</summary>
Motivation: 传统项目管理方法如CPM和PERT依赖简化的静态假设，难以准确预测资源受限和任务相互依赖的项目工期和成本

Method: 构建异构活动-资源图表示项目，使用图神经网络（GraphSAGE和时序图网络）捕捉任务、资源和时间成本动态之间的结构关系

Result: 相比传统回归和基于树的方法，平均绝对误差降低23-31%，决定系数R2从0.78提升至0.91，并为复杂项目网络提供可解释的资源瓶颈分析

Conclusion: 图神经网络框架在项目工期和成本预测方面优于传统方法，提供更准确和可解释的预测结果

Abstract: Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies.
  We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [21] [Convex Clustering Redefined: Robust Learning with the Median of Means Estimator](https://arxiv.org/abs/2511.14784)
*Sourav De,Koustav Chowdhury,Bibhabasu Mandal,Sagar Ghosh,Swagatam Das,Debolina Paul,Saptarshi Chakraborty*

Main category: stat.ML

TL;DR: 提出了一种结合凸聚类和中位数均值估计器的鲁棒聚类方法，能够抵抗异常值且无需预先指定聚类数量。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法需要预先指定聚类数量且对初始化和异常值敏感，凸聚类虽然稳定但面临高维数据和强融合正则化的挑战。

Method: 将凸聚类与中位数均值估计器相结合，利用MoM的鲁棒性和凸聚类的稳定性构建抗异常值的聚类框架。

Result: 理论分析表明在特定条件下具有弱一致性，在合成和真实数据集上的实验验证了该方法优于现有方法。

Conclusion: 该方法通过整合MoM和凸聚类的优势，实现了对异常值鲁棒且无需预设聚类数量的高效聚类。

Abstract: Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.

</details>


### [22] [Implicit Bias of the JKO Scheme](https://arxiv.org/abs/2511.14827)
*Peter Halmos,Boris Hanin*

Main category: stat.ML

TL;DR: 本文分析了JKO（Jordan-Kinderlehrer-Otto）方案在Wasserstein梯度流中的二阶隐式偏差，发现该方案实际上是在最小化一个修正的能量函数J^η，该函数在原始能量J的基础上减去了与度量曲率相关的项。


<details>
  <summary>Details</summary>
Motivation: JKO方案作为Wasserstein梯度流的经典时间离散化方法，具有许多独特性质（如保持能量耗散、无条件稳定性等），但对其二阶隐式偏差的理解尚不充分。本文旨在深入分析JKO方案在二阶精度下的行为特性。

Method: 通过理论分析，推导出JKO方案在二阶精度下实际上是在最小化修正能量J^η = J - (η/4)∫∥∇_g(δJ/δρ)∥²ρ(dx)，并研究了相应的JKO-Flow（在J^η上的Wasserstein梯度流）。

Result: 发现JKO方案在二阶精度下引入了与度量曲率相关的减速项，对于常见泛函（如熵、KL散度、黎曼梯度下降）产生了特定的隐式偏差。通过数值实验验证了理论结果。

Conclusion: JKO方案在二阶精度下表现出独特的隐式偏差特性，这种偏差对应于在能量泛函度量曲率变化剧烈方向上的减速，为理解JKO方案的优越性能提供了新的理论视角。

Abstract: Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \emph{modified} energy \[ J^η(ρ) = J(ρ) - \fracη{4}\int_M \Big\lVert \nabla_g \frac{δJ}{δρ} (ρ) \Big\rVert_{2}^{2} \,ρ(dx), \] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.

</details>


### [23] [Latent space analysis and generalization to out-of-distribution data](https://arxiv.org/abs/2511.15010)
*Katie Rainey,Erin Hausmann,Donald Waagen,David Gray,Donald Hulsey*

Main category: stat.ML

TL;DR: 研究表明，潜在空间中的OOD检测不能作为模型性能的代理指标，需要进一步研究潜在空间的几何特性以理解深度学习鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习系统在潜在决策空间中数据点之间的关系对于评估和解释系统在真实世界数据上的性能至关重要。OOD检测一直是活跃的研究领域。

Method: 使用开源模拟和实测合成孔径雷达数据集，实证研究潜在空间OOD检测与模型分类准确性之间的联系。

Result: 实证证明OOD检测不能作为模型性能的代理衡量标准。

Conclusion: 需要进一步研究潜在空间的几何特性，这可能为深度学习的鲁棒性和泛化能力提供新的见解。

Abstract: Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.

</details>


### [24] [Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit](https://arxiv.org/abs/2511.15120)
*Bohan Zhang,Zihao Wang,Hengyu Fu,Jason D. Lee*

Main category: stat.ML

TL;DR: 该论文证明了两层神经网络通过分层梯度下降可以最优地学习高斯多索引模型，样本复杂度为O(d)，时间复杂度为O(d²)，均达到信息论极限。关键发现是第一层需要训练超过O(1)步才能实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何高效学习高维特征，特别是理解表示学习机制，探索在通用高斯多索引模型设置下神经网络的梯度下降学习过程。

Method: 使用标准两层神经网络，通过分层梯度下降训练，分析内层权重执行幂迭代过程，隐式模拟隐藏子空间的谱启动，最终消除有限样本噪声并恢复子空间。

Result: 证明神经网络可以以o_d(1)测试误差不可知地学习目标函数，样本复杂度O(d)和时间复杂度O(d²)均达到信息论极限，是最优的。

Conclusion: 这项工作展示了神经网络在样本和时间效率方面有效学习层次函数的能力，证明了第一层训练超过O(1)步是实现最优性能的关键。

Abstract: In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.

</details>


### [25] [Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings](https://arxiv.org/abs/2511.15146)
*Eugene Ndiaye*

Main category: stat.ML

TL;DR: 本文提出了多变量保形预测方法，通过最优运输理论定义向量秩，构建具有有限样本覆盖保证的多变量预测集和保形预测分布。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测仅限于标量分数，无法处理向量值分数。最优运输理论虽然提供了向量排序方法，但只有渐近覆盖保证。需要将保形预测扩展到多变量设置，并解决预测集只能指示哪些结果是可能的而不能表示相对似然的问题。

Method: 使用最优运输理论定义向量秩，通过保形化向量值OT分位数区域来恢复有限样本覆盖保证。构建多变量保形预测分布，包括保守版本和精确随机化版本。

Result: 提出了第一个具有有限样本校准的多变量保形预测分布，可以定义有效的多变量分布，其中任何导出的不确定区域自动具有保证的覆盖范围。

Conclusion: 成功将保形预测扩展到多变量设置，提供了具有有限样本覆盖保证的多变量预测集和预测分布，解决了传统方法只能处理标量分数的限制。

Abstract: Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.

</details>


### [26] [Particle Monte Carlo methods for Lattice Field Theory](https://arxiv.org/abs/2511.15196)
*David Yallup*

Main category: stat.ML

TL;DR: GPU加速的粒子方法（SMC和嵌套采样）在格点场论的高维多模态采样基准测试中，比最先进的神经采样器表现更好，同时还能估计配分函数。


<details>
  <summary>Details</summary>
Motivation: 解决格点场论中的高维多模态采样问题，为机器学习辅助采样方法建立基准。

Method: 使用GPU加速的粒子方法，包括顺序蒙特卡洛（SMC）和嵌套采样，仅使用单一数据驱动的协方差进行调优。

Result: 在样本质量和运行时间上匹配或优于最先进的神经采样器，同时估计配分函数，无需特定问题结构即可实现竞争性能。

Conclusion: 这些方法为学习型采样器的训练成本提供了更高的基准，表明在什么情况下学习型采样器才值得其训练成本。

Abstract: High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.

</details>


### [27] [Robust Bayesian Optimisation with Unbounded Corruptions](https://arxiv.org/abs/2511.15315)
*Abdelhamid Ezzerg,Ilija Bogunovic,Jeremias Knoblauch*

Main category: stat.ML

TL;DR: 提出了RCGP-UCB算法，将UCB方法与鲁棒共轭高斯过程结合，能够处理频率有界但幅度可能无限的异常值，在无异常值时性能与标准GP-UCB相当。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化对极端异常值非常脆弱，现有鲁棒方法假设累积污染有界，无法应对单个幅度足够大的污染。

Method: 引入频率有界但幅度无界的对抗模型，开发RCGP-UCB算法，结合UCB和鲁棒共轭高斯过程，提供稳定和自适应版本。

Result: 算法在最多O(T^{1/2})和O(T^{1/3})次污染下实现次线性遗憾，且污染幅度可能无限大。

Conclusion: RCGP-UCB在保持标准GP-UCB性能的同时，显著增强了对抗极端异常值的鲁棒性。

Abstract: Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.

</details>


### [28] [Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss](https://arxiv.org/abs/2511.15332)
*The Tien Mai*

Main category: stat.ML

TL;DR: 提出Exponential Lasso方法，通过指数型损失函数在Lasso框架中实现鲁棒性，在保持高斯噪声下统计效率的同时，有效抵抗离群值和重尾噪声的影响。


<details>
  <summary>Details</summary>
Motivation: 传统Lasso方法使用平方损失函数，对离群值和重尾噪声高度敏感，导致模型选择不可靠和估计偏差。需要开发一种既能保持统计效率又具备鲁棒性的方法。

Method: 在Lasso框架中引入指数型损失函数，该函数能平滑地在高斯噪声下的统计效率和数据污染下的鲁棒性之间权衡。使用Majorization-Minimization算法高效优化，迭代求解加权Lasso子问题。

Result: 理论保证显示Exponential Lasso在理想条件下能达到与经典Lasso相同的收敛速率，同时在重尾污染下保持鲁棒性。数值实验表明该方法在污染设置下优于经典Lasso，在Gaussian噪声下仍保持强性能。

Conclusion: Exponential Lasso是一种有效的鲁棒变量选择方法，成功解决了传统Lasso对离群值敏感的问题，实现了统计效率和鲁棒性的平衡。

Abstract: In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.
  Our method is implemented in the \texttt{R} package \texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso

</details>


### [29] [Gini Score under Ties and Case Weights](https://arxiv.org/abs/2511.15446)
*Alexej Brauer,Mario V. Wüthrich*

Main category: stat.ML

TL;DR: 本文讨论了基尼分数在统计建模中的应用，特别是在存在并列排名和案例权重的情况下如何扩展使用。


<details>
  <summary>Details</summary>
Motivation: 基尼分数在二元响应模型中广泛应用，但在实际应用中经常遇到并列排名和案例权重的情况，需要扩展其适用性。

Method: 通过使用洛伦兹曲线和集中曲线，将基尼分数从二元响应扩展到一般实值随机变量，并处理并列排名和案例权重的情况。

Result: 提出了在存在并列排名和案例权重情况下使用基尼分数的方法，扩展了其在精算实践中的应用范围。

Conclusion: 基尼分数可以成功扩展到处理并列排名和案例权重的情况，使其在精算建模中更具实用价值。

Abstract: The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.

</details>


### [30] [A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation](https://arxiv.org/abs/2511.15543)
*Georgios Venianakis,Constantinos Theodoropoulos,Michail Kavousanakis*

Main category: stat.ML

TL;DR: 提出了一个基于PINNs的综合框架，同时解决最优传感器布置和参数估计问题，通过将待估参数作为额外输入来训练PINN模型，利用自动微分计算灵敏度函数，并采用D最优准则确定最优传感器位置。


<details>
  <summary>Details</summary>
Motivation: 参数估计在工程领域具有挑战性，特别是在数据获取成本高、有限或存在噪声的情况下。虽然PINNs已成为解决逆问题的强大工具，但很少研究其性能如何依赖于传感器布置。

Method: 训练一个将待估参数作为额外输入的PINN模型，通过自动微分计算灵敏度函数，利用D最优准则确定最优传感器位置，同时进行参数估计。

Result: 在两个分布式参数反应-扩散-对流问题上验证，结果表明该方法比直觉或随机选择的传感器位置获得更高的参数估计精度。

Conclusion: 提出的PINNs框架能够有效确定最优传感器位置并提高参数估计精度，为分布式参数系统的参数估计提供了有效解决方案。

Abstract: Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.

</details>


### [31] [Near-optimal delta-convex estimation of Lipschitz functions](https://arxiv.org/abs/2511.15615)
*Gábor Balázs*

Main category: stat.ML

TL;DR: 提出了一种可处理的Lipschitz函数估计算法，通过最大仿射方法扩展到Lipschitz设置，使用非线性特征映射将最大仿射函数转换为delta-凸函数子类，实现了最小最大收敛率。


<details>
  <summary>Details</summary>
Motivation: 扩展凸形状限制回归中的最大仿射方法到更一般的Lipschitz设置，开发能够自适应捕获数据内在维度且无需知道真实Lipschitz常数的估计算法。

Method: 结合自适应分区捕获内在维度、基于惩罚的正则化机制、两阶段优化过程（凸初始化+局部优化），通过非线性特征扩展将最大仿射函数映射为delta-凸函数子类。

Result: 在平方损失和次高斯分布的随机设计设置下，估计器达到了关于数据内在维度的最小最大收敛率（除对数因子外），实验显示相对于最近邻和基于核的回归器具有竞争力。

Conclusion: 该框架为Lipschitz函数估计提供了一种可处理的算法，能够自适应捕获内在维度且无需知道真实Lipschitz常数，在理论和实验上都表现出色，且易于适应凸形状限制回归。

Abstract: This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.

</details>


### [32] [Rényi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincaré Inequalities](https://arxiv.org/abs/2511.15634)
*Benjamin Dupuis,Mert Gürbüzbalaban,Umut Şimşekli,Jian Wang,Sinan Yildirim,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 本文提出了首个针对重尾随机微分方程及其离散化版本的Rényi差分隐私保证，通过新的Rényi流计算和分数Poincaré不等式，显著降低了维度依赖。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私研究主要关注轻尾噪声，而重尾SGD的DP保证存在维度依赖强、无法扩展到RDP等限制，需要新的理论框架来解决这些问题。

Method: 基于新的Rényi流计算和分数Poincaré不等式，推导重尾SDE及其离散化版本的RDP保证，不依赖梯度裁剪。

Result: 在满足分数Poincaré不等式条件下，获得了比现有方法维度依赖更弱的DP保证，首次实现了重尾算法的RDP分析。

Conclusion: 提出的框架为重尾随机算法的隐私分析提供了新工具，显著改善了维度依赖问题，扩展了DP理论在重尾设置下的应用范围。

Abstract: Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,δ)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established Rényi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new Rényi flow computations and the use of well-established fractional Poincaré inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.

</details>


### [33] [Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion](https://arxiv.org/abs/2511.15679)
*Jianqiao Mao,Max A. Little*

Main category: stat.ML

TL;DR: 本文提出了前门可约性(FDR)概念，扩展了经典前门准则的适用范围，将复杂因果图通过聚合变量为超节点的方式简化为前门设置。


<details>
  <summary>Details</summary>
Motivation: 经典前门准则的适用性被认为很有限，而ID算法虽然通用但得到的干预分布表达式往往不实用、难以估计。作者认为前门准则的适用性比看起来更广泛。

Method: 引入前门可约性(FDR)的图条件，通过将变量聚合为超节点(FDR三元组)来简化复杂因果图；提出FDR-TID算法来检测可接受的FDR三元组。

Result: 证明了FDR准则满足性与FDR调整适用性之间的图级等价性；FDR-TID算法被证明具有正确性、完备性和有限终止性。

Conclusion: FDR补充了现有的识别方法，在不牺牲通用性的前提下优先考虑可解释性和计算简单性，许多超出教科书前门设置的图都是FDR可约的。

Abstract: Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\left(\boldsymbol{X}^{*},\boldsymbol{Y}^{*},\boldsymbol{M}^{*}\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.

</details>
