<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 21]
- [stat.ML](#stat.ML) [Total: 18]
- [stat.AP](#stat.AP) [Total: 7]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [BayesFlow 2.0: Multi-Backend Amortized Bayesian Inference in Python](https://arxiv.org/abs/2602.07098)
*Lars Kühmichel,Jerry M. Huang,Valentin Pratz,Jonas Arruda,Hans Olischläger,Daniel Habermann,Simon Kucharsky,Lasse Elsemüller,Aayush Mishra,Niels Bracher,Svenja Jedhoff,Marvin Schmitt,Paul-Christian Bürkner,Stefan T. Radev*

Main category: stat.CO

TL;DR: BayesFlow 2.0：一个用于摊销贝叶斯推理的Python库，通过神经网络训练模型模拟实现快速推理，支持多种深度学习后端和高级功能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法在处理复杂模型和大数据集时计算成本高昂，摊销贝叶斯推理（ABI）通过神经网络训练模型模拟来解决这一计算挑战，实现快速推理。

Method: 开发BayesFlow 2.0 Python库，支持直接后验、似然和比率估计，提供多种深度学习后端支持、丰富的生成网络集合、完整定制和高层接口，以及超参数优化、设计优化和层次建模等新功能。

Result: 通过动态系统参数估计案例研究，结合与其他软件的对比，表明该库的简化、用户友好工作流程具有广泛采用的强大潜力。

Conclusion: BayesFlow 2.0为摊销贝叶斯推理提供了一个通用、高效且用户友好的解决方案，有望推动ABI在复杂贝叶斯建模中的广泛应用。

Abstract: Modern Bayesian inference involves a mixture of computational methods for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows. An overarching motif of many Bayesian methods is that they are relatively slow, which often becomes prohibitive when fitting complex models to large data sets. Amortized Bayesian inference (ABI) offers a path to solving the computational challenges of Bayes. ABI trains neural networks on model simulations, rewarding users with rapid inference of any model-implied quantity, such as point estimates, likelihoods, or full posterior distributions. In this work, we present the Python library BayesFlow, Version 2.0, for general-purpose ABI. Along with direct posterior, likelihood, and ratio estimation, the software includes support for multiple popular deep learning backends, a rich collection of generative networks for sampling and density estimation, complete customization and high-level interfaces, as well as new capabilities for hyperparameter optimization, design optimization, and hierarchical modeling. Using a case study on dynamical system parameter estimation, combined with comparisons to similar software, we show that our streamlined, user-friendly workflow has strong potential to support broad adoption.

</details>


### [2] [PoissonRatioUQ: An R package for band ratio uncertainty quantification](https://arxiv.org/abs/2602.07165)
*Matthew LeDuc,Tomoko Matsuo*

Main category: stat.CO

TL;DR: 开发了一个R包，用于处理计数比问题的贝叶斯建模和不确定性量化，假设关注的是泊松均值比而非计数比，提供多种估计方法，支持空间信息，并能处理Z=(mT+z₀)^p形式的变换问题。


<details>
  <summary>Details</summary>
Motivation: 在涉及计数比的实际问题中，通常需要估计的是底层泊松过程的均值比，而非简单的观测计数比。现有方法往往忽视这一区别，导致估计偏差。此外，需要处理空间信息以及非线性变换下的不确定性量化问题。

Method: 基于贝叶斯框架，假设关注的是泊松均值比而非计数比。提供多种估计方法，包括处理空间信息的方法。特别包含了对Z=(mT+z₀)^p形式变换的不确定性量化能力，其中Z是强度比，T是关注量。

Result: 开发了一个功能完整的R包，能够：1) 正确估计泊松均值比而非简单计数比；2) 处理包含空间信息的问题；3) 对非线性变换后的量进行不确定性量化；4) 提供多种不同的估计方法供用户选择。

Conclusion: 该R包为计数比问题提供了全面的贝叶斯建模和不确定性量化工具，特别强调了泊松均值比与计数比的区别，并扩展了处理空间信息和非线性变换的能力，填补了现有方法的空白。

Abstract: We introduce an R package for Bayesian modeling and uncertainty quantification for problems involving count ratios. The modeling relies on the assumption that the quantity of interest is the ratio of Poisson means rather than the ratio of counts. We provide multiple different options for retrieval of this quantity for problems with and without spatial information included. Some added capability for uncertainty quantification for problems of the form $Z=(mT+z_0)^{p}$, where $Z$ is the intensity ratio and $T$ the quantity of interest, is included.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [Adaptive Experimental Design Using Shrinkage Estimators](https://arxiv.org/abs/2602.07404)
*Evan T. R. Rosenman,Kristen B. Hunter*

Main category: stat.ME

TL;DR: 本文提出了一种在序贯多臂试验中使用Stein-like收缩估计器进行因果效应估计的自适应设计方法，相比传统的Neyman分配方案，该方法能通过跨治疗臂共享信息来降低估计误差。


<details>
  <summary>Details</summary>
Motivation: 在多臂试验中，传统的序贯Neyman分配方案虽然能提高估计效率，但存在一个关键缺陷：它未能跨治疗臂共享信息，导致估计效率可能不足。本文旨在解决这一局限性。

Method: 提出使用适用于异方差数据的Stein-like收缩估计器进行最终因果估计，该估计器能跨治疗效应估计共享信息。推导出收缩估计器的期望损失具有高斯二次型形式，可通过数值积分高效计算，从而支持序贯自适应性，使治疗分配能最小化收缩损失。

Result: 通过模拟实验证明，该方法能显著降低估计误差。同时分析了自适应算法与序贯Neyman分配在治疗分配方式上的差异。

Conclusion: 基于收缩估计器的自适应设计能有效提高多臂序贯试验的估计效率，通过跨臂信息共享实现比传统方法更优的误差控制，为序贯自适应治疗分配提供了新思路。

Abstract: In the setting of multi-armed trials, adaptive designs are a popular way to increase estimation efficiency, identify optimal treatments, or maximize rewards to individuals. Recent work has considered the case of estimating the effects of K active treatments, relative to a control arm, in a sequential trial. Several papers have proposed sequential versions of the classical Neyman allocation scheme to assign treatments as individuals arrive, typically with the goal of using Horvitz-Thompson-style estimators to obtain causal estimates at the end of the trial. However, this approach may be inefficient in that it fails to borrow information across the treatment arms.
  In this paper, we consider adaptivity when the final causal estimation is obtained using a Stein-like shrinkage estimator for heteroscedastic data. Such an estimator shares information across treatment effect estimates, providing provable reductions in expected squared error loss relative to estimating each causal effect in isolation. Moreover, we show that the expected loss of the shrinkage estimator takes the form of a Gaussian quadratic form, allowing it to be computed efficiently using numerical integration. This result paves the way for sequential adaptivity, allowing treatments to be assigned to minimize the shrinker loss. Through simulations, we demonstrate that this approach can yield meaningful reductions in estimation error. We also characterize how our adaptive algorithm assigns treatments differently than would a sequential Neyman allocation.

</details>


### [4] [Fast Rerandomization for Balancing Covariates in Randomized Experiments: A Metropolis-Hastings Framework](https://arxiv.org/abs/2602.07613)
*Jiuyao Lu,Tianruo Zhang,Ke Zhu*

Main category: stat.ME

TL;DR: PSRSRR算法通过Metropolis-Hastings框架和重采样步骤，在保持再随机化理论保证的同时，将计算速度提升10-10,000倍。


<details>
  <summary>Details</summary>
Motivation: 再随机化虽然能通过重复生成处理分配来平衡协变量，但传统拒绝采样方法在阈值较小时计算效率极低，迫使实践者采用次优设置，导致性能下降。现有加速方法通常无法保持可接受分配空间的均匀性，从而失去了经典再随机化的理论基础。

Method: 基于Metropolis-Hastings框架，引入额外的采样-重要性重采样步骤来恢复均匀性并保持统计保证。提出的PSRSRR算法通过这一创新设计，在保持理论有效性的同时大幅提升计算效率。

Result: PSRSRR算法在模拟和两个真实数据应用中展示了10到10,000倍的速度提升，同时保持了精确和渐近有效性，解决了传统再随机化计算效率低下的问题。

Conclusion: PSRSRR算法成功解决了再随机化实践中的计算效率瓶颈，在保持理论保证的前提下实现了数量级的加速，使再随机化在实际应用中更加可行。

Abstract: Balancing covariates is critical for credible and efficient randomized experiments. Rerandomization addresses this by repeatedly generating treatment assignments until covariate balance meets a prespecified threshold. By shrinking this threshold, it can achieve arbitrarily strong balance, with established results guaranteeing optimal estimation and valid inference in both finite-sample and asymptotic settings across diverse complex experimental settings. Despite its rigorous theoretical foundations, practical use is limited by the extreme inefficiency of rejection sampling, which becomes prohibitively slow under small thresholds and often forces practitioners to adopt suboptimal settings, leading to degraded performance. Existing work focusing on acceleration typically fail to maintain the uniformity over the acceptable assignment space, thus losing the theoretical grounds of classical rerandomization. Building upon a Metropolis-Hastings framework, we address this challenge by introducing an additional sampling-importance resampling step, which restores uniformity and preserves statistical guarantees. Our proposed algorithm, PSRSRR, achieves speedups ranging from 10 to 10,000 times while maintaining exact and asymptotic validity, as demonstrated by simulations and two real-data applications.

</details>


### [5] [Beyond Euclidean Summaries: Online Change Point Detection for Distribution-Valued Data](https://arxiv.org/abs/2602.07252)
*Yingyan Zeng,Yujing Huang,Xiaoyu Chen*

Main category: stat.ME

TL;DR: 提出一种基于2-Wasserstein空间的在线变点检测框架，将流式批次数据视为分布值随机过程，通过切空间映射实现复杂分布变化的检测


<details>
  <summary>Details</summary>
Motivation: 现有在线变点检测方法依赖固定维度的欧几里得摘要，假设分布变化能被矩或特征表示捕获，但可能掩盖分布形状或几何结构的重要变化

Method: 将流式批次数据视为2-Wasserstein空间上的随机过程，将每个经验分布映射到相对于预变化Frèchet重心的切空间，实现参考中心的局部线性化，然后应用经典多元监控统计量到切场

Result: 在合成和真实世界实验中，该方法能检测复杂分布偏移，在匹配ARL0条件下减少检测延迟，优于基于矩和无模型基线方法

Conclusion: 提出的内在分布值变点检测框架能有效捕捉分布形状和几何结构的变化，为复杂分布偏移检测提供了理论保证和实际优势

Abstract: Existing online change-point detection (CPD) methods rely on fixed-dimensional Euclidean summaries, implicitly assuming that distributional changes are well captured by moment-based or feature-based representations. They can obscure important changes in distributional shape or geometry. We propose an intrinsic distribution-valued CPD framework that treats streaming batch data as a stochastic process on the 2-Wasserstein space. Our method detects changes in the law of this process by mapping each empirical distribution to a tangent space relative to a pre-change Fréchet barycenter, yielding a reference-centered local linearization of 2-Wasserstein space. This representation enables sequential detectors by adapting classical multivariate monitoring statistics to tangent fields. We provide theoretical guarantees and demonstrate, via synthetic and real-world experiments, that our approach detects complex distributional shifts with reduced detection delay at matched $\mathrm{ARL}_0$ compared with moments-based and model-free baselines.

</details>


### [6] [Balancing Covariates in Survey Experiments](https://arxiv.org/abs/2602.07390)
*Pengfei Tian,Jiyang Ren,Yingying Ma*

Main category: stat.ME

TL;DR: 提出分层拒绝抽样与再随机化设计，用于改善调查实验中协变量不平衡问题，提高平均处理效应估计效率


<details>
  <summary>Details</summary>
Motivation: 标准人口调查实验中，虽然随机抽样和随机分配理论上能平衡协变量，但有限样本中常存在协变量不平衡问题，影响估计精度

Method: 提出分层拒绝抽样与再随机化设计，在抽样和分配阶段分层处理协变量；开发基于设计的渐近理论分析分层均值差估计量；提出分析阶段的协变量调整方法

Result: 分层均值差估计量具有一致性，其渐近分布是正态分布与两个截断正态分布的卷积，比现有实验设计更集中于真实平均处理效应；协变量调整能进一步提高估计效率

Conclusion: 分层拒绝抽样与再随机化设计能有效改善调查实验中的协变量平衡问题，提高平均处理效应估计的精度和效率

Abstract: The survey experiment is widely used in economics and social sciences to evaluate the effects of treatments or programs. In a standard population-based survey experiment, the experimenter randomly draws experimental units from a target population of interest and then randomly assigns the sampled units to treatment or control conditions to explore the treatment effect of an intervention. Simple random sampling and treatment assignment can balance covariates on average. However, covariate imbalance often exists in finite samples. To address the imbalance issue, we study a stratified approach to balance covariates in a survey experiment. A stratified rejective sampling and rerandomization design is further proposed to enhance the covariate balance. We develop a design-based asymptotic theory for the widely used stratified difference-in-means estimator of the average treatment effect under the proposed design. In particular, we show that it is consistent and asymptotically a convolution of a normal distribution and two truncated normal distributions. This limiting distribution is more concentrated at the true average treatment effect than that under the existing experimental designs. Moreover, we propose a covariate adjustment method in the analysis stage, which can further improve the estimation efficiency. Numerical studies demonstrate the validity and improved efficiency of the proposed method.

</details>


### [7] [Estimating the Shannon Entropy Using the Pitman--Yor Process](https://arxiv.org/abs/2602.08347)
*Takato Hashino,Koji Tsukuda*

Main category: stat.ME

TL;DR: 提出基于Pitman-Yor过程的熵估计方法，用于处理物种数量未知情况下的多样性评估，通过贝叶斯非参数方法实现稳定估计


<details>
  <summary>Details</summary>
Motivation: 现有香农熵估计方法通常假设物种数量已知，但在实际应用中这一假设往往不成立。近年来虽有研究尝试放宽此限制，但仍需更稳健的方法来处理物种多样性不确定性

Method: 基于Pitman-Yor过程（贝叶斯非参数代表性方法），将真实分布近似为无限维过程，即使在观测物种数小于真实物种数时也能实现稳定估计

Result: 研究了正则变化分布的香农熵收敛性质，并以此建立了所提估计量的一致性。通过数值实验验证了方法的有效性

Conclusion: 该方法为处理物种多样性不确定性提供了原则性方法，增强了基于熵的多样性评估的可靠性和鲁棒性

Abstract: The Shannon entropy is a fundamental measure for quantifying diversity and model complexity in fields such as information theory, ecology, and genetics. However, many existing studies assume that the number of species is known, an assumption that is often unrealistic in practice. In recent years, efforts have been made to relax this restriction. Motivated by these developments, this study proposes an entropy estimation method based on the Pitman--Yor process, a representative approach in Bayesian nonparametrics. By approximating the true distribution as an infinite-dimensional process, the proposed method enables stable estimation even when the number of observed species is smaller than the true number of species. This approach provides a principled way to deal with the uncertainty in species diversity and enhances the reliability and robustness of entropy-based diversity assessment. In addition, we investigate the convergence property of the Shannon entropy for regularly varying distributions and use this result to establish the consistency of the proposed estimator. Finally, we demonstrate the effectiveness of the proposed method through numerical experiments.

</details>


### [8] [Adaptive Markovian Spatiotemporal Transfer Learning in Multivariate Bayesian Modeling](https://arxiv.org/abs/2602.08544)
*Luca Presicce,Sudipto Banerjee*

Main category: stat.ME

TL;DR: 提出一种基于矩阵变量高斯分布、动态线性模型和贝叶斯预测堆叠的高效多元时空模型在线学习方法，通过时间分片间的信息共享和马尔可夫依赖结构实现可扩展的时空建模。


<details>
  <summary>Details</summary>
Motivation: 解决高维时空现象建模中的计算挑战，传统方法在维度增加时计算复杂度迅速上升，需要开发能够有效传播信息、整合空间组件并适应动态数据环境的可扩展方法。

Method: 使用矩阵变量高斯分布和动态线性模型构建马尔可夫依赖结构，通过贝叶斯预测堆叠在时间分片间高效共享信息，结合顺序和并行处理，采用前向同化后向平滑策略改进后验估计。

Result: 实现了精确推理和增强的互操作性，提高了时空建模的可扩展性和适应性，能够有效处理动态、多元和数据丰富的环境，支持复杂依赖模式的高维建模。

Conclusion: 该方法通过创新的计算框架推进了时空建模的可扩展性，为处理大规模时空数据提供了高效解决方案，特别适用于需要实时更新和复杂依赖建模的应用场景。

Abstract: This manuscript develops computationally efficient online learning for multivariate spatiotemporal models. The method relies on matrix-variate Gaussian distributions, dynamic linear models, and Bayesian predictive stacking to efficiently share information across temporal data shards. The model facilitates effective information propagation over time while seamlessly integrating spatial components within a dynamic framework, building a Markovian dependence structure between datasets at successive time instants. This structure supports flexible, high-dimensional modeling of complex dependence patterns, as commonly found in spatiotemporal phenomena, where computational challenges arise rapidly with increasing dimensions. The proposed approach further manages exact inference through predictive stacking, enhancing accuracy and interoperability. Combining sequential and parallel processing of temporal shards, each unit passes assimilated information forward, then back-smoothed to improve posterior estimates, incorporating all available information. This framework advances the scalability and adaptability of spatiotemporal modeling, making it suitable for dynamic, multivariate, and data-rich environments.

</details>


### [9] [Estimation of log-Gaussian gamma processes with iterated posterior linearization and Hamiltonian Monte Carlo](https://arxiv.org/abs/2602.07454)
*Teemu Härkönen,Simo Särkkä*

Main category: stat.ME

TL;DR: 提出两种基于迭代后验线性化和哈密顿蒙特卡洛的方法，用于采样对数-高斯伽马过程等非高斯误差模型的潜在变量后验分布


<details>
  <summary>Details</summary>
Motivation: 随机过程是统计建模中灵活且广泛使用的模型族，但除了高斯过程外，非高斯误差模型的推断通常难以处理，涉及高维潜在变量的估计

Method: 提出两种方法：使用迭代后验线性化，然后结合哈密顿蒙特卡洛来采样潜在模型的后验分布，特别关注对数-高斯伽马过程

Result: 方法在两种合成数据集（来自对数-高斯伽马过程和多尺度生物复合材料刚度模型）上得到验证，并应用于实验性的黄铁矿拉曼光谱数据

Conclusion: 提出的方法能够有效处理非高斯误差随机过程模型的推断问题，为复杂统计建模提供了可行的计算框架

Abstract: Stochastic processes are a flexible and widely used family of models for statistical modeling. While stochastic processes offer attractive properties such as inclusion of uncertainty properties, their inference is typically intractable, with the notable exception of Gaussian processes. Inference of models with non-Gaussian errors typically involves estimation of a high-dimensional latent variable. We propose two methods that use iterated posterior linearization followed by Hamiltonian Monte Carlo to sample the posterior distributions of such latent models with a particular focus on log-Gaussian gamma processes. The proposed methods are validated with two synthetic datasets generated from the log-Gaussian gamma process and a multiscale biocomposite stiffness model. In addition, we apply the methodology to an experimental Raman spectrum of argentopyrite.

</details>


### [10] [Statistical inference after variable selection in Cox models: A simulation study](https://arxiv.org/abs/2602.07477)
*Lena Schemet,Sarah Friedrich-Welz*

Main category: stat.ME

TL;DR: 该论文研究了在Cox生存模型中，对Lasso和自适应Lasso进行变量选择后的几种统计推断方法，包括样本分割、精确后选择推断和去偏Lasso，通过模拟研究和实际数据评估这些方法在存在删失情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 在生物医学时间-事件数据分析中，变量选择至关重要，但传统的频率推断假设协变量集是预先固定的，不考虑数据驱动的变量选择。这导致后选择推断可能存在偏差，特别是在存在右删失的生存分析中，删失带来的不确定性会进一步加剧这些问题。

Method: 研究了在Cox模型中，对Lasso和自适应Lasso进行变量选择后的几种推断方法：样本分割、精确后选择推断和去偏Lasso。通过模拟研究评估这些方法，模拟设置反映了生物医学应用中常见的协变量结构和删失率，并使用公开可用的生存数据集进行实际应用示例。

Result: 通过模拟研究评估了不同后选择推断方法在反映生物医学应用现实的协变量结构和删失率下的表现。同时通过实际生存数据集的示例，展示了这些方法在实际应用中的行为。

Conclusion: 在Cox生存模型中，进行变量选择后的统计推断需要专门的方法来避免偏差。研究比较了几种后选择推断方法在存在删失情况下的表现，为生物医学时间-事件数据分析提供了方法学参考。

Abstract: Choosing relevant predictors is central to the analysis of biomedical time-to-event data. Classical frequentist inference, however, presumes that the set of covariates is fixed in advance and does not account for data-driven variable selection. As a consequence, naive post-selection inference may be biased and misleading. In right-censored survival settings, these issues may be further exacerbated by the additional uncertainty induced by censoring. We investigate several inference procedures applied after variable selection for the coefficients of the Lasso and its extension, the adaptive Lasso, in the context of the Cox model. The methods considered include sample splitting, exact post-selection inference, and the debiased Lasso. Their performance is examined in a neutral simulation study reflecting realistic covariate structures and censoring rates commonly encountered in biomedical applications. To complement the simulation results, we illustrate the practical behavior of these procedures in an applied example using a publicly available survival dataset.

</details>


### [11] [Event-driven type design for clinical trials with recurrent events](https://arxiv.org/abs/2602.07482)
*Jingwen Zhang,Satoshi Hattori*

Main category: stat.ME

TL;DR: 提出一种针对复发事件结局的事件驱动型设计，通过盲法监测边际率/均值模型的稳健方差，确保目标检验效能，无需考虑其他参数设定


<details>
  <summary>Details</summary>
Motivation: 在随机对照试验中，复发事件分析越来越重要，但存在个体内相关性时，传统事件驱动设计可能因相关性建模困难而导致研究效能不足

Method: 提出事件驱动型设计，在边际率/均值模型下盲法监测稳健方差，确保达到目标检验效能，不受其他参数设定的影响

Result: 模拟研究表明，提出的盲法监测程序能很好地控制检验效能，使检验达到目标效能，且不会导致I类错误率的严重膨胀

Conclusion: 该方法为复发事件结局提供了一种稳健的事件驱动设计，确保目标检验效能，解决了个体内相关性建模困难的问题

Abstract: It is a common practice in randomized clinical trials with the standard survival outcome to follow patients until a prespecified number of events have been observed, a type of trial known as the event-driven trial. The event-driven design ensures that the target power for a specified type 1 error rate is achieved to detect the target hazard ratio, regardless of the specification of other quantities. To understand the treatment effect for chronic conditions, the analysis of recurrent events has gained popularity in randomized controlled trials, particularly large-scale confirmatory trials. In the absence of within-subject correlation among multiple events, a similar event-driven design can be employed for recurrent event outcomes. On the other hand, in the presence of the within-subject correlation, one needs to model the correlation among recurrent events in evaluating power and setting the sample size. However, information useful in modeling the within-subject correlation is limited at the design stage. Failing to consider the correlation properly may lead to underpowered studies. We propose an event-driven type design for recurrent event outcomes. Our method ensures the target power for the target treatment effect, regardless of the specification of other quantities, by monitoring the robust variance under the marginal rates/means model in a blinded manner. We investigate the operating characteristics of the proposed monitoring procedure in simulation studies. The results of simulation studies showed that the proposed blinded monitoring procedure controlled the power well so that the test possessed the target power and did not lead to serious inflation of the type 1 error rate. Furthermore, we illustrate the proposed method using a real clinical trial dataset.

</details>


### [12] [Mapping Drivers of Greenness: Spatial Variable Selection for MODIS Vegetation Indices](https://arxiv.org/abs/2602.07681)
*Qishi Zhan,Cheng-Han Yu,Yuchi Chen,Zhikang Dong,Rajarshi Guhaniyogi*

Main category: stat.ME

TL;DR: 提出了一种空间变系数模型，使用张量积B样条基和贝叶斯组套索先验，能够同时建模空间变化效应并识别重要预测因子。


<details>
  <summary>Details</summary>
Motivation: 在植被研究中，环境驱动因素与植被状况的关系需要空间变化的回归模型，但当许多预测因子不相关时，为每个预测因子估计单独的系数表面会导致噪声模式且难以解释。

Method: 提出空间变系数模型，每个系数表面使用张量积B样条基，并对基系数施加贝叶斯组套索先验，该先验能诱导预测因子层面的收缩，将可忽略的效应推向零同时保留空间结构。

Result: 模拟实验能够恢复稀疏性并实现良好预测。MODIS应用得到了一个简约的预测因子子集，其效应图清晰地展示了跨景观的主导控制因素。

Conclusion: 该方法能够同时建模空间变化效应并识别重要预测因子，为植被研究提供了有效的分析工具，通过空间显著性图和空间覆盖概率提供了不确定性量化。

Abstract: Understanding how environmental drivers relate to vegetation condition motivates spatially varying regression models, but estimating a separate coefficient surface for every predictor can yield noisy patterns and poor interpretability when many predictors are irrelevant. Motivated by MODIS vegetation index studies, we examine predictors from spectral bands, productivity and energy fluxes, observation geometry, and land surface characteristics. Because these relationships vary with canopy structure, climate, land use, and measurement conditions, methods should both model spatially varying effects and identify where predictors matter. We propose a spatially varying coefficient model where each coefficient surface uses a tensor product B-spline basis and a Bayesian group lasso prior on the basis coefficients. This prior induces predictor level shrinkage, pushing negligible effects toward zero while preserving spatial structure. Posterior inference uses Markov chain Monte Carlo and provides uncertainty quantification for each effect surface. We summarize retained effects with spatial significance maps that mark locations where the 95 percent posterior credible interval excludes zero, and we define a spatial coverage probability as the proportion of locations where the credible interval excludes zero. Simulations recover sparsity and achieve prediction. A MODIS application yields a parsimonious subset of predictors whose effect maps clarify dominant controls across landscapes.

</details>


### [13] [Correcting for Nonignorable Nonresponse Bias in Ordinal Observational Survey Data](https://arxiv.org/abs/2602.07704)
*Lukáš Lafférs,Jozef Michal Mintal,Ivan Sutóris*

Main category: stat.ME

TL;DR: 提出一种校正不可忽略无回答偏差的方法，利用受访者中的回答倾向代理变量（如访谈员编码的合作程度）外推至未回答者，同时整合可观测协变量并保留事后分层的优势。


<details>
  <summary>Details</summary>
Motivation: 政治调查通常使用事后分层、加权调整等方法使受访者与目标人群对齐，但当受访者与未回答者在结果变量上存在差异（不可忽略的无回答）时，这些调整方法会失效，导致基本描述性统计产生偏差。

Method: 将Peress(2010)的变量回答倾向(VRP)框架从二元结果推广到序数结果，利用受访者中观察到的回答倾向代理变量（如访谈员编码的合作程度）外推至未回答者，同时整合可观测协变量并保留已知总体份额的事后分层优势。通过最大似然估计计算估计量，并实现为处理序数和二元结果的紧凑R程序。

Result: 使用2024年美国国家选举研究(ANES)数据，发现考虑不可忽略无回答对生活满意度产生实质性影响（估计潜在相关性ρ≈0.49），而对回顾性经济评价影响可忽略（ρ≈0），突显了不可忽略无回答何时实质性影响调查估计。

Conclusion: 该方法提供了一种实用的校正不可忽略无回答偏差的方法，能够识别何时无回答会实质性影响调查估计，为广泛用于测量信任、满意度和政策态度的序数结果提供了有效的偏差校正工具。

Abstract: Many political surveys rely on post-stratification, raking, or related weighting adjustments to align respondents with the target population. But when respondents differ from nonrespondents on the outcome itself (nonignorable nonresponse), these adjustments can fail, introducing bias even into basic descriptives.We provide a practical method that corrects for nonignorable nonresponse by leveraging response-propensity proxies (e.g., interviewer-coded cooperativeness) observed among respondents to extrapolate toward nonrespondents, while directly integrating observable covariates and retaining the benefits of post-stratification with known population shares. The method generalizes the variable-response-propensity (VRP) framework of Peress (2010) from binary to ordinal outcomes, which are widely used to measure trust, satisfaction, and policy attitudes. The resulting estimator is computed by maximum likelihood and implemented in a compact R routine that handles both ordinal and binary outcomes. Using the 2024 American National Election Study (ANES), we show that accounting for nonignorable nonresponse produces substantively meaningful shifts for life satisfaction (estimated latent correlation $ρ\approx 0.49$), while yielding negligible changes for retrospective economic evaluations ($ρ\approx 0$), highlighting when nonignorable nonresponse substantively affects survey estimates.

</details>


### [14] [State policy heterogeneity analyses: considerations and proposals](https://arxiv.org/abs/2602.08643)
*Max Rubinstein,Megan S. Schuler,Elizabeth A. Stuart,Bradley D. Stein,Max Griswold,Elizabeth M. Stone,Beth Ann Griffin*

Main category: stat.ME

TL;DR: 论文区分了州级政策研究中异质性分析的不同因果估计量，指出CATE虽易识别但政策相关性不足，提出通过边界法估计州特定处理效应(ITE)作为替代方法


<details>
  <summary>Details</summary>
Motivation: 州级政策研究中的异质性分析常存在因果估计量与推断目标不匹配的问题，不同政策版本和实施的差异使得传统分析方法难以提供政策相关的推断

Method: 提出边界法估计州特定处理效应(ITE)，在双重差分框架下开发方法，利用预处理数据确定敏感性参数，通过边界而非点估计来推断政策效果

Result: 模拟研究表明，边界法比CATE估计能更可靠地确定ITE的符号；应用分析显示该方法能有效评估平价医疗法案医疗补助扩展对丁丙诺啡处方的影响

Conclusion: 州特定处理效应的边界估计提供了更明确、政策相关的推断目标，能更好地应对州级政策研究中的异质性和实施差异问题

Abstract: State-level policy studies often conduct heterogeneity analyses that quantify how treatment effects vary across state characteristics. These analyses may be used to inform state-specific policy decisions, or to infer how the effect of a policy changes in combination with other state characteristics. However, in state-level settings with varied contexts and policy landscapes, multiple versions of similar policies, and differential policy implementation, the causal quantities targeted by these analyses may not align with the inferential goals. This paper clarifies these issues by distinguishing several causal estimands relevant to heterogeneity analyses in state-policy settings, including state-specific treatment effects (ITE), conditional average treatment effects (CATE), and controlled direct effects (CDE). We argue that the CATE is often the easiest to identify and estimate, but may not be the most policy relevant target of inference. Moreover, the widespread practice of coarsening distinct policies or implementations into a single indicator further complicates the interpretation of these analyses. Motivated by these limitations, we propose bounding ITEs as an alternative inferential goal, yielding ranges for each state's policy effect under explicit assumptions that quantify deviations from the ideal identifying conditions. These bounds target a well-defined and policy-relevant quantity, the effect for specific states. We develop this approach within a difference-in-differences framework and discuss how sensitivity parameters may be informed using pre-treatment data. Through simulations we demonstrate that bounding state-specific effects can more reliably determine the sign of the ITEs than CATE estimates. We then illustrate this method to examine the effect of the Affordable Care Act Medicaid expansion on high-volume buprenorphine prescribing.

</details>


### [15] [Generation of Multivariate Discrete Data with Generalized Poisson, Negative Binomial and Binomial Marginal Distributions](https://arxiv.org/abs/2602.07707)
*Chak Kwong,Cheng,Hakan Demirtas*

Main category: stat.ME

TL;DR: 提出一种生成具有指定相关矩阵的多变量离散数据算法，支持广义泊松、负二项和二项分布


<details>
  <summary>Details</summary>
Motivation: 随着多变量离散数据在流行病学、社会科学、基因组学等领域的应用增加，需要开发强大的数据生成工具来理解变量间关系

Method: 基于先前多变量有序数据生成框架，提出生成广义泊松、负二项和二项分布的多变量离散数据算法，提供逐步算法

Result: 在四个模拟数据场景和三个真实数据场景中验证算法性能，展示其有效性

Conclusion: 该技术有潜力广泛应用于生成相关离散数据的各种场景

Abstract: The analysis of multivariate discrete data is crucial in various scientific research areas, such as epidemiology, the social sciences, genomics, and environmental studies. As the availability of such data increases, developing robust analytical and data generation tools is necessary to understand the relationships among variables. This paper builds upon previous work on data generation frameworks for multivariate ordinal data with a prespecified correlation matrix. The proposed algorithm generates multivariate discrete data from marginal distributions that follow the generalized Poisson, negative binomial, and binomial distributions. A step-by-step algorithm is provided, and its performance is illustrated in four simulated data scenarios and three real-data scenarios. This technique has the potential to be applied in a wide range of settings involving the generation of correlated discrete data.

</details>


### [16] [Hyperbolic statistical inference for Treatment Effects with Circular biomarker of astigmatism](https://arxiv.org/abs/2602.07740)
*Buddhananda Banerjee,Surojit Biswas,Daitari Prusty*

Main category: stat.ME

TL;DR: 提出基于双曲几何的圆形数据两样本检验框架，将von Mises分布参数嵌入庞加莱圆盘，通过双曲距离比较组间分布差异


<details>
  <summary>Details</summary>
Motivation: 圆形生物标志物在生物医学应用中普遍存在（如眼科散光测量、人体旋转研究），需要处理圆形数据的统计推断方法。受白内障手术两种方法诱导散光角度测量数据的启发，需要比较两种手术方法的效果差异。

Method: 将von Mises分布参数空间嵌入庞加莱圆盘（配备庞加莱度量的单位开圆盘），每个分布映射为圆盘中的点。通过双曲距离比较组间分布差异，针对等浓度和不等浓度两种情况分别开发置换检验和自助法检验。

Result: 模拟研究显示该方法具有稳定的经验尺寸、强一致性和优于现有方法的渐近功效。在白内障手术数据集分析中，通过临床信息重构原始观测，展示了方法的实际优势。

Conclusion: 将双曲几何融入圆形生物医学数据分析具有实际优势，几何感知的推断方法对方向性生物标志物研究具有潜力。

Abstract: Circular biomarkers arise naturally in many biomedical applications, particularly in ophthalmology, where angular measurements such as astigmatism are routinely recorded. Similar directional variables also occur in the study of human body rotations, including movements of the hand, waist, neck, and lower limbs. Motivated by a clinical dataset comprising angular measurements of astigmatism induced by two cataract surgery procedures, we propose a novel two-sample testing framework for circular data grounded in hyperbolic geometry. Assuming von Mises distributions with either common or group-specific concentration parameters, we embed the corresponding parameter spaces into the Poincaré disk, an open unit disk endowed with the Poincaré metric.Under this construction, each von Mises distribution is mapped uniquely to a point in the Poincaré disk, yielding a continuous geometric representation that preserves the intrinsic structure of the parameter space. This embedding enables direct comparison of group distributions via hyperbolic distances, leading to natural and interpretable test statistics. We develop permutation-based tests for the common concentration case and bootstrap-based procedures for unequal concentrations. Extensive simulation studies demonstrate stable empirical size, strong consistency, and superior asymptotic power compared with existing competing methods. The proposed methodology is illustrated through a detailed analysis of the cataract surgery dataset, including a clinically informed restructuring of the original observations. The results highlight the practical advantages of incorporating hyperbolic geometry into the analysis of circular biomedical data and underscore the potential of geometry-aware inference for directional biomarkers.

</details>


### [17] [Estimation Strategies for Causal Decomposition Analysis with Allowability Specifications](https://arxiv.org/abs/2602.07825)
*John W. Jackson,Ting-Hsuan Chang,Aster Meche,Trang Q. Nguyen*

Main category: stat.ME

TL;DR: 本文系统介绍了因果分解分析（CDA）的估计方法，提出了避免直接建模密度的"桥接"估计器和多重稳健的加权序贯回归估计器，并提供了模型诊断工具，应用于高血压控制差异研究。


<details>
  <summary>Details</summary>
Motivation: CDA是一种用于模拟干预措施以减少差异的方法，但现有估计方法存在挑战：需要正确建模密度、与常用估计器的区别不明确，这限制了其应用。需要开发更稳健、透明的估计策略。

Method: 提出了两种新型估计器：1)"桥接"估计器，避免直接建模任何密度；2)加权序贯回归估计器，具有多重稳健性。同时提供了评估辅助密度模型和加权函数质量的诊断工具。

Result: 正式建立了估计器对模型误设的稳健性，通过基于真实数据的模拟研究验证了其性能，并应用于大型医疗系统的电子健康记录，研究高血压控制差异。

Conclusion: 本文提出的估计策略解决了CDA实施中的关键挑战，促进了差异研究的稳健性、透明度和对话，有助于更有效地识别减少健康差异的干预焦点。

Abstract: Causal decomposition analysis (CDA) is an approach for modeling the impact of hypothetical interventions to reduce disparities. It is useful for identifying foci that future interventions, including multilevel and multimodal interventions, could focus on to reduce disparities. Based within the potential outcomes framework, CDA has a causal interpretation when the identifying assumptions are met. CDA also allows an analyst to consider which covariates are allowable (i.e., fair) for defining the disparity in the outcome and in the point of intervention, so that its interpretation is also meaningful. While the incorporation of causal inference and allowability promotes robustness, transparency, and dialogue in disparities research, it can lead to challenges in estimation such as the need to correctly model densities. Also, how CDA differs from commonly used estimators may not be clear, which may limit its uptake. To address these challenges, we provide a tour of estimation strategies for CDA, reviewing existing proposals and introducing novel estimators that overcome key estimation challenges. Among them we introduce what we call "bridging" estimators that avoid directly modeling any density, and weighted sequential regression estimators that are multiply robust. Additionally, we provide diagnostics to assess the quality of the nuisance density models and weighting functions they rely on. We formally establish the estimators' robustness to model mis-specification, demonstrate their performance through a simulation study based on real data, and apply them to study disparities in hypertension control using electronic health records in a large healthcare system.

</details>


### [18] [GAAVI: Global Asymptotic Anytime Valid Inference for the Conditional Mean Function](https://arxiv.org/abs/2602.08096)
*Brian M Cho,Raaz Dwivedi,Nathan Kallus*

Main category: stat.ME

TL;DR: 提出了一种新颖的渐近任意时间有效检验方法，用于条件均值函数的全局零假设检验和对比分析，支持实验者在达到最小样本量后的任意时间点做出高置信度决策。


<details>
  <summary>Details</summary>
Motivation: 条件均值函数推断在自适应实验、最优治疗分配和算法公平性审计等任务中至关重要。现有方法无法在连续监测下同时保证渐近第一类错误控制、功效为1和最优样本复杂度。

Method: 开发了渐近任意时间有效检验方法，用于检验条件均值函数的全局零假设（如所有条件均值为零）以及不同条件均值函数之间的对比。通过反演检验构建了条件均值函数及其对比的函数值渐近置信序列。

Result: 在温和条件下，该方法实现了：(i)渐近第一类错误保证，(ii)功效为1，(iii)相对于高斯位置检验的最优样本复杂度。在合成和真实数据上的实验表明，该方法在各种分布下都具有良好的功效，同时在连续监测下保持名义错误率。

Conclusion: 该方法为条件均值函数推断提供了一种强大的任意时间检验框架，能够在连续监测下实现渐近错误率控制、高功效和最优样本效率，适用于自适应实验和算法公平性审计等应用场景。

Abstract: Inference on the conditional mean function (CMF) is central to tasks from adaptive experimentation to optimal treatment assignment and algorithmic fairness auditing. In this work, we provide a novel asymptotic anytime-valid test for a CMF global null (e.g., that all conditional means are zero) and contrasts between CMFs, enabling experimenters to make high confidence decisions at any time during the experiment beyond a minimum sample size. We provide mild conditions under which our tests achieve (i) asymptotic type-I error guarantees, (i) power one, and, unlike past tests, (iii) optimal sample complexity relative to a Gaussian location testing. By inverting our tests, we show how to construct function-valued asymptotic confidence sequences for the CMF and contrasts thereof. Experiments on both synthetic and real-world data show our method is well-powered across various distributions while preserving the nominal error rate under continuous monitoring.

</details>


### [19] [Goodness-of-Fit Tests for Censored and Truncated Data: Maximum Mean Discrepancy Over Regular Functionals](https://arxiv.org/abs/2602.08108)
*Juan Carlos Escanciano,Jacobo de Uña-Álvarez*

Main category: stat.ME

TL;DR: 提出一种针对删失和/或截断数据的参数分布模型的系统化拟合优度检验方法，使用Neyman正交得分过程和核希尔伯特空间聚合，通过乘子bootstrap获得临界值。


<details>
  <summary>Details</summary>
Motivation: 在删失和截断数据中，基于非参数最大似然估计的检验方法存在不存在性、计算不稳定或收敛速度慢等问题，难以在复合零假设下进行可靠校准。

Method: 构建由检验函数索引的Neyman正交得分过程，在再生核希尔伯特空间球上聚合，得到最大均值差异型统计量，使用乘子bootstrap保持nuisance估计固定来获得临界值。

Result: 建立了零假设和局部备择假设下的渐近有效性，为左截断右删失数据、当前状态数据和随机双截断数据提供了具体构造，首次给出了随机双截断下复合假设的参数族拟合优度检验。

Conclusion: 该方法在实践相关的不完全数据设计中表现出良好的尺寸控制和功效，为解决删失/截断数据的参数模型检验问题提供了有效的系统化方法。

Abstract: We develop a systematic, omnibus approach to goodness-of-fit testing for parametric distributional models when the variable of interest is only partially observed due to censoring and/or truncation. In many such designs, tests based on the nonparametric maximum likelihood estimator are hindered by nonexistence, computational instability, or convergence rates too slow to support reliable calibration under composite nulls. We avoid these difficulties by constructing a regular (pathwise differentiable) Neyman-orthogonal score process indexed by test functions, and aggregating it over a reproducing kernel Hilbert space ball. This yields a maximum-mean-discrepancy-type supremum statistic with a convenient quadratic-form representation. Critical values are obtained via a multiplier bootstrap that keeps nuisance estimates fixed. We establish asymptotic validity under the null and local alternatives and provide concrete constructions for left-truncated right-censored data, current status data, and random double truncation; in particular, to the best of our knowledge, we give the first omnibus goodness-of-fit test for a parametric family under random double truncation in the composite-hypothesis case. Simulations and an empirical illustration demonstrate size control and power in practically relevant incomplete-data designs.

</details>


### [20] [Improved Conditional Logistic Regression using Information in Concordant Pairs with Software](https://arxiv.org/abs/2602.08212)
*Jacob Tennenbaum,Adam Kapelner*

Main category: stat.ME

TL;DR: 提出一种改进的条件逻辑回归方法，利用通常被丢弃的"一致响应对"信息构建先验，提高二元处理效应估计的统计功效


<details>
  <summary>Details</summary>
Motivation: 传统条件逻辑回归在估计二元处理对二元响应对数几率的影响时，会丢弃"一致响应对"中的信息，这些信息可能包含有关协变量系数的有用信息

Method: 从一致响应对中学习有关协变量系数的信息，构建信息性先验，然后将该先验应用于不一致对的条件逻辑回归分析

Result: 与标准条件逻辑回归相比，新方法在小样本和非线性对数几率模型中显示出显著的统计功效提升

Conclusion: 通过利用通常被丢弃的一致响应对信息构建先验，可以显著提高条件逻辑回归的统计功效，特别是在小样本和非线性模型中，已实现为优化的R包bclogit

Abstract: We develop an improvement to conditional logistic regression (CLR) in the setting where the parameter of interest is the additive effect of binary treatment effect on log-odds of the positive level in the binary response. Our improvement is simply to use information learned above the nuisance control covariates found in the concordant response pairs' observations (which is usually discarded) to create an informative prior on their coefficients. This prior is then used in the CLR which is run on the discordant pairs. Our power improvements over CLR are most notable in small sample sizes and in nonlinear log-odds-of-positive-response models. Our methods are released in an optimized R package called bclogit.

</details>


### [21] [A Bayesian regression framework for circular models with INLA](https://arxiv.org/abs/2602.08413)
*Xiang Ye,Janet Van Niekerk,Haavard Rue*

Main category: stat.ME

TL;DR: 提出一种新的圆形变量回归方法，解决传统线性预测器破坏圆形变量特性的问题，适用于集中数据，支持混合响应类型和多种协变量，并与INLA框架自然结合实现快速贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 圆形变量的回归模型发展不足，因为传统的线性预测器（协变量和随机效应的线性组合）会破坏圆形变量的自然特性，需要一种能保持圆形性质的新方法。

Method: 提出一种新方法，通过重新定义回归结构来保持圆形变量的特性，该方法自然地扩展到联合回归模型，支持多个圆形和非圆形响应，并能处理线性协变量、圆形协变量和各种随机效应，与INLA框架集成实现贝叶斯推断。

Result: 通过多个模拟和真实案例展示了该方法的有效性，能够为集中数据提供定义良好的圆形响应回归模型，并实现快速准确的贝叶斯推断。

Conclusion: 该方法成功解决了圆形变量回归中的基本问题，提供了一种灵活且计算高效的框架，适用于复杂的多响应场景，为圆形数据分析开辟了新途径。

Abstract: Regression models for circular variables are less developed, since the concept of building a linear predictor from linear combinations of covariates and various random effects, breaks the circular nature of the variable. In this paper, we introduce a new approach to rectify this issue, leading to well-defined regression models for circular responses when the data are concentrated. Our approach extends naturally to joint regression models where we can have several circular and non-circular responses, and allow us to handle a mix of linear covariates, circular covariates and various random effects. Our formulation aligns naturally with the integrated nested Laplace approximation (INLA), which provides fast and accurate Bayesian inference. We illustrate our approach through several simulated and real examples.

</details>


### [22] [Measures for Assessing Causal Effect Heterogeneity Unexplained by Covariates](https://arxiv.org/abs/2602.08647)
*Yuta Kawakami,Jin Tian*

Main category: stat.ME

TL;DR: 提出了针对二元处理和连续结果的P-CACE和N-CACE新异质性度量，以及针对连续处理和连续结果的P-CPICE和N-CPICE度量，建立了识别和边界定理，并应用于真实数据集。


<details>
  <summary>Details</summary>
Motivation: 现有因果效应异质性研究主要关注二元处理和二元结果，即使考虑了协变量后仍可能存在显著的异质性。需要新的度量方法来评估协变量无法解释的因果效应异质性，特别是针对连续结果和连续处理的情况。

Method: 针对二元处理和连续结果，提出了P-CACE（正影响受试者的条件平均因果效应）和N-CACE（负影响受试者的条件平均因果效应）新度量；针对连续处理和连续结果，利用随机干预提出了P-CPICE和N-CPICE度量；建立了这些新度量的识别和边界定理。

Result: 提出了四种新的因果效应异质性度量方法，扩展了研究者能够回答的因果问题范围，特别是针对连续处理和连续结果的情况。建立了这些度量的理论框架和识别条件。

Conclusion: 新提出的P-CACE、N-CACE、P-CPICE和N-CPICE度量能够更好地评估协变量无法解释的因果效应异质性，为研究者提供了更全面的工具来分析二元和连续处理下的因果效应异质性，并在真实数据集中得到应用验证。

Abstract: There has been considerable interest in estimating heterogeneous causal effects across individuals or subpopulations. Researchers often assess causal effect heterogeneity based on the subjects' covariates using the conditional average causal effect (CACE). However, substantial heterogeneity may persist even after accounting for the covariates. Existing work on causal effect heterogeneity unexplained by covariates mainly focused on binary treatment and outcome. In this paper, we introduce novel heterogeneity measures, P-CACE and N-CACE, for binary treatment and continuous outcome that represent CACE over the positively and negatively affected subjects, respectively. We also introduce new heterogeneity measures, P-CPICE and N-CPICE, for continuous treatment and continuous outcome by leveraging stochastic interventions, expanding causal questions that researchers can answer. We establish identification and bounding theorems for these new measures. Finally, we show their application to a real-world dataset.

</details>


### [23] [Regression modeling of multivariate precipitation extremes under regular variation](https://arxiv.org/abs/2602.08865)
*Rishikesh Yadav,Arnab Hazra*

Main category: stat.ME

TL;DR: 提出基于正则变化框架的回归策略，用于估计CESM2大型集合项目中高降水极值的发生频率和强度，该方法通过两阶段回归和预测，在EVA2025数据挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 为应对EVA2025数据挑战赛，需要从CESM2大型集合项目的气候运行数据中估计高降水极值的发生频率和强度。虽然存在季节性模式，但降水强度没有显著的长期趋势，且不同气候模型运行之间数据独立，这简化了建模框架。

Method: 1. 在正则变化框架内采用回归策略；2. 首先在亚渐近（较低阈值）水平上经验估计目标量，并将其设为响应变量；3. 基于联合正则变化的理论表达式建立简单回归框架；4. 估计回归参数后，采用标准预测方法推断极高分位数的降水水平；5. 使用非参数块自助法计算置信区间。

Result: 该方法为所需量提供了合理的估计，在EVA2025会议数据挑战赛的最终排名中获得了并列第二的位置。虽然基于多元极值理论的似然推断可能提供更准确的估计和置信区间，但计算负担显著更高。

Conclusion: 提出的简单且计算直接的两阶段方法能够合理估计高降水极值的发生频率和强度，在计算效率和准确性之间取得了良好平衡，在数据挑战赛中表现优异。

Abstract: Motivated by the EVA2025 data challenge, where we participated as the team DesiBoys, we propose a regression strategy within the framework of regular variation to estimate the occurrences and intensities of high precipitation extremes derived from different climate runs of the CESM2 Large Ensemble Community Project (LENS2). Our approach first empirically estimates the target quantities at sub-asymptotic (lower threshold) levels and sets them as response variables within a simple regression framework arising from the theoretical expressions of joint regular variation. Although a seasonal pattern is evident in the data, the precipitation intensities do not exhibit any significant long-term trends across years. Besides, we can safely assume the data to be independent across different climate model runs, thereby simplifying the modeling framework. Once the regression parameters are estimated, we employ a standard prediction approach to infer precipitation levels at very high quantiles. We calculate the confidence intervals using a nonparametric block bootstrap procedure. While a likelihood-based inference grounded in multivariate extreme value theory may provide more accurate estimates and confidence intervals, it would involve a significantly higher computational burden. Our proposed simple and computationally straightforward two-stage approach provides reasonable estimates for the desired quantities, securing us a joint second position in the final rankings of the EVA2025 conference data challenge competition.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [24] [Fast and Robust Likelihood-Guided Diffusion Posterior Sampling with Amortized Variational Inference](https://arxiv.org/abs/2602.07102)
*Léon Zheng,Thomas Hirtz,Yazid Janati,Eric Moulines*

Main category: stat.ML

TL;DR: 提出一种摊销策略，在扩散后验采样中保持显式似然引导，通过摊销变分扩散后验采样中的内部优化问题，加速推理同时保持对未见退化算子的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 零样本扩散后验采样虽然灵活但计算成本高，而之前的摊销扩散方法虽然推理快但缺乏对未见退化算子的鲁棒性。需要一种方法在效率和灵活性之间取得更好平衡。

Method: 引入扩散后验采样的摊销策略，通过摊销变分扩散后验采样中出现的内部优化问题，保持显式似然引导，加速推理过程。

Result: 该方法在分布内退化算子下加速推理，同时保持对先前未见算子的鲁棒性，改善了基于扩散的逆问题中效率与灵活性之间的权衡。

Conclusion: 提出的摊销策略在扩散后验采样中实现了更好的效率-灵活性平衡，既加速了推理又保持了鲁棒性，为基于扩散的逆问题提供了实用解决方案。

Abstract: Zero-shot diffusion posterior sampling offers a flexible framework for inverse problems by accommodating arbitrary degradation operators at test time, but incurs high computational cost due to repeated likelihood-guided updates. In contrast, previous amortized diffusion approaches enable fast inference by replacing likelihood-based sampling with implicit inference models, but at the expense of robustness to unseen degradations. We introduce an amortization strategy for diffusion posterior sampling that preserves explicit likelihood guidance by amortizing the inner optimization problems arising in variational diffusion posterior sampling. This accelerates inference for in-distribution degradations while maintaining robustness to previously unseen operators, thereby improving the trade-off between efficiency and flexibility in diffusion-based inverse problems.

</details>


### [25] [Discrete Adjoint Matching](https://arxiv.org/abs/2602.07132)
*Oswin So,Brian Karrer,Chuchu Fan,Ricky T. Q. Chen,Guan-Horng Liu*

Main category: stat.ML

TL;DR: 提出离散伴随匹配(DAM)方法，用于在离散状态空间（如扩散大语言模型）中微调生成模型，解决了传统伴随匹配方法在离散域不可微的问题。


<details>
  <summary>Details</summary>
Motivation: 伴随匹配(AM)方法在连续状态空间和可微奖励的奖励优化问题上效果显著，但将其应用于离散生成模型（如扩散大语言模型）面临挑战，因为离散状态空间不可微，这一问题尚未得到充分探索。

Method: 提出离散伴随匹配(DAM)，通过引入离散伴随——在离散域上构建原始问题最优解的估计器，从而应用标准匹配框架。该方法从纯统计角度推导，而非控制理论视角，为基于伴随的估计器开辟了新算法机会。

Result: 在合成任务和数学推理任务上展示了DAM的有效性。

Conclusion: DAM成功将伴随匹配方法扩展到离散生成模型领域，解决了离散状态空间不可微的问题，为离散生成模型的微调提供了有效工具。

Abstract: Computation methods for solving entropy-regularized reward optimization -- a class of problems widely used for fine-tuning generative models -- have advanced rapidly. Among those, Adjoint Matching (AM, Domingo-Enrich et al., 2025) has proven highly effective in continuous state spaces with differentiable rewards. Transferring these practical successes to discrete generative modeling, however, remains particularly challenging and largely unexplored, mainly due to the drastic shift in generative model classes to discrete state spaces, which are nowhere differentiable. In this work, we propose Discrete Adjoint Matching (DAM) -- a discrete variant of AM for fine-tuning discrete generative models characterized by Continuous-Time Markov Chains, such as diffusion-based large language models. The core of DAM is the introduction of discrete adjoint-an estimator of the optimal solution to the original problem but formulated on discrete domains-from which standard matching frameworks can be applied. This is derived via a purely statistical standpoint, in contrast to the control-theoretic viewpoint in AM, thereby opening up new algorithmic opportunities for general adjoint-based estimators. We showcase DAM's effectiveness on synthetic and mathematical reasoning tasks.

</details>


### [26] [Scalable Mean-Field Variational Inference via Preconditioned Primal-Dual Optimization](https://arxiv.org/abs/2602.07632)
*Jinhua Lyu,Tianmin Yu,Ying Ma,Naichen Shi*

Main category: stat.ML

TL;DR: 提出基于增广拉格朗日方法的原始-对偶变分推断算法PD-VI及其块预条件扩展P²D-VI，用于大规模平均场变分推断问题，在合成和真实数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模平均场变分推断(MFVI)问题，传统方法在处理大规模数据时存在收敛速度慢和数值稳定性问题，需要开发更高效、稳健的算法。

Method: 将MFVI重新表述为约束有限和问题，基于增广拉格朗日公式开发原始-对偶变分推断(PD-VI)算法，可扩展地联合更新全局和局部变分参数。进一步提出块预条件扩展P²D-VI，适应不同参数块的损失几何结构。

Result: 在适当选择的恒定步长下，PD-VI和P²D-VI均具有收敛保证：一般设置下达到O(1/T)收敛到稳定点，强凸条件下实现线性收敛。在合成数据和真实大规模空间转录组数据集上的实验表明，该方法在收敛速度和解质量上优于现有随机变分推断方法。

Conclusion: 提出的原始-对偶变分推断框架为大规模平均场变分推断提供了高效、稳健的解决方案，无需共轭假设或显式有界方差条件，在实际应用中表现出优越性能。

Abstract: In this work, we investigate the large-scale mean-field variational inference (MFVI) problem from a mini-batch primal-dual perspective. By reformulating MFVI as a constrained finite-sum problem, we develop a novel primal-dual algorithm based on an augmented Lagrangian formulation, termed primal-dual variational inference (PD-VI). PD-VI jointly updates global and local variational parameters in the evidence lower bound in a scalable manner. To further account for heterogeneous loss geometry across different variational parameter blocks, we introduce a block-preconditioned extension, P$^2$D-VI, which adapts the primal-dual updates to the geometry of each parameter block and improves both numerical robustness and practical efficiency. We establish convergence guarantees for both PD-VI and P$^2$D-VI under properly chosen constant step size, without relying on conjugacy assumptions or explicit bounded-variance conditions. In particular, we prove $O(1/T)$ convergence to a stationary point in general settings and linear convergence under strong convexity. Numerical experiments on synthetic data and a real large-scale spatial transcriptomics dataset demonstrate that our methods consistently outperform existing stochastic variational inference approaches in terms of convergence speed and solution quality.

</details>


### [27] [Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models](https://arxiv.org/abs/2602.07997)
*TrungKhang Tran,TrungTin Nguyen,Md Abul Bashar,Nhat Ho,Richi Nayak,Christopher Drovandi*

Main category: stat.ML

TL;DR: 提出用于softmax门控多项式逻辑MoE的批量MM算法，保证单调上升和全局收敛，并开发基于混合度量树状图的专家数量选择器，在蛋白质相互作用预测中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 对于使用softmax多项式逻辑门控的混合专家模型，在最大似然训练的稳定性保证和原则性模型选择方面存在理论空白，需要解决这两个问题。

Method: 1) 推导使用显式二次下界的批量MM算法，获得坐标闭式更新，保证目标函数单调上升和全局收敛；2) 提出基于混合度量树状图的专家数量选择器，通过合并冗余拟合原子实现近参数最优速率。

Result: 获得条件密度估计和参数恢复的有限样本速率，在蛋白质-蛋白质相互作用预测实验中，相比强统计和机器学习基线，实现了更高的准确率和更好的概率校准。

Conclusion: 为softmax门控多项式逻辑MoE提供了理论保证的批量训练算法和模型选择方法，在生物信息学应用中验证了其优越性能。

Abstract: Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines.

</details>


### [28] [Flow-Based Conformal Predictive Distributions](https://arxiv.org/abs/2602.07633)
*Trevor Harris*

Main category: stat.ML

TL;DR: 提出一种通过确定性流采样高维置信预测集边界的方法，将置信预测扩展到结构化输出空间


<details>
  <summary>Details</summary>
Motivation: 传统置信预测在低维空间容易解释，但在高维或结构化输出空间中难以表示和使用，限制了其在下游任务（如采样和概率预测）中的应用

Method: 利用可微分非一致性分数诱导输出空间上的确定性流，其轨迹收敛到置信预测集边界，实现训练自由的高维边界采样方法

Result: 方法在PDE反问题、降水降尺度、气候模型去偏和飓风轨迹预测等任务中表现良好，能够生成精确的置信预测集和置信预测分布

Conclusion: 提出的确定性流方法为高维置信预测提供了计算高效、训练自由的边界采样技术，扩展了置信预测在复杂输出空间中的应用能力

Abstract: Conformal prediction provides a distribution-free framework for uncertainty quantification via prediction sets with exact finite-sample coverage. In low dimensions these sets are easy to interpret, but in high-dimensional or structured output spaces they are difficult to represent and use, which can limit their ability to integrate with downstream tasks such as sampling and probabilistic forecasting. We show that any differentiable nonconformity score induces a deterministic flow on the output space whose trajectories converge to the boundary of the corresponding conformal prediction set. This leads to a computationally efficient, training-free method for sampling conformal boundaries in arbitrary dimensions. Boundary samples can be reconformalized to form pointwise prediction sets with controlled risk, and mixing across confidence levels yields conformal predictive distributions whose quantile regions coincide exactly with conformal prediction sets. We evaluate the approach on PDE inverse problems, precipitation downscaling, climate model debiasing, and hurricane trajectory forecasting.

</details>


### [29] [On Generation in Metric Spaces](https://arxiv.org/abs/2602.07710)
*Jiaxun Li,Vinod Raman,Ambuj Tewari*

Main category: stat.ML

TL;DR: 论文扩展了语言生成框架到可分度量空间，通过度量分离定义新颖性，引入(ε,ε')-闭包维度来刻画生成性，发现在加倍空间中生成性稳定，但在一般度量空间中高度敏感。


<details>
  <summary>Details</summary>
Motivation: 将Kleinberg和Mullainathan[2024]的可数域语言生成框架扩展到可分度量空间，研究在更一般的度量空间中生成性的数学特性。

Method: 通过度量分离定义新颖性，允许对抗者和生成器的不对称新颖性参数，引入(ε,ε')-闭包维度作为尺度敏感的闭包维度类比。

Result: 在加倍空间（包括所有有限维赋范空间）中，生成性在不同新颖性尺度下稳定且对等价度量不变；但在一般度量空间中，生成性高度尺度敏感且依赖于度量选择，甚至在ℓ²空间中生成性可能随新颖性参数变化而突然失效。

Conclusion: 度量空间的几何结构对生成性有决定性影响：在加倍空间中生成性稳定，但在一般度量空间中生成性高度敏感且可能突然失效，揭示了生成性理论的几何本质。

Abstract: We study generation in separable metric instance spaces. We extend the language generation framework from Kleinberg and Mullainathan [2024] beyond countable domains by defining novelty through metric separation and allowing asymmetric novelty parameters for the adversary and the generator. We introduce the $(\varepsilon,\varepsilon')$-closure dimension, a scale-sensitive analogue of closure dimension, which yields characterizations of uniform and non-uniform generatability and a sufficient condition for generation in the limit. Along the way, we identify a sharp geometric contrast. Namely, in doubling spaces, including all finite-dimensional normed spaces, generatability is stable across novelty scales and invariant under equivalent metrics. In general metric spaces, however, generatability can be highly scale-sensitive and metric-dependent; even in the natural infinite-dimensional Hilbert space $\ell^2$, all notions of generation may fail abruptly as the novelty parameters vary.

</details>


### [30] [Schrödinger bridge problem via empirical risk minimization](https://arxiv.org/abs/2602.08374)
*Denis Belomestny,Alexey Naumov,Nikita Puchkin,Denis Suchkov*

Main category: stat.ML

TL;DR: 提出一种基于学习理论的薛定谔桥问题求解方法，通过经验风险最小化估计变换势函数，而非传统的Sinkhorn迭代平滑方法


<details>
  <summary>Details</summary>
Motivation: 传统薛定谔桥计算方法基于经验测度的Sinkhorn迭代估计薛定谔势，然后通过核平滑对偶解微分构造时变漂移。本文提出一种学习理论替代方案

Method: 将薛定谔系统重写为满足非线性不动点方程的单个正变换势，通过函数类上的经验风险最小化估计该势函数，然后将其插入桥的随机控制表示中生成样本

Result: 在参考核和终端密度的次高斯假设下，建立了经验风险围绕其总体对应物的均匀集中性，并通过数值实验展示了所提方法的性能

Conclusion: 提出了一种基于学习理论的薛定谔桥问题求解框架，为端点分布仅通过样本可得的情况提供了新的计算方法

Abstract: We study the Schrödinger bridge problem when the endpoint distributions are available only through samples. Classical computational approaches estimate Schrödinger potentials via Sinkhorn iterations on empirical measures and then construct a time-inhomogeneous drift by differentiating a kernel-smoothed dual solution. In contrast, we propose a learning-theoretic route: we rewrite the Schrödinger system in terms of a single positive transformed potential that satisfies a nonlinear fixed-point equation and estimate this potential by empirical risk minimization over a function class. We establish uniform concentration of the empirical risk around its population counterpart under sub-Gaussian assumptions on the reference kernel and terminal density. We plug the learned potential into a stochastic control representation of the bridge to generate samples. We illustrate performance of the suggested approach with numerical experiments.

</details>


### [31] [BFTS: Thompson Sampling with Bayesian Additive Regression Trees](https://arxiv.org/abs/2602.07767)
*Ruizhe Deng,Bibhas Chakraborty,Ran Chen,Yan Shuo Tan*

Main category: stat.ML

TL;DR: BFTS将贝叶斯加性回归树集成到Thompson采样中，为上下文赌博机提供首个基于树的概率方法，在理论和实践上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动健康干预需要适应复杂的非线性用户行为。现有方法存在局限：线性模型偏差高，神经网络在线设置中脆弱难调优，树集成方法缺乏概率基础。需要一种既能处理非线性关系又具有概率保证的方法。

Method: 提出贝叶斯森林Thompson采样（BFTS），将贝叶斯加性回归树（BART）这一完全概率的树和模型直接集成到探索循环中。BART提供概率预测和不确定性量化，支持理论上的后悔界分析。

Result: 理论证明BFTS具有信息论贝叶斯后悔界Õ(√T)，其"feel-good"变体达到频率主义极小极大最优性。实证显示在表格基准测试中达到最先进后悔性能，不确定性校准接近名义水平。在Drink Less微随机试验中，离线策略评估显示比部署策略提高30%以上参与率。

Conclusion: BFTS是首个将BART集成到Thompson采样中的上下文赌博机算法，为非线性奖励建模提供了概率框架，在理论和实践上均表现优异，特别适合移动健康干预等行为干预应用。

Abstract: Contextual bandits are a core technology for personalized mobile health interventions, where decision-making requires adapting to complex, non-linear user behaviors. While Thompson Sampling (TS) is a preferred strategy for these problems, its performance hinges on the quality of the underlying reward model. Standard linear models suffer from high bias, while neural network approaches are often brittle and difficult to tune in online settings. Conversely, tree ensembles dominate tabular data prediction but typically rely on heuristic uncertainty quantification, lacking a principled probabilistic basis for TS. We propose Bayesian Forest Thompson Sampling (BFTS), the first contextual bandit algorithm to integrate Bayesian Additive Regression Trees (BART), a fully probabilistic sum-of-trees model, directly into the exploration loop. We prove that BFTS is theoretically sound, deriving an information-theoretic Bayesian regret bound of $\tilde{O}(\sqrt{T})$. As a complementary result, we establish frequentist minimax optimality for a "feel-good" variant, confirming the structural suitability of BART priors for non-parametric bandits. Empirically, BFTS achieves state-of-the-art regret on tabular benchmarks with near-nominal uncertainty calibration. Furthermore, in an offline policy evaluation on the Drink Less micro-randomized trial, BFTS improves engagement rates by over 30% compared to the deployed policy, demonstrating its practical effectiveness for behavioral interventions.

</details>


### [32] [Graph-based Semi-Supervised Learning via Maximum Discrimination](https://arxiv.org/abs/2602.08042)
*Nadav Katz,Ariel Jaffe*

Main category: stat.ML

TL;DR: 提出AUC-spec方法，通过优化AUC来最大化类别分离，在标签数据稀缺时实现更好的半监督学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统图半监督学习方法（如标签传播）在复杂标签分布数据上可能表现不佳，需要一种能更好处理类别分离的方法。

Method: 开发AUC-spec图方法，通过优化ROC曲线下面积（AUC）来计算低维表示，最大化类别分离，并在流形乘积模型下进行理论分析。

Result: 理论证明所需标签点数量与模型参数呈多项式关系；实验显示AUC-spec能平衡类别分离与图平滑性，在合成和真实数据集上表现竞争力，计算效率与传统方法相当。

Conclusion: AUC-spec是一种有效的图半监督学习方法，通过优化AUC实现更好的类别分离，在标签稀缺情况下具有理论保证和实际优势。

Abstract: Semi-supervised learning (SSL) addresses the critical challenge of training accurate models when labeled data is scarce but unlabeled data is abundant. Graph-based SSL (GSSL) has emerged as a popular framework that captures data structure through graph representations. Classic graph SSL methods, such as Label Propagation and Label Spreading, aim to compute low-dimensional representations where points with the same labels are close in representation space. Although often effective, these methods can be suboptimal on data with complex label distributions. In our work, we develop AUC-spec, a graph approach that computes a low-dimensional representation that maximizes class separation. We compute this representation by optimizing the Area Under the ROC Curve (AUC) as estimated via the labeled points. We provide a detailed analysis of our approach under a product-of-manifold model, and show that the required number of labeled points for AUC-spec is polynomial in the model parameters. Empirically, we show that AUC-spec balances class separation with graph smoothness. It demonstrates competitive results on synthetic and real-world datasets while maintaining computational efficiency comparable to the field's classic and state-of-the-art methods.

</details>


### [33] [Information Geometry of Absorbing Markov-Chain and Discriminative Random Walks](https://arxiv.org/abs/2602.08185)
*Masanari Kimura*

Main category: stat.ML

TL;DR: 本文从信息几何角度重新审视判别随机游走，将吸收马尔可夫链中的类特定命中时间分布族视为统计流形，推导了命中时间概率质量函数、矩层次和观测Fisher信息的闭式表达式，并提出了用于主动标签获取、边重加权和解释的敏感性评分。


<details>
  <summary>Details</summary>
Motivation: 判别随机游走是半监督节点分类的简单而强大的工具，但其理论基础仍然零散。本文旨在通过信息几何的视角为DRW建立更坚实的理论框架，理解其统计特性并开发新的应用方法。

Method: 从对数线性边权重模型出发，推导吸收马尔可夫链中类特定命中时间分布的闭式表达式、完整矩层次和观测Fisher信息。利用Fisher矩阵的秩一特性，通过商空间得到低维全局平坦流形。基于几何结构提出敏感性评分，用于量化未标记节点在单位Fisher扰动下的DRW介数最大一阶变化。

Result: 获得了命中时间概率质量函数、矩层次和Fisher信息的闭式表达式。发现每个种子节点的Fisher矩阵是秩一的，通过商空间得到低维全局平坦流形。提出的敏感性评分能够界定（在一维情况下达到）单位Fisher扰动下DRW介数的最大一阶变化。

Conclusion: 信息几何视角为判别随机游走提供了坚实的理论基础，揭示了其统计流形结构。提出的敏感性评分为主动标签获取、边重加权和模型解释提供了原则性策略，扩展了DRW的实际应用能力。

Abstract: Discriminative Random Walks (DRWs) are a simple yet powerful tool for semi-supervised node classification, but their theoretical foundations remain fragmentary. We revisit DRWs through the lens of information geometry, treating the family of class-specific hitting-time laws on an absorbing Markov chain as a statistical manifold. Starting from a log-linear edge-weight model, we derive closed-form expressions for the hitting-time probability mass function, its full moment hierarchy, and the observed Fisher information. The Fisher matrix of each seed node turns out to be rank-one, taking the quotient by its null space yields a low-dimensional, globally flat manifold that captures all identifiable directions of the model. Leveraging the geometry, we introduce a sensitivity score for unlabeled nodes that bounds, and in one-dimensional cases attains, the maximal first-order change in DRW betweenness under unit Fisher perturbations. The score can lead to principled strategies for active label acquisition, edge re-weighting, and explanation.

</details>


### [34] [Discrete Adjoint Schrödinger Bridge Sampler](https://arxiv.org/abs/2602.08243)
*Wei Guo,Yuchen Zhu,Xiaochen Du,Juno Nam,Yongxin Chen,Rafael Gómez-Bombarelli,Guan-Horng Liu,Molei Tao,Jaemoo Choi*

Main category: stat.ML

TL;DR: 将连续域中的伴随匹配（AM）和薛定谔桥采样器（ASBS）扩展到离散空间，提出离散ASBS框架，在训练效率和可扩展性方面具有显著优势


<details>
  <summary>Details</summary>
Motivation: 学习离散神经采样器面临梯度缺失和组合复杂性的挑战。虽然随机最优控制（SOC）和薛定谔桥（SB）提供了理论解决方案，但像伴随匹配（AM）这样在连续域中表现出色的高效SOC求解器在离散空间中尚未探索

Method: 通过揭示AM的核心机制是状态空间无关的，提出离散ASBS框架，将AM和伴随薛定谔桥采样器扩展到离散空间。理论上分析了离散SB问题的最优性条件及其与SOC的联系，确定了状态空间上必要的循环群结构以实现此扩展

Result: 离散ASBS实现了具有竞争力的样本质量，在训练效率和可扩展性方面具有显著优势

Conclusion: 成功将连续域中的伴随匹配和薛定谔桥方法扩展到离散空间，为离散神经采样提供了高效且可扩展的解决方案

Abstract: Learning discrete neural samplers is challenging due to the lack of gradients and combinatorial complexity. While stochastic optimal control (SOC) and Schrödinger bridge (SB) provide principled solutions, efficient SOC solvers like adjoint matching (AM), which excel in continuous domains, remain unexplored for discrete spaces. We bridge this gap by revealing that the core mechanism of AM is $\mathit{state}\text{-}\mathit{space~agnostic}$, and introduce $\mathbf{discrete~ASBS}$, a unified framework that extends AM and adjoint Schrödinger bridge sampler (ASBS) to discrete spaces. Theoretically, we analyze the optimality conditions of the discrete SB problem and its connection to SOC, identifying a necessary cyclic group structure on the state space to enable this extension. Empirically, discrete ASBS achieves competitive sample quality with significant advantages in training efficiency and scalability.

</details>


### [35] [A Statistical Framework for Alignment with Biased AI Feedback](https://arxiv.org/abs/2602.08259)
*Xintao Xia,Zhiqiu Xia,Linjun Zhang,Zhanrui Cai*

Main category: stat.ML

TL;DR: 提出两种去偏对齐方法DDPO和DIPO，通过校正AI评估的系统性偏差，在减少人类标注依赖的同时保持对齐效果接近全人类标注的oracle模型。


<details>
  <summary>Details</summary>
Motivation: 现代对齐流程越来越多地使用LLM-as-Judge替代昂贵的人类偏好标注，但AI标签相比高质量人类反馈存在系统性偏差，需要开发去偏方法。

Method: 提出DDPO（基于残差校正和密度比重加权增强标准DPO）和DIPO（直接估计人类偏好概率，不依赖参数化奖励模型）两种去偏对齐方法。

Result: 在情感生成、摘要和单轮对话任务上的实证研究表明，所提方法显著提升对齐效率，性能接近全人类标注训练的oracle模型。

Conclusion: DDPO提供实用高效的大规模对齐方案，DIPO作为统计最优的稳健替代方案达到半参数效率界，两者都能有效缓解AI评估的系统性偏差。

Abstract: Modern alignment pipelines are increasingly replacing expensive human preference labels with evaluations from large language models (LLM-as-Judge). However, AI labels can be systematically biased compared to high-quality human feedback datasets. In this paper, we develop two debiased alignment methods within a general framework that accommodates heterogeneous prompt-response distributions and external human feedback sources. Debiased Direct Preference Optimization (DDPO) augments standard DPO with a residual-based correction and density-ratio reweighting to mitigate systematic bias, while retaining DPO's computational efficiency. Debiased Identity Preference Optimization (DIPO) directly estimates human preference probabilities without imposing a parametric reward model. We provide theoretical guarantees for both methods: DDPO offers a practical and computationally efficient solution for large-scale alignment, whereas DIPO serves as a robust, statistically optimal alternative that attains the semiparametric efficiency bound. Empirical studies on sentiment generation, summarization, and single-turn dialogue demonstrate that the proposed methods substantially improve alignment efficiency and recover performance close to that of an oracle trained on fully human-labeled data.

</details>


### [36] [Is Flow Matching Just Trajectory Replay for Sequential Data?](https://arxiv.org/abs/2602.08318)
*Soon Hoe Lim,Shizheng Lin,Michael W. Mahoney,N. Benjamin Erichson*

Main category: stat.ML

TL;DR: 论文分析了流匹配（FM）在时间序列生成中的本质，推导了完美函数逼近下FM目标的隐含速度场，揭示了其作为非参数记忆增强连续时间动力系统的性质，并提出了改进ODE生成效率的采样方案。


<details>
  <summary>Details</summary>
Motivation: 流匹配（FM）在时间序列生成中应用日益广泛，但人们对其学习机制理解不足：它究竟是学习了一般的动力结构，还是仅仅执行了有效的"轨迹回放"？本文旨在回答这个根本问题。

Method: 推导了在完美函数逼近下，序列数据上经验FM目标所针对的速度场。针对实践中常用的高斯条件路径，证明了隐含采样器是一个ODE，其动力学构成了非参数记忆增强连续时间动力系统。最优速度场具有闭式解，表示为过去转移诱导的瞬时速度的相似性加权混合。

Result: 最优速度场具有明确的闭式表达式，使数据集依赖性变得显式和可解释。基于最优场的结构，提出了改进ODE生成效率和数值鲁棒性的采样和逼近方案。在非线性动力系统基准测试中，所得闭式采样器无需训练即可直接从历史转移产生强大的概率预测。

Conclusion: 该研究将神经FM模型定位为理想非参数解的参数化替代品，揭示了FM学习的本质是构建记忆增强动力系统，而非简单的轨迹回放。提出的闭式采样方案为时间序列生成提供了更高效和鲁棒的方法。

Abstract: Flow matching (FM) is increasingly used for time-series generation, but it is not well understood whether it learns a general dynamical structure or simply performs an effective "trajectory replay". We study this question by deriving the velocity field targeted by the empirical FM objective on sequential data, in the limit of perfect function approximation. For the Gaussian conditional paths commonly used in practice, we show that the implied sampler is an ODE whose dynamics constitutes a nonparametric, memory-augmented continuous-time dynamical system. The optimal field admits a closed-form expression as a similarity-weighted mixture of instantaneous velocities induced by past transitions, making the dataset dependence explicit and interpretable. This perspective positions neural FM models trained by stochastic optimization as parametric surrogates of an ideal nonparametric solution. Using the structure of the optimal field, we study sampling and approximation schemes that improve the efficiency and numerical robustness of ODE-based generation. On nonlinear dynamical system benchmarks, the resulting closed-form sampler yields strong probabilistic forecasts directly from historical transitions, without training.

</details>


### [37] [Amortising Inference and Meta-Learning Priors in Neural Networks](https://arxiv.org/abs/2602.08782)
*Tommy Rochussen,Vincent Fortuin*

Main category: stat.ML

TL;DR: 该论文提出了一种从多个数据集中学习权重先验的方法，通过结合贝叶斯深度学习和概率元学习，解决了贝叶斯深度学习中先验信念缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯深度学习中面临的核心挑战是如何在没有先验信念的情况下进行贝叶斯更新。传统方法难以表示预测任务的先验分布，特别是在模型参数层面。

Method: 提出了一种基于神经过程的模型，其潜在变量是贝叶斯神经网络的权重集合，解码器由潜在变量样本参数化的神经网络组成。通过跨数据集学习权重先验，实现了每个数据集的摊销变分推断。

Result: 该方法能够在良好指定的先验下研究贝叶斯神经网络的行为，将贝叶斯神经网络用作灵活的生成模型，并在神经过程中实现之前难以实现的功能，如任务内小批量处理或极端数据稀缺下的元学习。

Conclusion: 通过将贝叶斯深度学习和概率元学习相结合，成功解决了贝叶斯深度学习中先验信念缺失的问题，为贝叶斯神经网络提供了更灵活和实用的应用框架。

Abstract: One of the core facets of Bayesianism is in the updating of prior beliefs in light of new evidence$\text{ -- }$so how can we maintain a Bayesian approach if we have no prior beliefs in the first place? This is one of the central challenges in the field of Bayesian deep learning, where it is not clear how to represent beliefs about a prediction task by prior distributions over model parameters. Bridging the fields of Bayesian deep learning and probabilistic meta-learning, we introduce a way to $\textit{learn}$ a weights prior from a collection of datasets by introducing a way to perform per-dataset amortised variational inference. The model we develop can be viewed as a neural process whose latent variable is the set of weights of a BNN and whose decoder is the neural network parameterised by a sample of the latent variable itself. This unique model allows us to study the behaviour of Bayesian neural networks under well-specified priors, use Bayesian neural networks as flexible generative models, and perform desirable but previously elusive feats in neural processes such as within-task minibatching or meta-learning under extreme data-starvation.

</details>


### [38] [Cutting Through the Noise: On-the-fly Outlier Detection for Robust Training of Machine Learning Interatomic Potentials](https://arxiv.org/abs/2602.08849)
*Terry C. W. Lam,Niamh O'Neill,Christoph Schran,Lars L. Schaaf*

Main category: stat.ML

TL;DR: 提出一种在线异常检测方法，通过指数移动平均跟踪损失分布，自动降低噪声样本权重，无需额外参考计算即可训练稳健的机器学习原子间势能模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习原子间势能的准确性受数值噪声影响，这些噪声来自未收敛或不一致的电子结构计算。现有缓解策略（如手动过滤或迭代优化）需要大量专家工作或多轮昂贵重训练，难以扩展到大型数据集。

Method: 引入在线异常检测方案，通过指数移动平均跟踪损失分布，在单次训练运行中自动识别并降低噪声样本权重，无需额外参考计算。

Result: 该方法防止过拟合，性能与迭代优化基线相当但开销显著降低。从未收敛参考数据中恢复液态水的准确物理观测值（包括扩散系数），在SPICE数据集上训练有机化学基础模型时，能量误差降低三倍。

Conclusion: 该框架为在不同规模的不完美数据集上训练稳健模型提供了简单、自动化的解决方案，显著提高了机器学习原子间势能的训练效率和准确性。

Abstract: The accuracy of machine learning interatomic potentials suffers from reference data that contains numerical noise. Often originating from unconverged or inconsistent electronic-structure calculations, this noise is challenging to identify. Existing mitigation strategies such as manual filtering or iterative refinement of outliers, require either substantial expert effort or multiple expensive retraining cycles, making them difficult to scale to large datasets. Here, we introduce an on-the-fly outlier detection scheme that automatically down-weights noisy samples, without requiring additional reference calculations. By tracking the loss distribution via an exponential moving average, this unsupervised method identifies outliers throughout a single training run. We show that this approach prevents overfitting and matches the performance of iterative refinement baselines with significantly reduced overhead. The method's effectiveness is demonstrated by recovering accurate physical observables for liquid water from unconverged reference data, including diffusion coefficients. Furthermore, we validate its scalability by training a foundation model for organic chemistry on the SPICE dataset, where it reduces energy errors by a factor of three. This framework provides a simple, automated solution for training robust models on imperfect datasets across dataset sizes.

</details>


### [39] [Winner's Curse Drives False Promises in Data-Driven Decisions: A Case Study in Refugee Matching](https://arxiv.org/abs/2602.08892)
*Hamsa Bastani,Osbert Bastani,Bryce McLaughlin*

Main category: stat.ML

TL;DR: 模型驱动的政策评估方法存在赢家诅咒问题，即使满足常见假设条件，仍会产生虚假的乐观效果估计。


<details>
  <summary>Details</summary>
Motivation: 数据驱动决策中的政策评估面临准确性挑战，当前流行的模型驱动评估方法存在系统性偏差，导致过度乐观的结果估计，但这一缺陷在管理科学文献中被广泛忽视。

Method: 通过文献综述（分析55篇管理科学论文）识别问题，理论分析证明赢家诅咒的存在性，以及基于难民匹配问题的仿真研究验证理论发现。

Result: 理论分析显示即使模型准确、数据随机、模型设定正确且使用样本分割，赢家诅咒仍会导致虚假的大幅收益报告。仿真研究在真实效应为零的情况下，模型方法仍报告60%的稳定收益，与文献中报告的22-75%改进相当。

Conclusion: 模型驱动的政策评估方法存在根本缺陷，常见辩护理由无法避免赢家诅咒问题，需要更可靠的评估方法。

Abstract: A major challenge in data-driven decision-making is accurate policy evaluation-i.e., guaranteeing that a learned decision-making policy achieves the promised benefits. A popular strategy is model-based policy evaluation, which estimates a model from data to infer counterfactual outcomes. This strategy is known to produce unwarrantedly optimistic estimates of the true benefit due to the winner's curse. We searched the recent literature on data-driven decision-making, identifying a sample of 55 papers published in the Management Science in the past decade; all but two relied on this flawed methodology. Several common justifications are provided: (1) the estimated models are accurate, stable, and well-calibrated, (2) the historical data uses random treatment assignment, (3) the model family is well-specified, and (4) the evaluation methodology uses sample splitting. Unfortunately, we show that no combination of these justifications avoids the winner's curse. First, we provide a theoretical analysis demonstrating that the winner's curse can cause large, spurious reported benefits even when all these justifications hold. Second, we perform a simulation study based on the recent and consequential data-driven refugee matching problem. We construct a synthetic refugee matching environment (calibrated to closely match the real setting) but designed so that no assignment policy can improve expected employment compared to random assignment. Model-based methods report large, stable gains of around 60% even when the true effect is zero; these gains are on par with improvements of 22-75% reported in the literature. Our results provide strong evidence against model-based evaluation.

</details>


### [40] [Online monotone density estimation and log-optimal calibration](https://arxiv.org/abs/2602.08927)
*Rohan Hore,Ruodu Wang,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 在线单调密度估计问题研究，提出两种在线估计器：经典Grenander估计器的在线版本和基于专家聚合的指数加权方法。在正确设定的随机设置下，证明了累积对数似然差距的O(n^{1/3})界，并为专家聚合估计器建立了√(n log n)路径后悔界。应用于序列假设检验中的p-to-e校准器构建。


<details>
  <summary>Details</summary>
Motivation: 研究在线单调密度估计问题，其中密度估计器必须从顺序观测数据中以可预测的方式构建。传统离线方法无法适应数据流式到达的场景，需要开发能够在线更新、具有理论保证的单调密度估计方法。

Method: 提出两种在线估计器：1) 经典Grenander估计器的在线版本；2) 基于指数加权方法的专家聚合估计器，受在线学习文献启发。在正确设定的随机设置下分析性能，建立理论界限，并将方法应用于序列假设检验中的p-to-e校准器构建。

Result: 在正确设定的随机设置下，在线估计器与真实密度之间的期望累积对数似然差距具有O(n^{1/3})界。专家聚合估计器相对于事后选择的最佳离线单调估计器具有√(n log n)路径后悔界。将方法应用于构建经验自适应的p-to-e校准器并证明其最优性。

Conclusion: 成功开发了两种在线单调密度估计器，建立了理论性能保证，并将方法应用于序列假设检验中的校准器构建问题。数值实验验证了理论结果，展示了方法的实用性和有效性。

Abstract: We study the problem of online monotone density estimation, where density estimators must be constructed in a predictable manner from sequentially observed data. We propose two online estimators: an online analogue of the classical Grenander estimator, and an expert aggregation estimator inspired by exponential weighting methods from the online learning literature. In the well-specified stochastic setting, where the underlying density is monotone, we show that the expected cumulative log-likelihood gap between the online estimators and the true density admits an $O(n^{1/3})$ bound. We further establish a $\sqrt{n\log{n}}$ pathwise regret bound for the expert aggregation estimator relative to the best offline monotone estimator chosen in hindsight, under minimal regularity assumptions on the observed sequence. As an application of independent interest, we show that the problem of constructing log-optimal p-to-e calibrators for sequential hypothesis testing can be formulated as an online monotone density estimation problem. We adapt the proposed estimators to build empirically adaptive p-to-e calibrators and establish their optimality. Numerical experiments illustrate the theoretical results.

</details>


### [41] [Provably robust learning of regression neural networks using $β$-divergences](https://arxiv.org/abs/2602.08933)
*Abhik Ghosh,Suryasis Jana*

Main category: stat.ML

TL;DR: 提出基于β散度的稳健回归神经网络学习框架rRNet，具有理论收敛保证、有界影响函数和最优50%渐近崩溃点，优于传统均方误差方法。


<details>
  <summary>Details</summary>
Motivation: 传统回归神经网络使用均方误差训练对异常值和数据污染高度敏感，现有稳健方法范围有限且缺乏理论保证，需要更稳健的理论框架。

Method: 提出基于β散度（密度幂散度）的rRNet框架，适用于广泛的回归神经网络，包括非光滑激活函数和误差密度模型，通过交替优化方案实现。

Result: 理论证明：在温和可验证条件下收敛到稳定点；参数估计和预测器的影响函数有界；对所有β∈(0,1]达到最优50%渐近崩溃点；实验显示在函数逼近和噪声预测任务中优于现有方法。

Conclusion: rRNet为回归神经网络提供了具有理论保证的稳健学习框架，结合了局部和全局稳健性，填补了现有方法在理论保证方面的空白。

Abstract: Regression neural networks (NNs) are most commonly trained by minimizing the mean squared prediction error, which is highly sensitive to outliers and data contamination. Existing robust training methods for regression NNs are often limited in scope and rely primarily on empirical validation, with only a few offering partial theoretical guarantees. In this paper, we propose a new robust learning framework for regression NNs based on the $β$-divergence (also known as the density power divergence) which we call `rRNet'. It applies to a broad class of regression NNs, including models with non-smooth activation functions and error densities, and recovers the classical maximum likelihood learning as a special case. The rRNet is implemented via an alternating optimization scheme, for which we establish convergence guarantees to stationary points under mild, verifiable conditions. The (local) robustness of rRNet is theoretically characterized through the influence functions of both the parameter estimates and the resulting rRNet predictor, which are shown to be bounded for suitable choices of the tuning parameter $β$, depending on the error density. We further prove that rRNet attains the optimal 50\% asymptotic breakdown point at the assumed model for all $β\in(0, 1]$, providing a strong global robustness guarantee that is largely absent for existing NN learning methods. Our theoretical results are complemented by simulation experiments and real-data analyses, illustrating practical advantages of rRNet over existing approaches in both function approximation problems and prediction tasks with noisy observations.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [42] [Digital exclusion among middle-aged and older adults in China: age-period-cohort evidence from three national surveys, 2011-2022](https://arxiv.org/abs/2602.07785)
*Yufei Zhang,Zhihao Ma*

Main category: stat.AP

TL;DR: 该研究使用多源全国代表性调查数据，通过分层年龄-时期-队列模型分析中国中老年人数字排斥问题，发现年龄增长、农村居住、西部地区、多病共存和认知风险是主要风险因素，城乡数字鸿沟未见缩小。


<details>
  <summary>Details</summary>
Motivation: 在中国人口老龄化和数字化转型的背景下，老年人数字排斥问题日益突出。研究旨在解析年龄、时期和出生队列对中老年人数字排斥的影响，为制定针对性干预措施提供依据。

Method: 使用CHARLS(2011-2020)、CFPS(2010-2022)和CGSS(2010-2021)三个全国代表性调查数据，采用加权分层年龄-时期-队列模型，并通过城乡居住地、地区、多病共存和认知风险进行异质性分析，使用APC边界分析评估稳健性。

Result: 数字排斥随年龄增长而增加，呈现轻度非线性特征（中年略有缓解，老年急剧上升）；时期效应在2010年代至2020年代初有所下降；1950年代出生队列存在额外风险；农村、西部地区居民以及多病共存、认知风险人群数字排斥更严重；城乡数字鸿沟未见缩小，认知风险差距扩大。

Conclusion: 数字包容是确保老年人在数字化社会中保持参与的重要途径。研究揭示了需要针对特定风险群体（农村、西部、多病、认知风险）制定差异化政策，特别需要关注城乡数字鸿沟的持续存在和认知风险差距的扩大趋势。

Abstract: Amid China's ageing and digital shift, digital exclusion among older adults poses an urgent challenge. To unpack this phenomenon, this study disentangles age, period, and cohort effects on digital exclusion among middle-aged and older Chinese adults. Using three nationally representative surveys (CHARLS 2011-2020, CFPS 2010-2022, and CGSS 2010-2021), we fitted hierarchical age-period-cohort (HAPC) models weighted by cross-sectional survey weights and stabilized inverse probability weights for item response. We further assessed heterogeneity by urban-rural residence, region, multimorbidity, and cognitive risk, and evaluated robustness with APC bounding analyses. Across datasets, digital exclusion increased with age and displayed mild non-linearity, with a small midlife easing followed by a sharper rise at older ages. Period effects declined over the 2010s and early 2020s, although the pace of improvement differed across survey windows. Cohort deviations were present but less consistent than age and period patterns, with an additional excess risk concentrated among cohorts born in the 1950s. Rural and western residents, as well as adults with multimorbidity or cognitive risk, remained consistently more excluded. Over the study period, the urban-rural divide showed no evidence of narrowing, whereas the cognitive-risk gap widened. These findings highlight digital inclusion as a vital pathway for older adults to remain integral participants in an evolving digital society.

</details>


### [43] [Adaptive Test Procedure for High Dimensional Regression Coefficient](https://arxiv.org/abs/2602.07911)
*Ping Zhao,Fengyi Song,Huifang Ma*

Main category: stat.AP

TL;DR: 提出一个统一的L-统计量测试框架，用于高维回归系数，能适应未知稀疏性。该框架通过排序坐标证据度量并聚合前k个信号，桥接了经典的最大值和求和型测试。


<details>
  <summary>Details</summary>
Motivation: 在高维回归分析中，需要开发能够适应未知稀疏性的测试方法。传统方法如最大值测试（max-type）和求和测试（sum-type）各有优缺点：最大值测试对稀疏替代假设有效但对密集替代假设效果差，求和测试则相反。需要一种统一的框架来桥接这两种方法，并能自适应地处理不同稀疏程度的替代假设。

Method: 开发统一的L-统计量测试框架：1）对坐标证据度量进行排序；2）聚合前k个信号；3）建立极值分量和标准化L-统计量的联合弱收敛理论；4）利用渐近独立性证明可以组合多个k值；5）通过dyadic网格上的Cauchy组合构建自适应omnibus测试；6）提供具有理论保证的wild bootstrap校准方法。

Result: 建立了L-统计量的理论性质：在温和条件下证明了极值分量和标准化L-统计量的联合弱收敛，以及它们之间的渐近独立性。模拟实验表明，该方法在各种稀疏和密集替代假设下（包括非高斯设计）都能保持准确的尺寸控制和强大的检验功效。

Conclusion: 提出的L-统计量框架为高维回归系数测试提供了一个统一的自适应方法，能够有效处理未知稀疏性。该方法通过桥接经典的最大值和求和型测试，在各种替代假设下都表现出色，并具有理论保证和实用的bootstrap校准。

Abstract: We develop a unified $L$-statistic testing framework for high-dimensional regression coefficients that adapts to unknown sparsity. The proposed statistics rank coordinate-wise evidence measures and aggregate the top $k$ signals, bridging classical max-type and sum-type tests. We establish joint weak convergence of the extreme-value component and standardized $L$-statistics under mild conditions, yielding an asymptotic independence that justifies combining multiple $k$'s. An adaptive omnibus test is constructed via a Cauchy combination over a dyadic grid of $k$, and a wild bootstrap calibration is provided with theoretical guarantees. Simulations demonstrate accurate size and strong power across sparse and dense alternatives, including non-Gaussian designs.

</details>


### [44] [Analysis of Repairable Systems Availability with Lindley Failure and Repair Behavior](https://arxiv.org/abs/2602.07935)
*Afshin Yaghoubi*

Main category: stat.AP

TL;DR: 本文提出使用更灵活的Lindley分布替代传统的指数分布进行系统可维护性分析，通过相型分布表示，推导了单组件系统的闭式解，并推广到串并联系统，数值研究显示非指数修复时间对可靠性指标有显著影响。


<details>
  <summary>Details</summary>
Motivation: 传统马尔可夫方法依赖指数分布假设，这在现实中往往不切实际，限制了可维护性分析的准确性。本文旨在克服这一关键限制，提供更广泛适用的分析框架。

Method: 使用Lindley分布（通过相型分布表示）建模系统可维护性。首先分析单组件系统，推导时间依赖和稳态可用性、平均修复时间的闭式表达式，然后推广到n个独立同分布组件的串并联系统配置。

Result: 建立了基于Lindley分布的可维护性分析框架，数值研究比较了Lindley分布和指数分布下的系统性能，证明非指数修复时间对关键可靠性指标有显著的实际影响。

Conclusion: 本文提供了一个更灵活、更广泛适用的分析框架，成功放宽了限制性的指数分布假设，为准确的可维护性评估提供了更大的现实性。

Abstract: Maintainability analysis is a cornerstone of reliability engineering. While the Markov approach is the classical analytical foundation, its reliance on the exponential distribution for failure and repair times is a major and often unrealistic limitation. This paper directly overcomes this critical constraint by investigating and modeling system maintainability using the more flexible and versatile Lindley distribution, which is represented via phase-type distributions. We first present a comprehensive maintainability analysis of a single-component system, deriving precise closed-form expressions for its time-dependent and steady-state availability, as well as the mean time to repair. The core methodology is then systematically generalized to analyze common series and parallel system configurations with n independent and identically distributed components. A dedicated numerical study compares the system performance under the Lindley and exponential distributions, conclusively demonstrating the significant and practical impact of non-exponential repair times on key reliability metrics. Our work provides a versatile and more widely applicable analytical framework for accurate maintainability assessment that successfully relaxes the restrictive exponential assumption, thereby offering greater realism in reliability modeling.

</details>


### [45] [A Unified Server Quality Metric for Tennis](https://arxiv.org/abs/2602.08083)
*Aiwen Li,Amrita Balajee,Harry Wieand,Jonathan Pipping-Gamón*

Main category: stat.AP

TL;DR: 开发了专门评估网球发球质量的Server Quality Scores (SQS)指标，通过发球速度、变化和落点等特征，比传统Elo评分能更好地预测发球效率


<details>
  <summary>Details</summary>
Motivation: 传统网球评分系统（如Elo）只能评估球员整体实力，无法独立衡量发球的价值。需要开发专门指标来分离发球质量与整体表现

Method: 使用温网和美网的逐分数据，建立逻辑混合效应模型，包含发球速度、速度变异性和落点特征，采用交叉的发球方和接发方随机截距来捕捉未观测到的球员效应

Result: SQS在样本外测试中比加权Elo更能预测发球效率（三拍内赢球概率），但在预测整体发球胜率方面与Elo各有优劣，没有明显优势

Conclusion: 发球专项指标是对整体评分的补充，能为教练指导、比赛预测和球员评估提供有价值的洞察

Abstract: Traditional tennis rating systems, such as Elo, summarize overall player strength but do not isolate the independent value of serving. Using point-by-point data from Wimbledon and the U.S. Open, we develop serve-specific player metrics to isolate serving quality from overall performance. For each tournament and gender, we fit logistic mixed-effects models using serve speed, speed variability, and placement features, with crossed server and returner random intercepts capturing unobserved server and returner-strength effects. We use these models to estimate Server Quality Scores (SQS) that reflect players' serving ability. In out-of-sample tests, SQS shows stronger alignment with serve efficiency (measured as points won within three shots) than weighted Elo. Associations with overall serve win percentage are smaller and mixed across datasets, and neither SQS nor wElo consistently dominates on that outcome. These findings highlight that serve-specific metrics complement holistic ratings and provide actionable insight for coaching, forecasting, and player evaluation.

</details>


### [46] [Learning from Literature: Integrating LLMs and Bayesian Hierarchical Modeling for Oncology Trial Design](https://arxiv.org/abs/2602.08172)
*Guannan Gong,Satrajit Roychoudhury,Allison Meisner,Lajos Pusztai,Sarah B Goldberg,Wei Wei*

Main category: stat.AP

TL;DR: LEAD-ONC是一个AI辅助框架，可将已发表的临床试验报告转化为定量、设计相关的证据，用于支持肿瘤学试验设计。


<details>
  <summary>Details</summary>
Motivation: 现代肿瘤学试验设计需要综合先前研究的证据来指导假设生成和样本量确定。基于不完整或不精确总结的试验设计可能导致假设错误设定和研究效能不足，产生假阳性或假阴性结论。

Method: LEAD-ONC使用大语言模型从专家筛选的试验出版物中提取基线特征，从Kaplan-Meier曲线重建个体患者数据，然后通过贝叶斯分层模型为目标试验人群生成预测性生存分布。

Result: 在5项一线非小细胞肺癌III期试验中，基于基线特征的聚类识别出三个临床可解释的组织学定义人群。对于混合组织学人群的随机试验，LEAD-ONC预测中位总生存期差异为2.8个月（95%可信区间-2.0至7.6），获得至少3个月益处的概率约为0.45。

Conclusion: LEAD-ONC作为初步演示，展示了该框架支持循证肿瘤学试验设计的潜力，但仍在积极开发中，这些结果不应被视为确定的临床结论。

Abstract: Designing modern oncology trials requires synthesizing evidence from prior studies to inform hypothesis generation and sample size determination. Trial designs based on incomplete or imprecise summaries can lead to misspecified hypotheses and underpowered studies, resulting in false positive or negative conclusions. To address this challenge, we developed LEAD-ONC (Literature to Evidence for Analytics and Design in Oncology), an AI-assisted framework that transforms published clinical trial reports into quantitative, design-relevant evidence. Given expert-curated trial publications that meet prespecified eligibility criteria, LEAD-ONC uses large language models to extract baseline characteristics and reconstruct individual patient data from Kaplan-Meier curves, followed by Bayesian hierarchical modeling to generate predictive survival distributions for a prespecified target trial population. We demonstrate the framework using five phase III trials in first-line non-small-cell lung cancer evaluating PD-1 or PD-L1 inhibitors with or without CTLA-4 blockade. Clustering based on baseline characteristics identified three clinically interpretable populations defined by histology. For a prospective randomized trial in the mixed-histology population comparing mono versus dual immune checkpoint inhibition, LEAD-ONC projected a modest median overall survival difference of 2.8 months (95 percent credible interval -2.0 to 7.6) and an estimated probability of at least a 3-month benefit of approximately 0.45. As LEAD-ONC remains under active development, these results are intended as preliminary demonstrations of the frameworks potential to support evidence-driven oncology trial design rather than definitive clinical conclusions.

</details>


### [47] [Temporal Trends in Incidence of Dementia in a Birth Cohorts Analysis of the Framingham Heart Study](https://arxiv.org/abs/2602.08414)
*Paula Staudt,Anika Schlosser,Annika Möhl,Martin Schumacher,Nadine Binder*

Main category: stat.AP

TL;DR: 本研究使用多状态模型分析Framingham心脏研究数据，发现痴呆症风险在过去40年间并未下降，女性终生痴呆风险显著高于男性。


<details>
  <summary>Details</summary>
Motivation: 尽管全球痴呆患者数量在1990-2016年间翻倍，但一些研究显示痴呆风险下降，这可能源于传统分析方法未能充分处理因死亡导致的疾病信息缺失偏差。本研究旨在重新评估痴呆发病率趋势。

Method: 采用多状态建模框架处理区间删失的疾病-死亡数据，将参与者分为三个出生队列（1915-1924、1925-1934、1935-1944），以年龄为时间尺度评估痴呆患病率和风险趋势，并估计按性别分层的年龄条件性痴呆概率。

Result: 3828名参与者中731人被诊断为痴呆。多状态模型分析显示，无论性别，痴呆风险在不同出生队列间均无时间下降趋势。经教育水平调整后，女性终生年龄条件性风险（46%-50%）始终高于男性（30%-34%）。

Conclusion: 建议在队列研究中结合多状态方法和出生队列分层来充分估计疾病风险趋势，并传达患者相关的结局指标如年龄条件性疾病风险。

Abstract: Background: Dementia leads to a high burden of disability and the number of dementia patients worldwide doubled between 1990 and 2016. Nevertheless, some studies indicated a decrease in dementia risk which may be due to a bias caused by conventional analysis methods that do not adequately account for missing disease information due to death.
  Methods: This study re-examines potential trends in dementia incidence over four decades in the Framingham Heart Study. We apply a multistate modeling framework tailored to interval-censored illness-death data and define three non-overlapping birth cohorts (1915-1924, 1925-1934, and 1935-1944). Trends are evaluated based on both dementia prevalence and dementia risk, using age as the underlying timescale. Additionally, age-conditional dementia probabilities stratified by sex are estimated.
  Results: A total of 731 out of 3828 individuals were diagnosed with dementia. The multistate model analysis revealed no temporal decline in dementia risk across birth cohorts, irrespective of sex. When stratified by sex and adjusted for education, women consistently exhibited higher lifetime age-conditional risks (46%-50%) than men (30%-34%) over the study period.
  Conclusions: We recommend using a combination of multistate approach and separation into birth cohorts to adequately estimate trends of disease risk in cohort studies as well as to communicate patient-relevant outcomes such age-conditional disease risks.

</details>


### [48] [Accessibility and Serviceability Assessment to Inform Offshore Wind Energy Development and Operations off the U.S. East Coast](https://arxiv.org/abs/2602.08787)
*Cory Petersen,Feng Ye,Jiaxiang Ji,Josh Kohut,Ahmed Aziz Ezzat,David Saginaw,Avril Montanti,Jack Cammarota*

Main category: stat.AP

TL;DR: 该研究为美国东海岸海上风电项目提供了高分辨率可及性评估，并提出了新的"可服务性"指标，强调考虑船舶航行路径而非单点位置，以更准确地评估海上运维成本。


<details>
  <summary>Details</summary>
Motivation: 海上风电项目的经济成功依赖于对建设和运维成本的准确预测，这些预测需要考虑恶劣海洋气象条件带来的物流复杂性。现有研究通常只评估特定站点的可及性，而忽略了船舶从港口到站点往返整个航行路径的实际操作条件。

Method: 研究采用高分辨率海洋气象数据，提出了新的"可服务性"操作指标，该指标评估船舶从港口到海上资产往返整个路径的可及性。同时，研究结合数值模拟数据和观测数据，采用统计处理方法减少偏差。

Result: 分析显示，即使对于邻近的海上位置，可及性和可服务性也存在高度时空变化。研究发现仅依赖数值海洋气象数据会引入显著偏差，需要结合观测数据进行统计处理。高分辨率海洋气象信息对支持海上运维具有重要价值。

Conclusion: 可服务性指标比传统可及性评估更贴近实际海上操作需求，因为它考虑了整个航行路径。研究强调了高分辨率海洋气象数据和模型对海上风电等海上运维活动的重要性，需要结合数值和观测数据以减少评估偏差。

Abstract: The economic success of offshore wind energy projects relies on accurate projections of the construction, and operations and maintenance (O&M) costs. These projections must consider the logistical complexities introduced by adverse met-ocean conditions that can prohibit access to the offshore assets for sustained periods of time. In response, the goal of this study is two-fold: (1) to provide high-resolution estimates of the accessibility of key offshore wind energy areas in the United States (U.S.) East Coast--a region with significant offshore wind energy potential; and (2) to introduce a new operational metric, called serviceability, as motivated by the need to assess the accessibility of an offshore asset along a vessel travel path, rather than at a specific site, as commonly carried out in the literature. We hypothesize that serviceability is more relevant to offshore operations than accessibility, since it more realistically reflects the success and safety of a vessel operation along its journey from port to site and back. Our analysis reveals high temporal and spatial variations in accessibility and serviceability, even for proximate offshore locations. We also find that solely relying on numerical met-ocean data can introduce considerable bias in estimating accessibility and serviceability, raising the need for a statistical treatment that combines both numerical and observational data sources, such as the one proposed herein. Collectively, our analysis sheds light on the value of high-resolution met-ocean information and models in supporting offshore operations, including but not limited to future offshore wind energy developments.

</details>
