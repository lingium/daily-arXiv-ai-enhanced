{"id": "2510.00073", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH", "68T05", "G.3"], "pdf": "https://arxiv.org/pdf/2510.00073", "abs": "https://arxiv.org/abs/2510.00073", "authors": ["Zhekai Li", "Tianyi Ma", "Cheng Hua", "Ruihao Zhu"], "title": "Identifying All ε-Best Arms in (Misspecified) Linear Bandits", "comment": "80 pages (33 pages for main text), 12 figures, 3 tables", "summary": "Motivated by the need to efficiently identify multiple candidates in high\ntrial-and-error cost tasks such as drug discovery, we propose a near-optimal\nalgorithm to identify all {\\epsilon}-best arms (i.e., those at most {\\epsilon}\nworse than the optimum). Specifically, we introduce LinFACT, an algorithm\ndesigned to optimize the identification of all {\\epsilon}-best arms in linear\nbandits. We establish a novel information-theoretic lower bound on the sample\ncomplexity of this problem and demonstrate that LinFACT achieves instance\noptimality by matching this lower bound up to a logarithmic factor. A key\ningredient of our proof is to integrate the lower bound directly into the\nscaling process for upper bound derivation, determining the termination round\nand thus the sample complexity. We also extend our analysis to settings with\nmodel misspecification and generalized linear models. Numerical experiments,\nincluding synthetic and real drug discovery data, demonstrate that LinFACT\nidentifies more promising candidates with reduced sample complexity, offering\nsignificant computational efficiency and accelerating early-stage exploratory\nexperiments."}
{"id": "2510.00076", "categories": ["stat.ML", "cs.CR", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00076", "abs": "https://arxiv.org/abs/2510.00076", "authors": ["Xin Lyu"], "title": "Private Learning of Littlestone Classes, Revisited", "comment": "Comments welcome", "summary": "We consider online and PAC learning of Littlestone classes subject to the\nconstraint of approximate differential privacy. Our main result is a private\nlearner to online-learn a Littlestone class with a mistake bound of\n$\\tilde{O}(d^{9.5}\\cdot \\log(T))$ in the realizable case, where $d$ denotes the\nLittlestone dimension and $T$ the time horizon. This is a doubly-exponential\nimprovement over the state-of-the-art [GL'21] and comes polynomially close to\nthe lower bound for this task.\n  The advancement is made possible by a couple of ingredients. The first is a\nclean and refined interpretation of the ``irreducibility'' technique from the\nstate-of-the-art private PAC-learner for Littlestone classes [GGKM'21]. Our new\nperspective also allows us to improve the PAC-learner of [GGKM'21] and give a\nsample complexity upper bound of $\\widetilde{O}(\\frac{d^5\n\\log(1/\\delta\\beta)}{\\varepsilon \\alpha})$ where $\\alpha$ and $\\beta$ denote\nthe accuracy and confidence of the PAC learner, respectively. This improves\nover [GGKM'21] by factors of $\\frac{d}{\\alpha}$ and attains an optimal\ndependence on $\\alpha$.\n  Our algorithm uses a private sparse selection algorithm to \\emph{sample} from\na pool of strongly input-dependent candidates. However, unlike most previous\nuses of sparse selection algorithms, where one only cares about the utility of\noutput, our algorithm requires understanding and manipulating the actual\ndistribution from which an output is drawn. In the proof, we use a sparse\nversion of the Exponential Mechanism from [GKM'21] which behaves nicely under\nour framework and is amenable to a very easy utility proof."}
{"id": "2510.00367", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G08"], "pdf": "https://arxiv.org/pdf/2510.00367", "abs": "https://arxiv.org/abs/2510.00367", "authors": ["Dehao Dai", "Jianqing Fan", "Yihong Gu", "Debarghya Mukherjee"], "title": "CINDES: Classification induced neural density estimator and simulator", "comment": "50 pages, 1 figure", "summary": "Neural network-based methods for (un)conditional density estimation have\nrecently gained substantial attention, as various neural density estimators\nhave outperformed classical approaches in real-data experiments. Despite these\nempirical successes, implementation can be challenging due to the need to\nensure non-negativity and unit-mass constraints, and theoretical understanding\nremains limited. In particular, it is unclear whether such estimators can\nadaptively achieve faster convergence rates when the underlying density\nexhibits a low-dimensional structure. This paper addresses these gaps by\nproposing a structure-agnostic neural density estimator that is (i)\nstraightforward to implement and (ii) provably adaptive, attaining faster rates\nwhen the true density admits a low-dimensional composition structure. Another\nkey contribution of our work is to show that the proposed estimator integrates\nnaturally into generative sampling pipelines, most notably score-based\ndiffusion models, where it achieves provably faster convergence when the\nunderlying density is structured. We validate its performance through extensive\nsimulations and a real-data application."}
{"id": "2510.00128", "categories": ["stat.ME", "62K99"], "pdf": "https://arxiv.org/pdf/2510.00128", "abs": "https://arxiv.org/abs/2510.00128", "authors": ["Connor T. Jerzak", "Adel Daoud"], "title": "Remote Auditing: Design-based Tests of Randomization, Selection, and Missingness with Broadly Accessible Satellite Imagery", "comment": "21 pages, 5 figures", "summary": "Randomized controlled trials (RCTs) are the benchmark for causal inference,\nyet field implementation can deviate. We here present a remote audit - a\ndesign-based, preregistrable diagnostic that uses only pre-treatment satellite\nimagery to test whether assignment is independent of local conditions. The\nconditional randomization test of the remote audit evaluates whether treatment\nassignment is more predictable from pre-treatment satellite features than\nexpected under the experiment's registered mechanism, providing a finite-sample\nvalid, design-based diagnostic that requires no parametric assumptions. The\nprocedure is finite-sample valid, honors blocks and clusters, and controls\nmultiplicity across image models and resolutions via a max-statistic. We\nillustrate with two RCTs: Uganda's Youth Opportunities Program, where the audit\ncorroborates randomization and flags selection and missing-data risks; and a\nschool-based trial in Bangladesh, where assignment is highly predictable from\npre-treatment features relative to the stated design, consistent with\nindependent concerns about irregularities. Remote audits complement balance\ntests, lower early-stage costs, and enable rapid design checks when baseline\nsurveys are expensive or infeasible."}
{"id": "2510.00533", "categories": ["stat.CO", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.00533", "abs": "https://arxiv.org/abs/2510.00533", "authors": ["Nazeela Aimen", "Patricio Maturana-Russel", "Avi Vajpeyi", "Nelson Christensen", "Renate Meyer"], "title": "Bayesian power spectral density estimation for LISA noise based on P-splines with a parametric boost", "comment": null, "summary": "Flexible and efficient noise characterization is crucial for the precise\nestimation of gravitational wave parameters. We introduce a fast and accurate\nBayesian method for estimating the power spectral density (PSD) of long,\nstationary time series tailored specifically for LISA data analysis. Our\napproach models the PSD as a geometric mean of a parametric and a nonparametric\ncomponent, combining the computational efficiency of parametric models with the\nflexibility to capture deviations from theoretical expectations. The\nnonparametric component is expressed by a mixture of penalized B-splines.\nAdaptive, data-driven knot placement performed once during initialization\neliminates computationally expensive reversible-jump Markov Chain Monte Carlo,\nwhile hierarchical roughness penalty priors prevent overfitting. This design\nyields stable, flexible PSD estimates with runtimes of minutes instead of\nhours. Validation on simulated autoregressive AR(4) data demonstrates estimator\nconsistency. It shows that well-matched parametric components reduce the\nintegrated absolute error compared to an uninformative baseline, requiring\nfewer spline knots to achieve comparable accuracy. Applied to a year of\nsimulated LISA $X$-channel noise, our method achieves relative integrated\nabsolute errors of $\\mathcal{O}(10^{-2})$ with computation times less than\nthree minutes, which makes it suitable for iterative analysis pipelines and\nmulti-year mission datasets."}
{"id": "2510.00463", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.00463", "abs": "https://arxiv.org/abs/2510.00463", "authors": ["Daofu Zhang", "Mehrdad Pournaderi", "Hanne M. Clifford", "Yu Xiang", "Pramod K. Varshney"], "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection", "comment": null, "summary": "This paper studies the adversarial robustness of conformal novelty detection.\nIn particular, we focus on AdaDetect, a powerful learning-based framework for\nnovelty detection with finite-sample false discovery rate (FDR) control. While\nAdaDetect provides rigorous statistical guarantees under benign conditions, its\nbehavior under adversarial perturbations remains unexplored. We first formulate\nan oracle attack setting that quantifies the worst-case degradation of FDR,\nderiving an upper bound that characterizes the statistical cost of attacks.\nThis idealized formulation directly motivates a practical and effective attack\nscheme that only requires query access to AdaDetect's output labels. Coupling\nthese formulations with two popular and complementary black-box adversarial\nalgorithms, we systematically evaluate the vulnerability of AdaDetect on\nsynthetic and real-world datasets. Our results show that adversarial\nperturbations can significantly increase the FDR while maintaining high\ndetection power, exposing fundamental limitations of current error-controlled\nnovelty detection methods and motivating the development of more robust\nalternatives."}
{"id": "2510.00217", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.00217", "abs": "https://arxiv.org/abs/2510.00217", "authors": ["Sung Hee Park", "Xin Zhang", "Elizabeth Slate", "Shuchun Sun", "Hai Yao"], "title": "Dimension Reduction for Characterizing Sexual Dimorphism in Biomechanics of the Temporomandibular Joint", "comment": "37 pages, 12 figures, and 9 tables", "summary": "Sexual dimorphism is a critical factor in many biological and medical\nresearch fields. In biomechanics and bioengineering, understanding sex\ndifferences is crucial for studying musculoskeletal conditions such as\ntemporomandibular disorder (TMD). This paper focuses on the association between\nthe craniofacial skeletal morphology and temporomandibular joint (TMJ) related\nmasticatory muscle attachments to discern sex differences. Data were collected\nfrom 10 male and 11 female cadaver heads to investigate sex-specific\nrelationships between the skull and muscles. We propose a conditional\ncross-covariance reduction (CCR) model, designed to examine the dynamic\nassociation between two sets of random variables conditioned on a third binary\nvariable (e.g., sex), highlighting the most distinctive sex-related\nrelationships between skull and muscle attachments in the human cadaver data.\nUnder the CCR model, we employ a sparse singular value decomposition algorithm\nand introduce a sequential permutation for selecting sparsity (SPSS) method to\nselect important variables and to determine the optimal number of selected\nvariables."}
{"id": "2510.01016", "categories": ["stat.CO", "cond-mat.mtrl-sci", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2510.01016", "abs": "https://arxiv.org/abs/2510.01016", "authors": ["Mohammad Ali Seyed Mahmoud", "Dominic Renner", "Ali Khosravani", "Surya R. Kalidindi"], "title": "Sequential Bayesian Inference of the GTN Damage Model Using Multimodal Experimental Data", "comment": "52 pages, 17 figures, Submitted to journal", "summary": "Reliable parameter identification in ductile damage models remains\nchallenging because the salient physics of damage progression are localized to\nsmall regions in material responses, and their signatures are often diluted in\nspecimen-level measurements. Here, we propose a sequential Bayesian Inference\n(BI) framework for the calibration of the Gurson-Tvergaard-Needleman (GTN)\nmodel using multimodal experimental data (i.e., the specimen-level\nforce-displacement (F-D) measurements and the spatially resolved digital image\ncorrelation (DIC) strain fields). This calibration approach builds on a\npreviously developed two-step BI framework that first establishes a\nlow-computational-cost emulator for a physics-based simulator (here, a finite\nelement model incorporating the GTN material model) and then uses the\nexperimental data to sample posteriors for the material model parameters using\nthe Transitional Markov Chain Monte Carlo (T-MCMC). A central challenge to the\nsuccessful application of this BI framework to the present problem arises from\nthe high-dimensional representations needed to capture the salient features\nembedded in the F-D curves and the DIC fields. In this paper, it is\ndemonstrated that Principal Component Analysis (PCA) provides low-dimensional\nrepresentations that make it possible to apply the BI framework to the problem.\nMost importantly, it is shown that the sequence in which the BI is applied has\na dramatic influence on the results obtained. Specifically, it is observed that\napplying BI first on F-D curves and subsequently on the DIC fields produces\nimproved estimates of the GTN parameters. Possible causes for these\nobservations are discussed in this paper, using AA6111 aluminum alloy as a case\nstudy."}
{"id": "2510.00504", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.00504", "abs": "https://arxiv.org/abs/2510.00504", "authors": ["Hong-Yi Wang", "Di Luo", "Tomaso Poggio", "Isaac L. Chuang", "Liu Ziyin"], "title": "A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws", "comment": "preprint", "summary": "When training large-scale models, the performance typically scales with the\nnumber of parameters and the dataset size according to a slow power law. A\nfundamental theoretical and practical question is whether comparable\nperformance can be achieved with significantly smaller models and substantially\nless data. In this work, we provide a positive and constructive answer. We\nprove that a generic permutation-invariant function of $d$ objects can be\nasymptotically compressed into a function of $\\operatorname{polylog} d$ objects\nwith vanishing error. This theorem yields two key implications: (Ia) a large\nneural network can be compressed to polylogarithmic width while preserving its\nlearning dynamics; (Ib) a large dataset can be compressed to polylogarithmic\nsize while leaving the loss landscape of the corresponding model unchanged.\n(Ia) directly establishes a proof of the \\textit{dynamical} lottery ticket\nhypothesis, which states that any ordinary network can be strongly compressed\nsuch that the learning dynamics and result remain unchanged. (Ib) shows that a\nneural scaling law of the form $L\\sim d^{-\\alpha}$ can be boosted to an\narbitrarily fast power law decay, and ultimately to $\\exp(-\\alpha'\n\\sqrt[m]{d})$."}
{"id": "2510.00287", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.00287", "abs": "https://arxiv.org/abs/2510.00287", "authors": ["Wei Li", "Nilanjan Chakraborty", "Robert Lunde"], "title": "Assumption-lean Inference for Network-linked Data", "comment": null, "summary": "We consider statistical inference for network-linked regression problems,\nwhere covariates may include network summary statistics computed for each node.\nIn settings involving network data, it is often natural to posit that latent\nvariables govern connection probabilities in the graph. Since the presence of\nthese latent features makes classical regression assumptions even less tenable,\nwe propose an assumption-lean framework for linear regression with jointly\nexchangeable regression arrays. We establish an analog of the Aldous-Hoover\nrepresentation for such arrays, which may be of independent interest. Moreover,\nwe consider two different projection parameters as potential targets and\nestablish conditions under which asymptotic normality and bootstrap consistency\nhold when commonly used network statistics, including local subgraph\nfrequencies and spectral embeddings, are used as covariates. In the case of\nlinear regression with local count statistics, we show that a bias-corrected\nestimator allows one to target a more natural inferential target under weaker\nsparsity conditions compared to the OLS estimator. Our inferential tools are\nillustrated using both simulated data and real data related to the academic\nclimate of elementary schools."}
{"id": "2510.00734", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.00734", "abs": "https://arxiv.org/abs/2510.00734", "authors": ["Chuntao Chen", "Tapio Helin", "Nuutti Hyvönen", "Yuya Suzuki"], "title": "Approximation of differential entropy in Bayesian optimal experimental design", "comment": "28 pages, 3 figures", "summary": "Bayesian optimal experimental design provides a principled framework for\nselecting experimental settings that maximize obtained information. In this\nwork, we focus on estimating the expected information gain in the setting where\nthe differential entropy of the likelihood is either independent of the design\nor can be evaluated explicitly. This reduces the problem to maximum entropy\nestimation, alleviating several challenges inherent in expected information\ngain computation.\n  Our study is motivated by large-scale inference problems, such as inverse\nproblems, where the computational cost is dominated by expensive likelihood\nevaluations. We propose a computational approach in which the evidence density\nis approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the\ndifferential entropy is evaluated using standard methods without additional\nlikelihood evaluations. We prove that this strategy achieves convergence rates\nthat are comparable to, or better than, state-of-the-art methods for full\nexpected information gain estimation, particularly when the cost of entropy\nevaluation is negligible. Moreover, our approach relies only on mild smoothness\nof the forward map and avoids stronger technical assumptions required in\nearlier work. We also present numerical experiments, which confirm our\ntheoretical findings."}
{"id": "2510.00087", "categories": ["stat.AP", "cs.LG", "math.PR", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.00087", "abs": "https://arxiv.org/abs/2510.00087", "authors": ["Anja Adamov", "Markus Chardonnet", "Florian Krach", "Jakob Heiss", "Josef Teichmann", "Nicholas A. Bokulich"], "title": "Revealing the temporal dynamics of antibiotic anomalies in the infant gut microbiome with neural jump ODEs", "comment": null, "summary": "Detecting anomalies in irregularly sampled multi-variate time-series is\nchallenging, especially in data-scarce settings. Here we introduce an anomaly\ndetection framework for irregularly sampled time-series that leverages neural\njump ordinary differential equations (NJODEs). The method infers conditional\nmean and variance trajectories in a fully path dependent way and computes\nanomaly scores. On synthetic data containing jump, drift, diffusion, and noise\nanomalies, the framework accurately identifies diverse deviations. Applied to\ninfant gut microbiome trajectories, it delineates the magnitude and persistence\nof antibiotic-induced disruptions: revealing prolonged anomalies after second\nantibiotic courses, extended duration treatments, and exposures during the\nsecond year of life. We further demonstrate the predictive capabilities of the\ninferred anomaly scores in accurately predicting antibiotic events and\noutperforming diversity-based baselines. Our approach accommodates unevenly\nspaced longitudinal observations, adjusts for static and dynamic covariates,\nand provides a foundation for inferring microbial anomalies induced by\nperturbations, offering a translational opportunity to optimize intervention\nregimens by minimizing microbial disruptions."}
{"id": "2510.00545", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00545", "abs": "https://arxiv.org/abs/2510.00545", "authors": ["Seokhun Park", "Choeun Kim", "Jihu Lee", "Yunseop Shin", "Insung Kong", "Yongdai Kim"], "title": "Bayesian Neural Networks for Functional ANOVA model", "comment": null, "summary": "With the increasing demand for interpretability in machine learning,\nfunctional ANOVA decomposition has gained renewed attention as a principled\ntool for breaking down high-dimensional function into low-dimensional\ncomponents that reveal the contributions of different variable groups.\nRecently, Tensor Product Neural Network (TPNN) has been developed and applied\nas basis functions in the functional ANOVA model, referred to as ANOVA-TPNN. A\ndisadvantage of ANOVA-TPNN, however, is that the components to be estimated\nmust be specified in advance, which makes it difficult to incorporate\nhigher-order TPNNs into the functional ANOVA model due to computational and\nmemory constraints. In this work, we propose Bayesian-TPNN, a Bayesian\ninference procedure for the functional ANOVA model with TPNN basis functions,\nenabling the detection of higher-order components with reduced computational\ncost compared to ANOVA-TPNN. We develop an efficient MCMC algorithm and\ndemonstrate that Bayesian-TPNN performs well by analyzing multiple benchmark\ndatasets. Theoretically, we prove that the posterior of Bayesian-TPNN is\nconsistent."}
{"id": "2510.00334", "categories": ["stat.ME", "cs.AI", "cs.LG", "stat.AP", "62H22, 62C99, 68T30, 68T37"], "pdf": "https://arxiv.org/pdf/2510.00334", "abs": "https://arxiv.org/abs/2510.00334", "authors": ["Kieran Drury", "Martine J. Barons", "Jim Q. Smith"], "title": "Structural Refinement of Bayesian Networks for Efficient Model Parameterisation", "comment": "38 pages, 10 figures, 3 tables, one appendix", "summary": "Many Bayesian network modelling applications suffer from the issue of data\nscarcity. Hence the use of expert judgement often becomes necessary to\ndetermine the parameters of the conditional probability tables (CPTs)\nthroughout the network. There are usually a prohibitively large number of these\nparameters to determine, even when complementing any available data with expert\njudgements. To address this challenge, a number of CPT approximation methods\nhave been developed that reduce the quantity and complexity of parameters\nneeding to be determined to fully parameterise a Bayesian network. This paper\nprovides a review of a variety of structural refinement methods that can be\nused in practice to efficiently approximate a CPT within a Bayesian network. We\nnot only introduce and discuss the intrinsic properties and requirements of\neach method, but we evaluate each method through a worked example on a Bayesian\nnetwork model of cardiovascular risk assessment. We conclude with practical\nguidance to help Bayesian network practitioners choose an alternative approach\nwhen direct parameterisation of a CPT is infeasible."}
{"id": "2503.13078", "categories": ["stat.ME", "q-bio.GN", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2503.13078", "abs": "https://arxiv.org/abs/2503.13078", "authors": ["Tobias Østmo Hermansen", "Manuela Zucknick", "Zhi Zhao"], "title": "Bayesian Cox model with graph-structured variable selection priors for multi-omics biomarker identification", "comment": null, "summary": "An important goal in cancer research is the survival prognosis of a patient\nbased on a minimal panel of genomic and molecular markers such as genes or\nproteins. Purely data-driven models without any biological knowledge can\nproduce non-interpretable results. We propose a penalized semiparametric\nBayesian Cox model with graph-structured selection priors for sparse\nidentification of multi-omics features by making use of a biologically\nmeaningful graph via a Markov random field (MRF) prior to capturing known\nrelationships between multi-omics features. Since the fixed graph in the MRF\nprior is for the prior probability distribution, it is not a hard constraint to\ndetermine variable selection, so the proposed model can verify known\ninformation and has the potential to identify new and novel biomarkers for\ndrawing new biological knowledge. Our simulation results show that the proposed\nBayesian Cox model with graph-based prior knowledge results in more trustable\nand stable variable selection and non-inferior survival prediction, compared to\nmethods modeling the covariates independently without any prior knowledge. The\nresults also indicate that the performance of the proposed model is robust to a\npartially correct graph in the MRF prior, meaning that in a real setting where\nnot all the true network information between covariates is known, the graph can\nstill be useful. The proposed model is applied to the primary invasive breast\ncancer patients data in The Cancer Genome Atlas project."}
{"id": "2510.00073", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH", "68T05", "G.3"], "pdf": "https://arxiv.org/pdf/2510.00073", "abs": "https://arxiv.org/abs/2510.00073", "authors": ["Zhekai Li", "Tianyi Ma", "Cheng Hua", "Ruihao Zhu"], "title": "Identifying All ε-Best Arms in (Misspecified) Linear Bandits", "comment": "80 pages (33 pages for main text), 12 figures, 3 tables", "summary": "Motivated by the need to efficiently identify multiple candidates in high\ntrial-and-error cost tasks such as drug discovery, we propose a near-optimal\nalgorithm to identify all {\\epsilon}-best arms (i.e., those at most {\\epsilon}\nworse than the optimum). Specifically, we introduce LinFACT, an algorithm\ndesigned to optimize the identification of all {\\epsilon}-best arms in linear\nbandits. We establish a novel information-theoretic lower bound on the sample\ncomplexity of this problem and demonstrate that LinFACT achieves instance\noptimality by matching this lower bound up to a logarithmic factor. A key\ningredient of our proof is to integrate the lower bound directly into the\nscaling process for upper bound derivation, determining the termination round\nand thus the sample complexity. We also extend our analysis to settings with\nmodel misspecification and generalized linear models. Numerical experiments,\nincluding synthetic and real drug discovery data, demonstrate that LinFACT\nidentifies more promising candidates with reduced sample complexity, offering\nsignificant computational efficiency and accelerating early-stage exploratory\nexperiments."}
{"id": "2510.00569", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.ME", "stat.TH", "90C26 (Primary) 15A69, 62F10, 62J05, 62H25 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.00569", "abs": "https://arxiv.org/abs/2510.00569", "authors": ["Ke Xu", "Yuefeng Han"], "title": "Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold", "comment": "33 pages, 7 figures", "summary": "Recovering a low-CP-rank tensor from noisy linear measurements is a central\nchallenge in high-dimensional data analysis, with applications spanning tensor\nPCA, tensor regression, and beyond. We exploit the intrinsic geometry of\nrank-one tensors by casting the recovery task as an optimization problem over\nthe Segre manifold, the smooth Riemannian manifold of rank-one tensors. This\ngeometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent\n(RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at\nevery iteration. Under mild noise assumptions, we prove that RGD converges at a\nlocal linear rate, while RGN exhibits an initial local quadratic convergence\nphase that transitions to a linear rate as the iterates approach the\nstatistical noise floor. Extensive synthetic experiments validate these\nconvergence guarantees and demonstrate the practical effectiveness of our\nmethods."}
{"id": "2510.00431", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.00431", "abs": "https://arxiv.org/abs/2510.00431", "authors": ["Ong Wei Yong", "Lee Shao-Man", "Hsueh Chia-Ming", "Chang Sheng-Mao"], "title": "An Accurate Standard Error Estimation for Quadratic Exponential Logistic Regressions by Applying Generalized Estimating Equations to Pseudo-Likelihoods", "comment": null, "summary": "For a set of binary response variables, conditional mean models characterize\nthe expected value of a response variable given the others and are popularly\napplied in longitudinal and network data analyses. The quadratic exponential\nbinary distribution is a natural choice in this context. However, maximum\nlikelihood estimation of this distribution is computationally demanding due to\nits intractable normalizing constant, while the pseudo-likelihood, though\ncomputationally convenient, tends to severely underestimate the standard\nerrors. In this work, we investigate valid estimation methods for the quadratic\nexponential binary distribution and its regression counterpart. We show that,\nwhen applying the generalized estimating equations to the pseudo-likelihood,\nusing the independence working correlation yields consistent estimates, whereas\nusing dependent structures, such as compound symmetric or autoregressive\ncorrelations, may introduce non-ignorable biases. Theoretical properties are\nderived, supported by simulation studies. For illustration, we apply the\nproposed approach to the carcinogenic toxicity of chemicals data and the\nconstitutional court opinion wringing data."}
{"id": "2510.00334", "categories": ["stat.ME", "cs.AI", "cs.LG", "stat.AP", "62H22, 62C99, 68T30, 68T37"], "pdf": "https://arxiv.org/pdf/2510.00334", "abs": "https://arxiv.org/abs/2510.00334", "authors": ["Kieran Drury", "Martine J. Barons", "Jim Q. Smith"], "title": "Structural Refinement of Bayesian Networks for Efficient Model Parameterisation", "comment": "38 pages, 10 figures, 3 tables, one appendix", "summary": "Many Bayesian network modelling applications suffer from the issue of data\nscarcity. Hence the use of expert judgement often becomes necessary to\ndetermine the parameters of the conditional probability tables (CPTs)\nthroughout the network. There are usually a prohibitively large number of these\nparameters to determine, even when complementing any available data with expert\njudgements. To address this challenge, a number of CPT approximation methods\nhave been developed that reduce the quantity and complexity of parameters\nneeding to be determined to fully parameterise a Bayesian network. This paper\nprovides a review of a variety of structural refinement methods that can be\nused in practice to efficiently approximate a CPT within a Bayesian network. We\nnot only introduce and discuss the intrinsic properties and requirements of\neach method, but we evaluate each method through a worked example on a Bayesian\nnetwork model of cardiovascular risk assessment. We conclude with practical\nguidance to help Bayesian network practitioners choose an alternative approach\nwhen direct parameterisation of a CPT is infeasible."}
{"id": "2510.00287", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.00287", "abs": "https://arxiv.org/abs/2510.00287", "authors": ["Wei Li", "Nilanjan Chakraborty", "Robert Lunde"], "title": "Assumption-lean Inference for Network-linked Data", "comment": null, "summary": "We consider statistical inference for network-linked regression problems,\nwhere covariates may include network summary statistics computed for each node.\nIn settings involving network data, it is often natural to posit that latent\nvariables govern connection probabilities in the graph. Since the presence of\nthese latent features makes classical regression assumptions even less tenable,\nwe propose an assumption-lean framework for linear regression with jointly\nexchangeable regression arrays. We establish an analog of the Aldous-Hoover\nrepresentation for such arrays, which may be of independent interest. Moreover,\nwe consider two different projection parameters as potential targets and\nestablish conditions under which asymptotic normality and bootstrap consistency\nhold when commonly used network statistics, including local subgraph\nfrequencies and spectral embeddings, are used as covariates. In the case of\nlinear regression with local count statistics, we show that a bias-corrected\nestimator allows one to target a more natural inferential target under weaker\nsparsity conditions compared to the OLS estimator. Our inferential tools are\nillustrated using both simulated data and real data related to the academic\nclimate of elementary schools."}
{"id": "2510.00734", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.00734", "abs": "https://arxiv.org/abs/2510.00734", "authors": ["Chuntao Chen", "Tapio Helin", "Nuutti Hyvönen", "Yuya Suzuki"], "title": "Approximation of differential entropy in Bayesian optimal experimental design", "comment": "28 pages, 3 figures", "summary": "Bayesian optimal experimental design provides a principled framework for\nselecting experimental settings that maximize obtained information. In this\nwork, we focus on estimating the expected information gain in the setting where\nthe differential entropy of the likelihood is either independent of the design\nor can be evaluated explicitly. This reduces the problem to maximum entropy\nestimation, alleviating several challenges inherent in expected information\ngain computation.\n  Our study is motivated by large-scale inference problems, such as inverse\nproblems, where the computational cost is dominated by expensive likelihood\nevaluations. We propose a computational approach in which the evidence density\nis approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the\ndifferential entropy is evaluated using standard methods without additional\nlikelihood evaluations. We prove that this strategy achieves convergence rates\nthat are comparable to, or better than, state-of-the-art methods for full\nexpected information gain estimation, particularly when the cost of entropy\nevaluation is negligible. Moreover, our approach relies only on mild smoothness\nof the forward map and avoids stronger technical assumptions required in\nearlier work. We also present numerical experiments, which confirm our\ntheoretical findings."}
{"id": "2510.00432", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.00432", "abs": "https://arxiv.org/abs/2510.00432", "authors": ["Wanyi Ling", "Wufang Hong", "Nikolaos Ignatiadis"], "title": "Empirical partially Bayes two sample testing", "comment": null, "summary": "A common task in high-throughput biology is to test for differences in means\nbetween two samples across thousands of features (e.g., genes or proteins),\noften with only a handful of replicates per sample. Moderated t-tests handle\nthis problem by assuming normality and equal variances, and by applying the\nempirical partially Bayes principle: a prior is posited and estimated for the\nnuisance parameters (variances) but not for the primary parameters (means).\nThis approach has been highly successful in genomics, yet the equal variance\nassumption is often violated in practice. Meanwhile, Welch's unequal variance\nt-test with few replicates suffers from inflated type-I error and low power.\nTaking inspiration from moderated t-tests, we extend the empirical partially\nBayes paradigm to two-sample testing with unequal variances. We develop two\nprocedures: one that models the ratio of the two sample-specific variances and\nanother that models the two variances jointly, with prior distributions\nestimated by nonparametric maximum likelihood. Our empirical partially Bayes\nmethods yield p-values that are asymptotically uniform as the number of\nfeatures grows while the number of replicates remains fixed, ensuring\nasymptotic type-I error control. Simulations and applications to genomic data\ndemonstrate substantial gains in power."}
{"id": "2510.00980", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.00980", "abs": "https://arxiv.org/abs/2510.00980", "authors": ["Yiran Wang", "Martin Lysy", "Audrey Béliveau"], "title": "Rapid Scaling of Compositional Uncertainty from Sample to Population Levels", "comment": "15 Pages, 7 Figures", "summary": "Understanding population composition is essential across ecological,\nevolutionary, conservation, and resource management contexts. Modern methods\nsuch as genetic stock identification (GSI) estimate the proportion of\nindividuals from each subpopulation using genetic data. Ideally, these\nestimates are obtained through mixture analysis, which captures both sampling\nand genetic uncertainty. However, historical datasets often rely on individual\nassignment methods that only account for sample-level uncertainty, limiting the\nvalidity of population-level inferences. To address this, we propose a reverse\nDirichlet-multinomial model and derive multiple variance estimators to\npropagate uncertainty from the sample to the population level. We extend this\nframework to genetic mark-recapture studies, assess performance via simulation,\nand apply our method to estimate the escapement of Sockeye Salmon (Oncorhynchus\nnerka) in the Taku River."}
{"id": "2510.00367", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G08"], "pdf": "https://arxiv.org/pdf/2510.00367", "abs": "https://arxiv.org/abs/2510.00367", "authors": ["Dehao Dai", "Jianqing Fan", "Yihong Gu", "Debarghya Mukherjee"], "title": "CINDES: Classification induced neural density estimator and simulator", "comment": "50 pages, 1 figure", "summary": "Neural network-based methods for (un)conditional density estimation have\nrecently gained substantial attention, as various neural density estimators\nhave outperformed classical approaches in real-data experiments. Despite these\nempirical successes, implementation can be challenging due to the need to\nensure non-negativity and unit-mass constraints, and theoretical understanding\nremains limited. In particular, it is unclear whether such estimators can\nadaptively achieve faster convergence rates when the underlying density\nexhibits a low-dimensional structure. This paper addresses these gaps by\nproposing a structure-agnostic neural density estimator that is (i)\nstraightforward to implement and (ii) provably adaptive, attaining faster rates\nwhen the true density admits a low-dimensional composition structure. Another\nkey contribution of our work is to show that the proposed estimator integrates\nnaturally into generative sampling pipelines, most notably score-based\ndiffusion models, where it achieves provably faster convergence when the\nunderlying density is structured. We validate its performance through extensive\nsimulations and a real-data application."}
{"id": "2510.01093", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01093", "abs": "https://arxiv.org/abs/2510.01093", "authors": ["Wenxiu Feng", "Antonio Alcántara", "Carlos Ruiz"], "title": "Optimal placement of wind farms via quantile constraint learning", "comment": null, "summary": "Wind farm placement arranges the size and the location of multiple wind farms\nwithin a given region. The power output is highly related to the wind speed on\nspatial and temporal levels, which can be modeled by advanced data-driven\napproaches. To this end, we use a probabilistic neural network as a surrogate\nthat accounts for the spatiotemporal correlations of wind speed. This neural\nnetwork uses ReLU activation functions so that it can be reformulated as\nmixed-integer linear set of constraints (constraint learning). We embed these\nconstraints into the placement decision problem, formulated as a two-stage\nstochastic optimization problem. Specifically, conditional quantiles of the\ntotal electricity production are regarded as recursive decisions in the second\nstage. We use real high-resolution regional data from a northern region in\nSpain. We validate that the constraint learning approach outperforms the\nclassical bilinear interpolation method. Numerical experiments are implemented\non risk-averse investors. The results indicate that risk-averse investors\nconcentrate on dominant sites with strong wind, while exhibiting spatial\ndiversification and sensitive capacity spread in non-dominant sites.\nFurthermore, we show that if we introduce transmission line costs in the\nproblem, risk-averse investors favor locations closer to the substations. On\nthe contrary, risk-neutral investors are willing to move to further locations\nto achieve higher expected profits. Our results conclude that the proposed\nnovel approach is able to tackle a portfolio of regional wind farm placements\nand further provide guidance for risk-averse investors."}
{"id": "2510.00558", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.00558", "abs": "https://arxiv.org/abs/2510.00558", "authors": ["Seeun Park", "Hee-Seok Oh"], "title": "A Data-Adaptive Factor Model Using Composite Quantile Approach", "comment": null, "summary": "This paper proposes a data-adaptive factor model (DAFM), a novel framework\nfor extracting common factors that explain the structures of high-dimensional\ndata. DAFM adopts a composite quantile strategy to adaptively capture the full\ndistributional structure of the data, thereby enhancing estimation accuracy and\nrevealing latent patterns that are invisible to conventional factor models. In\nthis paper, we develop a practical algorithm for estimating DAFM by minimizing\nan objective function based on a weighted average of check functions across\nquantiles. We also establish the theoretical properties of the estimators,\nincluding their consistency and convergence rates. Furthermore, we derive their\nasymptotic distributions by introducing approximated estimators from a\nkernel-smoothed objective function, and propose two consistent methods for\ndetermining the number of factors. Simulation studies demonstrate that DAFM\noutperforms existing factor models across different data distributions, and\nreal data analyses on volatility and forecasting further validate its\neffectiveness."}
{"id": "2510.00569", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.ME", "stat.TH", "90C26 (Primary) 15A69, 62F10, 62J05, 62H25 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.00569", "abs": "https://arxiv.org/abs/2510.00569", "authors": ["Ke Xu", "Yuefeng Han"], "title": "Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold", "comment": "33 pages, 7 figures", "summary": "Recovering a low-CP-rank tensor from noisy linear measurements is a central\nchallenge in high-dimensional data analysis, with applications spanning tensor\nPCA, tensor regression, and beyond. We exploit the intrinsic geometry of\nrank-one tensors by casting the recovery task as an optimization problem over\nthe Segre manifold, the smooth Riemannian manifold of rank-one tensors. This\ngeometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent\n(RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at\nevery iteration. Under mild noise assumptions, we prove that RGD converges at a\nlocal linear rate, while RGN exhibits an initial local quadratic convergence\nphase that transitions to a linear rate as the iterates approach the\nstatistical noise floor. Extensive synthetic experiments validate these\nconvergence guarantees and demonstrate the practical effectiveness of our\nmethods."}
{"id": "2510.01098", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01098", "abs": "https://arxiv.org/abs/2510.01098", "authors": ["Blake Bordelon", "Mary I. Letey", "Cengiz Pehlevan"], "title": "Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time", "comment": "preprint with 29 pages", "summary": "We study in-context learning (ICL) of linear regression in a deep linear\nself-attention model, characterizing how performance depends on various\ncomputational and statistical resources (width, depth, number of training\nsteps, batch size and data per context). In a joint limit where data dimension,\ncontext length, and residual stream width scale proportionally, we analyze the\nlimiting asymptotics for three ICL settings: (1) isotropic covariates and tasks\n(ISO), (2) fixed and structured covariance (FS), and (3) where covariances are\nrandomly rotated and structured (RRS). For ISO and FS settings, we find that\ndepth only aids ICL performance if context length is limited. Alternatively, in\nthe RRS setting where covariances change across contexts, increasing the depth\nleads to significant improvements in ICL, even at infinite context length. This\nprovides a new solvable toy model of neural scaling laws which depends on both\nwidth and depth of a transformer and predicts an optimal transformer shape as a\nfunction of compute. This toy model enables computation of exact asymptotics\nfor the risk as well as derivation of powerlaws under source/capacity\nconditions for the ICL tasks."}
{"id": "2510.00598", "categories": ["stat.ME", "math.ST", "stat.TH", "62G10"], "pdf": "https://arxiv.org/pdf/2510.00598", "abs": "https://arxiv.org/abs/2510.00598", "authors": ["Charl Pretorius", "Heinrich Roodt"], "title": "A Weighted Regression Approach to Break-Point Detection in Panel Data", "comment": null, "summary": "New procedures for detecting a change in the cross-sectional mean of panel\ndata are proposed. The procedures rely on estimating nuisance parameters using\ncertain cross-sectional means across panels using a weighted least squares\nregression. In the case of weak cross-sectional dependence between panels, we\nshow how test statistics can be constructed to have a limit null distribution\nnot depending on any choice of bandwidths typically needed to estimate the\nlong-run variances of the panel errors. The theoretical assertions are derived\nfor general choices of the regression weights, and it is shown that consistent\ntest procedures can be obtained from the proposed process. The theoretical\nresults are extended to the case where strong cross-sectional dependence exist\nbetween panels. The paper concludes with a numerical study illustrating the\nbehavior of several special cases of the test procedure in finite samples."}
{"id": "2510.00598", "categories": ["stat.ME", "math.ST", "stat.TH", "62G10"], "pdf": "https://arxiv.org/pdf/2510.00598", "abs": "https://arxiv.org/abs/2510.00598", "authors": ["Charl Pretorius", "Heinrich Roodt"], "title": "A Weighted Regression Approach to Break-Point Detection in Panel Data", "comment": null, "summary": "New procedures for detecting a change in the cross-sectional mean of panel\ndata are proposed. The procedures rely on estimating nuisance parameters using\ncertain cross-sectional means across panels using a weighted least squares\nregression. In the case of weak cross-sectional dependence between panels, we\nshow how test statistics can be constructed to have a limit null distribution\nnot depending on any choice of bandwidths typically needed to estimate the\nlong-run variances of the panel errors. The theoretical assertions are derived\nfor general choices of the regression weights, and it is shown that consistent\ntest procedures can be obtained from the proposed process. The theoretical\nresults are extended to the case where strong cross-sectional dependence exist\nbetween panels. The paper concludes with a numerical study illustrating the\nbehavior of several special cases of the test procedure in finite samples."}
{"id": "2510.00875", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.00875", "abs": "https://arxiv.org/abs/2510.00875", "authors": ["Marco Molinari", "Magne Thoresen"], "title": "False Discovery Rate Control via Bayesian Mirror Statistic", "comment": null, "summary": "Simultaneously performing variable selection and inference in\nhigh-dimensional models is an open challenge in statistics and machine\nlearning. The increasing availability of vast amounts of variables requires the\nadoption of specific statistical procedures to accurately select the most\nimportant predictors in a high-dimensional space, while being able to control\nsome form of selection error. In this work we adapt the Mirror Statistic\napproach to False Discovery Rate (FDR) control into a Bayesian modelling\nframework. The Mirror Statistic, developed in the classic frequentist\nstatistical framework, is a flexible method to control FDR, which only requires\nmild model assumptions, but requires two sets of independent regression\ncoefficient estimates, usually obtained after splitting the original dataset.\nHere we propose to rely on a Bayesian formulation of the model and use the\nposterior distributions of the coefficients of interest to build the Mirror\nStatistic and effectively control the FDR without the need to split the data.\nMoreover, the method is very flexible since it can be used with continuous and\ndiscrete outcomes and more complex predictors, such as with mixed models. We\nkeep the approach scalable to high-dimensions by relying on Automatic\nDifferentiation Variational Inference and fully continuous prior choices."}
{"id": "2510.00611", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.00611", "abs": "https://arxiv.org/abs/2510.00611", "authors": ["Martina Le-Bert Heyl", "Janet van Niekerk", "Haavard Rue"], "title": "Spatial Gaussian fields for complex areas with application to marine megafauna conservation", "comment": null, "summary": "Spatial Gaussian fields (SGFs) are widely employed in modeling the\ndistributions of marine megafauna, yet they traditionally rely on assumptions\nof isotropy and stationarity, conditions that often prove unrealistic in\ncomplex ecological environments featuring coastlines, islands, and depth\ngradients acting as partial movement barriers. Existing spatial models\ntypically treat these barriers as either fully impermeable, completely blocking\nspecies movement and dispersal, or entirely absent, which inadequately\nrepresents most real-world scenarios. To address this limitation, we introduce\nthe Transparent Barrier Model, an extension of spatial Gaussian fields that\nexplicitly incorporates barriers with varying levels of permeability. The model\nassigns spatially varying range parameters to distinct barrier regions,\nallowing ecological and geographical knowledge about barrier permeability to\ndirectly inform model specifications. This approach maintains computational\nefficiency by utilizing the integrated nested Laplace approximation (INLA)\nframework combined with stochastic partial differential equations (SPDEs),\nensuring feasible application even in large, complex spatial domains.We\ndemonstrate the practical utility and flexibility of the Transparent Barrier\nModel through its application to dugong (Dugong dugon) distribution data from\nthe Red Sea."}
{"id": "2510.00875", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.00875", "abs": "https://arxiv.org/abs/2510.00875", "authors": ["Marco Molinari", "Magne Thoresen"], "title": "False Discovery Rate Control via Bayesian Mirror Statistic", "comment": null, "summary": "Simultaneously performing variable selection and inference in\nhigh-dimensional models is an open challenge in statistics and machine\nlearning. The increasing availability of vast amounts of variables requires the\nadoption of specific statistical procedures to accurately select the most\nimportant predictors in a high-dimensional space, while being able to control\nsome form of selection error. In this work we adapt the Mirror Statistic\napproach to False Discovery Rate (FDR) control into a Bayesian modelling\nframework. The Mirror Statistic, developed in the classic frequentist\nstatistical framework, is a flexible method to control FDR, which only requires\nmild model assumptions, but requires two sets of independent regression\ncoefficient estimates, usually obtained after splitting the original dataset.\nHere we propose to rely on a Bayesian formulation of the model and use the\nposterior distributions of the coefficients of interest to build the Mirror\nStatistic and effectively control the FDR without the need to split the data.\nMoreover, the method is very flexible since it can be used with continuous and\ndiscrete outcomes and more complex predictors, such as with mixed models. We\nkeep the approach scalable to high-dimensions by relying on Automatic\nDifferentiation Variational Inference and fully continuous prior choices."}
{"id": "2510.00900", "categories": ["stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.00900", "abs": "https://arxiv.org/abs/2510.00900", "authors": ["Georgia D Tomova", "Richard J Silverwood", "Peter WG Tennant", "Liam Wright"], "title": "How can the use of different modes of survey data collection introduce bias? A simple introduction to mode effects using directed acyclic graphs (DAGs)", "comment": null, "summary": "Survey data are self-reported data collected directly from respondents by a\nquestionnaire or an interview and are commonly used in epidemiology. Such data\nare traditionally collected via a single mode (e.g. face-to-face interview\nalone), but use of mixed-mode designs (e.g. offering face-to-face interview or\nonline survey) has become more common. This introduces two key challenges.\nFirst, individuals may respond differently to the same question depending on\nthe mode; these differences due to measurement are known as 'mode effects'.\nSecond, different individuals may participate via different modes; these\ndifferences in sample composition between modes are known as 'mode selection'.\nWhere recognised, mode effects are often handled by straightforward approaches\nsuch as conditioning on survey mode. However, while reducing mode effects, this\nand other equivalent approaches may introduce collider bias in the presence of\nmode selection. The existence of mode effects and the consequences of na\\\"ive\nconditioning may be underappreciated in epidemiology. This paper offers a\nsimple introduction to these challenges using directed acyclic graphs by\nexploring a range of possible data structures. We discuss the potential\nimplications of using conditioning- or imputation-based approaches and outline\nthe advantages of quantitative bias analyses for dealing with mode effects."}
{"id": "2510.00947", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2510.00947", "abs": "https://arxiv.org/abs/2510.00947", "authors": ["Peiyun Jiang", "Takashi Yamagata"], "title": "An alternative bootstrap procedure for factor-augmented regression models", "comment": null, "summary": "In this paper, we propose a novel bootstrap algorithm that is more efficient\nthan existing methods for approximating the distribution of the\nfactor-augmented regression estimator for a rotated parameter vector. The\nregression is augmented by $r$ factors extracted from a large panel of $N$\nvariables observed over $T$ time periods. We consider general weak factor (WF)\nmodels with $r$ signal eigenvalues that may diverge at different rates,\n$N^{\\alpha _{k}}$, where $0<\\alpha _{k}\\leq 1$ for $k=1,2,...,r$. We establish\nthe asymptotic validity of our bootstrap method using not only the conventional\ndata-dependent rotation matrix $\\hat{\\bH}$, but also an alternative\ndata-dependent rotation matrix, $\\hat{\\bH}_q$, which typically exhibits smaller\nasymptotic bias and achieves a faster convergence rate. Furthermore, we\ndemonstrate the asymptotic validity of the bootstrap under a purely\nsignal-dependent rotation matrix ${\\bH}$, which is unique and can be regarded\nas the population analogue of both $\\hat{\\bH}$ and $\\hat{\\bH}_q$. Experimental\nresults provide compelling evidence that the proposed bootstrap procedure\nachieves superior performance relative to the existing procedure."}
{"id": "2510.00968", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.00968", "abs": "https://arxiv.org/abs/2510.00968", "authors": ["Daisuke Murakami", "Alexis Comber", "Takahiro Yoshida", "Narumasa Tsutsumida", "Chris Brunsdon", "Tomoki Nakaya"], "title": "Local aggregate multiscale processes: A scalable, machine-learning-compatible spatial model", "comment": null, "summary": "This study develops the Local Aggregate Multiscale Process (LAMP), a scalable\nand machine-learning-compatible alternative to conventional spatial Gaussian\nprocesses (GPs, or kriging). Unlike conventional covariance-based spatial\nmodels, LAMP represents spatial processes by a multiscale ensemble of local\nmodels, inspired by geographically weighted regression. To ensure stable model\ntraining, larger-scale patterns that are easier to learn are modeled first,\nfollowed by smaller-scale patterns, with training terminated once the\nvalidation score stops improving. The training procedure, which is based on\nholdout validation, is easily integrated with other machine learning algorithms\n(e.g., random forests and neural networks). LAMP training is computationally\nefficient as it avoids explicit matrix inversion, a major computational\nbottleneck in conventional GPs. Comparative Monte Carlo experiments demonstrate\nthat LAMP, as well as its integration with random forests, achieves superior\npredictive performance compared to existing models. Finally, we apply the\nproposed methods to an analysis of residential land prices in the Tokyo\nmetropolitan area, Japan.\n  The R code is available from available from\nhttps://github.com/dmuraka/spLAMP_dev_version/tree/main"}
{"id": "2510.00980", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.00980", "abs": "https://arxiv.org/abs/2510.00980", "authors": ["Yiran Wang", "Martin Lysy", "Audrey Béliveau"], "title": "Rapid Scaling of Compositional Uncertainty from Sample to Population Levels", "comment": "15 Pages, 7 Figures", "summary": "Understanding population composition is essential across ecological,\nevolutionary, conservation, and resource management contexts. Modern methods\nsuch as genetic stock identification (GSI) estimate the proportion of\nindividuals from each subpopulation using genetic data. Ideally, these\nestimates are obtained through mixture analysis, which captures both sampling\nand genetic uncertainty. However, historical datasets often rely on individual\nassignment methods that only account for sample-level uncertainty, limiting the\nvalidity of population-level inferences. To address this, we propose a reverse\nDirichlet-multinomial model and derive multiple variance estimators to\npropagate uncertainty from the sample to the population level. We extend this\nframework to genetic mark-recapture studies, assess performance via simulation,\nand apply our method to estimate the escapement of Sockeye Salmon (Oncorhynchus\nnerka) in the Taku River."}
{"id": "2510.01127", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01127", "abs": "https://arxiv.org/abs/2510.01127", "authors": ["Bryan S. Blette", "Zhe Chen", "Brennan C. Kahan", "Andrew Forbes", "Michael O. Harhay", "Fan Li"], "title": "Evaluating Informative Cluster Size in Cluster Randomized Trials", "comment": null, "summary": "In cluster randomized trials, the average treatment effect among individuals\n(i-ATE) can be different from the cluster average treatment effect (c-ATE) when\ninformative cluster size is present, i.e., when treatment effects or\nparticipant outcomes depend on cluster size. In such scenarios, mixed-effects\nmodels and generalized estimating equations (GEEs) with exchangeable\ncorrelation structure are biased for both the i-ATE and c-ATE estimands,\nwhereas GEEs with an independence correlation structure or analyses of\ncluster-level summaries are recommended in practice. However, when cluster size\nis non-informative, mixed-effects models and GEEs with exchangeable correlation\nstructure can provide unbiased estimation and notable efficiency gains over\nother methods. Thus, hypothesis tests for informative cluster size would be\nuseful to assess this key phenomenon under cluster randomization. In this work,\nwe develop model-based, model-assisted, and randomization-based tests for\ninformative cluster size in cluster randomized trials. We construct simulation\nstudies to examine the operating characteristics of these tests, show they have\nappropriate Type I error control and meaningful power, and contrast them to\nexisting model-based tests used in the observational study setting. The\nproposed tests are then applied to data from a recent cluster randomized trial,\nand practical recommendations for using these tests are discussed."}
{"id": "2510.00367", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G08"], "pdf": "https://arxiv.org/pdf/2510.00367", "abs": "https://arxiv.org/abs/2510.00367", "authors": ["Dehao Dai", "Jianqing Fan", "Yihong Gu", "Debarghya Mukherjee"], "title": "CINDES: Classification induced neural density estimator and simulator", "comment": "50 pages, 1 figure", "summary": "Neural network-based methods for (un)conditional density estimation have\nrecently gained substantial attention, as various neural density estimators\nhave outperformed classical approaches in real-data experiments. Despite these\nempirical successes, implementation can be challenging due to the need to\nensure non-negativity and unit-mass constraints, and theoretical understanding\nremains limited. In particular, it is unclear whether such estimators can\nadaptively achieve faster convergence rates when the underlying density\nexhibits a low-dimensional structure. This paper addresses these gaps by\nproposing a structure-agnostic neural density estimator that is (i)\nstraightforward to implement and (ii) provably adaptive, attaining faster rates\nwhen the true density admits a low-dimensional composition structure. Another\nkey contribution of our work is to show that the proposed estimator integrates\nnaturally into generative sampling pipelines, most notably score-based\ndiffusion models, where it achieves provably faster convergence when the\nunderlying density is structured. We validate its performance through extensive\nsimulations and a real-data application."}
{"id": "2510.00463", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.00463", "abs": "https://arxiv.org/abs/2510.00463", "authors": ["Daofu Zhang", "Mehrdad Pournaderi", "Hanne M. Clifford", "Yu Xiang", "Pramod K. Varshney"], "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection", "comment": null, "summary": "This paper studies the adversarial robustness of conformal novelty detection.\nIn particular, we focus on AdaDetect, a powerful learning-based framework for\nnovelty detection with finite-sample false discovery rate (FDR) control. While\nAdaDetect provides rigorous statistical guarantees under benign conditions, its\nbehavior under adversarial perturbations remains unexplored. We first formulate\nan oracle attack setting that quantifies the worst-case degradation of FDR,\nderiving an upper bound that characterizes the statistical cost of attacks.\nThis idealized formulation directly motivates a practical and effective attack\nscheme that only requires query access to AdaDetect's output labels. Coupling\nthese formulations with two popular and complementary black-box adversarial\nalgorithms, we systematically evaluate the vulnerability of AdaDetect on\nsynthetic and real-world datasets. Our results show that adversarial\nperturbations can significantly increase the FDR while maintaining high\ndetection power, exposing fundamental limitations of current error-controlled\nnovelty detection methods and motivating the development of more robust\nalternatives."}
{"id": "2510.00569", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.ME", "stat.TH", "90C26 (Primary) 15A69, 62F10, 62J05, 62H25 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.00569", "abs": "https://arxiv.org/abs/2510.00569", "authors": ["Ke Xu", "Yuefeng Han"], "title": "Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold", "comment": "33 pages, 7 figures", "summary": "Recovering a low-CP-rank tensor from noisy linear measurements is a central\nchallenge in high-dimensional data analysis, with applications spanning tensor\nPCA, tensor regression, and beyond. We exploit the intrinsic geometry of\nrank-one tensors by casting the recovery task as an optimization problem over\nthe Segre manifold, the smooth Riemannian manifold of rank-one tensors. This\ngeometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent\n(RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at\nevery iteration. Under mild noise assumptions, we prove that RGD converges at a\nlocal linear rate, while RGN exhibits an initial local quadratic convergence\nphase that transitions to a linear rate as the iterates approach the\nstatistical noise floor. Extensive synthetic experiments validate these\nconvergence guarantees and demonstrate the practical effectiveness of our\nmethods."}
