{"id": "2512.11475", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.11475", "abs": "https://arxiv.org/abs/2512.11475", "authors": ["Shifeng Xiong"], "title": "Candidate set sampling: A note on theoretical guarantees", "comment": null, "summary": "In this note we introduce a simple numerical sampling method, called candidate set sampling, which is based on an straightforward discretization to the density function. This method requires the knowledge of the density function (up to an unknown normalizing constant) only. Furthermore, candidate set sampling is non-iterative, dimension-free, and easy to implement, with fast convergence and low computational cost. We present its basic convergence properties in the note."}
{"id": "2512.11012", "categories": ["stat.ME", "math.PR", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.11012", "abs": "https://arxiv.org/abs/2512.11012", "authors": ["Utku Erdogan", "Gabriel J. Lord", "Joaquin Miguez"], "title": "On a class of constrained Bayesian filters and their numerical implementation in high-dimensional state-space Markov models", "comment": null, "summary": "Bayesian filtering is a key tool in many problems that involve the online processing of data, including data assimilation, optimal control, nonlinear tracking and others. Unfortunately, the implementation of filters for nonlinear, possibly high-dimensional, dynamical systems is far from straightforward, as computational methods have to meet a delicate trade-off involving stability, accuracy and computational cost. In this paper we investigate the design, and theoretical features, of constrained Bayesian filters for state space models. The constraint on the filter is given by a sequence of compact subsets of the state space that determines the sources and targets of the Markov transition kernels in the dynamical model. Subject to such constraints, we provide sufficient conditions for filter stability and approximation error rates with respect to the original (unconstrained) Bayesian filter. Then, we look specifically into the implementation of constrained filters in a continuous-discrete setting where the state of the system is a continuous-time stochastic Itô process but data are collected sequentially over a time grid. We propose an implementation of the constraint that relies on a data-driven modification of the drift of the Itô process using barrier functions, and discuss the relation of this scheme with methods based on the Doob $h$-transform. Finally, we illustrate the theoretical results and the performance of the proposed methods in computer experiments for a partially-observed stochastic Lorenz 96 model."}
{"id": "2512.11707", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.11707", "abs": "https://arxiv.org/abs/2512.11707", "authors": ["John Mahlon Scott", "Hsin-Hsiung Huang"], "title": "Maritime Vessel Tracking", "comment": null, "summary": "The Automatic Identification System (AIS) provides time stamped vessel positions and kinematic reports that enable maritime authorities to monitor traffic. We consider the problem of relabeling AIS trajectories when vessel identifiers are missing, focusing on a challenging nationwide setting in which tracks are heavily downsampled and span diverse operating environments across continental U.S. waters. We propose a hybrid pipeline that first applies a physics-based screening step to project active track endpoints forward in time and select a small set of plausible ancestors for each new observation. A supervised neural classifier then chooses among these candidates, or initiates a new track, using engineered space time and kinematic consistency features. On held out data, this approach improves posit accuracy relative to unsupervised baselines, demonstrating that combining simple motion models with learned disambiguation can scale vessel relabeling to heterogeneous, high volume AIS streams."}
{"id": "2512.10994", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10994", "abs": "https://arxiv.org/abs/2512.10994", "authors": ["Sharvaj Kubal", "Naomi Graham", "Matthieu Heitz", "Andrew Warren", "Michael P. Friedlander", "Yaniv Plan", "Geoffrey Schiebinger"], "title": "STARK denoises spatial transcriptomics images via adaptive regularization", "comment": "34 pages, 10 figures", "summary": "We present an approach to denoising spatial transcriptomics images that is particularly effective for uncovering cell identities in the regime of ultra-low sequencing depths, and also allows for interpolation of gene expression. The method -- Spatial Transcriptomics via Adaptive Regularization and Kernels (STARK) -- augments kernel ridge regression with an incrementally adaptive graph Laplacian regularizer. In each iteration, we (1) perform kernel ridge regression with a fixed graph to update the image, and (2) update the graph based on the new image. The kernel ridge regression step involves reducing the infinite dimensional problem on a space of images to finite dimensions via a modified representer theorem. Starting with a purely spatial graph, and updating it as we improve our image makes the graph more robust to noise in low sequencing depth regimes. We show that the aforementioned approach optimizes a block-convex objective through an alternating minimization scheme wherein the sub-problems have closed form expressions that are easily computed. This perspective allows us to prove convergence of the iterates to a stationary point of this non-convex objective. Statistically, such stationary points converge to the ground truth with rate $\\mathcal{O}(R^{-1/2})$ where $R$ is the number of reads. In numerical experiments on real spatial transcriptomics data, the denoising performance of STARK, evaluated in terms of label transfer accuracy, shows consistent improvement over the competing methods tested."}
{"id": "2512.10994", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10994", "abs": "https://arxiv.org/abs/2512.10994", "authors": ["Sharvaj Kubal", "Naomi Graham", "Matthieu Heitz", "Andrew Warren", "Michael P. Friedlander", "Yaniv Plan", "Geoffrey Schiebinger"], "title": "STARK denoises spatial transcriptomics images via adaptive regularization", "comment": "34 pages, 10 figures", "summary": "We present an approach to denoising spatial transcriptomics images that is particularly effective for uncovering cell identities in the regime of ultra-low sequencing depths, and also allows for interpolation of gene expression. The method -- Spatial Transcriptomics via Adaptive Regularization and Kernels (STARK) -- augments kernel ridge regression with an incrementally adaptive graph Laplacian regularizer. In each iteration, we (1) perform kernel ridge regression with a fixed graph to update the image, and (2) update the graph based on the new image. The kernel ridge regression step involves reducing the infinite dimensional problem on a space of images to finite dimensions via a modified representer theorem. Starting with a purely spatial graph, and updating it as we improve our image makes the graph more robust to noise in low sequencing depth regimes. We show that the aforementioned approach optimizes a block-convex objective through an alternating minimization scheme wherein the sub-problems have closed form expressions that are easily computed. This perspective allows us to prove convergence of the iterates to a stationary point of this non-convex objective. Statistically, such stationary points converge to the ground truth with rate $\\mathcal{O}(R^{-1/2})$ where $R$ is the number of reads. In numerical experiments on real spatial transcriptomics data, the denoising performance of STARK, evaluated in terms of label transfer accuracy, shows consistent improvement over the competing methods tested."}
{"id": "2512.11063", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11063", "abs": "https://arxiv.org/abs/2512.11063", "authors": ["Luis FS Castro-de-Araujo", "Nathan Gillespie", "Michael C Neale", "Timothy Bates"], "title": "umx version 4.5: Extending Twin and Path-Based SEM in R with CLPM, MR-DoC, Definition Variables, $Ω$nyx Integration, and Censored distributions", "comment": "13 pages, 2 figures", "summary": "Structural Equation Modeling (SEM) provides a powerful and flexible framework widely used in behavioral genetics and social sciences. Building on the original design of the umx package, which enhanced accessibility to OpenMx using concise syntax and helpful defaults, umx v4.5 significantly extends functionality for longitudinal and causal twin designs while improving interoperability with graphical modelling tools such as Onyx. New capabilities include: classic and modern cross-lagged panel model; Mendelian Randomization Direction-of-Causation (MR-DoC) twin models incorporating polygenic scores as instruments; expanded support for definition variables directly in umxRAM(); streamlined workflows for importing paths from $Ω$nyx; a dedicated tool for analyzing censored variables, particularly valuable in biomarker research; improved covariate placeholder handling for definition variables; umxSexLim() for simplified sex-limitation modelling across five twin groups, accommodating quantitative and qualitative sex differences; and umx_residualize() for efficient covariate residualization in wide- or long-format data. These advances accelerate reproducible, reliable, publication-ready twin and family modelling using intelligent defaults, and integrated journal-quality reporting, thereby lowering barriers to genetic epidemiological analyzes."}
{"id": "2512.10997", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10997", "abs": "https://arxiv.org/abs/2512.10997", "authors": ["Anija C. R", "Smitha S.", "Sudheesh K. Kattumannil"], "title": "The Cumulative Residual Mathai--Haubold Entropy and its Non-parametric Inference", "comment": null, "summary": "We introduce the cumulative residual Mathai--Haubold entropy (CRMHE) and investigate its properties. We then propose a dynamic counterpart, the dynamic cumulative residual Mathai--Haubold entropy (DCRMHE), and establish its uniqueness in characterizing the distribution function. Non-parametric estimators for the CRMHE and DCRMHE are developed based on the kernel density estimation of the survival function. The efficacy of the estimators is assessed through a comprehensive Monte Carlo simulation study. The relevance of the proposed DCRMHE estimator is illustrated using two real-world datasets: on the failure times of 70 aircraft windshields and failure times of 40 randomly selected mechanical switches."}
{"id": "2512.11052", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11052", "abs": "https://arxiv.org/abs/2512.11052", "authors": ["Joe Suk", "Samory Kpotufe"], "title": "An Efficient Variant of One-Class SVM with Lifelong Online Learning Guarantees", "comment": null, "summary": "We study outlier (a.k.a., anomaly) detection for single-pass non-stationary streaming data. In the well-studied offline or batch outlier detection problem, traditional methods such as kernel One-Class SVM (OCSVM) are both computationally heavy and prone to large false-negative (Type II) errors under non-stationarity. To remedy this, we introduce SONAR, an efficient SGD-based OCSVM solver with strongly convex regularization. We show novel theoretical guarantees on the Type I/II errors of SONAR, superior to those known for OCSVM, and further prove that SONAR ensures favorable lifelong learning guarantees under benign distribution shifts. In the more challenging problem of adversarial non-stationary data, we show that SONAR can be used within an ensemble method and equipped with changepoint detection to achieve adaptive guarantees, ensuring small Type I/II errors on each phase of data. We validate our theoretical findings on synthetic and real-world datasets."}
{"id": "2512.10997", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.10997", "abs": "https://arxiv.org/abs/2512.10997", "authors": ["Anija C. R", "Smitha S.", "Sudheesh K. Kattumannil"], "title": "The Cumulative Residual Mathai--Haubold Entropy and its Non-parametric Inference", "comment": null, "summary": "We introduce the cumulative residual Mathai--Haubold entropy (CRMHE) and investigate its properties. We then propose a dynamic counterpart, the dynamic cumulative residual Mathai--Haubold entropy (DCRMHE), and establish its uniqueness in characterizing the distribution function. Non-parametric estimators for the CRMHE and DCRMHE are developed based on the kernel density estimation of the survival function. The efficacy of the estimators is assessed through a comprehensive Monte Carlo simulation study. The relevance of the proposed DCRMHE estimator is illustrated using two real-world datasets: on the failure times of 70 aircraft windshields and failure times of 40 randomly selected mechanical switches."}
{"id": "2512.11197", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11197", "abs": "https://arxiv.org/abs/2512.11197", "authors": ["Emmanuel Hamel", "Anas Abdallah", "Ghislain Léveillé"], "title": "A Unified Micro-Model for Loss Reserves, IBNR and Unearned Premium Risk with Dependence, Inflation, and Discounting", "comment": null, "summary": "This paper introduces a unified micro-level stochastic framework for the joint modeling of loss reserves (RBNS), incurred but not reported (IBNR) reserves, and unearned premium risk under dependence, inflation, and discounting. The proposed framework accommodates interactions between indemnities, expenses, reporting delays, and settlement delays, while allowing for flexible parametric dependence structures and dynamic financial adjustments. An Aggregate Trend Renewal Process (ATRP) is used as one possible implementation of the joint model for payments, expenses, and delays; however, the methodological contribution of the paper lies in the unified micro-level reserving architecture rather than in the ATRP itself. The framework produces forward-looking reserve and premium risk measures with direct applications to pricing, reserving, and capital management.\n  We implement the framework using an aggregate trend renewal process at the individual claim level, which can be applied to the usual run-off triangle to obtain predictions for each accident-development year. Closed-form expressions for the first two raw and joint conditional moments of predicted payments are derived, together with approximations of their distribution functions. A detailed case study on medical malpractice insurance illustrates the practical relevance of the approach and its calibration on real-world data. We also investigate data heterogeneity, parameter uncertainty, distributional approximations, premium risk, UPR sensitivity to operational delays and inflation, and risk capital implications under alternative assumptions. The results highlight the advantages of unified micro-level modeling for dynamic liability and premium risk assessment in long-tailed lines of business."}
{"id": "2512.11012", "categories": ["stat.ME", "math.PR", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.11012", "abs": "https://arxiv.org/abs/2512.11012", "authors": ["Utku Erdogan", "Gabriel J. Lord", "Joaquin Miguez"], "title": "On a class of constrained Bayesian filters and their numerical implementation in high-dimensional state-space Markov models", "comment": null, "summary": "Bayesian filtering is a key tool in many problems that involve the online processing of data, including data assimilation, optimal control, nonlinear tracking and others. Unfortunately, the implementation of filters for nonlinear, possibly high-dimensional, dynamical systems is far from straightforward, as computational methods have to meet a delicate trade-off involving stability, accuracy and computational cost. In this paper we investigate the design, and theoretical features, of constrained Bayesian filters for state space models. The constraint on the filter is given by a sequence of compact subsets of the state space that determines the sources and targets of the Markov transition kernels in the dynamical model. Subject to such constraints, we provide sufficient conditions for filter stability and approximation error rates with respect to the original (unconstrained) Bayesian filter. Then, we look specifically into the implementation of constrained filters in a continuous-discrete setting where the state of the system is a continuous-time stochastic Itô process but data are collected sequentially over a time grid. We propose an implementation of the constraint that relies on a data-driven modification of the drift of the Itô process using barrier functions, and discuss the relation of this scheme with methods based on the Doob $h$-transform. Finally, we illustrate the theoretical results and the performance of the proposed methods in computer experiments for a partially-observed stochastic Lorenz 96 model."}
{"id": "2512.11081", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11081", "abs": "https://arxiv.org/abs/2512.11081", "authors": ["Kata Vuk", "Nicolas Alexander Ihlo", "Merle Behr"], "title": "Provable Recovery of Locally Important Signed Features and Interactions from Random Forest", "comment": null, "summary": "Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example."}
{"id": "2512.11085", "categories": ["stat.ME", "math.PR", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11085", "abs": "https://arxiv.org/abs/2512.11085", "authors": ["Jean-Marc Azaïs", "Federico Dalmao", "Yohann De Castro"], "title": "Gaussian random field's anisotropy using excursion sets", "comment": "https://github.com/ydecastro/COMETE-Contour-method-Gaussian-Random-Fields/", "summary": "This paper addresses the problem of detecting and estimating the anisotropy of a stationary real-valued random field from a single realization of one of its excursion sets. This setting is challenging as it relies on observing a binary image without prior knowledge of the field's mean, variance, or the specific threshold value.\n  Our first contribution is to propose a generalization of Cabaña's contour method to arbitrary dimensions by analyzing the Palm distribution of normal vectors along the excursion set boundaries. We demonstrate that the anisotropy parameters can be recovered by solving a smooth and strongly convex optimization problem involving the eigenvalues of the empirical covariance matrix of these normal vectors.\n  Our second main contribution is a new, model-agnostic statistical test for isotropy in dimension two. We introduce a statistic based on the contour method which is asymptotically distributed as a chi-squared variable with two degrees of freedom under the null hypothesis of quasi-isotropy. Unlike existing methods based on Lipschitz-Killing curvatures, this procedure does not require knowledge of the random field's covariance structure.\n  Extensive numerical experiments show that our test is well-calibrated and more powerful than model-based alternatives as well as that the estimation of the anisotropy parameters, including the directions, is robust and efficient. Finally, we apply this framework to test the quasi-isotropy of the Cosmic Microwave Background (CMB) using the Planck data release 3 mission."}
{"id": "2512.11593", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11593", "abs": "https://arxiv.org/abs/2512.11593", "authors": ["Hyungrok Do", "Yuyan Wang", "Mengling Liu", "Myeonggyun Lee"], "title": "Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis", "comment": null, "summary": "Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\\texttt{https://github.com/hyungrok-do/NeuralPLSI})."}
{"id": "2512.11085", "categories": ["stat.ME", "math.PR", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11085", "abs": "https://arxiv.org/abs/2512.11085", "authors": ["Jean-Marc Azaïs", "Federico Dalmao", "Yohann De Castro"], "title": "Gaussian random field's anisotropy using excursion sets", "comment": "https://github.com/ydecastro/COMETE-Contour-method-Gaussian-Random-Fields/", "summary": "This paper addresses the problem of detecting and estimating the anisotropy of a stationary real-valued random field from a single realization of one of its excursion sets. This setting is challenging as it relies on observing a binary image without prior knowledge of the field's mean, variance, or the specific threshold value.\n  Our first contribution is to propose a generalization of Cabaña's contour method to arbitrary dimensions by analyzing the Palm distribution of normal vectors along the excursion set boundaries. We demonstrate that the anisotropy parameters can be recovered by solving a smooth and strongly convex optimization problem involving the eigenvalues of the empirical covariance matrix of these normal vectors.\n  Our second main contribution is a new, model-agnostic statistical test for isotropy in dimension two. We introduce a statistic based on the contour method which is asymptotically distributed as a chi-squared variable with two degrees of freedom under the null hypothesis of quasi-isotropy. Unlike existing methods based on Lipschitz-Killing curvatures, this procedure does not require knowledge of the random field's covariance structure.\n  Extensive numerical experiments show that our test is well-calibrated and more powerful than model-based alternatives as well as that the estimation of the anisotropy parameters, including the directions, is robust and efficient. Finally, we apply this framework to test the quasi-isotropy of the Cosmic Microwave Background (CMB) using the Planck data release 3 mission."}
{"id": "2512.11089", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11089", "abs": "https://arxiv.org/abs/2512.11089", "authors": ["Devansh Arpit"], "title": "TPV: Parameter Perturbations Through the Lens of Test Prediction Variance", "comment": null, "summary": "We identify test prediction variance (TPV) -- the first-order sensitivity of model outputs to parameter perturbations around a trained solution -- as a unifying quantity that links several classical observations about generalization in deep networks. TPV is a fully label-free object whose trace form separates the geometry of the trained model from the specific perturbation mechanism, allowing a broad family of parameter perturbations like SGD noise, label noise, finite-precision noise, and other post-training perturbations to be analyzed under a single framework. Theoretically, we show that TPV estimated on the training set converges to its test-set value in the overparameterized limit, providing the first result that prediction variance under local parameter perturbations can be inferred from training inputs alone. Empirically, TPV exhibits a striking stability across datasets and architectures -- including extremely narrow networks -- and correlates well with clean test loss. Finally, we demonstrate that modeling pruning as a TPV perturbation yields a simple label-free importance measure that performs competitively with state-of-the-art pruning methods, illustrating the practical utility of TPV. Code available at github.com/devansharpit/TPV."}
{"id": "2512.11610", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11610", "abs": "https://arxiv.org/abs/2512.11610", "authors": ["Seungju Lee", "In Kyun Kim", "Jong Hee Park", "Ick Hoon Jin"], "title": "Euclidean Ideal Point Estimation From Roll-Call Data via Distance-Based Bipartite Network Models", "comment": null, "summary": "Conventional ideal point models rely on Gaussian or quadratic utility functions that violate the triangle inequality, producing non-metric distances that complicate geometric interpretation and undermine clustering and dispersion-based analyses. We introduce a distance-based alternative that adapts the Latent Space Item Response Model (LSIRM) to roll-call data, treating legislators and bills as nodes in a bipartite network jointly embedded in a Euclidean metric space. Through controlled simulations, Euclidean LSIRM consistently recovers latent coalition structure with superior cluster separation relative to existing methods. Applied to the 118th U.S. House, the model improves vote prediction and yields bill embeddings that clarify cross-cutting issue alignments. The results show that restoring metric structure to ideal point estimation provides a clearer and more coherent inference about party cohesion, factional divisions, and multidimensional legislative behavior."}
{"id": "2512.11139", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11139", "abs": "https://arxiv.org/abs/2512.11139", "authors": ["Tathagata Sadhukhan", "Ines Wilms", "Stephan Smeekes", "Sumanta Basu"], "title": "Autotune: fast, accurate, and automatic tuning parameter selection for LASSO", "comment": "53 pages, 35 figures", "summary": "Least absolute shrinkage and selection operator (Lasso), a popular method for high-dimensional regression, is now used widely for estimating high-dimensional time series models such as the vector autoregression (VAR). Selecting its tuning parameter efficiently and accurately remains a challenge, despite the abundance of available methods for doing so. We propose $\\mathsf{autotune}$, a strategy for Lasso to automatically tune itself by optimizing a penalized Gaussian log-likelihood alternately over regression coefficients and noise standard deviation. Using extensive simulation experiments on regression and VAR models, we show that $\\mathsf{autotune}$ is faster, and provides better generalization and model selection than established alternatives in low signal-to-noise regimes. In the process, $\\mathsf{autotune}$ provides a new estimator of noise standard deviation that can be used for high-dimensional inference, and a new visual diagnostic procedure for checking the sparsity assumption on regression coefficients. Finally, we demonstrate the utility of $\\mathsf{autotune}$ on a real-world financial data set. An R package based on C++ is also made publicly available on Github."}
{"id": "2512.11090", "categories": ["stat.ML", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.11090", "abs": "https://arxiv.org/abs/2512.11090", "authors": ["Biraj Dahal", "Jiahui Cheng", "Hao Liu", "Rongjie Lai", "Wenjing Liao"], "title": "Data-Driven Model Reduction using WeldNet: Windowed Encoders for Learning Dynamics", "comment": null, "summary": "Many problems in science and engineering involve time-dependent, high dimensional datasets arising from complex physical processes, which are costly to simulate. In this work, we propose WeldNet: Windowed Encoders for Learning Dynamics, a data-driven nonlinear model reduction framework to build a low-dimensional surrogate model for complex evolution systems. Given time-dependent training data, we split the time domain into multiple overlapping windows, within which nonlinear dimension reduction is performed by auto-encoders to capture latent codes. Once a low-dimensional representation of the data is learned, a propagator network is trained to capture the evolution of the latent codes in each window, and a transcoder is trained to connect the latent codes between adjacent windows. The proposed windowed decomposition significantly simplifies propagator training by breaking long-horizon dynamics into multiple short, manageable segments, while the transcoders ensure consistency across windows. In addition to the algorithmic framework, we develop a mathematical theory establishing the representation power of WeldNet under the manifold hypothesis, justifying the success of nonlinear model reduction via deep autoencoder-based architectures. Our numerical experiments on various differential equations indicate that WeldNet can capture nonlinear latent structures and their underlying dynamics, outperforming both traditional projection-based approaches and recently developed nonlinear model reduction methods."}
{"id": "2512.11648", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11648", "abs": "https://arxiv.org/abs/2512.11648", "authors": ["Gabriele Di Luzio", "Giacomo Morelli"], "title": "Dynamic Conditional SKEPTIC", "comment": null, "summary": "We introduce the Dynamic Conditional SKEPTIC (DCS), a semiparametric approach for efficiently and robustly estimating time-varying correlations in multivariate models. We exploit nonparametric rank-based statistics, namely Spearman's rho and Kendall's tau, to estimate the unknown correlation matrix and discuss the stationarity, beta- and rho- mixing conditions of the model. We illustrate the methodology by estimating the time-varying conditional correlation matrix of the stocks included in the S&P100 and S&P500 during the period from 02/01/2013 to 23/01/2025. The results show that DCS improves diagnostic checks compared to the classical Dynamic Conditional Correlation (DCC) models, providing uncorrelated and normally distributed residuals. A risk management application shows that global minimum variance portfolios estimated using the DCS model exhibit lower turnover than those based on the DCC and DCC-NL models, while also achieving higher Sharpe ratios for portfolios constructed from S&P 100 constituents."}
{"id": "2512.11150", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11150", "abs": "https://arxiv.org/abs/2512.11150", "authors": ["Eddie Landesberg"], "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems", "comment": "Code: https://github.com/cimo-labs/cje Experiments for Reproducibility: https://github.com/cimo-labs/cje-arena-experiments Original Preprint: https://zenodo.org/records/17903629", "summary": "LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover."}
{"id": "2512.11779", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11779", "abs": "https://arxiv.org/abs/2512.11779", "authors": ["Sacha Braun", "David Holzmüller", "Michael I. Jordan", "Francis Bach"], "title": "Conditional Coverage Diagnostics for Conformal Prediction", "comment": null, "summary": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems."}
{"id": "2512.11707", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.11707", "abs": "https://arxiv.org/abs/2512.11707", "authors": ["John Mahlon Scott", "Hsin-Hsiung Huang"], "title": "Maritime Vessel Tracking", "comment": null, "summary": "The Automatic Identification System (AIS) provides time stamped vessel positions and kinematic reports that enable maritime authorities to monitor traffic. We consider the problem of relabeling AIS trajectories when vessel identifiers are missing, focusing on a challenging nationwide setting in which tracks are heavily downsampled and span diverse operating environments across continental U.S. waters. We propose a hybrid pipeline that first applies a physics-based screening step to project active track endpoints forward in time and select a small set of plausible ancestors for each new observation. A supervised neural classifier then chooses among these candidates, or initiates a new track, using engineered space time and kinematic consistency features. On held out data, this approach improves posit accuracy relative to unsupervised baselines, demonstrating that combining simple motion models with learned disambiguation can scale vessel relabeling to heterogeneous, high volume AIS streams."}
{"id": "2512.11159", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11159", "abs": "https://arxiv.org/abs/2512.11159", "authors": ["Haoyang Wu", "Zhaoxing Wu", "Thulile Mathenjwa", "Elphas Okango", "Khai Hoan Tram", "Margot Otto", "Maxime Inghels", "Paul Mee", "Diego Cuadros", "Hae-Young Kim", "Till Barnighausen", "Frank Tanser", "Adrian Dobra"], "title": "Estimation of Contextual Exposure to HIV from GPS Data", "comment": "31 pages, 14 figures", "summary": "We present a comprehensive statistical methodological framework for estimating contextual exposure to HIV that includes local (grid-cell level) estimation of HIV prevalence and human activity space estimation based on GPS data. The development of our framework was necessary to analyze HIV surveillance and sociodemographic survey data in conjunction with GPS data collected in rural KwaZulu-Natal, South Africa, to study the mobility patterns of young people. Based on mobility and contextual exposure measures, we examine whether the sex and age of study participants systematically influence the extent and structure of their mobility patterns. We discuss techniques for investigating how the study participants' contextual exposure to HIV changes as their activity spaces expand beyond residential locations, as well as methods for identifying study participants who may be at increased risk of acquiring HIV. KEYWORDS: Contextual HIV exposure; GPS-based mobility analysis; Activity space; HIV prevalence mapping"}
{"id": "2512.11139", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11139", "abs": "https://arxiv.org/abs/2512.11139", "authors": ["Tathagata Sadhukhan", "Ines Wilms", "Stephan Smeekes", "Sumanta Basu"], "title": "Autotune: fast, accurate, and automatic tuning parameter selection for LASSO", "comment": "53 pages, 35 figures", "summary": "Least absolute shrinkage and selection operator (Lasso), a popular method for high-dimensional regression, is now used widely for estimating high-dimensional time series models such as the vector autoregression (VAR). Selecting its tuning parameter efficiently and accurately remains a challenge, despite the abundance of available methods for doing so. We propose $\\mathsf{autotune}$, a strategy for Lasso to automatically tune itself by optimizing a penalized Gaussian log-likelihood alternately over regression coefficients and noise standard deviation. Using extensive simulation experiments on regression and VAR models, we show that $\\mathsf{autotune}$ is faster, and provides better generalization and model selection than established alternatives in low signal-to-noise regimes. In the process, $\\mathsf{autotune}$ provides a new estimator of noise standard deviation that can be used for high-dimensional inference, and a new visual diagnostic procedure for checking the sparsity assumption on regression coefficients. Finally, we demonstrate the utility of $\\mathsf{autotune}$ on a real-world financial data set. An R package based on C++ is also made publicly available on Github."}
{"id": "2512.11085", "categories": ["stat.ME", "math.PR", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11085", "abs": "https://arxiv.org/abs/2512.11085", "authors": ["Jean-Marc Azaïs", "Federico Dalmao", "Yohann De Castro"], "title": "Gaussian random field's anisotropy using excursion sets", "comment": "https://github.com/ydecastro/COMETE-Contour-method-Gaussian-Random-Fields/", "summary": "This paper addresses the problem of detecting and estimating the anisotropy of a stationary real-valued random field from a single realization of one of its excursion sets. This setting is challenging as it relies on observing a binary image without prior knowledge of the field's mean, variance, or the specific threshold value.\n  Our first contribution is to propose a generalization of Cabaña's contour method to arbitrary dimensions by analyzing the Palm distribution of normal vectors along the excursion set boundaries. We demonstrate that the anisotropy parameters can be recovered by solving a smooth and strongly convex optimization problem involving the eigenvalues of the empirical covariance matrix of these normal vectors.\n  Our second main contribution is a new, model-agnostic statistical test for isotropy in dimension two. We introduce a statistic based on the contour method which is asymptotically distributed as a chi-squared variable with two degrees of freedom under the null hypothesis of quasi-isotropy. Unlike existing methods based on Lipschitz-Killing curvatures, this procedure does not require knowledge of the random field's covariance structure.\n  Extensive numerical experiments show that our test is well-calibrated and more powerful than model-based alternatives as well as that the estimation of the anisotropy parameters, including the directions, is robust and efficient. Finally, we apply this framework to test the quasi-isotropy of the Cosmic Microwave Background (CMB) using the Planck data release 3 mission."}
{"id": "2512.11406", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11406", "abs": "https://arxiv.org/abs/2512.11406", "authors": ["Madeline A. Shelley", "Chiara Boetti", "Marina I. Knight", "Matthew A. Nunes"], "title": "Network Estimation for Stationary Time Series", "comment": null, "summary": "High-dimensional multivariate time series are common in many scientific and industrial applications, where the interest lies in identifying key dependence structure within the data for subsequent analysis tasks, such as forecasting. An important avenue to achieve this is through the estimation of the conditional independence graph via graphical models, although for time series data settings the underpinning temporal dependence can make this task challenging. In this article, we propose a novel wavelet domain technique that allows the data-driven inference of the (sparse) conditional independence graph of a high-dimensional stationary multivariate time series. By adopting the locally stationary wavelet modelling framework, we repose the estimation problem as a well-principled wavelet domain graphical lasso formulation. Theoretical results establish that our associated estimation scheme enjoys good consistency properties when determining sparse dependence structure in input time series data. The performance of the proposed method is illustrated using extensive simulations and we demonstrate its applicability on a real-world dataset representing hospitalisations of COVID-19 patients."}
{"id": "2512.11150", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11150", "abs": "https://arxiv.org/abs/2512.11150", "authors": ["Eddie Landesberg"], "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems", "comment": "Code: https://github.com/cimo-labs/cje Experiments for Reproducibility: https://github.com/cimo-labs/cje-arena-experiments Original Preprint: https://zenodo.org/records/17903629", "summary": "LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover."}
{"id": "2512.11150", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11150", "abs": "https://arxiv.org/abs/2512.11150", "authors": ["Eddie Landesberg"], "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems", "comment": "Code: https://github.com/cimo-labs/cje Experiments for Reproducibility: https://github.com/cimo-labs/cje-arena-experiments Original Preprint: https://zenodo.org/records/17903629", "summary": "LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover."}
{"id": "2512.11427", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11427", "abs": "https://arxiv.org/abs/2512.11427", "authors": ["Tathagata Basu", "Fabrizio Leisen", "Cristiano Villa", "Kevin Wilson"], "title": "Conditional Copula models using loss-based Bayesian Additive Regression Trees", "comment": null, "summary": "The study of dependence between random variables under external influences is a challenging problem in multivariate analysis. We address this by proposing a novel semi-parametric approach for conditional copula models using Bayesian additive regression trees (BART) models. BART is becoming a popular approach in statistical modelling due to its simple ensemble type formulation complemented by its ability to provide inferential insights. Although BART allows us to model complex functional relationships, it tends to suffer from overfitting. In this article, we exploit a loss-based prior for the tree topology that is designed to reduce the tree complexity. In addition, we propose a novel adaptive Reversible Jump Markov Chain Monte Carlo algorithm that is ergodic in nature and requires very few assumptions allowing us to model complex and non-smooth likelihood functions with ease. Moreover, we show that our method can efficiently recover the true tree structure and approximate a complex conditional copula parameter, and that our adaptive routine can explore the true likelihood region under a sub-optimal proposal variance. Lastly, we provide case studies concerning the effect of gross domestic product on the dependence between the life expectancies and literacy rates of the male and female populations of different countries."}
{"id": "2512.11761", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11761", "abs": "https://arxiv.org/abs/2512.11761", "authors": ["Trisha Dawn", "Jesús Arroyo"], "title": "Covariate-assisted graph matching", "comment": null, "summary": "Data integration is essential across diverse domains, from historical records to biomedical research, facilitating joint statistical inference. A crucial initial step in this process involves merging multiple data sources based on matching individual records, often in the absence of unique identifiers. When the datasets are networks, this problem is typically addressed through graph matching methodologies. For such cases, auxiliary features or covariates associated with nodes or edges can be instrumental in achieving improved accuracy. However, most existing graph matching techniques do not incorporate this information, limiting their performance against non-identifiable and erroneous matches. To overcome these limitations, we propose two novel covariate-assisted seeded graph matching methods, where a partial alignment for a set of nodes, called seeds, is known. The first one solves a quadratic assignment problem (QAP) over the whole graph, while the second one only leverages the local neighborhood structure of seed nodes for computational scalability. Both methods are grounded in a conditional modeling framework, where elements of one graph's adjacency matrix are modeled using a generalized linear model (GLM), given the other graph and the available covariates. We establish theoretical guarantees for model estimation error and exact recovery of the solution of the QAP. The effectiveness of our methods is demonstrated through numerical experiments and in an application to matching the statistics academic genealogy and the collaboration networks. By leveraging additional covariates, we achieve improved alignment accuracy. Our work highlights the power of integrating covariate information in the classical graph matching setup, offering a practical and improved framework for combining network data with wide-ranging applications."}
{"id": "2512.11159", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11159", "abs": "https://arxiv.org/abs/2512.11159", "authors": ["Haoyang Wu", "Zhaoxing Wu", "Thulile Mathenjwa", "Elphas Okango", "Khai Hoan Tram", "Margot Otto", "Maxime Inghels", "Paul Mee", "Diego Cuadros", "Hae-Young Kim", "Till Barnighausen", "Frank Tanser", "Adrian Dobra"], "title": "Estimation of Contextual Exposure to HIV from GPS Data", "comment": "31 pages, 14 figures", "summary": "We present a comprehensive statistical methodological framework for estimating contextual exposure to HIV that includes local (grid-cell level) estimation of HIV prevalence and human activity space estimation based on GPS data. The development of our framework was necessary to analyze HIV surveillance and sociodemographic survey data in conjunction with GPS data collected in rural KwaZulu-Natal, South Africa, to study the mobility patterns of young people. Based on mobility and contextual exposure measures, we examine whether the sex and age of study participants systematically influence the extent and structure of their mobility patterns. We discuss techniques for investigating how the study participants' contextual exposure to HIV changes as their activity spaces expand beyond residential locations, as well as methods for identifying study participants who may be at increased risk of acquiring HIV. KEYWORDS: Contextual HIV exposure; GPS-based mobility analysis; Activity space; HIV prevalence mapping"}
{"id": "2512.11452", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11452", "abs": "https://arxiv.org/abs/2512.11452", "authors": ["Kenenisa Tadesse Dame", "Pietro Belloni", "Ugo Moretti", "Fabio Scapini", "Marco Tuccori", "Alessandra R. Brazzale"], "title": "Advances in Ontology--Based Mining of Adverse Drug Reactions", "comment": "20 pages, 22 figures, 2 tables", "summary": "Post--marketing pharmacovigilance is essential for identifying adverse drug reactions (ADRs) that elude detection during pre--marketing clinical trials. This study explores a novel approach that integrates an adverse event (AE) ontology into a zero--inflated negative binomial model to improve ADR detection. By accounting for the biological similarities among correlated AEs and addressing the excess of zero counts, this method more effectively disentangles AE associations. Statistical significance is evaluated using a permutation--based maximum statistic that preserves AE correlations within individual reports. Simulations and an application to real data from the Veneto drug safety database demonstrate that the ontology--based model consistently outperforms classical models such as the Gamma--Poisson Shrinker (GPS). For post--selection inference, we furthermore explore a data thinning technique for convolution--closed families, enabling the creation of independent training and validation datasets while retaining all drug--AE pairs. This approach is compared with conventional random train/test splitting, which may leave some drugs or AEs absent from one subset, and stratified splitting, which requires expanding aggregated counts into individual instances. The data--thinning technique and stratified splitting yield very similar results, with stratified splitting showing a slight benefit, and both clearly outperform random splitting in ensuring reliable and consistent model evaluation."}
{"id": "2512.11751", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11751", "abs": "https://arxiv.org/abs/2512.11751", "authors": ["Andy A. Shen", "Eli Ben-Michael", "Avi Feller", "Luke Keele", "Jared Murray"], "title": "Forest Kernel Balancing Weights: Outcome-Guided Features for Causal Inference", "comment": null, "summary": "While balancing covariates between groups is central for observational causal inference, selecting which features to balance remains a challenging problem. Kernel balancing is a promising approach that first estimates a kernel that captures similarity across units and then balances a (possibly low-dimensional) summary of that kernel, indirectly learning important features to balance. In this paper, we propose forest kernel balancing, which leverages the underappreciated fact that tree-based machine learning models, namely random forests and Bayesian additive regression trees (BART), implicitly estimate a kernel based on the co-occurrence of observations in the same terminal leaf node. Thus, even though the resulting kernel is solely a function of baseline features, the selected nonlinearities and other interactions are important for predicting the outcome -- and therefore are important for addressing confounding. Through simulations and applied illustrations, we show that forest kernel balancing leads to meaningful computational and statistical improvement relative to standard kernel methods, which do not incorporate outcome information when learning features."}
{"id": "2512.11549", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11549", "abs": "https://arxiv.org/abs/2512.11549", "authors": ["Marie S. Breum", "Vanessa Didelez", "Erin E. Gabriel", "Michael C. Sachs"], "title": "Bounds for causal mediation effects", "comment": null, "summary": "Several frameworks have been proposed for studying causal mediation analysis. What these frameworks have in common is that they all make assumptions for point identifications that can be violated even when treatment is randomized. When a causal effect is not point-identified, one can sometimes derive bounds, i.e. a range of possible values that are consistent with the observed data. In this work, we study causal bounds for mediation effects under both the natural effects framework and the separable effects framework. In particular, we show that when there are unmeasured confounders for the intermediate variables(s) the sharp symbolic bounds on separable (in)direct effect coincide with existing bounds for natural (in)direct effects in the analogous setting. We compare these bounds to valid bounds for the natural direct effects when only the cross-world independence assumption does not hold. Furthermore, we demonstrate the use and compare the results of the bounds on data from a trial investigating the effect of peanut consumption on the development of peanut allergy in infants through specific pathways of measured immunological biomarkers."}
{"id": "2512.11599", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11599", "abs": "https://arxiv.org/abs/2512.11599", "authors": ["Sheila T. Görz", "Roland Fried"], "title": "Detecting changes in the mean of spatial random fields on a regular grid", "comment": null, "summary": "We propose statistical procedures for detecting changes in the mean of spatial random fields observed on regular grids. The proposed framework provides a general approach to change detection in spatial processes. Extending a block-based method originally developed for time series, we introduce two test statistics, one based on Gini's mean difference and a novel variance-based variant. Under mild moment conditions, we derive asymptotic normality of the variance-based statistic and prove its consistency against almost all non-constant mean functions (in a sense of positive Lebesgue measure). To accommodate spatial dependence, we further develop a de-correlation algorithm based on estimated autocovariances. Monte Carlo simulations demonstrate that both tests maintain appropriate size and power for both independent and dependent data. In an application to satellite images, especially our variance-based test reliably detects regions undergoing deforestation."}
{"id": "2512.11732", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11732", "abs": "https://arxiv.org/abs/2512.11732", "authors": ["Trisha Dawn", "Yang Ni"], "title": "Spatially Varying Gene Regulatory Networks via Bayesian Nonparametric Covariate-Dependent Directed Cyclic Graphical Models", "comment": null, "summary": "Spatial transcriptomics technologies enable the measurement of gene expression with spatial context, providing opportunities to understand how gene regulatory networks vary across tissue regions. However, existing graphical models focus primarily on undirected graphs or directed acyclic graphs, limiting their ability to capture feedback loops that are prevalent in gene regulation. Moreover, ensuring the so-called stability condition of cyclic graphs, while allowing graph structures to vary continuously with spatial covariates, presents significant statistical and computational challenges. We propose BNP-DCGx, a Bayesian nonparametric approach for learning spatially varying gene regulatory networks via covariate-dependent directed cyclic graphical models. Our method introduces a covariate-dependent random partition as an intermediary layer in a hierarchical model, which discretizes the covariate space into clusters with cluster-specific stable directed cyclic graphs. Through partition averaging, we obtain smoothly varying graph structures over space while maintaining theoretical guarantees of stability. We develop an efficient parallel tempered Markov chain Monte Carlo algorithm for posterior inference and demonstrate through simulations that our method accurately recovers both piecewise constant and continuously varying graph structures. Application to spatial transcriptomics data from human dorsolateral prefrontal cortex reveals spatially varying regulatory networks with feedback loops, identifies potential cell subtypes within established cell types based on distinct regulatory mechanisms, and provides new insights into spatial organization of gene regulation in brain tissue."}
{"id": "2512.11751", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11751", "abs": "https://arxiv.org/abs/2512.11751", "authors": ["Andy A. Shen", "Eli Ben-Michael", "Avi Feller", "Luke Keele", "Jared Murray"], "title": "Forest Kernel Balancing Weights: Outcome-Guided Features for Causal Inference", "comment": null, "summary": "While balancing covariates between groups is central for observational causal inference, selecting which features to balance remains a challenging problem. Kernel balancing is a promising approach that first estimates a kernel that captures similarity across units and then balances a (possibly low-dimensional) summary of that kernel, indirectly learning important features to balance. In this paper, we propose forest kernel balancing, which leverages the underappreciated fact that tree-based machine learning models, namely random forests and Bayesian additive regression trees (BART), implicitly estimate a kernel based on the co-occurrence of observations in the same terminal leaf node. Thus, even though the resulting kernel is solely a function of baseline features, the selected nonlinearities and other interactions are important for predicting the outcome -- and therefore are important for addressing confounding. Through simulations and applied illustrations, we show that forest kernel balancing leads to meaningful computational and statistical improvement relative to standard kernel methods, which do not incorporate outcome information when learning features."}
{"id": "2512.11761", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11761", "abs": "https://arxiv.org/abs/2512.11761", "authors": ["Trisha Dawn", "Jesús Arroyo"], "title": "Covariate-assisted graph matching", "comment": null, "summary": "Data integration is essential across diverse domains, from historical records to biomedical research, facilitating joint statistical inference. A crucial initial step in this process involves merging multiple data sources based on matching individual records, often in the absence of unique identifiers. When the datasets are networks, this problem is typically addressed through graph matching methodologies. For such cases, auxiliary features or covariates associated with nodes or edges can be instrumental in achieving improved accuracy. However, most existing graph matching techniques do not incorporate this information, limiting their performance against non-identifiable and erroneous matches. To overcome these limitations, we propose two novel covariate-assisted seeded graph matching methods, where a partial alignment for a set of nodes, called seeds, is known. The first one solves a quadratic assignment problem (QAP) over the whole graph, while the second one only leverages the local neighborhood structure of seed nodes for computational scalability. Both methods are grounded in a conditional modeling framework, where elements of one graph's adjacency matrix are modeled using a generalized linear model (GLM), given the other graph and the available covariates. We establish theoretical guarantees for model estimation error and exact recovery of the solution of the QAP. The effectiveness of our methods is demonstrated through numerical experiments and in an application to matching the statistics academic genealogy and the collaboration networks. By leveraging additional covariates, we achieve improved alignment accuracy. Our work highlights the power of integrating covariate information in the classical graph matching setup, offering a practical and improved framework for combining network data with wide-ranging applications."}
{"id": "2512.11777", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11777", "abs": "https://arxiv.org/abs/2512.11777", "authors": ["Sinyoung Park", "Matthew Nunes", "Sandipan Roy"], "title": "A Doubled Adjacency Spectral Embedding Approach to Graph Clustering", "comment": null, "summary": "Spectral clustering is a popular tool in network data analysis, with applications in a variety of scientific application areas. However, many studies have shown that spectral clustering does not perform well on certain network structures, particularly core-periphery networks. To improve clustering performance in core-periphery structures, Adjacency Spectral Embedding (ASE) has been introduced, which performs clustering via a network's adjacency matrix instead of the graph Laplacian. Despite its advantages in this setting, the optimal performance of ASE is limited to dense networks, whilst network data observed in practice is often sparse in nature. To address this limitation, we propose a new approach which we term Doubled Adjacency Spectral Embedding (DASE), motivated by the observation that the squared adjacency matrix will leverage the fewer connections in sparse structures more efficiently in clustering applications. Theoretical results establish that DASE enjoys good consistency properties when determining sparse community structure. The performance and general applicability of the proposed method is evaluated using extensive simulations on both directed and undirected networks. Our results highlight the improved clustering performance on both sparse and dense networks in the presence of core-periphery structures. We illustrate our proposed technique on real-world employment and transportation datasets."}
{"id": "2512.11081", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.11081", "abs": "https://arxiv.org/abs/2512.11081", "authors": ["Kata Vuk", "Nicolas Alexander Ihlo", "Merle Behr"], "title": "Provable Recovery of Locally Important Signed Features and Interactions from Random Forest", "comment": null, "summary": "Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example."}
