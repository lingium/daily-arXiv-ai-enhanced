{"id": "2510.12916", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12916", "abs": "https://arxiv.org/abs/2510.12916", "authors": ["Giosue Migliorini", "Padhraic Smyth"], "title": "Efficient Inference for Coupled Hidden Markov Models in Continuous Time and Discrete Space", "comment": null, "summary": "Systems of interacting continuous-time Markov chains are a powerful model\nclass, but inference is typically intractable in high dimensional settings.\nAuxiliary information, such as noisy observations, is typically only available\nat discrete times, and incorporating it via a Doob's $h-$transform gives rise\nto an intractable posterior process that requires approximation. We introduce\nLatent Interacting Particle Systems, a model class parameterizing the generator\nof each Markov chain in the system. Our inference method involves estimating\nlook-ahead functions (twist potentials) that anticipate future information, for\nwhich we introduce an efficient parameterization. We incorporate this\napproximation in a twisted Sequential Monte Carlo sampling scheme. We\ndemonstrate the effectiveness of our approach on a challenging posterior\ninference task for a latent SIRS model on a graph, and on a neural model for\nwildfire spread dynamics trained on real data."}
{"id": "2510.12983", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.12983", "abs": "https://arxiv.org/abs/2510.12983", "authors": ["Lorenzo Marinucci", "Gabriele D'Acunto", "Paolo Di Lorenzo", "Sergio Barbarossa"], "title": "Simplicial Gaussian Models: Representation and Inference", "comment": null, "summary": "Probabilistic graphical models (PGMs) are powerful tools for representing\nstatistical dependencies through graphs in high-dimensional systems. However,\nthey are limited to pairwise interactions. In this work, we propose the\nsimplicial Gaussian model (SGM), which extends Gaussian PGM to simplicial\ncomplexes. SGM jointly models random variables supported on vertices, edges,\nand triangles, within a single parametrized Gaussian distribution. Our model\nbuilds upon discrete Hodge theory and incorporates uncertainty at every\ntopological level through independent random components. Motivated by\napplications, we focus on the marginal edge-level distribution while treating\nnode- and triangle-level variables as latent. We then develop a\nmaximum-likelihood inference algorithm to recover the parameters of the full\nSGM and the induced conditional dependence structure. Numerical experiments on\nsynthetic simplicial complexes with varying size and sparsity confirm the\neffectiveness of our algorithm."}
{"id": "2510.13037", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13037", "abs": "https://arxiv.org/abs/2510.13037", "authors": ["Tianmin Xie", "Yanfei Zhou", "Ziyi Liang", "Stefano Favaro", "Matteo Sesia"], "title": "Conformal Inference for Open-Set and Imbalanced Classification", "comment": null, "summary": "This paper presents a conformal prediction method for classification in\nhighly imbalanced and open-set settings, where there are many possible classes\nand not all may be represented in the data. Existing approaches require a\nfinite, known label space and typically involve random sample splitting, which\nworks well when there is a sufficient number of observations from each class.\nConsequently, they have two limitations: (i) they fail to provide adequate\ncoverage when encountering new labels at test time, and (ii) they may become\noverly conservative when predicting previously seen labels. To obtain valid\nprediction sets in the presence of unseen labels, we compute and integrate into\nour predictions a new family of conformal p-values that can test whether a new\ndata point belongs to a previously unseen class. We study these p-values\ntheoretically, establishing their optimality, and uncover an intriguing\nconnection with the classical Good--Turing estimator for the probability of\nobserving a new species. To make more efficient use of imbalanced data, we also\ndevelop a selective sample splitting algorithm that partitions training and\ncalibration data based on label frequency, leading to more informative\npredictions. Despite breaking exchangeability, this allows maintaining\nfinite-sample guarantees through suitable re-weighting. With both simulated and\nreal data, we demonstrate our method leads to prediction sets with valid\ncoverage even in challenging open-set scenarios with infinite numbers of\npossible labels, and produces more informative predictions under extreme class\nimbalance."}
{"id": "2510.13093", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13093", "abs": "https://arxiv.org/abs/2510.13093", "authors": ["Ningkang Peng", "Yuzhe Mao", "Yuhao Zhang", "Linjin Qian", "Qianfeng Yu", "Yanhui Gu", "Yi Chen", "Li Kong"], "title": "A Multi-dimensional Semantic Surprise Framework Based on Low-Entropy Semantic Manifolds for Fine-Grained Out-of-Distribution Detection", "comment": null, "summary": "Out-of-Distribution (OOD) detection is a cornerstone for the safe deployment\nof AI systems in the open world. However, existing methods treat OOD detection\nas a binary classification problem, a cognitive flattening that fails to\ndistinguish between semantically close (Near-OOD) and distant (Far-OOD) unknown\nrisks. This limitation poses a significant safety bottleneck in applications\nrequiring fine-grained risk stratification. To address this, we propose a\nparadigm shift from a conventional probabilistic view to a principled\ninformation-theoretic framework. We formalize the core task as quantifying the\nSemantic Surprise of a new sample and introduce a novel ternary classification\nchallenge: In-Distribution (ID) vs. Near-OOD vs. Far-OOD. The theoretical\nfoundation of our work is the concept of Low-Entropy Semantic Manifolds, which\nare explicitly structured to reflect the data's intrinsic semantic hierarchy.\nTo construct these manifolds, we design a Hierarchical Prototypical Network. We\nthen introduce the Semantic Surprise Vector (SSV), a universal probe that\ndecomposes a sample's total surprise into three complementary and interpretable\ndimensions: conformity, novelty, and ambiguity. To evaluate performance on this\nnew task, we propose the Normalized Semantic Risk (nSR), a cost-sensitive\nmetric. Experiments demonstrate that our framework not only establishes a new\nstate-of-the-art (sota) on the challenging ternary task, but its robust\nrepresentations also achieve top results on conventional binary benchmarks,\nreducing the False Positive Rate by over 60% on datasets like LSUN."}
{"id": "2510.12963", "categories": ["stat.AP", "physics.soc-ph", "stat.ME", "60G70 (Primary) 62F15, 62G32, 62P30 (Secondary)", "I.6.4; J.2; G.3; J.7"], "pdf": "https://arxiv.org/pdf/2510.12963", "abs": "https://arxiv.org/abs/2510.12963", "authors": ["Parvez Anowar", "Nazmul Haque", "Md Asif Raihan", "Md Hadiuzzaman"], "title": "Trajectory-based real-time pedestrian crash prediction at intersections: A novel non-linear link function for block maxima led Bayesian GEV framework addressing heterogeneous traffic condition", "comment": "This manuscript is a preprint and has not yet been peer-reviewed. It\n  is currently being considered for submission to a peer-reviewed journal", "summary": "This study develops a real-time framework for estimating pedestrian crash\nrisk at signalized intersections under heterogeneous, non-lane-based traffic.\nExisting approaches often assume linear relationships between covariates and\nparameters, oversimplifying the complex, non-monotonic interactions among\ndifferent road users. To overcome this, the framework introduces a non-linear\nlink function within a Bayesian generalized extreme value (GEV) structure to\ncapture traffic variability more accurately. The framework applies extreme\nvalue theory through the block maxima approach using post-encroachment time as\na surrogate safety measure. A hierarchical Bayesian model incorporating both\nlinear and non-linear link functions into GEV parameters is estimated using\nMarkov Chain Monte Carlo simulation. It also introduces a behavior-normalized\nModified Crash Risk (MRC) formula to account for pedestrians' habitual\nrisk-taking behavior. Seven Bayesian hierarchical models were developed and\ncompared using deviance information criterion. Models employing non-linear link\nfunctions for the location and scale parameters significantly outperformed\ntheir linear counterparts. The results revealed that pedestrian speed has a\nnegative relationship with crash risk, while flow and speed of motorized\nvehicles, pedestrian flow, and non-motorized vehicles conflicting speed\ncontribute positively. The MRC formulation reduced overestimation and provided\ncrash predictions with 93% confidence. The integration of non-linear link\nfunctions enhances model flexibility, capturing the non-linear nature of\ntraffic extremes. The proposed MRC metric aligns crash risk estimates with\nreal-world pedestrian behavior in mixed-traffic environments. This framework\noffers a practical analytical tool for traffic engineers and planners to design\nadaptive signal control and pedestrian safety interventions before crashes\noccur."}
{"id": "2510.13159", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.13159", "abs": "https://arxiv.org/abs/2510.13159", "authors": ["Hung Hung", "Zhi-Yu Jou", "Su-Yun Huang", "Shinto Eguchi"], "title": "The $φ$-PCA Framework: A Unified and Efficiency-Preserving Approach with Robust Variants", "comment": "27 pages, 4 figures", "summary": "Principal component analysis (PCA) is a fundamental tool in multivariate\nstatistics, yet its sensitivity to outliers and limitations in distributed\nenvironments restrict its effectiveness in modern large-scale applications. To\naddress these challenges, we introduce the $\\phi$-PCA framework which provides\na unified formulation of robust and distributed PCA. The class of $\\phi$-PCA\nmethods retains the asymptotic efficiency of standard PCA, while aggregating\nmultiple local estimates using a proper $\\phi$ function enhances\nordering-robustness, leading to more accurate eigensubspace estimation under\ncontamination. Notably, the harmonic mean PCA (HM-PCA), corresponding to the\nchoice $\\phi(u)=u^{-1}$, achieves optimal ordering-robustness and is\nrecommended for practical use. Theoretical results further show that robustness\nincreases with the number of partitions, a phenomenon seldom explored in the\nliterature on robust or distributed PCA. Altogether, the partition-aggregation\nprinciple underlying $\\phi$-PCA offers a general strategy for developing robust\nand efficiency-preserving methodologies applicable to both robust and\ndistributed data analysis."}
{"id": "2510.13094", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13094", "abs": "https://arxiv.org/abs/2510.13094", "authors": ["Aaradhya Pandey", "Arnab Auddy", "Haolin Zou", "Arian Maleki", "Sanjeev Kulkarni"], "title": "Gaussian Certified Unlearning in High Dimensions: A Hypothesis Testing Approach", "comment": "Comments welcome!", "summary": "Machine unlearning seeks to efficiently remove the influence of selected data\nwhile preserving generalization. Significant progress has been made in low\ndimensions $(p \\ll n)$, but high dimensions pose serious theoretical challenges\nas standard optimization assumptions of $\\Omega(1)$ strong convexity and $O(1)$\nsmoothness of the per-example loss $f$ rarely hold simultaneously in\nproportional regimes $(p\\sim n)$. In this work, we introduce\n$\\varepsilon$-Gaussian certifiability, a canonical and robust notion\nwell-suited to high-dimensional regimes, that optimally captures a broad class\nof noise adding mechanisms. Then we theoretically analyze the performance of a\nwidely used unlearning algorithm based on one step of the Newton method in the\nhigh-dimensional setting described above. Our analysis shows that a single\nNewton step, followed by a well-calibrated Gaussian noise, is sufficient to\nachieve both privacy and accuracy in this setting. This result stands in sharp\ncontrast to the only prior work that analyzes machine unlearning in high\ndimensions \\citet{zou2025certified}, which relaxes some of the standard\noptimization assumptions for high-dimensional applicability, but operates under\nthe notion of $\\varepsilon$-certifiability. That work concludes %that a single\nNewton step is insufficient even for removing a single data point, and that at\nleast two steps are required to ensure both privacy and accuracy. Our result\nleads us to conclude that the discrepancy in the number of steps arises because\nof the sub optimality of the notion of $\\varepsilon$-certifiability and its\nincompatibility with noise adding mechanisms, which $\\varepsilon$-Gaussian\ncertifiability is able to overcome optimally."}
{"id": "2510.12986", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.12986", "abs": "https://arxiv.org/abs/2510.12986", "authors": ["Mohammad Ahmadi Gharehtoragh", "David R Johnson"], "title": "Surrogate Models to Predict Wave Hydrodynamics on Evolving Landscapes", "comment": null, "summary": "Coastal planners using probabilistic risk assessments to evaluate structural\nflood risk reduction projects may wish to simulate the hydrodynamics associated\nwith large suites of tropical cyclones in large ensembles of landscapes: with\nand without projects' implementation; over decades of their useful lifetimes;\nand under multiple scenarios reflecting uncertainty about sea level rise, land\nsubsidence, and other factors. Wave action can be a substantial contributor to\nflood losses and overtopping of structural features like levees and floodwalls,\nbut numerical methods solving for wave dynamics are computationally expensive,\npotentially limiting budget-constrained planning efforts. In this study, we\npresent and evaluate the performance of deep learning-based surrogate models\nfor predicting peak significant wave heights under a variety of relevant use\ncases: predicting waves with or without modeled peak storm surge as a feature,\npredicting wave heights while simultaneously predicting peak storm surge, or\nusing storm surge predicted by another surrogate model as an input feature. All\nmodels incorporate landscape morphological elements (e.g., elevation,\nroughness, canopy) and global boundary conditions (e.g., sea level) in addition\nto tropical cyclone characteristics as predictive features to improve accuracy\nas landscapes evolve over time. Using simulations from Louisiana's 2023 Coastal\nMaster Plan as a case study, we demonstrate suitable accuracy of surrogate\nmodels for planning-level studies, with a two-sided Kolmogorov-Smirnov test\nindicating no significant difference between significant wave heights generated\nby the Simulating Waves Nearshore model and those predicted by our surrogate\nmodels in approximately 89% of grid cells and landscapes evaluated in the\nstudy, with performance varying by landscape and model. On average, the models\nproduced a root mean squared error of 0.05-0.06 m."}
{"id": "2510.13636", "categories": ["stat.ME", "cs.SI", "math.ST", "stat.TH", "62R01,"], "pdf": "https://arxiv.org/pdf/2510.13636", "abs": "https://arxiv.org/abs/2510.13636", "authors": ["Félix Almendra-Hernández", "Miles Bakenhus", "Vishesh Karwa", "Mitsunori Ogawa", "Sonja Petrović"], "title": "Non-asymptotic goodness-of-fit tests and model selection in valued stochastic blockmodels", "comment": null, "summary": "A valued stochastic blockmodel (SBM) is a general way to view networked data\nin which nodes are grouped into blocks and links between them are measured by\ncounts or labels. This family allows for varying dyad sampling schemes, thereby\nincluding the classical, Poisson, and labeled SBMs, as well as those in which\nsome edge observations are censored. This paper addresses the question of\ntesting goodness-of-fit of such non-Bernoulli SBMs, focusing in particular on\nfinite-sample tests. We derive explicit Markov bases moves necessary to\ngenerate samples from reference distributions and define goodness-of-fit\nstatistics for determining model fit, comparable to those in the literature for\nrelated model families.\n  For the labeled SBM, which includes in particular the censored-edge model, we\nstudy the asymptotic behavior of said statistics. One of the main purposes of\ntesting goodness-of-fit of an SBM is to determine whether block membership of\nthe nodes influences network formation. Power and Type 1 error rates are\nverified on simulated data. Additionally, we discuss the use of asymptotic\nresults in selecting the number of blocks under the latent-block modeling\nassumption. The method derived for Poisson SBM is applied to ecological\nnetworks of host-parasite interactions. Our data analysis conclusions differ in\nselecting the number of blocks for the species from previous results in the\nliterature."}
{"id": "2510.13389", "categories": ["stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.13389", "abs": "https://arxiv.org/abs/2510.13389", "authors": ["Tien-En Chang", "Argon Chen"], "title": "Understanding and Using the Relative Importance Measures Based on Orthonormality Transformation", "comment": "20 pages, 10 figures", "summary": "A class of relative importance measures based on orthonormality\ntransformation (OTMs), has been found to effectively approximate the General\nDominance index (GD). In particular, Johnson's Relative Weight (RW) has been\ndeemed the most successful OTM in the literature. Nevertheless, the theoretical\nfoundation of the OTMs remains unclear. To further understand the OTMs, we\nprovide a generalized framework that breaks down the OTM into two functional\nsteps: orthogonalization and reallocation. To assess the impact of each step on\nthe performance of OTMs, we conduct extensive Monte Carlo simulations under\nvarious predictors' correlation structures and response variable distributions.\nOur findings reveal that Johnson's minimal transformation consistently\noutperforms other common orthogonalization methods. We also summarize the\nperformance of reallocation methods under four scenarios of predictors'\ncorrelation structures in terms of the first principal component and the\nvariance inflation factor (VIF). This analysis provides guidelines for\nselecting appropriate reallocation methods in different scenarios, illustrated\nwith real-world dataset examples. Our research offers a deeper understanding of\nOTMs and provides valuable insights for practitioners seeking to accurately\nmeasure variable importance in various modeling contexts."}
{"id": "2510.12917", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.12917", "abs": "https://arxiv.org/abs/2510.12917", "authors": ["Aiden Gundersen", "Neil J. Cornish"], "title": "Escaping Neal's Funnel: a multi-stage sampling method for hierarchical models", "comment": "8 pages, 4 figures", "summary": "Neal's funnel refers to an exponential tapering in probability densities\ncommon to Bayesian hierarchical models. Usual sampling methods, such as Markov\nChain Monte Carlo, struggle to efficiently sample the funnel. Reparameterizing\nthe model or analytically marginalizing local parameters are common techniques\nto remedy sampling pathologies in distributions exhibiting Neal's funnel. In\nthis paper, we show that the challenges of Neal's funnel can be avoided by\nperforming the hierarchical analysis, well, hierarchically. That is, instead of\nsampling all parameters of the hierarchical model jointly, we break the\nsampling into multiple stages. The first stage samples a generalized\n(higher-dimensional) hierarchical model which is parameterized to lessen the\nsharpness of the funnel. The next stage samples from the estimated density of\nthe first stage, but under a constraint which restricts the sampling to recover\nthe marginal distributions on the hyper-parameters of the original\n(lower-dimensional) hierarchical model. A normalizing flow can be used to\nrepresent the distribution from the first stage, such that it can easily be\nsampled from for the second stage of the analysis. This technique is useful\nwhen effective reparameterizations are computationally expensive to calculate,\nor a generalized hierarchical model already exists from which it is easy to\nsample."}
{"id": "2510.13438", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13438", "abs": "https://arxiv.org/abs/2510.13438", "authors": ["Pierre Glaser", "Kevin Han Huang", "Arthur Gretton"], "title": "Near-Optimality of Contrastive Divergence Algorithms", "comment": "54 pages", "summary": "We perform a non-asymptotic analysis of the contrastive divergence (CD)\nalgorithm, a training method for unnormalized models. While prior work has\nestablished that (for exponential family distributions) the CD iterates\nasymptotically converge at an $O(n^{-1 / 3})$ rate to the true parameter of the\ndata distribution, we show, under some regularity assumptions, that CD can\nachieve the parametric rate $O(n^{-1 / 2})$. Our analysis provides results for\nvarious data batching schemes, including the fully online and minibatch ones.\nWe additionally show that CD can be near-optimal, in the sense that its\nasymptotic variance is close to the Cram\\'er-Rao lower bound."}
{"id": "2510.13517", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.13517", "abs": "https://arxiv.org/abs/2510.13517", "authors": ["Maksym Koltunov", "Filippo Beltrami", "Luigi Grossi", "Nicola Blasuttigh"], "title": "The Impact of Renewable Energy Communities in the Italian Day-Ahead Electricity Market: A Scenario Analysis", "comment": null, "summary": "This paper evaluates the economic impact of Renewable Energy Communities\n(RECs) on the Italian wholesale power market. Combining a bottom-up engineering\napproach with a short-run economic impact assessment, the study begins by\nmapping existing and emerging RECs in Italy. We identify key characteristics of\nRECs, such as average installed capacity, institutional profiles of members,\ntypes of renewable systems used, and distribution across Italy's electricity\nmarket zones. This mapping yields representative REC configurations, which are\nemployed within a bottom-up engineering model to generate energy injection and\nself-consumption profiles for different REC prosumer and producer categories\n(residential, public, small and medium enterprise, non-profit organization, and\nstandalone installation), considering the different levels of solar irradiance\nin Italy based on latitude. These zonal results, aggregated on an hourly basis,\ninform the implementation of the synthetic counterfactual approach, which\ndevelops alternative scenarios (e.g., 5 GW target for REC-driven capacity set\nby Italian policy for 2027) to assess the impact of REC-driven injection and\nself-consumption on the Italian day-ahead power market. The findings suggest\nthat REC deployment can increase equilibrium quantities during daylight in most\nof the time, while decreasing equilibrium quantities mostly during the cold\nmonths, as electrified heating drives greater self-consumption and offsets\nlower grid injections. Both positive and negative effects on equilibrium\nquantities suggest that REC deployment also has a potential to reduce wholesale\nelectricity prices. Moreover, by reducing grid exchanges through higher\nself-consumption, REC proliferation can alleviate pressure on the distribution\nsystem."}
{"id": "2510.13233", "categories": ["stat.ME", "stat.CO", "62M30"], "pdf": "https://arxiv.org/pdf/2510.13233", "abs": "https://arxiv.org/abs/2510.13233", "authors": ["Arghya Mukherjee", "Arnab Hazra", "Dootika Vats"], "title": "Scalable Bayesian inference for high-dimensional mixed-type multivariate spatial data", "comment": "38 pages, 3 figures, 8 tables", "summary": "Spatial generalized linear mixed-effects methods are popularly used to model\nspatially indexed univariate responses. However, with modern technology, it is\ncommon to observe vector-valued mixed-type responses, e.g., a combination of\nbinary, count, or continuous types, at each location. Methods that allow joint\nmodeling of such mixed-type multivariate spatial responses are rare. Using\nlatent multivariate Gaussian processes (GPs), we present a class of Bayesian\nspatial methods that can be employed for any combination of exponential family\nresponses. Since multivariate GP-based methods can suffer from computational\nbottlenecks when the number of spatial locations is high, we further employ a\ncomputationally efficient Vecchia approximation for fast posterior inference\nand prediction. Key theoretical properties of the proposed model, such as\nidentifiability and the structure of the induced covariance, are established.\nOur approach employs a Markov chain Monte Carlo-based inference method that\nutilizes elliptical slice sampling in a blocked Metropolis-within-Gibbs\nsampling framework. We illustrate the efficacy of the proposed method through\nsimulation studies and a real-data application on joint modeling of wildfire\ncounts and burnt areas across the United States."}
{"id": "2510.12917", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.12917", "abs": "https://arxiv.org/abs/2510.12917", "authors": ["Aiden Gundersen", "Neil J. Cornish"], "title": "Escaping Neal's Funnel: a multi-stage sampling method for hierarchical models", "comment": "8 pages, 4 figures", "summary": "Neal's funnel refers to an exponential tapering in probability densities\ncommon to Bayesian hierarchical models. Usual sampling methods, such as Markov\nChain Monte Carlo, struggle to efficiently sample the funnel. Reparameterizing\nthe model or analytically marginalizing local parameters are common techniques\nto remedy sampling pathologies in distributions exhibiting Neal's funnel. In\nthis paper, we show that the challenges of Neal's funnel can be avoided by\nperforming the hierarchical analysis, well, hierarchically. That is, instead of\nsampling all parameters of the hierarchical model jointly, we break the\nsampling into multiple stages. The first stage samples a generalized\n(higher-dimensional) hierarchical model which is parameterized to lessen the\nsharpness of the funnel. The next stage samples from the estimated density of\nthe first stage, but under a constraint which restricts the sampling to recover\nthe marginal distributions on the hyper-parameters of the original\n(lower-dimensional) hierarchical model. A normalizing flow can be used to\nrepresent the distribution from the first stage, such that it can easily be\nsampled from for the second stage of the analysis. This technique is useful\nwhen effective reparameterizations are computationally expensive to calculate,\nor a generalized hierarchical model already exists from which it is easy to\nsample."}
{"id": "2510.13445", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13445", "abs": "https://arxiv.org/abs/2510.13445", "authors": ["Santiago Mazuelas", "Veronica Alvarez"], "title": "Robust Minimax Boosting with Performance Guarantees", "comment": null, "summary": "Boosting methods often achieve excellent classification accuracy, but can\nexperience notable performance degradation in the presence of label noise.\nExisting robust methods for boosting provide theoretical robustness guarantees\nfor certain types of label noise, and can exhibit only moderate performance\ndegradation. However, previous theoretical results do not account for realistic\ntypes of noise and finite training sizes, and existing robust methods can\nprovide unsatisfactory accuracies, even without noise. This paper presents\nmethods for robust minimax boosting (RMBoost) that minimize worst-case error\nprobabilities and are robust to general types of label noise. In addition, we\nprovide finite-sample performance guarantees for RMBoost with respect to the\nerror obtained without noise and with respect to the best possible error (Bayes\nrisk). The experimental results corroborate that RMBoost is not only resilient\nto label noise but can also provide strong classification accuracy."}
{"id": "2510.13609", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.13609", "abs": "https://arxiv.org/abs/2510.13609", "authors": ["Ahmad Awad", "Erik Scharwächter"], "title": "Model-assisted estimation for MRV: How to boost the economics of SOC sequestration projects without compromising on scientific integrity", "comment": null, "summary": "Soil organic carbon (SOC) sequestration projects require unbiased, precise\nand cost-effective Monitoring, Reporting, and Verification (MRV) systems that\nbalance sampling costs against uncertainty deductions imposed by regulatory\nframeworks. Design-based estimators guarantee unbiasedness but cannot exploit\nauxiliary data. Model-based approaches (VCS Methodology VT0014 v1.0 (2025)) can\nimprove precision but require independent validation for each project.\nModel-assisted estimation offers a robust compromise, combining model\npredictions with probability sampling to retain design-based guarantees while\nimproving precision. We evaluate the scientific integrity and efficiency of the\nsimple regression estimator (SRE), a well-known model-assisted estimator, via\nan extensive simulation study. Our simulations span diverse SOC stock\nvariances, sample sizes, and model performances. We assess three core\nproperties: empirical bias, empirical confidence interval coverage, and\nprecision gain relative to the design-based Horvitz-Thompson estimator (HTE).\nResults show negligible bias and valid coverage probabilities for n > 40,\nregardless of SOC stock variance. Below this threshold, variance approximations\nand normality assumptions yield unreliable uncertainty estimates. With\ncorrelated ancillary variables (r^2 = 0.3), SRE achieves 30% precision gains\nover HTE. With uncorrelated variables, no gains are observed, but performance\nconverges to HTE for n >= 40. Model-assisted estimation can enhance project\neconomics without compromising scientific rigor. Regulators should permit such\nestimators while mandating minimum sample size thresholds. Project proponents\nshould routinely employ such estimators when correlated ancillary variables\nexist. The industry should prioritize the retrieval of high-quality,\nproject-specific covariates to maximize precision gains and thereby the project\neconomics."}
{"id": "2510.13010", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.13010", "abs": "https://arxiv.org/abs/2510.13010", "authors": ["Chi Zhang", "Peijun Sang", "Yingli Qin"], "title": "Learning Shared and Source-specific Subspaces across Multiple Data Sources for Functional Data", "comment": null, "summary": "In the era of big data, integrating multi-source functional data to extract a\nsubspace that captures the shared subspace across sources has attracted\nconsiderable attention. In practice, data collection procedures often follow\nsource-specific protocols. Directly averaging sample covariance operators\nacross sources implicitly assumes homogeneity, which may bias the recovery of\nboth shared and source-specific variation patterns. To address this issue, we\npropose a projection-based data integration method that explicitly separates\nthe shared and source-specific subspaces. The method first estimates\nsource-specific projection operators via smoothing to accommodate the\nnonparametric nature of functional data. The shared subspace is then isolated\nby examining the eigenvalues of the averaged projection operator across all\nsources. If a source-specific subspace is of interest, we re-project the\nassociated source-specific covariance estimator onto the subspace orthogonal to\nthe estimated shared subspace, and estimate the source-specific subspace from\nthe resulting projection. We further establish the asymptotic properties of\nboth the shared and source-specific subspace estimators. Extensive simulation\nstudies demonstrate the effectiveness of the proposed method across a wide\nrange of settings. Finally, we illustrate its practical utility with an example\nof air pollutant data."}
{"id": "2510.13583", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13583", "abs": "https://arxiv.org/abs/2510.13583", "authors": ["Francesco Montagna"], "title": "On the identifiability of causal graphs with multiple environments", "comment": "Preprint", "summary": "Causal discovery from i.i.d. observational data is known to be generally\nill-posed. We demonstrate that if we have access to the distribution of a\nstructural causal model, and additional data from only two environments that\nsufficiently differ in the noise statistics, the unique causal graph is\nidentifiable. Notably, this is the first result in the literature that\nguarantees the entire causal graph recovery with a constant number of\nenvironments and arbitrary nonlinear mechanisms. Our only constraint is the\nGaussianity of the noise terms; however, we propose potential ways to relax\nthis requirement. Of interest on its own, we expand on the well-known duality\nbetween independent component analysis (ICA) and causal discovery; recent\nadvancements have shown that nonlinear ICA can be solved from multiple\nenvironments, at least as many as the number of sources: we show that the same\ncan be achieved for causal discovery while having access to much less auxiliary\ninformation."}
{"id": "2510.13672", "categories": ["stat.AP", "q-bio.QM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.13672", "abs": "https://arxiv.org/abs/2510.13672", "authors": ["Marcílio Ferreira dos Santos", "Andreza dos Santos Rodrigues de Melo"], "title": "Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping", "comment": "12 pages, 12 figures, 8 tables", "summary": "Dengue remains one of Brazil's major epidemiological challenges, marked by\nstrong intra-urban inequalities and the influence of climatic and\nsocio-environmental factors. This study analyzed confirmed dengue cases in\nRecife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model\nimplemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal\ncomponent. Covariates included population density, household size, income,\ndrainage channels, lagged precipitation, and mean temperature. Population\ndensity and household size had positive effects on dengue risk, while income\nand channel presence were protective. Lagged precipitation increased risk, and\nhigher temperatures showed an inverse association, suggesting thermal\nthresholds for vector activity. The model achieved good fit (DIC=65817;\nWAIC=64506) and stable convergence, with moderate residual spatial\nautocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019.\nSpatio-temporal estimates revealed persistent high-risk clusters in northern\nand western Recife, overlapping with areas of higher density and social\nvulnerability. Beyond reproducing historical patterns, the Bayesian model\nsupports probabilistic forecasting and early warning systems. Compared with\nclassical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty\nand spatial-temporal dependence, offering credible interval inference for\ndecision-making in urban health management."}
{"id": "2510.13159", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.13159", "abs": "https://arxiv.org/abs/2510.13159", "authors": ["Hung Hung", "Zhi-Yu Jou", "Su-Yun Huang", "Shinto Eguchi"], "title": "The $φ$-PCA Framework: A Unified and Efficiency-Preserving Approach with Robust Variants", "comment": "27 pages, 4 figures", "summary": "Principal component analysis (PCA) is a fundamental tool in multivariate\nstatistics, yet its sensitivity to outliers and limitations in distributed\nenvironments restrict its effectiveness in modern large-scale applications. To\naddress these challenges, we introduce the $\\phi$-PCA framework which provides\na unified formulation of robust and distributed PCA. The class of $\\phi$-PCA\nmethods retains the asymptotic efficiency of standard PCA, while aggregating\nmultiple local estimates using a proper $\\phi$ function enhances\nordering-robustness, leading to more accurate eigensubspace estimation under\ncontamination. Notably, the harmonic mean PCA (HM-PCA), corresponding to the\nchoice $\\phi(u)=u^{-1}$, achieves optimal ordering-robustness and is\nrecommended for practical use. Theoretical results further show that robustness\nincreases with the number of partitions, a phenomenon seldom explored in the\nliterature on robust or distributed PCA. Altogether, the partition-aggregation\nprinciple underlying $\\phi$-PCA offers a general strategy for developing robust\nand efficiency-preserving methodologies applicable to both robust and\ndistributed data analysis."}
{"id": "2510.13763", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13763", "abs": "https://arxiv.org/abs/2510.13763", "authors": ["Yang Yang", "Severi Rissanen", "Paul E. Chang", "Nasrulloh Loka", "Daolang Huang", "Arno Solin", "Markus Heinonen", "Luigi Acerbi"], "title": "PriorGuide: Test-Time Prior Adaptation for Simulation-Based Inference", "comment": "35 pages, 6 figures", "summary": "Amortized simulator-based inference offers a powerful framework for tackling\nBayesian inference in computational fields such as engineering or neuroscience,\nincreasingly leveraging modern generative methods like diffusion models to map\nobserved data to model parameters or future predictions. These approaches yield\nposterior or posterior-predictive samples for new datasets without requiring\nfurther simulator calls after training on simulated parameter-data pairs.\nHowever, their applicability is often limited by the prior distribution(s) used\nto generate model parameters during this training phase. To overcome this\nconstraint, we introduce PriorGuide, a technique specifically designed for\ndiffusion-based amortized inference methods. PriorGuide leverages a novel\nguidance approximation that enables flexible adaptation of the trained\ndiffusion model to new priors at test time, crucially without costly\nretraining. This allows users to readily incorporate updated information or\nexpert knowledge post-training, enhancing the versatility of pre-trained\ninference models."}
{"id": "2510.13780", "categories": ["stat.AP", "G.3; I.5.4; I.2.1"], "pdf": "https://arxiv.org/pdf/2510.13780", "abs": "https://arxiv.org/abs/2510.13780", "authors": ["Yingzhi Tao", "Chang Yang"], "title": "Macro-Level Correlational Analysis of Mental Disorders: Economy, Education, Society, and Technology Development", "comment": "8 pages, 5 figures, ICDM workshop", "summary": "This paper quantifies the age-stratified global burden of four mental\ndisorders in 27 regions from 1990 to 2021 using GBD 2021. To put it in detail,\nit links the age-standardized years of disability adjustment with 18 world\ndevelopment indicators across economic, educational, social and information\ntechnology sectors. Then, by means of Pearson correlation, mutual information,\nGranger causality and maximum information coefficient and other methods, the\nlinear, nonlinear and lagged dependency relationships were evaluated. After\nresearch, it was found that there is a very prominent spatio-temporal\nheterogeneity among young people aged 20 to 39, and the coupling relationship\nis stronger. From the overall situation, education corresponds to a low burden.\nUnemployment corresponds to a high burden. Through lag analysis, it can be\nknown that the influence time of economic and technological factors is\nrelatively short, while that of educational factors is relatively long. These\nresults highlight the macro determinants that play a role at different time\nscales and also provide population-level references for verifying computational\nmental health models and for intervention measures in specific regions and for\nspecific ages."}
{"id": "2510.13216", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.13216", "abs": "https://arxiv.org/abs/2510.13216", "authors": ["David Kronthaler", "Leonhard Held"], "title": "Edgington's Method for Random-Effects Meta-Analysis Part II: Prediction", "comment": null, "summary": "Statistical inference about the average effect in random-effects\nmeta-analysis has been considered insufficient in the presence of substantial\nbetween-study heterogeneity. Predictive distributions are well-suited for\nquantifying heterogeneity since they are interpretable on the effect scale and\nprovide clinically relevant information about future events. We construct\npredictive distributions accounting for uncertainty through confidence\ndistributions from Edgington's $p$-value combination method and the generalized\nheterogeneity statistic. Simulation results suggest that 95% prediction\nintervals typically achieve nominal coverage when more than three studies are\navailable and effectively reflect skewness in effect estimates in scenarios\nwith 20 or less studies. Formulations that ignore uncertainty in heterogeneity\nestimation typically fail to achieve correct coverage, underscoring the need\nfor this adjustment in random-effects meta-analysis."}
{"id": "2510.13159", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.13159", "abs": "https://arxiv.org/abs/2510.13159", "authors": ["Hung Hung", "Zhi-Yu Jou", "Su-Yun Huang", "Shinto Eguchi"], "title": "The $φ$-PCA Framework: A Unified and Efficiency-Preserving Approach with Robust Variants", "comment": "27 pages, 4 figures", "summary": "Principal component analysis (PCA) is a fundamental tool in multivariate\nstatistics, yet its sensitivity to outliers and limitations in distributed\nenvironments restrict its effectiveness in modern large-scale applications. To\naddress these challenges, we introduce the $\\phi$-PCA framework which provides\na unified formulation of robust and distributed PCA. The class of $\\phi$-PCA\nmethods retains the asymptotic efficiency of standard PCA, while aggregating\nmultiple local estimates using a proper $\\phi$ function enhances\nordering-robustness, leading to more accurate eigensubspace estimation under\ncontamination. Notably, the harmonic mean PCA (HM-PCA), corresponding to the\nchoice $\\phi(u)=u^{-1}$, achieves optimal ordering-robustness and is\nrecommended for practical use. Theoretical results further show that robustness\nincreases with the number of partitions, a phenomenon seldom explored in the\nliterature on robust or distributed PCA. Altogether, the partition-aggregation\nprinciple underlying $\\phi$-PCA offers a general strategy for developing robust\nand efficiency-preserving methodologies applicable to both robust and\ndistributed data analysis."}
{"id": "2510.13233", "categories": ["stat.ME", "stat.CO", "62M30"], "pdf": "https://arxiv.org/pdf/2510.13233", "abs": "https://arxiv.org/abs/2510.13233", "authors": ["Arghya Mukherjee", "Arnab Hazra", "Dootika Vats"], "title": "Scalable Bayesian inference for high-dimensional mixed-type multivariate spatial data", "comment": "38 pages, 3 figures, 8 tables", "summary": "Spatial generalized linear mixed-effects methods are popularly used to model\nspatially indexed univariate responses. However, with modern technology, it is\ncommon to observe vector-valued mixed-type responses, e.g., a combination of\nbinary, count, or continuous types, at each location. Methods that allow joint\nmodeling of such mixed-type multivariate spatial responses are rare. Using\nlatent multivariate Gaussian processes (GPs), we present a class of Bayesian\nspatial methods that can be employed for any combination of exponential family\nresponses. Since multivariate GP-based methods can suffer from computational\nbottlenecks when the number of spatial locations is high, we further employ a\ncomputationally efficient Vecchia approximation for fast posterior inference\nand prediction. Key theoretical properties of the proposed model, such as\nidentifiability and the structure of the induced covariance, are established.\nOur approach employs a Markov chain Monte Carlo-based inference method that\nutilizes elliptical slice sampling in a blocked Metropolis-within-Gibbs\nsampling framework. We illustrate the efficacy of the proposed method through\nsimulation studies and a real-data application on joint modeling of wildfire\ncounts and burnt areas across the United States."}
{"id": "2510.13347", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.13347", "abs": "https://arxiv.org/abs/2510.13347", "authors": ["Mathias Lerbech Jeppesen", "Emilie Højbjerre-Frandsen"], "title": "postcard: An R Package for Marginal Effect Estimation with or without Prognostic Score Adjustment", "comment": "21 pages, 2 figures", "summary": "Covariate adjustment is a widely used technique in randomized clinical trials\n(RCTs) for improving the efficiency of treatment effect estimators. By\nadjusting for predictive baseline covariates, variance can be reduced,\nenhancing statistical precision and study power. Rosenblum and van der Laan\n[2010] use the framework of generalized linear models (GLMs) in a plug-in\nanalysis to show efficiency gains using covariate adjustment for marginal\neffect estimation. Recently the use of prognostic scores as adjustment\ncovariates has gained popularity. Schuler et al. [2022] introduce and validate\nthe method for continuous endpoints using linear models. Building on this work\nH{\\o}jbjerre-Frandsen et al. [2025] extends the method proposed by Schuler et\nal. [2022] to be used in combination with the GLM plug-in procedure [Rosenblum\nand van der Laan, 2010]. This method achieves semi-parametric efficiency under\nassumptions of additive treatment effects on the link scale. Additionally,\nH{\\o}jbjerre-Frandsen et al. [2025] provide a formula for power approximation\nwhich is valid even under model misspecification, enabling realistic sample\nsize estimation. This article introduces an R package, which implements the GLM\nplug-in method with or without PrOgnoSTic CovARiate aDjustment, postcard. The\npackage has two core features: (1) estimating marginal effects and the variance\nhereof (with or without prognostic adjustment) and (2) approximating\nstatistical power. Functionalities also include integration of the Discrete\nSuper Learner for constructing prognostic scores and simulation capabilities\nfor exploring the methods in practice. Through examples and simulations, we\ndemonstrate postcard as a practical toolkit for statisticians."}
{"id": "2510.13377", "categories": ["stat.ME", "62P10"], "pdf": "https://arxiv.org/pdf/2510.13377", "abs": "https://arxiv.org/abs/2510.13377", "authors": ["Na Lei", "Mark A. Wolters", "Wenqing He"], "title": "A Flexible Partially Linear Single Index Proportional Hazards Regression Model for Multivariate Survival Data", "comment": "19 pages, 4 figures", "summary": "We address the problem of survival regression modelling with multivariate\nresponses and nonlinear covariate effects. Our model extends the proportional\nhazards model by introducing several weakly-parametric elements: the marginal\nbaseline hazard functions are expressed as piecewise constants, association is\nmodelled with copulas, and nonlinear covariate effects are handled by a\nsingle-index structure using a spline. The model permits a full likelihood\napproach to inference, making it possible to obtain individual-level survival\nor hazard function estimates. Performance of the new model is evaluated through\nsimulation studies and application to the Busselton health study data. The\nresults suggest that the proposed method can capture nonlinear covariate\neffects well, and that there is benefit to modeling the association between the\ncorrelated responses."}
{"id": "2510.13389", "categories": ["stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.13389", "abs": "https://arxiv.org/abs/2510.13389", "authors": ["Tien-En Chang", "Argon Chen"], "title": "Understanding and Using the Relative Importance Measures Based on Orthonormality Transformation", "comment": "20 pages, 10 figures", "summary": "A class of relative importance measures based on orthonormality\ntransformation (OTMs), has been found to effectively approximate the General\nDominance index (GD). In particular, Johnson's Relative Weight (RW) has been\ndeemed the most successful OTM in the literature. Nevertheless, the theoretical\nfoundation of the OTMs remains unclear. To further understand the OTMs, we\nprovide a generalized framework that breaks down the OTM into two functional\nsteps: orthogonalization and reallocation. To assess the impact of each step on\nthe performance of OTMs, we conduct extensive Monte Carlo simulations under\nvarious predictors' correlation structures and response variable distributions.\nOur findings reveal that Johnson's minimal transformation consistently\noutperforms other common orthogonalization methods. We also summarize the\nperformance of reallocation methods under four scenarios of predictors'\ncorrelation structures in terms of the first principal component and the\nvariance inflation factor (VIF). This analysis provides guidelines for\nselecting appropriate reallocation methods in different scenarios, illustrated\nwith real-world dataset examples. Our research offers a deeper understanding of\nOTMs and provides valuable insights for practitioners seeking to accurately\nmeasure variable importance in various modeling contexts."}
{"id": "2510.13636", "categories": ["stat.ME", "cs.SI", "math.ST", "stat.TH", "62R01,"], "pdf": "https://arxiv.org/pdf/2510.13636", "abs": "https://arxiv.org/abs/2510.13636", "authors": ["Félix Almendra-Hernández", "Miles Bakenhus", "Vishesh Karwa", "Mitsunori Ogawa", "Sonja Petrović"], "title": "Non-asymptotic goodness-of-fit tests and model selection in valued stochastic blockmodels", "comment": null, "summary": "A valued stochastic blockmodel (SBM) is a general way to view networked data\nin which nodes are grouped into blocks and links between them are measured by\ncounts or labels. This family allows for varying dyad sampling schemes, thereby\nincluding the classical, Poisson, and labeled SBMs, as well as those in which\nsome edge observations are censored. This paper addresses the question of\ntesting goodness-of-fit of such non-Bernoulli SBMs, focusing in particular on\nfinite-sample tests. We derive explicit Markov bases moves necessary to\ngenerate samples from reference distributions and define goodness-of-fit\nstatistics for determining model fit, comparable to those in the literature for\nrelated model families.\n  For the labeled SBM, which includes in particular the censored-edge model, we\nstudy the asymptotic behavior of said statistics. One of the main purposes of\ntesting goodness-of-fit of an SBM is to determine whether block membership of\nthe nodes influences network formation. Power and Type 1 error rates are\nverified on simulated data. Additionally, we discuss the use of asymptotic\nresults in selecting the number of blocks under the latent-block modeling\nassumption. The method derived for Poisson SBM is applied to ecological\nnetworks of host-parasite interactions. Our data analysis conclusions differ in\nselecting the number of blocks for the species from previous results in the\nliterature."}
{"id": "2510.13715", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.13715", "abs": "https://arxiv.org/abs/2510.13715", "authors": ["Younghoon Kim", "Po-Ling Loh", "Sumanta Basu"], "title": "Exact Coordinate Descent for High-Dimensional Regularized Huber Regression", "comment": null, "summary": "We develop an exact coordinate descent algorithm for high-dimensional\nregularized Huber regression. In contrast to composite gradient descent\nmethods, our algorithm fully exploits the advantages of coordinate descent when\nthe underlying model is sparse. Moreover, unlike existing second-order\napproximation methods previously introduced in the literature, it remains\neffective even when the Hessian becomes ill-conditioned due to high\ncorrelations among covariates drawn from heavy-tailed distributions. The key\nidea is that, for each coordinate, marginal increments arise only from inlier\nobservations, while the derivatives remain monotonically increasing over a grid\nconstructed from the partial residuals. Building on conventional coordinate\ndescent strategies, we further propose variable screening rules that\nselectively determine which variables to update at each iteration, thereby\naccelerating convergence. To the best of our knowledge, this is the first work\nto develop a first-order coordinate descent algorithm for penalized Huber loss\nminimization. We bound the nonasymptotic convergence rate of the proposed\nalgorithm by extending arguments developed for the Lasso and formally\ncharacterize the operation of the proposed screening rule. Extensive simulation\nstudies under heavy-tailed and highly-correlated predictors, together with a\nreal data application, demonstrate both the practical efficiency of the method\nand the benefits of the computational enhancements."}
{"id": "2510.12963", "categories": ["stat.AP", "physics.soc-ph", "stat.ME", "60G70 (Primary) 62F15, 62G32, 62P30 (Secondary)", "I.6.4; J.2; G.3; J.7"], "pdf": "https://arxiv.org/pdf/2510.12963", "abs": "https://arxiv.org/abs/2510.12963", "authors": ["Parvez Anowar", "Nazmul Haque", "Md Asif Raihan", "Md Hadiuzzaman"], "title": "Trajectory-based real-time pedestrian crash prediction at intersections: A novel non-linear link function for block maxima led Bayesian GEV framework addressing heterogeneous traffic condition", "comment": "This manuscript is a preprint and has not yet been peer-reviewed. It\n  is currently being considered for submission to a peer-reviewed journal", "summary": "This study develops a real-time framework for estimating pedestrian crash\nrisk at signalized intersections under heterogeneous, non-lane-based traffic.\nExisting approaches often assume linear relationships between covariates and\nparameters, oversimplifying the complex, non-monotonic interactions among\ndifferent road users. To overcome this, the framework introduces a non-linear\nlink function within a Bayesian generalized extreme value (GEV) structure to\ncapture traffic variability more accurately. The framework applies extreme\nvalue theory through the block maxima approach using post-encroachment time as\na surrogate safety measure. A hierarchical Bayesian model incorporating both\nlinear and non-linear link functions into GEV parameters is estimated using\nMarkov Chain Monte Carlo simulation. It also introduces a behavior-normalized\nModified Crash Risk (MRC) formula to account for pedestrians' habitual\nrisk-taking behavior. Seven Bayesian hierarchical models were developed and\ncompared using deviance information criterion. Models employing non-linear link\nfunctions for the location and scale parameters significantly outperformed\ntheir linear counterparts. The results revealed that pedestrian speed has a\nnegative relationship with crash risk, while flow and speed of motorized\nvehicles, pedestrian flow, and non-motorized vehicles conflicting speed\ncontribute positively. The MRC formulation reduced overestimation and provided\ncrash predictions with 93% confidence. The integration of non-linear link\nfunctions enhances model flexibility, capturing the non-linear nature of\ntraffic extremes. The proposed MRC metric aligns crash risk estimates with\nreal-world pedestrian behavior in mixed-traffic environments. This framework\noffers a practical analytical tool for traffic engineers and planners to design\nadaptive signal control and pedestrian safety interventions before crashes\noccur."}
{"id": "2510.12983", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.12983", "abs": "https://arxiv.org/abs/2510.12983", "authors": ["Lorenzo Marinucci", "Gabriele D'Acunto", "Paolo Di Lorenzo", "Sergio Barbarossa"], "title": "Simplicial Gaussian Models: Representation and Inference", "comment": null, "summary": "Probabilistic graphical models (PGMs) are powerful tools for representing\nstatistical dependencies through graphs in high-dimensional systems. However,\nthey are limited to pairwise interactions. In this work, we propose the\nsimplicial Gaussian model (SGM), which extends Gaussian PGM to simplicial\ncomplexes. SGM jointly models random variables supported on vertices, edges,\nand triangles, within a single parametrized Gaussian distribution. Our model\nbuilds upon discrete Hodge theory and incorporates uncertainty at every\ntopological level through independent random components. Motivated by\napplications, we focus on the marginal edge-level distribution while treating\nnode- and triangle-level variables as latent. We then develop a\nmaximum-likelihood inference algorithm to recover the parameters of the full\nSGM and the induced conditional dependence structure. Numerical experiments on\nsynthetic simplicial complexes with varying size and sparsity confirm the\neffectiveness of our algorithm."}
{"id": "2510.13609", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.13609", "abs": "https://arxiv.org/abs/2510.13609", "authors": ["Ahmad Awad", "Erik Scharwächter"], "title": "Model-assisted estimation for MRV: How to boost the economics of SOC sequestration projects without compromising on scientific integrity", "comment": null, "summary": "Soil organic carbon (SOC) sequestration projects require unbiased, precise\nand cost-effective Monitoring, Reporting, and Verification (MRV) systems that\nbalance sampling costs against uncertainty deductions imposed by regulatory\nframeworks. Design-based estimators guarantee unbiasedness but cannot exploit\nauxiliary data. Model-based approaches (VCS Methodology VT0014 v1.0 (2025)) can\nimprove precision but require independent validation for each project.\nModel-assisted estimation offers a robust compromise, combining model\npredictions with probability sampling to retain design-based guarantees while\nimproving precision. We evaluate the scientific integrity and efficiency of the\nsimple regression estimator (SRE), a well-known model-assisted estimator, via\nan extensive simulation study. Our simulations span diverse SOC stock\nvariances, sample sizes, and model performances. We assess three core\nproperties: empirical bias, empirical confidence interval coverage, and\nprecision gain relative to the design-based Horvitz-Thompson estimator (HTE).\nResults show negligible bias and valid coverage probabilities for n > 40,\nregardless of SOC stock variance. Below this threshold, variance approximations\nand normality assumptions yield unreliable uncertainty estimates. With\ncorrelated ancillary variables (r^2 = 0.3), SRE achieves 30% precision gains\nover HTE. With uncorrelated variables, no gains are observed, but performance\nconverges to HTE for n >= 40. Model-assisted estimation can enhance project\neconomics without compromising scientific rigor. Regulators should permit such\nestimators while mandating minimum sample size thresholds. Project proponents\nshould routinely employ such estimators when correlated ancillary variables\nexist. The industry should prioritize the retrieval of high-quality,\nproject-specific covariates to maximize precision gains and thereby the project\neconomics."}
{"id": "2510.13672", "categories": ["stat.AP", "q-bio.QM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.13672", "abs": "https://arxiv.org/abs/2510.13672", "authors": ["Marcílio Ferreira dos Santos", "Andreza dos Santos Rodrigues de Melo"], "title": "Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping", "comment": "12 pages, 12 figures, 8 tables", "summary": "Dengue remains one of Brazil's major epidemiological challenges, marked by\nstrong intra-urban inequalities and the influence of climatic and\nsocio-environmental factors. This study analyzed confirmed dengue cases in\nRecife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model\nimplemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal\ncomponent. Covariates included population density, household size, income,\ndrainage channels, lagged precipitation, and mean temperature. Population\ndensity and household size had positive effects on dengue risk, while income\nand channel presence were protective. Lagged precipitation increased risk, and\nhigher temperatures showed an inverse association, suggesting thermal\nthresholds for vector activity. The model achieved good fit (DIC=65817;\nWAIC=64506) and stable convergence, with moderate residual spatial\nautocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019.\nSpatio-temporal estimates revealed persistent high-risk clusters in northern\nand western Recife, overlapping with areas of higher density and social\nvulnerability. Beyond reproducing historical patterns, the Bayesian model\nsupports probabilistic forecasting and early warning systems. Compared with\nclassical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty\nand spatial-temporal dependence, offering credible interval inference for\ndecision-making in urban health management."}
