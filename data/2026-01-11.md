<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 3]
- [stat.ML](#stat.ML) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [A Generalized Adaptive Joint Learning Framework for High-Dimensional Time-Varying Models](https://arxiv.org/abs/2601.04499)
*Baolin Chen,Mengfei Ran*

Main category: stat.ME

TL;DR: 提出AJL框架，用于多变量时变系数模型中的函数变量选择和结构变点检测，通过自适应组惩罚和融合正则化实现高效估计，在超高维设置下具有理论保证。


<details>
  <summary>Details</summary>
Motivation: 现代生物医学和计量经济学研究中，纵向过程常具有复杂的时变关联和跨相关结果的突发机制转换。标准函数数据分析方法强调平滑性，难以捕捉这些动态结构特征，特别是在高维设置下。

Method: 提出自适应联合学习(AJL)框架，结合自适应组惩罚和融合正则化的凸优化过程，同时进行函数变量选择和结构变点检测，通过跨结果借用强度提高估计效率。

Result: 在超高维(p>>n)设置下建立了非渐近误差界，证明AJL具有oracle性质(如同已知真实活跃集和变点位置)。通过模拟和原发性胆汁性肝硬化数据分析验证了方法有效性。

Conclusion: AJL框架能有效捕捉多变量时变系数模型中的动态结构特征，发现疾病进展中的同步阶段转换，识别简约的时变预后标志物，为高维纵向数据分析提供有力工具。

Abstract: In modern biomedical and econometric studies, longitudinal processes are often characterized by complex time-varying associations and abrupt regime shifts that are shared across correlated outcomes. Standard functional data analysis (FDA) methods, which prioritize smoothness, often fail to capture these dynamic structural features, particularly in high-dimensional settings. This article introduces Adaptive Joint Learning (AJL), a regularization framework designed to simultaneously perform functional variable selection and structural changepoint detection in multivariate time-varying coefficient models. We propose a convex optimization procedure that synergizes adaptive group-wise penalization with fused regularization, effectively borrowing strength across multiple outcomes to enhance estimation efficiency. We provide a rigorous theoretical analysis of the estimator in the ultra-high-dimensional regime (p >> n), establishing non-asymptotic error bounds and proving that AJL achieves the oracle property--performing as well as if the true active set and changepoint locations were known a priori. A key theoretical contribution is the explicit handling of approximation bias via undersmoothing conditions to ensure valid asymptotic inference. The proposed method is validated through comprehensive simulations and an application to Primary Biliary Cirrhosis (PBC) data. The analysis uncovers synchronized phase transitions in disease progression and identifies a parsimonious set of time-varying prognostic markers.

</details>


### [2] [A new method for augmenting short time series, with application to pain events in sickle cell disease](https://arxiv.org/abs/2601.04538)
*Kumar Utkarsh,Nirmish R. Shah,Tanvi Banerjee,Daniel M. Abrams*

Main category: stat.ME

TL;DR: 提出一种结合多个稀疏时间序列数据集的新数据增强方法，用于改善参数估计和模型选择可靠性。


<details>
  <summary>Details</summary>
Motivation: 生态学、生物学和医疗保健等领域的研究者常面临稀疏数据的挑战，这会导致不确定性、估计困难和建模偏差。

Method: 开发了一种新颖的数据增强方法，当多个稀疏时间序列数据集具有相似统计特性时，将它们结合起来使用。

Result: 通过验证研究（比较Hawkes和Poisson过程）以及在镰状细胞病患者主观疼痛动态分析中的应用，证明了该方法的有效性。

Conclusion: 该方法能有效改善稀疏时间序列数据的参数估计和模型选择可靠性，特别适用于具有相似统计特性的多个数据集。

Abstract: Researchers across different fields, including but not limited to ecology, biology, and healthcare, often face the challenge of sparse data. Such sparsity can lead to uncertainties, estimation difficulties, and potential biases in modeling. Here we introduce a novel data augmentation method that combines multiple sparse time series datasets when they share similar statistical properties, thereby improving parameter estimation and model selection reliability. We demonstrate the effectiveness of this approach through validation studies comparing Hawkes and Poisson processes, followed by application to subjective pain dynamics in patients with sickle cell disease (SCD), a condition affecting millions worldwide, particularly those of African, Mediterranean, Middle Eastern, and Indian descent.

</details>


### [3] [Bayesian Additive Regression Tree Copula Processes for Scalable Distributional Prediction](https://arxiv.org/abs/2601.04913)
*Jan Martin Wenkel,Michael Stanley Smith,Nadja Klein*

Main category: stat.ME

TL;DR: 提出基于BART的copula过程模型，通过构建响应值的隐含copula过程，结合任意边际分布构建灵活分布回归，保持BART计算效率的同时提升分布预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统BART模型在分布预测方面存在局限，需要更灵活的分布回归方法。本文旨在构建一个既能保持BART计算效率，又能提供更准确分布预测的模型。

Method: 从带叶节点方差先验的BART模型中构建隐含copula过程，该过程定义在协变量空间上，可与任意边际分布结合。使用MCMC进行贝叶斯推断，通过增广后验分布实现，关键采样步骤保持与原始BART相同的计算效率。

Result: 推导出copula过程模型的后验预测分布闭式解，证明回归函数和后验均值的后验一致性，以及预测过程和条件期望的分布收敛。模拟研究显示相比原始BART和主流基准模型，分布预测精度显著提升。

Conclusion: 提出的BART copula过程模型在保持计算效率的同时，显著提升了分布预测的准确性和灵活性，在从506到515,345个观测、8到90个协变量的真实数据集中表现出色。

Abstract: We show how to construct the implied copula process of response values from a Bayesian additive regression tree (BART) model with prior on the leaf node variances. This copula process, defined on the covariate space, can be paired with any marginal distribution for the dependent variable to construct a flexible distributional BART model. Bayesian inference is performed via Markov chain Monte Carlo on an augmented posterior, where we show that key sampling steps can be realized as those of Chipman et al. (2010), preserving scalability and computational efficiency even though the copula process is high dimensional. The posterior predictive distribution from the copula process model is derived in closed form as the push-forward of the posterior predictive distribution of the underlying BART model with an optimal transport map. Under suitable conditions, we establish posterior consistency for the regression function and posterior means and prove convergence in distribution of the predictive process and conditional expectation. Simulation studies demonstrate improved accuracy of distributional predictions compared to the original BART model and leading benchmarks. Applications to five real datasets with 506 to 515,345 observations and 8 to 90 covariates further highlight the efficacy and scalability of our proposed BART copula process model.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [4] [CAOS: Conformal Aggregation of One-Shot Predictors](https://arxiv.org/abs/2601.05219)
*Maja Waldron*

Main category: stat.ML

TL;DR: CAOS：一种用于单次预测的保形聚合框架，通过自适应聚合多个单次预测器和使用留一校准方案，在违反经典可交换性假设的情况下仍能实现有效的边际覆盖，相比标准分割保形方法显著减小预测集大小。


<details>
  <summary>Details</summary>
Motivation: 单次预测能够使用单个标注样本快速适应预训练基础模型到新任务，但缺乏原则性的不确定性量化。标准分割保形方法在单次设置中效率低下，因为需要数据分割且依赖单一预测器。

Method: 提出CAOS（Conformal Aggregation of One-Shot Predictors）框架，自适应聚合多个单次预测器，使用留一校准方案充分利用稀缺的标注数据。尽管违反经典可交换性假设，但通过单调性论证证明了CAOS能够实现有效的边际覆盖。

Result: 在单次人脸关键点检测和RAFT文本分类任务上的实验表明，CAOS相比分割保形基线方法能够产生显著更小的预测集，同时保持可靠的覆盖。

Conclusion: CAOS为单次预测提供了有效的保形框架，通过聚合多个预测器和留一校准方案解决了数据稀缺问题，在违反经典假设的情况下仍能保证覆盖，显著提升了单次预测的不确定性量化效率。

Abstract: One-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient in the one-shot setting due to data splitting and reliance on a single predictor. We propose Conformal Aggregation of One-Shot Predictors (CAOS), a conformal framework that adaptively aggregates multiple one-shot predictors and uses a leave-one-out calibration scheme to fully exploit scarce labeled data. Despite violating classical exchangeability assumptions, we prove that CAOS achieves valid marginal coverage using a monotonicity-based argument. Experiments on one-shot facial landmarking and RAFT text classification tasks show that CAOS produces substantially smaller prediction sets than split conformal baselines while maintaining reliable coverage.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [5] [Comparison of Maximum Likelihood Classification Before and After Applying Weierstrass Transform](https://arxiv.org/abs/2601.04808)
*Muhammad Shoaib,Zaka Ur Rehman,Muhammad Qasim*

Main category: stat.AP

TL;DR: 使用最大似然分类法处理多光谱数据，比较应用Weierstrass变换前后的分类精度，发现变换后分类精度更高。


<details>
  <summary>Details</summary>
Motivation: 研究最大似然分类法在多光谱数据中的应用效果，探索Weierstrass变换对分类精度的影响，提高高分辨率Quickbird卫星影像的分类准确性。

Method: 采用最大似然分类算法，基于贝叶斯定理和判别函数；使用Weierstrass变换处理数据；应用主成分分析进行降维和波段变异分析；通过定性和定量方法比较变换前后的分类结果。

Result: 结果显示，应用Weierstrass变换后，最大似然分类的精度显著提高；决策空间中类别均值之间的分离度是导致精度提升的主要因素。

Conclusion: Weierstrass变换能有效提升最大似然分类在多光谱数据中的性能，通过增强类别间的分离度来实现更高的分类精度。

Abstract: The aim of this paper is to use Maximum Likelihood (ML) Classification on multispectral data by means of qualitative and quantitative approaches. Maximum Likelihood is a supervised classification algorithm which is based on the Classical Bayes theorem. It makes use of a discriminant function to assign pixel to the class with the highest likelihood. Class means vector and covariance matrix are the key inputs to the function and can be estimated from training pixels of a particular class. As Maximum Likelihood need some assumptions before it has to be applied on the data. In this paper we will compare the results of Maximum Likelihood Classification (ML) before apply the Weierstrass Transform and apply Weierstrass Transform and will see the difference between the accuracy on training pixels of high resolution Quickbird satellite image. Principle Component analysis (PCA) is also used for dimension reduction and also used to check the variation in bands. The results shows that the separation between mean of the classes in the decision space is to be the main factor that leads to the high classification accuracy of Maximum Likelihood (ML) after using Weierstrass Transform than without using it.

</details>


### [6] [A Bayesian Multi-State Data Integration Approach for Estimating County-level Prevalence of Opioid Misuse in the United States](https://arxiv.org/abs/2601.04966)
*Zixuan Feng,Qiushi Chen,Paul Griffin,Le Bao*

Main category: stat.AP

TL;DR: 提出贝叶斯多州数据整合方法，利用公开数据首次估计全美县级阿片类药物滥用流行率


<details>
  <summary>Details</summary>
Motivation: 阿片类药物滥用是美国重大公共卫生威胁，需要县级流行率数据来制定有效的资源分配策略，但现有研究仅限于少数州，缺乏全国范围的县级估计

Method: 贝叶斯多州数据整合方法，分层结构联合建模阿片滥用流行率和过量死亡结果，利用有限州的县级估计和国家级调查数据，结合协变量和混合效应，使用horseshoe+先验防止过拟合

Result: 模型通过交叉验证显示高估计精度，首次提供全国范围的县级阿片滥用流行率估计

Conclusion: 该方法填补了全国县级阿片滥用流行率估计的空白，为社区制定应对策略提供了重要数据支持，且模型具有可扩展性，能适应未来新数据的加入

Abstract: Drug overdose deaths, including from opioids, remain a significant public health threat to the United States (US). To abate the harms of opioid misuse, understanding its prevalence at the local level is crucial for stakeholders in communities to develop response strategies that effectively use limited resources. Although there exist several state-specific studies that provide county-level prevalence estimates, such estimates are not widely available across the country, as the datasets used in these studies are not always readily available in other states, which, therefore, has limited the wider applications of existing models. To fill this gap, we propose a Bayesian multi-state data integration approach that fully utilizes publicly available data sources to estimate county-level opioid misuse prevalence for all counties in the US. The hierarchical structure jointly models opioid misuse prevalence and overdose death outcomes, leverages existing county-level prevalence estimates in limited states and state-level estimates from national surveys, and accounts for heterogeneity across counties and states with counties' covariates and mixed effects. Furthermore, our parsimonious and generalizable modeling framework employs horseshoe+ prior to flexibly shrink coefficients and prevent overfitting, ensuring adaptability as new county-level prevalence data in additional states become available. Using real-world data, our model shows high estimation accuracy through cross-validation and provides nationwide county-level estimates of opioid misuse for the first time.

</details>


### [7] [Estimating Consensus Ideal Points Using Multi-Source Data](https://arxiv.org/abs/2601.05213)
*Mellissa Meisels,Melody Huang,Tiffany M. Tang*

Main category: stat.AP

TL;DR: 提出共识多维标度(CoMDS)方法，从多个国会候选人理想点估计中提取共享空间表示，解决现有测量间弱相关性和内生性问题


<details>
  <summary>Details</summary>
Motivation: 在大数据和机器学习时代，研究者拥有大量国会候选人理想点估计用于理论检验，但这些测量之间存在弱相关性，引发它们是否捕捉到共享量度而非特定领域因素的疑问。同时，现有研究通常互换使用不同测量，且美国政治研究中的核心问题涉及从相同数据源衍生的变量间关系，存在内生性问题。

Method: 提出共识多维标度(CoMDS)方法，从一组基础理想点估计中捕捉共享、稳定的关联，可解释为它们的共同空间表示。该方法更符合应用学者在实践中使用理想点的方式。

Result: CoMDS方法能够评估现有测量领域内的关系，并提供一套诊断工具辅助实际使用，有助于解决测量间的弱相关性和内生性问题。

Conclusion: 共识多维标度(CoMDS)为国会候选人理想点研究提供了更可靠的方法框架，能够从多个测量中提取共享空间表示，改善理论检验的准确性，并提供实用诊断工具。

Abstract: In the advent of big data and machine learning, researchers now have a wealth of congressional candidate ideal point estimates at their disposal for theory testing. Weak relationships raise questions about the extent to which they capture a shared quantity -- rather than idiosyncratic, domain-specific factors -- yet different measures are used interchangeably in most substantive analyses. Moreover, questions central to the study of American politics implicate relationships between candidate ideal points and other variables derived from the same data sources, introducing endogeneity. We propose a method, consensus multidimensional scaling (CoMDS), which better aligns with how applied scholars use ideal points in practice. CoMDS captures the shared, stable associations of a set of underlying ideal point estimates and can be interpreted as their common spatial representation. We illustrate the utility of our approach for assessing relationships within domains of existing measures and provide a suite of diagnostic tools to aid in practical usage.

</details>
