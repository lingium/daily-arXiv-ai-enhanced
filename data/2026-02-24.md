<div id=toc></div>

# Table of Contents

- [stat.OT](#stat.OT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 16]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ML](#stat.ML) [Total: 6]


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [1] [Reflections on the Future of Statistics Education in a Technological Era](https://arxiv.org/abs/2602.18242)
*Craig Alexander,Jennifer Gaskell,Vinny Davies*

Main category: stat.OT

TL;DR: 本文探讨了统计学教育如何整合现代技术工具（R、Python、数据库、机器学习、AI）以适应技术快速发展的挑战


<details>
  <summary>Details</summary>
Motivation: 技术快速演变给统计学教学带来挑战，需要为学生配备现代职场所需技能，因此必须将相关技术整合到统计学课程中

Method: 通过分析大学统计学教育的技术变革，探讨统计编程（R和Python）、编码实践、数据库管理、机器学习等技术在课程中的整合策略

Result: 统计学教育已发生重大技术变革，统计编程成为核心，R广泛使用，Python在统计和数据分析项目中日益普及，编码、数据库管理和机器学习已进入部分课程

Conclusion: 未来统计学教育将更加重视人工智能，特别是生成式AI工具（如ChatGPT）的教学意义，需要制定策略将这些技术发展整合到当代统计学教育中

Abstract: Keeping pace with rapidly evolving technology is a key challenge in teaching statistics. To equip students with essential skills for the modern workplace, educators must integrate relevant technologies into the statistical curriculum where possible. University-level statistics education has experienced substantial technological change, particularly in the tools and practices that underpin teaching and learning. Statistical programming has become central to many courses, with R widely used and Python increasingly incorporated into statistics and data analytics programmes. Additionally, coding practices, database management, and machine learning now feature within some statistics curricula. Looking ahead, we anticipate a growing emphasis on artificial intelligence (AI), particularly the pedagogical implications of generative AI tools such as ChatGPT. In this article, we explore these technological developments and discuss strategies for their integration into contemporary statistics education.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [2] [Model Error Embedding with Orthogonal Gaussian Processes](https://arxiv.org/abs/2602.17923)
*Mridula Kuppa,Khachik Sargsyan,Marco Panesi,Habib N. Najm*

Main category: stat.ME

TL;DR: 提出一种嵌入模型误差框架，使用高斯过程权重空间表示来捕捉模型误差的时空相关性，通过正交约束分离模型参数和模型误差参数，并采用似然信息子空间方法处理高维问题。


<details>
  <summary>Details</summary>
Motivation: 复杂物理系统的计算模型通常依赖简化假设，这不可避免地引入模型误差，导致预测误差。需要一种方法能够从数据中估计参数化的模型误差表示，同时分离模型参数和模型误差参数的贡献，以确保有意义的独立模型预测。

Method: 构建嵌入模型误差框架，使用高斯过程的权重空间表示来灵活捕捉模型误差的时空相关性；扩展现有的正交高斯过程方法到嵌入模型误差设置，推导适当的正交性约束；采用似然信息子空间方法处理高斯过程表示引入的高维度问题。

Result: 在线性和非线性示例中，该方法有效地修正模型预测以匹配数据趋势；在训练数据之外的推断恢复了先验预测分布；正交性约束导致有意义的独立模型预测，以及模型参数和模型误差参数之间几乎不相关的后验分布。

Conclusion: 该框架成功地将模型误差表示为高斯过程，通过正交约束实现了模型参数和模型误差参数的分离，为复杂物理系统的模型校正提供了一种有效的方法，能够产生有意义的独立模型预测并处理高维问题。

Abstract: Computational models of complex physical systems often rely on simplifying assumptions which inevitably introduce model error, with consequent predictive errors. Given data on model observables, the estimation of parameterized model-error representations, along with other model parameters, would be ideally done while separating the contributions of each of the two sets of parameters, in order to ensure meaningful stand-alone model predictions. This work builds an embedded model error framework using a weight-space representation of Gaussian processes (GPs) to flexibly capture model-error spatiotemporal correlations and enable inference with GP-embedding in non-linear models. To disambiguate model and model-error/bias parameters, we extend an existing orthogonal GP method to the embedded model-error setting and derive appropriate orthogonality constraints. To address the increased dimensionality introduced by the GP representation, we employ the likelihood-informed subspace method. The construction is demonstrated on linear and non-linear examples, where it effectively corrects model predictions to match data trends. Extrapolation beyond the training data recovers the prior predictive distribution, and the orthogonality constraints lead to meaningful stand-alone model predictions and nearly uncorrelated posteriors between model and model-error parameters.

</details>


### [3] [A variational framework for modal estimation](https://arxiv.org/abs/2602.17956)
*Tâm LeMinh,Julyan Arbel,Florence Forbes,Hien Duy Nguyen*

Main category: stat.ME

TL;DR: GERVE是一种基于吉布斯分布的无似然多变量众数估计框架，通过熵正则化变分目标直接近似吉布斯测度，结合核密度估计、均值漂移、变分推断和退火技术，用于众数估计和聚类。


<details>
  <summary>Details</summary>
Motivation: 当众数数量或分组未知且完整密度估计不可行时，需要一种实用的无似然方法来估计多变量众数。现有方法通常需要指定成分数量或进行完整的密度估计，这在复杂场景中不切实际。

Method: GERVE通过最大化熵正则化变分目标直接近似吉布斯测度，使用自然梯度更新。它结合了核密度估计、均值漂移、变分推断和退火技术，拟合集中在高密度区域的高斯混合模型，通过责任分配获得聚类结果。

Result: 理论分析表明：当吉布斯温度趋近于零时，混合成分收敛于总体众数；在固定温度下，经验目标的最大化器存在、一致且渐近正态。模拟和真实数据研究显示准确的众数恢复和涌现聚类，对混合模型过参数化具有鲁棒性。

Conclusion: GERVE是一个实用的无似然框架，适用于众数数量未知且完整密度估计不可行的情况，能够准确恢复众数并产生鲁棒的聚类结果，减少了对选定成分数量的敏感性。

Abstract: We approach multivariate mode estimation through Gibbs distributions and introduce GERVE (Gibbs-measure Entropy-Regularised Variational Estimation), a likelihood-free framework that approximates Gibbs measures directly from samples by maximizing an entropy-regularised variational objective with natural-gradient updates. GERVE brings together kernel density estimation, mean-shift, variational inference, and annealing in a single platform for mode estimation. It fits Gaussian mixtures that concentrate on high-density regions and yields cluster assignments from responsibilities, with reduced sensitivity to the chosen number of components. We provide theory in two regimes: as the Gibbs temperature approaches zero, mixture components converge to population modes; at fixed temperature, maximisers of the empirical objective exist, are consistent, and are asymptotically normal. We also propose a bootstrap procedure for per-mode confidence ellipses and stability scores. Simulation and real-data studies show accurate mode recovery and emergent clustering, robust to mixture overspecification. GERVE is a practical likelihood-free approach when the number of modes or groups is unknown and full density estimation is impractical.

</details>


### [4] [Sparse Bayesian Modeling of EEG Channel Interactions Improves P300 Brain-Computer Interface Performance](https://arxiv.org/abs/2602.17772)
*Guoxuan Ma,Yuan Zhong,Moyan Li,Yuxiao Nie,Jian Kang*

Main category: stat.ME

TL;DR: 提出稀疏贝叶斯时变回归框架，显式建模EEG通道间交互作用，实现P300 BCI的高精度解码和个性化优化


<details>
  <summary>Details</summary>
Motivation: 现有EEG-based P300 BCI方法通常将通道独立处理或依赖黑盒模型，限制了可解释性和个性化能力。需要一种能显式建模通道间复杂交互作用的方法来提高解码精度和系统实用性。

Method: 提出稀疏贝叶斯时变回归框架，采用松弛阈值高斯过程先验，在通道特定效应和交互效应中诱导结构化稀疏性，实现自动时间特征选择和任务相关通道/通道对的识别。

Result: 在55名参与者的公开P300拼写数据集上，该方法在使用所有刺激序列时达到中位字符级准确率100%，在统计和深度学习方法中表现最佳。建模通道交互使特定亚组字符级准确率提升达7%（戒酒参与者提升达18%），BCI-Utility中位提升约10%。

Conclusion: 在原则性贝叶斯框架内显式建模结构化EEG通道交互作用，能提高P300 BCI系统的预测准确性、用户中心吞吐量，并支持个性化优化。

Abstract: Electroencephalography (EEG)-based P300 brain-computer interfaces (BCIs) enable communication without physical movement by detecting stimulus-evoked neural responses. Accurate and efficient decoding remains challenging due to high dimensionality, temporal dependence, and complex interactions across EEG channels. Most existing approaches treat channels independently or rely on black-box machine learning models, limiting interpretability and personalization. We propose a sparse Bayesian time-varying regression framework that explicitly models pairwise EEG channel interactions while performing automatic temporal feature selection. The model employs a relaxed-thresholded Gaussian process prior to induce structured sparsity in both channel-specific and interaction effects, enabling interpretable identification of task-relevant channels and channel pairs. Applied to a publicly available P300 speller dataset of 55 participants, the proposed method achieves a median character-level accuracy of 100\% using all stimulus sequences and attains the highest overall decoding performance among competing statistical and deep learning approaches. Incorporating channel interactions yields subgroup-specific gains of up to 7\% in character-level accuracy, particularly among participants who abstained from alcohol (up to 18\% improvement). Importantly, the proposed method improves median BCI-Utility by approximately 10\% at its optimal operating point, achieving peak throughput after only seven stimulus sequences. These results demonstrate that explicitly modeling structured EEG channel interactions within a principled Bayesian framework enhances predictive accuracy, improves user-centric throughput, and supports personalization in P300 BCI systems.

</details>


### [5] [Preconditioned Robust Neural Posterior Estimation for Misspecified Simulators](https://arxiv.org/abs/2602.18004)
*Ryan P. Kelly,David T. Frazier,David J. Warne,Christopher C. Drovandi*

Main category: stat.ME

TL;DR: 提出预条件鲁棒神经后验估计方法，通过数据依赖权重聚焦训练于观测摘要附近，结合森林邻近预条件处理模型误设和极端先验预测问题


<details>
  <summary>Details</summary>
Motivation: 神经后验估计(NPE)在仿真推理中常因模型误设导致摘要不兼容，以及先验预测产生极端摘要而不可靠。现有预条件方法针对正确设定模型，在误设下的行为未探索

Method: 提出预条件鲁棒神经后验估计：1)计算数据依赖权重聚焦训练于观测摘要附近；2)拟合鲁棒神经后验近似；3)引入森林邻近预条件方法，使用树基邻近分数降低异常仿真权重，集中计算于观测数据集

Result: 在两个合成示例和一个真实示例中，预条件结合鲁棒NPE相比标准基线方法提高了稳定性，改善了准确性、校准和后验预测拟合度

Conclusion: 预条件与鲁棒NPE结合能有效处理模型误设和极端先验预测问题，提高仿真推理的可靠性和性能

Abstract: Simulation-based inference (SBI) enables parameter estimation for complex stochastic models with intractable likelihoods when model simulation is feasible. Neural posterior estimation (NPE) is a popular SBI approach that often achieves accurate inference with far fewer simulations than classical approaches. But in practice, neural approaches can be unreliable for two reasons: incompatible data summaries arising from model misspecification yield unreliable posteriors due to extrapolation, and prior-predictive draws can produce extreme summaries that lead to difficulties in obtaining an accurate posterior for the observed data of interest. Existing preconditioning schemes target well-specified settings, and their behaviour under misspecification remains unexplored. We study preconditioning under misspecification and propose preconditioned robust neural posterior estimation, which computes data-dependent weights that focus training near the observed summaries and fits a robust neural posterior approximation. We also introduce a forest-proximity preconditioning approach that uses tree-based proximity scores to down-weight outlying simulations and concentrate computation around the observed dataset. Across two synthetic examples and one real example with incompatible summaries and extreme prior-predictive behaviour, we demonstrate that preconditioning combined with robust NPE increases stability and improves accuracy, calibration, and posterior-predictive fit over standard baseline methods.

</details>


### [6] [Spatial Confounding: A review of concepts, challenges, and current approaches](https://arxiv.org/abs/2602.17792)
*Isaque Vieira Machado Pim,Luiz Max Fagundes de Carvalho,Marcos Oliveira Prates*

Main category: stat.ME

TL;DR: 本文综述了空间混杂问题的定义、经典模型和最新方法进展，为面数据和地统计数据提供了统一视角，并通过实证比较讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 空间混杂是空间统计学中持续存在的挑战，会影响空间结构化数据模型统计推断的有效性。本文旨在系统梳理这一问题的各种定义、经典模型和最新方法，为研究者提供清晰的理论框架。

Method: 采用文献综述方法，整合空间统计学和因果推断领域的方法，为面数据和地统计数据分析提供统一视角。通过理论分析和实证比较（包括真实数据集上的头对头比较）来评估各种方法的优劣。

Result: 论文系统梳理了空间混杂问题的多种定义和解决方法，通过实证比较展示了不同方法在实际数据集上的表现，为研究者选择合适方法提供了依据。

Conclusion: 空间混杂问题需要综合空间统计学和因果推断的方法来解决。基于实证比较结果，论文提出了未来研究方向，包括开发更稳健的模型和整合不同领域的方法。

Abstract: Spatial confounding is a persistent challenge in spatial statistics, influencing the validity of statistical inference in models that analyze spatially-structured data. The concept has been interpreted in various ways but is broadly defined as bias in estimates arising from unmeasured spatial variation. In this paper we review definitions, classical spatial models, and recent methodological advances, including approaches from spatial statistics and causal inference. We provide an unified view of the many available approaches for areal as well as geostatistical data and discuss their relative merits both theoretically and empirically with a head-to-head comparison on real datasets. Finally, we leverage the results of the empirical comparisons to discuss directions for future research.

</details>


### [7] [Semiparametric Uncertainty Quantification via Isotonized Posterior for Deconvolutions](https://arxiv.org/abs/2602.18210)
*Francesco Gili,Geurt Jongbloed*

Main category: stat.ME

TL;DR: 提出一种基于投影后验的非参数贝叶斯方法，用于去卷积模型的不确定性量化，无需估计讨厌参数，具有渐近正确的频率覆盖性。


<details>
  <summary>Details</summary>
Motivation: 现有频率方法产生的置信区间依赖于难以估计的讨厌参数（如X的密度及其导数），需要一种不依赖这些参数的不确定性量化方法。

Method: 采用非参数贝叶斯方法，直接在观测数据Z的分布上放置狄利克雷过程先验，通过求解Volterra积分方程得到信号分布F0，使用等渗逆后验确保估计为有效CDF。

Result: 该方法计算高效，产生的后验可信集经过简单重新校准后具有渐近正确的频率覆盖性，无需估计讨厌参数，在各种噪声分布下表现稳健。

Conclusion: 提出的投影后验贝叶斯框架为去卷积模型提供了有效的不确定性量化方法，克服了现有频率方法的局限性，具有实际应用价值。

Abstract: We address the problem of uncertainty quantification for the deconvolution model \(Z = X + Y\), where \(X\) and \(Y\) are nonnegative random variables and the goal is to estimate the signal's distribution of \(X \sim F_0\) supported on~\([0,\infty)\), from observations where the noise distribution is known. Existing frequentist methods often produce confidence intervals for $F_0(x)$ that depend on unknown nuisance parameters, such as the density of \(X\) and its derivative, which are difficult to estimate in practice. This paper introduces a novel and computationally efficient nonparametric Bayesian approach, based on projecting the posterior, to overcome this limitation. Our method leverages the solution \(p\) to a specific Volterra integral equation as in \cite{74}, which relates the cumulative distribution function (CDF) of the signal, \(F_0\), to the distribution of the observables. We place a Dirichlet Process prior directly on the distribution of the observed data $Z$, yielding a simple, conjugate posterior. To ensure the resulting estimates for \(F_0\) are valid CDFs, we isotonize posterior draws taking the Greatest Convex Majorant of the primitive of the posterior draws and defining what we term the Isotonic Inverse Posterior. We show that this framework yields posterior credible sets for \(F_0\) that are not only computationally fast to generate but also possess asymptotically correct frequentist coverage after a straightforward recalibration technique for the so-called Bayes Chernoff distribution introduced in \cite{54}. Our approach thus does not require the estimation of nuisance parameters to deliver uncertainty quantification for the parameter of interest $F_0(x)$. The practical effectiveness and robustness of the method are demonstrated through a simulation study with various noise distributions for $Y$.

</details>


### [8] [Hybrid Non-informative and Informative Prior Model-assisted Designs for Mid-trial Dose Insertion](https://arxiv.org/abs/2602.17995)
*Kana Yamada,Hisato Sunami,Kentaro Takeda,Keisuke Hanada,Masahiro Kojima*

Main category: stat.ME

TL;DR: 提出一种混合设计，在肿瘤I期试验中处理中途剂量插入问题，结合非信息性模型辅助设计和信息性先验扩展，并引入自适应骨架调整方法


<details>
  <summary>Details</summary>
Motivation: 在肿瘤I期试验中，模型辅助设计已被广泛采用，但在试验中途可能需要插入新剂量水平以评估更安全的剂量或收集更多疗效数据。现有方法在处理中途剂量插入时存在挑战，需要改进剂量分配和MTD/OBD选择策略。

Method: 提出混合设计：试验开始时使用非信息性模型辅助设计，剂量插入时仅对新添加的剂量应用信息性先验扩展。为应对骨架错误设定，提出两种自适应扩展：(1)在线加权方法随时间更新骨架，(2)贝叶斯混合方法稳健结合多个候选骨架。

Result: 通过模拟研究评估了所提方法，结果表明这些方法能有效改善中途剂量插入情况下的剂量分配和MTD/OBD选择。

Conclusion: 提出的混合设计和自适应扩展方法为肿瘤I期试验中的中途剂量插入问题提供了有效的解决方案，能提高剂量分配效率和MTD/OBD选择的准确性。

Abstract: In oncology phase I trials, model-assisted designs have been increasingly adopted because they enable adaptive yet operationally simple dose adjustment based on accumulating safety data, leading to a paradigm shift in dose-escalation methodology. In practice, a single mid-trial dose insertion may be considered to examine safer doses and/or to collect more informative efficacy data. In this study, we investigate methods to improve dose assignment and the selection of the maximum tolerated dose (MTD) or the optimal biological dose (OBD) when a new dose level is added during an ongoing trial under a model-assisted framework, by assigning informative prior information to the inserted dose. We propose a hybrid design that uses a non-informative model-assisted design at trial initiation and, upon dose insertion, applies an informative-prior extension only to the newly added dose. In addition, to address potential skeleton misspecification, we propose two adaptive extensions: (i) an online-weighting approach that updates the skeleton over time, and (ii) a Bayesian-mixture approach that robustly combines multiple candidate skeletons. We evaluate the proposed methods through simulation studies.

</details>


### [9] [Design-based inference for generalized causal effects in randomized experiments](https://arxiv.org/abs/2602.18383)
*Xinyuan Chen,Fan Li*

Main category: stat.ME

TL;DR: 提出基于U统计量和有限总体渐近的统一设计推断框架，用于广义因果效应估计量的回归调整和方差估计，证明回归估计量的一致性和渐近正态性，并开发了完全双向聚类稳健方差估计量。


<details>
  <summary>Details</summary>
Motivation: 传统平均处理效应估计量在处理非高斯或多变量结果时存在局限，需要发展更灵活的广义因果效应估计量（如Mann-Whitney参数和因果净效益）的推断方法。

Method: 基于U统计量理论和有限总体渐近，开发统一设计推断框架，使用成对对比函数定义广义因果效应估计量，构建基于个体对和单位对平均的回归估计量，并证明其模型辅助性质。

Result: 回归估计量即使在工作模型误设时仍保持一致性和渐近正态性；对于非线性对比函数，协变量调整保持一致性但无普遍效率保证；标准稳健方差估计量不一致，而完全双向聚类稳健方差估计量一致。

Conclusion: 为广义因果效应估计量提供了统一的模型辅助推断框架，揭示了协变量调整的效率特性，并开发了有效的方差估计方法，扩展了随机实验中的因果推断工具。

Abstract: Generalized causal effect estimands, including the Mann-Whitney parameter and causal net benefit, provide flexible summaries of treatment effects in randomized experiments with non-Gaussian or multivariate outcomes. We develop a unified design-based inference framework for regression adjustment and variance estimation of a broad class of generalized causal effect estimands defined through pairwise contrast functions. Leveraging the theory of U-statistics and finite-population asymptotics, we establish the consistency and asymptotic normality of regression estimators constructed from individual pairs and per-unit pair averages, even when the working models are misspecified. Consequently, these estimators are model-assisted rather than model-based. In contrast to classical average treatment effect estimands, we show that for nonlinear contrast functions, covariate adjustment preserves consistency but does not admit a universal efficiency guarantee. For inference, we demonstrate that standard heteroskedasticity-robust and cluster-robust variance estimators are generally inconsistent in this setting. As a remedy, we prove that a complete two-way cluster-robust variance estimator, which fully accounts for pairwise dependence and reverse comparisons, is consistent.

</details>


### [10] [Developing Performance-Guaranteed Biomarker Combination Rules with Integrated External Information under Practical Constraint](https://arxiv.org/abs/2602.17984)
*Albert Osom,Camden Lopez,Ashley Alexander,Suresh Chari,Ziding Feng,Ying-Qi Zhao*

Main category: stat.ME

TL;DR: 提出一个构建最优生物标志物临床决策规则的框架，通过最大化真阳性率并满足预设阳性预测值约束，同时可自适应整合外部风险信息来增强性能。


<details>
  <summary>Details</summary>
Motivation: 临床实践中需要将新型生物标志物与现有临床数据结合，构建可解释且稳健的决策规则，以改进早期疾病检测的决策制定。

Method: 构建线性决策规则，在满足预设阳性预测值约束的条件下最大化真阳性率，实现线性规则类中的最优性能。方法可自适应整合外部风险信息来提升性能。

Result: 建立了所提估计量的渐近性质，通过大量模拟研究表明方法具有强大的有限样本性能。应用于胰腺导管腺癌在新发糖尿病患者中的筛查规则开发。

Conclusion: 提出的框架能够构建既具有临床意义又实际可行的最优生物标志物临床决策规则，在早期疾病检测中提供改进的决策支持。

Abstract: In clinical practice, there is significant interest in integrating novel biomarkers with existing clinical data to construct interpretable and robust decision rules. Motivated by the need to improve decision-making for early disease detection, we propose a framework for developing an optimal biomarker-based clinical decision rule that is both clinically meaningful and practically feasible. Specifically, our procedure constructs a linear decision rule designed to achieve optimal performance among class of linear rules by maximizing the true positive rate while adhering to a pre-specified positive predictive value constraint. Additionally, our method can adaptively incorporate individual risk information from external source to enhance performance when such information is beneficial. We establish the asymptotic properties of our proposed estimator and compare to the standard approach used in practice through extensive simulation studies. Results indicate that our approach offers strong finite-sample performance. We also apply the proposed methods to develop biomarker-based screening rules for pancreatic ductal adenocarcinoma (PDAC) among new-onset diabetes (NOD) patients.

</details>


### [11] [Conformal Tradeoffs: Guarantees Beyond Coverage](https://arxiv.org/abs/2602.18045)
*Petrus H. Zwart*

Main category: stat.ME

TL;DR: 提出一个超越边际覆盖率的操作认证框架，包含小样本Beta校正、校准与审计两阶段设计、几何特征分析，为部署的共形预测器提供有限窗口操作保证


<details>
  <summary>Details</summary>
Motivation: 实际部署的共形预测器需要考虑操作层面的问题：系统何时承诺预测vs推迟预测？行动时产生多少错误暴露？这些操作率如何权衡？边际覆盖率无法确定这些部署相关的量

Method: 1. 小样本Beta校正(SSBC)：基于精确有限样本Beta/秩律反演，将用户请求映射到具有PAC风格语义的校准网格点；2. 校准与审计两阶段设计：使用独立审计集生成可重用区域和认证有限窗口包络；3. 几何特征分析：描述可行性约束、机制边界和成本一致性条件

Result: 在Tox21毒性预测(12个终点)和AquaSolDB水溶性筛选上演示了该方法，能够追踪不同校准设置下可达到的操作配置文件，并附加有限窗口不确定性包络

Conclusion: 提出了一个超越覆盖率的操作认证框架，为部署的共形预测器提供可审计的操作菜单，能够明确量化操作率之间的权衡关系，并给出有限窗口的统计保证

Abstract: Deployed conformal predictors are long-lived decision infrastructure operating over finite operational windows. The real-world question is not only ``Does the true label lie in the prediction set at the target rate?'' (marginal coverage), but ``How often does the system commit versus defer? What error exposure does it induce when it acts? How do these rates trade off?'' Marginal coverage does not determine these deployment-facing quantities: the same calibrated thresholds can yield different operational profiles depending on score geometry. We provide a framework for operational certification and planning beyond coverage with three contributions. (1) Small-Sample Beta Correction (SSBC): we invert the exact finite-sample Beta/rank law for split conformal to map a user request $(α^\star,δ)$ to a calibrated grid point with PAC-style semantics, yielding explicit finite-window coverage guarantees. (2) Calibrate-and-Audit: since no distribution-free pivot exists for rates beyond coverage, we introduce a two-stage design in which an independent audit set produces a reusable region -- label table and certified finite-window envelopes (Binomial/Beta-Binomial) for operational quantities -- commitment frequency, deferral, decisive error exposure, and commit purity -- via linear projection. (3) Geometric characterization: we describe feasibility constraints, regime boundaries (hedging vs.\ rejection), and cost-coherence conditions induced by a fixed conformal partition, explaining why operational rates are coupled and how calibration navigates their trade-offs. The output is an auditable operational menu: for a fixed scoring model, we trace attainable operational profiles across calibration settings and attach finite-window uncertainty envelopes. We demonstrate the approach on Tox21 toxicity prediction (12 endpoints) and aqueous solubility screening using AquaSolDB.

</details>


### [12] [Optimal inference via confidence distributions for two-by-two tables modelled as Poisson pairs: fixed and random effects](https://arxiv.org/abs/2602.18087)
*Céline Cunen,Nils Lid Hjort*

Main category: stat.ME

TL;DR: 本文提出了基于置信分布的2×2表格荟萃分析方法，适用于罕见事件场景，使用泊松对建模而非传统的二项分布，提供治疗效应参数的最优推断。


<details>
  <summary>Details</summary>
Motivation: 在医学研究中，2×2表格的荟萃分析很常见，但现有方法大多不适用于罕见事件场景。传统方法通常将表格建模为二项分布对，但在罕见事件情况下效果不佳。

Method: 采用泊松对建模2×2表格，基于置信分布方法，允许或不允许治疗效应的异质性。提出了治疗效应参数的最优推断方法，以及治疗效应比值的推断方法。

Result: 该方法提供了治疗效应参数的最优推断，能够有效处理罕见事件情况下的2×2表格荟萃分析，并在真实数据集上进行了验证。

Conclusion: 基于置信分布的泊松对建模方法为2×2表格荟萃分析提供了有效的解决方案，特别适用于罕见事件场景，提供了治疗效应参数和比值的最优推断。

Abstract: This paper presents methods for meta-analysis of $2 \times 2$ tables, both with and without allowing heterogeneity in the treatment effects. Meta-analysis is common in medical research, but most existing methods are unsuited for $2 \times 2$ tables with rare events. Usually the tables are modelled as pairs of binomial variables, but we will model them as Poisson pairs. The methods presented here are based on confidence distributions, and offer optimal inference for the treatment effect parameter. We also propose an optimal method for inference on the ratio between treatment effects, and illustrate our methods on a real dataset.

</details>


### [13] [Inclusive Ranking of Indian States via Bayesian Bradley-Terry Model](https://arxiv.org/abs/2602.18150)
*Arshi Rizvi,Rahul Singh*

Main category: stat.ME

TL;DR: 使用贝叶斯Bradley-Terry模型结合NFHS-5数据和人均收入先验信息对印度各邦进行综合排名


<details>
  <summary>Details</summary>
Motivation: 评估和排名不同行政区域的绩效对国家发展和政策制定至关重要。现有评估主要基于健康、教育、人均收入等指标，但需要更全面的排名方法，同时考虑人类发展因素和经济背景。

Method: 采用贝叶斯方法，通过著名的Bradley-Terry配对比较模型，结合NFHS-5数据集指标和各邦/UT的人均收入先验信息，进行综合排名。使用马尔可夫链蒙特卡洛方法进行参数估计和诊断。

Result: 该方法为印度各邦/UT提供了基于综合指标的排名结果，这些排名结果可用于制定明智的政策决策。同时进行了MCMC诊断以确保估计的可靠性。

Conclusion: 提出的贝叶斯Bradley-Terry模型能够综合考虑人类发展因素和经济背景，为印度各邦提供更全面的排名系统，有助于政策制定和发展规划。

Abstract: Evaluating the performance of different administrative regions within a country is crucial for its development and policy formulation. The performance evaluators are mostly based on health, education, per capita income, awareness, family planning and so on. Not only evaluating regions, but also ranking them is a crucial step, and various methods have been proposed to date. We aim to provide a ranking system for Indian states that uses a Bayesian approach via the famous Bradley-Terry model for paired comparisons. The ranking method uses indicators from the NFHS-5 dataset with the prior information of per-capita incomes of the states/UTs, thus leading to a holistic ranking, which not only includes human development factors but also take account the economic background of the states. We also carried out various Markov chain Monte Carlo diagnostics required for the reliability of the estimates of merits for these states. These merits thus provide a ranking for the states/UTs and can further be utilised to make informed policy decisions.

</details>


### [14] [Equal Marginal Power for Co-Primary Endpoints](https://arxiv.org/abs/2602.18161)
*Simon Bond*

Main category: stat.ME

TL;DR: 提出一种在随机试验中针对共同主要终点的样本量确定方法，确保各终点具有相等的边际功效，通过多重检验程序的灵活性实现。


<details>
  <summary>Details</summary>
Motivation: 当前指南在处理共同主要终点的随机试验时，可能导致各终点的边际功效不均衡，需要改进样本量确定方法以实现更公平的终点评估。

Method: 利用多重检验程序的灵活性，提出一种新的样本量确定方法，确保所有共同主要终点具有相等的边际功效，并与多种现有样本量确定规则进行比较。

Result: 该方法能够实现各终点边际功效的均衡，在试验设计和操作特性方面优于其他样本量确定规则。

Conclusion: 通过多重检验程序的灵活性，可以设计出更公平的共同主要终点试验，确保各终点具有相等的边际功效，提高试验设计的科学性和合理性。

Abstract: The choice of sample size in the context of co-primary endpoints for a randomised trial is discussed. Current guidance can leave endpoints with unequal marginal power. A method is provided to achieve equal marginal power by using the flexibility provided in multiple testing procedures. A comparison is made to several choices of rule to determine the sample size, in terms of the study design and its operating characteristics.

</details>


### [15] [Minimum L2 and robust Kullback-Leibler estimation](https://arxiv.org/abs/2602.18170)
*Nils Lid Hjort*

Main category: stat.ME

TL;DR: 提出两种新的稳健参数估计方法：最小加权L2法和稳健Kullback-Leibler法，用于参数族估计，具有稳健性和效率性


<details>
  <summary>Details</summary>
Motivation: 传统参数估计方法（如最大似然估计）对异常值敏感，需要开发稳健的估计方法，能够在保持参数模型效率的同时抵抗异常值影响

Method: 1. 最小加权L2法：最小化估计的加权积分平方误差；2. 稳健Kullback-Leibler法：最小化经验Kullback-Leibler距离的稳健版本，可视为最大似然估计的稳健改进

Result: 推导了两种方法的影响函数和渐近方差公式，计算了在基础参数模型条件下的渐近效率，并以正态模型为例进行了说明

Conclusion: 提出的两种稳健参数估计方法在保持效率的同时具有稳健性，稳健Kullback-Leibler法特别与半参数密度估计的局部似然思想相关

Abstract: This paper introduces two new robust methods for estimation of parameters in a given parametric family. The first method is that of `minimum weighted L2', effectively minimising an estimate of the integrated (and possibly weighted) squared error. The second is `robust Kullback-Leibler', consisting of minimising a robust version of the empirical Kullback-Leibler distance, and can be viewed as a general robust modification of the maximum likelihood procedure. This second method is also related to recent local likelihood ideas for semiparametric density estimation. The methods are described, influence functions are found, as are formulae for asymptotic variances. In particular large-sample efficiencies are computed under the home turf conditions of the underlying parametric model. The methods and formulae are illustrated for the normal model.

</details>


### [16] [Online FDR Controlling procedures for statistical SIS Model and its application to COVID19 data](https://arxiv.org/abs/2602.18241)
*Seohwa Hwang,Junyong Park*

Main category: stat.ME

TL;DR: 提出基于条件局部错误发现率的在线FDR控制方法，用于处理具有复杂依赖性的离散传染病数据，在保持FDR控制的同时提高检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有在线FDR控制方法通常假设独立性，在依赖数据中统计功效较低，无法有效处理传染病数据中的复杂依赖关系。

Method: 在SIS传染病模型框架下建立动态贝叶斯网络结构，使用条件局部错误发现率方法，仅需滑动窗口宽度参数，无需额外调参。

Result: 方法在平稳遍历依赖条件下保证FDR控制，相比现有方法获得更高的统计功效，在模拟和真实传染病数据分析中表现优异。

Conclusion: 提出的方法将在线假设检验扩展到依赖性和离散数据集，为实时疾病监测提供了实用且强大的统计工具。

Abstract: We propose an online false discovery rate (FDR) controlling method based on conditional local FDR (LIS), designed for infectious disease datasets that are discrete and exhibit complex dependencies. Unlike existing online FDR methods, which often assume independence or suffer from low statistical power in dependent settings, our approach effectively controls FDR while maintaining high detection power in realistic epidemic scenarios. For disease modeling, we establish a Dynamic Bayesian Network (DBN) structure within the Susceptible-Infected-Susceptible (SIS) model, a widely used epidemiological framework for infectious diseases. Our method requires no additional tuning parameters apart from the width of the sliding window, making it practical for real-time disease monitoring. From a statistical perspective, we prove that our method ensures valid FDR control under stationary and ergodic dependencies, extending online hypothesis testing to a broader range of dependent and discrete datasets. Additionally, our method achieves higher statistical power than existing approaches by leveraging LIS, which has been shown to be more powerful than traditional $p$-value-based methods. We validate our method through extensive simulations and real-world applications, including the analysis of infectious disease incidence data. Our results demonstrate that the proposed approach outperforms existing methods by achieving higher detection power while maintaining rigorous FDR control.

</details>


### [17] [Two-Stage Multiple Test Procedures Controlling False Discovery Rate with auxiliary variable and their Application to Set4Delta Mutant Data](https://arxiv.org/abs/2602.18271)
*Seohwa Hwang,Mark Louie Ramos,DoHwan Park,Junyong Park,Johan Lim,Erin Green*

Main category: stat.ME

TL;DR: 提出两种利用辅助变量进行多重假设检验的方法，有效控制FDR并提高统计功效


<details>
  <summary>Details</summary>
Motivation: 在多重假设检验中，仅使用主要变量可能效率不足，需要利用辅助变量信息来增强检验效能

Method: 提出两种方法：1) 当主要变量不满足预设条件时终止检验；2) 基于辅助变量调整评估标准。使用copula方法描述辅助变量与主要变量的依赖关系

Result: 数值研究表明，相比现有方法，新方法能有效控制FDR并获得更高的统计功效。在Set4Δ突变体数据集上的应用验证了方法的优势

Conclusion: 引入辅助变量进行多重假设检验的方法优于传统仅依赖主要变量的方法，能选择更多基因并提高检验效率

Abstract: In this paper, we present novel methodologies that incorporate auxiliary variables for multiple hypotheses testing related to the main point of interest while effectively controlling the false discovery rate. When dealing with multiple tests concerning the primary variable of interest, researchers can use auxiliary variables to set preconditions for the significance of primary variables, thereby enhancing test efficacy. Depending on the auxiliary variable's role, we propose two approaches: one terminates testing of the primary variable if it does not meet predefined conditions, and the other adjusts the evaluation criteria based on the auxiliary variable. Employing the copula method, we elucidate the dependence between the auxiliary and primary variables by deriving their joint distribution from individual marginal distributions.Our numerical studies, compared with existing methods, demonstrate that the proposed methodologies effectively control the FDR and yield greater statistical power than previous approaches solely based on the primary variable. As an illustrative example, we apply our methods to the Set4$Δ$ mutant dataset. Our findings highlight the distinctions between our methodologies and traditional approaches, emphasising the potential advantages of our methods in introducing the auxiliary variable for selecting more genes.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [18] [A context-specific causal model for estimating the effect of extended length of overnight stay on traveller's total expenditure](https://arxiv.org/abs/2602.18039)
*Lauri Valkonen,Juha Karvanen*

Main category: stat.AP

TL;DR: 该研究使用贝叶斯因果推断方法，分析旅行停留时间对支出的因果效应，特别区分个人和工作旅行目的


<details>
  <summary>Details</summary>
Motivation: 旅游业对国家经济有重要影响，理解停留时间与旅行者支出之间的因果关系对利益相关者制定营销策略和刻画消费特征至关重要。个人旅行和工作旅行的决策机制不同，需要分别分析。

Method: 采用特定情境独立性关系建模不同旅行目的（个人/工作）的因果机制，使用芬兰国际游客调查数据，拟合分层贝叶斯模型估计停留时间延长一晚的反事实支出分布，并进行贝叶斯敏感性分析评估遗漏变量偏误。

Result: 通过贝叶斯模型估计了停留时间延长一晚对支出的因果效应后验分布，并分析了不同旅行目的下的效应差异，同时评估了遗漏变量对因果效应估计的敏感性。

Conclusion: 研究提供了分析停留时间对旅行支出因果效应的统计框架，区分不同旅行目的有助于更准确地理解消费行为，为旅游营销策略制定提供数据支持。

Abstract: Tourism significantly affects the economies of many countries. Understanding the causal relationship between the length of overnight stay and traveller's expenditure is crucial for stakeholders to characterize spending profiles and to design marketing strategies. Causal mechanisms differ between personal and work-related travel because the decision-making processes have different drivers and constraints. We apply context-specific independence relations to model causal mechanisms in contexts specified by trip purpose and identify the causal effect of the length of stay on expenditure. Using the international visitor survey data on foreign travellers to Finland, we fit a hierarchical Bayesian model to estimate the posterior distribution of the counterfactual expenditure due to extending the length of stay by one night. We also perform a Bayesian sensitivity analysis of the estimated causal effect with respect to omitted variable bias.

</details>


### [19] [Forecasting the Evolving Composition of Inbound Tourism Demand: A Bayesian Compositional Time Series Approach Using Platform Booking Data](https://arxiv.org/abs/2602.18358)
*Harrison Katz*

Main category: stat.AP

TL;DR: 开发贝叶斯狄利克雷自回归移动平均(BDARMA)模型预测Airbnb客源市场份额构成动态，在四个主要旅游目的地验证其优于传统预测方法


<details>
  <summary>Details</summary>
Motivation: 理解客源市场构成随时间演变对目的地营销组织、酒店企业和旅游规划者至关重要，需要能够准确预测市场份额构成的模型

Method: 开发BDARMA模型，直接在单纯形上建模构成，使用狄利克雷似然函数，在均值和精度参数中纳入季节变化，利用2017-2024年Airbnb预订数据进行验证

Result: BDARMA在所有目的地区域实现最低平均预测误差，优于朴素预测、指数平滑和SARIMA等方法；对EMEA目的地，预测误差比朴素方法降低23%，改进具有统计显著性

Conclusion: BDARMA框架能够产生尊重单位总和约束的连贯预测，捕捉复杂时间依赖关系，为目的地利益相关者提供客源市场份额的概率预测，支持更明智的战略规划

Abstract: Understanding how the composition of guest origin markets evolves over time is critical for destination marketing organizations, hospitality businesses, and tourism planners. We develop and apply Bayesian Dirichlet autoregressive moving average (BDARMA) models to forecast the compositional dynamics of guest origin market shares using proprietary Airbnb booking data spanning 2017--2024 across four major destination regions. Our analysis reveals substantial pandemic-induced structural breaks in origin composition, with heterogeneous recovery patterns across markets. The BDARMA framework achieves the lowest average forecast error across all destination regions, outperforming standard benchmarks including naïve forecasts, exponential smoothing, and SARIMA on log-ratio transformed data. For EMEA destinations, BDARMA achieves 23% lower forecast error than naive methods, with statistically significant improvements. By modeling compositions directly on the simplex with a Dirichlet likelihood and incorporating seasonal variation in both mean and precision parameters, our approach produces coherent forecasts that respect the unit-sum constraint while capturing complex temporal dependencies. The methodology provides destination stakeholders with probabilistic forecasts of source market shares, enabling more informed strategic planning for marketing resource allocation, infrastructure investment, and crisis response.

</details>


### [20] [Hidden multistate models to study multimorbidity trajectories](https://arxiv.org/abs/2602.18369)
*Valentina Manzoni,Francesca Ieva,Amaia Calderón-Larrañaga,Davide Liborio Vetrano,Caterina Gregorio*

Main category: stat.AP

TL;DR: 开发连续时间隐马尔可夫多状态模型分析老年人共病轨迹，相比传统方法能更好处理不规则观测间隔和右删失，在瑞典SNAC-K队列数据中验证了实用性。


<details>
  <summary>Details</summary>
Motivation: 老年人共病现象普遍、异质且动态变化，与残疾和医疗资源使用密切相关。现有研究多采用描述性或离散时间模型，难以处理不规则观测间隔和右删失问题。

Method: 开发连续时间隐马尔可夫多状态建模框架，捕捉潜在共病模式间的转移，同时考虑区间删失和误分类。通过模拟研究比较不同模型规格，选择最佳规格应用于瑞典SNAC-K队列数据（2716名共病患者，随访长达18年）。

Result: 模拟研究显示隐马尔可夫多状态模型相比非隐马尔可夫模型显著减少转移风险估计的偏差，完全时间非齐次模型优于分段近似模型。SNAC-K数据分析证实了该框架的可行性，能识别加速进展为复杂共病的风险因素，并揭示不同模式间的死亡风险梯度。

Conclusion: 连续时间隐马尔可夫多状态模型为传统方法提供了稳健替代方案，支持个体化预测，为老年共病群体的靶向干预和二级预防策略提供信息。

Abstract: Multimorbidity in older adults is common, heterogeneous, and highly dynamic, and it is strongly associated with disability and increased healthcare utilization. However, existing approaches to studying multimorbidity trajectories are largely descriptive or rely on discrete-time models, which struggle to handle irregular observation intervals and right-censoring. We developed a continuous-time hidden multistate modeling framework to capture transitions among latent multimorbidity patterns while accounting for interval censoring and misclassification. A simulation study compared alternative model specifications under varying sample sizes and follow-up schemes, and the best-performing specification was applied to longitudinal data from the Swedish National study on Aging and Care-Kungsholmen (SNAC-K), including 2,716 multimorbid participants followed for up to 18 years. Simulation results showed that hidden multistate models substantially reduced bias in transition hazard estimates compared to non-hidden models, with fully time-inhomogeneous models outperforming piecewise approximations. Application to SNAC-K confirmed the feasibility and practical utility of this framework, enabling identification of risk factors for accelerated progression toward complex multimorbidity and revealing a gradient of mortality risk across patterns. Continuous-time hidden multistate models provide a robust alternative to traditional approaches, supporting individualized predictions and informing targeted interventions and secondary prevention strategies for multimorbidity in aging populations.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [21] [Data-driven configuration tuning of glmnet to balance accuracy and computation time](https://arxiv.org/abs/2602.17922)
*Shuhei Muroya,Kei Hirose*

Main category: stat.CO

TL;DR: 提出glmnetconf框架，通过神经网络预测不同配置下的精度和计算时间，自动寻找精度与效率的最优平衡点


<details>
  <summary>Details</summary>
Motivation: glmnet作为广泛使用的lasso估计R包，其默认配置有时会导致解与真实解差异较大。虽然可以通过调整配置提高精度，但这会增加计算时间，需要在精度和计算效率之间进行权衡。目前缺乏系统化的配置优化方法。

Method: 提出统一的数据驱动框架：1) 生成大规模模拟数据集；2) 在不同配置下运行glmnet获取精度和计算时间；3) 构建神经网络模型，从数据特征和配置预测精度和计算时间；4) 对新数据集，使用神经网络探索配置空间，生成精度与计算成本的帕累托前沿；5) 在用户指定的时间约束下自动选择最优配置。

Result: 开发了R包'glmnetconf'，实现了该框架。该框架能够自动识别在给定时间约束下最大化精度的配置，为glmnet用户提供了系统化的配置优化方法。

Conclusion: 提出的glmnetconf框架解决了glmnet配置优化问题，通过数据驱动方法平衡精度与计算效率，为用户提供了自动化的配置选择工具，提高了glmnet的实际应用效果。

Abstract: glmnet is a widely adopted R package for lasso estimation due to its computational efficiency. Despite its popularity, glmnet sometimes yields solutions that are substantially different from the true ones because of the inappropriate default configuration of the algorithm. The accuracy of the obtained solutions can be improved by appropriately tuning the configuration. However, improving accuracy typically increases computational time, resulting in a trade-off between accuracy and computational efficiency. Therefore, it is essential to establish a systematic approach to determine appropriate configuration. To address this need, we propose a unified data-driven framework specifically designed to optimize the configuration by balancing the trade-off between accuracy and computational efficiency. We generate large-scale simulated datasets and apply glmnet under various configurations to obtain accuracy and computation time. Based on these results, we construct neural networks that predict accuracy and computation time from data characteristics and configuration. Given a new dataset, our framework uses the neural networks to explore the configuration space and derive a Pareto front that represents the trade-off between accuracy and computational cost. This front allows us to automatically identify the configuration that maximize accuracy under a user-specified time constraint. The proposed method is implemented in the R package 'glmnetconf', available at https://github.com/Shuhei-Muroya/glmnetconf.

</details>


### [22] [Smoothness and other hyperparameter estimation for inverse problems related to data assimilation](https://arxiv.org/abs/2602.18328)
*Baptiste Simandoux,Nikolas Kantas,Dan Crisan*

Main category: stat.CO

TL;DR: 论文提出分层贝叶斯框架，联合推断时空场和先验平滑度等超参数，解决贝叶斯反问题中的平滑度误设问题，在Navier-Stokes和随机对流扩散方程上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在数据同化的贝叶斯反问题中，先验平滑度超参数对保证适定性、塑造后验结构和确定预测不确定性至关重要。传统方法通常假设该参数已知固定，但实际中平滑度误设会导致不确定性量化和参数估计误差。

Method: 采用分层贝叶斯框架，将平滑度和其他超参数视为未知并赋予超先验。使用适用于高维度的Metropolis-within-Gibbs采样进行后验推断，超参数估计计算开销小。在Navier-Stokes方程和随机对流扩散方程的反问题中验证，使用不同协方差结构的高斯先验，考虑稀疏和密集观测机制。

Result: 数值结果表明，联合估计平滑度能显著减少由平滑度误设引起的不确定性量化和参数估计误差，达到与已知真实平滑度情况相当的性能。

Conclusion: 分层贝叶斯框架能有效处理贝叶斯反问题中的平滑度不确定性，通过联合估计超参数改善不确定性量化和参数估计精度，方法计算高效且适用于高维问题。

Abstract: We consider Bayesian inverse problems arising in data assimilation for dynamical systems governed by partial and stochastic partial differential equations. The space-time dependent field is inferred jointly with static parameters of the prior and likelihood densities. Particular emphasis is placed on the hyperparameter controlling the prior smoothness and regularity, which is critical in ensuring well-posedness, shaping posterior structure, and determining predictive uncertainty. Commonly it is assumed to be known and fixed a priori; however in this paper we will adopt a hierarchical Bayesian framework in which smoothness and other hyperparameters are treated as unknown and assigned hyperpriors. Posterior inference is performed using Metropolis-within-Gibbs sampling suitable to high dimensions, for which hyperparameter estimation involves little computational overhead. The methodology is demonstrated on inverse problems for the Navier-Stokes equations and the stochastic advection-diffusion equation, under sparse and dense observation regimes, using Gaussian priors with different covariance structure. Numerical results show that jointly estimating the smoothness substantially reduces the errors in uncertainty quantification and parameter estimation induced by smoothness misspecification, by achieving performance comparable to scenarios in which the true smoothness is known.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [23] [Interactive Learning of Single-Index Models via Stochastic Gradient Descent](https://arxiv.org/abs/2602.17876)
*Nived Rajaraman,Yanjun Han*

Main category: stat.ML

TL;DR: SGD在单指数模型的序列学习问题中表现出色，通过适当的步长调度，能够同时实现接近最优的样本复杂度和遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 虽然SGD在高维非线性模型中的特征学习已有深入理论理解，但在序列学习问题（如广义线性赌博机）中，SGD的学习动态仍未被充分探索。研究SGD在这种自适应数据设置下的表现具有重要意义。

Method: 研究SGD在单指数模型序列学习问题中的表现，分析其学习动态，特别是"预热"和"学习"两个阶段，并设计适当的步长调度策略。

Result: SGD在序列学习问题中表现出与最优交互学习器相似的学习动态，经历明显的"预热"阶段后进入"学习"阶段。通过适当的步长调度，单个SGD过程能够同时实现接近最优的样本复杂度和遗憾保证。

Conclusion: SGD在自适应数据设置下学习单指数模型时仍然具有高度竞争力，能够有效处理序列学习问题，为实际应用提供了理论支持。

Abstract: Stochastic gradient descent (SGD) is a cornerstone algorithm for high-dimensional optimization, renowned for its empirical successes. Recent theoretical advances have provided a deep understanding of how SGD enables feature learning in high-dimensional nonlinear models, most notably the \textit{single-index model} with i.i.d. data. In this work, we study the sequential learning problem for single-index models, also known as generalized linear bandits or ridge bandits, where SGD is a simple and natural solution, yet its learning dynamics remain largely unexplored. We show that, similar to the optimal interactive learner, SGD undergoes a distinct ``burn-in'' phase before entering the ``learning'' phase in this setting. Moreover, with an appropriately chosen learning rate schedule, a single SGD procedure simultaneously achieves near-optimal (or best-known) sample complexity and regret guarantees across both phases, for a broad class of link functions. Our results demonstrate that SGD remains highly competitive for learning single-index models under adaptive data.

</details>


### [24] [Learning from Biased and Costly Data Sources: Minimax-optimal Data Collection under a Budget](https://arxiv.org/abs/2602.17894)
*Michael O. Harding,Vikas Singh,Kirthevasan Kandasamy*

Main category: stat.ML

TL;DR: 该论文研究多源数据收集问题，在固定预算下优化人口均值和组条件均值的估计，提出最大化有效样本量的采样策略和分层后估计方法，达到预算最小最大最优风险。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据收集常需从多个异质源获取，不同源采样成本不同，且组别分布（如健康指标、人口统计、政治派别）在源与目标群体间差异显著。现有简单策略（如匹配目标分布）和标准估计器（如样本均值）往往效果不佳。

Method: 开发最大化有效样本量的采样计划：总样本量除以(D_χ²(q||p̄)+1)，其中q为目标分布，p̄为聚合源分布，D_χ²为χ²散度。结合经典的分层后估计器，并给出风险上界。

Result: 证明了该方法达到预算最小最大最优风险，提供了匹配的下界。技术还可扩展到预测问题中最小化超额风险，为多源学习提供原则性方法。

Conclusion: 提出的采样策略和估计方法在多源数据收集中优于传统方法，在固定预算下实现了最优统计效率，为处理成本高且异质的数据源提供了理论保证。

Abstract: Data collection is a critical component of modern statistical and machine learning pipelines, particularly when data must be gathered from multiple heterogeneous sources to study a target population of interest. In many use cases, such as medical studies or political polling, different sources incur different sampling costs. Observations often have associated group identities (for example, health markers, demographics, or political affiliations) and the relative composition of these groups may differ substantially, both among the source populations and between sources and target population.
  In this work, we study multi-source data collection under a fixed budget, focusing on the estimation of population means and group-conditional means. We show that naive data collection strategies (e.g. attempting to "match" the target distribution) or relying on standard estimators (e.g. sample mean) can be highly suboptimal. Instead, we develop a sampling plan which maximizes the effective sample size: the total sample size divided by $D_{χ^2}(q\mid\mid\overline{p}) + 1$, where $q$ is the target distribution, $\overline{p}$ is the aggregated source distribution, and $D_{χ^2}$ is the $χ^2$-divergence. We pair this sampling plan with a classical post-stratification estimator and upper bound its risk. We provide matching lower bounds, establishing that our approach achieves the budgeted minimax optimal risk. Our techniques also extend to prediction problems when minimizing the excess risk, providing a principled approach to multi-source learning with costly and heterogeneous data sources.

</details>


### [25] [Topological Exploration of High-Dimensional Empirical Risk Landscapes: general approach, and applications to phase retrieval](https://arxiv.org/abs/2602.17779)
*Antoine Maillard,Tony Bonnaire,Giulio Biroli*

Main category: stat.ML

TL;DR: 该论文使用Kac-Rice公式分析高维高斯单指标模型经验风险最小化的临界点景观，推导出简化的变分公式，并应用于实相位恢复问题，揭示了损失景观的拓扑相图和BBP型转变。


<details>
  <summary>Details</summary>
Motivation: 研究高维统计模型中经验风险最小化的损失景观拓扑结构，特别是在比例渐近机制下（n∝d），理解临界点的分布和性质，这对于优化算法的行为和统计推断至关重要。

Method: 使用Kac-Rice公式分析不同临界点的复杂度（期望数量），将复杂的变分公式简化为有限标量参数的显式变分问题，并应用于实相位恢复问题，通过数值求解验证理论预测。

Result: 成功简化了临界点复杂度的变分公式，得到了可数值求解的显式形式；为实相位恢复问题建立了完整的拓扑相图，揭示了局部最小值处Hessian矩阵在信号方向上的BBP型不稳定性转变；理论预测与有限尺寸模拟的梯度流动力学高度一致。

Conclusion: 该研究为高维统计模型的损失景观渐近分析和拓扑平凡化现象开辟了新途径，提供了分析临界点性质、Hessian谱特性和标签联合分布的理论框架，对理解优化算法行为和统计推断有重要意义。

Abstract: We consider the landscape of empirical risk minimization for high-dimensional Gaussian single-index models (generalized linear models). The objective is to recover an unknown signal $\boldsymbolθ^\star \in \mathbb{R}^d$ (where $d \gg 1$) from a loss function $\hat{R}(\boldsymbolθ)$ that depends on pairs of labels $(\mathbf{x}_i \cdot \boldsymbolθ, \mathbf{x}_i \cdot \boldsymbolθ^\star)_{i=1}^n$, with $\mathbf{x}_i \sim \mathcal{N}(0, I_d)$, in the proportional asymptotic regime $n \asymp d$. Using the Kac-Rice formula, we analyze different complexities of the landscape -- defined as the expected number of critical points -- corresponding to various types of critical points, including local minima. We first show that some variational formulas previously established in the literature for these complexities can be drastically simplified, reducing to explicit variational problems over a finite number of scalar parameters that we can efficiently solve numerically. Our framework also provides detailed predictions for properties of the critical points, including the spectral properties of the Hessian and the joint distribution of labels. We apply our analysis to the real phase retrieval problem for which we derive complete topological phase diagrams of the loss landscape, characterizing notably BBP-type transitions where the Hessian at local minima (as predicted by the Kac-Rice formula) becomes unstable in the direction of the signal. We test the predictive power of our analysis to characterize gradient flow dynamics, finding excellent agreement with finite-size simulations of local optimization algorithms, and capturing fine-grained details such as the empirical distribution of labels. Overall, our results open new avenues for the asymptotic study of loss landscapes and topological trivialization phenomena in high-dimensional statistical models.

</details>


### [26] [Drift Estimation for Stochastic Differential Equations with Denoising Diffusion Models](https://arxiv.org/abs/2602.17830)
*Marcos Tapia Costa,Nikolas Kantas,George Deligiannidis*

Main category: stat.ML

TL;DR: 提出基于条件扩散模型的漂移函数估计方法，用于高维随机微分方程，在低维与传统方法相当，高维表现更优


<details>
  <summary>Details</summary>
Motivation: 研究多元随机微分方程中时间齐次漂移函数的估计问题，已知扩散系数，从高频观测的多条轨迹中估计漂移函数

Method: 将漂移估计构建为基于先前观测的去噪问题，通过训练能够动态模拟新轨迹的条件扩散模型，其副产品即为漂移函数估计器

Result: 在不同漂移函数类别中，该方法在低维情况下与传统方法相当，在高维情况下保持竞争力，且优势不能仅归因于架构设计选择

Conclusion: 基于条件扩散模型的漂移估计方法在高维随机微分方程中具有竞争力，为漂移函数估计提供了新思路

Abstract: We study the estimation of time-homogeneous drift functions in multivariate stochastic differential equations with known diffusion coefficient, from multiple trajectories observed at high frequency over a fixed time horizon. We formulate drift estimation as a denoising problem conditional on previous observations, and propose an estimator of the drift function which is a by-product of training a conditional diffusion model capable of simulating new trajectories dynamically. Across different drift classes, the proposed estimator was found to match classical methods in low dimensions and remained consistently competitive in higher dimensions, with gains that cannot be attributed to architectural design choices alone.

</details>


### [27] [On the Generalization and Robustness in Conditional Value-at-Risk](https://arxiv.org/abs/2602.18053)
*Dinesh Karthik Mulumudi,Piyushi Manupriya,Gholamali Aminian,Anant Raj*

Main category: stat.ML

TL;DR: 本文系统分析了在重尾和污染数据下CVaR经验风险最小化的统计性质，建立了最优泛化界和鲁棒性保证，同时揭示了CVaR决策在重尾下的内在不稳定性。


<details>
  <summary>Details</summary>
Motivation: CVaR是处理罕见但高影响损失的重要风险敏感目标，但在重尾数据下的统计行为尚未被充分理解。CVaR依赖于内生的数据依赖分位数，这改变了泛化和鲁棒性特性，需要系统分析。

Method: 开发了CVaR经验风险最小化的学习理论分析框架，建立了高概率泛化和超额风险界；推导了统一的Bahadur-Kiefer型展开来分离阈值驱动误差；提出了截断中位数均值CVaR估计器处理对抗污染。

Result: 在最小矩假设下建立了尖锐的泛化界和超额风险界，证明这些速率是极小极大最优的；提出的估计器在对抗污染下达到最优速率；揭示了CVaR决策在重尾下的内在不稳定性。

Conclusion: 研究系统刻画了CVaR学习何时能够泛化且鲁棒，以及何时由于尾部稀缺性导致不稳定性不可避免，为CVaR在重尾和污染数据下的应用提供了理论基础。

Abstract: Conditional Value-at-Risk (CVaR) is a widely used risk-sensitive objective for learning under rare but high-impact losses, yet its statistical behavior under heavy-tailed data remains poorly understood. Unlike expectation-based risk, CVaR depends on an endogenous, data-dependent quantile, which couples tail averaging with threshold estimation and fundamentally alters both generalization and robustness properties. In this work, we develop a learning-theoretic analysis of CVaR-based empirical risk minimization under heavy-tailed and contaminated data. We establish sharp, high-probability generalization and excess risk bounds under minimal moment assumptions, covering fixed hypotheses, finite and infinite classes, and extending to $β$-mixing dependent data; we further show that these rates are minimax optimal. To capture the intrinsic quantile sensitivity of CVaR, we derive a uniform Bahadur-Kiefer type expansion that isolates a threshold-driven error term absent in mean-risk ERM and essential in heavy-tailed regimes. We complement these results with robustness guarantees by proposing a truncated median-of-means CVaR estimator that achieves optimal rates under adversarial contamination. Finally, we show that CVaR decisions themselves can be intrinsically unstable under heavy tails, establishing a fundamental limitation on decision robustness even when the population optimum is well separated. Together, our results provide a principled characterization of when CVaR learning generalizes and is robust, and when instability is unavoidable due to tail scarcity.

</details>


### [28] [Box Thirding: Anytime Best Arm Identification under Insufficient Sampling](https://arxiv.org/abs/2602.18186)
*Seohwa Hwang,Junyong Park*

Main category: stat.ML

TL;DR: B3算法是一种用于固定预算下最佳臂识别的灵活高效方法，通过三臂比较迭代筛选，无需预先知道总预算T，在有限预算下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决固定预算约束下的最佳臂识别问题，特别是在臂数N很大时无法在有限预算T内进行穷举评估的情况，以及需要无需预先知道T的随时BAI算法。

Method: 采用迭代的三臂比较策略：每轮比较三个臂，最佳臂进一步探索，中位臂推迟到后续比较，最弱臂被丢弃。算法无需预先知道总预算T，适用于大规模臂集。

Result: B3在有限预算约束下的简单遗憾方面优于现有方法，在New Yorker Cartoon Caption Contest数据集上验证了其有效性，即使不知道T也能达到与需要T作为预定义参数的Successive Halving相当的ε-最佳臂误识别概率。

Conclusion: B3是一种灵活高效的固定预算最佳臂识别算法，特别适合大规模臂集和无需预先知道预算的场景，在实际应用中表现出优越性能。

Abstract: We introduce Box Thirding (B3), a flexible and efficient algorithm for Best Arm Identification (BAI) under fixed-budget constraints. It is designed for both anytime BAI and scenarios with large N, where the number of arms is too large for exhaustive evaluation within a limited budget T. The algorithm employs an iterative ternary comparison: in each iteration, three arms are compared--the best-performing arm is explored further, the median is deferred for future comparisons, and the weakest is discarded. Even without prior knowledge of T, B3 achieves an epsilon-best arm misidentification probability comparable to Successive Halving (SH), which requires T as a predefined parameter, applied to a randomly selected subset of c0 arms that fit within the budget. Empirical results show that B3 outperforms existing methods under limited-budget constraints in terms of simple regret, as demonstrated on the New Yorker Cartoon Caption Contest dataset.

</details>
