<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.ME](#stat.ME) [Total: 7]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Clustering country-level all-cause mortality data: a review](https://arxiv.org/abs/2512.04831)
*Pedro Menezes de Araujo,Isobel Claire Gormley,Thomas Brendan Murphy*

Main category: stat.AP

TL;DR: 本文综述了聚类方法在国家层面全因死亡率数据分析中的应用，总结了研究动机、数据来源、常用方法及主要发现，并指出了现有研究的局限性。


<details>
  <summary>Details</summary>
Motivation: 死亡率数据对人口学、公共卫生和精算科学都很重要。虽然聚类方法越来越多地用于探索此类数据的模式，但尚无研究系统回顾其在国家层面全因死亡率分析中的应用。因此，本综述旨在总结近期研究，回答关键问题：为什么使用聚类、分析哪些死亡率数据、哪些方法最常用、以及主要发现是什么。

Method: 通过系统回顾将聚类应用于国家层面全因死亡率的研究，重点关注死亡率指标、数据来源和方法选择，并使用人类死亡率数据库（HMD）数据复制部分方法。

Result: 分析显示：聚类主要用于预测以及研究收敛性和不平等性；大多数研究使用发达国家的人类死亡率数据库数据；主要依赖k-means、层次聚类或函数型聚类方法；主要发现包括东欧与西欧之间的持续分化模式，且聚类方法通常比单一国家模型能提高预测准确性。

Conclusion: 本综述展示了文献中的方法多样性，总结了聚类结果，并指出了研究空白，如聚类质量评估有限、对高收入国家以外数据利用不足等问题。

Abstract: Mortality data are relevant to demography, public health, and actuarial science. Whilst clustering is increasingly used to explore patterns in such data, no study has reviewed its application to country-level all-cause mortality. This review therefore summarises recent work and addresses key questions: why clustering is used, which mortality data are analysed, which methods are most common, and what main findings emerge. To address these questions, we examine studies applying clustering to country-level all-cause mortality, focusing on mortality indices, data sources, and methodological choices, and we replicate some approaches using Human Mortality Database (HMD) data. Our analysis reveals that clustering is mainly motivated by forecasting and by studying convergence and inequality. Most studies use HMD data from developed countries and rely on k-means, hierarchical, or functional clustering. Main findings include a persistent East-West European division across applications, with clustering generally improving forecast accuracy over single-country models. Overall, this review highlights the methodological range in the literature, summarises clustering results, and identifies gaps, such as the limited evaluation of clustering quality and the underuse of data from countries outside the high-income world.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [2] [A Benchmark Study of Classical and Dual Polynomial Regression (DPR)-Based Probability Density Estimation Technique](https://arxiv.org/abs/2512.04235)
*Shantanu Sarkar,Mousumi Sinha,Dexter Cahoy*

Main category: stat.CO

TL;DR: 提出Dual Polynomial Regression (DPR)方法，通过分段多项式回归更好地拟合单峰分布的非对称形状，结合GPU加速的KDE和直方图估计，在准确性和计算效率之间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现实数据常偏离高斯假设，表现出偏态和指数衰减特性。现有密度估计方法（如局部多项式回归）计算量大，需要开发既能捕捉分布不规则性又计算高效的方法。

Method: 提出双多项式回归(DPR)方法：1) 在分布众数处分割PDF；2) 对左右半部分分别拟合多项式回归；3) 使用GPU加速的KDE(tKDE)或直方图估计(tHDE)进行训练；4) 实现TensorFlow版本的GPU加速算法。

Result: 在6个单峰数据集上验证，使用MSE、JSD和Pearson相关系数评估。4阶DPR结合tKDE训练在30万患者血压数据上表现最佳，在准确性和计算开销之间达到最优平衡，且GPU加速版本优于Python SciPy的KDE。

Conclusion: DPR方法通过分段建模策略有效捕捉单峰分布的非对称形状，结合GPU加速技术显著提升计算效率，为现实世界非高斯数据的密度估计提供了准确且高效的计算框架。

Abstract: The probability density function (PDF) plays a central role in statistical and machine learning modeling. Real-world data often deviates from Gaussian assumptions, exhibiting skewness and exponential decay. To evaluate how well different density estimation methods capture such irregularities, we generated six unimodal datasets from diverse distributions that reflect real-world anomalies. These were compared using parametric methods (Pearson Type I and Normal distribution) as well as non-parametric approaches, including histograms, kernel density estimation (KDE), and our proposed method. To accelerate computation, we implemented GPU-based versions of KDE (tKDE) and histogram estimation (tHDE) in TensorFlow, both of which outperform Python SciPy's KDE. Prior work demonstrated the use of piecewise modeling for density estimation, such as local polynomial regression; however, these methods are computationally intensive. Based on the concept of piecewise modeling, we developed a computationally efficient model, the Dual Polynomial Regression (DPR) method, which leverages tKDE or tHDE for training. DPR employs the piecewise strategy to split the PDF at its mode and fit polynomial regressions to the left and right halves independently, enabling better capture of the asymmetric shape of the unimodal distribution. We used the Mean Squared Error (MSE), Jensen-Shannon Divergence (JSD), and Pearson's correlation coefficient, with reference to the baseline PDF, to validate accuracy. We verified normalization using Area Under the Curve (AUC) and computational overhead via execution time. Validation on real-world systolic and diastolic data from 300,000 unique patients shows that the DPR of order 4, trained with tKDE, offers the best balance between accuracy and computational overhead.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [3] [Informative missingness and its implications in semi-supervised learning](https://arxiv.org/abs/2512.04392)
*Jinran Wu,You-Gan Wang,Geoffrey J. McLachlan*

Main category: stat.ML

TL;DR: 半监督学习通过建模信息性缺失标签机制，在某些情况下可以获得比完全标注样本更好的分类器性能


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习假设标签缺失是随机的，但实际中标签缺失机制可能依赖于观测特征或类别标签，这种信息性缺失本身包含有用信息，建模这种机制可以提升分类性能

Method: 在有限混合模型的似然框架内，使用期望最大化(EM)算法拟合模型，同时建模标签缺失机制，将缺失标签视为不完全数据问题

Result: 当类别重叠适中、标注数据稀疏且缺失具有信息性时，建模信息性缺失机制的分类器比基于完全标注样本的分类器具有更小的期望误差

Conclusion: 建模信息性标签缺失机制为半监督学习提供了统一的统计框架，将基于似然的推断与经验SSL方法的行为统一起来，在某些条件下可以超越完全标注样本的性能

Abstract: Semi-supervised learning (SSL) constructs classifiers using both labelled and unlabelled data. It leverages information from labelled samples, whose acquisition is often costly or labour-intensive, together with unlabelled data to enhance prediction performance. This defines an incomplete-data problem, which statistically can be formulated within the likelihood framework for finite mixture models that can be fitted using the expectation-maximisation (EM) algorithm. Ideally, one would prefer a completely labelled sample, as one would anticipate that a labelled observation provides more information than an unlabelled one. However, when the mechanism governing label absence depends on the observed features or the class labels or both, the missingness indicators themselves contain useful information. In certain situations, the information gained from modelling the missing-label mechanism can even outweigh the loss due to missing labels, yielding a classifier with a smaller expected error than one based on a completely labelled sample analysed. This improvement arises particularly when class overlap is moderate, labelled data are sparse, and the missingness is informative. Modelling such informative missingness thus offers a coherent statistical framework that unifies likelihood-based inference with the behaviour of empirical SSL methods.

</details>


### [4] [Provable FDR Control for Deep Feature Selection: Deep MLPs and Beyond](https://arxiv.org/abs/2512.04696)
*Kazuma Sawaya*

Main category: stat.ML

TL;DR: 提出基于深度神经网络的灵活特征选择框架，可近似控制假发现率(FDR)，适用于多种网络架构，是首个在深度学习环境下提供FDR控制理论保证的工作。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习特征选择方法缺乏对假发现率的理论控制保证，特别是在复杂的深度神经网络架构中。需要开发一个既能利用深度学习强大表示能力，又能提供统计可靠性保证的特征选择框架。

Method: 基于梯度特征重要性向量的深度神经网络特征选择框架。适用于第一层全连接、后续层可包含MLP、卷积网络、循环网络、注意力机制、残差连接和dropout等多种架构。使用随机梯度下降训练，支持数据无关的初始化和学习率。

Result: 在多指标数据生成模型和渐近框架下，证明了梯度特征重要性向量的每个坐标都服从边际正态分布，从而支持渐近FDR控制的有效性。数值实验验证了理论发现。

Conclusion: 该研究首次在深度学习环境下提供了特征选择FDR控制的理论保证，为复杂神经网络架构中的可靠特征选择提供了理论基础，尽管存在设计矩阵正交不变性的理论限制。

Abstract: We develop a flexible feature selection framework based on deep neural networks that approximately controls the false discovery rate (FDR), a measure of Type-I error. The method applies to architectures whose first layer is fully connected. From the second layer onward, it accommodates multilayer perceptrons (MLPs) of arbitrary width and depth, convolutional and recurrent networks, attention mechanisms, residual connections, and dropout. The procedure also accommodates stochastic gradient descent with data-independent initializations and learning rates. To the best of our knowledge, this is the first work to provide a theoretical guarantee of FDR control for feature selection within such a general deep learning setting.
  Our analysis is built upon a multi-index data-generating model and an asymptotic regime in which the feature dimension $n$ diverges faster than the latent dimension $q^{*}$, while the sample size, the number of training iterations, the network depth, and hidden layer widths are left unrestricted. Under this setting, we show that each coordinate of the gradient-based feature-importance vector admits a marginal normal approximation, thereby supporting the validity of asymptotic FDR control. As a theoretical limitation, we assume $\mathbf{B}$-right orthogonal invariance of the design matrix, and we discuss broader generalizations. We also present numerical experiments that underscore the theoretical findings.

</details>


### [5] [Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting](https://arxiv.org/abs/2512.04690)
*Souhir Ben Amor,Florian Ziel*

Main category: stat.ML

TL;DR: 提出一种结合线性结构和循环神经网络的混合模型，用于日前电价预测，在欧洲电力市场数据上比现有方法准确率提高约12%。


<details>
  <summary>Details</summary>
Motivation: 改进电力系统的短期决策和运营管理，需要更准确的电价预测模型。现有方法在捕捉电力市场价格特征方面存在局限，需要结合线性和非线性结构的优势。

Method: 设计新型循环神经网络架构，将专家模型和卡尔曼滤波器等线性结构嵌入循环网络中，形成混合预测模型。该模型能够捕捉日历效应、自回归效应、负荷、可再生能源以及燃料和碳市场的影响。

Result: 使用2018-2025年欧洲最大电力市场的每小时数据进行实证测试，与最先进的高维线性和神经网络模型相比，提出的模型准确率提高了约12%。

Conclusion: 结合线性和非线性结构的混合模型在电价预测中表现优异，可解释性组件有助于理解模型贡献，这种架构设计对电力市场预测有重要影响。

Abstract: We present a novel recurrent neural network architecture designed explicitly for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylised price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. The proposed model achieves approximately 12% higher accuracy than leading benchmarks. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures.

</details>


### [6] [Towards a unified framework for guided diffusion models](https://arxiv.org/abs/2512.04985)
*Yuchen Jiao,Yuxin Chen,Gen Li*

Main category: stat.ML

TL;DR: 本文提出了一个统一的算法和理论框架，用于分析引导扩散采样器，包括扩散引导和奖励引导扩散，并量化了引导带来的奖励改进。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型理论取得了重大进展，但对引导扩散采样器的理论理解仍然非常有限。本文旨在填补这一空白，为引导扩散提供统一的理论框架。

Method: 提出了一个统一的算法和理论框架，通过注入奖励引导项（由原始分数和奖励重加权分数之差构建）到反向扩散过程中，来微调扩散模型以提高特定奖励。

Result: 框架表明无分类器引导（CFG）降低了分类器概率的期望倒数，首次从理论上描述了CFG改进的具体性能指标。应用于奖励引导扩散时，产生了一个易于训练且训练期间不需要完整扩散轨迹的新采样器。

Conclusion: 本文为引导扩散采样器提供了首个统一的理论框架，量化了引导带来的奖励改进，并通过数值实验验证了理论发现。

Abstract: Guided or controlled data generation with diffusion models\blfootnote{Partial preliminary results of this work appeared in International Conference on Machine Learning 2025 \citep{li2025provable}.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion. Aimed at fine-tuning diffusion models to improve certain rewards, we propose injecting a reward guidance term -- constructed from the difference between the original and reward-reweighted scores -- into the backward diffusion process, and rigorously quantify the resulting reward improvement over the unguided counterpart. As a key application, our framework shows that classifier-free guidance (CFG) decreases the expected reciprocal of the classifier probability, providing the first theoretical characterization of the specific performance metric that CFG improves for general target distributions. When applied to reward-guided diffusion, our framework yields a new sampler that is easy-to-train and requires no full diffusion trajectories during training. Numerical experiments further corroborate our theoretical findings.

</details>


### [7] [Learning Causality for Longitudinal Data](https://arxiv.org/abs/2512.04980)
*Mouad EL Bouchattaoui*

Main category: stat.ML

TL;DR: 该论文提出三种方法：CDVAE用于个体治疗效果估计，RNN-CPC用于长期反事实回归，以及基于解码器雅可比几何的因果表示学习可解释性层。


<details>
  <summary>Details</summary>
Motivation: 解决高维时变数据中的因果推断和因果表示学习问题，特别是在存在未观测异质性、时间变化混杂因素以及潜在因果结构不明确的情况下。

Method: 1. CDVAE：因果动态变分自编码器，通过潜在风险因子捕获治疗响应异质性；2. RNN-CPC：结合对比预测编码和信息最大化的循环神经网络框架；3. 基于解码器雅可比几何的模型无关可解释性层，使用稀疏自表达先验。

Result: CDVAE在合成和真实数据集上优于基线模型；RNN-CPC在长期反事实回归中达到最先进性能；可解释性层能在无锚特征或单亲假设下恢复有意义的潜在到观测结构。

Conclusion: 该论文为高维时变数据中的因果推断提供了有效的理论保证方法，并在因果表示学习的可解释性方面取得进展，为实际应用提供了实用工具。

Abstract: This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.
  The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables.
  The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference.
  The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.

</details>


### [8] [Control Consistency Losses for Diffusion Bridges](https://arxiv.org/abs/2512.05070)
*Samuel Howard,Nikolas Nüsken,Jakiw Pidstrigach*

Main category: stat.ML

TL;DR: 提出一种迭代在线学习方法，利用条件动力学的自洽性来学习扩散桥，特别适用于罕见事件的模拟。


<details>
  <summary>Details</summary>
Motivation: 模拟扩散过程在给定初始和终止状态下的条件动力学是一个重要但具有挑战性的问题，特别是在罕见事件场景中，无条件动力学很少能达到终止状态。

Method: 利用条件动力学的自洽性，采用迭代在线学习方法来学习扩散桥。

Result: 在多种设置下展示了有希望的实证结果。

Conclusion: 提出的方法能够有效处理扩散桥学习问题，特别是在罕见事件模拟中表现出良好性能。

Abstract: Simulating the conditioned dynamics of diffusion processes, given their initial and terminal states, is an important but challenging problem in the sciences. The difficulty is particularly pronounced for rare events, for which the unconditioned dynamics rarely reach the terminal state. In this work, we leverage a self-consistency property of the conditioned dynamics to learn the diffusion bridge in an iterative online manner, and demonstrate promising empirical results in a range of settings.

</details>


### [9] [Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction](https://arxiv.org/abs/2512.05092)
*Vincent Pauline,Tobias Höppe,Kirill Neklyudov,Alexander Tong,Stefan Bauer,Andrea Dittadi*

Main category: stat.ML

TL;DR: 本文提供了扩散模型在一般状态空间上的统一理论框架，涵盖连续和离散数据，通过变分方法推导ELBO，为不同背景的读者提供分层介绍。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型介绍通常局限于欧几里得数据，缺乏对连续和离散状态空间的统一处理。本文旨在弥合这一差距，为不同背景的研究者提供全面的理论框架。

Method: 采用统一框架处理一般状态空间，同时考虑离散时间（马尔可夫核）和连续时间（SDEs和CTMCs）视角，推导Fokker-Planck和主方程，通过变分方法得到ELBO训练损失。

Result: 建立了扩散模型在连续和离散状态空间的统一理论，明确了前向噪声过程选择（高斯过程、分类转移核等）如何影响反向动态和ELBO，提供了可重用的证明和核心原理。

Conclusion: 本文为现代扩散方法提供了跨连续域和离散序列的统一路线图，通过分层介绍满足不同读者需求，强调了可重用的理论原则和核心概念。

Abstract: Although diffusion models now occupy a central place in generative modeling, introductory treatments commonly assume Euclidean data and seldom clarify their connection to discrete-state analogues. This article is a self-contained primer on diffusion over general state spaces, unifying continuous domains and discrete/categorical structures under one lens. We develop the discrete-time view (forward noising via Markov kernels and learned reverse dynamics) alongside its continuous-time limits -- stochastic differential equations (SDEs) in $\mathbb{R}^d$ and continuous-time Markov chains (CTMCs) on finite alphabets -- and derive the associated Fokker--Planck and master equations. A common variational treatment yields the ELBO that underpins standard training losses. We make explicit how forward corruption choices -- Gaussian processes in continuous spaces and structured categorical transition kernels (uniform, masking/absorbing and more) in discrete spaces -- shape reverse dynamics and the ELBO. The presentation is layered for three audiences: newcomers seeking a self-contained intuitive introduction; diffusion practitioners wanting a global theoretical synthesis; and continuous-diffusion experts looking for an analogy-first path into discrete diffusion. The result is a unified roadmap to modern diffusion methodology across continuous domains and discrete sequences, highlighting a compact set of reusable proofs, identities, and core theoretical principles.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [10] [Reyes's I: Measuring Spatial Autocorrelation in Compositions](https://arxiv.org/abs/2512.04289)
*Lina Buitrago,Juan Sosa,Oscar Melo*

Main category: stat.ME

TL;DR: 提出了Reyes's I统计量，用于评估区域成分数据的空间自相关性，该统计量基于Aitchison内积和范数，具有尺度不变性、部分排列不变性和ilr对比矩阵选择不变性。


<details>
  <summary>Details</summary>
Motivation: 成分数据（测量值作为整体的一部分）在空间分析中很常见，但缺乏直接类似Moran's I的统计量来评估区域成分数据的空间自相关性。

Method: 提出了Reyes's I统计量，基于Aitchison几何定义，推导了随机化假设下的上界、期望值和非中心二阶矩，并描述了精确和蒙特卡洛置换推断方法。

Result: 通过模拟显示Reyes's I在多种协方差结构和邻域定义下具有稳定行为、竞争性校准和相对于基于分量平均的朴素方法的改进效率。应用于哥伦比亚部门COVID-19严重程度成分数据，发现月初存在显著正自相关，随时间减弱。

Conclusion: Reyes's I为成分数据的空间自相关分析提供了有效的统计工具，填补了现有方法的空白，并在理论和实际应用中表现出良好性能。

Abstract: Compositional observations arise when measurements are recorded as parts of a whole, so that only relative information is meaningful and the natural sample space is the simplex equipped with Aitchison geometry. Despite extensive development of compositional methods, a direct analogue of Moran's \(I\) for assessing spatial autocorrelation in areal compositional data has been lacking. We propose Reyes's \(I\), a Moran type statistic defined through the Aitchison inner product and norm, which is invariant to scale, to permutations of the parts, and to the choice of the \(\operatorname{ilr}\) contrast matrix. Under the randomization assumption, we derive an upper bound, the expected value, and the noncentral second moment, and we describe exact and Monte Carlo permutation procedures for inference. Through simulations covering identical, independent, and spatially correlated compositions under multiple covariance structures and neighborhood definitions, we show that Reyes's \(I\) provides stable behavior, competitive calibration, and improved efficiency relative to a naive alternative based on averaging componentwise Moran statistics. We illustrate practical utility by studying the spatial dependence of a composition measuring COVID-19 severity across Colombian departments during January 2021, documenting significant positive autocorrelation early in the month that attenuates over time.

</details>


### [11] [Sequential Randomization Tests Using E-values: A Betting Approach for Clinical Trials](https://arxiv.org/abs/2512.04366)
*Fernando G Zampieri*

Main category: stat.ME

TL;DR: 提出一种基于随机化机制的非参数序贯检验方法e-RT，通过赌博框架构建检验鞅，在零假设下保证任意停止时间的I类错误控制


<details>
  <summary>Details</summary>
Motivation: 传统的序贯监测依赖于参数假设或渐近近似，需要一种仅从随机化机制推导有效性的非参数方法

Method: 使用赌博框架，通过顺序对给定观察结果的分配进行投注来构建检验鞅，称为随机化e过程(e-RT)

Result: 证明了方法的有效性，模拟研究展示了校准和功效，e-RT提供了对基于模型序贯分析的保守、无假设补充

Conclusion: e-RT为随机化试验提供了一种非参数序贯检验方法，仅从随机化机制获得有效性，保证任意停止时间的I类错误控制

Abstract: Sequential monitoring of randomized trials traditionally relies on parametric assumptions or asymptotic approximations. We present a nonparametric sequential test, the randomization e-process (e-RT), that derives validity solely from the randomization mechanism. Using a betting framework, e-RT constructs a test martingale by sequentially wagering on treatment assignments given observed outcomes. Under the null hypothesis of no treatment effect, the expected wealth cannot grow, guaranteeing anytime-valid Type I error control regardless of stopping rule. We prove validity and present simulation studies demonstrating calibration and power. The e-RT provides a conservative, assumption-free complement to model-based sequential analyses.

</details>


### [12] [Bayesian Graphical High-Dimensional Time Series Models for Detecting Structural Changes](https://arxiv.org/abs/2512.04444)
*Shuvrarghya Ghosh,Arkaprava Roy,Anindya Roy,Subhashis Ghosal*

Main category: stat.ME

TL;DR: 提出spOUTAR模型，用于联合建模两个相关的多元时间序列，通过贝叶斯方法估计其平稳精度矩阵，以检测和量化经济危机前后变量间条件依赖关系的变化。


<details>
  <summary>Details</summary>
Motivation: 研究经济危机（如大衰退）如何改变宏观经济变量间的条件依赖结构。虽然已有研究记录了经济后果，但对危机前后变量间条件依赖关系如何变化了解甚少。

Method: 基于OUT（正交旋转单变量时间序列）框架，提出spOUTAR（共享参数OUT自回归）模型，联合建模两个相关的多元时间序列，实现其平稳精度矩阵的相干贝叶斯估计。

Result: 应用于美国和OECD宏观经济数据，证明spOUTAR能有效捕捉衰退引起的平稳图结构变化，为研究经济系统结构转变提供灵活可解释的工具。

Conclusion: spOUTAR框架为检测和量化经济危机前后变量间条件关系变化提供了原则性机制，有助于理解经济系统结构转变。

Abstract: We study the structural changes in multivariate time-series by estimating and comparing stationary graphs for macroeconomic time series before and after an economic crisis such as the Great Recession. Building on a latent time series framework called Orthogonally-rotated Univariate Time-series (OUT), we propose a shared-parameter framework-the spOUT autoregressive model (spOUTAR)-that jointly models two related multivariate time series and enables coherent Bayesian estimation of their corresponding stationary precision matrices. This framework provides a principled mechanism to detect and quantify which conditional relationships among the variables changed, or formed following the crisis. Specifically, we study the impact of the Great Recession (December 2007-June 2009) that substantially disrupted global and national economies, prompting long-lasting shifts in macroeconomic indicators and their interrelationships. While many studies document its economic consequences, far less is known about how the underlying conditional dependency structure among economic variables changed as economies moved from pre-crisis stability through the shock and back to normalcy. Using the proposed approach to analyze U.S. and OECD macroeconomic data, we demonstrate that spOUTAR effectively captures recession-induced changes in stationary graphical structure, offering a flexible and interpretable tool for studying structural shifts in economic systems.

</details>


### [13] [Learning Heterogeneous Ordinal Graphical Models via Bayesian Nonparametric Clustering](https://arxiv.org/abs/2512.04407)
*Wang Wen,Ziqi Chen,Guanyu Hu*

Main category: stat.ME

TL;DR: 提出一种基于混合有限混合的非参数贝叶斯框架，用于分析体育数据中的序数变量，能自动发现异质子群并估计群组特定的图模型结构。


<details>
  <summary>Details</summary>
Motivation: 图模型在分析序数数据（如体育中的球队排名、球员评分）方面研究不足，现有方法难以有效处理异质子群（如不同风格的球队）和选择子群数量。

Method: 使用混合有限混合的非参数贝叶斯框架，结合probit图模型，为每个子群建模，同时估计聚类数量和配置，开发了高效的Gibbs采样算法进行推断。

Result: 该方法能灵活发现子群并估计群组特定结构，特别适用于体育分析，能揭示球员表现指标中的潜在模式。

Conclusion: 填补了序数数据建模的关键空白，为体育表现和策略的高级决策提供了基础。

Abstract: Graphical models are powerful tools for capturing conditional dependence structures in complex systems but remain underexplored in analyzing ordinal data, especially in sports analytics. Ordinal variables, such as team rankings, player performance ratings, and survey responses, are pervasive in sports data but present unique challenges, particularly when accounting for heterogeneous subgroups, such as teams with varying styles or players with distinct roles. Existing methods, including probit graphical models, struggle with modeling heterogeneity and selecting the number of subgroups effectively. We propose a novel nonparametric Bayesian framework using the Mixture of Finite Mixtures (MFM) approach to address these challenges. Our method allows for flexible subgroup discovery and models each subgroup with a probit graphical model, simultaneously estimating the number of clusters and their configurations. We develop an efficient Gibbs sampling algorithm for inference, enabling robust estimation of cluster-specific structures and parameters. This framework is particularly suited to sports analytics, uncovering latent patterns in player performance metrics. Our work bridges critical gaps in modeling ordinal data and provides a foundation for advanced decision-making in sports performance and strategy.

</details>


### [14] [Multi-source Learning for Target Population by High-dimensional Calibration](https://arxiv.org/abs/2512.04412)
*Haoxiang Zhan,Jae Kwang Kim,Yumou Qiu*

Main category: stat.ME

TL;DR: 提出高维去偏校准（HDC）方法和多源HDC（MHDC）估计器，用于多源学习中的参数估计，通过高维协变量平衡实现Neyman正交性，避免逆概率加权，支持灵活的密度比和结果回归模型，具有多重稳健性。


<details>
  <summary>Details</summary>
Motivation: 在多源学习场景中，需要结合具有异质分布的多个数据集来估计目标人群的参数，但目标人群没有观测到的响应。现有方法存在计算复杂性和模型误设敏感性的问题，需要开发更稳健高效的方法。

Method: 提出HDC方法通过高维协变量平衡在增强的协变量集上实现Neyman正交性，避免逆概率加权公式。MHDC估计器整合多源数据，支持密度比和结果回归模型的灵活设定，通过联合利用所有数据源提高效率。

Result: 建立了MHDC估计器的渐近正态性，提出了检验多源数据可转移性的规范检验。模拟研究表明MHDC能有效处理多源和多工作模型，性能优于现有的双重稳健估计器。气象数据集实证分析验证了方法的实用性。

Conclusion: 提出的HDC和MHDC方法为多源学习提供了有效的解决方案，通过高维协变量平衡实现稳健估计，避免计算复杂性，支持灵活模型设定，并在理论和实证上都表现出优越性能。

Abstract: Multi-source learning is an emerging area of research in statistics, where information from multiple datasets with heterogeneous distributions is combined to estimate the parameter of interest for a target population without observed responses. We propose a high-dimensional debiased calibration (HDC) method and a multi-source HDC (MHDC) estimator for general estimating equations. The HDC method uses a novel approach to achieve Neyman orthogonality for the target parameter via high-dimensional covariate balancing on an augmented set of covariates. It avoids the augmented inverse probability weighting formulation and leads to an easier optimization algorithm for the target parameter in estimating equations and M-estimation. The proposed MHDC estimator integrates multi-source data while supporting flexible specifications for both density ratio and outcome regression models, achieving multiple robustness against model misspecification. Its asymptotic normality is established, and a specification test is proposed to examine the transferability condition for the multi-source data. Compared to the linear combination of single-source HDC estimators, the MHDC estimator improves efficiency by jointly utilizing all data sources. Through simulation studies, we show that the MHDC estimator accommodates multiple sources and multiple working models effectively and performs better than the existing doubly robust estimators for multi-source learning. An empirical analysis of a meteorological dataset demonstrates the utility of the proposed method in practice.

</details>


### [15] [Tensor Neyman-Pearson Classification: Theory, Algorithms, and Error Control](https://arxiv.org/abs/2512.04583)
*Lingchong Liu,Elynn Chen,Yuefeng Han,Lucy Xia*

Main category: stat.ME

TL;DR: 提出首个张量Neyman-Pearson分类框架，在分子张量数据上实现有限样本的I类错误控制，适用于生化发现中错误代价不对称的场景。


<details>
  <summary>Details</summary>
Motivation: 生化发现中分子结构分类存在严重的不对称错误代价：将有害化合物误判为良性会带来重大科学、监管和健康风险，而误报主要增加实验室工作量。现有张量分类器缺乏有限样本的I类错误保证，实践中常出现严重错误膨胀。

Method: 开发Tensor-NP分类框架：1) 在张量正态混合模型下推导oracle NP判别式，刻画其Tucker低秩流形几何；2) 建立张量特定的边界和条件检测条件；3) 提出判别张量迭代投影估计器和Tensor-NP神经网络分类器，结合深度学习与Tensor-NP校准。

Result: 在四个生化数据集上，Tensor-NP分类器能将I类错误维持在预设水平，同时保持竞争力的II类错误性能，为复杂分子张量的不对称风险决策提供可靠工具。

Conclusion: Tensor-NP框架首次实现了多模态张量数据的有限样本I类错误控制，填补了现有方法在不对称风险场景下的理论空白，为生化发现中的可靠分类提供了新工具。

Abstract: Biochemical discovery increasingly relies on classifying molecular structures when the consequences of different errors are highly asymmetric. In mutagenicity and carcinogenicity, misclassifying a harmful compound as benign can trigger substantial scientific, regulatory, and health risks, whereas false alarms primarily increase laboratory workload. Modern representations transform molecular graphs into persistence image tensors that preserve multiscale geometric and topological structure, yet existing tensor classifiers and deep tensor neural networks provide no finite-sample guarantees on type I error and often exhibit severe error inflation in practice.
  We develop the first Tensor Neyman-Pearson (Tensor-NP) classification framework that achieves finite-sample control of type I error while exploiting the multi-mode structure of tensor data. Under a tensor-normal mixture model, we derive the oracle NP discriminant, characterize its Tucker low-rank manifold geometry, and establish tensor-specific margin and conditional detection conditions enabling high-probability bounds on excess type II error. We further propose a Discriminant Tensor Iterative Projection estimator and a Tensor-NP Neural Classifier combining deep learning with Tensor-NP umbrella calibration, yielding the first distribution-free NP-valid methods for multiway data. Across four biochemical datasets, Tensor-NP classifiers maintain type I errors at prespecified levels while delivering competitive type II error performance, providing reliable tools for asymmetric-risk decisions with complex molecular tensors.

</details>


### [16] [Model-Free Assessment of Simulator Fidelity via Quantile Curves](https://arxiv.org/abs/2512.05024)
*Garud Iyengar,Yu-Shiou Willy Lin,Kaizheng Wang*

Main category: stat.ME

TL;DR: 提出一种计算可行的方法来估计模拟器与真实结果分布之间差异的分位数函数，适用于各种参数族，支持置信区间构建和风险感知的模拟器性能评估。


<details>
  <summary>Details</summary>
Motivation: 随着复杂、基于机器学习的系统日益增多，表征模拟器与真实情况之间的差异变得困难。现有方法难以处理复杂的ML系统，需要一种通用的方法来量化模拟器的不确定性。

Method: 提出计算可行的方法来估计模拟器与真实结果分布差异的分位数函数。该方法将模拟器视为黑盒，不对其内部做建模假设，适用于从伯努利、多项到连续向量值等多种参数族。

Result: 生成的分位数曲线支持：1）未见场景的置信区间构建；2）模拟-真实差异的风险感知总结（如VaR/CVaR）；3）模拟器性能比较。在WorldValueBench数据集上评估了四个LLM的模拟保真度。

Conclusion: 该方法为复杂ML系统的模拟器评估提供了通用的量化框架，能够处理各种参数族，支持风险感知的模拟器性能分析和比较。

Abstract: Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.

</details>
