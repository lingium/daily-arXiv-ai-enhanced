{"id": "2510.21630", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.21630", "abs": "https://arxiv.org/abs/2510.21630", "authors": ["Forough Mahpouya", "Sabrina Casucci", "Suzanne Sullivan", "Christopher Barrick"], "title": "Representing caregiver burden in observational studies: Development of the Caregiver Burden Index (CareBI) using NSOC", "comment": null, "summary": "Informal caregiving often carries a significant emotional, physical, and\nfinancial toll, yet caregiver burden is often underrepresented in healthcare\nresearch and methods. Existing caregiver burden instruments, while valuable in\nclinical research, often lack compatibility with observational datasets\nregularly used in health services research and planning. This study introduces\nthe Caregiver Burden Index (CareBI) developed for the National Study of\nCaregiving (NSOC), that can be used to represent caregiver burden in\nquantitative models and observational research studies. CareBI was developed\nand validated using a multistep process that included the identification and\npreparation of individual NSOC survey items, exploratory and confirmatory\nfactor analysis, score estimation, interpretation, and external validation. The\nstudy used data from round 12 of the NSOC. CareBI represents three domains of\nburden: objective, subjective, and interpersonal, providing a comprehensive\nview of both the positive and negative aspects of caregiving. It also aligns\nwith the Zarit Burden Interview, a widely used tool for prospectively assessing\ncaregiver burden. Construct validity was assessed by comparing CareBI's\nrelationship with caregiver and care recipient outcomes, as well as sensitivity\nto known burden-related risk and mitigation factors. Early findings affirm the\nscale's utility in categorizing low-, moderate-, and high-burden caregivers and\nguiding resource-oriented strategies. CareBI represents a reproducible tool for\nembedding caregiver metrics into health operations, predictive modeling, and\npublic policy frameworks, and provides a template for applying operations\nresearch and industrial engineering methods to psychosocial measurement\nchallenges in aging and long-term care."}
{"id": "2510.21116", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.21116", "abs": "https://arxiv.org/abs/2510.21116", "authors": ["Bolun Liu", "Trang Quynh Nguyen", "Elizabeth A. Stuart", "Bryan Lau", "Amii M. Kress", "Michael R. Elliott", "Kyle R. Busse", "Ellen C. Caniglia", "Yajnaseni Chakraborti", "Amy J. Elliott", "James E. Gern", "Alison E. Hipwell", "Catherine J. Karr", "Kaja Z. LeWinn", "Li Luo", "Hans-Georg Müller", "Sunni L. Mumford", "Ruby H. N. Nguyen", "Emily Oken", "Janet L. Peacock", "Enrique F. Schisterman", "Arjun Sondhi", "Rosalind J. Wright", "Yidong Zhou", "Elizabeth L. Ogburn"], "title": "Sensitivity Analysis when Generalizing Causal Effects from Multiple Studies to a Target Population: Motivation from the ECHO Program", "comment": null, "summary": "Unobserved effect modifiers can induce bias when generalizing causal effect\nestimates to target populations. In this work, we extend a sensitivity analysis\nframework assessing the robustness of study results to unobserved effect\nmodification that adapts to various generalizability scenarios, including\nmultiple (conditionally) randomized trials, observational studies, or\ncombinations thereof. This framework is interpretable and does not rely on\ndistributional or functional assumptions about unknown parameters. We\ndemonstrate how to leverage the multi-study setting to detect violation of the\ngeneralizability assumption through hypothesis testing, showing with\nsimulations that the proposed test achieves high power under real-world sample\nsizes. Finally, we apply our sensitivity analysis framework to analyze the\ngeneralized effect estimate of secondhand smoke exposure on birth weight using\ncohort sites from the Environmental influences on Child Health Outcomes (ECHO)\nstudy."}
{"id": "2510.21249", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21249", "abs": "https://arxiv.org/abs/2510.21249", "authors": ["Daniele Girolimetto", "Anastasios Panagiotelis", "Tommaso Di Fonzo", "Han Li"], "title": "Forecast reconciliation with non-linear constraints", "comment": null, "summary": "Methods for forecasting time series adhering to linear constraints have seen\nnotable development in recent years, especially with the advent of forecast\nreconciliation. This paper extends forecast reconciliation to the open question\nof non-linearly constrained time series. Non-linear constraints can emerge with\nvariables that are formed as ratios such as mortality rates and unemployment\nrates. On the methodological side, Non-linearly Constrained Reconciliation\n(NLCR) is proposed. This algorithm adjusts forecasts that fail to meet\nnon-linear constraints, in a way that ensures the new forecasts meet the\nconstraints. The NLCR method is a projection onto a non-linear surface,\nformulated as a constrained optimisation problem. On the theoretical side,\noptimisation methods are again used, this time to derive sufficient conditions\nfor when the NLCR methodology is guaranteed to improve forecast accuracy.\nFinally on the empirical side, NLCR is applied to two datasets from demography\nand economics and shown to significantly improve forecast accuracy relative to\nrelevant benchmarks."}
{"id": "2510.21047", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.21047", "abs": "https://arxiv.org/abs/2510.21047", "authors": ["Ziyang Liu", "Ning Hao", "Yue Selena Niu", "Han Xiao", "Hongxu Ding"], "title": "Autocorrelation Test under Frequent Mean Shifts", "comment": null, "summary": "Testing for the presence of autocorrelation is a fundamental problem in time\nseries analysis. Classical methods such as the Box-Pierce test rely on the\nassumption of stationarity, necessitating the removal of non-stationary\ncomponents such as trends or shifts in the mean prior to application. However,\nthis is not always practical, particularly when the mean structure is complex,\nsuch as being piecewise constant with frequent shifts. In this work, we propose\na new inferential framework for autocorrelation in time series data under\nfrequent mean shifts. In particular, we introduce a Shift-Immune Portmanteau\n(SIP) test that reliably tests for autocorrelation and is robust against mean\nshifts. We illustrate an application of our method to nanopore sequencing data."}
{"id": "2510.20871", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.20871", "abs": "https://arxiv.org/abs/2510.20871", "authors": ["Marta Gentiloni Silveri", "Giovanni Conforti", "Alain Durmus"], "title": "Exponential Convergence Guarantees for Iterative Markovian Fitting", "comment": null, "summary": "The Schr\\\"odinger Bridge (SB) problem has become a fundamental tool in\ncomputational optimal transport and generative modeling. To address this\nproblem, ideal methods such as Iterative Proportional Fitting and Iterative\nMarkovian Fitting (IMF) have been proposed-alongside practical approximations\nlike Diffusion Schr\\\"odinger Bridge and its Matching (DSBM) variant. While\nprevious work have established asymptotic convergence guarantees for IMF, a\nquantitative, non-asymptotic understanding remains unknown. In this paper, we\nprovide the first non-asymptotic exponential convergence guarantees for IMF\nunder mild structural assumptions on the reference measure and marginal\ndistributions, assuming a sufficiently large time horizon. Our results\nencompass two key regimes: one where the marginals are log-concave, and another\nwhere they are weakly log-concave. The analysis relies on new contraction\nresults for the Markovian projection operator and paves the way to theoretical\nguarantees for DSBM."}
{"id": "2510.20883", "categories": ["stat.ML", "cs.CR", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.20883", "abs": "https://arxiv.org/abs/2510.20883", "authors": ["Antônio H. Ribeiro", "David Vävinggren", "Dave Zachariah", "Thomas B. Schön", "Francis Bach"], "title": "Kernel Learning with Adversarial Features: Numerical Efficiency and Adaptive Regularization", "comment": "Accepted NeurIPS 2025", "summary": "Adversarial training has emerged as a key technique to enhance model\nrobustness against adversarial input perturbations. Many of the existing\nmethods rely on computationally expensive min-max problems that limit their\napplication in practice. We propose a novel formulation of adversarial training\nin reproducing kernel Hilbert spaces, shifting from input to feature-space\nperturbations. This reformulation enables the exact solution of inner\nmaximization and efficient optimization. It also provides a regularized\nestimator that naturally adapts to the noise level and the smoothness of the\nunderlying function. We establish conditions under which the feature-perturbed\nformulation is a relaxation of the original problem and propose an efficient\noptimization algorithm based on iterative kernel ridge regression. We provide\ngeneralization bounds that help to understand the properties of the method. We\nalso extend the formulation to multiple kernel learning. Empirical evaluation\nshows good performance in both clean and adversarial settings."}
{"id": "2510.20954", "categories": ["stat.ML", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.20954", "abs": "https://arxiv.org/abs/2510.20954", "authors": ["Roxanne Holden", "Luana Ruiz"], "title": "A Short Note on Upper Bounds for Graph Neural Operator Convergence Rate", "comment": null, "summary": "Graphons, as limits of graph sequences, provide a framework for analyzing the\nasymptotic behavior of graph neural operators. Spectral convergence of sampled\ngraphs to graphons yields operator-level convergence rates, enabling\ntransferability analyses of GNNs. This note summarizes known bounds under no\nassumptions, global Lipschitz continuity, and piecewise-Lipschitz continuity,\nhighlighting tradeoffs between assumptions and rates, and illustrating their\nempirical tightness on synthetic and real data."}
{"id": "2510.20947", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20947", "abs": "https://arxiv.org/abs/2510.20947", "authors": ["Florian D. van Leeuwen", "Sara van Erp"], "title": "To MCMC or not to MCMC: Evaluating non-MCMC methods for Bayesian penalized regression", "comment": null, "summary": "Markov Chain Monte Carlo (MCMC) sampling is computationally expensive,\nespecially for complex models. Alternative methods make simplifying assumptions\nabout the posterior to reduce computational burden, but their impact on\npredictive performance remains unclear. This paper compares MCMC and non-MCMC\nmethods for high-dimensional penalized regression, examining when computational\nshortcuts are justified for prediction tasks.\n  We conduct a comprehensive simulation study using high-dimensional tabular\ndata, then validate findings with empirical datasets featuring both continuous\nand binary outcomes. An in-depth analysis of one dataset provides a\nstep-by-step tutorial implementing various algorithms in R.\n  Our results show that mean-field variational inference consistently performs\ncomparably to MCMC methods. In simulations, mean-field VI exhibited 3-90\\%\nhigher MSE across scenarios while reducing runtime by 7-30x compared to\nHamiltonian Monte Carlo. Empirical datasets revealed dramatic speed-ups\n(100-400x) in some cases with similar or superior predictive performance.\nHowever, performance varied: some cases showed over 100x MSE increases with\nonly 30x speed-ups, highlighting the context-dependent nature of these\ntrade-offs."}
{"id": "2510.21091", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.21091", "abs": "https://arxiv.org/abs/2510.21091", "authors": ["Kyungseon Lee", "Kunwoong Kim", "Jihu Lee", "Dongyoon Yang", "Yongdai Kim"], "title": "Doubly-Regressing Approach for Subgroup Fairness", "comment": null, "summary": "Algorithmic fairness is a socially crucial topic in real-world applications\nof AI.\n  Among many notions of fairness, subgroup fairness is widely studied when\nmultiple sensitive attributes (e.g., gender, race, age) are present.\n  However, as the number of sensitive attributes grows, the number of subgroups\nincreases accordingly, creating heavy computational burdens and data sparsity\nproblem (subgroups with too small sizes).\n  In this paper, we develop a novel learning algorithm for subgroup fairness\nwhich resolves these issues by focusing on subgroups with sufficient sample\nsizes as well as marginal fairness (fairness for each sensitive attribute).\n  To this end, we formalize a notion of subgroup-subset fairness and introduce\na corresponding distributional fairness measure called the supremum Integral\nProbability Metric (supIPM).\n  Building on this formulation, we propose the Doubly Regressing Adversarial\nlearning for subgroup Fairness (DRAF) algorithm, which reduces a surrogate\nfairness gap for supIPM with much less computation than directly reducing\nsupIPM.\n  Theoretically, we prove that the proposed surrogate fairness gap is an upper\nbound of supIPM.\n  Empirically, we show that the DRAF algorithm outperforms baseline methods in\nbenchmark datasets, specifically when the number of sensitive attributes is\nlarge so that many subgroups are very small."}
{"id": "2510.20928", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20928", "abs": "https://arxiv.org/abs/2510.20928", "authors": ["Zhenghao Zeng", "David Arbour", "Avi Feller", "Ishita Dasgupta", "Atanu R Sinha", "Edward H. Kennedy"], "title": "Handling Missing Responses under Cluster Dependence with Applications to Language Model Evaluation", "comment": "27 pages, 3 figures", "summary": "Human annotations play a crucial role in evaluating the performance of GenAI\nmodels. Two common challenges in practice, however, are missing annotations\n(the response variable of interest) and cluster dependence among human-AI\ninteractions (e.g., questions asked by the same user may be highly correlated).\nReliable inference must address both these issues to achieve unbiased\nestimation and appropriately quantify uncertainty when estimating average\nscores from human annotations. In this paper, we analyze the doubly robust\nestimator, a widely used method in missing data analysis and causal inference,\napplied to this setting and establish novel theoretical properties under\ncluster dependence. We further illustrate our findings through simulations and\na real-world conversation quality dataset. Our theoretical and empirical\nresults underscore the importance of incorporating cluster dependence in\nmissing response problems to perform valid statistical inference."}
{"id": "2510.21273", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21273", "abs": "https://arxiv.org/abs/2510.21273", "authors": ["Naomi Desobry", "Elnura Zhalieva", "Souhaib Ben Taieb"], "title": "Enforcing Calibration in Multi-Output Probabilistic Regression with Pre-rank Regularization", "comment": null, "summary": "Probabilistic models must be well calibrated to support reliable\ndecision-making. While calibration in single-output regression is well studied,\ndefining and achieving multivariate calibration in multi-output regression\nremains considerably more challenging. The existing literature on multivariate\ncalibration primarily focuses on diagnostic tools based on pre-rank functions,\nwhich are projections that reduce multivariate prediction-observation pairs to\nunivariate summaries to detect specific types of miscalibration. In this work,\nwe go beyond diagnostics and introduce a general regularization framework to\nenforce multivariate calibration during training for arbitrary pre-rank\nfunctions. This framework encompasses existing approaches such as highest\ndensity region calibration and copula calibration. Our method enforces\ncalibration by penalizing deviations of the projected probability integral\ntransforms (PITs) from the uniform distribution, and can be added as a\nregularization term to the loss function of any probabilistic predictor.\nSpecifically, we propose a regularization loss that jointly enforces both\nmarginal and multivariate pre-rank calibration. We also introduce a new\nPCA-based pre-rank that captures calibration along directions of maximal\nvariance in the predictive distribution, while also enabling dimensionality\nreduction. Across 18 real-world multi-output regression datasets, we show that\nunregularized models are consistently miscalibrated, and that our methods\nsignificantly improve calibration across all pre-rank functions without\nsacrificing predictive accuracy."}
{"id": "2510.21174", "categories": ["stat.ME", "stat.CO", "62F99"], "pdf": "https://arxiv.org/pdf/2510.21174", "abs": "https://arxiv.org/abs/2510.21174", "authors": ["Kenyon Ng", "Weichang Yu", "Howard D. Bondell"], "title": "Expectation-propagation for Bayesian empirical likelihood inference", "comment": "15 pages (+31 appendix). 4 figures", "summary": "Bayesian inference typically relies on specifying a parametric model that\napproximates the data-generating process. However, misspecified models can\nyield poor convergence rates and unreliable posterior calibration. Bayesian\nempirical likelihood offers a semi-parametric alternative by replacing the\nparametric likelihood with a profile empirical likelihood defined through\nmoment constraints, thereby avoiding explicit distributional assumptions.\nDespite these advantages, Bayesian empirical likelihood faces substantial\ncomputational challenges, including the need to solve a constrained\noptimization problem for each likelihood evaluation and difficulties with\nnon-convex posterior support, particularly in small-sample settings. This paper\nintroduces a variational approach based on expectation-propagation to\napproximate the Bayesian empirical-likelihood posterior, balancing\ncomputational cost and accuracy without altering the target posterior via\nadjustments such as pseudo-observations. Empirically, we show that our approach\ncan achieve a superior cost-accuracy trade-off relative to existing methods,\nincluding Hamiltonian Monte Carlo and variational Bayes. Theoretically, we show\nthat the approximation and the Bayesian empirical-likelihood posterior are\nasymptotically equivalent."}
{"id": "2510.20942", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.20942", "abs": "https://arxiv.org/abs/2510.20942", "authors": ["Heeju Lim", "Victor E. Lachos", "Victor H. Lachos"], "title": "Bayesian analysis of flexible Heckman selection models using Hamiltonian Monte Carlo", "comment": null, "summary": "The Heckman selection model is widely used in econometric analysis and other\nsocial sciences to address sample selection bias in data modeling. A common\nassumption in Heckman selection models is that the error terms follow an\nindependent bivariate normal distribution. However, real-world data often\ndeviates from this assumption, exhibiting heavy-tailed behavior, which can lead\nto inconsistent estimates if not properly addressed. In this paper, we propose\na Bayesian analysis of Heckman selection models that replace the Gaussian\nassumption with well-known members of the class of scale mixture of normal\ndistributions, such as the Student's-t and contaminated normal distributions.\nFor these complex structures, Stan's default No-U-Turn sampler is utilized to\nobtain posterior simulations. Through extensive simulation studies, we compare\nthe performance of the Heckman selection models with normal, Student's-t and\ncontaminated normal distributions. We also demonstrate the broad applicability\nof this methodology by applying it to medical care and labor supply data. The\nproposed algorithms are implemented in the R package HeckmanStan."}
{"id": "2510.21661", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.21661", "abs": "https://arxiv.org/abs/2510.21661", "authors": ["Heyang Ji", "Carmen Tekwe"], "title": "MECfda: An R Package for Bias Correction Due to Measurement Error in Functional and Scalar Covariates in Scalar-on-Function Regression Models", "comment": null, "summary": "Functional data analysis (FDA) deals with high-resolution data recorded over\na continuum, such as time, space or frequency. Device-based assessments of\nphysical activity or sleep are objective yet still prone to measurement error.\nWe present MECfda, an R package that (i) fits scalar-on-function, generalized\nscalar-on-function, and functional quantile regression models, and (ii)\nprovides bias-corrected estimation when functional covariates are measured with\nerror. By unifying these tools under a consistent syntax, MECfda enables robust\ninference for FDA applications that involve noisy functional data."}
{"id": "2510.21047", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.21047", "abs": "https://arxiv.org/abs/2510.21047", "authors": ["Ziyang Liu", "Ning Hao", "Yue Selena Niu", "Han Xiao", "Hongxu Ding"], "title": "Autocorrelation Test under Frequent Mean Shifts", "comment": null, "summary": "Testing for the presence of autocorrelation is a fundamental problem in time\nseries analysis. Classical methods such as the Box-Pierce test rely on the\nassumption of stationarity, necessitating the removal of non-stationary\ncomponents such as trends or shifts in the mean prior to application. However,\nthis is not always practical, particularly when the mean structure is complex,\nsuch as being piecewise constant with frequent shifts. In this work, we propose\na new inferential framework for autocorrelation in time series data under\nfrequent mean shifts. In particular, we introduce a Shift-Immune Portmanteau\n(SIP) test that reliably tests for autocorrelation and is robust against mean\nshifts. We illustrate an application of our method to nanopore sequencing data."}
{"id": "2510.21542", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21542", "abs": "https://arxiv.org/abs/2510.21542", "authors": ["Johann Flemming Gloy", "Simon Olsson"], "title": "HollowFlow: Efficient Sample Likelihood Evaluation using Hollow Message Passing", "comment": "Accepted to NeurIPS 2025", "summary": "Flow and diffusion-based models have emerged as powerful tools for scientific\napplications, particularly for sampling non-normalized probability\ndistributions, as exemplified by Boltzmann Generators (BGs). A critical\nchallenge in deploying these models is their reliance on sample likelihood\ncomputations, which scale prohibitively with system size $n$, often rendering\nthem infeasible for large-scale problems. To address this, we introduce\n$\\textit{HollowFlow}$, a flow-based generative model leveraging a novel\nnon-backtracking graph neural network (NoBGNN). By enforcing a block-diagonal\nJacobian structure, HollowFlow likelihoods are evaluated with a constant number\nof backward passes in $n$, yielding speed-ups of up to $\\mathcal{O}(n^2)$: a\nsignificant step towards scaling BGs to larger systems. Crucially, our\nframework generalizes: $\\textbf{any equivariant GNN or attention-based\narchitecture}$ can be adapted into a NoBGNN. We validate HollowFlow by training\nBGs on two different systems of increasing size. For both systems, the sampling\nand likelihood evaluation time decreases dramatically, following our\ntheoretical scaling laws. For the larger system we obtain a $10^2\\times$\nspeed-up, clearly illustrating the potential of HollowFlow-based approaches for\nhigh-dimensional scientific problems previously hindered by computational\nbottlenecks."}
{"id": "2510.21116", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.21116", "abs": "https://arxiv.org/abs/2510.21116", "authors": ["Bolun Liu", "Trang Quynh Nguyen", "Elizabeth A. Stuart", "Bryan Lau", "Amii M. Kress", "Michael R. Elliott", "Kyle R. Busse", "Ellen C. Caniglia", "Yajnaseni Chakraborti", "Amy J. Elliott", "James E. Gern", "Alison E. Hipwell", "Catherine J. Karr", "Kaja Z. LeWinn", "Li Luo", "Hans-Georg Müller", "Sunni L. Mumford", "Ruby H. N. Nguyen", "Emily Oken", "Janet L. Peacock", "Enrique F. Schisterman", "Arjun Sondhi", "Rosalind J. Wright", "Yidong Zhou", "Elizabeth L. Ogburn"], "title": "Sensitivity Analysis when Generalizing Causal Effects from Multiple Studies to a Target Population: Motivation from the ECHO Program", "comment": null, "summary": "Unobserved effect modifiers can induce bias when generalizing causal effect\nestimates to target populations. In this work, we extend a sensitivity analysis\nframework assessing the robustness of study results to unobserved effect\nmodification that adapts to various generalizability scenarios, including\nmultiple (conditionally) randomized trials, observational studies, or\ncombinations thereof. This framework is interpretable and does not rely on\ndistributional or functional assumptions about unknown parameters. We\ndemonstrate how to leverage the multi-study setting to detect violation of the\ngeneralizability assumption through hypothesis testing, showing with\nsimulations that the proposed test achieves high power under real-world sample\nsizes. Finally, we apply our sensitivity analysis framework to analyze the\ngeneralized effect estimate of secondhand smoke exposure on birth weight using\ncohort sites from the Environmental influences on Child Health Outcomes (ECHO)\nstudy."}
{"id": "2510.21598", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21598", "abs": "https://arxiv.org/abs/2510.21598", "authors": ["Diana Cai", "Robert M. Gower", "David M. Blei", "Lawrence K. Saul"], "title": "Fisher meets Feynman: score-based variational inference with a product of experts", "comment": "27 pages, 11 figures. To appear in Advances in Neural Processing\n  Information Systems (NeurIPS), 2025", "summary": "We introduce a highly expressive yet distinctly tractable family for\nblack-box variational inference (BBVI). Each member of this family is a\nweighted product of experts (PoE), and each weighted expert in the product is\nproportional to a multivariate $t$-distribution. These products of experts can\nmodel distributions with skew, heavy tails, and multiple modes, but to use them\nfor BBVI, we must be able to sample from their densities. We show how to do\nthis by reformulating these products of experts as latent variable models with\nauxiliary Dirichlet random variables. These Dirichlet variables emerge from a\nFeynman identity, originally developed for loop integrals in quantum field\ntheory, that expresses the product of multiple fractions (or in our case,\n$t$-distributions) as an integral over the simplex. We leverage this simplicial\nlatent space to draw weighted samples from these products of experts -- samples\nwhich BBVI then uses to find the PoE that best approximates a target density.\nGiven a collection of experts, we derive an iterative procedure to optimize the\nexponents that determine their geometric weighting in the PoE. At each\niteration, this procedure minimizes a regularized Fisher divergence to match\nthe scores of the variational and target densities at a batch of samples drawn\nfrom the current approximation. This minimization reduces to a convex quadratic\nprogram, and we prove under general conditions that these updates converge\nexponentially fast to a near-optimal weighting of experts. We conclude by\nevaluating this approach on a variety of synthetic and real-world target\ndistributions."}
{"id": "2510.21119", "categories": ["stat.ME", "stat.ML", "62K86, 65F55"], "pdf": "https://arxiv.org/pdf/2510.21119", "abs": "https://arxiv.org/abs/2510.21119", "authors": ["Lei Shi", "David Arbour", "Raghavendra Addanki", "Ritwik Sinha", "Avi Feller"], "title": "Leveraging semantic similarity for experimentation with AI-generated treatments", "comment": "31 pages, 5 figures", "summary": "Large Language Models (LLMs) enable a new form of digital experimentation\nwhere treatments combine human and model-generated content in increasingly\nsophisticated ways. The main methodological challenge in this setting is\nrepresenting these high-dimensional treatments without losing their semantic\nmeaning or rendering analysis intractable. Here, we address this problem by\nfocusing on learning low-dimensional representations that capture the\nunderlying structure of such treatments. These representations enable\ndownstream applications such as guiding generative models to produce meaningful\ntreatment variants and facilitating adaptive assignment in online experiments.\nWe propose double kernel representation learning, which models the causal\neffect through the inner product of kernel-based representations of treatments\nand user covariates. We develop an alternating-minimization algorithm that\nlearns these representations efficiently from data and provides convergence\nguarantees under a low-rank factor model. As an application of this framework,\nwe introduce an adaptive design strategy for online experimentation and\ndemonstrate the method's effectiveness through numerical experiments."}
{"id": "2510.21686", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21686", "abs": "https://arxiv.org/abs/2510.21686", "authors": ["Raheem Karim Hashmani", "Garrett W. Merz", "Helen Qu", "Mariel Pettee", "Kyle Cranmer"], "title": "Multimodal Datasets with Controllable Mutual Information", "comment": "15 pages, 4 figures, 1 table. Our code is publicly available at\n  https://github.com/RKHashmani/MmMi-Datasets", "summary": "We introduce a framework for generating highly multimodal datasets with\nexplicitly calculable mutual information between modalities. This enables the\nconstruction of benchmark datasets that provide a novel testbed for systematic\nstudies of mutual information estimators and multimodal self-supervised\nlearning techniques. Our framework constructs realistic datasets with known\nmutual information using a flow-based generative model and a structured causal\nframework for generating correlated latent variables."}
{"id": "2510.21174", "categories": ["stat.ME", "stat.CO", "62F99"], "pdf": "https://arxiv.org/pdf/2510.21174", "abs": "https://arxiv.org/abs/2510.21174", "authors": ["Kenyon Ng", "Weichang Yu", "Howard D. Bondell"], "title": "Expectation-propagation for Bayesian empirical likelihood inference", "comment": "15 pages (+31 appendix). 4 figures", "summary": "Bayesian inference typically relies on specifying a parametric model that\napproximates the data-generating process. However, misspecified models can\nyield poor convergence rates and unreliable posterior calibration. Bayesian\nempirical likelihood offers a semi-parametric alternative by replacing the\nparametric likelihood with a profile empirical likelihood defined through\nmoment constraints, thereby avoiding explicit distributional assumptions.\nDespite these advantages, Bayesian empirical likelihood faces substantial\ncomputational challenges, including the need to solve a constrained\noptimization problem for each likelihood evaluation and difficulties with\nnon-convex posterior support, particularly in small-sample settings. This paper\nintroduces a variational approach based on expectation-propagation to\napproximate the Bayesian empirical-likelihood posterior, balancing\ncomputational cost and accuracy without altering the target posterior via\nadjustments such as pseudo-observations. Empirically, we show that our approach\ncan achieve a superior cost-accuracy trade-off relative to existing methods,\nincluding Hamiltonian Monte Carlo and variational Bayes. Theoretically, we show\nthat the approximation and the Bayesian empirical-likelihood posterior are\nasymptotically equivalent."}
{"id": "2510.21119", "categories": ["stat.ME", "stat.ML", "62K86, 65F55"], "pdf": "https://arxiv.org/pdf/2510.21119", "abs": "https://arxiv.org/abs/2510.21119", "authors": ["Lei Shi", "David Arbour", "Raghavendra Addanki", "Ritwik Sinha", "Avi Feller"], "title": "Leveraging semantic similarity for experimentation with AI-generated treatments", "comment": "31 pages, 5 figures", "summary": "Large Language Models (LLMs) enable a new form of digital experimentation\nwhere treatments combine human and model-generated content in increasingly\nsophisticated ways. The main methodological challenge in this setting is\nrepresenting these high-dimensional treatments without losing their semantic\nmeaning or rendering analysis intractable. Here, we address this problem by\nfocusing on learning low-dimensional representations that capture the\nunderlying structure of such treatments. These representations enable\ndownstream applications such as guiding generative models to produce meaningful\ntreatment variants and facilitating adaptive assignment in online experiments.\nWe propose double kernel representation learning, which models the causal\neffect through the inner product of kernel-based representations of treatments\nand user covariates. We develop an alternating-minimization algorithm that\nlearns these representations efficiently from data and provides convergence\nguarantees under a low-rank factor model. As an application of this framework,\nwe introduce an adaptive design strategy for online experimentation and\ndemonstrate the method's effectiveness through numerical experiments."}
{"id": "2510.21249", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21249", "abs": "https://arxiv.org/abs/2510.21249", "authors": ["Daniele Girolimetto", "Anastasios Panagiotelis", "Tommaso Di Fonzo", "Han Li"], "title": "Forecast reconciliation with non-linear constraints", "comment": null, "summary": "Methods for forecasting time series adhering to linear constraints have seen\nnotable development in recent years, especially with the advent of forecast\nreconciliation. This paper extends forecast reconciliation to the open question\nof non-linearly constrained time series. Non-linear constraints can emerge with\nvariables that are formed as ratios such as mortality rates and unemployment\nrates. On the methodological side, Non-linearly Constrained Reconciliation\n(NLCR) is proposed. This algorithm adjusts forecasts that fail to meet\nnon-linear constraints, in a way that ensures the new forecasts meet the\nconstraints. The NLCR method is a projection onto a non-linear surface,\nformulated as a constrained optimisation problem. On the theoretical side,\noptimisation methods are again used, this time to derive sufficient conditions\nfor when the NLCR methodology is guaranteed to improve forecast accuracy.\nFinally on the empirical side, NLCR is applied to two datasets from demography\nand economics and shown to significantly improve forecast accuracy relative to\nrelevant benchmarks."}
{"id": "2510.21249", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21249", "abs": "https://arxiv.org/abs/2510.21249", "authors": ["Daniele Girolimetto", "Anastasios Panagiotelis", "Tommaso Di Fonzo", "Han Li"], "title": "Forecast reconciliation with non-linear constraints", "comment": null, "summary": "Methods for forecasting time series adhering to linear constraints have seen\nnotable development in recent years, especially with the advent of forecast\nreconciliation. This paper extends forecast reconciliation to the open question\nof non-linearly constrained time series. Non-linear constraints can emerge with\nvariables that are formed as ratios such as mortality rates and unemployment\nrates. On the methodological side, Non-linearly Constrained Reconciliation\n(NLCR) is proposed. This algorithm adjusts forecasts that fail to meet\nnon-linear constraints, in a way that ensures the new forecasts meet the\nconstraints. The NLCR method is a projection onto a non-linear surface,\nformulated as a constrained optimisation problem. On the theoretical side,\noptimisation methods are again used, this time to derive sufficient conditions\nfor when the NLCR methodology is guaranteed to improve forecast accuracy.\nFinally on the empirical side, NLCR is applied to two datasets from demography\nand economics and shown to significantly improve forecast accuracy relative to\nrelevant benchmarks."}
{"id": "2510.21579", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.21579", "abs": "https://arxiv.org/abs/2510.21579", "authors": ["Ken Newman", "Shaini Naha", "Leah Jackson-Blake", "Cairistiona Topp", "Miriam Glendell", "Adam Butler"], "title": "A Comparison for Non-Specialists of Workflow Steps and Similarity of Factor Rankings for Several Global Sensitivity Analysis Methods", "comment": null, "summary": "Global sensitivity analysis (GSA) is a recommended step in the use of\ncomputer simulation models. GSA quantifies the relative importance of model\ninputs on outputs (Factor Ranking), identifies inputs that could be fixed, thus\nsimplifying model calibration (Factor Fixing), and pinpointing areas for future\ndata collection (Factor Prioritization). Given the wide variety of GSA methods,\nchoosing between methods can be challenging for non-GSA experts. Issues include\nworkflow steps and complexity, interpretation of GSA outputs, and the degree of\nsimilarity between methods in Factor Ranking. We conducted a study of both\nwidely and less commonly used GSA methods applied to three simulators of\ndiffering complexity. All methods share common issues around implementation\nwith specification of parameter ranges particularly critical. Similarities in\nFactor Rankings were generally high based on Kendall's W. Sobol' first order\nand total sensitivity indices were easy to interpret and informative with\nregression trees providing additional insight into interactions."}
{"id": "2510.21661", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.21661", "abs": "https://arxiv.org/abs/2510.21661", "authors": ["Heyang Ji", "Carmen Tekwe"], "title": "MECfda: An R Package for Bias Correction Due to Measurement Error in Functional and Scalar Covariates in Scalar-on-Function Regression Models", "comment": null, "summary": "Functional data analysis (FDA) deals with high-resolution data recorded over\na continuum, such as time, space or frequency. Device-based assessments of\nphysical activity or sleep are objective yet still prone to measurement error.\nWe present MECfda, an R package that (i) fits scalar-on-function, generalized\nscalar-on-function, and functional quantile regression models, and (ii)\nprovides bias-corrected estimation when functional covariates are measured with\nerror. By unifying these tools under a consistent syntax, MECfda enables robust\ninference for FDA applications that involve noisy functional data."}
{"id": "2510.21708", "categories": ["stat.ME", "62J15"], "pdf": "https://arxiv.org/pdf/2510.21708", "abs": "https://arxiv.org/abs/2510.21708", "authors": ["David S. Robertson", "Thomas Jaki"], "title": "Optimal weighted tests for replication studies and the two-trials rule", "comment": "28 pages", "summary": "Replication studies for scientific research are an important part of ensuring\nthe reliability and integrity of experimental findings. In the context of\nclinical trials, the concept of replication has been formalised by the\n'two-trials' rule, where two pivotal studies are required to show positive\nresults before a drug can be approved. In experiments testing multiple\nhypotheses simultaneously, control of the overall familywise error rate (FWER)\nis additionally required in many contexts. The well-known Bonferroni procedure\ncontrols the FWER, and a natural extension is to introduce weights into this\nprocedure to reflect the a-priori importance of hypotheses or to maximise some\nmeasure of the overall power of the experiment. In this paper, we consider\nanalysing a replication study using an optimal weighted Bonferroni procedure,\nwith the weights based on the results of the original study that is being\nreplicated and the optimality criterion being to maximise the disjunctive power\nof the trial (the power to reject at least one non-null hypothesis). We show\nthat using the proposed procedure can lead to a substantial increase in the\ndisjunctive power of the replication study, and is robust to changes in the\neffect sizes between the two studies."}
{"id": "2510.20947", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20947", "abs": "https://arxiv.org/abs/2510.20947", "authors": ["Florian D. van Leeuwen", "Sara van Erp"], "title": "To MCMC or not to MCMC: Evaluating non-MCMC methods for Bayesian penalized regression", "comment": null, "summary": "Markov Chain Monte Carlo (MCMC) sampling is computationally expensive,\nespecially for complex models. Alternative methods make simplifying assumptions\nabout the posterior to reduce computational burden, but their impact on\npredictive performance remains unclear. This paper compares MCMC and non-MCMC\nmethods for high-dimensional penalized regression, examining when computational\nshortcuts are justified for prediction tasks.\n  We conduct a comprehensive simulation study using high-dimensional tabular\ndata, then validate findings with empirical datasets featuring both continuous\nand binary outcomes. An in-depth analysis of one dataset provides a\nstep-by-step tutorial implementing various algorithms in R.\n  Our results show that mean-field variational inference consistently performs\ncomparably to MCMC methods. In simulations, mean-field VI exhibited 3-90\\%\nhigher MSE across scenarios while reducing runtime by 7-30x compared to\nHamiltonian Monte Carlo. Empirical datasets revealed dramatic speed-ups\n(100-400x) in some cases with similar or superior predictive performance.\nHowever, performance varied: some cases showed over 100x MSE increases with\nonly 30x speed-ups, highlighting the context-dependent nature of these\ntrade-offs."}
{"id": "2510.21091", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.21091", "abs": "https://arxiv.org/abs/2510.21091", "authors": ["Kyungseon Lee", "Kunwoong Kim", "Jihu Lee", "Dongyoon Yang", "Yongdai Kim"], "title": "Doubly-Regressing Approach for Subgroup Fairness", "comment": null, "summary": "Algorithmic fairness is a socially crucial topic in real-world applications\nof AI.\n  Among many notions of fairness, subgroup fairness is widely studied when\nmultiple sensitive attributes (e.g., gender, race, age) are present.\n  However, as the number of sensitive attributes grows, the number of subgroups\nincreases accordingly, creating heavy computational burdens and data sparsity\nproblem (subgroups with too small sizes).\n  In this paper, we develop a novel learning algorithm for subgroup fairness\nwhich resolves these issues by focusing on subgroups with sufficient sample\nsizes as well as marginal fairness (fairness for each sensitive attribute).\n  To this end, we formalize a notion of subgroup-subset fairness and introduce\na corresponding distributional fairness measure called the supremum Integral\nProbability Metric (supIPM).\n  Building on this formulation, we propose the Doubly Regressing Adversarial\nlearning for subgroup Fairness (DRAF) algorithm, which reduces a surrogate\nfairness gap for supIPM with much less computation than directly reducing\nsupIPM.\n  Theoretically, we prove that the proposed surrogate fairness gap is an upper\nbound of supIPM.\n  Empirically, we show that the DRAF algorithm outperforms baseline methods in\nbenchmark datasets, specifically when the number of sensitive attributes is\nlarge so that many subgroups are very small."}
