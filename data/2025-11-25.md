<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 1]
- [stat.AP](#stat.AP) [Total: 4]
- [stat.ME](#stat.ME) [Total: 14]
- [stat.CO](#stat.CO) [Total: 1]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates](https://arxiv.org/abs/2511.16815)
*Kyla D. Jones,Alexander W. Dowling*

Main category: stat.ML

TL;DR: 提出了BITS for GAPS框架，用于混合物理系统中的隐变量建模，通过熵基采集函数指导数据采样，提高样本效率


<details>
  <summary>Details</summary>
Motivation: 解决混合物理系统建模问题，其中部分系统由已知物理规律控制，剩余动态作为从数据推断的隐函数

Method: 使用高斯过程先验对隐函数建模，采用分层先验对超参数编码物理意义结构，推导闭式微分熵表达式和可处理下界

Result: 数值实验表明熵引导采样通过针对高不确定性和潜在信息增益区域，提高了样本效率，加速了代理模型收敛

Conclusion: BITS for GAPS为复杂物理系统的混合建模提供了高效、可解释且考虑不确定性的框架

Abstract: We introduce the Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates (BITS for GAPS) framework to emulate latent components in hybrid physical systems. BITS for GAPS supports serial hybrid modeling, where known physics governs part of the system and residual dynamics are represented as a latent function inferred from data. A Gaussian process prior is placed over the latent function, with hierarchical priors on its hyperparameters to encode physically meaningful structure in the predictive posterior.
  To guide data acquisition, we derive entropy-based acquisition functions that quantify expected information gain from candidate input locations, identifying samples most informative for training the surrogate. Specifically, we obtain a closed-form expression for the differential entropy of the predictive posterior and establish a tractable lower bound for efficient evaluation. These derivations approximate the predictive posterior as a finite, uniformly weighted mixture of Gaussian processes.
  We demonstrate the framework's utility by modeling activity coefficients in vapor-liquid equilibrium systems, embedding the surrogate into extended Raoult's law for distillation design. Numerical results show that entropy-guided sampling improves sample efficiency by targeting regions of high uncertainty and potential information gain. This accelerates surrogate convergence, enhances predictive accuracy in non-ideal regimes, and preserves physical consistency. Overall, BITS for GAPS provides an efficient, interpretable, and uncertainty-aware framework for hybrid modeling of complex physical systems.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [2] [Trust-Aware Multimodal Data Fusion for Yield Estimation: A Case Study of the 2020 Beirut Explosion](https://arxiv.org/abs/2511.16816)
*Lekha Patel,Craig Ulmer,Stephen J. Verzi,Daniel J. Krofcheck,Indu Manickam,Asmeret Naugle,Jaideep Ray*

Main category: stat.AP

TL;DR: 提出了一种新颖的贝叶斯分数后验框架，融合多种观测数据来估计爆炸当量，特别应用于2020年贝鲁特爆炸事件。


<details>
  <summary>Details</summary>
Motivation: 解决从异构观测数据估计爆炸当量的基本挑战，特别是结合传统物理测量和现代人工智能解释模态时的困难。

Method: 使用贝叶斯分数后验框架，通过狄利克雷先验学习每种数据模态的信任权重，自动校准不同观测的相对信息内容。融合地震波、弹坑尺寸、合成孔径雷达图像和视觉语言模型解释的地面图像。

Result: 应用于贝鲁特爆炸，估计当量为0.34-0.48千吨TNT当量，相对于爆炸储存的2.75千吨硝酸铵的理论最大值，代表12-17%的爆炸效率。

Conclusion: 分数后验方法在不确定性量化方面优于单模态估计，同时对系统偏差具有鲁棒性，为将定性评估与定量物理测量整合提供了原则性框架。

Abstract: The estimation of explosive yield from heterogeneous observational data presents fundamental challenges in inverse problems, particularly when combining traditional physical measurements with modern artificial intelligence-interpreted modalities. We present a novel Bayesian fractional posterior framework that fuses seismic waves, crater dimensions, synthetic aperture radar imagery, and vision-language model interpreted ground-level images to estimate the yield of the 2020 Beirut explosion. Unlike conventional approaches that may treat data sources equally, our method learns trust weights for each modality through a Dirichlet prior, automatically calibrating the relative information content of disparate observations. Applied to the Beirut explosion, the framework yields an estimate of 0.34--0.48 kt TNT equivalent, representing 12 to 17 percent detonation efficiency relative to the 2.75 kt theoretical maximum from the blast's stored ammonium nitrate. The fractional posterior approach demonstrates superior uncertainty quantification compared to single-modality estimates while providing robustness against systematic biases. This work establishes a principled framework for integrating qualitative assessments with quantitative physical measurements, with applications to explosion monitoring, disaster response, and forensic analysis.

</details>


### [3] [Optimising pandemic response through vaccination strategies using neural networks](https://arxiv.org/abs/2511.16932)
*Chang Zhai,Ping Chen,Zhuo Jin,David Pitt*

Main category: stat.AP

TL;DR: 开发了一个数据驱动的决策支持工具，通过三阶段经济流行病学框架来优化疫苗接种策略，在控制疾病传播的同时最小化经济损失。


<details>
  <summary>Details</summary>
Motivation: 传统流行病风险评估方法难以平衡健康结果和经济约束，需要一种能够同时优化疾病控制和成本效益的方法。

Method: 采用三阶段框架：1) 随机隔室模型模拟流行病动态；2) 最优控制问题制定最小化支出的疫苗接种策略；3) 使用神经网络校准参数并解决高维控制问题。

Result: 使用澳大利亚维多利亚州COVID-19数据进行实证分析，得出同时最小化疾病发病率和政府支出的最优疫苗接种策略。

Conclusion: 该三阶段框架使政策制定者能够根据不断变化的传播动态调整策略，最小化总成本，有助于未来大流行防范。

Abstract: Epidemic risk assessment poses inherent challenges, with traditional approaches often failing to balance health outcomes and economic constraints. This paper presents a data-driven decision support tool that models epidemiological dynamics and optimises vaccination strategies to control disease spread whilst minimising economic losses. The proposed economic-epidemiological framework comprises three phases: modelling, optimising, and analysing. First, a stochastic compartmental model captures epidemic dynamics. Second, an optimal control problem is formulated to derive vaccination strategies that minimise pandemic-related expenditure. Given the analytical intractability of epidemiological models, neural networks are employed to calibrate parameters and solve the high-dimensional control problem. The framework is demonstrated using COVID-19 data from Victoria, Australia, empirically deriving optimal vaccination strategies that simultaneously minimise disease incidence and governmental expenditure. By employing this three-phase framework, policymakers can adjust input values to reflect evolving transmission dynamics and continuously update strategies, thereby minimising aggregate costs, aiding future pandemic preparedness.

</details>


### [4] [Effects of Distance Metrics and Scaling on the Perturbation Discrimination Score](https://arxiv.org/abs/2511.16954)
*Qiyuan Liu,Qirui Zhang,Jinhong Du,Siming Zhao,Jingshu Wang*

Main category: stat.AP

TL;DR: PDS评估指标在高维基因表达场景中对相似性度量和预测效应尺度高度敏感，不同距离度量（ℓ₁、ℓ₂、余弦）表现差异显著，即使经过范数匹配。


<details>
  <summary>Details</summary>
Motivation: PDS被广泛用于评估预测扰动效应是否可区分，但其在高维基因表达设置中的行为尚未详细研究，需要分析其对不同相似性度量和尺度选择的敏感性。

Method: 通过分析观测到的扰动响应，比较ℓ₁、ℓ₂和余弦相似性度量的PDS表现，提供几何解释，并探讨范数匹配后的效果。

Result: 发现ℓ₁和ℓ₂基础的PDS与余弦基础度量表现截然不同，即使经过范数匹配，PDS对相似性度量和预测效应尺度选择高度敏感。

Conclusion: PDS在高维基因表达评估中存在局限性，未来基于区分度的评估指标需要考虑相似性度量的选择敏感性问题。

Abstract: The Perturbation Discrimination Score (PDS) is increasingly used to evaluate whether predicted perturbation effects remain distinguishable, including in Systema and the Virtual Cell Challenge. However, its behavior in high-dimensional gene-expression settings has not been examined in detail. We show that PDS is highly sensitive to the choice of similarity or distance measure and to the scale of predicted effects. Analysis of observed perturbation responses reveals that $\ell_1$ and $\ell_2$-based PDS behave very differently from cosine-based measures, even after norm matching. We provide geometric insight and discuss implications for future discrimination-based evaluation metrics.

</details>


### [5] [A spatiotemporal Bayesian hierarchical model of heat-related mortality in Catalonia, Spain (2012--2022): The role of environmental and socioeconomic modifiers](https://arxiv.org/abs/2511.17148)
*David Solano,Marta Solans,Xavier Perafita,Anna Ruiz-Comellas,Marc Saez,Maria A. Barceló*

Main category: stat.AP

TL;DR: 研究分析了2012-2022年加泰罗尼亚地区极端高温与死亡率的关系，发现高温本身不独立影响死亡率，其效应被臭氧污染和社会经济因素完全或部分混淆。


<details>
  <summary>Details</summary>
Motivation: 极端高温是重大公共卫生风险，但其与死亡率的关系可能受到空气污染和社会因素的混杂或修饰影响，需要量化这些复杂关系。

Method: 在379个基础卫生区域进行时间序列生态学研究，使用分层贝叶斯时空模型，纳入结构化和非结构化随机效应，考虑社会经济混杂因素。

Result: 极端高温本身不独立关联死亡率，其效应被高臭氧水平完全混淆，被社会经济指标部分混淆。臭氧浓度≥120 μg/m³显著增加死亡率风险，特别是≥85岁人群。收入不平等和老年人口比例增加脆弱性。

Conclusion: 加泰罗尼亚地区极端高温的死亡风险强烈受臭氧水平和社会决定因素影响，适应策略应同时解决复合环境暴露和社会经济脆弱性，以更好保护老年和弱势群体。

Abstract: Background: Extreme heat is a major public health risk, yet its relationship with mortality may be confounded or modified by air pollution and social determinants. Objectives: We aimed to quantify the effects of extreme maximum temperatures and heatwaves on daily mortality in Catalonia (2012--2022), and to assess the modifying and confounding roles of air pollutants and socioeconomic factors. Methods: We conducted a time--series ecological study across 379 basic health areas (ABS) during summer months. Mortality data from the Spanish National Statistics Institute were linked with meteorological and air pollution data. A hierarchical Bayesian spatiotemporal model, incorporating structured and unstructured random effects, was used to account for spatial and temporal dependencies, as well as observed socioeconomic confounders. Results: In total, 730,634 deaths occurred, with 216,989 in summer. Extreme heat alone was not independently associated with mortality, as its effect was fully confounded by high ozone levels and partly by socioeconomic indicators. Ozone concentrations ($\ge 120 μg/m^3$) significantly increased mortality risk, especially among individuals aged $\ge 85$ years. Greater income inequality and higher proportions of older residents also amplified vulnerability. Conclusion: Mortality risks from extreme heat in Catalonia were strongly influenced by ozone levels and social determinants. Adaptation strategies should address both compound environmental exposures together with socioeconomic vulnerability to better protect older and disadvantaged populations.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [6] [Regularized Reduced Rank Regression for mixed predictor and response variables](https://arxiv.org/abs/2511.16718)
*Lorenza Cotugno,Mark de Rooij,Roberta Siciliano*

Main category: stat.ME

TL;DR: GMR4模型是GMR3的扩展，通过加入正则化技术（Ridge、Lasso、Group Lasso等）改进了在高维设置下的性能，适用于混合类型预测变量和响应变量的回归分析。


<details>
  <summary>Details</summary>
Motivation: GMR3模型在处理混合类型数据时表现良好，但在高维数据或存在共线性的情况下性能有限，需要引入正则化技术来提高模型性能。

Method: 提出GMR4模型，结合正则化技术，开发交叉验证程序来估计秩S和惩罚参数lambda，通过模拟研究评估模型在不同场景下的表现。

Result: 模拟研究表明GMR4在不同样本量、非信息预测变量数量和响应维度下表现良好，在ISSP健康与医疗调查应用中得到了稀疏且可解释的解决方案。

Conclusion: GMR4模型能够有效处理高维混合类型数据，提供稀疏且可解释的结果，在公共卫生态度研究中识别出有限但重要的预测变量。

Abstract: In this paper, we introduce the Generalized Mixed Regularized Reduced Rank Regression model (GMR4), an extension of the GMR3 model designed to improve performance in high-dimensional settings. GMR3 is a regression method for a mix of numeric, binary and ordinal response variables, while also allowing for mixed-type predictors through optimal scaling. GMR4 extends this approach by incorporating regularization techniques, such as Ridge, Lasso, Group Lasso, or any combination thereof, making the model suitable for datasets with a large number of predictors or collinearity among them. In addition, we propose a cross-validation procedure that enables the estimation of the rank S and the penalty parameter lambda. Through a simulation study, we evaluate the performance of the model under different scenarios, varying the sample size, the number of non-informative predictors and response dimension. The results of the simulation study guide the choice of the penalty parameter lambda in the empirical application ISSP: Health and Healthcare I-II (2023), which includes mixed-type predictors and ordinal responses. In this application, the model results in a sparse and interpretable solution, with a limited set of influential predictors that provide insights into public attitudes toward healthcare.

</details>


### [7] [Correlation Matters! Streamlining the Sample Size Procedure with Composite Time-to-event Endpoints](https://arxiv.org/abs/2511.16773)
*Yunhan Mou,Fan Li,Denise Esserman,Yuan Huang*

Main category: stat.ME

TL;DR: 本文开发了改进的公式来计算多个优先排序的生存终点中的赢、输和平局比例，解决了Win Ratio分析中样本量确定依赖模拟且忽略终点相关性的问题。


<details>
  <summary>Details</summary>
Motivation: Win Ratio方法在心血管临床试验中日益流行，但现有的样本量公式依赖难以指定的设计参数，通常需要计算密集的模拟，且常假设终点独立，这可能导致误导性的功效估计。

Method: 推导了计算多个优先排序生存终点的赢、输和平局比例的改进公式，这些公式使用熟悉的设计输入，并与现有样本量方法结合使用。通过基于SPRINT和STICH试验的案例研究评估终点相关性对样本量要求的影响。

Result: 开发了可直接应用的公式，能够准确计算终点相关情况下的赢/输/平局比例，为Win Ratio分析的样本量确定提供了更实用的方法。

Conclusion: 提出的改进公式解决了Win Ratio分析中样本量确定的挑战，考虑了终点相关性对样本量要求的影响，为临床试验设计提供了更可靠的指导。

Abstract: Composite endpoints are widely used in cardiovascular clinical trials to improve statistical efficiency while preserving clinical relevance. The Win Ratio (WR) measure and more general frameworks of Win Statistics have emerged as increasingly popular alternatives to traditional time-to-first-event analyses. Although analytic sample size formulas for WR have been developed, they rely on design parameters that are often not straightforward to specify. Consequently, sample size determination in clinical trials with WR as the primary analysis is most often based on simulations, which can be computationally intensive. Moreover, these simulations commonly assume independence among component endpoints, an assumption that may not hold in practice and can lead to misleading power estimates. To address this challenge, we derive refined formulas to calculate the proportions of wins, losses, and ties for multiple prioritized time-to-event endpoints. These formulas rely on familiar design inputs and become directly applicable when integrated with existing sample size methods. We conduct a comprehensive assessment of how correlation among endpoints affects sample size requirements across varying design features. We further demonstrate the role of correlations through two case studies based on the landmark SPRINT and STICH clinical trials to generate further insights.

</details>


### [8] [Single-Dataset Meta-Analysis For Many-Analysts And Multiverse Studies](https://arxiv.org/abs/2511.17064)
*František Bartoš,Suzanne Hoogeveen,Alexandra Sarafoglou,Samuel Pawel*

Main category: stat.ME

TL;DR: 提出单数据集元分析方法，解决多分析者研究中因使用同一数据集导致结果合成困难的问题，防止标准元分析产生过度自信的推断。


<details>
  <summary>Details</summary>
Motivation: 多分析者、多宇宙和稳健性研究显示结果会因分析选择而变，但所有结果来自同一数据集，传统元分析方法会导致过度自信的推断。

Method: 采用加权似然方法，确保数据集信息最多只使用一次，可估计分析方法的平均效应和异质性，支持经典和贝叶斯假设检验。

Result: 应用于足球种族偏见、婚姻状况与心血管疾病、技术使用与幸福感等研究，展示了该方法对多分析者研究的补充价值。

Conclusion: 单数据集元分析为多分析者研究提供了定量合成工具，避免传统元分析的局限性，增强结果的可解释性。

Abstract: Empirical claims often rely on one population, design, and analysis. Many-analysts, multiverse, and robustness studies expose how results can vary across plausible analytic choices. Synthesizing these results, however, is nontrivial as all results are computed from the same dataset. We introduce single-dataset meta-analysis, a weighted-likelihood approach that incorporates the information in the dataset at most once. It prevents overconfident inferences that would arise if a standard meta-analysis was applied to the data. Single-dataset meta-analysis yields meta-analytic point and interval estimates of the average effect across analytic approaches and of between-analyst heterogeneity, and can be supplied by classical and Bayesian hypothesis tests. Both the common-effect and random-effects versions of the model can be estimated by standard meta-analytic software with small input adjustments. We demonstrate the method via application to the many-analysts study on racial bias in soccer, the many-analysts study of marital status and cardiovascular disease, and the multiverse study on technology use and well-being. The results show how single-dataset meta-analysis complements the qualitative evaluation of many-analysts and multiverse studies.

</details>


### [9] [Shape Analysis of Euclidean Curves under Frenet-Serret Framework](https://arxiv.org/abs/2511.17065)
*Perrine Chassat,Juhyun Park,Nicolas Brunel*

Main category: stat.ME

TL;DR: 本文提出了一种基于Frenet-Serret方程广义曲率的黎曼几何框架，通过平方根曲率变换扩展了欧几里得曲线的平方根速度变换，能够考虑形状的所有几何特征，避免仅使用一阶几何信息导致的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有的几何框架主要关注不变特征，但在处理曲线时往往只考虑一阶几何信息，导致在计算不变距离、曲线平均或配准时出现伪影。需要一种能够充分考虑所有几何特征的框架。

Method: 使用Frenet-Serret方程的广义曲率定义黎曼几何，提出平方根曲率变换来扩展欧几里得曲线的平方根速度变换，该方法适用于任意维度的曲线。

Result: 通过模拟数据验证了该方法的有效性，特别是在分析手语轨迹时，考虑曲率和扭转这两个物理意义明确的参数具有重要价值。

Conclusion: 提出的黎曼几何框架能够有效分析曲线的所有几何特征，在人体运动分析特别是手语轨迹研究中显示出显著优势，避免了传统方法中的伪影问题。

Abstract: Geometric frameworks for analyzing curves are common in applications as they focus on invariant features and provide visually satisfying solutions to standard problems such as computing invariant distances, averaging curves, or registering curves. We show that for any smooth curve in R^d, d>1, the generalized curvatures associated with the Frenet-Serret equation can be used to define a Riemannian geometry that takes into account all the geometric features of the shape. This geometry is based on a Square Root Curvature Transform that extends the square root-velocity transform for Euclidean curves (in any dimensions) and provides likely geodesics that avoid artefacts encountered by representations using only first-order geometric information. Our analysis is supported by simulated data and is especially relevant for analyzing human motions. We consider trajectories acquired from sign language, and show the interest of considering curvature and also torsion in their analysis, both being physically meaningful.

</details>


### [10] [On treating right-censoring events like treatments](https://arxiv.org/abs/2511.17379)
*Lan Wen,Aaron L. Sarvet,Jessica G. Young*

Main category: stat.ME

TL;DR: 本文澄清了在因果推断中定义潜在结果时不需要限制"消除所有右删失事件"，提出了从因果角度区分不同类型右删失事件的框架，并提供了存在右删失时识别因果估计量的指导。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断文献中，潜在结果通常基于"消除所有右删失事件"来定义，这给人们造成该限制是定义良好因果估计量所必需的印象。本文旨在澄清这种限制并非必要。

Method: 提出了一个从因果角度区分不同类型右删失事件的框架，将这一框架与经典生存分析文献中的删失定义和假设联系起来。

Result: 证明了无需基于消除右删失事件来定义潜在结果也能构建良好定义的估计量，提供了更精确的右删失事件分类方法。

Conclusion: 通过桥接不同视角，为处理右删失事件和在存在右删失时识别因果估计量提供了更清晰的理解和指导。

Abstract: In causal inference literature, potential outcomes are often indexed by the "elimination of all right-censoring events," leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present.

</details>


### [11] [Flexible unimodal density estimation in hidden Markov models](https://arxiv.org/abs/2511.17071)
*Jan-Ole Koslik,Fanny Dupont,Marie Auger-Méthé,Marianne Marcoux,Nancy Heckman*

Main category: stat.ME

TL;DR: 提出了一种基于形状约束样条的方法，在HMM状态依赖密度估计中强制单峰性约束，平衡模型灵活性和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统HMM中状态依赖分布参数形式选择困难，可能导致模型误设；而完全非参数方法虽然灵活但可能产生过于复杂的密度函数（如双峰），影响可解释性

Method: 基于形状约束样条理论，通过对样条系数施加单峰性约束来强制状态依赖密度估计的单峰性

Result: 通过两个模拟研究和真实世界案例（独角鲸潜水数据）证明，该方法相比完全灵活的无约束模型产生更稳定的估计，提高了模型性能和可解释性

Conclusion: 该方法填补了方法学空白，提供了一个简约的HMM框架，平衡了参数模型的可解释性和非参数估计的灵活性，为生态学家从遥测数据中获得生态学意义推断提供了有力工具

Abstract: 1. Hidden Markov models (HMMs) are powerful tools for modelling time-series data with underlying state structure. However, selecting appropriate parametric forms for the state-dependent distributions is often challenging and can lead to model misspecification. To address this, P-spline-based nonparametric estimation of state-dependent densities has been proposed. While offering great flexibility, these approaches can result in overly complex densities (e.g. bimodal) that hinder interpretability. 2. We propose a straightforward method that builds on shape-constrained spline theory to enforce unimodality in the estimated state-dependent densities through enforcing unimodality of the spline coefficients. This constraint strikes a practical balance between model flexibility, interpretability, and parsimony. 3. Through two simulation studies and a real-world case study using narwhal (Monodon monoceros) dive data, we demonstrate the proposed approach yields more stable estimates compared to fully flexible, unconstrained models improving model performance and interpretability. 4. Our method bridges a key methodological gap, by providing a parsimonious HMM framework that balances the interpretability of parametric models with the flexibility of nonparametric estimation. This provides ecologists with a powerful tool to derive ecologically meaningful inference from telemetry data while avoiding the pitfalls of overly complex models.

</details>


### [12] [Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models](https://arxiv.org/abs/2511.17438)
*Jesse Wheeler,Aaron J. Abkemeier,Edward L. Ionides*

Main category: stat.ME

TL;DR: 提出了一种新的迭代滤波算法，通过引入边缘化步骤解决高维粒子滤波问题，使原本难以处理的动态模型能够用于面板数据分析。


<details>
  <summary>Details</summary>
Motivation: 随着面板数据的普及，传统蒙特卡洛推理方法在高维机制模型中计算不可行，需要开发适用于高维机制模型的推理算法。

Method: 引入包含边缘化步骤的迭代滤波算法，缓解高维粒子滤波中的问题。

Result: 该方法使得原本难以处理的模型能够进行基于似然的推理，扩展了可用于面板数据分析的动态模型范围。

Conclusion: 提出的算法成功解决了高维机制模型在面板数据分析中的计算难题，为复杂动态系统研究提供了有效工具。

Abstract: Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis.

</details>


### [13] [ggskewboxplots: Enhanced Boxplots for Skewed Data in R](https://arxiv.org/abs/2511.17091)
*Mustafa Cavus*

Main category: stat.ME

TL;DR: 本文评估了多种针对偏态分布的替代箱线图方法，开发了ggskewboxplots R包，通过模拟研究发现传统Tukey箱线图在偏态分布中容易误判异常值，而基于四分位数或中位数耦合的鲁棒偏态调整方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统箱线图在处理偏态或重尾分布时存在显著局限性，容易产生异常值误判问题（淹没或掩盖），需要开发能够适应分布不对称性的替代方法。

Method: 使用基于偏态指数幂分布的mosaic方法进行广泛的蒙特卡洛模拟，在控制偏度和峰度的条件下评估多种鲁棒和偏态感知箱线图变体的敏感性和特异性。

Result: 模拟结果表明传统Tukey箱线图极易出现淹没和掩盖问题，而基于四分位数偏度度量或中位数耦合调整的鲁棒偏态调整变体实现了显著更好的性能。

Conclusion: 研究结果为应用环境中选择可靠的箱线图方法提供了实用指导，并展示了ggskewboxplots包如何在熟悉的ggplot2工作流中促进可访问的分布感知可视化。

Abstract: Traditional boxplots are widely used for summarizing and visualizing the distribution of numerical data, yet they exhibit significant limitations when applied to skewed or heavy-tailed distributions, often leading to misclassification of outliers through swamping -- flagging typical observations as outliers -- or masking -- failing to detect true outliers. This paper addresses these limitations by systematically evaluating several alternative boxplots specifically designed to accommodate distributional asymmetry. We introduce ggskewboxplots, an R package that integrates multiple robust and skewness-aware boxplot variants, providing a unified and user-friendly framework for exploratory data analysis. Using extensive Monte Carlo simulations under controlled skewness and kurtosis conditions, implemented via the mosaic approach based on the Skewed Exponential Power distribution, we assess the sensitivity and specificity of each method. Simulation results indicate that classical Tukey-style boxplots are highly prone to swamping and masking, whereas robust skewness-adjusted variants -- particularly those leveraging quartile-based skewness measures or medcouple-based adjustments -- achieve substantially better performance. These findings offer practical guidance for selecting reliable boxplot methods in applied settings and demonstrate how the ggskewboxplots package facilitates accessible, distribution-aware visualizations within the familiar ggplot2 workflow.

</details>


### [14] [Extending the Accelerated Failure Conditionals Model to Location-Scale Families](https://arxiv.org/abs/2511.17463)
*Jared N. Lakhani*

Main category: stat.ME

TL;DR: 本文提出了一种基于位置-尺度分布族的条件生存函数规范方法，通过加速函数和位置函数建立X和Y之间的依赖关系，并以Weibull边际为例进行说明。


<details>
  <summary>Details</summary>
Motivation: 扩展Arnold和Arvanitis(2020)的条件分布模型，从仅通过生存函数建立依赖关系扩展到包含位置函数的更一般框架，以处理更广泛的分布类型。

Method: 采用位置-尺度分布族构建条件生存函数规范，依赖关系通过加速函数和位置函数共同建立，使用Metropolis-Hastings算法进行模拟。

Result: 模型具有闭式矩表达式，能够处理定义在实数域上的Y边际分布，并通过实际数据集验证了模型的适用性。

Conclusion: 提出的条件生存函数规范方法成功扩展了现有模型，能够处理更一般的分布类型，并为实际数据分析提供了有效工具。

Abstract: Arnold and Arvanitis (2020) introduced a novel class of bivariate conditionally specified distributions, in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This conditioning regime was formulated through survival functions and termed the accelerated failure conditionals model. Subsequently, Lakhani (2025) extended this conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, thereby moving beyond families with non-increasing densities. The present study builds on this line of work by proposing a conditional survival specification derived from a location-scale distributional family, where the dependence between $X$ and $Y$ arises not only through the acceleration function but also via a location function. An illustrative example of this new specification is developed using a Weibull marginal for $X$. The resulting models are fully characterized by closed-form expressions for their moments, and simulations are implemented using the Metropolis-Hastings algorithm. Finally, the model is applied to a dataset in which the empirical distribution of $Y$ lies on the real line, demonstrating the models' capacity to accommodate $Y$ marginals defined over $\mathbb{R}$.

</details>


### [15] [Nonparametric Inference for Extreme CoVaR and CoES](https://arxiv.org/abs/2511.17180)
*Qingzhao Zhong,Yanxi Hou*

Main category: stat.ME

TL;DR: 本文提出了几种基于上尾依赖框架的非参数外推方法来估计极端条件风险价值(CoVaR)和条件期望损失(CoES)，解决了现有方法在高风险水平下统计推断和渐近理论的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的CoES估计方法在统计推断和渐近理论方面面临挑战，特别是在高风险水平下。CoVaR和CoES作为系统性风险度量的重要指标，在经济学和金融学中应用日益广泛，需要更有效的估计方法。

Method: 在尾部依赖框架下，通过调整因子非参数地估计极端CoVaR和CoES，与尾部依赖函数的非参数建模密切相关。基于多元极值理论研究所有提出外推方法的渐近理论。

Result: 通过模拟和真实数据分析验证了所提出方法的实证表现。

Conclusion: 提出的非参数外推方法能够有效估计极端CoVaR和CoES，为解决高风险水平下的系统性风险度量问题提供了可行的解决方案。

Abstract: Systemic risk measures quantify the potential risk to an individual financial constituent arising from the distress of entire financial system. As a generalization of two widely applied risk measures, Value-at-Risk and Expected Shortfall, the Conditional Value-at-Risk (CoVaR) and Conditional Expected Shortfall (CoES) have recently been receiving growing attention on applications in economics and finance, since they serve as crucial metrics for systemic risk measurement. However, existing approaches confront some challenges in statistical inference and asymptotic theories when estimating CoES, particularly at high risk levels. In this paper, within a framework of upper tail dependence, we propose several extrapolative methods to estimate both extreme CoVaR and CoES nonparametrically via an adjustment factor, which are intimately related to the nonparametric modelling of the tail dependence function. In addition, we study the asymptotic theories of all proposed extrapolative methods based on multivariate extreme value theory. Finally, some simulations and real data analyses are conducted to demonstrate the empirical performances of our methods.

</details>


### [16] [Covariate Connectivity Combined Clustering for Weighted Networks](https://arxiv.org/abs/2511.17302)
*Zeyu Hu,Wenrui Li,Jun Yan,Panpan Zhang*

Main category: stat.ME

TL;DR: 提出C⁴算法，一种结合网络连接性和节点属性的自适应谱聚类方法，用于社区检测，特别适用于加权网络且无需预先指定聚类数量。


<details>
  <summary>Details</summary>
Motivation: 传统社区检测算法仅依赖网络拓扑结构，当社区信号部分编码在节点属性中时会失效。现有协变量辅助方法通常需要已知聚类数量、计算复杂或不适用于加权网络。

Method: C⁴算法将网络连接性和节点级协变量整合到统一的相似性表示中，通过数据驱动的调参平衡两种信息源，使用特征间隙启发式估计社区数量，避免依赖昂贵的基于采样的过程。

Result: 模拟研究显示C⁴在不同场景下比竞争方法具有更高的准确性和鲁棒性。在机场可达性网络中的应用证明了该方法对现实世界加权网络的可扩展性、可解释性和实用性。

Conclusion: C⁴算法提供了一种有效整合网络拓扑和节点属性的社区检测方法，特别适用于加权网络，且无需预先指定聚类数量，具有实际应用价值。

Abstract: Community detection is a central task in network analysis, with applications in social, biological, and technological systems. Traditional algorithms rely primarily on network topology, which can fail when community signals are partly encoded in node-specific attributes. Existing covariate-assisted methods often assume the number of clusters is known, involve computationally intensive inference, or are not designed for weighted networks. We propose $\text{C}^4$: Covariate Connectivity Combined Clustering, an adaptive spectral clustering algorithm that integrates network connectivity and node-level covariates into a unified similarity representation. $\text{C}^4$ balances the two sources of information through a data-driven tuning parameter, estimates the number of communities via an eigengap heuristic, and avoids reliance on costly sampling-based procedures. Simulation studies show that $\text{C}^4$ achieves higher accuracy and robustness than competing approaches across diverse scenarios. Application to an airport reachability network demonstrates the method's scalability, interpretability, and practical utility for real-world weighted networks.

</details>


### [17] [The Experimental Unit Information Index: Balancing Evidentiary Value and Sample Size of Adaptive Designs](https://arxiv.org/abs/2511.17292)
*Leonhard Held,Fadoua Balabdaoui,Samuel Pawel*

Main category: stat.ME

TL;DR: 本文提出了一种新的实验单元信息指数（EUII），用于量化单个实验单元在给定研究设计中的证据价值，该指数基于统计功效、I类错误和样本大小，在控制统计错误率的同时支持动物研究中的3R原则（减少实验单元数量）。


<details>
  <summary>Details</summary>
Motivation: 在动物研究中遵循3R原则（替代、减少、优化）需要减少实验单元数量，同时需要控制统计错误率以确保可靠推断。现有方法缺乏量化单个实验单元证据价值的指标。

Method: 提出实验单元信息指数（EUII），该指数基于统计功效、I类错误和样本大小，具有频率主义错误率和贝叶斯后验概率的双重解释。首先在简单统计检验设置中引入EUII，然后扩展到允许因有效或无效而提前停止的自适应设计。

Result: EUII的渐近值仅取决于备择假设下的相对效应大小。在组序贯设计和最近提出的自适应统计检验程序中的应用表明，该方法在最大化单个实验单元证据价值方面具有实用性。

Conclusion: EUII为量化实验单元的证据价值提供了一个有用的框架，特别适用于需要在减少实验单元数量的同时保持统计可靠性的研究设计，如自适应临床试验和动物研究。

Abstract: Reducing the number of experimental units is one of the three pillars of the 3R principles (Replace, Reduce, Refine) in animal research. At the same time, statistical error rates need to be controlled to enable reliable inferences and decisions. This paper proposes a novel measure to quantify the evidentiary value of one experimental unit for a given study design. The experimental unit information index (EUII) is based on power, Type-I error and sample size, and has attractive interpretations both in terms of frequentist error rates and Bayesian posterior odds. We introduce the EUII in simple statistical test settings and show that its asymptotic value depends only on the assumed relative effect size under the alternative. We then extend the definition to adaptive designs where early stopping for efficacy or futility may cause reductions in sample size. Applications to group-sequential designs and a recently proposed adaptive statistical test procedure show the usefulness of the approach when the goal is to maximize the evidentiary value of one experimental unit.

</details>


### [18] [U-DESPE: a Bayesian Utility-based methodology for dosing regimen optimization in early-phase oncology trials based on Dose-Exposure, Safety, Pharmacodynamics, Efficacy](https://arxiv.org/abs/2511.17376)
*Anaïs Andrillon,Sandrine Micallef,Moreno Ursino,Pavel Mozgunov,Marie-Karelle Riviere*

Main category: stat.ME

TL;DR: U-DESPE是一种贝叶斯剂量寻找设计，通过剂量-暴露模型和暴露-终点关系模型来优化肿瘤药物剂量方案，平衡安全性、有效性和药效学终点。


<details>
  <summary>Details</summary>
Motivation: 随着分子靶向药物和免疫疗法的发展，传统"剂量越高越好"的最大耐受剂量范式不再适用，需要重新评估肿瘤药物的剂量寻找方法。

Method: 基于药代动力学数据构建剂量-暴露模型，然后建立三个模型评估暴露与安全性、有效性和药效学终点之间的关系，最后通过效用函数量化这些终点的权衡来确定最佳剂量方案。

Result: 通过在临床试验案例研究和广泛的模拟研究中应用该方法，评估了该方法的操作特性。

Conclusion: U-DESPE设计能够有效确定最佳给药方案，或在剂量递增阶段后通过随机化患者到候选最佳剂量方案来优化治疗。

Abstract: With the development of novel therapies such as molecularly targeted agents and immunotherapy, the maximum tolerated dose paradigm that "more is better" does not necessarily hold anymore. In this context, doses and schedules of novel therapies may be inadequately characterized and oncology drug dose-finding approaches should be revised. This is increasingly recognized by health authorities, notably through the Optimus project. We developed a Bayesian dose-finding design, called U-DESPE, which allows to either determine the optimal dosing regimen at the end of the dose-escalation phase, or use of dedicated cohorts for randomizing patients to candidate optimal dosing regimens after that safe dosing regimens have been found. U-DESPE design relies on a dose-exposure model built from pharmacokinetic data using non-linear mixed-effect modeling approaches. Then three models are built to assess the relationships between exposure and the probability of selected relevant endpoints on safety, efficacy, and pharmacodynamics. These models are then combined to predict the different endpoints for every candidate dosing regimens. Finally, a utility function is proposed to quantify the trade-off between these endpoints and to determine the optimal dosing regimen. We applied the proposed method on a clinical trial case study and performed an extensive simulation study to evaluate the operating characteristics of the method.

</details>


### [19] [Bayesian Bridge Gaussian Process Regression](https://arxiv.org/abs/2511.17415)
*Minshen Xu,Shiwei Lan,Lulu Kang*

Main category: stat.ME

TL;DR: 提出了贝叶斯桥高斯过程回归(B²GPR)模型，通过ℓq范数约束实现变量选择，解决高维GP回归中的维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归在高维问题中面临计算成本高和预测能力下降的维度灾难问题，变量选择对于构建高效准确的GP模型至关重要。

Method: 受贝叶斯桥回归启发，在关键GP参数上施加ℓq范数约束来自动诱导稀疏性和识别活跃变量。提出两个版本：q=2时使用共轭高斯先验，0<q<2时使用约束平坦先验。设计了结合球形哈密顿蒙特卡洛的Gibbs采样算法进行后验推断。

Result: 模拟和真实数据应用证实B²GPR在变量选择和预测方面优于其他方法。

Conclusion: B²GPR框架通过ℓq范数约束有效解决了高维GP回归中的维度灾难问题，提供了优越的变量选择和预测性能。

Abstract: The performance of Gaussian Process (GP) regression is often hampered by the curse of dimensionality, which inflates computational cost and reduces predictive power in high-dimensional problems. Variable selection is thus crucial for building efficient and accurate GP models. Inspired by Bayesian bridge regression, we propose the Bayesian Bridge Gaussian Process Regression (B\textsuperscript{2}GPR) model. This framework places $\ell_q$-norm constraints on key GP parameters to automatically induce sparsity and identify active variables. We formulate two distinct versions: one for $q=2$ using conjugate Gaussian priors, and another for $0<q<2$ that employs constrained flat priors, leading to non-standard, norm-constrained posterior distributions. To enable posterior inference, we design a Gibbs sampling algorithm that integrates Spherical Hamiltonian Monte Carlo (SphHMC) to efficiently sample from the constrained posteriors when $0<q<2$. Simulations and a real-data application confirm that B\textsuperscript{2}GPR offers superior variable selection and prediction compared to alternative approaches.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [20] [Modified Delayed Acceptance MCMC for Quasi-Bayesian Inference with Linear Moment Conditions](https://arxiv.org/abs/2511.17117)
*Masahiro Tanaka*

Main category: stat.CO

TL;DR: 提出了基于线性矩条件的准贝叶斯推断计算框架，使用延迟接受MCMC算法，包含Exact和Approx两种实现，在模拟和实证应用中均优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 为解决准贝叶斯推断中的计算效率问题，特别是在高维情况下传统MCMC方法计算成本高的问题。

Method: 使用延迟接受MCMC算法，利用代理目标核和基于近似条件后验的提议分布，开发了DA-MCMC-Exact和DA-MCMC-Approx两种实现。

Result: 模拟研究显示在异方差线性回归中，相比标准MCMC和传统DA-MCMC，多变量有效样本量每迭代和每秒均有显著提升。Approx版本整体吞吐量最佳，Exact版本每迭代效率最高。

Conclusion: 提出的算法为准贝叶斯分析提供了实用且稳健的工具，特别适用于高维设计，并能扩展到风险基础的准贝叶斯公式。

Abstract: We develop a computationally efficient framework for quasi-Bayesian inference based on linear moment conditions. The approach employs a delayed acceptance Markov chain Monte Carlo (DA-MCMC) algorithm that uses a surrogate target kernel and a proposal distribution derived from an approximate conditional posterior, thereby exploiting the structure of the quasi-likelihood. Two implementations are introduced. DA-MCMC-Exact fully incorporates prior information into the proposal distribution and maximizes per-iteration efficiency, whereas DA-MCMC-Approx omits the prior in the proposal to reduce matrix inversions, improving numerical stability and computational speed in higher dimensions. Simulation studies on heteroskedastic linear regressions show substantial gains over standard MCMC and conventional DA-MCMC baselines, measured by multivariate effective sample size per iteration and per second. The Approx variant yields the best overall throughput, while the Exact variant attains the highest per-iteration efficiency. Applications to two empirical instrumental variable regressions corroborate these findings: the Approx implementation scales to larger designs where other methods become impractical, while still delivering precise inference. Although developed for moment-based quasi-posteriors, the proposed approach also extends to risk-based quasi-Bayesian formulations when first-order conditions are linear and can be transformed analogously. Overall, the proposed algorithms provide a practical and robust tool for quasi-Bayesian analysis in statistical applications.

</details>
