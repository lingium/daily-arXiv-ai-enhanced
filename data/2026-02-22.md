<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [stat.ME](#stat.ME) [Total: 5]
- [stat.AP](#stat.AP) [Total: 1]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models](https://arxiv.org/abs/2602.17414)
*David Yallup*

Main category: stat.CO

TL;DR: NS-SwiG是一种用于高维贝叶斯推断和证据估计的算法，特别适用于似然函数可分解的模型（如分层贝叶斯模型），通过切片-吉布斯采样和似然预算分解实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 高维贝叶斯推断中，传统嵌套采样方法在计算似然约束先验时成本高昂，特别是对于具有可分解似然函数的分层模型，需要更高效的算法来处理大规模参数空间。

Method: 使用切片-吉布斯核采样似然约束先验：外层更新超参数，内层分块更新局部参数。通过似然预算分解缓存每块贡献，使局部更新以常数时间检查可行性，而非线性增长的全局约束计算。

Result: 将每次替换成本从组数的二次方降低到线性，整体算法复杂度在标准假设下从三次方降低到二次方。算法可扩展到数千维度，在梯度采样器可能失效的后验几何上仍能提供准确的证据估计。

Conclusion: NS-SwiG为高维分层贝叶斯模型提供了一种高效、可扩展的推断方法，通过创新的计算分解策略显著降低了计算复杂度，并在挑战性基准测试中表现出色。

Abstract: We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [2] [Anti-causal domain generalization: Leveraging unlabeled data](https://arxiv.org/abs/2602.17187)
*Sorawit Saengkyongam,Juan L. Gamella,Andrew C. Miller,Jonas Peters,Nicolai Meinshausen,Christina Heinze-Deml*

Main category: stat.ML

TL;DR: 本文研究反因果设定下的领域泛化问题，提出利用无标签数据通过正则化模型对协变量扰动的敏感性来提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法通常需要多个训练环境的标签数据，这在标签数据稀缺时限制了应用。本文研究反因果设定下的领域泛化，其中结果变量导致观测协变量，这种结构使得环境扰动影响协变量但不会传播到结果变量。

Method: 提出两种方法：分别惩罚模型对协变量均值和协方差跨环境变化的敏感性。这些方法的关键优势是估计扰动方向不需要标签，因此可以利用多个环境的无标签数据。

Result: 证明了这些方法在特定环境类别下具有最坏情况最优性保证。在受控物理系统和生理信号数据集上展示了方法的实证性能。

Conclusion: 本文提出了在反因果设定下利用无标签数据进行领域泛化的新方法，通过正则化模型对协变量扰动的敏感性，在标签稀缺时仍能实现有效的泛化性能。

Abstract: The problem of domain generalization concerns learning predictive models that are robust to distribution shifts when deployed in new, previously unseen environments. Existing methods typically require labeled data from multiple training environments, limiting their applicability when labeled data are scarce. In this work, we study domain generalization in an anti-causal setting, where the outcome causes the observed covariates. Under this structure, environment perturbations that affect the covariates do not propagate to the outcome, which motivates regularizing the model's sensitivity to these perturbations. Crucially, estimating these perturbation directions does not require labels, enabling us to leverage unlabeled data from multiple environments. We propose two methods that penalize the model's sensitivity to variations in the mean and covariance of the covariates across environments, respectively, and prove that these methods have worst-case optimality guarantees under certain classes of environments. Finally, we demonstrate the empirical performance of our approach on a controlled physical system and a physiological signal dataset.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [A statistical perspective on transformers for small longitudinal cohort data](https://arxiv.org/abs/2602.16914)
*Kiana Farhadyar,Maren Hackenberg,Kira Ahrens,Charlotte Schenk,Bianca Kollmann,Oliver Tüscher,Klaus Lieb,Michael M. Plichta,Andreas Reif,Raffael Kalisch,Martin Wolkewitz,Moritz Hess,Harald Binder*

Main category: stat.ME

TL;DR: 提出一种简化的Transformer架构，适用于小规模纵向队列数据，通过注意力机制识别时间依赖模式，在保持预测性能的同时减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer需要大数据集，而纵向队列数据通常规模小、时间点少，需要简化架构来适应这种数据特点，同时保留注意力机制的核心优势。

Method: 基于统计视角简化Transformer：以自回归模型为基础，引入基于核函数的时间衰减注意力机制，通过多头注意力聚合不同权重方案，并开发基于排列的统计检验方法识别上下文模式。

Result: 模拟研究显示该方法能在少量个体和时间点下恢复上下文依赖；应用于韧性研究数据时，成功识别了压力与心理健康动态中的时间模式。

Conclusion: 适当简化的Transformer不仅能在小数据场景中保持竞争力预测性能，还能揭示复杂的上下文依赖关系，为纵向数据分析提供了新工具。

Abstract: Modeling of longitudinal cohort data typically involves complex temporal dependencies between multiple variables. There, the transformer architecture, which has been highly successful in language and vision applications, allows us to account for the fact that the most recently observed time points in an individual's history may not always be the most important for the immediate future. This is achieved by assigning attention weights to observations of an individual based on a transformation of their values. One reason why these ideas have not yet been fully leveraged for longitudinal cohort data is that typically, large datasets are required. Therefore, we present a simplified transformer architecture that retains the core attention mechanism while reducing the number of parameters to be estimated, to be more suitable for small datasets with few time points. Guided by a statistical perspective on transformers, we use an autoregressive model as a starting point and incorporate attention as a kernel-based operation with temporal decay, where aggregation of multiple transformer heads, i.e. different candidate weighting schemes, is expressed as accumulating evidence on different types of underlying characteristics of individuals. This also enables a permutation-based statistical testing procedure for identifying contextual patterns. In a simulation study, the approach is shown to recover contextual dependencies even with a small number of individuals and time points. In an application to data from a resilience study, we identify temporal patterns in the dynamics of stress and mental health. This indicates that properly adapted transformers can not only achieve competitive predictive performance, but also uncover complex context dependencies in small data settings.

</details>


### [4] [Generative modeling for the bootstrap](https://arxiv.org/abs/2602.17052)
*Leon Tran,Ting Ye,Peng Ding,Fang Han*

Main category: stat.ME

TL;DR: 提出基于生成建模的bootstrap方法，能产生统计有效的置信区间，适用于常规和非常规估计器，在Efron bootstrap失效的场景中仍能工作


<details>
  <summary>Details</summary>
Motivation: 传统bootstrap方法（如Efron bootstrap）在某些情况下会失效，特别是在估计器缺乏根号n一致性或高斯极限的挑战性场景中。需要一种更稳健的bootstrap方法，既能处理常规估计器，也能处理不规则估计器。

Method: 采用生成建模方法构建bootstrap，这可以看作是平滑bootstrap的现代版本。通过生成合成数据来模拟观测样本，利用生成模型缓解维度诅咒问题。

Result: 该方法能产生统计有效的置信区间，适用于同时处理常规和非常规估计器，在Efron bootstrap失效的场景中仍然有效，特别是在估计器缺乏根号n一致性或高斯极限的挑战性情况下。

Conclusion: 基于生成建模的bootstrap方法为统计推断提供了理论完善且实用的工具，能够克服传统bootstrap的局限性，在复杂和高维场景中保持有效性，代表了平滑bootstrap思想的现代化发展。

Abstract: Generative modeling builds on and substantially advances the classical idea of simulating synthetic data from observed samples. This paper shows that this principle is not only natural but also theoretically well-founded for bootstrap inference: it yields statistically valid confidence intervals that apply simultaneously to both regular and irregular estimators, including settings in which Efron's bootstrap fails. In this sense, the generative modeling-based bootstrap can be viewed as a modern version of the smoothed bootstrap: it could mitigate the curse of dimensionality and remain effective in challenging regimes where estimators may lack root-$n$ consistency or a Gaussian limit.

</details>


### [5] [Dynamic likelihood hazard rate estimation](https://arxiv.org/abs/2602.17161)
*Nils Lid Hjort*

Main category: stat.ME

TL;DR: 本文提出了一种半参数风险率估计方法，结合参数和非参数方法的优点，通过动态局部似然在给定参数类中拟合局部最合适的风险率函数。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析中的风险率函数估计方法要么是完全参数化的（可能偏差过大），要么是完全非参数化的（可能方差过大）。需要一种能结合两者优点的半参数方法。

Method: 采用Hjort（1991）提出的半参数方法，使用动态局部似然方法在给定的参数风险率类中拟合局部最合适的成员，本质上是在参数类内进行非参数参数平滑。

Result: 动态似然估计通常比纯非参数方法表现更好，同时在模型本身合适的情况下，与参数方法相比损失不大。研究了估计量的偏差和方差特性以及局部平滑参数选择方法。

Conclusion: 提出的半参数方法有效地结合了参数和非参数方法的优点，在减少偏差的同时控制方差，为风险率估计提供了更平衡的解决方案。

Abstract: The best known methods for estimating hazard rate functions in survival analysis models are either purely parametric or purely nonparametric. The parametric ones are sometimes too biased while the nonparametric ones are sometimes too variable. In the present paper a certain semiparametric approach to hazard rate estimation, proposed in Hjort (1991), is developed further, aiming to combine parametric and nonparametric features. It uses a dynamic local likelihood approach to fit the locally most suitable member in a given parametric class of hazard rates, and amounts to a version of nonparametric parameter smoothing within the parametric class. Thus the parametric hazard rate estimate at time $s$ inserts a parameter estimate that also depends on $s$. We study bias and variance properties of the resulting estimator and methods for choosing the local smoothing parameter. It is shown that dynamic likelihood estimation often leads to better performance than the purely nonparametric methods, while also having capacity for not losing much to the parametric methods in cases where the model being smoothed is adequate.

</details>


### [6] [Selection and Collider Restriction Bias Due to Predictor Availability in Prognostic Models](https://arxiv.org/abs/2602.17255)
*Marc Delord*

Main category: stat.ME

TL;DR: 探讨预后模型中预测变量可用性可能导致的样本选择和碰撞器限制偏倚


<details>
  <summary>Details</summary>
Motivation: 在预后模型开发中，预测变量的可用性可能导致样本选择偏倚和碰撞器限制偏倚，影响模型的准确性和泛化能力

Method: 方法学分析，探讨预测变量可用性如何通过样本选择和碰撞器限制机制引入偏倚

Result: 识别了预测变量可用性可能导致的两种偏倚机制，并讨论了其对预后模型的影响

Conclusion: 预后模型开发中需要考虑预测变量可用性带来的偏倚问题，以确保模型的准确性和可靠性

Abstract: This methodological note investigates and discuss possible selection and collider restriction bias due to predictor availability in prognostic models.

</details>


### [7] [Estimating Zero-inflated Negative Binomial GAMLSS via a Balanced Gradient Boosting Approach with an Application to Antenatal Care Data from Nigeria](https://arxiv.org/abs/2602.17272)
*Alexandra Daub,Elisabeth Bergherr*

Main category: stat.ME

TL;DR: 该论文提出了一种改进的GAMLSS boosting方法，通过引入收缩最优步长来解决参数更新不平衡和计算时间长的问题，并在尼日利亚产前护理访问次数的应用中进行验证。


<details>
  <summary>Details</summary>
Motivation: 统计boosting算法在复杂模型如GAMLSS中具有优势，但存在分布参数更新不平衡和计算时间长的问题。收缩最优步长已被证明能解决这些问题，但需要扩展到更复杂的基学习器和响应变量分布。

Method: 将收缩最优步长的GAMLSS boosting方法推广到超越简单线性模型的基学习器，并应用于更复杂的响应变量分布。通过广泛的模拟研究和实际应用来验证方法的有效性。

Result: 收缩最优步长能够在不同设置下实现更平衡的整体模型正则化，特别是在存在惩罚拟合大小的基学习器时，显著提高了计算效率。

Conclusion: 收缩最优步长是改进GAMLSS boosting的有效方法，能够解决参数更新不平衡和计算效率问题，特别适用于复杂基学习器和响应分布的实际应用场景。

Abstract: Statistical boosting algorithms are renowned for their intrinsic variable selection and enhanced predictive performance compared to classical statistical methods, making them especially useful for complex models such as generalized additive models for location scale and shape (GAMLSS). Boosting this model class can suffer from imbalanced updates across the distribution parameters as well as long computation times. Shrunk optimal step lengths have been shown to address these issues. To examine the influence of socio-economic factors on the distribution of the number of antenatal care visits in Nigeria, we generalize boosting of GAMLSS with shrunk optimal step lengths to base-learners beyond simple linear models and to a more complex response variable distribution. In an extensive simulation study and in the application we demonstrate that shrunk optimal step lengths yield a more balanced regularization of the overall model and enhance computational efficiency across diverse settings, in particular in the presence of base-learners penalizing the size of the fit.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [8] [Environmental policy in the context of complex systems: Statistical optimization and sensitivity analysis for ABMs](https://arxiv.org/abs/2602.17079)
*Dylan Munson,Arijit Dey,Simon Mak*

Main category: stat.AP

TL;DR: 提出结合机器学习与强化学习的统计框架，加速基于智能体模型的政策优化


<details>
  <summary>Details</summary>
Motivation: 人类-环境耦合系统是复杂适应系统，基于智能体模型能捕捉其复杂行为，但计算成本高，阻碍政策优化应用

Method: 开发统计框架：1) 最优政策敏感性测试的统计方法；2) 利用强化学习方法进行高效政策优化；在经典"糖域"资源收获模型上测试

Result: 方法能快速识别最优且可解释的政策，优于基线技术，提供有洞察力的敏感性分析和动态分析，与经济理论相连接

Conclusion: 提出的机器学习增强框架能有效加速基于智能体模型的政策优化，为复杂适应系统的政策设计提供实用工具

Abstract: Coupled human-environment systems are increasingly being understood as complex adaptive systems (CAS), in which micro-level interactions between components lead to emergent behavior. Agent-based models (ABMs) hold great promise for environmental policy design by capturing such complex behavior, enabling a sophisticated understanding of potential interventions. One limitation, however, is that ABMs can be computationally costly to simulate, which hinders their use for policy optimization. To address this, we propose a new statistical framework that exploits machine learning techniques to accelerate policy optimization with costly ABMs. We first develop a statistical approach for sensitivity testing of the optimal policy, then leverage a reinforcement learning method for efficient policy optimization. We test this framework on the classic ``Sugarscape'' model, an ABM for resource harvesting. We show that our approach can quickly identify optimal and interpretable policies that improve upon baseline techniques, with insightful sensitivity and dynamic analyses that connect back to economic theory.

</details>
