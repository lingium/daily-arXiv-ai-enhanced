<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 16]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ML](#stat.ML) [Total: 20]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Modelling non-stationary extremal dependence through a geometric approach](https://arxiv.org/abs/2509.22501)
*C. J. R. Murphy-Barltrop,J. L. Wadsworth,M. de Carvalho,B. D. Youngman*

Main category: stat.ME

TL;DR: 本文提出了一种非平稳极值依赖的几何建模框架，通过扩展几何框架到非平稳设置，建立了半参数建模方法来估计极限集，并在金融回报数据中展示了实用价值。


<details>
  <summary>Details</summary>
Motivation: 许多环境和金融数据集中的极值依赖关系随时间变化，但现有多变量极值模型主要适用于平稳数据，需要开发能够处理非平稳极值依赖的方法。

Method: 扩展几何框架到非平稳设置，提出半参数建模框架来估计极限集，通过尺度样本云的极限形状推断极值依赖特征。

Result: 模拟研究表明该框架能够捕捉多种依赖形式，对不同模型设定具有鲁棒性，在金融回报数据应用中展示了实用价值。

Conclusion: 提出的非平稳几何框架为分析非平稳极值依赖提供了灵活有效的工具，在金融等领域具有重要应用前景。

Abstract: Non-stationary extremal dependence, whereby the relationship between the
extremes of multiple variables evolves over time, is commonly observed in many
environmental and financial data sets. However, most multivariate extreme value
models are only suited to stationary data. A recent approach to multivariate
extreme value modelling uses a geometric framework, whereby extremal dependence
features are inferred through the limiting shapes of scaled sample clouds. This
framework can capture a wide range of dependence structures, and a variety of
inference procedures have been proposed in the stationary setting. In this
work, we first extend the geometric framework to the non-stationary setting and
outline assumptions to ensure the necessary convergence conditions hold. We
then introduce a flexible, semi-parametric modelling framework for obtaining
estimates of limit sets in the non-stationary setting. Through rigorous
simulation studies, we demonstrate that our proposed framework can capture a
wide range of dependence forms and is robust to different model formulations.
We illustrate the proposed methods on financial returns data and present
several practical uses.

</details>


### [2] [Modeling discrete lattice data using the Potts and tapered Potts models](https://arxiv.org/abs/2509.21478)
*Maria Paula Duenas-Herrera,Stephen Berg,Murali Haran*

Main category: stat.ME

TL;DR: 本文开发了Ising和Potts模型的锥化版本，解决了模型拟合不佳和相变/基态问题，并提出了高效的MCMCMLE算法来处理归一化常数问题。


<details>
  <summary>Details</summary>
Motivation: Ising和Potts模型在统计物理中广泛应用，但存在三个主要问题：模型拟合不佳、相变和基态带来的统计推断挑战、以及归一化常数导致的计算困难。

Method: 开发了锥化版本的Ising和Potts模型，并设计了高效的马尔可夫链蒙特卡罗最大似然估计(MCMCMLE)算法。

Result: 通过广泛的模拟研究，提供了关于相变和基态问题的见解，并在模拟数据和2021年国家土地覆盖数据库数据上验证了方法的有效性。

Conclusion: 为建模和计算提供了实用建议，锥化模型能有效解决传统Ising和Potts模型的关键问题。

Abstract: The Ising and Potts models, among the most important models in statistical
physics, have been used for modeling binary and multinomial data on lattices in
a wide variety of disciplines such as psychology, image analysis, biology, and
forestry. However, these models have several well known shortcomings: (i) they
can result in poorly fitting models, that is, simulations from fitted models
often do not produce realizations that look like the observed data; (ii) phase
transitions and the presence of ground states introduce significant challenges
for statistical inference, model interpretation, and goodness of fit; (iii)
intractable normalizing constants that are functions of the model parameters
pose serious computational problems for likelihood-based inference.
  Here we develop a tapered version of the Ising and Potts models that
addresses issues (i) and (ii). We develop efficient Markov Chain Monte Carlo
Maximum Likelihood Estimation (MCMCMLE) algorithms that address issue (iii). We
perform an extensive simulation study for the classical and Tapered Potts
models that provide insights regarding the issues generated by the phase
transition and ground states. Finally, we offer practical recommendations for
modeling and computation based on applications of our approach to simulated
data as well as data from the 2021 National Land Cover Database.

</details>


### [3] [A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse Problem](https://arxiv.org/abs/2509.22597)
*Haiyi Shi,Lei Yang,Jiarui Chi,Troy Butler,Haonan Wang,Derek Bingham,Don Estep*

Main category: stat.ME

TL;DR: 提出了一个非参数贝叶斯方法来解决随机逆问题，证明了关键性质，分析了计算解的收敛性和误差，并通过多个应用验证了结果。


<details>
  <summary>Details</summary>
Motivation: 随机逆问题是复杂科学和工程系统中进行推断、预测和决策的关键要素，需要有效的解决方案。

Method: 采用非参数贝叶斯方法，通过随机采样获得计算解，并分析其收敛性和误差。

Result: 证明了解决方案的关键性质，分析了计算解的收敛性和误差，并通过多个应用验证了方法的有效性。

Conclusion: 所提出的非参数贝叶斯方法为随机逆问题提供了有效的解决方案，具有良好的理论性质和实际应用价值。

Abstract: The stochastic inverse problem is a key ingredient in making inferences,
predictions, and decisions for complex science and engineering systems. We
formulate and analyze a nonparametric Bayesian solution for the stochastic
inverse problem. Key properties of the solution are proved and the convergence
and error of a computational solution obtained by random sampling is analyzed.
Several applications illustrate the results.

</details>


### [4] [Estimating average treatment effects when treatment data are absent in a target study](https://arxiv.org/abs/2509.22543)
*Lan Wen,Aaron L Sarvet*

Main category: stat.ME

TL;DR: 本文提出了一种数据整合方法，用于在观察性数据缺乏治疗信息的情况下，通过外部数据集来估计治疗干预的反事实平均结果。


<details>
  <summary>Details</summary>
Motivation: 研究者经常需要理解治疗干预的因果效应，但在观察性数据中，感兴趣的治疗要么未被直接测量，要么完全不可用。这促使开发了随机增量倾向评分干预方法，但关键挑战在于治疗后暴露的分布变化通常是未知的。

Method: 探索数据整合方法，通过外部数据集描述治疗后暴露的分布，并使用这些信息来估计治疗干预下的反事实平均结果。讨论了所需的基本假设，并提供了估计策略的方法指导。

Result: 提出了一种在观察性数据缺乏治疗信息、外部数据可能不包含感兴趣结果测量的情况下，估计治疗干预反事实结果的方法框架。

Conclusion: 通过数据整合方法可以解决观察性数据中治疗信息缺失的问题，为估计治疗干预的因果效应提供了可行的解决方案。

Abstract: Researchers are frequently interested in understanding the causal effect of
treatment interventions. However, in some cases, the treatment of
interest--readily available in a randomized controlled trial (RCT)--is either
not directly measured or entirely unavailable in observational datasets. This
challenge has motivated the development of stochastic incremental propensity
score interventions which operate on post-treatment exposures affected by the
treatment of interest with the aim of approximating the causal effects of the
treatment intervention. Yet, a key challenge lies in the fact that the precise
distributional shift of these post-treatment exposures induced by the treatment
is typically unknown, making it uncertain whether the approximation truly
reflects the causal effect of interest. The primary objective of this paper is
to explore data integration methodologies to characterize a distribution of
post-treatment exposures resulting from the treatment in an external dataset,
and to use this information to estimate counterfactual mean outcomes under
treatment interventions, in settings where the observational data lack
treatment information and the external data may not contain measurements of the
outcome of interest. We will discuss the underlying assumptions required for
this approach and provide methodological guidance on estimation strategies to
address these challenges.

</details>


### [5] [Conditional predictive inference with $L^k$-coverage control](https://arxiv.org/abs/2509.21691)
*Yonghoon Lee,Zhimei Ren*

Main category: stat.ME

TL;DR: 本文提出了一个可实现的分布自由条件预测推断的放松版本，通过控制条件覆盖率的函数范数（特别是L²范数）来提供精确的有限样本控制。


<details>
  <summary>Details</summary>
Motivation: 由于在无分布假设下实现精确有限样本条件覆盖控制是不可能的，现有方法只能提供近似保证或解释性较差的结果，因此需要一种既能提供精确分布自由有限样本控制又具有直观解释的放松条件预测推断方法。

Method: 将条件覆盖率视为函数而非标量，控制其函数范数（主要关注L²范数），提出一个程序来控制放松条件覆盖率的L^k范数，适应不同的超参数选择方法（如局部条件覆盖、平滑条件覆盖或扰动样本的条件覆盖）。

Result: 通过模拟和真实数据集实验验证了所提程序作为条件预测推断工具的性能表现。

Conclusion: 该方法在保持分布自由有限样本精确控制的同时，提供了直观的解释，是条件预测推断的一个可行替代方案。

Abstract: We consider the problem of distribution-free conditional predictive
inference. Prior work has established that achieving exact finite-sample
control of conditional coverage without distributional assumptions is
impossible, in the sense that it necessarily results in trivial prediction
sets. While several lines of work have proposed methods targeting relaxed
notions of conditional coverage guarantee, the inherent difficulty of the
problem typically leads such methods to offer only approximate guarantees or
yield less direct interpretations, even with the relaxations. In this work, we
propose an inferential target as a relaxed version of conditional predictive
inference that is achievable with exact distribution-free finite sample
control, while also offering intuitive interpretations. One of the key ideas,
though simple, is to view conditional coverage as a function rather than a
scalar, and thereby aim to control its function norm. We propose a procedure
that controls the $L^k$-norm -- while primarily focusing on the $L^2$-norm --
of a relaxed notion of conditional coverage, adapting to different approaches
depending on the choice of hyperparameter (e.g., local-conditional coverage,
smoothed conditional coverage, or conditional coverage for a perturbed sample).
We illustrate the performance of our procedure as a tool for conditional
predictive inference, through simulations and experiments on a real dataset.

</details>


### [6] [Optimal Stopping for Sequential Bayesian Experimental Design](https://arxiv.org/abs/2509.21734)
*Chen Cheng,Xun Huan*

Main category: stat.ME

TL;DR: 提出了一个贝叶斯框架用于顺序实验设计中的最优停止问题，将停止和设计策略联合优化，并通过课程学习解决训练不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统顺序贝叶斯实验设计通常预先固定实验次数，但实践中可能提前终止，需要解决何时停止的问题。基于阈值的规则简单但短视，忽略了未来实验的预期信息增益。

Method: 将最优停止问题建模为马尔可夫决策过程，联合优化停止和设计策略。采用策略梯度方法学习策略，并使用课程学习策略从强制继续逐渐过渡到自适应停止，解决训练不稳定性。

Result: 在线性高斯基准和污染物源检测问题上的数值研究表明，课程学习实现了稳定收敛，在具有强顺序依赖性的设置中优于普通方法。

Conclusion: 证明了最优停止规则是当立即终止奖励超过预期继续价值时停止，课程学习策略能有效解决联合优化中的循环依赖问题，提升性能。

Abstract: In sequential Bayesian experimental design, the number of experiments is
usually fixed in advance. In practice, however, campaigns may terminate early,
raising the fundamental question: when should one stop? Threshold-based rules
are simple to implement but inherently myopic, as they trigger termination
based on a fixed criterion while ignoring the expected future information gain
that additional experiments might provide. We develop a principled Bayesian
framework for optimal stopping in sequential experimental design, formulated as
a Markov decision process where stopping and design policies are jointly
optimized. We prove that the optimal rule is to stop precisely when the
immediate terminal reward outweighs the expected continuation value. To learn
such policies, we introduce a policy gradient method, but show that na\"ive
joint optimization suffers from circular dependencies that destabilize
training. We resolve this with a curriculum learning strategy that gradually
transitions from forced continuation to adaptive stopping. Numerical studies on
a linear-Gaussian benchmark and a contaminant source detection problem
demonstrate that curriculum learning achieves stable convergence and
outperforms vanilla methods, particularly in settings with strong sequential
dependencies.

</details>


### [7] [Transfer Learning under Group-Label Shift: A Semiparametric Exponential Tilting Approach](https://arxiv.org/abs/2509.22268)
*Manli Cheng,Subha Maity,Qinglong Tian,Pengfei Li*

Main category: stat.ME

TL;DR: 提出了一种新的迁移学习二分类框架，处理源域和目标域之间协变量和标签分布都可能变化的情况，通过组标签偏移假设和指数倾斜建模来改善对真实世界分布变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常假设只有协变量偏移或标签偏移，但现实世界中两者可能同时变化，且存在子群体不平衡和虚假相关性问题，需要更灵活的框架来处理这些复杂分布变化。

Method: 采用指数倾斜公式建模联合分布差异，通过工具变量策略建立可验证的识别条件，开发了计算高效的两步似然估计程序，结合源域结果模型的逻辑回归和源域目标域协变量的条件似然估计。

Result: 推导了估计量的一致性和渐近正态性，将理论扩展到ROC曲线、AUC等目标泛函，模拟研究显示在子群体偏移场景下优于现有方法，水鸟数据集的半合成应用证实了方法的有效性。

Conclusion: 所提出的框架能够有效处理复杂的分布变化，提高目标域分类精度，为迁移学习中的分布偏移问题提供了实用的解决方案。

Abstract: We propose a new framework for binary classification in transfer learning
settings where both covariate and label distributions may shift between source
and target domains. Unlike traditional covariate shift or label shift
assumptions, we introduce a group-label shift assumption that accommodates
subpopulation imbalance and mitigates spurious correlations, thereby improving
robustness to real-world distributional changes. To model the joint
distribution difference, we adopt a flexible exponential tilting formulation
and establish mild, verifiable identification conditions via an instrumental
variable strategy. We develop a computationally efficient two-step
likelihood-based estimation procedure that combines logistic regression for the
source outcome model with conditional likelihood estimation using both source
and target covariates. We derive consistency and asymptotic normality for the
resulting estimators, and extend the theory to receiver operating
characteristic curves, the area under the curve, and other target functionals,
addressing the nonstandard challenges posed by plug-in classifiers. Simulation
studies demonstrate that our method outperforms existing alternatives under
subpopulation shift scenarios. A semi-synthetic application using the
waterbirds dataset further confirms the proposed method's ability to transfer
information effectively and improve target-domain classification accuracy.

</details>


### [8] [Federated Learning of Quantile Inference under Local Differential Privacy](https://arxiv.org/abs/2509.21800)
*Leheng Cai,Qirui Hu,Shuyuan Wu*

Main category: stat.ME

TL;DR: 提出了一种在本地差分隐私下的联邦学习分位数推断方法，基于本地随机梯度下降，通过随机机制扰动梯度，在保持统计效率的同时适应通信和存储限制。


<details>
  <summary>Details</summary>
Motivation: 研究在本地差分隐私约束下的联邦学习分位数推断问题，解决现有方法对标准平滑性条件的依赖，同时处理数据异质性和个体隐私预算需求。

Method: 使用本地随机梯度下降方法，通过带有全局参数的随机机制扰动本地梯度，采用自归一化方法构建置信区间，避免估计额外参数。

Result: 建立了估计量的渐近正态性和函数中心极限定理，数值实验和实际数据应用验证了理论保证。

Conclusion: 所提方法能够有效处理本地差分隐私下的联邦学习分位数推断，适应数据异质性和个体隐私预算，无需标准平滑性条件，具有理论保证和实际应用价值。

Abstract: In this paper, we investigate federated learning for quantile inference under
local differential privacy (LDP). We propose an estimator based on local
stochastic gradient descent (SGD), whose local gradients are perturbed via a
randomized mechanism with global parameters, making the procedure tolerant of
communication and storage constraints without compromising statistical
efficiency. Although the quantile loss and its corresponding gradient do not
satisfy standard smoothness conditions typically assumed in existing
literature, we establish asymptotic normality for our estimator as well as a
functional central limit theorem. The proposed method accommodates data
heterogeneity and allows each server to operate with an individual privacy
budget. Furthermore, we construct confidence intervals for the target value
through a self-normalization approach, thereby circumventing the need to
estimate additional nuisance parameters. Extensive numerical experiments and
real data application validate the theoretical guarantees of the proposed
methodology.

</details>


### [9] [General CoVaR Based on Entropy Pooling](https://arxiv.org/abs/2509.21904)
*Yuhong Xu,Xinyao Zhao*

Main category: stat.ME

TL;DR: 提出了一个通用的CoVaR框架，通过整合专家观点和信息来扩展传统CoVaR，使用熵池化方法计算后验分布，并推导了在多元正态分布假设下的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 传统CoVaR方法未能充分利用专家观点和信息，如资产矩特征、分位数洞察和资产间相对损失分布视角，需要开发一个更全面的框架来整合这些信息。

Method: 采用熵池化方法有效整合专家观点，同时最小化与先验分布的偏差，推导后验分布来计算通用CoVaR，假设二元正态分布并推导各种视角下的解析表达式。

Result: 敏感性分析显示CoVaR与视图中的变量期望及其差异呈线性关系，而与方差、分位数和相关性呈非线性依赖关系。对美国银行系统的实证分析证明了该方法的有效性。

Conclusion: 通用CoVaR框架在适当指定专家观点时表现有效，并可扩展到通用ΔCoVaR来从多个视角评估风险溢出效应。

Abstract: We propose a general CoVaR framework that extends the traditional CoVaR by
incorporating diverse expert views and information, such as asset moment
characteristics, quantile insights, and perspectives on the relative loss
distribution between two assets. To integrate these expert views effectively
while minimizing deviations from the prior distribution, we employ the entropy
pooling method to derive the posterior distribution, which in turn enables us
to compute the general CoVaR. Assuming bivariate normal distributions, we
derive its analytical expressions under various perspectives. Sensitivity
analysis reveals that CoVaR exhibits a linear relationship with both the
expectations of the variables in the views and the differences in expectations
between them. In contrast, CoVaR shows nonlinear dependencies with respect to
the variance, quantiles, and correlation within these views.
  Empirical analysis of the US banking system during the Federal Reserve's
interest rate hikes demonstrates the effectiveness of the general CoVaR when
expert views are appropriately specified. Furthermore, we extend this framework
to the general $\Delta$CoVaR, which allows for the assessment of risk spillover
effects from various perspectives.

</details>


### [10] [Rescuing double robustness: safe estimation under complete misspecification](https://arxiv.org/abs/2509.22446)
*Lorenzo Testa,Francesca Chiaromonte,Kathryn Roeder*

Main category: stat.ME

TL;DR: 本文提出了自适应校正裁剪(ACC)方法来解决双重稳健估计器在完全错误设定下的脆弱性问题，确保估计误差受限于个体模型误差的凸组合，防止误差累积。


<details>
  <summary>Details</summary>
Motivation: 双重稳健估计器在部分错误设定下具有优势，但在实际应用中完全错误设定是常态，此时这些估计器可能表现脆弱，误差会因模型错误的乘积效应而放大。

Method: 提出自适应校正裁剪(ACC)方法，该方法在正确设定下继承双重稳健估计器的优良性质，但在错误设定下通过凸组合限制误差，防止误差的不稳定累积。

Result: ACC估计器在模拟和阿尔茨海默病蛋白质组学数据分析中表现出色，在正确设定下保持有效性，在错误设定下提供有界误差。

Conclusion: ACC方法为双重稳健估计器提供了一种安全替代方案，既保留了正确设定下的优势，又解决了完全错误设定下的脆弱性问题。

Abstract: Double robustness is a major selling point of semiparametric and missing data
methodology. Its virtues lie in protection against partial nuisance
misspecification and asymptotic semiparametric efficiency under correct
nuisance specification. However, in many applications, complete nuisance
misspecification should be regarded as the norm (or at the very least the
expected default), and thus doubly robust estimators may behave fragilely. In
fact, it has been amply verified empirically that these estimators can perform
poorly when all nuisance functions are misspecified. Here, we first
characterize this phenomenon of double fragility, and then propose a solution
based on adaptive correction clipping (ACC). We argue that our ACC proposal is
safe, in that it inherits the favorable properties of doubly robust estimators
under correct nuisance specification, but its error is guaranteed to be bounded
by a convex combination of the individual nuisance model errors, which prevents
the instability caused by the compounding product of errors of doubly robust
estimators. We also show that our proposal provides valid inference through the
parametric bootstrap when nuisances are well-specified. We showcase the
efficacy of our ACC estimator both through extensive simulations and by
applying it to the analysis of Alzheimer's disease proteomics data.

</details>


### [11] [Deep learning for interval-censored failure time data from case-cohort studies](https://arxiv.org/abs/2509.22081)
*Yeyu Xiao,Yonghong Long*

Main category: stat.ME

TL;DR: 提出了一种结合深度神经网络和Bernstein多项式的筛极大加权似然方法，用于处理病例队列设计中的区间删失数据，能够捕捉协变量的非线性关系并达到最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设协变量线性关系，但实际数据中可能存在复杂的非线性关系。病例队列设计可降低研究成本，但需要更灵活的建模方法。

Method: 使用深度神经网络灵活表示协变量依赖函数，用Bernstein多项式逼近累积基准风险函数，提出筛极大加权似然方法。

Result: 模拟研究表明该方法表现良好，实际数据分析显示方法准确且可解释。

Conclusion: 该方法能有效处理病例队列设计中的区间删失数据，捕捉非线性关系，并具有良好的准确性和可解释性。

Abstract: Interval-censored data are common in fields such as epidemiology and
demography. When the failure event of interest is relatively rare and the
collection of covariates is costly, researchers often adopt the case-cohort
design to reduce study costs. However, existing studies typically rely on the
assumption of linearity in modeling covariates, which may not capture the
complex and nonlinear relationships present in real data. To address this
limitation, we consider a class of transformation models with unspecified
covariate-dependent functions. We propose a sieve maximum weighted likelihood
approach for interval-censored data arising from the case-cohort design, which
combines deep neural networks with Bernstein polynomials. The method employs a
deep neural network to flexibly represent the covariate-dependent function and
uses Bernstein polynomials to approximate the cumulative baseline hazard
function. We establish the consistency and convergence rate of the proposed
estimator and show that the resulting nonparametric deep neural network
estimator attains the minimax optimal rate of convergence (up to a
polylogarithmic factor). Simulation studies suggest that the proposed method
performs well in practice. Finally, we apply the method to a real dataset and
use the SHAP (Shapley Additive Explanations) approach to attribute the neural
network predictions of the covariate-dependent function to covariates. The
results indicate that our method is both accurate and interpretable.

</details>


### [12] [Bayesian approach to the PC component](https://arxiv.org/abs/2509.22217)
*Jie Yao,Kai Zhang,Eric Rose,Edward Valachovic*

Main category: stat.ME

TL;DR: 提出了一种结合频率分离技术和贝叶斯技术的两阶段方法来预测周期性相关(PC)和多周期性相关(MPC)时间序列数据


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型难以处理复杂的多周期性相关组件，容易出现过参数化和优化问题，且无法建模复杂的PC组件模式

Method: 采用两阶段方法：第一阶段使用频率分离技术保持特定PC组件的相关结构，第二阶段使用贝叶斯技术结合先验信息更新对这些组件的信念

Result: 预期新方法在建模MPC组件方面比传统方法更合适

Conclusion: 结合频率分离和贝叶斯技术的两阶段方法为处理复杂多周期性相关时间序列提供了有效解决方案

Abstract: Time series with multiple periodically correlated components is a complex
problem with comparatively limited prior research. Most existing time series
models are designed to accommodate simple periodically correlated components
and tend to be sensitive to over-parameterization and optimization issues and
are also unable to model complex PC components patterns in a time series.
Frequency separation techniques can be used to maintain the correlation
structure of each specific PC component, whereas Bayesian techniques can
combine new and existing prior information to update beliefs about these
components. This study introduces a method to combine the frequency separation
techniques and Bayesian techniques to forecast PC and MPC time series data in a
two stage form which is expected to show the new method's suitability in
modeling MPC components compared to classical methods.

</details>


### [13] [Tail-robust estimation of factor-adjusted vector autoregressive models for high-dimensional time series](https://arxiv.org/abs/2509.22235)
*Dylan Dijk,Haeran Cho*

Main category: stat.ME

TL;DR: 提出一种因子调整的向量自回归模型，用于建模高维、厚尾时间序列数据，通过截断和两阶段估计方法处理厚尾性，并在仅假设(2+2ε)阶矩存在的情况下获得估计速率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的高维时间序列数据往往具有厚尾特征和普遍共动性，需要同时考虑因子结构和剩余关联性，而现有方法对厚尾性的处理不足。

Method: 采用元素级截断步骤和两阶段估计程序，先估计潜在因子，再估计稀疏VAR参数矩阵，在仅假设(2+2ε)阶矩存在的情况下进行建模。

Result: 模拟研究和宏观经济应用表明，所提出的估计器具有竞争性性能，估计速率明确反映了厚尾性通过ε参数的影响。

Conclusion: 该方法能够有效处理高维厚尾时间序列数据，在仅需有限矩假设的情况下获得良好的估计性能，为实际应用提供了实用工具。

Abstract: We study the problem of modelling high-dimensional, heavy-tailed time series
data via a factor-adjusted vector autoregressive (VAR) model, which
simultaneously accounts for pervasive co-movements of the variables by a
handful of factors, as well as their remaining interconnectedness using a
sparse VAR model. To accommodate heavy tails, we adopt an element-wise
truncation step followed by a two-stage estimation procedure for estimating the
latent factors and the VAR parameter matrices. Assuming the existence of the
$(2 + 2\epsilon)$-th moment only for some $\epsilon \in (0, 1)$, we derive the
rates of estimation that make explicit the effect of heavy tails through
$\epsilon$. Simulation studies and an application in macroeconomics demonstrate
the competitive performance of the proposed estimators.

</details>


### [14] [SensIAT: An R Package for Conducting Sensitivity Analysis of Randomized Trials with Irregular Assessment Times](https://arxiv.org/abs/2509.22389)
*Andrew Redd,Yujing Gao,Bonnie B. Smith,Ravi Varadhan,Andrea J. Apter,Daniel O. Scharfstein*

Main category: stat.ME

TL;DR: SensIAT是一个R包，实现了基于增强逆强度加权的敏感性分析方法，用于处理具有不规则且可能信息性评估时间的随机试验。该方法可以分析不同治疗组在指定时间点的平均结果差异，并评估信息性时间假设对推断的影响。


<details>
  <summary>Details</summary>
Motivation: 在随机试验中，当研究参与者在结果较差时更可能或更不可能进行评估时，未经调整的估计可能存在偏差。需要一种方法来评估信息性时间假设对推断的影响。

Method: 基于增强逆强度加权的敏感性分析方法，允许研究人员查看不同信息性时间假设强度和方向对推断的影响，同时结合灵活的半参数建模。

Result: 开发了SensIAT R包，并通过基于HAP2哮喘随机临床试验的合成数据集分析进行了功能演示。

Conclusion: SensIAT为处理不规则和信息性评估时间的随机试验提供了一种实用的敏感性分析工具，帮助研究人员评估信息性时间假设对治疗效应推断的影响。

Abstract: This paper introduces an R package SensIAT that implements a sensitivity
analysis methodology, based on augmented inverse intensity weighting, for
randomized trials with irregular and potentially informative assessment times.
Targets of inference involve the population mean outcome in each treatment arm
as well as the difference in these means (i.e., treatment effect) at specified
times after randomization. This methodology is useful in settings where there
is concern that study participants are either more, or less, likely to have
assessments at times when their outcomes are worse. In such settings,
unadjusted estimates can be biased. The methodology allows researchers to see
how inferences are impacted by a range of assumptions about the strength and
direction of informative timing in each arm, while incorporating flexible
semi-parametric modeling. We describe the functions implemented in SensIAT and
illustrate them through an analysis of a synthetic dataset motivated by the
HAP2 asthma randomized clinical trial.

</details>


### [15] [Generative multi-fidelity modeling and downscaling via spatial autoregressive transport maps](https://arxiv.org/abs/2509.22474)
*Alejandro Calle-Saldarriaga,Paul F. V. Wiemann,Matthias Katzfuss*

Main category: stat.ME

TL;DR: 提出了一种可扩展的贝叶斯方法，用于从少量训练样本中学习多保真度非平稳空间场的联合非高斯分布和非线性依赖结构，通过保真度感知自回归高斯过程实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 空间场数据通常以不同保真度或分辨率提供，高保真数据获取成本高，而低保真数据便宜。需要开发能够从低保真输出预测高保真场的统计代理模型。

Method: 基于保真度感知自回归高斯过程，采用正则化诱导先验，利用共轭性获得封闭形式的积分似然，通过随机梯度下降进行高效超参数优化。

Result: 该方法在数值比较中显著优于现有方法，能够基于粗分辨率（低保真）全球环流模型输出表征和模拟高保真精细尺度气候行为。

Conclusion: 提出的方法能够高效学习多保真度空间场的非线性依赖结构，为基于低保真数据预测高保真场提供了一种可扩展的贝叶斯解决方案。

Abstract: Spatial fields are often available at multiple fidelities or resolutions,
where high-fidelity data is typically more costly to obtain than low-fidelity
data. Statistical surrogates or emulators can predict high-fidelity fields from
cheap low-fidelity output. We propose a highly scalable Bayesian approach that
can learn the joint non-Gaussian distribution and nonlinear dependence
structure of nonstationary spatial fields at multiple fidelities from a small
number of training samples. Our method is based on fidelity-aware
autoregressive GPs with suitably chosen regularization-inducing priors.
Exploiting conjugacy, the integrated likelihood is available in closed form,
enabling efficient hyperparameter optimization via stochastic gradient descent.
After training, the method also characterizes in closed form the distribution
of higher-fidelity fields given lower-fidelity data. In our numerical
comparisons, we show that our approach substantially outperforms existing
methods and that it can be used to characterize and simulate high-fidelity
fine-scale climate behavior based on output from coarse (low-fidelity) global
circulation models.

</details>


### [16] [A Multiplicative Instrumental Variable Model for Data Missing Not-at-Random](https://arxiv.org/abs/2509.22499)
*Yunshu Zhang,Chan Park,Jiewen Liu,Yonghoon Lee,Mengxin Yu,James M. Robins,Eric J. Tchetgen Tchetgen*

Main category: stat.ME

TL;DR: 提出了一个不限制选择偏差程度的工具变量框架，通过乘法选择模型识别缺失数据，并开发了半参数多重稳健估计器。


<details>
  <summary>Details</summary>
Motivation: 现有的工具变量方法（包括Heckman选择模型）都先验地限制结果尺度上的选择偏差程度，可能低估缺失数据带来的不确定性。

Method: 引入乘法选择模型，假设工具变量与隐藏的共同相关因素在乘法尺度上不交互，通过单臂Wald比率估计量进行非参数识别，并推导影响函数开发半参数多重稳健估计器。

Result: 模拟研究显示所提方法在有限样本下表现良好，在博茨瓦纳的HIV调查研究中成功使用访谈员特征作为工具变量校正依赖非响应带来的选择偏差。

Conclusion: 新框架允许选择偏差程度完全不受限制，提供了更灵活的工具变量方法，在实践应用中表现出色。

Abstract: Instrumental variable (IV) methods offer a valuable approach to account for
outcome data missing not-at-random. A valid missing data instrument is a
measured factor which (i) predicts the nonresponse process and (ii) is
independent of the outcome in the underlying population. For point
identification, all existing IV methods for missing data including the
celebrated Heckman selection model, a priori restrict the extent of selection
bias on the outcome scale, therefore potentially understating uncertainty due
to missing data. In this work, we introduce an IV framework which allows the
degree of selection bias on the outcome scale to remain completely
unrestricted. The new approach instead relies for identification on (iii) a key
multiplicative selection model, which posits that the instrument and any hidden
common correlate of selection and the outcome, do not interact on the
multiplicative scale. Interestingly, we establish that any regular statistical
functional of the missing outcome is nonparametrically identified under
(i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald
ratio estimand in causal inference. For estimation and inference, we
characterize the influence function for any functional defined on a
nonparametric model for the observed data, which we leverage to develop
semiparametric multiply robust IV estimators. Several extensions of the methods
are also considered, including the important practical setting of polytomous
and continuous instruments. Simulation studies illustrate the favorable finite
sample performance of proposed methods, which we further showcase in an HIV
study nested within a household health survey study we conducted in Mochudi,
Botswana, in which interviewer characteristics are used as instruments to
correct for selection bias due to dependent nonresponse in the HIV component of
the survey study.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [17] [Quantifying Fire Risk Index in Chemical Industry Using Statistical Modeling Procedure](https://arxiv.org/abs/2509.21736)
*Hyewon Jung,Seungil Ahn,Seungho Choi,Yeseul Jeon*

Main category: stat.AP

TL;DR: 开发了一个基于文本的风险指数框架，通过整合火灾事故报告中的文本叙述和财务损失数据，量化关键词与财产损失之间的关联，为火灾风险评估提供可解释的指标。


<details>
  <summary>Details</summary>
Motivation: 火灾事故报告中的文本叙述包含了结构化记录中常被忽略的因果因素，而财务损失金额则提供了可量化的结果。整合这两种信息源对于揭示描述性原因与经济后果之间的可解释联系至关重要。

Method: 使用韩国2013-2024年化工业特殊建筑的火灾调查报告，采用主题建模和基于网络的嵌入方法估计词语间的语义相似度，然后应用Lasso回归量化这些词语与财产损失金额的关联，从而估计火灾风险指数。

Result: 分析识别了多个风险领域，包括危险化学品泄漏、不安全存储实践、设备和设施故障以及环境引发的点火。结果表明文本衍生的指数提供了可解释且具有实际相关性的见解。

Conclusion: 文本衍生的风险指数能够桥接非结构化叙述与结构化损失信息，为基于证据的火灾风险评估和管理提供基础，展示了该方法在实践中的相关性和应用价值。

Abstract: Fire incident reports contain detailed textual narratives that capture causal
factors often overlooked in structured records, while financial damage amounts
provide measurable outcomes of these events. Integrating these two sources of
information is essential for uncovering interpretable links between descriptive
causes and their economic consequences. To this end, we develop a data-driven
framework that constructs a composite Risk Index, enabling systematic
quantification of how specific keywords relate to property damage amounts. This
index facilitates both the identification of high-impact terms and the
aggregation of risks across semantically related clusters, thereby offering a
principled measure of fire-related financial risk. Using more than a decade of
Korean fire investigation reports on the chemical industry classified as
Special Buildings (2013 through 2024), we employ topic modeling and
network-based embedding to estimate semantic similarities from interactions
among words and subsequently apply Lasso regression to quantify their
associations with property damage amounts, thereby estimate fire risk index.
This approach enables us to assess fire risk not only at the level of
individual terms but also within their broader textual context, where highly
interactive related words provide insights into collective patterns of hazard
representation and their potential impact on expected losses. The analysis
highlights several domains of risk, including hazardous chemical leakage,
unsafe storage practices, equipment and facility malfunctions, and
environmentally induced ignition. The results demonstrate that text-derived
indices provide interpretable and practically relevant insights, bridging
unstructured narratives with structured loss information and offering a basis
for evidence-based fire risk assessment and management.

</details>


### [18] [Personalized Oncology: Feasibility of Evaluating Treatment Effects for Individual Patients](https://arxiv.org/abs/2509.22089)
*Lydia Jang,Stefan Konigorski*

Main category: stat.AP

TL;DR: 本文探讨了在单例癌症患者中应用因果推断方法评估治疗效果的可行性，通过分析一位转移性癌症患者的纵向数据，展示了如何使用时变g-公式计算个体特异性治疗效果。


<details>
  <summary>Details</summary>
Motivation: 精准肿瘤学虽然改善了分子分析和个体化治疗，但治疗效果仍存在个体差异。癌症的异质性和动态特性需要定制化方法来识别因果关系，而现有N-of-1试验的因果框架是否适用于单例癌症患者尚不明确。

Method: 使用一位转移性癌症患者的纵向数据集，包含适应性选择的治疗方案以及随时间记录的生物标志物和病灶测量数据。选择具有足够数据点的治疗期，应用因果框架定义估计量、识别因果关系和假设，使用时变g-公式计算个体特异性治疗效果。

Result: 研究证明了在单例癌症患者环境中应用因果方法的可行性，并明确展示了何时以及如何估计因果治疗效果。

Conclusion: 该研究不仅证明了因果方法在单例癌症患者环境中的可行性，还为在更广泛癌症类型的个体化环境中使用因果方法提供了蓝图。

Abstract: The effectiveness of personalized oncology treatments ultimately depends on
whether outcomes can be causally attributed to the treatment. Advances in
precision oncology have improved molecular profiling of individuals, and
tailored therapies have led to more effective treatments for select patient
groups. However, treatment responses still vary among individuals. As cancer is
a heterogeneous and dynamic disease with varying treatment outcomes across
different molecular types and resistance mechanisms, it requires customized
approaches to identify cause-and-effect relationships. N-of-1 trials, or
single-subject clinical trials, are designed to evaluate individual treatment
effects. Several works have described different causal frameworks to identify
treatment effects in N-of-1 trials, yet whether these approaches can be
extended to single-cancer patient settings remains unclear. To explore this
possibility, a longitudinal dataset from a single metastatic cancer patient
with adaptively chosen treatments was considered. The dataset consisted of a
detailed treatment plan as well as biomarker and lesion measurements recorded
over time. After data processing, a treatment period with sufficient data
points to conduct causal inference was selected. Under this setting, a causal
framework was applied to define an estimand, identify causal relationships and
assumptions, and calculate an individual-specific treatment effect using a
time-varying g-formula. Through this application, we illustrate explicitly when
and how causal treatment effects can be estimated in single-patient oncology
settings. Our findings not only demonstrate the feasibility of applying causal
methods in a single-cancer patient setting but also offer a blueprint for using
causal methods across a broader spectrum of cancer types in individualized
settings.

</details>


### [19] [Chronic Stress, Immune Suppression, and Cancer Occurrence: Unveiling the Connection using Survey Data and Predictive Models](https://arxiv.org/abs/2509.22275)
*Teddy Lazebnik,Vered Aharonson*

Main category: stat.AP

TL;DR: 该研究使用机器学习和因果建模方法，揭示了慢性压力与癌症发生之间的直接因果联系，发现压力频率、压力水平和感知健康影响与癌症发病率显著相关。


<details>
  <summary>Details</summary>
Motivation: 虽然慢性压力被认为与癌症发生有关，但直接的因果关系尚未得到一致证实。机器学习和因果建模为探索心理慢性压力与癌症发生之间的复杂因果相互作用提供了机会。

Method: 开发预测模型，使用自我报告调查中的压力指标、癌症史和人口统计数据变量，揭示慢性压力与癌症发生之间的直接联系和免疫抑制介导的途径，并通过传统统计方法验证模型。

Result: 研究发现压力频率、压力水平和感知健康影响与癌症发病率存在显著因果相关性。虽然单独的压力预测能力有限，但结合社会人口学和家族癌症史数据显著提高了模型准确性。

Conclusion: 研究结果强调了癌症风险的多维性，压力与遗传易感性一样成为重要因素，支持将慢性压力作为可改变癌症风险因素纳入个性化预防策略和公共卫生干预措施。

Abstract: Chronic stress was implicated in cancer occurrence, but a direct causal
connection has not been consistently established. Machine learning and causal
modeling offer opportunities to explore complex causal interactions between
psychological chronic stress and cancer occurrences. We developed predictive
models employing variables from stress indicators, cancer history, and
demographic data from self-reported surveys, unveiling the direct and immune
suppression mitigated connection between chronic stress and cancer occurrence.
The models were corroborated by traditional statistical methods. Our findings
indicated significant causal correlations between stress frequency, stress
level and perceived health impact, and cancer incidence. Although stress alone
showed limited predictive power, integrating socio-demographic and familial
cancer history data significantly enhanced model accuracy. These results
highlight the multidimensional nature of cancer risk, with stress emerging as a
notable factor alongside genetic predisposition. These findings strengthen the
case for addressing chronic stress as a modifiable cancer risk factor,
supporting its integration into personalized prevention strategies and public
health interventions to reduce cancer incidence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [20] [Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models](https://arxiv.org/abs/2509.21423)
*Ehsan Sharifian,Saber Salehkaleybar,Negar Kiyavash*

Main category: stat.ML

TL;DR: 该论文研究从线性和非高斯结构方程模型的观测和干预数据中学习因果结构的问题，提出了基于二分图完美匹配的等价类组合表征方法，并设计了自适应次模优化的最优实验设计策略。


<details>
  <summary>Details</summary>
Motivation: 仅使用观测数据只能识别因果图到置换等价类，需要利用干预数据来进一步约束和识别真实的因果结构。

Method: 将因果图的等价类表示为二分图中的完美匹配，每个原子干预揭示一条匹配边并消除不兼容的因果图。将最优实验设计形式化为自适应随机优化问题，使用随机匹配采样来高效估计奖励函数。

Result: 证明了奖励函数具有自适应次模性，提出了具有可证明近优性能保证的贪心策略。仿真结果表明，少量干预即可恢复真实的因果结构。

Conclusion: 提出的基于二分图匹配和自适应次模优化的框架能够有效利用干预数据来学习包含循环的因果结构，解决了显式枚举等价类的计算挑战。

Abstract: We study the problem of causal structure learning from a combination of
observational and interventional data generated by a linear non-Gaussian
structural equation model that might contain cycles. Recent results show that
using mere observational data identifies the causal graph only up to a
permutation-equivalence class. We obtain a combinatorial characterization of
this class by showing that each graph in an equivalence class corresponds to a
perfect matching in a bipartite graph. This bipartite representation allows us
to analyze how interventions modify or constrain the matchings. Specifically,
we show that each atomic intervention reveals one edge of the true matching and
eliminates all incompatible causal graphs. Consequently, we formalize the
optimal experiment design task as an adaptive stochastic optimization problem
over the set of equivalence classes with a natural reward function that
quantifies how many graphs are eliminated from the equivalence class by an
intervention. We show that this reward function is adaptive submodular and
provide a greedy policy with a provable near-optimal performance guarantee. A
key technical challenge is to efficiently estimate the reward function without
having to explicitly enumerate all the graphs in the equivalence class. We
propose a sampling-based estimator using random matchings and analyze its bias
and concentration behavior. Our simulation results show that performing a small
number of interventions guided by our stochastic optimization framework
recovers the true underlying causal structure.

</details>


### [21] [Sequential 1-bit Mean Estimation with Near-Optimal Sample Complexity](https://arxiv.org/abs/2509.21940)
*Ivan Lau,Jonathan Scarlett*

Main category: stat.ML

TL;DR: 提出了一种基于自适应区间查询的1比特通信分布式均值估计方法，在已知均值和方差边界的情况下达到最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究在1比特通信约束下的分布式均值估计问题，旨在设计高效的通信协议来减少数据传输量。

Method: 使用随机化和顺序选择的区间查询，每个查询返回1比特指示样本是否在指定区间内，通过自适应策略优化估计精度。

Result: 推导出样本复杂度为Õ(σ²/ε² log(1/δ) + log(λ/σ))，匹配无量化设置的最小最大下界，并证明log(λ/σ)项是不可避免的。

Conclusion: 自适应区间查询方法在1比特通信约束下达到近乎最优性能，并展示了自适应策略相对于非自适应方法的优势。

Abstract: In this paper, we study the problem of distributed mean estimation with 1-bit
communication constraints. We propose a mean estimator that is based on
(randomized and sequentially-chosen) interval queries, whose 1-bit outcome
indicates whether the given sample lies in the specified interval. Our
estimator is $(\epsilon, \delta)$-PAC for all distributions with bounded mean
($-\lambda \le \mathbb{E}(X) \le \lambda $) and variance ($\mathrm{Var}(X) \le
\sigma^2$) for some known parameters $\lambda$ and $\sigma$. We derive a sample
complexity bound $\widetilde{O}\big(
\frac{\sigma^2}{\epsilon^2}\log\frac{1}{\delta} +
\log\frac{\lambda}{\sigma}\big)$, which matches the minimax lower bound for the
unquantized setting up to logarithmic factors and the additional
$\log\frac{\lambda}{\sigma}$ term that we show to be unavoidable. We also
establish an adaptivity gap for interval-query based estimators: the best
non-adaptive mean estimator is considerably worse than our adaptive mean
estimator for large $\frac{\lambda}{\sigma}$. Finally, we give tightened sample
complexity bounds for distributions with stronger tail decay, and present
additional variants that (i) handle an unknown sampling budget (ii) adapt to
the unknown true variance given (possibly loose) upper and lower bounds on the
variance, and (iii) use only two stages of adaptivity at the expense of more
complicated (non-interval) queries.

</details>


### [22] [General Pruning Criteria for Fast SBL](https://arxiv.org/abs/2509.21572)
*Jakob Möderl,Erik Leitinger,Bernard Henri Fleury*

Main category: stat.ML

TL;DR: 本文分析了稀疏贝叶斯学习(SBL)在放宽高斯假设条件下，单个超参数的边际似然函数行为，推导了导致有限和无限超参数估计的充分条件，并在高斯情况下与快速SBL的剪枝条件一致。


<details>
  <summary>Details</summary>
Motivation: 稀疏贝叶斯学习通过假设权重服从高斯分布并估计超参数来实现稀疏性，但传统分析基于高斯假设。本文旨在研究当噪声和权重分布的高斯假设被放宽时，SBL的超参数估计行为。

Method: 分析单个超参数的边际似然函数，同时固定其他超参数，推导在非高斯条件下导致有限和无限超参数估计的充分条件。

Result: 建立了在放宽高斯假设下，导致超参数估计有限或无限的充分条件，并在高斯情况下验证这些条件与快速SBL的剪枝条件一致。

Conclusion: 本文为稀疏贝叶斯学习提供了更一般的理论分析框架，揭示了在非高斯条件下超参数估计的行为特性，并加深了对快速SBL算法的理解。

Abstract: Sparse Bayesian learning (SBL) associates to each weight in the underlying
linear model a hyperparameter by assuming that each weight is Gaussian
distributed with zero mean and precision (inverse variance) equal to its
associated hyperparameter. The method estimates the hyperparameters by
marginalizing out the weights and performing (marginalized) maximum likelihood
(ML) estimation. SBL returns many hyperparameter estimates to diverge to
infinity, effectively setting the estimates of the corresponding weights to
zero (i.e., pruning the corresponding weights from the model) and thereby
yielding a sparse estimate of the weight vector.
  In this letter, we analyze the marginal likelihood as function of a single
hyperparameter while keeping the others fixed, when the Gaussian assumptions on
the noise samples and the weight distribution that underlies the derivation of
SBL are weakened. We derive sufficient conditions that lead, on the one hand,
to finite hyperparameter estimates and, on the other, to infinite ones.
Finally, we show that in the Gaussian case, the two conditions are
complementary and coincide with the pruning condition of fast SBL (F-SBL),
thereby providing additional insights into this algorithm.

</details>


### [23] [A Random Matrix Perspective of Echo State Networks: From Precise Bias--Variance Characterization to Optimal Regularization](https://arxiv.org/abs/2509.22011)
*Yessin Moakher,Malik Tiomoko,Cosme Louart,Zhenyu Liao*

Main category: stat.ML

TL;DR: 本文对回声状态网络(ESN)在师生设置下进行了严格的渐近分析，推导出了偏差、方差和均方误差的闭式表达式，揭示了ESN与经典岭回归的两个关键差异：不出现双下降现象，在训练样本和教师记忆长度有限时获得更低的MSE。


<details>
  <summary>Details</summary>
Motivation: 通过严格的数学分析来理解ESN的性能特性，为调参提供理论指导，并解释最近的实证观察结果。

Method: 利用随机矩阵理论，在具有线性教师和预言权重的师生设置下，推导ESN的渐近性能表达式。

Result: 获得了ESN偏差、方差和MSE的闭式表达式；发现了ESN不出现双下降现象；在有限样本和记忆长度下ESN表现更好；提供了最优正则化的显式公式和数值计算方案。

Conclusion: 研究为ESN调参提供了可解释的理论和实用指南，有助于将实证观察与可证明的性能保证相协调。

Abstract: We present a rigorous asymptotic analysis of Echo State Networks (ESNs) in a
teacher student setting with a linear teacher with oracle weights. Leveraging
random matrix theory, we derive closed form expressions for the asymptotic
bias, variance, and mean-squared error (MSE) as functions of the input
statistics, the oracle vector, and the ridge regularization parameter. The
analysis reveals two key departures from classical ridge regression: (i) ESNs
do not exhibit double descent, and (ii) ESNs attain lower MSE when both the
number of training samples and the teacher memory length are limited. We
further provide an explicit formula for the optimal regularization in the
identity input covariance case, and propose an efficient numerical scheme to
compute the optimum in the general case. Together, these results offer
interpretable theory and practical guidelines for tuning ESNs, helping
reconcile recent empirical observations with provable performance guarantees

</details>


### [24] [IndiSeek learns information-guided disentangled representations](https://arxiv.org/abs/2509.21584)
*Yu Gui,Cong Ma,Zongming Ma*

Main category: stat.ML

TL;DR: IndiSeek是一种新的解耦表示学习方法，通过结合独立性强制目标和计算高效的重建损失来解决多模态学习中共享和模态特定特征提取的挑战。


<details>
  <summary>Details</summary>
Motivation: 在多模态学习（如单细胞多组学）中，共享和模态特定特征对于表征细胞状态和支持下游分析都至关重要。理想情况下，模态特定特征应与共享特征独立，同时捕获每个模态内的所有互补信息。

Method: IndiSeek结合了独立性强制目标和计算高效的重建损失，该重建损失能够约束条件互信息，明确平衡独立性和完整性。

Result: 在合成模拟、CITE-seq数据集和多个真实世界多模态基准测试中证明了IndiSeek的有效性。

Conclusion: IndiSeek能够实现模态特定特征的原则性提取，解决了基于互信息的目标难以可靠估计及其变分替代在实践中表现不佳的问题。

Abstract: Learning disentangled representations is a fundamental task in multi-modal
learning. In modern applications such as single-cell multi-omics, both shared
and modality-specific features are critical for characterizing cell states and
supporting downstream analyses. Ideally, modality-specific features should be
independent of shared ones while also capturing all complementary information
within each modality. This tradeoff is naturally expressed through
information-theoretic criteria, but mutual-information-based objectives are
difficult to estimate reliably, and their variational surrogates often
underperform in practice. In this paper, we introduce IndiSeek, a novel
disentangled representation learning approach that addresses this challenge by
combining an independence-enforcing objective with a computationally efficient
reconstruction loss that bounds conditional mutual information. This
formulation explicitly balances independence and completeness, enabling
principled extraction of modality-specific features. We demonstrate the
effectiveness of IndiSeek on synthetic simulations, a CITE-seq dataset and
multiple real-world multi-modal benchmarks.

</details>


### [25] [Effective continuous equations for adaptive SGD: a stochastic analysis view](https://arxiv.org/abs/2509.21614)
*Luca Callisti,Marco Romito,Francesco Triggiano*

Main category: stat.ML

TL;DR: 对自适应SGD方法在小学习率下的理论分析，使用随机修正方程框架推导有效连续随机动力学，揭示采样噪声在极限中表现为驱动参数和梯度二阶矩演化的独立布朗运动。


<details>
  <summary>Details</summary>
Motivation: 分析流行的自适应随机梯度下降方法在小学习率下的理论特性，理解采样噪声在这些方法中的表现机制。

Method: 使用Li等人提出的随机修正方程框架，推导自适应SGD方法的有效连续随机动力学，并扩展Malladi等人的方法研究学习率与关键超参数之间的缩放规则。

Result: 发现采样诱导的噪声在极限中表现为独立的布朗运动，分别驱动参数和梯度二阶矩的演化，并表征了所有非平凡极限动力学。

Conclusion: 为自适应SGD方法提供了理论分析框架，揭示了采样噪声在连续极限中的具体表现形式，有助于理解这些方法的收敛特性。

Abstract: We present a theoretical analysis of some popular adaptive Stochastic
Gradient Descent (SGD) methods in the small learning rate regime. Using the
stochastic modified equations framework introduced by Li et al., we derive
effective continuous stochastic dynamics for these methods. Our key
contribution is that sampling-induced noise in SGD manifests in the limit as
independent Brownian motions driving the parameter and gradient second momentum
evolutions. Furthermore, extending the approach of Malladi et al., we
investigate scaling rules between the learning rate and key hyperparameters in
adaptive methods, characterising all non-trivial limiting dynamics.

</details>


### [26] [Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression](https://arxiv.org/abs/2509.22341)
*Anvit Garg,Sohom Bhattacharya,Pragya Sur*

Main category: stat.ML

TL;DR: 本文研究了过参数化线性回归中的模型崩溃现象，分析了在迭代训练中使用真实数据和合成数据混合时，最小L2范数插值和岭回归的泛化误差，揭示了最优混合权重可以防止模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 研究模型崩溃现象——当生成模型反复在自身合成输出上训练时性能会退化，旨在理解在迭代训练方案中混合真实数据和合成数据对模型性能的影响。

Method: 使用过参数化线性回归框架，分析最小L2范数插值和岭回归在迭代训练中的泛化误差，推导精确的误差公式，并研究最优混合权重。

Result: 发现最小L2范数插值的最优真实数据比例收敛于黄金分割比的倒数，岭回归的最优混合比至少为1/2，表明需要偏向真实数据。

Conclusion: 通过理论分析和模拟验证，证明了适当混合真实数据和合成数据可以防止模型崩溃，最优混合权重由谱几何特性决定。

Abstract: Model collapse occurs when generative models degrade after repeatedly
training on their own synthetic outputs. We study this effect in
overparameterized linear regression in a setting where each iteration mixes
fresh real labels with synthetic labels drawn from the model fitted in the
previous iteration. We derive precise generalization error formulae for
minimum-$\ell_2$-norm interpolation and ridge regression under this iterative
scheme. Our analysis reveals intriguing properties of the optimal mixing weight
that minimizes long-term prediction error and provably prevents model collapse.
For instance, in the case of min-$\ell_2$-norm interpolation, we establish that
the optimal real-data proportion converges to the reciprocal of the golden
ratio for fairly general classes of covariate distributions. Previously, this
property was known only for ordinary least squares, and additionally in low
dimensions. For ridge regression, we further analyze two popular model classes
-- the random-effects model and the spiked covariance model -- demonstrating
how spectral geometry governs optimal weighting. In both cases, as well as for
isotropic features, we uncover that the optimal mixing ratio should be at least
one-half, reflecting the necessity of favoring real-data over synthetic. We
validate our theoretical results with extensive simulations.

</details>


### [27] [SADA: Safe and Adaptive Inference with Multiple Black-Box Predictions](https://arxiv.org/abs/2509.21707)
*Jiawei Shan,Yiming Dong,Jiwei Zhao*

Main category: stat.ML

TL;DR: 提出一种安全自适应聚合多个黑盒预测的方法，在未知预测质量的情况下保持有效统计推断，保证不差于仅使用标注数据，并在有完美预测时达到更优性能


<details>
  <summary>Details</summary>
Motivation: 现实应用中标注数据稀缺而成本高，未标注数据丰富，机器学习技术发展使得能够生成多种预测标签，需要安全有效地利用这些预测

Method: 提出新颖算法安全自适应地聚合多个黑盒预测，在未知预测质量情况下保持统计推断有效性

Result: 实验在合成和基准数据集上验证了算法的有效性

Conclusion: 该方法能安全利用多种预测源，保证性能不退化，并在有高质量预测时实现更优收敛速度或半参数效率界

Abstract: Real-world applications often face scarce labeled data due to the high cost
and time requirements of gold-standard experiments, whereas unlabeled data are
typically abundant. With the growing adoption of machine learning techniques,
it has become increasingly feasible to generate multiple predicted labels using
a variety of models and algorithms, including deep learning, large language
models, and generative AI. In this paper, we propose a novel approach that
safely and adaptively aggregates multiple black-box predictions with unknown
quality while preserving valid statistical inference. Our method provides two
key guarantees: (i) it never performs worse than using the labeled data alone,
regardless of the quality of the predictions; and (ii) if any one of the
predictions (without knowing which one) perfectly fits the ground truth, the
algorithm adaptively exploits this to achieve either a faster convergence rate
or the semiparametric efficiency bound. We demonstrate the effectiveness of the
proposed algorithm through experiments on both synthetic and benchmark
datasets.

</details>


### [28] [Multi-modal Bayesian Neural Network Surrogates with Conjugate Last-Layer Estimation](https://arxiv.org/abs/2509.21711)
*Ian Taylor,Juliane Mueller,Julie Bessac*

Main category: stat.ML

TL;DR: 开发了两种多模态贝叶斯神经网络代理模型，利用条件共轭分布和随机变分推理来处理部分缺失观测，相比单模态模型在预测精度和不确定性量化方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着数据收集和模拟能力的进步，多模态学习变得越来越重要。多模态代理模型可以从多个辅助模态的数据中学习，支持对昂贵感兴趣量的建模，有助于优化、逆问题或敏感性分析等外循环应用。

Method: 开发了两种多模态贝叶斯神经网络代理模型，利用最后一层的条件共轭分布，使用随机变分推理（SVI）来估计模型参数，并提供了在部分缺失观测情况下进行共轭SVI估计的方法。

Result: 与单模态代理模型相比，在标量和时间序列数据上都展示了改进的预测精度和不确定性量化。

Conclusion: 多模态贝叶斯神经网络代理模型在处理多模态数据和部分缺失观测时表现优越，为外循环应用提供了有效的支持。

Abstract: As data collection and simulation capabilities advance, multi-modal learning,
the task of learning from multiple modalities and sources of data, is becoming
an increasingly important area of research. Surrogate models that learn from
data of multiple auxiliary modalities to support the modeling of a highly
expensive quantity of interest have the potential to aid outer loop
applications such as optimization, inverse problems, or sensitivity analyses
when multi-modal data are available. We develop two multi-modal Bayesian neural
network surrogate models and leverage conditionally conjugate distributions in
the last layer to estimate model parameters using stochastic variational
inference (SVI). We provide a method to perform this conjugate SVI estimation
in the presence of partially missing observations. We demonstrate improved
prediction accuracy and uncertainty quantification compared to uni-modal
surrogate models for both scalar and time series data.

</details>


### [29] [Causal-EPIG: A Prediction-Oriented Active Learning Framework for CATE Estimation](https://arxiv.org/abs/2509.21866)
*Erdun Gao,Jake Fawkes,Dino Sejdinovic*

Main category: stat.ML

TL;DR: 论文提出因果目标对齐原则，通过Causal-EPIG框架将主动学习策略从减少模型参数不确定性转向直接针对不可观测的因果量（如潜在结果和CATE），并推导出两种权衡策略：全面建模因果机制和直接针对CATE估计。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习策略存在目标不匹配问题，它们旨在减少模型参数或可观测事实结果的不确定性，而非直接针对真正的因果关注对象——不可观测的因果量。

Method: 引入因果目标对齐原则，提出Causal-EPIG框架，基于期望预测信息增益(EPIG)准则量化查询对减少不可观测因果量不确定性的价值，并推导出两种策略：全面方法（联合建模潜在结果）和聚焦方法（直接针对CATE估计）。

Result: 实验表明，所提策略持续优于标准基线方法，并揭示最优策略具有上下文依赖性，取决于基础估计器和数据复杂性。

Conclusion: 该框架为实践中样本高效的CATE估计提供了原则性指导，强调应根据具体情境选择适当的主动学习策略。

Abstract: Estimating the Conditional Average Treatment Effect (CATE) is often
constrained by the high cost of obtaining outcome measurements, making active
learning essential. However, conventional active learning strategies suffer
from a fundamental objective mismatch. They are designed to reduce uncertainty
in model parameters or in observable factual outcomes, failing to directly
target the unobservable causal quantities that are the true objects of
interest. To address this misalignment, we introduce the principle of causal
objective alignment, which posits that acquisition functions should target
unobservable causal quantities, such as the potential outcomes and the CATE,
rather than indirect proxies. We operationalize this principle through the
Causal-EPIG framework, which adapts the information-theoretic criterion of
Expected Predictive Information Gain (EPIG) to explicitly quantify the value of
a query in terms of reducing uncertainty about unobservable causal quantities.
From this unified framework, we derive two distinct strategies that embody a
fundamental trade-off: a comprehensive approach that robustly models the full
causal mechanisms via the joint potential outcomes, and a focused approach that
directly targets the CATE estimand for maximum sample efficiency. Extensive
experiments demonstrate that our strategies consistently outperform standard
baselines, and crucially, reveal that the optimal strategy is
context-dependent, contingent on the base estimator and data complexity. Our
framework thus provides a principled guide for sample-efficient CATE estimation
in practice.

</details>


### [30] [Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2509.22633)
*Gen Li,Yuling Yan*

Main category: stat.ML

TL;DR: 本文研究了在线RLHF中的探索策略，发现现有乐观探索算法存在采样效率低的问题，并提出了一种新的探索方案，能够更有效地减少奖励差异的不确定性，从而获得多项式级别的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有的基于乐观探索的在线RLHF算法在采样协议上存在缺陷，倾向于收集无法有效减少奖励差异不确定性的比较数据，导致在指数级长的时间范围内产生线性遗憾。

Method: 提出了一种新的探索方案，将偏好查询导向减少与策略改进最相关的奖励差异不确定性。在RLHF的多臂老虎机模型下，建立了T^{(β+1)/(β+2)}的遗憾界。

Result: 新方法获得了多项式级别的遗憾界，这是首个在所有模型参数上都具有多项式遗憾缩放的在线RLHF算法。

Conclusion: 通过将探索重点放在与策略改进最相关的奖励差异不确定性上，可以显著提高在线RLHF的数据效率，实现更好的遗憾性能。

Abstract: Reinforcement learning with human feedback (RLHF), which learns a reward
model from human preference data and then optimizes a policy to favor preferred
responses, has emerged as a central paradigm for aligning large language models
(LLMs) with human preferences. In this paper, we investigate exploration
principles for online RLHF, where one seeks to adaptively collect new
preference data to refine both the reward model and the policy in a
data-efficient manner. By examining existing optimism-based exploration
algorithms, we identify a drawback in their sampling protocol: they tend to
gather comparisons that fail to reduce the most informative uncertainties in
reward differences, and we prove lower bounds showing that such methods can
incur linear regret over exponentially long horizons. Motivated by this
insight, we propose a new exploration scheme that directs preference queries
toward reducing uncertainty in reward differences most relevant to policy
improvement. Under a multi-armed bandit model of RLHF, we establish regret
bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter
that balances reward maximization against mitigating distribution shift. To our
knowledge, this is the first online RLHF algorithm with regret scaling
polynomially in all model parameters.

</details>


### [31] [A Nonparametric Discrete Hawkes Model with a Collapsed Gaussian-Process Prior](https://arxiv.org/abs/2509.21996)
*Trinnhallen Brisley,Gordon Ross,Daniel Paulin*

Main category: stat.ML

TL;DR: 提出了高斯过程离散霍克斯过程（GP-DHP），一种非参数框架，将高斯过程先验置于基线和激励上，通过折叠潜在表示进行推理，实现了灵活的自激励建模，同时保持可扩展性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有离散时间霍克斯模型通常受限于固定形式的基线和激励核，缺乏对基线和激励的灵活非参数处理，特别是在规则网格上记录计数数据的应用中。

Method: 在基线和激励上放置高斯过程先验，通过折叠潜在表示进行推理，实现最大后验估计，具有近线性时间O(T log T)复杂度，通过闭式投影从优化的潜在轨迹恢复可解释的基线和激励函数。

Result: 在模拟中成功恢复了多样化的激励形状和演化基线；在美国恐怖主义事件和隐孢子虫病周计数案例研究中，相比标准参数化离散霍克斯基线，提高了测试预测对数似然，同时捕捉到爆发、延迟和季节性背景变化。

Conclusion: 结果表明，灵活的自激励建模可以在不牺牲可扩展性或可解释性的情况下实现，为离散时间自激励过程提供了强大的非参数框架。

Abstract: Hawkes process models are used in settings where past events increase the
likelihood of future events occurring. Many applications record events as
counts on a regular grid, yet discrete-time Hawkes models remain comparatively
underused and are often constrained by fixed-form baselines and excitation
kernels. In particular, there is a lack of flexible, nonparametric treatments
of both the baseline and the excitation in discrete time. To this end, we
propose the Gaussian Process Discrete Hawkes Process (GP-DHP), a nonparametric
framework that places Gaussian process priors on both the baseline and the
excitation and performs inference through a collapsed latent representation.
This yields smooth, data-adaptive structure without prespecifying trends,
periodicities, or decay shapes, and enables maximum a posteriori (MAP)
estimation with near-linear-time \(O(T\log T)\) complexity. A closed-form
projection recovers interpretable baseline and excitation functions from the
optimized latent trajectory. In simulations, GP-DHP recovers diverse excitation
shapes and evolving baselines. In case studies on U.S. terrorism incidents and
weekly Cryptosporidiosis counts, it improves test predictive log-likelihood
over standard parametric discrete Hawkes baselines while capturing bursts,
delays, and seasonal background variation. The results indicate that flexible
discrete-time self-excitation can be achieved without sacrificing scalability
or interpretability.

</details>


### [32] [Incorporating priors in learning: a random matrix study under a teacher-student framework](https://arxiv.org/abs/2509.22124)
*Malik Tiomoko,Ekkehard Schnoor*

Main category: stat.ML

TL;DR: 本文首次提供了带高斯先验的MAP线性回归在训练和测试风险上的精确渐近特征，统一了岭回归、最小二乘和先验信息估计器，揭示了偏差-方差-先验权衡，解释了双下降现象，并量化了先验失配。


<details>
  <summary>Details</summary>
Motivation: 正则化线性回归在机器学习中至关重要，但具有信息先验的高维行为仍未被充分理解。需要建立统一框架来连接贝叶斯先验、经典正则化和现代渐近理论。

Method: 使用随机矩阵理论，为以领域信息初始化为中心的高斯先验MAP回归提供精确渐近分析，推导出闭式风险公式。

Result: 获得了训练和测试风险的闭式表达式，识别了测试风险的最小化闭式解，能够简单估计最优正则化参数，仿真验证了理论的高精度。

Conclusion: 通过连接贝叶斯先验、经典正则化和现代渐近理论，结果为结构化先验知识的学习提供了概念清晰性和实践指导。

Abstract: Regularized linear regression is central to machine learning, yet its
high-dimensional behavior with informative priors remains poorly understood. We
provide the first exact asymptotic characterization of training and test risks
for maximum a posteriori (MAP) regression with Gaussian priors centered at a
domain-informed initialization. Our framework unifies ridge regression, least
squares, and prior-informed estimators, and -- using random matrix theory --
yields closed-form risk formulas that expose the bias-variance-prior tradeoff,
explain double descent, and quantify prior mismatch. We also identify a
closed-form minimizer of test risk, enabling a simple estimator of the optimal
regularization parameter. Simulations confirm the theory with high accuracy. By
connecting Bayesian priors, classical regularization, and modern asymptotics,
our results provide both conceptual clarity and practical guidance for learning
with structured prior knowledge.

</details>


### [33] [Multidimensional Uncertainty Quantification via Optimal Transport](https://arxiv.org/abs/2509.22380)
*Nikita Kotelevskii,Maiya Goloburda,Vladimir Kondratyev,Alexander Fishkov,Mohsen Guizani,Eric Moulines,Maxim Panov*

Main category: stat.ML

TL;DR: VecUQ-OT是一个多维不确定性量化框架，通过将互补的不确定性度量堆叠成向量，并使用基于最优传输的排序方法来产生Monge-Kantorovich秩，从而提供更稳健的不确定性排序。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性量化方法通常提供单一标量值来衡量模型可靠性，但不同的不确定性度量可能提供互补信息，即使针对相同类型的不确定性也可能捕捉到不同的失败模式。

Method: 将互补的不确定性度量堆叠成向量，使用熵正则化最优传输学习传输映射，基于分布内数据的得分向量，设计适用于未见输入（包括分布外情况）的排序方法。

Result: 在合成、图像和文本数据上的实验表明，VecUQ-OT即使在单个度量失效时也表现出高效率，支持选择性预测、误分类检测、分布外检测和选择性生成等下游任务。

Conclusion: VecUQ-OT提供了一个灵活的非加性不确定性融合框架，能够产生稳健的排序，在各种下游任务中表现出色，代码已开源。

Abstract: Most uncertainty quantification (UQ) approaches provide a single scalar value
as a measure of model reliability. However, different uncertainty measures
could provide complementary information on the prediction confidence. Even
measures targeting the same type of uncertainty (e.g., ensemble-based and
density-based measures of epistemic uncertainty) may capture different failure
modes.
  We take a multidimensional view on UQ by stacking complementary UQ measures
into a vector. Such vectors are assigned with Monge-Kantorovich ranks produced
by an optimal-transport-based ordering method. The prediction is then deemed
more uncertain than the other if it has a higher rank.
  The resulting VecUQ-OT algorithm uses entropy-regularized optimal transport.
The transport map is learned on vectors of scores from in-distribution data
and, by design, applies to unseen inputs, including out-of-distribution cases,
without retraining.
  Our framework supports flexible non-additive uncertainty fusion (including
aleatoric and epistemic components). It yields a robust ordering for downstream
tasks such as selective prediction, misclassification detection,
out-of-distribution detection, and selective generation. Across synthetic,
image, and text data, VecUQ-OT shows high efficiency even when individual
measures fail. The code for the method is available at:
https://github.com/stat-ml/multidimensional_uncertainty.

</details>


### [34] [Universal Inverse Distillation for Matching Models with Real-Data Supervision (No GANs)](https://arxiv.org/abs/2509.22459)
*Nikita Kornilov,David Li,Tikhon Mavrin,Aleksei Leonov,Nikita Gushchin,Evgeny Burnaev,Iaroslav Koshelev,Alexander Korotin*

Main category: stat.ML

TL;DR: RealUID是一个通用的匹配模型蒸馏框架，能够无缝整合真实数据到蒸馏过程中，无需使用GANs，解决了现有方法局限于特定框架和数据依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 现代扩散、流匹配等模型虽然生成质量优秀，但推理速度慢。现有蒸馏方法局限于特定框架且需要复杂对抗训练，限制了其应用范围。

Method: 提出RealUID通用蒸馏框架，基于简单理论基础，覆盖Flow Matching和Diffusion模型及其变体，无需额外判别器即可利用真实数据。

Result: 该框架能够统一处理多种匹配模型，包括Bridge Matching和Stochastic Interpolants等变体，简化了蒸馏过程。

Conclusion: RealUID提供了一个简单有效的通用蒸馏解决方案，突破了现有方法的框架限制和数据依赖问题。

Abstract: While achieving exceptional generative quality, modern diffusion, flow, and
other matching models suffer from slow inference, as they require many steps of
iterative generation. Recent distillation methods address this by training
efficient one-step generators under the guidance of a pre-trained teacher
model. However, these methods are often constrained to only one specific
framework, e.g., only to diffusion or only to flow models. Furthermore, these
methods are naturally data-free, and to benefit from the usage of real data, it
is required to use an additional complex adversarial training with an extra
discriminator model. In this paper, we present RealUID, a universal
distillation framework for all matching models that seamlessly incorporates
real data into the distillation procedure without GANs. Our RealUID approach
offers a simple theoretical foundation that covers previous distillation
methods for Flow Matching and Diffusion models, and is also extended to their
modifications, such as Bridge Matching and Stochastic Interpolants.

</details>


### [35] [CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks](https://arxiv.org/abs/2509.22467)
*Alejandro Almodóvar,Patricia A. Apellániz,Santiago Zazo,Juan Parras*

Main category: stat.ML

TL;DR: causalKANs框架将因果神经网络的CATE估计器转换为Kolmogorov-Arnold Networks，通过剪枝和符号简化获得可解释的闭式公式，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在估计异质处理效应方面表现优异，但其不透明性限制了在医学、经济学和公共政策等敏感领域的信任和采用。

Method: 基于成熟的因果神经架构，将条件平均处理效应估计器转换为KANs，结合剪枝和符号简化技术。

Result: 在基准数据集上的实验表明，causalKANs在CATE误差指标上与神经基线方法表现相当，即使简单的KAN变体也能达到竞争性性能。

Conclusion: causalKANs通过结合可靠性和分析可访问性，提供了由闭式表达式和可解释图支持的可审计估计器，使高风险环境中的个体化决策更加可信。

Abstract: Deep neural networks achieve state-of-the-art performance in estimating
heterogeneous treatment effects, but their opacity limits trust and adoption in
sensitive domains such as medicine, economics, and public policy. Building on
well-established and high-performing causal neural architectures, we propose
causalKANs, a framework that transforms neural estimators of conditional
average treatment effects (CATEs) into Kolmogorov--Arnold Networks (KANs). By
incorporating pruning and symbolic simplification, causalKANs yields
interpretable closed-form formulas while preserving predictive accuracy.
Experiments on benchmark datasets demonstrate that causalKANs perform on par
with neural baselines in CATE error metrics, and that even simple KAN variants
achieve competitive performance, offering a favorable
accuracy--interpretability trade-off. By combining reliability with analytic
accessibility, causalKANs provide auditable estimators supported by closed-form
expressions and interpretable plots, enabling trustworthy individualized
decision-making in high-stakes settings. We release the code for
reproducibility at https://github.com/aalmodovares/causalkans .

</details>


### [36] [Smoothing-Based Conformal Prediction for Balancing Efficiency and Interpretability](https://arxiv.org/abs/2509.22529)
*Mingyi Zheng,Hongyu Jiang,Yizhou Lu,Jiaye Teng*

Main category: stat.ML

TL;DR: 提出了SCD-split方法，在保形预测框架中引入平滑操作，将多个不连通的子区间合并为可解释的预测集，在保持覆盖保证和区间长度的同时减少不连通子区间数量。


<details>
  <summary>Details</summary>
Motivation: 现有的保形预测方法如CD-split虽然提高了效率，但产生的预测集通常由多个不连通的子区间组成，难以解释。

Method: 在保形预测框架中引入平滑操作，帮助合并子区间，从而获得更可解释的预测集。

Result: 在合成和真实数据集上的实验表明，SCD-split能够平衡区间长度和不连通子区间数量。理论上，在特定条件下，SCD-split可证明减少不连通子区间数量，同时保持与CD-split相当的覆盖保证和区间长度。

Conclusion: SCD-split方法通过平滑操作有效解决了保形预测中预测集不连通的问题，在保持统计性能的同时提高了可解释性。

Abstract: Conformal Prediction (CP) is a distribution-free framework for constructing
statistically rigorous prediction sets. While popular variants such as CD-split
improve CP's efficiency, they often yield prediction sets composed of multiple
disconnected subintervals, which are difficult to interpret. In this paper, we
propose SCD-split, which incorporates smoothing operations into the CP
framework. Such smoothing operations potentially help merge the subintervals,
thus leading to interpretable prediction sets. Experimental results on both
synthetic and real-world datasets demonstrate that SCD-split balances the
interval length and the number of disconnected subintervals. Theoretically,
under specific conditions, SCD-split provably reduces the number of
disconnected subintervals while maintaining comparable coverage guarantees and
interval length compared with CD-split.

</details>


### [37] [Debiased Front-Door Learners for Heterogeneous Effects](https://arxiv.org/abs/2509.22531)
*Yonghan Jung*

Main category: stat.ML

TL;DR: 提出了两种基于前门调整的去偏学习器（FD-DR-Learner和FD-R-Learner），用于在存在未测量混杂因素的情况下估计异质性处理效应，即使干扰函数收敛速度较慢也能达到准oracle速率。


<details>
  <summary>Details</summary>
Motivation: 在观测性研究中，当处理和结果存在未测量的混杂因素但观测到的中介变量无混杂时，前门调整可以通过中介变量识别因果效应。需要开发能够在这种设置下可靠估计异质性处理效应的方法。

Method: 引入了两种去偏学习器：FD-DR-Learner和FD-R-Learner，基于前门调整框架，即使在干扰函数收敛速度仅为n^-1/4时也能实现去偏。

Result: 在合成研究和真实世界案例（使用FARS数据集分析安全带法律）中表现出稳健的实证性能，实现了准oracle速率，提供了可靠的异质性处理效应估计。

Conclusion: 所提出的学习器在前门调整场景下能够提供可靠且样本效率高的异质性处理效应估计，代码已在GitHub上开源。

Abstract: In observational settings where treatment and outcome share unmeasured
confounders but an observed mediator remains unconfounded, the front-door (FD)
adjustment identifies causal effects through the mediator. We study the
heterogeneous treatment effect (HTE) under FD identification and introduce two
debiased learners: FD-DR-Learner and FD-R-Learner. Both attain fast,
quasi-oracle rates (i.e., performance comparable to an oracle that knows the
nuisances) even when nuisance functions converge as slowly as n^-1/4. We
provide error analyses establishing debiasedness and demonstrate robust
empirical performance in synthetic studies and a real-world case study of
primary seat-belt laws using Fatality Analysis Reporting System (FARS) dataset.
Together, these results indicate that the proposed learners deliver reliable
and sample-efficient HTE estimates in FD scenarios. The implementation is
available at https://github.com/yonghanjung/FD-CATE.
  Keywords: Front-door adjustment; Heterogeneous treatment effects; Debiased
learning; Quasi-oracle rates; Causal inference.

</details>


### [38] [Metrics for Parametric Families of Networks](https://arxiv.org/abs/2509.22549)
*Mario Gómez,Guanqun Ma,Tom Needham,Bei Wang*

Main category: stat.ML

TL;DR: 提出了一个参数化Gromov-Wasserstein距离框架，用于比较参数化网络数据，包括时间变化度量空间、演化社交网络和随机图模型，并建立了理论保证和计算下界。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对参数化网络数据的统一比较框架，需要开发能够处理时间变化网络、演化社交网络和随机图模型等参数化数据的距离度量方法。

Method: 基于Gromov-Wasserstein最优传输变体，定义参数化Gromov-Wasserstein距离族，建立理论性质，开发计算下界，并与图统计量关联。

Result: 证明了该距离包含多个现有度量，建立了理论近似保证，开发了可计算下界，并在随机图设置中证明了经验估计的一致性。

Conclusion: 该框架为参数化网络数据提供了统一的比较方法，具有理论保证和实际应用价值，通过数值实验验证了其有效性。

Abstract: We introduce a general framework for analyzing data modeled as parameterized
families of networks. Building on a Gromov-Wasserstein variant of optimal
transport, we define a family of parameterized Gromov-Wasserstein distances for
comparing such parametric data, including time-varying metric spaces induced by
collective motion, temporally evolving weighted social networks, and random
graph models. We establish foundational properties of these distances, showing
that they subsume several existing metrics in the literature, and derive
theoretical approximation guarantees. In particular, we develop computationally
tractable lower bounds and relate them to graph statistics commonly used in
random graph theory. Furthermore, we prove that our distances can be
consistently approximated in random graph and random metric space settings via
empirical estimates from generative models. Finally, we demonstrate the
practical utility of our framework through a series of numerical experiments.

</details>


### [39] [Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement](https://arxiv.org/abs/2509.22553)
*Hao Chen,Lin Liu,Yu Guang Wang*

Main category: stat.ML

TL;DR: 提出了一种新的线性因果表示学习算法，在较弱的假设条件下仍能恢复潜在的因果特征，并在合成实验和大型语言模型可解释性分析中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的线性因果表示学习方法通常依赖严格的假设，如需要单节点干预数据或对潜在特征和外生测量噪声施加限制性分布约束，这些前提在某些场景中难以满足。

Method: 开发了一种新的线性因果表示学习算法，在线性结构因果模型和线性混合函数的框架下，基于较弱的环境异质性和数据生成分布假设进行操作。

Result: 该算法在有限样本中优于竞争方法，能够恢复潜在因果特征至等价类，并在大型语言模型的可解释性分析中展示了将因果关系整合到人工智能中的潜力。

Conclusion: 提出的线性因果表示学习算法在较弱假设下仍能有效工作，为将因果关系整合到人工智能系统提供了有前景的方法。

Abstract: Causal representation learning (CRL) has garnered increasing interests from
the causal inference and artificial intelligence community, due to its
capability of disentangling potentially complex data-generating mechanism into
causally interpretable latent features, by leveraging the heterogeneity of
modern datasets. In this paper, we further contribute to the CRL literature, by
focusing on the stylized linear structural causal model over the latent
features and assuming a linear mixing function that maps latent features to the
observed data or measurements. Existing linear CRL methods often rely on
stringent assumptions, such as accessibility to single-node interventional data
or restrictive distributional constraints on latent features and exogenous
measurement noise. However, these prerequisites can be challenging to satisfy
in certain scenarios. In this work, we propose a novel linear CRL algorithm
that, unlike most existing linear CRL methods, operates under weaker
assumptions about environment heterogeneity and data-generating distributions
while still recovering latent causal features up to an equivalence class. We
further validate our new algorithm via synthetic experiments and an
interpretability analysis of large language models (LLMs), demonstrating both
its superiority over competing methods in finite samples and its potential in
integrating causality into AI.

</details>
