{"id": "2510.08758", "categories": ["stat.ME", "cs.CL", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.08758", "abs": "https://arxiv.org/abs/2510.08758", "authors": ["Graham Tierney", "Srikar Katta", "Christopher Bail", "Sunshine Hillygus", "Alexander Volfovsky"], "title": "A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?", "comment": null, "summary": "Many social science questions ask how linguistic properties causally affect\nan audience's attitudes and behaviors. Because text properties are often\ninterlinked (e.g., angry reviews use profane language), we must control for\npossible latent confounding to isolate causal effects. Recent literature\nproposes adapting large language models (LLMs) to learn latent representations\nof text that successfully predict both treatment and the outcome. However,\nbecause the treatment is a component of the text, these deep learning methods\nrisk learning representations that actually encode the treatment itself,\ninducing overlap bias. Rather than depending on post-hoc adjustments, we\nintroduce a new experimental design that handles latent confounding, avoids the\noverlap issue, and unbiasedly estimates treatment effects. We apply this design\nin an experiment evaluating the persuasiveness of expressing humility in\npolitical communication. Methodologically, we demonstrate that LLM-based\nmethods perform worse than even simple bag-of-words models using our real text\nand outcomes from our experiment. Substantively, we isolate the causal effect\nof expressing humility on the perceived persuasiveness of political statements,\noffering new insights on communication effects for social media platforms,\npolicy makers, and social scientists."}
{"id": "2510.08838", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.08838", "abs": "https://arxiv.org/abs/2510.08838", "authors": ["Ziyi Song", "Federico Camerlenghi", "Weining Shen", "Michele Guindani", "Mario Beraha"], "title": "Repulsive Mixture Model with Projection Determinantal Point Process", "comment": null, "summary": "In many scientific domains, clustering aims to reveal interpretable latent\nstructure that reflects relevant subpopulations or processes. Widely used\nBayesian mixture models for model-based clustering often produce overlapping or\nredundant components because priors on cluster locations are specified\nindependently, hindering interpretability. To mitigate this, repulsive priors\nhave been proposed to encourage well-separated components, yet existing\napproaches face both computational and theoretical challenges. We introduce a\nfully tractable Bayesian repulsive mixture model by assigning a projection\nDeterminantal Point Process (DPP) prior to the component locations. Projection\nDPPs induce strong repulsion and allow exact sampling, enabling parsimonious\nand interpretable posterior clustering. Leveraging their analytical\ntractability, we derive closed-form posterior and predictive distributions.\nThese results, in turn, enable two efficient inference algorithms: a\nconditional Gibbs sampler and the first fully implementable marginal sampler\nfor DPP-based mixtures. We also provide strong frequentist guarantees,\nincluding posterior consistency for density estimation, elimination of\nredundant components, and contraction of the mixing measure. Simulation studies\nconfirm superior mixing and clustering performance compared to alternatives in\nmisspecified settings. Finally, we demonstrate the utility of our method on\nevent-related potential functional data, where it uncovers interpretable\nneuro-cognitive subgroups. Our results support the projection DPP mixtures as a\ntheoretically sound and practically effective solution for Bayesian clustering."}
{"id": "2510.08853", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.08853", "abs": "https://arxiv.org/abs/2510.08853", "authors": ["Caitlin H. Daly", "Chloe Tan", "Audrey Béliveau"], "title": "Uncovering All Highly Credible Binary Treatment Hierarchy Questions in Network Meta-Analysis", "comment": "14 pages, 4 figures, 1 table", "summary": "In recent years, there has been growing research interest in addressing\ntreatment hierarchy questions within network meta-analysis (NMA). In NMAs\ninvolving many treatments, the number of possible hierarchy questions becomes\nprohibitively large. To manage this complexity, previous work has recommended\npre-selecting specific hierarchy questions of interest (e.g., ``among options\nA, B, C, D, E, do treatments A and B have the two best effects in terms of\nimproving outcome X?\") and calculating the empirical probabilities of the\nanswers being true given the data. In contrast, we propose an efficient and\nscalable algorithmic approach that eliminates the need for pre-specification by\nsystematically generating a comprehensive catalog of highly credible treatment\nhierarchy questions, specifically, those with empirical probabilities exceeding\na chosen threshold (e.g., 95%). This enables decision-makers to extract all\nmeaningful insights supported by the data. An additional algorithm trims\nredundant insights from the output to facilitate interpretation. We define and\naddress six broad types of binary hierarchy questions (i.e., those with\ntrue/false answers), covering standard hierarchy questions answered using\nexisting ranking metrics - pairwise comparisons and (cumulative) ranking\nprobabilities - as well as many other complex hierarchy questions. We have\nimplemented our methods in an R package and illustrate their application using\nreal NMA datasets on diabetes and depression interventions. Beyond NMA, our\napproach is relevant to any decision problem concerning three or more treatment\noptions."}
{"id": "2510.08898", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.08898", "abs": "https://arxiv.org/abs/2510.08898", "authors": ["Soumojit Das", "Dilshanie Deepawansa", "Partha Lahiri"], "title": "Multidimensional Poverty Mapping for Small Areas", "comment": null, "summary": "Many countries measure poverty based only on income or consumption. However,\nthere is a growing awareness of measuring poverty through multiple dimensions\nthat captures a more reasonable status of poverty. Estimating poverty\nmeasure(s) for small geographical areas, commonly referred to as poverty\nmapping, is challenging due to small or no sample for the small areas. While\nthere is a huge literature available on unidimensional poverty mapping, only a\nlimited effort has been made to address special challenges that arise only in\nthe multidimensional poverty mapping. For example, in multidimensional poverty\nmapping, a new problem arises involving estimation of relative contributions of\ndifferent dimensions to overall poverty for small areas. This problem has been\ngrossly ignored in the small area estimation (SAE) literature. We address this\nissue using a multivariate hierarchical model implemented via a Bayesian\nmethod. Moreover, we demonstrate how a multidimensional poverty composite\nmeasure can be estimated for small areas. In this paper, we demonstrate our\nproposed methodology using a survey data specially designed by one of us for\nmultidimensional poverty mapping. This paper adds a new direction to poverty\nmapping literature."}
{"id": "2510.08906", "categories": ["stat.ML", "cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2510.08906", "abs": "https://arxiv.org/abs/2510.08906", "authors": ["Morris Trestman", "Stefan Gugler", "Felix A. Faber", "O. A. von Lilienfeld"], "title": "Gradient-Guided Furthest Point Sampling for Robust Training Set Selection", "comment": "18 pages, 18 figures, journal article", "summary": "Smart training set selections procedures enable the reduction of data needs\nand improves predictive robustness in machine learning problems relevant to\nchemistry. We introduce Gradient Guided Furthest Point Sampling (GGFPS), a\nsimple extension of Furthest Point Sampling (FPS) that leverages molecular\nforce norms to guide efficient sampling of configurational spaces of molecules.\nNumerical evidence is presented for a toy-system (Styblinski-Tang function) as\nwell as for molecular dynamics trajectories from the MD17 dataset. Compared to\nFPS and uniform sampling, our numerical results indicate superior data\nefficiency and robustness when using GGFPS. Distribution analysis of the MD17\ndata suggests that FPS systematically under-samples equilibrium geometries,\nresulting in large test errors for relaxed structures. GGFPS cures this\nartifact and (i) enables up to two fold reductions in training cost without\nsacrificing predictive accuracy compared to FPS in the 2-dimensional\nStyblinksi-Tang system, (ii) systematically lowers prediction errors for\nequilibrium as well as strained structures in MD17, and (iii) systematically\ndecreases prediction error variances across all of the MD17 configuration\nspaces. These results suggest that gradient-aware sampling methods hold great\npromise as effective training set selection tools, and that naive use of FPS\nmay result in imbalanced training and inconsistent prediction outcomes."}
{"id": "2510.08972", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.08972", "abs": "https://arxiv.org/abs/2510.08972", "authors": ["Fan Xia", "K. C. Gary Chan", "Emily Voldal", "Avi Kenny", "Patrick J. Heagerty", "James P. Hughes"], "title": "Robust and Efficient Semiparametric Inference for the Stepped Wedge Design", "comment": null, "summary": "Stepped wedge designs (SWDs) are increasingly used to evaluate longitudinal\ncluster-level interventions but pose substantial challenges for valid\ninference. Because crossover times are randomized, intervention effects are\nintrinsically confounded with secular time trends, while heterogeneity across\nclusters, complex correlation structures, baseline covariate imbalances, and\nsmall numbers of clusters further complicate inference. We propose a unified\nsemiparametric framework for estimating possibly time-varying intervention\neffects in SWDs. Under a semiparametric model on treatment contrast, we develop\na nonstandard semiparametric efficiency theory that accommodates correlated\nobservations within clusters, varying cluster-period sizes, and weakly\ndependent treatment assignments. The resulting estimator is consistent and\nasymptotically normal even under misspecified covariance structure and control\ncluster-period means, and is efficient when both are correctly specified. To\nenable inference with few clusters, we exploit the permutation structure of\ntreatment assignment to propose a standard error estimator that reflects\nfinite-sample variability, with a leave-one-out correction to reduce plug-in\nbias. The framework also allows incorporation of effect modification and\nadjustment for imbalanced precision variables through design-based adjustment\nor double adjustment that additionally incorporates an outcome-based component.\nSimulations and application to a public health trial demonstrate the robustness\nand efficiency of the proposed method relative to standard approaches."}
{"id": "2510.08916", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08916", "abs": "https://arxiv.org/abs/2510.08916", "authors": ["Hideaki Kim", "Tomoharu Iwata"], "title": "A Representer Theorem for Hawkes Processes via Penalized Least Squares Minimization", "comment": "Under review", "summary": "The representer theorem is a cornerstone of kernel methods, which aim to\nestimate latent functions in reproducing kernel Hilbert spaces (RKHSs) in a\nnonparametric manner. Its significance lies in converting inherently\ninfinite-dimensional optimization problems into finite-dimensional ones over\ndual coefficients, thereby enabling practical and computationally tractable\nalgorithms. In this paper, we address the problem of estimating the latent\ntriggering kernels--functions that encode the interaction structure between\nevents--for linear multivariate Hawkes processes based on observed event\nsequences within an RKHS framework. We show that, under the principle of\npenalized least squares minimization, a novel form of representer theorem\nemerges: a family of transformed kernels can be defined via a system of\nsimultaneous integral equations, and the optimal estimator of each triggering\nkernel is expressed as a linear combination of these transformed kernels\nevaluated at the data points. Remarkably, the dual coefficients are all\nanalytically fixed to unity, obviating the need to solve a costly optimization\nproblem to obtain the dual coefficients. This leads to a highly efficient\nestimator capable of handling large-scale data more effectively than\nconventional nonparametric approaches. Empirical evaluations on synthetic\ndatasets reveal that the proposed method attains competitive predictive\naccuracy while substantially improving computational efficiency over existing\nstate-of-the-art kernel method-based estimators."}
{"id": "2510.08974", "categories": ["stat.CO", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.08974", "abs": "https://arxiv.org/abs/2510.08974", "authors": ["Jingwen Song", "Pengfei Wei"], "title": "Bayesian Model Inference using Bayesian Quadrature: the Art of Acquisition Functions and Beyond", "comment": "47 pages, 15 figures, submitted to Elsevier journal", "summary": "Estimating posteriors and the associated model evidences is a core issue of\nBayesian model inference, and can be of great challenge given complex features\nof the posteriors such as multi-modalities of unequal importance, nonlinear\ndependencies and high sharpness. Bayesian Quadrature (BQ) has emerged as a\ncompetitive framework for tackling this challenge, as it provides flexible\nbalance between computational cost and accuracy. The performance of a BQ scheme\nis fundamentally dictated by the acquisition function as it exclusively governs\nthe generation of integration points. After reexamining one of the most\nadvanced acquisition function from a prospective inference perspective and\nreformulating the quadrature rules for prediction, four new acquisition\nfunctions, inspired by distinct intuitions on expected rewards, are primarily\ndeveloped, all of which are accompanied by elegant interpretations and highly\nefficient numerical estimators. Mathematically, these four acquisition\nfunctions measure, respectively, the prediction uncertainty of posterior, the\ncontribution to prediction uncertainty of evidence, as well as the expected\nreduction of prediction uncertainties concerning posterior and evidence, and\nthus provide flexibility for highly effective design of integration points.\nThese acquisition functions are further extended to the transitional BQ scheme,\nalong with several specific refinements, to tackle the above-mentioned\nchallenges with high efficiency and robustness. Effectiveness of the\ndevelopments is ultimately demonstrated with extensive benchmark studies and\napplication to an engineering example."}
{"id": "2510.08893", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.08893", "abs": "https://arxiv.org/abs/2510.08893", "authors": ["Christopher J. Paciorek", "Daniel Cooley"], "title": "Quantifying Very Extreme Precipitation and Temperature Using Huge Ensembles Generated by Machine Learning-based Climate Model Emulators", "comment": "28 pages, 11 figures, 5 appendix figures", "summary": "Weather extremes produce major impacts on society and ecosystems and are\nlikely to change in likelihood and magnitude with climate change. However, very\nlow probability events are hard to characterize statistically using\nobservations or climate model output because of short records/runs. For\nprecipitation, consideration of such events arises in quantifying Probable\nMaximum Precipitation (PMP), namely estimating extreme precipitation magnitudes\nfor designing and assessing critical infrastructure. A recent National\nAcademies report on modernizing PMP estimation proposed using huge climate\nmodel-based ensembles to estimate extreme quantiles, possibly through machine\nlearning-based ensemble boosting. Here we assess such an approach for the\ncontiguous United States using a huge ensemble (10560 years) from a\nstate-of-the-art emulator (ACE2) trained on ERA5 reanalysis. The results\nindicate that one can practically estimate very extreme precipitation and\ntemperature quantiles using appropriate statistical extreme value techniques.\nMore specifically, the results provide evidence for (1) the use of\nthreshold-exceedance methods with a sufficiently high threshold for reliable\nestimation (necessary for precipitation), (2) the robustness of results to\nvariations in extremes by season and storm type, and (3) well-constrained\nstatistical uncertainty. Our results also show that the emulator produces\nextremes outside the range of the ERA5 training data. While this suggests that\nsuch emulators have potential for quantifying the climatology of extremes, we\ndo not extensively investigate if this particular emulator is fit for purpose.\nOur focus is on how to use huge ensembles to estimate very extreme statistics,\nand we expect the results to be relevant for future improved emulators."}
{"id": "2510.09024", "categories": ["stat.ME", "62H05 (Primary), 62C10, 05C90 (Secondary)", "G.2.2; G.3"], "pdf": "https://arxiv.org/pdf/2510.09024", "abs": "https://arxiv.org/abs/2510.09024", "authors": ["Pei Heng", "Yi Sun", "Shiyuan He", "Jianhua Guo"], "title": "Revisiting Madigan and Mosurski: Collapsibility via Minimal Separators", "comment": "8 pages, 3 figures, submitted to Biometrika. Code available at\n  https://github.com/Balance-H/Algorithms", "summary": "Collapsibility provides a principled approach for dimension reduction in\ncontingency tables and graphical models. Madigan and Mosurski (1990) pioneered\nthe study of minimal collapsible sets in decomposable models, but existing\nalgorithms for general graphs remain computationally demanding. We show that a\nmodel is collapsible onto a target set precisely when that set contains all\nminimal separators between its non-adjacent vertices. This insight motivates\nthe Close Minimal Separator Absorption (CMSA) algorithm, which constructs\nminimal collapsible sets using only local separator searches at very low costs.\nSimulations confirm substantial efficiency gains, making collapsibility\nanalysis practical in high-dimensional settings."}
{"id": "2510.08929", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08929", "abs": "https://arxiv.org/abs/2510.08929", "authors": ["Yunrui Guan", "Krishnakumar Balasubramanian", "Shiqian Ma"], "title": "Mirror Flow Matching with Heavy-Tailed Priors for Generative Modeling on Convex Domains", "comment": null, "summary": "We study generative modeling on convex domains using flow matching and mirror\nmaps, and identify two fundamental challenges. First, standard log-barrier\nmirror maps induce heavy-tailed dual distributions, leading to ill-posed\ndynamics. Second, coupling with Gaussian priors performs poorly when matching\nheavy-tailed targets. To address these issues, we propose Mirror Flow Matching\nbased on a \\emph{regularized mirror map} that controls dual tail behavior and\nguarantees finite moments, together with coupling to a Student-$t$ prior that\naligns with heavy-tailed targets and stabilizes training. We provide\ntheoretical guarantees, including spatial Lipschitzness and temporal regularity\nof the velocity field, Wasserstein convergence rates for flow matching with\nStudent-$t$ priors and primal-space guarantees for constrained generation,\nunder $\\varepsilon$-accurate learned velocity fields. Empirically, our method\noutperforms baselines in synthetic convex-domain simulations and achieves\ncompetitive sample quality on real-world constrained generative tasks."}
{"id": "2510.09422", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2510.09422", "abs": "https://arxiv.org/abs/2510.09422", "authors": ["Yi Zhang", "Yiting Duan", "Xiangjun Wang", "Zhikun Zhang"], "title": "Solving Fokker-Planck-Kolmogorov Equation by Distribution Self-adaptation Normalized Physics-informed Neural Networks", "comment": null, "summary": "Stochastic dynamical systems provide essential mathematical frameworks for\nmodeling complex real-world phenomena. The Fokker-Planck-Kolmogorov (FPK)\nequation governs the evolution of probability density functions associated with\nstochastic system trajectories. Developing robust numerical methods for solving\nthe FPK equation is critical for understanding and predicting stochastic\nbehavior. Here, we introduce the distribution self-adaptive normalized\nphysics-informed neural network (DSN-PINNs) for solving time-dependent FPK\nequations through the integration of soft normalization constraints with\nadaptive resampling strategies. Specifically, we employ a\nnormalization-enhanced PINN model in a pretraining phase to establish the\nsolution's global structure and scale, generating a reliable prior\ndistribution. Subsequently, guided by this prior, we dynamically reallocate\ntraining points via weighted kernel density estimation, concentrating\ncomputational resources on regions most representative of the underlying\nprobability distribution throughout the learning process. The key innovation\nlies in our method's ability to exploit the intrinsic structural properties of\nstochastic dynamics while maintaining computational accuracy and implementation\nsimplicity. We demonstrate the framework's effectiveness through comprehensive\nnumerical experiments and comparative analyses with existing methods, including\nvalidation on real-world economic datasets."}
{"id": "2510.08758", "categories": ["stat.ME", "cs.CL", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.08758", "abs": "https://arxiv.org/abs/2510.08758", "authors": ["Graham Tierney", "Srikar Katta", "Christopher Bail", "Sunshine Hillygus", "Alexander Volfovsky"], "title": "A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?", "comment": null, "summary": "Many social science questions ask how linguistic properties causally affect\nan audience's attitudes and behaviors. Because text properties are often\ninterlinked (e.g., angry reviews use profane language), we must control for\npossible latent confounding to isolate causal effects. Recent literature\nproposes adapting large language models (LLMs) to learn latent representations\nof text that successfully predict both treatment and the outcome. However,\nbecause the treatment is a component of the text, these deep learning methods\nrisk learning representations that actually encode the treatment itself,\ninducing overlap bias. Rather than depending on post-hoc adjustments, we\nintroduce a new experimental design that handles latent confounding, avoids the\noverlap issue, and unbiasedly estimates treatment effects. We apply this design\nin an experiment evaluating the persuasiveness of expressing humility in\npolitical communication. Methodologically, we demonstrate that LLM-based\nmethods perform worse than even simple bag-of-words models using our real text\nand outcomes from our experiment. Substantively, we isolate the causal effect\nof expressing humility on the perceived persuasiveness of political statements,\noffering new insights on communication effects for social media platforms,\npolicy makers, and social scientists."}
{"id": "2510.09276", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.09276", "abs": "https://arxiv.org/abs/2510.09276", "authors": ["Camille M. Montalcini", "Peter J. Rousseeuw"], "title": "The bixplot: A variation on the boxplot suited for bimodal data", "comment": null, "summary": "Boxplots and related visualization methods are widely used exploratory tools\nfor taking a first look at collections of univariate variables. In this note an\nextension is provided that is specifically designed to detect and display\nbimodality and multimodality when the data warrant it. For this purpose a\nunivariate clustering method is constructed that ensures contiguous clusters,\nmeaning that no cluster has members inside another cluster, and such that each\ncluster contains at least a given number of unique members. The resulting\nbixplot display facilitates the identification and interpretation of\npotentially meaningful subgroups underlying the data. The bixplot also displays\nthe individual data values, which can draw attention to isolated points.\nImplementations of the bixplot are available in both Python and R, and their\nmany options are illustrated on several real datasets. For instance, an\nexternal variable can be visualized by color gradations inside the display."}
{"id": "2510.09177", "categories": ["stat.ML", "cs.LG", "math.FA", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.09177", "abs": "https://arxiv.org/abs/2510.09177", "authors": ["Mihriban Ceylan", "David J. Prömel"], "title": "Distributionally robust approximation property of neural networks", "comment": null, "summary": "The universal approximation property uniformly with respect to weakly compact\nfamilies of measures is established for several classes of neural networks. To\nthat end, we prove that these neural networks are dense in Orlicz spaces,\nthereby extending classical universal approximation theorems even beyond the\ntraditional $L^p$-setting. The covered classes of neural networks include\nwidely used architectures like feedforward neural networks with non-polynomial\nactivation functions, deep narrow networks with ReLU activation functions and\nfunctional input neural networks."}
{"id": "2510.08853", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.08853", "abs": "https://arxiv.org/abs/2510.08853", "authors": ["Caitlin H. Daly", "Chloe Tan", "Audrey Béliveau"], "title": "Uncovering All Highly Credible Binary Treatment Hierarchy Questions in Network Meta-Analysis", "comment": "14 pages, 4 figures, 1 table", "summary": "In recent years, there has been growing research interest in addressing\ntreatment hierarchy questions within network meta-analysis (NMA). In NMAs\ninvolving many treatments, the number of possible hierarchy questions becomes\nprohibitively large. To manage this complexity, previous work has recommended\npre-selecting specific hierarchy questions of interest (e.g., ``among options\nA, B, C, D, E, do treatments A and B have the two best effects in terms of\nimproving outcome X?\") and calculating the empirical probabilities of the\nanswers being true given the data. In contrast, we propose an efficient and\nscalable algorithmic approach that eliminates the need for pre-specification by\nsystematically generating a comprehensive catalog of highly credible treatment\nhierarchy questions, specifically, those with empirical probabilities exceeding\na chosen threshold (e.g., 95%). This enables decision-makers to extract all\nmeaningful insights supported by the data. An additional algorithm trims\nredundant insights from the output to facilitate interpretation. We define and\naddress six broad types of binary hierarchy questions (i.e., those with\ntrue/false answers), covering standard hierarchy questions answered using\nexisting ranking metrics - pairwise comparisons and (cumulative) ranking\nprobabilities - as well as many other complex hierarchy questions. We have\nimplemented our methods in an R package and illustrate their application using\nreal NMA datasets on diabetes and depression interventions. Beyond NMA, our\napproach is relevant to any decision problem concerning three or more treatment\noptions."}
{"id": "2510.08898", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.08898", "abs": "https://arxiv.org/abs/2510.08898", "authors": ["Soumojit Das", "Dilshanie Deepawansa", "Partha Lahiri"], "title": "Multidimensional Poverty Mapping for Small Areas", "comment": null, "summary": "Many countries measure poverty based only on income or consumption. However,\nthere is a growing awareness of measuring poverty through multiple dimensions\nthat captures a more reasonable status of poverty. Estimating poverty\nmeasure(s) for small geographical areas, commonly referred to as poverty\nmapping, is challenging due to small or no sample for the small areas. While\nthere is a huge literature available on unidimensional poverty mapping, only a\nlimited effort has been made to address special challenges that arise only in\nthe multidimensional poverty mapping. For example, in multidimensional poverty\nmapping, a new problem arises involving estimation of relative contributions of\ndifferent dimensions to overall poverty for small areas. This problem has been\ngrossly ignored in the small area estimation (SAE) literature. We address this\nissue using a multivariate hierarchical model implemented via a Bayesian\nmethod. Moreover, we demonstrate how a multidimensional poverty composite\nmeasure can be estimated for small areas. In this paper, we demonstrate our\nproposed methodology using a survey data specially designed by one of us for\nmultidimensional poverty mapping. This paper adds a new direction to poverty\nmapping literature."}
{"id": "2510.09315", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09315", "abs": "https://arxiv.org/abs/2510.09315", "authors": ["Siu-Kui Au", "Zi-Jun Cao"], "title": "Reliability Sensitivity with Response Gradient", "comment": "45 pages, 8 figures. Submitted to Structural Safety (Elsevier) on 5\n  Oct 2025", "summary": "Engineering risk is concerned with the likelihood of failure and the\nscenarios when it occurs. The sensitivity of failure probability to change in\nsystem parameters is relevant to risk-informed decision making. Computing\nsensitivity is at least one level more difficult than the probability itself,\nwhich is already challenged by a large number of input random variables, rare\nevents and implicit nonlinear `black-box' response. Finite difference with\nMonte Carlo probability estimates is spurious, requiring the number of samples\nto grow with the reciprocal of step size to suppress estimation variance. Many\nexisting works gain efficiency by exploiting a specific class of input\nvariables, sensitivity parameters, or response in its exact or surrogate form.\nFor general systems, this work presents a theory and associated Monte Carlo\nstrategy for computing sensitivity using response values and gradients with\nrespect to sensitivity parameters. It is shown that the sensitivity at a given\nresponse threshold can be expressed via the expectation of response gradient\nconditional on the threshold. Determining the expectation requires conditioning\non the threshold that is a zero-probability event, but it can be resolved by\nthe concept of kernel smoothing. The proposed method offers sensitivity\nestimates for all response thresholds generated in a single Monte Carlo run. It\nis investigated in a number of examples featuring sensitivity parameters of\ndifferent nature. As response gradient becomes increasingly available, it is\nhoped that this work can provide the basis for embedding sensitivity\ncalculations with reliability in the same Monte Carlo run."}
{"id": "2510.09288", "categories": ["stat.ML", "cs.LG", "68T37"], "pdf": "https://arxiv.org/pdf/2510.09288", "abs": "https://arxiv.org/abs/2510.09288", "authors": ["Pablo G. Arce", "Roi Naveiro", "David Ríos Insua"], "title": "A unified Bayesian framework for adversarial robustness", "comment": null, "summary": "The vulnerability of machine learning models to adversarial attacks remains a\ncritical security challenge. Traditional defenses, such as adversarial\ntraining, typically robustify models by minimizing a worst-case loss. However,\nthese deterministic approaches do not account for uncertainty in the\nadversary's attack. While stochastic defenses placing a probability\ndistribution on the adversary exist, they often lack statistical rigor and fail\nto make explicit their underlying assumptions. To resolve these issues, we\nintroduce a formal Bayesian framework that models adversarial uncertainty\nthrough a stochastic channel, articulating all probabilistic assumptions. This\nyields two robustification strategies: a proactive defense enacted during\ntraining, aligned with adversarial training, and a reactive defense enacted\nduring operations, aligned with adversarial purification. Several previous\ndefenses can be recovered as limiting cases of our model. We empirically\nvalidate our methodology, showcasing the benefits of explicitly modeling\nadversarial uncertainty."}
{"id": "2510.09401", "categories": ["stat.ME", "stat.CO", "62F15, 62F12, 62F40", "G.3"], "pdf": "https://arxiv.org/pdf/2510.09401", "abs": "https://arxiv.org/abs/2510.09401", "authors": ["Matthew R. Williams", "F. Hunter McGuire", "Terrance D. Savitsky"], "title": "Uncertainty Quantification for Multi-level Models Using the Survey-Weighted Pseudo-Posterior", "comment": "13 pages, 2 tables, 2 figures. arXiv admin note: text overlap with\n  arXiv:2308.06845", "summary": "Parameter estimation and inference from complex survey samples typically\nfocuses on global model parameters whose estimators have asymptotic properties,\nsuch as from fixed effects regression models. We present a motivating example\nof Bayesian inference for a multi-level or mixed effects model in which both\nthe local parameters (e.g. group level random effects) and the global\nparameters may need to be adjusted for the complex sampling design. We evaluate\nthe limitations of the survey-weighted pseudo-posterior and an existing\nautomated post-processing method to incorporate the complex survey sample\ndesign for a wide variety of Bayesian models. We propose modifications to the\nautomated process and demonstrate their improvements for multi-level models via\na simulation study and a motivating example from the National Survey on Drug\nUse and Health. Reproduction examples are available from the authors and the\nupdated R package is available via\ngithub:https://github.com/RyanHornby/csSampling"}
{"id": "2510.09276", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.09276", "abs": "https://arxiv.org/abs/2510.09276", "authors": ["Camille M. Montalcini", "Peter J. Rousseeuw"], "title": "The bixplot: A variation on the boxplot suited for bimodal data", "comment": null, "summary": "Boxplots and related visualization methods are widely used exploratory tools\nfor taking a first look at collections of univariate variables. In this note an\nextension is provided that is specifically designed to detect and display\nbimodality and multimodality when the data warrant it. For this purpose a\nunivariate clustering method is constructed that ensures contiguous clusters,\nmeaning that no cluster has members inside another cluster, and such that each\ncluster contains at least a given number of unique members. The resulting\nbixplot display facilitates the identification and interpretation of\npotentially meaningful subgroups underlying the data. The bixplot also displays\nthe individual data values, which can draw attention to isolated points.\nImplementations of the bixplot are available in both Python and R, and their\nmany options are illustrated on several real datasets. For instance, an\nexternal variable can be visualized by color gradations inside the display."}
{"id": "2510.09401", "categories": ["stat.ME", "stat.CO", "62F15, 62F12, 62F40", "G.3"], "pdf": "https://arxiv.org/pdf/2510.09401", "abs": "https://arxiv.org/abs/2510.09401", "authors": ["Matthew R. Williams", "F. Hunter McGuire", "Terrance D. Savitsky"], "title": "Uncertainty Quantification for Multi-level Models Using the Survey-Weighted Pseudo-Posterior", "comment": "13 pages, 2 tables, 2 figures. arXiv admin note: text overlap with\n  arXiv:2308.06845", "summary": "Parameter estimation and inference from complex survey samples typically\nfocuses on global model parameters whose estimators have asymptotic properties,\nsuch as from fixed effects regression models. We present a motivating example\nof Bayesian inference for a multi-level or mixed effects model in which both\nthe local parameters (e.g. group level random effects) and the global\nparameters may need to be adjusted for the complex sampling design. We evaluate\nthe limitations of the survey-weighted pseudo-posterior and an existing\nautomated post-processing method to incorporate the complex survey sample\ndesign for a wide variety of Bayesian models. We propose modifications to the\nautomated process and demonstrate their improvements for multi-level models via\na simulation study and a motivating example from the National Survey on Drug\nUse and Health. Reproduction examples are available from the authors and the\nupdated R package is available via\ngithub:https://github.com/RyanHornby/csSampling"}
{"id": "2510.09477", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09477", "abs": "https://arxiv.org/abs/2510.09477", "authors": ["Conor Hassan", "Nasrulloh Loka", "Cen-You Li", "Daolang Huang", "Paul E. Chang", "Yang Yang", "Francesco Silvestrin", "Samuel Kaski", "Luigi Acerbi"], "title": "Efficient Autoregressive Inference for Transformer Probabilistic Models", "comment": null, "summary": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models."}
{"id": "2510.09598", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.09598", "abs": "https://arxiv.org/abs/2510.09598", "authors": ["Antonio R. Linero"], "title": "Defensive Model Expansion for Robust Bayesian Inference", "comment": null, "summary": "Some applied researchers hesitate to use nonparametric methods, worrying that\nthey will lose power in small samples or overfit the data when simpler models\nare sufficient. We argue that at least some of these concerns are unfounded\nwhen nonparametric models are strongly shrunk towards parametric submodels. We\nconsider expanding a parametric model with a nonparametric component that is\nheavily shrunk toward zero. This construction allows the model to adapt\nautomatically: if the parametric model is correct, the nonparametric component\ndisappears, recovering parametric efficiency, while if it is misspecified, the\nflexible component activates to capture the missing signal. We show that this\nadaptive behavior follows from simple and general conditions. Specifically, we\nprove that Bayesian nonparametric models anchored to linear regression,\nincluding variants of Gaussian processes regression and Bayesian additive\nregression trees, consistently identify the correct parametric submodel when it\nholds and give asymptotically efficient inference for regression coefficients.\nIn simulations, we find that the \"general BART\" model performs identically to\ncorrectly specified linear regression when the parametric model holds, and\nsubstantially outperform it when nonlinear effects are present. This suggests a\npractical paradigm: \"defensive model expansion\" as a safeguard against model\nmisspecification."}
{"id": "2510.09513", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09513", "abs": "https://arxiv.org/abs/2510.09513", "authors": ["Albert Belenguer-Llorens", "Carlos Sevilla-Salcedo", "Janaina Mourao-Miranda", "Vanessa Gómez-Verdejo"], "title": "Interpretable Generative and Discriminative Learning for Multimodal and Incomplete Clinical Data", "comment": null, "summary": "Real-world clinical problems are often characterized by multimodal data,\nusually associated with incomplete views and limited sample sizes in their\ncohorts, posing significant limitations for machine learning algorithms. In\nthis work, we propose a Bayesian approach designed to efficiently handle these\nchallenges while providing interpretable solutions. Our approach integrates (1)\na generative formulation to capture cross-view relationships with a\nsemi-supervised strategy, and (2) a discriminative task-oriented formulation to\nidentify relevant information for specific downstream objectives. This dual\ngenerative-discriminative formulation offers both general understanding and\ntask-specific insights; thus, it provides an automatic imputation of the\nmissing views while enabling robust inference across different data sources.\nThe potential of this approach becomes evident when applied to the multimodal\nclinical data, where our algorithm is able to capture and disentangle the\ncomplex interactions among biological, psychological, and sociodemographic\nmodalities."}
{"id": "2510.09534", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09534", "abs": "https://arxiv.org/abs/2510.09534", "authors": ["So Won Jeong", "Percy S. Zhai", "Veronika Ročová"], "title": "Conditional Flow Matching for Bayesian Posterior Inference", "comment": null, "summary": "We propose a generative multivariate posterior sampler via flow matching. It\noffers a simple training objective, and does not require access to likelihood\nevaluation. The method learns a dynamic, block-triangular velocity field in the\njoint space of data and parameters, which results in a deterministic transport\nmap from a source distribution to the desired posterior. The inverse map, named\nvector rank, is accessible by reversibly integrating the velocity over time. It\nis advantageous to leverage the dynamic design: proper constraints on the\nvelocity yield a monotone map, which leads to a conditional Brenier map,\nenabling a fast and simultaneous generation of Bayesian credible sets whose\ncontours correspond to level sets of Monge-Kantorovich data depth. Our approach\nis computationally lighter compared to GAN-based and diffusion-based\ncounterparts, and is capable of capturing complex posterior structures.\nFinally, frequentist theoretical guarantee on the consistency of the recovered\nposterior distribution, and of the corresponding Bayesian credible sets, is\nprovided."}
{"id": "2510.09315", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09315", "abs": "https://arxiv.org/abs/2510.09315", "authors": ["Siu-Kui Au", "Zi-Jun Cao"], "title": "Reliability Sensitivity with Response Gradient", "comment": "45 pages, 8 figures. Submitted to Structural Safety (Elsevier) on 5\n  Oct 2025", "summary": "Engineering risk is concerned with the likelihood of failure and the\nscenarios when it occurs. The sensitivity of failure probability to change in\nsystem parameters is relevant to risk-informed decision making. Computing\nsensitivity is at least one level more difficult than the probability itself,\nwhich is already challenged by a large number of input random variables, rare\nevents and implicit nonlinear `black-box' response. Finite difference with\nMonte Carlo probability estimates is spurious, requiring the number of samples\nto grow with the reciprocal of step size to suppress estimation variance. Many\nexisting works gain efficiency by exploiting a specific class of input\nvariables, sensitivity parameters, or response in its exact or surrogate form.\nFor general systems, this work presents a theory and associated Monte Carlo\nstrategy for computing sensitivity using response values and gradients with\nrespect to sensitivity parameters. It is shown that the sensitivity at a given\nresponse threshold can be expressed via the expectation of response gradient\nconditional on the threshold. Determining the expectation requires conditioning\non the threshold that is a zero-probability event, but it can be resolved by\nthe concept of kernel smoothing. The proposed method offers sensitivity\nestimates for all response thresholds generated in a single Monte Carlo run. It\nis investigated in a number of examples featuring sensitivity parameters of\ndifferent nature. As response gradient becomes increasingly available, it is\nhoped that this work can provide the basis for embedding sensitivity\ncalculations with reliability in the same Monte Carlo run."}
