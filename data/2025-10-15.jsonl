{"id": "2510.11789", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.11789", "abs": "https://arxiv.org/abs/2510.11789", "authors": ["Shai Zucker", "Xiong Wang", "Fei Lu", "Inbar Seroussi"], "title": "Dimension-Free Minimax Rates for Learning Pairwise Interactions in Attention-Style Models", "comment": null, "summary": "We study the convergence rate of learning pairwise interactions in\nsingle-layer attention-style models, where tokens interact through a weight\nmatrix and a non-linear activation function. We prove that the minimax rate is\n$M^{-\\frac{2\\beta}{2\\beta+1}}$ with $M$ being the sample size, depending only\non the smoothness $\\beta$ of the activation, and crucially independent of token\ncount, ambient dimension, or rank of the weight matrix. These results highlight\na fundamental dimension-free statistical efficiency of attention-style nonlocal\nmodels, even when the weight matrix and activation are not separately\nidentifiable and provide a theoretical understanding of the attention mechanism\nand its training."}
{"id": "2510.11847", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML", "stat.TH", "G.3; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.11847", "abs": "https://arxiv.org/abs/2510.11847", "authors": ["Sam Hawke", "Eric Zhang", "Jiawen Chen", "Didong Li"], "title": "Contrastive Dimension Reduction: A Systematic Review", "comment": null, "summary": "Contrastive dimension reduction (CDR) methods aim to extract signal unique to\nor enriched in a treatment (foreground) group relative to a control\n(background) group. This setting arises in many scientific domains, such as\ngenomics, imaging, and time series analysis, where traditional dimension\nreduction techniques such as Principal Component Analysis (PCA) may fail to\nisolate the signal of interest. In this review, we provide a systematic\noverview of existing CDR methods. We propose a pipeline for analyzing\ncase-control studies together with a taxonomy of CDR methods based on their\nassumptions, objectives, and mathematical formulations, unifying disparate\napproaches under a shared conceptual framework. We highlight key applications\nand challenges in existing CDR methods, and identify open questions and future\ndirections. By providing a clear framework for CDR and its applications, we aim\nto facilitate broader adoption and motivate further developments in this\nemerging field."}
{"id": "2510.11853", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.11853", "abs": "https://arxiv.org/abs/2510.11853", "authors": ["Anirban Chatterjee", "Aaditya Ramdas"], "title": "A Martingale Kernel Two-Sample Test", "comment": null, "summary": "The Maximum Mean Discrepancy (MMD) is a widely used multivariate distance\nmetric for two-sample testing. The standard MMD test statistic has an\nintractable null distribution typically requiring costly resampling or\npermutation approaches for calibration. In this work we leverage a martingale\ninterpretation of the estimated squared MMD to propose martingale MMD (mMMD), a\nquadratic-time statistic which has a limiting standard Gaussian distribution\nunder the null. Moreover we show that the test is consistent against any fixed\nalternative and for large sample sizes, mMMD offers substantial computational\nsavings over the standard MMD test, with only a minor loss in power."}
{"id": "2510.12321", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.12321", "abs": "https://arxiv.org/abs/2510.12321", "authors": ["Yue Zhang", "Shanshan Luo", "Zhi Geng", "Yangbo He"], "title": "Optimal Treatment Rules under Missing Predictive Covariates: A Covariate-Balancing Doubly Robust Approach", "comment": null, "summary": "In precision medicine, one of the most important problems is estimating the\noptimal individualized treatment rules (ITR), which typically involves\nrecommending treatment decisions based on fully observed individual\ncharacteristics of patients to maximize overall clinical benefit. In practice,\nhowever, there may be missing covariates that are not necessarily confounders,\nand it remains uncertain whether these missing covariates should be included\nfor learning optimal ITRs. In this paper, we propose a covariate-balancing\ndoubly robust estimator for constructing optimal ITRs, which is particularly\nsuitable for situations with additional predictive covariates. The proposed\nmethod is based on two main steps: First, the propensity scores are estimated\nby solving the covariate-balancing equation. Second, an objective function is\nminimized to estimate the outcome model, with the function defined by the\nasymptotic variance under the correctly specified propensity score. The method\nhas three significant advantages: (i) It is doubly robust, ensuring consistency\nwhen either the propensity score or outcome model is correctly specified. (ii)\nIt minimizes variance within the class of augmented inverse probability\nweighted estimators. (iii) When applied to partially observed covariates\nrelated to the outcome, the method may further improve estimation efficiency.\nWe demonstrate the proposed method through extensive numerical simulations and\ntwo real-world datasets."}
{"id": "2510.11789", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.11789", "abs": "https://arxiv.org/abs/2510.11789", "authors": ["Shai Zucker", "Xiong Wang", "Fei Lu", "Inbar Seroussi"], "title": "Dimension-Free Minimax Rates for Learning Pairwise Interactions in Attention-Style Models", "comment": null, "summary": "We study the convergence rate of learning pairwise interactions in\nsingle-layer attention-style models, where tokens interact through a weight\nmatrix and a non-linear activation function. We prove that the minimax rate is\n$M^{-\\frac{2\\beta}{2\\beta+1}}$ with $M$ being the sample size, depending only\non the smoothness $\\beta$ of the activation, and crucially independent of token\ncount, ambient dimension, or rank of the weight matrix. These results highlight\na fundamental dimension-free statistical efficiency of attention-style nonlocal\nmodels, even when the weight matrix and activation are not separately\nidentifiable and provide a theoretical understanding of the attention mechanism\nand its training."}
{"id": "2510.11960", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.11960", "abs": "https://arxiv.org/abs/2510.11960", "authors": ["Shehzaib Irfan", "Nabeel Ahmad", "Alexander Vinel", "Daniel F. Silva", "Shuai Shao", "Nima Shamsaei", "Jia Liu"], "title": "Multi-objective Bayesian optimization for blocking in extreme value analysis and its application in additive manufacturing", "comment": "32 pages, 14 figures, Submitted to Journal of Quality Technology on\n  September 16, 2025 and is under review", "summary": "Extreme value theory (EVT) is well suited to model extreme events, such as\nfloods, heatwaves, or mechanical failures, which is required for reliability\nassessment of systems across multiple domains for risk management and loss\nprevention. The block maxima (BM) method, a particular approach within EVT,\nstarts by dividing the historical observations into blocks. Then the sample of\nthe maxima for each block can be shown, under some assumptions, to converge to\na known class of distributions, which can then be used for analysis. The\nquestion of automatic (i.e., without explicit expert input) selection of the\nblock size remains an open challenge. This work proposes a novel Bayesian\nframework, namely, multi-objective Bayesian optimization (MOBO-D*), to optimize\nBM blocking for accurate modeling and prediction of extremes in EVT. MOBO-D*\nformulates two objectives: goodness-of-fit of the distribution of extreme\nevents and the accurate prediction of extreme events to construct an estimated\nPareto front for optimal blocking choices. The efficacy of the proposed\nframework is illustrated by applying it to a real-world case study from the\ndomain of additive manufacturing as well as a synthetic dataset. MOBO-D*\noutperforms a number of benchmarks and can be naturally extended to\nhigh-dimensional cases. The computational experiments show that it can be a\npromising approach in applications that require repeated automated block size\nselection, such as optimization or analysis of many datasets at once."}
{"id": "2510.12375", "categories": ["stat.ML", "cs.LG", "math.OC", "math.PR", "math.ST", "stat.TH", "60F05, 62L20, 62E20"], "pdf": "https://arxiv.org/pdf/2510.12375", "abs": "https://arxiv.org/abs/2510.12375", "authors": ["Bogdan Butyrin", "Eric Moulines", "Alexey Naumov", "Sergey Samsonov", "Qi-Man Shao", "Zhuo-Song Zhang"], "title": "Improved Central Limit Theorem and Bootstrap Approximations for Linear Stochastic Approximation", "comment": null, "summary": "In this paper, we refine the Berry-Esseen bounds for the multivariate normal\napproximation of Polyak-Ruppert averaged iterates arising from the linear\nstochastic approximation (LSA) algorithm with decreasing step size. We consider\nthe normal approximation by the Gaussian distribution with covariance matrix\npredicted by the Polyak-Juditsky central limit theorem and establish the rate\nup to order $n^{-1/3}$ in convex distance, where $n$ is the number of samples\nused in the algorithm. We also prove a non-asymptotic validity of the\nmultiplier bootstrap procedure for approximating the distribution of the\nrescaled error of the averaged LSA estimator. We establish approximation rates\nof order up to $1/\\sqrt{n}$ for the latter distribution, which significantly\nimproves upon the previous results obtained by Samsonov et al. (2024)."}
{"id": "2510.11792", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11792", "abs": "https://arxiv.org/abs/2510.11792", "authors": ["Nathan Wycoff"], "title": "On Thompson Sampling and Bilateral Uncertainty in Additive Bayesian Optimization", "comment": null, "summary": "In Bayesian Optimization (BO), additive assumptions can mitigate the twin\ndifficulties of modeling and searching a complex function in high dimension.\nHowever, common acquisition functions, like the Additive Lower Confidence\nBound, ignore pairwise covariances between dimensions, which we'll call\n\\textit{bilateral uncertainty} (BU), imposing a second layer of approximations.\nWhile theoretical results indicate that asymptotically not much is lost in\ndoing so, little is known about the practical effects of this assumption in\nsmall budgets. In this article, we show that by exploiting conditional\nindependence, Thompson Sampling respecting BU can be efficiently conducted. We\nuse this fact to execute an empirical investigation into the loss incurred by\nignoring BU, finding that the additive approximation to Thompson Sampling does\nindeed have, on balance, worse performance than the exact method, but that this\ndifference is of little practical significance. This buttresses the theoretical\nunderstanding and suggests that the BU-ignoring approximation is sufficient for\nBO in practice, even in the non-asymptotic regime."}
{"id": "2510.12271", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12271", "abs": "https://arxiv.org/abs/2510.12271", "authors": ["Kutay Bölat", "Peter Palensky", "Simon Tindemans"], "title": "The Living Forecast: Evolving Day-Ahead Predictions into Intraday Reality", "comment": null, "summary": "Accurate intraday forecasts are essential for power system operations,\ncomplementing day-ahead forecasts that gradually lose relevance as new\ninformation becomes available. This paper introduces a Bayesian updating\nmechanism that converts fully probabilistic day-ahead forecasts into intraday\nforecasts without retraining or re-inference. The approach conditions the\nGaussian mixture output of a conditional variational autoencoder-based\nforecaster on observed measurements, yielding an updated distribution for the\nremaining horizon that preserves its probabilistic structure. This enables\nconsistent point, quantile, and ensemble forecasts while remaining\ncomputationally efficient and suitable for real-time applications. Experiments\non household electricity consumption and photovoltaic generation datasets\ndemonstrate that the proposed method improves forecast accuracy up to 25%\nacross likelihood-, sample-, quantile-, and point-based metrics. The largest\ngains occur in time steps with strong temporal correlation to observed data,\nand the use of pattern dictionary-based covariance structures further enhances\nperformance. The results highlight a theoretically grounded framework for\nintraday forecasting in modern power systems."}
{"id": "2510.11847", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML", "stat.TH", "G.3; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.11847", "abs": "https://arxiv.org/abs/2510.11847", "authors": ["Sam Hawke", "Eric Zhang", "Jiawen Chen", "Didong Li"], "title": "Contrastive Dimension Reduction: A Systematic Review", "comment": null, "summary": "Contrastive dimension reduction (CDR) methods aim to extract signal unique to\nor enriched in a treatment (foreground) group relative to a control\n(background) group. This setting arises in many scientific domains, such as\ngenomics, imaging, and time series analysis, where traditional dimension\nreduction techniques such as Principal Component Analysis (PCA) may fail to\nisolate the signal of interest. In this review, we provide a systematic\noverview of existing CDR methods. We propose a pipeline for analyzing\ncase-control studies together with a taxonomy of CDR methods based on their\nassumptions, objectives, and mathematical formulations, unifying disparate\napproaches under a shared conceptual framework. We highlight key applications\nand challenges in existing CDR methods, and identify open questions and future\ndirections. By providing a clear framework for CDR and its applications, we aim\nto facilitate broader adoption and motivate further developments in this\nemerging field."}
{"id": "2510.12744", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.12744", "abs": "https://arxiv.org/abs/2510.12744", "authors": ["Do Tien Hai", "Trung Nguyen Mai", "TrungTin Nguyen", "Nhat Ho", "Binh T. Nguyen", "Christopher Drovandi"], "title": "Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps", "comment": "Do Tien Hai, Trung Nguyen Mai, and TrungTin Nguyen are co-first\n  authors", "summary": "We develop a unified statistical framework for softmax-gated Gaussian mixture\nof experts (SGMoE) that addresses three long-standing obstacles in parameter\nestimation and model selection: (i) non-identifiability of gating parameters up\nto common translations, (ii) intrinsic gate-expert interactions that induce\ncoupled differential relations in the likelihood, and (iii) the tight\nnumerator-denominator coupling in the softmax-induced conditional density. Our\napproach introduces Voronoi-type loss functions aligned with the gate-partition\ngeometry and establishes finite-sample convergence rates for the maximum\nlikelihood estimator (MLE). In over-specified models, we reveal a link between\nthe MLE's convergence rate and the solvability of an associated system of\npolynomial equations characterizing near-nonidentifiable directions. For model\nselection, we adapt dendrograms of mixing measures to SGMoE, yielding a\nconsistent, sweep-free selector of the number of experts that attains\npointwise-optimal parameter rates under overfitting while avoiding multi-size\ntraining. Simulations on synthetic data corroborate the theory, accurately\nrecovering the expert count and achieving the predicted rates for parameter\nestimation while closely approximating the regression function. Under model\nmisspecification (e.g., $\\epsilon$-contamination), the dendrogram selection\ncriterion is robust, recovering the true number of mixture components, while\nthe Akaike information criterion, the Bayesian information criterion, and the\nintegrated completed likelihood tend to overselect as sample size grows. On a\nmaize proteomics dataset of drought-responsive traits, our dendrogram-guided\nSGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes\nthe likelihood early, and yields interpretable genotype-phenotype maps,\noutperforming standard criteria without multi-size training."}
{"id": "2510.11871", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11871", "abs": "https://arxiv.org/abs/2510.11871", "authors": ["Poorbita Kundu", "Nathan Wycoff"], "title": "Active Subspaces in Infinite Dimension", "comment": null, "summary": "Active subspace analysis uses the leading eigenspace of the gradient's second\nmoment to conduct supervised dimension reduction. In this article, we extend\nthis methodology to real-valued functionals on Hilbert space. We define an\noperator which coincides with the active subspace matrix when applied to a\nEuclidean space. We show that many of the desirable properties of Active\nSubspace analysis extend directly to the infinite dimensional setting. We also\npropose a Monte Carlo procedure and discuss its convergence properties.\nFinally, we deploy this methodology to create visualizations and improve\nmodeling and optimization on complex test problems."}
{"id": "2510.11740", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.11740", "abs": "https://arxiv.org/abs/2510.11740", "authors": ["Yulin An", "Xueqi Zhao", "Enrique del Castillo"], "title": "Monitoring 3D Lattice Structures in Additive Manufacturing Using Topological Data Analysis", "comment": "22 pages, 13 figures, 12 tables", "summary": "We present a new method for the statistical process control of lattice\nstructures using tools from Topological Data Analysis. Motivated by\napplications in additive manufacturing, such as aerospace components and\nbiomedical implants, where hollow lattice geometries are critical, the proposed\nframework is based on monitoring the persistent homology properties of parts.\nSpecifically, we focus on homological features of dimensions zero and one,\ncorresponding to connected components and one-dimensional loops, to\ncharacterize and detect changes in the topology of lattice structures. A\nnonparametric hypothesis testing procedure and a control charting scheme are\nintroduced to monitor these features during production. Furthermore, we conduct\nextensive run-length analysis via various simulated but real-life\nlattice-structured parts. Our results demonstrate that persistent homology is\nwell-suited for detecting topological anomalies in complex geometries and\noffers a robust, intrinsically geometrical alternative to other SPC methods for\nmesh and point data."}
{"id": "2510.12311", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.12311", "abs": "https://arxiv.org/abs/2510.12311", "authors": ["Joanna Marks", "Tim Y. J. Wang", "O. Deniz Akyildiz"], "title": "Learning Latent Energy-Based Models via Interacting Particle Langevin Dynamics", "comment": null, "summary": "We develop interacting particle algorithms for learning latent variable\nmodels with energy-based priors. To do so, we leverage recent developments in\nparticle-based methods for solving maximum marginal likelihood estimation\n(MMLE) problems. Specifically, we provide a continuous-time framework for\nlearning latent energy-based models, by defining stochastic differential\nequations (SDEs) that provably solve the MMLE problem. We obtain a practical\nalgorithm as a discretisation of these SDEs and provide theoretical guarantees\nfor the convergence of the proposed algorithm. Finally, we demonstrate the\nempirical effectiveness of our method on synthetic and image datasets."}
{"id": "2510.11740", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.11740", "abs": "https://arxiv.org/abs/2510.11740", "authors": ["Yulin An", "Xueqi Zhao", "Enrique del Castillo"], "title": "Monitoring 3D Lattice Structures in Additive Manufacturing Using Topological Data Analysis", "comment": "22 pages, 13 figures, 12 tables", "summary": "We present a new method for the statistical process control of lattice\nstructures using tools from Topological Data Analysis. Motivated by\napplications in additive manufacturing, such as aerospace components and\nbiomedical implants, where hollow lattice geometries are critical, the proposed\nframework is based on monitoring the persistent homology properties of parts.\nSpecifically, we focus on homological features of dimensions zero and one,\ncorresponding to connected components and one-dimensional loops, to\ncharacterize and detect changes in the topology of lattice structures. A\nnonparametric hypothesis testing procedure and a control charting scheme are\nintroduced to monitor these features during production. Furthermore, we conduct\nextensive run-length analysis via various simulated but real-life\nlattice-structured parts. Our results demonstrate that persistent homology is\nwell-suited for detecting topological anomalies in complex geometries and\noffers a robust, intrinsically geometrical alternative to other SPC methods for\nmesh and point data."}
{"id": "2510.11895", "categories": ["stat.ML", "cs.CR", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11895", "abs": "https://arxiv.org/abs/2510.11895", "authors": ["Maryam Aliakbarpour", "Alireza Fallah", "Swaha Roy", "Ria Stevens"], "title": "High-Probability Bounds For Heterogeneous Local Differential Privacy", "comment": null, "summary": "We study statistical estimation under local differential privacy (LDP) when\nusers may hold heterogeneous privacy levels and accuracy must be guaranteed\nwith high probability. Departing from the common in-expectation analyses, and\nfor one-dimensional and multi-dimensional mean estimation problems, we develop\nfinite sample upper bounds in $\\ell_2$-norm that hold with probability at least\n$1-\\beta$. We complement these results with matching minimax lower bounds,\nestablishing the optimality (up to constants) of our guarantees in the\nheterogeneous LDP regime. We further study distribution learning in\n$\\ell_\\infty$-distance, designing an algorithm with high-probability guarantees\nunder heterogeneous privacy demands. Our techniques offer principled guidance\nfor designing mechanisms in settings with user-specific privacy levels."}
{"id": "2510.12048", "categories": ["stat.ME", "stat.AP", "62R10"], "pdf": "https://arxiv.org/pdf/2510.12048", "abs": "https://arxiv.org/abs/2510.12048", "authors": ["Berkay Akturk", "Ufuk Beyaztas", "Han Lin Shang"], "title": "Robust Functional Logistic Regression", "comment": "31 pages, 5 figures, 2 tables", "summary": "Functional logistic regression is a popular model to capture a linear\nrelationship between binary response and functional predictor variables.\nHowever, many methods used for parameter estimation in functional logistic\nregression are sensitive to outliers, which may lead to inaccurate parameter\nestimates and inferior classification accuracy. We propose a robust estimation\nprocedure for functional logistic regression, in which the observations of the\nfunctional predictor are projected onto a set of finite-dimensional subspaces\nvia robust functional principal component analysis. This dimension-reduction\nstep reduces the outlying effects in the functional predictor. The logistic\nregression coefficient is estimated using an M-type estimator based on binary\nresponse and robust principal component scores. In doing so, we provide robust\nestimates by minimizing the effects of outliers in the binary response and\nfunctional predictor variables. Via a series of Monte-Carlo simulations and\nusing hand radiograph data, we examine the parameter estimation and\nclassification accuracy for the response variable. We find that the robust\nprocedure outperforms some existing robust and non-robust methods when outliers\nare present, while producing competitive results when outliers are absent. In\naddition, the proposed method is computationally more efficient than some\nexisting robust alternatives."}
{"id": "2510.12744", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.12744", "abs": "https://arxiv.org/abs/2510.12744", "authors": ["Do Tien Hai", "Trung Nguyen Mai", "TrungTin Nguyen", "Nhat Ho", "Binh T. Nguyen", "Christopher Drovandi"], "title": "Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps", "comment": "Do Tien Hai, Trung Nguyen Mai, and TrungTin Nguyen are co-first\n  authors", "summary": "We develop a unified statistical framework for softmax-gated Gaussian mixture\nof experts (SGMoE) that addresses three long-standing obstacles in parameter\nestimation and model selection: (i) non-identifiability of gating parameters up\nto common translations, (ii) intrinsic gate-expert interactions that induce\ncoupled differential relations in the likelihood, and (iii) the tight\nnumerator-denominator coupling in the softmax-induced conditional density. Our\napproach introduces Voronoi-type loss functions aligned with the gate-partition\ngeometry and establishes finite-sample convergence rates for the maximum\nlikelihood estimator (MLE). In over-specified models, we reveal a link between\nthe MLE's convergence rate and the solvability of an associated system of\npolynomial equations characterizing near-nonidentifiable directions. For model\nselection, we adapt dendrograms of mixing measures to SGMoE, yielding a\nconsistent, sweep-free selector of the number of experts that attains\npointwise-optimal parameter rates under overfitting while avoiding multi-size\ntraining. Simulations on synthetic data corroborate the theory, accurately\nrecovering the expert count and achieving the predicted rates for parameter\nestimation while closely approximating the regression function. Under model\nmisspecification (e.g., $\\epsilon$-contamination), the dendrogram selection\ncriterion is robust, recovering the true number of mixture components, while\nthe Akaike information criterion, the Bayesian information criterion, and the\nintegrated completed likelihood tend to overselect as sample size grows. On a\nmaize proteomics dataset of drought-responsive traits, our dendrogram-guided\nSGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes\nthe likelihood early, and yields interpretable genotype-phenotype maps,\noutperforming standard criteria without multi-size training."}
{"id": "2510.11844", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.11844", "abs": "https://arxiv.org/abs/2510.11844", "authors": ["Mingao Yuan", "Feng Yu"], "title": "Hypothesis testing for the dimension of random geometric graph", "comment": null, "summary": "Random geometric graphs (RGGs) offer a powerful tool for analyzing the\ngeometric and dependence structures in real-world networks. For example, it has\nbeen observed that RGGs are a good model for protein-protein interaction\nnetworks. In RGGs, nodes are randomly distributed over an $m$-dimensional\nmetric space, and edges connect the nodes if and only if their distance is less\nthan some threshold. When fitting RGGs to real-world networks, the first step\nis probably to input or estimate the dimension $m$. However, it is not clear\nwhether the prespecified dimension is equal to the true dimension. In this\npaper, we investigate this problem using hypothesis testing. Under the null\nhypothesis, the dimension is equal to a specific value, while the alternative\nhypothesis asserts the dimension is not equal to that value. We propose the\nfirst statistical test. Under the null hypothesis, the proposed test statistic\nconverges in law to the standard normal distribution, and under the alternative\nhypothesis, the test statistic is unbounded in probability. We derive the\nasymptotic distribution by leveraging the asymptotic theory of degenerate\nU-statistics with kernel function dependent on the number of nodes. This\napproach differs significantly from prevailing methods used in network\nhypothesis testing problems. Moreover, we also propose an efficient approach to\ncompute the test statistic based on the adjacency matrix. Simulation studies\nshow that the proposed test performs well. We also apply the proposed test to\nmultiple real-world networks to test their dimensions."}
{"id": "2510.11910", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11910", "abs": "https://arxiv.org/abs/2510.11910", "authors": ["Tyler Maunu"], "title": "Simplifying Optimal Transport through Schatten-$p$ Regularization", "comment": "26 pages, 4 figures", "summary": "We propose a new general framework for recovering low-rank structure in\noptimal transport using Schatten-$p$ norm regularization. Our approach extends\nexisting methods that promote sparse and interpretable transport maps or plans,\nwhile providing a unified and principled family of convex programs that\nencourage low-dimensional structure. The convexity of our formulation enables\ndirect theoretical analysis: we derive optimality conditions and prove recovery\nguarantees for low-rank couplings and barycentric maps in simplified settings.\nTo efficiently solve the proposed program, we develop a mirror descent\nalgorithm with convergence guarantees for $p \\geq 1$. Experiments on synthetic\nand real data demonstrate the method's efficiency, scalability, and ability to\nrecover low-rank transport structures."}
{"id": "2510.11847", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML", "stat.TH", "G.3; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.11847", "abs": "https://arxiv.org/abs/2510.11847", "authors": ["Sam Hawke", "Eric Zhang", "Jiawen Chen", "Didong Li"], "title": "Contrastive Dimension Reduction: A Systematic Review", "comment": null, "summary": "Contrastive dimension reduction (CDR) methods aim to extract signal unique to\nor enriched in a treatment (foreground) group relative to a control\n(background) group. This setting arises in many scientific domains, such as\ngenomics, imaging, and time series analysis, where traditional dimension\nreduction techniques such as Principal Component Analysis (PCA) may fail to\nisolate the signal of interest. In this review, we provide a systematic\noverview of existing CDR methods. We propose a pipeline for analyzing\ncase-control studies together with a taxonomy of CDR methods based on their\nassumptions, objectives, and mathematical formulations, unifying disparate\napproaches under a shared conceptual framework. We highlight key applications\nand challenges in existing CDR methods, and identify open questions and future\ndirections. By providing a clear framework for CDR and its applications, we aim\nto facilitate broader adoption and motivate further developments in this\nemerging field."}
{"id": "2510.12013", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12013", "abs": "https://arxiv.org/abs/2510.12013", "authors": ["Jiaqi Li", "Zhipeng Lou", "Johannes Schmidt-Hieber", "Wei Biao Wu"], "title": "Statistical Guarantees for High-Dimensional Stochastic Gradient Descent", "comment": "Accepted to NeurIPS 2025", "summary": "Stochastic Gradient Descent (SGD) and its Ruppert-Polyak averaged variant\n(ASGD) lie at the heart of modern large-scale learning, yet their theoretical\nproperties in high-dimensional settings are rarely understood. In this paper,\nwe provide rigorous statistical guarantees for constant learning-rate SGD and\nASGD in high-dimensional regimes. Our key innovation is to transfer powerful\ntools from high-dimensional time series to online learning. Specifically, by\nviewing SGD as a nonlinear autoregressive process and adapting existing\ncoupling techniques, we prove the geometric-moment contraction of\nhigh-dimensional SGD for constant learning rates, thereby establishing\nasymptotic stationarity of the iterates. Building on this, we derive the $q$-th\nmoment convergence of SGD and ASGD for any $q\\ge2$ in general $\\ell^s$-norms,\nand, in particular, the $\\ell^{\\infty}$-norm that is frequently adopted in\nhigh-dimensional sparse or structured models. Furthermore, we provide sharp\nhigh-probability concentration analysis which entails the probabilistic bound\nof high-dimensional ASGD. Beyond closing a critical gap in SGD theory, our\nproposed framework offers a novel toolkit for analyzing a broad class of\nhigh-dimensional learning algorithms."}
{"id": "2510.11853", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.11853", "abs": "https://arxiv.org/abs/2510.11853", "authors": ["Anirban Chatterjee", "Aaditya Ramdas"], "title": "A Martingale Kernel Two-Sample Test", "comment": null, "summary": "The Maximum Mean Discrepancy (MMD) is a widely used multivariate distance\nmetric for two-sample testing. The standard MMD test statistic has an\nintractable null distribution typically requiring costly resampling or\npermutation approaches for calibration. In this work we leverage a martingale\ninterpretation of the estimated squared MMD to propose martingale MMD (mMMD), a\nquadratic-time statistic which has a limiting standard Gaussian distribution\nunder the null. Moreover we show that the test is consistent against any fixed\nalternative and for large sample sizes, mMMD offers substantial computational\nsavings over the standard MMD test, with only a minor loss in power."}
{"id": "2510.12077", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12077", "abs": "https://arxiv.org/abs/2510.12077", "authors": ["Einar Urdshals", "Edmund Lau", "Jesse Hoogland", "Stan van Wingerden", "Daniel Murfet"], "title": "Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory", "comment": "33 pages, 21 figures", "summary": "We study neural network compressibility by using singular learning theory to\nextend the minimum description length (MDL) principle to singular models like\nneural networks. Through extensive experiments on the Pythia suite with\nquantization, factorization, and other compression techniques, we find that\ncomplexity estimates based on the local learning coefficient (LLC) are closely,\nand in some cases, linearly correlated with compressibility. Our results\nprovide a path toward rigorously evaluating the limits of model compression."}
{"id": "2510.11863", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.11863", "abs": "https://arxiv.org/abs/2510.11863", "authors": ["Jiaqi Tong", "Fan Li"], "title": "On the permutation invariance principle for causal estimands", "comment": null, "summary": "In many causal inference problems, multiple action variables share the same\ncausal role, such as mediators, factors, network units, or genotypes, yet lack\na natural ordering. To avoid ambiguity in interpretation, causal estimands\nshould remain unchanged under relabeling, an implicit principle we refer to as\npermutation invariance. We formally characterize this principle, analyze its\nalgebraic and combinatorial structure for verification, and present a class of\nweighted estimands that are permutation-invariant while capturing interactions\nof all orders. We further provide guidance on selecting weights that yield\nresidual-free estimands, whose inclusion-exclusion sums capture the maximal\neffect, and extend our results to ratio effect measures."}
{"id": "2510.12152", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12152", "abs": "https://arxiv.org/abs/2510.12152", "authors": ["Chaiwon Kim", "Jongyeong Lee", "Min-hwan Oh"], "title": "Follow-the-Perturbed-Leader for Decoupled Bandits: Best-of-Both-Worlds and Practicality", "comment": "Preprint, 29 pages", "summary": "We study the decoupled multi-armed bandit (MAB) problem, where the learner\nselects one arm for exploration and one arm for exploitation in each round. The\nloss of the explored arm is observed but not counted, while the loss of the\nexploited arm is incurred without being observed. We propose a policy within\nthe Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations.\nOur policy achieves (near-)optimal regret regardless of the environment, i.e.,\nBest-of-Both-Worlds (BOBW): constant regret in the stochastic regime, improving\nupon the optimal bound of the standard MABs, and minimax optimal regret in the\nadversarial regime. Moreover, the practicality of our policy stems from\navoiding both the convex optimization step required by the previous BOBW\npolicy, Decoupled-Tsallis-INF (Rouyer & Seldin, 2020), and the resampling step\nthat is typically necessary in FTPL. Consequently, it achieves substantial\ncomputational improvement, about $20$ times faster than Decoupled-Tsallis-INF,\nwhile also demonstrating better empirical performance in both regimes. Finally,\nwe empirically show that our approach outperforms a pure exploration policy,\nand that naively combining a pure exploration with a standard exploitation\npolicy is suboptimal."}
{"id": "2510.11982", "categories": ["stat.ME", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2510.11982", "abs": "https://arxiv.org/abs/2510.11982", "authors": ["Pratyusa Datta", "Philippe Lemey", "Marc A. Suchard"], "title": "Inhomogeneous continuous-time Markov chains to infer flexible time-varying evolutionary rates", "comment": "24 pages, 11 figures", "summary": "Reconstructing evolutionary histories and estimating the rate of evolution\nfrom molecular sequence data is of central importance in evolutionary biology\nand infectious disease research. We introduce a flexible Bayesian phylogenetic\ninference framework that accommodates changing evolutionary rates over time by\nmodeling sequence character substitution processes as inhomogeneous\ncontinuous-time Markov chains (ICTMCs) acting along the unknown phylogeny,\nwhere the rate remains as an unknown, positive and integrable function of time.\nThe integral of the rate function appears in the finite-time transition\nprobabilities of the ICTMCs that must be efficiently computed for all branches\nof the phylogeny to evaluate the observed data likelihood. Circumventing\ncomputational challenges that arise from a fully nonparametric function, we\nsuccessfully parameterize the rate function as piecewise constant with a large\nnumber of epochs that we call the polyepoch clock model. This makes the\ntransition probability computation relatively inexpensive and continues to\nflexibly capture rate change over time. We employ a Gaussian Markov random\nfield prior to achieve temporal smoothing of the estimated rate function.\nHamiltonian Monte Carlo sampling enabled by scalable gradient evaluation under\nthis model makes our framework computationally efficient. We assess the\nperformance of the polyepoch clock model in recovering the true timescales and\nrates through simulations under two different evolutionary scenarios. We then\napply the polyepoch clock model to examine the rates of West Nile virus, Dengue\nvirus and influenza A/H3N2 evolution, and estimate the time-varying rate of\nSARS-CoV-2 spread in Europe in 2020."}
{"id": "2510.12311", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.12311", "abs": "https://arxiv.org/abs/2510.12311", "authors": ["Joanna Marks", "Tim Y. J. Wang", "O. Deniz Akyildiz"], "title": "Learning Latent Energy-Based Models via Interacting Particle Langevin Dynamics", "comment": null, "summary": "We develop interacting particle algorithms for learning latent variable\nmodels with energy-based priors. To do so, we leverage recent developments in\nparticle-based methods for solving maximum marginal likelihood estimation\n(MMLE) problems. Specifically, we provide a continuous-time framework for\nlearning latent energy-based models, by defining stochastic differential\nequations (SDEs) that provably solve the MMLE problem. We obtain a practical\nalgorithm as a discretisation of these SDEs and provide theoretical guarantees\nfor the convergence of the proposed algorithm. Finally, we demonstrate the\nempirical effectiveness of our method on synthetic and image datasets."}
{"id": "2510.12048", "categories": ["stat.ME", "stat.AP", "62R10"], "pdf": "https://arxiv.org/pdf/2510.12048", "abs": "https://arxiv.org/abs/2510.12048", "authors": ["Berkay Akturk", "Ufuk Beyaztas", "Han Lin Shang"], "title": "Robust Functional Logistic Regression", "comment": "31 pages, 5 figures, 2 tables", "summary": "Functional logistic regression is a popular model to capture a linear\nrelationship between binary response and functional predictor variables.\nHowever, many methods used for parameter estimation in functional logistic\nregression are sensitive to outliers, which may lead to inaccurate parameter\nestimates and inferior classification accuracy. We propose a robust estimation\nprocedure for functional logistic regression, in which the observations of the\nfunctional predictor are projected onto a set of finite-dimensional subspaces\nvia robust functional principal component analysis. This dimension-reduction\nstep reduces the outlying effects in the functional predictor. The logistic\nregression coefficient is estimated using an M-type estimator based on binary\nresponse and robust principal component scores. In doing so, we provide robust\nestimates by minimizing the effects of outliers in the binary response and\nfunctional predictor variables. Via a series of Monte-Carlo simulations and\nusing hand radiograph data, we examine the parameter estimation and\nclassification accuracy for the response variable. We find that the robust\nprocedure outperforms some existing robust and non-robust methods when outliers\nare present, while producing competitive results when outliers are absent. In\naddition, the proposed method is computationally more efficient than some\nexisting robust alternatives."}
{"id": "2510.12375", "categories": ["stat.ML", "cs.LG", "math.OC", "math.PR", "math.ST", "stat.TH", "60F05, 62L20, 62E20"], "pdf": "https://arxiv.org/pdf/2510.12375", "abs": "https://arxiv.org/abs/2510.12375", "authors": ["Bogdan Butyrin", "Eric Moulines", "Alexey Naumov", "Sergey Samsonov", "Qi-Man Shao", "Zhuo-Song Zhang"], "title": "Improved Central Limit Theorem and Bootstrap Approximations for Linear Stochastic Approximation", "comment": null, "summary": "In this paper, we refine the Berry-Esseen bounds for the multivariate normal\napproximation of Polyak-Ruppert averaged iterates arising from the linear\nstochastic approximation (LSA) algorithm with decreasing step size. We consider\nthe normal approximation by the Gaussian distribution with covariance matrix\npredicted by the Polyak-Juditsky central limit theorem and establish the rate\nup to order $n^{-1/3}$ in convex distance, where $n$ is the number of samples\nused in the algorithm. We also prove a non-asymptotic validity of the\nmultiplier bootstrap procedure for approximating the distribution of the\nrescaled error of the averaged LSA estimator. We establish approximation rates\nof order up to $1/\\sqrt{n}$ for the latter distribution, which significantly\nimproves upon the previous results obtained by Samsonov et al. (2024)."}
{"id": "2510.12301", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.12301", "abs": "https://arxiv.org/abs/2510.12301", "authors": ["David Kronthaler", "Leonhard Held"], "title": "Edgington's Method for Random-Effects Meta-Analysis Part I: Estimation", "comment": null, "summary": "Meta-analysis can be formulated as combining $p$-values across studies into a\njoint $p$-value function, from which point estimates and confidence intervals\ncan be derived. We extend the meta-analytic estimation framework based on\ncombined $p$-value functions to incorporate uncertainty in heterogeneity\nestimation by employing a confidence distribution approach. Specifically, the\nconfidence distribution of Edgington's method is adjusted according to the\nconfidence distribution of the heterogeneity parameter constructed from the\ngeneralized heterogeneity statistic. Simulation results suggest that 95%\nconfidence intervals approach nominal coverage under most scenarios involving\nmore than three studies and heterogeneity. Under no heterogeneity or for only\nthree studies, the confidence interval typically overcovers, but is often\nnarrower than the Hartung-Knapp-Sidik-Jonkman interval. The point estimator\nexhibits small bias under model misspecification and moderate to large\nheterogeneity. Edgington's method provides a practical alternative to classical\napproaches, with adjustment for heterogeneity estimation uncertainty often\nimproving confidence interval coverage."}
{"id": "2510.12416", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12416", "abs": "https://arxiv.org/abs/2510.12416", "authors": ["Alvaro Ortiz", "Tomasa Rodrigo"], "title": "Geopolitics, Geoeconomics and Risk:A Machine Learning Approach", "comment": null, "summary": "We introduce a novel high-frequency daily panel dataset of both markets and\nnews-based indicators -- including Geopolitical Risk, Economic Policy\nUncertainty, Trade Policy Uncertainty, and Political Sentiment -- for 42\ncountries across both emerging and developed markets. Using this dataset, we\nstudy how sentiment dynamics shape sovereign risk, measured by Credit Default\nSwap (CDS) spreads, and evaluate their forecasting value relative to\ntraditional drivers such as global monetary policy and market volatility. Our\nhorse-race analysis of forecasting models demonstrates that incorporating\nnews-based indicators significantly enhances predictive accuracy and enriches\nthe analysis, with non-linear machine learning methods -- particularly Random\nForests -- delivering the largest gains. Our analysis reveals that while global\nfinancial variables remain the dominant drivers of sovereign risk, geopolitical\nrisk and economic policy uncertainty also play a meaningful role. Crucially,\ntheir effects are amplified through non-linear interactions with global\nfinancial conditions. Finally, we document pronounced regional heterogeneity,\nas certain asset classes and emerging markets exhibit heightened sensitivity to\nshocks in policy rates, global financial volatility, and geopolitical risk."}
{"id": "2510.12321", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.12321", "abs": "https://arxiv.org/abs/2510.12321", "authors": ["Yue Zhang", "Shanshan Luo", "Zhi Geng", "Yangbo He"], "title": "Optimal Treatment Rules under Missing Predictive Covariates: A Covariate-Balancing Doubly Robust Approach", "comment": null, "summary": "In precision medicine, one of the most important problems is estimating the\noptimal individualized treatment rules (ITR), which typically involves\nrecommending treatment decisions based on fully observed individual\ncharacteristics of patients to maximize overall clinical benefit. In practice,\nhowever, there may be missing covariates that are not necessarily confounders,\nand it remains uncertain whether these missing covariates should be included\nfor learning optimal ITRs. In this paper, we propose a covariate-balancing\ndoubly robust estimator for constructing optimal ITRs, which is particularly\nsuitable for situations with additional predictive covariates. The proposed\nmethod is based on two main steps: First, the propensity scores are estimated\nby solving the covariate-balancing equation. Second, an objective function is\nminimized to estimate the outcome model, with the function defined by the\nasymptotic variance under the correctly specified propensity score. The method\nhas three significant advantages: (i) It is doubly robust, ensuring consistency\nwhen either the propensity score or outcome model is correctly specified. (ii)\nIt minimizes variance within the class of augmented inverse probability\nweighted estimators. (iii) When applied to partially observed covariates\nrelated to the outcome, the method may further improve estimation efficiency.\nWe demonstrate the proposed method through extensive numerical simulations and\ntwo real-world datasets."}
{"id": "2510.12547", "categories": ["stat.ML", "cs.LG", "62F15, 68T07 (Primary) 62M45, 62C10, 65C60 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.12547", "abs": "https://arxiv.org/abs/2510.12547", "authors": ["Madi Matymov", "Ba-Hien Tran", "Maurizio Filippone"], "title": "Universal Adaptive Environment Discovery", "comment": "8 papes in the main body, 4 pages in the appendix, 4 figures and 9\n  tables overall, conference", "summary": "An open problem in Machine Learning is how to avoid models to exploit\nspurious correlations in the data; a famous example is the background-label\nshortcut in the Waterbirds dataset. A common remedy is to train a model across\nmultiple environments; in the Waterbirds dataset, this corresponds to training\nby randomizing the background. However, selecting the right environments is a\nchallenging problem, given that these are rarely known a priori. We propose\nUniversal Adaptive Environment Discovery (UAED), a unified framework that\nlearns a distribution over data transformations that instantiate environments,\nand optimizes any robust objective averaged over this learned distribution.\nUAED yields adaptive variants of IRM, REx, GroupDRO, and CORAL without\npredefined groups or manual environment design. We provide a theoretical\nanalysis by providing PAC-Bayes bounds and by showing robustness to test\nenvironment distributions under standard conditions. Empirically, UAED\ndiscovers interpretable environment distributions and improves worst-case\naccuracy on standard benchmarks, while remaining competitive on mean accuracy.\nOur results indicate that making environments adaptive is a practical route to\nout-of-distribution generalization."}
{"id": "2510.12337", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.12337", "abs": "https://arxiv.org/abs/2510.12337", "authors": ["Nina Drobac", "Margaux Brégère", "Joseph de Vilmarest", "Olivier Wintenberger"], "title": "Sliding-Window Signatures for Time Series: Application to Electricity Demand Forecasting", "comment": null, "summary": "Nonlinear and delayed effects of covariates often render time series\nforecasting challenging. To this end, we propose a novel forecasting framework\nbased on ridge regression with signature features calculated on sliding\nwindows. These features capture complex temporal dynamics without relying on\nlearned or hand-crafted representations. Focusing on the discrete-time setting,\nwe establish theoretical guarantees, namely universality of approximation and\nstationarity of signatures. We introduce an efficient sequential algorithm for\ncomputing signatures on sliding windows. The method is evaluated on both\nsynthetic and real electricity demand data. Results show that signature\nfeatures effectively encode temporal and nonlinear dependencies, yielding\naccurate forecasts competitive with those based on expert knowledge."}
{"id": "2510.12636", "categories": ["stat.ML", "cs.LG", "math.AP"], "pdf": "https://arxiv.org/pdf/2510.12636", "abs": "https://arxiv.org/abs/2510.12636", "authors": ["Jannis Chemseddine", "Gregor Kornhardt", "Richard Duong", "Gabriele Steidl"], "title": "Adapting Noise to Data: Generative Flows from 1D Processes", "comment": null, "summary": "We introduce a general framework for constructing generative models using\none-dimensional noising processes. Beyond diffusion processes, we outline\nexamples that demonstrate the flexibility of our approach. Motivated by this,\nwe propose a novel framework in which the 1D processes themselves are\nlearnable, achieved by parameterizing the noise distribution through quantile\nfunctions that adapt to the data. Our construction integrates seamlessly with\nstandard objectives, including Flow Matching and consistency models. Learning\nquantile-based noise naturally captures heavy tails and compact supports when\npresent. Numerical experiments highlight both the flexibility and the\neffectiveness of our method."}
{"id": "2510.12356", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.12356", "abs": "https://arxiv.org/abs/2510.12356", "authors": ["Virginia Murru", "Matt P. Wand"], "title": "Variational Inference for Count Response Semiparametric Regression: A Convex Solution", "comment": null, "summary": "We develop a version of variational inference for Bayesian count response\nregression-type models that possesses attractive attributes such as convexity\nand closed form updates. The convex solution aspect entails numerically stable\nfitting algorithms, whilst the closed form aspect makes the methodology fast\nand easy to implement. The essence of the approach is the use of P\\'olya-Gamma\naugmentation of a Negative Binomial likelihood, a finite-valued prior on the\nshape parameter and the structured mean field variational Bayes paradigm. The\napproach applies to general count response situations. For concreteness, we\nfocus on generalized linear mixed models within the semiparametric regression\nclass of models. Real-time fitting is also described."}
{"id": "2510.12639", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.12639", "abs": "https://arxiv.org/abs/2510.12639", "authors": ["Anand Srinivasan", "Jean-Jacques Slotine"], "title": "Contraction and entropy production in continuous-time Sinkhorn dynamics", "comment": "10 pages excluding references", "summary": "Recently, the vanishing-step-size limit of the Sinkhorn algorithm at finite\nregularization parameter $\\varepsilon$ was shown to be a mirror descent in the\nspace of probability measures. We give $L^2$ contraction criteria in two\ntime-dependent metrics induced by the mirror Hessian, which reduce to the\ncoercivity of certain conditional expectation operators. We then give an exact\nidentity for the entropy production rate of the Sinkhorn flow, which was\npreviously known only to be nonpositive. Examining this rate shows that the\nstandard semigroup analysis of diffusion processes extends systematically to\nthe Sinkhorn flow. We show that the flow induces a reversible Markov dynamics\non the target marginal as an Onsager gradient flow. We define the Dirichlet\nform associated to its (nonlocal) infinitesimal generator, prove a Poincar\\'e\ninequality for it, and show that the spectral gap is strictly positive along\nthe Sinkhorn flow whenever $\\varepsilon > 0$. Lastly, we show that the entropy\ndecay is exponential if and only if a logarithmic Sobolev inequality (LSI)\nholds. We give for illustration two immediate practical use-cases for the\nSinkhorn LSI: as a design principle for the latent space in which generative\nmodels are trained, and as a stopping heuristic for discrete-time algorithms."}
{"id": "2510.12504", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.12504", "abs": "https://arxiv.org/abs/2510.12504", "authors": ["Rubén Martos", "Christophe Ambroise", "Guillem Rigaill"], "title": "Causal inference of post-transcriptional regulation timelines from long-read sequencing in Arabidopsis thaliana", "comment": "25 pages. GitHub repository at\n  https://github.com/rmartosprieto/chloroDAG.git", "summary": "We propose a novel framework for reconstructing the chronology of genetic\nregulation using causal inference based on Pearl's theory. The approach\nproceeds in three main stages: causal discovery, causal inference, and\nchronology construction. We apply it to the ndhB and ndhD genes of the\nchloroplast in Arabidopsis thaliana, generating four alternative maturation\ntimeline models per gene, each derived from a different causal discovery\nalgorithm (HC, PC, LiNGAM, or NOTEARS). Two methodological challenges are\naddressed: the presence of missing data, handled via an EM algorithm that\njointly imputes missing values and estimates the Bayesian network, and the\nselection of the $\\ell_1$-regularization parameter in NOTEARS, for which we\nintroduce a stability selection strategy. The resulting causal models\nconsistently outperform reference chronologies in terms of both reliability and\nmodel fit. Moreover, by combining causal reasoning with domain expertise, the\nframework enables the formulation of testable hypotheses and the design of\ntargeted experimental interventions grounded in theoretical predictions."}
{"id": "2510.12744", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.12744", "abs": "https://arxiv.org/abs/2510.12744", "authors": ["Do Tien Hai", "Trung Nguyen Mai", "TrungTin Nguyen", "Nhat Ho", "Binh T. Nguyen", "Christopher Drovandi"], "title": "Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps", "comment": "Do Tien Hai, Trung Nguyen Mai, and TrungTin Nguyen are co-first\n  authors", "summary": "We develop a unified statistical framework for softmax-gated Gaussian mixture\nof experts (SGMoE) that addresses three long-standing obstacles in parameter\nestimation and model selection: (i) non-identifiability of gating parameters up\nto common translations, (ii) intrinsic gate-expert interactions that induce\ncoupled differential relations in the likelihood, and (iii) the tight\nnumerator-denominator coupling in the softmax-induced conditional density. Our\napproach introduces Voronoi-type loss functions aligned with the gate-partition\ngeometry and establishes finite-sample convergence rates for the maximum\nlikelihood estimator (MLE). In over-specified models, we reveal a link between\nthe MLE's convergence rate and the solvability of an associated system of\npolynomial equations characterizing near-nonidentifiable directions. For model\nselection, we adapt dendrograms of mixing measures to SGMoE, yielding a\nconsistent, sweep-free selector of the number of experts that attains\npointwise-optimal parameter rates under overfitting while avoiding multi-size\ntraining. Simulations on synthetic data corroborate the theory, accurately\nrecovering the expert count and achieving the predicted rates for parameter\nestimation while closely approximating the regression function. Under model\nmisspecification (e.g., $\\epsilon$-contamination), the dendrogram selection\ncriterion is robust, recovering the true number of mixture components, while\nthe Akaike information criterion, the Bayesian information criterion, and the\nintegrated completed likelihood tend to overselect as sample size grows. On a\nmaize proteomics dataset of drought-responsive traits, our dendrogram-guided\nSGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes\nthe likelihood early, and yields interpretable genotype-phenotype maps,\noutperforming standard criteria without multi-size training."}
{"id": "2510.12607", "categories": ["stat.ME", "62E20, 62G10, 60F15, 60H10"], "pdf": "https://arxiv.org/pdf/2510.12607", "abs": "https://arxiv.org/abs/2510.12607", "authors": ["Akram Heidari", "Mark Podolskij"], "title": "On goodness-of-fit testing for volatility in McKean-Vlasov models", "comment": "13 pages", "summary": "This paper develops a statistical framework for goodness-of-fit testing of\nvolatility functions in McKean-Vlasov stochastic differential equations, which\ndescribe large systems of interacting particles with distribution-dependent\ndynamics. While integrated volatility estimation in classical SDEs is now well\nestablished, formal model validation and goodness-of-fit testing for\nMcKean-Vlasov systems remain largely unexplored, particularly in regimes with\nboth large particle limits and high-frequency sampling. We propose a test\nstatistic based on discrete observations of particle systems, analysed in a\njoint regime where both the number of particles and the sampling frequency\nincrease. The estimators involved are proven to be consistent, and the test\nstatistic is shown to satisfy a central limit theorem, converging in\ndistribution to a centred Gaussian law."}
{"id": "2510.11847", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML", "stat.TH", "G.3; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.11847", "abs": "https://arxiv.org/abs/2510.11847", "authors": ["Sam Hawke", "Eric Zhang", "Jiawen Chen", "Didong Li"], "title": "Contrastive Dimension Reduction: A Systematic Review", "comment": null, "summary": "Contrastive dimension reduction (CDR) methods aim to extract signal unique to\nor enriched in a treatment (foreground) group relative to a control\n(background) group. This setting arises in many scientific domains, such as\ngenomics, imaging, and time series analysis, where traditional dimension\nreduction techniques such as Principal Component Analysis (PCA) may fail to\nisolate the signal of interest. In this review, we provide a systematic\noverview of existing CDR methods. We propose a pipeline for analyzing\ncase-control studies together with a taxonomy of CDR methods based on their\nassumptions, objectives, and mathematical formulations, unifying disparate\napproaches under a shared conceptual framework. We highlight key applications\nand challenges in existing CDR methods, and identify open questions and future\ndirections. By providing a clear framework for CDR and its applications, we aim\nto facilitate broader adoption and motivate further developments in this\nemerging field."}
{"id": "2510.12663", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.12663", "abs": "https://arxiv.org/abs/2510.12663", "authors": ["Michail Tsagris"], "title": "The $α$--regression for compositional data: a unified framework for standard, spatially-lagged, and geographically-weighted regression models", "comment": null, "summary": "Compositional data-vectors of non--negative components summing to\nunity--frequently arise in scientific applications where covariates influence\nthe relative proportions of components, yet traditional regression approaches\nstruggle with the unit-sum constraint and zero values. This paper revisits the\n$\\alpha$--regression framework, which uses a flexible power transformation\nparameterized by $\\alpha$ to interpolate between raw data analysis and\nlog-ratio methods, naturally handling zeros without imputation while allowing\ndata-driven transformation selection. We formulate $\\alpha$--regression as a\nnon-linear least squares problem, provide efficient estimation via the\nLevenberg-Marquardt algorithm with explicit gradient and Hessian derivations,\nestablish asymptotic normality of the estimators, and derive marginal effects\nfor interpretation. The framework is extended to spatial settings through two\nmodels: the $\\alpha$--spatially lagged X regression model, which incorporates\nspatial spillover effects via spatially lagged covariates with decomposition\ninto direct and indirect effects, and the geographically weighted\n$\\alpha$--regression, which allows coefficients to vary spatially for capturing\nlocal relationships. Application to Greek agricultural land-use data\ndemonstrates that spatial extensions substantially improve predictive\nperformance."}
{"id": "2510.12337", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.12337", "abs": "https://arxiv.org/abs/2510.12337", "authors": ["Nina Drobac", "Margaux Brégère", "Joseph de Vilmarest", "Olivier Wintenberger"], "title": "Sliding-Window Signatures for Time Series: Application to Electricity Demand Forecasting", "comment": null, "summary": "Nonlinear and delayed effects of covariates often render time series\nforecasting challenging. To this end, we propose a novel forecasting framework\nbased on ridge regression with signature features calculated on sliding\nwindows. These features capture complex temporal dynamics without relying on\nlearned or hand-crafted representations. Focusing on the discrete-time setting,\nwe establish theoretical guarantees, namely universality of approximation and\nstationarity of signatures. We introduce an efficient sequential algorithm for\ncomputing signatures on sliding windows. The method is evaluated on both\nsynthetic and real electricity demand data. Results show that signature\nfeatures effectively encode temporal and nonlinear dependencies, yielding\naccurate forecasts competitive with those based on expert knowledge."}
{"id": "2510.12744", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.12744", "abs": "https://arxiv.org/abs/2510.12744", "authors": ["Do Tien Hai", "Trung Nguyen Mai", "TrungTin Nguyen", "Nhat Ho", "Binh T. Nguyen", "Christopher Drovandi"], "title": "Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps", "comment": "Do Tien Hai, Trung Nguyen Mai, and TrungTin Nguyen are co-first\n  authors", "summary": "We develop a unified statistical framework for softmax-gated Gaussian mixture\nof experts (SGMoE) that addresses three long-standing obstacles in parameter\nestimation and model selection: (i) non-identifiability of gating parameters up\nto common translations, (ii) intrinsic gate-expert interactions that induce\ncoupled differential relations in the likelihood, and (iii) the tight\nnumerator-denominator coupling in the softmax-induced conditional density. Our\napproach introduces Voronoi-type loss functions aligned with the gate-partition\ngeometry and establishes finite-sample convergence rates for the maximum\nlikelihood estimator (MLE). In over-specified models, we reveal a link between\nthe MLE's convergence rate and the solvability of an associated system of\npolynomial equations characterizing near-nonidentifiable directions. For model\nselection, we adapt dendrograms of mixing measures to SGMoE, yielding a\nconsistent, sweep-free selector of the number of experts that attains\npointwise-optimal parameter rates under overfitting while avoiding multi-size\ntraining. Simulations on synthetic data corroborate the theory, accurately\nrecovering the expert count and achieving the predicted rates for parameter\nestimation while closely approximating the regression function. Under model\nmisspecification (e.g., $\\epsilon$-contamination), the dendrogram selection\ncriterion is robust, recovering the true number of mixture components, while\nthe Akaike information criterion, the Bayesian information criterion, and the\nintegrated completed likelihood tend to overselect as sample size grows. On a\nmaize proteomics dataset of drought-responsive traits, our dendrogram-guided\nSGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes\nthe likelihood early, and yields interpretable genotype-phenotype maps,\noutperforming standard criteria without multi-size training."}
