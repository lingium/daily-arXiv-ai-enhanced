<div id=toc></div>

# Table of Contents

- [stat.OT](#stat.OT) [Total: 2]
- [stat.AP](#stat.AP) [Total: 4]
- [stat.CO](#stat.CO) [Total: 4]
- [stat.ME](#stat.ME) [Total: 14]
- [stat.ML](#stat.ML) [Total: 18]


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [1] [Ten simple rules for teaching data science](https://arxiv.org/abs/2602.02874)
*Tiffany A. Timbers,Mine Çetinkaya-Rundel*

Main category: stat.OT

TL;DR: 提出数据科学教学的十条简单规则，由社区领先教育者开发并在实际课堂中成功应用


<details>
  <summary>Details</summary>
Motivation: 数据科学教学面临独特挑战，不能简单借用统计学和计算机科学的教学策略，需要专门的教学方法

Method: 基于社区领先教育者的经验，开发并提炼出十条简单规则，在实际数据科学课堂中应用验证

Result: 成功开发出专门针对数据科学教学的十条规则，并在作者自己的数据科学课堂中成功应用

Conclusion: 数据科学教学需要专门的教学策略，提出的十条简单规则为数据科学教育提供了实用指导

Abstract: Teaching data science presents unique challenges and opportunities that cannot be fully addressed by simply borrowing pedagogical strategies from its parent disciplines of statistics and computer science. Here, we present ten simple rules for teaching data science, developed and refined by leading educators in the community and successfully applied in our own data science classrooms.

</details>


### [2] [Six-Minute Man Sander Eitrem 5:58.52 -- first man below the 6:00.00 barrier](https://arxiv.org/abs/2602.03274)
*Nils Lid Hjort*

Main category: stat.OT

TL;DR: 使用极值统计模型分析速度滑冰5000米项目，预测破纪录概率，并评估6分钟大关突破后的未来极限


<details>
  <summary>Details</summary>
Motivation: 分析速度滑冰5000米项目的历史发展，预测何时能突破6分钟大关，并评估突破后的极限成绩

Method: 采用极值统计模型，基于截至2024-2025赛季的126次低于6分10秒的比赛数据，进行预测分析

Result: 预测2025-2026赛季破纪录概率约10%；实际赛季中Loubineaud打破纪录，Eitrem以5:58.52突破6分钟大关；模型分析显示未来极限成绩可达5:57左右

Conclusion: 极值统计模型能有效预测速度滑冰纪录突破，6分钟大关已被突破，未来极限可能在5:57左右，但实际成绩受多种因素影响

Abstract: In Calgary, November 2005, Chad Hedrick was the first to skate the 5,000 m below 6:10. His world record time 6:09.68 was then beaten a week later, in Salt Lake City, by Sven Kramer's 6:08.78. Further top races and world records followed over the ensuing seasons; up to and including the 2024-2025 season, a total of 126 races have been below 6:10, with Nils van der Poel's 2021 world record being 6:01.56. The appropriately hyped-up canonical question for the friends and followers and aficionados of speedskating has then been when (and by whom  we for the first time would witness a below 6:00.00 race. In this note I first use extreme value statistics modelling to assess the state of affairs, as per the end of the 2024-2025 season, with predictions and probabilities for the 2025-2026 season. Under natural modelling assumptions the probability of seeing a new world record during this new season is shown to be about ten percent. We were indeed excited but in reality merely modestly surprised that a race better than van der Poel's record was clocked, by Timothy Loubineaud, in Salt Lake City, November 14, 2025. But Six-Minute Man Sander Eitrem's outstanding 5:58.52 in Inzell, on January 24, 2026, is truly beamonesquely shocking. I also use the modelling machinery to analyse the post-Eitrem situation, and suggest answers to the question of how fast the 5,000 m ever can be skated.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [3] [De-Linearizing Agent Traces: Bayesian Inference of Latent Partial Orders for Efficient Execution](https://arxiv.org/abs/2602.02806)
*Dongqing Li,Zheqiao Cheng,Geoff K. Nicholls,Quyu Kong*

Main category: stat.AP

TL;DR: BPOP：一种贝叶斯框架，从噪声线性化轨迹中推断潜在的依赖偏序关系，通过可处理的前沿softmax似然进行高效MCMC推理，在云配置任务和科学工作流中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: AI代理越来越多地以顺序动作轨迹执行程序化工作流，这掩盖了潜在的并发性并导致重复的逐步推理。需要从噪声的线性化轨迹中恢复潜在的依赖结构。

Method: BPOP是一个贝叶斯框架，将轨迹建模为底层图的随机线性扩展，通过可处理的前沿softmax似然进行高效MCMC推理，避免了#P-hard的线性扩展边缘化计算。

Result: 在开源的Cloud-IaC-6云配置任务套件和WFCommons科学工作流上评估，BPOP恢复依赖结构比仅基于轨迹和流程挖掘的基线方法更准确。推断出的图支持编译执行器，能够剪枝无关上下文，显著减少令牌使用和执行时间。

Conclusion: BPOP能够从噪声线性化轨迹中有效推断潜在的依赖偏序关系，为AI代理的工作流执行提供了更高效的依赖结构恢复和上下文剪枝能力。

Abstract: AI agents increasingly execute procedural workflows as sequential action traces, which obscures latent concurrency and induces repeated step-by-step reasoning. We introduce BPOP, a Bayesianframework that infers a latent dependency partial order from noisy linearized traces. BPOP models traces as stochastic linear extensions of an underlying graph and performs efficient MCMC inference via a tractable frontier-softmax likelihood that avoids #P-hard marginalization over linear extensions. We evaluate on our open-sourced Cloud-IaC-6, a suite of cloud provisioning tasks with heterogeneous LLM-generated traces, and WFCommons scientific workflows. BPOP recover dependency structure more accurately than trace-only and process-mining baselines, and the inferred graphs support a compiled executor that prunes irrelevant context, yielding substantial reductions in token usage and execution time.

</details>


### [4] [Downscaling land surface temperature data using edge detection and block-diagonal Gaussian process regression](https://arxiv.org/abs/2602.02813)
*Sanjit Dandapanthula,Margaret Johnson,Madeleine Pascolini-Campbell,Glynn Hulley,Mikael Kuusela*

Main category: stat.AP

TL;DR: 提出一种基于块对角高斯过程的统计方法，利用Landsat 8数据识别农田边界，将ECOSTRESS地表温度数据降尺度到高分辨率


<details>
  <summary>Details</summary>
Motivation: 高分辨率地表温度估算对估算蒸散发（植物水分利用）至关重要，而蒸散发是农业应用中的核心参数。ECOSTRESS任务提供的地表温度数据需要降尺度到更高分辨率以满足农业应用需求。

Method: 使用Landsat 8数据通过边缘检测技术识别农田边界，捕捉空间域中的固有块结构。提出块对角高斯过程模型，利用农田间的独立性提高计算效率，并考虑ECOSTRESS观测中的支持变化问题。

Result: 该方法能够从ECOSTRESS数据中获得高分辨率的地表温度估计，并提供不确定性量化。结果表明该方法在实际应用中能够产生可靠的高分辨率地表温度估算。

Conclusion: 提出的块对角高斯过程模型在农业、城市规划和气候研究等领域具有实际应用潜力，能够为地表温度的高分辨率估算提供可靠方法。

Abstract: Accurate and high-resolution estimation of land surface temperature (LST) is crucial in estimating evapotranspiration, a measure of plant water use and a central quantity in agricultural applications. In this work, we develop a novel statistical method for downscaling LST data obtained from NASA's ECOSTRESS mission, using high-resolution data from the Landsat 8 mission as a proxy for modeling agricultural field structure. Using the Landsat data, we identify the boundaries of agricultural fields through edge detection techniques, allowing us to capture the inherent block structure present in the spatial domain. We propose a block-diagonal Gaussian process (BDGP) model that captures the spatial structure of the agricultural fields, leverages independence of LST across fields for computational tractability, and accounts for the change of support present in ECOSTRESS observations. We use the resulting BDGP model to perform Gaussian process regression and obtain high-resolution estimates of LST from ECOSTRESS data, along with uncertainty quantification. Our results demonstrate the practicality of the proposed method in producing reliable high-resolution LST estimates, with potential applications in agriculture, urban planning, and climate studies.

</details>


### [5] [On the consistent and scalable detection of spatial patterns](https://arxiv.org/abs/2602.02825)
*Jiayu Su,Jun Hou Fung,Haoyu Wang,Dian Yang,David A. Knowles,Raul Rabadan*

Main category: stat.AP

TL;DR: 论文提出统一空间模式检测方法的统计框架，揭示常用方法的不一致性，并提供可扩展的修正方案


<details>
  <summary>Details</summary>
Motivation: 当前空间模式检测方法缺乏统计共识，且在大规模空间组学数据集上面临计算瓶颈，需要统一框架和可扩展解决方案

Method: 通过单一二次型统一主要方法，推导一般一致性条件，揭示Moran's I等常用方法的不一致性，提出可扩展的修正方案

Result: 开发出能够在数百万空间位置和单细胞谱系追踪数据集中进行稳健模式检测的测试方法

Conclusion: 提出的统一框架解决了空间模式检测中的统计一致性和计算可扩展性问题，为大规模空间组学分析提供了可靠工具

Abstract: Detecting spatial patterns is fundamental to scientific discovery, yet current methods lack statistical consensus and face computational barriers when applied to large-scale spatial omics datasets. We unify major approaches through a single quadratic form and derive general consistency conditions. We reveal that several widely used methods, including Moran's I, are inconsistent, and propose scalable corrections. The resulting test enables robust pattern detection across millions of spatial locations and single-cell lineage-tracing datasets.

</details>


### [6] [Scalable non-separable spatio-temporal Gaussian process models for large-scale short-term weather prediction](https://arxiv.org/abs/2602.03609)
*Tim Gyger,Reinhard Furrer,Fabio Sigrist*

Main category: stat.AP

TL;DR: 本文提出三种可扩展的时空高斯过程方法，用于美国大陆的日最高温和降水短期预测，通过邻居选择策略、诱导点算法和GPU加速解决了大规模时空建模的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 监测每日天气对气候科学、农业和环境规划至关重要，但完全概率时空模型在大陆尺度上计算成本过高。需要开发可扩展的时空高斯过程方法来实现准确且计算可行的大规模天气预测。

Method: 基于三种近似方法（FITC诱导点法、Vecchia近似和混合VIF方法），提出三个扩展：1）基于相关性的可扩展邻居选择策略；2）时空kMeans++诱导点选择算法；3）GPU加速的矩阵运算和邻居搜索实现。

Result: 通过合成实验和包含约170万时空观测的NOAA站点数据集验证，结果表明可扩展高斯过程模型能够提供准确的大陆尺度预测，同时保持计算可行性，在预测性能、参数估计和计算效率方面表现良好。

Conclusion: 可扩展高斯过程模型为天气应用提供了实用工具，能够在大陆尺度上实现准确预测，同时解决大规模时空建模的计算挑战，为气候监测和环境规划提供有效解决方案。

Abstract: Monitoring daily weather fields is critical for climate science, agriculture, and environmental planning, yet fully probabilistic spatio-temporal models become computationally prohibitive at continental scale. We present a case study on short-term forecasting of daily maximum temperature and precipitation across the conterminous United States using novel scalable spatio-temporal Gaussian process methodology. Building on three approximation families - inducing-point methods (FITC), Vecchia approximations, and a hybrid Vecchia-inducing-point full-scale approach (VIF) - we introduce three extensions that address key bottlenecks in large space-time settings: (i) a scalable correlation-based neighbor selection strategy for Vecchia approximations with point-referenced data, enabling accurate conditioning under complex dependence structures, (ii) a space-time kMeans++ inducing-point selection algorithm, and (iii) GPU-accelerated implementations of computationally expensive operations, including matrix operations and neighbor searches. Using both synthetic experiments and a large NOAA station dataset containing approximately 1.7 million space-time observations, we analyze the models with respect to predictive performance, parameter estimation, and computational efficiency. Our results demonstrate that scalable Gaussian process models can yield accurate continental-scale forecasts while remaining computationally feasible, offering practical tools for weather applications.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [7] [From Accessibility to Allocation: An Integrated Workflow for Land-Use Assignment and FAR Estimation](https://arxiv.org/abs/2602.02887)
*Yue Sun,Ryan Weightman,Yang Yang,Anye Shi,Timur Dogan,Samitha Samaranayake*

Main category: stat.CO

TL;DR: 将多半径街道可达性从诊断工具提升为设计杠杆，通过三阶段流程透明地分配土地利用和容积率，支持多目标政策搜索。


<details>
  <summary>Details</summary>
Motivation: 当前城市规划中土地利用和建筑强度往往缺乏与网络可达性的直接、可审计联系，限制了事前政策评估。需要将可达性分析从诊断工具转变为设计杠杆。

Method: 三阶段流程：1) 计算多半径街道可达性并转换为街区尺度；2) 构建嵌套服务盆地，基于规则分配土地利用；3) 通过可达性加权线性模型分配容积率，满足总体建设总量约束。

Result: 应用于实际城区，成功重现了走廊偏向的商业选址和工业带模式，将开发强度集中在高连通性街区。多目标政策搜索产生帕累托最优方案，平衡可达性收益与土地利用/建设结构目标。

Conclusion: 方法上将空间句法指标转化为具有明确保证的集群感知、规则驱动的土地利用和容积率分配；实践上为规划者提供了透明工具，支持反事实测试和权衡谈判。

Abstract: Urban land use and building intensity are often planned without a direct, auditable link to network accessibility, limiting ex-ante policy evaluation. This study asks whether multi-radius street centralities can be elevated from diagnosis to design lever to allocate land use and floor area in a transparent, optimization-ready workflow. We introduce a three-stage pipeline that connects configuration to program and intensity. First, multi-radius accessibility is computed on the street network and translated to blocks to provide scale-legible measures of reach. Second, these measures structure nested service basins that guide a rule-based placement of land uses with explicit priorities and minimum parcel footprints, ensuring reproducibility. Third, within each use, floor-area ratio (FAR) is assigned by an accessibility-weighted linear model that satisfies global construction totals while anchoring the average FAR, thereby tilting height toward better-connected blocks without pathological extremes. The framework supports multi-objective policy search via sampling and Pareto screening. Applied to a real urban district, the workflow reproduces corridor-biased commercial siting and industrial belts while concentrating intensity on highly connected blocks. Policy sampling via multi-objective screening yields Pareto-efficient plans that reconcile accessibility gains with deviations from target land-share and construction-share structures. The contribution is twofold: methodologically, it translates familiar space-syntax measures into cluster-aware, rule-governed land-use and FAR assignment with explicit guarantees (scale-legible radii, parcel minima, and an average-FAR anchor). Practically, it offers planners a transparent instrument for counterfactual testing and negotiated trade-offs at neighborhood/district/city scales.

</details>


### [8] [Bayesian Methods for the Navier-Stokes Equations](https://arxiv.org/abs/2602.02945)
*Nicholas Polson,Vadim Sokolov*

Main category: stat.CO

TL;DR: 提出贝叶斯方法求解不可压缩Navier-Stokes方程，将数值解视为后验计算，输出状态和感兴趣量的概率分布而非单一轨迹


<details>
  <summary>Details</summary>
Motivation: 传统数值方法只提供单一解，缺乏对不确定性的量化。需要开发能提供概率分布输出的求解方法，以更好地处理物理结构、建模误差和不确定性传播

Method: 将离散化Navier-Stokes动力学视为状态空间模型，使用贝叶斯框架：先验编码物理结构和建模误差，通过粒子学习和重采样-传播构造避免权重崩溃，支持顺序观测更新和非高斯误差模型

Result: 开发了二维和三维的随机Navier-Stokes模型，提出了基于粒子和集合的贝叶斯工作流，实现了参数学习的稳定计算，支持部分观测下的顺序更新

Conclusion: 贝叶斯方法为Navier-Stokes方程的数值求解提供了概率框架，能够量化不确定性，支持参数学习和观测更新，在计算流体力学中具有重要应用价值

Abstract: We develop a Bayesian methodology for numerical solution of the incompressible Navier--Stokes equations with quantified uncertainty. The central idea is to treat discretized Navier--Stokes dynamics as a state-space model and to view numerical solution as posterior computation: priors encode physical structure and modeling error, and the solver outputs a distribution over states and quantities of interest rather than a single trajectory. In two dimensions, stochastic representations (Feynman--Kac and stochastic characteristics for linear advection--diffusion with prescribed drift) motivate Monte Carlo solvers and provide intuition for uncertainty propagation. In three dimensions, we formulate stochastic Navier--Stokes models and describe particle-based and ensemble-based Bayesian workflows for uncertainty propagation in spectral discretizations. A key computational advantage is that parameter learning can be performed stably via particle learning: marginalization and resample--propagate (one-step smoothing) constructions avoid the weight-collapse that plagues naive sequential importance sampling on static parameters. When partial observations are available, the same machinery supports sequential observational updating as an additional capability. We also discuss non-Gaussian (heavy-tailed) error models based on normal variance-mean mixtures, which yield conditionally Gaussian updates via latent scale augmentation.

</details>


### [9] [MARADONER: Motif Activity Response Analysis Done Right](https://arxiv.org/abs/2602.03343)
*Georgy Meshcheryakov,Andrey I. Buyan*

Main category: stat.CO

TL;DR: MARADONER是一个改进的MARA（motif activity response analysis）统计框架，用于从转录组或开放染色质数据推断转录因子活性，通过无偏方差参数估计和偏差校正的似然估计提高拟合优度和活性估计准确性。


<details>
  <summary>Details</summary>
Motivation: 从高通量转录组或开放染色质数据（如RNA-Seq、CAGE-Seq、ATAC-Seq）推断转录因子活性是系统生物学中长期存在的挑战。识别高度活跃的主调控因子有助于对差异基因表达、染色质状态变化或不同条件、细胞类型和疾病中的扰动响应进行机制性解释。

Method: MARADONER是一个统计框架和软件实现，利用通过模式匹配（motif扫描）获得的单个启动子序列特征以及启动子或基因水平的活性或表达估计。相比经典MARA，MARADONER采用无偏方差参数估计和偏差校正的固定效应似然估计，并能处理motif分数和活性估计的异方差性。

Result: 该方法提高了拟合优度和活性估计的准确性，能够更好地从转录组或开放染色质数据推断转录因子活性。

Conclusion: MARADONER提供了一个改进的MARA统计框架，通过更准确的统计方法增强了从高通量数据推断转录因子活性的能力，有助于更好地理解基因调控机制。

Abstract: Inferring the activities of transcription factors from high-throughput transcriptomic or open chromatin profiling, such as RNA-/CAGE-/ATAC-Seq, is a long-standing challenge in systems biology. Identification of highly active master regulators enables mechanistic interpretation of differential gene expression, chromatin state changes, or perturbation responses across conditions, cell types, and diseases. Here, we describe MARADONER, a statistical framework and its software implementation for motif activity response analysis (MARA), utilizing the sequence-level features obtained with pattern matching (motif scanning) of individual promoters and promoter- or gene-level activity or expression estimates. Compared to the classic MARA, MARADONER (MARA-done-right) employs an unbiased variance parameter estimation and a bias-adjusted likelihood estimation of fixed effects, thereby enhancing goodness-of-fit and the accuracy of activity estimation. Further, MARADONER is capable of accounting for heteroscedasticity of motif scores and activity estimates.

</details>


### [10] [On the Convergence of Wasserstein Gradient Descent for Sampling](https://arxiv.org/abs/2602.03413)
*Van Chien Ta,Thi Mai Hong Chu,Minh-Ngoc Tran*

Main category: stat.CO

TL;DR: 本文研究Wasserstein空间中KL泛函的优化，开发基于Wasserstein梯度下降的采样框架，识别保证收敛的两个重要子类，构建粒子算法，实验证明能有效逼近复杂分布


<details>
  <summary>Details</summary>
Motivation: 传统MCMC和参数化变分贝叶斯方法在处理高维或多模态复杂目标分布时面临挑战，需要开发更灵活可扩展的贝叶斯推断方法

Method: 在Wasserstein概率测度空间中优化KL泛函，开发Wasserstein梯度下降采样框架，识别保证收敛的两个重要子类，构建基于粒子的WGD算法，使用分数匹配估计得分函数

Result: WGD能有效逼近各种复杂目标分布，包括对标准MCMC和参数化变分贝叶斯方法构成重大挑战的分布，在高维或多模态设置中表现出良好性能

Conclusion: WGD为可扩展贝叶斯推断提供了有前景且灵活的替代方案，特别是在高维或多模态设置中，为基于优化的测度空间采样方法提供了新的理论基础

Abstract: This paper studies the optimization of the KL functional on the Wasserstein space of probability measures, and develops a sampling framework based on Wasserstein gradient descent (WGD). We identify two important subclasses of the Wasserstein space for which the WGD scheme is guaranteed to converge, thereby providing new theoretical foundations for optimization-based sampling methods on measure spaces. For practical implementation, we construct a particle-based WGD algorithm in which the score function is estimated via score matching. Through a series of numerical experiments, we demonstrate that WGD can provide good approximation to a variety of complex target distributions, including those that pose substantial challenges for standard MCMC and parametric variational Bayes methods. These results suggest that WGD offers a promising and flexible alternative for scalable Bayesian inference in high-dimensional or multimodal settings.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [11] [Effect-Wise Inference for Smoothing Spline ANOVA on Tensor-Product Sobolev Space](https://arxiv.org/abs/2602.02753)
*Youngjin Cho,Meimei Liu*

Main category: stat.ME

TL;DR: 提出了一个用于平滑样条ANOVA中效应推断的统一框架，支持交互作用，提供收敛率、置信区间和Wald检验，理论和实证表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有功能ANOVA方法在效应推断方面存在局限：无法处理交互作用、缺乏理论基础或仅限于逐点推断。需要开发一个统一框架来解决这些限制。

Method: 在张量积Sobolev空间的子空间上构建平滑样条ANOVA的统一效应推断框架，利用效应子空间的正交分解，将功能Bahadur表示框架扩展到包含交互作用的效应推断。

Result: 为每个效应函数建立了收敛率、逐点置信区间和Wald检验，主效应达到最优单变量率，交互作用达到最优率（对数因子内），模拟研究和科罗拉多温度数据集应用显示优于现有方法。

Conclusion: 该框架解决了功能ANOVA中效应推断的关键挑战，提供了理论严谨且实用的方法，支持交互作用，在理论和实证上都表现出色。

Abstract: Functional ANOVA provides a nonparametric modeling framework for multivariate covariates, enabling flexible estimation and interpretation of effect functions such as main effects and interaction effects. However, effect-wise inference in such models remains challenging. Existing methods focus primarily on inference for entire functions rather than individual effects. Methods addressing effect-wise inference face substantial limitations: the inability to accommodate interactions, a lack of rigorous theoretical foundations, or restriction to pointwise inference. To address these limitations, we develop a unified framework for effect-wise inference in smoothing spline ANOVA on a subspace of tensor product Sobolev space. For each effect function, we establish rates of convergence, pointwise confidence intervals, and a Wald-type test for whether the effect is zero, with power achieving the minimax distinguishable rate up to a logarithmic factor. Main effects achieve the optimal univariate rates, and interactions achieve optimal rates up to logarithmic factors. The theoretical foundation relies on an orthogonality decomposition of effect subspaces, which enables the extension of the functional Bahadur representation framework to effect-wise inference in smoothing spline ANOVA with interactions. Simulation studies and real-data application to the Colorado temperature dataset demonstrate superior performance compared to existing methods.

</details>


### [12] [Selective Information Borrowing for Region-Specific Treatment Effect Inference under Covariate Mismatch in Multi-Regional Clinical Trials](https://arxiv.org/abs/2602.02703)
*Chenxi Li,Ke Zhu,Shu Yang,Xiaofei Wang*

Main category: stat.ME

TL;DR: 提出一个统一因果推断框架，通过选择性信息借用来解决多区域临床试验中目标区域样本量小且与辅助区域存在差异时的区域特异性平均处理效应估计问题。


<details>
  <summary>Details</summary>
Motivation: 多区域临床试验中，当目标区域样本量小且与辅助区域在基线协变量或未测量因素上存在差异时，区域特异性平均处理效应的有效且有效推断面临挑战。跨区域差异可能导致协变量偏移、协变量不匹配和结果漂移，从而偏差信息借用并影响推断有效性。

Method: 1) 提出逆方差加权估计器，结合"小样本、丰富协变量"的目标区域估计器和"大样本、有限协变量"的全借用双重稳健估计器；2) 使用保形预测评估患者级可比性，自适应选择辅助区域患者进行借用以应对结果漂移；3) 采用条件随机化测试确保有限样本推断的严格性，实现精确、模型无关、选择感知的I类错误控制。

Result: 模拟研究显示，相比不借用和全借用方法，提出的估计器提高了效率，均方误差降低10-50%，功效更高，同时在不同场景下保持有效推断。在POWER试验应用中进一步展示了区域特异性平均处理效应估计精度的提升。

Conclusion: 该框架为多区域临床试验中区域特异性平均处理效应的有效推断提供了统一解决方案，通过选择性信息借用平衡了效率和稳健性，特别适用于目标区域样本量小且存在跨区域差异的场景。

Abstract: Multi-regional clinical trials (MRCTs) are central to global drug development, enabling evaluation of treatment effects across diverse populations. A key challenge is valid and efficient inference for a region-specific estimand when the target region is small and differs from auxiliary regions in baseline covariates or unmeasured factors. We adopt an estimand-based framework and focus on the region-specific average treatment effect (RSATE) in a prespecified target region, which is directly relevant to local regulatory decision-making. Cross-region differences can induce covariate shift, covariate mismatch, and outcome drift, potentially biasing information borrowing and invalidating RSATE inference. To address these issues, we develop a unified causal inference framework with selective information borrowing. First, we introduce an inverse-variance weighting estimator that combines a "small-sample, rich-covariate" target-only estimator with a "large-sample, limited-covariate" full-borrowing doubly robust estimator, maximizing efficiency under no outcome drift. Second, to accommodate outcome drift, we apply conformal prediction to assess patient-level comparability and adaptively select auxiliary-region patients for borrowing. Third, to ensure rigorous finite-sample inference, we employ a conditional randomization test with exact, model-free, selection-aware type I error control. Simulation studies show the proposed estimator improves efficiency, yielding 10-50% reductions in mean squared error and higher power relative to no-borrowing and full-borrowing approaches, while maintaining valid inference across diverse scenarios. An application to the POWER trial further demonstrates improved precision for RSATE estimation.

</details>


### [13] [Shiha Distribution: Statistical Properties and Applications to Reliability Engineering and Environmental Data](https://arxiv.org/abs/2602.02875)
*F. A. Shiha*

Main category: stat.ME

TL;DR: 提出新的Shiha分布，用于建模偏态寿命数据，具有重尾或轻尾特性，在多个领域应用并优于现有模型


<details>
  <summary>Details</summary>
Motivation: 需要一种灵活的两参数分布来建模偏态寿命数据，能够处理重尾和轻尾情况，适用于可靠性工程、环境研究等领域

Method: 提出新的Shiha分布，推导其主要统计性质（矩生成函数、矩、风险率函数、分位数函数、熵），推导应力-强度可靠性参数的闭式解，进行模拟研究评估性能

Result: Shiha分布在多个真实数据集上表现出优于现有竞争模型的拟合效果，模拟研究证实其良好性能，证明了在实际寿命数据分析中的有效性

Conclusion: Shiha分布为偏态寿命数据提供了一种灵活有效的建模工具，在多个应用领域具有实用价值，能够提供比现有模型更好的拟合效果

Abstract: This paper introduces a new two-parameter distribution, referred to as the Shiha distribution, which provides a flexible model for skewed lifetime data with either heavy or light tails. The proposed distribution is applicable to various fields, including reliability engineering, environmental studies, and related areas. We derive its main statistical properties, including the moment generating function, moments, hazard rate function, quantile function, and entropy. The stress--strength reliability parameter is also derived in closed form. A simulation study is conducted to evaluate its performance. Applications to several real data sets demonstrate that the Shiha distribution consistently provides a superior fit compared with established competing models, confirming its practical effectiveness for lifetime data analysis.

</details>


### [14] [Markov Random Fields: Structural Properties, Phase Transition, and Response Function Analysis](https://arxiv.org/abs/2602.02771)
*J. Brandon Carter,Catherine A. Calder*

Main category: stat.ME

TL;DR: 本文对马尔可夫随机场（MRFs）在分类数据中的应用进行了聚焦式综述，重点关注二元观测或潜变量模型，介绍了响应函数作为统一分析工具，并通过案例研究说明不同MRF公式如何编码依赖性。


<details>
  <summary>Details</summary>
Motivation: 马尔可夫随机场是离散空间域中空间依赖性的常用概率表示方法，但不同公式对边际和联合分布的影响需要系统分析。本文旨在为MRF模型提供理论基础和实践工具，帮助解释和扩展基于MRF的模型。

Method: 本文采用综述方法，首先考察MRF的核心结构特性（团分解、条件独立性、邻域结构），讨论相变现象及其对统计模型规范和推断的影响。引入响应函数作为统一分析工具，用于先验分析，并通过带有协变量的直接数据MRF模型案例研究来说明概念。

Result: 通过响应函数分析，揭示了不同MRF公式如何影响隐含的边际和联合分布。案例研究展示了不同公式如何编码依赖性，为理解MRF模型提供了新的视角和分析工具。

Conclusion: 本文为二元场的MRF模型提供了理论基础和实践分析工具，这些原则自然扩展到更复杂的分类MRF模型。响应函数作为统一分析工具，为解释和扩展基于MRF的模型提供了有价值的见解。

Abstract: This paper presents a focused review of Markov random fields (MRFs)--commonly used probabilistic representations of spatial dependence in discrete spatial domains--for categorical data, with an emphasis on models for binary-valued observations or latent variables. We examine core structural properties of these models, including clique factorization, conditional independence, and the role of neighborhood structures. We also discuss the phenomenon of phase transition and its implications for statistical model specification and inference. A central contribution of this review is the use of response functions, a unifying tool we introduce for prior analysis that provides insight into how different formulations of MRFs influence implied marginal and joint distributions. We illustrate these concepts through a case study of direct-data MRF models with covariates, highlighting how different formulations encode dependence. While our focus is on binary fields, the principles outlined here extend naturally to more complex categorical MRFs and we draw connections to these higher-dimensional modeling scenarios. This review provides both theoretical grounding and practical tools for interpreting and extending MRF-based models.

</details>


### [15] [Disentangling spatial interference and spatial confounding biases in causal inference](https://arxiv.org/abs/2602.02777)
*Isqeel Ogunsola,Olatunji Johnson*

Main category: stat.ME

TL;DR: 该论文从DAG视角澄清空间混杂的误解，推导了更一般分布假设下的空间混杂偏倚表达式，并区分了直接/间接空间混杂与空间干扰对因果估计的影响。


<details>
  <summary>Details</summary>
Motivation: 空间干扰和空间混杂是处理观测空间数据时阻碍精确因果估计的两个主要问题，且空间混杂的定义和解释在文献中存在争议。现有分析通常依赖正态性假设，这在实践中常被违反。

Method: 从有向无环图(DAG)视角澄清空间混杂的误解；推导更一般分布假设下的空间混杂偏倚解析表达式(以泊松分布为例)；通过空间权重矩阵、处理变量分布和干扰强度分析偏倚来源；区分直接和间接空间混杂的影响。

Result: 空间权重的选择、处理变量的分布和干扰的强度共同决定了空间干扰引起的偏倚程度；直接和间接空间混杂可以被区分，权重矩阵和暴露性质在决定间接偏倚大小中起关键作用；理论结果得到模拟研究和真实空间数据应用的支持。

Conclusion: 论文为理解空间混杂提供了新的理论框架，未来将开发同时调整空间干扰、直接和间接空间混杂的参数框架，用于直接效应和中介效应的估计。

Abstract: Spatial interference and spatial confounding are two major issues inhibiting precise causal estimates when dealing with observational spatial data. Moreover, the definition and interpretation of spatial confounding remain arguable in the literature. In this paper, our goal is to provide clarity in a novel way on misconception and issues around spatial confounding from Directed Acyclic Graph (DAG) perspective and to disentangle both direct, indirect spatial confounding and spatial interference based on bias induced on causal estimates. Also, existing analyses of spatial confounding bias typically rely on Normality assumptions for treatments and confounders, assumptions that are often violated in practice. Relaxing these assumptions, we derive analytical expressions for spatial confounding bias under more general distributional settings using Poisson as example . We showed that the choice of spatial weights, the distribution of the treatment, and the magnitude of interference critically determine the extent of bias due to spatial interference. We further demonstrate that direct and indirect spatial confounding can be disentangled, with both the weight matrix and the nature of exposure playing central roles in determining the magnitude of indirect bias. Theoretical results are supported by simulation studies and an application to real-world spatial data. In future, parametric frameworks for concomitantly adjusting for spatial interference, direct and indirect spatial confounding for both direct and mediated effects estimation will be developed.

</details>


### [16] [A Model-Robust G-Computation Method for Analyzing Hybrid Control Studies Without Assuming Exchangeability](https://arxiv.org/abs/2602.02809)
*Zhiwei Zhang,Peisong Han,Wei Zhang*

Main category: stat.ME

TL;DR: 提出一种模型稳健的g-computation方法，用于混合控制设计中的治疗评估，该方法能有效利用外部对照数据提高效率，同时对结果回归模型的错误设定具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 混合控制设计（结合随机对照试验和外部对照数据）能提高效率但可能引入偏倚。现有方法通常假设内部和外部对照在调整协变量后可交换，但这一假设可能被违反，且现有g-computation方法需要正确设定结果回归模型。

Method: 提出一种模型稳健的g-computation方法，该方法对结果回归模型的错误设定具有保护作用。方法简单易实现，在最小假设下具有一致性和渐近正态性，能通过利用内部和外部对照组的相似性来提高效率。

Result: 通过模拟研究和HIV治疗试验的真实数据评估，证明该方法在存在模型错误设定时仍能保持稳健性，并能有效利用外部对照数据提高估计效率。

Conclusion: 该模型稳健的g-computation方法为混合控制设计提供了一种简单、稳健且高效的解决方案，特别适用于结果回归模型可能被错误设定的情况，在利用外部对照数据的同时控制了偏倚风险。

Abstract: There is growing interest in a hybrid control design for treatment evaluation, where a randomized controlled trial is augmented with external control data from a previous trial or a real world data source. The hybrid control design has the potential to improve efficiency but also carries the risk of introducing bias. The potential bias in a hybrid control study can be mitigated by adjusting for baseline covariates that are related to the control outcome. Existing methods that serve this purpose commonly assume that the internal and external control outcomes are exchangeable upon conditioning on a set of measured covariates. Possible violations of the exchangeability assumption can be addressed using a g-computation method with variable selection under a correctly specified outcome regression model. In this article, we note that a particular version of this g-computation method is protected against misspecification of the outcome regression model. This observation leads to a model-robust g-computation method that is remarkably simple and easy to implement, consistent and asymptotically normal under minimal assumptions, and able to improve efficiency by exploiting similarities between the internal and external control groups. The method is evaluated in a simulation study and illustrated using real data from HIV treatment trials.

</details>


### [17] [Empirical Bayes Shrinkage of Functional Effects, with Application to Analysis of Dynamic eQTLs](https://arxiv.org/abs/2602.03077)
*Ziang Zhang,Peter Carbonetto,Matthew Stephens*

Main category: stat.ME

TL;DR: FASH是一种用于联合分析功能数据的经验贝叶斯方法，特别适用于动态eQTL研究，通过高斯过程和自适应收缩改进效应函数估计和假设检验。


<details>
  <summary>Details</summary>
Motivation: 本文的动机源于动态表达数量性状位点(eQTL)研究，这类研究旨在表征基因对基因表达的影响如何随时间或其他连续条件变化。传统方法在联合分析多个观测单元时面临挑战，需要一种能够自适应平滑和跨单元信息共享的统计框架。

Method: FASH将线性微分算子定义的广泛高斯过程族集成到经验贝叶斯收缩框架中。该方法支持自适应平滑和跨单元信息共享，提供改进的效应函数估计和原则性假设检验。作者还提出了一种具有理论保证的先验调整方法，以鼓励保守推断。

Result: 在心肌细胞分化的动态eQTL数据重新分析中，FASH识别了新的动态eQTL，揭示了多样的时间效应模式，并相比原始分析提供了改进的统计功效。方法在基因组学及其他领域具有广泛应用潜力。

Conclusion: FASH为功能数据的联合分析提供了一个灵活的统计框架，特别适用于动态eQTL研究。该方法通过R包实现，便于实际应用。提出的先验调整方法也可广泛应用于其他经验贝叶斯方法。

Abstract: We introduce functional adaptive shrinkage (FASH), an empirical Bayes method for joint analysis of observation units in which each unit estimates an effect function at several values of a continuous condition variable. The ideas in this paper are motivated by dynamic expression quantitative trait locus (eQTL) studies, which aim to characterize how genetic effects on gene expression vary with time or another continuous condition. FASH integrates a broad family of Gaussian processes defined through linear differential operators into an empirical Bayes shrinkage framework, enabling adaptive smoothing and borrowing of information across units. This provides improved estimation of effect functions and principled hypothesis testing, allowing straightforward computation of significance measures such as local false discovery and false sign rates. To encourage conservative inferences, we propose a simple prior- adjustment method that has theoretical guarantees and can be more broadly used with other empirical Bayes methods. We illustrate the benefits of FASH by reanalyzing dynamic eQTL data on cardiomyocyte differentiation from induced pluripotent stem cells. FASH identified novel dynamic eQTLs, revealed diverse temporal effect patterns, and provided improved power compared with the original analysis. More broadly, FASH offers a flexible statistical framework for joint analysis of functional data, with applications extending beyond genomics. To facilitate use of FASH in dynamic eQTL studies and other settings, we provide an accompanying R package at https: //github.com/stephenslab/fashr.

</details>


### [18] [Kriging for large datasets via penalized neighbor selection](https://arxiv.org/abs/2602.03483)
*Francisco Cuevas-Pacheco,Jonathan Acosta*

Main category: stat.ME

TL;DR: 提出惩罚克里金框架，通过LASSO惩罚实现自动邻居选择，平衡计算效率与预测精度


<details>
  <summary>Details</summary>
Motivation: 传统克里金计算复杂度O(N³)在大数据集上不可行，而局部克里金的邻居选择通常基于启发式标准，未能考虑空间相关结构

Method: 将LASSO类型惩罚直接纳入克里金方程，实现自动数据驱动的邻居选择；扩展到自适应LASSO，使用考虑空间相关结构的数据驱动惩罚权重；通过ℓ₁正则化确定非零权重观测值，基于有效样本量的新准则选择惩罚参数

Result: 惩罚克里金能自动适应底层空间相关的邻域结构：对平滑过程选择较少邻居，对高度可变场选择更多邻居；在显著降低计算成本的同时，保持与全局克里金相当的预测精度

Conclusion: 提出的惩罚克里金框架通过自动邻居选择解决了大规模空间预测的计算挑战，同时考虑了空间相关结构，为高效准确的空间预测提供了新方法

Abstract: Kriging is a fundamental tool for spatial prediction, but its computational complexity of $O(N^3)$ becomes prohibitive for large datasets. While local kriging using $K$-nearest neighbors addresses this issue, the selection of $K$ typically relies on ad-hoc criteria that fail to account for spatial correlation structure. We propose a penalized kriging framework that incorporates LASSO-type penalties directly into the kriging equations to achieve automatic, data-driven neighbor selection. We further extend this to adaptive LASSO, using data-driven penalty weights that account for the spatial correlation structure. Our method determines which observations contribute non-zero weights through $\ell_1$ regularization, with the penalty parameter selected via a novel criterion based on effective sample size that balances prediction accuracy against information redundancy. Numerical experiments demonstrate that penalized kriging automatically adapts neighborhood structure to the underlying spatial correlation, selecting fewer neighbors for smoother processes and more for highly variable fields, while maintaining prediction accuracy comparable to global kriging at substantially reduced computational cost.

</details>


### [19] [Functional regression with multivariate responses](https://arxiv.org/abs/2602.02860)
*Ruiyan Luo,Xin Qi*

Main category: stat.ME

TL;DR: 提出多变量响应函数回归模型，利用响应变量间的相关性提高估计和预测精度，针对不同预测变量数量设计方法，并开发R包实现


<details>
  <summary>Details</summary>
Motivation: 在函数回归模型中，相比单独拟合每个响应变量，利用响应变量间的相关性可以改进估计和预测精度

Method: 识别系数函数的最优分解，针对少量和大量函数预测变量分别提出估计方法，对大量预测变量使用同时平滑稀疏惩罚进行曲线选择

Result: 方法可应用于数千个函数预测变量的模型，已实现为R包FRegSigCom，提供了样本量和预测变量数趋于无穷时的渐近结果

Conclusion: 提出的方法能有效利用函数预测变量和多变量响应中的信息，提高估计和预测精度，特别适用于高维函数数据

Abstract: We consider the functional regression model with multivariate response and functional predictors. Compared to fitting each individual response variable separately, taking advantage of the correlation between the response variables can improve the estimation and prediction accuracy. Using information in both functional predictors and multivariate response, we identify the optimal decomposition of the coefficient functions for prediction in population level. Then we propose methods to estimate this decomposition and fit the regression model for the situations of a small and a large number $p$ of functional predictors separately. For a large $p$, we propose a simultaneous smooth-sparse penalty which can both make curve selection and improve estimation and prediction accuracy. We provide the asymptotic results when both the sample size and the number of functional predictors go to infinity. Our method can be applied to models with thousands of functional predictors and has been implemented in the R package FRegSigCom.

</details>


### [20] [Blinded sample size re-estimation accounting for uncertainty in mid-trial estimation](https://arxiv.org/abs/2602.03218)
*Hirotada Maeda,Satoshi Hattori,Tim Friede*

Main category: stat.ME

TL;DR: 提出一种基于方差上置信限的盲法样本量再估计方法，通过选择适当置信水平确保再估计样本量达到目标功效，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验中，样本量计算需要准确设定方差，但方差误设可能导致研究功效不足。现有盲法样本量再估计方法使用总方差估计，但其期望值大于等于真实方差，可能导致小样本盲法评估仍出现功效不足问题。

Method: 提出改进方法：1) 使用方差的上置信限来考虑盲法评估中的估计误差；2) 开发选择适当置信水平的方法，使再估计样本量达到目标功效。该方法借鉴了外部预试验中的类似思路。

Result: 数值研究表明，该方法表现良好且优于现有方法。通过近期随机临床试验进行了动机说明和实例展示。

Conclusion: 提出的基于方差上置信限的盲法样本量再估计方法能有效降低研究功效不足的风险，为临床试验设计提供了更可靠的样本量调整策略。

Abstract: For randomized controlled trials to be conclusive, it is important to set the target sample size accurately at the design stage. Comparing two normal populations, the sample size calculation requires specification of the variance other than the treatment effect and misspecification can lead to underpowered studies. Blinded sample size re-estimation is an approach to minimize the risk of inconclusive studies. Existing methods proposed to use the total (one-sample) variance that is estimable from blinded data without knowledge of the treatment allocation. We demonstrate that, since the expectation of this estimator is greater than or equal to the true variance, the one-sample variance approach can be regarded as providing an upper bound of the variance in blind reviews. This worst-case evaluation can likely reduce a risk of underpowered studies. However, blinded reviews of small sample size may still lead to underpowered studies. We propose a refined method accounting for estimation error in blind reviews using an upper confidence limit of the variance. A similar idea had been proposed in the setting of external pilot studies. Furthermore, we developed a method to select an appropriate confidence level so that the re-estimated sample size attains the target power. Numerical studies showed that our method works well and outperforms existing methods. The proposed procedure is motivated and illustrated by recent randomized clinical trials.

</details>


### [21] [Weighted Sum-of-Trees Model for Clustered Data](https://arxiv.org/abs/2602.02931)
*Kevin McCoy,Zachary Wooten,Katarzyna Tomczak,Christine B. Peterson*

Main category: stat.ME

TL;DR: 提出一种针对聚类数据的轻量级树模型，为每个样本组学习单独的决策树，并通过权重组合预测，使新组预测更接近训练数据中最相似的组。


<details>
  <summary>Details</summary>
Motivation: 传统线性混合模型和现有扩展方法在处理聚类数据时，对新样本组的预测假设所有组共享相同的模型，这在实际中可能不成立。需要一种能够捕捉组间差异并提高对新组预测准确性的方法。

Method: 提出轻量级树模型：为每个训练组学习单独的决策树，预测时通过权重组合各树的预测结果，权重反映新组与训练组的相似性。该方法还允许通过比较各组的树结构和变量重要性来推断组间相似性。

Result: 在各种模拟设置中，该方法优于传统决策树和随机森林。在真实世界的癌症基因组图谱肉瘤队列数据中（患者按肉瘤亚型分组），展示了方法的有效性。

Conclusion: 该方法为聚类数据提供了一种灵活的非参数建模方法，能够更好地处理组间差异，提高对新样本组的预测准确性，同时提供组间相似性的推断能力。

Abstract: Clustered data, which arise when observations are nested within groups, are incredibly common in clinical, education, and social science research. Traditionally, a linear mixed model, which includes random effects to account for within-group correlation, would be used to model the observed data and make new predictions on unseen data. Some work has been done to extend the mixed model approach beyond linear regression into more complex and non-parametric models, such as decision trees and random forests. However, existing methods are limited to using the global fixed effects for prediction on data from out-of-sample groups, effectively assuming that all clusters share a common outcome model. We propose a lightweight sum-of-trees model in which we learn a decision tree for each sample group. We combine the predictions from these trees using weights so that out-of-sample group predictions are more closely aligned with the most similar groups in the training data. This strategy also allows for inference on the similarity across groups in the outcome prediction model, as the unique tree structures and variable importances for each group can be directly compared. We show our model outperforms traditional decision trees and random forests in a variety of simulation settings. Finally, we showcase our method on real-world data from the sarcoma cohort of The Cancer Genome Atlas, where patient samples are grouped by sarcoma subtype.

</details>


### [22] [Entropic Mirror Monte Carlo](https://arxiv.org/abs/2602.03165)
*Anas Cherradi,Yazid Janati,Alain Durmus,Sylvain Le Corff,Yohan Petetin,Julien Stoehr*

Main category: stat.ME

TL;DR: 提出一种新颖的自适应重要性采样算法，通过结合全局采样机制和延迟加权程序来构建高效的建议分布，在复杂目标分布下实现几何收敛。


<details>
  <summary>Details</summary>
Motivation: 当目标分布复杂（如高维空间中的多峰分布）时，重要性采样的效率严重依赖于建议分布的选择。传统方法在复杂分布下效率低下，需要更有效的自适应方案。

Method: 提出结合全局采样机制和延迟加权程序的自适应算法。延迟加权机制在建议分布与目标分布不匹配的区域实现快速重采样，促进对目标分布的有效探索。

Result: 算法在温和假设下被证明具有几何收敛性，并通过各种数值实验验证了其有效性。

Conclusion: 该自适应重要性采样算法能够有效处理复杂目标分布，通过创新的加权机制显著提升采样效率，为高维多峰分布下的蒙特卡洛估计提供了实用解决方案。

Abstract: Importance sampling is a Monte Carlo method which designs estimators of expectations under a target distribution using weighted samples from a proposal distribution. When the target distribution is complex, such as multimodal distributions in highdimensional spaces, the efficiency of importance sampling critically depends on the choice of the proposal distribution. In this paper, we propose a novel adaptive scheme for the construction of efficient proposal distributions. Our algorithm promotes efficient exploration of the target distribution by combining global sampling mechanisms with a delayed weighting procedure. The proposed weighting mechanism plays a key role by enabling rapid resampling in regions where the proposal distribution is poorly adapted to the target. Our sampling algorithm is shown to be geometrically convergent under mild assumptions and is illustrated through various numerical experiments.

</details>


### [23] [Simulation-Based Inference via Regression Projection and Batched Discrepancies](https://arxiv.org/abs/2602.03613)
*Arya Farahi,Jonah Rose,Paul Torrey*

Main category: stat.ME

TL;DR: 提出一种基于回归投影的轻量级模拟推理方法，通过拟合代理线性回归，使用小批量模拟和核权重分配生成伪后验分布，该方法简单、可并行化，且仅需回归系数而非原始观测数据。


<details>
  <summary>Details</summary>
Motivation: 传统模拟推理方法通常计算成本高，需要大量模拟或复杂的似然函数。本文旨在开发一种轻量级方法，通过回归投影减少计算负担，同时保持统计一致性，特别适用于复杂模拟器参数推断场景。

Method: 方法核心是：1）拟合代理线性回归作为数据投影；2）在提议参数值处进行小批量模拟；3）基于批次残差差异分配核权重；4）生成自归一化伪后验分布。该方法仅需回归系数而非原始观测，支持并行计算。

Result: 理论证明：随着参数抽取数量增加，方法具有一致性；随着批次大小增加和带宽缩小，伪后验集中在由所选投影确定的识别集上。实验验证了回归投影的计算优势，并展示了低信息摘要带来的可识别性限制。

Conclusion: 该方法提供了一种计算高效的模拟推理框架，通过回归投影平衡计算效率与统计可靠性。方法明确了点识别与集合识别的条件，为复杂模拟器参数校准提供了实用工具，特别适用于计算资源受限或模拟成本高的应用场景。

Abstract: We analyze a lightweight simulation-based inference method that infers simulator parameters using only a regression-based projection of the observed data. After fitting a surrogate linear regression once, the procedure simulates small batches at the proposed parameter values and assigns kernel weights based on the resulting batch-residual discrepancy, producing a self-normalized pseudo-posterior that is simple, parallelizable, and requires access only to the fitted regression coefficients rather than raw observations. We formalize the construction as an importance-sampling approximation to a population target that averages over simulator randomness, prove consistency as the number of parameter draws grows, and establish stability in estimating the surrogate regression from finite samples. We then characterize the asymptotic concentration as the batch size increases and the bandwidth shrinks, showing that the pseudo-posterior concentrates on an identified set determined by the chosen projection, thereby clarifying when the method yields point versus set identification. Experiments on a tractable nonlinear model and on a cosmological calibration task using the DREAMS simulation suite illustrate the computational advantages of regression-based projections and the identifiability limitations arising from low-information summaries.

</details>


### [24] [Bayesian variable and hazard structure selection in the General Hazard model](https://arxiv.org/abs/2602.03756)
*Yulong Chen,Jim Griffin,Francisco Javier Rubio*

Main category: stat.ME

TL;DR: 提出贝叶斯变量选择方法，在广义风险模型中同时选择相关变量和风险结构，包括PH和AFT模型作为特例


<details>
  <summary>Details</summary>
Motivation: 传统的生存分析变量选择通常基于单一风险结构（如PH或AFT模型），这强加了协变量如何影响风险函数的强假设。需要一种方法能够同时选择相关变量和最适合的风险结构。

Method: 在广义风险模型中开发贝叶斯变量选择方法，提出两种g-先验用于回归系数，引入考虑多重性和惩罚模型复杂性的分层先验，扩展Add-Delete-Swap算法联合采样变量包含指标和风险结构

Result: 模拟研究显示在不同样本量和删失水平下都能准确恢复真实风险结构和活跃变量，实际数据应用验证了方法的有效性

Conclusion: 提出的贝叶斯方法能够在广义风险模型框架内同时进行变量选择和风险结构选择，提供了一种灵活且计算可行的生存数据分析方法

Abstract: The proportional hazards (PH) and accelerated failure time (AFT) models are the most widely used hazard structures for analysing time-to-event data. When the goal is to identify variables associated with event times, variable selection is typically performed within a single hazard structure, imposing strong assumptions on how covariates affect the hazard function. To allow simultaneous selection of relevant variables and the hazard structure itself, we develop a Bayesian variable selection approach within the general hazard (GH) model, which includes the PH, AFT, and other structures as special cases. We propose two types of g-priors for the regression coefficients that enable tractable computation and show that both lead to consistent model selection. We also introduce a hierarchical prior on the model space that accounts for multiplicity and penalises model complexity. To efficiently explore the GH model space, we extend the Add-Delete-Swap algorithm to jointly sample variable inclusion indicators and hazard structures. Simulation studies show accurate recovery of both the true hazard structure and active variables across different sample sizes and censoring levels. Two real-data applications are presented to illustrate the use of the proposed methodology and to compare it with existing variable selection methods.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [25] [Relaxed Triangle Inequality for Kullback-Leibler Divergence Between Multivariate Gaussian Distributions](https://arxiv.org/abs/2602.02577)
*Shiji Xiao,Yufeng Zhang,Chubo Liu,Yan Ding,Keqin Li,Kenli Li*

Main category: stat.ML

TL;DR: 本文研究了多元高斯分布之间KL散度的松弛三角不等式，给出了KL(N₁,N₃)的上确界及其可达条件，改进了现有边界。


<details>
  <summary>Details</summary>
Motivation: KL散度不是真正的距离度量，不满足三角不等式，这给实际应用带来理论挑战。现有研究已证明多元高斯分布间的KL散度满足松弛三角不等式，但上确界仍未知。

Method: 研究多元高斯分布间KL散度的松弛三角不等式，推导出KL(N₁,N₃)的上确界表达式，并确定该上确界可达的条件。

Result: 当ε₁和ε₂较小时，上确界为ε₁+ε₂+√(ε₁ε₂)+o(ε₁)+o(ε₂)，比现有边界3ε₁+3ε₂+2√(ε₁ε₂)+o(ε₁)+o(ε₂)更紧。给出了上确界可达的具体条件。

Conclusion: 本文解决了多元高斯分布KL散度松弛三角不等式的上确界问题，提供了更精确的边界，并在基于流的生成模型的分布外检测和安全强化学习中展示了应用价值。

Abstract: The Kullback-Leibler (KL) divergence is not a proper distance metric and does not satisfy the triangle inequality, posing theoretical challenges in certain practical applications. Existing work has demonstrated that KL divergence between multivariate Gaussian distributions follows a relaxed triangle inequality. Given any three multivariate Gaussian distributions $\mathcal{N}_1, \mathcal{N}_2$, and $\mathcal{N}_3$, if $KL(\mathcal{N}_1, \mathcal{N}_2)\leq ε_1$ and $KL(\mathcal{N}_2, \mathcal{N}_3)\leq ε_2$, then $KL(\mathcal{N}_1, \mathcal{N}_3)< 3ε_1+3ε_2+2\sqrt{ε_1ε_2}+o(ε_1)+o(ε_2)$. However, the supremum of $KL(\mathcal{N}_1, \mathcal{N}_3)$ is still unknown. In this paper, we investigate the relaxed triangle inequality for the KL divergence between multivariate Gaussian distributions and give the supremum of $KL(\mathcal{N}_1, \mathcal{N}_3)$ as well as the conditions when the supremum can be attained. When $ε_1$ and $ε_2$ are small, the supremum is $ε_1+ε_2+\sqrt{ε_1ε_2}+o(ε_1)+o(ε_2)$. Finally, we demonstrate several applications of our results in out-of-distribution detection with flow-based generative models and safe reinforcement learning.

</details>


### [26] [Plug-In Classification of Drift Functions in Diffusion Processes Using Neural Networks](https://arxiv.org/abs/2602.02791)
*Yuzhen Zhao,Jiarong Fan,Yating Liu*

Main category: stat.ML

TL;DR: 提出基于神经网络的插件分类器，用于多维扩散过程的监督多类分类，通过估计每类的漂移函数并应用贝叶斯型决策规则，在标准正则性假设下建立了超额误分类风险的收敛率。


<details>
  <summary>Details</summary>
Motivation: 将Denis等人（2024）的一维多类分类框架扩展到多维扩散过程，解决离散时间观测轨迹的分类问题，利用扩散模型结构提升分类性能。

Method: 提出神经网络插件分类器：从独立样本路径估计每类的漂移函数，基于贝叶斯型决策规则分配标签。方法考虑了漂移估计误差和时间离散化的影响。

Result: 建立了超额误分类风险的收敛率，数值实验显示：在一维设置中比Denis等人（2024）收敛更快、分类性能更好；在漂移函数具有组合结构的高维情况下仍有效；始终优于不利用扩散模型结构的端到端神经网络分类器。

Conclusion: 提出的基于神经网络的插件分类器在多维扩散过程分类中表现优异，通过利用扩散模型结构显著提升了分类性能，为离散时间观测的扩散过程分类提供了有效解决方案。

Abstract: We study a supervised multiclass classification problem for diffusion processes, where each class is characterized by a distinct drift function and trajectories are observed at discrete times. Extending the one-dimensional multiclass framework of Denis et al. (2024) to multidimensional diffusions, we propose a neural network-based plug-in classifier that estimates the drift functions for each class from independent sample paths and assigns labels based on a Bayes-type decision rule. Under standard regularity assumptions, we establish convergence rates for the excess misclassification risk, explicitly capturing the effects of drift estimation error and time discretization. Numerical experiments demonstrate that the proposed method achieves faster convergence and improved classification performance compared to Denis et al. (2024) in the one-dimensional setting, remains effective in higher dimensions when the underlying drift functions admit a compositional structure, and consistently outperforms direct neural network classifiers trained end-to-end on trajectories without exploiting the diffusion model structure.

</details>


### [27] [Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation](https://arxiv.org/abs/2602.02633)
*Tahir Qasim Syed,Behraj Khan*

Main category: stat.ML

TL;DR: 提出首个无需训练、完全冻结模型参数的测试时适应方法，通过指数倾斜重加权潜在嵌入分布来适应新任务


<details>
  <summary>Details</summary>
Motivation: 在部署环境中，即使是轻量级的参数更新也可能导致模型偏移或调优不稳定，需要研究在完全冻结模型且无法访问上游数据情况下的测试时适应方法

Method: 使用少量标注支持集计算任务相似度分数，通过指数倾斜在KL最优方式下重加权编码器诱导的潜在嵌入分布，不修改模型参数

Result: 方法在多个基准测试和不同样本数量下与基于参数更新的方法表现相当，同时在更严格的约束条件下运行

Conclusion: 即使采用完全冻结的模型管道，推理级别的分布校正对于测试时适应也是可行的

Abstract: Often, constraints arise in deployment settings where even lightweight parameter updates e.g. parameter-efficient fine-tuning could induce model shift or tuning instability. We study test-time adaptation of foundation models for few-shot classification under a completely frozen-model regime, where additionally, no upstream data are accessible. We propose arguably the first training-free inference method that adapts predictions to the new task by performing a change of measure over the latent embedding distribution induced by the encoder. Using task-similarity scores derived from a small labeled support set, exponential tilting reweights latent distributions in a KL-optimal manner without modifying model parameters. Empirically, the method consistently competes with parameter-update-based methods across multiple benchmarks and shot regimes, while operating under strictly and universally stronger constraints. These results demonstrate the viability of inference-level distributional correction for test-time adaptation even with a fully-frozen model pipeline.

</details>


### [28] [Near-Universal Multiplicative Updates for Nonnegative Einsum Factorization](https://arxiv.org/abs/2602.02759)
*John Hood,Aaron Schein*

Main category: stat.ML

TL;DR: NNEinFact：基于einsum的乘法更新算法，用于拟合任何可表示为张量收缩的非负张量分解，支持多种损失函数和缺失数据，在真实数据上比标准方法提升37%以上。


<details>
  <summary>Details</summary>
Motivation: 尽管多路数据在科学领域普遍存在，但缺乏用户友好的非负张量分解工具。现有方法要么使用梯度自动微分（在非负设置中效果不佳），要么选择有限的方法实现，要么需要从头实现自己的模型。

Method: 引入NNEinFact，一种基于einsum的乘法更新算法，可以拟合任何可表示为张量收缩的非负张量分解。用户只需用字符串指定模型，算法通过最小化用户指定的损失函数（包括(α,β)-散度）来收敛到局部最小值。

Result: NNEinFact支持缺失数据，能在数秒内拟合包含数亿条目的张量。在真实张量数据的保留预测任务中，定制模型比标准模型性能提升超过37%，测试损失比基于梯度的方法减少一半以上，收敛速度提升高达90倍。

Conclusion: NNEinFact为非负张量分解提供了一个灵活、高效且用户友好的解决方案，填补了现有工具的空白，使研究人员能够轻松实现和测试定制模型。

Abstract: Despite the ubiquity of multiway data across scientific domains, there are few user-friendly tools that fit tailored nonnegative tensor factorizations. Researchers may use gradient-based automatic differentiation (which often struggles in nonnegative settings), choose between a limited set of methods with mature implementations, or implement their own model from scratch. As an alternative, we introduce NNEinFact, an einsum-based multiplicative update algorithm that fits any nonnegative tensor factorization expressible as a tensor contraction by minimizing one of many user-specified loss functions (including the $(α,β)$-divergence). To use NNEinFact, the researcher simply specifies their model with a string. NNEinFact converges to a local minimum of the loss, supports missing data, and fits to tensors with hundreds of millions of entries in seconds. Empirically, NNEinFact fits custom models which outperform standard ones in heldout prediction tasks on real-world tensor data by over $37\%$ and attains less than half the test loss of gradient-based methods while converging up to 90 times faster.

</details>


### [29] [Unified Inference Framework for Single and Multi-Player Performative Prediction: Method and Asymptotic Optimality](https://arxiv.org/abs/2602.03049)
*Zhixian Zhang,Xiaotian Hou,Linjun Zhang*

Main category: stat.ML

TL;DR: 本文提出了一个统一框架，将单智能体和多智能体预测性能问题联系起来，开发了两种估计方法：用于稳定性估计的重复风险最小化（RRM）和用于最优性估计的两步插件估计器，均具有统计保证。


<details>
  <summary>Details</summary>
Motivation: 预测性能环境中的模型会改变其试图预测的数据分布，形成复杂反馈循环。先前研究将单智能体和多智能体预测性能视为不同现象，缺乏统一框架。本文旨在弥合这一差距，为动态预测性能环境提供可靠的估计和决策工具。

Method: 1. 提出重复风险最小化（RRM）程序用于估计预测稳定性，建立严格的推断理论证明其渐近正态性和渐近效率。2. 针对预测最优性，提出新颖的两步插件估计器，结合重新校准的预测增强推断（RePPI）和重要性采样思想，推导中心极限定理。

Result: 理论分析表明：RRM估计器具有渐近正态性和渐近效率；两步插件估计器达到半参数效率界，在温和分布错误设定下保持鲁棒性。框架将单智能体预测性能视为多智能体情况的特例。

Conclusion: 本文提供了一个原则性工具包，用于在动态预测性能环境中进行可靠估计和决策。统一框架连接了单智能体和多智能体预测性能，提出的估计方法具有统计保证和鲁棒性，为实际应用提供了理论基础。

Abstract: Performative prediction characterizes environments where predictive models alter the very data distributions they aim to forecast, triggering complex feedback loops. While prior research treats single-agent and multi-agent performativity as distinct phenomena, this paper introduces a unified statistical inference framework that bridges these contexts, treating the former as a special case of the latter. Our contribution is two-fold. First, we put forward the Repeated Risk Minimization (RRM) procedure for estimating the performative stability, and establish a rigorous inferential theory for admitting its asymptotic normality and confirming its asymptotic efficiency. Second, for the performative optimality, we introduce a novel two-step plug-in estimator that integrates the idea of Recalibrated Prediction Powered Inference (RePPI) with Importance Sampling, and further provide formal derivations for the Central Limit Theorems of both the underlying distributional parameters and the plug-in results. The theoretical analysis demonstrates that our estimator achieves the semiparametric efficiency bound and maintains robustness under mild distributional misspecification. This work provides a principled toolkit for reliable estimation and decision-making in dynamic, performative environments.

</details>


### [30] [Training-Free Self-Correction for Multimodal Masked Diffusion Models](https://arxiv.org/abs/2602.02927)
*Yidong Ouyang,Panwen Hu,Zhengyan Wan,Zhe Wang,Liyan Xie,Dmitriy Bespalov,Ying Nian Wu,Guang Cheng,Hongyuan Zha,Qiang Sun*

Main category: stat.ML

TL;DR: 提出了一种无需训练的自校正框架，利用预训练掩码扩散模型的归纳偏置，在文本到图像生成和多模态理解任务上显著提升生成质量并减少采样步骤。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型在采样时同时更新多个token并将已生成token视为不可变，可能导致早期错误无法修正的错误累积问题。现有自校正方法存在额外训练需求或依赖未对齐似然估计的局限性。

Method: 提出无需训练的自校正框架，利用预训练掩码扩散模型的归纳偏置，不修改模型参数或引入辅助评估器，通过自校正机制改进生成过程。

Result: 在文本到图像生成和多模态理解任务上显著提升生成质量，减少采样步骤，且框架可泛化到不同掩码扩散架构，具有鲁棒性和实用价值。

Conclusion: 提出的训练免费自校正框架有效解决了掩码扩散模型中的错误累积问题，提高了生成质量并加速采样，具有广泛的适用性和实用性。

Abstract: Masked diffusion models have emerged as a powerful framework for text and multimodal generation. However, their sampling procedure updates multiple tokens simultaneously and treats generated tokens as immutable, which may lead to error accumulation when early mistakes cannot be revised. In this work, we revisit existing self-correction methods and identify limitations stemming from additional training requirements or reliance on misaligned likelihood estimates. We propose a training-free self-correction framework that exploits the inductive biases of pre-trained masked diffusion models. Without modifying model parameters or introducing auxiliary evaluators, our method significantly improves generation quality on text-to-image generation and multimodal understanding tasks with reduced sampling steps. Moreover, the proposed framework generalizes across different masked diffusion architectures, highlighting its robustness and practical applicability. Code can be found in https://github.com/huge123/FreeCorrection.

</details>


### [31] [Online Conformal Prediction via Universal Portfolio Algorithms](https://arxiv.org/abs/2602.03168)
*Tuo Liu,Edgar Dobriban,Francesco Orabona*

Main category: stat.ML

TL;DR: 提出UP-OCP方法，通过线性化遗憾理论和投资组合选择问题，实现无需参数调优的在线共形预测，在任意数据流上保证长期覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有在线共形预测方法需要手动调整学习率，且需要算法特定分析。需要一种通用、无需参数调优的方法来处理任意（可能对抗性）数据流，同时保持预测区间的信息量。

Method: 基于(1-α)-分位数损失建立线性化遗憾理论，证明控制线性化遗憾可推导出覆盖率边界。提出UP-OCP方法，通过将问题转化为两资产投资组合选择问题，利用通用投资组合算法实现参数自由。

Result: UP-OCP在多项式增长预测下仍能获得强有限时间误覆盖率边界。实验表明，UP-OCP在大小/覆盖率权衡上持续优于现有在线共形预测基线方法。

Conclusion: 建立了在线共形预测的通用遗憾到覆盖率理论，提出了无需参数调优的UP-OCP方法，为任意数据流提供了理论保证和实际性能提升。

Abstract: Online conformal prediction (OCP) seeks prediction intervals that achieve long-run $1-α$ coverage for arbitrary (possibly adversarial) data streams, while remaining as informative as possible. Existing OCP methods often require manual learning-rate tuning to work well, and may also require algorithm-specific analyses. Here, we develop a general regret-to-coverage theory for interval-valued OCP based on the $(1-α)$-pinball loss. Our first contribution is to identify \emph{linearized regret} as a key notion, showing that controlling it implies coverage bounds for any online algorithm. This relies on a black-box reduction that depends only on the Fenchel conjugate of an upper bound on the linearized regret. Building on this theory, we propose UP-OCP, a parameter-free method for OCP, via a reduction to a two-asset portfolio selection problem, leveraging universal portfolio algorithms. We show strong finite-time bounds on the miscoverage of UP-OCP, even for polynomially growing predictions. Extensive experiments support that UP-OCP delivers consistently better size/coverage trade-offs than prior online conformal baselines.

</details>


### [32] [Score-based diffusion models for diffuse optical tomography with uncertainty quantification](https://arxiv.org/abs/2602.03449)
*Fabian Schneider,Meghdoot Mozumder,Konstantin Tamarov,Leila Taghizadeh,Tanja Tarvainen,Tapio Helin,Duc-Lam Duong*

Main category: stat.ML

TL;DR: 本文评估了UCoS框架在扩散光学层析成像中的应用，提出了一种结合学习先验和模型基础的混合分数正则化方法，在模拟和实验数据上都取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 尽管基于分数的扩散模型在贝叶斯逆问题中表现出色，但在存在建模误差和实际测量数据的真实逆问题中仍缺乏深入研究。扩散光学层析成像(DOT)是一个高度不适定的问题，对噪声和建模误差敏感，需要更稳健的后验采样方法。

Method: 采用无条件表示条件分数函数(UCoS)框架，提出了一种新颖的正则化方法：构建混合分数函数，包含学习组件和模型基础组件，防止分数函数过拟合。在扩散光学层析成像的线性化差分成像中进行了评估。

Result: 实验表明，与经典模型基础估计相比，数据驱动的先验分布能产生低方差的后验样本，并且即使在高不适定问题和存在建模误差的情况下，样本仍围绕真实值分布。

Conclusion: UCoS框架结合混合分数正则化方法在扩散光学层析成像中表现良好，数据驱动先验能有效处理高度不适定问题和建模误差，为实际逆问题提供了有前景的解决方案。

Abstract: Score-based diffusion models are a recently developed framework for posterior sampling in Bayesian inverse problems with a state-of-the-art performance for severely ill-posed problems by leveraging a powerful prior distribution learned from empirical data. Despite generating significant interest especially in the machine-learning community, a thorough study of realistic inverse problems in the presence of modelling error and utilization of physical measurement data is still outstanding. In this work, the framework of unconditional representation for the conditional score function (UCoS) is evaluated for linearized difference imaging in diffuse optical tomography (DOT). DOT uses boundary measurements of near-infrared light to estimate the spatial distribution of absorption and scattering parameters in biological tissues. The problem is highly ill-posed and thus sensitive to noise and modelling errors. We introduce a novel regularization approach that prevents overfitting of the score function by constructing a mixed score composed of a learned and a model-based component. Validation of this approach is done using both simulated and experimental measurement data. The experiments demonstrate that a data-driven prior distribution results in posterior samples with low variance, compared to classical model-based estimation, and centred around the ground truth, even in the context of a highly ill-posed problem and in the presence of modelling errors.

</details>


### [33] [NeuralFLoC: Neural Flow-Based Joint Registration and Clustering of Functional Data](https://arxiv.org/abs/2602.03169)
*Xinyang Xiong,Siyuan jiang,Pengcheng Zeng*

Main category: stat.ML

TL;DR: NeuralFLoC是一个端到端的无监督深度学习框架，用于联合功能数据配准和聚类，通过神经ODE驱动的微分同胚流和谱聚类解决相位变化问题。


<details>
  <summary>Details</summary>
Motivation: 功能数据中的相位变化会掩盖内在形状差异并降低聚类性能。现有方法通常将配准和聚类作为独立任务处理，或依赖限制性参数假设。

Method: 基于神经ODE驱动的微分同胚流和谱聚类，学习平滑可逆的扭曲函数和聚类特定模板，同时解耦相位和幅度变化。

Result: 在功能基准测试中展示了最先进的配准和聚类性能，对缺失数据、不规则采样和噪声具有鲁棒性，且保持可扩展性。

Conclusion: NeuralFLoC是一个有效的端到端无监督框架，能够同时处理功能数据的配准和聚类问题，具有理论保证和实际应用价值。

Abstract: Clustering functional data in the presence of phase variation is challenging, as temporal misalignment can obscure intrinsic shape differences and degrade clustering performance. Most existing approaches treat registration and clustering as separate tasks or rely on restrictive parametric assumptions. We present \textbf{NeuralFLoC}, a fully unsupervised, end-to-end deep learning framework for joint functional registration and clustering based on Neural ODE-driven diffeomorphic flows and spectral clustering. The proposed model learns smooth, invertible warping functions and cluster-specific templates simultaneously, effectively disentangling phase and amplitude variation. We establish universal approximation guarantees and asymptotic consistency for the proposed framework. Experiments on functional benchmarks show state-of-the-art performance in both registration and clustering, with robustness to missing data, irregular sampling, and noise, while maintaining scalability. Code is available at https://anonymous.4open.science/r/NeuralFLoC-FEC8.

</details>


### [34] [Latent Neural-ODE for Model-Informed Precision Dosing: Overcoming Structural Assumptions in Pharmacokinetics](https://arxiv.org/abs/2602.03215)
*Benjamin Maurel,Agathe Guilloux,Sarah Zohar,Moreno Ursino,Jean-Baptiste Woillard*

Main category: stat.ML

TL;DR: 提出基于潜在常微分方程（Latent ODEs）的深度学习模型，用于肾移植后他克莫司暴露量预测，相比传统方法具有更好的灵活性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于非线性混合效应（NLME）的群体药代动力学模型依赖预设假设，难以捕捉复杂的患者特异性动态，可能导致模型误设，需要更灵活的数据驱动方法。

Method: 使用潜在常微分方程（Latent ODEs）深度学习框架，直接从稀疏临床数据中学习个体化药代动力学动态，并与NLME和迭代两阶段贝叶斯（it2B）方法进行对比。

Result: 模拟显示Latent ODE模型具有更好的鲁棒性；临床验证中，内部验证的RMSPE为7.99%（优于it2B的9.24%），外部验证的RMSPE为10.82%（与标准方法相当）。

Conclusion: Latent ODE模型是AUC预测的强大可靠工具，其灵活架构为个性化医疗中的下一代多模态模型提供了有前景的基础。

Abstract: Accurate estimation of tacrolimus exposure, quantified by the area under the concentration-time curve (AUC), is essential for precision dosing after renal transplantation. Current practice relies on population pharmacokinetic (PopPK) models based on nonlinear mixed-effects (NLME) methods. However, these models depend on rigid, pre-specified assumptions and may struggle to capture complex, patient-specific dynamics, leading to model misspecification.
  In this study, we introduce a novel data-driven alternative based on Latent Ordinary Differential Equations (Latent ODEs) for tacrolimus AUC prediction. This deep learning approach learns individualized pharmacokinetic dynamics directly from sparse clinical data, enabling greater flexibility in modeling complex biological behavior. The model was evaluated through extensive simulations across multiple scenarios and benchmarked against two standard approaches: NLME-based estimation and the iterative two-stage Bayesian (it2B) method. We further performed a rigorous clinical validation using a development dataset (n = 178) and a completely independent external dataset (n = 75).
  In simulation, the Latent ODE model demonstrated superior robustness, maintaining high accuracy even when underlying biological mechanisms deviated from standard assumptions. Regarding experiments on clinical datasets, in internal validation, it achieved significantly higher precision with a mean RMSPE of 7.99% compared with 9.24% for it2B (p < 0.001). On the external cohort, it achieved an RMSPE of 10.82%, comparable to the two standard estimators (11.48% and 11.54%).
  These results establish the Latent ODE as a powerful and reliable tool for AUC prediction. Its flexible architecture provides a promising foundation for next-generation, multi-modal models in personalized medicine.

</details>


### [35] [Principled Federated Random Forests for Heterogeneous Data](https://arxiv.org/abs/2602.03258)
*Rémi Khellaf,Erwan Scornet,Aurélien Bellet,Julie Josse*

Main category: stat.ML

TL;DR: FedForest：一种新的联邦随机森林算法，通过聚合客户端统计信息来近似集中式分割，支持客户端指示器分割实现个性化，在异构数据上匹配集中式性能且通信高效。


<details>
  <summary>Details</summary>
Motivation: 随机森林在集中式表格数据中表现出色，但缺乏有效的联邦学习方法。现有方法依赖启发式策略，无法优化全局不纯度标准，特别是在数据分布偏移情况下。

Method: 提出FedForest算法，基于聚合精心选择的客户端统计信息进行分割决策，支持水平分区数据。允许在客户端指示器上进行分割，实现非参数形式的个性化。

Result: 理论证明分割过程能近似集中式算法选择的分割。实证显示在异构基准测试中匹配集中式性能，同时保持通信效率。

Conclusion: FedForest为联邦随机森林提供了理论保证的方法，有效处理客户端数据异构性，实现个性化且通信高效，填补了现有方法的空白。

Abstract: Random Forests (RF) are among the most powerful and widely used predictive models for centralized tabular data, yet few methods exist to adapt them to the federated learning setting. Unlike most federated learning approaches, the piecewise-constant nature of RF prevents exact gradient-based optimization. As a result, existing federated RF implementations rely on unprincipled heuristics: for instance, aggregating decision trees trained independently on clients fails to optimize the global impurity criterion, even under simple distribution shifts. We propose FedForest, a new federated RF algorithm for horizontally partitioned data that naturally accommodates diverse forms of client data heterogeneity, from covariate shift to more complex outcome shift mechanisms. We prove that our splitting procedure, based on aggregating carefully chosen client statistics, closely approximates the split selected by a centralized algorithm. Moreover, FedForest allows splits on client indicators, enabling a non-parametric form of personalization that is absent from prior federated random forest methods. Empirically, we demonstrate that the resulting federated forests closely match centralized performance across heterogeneous benchmarks while remaining communication-efficient.

</details>


### [36] [Multiparameter Uncertainty Mapping in Quantitative Molecular MRI using a Physics-Structured Variational Autoencoder (PS-VAE)](https://arxiv.org/abs/2602.03317)
*Alex Finkelstein,Ron Moneta,Or Zohar,Michal Rivlin,Moritz Zaiss,Dinora Friedmann Morvinski,Or Perlman*

Main category: stat.ML

TL;DR: 提出物理结构变分自编码器（PS-VAE），用于快速提取体素级多参数后验分布，为定量成像提供不确定性量化，显著加速全脑量化过程。


<details>
  <summary>Details</summary>
Motivation: 现有定量成像方法（如磁共振指纹）在反问题求解中缺乏原则性的不确定性量化，限制了临床应用的信任度和透明度。

Method: 结合可微分自旋物理模拟器与自监督学习，构建物理结构变分自编码器（PS-VAE），能够提取完整的协方差矩阵以捕捉潜在生物物理空间的参数间相关性。

Result: 在多质子池化学交换饱和转移和半固体磁化转移分子MRF研究中验证，结果与暴力贝叶斯分析一致，同时实现数量级加速的全脑量化，并能通过后验动态监测优化采集协议。

Conclusion: PS-VAE为定量成像提供了可靠的不确定性量化框架，显著加速参数估计过程，有望促进实时自适应采集和临床转化。

Abstract: Quantitative imaging methods, such as magnetic resonance fingerprinting (MRF), aim to extract interpretable pathology biomarkers by estimating biophysical tissue parameters from signal evolutions. However, the pattern-matching algorithms or neural networks used in such inverse problems often lack principled uncertainty quantification, which limits the trustworthiness and transparency, required for clinical acceptance. Here, we describe a physics-structured variational autoencoder (PS-VAE) designed for rapid extraction of voxelwise multi-parameter posterior distributions. Our approach integrates a differentiable spin physics simulator with self-supervised learning, and provides a full covariance that captures the inter-parameter correlations of the latent biophysical space. The method was validated in a multi-proton pool chemical exchange saturation transfer (CEST) and semisolid magnetization transfer (MT) molecular MRF study, across in-vitro phantoms, tumor-bearing mice, healthy human volunteers, and a subject with glioblastoma. The resulting multi-parametric posteriors are in good agreement with those calculated using a brute-force Bayesian analysis, while providing an orders-of-magnitude acceleration in whole brain quantification. In addition, we demonstrate how monitoring the multi-parameter posterior dynamics across progressively acquired signals provides practical insights for protocol optimization and may facilitate real-time adaptive acquisition.

</details>


### [37] [Improving the Linearized Laplace Approximation via Quadratic Approximations](https://arxiv.org/abs/2602.03394)
*Pedro Jiménez,Luis A. Ortega,Pablo Morales-Álvarez,Daniel Hernández-Lobato*

Main category: stat.ML

TL;DR: 提出Quadratic Laplace Approximation (QLA)方法，通过二次近似改进线性化拉普拉斯近似(LLA)的后验精度，提升不确定性估计效果


<details>
  <summary>Details</summary>
Motivation: 深度神经网络(DNNs)对分布外样本常产生过度自信的预测，需要贝叶斯不确定性量化。线性化拉普拉斯近似(LLA)通过线性化DNN并应用拉普拉斯推理来实现，但线性化可能降低对真实拉普拉斯近似的保真度

Method: 提出二次拉普拉斯近似(QLA)，使用高效幂迭代获得秩一因子来近似拉普拉斯对数后验中的每个二阶因子，避免计算完整的Hessian矩阵，预测时仍使用线性化模型

Result: 在五个回归数据集上，QLA相比LLA在不确定性估计方面取得了适度但一致的改进

Conclusion: QLA方法在不显著增加计算成本的情况下，通过二次近似改进了线性化拉普拉斯近似的后验精度，为深度神经网络的不确定性量化提供了更好的解决方案

Abstract: Deep neural networks (DNNs) often produce overconfident out-of-distribution predictions, motivating Bayesian uncertainty quantification. The Linearized Laplace Approximation (LLA) achieves this by linearizing the DNN and applying Laplace inference to the resulting model. Importantly, the linear model is also used for prediction. We argue this linearization in the posterior may degrade fidelity to the true Laplace approximation. To alleviate this problem, without increasing significantly the computational cost, we propose the Quadratic Laplace Approximation (QLA). QLA approximates each second order factor in the approximate Laplace log-posterior using a rank-one factor obtained via efficient power iterations. QLA is expected to yield a posterior precision closer to that of the full Laplace without forming the full Hessian, which is typically intractable. For prediction, QLA also uses the linearized model. Empirically, QLA yields modest yet consistent uncertainty estimation improvements over LLA on five regression datasets.

</details>


### [38] [Generator-based Graph Generation via Heat Diffusion](https://arxiv.org/abs/2602.03612)
*Anthony Stephenson,Ian Gallagher,Christopher Nemeth*

Main category: stat.ML

TL;DR: 提出基于生成器匹配的图生成框架，利用图拉普拉斯算子和热核定义连续时间扩散，通过最小化Bregman散度训练神经网络匹配生成器，实现图结构生成。


<details>
  <summary>Details</summary>
Motivation: 图生成建模在化学、生物、社交网络和知识表示等领域有广泛应用需求。现有扩散模型需要统一框架来注入领域特定的归纳偏置，同时保持神经近似器的灵活性。

Method: 将生成器匹配范式应用于图结构数据：1) 利用图拉普拉斯算子及其热核定义连续时间扩散；2) 拉普拉斯算子作为扩散的无穷小生成器；3) 热核提供初始图的条件扰动族；4) 通过最小化Bregman散度训练神经网络匹配真实生成器；5) 使用训练后的代理生成器模拟时间反转扩散过程生成新图。

Result: 实验研究表明，该方法能有效捕捉真实图和合成图的结构特性，统一并推广了现有的基于扩散的图生成模型。

Conclusion: 提出的框架通过拉普拉斯算子注入领域特定归纳偏置，同时保持神经近似器的灵活性，为图生成建模提供了统一且有效的方法。

Abstract: Graph generative modelling has become an essential task due to the wide range of applications in chemistry, biology, social networks, and knowledge representation. In this work, we propose a novel framework for generating graphs by adapting the Generator Matching (arXiv:2410.20587) paradigm to graph-structured data. We leverage the graph Laplacian and its associated heat kernel to define a continous-time diffusion on each graph. The Laplacian serves as the infinitesimal generator of this diffusion, and its heat kernel provides a family of conditional perturbations of the initial graph. A neural network is trained to match this generator by minimising a Bregman divergence between the true generator and a learnable surrogate. Once trained, the surrogate generator is used to simulate a time-reversed diffusion process to sample new graph structures. Our framework unifies and generalises existing diffusion-based graph generative models, injecting domain-specific inductive bias via the Laplacian, while retaining the flexibility of neural approximators. Experimental studies demonstrate that our approach captures structural properties of real and synthetic graphs effectively.

</details>


### [39] [Improved Analysis of the Accelerated Noisy Power Method with Applications to Decentralized PCA](https://arxiv.org/abs/2602.03682)
*Pierre Aguié,Mathieu Even,Laurent Massoulié*

Main category: stat.ML

TL;DR: 改进了加速噪声幂方法的分析，放宽了对扰动幅度的限制条件，保持了加速收敛率，并应用于去中心化PCA算法。


<details>
  <summary>Details</summary>
Motivation: 先前关于加速噪声幂方法的研究需要过于严格的扰动幅度上限限制，限制了实际应用。需要更宽松条件下的理论保证。

Method: 对加速噪声幂方法进行改进分析，推导出更宽松的噪声条件，证明分析是最坏情况最优的，并将结果应用于去中心化PCA算法设计。

Result: 在更温和的扰动条件下保持了加速收敛率，分析是最坏情况最优的，噪声条件无法进一步放宽而不牺牲收敛保证，设计了首个具有可证明加速收敛的去中心化PCA算法。

Conclusion: 改进了加速噪声幂方法的理论分析，放宽了实际应用限制，为去中心化PCA提供了首个具有加速收敛保证的算法，具有重要的理论和实践意义。

Abstract: We analyze the Accelerated Noisy Power Method, an algorithm for Principal Component Analysis in the setting where only inexact matrix-vector products are available, which can arise for instance in decentralized PCA. While previous works have established that acceleration can improve convergence rates compared to the standard Noisy Power Method, these guarantees require overly restrictive upper bounds on the magnitude of the perturbations, limiting their practical applicability. We provide an improved analysis of this algorithm, which preserves the accelerated convergence rate under much milder conditions on the perturbations. We show that our new analysis is worst-case optimal, in the sense that the convergence rate cannot be improved, and that the noise conditions we derive cannot be relaxed without sacrificing convergence guarantees. We demonstrate the practical relevance of our results by deriving an accelerated algorithm for decentralized PCA, which has similar communication costs to non-accelerated methods. To our knowledge, this is the first decentralized algorithm for PCA with provably accelerated convergence.

</details>


### [40] [Efficient Variance-reduced Estimation from Generative EHR Models: The SCOPE and REACH Estimators](https://arxiv.org/abs/2602.03730)
*Luke Solo,Matthew B. A. McDermott,William F. Parker,Bashar Ramadan,Michael C. Burkhart,Brett K. Beaulieu-Jones*

Main category: stat.ML

TL;DR: 提出SCOPE和REACH两种新估计器，利用生成模型丢弃的下一令牌概率分布，相比蒙特卡洛模拟实现10倍效率提升，降低临床部署成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于自监督训练的生成模型使用蒙特卡洛模拟预测临床结果存在三个关键局限：估计分布稀疏难以区分风险水平、计算成本极高、采样方差大，限制了在资源受限临床环境中的部署可行性。

Method: 提出两种新估计器：条件结果概率和估计器(SCOPE)和预期条件风险估计器(REACH)，利用标准蒙特卡洛方法丢弃的下一令牌概率分布，证明两者都是无偏估计，且REACH保证对任何模型和结果都能减少方差。

Result: 在MIMIC-IV数据集上使用ETHOS-ARES框架进行医院死亡率预测，SCOPE和REACH仅用10-11个样本就能匹配100个样本的蒙特卡洛性能，实现约10倍推理成本降低且不损害校准；ICU入院预测效率提升较温和(~1.2倍)，归因于结果的"自发性"较低。

Conclusion: SCOPE和REACH方法显著提高了生成式EHR模型在资源受限临床环境中部署的可行性，通过利用丢弃的概率信息实现更高效的临床结果预测。

Abstract: Generative models trained using self-supervision of tokenized electronic health record (EHR) timelines show promise for clinical outcome prediction. This is typically done using Monte Carlo simulation for future patient trajectories. However, existing approaches suffer from three key limitations: sparse estimate distributions that poorly differentiate patient risk levels, extreme computational costs, and high sampling variance. We propose two new estimators: the Sum of Conditional Outcome Probability Estimator (SCOPE) and Risk Estimation from Anticipated Conditional Hazards (REACH), that leverage next-token probability distributions discarded by standard Monte Carlo. We prove both estimators are unbiased and that REACH guarantees variance reduction over Monte Carlo sampling for any model and outcome. Empirically, on hospital mortality prediction in MIMIC-IV using the ETHOS-ARES framework, SCOPE and REACH match 100-sample Monte Carlo performance using only 10-11 samples (95% CI: [9,11]), representing a ~10x reduction in inference cost without degrading calibration. For ICU admission prediction, efficiency gains are more modest (~1.2x), which we attribute to the outcome's lower "spontaneity," a property we characterize theoretically and empirically. These methods substantially improve the feasibility of deploying generative EHR models in resource-constrained clinical settings.

</details>


### [41] [Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants](https://arxiv.org/abs/2602.03789)
*Gabriel Damsholt,Jes Frellsen,Susanne Ditlevsen*

Main category: stat.ML

TL;DR: 本文提出了一种在随机插值框架中转换不同调度方案和扩散系数的方法，证明了任意SDE样本路径在不同调度方案间的可转换性，并引入了点质量调度方案，在图像生成任务中实现了更少步数的采样。


<details>
  <summary>Details</summary>
Motivation: 随机插值方法统一了流和扩散模型，但其中的关键超参数——插值调度方案——对模型性能有重要影响。现有方法缺乏在不同调度方案间转换的理论基础，限制了模型的灵活性和效率。

Method: 1) 证明了任意扩散系数和调度方案的SDE样本路径可以唯一转换为另一任意调度方案和扩散系数的样本路径；2) 扩展随机插值框架以包含点质量调度方案；3) 在高斯数据假设下识别出使漂移项为零的惰性调度族；4) 将惰性调度转换应用于预训练流模型。

Result: 1) 建立了调度方案转换的理论基础；2) 识别出惰性调度族，其中确定性采样得到方差保持调度，统计最优SDE采样得到点质量调度；3) 在预训练流模型上应用惰性调度转换，实现了更少步数的图像生成而无需重新训练模型。

Conclusion: 本文提供了随机插值框架中调度方案转换的理论工具，扩展了调度方案的选择范围，特别是点质量调度方案。实际应用表明，这些理论结果能够提升现有生成模型的采样效率，为更灵活的生成建模提供了理论基础。

Abstract: Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A primary hyperparameter in these methods is the interpolation schedule that determines how to bridge a standard Gaussian base measure to an arbitrary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule into the unique sample path under another arbitrary schedule and diffusion coefficient. We then extend the stochastic interpolant framework to admit a larger class of point mass schedules in which the Gaussian base measure collapses to a point mass measure. Under the assumption of Gaussian data, we identify lazy schedule families that make the drift identically zero and show that with deterministic sampling one gets a variance-preserving schedule commonly used in diffusion models, whereas with statistically optimal SDE sampling one gets our point mass schedule. Finally, to demonstrate the usefulness of our theoretical results on realistic highly non-Gaussian data, we apply our lazy schedule conversion to a state-of-the-art pretrained flow model and show that this allows for generating images in fewer steps without retraining the model.

</details>


### [42] [Preference-based Conditional Treatment Effects and Policy Learning](https://arxiv.org/abs/2602.03823)
*Dovid Parnas,Mathieu Even,Julie Josse,Uri Shalit*

Main category: stat.ML

TL;DR: 提出基于偏好的条件处理效应估计新框架CPTE，仅需结果排序而非具体数值，可处理多变量、序数或偏好驱动结果，统一多种应用场景


<details>
  <summary>Details</summary>
Motivation: 传统处理效应估计需要具体数值结果，但许多实际场景（如医疗、政策评估）只有偏好排序信息。现有方法无法灵活处理多变量、序数或偏好驱动的异质性效应估计

Method: 基于条件偏好处理效应（CPTE）框架，仅要求结果在偏好规则下排序。提出匹配、分位数和分布回归估计策略，设计高效影响函数估计器校正插件偏差并最大化策略价值

Result: CPTE为先前不可识别的估计量提供新的可识别条件，合成和半合成实验显示明显的性能提升和实际影响

Conclusion: CPTE框架通过仅需偏好排序的灵活建模，统一了多种处理效应估计应用，为解决比较型估计量的固有不可识别性问题提供了新途径

Abstract: We introduce a new preference-based framework for conditional treatment effect estimation and policy learning, built on the Conditional Preference-based Treatment Effect (CPTE). CPTE requires only that outcomes be ranked under a preference rule, unlocking flexible modeling of heterogeneous effects with multivariate, ordinal, or preference-driven outcomes. This unifies applications such as conditional probability of necessity and sufficiency, conditional Win Ratio, and Generalized Pairwise Comparisons. Despite the intrinsic non-identifiability of comparison-based estimands, CPTE provides interpretable targets and delivers new identifiability conditions for previous unidentifiable estimands. We present estimation strategies via matching, quantile, and distributional regression, and further design efficient influence-function estimators to correct plug-in bias and maximize policy value. Synthetic and semi-synthetic experiments demonstrate clear performance gains and practical impact.

</details>
