<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 6]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 15]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Maritime Vessel Tracking](https://arxiv.org/abs/2512.11707)
*John Mahlon Scott,Hsin-Hsiung Huang*

Main category: stat.AP

TL;DR: 提出一个混合管道方法，结合物理模型筛选和神经网络分类器，解决AIS轨迹在船舶标识缺失情况下的重标记问题，特别是在数据稀疏、环境多样的美国水域场景中。


<details>
  <summary>Details</summary>
Motivation: AIS系统提供船舶位置和运动报告，但船舶标识可能缺失，需要重新标记轨迹。在数据稀疏、环境多样的全国性水域中，这是一个具有挑战性的问题。

Method: 提出混合管道方法：1）基于物理的筛选步骤，预测活动轨迹端点并选择可能的祖先轨迹；2）监督神经网络分类器，使用时空和运动一致性特征，从候选轨迹中选择或启动新轨迹。

Result: 在保留数据上，该方法相比无监督基线提高了位置准确性，表明结合简单运动模型和学习消歧可以扩展到异构、高容量的AIS数据流。

Conclusion: 结合物理模型筛选和机器学习分类器的混合方法，能够有效解决大规模、异构AIS数据流中的船舶轨迹重标记问题。

Abstract: The Automatic Identification System (AIS) provides time stamped vessel positions and kinematic reports that enable maritime authorities to monitor traffic. We consider the problem of relabeling AIS trajectories when vessel identifiers are missing, focusing on a challenging nationwide setting in which tracks are heavily downsampled and span diverse operating environments across continental U.S. waters. We propose a hybrid pipeline that first applies a physics-based screening step to project active track endpoints forward in time and select a small set of plausible ancestors for each new observation. A supervised neural classifier then chooses among these candidates, or initiates a new track, using engineered space time and kinematic consistency features. On held out data, this approach improves posit accuracy relative to unsupervised baselines, demonstrating that combining simple motion models with learned disambiguation can scale vessel relabeling to heterogeneous, high volume AIS streams.

</details>


### [2] [umx version 4.5: Extending Twin and Path-Based SEM in R with CLPM, MR-DoC, Definition Variables, $Ω$nyx Integration, and Censored distributions](https://arxiv.org/abs/2512.11063)
*Luis FS Castro-de-Araujo,Nathan Gillespie,Michael C Neale,Timothy Bates*

Main category: stat.AP

TL;DR: umx v4.5扩展了OpenMx包的功能，专注于纵向和因果双生子设计，增强了与图形建模工具的互操作性，提供了新的模型类型和简化的工作流程。


<details>
  <summary>Details</summary>
Motivation: 结构方程模型在行为遗传学和社会科学中应用广泛，但现有工具在纵向和因果双生子设计方面功能有限，且与图形建模工具的互操作性不足。需要增强umx包的功能，降低遗传流行病学分析的障碍。

Method: 在umx包原有基础上扩展功能，包括：经典和现代交叉滞后面板模型、结合多基因评分的孟德尔随机化因果方向双生子模型、改进定义变量支持、简化从Onyx导入路径的工作流程、增加审查变量分析工具、改进协变量占位符处理、简化性别限制建模、提供高效协变量残差化功能。

Result: umx v4.5显著扩展了纵向和因果双生子设计的功能，提高了与图形建模工具的互操作性，通过智能默认值和集成的高质量报告功能，加速了可重复、可靠、可发表的双生子和家庭建模。

Conclusion: umx v4.5的增强功能降低了遗传流行病学分析的障碍，通过提供更强大的建模工具和简化的工作流程，使研究人员能够更高效地进行可重复、可发表的双生子和家庭研究。

Abstract: Structural Equation Modeling (SEM) provides a powerful and flexible framework widely used in behavioral genetics and social sciences. Building on the original design of the umx package, which enhanced accessibility to OpenMx using concise syntax and helpful defaults, umx v4.5 significantly extends functionality for longitudinal and causal twin designs while improving interoperability with graphical modelling tools such as Onyx. New capabilities include: classic and modern cross-lagged panel model; Mendelian Randomization Direction-of-Causation (MR-DoC) twin models incorporating polygenic scores as instruments; expanded support for definition variables directly in umxRAM(); streamlined workflows for importing paths from $Ω$nyx; a dedicated tool for analyzing censored variables, particularly valuable in biomarker research; improved covariate placeholder handling for definition variables; umxSexLim() for simplified sex-limitation modelling across five twin groups, accommodating quantitative and qualitative sex differences; and umx_residualize() for efficient covariate residualization in wide- or long-format data. These advances accelerate reproducible, reliable, publication-ready twin and family modelling using intelligent defaults, and integrated journal-quality reporting, thereby lowering barriers to genetic epidemiological analyzes.

</details>


### [3] [A Unified Micro-Model for Loss Reserves, IBNR and Unearned Premium Risk with Dependence, Inflation, and Discounting](https://arxiv.org/abs/2512.11197)
*Emmanuel Hamel,Anas Abdallah,Ghislain Léveillé*

Main category: stat.AP

TL;DR: 提出统一的微观随机框架，联合建模损失准备金、已发生未报告准备金和未满期保费风险，考虑相关性、通胀和折现因素。


<details>
  <summary>Details</summary>
Motivation: 传统准备金模型通常独立处理不同风险，缺乏对相关性、通胀和折现的统一考虑。需要一种能够联合建模损失准备金、IBNR准备金和未满期保费风险的微观框架，以支持定价、准备金和资本管理的动态决策。

Method: 采用统一的微观随机框架，使用聚合趋势更新过程（ATRP）作为实现方式，在个体索赔层面建模支付、费用和延迟的相互作用。框架允许灵活的参数依赖结构和动态财务调整，并推导了预测支付的前两阶原始矩和联合条件矩的闭式表达式。

Result: 框架能够产生前瞻性的准备金和保费风险度量，可直接应用于定价、准备金和资本管理。通过医疗责任保险案例研究验证了方法的实际相关性，展示了数据异质性、参数不确定性、分布近似等方面的分析能力。

Conclusion: 统一的微观建模方法为长尾业务的动态负债和保费风险评估提供了优势，能够更好地处理相关性、通胀和折现等复杂因素，支持更准确的风险资本计算和决策制定。

Abstract: This paper introduces a unified micro-level stochastic framework for the joint modeling of loss reserves (RBNS), incurred but not reported (IBNR) reserves, and unearned premium risk under dependence, inflation, and discounting. The proposed framework accommodates interactions between indemnities, expenses, reporting delays, and settlement delays, while allowing for flexible parametric dependence structures and dynamic financial adjustments. An Aggregate Trend Renewal Process (ATRP) is used as one possible implementation of the joint model for payments, expenses, and delays; however, the methodological contribution of the paper lies in the unified micro-level reserving architecture rather than in the ATRP itself. The framework produces forward-looking reserve and premium risk measures with direct applications to pricing, reserving, and capital management.
  We implement the framework using an aggregate trend renewal process at the individual claim level, which can be applied to the usual run-off triangle to obtain predictions for each accident-development year. Closed-form expressions for the first two raw and joint conditional moments of predicted payments are derived, together with approximations of their distribution functions. A detailed case study on medical malpractice insurance illustrates the practical relevance of the approach and its calibration on real-world data. We also investigate data heterogeneity, parameter uncertainty, distributional approximations, premium risk, UPR sensitivity to operational delays and inflation, and risk capital implications under alternative assumptions. The results highlight the advantages of unified micro-level modeling for dynamic liability and premium risk assessment in long-tailed lines of business.

</details>


### [4] [Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis](https://arxiv.org/abs/2512.11593)
*Hyungrok Do,Yuyan Wang,Mengling Liu,Myeonggyun Lee*

Main category: stat.AP

TL;DR: NeuralPLSI：结合神经网络表达能力和半参数回归可解释性的环境混合物健康效应评估框架


<details>
  <summary>Details</summary>
Motivation: 现有评估复杂环境混合物健康效应的方法在灵活性、可解释性、可扩展性和对多种结局类型的支持方面存在局限，限制了实际应用价值。

Method: 提出基于神经网络的部分线性单指数模型框架，通过可学习投影构建可解释的暴露指数，并用灵活神经网络建模其与结局的关系。支持连续、二元和生存时间结局，通过bootstrap程序进行推断。

Result: 通过多种场景的模拟研究验证了方法性能，并在NHANES数据中展示了实际应用价值。方法具有可扩展性、可解释性和通用性。

Conclusion: NeuralPLSI为混合物分析提供了可扩展、可解释且通用的建模工具，并发布了开源软件包促进采用和可重复性。

Abstract: Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\texttt{https://github.com/hyungrok-do/NeuralPLSI}).

</details>


### [5] [Euclidean Ideal Point Estimation From Roll-Call Data via Distance-Based Bipartite Network Models](https://arxiv.org/abs/2512.11610)
*Seungju Lee,In Kyun Kim,Jong Hee Park,Ick Hoon Jin*

Main category: stat.AP

TL;DR: 提出基于距离的LSIRM模型替代传统理想点模型，在欧几里得度量空间中嵌入议员和法案，恢复度量结构以改进聚类分析和几何解释。


<details>
  <summary>Details</summary>
Motivation: 传统理想点模型使用高斯或二次效用函数，违反三角形不等式，产生非度量距离，这使几何解释复杂化并削弱了基于聚类和分散度的分析。

Method: 将潜在空间项目反应模型(LSIRM)应用于点名投票数据，将议员和法案视为二分网络中的节点，在欧几里得度量空间中联合嵌入。

Result: 通过模拟实验，欧几里得LSIRM能一致恢复潜在联盟结构，聚类分离度优于现有方法。应用于第118届美国众议院，模型改进了投票预测，法案嵌入澄清了交叉议题联盟。

Conclusion: 恢复理想点估计的度量结构能提供更清晰、更一致的推断，有助于理解党派凝聚力、派系分裂和多维立法行为。

Abstract: Conventional ideal point models rely on Gaussian or quadratic utility functions that violate the triangle inequality, producing non-metric distances that complicate geometric interpretation and undermine clustering and dispersion-based analyses. We introduce a distance-based alternative that adapts the Latent Space Item Response Model (LSIRM) to roll-call data, treating legislators and bills as nodes in a bipartite network jointly embedded in a Euclidean metric space. Through controlled simulations, Euclidean LSIRM consistently recovers latent coalition structure with superior cluster separation relative to existing methods. Applied to the 118th U.S. House, the model improves vote prediction and yields bill embeddings that clarify cross-cutting issue alignments. The results show that restoring metric structure to ideal point estimation provides a clearer and more coherent inference about party cohesion, factional divisions, and multidimensional legislative behavior.

</details>


### [6] [Dynamic Conditional SKEPTIC](https://arxiv.org/abs/2512.11648)
*Gabriele Di Luzio,Giacomo Morelli*

Main category: stat.AP

TL;DR: DCS是一种半参数方法，利用非参数秩统计量（Spearman's rho和Kendall's tau）来高效稳健地估计多元模型中的时变相关性，相比传统DCC模型有更好的诊断检验效果。


<details>
  <summary>Details</summary>
Motivation: 传统动态条件相关（DCC）模型在估计时变相关性时可能存在局限性，需要更稳健高效的方法来估计多元金融时间序列中的时变相关矩阵。

Method: 提出动态条件SKEPTIC（DCS）半参数方法，利用非参数秩统计量（Spearman's rho和Kendall's tau）估计未知相关矩阵，并讨论了模型的平稳性、beta-和rho-混合条件。

Result: 在S&P100和S&P500股票数据（2013-2025）上的应用表明，DCS相比经典DCC模型改善了诊断检验，提供了不相关且正态分布的残差。风险管理应用中，基于DCS的全局最小方差组合比DCC和DCC-NL模型具有更低的换手率，且在S&P100成分股组合中实现了更高的夏普比率。

Conclusion: DCS方法为时变相关矩阵估计提供了一种更稳健高效的半参数替代方案，在金融风险管理和投资组合构建中具有实际应用价值。

Abstract: We introduce the Dynamic Conditional SKEPTIC (DCS), a semiparametric approach for efficiently and robustly estimating time-varying correlations in multivariate models. We exploit nonparametric rank-based statistics, namely Spearman's rho and Kendall's tau, to estimate the unknown correlation matrix and discuss the stationarity, beta- and rho- mixing conditions of the model. We illustrate the methodology by estimating the time-varying conditional correlation matrix of the stocks included in the S&P100 and S&P500 during the period from 02/01/2013 to 23/01/2025. The results show that DCS improves diagnostic checks compared to the classical Dynamic Conditional Correlation (DCC) models, providing uncorrelated and normally distributed residuals. A risk management application shows that global minimum variance portfolios estimated using the DCS model exhibit lower turnover than those based on the DCC and DCC-NL models, while also achieving higher Sharpe ratios for portfolios constructed from S&P 100 constituents.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [7] [STARK denoises spatial transcriptomics images via adaptive regularization](https://arxiv.org/abs/2512.10994)
*Sharvaj Kubal,Naomi Graham,Matthieu Heitz,Andrew Warren,Michael P. Friedlander,Yaniv Plan,Geoffrey Schiebinger*

Main category: stat.ML

TL;DR: STARK是一种用于空间转录组学图像去噪的新方法，通过自适应图拉普拉斯正则化和核岭回归，在超低测序深度下有效揭示细胞身份并实现基因表达插值。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学在超低测序深度下噪声严重，难以准确识别细胞身份，需要一种有效的去噪方法来提高数据质量。

Method: STARK方法结合核岭回归和自适应图拉普拉斯正则化，采用交替最小化方案：1)用固定图更新图像，2)基于新图像更新图。通过改进的表示定理将无限维问题降为有限维。

Result: STARK在真实空间转录组数据上的去噪性能（通过标签转移准确度评估）持续优于竞争方法，理论证明迭代收敛到平稳点，统计收敛速度为O(R^{-1/2})。

Conclusion: STARK为超低测序深度下的空间转录组学去噪提供了有效解决方案，通过自适应图正则化显著提高了细胞身份识别的准确性。

Abstract: We present an approach to denoising spatial transcriptomics images that is particularly effective for uncovering cell identities in the regime of ultra-low sequencing depths, and also allows for interpolation of gene expression. The method -- Spatial Transcriptomics via Adaptive Regularization and Kernels (STARK) -- augments kernel ridge regression with an incrementally adaptive graph Laplacian regularizer. In each iteration, we (1) perform kernel ridge regression with a fixed graph to update the image, and (2) update the graph based on the new image. The kernel ridge regression step involves reducing the infinite dimensional problem on a space of images to finite dimensions via a modified representer theorem. Starting with a purely spatial graph, and updating it as we improve our image makes the graph more robust to noise in low sequencing depth regimes. We show that the aforementioned approach optimizes a block-convex objective through an alternating minimization scheme wherein the sub-problems have closed form expressions that are easily computed. This perspective allows us to prove convergence of the iterates to a stationary point of this non-convex objective. Statistically, such stationary points converge to the ground truth with rate $\mathcal{O}(R^{-1/2})$ where $R$ is the number of reads. In numerical experiments on real spatial transcriptomics data, the denoising performance of STARK, evaluated in terms of label transfer accuracy, shows consistent improvement over the competing methods tested.

</details>


### [8] [An Efficient Variant of One-Class SVM with Lifelong Online Learning Guarantees](https://arxiv.org/abs/2512.11052)
*Joe Suk,Samory Kpotufe*

Main category: stat.ML

TL;DR: SONAR：一种用于非平稳流数据的高效单类SVM求解器，具有理论保证和自适应能力


<details>
  <summary>Details</summary>
Motivation: 传统离线异常检测方法（如核单类SVM）在非平稳流数据场景下计算量大且容易产生高假阴性错误，需要更高效的在线解决方案

Method: 提出SONAR：基于SGD的强凸正则化单类SVM求解器，支持集成方法和变点检测来处理对抗性非平稳数据

Result: SONAR在I/II类错误上优于传统OCSVM，在良性分布漂移下具有终身学习保证，在对抗性非平稳数据中通过集成方法实现自适应保证

Conclusion: SONAR为单通道非平稳流数据异常检测提供了高效且理论保证的解决方案，在合成和真实数据集上验证了有效性

Abstract: We study outlier (a.k.a., anomaly) detection for single-pass non-stationary streaming data. In the well-studied offline or batch outlier detection problem, traditional methods such as kernel One-Class SVM (OCSVM) are both computationally heavy and prone to large false-negative (Type II) errors under non-stationarity. To remedy this, we introduce SONAR, an efficient SGD-based OCSVM solver with strongly convex regularization. We show novel theoretical guarantees on the Type I/II errors of SONAR, superior to those known for OCSVM, and further prove that SONAR ensures favorable lifelong learning guarantees under benign distribution shifts. In the more challenging problem of adversarial non-stationary data, we show that SONAR can be used within an ensemble method and equipped with changepoint detection to achieve adaptive guarantees, ensuring small Type I/II errors on each phase of data. We validate our theoretical findings on synthetic and real-world datasets.

</details>


### [9] [Provable Recovery of Locally Important Signed Features and Interactions from Random Forest](https://arxiv.org/abs/2512.11081)
*Kata Vuk,Nicolas Alexander Ihlo,Merle Behr*

Main category: stat.ML

TL;DR: 提出一种新的局部特征与交互重要性方法，用于随机森林的个体预测解释，能识别特征共现模式并区分特征值大小对预测的影响。


<details>
  <summary>Details</summary>
Motivation: 在个性化医疗等领域，需要局部解释而非全局特征重要性。随机森林广泛使用，但现有局部FII方法理论理解有限，难以解释个体预测中的高重要性分数。

Method: 提出新的局部、模型特定的FII方法，识别决策路径上特征的频繁共现模式，结合全局模式和特定测试点的路径模式。

Result: 在局部稀疏尖峰模型下，证明方法能一致恢复真实的局部信号特征及其交互作用，并能识别是大特征值还是小特征值驱动预测。

Conclusion: 该方法通过模拟研究和真实数据示例展示了实用性，为随机森林的局部解释提供了理论保证和实用工具。

Abstract: Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example.

</details>


### [10] [TPV: Parameter Perturbations Through the Lens of Test Prediction Variance](https://arxiv.org/abs/2512.11089)
*Devansh Arpit*

Main category: stat.ML

TL;DR: 该论文提出了测试预测方差（TPV）的概念，将其作为连接深度网络泛化能力的统一框架，并展示了TPV在训练集上的估计能收敛到测试集值，且具有跨数据集和架构的稳定性，最后应用于剪枝任务。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络泛化能力时存在多种观察现象，需要统一的框架来连接这些经典观察。作者旨在找到一个能统一分析不同参数扰动机制（如SGD噪声、标签噪声等）的量化指标。

Method: 提出测试预测方差（TPV）作为核心概念，即模型输出对训练解附近参数扰动的一阶敏感性。TPV的迹形式将训练模型的几何结构与特定扰动机制分离，允许在单一框架下分析多种参数扰动。

Result: 理论上证明在过参数化极限下，训练集上估计的TPV收敛到测试集值；实证显示TPV在不同数据集和架构（包括极窄网络）中表现出稳定性，并与干净测试损失良好相关；将剪枝建模为TPV扰动，得到简单无标签的重要性度量，性能与最先进剪枝方法相当。

Conclusion: TPV为深度网络泛化提供了统一的理论框架，能够连接多种经典观察，并在实践中具有应用价值，特别是在模型剪枝等任务中表现出实用性。

Abstract: We identify test prediction variance (TPV) -- the first-order sensitivity of model outputs to parameter perturbations around a trained solution -- as a unifying quantity that links several classical observations about generalization in deep networks. TPV is a fully label-free object whose trace form separates the geometry of the trained model from the specific perturbation mechanism, allowing a broad family of parameter perturbations like SGD noise, label noise, finite-precision noise, and other post-training perturbations to be analyzed under a single framework. Theoretically, we show that TPV estimated on the training set converges to its test-set value in the overparameterized limit, providing the first result that prediction variance under local parameter perturbations can be inferred from training inputs alone. Empirically, TPV exhibits a striking stability across datasets and architectures -- including extremely narrow networks -- and correlates well with clean test loss. Finally, we demonstrate that modeling pruning as a TPV perturbation yields a simple label-free importance measure that performs competitively with state-of-the-art pruning methods, illustrating the practical utility of TPV. Code available at github.com/devansharpit/TPV.

</details>


### [11] [Data-Driven Model Reduction using WeldNet: Windowed Encoders for Learning Dynamics](https://arxiv.org/abs/2512.11090)
*Biraj Dahal,Jiahui Cheng,Hao Liu,Rongjie Lai,Wenjing Liao*

Main category: stat.ML

TL;DR: WeldNet：一种基于窗口化编码器的非线性模型降阶框架，通过分窗口训练自编码器捕捉复杂演化系统的低维表示，使用传播网络学习窗口内动力学，转码器连接相邻窗口，简化长时程动力学建模。


<details>
  <summary>Details</summary>
Motivation: 科学和工程中的许多问题涉及复杂物理过程产生的高维时变数据集，这些模拟成本高昂。传统方法难以有效处理非线性、高维的演化系统，需要开发更高效的数据驱动降阶模型。

Method: 1. 将时域划分为多个重叠窗口；2. 在每个窗口内使用自编码器进行非线性降维，捕捉潜在编码；3. 训练传播网络学习窗口内潜在编码的演化；4. 训练转码器连接相邻窗口的潜在编码，确保跨窗口一致性。

Result: 数值实验表明，WeldNet能够有效捕捉非线性潜在结构及其底层动力学，在多种微分方程上表现优于传统投影方法和近期发展的非线性模型降阶方法。

Conclusion: WeldNet通过窗口化分解将长时程动力学分解为多个短片段，简化了传播网络训练，同时通过转码器保持窗口间一致性，为复杂演化系统提供了一种有效的非线性模型降阶框架，并建立了理论保证其表示能力。

Abstract: Many problems in science and engineering involve time-dependent, high dimensional datasets arising from complex physical processes, which are costly to simulate. In this work, we propose WeldNet: Windowed Encoders for Learning Dynamics, a data-driven nonlinear model reduction framework to build a low-dimensional surrogate model for complex evolution systems. Given time-dependent training data, we split the time domain into multiple overlapping windows, within which nonlinear dimension reduction is performed by auto-encoders to capture latent codes. Once a low-dimensional representation of the data is learned, a propagator network is trained to capture the evolution of the latent codes in each window, and a transcoder is trained to connect the latent codes between adjacent windows. The proposed windowed decomposition significantly simplifies propagator training by breaking long-horizon dynamics into multiple short, manageable segments, while the transcoders ensure consistency across windows. In addition to the algorithmic framework, we develop a mathematical theory establishing the representation power of WeldNet under the manifold hypothesis, justifying the success of nonlinear model reduction via deep autoencoder-based architectures. Our numerical experiments on various differential equations indicate that WeldNet can capture nonlinear latent structures and their underlying dynamics, outperforming both traditional projection-based approaches and recently developed nonlinear model reduction methods.

</details>


### [12] [Conditional Coverage Diagnostics for Conformal Prediction](https://arxiv.org/abs/2512.11779)
*Sacha Braun,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: stat.ML

TL;DR: 提出ERT指标，将条件覆盖率评估转化为分类问题，通过分类器风险与目标覆盖率的差异来保守估计条件覆盖偏差，比现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 条件覆盖率评估是预测系统可靠性评估中最持久的挑战之一。现有方法无法保证条件覆盖率，且现有指标存在样本效率低和过拟合问题，缺乏解释局部偏差的清晰方法。

Method: 将条件覆盖率估计转化为分类问题：当任何分类器的风险低于目标覆盖率时，条件覆盖率就被违反。通过选择适当的损失函数，风险差异可以保守估计L1/L2距离等自然误覆盖率度量，并能区分过覆盖、欠覆盖和非恒定目标覆盖率的影响。

Result: 提出的ERT指标使用现代分类器比简单分类器（如CovGap）具有更高的统计功效。实验验证了ERT的有效性，并用于基准测试不同保形预测方法。

Conclusion: ERT指标为理解、诊断和改进预测系统的条件可靠性提供了新视角，并发布了开源实现包，有助于实际应用。

Abstract: Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [13] [Candidate set sampling: A note on theoretical guarantees](https://arxiv.org/abs/2512.11475)
*Shifeng Xiong*

Main category: stat.CO

TL;DR: 提出一种基于密度函数离散化的简单数值采样方法——候选集采样，该方法仅需知道密度函数（至多未知归一化常数），具有非迭代、维度无关、易于实现、收敛快、计算成本低的特点。


<details>
  <summary>Details</summary>
Motivation: 需要一种简单高效的采样方法，仅依赖密度函数（可包含未知归一化常数），避免复杂的迭代过程，降低计算成本，同时保持维度无关性。

Method: 候选集采样方法：通过对密度函数进行直接离散化，生成候选集，然后基于离散化结果进行采样。该方法是非迭代的，仅需密度函数信息。

Result: 方法具有快速收敛和低计算成本的特点，文中给出了其基本收敛性质的理论分析。

Conclusion: 候选集采样是一种简单、高效、维度无关的采样方法，仅需密度函数信息，易于实现，适用于多种采样场景。

Abstract: In this note we introduce a simple numerical sampling method, called candidate set sampling, which is based on an straightforward discretization to the density function. This method requires the knowledge of the density function (up to an unknown normalizing constant) only. Furthermore, candidate set sampling is non-iterative, dimension-free, and easy to implement, with fast convergence and low computational cost. We present its basic convergence properties in the note.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [14] [On a class of constrained Bayesian filters and their numerical implementation in high-dimensional state-space Markov models](https://arxiv.org/abs/2512.11012)
*Utku Erdogan,Gabriel J. Lord,Joaquin Miguez*

Main category: stat.ME

TL;DR: 提出约束贝叶斯滤波框架，通过状态空间约束确保滤波稳定性，在连续-离散设置中利用屏障函数修改漂移项实现约束，理论证明稳定性并提供误差界


<details>
  <summary>Details</summary>
Motivation: 非线性高维动态系统的贝叶斯滤波实现面临稳定性、精度和计算成本之间的微妙权衡挑战，需要设计既能保证稳定性又能控制计算复杂度的滤波方法

Method: 提出约束贝叶斯滤波框架，通过状态空间紧凑子集约束滤波过程；在连续-离散设置中，使用屏障函数修改Itô过程的漂移项来实现约束，与Doob h-变换方法相关

Result: 为约束滤波提供稳定性充分条件和相对于原始无约束滤波的近似误差率；在部分可观测的随机Lorenz 96模型实验中验证理论结果和方法性能

Conclusion: 约束贝叶斯滤波框架为解决非线性高维系统的滤波问题提供了理论保证和实用实现方法，通过状态约束平衡稳定性与计算效率

Abstract: Bayesian filtering is a key tool in many problems that involve the online processing of data, including data assimilation, optimal control, nonlinear tracking and others. Unfortunately, the implementation of filters for nonlinear, possibly high-dimensional, dynamical systems is far from straightforward, as computational methods have to meet a delicate trade-off involving stability, accuracy and computational cost. In this paper we investigate the design, and theoretical features, of constrained Bayesian filters for state space models. The constraint on the filter is given by a sequence of compact subsets of the state space that determines the sources and targets of the Markov transition kernels in the dynamical model. Subject to such constraints, we provide sufficient conditions for filter stability and approximation error rates with respect to the original (unconstrained) Bayesian filter. Then, we look specifically into the implementation of constrained filters in a continuous-discrete setting where the state of the system is a continuous-time stochastic Itô process but data are collected sequentially over a time grid. We propose an implementation of the constraint that relies on a data-driven modification of the drift of the Itô process using barrier functions, and discuss the relation of this scheme with methods based on the Doob $h$-transform. Finally, we illustrate the theoretical results and the performance of the proposed methods in computer experiments for a partially-observed stochastic Lorenz 96 model.

</details>


### [15] [The Cumulative Residual Mathai--Haubold Entropy and its Non-parametric Inference](https://arxiv.org/abs/2512.10997)
*Anija C. R,Smitha S.,Sudheesh K. Kattumannil*

Main category: stat.ME

TL;DR: 提出累积残差Mathai-Haubold熵及其动态版本，建立分布函数表征的唯一性，开发基于核密度估计的非参数估计器，并通过模拟和实际数据验证


<details>
  <summary>Details</summary>
Motivation: 扩展信息论中的熵度量，引入累积残差Mathai-Haubold熵及其动态版本，以更好地表征分布特性并应用于可靠性分析

Method: 提出CRMHE和DCRMHE概念，建立分布函数表征的唯一性定理，基于生存函数的核密度估计开发非参数估计器，通过蒙特卡洛模拟评估估计器性能

Result: 建立了DCRMHE在分布函数表征中的唯一性，开发了有效的非参数估计器，模拟研究验证了估计器性能，实际数据集（飞机挡风玻璃和机械开关失效时间）展示了应用价值

Conclusion: 累积残差Mathai-Haubold熵及其动态版本是有效的分布表征工具，提出的非参数估计器在实际应用中具有良好性能，为可靠性分析提供了新方法

Abstract: We introduce the cumulative residual Mathai--Haubold entropy (CRMHE) and investigate its properties. We then propose a dynamic counterpart, the dynamic cumulative residual Mathai--Haubold entropy (DCRMHE), and establish its uniqueness in characterizing the distribution function. Non-parametric estimators for the CRMHE and DCRMHE are developed based on the kernel density estimation of the survival function. The efficacy of the estimators is assessed through a comprehensive Monte Carlo simulation study. The relevance of the proposed DCRMHE estimator is illustrated using two real-world datasets: on the failure times of 70 aircraft windshields and failure times of 40 randomly selected mechanical switches.

</details>


### [16] [Gaussian random field's anisotropy using excursion sets](https://arxiv.org/abs/2512.11085)
*Jean-Marc Azaïs,Federico Dalmao,Yohann De Castro*

Main category: stat.ME

TL;DR: 提出从单次观测的二值图像中检测和估计平稳随机场各向异性的方法，包括推广Cabaña轮廓法到任意维度，以及新的模型无关各向同性统计检验。


<details>
  <summary>Details</summary>
Motivation: 从单次观测的二值图像（无均值、方差、阈值先验知识）中检测随机场的各向异性是一个具有挑战性的问题，现有方法通常需要知道协方差结构。

Method: 1. 将Cabaña轮廓法推广到任意维度，通过分析游程集边界法向量的Palm分布；2. 提出基于轮廓法的模型无关统计检验，构建渐近卡方分布的统计量。

Result: 数值实验表明检验方法校准良好且比基于模型的方法更有效，各向异性参数估计稳健高效；成功应用于Planck CMB数据的准各向同性检验。

Conclusion: 该方法能从单次二值观测中有效检测和估计随机场各向异性，无需协方差结构先验知识，为各向异性分析提供了实用工具。

Abstract: This paper addresses the problem of detecting and estimating the anisotropy of a stationary real-valued random field from a single realization of one of its excursion sets. This setting is challenging as it relies on observing a binary image without prior knowledge of the field's mean, variance, or the specific threshold value.
  Our first contribution is to propose a generalization of Cabaña's contour method to arbitrary dimensions by analyzing the Palm distribution of normal vectors along the excursion set boundaries. We demonstrate that the anisotropy parameters can be recovered by solving a smooth and strongly convex optimization problem involving the eigenvalues of the empirical covariance matrix of these normal vectors.
  Our second main contribution is a new, model-agnostic statistical test for isotropy in dimension two. We introduce a statistic based on the contour method which is asymptotically distributed as a chi-squared variable with two degrees of freedom under the null hypothesis of quasi-isotropy. Unlike existing methods based on Lipschitz-Killing curvatures, this procedure does not require knowledge of the random field's covariance structure.
  Extensive numerical experiments show that our test is well-calibrated and more powerful than model-based alternatives as well as that the estimation of the anisotropy parameters, including the directions, is robust and efficient. Finally, we apply this framework to test the quasi-isotropy of the Cosmic Microwave Background (CMB) using the Planck data release 3 mission.

</details>


### [17] [Autotune: fast, accurate, and automatic tuning parameter selection for LASSO](https://arxiv.org/abs/2512.11139)
*Tathagata Sadhukhan,Ines Wilms,Stephan Smeekes,Sumanta Basu*

Main category: stat.ME

TL;DR: 提出autotune方法，通过交替优化回归系数和噪声标准差来自动调节Lasso参数，在低信噪比下表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: Lasso在高维回归和时间序列模型（如VAR）中广泛应用，但参数选择仍然是一个挑战。现有方法在低信噪比情况下表现不佳，需要更高效准确的调参方法。

Method: 提出autotune策略，通过交替优化惩罚高斯对数似然函数来同时估计回归系数和噪声标准差，实现Lasso参数的自动调节。

Result: 在回归和VAR模型的模拟实验中，autotune比现有方法更快，在低信噪比下具有更好的泛化能力和模型选择性能。同时提供了新的噪声标准差估计器和高维推断方法。

Conclusion: autotune是一种有效的Lasso自动调参方法，在低信噪比情况下表现优异，提供了实用的噪声估计和稀疏性诊断工具，并在实际金融数据中验证了其有效性。

Abstract: Least absolute shrinkage and selection operator (Lasso), a popular method for high-dimensional regression, is now used widely for estimating high-dimensional time series models such as the vector autoregression (VAR). Selecting its tuning parameter efficiently and accurately remains a challenge, despite the abundance of available methods for doing so. We propose $\mathsf{autotune}$, a strategy for Lasso to automatically tune itself by optimizing a penalized Gaussian log-likelihood alternately over regression coefficients and noise standard deviation. Using extensive simulation experiments on regression and VAR models, we show that $\mathsf{autotune}$ is faster, and provides better generalization and model selection than established alternatives in low signal-to-noise regimes. In the process, $\mathsf{autotune}$ provides a new estimator of noise standard deviation that can be used for high-dimensional inference, and a new visual diagnostic procedure for checking the sparsity assumption on regression coefficients. Finally, we demonstrate the utility of $\mathsf{autotune}$ on a real-world financial data set. An R package based on C++ is also made publicly available on Github.

</details>


### [18] [Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems](https://arxiv.org/abs/2512.11150)
*Eddie Landesberg*

Main category: stat.ME

TL;DR: Causal Judge Evaluation (CJE)框架解决了LLM-as-judge评估中的三个统计缺陷：分数未校准导致偏好反转、置信区间覆盖率接近0%、重要性加权估计器在有限重叠下失效。CJE通过自动校准、权重稳定化和不确定性感知推理，以14倍低成本实现接近oracle质量的排名准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-judge评估方法存在统计缺陷：未校准分数可能导致偏好反转，朴素置信区间覆盖率接近0%，重要性加权估计器在有限重叠下失效，即使有效样本量很高。这些问题使得模型评估结果不可靠。

Method: CJE框架包含三个核心组件：1) AutoCal-R：通过均值保持等渗回归进行奖励校准；2) SIMCal-W：通过S-单调候选堆叠进行权重稳定化；3) Oracle-Uncertainty Aware (OUA)推理：将校准不确定性传播到置信区间中。还提出了Coverage-Limited Efficiency (CLE)诊断来解释IPS估计器失效的原因。

Result: 在4,961个Chatbot Arena提示上，CJE以14倍低成本（仅需5% oracle标签）实现了99%的成对排名准确性（平均94%），接近oracle质量。OUA将覆盖率从接近0%提升到约86%（直接法）和96%（堆叠DR法），而朴素区间严重欠覆盖。

Conclusion: CJE框架有效解决了LLM-as-judge评估中的统计缺陷，通过校准、权重稳定化和不确定性感知推理，以显著更低的成本获得可靠的模型评估结果。该方法为大规模模型评估提供了统计上健全的解决方案。

Abstract: LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover.

</details>


### [19] [Estimation of Contextual Exposure to HIV from GPS Data](https://arxiv.org/abs/2512.11159)
*Haoyang Wu,Zhaoxing Wu,Thulile Mathenjwa,Elphas Okango,Khai Hoan Tram,Margot Otto,Maxime Inghels,Paul Mee,Diego Cuadros,Hae-Young Kim,Till Barnighausen,Frank Tanser,Adrian Dobra*

Main category: stat.ME

TL;DR: 提出一个统计方法框架，结合GPS数据和HIV监测数据，估计农村地区年轻人的HIV情境暴露风险，分析移动模式与HIV暴露的关系。


<details>
  <summary>Details</summary>
Motivation: 需要分析南非农村地区年轻人的移动模式与HIV暴露风险之间的关系，现有方法无法有效结合GPS移动数据、HIV监测数据和社会人口调查数据进行综合分析。

Method: 开发综合性统计方法框架，包括：1) 网格单元级别的本地HIV流行率估计；2) 基于GPS数据的人类活动空间估计；3) 结合HIV监测、社会人口调查和GPS数据；4) 分析移动模式与HIV暴露的关系。

Result: 建立了能够分析性别和年龄如何系统影响移动模式范围和结构的框架，能够评估活动空间扩展如何改变HIV情境暴露，并识别HIV感染风险增加的个体。

Conclusion: 该框架为研究移动模式与HIV暴露风险的关系提供了有效工具，特别适用于资源有限地区，有助于识别高风险人群并制定针对性干预措施。

Abstract: We present a comprehensive statistical methodological framework for estimating contextual exposure to HIV that includes local (grid-cell level) estimation of HIV prevalence and human activity space estimation based on GPS data. The development of our framework was necessary to analyze HIV surveillance and sociodemographic survey data in conjunction with GPS data collected in rural KwaZulu-Natal, South Africa, to study the mobility patterns of young people. Based on mobility and contextual exposure measures, we examine whether the sex and age of study participants systematically influence the extent and structure of their mobility patterns. We discuss techniques for investigating how the study participants' contextual exposure to HIV changes as their activity spaces expand beyond residential locations, as well as methods for identifying study participants who may be at increased risk of acquiring HIV. KEYWORDS: Contextual HIV exposure; GPS-based mobility analysis; Activity space; HIV prevalence mapping

</details>


### [20] [Network Estimation for Stationary Time Series](https://arxiv.org/abs/2512.11406)
*Madeline A. Shelley,Chiara Boetti,Marina I. Knight,Matthew A. Nunes*

Main category: stat.ME

TL;DR: 提出一种基于小波域的图形套索方法，用于推断高维平稳多元时间序列的稀疏条件独立图


<details>
  <summary>Details</summary>
Motivation: 高维多变量时间序列在科学和工业应用中很常见，需要识别数据中的关键依赖结构以支持后续分析任务（如预测）。虽然图形模型可以估计条件独立图，但时间序列数据中的时间依赖性使这一任务具有挑战性。

Method: 采用局部平稳小波建模框架，将估计问题重新表述为基于小波域的图形套索（graphical lasso）公式，通过数据驱动的方式推断高维平稳多元时间序列的稀疏条件独立图。

Result: 理论结果表明该估计方案在确定输入时间序列数据的稀疏依赖结构时具有良好的收敛性。通过大量模拟实验验证了方法的性能，并在COVID-19患者住院的真实数据集上展示了其适用性。

Conclusion: 提出了一种新颖的小波域技术，能够有效推断高维平稳多元时间序列的稀疏条件独立图，为时间序列数据的依赖结构分析提供了新的解决方案。

Abstract: High-dimensional multivariate time series are common in many scientific and industrial applications, where the interest lies in identifying key dependence structure within the data for subsequent analysis tasks, such as forecasting. An important avenue to achieve this is through the estimation of the conditional independence graph via graphical models, although for time series data settings the underpinning temporal dependence can make this task challenging. In this article, we propose a novel wavelet domain technique that allows the data-driven inference of the (sparse) conditional independence graph of a high-dimensional stationary multivariate time series. By adopting the locally stationary wavelet modelling framework, we repose the estimation problem as a well-principled wavelet domain graphical lasso formulation. Theoretical results establish that our associated estimation scheme enjoys good consistency properties when determining sparse dependence structure in input time series data. The performance of the proposed method is illustrated using extensive simulations and we demonstrate its applicability on a real-world dataset representing hospitalisations of COVID-19 patients.

</details>


### [21] [Conditional Copula models using loss-based Bayesian Additive Regression Trees](https://arxiv.org/abs/2512.11427)
*Tathagata Basu,Fabrizio Leisen,Cristiano Villa,Kevin Wilson*

Main category: stat.ME

TL;DR: 提出基于BART的条件copula半参数模型，采用损失先验降低树复杂度，开发自适应RJMCMC算法，应用于GDP对男女预期寿命与识字率依赖关系的影响分析


<details>
  <summary>Details</summary>
Motivation: 研究外部影响下随机变量间的依赖关系是多元分析中的挑战性问题，需要能够建模复杂函数关系同时避免过拟合的方法

Method: 提出基于BART的条件copula半参数模型，采用损失先验控制树复杂度，开发自适应RJMCMC算法处理复杂非光滑似然函数

Result: 方法能有效恢复真实树结构，近似复杂条件copula参数，自适应算法能在次优提案方差下探索真实似然区域

Conclusion: 该方法为外部影响下的依赖关系建模提供了有效工具，通过GDP对男女预期寿命与识字率依赖关系的案例研究验证了实用性

Abstract: The study of dependence between random variables under external influences is a challenging problem in multivariate analysis. We address this by proposing a novel semi-parametric approach for conditional copula models using Bayesian additive regression trees (BART) models. BART is becoming a popular approach in statistical modelling due to its simple ensemble type formulation complemented by its ability to provide inferential insights. Although BART allows us to model complex functional relationships, it tends to suffer from overfitting. In this article, we exploit a loss-based prior for the tree topology that is designed to reduce the tree complexity. In addition, we propose a novel adaptive Reversible Jump Markov Chain Monte Carlo algorithm that is ergodic in nature and requires very few assumptions allowing us to model complex and non-smooth likelihood functions with ease. Moreover, we show that our method can efficiently recover the true tree structure and approximate a complex conditional copula parameter, and that our adaptive routine can explore the true likelihood region under a sub-optimal proposal variance. Lastly, we provide case studies concerning the effect of gross domestic product on the dependence between the life expectancies and literacy rates of the male and female populations of different countries.

</details>


### [22] [Covariate-assisted graph matching](https://arxiv.org/abs/2512.11761)
*Trisha Dawn,Jesús Arroyo*

Main category: stat.ME

TL;DR: 提出两种基于协变量的种子图匹配方法，通过整合节点/边特征信息提升网络数据对齐精度


<details>
  <summary>Details</summary>
Motivation: 现有图匹配方法大多忽略协变量信息，限制了在非可识别和错误匹配情况下的性能，需要开发能够利用辅助特征的改进方法

Method: 提出两种协变量辅助的种子图匹配方法：1) 在全图上求解二次分配问题(QAP)；2) 仅利用种子节点的局部邻域结构以提高计算可扩展性。基于条件建模框架，使用广义线性模型(GLM)建模邻接矩阵元素

Result: 建立了模型估计误差和QAP解精确恢复的理论保证，数值实验和学术谱系与协作网络匹配应用显示方法能显著提升对齐精度

Conclusion: 通过整合协变量信息，为经典图匹配问题提供了实用且改进的框架，在广泛应用中能更有效地结合网络数据

Abstract: Data integration is essential across diverse domains, from historical records to biomedical research, facilitating joint statistical inference. A crucial initial step in this process involves merging multiple data sources based on matching individual records, often in the absence of unique identifiers. When the datasets are networks, this problem is typically addressed through graph matching methodologies. For such cases, auxiliary features or covariates associated with nodes or edges can be instrumental in achieving improved accuracy. However, most existing graph matching techniques do not incorporate this information, limiting their performance against non-identifiable and erroneous matches. To overcome these limitations, we propose two novel covariate-assisted seeded graph matching methods, where a partial alignment for a set of nodes, called seeds, is known. The first one solves a quadratic assignment problem (QAP) over the whole graph, while the second one only leverages the local neighborhood structure of seed nodes for computational scalability. Both methods are grounded in a conditional modeling framework, where elements of one graph's adjacency matrix are modeled using a generalized linear model (GLM), given the other graph and the available covariates. We establish theoretical guarantees for model estimation error and exact recovery of the solution of the QAP. The effectiveness of our methods is demonstrated through numerical experiments and in an application to matching the statistics academic genealogy and the collaboration networks. By leveraging additional covariates, we achieve improved alignment accuracy. Our work highlights the power of integrating covariate information in the classical graph matching setup, offering a practical and improved framework for combining network data with wide-ranging applications.

</details>


### [23] [Advances in Ontology--Based Mining of Adverse Drug Reactions](https://arxiv.org/abs/2512.11452)
*Kenenisa Tadesse Dame,Pietro Belloni,Ugo Moretti,Fabio Scapini,Marco Tuccori,Alessandra R. Brazzale*

Main category: stat.ME

TL;DR: 该研究提出一种整合不良事件本体论与零膨胀负二项模型的新方法，用于改进药物不良反应检测，通过考虑相关不良事件的生物学相似性和处理零计数过多问题，在模拟和真实数据中均优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 上市后药物警戒对于发现上市前临床试验中遗漏的药物不良反应至关重要。传统模型在处理相关不良事件的生物学相似性和零计数过多问题时存在局限，需要更有效的方法来解耦不良事件关联。

Method: 1. 将不良事件本体论整合到零膨胀负二项模型中；2. 使用基于置换的最大统计量评估统计显著性，保留个体报告中的不良事件相关性；3. 探索用于卷积封闭族的数据细化技术，创建独立的训练和验证数据集；4. 与传统随机训练/测试分割和分层分割方法进行比较。

Result: 1. 基于本体论的模型在模拟和威尼托药物安全数据库的真实数据应用中，始终优于Gamma-Poisson收缩器等经典模型；2. 数据细化技术和分层分割产生非常相似的结果，分层分割略有优势；3. 两种方法在确保可靠和一致的模型评估方面明显优于随机分割。

Conclusion: 整合不良事件本体论的零膨胀负二项模型能更有效地检测药物不良反应，通过考虑生物学相似性和处理零计数问题提高检测性能。数据细化技术和分层分割方法为模型评估提供了更可靠的框架。

Abstract: Post--marketing pharmacovigilance is essential for identifying adverse drug reactions (ADRs) that elude detection during pre--marketing clinical trials. This study explores a novel approach that integrates an adverse event (AE) ontology into a zero--inflated negative binomial model to improve ADR detection. By accounting for the biological similarities among correlated AEs and addressing the excess of zero counts, this method more effectively disentangles AE associations. Statistical significance is evaluated using a permutation--based maximum statistic that preserves AE correlations within individual reports. Simulations and an application to real data from the Veneto drug safety database demonstrate that the ontology--based model consistently outperforms classical models such as the Gamma--Poisson Shrinker (GPS). For post--selection inference, we furthermore explore a data thinning technique for convolution--closed families, enabling the creation of independent training and validation datasets while retaining all drug--AE pairs. This approach is compared with conventional random train/test splitting, which may leave some drugs or AEs absent from one subset, and stratified splitting, which requires expanding aggregated counts into individual instances. The data--thinning technique and stratified splitting yield very similar results, with stratified splitting showing a slight benefit, and both clearly outperform random splitting in ensuring reliable and consistent model evaluation.

</details>


### [24] [Forest Kernel Balancing Weights: Outcome-Guided Features for Causal Inference](https://arxiv.org/abs/2512.11751)
*Andy A. Shen,Eli Ben-Michael,Avi Feller,Luke Keele,Jared Murray*

Main category: stat.ME

TL;DR: 提出森林核平衡方法，利用树模型（随机森林和BART）隐式估计的核函数进行协变量平衡，相比标准核方法在计算和统计性能上有显著改进


<details>
  <summary>Details</summary>
Motivation: 在观察性因果推断中，选择哪些特征进行平衡是一个挑战性问题。现有核平衡方法虽然能捕捉单元间的相似性，但未充分利用结果信息来学习重要的平衡特征

Method: 提出森林核平衡方法，利用树模型（随机森林和BART）隐式估计的核函数。这些模型通过观测值在同一终端叶节点的共现来估计核函数，该核函数虽然是基线特征的函数，但选择的非线性项和交互作用对预测结果很重要，因此对解决混杂偏倚很重要

Result: 通过模拟和应用实例表明，森林核平衡相比标准核方法（在学习特征时未纳入结果信息）在计算和统计性能上有显著改进

Conclusion: 森林核平衡方法通过利用树模型隐式估计的核函数，有效结合了结果信息来学习重要的平衡特征，为观察性因果推断中的特征选择问题提供了有前景的解决方案

Abstract: While balancing covariates between groups is central for observational causal inference, selecting which features to balance remains a challenging problem. Kernel balancing is a promising approach that first estimates a kernel that captures similarity across units and then balances a (possibly low-dimensional) summary of that kernel, indirectly learning important features to balance. In this paper, we propose forest kernel balancing, which leverages the underappreciated fact that tree-based machine learning models, namely random forests and Bayesian additive regression trees (BART), implicitly estimate a kernel based on the co-occurrence of observations in the same terminal leaf node. Thus, even though the resulting kernel is solely a function of baseline features, the selected nonlinearities and other interactions are important for predicting the outcome -- and therefore are important for addressing confounding. Through simulations and applied illustrations, we show that forest kernel balancing leads to meaningful computational and statistical improvement relative to standard kernel methods, which do not incorporate outcome information when learning features.

</details>


### [25] [Bounds for causal mediation effects](https://arxiv.org/abs/2512.11549)
*Marie S. Breum,Vanessa Didelez,Erin E. Gabriel,Michael C. Sachs*

Main category: stat.ME

TL;DR: 该论文研究了中介效应因果推断中的边界分析，比较了自然效应框架和可分离效应框架下的因果边界，并在花生过敏试验数据上进行了应用。


<details>
  <summary>Details</summary>
Motivation: 现有的因果中介分析框架通常需要点识别假设，但这些假设即使在随机化处理下也可能被违反。当因果效应无法点识别时，需要推导与观测数据一致的边界范围。

Method: 研究自然效应框架和可分离效应框架下的因果边界，特别关注中间变量存在未测量混杂因素时的情况。比较不同框架下的边界结果，并在花生过敏试验的免疫生物标志物数据上进行应用。

Result: 当中间变量存在未测量混杂因素时，可分离（间接）效应的尖锐符号边界与类似设置下自然（间接）效应的现有边界一致。比较了仅跨世界独立性假设不成立时自然直接效应的有效边界。

Conclusion: 该研究为因果中介分析提供了边界分析方法，特别是在识别假设可能被违反的情况下。通过实际数据应用展示了不同框架下边界结果的比较，为因果推断提供了更稳健的工具。

Abstract: Several frameworks have been proposed for studying causal mediation analysis. What these frameworks have in common is that they all make assumptions for point identifications that can be violated even when treatment is randomized. When a causal effect is not point-identified, one can sometimes derive bounds, i.e. a range of possible values that are consistent with the observed data. In this work, we study causal bounds for mediation effects under both the natural effects framework and the separable effects framework. In particular, we show that when there are unmeasured confounders for the intermediate variables(s) the sharp symbolic bounds on separable (in)direct effect coincide with existing bounds for natural (in)direct effects in the analogous setting. We compare these bounds to valid bounds for the natural direct effects when only the cross-world independence assumption does not hold. Furthermore, we demonstrate the use and compare the results of the bounds on data from a trial investigating the effect of peanut consumption on the development of peanut allergy in infants through specific pathways of measured immunological biomarkers.

</details>


### [26] [Detecting changes in the mean of spatial random fields on a regular grid](https://arxiv.org/abs/2512.11599)
*Sheila T. Görz,Roland Fried*

Main category: stat.ME

TL;DR: 提出用于检测规则网格上空间随机场均值变化的统计方法，包括基于基尼平均差和方差变体的两种检验统计量，适用于独立和依赖数据，在卫星图像中有效检测森林砍伐区域


<details>
  <summary>Details</summary>
Motivation: 需要开发能够检测空间过程均值变化的通用框架，特别是在卫星图像等空间数据中识别变化区域（如森林砍伐）

Method: 扩展时间序列的块方法到空间领域，提出两种检验统计量：基于基尼平均差和新的方差变体；推导方差统计量的渐近正态性；开发基于估计自协方差的去相关算法处理空间依赖性

Result: 蒙特卡洛模拟显示两种检验在独立和依赖数据中都保持适当尺寸和功效；方差检验对几乎所有非常数均值函数具有一致性；在卫星图像应用中，方差检验能可靠检测森林砍伐区域

Conclusion: 提出的空间变化检测框架有效，特别是方差检验在理论和应用中都表现良好，为空间过程均值变化检测提供了通用方法

Abstract: We propose statistical procedures for detecting changes in the mean of spatial random fields observed on regular grids. The proposed framework provides a general approach to change detection in spatial processes. Extending a block-based method originally developed for time series, we introduce two test statistics, one based on Gini's mean difference and a novel variance-based variant. Under mild moment conditions, we derive asymptotic normality of the variance-based statistic and prove its consistency against almost all non-constant mean functions (in a sense of positive Lebesgue measure). To accommodate spatial dependence, we further develop a de-correlation algorithm based on estimated autocovariances. Monte Carlo simulations demonstrate that both tests maintain appropriate size and power for both independent and dependent data. In an application to satellite images, especially our variance-based test reliably detects regions undergoing deforestation.

</details>


### [27] [Spatially Varying Gene Regulatory Networks via Bayesian Nonparametric Covariate-Dependent Directed Cyclic Graphical Models](https://arxiv.org/abs/2512.11732)
*Trisha Dawn,Yang Ni*

Main category: stat.ME

TL;DR: BNP-DCGx：一种贝叶斯非参数方法，通过协变量依赖的有向循环图模型学习空间变化的基因调控网络，解决了现有方法无法捕捉反馈回路和空间连续变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图模型主要关注无向图或有向无环图，限制了捕捉基因调控中普遍存在的反馈回路的能力。同时，在允许图结构随空间协变量连续变化的同时，确保循环图的稳定性条件存在显著的统计和计算挑战。

Method: 提出BNP-DCGx方法，引入协变量依赖的随机分区作为分层模型的中间层，将协变量空间离散化为具有特定稳定有向循环图的聚类。通过分区平均，获得空间上平滑变化的图结构，同时保持理论上的稳定性保证。开发了高效的并行回火马尔可夫链蒙特卡洛算法进行后验推断。

Result: 模拟实验表明该方法能准确恢复分段常数和连续变化的图结构。应用于人类背外侧前额叶皮层的空间转录组数据，揭示了具有反馈回路的空间变化调控网络，基于不同的调控机制在已建立的细胞类型中识别出潜在的细胞亚型，并为脑组织中基因调控的空间组织提供了新见解。

Conclusion: BNP-DCGx方法能够有效学习空间变化的基因调控网络，捕捉反馈回路，同时处理图结构的空间连续变化和稳定性要求，为空间转录组数据分析提供了强大的工具。

Abstract: Spatial transcriptomics technologies enable the measurement of gene expression with spatial context, providing opportunities to understand how gene regulatory networks vary across tissue regions. However, existing graphical models focus primarily on undirected graphs or directed acyclic graphs, limiting their ability to capture feedback loops that are prevalent in gene regulation. Moreover, ensuring the so-called stability condition of cyclic graphs, while allowing graph structures to vary continuously with spatial covariates, presents significant statistical and computational challenges. We propose BNP-DCGx, a Bayesian nonparametric approach for learning spatially varying gene regulatory networks via covariate-dependent directed cyclic graphical models. Our method introduces a covariate-dependent random partition as an intermediary layer in a hierarchical model, which discretizes the covariate space into clusters with cluster-specific stable directed cyclic graphs. Through partition averaging, we obtain smoothly varying graph structures over space while maintaining theoretical guarantees of stability. We develop an efficient parallel tempered Markov chain Monte Carlo algorithm for posterior inference and demonstrate through simulations that our method accurately recovers both piecewise constant and continuously varying graph structures. Application to spatial transcriptomics data from human dorsolateral prefrontal cortex reveals spatially varying regulatory networks with feedback loops, identifies potential cell subtypes within established cell types based on distinct regulatory mechanisms, and provides new insights into spatial organization of gene regulation in brain tissue.

</details>


### [28] [A Doubled Adjacency Spectral Embedding Approach to Graph Clustering](https://arxiv.org/abs/2512.11777)
*Sinyoung Park,Matthew Nunes,Sandipan Roy*

Main category: stat.ME

TL;DR: 提出DASE方法改进稀疏网络中的谱聚类性能，特别针对核心-外围结构


<details>
  <summary>Details</summary>
Motivation: 传统谱聚类在核心-外围网络结构中表现不佳，ASE方法虽然改进但仅限于密集网络，而实际网络数据往往是稀疏的

Method: 提出Doubled Adjacency Spectral Embedding (DASE)方法，利用邻接矩阵的平方来更有效地利用稀疏结构中的连接信息进行聚类

Result: 理论证明DASE在确定稀疏社区结构时具有良好的性质，实验表明在稀疏和密集网络中都改进了聚类性能，特别适用于核心-外围结构

Conclusion: DASE方法有效解决了稀疏网络中的谱聚类问题，在真实世界的就业和交通数据集上展示了实用价值

Abstract: Spectral clustering is a popular tool in network data analysis, with applications in a variety of scientific application areas. However, many studies have shown that spectral clustering does not perform well on certain network structures, particularly core-periphery networks. To improve clustering performance in core-periphery structures, Adjacency Spectral Embedding (ASE) has been introduced, which performs clustering via a network's adjacency matrix instead of the graph Laplacian. Despite its advantages in this setting, the optimal performance of ASE is limited to dense networks, whilst network data observed in practice is often sparse in nature. To address this limitation, we propose a new approach which we term Doubled Adjacency Spectral Embedding (DASE), motivated by the observation that the squared adjacency matrix will leverage the fewer connections in sparse structures more efficiently in clustering applications. Theoretical results establish that DASE enjoys good consistency properties when determining sparse community structure. The performance and general applicability of the proposed method is evaluated using extensive simulations on both directed and undirected networks. Our results highlight the improved clustering performance on both sparse and dense networks in the presence of core-periphery structures. We illustrate our proposed technique on real-world employment and transportation datasets.

</details>
