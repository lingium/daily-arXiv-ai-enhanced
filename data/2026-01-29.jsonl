{"id": "2601.19004", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.19004", "abs": "https://arxiv.org/abs/2601.19004", "authors": ["Xinyu Zhang", "Rachael Muscatello", "Megan Jones", "Blythe Corbett", "Simon Vandekar"], "title": "Asymptotic Distribution of Robust Effect Size Index", "comment": null, "summary": "The Robust Effect Size Index (RESI) is a recently proposed standardized effect size to quantify association strength across models. However, its confidence interval construction has relied on computationally intensive bootstrap procedures. We establish a general theorem for the asymptotic distribution of the RESI using a Taylor expansion that accommodates a broad class of models. Simulations under various linear and logistic regression settings show that RESI and its CI have smaller bias and more reliable coverage than commonly used effect sizes such as Cohen's d and f. Combining with robust covariance estimation yields valid inference under model misspecification. We use the methods to investigate associations of depression and behavioral problems with sex and diagnosis in Autism spectrum disorders, and demonstrate that the asymptotic approach achieves up to a 50-fold speedup over the bootstrap. Our work provides a scalable and reliable alternative to bootstrap inference, greatly enhancing the applicability of RESI to high-dimensional studies."}
{"id": "2601.19666", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.19666", "abs": "https://arxiv.org/abs/2601.19666", "authors": ["Josh Givens", "Song Liu", "Henry W J Reeve", "Katarzyna Reluga"], "title": "Direct Doubly Robust Estimation of Conditional Quantile Contrasts", "comment": "To be published as a conference paper at ICLR 2026", "summary": "Within heterogeneous treatment effect (HTE) analysis, various estimands have been proposed to capture the effect of a treatment conditional on covariates. Recently, the conditional quantile comparator (CQC) has emerged as a promising estimand, offering quantile-level summaries akin to the conditional quantile treatment effect (CQTE) while preserving some interpretability of the conditional average treatment effect (CATE). It achieves this by summarising the treated response conditional on both the covariates and the untreated response. Despite these desirable properties, the CQC's current estimation is limited by the need to first estimate the difference in conditional cumulative distribution functions and then invert it. This inversion obscures the CQC estimate, hampering our ability to both model and interpret it. To address this, we propose the first direct estimator of the CQC, allowing for explicit modelling and parameterisation. This explicit parameterisation enables better interpretation of our estimate while also providing a means to constrain and inform the model. We show, both theoretically and empirically, that our estimation error depends directly on the complexity of the CQC itself, improving upon the existing estimation procedure. Furthermore, it retains the desirable double robustness property with respect to nuisance parameter estimation. We further show our method to outperform existing procedures in estimation accuracy across multiple data scenarios while varying sample size and nuisance error. Finally, we apply it to real-world data from an employment scheme, uncovering a reduced range of potential earnings improvement as participant age increases."}
{"id": "2601.19710", "categories": ["stat.CO", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19710", "abs": "https://arxiv.org/abs/2601.19710", "authors": ["Sebastiano Grazzi", "Samuel Livingstone", "Lionel Riou-Durand"], "title": "On randomized step sizes in Metropolis-Hastings algorithms", "comment": null, "summary": "The performance of Metropolis-Hastings algorithms is highly sensitive to the choice of step size, and miss-specification can lead to severe loss of efficiency. We study algorithms with randomized step sizes, considering both auxiliary-variable and marginalized constructions. We show that algorithms with a randomized step size inherit weak Poincaré inequalities/spectral gaps from their fixed-step-size counterparts under minimal conditions, and that the marginalized kernel should always be preferred in terms of asymptotic variance to the auxiliary-variable choice if it is implementable. In addition we show that both types of randomization make an algorithm robust to tuning, meaning that spectral gaps decay polynomially as the step size is increasingly poorly chosen. We further show that step-size randomization often preserves high-dimensional scaling limits and algorithmic complexity, while increasing the optimal acceptance rate for Langevin and Hamiltonian samplers when an Exponential or Uniform distribution is chosen to randomize the step size. Theoretical results are complemented with a numerical study on challenging benchmarks such as Poisson regression, Neal's funnel and the Rosenbrock (banana) distribution."}
{"id": "2601.19722", "categories": ["stat.CO", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19722", "abs": "https://arxiv.org/abs/2601.19722", "authors": ["Francesco Pozza", "Giacomo Zanella"], "title": "Zeroth-order parallel sampling", "comment": "32 pages, 5 figures", "summary": "Finding effective ways to exploit parallel computing to accelerate Markov chain Monte Carlo methods is an important problem in Bayesian computation and related disciplines. In this paper, we consider the zeroth-order setting where the unnormalized target distribution can be evaluated but its gradient is unavailable for theoretical, practical, or computational reasons. We also assume access to $m$ parallel processors to accelerate convergence. The proposed approach is inspired by modern zeroth-order optimization methods, which mimic gradient-based schemes by replacing the gradient with a zeroth-order stochastic gradient estimator. Our contribution is twofold. First, we show that a naive application of popular zeroth-order stochastic gradient estimators within Markov chain Monte Carlo methods leads to algorithms with poor dependence on $m$, both for unadjusted and Metropolis-adjusted schemes. We then propose a simple remedy to this problem, based on a random-slice perspective, as opposed to a stochastic gradient one, obtaining a new class of zeroth-order samplers that provably achieve a polynomial speed-up in $m$. Theoretical findings are supported by numerical studies."}
{"id": "2601.19710", "categories": ["stat.CO", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19710", "abs": "https://arxiv.org/abs/2601.19710", "authors": ["Sebastiano Grazzi", "Samuel Livingstone", "Lionel Riou-Durand"], "title": "On randomized step sizes in Metropolis-Hastings algorithms", "comment": null, "summary": "The performance of Metropolis-Hastings algorithms is highly sensitive to the choice of step size, and miss-specification can lead to severe loss of efficiency. We study algorithms with randomized step sizes, considering both auxiliary-variable and marginalized constructions. We show that algorithms with a randomized step size inherit weak Poincaré inequalities/spectral gaps from their fixed-step-size counterparts under minimal conditions, and that the marginalized kernel should always be preferred in terms of asymptotic variance to the auxiliary-variable choice if it is implementable. In addition we show that both types of randomization make an algorithm robust to tuning, meaning that spectral gaps decay polynomially as the step size is increasingly poorly chosen. We further show that step-size randomization often preserves high-dimensional scaling limits and algorithmic complexity, while increasing the optimal acceptance rate for Langevin and Hamiltonian samplers when an Exponential or Uniform distribution is chosen to randomize the step size. Theoretical results are complemented with a numerical study on challenging benchmarks such as Poisson regression, Neal's funnel and the Rosenbrock (banana) distribution."}
{"id": "2601.18857", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18857", "abs": "https://arxiv.org/abs/2601.18857", "authors": ["Haimo Fang", "Kevin Tan", "Jonathan Pipping", "Giles Hooker"], "title": "Statistical Inference for Explainable Boosting Machines", "comment": "Accepted to AISTATS 2026 (poster)", "summary": "Explainable boosting machines (EBMs) are popular \"glass-box\" models that learn a set of univariate functions using boosting trees. These achieve explainability through visualizations of each feature's effect. However, unlike linear model coefficients, uncertainty quantification for the learned univariate functions requires computationally intensive bootstrapping, making it hard to know which features truly matter. We provide an alternative using recent advances in statistical inference for gradient boosting, deriving methods for statistical inference as well as end-to-end theoretical guarantees. Using a moving average instead of a sum of trees (Boulevard regularization) allows the boosting process to converge to a feature-wise kernel ridge regression. This produces asymptotically normal predictions that achieve the minimax-optimal mean squared error for fitting Lipschitz GAMs with $p$ features at rate $O(pn^{-2/3})$, successfully avoiding the curse of dimensionality. We then construct prediction intervals for the response and confidence intervals for each learned univariate function with a runtime independent of the number of datapoints, enabling further explainability within EBMs."}
{"id": "2601.18992", "categories": ["stat.ME", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.18992", "abs": "https://arxiv.org/abs/2601.18992", "authors": ["Ilja Klebanov", "Claudia Schillings", "Dana Wrischnig"], "title": "Mixture-Weighted Ensemble Kalman Filter with Quasi-Monte Carlo Transport", "comment": null, "summary": "The Bootstrap Particle Filter (BPF) and the Ensemble Kalman Filter (EnKF) are two widely used methods for sequential Bayesian filtering: the BPF is asymptotically exact but can suffer from weight degeneracy, while the EnKF scales well in high dimension yet is exact only in the linear-Gaussian case. We combine these approaches by retaining the EnKF transport step and adding a principled importance-sampling correction. Our first contribution is a general importance-sampling theory for mixture targets and proposals, including variance comparisons between individual- and mixture-based estimators. We then interpret the stochastic EnKF analysis as sampling from explicit Gaussian-mixture proposals obtained by conditioning on the current or previous ensemble, which leads to six self-normalized IS-EnKF schemes. We embed these updates into a broader class of ensemble-based filters and prove consistency and error bounds, including weight-variance comparisons and sufficient conditions ensuring finite-variance importance weights. As a second contribution, we construct transported quasi-Monte Carlo (TQMC) point sets for the Gaussian-mixture laws arising in prediction and analysis, yielding TQMC-enhanced variants that can substantially reduce sampling error without changing the filtering pipeline. Numerical experiments on benchmark models compare the proposed mixture-weighted and TQMC-enhanced filters, showing improved filtering accuracy relative to BPF, EnKF, and the standard weighted EnKF, and that the weighted schemes eliminate the EnKF error plateau often caused by analysis-target mismatch."}
{"id": "2601.18889", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.18889", "abs": "https://arxiv.org/abs/2601.18889", "authors": ["R Noah Padgett"], "title": "A penalized heteroskedastic ordered probit model for DIF (measurement invariance) testing of single-item assessments in cross-cultural research", "comment": "10 pages, 3 figures", "summary": "Differential item functioning (DIF) or measurement invariance (MI) testing for single-item assessments has previously been impossible. Part of the issue is that there are no conditioning variables to serve as a proxy for the latent variable--regression-based DIF methods. Another reason is that factor-analytic approaches require multiple items to estimate parameters. In this technical working paper, I propose an approach for evaluating DIF/MI in a single-item assessment of a construct. The current methods should NOT replace using multiple-indicator MG-CFA/IRT analyses of DIF/MI or regression mased methods when possible. More items generally provide significantly better construct coverage and provide more rigorous DIF/MI evaluation."}
{"id": "2601.19811", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19811", "abs": "https://arxiv.org/abs/2601.19811", "authors": ["TrungKhang Tran", "TrungTin Nguyen", "Gersende Fort", "Tung Doan", "Hien Duy Nguyen", "Binh T. Nguyen", "Florence Forbes", "Christopher Drovandi"], "title": "Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts", "comment": "TrungKhang Tran and TrungTin Nguyen are co-first authors", "summary": "Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance."}
{"id": "2601.19722", "categories": ["stat.CO", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19722", "abs": "https://arxiv.org/abs/2601.19722", "authors": ["Francesco Pozza", "Giacomo Zanella"], "title": "Zeroth-order parallel sampling", "comment": "32 pages, 5 figures", "summary": "Finding effective ways to exploit parallel computing to accelerate Markov chain Monte Carlo methods is an important problem in Bayesian computation and related disciplines. In this paper, we consider the zeroth-order setting where the unnormalized target distribution can be evaluated but its gradient is unavailable for theoretical, practical, or computational reasons. We also assume access to $m$ parallel processors to accelerate convergence. The proposed approach is inspired by modern zeroth-order optimization methods, which mimic gradient-based schemes by replacing the gradient with a zeroth-order stochastic gradient estimator. Our contribution is twofold. First, we show that a naive application of popular zeroth-order stochastic gradient estimators within Markov chain Monte Carlo methods leads to algorithms with poor dependence on $m$, both for unadjusted and Metropolis-adjusted schemes. We then propose a simple remedy to this problem, based on a random-slice perspective, as opposed to a stochastic gradient one, obtaining a new class of zeroth-order samplers that provably achieve a polynomial speed-up in $m$. Theoretical findings are supported by numerical studies."}
{"id": "2601.18907", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18907", "abs": "https://arxiv.org/abs/2601.18907", "authors": ["Hwanwoo Kim", "Eric Laber"], "title": "Implicit Q-Learning and SARSA: Liberating Policy Control from Step-Size Calibration", "comment": null, "summary": "Q-learning and SARSA are foundational reinforcement learning algorithms whose practical success depends critically on step-size calibration. Step-sizes that are too large can cause numerical instability, while step-sizes that are too small can lead to slow progress. We propose implicit variants of Q-learning and SARSA that reformulate their iterative updates as fixed-point equations. This yields an adaptive step-size adjustment that scales inversely with feature norms, providing automatic regularization without manual tuning. Our non-asymptotic analyses demonstrate that implicit methods maintain stability over significantly broader step-size ranges. Under favorable conditions, it permits arbitrarily large step-sizes while achieving comparable convergence rates. Empirical validation across benchmark environments spanning discrete and continuous state spaces shows that implicit Q-learning and SARSA exhibit substantially reduced sensitivity to step-size selection, achieving stable performance with step-sizes that would cause standard methods to fail."}
{"id": "2601.19004", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.19004", "abs": "https://arxiv.org/abs/2601.19004", "authors": ["Xinyu Zhang", "Rachael Muscatello", "Megan Jones", "Blythe Corbett", "Simon Vandekar"], "title": "Asymptotic Distribution of Robust Effect Size Index", "comment": null, "summary": "The Robust Effect Size Index (RESI) is a recently proposed standardized effect size to quantify association strength across models. However, its confidence interval construction has relied on computationally intensive bootstrap procedures. We establish a general theorem for the asymptotic distribution of the RESI using a Taylor expansion that accommodates a broad class of models. Simulations under various linear and logistic regression settings show that RESI and its CI have smaller bias and more reliable coverage than commonly used effect sizes such as Cohen's d and f. Combining with robust covariance estimation yields valid inference under model misspecification. We use the methods to investigate associations of depression and behavioral problems with sex and diagnosis in Autism spectrum disorders, and demonstrate that the asymptotic approach achieves up to a 50-fold speedup over the bootstrap. Our work provides a scalable and reliable alternative to bootstrap inference, greatly enhancing the applicability of RESI to high-dimensional studies."}
{"id": "2601.19277", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.19277", "abs": "https://arxiv.org/abs/2601.19277", "authors": ["Gabriela Bayolo Soler", "Miraine Dávila Felipe", "Ghislaine Gayraud"], "title": "Embedding Birth-Death Processes within a Dynamic Stochastic Block Model", "comment": null, "summary": "Statistical clustering in dynamic networks aims to identify groups of nodes with similar or distinct internal connectivity patterns as the network evolves over time. While early research primarily focused on static Stochastic Block Models (SBMs), recent advancements have extended these models to handle dynamic and weighted networks, allowing for a more accurate representation of temporal variations in structure. Additional developments have introduced methods for detecting structural changes, such as shifts in community membership. However, limited attention has been paid to dynamic networks with variable population sizes, where nodes may enter or exit the network. To address this gap, we propose an extension of dynamic SBMs (dSBMs) that incorporates a birth-death process, enabling the statistical clustering of nodes in dynamic networks with evolving population sizes. This work makes three main contributions: (1) the introduction of a novel model for dSBMs with birth-death processes, (2) a framework for parameter inference and prediction of latent communities in this model, and (3) the development of an adapted Variational Expectation-Maximization (VEM) algorithm for efficient inference within this extended framework."}
{"id": "2601.19553", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.19553", "abs": "https://arxiv.org/abs/2601.19553", "authors": ["Johan Hallberg Szabadváry"], "title": "A Fast, Closed-Form Bandwidth Selector for the Beta Kernel Density Estimator", "comment": "30 pages, 6 figures", "summary": "The Beta kernel estimator offers a theoretically superior alternative to the Gaussian kernel for unit interval data, eliminating boundary bias without requiring reflection or transformation. However, its adoption remains limited by the lack of a reliable bandwidth selector; practitioners currently rely on iterative optimization methods that are computationally expensive and prone to instability. We derive the ``\\rot,'' a fast, closed-form bandwidth selector based on the unweighted Asymptotic Mean Integrated Squared Error (AMISE) of a beta reference distribution. To address boundary integrability issues, we introduce a principled heuristic for U-shaped and J-shaped distributions. By employing a method-of-moments approximation, we reduce the bandwidth selection complexity from iterative optimization to $\\mathcal{O}(1)$. Extensive Monte Carlo simulations demonstrate that our rule matches the accuracy of numerical optimization while delivering a speedup of over 35,000 times. Real-world validation on socioeconomic data shows that it avoids the ``vanishing boundary'' and ``shoulder'' artifacts common to Gaussian-based methods. We provide a comprehensive, open-source Python package to facilitate the immediate adoption of the Beta kernel as a drop-in replacement for standard density estimation tools."}
{"id": "2601.18950", "categories": ["stat.ML", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18950", "abs": "https://arxiv.org/abs/2601.18950", "authors": ["Harsh Vardhan", "Arya Mazumdar"], "title": "Collaborative Compressors in Distributed Mean Estimation with Limited Communication Budget", "comment": null, "summary": "Distributed high dimensional mean estimation is a common aggregation routine used often in distributed optimization methods. Most of these applications call for a communication-constrained setting where vectors, whose mean is to be estimated, have to be compressed before sharing. One could independently encode and decode these to achieve compression, but that overlooks the fact that these vectors are often close to each other. To exploit these similarities, recently Suresh et al., 2022, Jhunjhunwala et al., 2021, Jiang et al, 2023, proposed multiple correlation-aware compression schemes. However, in most cases, the correlations have to be known for these schemes to work. Moreover, a theoretical analysis of graceful degradation of these correlation-aware compression schemes with increasing dissimilarity is limited to only the $\\ell_2$-error in the literature. In this paper, we propose four different collaborative compression schemes that agnostically exploit the similarities among vectors in a distributed setting. Our schemes are all simple to implement and computationally efficient, while resulting in big savings in communication. The analysis of our proposed schemes show how the $\\ell_2$, $\\ell_\\infty$ and cosine estimation error varies with the degree of similarity among vectors."}
{"id": "2601.19044", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.19044", "abs": "https://arxiv.org/abs/2601.19044", "authors": ["Debjoy Thakur", "Lingyuan Zhao", "Soutir Bandyopadhyay"], "title": "Local Variable and Neighborhood Selection for Firearm Fatality in the Southeast USA", "comment": null, "summary": "A major public health concern in the United States (US) is gun-related deaths. The number of gun injuries largely varies spatially because of county-wise heterogeneity of race, sex, age, and income distributions. But still, a major challenge is to locally identify the influential socio-economic factors behind these firearm fatality incidents. For a diverging number of predictors, a rich literature exists regarding SCAD under the independence framework; however, a vacuum remains when discussing local variable selection for spatially correlated, over-dispersed data. This research presents a two-step localized variable selection and inference framework for spatially indexed gunshot fatality data. In the first step, we select variables locally using the SCAD penalty for specific locations where the number of gunshot incidents exceeds a threshold. For these locations, after selecting the predictors, we proceed to the next step, which involves examining the directional variation in the latent spatial neighborhood structure. We further discuss the theoretical properties of this county-specific local variable selection under infill asymptotics. This method has threefold advantages: (i) this method selects the variables locally, (ii) this method provides inference about directional variation of a selected predictor, and (iii) instead of assuming the spatial neighborhood structure in an ad hoc manner, this method identifies the specific type of spatial neighborhood structure that is most appropriate for modeling the random effects."}
{"id": "2601.19688", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.19688", "abs": "https://arxiv.org/abs/2601.19688", "authors": ["Ping Zhao", "Huifang Ma"], "title": "Adaptive L-tests for high dimensional independence", "comment": null, "summary": "Testing mutual independence among multiple random variables is a fundamental problem in statistics, with wide applications in genomics, finance, and neuroscience. In this paper, we propose a new class of tests for high-dimensional mutual independence based on $L$-statistics. We establish the asymptotic distribution of the proposed test when the order parameter $k$ is fixed, and prove asymptotic normality when $k$ diverges with the dimension. Moreover, we show the asymptotic independence of the fixed-$k$ and diverging-$k$ statistics, enabling their combination through the Cauchy method. The resulting adaptive test is both theoretically justified and practically powerful across a wide range of alternatives. Simulation studies demonstrate the advantages of our method."}
{"id": "2601.19156", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.19156", "abs": "https://arxiv.org/abs/2601.19156", "authors": ["Gyu Yeol Kim", "Min-hwan Oh"], "title": "Convergence of Muon with Newton-Schulz", "comment": "Accepted at ICLR 2026", "summary": "We analyze Muon as originally proposed and used in practice -- using the momentum orthogonalization with a few Newton-Schulz steps. The prior theoretical results replace this key step in Muon with an exact SVD-based polar factor. We prove that Muon with Newton-Schulz converges to a stationary point at the same rate as the SVD-polar idealization, up to a constant factor for a given number $q$ of Newton-Schulz steps. We further analyze this constant factor and prove that it converges to 1 doubly exponentially in $q$ and improves with the degree of the polynomial used in Newton-Schulz for approximating the orthogonalization direction. We also prove that Muon removes the typical square-root-of-rank loss compared to its vector-based counterpart, SGD with momentum. Our results explain why Muon with a few low-degree Newton-Schulz steps matches exact-polar (SVD) behavior at a much faster wall-clock time and explain how much momentum matrix orthogonalization via Newton-Schulz benefits over the vector-based optimizer. Overall, our theory justifies the practical Newton-Schulz design of Muon, narrowing its practice-theory gap."}
{"id": "2601.19049", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19049", "abs": "https://arxiv.org/abs/2601.19049", "authors": ["Pavel Krupskii"], "title": "A class of skew-multivariate distributions for spatial data", "comment": "35 pages, 6 figures, and 3 tables", "summary": "This paper introduces a class of copula models for spatial data, based on multivariate Pareto-mixture distributions. We explore the tail properties of these models, demonstrating their ability to capture both tail dependence and asymptotic independence, as well as the tail asymmetry frequently observed in real-world data. The proposed models also offer flexibility in accounting for permutation asymmetry and can effectively represent both the bulk and extreme tails of the distribution. We consider special cases of these models with computationally tractable likelihoods and present an extensive simulation study to assess the finite-sample performance of the maximum likelihood estimators. Finally, we apply our models to analyze a temperature dataset, showcasing their practical utility."}
{"id": "2601.19044", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.19044", "abs": "https://arxiv.org/abs/2601.19044", "authors": ["Debjoy Thakur", "Lingyuan Zhao", "Soutir Bandyopadhyay"], "title": "Local Variable and Neighborhood Selection for Firearm Fatality in the Southeast USA", "comment": null, "summary": "A major public health concern in the United States (US) is gun-related deaths. The number of gun injuries largely varies spatially because of county-wise heterogeneity of race, sex, age, and income distributions. But still, a major challenge is to locally identify the influential socio-economic factors behind these firearm fatality incidents. For a diverging number of predictors, a rich literature exists regarding SCAD under the independence framework; however, a vacuum remains when discussing local variable selection for spatially correlated, over-dispersed data. This research presents a two-step localized variable selection and inference framework for spatially indexed gunshot fatality data. In the first step, we select variables locally using the SCAD penalty for specific locations where the number of gunshot incidents exceeds a threshold. For these locations, after selecting the predictors, we proceed to the next step, which involves examining the directional variation in the latent spatial neighborhood structure. We further discuss the theoretical properties of this county-specific local variable selection under infill asymptotics. This method has threefold advantages: (i) this method selects the variables locally, (ii) this method provides inference about directional variation of a selected predictor, and (iii) instead of assuming the spatial neighborhood structure in an ad hoc manner, this method identifies the specific type of spatial neighborhood structure that is most appropriate for modeling the random effects."}
{"id": "2601.19186", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19186", "abs": "https://arxiv.org/abs/2601.19186", "authors": ["Zeyu Bian", "Lan Wang", "Chengchun Shi", "Zhengling Qi"], "title": "Double Fairness Policy Learning: Integrating Action Fairness and Outcome Fairness in Decision-making", "comment": null, "summary": "Fairness is a central pillar of trustworthy machine learning, especially in domains where accuracy- or profit-driven optimization is insufficient. While most fairness research focuses on supervised learning, fairness in policy learning remains less explored. Because policy learning is interventional, it induces two distinct fairness targets: action fairness (equitable action assignments) and outcome fairness (equitable downstream consequences). Crucially, equalizing actions does not generally equalize outcomes when groups face different constraints or respond differently to the same action. We propose a novel double fairness learning (DFL) framework that explicitly manages the trade-off among three objectives: action fairness, outcome fairness, and value maximization. We integrate fairness directly into a multi-objective optimization problem for policy learning and employ a lexicographic weighted Tchebyshev method that recovers Pareto solutions beyond convex settings, with theoretical guarantees on the regret bounds. Our framework is flexible and accommodates various commonly used fairness notions. Extensive simulations demonstrate improved performance relative to competing methods. In applications to a motor third-party liability insurance dataset and an entrepreneurship training dataset, DFL substantially improves both action and outcome fairness while incurring only a modest reduction in overall value."}
{"id": "2601.19167", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19167", "abs": "https://arxiv.org/abs/2601.19167", "authors": ["Rayleigh Lei", "Abel Rodriguez"], "title": "Modeling Ordinal Survey Data with Unfolding Models", "comment": null, "summary": "Surveys that rely on ordinal polychotomous (Likert-like) items are widely employed to capture individual preferences because they allow respondents to express both the direction and strength of their preferences. Latent factor models traditionally used in this context implicitly assume that the response functions (the cumulative distribution of the ordinal outcome) are monotonic on the latent trait. This assumption can be too restrictive in several application areas, including in political science and marketing. In this work, we propose a novel ordinal probit unfolding model that can accommodate both monotonic and non-monotonic response functions. The advantages of the model are illustrated by analyzing an immigration attitude survey conducted in the United States."}
{"id": "2601.19729", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.19729", "abs": "https://arxiv.org/abs/2601.19729", "authors": ["Aldo Gardini", "Lorenzo Mori"], "title": "Coarsened data in small area estimation: a Bayesian two-part model for mapping smoking behaviour", "comment": "25 pages, 7 Figures, 2 Tables", "summary": "Estimating health indicators for restricted sub-populations is a recurring challenge in epidemiology and public health. When survey data are used, Small Area Estimation (SAE) methods can improve precision by borrowing strength across domains. In many applications, however, outcomes are self-reported and affected by coarsening mechanisms, such as rounding and digit preference, that reduce data resolution and may bias inference. This paper addresses both issues by developing a Bayesian unit-level SAE framework for semi-continuous, coarsened responses. Motivated by the 2019 Italian European Health Interview Survey, we estimate smoking indicators for domains defined by the cross-classification of Italian regions and age groups, capturing both smoking prevalence and intensity. The model adopts a two-part structure: a logistic component for smoking prevalence and a flexible mixture of Lognormal distributions for average cigarette consumption, coupled with an explicit model for coarsening and topcoding. Simulation studies show that ignoring coarsening can yield biased and unstable domain estimates with poor interval coverage, whereas the proposed model improves accuracy and achieves near-nominal coverage. The empirical application provides a detailed picture of smoking patterns across region-age domains, helping to characterize the dynamics of the phenomenon and inform targeted public health policies."}
{"id": "2601.19755", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19755", "abs": "https://arxiv.org/abs/2601.19755", "authors": ["Mónica Ribero", "Antonin Schrab", "Arthur Gretton"], "title": "Regularized $f$-Divergence Kernel Tests", "comment": null, "summary": "We propose a framework to construct practical kernel-based two-sample tests from the family of $f$-divergences. The test statistic is computed from the witness function of a regularized variational representation of the divergence, which we estimate using kernel methods. The proposed test is adaptive over hyperparameters such as the kernel bandwidth and the regularization parameter. We provide theoretical guarantees for statistical test power across our family of $f$-divergence estimates. While our test covers a variety of $f$-divergences, we bring particular focus to the Hockey-Stick divergence, motivated by its applications to differential privacy auditing and machine unlearning evaluation. For two-sample testing, experiments demonstrate that different $f$-divergences are sensitive to different localized differences, illustrating the importance of leveraging diverse statistics. For machine unlearning, we propose a relative test that distinguishes true unlearning failures from safe distributional variations."}
{"id": "2601.19553", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.19553", "abs": "https://arxiv.org/abs/2601.19553", "authors": ["Johan Hallberg Szabadváry"], "title": "A Fast, Closed-Form Bandwidth Selector for the Beta Kernel Density Estimator", "comment": "30 pages, 6 figures", "summary": "The Beta kernel estimator offers a theoretically superior alternative to the Gaussian kernel for unit interval data, eliminating boundary bias without requiring reflection or transformation. However, its adoption remains limited by the lack of a reliable bandwidth selector; practitioners currently rely on iterative optimization methods that are computationally expensive and prone to instability. We derive the ``\\rot,'' a fast, closed-form bandwidth selector based on the unweighted Asymptotic Mean Integrated Squared Error (AMISE) of a beta reference distribution. To address boundary integrability issues, we introduce a principled heuristic for U-shaped and J-shaped distributions. By employing a method-of-moments approximation, we reduce the bandwidth selection complexity from iterative optimization to $\\mathcal{O}(1)$. Extensive Monte Carlo simulations demonstrate that our rule matches the accuracy of numerical optimization while delivering a speedup of over 35,000 times. Real-world validation on socioeconomic data shows that it avoids the ``vanishing boundary'' and ``shoulder'' artifacts common to Gaussian-based methods. We provide a comprehensive, open-source Python package to facilitate the immediate adoption of the Beta kernel as a drop-in replacement for standard density estimation tools."}
{"id": "2601.19836", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.19836", "abs": "https://arxiv.org/abs/2601.19836", "authors": ["Augustine Wigle", "Erica E. M. Moodie"], "title": "Personalized Treatment Hierarchies in Bayesian Network Meta-Analysis", "comment": null, "summary": "Network Meta-Analysis (NMA) is an increasingly popular evidence synthesis tool that can provide a ranking of competing treatments, also known as a treatment hierarchy. Treatment-Covariate Interactions (TCIs) can be included in NMA models to allow relative treatment effects to vary with covariate values. We show that in an NMA model that includes TCIs, treatment hierarchies should be created with a particular covariate profile in mind. We outline the typical approach for creating a treatment hierarchy in standard Bayesian NMA and show how a treatment hierarchy for a particular covariate profile can be created from an NMA model that estimates TCIs. We demonstrate our methods using a real network of studies for treatments of major depressive disorder."}
{"id": "2601.19811", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19811", "abs": "https://arxiv.org/abs/2601.19811", "authors": ["TrungKhang Tran", "TrungTin Nguyen", "Gersende Fort", "Tung Doan", "Hien Duy Nguyen", "Binh T. Nguyen", "Florence Forbes", "Christopher Drovandi"], "title": "Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts", "comment": "TrungKhang Tran and TrungTin Nguyen are co-first authors", "summary": "Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance."}
{"id": "2601.19666", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.19666", "abs": "https://arxiv.org/abs/2601.19666", "authors": ["Josh Givens", "Song Liu", "Henry W J Reeve", "Katarzyna Reluga"], "title": "Direct Doubly Robust Estimation of Conditional Quantile Contrasts", "comment": "To be published as a conference paper at ICLR 2026", "summary": "Within heterogeneous treatment effect (HTE) analysis, various estimands have been proposed to capture the effect of a treatment conditional on covariates. Recently, the conditional quantile comparator (CQC) has emerged as a promising estimand, offering quantile-level summaries akin to the conditional quantile treatment effect (CQTE) while preserving some interpretability of the conditional average treatment effect (CATE). It achieves this by summarising the treated response conditional on both the covariates and the untreated response. Despite these desirable properties, the CQC's current estimation is limited by the need to first estimate the difference in conditional cumulative distribution functions and then invert it. This inversion obscures the CQC estimate, hampering our ability to both model and interpret it. To address this, we propose the first direct estimator of the CQC, allowing for explicit modelling and parameterisation. This explicit parameterisation enables better interpretation of our estimate while also providing a means to constrain and inform the model. We show, both theoretically and empirically, that our estimation error depends directly on the complexity of the CQC itself, improving upon the existing estimation procedure. Furthermore, it retains the desirable double robustness property with respect to nuisance parameter estimation. We further show our method to outperform existing procedures in estimation accuracy across multiple data scenarios while varying sample size and nuisance error. Finally, we apply it to real-world data from an employment scheme, uncovering a reduced range of potential earnings improvement as participant age increases."}
{"id": "2601.19666", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.19666", "abs": "https://arxiv.org/abs/2601.19666", "authors": ["Josh Givens", "Song Liu", "Henry W J Reeve", "Katarzyna Reluga"], "title": "Direct Doubly Robust Estimation of Conditional Quantile Contrasts", "comment": "To be published as a conference paper at ICLR 2026", "summary": "Within heterogeneous treatment effect (HTE) analysis, various estimands have been proposed to capture the effect of a treatment conditional on covariates. Recently, the conditional quantile comparator (CQC) has emerged as a promising estimand, offering quantile-level summaries akin to the conditional quantile treatment effect (CQTE) while preserving some interpretability of the conditional average treatment effect (CATE). It achieves this by summarising the treated response conditional on both the covariates and the untreated response. Despite these desirable properties, the CQC's current estimation is limited by the need to first estimate the difference in conditional cumulative distribution functions and then invert it. This inversion obscures the CQC estimate, hampering our ability to both model and interpret it. To address this, we propose the first direct estimator of the CQC, allowing for explicit modelling and parameterisation. This explicit parameterisation enables better interpretation of our estimate while also providing a means to constrain and inform the model. We show, both theoretically and empirically, that our estimation error depends directly on the complexity of the CQC itself, improving upon the existing estimation procedure. Furthermore, it retains the desirable double robustness property with respect to nuisance parameter estimation. We further show our method to outperform existing procedures in estimation accuracy across multiple data scenarios while varying sample size and nuisance error. Finally, we apply it to real-world data from an employment scheme, uncovering a reduced range of potential earnings improvement as participant age increases."}
{"id": "2601.19729", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.19729", "abs": "https://arxiv.org/abs/2601.19729", "authors": ["Aldo Gardini", "Lorenzo Mori"], "title": "Coarsened data in small area estimation: a Bayesian two-part model for mapping smoking behaviour", "comment": "25 pages, 7 Figures, 2 Tables", "summary": "Estimating health indicators for restricted sub-populations is a recurring challenge in epidemiology and public health. When survey data are used, Small Area Estimation (SAE) methods can improve precision by borrowing strength across domains. In many applications, however, outcomes are self-reported and affected by coarsening mechanisms, such as rounding and digit preference, that reduce data resolution and may bias inference. This paper addresses both issues by developing a Bayesian unit-level SAE framework for semi-continuous, coarsened responses. Motivated by the 2019 Italian European Health Interview Survey, we estimate smoking indicators for domains defined by the cross-classification of Italian regions and age groups, capturing both smoking prevalence and intensity. The model adopts a two-part structure: a logistic component for smoking prevalence and a flexible mixture of Lognormal distributions for average cigarette consumption, coupled with an explicit model for coarsening and topcoding. Simulation studies show that ignoring coarsening can yield biased and unstable domain estimates with poor interval coverage, whereas the proposed model improves accuracy and achieves near-nominal coverage. The empirical application provides a detailed picture of smoking patterns across region-age domains, helping to characterize the dynamics of the phenomenon and inform targeted public health policies."}
{"id": "2601.19774", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19774", "abs": "https://arxiv.org/abs/2601.19774", "authors": ["Martin Bladt", "Jorge Yslas"], "title": "Cure models: from mixture to matrix distributions", "comment": null, "summary": "Cure rate models address survival data in which a proportion of individuals will never experience the event of interest. Existing parametric approaches are predominantly based on finite mixtures, which impose restrictive assumptions on both the cure mechanism and the distribution of susceptible event times. A cure model based on phase-type distributions is introduced, leveraging their latent Markov jump process representation to allow immunity to occur either at baseline or dynamically during follow-up. This structure yields a flexible and interpretable formulation of long-term survival while encompassing classical mixture cure models as special cases. A unified regression framework is developed for covariate effects on both the cure rate and the susceptible survival distribution, and the proposed model class is dense, reducing the impact of parametric misspecification. Estimation is performed via expectation-maximization algorithms, accompanied by an automatic model selection strategy. Simulation studies and a real-data example demonstrate the practical advantages of the approach."}
{"id": "2601.19836", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.19836", "abs": "https://arxiv.org/abs/2601.19836", "authors": ["Augustine Wigle", "Erica E. M. Moodie"], "title": "Personalized Treatment Hierarchies in Bayesian Network Meta-Analysis", "comment": null, "summary": "Network Meta-Analysis (NMA) is an increasingly popular evidence synthesis tool that can provide a ranking of competing treatments, also known as a treatment hierarchy. Treatment-Covariate Interactions (TCIs) can be included in NMA models to allow relative treatment effects to vary with covariate values. We show that in an NMA model that includes TCIs, treatment hierarchies should be created with a particular covariate profile in mind. We outline the typical approach for creating a treatment hierarchy in standard Bayesian NMA and show how a treatment hierarchy for a particular covariate profile can be created from an NMA model that estimates TCIs. We demonstrate our methods using a real network of studies for treatments of major depressive disorder."}
{"id": "2601.19888", "categories": ["stat.ME", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19888", "abs": "https://arxiv.org/abs/2601.19888", "authors": ["M. Naser Lessani", "Zhenlong Li", "Manzhu Yu", "Helen Greatrex", "Chan Shen"], "title": "M-SGWR: Multiscale Similarity and Geographically Weighted Regression", "comment": null, "summary": "The first law of geography is a cornerstone of spatial analysis, emphasizing that nearby and related locations tend to be more similar, however, defining what constitutes \"near\" and \"related\" remains challenging, as different phenomena exhibit distinct spatial patterns. Traditional local regression models, such as Geographically Weighted Regression (GWR) and Multiscale GWR (MGWR), quantify spatial relationships solely through geographic proximity. In an era of globalization and digital connectivity, however, geographic proximity alone may be insufficient to capture how locations are interconnected. To address this limitation, we propose a new multiscale local regression framework, termed M-SGWR, which characterizes spatial interaction across two dimensions: geographic proximity and attribute (variable) similarity. For each predictor, geographic and attribute-based weight matrices are constructed separately and then combined using an optimized parameter, alpha, which governs their relative contribution to local model fitting. Analogous to variable-specific bandwidths in MGWR, the optimal alpha varies by predictor, allowing the model to flexibly account for geographic, mixed, or non-spatial (remote similarity) effects. Results from two simulation experiments and one empirical application demonstrate that M-SGWR consistently outperforms GWR, SGWR, and MGWR across all goodness-of-fit metrics."}
{"id": "2601.19710", "categories": ["stat.CO", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19710", "abs": "https://arxiv.org/abs/2601.19710", "authors": ["Sebastiano Grazzi", "Samuel Livingstone", "Lionel Riou-Durand"], "title": "On randomized step sizes in Metropolis-Hastings algorithms", "comment": null, "summary": "The performance of Metropolis-Hastings algorithms is highly sensitive to the choice of step size, and miss-specification can lead to severe loss of efficiency. We study algorithms with randomized step sizes, considering both auxiliary-variable and marginalized constructions. We show that algorithms with a randomized step size inherit weak Poincaré inequalities/spectral gaps from their fixed-step-size counterparts under minimal conditions, and that the marginalized kernel should always be preferred in terms of asymptotic variance to the auxiliary-variable choice if it is implementable. In addition we show that both types of randomization make an algorithm robust to tuning, meaning that spectral gaps decay polynomially as the step size is increasingly poorly chosen. We further show that step-size randomization often preserves high-dimensional scaling limits and algorithmic complexity, while increasing the optimal acceptance rate for Langevin and Hamiltonian samplers when an Exponential or Uniform distribution is chosen to randomize the step size. Theoretical results are complemented with a numerical study on challenging benchmarks such as Poisson regression, Neal's funnel and the Rosenbrock (banana) distribution."}
{"id": "2601.19722", "categories": ["stat.CO", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19722", "abs": "https://arxiv.org/abs/2601.19722", "authors": ["Francesco Pozza", "Giacomo Zanella"], "title": "Zeroth-order parallel sampling", "comment": "32 pages, 5 figures", "summary": "Finding effective ways to exploit parallel computing to accelerate Markov chain Monte Carlo methods is an important problem in Bayesian computation and related disciplines. In this paper, we consider the zeroth-order setting where the unnormalized target distribution can be evaluated but its gradient is unavailable for theoretical, practical, or computational reasons. We also assume access to $m$ parallel processors to accelerate convergence. The proposed approach is inspired by modern zeroth-order optimization methods, which mimic gradient-based schemes by replacing the gradient with a zeroth-order stochastic gradient estimator. Our contribution is twofold. First, we show that a naive application of popular zeroth-order stochastic gradient estimators within Markov chain Monte Carlo methods leads to algorithms with poor dependence on $m$, both for unadjusted and Metropolis-adjusted schemes. We then propose a simple remedy to this problem, based on a random-slice perspective, as opposed to a stochastic gradient one, obtaining a new class of zeroth-order samplers that provably achieve a polynomial speed-up in $m$. Theoretical findings are supported by numerical studies."}
{"id": "2601.19811", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.19811", "abs": "https://arxiv.org/abs/2601.19811", "authors": ["TrungKhang Tran", "TrungTin Nguyen", "Gersende Fort", "Tung Doan", "Hien Duy Nguyen", "Binh T. Nguyen", "Florence Forbes", "Christopher Drovandi"], "title": "Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts", "comment": "TrungKhang Tran and TrungTin Nguyen are co-first authors", "summary": "Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance."}
