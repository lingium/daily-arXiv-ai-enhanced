<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 12]
- [stat.ML](#stat.ML) [Total: 16]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.CO](#stat.CO) [Total: 4]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Mixed Latent Position Cluster Models for Networks](https://arxiv.org/abs/2601.22380)
*Chaoyi Lu,Riccardo Rastelli*

Main category: stat.ME

TL;DR: 提出Mixed Latent Position Cluster Model (MLPCM)来改进传统LPM，解决有向网络和加权网络中的不对称性和非欧几里得模式问题，通过分离节点的"显性"和"隐性"社会位置来更好地表示网络结构。


<details>
  <summary>Details</summary>
Motivation: 传统潜在位置模型(LPM)具有固有的对称性结构，假设出边和入边遵循相同的统计分布，因此不适合分析有向网络。同时，加权边可能导致非聚集模式和其他难以通过底层几何结构捕捉的图案。

Method: 开发了Mixed Latent Position Cluster Model (MLPCM)，通过分离节点的行为方式（如何对待他人）和被感知方式（如何被他人对待）来解构有向边，形成节点的"显性"和"隐性"社会位置的双重表示。采用变分贝叶斯方法估计参数，无需额外数值近似。引入部分集成完全似然准则进行模型选择。

Result: 在合成数据集上验证了方法的准确性，并在国际武器转让数据集上展示了实际应用价值。MLPCM能够处理不对称性和非欧几里得模式，同时为潜在空间提供新的解释。

Conclusion: MLPCM成功扩展了传统LPM框架，能够更好地处理有向网络和加权网络中的复杂模式，通过分离节点的双重社会位置提供了更丰富的网络结构解释，为网络分析提供了更强大的工具。

Abstract: Over the last two decades, the Latent Position Model (LPM) has become a prominent tool to obtain model-based visualizations of networks. However, the geometric structure of the LPM is inherently symmetric, in the sense that outgoing and incoming edges are assumed to follow the same statistical distribution. As a consequence, the canonical LPM framework is not ideal for the analysis of directed networks. In addition, edges may be weighted to describe the duration or intensity of a connection. This can lead to disassortative patterns and other motifs that cannot be easily captured by the underlying geometry. To address these limitations, we develop a novel extension of the LPM, called the Mixed Latent Position Cluster Model (MLPCM), which can deal with asymmetry and non-Euclidean patterns, while providing new interpretations of the latent space. We dissect the directed edges of the network by formally disentangling how a node behaves from how it is perceived by others. This leads to a dual representation of a node's profile, identifying its ``overt'' and ``covert'' social positions. In order to efficiently estimate the parameters of our model, we develop a variational Bayes approach to approximate the posterior distribution. Unlike many existing variational frameworks, our algorithm does not require any additional numerical approximations. Model selection is performed by introducing a novel partially integrated complete likelihood criteria, which builds upon the literature on penalized likelihood methods. We demonstrate the accuracy of our proposed methodology using synthetic datasets, and we illustrate its practical utility with an application to a dataset of international arms transfers.

</details>


### [2] [Changepoint Detection As Model Selection: A General Framework](https://arxiv.org/abs/2601.22481)
*Michael Grantham,Xueheng Shi,Bertrand Clarke*

Main category: stat.ME

TL;DR: 提出基于L0模型选择的变点检测通用框架IRFL，通过自适应重加权惩罚改进广义lasso，在包含趋势、季节性和自相关误差的复杂场景中实现准确变点检测，并扩展到图像数据处理。


<details>
  <summary>Details</summary>
Motivation: 现有变点检测方法在处理复杂数据（如包含趋势、季节性模式、自相关误差等干扰因素）时存在局限性，需要一种更鲁棒、可扩展的框架来准确检测结构变化。

Method: 提出迭代重加权融合lasso（IRFL）方法，基于L0模型选择框架，通过自适应重加权惩罚项来改进广义lasso，支持季节性模式、线性和二次趋势以及自回归依赖的灵活建模。

Result: 模拟研究表明IRFL在各种挑战性场景中实现准确变点检测；应用于图像数据可实现边缘保持去噪和分割；在Mauna Loa CO2时间序列分析中检测到与火山喷发和ENSO事件一致的变点，趋势分解比普通最小二乘法更准确。

Conclusion: IRFL为复杂数据中的结构变化检测提供了一个鲁棒、可扩展的工具，在时间序列分析和图像处理领域均有广泛应用价值。

Abstract: This dissertation presents a general framework for changepoint detection based on L0 model selection. The core method, Iteratively Reweighted Fused Lasso (IRFL), improves upon the generalized lasso by adaptively reweighting penalties to enhance support recovery and minimize criteria such as the Bayesian Information Criterion (BIC). The approach allows for flexible modeling of seasonal patterns, linear and quadratic trends, and autoregressive dependence in the presence of changepoints.
  Simulation studies demonstrate that IRFL achieves accurate changepoint detection across a wide range of challenging scenarios, including those involving nuisance factors such as trends, seasonal patterns, and serially correlated errors. The framework is further extended to image data, where it enables edge-preserving denoising and segmentation, with applications spanning medical imaging and high-throughput plant phenotyping.
  Applications to real-world data demonstrate IRFL's utility. In particular, analysis of the Mauna Loa CO2 time series reveals changepoints that align with volcanic eruptions and ENSO events, yielding a more accurate trend decomposition than ordinary least squares. Overall, IRFL provides a robust, extensible tool for detecting structural change in complex data.

</details>


### [3] [Group Sequential Methods for the Win Ratio](https://arxiv.org/abs/2601.22525)
*Tracy Bergemann,Tim Hanson*

Main category: stat.ME

TL;DR: 本文证明了在渐近分布下，用于检验赢率的U统计量的增量具有独立性，使得传统的α消耗法可直接应用于赢率作为主要终点的群序贯设计。


<details>
  <summary>Details</summary>
Motivation: 赢率在随机试验中应用日益广泛，但如何将其应用于适应性设计（特别是群序贯设计）仍存在开放性问题。关键挑战在于需要确保增量检验统计量的独立性，以便应用经典的群序贯方法。

Method: 推导了评估赢率的U统计量在渐近分布下的协方差结构，证明了增量独立性假设成立。通过模拟验证传统α消耗法在期中分析中保持I类错误率，并使用IN.PACT SFA临床试验数据进行回顾性分析。

Result: 推导出的协方差结构表明，用于检验赢率的U统计量的渐近分布满足增量独立性假设。模拟证实传统α消耗法在期中分析中能保持I类错误率，回顾性分析显示了在群序贯设计中早期停止的潜力。

Conclusion: 在常见条件下，Lan-DeMets α消耗法可直接应用于涉及赢率的随机试验，现有能够计算传统群序贯边界的软件均可使用，为赢率作为主要终点的适应性设计提供了理论基础。

Abstract: The win ratio is increasingly used in randomized trials due to its intuitive clinical interpretation, ability to incorporate the relative importance of composite endpoints, and its capacity for combining different types of outcomes (e.g. time-to-event, binary, counts, etc.) to be combined. There are open questions, however, about how to implement adaptive design approaches when the primary endpoint is a win ratio, including in group sequential designs. A key requirement allowing for straightforward application of classical group sequential methods is the independence of incremental interim test statistics. This paper derives the covariance structure of incremental U-statistics that evaluate the win ratio under its asymptotic distribution. The derived covariance shows that the independent increments assumption holds for the asymptotic distribution of U-statistics that test the win ratio. Simulations confirm that traditional $α$-spending preserves Type I error across interim looks. A retrospective look at the IN.PACT SFA clinical trial data illustrates the potential for stopping early in a group sequential design using the win ratio. We have demonstrated that straightforward use of Lan-De\uppercase{M}ets $α$-spending is possible for randomized trials involving the win ratio under certain common conditions. Thus, existing software capable of computing traditional group sequential boundaries can be employed.

</details>


### [4] [Propensity score weighted Cox regression for survival outcomes in observational studies with multiple or factorial treatments](https://arxiv.org/abs/2601.22572)
*Zixian Zhao,Chengxin Yang,Fan Li*

Main category: stat.ME

TL;DR: 提出了一种结合倾向得分加权与边际Cox比例风险模型的方法，用于估计多治疗组与共同参照治疗之间的因果边际风险比


<details>
  <summary>Details</summary>
Motivation: 观察性研究中常有多于两种治疗方案，但相应的分析方法有限，需要开发适用于多治疗组的因果推断方法

Method: 结合多治疗倾向得分加权方法（逆概率治疗加权和重叠加权）与边际Cox模型，使用治疗指标作为预测变量，推导加权偏似然估计的相合性和稳健三明治方差估计

Result: 开发了R包'PSsurvival'，应用于评估三种抗肥胖药物对心力衰竭的真实世界比较效果，并详细阐述了双向因子治疗这一重要特例

Conclusion: 该方法为观察性研究中多治疗组的生存分析提供了有效的因果推断框架，扩展了传统两治疗组方法的适用范围

Abstract: In observational studies with survival or time-to-event outcomes, a propensity score weighted marginal Cox proportional hazard model with the treatment variable as the only predictor is commonly used to estimate the causal marginal hazard ratio between two treatments. Observational studies often have more than two treatments, but corresponding analysis methods are limited. In this paper, we combine the propensity score weighting method for multiple treatments and a marginal Cox model with indicators for each treatment to estimate the causal hazard ratios between multiple treatments and a common reference treatment. We illustrate two weighting schemes: inverse probability of treatment weighting and overlap weighting. We prove the consistency of the maximum weighted partial likelihood estimator of the causal marginal hazard ratio and derive a robust sandwich variance estimator. As an important special case of multiple treatments, we elaborate the Cox model for two-way factorial treatments. We apply the method to evaluate the real-world comparative effectiveness of three types of anti-obesity medications on heart failure. We develop an associated R package 'PSsurvival'.

</details>


### [5] [Quadratic robust methods for causal mediation analysis](https://arxiv.org/abs/2601.22592)
*Zhen Qi,Yuqian Zhang*

Main category: stat.ME

TL;DR: 提出新的四重稳健框架用于因果中介效应估计，扩展了三重稳健方法，支持非参数建模和高维场景


<details>
  <summary>Details</summary>
Motivation: 现有三重稳健框架及其扩展已用于估计自然效应，但需要更稳健的识别框架来扩大模型类别并减少模型误设影响

Method: 提出四重稳健框架：1) 非参数建模方法，支持机器学习进行干扰参数估计；2) 高维场景下的参数建模策略，开发模型四重稳健估计器

Result: 模拟研究和真实数据应用证明了所提方法在有限样本下的性能表现

Conclusion: 四重稳健框架扩展了因果中介分析的模型类别，提供了更稳健的估计方法，特别适用于高维和非参数场景

Abstract: Estimating natural effects is a core task in causal mediation analysis. Existing triply robust (TR) frameworks (Tchetgen Tchetgen & Shpitser 2012) and their extensions have been developed to estimate the natural effects. In this work, we introduce a new quadruply robust (QR) framework that enlarges the model class for unbiased identification. We study two modeling strategies. The first is a nonparametric modeling approach, under which we propose a general QR estimator that supports the use of machine learning methods for nuisance estimation. We also study high-dimensional settings, where the dimensions of covariates and mediators may both be large. In these settings, we adopt a parametric modeling strategy and develop a model quadruply robust (MQR) estimator to limit the impact of model misspecification. Simulation studies and a real data application demonstrate the finite-sample performance of the proposed methods.

</details>


### [6] [Policy learning under constraint: Maximizing a primary outcome while controlling an adverse event](https://arxiv.org/abs/2601.22717)
*Laura Fuentes-Vicente,Mathieu Even,Gaelle Dormion,Julie Josse,Antoine Chambaz*

Main category: stat.ME

TL;DR: PLUC是一种在观察性数据中学习医疗策略的方法，通过优化强凸拉格朗日准则来平衡治疗效果和不良事件风险，使用Frank-Wolfe算法迭代更新策略。


<details>
  <summary>Details</summary>
Motivation: 传统医疗策略优化通常只关注单一结果（如治疗效果），忽略了多结果设置中的不良事件风险。在观察性数据中，需要一种能够同时考虑治疗效果和不良事件概率的个性化治疗推荐方法。

Method: PLUC通过优化强凸拉格朗日准则在函数凸包上学习CATE估计器，采用交替程序：先用Frank-Wolfe算法最小化当前准则，然后通过目标化步骤更新准则，使其在先前访问的"地标"处成为对应理论量的目标估计器。

Result: 开发了R包PLUC-R实现该方法，并通过一系列数值实验展示了PLUC的性能，表明其能够有效控制不良事件概率的同时优化治疗效果。

Conclusion: PLUC为多结果医疗决策提供了一种有效的约束策略学习方法，能够在观察性数据中平衡治疗效果和不良事件风险，具有实际应用价值。

Abstract: A medical policy aims to support decision-making by mapping patient characteristics to individualized treatment recommendations. Standard approaches typically optimize a single outcome criterion. For example, recommending treatment according to the sign of the Conditional Average Treatment Effect (CATE) maximizes the policy "value" by exploiting treatment effect heterogeneity. This point of view shifts policy learning towards the challenge of learning a reliable CATE estimator. However, in multi-outcome settings, such strategies ignore the risk of adverse events, despite their relevance. PLUC (Policy Learning Under Constraint) addresses this challenges by learning an estimator of the CATE that yields smoothed policies controlling the probability of an adverse event in observational settings. Inspired by insights from EP-learning, PLUC involves the optimization of strongly convex Lagrangian criteria over a convex hull of functions. Its alternating procedure iteratively applies the Frank-Wolfe algorithm to minimize the current criterion, then performs a targeting step that updates the criterion so that its evaluations at previously visited landmarks become targeted estimators of the corresponding theoretical quantities. An R package PLUC-R provides a practical implementation. We illustrate PLUC's performance through a series of numerical experiments.

</details>


### [7] [Depth-based estimation for multivariate functional data with phase variability](https://arxiv.org/abs/2601.22884)
*Ana Arribas-Gil,Sara López-Pintado*

Main category: stat.ME

TL;DR: 针对具有个体相位变异的多变量函数数据，开发了一种基于深度的鲁棒方法，用于估计存在跨分量时间扭曲时的主模式函数。


<details>
  <summary>Details</summary>
Motivation: 在多变量函数数据分析中，个体相位变异和跨分量时间扭曲是常见挑战。现有方法在处理这些复杂变异时可能不够鲁棒，特别是在存在异常观测或模型假设违反的情况下。需要开发一种能够同时处理个体相位变异和跨分量时间扭曲的鲁棒估计方法。

Method: 采用基于函数深度的鲁棒方法，基于潜在变形模型（Carroll和Müller，2023），其中多变量函数的不同分量是共同模板函数的时间扭曲版本。不局限于特定的深度度量，而是讨论深度函数需要满足的条件，以确保在中心模式估计中的一致性。

Result: 通过模拟研究评估了方法的性能，验证了其对异常观测和模型假设违反的鲁棒性。在两个真实数据集上展示了方法的实际应用效果，证明了方法的有效性。

Conclusion: 提出的基于深度的鲁棒方法能够有效估计具有个体相位变异和跨分量时间扭曲的多变量函数数据的主模式函数，对异常观测和模型假设违反具有鲁棒性，为复杂函数数据分析提供了实用工具。

Abstract: In the context of multivariate functional data with individual phase variation, we develop a robust depth-based approach to estimate the main pattern function when cross-component time warping is also present. In particular, we consider the latent deformation model (Carroll and Müller, 2023) in which the different components of a multivariate functional variable are also time-distorted versions of a common template function. Rather than focusing on a particular functional depth measure, we discuss the necessary conditions on a depth function to be able to provide a consistent estimation of the central pattern, considering different model assumptions. We evaluate the method performance and its robustness against atypical observations and violations of the model assumptions through simulations, and illustrate its use on two real data sets.

</details>


### [8] [Optimal Sample Splitting for Observational Studies](https://arxiv.org/abs/2601.22782)
*Qishuo Yin,Dylan S. Small*

Main category: stat.ME

TL;DR: 提出使用plasmode数据集优化规划样本与分析样本比例的方法，以减少未测量混杂偏倚对观察性研究的影响


<details>
  <summary>Details</summary>
Motivation: 观察性研究中，未测量的混杂因素可能导致估计偏倚，影响结果有效性。虽然已有研究通过将数据分为规划样本和分析样本来降低偏倚敏感性，但样本比例的选择缺乏理论依据。

Method: 开发基于plasmode数据集的方法，寻找规划样本与分析样本的最优比例。该方法适用于高维结果空间，并实现了R包OptimalSampling。

Result: 方法在高维结果空间中表现良好，应用于儿童二手烟暴露影响研究，并提供了可用的R软件包。

Conclusion: 提出的优化方法为观察性研究设计提供了更科学的样本分配方案，有助于提高研究对未测量混杂偏倚的稳健性。

Abstract: In observational studies of treatment effects, estimates may be biased by unmeasured confounders, which can potentially affect the validity of the results. Understanding sensitivity to such biases helps assess how unmeasured confounding impacts credibility. The design of an observational study strongly influences its sensitivity to bias. Previous work has shown that the sensitivity to bias can be reduced by dividing a dataset into a planning sample and a larger analysis sample, where the planning sample guides design decisions. But the choice of what fraction of the data to put in the planning sample vs. the analysis sample was ad hoc. Here, we develop an approach to find the optimal fraction using plasmode datasets. We show that our method works well in high-dimensional outcome spaces. We apply our method to study the effects of exposure to second-hand smoke in children. The OptimalSampling R package implementing our method is available at GitHub.

</details>


### [9] [Computationally efficient segmentation for non-stationary time series with oscillatory patterns](https://arxiv.org/abs/2601.22999)
*Nicolas Bianco,Lorenzo Cappello*

Main category: stat.ME

TL;DR: 提出一种用于多变量非平稳时间序列中变化点检测和参数学习的新方法，将振荡过程近似为分段正弦函数，通过参数空间离散化转换为线性模型，显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理具有振荡行为的非平稳时间序列时，通常需要使用复杂的跨维度MCMC算法，计算成本高昂。需要一种更高效的方法来进行变化点检测和参数学习。

Method: 将振荡过程近似为分段正弦函数（未知频率和振幅加噪声），通过离散化参数空间将复杂估计问题转化为线性模型，其中协变量为傅里叶基函数，然后使用任何分段算法进行变化点检测。

Result: 方法比现有方法显著更快，同时保持可比的数值精度；提供了变化点定位误差的高概率边界；成功应用于气候和EEG睡眠数据。

Conclusion: 该方法提供了一种高效的变化点检测和参数学习框架，避免了复杂的跨维度MCMC算法，在保持精度的同时大幅提升计算速度，适用于具有振荡行为的非平稳时间序列分析。

Abstract: We propose a novel approach for change-point detection and parameter learning in multivariate non-stationary time series exhibiting oscillatory behaviour. We approximate the process through a piecewise function defined by a sum of sinusoidal functions with unknown frequencies and amplitudes plus noise. The inference for this model is non-trivial. However, discretising the parameter space allows us to recast this complex estimation problem into a more tractable linear model, where the covariates are Fourier basis functions. Then, any change-point detection algorithms for segmentation can be used. The advantage of our proposal is that it bypasses the need for trans-dimensional Markov chain Monte Carlo algorithms used by state-of-the-art methods. Through simulations, we demonstrate that our method is significantly faster than existing approaches while maintaining comparable numerical accuracy. We also provide high probability bounds on the change-point localization error. We apply our methodology to climate and EEG sleep data.

</details>


### [10] [Dynamic modelling and evaluation of preclinical trials in acute leukaemia](https://arxiv.org/abs/2601.22971)
*Julian Wäsche,Romina Ludwig,Irmela Jeremias,Christiane Fuchs*

Main category: stat.ME

TL;DR: 该研究使用指数增长和逻辑增长模型分析白血病细胞生长数据，相比传统双时间点统计方法能更可靠地识别基因修饰效果。


<details>
  <summary>Details</summary>
Motivation: 当前白血病研究中，时间序列数据常被简化为两个时间点的统计比较，既浪费了时间序列信息，又忽略了已知的细胞生长机制知识。需要更充分利用时间序列数据和生物学知识的分析方法。

Method: 采用两种种群增长模型（指数增长模型和逻辑增长模型）来捕捉整个实验时间范围内的细胞动态变化，同时考虑所有测量时间点。通过估计模型参数来推导基因修饰效果。

Result: 指数增长模型在模拟场景中比逻辑增长模型和传统统计测试更可靠地识别基因修饰效果。在急性白血病患者来源异种移植（PDX）模型中成功应用于评估候选基因敲除的疗效。

Conclusion: 基于数学模型的动态分析方法优于传统的双时间点统计测试，能更充分利用时间序列数据和生物学知识，为白血病研究提供更可靠的基因修饰效果评估。

Abstract: Dynamic models are widely used to mathematically describe biological phenomena that evolve over time. One important area of application is leukaemia research, where leukaemia cells are genetically modified in preclinical studies to explore new therapeutic targets for reducing leukaemic burden. In advanced experiments, these studies are often conducted in mice and generate time-resolved data, the analysis of which may reveal growth-inhibiting effects of the investigated gene modifications. However, the experimental data is often times evaluated using statistical tests which compare measurements from only two different time points. This approach does not only reduce the time series to two instances but also neglects biological knowledge about cell mechanisms. Such knowledge, translated into mathematical models, expands the power to investigate and understand effects of modifications on underlying mechanisms based on experimental data. We utilise two population growth models -- an exponential and a logistic growth model -- to capture cell dynamics over the whole experimental time horizon and to consider all measurement times jointly. This approach enables us to derive modification effects from estimated model parameters. We demonstrate that the exponential growth model recognises simulated scenarios more reliably than the other candidate model and than a statistical test. Moreover, we apply the population growth models to evaluate the efficacy of candidate gene knockouts in patient-derived xenograft (PDX) models of acute leukaemia.

</details>


### [11] [Differences in Performance of Bayesian Dynamic Borrowing and Synthetic Control Methods: A Case Study of Pediatric Atopic Dermatitis](https://arxiv.org/abs/2601.23021)
*Nicole Cizauskas,Foteini Strimenopoulou,Svetlana S. Cherlin,James M. S. Wason*

Main category: stat.ME

TL;DR: 比较贝叶斯动态借用(BDB)和合成控制方法(SCM)在儿科特应性皮炎临床试验中的表现，发现SCM具有更高的检验功效(0.641 vs 0.580)且第一类错误率相似(0.027 vs 0.026)。


<details>
  <summary>Details</summary>
Motivation: 当临床试验面临招募、保留或分配困难时，BDB和SCM都是常用的设计方法，但之前由于应用场景、产品和测量指标的不同，这两种方法的性能从未被直接比较过。

Method: 使用儿科特应性皮炎的案例研究，选取6个历史随机对照试验数据，分别用RBesT库创建MAP先验(BDB方法)和Synthpop库创建合成控制臂(SCM方法)，以检验功效和第一类错误率作为比较指标。

Result: BDB方法的检验功效为0.580，第一类错误率为0.026；SCM方法的检验功效为0.641，第一类错误率为0.027。SCM在此案例中表现出更高的检验功效，且第一类错误率与BDB相似。

Conclusion: 在此案例研究中，SCM模型比BDB方法具有更高的检验功效，且第一类错误率相似。但选择使用SCM还是BDB应基于具体试验需求，因为它们的检验功效和第一类错误率可能因具体情况而异。

Abstract: Bayesian dynamic borrowing (BDB) and synthetic control methods (SCM) are both used in clinical trial design when recruitment, retention, or allocation is a challenge. The performance of these approaches has not previously been directly compared due to differences in application, product, and measurement metrics. This study aims to conduct a comparison of power and type 1 error rates of BDB (using meta-analytic predictive prior (MAP)) and SCM using a case study of Pediatric Atopic Dermatitis. Six historical randomised control trials were selected for use in both the creation of the MAP prior and synthetic control arm. The R library RBesT was used to create a MAP prior and the R library Synthpop was used to create a synthetic control arm for the SCM. Power and type 1 error rate were used as comparison metrics. BDB produced a power of 0.580 and a type 1 error rate of 0.026. SCM produced a power of 0.641 and a type 1 error rate of 0.027. In this case study, the SCM model produced a higher power than the BDB method with a similar type 1 error rate. However, the decision to use SCM or BDB should come from the specific needs of the potential trial, since their power and type 1 error rate may differ on a case-by-case basis.

</details>


### [12] [Robust, partially alive particle Metropolis-Hastings via the Frankenfilter](https://arxiv.org/abs/2601.23173)
*Chris Sherlock,Andrew Golightly,Anthony Lee*

Main category: stat.ME

TL;DR: 提出Frankenfilter粒子滤波器，通过控制成功模拟数量来解决传统粒子滤波在条件似然为零时的计算问题，提高PMMH算法的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统粒子滤波在隐马尔可夫模型中，当观测给定隐藏过程的条件似然为零时，所有粒子模拟都可能产生零值，导致无法估计滤波分布和参数似然。对于异常观测或参数值不佳的情况，非零结果概率极低，计算成本过高。

Method: 提出Frankenfilter，一种有原则的部分存活粒子滤波器，通过设定用户定义的成功数量目标，同时固定模拟次数的上下界。该滤波器产生无偏的似然估计器，适用于伪边际Metropolis-Hastings算法。

Result: 使用Frankenfilter的PMMH比使用标准粒子滤波的PMMH对异常值和初始参数误设更鲁棒，通常效率至少提高2-3倍。对于n个精确观测的情况，建议目标设为n次成功。

Conclusion: Frankenfilter通过控制成功模拟数量解决了传统粒子滤波的计算瓶颈，提高了贝叶斯推断的鲁棒性和计算效率，为处理条件似然为零的情况提供了实用解决方案。

Abstract: When a hidden Markov model permits the conditional likelihood of an observation given the hidden process to be zero, all particle simulations from one observation time to the next could produce zeros. If so, the filtering distribution cannot be estimated and the estimated parameter likelihood is zero. The alive particle filter addresses this by simulating a random number of particles for each inter-observation interval, stopping after a target number of non-zero conditional likelihoods. For outlying observations or poor parameter values, a non-zero result can be extremely unlikely, and computational costs prohibitive. We introduce the Frankenfilter, a principled, partially alive particle filter that targets a user-defined amount of success whilst fixing lower and upper bounds on the number of simulations. The Frankenfilter produces unbiased estimators of the likelihood, suitable for pseudo-marginal Metropolis--Hastings (PMMH). We demonstrate that PMMH with the Frankenfilter is more robust to outliers and mis-specified initial parameter values than PMMH using standard particle filters, and is typically at least 2-3 times more efficient. We also provide advice for choosing the amount of success. In the case of n exact observations, this is particularly simple: target n successes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [13] [Dependence-Aware Label Aggregation for LLM-as-a-Judge via Ising Models](https://arxiv.org/abs/2601.22336)
*Krishnakumar Balasubramanian,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan*

Main category: stat.ML

TL;DR: 论文提出了一种考虑标注者依赖性的标签聚合方法，使用伊辛图模型和潜在因子模型，相比传统条件独立假设方法有更好性能


<details>
  <summary>Details</summary>
Motivation: 大规模AI评估依赖多个标注者（包括LLM作为裁判）的二元判断。传统方法假设标注者在给定真实标签下条件独立，但LLM标注者由于共享数据、架构、提示和失败模式，这一假设常被违反，导致后验概率校准错误和自信的错误预测。

Method: 提出基于伊辛图模型和潜在因子模型的层次化依赖感知模型。对于类别依赖的伊辛模型，贝叶斯对数几率是投票的二次函数；对于类别无关耦合，简化为具有相关性调整参数的线性加权投票。

Result: 展示了有限标注者示例中，基于条件独立的方法可能翻转贝叶斯标签；证明了这些方法在标注者数量增长时仍严格次优，在潜在因子下产生非零超额风险；在三个真实数据集上评估，相比传统基线有改进性能。

Conclusion: 标注者依赖性对标签聚合至关重要，忽略依赖性的传统方法会导致次优结果。提出的依赖感知模型能更好地处理LLM标注者间的相关性，提高聚合性能。

Abstract: Large-scale AI evaluation increasingly relies on aggregating binary judgments from $K$ annotators, including LLMs used as judges. Most classical methods, e.g., Dawid-Skene or (weighted) majority voting, assume annotators are conditionally independent given the true label $Y\in\{0,1\}$, an assumption often violated by LLM judges due to shared data, architectures, prompts, and failure modes. Ignoring such dependencies can yield miscalibrated posteriors and even confidently incorrect predictions. We study label aggregation through a hierarchy of dependence-aware models based on Ising graphical models and latent factors. For class-dependent Ising models, the Bayes log-odds is generally quadratic in votes; for class-independent couplings, it reduces to a linear weighted vote with correlation-adjusted parameters. We present finite-$K$ examples showing that methods based on conditional independence can flip the Bayes label despite matching per-annotator marginals. We prove separation results demonstrating that these methods remain strictly suboptimal as the number of judges grows, incurring nonvanishing excess risk under latent factors. Finally, we evaluate the proposed method on three real-world datasets, demonstrating improved performance over the classical baselines.

</details>


### [14] [Amortized Simulation-Based Inference in Generalized Bayes via Neural Posterior Estimation](https://arxiv.org/abs/2601.22367)
*Shiyi Sun,Geoff K. Nicholls,Jeong Eun Lee*

Main category: stat.ML

TL;DR: 提出首个完全摊销的变分近似方法，通过训练单个(x,β)-条件化神经后验估计器，实现广义贝叶斯推断的快速采样，无需MCMC或模拟器调用。


<details>
  <summary>Details</summary>
Motivation: 现有广义贝叶斯推断方法依赖昂贵的MCMC或SDE采样器，且需要为每个新数据集和每个温度β重新运行，计算成本高。

Method: 训练单个(x,β)-条件化神经后验估计器q_φ(θ|x,β)，支持单次前向传播采样。采用两种互补训练策略：(1)合成离流形样本(θ,x)∼π(θ)p(x|θ)^β；(2)使用自归一化重要性采样重新加权固定基础数据集。

Result: 在四个标准基于模拟的推理基准测试（包括混沌Lorenz-96系统）中，β-摊销估计器在标准双样本指标上实现了有竞争力的后验近似，在广泛温度范围内匹配非摊销MCMC功率后验采样器。

Conclusion: 提出的β-摊销变分近似方法能够高效实现广义贝叶斯推断，提供快速、准确的温度后验采样，解决了现有方法计算成本高的问题。

Abstract: Generalized Bayesian Inference (GBI) tempers a loss with a temperature $β>0$ to mitigate overconfidence and improve robustness under model misspecification, but existing GBI methods typically rely on costly MCMC or SDE-based samplers and must be re-run for each new dataset and each $β$ value. We give the first fully amortized variational approximation to the tempered posterior family $p_β(θ\mid x) \propto π(θ)\,p(x \mid θ)^β$ by training a single $(x,β)$-conditioned neural posterior estimator $q_φ(θ\mid x,β)$ that enables sampling in a single forward pass, without simulator calls or inference-time MCMC. We introduce two complementary training routes: (i) synthesize off-manifold samples $(θ,x) \sim π(θ)\,p(x \mid θ)^β$ and (ii) reweight a fixed base dataset $π(θ)\,p(x \mid θ)$ using self-normalized importance sampling (SNIS). We show that the SNIS-weighted objective provides a consistent forward-KL fit to the tempered posterior with finite weight variance. Across four standard simulation-based inference (SBI) benchmarks, including the chaotic Lorenz-96 system, our $β$-amortized estimator achieves competitive posterior approximations in standard two-sample metrics, matching non-amortized MCMC-based power-posterior samplers over a wide range of temperatures.

</details>


### [15] [It's all the (Exponential) Family: An Equivalence between Maximum Likelihood Estimation and Control Variates for Sketching Algorithms](https://arxiv.org/abs/2601.22378)
*Keegan Kang,Kerong Wang,Ding Zhang,Rameshwar Pratap,Bhisham Dev Verma,Benedict H. W. Wong*

Main category: stat.ML

TL;DR: 论文证明了在指数族分布中，最优控制变量估计器能达到与最大似然估计相同的渐近方差，并提出了相应的EM算法。实验显示该算法在双变量正态分布中比其他求根算法更快、数值更稳定。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和草图算法中，最大似然估计和控制变量估计常结合已知信息使用。研究这两种估计器之间的关系，特别是它们渐近性能的等价性，有助于开发更高效稳定的估计算法。

Method: 在指数族分布中，证明最优控制变量估计器与最大似然估计具有相同渐近方差的条件。基于此推导出最大似然估计的期望最大化算法，并在双变量正态分布中进行实验验证。

Result: 实验表明，提出的EM算法在双变量正态分布中比其他求根算法更快且数值更稳定。该算法还能提高使用MLE/CVE算法的可重复性，并在已知控制变量权重时找到最大似然估计。

Conclusion: 在指数族分布满足特定条件下，最优控制变量估计器与最大似然估计具有相同的渐近方差，相应的EM算法提供了更高效稳定的求解方法，有望推广到满足条件的其他分布。

Abstract: Maximum likelihood estimators (MLE) and control variate estimators (CVE) have been used in conjunction with known information across sketching algorithms and applications in machine learning. We prove that under certain conditions in an exponential family, an optimal CVE will achieve the same asymptotic variance as the MLE, giving an Expectation-Maximization (EM) algorithm for the MLE. Experiments show the EM algorithm is faster and numerically stable compared to other root finding algorithms for the MLE for the bivariate Normal distribution, and we expect this to hold across distributions satisfying these conditions. We show how the EM algorithm leads to reproducibility for algorithms using MLE / CVE, and demonstrate how the EM algorithm leads to finding the MLE when the CV weights are known.

</details>


### [16] [Simulation-based Bayesian inference with ameliorative learned summary statistics -- Part I](https://arxiv.org/abs/2601.22441)
*Getachew K. Befekadu*

Main category: stat.ML

TL;DR: 该论文提出了一种基于模拟的推理框架，使用学习到的摘要统计量作为经验似然，解决精确似然函数难以获得或计算不可行的问题，特别适用于分布式计算环境。


<details>
  <summary>Details</summary>
Motivation: 当观测数据与模拟模型相关的精确似然函数难以获得闭合形式或计算不可行时，需要一种有效的推理方法。传统方法在处理复杂模拟模型和大数据集时面临挑战。

Method: 使用基于Cressie-Read差异准则的转换技术，在矩约束下总结观测数据与模拟输出之间的学习统计量，同时保持推理的统计功效。将数据转换为学习摘要统计量，使模拟输出能够以观测数据为条件。

Result: 提出的框架能够处理弱依赖观测数据，并适用于分布式计算实现。数据到学习摘要统计量的转换和贝叶斯推理任务可以统一为分布式推理问题，利用分布式优化和MCMC算法支持复杂模拟模型的大数据集。

Conclusion: 该论文提出的基于模拟的推理框架通过学习摘要统计量作为经验似然，为难以获得精确似然函数的情况提供了有效的解决方案，特别适合分布式计算环境和大规模复杂模拟模型。

Abstract: This paper, which is Part 1 of a two-part paper series, considers a simulation-based inference with learned summary statistics, in which such a learned summary statistic serves as an empirical-likelihood with ameliorative effects in the Bayesian setting, when the exact likelihood function associated with the observation data and the simulation model is difficult to obtain in a closed form or computationally intractable. In particular, a transformation technique which leverages the Cressie-Read discrepancy criterion under moment restrictions is used for summarizing the learned statistics between the observation data and the simulation outputs, while preserving the statistical power of the inference. Here, such a transformation of data-to-learned summary statistics also allows the simulation outputs to be conditioned on the observation data, so that the inference task can be performed over certain sample sets of the observation data that are considered as an empirical relevance or believed to be particular importance. Moreover, the simulation-based inference framework discussed in this paper can be extended further, and thus handling weakly dependent observation data. Finally, we remark that such an inference framework is suitable for implementation in distributed computing, i.e., computational tasks involving both the data-to-learned summary statistics and the Bayesian inferencing problem can be posed as a unified distributed inference problem that will exploit distributed optimization and MCMC algorithms for supporting large datasets associated with complex simulation models.

</details>


### [17] [Corrected Samplers for Discrete Flow Models](https://arxiv.org/abs/2601.22519)
*Zhengyan Wan,Yidong Ouyang,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: 提出两种修正采样器（时间修正和位置修正），用于离散流模型，降低离散化误差，减少迭代次数，提高生成质量和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型采样器（如tau-leaping和Euler求解器）需要大量迭代来控制离散化误差，且理论结果通常需要转移率有界或特定源分布限制。需要解决这些局限性。

Method: 建立无限制转移率和源分布的非渐近离散化误差界，分析Euler采样器的一步下界，提出时间修正采样器和位置修正采样器，几乎不增加计算成本。

Result: 位置修正采样器比现有并行采样器具有更低的迭代复杂度，在仿真和文生图任务中验证了生成质量提升和推理时间减少。

Conclusion: 提出的修正采样器有效解决了离散流模型采样中的离散化误差问题，在理论和实验上均表现出优越性能。

Abstract: Discrete flow models (DFMs) have been proposed to learn the data distribution on a finite state space, offering a flexible framework as an alternative to discrete diffusion models. A line of recent work has studied samplers for discrete diffusion models, such as tau-leaping and Euler solver. However, these samplers require a large number of iterations to control discretization error, since the transition rates are frozen in time and evaluated at the initial state within each time interval. Moreover, theoretical results for these samplers often require boundedness conditions of the transition rate or they focus on a specific type of source distributions. To address those limitations, we establish non-asymptotic discretization error bounds for those samplers without any restriction on transition rates and source distributions, under the framework of discrete flow models. Furthermore, by analyzing a one-step lower bound of the Euler sampler, we propose two corrected samplers: \textit{time-corrected sampler} and \textit{location-corrected sampler}, which can reduce the discretization error of tau-leaping and Euler solver with almost no additional computational cost. We rigorously show that the location-corrected sampler has a lower iteration complexity than existing parallel samplers. We validate the effectiveness of the proposed method by demonstrating improved generation quality and reduced inference time on both simulation and text-to-image generation tasks. Code can be found in https://github.com/WanZhengyan/Corrected-Samplers-for-Discrete-Flow-Models.

</details>


### [18] [Graph Attention Network for Node Regression on Random Geometric Graphs with Erdős--Rényi contamination](https://arxiv.org/abs/2601.23239)
*Somak Laha,Suqi Liu,Morgane Austern*

Main category: stat.ML

TL;DR: 该论文证明了在节点回归任务中，经过精心设计的图注意力网络(GAT)相比普通最小二乘估计器(OLS)和非注意力图神经网络(GCN)具有理论优势，特别是在处理噪声节点特征和边污染的情况下。


<details>
  <summary>Details</summary>
Motivation: 尽管图注意力网络(GATs)被广泛使用且通常表现出对节点特征和边噪声的鲁棒性，但缺乏严格的统计理论保证来证明GATs相比非注意力图神经网络(GNNs)具有可证明的优势。作者旨在填补这一理论空白。

Method: 针对节点回归任务，在同时存在特征和边污染的情况下，提出了一个精心设计的任务特定GAT架构。该GAT构建去噪的代理特征用于回归，并利用高维几何尾界和邻域计数与样本协方差的集中性进行分析。

Result: 理论证明表明，在温和的增长条件下，使用GAT构建的代理特征进行回归相比：1)在噪声节点特征上的普通最小二乘估计器(OLS)具有更低的回归系数估计误差；2)相比普通图卷积网络(GCN)在未标记节点响应预测上具有更低误差。实验在合成数据和真实世界图上验证了理论发现。

Conclusion: 该研究为图注意力网络提供了理论依据，证明了在存在特征和边噪声的情况下，精心设计的GAT相比传统方法具有统计优势，填补了GAT理论保证的空白，并通过实验验证了注意力机制在节点回归任务中的有效性。

Abstract: Graph attention networks (GATs) are widely used and often appear robust to noise in node covariates and edges, yet rigorous statistical guarantees demonstrating a provable advantage of GATs over non-attention graph neural networks~(GNNs) are scarce. We partially address this gap for node regression with graph-based errors-in-variables models under simultaneous covariate and edge corruption: responses are generated from latent node-level covariates, but only noise-perturbed versions of the latent covariates are observed; and the sample graph is a random geometric graph created from the node covariates but contaminated by independent Erdős--Rényi edges. We propose and analyze a carefully designed, task-specific GAT that constructs denoised proxy features for regression. We prove that regressing the response variables on the proxies achieves lower error asymptotically in (a) estimating the regression coefficient compared to the ordinary least squares (OLS) estimator on the noisy node covariates, and (b) predicting the response for an unlabelled node compared to a vanilla graph convolutional network~(GCN) -- under mild growth conditions. Our analysis leverages high-dimensional geometric tail bounds and concentration for neighbourhood counts and sample covariances. We verify our theoretical findings through experiments on synthetically generated data. We also perform experiments on real-world graphs and demonstrate the effectiveness of the attention mechanism in several node regression tasks.

</details>


### [19] [An Efficient Algorithm for Thresholding Monte Carlo Tree Search](https://arxiv.org/abs/2601.22600)
*Shoma Nameki,Atsuyoshi Nakamura,Junpei Komiyama,Koji Tabata*

Main category: stat.ML

TL;DR: 论文提出了阈值蒙特卡洛树搜索问题，开发了基于Track-and-Stop策略的δ正确顺序采样算法，具有渐近最优样本复杂度，并通过比率修改D-Tracking策略显著改善了实际样本复杂度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛树搜索在实际应用中需要处理具有未知分布叶节点的树结构决策问题，传统方法在样本效率和计算成本方面存在不足，需要开发更高效的算法来解决阈值判断问题。

Method: 基于Track-and-Stop策略开发δ正确顺序采样算法，采用比率修改的D-Tracking臂拉动策略，将每轮计算成本从线性降低到对数级别。

Result: 算法具有渐近最优样本复杂度，比率修改显著改善了实际样本复杂度，同时将每轮计算成本从O(n)降低到O(log n)，其中n为臂的数量。

Conclusion: 提出的阈值蒙特卡洛树搜索算法在理论保证和实际性能上都取得了显著改进，为处理具有未知分布叶节点的树结构决策问题提供了高效解决方案。

Abstract: We introduce the Thresholding Monte Carlo Tree Search problem, in which, given a tree $\mathcal{T}$ and a threshold $θ$, a player must answer whether the root node value of $\mathcal{T}$ is at least $θ$ or not. In the given tree, `MAX' or `MIN' is labeled on each internal node, and the value of a `MAX'-labeled (`MIN'-labeled) internal node is the maximum (minimum) of its child values. The value of a leaf node is the mean reward of an unknown distribution, from which the player can sample rewards. For this problem, we develop a $δ$-correct sequential sampling algorithm based on the Track-and-Stop strategy that has asymptotically optimal sample complexity. We show that a ratio-based modification of the D-Tracking arm-pulling strategy leads to a substantial improvement in empirical sample complexity, as well as reducing the per-round computational cost from linear to logarithmic in the number of arms.

</details>


### [20] [RPWithPrior: Label Differential Privacy in Regression](https://arxiv.org/abs/2601.22625)
*Haixia Liu,Ruifan Huang*

Main category: stat.ML

TL;DR: 提出一种新的回归任务ε-标签差分隐私保护方法，通过建模连续随机变量避免离散化，在已知和未知先验情况下都能获得更好性能


<details>
  <summary>Details</summary>
Motivation: 现有回归任务的ε-标签差分隐私方法（如RR-On-Bins）需要将输出空间离散化为有限区间，并通过取整操作确定这些区间，这与现实场景不匹配，存在局限性

Method: 将原始响应和随机化响应建模为连续随机变量，避免离散化；提出估计随机化响应最优区间的新算法，包括已知先验的RPWithPrior算法和未知先验情况下的算法；证明RPWithPrior算法保证ε-标签差分隐私

Result: 在Communities and Crime、Criteo Sponsored Search Conversion Log、California Housing数据集上的数值结果表明，该方法相比Gaussian、Laplace、Staircase、RRonBins、Unbiased等机制获得更好的性能

Conclusion: 提出的连续随机变量建模方法克服了现有离散化方法的局限性，在保护用户隐私的同时实现了更小的精度损失，为回归任务的差分隐私保护提供了更优方案

Abstract: With the wide application of machine learning techniques in practice, privacy preservation has gained increasing attention. Protecting user privacy with minimal accuracy loss is a fundamental task in the data analysis and mining community. In this paper, we focus on regression tasks under $ε$-label differential privacy guarantees. Some existing methods for regression with $ε$-label differential privacy, such as the RR-On-Bins mechanism, discretized the output space into finite bins and then applied RR algorithm. To efficiently determine these finite bins, the authors rounded the original responses down to integer values. However, such operations does not align well with real-world scenarios. To overcome these limitations, we model both original and randomized responses as continuous random variables, avoiding discretization entirely. Our novel approach estimates an optimal interval for randomized responses and introduces new algorithms designed for scenarios where a prior is either known or unknown. Additionally, we prove that our algorithm, RPWithPrior, guarantees $ε$-label differential privacy. Numerical results demonstrate that our approach gets better performance compared with the Gaussian, Laplace, Staircase, and RRonBins, Unbiased mechanisms on the Communities and Crime, Criteo Sponsored Search Conversion Log, California Housing datasets.

</details>


### [21] [Generative and Nonparametric Approaches for Conditional Distribution Estimation: Methods, Perspectives, and Comparative Evaluations](https://arxiv.org/abs/2601.22650)
*Yen-Shiu Chin,Zhi-Yu Jou,Toshinari Morimoto,Chia-Tse Wang,Ming-Chung Chang,Tso-Jung Yen,Su-Yun Huang,Tailen Hsing*

Main category: stat.ML

TL;DR: 本文综述并比较了条件分布推断的多种代表性方法，包括经典非参数方法和现代生成模型，通过统一评估框架进行系统数值比较。


<details>
  <summary>Details</summary>
Motivation: 条件分布推断是统计学中的基本问题，对预测、不确定性量化和概率建模至关重要。已有多种方法被开发，但缺乏系统比较，需要全面评估不同方法的性能、灵活性和计算成本。

Method: 1. Hall和Yao(2005)的单指标方法：通过降维指标和非参数平滑估计条件分布；2. 基展开方法：包括FlexCode和DeepCDE，将条件密度估计转化为非参数回归问题；3. 生成模拟方法：包括生成条件分布采样器和条件去噪扩散概率模型；4. 使用统一评估框架进行系统数值比较，评估指标包括条件均值和标准差的均方误差以及Wasserstein距离。

Result: 通过系统比较揭示了不同方法的性能特征：单指标方法在降维场景下有效；基展开方法在回归框架下表现良好；生成模型方法在复杂分布建模方面有优势。同时分析了各方法的灵活性和计算成本，指出了各自的优缺点。

Conclusion: 不同条件分布推断方法各有优劣，适用于不同场景。经典方法在简单场景中计算效率高，而现代生成模型在处理复杂分布时更灵活。选择方法需权衡性能、灵活性和计算成本。统一评估框架为方法比较提供了公平基准。

Abstract: The inference of conditional distributions is a fundamental problem in statistics, essential for prediction, uncertainty quantification, and probabilistic modeling. A wide range of methodologies have been developed for this task. This article reviews and compares several representative approaches spanning classical nonparametric methods and modern generative models. We begin with the single-index method of Hall and Yao (2005), which estimates the conditional distribution through a dimension-reducing index and nonparametric smoothing of the resulting one-dimensional cumulative conditional distribution function. We then examine the basis-expansion approaches, including FlexCode (Izbicki and Lee, 2017) and DeepCDE (Dalmasso et al., 2020), which convert conditional density estimation into a set of nonparametric regression problems. In addition, we discuss two recent generative simulation-based methods that leverage modern deep generative architectures: the generative conditional distribution sampler (Zhou et al., 2023) and the conditional denoising diffusion probabilistic model (Fu et al., 2024; Yang et al., 2025). A systematic numerical comparison of these approaches is provided using a unified evaluation framework that ensures fairness and reproducibility. The performance metrics used for the estimated conditional distribution include the mean-squared errors of conditional mean and standard deviation, as well as the Wasserstein distance. We also discuss their flexibility and computational costs, highlighting the distinct advantages and limitations of each approach.

</details>


### [22] [Spectral Gradient Descent Mitigates Anisotropy-Driven Misalignment: A Case Study in Phase Retrieval](https://arxiv.org/abs/2601.22652)
*Guillaume Braun,Han Bao,Wei Huang,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 光谱梯度方法（如Muon优化器）通过保留方向信息而丢弃尺度信息来改进梯度更新，在深度学习中表现出色。本文通过非线性相位恢复模型分析其机制，发现在各向异性高斯输入下，梯度下降会放大高方差但无信息的尖峰方向，导致与真实信号对齐变差，而光谱梯度下降能消除这种放大效应，实现稳定对齐和加速噪声收缩。


<details>
  <summary>Details</summary>
Motivation: 光谱梯度方法（如Muon优化器）在深度学习中表现出强大的经验性能，但其增益机制尚不清楚。本文旨在通过理论分析揭示这些方法相对于传统梯度下降的优势机制，特别是在各向异性输入分布下的表现差异。

Method: 采用非线性相位恢复模型（相当于训练具有二次激活和固定第二层权重的两层神经网络），在尖峰协方差设置下进行分析，其中主导方差方向与信号正交。通过动力学分析比较梯度下降（GD）和光谱梯度下降（SpecGD）的行为，并进行数值实验验证理论。

Result: 梯度下降在早期逃逸阶段会乘法放大高方差但无信息的尖峰方向，导致与真实信号对齐变差（方差诱导的错位）。而光谱梯度下降能消除这种尖峰放大效应，实现稳定的对齐和加速的噪声收缩。数值实验证实了理论，并表明这些现象在更广泛的各向异性协方差下持续存在。

Conclusion: 光谱梯度方法通过消除梯度更新中的尺度信息，避免了各向异性输入分布下高方差方向的放大效应，从而改善了优化过程中的信号对齐和收敛性能。这解释了光谱梯度方法在深度学习中的经验优势。

Abstract: Spectral gradient methods, such as the Muon optimizer, modify gradient updates by preserving directional information while discarding scale, and have shown strong empirical performance in deep learning. We investigate the mechanisms underlying these gains through a dynamical analysis of a nonlinear phase retrieval model with anisotropic Gaussian inputs, equivalent to training a two-layer neural network with the quadratic activation and fixed second-layer weights. Focusing on a spiked covariance setting where the dominant variance direction is orthogonal to the signal, we show that gradient descent (GD) suffers from a variance-induced misalignment: during the early escaping stage, the high-variance but uninformative spike direction is multiplicatively amplified, degrading alignment with the true signal under strong anisotropy. In contrast, spectral gradient descent (SpecGD) removes this spike amplification effect, leading to stable alignment and accelerated noise contraction. Numerical experiments confirm the theory and show that these phenomena persist under broader anisotropic covariances.

</details>


### [23] [GRANITE: A Generalized Regional Framework for Identifying Agreement in Feature-Based Explanations](https://arxiv.org/abs/2601.22771)
*Julia Herbinger,Gabriel Laberge,Maximilian Muschalik,Yann Pequignot,Marvin N. Wright,Fabian Fumagalli*

Main category: stat.ML

TL;DR: GRANITE是一个广义的区域解释框架，通过将特征空间划分为交互和分布影响最小的区域来统一不同的特征解释方法，提高解释的一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基于特征的解释方法经常产生相互矛盾的解释，这种分歧主要源于两个来源：特征交互的处理方式和特征依赖关系的纳入方式。需要一种能够统一不同解释方法、提供一致解释的框架。

Method: 提出GRANITE框架，将特征空间划分为区域，在这些区域内最小化特征交互和分布影响。该框架统一了现有的区域方法，扩展到特征组，并引入了递归分区算法来估计这些区域。

Result: 在真实世界数据集上证明了GRANITE的有效性，能够对齐不同的解释方法，产生更一致和可解释的特征解释，提供了一个实用的工具。

Conclusion: GRANITE通过最小化特征交互和分布影响的分区方法，解决了不同解释方法之间的分歧问题，为特征解释提供了一致且可解释的框架，具有实际应用价值。

Abstract: Feature-based explanation methods aim to quantify how features influence the model's behavior, either locally or globally, but different methods often disagree, producing conflicting explanations. This disagreement arises primarily from two sources: how feature interactions are handled and how feature dependencies are incorporated. We propose GRANITE, a generalized regional explanation framework that partitions the feature space into regions where interaction and distribution influences are minimized. This approach aligns different explanation methods, yielding more consistent and interpretable explanations. GRANITE unifies existing regional approaches, extends them to feature groups, and introduces a recursive partitioning algorithm to estimate such regions. We demonstrate its effectiveness on real-world datasets, providing a practical tool for consistent and interpretable feature explanations.

</details>


### [24] [Approximating $f$-Divergences with Rank Statistics](https://arxiv.org/abs/2601.22784)
*Viktor Stein,José Manuel de Frutos*

Main category: stat.ML

TL;DR: 提出基于秩统计量的f-散度近似方法，通过秩直方图避免显式密度比估计，适用于高维数据的切片版本，具有理论保证和实证验证。


<details>
  <summary>Details</summary>
Motivation: 传统f-散度估计需要显式密度比估计，这在实践中具有挑战性。本文旨在开发一种避免密度比估计的f-散度近似方法，通过秩统计量直接测量分布间的差异。

Method: 将两个单变量分布μ和ν的差异映射到{0,...,K}上的秩直方图，通过离散f-散度测量其与均匀分布的偏差。对于高维数据，通过随机投影平均单变量构造定义切片秩统计量f-散度。

Result: 证明了估计量在K上是单调的，总是真实f-散度的下界，在温和的正则条件下建立了K→∞的收敛速率。推导了有限样本偏差界和渐近正态性结果。

Conclusion: 提出的秩统计量f-散度近似方法避免了密度比估计，具有理论保证，在生成建模实验中作为学习目标表现良好，优于神经基线方法。

Abstract: We introduce a rank-statistic approximation of $f$-divergences that avoids explicit density-ratio estimation by working directly with the distribution of ranks. For a resolution parameter $K$, we map the mismatch between two univariate distributions $μ$ and $ν$ to a rank histogram on $\{ 0, \ldots, K\}$ and measure its deviation from uniformity via a discrete $f$-divergence, yielding a rank-statistic divergence estimator. We prove that the resulting estimator of the divergence is monotone in $K$, is always a lower bound of the true $f$-divergence, and we establish quantitative convergence rates for $K\to\infty$ under mild regularity of the quantile-domain density ratio. To handle high-dimensional data, we define the sliced rank-statistic $f$-divergence by averaging the univariate construction over random projections, and we provide convergence results for the sliced limit as well. We also derive finite-sample deviation bounds along with asymptotic normality results for the estimator. Finally, we empirically validate the approach by benchmarking against neural baselines and illustrating its use as a learning objective in generative modelling experiments.

</details>


### [25] [OneFlowSBI: One Model, Many Queries for Simulation-Based Inference](https://arxiv.org/abs/2601.22951)
*Mayank Nautiyal,Li Ju,Melker Ernfors,Klara Hagland,Ville Holma,Maximilian Werkö Söderholm,Andreas Hellander,Prashant Singh*

Main category: stat.ML

TL;DR: OneFlowSBI是一个统一的仿真推断框架，通过单个流匹配生成模型学习参数和观测的联合分布，支持多种推断任务而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的仿真推断方法通常需要为不同任务训练专门模型，缺乏统一框架来同时处理后验采样、似然估计和任意条件分布等多种推断任务。

Method: 使用流匹配生成模型学习参数和观测的联合分布，在训练时采用查询感知的掩码分布，使同一模型支持多种推断任务。

Result: 在10个基准推断问题和2个高维真实世界逆问题上评估，OneFlowSBI在多种仿真预算下表现出与最先进方法竞争的性能，同时支持高效采样和噪声/部分观测数据的鲁棒性。

Conclusion: OneFlowSBI提供了一个统一的仿真推断框架，能够以单个模型支持多种推断任务，在保持性能的同时提高了效率和灵活性。

Abstract: We introduce \textit{OneFlowSBI}, a unified framework for simulation-based inference that learns a single flow-matching generative model over the joint distribution of parameters and observations. Leveraging a query-aware masking distribution during training, the same model supports multiple inference tasks, including posterior sampling, likelihood estimation, and arbitrary conditional distributions, without task-specific retraining. We evaluate \textit{OneFlowSBI} on ten benchmark inference problems and two high-dimensional real-world inverse problems across multiple simulation budgets. \textit{OneFlowSBI} is shown to deliver competitive performance against state-of-the-art generalized inference solvers and specialized posterior estimators, while enabling efficient sampling with few ODE integration steps and remaining robust under noisy and partially observed data.

</details>


### [26] [Neural Backward Filtering Forward Guiding](https://arxiv.org/abs/2601.23030)
*Gefan Yang,Frank van der Meulen,Stefan Sommer*

Main category: stat.ML

TL;DR: 提出NBFFG框架，用于树结构非线性连续随机过程的推理，通过辅助线性高斯过程构建变分后验，实现高效的无偏路径采样，在系统发育分析中成功重建蝴蝶翅膀形状。


<details>
  <summary>Details</summary>
Motivation: 树结构非线性连续随机过程的推理具有挑战性，特别是当观测稀疏（仅叶节点）且拓扑复杂时。精确平滑方法对于一般非线性动态难以处理，而基于粒子的方法在高维情况下性能下降。

Method: 提出神经后向滤波前向导引（NBFFG）框架，利用辅助线性高斯过程构建变分后验。该辅助过程产生闭式后向滤波器作为"引导"，将生成路径导向高似然区域。然后学习神经残差（参数化为归一化流或受控SDE）来捕捉非线性差异。该公式允许无偏路径子采样方案，将训练复杂度从依赖树大小降低到依赖路径长度。

Result: NBFFG在合成基准测试中优于基线方法，并在系统发育分析的高维推理任务中成功重建了蝴蝶祖先翅膀形状。

Conclusion: NBFFG为树结构非线性连续随机过程提供了一种有效的推理框架，通过结合辅助线性高斯过程和神经残差学习，实现了高效的无偏采样，在复杂拓扑和高维场景中表现出色。

Abstract: Inference in non-linear continuous stochastic processes on trees is challenging, particularly when observations are sparse (leaf-only) and the topology is complex. Exact smoothing via Doob's $h$-transform is intractable for general non-linear dynamics, while particle-based methods degrade in high dimensions. We propose Neural Backward Filtering Forward Guiding (NBFFG), a unified framework for both discrete transitions and continuous diffusions. Our method constructs a variational posterior by leveraging an auxiliary linear-Gaussian process. This auxiliary process yields a closed-form backward filter that serves as a ``guide'', steering the generative path toward high-likelihood regions. We then learn a neural residual--parameterized as a normalizing flow or a controlled SDE--to capture the non-linear discrepancies. This formulation allows for an unbiased path-wise subsampling scheme, reducing the training complexity from tree-size dependent to path-length dependent. Empirical results show that NBFFG outperforms baselines on synthetic benchmarks, and we demonstrate the method on a high-dimensional inference task in phylogenetic analysis with reconstruction of ancestral butterfly wing shapes.

</details>


### [27] [Asymptotic Theory of Iterated Empirical Risk Minimization, with Applications to Active Learning](https://arxiv.org/abs/2601.23031)
*Hugo Cui,Yue M. Lu*

Main category: stat.ML

TL;DR: 研究两阶段迭代经验风险最小化（ERM）方法，其中第一阶段预测作为第二阶段损失函数的输入，应用于主动学习和重加权场景，在高维高斯混合数据下推导了测试误差的渐近特性。


<details>
  <summary>Details</summary>
Motivation: 传统的单阶段ERM分析无法处理两阶段ERM中数据重用和预测依赖损失函数带来的复杂统计依赖问题，这在主动学习和重加权方案中自然出现，需要新的理论框架。

Method: 针对高斯混合数据上的线性模型，使用凸损失函数，在高维渐近框架下（样本量与维度成比例）推导两阶段ERM测试误差的精确渐近特性，应用于池式主动学习问题。

Result: 获得了第二阶段估计器性能的显式渐近预测，揭示了标注预算在阶段间分配的基本权衡，发现了纯粹由数据选择驱动的测试误差双下降现象。

Conclusion: 该理论为数据重用和预测依赖损失的两阶段ERM提供了严格分析框架，去除了先前工作中的oracle和样本分割假设，揭示了主动学习中标注预算分配的基本权衡和双下降现象。

Abstract: We study a class of iterated empirical risk minimization (ERM) procedures in which two successive ERMs are performed on the same dataset, and the predictions of the first estimator enter as an argument in the loss function of the second. This setting, which arises naturally in active learning and reweighting schemes, introduces intricate statistical dependencies across samples and fundamentally distinguishes the problem from classical single-stage ERM analyses. For linear models trained with a broad class of convex losses on Gaussian mixture data, we derive a sharp asymptotic characterization of the test error in the high-dimensional regime where the sample size and ambient dimension scale proportionally. Our results provide explicit, fully asymptotic predictions for the performance of the second-stage estimator despite the reuse of data and the presence of prediction-dependent losses. We apply this theory to revisit a well-studied pool-based active learning problem, removing oracle and sample-splitting assumptions made in prior work. We uncover a fundamental tradeoff in how the labeling budget should be allocated across stages, and demonstrate a double-descent behavior of the test error driven purely by data selection, rather than model size or sample count.

</details>


### [28] [A Random Matrix Theory of Masked Self-Supervised Regression](https://arxiv.org/abs/2601.23208)
*Arie Wortsman Zurich,Federica Gerace,Bruno Loureiro,Yue M. Lu*

Main category: stat.ML

TL;DR: 该论文对掩码自监督学习（SSL）进行高维分析，揭示了其联合预测器的谱结构，证明了在特定条件下掩码SSL优于PCA


<details>
  <summary>Details</summary>
Motivation: 在Transformer时代，掩码自监督学习已成为基础训练范式，但其联合矩阵值预测器的理论分析仍面临挑战，需要理解其如何从数据中提取结构

Method: 在样本数与维度成比例的渐进高维比例机制下，开发精确的掩码建模目标分析框架，推导泛化误差的显式表达式，并表征学习预测器的谱结构

Result: 对于尖峰协方差模型，发现联合预测器经历BBP型相变，确定了掩码SSL开始恢复潜在信号的条件；识别出掩码SSL可证明优于PCA的结构化机制

Conclusion: 掩码自监督学习通过聚合多个掩码模式的预测形成联合预测器，在特定条件下比传统无监督方法（如PCA）具有理论优势，揭示了SSL目标的内在机制

Abstract: In the era of transformer models, masked self-supervised learning (SSL) has become a foundational training paradigm. A defining feature of masked SSL is that training aggregates predictions across many masking patterns, giving rise to a joint, matrix-valued predictor rather than a single vector-valued estimator. This object encodes how coordinates condition on one another and poses new analytical challenges. We develop a precise high-dimensional analysis of masked modeling objectives in the proportional regime where the number of samples scales with the ambient dimension. Our results provide explicit expressions for the generalization error and characterize the spectral structure of the learned predictor, revealing how masked modeling extracts structure from data. For spiked covariance models, we show that the joint predictor undergoes a Baik--Ben Arous--Péché (BBP)-type phase transition, identifying when masked SSL begins to recover latent signals. Finally, we identify structured regimes in which masked self-supervised learning provably outperforms PCA, highlighting potential advantages of SSL objectives over classical unsupervised methods

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [29] [A Time-Varying Branching Process Approach to Model Self-Renewing Cells](https://arxiv.org/abs/2601.22282)
*Huyen Nguyen,Haim Bar,Zhiyi Chi,Vladimir Pozdnyakov*

Main category: stat.AP

TL;DR: 本文开发了一个具有时间依赖后代分布的连续时间分支过程模型来表征干细胞增殖过程，推导了干细胞计数的均值、方差和自协方差的解析表达式，并开发了基于似然的推断程序来估计模型参数。


<details>
  <summary>Details</summary>
Motivation: 理解细胞群体在增殖过程中的动态不仅揭示了干细胞的增殖特性，还为正常条件和病理破坏下的组织发育提供了见解。干细胞通过产生子代干细胞和分化为特化细胞，在生物组织的生长、维持和修复中至关重要。

Method: 开发了一个具有时间依赖后代分布的连续时间分支过程模型来表征干细胞增殖过程。推导了干细胞计数的均值、方差和自协方差的解析表达式，并开发了基于似然的推断程序来估计模型参数。特别地，构建了前向算法似然来处理某些细胞类型无法直接观察的情况。

Result: 模拟结果表明，估计方法能够以良好的准确性恢复时间依赖的分裂概率。

Conclusion: 该模型和分析方法为研究干细胞增殖动态提供了有效的统计框架，特别是在存在部分观测限制的情况下，能够准确估计时间依赖的细胞分裂参数。

Abstract: Stem cells, through their ability to produce daughter stem cells and differentiate into specialized cells, are essential in the growth, maintenance, and repair of biological tissues. Understanding the dynamics of cell populations in the proliferation process not only uncovers proliferative properties of stem cells, but also offers insight into tissue development under both normal conditions and pathological disruption. In this paper, we develop a continuous time branching process model with time-dependent offspring distribution to characterize stem cell proliferation process. We derive analytical expressions for mean, variance, and autocovariance of the stem cell counts, and develop likelihood-based inference procedures to estimate model parameters. Particularly, we construct a forward algorithm likelihood to handle situations when some cell types cannot be directly observed. Simulation results demonstrate that our estimation method recovers the time-dependent division probabilities with good accuracy.

</details>


### [30] [Beyond the Null Effect: Unmasking the True Impact of Teacher-Child Interaction Quality on Child Outcomes in Early Head Start](https://arxiv.org/abs/2601.23203)
*JoonHo Lee,Alison Hooper*

Main category: stat.AP

TL;DR: 研究通过改进方法学（GALAMM模型、协变量平衡权重等）发现，早期开端计划中师生互动质量对儿童发展有显著影响，纠正了以往大规模研究中常见的"零效应"结论。


<details>
  <summary>Details</summary>
Motivation: 早期开端计划中师生互动被认为影响婴幼儿发展，但大规模研究常发现微弱或零关联。本研究旨在解决四个方法学问题（项目级测量误差、中心级混杂、教师/课堂级协变量不平衡、忽略非线性）以澄清课堂过程质量的真实影响。

Method: 使用2018年Baby FACES数据，应用三级广义加性潜变量混合模型（GALAMM）区分课堂级过程质量变异性与项目级噪声和中心级效应；使用CLASS和QCIT测量；采用协变量平衡权重和广义加性模型估计剂量-反应关系。

Result: 近一半项目方差反映课堂级过程，其余为测量误差或中心影响；纠正偏差后显示：认知/语言支持与英语沟通技能呈稳健线性关联，情感行为支持更好地预测社会情感能力；某些领域在极端情况下出现平台期，显示非线性关系。

Conclusion: 研究挑战了"零效应"叙事，证明严谨方法能揭示师生互动质量的关键领域特异性影响，为早期开端计划的针对性专业发展和政策提供更清晰指导。

Abstract: In Early Head Start (EHS), teacher-child interactions are widely believed to shape infant-toddler outcomes, yet large-scale studies often find only modest or null associations. This study addresses four methodological sources of attenuation -- item-level measurement error, center-level confounding, teacher- and classroom-level covariate imbalance, and overlooked nonlinearities -- to clarify classroom process quality's true influence on child development. Using data from the 2018 wave of the Early Head Start Family and Child Experiences Survey (Baby FACES), we applied a three-level generalized additive latent and mixed model (GALAMM) to distinguish genuine classroom-level variability in process quality, as measured by the Classroom Assessment Scoring System (CLASS) and Quality of Caregiver-Child Interactions for Infants and Toddlers (QCIT), from item-level noise and center-level effects. We then estimated dose-response relationships with children's language and socioemotional outcomes, employing covariate balancing weights and generalized additive models. Results show that nearly half of each item's variance reflects classroom-level processes, with the remainder tied to measurement error or center-wide influences, masking true classroom effects. After correcting for these biases, domain-focused dose-response analyses reveal robust linear associations between cognitive/language supports and children's English communicative skills, while emotional-behavioral supports better predict social-emotional competence. Some domains display plateaus when pushed to extremes, underscoring potential nonlinearities. These findings challenge the "null effect" narrative, demonstrating that rigorous methodology can uncover the critical, domain-specific impacts of teacher-child interaction quality, offering clearer guidance for targeted professional development and policy in EHS.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [31] [A Framework for the Bayesian Calibration of Complex and Data-Scarce Models in Applied Sciences](https://arxiv.org/abs/2601.22890)
*Christina Schenk,Ignacio Romero*

Main category: stat.CO

TL;DR: 本文综述了复杂计算机模型的贝叶斯校准理论，特别关注计算昂贵模拟和稀缺实验数据的应用，提出了统一框架并开发了开源Python库ACBICI。


<details>
  <summary>Details</summary>
Motivation: 工程和相关领域中，复杂计算机模型校准面临计算昂贵、实验数据稀缺的挑战，现有贝叶斯校准方法缺乏统一框架和易用工具，需要系统性的理论和实践指导。

Method: 提出统一的贝叶斯校准框架，整合多种现有方法，开发开源Python库ACBICI，采用面向对象设计支持单输出和多输出校准，并提供实用校准指南。

Result: 建立了包含多种贝叶斯校准方法的统一理论框架，实现了易于使用和扩展的开源软件工具，为工程领域的计算模型校准提供了系统解决方案。

Conclusion: 本文提供了贝叶斯校准的全面理论综述和实用工具，通过统一框架和开源软件支持工程领域复杂计算模型的可靠校准，填补了现有理论和实践指导的空白。

Abstract: In this work, we review the theory involved in the Bayesian calibration of complex computer models, with particular emphasis on their use for applications involving computationally expensive simulations and scarce experimental data. In the article, we present a unified framework that incorporates various Bayesian calibration methods, including well-established approaches. Furthermore, we describe their implementation and use with a new, open-source Python library, ACBICI (A Configurable BayesIan Calibration and Inference Package). All algorithms are implemented with an object-oriented structure designed to be both easy to use and readily extensible. In particular, single-output and multiple-output calibration are addressed in a consistent manner. The article completes the theory and its implementation with practical recommendations for calibrating the problems of interest. These guidelines -- currently unavailable in a unified form elsewhere -- together with the open-source Python library, are intended to support the reliable calibration of computational codes and models commonly used in engineering and related fields. Overall, this work aims to serve both as a comprehensive review of the statistical foundations and (computational) tools required to perform such calculations, and as a practical guide to Bayesian calibration with modern software tools.

</details>


### [32] [Wasserstein Geometry of Information Loss in Nonlinear Dynamical Systems](https://arxiv.org/abs/2601.22814)
*Yiting Duan,Zhikun Zhang,Yi Guo*

Main category: stat.CO

TL;DR: 时间延迟嵌入并非总是嵌入，非单射性会导致不可约信息损失，作者提出内在随机性指标来量化这种损失并提升重建质量


<details>
  <summary>Details</summary>
Motivation: 时间延迟嵌入技术基于Takens嵌入定理，但实践中很少验证其是否真正构成嵌入。作者发现时间延迟重建并不总是嵌入，非单射性会导致不可约信息损失，影响下游模型性能

Method: 建立测度理论框架，将动力学提升到概率测度空间，量化非单射性引起的多值演化，引入内在随机性指标作为确定性闭包的几乎处处数据驱动证书

Result: 内在随机性指标能够提高重建质量和下游模型性能，在合成和真实世界的非线性数据集上都得到了验证

Conclusion: 时间延迟重建的非单射性会导致不可约信息损失，提出的内在随机性指标能够有效量化这种损失并改善重建效果，为非线性时间序列分析提供了新的理论框架

Abstract: Time-delay embedding is a powerful technique for reconstructing the state space of nonlinear time series. However, the fidelity of reconstruction relies on the assumption that the time-delay map is an embedding, which is implicitly justified by Takens' embedding theorem but rarely scrutinised in practice. In this work, we argue that time-delay reconstruction is not always an embedding, and that the non-injectivity of the time-delay map induced by a given measurement function causes irreducible information loss, degrading downstream model performance. Our analysis reveals that this local self-overlap stems from inherent dynamical properties, governed by the competition between the dynamical and the curvature penalty, and the irreducible information loss scales with the product of the geometric separation and the probability mass. We establish a measure-theoretic framework that lifts the dynamics to the space of probability measures, where the multi-valued evolution induced by the non-injectivity is quantified by how far the $n$-step conditional kernel $K^{n}(x, \cdot)$ deviates from a Dirac mass and introduce intrinsic stochasticity $\mathcal{E}^{*}_{n}$, an almost-everywhere, data-driven certificate of deterministic closure, to quantify irreducible information loss without any prior information. We demonstrate that $\mathcal{E}^{*}_{n}$ improves reconstruction quality and downstream model performance on both synthetic and real-world nonlinear data sets.

</details>


### [33] [A categorical account of the Metropolis-Hastings algorithm](https://arxiv.org/abs/2601.22911)
*Rob Cornish,Andi Q. Wang*

Main category: stat.CO

TL;DR: 该论文将Metropolis-Hastings算法置于范畴概率框架中，使用CD范畴及其对交换幺半群的丰富化来形式化分析MCMC概念，并给出MH型采样器可逆性的合成条件。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在范畴概率框架中形式化并分析Metropolis-Hastings算法，利用最近提出的对合框架作为具体案例研究，将MCMC的基本概念如不变性和可逆性置于更抽象的数学结构中。

Method: 首先在马尔可夫范畴中形式化MCMC基本概念，然后使用标准CD范畴分析MH核的一部分。进一步研究CD范畴对交换幺半群的丰富化，为概率概念提供表达性框架，包括子随机核、有限和σ有限测度、绝对连续性、奇异测度和Lebesgue分解。

Result: 建立了范畴概率框架下MCMC概念的形式化表达，给出了MH型采样器相对于给定目标分布可逆性的合成必要和充分条件。

Conclusion: 成功将Metropolis-Hastings算法置于范畴概率框架中分析，展示了范畴方法为MCMC理论提供统一抽象框架的潜力，特别是通过CD范畴的丰富化能够表达重要的概率概念并推导可逆性条件。

Abstract: Metropolis-Hastings (MH) is a foundational Markov chain Monte Carlo (MCMC) algorithm. In this paper, we ask whether it is possible to formulate and analyse MH in terms of categorical probability, using a recent involutive framework for MH-type procedures as a concrete case study. We show how basic MCMC concepts such as invariance and reversibility can be formulated in Markov categories, and how one part of the MH kernel can be analysed using standard CD categories. To go further, we then study enrichments of CD categories over commutative monoids. This gives an expressive setting for reasoning abstractly about a range of important probabilistic concepts, including substochastic kernels, finite and $σ$-finite measures, absolute continuity, singular measures, and Lebesgue decompositions. Using these tools, we give synthetic necessary and sufficient conditions for a general MH-type sampler to be reversible with respect to a given target distribution.

</details>


### [34] [Nested Slice Sampling: Vectorized Nested Sampling for GPU-Accelerated Inference](https://arxiv.org/abs/2601.23252)
*David Yallup,Namu Kroupa,Will Handley*

Main category: stat.CO

TL;DR: 提出Nested Slice Sampling (NSS)，一种GPU友好的向量化嵌套采样方法，使用Hit-and-Run切片采样进行约束更新，在高维和多模态问题上表现优异


<details>
  <summary>Details</summary>
Motivation: 复杂多模态目标的可扩展推理具有挑战性，传统嵌套采样通常顺序执行且硬约束使得GPU加速实现困难

Method: 引入Nested Slice Sampling (NSS)，使用Hit-and-Run Slice Sampling进行约束更新，通过调优分析得到简单的近最优切片宽度设置规则

Result: 在合成目标、高维贝叶斯推理和高斯过程超参数边缘化实验中，NSS保持准确的证据估计和高质量后验样本，在多模态问题上优于当前SOTA方法

Conclusion: NSS是一种GPU友好的向量化嵌套采样方法，在多模态和高维问题上表现稳健，已发布开源实现

Abstract: Model comparison and calibrated uncertainty quantification often require integrating over parameters, but scalable inference can be challenging for complex, multimodal targets. Nested Sampling is a robust alternative to standard MCMC, yet its typically sequential structure and hard constraints make efficient accelerator implementations difficult. This paper introduces Nested Slice Sampling (NSS), a GPU-friendly, vectorized formulation of Nested Sampling that uses Hit-and-Run Slice Sampling for constrained updates. A tuning analysis yields a simple near-optimal rule for setting the slice width, improving high-dimensional behavior and making per-step compute more predictable for parallel execution. Experiments on challenging synthetic targets, high dimensional Bayesian inference, and Gaussian process hyperparameter marginalization show that NSS maintains accurate evidence estimates and high-quality posterior samples, and is particularly robust on difficult multimodal problems where current state-of-the-art methods such as tempered SMC baselines can struggle. An open-source implementation is released to facilitate adoption and reproducibility.

</details>
