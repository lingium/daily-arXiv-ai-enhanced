{"id": "2601.05297", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05297", "abs": "https://arxiv.org/abs/2601.05297", "authors": ["Rohan Vitthal Thorat", "Rajdip Nayek"], "title": "Machine learning assisted state prediction of misspecified linear dynamical system via modal reduction", "comment": null, "summary": "Accurate prediction of structural dynamics is imperative for preserving digital twin fidelity throughout operational lifetimes. Parametric models with fixed nominal parameters often omit critical physical effects due to simplifications in geometry, material behavior, damping, or boundary conditions, resulting in model form errors (MFEs) that impair predictive accuracy. This work introduces a comprehensive framework for MFE estimation and correction in high-dimensional finite element (FE) based structural dynamical systems. The Gaussian Process Latent Force Model (GPLFM) represents discrepancies non-parametrically in the reduced modal domain, allowing a flexible data-driven characterization of unmodeled dynamics. A linear Bayesian filtering approach jointly estimates system states and discrepancies, incorporating epistemic and aleatoric uncertainties. To ensure computational tractability, the FE system is projected onto a reduced modal basis, and a mesh-invariant neural network maps modal states to discrepancy estimates, permitting model rectification across different FE discretizations without retraining. Validation is undertaken across five MFE scenarios-including incorrect beam theory, damping misspecification, misspecified boundary condition, unmodeled material nonlinearity, and local damage demonstrating the surrogate model's substantial reduction of displacement and rotation prediction errors under unseen excitations. The proposed methodology offers a potential means to uphold digital twin accuracy amid inherent modeling uncertainties."}
{"id": "2601.05355", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05355", "abs": "https://arxiv.org/abs/2601.05355", "authors": ["Qiao Liu", "Wing Hung Wong"], "title": "A Bayesian Generative Modeling Approach for Arbitrary Conditional Inference", "comment": null, "summary": "Modern data analysis increasingly requires flexible conditional inference P(X_B | X_A) where (X_A, X_B) is an arbitrary partition of observed variable X. Existing conditional inference methods lack this flexibility as they are tied to a fixed conditioning structure and cannot perform new conditional inference once trained. To solve this, we propose a Bayesian generative modeling (BGM) approach for arbitrary conditional inference without retraining. BGM learns a generative model of X through an iterative Bayesian updating algorithm where model parameters and latent variables are updated until convergence. Once trained, any conditional distribution can be obtained without retraining. Empirically, BGM achieves superior prediction performance with well calibrated predictive intervals, demonstrating that a single learned model can serve as a universal engine for conditional prediction with uncertainty quantification. We provide theoretical guarantees for the convergence of the stochastic iterative algorithm, statistical consistency and conditional-risk bounds. The proposed BGM framework leverages the power of AI to capture complex relationships among variables while adhering to Bayesian principles, emerging as a promising framework for advancing various applications in modern data science. The code for BGM is freely available at https://github.com/liuq-lab/bayesgm."}
{"id": "2601.05441", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.05441", "abs": "https://arxiv.org/abs/2601.05441", "authors": ["Getachew K. Befekadu"], "title": "A brief note on learning problem with global perspectives", "comment": "7 Pages with 1 Figure", "summary": "This brief note considers the problem of learning with dynamic-optimizing principal-agent setting, in which the agents are allowed to have global perspectives about the learning process, i.e., the ability to view things according to their relative importances or in their true relations based-on some aggregated information shared by the principal. Whereas, the principal, which is exerting an influence on the learning process of the agents in the aggregation, is primarily tasked to solve a high-level optimization problem posed as an empirical-likelihood estimator under conditional moment restrictions model that also accounts information about the agents' predictive performances on out-of-samples as well as a set of private datasets available only to the principal. In particular, we present a coherent mathematical argument which is necessary for characterizing the learning process behind this abstract principal-agent learning framework, although we acknowledge that there are a few conceptual and theoretical issues still need to be addressed."}
{"id": "2601.05910", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.05910", "abs": "https://arxiv.org/abs/2601.05910", "authors": ["Yigitcan Comlek", "R. Murali Krishnan", "Sandipp Krishnan Ravi", "Amin Moghaddas", "Rafael Giorjao", "Michael Eff", "Anirban Samaddar", "Nesar S. Ramachandra", "Sandeep Madireddy", "Liping Wang"], "title": "Multi-task Modeling for Engineering Applications with Sparse Data", "comment": "15 pages, 5 figures, 6 tables", "summary": "Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of data sparsity and varying task correlations. The proposed framework leverages inter-task relationships across outputs and fidelity levels to improve predictive performance and reduce computational costs. The framework is validated across three representative scenarios: Forrester function benchmark, 3D ellipsoidal void modeling, and friction-stir welding. By quantifying and leveraging inter-task relationships, the proposed MTGP framework offers a robust and scalable solution for predictive modeling in domains with significant computational and experimental costs, supporting informed decision-making and efficient resource utilization."}
{"id": "2601.05400", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05400", "abs": "https://arxiv.org/abs/2601.05400", "authors": ["Aleix Alcacer", "Irene Epifanio"], "title": "Representing asymmetric relationships by h-plots. Discovering the archetypal patterns of cross-journal citation relationships", "comment": null, "summary": "This work approaches the multidimensional scaling problem from a novel angle. We introduce a scalable method based on the h-plot, which inherently accommodates asymmetric proximity data. Instead of embedding the objects themselves, the method embeds the variables that define the proximity to or from each object. It is straightforward to implement, and the quality of the resulting representation can be easily evaluated. The methodology is illustrated by visualizing the asymmetric relationships between the citing and cited profiles of journals on a common map. Two profiles that are far apart (or close together) in the h-plot, as measured by Euclidean distance, are different (or similar), respectively. This representation allows archetypoid analysis (ADA) to be calculated. ADA is used to find archetypal journals (or extreme cases). We can represent the dataset as convex combinations of these archetypal journals, making the results easy to interpret, even for non-experts. Comparisons with other methodologies are carried out, showing the good performance of our proposal. Code and data are available for reproducibility."}
{"id": "2601.05345", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.05345", "abs": "https://arxiv.org/abs/2601.05345", "authors": ["Sphiwe B. Skhosana", "Najmeh Nakhaei Rad"], "title": "Model-based clustering using a new mixture of circular regressions", "comment": null, "summary": "Regression models, where the response variable is circular, are common in areas such as biology, geology and meteorology. A typical model assumes that the conditional distribution of the response follows a von-Mises distribution. However, this assumption is inadequate when the response variable is multimodal. For this reason, in this paper, a finite mixture of regressions model is proposed for the case of a circular response variable and a set of circular and/or linear covariates. Mixture models are very useful when the underlying population is multimodal. Despite the prevalence of multimodality in regression modelling of circular data, the use of mixtures of regressions has received no attention in the literature. This paper aims to close this knowledge gap. To estimate the proposed model, we develop a maximum likelihood estimation procedure via the Expectation-Maximization algorithm. An extensive simulation study is used to demonstrate the practical use and performance of the proposed model and estimation procedure. In addition, the model is shown to be useful as a model-based clustering tool. Lastly, the model is applied to a real dataset from a wind farm in South Africa."}
{"id": "2601.05345", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.05345", "abs": "https://arxiv.org/abs/2601.05345", "authors": ["Sphiwe B. Skhosana", "Najmeh Nakhaei Rad"], "title": "Model-based clustering using a new mixture of circular regressions", "comment": null, "summary": "Regression models, where the response variable is circular, are common in areas such as biology, geology and meteorology. A typical model assumes that the conditional distribution of the response follows a von-Mises distribution. However, this assumption is inadequate when the response variable is multimodal. For this reason, in this paper, a finite mixture of regressions model is proposed for the case of a circular response variable and a set of circular and/or linear covariates. Mixture models are very useful when the underlying population is multimodal. Despite the prevalence of multimodality in regression modelling of circular data, the use of mixtures of regressions has received no attention in the literature. This paper aims to close this knowledge gap. To estimate the proposed model, we develop a maximum likelihood estimation procedure via the Expectation-Maximization algorithm. An extensive simulation study is used to demonstrate the practical use and performance of the proposed model and estimation procedure. In addition, the model is shown to be useful as a model-based clustering tool. Lastly, the model is applied to a real dataset from a wind farm in South Africa."}
{"id": "2601.06009", "categories": ["stat.ML", "cs.LG", "eess.SP", "math.PR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.06009", "abs": "https://arxiv.org/abs/2601.06009", "authors": ["Sunia Tanweer", "Firas A. Khasawneh"], "title": "Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem", "comment": null, "summary": "We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\\varepsilon$ of excursions of magnitude at least $\\varepsilon$ with the quadratic variation $[X]_T$ of the process. The scaling law holds universally for all continuous semimartingales with finite quadratic variation, including general Ito diffusions with nonlinear or state-dependent volatility, but fails sharply for deterministic systems -- thereby providing a theoretically-certfied method of distinguishing between these dynamics, as opposed to the subjective entropy or recurrence based state of the art methods. We construct a robust data-driven diffusion test. The method compares the empirical excursion counts against the theoretical expectation. The resulting ratio $K(\\varepsilon)=N_{\\varepsilon}^{\\mathrm{emp}}/N_{\\varepsilon}^{\\mathrm{theory}}$ is then summarized by a log-log slope deviation measuring the $\\varepsilon^{-2}$ law that provides a classification into diffusion-like or not. We demonstrate the method on canonical stochastic systems, some periodic and chaotic maps and systems with additive white noise, as well as the stochastic Duffing system. The approach is nonparametric, model-free, and relies only on the universal small-scale structure of continuous semimartingales."}
{"id": "2601.05842", "categories": ["stat.AP", "q-bio.QM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05842", "abs": "https://arxiv.org/abs/2601.05842", "authors": ["Jonathan F. Kunst", "Killian A. C. Melsen", "Willem Kruijer", "José Crossa", "Chris Maliepaard", "Fred A. van Eeuwijk", "Carel F. W. Peeters"], "title": "A latent factor approach to hyperspectral time series data for multivariate genomic prediction of grain yield in wheat", "comment": "20 pages, 8 figures", "summary": "High-dimensional time series phenotypic data is becoming increasingly common within plant breeding programmes. However, analysing and integrating such data for genetic analysis and genomic prediction remains difficult. Here we show how factor analysis with Procrustes rotation on the genetic correlation matrix of hyperspectral secondary phenotype data can help in extracting relevant features for within-trial prediction. We use a subset of Centro Internacional de Mejoramiento de Maíz y Trigo (CIMMYT) elite yield wheat trial of 2014-2015, consisting of 1,033 genotypes. These were measured across three irrigation treatments at several timepoints during the season, using manned airplane flights with hyperspectral sensors capturing 62 bands in the spectrum of 385-850 nm. We perform multivariate genomic prediction using latent variables to improve within-trial genomic predictive ability (PA) of wheat grain yield within three distinct watering treatments. By integrating latent variables of the hyperspectral data in a multivariate genomic prediction model, we are able to achieve an absolute gain of .1 to .3 (on the correlation scale) in PA compared to univariate genomic prediction. Furthermore, we show which timepoints within a trial are important and how these relate to plant growth stages. This paper showcases how domain knowledge and data-driven approaches can be combined to increase PA and gain new insights from sensor data of high-throughput phenotyping platforms."}
{"id": "2601.05392", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05392", "abs": "https://arxiv.org/abs/2601.05392", "authors": ["Aleix Alcacer", "Irene Epifanio"], "title": "Archetypal cases for questionnaires with nominal multiple choice questions", "comment": "Statistical Methods for Data Analysis and Decision Sciences. Third Conference of the Statistics and Data Science Group of the Italian Statistical Society. Milan, April 2-3, 2025", "summary": "Archetypal analysis serves as an exploratory tool that interprets a collection of observations as convex combinations of pure (extreme) patterns. When these patterns correspond to actual observations within the sample, they are termed archetypoids. For the first time, we propose applying archetypoid analysis to nominal observations, specifically for identifying archetypal cases from questionnaires featuring nominal multiple-choice questions with a single possible answer. This approach can enhance our understanding of a nominal data set, similar to its application in multivariate contexts. We compare this methodology with the use of archetype analysis and probabilistic archetypal analysis and demonstrate the benefits of this methodology using a real-world example: the German credit dataset."}
{"id": "2601.05355", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05355", "abs": "https://arxiv.org/abs/2601.05355", "authors": ["Qiao Liu", "Wing Hung Wong"], "title": "A Bayesian Generative Modeling Approach for Arbitrary Conditional Inference", "comment": null, "summary": "Modern data analysis increasingly requires flexible conditional inference P(X_B | X_A) where (X_A, X_B) is an arbitrary partition of observed variable X. Existing conditional inference methods lack this flexibility as they are tied to a fixed conditioning structure and cannot perform new conditional inference once trained. To solve this, we propose a Bayesian generative modeling (BGM) approach for arbitrary conditional inference without retraining. BGM learns a generative model of X through an iterative Bayesian updating algorithm where model parameters and latent variables are updated until convergence. Once trained, any conditional distribution can be obtained without retraining. Empirically, BGM achieves superior prediction performance with well calibrated predictive intervals, demonstrating that a single learned model can serve as a universal engine for conditional prediction with uncertainty quantification. We provide theoretical guarantees for the convergence of the stochastic iterative algorithm, statistical consistency and conditional-risk bounds. The proposed BGM framework leverages the power of AI to capture complex relationships among variables while adhering to Bayesian principles, emerging as a promising framework for advancing various applications in modern data science. The code for BGM is freely available at https://github.com/liuq-lab/bayesgm."}
{"id": "2601.06025", "categories": ["stat.ML", "cs.LG", "math.FA", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.06025", "abs": "https://arxiv.org/abs/2601.06025", "authors": ["Johanna Tengler", "Christoph Brune", "José A. Iglesias"], "title": "Manifold limit for the training of shallow graph convolutional neural networks", "comment": "44 pages, 0 figures, 1 table", "summary": "We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a manifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose low-frequency spectrum approximates that of the Laplace-Beltrami operator of the underlying smooth manifold, and shallow GCNNs of possibly infinite width are linear functionals on the space of measures on the parameter space. From this functional-analytic perspective, graph signals are seen as spatial discretizations of functions on the manifold, which leads to a natural notion of training data consistent across graph resolutions. To enable convergence results, the continuum parameter space is chosen as a weakly compact product of unit balls, with Sobolev regularity imposed on the output weight and bias, but not on the convolutional parameter. The corresponding discrete parameter spaces inherit the corresponding spectral decay, and are additionally restricted by a frequency cutoff adapted to the informative spectral window of the graph Laplacians. Under these assumptions, we prove $Γ$-convergence of regularized empirical risk minimization functionals and corresponding convergence of their global minimizers, in the sense of weak convergence of the parameter measures and uniform convergence of the functions over compact sets. This provides a formalization of mesh and sample independence for the training of such networks."}
{"id": "2601.05859", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.05859", "abs": "https://arxiv.org/abs/2601.05859", "authors": ["Joseph Marsh", "Nathan A. Judd", "Lax Chan", "Rowland G. Seymour"], "title": "Neural Methods for Multiple Systems Estimation Models", "comment": "28 pages, 15 figures, 3 tables. Includes supplementary material. Code available at https://github.com/HiddenHarmsHub/npe-mse-censored-missing", "summary": "Estimating the size of hidden populations using Multiple Systems Estimation (MSE) is a critical task in quantitative sociology; however, practical application is often hindered by imperfect administrative data and computational constraints. Real-world datasets frequently suffer from censoring and missingness due to privacy concerns, while standard inference methods, such as Maximum Likelihood Estimation (MLE) and Markov chain Monte Carlo (MCMC), can become computationally intractable or fail to converge when data are sparse. To address these limitations, we propose a novel simulation-based Bayesian inference framework utilizing Neural Bayes Estimators (NBE) and Neural Posterior Estimators (NPE). These neural methods are amortized: once trained, they provide instantaneous, computationally efficient posterior estimates, making them ideal for use in secure research environments where computational resources are limited. Through extensive simulation studies, we demonstrate that neural estimators achieve accuracy comparable to MCMC while being orders of magnitude faster and robust to the convergence failures that plague traditional samplers in sparse settings. We demonstrate our method on two real-world cases estimating the prevalence of modern slavery in the UK and female drug use in North East England."}
{"id": "2601.05396", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.05396", "abs": "https://arxiv.org/abs/2601.05396", "authors": ["Yezhuo Li", "Fan Zhang", "Dhanashree Shinde", "Qiong Zhang", "Sai Pradeep", "Srikanth Pilla", "Gang Li"], "title": "Uncertainty Analysis of Experimental Parameters for Reducing Warpage in Injection Molding", "comment": null, "summary": "Injection molding is a critical manufacturing process, but controlling warpage remains a major challenge due to complex thermomechanical interactions. Simulation-based optimization is widely used to address this, yet traditional methods often overlook the uncertainty in model parameters. In this paper, we propose a data-driven framework to minimize warpage and quantify the uncertainty of optimal process settings. We employ polynomial regression models as surrogates for the injection molding simulations of a box-shaped part. By adopting a Bayesian framework, we estimate the posterior distribution of the regression coefficients. This approach allows us to generate a distribution of optimal decisions rather than a single point estimate, providing a measure of solution robustness. Furthermore, we develop a Monte Carlo-based boundary analysis method. This method constructs confidence bands for the zero-level sets of the response surfaces, helping to visualize the regions where warpage transitions between convex and concave profiles. We apply this framework to optimize four key process parameters: mold temperature, injection speed, packing pressure, and packing time. The results show that our approach finds stable process settings and clearly marks the boundaries of defects in the parameter space."}
{"id": "2601.05859", "categories": ["stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.05859", "abs": "https://arxiv.org/abs/2601.05859", "authors": ["Joseph Marsh", "Nathan A. Judd", "Lax Chan", "Rowland G. Seymour"], "title": "Neural Methods for Multiple Systems Estimation Models", "comment": "28 pages, 15 figures, 3 tables. Includes supplementary material. Code available at https://github.com/HiddenHarmsHub/npe-mse-censored-missing", "summary": "Estimating the size of hidden populations using Multiple Systems Estimation (MSE) is a critical task in quantitative sociology; however, practical application is often hindered by imperfect administrative data and computational constraints. Real-world datasets frequently suffer from censoring and missingness due to privacy concerns, while standard inference methods, such as Maximum Likelihood Estimation (MLE) and Markov chain Monte Carlo (MCMC), can become computationally intractable or fail to converge when data are sparse. To address these limitations, we propose a novel simulation-based Bayesian inference framework utilizing Neural Bayes Estimators (NBE) and Neural Posterior Estimators (NPE). These neural methods are amortized: once trained, they provide instantaneous, computationally efficient posterior estimates, making them ideal for use in secure research environments where computational resources are limited. Through extensive simulation studies, we demonstrate that neural estimators achieve accuracy comparable to MCMC while being orders of magnitude faster and robust to the convergence failures that plague traditional samplers in sparse settings. We demonstrate our method on two real-world cases estimating the prevalence of modern slavery in the UK and female drug use in North East England."}
{"id": "2601.05415", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.05415", "abs": "https://arxiv.org/abs/2601.05415", "authors": ["Yuchao Wang", "Tianying Wang"], "title": "Multi-Group Quadratic Discriminant Analysis via Projection", "comment": null, "summary": "Multi-group classification arises in many prediction and decision-making problems, including applications in epidemiology, genomics, finance, and image recognition. Although classification methods have advanced considerably, much of the literature focuses on binary problems, and available extensions often provide limited flexibility for multi-group settings. Recent work has extended linear discriminant analysis to multiple groups, but more general methods are still needed to handle complex structures such as nonlinear decision boundaries and group-specific covariance patterns.\n  We develop Multi-Group Quadratic Discriminant Analysis (MGQDA), a method for multi-group classification built on quadratic discriminant analysis. MGQDA projects high-dimensional predictors onto a lower-dimensional subspace, which enables accurate classification while capturing nonlinearity and heterogeneity in group-specific covariance structures. We derive theoretical guarantees, including variable selection consistency, to support the reliability of the procedure. In simulations and a gene-expression application, MGQDA achieves competitive or improved predictive performance compared with existing methods while selecting group-specific informative variables, indicating its practical value for high-dimensional multi-group classification problems. Supplementary materials for this article are available online."}
{"id": "2601.05396", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.05396", "abs": "https://arxiv.org/abs/2601.05396", "authors": ["Yezhuo Li", "Fan Zhang", "Dhanashree Shinde", "Qiong Zhang", "Sai Pradeep", "Srikanth Pilla", "Gang Li"], "title": "Uncertainty Analysis of Experimental Parameters for Reducing Warpage in Injection Molding", "comment": null, "summary": "Injection molding is a critical manufacturing process, but controlling warpage remains a major challenge due to complex thermomechanical interactions. Simulation-based optimization is widely used to address this, yet traditional methods often overlook the uncertainty in model parameters. In this paper, we propose a data-driven framework to minimize warpage and quantify the uncertainty of optimal process settings. We employ polynomial regression models as surrogates for the injection molding simulations of a box-shaped part. By adopting a Bayesian framework, we estimate the posterior distribution of the regression coefficients. This approach allows us to generate a distribution of optimal decisions rather than a single point estimate, providing a measure of solution robustness. Furthermore, we develop a Monte Carlo-based boundary analysis method. This method constructs confidence bands for the zero-level sets of the response surfaces, helping to visualize the regions where warpage transitions between convex and concave profiles. We apply this framework to optimize four key process parameters: mold temperature, injection speed, packing pressure, and packing time. The results show that our approach finds stable process settings and clearly marks the boundaries of defects in the parameter space."}
{"id": "2601.05415", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.05415", "abs": "https://arxiv.org/abs/2601.05415", "authors": ["Yuchao Wang", "Tianying Wang"], "title": "Multi-Group Quadratic Discriminant Analysis via Projection", "comment": null, "summary": "Multi-group classification arises in many prediction and decision-making problems, including applications in epidemiology, genomics, finance, and image recognition. Although classification methods have advanced considerably, much of the literature focuses on binary problems, and available extensions often provide limited flexibility for multi-group settings. Recent work has extended linear discriminant analysis to multiple groups, but more general methods are still needed to handle complex structures such as nonlinear decision boundaries and group-specific covariance patterns.\n  We develop Multi-Group Quadratic Discriminant Analysis (MGQDA), a method for multi-group classification built on quadratic discriminant analysis. MGQDA projects high-dimensional predictors onto a lower-dimensional subspace, which enables accurate classification while capturing nonlinearity and heterogeneity in group-specific covariance structures. We derive theoretical guarantees, including variable selection consistency, to support the reliability of the procedure. In simulations and a gene-expression application, MGQDA achieves competitive or improved predictive performance compared with existing methods while selecting group-specific informative variables, indicating its practical value for high-dimensional multi-group classification problems. Supplementary materials for this article are available online."}
{"id": "2601.05910", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.05910", "abs": "https://arxiv.org/abs/2601.05910", "authors": ["Yigitcan Comlek", "R. Murali Krishnan", "Sandipp Krishnan Ravi", "Amin Moghaddas", "Rafael Giorjao", "Michael Eff", "Anirban Samaddar", "Nesar S. Ramachandra", "Sandeep Madireddy", "Liping Wang"], "title": "Multi-task Modeling for Engineering Applications with Sparse Data", "comment": "15 pages, 5 figures, 6 tables", "summary": "Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of data sparsity and varying task correlations. The proposed framework leverages inter-task relationships across outputs and fidelity levels to improve predictive performance and reduce computational costs. The framework is validated across three representative scenarios: Forrester function benchmark, 3D ellipsoidal void modeling, and friction-stir welding. By quantifying and leveraging inter-task relationships, the proposed MTGP framework offers a robust and scalable solution for predictive modeling in domains with significant computational and experimental costs, supporting informed decision-making and efficient resource utilization."}
{"id": "2601.05669", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05669", "abs": "https://arxiv.org/abs/2601.05669", "authors": ["Kaiyuan Zhou", "Xiaoyu Zhang", "Wenyang Zhang", "Di Wang"], "title": "Minimax Optimal Robust Sparse Regression with Heavy-Tailed Designs: A Gradient-Based Approach", "comment": null, "summary": "We investigate high-dimensional sparse regression when both the noise and the design matrix exhibit heavy-tailed behavior. Standard algorithms typically fail in this regime, as heavy-tailed covariates distort the empirical risk geometry. We propose a unified framework, Robust Iterative Gradient descent with Hard Thresholding (RIGHT), which employs a robust gradient estimator to bypass the need for higher-order moment conditions. Our analysis reveals a fundamental decoupling phenomenon: in linear regression, the estimation error rate is governed by the noise tail index, while the sample complexity required for stability is governed by the design tail index. This implies that while heavy-tailed noise limits precision, heavy-tailed designs primarily raise the sample size barrier for convergence. In contrast, for logistic regression, we show that the bounded gradient naturally robustifies the estimator against heavy-tailed designs, restoring standard parametric rates. We derive matching minimax lower bounds to prove that RIGHT achieves optimal estimation accuracy and sample complexity across these regimes, without requiring sample splitting or the existence of the population risk."}
{"id": "2601.06009", "categories": ["stat.ML", "cs.LG", "eess.SP", "math.PR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.06009", "abs": "https://arxiv.org/abs/2601.06009", "authors": ["Sunia Tanweer", "Firas A. Khasawneh"], "title": "Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem", "comment": null, "summary": "We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\\varepsilon$ of excursions of magnitude at least $\\varepsilon$ with the quadratic variation $[X]_T$ of the process. The scaling law holds universally for all continuous semimartingales with finite quadratic variation, including general Ito diffusions with nonlinear or state-dependent volatility, but fails sharply for deterministic systems -- thereby providing a theoretically-certfied method of distinguishing between these dynamics, as opposed to the subjective entropy or recurrence based state of the art methods. We construct a robust data-driven diffusion test. The method compares the empirical excursion counts against the theoretical expectation. The resulting ratio $K(\\varepsilon)=N_{\\varepsilon}^{\\mathrm{emp}}/N_{\\varepsilon}^{\\mathrm{theory}}$ is then summarized by a log-log slope deviation measuring the $\\varepsilon^{-2}$ law that provides a classification into diffusion-like or not. We demonstrate the method on canonical stochastic systems, some periodic and chaotic maps and systems with additive white noise, as well as the stochastic Duffing system. The approach is nonparametric, model-free, and relies only on the universal small-scale structure of continuous semimartingales."}
{"id": "2601.05711", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05711", "abs": "https://arxiv.org/abs/2601.05711", "authors": ["Jiayi Wang"], "title": "Conditional Cauchy-Schwarz Divergence for Time Series Analysis: Kernelized Estimation and Applications in Clustering and Fraud Detection", "comment": "22 pages, 1 figure, 3 tables", "summary": "We study the conditional Cauchy-Schwarz divergence (C-CSD) as a symmetric and density-free measure for time series analysis. We derive a practical kernel based estimator using radial basis function kernels on both the condition and output spaces, together with numerical stabilizations including a symmetric logarithmic form with an epsilon ridge and a robust bandwidth selection rule based on the interquartile range. Median heuristic bandwidths are applied to window vectors, and effective rank filtering is used to avoid degenerate kernels.\n  We demonstrate the framework in two applications. In time series clustering, conditioning on the time index and comparing scalar series values yields a pairwise C-CSD dissimilarity. Bandwidths are selected on the training split, after which precomputed distance k-medoids clustering is performed on the test split and evaluated using normalized mutual information. In fraud detection, conditioning on sliding transaction windows and comparing the magnitude of value changes with categorical and merchant change indicators, each query window is scored by contrasting a global normal reference mixture against a same account local history mixture with recency decay and change flag weighting. Account level decisions are obtained by aggregating window scores using the maximum value. Experiments on benchmark time series datasets and a transactional fraud detection dataset demonstrate stable estimation and effective performance under a strictly leak free evaluation protocol."}
{"id": "2601.05875", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05875", "abs": "https://arxiv.org/abs/2601.05875", "authors": ["Yunshu Zhang", "Shu Yang", "Wendy Ye", "Ilya Lipkovich", "Douglas E. Faries"], "title": "Estimating optimal interpretable individualized treatment regimes from a classification perspective using adaptive LASSO", "comment": "24 pages, 4 figures", "summary": "Real-world data (RWD) gains growing interests to provide a representative sample of the population for selecting the optimal treatment options. However, existing complex black box methods for estimating individualized treatment rules (ITR) from RWD have problems in interpretability and convergence. Providing an interpretable and sparse ITR can be used to overcome the limitation of existing methods. We developed an algorithm using Adaptive LASSO to predict optimal interpretable linear ITR in the RWD. To encourage sparsity, we obtain an ITR by minimizing the risk function with various types of penalties and different methods of contrast estimation. Simulation studies were conducted to select the best configuration and to compare the novel algorithm with the existing state-of-the-art methods. The proposed algorithm was applied to RWD to predict the optimal interpretable ITR. Simulations show that adaptive LASSO had the highest rates of correctly selected variables and augmented inverse probability weighting with Super Learner performed best for estimating treatment contrast. Our method had a better performance than causal forest and R-learning in terms of the value function and variable selection. The proposed algorithm can strike a balance between the interpretability of estimated ITR (by selecting a small set of important variables) and its value."}
{"id": "2601.05964", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05964", "abs": "https://arxiv.org/abs/2601.05964", "authors": ["Luis E. Nieto-Barajas", "Rodrigo S. Targino"], "title": "Negative binomial models for development triangles of counts", "comment": null, "summary": "Prediction of outstanding claims has been done via nonparametric models (chain ladder), semiparametric models (overdispersed poisson) or fully parametric models. In this paper, we propose models based on negative binomial distributions for the prediction of outstanding number of claims, which are particularly useful to account for overdispersion. We first assume independence of random variables and introduce appropriate notation. Later, we generalise the model to account for dependence across development years. In both cases, the marginal distributions are negative binomials. We study the properties of the models and carry out bayesian inference. We illustrate the performance of the models with simulated and real datasets."}
{"id": "2601.05355", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05355", "abs": "https://arxiv.org/abs/2601.05355", "authors": ["Qiao Liu", "Wing Hung Wong"], "title": "A Bayesian Generative Modeling Approach for Arbitrary Conditional Inference", "comment": null, "summary": "Modern data analysis increasingly requires flexible conditional inference P(X_B | X_A) where (X_A, X_B) is an arbitrary partition of observed variable X. Existing conditional inference methods lack this flexibility as they are tied to a fixed conditioning structure and cannot perform new conditional inference once trained. To solve this, we propose a Bayesian generative modeling (BGM) approach for arbitrary conditional inference without retraining. BGM learns a generative model of X through an iterative Bayesian updating algorithm where model parameters and latent variables are updated until convergence. Once trained, any conditional distribution can be obtained without retraining. Empirically, BGM achieves superior prediction performance with well calibrated predictive intervals, demonstrating that a single learned model can serve as a universal engine for conditional prediction with uncertainty quantification. We provide theoretical guarantees for the convergence of the stochastic iterative algorithm, statistical consistency and conditional-risk bounds. The proposed BGM framework leverages the power of AI to capture complex relationships among variables while adhering to Bayesian principles, emerging as a promising framework for advancing various applications in modern data science. The code for BGM is freely available at https://github.com/liuq-lab/bayesgm."}
{"id": "2601.05400", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05400", "abs": "https://arxiv.org/abs/2601.05400", "authors": ["Aleix Alcacer", "Irene Epifanio"], "title": "Representing asymmetric relationships by h-plots. Discovering the archetypal patterns of cross-journal citation relationships", "comment": null, "summary": "This work approaches the multidimensional scaling problem from a novel angle. We introduce a scalable method based on the h-plot, which inherently accommodates asymmetric proximity data. Instead of embedding the objects themselves, the method embeds the variables that define the proximity to or from each object. It is straightforward to implement, and the quality of the resulting representation can be easily evaluated. The methodology is illustrated by visualizing the asymmetric relationships between the citing and cited profiles of journals on a common map. Two profiles that are far apart (or close together) in the h-plot, as measured by Euclidean distance, are different (or similar), respectively. This representation allows archetypoid analysis (ADA) to be calculated. ADA is used to find archetypal journals (or extreme cases). We can represent the dataset as convex combinations of these archetypal journals, making the results easy to interpret, even for non-experts. Comparisons with other methodologies are carried out, showing the good performance of our proposal. Code and data are available for reproducibility."}
{"id": "2601.05842", "categories": ["stat.AP", "q-bio.QM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05842", "abs": "https://arxiv.org/abs/2601.05842", "authors": ["Jonathan F. Kunst", "Killian A. C. Melsen", "Willem Kruijer", "José Crossa", "Chris Maliepaard", "Fred A. van Eeuwijk", "Carel F. W. Peeters"], "title": "A latent factor approach to hyperspectral time series data for multivariate genomic prediction of grain yield in wheat", "comment": "20 pages, 8 figures", "summary": "High-dimensional time series phenotypic data is becoming increasingly common within plant breeding programmes. However, analysing and integrating such data for genetic analysis and genomic prediction remains difficult. Here we show how factor analysis with Procrustes rotation on the genetic correlation matrix of hyperspectral secondary phenotype data can help in extracting relevant features for within-trial prediction. We use a subset of Centro Internacional de Mejoramiento de Maíz y Trigo (CIMMYT) elite yield wheat trial of 2014-2015, consisting of 1,033 genotypes. These were measured across three irrigation treatments at several timepoints during the season, using manned airplane flights with hyperspectral sensors capturing 62 bands in the spectrum of 385-850 nm. We perform multivariate genomic prediction using latent variables to improve within-trial genomic predictive ability (PA) of wheat grain yield within three distinct watering treatments. By integrating latent variables of the hyperspectral data in a multivariate genomic prediction model, we are able to achieve an absolute gain of .1 to .3 (on the correlation scale) in PA compared to univariate genomic prediction. Furthermore, we show which timepoints within a trial are important and how these relate to plant growth stages. This paper showcases how domain knowledge and data-driven approaches can be combined to increase PA and gain new insights from sensor data of high-throughput phenotyping platforms."}
