{"id": "2511.03735", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.03735", "abs": "https://arxiv.org/abs/2511.03735", "authors": ["Valentin Mouton", "Adrien Mélot"], "title": "Friction on Demand: A Generative Framework for the Inverse Design of Metainterfaces", "comment": "Preprint", "summary": "Designing frictional interfaces to exhibit prescribed macroscopic behavior is\na challenging inverse problem, made difficult by the non-uniqueness of\nsolutions and the computational cost of contact simulations. Traditional\napproaches rely on heuristic search over low-dimensional parameterizations,\nwhich limits their applicability to more complex or nonlinear friction laws. We\nintroduce a generative modeling framework using Variational Autoencoders (VAEs)\nto infer surface topographies from target friction laws. Trained on a synthetic\ndataset composed of 200 million samples constructed from a parameterized\ncontact mechanics model, the proposed method enables efficient, simulation-free\ngeneration of candidate topographies. We examine the potential and limitations\nof generative modeling for this inverse design task, focusing on balancing\naccuracy, throughput, and diversity in the generated solutions. Our results\nhighlight trade-offs and outline practical considerations when balancing these\nobjectives. This approach paves the way for near-real-time control of\nfrictional behavior through tailored surface topographies."}
{"id": "2511.03756", "categories": ["stat.ML", "cs.LG", "physics.flu-dyn", "stat.AP", "60G60 (Primary), 68T05"], "pdf": "https://arxiv.org/pdf/2511.03756", "abs": "https://arxiv.org/abs/2511.03756", "authors": ["Aniket Jivani", "Cosmin Safta", "Beckett Y. Zhou", "Xun Huan"], "title": "Bifidelity Karhunen-Loève Expansion Surrogate with Active Learning for Random Fields", "comment": null, "summary": "We present a bifidelity Karhunen-Lo\\`eve expansion (KLE) surrogate model for\nfield-valued quantities of interest (QoIs) under uncertain inputs. The approach\ncombines the spectral efficiency of the KLE with polynomial chaos expansions\n(PCEs) to preserve an explicit mapping between input uncertainties and output\nfields. By coupling inexpensive low-fidelity (LF) simulations that capture\ndominant response trends with a limited number of high-fidelity (HF)\nsimulations that correct for systematic bias, the proposed method enables\naccurate and computationally affordable surrogate construction. To further\nimprove surrogate accuracy, we form an active learning strategy that adaptively\nselects new HF evaluations based on the surrogate's generalization error,\nestimated via cross-validation and modeled using Gaussian process regression.\nNew HF samples are then acquired by maximizing an expected improvement\ncriterion, targeting regions of high surrogate error. The resulting BF-KLE-AL\nframework is demonstrated on three examples of increasing complexity: a\none-dimensional analytical benchmark, a two-dimensional convection-diffusion\nsystem, and a three-dimensional turbulent round jet simulation based on\nReynolds-averaged Navier--Stokes (RANS) and enhanced delayed detached-eddy\nsimulations (EDDES). Across these cases, the method achieves consistent\nimprovements in predictive accuracy and sample efficiency relative to\nsingle-fidelity and random-sampling approaches."}
{"id": "2511.03797", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03797", "abs": "https://arxiv.org/abs/2511.03797", "authors": ["Aimee Maurais", "Bamdad Hosseini", "Youssef Marzouk"], "title": "Learning Paths for Dynamic Measure Transport: A Control Perspective", "comment": "To appear at NeurIPS 2025 Workshop on Frontiers of Probabilistic\n  Inference: Sampling Meets Learning", "summary": "We bring a control perspective to the problem of identifying paths of\nmeasures for sampling via dynamic measure transport (DMT). We highlight the\nfact that commonly used paths may be poor choices for DMT and connect existing\nmethods for learning alternate paths to mean-field games. Based on these\nconnections we pose a flexible family of optimization problems for identifying\ntilted paths of measures for DMT and advocate for the use of objective terms\nwhich encourage smoothness of the corresponding velocities. We present a\nnumerical algorithm for solving these problems based on recent Gaussian process\nmethods for solution of partial differential equations and demonstrate the\nability of our method to recover more efficient and smooth transport models\ncompared to those which use an untilted reference path."}
{"id": "2511.03892", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03892", "abs": "https://arxiv.org/abs/2511.03892", "authors": ["Chiraag Kaushik", "Justin Romberg", "Vidya Muthukumar"], "title": "A general technique for approximating high-dimensional empirical kernel matrices", "comment": "32 pages", "summary": "We present simple, user-friendly bounds for the expected operator norm of a\nrandom kernel matrix under general conditions on the kernel function\n$k(\\cdot,\\cdot)$. Our approach uses decoupling results for U-statistics and the\nnon-commutative Khintchine inequality to obtain upper and lower bounds\ndepending only on scalar statistics of the kernel function and a ``correlation\nkernel'' matrix corresponding to $k(\\cdot,\\cdot)$. We then apply our method to\nprovide new, tighter approximations for inner-product kernel matrices on\ngeneral high-dimensional data, where the sample size and data dimension are\npolynomially related. Our method obtains simplified proofs of existing results\nthat rely on the moment method and combinatorial arguments while also providing\nnovel approximation results for the case of anisotropic Gaussian data. Finally,\nusing similar techniques to our approximation result, we show a tighter lower\nbound on the bias of kernel regression with anisotropic Gaussian data."}
{"id": "2511.04213", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2511.04213", "abs": "https://arxiv.org/abs/2511.04213", "authors": ["Markus Herklotz", "Niklas Ippisch", "Anna-Carolina Haensch"], "title": "Can we trust LLMs as a tutor for our students? Evaluating the Quality of LLM-generated Feedback in Statistics Exams", "comment": "Preprint", "summary": "One of the central challenges for instructors is offering meaningful\nindividual feedback, especially in large courses. Faced with limited time and\nresources, educators are often forced to rely on generalized feedback, even\nwhen more personalized support would be pedagogically valuable. To overcome\nthis limitation, one potential technical solution is to utilize large language\nmodels (LLMs). For an exploratory study using a new platform connected with\nLLMs, we conducted a LLM-corrected mock exam during the \"Introduction to\nStatistics\" lecture at the University of Munich (Germany). The online platform\nallows instructors to upload exercises along with the correct solutions.\nStudents complete these exercises and receive overall feedback on their\nresults, as well as individualized feedback generated by GPT-4 based on the\ncorrect answers provided by the lecturers. The resulting dataset comprised\ntask-level information for all participating students, including individual\nresponses and the corresponding LLM-generated feedback. Our systematic analysis\nrevealed that approximately 7 \\% of the 2,389 feedback instances contained\nerrors, ranging from minor technical inaccuracies to conceptually misleading\nexplanations. Further, using a combined feedback framework approach, we found\nthat the feedback predominantly focused on explaining why an answer was correct\nor incorrect, with fewer instances providing deeper conceptual insights,\nlearning strategies or self-regulatory advice. These findings highlight both\nthe potential and the limitations of deploying LLMs as scalable feedback tools\nin higher education, emphasizing the need for careful quality monitoring and\nprompt design to maximize their pedagogical value."}
{"id": "2511.03797", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03797", "abs": "https://arxiv.org/abs/2511.03797", "authors": ["Aimee Maurais", "Bamdad Hosseini", "Youssef Marzouk"], "title": "Learning Paths for Dynamic Measure Transport: A Control Perspective", "comment": "To appear at NeurIPS 2025 Workshop on Frontiers of Probabilistic\n  Inference: Sampling Meets Learning", "summary": "We bring a control perspective to the problem of identifying paths of\nmeasures for sampling via dynamic measure transport (DMT). We highlight the\nfact that commonly used paths may be poor choices for DMT and connect existing\nmethods for learning alternate paths to mean-field games. Based on these\nconnections we pose a flexible family of optimization problems for identifying\ntilted paths of measures for DMT and advocate for the use of objective terms\nwhich encourage smoothness of the corresponding velocities. We present a\nnumerical algorithm for solving these problems based on recent Gaussian process\nmethods for solution of partial differential equations and demonstrate the\nability of our method to recover more efficient and smooth transport models\ncompared to those which use an untilted reference path."}
{"id": "2511.03750", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.03750", "abs": "https://arxiv.org/abs/2511.03750", "authors": ["Heidi A. Hanson", "Joemy Ramsay", "Josh Grant", "Maggie Davis", "Janet O. Agbaje", "Dakotah Maguire", "Jeremy Logan", "Marissa Taddie", "Chad Melton", "Midgie MacFarland", "James VanDerslice"], "title": "Centralized Health and Exposomic Resource (C-HER): Analytic and AI-Ready Data for External Exposomic Research", "comment": null, "summary": "The Centralized Health and Exposomic Resource (C-HER) project has identified,\nprofiled, spatially indexed, and stored over 30 external exposomic datasets.\nThe resulting analytic and AI-ready data (AAIRD) provides a significant\nopportunity to develop an integrated picture of the exposome for health\nresearch. The exposome is a conceptual framework designed to guide the study of\nthe complex environmental and genetic factors that together shape human health.\nFew composite measures of the exposome exist due to the high dimensionality of\nexposure data, multimodal data sources, and varying spatiotemporal scales. We\ndevelop a data engineering solution that overcomes the challenges of\nspatio-temporal linkage in this field. We provide examples of how environmental\ndata can be combined to characterize a region, model air pollution, or provide\nindicators for cancer research. The development of AAIRD will allow future\nstudies to use ML and deep learning methods to generate spatial and contextual\nexposure data for disease prediction."}
{"id": "2511.03817", "categories": ["stat.ME", "math.ST", "stat.TH", "62G08, 62G20, 05C50, 58J35"], "pdf": "https://arxiv.org/pdf/2511.03817", "abs": "https://arxiv.org/abs/2511.03817", "authors": ["Pawel Gajer", "Jacques Ravel"], "title": "Adaptive Geometric Regression for High-Dimensional Structured Data", "comment": "35 pages, no figures", "summary": "We present a geometric framework for regression on structured\nhigh-dimensional\n  data that shifts the analysis from the ambient space to a geometric object\n  capturing the data's intrinsic structure. The method addresses a fundamental\n  challenge in analyzing datasets with high ambient dimension but low intrinsic\n  dimension, such as microbiome compositions, where traditional approaches fail\n  to capture the underlying geometric structure. Starting from a k-nearest\n  neighbor covering of the feature space, the geometry evolves iteratively\n  through heat diffusion and response-coherence modulation, concentrating mass\n  within regions where the response varies smoothly while creating diffusion\n  barriers where the response changes rapidly. This iterative refinement\n  produces conditional expectation estimates that respect both the intrinsic\n  geometry of the feature space and the structure of the response."}
{"id": "2511.03952", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03952", "abs": "https://arxiv.org/abs/2511.03952", "authors": ["Aukosh Jagannath", "Taj Jones-McCormick", "Varnan Sarangian"], "title": "High-dimensional limit theorems for SGD: Momentum and Adaptive Step-sizes", "comment": null, "summary": "We develop a high-dimensional scaling limit for Stochastic Gradient Descent\nwith Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework\nto rigourously compare online SGD with some of its popular variants. We show\nthat the scaling limits of SGD-M coincide with those of online SGD after an\nappropriate time rescaling and a specific choice of step-size. However, if the\nstep-size is kept the same between the two algorithms, SGD-M will amplify\nhigh-dimensional effects, potentially degrading performance relative to online\nSGD. We demonstrate our framework on two popular learning problems: Spiked\nTensor PCA and Single Index Models. In both cases, we also examine online SGD\nwith an adaptive step-size based on normalized gradients. In the\nhigh-dimensional regime, this algorithm yields multiple benefits: its dynamics\nadmit fixed points closer to the population minimum and widens the range of\nadmissible step-sizes for which the iterates converge to such solutions. These\nexamples provide a rigorous account, aligning with empirical motivation, of how\nearly preconditioners can stabilize and improve dynamics in settings where\nonline SGD fails."}
{"id": "2511.03954", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03954", "abs": "https://arxiv.org/abs/2511.03954", "authors": ["Filippo Monti", "Xiang Ji", "Marc A. Suchard"], "title": "Nonparametric Modeling of Continuous-Time Markov Chains", "comment": null, "summary": "Inferring the infinitesimal rates of continuous-time Markov chains (CTMCs) is\na central challenge in many scientific domains. This task is hindered by three\nfactors: quadratic growth in the number of rates as the CTMC state space\nexpands, strong dependencies among rates, and incomplete information for many\ntransitions. We introduce a new Bayesian framework that flexibly models the\nCTMC rates by incorporating covariates through Gaussian processes (GPs). This\napproach improves inference by integrating new information and contributes to\nthe understanding of the CTMC stochastic behavior by shedding light on\npotential external drivers. Unlike previous approaches limited to linear\ncovariate effects, our method captures complex non-linear relationships,\nenabling fuller use of covariate information and more accurate characterization\nof their influence. To perform efficient inference, we employ a scalable\nHamiltonian Monte Carlo (HMC) sampler. We address the prohibitive cost of\ncomputing the exact likelihood gradient by integrating the HMC trajectories\nwith a scalable gradient approximation, reducing the computational complexity\nfrom $O(K^5)$ to $O(K^2)$, where $K$ is the number of CTMC states. Finally, we\ndemonstrate our method on Bayesian phylogeography inference -- a domain where\nCTMCs are central -- showing effectiveness on both synthetic and real datasets."}
{"id": "2511.04065", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.04065", "abs": "https://arxiv.org/abs/2511.04065", "authors": ["Mohsen Sadatsafavi", "Gavin Pereira", "Wenjia Chen"], "title": "Transportability of Prognostic Markers: Rethinking Common Practices through a Sufficient-Component-Cause Perspective", "comment": "15 pages, 2 tables, 2 figures, 1 appendix", "summary": "Transportability, the ability to maintain performance across populations, is\na desirable property of of markers of clinical outcomes. However, empirical\nfindings indicate that markers often exhibit varying performances across\npopulations. For prognostic markers whose results are used to quantify of the\nrisk of an outcome, oftentimes a form of updating is required when the marker\nis transported to populations with different disease prevalences. Here, we\nrevisit transportability of prognostic markers through the lens of the\nfoundational framework of sufficient component causes (SCC). We argue that\ntransporting a marker \"as is\" implicitly assumes predictive values are\ntransportable, whereas conventional prevalence-adjustment shifts the locus of\ntransportability to accuracy metrics (sensitivity and specificity). Using a\nminimalist SCC framework that decomposes risk prediction into its causal\nconstituents, we show that both approaches rely on strong assumptions about the\nstability of cause distributions across populations. A SCC framework instead\ninvites making transparent assumptions about how different causes vary across\npopulations, leading to different transportation methods. For example, in the\nabsence of any external information other than disease prevalence, a\ncause-neutral perspective can assume all causes are responsible for change in\nprevalence, leading to a new form of marker transportation. Numerical\nexperiments demonstrate that different transportability assumptions lead to\nvarying degrees of information loss, depending on how population differ from\neach other in the distribution of causes. A SCC perspective challenges common\nassumptions and practices for marker transportability, and proposes\ntransportation algorithms that reflect our knowledge or assumptions about how\ncauses vary across populations."}
{"id": "2511.03932", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.03932", "abs": "https://arxiv.org/abs/2511.03932", "authors": ["Will G. Hopkins"], "title": "A Pragmatic Framework for Bayesian Utility Magnitude-Based Decisions", "comment": "For associated spreadsheet, see\n  https://sportsci.org/resource/stats/UtilityMBD.xlsx", "summary": "This article presents a pragmatic framework for making formal, utility-based\ndecisions from statistical inferences. The method calculates an expected\nutility score for an intervention by combining Bayesian posterior probabilities\nof different effect magnitudes with points representing their practical value.\nA key innovation is a unified, non-arbitrary points scale (1-9 for small to\nextremely large) derived from a principle linking tangible outcomes across\ndifferent effect types. This tangible scale enables a principled \"trade-off\"\nmethod for including values for loss aversion, side effects, and implementation\ncost. The framework produces a single, definitive expected utility score, and\nthe initial decision is made by comparing the magnitude of this single score to\na user-defined smallest important net benefit, a direct and intuitive\ncomparison made possible by the scale's tangible nature. This expected utility\ndecision is interpreted alongside clinical magnitude-based decision\nprobabilities or credible interval coverage to assess evidence strength.\nInclusion of a standard deviation representing individual responses to an\nintervention (or differences between settings with meta-analytic data) allows\ncharacterization of differences between individuals (or settings) in the\nutility score expressed as proportions expected to experience benefit, a\nnegligible effect, and harm. These proportions provide context for the final\ndecision about implementation. Users must perform sensitivity analyses to\ninvestigate the effects of systematic bias and of the subjective inputs on the\nfinal decision. This framework, implemented in an accessible spreadsheet, has\nnot been empirically validated. It represents a tool in development, designed\nfor practical decision-making from available statistical evidence and\nstructured thinking about values of outcomes."}
{"id": "2511.03963", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03963", "abs": "https://arxiv.org/abs/2511.03963", "authors": ["Shinto Eguchi"], "title": "Robust inference using density-powered Stein operators", "comment": null, "summary": "We introduce a density-power weighted variant for the Stein operator, called\nthe $\\gamma$-Stein operator. This is a novel class of operators derived from\nthe $\\gamma$-divergence, designed to build robust inference methods for\nunnormalized probability models. The operator's construction (weighting by the\nmodel density raised to a positive power $\\gamma$ inherently down-weights the\ninfluence of outliers, providing a principled mechanism for robustness.\nApplying this operator yields a robust generalization of score matching that\nretains the crucial property of being independent of the model's normalizing\nconstant. We extend this framework to develop two key applications: the\n$\\gamma$-kernelized Stein discrepancy for robust goodness-of-fit testing, and\n$\\gamma$-Stein variational gradient descent for robust Bayesian posterior\napproximation. Empirical results on contaminated Gaussian and quartic potential\nmodels show our methods significantly outperform standard baselines in both\nrobustness and statistical efficiency."}
{"id": "2511.04403", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04403", "abs": "https://arxiv.org/abs/2511.04403", "authors": ["Sara Pérez-Vieites", "Sahel Iqbal", "Simo Särkkä", "Dominik Baumann"], "title": "Online Bayesian Experimental Design for Partially Observed Dynamical Systems", "comment": "19 pages, 5 figures", "summary": "Bayesian experimental design (BED) provides a principled framework for\noptimizing data collection, but existing approaches do not apply to crucial\nreal-world settings such as dynamical systems with partial observability, where\nonly noisy and incomplete observations are available. These systems are\nnaturally modeled as state-space models (SSMs), where latent states mediate the\nlink between parameters and data, making the likelihood -- and thus\ninformation-theoretic objectives like the expected information gain (EIG) --\nintractable. In addition, the dynamical nature of the system requires online\nalgorithms that update posterior distributions and select designs sequentially\nin a computationally efficient manner. We address these challenges by deriving\nnew estimators of the EIG and its gradient that explicitly marginalize latent\nstates, enabling scalable stochastic optimization in nonlinear SSMs. Our\napproach leverages nested particle filters (NPFs) for efficient online\ninference with convergence guarantees. Applications to realistic models, such\nas the susceptible-infected-recovered (SIR) and a moving source location task,\nshow that our framework successfully handles both partial observability and\nonline computation."}
{"id": "2511.04616", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.04616", "abs": "https://arxiv.org/abs/2511.04616", "authors": ["Elvis Agbenyega", "Cody Quick"], "title": "Nonparametric Safety Stock Dimensioning: A Data-Driven Approach for Supply Chains of Hardware OEMs", "comment": "17 pages, 3 figures, 3 tables. To appear in INFORMs journal", "summary": "Resilient supply chains are critical, especially for Original Equipment\nManufacturers (OEMs) that power today's digital economy. Safety Stock\ndimensioning-the computation of the appropriate safety stock quantity-is one of\nseveral mechanisms to ensure supply chain resiliency, as it protects the supply\nchain against demand and supply uncertainties. Unfortunately, the major\napproaches to dimensioning safety stock heavily assume that demand is normally\ndistributed and ignore future demand variability, limiting their applicability\nin manufacturing contexts where demand is non-normal, intermittent, and highly\nskewed. In this paper, we propose a data-driven approach that relaxes the\nassumption of normality, enabling the demand distribution of each inventory\nitem to be analytically determined using Kernel Density Estimation. Also, we\nextended the analysis from historical demand variability to forecasted demand\nvariability. We evaluated the proposed approach against a normal distribution\nmodel in a near-world inventory replenishment simulation. Afterwards, we used a\nlinear optimization model to determine the optimal safety stock configuration.\nThe results from the simulation and linear optimization models showed that the\ndata-driven approach outperformed traditional approaches. In particular, the\ndata-driven approach achieved the desired service levels at lower safety stock\nlevels than the conventional approaches."}
{"id": "2511.03954", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03954", "abs": "https://arxiv.org/abs/2511.03954", "authors": ["Filippo Monti", "Xiang Ji", "Marc A. Suchard"], "title": "Nonparametric Modeling of Continuous-Time Markov Chains", "comment": null, "summary": "Inferring the infinitesimal rates of continuous-time Markov chains (CTMCs) is\na central challenge in many scientific domains. This task is hindered by three\nfactors: quadratic growth in the number of rates as the CTMC state space\nexpands, strong dependencies among rates, and incomplete information for many\ntransitions. We introduce a new Bayesian framework that flexibly models the\nCTMC rates by incorporating covariates through Gaussian processes (GPs). This\napproach improves inference by integrating new information and contributes to\nthe understanding of the CTMC stochastic behavior by shedding light on\npotential external drivers. Unlike previous approaches limited to linear\ncovariate effects, our method captures complex non-linear relationships,\nenabling fuller use of covariate information and more accurate characterization\nof their influence. To perform efficient inference, we employ a scalable\nHamiltonian Monte Carlo (HMC) sampler. We address the prohibitive cost of\ncomputing the exact likelihood gradient by integrating the HMC trajectories\nwith a scalable gradient approximation, reducing the computational complexity\nfrom $O(K^5)$ to $O(K^2)$, where $K$ is the number of CTMC states. Finally, we\ndemonstrate our method on Bayesian phylogeography inference -- a domain where\nCTMCs are central -- showing effectiveness on both synthetic and real datasets."}
{"id": "2511.03817", "categories": ["stat.ME", "math.ST", "stat.TH", "62G08, 62G20, 05C50, 58J35"], "pdf": "https://arxiv.org/pdf/2511.03817", "abs": "https://arxiv.org/abs/2511.03817", "authors": ["Pawel Gajer", "Jacques Ravel"], "title": "Adaptive Geometric Regression for High-Dimensional Structured Data", "comment": "35 pages, no figures", "summary": "We present a geometric framework for regression on structured\nhigh-dimensional\n  data that shifts the analysis from the ambient space to a geometric object\n  capturing the data's intrinsic structure. The method addresses a fundamental\n  challenge in analyzing datasets with high ambient dimension but low intrinsic\n  dimension, such as microbiome compositions, where traditional approaches fail\n  to capture the underlying geometric structure. Starting from a k-nearest\n  neighbor covering of the feature space, the geometry evolves iteratively\n  through heat diffusion and response-coherence modulation, concentrating mass\n  within regions where the response varies smoothly while creating diffusion\n  barriers where the response changes rapidly. This iterative refinement\n  produces conditional expectation estimates that respect both the intrinsic\n  geometry of the feature space and the structure of the response."}
{"id": "2511.04275", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04275", "abs": "https://arxiv.org/abs/2511.04275", "authors": ["Jungbin Jun", "Ilsang Ohn"], "title": "Online Conformal Inference with Retrospective Adjustment for Faster Adaptation to Distribution Shift", "comment": null, "summary": "Conformal prediction has emerged as a powerful framework for constructing\ndistribution-free prediction sets with guaranteed coverage assuming only the\nexchangeability assumption. However, this assumption is often violated in\nonline environments where data distributions evolve over time. Several recent\napproaches have been proposed to address this limitation, but, typically, they\nslowly adapt to distribution shifts because they update predictions only in a\nforward manner, that is, they generate a prediction for a newly observed data\npoint while previously computed predictions are not updated. In this paper, we\npropose a novel online conformal inference method with retrospective\nadjustment, which is designed to achieve faster adaptation to distributional\nshifts. Our method leverages regression approaches with efficient leave-one-out\nupdate formulas to retroactively adjust past predictions when new data arrive,\nthereby aligning the entire set of predictions with the most recent data\ndistribution. Through extensive numerical studies performed on both synthetic\nand real-world data sets, we show that the proposed approach achieves faster\ncoverage recalibration and improved statistical efficiency compared to existing\nonline conformal prediction methods."}
{"id": "2511.04552", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.04552", "abs": "https://arxiv.org/abs/2511.04552", "authors": ["Edoardo Marcelli", "Sean O'Hagan", "Veronika Rockova"], "title": "Generative Bayesian Filtering and Parameter Learning", "comment": null, "summary": "Generative Bayesian Filtering (GBF) provides a powerful and flexible\nframework for performing posterior inference in complex nonlinear and\nnon-Gaussian state-space models. Our approach extends Generative Bayesian\nComputation (GBC) to dynamic settings, enabling recursive posterior inference\nusing simulation-based methods powered by deep neural networks. GBF does not\nrequire explicit density evaluations, making it particularly effective when\nobservation or transition distributions are analytically intractable. To\naddress parameter learning, we introduce the Generative-Gibbs sampler, which\nbypasses explicit density evaluation by iteratively sampling each variable from\nits implicit full conditional distribution. Such technique is broadly\napplicable and enables inference in hierarchical Bayesian models with\nintractable densities, including state-space models. We assess the performance\nof the proposed methodologies through both simulated and empirical studies,\nincluding the estimation of $\\alpha$-stable stochastic volatility models. Our\nfindings indicate that GBF significantly outperforms existing likelihood-free\napproaches in accuracy and robustness when dealing with intractable state-space\nmodels."}
{"id": "2511.04619", "categories": ["stat.AP", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04619", "abs": "https://arxiv.org/abs/2511.04619", "authors": ["Natalia Glazman", "Jyoti Mangal", "Pedro Borges", "Sebastien Ourselin", "M. Jorge Cardoso"], "title": "Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling", "comment": "Accepted to the NeurIPS 2025 Workshop on CauScien: Uncovering\n  Causality in Science", "summary": "The application of causal discovery to diseases like Alzheimer's (AD) is\nlimited by the static graph assumptions of most methods; such models cannot\naccount for an evolving pathophysiology, modulated by a latent disease\npseudotime. We propose to apply an existing latent variable model to real-world\nAD data, inferring a pseudotime that orders patients along a data-driven\ndisease trajectory independent of chronological age, then learning how causal\nrelationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC\n0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge\nsubstantially improved graph accuracy and orientation. Our framework reveals\ndynamic interactions between novel (NfL, GFAP) and established AD markers,\nenabling practical causal discovery despite violated assumptions."}
{"id": "2511.04130", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.04130", "abs": "https://arxiv.org/abs/2511.04130", "authors": ["Monitirtha Dey", "Trambak Banerjee", "Prajamitra Bhuyan", "Arunabha Majumdar"], "title": "Assessing Replicability Across Dependent Studies: A Framework for Testing Partial Conjunction Hypotheses with Application to GWAS", "comment": null, "summary": "Replicability is central to scientific progress, and the partial conjunction\n(PC) hypothesis testing framework provides an objective tool to quantify it\nacross disciplines. Existing PC methods assume independent studies. Yet many\nmodern applications, such as genome-wide association studies (GWAS) with sample\noverlap, violate this assumption, leading to dependence among study-specific\nsummary statistics. Failure to account for this dependence can drastically\ninflate type I errors when combining inferences. We propose e-Filter, a\npowerful procedure grounded on the theory of e-values. It involves a filtering\nstep that retains a set of the most promising PC hypotheses, and a selection\nstep where PC hypotheses from the filtering step are marked as discoveries\nwhenever their e-values exceed a selection threshold. We establish the validity\nof e-Filter for FWER and FDR control under unknown study dependence. A\ncomprehensive simulation study demonstrates its excellent power gains over\ncompeting methods. We apply e-Filter to a GWAS replicability study to identify\nconsistent genetic signals for low-density lipoprotein cholesterol (LDL-C).\nHere, the participating studies exhibit varying levels of sample overlap,\nrendering existing methods unsuitable for combining inferences. A subsequent\npathway enrichment analysis shows that e-Filter replicated signals achieve\nstronger statistical enrichment on biologically relevant LDL-C pathways than\ncompeting approaches."}
{"id": "2511.04273", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.04273", "abs": "https://arxiv.org/abs/2511.04273", "authors": ["Vincent Starck"], "title": "Estimation of Independent Component Analysis Systems", "comment": null, "summary": "Although approaches to Independent Component Analysis (ICA) based on\ncharacteristic function seem theoretically elegant, they may suffer from\nimplementational challenges because of numerical integration steps or selection\nof tuning parameters. Extending previously considered objective functions and\nleveraging results from the continuum Generalized Method of Moments of Carrasco\nand Florens (2000), I derive an optimal estimator that can take a tractable\nform and thus bypass these concerns. The method shares advantages with\ncharacteristic function approaches -- it does not require the existence of\nhigher-order moments or parametric restrictions -- while retaining\ncomputational feasibility and asymptotic efficiency. The results are adapted to\nhandle a possible first step that delivers estimated sensors. Finally, a\nby-product of the approach is a specification test that is valuable in many ICA\napplications. The method's effectiveness is illustrated through simulations,\nwhere the estimator outperforms efficient GMM, JADE, or FastICA, and an\napplication to the estimation of Structural Vector Autoregressions (SVAR), a\nworkhorse of the macroeconometric time series literature."}
{"id": "2511.04291", "categories": ["stat.ML", "cs.LG", "cs.NA", "eess.SP", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.04291", "abs": "https://arxiv.org/abs/2511.04291", "authors": ["Giovanni Barbarino", "Nicolas Gillis", "Subhayan Saha"], "title": "Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition", "comment": "38 pages, 4 figures", "summary": "Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used\nsuccessfully in many applications, such as hyperspectral imaging, chemical\nkinetics, spectroscopy, topic modeling, and audio source separation. However,\nits robustness to noise has been a long-standing open problem. In this paper,\nwe prove that min-vol NMF identifies the groundtruth factors in the presence of\nnoise under a condition referred to as the expanded sufficiently scattered\ncondition which requires the data points to be sufficiently well scattered in\nthe latent simplex generated by the basis vectors."}
{"id": "2511.03756", "categories": ["stat.ML", "cs.LG", "physics.flu-dyn", "stat.AP", "60G60 (Primary), 68T05"], "pdf": "https://arxiv.org/pdf/2511.03756", "abs": "https://arxiv.org/abs/2511.03756", "authors": ["Aniket Jivani", "Cosmin Safta", "Beckett Y. Zhou", "Xun Huan"], "title": "Bifidelity Karhunen-Loève Expansion Surrogate with Active Learning for Random Fields", "comment": null, "summary": "We present a bifidelity Karhunen-Lo\\`eve expansion (KLE) surrogate model for\nfield-valued quantities of interest (QoIs) under uncertain inputs. The approach\ncombines the spectral efficiency of the KLE with polynomial chaos expansions\n(PCEs) to preserve an explicit mapping between input uncertainties and output\nfields. By coupling inexpensive low-fidelity (LF) simulations that capture\ndominant response trends with a limited number of high-fidelity (HF)\nsimulations that correct for systematic bias, the proposed method enables\naccurate and computationally affordable surrogate construction. To further\nimprove surrogate accuracy, we form an active learning strategy that adaptively\nselects new HF evaluations based on the surrogate's generalization error,\nestimated via cross-validation and modeled using Gaussian process regression.\nNew HF samples are then acquired by maximizing an expected improvement\ncriterion, targeting regions of high surrogate error. The resulting BF-KLE-AL\nframework is demonstrated on three examples of increasing complexity: a\none-dimensional analytical benchmark, a two-dimensional convection-diffusion\nsystem, and a three-dimensional turbulent round jet simulation based on\nReynolds-averaged Navier--Stokes (RANS) and enhanced delayed detached-eddy\nsimulations (EDDES). Across these cases, the method achieves consistent\nimprovements in predictive accuracy and sample efficiency relative to\nsingle-fidelity and random-sampling approaches."}
{"id": "2511.04273", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.04273", "abs": "https://arxiv.org/abs/2511.04273", "authors": ["Vincent Starck"], "title": "Estimation of Independent Component Analysis Systems", "comment": null, "summary": "Although approaches to Independent Component Analysis (ICA) based on\ncharacteristic function seem theoretically elegant, they may suffer from\nimplementational challenges because of numerical integration steps or selection\nof tuning parameters. Extending previously considered objective functions and\nleveraging results from the continuum Generalized Method of Moments of Carrasco\nand Florens (2000), I derive an optimal estimator that can take a tractable\nform and thus bypass these concerns. The method shares advantages with\ncharacteristic function approaches -- it does not require the existence of\nhigher-order moments or parametric restrictions -- while retaining\ncomputational feasibility and asymptotic efficiency. The results are adapted to\nhandle a possible first step that delivers estimated sensors. Finally, a\nby-product of the approach is a specification test that is valuable in many ICA\napplications. The method's effectiveness is illustrated through simulations,\nwhere the estimator outperforms efficient GMM, JADE, or FastICA, and an\napplication to the estimation of Structural Vector Autoregressions (SVAR), a\nworkhorse of the macroeconometric time series literature."}
{"id": "2511.04568", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.04568", "abs": "https://arxiv.org/abs/2511.04568", "authors": ["Masahiro Kato"], "title": "Riesz Regression As Direct Density Ratio Estimation", "comment": null, "summary": "Riesz regression has garnered attention as a tool in debiased machine\nlearning for causal and structural parameter estimation (Chernozhukov et al.,\n2021). This study shows that Riesz regression is closely related to direct\ndensity-ratio estimation (DRE) in important cases, including average treat-\nment effect (ATE) estimation. Specifically, the idea and objective in Riesz\nregression coincide with the one in least-squares importance fitting (LSIF,\nKanamori et al., 2009) in direct density-ratio estimation. While Riesz\nregression is general in the sense that it can be applied to Riesz representer\nestimation in a wide class of problems, the equivalence with DRE allows us to\ndirectly import exist- ing results in specific cases, including\nconvergence-rate analyses, the selection of loss functions via\nBregman-divergence minimization, and regularization techniques for flexible\nmodels, such as neural networks. Conversely, insights about the Riesz\nrepresenter in debiased machine learning broaden the applications of direct\ndensity-ratio estimation methods. This paper consolidates our prior results in\nKato (2025a) and Kato (2025b)."}
{"id": "2511.04301", "categories": ["stat.ML", "math.DG"], "pdf": "https://arxiv.org/pdf/2511.04301", "abs": "https://arxiv.org/abs/2511.04301", "authors": ["Frederik Möbius Rygaard", "Søren Hauberg", "Steen Markvorsen"], "title": "Simultaneous Optimization of Geodesics and Fréchet Means", "comment": null, "summary": "A central part of geometric statistics is to compute the Fr\\'echet mean. This\nis a well-known intrinsic mean on a Riemannian manifold that minimizes the sum\nof squared Riemannian distances from the mean point to all other data points.\nThe Fr\\'echet mean is simple to define and generalizes the Euclidean mean, but\nfor most manifolds even minimizing the Riemannian distance involves solving an\noptimization problem. Therefore, numerical computations of the Fr\\'echet mean\nrequire solving an embedded optimization problem in each iteration. We\nintroduce the GEORCE-FM algorithm to simultaneously compute the Fr\\'echet mean\nand Riemannian distances in each iteration in a local chart, making it faster\nthan previous methods. We extend the algorithm to Finsler manifolds and\nintroduce an adaptive extension such that GEORCE-FM scales to a large number of\ndata points. Theoretically, we show that GEORCE-FM has global convergence and\nlocal quadratic convergence and prove that the adaptive extension converges in\nexpectation to the Fr\\'echet mean. We further empirically demonstrate that\nGEORCE-FM outperforms existing baseline methods to estimate the Fr\\'echet mean\nin terms of both accuracy and runtime."}
{"id": "2511.04130", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.04130", "abs": "https://arxiv.org/abs/2511.04130", "authors": ["Monitirtha Dey", "Trambak Banerjee", "Prajamitra Bhuyan", "Arunabha Majumdar"], "title": "Assessing Replicability Across Dependent Studies: A Framework for Testing Partial Conjunction Hypotheses with Application to GWAS", "comment": null, "summary": "Replicability is central to scientific progress, and the partial conjunction\n(PC) hypothesis testing framework provides an objective tool to quantify it\nacross disciplines. Existing PC methods assume independent studies. Yet many\nmodern applications, such as genome-wide association studies (GWAS) with sample\noverlap, violate this assumption, leading to dependence among study-specific\nsummary statistics. Failure to account for this dependence can drastically\ninflate type I errors when combining inferences. We propose e-Filter, a\npowerful procedure grounded on the theory of e-values. It involves a filtering\nstep that retains a set of the most promising PC hypotheses, and a selection\nstep where PC hypotheses from the filtering step are marked as discoveries\nwhenever their e-values exceed a selection threshold. We establish the validity\nof e-Filter for FWER and FDR control under unknown study dependence. A\ncomprehensive simulation study demonstrates its excellent power gains over\ncompeting methods. We apply e-Filter to a GWAS replicability study to identify\nconsistent genetic signals for low-density lipoprotein cholesterol (LDL-C).\nHere, the participating studies exhibit varying levels of sample overlap,\nrendering existing methods unsuitable for combining inferences. A subsequent\npathway enrichment analysis shows that e-Filter replicated signals achieve\nstronger statistical enrichment on biologically relevant LDL-C pathways than\ncompeting approaches."}
{"id": "2511.04331", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.04331", "abs": "https://arxiv.org/abs/2511.04331", "authors": ["Carlos A. Ribeiro Diniz", "Victor E. Lachos Olivares", "Victor H. Lachos Davila"], "title": "Matrix-Variate Regression Model for Multivariate Spatio-Temporal Data", "comment": null, "summary": "This paper introduces a matrix-variate regression model for analyzing\nmultivariate data observed across spatial locations and over time. The model's\ndesign incorporates a mean structure that links covariates to the response\nmatrix and a separable covariance structure, based on a Kronecker product, to\ncapture spatial and temporal dependencies efficiently. We derive maximum\nlikelihood estimators for all model parameters. A simulation study validates\nthe model, showing its effectiveness in parameter recovery across different\nspatial resolutions. Finally, an application to real-world data on agricultural\nand livestock production from Brazilian municipalities showcases the model's\npractical utility in revealing structured spatio-temporal patterns of variation\nand covariate effects."}
{"id": "2511.04599", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH", "62G08, 62H30, 58E05"], "pdf": "https://arxiv.org/pdf/2511.04599", "abs": "https://arxiv.org/abs/2511.04599", "authors": ["Pawel Gajer", "Jacques Ravel"], "title": "Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures", "comment": "48 pages, 3 figures", "summary": "Understanding feature-outcome associations in high-dimensional data remains\n  challenging when relationships vary across subpopulations, yet standard\n  methods assuming global associations miss context-dependent patterns,\nreducing\n  statistical power and interpretability. We develop a geometric decomposition\n  framework offering two strategies for partitioning inference problems into\n  regional analyses on data-derived Riemannian graphs. Gradient flow\n  decomposition uses path-monotonicity-validated discrete Morse theory to\n  partition samples into basins where outcomes exhibit monotonic behavior.\n  Co-monotonicity decomposition leverages association structure: vertex-level\n  coefficients measuring directional concordance between outcome and features,\n  or between feature pairs, define embeddings of samples into association\nspace.\n  These embeddings induce Riemannian k-NN graphs on which biclustering\n  identifies co-monotonicity cells (coherent regions) and feature modules. This\n  extends naturally to multi-modal integration across multiple feature sets.\n  Both strategies apply independently or jointly, with Bayesian posterior\n  sampling providing credible intervals."}
{"id": "2511.04403", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.04403", "abs": "https://arxiv.org/abs/2511.04403", "authors": ["Sara Pérez-Vieites", "Sahel Iqbal", "Simo Särkkä", "Dominik Baumann"], "title": "Online Bayesian Experimental Design for Partially Observed Dynamical Systems", "comment": "19 pages, 5 figures", "summary": "Bayesian experimental design (BED) provides a principled framework for\noptimizing data collection, but existing approaches do not apply to crucial\nreal-world settings such as dynamical systems with partial observability, where\nonly noisy and incomplete observations are available. These systems are\nnaturally modeled as state-space models (SSMs), where latent states mediate the\nlink between parameters and data, making the likelihood -- and thus\ninformation-theoretic objectives like the expected information gain (EIG) --\nintractable. In addition, the dynamical nature of the system requires online\nalgorithms that update posterior distributions and select designs sequentially\nin a computationally efficient manner. We address these challenges by deriving\nnew estimators of the EIG and its gradient that explicitly marginalize latent\nstates, enabling scalable stochastic optimization in nonlinear SSMs. Our\napproach leverages nested particle filters (NPFs) for efficient online\ninference with convergence guarantees. Applications to realistic models, such\nas the susceptible-infected-recovered (SIR) and a moving source location task,\nshow that our framework successfully handles both partial observability and\nonline computation."}
{"id": "2511.04457", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.04457", "abs": "https://arxiv.org/abs/2511.04457", "authors": ["Jaime Gonzalez-Hodar", "Johannes Milz", "Eunhye Song"], "title": "Nonparametric Robust Comparison of Solutions under Input Uncertainty", "comment": "27 pages, 4 figures", "summary": "We study ranking and selection under input uncertainty in settings where\nadditional data cannot be collected. We propose the Nonparametric Input-Output\nUncertainty Comparisons (NIOU-C) procedure to construct a confidence set that\nincludes the optimal solution with a user-specified probability. We construct\nan ambiguity set of input distributions using empirical likelihood and\napproximate the mean performance of each solution using a linear functional\nrepresentation of the input distributions. By solving optimization problems\nevaluating worst-case pairwise mean differences within the ambiguity set, we\nbuild a confidence set of solutions indistinguishable from the optimum. We\ncharacterize sample size requirements for NIOU-C to achieve the asymptotic\nvalidity under mild conditions. Moreover, we propose an extension to NIOU-C,\nNIOU-C:E, that mitigates conservatism and yields a smaller confidence set. In\nnumerical experiments, NIOU-C provides a smaller confidence set that includes\nthe optimum more frequently than a parametric procedure that takes advantage of\nthe parametric distribution families."}
{"id": "2511.04568", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.04568", "abs": "https://arxiv.org/abs/2511.04568", "authors": ["Masahiro Kato"], "title": "Riesz Regression As Direct Density Ratio Estimation", "comment": null, "summary": "Riesz regression has garnered attention as a tool in debiased machine\nlearning for causal and structural parameter estimation (Chernozhukov et al.,\n2021). This study shows that Riesz regression is closely related to direct\ndensity-ratio estimation (DRE) in important cases, including average treat-\nment effect (ATE) estimation. Specifically, the idea and objective in Riesz\nregression coincide with the one in least-squares importance fitting (LSIF,\nKanamori et al., 2009) in direct density-ratio estimation. While Riesz\nregression is general in the sense that it can be applied to Riesz representer\nestimation in a wide class of problems, the equivalence with DRE allows us to\ndirectly import exist- ing results in specific cases, including\nconvergence-rate analyses, the selection of loss functions via\nBregman-divergence minimization, and regularization techniques for flexible\nmodels, such as neural networks. Conversely, insights about the Riesz\nrepresenter in debiased machine learning broaden the applications of direct\ndensity-ratio estimation methods. This paper consolidates our prior results in\nKato (2025a) and Kato (2025b)."}
{"id": "2511.04466", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.04466", "abs": "https://arxiv.org/abs/2511.04466", "authors": ["Chuang Wan", "Jiajun Sun", "Xingbai Xu"], "title": "Conditional Selective Inference for the Selected Groups in Panel Data", "comment": null, "summary": "We consider the problem of testing for differences in group-specific slopes\nbetween the selected groups in panel data identified via k-means clustering. In\nthis setting, the classical Wald-type test statistic is problematic because it\nproduces an extremely inflated type I error probability. The underlying reason\nis that the same dataset is used to identify the group structure and construct\nthe test statistic, simultaneously. This creates dependence between the\nselection and inference stages. To address this issue, we propose a valid\nselective inference approach conditional on the selection event to account for\nthe selection effect. We formally define the selective type I error and\ndescribe how to efficiently compute the correct p-values for clusters obtained\nusing k-means clustering. Furthermore, the same idea can be extended to test\nfor differences in coefficients due to a single covariate and can be\nincorporated into the GMM estimation framework. Simulation studies show that\nour method has satisfactory finite sample performance. We apply this method to\nexplore the heterogeneous relationships between economic growth and the $CO_2$\nemission across countries for which some new findings are discovered. An R\npackage TestHomoPanel is provided to implement the proposed selective inference\nframework for panel data."}
{"id": "2511.04576", "categories": ["stat.ML", "cs.LG", "68T01"], "pdf": "https://arxiv.org/pdf/2511.04576", "abs": "https://arxiv.org/abs/2511.04576", "authors": ["Zhuo Zhang", "Xiong Xiong", "Sen Zhang", "Yuan Zhao", "Xi Yang"], "title": "Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis", "comment": "61 pages, 3 figures. Submitted to The 1st International Conference on\n  AI Scientists (ICAIS 2025)", "summary": "PDEs arise ubiquitously in science and engineering, where solutions depend on\nparameters (physical properties, boundary conditions, geometry). Traditional\nnumerical methods require re-solving the PDE for each parameter, making\nparameter space exploration prohibitively expensive. Recent machine learning\nadvances, particularly physics-informed neural networks (PINNs) and neural\noperators, have revolutionized parametric PDE solving by learning solution\noperators that generalize across parameter spaces. We critically analyze two\nmain paradigms: (1) PINNs, which embed physical laws as soft constraints and\nexcel at inverse problems with sparse data, and (2) neural operators (e.g.,\nDeepONet, Fourier Neural Operator), which learn mappings between\ninfinite-dimensional function spaces and achieve unprecedented generalization.\nThrough comparisons across fluid dynamics, solid mechanics, heat transfer, and\nelectromagnetics, we show neural operators can achieve computational speedups\nof $10^3$ to $10^5$ times faster than traditional solvers for multi-query\nscenarios, while maintaining comparable accuracy. We provide practical guidance\nfor method selection, discuss theoretical foundations (universal approximation,\nconvergence), and identify critical open challenges: high-dimensional\nparameters, complex geometries, and out-of-distribution generalization. This\nwork establishes a unified framework for understanding parametric PDE solvers\nvia operator learning, offering a comprehensive, incrementally updated resource\nfor this rapidly evolving field"}
{"id": "2511.04496", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.04496", "abs": "https://arxiv.org/abs/2511.04496", "authors": ["Yonghyun Kwon", "Jae Kwang Kim", "Yumou Qiu"], "title": "A General Approach for Calibration Weighting under Missing at Random", "comment": null, "summary": "We propose a unified class of calibration weighting methods based on weighted\ngeneralized entropy to handle missing at random (MAR) data with improved\nstability and efficiency. The proposed generalized entropy calibration (GEC)\nformulates weight construction as a convex optimization program that unifies\nentropy-based approaches and generalized regression weighting. Double\nrobustness is achieved by augmenting standard covariate balancing with a\ndebiasing constraint tied to the propensity score model and a Neyman-orthogonal\nconstraint that removes first-order sensitivity to nuisance estimation.\nSelection of the weights on the entropy function can lead to the optimal\ncalibration estimator under a correctly specified outcome regression model. The\nproposed GEC weighting ha a nice geometric characterization: the GEC solution\nis the Bregman projection of the initial weights onto a constraint set, which\nyields a generalized Pythagorean identity and a nested decomposition that\nquantifies the incremental distance paid for additional constraints. We also\ndevelop a high-dimensional extension with soft calibration and a projection\ncalibration constraint that preserves doubly robust inference. Two simulation\nstudies are presented to compare the performance of the proposed method with\nthe existing methods."}
{"id": "2511.04552", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.04552", "abs": "https://arxiv.org/abs/2511.04552", "authors": ["Edoardo Marcelli", "Sean O'Hagan", "Veronika Rockova"], "title": "Generative Bayesian Filtering and Parameter Learning", "comment": null, "summary": "Generative Bayesian Filtering (GBF) provides a powerful and flexible\nframework for performing posterior inference in complex nonlinear and\nnon-Gaussian state-space models. Our approach extends Generative Bayesian\nComputation (GBC) to dynamic settings, enabling recursive posterior inference\nusing simulation-based methods powered by deep neural networks. GBF does not\nrequire explicit density evaluations, making it particularly effective when\nobservation or transition distributions are analytically intractable. To\naddress parameter learning, we introduce the Generative-Gibbs sampler, which\nbypasses explicit density evaluation by iteratively sampling each variable from\nits implicit full conditional distribution. Such technique is broadly\napplicable and enables inference in hierarchical Bayesian models with\nintractable densities, including state-space models. We assess the performance\nof the proposed methodologies through both simulated and empirical studies,\nincluding the estimation of $\\alpha$-stable stochastic volatility models. Our\nfindings indicate that GBF significantly outperforms existing likelihood-free\napproaches in accuracy and robustness when dealing with intractable state-space\nmodels."}
{"id": "2511.04552", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.04552", "abs": "https://arxiv.org/abs/2511.04552", "authors": ["Edoardo Marcelli", "Sean O'Hagan", "Veronika Rockova"], "title": "Generative Bayesian Filtering and Parameter Learning", "comment": null, "summary": "Generative Bayesian Filtering (GBF) provides a powerful and flexible\nframework for performing posterior inference in complex nonlinear and\nnon-Gaussian state-space models. Our approach extends Generative Bayesian\nComputation (GBC) to dynamic settings, enabling recursive posterior inference\nusing simulation-based methods powered by deep neural networks. GBF does not\nrequire explicit density evaluations, making it particularly effective when\nobservation or transition distributions are analytically intractable. To\naddress parameter learning, we introduce the Generative-Gibbs sampler, which\nbypasses explicit density evaluation by iteratively sampling each variable from\nits implicit full conditional distribution. Such technique is broadly\napplicable and enables inference in hierarchical Bayesian models with\nintractable densities, including state-space models. We assess the performance\nof the proposed methodologies through both simulated and empirical studies,\nincluding the estimation of $\\alpha$-stable stochastic volatility models. Our\nfindings indicate that GBF significantly outperforms existing likelihood-free\napproaches in accuracy and robustness when dealing with intractable state-space\nmodels."}
{"id": "2511.04599", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH", "62G08, 62H30, 58E05"], "pdf": "https://arxiv.org/pdf/2511.04599", "abs": "https://arxiv.org/abs/2511.04599", "authors": ["Pawel Gajer", "Jacques Ravel"], "title": "Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures", "comment": "48 pages, 3 figures", "summary": "Understanding feature-outcome associations in high-dimensional data remains\n  challenging when relationships vary across subpopulations, yet standard\n  methods assuming global associations miss context-dependent patterns,\nreducing\n  statistical power and interpretability. We develop a geometric decomposition\n  framework offering two strategies for partitioning inference problems into\n  regional analyses on data-derived Riemannian graphs. Gradient flow\n  decomposition uses path-monotonicity-validated discrete Morse theory to\n  partition samples into basins where outcomes exhibit monotonic behavior.\n  Co-monotonicity decomposition leverages association structure: vertex-level\n  coefficients measuring directional concordance between outcome and features,\n  or between feature pairs, define embeddings of samples into association\nspace.\n  These embeddings induce Riemannian k-NN graphs on which biclustering\n  identifies co-monotonicity cells (coherent regions) and feature modules. This\n  extends naturally to multi-modal integration across multiple feature sets.\n  Both strategies apply independently or jointly, with Bayesian posterior\n  sampling providing credible intervals."}
{"id": "2511.04599", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH", "62G08, 62H30, 58E05"], "pdf": "https://arxiv.org/pdf/2511.04599", "abs": "https://arxiv.org/abs/2511.04599", "authors": ["Pawel Gajer", "Jacques Ravel"], "title": "Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures", "comment": "48 pages, 3 figures", "summary": "Understanding feature-outcome associations in high-dimensional data remains\n  challenging when relationships vary across subpopulations, yet standard\n  methods assuming global associations miss context-dependent patterns,\nreducing\n  statistical power and interpretability. We develop a geometric decomposition\n  framework offering two strategies for partitioning inference problems into\n  regional analyses on data-derived Riemannian graphs. Gradient flow\n  decomposition uses path-monotonicity-validated discrete Morse theory to\n  partition samples into basins where outcomes exhibit monotonic behavior.\n  Co-monotonicity decomposition leverages association structure: vertex-level\n  coefficients measuring directional concordance between outcome and features,\n  or between feature pairs, define embeddings of samples into association\nspace.\n  These embeddings induce Riemannian k-NN graphs on which biclustering\n  identifies co-monotonicity cells (coherent regions) and feature modules. This\n  extends naturally to multi-modal integration across multiple feature sets.\n  Both strategies apply independently or jointly, with Bayesian posterior\n  sampling providing credible intervals."}
{"id": "2511.04658", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.04658", "abs": "https://arxiv.org/abs/2511.04658", "authors": ["Adam Bouyamourn"], "title": "Where to Experiment? Site Selection Under Distribution Shift via Optimal Transport and Wasserstein DRO", "comment": "71 pages", "summary": "How should researchers select experimental sites when the deployment\npopulation differs from observed data? I formulate the problem of experimental\nsite selection as an optimal transport problem, developing methods to minimize\ndownstream estimation error by choosing sites that minimize the Wasserstein\ndistance between population and sample covariate distributions. I develop new\ntheoretical upper bounds on PATE and CATE estimation errors, and show that\nthese different objectives lead to different site selection strategies. I\nextend this approach by using Wasserstein Distributionally Robust Optimization\nto develop a site selection procedure robust to adversarial perturbations of\ncovariate information: a specific model of distribution shift. I also propose a\nnovel data-driven procedure for selecting the uncertainty radius the\nWasserstein DRO problem, which allows the user to benchmark robustness levels\nagainst observed variation in their data. Simulation evidence, and a reanalysis\nof a randomized microcredit experiment in Morocco (Cr\\'epon et al.), show that\nthese methods outperform random and stratified sampling of sites when\ncovariates have prognostic R-squared > .5, and alternative optimization methods\ni) for moderate-to-large size problem instances ii) when covariates are\nmoderately informative about treatment effects, and iii) under induced\ndistribution shift."}
{"id": "2511.04568", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.04568", "abs": "https://arxiv.org/abs/2511.04568", "authors": ["Masahiro Kato"], "title": "Riesz Regression As Direct Density Ratio Estimation", "comment": null, "summary": "Riesz regression has garnered attention as a tool in debiased machine\nlearning for causal and structural parameter estimation (Chernozhukov et al.,\n2021). This study shows that Riesz regression is closely related to direct\ndensity-ratio estimation (DRE) in important cases, including average treat-\nment effect (ATE) estimation. Specifically, the idea and objective in Riesz\nregression coincide with the one in least-squares importance fitting (LSIF,\nKanamori et al., 2009) in direct density-ratio estimation. While Riesz\nregression is general in the sense that it can be applied to Riesz representer\nestimation in a wide class of problems, the equivalence with DRE allows us to\ndirectly import exist- ing results in specific cases, including\nconvergence-rate analyses, the selection of loss functions via\nBregman-divergence minimization, and regularization techniques for flexible\nmodels, such as neural networks. Conversely, insights about the Riesz\nrepresenter in debiased machine learning broaden the applications of direct\ndensity-ratio estimation methods. This paper consolidates our prior results in\nKato (2025a) and Kato (2025b)."}
