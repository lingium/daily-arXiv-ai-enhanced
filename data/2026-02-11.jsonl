{"id": "2602.07102", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07102", "abs": "https://arxiv.org/abs/2602.07102", "authors": ["Léon Zheng", "Thomas Hirtz", "Yazid Janati", "Eric Moulines"], "title": "Fast and Robust Likelihood-Guided Diffusion Posterior Sampling with Amortized Variational Inference", "comment": null, "summary": "Zero-shot diffusion posterior sampling offers a flexible framework for inverse problems by accommodating arbitrary degradation operators at test time, but incurs high computational cost due to repeated likelihood-guided updates. In contrast, previous amortized diffusion approaches enable fast inference by replacing likelihood-based sampling with implicit inference models, but at the expense of robustness to unseen degradations. We introduce an amortization strategy for diffusion posterior sampling that preserves explicit likelihood guidance by amortizing the inner optimization problems arising in variational diffusion posterior sampling. This accelerates inference for in-distribution degradations while maintaining robustness to previously unseen operators, thereby improving the trade-off between efficiency and flexibility in diffusion-based inverse problems."}
{"id": "2602.07132", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07132", "abs": "https://arxiv.org/abs/2602.07132", "authors": ["Oswin So", "Brian Karrer", "Chuchu Fan", "Ricky T. Q. Chen", "Guan-Horng Liu"], "title": "Discrete Adjoint Matching", "comment": "ICLR 2026", "summary": "Computation methods for solving entropy-regularized reward optimization -- a class of problems widely used for fine-tuning generative models -- have advanced rapidly. Among those, Adjoint Matching (AM, Domingo-Enrich et al., 2025) has proven highly effective in continuous state spaces with differentiable rewards. Transferring these practical successes to discrete generative modeling, however, remains particularly challenging and largely unexplored, mainly due to the drastic shift in generative model classes to discrete state spaces, which are nowhere differentiable. In this work, we propose Discrete Adjoint Matching (DAM) -- a discrete variant of AM for fine-tuning discrete generative models characterized by Continuous-Time Markov Chains, such as diffusion-based large language models. The core of DAM is the introduction of discrete adjoint-an estimator of the optimal solution to the original problem but formulated on discrete domains-from which standard matching frameworks can be applied. This is derived via a purely statistical standpoint, in contrast to the control-theoretic viewpoint in AM, thereby opening up new algorithmic opportunities for general adjoint-based estimators. We showcase DAM's effectiveness on synthetic and mathematical reasoning tasks."}
{"id": "2602.07404", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.07404", "abs": "https://arxiv.org/abs/2602.07404", "authors": ["Evan T. R. Rosenman", "Kristen B. Hunter"], "title": "Adaptive Experimental Design Using Shrinkage Estimators", "comment": null, "summary": "In the setting of multi-armed trials, adaptive designs are a popular way to increase estimation efficiency, identify optimal treatments, or maximize rewards to individuals. Recent work has considered the case of estimating the effects of K active treatments, relative to a control arm, in a sequential trial. Several papers have proposed sequential versions of the classical Neyman allocation scheme to assign treatments as individuals arrive, typically with the goal of using Horvitz-Thompson-style estimators to obtain causal estimates at the end of the trial. However, this approach may be inefficient in that it fails to borrow information across the treatment arms.\n  In this paper, we consider adaptivity when the final causal estimation is obtained using a Stein-like shrinkage estimator for heteroscedastic data. Such an estimator shares information across treatment effect estimates, providing provable reductions in expected squared error loss relative to estimating each causal effect in isolation. Moreover, we show that the expected loss of the shrinkage estimator takes the form of a Gaussian quadratic form, allowing it to be computed efficiently using numerical integration. This result paves the way for sequential adaptivity, allowing treatments to be assigned to minimize the shrinker loss. Through simulations, we demonstrate that this approach can yield meaningful reductions in estimation error. We also characterize how our adaptive algorithm assigns treatments differently than would a sequential Neyman allocation."}
{"id": "2602.07613", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07613", "abs": "https://arxiv.org/abs/2602.07613", "authors": ["Jiuyao Lu", "Tianruo Zhang", "Ke Zhu"], "title": "Fast Rerandomization for Balancing Covariates in Randomized Experiments: A Metropolis-Hastings Framework", "comment": null, "summary": "Balancing covariates is critical for credible and efficient randomized experiments. Rerandomization addresses this by repeatedly generating treatment assignments until covariate balance meets a prespecified threshold. By shrinking this threshold, it can achieve arbitrarily strong balance, with established results guaranteeing optimal estimation and valid inference in both finite-sample and asymptotic settings across diverse complex experimental settings. Despite its rigorous theoretical foundations, practical use is limited by the extreme inefficiency of rejection sampling, which becomes prohibitively slow under small thresholds and often forces practitioners to adopt suboptimal settings, leading to degraded performance. Existing work focusing on acceleration typically fail to maintain the uniformity over the acceptable assignment space, thus losing the theoretical grounds of classical rerandomization. Building upon a Metropolis-Hastings framework, we address this challenge by introducing an additional sampling-importance resampling step, which restores uniformity and preserves statistical guarantees. Our proposed algorithm, PSRSRR, achieves speedups ranging from 10 to 10,000 times while maintaining exact and asymptotic validity, as demonstrated by simulations and two real-data applications."}
{"id": "2602.07252", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07252", "abs": "https://arxiv.org/abs/2602.07252", "authors": ["Yingyan Zeng", "Yujing Huang", "Xiaoyu Chen"], "title": "Beyond Euclidean Summaries: Online Change Point Detection for Distribution-Valued Data", "comment": null, "summary": "Existing online change-point detection (CPD) methods rely on fixed-dimensional Euclidean summaries, implicitly assuming that distributional changes are well captured by moment-based or feature-based representations. They can obscure important changes in distributional shape or geometry. We propose an intrinsic distribution-valued CPD framework that treats streaming batch data as a stochastic process on the 2-Wasserstein space. Our method detects changes in the law of this process by mapping each empirical distribution to a tangent space relative to a pre-change Fréchet barycenter, yielding a reference-centered local linearization of 2-Wasserstein space. This representation enables sequential detectors by adapting classical multivariate monitoring statistics to tangent fields. We provide theoretical guarantees and demonstrate, via synthetic and real-world experiments, that our approach detects complex distributional shifts with reduced detection delay at matched $\\mathrm{ARL}_0$ compared with moments-based and model-free baselines."}
{"id": "2602.07785", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07785", "abs": "https://arxiv.org/abs/2602.07785", "authors": ["Yufei Zhang", "Zhihao Ma"], "title": "Digital exclusion among middle-aged and older adults in China: age-period-cohort evidence from three national surveys, 2011-2022", "comment": "Also available as a preprint on OSF Preprints:https://osf.io/hpv68_v1", "summary": "Amid China's ageing and digital shift, digital exclusion among older adults poses an urgent challenge. To unpack this phenomenon, this study disentangles age, period, and cohort effects on digital exclusion among middle-aged and older Chinese adults. Using three nationally representative surveys (CHARLS 2011-2020, CFPS 2010-2022, and CGSS 2010-2021), we fitted hierarchical age-period-cohort (HAPC) models weighted by cross-sectional survey weights and stabilized inverse probability weights for item response. We further assessed heterogeneity by urban-rural residence, region, multimorbidity, and cognitive risk, and evaluated robustness with APC bounding analyses. Across datasets, digital exclusion increased with age and displayed mild non-linearity, with a small midlife easing followed by a sharper rise at older ages. Period effects declined over the 2010s and early 2020s, although the pace of improvement differed across survey windows. Cohort deviations were present but less consistent than age and period patterns, with an additional excess risk concentrated among cohorts born in the 1950s. Rural and western residents, as well as adults with multimorbidity or cognitive risk, remained consistently more excluded. Over the study period, the urban-rural divide showed no evidence of narrowing, whereas the cognitive-risk gap widened. These findings highlight digital inclusion as a vital pathway for older adults to remain integral participants in an evolving digital society."}
{"id": "2602.07632", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07632", "abs": "https://arxiv.org/abs/2602.07632", "authors": ["Jinhua Lyu", "Tianmin Yu", "Ying Ma", "Naichen Shi"], "title": "Scalable Mean-Field Variational Inference via Preconditioned Primal-Dual Optimization", "comment": null, "summary": "In this work, we investigate the large-scale mean-field variational inference (MFVI) problem from a mini-batch primal-dual perspective. By reformulating MFVI as a constrained finite-sum problem, we develop a novel primal-dual algorithm based on an augmented Lagrangian formulation, termed primal-dual variational inference (PD-VI). PD-VI jointly updates global and local variational parameters in the evidence lower bound in a scalable manner. To further account for heterogeneous loss geometry across different variational parameter blocks, we introduce a block-preconditioned extension, P$^2$D-VI, which adapts the primal-dual updates to the geometry of each parameter block and improves both numerical robustness and practical efficiency. We establish convergence guarantees for both PD-VI and P$^2$D-VI under properly chosen constant step size, without relying on conjugacy assumptions or explicit bounded-variance conditions. In particular, we prove $O(1/T)$ convergence to a stationary point in general settings and linear convergence under strong convexity. Numerical experiments on synthetic data and a real large-scale spatial transcriptomics dataset demonstrate that our methods consistently outperform existing stochastic variational inference approaches in terms of convergence speed and solution quality."}
{"id": "2602.07997", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07997", "abs": "https://arxiv.org/abs/2602.07997", "authors": ["TrungKhang Tran", "TrungTin Nguyen", "Md Abul Bashar", "Nhat Ho", "Richi Nayak", "Christopher Drovandi"], "title": "Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models", "comment": "TrungKhang Tran and TrungTin Nguyen are co-first authors", "summary": "Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines."}
{"id": "2602.07997", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07997", "abs": "https://arxiv.org/abs/2602.07997", "authors": ["TrungKhang Tran", "TrungTin Nguyen", "Md Abul Bashar", "Nhat Ho", "Richi Nayak", "Christopher Drovandi"], "title": "Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models", "comment": "TrungKhang Tran and TrungTin Nguyen are co-first authors", "summary": "Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines."}
{"id": "2602.07390", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07390", "abs": "https://arxiv.org/abs/2602.07390", "authors": ["Pengfei Tian", "Jiyang Ren", "Yingying Ma"], "title": "Balancing Covariates in Survey Experiments", "comment": null, "summary": "The survey experiment is widely used in economics and social sciences to evaluate the effects of treatments or programs. In a standard population-based survey experiment, the experimenter randomly draws experimental units from a target population of interest and then randomly assigns the sampled units to treatment or control conditions to explore the treatment effect of an intervention. Simple random sampling and treatment assignment can balance covariates on average. However, covariate imbalance often exists in finite samples. To address the imbalance issue, we study a stratified approach to balance covariates in a survey experiment. A stratified rejective sampling and rerandomization design is further proposed to enhance the covariate balance. We develop a design-based asymptotic theory for the widely used stratified difference-in-means estimator of the average treatment effect under the proposed design. In particular, we show that it is consistent and asymptotically a convolution of a normal distribution and two truncated normal distributions. This limiting distribution is more concentrated at the true average treatment effect than that under the existing experimental designs. Moreover, we propose a covariate adjustment method in the analysis stage, which can further improve the estimation efficiency. Numerical studies demonstrate the validity and improved efficiency of the proposed method."}
{"id": "2602.07911", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.07911", "abs": "https://arxiv.org/abs/2602.07911", "authors": ["Ping Zhao", "Fengyi Song", "Huifang Ma"], "title": "Adaptive Test Procedure for High Dimensional Regression Coefficient", "comment": null, "summary": "We develop a unified $L$-statistic testing framework for high-dimensional regression coefficients that adapts to unknown sparsity. The proposed statistics rank coordinate-wise evidence measures and aggregate the top $k$ signals, bridging classical max-type and sum-type tests. We establish joint weak convergence of the extreme-value component and standardized $L$-statistics under mild conditions, yielding an asymptotic independence that justifies combining multiple $k$'s. An adaptive omnibus test is constructed via a Cauchy combination over a dyadic grid of $k$, and a wild bootstrap calibration is provided with theoretical guarantees. Simulations demonstrate accurate size and strong power across sparse and dense alternatives, including non-Gaussian designs."}
{"id": "2602.07633", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07633", "abs": "https://arxiv.org/abs/2602.07633", "authors": ["Trevor Harris"], "title": "Flow-Based Conformal Predictive Distributions", "comment": "9 pages, 6 figures, 10 appendix pages", "summary": "Conformal prediction provides a distribution-free framework for uncertainty quantification via prediction sets with exact finite-sample coverage. In low dimensions these sets are easy to interpret, but in high-dimensional or structured output spaces they are difficult to represent and use, which can limit their ability to integrate with downstream tasks such as sampling and probabilistic forecasting. We show that any differentiable nonconformity score induces a deterministic flow on the output space whose trajectories converge to the boundary of the corresponding conformal prediction set. This leads to a computationally efficient, training-free method for sampling conformal boundaries in arbitrary dimensions. Boundary samples can be reconformalized to form pointwise prediction sets with controlled risk, and mixing across confidence levels yields conformal predictive distributions whose quantile regions coincide exactly with conformal prediction sets. We evaluate the approach on PDE inverse problems, precipitation downscaling, climate model debiasing, and hurricane trajectory forecasting."}
{"id": "2602.08347", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.08347", "abs": "https://arxiv.org/abs/2602.08347", "authors": ["Takato Hashino", "Koji Tsukuda"], "title": "Estimating the Shannon Entropy Using the Pitman--Yor Process", "comment": "23 pages, 5 figures", "summary": "The Shannon entropy is a fundamental measure for quantifying diversity and model complexity in fields such as information theory, ecology, and genetics. However, many existing studies assume that the number of species is known, an assumption that is often unrealistic in practice. In recent years, efforts have been made to relax this restriction. Motivated by these developments, this study proposes an entropy estimation method based on the Pitman--Yor process, a representative approach in Bayesian nonparametrics. By approximating the true distribution as an infinite-dimensional process, the proposed method enables stable estimation even when the number of observed species is smaller than the true number of species. This approach provides a principled way to deal with the uncertainty in species diversity and enhances the reliability and robustness of entropy-based diversity assessment. In addition, we investigate the convergence property of the Shannon entropy for regularly varying distributions and use this result to establish the consistency of the proposed estimator. Finally, we demonstrate the effectiveness of the proposed method through numerical experiments."}
{"id": "2602.08544", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.08544", "abs": "https://arxiv.org/abs/2602.08544", "authors": ["Luca Presicce", "Sudipto Banerjee"], "title": "Adaptive Markovian Spatiotemporal Transfer Learning in Multivariate Bayesian Modeling", "comment": null, "summary": "This manuscript develops computationally efficient online learning for multivariate spatiotemporal models. The method relies on matrix-variate Gaussian distributions, dynamic linear models, and Bayesian predictive stacking to efficiently share information across temporal data shards. The model facilitates effective information propagation over time while seamlessly integrating spatial components within a dynamic framework, building a Markovian dependence structure between datasets at successive time instants. This structure supports flexible, high-dimensional modeling of complex dependence patterns, as commonly found in spatiotemporal phenomena, where computational challenges arise rapidly with increasing dimensions. The proposed approach further manages exact inference through predictive stacking, enhancing accuracy and interoperability. Combining sequential and parallel processing of temporal shards, each unit passes assimilated information forward, then back-smoothed to improve posterior estimates, incorporating all available information. This framework advances the scalability and adaptability of spatiotemporal modeling, making it suitable for dynamic, multivariate, and data-rich environments."}
{"id": "2602.07404", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.07404", "abs": "https://arxiv.org/abs/2602.07404", "authors": ["Evan T. R. Rosenman", "Kristen B. Hunter"], "title": "Adaptive Experimental Design Using Shrinkage Estimators", "comment": null, "summary": "In the setting of multi-armed trials, adaptive designs are a popular way to increase estimation efficiency, identify optimal treatments, or maximize rewards to individuals. Recent work has considered the case of estimating the effects of K active treatments, relative to a control arm, in a sequential trial. Several papers have proposed sequential versions of the classical Neyman allocation scheme to assign treatments as individuals arrive, typically with the goal of using Horvitz-Thompson-style estimators to obtain causal estimates at the end of the trial. However, this approach may be inefficient in that it fails to borrow information across the treatment arms.\n  In this paper, we consider adaptivity when the final causal estimation is obtained using a Stein-like shrinkage estimator for heteroscedastic data. Such an estimator shares information across treatment effect estimates, providing provable reductions in expected squared error loss relative to estimating each causal effect in isolation. Moreover, we show that the expected loss of the shrinkage estimator takes the form of a Gaussian quadratic form, allowing it to be computed efficiently using numerical integration. This result paves the way for sequential adaptivity, allowing treatments to be assigned to minimize the shrinker loss. Through simulations, we demonstrate that this approach can yield meaningful reductions in estimation error. We also characterize how our adaptive algorithm assigns treatments differently than would a sequential Neyman allocation."}
{"id": "2602.07935", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.07935", "abs": "https://arxiv.org/abs/2602.07935", "authors": ["Afshin Yaghoubi"], "title": "Analysis of Repairable Systems Availability with Lindley Failure and Repair Behavior", "comment": null, "summary": "Maintainability analysis is a cornerstone of reliability engineering. While the Markov approach is the classical analytical foundation, its reliance on the exponential distribution for failure and repair times is a major and often unrealistic limitation. This paper directly overcomes this critical constraint by investigating and modeling system maintainability using the more flexible and versatile Lindley distribution, which is represented via phase-type distributions. We first present a comprehensive maintainability analysis of a single-component system, deriving precise closed-form expressions for its time-dependent and steady-state availability, as well as the mean time to repair. The core methodology is then systematically generalized to analyze common series and parallel system configurations with n independent and identically distributed components. A dedicated numerical study compares the system performance under the Lindley and exponential distributions, conclusively demonstrating the significant and practical impact of non-exponential repair times on key reliability metrics. Our work provides a versatile and more widely applicable analytical framework for accurate maintainability assessment that successfully relaxes the restrictive exponential assumption, thereby offering greater realism in reliability modeling."}
{"id": "2602.07710", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07710", "abs": "https://arxiv.org/abs/2602.07710", "authors": ["Jiaxun Li", "Vinod Raman", "Ambuj Tewari"], "title": "On Generation in Metric Spaces", "comment": null, "summary": "We study generation in separable metric instance spaces. We extend the language generation framework from Kleinberg and Mullainathan [2024] beyond countable domains by defining novelty through metric separation and allowing asymmetric novelty parameters for the adversary and the generator. We introduce the $(\\varepsilon,\\varepsilon')$-closure dimension, a scale-sensitive analogue of closure dimension, which yields characterizations of uniform and non-uniform generatability and a sufficient condition for generation in the limit. Along the way, we identify a sharp geometric contrast. Namely, in doubling spaces, including all finite-dimensional normed spaces, generatability is stable across novelty scales and invariant under equivalent metrics. In general metric spaces, however, generatability can be highly scale-sensitive and metric-dependent; even in the natural infinite-dimensional Hilbert space $\\ell^2$, all notions of generation may fail abruptly as the novelty parameters vary."}
{"id": "2602.08374", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.08374", "abs": "https://arxiv.org/abs/2602.08374", "authors": ["Denis Belomestny", "Alexey Naumov", "Nikita Puchkin", "Denis Suchkov"], "title": "Schrödinger bridge problem via empirical risk minimization", "comment": null, "summary": "We study the Schrödinger bridge problem when the endpoint distributions are available only through samples. Classical computational approaches estimate Schrödinger potentials via Sinkhorn iterations on empirical measures and then construct a time-inhomogeneous drift by differentiating a kernel-smoothed dual solution. In contrast, we propose a learning-theoretic route: we rewrite the Schrödinger system in terms of a single positive transformed potential that satisfies a nonlinear fixed-point equation and estimate this potential by empirical risk minimization over a function class. We establish uniform concentration of the empirical risk around its population counterpart under sub-Gaussian assumptions on the reference kernel and terminal density. We plug the learned potential into a stochastic control representation of the bridge to generate samples. We illustrate performance of the suggested approach with numerical experiments."}
{"id": "2602.07454", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.07454", "abs": "https://arxiv.org/abs/2602.07454", "authors": ["Teemu Härkönen", "Simo Särkkä"], "title": "Estimation of log-Gaussian gamma processes with iterated posterior linearization and Hamiltonian Monte Carlo", "comment": null, "summary": "Stochastic processes are a flexible and widely used family of models for statistical modeling. While stochastic processes offer attractive properties such as inclusion of uncertainty properties, their inference is typically intractable, with the notable exception of Gaussian processes. Inference of models with non-Gaussian errors typically involves estimation of a high-dimensional latent variable. We propose two methods that use iterated posterior linearization followed by Hamiltonian Monte Carlo to sample the posterior distributions of such latent models with a particular focus on log-Gaussian gamma processes. The proposed methods are validated with two synthetic datasets generated from the log-Gaussian gamma process and a multiscale biocomposite stiffness model. In addition, we apply the methodology to an experimental Raman spectrum of argentopyrite."}
{"id": "2602.08083", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.08083", "abs": "https://arxiv.org/abs/2602.08083", "authors": ["Aiwen Li", "Amrita Balajee", "Harry Wieand", "Jonathan Pipping-Gamón"], "title": "A Unified Server Quality Metric for Tennis", "comment": "13 pages, submitted to the Journal of Quantitative Analysis in Sports (JQAS)", "summary": "Traditional tennis rating systems, such as Elo, summarize overall player strength but do not isolate the independent value of serving. Using point-by-point data from Wimbledon and the U.S. Open, we develop serve-specific player metrics to isolate serving quality from overall performance. For each tournament and gender, we fit logistic mixed-effects models using serve speed, speed variability, and placement features, with crossed server and returner random intercepts capturing unobserved server and returner-strength effects. We use these models to estimate Server Quality Scores (SQS) that reflect players' serving ability. In out-of-sample tests, SQS shows stronger alignment with serve efficiency (measured as points won within three shots) than weighted Elo. Associations with overall serve win percentage are smaller and mixed across datasets, and neither SQS nor wElo consistently dominates on that outcome. These findings highlight that serve-specific metrics complement holistic ratings and provide actionable insight for coaching, forecasting, and player evaluation."}
{"id": "2602.07767", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07767", "abs": "https://arxiv.org/abs/2602.07767", "authors": ["Ruizhe Deng", "Bibhas Chakraborty", "Ran Chen", "Yan Shuo Tan"], "title": "BFTS: Thompson Sampling with Bayesian Additive Regression Trees", "comment": null, "summary": "Contextual bandits are a core technology for personalized mobile health interventions, where decision-making requires adapting to complex, non-linear user behaviors. While Thompson Sampling (TS) is a preferred strategy for these problems, its performance hinges on the quality of the underlying reward model. Standard linear models suffer from high bias, while neural network approaches are often brittle and difficult to tune in online settings. Conversely, tree ensembles dominate tabular data prediction but typically rely on heuristic uncertainty quantification, lacking a principled probabilistic basis for TS. We propose Bayesian Forest Thompson Sampling (BFTS), the first contextual bandit algorithm to integrate Bayesian Additive Regression Trees (BART), a fully probabilistic sum-of-trees model, directly into the exploration loop. We prove that BFTS is theoretically sound, deriving an information-theoretic Bayesian regret bound of $\\tilde{O}(\\sqrt{T})$. As a complementary result, we establish frequentist minimax optimality for a \"feel-good\" variant, confirming the structural suitability of BART priors for non-parametric bandits. Empirically, BFTS achieves state-of-the-art regret on tabular benchmarks with near-nominal uncertainty calibration. Furthermore, in an offline policy evaluation on the Drink Less micro-randomized trial, BFTS improves engagement rates by over 30% compared to the deployed policy, demonstrating its practical effectiveness for behavioral interventions."}
{"id": "2602.07477", "categories": ["stat.ME", "cs.LG", "stat.AP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07477", "abs": "https://arxiv.org/abs/2602.07477", "authors": ["Lena Schemet", "Sarah Friedrich-Welz"], "title": "Statistical inference after variable selection in Cox models: A simulation study", "comment": null, "summary": "Choosing relevant predictors is central to the analysis of biomedical time-to-event data. Classical frequentist inference, however, presumes that the set of covariates is fixed in advance and does not account for data-driven variable selection. As a consequence, naive post-selection inference may be biased and misleading. In right-censored survival settings, these issues may be further exacerbated by the additional uncertainty induced by censoring. We investigate several inference procedures applied after variable selection for the coefficients of the Lasso and its extension, the adaptive Lasso, in the context of the Cox model. The methods considered include sample splitting, exact post-selection inference, and the debiased Lasso. Their performance is examined in a neutral simulation study reflecting realistic covariate structures and censoring rates commonly encountered in biomedical applications. To complement the simulation results, we illustrate the practical behavior of these procedures in an applied example using a publicly available survival dataset."}
{"id": "2602.08172", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.08172", "abs": "https://arxiv.org/abs/2602.08172", "authors": ["Guannan Gong", "Satrajit Roychoudhury", "Allison Meisner", "Lajos Pusztai", "Sarah B Goldberg", "Wei Wei"], "title": "Learning from Literature: Integrating LLMs and Bayesian Hierarchical Modeling for Oncology Trial Design", "comment": null, "summary": "Designing modern oncology trials requires synthesizing evidence from prior studies to inform hypothesis generation and sample size determination. Trial designs based on incomplete or imprecise summaries can lead to misspecified hypotheses and underpowered studies, resulting in false positive or negative conclusions. To address this challenge, we developed LEAD-ONC (Literature to Evidence for Analytics and Design in Oncology), an AI-assisted framework that transforms published clinical trial reports into quantitative, design-relevant evidence. Given expert-curated trial publications that meet prespecified eligibility criteria, LEAD-ONC uses large language models to extract baseline characteristics and reconstruct individual patient data from Kaplan-Meier curves, followed by Bayesian hierarchical modeling to generate predictive survival distributions for a prespecified target trial population. We demonstrate the framework using five phase III trials in first-line non-small-cell lung cancer evaluating PD-1 or PD-L1 inhibitors with or without CTLA-4 blockade. Clustering based on baseline characteristics identified three clinically interpretable populations defined by histology. For a prospective randomized trial in the mixed-histology population comparing mono versus dual immune checkpoint inhibition, LEAD-ONC projected a modest median overall survival difference of 2.8 months (95 percent credible interval -2.0 to 7.6) and an estimated probability of at least a 3-month benefit of approximately 0.45. As LEAD-ONC remains under active development, these results are intended as preliminary demonstrations of the frameworks potential to support evidence-driven oncology trial design rather than definitive clinical conclusions."}
{"id": "2602.07997", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07997", "abs": "https://arxiv.org/abs/2602.07997", "authors": ["TrungKhang Tran", "TrungTin Nguyen", "Md Abul Bashar", "Nhat Ho", "Richi Nayak", "Christopher Drovandi"], "title": "Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models", "comment": "TrungKhang Tran and TrungTin Nguyen are co-first authors", "summary": "Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines."}
{"id": "2602.07482", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07482", "abs": "https://arxiv.org/abs/2602.07482", "authors": ["Jingwen Zhang", "Satoshi Hattori"], "title": "Event-driven type design for clinical trials with recurrent events", "comment": null, "summary": "It is a common practice in randomized clinical trials with the standard survival outcome to follow patients until a prespecified number of events have been observed, a type of trial known as the event-driven trial. The event-driven design ensures that the target power for a specified type 1 error rate is achieved to detect the target hazard ratio, regardless of the specification of other quantities. To understand the treatment effect for chronic conditions, the analysis of recurrent events has gained popularity in randomized controlled trials, particularly large-scale confirmatory trials. In the absence of within-subject correlation among multiple events, a similar event-driven design can be employed for recurrent event outcomes. On the other hand, in the presence of the within-subject correlation, one needs to model the correlation among recurrent events in evaluating power and setting the sample size. However, information useful in modeling the within-subject correlation is limited at the design stage. Failing to consider the correlation properly may lead to underpowered studies. We propose an event-driven type design for recurrent event outcomes. Our method ensures the target power for the target treatment effect, regardless of the specification of other quantities, by monitoring the robust variance under the marginal rates/means model in a blinded manner. We investigate the operating characteristics of the proposed monitoring procedure in simulation studies. The results of simulation studies showed that the proposed blinded monitoring procedure controlled the power well so that the test possessed the target power and did not lead to serious inflation of the type 1 error rate. Furthermore, we illustrate the proposed method using a real clinical trial dataset."}
{"id": "2602.08414", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.08414", "abs": "https://arxiv.org/abs/2602.08414", "authors": ["Paula Staudt", "Anika Schlosser", "Annika Möhl", "Martin Schumacher", "Nadine Binder"], "title": "Temporal Trends in Incidence of Dementia in a Birth Cohorts Analysis of the Framingham Heart Study", "comment": "14 pages, 3 figures, 2 tables", "summary": "Background: Dementia leads to a high burden of disability and the number of dementia patients worldwide doubled between 1990 and 2016. Nevertheless, some studies indicated a decrease in dementia risk which may be due to a bias caused by conventional analysis methods that do not adequately account for missing disease information due to death.\n  Methods: This study re-examines potential trends in dementia incidence over four decades in the Framingham Heart Study. We apply a multistate modeling framework tailored to interval-censored illness-death data and define three non-overlapping birth cohorts (1915-1924, 1925-1934, and 1935-1944). Trends are evaluated based on both dementia prevalence and dementia risk, using age as the underlying timescale. Additionally, age-conditional dementia probabilities stratified by sex are estimated.\n  Results: A total of 731 out of 3828 individuals were diagnosed with dementia. The multistate model analysis revealed no temporal decline in dementia risk across birth cohorts, irrespective of sex. When stratified by sex and adjusted for education, women consistently exhibited higher lifetime age-conditional risks (46%-50%) than men (30%-34%) over the study period.\n  Conclusions: We recommend using a combination of multistate approach and separation into birth cohorts to adequately estimate trends of disease risk in cohort studies as well as to communicate patient-relevant outcomes such age-conditional disease risks."}
{"id": "2602.08042", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08042", "abs": "https://arxiv.org/abs/2602.08042", "authors": ["Nadav Katz", "Ariel Jaffe"], "title": "Graph-based Semi-Supervised Learning via Maximum Discrimination", "comment": null, "summary": "Semi-supervised learning (SSL) addresses the critical challenge of training accurate models when labeled data is scarce but unlabeled data is abundant. Graph-based SSL (GSSL) has emerged as a popular framework that captures data structure through graph representations. Classic graph SSL methods, such as Label Propagation and Label Spreading, aim to compute low-dimensional representations where points with the same labels are close in representation space. Although often effective, these methods can be suboptimal on data with complex label distributions. In our work, we develop AUC-spec, a graph approach that computes a low-dimensional representation that maximizes class separation. We compute this representation by optimizing the Area Under the ROC Curve (AUC) as estimated via the labeled points. We provide a detailed analysis of our approach under a product-of-manifold model, and show that the required number of labeled points for AUC-spec is polynomial in the model parameters. Empirically, we show that AUC-spec balances class separation with graph smoothness. It demonstrates competitive results on synthetic and real-world datasets while maintaining computational efficiency comparable to the field's classic and state-of-the-art methods."}
{"id": "2602.07613", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07613", "abs": "https://arxiv.org/abs/2602.07613", "authors": ["Jiuyao Lu", "Tianruo Zhang", "Ke Zhu"], "title": "Fast Rerandomization for Balancing Covariates in Randomized Experiments: A Metropolis-Hastings Framework", "comment": null, "summary": "Balancing covariates is critical for credible and efficient randomized experiments. Rerandomization addresses this by repeatedly generating treatment assignments until covariate balance meets a prespecified threshold. By shrinking this threshold, it can achieve arbitrarily strong balance, with established results guaranteeing optimal estimation and valid inference in both finite-sample and asymptotic settings across diverse complex experimental settings. Despite its rigorous theoretical foundations, practical use is limited by the extreme inefficiency of rejection sampling, which becomes prohibitively slow under small thresholds and often forces practitioners to adopt suboptimal settings, leading to degraded performance. Existing work focusing on acceleration typically fail to maintain the uniformity over the acceptable assignment space, thus losing the theoretical grounds of classical rerandomization. Building upon a Metropolis-Hastings framework, we address this challenge by introducing an additional sampling-importance resampling step, which restores uniformity and preserves statistical guarantees. Our proposed algorithm, PSRSRR, achieves speedups ranging from 10 to 10,000 times while maintaining exact and asymptotic validity, as demonstrated by simulations and two real-data applications."}
{"id": "2602.08787", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.08787", "abs": "https://arxiv.org/abs/2602.08787", "authors": ["Cory Petersen", "Feng Ye", "Jiaxiang Ji", "Josh Kohut", "Ahmed Aziz Ezzat", "David Saginaw", "Avril Montanti", "Jack Cammarota"], "title": "Accessibility and Serviceability Assessment to Inform Offshore Wind Energy Development and Operations off the U.S. East Coast", "comment": null, "summary": "The economic success of offshore wind energy projects relies on accurate projections of the construction, and operations and maintenance (O&M) costs. These projections must consider the logistical complexities introduced by adverse met-ocean conditions that can prohibit access to the offshore assets for sustained periods of time. In response, the goal of this study is two-fold: (1) to provide high-resolution estimates of the accessibility of key offshore wind energy areas in the United States (U.S.) East Coast--a region with significant offshore wind energy potential; and (2) to introduce a new operational metric, called serviceability, as motivated by the need to assess the accessibility of an offshore asset along a vessel travel path, rather than at a specific site, as commonly carried out in the literature. We hypothesize that serviceability is more relevant to offshore operations than accessibility, since it more realistically reflects the success and safety of a vessel operation along its journey from port to site and back. Our analysis reveals high temporal and spatial variations in accessibility and serviceability, even for proximate offshore locations. We also find that solely relying on numerical met-ocean data can introduce considerable bias in estimating accessibility and serviceability, raising the need for a statistical treatment that combines both numerical and observational data sources, such as the one proposed herein. Collectively, our analysis sheds light on the value of high-resolution met-ocean information and models in supporting offshore operations, including but not limited to future offshore wind energy developments."}
{"id": "2602.08185", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08185", "abs": "https://arxiv.org/abs/2602.08185", "authors": ["Masanari Kimura"], "title": "Information Geometry of Absorbing Markov-Chain and Discriminative Random Walks", "comment": null, "summary": "Discriminative Random Walks (DRWs) are a simple yet powerful tool for semi-supervised node classification, but their theoretical foundations remain fragmentary. We revisit DRWs through the lens of information geometry, treating the family of class-specific hitting-time laws on an absorbing Markov chain as a statistical manifold. Starting from a log-linear edge-weight model, we derive closed-form expressions for the hitting-time probability mass function, its full moment hierarchy, and the observed Fisher information. The Fisher matrix of each seed node turns out to be rank-one, taking the quotient by its null space yields a low-dimensional, globally flat manifold that captures all identifiable directions of the model. Leveraging the geometry, we introduce a sensitivity score for unlabeled nodes that bounds, and in one-dimensional cases attains, the maximal first-order change in DRW betweenness under unit Fisher perturbations. The score can lead to principled strategies for active label acquisition, edge re-weighting, and explanation."}
{"id": "2602.07681", "categories": ["stat.ME", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07681", "abs": "https://arxiv.org/abs/2602.07681", "authors": ["Qishi Zhan", "Cheng-Han Yu", "Yuchi Chen", "Zhikang Dong", "Rajarshi Guhaniyogi"], "title": "Mapping Drivers of Greenness: Spatial Variable Selection for MODIS Vegetation Indices", "comment": null, "summary": "Understanding how environmental drivers relate to vegetation condition motivates spatially varying regression models, but estimating a separate coefficient surface for every predictor can yield noisy patterns and poor interpretability when many predictors are irrelevant. Motivated by MODIS vegetation index studies, we examine predictors from spectral bands, productivity and energy fluxes, observation geometry, and land surface characteristics. Because these relationships vary with canopy structure, climate, land use, and measurement conditions, methods should both model spatially varying effects and identify where predictors matter. We propose a spatially varying coefficient model where each coefficient surface uses a tensor product B-spline basis and a Bayesian group lasso prior on the basis coefficients. This prior induces predictor level shrinkage, pushing negligible effects toward zero while preserving spatial structure. Posterior inference uses Markov chain Monte Carlo and provides uncertainty quantification for each effect surface. We summarize retained effects with spatial significance maps that mark locations where the 95 percent posterior credible interval excludes zero, and we define a spatial coverage probability as the proportion of locations where the credible interval excludes zero. Simulations recover sparsity and achieve prediction. A MODIS application yields a parsimonious subset of predictors whose effect maps clarify dominant controls across landscapes."}
{"id": "2602.07477", "categories": ["stat.ME", "cs.LG", "stat.AP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07477", "abs": "https://arxiv.org/abs/2602.07477", "authors": ["Lena Schemet", "Sarah Friedrich-Welz"], "title": "Statistical inference after variable selection in Cox models: A simulation study", "comment": null, "summary": "Choosing relevant predictors is central to the analysis of biomedical time-to-event data. Classical frequentist inference, however, presumes that the set of covariates is fixed in advance and does not account for data-driven variable selection. As a consequence, naive post-selection inference may be biased and misleading. In right-censored survival settings, these issues may be further exacerbated by the additional uncertainty induced by censoring. We investigate several inference procedures applied after variable selection for the coefficients of the Lasso and its extension, the adaptive Lasso, in the context of the Cox model. The methods considered include sample splitting, exact post-selection inference, and the debiased Lasso. Their performance is examined in a neutral simulation study reflecting realistic covariate structures and censoring rates commonly encountered in biomedical applications. To complement the simulation results, we illustrate the practical behavior of these procedures in an applied example using a publicly available survival dataset."}
{"id": "2602.08243", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08243", "abs": "https://arxiv.org/abs/2602.08243", "authors": ["Wei Guo", "Yuchen Zhu", "Xiaochen Du", "Juno Nam", "Yongxin Chen", "Rafael Gómez-Bombarelli", "Guan-Horng Liu", "Molei Tao", "Jaemoo Choi"], "title": "Discrete Adjoint Schrödinger Bridge Sampler", "comment": null, "summary": "Learning discrete neural samplers is challenging due to the lack of gradients and combinatorial complexity. While stochastic optimal control (SOC) and Schrödinger bridge (SB) provide principled solutions, efficient SOC solvers like adjoint matching (AM), which excel in continuous domains, remain unexplored for discrete spaces. We bridge this gap by revealing that the core mechanism of AM is $\\mathit{state}\\text{-}\\mathit{space~agnostic}$, and introduce $\\mathbf{discrete~ASBS}$, a unified framework that extends AM and adjoint Schrödinger bridge sampler (ASBS) to discrete spaces. Theoretically, we analyze the optimality conditions of the discrete SB problem and its connection to SOC, identifying a necessary cyclic group structure on the state space to enable this extension. Empirically, discrete ASBS achieves competitive sample quality with significant advantages in training efficiency and scalability."}
{"id": "2602.07704", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07704", "abs": "https://arxiv.org/abs/2602.07704", "authors": ["Lukáš Lafférs", "Jozef Michal Mintal", "Ivan Sutóris"], "title": "Correcting for Nonignorable Nonresponse Bias in Ordinal Observational Survey Data", "comment": "17 pages", "summary": "Many political surveys rely on post-stratification, raking, or related weighting adjustments to align respondents with the target population. But when respondents differ from nonrespondents on the outcome itself (nonignorable nonresponse), these adjustments can fail, introducing bias even into basic descriptives.We provide a practical method that corrects for nonignorable nonresponse by leveraging response-propensity proxies (e.g., interviewer-coded cooperativeness) observed among respondents to extrapolate toward nonrespondents, while directly integrating observable covariates and retaining the benefits of post-stratification with known population shares. The method generalizes the variable-response-propensity (VRP) framework of Peress (2010) from binary to ordinal outcomes, which are widely used to measure trust, satisfaction, and policy attitudes. The resulting estimator is computed by maximum likelihood and implemented in a compact R routine that handles both ordinal and binary outcomes. Using the 2024 American National Election Study (ANES), we show that accounting for nonignorable nonresponse produces substantively meaningful shifts for life satisfaction (estimated latent correlation $ρ\\approx 0.49$), while yielding negligible changes for retrospective economic evaluations ($ρ\\approx 0$), highlighting when nonignorable nonresponse substantively affects survey estimates."}
{"id": "2602.08643", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.08643", "abs": "https://arxiv.org/abs/2602.08643", "authors": ["Max Rubinstein", "Megan S. Schuler", "Elizabeth A. Stuart", "Bradley D. Stein", "Max Griswold", "Elizabeth M. Stone", "Beth Ann Griffin"], "title": "State policy heterogeneity analyses: considerations and proposals", "comment": null, "summary": "State-level policy studies often conduct heterogeneity analyses that quantify how treatment effects vary across state characteristics. These analyses may be used to inform state-specific policy decisions, or to infer how the effect of a policy changes in combination with other state characteristics. However, in state-level settings with varied contexts and policy landscapes, multiple versions of similar policies, and differential policy implementation, the causal quantities targeted by these analyses may not align with the inferential goals. This paper clarifies these issues by distinguishing several causal estimands relevant to heterogeneity analyses in state-policy settings, including state-specific treatment effects (ITE), conditional average treatment effects (CATE), and controlled direct effects (CDE). We argue that the CATE is often the easiest to identify and estimate, but may not be the most policy relevant target of inference. Moreover, the widespread practice of coarsening distinct policies or implementations into a single indicator further complicates the interpretation of these analyses. Motivated by these limitations, we propose bounding ITEs as an alternative inferential goal, yielding ranges for each state's policy effect under explicit assumptions that quantify deviations from the ideal identifying conditions. These bounds target a well-defined and policy-relevant quantity, the effect for specific states. We develop this approach within a difference-in-differences framework and discuss how sensitivity parameters may be informed using pre-treatment data. Through simulations we demonstrate that bounding state-specific effects can more reliably determine the sign of the ITEs than CATE estimates. We then illustrate this method to examine the effect of the Affordable Care Act Medicaid expansion on high-volume buprenorphine prescribing."}
{"id": "2602.08259", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08259", "abs": "https://arxiv.org/abs/2602.08259", "authors": ["Xintao Xia", "Zhiqiu Xia", "Linjun Zhang", "Zhanrui Cai"], "title": "A Statistical Framework for Alignment with Biased AI Feedback", "comment": null, "summary": "Modern alignment pipelines are increasingly replacing expensive human preference labels with evaluations from large language models (LLM-as-Judge). However, AI labels can be systematically biased compared to high-quality human feedback datasets. In this paper, we develop two debiased alignment methods within a general framework that accommodates heterogeneous prompt-response distributions and external human feedback sources. Debiased Direct Preference Optimization (DDPO) augments standard DPO with a residual-based correction and density-ratio reweighting to mitigate systematic bias, while retaining DPO's computational efficiency. Debiased Identity Preference Optimization (DIPO) directly estimates human preference probabilities without imposing a parametric reward model. We provide theoretical guarantees for both methods: DDPO offers a practical and computationally efficient solution for large-scale alignment, whereas DIPO serves as a robust, statistically optimal alternative that attains the semiparametric efficiency bound. Empirical studies on sentiment generation, summarization, and single-turn dialogue demonstrate that the proposed methods substantially improve alignment efficiency and recover performance close to that of an oracle trained on fully human-labeled data."}
{"id": "2602.07707", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07707", "abs": "https://arxiv.org/abs/2602.07707", "authors": ["Chak Kwong", "Cheng", "Hakan Demirtas"], "title": "Generation of Multivariate Discrete Data with Generalized Poisson, Negative Binomial and Binomial Marginal Distributions", "comment": null, "summary": "The analysis of multivariate discrete data is crucial in various scientific research areas, such as epidemiology, the social sciences, genomics, and environmental studies. As the availability of such data increases, developing robust analytical and data generation tools is necessary to understand the relationships among variables. This paper builds upon previous work on data generation frameworks for multivariate ordinal data with a prespecified correlation matrix. The proposed algorithm generates multivariate discrete data from marginal distributions that follow the generalized Poisson, negative binomial, and binomial distributions. A step-by-step algorithm is provided, and its performance is illustrated in four simulated data scenarios and three real-data scenarios. This technique has the potential to be applied in a wide range of settings involving the generation of correlated discrete data."}
{"id": "2602.08318", "categories": ["stat.ML", "cs.LG", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.08318", "abs": "https://arxiv.org/abs/2602.08318", "authors": ["Soon Hoe Lim", "Shizheng Lin", "Michael W. Mahoney", "N. Benjamin Erichson"], "title": "Is Flow Matching Just Trajectory Replay for Sequential Data?", "comment": "51 pages", "summary": "Flow matching (FM) is increasingly used for time-series generation, but it is not well understood whether it learns a general dynamical structure or simply performs an effective \"trajectory replay\". We study this question by deriving the velocity field targeted by the empirical FM objective on sequential data, in the limit of perfect function approximation. For the Gaussian conditional paths commonly used in practice, we show that the implied sampler is an ODE whose dynamics constitutes a nonparametric, memory-augmented continuous-time dynamical system. The optimal field admits a closed-form expression as a similarity-weighted mixture of instantaneous velocities induced by past transitions, making the dataset dependence explicit and interpretable. This perspective positions neural FM models trained by stochastic optimization as parametric surrogates of an ideal nonparametric solution. Using the structure of the optimal field, we study sampling and approximation schemes that improve the efficiency and numerical robustness of ODE-based generation. On nonlinear dynamical system benchmarks, the resulting closed-form sampler yields strong probabilistic forecasts directly from historical transitions, without training."}
{"id": "2602.07740", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07740", "abs": "https://arxiv.org/abs/2602.07740", "authors": ["Buddhananda Banerjee", "Surojit Biswas", "Daitari Prusty"], "title": "Hyperbolic statistical inference for Treatment Effects with Circular biomarker of astigmatism", "comment": null, "summary": "Circular biomarkers arise naturally in many biomedical applications, particularly in ophthalmology, where angular measurements such as astigmatism are routinely recorded. Similar directional variables also occur in the study of human body rotations, including movements of the hand, waist, neck, and lower limbs. Motivated by a clinical dataset comprising angular measurements of astigmatism induced by two cataract surgery procedures, we propose a novel two-sample testing framework for circular data grounded in hyperbolic geometry. Assuming von Mises distributions with either common or group-specific concentration parameters, we embed the corresponding parameter spaces into the Poincaré disk, an open unit disk endowed with the Poincaré metric.Under this construction, each von Mises distribution is mapped uniquely to a point in the Poincaré disk, yielding a continuous geometric representation that preserves the intrinsic structure of the parameter space. This embedding enables direct comparison of group distributions via hyperbolic distances, leading to natural and interpretable test statistics. We develop permutation-based tests for the common concentration case and bootstrap-based procedures for unequal concentrations. Extensive simulation studies demonstrate stable empirical size, strong consistency, and superior asymptotic power compared with existing competing methods. The proposed methodology is illustrated through a detailed analysis of the cataract surgery dataset, including a clinically informed restructuring of the original observations. The results highlight the practical advantages of incorporating hyperbolic geometry into the analysis of circular biomedical data and underscore the potential of geometry-aware inference for directional biomarkers."}
{"id": "2602.08374", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.08374", "abs": "https://arxiv.org/abs/2602.08374", "authors": ["Denis Belomestny", "Alexey Naumov", "Nikita Puchkin", "Denis Suchkov"], "title": "Schrödinger bridge problem via empirical risk minimization", "comment": null, "summary": "We study the Schrödinger bridge problem when the endpoint distributions are available only through samples. Classical computational approaches estimate Schrödinger potentials via Sinkhorn iterations on empirical measures and then construct a time-inhomogeneous drift by differentiating a kernel-smoothed dual solution. In contrast, we propose a learning-theoretic route: we rewrite the Schrödinger system in terms of a single positive transformed potential that satisfies a nonlinear fixed-point equation and estimate this potential by empirical risk minimization over a function class. We establish uniform concentration of the empirical risk around its population counterpart under sub-Gaussian assumptions on the reference kernel and terminal density. We plug the learned potential into a stochastic control representation of the bridge to generate samples. We illustrate performance of the suggested approach with numerical experiments."}
{"id": "2602.07825", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07825", "abs": "https://arxiv.org/abs/2602.07825", "authors": ["John W. Jackson", "Ting-Hsuan Chang", "Aster Meche", "Trang Q. Nguyen"], "title": "Estimation Strategies for Causal Decomposition Analysis with Allowability Specifications", "comment": null, "summary": "Causal decomposition analysis (CDA) is an approach for modeling the impact of hypothetical interventions to reduce disparities. It is useful for identifying foci that future interventions, including multilevel and multimodal interventions, could focus on to reduce disparities. Based within the potential outcomes framework, CDA has a causal interpretation when the identifying assumptions are met. CDA also allows an analyst to consider which covariates are allowable (i.e., fair) for defining the disparity in the outcome and in the point of intervention, so that its interpretation is also meaningful. While the incorporation of causal inference and allowability promotes robustness, transparency, and dialogue in disparities research, it can lead to challenges in estimation such as the need to correctly model densities. Also, how CDA differs from commonly used estimators may not be clear, which may limit its uptake. To address these challenges, we provide a tour of estimation strategies for CDA, reviewing existing proposals and introducing novel estimators that overcome key estimation challenges. Among them we introduce what we call \"bridging\" estimators that avoid directly modeling any density, and weighted sequential regression estimators that are multiply robust. Additionally, we provide diagnostics to assess the quality of the nuisance density models and weighting functions they rely on. We formally establish the estimators' robustness to model mis-specification, demonstrate their performance through a simulation study based on real data, and apply them to study disparities in hypertension control using electronic health records in a large healthcare system."}
{"id": "2602.08782", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08782", "abs": "https://arxiv.org/abs/2602.08782", "authors": ["Tommy Rochussen", "Vincent Fortuin"], "title": "Amortising Inference and Meta-Learning Priors in Neural Networks", "comment": "Accepted at ICLR 2026", "summary": "One of the core facets of Bayesianism is in the updating of prior beliefs in light of new evidence$\\text{ -- }$so how can we maintain a Bayesian approach if we have no prior beliefs in the first place? This is one of the central challenges in the field of Bayesian deep learning, where it is not clear how to represent beliefs about a prediction task by prior distributions over model parameters. Bridging the fields of Bayesian deep learning and probabilistic meta-learning, we introduce a way to $\\textit{learn}$ a weights prior from a collection of datasets by introducing a way to perform per-dataset amortised variational inference. The model we develop can be viewed as a neural process whose latent variable is the set of weights of a BNN and whose decoder is the neural network parameterised by a sample of the latent variable itself. This unique model allows us to study the behaviour of Bayesian neural networks under well-specified priors, use Bayesian neural networks as flexible generative models, and perform desirable but previously elusive feats in neural processes such as within-task minibatching or meta-learning under extreme data-starvation."}
{"id": "2602.08096", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08096", "abs": "https://arxiv.org/abs/2602.08096", "authors": ["Brian M Cho", "Raaz Dwivedi", "Nathan Kallus"], "title": "GAAVI: Global Asymptotic Anytime Valid Inference for the Conditional Mean Function", "comment": null, "summary": "Inference on the conditional mean function (CMF) is central to tasks from adaptive experimentation to optimal treatment assignment and algorithmic fairness auditing. In this work, we provide a novel asymptotic anytime-valid test for a CMF global null (e.g., that all conditional means are zero) and contrasts between CMFs, enabling experimenters to make high confidence decisions at any time during the experiment beyond a minimum sample size. We provide mild conditions under which our tests achieve (i) asymptotic type-I error guarantees, (i) power one, and, unlike past tests, (iii) optimal sample complexity relative to a Gaussian location testing. By inverting our tests, we show how to construct function-valued asymptotic confidence sequences for the CMF and contrasts thereof. Experiments on both synthetic and real-world data show our method is well-powered across various distributions while preserving the nominal error rate under continuous monitoring."}
{"id": "2602.08849", "categories": ["stat.ML", "cond-mat.mtrl-sci", "cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.08849", "abs": "https://arxiv.org/abs/2602.08849", "authors": ["Terry C. W. Lam", "Niamh O'Neill", "Christoph Schran", "Lars L. Schaaf"], "title": "Cutting Through the Noise: On-the-fly Outlier Detection for Robust Training of Machine Learning Interatomic Potentials", "comment": "12 pages, 6 figures", "summary": "The accuracy of machine learning interatomic potentials suffers from reference data that contains numerical noise. Often originating from unconverged or inconsistent electronic-structure calculations, this noise is challenging to identify. Existing mitigation strategies such as manual filtering or iterative refinement of outliers, require either substantial expert effort or multiple expensive retraining cycles, making them difficult to scale to large datasets. Here, we introduce an on-the-fly outlier detection scheme that automatically down-weights noisy samples, without requiring additional reference calculations. By tracking the loss distribution via an exponential moving average, this unsupervised method identifies outliers throughout a single training run. We show that this approach prevents overfitting and matches the performance of iterative refinement baselines with significantly reduced overhead. The method's effectiveness is demonstrated by recovering accurate physical observables for liquid water from unconverged reference data, including diffusion coefficients. Furthermore, we validate its scalability by training a foundation model for organic chemistry on the SPICE dataset, where it reduces energy errors by a factor of three. This framework provides a simple, automated solution for training robust models on imperfect datasets across dataset sizes."}
{"id": "2602.08108", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08108", "abs": "https://arxiv.org/abs/2602.08108", "authors": ["Juan Carlos Escanciano", "Jacobo de Uña-Álvarez"], "title": "Goodness-of-Fit Tests for Censored and Truncated Data: Maximum Mean Discrepancy Over Regular Functionals", "comment": null, "summary": "We develop a systematic, omnibus approach to goodness-of-fit testing for parametric distributional models when the variable of interest is only partially observed due to censoring and/or truncation. In many such designs, tests based on the nonparametric maximum likelihood estimator are hindered by nonexistence, computational instability, or convergence rates too slow to support reliable calibration under composite nulls. We avoid these difficulties by constructing a regular (pathwise differentiable) Neyman-orthogonal score process indexed by test functions, and aggregating it over a reproducing kernel Hilbert space ball. This yields a maximum-mean-discrepancy-type supremum statistic with a convenient quadratic-form representation. Critical values are obtained via a multiplier bootstrap that keeps nuisance estimates fixed. We establish asymptotic validity under the null and local alternatives and provide concrete constructions for left-truncated right-censored data, current status data, and random double truncation; in particular, to the best of our knowledge, we give the first omnibus goodness-of-fit test for a parametric family under random double truncation in the composite-hypothesis case. Simulations and an empirical illustration demonstrate size control and power in practically relevant incomplete-data designs."}
{"id": "2602.08892", "categories": ["stat.ML", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.08892", "abs": "https://arxiv.org/abs/2602.08892", "authors": ["Hamsa Bastani", "Osbert Bastani", "Bryce McLaughlin"], "title": "Winner's Curse Drives False Promises in Data-Driven Decisions: A Case Study in Refugee Matching", "comment": null, "summary": "A major challenge in data-driven decision-making is accurate policy evaluation-i.e., guaranteeing that a learned decision-making policy achieves the promised benefits. A popular strategy is model-based policy evaluation, which estimates a model from data to infer counterfactual outcomes. This strategy is known to produce unwarrantedly optimistic estimates of the true benefit due to the winner's curse. We searched the recent literature on data-driven decision-making, identifying a sample of 55 papers published in the Management Science in the past decade; all but two relied on this flawed methodology. Several common justifications are provided: (1) the estimated models are accurate, stable, and well-calibrated, (2) the historical data uses random treatment assignment, (3) the model family is well-specified, and (4) the evaluation methodology uses sample splitting. Unfortunately, we show that no combination of these justifications avoids the winner's curse. First, we provide a theoretical analysis demonstrating that the winner's curse can cause large, spurious reported benefits even when all these justifications hold. Second, we perform a simulation study based on the recent and consequential data-driven refugee matching problem. We construct a synthetic refugee matching environment (calibrated to closely match the real setting) but designed so that no assignment policy can improve expected employment compared to random assignment. Model-based methods report large, stable gains of around 60% even when the true effect is zero; these gains are on par with improvements of 22-75% reported in the literature. Our results provide strong evidence against model-based evaluation."}
{"id": "2602.08212", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08212", "abs": "https://arxiv.org/abs/2602.08212", "authors": ["Jacob Tennenbaum", "Adam Kapelner"], "title": "Improved Conditional Logistic Regression using Information in Concordant Pairs with Software", "comment": "16 pages, 9 tables", "summary": "We develop an improvement to conditional logistic regression (CLR) in the setting where the parameter of interest is the additive effect of binary treatment effect on log-odds of the positive level in the binary response. Our improvement is simply to use information learned above the nuisance control covariates found in the concordant response pairs' observations (which is usually discarded) to create an informative prior on their coefficients. This prior is then used in the CLR which is run on the discordant pairs. Our power improvements over CLR are most notable in small sample sizes and in nonlinear log-odds-of-positive-response models. Our methods are released in an optimized R package called bclogit."}
{"id": "2602.08927", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08927", "abs": "https://arxiv.org/abs/2602.08927", "authors": ["Rohan Hore", "Ruodu Wang", "Aaditya Ramdas"], "title": "Online monotone density estimation and log-optimal calibration", "comment": "28 pages, 1 figure", "summary": "We study the problem of online monotone density estimation, where density estimators must be constructed in a predictable manner from sequentially observed data. We propose two online estimators: an online analogue of the classical Grenander estimator, and an expert aggregation estimator inspired by exponential weighting methods from the online learning literature. In the well-specified stochastic setting, where the underlying density is monotone, we show that the expected cumulative log-likelihood gap between the online estimators and the true density admits an $O(n^{1/3})$ bound. We further establish a $\\sqrt{n\\log{n}}$ pathwise regret bound for the expert aggregation estimator relative to the best offline monotone estimator chosen in hindsight, under minimal regularity assumptions on the observed sequence. As an application of independent interest, we show that the problem of constructing log-optimal p-to-e calibrators for sequential hypothesis testing can be formulated as an online monotone density estimation problem. We adapt the proposed estimators to build empirically adaptive p-to-e calibrators and establish their optimality. Numerical experiments illustrate the theoretical results."}
{"id": "2602.08347", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.08347", "abs": "https://arxiv.org/abs/2602.08347", "authors": ["Takato Hashino", "Koji Tsukuda"], "title": "Estimating the Shannon Entropy Using the Pitman--Yor Process", "comment": "23 pages, 5 figures", "summary": "The Shannon entropy is a fundamental measure for quantifying diversity and model complexity in fields such as information theory, ecology, and genetics. However, many existing studies assume that the number of species is known, an assumption that is often unrealistic in practice. In recent years, efforts have been made to relax this restriction. Motivated by these developments, this study proposes an entropy estimation method based on the Pitman--Yor process, a representative approach in Bayesian nonparametrics. By approximating the true distribution as an infinite-dimensional process, the proposed method enables stable estimation even when the number of observed species is smaller than the true number of species. This approach provides a principled way to deal with the uncertainty in species diversity and enhances the reliability and robustness of entropy-based diversity assessment. In addition, we investigate the convergence property of the Shannon entropy for regularly varying distributions and use this result to establish the consistency of the proposed estimator. Finally, we demonstrate the effectiveness of the proposed method through numerical experiments."}
{"id": "2602.08933", "categories": ["stat.ML", "cs.LG", "cs.NE", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08933", "abs": "https://arxiv.org/abs/2602.08933", "authors": ["Abhik Ghosh", "Suryasis Jana"], "title": "Provably robust learning of regression neural networks using $β$-divergences", "comment": "Pre-print, under review", "summary": "Regression neural networks (NNs) are most commonly trained by minimizing the mean squared prediction error, which is highly sensitive to outliers and data contamination. Existing robust training methods for regression NNs are often limited in scope and rely primarily on empirical validation, with only a few offering partial theoretical guarantees. In this paper, we propose a new robust learning framework for regression NNs based on the $β$-divergence (also known as the density power divergence) which we call `rRNet'. It applies to a broad class of regression NNs, including models with non-smooth activation functions and error densities, and recovers the classical maximum likelihood learning as a special case. The rRNet is implemented via an alternating optimization scheme, for which we establish convergence guarantees to stationary points under mild, verifiable conditions. The (local) robustness of rRNet is theoretically characterized through the influence functions of both the parameter estimates and the resulting rRNet predictor, which are shown to be bounded for suitable choices of the tuning parameter $β$, depending on the error density. We further prove that rRNet attains the optimal 50\\% asymptotic breakdown point at the assumed model for all $β\\in(0, 1]$, providing a strong global robustness guarantee that is largely absent for existing NN learning methods. Our theoretical results are complemented by simulation experiments and real-data analyses, illustrating practical advantages of rRNet over existing approaches in both function approximation problems and prediction tasks with noisy observations."}
{"id": "2602.08413", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08413", "abs": "https://arxiv.org/abs/2602.08413", "authors": ["Xiang Ye", "Janet Van Niekerk", "Haavard Rue"], "title": "A Bayesian regression framework for circular models with INLA", "comment": "19 pages, 12 figures", "summary": "Regression models for circular variables are less developed, since the concept of building a linear predictor from linear combinations of covariates and various random effects, breaks the circular nature of the variable. In this paper, we introduce a new approach to rectify this issue, leading to well-defined regression models for circular responses when the data are concentrated. Our approach extends naturally to joint regression models where we can have several circular and non-circular responses, and allow us to handle a mix of linear covariates, circular covariates and various random effects. Our formulation aligns naturally with the integrated nested Laplace approximation (INLA), which provides fast and accurate Bayesian inference. We illustrate our approach through several simulated and real examples."}
{"id": "2602.08544", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.08544", "abs": "https://arxiv.org/abs/2602.08544", "authors": ["Luca Presicce", "Sudipto Banerjee"], "title": "Adaptive Markovian Spatiotemporal Transfer Learning in Multivariate Bayesian Modeling", "comment": null, "summary": "This manuscript develops computationally efficient online learning for multivariate spatiotemporal models. The method relies on matrix-variate Gaussian distributions, dynamic linear models, and Bayesian predictive stacking to efficiently share information across temporal data shards. The model facilitates effective information propagation over time while seamlessly integrating spatial components within a dynamic framework, building a Markovian dependence structure between datasets at successive time instants. This structure supports flexible, high-dimensional modeling of complex dependence patterns, as commonly found in spatiotemporal phenomena, where computational challenges arise rapidly with increasing dimensions. The proposed approach further manages exact inference through predictive stacking, enhancing accuracy and interoperability. Combining sequential and parallel processing of temporal shards, each unit passes assimilated information forward, then back-smoothed to improve posterior estimates, incorporating all available information. This framework advances the scalability and adaptability of spatiotemporal modeling, making it suitable for dynamic, multivariate, and data-rich environments."}
{"id": "2602.07098", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07098", "abs": "https://arxiv.org/abs/2602.07098", "authors": ["Lars Kühmichel", "Jerry M. Huang", "Valentin Pratz", "Jonas Arruda", "Hans Olischläger", "Daniel Habermann", "Simon Kucharsky", "Lasse Elsemüller", "Aayush Mishra", "Niels Bracher", "Svenja Jedhoff", "Marvin Schmitt", "Paul-Christian Bürkner", "Stefan T. Radev"], "title": "BayesFlow 2.0: Multi-Backend Amortized Bayesian Inference in Python", "comment": null, "summary": "Modern Bayesian inference involves a mixture of computational methods for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows. An overarching motif of many Bayesian methods is that they are relatively slow, which often becomes prohibitive when fitting complex models to large data sets. Amortized Bayesian inference (ABI) offers a path to solving the computational challenges of Bayes. ABI trains neural networks on model simulations, rewarding users with rapid inference of any model-implied quantity, such as point estimates, likelihoods, or full posterior distributions. In this work, we present the Python library BayesFlow, Version 2.0, for general-purpose ABI. Along with direct posterior, likelihood, and ratio estimation, the software includes support for multiple popular deep learning backends, a rich collection of generative networks for sampling and density estimation, complete customization and high-level interfaces, as well as new capabilities for hyperparameter optimization, design optimization, and hierarchical modeling. Using a case study on dynamical system parameter estimation, combined with comparisons to similar software, we show that our streamlined, user-friendly workflow has strong potential to support broad adoption."}
{"id": "2602.08643", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.08643", "abs": "https://arxiv.org/abs/2602.08643", "authors": ["Max Rubinstein", "Megan S. Schuler", "Elizabeth A. Stuart", "Bradley D. Stein", "Max Griswold", "Elizabeth M. Stone", "Beth Ann Griffin"], "title": "State policy heterogeneity analyses: considerations and proposals", "comment": null, "summary": "State-level policy studies often conduct heterogeneity analyses that quantify how treatment effects vary across state characteristics. These analyses may be used to inform state-specific policy decisions, or to infer how the effect of a policy changes in combination with other state characteristics. However, in state-level settings with varied contexts and policy landscapes, multiple versions of similar policies, and differential policy implementation, the causal quantities targeted by these analyses may not align with the inferential goals. This paper clarifies these issues by distinguishing several causal estimands relevant to heterogeneity analyses in state-policy settings, including state-specific treatment effects (ITE), conditional average treatment effects (CATE), and controlled direct effects (CDE). We argue that the CATE is often the easiest to identify and estimate, but may not be the most policy relevant target of inference. Moreover, the widespread practice of coarsening distinct policies or implementations into a single indicator further complicates the interpretation of these analyses. Motivated by these limitations, we propose bounding ITEs as an alternative inferential goal, yielding ranges for each state's policy effect under explicit assumptions that quantify deviations from the ideal identifying conditions. These bounds target a well-defined and policy-relevant quantity, the effect for specific states. We develop this approach within a difference-in-differences framework and discuss how sensitivity parameters may be informed using pre-treatment data. Through simulations we demonstrate that bounding state-specific effects can more reliably determine the sign of the ITEs than CATE estimates. We then illustrate this method to examine the effect of the Affordable Care Act Medicaid expansion on high-volume buprenorphine prescribing."}
{"id": "2602.07477", "categories": ["stat.ME", "cs.LG", "stat.AP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07477", "abs": "https://arxiv.org/abs/2602.07477", "authors": ["Lena Schemet", "Sarah Friedrich-Welz"], "title": "Statistical inference after variable selection in Cox models: A simulation study", "comment": null, "summary": "Choosing relevant predictors is central to the analysis of biomedical time-to-event data. Classical frequentist inference, however, presumes that the set of covariates is fixed in advance and does not account for data-driven variable selection. As a consequence, naive post-selection inference may be biased and misleading. In right-censored survival settings, these issues may be further exacerbated by the additional uncertainty induced by censoring. We investigate several inference procedures applied after variable selection for the coefficients of the Lasso and its extension, the adaptive Lasso, in the context of the Cox model. The methods considered include sample splitting, exact post-selection inference, and the debiased Lasso. Their performance is examined in a neutral simulation study reflecting realistic covariate structures and censoring rates commonly encountered in biomedical applications. To complement the simulation results, we illustrate the practical behavior of these procedures in an applied example using a publicly available survival dataset."}
{"id": "2602.08647", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08647", "abs": "https://arxiv.org/abs/2602.08647", "authors": ["Yuta Kawakami", "Jin Tian"], "title": "Measures for Assessing Causal Effect Heterogeneity Unexplained by Covariates", "comment": null, "summary": "There has been considerable interest in estimating heterogeneous causal effects across individuals or subpopulations. Researchers often assess causal effect heterogeneity based on the subjects' covariates using the conditional average causal effect (CACE). However, substantial heterogeneity may persist even after accounting for the covariates. Existing work on causal effect heterogeneity unexplained by covariates mainly focused on binary treatment and outcome. In this paper, we introduce novel heterogeneity measures, P-CACE and N-CACE, for binary treatment and continuous outcome that represent CACE over the positively and negatively affected subjects, respectively. We also introduce new heterogeneity measures, P-CPICE and N-CPICE, for continuous treatment and continuous outcome by leveraging stochastic interventions, expanding causal questions that researchers can answer. We establish identification and bounding theorems for these new measures. Finally, we show their application to a real-world dataset."}
{"id": "2602.07613", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07613", "abs": "https://arxiv.org/abs/2602.07613", "authors": ["Jiuyao Lu", "Tianruo Zhang", "Ke Zhu"], "title": "Fast Rerandomization for Balancing Covariates in Randomized Experiments: A Metropolis-Hastings Framework", "comment": null, "summary": "Balancing covariates is critical for credible and efficient randomized experiments. Rerandomization addresses this by repeatedly generating treatment assignments until covariate balance meets a prespecified threshold. By shrinking this threshold, it can achieve arbitrarily strong balance, with established results guaranteeing optimal estimation and valid inference in both finite-sample and asymptotic settings across diverse complex experimental settings. Despite its rigorous theoretical foundations, practical use is limited by the extreme inefficiency of rejection sampling, which becomes prohibitively slow under small thresholds and often forces practitioners to adopt suboptimal settings, leading to degraded performance. Existing work focusing on acceleration typically fail to maintain the uniformity over the acceptable assignment space, thus losing the theoretical grounds of classical rerandomization. Building upon a Metropolis-Hastings framework, we address this challenge by introducing an additional sampling-importance resampling step, which restores uniformity and preserves statistical guarantees. Our proposed algorithm, PSRSRR, achieves speedups ranging from 10 to 10,000 times while maintaining exact and asymptotic validity, as demonstrated by simulations and two real-data applications."}
{"id": "2602.08865", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08865", "abs": "https://arxiv.org/abs/2602.08865", "authors": ["Rishikesh Yadav", "Arnab Hazra"], "title": "Regression modeling of multivariate precipitation extremes under regular variation", "comment": "15 Pages, 3 figures, 1 table", "summary": "Motivated by the EVA2025 data challenge, where we participated as the team DesiBoys, we propose a regression strategy within the framework of regular variation to estimate the occurrences and intensities of high precipitation extremes derived from different climate runs of the CESM2 Large Ensemble Community Project (LENS2). Our approach first empirically estimates the target quantities at sub-asymptotic (lower threshold) levels and sets them as response variables within a simple regression framework arising from the theoretical expressions of joint regular variation. Although a seasonal pattern is evident in the data, the precipitation intensities do not exhibit any significant long-term trends across years. Besides, we can safely assume the data to be independent across different climate model runs, thereby simplifying the modeling framework. Once the regression parameters are estimated, we employ a standard prediction approach to infer precipitation levels at very high quantiles. We calculate the confidence intervals using a nonparametric block bootstrap procedure. While a likelihood-based inference grounded in multivariate extreme value theory may provide more accurate estimates and confidence intervals, it would involve a significantly higher computational burden. Our proposed simple and computationally straightforward two-stage approach provides reasonable estimates for the desired quantities, securing us a joint second position in the final rankings of the EVA2025 conference data challenge competition."}
{"id": "2602.08096", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08096", "abs": "https://arxiv.org/abs/2602.08096", "authors": ["Brian M Cho", "Raaz Dwivedi", "Nathan Kallus"], "title": "GAAVI: Global Asymptotic Anytime Valid Inference for the Conditional Mean Function", "comment": null, "summary": "Inference on the conditional mean function (CMF) is central to tasks from adaptive experimentation to optimal treatment assignment and algorithmic fairness auditing. In this work, we provide a novel asymptotic anytime-valid test for a CMF global null (e.g., that all conditional means are zero) and contrasts between CMFs, enabling experimenters to make high confidence decisions at any time during the experiment beyond a minimum sample size. We provide mild conditions under which our tests achieve (i) asymptotic type-I error guarantees, (i) power one, and, unlike past tests, (iii) optimal sample complexity relative to a Gaussian location testing. By inverting our tests, we show how to construct function-valued asymptotic confidence sequences for the CMF and contrasts thereof. Experiments on both synthetic and real-world data show our method is well-powered across various distributions while preserving the nominal error rate under continuous monitoring."}
{"id": "2602.07165", "categories": ["stat.CO", "physics.data-an", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07165", "abs": "https://arxiv.org/abs/2602.07165", "authors": ["Matthew LeDuc", "Tomoko Matsuo"], "title": "PoissonRatioUQ: An R package for band ratio uncertainty quantification", "comment": "Description of the R package in https://github.com/mfleduc/PoissonRatioUQ", "summary": "We introduce an R package for Bayesian modeling and uncertainty quantification for problems involving count ratios. The modeling relies on the assumption that the quantity of interest is the ratio of Poisson means rather than the ratio of counts. We provide multiple different options for retrieval of this quantity for problems with and without spatial information included. Some added capability for uncertainty quantification for problems of the form $Z=(mT+z_0)^{p}$, where $Z$ is the intensity ratio and $T$ the quantity of interest, is included."}
{"id": "2602.07633", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07633", "abs": "https://arxiv.org/abs/2602.07633", "authors": ["Trevor Harris"], "title": "Flow-Based Conformal Predictive Distributions", "comment": "9 pages, 6 figures, 10 appendix pages", "summary": "Conformal prediction provides a distribution-free framework for uncertainty quantification via prediction sets with exact finite-sample coverage. In low dimensions these sets are easy to interpret, but in high-dimensional or structured output spaces they are difficult to represent and use, which can limit their ability to integrate with downstream tasks such as sampling and probabilistic forecasting. We show that any differentiable nonconformity score induces a deterministic flow on the output space whose trajectories converge to the boundary of the corresponding conformal prediction set. This leads to a computationally efficient, training-free method for sampling conformal boundaries in arbitrary dimensions. Boundary samples can be reconformalized to form pointwise prediction sets with controlled risk, and mixing across confidence levels yields conformal predictive distributions whose quantile regions coincide exactly with conformal prediction sets. We evaluate the approach on PDE inverse problems, precipitation downscaling, climate model debiasing, and hurricane trajectory forecasting."}
{"id": "2602.07767", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07767", "abs": "https://arxiv.org/abs/2602.07767", "authors": ["Ruizhe Deng", "Bibhas Chakraborty", "Ran Chen", "Yan Shuo Tan"], "title": "BFTS: Thompson Sampling with Bayesian Additive Regression Trees", "comment": null, "summary": "Contextual bandits are a core technology for personalized mobile health interventions, where decision-making requires adapting to complex, non-linear user behaviors. While Thompson Sampling (TS) is a preferred strategy for these problems, its performance hinges on the quality of the underlying reward model. Standard linear models suffer from high bias, while neural network approaches are often brittle and difficult to tune in online settings. Conversely, tree ensembles dominate tabular data prediction but typically rely on heuristic uncertainty quantification, lacking a principled probabilistic basis for TS. We propose Bayesian Forest Thompson Sampling (BFTS), the first contextual bandit algorithm to integrate Bayesian Additive Regression Trees (BART), a fully probabilistic sum-of-trees model, directly into the exploration loop. We prove that BFTS is theoretically sound, deriving an information-theoretic Bayesian regret bound of $\\tilde{O}(\\sqrt{T})$. As a complementary result, we establish frequentist minimax optimality for a \"feel-good\" variant, confirming the structural suitability of BART priors for non-parametric bandits. Empirically, BFTS achieves state-of-the-art regret on tabular benchmarks with near-nominal uncertainty calibration. Furthermore, in an offline policy evaluation on the Drink Less micro-randomized trial, BFTS improves engagement rates by over 30% compared to the deployed policy, demonstrating its practical effectiveness for behavioral interventions."}
{"id": "2602.07785", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07785", "abs": "https://arxiv.org/abs/2602.07785", "authors": ["Yufei Zhang", "Zhihao Ma"], "title": "Digital exclusion among middle-aged and older adults in China: age-period-cohort evidence from three national surveys, 2011-2022", "comment": "Also available as a preprint on OSF Preprints:https://osf.io/hpv68_v1", "summary": "Amid China's ageing and digital shift, digital exclusion among older adults poses an urgent challenge. To unpack this phenomenon, this study disentangles age, period, and cohort effects on digital exclusion among middle-aged and older Chinese adults. Using three nationally representative surveys (CHARLS 2011-2020, CFPS 2010-2022, and CGSS 2010-2021), we fitted hierarchical age-period-cohort (HAPC) models weighted by cross-sectional survey weights and stabilized inverse probability weights for item response. We further assessed heterogeneity by urban-rural residence, region, multimorbidity, and cognitive risk, and evaluated robustness with APC bounding analyses. Across datasets, digital exclusion increased with age and displayed mild non-linearity, with a small midlife easing followed by a sharper rise at older ages. Period effects declined over the 2010s and early 2020s, although the pace of improvement differed across survey windows. Cohort deviations were present but less consistent than age and period patterns, with an additional excess risk concentrated among cohorts born in the 1950s. Rural and western residents, as well as adults with multimorbidity or cognitive risk, remained consistently more excluded. Over the study period, the urban-rural divide showed no evidence of narrowing, whereas the cognitive-risk gap widened. These findings highlight digital inclusion as a vital pathway for older adults to remain integral participants in an evolving digital society."}
{"id": "2602.07997", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07997", "abs": "https://arxiv.org/abs/2602.07997", "authors": ["TrungKhang Tran", "TrungTin Nguyen", "Md Abul Bashar", "Nhat Ho", "Richi Nayak", "Christopher Drovandi"], "title": "Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models", "comment": "TrungKhang Tran and TrungTin Nguyen are co-first authors", "summary": "Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines."}
{"id": "2602.08927", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08927", "abs": "https://arxiv.org/abs/2602.08927", "authors": ["Rohan Hore", "Ruodu Wang", "Aaditya Ramdas"], "title": "Online monotone density estimation and log-optimal calibration", "comment": "28 pages, 1 figure", "summary": "We study the problem of online monotone density estimation, where density estimators must be constructed in a predictable manner from sequentially observed data. We propose two online estimators: an online analogue of the classical Grenander estimator, and an expert aggregation estimator inspired by exponential weighting methods from the online learning literature. In the well-specified stochastic setting, where the underlying density is monotone, we show that the expected cumulative log-likelihood gap between the online estimators and the true density admits an $O(n^{1/3})$ bound. We further establish a $\\sqrt{n\\log{n}}$ pathwise regret bound for the expert aggregation estimator relative to the best offline monotone estimator chosen in hindsight, under minimal regularity assumptions on the observed sequence. As an application of independent interest, we show that the problem of constructing log-optimal p-to-e calibrators for sequential hypothesis testing can be formulated as an online monotone density estimation problem. We adapt the proposed estimators to build empirically adaptive p-to-e calibrators and establish their optimality. Numerical experiments illustrate the theoretical results."}
{"id": "2602.08933", "categories": ["stat.ML", "cs.LG", "cs.NE", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08933", "abs": "https://arxiv.org/abs/2602.08933", "authors": ["Abhik Ghosh", "Suryasis Jana"], "title": "Provably robust learning of regression neural networks using $β$-divergences", "comment": "Pre-print, under review", "summary": "Regression neural networks (NNs) are most commonly trained by minimizing the mean squared prediction error, which is highly sensitive to outliers and data contamination. Existing robust training methods for regression NNs are often limited in scope and rely primarily on empirical validation, with only a few offering partial theoretical guarantees. In this paper, we propose a new robust learning framework for regression NNs based on the $β$-divergence (also known as the density power divergence) which we call `rRNet'. It applies to a broad class of regression NNs, including models with non-smooth activation functions and error densities, and recovers the classical maximum likelihood learning as a special case. The rRNet is implemented via an alternating optimization scheme, for which we establish convergence guarantees to stationary points under mild, verifiable conditions. The (local) robustness of rRNet is theoretically characterized through the influence functions of both the parameter estimates and the resulting rRNet predictor, which are shown to be bounded for suitable choices of the tuning parameter $β$, depending on the error density. We further prove that rRNet attains the optimal 50\\% asymptotic breakdown point at the assumed model for all $β\\in(0, 1]$, providing a strong global robustness guarantee that is largely absent for existing NN learning methods. Our theoretical results are complemented by simulation experiments and real-data analyses, illustrating practical advantages of rRNet over existing approaches in both function approximation problems and prediction tasks with noisy observations."}
