{"id": "2511.05962", "categories": ["stat.ME", "math.CO", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.05962", "abs": "https://arxiv.org/abs/2511.05962", "authors": ["Kamillo Ferry"], "title": "Minimum bounding polytropes for estimation of max-linear Bayesian networks", "comment": "22 pages, 8 figures, 6 tables", "summary": "Max-linear Bayesian networks are recursive max-linear structural equation models represented by an edge weighted directed acyclic graph (DAG). The identifiability and estimation of max-linear Bayesian networks is an intricate issue as Gissibl, Klüppelberg, and Lauritzen have shown. As such, a max-linear Bayesian network is generally unidentifiable and standard likelihood theory cannot be applied. We can associate tropical polyhedra to max-linear Bayesian networks. Using this, we investigate the minimum-ratio estimator proposed by Gissibl, Klüppelberg, and Lauritzen and give insight on the structure of minimal best-case samples for parameter recovery which we describe in terms of set covers of certain triangulations. We also combine previous work on estimating max-linear models from Tran, Buck, and Klüppelberg to apply our geometric approach to the structural inference of max-linear models. This is tested extensively on simulated data and on real world data set, the NHANES report for 2015--2016 and the upper Danube network data."}
{"id": "2511.06189", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.06189", "abs": "https://arxiv.org/abs/2511.06189", "authors": ["Navonil Deb", "Raaz Dwivedi", "Sumanta Basu"], "title": "Counterfactual Forecasting For Panel Data", "comment": "32 pages, 6 figures", "summary": "We address the challenge of forecasting counterfactual outcomes in a panel data with missing entries and temporally dependent latent factors -- a common scenario in causal inference, where estimating unobserved potential outcomes ahead of time is essential. We propose Forecasting Counterfactuals under Stochastic Dynamics (FOCUS), a method that extends traditional matrix completion methods by leveraging time series dynamics of the factors, thereby enhancing the prediction accuracy of future counterfactuals. Building upon a PCA estimator, our method accommodates both stochastic and deterministic components within the factors, and provides a flexible framework for various applications. In case of stationary autoregressive factors and under standard conditions, we derive error bounds and establish asymptotic normality of our estimator. Empirical evaluations demonstrate that our method outperforms existing benchmarks when the latent factors have an autoregressive component. We illustrate FOCUS results on HeartSteps, a mobile health study, illustrating its effectiveness in forecasting step counts for users receiving activity prompts, thereby leveraging temporal patterns in user behavior."}
{"id": "2511.06542", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.06542", "abs": "https://arxiv.org/abs/2511.06542", "authors": ["Chaegeun Song", "Zhong Zheng", "Bing Li", "Lingzhou Xue"], "title": "Collapsing Categories for Regression with Mixed Predictors", "comment": "35 pages", "summary": "Categorical predictors are omnipresent in everyday regression practice: in fact, most regression data involve some categorical predictors, and this tendency is increasing in modern applications with more complex structures and larger data sizes. However, including too many categories in a regression model would seriously hamper accuracy, as the information in the data is fragmented by the multitude of categories. In this paper, we introduce a systematic method to reduce the complexity of categorical predictors by adaptively collapsing categories in regressions, so as to enhance the performance of regression estimation. Our method is based on the {\\em pairwise vector fused LASSO}, which automatically fuses the categories that bear a similar regression relation with the response. We develop our method under a wide class of regression models defined by a general loss function, which includes linear models and generalized linear models as special cases. We rigorously established the category collapsing consistency of our method, developed an Inexact Proximal Gradient Descent algorithm to implement it, and proved the feasibility and convergence of our algorithm. Through simulations and an application to Spotify music data, we demonstrate that our method can effectively reduce categorical complexity while improving prediction performance, making it a powerful tool for regression with mixed predictors."}
{"id": "2511.05983", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05983", "abs": "https://arxiv.org/abs/2511.05983", "authors": ["Connor Simpson", "Ricardo J. G. B. Campello", "Elizabeth Stojanovski"], "title": "Benchmarking of Clustering Validity Measures Revisited", "comment": "48 pages, 17 tables, 17 figures", "summary": "Validation plays a crucial role in the clustering process. Many different internal validity indexes exist for the purpose of determining the best clustering solution(s) from a given collection of candidates, e.g., as produced by different algorithms or different algorithm hyper-parameters. In this study, we present a comprehensive benchmark study of 26 internal validity indexes, which includes highly popular classic indexes as well as more recently developed ones. We adopted an enhanced revision of the methodology presented in Vendramin et al. (2010), developed here to address several shortcomings of this previous work. This overall new approach consists of three complementary custom-tailored evaluation sub-methodologies, each of which has been designed to assess specific aspects of an index's behaviour while preventing potential biases of the other sub-methodologies. Each sub-methodology features two complementary measures of performance, alongside mechanisms that allow for an in-depth investigation of more complex behaviours of the internal validity indexes under study. Additionally, a new collection of 16177 datasets has been produced, paired with eight widely-used clustering algorithms, for a wider applicability scope and representation of more diverse clustering scenarios."}
{"id": "2511.05834", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2511.05834", "abs": "https://arxiv.org/abs/2511.05834", "authors": ["Xinshan Jiao", "Yuxin Luo", "Yilin Bi", "Tao Zhou"], "title": "Impacts of Data Splitting Strategies on Parameterized Link Prediction Algorithms", "comment": "18 pages, 3 figures", "summary": "Link prediction is a fundamental problem in network science, aiming to infer potential or missing links based on observed network structures. With the increasing adoption of parameterized models, the rigor of evaluation protocols has become critically important. However, a previously common practice of using the test set during hyperparameter tuning has led to human-induced information leakage, thereby inflating the reported model performance. To address this issue, this study introduces a novel evaluation metric, Loss Ratio, which quantitatively measures the extent of performance overestimation. We conduct large-scale experiments on 60 real-world networks across six domains. The results demonstrate that the information leakage leads to an average overestimation about 3.6\\%, with the bias reaching over 15\\% for specific algorithms. Meanwhile, heuristic and random-walk-based methods exhibit greater robustness and stability. The analysis uncovers a pervasive information leakage issue in link prediction evaluation and underscores the necessity of adopting standardized data splitting strategies to enable fair and reproducible benchmarking of link prediction models."}
{"id": "2511.05725", "categories": ["stat.AP", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.05725", "abs": "https://arxiv.org/abs/2511.05725", "authors": ["RJ Waken", "Fengxian Wang", "Sarah A. Eisenstein", "Tim McBride", "Kim Johnson", "Karen Joynt-Maddox"], "title": "Multilevel non-linear interrupted time series analysis", "comment": null, "summary": "Recent advances in interrupted time series analysis permit characterization of a typical non-linear interruption effect through use of generalized additive models. Concurrently, advances in latent time series modeling allow efficient Bayesian multilevel time series models. We propose to combine these concepts with a hierarchical model selection prior to characterize interruption effects with a multilevel structure, encouraging parsimony and partial pooling while incorporating meaningful variability in causal effects across subpopulations of interest, while allowing poststratification. These models are demonstrated with three applications: 1) the effect of the introduction of the prostate specific antigen test on prostate cancer diagnosis rates by race and age group, 2) the change in stroke or trans-ischemic attack hospitalization rates across Medicare beneficiaries by rurality in the months after the start of the COVID-19 pandemic, and 3) the effect of Medicaid expansion in Missouri on the proportion of inpatient hospitalizations discharged with Medicaid as a primary payer by key age groupings and sex."}
{"id": "2511.07027", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.07027", "abs": "https://arxiv.org/abs/2511.07027", "authors": ["Oluwayomi Akinfenwa", "Niamh Cahill", "Catherine Hurley"], "title": "wdiexplorer: An R package Designed for Exploratory Analysis of World Development Indicators (WDI) Data", "comment": null, "summary": "The World Development Indicators (WDI) database provides a wide range of global development data, maintained and published by the World Bank. Our \\textit{wdiexplorer} package offers a comprehensive workflow that sources WDI data via the \\textit{WDI} R package, prepares and explores country-level panel data of the WDI through computational functions to calculate diagnostic metrics and visualise the outputs. By leveraging the functionalities of \\textit{wdiexplorer} package, users can efficiently explore any indicator dataset of the WDI, compute diagnostic indices, and visualise the metrics by incorporating the pre-defined grouping structures to identify patterns, outliers, and other interesting features of temporal behaviours. This paper presents the \\textit{wdiexplorer} package, demonstrates its functionalities using the WDI: PM$_{2.5}$ air pollution dataset, and discusses the observed patterns and outliers across countries and within groups of country-level panel data."}
{"id": "2511.05709", "categories": ["stat.ME", "math.CO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.05709", "abs": "https://arxiv.org/abs/2511.05709", "authors": ["Patrick Scharpfenecker", "Tobias Windisch"], "title": "SAT-sampling for statistical significance testing in sparse contingency tables", "comment": "16 pages, 10 figures", "summary": "Exact conditional tests for contingency tables require sampling from fibers with fixed margins. Classical Markov basis MCMC is general but often impractical: computing full Markov bases that connect all fibers of a given constraint matrix can be infeasible and the resulting chains may converge slowly, especially in sparse settings or in presence of structural zeros. We introduce a SAT-based alternative that encodes fibers as Boolean circuits which allows modern SAT samplers to generate tables randomly. We analyze the sampling bias that SAT samplers may introduce, provide diagnostics, and propose practical mitigation. We propose hybrid MCMC schemes that combine SAT proposals with local moves to ensure correct stationary distributions which do not necessarily require connectivity via local moves which is particularly beneficial in presence of structural zeros. Across benchmarks, including small and involved tables with many structural zeros where pure Markov-basis methods underperform, our methods deliver reliable conditional p-values and often outperform samplers that rely on precomputed Markov bases."}
{"id": "2511.06235", "categories": ["stat.ML", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.06235", "abs": "https://arxiv.org/abs/2511.06235", "authors": ["Zhitao Li", "Yiqiu Dong", "Xueying Zeng"], "title": "Sparsity via Hyperpriors: A Theoretical and Algorithmic Study under Empirical Bayes Framework", "comment": null, "summary": "This paper presents a comprehensive analysis of hyperparameter estimation within the empirical Bayes framework (EBF) for sparse learning. By studying the influence of hyperpriors on the solution of EBF, we establish a theoretical connection between the choice of the hyperprior and the sparsity as well as the local optimality of the resulting solutions. We show that some strictly increasing hyperpriors, such as half-Laplace and half-generalized Gaussian with the power in $(0,1)$, effectively promote sparsity and improve solution stability with respect to measurement noise. Based on this analysis, we adopt a proximal alternating linearized minimization (PALM) algorithm with convergence guaranties for both convex and concave hyperpriors. Extensive numerical tests on two-dimensional image deblurring problems demonstrate that introducing appropriate hyperpriors significantly promotes the sparsity of the solution and enhances restoration accuracy. Furthermore, we illustrate the influence of the noise level and the ill-posedness of inverse problems to EBF solutions."}
{"id": "2511.06107", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.06107", "abs": "https://arxiv.org/abs/2511.06107", "authors": ["David Kaplan", "Nina Jude", "Kjorte Harra", "Jonas Stampka"], "title": "On the Development of Probabilistic Projections of Country-level Progress to the UN SDG Indicator of Minimum Proficiency in Reading and Mathematics", "comment": null, "summary": "As of this writing, there are five years remaining for countries to reach their Sustainable Development Goals deadline of 2030 as agreed to by the member countries of the United Nations. Countries are, therefore, naturally interested in projections of progress toward these goals. A variety of statistical measures have been used to report on country-level progress toward the goals, but they have not utilized methodologies explicitly designed to obtain optimally predictive measures of rate of progress as the foundation for projecting trends. The focus of this paper is to provide Bayesian probabilistic projections of progress to SDG indicator 4.1.1, attaining minimum proficiency in reading and mathematics, with particular emphasis on competencies among lower secondary school children. Using data from the OECD PISA, as well as indicators drawn from the World Bank, the OECD, UNDP, and UNESCO, we employ a novel combination of Bayesian latent growth curve modeling Bayesian model averaging to obtain optimal estimates of the rate of progress in minimum proficiency percentages and then use those estimate to develop probabilistic projections into the future overall for all countries in the analysis. Four case study countries are also presented to show how the methods can be used for individual country projections."}
{"id": "2511.07050", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.07050", "abs": "https://arxiv.org/abs/2511.07050", "authors": ["Marco Grzegorczyk"], "title": "A BGe score for tied-covariance mixtures of Gaussian Bayesian networks", "comment": null, "summary": "Mixtures of Gaussian Bayesian networks have previously been studied under full-covariance assumptions, where each mixture component has its own covariance matrix. We propose a mixture model with tied-covariance, in which all components share a common covariance matrix. Our main contribution is the derivation of its marginal likelihood, which remains analytic. Unlike in the full-covariance case, however, the marginal likelihood no longer factorizes into component-specific terms. We refer to the new likelihood as the BGe scoring metric for tied-covariance mixtures of Gaussian Bayesian networks. For model inference, we implement MCMC schemes combining structure MCMC with a fast Gibbs sampler for mixtures, and we empirically compare the tied- and full-covariance mixtures of Gaussian Bayesian networks on simulated and benchmark data."}
{"id": "2511.05733", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.05733", "abs": "https://arxiv.org/abs/2511.05733", "authors": ["Mathew Chandy", "Elizabeth Schifano", "Jun Yan", "Xianyang Zhang"], "title": "Nonparametric Block Bootstrap Kolmogorov-Smirnov Goodness-of-Fit Test", "comment": null, "summary": "The Kolmogorov--Smirnov (KS) test is a widely used statistical test that assesses the conformity of a sample to a specified distribution. Its efficacy, however, diminishes with serially dependent data and when parameters within the hypothesized distribution are unknown. For independent data, parametric and nonparametric bootstrap procedures are available to adjust for estimated parameters. For serially dependent stationary data, parametric bootstrap has been developed with a working serial dependence structure. A counterpart for the nonparametric bootstrap approach, which needs a bias correction, has not been studied. Addressing this gap, our study introduces a bias correction method employing a nonparametric block bootstrap, which approximates the distribution of the KS statistic in assessing the goodness-of-fit of the marginal distribution of a stationary series, accounting for unspecified serial dependence and unspecified parameters. We assess its effectiveness through simulations, scrutinizing both its size and power. The practicality of our method is further illustrated with an examination of stock returns from the S\\&P 500 index, showcasing its utility in real-world applications."}
{"id": "2511.06239", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06239", "abs": "https://arxiv.org/abs/2511.06239", "authors": ["Byoungwoo Park", "Juho Lee", "Guan-Horng Liu"], "title": "Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces", "comment": null, "summary": "Learning-based methods for sampling from the Gibbs distribution in finite-dimensional spaces have progressed quickly, yet theory and algorithmic design for infinite-dimensional function spaces remain limited. This gap persists despite their strong potential for sampling the paths of conditional diffusion processes, enabling efficient simulation of trajectories of diffusion processes that respect rare events or boundary constraints. In this work, we present the adjoint sampler for infinite-dimensional function spaces, a stochastic optimal control-based diffusion sampler that operates in function space and targets Gibbs-type distributions on infinite-dimensional Hilbert spaces. Our Functional Adjoint Sampler (FAS) generalizes Adjoint Sampling (Havens et al., 2025) to Hilbert spaces based on a SOC theory called stochastic maximum principle, yielding a simple and scalable matching-type objective for a functional representation. We show that FAS achieves superior transition path sampling performance across synthetic potential and real molecular systems, including Alanine Dipeptide and Chignolin."}
{"id": "2511.05746", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.05746", "abs": "https://arxiv.org/abs/2511.05746", "authors": ["Nicola Bariletto", "Nhat Ho", "Alessandro Rinaldo"], "title": "Conformalized Bayesian Inference, with Applications to Random Partition Models", "comment": null, "summary": "Bayesian posterior distributions naturally represent parameter uncertainty informed by data. However, when the parameter space is complex, as in many nonparametric settings where it is infinite dimensional or combinatorially large, standard summaries such as posterior means, credible intervals, or simple notions of multimodality are often unavailable, hindering interpretable posterior uncertainty quantification. We introduce Conformalized Bayesian Inference (CBI), a broadly applicable and computationally efficient framework for posterior inference on nonstandard parameter spaces. CBI yields a point estimate, a credible region with assumption-free posterior coverage guarantees, and a principled analysis of posterior multimodality, requiring only Monte Carlo samples from the posterior and a notion of discrepancy between parameters. The method builds a discrepancy-based kernel density score for each parameter value, yielding a maximum-a-posteriori-like point estimate and a credible region derived from conformal prediction principles. The key conceptual step underlying this construction is the reinterpretation of posterior inference as prediction on the parameter space. A final density-based clustering step identifies representative posterior modes. We investigate a number of theoretical and methodological properties of CBI and demonstrate its practicality, scalability, and versatility in simulated and real data clustering applications with Bayesian random partition models."}
{"id": "2511.06407", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.06407", "abs": "https://arxiv.org/abs/2511.06407", "authors": ["Takashi Hayakawa", "Satoshi Asai"], "title": "Fast Riemannian-manifold Hamiltonian Monte Carlo for hierarchical Gaussian-process models", "comment": null, "summary": "Hierarchical Bayesian models based on Gaussian processes are considered useful for describing complex nonlinear statistical dependencies among variables in real-world data. However, effective Monte Carlo algorithms for inference with these models have not yet been established, except for several simple cases. In this study, we show that, compared with the slow inference achieved with existing program libraries, the performance of Riemannian-manifold Hamiltonian Monte Carlo (RMHMC) can be drastically improved by optimising the computation order according to the model structure and dynamically programming the eigendecomposition. This improvement cannot be achieved when using an existing library based on a naive automatic differentiator. We numerically demonstrate that RMHMC effectively samples from the posterior, allowing the calculation of model evidence, in a Bayesian logistic regression on simulated data and in the estimation of propensity functions for the American national medical expenditure data using several Bayesian multiple-kernel models. These results lay a foundation for implementing effective Monte Carlo algorithms for analysing real-world data with Gaussian processes, and highlight the need to develop a customisable library set that allows users to incorporate dynamically programmed objects and finely optimises the mode of automatic differentiation depending on the model structure."}
{"id": "2511.06204", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06204", "abs": "https://arxiv.org/abs/2511.06204", "authors": ["Hyun Jung Koo", "Aaron J. Molstad"], "title": "A unified approach to spatial domain detection and cell-type deconvolution in spot-based spatial transcriptomics", "comment": null, "summary": "Many popular technologies for generating spatially resolved transcriptomic (SRT) data measure gene expression at the resolution of a \"spot\", i.e., a small tissue region 55 microns in diameter. Each spot can contain many cells of different types. In typical analyses, researchers are interested in using these data to identify discrete spatial domains in the tissue. In this paper, we propose a new method, DUET, that simultaneously identifies discrete spatial domains and estimates each spot's cell-type proportion. This allows the identified spatial domains to be characterized in terms of the cell type proportions, which affords interpretability and biological insight. DUET utilizes a constrained version of model-based convex clustering, and as such, can accommodate Poisson, negative binomial, normal, and other types of expression data. Through simulation studies and multiple applications, we show that our method can achieve better clustering and deconvolution performance than existing methods."}
{"id": "2511.05755", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.05755", "abs": "https://arxiv.org/abs/2511.05755", "authors": ["Ivano Lodato", "Aditya V. Iyer", "Isaac Z. To"], "title": "Bounding interventional queries from generalized incomplete contingency tables", "comment": "8 pages ; 5 tables ; 2 algorithms", "summary": "We introduce a method for evaluating interventional queries and Average Treatment Effects (ATEs) in the presence of generalized incomplete contingency tables (GICTs), contingency tables containing a full row of random (sampling) zeros, rendering some conditional probabilities undefined. Rather than discarding such entries or imputing missing values, we model the unknown probabilities as free parameters and derive symbolic expressions for the queries that incorporate them. By extremizing these expressions over all values consistent with basic probability constraints and the support of all variables, we obtain sharp bounds for the query of interest under weak assumptions of small missing frequencies. These bounds provide a formal quantification of the uncertainty induced by the generalized incompleteness of the contingency table and ensure that the true value of the query will always lie within them. The framework applies independently of the missingness mechanism and offers a conservative yet rigorous approach to causal inference under random data gaps."}
{"id": "2511.06425", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06425", "abs": "https://arxiv.org/abs/2511.06425", "authors": ["Brian B. Avants", "Nicholas J. Tustison", "James R Stone"], "title": "Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings", "comment": null, "summary": "Interpretable representation learning is a central challenge in modern machine learning, particularly in high-dimensional settings such as neuroimaging, genomics, and text analysis. Current methods often struggle to balance the competing demands of interpretability and model flexibility, limiting their effectiveness in extracting meaningful insights from complex data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a general-purpose matrix estimation framework that unifies ideas from sparse matrix factorization, orthogonalization, and constrained manifold learning. NSA-Flow enforces structured sparsity through a continuous balance between reconstruction fidelity and column-wise decorrelation, parameterized by a single tunable weight. The method operates as a smooth flow near the Stiefel manifold with proximal updates for non-negativity and adaptive gradient control, yielding representations that are simultaneously sparse, stable, and interpretable. Unlike classical regularization schemes, NSA-Flow provides an intuitive geometric mechanism for manipulating sparsity at the level of global structure while simplifying latent features. We demonstrate that the NSA-Flow objective can be optimized smoothly and integrates seamlessly with existing pipelines for dimensionality reduction while improving interpretability and generalization in both simulated and real biomedical data. Empirical validation on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the NSA-Flow constraints can maintain or improve performance over related methods with little additional methodological effort. NSA-Flow offers a scalable, general-purpose tool for interpretable ML, applicable across data science domains."}
{"id": "2511.06276", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06276", "abs": "https://arxiv.org/abs/2511.06276", "authors": ["Fernando Rodriguez Avellaneda", "Paula Moraga"], "title": "Bayesian spatio-temporal disaggregation modeling using a diffusion-SPDE approach: a case study of Aerosol Optical Depth in India", "comment": null, "summary": "Accurate estimation of Aerosol Optical Depth (AOD) is crucial for understanding climate change and its impacts on public health, as aerosols are a measure of air quality conditions. AOD is usually retrieved from satellite imagery at coarse spatial and temporal resolutions. However, producing high-resolution AOD estimates in both space and time can better support evidence-based policies and interventions. We propose a spatio-temporal disaggregation model that assumes a latent spatio--temporal continuous Gaussian process observed through aggregated measurements. The model links discrete observations to the continuous domain and accommodates covariates to improve explanatory power and interpretability. The approach employs Gaussian processes with separable or non-separable covariance structures derived from a diffusion-based spatio-temporal stochastic partial differential equation (SPDE). Bayesian inference is conducted using the INLA-SPDE framework for computational efficiency. Simulation studies and an application to nowcasting AOD at 550 nm in India demonstrate the model's effectiveness, improving spatial resolution from 0.75° to 0.25° and temporal resolution from 3 hours to 1 hour."}
{"id": "2511.05709", "categories": ["stat.ME", "math.CO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.05709", "abs": "https://arxiv.org/abs/2511.05709", "authors": ["Patrick Scharpfenecker", "Tobias Windisch"], "title": "SAT-sampling for statistical significance testing in sparse contingency tables", "comment": "16 pages, 10 figures", "summary": "Exact conditional tests for contingency tables require sampling from fibers with fixed margins. Classical Markov basis MCMC is general but often impractical: computing full Markov bases that connect all fibers of a given constraint matrix can be infeasible and the resulting chains may converge slowly, especially in sparse settings or in presence of structural zeros. We introduce a SAT-based alternative that encodes fibers as Boolean circuits which allows modern SAT samplers to generate tables randomly. We analyze the sampling bias that SAT samplers may introduce, provide diagnostics, and propose practical mitigation. We propose hybrid MCMC schemes that combine SAT proposals with local moves to ensure correct stationary distributions which do not necessarily require connectivity via local moves which is particularly beneficial in presence of structural zeros. Across benchmarks, including small and involved tables with many structural zeros where pure Markov-basis methods underperform, our methods deliver reliable conditional p-values and often outperform samplers that rely on precomputed Markov bases."}
{"id": "2511.05840", "categories": ["stat.ME", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.05840", "abs": "https://arxiv.org/abs/2511.05840", "authors": ["Zhanyi Jiao", "Qiuqi Wang", "Yimiao Zhao"], "title": "Standard and comparative e-backtests for general risk measures", "comment": null, "summary": "Backtesting risk measures is a unique and important problem for financial regulators to evaluate risk forecasts reported by financial institutions. As a natural extension to standard (or traditional) backtests, comparative backtests are introduced to evaluate different forecasts against regulatory standard models. Based on recently developed concepts of e-values and e-processes, we focus on how standard and comparative backtests can be manipulated in financial regulation by constructing e-processes. We design a model-free (non-parametric) method for standard backtests of identifiable risk measures and comparative backtests of elicitable risk measures. Our e-backtests are applicable to a wide range of common risk measures including the mean, the variance, the Value-at-Risk, the Expected Shortfall, and the expectile. Our results are illustrated by ample simulation studies and real data analysis."}
{"id": "2511.06479", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.06479", "abs": "https://arxiv.org/abs/2511.06479", "authors": ["Muhammad Shahnawaz", "Adeel Safder"], "title": "Bridging Theory and Practice: A Stochastic Learning-Optimization Model for Resilient Automotive Supply Chains", "comment": "14 pages, 4 figures", "summary": "Supply chain disruptions and volatile demand pose significant challenges to the UK automotive industry, which relies heavily on Just-In-Time (JIT) manufacturing. While qualitative studies highlight the potential of integrating Artificial Intelligence (AI) with traditional optimization, a formal, quantitative demonstration of this synergy is lacking. This paper introduces a novel stochastic learning-optimization framework that integrates Bayesian inference with inventory optimization for supply chain management (SCM). We model a two-echelon inventory system subject to stochastic demand and supply disruptions, comparing a traditional static optimization policy against an adaptive policy where Bayesian learning continuously updates parameter estimates to inform stochastic optimization. Our simulations over 365 periods across three operational scenarios demonstrate that the integrated approach achieves 7.4\\% cost reduction in stable environments and 5.7\\% improvement during supply disruptions, while revealing important limitations during sudden demand shocks due to the inherent conservatism of Bayesian updating. This work provides mathematical validation for practitioner observations and establishes a formal framework for understanding AI-driven supply chain resilience, while identifying critical boundary conditions for successful implementation."}
{"id": "2511.06320", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.06320", "abs": "https://arxiv.org/abs/2511.06320", "authors": ["Abbas Zaidi", "Rina Friedberg", "Samir Khan", "Yao-Yang Leow", "Maulik Soneji", "Houssam Nassif", "Richard Mudd"], "title": "Bayesian Predictive Probabilities for Online Experimentation", "comment": "5 pages, 1 figure", "summary": "The widespread adoption of online randomized controlled experiments (A/B Tests) for decision-making has created ongoing capacity constraints which necessitate interim analyses. As a consequence, platform users are increasingly motivated to use ad-hoc means of optimizing limited resources via peeking. Such processes, however, are error prone and often misaligned with end-of-experiment outcomes (e.g., inflated type-I error). We introduce a system based on Bayesian Predictive Probabilities that enable us to perform interim analyses without compromising fidelity of the experiment; This idea has been widely utilized in applications outside of the technology domain to more efficiently make decisions in experiments. Motivated by at-scale deployment within an experimentation platform, we demonstrate how predictive probabilities can be estimated without numerical integration techniques and recommend systems to study its properties at scale as an ongoing health check, along with system design recommendations - all on experiment data from Instagram - to demonstrate practical benefits that it enables."}
{"id": "2511.05842", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.05842", "abs": "https://arxiv.org/abs/2511.05842", "authors": ["Nan Qiao", "Wangcheng Li", "Jingxiao Zhang", "Canyi Chen"], "title": "Scalable and Distributed Individualized Treatment Rules for Massive Datasets", "comment": null, "summary": "Synthesizing information from multiple data sources is crucial for constructing accurate individualized treatment rules (ITRs). However, privacy concerns often present significant barriers to the integrative analysis of such multi-source data. Classical meta-learning, which averages local estimates to derive the final ITR, is frequently suboptimal due to biases in these local estimates. To address these challenges, we propose a convolution-smoothed weighted support vector machine for learning the optimal ITR. The accompanying loss function is both convex and smooth, which allows us to develop an efficient multi-round distributed learning procedure for ITRs. Such distributed learning ensures optimal statistical performance with a fixed number of communication rounds, thereby minimizing coordination costs across data centers while preserving data privacy. Our method avoids pooling subject-level raw data and instead requires only sharing summary statistics. Additionally, we develop an efficient coordinate gradient descent algorithm, which guarantees at least linear convergence for the resulting optimization problem. Extensive simulations and an application to sepsis treatment across multiple intensive care units validate the effectiveness of the proposed method."}
{"id": "2511.05842", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.05842", "abs": "https://arxiv.org/abs/2511.05842", "authors": ["Nan Qiao", "Wangcheng Li", "Jingxiao Zhang", "Canyi Chen"], "title": "Scalable and Distributed Individualized Treatment Rules for Massive Datasets", "comment": null, "summary": "Synthesizing information from multiple data sources is crucial for constructing accurate individualized treatment rules (ITRs). However, privacy concerns often present significant barriers to the integrative analysis of such multi-source data. Classical meta-learning, which averages local estimates to derive the final ITR, is frequently suboptimal due to biases in these local estimates. To address these challenges, we propose a convolution-smoothed weighted support vector machine for learning the optimal ITR. The accompanying loss function is both convex and smooth, which allows us to develop an efficient multi-round distributed learning procedure for ITRs. Such distributed learning ensures optimal statistical performance with a fixed number of communication rounds, thereby minimizing coordination costs across data centers while preserving data privacy. Our method avoids pooling subject-level raw data and instead requires only sharing summary statistics. Additionally, we develop an efficient coordinate gradient descent algorithm, which guarantees at least linear convergence for the resulting optimization problem. Extensive simulations and an application to sepsis treatment across multiple intensive care units validate the effectiveness of the proposed method."}
{"id": "2511.06645", "categories": ["stat.ML", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06645", "abs": "https://arxiv.org/abs/2511.06645", "authors": ["Xingchi Li", "Xiaochi Liu", "Guanxun Li"], "title": "Adaptive Testing for Segmenting Watermarked Texts From Language Models", "comment": "13 pages, 3 figures, accepted for publication in STAT, October 28, 2025", "summary": "The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content."}
{"id": "2511.06407", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.06407", "abs": "https://arxiv.org/abs/2511.06407", "authors": ["Takashi Hayakawa", "Satoshi Asai"], "title": "Fast Riemannian-manifold Hamiltonian Monte Carlo for hierarchical Gaussian-process models", "comment": null, "summary": "Hierarchical Bayesian models based on Gaussian processes are considered useful for describing complex nonlinear statistical dependencies among variables in real-world data. However, effective Monte Carlo algorithms for inference with these models have not yet been established, except for several simple cases. In this study, we show that, compared with the slow inference achieved with existing program libraries, the performance of Riemannian-manifold Hamiltonian Monte Carlo (RMHMC) can be drastically improved by optimising the computation order according to the model structure and dynamically programming the eigendecomposition. This improvement cannot be achieved when using an existing library based on a naive automatic differentiator. We numerically demonstrate that RMHMC effectively samples from the posterior, allowing the calculation of model evidence, in a Bayesian logistic regression on simulated data and in the estimation of propensity functions for the American national medical expenditure data using several Bayesian multiple-kernel models. These results lay a foundation for implementing effective Monte Carlo algorithms for analysing real-world data with Gaussian processes, and highlight the need to develop a customisable library set that allows users to incorporate dynamically programmed objects and finely optimises the mode of automatic differentiation depending on the model structure."}
{"id": "2511.05887", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.05887", "abs": "https://arxiv.org/abs/2511.05887", "authors": ["Younghoon Kim", "Sumanta Basu", "Samprit Banerjee"], "title": "Identification of Emotionally Stressful Periods Through Tracking Changes in Statistical Features of mHealth Data", "comment": null, "summary": "Identifying the onset of emotional stress in older patients with mood disorders and chronic pain is crucial in mental health studies. To this end, studying the associations between passively sensed variables that measure human behaviors and self-reported stress levels collected from mobile devices is emerging. Existing algorithms rely on conventional change point detection (CPD) methods due to the nonstationary nature of the data. They also require explicit modeling of the associations between variables and output only discrete time points, which can lead to misinterpretation of stress onset timings. This is problematic when distributional shifts are complex, dependencies between variables are difficult to capture, and changes occur asynchronously across series with weak signals. In this study, we propose an algorithm that detects hotspots, defined as collections of time intervals during which statistical features of passive sensing variables and stress indicators shift, highlighting periods that require investigation. We first extend the moving sum (MOSUM) scheme to detect simultaneous changes both within and across series, and then define hotspots in two ways: using distance-based test statistics and confidence intervals. The proposed method tracks local changes in combined distributional features, enabling it to capture all types of simultaneous and asynchronous change. It does not require a specific functional relationship between series, and the results are expressed as intervals rather than as individual time points. We conduct simulations under varying signal strengths with mixed and asynchronous distributional shifts, where the proposed method outperforms benchmarks. Results on hotspot identification indicate that the two definitions are complementary. We further apply our method to ALACRITY Phase I data, analyzing hotspots from patients' stress levels and activity measures."}
{"id": "2511.06698", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06698", "abs": "https://arxiv.org/abs/2511.06698", "authors": ["Jing Shang", "James Bannon", "Benjamin Haibe-Kains", "Robert Tibshirani"], "title": "Lassoed Forests: Random Forests with Adaptive Lasso Post-selection", "comment": null, "summary": "Random forests are a statistical learning technique that use bootstrap aggregation to average high-variance and low-bias trees. Improvements to random forests, such as applying Lasso regression to the tree predictions, have been proposed in order to reduce model bias. However, these changes can sometimes degrade performance (e.g., an increase in mean squared error). In this paper, we show in theory that the relative performance of these two methods, standard and Lasso-weighted random forests, depends on the signal-to-noise ratio. We further propose a unified framework to combine random forests and Lasso selection by applying adaptive weighting and show mathematically that it can strictly outperform the other two methods. We compare the three methods through simulation, including bias-variance decomposition, error estimates evaluation, and variable importance analysis. We also show the versatility of our method by applications to a variety of real-world datasets."}
{"id": "2511.07038", "categories": ["stat.AP", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07038", "abs": "https://arxiv.org/abs/2511.07038", "authors": ["Kizito Salako", "Rabiu Tsoho Muhammad"], "title": "Conservative Software Reliability Assessments Using Collections of Bayesian Inference Problems", "comment": null, "summary": "When using Bayesian inference to support conservative software reliability assessments, it is useful to consider a collection of Bayesian inference problems, with the aim of determining the worst-case value (from this collection) for a posterior predictive probability that characterizes how reliable the software is. Using a Bernoulli process to model the occurrence of software failures, we explicitly determine (from collections of Bayesian inference problems) worst-case posterior predictive probabilities of the software operating without failure in the future. We deduce asymptotic properties of these conservative posterior probabilities and their priors, and illustrate how to use these results in assessments of safety-critical software. This work extends robust Bayesian inference results and so-called conservative Bayesian inference methods."}
{"id": "2511.07197", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07197", "abs": "https://arxiv.org/abs/2511.07197", "authors": ["Tuan Minh Ha", "Binh Thanh Nguyen", "Lam Si Tung Ho"], "title": "Simulation-based Methods for Optimal Sampling Design in Systems Biology", "comment": null, "summary": "In many areas of systems biology, including virology, pharmacokinetics, and population biology, dynamical systems are commonly used to describe biological processes. These systems can be characterized by estimating their parameters from sampled data. The key problem is how to optimally select sampling points to achieve accurate parameter estimation. Classical approaches often rely on Fisher information matrix-based criteria such as A-, D-, and E-optimality, which require an initial parameter estimate and may yield suboptimal results when the estimate is inaccurate. This study proposes two simulation-based methods for optimal sampling design that do not depend on initial parameter estimates. The first method, E-optimal-ranking (EOR), employs the E-optimal criterion, while the second utilizes a Long Short-Term Memory (LSTM) neural network. Simulation studies based on the Lotka-Volterra and three-compartment models demonstrate that the proposed methods outperform both random selection and classical E-optimal design."}
{"id": "2511.05962", "categories": ["stat.ME", "math.CO", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.05962", "abs": "https://arxiv.org/abs/2511.05962", "authors": ["Kamillo Ferry"], "title": "Minimum bounding polytropes for estimation of max-linear Bayesian networks", "comment": "22 pages, 8 figures, 6 tables", "summary": "Max-linear Bayesian networks are recursive max-linear structural equation models represented by an edge weighted directed acyclic graph (DAG). The identifiability and estimation of max-linear Bayesian networks is an intricate issue as Gissibl, Klüppelberg, and Lauritzen have shown. As such, a max-linear Bayesian network is generally unidentifiable and standard likelihood theory cannot be applied. We can associate tropical polyhedra to max-linear Bayesian networks. Using this, we investigate the minimum-ratio estimator proposed by Gissibl, Klüppelberg, and Lauritzen and give insight on the structure of minimal best-case samples for parameter recovery which we describe in terms of set covers of certain triangulations. We also combine previous work on estimating max-linear models from Tran, Buck, and Klüppelberg to apply our geometric approach to the structural inference of max-linear models. This is tested extensively on simulated data and on real world data set, the NHANES report for 2015--2016 and the upper Danube network data."}
{"id": "2511.07417", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07417", "abs": "https://arxiv.org/abs/2511.07417", "authors": ["Anay Mehrotra", "Grigoris Velegkas", "Xifan Yu", "Felix Zhou"], "title": "Language Generation with Infinite Contamination", "comment": null, "summary": "We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.\n  Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).\n  We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.\n  Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data."}
{"id": "2511.05755", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.05755", "abs": "https://arxiv.org/abs/2511.05755", "authors": ["Ivano Lodato", "Aditya V. Iyer", "Isaac Z. To"], "title": "Bounding interventional queries from generalized incomplete contingency tables", "comment": "8 pages ; 5 tables ; 2 algorithms", "summary": "We introduce a method for evaluating interventional queries and Average Treatment Effects (ATEs) in the presence of generalized incomplete contingency tables (GICTs), contingency tables containing a full row of random (sampling) zeros, rendering some conditional probabilities undefined. Rather than discarding such entries or imputing missing values, we model the unknown probabilities as free parameters and derive symbolic expressions for the queries that incorporate them. By extremizing these expressions over all values consistent with basic probability constraints and the support of all variables, we obtain sharp bounds for the query of interest under weak assumptions of small missing frequencies. These bounds provide a formal quantification of the uncertainty induced by the generalized incompleteness of the contingency table and ensure that the true value of the query will always lie within them. The framework applies independently of the missingness mechanism and offers a conservative yet rigorous approach to causal inference under random data gaps."}
{"id": "2511.06027", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06027", "abs": "https://arxiv.org/abs/2511.06027", "authors": ["Susovan Pal", "Roger P. Woods", "Suchit Panjiyar", "Elizabeth Sowell", "Katherine L. Narr", "Shantanu H. Joshi"], "title": "A Riemannian Framework for Linear and Quadratic Discriminant Analysis on the Tangent Space of Shapes", "comment": null, "summary": "We present a Riemannian framework for linear and quadratic discriminant classification on the tangent plane of the shape space of curves. The shape space is infinite dimensional and is constructed out of square root velocity functions of curves. We introduce the idea of mean and covariance of shape-valued random variables and samples from a tangent space to the pre-shape space (invariant to translation and scaling) and then extend it to the full shape space (rotational invariance). The shape observations from the population are approximated by coefficients of a Fourier basis of the tangent space. The algorithms for linear and quadratic discriminant analysis are then defined using reduced dimensional features obtained by projecting the original shape observations on to the truncated Fourier basis. We show classification results on synthetic data and shapes of cortical sulci, corpus callosum curves, as well as facial midline curve profiles from patients with fetal alcohol syndrome (FAS)."}
{"id": "2511.06189", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.06189", "abs": "https://arxiv.org/abs/2511.06189", "authors": ["Navonil Deb", "Raaz Dwivedi", "Sumanta Basu"], "title": "Counterfactual Forecasting For Panel Data", "comment": "32 pages, 6 figures", "summary": "We address the challenge of forecasting counterfactual outcomes in a panel data with missing entries and temporally dependent latent factors -- a common scenario in causal inference, where estimating unobserved potential outcomes ahead of time is essential. We propose Forecasting Counterfactuals under Stochastic Dynamics (FOCUS), a method that extends traditional matrix completion methods by leveraging time series dynamics of the factors, thereby enhancing the prediction accuracy of future counterfactuals. Building upon a PCA estimator, our method accommodates both stochastic and deterministic components within the factors, and provides a flexible framework for various applications. In case of stationary autoregressive factors and under standard conditions, we derive error bounds and establish asymptotic normality of our estimator. Empirical evaluations demonstrate that our method outperforms existing benchmarks when the latent factors have an autoregressive component. We illustrate FOCUS results on HeartSteps, a mobile health study, illustrating its effectiveness in forecasting step counts for users receiving activity prompts, thereby leveraging temporal patterns in user behavior."}
{"id": "2511.05840", "categories": ["stat.ME", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.05840", "abs": "https://arxiv.org/abs/2511.05840", "authors": ["Zhanyi Jiao", "Qiuqi Wang", "Yimiao Zhao"], "title": "Standard and comparative e-backtests for general risk measures", "comment": null, "summary": "Backtesting risk measures is a unique and important problem for financial regulators to evaluate risk forecasts reported by financial institutions. As a natural extension to standard (or traditional) backtests, comparative backtests are introduced to evaluate different forecasts against regulatory standard models. Based on recently developed concepts of e-values and e-processes, we focus on how standard and comparative backtests can be manipulated in financial regulation by constructing e-processes. We design a model-free (non-parametric) method for standard backtests of identifiable risk measures and comparative backtests of elicitable risk measures. Our e-backtests are applicable to a wide range of common risk measures including the mean, the variance, the Value-at-Risk, the Expected Shortfall, and the expectile. Our results are illustrated by ample simulation studies and real data analysis."}
{"id": "2511.06070", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06070", "abs": "https://arxiv.org/abs/2511.06070", "authors": ["Bo Fu", "Dandan Jiang"], "title": "Unifiedly Efficient Inference on All-Dimensional Targets for Large-Scale GLMs", "comment": "53 pages, 2 figures", "summary": "The scalability of Generalized Linear Models (GLMs) for large-scale, high-dimensional data often forces a trade-off between computational feasibility and statistical accuracy, particularly for inference on pre-specified parameters. While subsampling methods mitigate computational costs, existing estimators are typically constrained by a suboptimal $r^{-1/2}$ convergence rate, where $r$ is the subsample size. This paper introduces a unified framework that systematically breaks this barrier, enabling efficient and precise inference regardless of the dimension of the target parameters. To overcome the accuracy loss and enhance computational efficiency, we propose three estimators tailored to different scenarios. For low-dimensional targets, we propose a de-variance subsampling (DVS) estimator that achieves a sharply improved convergence rate of $\\max\\{r^{-1}, n^{-1/2}\\}$, permitting valid inference even with very small subsamples. As $r$ grows, a multi-step refinement of our estimator is proven to be asymptotically normal and semiparametric efficient when $r/\\sqrt{n} \\to \\infty$, matching the performance of the full-sample estimator-a property confirmed by its Bahadur representation. Critically, we provide an improved principle to high-dimensional targets, developing a novel decorrelated score function that facilitates simultaneous inference for a diverging number of pre-specified parameters. Comprehensive numerical experiments demonstrate that our framework delivers a superior balance of computational efficiency and statistical accuracy across both low- and high-dimensional inferential tasks in large-scale GLM, thereby realizing the promise of unifiedly efficient inference for large-scale GLMs."}
{"id": "2511.06967", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.06967", "abs": "https://arxiv.org/abs/2511.06967", "authors": ["Emanuele Aliverti"], "title": "Approximate Bayesian inference for cumulative probit regression models", "comment": null, "summary": "Ordinal categorical data are routinely encountered in a wide range of practical applications. When the primary goal is to construct a regression model for ordinal outcomes, cumulative link models represent one of the most popular choices to link the cumulative probabilities of the response with a set of covariates through a parsimonious linear predictor, shared across response categories. When the number of observations grows, standard sampling algorithms for Bayesian inference scale poorly, making posterior computation increasingly challenging in large datasets. In this article, we propose three scalable algorithms for approximating the posterior distribution of the regression coefficients in cumulative probit models relying on Variational Bayes and Expectation Propagation. We compare the proposed approaches with inference based on Markov Chain Monte Carlo, demonstrating superior computational performance and remarkable accuracy; finally, we illustrate the utility of the proposed algorithms on a challenging case study to investigate the structure of a criminal network."}
{"id": "2511.06318", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.06318", "abs": "https://arxiv.org/abs/2511.06318", "authors": ["Richard Mudd", "Rina Friedberg", "Ilya Gorbachev", "Houssam Nassif", "Abbas Zaidi"], "title": "Breaking the Winner's Curse with Bayesian Hybrid Shrinkage", "comment": "5 pages, 1 figure", "summary": "A 'Winner's Curse' arises in large-scale online experimentation platforms when the same experiments are used to both select treatments and evaluate their effects. In these settings, classical difference-in-means estimators of treatment effects are upwardly biased and conventional confidence intervals are rendered invalid. The bias scales with the magnitude of sampling variability and the selection threshold, and inversely with the treatment's true effect size. We propose a new Bayesian approach that incorporates experiment-specific 'local shrinkage' factors that mitigate sensitivity to the choice of prior and improve robustness to assumption violations. We demonstrate how the associated posterior distribution can be estimated without numerical integration techniques, making it a practical choice for at-scale deployment. Through simulation, we evaluate the performance of our approach under various scenarios and find that it performs well even when assumptions about the sampling and selection processes are violated. In an empirical evaluation, our approach demonstrated superior performance over alternative methods, providing more accurate estimates with well-calibrated uncertainty quantification."}
{"id": "2511.06189", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.06189", "abs": "https://arxiv.org/abs/2511.06189", "authors": ["Navonil Deb", "Raaz Dwivedi", "Sumanta Basu"], "title": "Counterfactual Forecasting For Panel Data", "comment": "32 pages, 6 figures", "summary": "We address the challenge of forecasting counterfactual outcomes in a panel data with missing entries and temporally dependent latent factors -- a common scenario in causal inference, where estimating unobserved potential outcomes ahead of time is essential. We propose Forecasting Counterfactuals under Stochastic Dynamics (FOCUS), a method that extends traditional matrix completion methods by leveraging time series dynamics of the factors, thereby enhancing the prediction accuracy of future counterfactuals. Building upon a PCA estimator, our method accommodates both stochastic and deterministic components within the factors, and provides a flexible framework for various applications. In case of stationary autoregressive factors and under standard conditions, we derive error bounds and establish asymptotic normality of our estimator. Empirical evaluations demonstrate that our method outperforms existing benchmarks when the latent factors have an autoregressive component. We illustrate FOCUS results on HeartSteps, a mobile health study, illustrating its effectiveness in forecasting step counts for users receiving activity prompts, thereby leveraging temporal patterns in user behavior."}
{"id": "2511.07027", "categories": ["stat.CO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.07027", "abs": "https://arxiv.org/abs/2511.07027", "authors": ["Oluwayomi Akinfenwa", "Niamh Cahill", "Catherine Hurley"], "title": "wdiexplorer: An R package Designed for Exploratory Analysis of World Development Indicators (WDI) Data", "comment": null, "summary": "The World Development Indicators (WDI) database provides a wide range of global development data, maintained and published by the World Bank. Our \\textit{wdiexplorer} package offers a comprehensive workflow that sources WDI data via the \\textit{WDI} R package, prepares and explores country-level panel data of the WDI through computational functions to calculate diagnostic metrics and visualise the outputs. By leveraging the functionalities of \\textit{wdiexplorer} package, users can efficiently explore any indicator dataset of the WDI, compute diagnostic indices, and visualise the metrics by incorporating the pre-defined grouping structures to identify patterns, outliers, and other interesting features of temporal behaviours. This paper presents the \\textit{wdiexplorer} package, demonstrates its functionalities using the WDI: PM$_{2.5}$ air pollution dataset, and discusses the observed patterns and outliers across countries and within groups of country-level panel data."}
{"id": "2511.06243", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.06243", "abs": "https://arxiv.org/abs/2511.06243", "authors": ["Jeffrey Zhang"], "title": "A sensitivity analysis for the average derivative effect", "comment": null, "summary": "In observational studies, exposures are often continuous rather than binary or discrete. At the same time, sensitivity analysis is an important tool that can help determine the robustness of a causal conclusion to a certain level of unmeasured confounding, which can never be ruled out in an observational study. Sensitivity analysis approaches for continuous exposures have now been proposed for several causal estimands. In this article, we focus on the average derivative effect (ADE). We obtain closed-form bounds for the ADE under a sensitivity model that constrains the odds ratio (at any two dose levels) between the latent and observed generalized propensity score. We propose flexible, efficient estimators for the bounds, as well as point-wise and simultaneous (over the sensitivity parameter) confidence intervals. We examine the finite sample performance of the methods through simulations and illustrate the methods on a study assessing the effect of parental income on educational attainment and a study assessing the price elasticity of petrol."}
{"id": "2511.06318", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.06318", "abs": "https://arxiv.org/abs/2511.06318", "authors": ["Richard Mudd", "Rina Friedberg", "Ilya Gorbachev", "Houssam Nassif", "Abbas Zaidi"], "title": "Breaking the Winner's Curse with Bayesian Hybrid Shrinkage", "comment": "5 pages, 1 figure", "summary": "A 'Winner's Curse' arises in large-scale online experimentation platforms when the same experiments are used to both select treatments and evaluate their effects. In these settings, classical difference-in-means estimators of treatment effects are upwardly biased and conventional confidence intervals are rendered invalid. The bias scales with the magnitude of sampling variability and the selection threshold, and inversely with the treatment's true effect size. We propose a new Bayesian approach that incorporates experiment-specific 'local shrinkage' factors that mitigate sensitivity to the choice of prior and improve robustness to assumption violations. We demonstrate how the associated posterior distribution can be estimated without numerical integration techniques, making it a practical choice for at-scale deployment. Through simulation, we evaluate the performance of our approach under various scenarios and find that it performs well even when assumptions about the sampling and selection processes are violated. In an empirical evaluation, our approach demonstrated superior performance over alternative methods, providing more accurate estimates with well-calibrated uncertainty quantification."}
{"id": "2511.06445", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06445", "abs": "https://arxiv.org/abs/2511.06445", "authors": ["Marco Borriero", "Luigi Augugliaro", "Gianluca Sottile", "Veronica Vinciotti"], "title": "Gaussian Graphical Models for Partially Observed Multivariate Functional Data", "comment": null, "summary": "In many applications, the variables that characterize a stochastic system are measured along a second dimension, such as time. This results in multivariate functional data and the interest is in describing the statistical dependences among these variables. It is often the case that the functional data are only partially observed. This creates additional challenges to statistical inference, since the functional principal component scores, which capture all the information from these data, cannot be computed. Under an assumption of Gaussianity and of partial separability of the covariance operator, we develop an EM-type algorithm for penalized inference of a functional graphical model from multivariate functional data which are only partially observed. A simulation study and an illustration on German electricity market data show the potential of the proposed method."}
{"id": "2511.06476", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06476", "abs": "https://arxiv.org/abs/2511.06476", "authors": ["Mulan Wu", "Mengyu Xu", "Dongyun Kim"], "title": "Confidence Intervals Based on the Modified Chi-Squared Distribution and its Applications in Medicine", "comment": null, "summary": "Small sample sizes in clinical studies arises from factors such as reduced costs, limited subject availability, and the rarity of studied conditions. This creates challenges for accurately calculating confidence intervals (CIs) using the normal distribution approximation. In this paper, we employ a quadratic-form based statistic, from which we derive more accurate confidence intervals, particularly for data with small sample sizes or proportions. Based on the study, we suggest reasonable values of sample sizes and proportions for the application of the quadratic method. Consequently, this method enhances the reliability of statistical inferences. We illustrate this method with real medical data from clinical trials."}
{"id": "2511.06542", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.06542", "abs": "https://arxiv.org/abs/2511.06542", "authors": ["Chaegeun Song", "Zhong Zheng", "Bing Li", "Lingzhou Xue"], "title": "Collapsing Categories for Regression with Mixed Predictors", "comment": "35 pages", "summary": "Categorical predictors are omnipresent in everyday regression practice: in fact, most regression data involve some categorical predictors, and this tendency is increasing in modern applications with more complex structures and larger data sizes. However, including too many categories in a regression model would seriously hamper accuracy, as the information in the data is fragmented by the multitude of categories. In this paper, we introduce a systematic method to reduce the complexity of categorical predictors by adaptively collapsing categories in regressions, so as to enhance the performance of regression estimation. Our method is based on the {\\em pairwise vector fused LASSO}, which automatically fuses the categories that bear a similar regression relation with the response. We develop our method under a wide class of regression models defined by a general loss function, which includes linear models and generalized linear models as special cases. We rigorously established the category collapsing consistency of our method, developed an Inexact Proximal Gradient Descent algorithm to implement it, and proved the feasibility and convergence of our algorithm. Through simulations and an application to Spotify music data, we demonstrate that our method can effectively reduce categorical complexity while improving prediction performance, making it a powerful tool for regression with mixed predictors."}
{"id": "2511.06544", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.06544", "abs": "https://arxiv.org/abs/2511.06544", "authors": ["Shihao Zhang", "Zudi Lu", "Chao Zheng"], "title": "A Simple and Effective Random Forest Modelling for Nonlinear Time Series Data", "comment": null, "summary": "In this paper, we propose Random Forests by Random Weights (RF-RW), a theoretically grounded and practically effective alternative RF modelling for nonlinear time series data, where existing RF-based approaches struggle to adequately capture temporal dependence. RF-RW reconciles the strengths of classic RF with the temporal dependence inherent in time series forecasting. Specifically, it avoids the bootstrap resampling procedure, therefore preserves the serial dependence structure, whilst incorporates independent random weights to reduce correlations among trees. We establish non-asymptotic concentration bounds and asymptotic uniform consistency guarantees, for both fixed- and high-dimensional feature spaces, which extend beyond existing theoretical analyses of RF. Extensive simulation studies demonstrate that RF-RW outperforms existing RF-based approaches and other benchmarks such as SVM and LSTM. It also achieves the lowest error among competitors in our real-data example of predicting UK COVID-19 daily cases."}
{"id": "2511.06967", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.06967", "abs": "https://arxiv.org/abs/2511.06967", "authors": ["Emanuele Aliverti"], "title": "Approximate Bayesian inference for cumulative probit regression models", "comment": null, "summary": "Ordinal categorical data are routinely encountered in a wide range of practical applications. When the primary goal is to construct a regression model for ordinal outcomes, cumulative link models represent one of the most popular choices to link the cumulative probabilities of the response with a set of covariates through a parsimonious linear predictor, shared across response categories. When the number of observations grows, standard sampling algorithms for Bayesian inference scale poorly, making posterior computation increasingly challenging in large datasets. In this article, we propose three scalable algorithms for approximating the posterior distribution of the regression coefficients in cumulative probit models relying on Variational Bayes and Expectation Propagation. We compare the proposed approaches with inference based on Markov Chain Monte Carlo, demonstrating superior computational performance and remarkable accuracy; finally, we illustrate the utility of the proposed algorithms on a challenging case study to investigate the structure of a criminal network."}
{"id": "2511.07096", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.07096", "abs": "https://arxiv.org/abs/2511.07096", "authors": ["Christian Bressen Pipper", "Andreas Nordland", "Klaus Kähler Holst"], "title": "A general approach to construct powerful tests for intersections of one-sided null-hypotheses based on influence functions", "comment": null, "summary": "Testing intersections of null-hypotheses is an integral part of closed testing procedures for assessing multiple null-hypotheses under family-wise type 1 error control. Popular intersection tests such as the minimum p-value test are based on marginal p-values and are typically evaluated conservatively by disregarding simultaneous behavior of the marginal p-values. We consider a general purpose Wald type test for testing intersections of one-sided null-hypotheses. The test is constructed on the basis of the simultaneous asymptotic behavior of the p values. The simultaneous asymptotic behavior is derived via influence functions of estimators using the so-called stacking approach. In particular, this approach does not require added assumptions on simultaneous behavior to be valid. The resulting test is shown to have attractive power properties and thus forms the basis of a powerful closed testing procedure for testing multiple one-sided hypotheses under family-wise type 1 error control."}
{"id": "2511.06204", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06204", "abs": "https://arxiv.org/abs/2511.06204", "authors": ["Hyun Jung Koo", "Aaron J. Molstad"], "title": "A unified approach to spatial domain detection and cell-type deconvolution in spot-based spatial transcriptomics", "comment": null, "summary": "Many popular technologies for generating spatially resolved transcriptomic (SRT) data measure gene expression at the resolution of a \"spot\", i.e., a small tissue region 55 microns in diameter. Each spot can contain many cells of different types. In typical analyses, researchers are interested in using these data to identify discrete spatial domains in the tissue. In this paper, we propose a new method, DUET, that simultaneously identifies discrete spatial domains and estimates each spot's cell-type proportion. This allows the identified spatial domains to be characterized in terms of the cell type proportions, which affords interpretability and biological insight. DUET utilizes a constrained version of model-based convex clustering, and as such, can accommodate Poisson, negative binomial, normal, and other types of expression data. Through simulation studies and multiple applications, we show that our method can achieve better clustering and deconvolution performance than existing methods."}
{"id": "2511.06276", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06276", "abs": "https://arxiv.org/abs/2511.06276", "authors": ["Fernando Rodriguez Avellaneda", "Paula Moraga"], "title": "Bayesian spatio-temporal disaggregation modeling using a diffusion-SPDE approach: a case study of Aerosol Optical Depth in India", "comment": null, "summary": "Accurate estimation of Aerosol Optical Depth (AOD) is crucial for understanding climate change and its impacts on public health, as aerosols are a measure of air quality conditions. AOD is usually retrieved from satellite imagery at coarse spatial and temporal resolutions. However, producing high-resolution AOD estimates in both space and time can better support evidence-based policies and interventions. We propose a spatio-temporal disaggregation model that assumes a latent spatio--temporal continuous Gaussian process observed through aggregated measurements. The model links discrete observations to the continuous domain and accommodates covariates to improve explanatory power and interpretability. The approach employs Gaussian processes with separable or non-separable covariance structures derived from a diffusion-based spatio-temporal stochastic partial differential equation (SPDE). Bayesian inference is conducted using the INLA-SPDE framework for computational efficiency. Simulation studies and an application to nowcasting AOD at 550 nm in India demonstrate the model's effectiveness, improving spatial resolution from 0.75° to 0.25° and temporal resolution from 3 hours to 1 hour."}
{"id": "2511.06425", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.06425", "abs": "https://arxiv.org/abs/2511.06425", "authors": ["Brian B. Avants", "Nicholas J. Tustison", "James R Stone"], "title": "Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings", "comment": null, "summary": "Interpretable representation learning is a central challenge in modern machine learning, particularly in high-dimensional settings such as neuroimaging, genomics, and text analysis. Current methods often struggle to balance the competing demands of interpretability and model flexibility, limiting their effectiveness in extracting meaningful insights from complex data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a general-purpose matrix estimation framework that unifies ideas from sparse matrix factorization, orthogonalization, and constrained manifold learning. NSA-Flow enforces structured sparsity through a continuous balance between reconstruction fidelity and column-wise decorrelation, parameterized by a single tunable weight. The method operates as a smooth flow near the Stiefel manifold with proximal updates for non-negativity and adaptive gradient control, yielding representations that are simultaneously sparse, stable, and interpretable. Unlike classical regularization schemes, NSA-Flow provides an intuitive geometric mechanism for manipulating sparsity at the level of global structure while simplifying latent features. We demonstrate that the NSA-Flow objective can be optimized smoothly and integrates seamlessly with existing pipelines for dimensionality reduction while improving interpretability and generalization in both simulated and real biomedical data. Empirical validation on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the NSA-Flow constraints can maintain or improve performance over related methods with little additional methodological effort. NSA-Flow offers a scalable, general-purpose tool for interpretable ML, applicable across data science domains."}
{"id": "2511.07340", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.07340", "abs": "https://arxiv.org/abs/2511.07340", "authors": ["Andrew Chin", "Akihiko Nishimura"], "title": "Smoothing Out Sticking Points: Sampling from Discrete-Continuous Mixtures with Dynamical Monte Carlo by Mapping Discrete Mass into a Latent Universe", "comment": null, "summary": "Combining a continuous \"slab\" density with discrete \"spike\" mass at zero, spike-and-slab priors provide important tools for inducing sparsity and carrying out variable selection in Bayesian models. However, the presence of discrete mass makes posterior inference challenging. \"Sticky\" extensions to piecewise-deterministic Markov process samplers have shown promising performance, where sampling from the spike is achieved by the process sticking there for an exponentially distributed duration. As it turns out, the sampler remains valid when the exponential sticking time is replaced with its expectation. We justify this by mapping the spike to a continuous density over a latent universe, allowing the sampler to be reinterpreted as traversing this universe while being stuck in the original space. This perspective opens up an array of possibilities to carry out posterior computation under spike-and-slab type priors. Notably, it enables us to construct sticky samplers using other dynamics-based paradigms such as Hamiltonian Monte Carlo, and, in fact, original sticky process can be established as a partial position-momentum refreshment limit of our Hamiltonian sticky sampler. Further, our theoretical and empirical findings suggest these alternatives to be at least as efficient as the original sticky approach."}
