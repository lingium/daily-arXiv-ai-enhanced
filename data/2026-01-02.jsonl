{"id": "2512.23910", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.23910", "abs": "https://arxiv.org/abs/2512.23910", "authors": ["Qihao Duan", "Alexandre B. Simas", "David Bolin", "Raphaël Huser"], "title": "Forecasting the Term Structure of Interest Rates with SPDE-Based Models", "comment": null, "summary": "The Dynamic Nelson--Siegel (DNS) model is a widely used framework for term structure forecasting. We propose a novel extension that models DNS residuals as a Gaussian random field, capturing dependence across both time and maturity. The residual field is represented via a stochastic partial differential equation (SPDE), enabling flexible covariance structures and scalable Bayesian inference through sparse precision matrices. We consider a range of SPDE specifications, including stationary, non-stationary, anisotropic, and nonseparable models. The SPDE--DNS model is estimated in a Bayesian framework using the integrated nested Laplace approximation (INLA), jointly inferring latent DNS factors and the residual field. Empirical results show that the SPDE-based extensions improve both point and probabilistic forecasts relative to standard benchmarks. When applied in a mean--variance bond portfolio framework, the forecasts generate economically meaningful utility gains, measured as performance fees relative to a Bayesian DNS benchmark under monthly rebalancing. Importantly, incorporating the structured SPDE residual substantially reduces cross-maturity and intertemporal dependence in the remaining measurement error, bringing it closer to white noise. These findings highlight the advantages of combining DNS with SPDE-driven residual modeling for flexible, interpretable, and computationally efficient yield curve forecasting."}
{"id": "2512.24041", "categories": ["stat.AP", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.24041", "abs": "https://arxiv.org/abs/2512.24041", "authors": ["Lucas Shen", "Gaurav Sood"], "title": "Exposed: Shedding Blacklight on Online Privacy", "comment": null, "summary": "To what extent are users surveilled on the web, by what technologies, and by whom? We answer these questions by combining passively observed, anonymized browsing data of a large, representative sample of Americans with domain-level data on tracking from Blacklight. We find that nearly all users ($ > 99\\%$) encounter at least one ad tracker or third-party cookie over the observation window. More invasive techniques like session recording, keylogging, and canvas fingerprinting are less widespread, but over half of the users visited a site employing at least one of these within the first 48 hours of the start of tracking. Linking trackers to their parent organizations reveals that a single organization, usually Google, can track over $50\\%$ of web activity of more than half the users. Demographic differences in exposure are modest and often attenuate when we account for browsing volume. However, disparities by age and race remain, suggesting that what users browse, not just how much, shapes their surveillance risk."}
{"id": "2512.24211", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.24211", "abs": "https://arxiv.org/abs/2512.24211", "authors": ["Thevesh Thevananthan", "Danesh Prakash Chacko"], "title": "The Malaysian Election Corpus (MECo): Electoral Maps and Cartograms from 1954 to 2025", "comment": "23 pages, 7 figures, 1 table", "summary": "Electoral boundaries in Malaysia are not publicly available in machine-readable form. This prevents rigorous analysis of geography-centric issues such as malapportionment and gerrymandering, and constrains spatial perspectives on electoral outcomes. We present the second component of the Malaysian Election Corpus (MECo), an open-access collection of digital electoral boundaries covering all 19 approved delimitation exercises in Malaysia's history, from the first set of Malayan boundaries in 1954 until the 2019 Sabah delimitation. We also auto-generate election-time maps for all federal and state elections up to 2025, and include equal-area and electorate-weighted cartograms to support deeper geospatial analysis. This is the first complete, publicly-available, and machine-readable record of Malaysia's electoral boundaries, and fills a critical gap in the country's electoral data infrastructure."}
{"id": "2512.23805", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23805", "abs": "https://arxiv.org/abs/2512.23805", "authors": ["Lars van der Laan", "Nathan Kallus"], "title": "Fitted Q Evaluation Without Bellman Completeness via Stationary Weighting", "comment": null, "summary": "Fitted Q-evaluation (FQE) is a central method for off-policy evaluation in reinforcement learning, but it generally requires Bellman completeness: that the hypothesis class is closed under the evaluation Bellman operator. This requirement is challenging because enlarging the hypothesis class can worsen completeness. We show that the need for this assumption stems from a fundamental norm mismatch: the Bellman operator is gamma-contractive under the stationary distribution of the target policy, whereas FQE minimizes Bellman error under the behavior distribution. We propose a simple fix: reweight each regression step using an estimate of the stationary density ratio, thereby aligning FQE with the norm in which the Bellman operator contracts. This enables strong evaluation guarantees in the absence of realizability or Bellman completeness, avoiding the geometric error blow-up of standard FQE in this setting while maintaining the practicality of regression-based evaluation."}
{"id": "2512.24642", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.24642", "abs": "https://arxiv.org/abs/2512.24642", "authors": ["Kwangok Seo", "Johan Lim", "Seokho Lee", "Jong Hee Park"], "title": "$\\ell_0$-Regularized Item Response Theory Model for Robust Ideal Point Estimation", "comment": null, "summary": "Ideal point estimation methods face a significant challenge when legislators engage in protest voting -- strategically voting against their party to express dissatisfaction. Such votes introduce attenuation bias, making ideologically extreme legislators appear artificially moderate. We propose a novel statistical framework that extends the fast EM-based estimation approach of \\cite{Imai2016} using $\\ell_0$ regularization method to handle protest votes. Through simulation studies, we demonstrate that our proposed method maintains estimation accuracy even with high proportions of protest votes, while being substantially faster than MCMC-based methods. Applying our method to the 116th and 117th U.S. House of Representatives, we successfully recover the extreme liberal positions of ``the Squad'', whose protest votes had caused conventional methods to misclassify them as moderates. While conventional methods rank Ocasio-Cortez as more conservative than 69\\% of Democrats, our method places her firmly in the progressive wing, aligning with her documented policy positions. This approach provides both robust ideal point estimates and systematic identification of protest votes, facilitating deeper analysis of strategic voting behavior in legislatures."}
{"id": "2512.23818", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23818", "abs": "https://arxiv.org/abs/2512.23818", "authors": ["Andrej Leban"], "title": "Energy-Tweedie: Score meets Score, Energy meets Energy", "comment": "22 pages, 5 figures", "summary": "Denoising and score estimation have long been known to be linked via the classical Tweedie's formula. In this work, we first extend the latter to a wider range of distributions often called \"energy models\" and denoted elliptical distributions in this work. Next, we examine an alternative view: we consider the denoising posterior $P(X|Y)$ as the optimizer of the energy score (a scoring rule) and derive a fundamental identity that connects the (path-) derivative of a (possibly) non-Euclidean energy score to the score of the noisy marginal. This identity can be seen as an analog of Tweedie's identity for the energy score, and allows for several interesting applications; for example, score estimation, noise distribution parameter estimation, as well as using energy score models in the context of \"traditional\" diffusion model samplers with a wider array of noising distributions."}
{"id": "2512.23772", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.23772", "abs": "https://arxiv.org/abs/2512.23772", "authors": ["Amélie Artis", "Achmad Choiruddin", "Jean-François Coeurjolly", "Frédérique Letué"], "title": "Marked point processes intensity estimation using sparse group Lasso method applied to locations of lucrative and cooperative banks in mainland France", "comment": null, "summary": "In this paper, we model the locations of five major banks in mainland France, two lucrative and three cooperative institutions based on socio-economic considerations. Locations of banks are collected using web scrapping and constitute a bivariate spatial point process for which we estimate nonparametrically summary functions (intensity, Ripley and cross-Ripley's K functions). This shows that the pattern is highly inhomogenenous and exhibits a clustering effect especially at small scales, and thus a significant departure to the bivariate (inhomogeneous) Poisson point process is pointed out. We also collect socio-economic datasets (at the living area level) from INSEE and propose a parametric modelling of the intensity function using these covariates. We propose a group-penalized bivariate composite likelihood method to estimate the model parameters, and we establish its asymptotic properties. The application of the methodology to the banking dataset provides new insights into the specificity of the cooperative model within the sector, particularly in relation to the theories of institutional isomorphism."}
{"id": "2512.23772", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.23772", "abs": "https://arxiv.org/abs/2512.23772", "authors": ["Amélie Artis", "Achmad Choiruddin", "Jean-François Coeurjolly", "Frédérique Letué"], "title": "Marked point processes intensity estimation using sparse group Lasso method applied to locations of lucrative and cooperative banks in mainland France", "comment": null, "summary": "In this paper, we model the locations of five major banks in mainland France, two lucrative and three cooperative institutions based on socio-economic considerations. Locations of banks are collected using web scrapping and constitute a bivariate spatial point process for which we estimate nonparametrically summary functions (intensity, Ripley and cross-Ripley's K functions). This shows that the pattern is highly inhomogenenous and exhibits a clustering effect especially at small scales, and thus a significant departure to the bivariate (inhomogeneous) Poisson point process is pointed out. We also collect socio-economic datasets (at the living area level) from INSEE and propose a parametric modelling of the intensity function using these covariates. We propose a group-penalized bivariate composite likelihood method to estimate the model parameters, and we establish its asymptotic properties. The application of the methodology to the banking dataset provides new insights into the specificity of the cooperative model within the sector, particularly in relation to the theories of institutional isomorphism."}
{"id": "2512.24450", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.24450", "abs": "https://arxiv.org/abs/2512.24450", "authors": ["The Tien Mai"], "title": "Robust reduced rank regression under heavy-tailed noise and missing data via non-convex penalization", "comment": null, "summary": "Reduced rank regression (RRR) is a fundamental tool for modeling multiple responses through low-dimensional latent structures, offering both interpretability and strong predictive performance in high-dimensional settings. Classical RRR methods, however, typically rely on squared loss and Gaussian noise assumptions, rendering them sensitive to heavy-tailed errors, outliers, and data contamination. Moreover, the presence of missing data--common in modern applications--further complicates reliable low-rank estimation. In this paper, we propose a robust reduced rank regression framework that simultaneously addresses heavy-tailed noise, outliers, and missing data. Our approach combines a robust Huber loss with nonconvex spectral regularization, specifically the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). Unlike convex nuclear-norm regularization, the proposed nonconvex penalties alleviate excessive shrinkage and enable more accurate recovery of the underlying low-rank structure. The method also accommodates missing data in the response matrix without requiring imputation. We develop an efficient proximal gradient algorithm based on alternating updates and tailored spectral thresholding. Extensive simulation studies demonstrate that the proposed methods substantially outperform nuclear-norm-based and non-robust alternatives under heavy-tailed noise and contamination. An application to cancer cell line data set further illustrates the practical advantages of the proposed robust RRR framework.\n  Our method is implemented in the R package rrpackrobust available at https://github.com/tienmt/rrpackrobust."}
{"id": "2512.23927", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23927", "abs": "https://arxiv.org/abs/2512.23927", "authors": ["Lars van der Laan", "Nathan Kallus"], "title": "Stationary Reweighting Yields Local Convergence of Soft Fitted Q-Iteration", "comment": null, "summary": "Fitted Q-iteration (FQI) and its entropy-regularized variant, soft FQI, are central tools for value-based model-free offline reinforcement learning, but can behave poorly under function approximation and distribution shift. In the entropy-regularized setting, we show that the soft Bellman operator is locally contractive in the stationary norm of the soft-optimal policy, rather than in the behavior norm used by standard FQI. This geometric mismatch explains the instability of soft Q-iteration with function approximation in the absence of Bellman completeness. To restore contraction, we introduce stationary-reweighted soft FQI, which reweights each regression update using the stationary distribution of the current policy. We prove local linear convergence under function approximation with geometrically damped weight-estimation errors, assuming approximate realizability. Our analysis further suggests that global convergence may be recovered by gradually reducing the softmax temperature, and that this continuation approach can extend to the hardmax limit under a mild margin condition."}
{"id": "2512.23866", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.23866", "abs": "https://arxiv.org/abs/2512.23866", "authors": ["Carlos Henrique Trigo Nasser Felix", "Nancy Lopes Garcia", "Alex Rodrigo dos Santos Sousa"], "title": "A Fuzzy Approach for Randomized Confidence Intervals", "comment": null, "summary": "We propose randomized confidence intervals based on the Neyman-Pearson lemma, in order to make them more broadly applicable to distributions that do not satisfy regularity conditions. This is achieved by using the definition of fuzzy confidence intervals. These intervals are compared with methods described in the literature for well-known distributions such as normal, binomial, and Poisson. The results show that in high-variance situations, the new intervals provide better performance. Furthermore, through these intervals, it is possible to compute a lower bound for the expected length, demonstrating that they achieve the minimal maximum expected length for a Bernoulli trial observation."}
{"id": "2512.23993", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.23993", "abs": "https://arxiv.org/abs/2512.23993", "authors": ["Landon Hurley"], "title": "Completing and studentising Spearman's correlation in the presence of ties", "comment": null, "summary": "Non-parametric correlation coefficients have been widely used for analysing arbitrary random variables upon common populations, when requiring an explicit error distribution to be known is an unacceptable assumption. We examine an \\(\\ell_{2}\\) representation of a correlation coefficient (Emond and Mason, 2002) from the perspective of a statistical estimator upon random variables, and verify a number of interesting and highly desirable mathematical properties, mathematically similar to the Whitney embedding of a Hilbert space into the \\(\\ell_{2}\\)-norm space. In particular, we show here that, in comparison to the traditional Spearman (1904) \\(ρ\\), the proposed Kemeny \\(ρ_κ\\) correlation coefficient satisfies Gauss-Markov conditions in the presence or absence of ties, thereby allowing both discrete and continuous marginal random variables. We also prove under standard regularity conditions a number of desirable scenarios, including the construction of a null hypothesis distribution which is Student-t distributed, parallel to standard practice with Pearson's r, but without requiring either continuous random variables nor particular Gaussian errors. Simulations in particular focus upon highly kurtotic data, with highly nominal empirical coverage consistent with theoretical expectation."}
{"id": "2512.24046", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.24046", "abs": "https://arxiv.org/abs/2512.24046", "authors": ["Xiaomei Yang", "Jiaying Jia"], "title": "A persistent-homology-based Bayesian prior to identify Robin coefficient in parabolic problems", "comment": null, "summary": "We adopt a Bayesian inference approach with persistent-homology-based prior to estimate a temporally dependent Robin coefficient arising in the analysis of convective heat transfer. And we also discuss the use of a hierarchical Bayesian method for automatic selection of the regularization parameter. Numerical results demonstrate that the PH prior shows consistent improvement compared to the Gaussian and the total variation prior."}
{"id": "2512.24521", "categories": ["stat.ME", "cs.HC", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.24521", "abs": "https://arxiv.org/abs/2512.24521", "authors": ["Ron Kohavi", "Jakub Linowski", "Lukas Vermeer", "Fabrice Boisseranc", "Joachim Furuseth", "Andrew Gelman", "Guido Imbens", "Ravikiran Rajagopal"], "title": "Power Analysis is Essential: High-Powered Tests Suggest Minimal to No Effect of Rounded Shapes on Click-Through Rates", "comment": "41 pages, 9 figures", "summary": "Underpowered studies (below 50%) suffer from the winner's curse: a statistically significant result must exaggerate the true treatment effect to meet the significance threshold. A study by Dipayan Biswas, Annika Abell, and Roger Chacko published in the Journal of Consumer Research (2023) reported that in an A/B test simply rounding the corners of square buttons increased the online click-through rate by 55% (p-value 0.037)$\\unicode{x2014}$a striking finding with potentially wide-ranging implications for the digital industry that is seeking to enhance consumer engagement. Drawing on our experience with tens of thousands of A/B tests, many involving similar user interface modifications, we found this dramatic claim implausibly large. To evaluate the claim, we conducted three high-powered A/B tests, each involving over two thousand times more users than the original study. All three experiments yielded effect size estimates that were approximately two orders of magnitude smaller than initially reported, with 95% confidence intervals that include zero, that is, not statistically significant at the 0.05 level. Two additional independent replications by Evidoo found similarly small effects. These findings underscore the critical importance of power analysis and experimental design to increase trust and reproducibility of results."}
{"id": "2512.23956", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23956", "abs": "https://arxiv.org/abs/2512.23956", "authors": ["Shinto Eguchi"], "title": "Implicit geometric regularization in flow matching via density weighted Stein operators", "comment": null, "summary": "Flow Matching (FM) has emerged as a powerful paradigm for continuous normalizing flows, yet standard FM implicitly performs an unweighted $L^2$ regression over the entire ambient space. In high dimensions, this leads to a fundamental inefficiency: the vast majority of the integration domain consists of low-density ``void'' regions where the target velocity fields are often chaotic or ill-defined. In this paper, we propose {$γ$-Flow Matching ($γ$-FM)}, a density-weighted variant that aligns the regression geometry with the underlying probability flow. While density weighting is desirable, naive implementations would require evaluating the intractable target density. We circumvent this by introducing a Dynamic Density-Weighting strategy that estimates the \\emph{target} density directly from training particles. This approach allows us to dynamically downweight the regression loss in void regions without compromising the simulation-free nature of FM. Theoretically, we establish that $γ$-FM minimizes the transport cost on a statistical manifold endowed with the $γ$-Stein metric. Spectral analysis further suggests that this geometry induces an implicit Sobolev regularization, effectively damping high-frequency oscillations in void regions. Empirically, $γ$-FM significantly improves vector field smoothness and sampling efficiency on high-dimensional latent datasets, while demonstrating intrinsic robustness to outliers."}
{"id": "2512.23993", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.23993", "abs": "https://arxiv.org/abs/2512.23993", "authors": ["Landon Hurley"], "title": "Completing and studentising Spearman's correlation in the presence of ties", "comment": null, "summary": "Non-parametric correlation coefficients have been widely used for analysing arbitrary random variables upon common populations, when requiring an explicit error distribution to be known is an unacceptable assumption. We examine an \\(\\ell_{2}\\) representation of a correlation coefficient (Emond and Mason, 2002) from the perspective of a statistical estimator upon random variables, and verify a number of interesting and highly desirable mathematical properties, mathematically similar to the Whitney embedding of a Hilbert space into the \\(\\ell_{2}\\)-norm space. In particular, we show here that, in comparison to the traditional Spearman (1904) \\(ρ\\), the proposed Kemeny \\(ρ_κ\\) correlation coefficient satisfies Gauss-Markov conditions in the presence or absence of ties, thereby allowing both discrete and continuous marginal random variables. We also prove under standard regularity conditions a number of desirable scenarios, including the construction of a null hypothesis distribution which is Student-t distributed, parallel to standard practice with Pearson's r, but without requiring either continuous random variables nor particular Gaussian errors. Simulations in particular focus upon highly kurtotic data, with highly nominal empirical coverage consistent with theoretical expectation."}
{"id": "2512.24009", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.24009", "abs": "https://arxiv.org/abs/2512.24009", "authors": ["Landon Hurley"], "title": "An exact unbiased semi-parametric maximum quasi-likelihood framework which is complete in the presence of ties", "comment": null, "summary": "This paper introduces a novel quasi-likelihood extension of the generalised Kendall \\(τ_{a}\\) estimator, together with an extension of the Kemeny metric and its associated covariance and correlation forms. The central contribution is to show that the U-statistic structure of the proposed coefficient \\(τ_κ\\) naturally induces a quasi-maximum likelihood estimation (QMLE) framework, yielding consistent Wald and likelihood ratio test statistics. The development builds on the uncentred correlation inner-product (Hilbert space) formulation of Emond and Mason (2002) and resolves the associated sub-Gaussian likelihood optimisation problem under the \\(\\ell_{2}\\)-norm via an Edgeworth expansion of higher-order moments. The Kemeny covariance coefficient \\(τ_κ\\) is derived within a novel likelihood framework for pairwise comparison-continuous random variables, enabling direct inference on population-level correlation between ranked or weakly ordered datasets. Unlike existing approaches that focus on marginal or pairwise summaries, the proposed framework supports sample-observed weak orderings and accommodates ties without information loss. Drawing parallels with Thurstone's Case V latent ordering model, we derive a quasi-likelihood-based tie model with analytic standard errors, generalising classical U-statistics. The framework applies to general continuous and discrete random variables and establishes formal equivalence to Bradley-Terry and Thurstone models, yielding a uniquely identified linear representation with both analytic and likelihood-based estimators."}
{"id": "2512.24604", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24604", "abs": "https://arxiv.org/abs/2512.24604", "authors": ["Ryo Ohashi", "Hiroyasu Abe", "Fumitake Sakaori"], "title": "Generalized Poisson Matrix Factorization for Overdispersed Count Data", "comment": null, "summary": "Non-negative matrix factorization (NMF) is widely used as a feature extraction technique for matrices with non-negative entries, such as image data, purchase histories, and other types of count data. In NMF, a non-negative matrix is decomposed into the product of two non-negative matrices, and the approximation accuracy is evaluated by a loss function. If the Kullback-Leibler divergence is chosen as the loss function, the estimation coincides with maximum likelihood under the assumption that the data entries are distributed according to a Poisson distribution. To address overdispersion, negative binomial matrix factorization has recently been proposed as an extension of the Poisson-based model. However, the negative binomial distribution often generates an excessive number of zeros, which limits its expressive capacity. In this study, we propose a non-negative matrix factorization based on the generalized Poisson distribution, which can flexibly accommodate overdispersion, and we introduce a maximum likelihood approach for parameter estimation. This methodology provides a more versatile framework than existing models, thereby extending the applicability of NMF to a broader class of count data."}
{"id": "2512.24748", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.24748", "abs": "https://arxiv.org/abs/2512.24748", "authors": ["Zhijian Wang", "Xingbai Xu", "Tuo Liu"], "title": "Quasi-Maximum Likelihood Estimation for a Genuinely Unbalanced Dynamic Network Panel Data Model", "comment": null, "summary": "This paper develops a quasi-maximum likelihood estimator for genuinely unbalanced dynamic network panel data models with individual fixed effects. We propose a model that accommodates contemporaneous and lagged network spillovers, temporal dependence, and a listing effect that activates upon a unit's first appearance in the panel. We establish the consistency of the QMLE as both $N$ and $T$ go to infinity, derive its asymptotic distribution, and identify an asymptotic bias arising from incidental parameters when $N$ is asymptotically large relative to $T$. Based on the asymptotic bias expression, we propose a bias-corrected estimator that is asymptotically unbiased and normally distributed under appropriate regularity conditions. Monte Carlo experiments examine the finite sample performance of the bias-corrected estimator across different criteria, including bias, RMSE, coverage probability, and the normality of the estimator. The empirical application to Airbnb listings from New Zealand and New York City reveals region-specific patterns in spatial and temporal price transmission, illustrating the importance of modeling genuine unbalancedness in dynamic network settings."}
{"id": "2512.24106", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.24106", "abs": "https://arxiv.org/abs/2512.24106", "authors": ["Sachin Saini", "Uaday Singh"], "title": "Constructive Approximation of Random Process via Stochastic Interpolation Neural Network Operators", "comment": "22 Pages, 10 Figures", "summary": "In this paper, we construct a class of stochastic interpolation neural network operators (SINNOs) with random coefficients activated by sigmoidal functions. We establish their boundedness, interpolation accuracy, and approximation capabilities in the mean square sense, in probability, as well as path-wise within the space of second-order stochastic (random) processes \\( L^2(Ω, \\mathcal{F},\\mathbb{P}) \\). Additionally, we provide quantitative error estimates using the modulus of continuity of the processes. These results highlight the effectiveness of SINNOs for approximating stochastic processes with potential applications in COVID-19 case prediction."}
{"id": "2512.24005", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24005", "abs": "https://arxiv.org/abs/2512.24005", "authors": ["Brijesh Kumar Jha", "Subhra Sankar Dhar", "Akash Ashirbad Panda"], "title": "Least Square Estimation: SDEs Perturbed by Lévy Noise with Sparse Sample Paths", "comment": null, "summary": "This article investigates the least squares estimators (LSE) for the unknown parameters in stochastic differential equations (SDEs) that are affected by Lévy noise, particularly when the sample paths are sparse. Specifically, given $n$ sparsely observed curves related to this model, we derive the least squares estimators for the unknown parameters: the drift coefficient, the diffusion coefficient, and the jump-diffusion coefficient. We also establish the asymptotic rate of convergence for the proposed LSE estimators. Additionally, in the supplementary materials, the proposed methodology is applied to a benchmark dataset of functional data/curves, and a small simulation study is conducted to illustrate the findings."}
{"id": "2512.24414", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.24414", "abs": "https://arxiv.org/abs/2512.24414", "authors": ["Ramsés H. Mena", "Christos Merkatas", "Theodoros Nicoleris", "Carlos E. Rodríguez"], "title": "Exact finite mixture representations for species sampling processes", "comment": "27 pages, 4 figures", "summary": "Random probability measures, together with their constructions, representations, and associated algorithms, play a central role in modern Bayesian inference. A key class is that of proper species sampling processes, which offer a relatively simple yet versatile framework that extends naturally to non-exchangeable settings. We revisit this class from a computational perspective and show that they admit exact finite mixture representations. In particular, we prove that any proper species sampling process can be written, at the prior level, as a finite mixture with a latent truncation variable and reweighted atoms, while preserving its distributional features exactly. These finite formulations can be used as drop-in replacements in Bayesian mixture models, recasting posterior computation in terms of familiar finite-mixture machinery. This yields straightforward MCMC implementations and tractable expressions, while avoiding ad hoc truncations and model-specific constructions. The resulting representation preserves the full generality of the original infinite-dimensional priors while enabling practical gains in algorithm design and implementation."}
{"id": "2512.24356", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.24356", "abs": "https://arxiv.org/abs/2512.24356", "authors": ["Max Thannheimer", "Marco Oesting"], "title": "Bayesian inference for functional extreme events defined via partially unobserved processes", "comment": "18 pages, 1 figure, 1 table", "summary": "In order to describe the extremal behaviour of some stochastic process $X$, approaches from univariate extreme value theory are typically generalized to the spatial domain. In particular, generalized peaks-over-threshold approaches allow for the consideration of single extreme events. These can be flexibly defined as exceedances of a risk functional $r$, such as a spatial average, applied to $X$. Inference for the resulting limit process, the so-called $r$-Pareto process, requires the evaluation of $r(X)$ and thus the knowledge of the whole process $X$. In many practical applications, however, observations of $X$ are only available at scattered sites. To overcome this issue, we propose a two-step MCMC-algorithm in a Bayesian framework. In a first step, we sample from $X$ conditionally on the observations in order to evaluate which observations lead to $r$-exceedances. In a second step, we use these exceedances to sample from the posterior distribution of the parameters of the limiting $r$-Pareto process. Alternating these steps results in a full Bayesian model for the extremes of $X$. We show that, under appropriate assumptions, the probability of classifying an observation as $r$-exceedance in the first step converges to the desired probability. Furthermore, given the first step, the distribution of the Markov chain constructed in the second step converges to the posterior distribution of interest. The procedure is compared to the Bayesian version of the standard procedure in a simulation study."}
{"id": "2512.24327", "categories": ["stat.ML", "cs.CG", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24327", "abs": "https://arxiv.org/abs/2512.24327", "authors": ["Anna Calissano", "Etienne Lasalle"], "title": "Topological Spatial Graph Coarsening", "comment": null, "summary": "Spatial graphs are particular graphs for which the nodes are localized in space (e.g., public transport network, molecules, branching biological structures). In this work, we consider the problem of spatial graph reduction, that aims to find a smaller spatial graph (i.e., with less nodes) with the same overall structure as the initial one. In this context, performing the graph reduction while preserving the main topological features of the initial graph is particularly relevant, due to the additional spatial information. Thus, we propose a topological spatial graph coarsening approach based on a new framework that finds a trade-off between the graph reduction and the preservation of the topological characteristics. The coarsening is realized by collapsing short edges. In order to capture the topological information required to calibrate the reduction level, we adapt the construction of classical topological descriptors made for point clouds (the so-called persistent diagrams) to spatial graphs. This construction relies on the introduction of a new filtration called triangle-aware graph filtration. Our coarsening approach is parameter-free and we prove that it is equivariant under rotations, translations and scaling of the initial spatial graph. We evaluate the performances of our method on synthetic and real spatial graphs, and show that it significantly reduces the graph sizes while preserving the relevant topological information."}
{"id": "2512.24009", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.24009", "abs": "https://arxiv.org/abs/2512.24009", "authors": ["Landon Hurley"], "title": "An exact unbiased semi-parametric maximum quasi-likelihood framework which is complete in the presence of ties", "comment": null, "summary": "This paper introduces a novel quasi-likelihood extension of the generalised Kendall \\(τ_{a}\\) estimator, together with an extension of the Kemeny metric and its associated covariance and correlation forms. The central contribution is to show that the U-statistic structure of the proposed coefficient \\(τ_κ\\) naturally induces a quasi-maximum likelihood estimation (QMLE) framework, yielding consistent Wald and likelihood ratio test statistics. The development builds on the uncentred correlation inner-product (Hilbert space) formulation of Emond and Mason (2002) and resolves the associated sub-Gaussian likelihood optimisation problem under the \\(\\ell_{2}\\)-norm via an Edgeworth expansion of higher-order moments. The Kemeny covariance coefficient \\(τ_κ\\) is derived within a novel likelihood framework for pairwise comparison-continuous random variables, enabling direct inference on population-level correlation between ranked or weakly ordered datasets. Unlike existing approaches that focus on marginal or pairwise summaries, the proposed framework supports sample-observed weak orderings and accommodates ties without information loss. Drawing parallels with Thurstone's Case V latent ordering model, we derive a quasi-likelihood-based tie model with analytic standard errors, generalising classical U-statistics. The framework applies to general continuous and discrete random variables and establishes formal equivalence to Bradley-Terry and Thurstone models, yielding a uniquely identified linear representation with both analytic and likelihood-based estimators."}
{"id": "2512.24927", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.24927", "abs": "https://arxiv.org/abs/2512.24927", "authors": ["Yuchen Jiao", "Na Li", "Changxiao Cai", "Gen Li"], "title": "Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach", "comment": null, "summary": "Higher-order ODE solvers have become a standard tool for accelerating diffusion probabilistic model (DPM) sampling, motivating the widespread view that first-order methods are inherently slower and that increasing discretization order is the primary path to faster generation. This paper challenges this belief and revisits acceleration from a complementary angle: beyond solver order, the placement of DPM evaluations along the reverse-time dynamics can substantially affect sampling accuracy in the low-neural function evaluation (NFE) regime.\n  We propose a novel training-free, first-order sampler whose leading discretization error has the opposite sign to that of DDIM. Algorithmically, the method approximates the forward-value evaluation via a cheap one-step lookahead predictor. We provide theoretical guarantees showing that the resulting sampler provably approximates the ideal forward-value trajectory while retaining first-order convergence. Empirically, across standard image generation benchmarks (CIFAR-10, ImageNet, FFHQ, and LSUN), the proposed sampler consistently improves sample quality under the same NFE budget and can be competitive with, and sometimes outperform, state-of-the-art higher-order samplers. Overall, the results suggest that the placement of DPM evaluations provides an additional and largely independent design angle for accelerating diffusion sampling."}
{"id": "2512.24414", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.24414", "abs": "https://arxiv.org/abs/2512.24414", "authors": ["Ramsés H. Mena", "Christos Merkatas", "Theodoros Nicoleris", "Carlos E. Rodríguez"], "title": "Exact finite mixture representations for species sampling processes", "comment": "27 pages, 4 figures", "summary": "Random probability measures, together with their constructions, representations, and associated algorithms, play a central role in modern Bayesian inference. A key class is that of proper species sampling processes, which offer a relatively simple yet versatile framework that extends naturally to non-exchangeable settings. We revisit this class from a computational perspective and show that they admit exact finite mixture representations. In particular, we prove that any proper species sampling process can be written, at the prior level, as a finite mixture with a latent truncation variable and reweighted atoms, while preserving its distributional features exactly. These finite formulations can be used as drop-in replacements in Bayesian mixture models, recasting posterior computation in terms of familiar finite-mixture machinery. This yields straightforward MCMC implementations and tractable expressions, while avoiding ad hoc truncations and model-specific constructions. The resulting representation preserves the full generality of the original infinite-dimensional priors while enabling practical gains in algorithm design and implementation."}
{"id": "2512.24515", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24515", "abs": "https://arxiv.org/abs/2512.24515", "authors": ["Jiani Wei", "Xiaocheng Shang"], "title": "Improving the stability of the covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling", "comment": null, "summary": "Stochastic gradient Langevin dynamics and its variants approximate the likelihood of an entire dataset, via random (and typically much smaller) subsets, in the setting of Bayesian sampling. Due to the (often substantial) improvement of the computational efficiency, they have been widely used in large-scale machine learning applications. It has been demonstrated that the so-called covariance-controlled adaptive Langevin (CCAdL) thermostat, which incorporates an additional term involving the covariance matrix of the noisy force, outperforms popular alternative methods. A moving average is used in CCAdL to estimate the covariance matrix of the noisy force, in which case the covariance matrix will converge to a constant matrix in long-time limit. Moreover, it appears in our numerical experiments that the use of a moving average could reduce the stability of the numerical integrators, thereby limiting the largest usable stepsize. In this article, we propose a modified CCAdL (i.e., mCCAdL) thermostat that uses the scaling part of the scaling and squaring method together with a truncated Taylor series approximation to the exponential to numerically approximate the exact solution to the subsystem involving the additional term proposed in CCAdL. We also propose a symmetric splitting method for mCCAdL, instead of an Euler-type discretisation used in the original CCAdL thermostat. We demonstrate in our numerical experiments that the newly proposed mCCAdL thermostat achieves a substantial improvement in the numerical stability over the original CCAdL thermostat, while significantly outperforming popular alternative stochastic gradient methods in terms of the numerical accuracy for large-scale machine learning applications."}
{"id": "2512.24222", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24222", "abs": "https://arxiv.org/abs/2512.24222", "authors": ["Tuhin Subhra Mahato", "Subhra Sankar Dhar"], "title": "A Robust Persistent Homology : Trimming Approach", "comment": null, "summary": "This article studies the robust version of persistent homology based on trimming methodology to capture the geometric feature through support of the data in presence of outliers. Precisely speaking, the proposed methodology works when the outliers lie outside the main data cloud as well as inside the data cloud. In the course of theoretical study, it is established that the Bottleneck distance between the proposed robust version of persistent homology and its population analogue can be made arbitrary small with a certain rate for a sufficiently large sample size. The practicability of the methodology is shown for various simulated data and bench mark real data associated with cellular biology."}
{"id": "2512.25025", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.25025", "abs": "https://arxiv.org/abs/2512.25025", "authors": ["Elynn Chen", "Yuefeng Han", "Jiayu Li", "Ke Xu"], "title": "Modewise Additive Factor Model for Matrix Time Series", "comment": null, "summary": "We introduce a Modewise Additive Factor Model (MAFM) for matrix-valued time series that captures row-specific and column-specific latent effects through an additive structure, offering greater flexibility than multiplicative frameworks such as Tucker and CP factor models. In MAFM, each observation decomposes into a row-factor component, a column-factor component, and noise, allowing distinct sources of variation along different modes to be modeled separately. We develop a computationally efficient two-stage estimation procedure: Modewise Inner-product Eigendecomposition (MINE) for initialization, followed by Complement-Projected Alternating Subspace Estimation (COMPAS) for iterative refinement. The key methodological innovation is that orthogonal complement projections completely eliminate cross-modal interference when estimating each loading space. We establish convergence rates for the estimated factor loading matrices under proper conditions. We further derive asymptotic distributions for the loading matrix estimators and develop consistent covariance estimators, yielding a data-driven inference framework that enables confidence interval construction and hypothesis testing. As a technical contribution of independent interest, we establish matrix Bernstein inequalities for quadratic forms of dependent matrix time series. Numerical experiments on synthetic and real data demonstrate the advantages of the proposed method over existing approaches."}
{"id": "2512.24515", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24515", "abs": "https://arxiv.org/abs/2512.24515", "authors": ["Jiani Wei", "Xiaocheng Shang"], "title": "Improving the stability of the covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling", "comment": null, "summary": "Stochastic gradient Langevin dynamics and its variants approximate the likelihood of an entire dataset, via random (and typically much smaller) subsets, in the setting of Bayesian sampling. Due to the (often substantial) improvement of the computational efficiency, they have been widely used in large-scale machine learning applications. It has been demonstrated that the so-called covariance-controlled adaptive Langevin (CCAdL) thermostat, which incorporates an additional term involving the covariance matrix of the noisy force, outperforms popular alternative methods. A moving average is used in CCAdL to estimate the covariance matrix of the noisy force, in which case the covariance matrix will converge to a constant matrix in long-time limit. Moreover, it appears in our numerical experiments that the use of a moving average could reduce the stability of the numerical integrators, thereby limiting the largest usable stepsize. In this article, we propose a modified CCAdL (i.e., mCCAdL) thermostat that uses the scaling part of the scaling and squaring method together with a truncated Taylor series approximation to the exponential to numerically approximate the exact solution to the subsystem involving the additional term proposed in CCAdL. We also propose a symmetric splitting method for mCCAdL, instead of an Euler-type discretisation used in the original CCAdL thermostat. We demonstrate in our numerical experiments that the newly proposed mCCAdL thermostat achieves a substantial improvement in the numerical stability over the original CCAdL thermostat, while significantly outperforming popular alternative stochastic gradient methods in terms of the numerical accuracy for large-scale machine learning applications."}
{"id": "2512.24587", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24587", "abs": "https://arxiv.org/abs/2512.24587", "authors": ["Sunay Joshi", "Yan Sun", "Hamed Hassani", "Edgar Dobriban"], "title": "MultiRisk: Multiple Risk Control via Iterative Score Thresholding", "comment": null, "summary": "As generative AI systems are increasingly deployed in real-world applications, regulating multiple dimensions of model behavior has become essential. We focus on test-time filtering: a lightweight mechanism for behavior control that compares performance scores to estimated thresholds, and modifies outputs when these bounds are violated. We formalize the problem of enforcing multiple risk constraints with user-defined priorities, and introduce two efficient dynamic programming algorithms that leverage this sequential structure. The first, MULTIRISK-BASE, provides a direct finite-sample procedure for selecting thresholds, while the second, MULTIRISK, leverages data exchangeability to guarantee simultaneous control of the risks. Under mild assumptions, we show that MULTIRISK achieves nearly tight control of all constraint risks. The analysis requires an intricate iterative argument, upper bounding the risks by introducing several forms of intermediate symmetrized risk functions, and carefully lower bounding the risks by recursively counting jumps in symmetrized risk functions between appropriate risk levels. We evaluate our framework on a three-constraint Large Language Model alignment task using the PKU-SafeRLHF dataset, where the goal is to maximize helpfulness subject to multiple safety constraints, and where scores are generated by a Large Language Model judge and a perplexity filter. Our experimental results show that our algorithm can control each individual risk at close to the target level."}
{"id": "2512.24223", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24223", "abs": "https://arxiv.org/abs/2512.24223", "authors": ["Yuanhui Luo", "Xinzhou Guo", "Yuqi Gu"], "title": "Valid and Efficient Two-Stage Latent Subgroup Analysis with Observational Data", "comment": null, "summary": "Subgroup analysis evaluates treatment effects across multiple sub-populations. When subgroups are defined by latent memberships inferred from imperfect measurements, the analysis typically involves two inter-connected models, a latent class model and a subgroup outcome model. The classical one-stage framework, which models the joint distribution of the two models, may be infeasible with observational data containing many confounders. The two-stage framework, which first estimates the latent class model and then performs subgroup analysis using estimated latent memberships, can accommodate potential confounders but may suffer from bias issues due to misclassification of latent subgroup memberships. This paper focuses on latent subgroups inferred from binary item responses and addresses when and how a valid two-stage latent subgroup analysis can be made with observational data. We investigate the maximum misclassification rate that a valid two-stage framework can tolerate. Introducing a spectral method perspective, we propose a two-stage approach to achieve the desired misclassification rate with the blessing of many item responses. Our method accommodates high-dimensional confounders, is computationally efficient and robust to noninformative items. In observational studies, our methods lead to consistent estimation and valid inference on latent subgroup effects. We demonstrate its merit through simulation studies and an application to educational assessment data."}
{"id": "2512.24768", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24768", "abs": "https://arxiv.org/abs/2512.24768", "authors": ["Nam Phuong Tran", "Andi Nika", "Goran Radanovic", "Long Tran-Thanh", "Debmalya Mandal"], "title": "Sparse Offline Reinforcement Learning with Corruption Robustness", "comment": null, "summary": "We investigate robustness to strong data corruption in offline sparse reinforcement learning (RL). In our setting, an adversary may arbitrarily perturb a fraction of the collected trajectories from a high-dimensional but sparse Markov decision process, and our goal is to estimate a near optimal policy. The main challenge is that, in the high-dimensional regime where the number of samples $N$ is smaller than the feature dimension $d$, exploiting sparsity is essential for obtaining non-vacuous guarantees but has not been systematically studied in offline RL. We analyse the problem under uniform coverage and sparse single-concentrability assumptions. While Least Square Value Iteration (LSVI), a standard approach for robust offline RL, performs well under uniform coverage, we show that integrating sparsity into LSVI is unnatural, and its analysis may break down due to overly pessimistic bonuses. To overcome this, we propose actor-critic methods with sparse robust estimator oracles, which avoid the use of pointwise pessimistic bonuses and provide the first non-vacuous guarantees for sparse offline RL under single-policy concentrability coverage. Moreover, we extend our results to the contaminated setting and show that our algorithm remains robust under strong contamination. Our results provide the first non-vacuous guarantees in high-dimensional sparse MDPs with single-policy concentrability coverage and corruption, showing that learning a near-optimal policy remains possible in regimes where traditional robust offline RL techniques may fail."}
{"id": "2512.24342", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24342", "abs": "https://arxiv.org/abs/2512.24342", "authors": ["Farimah Shamsi", "Andriy Derkach"], "title": "A Novel Approach for Data Integration with Multiple Heterogeneous Data Sources", "comment": null, "summary": "The integration of data from multiple sources is increasingly used to achieve larger sample sizes and enhance population diversity. Our previous work established that, under random sampling from the same underlying population, integrating large incomplete datasets with summary-level data produces unbiased parameter estimates. In this study, we develop a novel statistical framework that enables the integration of summary-level data with information from heterogeneous data sources by leveraging auxiliary information. The proposed approach estimates study-specific sampling weights using this auxiliary information and calibrates the estimating equations to obtain the full set of model parameters. We evaluate the performance of the proposed method through simulation studies under various sampling designs and illustrate its application by reanalyzing U.S. cancer registry data combined with summary-level odds ratio estimates for selected colorectal cancer (CRC) risk factors, while relaxing the random sampling assumption."}
{"id": "2512.24927", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.24927", "abs": "https://arxiv.org/abs/2512.24927", "authors": ["Yuchen Jiao", "Na Li", "Changxiao Cai", "Gen Li"], "title": "Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach", "comment": null, "summary": "Higher-order ODE solvers have become a standard tool for accelerating diffusion probabilistic model (DPM) sampling, motivating the widespread view that first-order methods are inherently slower and that increasing discretization order is the primary path to faster generation. This paper challenges this belief and revisits acceleration from a complementary angle: beyond solver order, the placement of DPM evaluations along the reverse-time dynamics can substantially affect sampling accuracy in the low-neural function evaluation (NFE) regime.\n  We propose a novel training-free, first-order sampler whose leading discretization error has the opposite sign to that of DDIM. Algorithmically, the method approximates the forward-value evaluation via a cheap one-step lookahead predictor. We provide theoretical guarantees showing that the resulting sampler provably approximates the ideal forward-value trajectory while retaining first-order convergence. Empirically, across standard image generation benchmarks (CIFAR-10, ImageNet, FFHQ, and LSUN), the proposed sampler consistently improves sample quality under the same NFE budget and can be competitive with, and sometimes outperform, state-of-the-art higher-order samplers. Overall, the results suggest that the placement of DPM evaluations provides an additional and largely independent design angle for accelerating diffusion sampling."}
{"id": "2512.24356", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.24356", "abs": "https://arxiv.org/abs/2512.24356", "authors": ["Max Thannheimer", "Marco Oesting"], "title": "Bayesian inference for functional extreme events defined via partially unobserved processes", "comment": "18 pages, 1 figure, 1 table", "summary": "In order to describe the extremal behaviour of some stochastic process $X$, approaches from univariate extreme value theory are typically generalized to the spatial domain. In particular, generalized peaks-over-threshold approaches allow for the consideration of single extreme events. These can be flexibly defined as exceedances of a risk functional $r$, such as a spatial average, applied to $X$. Inference for the resulting limit process, the so-called $r$-Pareto process, requires the evaluation of $r(X)$ and thus the knowledge of the whole process $X$. In many practical applications, however, observations of $X$ are only available at scattered sites. To overcome this issue, we propose a two-step MCMC-algorithm in a Bayesian framework. In a first step, we sample from $X$ conditionally on the observations in order to evaluate which observations lead to $r$-exceedances. In a second step, we use these exceedances to sample from the posterior distribution of the parameters of the limiting $r$-Pareto process. Alternating these steps results in a full Bayesian model for the extremes of $X$. We show that, under appropriate assumptions, the probability of classifying an observation as $r$-exceedance in the first step converges to the desired probability. Furthermore, given the first step, the distribution of the Markov chain constructed in the second step converges to the posterior distribution of interest. The procedure is compared to the Bayesian version of the standard procedure in a simulation study."}
{"id": "2512.24392", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24392", "abs": "https://arxiv.org/abs/2512.24392", "authors": ["Jeongjin Lee", "Jennifer Wadsworth"], "title": "Geometric criteria for identifying extremal dependence and flexible modeling via additive mixtures", "comment": null, "summary": "The framework of geometric extremes is based on the convergence of scaled sample clouds onto a limit set, characterized by a gauge function, with the shape of the limit set determining extremal dependence structures. While it is known that a blunt limit set implies asymptotic independence, the absence of bluntness can be linked to both asymptotic dependence and independence. Focusing on the bivariate case, under a truncated gamma modeling assumption with bounded angular density, we show that a ``pointy'' limit set implies asymptotic dependence, thus offering practical geometric criteria for identifying extremal dependence classes. Suitable models for the gauge function offer the ability to capture asymptotically independent or dependent data structures, without requiring prior knowledge of the true extremal dependence structure. The geometric approach thus offers a simple alternative to various parametric copula models that have been developed for this purpose in recent years. We consider two types of additively mixed gauge functions that provide a smooth interpolation between asymptotic dependence and asymptotic independence. We derive their explicit forms, explore their properties, and establish connections to the developed geometric criteria. Through a simulation study, we evaluate the effectiveness of the geometric approach with additively mixed gauge functions, comparing its performance to existing methodologies that account for both asymptotic dependence and asymptotic independence. The methodology is computationally efficient and yields reliable performance across various extremal dependence scenarios."}
{"id": "2512.24413", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24413", "abs": "https://arxiv.org/abs/2512.24413", "authors": ["Grace V. Ringlein", "Trang Quynh Nguyen", "Peter P. Zandi", "Elizabeth A. Stuart", "Harsh Parikh"], "title": "Demystifying Proximal Causal Inference", "comment": "32 pages, 5 figures", "summary": "Proximal causal inference (PCI) has emerged as a promising framework for identifying and estimating causal effects in the presence of unobserved confounders. While many traditional causal inference methods rely on the assumption of no unobserved confounding, this assumption is likely often violated. PCI mitigates this challenge by relying on an alternative set of assumptions regarding the relationships between treatment, outcome, and auxiliary variables that serve as proxies for unmeasured confounders. We review existing identification results, discuss the assumptions necessary for valid causal effect estimation via PCI, and compare different PCI estimation methods. We offer practical guidance on operationalizing PCI, with a focus on selecting and evaluating proxy variables using domain knowledge, measurement error perspectives, and negative control analogies. Through conceptual examples, we demonstrate tensions in proxy selection and discuss the importance of clearly defining the unobserved confounding mechanism. By bridging formal results with applied considerations, this work aims to demystify PCI, encourage thoughtful use in practice, and identify open directions for methodological development and empirical research."}
{"id": "2512.24414", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.24414", "abs": "https://arxiv.org/abs/2512.24414", "authors": ["Ramsés H. Mena", "Christos Merkatas", "Theodoros Nicoleris", "Carlos E. Rodríguez"], "title": "Exact finite mixture representations for species sampling processes", "comment": "27 pages, 4 figures", "summary": "Random probability measures, together with their constructions, representations, and associated algorithms, play a central role in modern Bayesian inference. A key class is that of proper species sampling processes, which offer a relatively simple yet versatile framework that extends naturally to non-exchangeable settings. We revisit this class from a computational perspective and show that they admit exact finite mixture representations. In particular, we prove that any proper species sampling process can be written, at the prior level, as a finite mixture with a latent truncation variable and reweighted atoms, while preserving its distributional features exactly. These finite formulations can be used as drop-in replacements in Bayesian mixture models, recasting posterior computation in terms of familiar finite-mixture machinery. This yields straightforward MCMC implementations and tractable expressions, while avoiding ad hoc truncations and model-specific constructions. The resulting representation preserves the full generality of the original infinite-dimensional priors while enabling practical gains in algorithm design and implementation."}
{"id": "2512.24442", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24442", "abs": "https://arxiv.org/abs/2512.24442", "authors": ["Lindsey E. Turner", "Carolyn T. Bramante", "Thomas A. Murray"], "title": "Model-Assisted Bayesian Estimators of Transparent Population Level Summary Measures for Ordinal Outcomes in Randomized Controlled Trials", "comment": "28 pages, 7 figures", "summary": "In randomized controlled trials, ordinal outcomes typically improve statistical efficiency over binary outcomes. The treatment effect on an ordinal outcome is usually described by the odds ratio from a proportional odds model, but this summary measure lacks transparency with respect to its emphasis on the components of the ordinal outcome when proportional odds is violated. We propose various summary measures for ordinal outcomes that are fully transparent in this regard, including 'weighted geometric mean' odds ratios and relative risks, and 'weighted mean' risk differences. We also develop and evaluate efficient model-assisted Bayesian estimators for these population level summary measures based on non-proportional odds models that facilitate covariate adjustment with marginalization via the Bayesian bootstrap. We propose a weighting scheme that engenders appealing invariance properties, including to whether the ordinal outcome is ordered from best to worst versus worst to best. Using computer simulation, we show that comparative testing based on the proposed population level summary measures performs well relative to the conventional proportional odds approach. We also report an analysis of the COVID-OUT trial, which exhibits evidence of non-proportional odds."}
{"id": "2512.24450", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.24450", "abs": "https://arxiv.org/abs/2512.24450", "authors": ["The Tien Mai"], "title": "Robust reduced rank regression under heavy-tailed noise and missing data via non-convex penalization", "comment": null, "summary": "Reduced rank regression (RRR) is a fundamental tool for modeling multiple responses through low-dimensional latent structures, offering both interpretability and strong predictive performance in high-dimensional settings. Classical RRR methods, however, typically rely on squared loss and Gaussian noise assumptions, rendering them sensitive to heavy-tailed errors, outliers, and data contamination. Moreover, the presence of missing data--common in modern applications--further complicates reliable low-rank estimation. In this paper, we propose a robust reduced rank regression framework that simultaneously addresses heavy-tailed noise, outliers, and missing data. Our approach combines a robust Huber loss with nonconvex spectral regularization, specifically the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). Unlike convex nuclear-norm regularization, the proposed nonconvex penalties alleviate excessive shrinkage and enable more accurate recovery of the underlying low-rank structure. The method also accommodates missing data in the response matrix without requiring imputation. We develop an efficient proximal gradient algorithm based on alternating updates and tailored spectral thresholding. Extensive simulation studies demonstrate that the proposed methods substantially outperform nuclear-norm-based and non-robust alternatives under heavy-tailed noise and contamination. An application to cancer cell line data set further illustrates the practical advantages of the proposed robust RRR framework.\n  Our method is implemented in the R package rrpackrobust available at https://github.com/tienmt/rrpackrobust."}
{"id": "2512.24521", "categories": ["stat.ME", "cs.HC", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.24521", "abs": "https://arxiv.org/abs/2512.24521", "authors": ["Ron Kohavi", "Jakub Linowski", "Lukas Vermeer", "Fabrice Boisseranc", "Joachim Furuseth", "Andrew Gelman", "Guido Imbens", "Ravikiran Rajagopal"], "title": "Power Analysis is Essential: High-Powered Tests Suggest Minimal to No Effect of Rounded Shapes on Click-Through Rates", "comment": "41 pages, 9 figures", "summary": "Underpowered studies (below 50%) suffer from the winner's curse: a statistically significant result must exaggerate the true treatment effect to meet the significance threshold. A study by Dipayan Biswas, Annika Abell, and Roger Chacko published in the Journal of Consumer Research (2023) reported that in an A/B test simply rounding the corners of square buttons increased the online click-through rate by 55% (p-value 0.037)$\\unicode{x2014}$a striking finding with potentially wide-ranging implications for the digital industry that is seeking to enhance consumer engagement. Drawing on our experience with tens of thousands of A/B tests, many involving similar user interface modifications, we found this dramatic claim implausibly large. To evaluate the claim, we conducted three high-powered A/B tests, each involving over two thousand times more users than the original study. All three experiments yielded effect size estimates that were approximately two orders of magnitude smaller than initially reported, with 95% confidence intervals that include zero, that is, not statistically significant at the 0.05 level. Two additional independent replications by Evidoo found similarly small effects. These findings underscore the critical importance of power analysis and experimental design to increase trust and reproducibility of results."}
{"id": "2512.24588", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24588", "abs": "https://arxiv.org/abs/2512.24588", "authors": ["Kwangok Seo", "Johan Lim", "Hyungwon Choi", "Jaesik Jeong"], "title": "Multiple Testing of One-Sided Hypotheses with Conservative $p$-values", "comment": null, "summary": "We study a large-scale one-sided multiple testing problem in which test statistics follow normal distributions with unit variance, and the goal is to identify signals with positive mean effects. A common approach is to compute $p$-values under the assumption that all null means are exactly zero and then apply standard multiple testing procedures such as the Benjamini--Hochberg (BH) or Storey--BH method. However, because the null hypothesis is composite, some null means may be strictly negative. In this case, the resulting $p$-values are conservative, leading to a substantial loss of power. Existing methods address this issue by modifying the multiple testing procedure itself, for example through conditioning strategies or discarding rules. In contrast, we focus on correcting the $p$-values so that they are exact under the null. Specifically, we estimate the marginal null distribution of the test statistics within an empirical Bayes framework and construct refined $p$-values based on this estimated distribution. These refined $p$-values can then be directly used in standard multiple testing procedures without modification. Extensive simulation studies show that the proposed method substantially improves power when $p$-values are conservative, while achieving comparable performance to existing methods when $p$-values are exact. An application to phosphorylation data further demonstrates the practical effectiveness of our approach."}
{"id": "2512.24611", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24611", "abs": "https://arxiv.org/abs/2512.24611", "authors": ["Kwangok Seo", "Johan Lim", "Kaiwen Wang", "Dohwan Park", "Shota Katayama", "Xinlei Wang"], "title": "Empirical Bayes Method for Large Scale Multiple Testing with Heteroscedastic Errors", "comment": null, "summary": "In this paper, we address the normal mean inference problem, which involves testing multiple means of normal random variables with heteroscedastic variances. Most existing empirical Bayes methods for this setting are developed under restrictive assumptions, such as the scaled inverse-chi-squared prior for variances and unimodality for the non-null mean distribution. However, when either of these assumptions is violated, these methods often fail to control the false discovery rate (FDR) at the target level or suffer from a substantial loss of power. To overcome these limitations, we propose a new empirical Bayes method, gg-Mix, which assumes only independence between the normal means and variances, without imposing any structural restrictions on their distributions. We thoroughly evaluate the FDR control and power of gg-Mix through extensive numerical studies and demonstrate its superior performance compared to existing methods. Finally, we apply gg-Mix to three real data examples to further illustrate the practical advantages of our approach."}
{"id": "2512.24748", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.24748", "abs": "https://arxiv.org/abs/2512.24748", "authors": ["Zhijian Wang", "Xingbai Xu", "Tuo Liu"], "title": "Quasi-Maximum Likelihood Estimation for a Genuinely Unbalanced Dynamic Network Panel Data Model", "comment": null, "summary": "This paper develops a quasi-maximum likelihood estimator for genuinely unbalanced dynamic network panel data models with individual fixed effects. We propose a model that accommodates contemporaneous and lagged network spillovers, temporal dependence, and a listing effect that activates upon a unit's first appearance in the panel. We establish the consistency of the QMLE as both $N$ and $T$ go to infinity, derive its asymptotic distribution, and identify an asymptotic bias arising from incidental parameters when $N$ is asymptotically large relative to $T$. Based on the asymptotic bias expression, we propose a bias-corrected estimator that is asymptotically unbiased and normally distributed under appropriate regularity conditions. Monte Carlo experiments examine the finite sample performance of the bias-corrected estimator across different criteria, including bias, RMSE, coverage probability, and the normality of the estimator. The empirical application to Airbnb listings from New Zealand and New York City reveals region-specific patterns in spatial and temporal price transmission, illustrating the importance of modeling genuine unbalancedness in dynamic network settings."}
{"id": "2512.25025", "categories": ["stat.ME", "econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.25025", "abs": "https://arxiv.org/abs/2512.25025", "authors": ["Elynn Chen", "Yuefeng Han", "Jiayu Li", "Ke Xu"], "title": "Modewise Additive Factor Model for Matrix Time Series", "comment": null, "summary": "We introduce a Modewise Additive Factor Model (MAFM) for matrix-valued time series that captures row-specific and column-specific latent effects through an additive structure, offering greater flexibility than multiplicative frameworks such as Tucker and CP factor models. In MAFM, each observation decomposes into a row-factor component, a column-factor component, and noise, allowing distinct sources of variation along different modes to be modeled separately. We develop a computationally efficient two-stage estimation procedure: Modewise Inner-product Eigendecomposition (MINE) for initialization, followed by Complement-Projected Alternating Subspace Estimation (COMPAS) for iterative refinement. The key methodological innovation is that orthogonal complement projections completely eliminate cross-modal interference when estimating each loading space. We establish convergence rates for the estimated factor loading matrices under proper conditions. We further derive asymptotic distributions for the loading matrix estimators and develop consistent covariance estimators, yielding a data-driven inference framework that enables confidence interval construction and hypothesis testing. As a technical contribution of independent interest, we establish matrix Bernstein inequalities for quadratic forms of dependent matrix time series. Numerical experiments on synthetic and real data demonstrate the advantages of the proposed method over existing approaches."}
{"id": "2512.25045", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.25045", "abs": "https://arxiv.org/abs/2512.25045", "authors": ["Christopher M. Hans", "Ningyi Liu"], "title": "Bayesian Elastic Net Regression with Structured Prior Dependence", "comment": null, "summary": "Many regularization priors for Bayesian regression assume the regression coefficients are a priori independent. In particular this is the case for standard Bayesian treatments of the lasso and the elastic net. While independence may be reasonable in some data-analytic settings, incorporating dependence in these prior distributions provides greater modeling flexibility. This paper introduces the orthant normal distribution in its general form and shows how it can be used to structure prior dependence in the Bayesian elastic net regression model. An L1-regularized version of Zellner's g prior is introduced as a special case, creating a new link between the literature on penalized optimization and an important class of regression priors. Computation is challenging due to an intractable normalizing constant in the prior. We avoid this issue by modifying slightly a standard prior of convenience for the hyperparameters in such a way to enable simple and fast Gibbs sampling of the posterior distribution. The benefit of including structured prior dependence in the Bayesian elastic net regression model is demonstrated through simulation and a near-infrared spectroscopy data example."}
{"id": "2512.25056", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.25056", "abs": "https://arxiv.org/abs/2512.25056", "authors": ["Liliang Wang", "Alex Gorodetsky"], "title": "Sequential Bayesian parameter-state estimation in dynamical systems with noisy and incomplete observations via a variational framework", "comment": "31 pages, 8 figures", "summary": "Online joint estimation of unknown parameters and states in a dynamical system with uncertainty quantification is crucial in many applications. For example, digital twins dynamically update their knowledge of model parameters and states to support prediction and decision-making. Reliability and computational speed are vital for DTs. Online parameter-state estimation ensures computational efficiency, while uncertainty quantification is essential for making reliable predictions and decisions. In parameter-state estimation, the joint distribution of the state and model parameters conditioned on the data, termed the joint posterior, provides accurate uncertainty quantification. Because the joint posterior is generally intractable to compute, this paper presents an online variational inference framework to compute its approximation at each time step. The approximation is factorized into a marginal distribution over the model parameters and a state distribution conditioned on the parameters. This factorization enables recursive updates through a two-stage procedure: first, the parameter posterior is approximated via variational inference; second, the state distribution conditioned on the parameters is computed using Gaussian filtering based on the estimated parameter posterior. The algorithmic design is supported by a theorem establishing upper bounds on the joint posterior approximation error. Numerical experiments demonstrate that the proposed method (i) matches the performance of the joint particle filter in low-dimensional problems, accurately inferring both unobserved states and unknown parameters of dynamical and observation models; (ii) remains robust under noisy, partial observations and model discrepancies in a chaotic Lorenz 96 system; and (iii) scales effectively to a high-dimensional convection-diffusion system, where it outperforms the joint ensemble Kalman filter."}
{"id": "2512.24515", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24515", "abs": "https://arxiv.org/abs/2512.24515", "authors": ["Jiani Wei", "Xiaocheng Shang"], "title": "Improving the stability of the covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling", "comment": null, "summary": "Stochastic gradient Langevin dynamics and its variants approximate the likelihood of an entire dataset, via random (and typically much smaller) subsets, in the setting of Bayesian sampling. Due to the (often substantial) improvement of the computational efficiency, they have been widely used in large-scale machine learning applications. It has been demonstrated that the so-called covariance-controlled adaptive Langevin (CCAdL) thermostat, which incorporates an additional term involving the covariance matrix of the noisy force, outperforms popular alternative methods. A moving average is used in CCAdL to estimate the covariance matrix of the noisy force, in which case the covariance matrix will converge to a constant matrix in long-time limit. Moreover, it appears in our numerical experiments that the use of a moving average could reduce the stability of the numerical integrators, thereby limiting the largest usable stepsize. In this article, we propose a modified CCAdL (i.e., mCCAdL) thermostat that uses the scaling part of the scaling and squaring method together with a truncated Taylor series approximation to the exponential to numerically approximate the exact solution to the subsystem involving the additional term proposed in CCAdL. We also propose a symmetric splitting method for mCCAdL, instead of an Euler-type discretisation used in the original CCAdL thermostat. We demonstrate in our numerical experiments that the newly proposed mCCAdL thermostat achieves a substantial improvement in the numerical stability over the original CCAdL thermostat, while significantly outperforming popular alternative stochastic gradient methods in terms of the numerical accuracy for large-scale machine learning applications."}
{"id": "2512.24604", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.24604", "abs": "https://arxiv.org/abs/2512.24604", "authors": ["Ryo Ohashi", "Hiroyasu Abe", "Fumitake Sakaori"], "title": "Generalized Poisson Matrix Factorization for Overdispersed Count Data", "comment": null, "summary": "Non-negative matrix factorization (NMF) is widely used as a feature extraction technique for matrices with non-negative entries, such as image data, purchase histories, and other types of count data. In NMF, a non-negative matrix is decomposed into the product of two non-negative matrices, and the approximation accuracy is evaluated by a loss function. If the Kullback-Leibler divergence is chosen as the loss function, the estimation coincides with maximum likelihood under the assumption that the data entries are distributed according to a Poisson distribution. To address overdispersion, negative binomial matrix factorization has recently been proposed as an extension of the Poisson-based model. However, the negative binomial distribution often generates an excessive number of zeros, which limits its expressive capacity. In this study, we propose a non-negative matrix factorization based on the generalized Poisson distribution, which can flexibly accommodate overdispersion, and we introduce a maximum likelihood approach for parameter estimation. This methodology provides a more versatile framework than existing models, thereby extending the applicability of NMF to a broader class of count data."}
