<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 16]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Bayesian power spectral density estimation for LISA noise based on P-splines with a parametric boost](https://arxiv.org/abs/2510.00533)
*Nazeela Aimen,Patricio Maturana-Russel,Avi Vajpeyi,Nelson Christensen,Renate Meyer*

Main category: stat.CO

TL;DR: 提出了一种快速贝叶斯方法用于估计LISA数据的功率谱密度，结合参数化和非参数化模型，计算效率高且灵活。


<details>
  <summary>Details</summary>
Motivation: 需要灵活高效的噪声表征方法来进行引力波参数的精确估计，特别是针对LISA数据分析的长平稳时间序列。

Method: 将PSD建模为参数化和非参数化分量的几何平均，非参数化分量使用惩罚B样条混合表示，采用自适应节点放置和分层粗糙度惩罚先验。

Result: 在模拟AR(4)数据上验证了估计器一致性，应用于模拟LISA噪声数据时相对积分绝对误差达到10^-2量级，计算时间少于3分钟。

Conclusion: 该方法实现了稳定灵活的PSD估计，计算时间从小时级缩短到分钟级，适合迭代分析流程和多年度任务数据集。

Abstract: Flexible and efficient noise characterization is crucial for the precise
estimation of gravitational wave parameters. We introduce a fast and accurate
Bayesian method for estimating the power spectral density (PSD) of long,
stationary time series tailored specifically for LISA data analysis. Our
approach models the PSD as a geometric mean of a parametric and a nonparametric
component, combining the computational efficiency of parametric models with the
flexibility to capture deviations from theoretical expectations. The
nonparametric component is expressed by a mixture of penalized B-splines.
Adaptive, data-driven knot placement performed once during initialization
eliminates computationally expensive reversible-jump Markov Chain Monte Carlo,
while hierarchical roughness penalty priors prevent overfitting. This design
yields stable, flexible PSD estimates with runtimes of minutes instead of
hours. Validation on simulated autoregressive AR(4) data demonstrates estimator
consistency. It shows that well-matched parametric components reduce the
integrated absolute error compared to an uninformative baseline, requiring
fewer spline knots to achieve comparable accuracy. Applied to a year of
simulated LISA $X$-channel noise, our method achieves relative integrated
absolute errors of $\mathcal{O}(10^{-2})$ with computation times less than
three minutes, which makes it suitable for iterative analysis pipelines and
multi-year mission datasets.

</details>


### [2] [Sequential Bayesian Inference of the GTN Damage Model Using Multimodal Experimental Data](https://arxiv.org/abs/2510.01016)
*Mohammad Ali Seyed Mahmoud,Dominic Renner,Ali Khosravani,Surya R. Kalidindi*

Main category: stat.CO

TL;DR: 提出了一种基于顺序贝叶斯推理的GTN损伤模型参数识别框架，利用多模态实验数据（力-位移曲线和DIC应变场），通过PCA降维和T-MCMC采样实现可靠参数校准。


<details>
  <summary>Details</summary>
Motivation: 韧性损伤模型参数识别具有挑战性，因为损伤物理过程局限于小区域，其信号在试样级测量中被稀释。需要开发能够有效利用多模态数据的参数校准方法。

Method: 采用顺序贝叶斯推理框架：1）建立物理模拟器的低成本代理模型；2）使用PCA对高维F-D曲线和DIC场进行降维；3）应用T-MCMC采样材料参数后验分布；4）探索不同数据顺序对结果的影响。

Result: PCA成功实现了高维数据的低维表示，使BI框架得以应用。更重要的是，发现先对F-D曲线后对DIC场应用贝叶斯推理能显著改善GTN参数估计精度。

Conclusion: 顺序贝叶斯推理结合PCA降维是校准GTN损伤模型的有效方法，数据应用顺序对参数识别结果有重要影响，先F-D后DIC的顺序效果最佳。

Abstract: Reliable parameter identification in ductile damage models remains
challenging because the salient physics of damage progression are localized to
small regions in material responses, and their signatures are often diluted in
specimen-level measurements. Here, we propose a sequential Bayesian Inference
(BI) framework for the calibration of the Gurson-Tvergaard-Needleman (GTN)
model using multimodal experimental data (i.e., the specimen-level
force-displacement (F-D) measurements and the spatially resolved digital image
correlation (DIC) strain fields). This calibration approach builds on a
previously developed two-step BI framework that first establishes a
low-computational-cost emulator for a physics-based simulator (here, a finite
element model incorporating the GTN material model) and then uses the
experimental data to sample posteriors for the material model parameters using
the Transitional Markov Chain Monte Carlo (T-MCMC). A central challenge to the
successful application of this BI framework to the present problem arises from
the high-dimensional representations needed to capture the salient features
embedded in the F-D curves and the DIC fields. In this paper, it is
demonstrated that Principal Component Analysis (PCA) provides low-dimensional
representations that make it possible to apply the BI framework to the problem.
Most importantly, it is shown that the sequence in which the BI is applied has
a dramatic influence on the results obtained. Specifically, it is observed that
applying BI first on F-D curves and subsequently on the DIC fields produces
improved estimates of the GTN parameters. Possible causes for these
observations are discussed in this paper, using AA6111 aluminum alloy as a case
study.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [Remote Auditing: Design-based Tests of Randomization, Selection, and Missingness with Broadly Accessible Satellite Imagery](https://arxiv.org/abs/2510.00128)
*Connor T. Jerzak,Adel Daoud*

Main category: stat.ME

TL;DR: 提出了一种基于设计的远程审计方法，使用预处理卫星图像来测试实验分配是否独立于当地条件，为随机对照试验提供有限样本有效的诊断工具。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验在现场实施中可能出现偏差，需要一种仅使用预处理数据就能验证随机化质量的诊断方法，特别是在基线调查昂贵或不可行的情况下。

Method: 基于条件随机化测试的远程审计，使用预处理卫星特征评估治疗分配的可预测性，通过最大统计量控制多重性，尊重区块和聚类设计。

Result: 在乌干达青年机会项目中审计证实了随机化质量，同时标记了选择和缺失数据风险；在孟加拉国学校试验中发现分配高度可预测，与已知的不规范问题一致。

Conclusion: 远程审计补充了平衡测试，降低了早期阶段成本，在基线调查昂贵或不可行时能够快速进行设计检查。

Abstract: Randomized controlled trials (RCTs) are the benchmark for causal inference,
yet field implementation can deviate. We here present a remote audit - a
design-based, preregistrable diagnostic that uses only pre-treatment satellite
imagery to test whether assignment is independent of local conditions. The
conditional randomization test of the remote audit evaluates whether treatment
assignment is more predictable from pre-treatment satellite features than
expected under the experiment's registered mechanism, providing a finite-sample
valid, design-based diagnostic that requires no parametric assumptions. The
procedure is finite-sample valid, honors blocks and clusters, and controls
multiplicity across image models and resolutions via a max-statistic. We
illustrate with two RCTs: Uganda's Youth Opportunities Program, where the audit
corroborates randomization and flags selection and missing-data risks; and a
school-based trial in Bangladesh, where assignment is highly predictable from
pre-treatment features relative to the stated design, consistent with
independent concerns about irregularities. Remote audits complement balance
tests, lower early-stage costs, and enable rapid design checks when baseline
surveys are expensive or infeasible.

</details>


### [4] [Dimension Reduction for Characterizing Sexual Dimorphism in Biomechanics of the Temporomandibular Joint](https://arxiv.org/abs/2510.00217)
*Sung Hee Park,Xin Zhang,Elizabeth Slate,Shuchun Sun,Hai Yao*

Main category: stat.ME

TL;DR: 提出条件交叉协方差缩减(CCR)模型分析颅面骨骼形态与颞下颌关节相关咀嚼肌附着之间的性别差异，使用稀疏SVD算法和SPSS方法选择重要变量


<details>
  <summary>Details</summary>
Motivation: 理解性别二态性对颞下颌关节疾病等肌肉骨骼状况研究至关重要，需要分析颅面骨骼形态与咀嚼肌附着之间的性别特异性关系

Method: 使用10男11女尸体头部数据，提出CCR模型分析两组随机变量在二元变量(性别)条件下的动态关联，采用稀疏SVD算法和SPSS方法选择重要变量

Result: 开发了能够识别颅骨与肌肉附着之间最显著性别相关关系的统计模型

Conclusion: CCR模型为分析生物医学数据中的性别差异提供了有效工具，特别适用于颞下颌关节相关研究

Abstract: Sexual dimorphism is a critical factor in many biological and medical
research fields. In biomechanics and bioengineering, understanding sex
differences is crucial for studying musculoskeletal conditions such as
temporomandibular disorder (TMD). This paper focuses on the association between
the craniofacial skeletal morphology and temporomandibular joint (TMJ) related
masticatory muscle attachments to discern sex differences. Data were collected
from 10 male and 11 female cadaver heads to investigate sex-specific
relationships between the skull and muscles. We propose a conditional
cross-covariance reduction (CCR) model, designed to examine the dynamic
association between two sets of random variables conditioned on a third binary
variable (e.g., sex), highlighting the most distinctive sex-related
relationships between skull and muscle attachments in the human cadaver data.
Under the CCR model, we employ a sparse singular value decomposition algorithm
and introduce a sequential permutation for selecting sparsity (SPSS) method to
select important variables and to determine the optimal number of selected
variables.

</details>


### [5] [Assumption-lean Inference for Network-linked Data](https://arxiv.org/abs/2510.00287)
*Wei Li,Nilanjan Chakraborty,Robert Lunde*

Main category: stat.ME

TL;DR: 该论文提出了一个假设宽松的框架，用于网络链接回归问题，其中协变量可能包括为每个节点计算的网络汇总统计量。作者建立了联合可交换回归阵列的表示定理，并针对不同投影参数建立了渐近正态性和bootstrap一致性条件。


<details>
  <summary>Details</summary>
Motivation: 在网络数据设置中，潜变量通常控制图中的连接概率，这使得经典回归假设更加不可靠。因此需要开发一个假设宽松的框架来处理网络链接回归问题。

Method: 提出了联合可交换回归阵列的表示定理，考虑了两种不同的投影参数作为潜在目标，并建立了使用网络统计量（包括局部子图频率和谱嵌入）作为协变量时的渐近正态性和bootstrap一致性条件。

Result: 在线性回归中使用局部计数统计量时，偏差校正估计器可以在较弱的稀疏条件下针对更自然的推断目标。模拟数据和实际数据验证了所提出的推断工具的有效性。

Conclusion: 该研究为网络链接回归问题提供了一个稳健的统计推断框架，能够处理网络数据中常见的潜变量问题，并在实际应用中表现出良好的性能。

Abstract: We consider statistical inference for network-linked regression problems,
where covariates may include network summary statistics computed for each node.
In settings involving network data, it is often natural to posit that latent
variables govern connection probabilities in the graph. Since the presence of
these latent features makes classical regression assumptions even less tenable,
we propose an assumption-lean framework for linear regression with jointly
exchangeable regression arrays. We establish an analog of the Aldous-Hoover
representation for such arrays, which may be of independent interest. Moreover,
we consider two different projection parameters as potential targets and
establish conditions under which asymptotic normality and bootstrap consistency
hold when commonly used network statistics, including local subgraph
frequencies and spectral embeddings, are used as covariates. In the case of
linear regression with local count statistics, we show that a bias-corrected
estimator allows one to target a more natural inferential target under weaker
sparsity conditions compared to the OLS estimator. Our inferential tools are
illustrated using both simulated data and real data related to the academic
climate of elementary schools.

</details>


### [6] [Structural Refinement of Bayesian Networks for Efficient Model Parameterisation](https://arxiv.org/abs/2510.00334)
*Kieran Drury,Martine J. Barons,Jim Q. Smith*

Main category: stat.ME

TL;DR: 本文回顾了贝叶斯网络中条件概率表(CPT)的结构精化近似方法，通过心血管风险评估案例评估各种方法，为实践者提供选择指导。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络建模常面临数据稀缺问题，需要专家判断来确定大量CPT参数，但参数数量通常过于庞大，因此需要开发CPT近似方法来减少参数数量和复杂度。

Method: 回顾多种结构精化方法，包括各种CPT近似技术，通过心血管风险评估的贝叶斯网络模型进行实例评估。

Result: 评估了各种方法的固有特性和要求，通过实际案例展示了不同近似方法的效果。

Conclusion: 为贝叶斯网络实践者提供了实用指导，帮助他们在直接参数化CPT不可行时选择合适的替代方法。

Abstract: Many Bayesian network modelling applications suffer from the issue of data
scarcity. Hence the use of expert judgement often becomes necessary to
determine the parameters of the conditional probability tables (CPTs)
throughout the network. There are usually a prohibitively large number of these
parameters to determine, even when complementing any available data with expert
judgements. To address this challenge, a number of CPT approximation methods
have been developed that reduce the quantity and complexity of parameters
needing to be determined to fully parameterise a Bayesian network. This paper
provides a review of a variety of structural refinement methods that can be
used in practice to efficiently approximate a CPT within a Bayesian network. We
not only introduce and discuss the intrinsic properties and requirements of
each method, but we evaluate each method through a worked example on a Bayesian
network model of cardiovascular risk assessment. We conclude with practical
guidance to help Bayesian network practitioners choose an alternative approach
when direct parameterisation of a CPT is infeasible.

</details>


### [7] [Bayesian Cox model with graph-structured variable selection priors for multi-omics biomarker identification](https://arxiv.org/abs/2503.13078)
*Tobias Østmo Hermansen,Manuela Zucknick,Zhi Zhao*

Main category: stat.ME

TL;DR: 提出了一种带有图结构选择先验的惩罚半参数贝叶斯Cox模型，用于通过生物意义图进行多组学特征的稀疏识别，提高变量选择的可靠性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 癌症研究的重要目标是基于最小基因组和分子标记面板进行患者生存预后。纯数据驱动模型可能产生不可解释的结果，需要结合生物知识。

Method: 使用马尔可夫随机场先验的惩罚半参数贝叶斯Cox模型，通过生物意义图捕捉多组学特征间的已知关系，进行稀疏变量选择。

Result: 模拟结果显示，与无先验知识的方法相比，该模型产生更可靠稳定的变量选择和不相上下的生存预测性能，且对部分正确的图结构具有鲁棒性。

Conclusion: 该模型能够验证已知信息并识别新的生物标志物，在真实癌症数据中具有应用价值，即使网络信息不完全已知，图结构仍然有用。

Abstract: An important goal in cancer research is the survival prognosis of a patient
based on a minimal panel of genomic and molecular markers such as genes or
proteins. Purely data-driven models without any biological knowledge can
produce non-interpretable results. We propose a penalized semiparametric
Bayesian Cox model with graph-structured selection priors for sparse
identification of multi-omics features by making use of a biologically
meaningful graph via a Markov random field (MRF) prior to capturing known
relationships between multi-omics features. Since the fixed graph in the MRF
prior is for the prior probability distribution, it is not a hard constraint to
determine variable selection, so the proposed model can verify known
information and has the potential to identify new and novel biomarkers for
drawing new biological knowledge. Our simulation results show that the proposed
Bayesian Cox model with graph-based prior knowledge results in more trustable
and stable variable selection and non-inferior survival prediction, compared to
methods modeling the covariates independently without any prior knowledge. The
results also indicate that the performance of the proposed model is robust to a
partially correct graph in the MRF prior, meaning that in a real setting where
not all the true network information between covariates is known, the graph can
still be useful. The proposed model is applied to the primary invasive breast
cancer patients data in The Cancer Genome Atlas project.

</details>


### [8] [An Accurate Standard Error Estimation for Quadratic Exponential Logistic Regressions by Applying Generalized Estimating Equations to Pseudo-Likelihoods](https://arxiv.org/abs/2510.00431)
*Ong Wei Yong,Lee Shao-Man,Hsueh Chia-Ming,Chang Sheng-Mao*

Main category: stat.ME

TL;DR: 本文研究了二次指数二元分布的估计方法，发现使用广义估计方程结合独立工作相关结构能得到一致估计，而依赖结构可能引入偏差。


<details>
  <summary>Details</summary>
Motivation: 二次指数二元分布在纵向和网络数据分析中很常用，但最大似然估计计算困难，而伪似然方法虽然计算方便但会严重低估标准误。

Method: 研究二次指数二元分布及其回归对应模型的有效估计方法，应用广义估计方程到伪似然，比较不同工作相关结构的效果。

Result: 使用独立工作相关结构能得到一致估计，而复合对称或自回归等依赖结构可能引入不可忽略的偏差。

Conclusion: 建议在应用广义估计方程到伪似然时使用独立工作相关结构，理论性质得到模拟研究支持，并在实际数据中得到验证。

Abstract: For a set of binary response variables, conditional mean models characterize
the expected value of a response variable given the others and are popularly
applied in longitudinal and network data analyses. The quadratic exponential
binary distribution is a natural choice in this context. However, maximum
likelihood estimation of this distribution is computationally demanding due to
its intractable normalizing constant, while the pseudo-likelihood, though
computationally convenient, tends to severely underestimate the standard
errors. In this work, we investigate valid estimation methods for the quadratic
exponential binary distribution and its regression counterpart. We show that,
when applying the generalized estimating equations to the pseudo-likelihood,
using the independence working correlation yields consistent estimates, whereas
using dependent structures, such as compound symmetric or autoregressive
correlations, may introduce non-ignorable biases. Theoretical properties are
derived, supported by simulation studies. For illustration, we apply the
proposed approach to the carcinogenic toxicity of chemicals data and the
constitutional court opinion wringing data.

</details>


### [9] [Empirical partially Bayes two sample testing](https://arxiv.org/abs/2510.00432)
*Wanyi Ling,Wufang Hong,Nikolaos Ignatiadis*

Main category: stat.ME

TL;DR: 本文提出了一种新的经验部分贝叶斯方法，用于处理具有不等方差的两样本均值检验问题，在保持I类错误控制的同时显著提高了统计功效。


<details>
  <summary>Details</summary>
Motivation: 传统调节t检验假设等方差，但在实际生物学数据中方差往往不等；而Welch不等方差t检验在小样本情况下存在I类错误膨胀和功效低的问题。

Method: 扩展经验部分贝叶斯范式到不等方差情况，开发了两种方法：一种建模两个样本方差的比率，另一种联合建模两个方差，使用非参数最大似然估计先验分布。

Result: 新方法产生的p值随着特征数量增加而渐近均匀分布，确保渐近I类错误控制。模拟和基因组数据应用显示统计功效显著提升。

Conclusion: 提出的经验部分贝叶斯方法有效解决了不等方差两样本检验问题，在保持错误控制的同时大幅提高了检测能力。

Abstract: A common task in high-throughput biology is to test for differences in means
between two samples across thousands of features (e.g., genes or proteins),
often with only a handful of replicates per sample. Moderated t-tests handle
this problem by assuming normality and equal variances, and by applying the
empirical partially Bayes principle: a prior is posited and estimated for the
nuisance parameters (variances) but not for the primary parameters (means).
This approach has been highly successful in genomics, yet the equal variance
assumption is often violated in practice. Meanwhile, Welch's unequal variance
t-test with few replicates suffers from inflated type-I error and low power.
Taking inspiration from moderated t-tests, we extend the empirical partially
Bayes paradigm to two-sample testing with unequal variances. We develop two
procedures: one that models the ratio of the two sample-specific variances and
another that models the two variances jointly, with prior distributions
estimated by nonparametric maximum likelihood. Our empirical partially Bayes
methods yield p-values that are asymptotically uniform as the number of
features grows while the number of replicates remains fixed, ensuring
asymptotic type-I error control. Simulations and applications to genomic data
demonstrate substantial gains in power.

</details>


### [10] [Rapid Scaling of Compositional Uncertainty from Sample to Population Levels](https://arxiv.org/abs/2510.00980)
*Yiran Wang,Martin Lysy,Audrey Béliveau*

Main category: stat.ME

TL;DR: 提出了一种反向Dirichlet-多项式模型和方差估计方法，用于将样本水平的不确定性传播到种群水平，解决了历史数据集中个体分配方法无法进行有效种群推断的问题。


<details>
  <summary>Details</summary>
Motivation: 理解种群组成在生态、进化、保护和资源管理等领域至关重要。历史数据集通常依赖仅考虑样本水平不确定性的个体分配方法，限制了种群水平推断的有效性。

Method: 提出反向Dirichlet-多项式模型，推导多个方差估计器来传播不确定性；将该框架扩展到遗传标记重捕研究；通过模拟评估性能；应用于塔库河红鲑的逃逸量估计。

Result: 开发的方法能够有效将样本水平的不确定性传播到种群水平，在遗传标记重捕研究中得到验证，并成功应用于实际的红鲑种群估计。

Conclusion: 该方法解决了历史数据集中种群推断的局限性，为生态和资源管理提供了更可靠的不确定性传播框架。

Abstract: Understanding population composition is essential across ecological,
evolutionary, conservation, and resource management contexts. Modern methods
such as genetic stock identification (GSI) estimate the proportion of
individuals from each subpopulation using genetic data. Ideally, these
estimates are obtained through mixture analysis, which captures both sampling
and genetic uncertainty. However, historical datasets often rely on individual
assignment methods that only account for sample-level uncertainty, limiting the
validity of population-level inferences. To address this, we propose a reverse
Dirichlet-multinomial model and derive multiple variance estimators to
propagate uncertainty from the sample to the population level. We extend this
framework to genetic mark-recapture studies, assess performance via simulation,
and apply our method to estimate the escapement of Sockeye Salmon (Oncorhynchus
nerka) in the Taku River.

</details>


### [11] [A Data-Adaptive Factor Model Using Composite Quantile Approach](https://arxiv.org/abs/2510.00558)
*Seeun Park,Hee-Seok Oh*

Main category: stat.ME

TL;DR: 提出数据自适应因子模型（DAFM），通过复合分位数策略自适应捕捉高维数据的完整分布结构，提高估计精度并揭示传统因子模型无法发现的潜在模式。


<details>
  <summary>Details</summary>
Motivation: 传统因子模型无法充分捕捉数据的完整分布结构，限制了在高维数据分析中的准确性和模式发现能力。

Method: 开发基于分位数检查函数加权平均的目标函数最小化算法，引入核平滑目标函数的近似估计器，并提出两种确定因子数量的方法。

Result: 模拟研究显示DAFM在不同数据分布下优于现有因子模型，波动率和预测的实际数据分析进一步验证了其有效性。

Conclusion: DAFM框架通过自适应捕捉数据分布结构，显著提升了因子模型的估计精度和模式发现能力，具有理论和实际应用价值。

Abstract: This paper proposes a data-adaptive factor model (DAFM), a novel framework
for extracting common factors that explain the structures of high-dimensional
data. DAFM adopts a composite quantile strategy to adaptively capture the full
distributional structure of the data, thereby enhancing estimation accuracy and
revealing latent patterns that are invisible to conventional factor models. In
this paper, we develop a practical algorithm for estimating DAFM by minimizing
an objective function based on a weighted average of check functions across
quantiles. We also establish the theoretical properties of the estimators,
including their consistency and convergence rates. Furthermore, we derive their
asymptotic distributions by introducing approximated estimators from a
kernel-smoothed objective function, and propose two consistent methods for
determining the number of factors. Simulation studies demonstrate that DAFM
outperforms existing factor models across different data distributions, and
real data analyses on volatility and forecasting further validate its
effectiveness.

</details>


### [12] [A Weighted Regression Approach to Break-Point Detection in Panel Data](https://arxiv.org/abs/2510.00598)
*Charl Pretorius,Heinrich Roodt*

Main category: stat.ME

TL;DR: 提出了检测面板数据截面均值变化的新程序，通过加权最小二乘回归估计参数，在弱截面依赖下构建不依赖带宽选择的检验统计量，并扩展到强截面依赖情况。


<details>
  <summary>Details</summary>
Motivation: 现有面板数据变化检测方法通常需要估计长期方差，这依赖于带宽选择，可能导致检验性能不稳定。本文旨在开发不依赖带宽选择的稳健检验方法。

Method: 使用加权最小二乘回归估计参数，构建检验统计量。在弱截面依赖下，统计量的极限分布不依赖带宽选择；在强截面依赖下扩展理论结果。

Result: 理论分析表明所提方法能获得一致的检验程序，数值研究验证了有限样本下几种特殊检验程序的表现。

Conclusion: 提出的新程序为面板数据截面均值变化检测提供了不依赖带宽选择的稳健方法，适用于弱和强截面依赖情况。

Abstract: New procedures for detecting a change in the cross-sectional mean of panel
data are proposed. The procedures rely on estimating nuisance parameters using
certain cross-sectional means across panels using a weighted least squares
regression. In the case of weak cross-sectional dependence between panels, we
show how test statistics can be constructed to have a limit null distribution
not depending on any choice of bandwidths typically needed to estimate the
long-run variances of the panel errors. The theoretical assertions are derived
for general choices of the regression weights, and it is shown that consistent
test procedures can be obtained from the proposed process. The theoretical
results are extended to the case where strong cross-sectional dependence exist
between panels. The paper concludes with a numerical study illustrating the
behavior of several special cases of the test procedure in finite samples.

</details>


### [13] [False Discovery Rate Control via Bayesian Mirror Statistic](https://arxiv.org/abs/2510.00875)
*Marco Molinari,Magne Thoresen*

Main category: stat.ME

TL;DR: 将Mirror Statistic方法从频率统计框架扩展到贝叶斯建模框架，用于高维模型中的变量选择和FDR控制，无需数据分割。


<details>
  <summary>Details</summary>
Motivation: 高维模型中同时进行变量选择和推断是一个开放挑战，需要既能准确选择重要预测变量又能控制选择错误的统计方法。

Method: 基于贝叶斯模型，利用感兴趣系数的后验分布构建Mirror Statistic来控制FDR，使用自动微分变分推理保持高维可扩展性。

Result: 提出的方法能够有效控制FDR，无需数据分割，适用于连续和离散结果以及更复杂的预测变量（如混合模型）。

Conclusion: 贝叶斯框架下的Mirror Statistic方法为高维变量选择提供了一种灵活且可扩展的FDR控制解决方案。

Abstract: Simultaneously performing variable selection and inference in
high-dimensional models is an open challenge in statistics and machine
learning. The increasing availability of vast amounts of variables requires the
adoption of specific statistical procedures to accurately select the most
important predictors in a high-dimensional space, while being able to control
some form of selection error. In this work we adapt the Mirror Statistic
approach to False Discovery Rate (FDR) control into a Bayesian modelling
framework. The Mirror Statistic, developed in the classic frequentist
statistical framework, is a flexible method to control FDR, which only requires
mild model assumptions, but requires two sets of independent regression
coefficient estimates, usually obtained after splitting the original dataset.
Here we propose to rely on a Bayesian formulation of the model and use the
posterior distributions of the coefficients of interest to build the Mirror
Statistic and effectively control the FDR without the need to split the data.
Moreover, the method is very flexible since it can be used with continuous and
discrete outcomes and more complex predictors, such as with mixed models. We
keep the approach scalable to high-dimensions by relying on Automatic
Differentiation Variational Inference and fully continuous prior choices.

</details>


### [14] [Spatial Gaussian fields for complex areas with application to marine megafauna conservation](https://arxiv.org/abs/2510.00611)
*Martina Le-Bert Heyl,Janet van Niekerk,Haavard Rue*

Main category: stat.ME

TL;DR: 提出了透明屏障模型，扩展空间高斯场以处理具有不同渗透性的屏障，解决了传统模型对屏障处理过于简化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统空间高斯场模型假设各向同性和平稳性，无法准确处理复杂生态环境中海岸线、岛屿和深度梯度等部分运动屏障的现实情况。

Method: 通过为不同屏障区域分配空间变化的范围参数，结合INLA框架和SPDE方法，构建计算高效的透明屏障模型。

Result: 在红海儒艮分布数据上的应用证明了该模型的实用性和灵活性。

Conclusion: 透明屏障模型能够更好地反映真实世界中的屏障渗透性，为海洋大型动物分布建模提供了更准确的方法。

Abstract: Spatial Gaussian fields (SGFs) are widely employed in modeling the
distributions of marine megafauna, yet they traditionally rely on assumptions
of isotropy and stationarity, conditions that often prove unrealistic in
complex ecological environments featuring coastlines, islands, and depth
gradients acting as partial movement barriers. Existing spatial models
typically treat these barriers as either fully impermeable, completely blocking
species movement and dispersal, or entirely absent, which inadequately
represents most real-world scenarios. To address this limitation, we introduce
the Transparent Barrier Model, an extension of spatial Gaussian fields that
explicitly incorporates barriers with varying levels of permeability. The model
assigns spatially varying range parameters to distinct barrier regions,
allowing ecological and geographical knowledge about barrier permeability to
directly inform model specifications. This approach maintains computational
efficiency by utilizing the integrated nested Laplace approximation (INLA)
framework combined with stochastic partial differential equations (SPDEs),
ensuring feasible application even in large, complex spatial domains.We
demonstrate the practical utility and flexibility of the Transparent Barrier
Model through its application to dugong (Dugong dugon) distribution data from
the Red Sea.

</details>


### [15] [How can the use of different modes of survey data collection introduce bias? A simple introduction to mode effects using directed acyclic graphs (DAGs)](https://arxiv.org/abs/2510.00900)
*Georgia D Tomova,Richard J Silverwood,Peter WG Tennant,Liam Wright*

Main category: stat.ME

TL;DR: 本文探讨了混合模式调查设计中的两个关键挑战：模式效应和模式选择，指出传统处理方法可能引入碰撞偏倚，并建议使用定量偏倚分析来应对模式效应。


<details>
  <summary>Details</summary>
Motivation: 随着混合模式调查设计在流行病学中越来越普遍，需要解决模式效应和模式选择带来的挑战，特别是传统条件化方法可能导致的碰撞偏倚问题。

Method: 使用有向无环图探索各种可能的数据结构，分析条件化和插补方法的潜在影响，并概述定量偏倚分析的优势。

Result: 分析表明，传统条件化方法在存在模式选择的情况下可能引入碰撞偏倚，而定量偏倚分析是处理模式效应的更优方法。

Conclusion: 流行病学中可能低估了模式效应的存在和朴素条件化的后果，建议采用定量偏倚分析来更好地处理混合模式调查中的挑战。

Abstract: Survey data are self-reported data collected directly from respondents by a
questionnaire or an interview and are commonly used in epidemiology. Such data
are traditionally collected via a single mode (e.g. face-to-face interview
alone), but use of mixed-mode designs (e.g. offering face-to-face interview or
online survey) has become more common. This introduces two key challenges.
First, individuals may respond differently to the same question depending on
the mode; these differences due to measurement are known as 'mode effects'.
Second, different individuals may participate via different modes; these
differences in sample composition between modes are known as 'mode selection'.
Where recognised, mode effects are often handled by straightforward approaches
such as conditioning on survey mode. However, while reducing mode effects, this
and other equivalent approaches may introduce collider bias in the presence of
mode selection. The existence of mode effects and the consequences of na\"ive
conditioning may be underappreciated in epidemiology. This paper offers a
simple introduction to these challenges using directed acyclic graphs by
exploring a range of possible data structures. We discuss the potential
implications of using conditioning- or imputation-based approaches and outline
the advantages of quantitative bias analyses for dealing with mode effects.

</details>


### [16] [An alternative bootstrap procedure for factor-augmented regression models](https://arxiv.org/abs/2510.00947)
*Peiyun Jiang,Takashi Yamagata*

Main category: stat.ME

TL;DR: 提出了一种新的bootstrap算法，用于近似因子增强回归估计量的分布，该算法比现有方法更高效，特别适用于弱因子模型。


<details>
  <summary>Details</summary>
Motivation: 现有的bootstrap方法在近似因子增强回归估计量分布时效率不高，特别是在弱因子模型下，需要更高效且有效的解决方案。

Method: 使用数据依赖的旋转矩阵（包括常规的$\hat{\bH}$和替代的$\hat{\bH}_q$）以及纯信号依赖的旋转矩阵${\bH}$，建立bootstrap方法的渐近有效性。

Result: 实验结果表明，提出的bootstrap程序相对于现有程序具有更优越的性能。

Conclusion: 该bootstrap方法在弱因子模型下是渐近有效的，并且在实践中表现出比现有方法更好的性能。

Abstract: In this paper, we propose a novel bootstrap algorithm that is more efficient
than existing methods for approximating the distribution of the
factor-augmented regression estimator for a rotated parameter vector. The
regression is augmented by $r$ factors extracted from a large panel of $N$
variables observed over $T$ time periods. We consider general weak factor (WF)
models with $r$ signal eigenvalues that may diverge at different rates,
$N^{\alpha _{k}}$, where $0<\alpha _{k}\leq 1$ for $k=1,2,...,r$. We establish
the asymptotic validity of our bootstrap method using not only the conventional
data-dependent rotation matrix $\hat{\bH}$, but also an alternative
data-dependent rotation matrix, $\hat{\bH}_q$, which typically exhibits smaller
asymptotic bias and achieves a faster convergence rate. Furthermore, we
demonstrate the asymptotic validity of the bootstrap under a purely
signal-dependent rotation matrix ${\bH}$, which is unique and can be regarded
as the population analogue of both $\hat{\bH}$ and $\hat{\bH}_q$. Experimental
results provide compelling evidence that the proposed bootstrap procedure
achieves superior performance relative to the existing procedure.

</details>


### [17] [Local aggregate multiscale processes: A scalable, machine-learning-compatible spatial model](https://arxiv.org/abs/2510.00968)
*Daisuke Murakami,Alexis Comber,Takahiro Yoshida,Narumasa Tsutsumida,Chris Brunsdon,Tomoki Nakaya*

Main category: stat.ME

TL;DR: 开发了LAMP（局部聚合多尺度过程），作为传统空间高斯过程的可扩展机器学习兼容替代方案，通过多尺度局部模型集合表示空间过程，避免显式矩阵求逆，计算效率高。


<details>
  <summary>Details</summary>
Motivation: 传统基于协方差的空间模型（如克里金法）存在计算瓶颈和与机器学习算法集成困难的问题，需要开发更高效、可扩展且与机器学习兼容的空间建模方法。

Method: 采用多尺度局部模型集合，受地理加权回归启发，从大尺度到小尺度顺序建模，基于留出验证的训练过程可与其他机器学习算法（如随机森林、神经网络）轻松集成。

Result: 蒙特卡洛比较实验显示LAMP及其与随机森林的集成相比现有模型具有更优的预测性能，在东京都市区住宅地价分析中成功应用。

Conclusion: LAMP为空间建模提供了计算高效、可扩展且与机器学习兼容的替代方案，在预测性能和实际应用中都表现出色。

Abstract: This study develops the Local Aggregate Multiscale Process (LAMP), a scalable
and machine-learning-compatible alternative to conventional spatial Gaussian
processes (GPs, or kriging). Unlike conventional covariance-based spatial
models, LAMP represents spatial processes by a multiscale ensemble of local
models, inspired by geographically weighted regression. To ensure stable model
training, larger-scale patterns that are easier to learn are modeled first,
followed by smaller-scale patterns, with training terminated once the
validation score stops improving. The training procedure, which is based on
holdout validation, is easily integrated with other machine learning algorithms
(e.g., random forests and neural networks). LAMP training is computationally
efficient as it avoids explicit matrix inversion, a major computational
bottleneck in conventional GPs. Comparative Monte Carlo experiments demonstrate
that LAMP, as well as its integration with random forests, achieves superior
predictive performance compared to existing models. Finally, we apply the
proposed methods to an analysis of residential land prices in the Tokyo
metropolitan area, Japan.
  The R code is available from available from
https://github.com/dmuraka/spLAMP_dev_version/tree/main

</details>


### [18] [Evaluating Informative Cluster Size in Cluster Randomized Trials](https://arxiv.org/abs/2510.01127)
*Bryan S. Blette,Zhe Chen,Brennan C. Kahan,Andrew Forbes,Michael O. Harhay,Fan Li*

Main category: stat.ME

TL;DR: 本文开发了三种检验方法来检测聚类随机试验中的信息性聚类规模现象，包括基于模型、模型辅助和基于随机化的检验方法。


<details>
  <summary>Details</summary>
Motivation: 在聚类随机试验中，当存在信息性聚类规模时，个体平均处理效应与聚类平均处理效应可能不同，而现有方法在这种情况下会产生偏差。需要开发专门的检验方法来识别这一关键现象。

Method: 开发了三种检验方法：基于模型的检验、模型辅助检验和基于随机化的检验，并通过模拟研究评估这些方法的性能特征。

Result: 模拟研究表明这些检验方法具有适当的I类错误控制和有意义的检验功效，与观察性研究中使用的现有基于模型检验方法形成对比。

Conclusion: 提出的检验方法可用于评估聚类随机试验中的信息性聚类规模现象，并为实际应用提供了实用建议。

Abstract: In cluster randomized trials, the average treatment effect among individuals
(i-ATE) can be different from the cluster average treatment effect (c-ATE) when
informative cluster size is present, i.e., when treatment effects or
participant outcomes depend on cluster size. In such scenarios, mixed-effects
models and generalized estimating equations (GEEs) with exchangeable
correlation structure are biased for both the i-ATE and c-ATE estimands,
whereas GEEs with an independence correlation structure or analyses of
cluster-level summaries are recommended in practice. However, when cluster size
is non-informative, mixed-effects models and GEEs with exchangeable correlation
structure can provide unbiased estimation and notable efficiency gains over
other methods. Thus, hypothesis tests for informative cluster size would be
useful to assess this key phenomenon under cluster randomization. In this work,
we develop model-based, model-assisted, and randomization-based tests for
informative cluster size in cluster randomized trials. We construct simulation
studies to examine the operating characteristics of these tests, show they have
appropriate Type I error control and meaningful power, and contrast them to
existing model-based tests used in the observational study setting. The
proposed tests are then applied to data from a recent cluster randomized trial,
and practical recommendations for using these tests are discussed.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [19] [Revealing the temporal dynamics of antibiotic anomalies in the infant gut microbiome with neural jump ODEs](https://arxiv.org/abs/2510.00087)
*Anja Adamov,Markus Chardonnet,Florian Krach,Jakob Heiss,Josef Teichmann,Nicholas A. Bokulich*

Main category: stat.AP

TL;DR: 提出了一个基于神经跳跃常微分方程(NJODEs)的异常检测框架，用于处理不规则采样的多变量时间序列，特别适用于数据稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 在不规则采样的多变量时间序列中检测异常具有挑战性，特别是在数据稀缺的情况下，需要开发能够处理这种复杂数据结构的有效方法。

Method: 使用神经跳跃常微分方程(NJODEs)推断条件均值和方差轨迹，以完全路径依赖的方式计算异常分数。

Result: 在包含跳跃、漂移、扩散和噪声异常的合成数据上准确识别各种偏差；在婴儿肠道微生物组轨迹中揭示了抗生素引起的破坏程度和持续性；异常分数在预测抗生素事件方面优于基于多样性的基线方法。

Conclusion: 该方法能够处理不均匀间隔的纵向观测，调整静态和动态协变量，为推断微生物异常提供了基础，为优化干预方案以最小化微生物破坏提供了转化机会。

Abstract: Detecting anomalies in irregularly sampled multi-variate time-series is
challenging, especially in data-scarce settings. Here we introduce an anomaly
detection framework for irregularly sampled time-series that leverages neural
jump ordinary differential equations (NJODEs). The method infers conditional
mean and variance trajectories in a fully path dependent way and computes
anomaly scores. On synthetic data containing jump, drift, diffusion, and noise
anomalies, the framework accurately identifies diverse deviations. Applied to
infant gut microbiome trajectories, it delineates the magnitude and persistence
of antibiotic-induced disruptions: revealing prolonged anomalies after second
antibiotic courses, extended duration treatments, and exposures during the
second year of life. We further demonstrate the predictive capabilities of the
inferred anomaly scores in accurately predicting antibiotic events and
outperforming diversity-based baselines. Our approach accommodates unevenly
spaced longitudinal observations, adjusts for static and dynamic covariates,
and provides a foundation for inferring microbial anomalies induced by
perturbations, offering a translational opportunity to optimize intervention
regimens by minimizing microbial disruptions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [20] [Identifying All ε-Best Arms in (Misspecified) Linear Bandits](https://arxiv.org/abs/2510.00073)
*Zhekai Li,Tianyi Ma,Cheng Hua,Ruihao Zhu*

Main category: stat.ML

TL;DR: 提出了LinFACT算法，在线性bandits中高效识别所有ε最优臂，实现了实例最优性，样本复杂度达到信息论下界。


<details>
  <summary>Details</summary>
Motivation: 在药物发现等高试错成本任务中，需要高效识别多个候选方案，即找出所有与最优解差距不超过ε的臂。

Method: 开发LinFACT算法，通过将下界直接整合到上界推导的缩放过程中来确定终止轮次和样本复杂度。

Result: LinFACT在样本复杂度上达到了信息论下界（最多相差对数因子），在合成和真实药物发现数据实验中表现出更高的候选识别效率和计算效率。

Conclusion: LinFACT能够显著减少样本复杂度，加速早期探索性实验，在药物发现等应用中具有重要价值。

Abstract: Motivated by the need to efficiently identify multiple candidates in high
trial-and-error cost tasks such as drug discovery, we propose a near-optimal
algorithm to identify all {\epsilon}-best arms (i.e., those at most {\epsilon}
worse than the optimum). Specifically, we introduce LinFACT, an algorithm
designed to optimize the identification of all {\epsilon}-best arms in linear
bandits. We establish a novel information-theoretic lower bound on the sample
complexity of this problem and demonstrate that LinFACT achieves instance
optimality by matching this lower bound up to a logarithmic factor. A key
ingredient of our proof is to integrate the lower bound directly into the
scaling process for upper bound derivation, determining the termination round
and thus the sample complexity. We also extend our analysis to settings with
model misspecification and generalized linear models. Numerical experiments,
including synthetic and real drug discovery data, demonstrate that LinFACT
identifies more promising candidates with reduced sample complexity, offering
significant computational efficiency and accelerating early-stage exploratory
experiments.

</details>


### [21] [Private Learning of Littlestone Classes, Revisited](https://arxiv.org/abs/2510.00076)
*Xin Lyu*

Main category: stat.ML

TL;DR: 本文提出了一种近似差分隐私约束下的在线学习和PAC学习Littlestone类的方法，在可实现情况下实现了$\tilde{O}(d^{9.5}\cdot \log(T))$的错误界限，相比现有技术有双重指数级改进。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私约束下学习Littlestone类时，现有方法的错误界限不够理想，需要开发更高效的私有学习算法。

Method: 使用私有稀疏选择算法从强输入依赖的候选池中采样，采用稀疏版本的指数机制，并改进了不可约性技术。

Result: 在线学习的错误界限为$\tilde{O}(d^{9.5}\cdot \log(T))$，PAC学习的样本复杂度为$\widetilde{O}(\\frac{d^5 \\log(1/\\delta\\beta)}{\\varepsilon \\alpha})$，相比现有技术有显著改进。

Conclusion: 该方法在差分隐私约束下显著提升了Littlestone类的学习效率，接近理论下界，为私有学习提供了新的技术路径。

Abstract: We consider online and PAC learning of Littlestone classes subject to the
constraint of approximate differential privacy. Our main result is a private
learner to online-learn a Littlestone class with a mistake bound of
$\tilde{O}(d^{9.5}\cdot \log(T))$ in the realizable case, where $d$ denotes the
Littlestone dimension and $T$ the time horizon. This is a doubly-exponential
improvement over the state-of-the-art [GL'21] and comes polynomially close to
the lower bound for this task.
  The advancement is made possible by a couple of ingredients. The first is a
clean and refined interpretation of the ``irreducibility'' technique from the
state-of-the-art private PAC-learner for Littlestone classes [GGKM'21]. Our new
perspective also allows us to improve the PAC-learner of [GGKM'21] and give a
sample complexity upper bound of $\widetilde{O}(\frac{d^5
\log(1/\delta\beta)}{\varepsilon \alpha})$ where $\alpha$ and $\beta$ denote
the accuracy and confidence of the PAC learner, respectively. This improves
over [GGKM'21] by factors of $\frac{d}{\alpha}$ and attains an optimal
dependence on $\alpha$.
  Our algorithm uses a private sparse selection algorithm to \emph{sample} from
a pool of strongly input-dependent candidates. However, unlike most previous
uses of sparse selection algorithms, where one only cares about the utility of
output, our algorithm requires understanding and manipulating the actual
distribution from which an output is drawn. In the proof, we use a sparse
version of the Exponential Mechanism from [GKM'21] which behaves nicely under
our framework and is amenable to a very easy utility proof.

</details>


### [22] [CINDES: Classification induced neural density estimator and simulator](https://arxiv.org/abs/2510.00367)
*Dehao Dai,Jianqing Fan,Yihong Gu,Debarghya Mukherjee*

Main category: stat.ML

TL;DR: 提出一种结构无关的神经密度估计器，该估计器易于实现且具有可证明的自适应性，在真实密度具有低维组合结构时能获得更快的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有的神经密度估计方法虽然在实际数据实验中表现出色，但实现复杂（需要确保非负性和单位质量约束），且理论理解有限，特别是在密度具有低维结构时是否能够自适应地获得更快收敛速率尚不明确。

Method: 提出一种结构无关的神经密度估计器，该估计器易于实现，并证明其具有自适应性，当真实密度具有低维组合结构时能够获得更快的收敛速率。

Result: 该估计器自然地集成到生成采样流程中，特别是在基于分数的扩散模型中，当底层密度具有结构时能够实现可证明的更快收敛。通过大量模拟和真实数据应用验证了其性能。

Conclusion: 所提出的神经密度估计器不仅解决了现有方法的实现挑战，还提供了理论保证，在密度具有低维结构时能够自适应地获得更快的收敛性能，并能有效集成到生成模型中。

Abstract: Neural network-based methods for (un)conditional density estimation have
recently gained substantial attention, as various neural density estimators
have outperformed classical approaches in real-data experiments. Despite these
empirical successes, implementation can be challenging due to the need to
ensure non-negativity and unit-mass constraints, and theoretical understanding
remains limited. In particular, it is unclear whether such estimators can
adaptively achieve faster convergence rates when the underlying density
exhibits a low-dimensional structure. This paper addresses these gaps by
proposing a structure-agnostic neural density estimator that is (i)
straightforward to implement and (ii) provably adaptive, attaining faster rates
when the true density admits a low-dimensional composition structure. Another
key contribution of our work is to show that the proposed estimator integrates
naturally into generative sampling pipelines, most notably score-based
diffusion models, where it achieves provably faster convergence when the
underlying density is structured. We validate its performance through extensive
simulations and a real-data application.

</details>


### [23] [On the Adversarial Robustness of Learning-based Conformal Novelty Detection](https://arxiv.org/abs/2510.00463)
*Daofu Zhang,Mehrdad Pournaderi,Hanne M. Clifford,Yu Xiang,Pramod K. Varshney*

Main category: stat.ML

TL;DR: 该论文研究了AdaDetect框架在对抗性扰动下的鲁棒性，发现即使有统计保证的异常检测方法在对抗攻击下也会失效，FDR显著增加。


<details>
  <summary>Details</summary>
Motivation: 探索AdaDetect这种具有有限样本FDR控制保证的异常检测方法在对抗性扰动下的行为，填补现有研究空白。

Method: 首先构建理论攻击框架量化FDR的最坏情况退化，然后开发仅需查询AdaDetect输出标签的实用攻击方案，结合两种黑盒对抗算法进行系统评估。

Result: 在合成和真实数据集上的实验表明，对抗性扰动能显著增加FDR同时保持高检测能力，暴露了当前错误控制异常检测方法的根本局限性。

Conclusion: 当前具有错误控制保证的异常检测方法存在严重安全漏洞，需要开发更鲁棒的替代方案。

Abstract: This paper studies the adversarial robustness of conformal novelty detection.
In particular, we focus on AdaDetect, a powerful learning-based framework for
novelty detection with finite-sample false discovery rate (FDR) control. While
AdaDetect provides rigorous statistical guarantees under benign conditions, its
behavior under adversarial perturbations remains unexplored. We first formulate
an oracle attack setting that quantifies the worst-case degradation of FDR,
deriving an upper bound that characterizes the statistical cost of attacks.
This idealized formulation directly motivates a practical and effective attack
scheme that only requires query access to AdaDetect's output labels. Coupling
these formulations with two popular and complementary black-box adversarial
algorithms, we systematically evaluate the vulnerability of AdaDetect on
synthetic and real-world datasets. Our results show that adversarial
perturbations can significantly increase the FDR while maintaining high
detection power, exposing fundamental limitations of current error-controlled
novelty detection methods and motivating the development of more robust
alternatives.

</details>


### [24] [A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws](https://arxiv.org/abs/2510.00504)
*Hong-Yi Wang,Di Luo,Tomaso Poggio,Isaac L. Chuang,Liu Ziyin*

Main category: stat.ML

TL;DR: 该论文证明了大模型和数据集可以被压缩到对数多项式大小而不损失性能，提出了动态彩票假设的证明，并展示了如何将神经缩放定律加速到任意快的幂律衰减。


<details>
  <summary>Details</summary>
Motivation: 研究是否能用显著更小的模型和更少的数据达到与大规模模型相当的性能，这是训练大模型时面临的基本理论和实践问题。

Method: 通过数学证明，表明对d个对象的通用置换不变函数可以渐近压缩到polylog d个对象的函数，且误差趋近于零。

Result: 证明了两个关键含义：(Ia)大神经网络可以压缩到对数多项式宽度而保持学习动态；(Ib)大数据集可以压缩到对数多项式大小而不改变损失景观。

Conclusion: 该工作为模型和数据压缩提供了理论依据，证明了动态彩票假设，并展示了如何显著提升神经缩放定律的性能衰减速率。

Abstract: When training large-scale models, the performance typically scales with the
number of parameters and the dataset size according to a slow power law. A
fundamental theoretical and practical question is whether comparable
performance can be achieved with significantly smaller models and substantially
less data. In this work, we provide a positive and constructive answer. We
prove that a generic permutation-invariant function of $d$ objects can be
asymptotically compressed into a function of $\operatorname{polylog} d$ objects
with vanishing error. This theorem yields two key implications: (Ia) a large
neural network can be compressed to polylogarithmic width while preserving its
learning dynamics; (Ib) a large dataset can be compressed to polylogarithmic
size while leaving the loss landscape of the corresponding model unchanged.
(Ia) directly establishes a proof of the \textit{dynamical} lottery ticket
hypothesis, which states that any ordinary network can be strongly compressed
such that the learning dynamics and result remain unchanged. (Ib) shows that a
neural scaling law of the form $L\sim d^{-\alpha}$ can be boosted to an
arbitrarily fast power law decay, and ultimately to $\exp(-\alpha'
\sqrt[m]{d})$.

</details>


### [25] [Approximation of differential entropy in Bayesian optimal experimental design](https://arxiv.org/abs/2510.00734)
*Chuntao Chen,Tapio Helin,Nuutti Hyvönen,Yuya Suzuki*

Main category: stat.ML

TL;DR: 提出了一种计算期望信息增益的新方法，通过最大熵估计简化了贝叶斯最优实验设计中的计算挑战，特别适用于计算成本高昂的大规模推理问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模推理问题（如逆问题）中，由于似然评估计算成本高昂而导致的贝叶斯最优实验设计计算困难。

Method: 使用蒙特卡洛或准蒙特卡洛代理近似证据密度，同时使用标准方法评估微分熵，无需额外的似然评估。

Result: 该方法实现了与最先进方法相当或更好的收敛速度，特别是在熵评估成本可忽略的情况下，且仅需较弱的平滑性假设。

Conclusion: 提出的计算策略在理论和数值实验中都表现出优越性能，为大规模贝叶斯最优实验设计提供了高效解决方案。

Abstract: Bayesian optimal experimental design provides a principled framework for
selecting experimental settings that maximize obtained information. In this
work, we focus on estimating the expected information gain in the setting where
the differential entropy of the likelihood is either independent of the design
or can be evaluated explicitly. This reduces the problem to maximum entropy
estimation, alleviating several challenges inherent in expected information
gain computation.
  Our study is motivated by large-scale inference problems, such as inverse
problems, where the computational cost is dominated by expensive likelihood
evaluations. We propose a computational approach in which the evidence density
is approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the
differential entropy is evaluated using standard methods without additional
likelihood evaluations. We prove that this strategy achieves convergence rates
that are comparable to, or better than, state-of-the-art methods for full
expected information gain estimation, particularly when the cost of entropy
evaluation is negligible. Moreover, our approach relies only on mild smoothness
of the forward map and avoids stronger technical assumptions required in
earlier work. We also present numerical experiments, which confirm our
theoretical findings.

</details>


### [26] [Bayesian Neural Networks for Functional ANOVA model](https://arxiv.org/abs/2510.00545)
*Seokhun Park,Choeun Kim,Jihu Lee,Yunseop Shin,Insung Kong,Yongdai Kim*

Main category: stat.ML

TL;DR: 提出了Bayesian-TPNN，一种用于功能ANOVA模型的贝叶斯推理方法，使用TPNN基函数，能够以比ANOVA-TPNN更低的计算成本检测高阶组件。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习可解释性需求的增加，功能ANOVA分解作为分解高维函数的重要工具受到关注。但现有的ANOVA-TPNN方法需要预先指定要估计的组件，由于计算和内存限制难以纳入高阶TPNN。

Method: 开发了Bayesian-TPNN，为功能ANOVA模型提供贝叶斯推理程序，使用TPNN基函数。设计了一种高效的MCMC算法，并理论上证明了其后验一致性。

Result: 通过分析多个基准数据集，证明Bayesian-TPNN表现良好，能够以比ANOVA-TPNN更低的计算成本检测高阶组件。

Conclusion: Bayesian-TPNN是功能ANOVA模型的有效贝叶斯推理方法，能够克服ANOVA-TPNN的计算限制，检测高阶组件，并具有理论上的后验一致性保证。

Abstract: With the increasing demand for interpretability in machine learning,
functional ANOVA decomposition has gained renewed attention as a principled
tool for breaking down high-dimensional function into low-dimensional
components that reveal the contributions of different variable groups.
Recently, Tensor Product Neural Network (TPNN) has been developed and applied
as basis functions in the functional ANOVA model, referred to as ANOVA-TPNN. A
disadvantage of ANOVA-TPNN, however, is that the components to be estimated
must be specified in advance, which makes it difficult to incorporate
higher-order TPNNs into the functional ANOVA model due to computational and
memory constraints. In this work, we propose Bayesian-TPNN, a Bayesian
inference procedure for the functional ANOVA model with TPNN basis functions,
enabling the detection of higher-order components with reduced computational
cost compared to ANOVA-TPNN. We develop an efficient MCMC algorithm and
demonstrate that Bayesian-TPNN performs well by analyzing multiple benchmark
datasets. Theoretically, we prove that the posterior of Bayesian-TPNN is
consistent.

</details>


### [27] [Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold](https://arxiv.org/abs/2510.00569)
*Ke Xu,Yuefeng Han*

Main category: stat.ML

TL;DR: 该论文提出在Segre流形上使用黎曼优化方法（RGD和RGN）来恢复低CP秩张量，证明了收敛性并在实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 从噪声线性测量中恢复低CP秩张量是高维数据分析中的核心挑战，应用于张量PCA、张量回归等领域。

Method: 利用秩一张量的内在几何特性，将恢复问题转化为Segre流形上的优化问题，提出黎曼梯度下降(RGD)和黎曼高斯-牛顿(RGN)两种算法。

Result: 在温和噪声假设下，RGD具有局部线性收敛率，RGN具有初始局部二次收敛阶段，随后转为线性收敛。合成实验验证了收敛保证。

Conclusion: 基于Segre流形的几何视角为低CP秩张量恢复提供了有效的黎曼优化方法，具有理论保证和实际效果。

Abstract: Recovering a low-CP-rank tensor from noisy linear measurements is a central
challenge in high-dimensional data analysis, with applications spanning tensor
PCA, tensor regression, and beyond. We exploit the intrinsic geometry of
rank-one tensors by casting the recovery task as an optimization problem over
the Segre manifold, the smooth Riemannian manifold of rank-one tensors. This
geometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent
(RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at
every iteration. Under mild noise assumptions, we prove that RGD converges at a
local linear rate, while RGN exhibits an initial local quadratic convergence
phase that transitions to a linear rate as the iterates approach the
statistical noise floor. Extensive synthetic experiments validate these
convergence guarantees and demonstrate the practical effectiveness of our
methods.

</details>


### [28] [Optimal placement of wind farms via quantile constraint learning](https://arxiv.org/abs/2510.01093)
*Wenxiu Feng,Antonio Alcántara,Carlos Ruiz*

Main category: stat.ML

TL;DR: 提出了一种基于概率神经网络的风电场选址优化方法，通过约束学习将神经网络转化为混合整数线性约束，嵌入两阶段随机优化问题中，为风险厌恶投资者提供决策指导。


<details>
  <summary>Details</summary>
Motivation: 风电场选址需要考虑风速的时空相关性，传统方法如双线性插值效果有限，需要更先进的数据驱动方法来准确建模风速与发电量的关系。

Method: 使用概率神经网络作为风速时空相关性的代理模型，通过ReLU激活函数将神经网络转化为混合整数线性约束，嵌入两阶段随机优化问题中，将总发电量的条件分位数作为第二阶段递归决策。

Result: 约束学习方法优于传统双线性插值法。风险厌恶投资者倾向于集中在主导风场位置，同时在非主导位置表现出空间多样性和敏感容量分布；引入输电线路成本后，风险厌恶投资者偏好靠近变电站的位置。

Conclusion: 所提出的新方法能够处理区域风电场组合选址问题，并为风险厌恶投资者提供决策指导，风险中性投资者愿意选择更远位置以获得更高预期利润。

Abstract: Wind farm placement arranges the size and the location of multiple wind farms
within a given region. The power output is highly related to the wind speed on
spatial and temporal levels, which can be modeled by advanced data-driven
approaches. To this end, we use a probabilistic neural network as a surrogate
that accounts for the spatiotemporal correlations of wind speed. This neural
network uses ReLU activation functions so that it can be reformulated as
mixed-integer linear set of constraints (constraint learning). We embed these
constraints into the placement decision problem, formulated as a two-stage
stochastic optimization problem. Specifically, conditional quantiles of the
total electricity production are regarded as recursive decisions in the second
stage. We use real high-resolution regional data from a northern region in
Spain. We validate that the constraint learning approach outperforms the
classical bilinear interpolation method. Numerical experiments are implemented
on risk-averse investors. The results indicate that risk-averse investors
concentrate on dominant sites with strong wind, while exhibiting spatial
diversification and sensitive capacity spread in non-dominant sites.
Furthermore, we show that if we introduce transmission line costs in the
problem, risk-averse investors favor locations closer to the substations. On
the contrary, risk-neutral investors are willing to move to further locations
to achieve higher expected profits. Our results conclude that the proposed
novel approach is able to tackle a portfolio of regional wind farm placements
and further provide guidance for risk-averse investors.

</details>


### [29] [Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time](https://arxiv.org/abs/2510.01098)
*Blake Bordelon,Mary I. Letey,Cengiz Pehlevan*

Main category: stat.ML

TL;DR: 该论文研究了深度线性自注意力模型中的上下文学习，分析了不同计算和统计资源对线性回归性能的影响，并提出了一个可解的玩具模型来预测transformer的最优形状。


<details>
  <summary>Details</summary>
Motivation: 研究深度线性自注意力模型中上下文学习的性能如何依赖于各种计算和统计资源（宽度、深度、训练步数、批次大小和上下文数据量），并理解transformer架构中宽度和深度对上下文学习的影响。

Method: 在数据维度、上下文长度和残差流宽度成比例缩放的联合极限下，分析了三种上下文学习设置的渐近行为：各向同性协方差和任务(ISO)、固定结构化协方差(FS)、以及随机旋转结构化协方差(RRS)。

Result: 在ISO和FS设置中，深度仅在上下文长度有限时有助于上下文学习性能；而在RRS设置中，增加深度即使在无限上下文长度下也能显著改善上下文学习性能。

Conclusion: 提出了一个新的可解玩具模型，该模型依赖于transformer的宽度和深度，能够预测计算资源下的最优transformer形状，并能够计算风险的确切渐近性以及在源/容量条件下推导上下文学习任务的幂律。

Abstract: We study in-context learning (ICL) of linear regression in a deep linear
self-attention model, characterizing how performance depends on various
computational and statistical resources (width, depth, number of training
steps, batch size and data per context). In a joint limit where data dimension,
context length, and residual stream width scale proportionally, we analyze the
limiting asymptotics for three ICL settings: (1) isotropic covariates and tasks
(ISO), (2) fixed and structured covariance (FS), and (3) where covariances are
randomly rotated and structured (RRS). For ISO and FS settings, we find that
depth only aids ICL performance if context length is limited. Alternatively, in
the RRS setting where covariances change across contexts, increasing the depth
leads to significant improvements in ICL, even at infinite context length. This
provides a new solvable toy model of neural scaling laws which depends on both
width and depth of a transformer and predicts an optimal transformer shape as a
function of compute. This toy model enables computation of exact asymptotics
for the risk as well as derivation of powerlaws under source/capacity
conditions for the ICL tasks.

</details>
