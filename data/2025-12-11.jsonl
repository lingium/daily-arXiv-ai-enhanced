{"id": "2512.08137", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08137", "abs": "https://arxiv.org/abs/2512.08137", "authors": ["Quan Vu", "Xuanjie Shao", "Raphaël Huser", "Andrew Zammit-Mangion"], "title": "deepspat: An R package for modeling nonstationary spatial and spatio-temporal Gaussian and extremes data through deep deformations", "comment": null, "summary": "Nonstationarity in spatial and spatio-temporal processes is ubiquitous in environmental datasets, but is not often addressed in practice, due to a scarcity of statistical software packages that implement nonstationary models. In this article, we introduce the R software package deepspat, which allows for modeling, fitting and prediction with nonstationary spatial and spatio-temporal models applied to Gaussian and extremes data. The nonstationary models in our package are constructed using a deep multi-layered deformation of the original spatial or spatio-temporal domain, and are straightforward to implement. Model parameters are estimated using gradient-based optimization of customized loss functions with tensorflow, which implements automatic differentiation. The functionalities of the package are illustrated through simulation studies and an application to Nepal temperature data."}
{"id": "2512.08689", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.08689", "abs": "https://arxiv.org/abs/2512.08689", "authors": ["Connor Panish", "Leo Villani"], "title": "Matrix Completion Survey: Theory, Algorithms, and Empirical Evaluation", "comment": null, "summary": "We present a concise survey of matrix completion methods and associated implementations of several fundamental algorithms. Our study covers both passive and adaptive strategies. We further illustrate the behavior of a simple adaptive sampling scheme through controlled synthetic experiments."}
{"id": "2512.07888", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.07888", "abs": "https://arxiv.org/abs/2512.07888", "authors": ["Fahad Mostafa", "Hafiz Khan"], "title": "Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification", "comment": "23 pages, 4 figures", "summary": "Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical."}
{"id": "2512.08146", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.08146", "abs": "https://arxiv.org/abs/2512.08146", "authors": ["Fangzheng Xie", "Hsin-Hsiung Huang"], "title": "Uncertainty quantification for mixed membership in multilayer networks with degree heterogeneity using Gaussian variational inference", "comment": null, "summary": "Analyzing multilayer networks is central to understanding complex relational measurements collected across multiple conditions or over time. A pivotal task in this setting is to quantify uncertainty in community structure while appropriately pooling information across layers and accommodating layer-specific heterogeneity. Building on the multilayer degree-corrected mixed-membership (ML-DCMM) model, which captures both stable community membership profiles and layer-specific vertex activity levels, we propose a Bayesian inference framework based on a spectral-assisted likelihood. We then develop a computationally efficient Gaussian variational inference algorithm implemented via stochastic gradient descent. Our theoretical analysis establishes a variational Bernstein--von Mises theorem, which provides a frequentist guarantee for using the variational posterior to construct confidence sets for mixed memberships. We demonstrate the utility of the method on a U.S. airport longitudinal network, where the procedure yields robust estimates, natural uncertainty quantification, and competitive performance relative to state-of-the-art methods."}
{"id": "2512.08035", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.08035", "abs": "https://arxiv.org/abs/2512.08035", "authors": ["Rafael Catoia Pulgrossi", "Nathan L R Williams", "Yubin Raut", "Jed Fuhrman", "Sangwon Hyun"], "title": "Defining 3-dimensional marine provinces with phytoplankton compositions", "comment": null, "summary": "Marine provinces rarely include fine-resolution biological data, and are often defined spatially across only latitude and longitude. Therefore, we aimed to determine how phytoplankton distributions define marine provinces across 3-dimensions (i.e., latitude, longitude, and depth). To do this, we developed a new algorithm called \\texttt{bioprovince} which can be applied to compositional biological data. The algorithm first clusters compositional samples to identify spatially coherent groups of samples, then makes flexible province predictions in the broader 3d spatial grid based on environmental similarity. We applied \\texttt{bioprovince} to phytoplankton Amplicon Sequencing Variants (ASVs) from five, depth-resolved ocean transects spanning north-south in the Pacific Ocean. In the surface layer of the ocean, our method agreed well with traditional Longhurst provinces. In some cases, the method revealed that with more granular taxonomic resolution afforded by ASVs, traditional Longhurst provinces were divided into smaller zones. Also, one of the major advances of this method is its ability to incorporate a third dimension, depth. Indeed, our analysis found significant depth-wise partitions throughout the Pacific with remarkable agreement in the equatorial region with the base of the euphotic zone. Our algorithm's ability to delineate 3-dimensional bioprovinces will enable scientists to discover new ecological interpretations of marine phytoplankton ecology and biogeography. Furthermore, as compositional biological data inherently exists in three spatial dimensions in nature, bioprovince is broadly applicable beyond marine plankton, offering a more holistic perspective on biological provinces across diverse environments."}
{"id": "2512.08022", "categories": ["stat.ML", "cs.LG", "math.NA", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.08022", "abs": "https://arxiv.org/abs/2512.08022", "authors": ["Jinyuan Chang", "Chenguang Duan", "Yuling Jiao", "Ruoxuan Li", "Jerry Zhijian Yang", "Cheng Yuan"], "title": "Provable Diffusion Posterior Sampling for Bayesian Inversion", "comment": null, "summary": "This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems."}
{"id": "2512.07888", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.07888", "abs": "https://arxiv.org/abs/2512.07888", "authors": ["Fahad Mostafa", "Hafiz Khan"], "title": "Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification", "comment": "23 pages, 4 figures", "summary": "Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical."}
{"id": "2512.07973", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.07973", "abs": "https://arxiv.org/abs/2512.07973", "authors": ["Mithun Kumar Acharjee", "AKM Fazlur Rahman"], "title": "Bayesian Semiparametric Joint Dynamic Model for Multitype Recurrent Events and a Terminal Event", "comment": "30 pages, 4 tables, 4 figures, prepared for Statistics in Medicine", "summary": "In many biomedical research, recurrent events such as myocardial infraction, stroke, and heart failure often result in a terminal outcome such as death. Understanding the relationship among the multi-type recurrent events and terminal event is essential for developing interventions to prolong the terminal event such as death. This study introduces a Bayesian semiparametric joint dynamic model for type-specific hazards that quantifies how the type-specific event history dynamically changes the intensities of each recurrent event type and the terminal event over calendar time. The framework jointly captures unmeasured heterogeneity through a shared frailty term, cumulative effects of past recurrent events on themselves and terminal events, and the effects of covariates. Gamma process priors (GPP) are used as a nonparametric prior for the baseline cumulative hazard function (CHF) and parametric priors for covariates and frailty. For a more accurate risk assessment, this model provides an analytical closed-form estimator of cumulative hazard functions (CHF) and frailties. The Breslow-Aalen-type estimators of CHFs are special cases of our estimators when the precision parameters are set to zero. We evaluate the performance of the model through extensive simulations and apply the method to the Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack Trial (ALLHAT). The analysis offers a practical past event effect based risk assessment for acute and chronic cardiovascular recurrent events with a terminal end point death and provides new information to support the prevention and treatment of cardiovascular disease to clinicians."}
{"id": "2512.08173", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08173", "abs": "https://arxiv.org/abs/2512.08173", "authors": ["Fatih Kızılaslan", "Valeria Vitelli"], "title": "Bayesian Semiparametric Mixture Cure (Frailty) Models", "comment": "27 pages, 4 tables, 8 figures", "summary": "In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial."}
{"id": "2512.08585", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.08585", "abs": "https://arxiv.org/abs/2512.08585", "authors": ["Ankita Sharma", "Partha Chakroborty", "Pranamesh Chakraborty"], "title": "Distribution of Gaps in Multi-lane Orderly and Disorderly Traffic Streams", "comment": null, "summary": "To study gap acceptance behaviour one needs the distribution (or probability density function) of gaps in the opposing stream. Further, in these times of widespread availability of large computing powers, traffic simulation has emerged as a popular analysis and design tool. Such simulations rely on randomly generating the arriving vehicles in a way that statistically resembles real-world streams. The generation process for disorderly streams requires information on gap distributions. A study of past literature reveals that very little work has been done to determine the distribution of gaps on multi-lane orderly and disorderly streams. This study aims to develop an analytical framework to specify the distribution of gaps for such streams. This analytical framework is built using the Renewal Process Theory. A maximum likelihood based process for the estimation of the parameters of the analytically derived distribution is also described. Later, real-world gap data from three different sites covering orderly and disorderly streams are used to show how the derived distribution function (using the proposed method) ably describes the observed gap distributions."}
{"id": "2512.08146", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.08146", "abs": "https://arxiv.org/abs/2512.08146", "authors": ["Fangzheng Xie", "Hsin-Hsiung Huang"], "title": "Uncertainty quantification for mixed membership in multilayer networks with degree heterogeneity using Gaussian variational inference", "comment": null, "summary": "Analyzing multilayer networks is central to understanding complex relational measurements collected across multiple conditions or over time. A pivotal task in this setting is to quantify uncertainty in community structure while appropriately pooling information across layers and accommodating layer-specific heterogeneity. Building on the multilayer degree-corrected mixed-membership (ML-DCMM) model, which captures both stable community membership profiles and layer-specific vertex activity levels, we propose a Bayesian inference framework based on a spectral-assisted likelihood. We then develop a computationally efficient Gaussian variational inference algorithm implemented via stochastic gradient descent. Our theoretical analysis establishes a variational Bernstein--von Mises theorem, which provides a frequentist guarantee for using the variational posterior to construct confidence sets for mixed memberships. We demonstrate the utility of the method on a U.S. airport longitudinal network, where the procedure yields robust estimates, natural uncertainty quantification, and competitive performance relative to state-of-the-art methods."}
{"id": "2512.08022", "categories": ["stat.ML", "cs.LG", "math.NA", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.08022", "abs": "https://arxiv.org/abs/2512.08022", "authors": ["Jinyuan Chang", "Chenguang Duan", "Yuling Jiao", "Ruoxuan Li", "Jerry Zhijian Yang", "Cheng Yuan"], "title": "Provable Diffusion Posterior Sampling for Bayesian Inversion", "comment": null, "summary": "This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems."}
{"id": "2512.08118", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08118", "abs": "https://arxiv.org/abs/2512.08118", "authors": ["Wookyeong Song", "Hans-Georg Müller"], "title": "ADOPT: Additive Optimal Transport Regression", "comment": "19 pages, 2 figures", "summary": "Regression analysis for responses taking values in general metric spaces has received increasing attention, particularly for settings with Euclidean predictors $X \\in \\mathbb{R}^p$ and non-Euclidean responses $Y$ in metric spaces. While additive regression is a powerful tool for enhancing interpretability and mitigating the curse of dimensionality in the presence of multivariate predictors, its direct extension is hindered by the absence of vector space operations in general metric spaces. We propose a novel framework for additive optimal transport regression, which incorporates additive structure through optimal geodesic transports. A key idea is to extend the notion of optimal transports in Wasserstein spaces to general geodesic metric spaces. This unified approach accommodates a wide range of responses, including probability distributions, symmetric positive definite (SPD) matrices with various metrics and spherical data. The practical utility of the method is illustrated with correlation matrices derived from resting state fMRI brain imaging data."}
{"id": "2512.08756", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.08756", "abs": "https://arxiv.org/abs/2512.08756", "authors": ["Keshav Motwani", "Ali Shojaie", "Ariel Rokem", "Eardi Lila"], "title": "Genetic Regression Analysis of Human Brain Connectivity Using an Efficient Estimator of Genetic Covariance", "comment": null, "summary": "Non-invasive measurements of the human brain using magnetic resonance imaging (MRI) have significantly improved our understanding the brain's network organization by enabling measurement of anatomical connections between brain regions (structural connectivity) and their coactivation (functional connectivity). Heritability analyses have established that genetics account for considerable intersubject variability in structural and functional connectivity. However, characterizing how genetics shape the relationship between structural and functional connectomes remains challenging, since this association is obscured by unique environmental exposures in observed data. To address this, we develop a regression analysis framework that enables characterization of the relationship between latent genetic contributions to structural and functional connectivity. Implementing the proposed framework requires estimating genetic covariance matrices in multivariate random effects models, which is computationally intractable for high-dimensional connectome data using existing methods. We introduce a constrained method-of-moments estimator that is several orders of magnitude faster than existing methods without sacrificing estimation accuracy. For the genetic regression analysis, we develop regularized estimation approaches, including ridge, lasso, and tensor regression. Applying our method to Human Connectome Project data, we find that functional connectivity is moderately predictable from structure at the genetic level (max R^2 = 0.34), though it is not directly predictable in the observed data (max R^2 = 0.03). This stark contrast suggests that unique environmental factors mask strong genetically-encoded structure-function relationships."}
{"id": "2512.08173", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08173", "abs": "https://arxiv.org/abs/2512.08173", "authors": ["Fatih Kızılaslan", "Valeria Vitelli"], "title": "Bayesian Semiparametric Mixture Cure (Frailty) Models", "comment": "27 pages, 4 tables, 8 figures", "summary": "In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial."}
{"id": "2512.08176", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.08176", "abs": "https://arxiv.org/abs/2512.08176", "authors": ["Xiuyuan Cheng", "Yao Xie", "Linglingzhi Zhu", "Yunqin Zhu"], "title": "Worst-case generation via minimax optimization in Wasserstein space", "comment": null, "summary": "Worst-case generation plays a critical role in evaluating robustness and stress-testing systems under distribution shifts, in applications ranging from machine learning models to power grids and medical prediction systems. We develop a generative modeling framework for worst-case generation for a pre-specified risk, based on min-max optimization over continuous probability distributions, namely the Wasserstein space. Unlike traditional discrete distributionally robust optimization approaches, which often suffer from scalability issues, limited generalization, and costly worst-case inference, our framework exploits the Brenier theorem to characterize the least favorable (worst-case) distribution as the pushforward of a transport map from a continuous reference measure, enabling a continuous and expressive notion of risk-induced generation beyond classical discrete DRO formulations. Based on the min-max formulation, we propose a Gradient Descent Ascent (GDA)-type scheme that updates the decision model and the transport map in a single loop, establishing global convergence guarantees under mild regularity assumptions and possibly without convexity-concavity. We also propose to parameterize the transport map using a neural network that can be trained simultaneously with the GDA iterations by matching the transported training samples, thereby achieving a simulation-free approach. The efficiency of the proposed method as a risk-induced worst-case generator is validated by numerical experiments on synthetic and image data."}
{"id": "2512.08140", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08140", "abs": "https://arxiv.org/abs/2512.08140", "authors": ["Mohsen Sadatsafavi", "Jeroen Hoogland", "Thomas P. A. Debray", "John Petkau"], "title": "Non-parametric assessment of the calibration of individualized treatment effects", "comment": "24 pages, 9 figures, 2 tables, 2 appendices", "summary": "An important aspect of the performance of algorithms that predict individualized treatment effects (ITE) is moderate calibration, i.e., the average treatment effect among individuals with predicted treatment effect of z being equal to z. The assessment of moderate calibration is a challenging task on two fronts: counterfactual responses are unobserved, and quantifying the conditional response function for models that generate continuous predicted values requires regularization or parametric modeling. Perhaps because of these challenges, there is currently no inferential method for the null hypothesis that an ITE model is moderately calibrated in a population. In this work, we propose non-parametric methods for the assessment of moderate calibration of ITE models for binary outcomes using data from a randomized trial. These methods simultaneously resolve both challenges, resulting in novel numerical, graphical, and inferential methods for the assessment of moderate calibration. The key idea is to formulate a stochastic process for the cumulative prediction errors that obeys a functional central limit theorem, enabling the use of the properties of Brownian motion for asymptotic inference. We propose two approaches to construct this process from a sample: a conditional approach that relies on predicted risks (often an output of ITE models), and a marginal approach based on replacing the cumulative conditional expected value and variance terms with their marginal counterparts. Numerical simulations confirm the desirable properties of both approaches and their ability to detect miscalibration of different forms. We use a case study to provide practical suggestions on graphical presentation and the interpretation of results. Moderate calibration of predicted ITEs can be assessed without requiring regularization techniques or making assumptions about the functional form of treatment response."}
{"id": "2512.08824", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.08824", "abs": "https://arxiv.org/abs/2512.08824", "authors": ["Jake McGrath", "Amanda Glazer", "Vanna Bushong", "Michelle Nguyen", "Kirk Goldsberry"], "title": "Commanding the Foul Shot: A New Ensemble of Free Throw Metrics", "comment": null, "summary": "With the NBA's adoption of in-game limb tracking in 2023, Sony's Hawk-Eye system now captures high-resolution, 3D poses of players and the ball 60 times per second. Linking these data to key events such as shots, passes, and rebounds opens a new era in NBA analytics. Here, we leverage Hawk-Eye tracking to introduce a novel ensemble of metrics for evaluating free-throw shooting and demonstrate that our framework captures skill more effectively than traditional make-or-miss statistics. Inspired by baseball analytics, we introduce command, which quantifies the quality of a free throw by measuring a shooter's accuracy and precision near the basket's bullseye. This metric recognizes that some makes (or misses) are better than others and captures a player's ability to execute quality attempts consistently. To identify what drives command, we define launch-based metrics assessing consistency in release velocity, angle, and 3D position. Players with greater touch -- i.e., more consistent launch dynamics -- exhibit stronger command as they can reliably control their shot trajectory. Finally, we develop a physics model to identify the range of launch conditions that result in a make and to determine which launch conditions are most robust to small perturbations. This framework reveals \"safe\" launch regions and explains why certain players, such as Steph Curry, excel at free throws, providing actionable insights for player development."}
{"id": "2512.08232", "categories": ["stat.ME", "math.PR", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.08232", "abs": "https://arxiv.org/abs/2512.08232", "authors": ["Léo R. Belzile", "Christian Genest", "Frédéric Ouimet", "Donald Richards"], "title": "Wishart kernel density estimation for strongly mixing time series on the cone of positive definite matrices", "comment": "40 pages, 4 figures, 2 tables", "summary": "A Wishart kernel density estimator (KDE) is introduced for density estimation in the cone of positive definite matrices. The estimator is boundary-aware and mitigates the boundary bias suffered by conventional KDEs, while remaining simple to implement. Its mean squared error, uniform strong consistency on expanding compact sets, and asymptotic normality are established under the Lebesgue measure and suitable mixing conditions. This work represents the first study of density estimation on this space under any metric. For independent observations, an asymptotic upper bound on the mean absolute error is also derived. A simulation study compares the performance of the Wishart KDE to another boundary-aware KDE that relies on the matrix-variate lognormal distribution proposed by Schwartzman [Int. Stat. Rev., 2016, 84(3), 456-486]. Results suggest that the Wishart KDE is superior for a selection of autoregressive coefficient matrices and innovation covariance matrices when estimating the stationary marginal density of a Wishart autoregressive process. To illustrate the practical utility of the Wishart KDE, an application to finance is made by estimating the marginal density function of a time series of realized covariance matrices, calculated from 5-minute intra-day returns, between the share prices of Amazon Corp. and the Standard & Poor's 500 exchange-traded fund over a one-year period. All code is publicly available via the R package ksm to facilitate implementation of the method and reproducibility of the findings."}
{"id": "2512.08601", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.08601", "abs": "https://arxiv.org/abs/2512.08601", "authors": ["Orit Davidovich", "Shimrit Shtern", "Segev Wasserkrug", "Nimrod Megiddo"], "title": "Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis", "comment": null, "summary": "Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context."}
{"id": "2512.08144", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08144", "abs": "https://arxiv.org/abs/2512.08144", "authors": ["Joshua Wasserman", "Michael R. Elliott", "Ben B. Hansen"], "title": "Propensity score adjustment when errors in achievement measures inform treatment assignment", "comment": "28 pages, 3 figures", "summary": "U.S. state education agencies mark schools displaying achievement gaps between demographic subgroups as needing improvement. Some schools may have few students in these subgroups, such that average end-of-year test scores only noisily measure the average \"true\" score--the score one would expect if students took the test many times. This, in addition to the masking of small subgroup averages in publicly available assessment data, poses challenges for evaluating interventions aimed at closing achievement gaps. We introduce propensity score estimates designed to achieve balance on subgroup average true scores. These estimates are available even when noisy measurements are not and improve overlap compared to those that ignore measurement error, leading to greater bias reduction of matching estimators. We demonstrate our methods through simulation and an application to a statewide initiative in Texas for curbing summer learning loss."}
{"id": "2512.08886", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.08886", "abs": "https://arxiv.org/abs/2512.08886", "authors": ["Wenderson Gomes Barbosa", "Kerolly Kedma Felix do Nascimento", "Fábio Sandro dos Santos", "Silvio Fernando Alves Xavier Júnior", "Tiago A. E. Ferreira"], "title": "Multifractal behavior of price changes in the Green Bonds funds", "comment": null, "summary": "Climate change has driven the market to seek new ways of raising funds to mitigate its effects. One such innovation is the emergence of Green Bonds financial assets specifically designed to support sustainable projects. This study explores the fractal behavior of daily price changes in thirty-five Green Bond funds using the Multifractal Detrended Fluctuation Analysis (MFDFA) method. Our results indicate that price changes exhibit persistent behavior and high multifractality, characterized by large fluctuations. Only one of the thirty-five time series analyzed showed an outlier result, suggesting that the funds display very similar behavior. By shuffling the series, we were able to reduce multifractality significantly. These findings suggest that Green Bond funds exhibit multifractal behavior typical of other financial assets."}
{"id": "2512.08173", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08173", "abs": "https://arxiv.org/abs/2512.08173", "authors": ["Fatih Kızılaslan", "Valeria Vitelli"], "title": "Bayesian Semiparametric Mixture Cure (Frailty) Models", "comment": "27 pages, 4 tables, 8 figures", "summary": "In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial."}
{"id": "2512.08146", "categories": ["stat.ME", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.08146", "abs": "https://arxiv.org/abs/2512.08146", "authors": ["Fangzheng Xie", "Hsin-Hsiung Huang"], "title": "Uncertainty quantification for mixed membership in multilayer networks with degree heterogeneity using Gaussian variational inference", "comment": null, "summary": "Analyzing multilayer networks is central to understanding complex relational measurements collected across multiple conditions or over time. A pivotal task in this setting is to quantify uncertainty in community structure while appropriately pooling information across layers and accommodating layer-specific heterogeneity. Building on the multilayer degree-corrected mixed-membership (ML-DCMM) model, which captures both stable community membership profiles and layer-specific vertex activity levels, we propose a Bayesian inference framework based on a spectral-assisted likelihood. We then develop a computationally efficient Gaussian variational inference algorithm implemented via stochastic gradient descent. Our theoretical analysis establishes a variational Bernstein--von Mises theorem, which provides a frequentist guarantee for using the variational posterior to construct confidence sets for mixed memberships. We demonstrate the utility of the method on a U.S. airport longitudinal network, where the procedure yields robust estimates, natural uncertainty quantification, and competitive performance relative to state-of-the-art methods."}
{"id": "2512.07888", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.07888", "abs": "https://arxiv.org/abs/2512.07888", "authors": ["Fahad Mostafa", "Hafiz Khan"], "title": "Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification", "comment": "23 pages, 4 figures", "summary": "Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical."}
{"id": "2512.08179", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08179", "abs": "https://arxiv.org/abs/2512.08179", "authors": ["Yating Zou", "Marcos Matabuena", "Michael R. Kosorok"], "title": "Distributional Random Forests for Complex Survey Designs on Reproducing Kernel Hilbert Spaces", "comment": null, "summary": "We study estimation of the conditional law $P(Y|X=\\mathbf{x})$ and continuous functionals $Ψ(P(Y|X=\\mathbf{x}))$ when $Y$ takes values in a locally compact Polish space, $X \\in \\mathbb{R}^p$, and the observations arise from a complex survey design. We propose a survey-calibrated distributional random forest (SDRF) that incorporates complex-design features via a pseudo-population bootstrap, PSU-level honesty, and a Maximum Mean Discrepancy (MMD) split criterion computed from kernel mean embeddings of Hájek-type (design-weighted) node distributions. We provide a framework for analyzing forest-style estimators under survey designs; establish design consistency for the finite-population target and model consistency for the super-population target under explicit conditions on the design, kernel, resampling multipliers, and tree partitions. As far as we are aware, these are the first results on model-free estimation of conditional distributions under survey designs. Simulations under a stratified two-stage cluster design provide finite sample performance and demonstrate the statistical error price of ignoring the survey design. The broad applicability of SDRF is demonstrated using NHANES: We estimate the tolerance regions of the conditional joint distribution of two diabetes biomarkers, illustrating how distributional heterogeneity can support subgroup-specific risk profiling for diabetes mellitus in the U.S. population."}
{"id": "2512.08173", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08173", "abs": "https://arxiv.org/abs/2512.08173", "authors": ["Fatih Kızılaslan", "Valeria Vitelli"], "title": "Bayesian Semiparametric Mixture Cure (Frailty) Models", "comment": "27 pages, 4 tables, 8 figures", "summary": "In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial."}
{"id": "2512.08232", "categories": ["stat.ME", "math.PR", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.08232", "abs": "https://arxiv.org/abs/2512.08232", "authors": ["Léo R. Belzile", "Christian Genest", "Frédéric Ouimet", "Donald Richards"], "title": "Wishart kernel density estimation for strongly mixing time series on the cone of positive definite matrices", "comment": "40 pages, 4 figures, 2 tables", "summary": "A Wishart kernel density estimator (KDE) is introduced for density estimation in the cone of positive definite matrices. The estimator is boundary-aware and mitigates the boundary bias suffered by conventional KDEs, while remaining simple to implement. Its mean squared error, uniform strong consistency on expanding compact sets, and asymptotic normality are established under the Lebesgue measure and suitable mixing conditions. This work represents the first study of density estimation on this space under any metric. For independent observations, an asymptotic upper bound on the mean absolute error is also derived. A simulation study compares the performance of the Wishart KDE to another boundary-aware KDE that relies on the matrix-variate lognormal distribution proposed by Schwartzman [Int. Stat. Rev., 2016, 84(3), 456-486]. Results suggest that the Wishart KDE is superior for a selection of autoregressive coefficient matrices and innovation covariance matrices when estimating the stationary marginal density of a Wishart autoregressive process. To illustrate the practical utility of the Wishart KDE, an application to finance is made by estimating the marginal density function of a time series of realized covariance matrices, calculated from 5-minute intra-day returns, between the share prices of Amazon Corp. and the Standard & Poor's 500 exchange-traded fund over a one-year period. All code is publicly available via the R package ksm to facilitate implementation of the method and reproducibility of the findings."}
{"id": "2512.08828", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08828", "abs": "https://arxiv.org/abs/2512.08828", "authors": ["Swaraj Bose", "Walter Dempsey"], "title": "Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference", "comment": null, "summary": "Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS)."}
{"id": "2512.08179", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08179", "abs": "https://arxiv.org/abs/2512.08179", "authors": ["Yating Zou", "Marcos Matabuena", "Michael R. Kosorok"], "title": "Distributional Random Forests for Complex Survey Designs on Reproducing Kernel Hilbert Spaces", "comment": null, "summary": "We study estimation of the conditional law $P(Y|X=\\mathbf{x})$ and continuous functionals $Ψ(P(Y|X=\\mathbf{x}))$ when $Y$ takes values in a locally compact Polish space, $X \\in \\mathbb{R}^p$, and the observations arise from a complex survey design. We propose a survey-calibrated distributional random forest (SDRF) that incorporates complex-design features via a pseudo-population bootstrap, PSU-level honesty, and a Maximum Mean Discrepancy (MMD) split criterion computed from kernel mean embeddings of Hájek-type (design-weighted) node distributions. We provide a framework for analyzing forest-style estimators under survey designs; establish design consistency for the finite-population target and model consistency for the super-population target under explicit conditions on the design, kernel, resampling multipliers, and tree partitions. As far as we are aware, these are the first results on model-free estimation of conditional distributions under survey designs. Simulations under a stratified two-stage cluster design provide finite sample performance and demonstrate the statistical error price of ignoring the survey design. The broad applicability of SDRF is demonstrated using NHANES: We estimate the tolerance regions of the conditional joint distribution of two diabetes biomarkers, illustrating how distributional heterogeneity can support subgroup-specific risk profiling for diabetes mellitus in the U.S. population."}
{"id": "2512.08182", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08182", "abs": "https://arxiv.org/abs/2512.08182", "authors": ["Yongda Wang", "Shifeng Xiong"], "title": "Nonparametric inference with massive data via grouped empirical likelihood", "comment": null, "summary": "To address the computational issue in empirical likelihood methods with massive data, this paper proposes a grouped empirical likelihood (GEL) method. It divides $N$ observations into $n$ groups, and assigns the same probability weight to all observations within the same group. GEL estimates the $n\\ (\\ll N)$ weights by maximizing the empirical likelihood ratio. The dimensionality of the optimization problem is thus reduced from $N$ to $n$, thereby lowering the computational complexity. We prove that GEL possesses the same first order asymptotic properties as the conventional empirical likelihood method under the estimating equation settings and the classical two-sample mean problem. A distributed GEL method is also proposed with several servers. Numerical simulations and real data analysis demonstrate that GEL can keep the same inferential accuracy as the conventional empirical likelihood method, and achieves substantial computational acceleration compared to the divide-and-conquer empirical likelihood method. We can analyze a billion data with GEL in tens of seconds on only one PC."}
{"id": "2512.08232", "categories": ["stat.ME", "math.PR", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.08232", "abs": "https://arxiv.org/abs/2512.08232", "authors": ["Léo R. Belzile", "Christian Genest", "Frédéric Ouimet", "Donald Richards"], "title": "Wishart kernel density estimation for strongly mixing time series on the cone of positive definite matrices", "comment": "40 pages, 4 figures, 2 tables", "summary": "A Wishart kernel density estimator (KDE) is introduced for density estimation in the cone of positive definite matrices. The estimator is boundary-aware and mitigates the boundary bias suffered by conventional KDEs, while remaining simple to implement. Its mean squared error, uniform strong consistency on expanding compact sets, and asymptotic normality are established under the Lebesgue measure and suitable mixing conditions. This work represents the first study of density estimation on this space under any metric. For independent observations, an asymptotic upper bound on the mean absolute error is also derived. A simulation study compares the performance of the Wishart KDE to another boundary-aware KDE that relies on the matrix-variate lognormal distribution proposed by Schwartzman [Int. Stat. Rev., 2016, 84(3), 456-486]. Results suggest that the Wishart KDE is superior for a selection of autoregressive coefficient matrices and innovation covariance matrices when estimating the stationary marginal density of a Wishart autoregressive process. To illustrate the practical utility of the Wishart KDE, an application to finance is made by estimating the marginal density function of a time series of realized covariance matrices, calculated from 5-minute intra-day returns, between the share prices of Amazon Corp. and the Standard & Poor's 500 exchange-traded fund over a one-year period. All code is publicly available via the R package ksm to facilitate implementation of the method and reproducibility of the findings."}
{"id": "2512.08258", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08258", "abs": "https://arxiv.org/abs/2512.08258", "authors": ["Yiwei Tang", "Judy Huixia Wang", "Deyuan Li"], "title": "Perturbation-based Inference for Extreme Value Index", "comment": null, "summary": "The extreme value index (EVI) characterizes the tail behavior of a distribution and is crucial for extreme value theory. Inference on the EVI is challenging due to data scarcity in the tail region. We propose a novel method for constructing confidence intervals for the EVI using synthetic exceedances generated via perturbation. Rather than perturbing the entire sample, we add noise to exceedances above a high threshold and apply the generalized Pareto distribution (GPD) approximation. Confidence intervals are derived by simulating the distribution of pivotal statistics from the perturbed data. We show that the pivotal statistic is consistent, ensuring the proposed method provides consistent intervals for the EVI. Additionally, we demonstrate that the perturbed data is differentially private. When the GPD approximation is inadequate, we introduce a refined perturbation method. Simulation results show that our approach outperforms existing methods, providing robust and reliable inference."}
{"id": "2512.08637", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08637", "abs": "https://arxiv.org/abs/2512.08637", "authors": ["Cagatay Ayhan", "Audrey N. Nash", "Roberto Vincis", "Martin Bauer", "Richard Bertram", "Tom Needham"], "title": "A Persistent Homology Pipeline for the Analysis of Neural Spike Train Data", "comment": null, "summary": "In this article, we introduce a Topological Data Analysis (TDA) pipeline for neural spike train data. Understanding how the brain transforms sensory information into perception and behavior requires analyzing coordinated neural population activity. Modern electrophysiology enables simultaneous recording of spike train ensembles, but extracting meaningful information from these datasets remains a central challenge in neuroscience. A fundamental question is how ensembles of neurons discriminate between different stimuli or behavioral states, particularly when individual neurons exhibit weak or no stimulus selectivity, yet their coordinated activity may still contribute to network-level encoding. We describe a TDA framework that identifies stimulus-discriminative structure in spike train ensembles recorded from the mouse insular cortex during presentation of deionized water stimuli at distinct non-nociceptive temperatures. We show that population-level topological signatures effectively differentiate oral thermal stimuli even when individual neurons provide little or no discrimination. These findings demonstrate that ensemble organization can carry perceptually relevant information that standard single-unit analysis may miss. The framework builds on a mathematical representation of spike train ensembles that enables persistent homology to be applied to collections of point processes. At its core is the widely-used Victor-Purpura (VP) distance. Using this metric, we construct persistence-based descriptors that capture multiscale topological features of ensemble geometry. Two key theoretical results support the method: a stability theorem establishing robustness of persistent homology to perturbations in the VP metric parameter, and a probabilistic stability theorem ensuring robustness of topological signatures."}
{"id": "2512.08658", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08658", "abs": "https://arxiv.org/abs/2512.08658", "authors": ["Moritz Fabian Danzer", "Kaspar Rufibach", "Jan Beyersmann", "René Schmidt"], "title": "Exhausting the type I error level in event-driven group-sequential designs with a closed testing procedure for progression-free and overall survival", "comment": "Main manuscript: 18 pages, 5 figures Supplementary Material: 11 pages, 4 figures", "summary": "In oncological clinical trials, overall survival (OS) is the gold-standard endpoint, but long follow-up and treatment switching can delay or dilute detectable effects. Progression-free survival (PFS) often provides earlier evidence and is therefore frequently used together with OS as multiple primary endpoints. Since in certain scenarios trial success may be defined if one of the two hypotheses involved can be rejected, a correction for multiple testing may be deemed necessary. Because PFS and OS are generally highly dependent, their test statistics are typically correlated. Ignoring this dependency (e.g. via a simple Bonferroni correction) is not power optimal. We develop a group-sequential testing procedure for the multiple primary endpoints PFS and OS that fully exhausts the family-wise error rate (FWER) by exploiting their dependence. Specifically, we characterize the joint asymptotic distribution of log-rank statistics across endpoints and multiple event-driven analysis cutoffs. Furthermore, we show that we can consistently estimate the covariance structure. Embedding these results in a closed testing procedure, we can recalculate critical values of the test statistics in order to spend the available type I error optimally. An important extension to the current literature is that we allow for both interim and final analysis to be event-driven. Simulations based on illness-death multi-state models empirically confirm FWER control for moderate to large sample sizes. Compared with a simple Bonferroni correction, the proposed methods recover roughly two thirds of the power loss for OS, increase disjunctive and conjunctive power, and enable meaningful early stopping. In planning, these gains translate into about 5% fewer OS events required to reach the targeted power. We also discuss practical issues in the implementation of such designs and possible extensions of the introduced method."}
{"id": "2512.08735", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08735", "abs": "https://arxiv.org/abs/2512.08735", "authors": ["Michael Price", "Debdeep Pati", "Ning Ning"], "title": "Stationary Point Constrained Inference via Diffeomorphisms", "comment": null, "summary": "Stationary points or derivative zero crossings of a regression function correspond to points where a trend reverses, making their estimation scientifically important. Existing approaches to uncertainty quantification for stationary points cannot deliver valid joint inference when multiple extrema are present, an essential capability in applications where the relative locations of peaks and troughs carry scientific significance. We develop a principled framework for functions with multiple regions of monotonicity by constraining the number of stationary points. We represent each function in the diffeomorphic formulation as the composition of a simple template and a smooth bijective transformation, and show that this parameterization enables coherent joint inference on the extrema. This construction guarantees a prespecified number of stationary points and provides a direct, interpretable parameterization of their locations. We derive non-asymptotic confidence bounds and establish approximate normality for the maximum likelihood estimators, with parallel results in the Bayesian setting. Simulations and an application to brain signal estimation demonstrate the method's accuracy and interpretability."}
{"id": "2512.08828", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.08828", "abs": "https://arxiv.org/abs/2512.08828", "authors": ["Swaraj Bose", "Walter Dempsey"], "title": "Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference", "comment": null, "summary": "Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS)."}
{"id": "2512.08847", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08847", "abs": "https://arxiv.org/abs/2512.08847", "authors": ["Nikolaos Ignatiadis", "Li Ma"], "title": "Partially Bayes p-values for large scale inference", "comment": null, "summary": "We seek to conduct statistical inference for a large collection of primary parameters, each with its own nuisance parameters. Our approach is partially Bayesian, in that we treat the primary parameters as fixed while we model the nuisance parameters as random and drawn from an unknown distribution which we endow with a nonparametric prior. We compute partially Bayes p-values by conditioning on nuisance parameter statistics, that is, statistics that are ancillary for the primary parameters and informative about the nuisance parameters. The proposed p-values have a Bayesian interpretation as tail areas computed with respect to the posterior distribution of the nuisance parameters. Similarly to the conditional predictive p-values of Bayarri and Berger, the partially Bayes p-values avoid double use of the data (unlike posterior predictive p-values). A key ingredient of our approach is that we model nuisance parameters hierarchically across problems; the sharing of information across problems leads to improved calibration. We illustrate the proposed partially Bayes p-values in two applications: the normal means problem with unknown variances and a location-scale model with unknown distribution shape. We model the scales via Dirichlet processes in both examples and the distribution shape via Pólya trees in the second. Our proposed partially Bayes p-values increase power and calibration compared to purely frequentist alternatives."}
{"id": "2512.08137", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.08137", "abs": "https://arxiv.org/abs/2512.08137", "authors": ["Quan Vu", "Xuanjie Shao", "Raphaël Huser", "Andrew Zammit-Mangion"], "title": "deepspat: An R package for modeling nonstationary spatial and spatio-temporal Gaussian and extremes data through deep deformations", "comment": null, "summary": "Nonstationarity in spatial and spatio-temporal processes is ubiquitous in environmental datasets, but is not often addressed in practice, due to a scarcity of statistical software packages that implement nonstationary models. In this article, we introduce the R software package deepspat, which allows for modeling, fitting and prediction with nonstationary spatial and spatio-temporal models applied to Gaussian and extremes data. The nonstationary models in our package are constructed using a deep multi-layered deformation of the original spatial or spatio-temporal domain, and are straightforward to implement. Model parameters are estimated using gradient-based optimization of customized loss functions with tensorflow, which implements automatic differentiation. The functionalities of the package are illustrated through simulation studies and an application to Nepal temperature data."}
