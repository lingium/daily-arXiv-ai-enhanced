<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 3]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.ME](#stat.ME) [Total: 11]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Language markers of emotion flexibility predict depression and anxiety treatment outcomes](https://arxiv.org/abs/2601.07961)
*Benjamin Brindle,George Bonanno,Thomas Derrick Hull,Nicolas Charon,Matteo Malgaroli*

Main category: stat.AP

TL;DR: 利用语言模型从心理治疗转录文本中提取情绪动态，识别治疗无反应者，发现情绪僵化可作为治疗风险分层的指标


<details>
  <summary>Details</summary>
Motivation: 预测焦虑和抑郁治疗无反应具有挑战性，部分原因是现实世界护理中症状评估稀疏。需要探索被动捕获的细粒度情绪是否可作为治疗结果的语言标记

Method: 分析12,043名美国患者的12周远程治疗转录文本，使用基于transformer的小型语言模型在对话轮次层面提取患者情绪，应用状态空间模型(VISTA)基于情绪动态聚类亚组并生成时间网络

Result: 识别出两个群体：改善组(8,230人)和无反应组(3,813人)。无反应组症状恶化几率增加，临床显著改善可能性降低。时间网络显示无反应者中悲伤和恐惧对情绪动态影响最大，而改善组表现出平衡的喜悦、悲伤和中性表达

Conclusion: 情绪僵化的语言标记可作为可扩展、可解释且理论基础的指标，用于治疗风险分层

Abstract: Predicting treatment non-response for anxiety and depression is challenging, in part because of sparse symptom assessments in real-world care. We examined whether passively captured, fine-grained emotions serve as linguistic markers of treatment outcomes by analyzing 12 weeks of de-identified teletherapy transcripts from 12,043 U.S. patients with moderate-to-severe anxiety and depression symptoms. A transformer-based small language model extracted patients' emotions at the talk-turn level; a state-space model (VISTA) clustered subgroups based on emotion dynamics over time and produced temporal networks. Two groups emerged: an improving group (n=8,230) and a non-response group (n=3813) showing increased odds of symptom deterioration, and lower likelihood of clinically significant improvement. Temporal networks indicated that sadness and fear exerted most influence on emotion dynamics in non-responders, whereas improving patients showed balanced joy, sadness, and neutral expressions. Findings suggest that linguistic markers of emotional inflexibility can serve as scalable, interpretable, and theoretically grounded indicators for treatment risk stratification.

</details>


### [2] [A parsimonious tail compliant multiscale statistical model for aggregated rainfall](https://arxiv.org/abs/2601.08350)
*Pierre Ailliot,Carlo Gaetan,Philippe Naveau*

Main category: stat.AP

TL;DR: 提出一个简约框架，使用扩展广义帕累托分布（EGPD）建模从6分钟到3天不同时间尺度（从亚小时到周）的降雨强度分布，确保不同尺度的重现水平不交叉，仅需8个参数即可捕捉完整分布。


<details>
  <summary>Details</summary>
Motivation: 降雨强度分布建模对水文风险分析和强度-持续时间-频率（IDF）曲线至关重要。现有方法大多关注季节性块最大值，缺乏能够描述完整分布且参数简约的模型。同时，不同时间尺度的降雨强度存在数学约束（如日累积必然超过亚日累积），需要确保重现水平不交叉。

Method: 使用扩展广义帕累托分布（EGPD）建模降雨强度，该分布符合极值理论（对两端尾部）同时对分布主体保持灵活性。通过将EGPD类与泊松复合和连接，利用Panjer算法进行高效复合似然评估，克服直接似然推断的困难。建立EGPD变量在各种聚合程序下行为的一般结果。

Result: 在法国六个不同气候站点上验证方法，仅需每个站点8个参数即可捕捉从6分钟到3天的时间尺度。提供了年尺度以上和以下的IDF曲线，确保不同尺度的重现水平不交叉，能够估计低于年或季节尺度的重现期。

Conclusion: 提出了一个简约且数学一致的框架，能够建模所有降雨强度（从低到高）在不同时间尺度上的分布。该方法克服了直接似然推断的困难，确保重现水平不交叉，为水文风险分析提供了实用的工具。

Abstract: Modeling rainfall intensity distributions across aggregation scales (from sub-hourly to weekly) is essential for hydrological risk analysis and IDF curves. Aggregation naturally imposes mathematical constraints: return levels must be ordered by time scale, as daily accumulations necessarily exceed sub-daily ones. From a statistical perspective, each aggregation step should ideally not require additional parameters, yet parsimonious models describing the full distribution remain scarce, as most literature focuses on seasonal block maxima.
  In this study, we propose a parsimonious framework to model all rainfall intensities (low to large) across scales. We utilize the Extended Generalized Pareto Distribution (EGPD), which aligns with extreme value theory for both tails while remaining flexible for the bulk of the distribution. We establish a general result on the behavior of EGPD variables under various aggregation procedures.
  To overcome the difficulty of direct likelihood inference, we link the EGPD class to Poisson compound sums. This allows the use of the Panjer algorithm for efficient composite likelihood evaluation. Our approach ensures that return levels do not cross across scales and enables estimation for return periods below annual or seasonal levels.
  We demonstrate the method using sub-hourly series from six French stations with diverse climates. Only eight parameters are needed per station to capture scales from six minutes to three days. IDF curves above and below the annual scale are provided.

</details>


### [3] [Reliability Modeling of Single-Sided Aluminized Polyimide Films during Storage Considering Stress-Induced Degradation Mechanism Transition](https://arxiv.org/abs/2601.08655)
*Shi-Shun Chen,Dong-Hua Niu,Wen-Bin Chen,Jia-Yun Song,Ya-Fei Zhang,Xiao-Yang Li,Enrico Zio*

Main category: stat.AP

TL;DR: 该论文研究了单面镀铝聚酰亚胺薄膜在储存环境中的可靠性，开发了考虑退化机制转变的新退化模型，并通过实验验证了其优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 单面镀铝聚酰亚胺薄膜在航空航天热管理中广泛应用，其空间环境可靠性已被充分研究，但储存环境中的可靠性常被忽视，可能导致系统故障。

Method: 首先识别SAPF性能退化与铝腐蚀的关系；考虑温度对铝腐蚀影响的两个不同阶段，开发考虑退化机制转变的新退化模型；提出基于实验数据确定SAPF退化机制的参数分析方法；采用改进的rime优化算法进行参数估计并建立可靠性模型。

Result: 实验结果表明，该方法能有效识别温度对SAPF性能退化影响的两个不同阶段；提出的退化模型在退化预测精度、外推能力和鲁棒性方面优于传统退化机制不变的模型。

Conclusion: 提出的考虑退化机制转变的退化模型适合描述SAPF的退化模式，为储存环境中的可靠性评估提供了有效方法。

Abstract: Single-sided aluminized polyimide films (SAPF) are widely used in thermal management of aerospace systems. Although the reliability of SAPF in space environments has been thoroughly studied, its reliability in ground environments during storage is always ignored, potentially leading to system failure. This paper aims to investigate the reliability of SAPF in storage environments, focusing on the effects of temperature and relative humidity. Firstly, the relationship between the performance degradation of SAPF and aluminum corrosion is identified. Next, considering the presence of two distinct stages in the influence of temperature on aluminum corrosion, a novel degradation model accounting for the degradation mechanism transition is developed. Additionally, a parameter analysis method is proposed for determining SAPF degradation mechanism based on experimental data. Then, a statistical analysis method incorporating an improved rime optimization algorithm is employed for parameter estimation, and the reliability model is established. Experimental results demonstrate that the proposed method effectively identifies two distinct stages in the impact of temperature on SAPF performance degradation. Furthermore, the proposed degradation model outperforms traditional degradation models with unchanged degradation mechanism in terms of degradation prediction accuracy, extrapolation capability and robustness, indicating its suitability for describing the degradation pattern of SAPFs.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [4] [Particle Filtering for a Class of State-Space Models with Low and Degenerate Observational Noise](https://arxiv.org/abs/2601.08411)
*Abylay Zhumekenov,Alexandros Beskos,Dan Crisan,Ajay Jasra,Nikolas Kantas*

Main category: stat.CO

TL;DR: 提出针对低噪声或退化观测噪声场景的新粒子滤波算法，确保在噪声退化时算法能继承退化情况下的性质，并扩展到扩散过程，开发对低噪声和精细时间离散化都鲁棒的算法。


<details>
  <summary>Details</summary>
Motivation: 解决离散时间滤波问题中观测噪声退化或低噪声的情况，特别是观测方程是状态的线性函数且加性噪声低或退化的场景，同时对隐状态过程做最小假设。

Method: 推导新的粒子滤波算法，确保在噪声退化时，近似低噪声滤波问题的PF能继承退化情况下PF的性质；扩展到扩散过程的隐状态，开发对低噪声和精细时间离散化都鲁棒的PF。

Result: 开发了新的粒子滤波算法，在多个示例上进行了数值验证，展示了算法在低噪声和退化噪声场景下的有效性。

Conclusion: 提出的粒子滤波算法能有效处理观测噪声退化或低噪声的滤波问题，并在扩散过程场景下保持对低噪声和精细时间离散化的鲁棒性。

Abstract: We consider the discrete-time filtering problem in scenarios where the observation noise is degenerate or low. We focus on the case where the observation equation is a linear function of the state and that additive noise is low or degenerate, however, we place minimal assumptions on the hidden state process. In this scenario we derive new particle filtering (PF) algorithms and, under assumptions, in such a way that as the noise becomes more degenerate a PF which approximates the low noise filtering problem provably inherits the properties of the PF used in the degenerate case. We extend our framework to the case where the hidden states are drawn from a diffusion process. In this scenario we develop new PFs which are robust to both low noise and fine levels of time discretization. We illustrate our algorithms numerically on several examples.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [5] [Decentralized Online Convex Optimization with Unknown Feedback Delays](https://arxiv.org/abs/2601.07901)
*Hao Qiu,Mengxiao Zhang,Juliette Achddou*

Main category: stat.ML

TL;DR: 提出了一种用于具有未知时变反馈延迟的分散式在线凸优化新算法，通过自适应学习率和去中心化通信协议，无需总延迟先验知识，实现了改进的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: 现有D-OCO算法需要总延迟的先验知识，且在延迟和网络参数上的依赖关系不够优化。本文旨在克服这些限制，处理未知、时变且随代理变化的反馈延迟问题。

Method: 基于D-OCO最新进展，引入自适应学习率机制和去中心化通信协议，通过基于gossip的策略让每个代理本地估计延迟，无需总延迟先验知识。算法扩展到强凸设置。

Result: 在一般凸设置下获得O(N√d_tot + N√T/(1-σ²)^{1/4})的遗憾边界，在强凸设置下获得O(Nδ_max ln T/α)的更优边界。实验验证了算法优于现有基准算法。

Conclusion: 提出的算法在无需总延迟先验知识的情况下，实现了对延迟和网络参数依赖关系的改进，遗憾边界在两种设置下都达到对数因子内的紧致性，实验验证了有效性。

Abstract: Decentralized online convex optimization (D-OCO), where multiple agents within a network collaboratively learn optimal decisions in real-time, arises naturally in applications such as federated learning, sensor networks, and multi-agent control.  In this paper, we study D-OCO under unknown, time-and agent-varying feedback delays. While recent work has addressed this problem (Nguyen et al., 2024), existing algorithms assume prior knowledge of the total delay over agents and still suffer from suboptimal dependence on both the delay and network parameters. To overcome these limitations, we propose a novel algorithm that achieves an improved regret bound of O N $\sqrt$ d tot + N $\sqrt$ T  (1-$σ$2) 1/4 , where T is the total horizon, d tot denotes the average total delay across agents, N is the number of agents, and 1 -$σ$ 2 is the spectral gap of the network. Our approach builds upon recent advances in D-OCO (Wan et al., 2024a), but crucially incorporates an adaptive learning rate mechanism via a decentralized communication protocol. This enables each agent to estimate delays locally using a gossip-based strategy without the prior knowledge of the total delay. We further extend our framework to the strongly convex setting and derive a sharper regret bound of O N $δ$max ln T $α$  , where $α$ is the strong convexity parameter and $δ$ max is the maximum number of missing observations averaged over agents. We also show that our upper bounds for both settings are tight up to logarithmic factors. Experimental results validate the effectiveness of our approach, showing improvements over existing benchmark algorithms.

</details>


### [6] [A Statistical Assessment of Amortized Inference Under Signal-to-Noise Variation and Distribution Shift](https://arxiv.org/abs/2601.07944)
*Roy Shivam Ram Shreshtth,Arnab Hazra,Gourab Mukherjee*

Main category: stat.ML

TL;DR: 该论文探讨了摊销贝叶斯推断的统计原理，分析了神经网络架构如何自然支持摊销推断，并通过模拟研究评估其准确性、鲁棒性和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络和基础模型的成功，摊销推断作为一种新的统计建模范式出现，但其统计解释和在贝叶斯推断中的作用仍不明确。论文旨在从统计角度理解摊销推断的工作原理。

Method: 论文从统计角度分析前馈网络、Deep Sets和Transformer等主要神经网络架构，探讨它们如何自然支持摊销贝叶斯推断。通过模拟研究评估在不同信噪比和分布偏移下的性能。

Result: 研究发现这些神经网络架构能够进行结构化近似和概率推理，从而在广泛部署场景中实现可控的泛化误差。模拟研究揭示了摊销推断在准确性、鲁棒性和不确定性量化方面的优势和局限性。

Conclusion: 摊销贝叶斯推断通过神经网络架构实现了高效的后验近似和预测，虽然具有计算优势，但在统计理解和实际应用中仍存在需要进一步研究的局限性。

Abstract: Since the turn of the century, approximate Bayesian inference has steadily evolved as new computational techniques have been incorporated to handle increasingly complex and large-scale predictive problems. The recent success of deep neural networks and foundation models has now given rise to a new paradigm in statistical modeling, in which Bayesian inference can be amortized through large-scale learned predictors. In amortized inference, substantial computation is invested upfront to train a neural network that can subsequently produce approximate posterior or predictions at negligible marginal cost across a wide range of tasks. At deployment, amortized inference offers substantial computational savings compared with traditional Bayesian procedures, which generally require repeated likelihood evaluations or Monte Carlo simulations for predictions for each new dataset.
  Despite the growing popularity of amortized inference, its statistical interpretation and its role within Bayesian inference remain poorly understood. This paper presents statistical perspectives on the working principles of several major neural architectures, including feedforward networks, Deep Sets, and Transformers, and examines how these architectures naturally support amortized Bayesian inference. We discuss how these models perform structured approximation and probabilistic reasoning in ways that yield controlled generalization error across a wide range of deployment scenarios, and how these properties can be harnessed for Bayesian computation. Through simulation studies, we evaluate the accuracy, robustness, and uncertainty quantification of amortized inference under varying signal-to-noise ratios and distributional shifts, highlighting both its strengths and its limitations.

</details>


### [7] [Towards A Unified PAC-Bayesian Framework for Norm-based Generalization Bounds](https://arxiv.org/abs/2601.08100)
*Xinping Yi,Gaojie Jin,Xiaowei Huang,Shi Jin*

Main category: stat.ML

TL;DR: 提出一个统一的PAC-Bayesian框架，通过各向异性高斯后验和敏感性矩阵来推导更紧致的深度神经网络泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有PAC-Bayesian范数界大多使用各向同性高斯后验、依赖谱范数集中性，且缺乏对网络架构的考虑，导致界限不够紧致且实用性有限。

Method: 将泛化界推导重构为各向异性高斯后验上的随机优化问题，引入敏感性矩阵量化网络输出对结构化权重扰动的响应，从而显式地纳入参数敏感性和架构结构。

Result: 通过敏感性矩阵的不同结构假设，推导出一族泛化界，这些界既包含现有PAC-Bayesian结果作为特例，又比当前最优方法更紧致或相当。

Conclusion: 该统一框架为深度学习提供了原则性、灵活且可解释的几何/结构感知泛化分析方法。

Abstract: Understanding the generalization behavior of deep neural networks remains a fundamental challenge in modern statistical learning theory. Among existing approaches, PAC-Bayesian norm-based bounds have demonstrated particular promise due to their data-dependent nature and their ability to capture algorithmic and geometric properties of learned models. However, most existing results rely on isotropic Gaussian posteriors, heavy use of spectral-norm concentration for weight perturbations, and largely architecture-agnostic analyses, which together limit both the tightness and practical relevance of the resulting bounds. To address these limitations, in this work, we propose a unified framework for PAC-Bayesian norm-based generalization by reformulating the derivation of generalization bounds as a stochastic optimization problem over anisotropic Gaussian posteriors. The key to our approach is a sensitivity matrix that quantifies the network outputs with respect to structured weight perturbations, enabling the explicit incorporation of heterogeneous parameter sensitivities and architectural structures. By imposing different structural assumptions on this sensitivity matrix, we derive a family of generalization bounds that recover several existing PAC-Bayesian results as special cases, while yielding bounds that are comparable to or tighter than state-of-the-art approaches. Such a unified framework provides a principled and flexible way for geometry-/structure-aware and interpretable generalization analysis in deep learning.

</details>


### [8] [Structural Dimension Reduction in Bayesian Networks](https://arxiv.org/abs/2601.08236)
*Pei Heng,Yi Sun,Jianhua Guo*

Main category: stat.ML

TL;DR: 提出一种名为"结构降维"的新技术，可将贝叶斯网络压缩为最小局部化网络，同时保持概率推理的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络推理方法如变量消除和信念传播算法效率有限，需要一种能保持推理一致性的网络压缩技术来提高推理效率。

Method: 提出有向无环图中的新组合结构"有向凸包"，证明其等价于最小局部化贝叶斯网络，并设计多项式时间算法通过确定包含感兴趣变量的唯一有向凸包来识别这些网络。

Result: 实验表明该技术在真实网络中具有高维度缩减能力，基于有向凸包的概率推理效率相比传统方法（变量消除和信念传播算法）显著提升。

Conclusion: 结构降维技术通过有向凸包实现了贝叶斯网络的有效压缩，在保持推理一致性的同时大幅提高了推理效率，为贝叶斯网络推理提供了新方法。

Abstract: This work introduces a novel technique, named structural dimension reduction, to collapse a Bayesian network onto a minimum and localized one while ensuring that probabilistic inferences between the original and reduced networks remain consistent. To this end, we propose a new combinatorial structure in directed acyclic graphs called the directed convex hull, which has turned out to be equivalent to their minimum localized Bayesian networks. An efficient polynomial-time algorithm is devised to identify them by determining the unique directed convex hulls containing the variables of interest from the original networks. Experiments demonstrate that the proposed technique has high dimension reduction capability in real networks, and the efficiency of probabilistic inference based on directed convex hulls can be significantly improved compared with traditional methods such as variable elimination and belief propagation algorithms. The code of this study is open at \href{https://github.com/Balance-H/Algorithms}{https://github.com/Balance-H/Algorithms} and the proofs of the results in the main body are postponed to the appendix.

</details>


### [9] [Robust low-rank estimation with multiple binary responses using pairwise AUC loss](https://arxiv.org/abs/2601.08618)
*The Tien Mai*

Main category: stat.ML

TL;DR: 提出一个针对多二元响应变量的统一框架，通过最小化AUC代理损失直接优化排序性能，同时利用低秩约束共享结构


<details>
  <summary>Details</summary>
Motivation: 现有方法对多二元响应变量要么采用独立逻辑回归忽略共享结构，要么基于似然方法关注点分类而非排序性能。需要一种能同时利用任务间依赖关系并直接优化排序性能的方法

Method: 提出统一框架，聚合跨响应的成对AUC代理损失，对系数矩阵施加低秩约束以利用共享结构。开发基于截断奇异值分解的可扩展投影梯度下降算法

Result: 建立了非渐近收敛保证，在适当正则条件下达到线性收敛至极小极大最优统计精度。模拟研究表明方法在标签切换和数据污染等挑战性设置中稳健，且持续优于基于似然的方法

Conclusion: 该方法通过直接优化AUC并利用低秩结构，为多二元响应问题提供了统计高效且计算可扩展的解决方案，特别适用于高维和类别不平衡场景

Abstract: Multiple binary responses arise in many modern data-analytic problems. Although fitting separate logistic regressions for each response is computationally attractive, it ignores shared structure and can be statistically inefficient, especially in high-dimensional and class-imbalanced regimes. Low-rank models offer a natural way to encode latent dependence across tasks, but existing methods for binary data are largely likelihood-based and focus on pointwise classification rather than ranking performance. In this work, we propose a unified framework for learning with multiple binary responses that directly targets discrimination by minimizing a surrogate loss for the area under the ROC curve (AUC). The method aggregates pairwise AUC surrogate losses across responses while imposing a low-rank constraint on the coefficient matrix to exploit shared structure. We develop a scalable projected gradient descent algorithm based on truncated singular value decomposition. Exploiting the fact that the pairwise loss depends only on differences of linear predictors, we simplify computation and analysis. We establish non-asymptotic convergence guarantees, showing that under suitable regularity conditions, leading to linear convergence up to the minimax-optimal statistical precision. Extensive simulation studies demonstrate that the proposed method is robust in challenging settings such as label switching and data contamination and consistently outperforms likelihood-based approaches.

</details>


### [10] [On the use of graph models to achieve individual and group fairness](https://arxiv.org/abs/2601.08784)
*Arturo Pérez-Peralta,Sandra Benítez-Peña,Rosa E. Lillo*

Main category: stat.ML

TL;DR: 该论文提出了一种基于层扩散的理论框架，通过将数据投影到无偏空间来处理机器学习中的公平性问题，统一处理个体和群体偏见，并提供可解释的SHAP值。


<details>
  <summary>Details</summary>
Motivation: 机器学习在司法、医疗和金融等关键决策场景中广泛应用，但对这些模型的公平性理论性质理解不足，个体公平性与群体公平性之间的关系缺乏直觉理解。

Method: 基于层扩散理论框架，利用动力系统和同调学工具建模公平性。将输入数据投影到编码公平约束的无偏空间，提出处理不同公平性度量的网络拓扑结构，统一处理个体和群体偏见。

Result: 在模拟研究和标准公平性基准测试中取得满意结果。展示了模型在准确性和公平性方面的性能，研究了帕累托前沿的权衡，检查了不同超参数的影响，并深入分析了输出的可解释性。

Conclusion: 提出的方法为机器学习公平性提供了理论框架和实用工具，通过层扩散技术实现公平约束编码，提供可解释的SHAP值，在负责任的人工智能领域具有重要价值。

Abstract: Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [11] [Permutation Inference under Multi-way Clustering and Missing Data](https://arxiv.org/abs/2601.08610)
*Wenxuan Guo,Panos Toulis,Yuhao Wang*

Main category: stat.ME

TL;DR: 提出有限样本有效的多向聚类线性回归置换检验方法，相比传统聚类稳健和自助法在有限样本下更可靠


<details>
  <summary>Details</summary>
Motivation: 多向聚类应用中常面临有效聚类数少或数据重尾问题，导致标准聚类稳健和自助推断在有限样本下不可靠

Method: 基于条件可交换性假设，构建有限样本有效的置换检验框架，扩展到缺失数据场景并分析可检测性的相变

Result: 模拟研究表明所提检验能保持正确规模且有竞争力功效，而标准方法可能出现显著规模扭曲

Conclusion: 置换检验为多向聚类回归提供有限样本有效的推断工具，特别适用于小有效聚类数或重尾数据场景

Abstract: Econometric applications with multi-way clustering often feature a small number of effective clusters or heavy-tailed data, making standard cluster-robust and bootstrap inference unreliable in finite samples. In this paper, we develop a framework for finite-sample valid permutation inference in linear regression with multi-way clustering under an assumption of conditional exchangeability of the errors. Our assumption is closely related to the notion of separate exchangeability studied in earlier work, but can be more realistic in many economic settings as it imposes minimal restrictions on the covariate distribution. We construct permutation tests of significance that are valid in finite samples and establish theoretical power guarantees, in contrast to existing methods that are justified only asymptotically. We also extend our methodology to settings with missing data and derive power results that reveal phase transitions in detectability. Through simulation studies, we demonstrate that the proposed tests maintain correct size and competitive power, while standard cluster-robust and bootstrap procedures can exhibit substantial size distortions.

</details>


### [12] [Semiparametric Efficient Data Integration Using the Dual-Frame Sampling Framework](https://arxiv.org/abs/2601.08707)
*Kosuke Morikawa,Jae Kwang Kim*

Main category: stat.ME

TL;DR: 提出两种互补的双框架数据整合方法：第一种通过参数化建模非概率样本的包含概率达到半参数效率界；第二种基于两阶段抽样近似，避免显式建模非概率机制，具有稳健性。


<details>
  <summary>Details</summary>
Motivation: 整合概率样本和非概率样本日益重要，但非概率来源中未知的抽样机制使识别和高效估计变得复杂。需要开发既能达到统计效率又具有稳健性的方法。

Method: 1. 参数化建模方法：对非概率样本的包含概率进行参数化建模，基于强单调性识别条件，无需工具变量即可识别抽样模型参数，即使存在信息性选择。2. 两阶段抽样近似方法：避免显式建模非概率机制，通过限制增强类实现效率，对模型误设具有稳健性。

Result: 模拟研究和实际应用（危机时期的文化与社区公共模拟数据集）显示：在正确设定下获得效率提升，在模型误设和弱识别情况下保持稳定性能。方法已在R包dfSEDI中实现。

Conclusion: 提出的两种互补估计器为双框架数据整合提供了有效解决方案：参数化方法达到半参数效率界，近似方法具有稳健性。强单调性识别条件无需记录链接即可识别抽样模型参数，为处理信息性选择提供了新途径。

Abstract: Integrating probability and non-probability samples is increasingly important, yet unknown sampling mechanisms in non-probability sources complicate identification and efficient estimation. We develop semiparametric theory for dual-frame data integration and propose two complementary estimators. The first models the non-probability inclusion probability parametrically and attains the semiparametric efficiency bound. We introduce an identifiability condition based on strong monotonicity that identifies sampling-model parameters without instrumental variables, even under informative (non-ignorable) selection, using auxiliary information from the probability sample; it remains valid without record linkage between samples. The second estimator, motivated by a two-stage sampling approximation, avoids explicit modeling of the non-probability mechanism; though not fully efficient, it is efficient within a restricted augmentation class and is robust to misspecification. Simulations and an application to the Culture and Community in a Time of Crisis public simulation dataset show efficiency gains under correct specification and stable performance under misspecification and weak identification. Methods are implemented in the R package \texttt{dfSEDI}.

</details>


### [13] [A Symmetric Random Scan Collapsed Gibbs Sampler for Fully Bayesian Variable Selection with Spike-and-Slab Priors](https://arxiv.org/abs/2601.07864)
*Mengta Chung*

Main category: stat.ME

TL;DR: 提出一种对称随机扫描Gibbs采样器用于可扩展的贝叶斯变量选择，通过实时计算避免存储完整叉积矩阵，使用数据驱动的提案权重提高效率，在10万预测变量模拟中达到1.000敏感度和>0.76精确度。


<details>
  <summary>Details</summary>
Motivation: 解决高维贝叶斯变量选择中的计算挑战，特别是避免存储大型叉积矩阵的内存问题，同时提高采样效率以处理大规模预测变量。

Method: 对称随机扫描Gibbs采样器，实时计算所需量避免存储叉积矩阵；使用基于边际相关性的数据驱动提案权重集中采样努力；结合均匀混合成分确保理论有效性；采用后验均值大小选择规则作为中位数概率模型的自适应替代。

Result: 在10万预测变量的模拟中，方法达到1.000的敏感度和高于0.76的精确度；在枯草芽孢杆菌核黄素生产的基因组数据应用中，识别出6个基因，均被先前研究验证。

Conclusion: 该方法为大规模贝叶斯变量选择提供了高效、可扩展的解决方案，结合Dirac spike-and-slab先验与Laplace型收缩，实现精确稀疏性和自适应正则化，在计算效率和统计性能方面表现优异。

Abstract: We introduce a symmetric random scan Gibbs sampler for scalable Bayesian variable selection that eliminates storage of the full cross-product matrix by computing required quantities on-the-fly. Data-informed proposal weights, constructed from marginal correlations, concentrate sampling effort on promising candidates while a uniform mixing component ensures theoretical validity. We provide explicit guidance for selecting tuning parameters based on the ratio of signal to null correlations, ensuring adequate posterior exploration. The posterior-mean-size selection rule provides an adaptive alternative to the median probability model that automatically calibrates to the effective signal density without requiring an arbitrary threshold. In simulations with one hundred thousand predictors, the method achieves sensitivity of 1.000 and precision above 0.76. Application to a genomic dataset studying riboflavin production in Bacillus subtilis identifies six genes, all validated by previous studies using alternative methods. The underlying model combines a Dirac spike-and-slab prior with Laplace-type shrinkage: the Dirac spike enforces exact sparsity by setting inactive coefficients to precisely zero, while the Laplace-type slab provides adaptive regularization for active coefficients through a local-global scale mixture.

</details>


### [14] [Joint Modeling of Two Stochastic Processes, with Application to Learning Hospitalization Dynamics from Wastewater Viral Concentrations](https://arxiv.org/abs/2601.07977)
*K. Ken Peng,Charmaine B. Dean,Robert Delatolla,X. Joan Hu,Elizabeth Renouf*

Main category: stat.ME

TL;DR: 该论文提出了一个统计框架，通过个体层面的潜在感染过程连接废水病毒信号与住院数据，解决了COVID-19监测中数据聚合和漏报的问题。


<details>
  <summary>Details</summary>
Motivation: 在COVID-19后疫情时代，住院治疗仍是主要公共卫生问题，废水监测已成为社区层面监测疫情动态的重要工具。然而，通常缺乏足够信息来了解导致废水病毒信号和住院数据的感染过程，这一关键挑战促使了本论文统计框架的提出。

Method: 通过个体层面的潜在感染过程建立废水病毒信号与住院数据的时间关联，提供了处理聚合监测数据的策略，并简化了传统统计学习过程，利用感染过程信息进行联合建模，能够处理漏报情况。

Result: 模拟研究表明，该方法在不同程度的漏报情况下都能产生稳定的推断。加拿大渥太华的COVID-19监测数据显示，该框架在多种报告假设下能够恢复感染流行率和变异特异性住院风险的一致时间模式。

Conclusion: 该统计框架成功解决了废水监测与住院数据之间的连接问题，能够处理数据聚合和漏报情况，为公共卫生监测提供了有效的分析工具。

Abstract: In the post-pandemic era of COVID-19, hospitalization remains a primary public health concern and wastewater surveillance has become an important tool for monitoring its dynamics at the level of community. However, there is usually no sufficient information to know the infection process that results in both wastewater viral signals and hospital admissions. That key challenge has motived a statistical framework proposed in this paper. We formulate the connection of overtime wastewater viral signals and hospitalization counts through a latent process of infection at the level of individual subject. We provide a strategy for accommodating aggregated data, a typical form of surveillance data. Moreover, we ease the conventional procedure of the statistical learning with the joint modeling using available information on the infection process, which can be under-reporting. A simulation study demonstrates that the proposed approach yields stable inference under different degrees of under-ascertainment. The COVID-19 surveillance data from Ottawa, Canada shows that the framework recovers coherent temporal patterns in infection prevalence and variant-specific hospitalization risk under several reporting assumptions.

</details>


### [15] [Spatial Covariance Constraints for Gaussian Mixture Models](https://arxiv.org/abs/2601.07979)
*Hanzhang Lu,Keiran Malott,Venkat Suprabath Bitra,Kirsty Milligan,Sanjeena Subedi,Edana Cassol,Vinita Chauhan,Connor McNairn,Bryan Muir,Prarthana Pasricha,Sangeeta Murugkar,Rowan Thomson,Andrew Jirasek,Jeffrey L. Andrews*

Main category: stat.ME

TL;DR: 提出一种空间协方差约束的高斯混合模型，仅需每个分量4个自由参数，用于空间数据聚类和模式推断


<details>
  <summary>Details</summary>
Motivation: 空间建模研究广泛，但针对空间数据的有限混合模型聚类方法较少。高斯混合模型在高维情况下面临协方差参数过多的问题

Method: 引入空间协方差约束，每个分量只需4个自由参数，与维度无关。结合EM算法和广义最小二乘估计进行参数估计

Result: 通过模拟研究和拉曼光谱数据应用验证了所提模型的有效性

Conclusion: 提出的空间约束高斯混合模型能够有效处理多路空间数据聚类和空间模式推断问题

Abstract: Although extensive research exists in spatial modeling, few studies have addressed finite mixture model-based clustering methods for spatial data. Finite mixture models, especially Gaussian mixture models, particularly suffer from high dimensionality due to the number of free covariance parameters. This study introduces a spatial covariance constraint for Gaussian mixture models that requires only four free parameters for each component, independent of dimensionality. Using a coordinate system, the spatially constrained Gaussian mixture model enables clustering of multi-way spatial data and inference of spatial patterns. The parameter estimation is conducted by combining the expectation-maximization (EM) algorithm with the generalized least squares (GLS) estimator. Simulation studies and applications to Raman spectroscopy data are provided to demonstrate the proposed model.

</details>


### [16] [Modeling Event Dynamics by Self-Exciting Processes with Random Memory](https://arxiv.org/abs/2601.07980)
*K. Ken Peng,X. Joan Hu,Tim B. Swartz*

Main category: stat.ME

TL;DR: 提出了一种具有随机自激励持续时间的扩展Hawkes过程模型，用于分析体育赛事中的事件历史数据，特别是角球事件的时间动态。


<details>
  <summary>Details</summary>
Motivation: 体育竞赛中的事件历史数据（如角球）常表现出自激励效应和事件簇内的依赖性，传统基于间隔时间的模型难以捕捉这些特征，特别是自激励效应往往具有不确定的持续时间。

Method: 提出扩展的Hawkes过程模型，引入随机自激励持续时间参数，开发了模型参数估计方法，并设计了事件过程模拟算法。

Result: 使用2019年中超联赛常规赛角球数据验证模型，展示了模型在捕捉事件自激励动态方面的有效性，并提供了模拟算法。

Conclusion: 该扩展Hawkes过程模型能更好地建模体育事件中的自激励动态，方法可推广至犯罪学和传染病等其他研究领域。

Abstract: Event history data from sports competitions have recently drawn increasing attention in sports analytics to generate data-driven strategies. Such data often exhibit self-excitation in the event occurrence and dependence within event clusters. The conventional event models based on gap times may struggle to capture those features. In particular, while consecutive events may occur within a short timeframe, the self-excitation effect caused by previous events is often transient and continues for a period of uncertain time. This paper introduces an extended Hawkes process model with random self-excitation duration to formulate the dynamics of event occurrence. We present examples of the proposed model and procedures for estimating the associated model parameters. We employ the collection of the corner kicks in the games of the 2019 regular season of the Chinese Super League to motivate and illustrate the modeling and its usefulness. We also design algorithms for simulating the event process under proposed models. The proposed approach can be adapted with little modification in many other research fields such as Criminology and Infectious Disease.

</details>


### [17] [Bayesian nonparametric models for zero-inflated count-compositional data using ensembles of regression trees](https://arxiv.org/abs/2601.08067)
*André F. B. Menezes,Andrew C. Parnell,Keefe Murphy*

Main category: stat.ME

TL;DR: 提出基于贝叶斯回归树集成的新模型，用于处理计数组合数据中的过离散、零值过多、样本异质性和非线性协变量效应等问题


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理计数组合数据中的关键挑战：过离散、零值过多、跨样本异质性和非线性协变量效应

Method: 使用零和N膨胀多项分布，为模型的组合成分和结构零概率分量分配独立的非参数贝叶斯加性回归树先验，并添加潜在随机效应来捕获过离散和类别间的依赖结构

Result: 开发了高效的推断算法，结合数据增强方案和BART采样程序，在模拟研究和微生物组与古气候建模案例中验证了模型的有效性

Conclusion: 提出的贝叶斯回归树集成模型能够灵活处理计数组合数据中的复杂挑战，在微生物组和古气候建模等领域具有实际应用价值

Abstract: Count-compositional data arise in many different fields, including high-throughput microbiome sequencing and palynology experiments, where a common, important goal is to understand how covariates relate to the observed compositions. Existing methods often fail to simultaneously address key challenges inherent in such data, namely: overdispersion, an excess of zeros, cross-sample heterogeneity, and nonlinear covariate effects. To address these concerns, we propose novel Bayesian models based on ensembles of regression trees. Specifically, we leverage the recently introduced zero-and-$N$-inflated multinomial distribution and assign independent nonparametric Bayesian additive regression tree (BART) priors to both the compositional and structural zero probability components of our model, to flexibly capture covariate effects. We further extend this by adding latent random effects to capture overdispersion and more general dependence structures among the categories. We develop an efficient inferential algorithm combining recent data augmentation schemes with established BART sampling routines. We evaluate our proposed models in simulation studies and illustrate their applicability with two case studies in microbiome and palaeoclimate modelling.

</details>


### [18] [REAMP: A Stochastic Resonance Approach for Multi-Change Point Detection in High-Dimensional Data](https://arxiv.org/abs/2601.08084)
*Xiaoping Shi,Baisuo Jin,Xianhui Liu,Qiong Li*

Main category: stat.ME

TL;DR: REAMP：结合最优传输理论和随机共振原理的高维数据多断点检测新框架，通过两阶段降维和共振机制提升检测性能


<details>
  <summary>Details</summary>
Motivation: 高维数据中的多结构断点检测面临挑战，特别是在高阶矩变化或复杂流形结构中，传统方法存在局限性

Method: 1）使用地球移动距离和最短哈密顿路径进行两阶段降维；2）基于随机共振系统利用随机Beta密度先验振动目标函数；3）通过双锐化程序精确定位断点

Result: 建立了共振估计量的渐近一致性，模拟显示REAMP优于现有方法，特别是在均值和方差同时变化的情况下

Conclusion: REAMP为高维多断点检测提供了有效框架，在胚胎时间序列监测中展示了实用价值，实现了准确检测和直观可视化

Abstract: Detecting multiple structural breaks in high-dimensional data remains a challenge, particularly when changes occur in higher-order moments or within complex manifold structures. In this paper, we propose REAMP (Resonance-Enhanced Analysis of Multi-change Points), a novel framework that integrates optimal transport theory with the physical principles of stochastic resonance. By utilizing a two-stage dimension reduction via the Earth Movers Distance (EMD) and Shortest Hamiltonian Paths (SHP), we map high-dimensional observations onto a graph-based count statistic. To overcome the locality constraints of traditional search algorithms, we implement a stochastic resonance system that utilizes randomized Beta-density priors to vibrate the objective function. This process allows multiple change points to resonate as global minima across iterative simulations, generating a candidate point cloud. A double-sharpening procedure is then applied to these candidates to pinpoint precise change point locations. We establish the asymptotic consistency of the resonance estimator and demonstrate through simulations that REAMP outperforms state-of-the-art methods, especially in scenarios involving simultaneous mean and variance shifts. The practical utility of the method is further validated through an application to time-lapse embryo monitoring, where REAMP provides both accurate detection and intuitive visualization of cell division stages.

</details>


### [19] [Sparsifying transform priors in Gaussian graphical models](https://arxiv.org/abs/2601.08596)
*Marcus Gehrmann,Håkon Tjelmeland*

Main category: stat.ME

TL;DR: 提出新的ST先验分布用于高斯图模型，解决G-Wishart分布中归一化常数难以处理导致的MCMC双难问题


<details>
  <summary>Details</summary>
Motivation: 贝叶斯方法在估计高斯图模型条件独立性结构时很受欢迎，但常用的G-Wishart先验分布存在难以处理的归一化常数，导致MCMC算法出现双难问题

Method: 提出ST先验分布类，通过对来自所有正定矩阵集合中分布的矩阵应用稀疏化变换获得，构建无双难问题的MCMC算法

Result: 在人类基因表达数据集上的数值实验表明，提出的MCMC算法能够收敛并获得可接受的混合效果

Conclusion: ST先验分布为高斯图模型的贝叶斯推断提供了有效的替代方案，解决了G-Wishart分布的双难问题，在实际数据应用中表现良好

Abstract: Bayesian methods constitute a popular approach for estimating the conditional independence structure in Gaussian graphical models, since they can quantify the uncertainty through the posterior distribution. Inference in this framework is typically carried out with Markov chain Monte Carlo (MCMC). However, the most widely used choice of prior distribution for the precision matrix, the so called G-Wishart distribution, suffers from an intractable normalizing constant, which gives rise to the problem of double intractability in the updating steps of the MCMC algorithm. In this article, we propose a new class of prior distributions for the precision matrix, termed ST priors, that allow for the construction of MCMC algorithms that do not suffer from double intractability issues. A realization from an ST prior distribution is obtained by applying a sparsifying transform on a matrix from a distribution with support in the set of all positive definite matrices. We carefully present the theory behind the construction of our proposed class of priors and also perform some numerical experiments, where we apply our methods on a human gene expression dataset. The results suggest that our proposed MCMC algorithm is able to converge and achieve acceptable mixing when applied on the real data.

</details>


### [20] [Flexible modeling of nonnegative continuous data: Box-Cox symmetric regression and its zero-adjusted extension](https://arxiv.org/abs/2601.08600)
*Rodrigo M. R. de Medeiros,Francisco F. Queiroz*

Main category: stat.ME

TL;DR: 本文正式化Box-Cox对称回归模型类，并引入新的零调整扩展，用于处理含零值的正连续数据，开发了相应的R包实现。


<details>
  <summary>Details</summary>
Motivation: Box-Cox对称分布为建模正连续数据提供了灵活框架，但现有研究仅探索了少数特定分布，且缺乏对含零值数据的正式处理方法。

Method: 正式化Box-Cox对称回归模型类，提出零调整扩展，讨论最大似然估计，通过模拟评估有限样本性能，开发诊断工具（残差分析、局部影响度量、拟合优度统计）。

Result: 开发了BCSreg R包实现所有方法，通过基础教育支出数据实证应用展示了模型处理零膨胀和高度偏斜非负数据的能力。

Conclusion: Box-Cox对称回归模型及其零调整扩展为建模含零值的正连续数据提供了有效框架，配套的R包支持实际应用。

Abstract: The Box-Cox symmetric distributions constitute a broad class of probability models for positive continuous data, offering flexibility in modeling skewness and tail behavior. Their parameterization allows a straightforward quantile-based interpretation, which is particularly useful in regression modeling. Despite their potential, only a few specific distributions within this class have been explored in regression contexts, and zero-adjusted extensions have not yet been formally addressed in the literature. This paper formalizes the class of Box-Cox symmetric regression models and introduces a new zero-adjusted extension suitable for modeling data with a non-negligible proportion of observations equal to zero. We discuss maximum likelihood estimation, assess finite-sample performance through simulations, and develop diagnostic tools including residual analysis, local influence measures, and goodness-of-fit statistics. An empirical application on basic education expenditure illustrates the models' ability to capture complex patterns in zero-inflated and highly skewed nonnegative data. To support practical use, we developed the new BCSreg R package, which implements all proposed methods.

</details>


### [21] [Note on High Dimensional Spatial-Sign Test for One Sample Problem](https://arxiv.org/abs/2601.08736)
*Ping Zhao,Long Feng*

Main category: stat.ME

TL;DR: 重新研究高维空间符号检验的零分布，发现标准化检验统计量收敛于非高斯极限（正态与加权卡方混合），提出wild bootstrap方法计算临界值并证明其渐近有效性。


<details>
  <summary>Details</summary>
Motivation: Wang等人（2015）提出的高维空间符号检验在温和的结构假设下，其零分布需要重新审视。现有理论可能不准确，需要更精确的分布特征和实用的实现方法。

Method: 在温和的散度矩阵结构假设下，推导标准化检验统计量的渐近分布，提出wild bootstrap程序计算临界值，并建立其渐近有效性理论。

Result: 标准化检验统计量收敛于非高斯极限，表现为正态分量和加权卡方分量的混合。提出的bootstrap检验在各种依赖设置和维度-样本量情况下都能提供准确的尺寸控制。

Conclusion: 高维空间符号检验的零分布比传统假设更复杂，wild bootstrap方法能有效处理这种复杂性，为实际应用提供可靠的检验工具。

Abstract: We revisit the null distribution of the high-dimensional spatial-sign test of Wang et al. (2015) under mild structural assumptions on the scatter matrix. We show that the standardized test statistic converges to a non-Gaussian limit, characterized as a mixture of a normal component and a weighted chi-square component. To facilitate practical implementation, we propose a wild bootstrap procedure for computing critical values and establish its asymptotic validity. Numerical experiments demonstrate that the proposed bootstrap test delivers accurate size control across a wide range of dependence settings and dimension-sample-size regimes.

</details>
