{"id": "2512.11945", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.11945", "abs": "https://arxiv.org/abs/2512.11945", "authors": ["Diogo Pinheiro", "M. Rosário Oliveira", "Igor Kravchenko", "Lina Oliveira"], "title": "Interval Fisher's Discriminant Analysis and Visualisation", "comment": null, "summary": "In Data Science, entities are typically represented by single valued measurements. Symbolic Data Analysis extends this framework to more complex structures, such as intervals and histograms, that express internal variability. We propose an extension of multiclass Fisher's Discriminant Analysis to interval-valued data, using Moore's interval arithmetic and the Mallows' distance. Fisher's objective function is generalised to consider simultaneously the contributions of the centres and the ranges of intervals and is numerically maximised. The resulting discriminant directions are then used to classify interval-valued observations.To support visual assessment, we adapt the class map, originally introduced for conventional data, to classifiers that assign labels through minimum distance rules. We also extend the silhouette plot to this setting and use stacked mosaic plots to complement the visual display of class assignments. Together, these graphical tools provide insight into classifier performance and the strength of class membership. Applications to real datasets illustrate the proposed methodology and demonstrate its value in interpreting classification results for interval-valued data."}
{"id": "2512.12267", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12267", "abs": "https://arxiv.org/abs/2512.12267", "authors": ["Giovanni Saraceno", "Anand N. Vidyashankar", "Claudio Agostinelli"], "title": "Hellinger loss function for Generative Adversarial Networks", "comment": "25 pages and Supplemental Material", "summary": "We propose Hellinger-type loss functions for training Generative Adversarial Networks (GANs), motivated by the boundedness, symmetry, and robustness properties of the Hellinger distance. We define an adversarial objective based on this divergence and study its statistical properties within a general parametric framework. We establish the existence, uniqueness, consistency, and joint asymptotic normality of the estimators obtained from the adversarial training procedure. In particular, we analyze the joint estimation of both generator and discriminator parameters, offering a comprehensive asymptotic characterization of the resulting estimators. We introduce two implementations of the Hellinger-type loss and we evaluate their empirical behavior in comparison with the classic (Maximum Likelihood-type) GAN loss. Through a controlled simulation study, we demonstrate that both proposed losses yield improved estimation accuracy and robustness under increasing levels of data contamination."}
{"id": "2512.12358", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12358", "abs": "https://arxiv.org/abs/2512.12358", "authors": ["Stéphanie M. van den Berg", "Ulrich Halekoh", "Sören Möller", "Andreas Kryger Jensen", "Jacob von Bornemann Hjelmborg"], "title": "Towards a pretrained deep learning estimator of the Linfoot informational correlation", "comment": "3 figures", "summary": "We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available."}
{"id": "2512.12435", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12435", "abs": "https://arxiv.org/abs/2512.12435", "authors": ["Bisakh Banerjee", "Mohammad Alwardat", "Tapabrata Maiti", "Selin Aviyente"], "title": "Co-Hub Node Based Multiview Graph Learning with Theoretical Guarantees", "comment": null, "summary": "Identifying the graphical structure underlying the observed multivariate data is essential in numerous applications. Current methodologies are predominantly confined to deducing a singular graph under the presumption that the observed data are uniform. However, many contexts involve heterogeneous datasets that feature multiple closely related graphs, typically referred to as multiview graphs. Previous research on multiview graph learning promotes edge-based similarity across layers using pairwise or consensus-based regularizers. However, multiview graphs frequently exhibit a shared node-based architecture across different views, such as common hub nodes. Such commonalities can enhance the precision of learning and provide interpretive insight. In this paper, we propose a co-hub node model, positing that different views share a common group of hub nodes. The associated optimization framework is developed by enforcing structured sparsity on the connections of these co-hub nodes. Moreover, we present a theoretical examination of layer identifiability and determine bounds on estimation error. The proposed methodology is validated using both synthetic graph data and fMRI time series data from multiple subjects to discern several closely related graphs."}
{"id": "2512.13354", "categories": ["stat.OT", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13354", "abs": "https://arxiv.org/abs/2512.13354", "authors": ["Geremy Loachamín", "Eleni D. Koronaki", "Dimitrios G. Giovanis", "Martin Kathrein", "Christoph Czettl", "Andreas G. Boudouvis", "Stéphane P. A. Bordas"], "title": "Data-driven inverse uncertainty quantification: application to the Chemical Vapor Deposition Reactor Modeling", "comment": null, "summary": "This study presents a Bayesian framework for (inverse) uncertainty quantification and parameter estimation in a two-step Chemical Vapor Deposition coating process using production data. We develop an XGBoost surrogate model that maps reactor setup parameters to coating thickness measurements, enabling efficient Bayesian analysis while reducing sampling costs. The methodology handles a mixture of data including continuous, discrete integer, binary, and encoded categorical variables. We establish parameter prior distributions through Bayesian Model Selection and perform Inverse Uncertainty Quantification via weighted Approximate Bayesian Computation with summary statistics, providing robust parameter credible intervals while filtering measurement noise across multiple reactor locations. Furthermore, we employ clustering methods guided by geometry embeddings to focus analysis within homogeneous production groups. This integrated approach provides a validated tool for improving industrial process control under uncertainty."}
{"id": "2512.11919", "categories": ["stat.ME", "cs.AI", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.11919", "abs": "https://arxiv.org/abs/2512.11919", "authors": ["Junhyung Park", "Yuqing Zhou"], "title": "A fine-grained look at causal effects in causal spaces", "comment": null, "summary": "The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases."}
{"id": "2512.12051", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2512.12051", "abs": "https://arxiv.org/abs/2512.12051", "authors": ["Andrew Herren", "P. Richard Hahn", "Jared Murray", "Carlos Carvalho"], "title": "StochTree: BART-based modeling in R and Python", "comment": null, "summary": "stochtree is a C++ library for Bayesian tree ensemble models such as BART and Bayesian Causal Forests (BCF), as well as user-specified variations. Unlike previous BART packages, stochtree provides bindings to both R and Python for full interoperability. stochtree boasts a more comprehensive range of models relative to previous packages, including heteroskedastic forests, random effects, and treed linear models. Additionally, stochtree offers flexible handling of model fits: the ability to save model fits, reinitialize models from existing fits (facilitating improved model initialization heuristics), and pass fits between R and Python. On both platforms, stochtree exposes lower-level functionality, allowing users to specify models incorporating Bayesian tree ensembles without needing to modify C++ code. We illustrate the use of stochtree in three settings: i) straightfoward applications of existing models such as BART and BCF, ii) models that include more sophisticated components like heteroskedasticity and leaf-wise regression models, and iii) as a component of custom MCMC routines to fit nonstandard tree ensemble models."}
{"id": "2512.12043", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12043", "abs": "https://arxiv.org/abs/2512.12043", "authors": ["Yi Zhao", "Chengyun Li", "Wanzhu Tu"], "title": "Estimation of Heterogeneous Causal Mediation Effects in a Hypertension Treatment Trial", "comment": null, "summary": "Hypertension is a highly prevalent condition and a major risk factor for cardiovascular disease. The landmark Systolic Blood Pressure Intervention Trial (SPRINT) showed that lowering systolic blood pressure (BP) goals from 140 mmHg to 120 mmHg leads to significantly reduced BP, cardiovascular mortality, and morbidity. However, the underlying mechanisms are not yet fully elucidated. In patients with impaired renal function, early reduction of albuminuria has been proposed as a potential mediation pathway. Evidence from the standard causal mediation analysis (CMA), however, yields inconsistent results, possibly due to heterogeneous mediation effects across individuals. To disseminate the heterogeneity, a new framework that incorporates covariate-treatment and mediator-treatment interactions within a linear structural equation modeling system is introduced. Causal assumptions are discussed and heterogeneous natural direct and indirect effects are parameterized as functions of patient characteristics. A modified covariate approach is proposed to relax the hierarchical constraints and the generalized lasso regularization is employed to ensure parsimony in high-dimensional settings. Asymptotic properties are studied. Simulation studies demonstrate good estimation and inference performance. Analysis of the SPRINT data reveals substantial heterogeneity in mediation effects, identifying a subset of patients who stand to gain from therapies targeting albuminuria."}
{"id": "2512.11919", "categories": ["stat.ME", "cs.AI", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.11919", "abs": "https://arxiv.org/abs/2512.11919", "authors": ["Junhyung Park", "Yuqing Zhou"], "title": "A fine-grained look at causal effects in causal spaces", "comment": null, "summary": "The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases."}
{"id": "2512.12442", "categories": ["stat.ML", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12442", "abs": "https://arxiv.org/abs/2512.12442", "authors": ["Haoyu Li", "Isaac J Michaud", "Ayan Biswas", "Han-Wei Shen"], "title": "Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data", "comment": "IEEE PacificVis 2024", "summary": "Almost all scientific data have uncertainties originating from different sources. Gaussian process regression (GPR) models are a natural way to model data with Gaussian-distributed uncertainties. GPR also has the benefit of reducing I/O bandwidth and storage requirements for large scientific simulations. However, the reconstruction from the GPR models suffers from high computation complexity. To make the situation worse, classic approaches for visualizing the data uncertainties, like probabilistic marching cubes, are also computationally very expensive, especially for data of high resolutions. In this paper, we accelerate the level-crossing probability calculation efficiency on GPR models by subdividing the data spatially into a hierarchical data structure and only reconstructing values adaptively in the regions that have a non-zero probability. For each region, leveraging the known GPR kernel and the saved data observations, we propose a novel approach to efficiently calculate an upper bound for the level-crossing probability inside the region and use this upper bound to make the subdivision and reconstruction decisions. We demonstrate that our value occurrence probability estimation is accurate with a low computation cost by experiments that calculate the level-crossing probability fields on different datasets."}
{"id": "2512.12579", "categories": ["stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.12579", "abs": "https://arxiv.org/abs/2512.12579", "authors": ["Malinda Iluppangama", "Dilmi Abeywardana", "Chris Tsokos"], "title": "A Real Data-Driven, Robust Survival Analysis on Patients who Underwent Deep Brain Stimulation for Parkinson's Disease by Utilizing Parametric, Non-Parametric, and Semi-Parametric Approaches", "comment": null, "summary": "Parkinson's Disease (PD) is a devastating neurodegenerative disorder that affects millions of people around the globe. Many researchers are continuously working to understand PD and develop treatments to improve the condition of PD patients, which affects their day-to-day lives. Since the last decades, the treatment, Deep Brain Stimulation (DBS) has given promising results for motor symptoms by improving the quality of daily living of PD patients. In the methodology of the present study, we have utilized sophisticated statistical approaches such as Nonparametric, Semi-parametric, and robust Parametric survival analysis to extract useful and important information about the long-term survival outcomes of the patients who underwent DBS for PD. Finally, we were able to conclude that the probabilistic behavior of the survival time of female patients is statistically different from that of male patients. Furthermore, we have identified that the probabilistic behavior of the survival times of Female patients is characterized by the 3-parameter Lognormal distribution, while that of Male patients is characterized by the 3-parameter Weibull distribution. More importantly, we have found that the Female patients have higher survival compared to the Male patients after conducting a robust parametric survival analysis. Using the semi-parametric COX-PH, we found that the initial implant of the right side leads to a high frequency of events occurring for the female patients with a bad prognostic factor, while for the male patients, a low events occurs with a good prognostic factor. Furthermore, we have found an interaction term between the number of revisions and the initial size of the implant, which increases the frequency of events occurring for the Male patients with a bad prognostic factor."}
{"id": "2512.12003", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.12003", "abs": "https://arxiv.org/abs/2512.12003", "authors": ["Yi Wang", "Yuhao Deng", "Yu Gu", "Yuanjia Wang", "Donglin Zeng"], "title": "Debiased Inference for High-Dimensional Regression Models Based on Profile M-Estimation", "comment": null, "summary": "Debiased inference for high-dimensional regression models has received substantial recent attention to ensure regularized estimators have valid inference. All existing methods focus on achieving Neyman orthogonality through explicitly constructing projections onto the space of nuisance parameters, which is infeasible when an explicit form of the projection is unavailable. We introduce a general debiasing framework, Debiased Profile M-Estimation (DPME), which applies to a broad class of models and does not require model-specific Neyman orthogonalization or projection derivations as in existing methods. Our approach begins by obtaining an initial estimator of the parameters by optimizing a penalized objective function. To correct for the bias introduced by penalization, we construct a one-step estimator using the Newton-Raphson update, applied to the gradient of a profile function defined as the optimal objective function with the parameter of interest held fixed. We use numerical differentiation without requiring the explicit calculation of the gradients. The resulting DPME estimator is shown to be asymptotically linear and normally distributed. Through extensive simulations, we demonstrate that the proposed method achieves better coverage rates than existing alternatives, with largely reduced computational cost. Finally, we illustrate the utility of our method with an application to estimating a treatment rule for multiple myeloma."}
{"id": "2512.12748", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12748", "abs": "https://arxiv.org/abs/2512.12748", "authors": ["Martin Chak", "Giacomo Zanella"], "title": "Complexity of Markov Chain Monte Carlo for Generalized Linear Models", "comment": null, "summary": "Markov Chain Monte Carlo (MCMC), Laplace approximation (LA) and variational inference (VI) methods are popular approaches to Bayesian inference, each with trade-offs between computational cost and accuracy. However, a theoretical understanding of these differences is missing, particularly when both the sample size $n$ and the dimension $d$ are large. LA and Gaussian VI are justified by Bernstein-von Mises (BvM) theorems, and recent work has derived the characteristic condition $n\\gg d^2$ for their validity, improving over the condition $n\\gg d^3$. In this paper, we show for linear, logistic and Poisson regression that for $n\\gtrsim d$, MCMC attains the same complexity scaling in $n$, $d$ as first-order optimization algorithms, up to sub-polynomial factors. Thus MCMC is competitive with LA and Gaussian VI in complexity, under a scaling between $n$ and $d$ more general than BvM regimes. Our complexities apply to appropriately scaled priors that are not necessarily Gaussian-tailed, including Student-$t$ and flat priors, with log-posteriors that are not necessarily globally concave or gradient-Lipschitz."}
{"id": "2512.12579", "categories": ["stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.12579", "abs": "https://arxiv.org/abs/2512.12579", "authors": ["Malinda Iluppangama", "Dilmi Abeywardana", "Chris Tsokos"], "title": "A Real Data-Driven, Robust Survival Analysis on Patients who Underwent Deep Brain Stimulation for Parkinson's Disease by Utilizing Parametric, Non-Parametric, and Semi-Parametric Approaches", "comment": null, "summary": "Parkinson's Disease (PD) is a devastating neurodegenerative disorder that affects millions of people around the globe. Many researchers are continuously working to understand PD and develop treatments to improve the condition of PD patients, which affects their day-to-day lives. Since the last decades, the treatment, Deep Brain Stimulation (DBS) has given promising results for motor symptoms by improving the quality of daily living of PD patients. In the methodology of the present study, we have utilized sophisticated statistical approaches such as Nonparametric, Semi-parametric, and robust Parametric survival analysis to extract useful and important information about the long-term survival outcomes of the patients who underwent DBS for PD. Finally, we were able to conclude that the probabilistic behavior of the survival time of female patients is statistically different from that of male patients. Furthermore, we have identified that the probabilistic behavior of the survival times of Female patients is characterized by the 3-parameter Lognormal distribution, while that of Male patients is characterized by the 3-parameter Weibull distribution. More importantly, we have found that the Female patients have higher survival compared to the Male patients after conducting a robust parametric survival analysis. Using the semi-parametric COX-PH, we found that the initial implant of the right side leads to a high frequency of events occurring for the female patients with a bad prognostic factor, while for the male patients, a low events occurs with a good prognostic factor. Furthermore, we have found an interaction term between the number of revisions and the initial size of the implant, which increases the frequency of events occurring for the Male patients with a bad prognostic factor."}
{"id": "2512.11945", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.11945", "abs": "https://arxiv.org/abs/2512.11945", "authors": ["Diogo Pinheiro", "M. Rosário Oliveira", "Igor Kravchenko", "Lina Oliveira"], "title": "Interval Fisher's Discriminant Analysis and Visualisation", "comment": null, "summary": "In Data Science, entities are typically represented by single valued measurements. Symbolic Data Analysis extends this framework to more complex structures, such as intervals and histograms, that express internal variability. We propose an extension of multiclass Fisher's Discriminant Analysis to interval-valued data, using Moore's interval arithmetic and the Mallows' distance. Fisher's objective function is generalised to consider simultaneously the contributions of the centres and the ranges of intervals and is numerically maximised. The resulting discriminant directions are then used to classify interval-valued observations.To support visual assessment, we adapt the class map, originally introduced for conventional data, to classifiers that assign labels through minimum distance rules. We also extend the silhouette plot to this setting and use stacked mosaic plots to complement the visual display of class assignments. Together, these graphical tools provide insight into classifier performance and the strength of class membership. Applications to real datasets illustrate the proposed methodology and demonstrate its value in interpreting classification results for interval-valued data."}
{"id": "2512.12463", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.12463", "abs": "https://arxiv.org/abs/2512.12463", "authors": ["Yin Liu", "Jianwen Cai", "Didong Li"], "title": "Understanding Overparametrization in Survival Models through Double-Descent", "comment": null, "summary": "Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models."}
{"id": "2512.12038", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12038", "abs": "https://arxiv.org/abs/2512.12038", "authors": ["Antonio Olivas-Martinez", "Peter B. Gilbert", "Andrea Rotnitzky"], "title": "Proximal Causal Inference for Modified Treatment Policies", "comment": "66 pages, 15 figures", "summary": "The proximal causal inference framework enables the identification and estimation of causal effects in the presence of unmeasured confounding by leveraging two disjoint sets of observed strong proxies: negative control treatments and negative control outcomes. In the point exposure setting, this framework has primarily been applied to estimands comparing counterfactual outcomes under a static fixed intervention or, possibly randomized, regime that depends on baseline covariates. For continuous exposures, alternative hypothetical scenarios can enrich our understanding of causal effects, such as those where each individual receives their observed treatment dose modified in a pre-specified manner - commonly referred to as modified treatment regimes. In this work, we extend the proximal causal inference framework to identify and estimate the mean outcome under a modified treatment regime, addressing this gap in the literature. We propose a flexible strategy that does not rely on the assumption that all confounders have been measured - unlike existing estimators - and leverages modern debiased machine learning techniques using non-parametric estimators of nuisance functions to avoid restrictive parametric assumptions. Our methodology was motivated by immunobridging studies of COVID-19 vaccines aimed at identifying correlates of protection, where the individual's underlying immune capacity is an important unmeasured confounder. We demonstrate its applicability using data from such a study and evaluate its finite-sample performance through simulation studies."}
{"id": "2512.12749", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12749", "abs": "https://arxiv.org/abs/2512.12749", "authors": ["Sahil Bhola", "Karthik Duraisamy"], "title": "Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations", "comment": null, "summary": "Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium.\n  We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data."}
{"id": "2512.13346", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13346", "abs": "https://arxiv.org/abs/2512.13346", "authors": ["Yukun Lu", "Bingjie Li", "Zhigang Yao"], "title": "Beyond Missing Data: Questionnaire Uncertainty Responses as Early Digital Biomarkers of Cognitive Decline and Neurodegenerative Diseases", "comment": null, "summary": "Identifying preclinical biomarkers of neurodegenerative diseases remains a major challenge in aging research. In this study, we demonstrate that frequent \"Don't know/can't remember\" (DK) responses, often treated as missing data in touchscreen questionnaires, serve as a novel digital behavioral biomarker of early cognitive vulnerability and neurodegenerative disease risk. Using data from 502,234 UK Biobank participants, we stratified individuals based on DK response frequency (0-1, 2-4, 5-7, >7) and observed a robust, dose-dependent association with an increased risk of Alzheimer's disease (HR = 1.64, 95% CI: 1.26-2.14) and vascular dementia (HR = 1.93, 95% CI: 1.37-2.72), independent of established risk factors. As DK response frequency increased, participants exhibited higher BMI, reduced physical activity, higher smoking rates, and a higher prevalence of chronic diseases, particularly hypertension, diabetes, and depression. Further analysis revealed a dose-dependent relationship between DK response frequency and the risk of Alzheimer's disease and vascular dementia, with high DK responders showing early neurodegenerative changes, marked by elevated levels of Abeta40, Abeta42, NFL, and pTau-181. Metabolomic analysis also revealed lipid metabolism abnormalities, which may mediate this relationship. Together, these findings reframe DK response patterns as clinically meaningful signals of multidimensional neurobiological alterations, offering a scalable, low-cost, non-invasive tool for early risk identification and prevention at the population level."}
{"id": "2512.12003", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.12003", "abs": "https://arxiv.org/abs/2512.12003", "authors": ["Yi Wang", "Yuhao Deng", "Yu Gu", "Yuanjia Wang", "Donglin Zeng"], "title": "Debiased Inference for High-Dimensional Regression Models Based on Profile M-Estimation", "comment": null, "summary": "Debiased inference for high-dimensional regression models has received substantial recent attention to ensure regularized estimators have valid inference. All existing methods focus on achieving Neyman orthogonality through explicitly constructing projections onto the space of nuisance parameters, which is infeasible when an explicit form of the projection is unavailable. We introduce a general debiasing framework, Debiased Profile M-Estimation (DPME), which applies to a broad class of models and does not require model-specific Neyman orthogonalization or projection derivations as in existing methods. Our approach begins by obtaining an initial estimator of the parameters by optimizing a penalized objective function. To correct for the bias introduced by penalization, we construct a one-step estimator using the Newton-Raphson update, applied to the gradient of a profile function defined as the optimal objective function with the parameter of interest held fixed. We use numerical differentiation without requiring the explicit calculation of the gradients. The resulting DPME estimator is shown to be asymptotically linear and normally distributed. Through extensive simulations, we demonstrate that the proposed method achieves better coverage rates than existing alternatives, with largely reduced computational cost. Finally, we illustrate the utility of our method with an application to estimating a treatment rule for multiple myeloma."}
{"id": "2512.12550", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12550", "abs": "https://arxiv.org/abs/2512.12550", "authors": ["Jie Wang"], "title": "Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization", "comment": "29 pages", "summary": "Distributionally robust optimization (DRO) has emerged as a powerful paradigm for reliable decision-making under uncertainty. This paper focuses on DRO with ambiguity sets defined via the Sinkhorn discrepancy: an entropy-regularized Wasserstein distance, referred to as Sinkhorn DRO. Existing work primarily addresses Sinkhorn DRO from a dual perspective, leveraging its formulation as a conditional stochastic optimization problem, for which many stochastic gradient methods are applicable. However, the theoretical analyses of such methods often rely on the boundedness of the loss function, and it is indirect to obtain the worst-case distribution associated with Sinkhorn DRO. In contrast, we study Sinkhorn DRO from the primal perspective, by reformulating it as a bilevel program with several infinite-dimensional lower-level subproblems over probability space. This formulation enables us to simultaneously obtain the optimal robust decision and the worst-case distribution, which is valuable in practical settings, such as generating stress-test scenarios or designing robust learning algorithms. We propose both double-loop and single-loop sampling-based algorithms with theoretical guarantees to solve this bilevel program. Finally, we demonstrate the effectiveness of our approach through a numerical study on adversarial classification."}
{"id": "2512.12040", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12040", "abs": "https://arxiv.org/abs/2512.12040", "authors": ["Won Gu", "Francesca Chiaromonte", "Justin D. Silverman"], "title": "Sparse Bayesian Partially Identified Models for Sequence Count Data", "comment": null, "summary": "In genomics, differential abundance and expression analyses are complicated by the compositional nature of sequence count data, which reflect only relative-not absolute-abundances or expression levels. Many existing methods attempt to address this limitation through data normalizations, but we have shown that such approaches imply strong, often biologically implausible assumptions about total microbial load or total gene expression. Even modest violations of these assumptions can inflate Type I and Type II error rates to over 70%. Sparse estimators have been proposed as an alternative, leveraging the assumption that only a small subset of taxa (or genes) change between conditions. However, we show that current sparse methods suffer from similar pathologies because they treat sparsity assumptions as fixed and ignore the uncertainty inherent in these assumptions. We introduce a sparse Bayesian Partially Identified Model (PIM) that addresses this limitation by explicitly modeling uncertainty in sparsity assumptions. Our method extends the Scale-Reliant Inference (SRI) framework to the sparse setting, providing a principled approach to differential analysis under scale uncertainty. We establish theoretical consistency of the proposed estimator and, through extensive simulations and real data analyses, demonstrate substantial reductions in both Type I and Type II errors compared to existing methods."}
{"id": "2512.12398", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.12398", "abs": "https://arxiv.org/abs/2512.12398", "authors": ["Jessica P. Kunke", "Julian D. Olden", "Tyler H. McCormick"], "title": "Scalable Spatial Stream Network (S3N) Models", "comment": null, "summary": "Understanding how habitats shape species distributions and abundances across spatially complex, dendritic freshwater networks remains a longstanding and fundamental challenge in ecology, with direct implications for effective biodiversity management and conservation. Existing spatial stream network (SSN) models adapt spatial process models to river networks by creating covariance functions that account for stream distance, but preprocessing and estimation with these models is both computationally and time intensive, thus precluding the application of these models to regional or continental scales. This paper introduces a new class of Scalable Spatial Stream Network (S3N) models, which extend nearest-neighbor Gaussian processes to incorporate ecologically relevant spatial dependence while greatly improving computational efficiency. The S3N framework enables scalable modeling of spatial stream networks, demonstrated here for 285 fish species in the Ohio River Basin (>4,000 river km). Validation analyses show that S3N accurately recovers spatial and covariance parameters, even with reduced bias and variance compared to standard SSN implementations. These results represent a key advancement toward large-scale mapping of freshwater fish distributions and quantifying the influence of environmental drivers across extensive river networks."}
{"id": "2512.12038", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12038", "abs": "https://arxiv.org/abs/2512.12038", "authors": ["Antonio Olivas-Martinez", "Peter B. Gilbert", "Andrea Rotnitzky"], "title": "Proximal Causal Inference for Modified Treatment Policies", "comment": "66 pages, 15 figures", "summary": "The proximal causal inference framework enables the identification and estimation of causal effects in the presence of unmeasured confounding by leveraging two disjoint sets of observed strong proxies: negative control treatments and negative control outcomes. In the point exposure setting, this framework has primarily been applied to estimands comparing counterfactual outcomes under a static fixed intervention or, possibly randomized, regime that depends on baseline covariates. For continuous exposures, alternative hypothetical scenarios can enrich our understanding of causal effects, such as those where each individual receives their observed treatment dose modified in a pre-specified manner - commonly referred to as modified treatment regimes. In this work, we extend the proximal causal inference framework to identify and estimate the mean outcome under a modified treatment regime, addressing this gap in the literature. We propose a flexible strategy that does not rely on the assumption that all confounders have been measured - unlike existing estimators - and leverages modern debiased machine learning techniques using non-parametric estimators of nuisance functions to avoid restrictive parametric assumptions. Our methodology was motivated by immunobridging studies of COVID-19 vaccines aimed at identifying correlates of protection, where the individual's underlying immune capacity is an important unmeasured confounder. We demonstrate its applicability using data from such a study and evaluate its finite-sample performance through simulation studies."}
{"id": "2512.12463", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.12463", "abs": "https://arxiv.org/abs/2512.12463", "authors": ["Yin Liu", "Jianwen Cai", "Didong Li"], "title": "Understanding Overparametrization in Survival Models through Double-Descent", "comment": null, "summary": "Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models."}
{"id": "2512.12574", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12574", "abs": "https://arxiv.org/abs/2512.12574", "authors": ["Isaac Adjetey", "Yiyuan She"], "title": "Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities", "comment": "25 pages, 4 figures", "summary": "Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.\n  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.\n  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications."}
{"id": "2512.12065", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12065", "abs": "https://arxiv.org/abs/2512.12065", "authors": ["Efthymia Derezea", "Nicky J Welton", "Gabriel Rogers", "Hayley E Jones"], "title": "Meta-analysis of diagnostic test accuracy with multiple disease stages: combining stage-specific and merged-stage data", "comment": null, "summary": "For many conditions, it is of clinical importance to know not just the ability of a test to distinguish between those with and without the disease, but also the sensitivity to detect disease at different stages: in particular, the test's ability to detect disease at a stage most amenable to treatment. In a systematic review of test accuracy, pooled stage-specific estimates can be produced using subgroup analysis or meta-regression. However, this requires stage-specific data from each study, which is often not reported. Studies may however report test sensitivity for merged stage categories (e.g. stages I-II) or merged across all stages, together with information on the proportion of patients with disease at each stage. We demonstrate how to incorporate studies reporting merged stage data alongside studies reporting stage-specific data, to allow the inclusion of more studies in the meta-analysis. We consider both meta-analysis of tests with binary results, and meta-analysis of tests with continuous results, where the sensitivity to detect disease of each stage across the whole range of observed thresholds is estimated. The methods are demonstrated using a series of simulated datasets and applied to data from a systematic review of the accuracy of tests used to screen for hepatocellular carcinoma in people with liver cirrhosis. We show that incorporating studies with merged stage data can lead to more precise estimates and, in some cases, corrects biologically implausible results that can arise when the availability of stage-specific data is limited."}
{"id": "2512.12464", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.12464", "abs": "https://arxiv.org/abs/2512.12464", "authors": ["Jason Pillay", "Cristina Tortora", "Antonio Punzo", "Andriette Bekker"], "title": "Sleep pattern profiling using a finite mixture of contaminated multivariate skew-normal distributions on incomplete data", "comment": "The article consists of 29 pages, 3 of which are for references, and one for the supporting material (as an appendix)", "summary": "Medical data often exhibit characteristics that make cluster analysis particularly challenging, such as missing values, outliers, and cluster features like skewness. Typically, such data would need to be preprocessed -- by cleaning outliers and missing values -- before clustering could be performed. However, these preliminary steps rely on objective functions different from those used in the clustering stage. In this paper, we propose a unified model-based clustering approach that simultaneously handles atypical observations, missing values, and cluster-wise skewness within a single framework. Each cluster is modelled using a contaminated multivariate skew-normal distribution -- a convenient two-component mixture of multivariate skew-normal densities -- in which one component represents the main data (the \"bulk\") and the other captures potential outliers. From an inferential perspective, we implement and use a variant of the EM algorithm to obtain the maximum likelihood estimates of the model parameters. Simulation studies demonstrate that the proposed model outperforms existing approaches in both clustering accuracy and outlier detection, across low- and high-dimensional settings, even in the presence of substantial missingness. The method is further applied to the Cleveland Children's Sleep and Health Study (CCSHS), a dataset characterised by incomplete observations. Without any preprocessing, the proposed approach identifies five distinct groups of sleepers, revealing meaningful differences in sleeper typologies."}
{"id": "2512.12340", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12340", "abs": "https://arxiv.org/abs/2512.12340", "authors": ["Wenwu Gao", "Dongyi Zheng", "Hanbing Zhu"], "title": "Quantile regression with generalized multiquadric loss function", "comment": null, "summary": "Quantile regression (QR) is now widely used to analyze the effect of covariates on the conditional distribution of a response variable. It provides a more comprehensive picture of the relationship between a response and covariates compared with classical least squares regression. However, the non-differentiability of the check loss function precludes the use of gradient-based methods to solve the optimization problem in quantile regression estimation. To this end, This paper constructs a smoothed loss function based on multiquadric (MQ) function. The proposed loss function leads to a globally convex optimization problem that can be efficiently solved via (stochastic) gradient descent methods. As an example, we apply the Barzilai-Borwein gradient descent method to obtain the estimation of quantile regression. We establish the theoretical results of the proposed estimator under some regularity conditions, and compare it with other estimation methods using Monte Carlo simulations."}
{"id": "2512.12748", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12748", "abs": "https://arxiv.org/abs/2512.12748", "authors": ["Martin Chak", "Giacomo Zanella"], "title": "Complexity of Markov Chain Monte Carlo for Generalized Linear Models", "comment": null, "summary": "Markov Chain Monte Carlo (MCMC), Laplace approximation (LA) and variational inference (VI) methods are popular approaches to Bayesian inference, each with trade-offs between computational cost and accuracy. However, a theoretical understanding of these differences is missing, particularly when both the sample size $n$ and the dimension $d$ are large. LA and Gaussian VI are justified by Bernstein-von Mises (BvM) theorems, and recent work has derived the characteristic condition $n\\gg d^2$ for their validity, improving over the condition $n\\gg d^3$. In this paper, we show for linear, logistic and Poisson regression that for $n\\gtrsim d$, MCMC attains the same complexity scaling in $n$, $d$ as first-order optimization algorithms, up to sub-polynomial factors. Thus MCMC is competitive with LA and Gaussian VI in complexity, under a scaling between $n$ and $d$ more general than BvM regimes. Our complexities apply to appropriately scaled priors that are not necessarily Gaussian-tailed, including Student-$t$ and flat priors, with log-posteriors that are not necessarily globally concave or gradient-Lipschitz."}
{"id": "2512.12735", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12735", "abs": "https://arxiv.org/abs/2512.12735", "authors": ["Zhimin Chen", "Bryan Kelly", "Semyon Malamud"], "title": "Limits To (Machine) Learning", "comment": null, "summary": "Machine learning (ML) methods are highly flexible, but their ability to approximate the true data-generating process is fundamentally constrained by finite samples. We characterize a universal lower bound, the Limits-to-Learning Gap (LLG), quantifying the unavoidable discrepancy between a model's empirical fit and the population benchmark. Recovering the true population $R^2$, therefore, requires correcting observed predictive performance by this bound. Using a broad set of variables, including excess returns, yields, credit spreads, and valuation ratios, we find that the implied LLGs are large. This indicates that standard ML approaches can substantially understate true predictability in financial data. We also derive LLG-based refinements to the classic Hansen and Jagannathan (1991) bounds, analyze implications for parameter learning in general-equilibrium settings, and show that the LLG provides a natural mechanism for generating excess volatility."}
{"id": "2512.12244", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12244", "abs": "https://arxiv.org/abs/2512.12244", "authors": ["Zeyu Yao", "Bowen Gang", "Wenguang Sun"], "title": "Safe, Always-Valid Alpha-Investing Rules For Doubly Sequential Online Inference", "comment": "68 pages, 13 figures", "summary": "Dynamic decision-making in rapidly evolving research domains, including marketing, finance, and pharmaceutical development, presents a significant challenge. Researchers frequently confront the need for real-time action within a doubly sequential framework characterized by the continuous influx of high-volume data streams and the intermittent arrival of novel tasks. This calls for the development and implementation of new online inference protocols capable of handling both the continuous processing of incoming information and the efficient allocation of resources to address emerging priorities. We introduce a novel class of Safe and Always-Valid Alpha-investing (SAVA) rules that leverages powerful tools including always valid p-values, e-processes, and online false discovery rate methods. The SAVA algorithm effectively integrates information across all tasks, mitigates the alpha-death problem, and controls the false selection rate (FSR) at all decision points. We validate the efficacy of the SAVA framework through rigorous theoretical analysis and extensive numerical experiments. Our results demonstrate that SAVA not only offers effective control of the FSR but also significantly improves statistical power compared to traditional online testing approaches."}
{"id": "2512.12988", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12988", "abs": "https://arxiv.org/abs/2512.12988", "authors": ["Yilei Zhang", "Yun Wei", "Aritra Guha", "XuanLong Nguyen"], "title": "A Bayesian approach to learning mixtures of nonparametric components", "comment": "76 pages, 9 figures", "summary": "Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution."}
{"id": "2512.12394", "categories": ["stat.ME", "cs.CL", "physics.soc-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12394", "abs": "https://arxiv.org/abs/2512.12394", "authors": ["Vladimir Berman"], "title": "The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework", "comment": null, "summary": "We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages."}
{"id": "2512.12953", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.12953", "abs": "https://arxiv.org/abs/2512.12953", "authors": ["Madhav Sankaranarayanan", "Yana Hrytsenko", "Jerome I. Rotter", "Tamar Sofer", "Rajarshi Mukherjee"], "title": "Asymptotic Inference for Constrained Regression", "comment": "34 pages, 21 figures", "summary": "We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework."}
{"id": "2512.12742", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12742", "abs": "https://arxiv.org/abs/2512.12742", "authors": ["Pingping Yin", "Xiyun Jiao"], "title": "Transport Reversible Jump Markov Chain Monte Carlo with proposals generated by Variational Inference with Normalizing Flows", "comment": null, "summary": "We present a framework using variational inference with normalizing flows (VI-NFs) to generate proposals of reversible jump Markov chain Monte Carlo (RJMCMC) for efficient trans-dimensional Bayesian inference. Unlike transport reversible jump methods relying on forward KL minimization with pilot MCMC samples, our approach minimizes the reverse KL divergence which requires only samples from a base distribution, eliminating costly target sampling. The method employs RealNVP-based flows to learn model-specific transport maps, enabling construction of both between-model and within-model proposals. Our framework provides accurate marginal likelihood estimates from the variational approximation. This facilitates efficient model comparison and proposal adaptation in RJMCMC. Experiments on illustrative example, factor analysis and variable selection tasks in linear regression show that TRJ designed by VI-NFs achieves faster mixing and more efficient model space exploration compared to existing baselines. The proposed algorithm can be extended to conditional flows for amortized vairiational inference across models. Code is available at https://github.com/YinPingping111/TRJ_VINFs."}
{"id": "2512.12289", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12289", "abs": "https://arxiv.org/abs/2512.12289", "authors": ["Bingbing Wang", "Shengyan Sun", "Jiaqi Wang", "Yu Tang"], "title": "Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture", "comment": null, "summary": "Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist."}
{"id": "2512.12574", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12574", "abs": "https://arxiv.org/abs/2512.12574", "authors": ["Isaac Adjetey", "Yiyuan She"], "title": "Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities", "comment": "25 pages, 4 figures", "summary": "Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.\n  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.\n  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications."}
{"id": "2512.12988", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12988", "abs": "https://arxiv.org/abs/2512.12988", "authors": ["Yilei Zhang", "Yun Wei", "Aritra Guha", "XuanLong Nguyen"], "title": "A Bayesian approach to learning mixtures of nonparametric components", "comment": "76 pages, 9 figures", "summary": "Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution."}
{"id": "2512.12905", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12905", "abs": "https://arxiv.org/abs/2512.12905", "authors": ["Ruixin Guo", "Ruoming Jin", "Xinyu Li", "Yang Zhou"], "title": "PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders", "comment": "Accepted at NeurIPS 2025 (https://openreview.net/forum?id=S1zkFSby8G)", "summary": "Linear Autoencoders (LAEs) have shown strong performance in state-of-the-art recommender systems. However, this success remains largely empirical, with limited theoretical understanding. In this paper, we investigate the generalizability -- a theoretical measure of model performance in statistical learning -- of multivariate linear regression and LAEs. We first propose a PAC-Bayes bound for multivariate linear regression, extending the earlier bound for single-output linear regression by Shalaeva et al., and establish sufficient conditions for its convergence. We then show that LAEs, when evaluated under a relaxed mean squared error, can be interpreted as constrained multivariate linear regression models on bounded data, to which our bound adapts. Furthermore, we develop theoretical methods to improve the computational efficiency of optimizing the LAE bound, enabling its practical evaluation on large models and real-world datasets. Experimental results demonstrate that our bound is tight and correlates well with practical ranking metrics such as Recall@K and NDCG@K."}
{"id": "2512.12340", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12340", "abs": "https://arxiv.org/abs/2512.12340", "authors": ["Wenwu Gao", "Dongyi Zheng", "Hanbing Zhu"], "title": "Quantile regression with generalized multiquadric loss function", "comment": null, "summary": "Quantile regression (QR) is now widely used to analyze the effect of covariates on the conditional distribution of a response variable. It provides a more comprehensive picture of the relationship between a response and covariates compared with classical least squares regression. However, the non-differentiability of the check loss function precludes the use of gradient-based methods to solve the optimization problem in quantile regression estimation. To this end, This paper constructs a smoothed loss function based on multiquadric (MQ) function. The proposed loss function leads to a globally convex optimization problem that can be efficiently solved via (stochastic) gradient descent methods. As an example, we apply the Barzilai-Borwein gradient descent method to obtain the estimation of quantile regression. We establish the theoretical results of the proposed estimator under some regularity conditions, and compare it with other estimation methods using Monte Carlo simulations."}
{"id": "2512.12857", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12857", "abs": "https://arxiv.org/abs/2512.12857", "authors": ["Cristian Parra-Aldana", "Juan Sosa"], "title": "Variational Inference for Fully Bayesian Hierarchical Linear Models", "comment": "44 pages, 10 figures, 6 tables", "summary": "Bayesian hierarchical linear models provide a natural framework to analyze nested and clustered data. Classical estimation with Markov chain Monte Carlo produces well calibrated posterior distributions but becomes computationally expensive in high dimensional or large sample settings. Variational Inference and Stochastic Variational Inference offer faster optimization based alternatives, but their accuracy in hierarchical structures is uncertain when group separation is weak. This paper compares these two paradigms across three model classes, the Linear Regression Model, the Hierarchical Linear Regression Model, and a Clustered Hierarchical Linear Regression Model. Through simulation studies and an application to real data, the results show that variational methods recover global regression effects and clustering structure with a fraction of the computing time, but distort posterior dependence and yield unstable values of information criteria such as WAIC and DIC. The findings clarify when variational methods can serve as practical surrogates for Markov chain Monte Carlo and when their limitations make full Bayesian sampling necessary, and they provide guidance for extending the same variational framework to generalized linear models and other members of the exponential family."}
{"id": "2512.13634", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13634", "abs": "https://arxiv.org/abs/2512.13634", "authors": ["Reza Gheissari", "Aukosh Jagannath"], "title": "Universality of high-dimensional scaling limits of stochastic gradient descent", "comment": "30 pages", "summary": "We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal."}
{"id": "2512.12911", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12911", "abs": "https://arxiv.org/abs/2512.12911", "authors": ["Kohei Nishikawa", "Koki Shimizu", "Hashiguchi Hiroki"], "title": "Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory", "comment": null, "summary": "This study evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices. Each weight matrix is modeled as the sum of signal and noise matrices. The low-rank approximation is obtained by removing noise-related singular values using a threshold based on random matrix theory. To assess the adequacy of this threshold, we propose an evaluation metric based on the cosine similarity between the singular vectors of the signal and original weight matrices. The proposed metric is used in numerical experiments to compare two threshold estimation methods."}
{"id": "2512.12362", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12362", "abs": "https://arxiv.org/abs/2512.12362", "authors": ["Antoine Barbieri", "Angelo Alcaraz", "Mouna Abed", "Hugues de Courson", "Hélène Jacqmin-Gadda"], "title": "Asymmetric Laplace distribution regression model for fitting heterogeneous longitudinal response", "comment": null, "summary": "The systematic collection of longitudinal data is very common in practice, making mixed models widely used. Most developments around these models focus on modeling the mean trajectory of repeated measurements, typically under the assumption of homoskedasticity. However, as data become increasingly rich through intensive collection over time, these models can become limiting and may introduce biases in analysis. In fact, such data are often heterogeneous, with the presence of outliers, heteroskedasticity, and asymmetry in the distribution of individual measurements. Therefore, ignoring these characteristics can lead to biased modeling results. In this work, we propose a mixed-effect distributional regression model based on the asymmetric Laplace distribution to: (1) address the presence of outliers, heteroskedasticity, and asymmetry in longitudinal measurements; (2) model the entire individual distribution of the heterogeneous longitudinal response over time, rather than just its conditional expectation; and (3) give a more comprehensive evaluation of the impact of covariates on the distribution of the responses through meaningful indicator. A Bayesian estimation procedure is presented. In order to choose between two distributional regression models, we also propose a new model selection criterion for longitudinal data. It measures the proximity between the individual distribution estimated by the model and the empirical individual distribution of the data over time, using a set of quantiles. The estimation procedure and the selection criterion are validated in a simulation study and the proposed model is compared to a distributional regression mixed model based on the Gaussian distribution and a location-scale linear quantile mixed model. Finally, the proposed model is applied to analyze blood pressure over time for hospitalized patients in the intensive care unit."}
{"id": "2512.13354", "categories": ["stat.OT", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13354", "abs": "https://arxiv.org/abs/2512.13354", "authors": ["Geremy Loachamín", "Eleni D. Koronaki", "Dimitrios G. Giovanis", "Martin Kathrein", "Christoph Czettl", "Andreas G. Boudouvis", "Stéphane P. A. Bordas"], "title": "Data-driven inverse uncertainty quantification: application to the Chemical Vapor Deposition Reactor Modeling", "comment": null, "summary": "This study presents a Bayesian framework for (inverse) uncertainty quantification and parameter estimation in a two-step Chemical Vapor Deposition coating process using production data. We develop an XGBoost surrogate model that maps reactor setup parameters to coating thickness measurements, enabling efficient Bayesian analysis while reducing sampling costs. The methodology handles a mixture of data including continuous, discrete integer, binary, and encoded categorical variables. We establish parameter prior distributions through Bayesian Model Selection and perform Inverse Uncertainty Quantification via weighted Approximate Bayesian Computation with summary statistics, providing robust parameter credible intervals while filtering measurement noise across multiple reactor locations. Furthermore, we employ clustering methods guided by geometry embeddings to focus analysis within homogeneous production groups. This integrated approach provides a validated tool for improving industrial process control under uncertainty."}
{"id": "2512.13003", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13003", "abs": "https://arxiv.org/abs/2512.13003", "authors": ["Min Lu", "Hemant Ishwaran"], "title": "General OOD Detection via Model-aware and Subspace-aware Variable Priority", "comment": "29 pages, 11 figures", "summary": "Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines."}
{"id": "2512.12394", "categories": ["stat.ME", "cs.CL", "physics.soc-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12394", "abs": "https://arxiv.org/abs/2512.12394", "authors": ["Vladimir Berman"], "title": "The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework", "comment": null, "summary": "We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages."}
{"id": "2512.13383", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13383", "abs": "https://arxiv.org/abs/2512.13383", "authors": ["Karen Wolf", "Pierre Fernique", "Hans-Peter Piepho"], "title": "Automatic Quality Control for Agricultural Field Trials -- Detection of Nonstationarity in Grid-indexed Data", "comment": null, "summary": "A common assumption in the spatial analysis of agricultural field trials is stationarity. In practice, however, this assumption is often violated due to unaccounted field effects. For instance, in plant breeding field trials, this can lead to inaccurate estimates of plant performance. Based on such inaccurate estimates, breeders may be impeded in selecting the best performing plant varieties, slowing breeding progress. We propose a method to automatically verify the hypothesis of stationarity. The method is sensitive towards mean as well as variance-covariance nonstationarity. It is specifically developed for the two-dimensional grid-structure of field trials. The method relies on the hypothesis that we can detect nonstationarity by partitioning the field into areas, within which stationarity holds. We applied the method to a large number of simulated datasets and a real-data example. The method reliably points out which trials exhibit quality issues and gives an indication about the severity of nonstationarity. This information can significantly reduce the time spent on manual quality control and enhance its overall reliability. Furthermore, the output of the method can be used to improve the analysis of conducted trials as well as the experimental design of future trials."}
{"id": "2512.13565", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13565", "abs": "https://arxiv.org/abs/2512.13565", "authors": ["Junye Du", "Zhenghao Li", "Zhutong Gu", "Long Feng"], "title": "A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees", "comment": "64 pages", "summary": "This paper tackles the problem of feature selection in a highly challenging setting: $\\mathbb{E}(y | \\boldsymbol{x}) = G(\\boldsymbol{x}_{\\mathcal{S}_0})$, where $\\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Ω(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Ω(s \\log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions."}
{"id": "2512.12398", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.12398", "abs": "https://arxiv.org/abs/2512.12398", "authors": ["Jessica P. Kunke", "Julian D. Olden", "Tyler H. McCormick"], "title": "Scalable Spatial Stream Network (S3N) Models", "comment": null, "summary": "Understanding how habitats shape species distributions and abundances across spatially complex, dendritic freshwater networks remains a longstanding and fundamental challenge in ecology, with direct implications for effective biodiversity management and conservation. Existing spatial stream network (SSN) models adapt spatial process models to river networks by creating covariance functions that account for stream distance, but preprocessing and estimation with these models is both computationally and time intensive, thus precluding the application of these models to regional or continental scales. This paper introduces a new class of Scalable Spatial Stream Network (S3N) models, which extend nearest-neighbor Gaussian processes to incorporate ecologically relevant spatial dependence while greatly improving computational efficiency. The S3N framework enables scalable modeling of spatial stream networks, demonstrated here for 285 fish species in the Ohio River Basin (>4,000 river km). Validation analyses show that S3N accurately recovers spatial and covariance parameters, even with reduced bias and variance compared to standard SSN implementations. These results represent a key advancement toward large-scale mapping of freshwater fish distributions and quantifying the influence of environmental drivers across extensive river networks."}
{"id": "2512.13446", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13446", "abs": "https://arxiv.org/abs/2512.13446", "authors": ["Murat Yaslioglu"], "title": "A Metadata-Only Feature-Augmented Method Factor for Ex-Post Correction and Attribution of Common Method Variance", "comment": null, "summary": "Common Method Variance (CMV) is a recurring problem that reduces survey accuracy. Popular fixes such as the Harman single-factor test, correlated uniquenesses, common latent factor models, and marker variable approaches have well known flaws. These approaches either poorly identify issues, rely too heavily on researchers' choices, omit real information, or require special marker items that many datasets lack. This paper introduces a metadata-only Feature-Augmented Method Factor (FAMF-SEM): a single extra method factor with fixed, item-specific weights based on questionnaire details like reverse coding, page and item order, scale width, wording direction, and item length. These weights are set using ridge regression, based on residual correlations in a basic CFA, and remain fixed in the model. The method avoids the need for additional data or marker variables and provides CMV-adjusted results with clear links to survey design features. An AMOS/LISREL-friendly, no-code Excel workflow demonstrates the method. The paper explains the rationale, provides model details, outlines setup, presents step-by-step instructions, describes checks and reliability tests, and notes limitations."}
{"id": "2512.13634", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.13634", "abs": "https://arxiv.org/abs/2512.13634", "authors": ["Reza Gheissari", "Aukosh Jagannath"], "title": "Universality of high-dimensional scaling limits of stochastic gradient descent", "comment": "30 pages", "summary": "We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal."}
{"id": "2512.12452", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12452", "abs": "https://arxiv.org/abs/2512.12452", "authors": ["Fei Fang", "Laura Forastiere"], "title": "Design-Based Weighted Regression Estimators for Average and Conditional Spillover Effects", "comment": null, "summary": "When individuals engage in social or physical interactions, a unit's outcome may depend on the treatments received by others. In such interference environments, we provide a unified framework characterizing a broad class of spillover estimands as weighted averages of unit-to-unit spillover effects, with estimand-specific weights. We then develop design-based weighted least squares (WLS) estimators for both average and conditional spillover effects. We introduce three nonparametric estimators under the dyadic, sender, and receiver perspectives, which distribute the estimand weights differently across the outcome vector, design matrix, and weight matrix. For the average-type estimands, we show that all three estimators are equivalent to the Hajek estimator. For conditional spillover effects, we establish conditions under which the estimands are consistent for the target conditional spillover effects. We further derive concentration inequalities, a central limit theorem, and conservative variance estimators in an asymptotic regime where both the number of clusters and cluster sizes grow."}
{"id": "2512.13622", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13622", "abs": "https://arxiv.org/abs/2512.13622", "authors": ["Hunter Chen", "Junming Guan", "Erik van Zwet", "Nikolaos Ignatiadis"], "title": "Empirical Bayes learning from selectively reported confidence intervals", "comment": null, "summary": "We develop a statistical framework for empirical Bayes learning from selectively reported confidence intervals, applied here to provide context for interpreting results published in MEDLINE abstracts. A collection of 326,060 z-scores from MEDLINE abstracts (2000-2018) provides context for interpreting individual studies; we formalize this as an empirical Bayes task complicated by selection bias. We address selection bias through a selective tilting approach that extends empirical Bayes confidence intervals to truncated sampling mechanisms. Sign information is unreliable (a positive z-score need not indicate benefit, and investigators may choose contrast directions post hoc), so we work with absolute z-scores and identify only the distribution of absolute signal-to-noise ratios (SNRs). Our framework provides coverage guarantees for functionals including posterior estimands describing idealized replications and the symmetrized posterior mean, which we justify decision-theoretically as optimal among sign-equivariant (odd) estimators and minimax among priors inducing the same absolute SNR distribution."}
{"id": "2512.12676", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12676", "abs": "https://arxiv.org/abs/2512.12676", "authors": ["Jiawei Yan", "Ju Liu", "Weidong Liu", "Jiyuan Tu"], "title": "Robust Variational Bayes by Min-Max Median Aggregation", "comment": "34 pages, 11 figures", "summary": "We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies."}
{"id": "2512.12464", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.12464", "abs": "https://arxiv.org/abs/2512.12464", "authors": ["Jason Pillay", "Cristina Tortora", "Antonio Punzo", "Andriette Bekker"], "title": "Sleep pattern profiling using a finite mixture of contaminated multivariate skew-normal distributions on incomplete data", "comment": "The article consists of 29 pages, 3 of which are for references, and one for the supporting material (as an appendix)", "summary": "Medical data often exhibit characteristics that make cluster analysis particularly challenging, such as missing values, outliers, and cluster features like skewness. Typically, such data would need to be preprocessed -- by cleaning outliers and missing values -- before clustering could be performed. However, these preliminary steps rely on objective functions different from those used in the clustering stage. In this paper, we propose a unified model-based clustering approach that simultaneously handles atypical observations, missing values, and cluster-wise skewness within a single framework. Each cluster is modelled using a contaminated multivariate skew-normal distribution -- a convenient two-component mixture of multivariate skew-normal densities -- in which one component represents the main data (the \"bulk\") and the other captures potential outliers. From an inferential perspective, we implement and use a variant of the EM algorithm to obtain the maximum likelihood estimates of the model parameters. Simulation studies demonstrate that the proposed model outperforms existing approaches in both clustering accuracy and outlier detection, across low- and high-dimensional settings, even in the presence of substantial missingness. The method is further applied to the Cleveland Children's Sleep and Health Study (CCSHS), a dataset characterised by incomplete observations. Without any preprocessing, the proposed approach identifies five distinct groups of sleepers, revealing meaningful differences in sleeper typologies."}
{"id": "2512.12748", "categories": ["stat.CO", "math.PR", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12748", "abs": "https://arxiv.org/abs/2512.12748", "authors": ["Martin Chak", "Giacomo Zanella"], "title": "Complexity of Markov Chain Monte Carlo for Generalized Linear Models", "comment": null, "summary": "Markov Chain Monte Carlo (MCMC), Laplace approximation (LA) and variational inference (VI) methods are popular approaches to Bayesian inference, each with trade-offs between computational cost and accuracy. However, a theoretical understanding of these differences is missing, particularly when both the sample size $n$ and the dimension $d$ are large. LA and Gaussian VI are justified by Bernstein-von Mises (BvM) theorems, and recent work has derived the characteristic condition $n\\gg d^2$ for their validity, improving over the condition $n\\gg d^3$. In this paper, we show for linear, logistic and Poisson regression that for $n\\gtrsim d$, MCMC attains the same complexity scaling in $n$, $d$ as first-order optimization algorithms, up to sub-polynomial factors. Thus MCMC is competitive with LA and Gaussian VI in complexity, under a scaling between $n$ and $d$ more general than BvM regimes. Our complexities apply to appropriately scaled priors that are not necessarily Gaussian-tailed, including Student-$t$ and flat priors, with log-posteriors that are not necessarily globally concave or gradient-Lipschitz."}
{"id": "2512.12676", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12676", "abs": "https://arxiv.org/abs/2512.12676", "authors": ["Jiawei Yan", "Ju Liu", "Weidong Liu", "Jiyuan Tu"], "title": "Robust Variational Bayes by Min-Max Median Aggregation", "comment": "34 pages, 11 figures", "summary": "We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies."}
{"id": "2512.12749", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12749", "abs": "https://arxiv.org/abs/2512.12749", "authors": ["Sahil Bhola", "Karthik Duraisamy"], "title": "Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations", "comment": null, "summary": "Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium.\n  We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data."}
{"id": "2512.12857", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12857", "abs": "https://arxiv.org/abs/2512.12857", "authors": ["Cristian Parra-Aldana", "Juan Sosa"], "title": "Variational Inference for Fully Bayesian Hierarchical Linear Models", "comment": "44 pages, 10 figures, 6 tables", "summary": "Bayesian hierarchical linear models provide a natural framework to analyze nested and clustered data. Classical estimation with Markov chain Monte Carlo produces well calibrated posterior distributions but becomes computationally expensive in high dimensional or large sample settings. Variational Inference and Stochastic Variational Inference offer faster optimization based alternatives, but their accuracy in hierarchical structures is uncertain when group separation is weak. This paper compares these two paradigms across three model classes, the Linear Regression Model, the Hierarchical Linear Regression Model, and a Clustered Hierarchical Linear Regression Model. Through simulation studies and an application to real data, the results show that variational methods recover global regression effects and clustering structure with a fraction of the computing time, but distort posterior dependence and yield unstable values of information criteria such as WAIC and DIC. The findings clarify when variational methods can serve as practical surrogates for Markov chain Monte Carlo and when their limitations make full Bayesian sampling necessary, and they provide guidance for extending the same variational framework to generalized linear models and other members of the exponential family."}
{"id": "2512.12988", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12988", "abs": "https://arxiv.org/abs/2512.12988", "authors": ["Yilei Zhang", "Yun Wei", "Aritra Guha", "XuanLong Nguyen"], "title": "A Bayesian approach to learning mixtures of nonparametric components", "comment": "76 pages, 9 figures", "summary": "Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution."}
{"id": "2512.12946", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12946", "abs": "https://arxiv.org/abs/2512.12946", "authors": ["Junmo Song"], "title": "Robust tests for parameter change in conditionally heteroscedastic time series models", "comment": "28 pages, 1 figure", "summary": "Structural changes and outliers often coexist, complicating statistical inference. This paper addresses the problem of testing for parameter changes in conditionally heteroscedastic time series models, particularly in the presence of outliers. To mitigate the impact of outliers, we introduce a two-step procedure comprising robust estimation and residual truncation. Based on this procedure, we propose a residual-based robust CUSUM test and its self-normalized counterpart. We derive the limiting null distributions of the proposed robust tests and establish their consistency. Simulation results demonstrate the strong robustness of the tests against outliers. To illustrate the practical application, we analyze Bitcoin data."}
{"id": "2512.13610", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.13610", "abs": "https://arxiv.org/abs/2512.13610", "authors": ["Laura B. Balzer", "Mark J. van der Laan", "Maya L. Petersen"], "title": "Machine learning to optimize precision in the analysis of randomized trials: A journey in pre-specified, yet data-adaptive learning", "comment": "accepted for publication at Clinical Trials", "summary": "Covariate adjustment is an approach to improve the precision of trial analyses by adjusting for baseline variables that are prognostic of the primary endpoint. Motivated by the SEARCH Universal HIV Test-and-Treat Trial (2013-2017), we tell our story of developing, evaluating, and implementing a machine learning-based approach for covariate adjustment. We provide the rationale for as well as the practical concerns with such an approach for estimating marginal effects. Using schematics, we illustrate our procedure: targeted machine learning estimation (TMLE) with Adaptive Pre-specification. Briefly, sample-splitting is used to data-adaptively select the combination of estimators of the outcome regression (i.e., the conditional expectation of the outcome given the trial arm and covariates) and known propensity score (i.e., the conditional probability of being randomized to the intervention given the covariates) that minimizes the cross-validated variance estimate and, thereby, maximizes empirical efficiency. We discuss our approach for evaluating finite sample performance with parametric and plasmode simulations, pre-specifying the Statistical Analysis Plan, and unblinding in real-time on video conference with our colleagues from around the world. We present the results from applying our approach in the primary, pre-specified analysis of 8 recently published trials (2022-2024). We conclude with practical recommendations and an invitation to implement our approach in the primary analysis of your next trial."}
{"id": "2512.12953", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.12953", "abs": "https://arxiv.org/abs/2512.12953", "authors": ["Madhav Sankaranarayanan", "Yana Hrytsenko", "Jerome I. Rotter", "Tamar Sofer", "Rajarshi Mukherjee"], "title": "Asymptotic Inference for Constrained Regression", "comment": "34 pages, 21 figures", "summary": "We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework."}
{"id": "2512.12988", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12988", "abs": "https://arxiv.org/abs/2512.12988", "authors": ["Yilei Zhang", "Yun Wei", "Aritra Guha", "XuanLong Nguyen"], "title": "A Bayesian approach to learning mixtures of nonparametric components", "comment": "76 pages, 9 figures", "summary": "Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution."}
{"id": "2512.13155", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13155", "abs": "https://arxiv.org/abs/2512.13155", "authors": ["Sarah J Valk", "Camila Caram-Deelder", "Rolf. H. H. Groenwold", "Johanna G van der Bom"], "title": "Clinical transfusion-outcomes research: A practical guide", "comment": null, "summary": "Clinical transfusion-outcomes research faces unique methodological challenges compared with other areas of clinical research. These challenges arise because patients frequently receive multiple transfusions, each unit originates from a different donor, and the probability of receiving specific blood product characteristics is influenced by external, often uncontrollable, factors. These complexities complicate causal inference in observational studies of transfusion effectiveness and safety. This guide addresses key challenges in observational transfusion research, with a focus on time-varying exposure, time-varying confounding, and treatment-confounder feedback. Using the example of donor sex and pregnancy history in relation to recipient mortality, we illustrate the strengths and limitations of commonly used analytical approaches. We compare restriction-based analyses, time-varying Cox regression, and inverse probability weighted marginal structural models using a large observational dataset of male transfusion recipients. In the applied example, restriction and conventional time-varying approaches suggested an increased mortality risk associated with transfusion of red blood cells from ever-pregnant female donors compared with male-only donors (hazard ratio [HR] 1.22; 95% CI 1.05-1.42 and HR 1.21; 95% CI 1.04-1.41, respectively). In contrast, inverse probability of treatment and censoring weighted analyses, which account for treatment-confounder feedback, showed no evidence of an association (HR 1.01; 95% CI 0.85-1.20). These findings demonstrate how conventional methods can yield biased estimates when complex longitudinal structures are not adequately handled. We provide practical guidance on study design, target trial emulation, and the use of g-methods, including a reproducible tutorial and example dataset, to support valid causal inference in clinical transfusion research."}
{"id": "2512.13383", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13383", "abs": "https://arxiv.org/abs/2512.13383", "authors": ["Karen Wolf", "Pierre Fernique", "Hans-Peter Piepho"], "title": "Automatic Quality Control for Agricultural Field Trials -- Detection of Nonstationarity in Grid-indexed Data", "comment": null, "summary": "A common assumption in the spatial analysis of agricultural field trials is stationarity. In practice, however, this assumption is often violated due to unaccounted field effects. For instance, in plant breeding field trials, this can lead to inaccurate estimates of plant performance. Based on such inaccurate estimates, breeders may be impeded in selecting the best performing plant varieties, slowing breeding progress. We propose a method to automatically verify the hypothesis of stationarity. The method is sensitive towards mean as well as variance-covariance nonstationarity. It is specifically developed for the two-dimensional grid-structure of field trials. The method relies on the hypothesis that we can detect nonstationarity by partitioning the field into areas, within which stationarity holds. We applied the method to a large number of simulated datasets and a real-data example. The method reliably points out which trials exhibit quality issues and gives an indication about the severity of nonstationarity. This information can significantly reduce the time spent on manual quality control and enhance its overall reliability. Furthermore, the output of the method can be used to improve the analysis of conducted trials as well as the experimental design of future trials."}
{"id": "2512.13446", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13446", "abs": "https://arxiv.org/abs/2512.13446", "authors": ["Murat Yaslioglu"], "title": "A Metadata-Only Feature-Augmented Method Factor for Ex-Post Correction and Attribution of Common Method Variance", "comment": null, "summary": "Common Method Variance (CMV) is a recurring problem that reduces survey accuracy. Popular fixes such as the Harman single-factor test, correlated uniquenesses, common latent factor models, and marker variable approaches have well known flaws. These approaches either poorly identify issues, rely too heavily on researchers' choices, omit real information, or require special marker items that many datasets lack. This paper introduces a metadata-only Feature-Augmented Method Factor (FAMF-SEM): a single extra method factor with fixed, item-specific weights based on questionnaire details like reverse coding, page and item order, scale width, wording direction, and item length. These weights are set using ridge regression, based on residual correlations in a basic CFA, and remain fixed in the model. The method avoids the need for additional data or marker variables and provides CMV-adjusted results with clear links to survey design features. An AMOS/LISREL-friendly, no-code Excel workflow demonstrates the method. The paper explains the rationale, provides model details, outlines setup, presents step-by-step instructions, describes checks and reliability tests, and notes limitations."}
{"id": "2512.13473", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13473", "abs": "https://arxiv.org/abs/2512.13473", "authors": ["Alexa A. Sochaniwsky", "Paul D. McNicholas"], "title": "Parsimonious Ultrametric Manly Mixture Models", "comment": null, "summary": "A family of parsimonious ultrametric mixture models with the Manly transformation is developed for clustering high-dimensional and asymmetric data. Advances in Gaussian mixture modeling sufficiently handle high-dimensional data but struggle with the common presence of skewness. While these advances reduce the number of free parameters, they often provide limited insight into the structure and interpretation of the clusters. To address this shortcoming, this research implements the extended ultrametric covariance structure and the Manly transformation resulting in the parsimonious ultrametric Manly mixture model family. The ultrametric covariance structure reduces the number of free parameters while identifying latent hierarchical relationships between and within groups of variables. This phenomenon allows the visualization of hierarchical relationships within individual clusters, improving cluster interpretability. Additionally, as with many classes of mixture models, model selection remains a fundamental challenge; a two-step model selection procedure is proposed herein. With simulation studies and real data analyses, we demonstrate improved model selection via the proposed two-step method, and the effective clustering performance for the proposed family."}
{"id": "2512.13530", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13530", "abs": "https://arxiv.org/abs/2512.13530", "authors": ["Shih-Ni Prim", "Kevin R. Quinlan", "Paul Hawkins", "Jagadeesh Movva", "Annie S. Booth"], "title": "Actively Learning Joint Contours of Multiple Computer Experiments", "comment": null, "summary": "Contour location$\\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a \"joint contour location\" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments."}
{"id": "2512.13610", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.13610", "abs": "https://arxiv.org/abs/2512.13610", "authors": ["Laura B. Balzer", "Mark J. van der Laan", "Maya L. Petersen"], "title": "Machine learning to optimize precision in the analysis of randomized trials: A journey in pre-specified, yet data-adaptive learning", "comment": "accepted for publication at Clinical Trials", "summary": "Covariate adjustment is an approach to improve the precision of trial analyses by adjusting for baseline variables that are prognostic of the primary endpoint. Motivated by the SEARCH Universal HIV Test-and-Treat Trial (2013-2017), we tell our story of developing, evaluating, and implementing a machine learning-based approach for covariate adjustment. We provide the rationale for as well as the practical concerns with such an approach for estimating marginal effects. Using schematics, we illustrate our procedure: targeted machine learning estimation (TMLE) with Adaptive Pre-specification. Briefly, sample-splitting is used to data-adaptively select the combination of estimators of the outcome regression (i.e., the conditional expectation of the outcome given the trial arm and covariates) and known propensity score (i.e., the conditional probability of being randomized to the intervention given the covariates) that minimizes the cross-validated variance estimate and, thereby, maximizes empirical efficiency. We discuss our approach for evaluating finite sample performance with parametric and plasmode simulations, pre-specifying the Statistical Analysis Plan, and unblinding in real-time on video conference with our colleagues from around the world. We present the results from applying our approach in the primary, pre-specified analysis of 8 recently published trials (2022-2024). We conclude with practical recommendations and an invitation to implement our approach in the primary analysis of your next trial."}
{"id": "2512.13622", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13622", "abs": "https://arxiv.org/abs/2512.13622", "authors": ["Hunter Chen", "Junming Guan", "Erik van Zwet", "Nikolaos Ignatiadis"], "title": "Empirical Bayes learning from selectively reported confidence intervals", "comment": null, "summary": "We develop a statistical framework for empirical Bayes learning from selectively reported confidence intervals, applied here to provide context for interpreting results published in MEDLINE abstracts. A collection of 326,060 z-scores from MEDLINE abstracts (2000-2018) provides context for interpreting individual studies; we formalize this as an empirical Bayes task complicated by selection bias. We address selection bias through a selective tilting approach that extends empirical Bayes confidence intervals to truncated sampling mechanisms. Sign information is unreliable (a positive z-score need not indicate benefit, and investigators may choose contrast directions post hoc), so we work with absolute z-scores and identify only the distribution of absolute signal-to-noise ratios (SNRs). Our framework provides coverage guarantees for functionals including posterior estimands describing idealized replications and the symmetrized posterior mean, which we justify decision-theoretically as optimal among sign-equivariant (odd) estimators and minimax among priors inducing the same absolute SNR distribution."}
{"id": "2512.13629", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13629", "abs": "https://arxiv.org/abs/2512.13629", "authors": ["Adrien Orué", "Derek Dinart", "Laurent Billot", "Carine Bellera", "Virginie Rondeau"], "title": "A comparative overview of win ratio and joint frailty models for recurrent event endpoints with applications in oncology and cardiology", "comment": null, "summary": "Composite endpoints that combine recurrent non-fatal events with a terminal event are increasingly used in randomized clinical trials, yet conventional time-to-first event analyses may obscure clinically relevant information. We compared two statistical frameworks tailored to such endpoints: the joint frailty model (JFM) and the last-event assisted recurrent-event win ratio (LWR). The JFM specifies proportional hazards for the recurrent and terminal events linked through a shared frailty, yielding covariate-adjusted, component-specific hazard ratios that account for informative recurrences and dependence with death. The LWR is a nonparametric, prioritized pairwise comparison that incorporates all observed events over follow-up and summarizes a population-level benefit of treatment while respecting a pre-specified hierarchy between death and recurrences. We first assessed the performance of the methods using simulations that varied both the gamma-frailty variance and the event rates. Next, we investigated these two frameworks using practical clinical applications, to assess the performance of the methods and to estimate the sample size required to achieve adequate power. These two approaches delivered complementary insights. The JFM provided component-specific estimates, while the LWR led to a summary measure of treatment effect with direction. Power was systematically improved with JFM, which thus appeared as the most reliable approach for inference and sample size estimation. Methodological extensions of the LWR to appropriately handle censoring and to formalize causal estimands remain a promising direction for future research."}
{"id": "2512.12358", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12358", "abs": "https://arxiv.org/abs/2512.12358", "authors": ["Stéphanie M. van den Berg", "Ulrich Halekoh", "Sören Möller", "Andreas Kryger Jensen", "Jacob von Bornemann Hjelmborg"], "title": "Towards a pretrained deep learning estimator of the Linfoot informational correlation", "comment": "3 figures", "summary": "We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available."}
