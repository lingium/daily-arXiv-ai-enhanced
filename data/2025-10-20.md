<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 20]
- [stat.ME](#stat.ME) [Total: 9]
- [stat.AP](#stat.AP) [Total: 8]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [From Universal Approximation Theorem to Tropical Geometry of Multi-Layer Perceptrons](https://arxiv.org/abs/2510.15012)
*Yi-Shan Chu,Yueh-Cheng Kuo*

Main category: stat.ML

TL;DR: 本文通过热带几何视角重新审视通用逼近定理，提出了一种针对S型多层感知器的几何感知初始化方法，能够在初始化时就让决策边界与预设形状对齐。


<details>
  <summary>Details</summary>
Motivation: 连接热带几何视角与平滑MLP之间的桥梁，实现可解释的、形状驱动的初始化，避免使用ReLU架构。

Method: 利用热带几何理论，设计纯S型MLP，使其遵循UAT的有限和格式：仿射函数的移位和缩放S型函数的有限线性组合。

Result: 生成的模型在初始化时就能产生与预设形状对齐的决策边界，可以通过标准训练进一步优化。

Conclusion: 为热带几何视角与平滑MLP之间提供了实用的桥梁，实现了无需ReLU架构的可解释形状驱动初始化。

Abstract: We revisit the Universal Approximation Theorem(UAT) through the lens of the
tropical geometry of neural networks and introduce a constructive,
geometry-aware initialization for sigmoidal multi-layer perceptrons (MLPs).
Tropical geometry shows that Rectified Linear Unit (ReLU) networks admit
decision functions with a combinatorial structure often described as a tropical
rational, namely a difference of tropical polynomials. Focusing on planar
binary classification, we design purely sigmoidal MLPs that adhere to the
finite-sum format of UAT: a finite linear combination of shifted and scaled
sigmoids of affine functions. The resulting models yield decision boundaries
that already align with prescribed shapes at initialization and can be refined
by standard training if desired. This provides a practical bridge between the
tropical perspective and smooth MLPs, enabling interpretable, shape-driven
initialization without resorting to ReLU architectures. We focus on the
construction and empirical demonstrations in two dimensions; theoretical
analysis and higher-dimensional extensions are left for future work.

</details>


### [2] [The Coverage Principle: How Pre-training Enables Post-Training](https://arxiv.org/abs/2510.15020)
*Fan Chen,Audrey Huang,Noah Golowich,Sadhika Malladi,Adam Block,Jordan T. Ash,Akshay Krishnamurthy,Dylan J. Foster*

Main category: stat.ML

TL;DR: 本文从覆盖度角度解释预训练语言模型成功的原因，提出覆盖度原则并证明其比交叉熵损失能更好预测下游任务性能，同时提出了改进覆盖度的算法干预方法。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练语言模型在特定任务上表现出色，但预训练如何影响最终模型成功的原因仍不清楚。交叉熵损失作为预训练成功指标往往不能很好预测下游性能。

Method: 从覆盖度理论视角分析预训练模型，提出覆盖度原则解释下一个词预测如何隐式优化模型覆盖度，并开发了模型选择、梯度归一化和测试时解码等算法干预方法。

Result: 覆盖度比交叉熵损失能更快泛化，避免对序列长度等问题相关参数的虚假依赖，从而更好预测下游任务性能。

Conclusion: 覆盖度是理解和改进预训练语言模型性能的关键概念，为模型选择、训练优化和推理策略提供了理论基础和实用方法。

Abstract: Language models demonstrate remarkable abilities when pre-trained on large
text corpora and fine-tuned for specific tasks, but how and why pre-training
shapes the success of the final model remains poorly understood. Notably,
although pre-training success is often quantified by cross entropy loss,
cross-entropy can be a poor predictor of downstream performance. Instead, we
provide a theoretical perspective on this relationship through the lens of
\emph{coverage}, which quantifies the probability mass the pre-trained model
places on high-quality responses and which is necessary and sufficient for
post-training and test-time scaling methods such as Best-of-N to succeed. Our
main results develop an understanding of \emph{the coverage principle}, a
phenomenon whereby next-token prediction implicitly optimizes toward a model
with good coverage. In particular, we uncover a mechanism that explains the
power of coverage in predicting downstream performance: \emph{coverage
generalizes faster than cross entropy}, avoiding spurious dependence on
problem-dependent parameters such as the sequence length. We also study
practical algorithmic interventions with provable benefits for improving
coverage, including (i) model/checkpoint selection procedures, (ii) gradient
normalization schemes, and (iii) test-time decoding strategies.

</details>


### [3] [Reliable data clustering with Bayesian community detection](https://arxiv.org/abs/2510.15013)
*Magnus Neuman,Jelena Smiljanić,Martin Rosvall*

Main category: stat.ML

TL;DR: 本文提出使用基于最小描述长度原则的贝叶斯社区检测方法（度校正随机块模型和正则化映射方程）来改进传统聚类方法，在噪声条件下更可靠地检测模块化结构。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法（如层次聚类、k-means、WGCNA）缺乏原则性模型选择，容易受噪声影响。常用的稀疏化预处理方法引入任意阈值，会扭曲数据结构并导致不可靠结果。

Method: 采用基于最小描述长度原则的两种贝叶斯社区检测方法：度校正随机块模型和正则化映射方程，将稀疏化和聚类统一在原则性模型选择框架下。

Result: 在合成数据中，新方法在高噪声条件和较少样本下能检测到预设的聚类；在基因共表达数据中，正则化映射方程比WGCNA识别出更稳健且功能更一致的基因模块。

Conclusion: 贝叶斯社区检测为跨领域高维数据中的模块化结构检测提供了一个原则性和抗噪声的框架。

Abstract: From neuroscience and genomics to systems biology and ecology, researchers
rely on clustering similarity data to uncover modular structure. Yet widely
used clustering methods, such as hierarchical clustering, k-means, and WGCNA,
lack principled model selection, leaving them susceptible to noise. A common
workaround sparsifies a correlation matrix representation to remove noise
before clustering, but this extra step introduces arbitrary thresholds that can
distort the structure and lead to unreliable results. To detect reliable
clusters, we capitalize on recent advances in network science to unite
sparsification and clustering with principled model selection. We test two
Bayesian community detection methods, the Degree-Corrected Stochastic Block
Model and the Regularized Map Equation, both grounded in the Minimum
Description Length principle for model selection. In synthetic data, they
outperform traditional approaches, detecting planted clusters under high-noise
conditions and with fewer samples. Compared to WGCNA on gene co-expression
data, the Regularized Map Equation identifies more robust and functionally
coherent gene modules. Our results establish Bayesian community detection as a
principled and noise-resistant framework for uncovering modular structure in
high-dimensional data across fields.

</details>


### [4] [The Minimax Lower Bound of Kernel Stein Discrepancy Estimation](https://arxiv.org/abs/2510.15058)
*Jose Cribeiro-Ramallo,Agnideep Aich,Florian Kalinke,Ashit Baran Aich,Zoltán Szabó*

Main category: stat.ML

TL;DR: 本文确定了核斯坦差异（KSD）估计的极小极大下界为n^{-1/2}，证明了现有√n收敛KSD估计器的最优性。


<details>
  <summary>Details</summary>
Motivation: 现有KSD估计器都只能达到√n收敛速率，但缺乏理论证明这是最优收敛速率，需要确定KSD估计的极小极大下界。

Method: 使用两种互补的证明策略：第一种针对ℝ^d空间上的Langevin-Stein算子和高斯核，第二种针对一般域上的KSD估计。

Result: 证明了KSD估计的极小极大下界为n^{-1/2}，且对于高斯核，估计难度随维度d呈指数级增长。

Conclusion: 现有√n收敛的KSD估计器已达到最优收敛速率，解决了KSD估计的理论最优性问题。

Abstract: Kernel Stein discrepancies (KSDs) have emerged as a powerful tool for
quantifying goodness-of-fit over the last decade, featuring numerous successful
applications. To the best of our knowledge, all existing KSD estimators with
known rate achieve $\sqrt n$-convergence. In this work, we present two
complementary results (with different proof strategies), establishing that the
minimax lower bound of KSD estimation is $n^{-1/2}$ and settling the optimality
of these estimators. Our first result focuses on KSD estimation on $\mathbb
R^d$ with the Langevin-Stein operator; our explicit constant for the Gaussian
kernel indicates that the difficulty of KSD estimation may increase
exponentially with the dimensionality $d$. Our second result settles the
minimax lower bound for KSD estimation on general domains.

</details>


### [5] [The Tree-SNE Tree Exists](https://arxiv.org/abs/2510.15014)
*Jack Kendrick*

Main category: stat.ML

TL;DR: 本文提出了tree-SNE方法，通过在t-SNE中引入尺度参数来解决高维数据聚类中的尺度问题，创建(2+1)维嵌入空间，实现连续尺度变化的可视化。


<details>
  <summary>Details</summary>
Motivation: 解决非线性降维方法（如t-SNE、UMAP）在处理高维数据时面临的尺度问题，即如何根据任务需求选择适当的聚类尺度来区分不同层次的特征。

Method: 基于Robinson & Pierce-Hoffman的思想，利用t-SNE中的尺度对称性，将2维嵌入扩展为(2+1)维嵌入，其中额外参数控制尺度，形成t-SNE树结构。

Result: 证明了在除零测集外的所有初始条件下，最优嵌入对尺度参数具有连续性，即tree-SNE树存在。该方法可扩展到其他吸引-排斥方法。

Conclusion: tree-SNE通过引入尺度参数有效解决了聚类中的尺度问题，为不同任务需求提供了灵活的尺度选择，并在多个示例中得到验证。

Abstract: The clustering and visualisation of high-dimensional data is a ubiquitous
task in modern data science. Popular techniques include nonlinear
dimensionality reduction methods like t-SNE or UMAP. These methods face the
`scale-problem' of clustering: when dealing with the MNIST dataset, do we want
to distinguish different digits or do we want to distinguish different ways of
writing the digits? The answer is task dependent and depends on scale. We
revisit an idea of Robinson & Pierce-Hoffman that exploits an underlying
scaling symmetry in t-SNE to replace 2-dimensional with (2+1)-dimensional
embeddings where the additional parameter accounts for scale. This gives rise
to the t-SNE tree (short: tree-SNE). We prove that the optimal embedding
depends continuously on the scaling parameter for all initial conditions
outside a set of measure 0: the tree-SNE tree exists. This idea conceivably
extends to other attraction-repulsion methods and is illustrated on several
examples.

</details>


### [6] [Foresighted Online Policy Optimization with Interference](https://arxiv.org/abs/2510.15273)
*Liner Xiang,Jiayi Wang,Hengrui Cai*

Main category: stat.ML

TL;DR: 提出了FRONT方法，在上下文赌博机问题中考虑决策间的干扰效应，通过前瞻性策略平衡探索与利用，实现长期回报最大化。


<details>
  <summary>Details</summary>
Motivation: 现有上下文赌博机方法假设无干扰，但在实际场景中决策会相互影响，忽略干扰会导致短视策略和次优决策。

Method: FRONT方法采用序列化的探索和利用策略，考虑当前决策对后续决策和奖励的长期影响，管理干扰复杂性。

Result: 建立了在线估计器的尾界，推导了参数渐近分布，证明FRONT在两种不同定义下都能获得次线性遗憾界。

Conclusion: FRONT能有效处理干扰效应，通过仿真和酒店利润应用验证了其优越性能，实现稳健参数推断和遗憾最小化。

Abstract: Contextual bandits, which leverage the baseline features of sequentially
arriving individuals to optimize cumulative rewards while balancing exploration
and exploitation, are critical for online decision-making. Existing approaches
typically assume no interference, where each individual's action affects only
their own reward. Yet, such an assumption can be violated in many practical
scenarios, and the oversight of interference can lead to short-sighted policies
that focus solely on maximizing the immediate outcomes for individuals, which
further results in suboptimal decisions and potentially increased regret over
time. To address this significant gap, we introduce the foresighted online
policy with interference (FRONT) that innovatively considers the long-term
impact of the current decision on subsequent decisions and rewards. The
proposed FRONT method employs a sequence of exploratory and exploitative
strategies to manage the intricacies of interference, ensuring robust parameter
inference and regret minimization. Theoretically, we establish a tail bound for
the online estimator and derive the asymptotic distribution of the parameters
of interest under suitable conditions on the interference network. We further
show that FRONT attains sublinear regret under two distinct definitions,
capturing both the immediate and consequential impacts of decisions, and we
establish these results with and without statistical inference. The
effectiveness of FRONT is further demonstrated through extensive simulations
and a real-world application to urban hotel profits.

</details>


### [7] [Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression](https://arxiv.org/abs/2510.15337)
*Yeichan Kim,Ilmun Kim,Seyoung Park*

Main category: stat.ML

TL;DR: 提出了一种新颖的两步转移最小L2范数插值器方法，分析了其在转移学习中的风险权衡，并发现了免费午餐协变量偏移机制。


<details>
  <summary>Details</summary>
Motivation: 转移学习和过参数化模型在现代机器学习中都很重要，但两者的结合研究不足。本文旨在填补转移学习与最小L2范数插值器之间的研究空白。

Method: 提出两步转移MNI方法，开发数据驱动程序检测信息源，并引入集成方法整合多个信息性转移MNI。

Result: 理论分析表明该方法在特定条件下优于仅使用目标数据的MNI，实验验证了方法对模型和数据异质性的鲁棒性。

Conclusion: 转移MNI方法能够有效利用异构数据实现知识转移，在免费午餐协变量偏移机制下以有限成本获得性能提升。

Abstract: Transfer learning is a key component of modern machine learning, enhancing
the performance of target tasks by leveraging diverse data sources.
Simultaneously, overparameterized models such as the minimum-$\ell_2$-norm
interpolator (MNI) in high-dimensional linear regression have garnered
significant attention for their remarkable generalization capabilities, a
property known as benign overfitting. Despite their individual importance, the
intersection of transfer learning and MNI remains largely unexplored. Our
research bridges this gap by proposing a novel two-step Transfer MNI approach
and analyzing its trade-offs. We characterize its non-asymptotic excess risk
and identify conditions under which it outperforms the target-only MNI. Our
analysis reveals free-lunch covariate shift regimes, where leveraging
heterogeneous data yields the benefit of knowledge transfer at limited cost. To
operationalize our findings, we develop a data-driven procedure to detect
informative sources and introduce an ensemble method incorporating multiple
informative Transfer MNIs. Finite-sample experiments demonstrate the robustness
of our methods to model and data heterogeneity, confirming their advantage.

</details>


### [8] [Beyond PCA: Manifold Dimension Estimation via Local Graph Structure](https://arxiv.org/abs/2510.15141)
*Zelong Bi,Pierre Lafaye de Micheaux*

Main category: stat.ML

TL;DR: 提出了一个结合PCA和回归技术的流形维度估计框架，包含QE和TLS两种估计器，在合成和真实数据集上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 改进现有的局部PCA和曲率调整PCA方法，通过捕捉流形的局部图结构来更准确地估计内在维度

Method: 提出了一个通用框架，将PCA与回归技术结合，具体包括二次嵌入(QE)和总体最小二乘(TLS)两种估计器

Result: 在合成和真实数据集上的实验表明，这些方法具有竞争力，并且经常优于最先进的替代方法

Conclusion: 所提出的框架能够有效估计流形维度，QE和TLS估计器在性能上优于现有方法

Abstract: Local principal component analysis (Local PCA) has proven to be an effective
tool for estimating the intrinsic dimension of a manifold. More recently,
curvature-adjusted PCA (CA-PCA) has improved upon this approach by explicitly
accounting for the curvature of the underlying manifold, rather than assuming
local flatness. Building on these insights, we propose a general framework for
manifold dimension estimation that captures the manifold's local graph
structure by integrating PCA with regression-based techniques. Within this
framework, we introduce two representative estimators: quadratic embedding (QE)
and total least squares (TLS). Experiments on both synthetic and real-world
datasets demonstrate that these methods perform competitively with, and often
outperform, state-of-the-art alternatives.

</details>


### [9] [RankSEG-RMA: An Efficient Segmentation Algorithm via Reciprocal Moment Approximation](https://arxiv.org/abs/2510.15362)
*Zixun Wang,Ben Dai*

Main category: stat.ML

TL;DR: 提出RankSEG-RMA方法，通过互逆矩近似降低RankSEG的计算复杂度，从O(d log d)和O(d²)降至O(d)，同时保持性能，并支持非重叠分割场景。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割方法通常使用argmax或阈值处理，无法直接优化分割指标，而RankSEG虽然能优化Dice和IoU指标，但计算复杂度高且仅适用于重叠分割场景。

Method: 使用互逆矩近似(RMA)改进RankSEG，提出RankSEG-RMA，降低计算复杂度，并开发像素级评分函数以支持非重叠分割。

Result: RankSEG-RMA将RankSEG的计算时间从16.33秒减少到接近argmax规则的0.01秒，同时保持可比的性能表现。

Conclusion: RankSEG-RMA成功解决了RankSEG的计算复杂度和适用场景限制问题，为语义分割指标优化提供了高效实用的解决方案。

Abstract: Semantic segmentation labels each pixel in an image with its corresponding
class, and is typically evaluated using the Intersection over Union (IoU) and
Dice metrics to quantify the overlap between predicted and ground-truth
segmentation masks. In the literature, most existing methods estimate
pixel-wise class probabilities, then apply argmax or thresholding to obtain the
final prediction. These methods have been shown to generally lead to
inconsistent or suboptimal results, as they do not directly maximize
segmentation metrics. To address this issue, a novel consistent segmentation
framework, RankSEG, has been proposed, which includes RankDice and RankIoU
specifically designed to optimize the Dice and IoU metrics, respectively.
Although RankSEG almost guarantees improved performance, it suffers from two
major drawbacks. First, it is its computational expense-RankDice has a
complexity of O(d log d) with a substantial constant factor (where d represents
the number of pixels), while RankIoU exhibits even higher complexity O(d^2),
thus limiting its practical application. For instance, in LiTS, prediction with
RankSEG takes 16.33 seconds compared to just 0.01 seconds with the argmax rule.
Second, RankSEG is only applicable to overlapping segmentation settings, where
multiple classes can occupy the same pixel, which contrasts with standard
benchmarks that typically assume non-overlapping segmentation. In this paper,
we overcome these two drawbacks via a reciprocal moment approximation (RMA) of
RankSEG with the following contributions: (i) we improve RankSEG using RMA,
namely RankSEG-RMA, reduces the complexity of both algorithms to O(d) while
maintaining comparable performance; (ii) inspired by RMA, we develop a
pixel-wise score function that allows efficient implementation for
non-overlapping segmentation settings.

</details>


### [10] [Kernel Regression in Structured Non-IID Settings: Theory and Implications for Denoising Score Learning](https://arxiv.org/abs/2510.15363)
*Dechen Zhang,Zhenmei Shi,Yi Zhang,Yingyu Liang,Difan Zou*

Main category: stat.ML

TL;DR: 该论文首次系统研究了具有信号-噪声因果结构的非独立同分布数据的核岭回归泛化性能，提出了新的块分解方法，并应用于去噪分数学习。


<details>
  <summary>Details</summary>
Motivation: 现有核岭回归理论主要处理独立同分布数据，但现实数据往往存在结构化依赖关系，特别是在去噪分数学习等应用中，多个噪声观测源自共享的底层信号。

Method: 开发了一种新颖的块分解方法，能够对依赖数据进行精确的集中分析，推导出核岭回归的过剩风险界限。

Result: 获得了明确依赖于核谱、因果结构参数和采样机制（包括信号和噪声的相对样本大小）的过剩风险界限。

Conclusion: 该工作推进了核岭回归理论，同时为分析现代机器学习应用中的依赖数据提供了实用工具，特别是在去噪分数学习方面建立了泛化保证和采样指导。

Abstract: Kernel ridge regression (KRR) is a foundational tool in machine learning,
with recent work emphasizing its connections to neural networks. However,
existing theory primarily addresses the i.i.d. setting, while real-world data
often exhibits structured dependencies - particularly in applications like
denoising score learning where multiple noisy observations derive from shared
underlying signals. We present the first systematic study of KRR generalization
for non-i.i.d. data with signal-noise causal structure, where observations
represent different noisy views of common signals. By developing a novel
blockwise decomposition method that enables precise concentration analysis for
dependent data, we derive excess risk bounds for KRR that explicitly depend on:
(1) the kernel spectrum, (2) causal structure parameters, and (3) sampling
mechanisms (including relative sample sizes for signals and noises). We further
apply our results to denoising score learning, establishing generalization
guarantees and providing principled guidance for sampling noisy data points.
This work advances KRR theory while providing practical tools for analyzing
dependent data in modern machine learning applications.

</details>


### [11] [Recursive Inference for Heterogeneous Multi-Output GP State-Space Models with Arbitrary Moment Matching](https://arxiv.org/abs/2510.15390)
*Tengjie Zheng,Jilan Mei,Di Wu,Lin Cheng,Shengping Gong*

Main category: stat.ML

TL;DR: 提出了一种用于高斯过程状态空间模型(GPSSM)的递归学习方法，通过异质多输出核、诱导点管理和统一递归推理框架，实现了在强非线性和显著噪声下的高效在线学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统通常具有多通道和高度非线性动态特性，传统建模方法难以应对，需要开发能够在线学习这些系统的有效方法。

Method: 设计异质多输出核允许每个输出维度采用不同的核类型、超参数和输入变量；开发诱导点管理算法提高计算效率；推导统一的递归推理框架支持扩展卡尔曼滤波、无迹卡尔曼滤波和假设密度滤波等方法。

Result: 在合成和真实数据集上的实验表明，该方法仅用1/100的运行时间就能达到最先进离线GPSSM的精度，在强噪声下比最先进在线GPSSM精度提高约70%，同时仅使用1/20的运行时间。

Conclusion: 所提出的方法为具有多通道和高度非线性动态的系统提供了一种高效准确的在线学习解决方案，在计算效率和精度方面均优于现有方法。

Abstract: Accurate learning of system dynamics is becoming increasingly crucial for
advanced control and decision-making in engineering. However, real-world
systems often exhibit multiple channels and highly nonlinear transition
dynamics, challenging traditional modeling methods. To enable online learning
for these systems, this paper formulates the system as Gaussian process
state-space models (GPSSMs) and develops a recursive learning method. The main
contributions are threefold. First, a heterogeneous multi-output kernel is
designed, allowing each output dimension to adopt distinct kernel types,
hyperparameters, and input variables, improving expressiveness in
multi-dimensional dynamics learning. Second, an inducing-point management
algorithm enhances computational efficiency through independent selection and
pruning for each output dimension. Third, a unified recursive inference
framework for GPSSMs is derived, supporting general moment matching approaches,
including the extended Kalman filter (EKF), unscented Kalman filter (UKF), and
assumed density filtering (ADF), enabling accurate learning under strong
nonlinearity and significant noise. Experiments on synthetic and real-world
datasets show that the proposed method matches the accuracy of SOTA offline
GPSSMs with only 1/100 of the runtime, and surpasses SOTA online GPSSMs by
around 70% in accuracy under heavy noise while using only 1/20 of the runtime.

</details>


### [12] [Information Theory in Open-world Machine Learning Foundations, Frameworks, and Future Direction](https://arxiv.org/abs/2510.15422)
*Lin Wang*

Main category: stat.ML

TL;DR: 本文综述了信息论方法在开放世界机器学习中的应用，重点阐述熵、互信息和KL散度等核心概念如何为开放世界条件下的知识获取、不确定性抑制和风险控制提供数学语言。


<details>
  <summary>Details</summary>
Motivation: 开放世界机器学习领域缺乏统一的理论基础来量化不确定性、描述信息传递和解释动态非平稳环境中的学习适应性，需要建立信息论的理论框架。

Method: 将现有研究综合为三大研究轴：信息论开放集识别、信息驱动的新颖性发现和信息保留的持续学习，并讨论信息理论与可证明学习框架的理论联系。

Result: 建立了信息论与PAC贝叶斯边界、开放空间风险理论和因果信息流等可证明学习框架之间的理论联系，为可证明和可信赖的开放世界智能铺平道路。

Conclusion: 识别了关键开放问题和未来研究方向，包括信息风险量化、动态互信息边界开发、多模态信息融合以及信息理论与因果推理和世界模型学习的集成。

Abstract: Open world Machine Learning (OWML) aims to develop intelligent systems
capable of recognizing known categories, rejecting unknown samples, and
continually learning from novel information. Despite significant progress in
open set recognition, novelty detection, and continual learning, the field
still lacks a unified theoretical foundation that can quantify uncertainty,
characterize information transfer, and explain learning adaptability in
dynamic, nonstationary environments. This paper presents a comprehensive review
of information theoretic approaches in open world machine learning, emphasizing
how core concepts such as entropy, mutual information, and Kullback Leibler
divergence provide a mathematical language for describing knowledge
acquisition, uncertainty suppression, and risk control under open world
conditions. We synthesize recent studies into three major research axes:
information theoretic open set recognition enabling safe rejection of unknowns,
information driven novelty discovery guiding new concept formation, and
information retentive continual learning ensuring stable long term adaptation.
Furthermore, we discuss theoretical connections between information theory and
provable learning frameworks, including PAC Bayes bounds, open-space risk
theory, and causal information flow, to establish a pathway toward provable and
trustworthy open world intelligence. Finally, the review identifies key open
problems and future research directions, such as the quantification of
information risk, development of dynamic mutual information bounds, multimodal
information fusion, and integration of information theory with causal reasoning
and world model learning.

</details>


### [13] [Robust Optimization in Causal Models and G-Causal Normalizing Flows](https://arxiv.org/abs/2510.15458)
*Gabriele Visentin,Patrick Cheridito*

Main category: stat.ML

TL;DR: 本文证明了因果模型中的干预鲁棒优化问题在G-因果Wasserstein距离下是连续的，但在标准Wasserstein距离下可能不连续，强调了在数据增强时使用尊重因果结构的生成模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在数据增强时可能不尊重因果结构，导致在因果干预鲁棒优化任务中表现不佳。需要开发能够准确建模因果结构的生成方法。

Method: 提出了一种新的归一化流架构，满足因果结构模型的通用逼近性质，并能有效训练以最小化G-因果Wasserstein距离。

Result: 实验表明，该模型在因果回归和因果因子模型中的均值-方差投资组合优化的数据增强任务中，优于标准（非因果）生成模型。

Conclusion: 使用尊重因果结构的生成模型对于因果干预鲁棒优化的数据增强至关重要，提出的归一化流架构在这方面表现出色。

Abstract: In this paper, we show that interventionally robust optimization problems in
causal models are continuous under the $G$-causal Wasserstein distance, but may
be discontinuous under the standard Wasserstein distance. This highlights the
importance of using generative models that respect the causal structure when
augmenting data for such tasks. To this end, we propose a new normalizing flow
architecture that satisfies a universal approximation property for causal
structural models and can be efficiently trained to minimize the $G$-causal
Wasserstein distance. Empirically, we demonstrate that our model outperforms
standard (non-causal) generative models in data augmentation for causal
regression and mean-variance portfolio optimization in causal factor models.

</details>


### [14] [Online Policy Learning via a Self-Normalized Maximal Inequality](https://arxiv.org/abs/2510.15483)
*Samuel Girard,Aurélien Bibaut,Houssam Zenati*

Main category: stat.ML

TL;DR: 提出了一个针对鞅经验过程的自归一化极大不等式，并基于此开发了适用于一般依赖数据的自适应样本方差惩罚方法，以及方差正则化的悲观离策略学习目标，在标准和复杂性条件下实现了比1/√n基线更快的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 自适应实验产生的依赖数据打破了经典集中不等式所基于的i.i.d.假设，使标准学习保证失效，需要开发适用于依赖数据的理论框架。

Method: 开发了鞅经验过程的自归一化极大不等式，提出了自适应样本方差惩罚程序，并推导了方差正则化的悲观离策略学习目标。

Result: 在标准和复杂性条件下，结合顺序更新，所得估计器在参数和非参数机制中均实现了比1/√n基线更快的收敛速率。

Conclusion: 该方法为依赖数据提供了有效的学习保证，并通过数值模拟验证了实际性能提升。

Abstract: Adaptive experiments produce dependent data that break i.i.d. assumptions
that underlie classical concentration bounds and invalidate standard learning
guarantees. In this paper, we develop a self-normalized maximal inequality for
martingale empirical processes. Building on this, we first propose an adaptive
sample-variance penalization procedure which balances empirical loss and sample
variance, valid for general dependent data. Next, this allows us to derive a
new variance-regularized pessimistic off-policy learning objective, for which
we establish excess-risk guarantees. Subsequently, we show that, when combined
with sequential updates and under standard complexity and margin conditions,
the resulting estimator achieves fast convergence rates in both parametric and
nonparametric regimes, improving over the usual $1/\sqrt{n}$
  baseline. We complement our theoretical findings with numerical simulations
that illustrate the practical gains of our approach.

</details>


### [15] [Geometric Convergence Analysis of Variational Inference via Bregman Divergences](https://arxiv.org/abs/2510.15548)
*Sushil Bohara,Amedeo Roberto Esposito*

Main category: stat.ML

TL;DR: 该论文提出了一个基于指数族分布的变分推断收敛性分析理论框架，通过将负ELBO表示为Bregman散度，证明了梯度下降算法的非渐近收敛率。


<details>
  <summary>Details</summary>
Motivation: 变分推断虽然提供了可扩展的贝叶斯推断框架，但由于ELBO目标在欧几里得空间中的非凸性和非光滑性，其收敛性分析一直具有挑战性。

Method: 利用指数族分布结构，将负ELBO表示为关于对数配分函数的Bregman散度，从而对优化景观进行几何分析，并推导参数空间中沿射线的目标函数界限。

Result: 建立了Bregman表示的弱单调性性质，在Fisher信息矩阵谱特性控制下，证明了梯度下降算法在恒定和递减步长下的非渐近收敛率。

Conclusion: 该几何框架为变分推断的收敛性分析提供了严格的理论基础，克服了传统欧几里得空间分析的局限性。

Abstract: Variational Inference (VI) provides a scalable framework for Bayesian
inference by optimizing the Evidence Lower Bound (ELBO), but convergence
analysis remains challenging due to the objective's non-convexity and
non-smoothness in Euclidean space. We establish a novel theoretical framework
for analyzing VI convergence by exploiting the exponential family structure of
distributions. We express negative ELBO as a Bregman divergence with respect to
the log-partition function, enabling a geometric analysis of the optimization
landscape. We show that this Bregman representation admits a weak monotonicity
property that, while weaker than convexity, provides sufficient structure for
rigorous convergence analysis. By deriving bounds on the objective function
along rays in parameter space, we establish properties governed by the spectral
characteristics of the Fisher information matrix. Under this geometric
framework, we prove non-asymptotic convergence rates for gradient descent
algorithms with both constant and diminishing step sizes.

</details>


### [16] [Kernel-Based Evaluation of Conditional Biological Sequence Models](https://arxiv.org/abs/2510.15601)
*Pierre Glaser,Steffanie Paul,Alissa M. Hummer,Charlotte M. Deane,Debora S. Marks,Alan N. Amin*

Main category: stat.ML

TL;DR: 提出了基于核的工具集，用于评估条件序列模型的设计和调整超参数，特别关注计算生物学问题。核心是新的差异度量ACMMD，可无偏估计模型拟合度，进行假设检验和评估模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 需要有效的方法来评估和优化条件序列模型在计算生物学中的表现，特别是量化模型与真实条件分布之间的差异。

Method: 使用增强条件最大均值差异(ACMMD)作为核心度量，该度量可以从数据中无偏估计，用于假设检验和模型可靠性评估。

Result: 通过分析ProteinMPNN蛋白质设计模型，能够拒绝模型拟合数据的假设，并通过调整温度超参数实现更好的拟合。

Conclusion: ACMMD提供了一种有效的工具来评估条件序列模型的拟合质量，并指导超参数调优，在计算生物学中具有实用价值。

Abstract: We propose a set of kernel-based tools to evaluate the designs and tune the
hyperparameters of conditional sequence models, with a focus on problems in
computational biology. The backbone of our tools is a new measure of
discrepancy between the true conditional distribution and the model's estimate,
called the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided
that the model can be sampled from, the ACMMD can be estimated unbiasedly from
data to quantify absolute model fit, integrated within hypothesis tests, and
used to evaluate model reliability. We demonstrate the utility of our approach
by analyzing a popular protein design model, ProteinMPNN. We are able to reject
the hypothesis that ProteinMPNN fits its data for various protein families, and
tune the model's temperature hyperparameter to achieve a better fit.

</details>


### [17] [Disentanglement of Sources in a Multi-Stream Variational Autoencoder](https://arxiv.org/abs/2510.15669)
*Veranika Boukun,Jörg Lücke*

Main category: stat.ML

TL;DR: 提出多流变分自编码器(MS-VAE)，使用离散潜在变量结合多个VAE表示，通过线性组合模型分离混合源，在数字分离和说话人日志任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统VAE使用单个连续潜在空间学习解缠表示，本文探索使用离散潜在变量结合多个VAE表示来分离混合源的新方法。

Method: 基于源组合的显式模型（线性组合模型），推导MS-VAE的推理和学习方程，使用离散潜在变量组合多个VAE流。

Result: 在叠加手写数字分离任务中观察到清晰分离效果，在说话人日志任务中表现出极低的说话人漏检率，在不同监督程度和训练数据量下都展现灵活性。

Conclusion: MS-VAE方法具有领域无关性，能够有效分离混合源，特别在说话人日志任务中表现突出，展示了该方法在源分离任务中的潜力。

Abstract: Variational autoencoders (VAEs) are a leading approach to address the problem
of learning disentangled representations. Typically a single VAE is used and
disentangled representations are sought in its continuous latent space. Here we
explore a different approach by using discrete latents to combine
VAE-representations of individual sources. The combination is done based on an
explicit model for source combination, and we here use a linear combination
model which is well suited, e.g., for acoustic data. We formally define such a
multi-stream VAE (MS-VAE) approach, derive its inference and learning
equations, and we numerically investigate its principled functionality. The
MS-VAE is domain-agnostic, and we here explore its ability to separate sources
into different streams using superimposed hand-written digits, and mixed
acoustic sources in a speaker diarization task. We observe a clear separation
of digits, and on speaker diarization we observe an especially low rate of
missed speakers. Numerical experiments further highlight the flexibility of the
approach across varying amounts of supervision and training data.

</details>


### [18] [On Universality of Deep Equivariant Networks](https://arxiv.org/abs/2510.15814)
*Marco Pacini,Mircea Petrache,Bruno Lepri,Shubhendu Trivedi,Robin Walters*

Main category: stat.ML

TL;DR: 该论文建立了更通用的等变神经网络普适性理论，证明了在分离约束条件下，通过添加全连接读出层或增加网络深度，等变神经网络可以实现普适性逼近。


<details>
  <summary>Details</summary>
Motivation: 现有的等变神经网络普适性结果大多局限于特定设置：要么依赖高维张量表示导致隐藏空间维度爆炸，要么针对专门架构且通常局限于不变设置。需要发展更通用的理论框架。

Method: 对于不变网络，在分离约束条件下建立普适性定理；对于等变网络，引入更严格的逐项可分性标准，并证明通过增加深度或添加适当读出层可以实现普适性。

Result: 证明了在分离约束条件下，不变网络通过添加全连接读出层可以逼近分离约束连续函数类；等变网络在逐项可分性机制下，通过足够深度或适当读出层可以实现普适性。

Conclusion: 深度和读出层是实现等变神经网络普适性的关键机制，该研究提供了一个统一视角，包含并扩展了早期的专门结果。

Abstract: Universality results for equivariant neural networks remain rare. Those that
do exist typically hold only in restrictive settings: either they rely on
regular or higher-order tensor representations, leading to impractically
high-dimensional hidden spaces, or they target specialized architectures, often
confined to the invariant setting. This work develops a more general account.
For invariant networks, we establish a universality theorem under separation
constraints, showing that the addition of a fully connected readout layer
secures approximation within the class of separation-constrained continuous
functions. For equivariant networks, where results are even scarcer, we
demonstrate that standard separability notions are inadequate and introduce the
sharper criterion of $\textit{entry-wise separability}$. We show that with
sufficient depth or with the addition of appropriate readout layers,
equivariant networks attain universality within the entry-wise separable
regime. Together with prior results showing the failure of universality for
shallow models, our findings identify depth and readout layers as a decisive
mechanism for universality, additionally offering a unified perspective that
subsumes and extends earlier specialized results.

</details>


### [19] [Error analysis of a compositional score-based algorithm for simulation-based inference](https://arxiv.org/abs/2510.15817)
*Camille Touron,Gabriel V. Cardoso,Julyan Arbel,Pedro L. C. Rodrigues*

Main category: stat.ML

TL;DR: 本文研究了基于扩散的模拟推断中组合评分方法的误差累积问题，为GAUSS算法建立了误差上界。


<details>
  <summary>Details</summary>
Motivation: 在基于模拟的推断中，如何有效结合多个观测来改进参数推断是一个核心问题。虽然扩散方法通过聚合个体后验评分来解决这个问题，但个体误差累积对采样质量的影响尚未得到理论分析。

Method: 研究GAUSS算法产生的组合评分，建立其均方误差的上界，该上界与个体评分误差和观测数量相关。

Result: 在可解析推导的高斯示例中验证了理论发现，展示了组合评分方法的误差累积特性。

Conclusion: 为基于扩散的模拟推断中组合评分方法提供了理论保证，揭示了误差随观测数量增长的变化规律。

Abstract: Simulation-based inference (SBI) has become a widely used framework in
applied sciences for estimating the parameters of stochastic models that best
explain experimental observations. A central question in this setting is how to
effectively combine multiple observations in order to improve parameter
inference and obtain sharper posterior distributions. Recent advances in
score-based diffusion methods address this problem by constructing a
compositional score, obtained by aggregating individual posterior scores within
the diffusion process. While it is natural to suspect that the accumulation of
individual errors may significantly degrade sampling quality as the number of
observations grows, this important theoretical issue has so far remained
unexplored. In this paper, we study the compositional score produced by the
GAUSS algorithm of Linhart et al. (2024) and establish an upper bound on its
mean squared error in terms of both the individual score errors and the number
of observations. We illustrate our theoretical findings on a Gaussian example,
where all analytical expressions can be derived in a closed form.

</details>


### [20] [Blackwell's Approachability for Sequential Conformal Inference](https://arxiv.org/abs/2510.15824)
*Guillaume Principato,Gilles Stoltz*

Main category: stat.ML

TL;DR: 该论文使用Blackwell方法理论研究了非交换环境下的保形推断，将自适应保形推断重新表述为重复向量值博弈，并设计了校准方法来实现覆盖率和效率目标。


<details>
  <summary>Details</summary>
Motivation: 研究非交换环境下的保形推断问题，通过博弈论视角分析覆盖率和效率之间的权衡关系。

Method: 将自适应保形推断重新表述为重复两玩家向量值有限博弈，使用Blackwell方法理论设计校准策略。

Result: 提出的算法具有强大的理论保证，提供了实用的见解，但计算负担可能限制实际部署。

Conclusion: 通过博弈论方法成功分析了保形推断在非交换环境中的表现，建立了理论框架但存在计算挑战。

Abstract: We study conformal inference in non-exchangeable environments through the
lens of Blackwell's theory of approachability. We first recast adaptive
conformal inference (ACI, Gibbs and Cand\`es, 2021) as a repeated two-player
vector-valued finite game and characterize attainable coverage--efficiency
tradeoffs. We then construct coverage and efficiency objectives under potential
restrictions on the adversary's play, and design a calibration-based
approachability strategy to achieve these goals. The resulting algorithm enjoys
strong theoretical guarantees and provides practical insights, though its
computational burden may limit deployment in practice.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [21] [Adaptive Influence Diagnostics in High-Dimensional Regression](https://arxiv.org/abs/2510.15618)
*Abdul-Nasah Soale,Adewale Lukman*

Main category: stat.ME

TL;DR: 提出了一种自适应Cook距离（ACD）方法，用于诊断高维单指标模型中存在多重共线性和异常值污染时的影响观测点。ACD基于稀疏局部线性梯度构建，能够缓解杠杆效应。


<details>
  <summary>Details</summary>
Motivation: 在高维单指标模型中，多重共线性和异常值污染会影响模型诊断的准确性，传统方法如经典Cook距离和局部影响分析在这种情况下表现不佳，需要开发更稳健的诊断工具。

Method: ACD是一种模型无关技术，基于稀疏局部线性梯度构建，使用LASSO（ACD-LASSO）和SCAD（ACD-SCAD）惩罚项来缓解杠杆效应。该方法通过修剪被ACD标记的点来稳定变量选择。

Result: 在包含强相关性的低维和高维设计设置的模拟中，ACD-LASSO和ACD-SCAD相对于经典Cook距离、局部影响分析以及LASSO的DF-Model和Case-Weight调整解决方案，减少了掩蔽和淹没现象。修剪ACD标记的点能够稳定变量选择同时保留核心信号。

Conclusion: 在两个数据集（1960年美国城市污染研究和核黄素基因组学实验）的应用表明，ACD方法在变量选择稳定性和可解释性方面具有一致的优势。

Abstract: An adaptive Cook's distance (ACD) for diagnosing influential observations in
high-dimensional single-index models with multicollinearity and outlier
contamination is proposed. ACD is a model-free technique built on sparse local
linear gradients to temper leverage effects. In simulations spanning low- and
high-dimensional design settings with strong correlation, ACD based on LASSO
(ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative
to classical Cook's distance and local influence as well as the DF-Model and
Case-Weight adjusted solution for LASSO. Trimming points flagged by ACD
stabilizes variable selection while preserving core signals. Applications to
two datasets--the 1960 US cities pollution study and a high-dimensional
riboflavin genomics experiment show consistent gains in selection stability and
interpretability.

</details>


### [22] [Asymptotic distribution of the global clustering coefficient in a random annulus graph](https://arxiv.org/abs/2510.15003)
*Mingao Yuan*

Main category: stat.ME

TL;DR: 本文研究了随机环面图中全局聚类系数的渐近分布，证明标准化后的全局聚类系数收敛于标准正态分布，并给出了其极限的显式表达式。


<details>
  <summary>Details</summary>
Motivation: 全局聚类系数是分析复杂网络结构的有效度量，随机环面图是Erdős-Rényi随机图的改进版本，最近被提出用于建模网络社区。

Method: 使用样本大小依赖核的退化U统计量渐近理论来推导网络统计量的渐近分布，该方法与现有方法不同。

Result: 证明了标准化全局聚类系数依分布收敛于标准正态分布，并获得了全局聚类系数极限的显式表达式。

Conclusion: 该研究为随机环面图中全局聚类系数的统计性质提供了理论保证，并展示了新的渐近分布推导方法。

Abstract: The global clustering coefficient is an effective measure for analyzing and
comparing the structures of complex networks. The random annulus graph is a
modified version of the well-known Erd\H{o}s-R\'{e}nyi random graph. It has
been recently proposed in modeling network communities. This paper investigates
the asymptotic distribution of the global clustering coefficient in a random
annulus graph. It is demonstrated that the standardized global clustering
coefficient converges in law to the standard normal distribution. The result is
established using the asymptotic theory of degenerate U-statistics with a
sample-size dependent kernel. As far as we know, this method is different from
established approaches for deriving asymptotic distributions of network
statistics. Moreover, we get the explicit expression of the limit of the global
clustering coefficient.

</details>


### [23] [Estimand framework and intercurrent events handling for clinical trials with time-to-event outcomes](https://arxiv.org/abs/2510.15000)
*Yixin Fang,Man Jin*

Main category: stat.ME

TL;DR: 本文扩展了ICH E9(R1)指南，针对时间到事件结局提出了六个处理并发事件的策略，包括五个ICH策略和一个新的竞争风险策略，并在潜在结局框架下定义估计目标。


<details>
  <summary>Details</summary>
Motivation: ICH E9(R1)指南虽然为定量和分类结局提供了并发事件处理框架，但对时间到事件结局的讨论不足，需要填补这一空白。

Method: 使用潜在结局框架定义估计目标，讨论六个并发事件处理策略（包括新的竞争风险策略），并利用时间依赖性协变量开发高效估计方法。

Result: 提出了针对时间到事件结局的完整估计目标框架，扩展了ICH E9(R1)指南的适用范围，并提供了相应的统计方法。

Conclusion: 本文为时间到事件结局的临床试验提供了完整的估计目标定义和并发事件处理策略，弥补了ICH E9(R1)指南的不足，具有重要的方法论价值。

Abstract: The ICH E9(R1) guideline presents a framework of estimand for clinical
trials, proposes five strategies for handling intercurrent events (ICEs), and
provides a comprehensive discussion and many real-life clinical examples for
quantitative outcomes and categorical outcomes. However, in ICH E9(R1) the
discussion is lacking for time-to-event (TTE) outcomes. In this paper, we
discuss how to define estimands and how to handle ICEs for clinical trials with
TTE outcomes. Specifically, we discuss six ICE handling strategies, including
those five strategies proposed by ICH E9(R1) and a new strategy, the
competing-risk strategy. Compared with ICH E9(R1), the novelty of this paper is
three-fold: (1) the estimands are defined in terms of potential outcomes, (2)
the methods can utilize time-dependent covariates straightforwardly, and (3)
the efficient estimators are discussed accordingly.

</details>


### [24] [Conditional GLMMs for reaction times in choice tasks](https://arxiv.org/abs/2510.15203)
*Mauricio Tejo,Cristian Meza,Fernando Marmolejo-Ramos*

Main category: stat.ME

TL;DR: 该研究将单障碍扩散模型的首达时间与广义线性混合模型(GLMMs)连接起来，通过分析条件反应时分布来统一这两种反应时建模方法。


<details>
  <summary>Details</summary>
Motivation: 连接扩散模型和GLMMs这两种反应时建模方法，建立认知过程与统计模型之间的桥梁。

Method: 分析条件反应时分布，利用扩散模型变体产生的逆高斯和Gamma分布来构建GLMMs，通过仿真和真实数据验证方法。

Result: 成功建立了扩散模型与GLMMs之间的连接，证明了使用逆高斯和Gamma分布在GLMMs中可以推断底层认知过程。

Conclusion: 该方法为反应时建模提供了统一的框架，能够从统计模型推断认知机制，并具有扩展潜力。

Abstract: This study connects two methods for modeling reaction times (RTs) in choice
tasks: (1) the first-hitting time of a simple diffusion model with a single
barrier, representing the cognitive process leading to a response, and (2)
Generalized Linear Mixed Models (GLMMs). We achieve this by analyzing RT
distributions conditioned on each response alternative. Because certain
diffusion model variants yield Inverse Gaussian (IG) and Gamma distributions
for first-hitting times, we can justify using these distributions in RT models.
Conversely, employing IG and Gamma distributions within GLMMs allows us to
infer the underlying cognitive processes. We demonstrate this concept through
simulations and apply it to previously published real-world data. Finally, we
discuss the scope and potential extensions of our approach.

</details>


### [25] [Bayesian Sequential Modeling of Time-to-Urination for Dynamic ED Triage](https://arxiv.org/abs/2510.15272)
*Atsushi Senda,Yuki Takatsu,Ryokan Ikebe,Hiroshi Suginaka,Koji Morishita,Akira Endo*

Main category: stat.ME

TL;DR: 开发了一个贝叶斯顺序更新框架，整合实时行为线索（如首次排尿时间）来预测急诊患者入院风险，在单中心前瞻性队列中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 常规急诊分诊工具大多是静态的，未能利用临床医生实时观察到的简单行为线索，需要开发能够整合这些线索的动态风险评估框架。

Method: 使用贝叶斯顺序更新框架，整合传入的行为线索，以首次排尿时间作为概念验证线索，在日本某救护车到达患者的前瞻性队列中进行评估。

Result: 在人群水平上对累积入院曲线的拟合效果优异，患者水平上经过年龄/性别调整后性能显著提升，Platt重新校准改进了概率标度，决策曲线分析显示在常见阈值下具有小的有利净收益。

Conclusion: 该框架易于扩展到多模态输入和外部验证，旨在补充而非替代现有的分诊系统。

Abstract: Triage tools in routine emergency care are largely static, failing to exploit
simple behavioral cues clinicians notice in real time. Here, we developed a
Bayesian, sequentially updating framework that integrates incoming cues to
produce calibrated, time-consistent risk. Using a prospective single-center
cohort of ambulance arrivals in Japan (February-August 2025; n=2,221), we
evaluated time to first urination (TTU) as a proof-of-concept bedside cue for
predicting hospital admission. Population-level fit to the cumulative admission
curve was excellent (integrated squared error 0.002; RMSE 0.003;
Kolmogorov-Smirnov 0.008; coverage 0.98). At the patient level, performance
improved markedly with age/sex adjustment (AUC[t] 0.70 vs. 0.50 unadjusted),
with lower Brier scores and positive calibration slopes. Platt recalibration
refined probability scaling without altering discrimination, and decision-curve
analysis showed small, favorable net benefit at common thresholds. This
framework is readily extensible to multimodal inputs and external validation
and is designed to complement, not replace, existing triage systems.

</details>


### [26] [Nonparametric Testing of Spatial Dependence in 2D and 3D Random Fields](https://arxiv.org/abs/2510.15381)
*Christian H. Weiß,Philipp Adämmer*

Main category: stat.ME

TL;DR: 提出了一种使用希尔伯特曲线将空间数据转换为一维时间序列，然后应用序数模式检验空间依赖性的非参数框架


<details>
  <summary>Details</summary>
Motivation: 现有基于空间序数模式的方法通常局限于二维设置，需要一种灵活且鲁棒的非参数框架来检验二维和三维随机场的空间依赖性

Method: 使用空间填充希尔伯特曲线将空间数据转换为一维时间序列，然后应用序数模式检验序列依赖性

Result: 该方法易于实现，通过广义希尔伯特曲线适应任意网格大小，并自然扩展到三维以上

Conclusion: 这为现有方法提供了一个实用且通用的替代方案

Abstract: We propose a flexible and robust nonparametric framework for testing spatial
dependence in two- and three-dimensional random fields. Our approach involves
converting spatial data into one-dimensional time series using space-filling
Hilbert curves. We then apply ordinal pattern-based tests for serial dependence
to this series. Because Hilbert curves preserve spatial locality, spatial
dependence in the original field manifests as serial dependence in the
transformed sequence. The approach is easy to implement, accommodates arbitrary
grid sizes through generalized Hilbert (``gilbert'') curves, and naturally
extends beyond three dimensions. This provides a practical and general
alternative to existing methods based on spatial ordinal patterns, which are
typically limited to two-dimensional settings.

</details>


### [27] [Robust Estimation of Polyserial Correlation](https://arxiv.org/abs/2510.15632)
*Max Welz*

Main category: stat.ME

TL;DR: 本文提出了一种针对多序列相关模型的稳健估计方法，能够有效处理模型误设问题，如异常值和不认真回答，同时保持高统计效率。


<details>
  <summary>Details</summary>
Motivation: 传统的多序列相关模型基于部分潜变量正态性假设，但在实践中容易因异常值等原因导致模型误设。最大似然估计对此非常敏感，单个异常观测就可能导致估计结果严重偏差。

Method: 提出了一种新的稳健估计器，通过隐式降低与模型不符的观测值的权重来实现稳健性。该方法推广了最大似然估计，具有一致性和渐近正态性。

Result: 模拟实验和人格心理学实证应用表明，该估计器具有很好的稳健性，能够识别异常值，同时保持了超过98%的最大似然估计效率。

Conclusion: 所提出的稳健估计方法能够有效处理多序列相关模型中的异常值问题，在牺牲少量效率的情况下获得显著的稳健性提升，并已实现为开源软件。

Abstract: The association between a continuous and an ordinal variable is commonly
modeled through the polyserial correlation model. However, this model, which is
based on a partially-latent normality assumption, may be misspecified in
practice, due to, for example (but not limited to), outliers or careless
responses. We demonstrate that the typically used maximum likelihood (ML)
estimator is highly susceptible to such misspecification: One single
observation not generated by partially-latent normality can suffice to produce
arbitrarily poor estimates. As a remedy, we propose a novel estimator of the
polyserial correlation model designed to be robust against the adverse effects
of observations discrepant to that model. The estimator achieves robustness by
implicitly downweighting such observations; the ensuing weights constitute a
useful tool for pinpointing potential sources of model misspecification. We
show that the proposed estimator generalizes ML and is consistent as well as
asymptotically Gaussian. As price for robustness, some efficiency must be
sacrificed, but substantial robustness can be gained while maintaining more
than 98% of ML efficiency. We demonstrate our estimator's robustness and
practical usefulness in simulation experiments and an empirical application in
personality psychology where our estimator helps identify outliers. Finally,
the proposed methodology is implemented in free open-source software.

</details>


### [28] [Bayesian Inference for PDE-based Inverse Problems using the Optimization of a Discrete Loss](https://arxiv.org/abs/2510.15664)
*Lucas Amoudruz,Sergey Litvinov,Costas Papadimitriou,Petros Koumoutsakos*

Main category: stat.ME

TL;DR: B-ODIL是ODIL方法的贝叶斯扩展，将PDE损失作为先验知识，结合数据似然来求解逆问题，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 逆问题在科学、工程和医学中至关重要，但测量数据通常不完整或间接，需要额外知识来准确求解。物理模型（PDEs）是弥补这一差距的有效方法。

Method: B-ODIL将ODIL的PDE损失作为先验知识，与数据似然结合，采用贝叶斯公式求解PDE逆问题，并量化不确定性。

Result: 在一维、二维和三维PDE的合成基准测试中验证了B-ODIL的能力，并在脑肿瘤浓度估计的实际应用中展示了其有效性。

Conclusion: B-ODIL成功扩展了ODIL方法，提供了贝叶斯框架下的逆问题求解和不确定性量化，在合成和实际应用中均表现出色。

Abstract: Inverse problems are crucial for many applications in science, engineering
and medicine that involve data assimilation, design, and imaging. Their
solution infers the parameters or latent states of a complex system from noisy
data and partially observable processes. When measurements are an incomplete or
indirect view of the system, additional knowledge is required to accurately
solve the inverse problem. Adopting a physical model of the system in the form
of partial differential equations (PDEs) is a potent method to close this gap.
In particular, the method of optimizing a discrete loss (ODIL) has shown great
potential in terms of robustness and computational cost. In this work, we
introduce B-ODIL, a Bayesian extension of ODIL, that integrates the PDE loss of
ODIL as prior knowledge and combines it with a likelihood describing the data.
B-ODIL employs a Bayesian formulation of PDE-based inverse problems to infer
solutions with quantified uncertainties. We demonstrate the capabilities of
B-ODIL in a series of synthetic benchmarks involving PDEs in one, two, and
three dimensions. We showcase the application of B-ODIL in estimating tumor
concentration and its uncertainty in a patient's brain from MRI scans using a
three-dimensional tumor growth model.

</details>


### [29] [A Multiclass ROC Curve](https://arxiv.org/abs/2510.15670)
*Paolo Giudici,Rosa C. Rosciano,Johanna Schrader,Delf-Magnus Kummerfeld*

Main category: stat.ME

TL;DR: 提出了一种基于多维基尼指数构建多类ROC曲线的新方法，将基尼系数与ROC曲线的关系扩展到多类场景，并在医疗和金融领域进行了验证。


<details>
  <summary>Details</summary>
Motivation: 解决多类性能评估的理论基础问题，特别针对不平衡数据集，强调谨慎评估应优先于类别频率考虑。

Method: 利用基尼系数与ROC曲线之间的已知关系，通过多维基尼指数扩展到多类设置。

Result: 通过医疗和金融两个综合案例研究验证了该框架的有效性。

Conclusion: 为多类性能评估提供了一个理论上有依据的解决方案，尤其适用于不平衡数据集。

Abstract: This paper introduces a novel methodology for constructing multiclass ROC
curves using the multidimensional Gini index. The proposed methodology
leverages the established relationship between the Gini coefficient and the ROC
Curve and extends it to multiclass settings through the multidimensional Gini
index. The framework is validated by means of two comprehensive case studies in
health care and finance. The paper provides a theoretically grounded solution
to multiclass performance evaluation, particularly valuable for imbalanced
datasets, for which a prudential assessment should take precedence over class
frequency considerations.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [30] [Data-driven Calibration Sample Selection and Forecast Combination in Electricity Price Forecasting: An Application of the ARHNN Method](https://arxiv.org/abs/2510.15011)
*Tomasz Serafin,Weronika Nitka*

Main category: stat.AP

TL;DR: 将校准样本选择和预测组合结合到ARHNN方法中，显著提高了电力价格预测精度，同时提出了计算效率更高的简化版本，并在电池储能交易案例中验证了实际商业价值。


<details>
  <summary>Details</summary>
Motivation: 虽然校准样本选择和预测组合在预测领域已被证明有效，但在电力价格预测领域仍未被充分探索，本研究旨在填补这一空白。

Method: 使用自回归混合最近邻(ARHNN)方法，结合校准样本选择和预测组合技术，应用于德国、西班牙和新英格兰电力市场的长期时间序列数据。

Result: ARHNN方法在预测精度上比流行基准方法提高达10%，提出的简化版本大幅降低计算时间且仅轻微损失精度，在电池储能交易案例中实现了理论最大利润的80%。

Conclusion: 该方法在电力价格预测中表现出色，简化版本在计算效率和精度间取得良好平衡，在实际应用中具有显著商业价值。

Abstract: Calibration sample selection and forecast combination are two simple yet
powerful tools used in forecasting. They can be combined with a variety of
models to significantly improve prediction accuracy, at the same time offering
easy implementation and low computational complexity. While their effectiveness
has been repeatedly confirmed in prior scientific literature, the topic is
still underexplored in the field of electricity price forecasting. In this
research article we apply the Autoregressive Hybrid Nearest Neighbors (ARHNN)
method to three long-term time series describing the German, Spanish and New
England electricity markets. We show that it outperforms popular literature
benchmarks in terms of forecast accuracy by up to 10%. We also propose two
simplified variants of the method, granting a vast decrease in computation time
with only minor loss of prediction accuracy. Finally, we compare the forecasts'
performance in a battery storage system trading case study. We find that using
a forecast-driven strategy can achieve up to 80% of theoretical maximum profits
while trading, demonstrating business value in practical applications.

</details>


### [31] [Bayesian Additive Regression Trees (BART) in Food Authenticity: A Classification Approach to Food Fraud Detection](https://arxiv.org/abs/2510.15105)
*Mengxiang Zhu,Riccardo Rastelli*

Main category: stat.AP

TL;DR: 本研究使用贝叶斯加性回归树（BART）进行橄榄油纯度分类，通过其内置变量选择机制识别关键光谱特征，在PCA降维后达到96.8%的分类准确率，调参后提升至97.2%。使用BART变量选择时实现完美分类，并识别出三个关键波长：1160.71nm、1328.57nm和1389.29nm。


<details>
  <summary>Details</summary>
Motivation: 特征工程在高光谱数据处理中至关重要，特别是在食品欺诈检测中识别关键波长。本研究旨在开发一种灵活的方法来区分和分类橄榄油样品的纯度水平。

Method: 采用贝叶斯加性回归树（BART）方法，利用其内置变量选择机制识别代表性光谱特征并捕捉变量间的复杂交互作用。使用网络表示法展示结果，并结合主成分分析进行降维。

Result: 在PCA降维后，BART在默认设置下达到96.8%的分类准确率，超参数调优后提升至97.2%。使用BART变量选择程序时实现完美分类性能，识别出三个关键波长（1160.71nm、1328.57nm、1389.29nm），这些变量协同作用实现准确分类并提高检测速度。

Conclusion: BART方法在橄榄油纯度分类中表现出色，不仅提高了分类准确性和可解释性，还识别出关键光谱特征及其协同交互作用，为食品质量检测提供了有效工具。

Abstract: Feature engineering plays a critical role in handling hyperspectral data and
is essential for identifying key wavelengths in food fraud detection. This
study employs Bayesian Additive Regression Trees (BART), a flexible machine
learning approach, to discriminate and classify samples of olive oil based on
their level of purity. Leveraging its built-in variable selection mechanism, we
employ BART to effectively identify the most representative spectral features
and to capture the complex interactions among variables. We use network
representation to illustrate our findings, highlighting the competitiveness of
our proposed methodology. Results demonstrate that when principal component
analysis is used for dimensionality reduction, BART outperforms
state-of-the-art models, achieving a classification accuracy of 96.8\% under
default settings, which further improves to 97.2\% after hyperparameter tuning.
If we leverage a variable selection procedure within BART, the model achieves
perfect classification performance on this dataset, improving upon previous
optimal results both in terms of accuracy and interpretability. Our results
demonstrate that three key wavelengths, 1160.71 nm, 1328.57 nm, and 1389.29 nm,
play a central role in discriminating the olive oil samples, thus highlighting
an application of our methodology in the context of food quality. Further
analysis reveals that these variables do not function independently but rather
interact synergistically to achieve accurate classification, and improved
detection speed.

</details>


### [32] [AI and analytics in sports: Leveraging BERTopic to map the past and chart the future](https://arxiv.org/abs/2510.15487)
*Manit Mishra*

Main category: stat.AP

TL;DR: 本研究通过系统文献综述和BERTopic主题建模，识别了AI和数据分析在体育领域的四个主要研究方向：表现建模、身心健康、社交媒体情感分析和战术追踪，并为未来研究提供了指导方向。


<details>
  <summary>Details</summary>
Motivation: 旨在系统梳理人工智能和数据分析在体育领域的学术文献，通过分析现有研究成果为未来研究提供指导方向。

Method: 采用系统文献综述方法，使用PRISMA协议筛选了2002-2024年间的204篇期刊文章，并利用BERTopic主题建模技术提取潜在研究主题。

Result: 识别出四个主要研究领域：表现建模、身心健康、社交媒体情感分析和战术追踪，分析了各主题的相对重要性、代表研究和关键词关联。

Conclusion: 研究为体育领域AI和数据分析研究提供了系统性框架，并引入BERTopic作为提取潜在研究结构的新方法，推动了该领域的方法论发展。

Abstract: Purpose: The purpose of this study is to map the body of scholarly literature
at the intersection of artificial intelligence (AI), analytics and sports and
thereafter, leverage the insights generated to chart guideposts for future
research. Design/methodology/approach: The study carries out systematic
literature review (SLR). Preferred Reporting Items for Systematic Reviews and
Meta-Analysis (PRISMA) protocol is leveraged to identify 204 journal articles
pertaining to utilization of AI and analytics in sports published during 2002
to 2024. We follow it up with extraction of the latent topics from sampled
articles by leveraging the topic modelling technique of BERTopic. Findings: The
study identifies the following as predominant areas of extant research on usage
of AI and analytics in sports: performance modelling, physical and mental
health, social media sentiment analysis, and tactical tracking. Each extracted
topic is further examined in terms of its relative prominence, representative
studies, and key term associations. Drawing on these insights, the study
delineates promising avenues for future inquiry. Research
limitations/implications: The study offers insights to academicians and sports
administrators on transformational impact of AI and analytics in sports.
Originality/value: The study introduces BERTopic as a novel approach for
extracting latent structures in sports research, thereby advancing both
scholarly understanding and the methodological toolkit of the field.

</details>


### [33] [Residual Kriging for Regional-Scale Canopy Height Mapping: Insights into GEDI-Induced Anisotropies and Sparse Sampling](https://arxiv.org/abs/2510.15572)
*Kamel Lahssini,Guerric le Maire,Nicolas Baghdadi,Ibrahim Fayad*

Main category: stat.AP

TL;DR: 本研究比较了U-Net深度学习模型(CHNET)和随机森林算法(RFH)在估算冠层高度方面的表现，发现GEDI数据采集参数会引入空间不一致性。通过残差克里金空间插值技术处理空间自相关性，显著提高了两种模型的精度，特别是RFH模型。


<details>
  <summary>Details</summary>
Motivation: 量化地上生物量对全球气候变化研究至关重要，而冠层高度与地上生物量相关。需要开发准确估算冠层高度的模型，但GEDI激光雷达数据采集参数会引入空间不一致性，影响模型精度。

Method: 使用GEDI激光雷达数据训练U-Net深度学习模型(CHNET)和随机森林算法(RFH)，输入包括光学、雷达和环境数据。通过残差克里金空间插值技术处理空间自相关性，仅使用GEDI功率波束并在GEDI轨道方位角方向进行空间自相关分析。

Result: 添加残差克里金校正后，CHNET和RFH的性能均得到改善，RFH的改进更为显著。校正效果集中在GEDI采样点周围，GEDI可用信息密度是空间插值有效性的重要因素。随机森林模型结合空间插值可以达到与单独使用U-Net模型相当的性能。

Conclusion: 空间插值技术能有效处理GEDI数据中的空间不一致性，提高冠层高度估算精度。随机森林模型结合空间插值是一种有效的替代方案，可以达到与深度学习模型相当的精度水平。

Abstract: Quantifying aboveground biomass (AGB) is essential in the context of global
climate change. Canopy height, which is related to AGB, can be mapped using
machine learning models trained with multi-source spatial data and GEDI
measurements. In this study, a comparative analysis of canopy height estimates
derived from two models is presented: a U-Net deep learning model (CHNET) and a
Random Forest algorithm (RFH). Both models were trained using GEDI lidar data
and utilized multi-source inputs, including optical, radar, and environmental
data. While CHNET can leverage its convolutional architecture to account for
spatial correlations, we observed that it does not fully incorporate all the
spatial autocorrelation present in GEDI canopy height measurements. By
conducting a spatial analysis of the models' residuals, we also identified that
GEDI data acquisition parameters, particularly the variability in laser beam
energy combined with the azimuthal directions of the observation tracks,
introduce spatial inconsistencies in the measurements in the form of periodic
patterns. To address these anisotropies, we considered exclusively GEDI power
beams, and we conducted our spatial autocorrelation analysis in the GEDI track
azimuthal direction. Next, we employed the residual kriging (RK) spatial
interpolation technique to account for the spatial autocorrelation of canopy
heights and improve the accuracies of CHNET and RFH estimates. Adding RK
corrections improved the performance of both CHNET and RFH, with more
substantial gains observed for RFH. The corrections appeared to be localized
around the GEDI sample points and the density of usable GEDI information is
therefore an important factor in the effectiveness of spatial interpolation.
Furthermore, our findings reveal that a Random Forest model combined with
spatial interpolation can deliver performance comparable to that of a U-Net
model alone.

</details>


### [34] [Temporal Functional Factor Analysis of Brain Connectivity](https://arxiv.org/abs/2510.15580)
*Kyle Stanley,Nicole Lazar,Matthew Reimherr*

Main category: stat.AP

TL;DR: 提出了一种基于因子分析和功能数据分析的fMRI功能连接分析方法，解决了传统因子分析在fMRI应用中的三个主要问题：短程空间依赖、大规模协方差矩阵分解和时间依赖性忽略。


<details>
  <summary>Details</summary>
Motivation: fMRI功能连接分析通常是探索性的，需要指导未来的验证性研究。因子分析因其灵活的模型假设适合这种探索性分析，但直接应用于fMRI数据存在三个关键问题需要解决。

Method: 开发了功能数据分析框架下的因子模型，结合矩阵补全技术过滤短程空间依赖，使用分布式算法分解大规模协方差矩阵，并利用功能回归利用时间动态特性。

Result: 该方法提供了一个全面且可扩展的功能连接分析方法，能够有效处理fMRI数据中的空间和时间依赖性。

Conclusion: 通过整合矩阵补全、分布式计算和功能回归技术，成功解决了因子分析在fMRI功能连接研究中的关键限制，为大规模脑连接分析提供了有效工具。

Abstract: Many analyses of functional magnetic resonance imaging (fMRI) examine
functional connectivity (FC), or the statistical dependencies among distant
brain regions. These analyses are typically exploratory, guiding future
confirmatory research. In this work, we present an approach based on factor
analysis (FA) that is well-suited to studying FC. FA is appealing in this
context because its flexible model assumptions permit a guided investigation of
its target subspace consistent with the exploratory role of connectivity
analyses. However, applying FA to fMRI data poses three problems: (1) its
target subspace captures short-range spatial dependencies that should be
treated as noise, (2) it requires factorization of a massive spatial
covariance, and (3) it overlooks temporal dependencies in the data. To address
these limitations, we develop a factor model within the framework of functional
data analysis--a field which views certain data as arising from smooth
underlying curves. The proposed approach (1) uses matrix completion techniques
to filter short-range spatial dependencies out of its target subspace, (2)
employs a distributed algorithm for factorizing large-scale covariance
matrices, and (3) leverages functional regression to exploit temporal dynamics.
Together, these innovations yield a comprehensive and scalable method for
studying FC.

</details>


### [35] [A nonstationary seasonal Dynamic Factor Model: an application to temperature time series from the state of Minas Gerais](https://arxiv.org/abs/2510.15667)
*Davi Oliveira Chaves,Chang Chiann,Pedro Alberto Morettin*

Main category: stat.AP

TL;DR: 使用非平稳季节性动态因子模型分析米纳斯吉拉斯州温度时间序列，发现两个季节性因子能有效表征数据：第一个捕获州内总体季节性模式，第二个对比两个不同区域最高温月份。


<details>
  <summary>Details</summary>
Motivation: 在农业等科学领域，温度时间序列分析面临挑战：包含所有位置信息过于复杂，而使用州平均温度等汇总指标会导致信息损失。动态因子模型提供了一种提取少量共同因子的替代方案。

Method: 应用非平稳季节性动态因子模型分析米纳斯吉拉斯州的多变量温度时间序列数据。

Result: 数据可被两个季节性因子有效表示：第一个因子捕获州的总体季节性模式，第二个因子对比两个不同区域最高温月份的差异。

Conclusion: 动态因子模型是分析多变量温度时间序列的有效方法，能够提取有意义的季节性因子，同时避免信息过载和重要信息损失。

Abstract: In many scientific fields, such as agriculture, temperature time series are
of interest both as explanatory variables and as objects of study in their own
right. However, at the state level, incorporating information from all possible
locations in an analysis can be overwhelming, while using a summary measure,
such as the state-wide average temperature, can result in significant
information loss. In this context, using Dynamic Factor Models (DFMs) provides
a compelling alternative for analyzing such multivariate time series, as they
allow for the extraction of a small number of common factors that capture the
majority of the variability in the data. Given that temperature series are
typically seasonal, this study applies a nonstationary seasonal DFM to analyze
a multivariate temperature time series from the state of Minas Gerais. The
results show that the data can be effectively represented by two seasonal
factors: the first captures the general seasonal pattern of the state, while
the second contrasts the months of highest annual temperatures between two
distinct regions.

</details>


### [36] [Incorporating estimands into meta-analyses of clinical trials](https://arxiv.org/abs/2510.15762)
*Antonio Remiro-Azócar,Pepa Polavieja,Emmanuelle Boutmy,Alessandro Ghiretti,Lise Lotte Nystrup Husemoen,Khadija Rerhou Rantell,Tatsiana Vaitsiakhovich,David M. Phillippo,Jay J. H. Park,Helle Lynggaard,Robert Bauer,Antonia Morga*

Main category: stat.AP

TL;DR: 提出在荟萃分析中使用估计量框架，通过明确处理并发事件策略来识别异质性来源并提高证据的适用性


<details>
  <summary>Details</summary>
Motivation: 估计量框架在确证性临床试验中已广泛应用，但在证据合成中应用较少，PICO框架更常用。估计量框架能更明确地处理并发事件策略，有助于系统识别和减轻定量异质性来源

Method: 提出在荟萃分析中使用估计量框架的实用方法，重点关注并发事件策略在卫生技术评估中的作用。以2型糖尿病临床试验网络荟萃分析为例，比较不同并发事件策略的影响

Result: 在荟萃分析层面指定不同目标估计量，可以明确驱动结果差异的异质性来源和并发事件策略。展示了治疗政策策略与假设策略对结果的影响

Conclusion: 建议将估计量整合到荟萃分析规划中，虽然缺乏个体水平数据存在挑战。估计量可以补充PICO框架，加强利益相关者沟通，确保生成的证据对医疗决策者具有最大相关性

Abstract: The estimand framework is increasingly established to pose research questions
in confirmatory clinical trials. In evidence synthesis, the uptake of estimands
has been modest, and the PICO (Population, Intervention, Comparator, Outcome)
framework is more often applied. While PICOs and estimands have overlapping
elements, the estimand framework explicitly considers different strategies for
intercurrent events. We propose a pragmatic framework for the use of estimands
in meta-analyses of clinical trials, highlighting the value of estimands to
systematically identify and mitigate key sources of quantitative heterogeneity,
and to enhance the applicability or external validity of pooled estimates.
Focus is placed on the role of strategies for intercurrent events, within the
specific context of meta-analyses for health technology assessment. We apply
the estimand framework to a network meta-analysis of clinical trials, comparing
the efficacy of semaglutide versus dulaglutide in type 2 diabetes. We explore
the impact of a treatment policy strategy for treatment discontinuation or
initiation of rescue medication versus a hypothetical strategy for the
corresponding intercurrent events. The specification of different target
estimands at the meta-analytical level allows us to be explicit about the
source of heterogeneity, the intercurrent event strategy, driving any potential
differences in results. We advocate for the integration of estimands into the
planning of meta-analyses, while acknowledging that potential challenges exist
in the absence of subject-level data. Estimands can complement PICOs to
strengthen communication between stakeholders about what evidence syntheses
seek to demonstrate, and to ensure that the generated evidence is maximally
relevant to healthcare decision-makers.

</details>


### [37] [Enhanced Renewable Energy Forecasting using Context-Aware Conformal Prediction](https://arxiv.org/abs/2510.15780)
*Alireza Moradi,Mathieu Tanneau,Reza Zandehshahvar,Pascal Van Hentenryck*

Main category: stat.AP

TL;DR: 提出了一种基于Conformal Predictions的定制化校准框架，通过新颖的加权方案构建上下文感知的校准集，提高了可再生能源概率预测的质量。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源在电网中占比增加，概率预测对于可靠电网运行至关重要，但现有预测常存在校准问题，可能降低决策性能。

Method: 基于Conformal Predictions的最新进展，引入定制化校准框架，使用新颖加权方案构建上下文感知的校准集。

Result: 在美国多个系统的大规模数据集上的数值实验表明，该方法在站点和舰队级别提高了概率预测质量，相比现有基线实现了更高的预测可靠性和鲁棒性。

Conclusion: 所提出的方法为可再生能源应用提供了比现有基线更可靠和鲁棒的预测性能。

Abstract: Accurate forecasting is critical for reliable power grid operations,
particularly as the share of renewable generation, such as wind and solar,
continues to grow. Given the inherent uncertainty and variability in renewable
generation, probabilistic forecasts have become essential for informed
operational decisions. However, such forecasts frequently suffer from
calibration issues, potentially degrading decision-making performance. Building
on recent advances in Conformal Predictions, this paper introduces a tailored
calibration framework that constructs context-aware calibration sets using a
novel weighting scheme. The proposed framework improves the quality of
probabilistic forecasts at the site and fleet levels, as demonstrated by
numerical experiments on large-scale datasets covering several systems in the
United States. The results demonstrate that the proposed approach achieves
higher forecast reliability and robustness for renewable energy applications
compared to existing baselines.

</details>
