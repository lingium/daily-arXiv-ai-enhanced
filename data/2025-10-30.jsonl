{"id": "2510.24754", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24754", "abs": "https://arxiv.org/abs/2510.24754", "authors": ["Yuqicheng Zhu", "Jingcheng Wu", "Yizhen Wang", "Hongkuan Zhou", "Jiaoyan Chen", "Evgeny Kharlamov", "Steffen Staab"], "title": "Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees", "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Uncertain knowledge graph embedding (UnKGE) methods learn vector\nrepresentations that capture both structural and uncertainty information to\npredict scores of unseen triples. However, existing methods produce only point\nestimates, without quantifying predictive uncertainty-limiting their\nreliability in high-stakes applications where understanding confidence in\npredictions is crucial. To address this limitation, we propose \\textsc{UnKGCP},\na framework that generates prediction intervals guaranteed to contain the true\nscore with a user-specified level of confidence. The length of the intervals\nreflects the model's predictive uncertainty. \\textsc{UnKGCP} builds on the\nconformal prediction framework but introduces a novel nonconformity measure\ntailored to UnKGE methods and an efficient procedure for interval construction.\nWe provide theoretical guarantees for the intervals and empirically verify\nthese guarantees. Extensive experiments on standard benchmarks across diverse\nUnKGE methods further demonstrate that the intervals are sharp and effectively\ncapture predictive uncertainty."}
{"id": "2510.24815", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24815", "abs": "https://arxiv.org/abs/2510.24815", "authors": ["Clément Bénard"], "title": "Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm", "comment": null, "summary": "Tree ensembles have demonstrated state-of-the-art predictive performance\nacross a wide range of problems involving tabular data. Nevertheless, the\nblack-box nature of tree ensembles is a strong limitation, especially for\napplications with critical decisions at stake. The Hoeffding or ANOVA\nfunctional decomposition is a powerful explainability method, as it breaks down\nblack-box models into a unique sum of lower-dimensional functions, provided\nthat input variables are independent. In standard learning settings, input\nvariables are often dependent, and the Hoeffding decomposition is generalized\nthrough hierarchical orthogonality constraints. Such generalization leads to\nunique and sparse decompositions with well-defined main effects and\ninteractions. However, the practical estimation of this decomposition from a\ndata sample is still an open problem. Therefore, we introduce the TreeHFD\nalgorithm to estimate the Hoeffding decomposition of a tree ensemble from a\ndata sample. We show the convergence of TreeHFD, along with the main properties\nof orthogonality, sparsity, and causal variable selection. The high performance\nof TreeHFD is demonstrated through experiments on both simulated and real data,\nusing our treehfd Python package (https://github.com/ThalesGroup/treehfd).\nBesides, we empirically show that the widely used TreeSHAP method, based on\nShapley values, is strongly connected to the Hoeffding decomposition."}
{"id": "2510.25240", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25240", "abs": "https://arxiv.org/abs/2510.25240", "authors": ["Rafael Oliveira", "Daniel M. Steinberg", "Edwin V. Bonilla"], "title": "Generative Bayesian Optimization: Generative Models as Acquisition Functions", "comment": "Under review", "summary": "We present a general strategy for turning generative models into candidate\nsolution samplers for batch Bayesian optimization (BO). The use of generative\nmodels for BO enables large batch scaling as generative sampling, optimization\nof non-continuous design spaces, and high-dimensional and combinatorial design.\nInspired by the success of direct preference optimization (DPO), we show that\none can train a generative model with noisy, simple utility values directly\ncomputed from observations to then form proposal distributions whose densities\nare proportional to the expected utility, i.e., BO's acquisition function\nvalues. Furthermore, this approach is generalizable beyond preference-based\nfeedback to general types of reward signals and loss functions. This\nperspective avoids the construction of surrogate (regression or classification)\nmodels, common in previous methods that have used generative models for\nblack-box optimization. Theoretically, we show that the generative models\nwithin the BO process approximately follow a sequence of distributions which\nasymptotically concentrate at the global optima under certain conditions. We\nalso demonstrate this effect through experiments on challenging optimization\nproblems involving large batches in high dimensions."}
{"id": "2510.25514", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25514", "abs": "https://arxiv.org/abs/2510.25514", "authors": ["Maik Overmars", "Jasper Goseling", "Richard Boucherie"], "title": "Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains", "comment": null, "summary": "We study the convergence of off-policy TD(0) with linear function\napproximation when used to approximate the expected discounted reward in a\nMarkov chain. It is well known that the combination of off-policy learning and\nfunction approximation can lead to divergence of the algorithm. Existing\nresults for this setting modify the algorithm, for instance by reweighing the\nupdates using importance sampling. This establishes convergence at the expense\nof additional complexity. In contrast, our approach is to analyse the standard\nalgorithm, but to restrict our attention to the class of reversible Markov\nchains. We demonstrate convergence under this mild reversibility condition on\nthe structure of the chain, which in many applications can be assumed using\ndomain knowledge. In particular, we establish a convergence guarantee under an\nupper bound on the discount factor in terms of the difference between the\non-policy and off-policy process. This improves upon known results in the\nliterature that state that convergence holds for a sufficiently small discount\nfactor by establishing an explicit bound. Convergence is with probability one\nand achieves projected Bellman error equal to zero. To obtain these results, we\nadapt the stochastic approximation framework that was used by Tsitsiklis and\nVan Roy [1997 for the on-policy case, to the off-policy case. We illustrate our\nresults using different types of reversible Markov chains, such as\none-dimensional random walks and random walks on a weighted graph."}
{"id": "2510.25001", "categories": ["stat.CO", "cs.LG", "68T07, 62G08, 62C10, 41A25"], "pdf": "https://arxiv.org/pdf/2510.25001", "abs": "https://arxiv.org/abs/2510.25001", "authors": ["Riddhi Pratim Ghosh", "Ian Barnett"], "title": "Bayesian Neural Networks vs. Mixture Density Networks: Theoretical and Empirical Insights for Uncertainty-Aware Nonlinear Modeling", "comment": "20 pages, 2 figures", "summary": "This paper investigates two prominent probabilistic neural modeling\nparadigms: Bayesian Neural Networks (BNNs) and Mixture Density Networks (MDNs)\nfor uncertainty-aware nonlinear regression. While BNNs incorporate epistemic\nuncertainty by placing prior distributions over network parameters, MDNs\ndirectly model the conditional output distribution, thereby capturing\nmultimodal and heteroscedastic data-generating mechanisms. We present a unified\ntheoretical and empirical framework comparing these approaches. On the\ntheoretical side, we derive convergence rates and error bounds under H\\\"older\nsmoothness conditions, showing that MDNs achieve faster Kullback-Leibler (KL)\ndivergence convergence due to their likelihood-based nature, whereas BNNs\nexhibit additional approximation bias induced by variational inference.\nEmpirically, we evaluate both architectures on synthetic nonlinear datasets and\na radiographic benchmark (RSNA Pediatric Bone Age Challenge). Quantitative and\nqualitative results demonstrate that MDNs more effectively capture multimodal\nresponses and adaptive uncertainty, whereas BNNs provide more interpretable\nepistemic uncertainty under limited data. Our findings clarify the\ncomplementary strengths of posterior-based and likelihood-based probabilistic\nlearning, offering guidance for uncertainty-aware modeling in nonlinear\nsystems."}
{"id": "2510.24948", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24948", "abs": "https://arxiv.org/abs/2510.24948", "authors": ["Hanxuan Ye", "Zachary Qian", "Hongzhe Li"], "title": "Differential Density Analysis in Single-Cell Genomics Using Specially Designed Exponential Families", "comment": "29 pages, 5 figures", "summary": "Recent advances in high-resolution sequencing have paved the way for\npopulation-scale analysis in single-cell RNA-sequencing (scRNA-seq) data.\nscRNA-seq data, in particular, have proven to be extremely powerful in\nprofiling a variety of outcomes such as disease and aging. The abundance of\nscRNA-seq data makes it possible to model each individual's gene expression as\na probability density across cells, offering a richer representation than\nsummary statistics such as means or variances, and allowing for more nuanced\ngroup comparisons. To this end, we propose a model-agnostic framework for\ndensity estimation and inference based on specially designed exponential\nfamilies~(SEF), which accommodates diverse underlying models without requiring\nprior specifications. The proposed method enables estimation and visualization\nfor both individual-specific and group-level gene expression densities, as well\nas conducting formal hypothesis testing for expression density difference\nacross groups of interest. It relies on relaxed assumptions with established\nasymptotic properties and a consistent covariance estimator for valid\ninference. Through simulation under various scenarios, the SEF-based approach\ndemonstrates good error control and improved statistical power over competing\nmethods,including pseudo-bulk tests and moment estimators. Application to a\npopulation-scale scRNA-seq dataset from patients with systemic lupus\nerythematosus identified genes and gene sets that are missed from pseudo-bulk\nbased tests."}
{"id": "2510.25531", "categories": ["stat.ML", "cs.AI", "cs.LG", "68T07", "G.3; I.2.6; J.3"], "pdf": "https://arxiv.org/pdf/2510.25531", "abs": "https://arxiv.org/abs/2510.25531", "authors": ["Clemens Schächter", "Maren Hackenberg", "Michelle Pfaffenlehner", "Félix B. Tambe-Ndonfack", "Thorsten Schmidt", "Astrid Pechmann", "Janbernd Kirschner", "Jan Hasenauser", "Harald Binder"], "title": "Using latent representations to link disjoint longitudinal data for mixed-effects regression", "comment": "31 pages, 3 figures, 3 tables", "summary": "Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges."}
{"id": "2510.25154", "categories": ["stat.ME", "stat.CO", "stat.ML", "62F15"], "pdf": "https://arxiv.org/pdf/2510.25154", "abs": "https://arxiv.org/abs/2510.25154", "authors": ["Kenyon Ng", "Edwin Fong", "David T. Frazier", "Jeremias Knoblauch", "Susan Wei"], "title": "TabMGP: Martingale Posterior with TabPFN", "comment": "11 pages (+3 reference, +22 appendix). Extra plots in\n  https://drive.google.com/drive/folders/1ct_effOoTEGpiWUf0_1xI3VqLWHtJY16", "summary": "Bayesian inference provides principled uncertainty quantification but is\noften limited by challenges of prior elicitation, likelihood misspecification,\nand computational burden. The martingale posterior (MGP, Fong et al., 2023)\noffers an alternative, replacing prior-likelihood elicitation with a predictive\nrule - namely, a sequence of one-step-ahead predictive distributions - for\nforward data generation. The utility of MGPs depends on the choice of\npredictive rule, yet the literature has offered few compelling examples.\nFoundation transformers are well-suited here, as their autoregressive\ngeneration mirrors this forward simulation and their general-purpose design\nenables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN,\na transformer foundation model that is currently state-of-the-art for tabular\ndata. TabMGP produces credible sets with near-nominal coverage and often\noutperforms both existing MGP constructions and standard Bayes."}
{"id": "2510.24969", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24969", "abs": "https://arxiv.org/abs/2510.24969", "authors": ["Jooyeon Lee", "M. S.", "Evan Kwiatkowski", "Ph. D"], "title": "Bayesian Spatial Point Process Modeling for Cluster Randomized Trials", "comment": "7 figures, 4 tables", "summary": "Cluster randomized trials (CRTs) offer a practical alternative for addressing\nlogistical challenges and ensuring feasibility in community health, education,\nand prevention studies, even though randomized controlled trials are considered\nthe gold standard in evaluating therapeutic interventions. Despite their\nutility, CRTs are often criticized for limited precision and complex modeling\nrequirements. Advances in robust Bayesian methods and the incorporation of\nspatial correlation into CRT design and analysis remain relatively\nunderdeveloped. This paper introduces a Bayesian spatial point process\nframework that models individuals nested within geographic clusters while\nexplicitly accounting for spatial dependence. We demonstrate that conventional\nnon-spatial models consistently underestimate uncertainty and lead to\nmisleading inferences, whereas our spatial approach improves estimation\nstability, controls type I error, and enhances statistical power. Our results\nunderscore the value and need for wider adoption of spatial methods in CRT."}
{"id": "2510.25544", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.25544", "abs": "https://arxiv.org/abs/2510.25544", "authors": ["Hugo Lavenant", "Giacomo Zanella"], "title": "Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations", "comment": null, "summary": "Recently proposed generative models for discrete data, such as Masked\nDiffusion Models (MDMs), exploit conditional independence approximations to\nreduce the computational cost of popular Auto-Regressive Models (ARMs), at the\nprice of some bias in the sampling distribution. We study the resulting\ncomputation-vs-accuracy trade-off, providing general error bounds (in relative\nentropy) that depend only on the average number of tokens generated per\niteration and are independent of the data dimensionality (i.e. sequence\nlength), thus supporting the empirical success of MDMs. We then investigate the\ngain obtained by using non-constant schedule sizes (i.e. varying the number of\nunmasked tokens during the generation process) and identify the optimal\nschedule as a function of a so-called information profile of the data\ndistribution, thus allowing for a principled optimization of schedule sizes. We\ndefine methods directly as sampling algorithms and do not use classical\nderivations as time-reversed diffusion processes, leading us to simple and\ntransparent proofs."}
{"id": "2510.25544", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.25544", "abs": "https://arxiv.org/abs/2510.25544", "authors": ["Hugo Lavenant", "Giacomo Zanella"], "title": "Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations", "comment": null, "summary": "Recently proposed generative models for discrete data, such as Masked\nDiffusion Models (MDMs), exploit conditional independence approximations to\nreduce the computational cost of popular Auto-Regressive Models (ARMs), at the\nprice of some bias in the sampling distribution. We study the resulting\ncomputation-vs-accuracy trade-off, providing general error bounds (in relative\nentropy) that depend only on the average number of tokens generated per\niteration and are independent of the data dimensionality (i.e. sequence\nlength), thus supporting the empirical success of MDMs. We then investigate the\ngain obtained by using non-constant schedule sizes (i.e. varying the number of\nunmasked tokens during the generation process) and identify the optimal\nschedule as a function of a so-called information profile of the data\ndistribution, thus allowing for a principled optimization of schedule sizes. We\ndefine methods directly as sampling algorithms and do not use classical\nderivations as time-reversed diffusion processes, leading us to simple and\ntransparent proofs."}
{"id": "2510.25036", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25036", "abs": "https://arxiv.org/abs/2510.25036", "authors": ["Kellin N. Rumsey", "Devin Francom", "Graham C. Gibson", "J. Derek Tucker", "Gabriel Huerta"], "title": "Bayesian Adaptive Polynomial Chaos Expansions", "comment": null, "summary": "Polynomial chaos expansions (PCE) are widely used for uncertainty\nquantification (UQ) tasks, particularly in the applied mathematics community.\nHowever, PCE has received comparatively less attention in the statistics\nliterature, and fully Bayesian formulations remain rare, especially with\nimplementations in R. Motivated by the success of adaptive Bayesian machine\nlearning models such as BART, BASS, and BPPR, we develop a new fully Bayesian\nadaptive PCE method with an efficient and accessible R implementation: khaos.\nOur approach includes a novel proposal distribution that enables data-driven\ninteraction selection, and supports a modified g-prior tailored to PCE\nstructure. Through simulation studies and real-world UQ applications, we\ndemonstrate that Bayesian adaptive PCE provides competitive performance for\nsurrogate modeling, global sensitivity analysis, and ordinal regression tasks."}
{"id": "2510.24751", "categories": ["stat.AP", "q-bio.NC", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24751", "abs": "https://arxiv.org/abs/2510.24751", "authors": ["Céline Bougel", "Sébastien Déjean", "Caroline Giulioli", "Philippe Saint-Pierre", "Nicolas Savy", "Sandrine Andrieu"], "title": "How can methods for classifying and clustering trajectories be used for prevention trials? An example in Alzheimer's disease area", "comment": "Fran{\\c c}aisCet article pr{\\'e}sente une approche m{\\'e}thodologique\n  appliqu{\\'e}e au domaine de la pr{\\'e}vention de la maladie d'Alzheimer,\n  illustr{\\'e}e {\\`a} partir des donn{\\'e}es de l'essai MAPT. L'objectif est\n  d'explorer diff{\\'e}rentes m{\\'e}thodes de classification et de regroupement\n  de trajectoires longitudinales dans les essais de pr{\\'e}vention, afin\n  d'identifier des sous-groupes homog{\\`e}nes de participants.Ce travail a\n  {\\'e}t{\\'e} soumis {\\`a} la revue Prevention Science et a fait l'objet de\n  trois tours d'{\\'e}valuation par les pairs. Malgr{\\'e} des retours positifs\n  sur l'approche m{\\'e}thodologique, l'article n'a pas {\\'e}t{\\'e} accept{\\'e}\n  pour publication, les reviewers soulignant principalement l'absence d'effet\n  de traitement, alors que l'objectif du travail {\\'e}tait avant tout\n  m{\\'e}thodologique et exploratoire.Ce d{\\'e}p{\\^o}t vise {\\`a} rendre ces\n  analyses disponibles {\\`a} la communaut{\\'e} scientifique et {\\`a} promouvoir\n  la transparence m{\\'e}thodologique dans l'analyse des essais de\n  pr{\\'e}vention.{\\copyright} 2025 C{\\'e}line Bougel et al.Distribu{\\'e} selon\n  les termes de la licence Creative Commons Attribution -- Pas d'Utilisation\n  Commerciale -- Pas de Modification 4.0 International (CC BY-NC-ND 4.0).Cet\n  ouvrage peut {\\^e}tre partag{\\'e} et cit{\\'e} {\\`a} des fins non\n  commerciales, {\\`a} condition qu'il ne soit pas modifi{\\'e} et que les\n  auteurs soient d{\\^u}ment cr{\\'e}dit{\\'e}s.EnglishThis methodological paper\n  explores different approaches for classifying and clustering longitudinal\n  trajectories in prevention trials, illustrated through data from the MAPT\n  study on Alzheimer's disease. The aim is to identify homogeneous subgroups of\n  participants and to discuss the relevance of data-driven exploratory methods\n  in inconclusive trials.The manuscript was submitted to Prevention Science and\n  underwent three rounds of peer review. Although the methodological\n  contribution was acknowledged, the paper was not accepted for publication,\n  mainly due to the reviewers' focus on treatment efficacy rather than\n  methodological innovation.The authors chose to make the manuscript publicly\n  available to promote open science and transparency in methodological research\n  on prevention trials.{\\copyright} 2025 C{\\'e}line Bougel et al.Distributed\n  under the terms of the Creative Commons\n  Attribution--NonCommercial--NoDerivatives 4.0 International License (CC\n  BY-NC-ND 4.0).This work may be shared and cited for non-commercial purposes,\n  provided that it is not modified and that proper credit is given to the\n  authors", "summary": "Background: Clinical trials are designed to prove the efficacy of an\nintervention by means of model-based approaches involving parametric hypothesis\ntesting. Issues arise when no effect is observed in the study population.\nIndeed, an effect may be present in a subgroup and the statistical test cannot\ndetect it. To investigate this possibility, we proposed to change the paradigm\nto a data-driven approach. We selected exploratory methods to provide another\nperspective on the data and to identify particular homogeneous subgroups of\nsubjects within which an effect might be detected. In the setting of prevention\ntrials, the endpoint is a trajectory of repeated measures. In the settings of\nprevention trials, the endpoint is a trajectory of repeated measures, which\nrequires the use of methods that can take data autocorrelation into account.\nThe primary aim of this work was to explore the applicability of different\nmethods for clustering and classifying trajectories. Methods: The Multidomain\nAlzheimer Preventive Trial (MAPT) was a three-year randomized controlled trial\nwith four parallel arms (NCT00672685). The primary outcome was a composite\nZ-score combining four cognitive tests. The data were analyzed by quadratic\nmixed effects model. This study was inconclusive. Exploratory analysis is\ntherefore relevant to investigate the use of data-driven methods for trajectory\nclassification. The methods used were unsupervised: k-means for longitudinal\ndata, Hierarchical Cluster Analysis (HCA), graphic semiology, and supervised\nanalysis with dichotomous classification according to responder status.\nResults: Using k-means for longitudinal data, three groups were obtained and\none of these groups showed cognitive decline over the three years of follow-up.\nThis method could be applied directly to the primary outcome, the composite\nZ-score with repeated observations over time. With the two others unsupervised\nmethods, we were unable to process longitudinal data directly. It was therefore\nnecessary to choose an indicator of change in trajectories and to consider the\nrate of change between two measurements. For the HCA method, Ward's aggregation\nwas performed. The Euclidean distance and rates of change were applied for the\ngraphic semiology method. Lastly, as there were no objective criteria to define\nresponder status, we defined our responders based on clinical criteria.\nDiscussion: In the princeps study, the prevention trial was found to be\ninconclusive, likely due to the heterogeneity of the population, which may have\nmasked a treatment effect later identified in a refined subgroup of high Beta\nAmyloid subjects. So, we have adopted an alternative unsupervised approach to\nsubject stratification based on their trajectories. We could then identify\npatterns of similar trajectories of cognitive decline and also highlight the\npotential problem of a large heterogeneity of the profiles, maybe due to the\nfinal endpoint considered."}
{"id": "2510.25573", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25573", "abs": "https://arxiv.org/abs/2510.25573", "authors": ["Christopher T. Franck", "Anne R. Driscoll", "Zoe Szajnfarber", "William H. Woodall"], "title": "Monitoring the calibration of probability forecasts with an application to concept drift detection involving image classification", "comment": null, "summary": "Machine learning approaches for image classification have led to impressive\nadvances in that field. For example, convolutional neural networks are able to\nachieve remarkable image classification accuracy across a wide range of\napplications in industry, defense, and other areas. While these machine\nlearning models boast impressive accuracy, a related concern is how to assess\nand maintain calibration in the predictions these models make. A classification\nmodel is said to be well calibrated if its predicted probabilities correspond\nwith the rates events actually occur. While there are many available methods to\nassess machine learning calibration and recalibrate faulty predictions, less\neffort has been spent on developing approaches that continually monitor\npredictive models for potential loss of calibration as time passes. We propose\na cumulative sum-based approach with dynamic limits that enable detection of\nmiscalibration in both traditional process monitoring and concept drift\napplications. This enables early detection of operational context changes that\nimpact image classification performance in the field. The proposed chart can be\nused broadly in any situation where the user needs to monitor probability\npredictions over time for potential lapses in calibration. Importantly, our\nmethod operates on probability predictions and event outcomes and does not\nrequire under-the-hood access to the machine learning model."}
{"id": "2510.25550", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25550", "abs": "https://arxiv.org/abs/2510.25550", "authors": ["Dominik Sturm", "Ivo F. Sbalzarini"], "title": "Robust variable selection for spatial point processes observed with noise", "comment": null, "summary": "We propose a method for variable selection in the intensity function of\nspatial point processes that combines sparsity-promoting estimation with\nnoise-robust model selection. As high-resolution spatial data becomes\nincreasingly available through remote sensing and automated image analysis,\nidentifying spatial covariates that influence the localization of events is\ncrucial to understand the underlying mechanism. However, results from automated\nacquisition techniques are often noisy, for example due to measurement\nuncertainties or detection errors, which leads to spurious displacements and\nmissed events. We study the impact of such noise on sparse point-process\nestimation across different models, including Poisson and Thomas processes. To\nimprove noise robustness, we propose to use stability selection based on\npoint-process subsampling and to incorporate a non-convex best-subset penalty\nto enhance model-selection performance. In extensive simulations, we\ndemonstrate that such an approach reliably recovers true covariates under\ndiverse noise scenarios and improves both selection accuracy and stability. We\nthen apply the proposed method to a forestry data set, analyzing the\ndistribution of trees in relation to elevation and soil nutrients in a tropical\nrain forest. This shows the practical utility of the method, which provides a\nsystematic framework for robust variable selection in spatial point-process\nmodels under noise, without requiring additional knowledge of the process."}
{"id": "2510.25052", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.25052", "abs": "https://arxiv.org/abs/2510.25052", "authors": ["Valerie Odeh-Couvertier", "Gabriel Zayas-Caban", "Brian Patterson", "Amy Cochran"], "title": "Designing a quasi-experiment to study the clinical impact of adaptive risk prediction models", "comment": null, "summary": "Clinical risk prediction is a valuable tool for guiding healthcare\ninterventions toward those most likely to benefit. Yet, evaluating the pairing\nof a risk prediction model with an intervention using randomized controlled\ntrials (RCTs) presents substantial challenges to today's healthcare systems.\nThis makes quasi-experimental designs, which can offer nearly the same level of\nevidence as an RCT, an attractive alternative. However, existing\nquasi-experimental designs do not allow models and thresholds to adapt. As a\nresult, they struggle to serve new populations, meet emerging trends, and\naddress practical issues. To address this gap, we introduce regression\ndiscontinuity designs for evaluating risk prediction models paired with\nspecific interventions in a modern healthcare system. In our designs, treatment\nis assigned when predicted risk crosses a defined threshold, with the design\nexplicitly accommodating adaptations in both the model and threshold. We\naccount for the interference that arises from these adaptations to estimate the\nlocal average treatment effect in a valid and efficient way. To that end, we\ncharacterize interference and provide sufficient conditions for identification.\nEstimators are introduced, and their performance is evaluated in a simulation\nthat emulates how cardiovascular risk calculators could guide interventions in\nprimary care settings."}
{"id": "2510.25185", "categories": ["stat.AP", "97K80, 91B74"], "pdf": "https://arxiv.org/pdf/2510.25185", "abs": "https://arxiv.org/abs/2510.25185", "authors": ["Han Lin Shang", "Lin Han", "Stefan Trück"], "title": "Forecasting Australian Electricity Generation by Fuel Mix", "comment": "25 pages, 3 figures, 2 tables", "summary": "Electricity demand and generation have become increasingly unpredictable with\nthe growing share of variable renewable energy sources in the power system.\nForecasting electricity supply by fuel mix is crucial for market operation,\nensuring grid stability, optimizing costs, integrating renewable energy\nsources, and supporting sustainable energy planning. We introduce two\nstatistical methods, centering on forecast reconciliation and compositional\ndata analysis, to forecast short-term electricity supply by different types of\nfuel mix. Using data for five electricity markets in Australia, we study the\nforecast accuracy of these techniques. The bottom-up hierarchical forecasting\nmethod consistently outperforms the other approaches. Moreover, fuel mix\nforecasting is most accurate in power systems with a higher share of stable\nfossil fuel generation."}
{"id": "2510.25753", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25753", "abs": "https://arxiv.org/abs/2510.25753", "authors": ["Samet Demir", "Zafer Dogan"], "title": "How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs", "comment": "NeurIPS 2025, 24 pages, 6 figures", "summary": "Pretrained Transformers demonstrate remarkable in-context learning (ICL)\ncapabilities, enabling them to adapt to new tasks from demonstrations without\nparameter updates. However, theoretical studies often rely on simplified\narchitectures (e.g., omitting MLPs), data models (e.g., linear regression with\nisotropic inputs), and single-source training, limiting their relevance to\nrealistic settings. In this work, we study ICL in pretrained Transformers with\nnonlinear MLP heads on nonlinear tasks drawn from multiple data sources with\nheterogeneous input, task, and noise distributions. We analyze a model where\nthe MLP comprises two layers, with the first layer trained via a single\ngradient step and the second layer fully optimized. Under high-dimensional\nasymptotics, we prove that such models are equivalent in ICL error to\nstructured polynomial predictors, leveraging results from the theory of\nGaussian universality and orthogonal polynomials. This equivalence reveals that\nnonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear\ntasks, compared to linear baselines. It also enables a precise analysis of data\nmixing effects: we identify key properties of high-quality data sources (low\nnoise, structured covariances) and show that feature learning emerges only when\nthe task covariance exhibits sufficient structure. These results are validated\nempirically across various activation functions, model sizes, and data\ndistributions. Finally, we experiment with a real-world scenario involving\nmultilingual sentiment analysis where each language is treated as a different\nsource. Our experimental results for this case exemplify how our findings\nextend to real-world cases. Overall, our work advances the theoretical\nfoundations of ICL in Transformers and provides actionable insight into the\nrole of architecture and data in ICL."}
{"id": "2510.25712", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.25712", "abs": "https://arxiv.org/abs/2510.25712", "authors": ["Jack Storror Carter", "Cesare Molinari"], "title": "Existence and optimisation of the partial correlation graphical lasso", "comment": "34 pages, 11 figures", "summary": "The partial correlation graphical LASSO (PCGLASSO) is a penalised likelihood\nmethod for Gaussian graphical models which provides scale invariant sparse\nestimation of the precision matrix and improves upon the popular graphical\nLASSO method. However, the PCGLASSO suffers from computational challenges due\nto the non-convexity of its associated optimisation problem. This paper\nprovides some important breakthroughs in the computation of the PCGLASSO.\nFirst, the existence of the PCGLASSO estimate is proven when the sample size is\nsmaller than the dimension - a case in which the maximum likelihood estimate\ndoes not exist. This means that the PCGLASSO can be used with any Gaussian\ndata. Second, a new alternating algorithm for computing the PCGLASSO is\nproposed and implemented in the R package PCGLASSO available at\nhttps://github.com/JackStorrorCarter/PCGLASSO. This was the first publicly\navailable implementation of the PCGLASSO and provides competitive computation\ntime for moderate dimension size."}
{"id": "2510.25153", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.25153", "abs": "https://arxiv.org/abs/2510.25153", "authors": ["Hannah Comiskey", "Niamh Cahill", "Leontine Alkema", "David Fraizer", "Worapree Maneesoonthorn"], "title": "Bayesian probabilistic projections of proportions with limited data: An application to subnational contraceptive method supply shares", "comment": null, "summary": "Engaging the private sector in contraceptive method supply is critical for\ncreating equitable, sustainable, and accessible healthcare systems. To achieve\nthis, it is essential to understand where women obtain their modern\ncontraceptives. While national-level estimates provide valuable insights into\noverall trends in contraceptive supply, they often obscure variation within and\nacross subnational regions. Addressing localized needs has become increasingly\nimportant as countries adopt decentralized models for family planning services.\nDecentralization has also underscored the need for reliable subnational\nestimates of key family planning indicators. The absence of regularly collected\nsubnational data has hindered effective monitoring and decision-making. To\nbridge this gap, we propose a novel approach that leverages latent attributes\nin Demographic and Health Survey (DHS) data to produce Bayesian probabilistic\nprojections of contraceptive method supply shares (the proportions of modern\ncontraceptive methods supplied by public and private sectors) with limited\ndata. Our modeling framework is built on Bayesian hierarchical models. Using\npenalized splines to track public and private supply shares over time, we\nleverage the spatial nature of the data and incorporate a correlation structure\nbetween recent supply share observations at national and subnational levels.\nThis framework contributes to the domain of subnational estimation of\nproportions in data-sparse settings, outperforming comparable and previous\napproaches. As decentralization continues to reshape family planning services,\nproducing reliable subnational estimates of key indicators is increasingly\nvital for researchers and policymakers."}
{"id": "2510.25424", "categories": ["stat.AP", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.25424", "abs": "https://arxiv.org/abs/2510.25424", "authors": ["Sydney Paltra", "Jonas Dehning", "Viola Priesemann", "Kai Nagel"], "title": "Inferring Mobility Reductions from COVID-19 Disease Spread along the Urban-Rural Gradient", "comment": "41 pages, 17 figures", "summary": "The COVID-19 pandemic reshaped human mobility through policy interventions\nand voluntary behavioral changes. Mobility adaptions helped mitigate pandemic\nspread, however our knowledge which environmental, social, and demographic\nfactors helped mobility reduction and pandemic mitigation is patchy. We\nintroduce a Bayesian hierarchical model to quantify heterogeneity in mobility\nresponses across time and space in Germany's 400 districts using anonymized\nmobile phone data. Decomposing mobility into a disease-responsive component and\ndisease-independent factors (temperature, school vacations, public holidays)\nallows us to quantify the impact of each factor. We find significant\ndifferences in reaction to disease spread along the urban-rural gradient, with\nlarge cities reducing mobility most strongly. Employment sectors further help\nexplain variance in reaction strength during the first wave, while political\nvariables gain significance during the second wave. However, reduced mobility\nonly partially translates to lower peak incidence, indicating the influence of\nother hidden factors. Our results identify key drivers of mobility reductions\nand demonstrate that mobility behavior can serve as an operational proxy for\npopulation response."}
{"id": "2510.25770", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25770", "abs": "https://arxiv.org/abs/2510.25770", "authors": ["Guneet S. Dhillon", "Javier González", "Teodora Pandeva", "Alicia Curth"], "title": "E-Scores for (In)Correctness Assessment of Generative Model Outputs", "comment": null, "summary": "While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction."}
{"id": "2510.25742", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.25742", "abs": "https://arxiv.org/abs/2510.25742", "authors": ["Fabio Centofanti"], "title": "Statistical Process Monitoring based on Functional Data Analysis", "comment": null, "summary": "In modern industrial settings, advanced acquisition systems allow for the\ncollection of data in the form of profiles, that is, as functional\nrelationships linking responses to explanatory variables. In this context,\nstatistical process monitoring (SPM) aims to assess the stability of profiles\nover time in order to detect unexpected behavior. This review focuses on SPM\nmethods that model profiles as functional data, i.e., smooth functions defined\nover a continuous domain, and apply functional data analysis (FDA) tools to\naddress limitations of traditional monitoring techniques. A reference framework\nfor monitoring multivariate functional data is first presented. This review\nthen offers a focused survey of several recent FDA-based profile monitoring\nmethods that extend this framework to address common challenges encountered in\nreal-world applications. These include approaches that integrate additional\nfunctional covariates to enhance detection power, a robust method designed to\naccommodate outlying observations, a real-time monitoring technique for\npartially observed profiles, and two adaptive strategies that target the\ncharacteristics of the out-of-control distribution. These methods are all\nimplemented in the R package funcharts, available on CRAN. Finally, a review of\nadditional existing FDA-based profile monitoring methods is also presented,\nalong with suggestions for future research."}
{"id": "2510.25154", "categories": ["stat.ME", "stat.CO", "stat.ML", "62F15"], "pdf": "https://arxiv.org/pdf/2510.25154", "abs": "https://arxiv.org/abs/2510.25154", "authors": ["Kenyon Ng", "Edwin Fong", "David T. Frazier", "Jeremias Knoblauch", "Susan Wei"], "title": "TabMGP: Martingale Posterior with TabPFN", "comment": "11 pages (+3 reference, +22 appendix). Extra plots in\n  https://drive.google.com/drive/folders/1ct_effOoTEGpiWUf0_1xI3VqLWHtJY16", "summary": "Bayesian inference provides principled uncertainty quantification but is\noften limited by challenges of prior elicitation, likelihood misspecification,\nand computational burden. The martingale posterior (MGP, Fong et al., 2023)\noffers an alternative, replacing prior-likelihood elicitation with a predictive\nrule - namely, a sequence of one-step-ahead predictive distributions - for\nforward data generation. The utility of MGPs depends on the choice of\npredictive rule, yet the literature has offered few compelling examples.\nFoundation transformers are well-suited here, as their autoregressive\ngeneration mirrors this forward simulation and their general-purpose design\nenables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN,\na transformer foundation model that is currently state-of-the-art for tabular\ndata. TabMGP produces credible sets with near-nominal coverage and often\noutperforms both existing MGP constructions and standard Bayes."}
{"id": "2510.25587", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.25587", "abs": "https://arxiv.org/abs/2510.25587", "authors": ["Omar AbdelGafar", "Selin Palaz", "Yihui Yang", "Christoph Holst"], "title": "General model for estimating range variances of terrestrial laser scanners based on (un-)scaled intensity values", "comment": null, "summary": "Recent advancements in technology have established terrestrial laser scanners\n(TLS) as a powerful instrument in geodetic deformation analysis. As TLS becomes\nincreasingly integrated into this field, it is essential to develop a\ncomprehensive stochastic model that accurately captures the measurement\nuncertainties. A key component of this model is the construction of a complete\nand valid variance-covariance matrix (VCM) for TLS polar measurements, which\nrequires the estimation of variances for range, vertical, and horizontal\nangles, as well as their correlations. While angular variances can be obtained\nfrom manufacturer specifications, the range variance varies with different\nintensity measurements. As a primary contribution, this study presents an\neffective methodology for measuring and estimating TLS range variances using\nboth raw and scaled intensity values. A two-dimensional scanning approach is\napplied to both controlled targets and arbitrary objects using TLS instruments\nthat provide raw intensity values (e.g., Z+F~Imager~5016A) and those that\noutput scaled intensities (e.g., Leica~ScanStation~P50). The methodology is\nfurther evaluated using field observations on a water dam surface. Overall,\nthis work introduces a comprehensive workflow for modeling range uncertainties\nin high-end TLS systems."}
{"id": "2510.25036", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25036", "abs": "https://arxiv.org/abs/2510.25036", "authors": ["Kellin N. Rumsey", "Devin Francom", "Graham C. Gibson", "J. Derek Tucker", "Gabriel Huerta"], "title": "Bayesian Adaptive Polynomial Chaos Expansions", "comment": null, "summary": "Polynomial chaos expansions (PCE) are widely used for uncertainty\nquantification (UQ) tasks, particularly in the applied mathematics community.\nHowever, PCE has received comparatively less attention in the statistics\nliterature, and fully Bayesian formulations remain rare, especially with\nimplementations in R. Motivated by the success of adaptive Bayesian machine\nlearning models such as BART, BASS, and BPPR, we develop a new fully Bayesian\nadaptive PCE method with an efficient and accessible R implementation: khaos.\nOur approach includes a novel proposal distribution that enables data-driven\ninteraction selection, and supports a modified g-prior tailored to PCE\nstructure. Through simulation studies and real-world UQ applications, we\ndemonstrate that Bayesian adaptive PCE provides competitive performance for\nsurrogate modeling, global sensitivity analysis, and ordinal regression tasks."}
{"id": "2510.25236", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.25236", "abs": "https://arxiv.org/abs/2510.25236", "authors": ["Yuchang Lin", "Qianqian Zhu", "Guodong Li"], "title": "Improving time series estimation and prediction via transfer learning", "comment": null, "summary": "There are many time series in the literature with high dimension yet limited\nsample sizes, such as macroeconomic variables, and it is almost impossible to\nobtain efficient estimation and accurate prediction by using the corresponding\ndatasets themselves. This paper fills the gap by introducing a novel\nrepresentation-based transfer learning framework for vector autoregressive\nmodels, and information from related source datasets with rich observations can\nbe leveraged to enhance estimation efficiency through representation learning.\nA two-stage regularized estimation procedure is proposed with well established\nnon-asymptotic properties, and algorithms with alternating updates are\nsuggested to search for the estimates. Our transfer learning framework can\nhandle time series with varying sample sizes and asynchronous starting and/or\nending time points, thereby offering remarkable flexibility in integrating\ninformation from diverse datasets. Simulation experiments are conducted to\nevaluate the finite-sample performance of the proposed methodology, and its\nusefulness is demonstrated by an empirical analysis on 20 macroeconomic\nvariables from Japan and another nine countries."}
{"id": "2510.25610", "categories": ["stat.AP", "62Pxx"], "pdf": "https://arxiv.org/pdf/2510.25610", "abs": "https://arxiv.org/abs/2510.25610", "authors": ["Maurits Flos", "Bastien François", "Irene Schicker", "Kirien Whan", "Elisa Perrone"], "title": "COBASE: A new copula-based shuffling method for ensemble weather forecast postprocessing", "comment": null, "summary": "Weather predictions are often provided as ensembles generated by repeated\nruns of numerical weather prediction models. These forecasts typically exhibit\nbias and inaccurate dependence structures due to numerical and dispersion\nerrors, requiring statistical postprocessing for improved precision. A common\ncorrection strategy is the two-step approach: first adjusting the univariate\nforecasts, then reconstructing the multivariate dependence. The second step is\nusually handled with nonparametric methods, which can underperform when\nhistorical data are limited. Parametric alternatives, such as the Gaussian\nCopula Approach (GCA), offer theoretical advantages but often produce poorly\ncalibrated multivariate forecasts due to random sampling of the corrected\nunivariate margins. In this work, we introduce COBASE, a novel copula-based\npostprocessing framework that preserves the flexibility of parametric modeling\nwhile mimicking the nonparametric techniques through a rank-shuffling\nmechanism. This design ensures calibrated margins and realistic dependence\nreconstruction. We evaluate COBASE on multi-site 2-meter temperature forecasts\nfrom the ALADIN-LAEF ensemble over Austria and on joint forecasts of\ntemperature and dew point temperature from the ECMWF system in the Netherlands.\nAcross all regions, COBASE variants consistently outperform traditional\ncopula-based approaches, such as GCA, and achieve performance on par with\nstate-of-the-art nonparametric methods like SimSchaake and ECC, with only\nminimal differences across settings. These results position COBASE as a\ncompetitive and robust alternative for multivariate ensemble postprocessing,\noffering a principled bridge between parametric and nonparametric dependence\nreconstruction."}
{"id": "2510.25154", "categories": ["stat.ME", "stat.CO", "stat.ML", "62F15"], "pdf": "https://arxiv.org/pdf/2510.25154", "abs": "https://arxiv.org/abs/2510.25154", "authors": ["Kenyon Ng", "Edwin Fong", "David T. Frazier", "Jeremias Knoblauch", "Susan Wei"], "title": "TabMGP: Martingale Posterior with TabPFN", "comment": "11 pages (+3 reference, +22 appendix). Extra plots in\n  https://drive.google.com/drive/folders/1ct_effOoTEGpiWUf0_1xI3VqLWHtJY16", "summary": "Bayesian inference provides principled uncertainty quantification but is\noften limited by challenges of prior elicitation, likelihood misspecification,\nand computational burden. The martingale posterior (MGP, Fong et al., 2023)\noffers an alternative, replacing prior-likelihood elicitation with a predictive\nrule - namely, a sequence of one-step-ahead predictive distributions - for\nforward data generation. The utility of MGPs depends on the choice of\npredictive rule, yet the literature has offered few compelling examples.\nFoundation transformers are well-suited here, as their autoregressive\ngeneration mirrors this forward simulation and their general-purpose design\nenables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN,\na transformer foundation model that is currently state-of-the-art for tabular\ndata. TabMGP produces credible sets with near-nominal coverage and often\noutperforms both existing MGP constructions and standard Bayes."}
{"id": "2510.25296", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.25296", "abs": "https://arxiv.org/abs/2510.25296", "authors": ["Rachel Axelrod", "Uri Obolski", "Daniel Nevo"], "title": "Nonparametric bounds for vaccine effects in randomized trials", "comment": null, "summary": "Vaccine randomized trials are typically designed to be blinded, ensuring that\nthe estimated vaccine efficacy (VE) reflects the immunological effect of the\nvaccine. When blinding is broken, however, the estimated VE reflects not only\nthe immunological effect but also behavioral effects stemming from\nparticipants' awareness of their treatment status. Recent work has proposed\nalternative causal estimands to the standard VE to address this issue, but\ntheir point identification results require a strong assumption: the absence of\nunmeasured common causes of infection risk and participants' belief about\nwhether they received the vaccine. Personality traits, for example, may\nplausibly violate this assumption. We relax this assumption and derive\nnonparametric causal bounds for different types of VE. We construct these\nbounds using two approaches: linear programming-based and monotonicity-based\nmethods. We further consider several possible causal structures for vaccine\ntrials and show how the nonparametric bounds differ across these scenarios.\nFinally, we illustrate the performance of the proposed bounds using fully\nsynthetic data and a semi-synthetic data example based on a COVID-19 vaccine\ntrial."}
{"id": "2510.24969", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24969", "abs": "https://arxiv.org/abs/2510.24969", "authors": ["Jooyeon Lee", "M. S.", "Evan Kwiatkowski", "Ph. D"], "title": "Bayesian Spatial Point Process Modeling for Cluster Randomized Trials", "comment": "7 figures, 4 tables", "summary": "Cluster randomized trials (CRTs) offer a practical alternative for addressing\nlogistical challenges and ensuring feasibility in community health, education,\nand prevention studies, even though randomized controlled trials are considered\nthe gold standard in evaluating therapeutic interventions. Despite their\nutility, CRTs are often criticized for limited precision and complex modeling\nrequirements. Advances in robust Bayesian methods and the incorporation of\nspatial correlation into CRT design and analysis remain relatively\nunderdeveloped. This paper introduces a Bayesian spatial point process\nframework that models individuals nested within geographic clusters while\nexplicitly accounting for spatial dependence. We demonstrate that conventional\nnon-spatial models consistently underestimate uncertainty and lead to\nmisleading inferences, whereas our spatial approach improves estimation\nstability, controls type I error, and enhances statistical power. Our results\nunderscore the value and need for wider adoption of spatial methods in CRT."}
{"id": "2510.25315", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25315", "abs": "https://arxiv.org/abs/2510.25315", "authors": ["Louis Sharrock", "Christopher Nemeth"], "title": "Tuning-Free Sampling via Optimization on the Space of Probability Measures", "comment": null, "summary": "We introduce adaptive, tuning-free step size schedules for gradient-based\nsampling algorithms obtained as time-discretizations of Wasserstein gradient\nflows. The result is a suite of tuning-free sampling algorithms, including\ntuning-free variants of the unadjusted Langevin algorithm (ULA), stochastic\ngradient Langevin dynamics (SGLD), mean-field Langevin dynamics (MFLD), Stein\nvariational gradient descent (SVGD), and variational gradient descent (VGD).\nMore widely, our approach yields tuning-free algorithms for solving a broad\nclass of stochastic optimization problems over the space of probability\nmeasures. Under mild assumptions (e.g., geodesic convexity and locally bounded\nstochastic gradients), we establish strong theoretical guarantees for our\napproach. In particular, we recover the convergence rate of optimally tuned\nversions of these algorithms up to logarithmic factors, in both nonsmooth and\nsmooth settings. We then benchmark the performance of our methods against\ncomparable existing approaches. Across a variety of tasks, our algorithms\nachieve similar performance to the optimal performance of existing algorithms,\nwith no need to tune a step size parameter."}
{"id": "2510.25315", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25315", "abs": "https://arxiv.org/abs/2510.25315", "authors": ["Louis Sharrock", "Christopher Nemeth"], "title": "Tuning-Free Sampling via Optimization on the Space of Probability Measures", "comment": null, "summary": "We introduce adaptive, tuning-free step size schedules for gradient-based\nsampling algorithms obtained as time-discretizations of Wasserstein gradient\nflows. The result is a suite of tuning-free sampling algorithms, including\ntuning-free variants of the unadjusted Langevin algorithm (ULA), stochastic\ngradient Langevin dynamics (SGLD), mean-field Langevin dynamics (MFLD), Stein\nvariational gradient descent (SVGD), and variational gradient descent (VGD).\nMore widely, our approach yields tuning-free algorithms for solving a broad\nclass of stochastic optimization problems over the space of probability\nmeasures. Under mild assumptions (e.g., geodesic convexity and locally bounded\nstochastic gradients), we establish strong theoretical guarantees for our\napproach. In particular, we recover the convergence rate of optimally tuned\nversions of these algorithms up to logarithmic factors, in both nonsmooth and\nsmooth settings. We then benchmark the performance of our methods against\ncomparable existing approaches. Across a variety of tasks, our algorithms\nachieve similar performance to the optimal performance of existing algorithms,\nwith no need to tune a step size parameter."}
{"id": "2510.25742", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.25742", "abs": "https://arxiv.org/abs/2510.25742", "authors": ["Fabio Centofanti"], "title": "Statistical Process Monitoring based on Functional Data Analysis", "comment": null, "summary": "In modern industrial settings, advanced acquisition systems allow for the\ncollection of data in the form of profiles, that is, as functional\nrelationships linking responses to explanatory variables. In this context,\nstatistical process monitoring (SPM) aims to assess the stability of profiles\nover time in order to detect unexpected behavior. This review focuses on SPM\nmethods that model profiles as functional data, i.e., smooth functions defined\nover a continuous domain, and apply functional data analysis (FDA) tools to\naddress limitations of traditional monitoring techniques. A reference framework\nfor monitoring multivariate functional data is first presented. This review\nthen offers a focused survey of several recent FDA-based profile monitoring\nmethods that extend this framework to address common challenges encountered in\nreal-world applications. These include approaches that integrate additional\nfunctional covariates to enhance detection power, a robust method designed to\naccommodate outlying observations, a real-time monitoring technique for\npartially observed profiles, and two adaptive strategies that target the\ncharacteristics of the out-of-control distribution. These methods are all\nimplemented in the R package funcharts, available on CRAN. Finally, a review of\nadditional existing FDA-based profile monitoring methods is also presented,\nalong with suggestions for future research."}
{"id": "2510.25507", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25507", "abs": "https://arxiv.org/abs/2510.25507", "authors": ["Yuliang Xu", "Yun Wei", "Li Ma"], "title": "Distributional Evaluation of Generative Models via Relative Density Ratio", "comment": null, "summary": "We propose a functional evaluation metric for generative models based on the\nrelative density ratio (RDR) designed to characterize distributional\ndifferences between real and generated samples. We show that the RDR as a\nfunctional summary of the goodness-of-fit for the generative model, possesses\nseveral desirable theoretical properties. It preserves $\\phi$-divergence\nbetween two distributions, enables sample-level evaluation that facilitates\ndownstream investigations of feature-specific distributional differences, and\nhas a bounded range that affords clear interpretability and numerical\nstability. Functional estimation of the RDR is achieved efficiently through\nconvex optimization on the variational form of $\\phi$-divergence. We provide\ntheoretical convergence rate guarantees for general estimators based on\nM-estimator theory, as well as the convergence rates of neural network-based\nestimators when the true ratio is in the anisotropic Besov space. We\ndemonstrate the power of the proposed RDR-based evaluation through numerical\nexperiments on MNIST, CelebA64, and the American Gut project microbiome data.\nWe show that the estimated RDR not only allows for an effective comparison of\nthe overall performance of competing generative models, but it can also offer a\nconvenient means of revealing the nature of the underlying goodness-of-fit.\nThis enables one to assess support overlap, coverage, and fidelity while\npinpointing regions of the sample space where generators concentrate and\nrevealing the features that drive the most salient distributional differences."}
{"id": "2510.25316", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.25316", "abs": "https://arxiv.org/abs/2510.25316", "authors": ["Tianbo Chen"], "title": "Asymmetric Huber Periodogram", "comment": null, "summary": "This paper introduces a novel spectral M-estimator, called the asymmetric\nHuber periodogram (AHP), for periodicity detection in time series. The AHP is\nconstructed from trigonometric asymmetric Huber regression, where a specially\ndesigned check function is used to substitute the squared L2 norm that defines\nthe ordinary periodogram (PG). The AHP is statistically more efficient than the\nquantile periodogram (QP), while offering a more comprehensive picture than the\nHuber periodogram (HP) by examining the data across the entire range of the\nasymmetric parameter. We prove the theoretical properties of the AHP and\ninvestigate the relationship between the AHP and the so-called asymmetric Huber\nspectrum (AHS). Finally, simulations and three real-world data examples\ndemonstrate that the AHP's capability in detecting periodicity and its\nrobustness against outliers."}
{"id": "2510.25550", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25550", "abs": "https://arxiv.org/abs/2510.25550", "authors": ["Dominik Sturm", "Ivo F. Sbalzarini"], "title": "Robust variable selection for spatial point processes observed with noise", "comment": null, "summary": "We propose a method for variable selection in the intensity function of\nspatial point processes that combines sparsity-promoting estimation with\nnoise-robust model selection. As high-resolution spatial data becomes\nincreasingly available through remote sensing and automated image analysis,\nidentifying spatial covariates that influence the localization of events is\ncrucial to understand the underlying mechanism. However, results from automated\nacquisition techniques are often noisy, for example due to measurement\nuncertainties or detection errors, which leads to spurious displacements and\nmissed events. We study the impact of such noise on sparse point-process\nestimation across different models, including Poisson and Thomas processes. To\nimprove noise robustness, we propose to use stability selection based on\npoint-process subsampling and to incorporate a non-convex best-subset penalty\nto enhance model-selection performance. In extensive simulations, we\ndemonstrate that such an approach reliably recovers true covariates under\ndiverse noise scenarios and improves both selection accuracy and stability. We\nthen apply the proposed method to a forestry data set, analyzing the\ndistribution of trees in relation to elevation and soil nutrients in a tropical\nrain forest. This shows the practical utility of the method, which provides a\nsystematic framework for robust variable selection in spatial point-process\nmodels under noise, without requiring additional knowledge of the process."}
{"id": "2510.25371", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.25371", "abs": "https://arxiv.org/abs/2510.25371", "authors": ["Soham Mukherjee", "Javier Enrique Aguilar", "Marcello Zago", "Manfred Claassen", "Paul-Christian Bürkner"], "title": "Latent variable estimation with composite Hilbert space Gaussian processes", "comment": "37 pages, 16 figures, 3 tables", "summary": "We develop a scalable class of models for latent variable estimation using\ncomposite Gaussian processes, with a focus on derivative Gaussian processes. We\njointly model multiple data sources as outputs to improve the accuracy of\nlatent variable inference under a single probabilistic framework. Similarly\nspecified exact Gaussian processes scale poorly with large datasets. To\novercome this, we extend the recently developed Hilbert space approximation\nmethods for Gaussian processes to obtain a reduced-rank representation of the\ncomposite covariance function through its spectral decomposition. Specifically,\nwe derive and analyze the spectral decomposition of derivative covariance\nfunctions and further study their properties theoretically. Using these\nspectral decompositions, our methods easily scale up to data scenarios\ninvolving thousands of samples. We validate our methods in terms of latent\nvariable estimation accuracy, uncertainty calibration, and inference speed\nacross diverse simulation scenarios. Finally, using a real world case study\nfrom single-cell biology, we demonstrate the potential of our models in\nestimating latent cellular ordering given gene expression levels, thus\nenhancing our understanding of the underlying biological process."}
{"id": "2510.25507", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25507", "abs": "https://arxiv.org/abs/2510.25507", "authors": ["Yuliang Xu", "Yun Wei", "Li Ma"], "title": "Distributional Evaluation of Generative Models via Relative Density Ratio", "comment": null, "summary": "We propose a functional evaluation metric for generative models based on the\nrelative density ratio (RDR) designed to characterize distributional\ndifferences between real and generated samples. We show that the RDR as a\nfunctional summary of the goodness-of-fit for the generative model, possesses\nseveral desirable theoretical properties. It preserves $\\phi$-divergence\nbetween two distributions, enables sample-level evaluation that facilitates\ndownstream investigations of feature-specific distributional differences, and\nhas a bounded range that affords clear interpretability and numerical\nstability. Functional estimation of the RDR is achieved efficiently through\nconvex optimization on the variational form of $\\phi$-divergence. We provide\ntheoretical convergence rate guarantees for general estimators based on\nM-estimator theory, as well as the convergence rates of neural network-based\nestimators when the true ratio is in the anisotropic Besov space. We\ndemonstrate the power of the proposed RDR-based evaluation through numerical\nexperiments on MNIST, CelebA64, and the American Gut project microbiome data.\nWe show that the estimated RDR not only allows for an effective comparison of\nthe overall performance of competing generative models, but it can also offer a\nconvenient means of revealing the nature of the underlying goodness-of-fit.\nThis enables one to assess support overlap, coverage, and fidelity while\npinpointing regions of the sample space where generators concentrate and\nrevealing the features that drive the most salient distributional differences."}
{"id": "2510.25550", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25550", "abs": "https://arxiv.org/abs/2510.25550", "authors": ["Dominik Sturm", "Ivo F. Sbalzarini"], "title": "Robust variable selection for spatial point processes observed with noise", "comment": null, "summary": "We propose a method for variable selection in the intensity function of\nspatial point processes that combines sparsity-promoting estimation with\nnoise-robust model selection. As high-resolution spatial data becomes\nincreasingly available through remote sensing and automated image analysis,\nidentifying spatial covariates that influence the localization of events is\ncrucial to understand the underlying mechanism. However, results from automated\nacquisition techniques are often noisy, for example due to measurement\nuncertainties or detection errors, which leads to spurious displacements and\nmissed events. We study the impact of such noise on sparse point-process\nestimation across different models, including Poisson and Thomas processes. To\nimprove noise robustness, we propose to use stability selection based on\npoint-process subsampling and to incorporate a non-convex best-subset penalty\nto enhance model-selection performance. In extensive simulations, we\ndemonstrate that such an approach reliably recovers true covariates under\ndiverse noise scenarios and improves both selection accuracy and stability. We\nthen apply the proposed method to a forestry data set, analyzing the\ndistribution of trees in relation to elevation and soil nutrients in a tropical\nrain forest. This shows the practical utility of the method, which provides a\nsystematic framework for robust variable selection in spatial point-process\nmodels under noise, without requiring additional knowledge of the process."}
{"id": "2510.25632", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.25632", "abs": "https://arxiv.org/abs/2510.25632", "authors": ["Gengyang Chen", "Mu Zhu"], "title": "Automatic selection of hyper-parameters via the use of softened profile likelihood", "comment": null, "summary": "We extend a heuristic method for automatic dimensionality selection, which\nmaximizes a profile likelihood to identify \"elbows\" in scree plots. Our\nextension enables researchers to make automatic choices of multiple\nhyper-parameters simultaneously. To facilitate our extension to\nmulti-dimensions, we propose a \"softened\" profile likelihood. We present two\ndistinct parameterizations of our solution and demonstrate our approach on\nelastic nets, support vector machines, and neural networks. We also briefly\ndiscuss applications of our method to other data-analytic tasks than\nhyper-parameter selection."}
{"id": "2510.25712", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.25712", "abs": "https://arxiv.org/abs/2510.25712", "authors": ["Jack Storror Carter", "Cesare Molinari"], "title": "Existence and optimisation of the partial correlation graphical lasso", "comment": "34 pages, 11 figures", "summary": "The partial correlation graphical LASSO (PCGLASSO) is a penalised likelihood\nmethod for Gaussian graphical models which provides scale invariant sparse\nestimation of the precision matrix and improves upon the popular graphical\nLASSO method. However, the PCGLASSO suffers from computational challenges due\nto the non-convexity of its associated optimisation problem. This paper\nprovides some important breakthroughs in the computation of the PCGLASSO.\nFirst, the existence of the PCGLASSO estimate is proven when the sample size is\nsmaller than the dimension - a case in which the maximum likelihood estimate\ndoes not exist. This means that the PCGLASSO can be used with any Gaussian\ndata. Second, a new alternating algorithm for computing the PCGLASSO is\nproposed and implemented in the R package PCGLASSO available at\nhttps://github.com/JackStorrorCarter/PCGLASSO. This was the first publicly\navailable implementation of the PCGLASSO and provides competitive computation\ntime for moderate dimension size."}
{"id": "2510.25742", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.25742", "abs": "https://arxiv.org/abs/2510.25742", "authors": ["Fabio Centofanti"], "title": "Statistical Process Monitoring based on Functional Data Analysis", "comment": null, "summary": "In modern industrial settings, advanced acquisition systems allow for the\ncollection of data in the form of profiles, that is, as functional\nrelationships linking responses to explanatory variables. In this context,\nstatistical process monitoring (SPM) aims to assess the stability of profiles\nover time in order to detect unexpected behavior. This review focuses on SPM\nmethods that model profiles as functional data, i.e., smooth functions defined\nover a continuous domain, and apply functional data analysis (FDA) tools to\naddress limitations of traditional monitoring techniques. A reference framework\nfor monitoring multivariate functional data is first presented. This review\nthen offers a focused survey of several recent FDA-based profile monitoring\nmethods that extend this framework to address common challenges encountered in\nreal-world applications. These include approaches that integrate additional\nfunctional covariates to enhance detection power, a robust method designed to\naccommodate outlying observations, a real-time monitoring technique for\npartially observed profiles, and two adaptive strategies that target the\ncharacteristics of the out-of-control distribution. These methods are all\nimplemented in the R package funcharts, available on CRAN. Finally, a review of\nadditional existing FDA-based profile monitoring methods is also presented,\nalong with suggestions for future research."}
{"id": "2510.24751", "categories": ["stat.AP", "q-bio.NC", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24751", "abs": "https://arxiv.org/abs/2510.24751", "authors": ["Céline Bougel", "Sébastien Déjean", "Caroline Giulioli", "Philippe Saint-Pierre", "Nicolas Savy", "Sandrine Andrieu"], "title": "How can methods for classifying and clustering trajectories be used for prevention trials? An example in Alzheimer's disease area", "comment": "Fran{\\c c}aisCet article pr{\\'e}sente une approche m{\\'e}thodologique\n  appliqu{\\'e}e au domaine de la pr{\\'e}vention de la maladie d'Alzheimer,\n  illustr{\\'e}e {\\`a} partir des donn{\\'e}es de l'essai MAPT. L'objectif est\n  d'explorer diff{\\'e}rentes m{\\'e}thodes de classification et de regroupement\n  de trajectoires longitudinales dans les essais de pr{\\'e}vention, afin\n  d'identifier des sous-groupes homog{\\`e}nes de participants.Ce travail a\n  {\\'e}t{\\'e} soumis {\\`a} la revue Prevention Science et a fait l'objet de\n  trois tours d'{\\'e}valuation par les pairs. Malgr{\\'e} des retours positifs\n  sur l'approche m{\\'e}thodologique, l'article n'a pas {\\'e}t{\\'e} accept{\\'e}\n  pour publication, les reviewers soulignant principalement l'absence d'effet\n  de traitement, alors que l'objectif du travail {\\'e}tait avant tout\n  m{\\'e}thodologique et exploratoire.Ce d{\\'e}p{\\^o}t vise {\\`a} rendre ces\n  analyses disponibles {\\`a} la communaut{\\'e} scientifique et {\\`a} promouvoir\n  la transparence m{\\'e}thodologique dans l'analyse des essais de\n  pr{\\'e}vention.{\\copyright} 2025 C{\\'e}line Bougel et al.Distribu{\\'e} selon\n  les termes de la licence Creative Commons Attribution -- Pas d'Utilisation\n  Commerciale -- Pas de Modification 4.0 International (CC BY-NC-ND 4.0).Cet\n  ouvrage peut {\\^e}tre partag{\\'e} et cit{\\'e} {\\`a} des fins non\n  commerciales, {\\`a} condition qu'il ne soit pas modifi{\\'e} et que les\n  auteurs soient d{\\^u}ment cr{\\'e}dit{\\'e}s.EnglishThis methodological paper\n  explores different approaches for classifying and clustering longitudinal\n  trajectories in prevention trials, illustrated through data from the MAPT\n  study on Alzheimer's disease. The aim is to identify homogeneous subgroups of\n  participants and to discuss the relevance of data-driven exploratory methods\n  in inconclusive trials.The manuscript was submitted to Prevention Science and\n  underwent three rounds of peer review. Although the methodological\n  contribution was acknowledged, the paper was not accepted for publication,\n  mainly due to the reviewers' focus on treatment efficacy rather than\n  methodological innovation.The authors chose to make the manuscript publicly\n  available to promote open science and transparency in methodological research\n  on prevention trials.{\\copyright} 2025 C{\\'e}line Bougel et al.Distributed\n  under the terms of the Creative Commons\n  Attribution--NonCommercial--NoDerivatives 4.0 International License (CC\n  BY-NC-ND 4.0).This work may be shared and cited for non-commercial purposes,\n  provided that it is not modified and that proper credit is given to the\n  authors", "summary": "Background: Clinical trials are designed to prove the efficacy of an\nintervention by means of model-based approaches involving parametric hypothesis\ntesting. Issues arise when no effect is observed in the study population.\nIndeed, an effect may be present in a subgroup and the statistical test cannot\ndetect it. To investigate this possibility, we proposed to change the paradigm\nto a data-driven approach. We selected exploratory methods to provide another\nperspective on the data and to identify particular homogeneous subgroups of\nsubjects within which an effect might be detected. In the setting of prevention\ntrials, the endpoint is a trajectory of repeated measures. In the settings of\nprevention trials, the endpoint is a trajectory of repeated measures, which\nrequires the use of methods that can take data autocorrelation into account.\nThe primary aim of this work was to explore the applicability of different\nmethods for clustering and classifying trajectories. Methods: The Multidomain\nAlzheimer Preventive Trial (MAPT) was a three-year randomized controlled trial\nwith four parallel arms (NCT00672685). The primary outcome was a composite\nZ-score combining four cognitive tests. The data were analyzed by quadratic\nmixed effects model. This study was inconclusive. Exploratory analysis is\ntherefore relevant to investigate the use of data-driven methods for trajectory\nclassification. The methods used were unsupervised: k-means for longitudinal\ndata, Hierarchical Cluster Analysis (HCA), graphic semiology, and supervised\nanalysis with dichotomous classification according to responder status.\nResults: Using k-means for longitudinal data, three groups were obtained and\none of these groups showed cognitive decline over the three years of follow-up.\nThis method could be applied directly to the primary outcome, the composite\nZ-score with repeated observations over time. With the two others unsupervised\nmethods, we were unable to process longitudinal data directly. It was therefore\nnecessary to choose an indicator of change in trajectories and to consider the\nrate of change between two measurements. For the HCA method, Ward's aggregation\nwas performed. The Euclidean distance and rates of change were applied for the\ngraphic semiology method. Lastly, as there were no objective criteria to define\nresponder status, we defined our responders based on clinical criteria.\nDiscussion: In the princeps study, the prevention trial was found to be\ninconclusive, likely due to the heterogeneity of the population, which may have\nmasked a treatment effect later identified in a refined subgroup of high Beta\nAmyloid subjects. So, we have adopted an alternative unsupervised approach to\nsubject stratification based on their trajectories. We could then identify\npatterns of similar trajectories of cognitive decline and also highlight the\npotential problem of a large heterogeneity of the profiles, maybe due to the\nfinal endpoint considered."}
