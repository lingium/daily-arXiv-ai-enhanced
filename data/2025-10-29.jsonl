{"id": "2510.23666", "categories": ["stat.ML", "cs.LG", "stat.ME", "I.2.6; G.3; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.23666", "abs": "https://arxiv.org/abs/2510.23666", "authors": ["Junpeng Gong", "Chunkai Wang", "Hao Li", "Jinyong Ma", "Haoxuan Li", "Xu He"], "title": "Beyond Normality: Reliable A/B Testing with Non-Gaussian Data", "comment": "11 pages, 3 figures", "summary": "A/B testing has become the cornerstone of decision-making in online markets,\nguiding how platforms launch new features, optimize pricing strategies, and\nimprove user experience. In practice, we typically employ the pairwise $t$-test\nto compare outcomes between the treatment and control groups, thereby assessing\nthe effectiveness of a given strategy. To be trustworthy, these experiments\nmust keep Type I error (i.e., false positive rate) under control; otherwise, we\nmay launch harmful strategies. However, in real-world applications, we find\nthat A/B testing often fails to deliver reliable results. When the data\ndistribution departs from normality or when the treatment and control groups\ndiffer in sample size, the commonly used pairwise $t$-test is no longer\ntrustworthy. In this paper, we quantify how skewed, long tailed data and\nunequal allocation distort error rates and derive explicit formulas for the\nminimum sample size required for the $t$-test to remain valid. We find that\nmany online feedback metrics require hundreds of millions samples to ensure\nreliable A/B testing. Thus we introduce an Edgeworth-based correction that\nprovides more accurate $p$-values when the available sample size is limited.\nOffline experiments on a leading A/B testing platform corroborate the practical\nvalue of our theoretical minimum sample size thresholds and demonstrate that\nthe corrected method substantially improves the reliability of A/B testing in\nreal-world conditions."}
{"id": "2510.23684", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23684", "abs": "https://arxiv.org/abs/2510.23684", "authors": ["Samuel G. Fadel", "Hrittik Roy", "Nicholas Krämer", "Yevgen Zainchkovskyy", "Stas Syrota", "Alejandro Valverde Mahou", "Carl Henrik Ek", "Søren Hauberg"], "title": "VIKING: Deep variational inference with stochastic projections", "comment": "NeurIPS 2025 (poster)", "summary": "Variational mean field approximations tend to struggle with contemporary\noverparametrized deep neural networks. Where a Bayesian treatment is usually\nassociated with high-quality predictions and uncertainties, the practical\nreality has been the opposite, with unstable training, poor predictive power,\nand subpar calibration. Building upon recent work on reparametrizations of\nneural networks, we propose a simple variational family that considers two\nindependent linear subspaces of the parameter space. These represent functional\nchanges inside and outside the support of training data. This allows us to\nbuild a fully-correlated approximate posterior reflecting the\noverparametrization that tunes easy-to-interpret hyperparameters. We develop\nscalable numerical routines that maximize the associated evidence lower bound\n(ELBO) and sample from the approximate posterior. Empirically, we observe\nstate-of-the-art performance across tasks, models, and datasets compared to a\nwide array of baseline methods. Our results show that approximate Bayesian\ninference applied to deep neural networks is far from a lost cause when\nconstructing inference mechanisms that reflect the geometry of\nreparametrizations."}
{"id": "2510.23745", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23745", "abs": "https://arxiv.org/abs/2510.23745", "authors": ["Alex Alberts", "Ilias Bilionis"], "title": "Bayesian neural networks with interpretable priors from Mercer kernels", "comment": null, "summary": "Quantifying the uncertainty in the output of a neural network is essential\nfor deployment in scientific or engineering applications where decisions must\nbe made under limited or noisy data. Bayesian neural networks (BNNs) provide a\nframework for this purpose by constructing a Bayesian posterior distribution\nover the network parameters. However, the prior, which is of key importance in\nany Bayesian setting, is rarely meaningful for BNNs. This is because the\ncomplexity of the input-to-output map of a BNN makes it difficult to understand\nhow certain distributions enforce any interpretable constraint on the output\nspace. Gaussian processes (GPs), on the other hand, are often preferred in\nuncertainty quantification tasks due to their interpretability. The drawback is\nthat GPs are limited to small datasets without advanced techniques, which often\nrely on the covariance kernel having a specific structure. To address these\nchallenges, we introduce a new class of priors for BNNs, called Mercer priors,\nsuch that the resulting BNN has samples which approximate that of a specified\nGP. The method works by defining a prior directly over the network parameters\nfrom the Mercer representation of the covariance kernel, and does not rely on\nthe network having a specific structure. In doing so, we can exploit the\nscalability of BNNs in a meaningful Bayesian way."}
{"id": "2510.23935", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23935", "abs": "https://arxiv.org/abs/2510.23935", "authors": ["Enze Shi", "Pankaj Bhagwat", "Zhixian Yang", "Linglong Kong", "Bei Jiang"], "title": "Understanding Fairness and Prediction Error through Subspace Decomposition and Influence Analysis", "comment": null, "summary": "Machine learning models have achieved widespread success but often inherit\nand amplify historical biases, resulting in unfair outcomes. Traditional\nfairness methods typically impose constraints at the prediction level, without\naddressing underlying biases in data representations. In this work, we propose\na principled framework that adjusts data representations to balance predictive\nutility and fairness. Using sufficient dimension reduction, we decompose the\nfeature space into target-relevant, sensitive, and shared components, and\ncontrol the fairness-utility trade-off by selectively removing sensitive\ninformation. We provide a theoretical analysis of how prediction error and\nfairness gaps evolve as shared subspaces are added, and employ influence\nfunctions to quantify their effects on the asymptotic behavior of parameter\nestimates. Experiments on both synthetic and real-world datasets validate our\ntheoretical insights and show that the proposed method effectively improves\nfairness while preserving predictive performance."}
{"id": "2510.24319", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.24319", "abs": "https://arxiv.org/abs/2510.24319", "authors": ["Mohamedou Ould Haye", "Anne Philippe"], "title": "A Frequency-Domain NonStationarity Test for dependent data", "comment": null, "summary": "Distinguishing long-memory behaviour from nonstationarity is challenging, as\nboth produce slowly decaying sample autocovariances. Existing stationarity\ntests either fail to account for long-memory processes or exhibit poor\nempirical size, particularly near the boundary between stationarity and\nnonstationarity. We propose a new, parameter-free testing procedure based on\nthe evaluation of periodograms across multiple epochs. The limiting\ndistributions derived here are obtained under stationarity and nonstationarity\nassumptions and analytically tractable, expressed as finite sums of weighted\nindependent $\\chi^2$ random variables. Simulation studies indicate that the\nproposed method performs favorably compared to existing approaches."}
{"id": "2510.23985", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "68T07, 60H35, 65C30, 60H10"], "pdf": "https://arxiv.org/pdf/2510.23985", "abs": "https://arxiv.org/abs/2510.23985", "authors": ["Adam Nordenhög", "Akash Sharma"], "title": "Score-based constrained generative modeling via Langevin diffusions with boundary conditions", "comment": null, "summary": "Score-based generative models based on stochastic differential equations\n(SDEs) achieve impressive performance in sampling from unknown distributions,\nbut often fail to satisfy underlying constraints. We propose a constrained\ngenerative model using kinetic (underdamped) Langevin dynamics with specular\nreflection of velocity on the boundary defining constraints. This results in\npiecewise continuously differentiable noising and denoising process where the\nlatter is characterized by a time-reversed dynamics restricted to a domain with\nboundary due to specular boundary condition. In addition, we also contribute to\nexisting reflected SDEs based constrained generative models, where the\nstochastic dynamics is restricted through an abstract local time term. By\npresenting efficient numerical samplers which converge with optimal rate in\nterms of discretizations step, we provide a comprehensive comparison of models\nbased on confined (specularly reflected kinetic) Langevin diffusion with models\nbased on reflected diffusion with local time."}
{"id": "2510.23651", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2510.23651", "abs": "https://arxiv.org/abs/2510.23651", "authors": ["Zehao Lu"], "title": "Multi-Dimensional Wasserstein Distance Implementation in Scipy", "comment": null, "summary": "The Wasserstein distance, also known as the Earth mover distance or optimal\ntransport distance, is a widely used measure of similarity between probability\ndistributions. This paper presents an linear programming based implementation\nof the multi-dimensional Wasserstein distance function in Scipy, a powerful\nscientific computing package in Python. Building upon the existing\none-dimensional scipy.stats.wasserstein_distance function, our work extends its\ncapabilities to handle multi-dimensional distributions. To compute the\nmulti-dimensional Wasserstein distance, we developed an implementation that\ntransforms the problem into a linear programming problem. We utilized the scipy\nlinear programming solver to effectively solve this transformed problem. The\nproposed implementation includes thorough documentation and comprehensive test\ncases to ensure accuracy and reliability. The resulting feature is set to be\nmerged into the main Scipy development branch and will be included in the\nupcoming release, further enhancing the capabilities of Scipy in the field of\nmulti-dimensional statistical analysis."}
{"id": "2510.23740", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.23740", "abs": "https://arxiv.org/abs/2510.23740", "authors": ["Shay Gilpin", "Michael Herty"], "title": "A modified particle filter that reduces weight collapse", "comment": null, "summary": "Particle filters are a widely used Monte Carlo based data assimilation\ntechnique that estimates the probability distribution of a system's state\nconditioned on observations through a collection of weights and particles. A\nknown problem for particle filters is weight collapse, or degeneracy, where a\nsingle weight attains a value of one while all others are close to zero,\nthereby collapsing the estimated distribution. We address this issue by\nintroducing a novel modification to the particle filter that is simple to\nimplement and inspired by energy-based diversity measures. Our approach adjusts\nparticle weights to minimize a two-body energy potential, promoting balanced\nweight distributions and mitigating collapse. We demonstrate the performance of\nthis modified particle filter in a series of numerical experiments with linear\nand nonlinear dynamical models, where we compare with the classical particle\nfilter and ensemble Kalman filters in the nonlinear case. We find that our new\napproach improves weight distributions compared to the classical particle\nfilter and thereby improve state estimates."}
{"id": "2510.24056", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24056", "abs": "https://arxiv.org/abs/2510.24056", "authors": ["Agnideep Aich", "Ashit Baran Aich"], "title": "Copula-Stein Discrepancy: A Generator-Based Stein Operator for Archimedean Dependence", "comment": null, "summary": "Kernel Stein discrepancies (KSDs) have become a principal tool for\ngoodness-of-fit testing, but standard KSDs are often insensitive to\nhigher-order dependency structures, such as tail dependence, which are critical\nin many scientific and financial domains. We address this gap by introducing\nthe Copula-Stein Discrepancy (CSD), a novel class of discrepancies tailored to\nthe geometry of statistical dependence. By defining a Stein operator directly\non the copula density, CSD leverages the generative structure of dependence,\nrather than relying on the joint density's score function. For the broad class\nof Archimedean copulas, this approach yields a closed-form Stein kernel derived\nfrom the scalar generator function. We provide a comprehensive theoretical\nanalysis, proving that CSD (i) metrizes weak convergence of copula\ndistributions, ensuring it detects any mismatch in dependence; (ii) has an\nempirical estimator that converges at the minimax optimal rate of\n$O_P(n^{-1/2})$; and (iii) is provably sensitive to differences in tail\ndependence coefficients. The framework is extended to general non-Archimedean\ncopulas, including elliptical and vine copulas. Computationally, the exact CSD\nkernel evaluation scales linearly in dimension, while a novel random feature\napproximation reduces the $n$-dependence from quadratic $O(n^2)$ to near-linear\n$\\tilde{O}(n)$, making CSD a practical and theoretically principled tool for\ndependence-aware inference."}
{"id": "2510.24604", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2510.24604", "abs": "https://arxiv.org/abs/2510.24604", "authors": ["Aleksei G. Sorokin", "Pieterjan Robbe", "Gianluca Geraci", "Michael S. Eldred", "Fred J. Hickernell"], "title": "Fast Bayesian Multilevel Quasi-Monte Carlo", "comment": "26 pages, 11 figures", "summary": "Existing multilevel quasi-Monte Carlo (MLQMC) methods often rely on multiple\nindependent randomizations of a low-discrepancy (LD) sequence to estimate\nstatistical errors on each level. While this approach is standard, it can be\nless efficient than simply increasing the number of points from a single LD\nsequence. However, a single LD sequence does not permit statistical error\nestimates in the current framework. We propose to recast the MLQMC problem in a\nBayesian cubature framework, which uses a single LD sequence and quantifies\nnumerical error through the posterior variance of a Gaussian process (GP)\nmodel. When paired with certain LD sequences, GP regression and hyperparameter\noptimization can be carried out at only $\\mathcal{O}(n \\log n)$ cost, where $n$\nis the number of samples. Building on the adaptive sample allocation used in\ntraditional MLQMC, where the number of samples is doubled on the level with the\ngreatest expected benefit, we introduce a new Bayesian utility function that\nbalances the computational cost of doubling against the anticipated reduction\nin posterior uncertainty. We also propose a new digitally-shift-invariant (DSI)\nkernel of adaptive smoothness, which combines multiple higher-order DSI kernels\nthrough a weighted sum of smoothness parameters, for use with fast digital net\nGPs. A series of numerical experiments illustrate the performance of our fast\nBayesian MLQMC method and error estimates for both single-level problems and\nmultilevel problems with a fixed number of levels. The Bayesian error estimates\nobtained using digital nets are found to be reliable, although, in some cases,\nmildly conservative."}
{"id": "2510.23805", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.23805", "abs": "https://arxiv.org/abs/2510.23805", "authors": ["Xueying Chen", "Jianfeng Ke", "Lauren Flynn", "Giovanni Parmigiani", "Danielle Braun"], "title": "A web-based user interface for Fam3PRO, a multi-gene, multi-cancer risk prediction model for families with cancer history", "comment": null, "summary": "Purpose: Hereditary cancer risk is key to guiding screening and prevention\nstrategies. Cancer risks can vary by individual due to the presence or absence\nof high- and moderate-risk pathogenic variants (PV) in cancer-associated genes,\nin addition to sex, age, and other risk factors. We previously developed\nFam3PRO, a flexible multi-gene, multi-cancer Mendelian risk prediction model\nthat estimates a patient's risk of carrying a PV in hereditary cancer genes and\ntheir future risk of developing several types of cancer. The Fam3PRO R package\nincludes 22 genes with 18 associated cancers, allowing users to build\ncustomized sub-models from any gene-cancer set. However, the current R package\nlacks a user interface (UI), limiting its practical use in clinical settings.\nTherefore, we aim to develop a web-based UI for broader use of the Fam3PRO\nfunctionalities.\n  Methods: The Fam3PRO UI (F3PI), built with R Shiny, collects and formats\ninputs including family health history, genetic test results, and other risk\nfactors. Pedigree data are interactively visualized and modified via\npedigreejs, while the backend Fam3PRO model takes all the inputs to generate\ncarrier probabilities and future cancer risks, presented through an interactive\nUI.\n  Results: F3PI streamlines the collection of patient and family history data,\nwhich is analyzed by the Fam3PRO models to provide personalized cancer risks\nfor each proband across 18 cancers, as well as probabilities that a proband has\na PV in up to 22 hereditary cancer genes. These results are returned to the\nuser, within one minute on average and are available in both interactive and\ndownloadable formats.\n  Conclusion: We have developed F3PI, an easy-to-use, interactive web\napplication that makes cancer and genetic risk information more accessible to\nproviders and their patients."}
{"id": "2510.24187", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24187", "abs": "https://arxiv.org/abs/2510.24187", "authors": ["Lucas Lévy", "Jean-Lou Valeau", "Arya Akhavan", "Patrick Rebeschini"], "title": "Self-Concordant Perturbations for Linear Bandits", "comment": null, "summary": "We study the adversarial linear bandits problem and present a unified\nalgorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and\nFollow-the-Perturbed-Leader (FTPL) methods, extending the known connection\nbetween them from the full-information setting. Within this framework, we\nintroduce self-concordant perturbations, a family of probability distributions\nthat mirror the role of self-concordant barriers previously employed in the\nFTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based\nalgorithm that combines self-concordant regularization with efficient\nstochastic exploration. Our approach achieves a regret of $O(d\\sqrt{n \\ln n})$\non both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean\nball, this matches the rate attained by existing self-concordant FTRL methods.\nFor the hypercube, this represents a $\\sqrt{d}$ improvement over these methods\nand matches the optimal bound up to logarithmic factors."}
{"id": "2510.23831", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML", "62J05, 62J07, 62F15, 62F40"], "pdf": "https://arxiv.org/pdf/2510.23831", "abs": "https://arxiv.org/abs/2510.23831", "authors": ["Jiasong Duan", "Hongmei Zhang", "Xianzheng Huang"], "title": "Testing-driven Variable Selection in Bayesian Modal Regression", "comment": "30 pages, 2 figures, preprint under review", "summary": "We propose a Bayesian variable selection method in the framework of modal\nregression for heavy-tailed responses. An efficient expectation-maximization\nalgorithm is employed to expedite parameter estimation. A test statistic is\nconstructed to exploit the shape of the model error distribution to effectively\nseparate informative covariates from unimportant ones. Through simulations, we\ndemonstrate and evaluate the efficacy of the proposed method in identifying\nimportant covariates in the presence of non-Gaussian model errors. Finally, we\napply the proposed method to analyze two datasets arising in genetic and\nepigenetic studies."}
{"id": "2510.23821", "categories": ["stat.AP", "62P05", "G.3"], "pdf": "https://arxiv.org/pdf/2510.23821", "abs": "https://arxiv.org/abs/2510.23821", "authors": ["Łukasz Delong", "Mario Wüthrich"], "title": "Universal Inference for Testing Calibration of Mean Estimates within the Exponential Dispersion Family", "comment": "34 pages", "summary": "Calibration of mean estimates for predictions is a crucial property in many\napplications, particularly in the fields of financial and actuarial\ndecision-making. In this paper, we first review classical approaches for\nvalidating mean-calibration, and we discuss the Likelihood Ratio Test (LRT)\nwithin the Exponential Dispersion Family (EDF). Then, we investigate the\nframework of universal inference to test for mean-calibration. We develop a\nsub-sampled split LRT within the EDF that provides finite sample guarantees\nwith universally valid critical values. We investigate type I error, power and\ne-power of this sub-sampled split LRT, we compare it to the classical LRT, and\nwe propose a novel test statistics based on the sub-sampled split LRT to\nenhance the performance of the calibration test. A numerical analysis verifies\nthat our proposal is an attractive alternative to the classical LRT achieving a\nhigh power in detecting miscalibration."}
{"id": "2510.23764", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.23764", "abs": "https://arxiv.org/abs/2510.23764", "authors": ["Abigail Loe", "Susan Murray", "Zhenke Wu"], "title": "A Random Forest Inverse Probability Weighted Pseudo-Observation Framework for Alternating Recurrent Events", "comment": null, "summary": "Alternating recurrent events, where subjects experience two potentially\ncorrelated event types over time, are common in healthcare, social, and\nbehavioral studies. Often there is a primary event of interest that, when\ntriggered, initiates a period of treatment and recovery measured via a\nsecondary time-to-event. For example, cancer patients can experience repeated\nblood clotting emergencies that require hospitalization followed by discharge,\npeople with alcohol use disorder can have periods of addiction and sobriety, or\ncare partners can experience periods of depression and recovery. Potential\ncensoring of the data requires special handling. Overlaying this are the\nmissing at-risk periods for the primary event type when individuals have\ninitiated the primary event but not reached the subsequent secondary event. In\nthis paper, we develop a framework for regression analysis of censored\nalternating recurrent events that uses a random forest inverse probability\nweighting strategy to avoid bias in the analysis of the time to the primary\nevent due to informative missingness from the alternate secondary state. The\nproposed regression model estimates $\\tau$-restricted mean time to the primary\nevent of interest while taking into account complexities of censored.\nSimulations show good performance of our method when the alternate\ntimes-to-event are either independent or correlated. We analyze a mobile health\nstudy data to evaluate the impact of self-care push notifications on the mental\nstate of caregivers of traumatic brain injury patients."}
{"id": "2510.24601", "categories": ["stat.ML", "cs.LG", "68T07, 62J02"], "pdf": "https://arxiv.org/pdf/2510.24601", "abs": "https://arxiv.org/abs/2510.24601", "authors": ["Jessica Doohan", "Lucas Kook", "Kevin Burke"], "title": "Comparison of generalised additive models and neural networks in applications: A systematic review", "comment": null, "summary": "Neural networks have become a popular tool in predictive modelling, more\ncommonly associated with machine learning and artificial intelligence than with\nstatistics. Generalised Additive Models (GAMs) are flexible non-linear\nstatistical models that retain interpretability. Both are state-of-the-art in\ntheir own right, with their respective advantages and disadvantages. This paper\nanalyses how these two model classes have performed on real-world tabular data.\nFollowing PRISMA guidelines, we conducted a systematic review of papers that\nperformed empirical comparisons of GAMs and neural networks. Eligible papers\nwere identified, yielding 143 papers, with 430 datasets. Key attributes at both\npaper and dataset levels were extracted and reported. Beyond summarising\ncomparisons, we analyse reported performance metrics using mixed-effects\nmodelling to investigate potential characteristics that can explain and\nquantify observed differences, including application area, study year, sample\nsize, number of predictors, and neural network complexity. Across datasets, no\nconsistent evidence of superiority was found for either GAMs or neural networks\nwhen considering the most frequently reported metrics (RMSE, $R^2$, and AUC).\nNeural networks tended to outperform in larger datasets and in those with more\npredictors, but this advantage narrowed over time. Conversely, GAMs remained\ncompetitive, particularly in smaller data settings, while retaining\ninterpretability. Reporting of dataset characteristics and neural network\ncomplexity was incomplete in much of the literature, limiting transparency and\nreproducibility. This review highlights that GAMs and neural networks should be\nviewed as complementary approaches rather than competitors. For many tabular\napplications, the performance trade-off is modest, and interpretability may\nfavour GAMs."}
{"id": "2510.24352", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.24352", "abs": "https://arxiv.org/abs/2510.24352", "authors": ["Hou Jian", "Meng Tan", "Tian Maozai"], "title": "Self-Normalized Quantile Empirical Saddlepoint Approximation", "comment": "24 pages, 3 figures, 12 tables", "summary": "We propose a density-free method for frequentist inference on population\nquantiles, termed Self-Normalized Quantile Empirical Saddlepoint Approximation\n(SNQESA). The approach builds a self-normalized pivot from the indicator score\nfor a fixed quantile threshold and then employs a constrained empirical\nsaddlepoint approximation to obtain highly accurate tail probabilities.\nInverting these tail areas yields confidence intervals and tests without\nestimating the unknown density at the target quantile, thereby eliminating\nbandwidth selection and the boundary issues that affect kernel-based\nWald/Hall-Sheather intervals. Under mild local regularity, the resulting\nprocedures attain higher-order tail accuracy and second-order coverage after\ninversion. Because the pivot is anchored in a bounded Bernoulli reduction, the\nmethod remains reliable for skewed and heavy-tailed distributions and for\nextreme quantiles. Extensive Monte Carlo experiments across light, heavy, and\nmultimodal distributions demonstrate that SNQESA delivers stable coverage and\ncompetitive interval lengths in small to moderate samples while being orders of\nmagnitude faster than large-B resampling schemes. An empirical study on\nValue-at-Risk with rolling windows further highlights the gains in tail\nperformance and computational efficiency. The framework naturally extends to\ntwo-sample quantile differences and to regression-type settings, offering a\npractical, analytically transparent alternative to kernel, bootstrap, and\nempirical-likelihood methods for distribution-free quantile inference."}
{"id": "2510.23976", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.23976", "abs": "https://arxiv.org/abs/2510.23976", "authors": ["Richard Berk"], "title": "Forecasting Melting Points in Svalbard, Norway Using Quantile Gradient Boosting and Adaptive Conformal Prediction Region", "comment": "26 pages, 7 figures, and two blocks of pseudocode", "summary": "Using data from the Longyearbyen weather station, quantile gradient boosting\n(``small AI'') is applied to forecast daily 2023 temperatures in Svalbard,\nNorway. The 0.60 quantile loss weights underestimates about 1.5 times more than\noverestimates. Predictors include five routinely collected indicators of\nweather conditions, each lagged by 14~days, yielding temperature forecasts with\na two-week lead time. Conformal prediction regions quantify forecasting\nuncertainty with provably valid coverage. Forecast accuracy is evaluated with\nattention to local stakeholder concerns, and implications for Arctic adaptation\npolicy are discussed."}
{"id": "2510.23799", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.23799", "abs": "https://arxiv.org/abs/2510.23799", "authors": ["Yujia Sun", "Yang Han", "Xingya Wang", "Szu-Yu Tang", "Yushi Liu", "Jason C. Hsu"], "title": "ETZ: A Modeling Principle for Confirmability of Drug-Development Studies", "comment": null, "summary": "Transitioning from Phase 2 to Phase 3 in drug development, at a rate of\n$\\approx$40%, is the most stringent among phase transitions (Hay et al.\n(2014)). Yet, success rate at Phase 3 leading to approval is only $\\approx$50%\n(Arrowsmith (2011b)). To improve Confirmability, we propose a methodological\nshift: replacing multiple hypothesis testing with inference based on confidence\nsets, and substituting conventional power and sample size calculations with a\nConfidently Bounded Quantile (CBQ) framework.\n  Our confidence set inferences to answer the questions of whether to\ntransition to a Confirmatory study as well as what to designate as the endpoint\nin that study. Construction of our directed confidence sets follows the\nPartitioning Principle, taking the best of each of Pivoting and Neyman\nConfidence Set Construction.\n  Rooted in Tukey's Confidently Bounded Allowance (CBA) (Tukey (1994a)), our\nproposed CBQ makes the transitioning decision following the Correct and Useful\nInference principle in Hsu (1996). CBQ removes from \"power\" the probability of\nrejecting for wrong reasons, eliminating the need for informal discounting in\npower calculation that has existed in the biopharmaceutical industry.\n  ETZ, the modeling principle proposed in Wang et al. (2025), quantifies the\nimpact of three variability components on confirmability. In repeated-measures\nRCTs, it separates within-subject and between-subject variability, further\ndividing the latter into baseline and trajectory components. This enables\ninformed investment decisions for the sponsors on targeting variability\nreduction to improve confirmability. A Shiny-based Confirmability App supports\nall computations."}
{"id": "2510.24616", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24616", "abs": "https://arxiv.org/abs/2510.24616", "authors": ["Jean Barbier", "Francesco Camilli", "Minh-Toan Nguyen", "Mauro Pastore", "Rudy Skerk"], "title": "Statistical physics of deep learning: Optimal learning of a multi-layer perceptron near interpolation", "comment": "30 pages, 19 figures + appendix. This submission supersedes both\n  arXiv:2505.24849 and arXiv:2501.18530", "summary": "For three decades statistical physics has been providing a framework to\nanalyse neural networks. A long-standing question remained on its capacity to\ntackle deep learning models capturing rich feature learning effects, thus going\nbeyond the narrow networks or kernel methods analysed until now. We positively\nanswer through the study of the supervised learning of a multi-layer\nperceptron. Importantly, (i) its width scales as the input dimension, making it\nmore prone to feature learning than ultra wide networks, and more expressive\nthan narrow ones or with fixed embedding layers; and (ii) we focus on the\nchallenging interpolation regime where the number of trainable parameters and\ndata are comparable, which forces the model to adapt to the task. We consider\nthe matched teacher-student setting. It provides the fundamental limits of\nlearning random deep neural network targets and helps in identifying the\nsufficient statistics describing what is learnt by an optimally trained network\nas the data budget increases. A rich phenomenology emerges with various\nlearning transitions. With enough data optimal performance is attained through\nmodel's \"specialisation\" towards the target, but it can be hard to reach for\ntraining algorithms which get attracted by sub-optimal solutions predicted by\nthe theory. Specialisation occurs inhomogeneously across layers, propagating\nfrom shallow towards deep ones, but also across neurons in each layer.\nFurthermore, deeper targets are harder to learn. Despite its simplicity, the\nBayesian-optimal setting provides insights on how the depth, non-linearity and\nfinite (proportional) width influence neural networks in the feature learning\nregime that are potentially relevant way beyond it."}
{"id": "2510.24153", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24153", "abs": "https://arxiv.org/abs/2510.24153", "authors": ["Yuya Takada", "Kiyoshi Izumi"], "title": "Machine Learning for the Production of Official Statistics: Density Ratio Estimation using Biased Transaction Data for Japanese labor statistics", "comment": "23 pages, 9 figures. Under review at Journal of Computational Social\n  Science", "summary": "National statistical institutes are beginning to use non-traditional data\nsources to produce official statistics. These sources, originally collected for\nnon-statistical purposes, include point-of-sales(POS) data and mobile phone\nglobal positioning system(GPS) data. Such data have the potential to\nsignificantly enhance the usefulness of official statistics. In the era of big\ndata, many private companies are accumulating vast amounts of transaction data.\nExploring how to leverage these data for official statistics is increasingly\nimportant. However, progress has been slower than expected, mainly because such\ndata are not collected through sample-based survey methods and therefore\nexhibit substantial selection bias. If this bias can be properly addressed,\nthese data could become a valuable resource for official statistics,\nsubstantially expanding their scope and improving the quality of\ndecision-making, including economic policy. This paper demonstrates that even\nbiased transaction data can be useful for producing official statistics for\nprompt release, by drawing on the concepts of density ratio estimation and\nsupervised learning under covariate shift, both developed in the field of\nmachine learning. As a case study, we show that preliminary statistics can be\nproduced in a timely manner using biased data from a Japanese private\nemployment agency. This approach enables the early release of a key labor\nmarket indicator that would otherwise be delayed by up to a year, thereby\nmaking it unavailable for timely decision-making."}
{"id": "2510.23831", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML", "62J05, 62J07, 62F15, 62F40"], "pdf": "https://arxiv.org/pdf/2510.23831", "abs": "https://arxiv.org/abs/2510.23831", "authors": ["Jiasong Duan", "Hongmei Zhang", "Xianzheng Huang"], "title": "Testing-driven Variable Selection in Bayesian Modal Regression", "comment": "30 pages, 2 figures, preprint under review", "summary": "We propose a Bayesian variable selection method in the framework of modal\nregression for heavy-tailed responses. An efficient expectation-maximization\nalgorithm is employed to expedite parameter estimation. A test statistic is\nconstructed to exploit the shape of the model error distribution to effectively\nseparate informative covariates from unimportant ones. Through simulations, we\ndemonstrate and evaluate the efficacy of the proposed method in identifying\nimportant covariates in the presence of non-Gaussian model errors. Finally, we\napply the proposed method to analyze two datasets arising in genetic and\nepigenetic studies."}
{"id": "2510.23831", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML", "62J05, 62J07, 62F15, 62F40"], "pdf": "https://arxiv.org/pdf/2510.23831", "abs": "https://arxiv.org/abs/2510.23831", "authors": ["Jiasong Duan", "Hongmei Zhang", "Xianzheng Huang"], "title": "Testing-driven Variable Selection in Bayesian Modal Regression", "comment": "30 pages, 2 figures, preprint under review", "summary": "We propose a Bayesian variable selection method in the framework of modal\nregression for heavy-tailed responses. An efficient expectation-maximization\nalgorithm is employed to expedite parameter estimation. A test statistic is\nconstructed to exploit the shape of the model error distribution to effectively\nseparate informative covariates from unimportant ones. Through simulations, we\ndemonstrate and evaluate the efficacy of the proposed method in identifying\nimportant covariates in the presence of non-Gaussian model errors. Finally, we\napply the proposed method to analyze two datasets arising in genetic and\nepigenetic studies."}
{"id": "2510.24394", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24394", "abs": "https://arxiv.org/abs/2510.24394", "authors": ["Sandra Barragán", "Adrián Pérez-Bote", "Carlos Sáez", "David Salgado", "Luis Sanguiao-Sande"], "title": "Streamlining business functions in official statistical production with Machine Learning", "comment": "42 pages, 14 figures, preprint version to appear as a chapter of F.\n  Dumpert (ed.), Foundations and Advances of Machine Learning in Official\n  Statistics", "summary": "We provide a description of pilot and production experiences to streamline\nsome business functions in the official statistical production process using\nstatistical learning models. Our approach is quality-oriented searching for an\nimprovement on accuracy, cost-efficiency, timeliness, granularity, response\nburden reduction, and frequency. Pilot experiences have been conducted with\ndata from real surveys in Statistics Spain (INE)."}
{"id": "2510.23874", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.23874", "abs": "https://arxiv.org/abs/2510.23874", "authors": ["Yichi Zhang", "Ignacio Martinez"], "title": "From Stochasticity to Signal: A Bayesian Latent State Model for Reliable Measurement with LLMs", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automate classification\ntasks in business, such as analyzing customer satisfaction from text. However,\nthe inherent stochasticity of LLMs, in terms of their tendency to produce\ndifferent outputs for the same input, creates a significant measurement error\nproblem that is often neglected with a single round of output, or addressed\nwith ad-hoc methods like majority voting. Such naive approaches fail to\nquantify uncertainty and can produce biased estimates of population-level\nmetrics. In this paper, we propose a principled solution by reframing LLM\nvariability as a statistical measurement error problem and introducing a\nBayesian latent state model to address it. Our model treats the true\nclassification (e.g., customer dissatisfaction) as an unobserved latent\nvariable and the multiple LLM ratings as noisy measurements of this state. This\nframework allows for the simultaneous estimation of the LLM's false positive\nand false negative error rates, the underlying base rate of the phenomenon in\nthe population, the posterior probability of the true state for each individual\nobservation, and the causal impact of a business intervention, if any, on the\nlatent state. Through simulation studies, we demonstrate that our model\naccurately recovers true parameters where naive methods fail. We conclude that\nthis methodology provides a general and reliable framework for converting\nnoisy, probabilistic outputs from LLMs into accurate and actionable insights\nfor scientific and business applications."}
{"id": "2510.24714", "categories": ["stat.ME", "econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24714", "abs": "https://arxiv.org/abs/2510.24714", "authors": ["Jian Yan", "Zhuoxi Li", "Yang Ning", "Yong Chen"], "title": "Machine-Learning-Assisted Comparison of Regression Functions", "comment": null, "summary": "We revisit the classical problem of comparing regression functions, a\nfundamental question in statistical inference with broad relevance to modern\napplications such as data integration, transfer learning, and causal inference.\nExisting approaches typically rely on smoothing techniques and are thus\nhindered by the curse of dimensionality. We propose a generalized notion of\nkernel-based conditional mean dependence that provides a new characterization\nof the null hypothesis of equal regression functions. Building on this\nreformulation, we develop two novel tests that leverage modern machine learning\nmethods for flexible estimation. We establish the asymptotic properties of the\ntest statistics, which hold under both fixed- and high-dimensional regimes.\nUnlike existing methods that often require restrictive distributional\nassumptions, our framework only imposes mild moment conditions. The efficacy of\nthe proposed tests is demonstrated through extensive numerical studies."}
{"id": "2510.24443", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24443", "abs": "https://arxiv.org/abs/2510.24443", "authors": ["Tom Ó Nualláin"], "title": "GNAR-HARX Models for Realised Volatility: Incorporating Exogenous Predictors and Network Effects", "comment": null, "summary": "This project introduces the GNAR-HARX model, which combines Generalised\nNetwork Autoregressive (GNAR) structure with Heterogeneous Autoregressive (HAR)\ndynamics and exogenous predictors such as implied volatility. The model is\ndesigned for forecasting realised volatility by capturing both temporal\npersistence and cross-sectional spillovers in financial markets. We apply it to\ndaily realised variance data for ten international stock indices, generating\none-step-ahead forecasts in a rolling window over an out-of-sample period of\napproximately 16 years (2005-2020).\n  Forecast accuracy is evaluated using the Quasi-Likelihood (QLIKE) loss and\nmean squared error (MSE), and we compare global, standard, and local variants\nacross different network structures and exogenous specifications. The best\nmodel found by QLIKE is a local GNAR-HAR without exogenous variables, while the\nlowest MSE is achieved by a standard GNAR-HARX with implied volatility. Fully\nconnected networks consistently outperform dynamically estimated graphical\nlasso networks.\n  Overall, local and standard GNAR-HAR(X) models deliver the strongest\nforecasts, though at the cost of more parameters than the parsimonious global\nvariant, which nevertheless remains competitive. Across all cases, GNAR-HAR(X)\nmodels outperform univariate HAR(X) benchmarks, which often require more\nparameters than the GNAR-based specifications. While the top model found by\nQLIKE does not use exogenous variables, implied volatility and overnight\nreturns emerge as the most useful predictors when included."}
{"id": "2510.23920", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.23920", "abs": "https://arxiv.org/abs/2510.23920", "authors": ["Grant Hopkins", "Sarah Teichman", "Ellen Graham", "Amy D Willis"], "title": "Nonparametric Identification and Estimation of Ratios of Multi-Category Means under Preferential Sampling", "comment": null, "summary": "Multi-category data arise in diverse fields including marketing, chemistry,\npublic policy, genomics, political science, and ecology. We consider the\nproblem of estimating ratios of category-specific means in a fully\nnonparametric setting, allowing for both observational units and categories to\nbe preferentially sampled. We consider covariate-adjusted and unadjusted\nestimands that are non-parametrically defined and straightforward to interpret.\nWhile identifiability for related models has been established through\nparametric distributions or restrictions on the conditional mean (e.g.,\nlog-linearity), we show that identifiability can be obtained through an\nindependence assumption or a category constraint, such as a reference category\nor a centering function. We develop an efficient, doubly-robust targeted\nminimum loss based estimator with excellent finite-sample performance,\nincluding in the setting of a large number of infrequently observed categories.\nWe contrast the performance of our method with related approaches via\nsimulation, and apply it to identify bacteria that are differentially abundant\nin diarrheal cases compared to controls. Our work provides a general framework\nfor studying parameter identifiability in compositional data settings without\nrequiring parametric assumptions on the data distribution."}
{"id": "2510.24018", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24018", "abs": "https://arxiv.org/abs/2510.24018", "authors": ["Takuya Kawahara", "Sean McGrath", "Jessica G Young"], "title": "Illustrating implications of misaligned causal questions and statistics in settings with competing events and interest in treatment mechanisms", "comment": "5 figures, 1 table", "summary": "In the presence of competing events, many investigators are interested in a\ndirect treatment effect on the event of interest that does not capture\ntreatment effects on competing events. Classical survival analysis methods that\ntreat competing events like censoring events, at best, target a controlled\ndirect effect: the effect of the treatment under a difficult to imagine and\ntypically clinically irrelevant scenario where competing events are somehow\neliminated. A separable direct effect, quantifying the effect of a future\nmodified version of the treatment, is an alternative direct effect notion that\nmay better align with an investigator's underlying causal question. In this\npaper, we provide insights into the implications of naively applying an\nestimator constructed for a controlled direct effect (i.e., \"censoring by\ncompeting events\") when the actual causal effect of interest is a separable\ndirect effect. We illustrate the degree to which controlled and separable\ndirect effects may take different values, possibly even different signs, and\nthe degree to which these two different effects may be differentially impacted\nby violation and/or near violation of their respective identifying conditions\nunder a range of data generating scenarios. Finally, we provide an empirical\ncomparison of inverse probability of censoring weighting to an alternative\nweighted estimator specifically structured for a separable effect using data\nfrom a randomized trial of estrogen therapy and prostate cancer mortality."}
{"id": "2510.24130", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24130", "abs": "https://arxiv.org/abs/2510.24130", "authors": ["Myra B. McGuinness", "Joanne E. McKenzie", "Andrew Forbes", "Flora Hui", "Keith R. Martin", "Robert J. Casson", "Amalia Karahalios"], "title": "Quantifying inconsistency in one-stage individual participant data meta-analyses of treatment-covariate interactions: a simulation study", "comment": null, "summary": "It is recommended that measures of between-study effect heterogeneity be\nreported when conducting individual-participant data meta-analyses (IPD-MA).\nMethods exist to quantify inconsistency between trials via I^2 (the percentage\nof variation in the treatment effect due to between-study heterogeneity) when\nconducting two-stage IPD-MA, and when conducting one-stage IPD-MA with\napproximately equal numbers of treatment and control group participants. We\nextend formulae to estimate I^2 when investigating treatment-covariate\ninteractions with unequal numbers of participants across subgroups and/or\ncontinuous covariates. A simulation study was conducted to assess the agreement\nin values of I^2 between those derived from two-stage models using traditional\nmethods and those derived from equivalent one-stage models. Fourteen scenarios\ndiffered by the magnitude of between-trial heterogeneity, the number of trials,\nand the average number of participants in each trial. Bias and precision of I^2\nwere similar between the one- and two-stage models. The mean difference in I^2\nbetween equivalent models ranged between -1.0 and 0.0 percentage points across\nscenarios. However, disparities were larger in simulated datasets with smaller\nsamples sizes with up to 19.4 percentage points difference between models.\nThus, the estimates of I^2 derived from these extended methods can be\ninterpreted similarly to those from existing formulae for two-stage models."}
{"id": "2510.24183", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24183", "abs": "https://arxiv.org/abs/2510.24183", "authors": ["Bardia Panahbehagh", "Mehdi Mohebbi", "Amir Mohammad HosseiniNasab"], "title": "Intelligent n-Means Spatial Sampling", "comment": null, "summary": "Well-spread samples are desirable in many disciplines because they improve\nestimation when target variables exhibit spatial structure. This paper\nintroduces an integrated methodological framework for spreading samples over\nthe population's spatial coordinates. First, we propose a new,\ntranslation-invariant spreadness index that quantifies spatial balance with a\nclear interpretation. Second, we develop a clustering method that balances\nclusters with respect to an auxiliary variable; when the auxiliary variable is\nthe inclusion probability, the procedure yields clusters whose totals are one,\nso that a single draw per cluster is, in principle, representative and produces\nunits optimally spread along the population coordinates, an attractive feature\nfor finite population sampling. Third, building on the graphical sampling\nframework, we design an efficient sampling scheme that further enhances spatial\nbalance. At its core lies an intelligent, computationally efficient search\nlayer that adapts to the population's spatial structure and inclusion\nprobabilities, tailoring a design to each specific population to maximize\nspread. Across diverse spatial patterns and both equal- and unequal-probability\nregimes, this intelligent coupling consistently outperformed all rival\nspread-oriented designs on dispersion metrics, while the spreadness index\nremained informative and the clustering step improved representativeness."}
{"id": "2510.24319", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.24319", "abs": "https://arxiv.org/abs/2510.24319", "authors": ["Mohamedou Ould Haye", "Anne Philippe"], "title": "A Frequency-Domain NonStationarity Test for dependent data", "comment": null, "summary": "Distinguishing long-memory behaviour from nonstationarity is challenging, as\nboth produce slowly decaying sample autocovariances. Existing stationarity\ntests either fail to account for long-memory processes or exhibit poor\nempirical size, particularly near the boundary between stationarity and\nnonstationarity. We propose a new, parameter-free testing procedure based on\nthe evaluation of periodograms across multiple epochs. The limiting\ndistributions derived here are obtained under stationarity and nonstationarity\nassumptions and analytically tractable, expressed as finite sums of weighted\nindependent $\\chi^2$ random variables. Simulation studies indicate that the\nproposed method performs favorably compared to existing approaches."}
{"id": "2510.24349", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24349", "abs": "https://arxiv.org/abs/2510.24349", "authors": ["Luzia A. Trinca", "Steven G. Gilmour"], "title": "Pseudo-Bayesian Optimal Designs for Fitting Fractional Polynomial Response Surface Models", "comment": "31 pages", "summary": "Fractional polynomial models are potentially useful for response surfaces\ninvestigations. With the availability of routines for fitting nonlinear models\nin statistical packages they are increasingly being used. However, as in all\nexperiments the design should be chosen such that the model parameters are\nestimated as efficiently as possible. The design choice for such models\ninvolves the known nonlinear models' design difficulties but\n\\cite{gilmour_trinca_2012b} proposed a methodology capable of producing exact\ndesigns that makes use of the computing facilities available today. In this\npaper, we use this methodology to find Bayesian optimal exact designs for\nseveral fractional polynomial models. The optimum designs are compared to\nvarious standard designs in response surface problems."}
{"id": "2510.24352", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.24352", "abs": "https://arxiv.org/abs/2510.24352", "authors": ["Hou Jian", "Meng Tan", "Tian Maozai"], "title": "Self-Normalized Quantile Empirical Saddlepoint Approximation", "comment": "24 pages, 3 figures, 12 tables", "summary": "We propose a density-free method for frequentist inference on population\nquantiles, termed Self-Normalized Quantile Empirical Saddlepoint Approximation\n(SNQESA). The approach builds a self-normalized pivot from the indicator score\nfor a fixed quantile threshold and then employs a constrained empirical\nsaddlepoint approximation to obtain highly accurate tail probabilities.\nInverting these tail areas yields confidence intervals and tests without\nestimating the unknown density at the target quantile, thereby eliminating\nbandwidth selection and the boundary issues that affect kernel-based\nWald/Hall-Sheather intervals. Under mild local regularity, the resulting\nprocedures attain higher-order tail accuracy and second-order coverage after\ninversion. Because the pivot is anchored in a bounded Bernoulli reduction, the\nmethod remains reliable for skewed and heavy-tailed distributions and for\nextreme quantiles. Extensive Monte Carlo experiments across light, heavy, and\nmultimodal distributions demonstrate that SNQESA delivers stable coverage and\ncompetitive interval lengths in small to moderate samples while being orders of\nmagnitude faster than large-B resampling schemes. An empirical study on\nValue-at-Risk with rolling windows further highlights the gains in tail\nperformance and computational efficiency. The framework naturally extends to\ntwo-sample quantile differences and to regression-type settings, offering a\npractical, analytically transparent alternative to kernel, bootstrap, and\nempirical-likelihood methods for distribution-free quantile inference."}
{"id": "2510.24453", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24453", "abs": "https://arxiv.org/abs/2510.24453", "authors": ["Carolin Drenda", "Dennis Dobler", "Merle Munko", "Andrew Titman"], "title": "Comparison of Estimators for Multi-State Models in Potentially Non-Markov Processes", "comment": null, "summary": "Various estimators for modelling the transition probabilities in multi-state\nmodels have been proposed, e.g., the Aalen-Johansen estimator, the landmark\nAalen-Johansen estimator, and a hybrid Aalen-Johansen estimator. While the\nAalen-Johansen estimator is generally only consistent under the rather\nrestrictive Markov assumption, the landmark Aalen-Johansen estimator can handle\nnon-Markov multi-state models. However, the landmark Aalen-Johansen estimator\nleads to a strict data reduction and, thus, to an increased variance. The\nhybrid Aalen-Johansen estimator serves as a compromise by, firstly, checking\nwith a log-rank-based test whether the Markov assumption is satisfied.\nSecondly, landmarking is only applied if the Markov assumption is rejected. In\nthis work, we propose a new hybrid Aalen-Johansen estimator which uses a Cox\nmodel instead of the log-rank-based test to check the Markov assumption in the\nfirst step. Furthermore, we compare the four estimators in an extensive\nsimulation study across Markov, semi-Markov, and distinct non-Markov settings.\nIn order to get deep insights into the performance of the estimators, we\nconsider four different measures: bias, variance, root mean squared error, and\ncoverage rate. Additionally, further influential factors on the estimators such\nas the form and degree of non-Markov behaviour, the different transitions, and\nthe starting time are analysed. The main result of the simulation study is that\nthe hybrid Aalen-Johansen estimators yield favourable results across various\nmeasures and settings."}
{"id": "2510.24526", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24526", "abs": "https://arxiv.org/abs/2510.24526", "authors": ["Lorenzo Ghilotti", "Federico Camerlenghi", "Tommaso Rigon", "Michele Guindani"], "title": "Bayesian nonparametric modeling of multivariate count data with an unknown number of traits", "comment": null, "summary": "Feature and trait allocation models are fundamental objects in Bayesian\nnonparametrics and play a prominent role in several applications. Existing\napproaches, however, typically assume full exchangeability of the data, which\nmay be restrictive in settings characterized by heterogeneous but related\ngroups. In this paper, we introduce a general and tractable class of Bayesian\nnonparametric priors for partially exchangeable trait allocation models,\nrelying on completely random vectors. We provide a comprehensive theoretical\nanalysis, including closed-form expressions for marginal and posterior\ndistributions, and illustrate the tractability of our framework in the cases of\nbinary and Poisson-distributed traits. A distinctive aspect of our approach is\nthat the number of traits is a random quantity, thereby allowing us to model\nand estimate unobserved traits. Building on these results, we also develop a\nnovel mixture model that infers the group partition structure from the data,\neffectively clustering trait allocations. This extension generalizes Bayesian\nnonparametric latent class models and avoids the systematic overclustering that\narises when the number of traits is assumed to be fixed. We demonstrate the\npractical usefulness of our methodology through an application to the\n`Ndrangheta criminal network from the Operazione Infinito investigation, where\nour model provides insights into the organization of illicit activities."}
{"id": "2510.24539", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24539", "abs": "https://arxiv.org/abs/2510.24539", "authors": ["Ron Ronald Togunov", "Simen Knutsen Furset", "Martin Emil Pettersen", "Robert Brian O'Hara"], "title": "Unbiased likelihood estimation of the Langevin diffusion for animal movement modelling", "comment": null, "summary": "The resource selection function provides a model for describing habitat\nsuitability, which can be used to predict the spatial utilisation distribution\nof a species. Tracking data can be modelled as a point process, but this is\nmade complicated by the presence of temporally irregular autocorrelation. One\nproposed model to handle this is the continuous-time Langevin diffusion.\nHowever, current estimation techniques obtain increasingly biased parameter\nestimates as the intervals between observations increase. In this paper, we\naddress this issue using Brownian bridges in an importance sampling scheme to\nimprove the likelihood approximation of the Langevin diffusion model. We show\nusing a series of simulation studies that this approach effectively removes the\nbias in many scenarios. Furthermore, we show that the model actually performs\nbetter at lower sampling rates over a longer duration than shorter duration at\na higher sampling frequency. This research broadens the applicability of\nLangevin diffusion models to telemetry data at coarser resolutions."}
{"id": "2510.24714", "categories": ["stat.ME", "econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24714", "abs": "https://arxiv.org/abs/2510.24714", "authors": ["Jian Yan", "Zhuoxi Li", "Yang Ning", "Yong Chen"], "title": "Machine-Learning-Assisted Comparison of Regression Functions", "comment": null, "summary": "We revisit the classical problem of comparing regression functions, a\nfundamental question in statistical inference with broad relevance to modern\napplications such as data integration, transfer learning, and causal inference.\nExisting approaches typically rely on smoothing techniques and are thus\nhindered by the curse of dimensionality. We propose a generalized notion of\nkernel-based conditional mean dependence that provides a new characterization\nof the null hypothesis of equal regression functions. Building on this\nreformulation, we develop two novel tests that leverage modern machine learning\nmethods for flexible estimation. We establish the asymptotic properties of the\ntest statistics, which hold under both fixed- and high-dimensional regimes.\nUnlike existing methods that often require restrictive distributional\nassumptions, our framework only imposes mild moment conditions. The efficacy of\nthe proposed tests is demonstrated through extensive numerical studies."}
{"id": "2510.23666", "categories": ["stat.ML", "cs.LG", "stat.ME", "I.2.6; G.3; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.23666", "abs": "https://arxiv.org/abs/2510.23666", "authors": ["Junpeng Gong", "Chunkai Wang", "Hao Li", "Jinyong Ma", "Haoxuan Li", "Xu He"], "title": "Beyond Normality: Reliable A/B Testing with Non-Gaussian Data", "comment": "11 pages, 3 figures", "summary": "A/B testing has become the cornerstone of decision-making in online markets,\nguiding how platforms launch new features, optimize pricing strategies, and\nimprove user experience. In practice, we typically employ the pairwise $t$-test\nto compare outcomes between the treatment and control groups, thereby assessing\nthe effectiveness of a given strategy. To be trustworthy, these experiments\nmust keep Type I error (i.e., false positive rate) under control; otherwise, we\nmay launch harmful strategies. However, in real-world applications, we find\nthat A/B testing often fails to deliver reliable results. When the data\ndistribution departs from normality or when the treatment and control groups\ndiffer in sample size, the commonly used pairwise $t$-test is no longer\ntrustworthy. In this paper, we quantify how skewed, long tailed data and\nunequal allocation distort error rates and derive explicit formulas for the\nminimum sample size required for the $t$-test to remain valid. We find that\nmany online feedback metrics require hundreds of millions samples to ensure\nreliable A/B testing. Thus we introduce an Edgeworth-based correction that\nprovides more accurate $p$-values when the available sample size is limited.\nOffline experiments on a leading A/B testing platform corroborate the practical\nvalue of our theoretical minimum sample size thresholds and demonstrate that\nthe corrected method substantially improves the reliability of A/B testing in\nreal-world conditions."}
{"id": "2510.24394", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24394", "abs": "https://arxiv.org/abs/2510.24394", "authors": ["Sandra Barragán", "Adrián Pérez-Bote", "Carlos Sáez", "David Salgado", "Luis Sanguiao-Sande"], "title": "Streamlining business functions in official statistical production with Machine Learning", "comment": "42 pages, 14 figures, preprint version to appear as a chapter of F.\n  Dumpert (ed.), Foundations and Advances of Machine Learning in Official\n  Statistics", "summary": "We provide a description of pilot and production experiences to streamline\nsome business functions in the official statistical production process using\nstatistical learning models. Our approach is quality-oriented searching for an\nimprovement on accuracy, cost-efficiency, timeliness, granularity, response\nburden reduction, and frequency. Pilot experiences have been conducted with\ndata from real surveys in Statistics Spain (INE)."}
