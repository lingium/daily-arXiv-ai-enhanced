<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 13]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.AP](#stat.AP) [Total: 5]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [A subsampling approach for large data sets when the Generalised Linear Model is potentially misspecified](https://arxiv.org/abs/2510.05902)
*Amalan Mahendran,Helen Thompson,James M. McGree*

Main category: stat.ME

TL;DR: 提出了一种基于可能错误指定的统计模型来确定子采样概率的新方法，通过评估预测均方误差来选择信息子集，避免对模型正确性的强假设。


<details>
  <summary>Details</summary>
Motivation: 现有子采样方法依赖于正确指定的统计模型来生成采样概率，但在实践中模型往往难以正确设定，这限制了方法的实用性。

Method: 基于承认可能错误指定的统计模型，通过最小化预测均方误差来确定子采样概率，不假设模型能完全描述大数据集。

Result: 在模拟研究和两个真实世界大数据集分析中，该方法性能优于现有子采样技术，显示出实用价值。

Conclusion: 提出的方法在模型可能错误指定的情况下仍能有效选择信息子集，相比现有实践具有优势。

Abstract: Subsampling is a computationally efficient and scalable method to draw
inference in large data settings based on a subset of the data rather than
needing to consider the whole dataset. When employing subsampling techniques, a
crucial consideration is how to select an informative subset based on the
queries posed by the data analyst. A recently proposed method for this purpose
involves randomly selecting samples from the large dataset based on subsampling
probabilities. However, a major drawback of this approach is that the derived
subsampling probabilities are typically based on an assumed statistical model
which may be difficult to correctly specify in practice. To address this
limitation, we propose to determine subsampling probabilities based on a
statistical model that we acknowledge may be misspecified. To do so, we propose
to evaluate the subsampling probabilities based on the Mean Squared Error (MSE)
of the predictions from a model that is not assumed to completely describe the
large dataset. We apply our subsampling approach in a simulation study and for
the analysis of two real-world large datasets, where its performance is
benchmarked against existing subsampling techniques. The findings suggest that
there is value in adopting our approach over current practice.

</details>


### [2] [Power-divergence copulas: A new class of Archimedean copulas, with an insurance application](https://arxiv.org/abs/2510.06177)
*Alan R. Pearse,Howard Bondell*

Main category: stat.ME

TL;DR: 论文证明在特定约定下，phi散度的凸函数也能生成至少二维的Archimedean copulas。特别地，开发了与重要幂散度族相关的Archimedean copulas家族，称为幂散度copulas，并全面研究了其性质。


<details>
  <summary>Details</summary>
Motivation: 探索phi散度凸函数与Archimedean copulas之间的联系，特别是为幂散度族开发新的copulas家族，以提供更灵活的多元依赖建模工具。

Method: 在特定约定下，利用phi散度的凸函数生成Archimedean copulas，特别关注幂散度copulas家族的构建和性质分析。

Result: 成功构建了幂散度copulas家族，研究了其连续性、排序、极限情况、Kendall's tau和尾部依赖系数等性质。在丹麦火灾保险数据集应用中，该copulas能充分拟合两种火灾相关损失的二元分布，而多个基准copulas无法做到。

Conclusion: 幂散度copulas提供了一个有效的多元依赖建模框架，在实证应用中表现出优于传统Archimedean、极值和椭圆copulas的拟合能力。

Abstract: This paper demonstrates that, under a particular convention, the convex
functions that characterise the phi divergences also generate Archimedean
copulas in at least two dimensions. As a special case, we develop the family of
Archimedean copulas associated with the important family of power divergences,
which we call the power-divergence copulas. The properties of the family are
extensively studied, including the subfamilies that are absolutely continuous
or have a singular component, the ordering of the family, limiting cases (i.e.,
the Frechet-Hoeffding lower bound and Frechet-Hoeffding upper bound), the
Kendall's tau and tail-dependence coefficients, and cases that extend to three
or more dimensions. In an illustrative application, the power-divergence
copulas are used to model a Danish fire insurance dataset. It is shown that the
power-divergence copulas provide an adequate fit to the bivariate distribution
of two kinds of fire-related losses claimed by businesses, while several
benchmarks (a suite of well known Archimedean, extreme-value, and elliptical
copulas) do not.

</details>


### [3] [Procrustes Problems on Random Matrices](https://arxiv.org/abs/2510.05182)
*Hajg Jasa,Ronny Bergmann,Christian Kümmerle,Avanti Athreya,Zachary Lubberts*

Main category: stat.ME

TL;DR: 该论文比较了不同Procrustes问题中矩阵范数选择对点集对齐的影响，发现在许多应用中可以用计算更简单的Frobenius范数替代谱范数和鲁棒范数。


<details>
  <summary>Details</summary>
Motivation: 比较不同观测集通常需要进行对齐或配准，这些优化问题的复杂度各不相同，从简单的闭式解到需要先进技术的问题都有。研究不同矩阵范数选择对Procrustes对齐问题的影响。

Method: 比较不同Procrustes问题中两个点集在各种扰动后的对齐，通过最小化一个矩阵与另一个矩阵正交变换后差异的范数。利用非光滑黎曼优化技术，分析不同范数选择对各种扰动的适用性。

Result: 研究表明，在从低维对齐到随机网络假设检验等多个应用中，当谱范数或鲁棒范数是适当选择时，通常可以用计算更便宜的Frobenius范数闭式解来替代。

Conclusion: 这项工作强化了优化、几何和统计学之间的协同作用，为点集对齐问题提供了更高效的计算解决方案。

Abstract: Meaningful comparison between sets of observations often necessitates
alignment or registration between them, and the resulting optimization problems
range in complexity from those admitting simple closed-form solutions to those
requiring advanced and novel techniques. We compare different Procrustes
problems in which we align two sets of points after various perturbations by
minimizing the norm of the difference between one matrix and an orthogonal
transformation of the other. The minimization problem depends significantly on
the choice of matrix norm; we highlight recent developments in nonsmooth
Riemannian optimization and characterize which choices of norm work best for
each perturbation. We show that in several applications, from low-dimensional
alignments to hypothesis testing for random networks, when Procrustes alignment
with the spectral or robust norm is the appropriate choice, it is often
feasible to replace the computationally more expensive spectral and robust
minimizers with their closed-form Frobenius-norm counterpart. Our work
reinforces the synergy between optimization, geometry, and statistics.

</details>


### [4] [An efficient hybrid approach of quantile and expectile regression](https://arxiv.org/abs/2510.05268)
*Abdellah Atanane,Abdallah Mkhadri,Karim Oualkacha*

Main category: stat.ME

TL;DR: 提出了一种结合分位数和期望回归的混合方法(HQER)，通过参数γ在0到1之间调节，平衡了稳健性和效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 分位数回归对异常值稳健但效率较低，期望回归效率高但对重尾分布缺乏稳健性。需要解决这种稳健性与效率之间的权衡问题。

Method: 引入参数γ(0-1)来结合分位数和期望回归的损失函数，在线性回归框架下估计新的位置参数族，称为HQER。

Result: 建立了HQER估计量的渐近性质，模拟研究表明在多个场景下HQER优于分位数、期望回归和k次幂期望回归估计量。

Conclusion: HQER方法在保持稳健性的同时提高了效率，在实际数据集应用中展示了实用价值。

Abstract: Quantiles and expectiles are determined by different loss functions:
asymmetric least absolute deviation for quantiles and asymmetric squared loss
for expectiles. This distinction ensures that quantile regression methods are
robust to outliers but somewhat less effective than expectile regression,
especially for normally distributed data. However, expectile regression is
vulnerable to lack of robustness, especially for heavy-tailed distributions. To
address this trade-off between robustness and effectiveness, we propose a novel
approach. By introducing a parameter $\gamma$ that ranges between 0 and 1, we
combine the aforementioned loss functions, resulting in a hybrid approach of
quantiles and expectiles. This fusion leads to the estimation of a new type of
location parameter family within the linear regression framework, termed Hybrid
of Quantile and Expectile Regression (HQER). The asymptotic properties of the
resulting estimaror are then established. Through simulation studies, we
compare the asymptotic relative efficiency of the HQER estimator with its
competitors, namely the quantile, expectile, and $k$th power expectile
regression estimators. Our results show that HQER outperforms its competitors
in several simulation scenarios. In addition, we apply HQER to a real dataset
to illustrate its practical utility.

</details>


### [5] [A new composite Mann-Whitney test for two-sample survival comparisons with right-censored data](https://arxiv.org/abs/2510.05353)
*Abid Hussain,Touqeer Ahmad*

Main category: stat.ME

TL;DR: 提出一种新的无分布两样本检验方法，通过将数据分解为未删失和删失子集，构建复合检验统计量来自动适应各种生存分布差异模式。


<details>
  <summary>Details</summary>
Motivation: 标准生存分析检验（如Log rank和Wilcoxon）的功效高度依赖于未知的备择假设性质，需要选择合适的检验方法。

Method: 基于数据战略分解为未删失和删失子集，构建由两个独立Mann-Whitney统计量之和组成的复合检验统计量。

Result: 模拟研究表明该方法能稳健维持名义I类错误率，在标准场景中与传统最优检验竞争力相当，在生存曲线交叉的复杂场景中表现更优，且对高删失水平具有显著鲁棒性。

Conclusion: 该方法提供了一个强大、通用且计算简单的生存分析工具，其功效在广泛备择假设下近似达到Log rank或Wilcoxon检验的最大功效。

Abstract: A fundamental challenge in comparing two survival distributions with right
censored data is the selection of an appropriate nonparametric test, as the
power of standard tests like the Log rank and Wilcoxon is highly dependent on
the often unknown nature of the alternative hypothesis. This paper introduces a
new, distribution free two sample test designed to overcome this limitation.
The proposed method is based on a strategic decomposition of the data into
uncensored and censored subsets, from which a composite test statistic is
constructed as the sum of two independent Mann Whitney statistics. This design
allows the test to automatically and inherently adapt to various patterns of
difference including early, late, and crossing hazards without requiring pre
specified parameters, pre testing, or complex weighting schemes. An extensive
Monte Carlo simulation study demonstrates that the proposed test robustly
maintains the nominal Type I error rate. Crucially, its power is highly
competitive with the optimal traditional tests in standard scenarios and
superior in complex settings with crossing survival curves, while also
exhibiting remarkable robustness to high levels of censoring. The test power
effectively approximates the maximum power achievable by either the Log rank or
Wilcoxon tests across a wide range of alternatives, offering a powerful,
versatile, and computationally simple tool for survival analysis.

</details>


### [6] [Automated Gating for Flow Cytometry Data Using a Kernel-Smoothed EM Algorithm](https://arxiv.org/abs/2510.06051)
*Farhad de Sousa,François Ribalet,Jacob Bien*

Main category: stat.ME

TL;DR: 提出了一种基于时间演化高斯混合模型和核平滑的快速自动门控方法，用于识别浮游植物亚群，优于专家手动门控。


<details>
  <summary>Details</summary>
Motivation: 浮游植物在全球碳循环和氧气生产中起关键作用，但现有高流量细胞术数据中的亚群识别（门控）主要依赖手动处理，效率低下且难以处理大规模数据。

Method: 使用期望最大化类算法配合核平滑，拟合时间演化高斯混合模型来自动识别浮游植物亚群。

Result: 在模拟数据上验证了方法的有效性和鲁棒性，在海洋巡航数据上显示该方法不仅能复制还能超越专家手动门控的效果。

Conclusion: 该方法为浮游植物亚群分析提供了快速、准确的自动化解决方案，并提供了高效的R包实现。

Abstract: Phytoplankton are microscopic algae responsible for roughly half of the
world's photosynthesis that play a critical role in global carbon cycles and
oxygen production, and measuring the abundance of their subtypes across a wide
range of spatiotemporal scales is of great relevance to oceanography.
High-frequency flow cytometry is a powerful technique in which oceanographers
at sea can rapidly record the optical properties of tens of thousands of
individual phytoplankton cells every few minutes. Identifying distinct
subpopulations within these vast datasets (a process known as "gating") remains
a major challenge and has largely been performed manually so far. In this
paper, we introduce a fast, automated gating method, which accurately
identifies phytoplankton populations by fitting a time-evolving mixture of
Gaussians model using an expectation-maximization-like algorithm with kernel
smoothing. We use simulated data to demonstrate the validity and robustness of
this approach, and use oceanographic cruise data to highlight the method's
ability to not only replicate but surpass expert manual gating. Finally, we
provide the flowkernel R package, written in literate programming, that
implements the algorithm efficiently.

</details>


### [7] [Sparse-Group Factor Analysis for High-Dimensional Time Series](https://arxiv.org/abs/2510.05370)
*Xin Wang,Xialu Liu*

Main category: stat.ME

TL;DR: 本文提出了一种结合个体稀疏性和组稀疏性的双稀疏因子模型，通过融入领域知识和先验信息来提高因子载荷的可解释性。


<details>
  <summary>Details</summary>
Motivation: 因子分析中潜在因子的可解释性是一个关键挑战。在高维时间序列应用中，变量通常具有自然的组结构，因此需要一种能够同时考虑个体稀疏性和组稀疏性的方法来提高因子载荷的可解释性。

Method: 基于Liu和Wang的稀疏载荷因子模型框架，本文提出了双稀疏因子模型，同时建模载荷矩阵的个体稀疏性和组稀疏性，并开发了相应的估计算法来估计载荷矩阵和公共成分。

Result: 模拟研究显示了所提出方法的优越性能，实际数据应用表明融入先验知识能够产生更可解释的结果。

Conclusion: 通过结合个体稀疏性和组稀疏性，本文提出的双稀疏因子模型能够有效利用领域知识，显著提高因子载荷的可解释性，为高维数据分析提供了更实用的工具。

Abstract: Factor analysis is a widely used technique for dimension reduction in
high-dimensional data. However, a key challenge in factor models lies in the
interpretability of the latent factors. One intuitive way to interpret these
factors is through their associated loadings. Liu and Wang proposed a novel
framework that redefines factor models with sparse loadings to enhance
interpretability. In many high-dimensional time series applications, variables
exhibit natural group structures. Building on this idea, our paper incorporates
domain knowledge and prior information by modeling both individual sparsity and
group sparsity in the loading matrix. This dual-sparsity framework further
improves the interpretability of the estimated factors. We develop an algorithm
to estimate both the loading matrix and the common component, and we establish
the asymptotic properties of the resulting estimators. Simulation studies
demonstrate the strong performance of the proposed method, and a real-data
application illustrates how incorporating prior knowledge leads to more
interpretable results.

</details>


### [8] [Can language models boost the power of randomized experiments without statistical bias?](https://arxiv.org/abs/2510.05545)
*Xinrui Ruan,Xinwei Ma,Yingfei Wang,Waverly Wei,Jingshen Wang*

Main category: stat.ME

TL;DR: CALM框架利用LLM预测增强RCT因果分析精度，通过异质校准步骤校正偏差，在保持统计有效性的同时提高效率


<details>
  <summary>Details</summary>
Motivation: 现代RCT收集大量非结构化数据但很少用于因果分析，传统方法受限于成本和样本量，需要利用LLM的预测能力提升分析精度

Method: 将LLM输出作为辅助预后信息，通过异质校准步骤进行残差化和最优重加权，开发few-shot变体聚合随机采样演示集的预测

Result: CALM在模拟实验中相比基准方法方差更低，在零样本和少样本设置中有效，对提示设计保持稳定

Conclusion: CALM通过原则性使用LLM利用非结构化数据和预训练知识，为RCT提供更精确因果分析的实用路径

Abstract: Randomized experiments or randomized controlled trials (RCTs) are gold
standards for causal inference, yet cost and sample-size constraints limit
power. Meanwhile, modern RCTs routinely collect rich, unstructured data that
are highly prognostic of outcomes but rarely used in causal analyses. We
introduce CALM (Causal Analysis leveraging Language Models), a statistical
framework that integrates large language models (LLMs) predictions with
established causal estimators to increase precision while preserving
statistical validity. CALM treats LLM outputs as auxiliary prognostic
information and corrects their potential bias via a heterogeneous calibration
step that residualizes and optimally reweights predictions. We prove that CALM
remains consistent even when LLM predictions are biased and achieves efficiency
gains over augmented inverse probability weighting estimators for various
causal effects. In particular, CALM develops a few-shot variant that aggregates
predictions across randomly sampled demonstration sets. The resulting
U-statistic-like predictor restores i.i.d. structure and also mitigates
prompt-selection variability. Empirically, in simulations calibrated to a
mobile-app depression RCT, CALM delivers lower variance relative to other
benchmarking methods, is effective in zero- and few-shot settings, and remains
stable across prompt designs. By principled use of LLMs to harness unstructured
data and external knowledge learned during pretraining, CALM provides a
practical path to more precise causal analyses in RCTs.

</details>


### [9] [A Bivariate DAR($1$) model for ordinal time series](https://arxiv.org/abs/2510.05680)
*Anna Nalpantidi,Dimitris Karlis*

Main category: stat.ME

TL;DR: 提出了一种二元向量值离散自回归模型(BDAR(1))，用于离散时间序列分析，通过copula定义随机机制和创新的联合分布。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理二元离散时间序列的模型，特别是针对二元有序时间序列，解决现有模型在处理依赖随机机制和依赖创新方面的局限性。

Method: BDAR(1)模型假设每个时间序列遵循其自身的单变量DAR(1)模型，使用copula定义伯努利向量的随机机制联合分布和创新项的联合分布。

Result: 模拟研究表明该模型即使在中等样本量下也能提供有效的估计，实际失业状态数据应用验证了模型的有效性。

Conclusion: BDAR(1)模型为二元离散时间序列分析提供了一个有效的框架，特别适用于二元有序时间序列，具有良好的估计性能和应用价值。

Abstract: We present a bivariate vector valued discrete autoregressive model of order
$1$ (BDAR($1$)) for discrete time series. The BDAR($1$) model assumes that each
time series follows its own univariate DAR($1$) model with dependent random
mechanisms that determine from which component the current status occurs and
dependent innovations. The joint distribution of the random mechanisms which
are expressed by Bernoulli vectors are proposed to be defined through copulas.
The same holds for the joint distribution of innovation terms. Properties of
the model are provided, while special focus is given to the case of bivariate
ordinal time series. A simulation study is presented, indicating that model
provides efficient estimates even in case of moderate sample size. Finally, a
real data application on unemployment state of two countries is presented, for
illustrating the proposed model.

</details>


### [10] [Extension of Wald-Wolfowitz Runs Test for Regression Validity Testing with Repeated Measures of Independent Variable](https://arxiv.org/abs/2510.05861)
*Bo-Yao Lian,Nelson G. Chen*

Main category: stat.ME

TL;DR: 扩展Wald-Wolfowitz游程检验以处理具有重复测量的回归分析，通过随机置换每个自变量值对应的数据点来避免信息损失和加权问题。


<details>
  <summary>Details</summary>
Motivation: 传统Wald-Wolfowitz游程检验只能评估单一自变量回归曲线的正确性，无法处理具有重复测量的情况。当存在多个数据点对应相同自变量值时，需要扩展该方法以避免信息损失和加权问题。

Method: 对每个自变量值对应的数据点进行随机置换，将置换后的残差序列视为原始序列，然后执行标准的游程检验。这种方法保持了数据点的完整性，避免了使用均值带来的信息损失。

Result: 该方法的结果与包含相同数量数据点且无重复测量的数据集等价，避免了因使用均值而导致的检验敏感性降低，也解决了不同参数值测量点数不同时的加权问题。

Conclusion: 提出的扩展方法有效解决了具有重复测量的回归分析中游程检验的应用问题，保持了检验的敏感性并避免了加权复杂性。

Abstract: The Wald-Wolfowitz runs test can assess the correctness of a regression curve
fitted to a data set with one independent parameter. The assessment is
performed through examination of the residuals, where the signs of the
residuals would appear randomly if the regression curve were correct. We
propose extending the test to the case where multiple data points were measured
for specific independent parameter values. By randomly permutating the data
points corresponding to each independent parameter value and treating their
residuals as occurring in their permutated sequence and then executing the runs
test, results are shown to be equivalent to those of a data set containing the
same number of points with no repeated measurements. This approach avoids the
loss of points, and hence loss of test sensitivity, were the means at each
independent parameter value used. It also avoids the problem of weighting each
mean differently if the number of data points measured at each parameter value
is not identical.

</details>


### [11] [Geometric Model Selection for Latent Space Network Models: Hypothesis Testing via Multidimensional Scaling and Resampling Techniques](https://arxiv.org/abs/2510.06136)
*Jieyun Wang,Anna L. Smith*

Main category: stat.ME

TL;DR: 该论文提出了一种改进的方法来识别网络潜在空间的几何结构，通过扩展置换检验和参数自举方法，结合高斯潜在位置模型，提高了在大型稀疏网络中检测潜在几何结构的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用多维缩放的应力差异来选择潜在几何结构，但这种方法没有考虑网络结构的不确定性，且在大型稀疏网络中效果有限。

Method: 扩展了基于置换的假设检验方法，提出了参数自举网络分布，结合高斯潜在位置模型和Davidson-MacKinnon J检验来比较不同潜在几何结构。

Result: 在模拟的欧几里得和双曲几何潜在空间网络中，置换检验和自举方法在检测潜在几何结构方面都表现出改进效果，特别是在大型稀疏网络中。

Conclusion: 所提出的方法能够更准确地识别网络潜在空间的几何结构，为复杂网络分析提供了更可靠的几何结构选择工具。

Abstract: Latent space models assume that network ties are more likely between nodes
that are closer together in an underlying latent space. Euclidean space is a
popular choice for the underlying geometry, but hyperbolic geometry can mimic
more realistic patterns of ties in complex networks. To identify the underlying
geometry, past research has applied non-Euclidean extensions of
multidimensional scaling (MDS) to the observed geodesic distances: the shortest
path lengths between nodes. The difference in stress, a standard
goodness-of-fit metric for MDS, across the geometries is then used to select a
latent geometry with superior model fit (lower stress). The effectiveness of
this method is assessed through simulations of latent space networks in
Euclidean and hyperbolic geometries. To better account for uncertainty, we
extend permutation-based hypothesis tests for MDS to the latent network
setting. However, these tests do not incorporate any network structure. We
propose a parametric bootstrap distribution of networks, conditioned on
observed geodesic distances and the Gaussian Latent Position Model (GLPM). Our
method extends the Davidson-MacKinnon J-test to latent space network models
with differing latent geometries. We pay particular attention to large and
sparse networks, and both the permutation test and the bootstrapping methods
show an improvement in detecting the underlying geometry.

</details>


### [12] [Tensor time series change-point detection in cryptocurrency network data](https://arxiv.org/abs/2510.06211)
*Andreas Anastasiou,Ivor Cribben*

Main category: stat.ME

TL;DR: 提出了一种名为TenSeg的新变化点检测方法，用于检测张量数据网络结构中的变化，特别适用于加密货币市场操纵检测。


<details>
  <summary>Details</summary>
Motivation: 加密货币诈骗日益增长，市场操纵者跨多个平台交易，使得传统基于单一交易网络的变化检测方法失效，需要检测跨多个交易网络或'网络网络'的变化。

Method: TenSeg方法首先使用张量分解，然后在分解数据的二阶（交叉协方差或网络）结构中检测多个变化点，能够处理频繁且幅度小的变化，计算速度快。

Result: 在模拟数据集和以太坊区块链网络张量数据上的应用表明，该方法显著优于其他最先进的变化点检测技术，检测到的变化点与跨多个交易网络的变化一致。

Conclusion: TenSeg方法能有效检测加密货币市场中的网络结构变化，为防范金融欺诈提供了有力工具，相关R代码已开源。

Abstract: Financial fraud has been growing exponentially in recent years. The rise of
cryptocurrencies as an investment asset has simultaneously seen a parallel
growth in cryptocurrency scams. To detect possible cryptocurrency fraud, and in
particular market manipulation, previous research focused on the detection of
changes in the network of trades; however, market manipulators are now trading
across multiple cryptocurrency platforms, making their detection more
difficult. Hence, it is important to consider the identification of changes
across several trading networks or a `network of networks' over time. To this
end, in this article, we propose a new change-point detection method in the
network structure of tensor-variate data. This new method, labeled TenSeg,
first employs a tensor decomposition, and second detects multiple change-points
in the second-order (cross-covariance or network) structure of the decomposed
data. It allows for change-point detection in the presence of frequent changes
of possibly small magnitudes and is computationally fast. We apply our method
to several simulated datasets and to a cryptocurrency dataset, which consists
of network tensor-variate data from the Ethereum blockchain. We demonstrate
that our approach substantially outperforms other state-of-the-art change-point
techniques, and the detected change-points in the Ethereum data set coincide
with changes across several trading networks or a `network of networks' over
time. Finally, all the relevant \textsf{R} code implementing the method in the
article are available on https://github.com/Anastasiou-Andreas/TenSeg.

</details>


### [13] [A GNAR-Based Framework for Spectral Estimation of Network Time Series: Application to Global Bank Network Connectedness](https://arxiv.org/abs/2510.06157)
*Cristian F. Jiménez-Varón,Marina I. Knight*

Main category: stat.ME

TL;DR: 提出了一个用于广义网络自回归（GNAR）过程的新型谱分析框架，能够同时捕捉时间动态和网络拓扑结构，特别关注全球银行网络连通性分析。


<details>
  <summary>Details</summary>
Motivation: 金融网络中的依赖模式（如全球银行连通性）会随时间变化并在不同频率上演变，需要能够同时捕捉时间动态和网络拓扑的统计工具。现有方法最多仅依赖基于邻接的相互作用，无法建模超出直接邻居的依赖关系。

Method: 开发了GNAR过程的谱分析框架，通过引入r阶段邻域效应来建模超出直接邻居的依赖关系。定义了GNAR谱密度及相关量（如相干性和偏相干性），提出了参数化和网络惩罚非参数化估计器。

Result: 广泛的模拟实验证明了参数化谱估计器的优异性能，理论分析也支持这一结果。在全球银行网络连通性分析中，GNAR谱量能有效捕捉频率特定的跨节点依赖关系，产生与既定度量一致的估计，同时揭示更丰富的波动传导时空模式。

Conclusion: 所提出的GNAR谱分析框架能够有效捕捉金融网络中频率特定的依赖模式，为分析全球银行连通性等复杂网络系统提供了强大的统计工具，能够揭示传统方法无法发现的丰富时空模式。

Abstract: Patterns of dependence in financial networks, such as global bank
connectedness, evolve over time and across frequencies. Analysing these systems
requires statistical tools that jointly capture temporal dynamics and the
underlying network topology. This work develops a novel spectral analysis
framework for Generalized Network Autoregressive (GNAR) processes, modeling
dependencies beyond direct neighbours by incorporating r-stage neighbourhood
effects, unlike existing methods that at best rely solely on adjacency-based
interactions. We define the GNAR spectral density and related quantities, such
as coherence and partial coherence, for which we propose both parametric and
network-penalized nonparametric estimators. Extensive simulations demonstrate
the strong performance of the parametric spectral estimator, as also backed up
by theoretical arguments. The proposed framework has wide applications, and
here we focus on the analysis of global bank network connectedness. The
findings illustrate how the GNAR spectral quantities effectively capture the
frequency-specific cross-nodal dependencies, thus yielding estimates consistent
with established measures, while also uncovering richer temporal and structural
patterns of volatility transmission.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [14] [Minima and Critical Points of the Bethe Free Energy Are Invariant Under Deformation Retractions of Factor Graphs](https://arxiv.org/abs/2510.05380)
*Grégoire Sergeant-Perthuis,Léo Boitel*

Main category: stat.ML

TL;DR: 该论文证明了对于长度最多为1的链的超图和偏序集，改变概率模型的交互偏序集（保持同伦类型不变）会在相关自由能的临界点之间诱导双射。


<details>
  <summary>Details</summary>
Motivation: 在因子图、能量模型等概率模型中，由于底层结构存在循环，无法进行精确推断，需要优化Bethe自由能进行近似变分推断。Bethe自由能的临界点对应Belief Propagation算法的固定点，但一般图、超图和偏序集的临界点完整表征仍是开放问题。

Method: 通过改变概率模型的交互偏序集（保持同伦类型不变），研究其对Bethe自由能临界点的影响。特别关注链长度最多为1的超图和偏序集。

Result: 证明了对于链长度最多为1的超图和偏序集，改变交互偏序集（保持同伦类型不变）会在相关自由能的临界点之间诱导双射关系。

Conclusion: 该结果扩展并统一了经典结果，这些经典结果假设特定形式的可折叠性来证明Bethe自由能临界点的唯一性。

Abstract: In graphical models, factor graphs, and more generally energy-based models,
the interactions between variables are encoded by a graph, a hypergraph, or, in
the most general case, a partially ordered set (poset). Inference on such
probabilistic models cannot be performed exactly due to cycles in the
underlying structures of interaction. Instead, one resorts to approximate
variational inference by optimizing the Bethe free energy. Critical points of
the Bethe free energy correspond to fixed points of the associated Belief
Propagation algorithm. A full characterization of these critical points for
general graphs, hypergraphs, and posets with a finite number of variables is
still an open problem. We show that, for hypergraphs and posets with chains of
length at most 1, changing the poset of interactions of the probabilistic model
to one with the same homotopy type induces a bijection between the critical
points of the associated free energy. This result extends and unifies classical
results that assume specific forms of collapsibility to prove uniqueness of the
critical points of the Bethe free energy.

</details>


### [15] [Refereed Learning](https://arxiv.org/abs/2510.05440)
*Ran Canetti,Ephraim Linder,Connor Wagaman*

Main category: stat.ML

TL;DR: 本文提出了仲裁学习框架，其中学习者可以访问两个竞争性的证明者（只有一个诚实），用于评估不透明模型的属性。该框架在仅使用一次真实标签查询的情况下，能够以接近最优的精度选择两个黑盒模型中更好的一个。


<details>
  <summary>Details</summary>
Motivation: 研究在有两个竞争性证明者（只有一个诚实）的情况下，学习者如何更有效地评估不透明模型的属性，特别是在资源有限的情况下获得比传统方法更高的准确性。

Method: 设计了仲裁学习协议，通过让两个证明者竞争来验证模型性能。关键创新是开发了一种技术，允许学习者使用证明者从未高效采样的分布中进行采样。

Result: 在仅进行一次真实标签查询、通信复杂度为(1+1/ε²)·poly(d)位的情况下，输出模型的损失与最佳模型损失的比值在(1+ε)范围内。相比单证明者方法需要访问几乎所有域点，效率大幅提升。

Conclusion: 仲裁学习框架在模型选择任务中显著优于传统方法，特别是在高精度要求下。提出的采样技术具有独立价值，下界分析证明了协议在多个维度上的最优性。

Abstract: We initiate an investigation of learning tasks in a setting where the learner
is given access to two competing provers, only one of which is honest.
Specifically, we consider the power of such learners in assessing purported
properties of opaque models. Following prior work that considers the power of
competing provers in different settings, we call this setting refereed
learning.
  After formulating a general definition of refereed learning tasks, we show
refereed learning protocols that obtain a level of accuracy that far exceeds
what is obtainable at comparable cost without provers, or even with a single
prover. We concentrate on the task of choosing the better one out of two
black-box models, with respect to some ground truth. While we consider a range
of parameters, perhaps our most notable result is in the high-precision range:
For all $\varepsilon>0$ and ambient dimension $d$, our learner makes only one
query to the ground truth function, communicates only
$(1+\frac{1}{\varepsilon^2})\cdot\text{poly}(d)$ bits with the provers, and
outputs a model whose loss is within a multiplicative factor of
$(1+\varepsilon)$ of the best model's loss. Obtaining comparable loss with a
single prover would require the learner to access the ground truth at almost
all of the points in the domain. To obtain this bound, we develop a technique
that allows the learner to sample, using the provers, from a distribution that
is not efficiently samplable to begin with. We find this technique to be of
independent interest.
  We also present lower bounds that demonstrate the optimality of our protocols
in a number of respects, including prover complexity, number of samples, and
need for query access.

</details>


### [16] [A Probabilistic Basis for Low-Rank Matrix Learning](https://arxiv.org/abs/2510.05447)
*Simon Segert,Nathan Wycoff*

Main category: stat.ML

TL;DR: 本文研究了基于核范数惩罚的低秩矩阵推断的概率分布基础，提出了改进的MCMC算法和自动学习惩罚参数的方法，提高了低秩贝叶斯矩阵去噪和补全的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 尽管基于核范数惩罚的低秩矩阵推断方法广泛使用，但对其背后的概率分布理解不足。本文旨在填补这一理论空白，并改进相关算法。

Method: 通过微分几何方法分析密度函数f(X)∝e^(-λ‖X‖_*)的分布特性，设计改进的MCMC算法，并自动学习惩罚参数λ以避免超参数调优。

Result: 获得了该概率分布的解析可处理性，提出的算法在数值实验中提高了低秩贝叶斯矩阵去噪和补全的准确性和效率。

Conclusion: 本文为基于核范数惩罚的低秩推断提供了理论基础，并开发了无需手动调参的高效算法，在矩阵去噪和补全任务中表现出色。

Abstract: Low rank inference on matrices is widely conducted by optimizing a cost
function augmented with a penalty proportional to the nuclear norm $\Vert \cdot
\Vert_*$. However, despite the assortment of computational methods for such
problems, there is a surprising lack of understanding of the underlying
probability distributions being referred to. In this article, we study the
distribution with density $f(X)\propto e^{-\lambda\Vert X\Vert_*}$, finding
many of its fundamental attributes to be analytically tractable via
differential geometry. We use these facts to design an improved MCMC algorithm
for low rank Bayesian inference as well as to learn the penalty parameter
$\lambda$, obviating the need for hyperparameter tuning when this is difficult
or impossible. Finally, we deploy these to improve the accuracy and efficiency
of low rank Bayesian matrix denoising and completion algorithms in numerical
experiments.

</details>


### [17] [Domain-Shift-Aware Conformal Prediction for Large Language Models](https://arxiv.org/abs/2510.05566)
*Zhexiao Lin,Yuanyuan Li,Neeraj Sarna,Yuanyuan Gao,Michael von Gablenz*

Main category: stat.ML

TL;DR: 提出DS-CP框架，通过重新加权校准样本来适应领域偏移下的语言模型，提供更可靠的置信度覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在过度自信和事实错误输出的风险，标准置信度预测在领域偏移下会失效，导致覆盖不足。

Method: 基于测试提示与校准样本的接近程度，系统性地重新加权校准样本，保持有效性同时增强适应性。

Result: 在MMLU基准测试中，该方法在显著分布偏移下比标准置信度预测提供更可靠的覆盖，同时保持效率。

Conclusion: 这是实现大语言模型在真实世界部署中可信不确定性量化的实用步骤。

Abstract: Large language models have achieved impressive performance across diverse
tasks. However, their tendency to produce overconfident and factually incorrect
outputs, known as hallucinations, poses risks in real world applications.
Conformal prediction provides finite-sample, distribution-free coverage
guarantees, but standard conformal prediction breaks down under domain shift,
often leading to under-coverage and unreliable prediction sets. We propose a
new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our
framework adapts conformal prediction to large language models under domain
shift, by systematically reweighting calibration samples based on their
proximity to the test prompt, thereby preserving validity while enhancing
adaptivity. Our theoretical analysis and experiments on the MMLU benchmark
demonstrate that the proposed method delivers more reliable coverage than
standard conformal prediction, especially under substantial distribution
shifts, while maintaining efficiency. This provides a practical step toward
trustworthy uncertainty quantification for large language models in real-world
deployment.

</details>


### [18] [Bilevel optimization for learning hyperparameters: Application to solving PDEs and inverse problems with Gaussian processes](https://arxiv.org/abs/2510.05568)
*Nicholas H. Nelsen,Houman Owhadi,Andrew M. Stuart,Xianjin Yang,Zongren Zou*

Main category: stat.ML

TL;DR: 提出了一种基于高斯-牛顿线性化的高效双层超参数优化方法，通过线性化内层优化步骤避免重复求解PDE，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 科学计算和推理方法（如核方法和神经网络）的性能严重依赖于超参数选择，而传统的双层优化框架计算成本高昂，特别是在PDE约束问题中。

Method: 采用高斯-牛顿线性化内层优化步骤，提供闭式更新，避免重复PDE求解，外层循环简化为单一线性化PDE求解和显式梯度更新。

Result: 在非线性PDE和PDE逆问题的数值实验中，相比传统随机初始化方法，在准确性和鲁棒性方面有显著提升，对高维超参数优化具有良好可扩展性。

Conclusion: 该方法为PDE约束问题提供了一种计算高效的超参数优化策略，特别适用于高维参数空间，在加性核和神经网络参数化深度核中表现出色。

Abstract: Methods for solving scientific computing and inference problems, such as
kernel- and neural network-based approaches for partial differential equations
(PDEs), inverse problems, and supervised learning tasks, depend crucially on
the choice of hyperparameters. Specifically, the efficacy of such methods, and
in particular their accuracy, stability, and generalization properties,
strongly depends on the choice of hyperparameters. While bilevel optimization
offers a principled framework for hyperparameter tuning, its nested
optimization structure can be computationally demanding, especially in
PDE-constrained contexts. In this paper, we propose an efficient strategy for
hyperparameter optimization within the bilevel framework by employing a
Gauss-Newton linearization of the inner optimization step. Our approach
provides closed-form updates, eliminating the need for repeated costly PDE
solves. As a result, each iteration of the outer loop reduces to a single
linearized PDE solve, followed by explicit gradient-based hyperparameter
updates. We demonstrate the effectiveness of the proposed method through
Gaussian process models applied to nonlinear PDEs and to PDE inverse problems.
Extensive numerical experiments highlight substantial improvements in accuracy
and robustness compared to conventional random hyperparameter initialization.
In particular, experiments with additive kernels and neural
network-parameterized deep kernels demonstrate the method's scalability and
effectiveness for high-dimensional hyperparameter optimization.

</details>


### [19] [On the Theory of Continual Learning with Gradient Descent for Neural Networks](https://arxiv.org/abs/2510.05573)
*Hossein Taheri,Avishek Ghosh,Arya Mazumdar*

Main category: stat.ML

TL;DR: 本文分析了持续学习在可处理设置下的遗忘机制，通过研究单隐藏层二次神经网络在XOR聚类数据集上的表现，揭示了不同参数对遗忘率的影响。


<details>
  <summary>Details</summary>
Motivation: 理解持续学习中的遗忘机制是人工智能的核心目标，本文旨在通过可分析的环境揭示其内在机制。

Method: 使用单隐藏层二次神经网络，在带有高斯噪声的XOR聚类数据集上通过梯度下降训练，其中不同任务对应具有正交均值的不同聚类。

Result: 获得了训练和测试期间遗忘率的上界，涉及迭代次数、样本大小、任务数量和隐藏层大小等参数，揭示了不同参数对遗忘率的影响。

Conclusion: 数值实验验证了理论结果的有效性，表明这些发现在分析设置之外仍然适用，为理解持续学习机制提供了重要见解。

Abstract: Continual learning, the ability of a model to adapt to an ongoing sequence of
tasks without forgetting the earlier ones, is a central goal of artificial
intelligence. To shed light on its underlying mechanisms, we analyze the
limitations of continual learning in a tractable yet representative setting. In
particular, we study one-hidden-layer quadratic neural networks trained by
gradient descent on an XOR cluster dataset with Gaussian noise, where different
tasks correspond to different clusters with orthogonal means. Our results
obtain bounds on the rate of forgetting during train and test-time in terms of
the number of iterations, the sample size, the number of tasks, and the
hidden-layer size. Our results reveal interesting phenomena on the role of
different problem parameters in the rate of forgetting. Numerical experiments
across diverse setups confirm our results, demonstrating their validity beyond
the analyzed settings.

</details>


### [20] [Implicit Updates for Average-Reward Temporal Difference Learning](https://arxiv.org/abs/2510.06149)
*Hwanwoo Kim,Dongkyu Derek Cho,Eric Laber*

Main category: stat.ML

TL;DR: 提出了平均奖励隐式TD(λ)算法，通过隐式固定点更新提供数据自适应稳定性，显著改善了数值稳定性，在更宽松的步长条件下仍能保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 标准平均奖励TD(λ)算法对步长选择高度敏感，需要精细调参来维持数值稳定性，这限制了其实际应用效果。

Method: 采用隐式固定点更新方法，在保持每迭代计算复杂度不变的同时，提供数据自适应稳定性机制。

Result: 与标准算法相比，隐式TD(λ)在更宽松的步长条件下建立了有限时间误差界，在更广泛的步长范围内可靠运行，显著改善了数值稳定性。

Conclusion: 平均奖励隐式TD(λ)是标准平均奖励TD(λ)的一个鲁棒替代方案，能够实现更高效的策略评估和策略学习。

Abstract: Temporal difference (TD) learning is a cornerstone of reinforcement learning.
In the average-reward setting, standard TD($\lambda$) is highly sensitive to
the choice of step-size and thus requires careful tuning to maintain numerical
stability. We introduce average-reward implicit TD($\lambda$), which employs an
implicit fixed point update to provide data-adaptive stabilization while
preserving the per iteration computational complexity of standard
average-reward TD($\lambda$). In contrast to prior finite-time analyses of
average-reward TD($\lambda$), which impose restrictive step-size conditions, we
establish finite-time error bounds for the implicit variant under substantially
weaker step-size requirements. Empirically, average-reward implicit
TD($\lambda$) operates reliably over a much broader range of step-sizes and
exhibits markedly improved numerical stability. This enables more efficient
policy evaluation and policy learning, highlighting its effectiveness as a
robust alternative to average-reward TD($\lambda$).

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [21] [Geographically Weighted Regression for Air Quality Low-Cost Sensor Calibration](https://arxiv.org/abs/2510.05646)
*Jean-Michel Poggi,Bruno Portier,Emma Thulliez*

Main category: stat.AP

TL;DR: 使用地理加权回归(GWR)方法校准低成本空气质量传感器的NO2测量值，基于安特卫普的SensEURCity数据集，包含9个参考站和34个微传感器。


<details>
  <summary>Details</summary>
Motivation: 低成本传感器在城市尺度高分辨率空气质量监测中具有重要价值，但需要通过与参考分析仪进行校准来提高测量准确性。

Method: 采用地理加权回归(GWR)方法，利用安特卫普SensEURCity数据集中的9个参考站和34个微传感器数据进行传感器测量值校准。

Result: 提供了NO2的校准结果，分析了GWR模型的估计效果以及估计系数的空间分布特征。

Conclusion: GWR方法能够有效校准低成本空气质量传感器的测量值，为城市尺度的高分辨率空气质量监测提供了可靠的技术支持。

Abstract: This article focuses on the use of Geographically Weighted Regression (GWR)
method to correct air quality low-cost sensors measurements. Those sensors are
of major interest in the current era of high-resolution air quality monitoring
at urban scale, but require calibration using reference analyzers. The results
for NO2 are provided along with comments on the estimated GWR model and the
spatial content of the estimated coefficients. The study has been carried out
using the publicly available SensEURCity dataset in Antwerp, which is
especially relevant since it includes 9 reference stations and 34 micro-sensors
collocated and deployed within the city.

</details>


### [22] [Copula-Based Clustering of Financial Time Series via Evidence Accumulation](https://arxiv.org/abs/2510.05960)
*Andrea Mecchina,Roberta Pappadà,Nicola Torelli*

Main category: stat.AP

TL;DR: 提出一种基于多重copula相异度量的聚类方法，用于分析资产收益的依赖结构，帮助风险规避投资者构建风险分散的投资组合。


<details>
  <summary>Details</summary>
Motivation: 理解资产收益的依赖结构对风险评估和投资组合分散策略至关重要，特别是在风险情境下识别具有相似随机行为的资产。

Method: 使用经典层次聚类程序和多重copula相异度量，通过累积多重分类证据进行聚类分析。

Result: 实证研究使用EURO STOXX 50指数数据展示了该策略的有效性，能够识别在风险情境下具有相似随机行为的资产群组。

Conclusion: 该方法为风险规避投资者提供了构建风险分散投资组合的有效工具，通过聚类分析识别资产间的依赖结构。

Abstract: Understanding the dependence structure of asset returns is fundamental in
risk assessment and is particularly relevant in a portfolio diversification
strategy. We propose a clustering approach where evidence accumulated in a
multiplicity of classifications is achieved using classical hierarchical
procedures and multiple copula-based dissimilarity measures. Assets that are
grouped in the same cluster are such that their stochastic behavior is similar
during risky scenarios, and riskaverse investors could exploit this information
to build a risk-diversified portfolio. An empirical demonstration of such a
strategy is presented by using data from the EURO STOXX 50 index.

</details>


### [23] [Measuring Data Quality for Project Lighthouse](https://arxiv.org/abs/2510.06121)
*Adam Bloomston,Elizabeth Burke,Megan Cacace,Anne Diaz,Wren Dougherty,Matthew Gonzalez,Remington Gregg,Yeliz Güngör,Bryce Hayes,Eeway Hsu,Oron Israeli,Heesoo Kim,Sara Kwasnick,Joanne Lacsina,Demma Rosa Rodriguez,Adam Schiller,Whitney Schumacher,Jessica Simon,Maggie Tang,Skyler Wharton,Marilyn Wilcken*

Main category: stat.AP

TL;DR: 提出基于机器学习分类的框架，用于实证证明数据质量指标及其最小阈值的选择，并在Project Lighthouse中实现定量数据最小化原则。


<details>
  <summary>Details</summary>
Motivation: 解决在Project Lighthouse中测量数据质量的挑战，特别是在更广泛的学术背景下，并确保在分析潜在体验差距时严格遵循数据最小化原则。

Method: 使用三个核心数据质量指标进行测量，其中两个扩展了先前的学术工作；提出基于机器学习分类的框架来实证证明数据质量指标及其最小阈值的选择。

Result: 开发了一个能够实证证明数据质量指标和阈值选择的框架，使Project Lighthouse能够严格满足数据最小化原则。

Conclusion: 通过提出的定量数据最小化方法，能够在分析潜在体验差距时确保数据质量，同时严格遵守数据最小化原则。

Abstract: In this paper, we first situate the challenges for measuring data quality
under Project Lighthouse in the broader academic context. We then discuss in
detail the three core data quality metrics we use for measurement--two of which
extend prior academic work. Using those data quality metrics as examples, we
propose a framework, based on machine learning classification, for empirically
justifying the choice of data quality metrics and their associated minimum
thresholds. Finally we outline how these methods enable us to rigorously meet
the principle of data minimization when analyzing potential experience gaps
under Project Lighthouse, which we term quantitative data minimization.

</details>


### [24] [Rapid calibration of atrial electrophysiology models using Gaussian process emulators in the ensemble Kalman filter](https://arxiv.org/abs/2510.06191)
*Mariya Mamajiwala,Cesare Corrado,Chris Lanyon,Steven A. Niederer,Richard D. Wilkinson,Richard H. Clayton*

Main category: stat.AP

TL;DR: 提出了一种基于集合卡尔曼滤波的静态非线性逆问题求解方法，用于快速校准心房颤动的数字孪生模型，实现近实时患者特异性参数估计。


<details>
  <summary>Details</summary>
Motivation: 心房颤动标准治疗导管消融具有侵入性和不可逆性，需要快速校准基于物理的患者特异性模型来指导临床决策。

Method: 将模型校准构建为静态逆问题，使用高斯过程仿真器替代昂贵的前向模型，并提出集合卡尔曼滤波的改进版本用于静态非线性逆问题求解。

Result: 方法能够生成接近后验分布最佳高斯近似的参数样本，与MCMC采样结果相当，支持近实时患者特异性校准。

Conclusion: 该方法是实现临床时间尺度内预测AF治疗效果的关键步骤，适用于广泛的科学和工程静态逆问题。

Abstract: Atrial fibrillation (AF) is a common cardiac arrhythmia characterised by
disordered electrical activity in the atria. The standard treatment is catheter
ablation, which is invasive and irreversible. Recent advances in computational
electrophysiology offer the potential for patient-specific models, often
referred to as digital twins, that can be used to guide clinical decisions. To
be of practical value, we must be able to rapidly calibrate physics-based
models using routine clinical measurements. We pose this calibration task as a
static inverse problem, where the goal is to infer tissue-level
electrophysiological parameters from the available observations. To make this
tractable, we replace the expensive forward model with Gaussian process
emulators (GPEs), and propose a novel adaptation of the ensemble Kalman filter
(EnKF) for static non-linear inverse problems. The approach yields parameter
samples that can be interpreted as coming from the best Gaussian approximation
of the posterior distribution. We compare our results with those obtained using
Markov chain Monte Carlo (MCMC) sampling and demonstrate the potential of the
approach to enable near-real-time patient-specific calibration, a key step
towards predicting outcomes of AF treatment within clinical timescales. The
approach is readily applicable to a wide range of static inverse problems in
science and engineering.

</details>


### [25] [Geographical inequalities in mortality by age and gender in Italy, 2002-2019: insights from a spatial extension of the Lee-Carter model](https://arxiv.org/abs/2510.06210)
*Francesca Fiori,Andrea Riebler,Sara Martino*

Main category: stat.AP

TL;DR: 本研究使用扩展的Lee Carter模型分析意大利107个省份2002-2019年的死亡率数据，发现尽管全国死亡率总体改善，但存在显著的地理不平等，且这种差距自2010年以来在扩大。


<details>
  <summary>Details</summary>
Motivation: 尽管意大利是发达国家中死亡率较低的国家，但近期证据表明死亡率改善速度放缓，地区不平等加剧，需要更精细的分析来揭示这些模式。

Method: 扩展Lee Carter模型，加入空间变化的年龄特定效应和空间-年龄-时间交互作用，使用贝叶斯框架和inlabru包进行估计，通过平滑先验减少小区域死亡计数的随机波动。

Result: 发现死亡率劣势集中在意大利中南部和西北部部分地区，而中北部和东北部表现较好。地理差异自2010年以来扩大，男性在较年轻成年期、女性在较年长成年期表现出更明显的模式差异。

Conclusion: 这种精细分析方法揭示了意大利省份间的不平等死亡率地理格局，未来可进一步分析死因或社会经济状况，为更有针对性的公共卫生政策提供依据。

Abstract: Italy reports some of the lowest levels of mortality in the developed world.
Recent evidence, however, suggests that even in low mortality countries
improvements may be slowing and regional inequalities widening. This study
contributes new empirical evidence to the debate by analysing mortality data by
single year of age for males and females across 107 provinces in Italy from
2002 to 2019. We extend the widely used Lee Carter model to include spatially
varying age specific effects, and further specify it to capture space age time
interactions. The model is estimated in a Bayesian framework using the inlabru
package, which builds on INLA (Integrated Nested Laplace Approximation) for non
linear models and facilitates the use of smoothing priors. This approach
borrows strength across provinces and years, mitigating random fluctuations in
small area death counts. Results demonstrate the value of such a granular
approach, highlighting the existence of an uneven geography of mortality
despite overall national improvements. Mortality disadvantage is concentrated
in parts of the Centre South and North West, while the Centre North and North
East fare relatively better. These geographical differences have widened since
2010, with clear age and gender specific patterns, being more pronounced at
younger adult ages for men and at older adult ages for women. Future work may
involve refining the analysis to mortality by cause of death or socioeconomic
status, informing more targeted public health policies to address mortality
disparities across Italy's provinces.

</details>
