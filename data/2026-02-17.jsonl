{"id": "2602.12435", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.12435", "abs": "https://arxiv.org/abs/2602.12435", "authors": ["Samantha Shi-Jun", "Bo Li"], "title": "Scalable Changepoint Detection for Large Spatiotemporal Data on the Sphere", "comment": null, "summary": "We propose a novel Bayesian framework for changepoint detection in large-scale spherical spatiotemporal data, with broad applicability in environmental and climate sciences. Our approach models changepoints as spatially dependent categorical variables using a multinomial probit model (MPM) with a latent Gaussian process, effectively capturing complex spatial correlation structures on the sphere. To handle the high dimensionality inherent in global datasets, we leverage stochastic partial differential equations (SPDE) and spherical harmonic transformations for efficient representation and scalable inference, drastically reducing computational burden while maintaining high accuracy. Through extensive simulation studies, we demonstrate the efficiency and robustness of the proposed method for changepoint estimation, as well as the significant computational gains achieved through the combined use of the MPM and truncated spectral representations of latent processes. Finally, we apply our method to global aerosol optical depth data, successfully identifying changepoints associated with a major atmospheric event."}
{"id": "2602.12435", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.12435", "abs": "https://arxiv.org/abs/2602.12435", "authors": ["Samantha Shi-Jun", "Bo Li"], "title": "Scalable Changepoint Detection for Large Spatiotemporal Data on the Sphere", "comment": null, "summary": "We propose a novel Bayesian framework for changepoint detection in large-scale spherical spatiotemporal data, with broad applicability in environmental and climate sciences. Our approach models changepoints as spatially dependent categorical variables using a multinomial probit model (MPM) with a latent Gaussian process, effectively capturing complex spatial correlation structures on the sphere. To handle the high dimensionality inherent in global datasets, we leverage stochastic partial differential equations (SPDE) and spherical harmonic transformations for efficient representation and scalable inference, drastically reducing computational burden while maintaining high accuracy. Through extensive simulation studies, we demonstrate the efficiency and robustness of the proposed method for changepoint estimation, as well as the significant computational gains achieved through the combined use of the MPM and truncated spectral representations of latent processes. Finally, we apply our method to global aerosol optical depth data, successfully identifying changepoints associated with a major atmospheric event."}
{"id": "2602.12534", "categories": ["stat.ML", "cs.DS", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12534", "abs": "https://arxiv.org/abs/2602.12534", "authors": ["Alexandros Kouridakis", "Anay Mehrotra", "Alkis Kalavasis", "Constantine Caramanis"], "title": "Linear Regression with Unknown Truncation Beyond Gaussian Features", "comment": null, "summary": "In truncated linear regression, samples $(x,y)$ are shown only when the outcome $y$ falls inside a certain survival set $S^\\star$ and the goal is to estimate the unknown $d$-dimensional regressor $w^\\star$. This problem has a long history of study in Statistics and Machine Learning going back to the works of (Galton, 1897; Tobin, 1958) and more recently in, e.g., (Daskalakis et al., 2019; 2021; Lee et al., 2023; 2024). Despite this long history, however, most prior works are limited to the special case where $S^\\star$ is precisely known. The more practically relevant case, where $S^\\star$ is unknown and must be learned from data, remains open: indeed, here the only available algorithms require strong assumptions on the distribution of the feature vectors (e.g., Gaussianity) and, even then, have a $d^{\\mathrm{poly} (1/\\varepsilon)}$ run time for achieving $\\varepsilon$ accuracy.\n  In this work, we give the first algorithm for truncated linear regression with unknown survival set that runs in $\\mathrm{poly} (d/\\varepsilon)$ time, by only requiring that the feature vectors are sub-Gaussian. Our algorithm relies on a novel subroutine for efficiently learning unions of a bounded number of intervals using access to positive examples (without any negative examples) under a certain smoothness condition. This learning guarantee adds to the line of works on positive-only PAC learning and may be of independent interest."}
{"id": "2602.12534", "categories": ["stat.ML", "cs.DS", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12534", "abs": "https://arxiv.org/abs/2602.12534", "authors": ["Alexandros Kouridakis", "Anay Mehrotra", "Alkis Kalavasis", "Constantine Caramanis"], "title": "Linear Regression with Unknown Truncation Beyond Gaussian Features", "comment": null, "summary": "In truncated linear regression, samples $(x,y)$ are shown only when the outcome $y$ falls inside a certain survival set $S^\\star$ and the goal is to estimate the unknown $d$-dimensional regressor $w^\\star$. This problem has a long history of study in Statistics and Machine Learning going back to the works of (Galton, 1897; Tobin, 1958) and more recently in, e.g., (Daskalakis et al., 2019; 2021; Lee et al., 2023; 2024). Despite this long history, however, most prior works are limited to the special case where $S^\\star$ is precisely known. The more practically relevant case, where $S^\\star$ is unknown and must be learned from data, remains open: indeed, here the only available algorithms require strong assumptions on the distribution of the feature vectors (e.g., Gaussianity) and, even then, have a $d^{\\mathrm{poly} (1/\\varepsilon)}$ run time for achieving $\\varepsilon$ accuracy.\n  In this work, we give the first algorithm for truncated linear regression with unknown survival set that runs in $\\mathrm{poly} (d/\\varepsilon)$ time, by only requiring that the feature vectors are sub-Gaussian. Our algorithm relies on a novel subroutine for efficiently learning unions of a bounded number of intervals using access to positive examples (without any negative examples) under a certain smoothness condition. This learning guarantee adds to the line of works on positive-only PAC learning and may be of independent interest."}
{"id": "2602.12291", "categories": ["stat.AP", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.12291", "abs": "https://arxiv.org/abs/2602.12291", "authors": ["Huan Ning", "Zhenlong Li", "Manzhu Yu", "Xiao Huang", "Shiyan Zhang", "Shan Qiao"], "title": "Nationwide Hourly Population Estimating at the Neighborhood Scale in the United States Using Stable-Attendance Anchor Calibration", "comment": null, "summary": "Traditional population datasets are largely static and therefore unable to capture the strong temporal dynamics of human presence driven by daily mobility. Recent smartphone-based mobility data offer unprecedented spatiotemporal coverage, yet translating these opportunistic observations into accurate population estimates remains challenging due to incomplete sensing, spatially heterogeneous device penetration, and unstable observation processes. We propose a Stable-Attendance Anchor Calibration (SAAC) framework to reconstruct hourly population presence at the Census block group level across the United States. SAAC formulates population estimation as a balance-based population accounting problem, combining residential population with time-varying inbound and outbound mobility inferred from device-event observations. To address observation bias and identifiability limitations, the framework leverages locations with highly regular attendance as calibration anchors, using high schools in this study. These anchors enable estimation of observation scaling factors that correct for under-recorded mobility events. By integrating anchor-based calibration with an explicit sampling model, SAAC enables consistent conversion from observed device events to population presence at fine temporal resolution. The inferred population patterns are consistent with established empirical findings in prior mobility and urban population studies. SAAC provides a generalizable framework for transforming large-scale, biased digital trace data into interpretable dynamic population products, with implications for urban science, public health, and human mobility research. The hourly population estimates can be accessed at: https://gladcolor.github.io/hourly_population."}
{"id": "2602.12577", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.12577", "abs": "https://arxiv.org/abs/2602.12577", "authors": ["Weiben Zhang", "Ruben Loaiza-Maya", "Michael Stanley Smith", "Worapree Maneesoonthorn"], "title": "Conjugate Variational Inference for Large Mixed Multinomial Logit Models and Consumer Choice", "comment": null, "summary": "Heterogeneity in multinomial choice data is often accounted for using logit models with random coefficients. Such models are called \"mixed\", but they can be difficult to estimate for large datasets. We review current Bayesian variational inference (VI) methods that can do so, and propose a new VI method that scales more effectively. The key innovation is a step that updates efficiently a Gaussian approximation to the conditional posterior of the random coefficients, addressing a bottleneck within the variational optimization. The approach is used to estimate three types of mixed logit models: standard, nested and bundle variants. We first demonstrate the improvement of our new approach over existing VI methods using simulations. Our method is then applied to a large scanner panel dataset of pasta choice. We find consumer response to price and promotion variables exhibits substantial heterogeneity at the grocery store and product levels. Store size, premium and geography are found to be drivers of store level estimates of price elasticities. Extension to bundle choice with pasta sauce improves model accuracy further. Predictions from the mixed models are more accurate than those from fixed coefficients equivalents, and our VI method provides insights in circumstances which other methods find challenging."}
{"id": "2602.12842", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12842", "abs": "https://arxiv.org/abs/2602.12842", "authors": ["Brajesh Kumar Dhakad", "Jayant Jha", "Debepsita Mukherjee"], "title": "Some bivariate distributions on a discrete torus with application to wind direction datasets", "comment": null, "summary": "Many datasets are observed on a finite set of equally spaced directions instead of the exact angles, such as the wind direction data. However, in the statistical literature, bivariate models are only available for continuous circular random variables. This article presents two bivariate circular distributions, namely bivariate wrapped geometric (BWG) and bivariate generalized wrapped geometric (BGWG), for analyzing bivariate discrete circular data. We consider wrapped geometric distributions and a trigonometric function to construct the models. The models are analytically tractable due to the exact closed-form expressions for the trigonometric moments. We thoroughly discuss the distributional properties of the models, including the interpretation of parameters and dependence structure. The estimation methodology based on maximizing the likelihood functions is illustrated for simulated datasets. Finally, the proposed distributions are utilized to analyze pairwise wind direction measurements obtained at different stations in India, and the interpretations for the fitted models are briefly discussed."}
{"id": "2602.12680", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12680", "abs": "https://arxiv.org/abs/2602.12680", "authors": ["Qingyi Hu", "Liam Hodgkinson"], "title": "A Regularization-Sharpness Tradeoff for Linear Interpolators", "comment": "29 pages, 4 figures", "summary": "The rule of thumb regarding the relationship between the bias-variance tradeoff and model size plays a key role in classical machine learning, but is now well-known to break down in the overparameterized setting as per the double descent curve. In particular, minimum-norm interpolating estimators can perform well, suggesting the need for new tradeoff in these settings. Accordingly, we propose a regularization-sharpness tradeoff for overparameterized linear regression with an $\\ell^p$ penalty. Inspired by the interpolating information criterion, our framework decomposes the selection penalty into a regularization term (quantifying the alignment of the regularizer and the interpolator) and a geometric sharpness term on the interpolating manifold (quantifying the effect of local perturbations), yielding a tradeoff analogous to bias-variance. Building on prior analyses that established this information criterion for ridge regularizers, this work first provides a general expression of the interpolating information criterion for $\\ell^p$ regularizers where $p \\ge 2$. Subsequently, we extend this to the LASSO interpolator with $\\ell^1$ regularizer, which induces stronger sparsity. Empirical results on real-world datasets with random Fourier features and polynomials validate our theory, demonstrating how the tradeoff terms can distinguish performant linear interpolators from weaker ones."}
{"id": "2602.12974", "categories": ["stat.AP", "cs.CV", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12974", "abs": "https://arxiv.org/abs/2602.12974", "authors": ["Jian Kang", "Thomas Nichols", "Lexin Li", "Martin A. Lindquist", "Hongtu Zhu"], "title": "Statistical Opportunities in Neuroimaging", "comment": "33 pages, 3 figures", "summary": "Neuroimaging has profoundly enhanced our understanding of the human brain by characterizing its structure, function, and connectivity through modalities like MRI, fMRI, EEG, and PET. These technologies have enabled major breakthroughs across the lifespan, from early brain development to neurodegenerative and neuropsychiatric disorders. Despite these advances, the brain is a complex, multiscale system, and neuroimaging measurements are correspondingly high-dimensional. This creates major statistical challenges, including measurement noise, motion-related artifacts, substantial inter-subject and site/scanner variability, and the sheer scale of modern studies. This paper explores statistical opportunities and challenges in neuroimaging across four key areas: (i) brain development from birth to age 20, (ii) the adult and aging brain, (iii) neurodegeneration and neuropsychiatric disorders, and (iv) brain encoding and decoding. After a quick tutorial on major imaging technologies, we review cutting-edge studies, underscore data and modeling challenges, and highlight research opportunities for statisticians. We conclude by emphasizing that close collaboration among statisticians, neuroscientists, and clinicians is essential for translating neuroimaging advances into improved diagnostics, deeper mechanistic insight, and more personalized treatments."}
{"id": "2602.12682", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12682", "abs": "https://arxiv.org/abs/2602.12682", "authors": ["Taekwon Hong", "Woojung Bae", "Sang Kyu Lee", "Dongrak Choi", "Jong-Hyeon Jeong"], "title": "A Causal Framework for Quantile Residual Lifetime", "comment": null, "summary": "Estimating prognosis conditional on surviving an initial high-risk period is crucial in clinical research. Yet, standard metrics such as hazard ratios are often difficult to interpret, while mean-based summaries are sensitive to outliers and censoring. We propose a formal causal framework for estimating quantiles of residual lifetime among individuals surviving to a landmark time $t_0$. Our primary estimand, the \"Observed Survivor Quantile Contrast\" (OSQC), targets pragmatic prognostic differences within the observed survivor population. To estimate the OSQC, we develop a doubly robust estimator that combines propensity scores, outcome regression, and inverse probability of censoring weights, ensuring consistency under confounding and informative censoring provided that the censoring model is correctly specified and at least one additional nuisance model is correctly specified. Recognizing that the OSQC conflates causal efficacy and compositional selection, we also introduce a reweighting-based supplementary estimator for the \"Principal Survivor Quantile Contrast\" (PSQC) to disentangle these mechanisms under stronger assumptions. Extensive simulations demonstrate the robustness of the proposed estimators and clarify the role of post-treatment selection. We illustrate the framework using data from the SUPPORT study to assess the impact of right heart catheterization on residual lifetime among intensive care unit survivors, and from the NSABP B-14 trial to examine post-surgical prognosis under adjuvant tamoxifen therapy across multiple landmark times."}
{"id": "2602.12900", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12900", "abs": "https://arxiv.org/abs/2602.12900", "authors": ["Ganesh Vishnu Avhad", "Sudheesh K. Kattumannil"], "title": "A unified testing approach for log-symmetry using Fourier methods", "comment": null, "summary": "Continuous and strictly positive data that exhibit skewness and outliers frequently arise in many applied disciplines. Log-symmetric distributions provide a flexible framework for modeling such data. In this article, we develop new goodness-of-fit tests for log-symmetric distributions based on a recent characterization. These tests utilize the characteristic function as a novel tool and are constructed using an $L^2$-type weighted distance measure. The asymptotic properties of the resulting test statistic are studied. The finite-sample performance of the proposed method is assessed via Monte Carlo simulations and compared with existing procedures. The results under a range of alternative distributions indicate superior empirical power, while the proposed test also exhibits substantial computational efficiency compared to existing methods. The methodology is further illustrated using real data sets to demonstrate practical applicability."}
{"id": "2602.12901", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12901", "abs": "https://arxiv.org/abs/2602.12901", "authors": ["Heesang Ann", "Min-hwan Oh"], "title": "Blessings of Multiple Good Arms in Multi-Objective Linear Bandits", "comment": "58 pages", "summary": "The multi objective bandit setting has traditionally been regarded as more complex than the single objective case, as multiple objectives must be optimized simultaneously. In contrast to this prevailing view, we demonstrate that when multiple good arms exist for multiple objectives, they can induce a surprising benefit, implicit exploration. Under this condition, we show that simple algorithms that greedily select actions in most rounds can nonetheless achieve strong performance, both theoretically and empirically. To our knowledge, this is the first study to introduce implicit exploration in both multi objective and parametric bandit settings without any distributional assumptions on the contexts. We further introduce a framework for effective Pareto fairness, which provides a principled approach to rigorously analyzing fairness of multi objective bandit algorithms."}
{"id": "2602.12577", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.12577", "abs": "https://arxiv.org/abs/2602.12577", "authors": ["Weiben Zhang", "Ruben Loaiza-Maya", "Michael Stanley Smith", "Worapree Maneesoonthorn"], "title": "Conjugate Variational Inference for Large Mixed Multinomial Logit Models and Consumer Choice", "comment": null, "summary": "Heterogeneity in multinomial choice data is often accounted for using logit models with random coefficients. Such models are called \"mixed\", but they can be difficult to estimate for large datasets. We review current Bayesian variational inference (VI) methods that can do so, and propose a new VI method that scales more effectively. The key innovation is a step that updates efficiently a Gaussian approximation to the conditional posterior of the random coefficients, addressing a bottleneck within the variational optimization. The approach is used to estimate three types of mixed logit models: standard, nested and bundle variants. We first demonstrate the improvement of our new approach over existing VI methods using simulations. Our method is then applied to a large scanner panel dataset of pasta choice. We find consumer response to price and promotion variables exhibits substantial heterogeneity at the grocery store and product levels. Store size, premium and geography are found to be drivers of store level estimates of price elasticities. Extension to bundle choice with pasta sauce improves model accuracy further. Predictions from the mixed models are more accurate than those from fixed coefficients equivalents, and our VI method provides insights in circumstances which other methods find challenging."}
{"id": "2602.12702", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12702", "abs": "https://arxiv.org/abs/2602.12702", "authors": ["Anna Nalpantidi", "Dimitris Karlis"], "title": "Modelling multivariate ordinal time series using pairwise likelihood", "comment": null, "summary": "We assume that we have multiple ordinal time series and we would like to specify their joint distribution. In general it is difficult to create multivariate distribution that can be easily used to jointly model ordinal variables and the problem becomes even more complex in the case of time series, since we have to take into consideration not only the autocorrelation of each time series and the dependence between time series, but also cross-correlation. Starting from the simplest case of two ordinal time series, we propose using copulas to specify their joint distribution. We extend our approach in higher dimensions, by approximating full likelihood with composite likelihood and especially conditional pairwise likelihood, where each bivariate model is specified by copulas. We suggest maximizing each bivariate model independently to avoid computational issues and synthesize individual estimates using weighted mean. Weights are related to the Hessian matrix of each bivariate model. Simulation studies showed that model fits well under different sample sizes. Forecasting approach is also discussed. A small real data application about unemployment state of different countries of European Union is presented to illustrate our approach."}
{"id": "2602.12992", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12992", "abs": "https://arxiv.org/abs/2602.12992", "authors": ["Reagan Mozer", "Nicole E. Pashley", "Luke Miratrix"], "title": "Stratified Sampling for Model-Assisted Estimation with Surrogate Outcomes", "comment": null, "summary": "In many randomized trials, outcomes such as essays or open-ended responses must be manually scored as a preliminary step to impact analysis, a process that is costly and limiting. Model-assisted estimation offers a way to combine surrogate outcomes generated by machine learning or large language models with a human-coded subset, yet typical implementations use simple random sampling and therefore overlook systematic variation in surrogate prediction error. We extend this framework by incorporating stratified sampling to more efficiently allocate human coding effort. We derive the exact variance of the stratified model-assisted estimator, characterize conditions under which stratification improves precision, and identify a Neyman-type optimal allocation rule that oversamples strata with larger residual variance. We evaluate our methods through a comprehensive simulation study to assess finite-sample performance. Overall, we find stratification consistently improves efficiency when surrogate prediction errors exhibit structured bias or heteroskedasticity. We also present two empirical applications, one using data from an education RCT and one using a large observational corpus, to illustrate how these methods can be implemented in practice using ChatGPT-generated surrogate outcomes. Overall, this framework provides a practical design-based approach for leveraging surrogate outcomes and strategically allocating human coding effort to obtain unbiased estimates with greater efficiency. While motivated by text-as-data applications, the methodology applies broadly to any setting where outcome measurement is costly or prohibitive, and can be applied to comparisons across groups or estimating the mean of a single group."}
{"id": "2602.12923", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12923", "abs": "https://arxiv.org/abs/2602.12923", "authors": ["Luigi Fogliani", "Bruno Loureiro", "Marylou Gabrié"], "title": "Annealing in variational inference mitigates mode collapse: A theoretical study on Gaussian mixtures", "comment": null, "summary": "Mode collapse, the failure to capture one or more modes when targetting a multimodal distribution, is a central challenge in modern variational inference. In this work, we provide a mathematical analysis of annealing based strategies for mitigating mode collapse in a tractable setting: learning a Gaussian mixture, where mode collapse is known to arise. Leveraging a low dimensional summary statistics description, we precisely characterize the interplay between the initial temperature and the annealing rate, and derive a sharp formula for the probability of mode collapse. Our analysis shows that an appropriately chosen annealing scheme can robustly prevent mode collapse. Finally, we present numerical evidence that these theoretical tradeoffs qualitatively extend to neural network based models, RealNVP normalizing flows, providing guidance for designing annealing strategies mitigating mode collapse in practical variational inference pipelines."}
{"id": "2602.12842", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12842", "abs": "https://arxiv.org/abs/2602.12842", "authors": ["Brajesh Kumar Dhakad", "Jayant Jha", "Debepsita Mukherjee"], "title": "Some bivariate distributions on a discrete torus with application to wind direction datasets", "comment": null, "summary": "Many datasets are observed on a finite set of equally spaced directions instead of the exact angles, such as the wind direction data. However, in the statistical literature, bivariate models are only available for continuous circular random variables. This article presents two bivariate circular distributions, namely bivariate wrapped geometric (BWG) and bivariate generalized wrapped geometric (BGWG), for analyzing bivariate discrete circular data. We consider wrapped geometric distributions and a trigonometric function to construct the models. The models are analytically tractable due to the exact closed-form expressions for the trigonometric moments. We thoroughly discuss the distributional properties of the models, including the interpretation of parameters and dependence structure. The estimation methodology based on maximizing the likelihood functions is illustrated for simulated datasets. Finally, the proposed distributions are utilized to analyze pairwise wind direction measurements obtained at different stations in India, and the interpretations for the fitted models are briefly discussed."}
{"id": "2602.13104", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.13104", "abs": "https://arxiv.org/abs/2602.13104", "authors": ["Nathaniel S. O'Connell"], "title": "Random Forests as Statistical Procedures: Design, Variance, and Dependence", "comment": "26 pages, 2 figures. Supplementary material included", "summary": "Random forests are widely used prediction procedures, yet are typically described algorithmically rather than as statistical designs acting on a fixed dataset. We develop a finite-sample, design-based formulation of random forests in which each tree is an explicit randomized conditional regression function. This perspective yields an exact variance identity for the forest predictor that separates finite-aggregation variability from a structural dependence term that persists even under infinite aggregation. We further decompose both single-tree dispersion and inter-tree covariance using the laws of total variance and covariance, isolating two fundamental design mechanisms-reuse of training observations and alignment of data-adaptive partitions. These mechanisms induce a strict covariance floor, demonstrating that predictive variability cannot be eliminated by increasing the number of trees alone. The resulting framework clarifies how resampling, feature-level randomization, and split selection govern resolution, tree variability, and dependence, and establishes random forests as explicit finite-sample statistical designs whose behavior is determined by their underlying randomized construction."}
{"id": "2602.12932", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12932", "abs": "https://arxiv.org/abs/2602.12932", "authors": ["Qianqian Qu", "Jun S. Liu"], "title": "TFTF: Training-Free Targeted Flow for Conditional Sampling", "comment": null, "summary": "We propose a training-free conditional sampling method for flow matching models based on importance sampling. Because a naïve application of importance sampling suffers from weight degeneracy in high-dimensional settings, we modify and incorporate a resampling technique in sequential Monte Carlo (SMC) during intermediate stages of the generation process. To encourage generated samples to diverge along distinct trajectories, we derive a stochastic flow with adjustable noise strength to replace the deterministic flow at the intermediate stage. Our framework requires no additional training, while providing theoretical guarantees of asymptotic accuracy. Experimentally, our method significantly outperforms existing approaches on conditional sampling tasks for MNIST and CIFAR-10. We further demonstrate the applicability of our approach in higher-dimensional, multimodal settings through text-to-image generation experiments on CelebA-HQ."}
{"id": "2602.12845", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12845", "abs": "https://arxiv.org/abs/2602.12845", "authors": ["Donatas Šlevinskas", "Ieva Burakauskaitė", "Andrius Čiginas"], "title": "Small area estimation using incomplete auxiliary information", "comment": "21 pages, 3 figures", "summary": "Auxiliary information is increasingly available from administrative and other data sources, but it is often incomplete and of non-probability origin. We propose a two-step small area estimation approach in which the first step relies on design-based model calibration and exploits a large non-probability source providing a noisy proxy of the study variable for only part of the population. A unit-level measurement-error working model is fitted on the linked overlap between the probability survey and the external source, and its predictions are incorporated through domain-specific model-calibration constraints to obtain approximately design-unbiased domain totals. These totals and their variance estimates are then used in a Fay-Herriot area-level model with exactly known covariates to produce empirical best linear unbiased predictors. The approach is demonstrated in three enterprise survey settings from official statistics by integrating probability sample data with (i) administrative records, (ii) a cut-off data source, and (iii) web-scraped online information. Empirical comparisons show consistent improvements in domain-level precision over direct estimation and over a Fay-Herriot benchmark that directly incorporates the proxy information as an error-prone covariate. These gains are achieved without modeling the selection mechanism of the non-probability sample."}
{"id": "2602.13152", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.13152", "abs": "https://arxiv.org/abs/2602.13152", "authors": ["Rupsa Basu", "Sven Otto"], "title": "Detecting Parameter Instabilities in Functional Concurrent Linear Regression", "comment": null, "summary": "We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under Hölder regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series."}
{"id": "2602.13104", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.13104", "abs": "https://arxiv.org/abs/2602.13104", "authors": ["Nathaniel S. O'Connell"], "title": "Random Forests as Statistical Procedures: Design, Variance, and Dependence", "comment": "26 pages, 2 figures. Supplementary material included", "summary": "Random forests are widely used prediction procedures, yet are typically described algorithmically rather than as statistical designs acting on a fixed dataset. We develop a finite-sample, design-based formulation of random forests in which each tree is an explicit randomized conditional regression function. This perspective yields an exact variance identity for the forest predictor that separates finite-aggregation variability from a structural dependence term that persists even under infinite aggregation. We further decompose both single-tree dispersion and inter-tree covariance using the laws of total variance and covariance, isolating two fundamental design mechanisms-reuse of training observations and alignment of data-adaptive partitions. These mechanisms induce a strict covariance floor, demonstrating that predictive variability cannot be eliminated by increasing the number of trees alone. The resulting framework clarifies how resampling, feature-level randomization, and split selection govern resolution, tree variability, and dependence, and establishes random forests as explicit finite-sample statistical designs whose behavior is determined by their underlying randomized construction."}
{"id": "2602.12900", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12900", "abs": "https://arxiv.org/abs/2602.12900", "authors": ["Ganesh Vishnu Avhad", "Sudheesh K. Kattumannil"], "title": "A unified testing approach for log-symmetry using Fourier methods", "comment": null, "summary": "Continuous and strictly positive data that exhibit skewness and outliers frequently arise in many applied disciplines. Log-symmetric distributions provide a flexible framework for modeling such data. In this article, we develop new goodness-of-fit tests for log-symmetric distributions based on a recent characterization. These tests utilize the characteristic function as a novel tool and are constructed using an $L^2$-type weighted distance measure. The asymptotic properties of the resulting test statistic are studied. The finite-sample performance of the proposed method is assessed via Monte Carlo simulations and compared with existing procedures. The results under a range of alternative distributions indicate superior empirical power, while the proposed test also exhibits substantial computational efficiency compared to existing methods. The methodology is further illustrated using real data sets to demonstrate practical applicability."}
{"id": "2602.13112", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.13112", "abs": "https://arxiv.org/abs/2602.13112", "authors": ["Matia Bojovic", "Saverio Salzo", "Massimiliano Pontil"], "title": "AdaGrad-Diff: A New Version of the Adaptive Gradient Algorithm", "comment": "24 pages", "summary": "Vanilla gradient methods are often highly sensitive to the choice of stepsize, which typically requires manual tuning. Adaptive methods alleviate this issue and have therefore become widely used. Among them, AdaGrad has been particularly influential. In this paper, we propose an AdaGrad-style adaptive method in which the adaptation is driven by the cumulative squared norms of successive gradient differences rather than gradient norms themselves. The key idea is that when gradients vary little across iterations, the stepsize is not unnecessarily reduced, while significant gradient fluctuations, reflecting curvature or instability, lead to automatic stepsize damping. Numerical experiments demonstrate that the proposed method is more robust than AdaGrad in several practically relevant settings."}
{"id": "2602.12992", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.12992", "abs": "https://arxiv.org/abs/2602.12992", "authors": ["Reagan Mozer", "Nicole E. Pashley", "Luke Miratrix"], "title": "Stratified Sampling for Model-Assisted Estimation with Surrogate Outcomes", "comment": null, "summary": "In many randomized trials, outcomes such as essays or open-ended responses must be manually scored as a preliminary step to impact analysis, a process that is costly and limiting. Model-assisted estimation offers a way to combine surrogate outcomes generated by machine learning or large language models with a human-coded subset, yet typical implementations use simple random sampling and therefore overlook systematic variation in surrogate prediction error. We extend this framework by incorporating stratified sampling to more efficiently allocate human coding effort. We derive the exact variance of the stratified model-assisted estimator, characterize conditions under which stratification improves precision, and identify a Neyman-type optimal allocation rule that oversamples strata with larger residual variance. We evaluate our methods through a comprehensive simulation study to assess finite-sample performance. Overall, we find stratification consistently improves efficiency when surrogate prediction errors exhibit structured bias or heteroskedasticity. We also present two empirical applications, one using data from an education RCT and one using a large observational corpus, to illustrate how these methods can be implemented in practice using ChatGPT-generated surrogate outcomes. Overall, this framework provides a practical design-based approach for leveraging surrogate outcomes and strategically allocating human coding effort to obtain unbiased estimates with greater efficiency. While motivated by text-as-data applications, the methodology applies broadly to any setting where outcome measurement is costly or prohibitive, and can be applied to comparisons across groups or estimating the mean of a single group."}
{"id": "2602.13098", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13098", "abs": "https://arxiv.org/abs/2602.13098", "authors": ["Rahul Manavalan", "Filip Tronarp"], "title": "Barron-Wiener-Laguerre models", "comment": null, "summary": "We propose a probabilistic extension of Wiener-Laguerre models for causal operator learning. Classical Wiener-Laguerre models parameterize stable linear dynamics using orthonormal Laguerre bases and apply a static nonlinear map to the resulting features. While structurally efficient and interpretable, they provide only deterministic point estimates. We reinterpret the nonlinear component through the lens of Barron function approximation, viewing two-layer networks, random Fourier features, and extreme learning machines as discretizations of integral representations over parameter measures. This perspective naturally admits Bayesian inference on the nonlinear map and yields posterior predictive uncertainty. By combining Laguerre-parameterized causal dynamics with probabilistic Barron-type nonlinear approximators, we obtain a structured yet expressive class of causal operators equipped with uncertainty quantification. The resulting framework bridges classical system identification and modern measure-based function approximation, providing a principled approach to time-series modeling and nonlinear systems identification."}
{"id": "2602.13152", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.13152", "abs": "https://arxiv.org/abs/2602.13152", "authors": ["Rupsa Basu", "Sven Otto"], "title": "Detecting Parameter Instabilities in Functional Concurrent Linear Regression", "comment": null, "summary": "We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under Hölder regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series."}
{"id": "2602.13158", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.13158", "abs": "https://arxiv.org/abs/2602.13158", "authors": ["Ryan Li", "Brian J. Reich", "Emily C. Hector", "Reetam Majumder"], "title": "A new mixture model for spatiotemporal exceedances with flexible tail dependence", "comment": null, "summary": "We propose a new model and estimation framework for spatiotemporal streamflow exceedances above a threshold that flexibly captures asymptotic dependence and independence in the tail of the distribution. We model streamflow using a mixture of processes with spatial, temporal and spatiotemporal asymptotic dependence regimes. A censoring mechanism allows us to use only observations above a threshold to estimate marginal and joint probabilities of extreme events. As the likelihood is intractable, we use simulation-based inference powered by random forests to estimate model parameters from summary statistics of the data. Simulations and modeling of streamflow data from the U.S. Geological Survey illustrate the feasibility and practicality of our approach."}
{"id": "2602.12974", "categories": ["stat.AP", "cs.CV", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.12974", "abs": "https://arxiv.org/abs/2602.12974", "authors": ["Jian Kang", "Thomas Nichols", "Lexin Li", "Martin A. Lindquist", "Hongtu Zhu"], "title": "Statistical Opportunities in Neuroimaging", "comment": "33 pages, 3 figures", "summary": "Neuroimaging has profoundly enhanced our understanding of the human brain by characterizing its structure, function, and connectivity through modalities like MRI, fMRI, EEG, and PET. These technologies have enabled major breakthroughs across the lifespan, from early brain development to neurodegenerative and neuropsychiatric disorders. Despite these advances, the brain is a complex, multiscale system, and neuroimaging measurements are correspondingly high-dimensional. This creates major statistical challenges, including measurement noise, motion-related artifacts, substantial inter-subject and site/scanner variability, and the sheer scale of modern studies. This paper explores statistical opportunities and challenges in neuroimaging across four key areas: (i) brain development from birth to age 20, (ii) the adult and aging brain, (iii) neurodegeneration and neuropsychiatric disorders, and (iv) brain encoding and decoding. After a quick tutorial on major imaging technologies, we review cutting-edge studies, underscore data and modeling challenges, and highlight research opportunities for statisticians. We conclude by emphasizing that close collaboration among statisticians, neuroscientists, and clinicians is essential for translating neuroimaging advances into improved diagnostics, deeper mechanistic insight, and more personalized treatments."}
