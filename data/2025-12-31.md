<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 3]
- [stat.ML](#stat.ML) [Total: 11]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 14]
- [stat.OT](#stat.OT) [Total: 2]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Robust Liu-Type Estimation for Multicollinearity in Fuzzy Logistic Regression](https://arxiv.org/abs/2512.22515)
*Ayad Habib Shemail,Ahmed Razzaq Al-Lami,Amal Hadi Rashid*

Main category: stat.AP

TL;DR: 针对模糊逻辑回归模型中的多重共线性问题，提出了多种Liu型估计器，通过模拟和实际数据验证，发现FLLTPE和FLLTE表现最优


<details>
  <summary>Details</summary>
Motivation: 模糊逻辑回归模型在多重共线性条件下会出现参数估计不稳定和方差膨胀的问题，需要有效的解决方法

Method: 使用多种Liu型估计器：FMLE、FLRE、FLLE、FLLTE和FLLTPE，通过模拟实验（不同样本量）和实际肾脏衰竭模糊数据分析

Result: FLLTPE和FLLTE在均方误差和拟合优度标准上表现优于其他估计器

Conclusion: FLLTPE和FLLTE是解决模糊逻辑回归中多重共线性问题的有效方法，具有更好的估计性能

Abstract: This article addresses the fuzzy logistic regression model under conditions of multicollinearity, which causes instability and inflated variance in parameter estimation. In this model, both the response variable and parameters are represented as fuzzy triangular numbers. To overcome the multicollinearity problem, various Liu-type estimators were employed: Fuzzy Maximum Likelihood Estimators (FMLE), Fuzzy Logistic Ridge Estimators (FLRE), Fuzzy Logistic Liu Estimators (FLLE), Fuzzy Logistic Liu-type Estimators (FLLTE), and Fuzzy Logistic Liu-type Parameter Estimators (FLLTPE). Through simulations with various sample sizes and application to real fuzzy data on kidney failure, model performance was evaluated using mean square error (MSE) and goodness of fit criteria. Results demonstrated superior performance of FLLTPE and FLLTE compared to other estimators.

</details>


### [2] [Counterfactual Harm: A Counter-argument](https://arxiv.org/abs/2512.22892)
*Amit N. Sawant,Mats J. Stensrud*

Main category: stat.AP

TL;DR: 论文指出基于反事实推理的"伤害"定义在多治疗方案场景下会产生非传递性问题，而基于期望效用的干预主义定义能确保传递性排序。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地用于指导决策，确保其遵循伦理原则至关重要。医学中的核心原则是非伤害原则（"不伤害"）。现有的基于反事实推理的伤害定义在简单二元治疗场景中流行，但在多治疗方案场景下存在问题。

Method: 通过一个包含三种结核病治疗方案（A、B、C）的示例，展示反事实伤害定义会产生非传递性结果：B比A伤害小，C比B伤害小，但C比A伤害大。对比基于期望效用的干预主义伤害定义，后者避免了反事实比较并确保传递性治疗排序。

Result: 反事实伤害定义在多治疗方案场景下会产生非传递性排序，这可能导致临床决策难以辩护。而干预主义定义基于期望效用，能产生一致的传递性治疗排序。

Conclusion: 在多治疗方案决策中，基于期望效用的干预主义伤害定义优于反事实定义，因为它能确保传递性排序，为临床决策提供更可靠的理论基础。

Abstract: As AI systems are increasingly used to guide decisions, it is essential that they follow ethical principles. A core principle in medicine is non-maleficence, often equated with ``do no harm''. A formal definition of harm based on counterfactual reasoning has been proposed and popularized. This notion of harm has been promoted in simple settings with binary treatments and outcomes. Here, we highlight a problem with this definition in settings involving multiple treatment options. Illustrated by an example with three tuberculosis treatments (say, A, B, and C), we demonstrate that the counterfactual definition of harm can produce intransitive results: B is less harmful than A, C is less harmful than B, yet C is more harmful than A when compared pairwise. This intransitivity poses a challenge as it may lead to practical (clinical) decisions that are difficult to justify or defend. In contrast, an interventionist definition of harm based on expected utility forgoes counterfactual comparisons and ensures transitive treatment rankings.

</details>


### [3] [Reliability Analysis of a 1-out-of-n Cold Standby Redundant System under the Generalized Lindley Distribution](https://arxiv.org/abs/2512.23019)
*Afshin Yaghoubi,Esmaile Khorram,Omid Naghshineh Arjmand*

Main category: stat.AP

TL;DR: 研究推导了1-out-of-n冷备冗余系统在广义Lindley分布下的可靠性闭式表达式


<details>
  <summary>Details</summary>
Motivation: 现有可靠性分析主要假设指数、Erlang或Weibull分布，而广义Lindley分布因其危险函数的优良特性可作为替代，但目前缺乏在冷备冗余系统中的研究

Method: 使用矩生成函数方法确定n个独立同分布广义Lindley随机变量和的分布，然后推导完美和不完美切换下系统的可靠性闭式表达式

Result: 获得了广义Lindley分布下1-out-of-n冷备冗余系统的可靠性闭式表达式

Conclusion: 广义Lindley分布可作为冷备冗余系统可靠性分析的有效替代分布，研究结果为该类系统提供了新的分析工具

Abstract: Cold standby 1-out-of-n redundant systems are well-established models in system reliability engineering. To date, reliability analyses of such systems have predominantly assumed exponential, Erlang, or Weibull failure distributions for their components. The Lindley distribution and its generalizations represent a significant class of statistical distributions in reliability engineering. Certain generalized Lindley distributions, due to the appealing characteristics of their hazard functions, can serve as suitable alternatives to other well-known lifetime distributions like the Weibull. This study investigates the reliability of a 1-out-of-n cold standby redundant system with perfect and imperfect switching, assuming that the active component failure times follow the Generalized Lindley distribution. We derive a closed-form expression for the system reliability. To achieve this, the distribution of the sum of n independent and identically distributed random variables following the Generalized Lindley distribution is first determined using the moment-generating function approach.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [4] [A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue](https://arxiv.org/abs/2512.22282)
*Qianqian Qi,Peter G. M. van der Heijden*

Main category: stat.ML

TL;DR: 论文分析了五种非负矩阵分解相关模型（LBA、LCA、EMA、PLSA、NMF）的相似性和可识别性问题，证明它们的解唯一性等价，并提供了算法综述和应用示例。


<details>
  <summary>Details</summary>
Motivation: 不同领域（机器学习、社会科学、地理学）中存在着多种相似的非负矩阵分解模型，但这些模型使用不同名称，其相似性未被充分认识，导致研究重复和知识碎片化。

Method: 通过理论分析比较五种流行模型：潜在预算分析（LBA）、潜在类别分析（LCA）、端元分析（EMA）、概率潜在语义分析（PLSA）和非负矩阵分解（NMF），重点研究模型的可识别性问题。

Result: 证明了LBA、EMA、LCA、PLSA的解唯一性当且仅当NMF的解唯一，建立了这些模型在可识别性上的等价关系。

Conclusion: 这些模型本质上是相似或等价的，统一理解有助于避免重复研究，促进跨领域知识交流，并讨论了相关模型如原型分析。

Abstract: Across fields such as machine learning, social science, geography, considerable attention has been given to models that factorize a nonnegative matrix into the product of two or three matrices, subject to nonnegative or row-sum-to-1 constraints. Although these models are to a large extend similar or even equivalent, they are presented under different names, and their similarity is not well known. This paper highlights similarities among five popular models, latent budget analysis (LBA), latent class analysis (LCA), end-member analysis (EMA), probabilistic latent semantic analysis (PLSA), and nonnegative matrix factorization (NMF). We focus on an essential issue-identifiability-of these models and prove that the solution of LBA, EMA, LCA, PLSA is unique if and only if the solution of NMF is unique. We also provide a brief review for algorithms of these models. We illustrate the models with a time budget dataset from social science, and end the paper with a discussion of closely related models such as archetypal analysis.

</details>


### [5] [On Fibonacci Ensembles: An Alternative Approach to Ensemble Learning Inspired by the Timeless Architecture of the Golden Ratio](https://arxiv.org/abs/2512.22284)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 提出Fibonacci Ensembles框架，使用斐波那契权重和二阶递归动态来改进集成学习，在回归实验中展现出优于均匀平均的效果。


<details>
  <summary>Details</summary>
Motivation: 受自然界斐波那契序列的启发，作者认为集成学习系统可以从这种自然界的和谐增长模式中获益，从而超越传统的bagging、boosting和随机森林等集成方法。

Method: 提出两种相互关联的公式：(1) 使用经过正交化和Rao-Blackwell优化的归一化斐波那契权重来减少基学习器的方差；(2) 采用二阶递归集成动态来丰富表示深度。

Result: 在一维回归实验中，使用随机傅里叶特征集成和多项式集成，斐波那契加权在某些情况下匹配或优于均匀平均，并能与正交Rao-Blackwell化进行原则性交互。

Conclusion: 斐波那契集成在集成学习理论中形成了一个自然且可解释的设计点，表明学习系统可以从自然界的和谐模式中获得指导。

Abstract: Nature rarely reveals her secrets bluntly, yet in the Fibonacci sequence she grants us a glimpse of her quiet architecture of growth, harmony, and recursive stability \citep{Koshy2001Fibonacci, Livio2002GoldenRatio}. From spiral galaxies to the unfolding of leaves, this humble sequence reflects a universal grammar of balance. In this work, we introduce \emph{Fibonacci Ensembles}, a mathematically principled yet philosophically inspired framework for ensemble learning that complements and extends classical aggregation schemes such as bagging, boosting, and random forests \citep{Breiman1996Bagging, Breiman2001RandomForests, Friedman2001GBM, Zhou2012Ensemble, HastieTibshiraniFriedman2009ESL}. Two intertwined formulations unfold: (1) the use of normalized Fibonacci weights -- tempered through orthogonalization and Rao--Blackwell optimization -- to achieve systematic variance reduction among base learners, and (2) a second-order recursive ensemble dynamic that mirrors the Fibonacci flow itself, enriching representational depth beyond classical boosting. The resulting methodology is at once rigorous and poetic: a reminder that learning systems flourish when guided by the same intrinsic harmonies that shape the natural world. Through controlled one-dimensional regression experiments using both random Fourier feature ensembles \citep{RahimiRecht2007RFF} and polynomial ensembles, we exhibit regimes in which Fibonacci weighting matches or improves upon uniform averaging and interacts in a principled way with orthogonal Rao--Blackwellization. These findings suggest that Fibonacci ensembles form a natural and interpretable design point within the broader theory of ensemble learning.

</details>


### [6] [A General Weighting Theory for Ensemble Learning: Beyond Variance Reduction via Spectral and Geometric Structure](https://arxiv.org/abs/2512.22286)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 该论文提出了一个超越传统方差减少解释的集成学习加权理论，解释了为什么对于方差已经很低的基础学习器（如平滑样条、核岭回归等），结构化加权集成仍然有效。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习理论主要基于方差减少来解释其有效性，但这无法解释为什么对于方差已经很低的基础学习器（如正则化RKHS方法），集成仍然有效。需要一个新的理论框架来解释这种现象。

Method: 将集成学习形式化为作用于假设空间的线性算子，赋予加权序列几何和谱约束。在这个框架下推导了改进的偏差-方差近似分解，展示非均匀结构化权重如何通过重塑近似几何和重新分配谱复杂性来超越均匀平均。

Result: 提出了结构化加权集成在理论上优于均匀集成的条件，并证明最优权重是约束二次规划的解。经典平均、堆叠和最近提出的基于斐波那契的集成都作为该统一理论的特例出现。

Conclusion: 该工作为结构驱动的集成学习建立了原则性基础，解释了为什么集成对于平滑、低方差的基础学习器仍然有效，并为后续开发分布自适应和动态演化的加权方案奠定了基础。

Abstract: Ensemble learning is traditionally justified as a variance-reduction strategy, explaining its strong performance for unstable predictors such as decision trees. This explanation, however, does not account for ensembles constructed from intrinsically stable estimators-including smoothing splines, kernel ridge regression, Gaussian process regression, and other regularized reproducing kernel Hilbert space (RKHS) methods whose variance is already tightly controlled by regularization and spectral shrinkage. This paper develops a general weighting theory for ensemble learning that moves beyond classical variance-reduction arguments. We formalize ensembles as linear operators acting on a hypothesis space and endow the space of weighting sequences with geometric and spectral constraints. Within this framework, we derive a refined bias-variance approximation decomposition showing how non-uniform, structured weights can outperform uniform averaging by reshaping approximation geometry and redistributing spectral complexity, even when variance reduction is negligible. Our main results provide conditions under which structured weighting provably dominates uniform ensembles, and show that optimal weights arise as solutions to constrained quadratic programs. Classical averaging, stacking, and recently proposed Fibonacci-based ensembles appear as special cases of this unified theory, which further accommodates geometric, sub-exponential, and heavy-tailed weighting laws. Overall, the work establishes a principled foundation for structure-driven ensemble learning, explaining why ensembles remain effective for smooth, low-variance base learners and setting the stage for distribution-adaptive and dynamically evolving weighting schemes developed in subsequent work.

</details>


### [7] [Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds](https://arxiv.org/abs/2512.22473)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: stat.ML

TL;DR: 论文分析了Transformer中注意力机制如何通过梯度下降训练形成支持贝叶斯推理的内部几何结构，揭示了注意力分数和值向量的更新规律。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在精心设计的"贝叶斯风洞"和大规模语言模型中表现出精确的概率推理能力，但梯度学习如何创建所需内部几何结构的机制仍不明确。本文旨在揭示交叉熵训练如何重塑注意力分数和值向量的机制。

Method: 提出注意力分数的"基于优势的路由法则"和值向量的"责任加权更新"公式，分析这些方程如何诱导正反馈循环。通过控制模拟（包括粘性马尔可夫链任务），将闭式EM风格更新与标准SGD进行比较，展示梯度动态如何塑造低维流形。

Result: 揭示了注意力权重实现E步（软责任分配），值向量实现M步（责任加权原型更新），查询和键调整假设框架。这种耦合专业化行为类似于两时间尺度EM过程，梯度动态最小化交叉熵的同时塑造了支持贝叶斯推理的低维流形。

Conclusion: 优化（梯度流）产生几何结构（贝叶斯流形），进而支持功能（上下文概率推理），为Transformer如何通过梯度学习实现贝叶斯推理提供了统一的理论框架。

Abstract: Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \emph{advantage-based routing law} for attention scores, \[ \frac{\partial L}{\partial s_{ij}} = α_{ij}\bigl(b_{ij}-\mathbb{E}_{α_i}[b]\bigr), \qquad b_{ij} := u_i^\top v_j, \] coupled with a \emph{responsibility-weighted update} for values, \[ Δv_j = -η\sum_i α_{ij} u_i, \] where $u_i$ is the upstream gradient at position $i$ and $α_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning).

</details>


### [8] [Likelihood-Preserving Embeddings for Statistical Inference](https://arxiv.org/abs/2512.22638)
*Deniz Akdemir*

Main category: stat.ML

TL;DR: 该论文提出了"似然保持嵌入"的理论框架，通过控制似然比失真度Δ_n来确保嵌入表示不改变统计推断结论，证明了Hinge定理的必要充分性，并提供了神经网络作为近似充分统计量的构造方法。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习嵌入虽然能有效压缩高维数据，但通常会破坏经典似然统计推断所需的几何结构。需要开发能够在似然推断工作流中替代原始数据而不改变推断结论的嵌入表示方法。

Method: 引入似然比失真度Δ_n度量嵌入引起的对数似然比最大误差；提出Hinge定理证明控制Δ_n是保持推断的必要充分条件；构建神经网络作为近似充分统计量的框架，建立训练损失与推断保证之间的显式界限。

Result: 证明了当Δ_n = o_p(1)时，所有基于似然比的检验和贝叶斯因子都渐近保持，替代最大似然估计与全数据MLE渐近等价；通过高斯和柯西分布实验验证了指数族理论的尖锐相变预测；临床推断应用展示了实际效用。

Conclusion: 该研究建立了似然保持嵌入的严格理论框架，为在压缩表示中保持统计推断提供了理论基础和实用方法，解决了机器学习嵌入与经典统计推断之间的兼容性问题。

Abstract: Modern machine learning embeddings provide powerful compression of high-dimensional data, yet they typically destroy the geometric structure required for classical likelihood-based statistical inference. This paper develops a rigorous theory of likelihood-preserving embeddings: learned representations that can replace raw data in likelihood-based workflows -- hypothesis testing, confidence interval construction, model selection -- without altering inferential conclusions. We introduce the Likelihood-Ratio Distortion metric $Δ_n$, which measures the maximum error in log-likelihood ratios induced by an embedding. Our main theoretical contribution is the Hinge Theorem, which establishes that controlling $Δ_n$ is necessary and sufficient for preserving inference. Specifically, if the distortion satisfies $Δ_n = o_p(1)$, then (i) all likelihood-ratio based tests and Bayes factors are asymptotically preserved, and (ii) surrogate maximum likelihood estimators are asymptotically equivalent to full-data MLEs. We prove an impossibility result showing that universal likelihood preservation requires essentially invertible embeddings, motivating the need for model-class-specific guarantees. We then provide a constructive framework using neural networks as approximate sufficient statistics, deriving explicit bounds connecting training loss to inferential guarantees. Experiments on Gaussian and Cauchy distributions validate the sharp phase transition predicted by exponential family theory, and applications to distributed clinical inference demonstrate practical utility.

</details>


### [9] [JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference](https://arxiv.org/abs/2512.22999)
*Niels Bracher,Lars Kühmichel,Desi R. Ivanova,Xavier Intes,Paul-Christian Bürkner,Stefan T. Radev*

Main category: stat.ML

TL;DR: JADAI框架通过端到端训练联合摊销贝叶斯自适应设计与推理，使用扩散模型估计高维多模态后验，在自适应设计基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 在参数估计问题中，需要主动优化设计变量以最大化信息增益，传统方法难以处理高维多模态后验分布

Method: 提出JADAI框架，端到端训练策略网络、历史网络和推理网络，使用扩散模型作为后验估计器，最小化实验序列中后验误差的增量减少

Result: 在标准自适应设计基准测试中，JADAI实现了优越或具有竞争力的性能

Conclusion: JADAI为贝叶斯自适应设计与推理提供了一种有效的联合摊销方法，特别适用于高维多模态后验估计问题

Abstract: We consider problems of parameter estimation where design variables can be actively optimized to maximize information gain. To this end, we introduce JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, a history network, and an inference network end-to-end. The networks minimize a generic loss that aggregates incremental reductions in posterior error along experimental sequences. Inference networks are instantiated with diffusion-based posterior estimators that can approximate high-dimensional and multimodal posteriors at every experimental step. Across standard adaptive design benchmarks, JADAI achieves superior or competitive performance.

</details>


### [10] [Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity](https://arxiv.org/abs/2512.23071)
*Krishna Harsha Kovelakuntla Huthasana,Alireza Olama,Andreas Lundell*

Main category: stat.ML

TL;DR: 联邦学习中引入L0约束实现稀疏化，通过概率门重参数化技术，在保持数据隐私的同时提升模型泛化能力和通信效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据和模型的固有稀疏性未得到充分利用，导致模型过于稠密，在数据和客户端参与异构性下泛化能力差。需要一种能在保持隐私的同时实现模型稀疏化的方法。

Method: 提出联邦学习中引入L0约束（非零参数密度约束），通过概率门重参数化及其连续松弛技术实现。该方法源自集中式机器学习中的稀疏化技术，通过联邦随机梯度下降算法进行分布式学习。

Result: 在数据和客户端参与异构性下，能够达到目标参数密度ρ，同时在统计性能上损失最小。在线性回归、逻辑回归、多类分类、多标签分类和卷积神经网络等任务上表现优异。在合成数据（ρ=0.05）和公开数据集RCV1、MNIST、EMNIST（ρ=0.005）上验证了方法的通信效率和统计性能优势。

Conclusion: 该方法在联邦学习中实现了有效的模型稀疏化，相比基于幅度剪枝的阈值算法，在通信效率和统计性能上表现更优，能够处理数据和客户端参与的异构性问题。

Abstract: Federated Learning (FL) is a distributed machine learning setting that requires multiple clients to collaborate on training a model while maintaining data privacy. The unaddressed inherent sparsity in data and models often results in overly dense models and poor generalizability under data and client participation heterogeneity. We propose FL with an L0 constraint on the density of non-zero parameters, achieved through a reparameterization using probabilistic gates and their continuous relaxation: originally proposed for sparsity in centralized machine learning. We show that the objective for L0 constrained stochastic minimization naturally arises from an entropy maximization problem of the stochastic gates and propose an algorithm based on federated stochastic gradient descent for distributed learning. We demonstrate that the target density (rho) of parameters can be achieved in FL, under data and client participation heterogeneity, with minimal loss in statistical performance for linear and non-linear models: Linear regression (LR), Logistic regression (LG), Softmax multi-class classification (MC), Multi-label classification with logistic units (MLC), Convolution Neural Network (CNN) for multi-class classification (MC). We compare the results with a magnitude pruning-based thresholding algorithm for sparsity in FL. Experiments on synthetic data with target density down to rho = 0.05 and publicly available RCV1, MNIST, and EMNIST datasets with target density down to rho = 0.005 demonstrate that our approach is communication-efficient and consistently better in statistical performance.

</details>


### [11] [Probabilistic Modelling is Sufficient for Causal Inference](https://arxiv.org/abs/2512.23408)
*Bruno Mlodozeniec,David Krueger,Richard E. Turner*

Main category: stat.ML

TL;DR: 本文主张因果推断问题可以在概率建模框架内解决，无需专门的因果工具或符号，通过"写下所有概率"的方法即可回答任何因果问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域对因果推断存在混淆，普遍认为需要专门的因果框架或符号来回答因果问题。本文旨在澄清这一误解，证明标准概率建模工具足以处理因果推断。

Method: 通过具体示例展示如何通过"写下所有概率"（write down the probability of everything）的方法来回答因果问题，将因果工具重新解释为标准概率建模和推断的产物。

Result: 证明任何因果推断问题都可以在概率建模和推断的范畴内解决，无需专门的因果工具或符号，阐明了因果工具的必要性和实用性。

Conclusion: 因果推断不需要专门的框架或符号，标准概率建模工具足够强大，能够回答所有因果问题，因果工具应被视为概率建模的自然产物。

Abstract: Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.

</details>


### [12] [The Nonstationarity-Complexity Tradeoff in Return Prediction](https://arxiv.org/abs/2512.23596)
*Agostino Capponi,Chengpiao Huang,J. Antonio Sidaoui,Kaizheng Wang,Jiacheng Zou*

Main category: stat.ML

TL;DR: 本文提出了一种在非平稳环境下进行股票收益预测的模型选择方法，通过联合优化模型类别和训练窗口大小，平衡模型复杂度与非平稳性，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中进行股票收益预测面临一个基本困境：复杂模型可以减少设定误差，但需要更长的训练窗口，而这会引入更强的非平稳性。现有方法难以有效平衡这一矛盾。

Method: 提出一种新颖的模型选择方法，通过锦标赛程序联合优化模型类别和训练窗口大小。该方法自适应地在非平稳验证数据上评估候选模型，理论分析表明它能平衡设定误差、估计方差和非平稳性。

Result: 在17个行业投资组合收益上的应用表明，该方法持续优于标准的滚动窗口基准，样本外R²平均提高14-23%。在经济衰退期间表现尤为突出：在1990年海湾战争衰退期间实现正R²（基准为负），在2001年衰退期间R²绝对提升至少80个基点，在2008年金融危机期间也表现优异。

Conclusion: 该方法有效解决了非平稳环境下的模型选择问题，通过平衡模型复杂度与非平稳性，实现了接近事后最佳模型的性能。基于所选模型的交易策略在经济上产生31%更高的累计收益，证明了方法的实用价值。

Abstract: We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.

</details>


### [13] [Calibrated Multi-Level Quantile Forecasting](https://arxiv.org/abs/2512.23671)
*Tiffany Ding,Isaac Gibbs,Ryan J. Tibshirani*

Main category: stat.ML

TL;DR: MultiQT是一种在线方法，可同时保证多个分位数预测的校准性，即使面对对抗性分布偏移也能确保预测有序且不降低原有预测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的分位数预测方法在实际应用中可能无法保证校准性，特别是在面对分布偏移时。需要一种能够同时保证多个分位数水平校准性、保持预测有序性，且不降低原有预测器性能的轻量级方法。

Method: 提出Multi-Level Quantile Tracker (MultiQT)方法，该方法可以包装在任何现有的点预测或分位数预测器上，通过在线校正来保证校准性。方法确保预测有序（如0.5分位数预测不大于0.6分位数预测），并具有无遗憾保证。

Result: 在流行病预测和能源预测的实际问题中，MultiQT显著提高了预测器的校准性。该方法能够有效应对对抗性分布偏移，同时保持预测的有序性。

Conclusion: MultiQT提供了一种轻量级、理论保证的在线校准方法，能够同时处理多个分位数预测的校准问题，确保预测有序且不降低原有预测器性能，在实际应用中表现出色。

Abstract: We present an online method for guaranteeing calibration of quantile forecasts at multiple quantile levels simultaneously. A sequence of $α$-level quantile forecasts is calibrated if the forecasts are larger than the target value at an $α$-fraction of time steps. We introduce a lightweight method called Multi-Level Quantile Tracker (MultiQT) that wraps around any existing point or quantile forecaster to produce corrected forecasts guaranteed to achieve calibration, even against adversarial distribution shifts, while ensuring that the forecasts are ordered -- e.g., the 0.5-level quantile forecast is never larger than the 0.6-level forecast. Furthermore, the method comes with a no-regret guarantee that implies it will not worsen the performance of an existing forecaster, asymptotically, with respect to the quantile loss. In experiments, we find that MultiQT significantly improves the calibration of real forecasters in epidemic and energy forecasting problems.

</details>


### [14] [Bellman Calibration for V-Learning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.23694)
*Lars van der Laan,Nathan Kallus*

Main category: stat.ML

TL;DR: 提出Iterated Bellman Calibration方法，用于无限时域马尔可夫决策过程中离策略价值预测的校准，无需Bellman完备性或可实现性假设


<details>
  <summary>Details</summary>
Motivation: 在无限时域马尔可夫决策过程中，离策略价值预测的校准是一个重要问题。现有方法通常需要强假设如Bellman完备性或可实现性，限制了实际应用。需要一种简单、模型无关、后处理的校准方法。

Method: 提出Iterated Bellman Calibration方法：1) 要求具有相似长期回报预测的状态在目标策略下表现出与Bellman方程一致的一步回报；2) 通过重复将拟合的Bellman目标回归到模型预测上，适应动态、反事实设置；3) 使用双重稳健伪结果处理离策略数据；4) 形成一维拟合价值迭代方案，可应用于任何价值估计器。

Result: 该方法在弱假设下为校准和预测提供了有限样本保证，且关键是不需要Bellman完备性或可实现性假设。可以应用于任何价值估计器，提高了离策略价值预测的可靠性。

Conclusion: Iterated Bellman Calibration是一种简单有效的离策略价值预测校准方法，具有理论保证且适用性广，为强化学习中的价值估计提供了实用的校准工具。

Abstract: We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model's predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [15] [Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials](https://arxiv.org/abs/2512.22153)
*Nicolas Zilberstein,Santiago Segarra,Luiz Chamon*

Main category: stat.CO

TL;DR: 提出shielded Langevin Monte Carlo方法，用于在带孔洞的凸集（非凸空间）中采样，结合自适应温度和排斥漂移确保样本保持在可行区域内。


<details>
  <summary>Details</summary>
Motivation: 传统采样方法难以处理带孔洞的非凸支持域问题，这在约束采样中是一个新颖且具有挑战性的问题。需要开发能够在这种复杂约束空间中有效采样的方法。

Method: 基于导航函数思想，提出shielded LMC方法，结合空间自适应温度和排斥漂移项，确保样本始终保持在可行区域内，避免落入孔洞中。

Result: 在2D高斯混合分布和MIMO符号检测任务上的实验表明，shielded LMC相比无约束方法具有明显优势，能够有效处理带孔洞的非凸约束采样问题。

Conclusion: shielded LMC为解决带孔洞的非凸约束采样问题提供了一种有效方法，在复杂约束空间中表现出优越性能，为相关应用开辟了新途径。

Abstract: We introduce shielded Langevin Monte Carlo (LMC), a constrained sampler inspired by navigation functions, capable of sampling from unnormalized target distributions defined over punctured supports. In other words, this approach samples from non-convex spaces defined as convex sets with convex holes. This defines a novel and challenging problem in constrained sampling. To do so, the sampler incorporates a combination of a spatially adaptive temperature and a repulsive drift to ensure that samples remain within the feasible region. Experiments on a 2D Gaussian mixture and multiple-input multiple-output (MIMO) symbol detection showcase the advantages of the proposed shielded LMC in contrast to unconstrained cases.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [16] [Random Subset Averaging](https://arxiv.org/abs/2512.22472)
*Wenhao Cui,Jie Hu*

Main category: stat.ME

TL;DR: 提出了一种新的集成预测方法RSA，专门针对高维强相关数据，通过随机子集策略构建候选模型，并采用两轮加权方案进行聚合，无需先验知识即可实现最优预测性能。


<details>
  <summary>Details</summary>
Motivation: 针对高维数据中存在的强相关性挑战，传统模型选择和集成学习方法在预测性能上存在局限，需要一种能够自动适应协变量相关结构且无需先验知识的新方法。

Method: RSA采用二项随机子集策略构建候选模型，通过两轮加权方案（类似两层神经网络结构）进行预测聚合，所有调优参数通过交叉验证自动选择，无需协变量相关性的先验知识。

Result: 理论证明RSA在一般条件下具有渐近最优性，在正交设计下获得更低的有限样本风险界；模拟实验显示RSA在各种样本量、维度、稀疏度和相关结构下均优于传统方法；金融回报预测实证应用验证了其实际效用。

Conclusion: RSA是一种有效的高维强相关数据集成预测方法，具有理论保证和实际优越性能，能够自动适应数据特征而无需先验知识，在预测建模中具有广泛应用前景。

Abstract: We propose a new ensemble prediction method, Random Subset Averaging (RSA), tailored for settings with many covariates, particularly in the presence of strong correlations. RSA constructs candidate models via binomial random subset strategy and aggregates their predictions through a two-round weighting scheme, resulting in a structure analogous to a two-layer neural network. All tuning parameters are selected via cross-validation, requiring no prior knowledge of covariate relevance. We establish the asymptotic optimality of RSA under general conditions, allowing the first-round weights to be data-dependent, and demonstrate that RSA achieves a lower finite-sample risk bound under orthogonal design. Simulation studies demonstrate that RSA consistently delivers superior and stable predictive performance across a wide range of sample sizes, dimensional settings, sparsity levels and correlation structures, outperforming conventional model selection and ensemble learning methods. An empirical application to financial return forecasting further illustrates its practical utility.

</details>


### [17] [On the Choice of Model Space Priors and Multiplicity Control in Bayesian Variable Selection: An Application to Streaming Logistic Regression](https://arxiv.org/abs/2512.22504)
*Joyee Ghosh*

Main category: stat.ME

TL;DR: 本文研究了贝叶斯变量选择中不同模型空间先验对逻辑回归的影响，特别关注流数据场景，比较了Beta-Binomial先验和新提出的matryoshka doll先验，发现没有单一先验在所有场景中占优。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯变量选择中模型空间先验的设定对控制稀疏性和多重性至关重要，特别是在流数据场景下，需要研究不同先验的实际影响。

Method: 回顾流行的Beta-Binomial先验和新提出的matryoshka doll先验，引入MD先验的简单近似使其具有独立包含指标，便于可扩展推断。使用基于BIC的边缘似然近似，通过模拟研究比较不同先验对后验包含概率和系数估计的影响。

Result: 结果表明没有单一模型空间先验在所有场景中占优，新提出的MD先验提供了一个有用的中间选项，介于具有不同稀疏度的常用Beta-Binomial先验之间。

Conclusion: 在贝叶斯变量选择中，模型空间先验的选择需要根据具体场景考虑，MD先验为实际应用提供了一个有价值的中间选择。

Abstract: Bayesian variable selection (BVS) depends critically on the specification of a prior distribution over the model space, particularly for controlling sparsity and multiplicity. This paper examines the practical consequences of different model space priors for BVS in logistic regression, with an emphasis on streaming data settings. We review some popular and well-known Beta--Binomial priors alongside the recently proposed matryoshka doll (MD) prior. We introduce a simple approximation to the MD prior that yields independent inclusion indicators and is convenient for scalable inference. Using BIC-based approximations to marginal likelihoods, we compare the effect of different model space priors on posterior inclusion probabilities and coefficient estimation at intermediate and final stages of the data stream via simulation studies. Overall, the results indicate that no single model space prior uniformly dominates across scenarios, and that the recently proposed MD prior provides a useful additional option that occupies an intermediate position between commonly used Beta--Binomial priors with differing degrees of sparsity.

</details>


### [18] [Intrinsic Whittle--Matérn fields and sparse spatial extremes](https://arxiv.org/abs/2512.23395)
*David Bolin,Peter Braunsteins,Sebastian Engelke,Raphaël Huser*

Main category: stat.ME

TL;DR: 提出新的内在Whittle-Matérn高斯随机场类别，通过SPDE方法解决现有内在场模型有限、理论软件不足的问题，开发快速估计和模拟方法，应用于空间极值过程和实际数据分析。


<details>
  <summary>Details</summary>
Motivation: 现有内在高斯场模型存在两大问题：1）模型数量和灵活性非常有限；2）理论、快速推断和软件发展不足。需要开发更灵活的内在场模型并改进相关方法。

Method: 引入新的内在Whittle-Matérn高斯随机场类别，通过随机偏微分方程（SPDE）定义，利用有限元近似产生的稀疏性开发快速估计和模拟方法，并将该方法扩展到Brown-Resnick过程建模空间极值事件。

Result: 开发了灵活的内在场模型和高效推断方法，在克里金外推任务中展示优势，建立了内在场与空间极值过程的联系，实现了前所未有的高维高效推断。

Conclusion: 新的内在SPDE方法为空间依赖建模提供了灵活框架，在肾脏功能纵向研究和海洋热浪高分辨率海温数据分析中展示了广泛适用性，为高维空间推断开辟了新途径。

Abstract: Intrinsic Gaussian fields are used in many areas of statistics as models for spatial or spatio-temporal dependence, or as priors for latent variables. However, there are two major gaps in the literature: first, the number and flexibility of existing intrinsic models are very limited; second, theory, fast inference, and software are currently underdeveloped for intrinsic fields. We tackle these challenges by introducing the new flexible class of intrinsic Whittle--Matérn Gaussian random fields obtained as the solution to a stochastic partial differential equation (SPDE). Exploiting sparsity resulting from finite-element approximations, we develop fast estimation and simulation methods for these models. We demonstrate the benefits of this intrinsic SPDE approach for the important task of kriging under extrapolation settings. Leveraging the connection of intrinsic fields to spatial extreme value processes, we translate our theory to an SPDE approach for Brown--Resnick processes for sparse modeling of spatial extreme events. This new paradigm paves the way for efficient inference in unprecedented dimensions. To demonstrate the wide applicability of our new methodology, we apply it in two very different areas: a longitudinal study of renal function data, and the modeling of marine heat waves using high-resolution sea surface temperature data.

</details>


### [19] [Bend to Mend: Toward Trustworthy Variational Bayes with Valid Uncertainty Quantification](https://arxiv.org/abs/2512.22655)
*Jiaming Liu,Meng Li*

Main category: stat.ME

TL;DR: 提出Trustworthy Variational Bayes (TVB)方法，通过故意错误设定似然函数来校正变分贝叶斯的不确定性量化，使置信区间达到渐近正确的频率覆盖概率。


<details>
  <summary>Details</summary>
Motivation: 变分贝叶斯(VB)虽然计算高效，但其不确定性量化(UQ)不可靠，置信区间往往覆盖不足，无法达到名义频率覆盖概率。

Method: 采用"弯曲以修复"策略：基于分数变分贝叶斯方法故意错误设定似然函数，然后使用符合性技术（样本分割和自助法）识别最优分数参数，从而重新校准UQ。

Result: 理论证明校准后的置信区间对给定感兴趣参数达到渐近正确的频率覆盖；数值实验显示TVB优于标准VB，在有限样本中达到正常频率覆盖。

Conclusion: TVB方法有效解决了VB不确定性量化不可靠的问题，首次为VB提供了理论保证，并通过"TVB表"实现高效并行化和参数无关的校准。

Abstract: Variational Bayes (VB) is a popular and computationally efficient method to approximate the posterior distribution in Bayesian inference, especially when the exact posterior is analytically intractable and sampling-based approaches are computationally prohibitive. While VB often yields accurate point estimates, its uncertainty quantification (UQ) is known to be unreliable. For example, credible intervals derived from VB posteriors tend to exhibit undercoverage, failing to achieve nominal frequentist coverage probabilities. In this article, we address this challenge by proposing Trustworthy Variational Bayes (TVB), a method to recalibrate the UQ of broad classes of VB procedures. Our approach follows a bend-to-mend strategy: we intentionally misspecify the likelihood to correct VB's flawed UQ. In particular, we first relax VB by building on a recent fractional VB method, and then identify the optimal fraction parameter using conformal techniques such as sample splitting and bootstrapping. This yields recalibrated UQ for any given parameter of interest. On the theoretical front, we establish that the calibrated credible intervals achieve asymptotically correct frequentist coverage for a given parameter of interest; this, to the best of our knowledge, is the first such theoretical guarantee for VB. On the practical front, we introduce the "TVB table", which enables (1) massive parallelization and remains agnostic to the parameter of interest during its construction, and (2) efficient identification of the optimal fraction parameter for any specified parameter of interest. The proposed method is illustrated via Gaussian mixture models and Bayesian mixture linear regression models, and numerical experiments demonstrate that the TVB method outperforms standard VB and achieves normal frequentist coverage in finite samples. A real data application is also discussed.

</details>


### [20] [Ranked Set Sampling in Survival Analysis](https://arxiv.org/abs/2512.22659)
*Nabil Awan,Richard J. Chappell*

Main category: stat.ME

TL;DR: 本文为平衡排序集抽样(RSS)开发了一个统一的生存分析框架，保留了效率优势，同时提供了应用实践中期望的推断工具，包括Kaplan-Meier估计、log-rank检验和限制平均生存时间等。


<details>
  <summary>Details</summary>
Motivation: 排序集抽样(RSS)是一种成本效益高的研究设计，但现有的生存分析主要局限于Kaplan-Meier生存曲线估计。许多标准工具如log-rank检验、限制平均生存时间等在RSS设置下尚未完全开发，特别是在排序不完美和存在删失的情况下。

Method: 为平衡RSS设计开发统一生存分析框架，形式化Kaplan-Meier和Nelson-Aalen估计器（适用于完美和不完美排序），使用适应RSS结构的鞅和实证过程方法建立大样本性质，提出基于排序的Greenwood型方差估计器，并扩展到log-rank检验、Fleming-Harrington加权检验以及限制和窗口平均生存时间函数。

Result: 通过模拟研究评估了相对于简单随机抽样的效率，考虑了集合大小、周期数、删失比例和排序质量的变化。提供了渐近方差公式和两样本比较，并给出了实施计划和实际数据说明。

Conclusion: 该框架在保留RSS效率优势的同时，为应用实践提供了完整的生存分析工具集，填补了RSS生存分析中标准推断工具的空白，特别是在排序不完美和存在删失的情况下。

Abstract: Ranked set sampling (RSS) is a cost-efficient study design that uses inexpensive baseline ranking to select a more informative subset of individuals for full measurement. While RSS is well known to improve precision over simple random sampling (SRS) for uncensored outcomes, survival analysis under RSS has largely been limited to estimation of the Kaplan-Meier survival curve under random censoring. Consequently, many standard tools routinely used with SRS data, including log-rank and weighted log-rank tests, restricted mean survival time summaries, and window-based mean life measures, are not yet fully developed for RSS settings, particularly when ranking is imperfect and censoring is present.
  This work develops a unified survival analysis framework for balanced RSS designs that preserves efficiency gains while providing the inferential tools expected in applied practice. We formalize Kaplan-Meier and Nelson-Aalen estimators for right-censored data under both perfect and concomitant-based imperfect ranking and establish their large-sample properties using martingale and empirical process methods adapted to the rank-wise RSS structure. Rank-aware Greenwood-type variance estimators are proposed, and efficiency relative to SRS is evaluated through simulation studies varying set size, number of cycles, censoring proportion, and ranking quality. The framework is further extended to log-rank and Fleming-Harrington weighted tests, as well as restricted and window mean life functionals with asymptotic variance formulas and two-sample comparisons. An implementation plan with real-data illustrations is provided to facilitate practical use.

</details>


### [21] [Considering parallel tempering and comparing post-treatment procedures in Bayesian Profile Regression Models for a survival outcome and correlated exposures](https://arxiv.org/abs/2512.23571)
*Fendler Julie,Guihenneuc Chantal,Ancelet Sophie*

Main category: stat.ME

TL;DR: 本文针对贝叶斯剖面回归混合模型（BPRM）在应用中结果不稳定的问题，提出了改进的MCMC算法以减少局部模式陷阱并降低计算时间，同时通过模拟研究比较不同后处理方法，为实际应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯剖面回归混合模型用于评估多暴露人群的健康风险，通过将个体按暴露剖面和健康风险聚类。然而，基于MCMC算法的结果在不同应用案例中表现出不稳定性，主要原因是MCMC算法可能陷入后验分布的局部模式，以及后处理方法的选择会导致不同的聚类结构。

Method: 1. 改进先前工作中提出的MCMC算法，以避免陷入后验分布的局部模式，同时减少计算时间。2. 进行模拟研究，比较不同MCMC算法和后处理方法的性能。3. 在辐射流行病学中进行实际应用。

Result: 通过改进的MCMC算法能够更有效地避免局部模式陷阱，同时提高了计算效率。模拟研究为不同算法和后处理方法的选择提供了性能比较和实用指南。辐射流行病学的应用案例验证了改进方法的有效性。

Conclusion: 本文提出的改进MCMC算法解决了BPRM模型结果不稳定的问题，通过系统比较不同方法为实际应用提供了明确的指导原则，特别是在辐射流行病学等复杂暴露评估场景中具有实用价值。

Abstract: Bayesian profile regression mixture models (BPRM) allow to assess a health risk in a multi-exposed population. These mixture models cluster individuals according to their exposure profile and their health risk. However, their results, based on Monte-Carlo Markov Chain (MCMC) algorithms, turned out to be unstable in different application cases. We suppose two reasons for this instability. The MCMC algorithm can be trapped in local modes of the posterior distribution and the choice of post-treatment procedures used on the output of the MCMC algorithm leads to different clustering structures. In this work, we propose improvements of the MCMC algorithms proposed in previous works in order to avoid the local modes of the posterior distribution while reducing the computation time. We also carry out a simulation study to compare the performances of the MCMC algorithms and different post-processing in order to provide guidelines on their use. An application in radiation epidemiology is considered.

</details>


### [22] [Robust and Well-conditioned Sparse Estimation for High-dimensional Covariance Matrices](https://arxiv.org/abs/2512.23250)
*Shaoxin Wang,Ziyun Ma*

Main category: stat.ME

TL;DR: 提出一种新的稳健且条件良好的稀疏协方差矩阵估计器，通过直接引入条件数约束在稳健自适应阈值框架中，同时保证正定性、数值稳定性和稀疏性。


<details>
  <summary>Details</summary>
Motivation: 高维复杂数据的协方差矩阵估计面临正定性、稀疏性和数值稳定性挑战。现有稳健稀疏估计器在有限样本中无法保证正定性，而后续的正定性修正会破坏稀疏性且缺乏对条件数的显式控制。

Method: 在稳健自适应阈值框架中直接引入条件数约束，将估计问题构建为凸优化问题，并开发了具有收敛保证的高效交替方向算法。

Result: 理论证明该估计器在Frobenius范数下达到极小极大最优收敛率。仿真和实际数据应用表明，该方法能一致产生正定、条件良好且稀疏的估计，数值稳定性与特征值边界方法相当或更优，且需要更少的调参。

Conclusion: 提出的方法通过直接整合条件数约束，同时解决了正定性、稀疏性和数值稳定性问题，为高维协方差矩阵估计提供了一个理论保证良好且实用的解决方案。

Abstract: Estimating covariance matrices with high-dimensional complex data presents significant challenges, particularly concerning positive definiteness, sparsity, and numerical stability. Existing robust sparse estimators often fail to guarantee positive definiteness in finite samples, while subsequent positive-definite correction can degrade sparsity and lack explicit control over the condition number. To address these limitations, we propose a novel robust and well-conditioned sparse covariance matrix estimator. Our key innovation is the direct incorporation of a condition number constraint within a robust adaptive thresholding framework. This constraint simultaneously ensures positive definiteness, enforces a controllable level of numerical stability, and preserves the desired sparse structure without resorting to post-hoc modifications that compromise sparsity. We formulate the estimation as a convex optimization problem and develop an efficient alternating direction algorithm with guaranteed convergence. Theoretically, we establish that the proposed estimator achieves the minimax optimal convergence rate under the Frobenius norm. Comprehensive simulations and real-data applications demonstrate that our method consistently produces positive definite, well-conditioned, and sparse estimates, and achieves comparable or superior numerical stability to eigenvalue-bound methods while requiring less tuning parameters.

</details>


### [23] [A Wide-Sense Stationarity Test Based on the Geometric Structure of Covariance](https://arxiv.org/abs/2512.23251)
*Wang Yinbu,Xu Yong*

Main category: stat.ME

TL;DR: 提出基于协方差函数几何特性的宽平稳性检验方法，通过估计协方差曲面的局部块并检查(1,1,0)方向导数是否为零来检验平稳性


<details>
  <summary>Details</summary>
Motivation: 需要一种不预先假设平稳性、仅要求协方差函数局部光滑的宽平稳性检验方法，能够应用于一般随机动力系统并提供时间分辨视图

Method: 估计协方差曲面的局部块，检查每个块上(1,1,0)方向的导数是否为零。该方法仅要求协方差函数局部光滑，不预先假设平稳性

Result: 方法应用于SDOF系统和随机Duffing振子，结果显示方法数值稳定，能够实际检测出对宽平稳性的偏离

Conclusion: 提出的基于协方差函数几何特性的检验方法有效，仅需局部光滑条件，不依赖平稳性先验假设，适用于一般随机动力系统

Abstract: This paper presents a test for wide-sense stationarity (WSS) based on the geometry of the covariance function. We estimate local patches of the covariance surface and then check whether the directional derivative in the $(1,1,0)$ direction is zero on each patch. The method only requires the covariance function to be locally smooth and does not assume stationarity in advance. It can be applied to general stochastic dynamical systems and provides a time-resolved view. We apply the test method to an SDOF system and to a stochastic Duffing oscillator. These examples show that the method is numerically stable and can detect departures from WSS in practice.

</details>


### [24] [Uncertainty calibration for latent-variable regression models](https://arxiv.org/abs/2512.23444)
*Zina-Sabrina Duma,Otto Lamminpää,Jouni Susiluoto,Heikki Haario,Ting Zheng,Tuomas Sihvonen,Amy Braverman,Philip A. Townsend,Satu-Pia Reinikainen*

Main category: stat.ME

TL;DR: 提出一种基于共形推理的方法，为多元统计回归模型（PLS、PCR及其核化版本）提供不确定性量化，生成依赖输入数据的预测区间


<details>
  <summary>Details</summary>
Motivation: 传统的多元统计回归模型（PLS、PCR及其核化版本）在原始形式中不包含不确定性量化，而科学分析需要评估复杂系统和数据集的变异性和可靠性

Method: 提出一种受共形推理启发的方法，用于估计和校准多元统计模型的不确定性。该方法为点预测提供依赖输入数据的预测区间，并在传统和核化版本的PLS、PCR上进行测试

Result: 在模拟数据中，模型能成功识别不确定区域并匹配不确定性大小；在实际案例中（实验室近红外和机载高光谱回归模型），优化后的模型在测试数据上既不过度自信也不过于保守（例如，95%预测区间包含95%的真实观测值）

Conclusion: 提出的共形推理方法能够有效为多元统计回归模型提供可靠的不确定性量化，在实际应用中表现出良好的校准性能

Abstract: Uncertainty quantification is essential for scientific analysis, as it allows for the evaluation and interpretation of variability and reliability in complex systems and datasets. In their original form, multivariate statistical regression models (partial least-squares regression, PLS, principal component regression, PCR) along with their kernelized versions (kernel partial least-squares regression, K-PLS, kernel principal component regression, K-PCR), do not incorporate uncertainty quantification as part of their output. In this study, we propose a method inspired by conformal inference to estimate and calibrate the uncertainty of multivariate statistical models. The result of this method is a point prediction accompanied by prediction intervals that depend on the input data. We tested the proposed method on both traditional and kernelized versions of PLS and PCR. The method is demonstrated using synthetic data, as well as laboratory near-infrared (NIR) and airborne hyperspectral regression models for estimating functional plant traits. The model was able to successfully identify the uncertain regions in the simulated data and match the magnitude of the uncertainty. In real-case scenarios, the optimised model was not overconfident nor underconfident when estimating from test data: for example, for a 95% prediction interval, 95% of the true observations were inside the prediction interval.

</details>


### [25] [Propensity Patchwork Kriging for Scalable Inference on Heterogeneous Treatment Effects](https://arxiv.org/abs/2512.23467)
*Hajime Ogawa,Shonosuke Sugasawa*

Main category: stat.ME

TL;DR: 提出基于倾向得分分区的Patchwork Kriging方法，用于高效估计异质性处理效应，通过沿倾向得分维度施加连续性约束来降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 高斯过程模型在估计异质性处理效应时具有吸引力，但其计算成本限制了在因果推断场景中的可扩展性。需要一种既能保持估计连续性又能降低计算复杂度的新方法。

Method: 扩展Patchwork Kriging到因果推断框架：1）根据估计的倾向得分对数据进行分区；2）应用Patchwork Kriging在相邻区域间强制HTE估计的连续性；3）仅沿倾向得分维度施加连续性约束，而非整个协变量空间。

Result: 该方法显著降低了计算成本，避免了简单局部近似固有的不连续性，可解释为分层的平滑扩展，为HTE估计提供了高效方法。通过模拟研究和实际数据应用验证了其有效性。

Conclusion: 提出的基于倾向得分分区的Patchwork Kriging方法在保持HTE估计连续性的同时，大幅降低了计算复杂度，为大规模因果推断中的异质性处理效应估计提供了实用解决方案。

Abstract: Gaussian process-based models are attractive for estimating heterogeneous treatment effects (HTE), but their computational cost limits scalability in causal inference settings. In this work, we address this challenge by extending Patchwork Kriging into the causal inference framework. Our proposed method partitions the data according to the estimated propensity score and applies Patchwork Kriging to enforce continuity of HTE estimates across adjacent regions. By imposing continuity constraints only along the propensity score dimension, rather than the full covariate space, the proposed approach substantially reduces computational cost while avoiding discontinuities inherent in simple local approximations. The resulting method can be interpreted as a smoothing extension of stratification and provides an efficient approach to HTE estimation. The proposed method is demonstrated through simulation studies and a real data application.

</details>


### [26] [Deep classifier kriging for probabilistic spatial prediction of air quality index](https://arxiv.org/abs/2512.23474)
*Junyu Chen,Pratik Nag,Huixia Judy-Wang,Ying Sun*

Main category: stat.ME

TL;DR: 提出深度分类克里金法(DCK)，一种用于非高斯空间过程预测的深度学习方法，特别适用于空气质量指数(AQI)的空间插值，能融合异构数据源并提供可靠的预测分布。


<details>
  <summary>Details</summary>
Motivation: 空气质量指数(AQI)的空间插值对监管决策至关重要，但AQI场具有非高斯性和复杂的非线性空间结构。传统的克里金等线性方法基于高斯假设，难以捕捉这些特征并提供可靠的预测分布。

Method: 提出深度分类克里金法(DCK)，这是一个灵活的、无分布的深度学习框架，用于估计单变量和双变量空间过程的完整预测分布函数。该方法包含数据融合机制，能够建模非共位双变量过程并整合异构空气质量数据源。

Result: 通过大量模拟实验表明，DCK在预测准确性和不确定性量化方面持续优于传统方法。应用于AQI概率空间预测时，能够融合稀疏但高质量的地面站观测数据与空间连续但有偏差的辅助模型输出，产生空间分辨的预测分布。

Conclusion: DCK为空气质量指数等非高斯空间过程提供了有效的概率预测框架，支持超标和极端事件概率估计等下游任务，有助于监管风险评估和政策制定。

Abstract: Accurate spatial interpolation of the air quality index (AQI), computed from concentrations of multiple air pollutants, is essential for regulatory decision-making, yet AQI fields are inherently non-Gaussian and often exhibit complex nonlinear spatial structure. Classical spatial prediction methods such as kriging are linear and rely on Gaussian assumptions, which limits their ability to capture these features and to provide reliable predictive distributions. In this study, we propose \textit{deep classifier kriging} (DCK), a flexible, distribution-free deep learning framework for estimating full predictive distribution functions for univariate and bivariate spatial processes, together with a \textit{data fusion} mechanism that enables modeling of non-collocated bivariate processes and integration of heterogeneous air pollution data sources. Through extensive simulation experiments, we show that DCK consistently outperforms conventional approaches in predictive accuracy and uncertainty quantification. We further apply DCK to probabilistic spatial prediction of AQI by fusing sparse but high-quality station observations with spatially continuous yet biased auxiliary model outputs, yielding spatially resolved predictive distributions that support downstream tasks such as exceedance and extreme-event probability estimation for regulatory risk assessment and policy formulation.

</details>


### [27] [Panel Coupled Matrix-Tensor Clustering Model with Applications to Asset Pricing](https://arxiv.org/abs/2512.23567)
*Liyuan Cui,Guanhao Feng,Yuefeng Han,Jiayan Li*

Main category: stat.ME

TL;DR: 提出PMTC模型，通过同时利用特征张量和收益矩阵来识别潜在资产分组，提高聚类精度和因子载荷估计


<details>
  <summary>Details</summary>
Motivation: 传统回归方法在稀疏数据和高噪声环境下难以估计资产定价模型中的分组结构和因子载荷。现有方法（如融合惩罚和多任务学习）强制跨截面单位的系数同质性，降低了灵活性。聚类方法（如谱聚类、Lloyd算法）在特定条件下能实现一致恢复，但通常只依赖单一数据源。

Method: 提出Panel Coupled Matrix-Tensor Clustering (PMTC)模型，同时利用特征张量和收益矩阵来识别潜在资产分组。开发了计算高效的张量聚类算法，整合多个数据源以提高聚类精度和因子载荷估计。

Result: 模拟实验表明，PMTC方法在聚类精度和系数估计方面优于单数据源替代方法，特别是在中等信噪比条件下。对美国股票的实证应用显示，PMTC能产生更高的样本外总R²和具有经济解释性的因子暴露变化。

Conclusion: PMTC模型通过整合特征张量和收益矩阵，有效解决了资产定价中分组结构和因子载荷的估计问题，在理论和实证上都表现出优越性能，为资产定价研究提供了实用的新工具。

Abstract: We tackle the challenge of estimating grouping structures and factor loadings in asset pricing models, where traditional regressions struggle due to sparse data and high noise. Existing approaches, such as those using fused penalties and multi-task learning, often enforce coefficient homogeneity across cross-sectional units, reducing flexibility. Clustering methods (e.g., spectral clustering, Lloyd's algorithm) achieve consistent recovery under specific conditions but typically rely on a single data source. To address these limitations, we introduce the Panel Coupled Matrix-Tensor Clustering (PMTC) model, which simultaneously leverages a characteristics tensor and a return matrix to identify latent asset groups. By integrating these data sources, we develop computationally efficient tensor clustering algorithms that enhance both clustering accuracy and factor loading estimation. Simulations demonstrate that our methods outperform single-source alternatives in clustering accuracy and coefficient estimation, particularly under moderate signal-to-noise conditions. Empirical application to U.S. equities demonstrates the practical value of PMTC, yielding higher out-of-sample total $R^2$ and economically interpretable variation in factor exposures.

</details>


### [28] [Profile Bayesian Optimization for Expensive Computer Experiments](https://arxiv.org/abs/2512.23581)
*Courtney Kyger,James Fernandez,John A. Grunenwald,James Braun,Annie Booth*

Main category: stat.ME

TL;DR: 提出一种新的贝叶斯优化方法，用于识别具有单个控制参数和多个干扰参数的确定性黑盒计算机模拟的"轮廓最优解"，该方法在多个基准测试和实际应用中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究旋转爆震燃烧发动机中扩散器的计算机模拟，该模拟返回通过扩散损失的能量作为各种设计参数的函数。目标是识别扩散器长度的最低可能能量损失，了解这种关系将有助于做出明智的设计选择。

Method: 开发了一种新颖的两阶段采集方案，平衡控制参数的探索和轮廓最优解的利用，利用深度和浅层高斯过程代理来促进不确定性量化。

Result: 提出的"轮廓贝叶斯优化"程序在各种基准测试中优于传统贝叶斯优化和轮廓优化方法，并在实际应用中证明有效。

Conclusion: 该方法成功解决了识别整个控制参数范围内轮廓最优解的问题，为具有单个控制参数和多个干扰参数的黑盒模拟提供了有效的优化框架。

Abstract: We propose a novel Bayesian optimization (BO) procedure aimed at identifying the ``profile optima'' of a deterministic black-box computer simulation that has a single control parameter and multiple nuisance parameters. The profile optima capture the optimal response values as a function of the control parameter. Our objective is to identify them across the entire plausible range of the control parameter. Classic BO, which targets a single optimum over all parameters, does not explore the entire control parameter range. Instead, we develop a novel two-stage acquisition scheme to balance exploration across the control parameter and exploitation of the profile optima, leveraging deep and shallow Gaussian process surrogates to facilitate uncertainty quantification. We are motivated by a computer simulation of a diffuser in a rotating detonation combustion engine, which returns the energy lost through diffusion as a function of various design parameters. We aim to identify the lowest possible energy loss as a function of the diffuser's length; understanding this relationship will enable well-informed design choices. Our ``profile Bayesian optimization'' procedure outperforms traditional BO and profile optimization methods on a variety of benchmarks and proves effective in our motivating application.

</details>


### [29] [Joint Modeling of Longitudinal and Survival Data: A Bayesian Approach for Predicting Disease Progression](https://arxiv.org/abs/2512.23627)
*Nithisha Suryadevara,Vivek Reddy Srigiri*

Main category: stat.ME

TL;DR: 提出贝叶斯分层联合建模框架，同时分析纵向生物标志物轨迹和生存结局，通过共享随机效应捕捉疾病动态与事件风险的关联，优于传统两阶段方法。


<details>
  <summary>Details</summary>
Motivation: 慢性病研究中需要同时分析重复测量的生物标志物和生存时间数据，传统两阶段方法因忽略两者相互依赖而导致估计偏差和预测不佳。

Method: 贝叶斯分层联合建模框架，通过共享随机效应同时建模纵向轨迹和生存结局，允许纳入先验信息、处理不规则测量时间和缺失数据，并提供后验分布进行不确定性量化。

Result: 在模拟数据和慢性肝病真实临床数据上，贝叶斯联合模型在参数估计准确性和预测性能（时间依赖性AUC和Brier评分）上均优于传统两阶段方法。

Conclusion: 该贝叶斯联合建模框架为动态、患者特异性预后提供了稳健且可解释的工具，支持个性化医疗中的临床决策。

Abstract: Joint modeling of longitudinal and survival data has become increasingly important in medical research, particularly for understanding disease progression in chronic conditions where both repeated biomarker measurements and time-to-event outcomes are available. Traditional two-stage methods, which analyze longitudinal and survival components separately, often result in biased estimates and suboptimal predictions due to failure to account for their interdependence.
  In this study, we propose a Bayesian hierarchical joint modeling framework with an emphasis on predictive evaluation and clinical interpretability. The model simultaneously characterizes the longitudinal trajectory of a biomarker and the associated survival outcome through shared random effects, capturing the intrinsic association between disease dynamics and event risk. The Bayesian formulation allows flexible incorporation of prior information, accommodates irregular measurement times and missing data, and provides full posterior distributions for uncertainty quantification via credible intervals.
  We evaluate the proposed framework using both simulated data designed to mimic realistic patient trajectories and a real-world clinical dataset involving patients with chronic liver disease. Results demonstrate that the Bayesian joint model consistently outperforms conventional two-stage approaches in terms of parameter estimation accuracy and predictive performance, as measured by time-dependent area under the curve and Brier scores. The proposed approach provides a robust and interpretable tool for dynamic, patient-specific prognosis, supporting clinical decision-making in personalized medicine.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [30] [LLteacher: A Tool for the Integration of Generative AI into Statistics Assignments](https://arxiv.org/abs/2512.23053)
*Emanuela Furfaro,Simone Mosciatti*

Main category: stat.OT

TL;DR: LLteacher是一个开源LLM工具，用于指导学生完成统计课程作业，既保留学习过程又确保与课程内容一致，支持回忆先验知识和发现新概念两种教学目标。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在日常生活中日益普及，需要将AI工具深思熟虑地整合到统计教育中，特别是在作业环节，以支持学生学习过程并确保教育公平。

Method: 开发开源工具LLteacher，作为LLM伴侣帮助学生完成作业，该工具设计为保留学习过程，引导学生以支持学习的方式与AI互动，同时确保与课程内容对齐和公平访问。

Result: 通过本科统计计算课程（使用R语言）的示例展示了LLteacher的设计和功能，证明它能有效支持回忆先验知识和发现新概念两种不同的教学目标。

Conclusion: LLteacher展示了将生成式AI整合到统计课程中的一条可行路径，虽然这是初始版本，但具有适应其他类型课程和作业的潜力。

Abstract: As generative AI becomes increasingly embedded in everyday life, the thoughtful and intentional integration of AI-based tools into statistics education has become essential. We address this need with a focus on homework assignments and we propose the use of LLMs as a companion to complete homework by developing an open-source tool named LLteacher. This LLM-based tool preserves learning processes and it guides students to engage with AI in ways that support their learning, while ensuring alignment with course content and equitable access. We illustrate LLteacher's design and functionality with examples from an undergraduate Statistical Computing course in R, showing how it supports two distinct pedagogical goals: recalling prior knowledge and discovering new concepts. While this is an initial version, LLteacher demonstrates one possible pathway for integrating generative AI into statistics courses, with strong potential for adaptation to other types of classes and assignments.

</details>


### [31] [Domain matters: Towards domain-informed evaluation for link prediction](https://arxiv.org/abs/2512.23371)
*Yilin Bi,Junhao Bian,Shuyan Wan,Shuaijia Wang,Tao Zhou*

Main category: stat.OT

TL;DR: 该研究系统评估了12种主流链接预测算法在7个领域740个真实网络上的表现，发现算法排名在不同领域间一致性很低，而在同一领域内一致性很高，揭示了领域特性是影响算法性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有链接预测算法的评估通常基于有限网络，并假设算法在不同领域具有一致的性能排名。然而，不同领域的网络在生成机制和语义背景上存在显著差异，先前研究基于跨领域网络的平均性能来强调"通用最优"算法是不恰当的。

Method: 研究系统评估了12种主流链接预测算法在740个真实世界网络上，这些网络涵盖7个不同领域。使用主成分分析(PCA)分析算法排名向量，并提出Winner Score指标来识别每个领域的最优算法。

Result: 发现算法排名在不同领域间一致性很低，而在同一领域内一致性很高。PCA显示算法排名向量在低维空间中按领域明显聚类。确定了各领域的最优算法：社交网络-非负矩阵分解，经济网络-邻域重叠感知图神经网络，化学网络-图卷积网络，生物网络-基于L3的资源分配算法。

Conclusion: 领域特性是影响链接预测算法性能的关键因素，领域特定的最优算法在其他领域往往表现不佳，这强调了算法机制与网络结构对齐的重要性，否定了存在"通用最优"算法的假设。

Abstract: Link prediction, a foundational task in complex network analysis, has extensive applications in critical scenarios such as social recommendation, drug target discovery, and knowledge graph completion. However, existing evaluations of algorithmic often rely on experiments conducted on a limited number of networks, assuming consistent performance rankings across domains. Despite the significant disparities in generative mechanisms and semantic contexts, previous studies often improperly highlight ``universally optimal" algorithms based solely on naive average over networks across domains. This paper systematically evaluates 12 mainstream link prediction algorithms across 740 real-world networks spanning seven domains. We present substantial empirical evidence elucidating the performance of algorithms in specific domains. This findings reveal a notably low degree of consistency in inter-domain algorithm rankings, a phenomenon that stands in stark contrast to the high degree of consistency observed within individual domains. Principal Component Analysis shows that response vectors formed by the rankings of the 12 algorithms cluster distinctly by domain in low-dimensional space, thus confirming domain attributes as a pivotal factor affecting algorithm performance. We propose a metric called Winner Score that could identify the superior algorithm in each domain: Non-Negative Matrix Factorization for social networks, Neighborhood Overlap-aware Graph Neural Networks for economics, Graph Convolutional Networks for chemistry, and L3-based Resource Allocation for biology. However, these domain-specific top-performing algorithms tend to exhibit suboptimal performance in other domains. This finding underscores the importance of aligning an algorithm's mechanism with the network structure.

</details>
