<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 20]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 11]
- [stat.AP](#stat.AP) [Total: 7]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Latent-IMH: Efficient Bayesian Inference for Inverse Problems with Approximate Operators](https://arxiv.org/abs/2601.20888)
*Youguang Chen,George Biros*

Main category: stat.ML

TL;DR: 提出Latent-IMH采样方法，用于计算昂贵的贝叶斯线性逆问题，通过近似算子离线预计算提升效率


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯线性逆问题中，观测算子A计算成本高，但通常可分解为近似算子Ã。需要开发能利用这种近似结构的高效采样方法，将计算成本转移到离线阶段。

Method: 基于Metropolis-Hastings独立性采样器，先使用近似算子Ã生成中间潜变量，再用精确算子A进行精炼。核心思想是将昂贵计算转移到离线预处理阶段。

Result: 理论分析显示在KL散度和混合时间上有良好性能。数值实验表明，在合理假设下，计算效率优于NUTS等先进方法，某些情况下比现有方案快几个数量级。

Conclusion: Latent-IMH为计算昂贵的贝叶斯逆问题提供了一种高效采样框架，通过利用算子近似结构，显著提升了采样效率，特别适合A可分解的应用场景。

Abstract: We study sampling from posterior distributions in Bayesian linear inverse problems where $A$, the parameters to observables operator, is computationally expensive. In many applications, $A$ can be factored in a manner that facilitates the construction of a cost-effective approximation $\tilde{A}$. In this framework, we introduce Latent-IMH, a sampling method based on the Metropolis-Hastings independence (IMH) sampler. Latent-IMH first generates intermediate latent variables using the approximate $\tilde{A}$, and then refines them using the exact $A$. Its primary benefit is that it shifts the computational cost to an offline phase. We theoretically analyze the performance of Latent-IMH using KL divergence and mixing time bounds. Using numerical experiments on several model problems, we show that, under reasonable assumptions, it outperforms state-of-the-art methods such as the No-U-Turn sampler (NUTS) in computational efficiency. In some cases, Latent-IMH can be orders of magnitude faster than existing schemes.

</details>


### [2] [High-dimensional learning dynamics of multi-pass Stochastic Gradient Descent in multi-index models](https://arxiv.org/abs/2601.21093)
*Zhou Fan,Leda Wang*

Main category: stat.ML

TL;DR: 该论文研究了高维多指数模型中多轮小批量SGD的学习动力学，在样本量n和维度d成比例增长的渐近机制下，对任意亚线性批量大小，给出了SGD坐标动力学的精确特征描述。


<details>
  <summary>Details</summary>
Motivation: 研究高维多指数模型中多轮小批量随机梯度下降（SGD）的学习动力学，特别是在样本量和数据维度成比例增长的渐近机制下，理解不同批量大小和学习率缩放对优化动态的影响。

Method: 在样本量n和维度d成比例增长的渐近机制下，对任意亚线性批量大小κ∝n^α（α∈[0,1)），采用相应的"临界"学习率缩放，通过系统性的动力学平均场方程描述SGD的坐标动力学，该方程由表示SGD采样噪声渐近极限的标量泊松跳跃过程驱动，并开发了随机修正方程（SME）的类似特征描述。

Result: 分析表明：1）对于任意批量大小缩放α∈[0,1)，SGD的极限动力学相同；2）在相应的学习率缩放下，SGD、SME和梯度流的动力学相互不同；3）在线性模型的特殊情况下，SGD和SME的动力学重合；4）在小学习率极限下恢复了梯度流的已知动力学平均场特征；5）在样本量n/d→∞的极限下恢复了一轮/在线SGD的特征。

Conclusion: 该研究为高维多指数模型中的多轮小批量SGD提供了精确的动力学特征描述，揭示了不同优化方法（SGD、SME、梯度流）在渐近机制下的本质差异，并建立了与已知极限情况的联系，为理解随机优化算法的学习动态提供了理论框架。

Abstract: We study the learning dynamics of a multi-pass, mini-batch Stochastic Gradient Descent (SGD) procedure for empirical risk minimization in high-dimensional multi-index models with isotropic random data. In an asymptotic regime where the sample size $n$ and data dimension $d$ increase proportionally, for any sub-linear batch size $κ\asymp n^α$ where $α\in [0,1)$, and for a commensurate ``critical'' scaling of the learning rate, we provide an asymptotically exact characterization of the coordinate-wise dynamics of SGD. This characterization takes the form of a system of dynamical mean-field equations, driven by a scalar Poisson jump process that represents the asymptotic limit of SGD sampling noise. We develop an analogous characterization of the Stochastic Modified Equation (SME) which provides a Gaussian diffusion approximation to SGD.
  Our analyses imply that the limiting dynamics for SGD are the same for any batch size scaling $α\in [0,1)$, and that under a commensurate scaling of the learning rate, dynamics of SGD, SME, and gradient flow are mutually distinct, with those of SGD and SME coinciding in the special case of a linear model. We recover a known dynamical mean-field characterization of gradient flow in a limit of small learning rate, and of one-pass/online SGD in a limit of increasing sample size $n/d \to \infty$.

</details>


### [3] [Efficient Causal Structure Learning via Modular Subgraph Integration](https://arxiv.org/abs/2601.21014)
*Haixiang Sun,Pengchao Tian,Zihan Zhou,Jielei Zhang,Peiyi Li,Andrew L. Liu*

Main category: stat.ML

TL;DR: VISTA是一个模块化框架，通过基于马尔可夫毯分解局部子图，使用加权投票机制整合全局因果结构，提高高维因果发现的计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 高维观测数据中的因果结构学习面临计算复杂度高、搜索空间超指数增长等挑战，现有方法难以应对。

Method: 基于马尔可夫毯将全局因果学习分解为局部子图，通过加权投票机制（指数衰减惩罚低支持边、自适应阈值过滤不可靠边、FAS算法保证无环性）整合全局结构。

Result: 理论证明有限样本误差界和渐近一致性，实验在合成和真实数据集上显示VISTA在准确性和效率上显著优于多种基线方法。

Conclusion: VISTA提供了一个模型无关、兼容任意数据设置、支持并行化的有效框架，显著提升了高维因果结构学习的性能。

Abstract: Learning causal structures from observational data remains a fundamental yet computationally intensive task, particularly in high-dimensional settings where existing methods face challenges such as the super-exponential growth of the search space and increasing computational demands. To address this, we introduce VISTA (Voting-based Integration of Subgraph Topologies for Acyclicity), a modular framework that decomposes the global causal structure learning problem into local subgraphs based on Markov Blankets. The global integration is achieved through a weighted voting mechanism that penalizes low-support edges via exponential decay, filters unreliable ones with an adaptive threshold, and ensures acyclicity using a Feedback Arc Set (FAS) algorithm. The framework is model-agnostic, imposing no assumptions on the inductive biases of base learners, is compatible with arbitrary data settings without requiring specific structural forms, and fully supports parallelization. We also theoretically establish finite-sample error bounds for VISTA, and prove its asymptotic consistency under mild conditions. Extensive experiments on both synthetic and real datasets consistently demonstrate the effectiveness of VISTA, yielding notable improvements in both accuracy and efficiency over a wide range of base learners.

</details>


### [4] [A Diffusive Classification Loss for Learning Energy-based Generative Models](https://arxiv.org/abs/2601.21025)
*Louis Grenioux,RuiKang OuYang,José Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: 论文提出DiffCLF目标函数，通过将EBM学习重构为跨噪声级别的监督分类问题，解决了基于分数的生成模型中EBM训练困难的问题，避免了模式盲区并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 基于分数的生成模型通常参数化为分数函数，但也可以使用时间相关的能量基模型（EBMs）。EBMs不仅可用于生成，还可用于组合采样或通过蒙特卡洛方法构建玻尔兹曼生成器等任务。然而，EBMs训练仍然具有挑战性：直接最大似然计算成本过高，而分数匹配虽然高效但存在模式盲区问题。

Method: 提出Diffusive Classification（DiffCLF）目标函数，将EBM学习重构为跨噪声级别的监督分类问题。该方法可以无缝结合标准的基于分数的目标函数，避免了模式盲区同时保持计算效率。

Result: 在分析性高斯混合案例中，DiffCLF估计的能量与真实值相比表现出色。训练后的模型在模型组合和玻尔兹曼生成器采样等任务中应用有效，显示出比现有方法更高的保真度和更广泛的应用性。

Conclusion: DiffCLF方法成功解决了EBM训练中的关键挑战，提供了一种既避免模式盲区又保持计算效率的解决方案，使EBMs在生成模型和相关任务中具有更高的实用价值。

Abstract: Score-based generative models have recently achieved remarkable success. While they are usually parameterized by the score, an alternative way is to use a series of time-dependent energy-based models (EBMs), where the score is obtained from the negative input-gradient of the energy. Crucially, EBMs can be leveraged not only for generation, but also for tasks such as compositional sampling or building Boltzmann Generators via Monte Carlo methods. However, training EBMs remains challenging. Direct maximum likelihood is computationally prohibitive due to the need for nested sampling, while score matching, though efficient, suffers from mode blindness. To address these issues, we introduce the Diffusive Classification (DiffCLF) objective, a simple method that avoids blindness while remaining computationally efficient. DiffCLF reframes EBM learning as a supervised classification problem across noise levels, and can be seamlessly combined with standard score-based objectives. We validate the effectiveness of DiffCLF by comparing the estimated energies against ground truth in analytical Gaussian mixture cases, and by applying the trained models to tasks such as model composition and Boltzmann Generator sampling. Our results show that DiffCLF enables EBMs with higher fidelity and broader applicability than existing approaches.

</details>


### [5] [A Flexible Empirical Bayes Approach to Generalized Linear Models, with Applications to Sparse Logistic Regression](https://arxiv.org/abs/2601.21217)
*Dongyue Xie,Wanrong Zhu,Matthew Stephens*

Main category: stat.ML

TL;DR: 提出一种灵活的贝叶斯广义线性模型经验贝叶斯方法，采用新型平均场变分推断，在VI算法中估计先验，无需调参，直接优化后验均值和先验参数，减少优化参数数量，支持L-BFGS和SGD等可扩展算法。


<details>
  <summary>Details</summary>
Motivation: 传统变分推断方法通常需要优化后验密度函数，且常假设高斯变分分布，针对不同似然和先验组合需要开发独特的VI方法。本文旨在提出一个统一框架，适用于广泛的指数族分布，自动确定最优后验，无需调参。

Method: 采用新型平均场变分推断方法，在VI算法中估计先验参数，直接优化后验均值和先验参数而非后验密度函数。这种方法减少了优化参数数量，支持L-BFGS和随机梯度下降等可扩展算法，形成适用于指数族分布的统一框架。

Result: 将框架应用于稀疏逻辑回归问题，通过大量数值研究证明该方法在预测性能上优于现有的稀疏逻辑回归方法。

Conclusion: 该方法提供了一个灵活、无需调参的经验贝叶斯框架，适用于广泛的指数族分布，能够自动确定最优后验，在稀疏逻辑回归中表现出优越的预测性能。

Abstract: We introduce a flexible empirical Bayes approach for fitting Bayesian generalized linear models. Specifically, we adopt a novel mean-field variational inference (VI) method and the prior is estimated within the VI algorithm, making the method tuning-free. Unlike traditional VI methods that optimize the posterior density function, our approach directly optimizes the posterior mean and prior parameters. This formulation reduces the number of parameters to optimize and enables the use of scalable algorithms such as L-BFGS and stochastic gradient descent. Furthermore, our method automatically determines the optimal posterior based on the prior and likelihood, distinguishing it from existing VI methods that often assume a Gaussian variational. Our approach represents a unified framework applicable to a wide range of exponential family distributions, removing the need to develop unique VI methods for each combination of likelihood and prior distributions. We apply the framework to solve sparse logistic regression and demonstrate the superior predictive performance of our method in extensive numerical studies, by comparing it to prevalent sparse logistic regression approaches.

</details>


### [6] [Diffusion-based Annealed Boltzmann Generators : benefits, pitfalls and hopes](https://arxiv.org/abs/2601.21026)
*Louis Grenioux,Maxence Noble*

Main category: stat.ML

TL;DR: 研究扩散模型结合退火蒙特卡洛的玻尔兹曼生成器，发现二阶去噪核比一阶表现更好，确定性方法优于随机方法，但学习型扩散模型因密度估计不准确而表现不佳


<details>
  <summary>Details</summary>
Motivation: 传统玻尔兹曼生成器使用重要性采样等方法，需要可处理的似然函数且在复杂目标分布中表现不佳。退火蒙特卡洛通过中间密度序列连接简单参考分布和目标分布，能克服这些限制。扩散模型作为强大生成模型，已通过扩散诱导密度路径整合到退火蒙特卡洛中，成为有前景的骨干模型。

Method: 基于扩散模型的退火蒙特卡洛玻尔兹曼生成器研究：1）在控制的多模态高斯混合模型上进行实证元分析；2）比较完美学习的扩散模型和从数据训练的扩散模型；3）分析一阶和二阶随机去噪核的性能；4）提出基于一阶传输映射的确定性退火蒙特卡洛集成方法。

Result: 1）即使使用完美扩散模型，标准一阶随机去噪核集成方法也系统性失败；2）当协方差信息可用时，二阶去噪核能显著改善性能；3）基于一阶传输映射的确定性方法在更高计算成本下优于随机一阶变体；4）在学习型扩散模型设置中，所有变体都难以产生准确的玻尔兹曼生成器，主要瓶颈在于扩散模型的对数密度估计不准确。

Conclusion: 扩散模型作为退火蒙特卡洛玻尔兹曼生成器的骨干模型具有潜力，但需要改进密度估计方法。二阶去噪核和确定性集成策略能提升性能，但学习型扩散模型的密度估计准确性是关键限制因素。

Abstract: Sampling configurations at thermodynamic equilibrium is a central challenge in statistical physics. Boltzmann Generators (BGs) tackle it by combining a generative model with a Monte Carlo (MC) correction step to obtain asymptotically unbiased samples from an unnormalized target. Most current BGs use classic MC mechanisms such as importance sampling, which both require tractable likelihoods from the backbone model and scale poorly in high-dimensional, multi-modal targets. We study BGs built on annealed Monte Carlo (aMC), which is designed to overcome these limitations by bridging a simple reference to the target through a sequence of intermediate densities. Diffusion models (DMs) are powerful generative models and have already been incorporated into aMC-based recalibration schemes via the diffusion-induced density path, making them appealing backbones for aMC-BGs. We provide an empirical meta-analysis of DM-based aMC-BGs on controlled multi-modal Gaussian mixtures (varying mode separation, number of modes, and dimension), explicitly disentangling inference effects from learning effects by comparing (i) a perfectly learned DM and (ii) a DM trained from data. Even with a perfect DM, standard integrations using only first-order stochastic denoising kernels fail systematically, whereas second-order denoising kernels can substantially improve performance when covariance information is available. We further propose a deterministic aMC integration based on first-order transport maps derived from DMs, which outperforms the stochastic first-order variant at higher computational cost. Finally, in the learned-DM setting, all DM-aMC variants struggle to produce accurate BGs; we trace the main bottleneck to inaccurate DM log-density estimation.

</details>


### [7] [Diffusion Path Samplers via Sequential Monte Carlo](https://arxiv.org/abs/2601.21951)
*James Matthew Young,Paula Cordero-Encinar,Sebastian Reich,Andrew Duncan,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: 提出一种基于扩散的采样器，用于从已知归一化常数的目标分布中采样，通过扩散退火Langevin Monte Carlo和顺序蒙特卡洛方法实现高效采样


<details>
  <summary>Details</summary>
Motivation: 需要从已知归一化常数的目标分布中采样，但传统方法效率有限。扩散模型中的扩散路径可以平滑地在简单基分布和目标分布之间插值，这为采样提供了新思路

Method: 1) 实现扩散退火Langevin Monte Carlo，近似扩散路径并保证收敛；2) 开发顺序蒙特卡洛采样器，沿路径演化辅助变量以估计时变分布的分数；3) 设计新的控制变量调度来最小化分数估计的方差

Result: 方法在多个合成和真实数据集上表现出有效性，提供了理论保证，并通过控制变量调度显著降低了分数估计的方差

Conclusion: 提出的基于扩散的采样器能够高效地从已知归一化常数的目标分布中采样，结合扩散路径、顺序蒙特卡洛和控制变量调度，为复杂分布的采样提供了有效解决方案

Abstract: We develop a diffusion-based sampler for target distributions known up to a normalising constant. To this end, we rely on the well-known diffusion path that smoothly interpolates between a (simple) base distribution and the target distribution, widely used in diffusion models. Our approach is based on a practical implementation of diffusion-annealed Langevin Monte Carlo, which approximates the diffusion path with convergence guarantees. We tackle the score estimation problem by developing an efficient sequential Monte Carlo sampler that evolves auxiliary variables from conditional distributions along the path, which provides principled score estimates for time-varying distributions. We further develop novel control variate schedules that minimise the variance of these score estimates. Finally, we provide theoretical guarantees and empirically demonstrate the effectiveness of our method on several synthetic and real-world datasets.

</details>


### [8] [An efficient, accurate, and interpretable machine learning method for computing probability of failure](https://arxiv.org/abs/2601.21089)
*Jacob Zhu,Donald Estep*

Main category: stat.ML

TL;DR: 提出基于Gabriel编辑集的惩罚轮廓支持向量机方法，用于计算复杂系统失效概率，通过自适应采样策略最小化计算机模型评估次数，同时保持决策边界几何结构。


<details>
  <summary>Details</summary>
Motivation: 在计算复杂系统失效概率时，需要大量评估计算机模型，计算成本高昂。现有方法要么需要过多模型评估，要么不能准确保持决定失效的决策边界几何结构。

Method: 基于Gabriel编辑集的惩罚轮廓支持向量机方法，采用自适应采样策略，战略性地在失效边界附近分配采样点，通过训练点的战略聚类构建局部线性代理边界，保持边界几何一致性。

Result: 证明了两个收敛结果，在四个测试问题上与多种先进分类方法比较表现优异，并成功应用于Lotka-Volterra竞争物种模型的生存概率计算。

Conclusion: 该方法能有效最小化计算机模型评估次数，同时准确保持决策边界几何结构，为复杂系统可靠性分析提供高效工具。

Abstract: We introduce a novel machine learning method called the Penalized Profile Support Vector Machine based on the Gabriel edited set for the computation of the probability of failure for a complex system as determined by a threshold condition on a computer model of system behavior. The method is designed to minimize the number of evaluations of the computer model while preserving the geometry of the decision boundary that determines the probability. It employs an adaptive sampling strategy designed to strategically allocate points near the boundary determining failure and builds a locally linear surrogate boundary that remains consistent with its geometry by strategic clustering of training points. We prove two convergence results and we compare the performance of the method against a number of state of the art classification methods on four test problems. We also apply the method to determine the probability of survival using the Lotka--Volterra model for competing species.

</details>


### [9] [Efficient Stochastic Optimisation via Sequential Monte Carlo](https://arxiv.org/abs/2601.22003)
*James Cuin,Davide Carbone,Yanbo Tang,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: 提出使用序列蒙特卡洛采样器优化不可微函数，替代传统需要内层采样的方法，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 机器学习中经常遇到梯度难以计算或不可得的函数优化问题（如最大边际似然估计、生成模型微调）。传统随机近似方法需要内层采样循环来获取有偏的随机梯度估计，计算成本很高。

Method: 开发序列蒙特卡洛采样器来优化梯度不可得的函数，用高效的SMC近似替代昂贵的内层采样方法。

Result: 建立了该方法所定义的基本递归的收敛性结果，并在各种设置下展示了基于能量的模型的奖励调优应用中的有效性。

Conclusion: SMC采样器为梯度不可得函数的优化提供了有效的计算框架，能够显著降低计算成本，在能量模型调优等应用中表现良好。

Abstract: The problem of optimising functions with intractable gradients frequently arise in machine learning and statistics, ranging from maximum marginal likelihood estimation procedures to fine-tuning of generative models. Stochastic approximation methods for this class of problems typically require inner sampling loops to obtain (biased) stochastic gradient estimates, which rapidly becomes computationally expensive. In this work, we develop sequential Monte Carlo (SMC) samplers for optimisation of functions with intractable gradients. Our approach replaces expensive inner sampling methods with efficient SMC approximations, which can result in significant computational gains. We establish convergence results for the basic recursions defined by our methodology which SMC samplers approximate. We demonstrate the effectiveness of our approach on the reward-tuning of energy-based models within various settings.

</details>


### [10] [Multilevel and Sequential Monte Carlo for Training-Free Diffusion Guidance](https://arxiv.org/abs/2601.21104)
*Aidan Gleich,Scott C. Schmidler*

Main category: stat.ML

TL;DR: 提出基于序贯蒙特卡洛(SMC)的训练免费引导方法，用于扩散模型的条件生成，通过蒙特卡洛近似整合完整去噪分布，实现无偏后验估计并捕捉多模态性


<details>
  <summary>Details</summary>
Motivation: 现有训练免费引导方法通常依赖点估计来近似后验分数，导致有偏近似且无法捕捉扩散模型反向过程固有的多模态性

Method: 提出序贯蒙特卡洛(SMC)框架，通过蒙特卡洛近似整合完整去噪分布来构建p_θ(y|x_t)的无偏估计器；为保持计算可行性，引入基于多级蒙特卡洛(MLMC)的方差缩减方案

Result: 在CIFAR-10类别条件生成上达到95.6%准确率，比基线方法降低3倍成本成功率；在ImageNet上比现有方法获得1.5倍成本成功率优势

Conclusion: SMC框架为扩散模型训练免费引导提供了准确、无偏的解决方案，能有效捕捉多模态性并在多个数据集上实现最先进性能

Abstract: We address the problem of accurate, training-free guidance for conditional generation in trained diffusion models. Existing methods typically rely on point-estimates to approximate the posterior score, often resulting in biased approximations that fail to capture multimodality inherent to the reverse process of diffusion models. We propose a sequential Monte Carlo (SMC) framework that constructs an unbiased estimator of $p_θ(y|x_t)$ by integrating over the full denoising distribution via Monte Carlo approximation. To ensure computational tractability, we incorporate variance-reduction schemes based on Multi-Level Monte Carlo (MLMC). Our approach achieves new state-of-the-art results for training-free guidance on CIFAR-10 class-conditional generation, achieving $95.6\%$ accuracy with $3\times$ lower cost-per-success than baselines. On ImageNet, our algorithm achieves $1.5\times$ cost-per-success advantage over existing methods.

</details>


### [11] [Provably Reliable Classifier Guidance through Cross-entropy Error Control](https://arxiv.org/abs/2601.21200)
*Sharan Sahu,Arisina Banerjee,Yuchen Wu*

Main category: stat.ML

TL;DR: 论文证明了在温和平滑性假设下，分类器的交叉熵误差控制可以保证扩散引导向量的误差控制，建立了分类器训练与引导对齐之间的定量联系。


<details>
  <summary>Details</summary>
Motivation: 虽然分类器引导扩散模型通过分类器梯度生成条件样本，但标准分类器训练程序是否能产生有效的扩散引导尚不明确。本文旨在填补这一理论空白。

Method: 在分类器满足平滑性假设的条件下，证明控制每个扩散步骤的交叉熵误差可以控制引导向量的误差。通过构造反例说明平滑性假设的必要性。

Result: 分类器达到条件KL散度ε²时，引导向量的均方误差为Õ(dε)。建立了采样误差的上界，类似于反向log-Sobolev型不等式。反例表明没有平滑性假设时，引导向量控制可能对几乎所有分布都失败。

Conclusion: 首次建立了分类器训练与引导对齐之间的定量联系，为分类器引导提供了理论基础，并为分类器选择提供了原则性指导。

Abstract: Classifier-guided diffusion models generate conditional samples by augmenting the reverse-time score with the gradient of a learned classifier, yet it remains unclear whether standard classifier training procedures yield effective diffusion guidance. We address this gap by showing that, under mild smoothness assumptions on the classifiers, controlling the cross-entropy error at each diffusion step also controls the error of the resulting guidance vectors: classifiers achieving conditional KL divergence $\varepsilon^2$ from the ground-truth conditional label probabilities induce guidance vectors with mean squared error $\widetilde{O}(d \varepsilon )$. Our result yields an upper bound on the sampling error under classifier guidance and bears resemblance to a reverse log-Sobolev-type inequality. Moreover, we show that the classifier smoothness assumption is essential, by constructing simple counterexamples demonstrating that, without it, control of the guidance vector can fail for almost all distributions. To our knowledge, our work establishes the first quantitative link between classifier training and guidance alignment, yielding both a theoretical foundation for classifier guidance and principled guidelines for classifier selection.

</details>


### [12] [Bulk-Calibrated Credal Ambiguity Sets: Fast, Tractable Decision Making under Out-of-Sample Contamination](https://arxiv.org/abs/2601.21324)
*Mengqi Chen,Thomas B. Berrett,Theodoros Damoulas,Michele Caprio*

Main category: stat.ML

TL;DR: 提出bulk-calibrated credal模糊集，通过分离处理高概率主体区域和尾部，解决传统DRO在Huber污染模型下可能无限风险的问题，得到闭式有限风险目标。


<details>
  <summary>Details</summary>
Motivation: 传统分布鲁棒优化(DRO)在包含Huber污染模型时，最坏情况风险可能无限大，导致目标无意义，除非施加强有界性或支撑集假设。需要解决这一挑战。

Method: 提出bulk-calibrated credal模糊集：从数据中学习高概率主体区域，考虑主体区域内的污染，同时单独限制尾部贡献。这导致闭式的有限"均值+上确界"鲁棒目标，并为常见损失和主体几何形状得到可处理的线性或二阶锥规划。

Result: 通过该框架，展示了不精确概率的上期望与最坏情况风险的等价性，将IP credal集转化为具有可解释容忍水平的DRO目标。在重尾库存控制、地理偏移房价回归和人口偏移文本分类实验中显示出有竞争力的鲁棒性-准确性权衡和高效优化时间。

Conclusion: 提出的bulk-calibrated credal模糊集解决了传统DRO在Huber污染模型下的无限风险问题，提供闭式有限风险目标，在多种实际应用中表现出良好的鲁棒性-准确性权衡和计算效率。

Abstract: Distributionally robust optimisation (DRO) minimises the worst-case expected loss over an ambiguity set that can capture distributional shifts in out-of-sample environments. While Huber (linear-vacuous) contamination is a classical minimal-assumption model for an $\varepsilon$-fraction of arbitrary perturbations, including it in an ambiguity set can make the worst-case risk infinite and the DRO objective vacuous unless one imposes strong boundedness or support assumptions. We address these challenges by introducing bulk-calibrated credal ambiguity sets: we learn a high-mass bulk set from data while considering contamination inside the bulk and bounding the remaining tail contribution separately. This leads to a closed-form, finite $\mathrm{mean}+\sup$ robust objective and tractable linear or second-order cone programs for common losses and bulk geometries. Through this framework, we highlight and exploit the equivalence between the imprecise probability (IP) notion of upper expectation and the worst-case risk, demonstrating how IP credal sets translate into DRO objectives with interpretable tolerance levels. Experiments on heavy-tailed inventory control, geographically shifted house-price regression, and demographically shifted text classification show competitive robustness-accuracy trade-offs and efficient optimisation times, using Bayesian, frequentist, or empirical reference distributions.

</details>


### [13] [Statsformer: Validated Ensemble Learning with LLM-Derived Semantic Priors](https://arxiv.org/abs/2601.21410)
*Erica Zhang,Naomi Sagan,Danny Tse,Fangzhao Zhang,Mert Pilanci,Jose Blanchet*

Main category: stat.ML

TL;DR: Statsformer是一个将大语言模型知识整合到监督统计学习的框架，通过保护性集成架构自适应校准LLM先验的影响，保证性能不低于基础学习器的凸组合。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将LLM指导作为未经验证的启发式方法（容易受幻觉影响），要么将语义信息嵌入到单一固定学习器中，存在适应性和范围限制的问题。

Method: 采用保护性集成架构，将LLM衍生的特征先验嵌入到线性和非线性学习器的集成中，通过交叉验证自适应校准其影响。

Result: 信息性先验带来一致的性能提升，而无信息或错误指定的LLM指导会被自动降权，减轻了幻觉对多样化预测任务的影响。

Conclusion: Statsformer提供了一个灵活系统，具有oracle式保证，性能不低于库中基础学习器的任何凸组合，有效整合LLM知识同时减轻幻觉风险。

Abstract: We introduce Statsformer, a principled framework for integrating large language model (LLM)-derived knowledge into supervised statistical learning. Existing approaches are limited in adaptability and scope: they either inject LLM guidance as an unvalidated heuristic, which is sensitive to LLM hallucination, or embed semantic information within a single fixed learner. Statsformer overcomes both limitations through a guardrailed ensemble architecture. We embed LLM-derived feature priors within an ensemble of linear and nonlinear learners, adaptively calibrating their influence via cross-validation. This design yields a flexible system with an oracle-style guarantee that it performs no worse than any convex combination of its in-library base learners, up to statistical error. Empirically, informative priors yield consistent performance improvements, while uninformative or misspecified LLM guidance is automatically downweighted, mitigating the impact of hallucinations across a diverse range of prediction tasks.

</details>


### [14] [Questioning the Coverage-Length Metric in Conformal Prediction: When Shorter Intervals Are Not Better](https://arxiv.org/abs/2601.21455)
*Yizhou Min,Yizhou Lu,Lanqi Li,Zhen Zhang,Jiaye Teng*

Main category: stat.ML

TL;DR: 本文批判性审视了传统共形预测评估指标（覆盖率和区间长度）的不足，揭示了一种称为"偏见技巧"的欺骗性方法，该方法能在保持覆盖率的同时虚假地降低区间长度，但会导致区间不稳定。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测评估主要关注覆盖率和区间长度，但作者发现这些指标可能存在不足。他们想要揭示一种潜在的欺骗性方法，该方法能在保持统计覆盖有效性的同时，通过技术手段虚假地改善区间长度指标。

Method: 提出了"偏见技巧"方法：对于每个测试样本，该方法以一定概率返回空区间，或以调整后的置信水平构建区间，从而在保持边际覆盖率的同时，可能虚假地降低平均区间长度。作者形式化推导了PT实现这种欺骗性改进的条件，并在多种回归和分类任务上进行了实证验证。

Result: 研究表明PT方法确实能在保持覆盖率的同时虚假地降低区间长度，但引入了严重的实践漏洞：同一输入在不同次算法运行中可能产生完全不同的预测区间。作者还提出了新的评估指标"区间稳定性"，用于检测新CP方法是否隐含地使用了类似PT的技巧。

Conclusion: 传统共形预测评估指标（覆盖率和区间长度）不足以全面评估方法质量，需要引入新的评估维度如区间稳定性。研究揭示了现有评估框架的局限性，并为更全面的共形预测评估提供了新视角。

Abstract: Conformal prediction (CP) has become a cornerstone of distribution-free uncertainty quantification, conventionally evaluated by its coverage and interval length. This work critically examines the sufficiency of these standard metrics. We demonstrate that the interval length might be deceptively improved through a counter-intuitive approach termed Prejudicial Trick (PT), while the coverage remains valid. Specifically, for any given test sample, PT probabilistically returns an interval, which is either null or constructed using an adjusted confidence level, thereby preserving marginal coverage. While PT potentially yields a deceptively lower interval length, it introduces practical vulnerabilities: the same input can yield completely different prediction intervals across repeated runs of the algorithm. We formally derive the conditions under which PT achieves these misleading improvements and provides extensive empirical evidence across various regression and classification tasks. Furthermore, we introduce a new metric interval stability which helps detect whether a new CP method implicitly improves the length based on such PT-like techniques.

</details>


### [15] [A Decomposable Forward Process in Diffusion Models for Time-Series Forecasting](https://arxiv.org/abs/2601.21812)
*Francisco Caldas,Sahil Kumar,Cláudia Soares*

Main category: stat.ML

TL;DR: 提出一种模型无关的前向扩散过程，通过谱分解将时间序列信号分解为频谱分量，比标准扩散方法更有效地保留季节性和结构化时序模式。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时间序列预测中难以有效保留长期模式（如季节性），标准扩散过程会破坏信号的结构化特征，导致预测质量下降。

Method: 提出基于谱分解的前向扩散过程，将信号分解为频谱分量（使用傅里叶或小波变换），根据分量能量分阶段注入噪声，使主导频率在整个扩散轨迹中保持高信噪比。

Result: 在标准预测基准测试中，该方法相比基线前向扩散过程持续改进扩散模型的性能，计算开销可忽略不计，且与现有扩散主干网络（如DiffWave、TimeGrad、CSDI）兼容。

Conclusion: 通过谱分解策略调整扩散过程本身（而非网络架构），能够更有效地保留时间序列中的结构化模式，显著提升预测质量，为扩散模型在时序预测中的应用提供了新思路。

Abstract: We introduce a model-agnostic forward diffusion process for time-series forecasting that decomposes signals into spectral components, preserving structured temporal patterns such as seasonality more effectively than standard diffusion. Unlike prior work that modifies the network architecture or diffuses directly in the frequency domain, our proposed method alters only the diffusion process itself, making it compatible with existing diffusion backbones (e.g., DiffWave, TimeGrad, CSDI). By staging noise injection according to component energy, it maintains high signal-to-noise ratios for dominant frequencies throughout the diffusion trajectory, thereby improving the recoverability of long-term patterns. This strategy enables the model to maintain the signal structure for a longer period in the forward process, leading to improved forecast quality. Across standard forecasting benchmarks, we show that applying spectral decomposition strategies, such as the Fourier or Wavelet transform, consistently improves upon diffusion models using the baseline forward process, with negligible computational overhead. The code for this paper is available at https://anonymous.4open.science/r/D-FDP-4A29.

</details>


### [16] [A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth](https://arxiv.org/abs/2601.21817)
*Mingyuan Xu,Xinzi Tan,Jiawei Wu,Doudou Zhou*

Main category: stat.ML

TL;DR: 提出一种考虑评委可靠性的LLM评估框架，通过引入评委特异性参数改进Bradley-Terry-Luce模型，实现更准确的模型排名和不确定性量化


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-judge评估范式存在严重问题：不同评委LLM的可靠性差异很大，但现有方法将所有评委同等对待，导致排名偏差和误导性的不确定性估计

Method: 扩展Bradley-Terry-Luce模型，引入评委特异性判别参数，通过最大似然估计联合推断模型质量和评委可靠性，无需参考标签

Result: 在多个公开基准和新收集数据集上，该方法提高了与人类偏好的一致性，比未加权基线具有更高的数据效率，并能产生校准的不确定性量化

Conclusion: 提出的评委感知排名框架能有效解决LLM评估中的评委可靠性问题，提供更准确、高效的模型排名和可靠的不确定性量化

Abstract: Evaluating large language models (LLMs) on open-ended tasks without ground-truth labels is increasingly done via the LLM-as-a-judge paradigm. A critical but under-modeled issue is that judge LLMs differ substantially in reliability; treating all judges equally can yield biased leaderboards and misleading uncertainty estimates. More data can make evaluation more confidently wrong under misspecified aggregation. We propose a judge-aware ranking framework that extends the Bradley-Terry-Luce model by introducing judge-specific discrimination parameters, jointly estimating latent model quality and judge reliability from pairwise comparisons without reference labels. We establish identifiability up to natural normalizations and prove consistency and asymptotic normality of the maximum likelihood estimator, enabling confidence intervals for score differences and rank comparisons. Across multiple public benchmarks and a newly collected dataset, our method improves agreement with human preferences, achieves higher data efficiency than unweighted baselines, and produces calibrated uncertainty quantification for LLM rankings.

</details>


### [17] [Generative Modeling of Discrete Data Using Geometric Latent Subspaces](https://arxiv.org/abs/2601.21831)
*Daniel Gonzalez-Alvarado,Jonas Cassel,Stefania Petra,Christoph Schnörr*

Main category: stat.ML

TL;DR: 该论文提出在分类分布乘积流形的指数参数空间中使用潜在子空间，作为学习离散数据生成模型的工具。低维潜在空间编码统计依赖关系并移除分类变量间的冗余自由度。


<details>
  <summary>Details</summary>
Motivation: 学习离散数据的生成模型面临挑战，因为分类变量之间存在复杂的统计依赖关系和冗余自由度。需要一种能够有效捕捉这些依赖关系并降低维度的框架。

Method: 在分类分布乘积流形的指数参数空间中引入潜在子空间，为参数域配备黎曼几何结构，使得空间和距离通过等距映射相关，从而实现一致的流匹配。特别地，测地线变为直线，使流匹配训练更有效。

Result: 实证结果表明，减少的潜在维度足以表示生成建模所需的数据，验证了该方法的有效性。

Conclusion: 通过在分类分布乘积流形的指数参数空间中使用潜在子空间，结合黎曼几何结构，可以有效地学习离散数据的生成模型，降低维度同时保持数据表示能力。

Abstract: We introduce the use of latent subspaces in the exponential parameter space of product manifolds of categorial distributions, as a tool for learning generative models of discrete data. The low-dimensional latent space encodes statistical dependencies and removes redundant degrees of freedom among the categorial variables. We equip the parameter domain with a Riemannian geometry such that the spaces and distances are related by isometries which enables consistent flow matching. In particular, geodesics become straight lines which makes model training by flow matching effective. Empirical results demonstrate that reduced latent dimensions suffice to represent data for generative modeling.

</details>


### [18] [On Forgetting and Stability of Score-based Generative models](https://arxiv.org/abs/2601.21868)
*Stanislas Strasman,Gabriel Cardoso,Sylvain Le Corff,Vincent Lemaire,Antonio Ocello*

Main category: stat.ML

TL;DR: 该论文通过分析反向扩散过程的马尔可夫链稳定性，为基于分数的生成模型提供了采样误差的定量界限，揭示了随机动力学在采样过程中的收缩机制。


<details>
  <summary>Details</summary>
Motivation: 理解生成模型的稳定性和长期行为是现代机器学习中的基本问题。基于分数的生成模型虽然取得了显著成功，但其采样误差的定量分析仍不充分，需要建立理论框架来确保采样过程的稳定性。

Method: 通过分析反向时间动力学相关的马尔可夫链的稳定性和遗忘性质，利用两个结构特性：Lyapunov漂移条件和Doeblin型小化条件，来确保反向过程初始化误差和离散化误差的传播控制。

Result: 在弱假设下，证明了反向扩散动力学在采样轨迹上诱导了收缩机制，从而实现了采样过程的定量稳定性，为基于分数的生成模型提供了采样误差的定量界限。

Conclusion: 该研究阐明了随机动力学在基于分数的模型中的作用，并为分析此类方法中误差传播提供了原则性框架，有助于理解和改进生成模型的采样稳定性。

Abstract: Understanding the stability and long-time behavior of generative models is a fundamental problem in modern machine learning. This paper provides quantitative bounds on the sampling error of score-based generative models by leveraging stability and forgetting properties of the Markov chain associated with the reverse-time dynamics. Under weak assumptions, we provide the two structural properties to ensure the propagation of initialization and discretization errors of the backward process: a Lyapunov drift condition and a Doeblin-type minorization condition. A practical consequence is quantitative stability of the sampling procedure, as the reverse diffusion dynamics induces a contraction mechanism along the sampling trajectory. Our results clarify the role of stochastic dynamics in score-based models and provide a principled framework for analyzing propagation of errors in such approaches.

</details>


### [19] [Clustering in Deep Stochastic Transformers](https://arxiv.org/abs/2601.21942)
*Lev Fedorov,Michaël E. Sander,Romuald Elie,Pierre Marion,Mathieu Laurière*

Main category: stat.ML

TL;DR: 随机初始化噪声改变了深度Transformer的token动态，防止了确定性模型预测的token坍缩到单点，揭示了相变现象


<details>
  <summary>Details</summary>
Motivation: 现有深度Transformer理论通常基于确定性权重假设，预测token会聚集到单点，但这无法捕捉标准随机初始化方案的实际行为。需要研究随机初始化噪声如何影响token动态。

Method: 分析深度Transformer中值矩阵随机初始化产生的噪声，在扩散缩放和token-wise RMS归一化条件下，证明当Transformer层数趋于无穷时，离散token动态收敛到球面上的交互粒子系统，其中token由共同的矩阵值布朗噪声驱动。

Result: 随机初始化噪声防止了确定性模型预测的token坍缩到单点。对于两个token，证明了由交互强度和token维度控制的相变：与确定性注意力流不同，反极点配置以正概率成为吸引子。数值实验证实了预测的相变，显示反极点形成在多于两个token时持续存在，并且抑制固有噪声会降低准确性。

Conclusion: 随机初始化噪声是深度Transformer动态的关键因素，它改变了token的聚集行为，揭示了相变现象，并且这种噪声对模型性能有积极影响，抑制噪声会降低准确性。

Abstract: Transformers have revolutionized deep learning across various domains but understanding the precise token dynamics remains a theoretical challenge. Existing theories of deep Transformers with layer normalization typically predict that tokens cluster to a single point; however, these results rely on deterministic weight assumptions, which fail to capture the standard initialization scheme in Transformers. In this work, we show that accounting for the intrinsic stochasticity of random initialization alters this picture. More precisely, we analyze deep Transformers where noise arises from the random initialization of value matrices. Under diffusion scaling and token-wise RMS normalization, we prove that, as the number of Transformer layers goes to infinity, the discrete token dynamics converge to an interacting-particle system on the sphere where tokens are driven by a \emph{common} matrix-valued Brownian noise. In this limit, we show that initialization noise prevents the collapse to a single cluster predicted by deterministic models. For two tokens, we prove a phase transition governed by the interaction strength and the token dimension: unlike deterministic attention flows, antipodal configurations become attracting with positive probability. Numerical experiments confirm the predicted transition, reveal that antipodal formations persist for more than two tokens, and demonstrate that suppressing the intrinsic noise degrades accuracy.

</details>


### [20] [Near-Optimal Private Tests for Simple and MLR Hypotheses](https://arxiv.org/abs/2601.21959)
*Yu-Wei Chen,Raghu Pasupathy,Jordan Awan*

Main category: stat.ML

TL;DR: 提出了一种基于高斯差分隐私框架的近最优检验方法，在单调似然比条件下实现单边和双边检验，其私有检验统计量能达到与非私有最优检验相同的渐近相对效率。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私框架下进行统计检验时，需要在保护数据隐私的同时保持检验的统计效率。现有方法在平衡隐私保护与检验功效方面存在不足，特别是在小样本和中等隐私预算下表现不佳。

Method: 基于具有数据驱动截断界限的私有均值估计器构建私有检验统计量。该估计器的总体风险与私有极小极大率匹配（对数因子内），在单调似然比条件下构造单边和双边检验。

Result: 理论证明私有检验能达到与非私有最优检验相同的渐近相对效率，同时保守控制第一类错误。数值实验显示该方法优于现有DP方法，在中等小样本和隐私预算下能与非私有最优检验的功效相当。

Conclusion: 该方法在差分隐私框架下实现了近最优的统计检验，在保护数据隐私的同时保持了高统计效率，为隐私保护下的假设检验提供了有效的解决方案。

Abstract: We develop a near-optimal testing procedure under the framework of Gaussian differential privacy for simple as well as one- and two-sided tests under monotone likelihood ratio conditions. Our mechanism is based on a private mean estimator with data-driven clamping bounds, whose population risk matches the private minimax rate up to logarithmic factors. Using this estimator, we construct private test statistics that achieve the same asymptotic relative efficiency as the non-private, most powerful tests while maintaining conservative type I error control. In addition to our theoretical results, our numerical experiments show that our private tests outperform competing DP methods and offer comparable power to the non-private most powerful tests, even at moderately small sample sizes and privacy loss budgets.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [21] [bigMICE: Multiple Imputation of Big Data](https://arxiv.org/abs/2601.21613)
*Hugo Morvan,Jonas Agholme,Bjorn Eliasson,Katarina Olofsson,Ludger Grote,Fredrik Iredahl,Oleg Sysoev*

Main category: stat.CO

TL;DR: 开发了bigMICE包，基于Apache Spark MLLib和Spark ML，将MICE方法扩展到大数据场景，解决了传统MICE在处理大数据时的计算时间和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 缺失数据是许多应用中的普遍问题，特别是在大型医疗注册系统（如瑞典医疗质量注册表）中，如果处理不当可能导致分析结果有偏或效率低下。传统的MICE方法在处理大数据集时面临计算时间和内存限制的挑战。

Method: 开发了bigMICE包，基于Apache Spark MLLib和Spark ML框架，将MICE方法扩展到大数据环境。该实现允许控制最大内存使用量，使得在内存有限的硬件（如普通笔记本电脑）上也能处理非常大的数据集。

Result: 在大型瑞典医疗注册表上测试显示，bigMICE在内存使用效率和运行速度上优于常用的MICE实现。同时证明，即使变量有大量缺失数据，使用非常大的数据集也能获得高质量的插补结果。

Conclusion: bigMICE方法在处理大数据集时比传统MICE实现更内存高效且更快。使用大数据集可以获得高质量的插补结果，即使变量有高比例的缺失数据。论文还提供了安装和使用该开源包的指南和建议。

Abstract: Missing data is a prevalent issue in many applications, including large medical registries such as the Swedish Healthcare Quality Registries, potentially leading to biased or inefficient analyses if not handled properly. Multiple Imputation by Chained Equations (MICE) is a popular and versatile method for handling multivariate missing data but traditional implementations face significant challenges when applied to big data sets due to computational time and memory limitations.
  To address this, the bigMICE package was developed, adapting the MICE framework to big data using Apache Spark MLLib and Spark ML. Our implementation allows for controlling the maximum memory usage during the execution, enabling processing of very large data sets on a hardware with a limited memory, such as ordinary laptops.
  The developed package was tested on a large Swedish medical registry to measure memory usage, runtime and dependence of the imputation quality on sample size and on missingness proportion in the data. In conclusion, our method is generally more memory efficient and faster on large data sets compared to a commonly used MICE implementation. We also demonstrate that working with very large datasets can result in high quality imputations even when a variable has a large proportion of missing data. This paper also provides guidelines and recommendations on how to install and use our open source package.

</details>


### [22] [Mean-field Variational Bayes for Sparse Probit Regression](https://arxiv.org/abs/2601.21765)
*Augusto Fasano,Giovanni Rebaudo*

Main category: stat.CO

TL;DR: 提出了一种用于二元结果变量选择的变分贝叶斯方法，使用probit连接函数和spike-and-slab先验，相比MCMC大幅提升计算效率


<details>
  <summary>Details</summary>
Motivation: 在高维情况下，马尔可夫链蒙特卡洛（MCMC）采样器面临计算挑战，需要开发更高效的贝叶斯变量选择方法

Method: 使用均值场变分贝叶斯近似，所有变分因子都有闭式更新，证据下界可闭式计算，采用坐标上升变分推断算法优化变分参数

Result: 方法能成功识别重要变量，比MCMC快几个数量级，同时保持相当的准确性，在模拟和真实数据中都表现良好

Conclusion: 该方法为二元结果的贝叶斯变量选择提供了一个高效、可解释的框架，在计算效率和准确性之间取得了良好平衡

Abstract: We consider Bayesian variable selection for binary outcomes under a probit link with a spike-and-slab prior on the regression coefficients. Motivated by the computational challenges encountered by Markov chain Monte Carlo (MCMC) samplers in high-dimensional regimes, we develop a mean-field variational Bayes approximation in which all variational factors admit closed-form updates, and the evidence lower bound is available in closed form. This, in turn, allows the development of an efficient coordinate ascent variational inference algorithm to find the optimal values of the variational parameters. The approach produces posterior inclusion probabilities and parameter estimates, enabling interpretable selection and prediction within a single framework. As shown in both simulated and real data applications, the proposed method successfully identifies the important variables and is orders of magnitude faster than MCMC, while maintaining comparable accuracy.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [23] [Experimental Design for Matching](https://arxiv.org/abs/2601.21036)
*Chonghuan Wang*

Main category: stat.ME

TL;DR: 提出一种用于比较两种匹配机制的随机实验设计方法，通过分解分歧集为交替路径和环，采用顺序随机化控制干扰，并扩展到多对一匹配场景。


<details>
  <summary>Details</summary>
Motivation: 在运营管理中比较新旧匹配算法面临根本性挑战，因为匹配干扰使得一个匹配中的分配会排除另一个匹配中的分配。需要设计随机实验来比较两种预定的匹配方案，而不依赖结果或行为模型。

Method: 引入分歧集概念，将其分解为不相交的交替路径和环。提出交替路径随机设计，沿这些路径和环顺序随机化以有效管理干扰。在极小化极大框架下优化条件随机化概率，并扩展到多对一匹配，使用增广路径和欧拉回路分解等图论工具。

Result: 证明了Horvitz-Thompson估计量的无偏性，推导了有限总体中心极限定理，能处理复杂不稳定的路径环结构。对于长路径，最优随机化概率收敛于√2-1，最小化最坏情况方差。方法可扩展到多对一匹配场景。

Conclusion: 该研究为比较匹配机制提供了基于设计的随机实验框架，通过图论分解有效处理匹配干扰，建立了理论保证并扩展到更一般的匹配场景，为运营管理中的匹配算法评估提供了实用方法。

Abstract: Matching mechanisms play a central role in operations management across diverse fields including education, healthcare, and online platforms. However, experimentally comparing a new matching algorithm against a status quo presents some fundamental challenges due to matching interference, where assigning a unit in one matching may preclude its assignment in the other. In this work, we take a design-based perspective to study the design of randomized experiments to compare two predetermined matching plans on a finite population, without imposing outcome or behavioral models. We introduce the notation of a disagreement set, which captures the difference between the two matching plans, and show that it admits a unique decomposition into disjoint alternating paths and cycles with useful structural properties. Based on these properties, we propose the Alternating Path Randomized Design, which sequentially randomizes along these paths and cycles to effectively manage interference. Within a minimax framework, we optimize the conditional randomization probability and show that, for long paths, the optimal choice converges to $\sqrt{2}-1$, minimizing worst-case variance. We establish the unbiasedness of the Horvitz-Thompson estimator and derive a finite-population Central Limit Theorem that accommodates complex and unstable path and cycle structures as the population grows. Furthermore, we extend the design to many-to-one matchings, where capacity constraints fundamentally alter the structure of the disagreement set. Using graph-theoretic tools, including finding augmenting paths and Euler-tour decomposition on an auxiliary unbalanced directed graph, we construct feasible alternating path and cycle decompositions that allow the design and inference results to carry over.

</details>


### [24] [Adaptive Dirichlet Process mixture model with unknown concentration parameter and variance: Scaling high dimensional clustering via collapsed variational inference](https://arxiv.org/abs/2601.21106)
*Annesh Pal,Aguirre Mimoun,Rodolphe Thiébaut,Boris P. Hejblum*

Main category: stat.ME

TL;DR: 提出一种基于DPMM的自适应聚类方法，使用折叠变分推断，并纳入DP浓度参数α和基分布G0的弱信息先验，在多种数据条件下表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高维数据时收敛速度慢，且对先验选择和协方差结构敏感，需要更高效稳健的聚类方法

Method: 使用折叠变分推断进行DPMM自适应聚类，引入DP浓度参数α和基分布G0的弱信息先验，考虑数据协方差矩阵的不同参数化

Result: 在高斯模拟中收敛速度显著优于最先进的MCMC拼接采样器；在负二项模拟中表现良好；在白血病转录组数据中成功识别所有已知亚型并发现新的生物相关亚簇

Conclusion: 该方法在多种数据条件下表现稳健高效，能够有效识别复杂生物数据中的亚型结构，具有重要的生物学应用价值

Abstract: We propose a novel method that performs adaptive clustering with DPMM using collapsed VI, while incorporating weakly-informative priors for DP concentration parameter alpha and base distribution G0. We illustrate the importance of G0 covariance structure and prior choice by considering different parameterisations of the data covariance matrix. On high-dimensional Gaussian simulations, our model demonstrates substantially faster convergence than a state-of-the-art MCMC splice sampler. We further evaluate performances on Negative Binomial simulations and conduct sensitivity analyses to assess robustness on realistic data conditions. Application to a publicly available leukemia transcriptomic data set comprising 72 samples and 2,194 gene expression successfully recovers every known sub-type, all while identifying additional gene expression-based sub-clusters with meaningful biological interpretation.

</details>


### [25] [A Poisson Factor Mixture Model for the Analysis of Linguistic Competence in Italian University Students' Writing](https://arxiv.org/abs/2601.21493)
*Silvia Dallari,Laura Anderlucci,Nicola Grandi,Angela Montanari*

Main category: stat.ME

TL;DR: 该研究分析了意大利大学生正式书面语的使用模式，通过新型聚类方法识别出写作能力的两个相关维度：交际能力和语言语法能力，并发现这些能力与学科领域和教育背景相关。


<details>
  <summary>Details</summary>
Motivation: 针对公众关于年轻一代语言能力下降的争论，本研究旨在客观分析大学生这一高教育群体如何实际使用正式书面意大利语，识别系统性的能力模式和异质性，而非简单评判语言是否退化。

Method: 基于UniversITA项目收集的全国代表性大学生正式文本数据，对拼写、词汇、句法、语篇连贯性等多方面语言特征进行标注，使用新型泊松因子混合模型进行聚类分析，该模型能处理低频多元计数数据并考虑特征间依赖性和未观测的群体异质性。

Result: 识别出写作能力的两个相关维度：交际能力和语言语法能力。结合教育和社会人口信息后，发现不同学生特征与学科领域和教育背景相关，形成了清晰的学生能力画像。

Conclusion: 研究为当代写作能力提供了量化证据，揭示了大学生正式书面语使用的系统性模式，对语言教育和高等教育政策具有重要启示意义。

Abstract: Public debate on the alleged decline of language skills among younger generations often focuses on university students, the most highly educated segment of the population. Rather than addressing the ill posed question of linguistic decline, this paper examines how formal written Italian is currently used by university students and whether systematic patterns of competence and heterogeneity can be identified. The analysis is based on data from the UniversITA project, which collected formal texts written by a large and nationally representative sample of Italian university students. Texts were annotated for linguistically motivated features covering orthography, lexicon, syntax, morphosyntax, coherence, register, and sentence structure, yielding low frequency multivariate count data. To analyse these data, we propose a novel model-based clustering approach based on a Poisson factor mixture model that accounts for dependence among linguistic features and unobserved population heterogeneity. The results identify two correlated dimensions of writing competence, interpretable as communicative competence and linguistic grammatical competence. When educational and socio demographic information is incorporated, distinct student profiles emerge that are associated with field of study and educational background. These findings provide quantitative evidence on contemporary writing and offer insights relevant for language education and higher education policy.

</details>


### [26] [A Matrix-Variate Log-Normal Model for Covariance Matrices](https://arxiv.org/abs/2601.21532)
*Edoardo Otranto*

Main category: stat.ME

TL;DR: 提出基于矩阵正态分布的时变协方差矩阵建模框架，通过对数协方差矩阵的BEKK结构保证正定性，使用矩阵指数恢复协方差并进行偏误校正


<details>
  <summary>Details</summary>
Motivation: 传统时变协方差矩阵建模面临维度诅咒和正定性约束问题，需要一种既能保证正定性又参数简约的灵活框架

Method: 假设已实现协方差矩阵的对数服从矩阵正态分布，条件均值采用BEKK结构，通过矩阵指数恢复协方差矩阵，并提出基于二阶泰勒展开的偏误校正

Result: 开发了保证正定性的简约参数化模型，通过最大似然估计，框架适用于各种对称正定矩阵问题

Conclusion: 该框架为时变协方差矩阵建模提供了灵活且理论严谨的方法，解决了正定性和维度问题，具有广泛适用性

Abstract: We propose a modeling framework for time-varying covariance matrices based on the assumption that the logarithm of a realized covariance matrix follows a matrix-variate oNrmal distribution. By operating in the space of symmetric matrices, the approach guarantees positive definiteness without imposing parameter constraints beyond stationarity. The conditional mean of the logarithmic covariance matrix is specified through a BEKK-type structure that can be rewritten as a diagonal vector representation, yielding a parsimonious specification that mitigates the curse of dimensionality. Estimation is performed by maximum likelihood exploiting properties of matrix-variate Normal distributions and expressing the scale parameter matrix as a function of the location matrix. The covariance matrix is recovered via the matrix exponential. Since this transformation induces an upward bias, an approximate, time-specific bias correction based on a second-order Taylor expansion is proposed. The framework is flexible and applicable to a wide class of problems involving symmetric positive definite matrices.

</details>


### [27] [Independent Component Discovery in Temporal Count Data](https://arxiv.org/abs/2601.21696)
*Alexandre Chaussard,Anna Bonnet,Sylvain Le Corff*

Main category: stat.ME

TL;DR: 提出一个用于时序计数数据的生成式独立成分分析框架，结合状态自适应动力学和泊松对数正态发射，支持表示学习和扰动分析。


<details>
  <summary>Details</summary>
Motivation: 数据收集技术的进步产生了越来越多的时序计数观测数据，需要适应的建模方法。现有方法在处理时序计数数据的独立成分分析方面存在不足。

Method: 提出生成式框架，结合状态自适应动力学与泊松对数正态发射模型，建立模型可识别性，并设计高效的分摊变分推断学习参数。

Result: 在模拟数据上评估了混合函数和潜在源的恢复效果，在体内纵向肠道微生物组研究中揭示了与临床扰动一致的微生物共变异模式和状态转移。

Conclusion: 该框架能够识别具有状态依赖贡献的解耦成分，支持表示学习和扰动分析，为时序计数数据提供有效的独立成分分析方法。

Abstract: Advances in data collection are producing growing volumes of temporal count observations, making adapted modeling increasingly necessary. In this work, we introduce a generative framework for independent component analysis of temporal count data, combining regime-adaptive dynamics with Poisson log-normal emissions. The model identifies disentangled components with regime-dependent contributions, enabling representation learning and perturbations analysis. Notably, we establish the identifiability of the model, supporting principled interpretation. To learn the parameters, we propose an efficient amortized variational inference procedure. Experiments on simulated data evaluate recovery of the mixing function and latent sources across diverse settings, while an in vivo longitudinal gut microbiome study reveals microbial co-variation patterns and regime shifts consistent with clinical perturbations.

</details>


### [28] [Neural Wasserstein Two-Sample Tests](https://arxiv.org/abs/2601.21732)
*Xiaoyu Hu,Zhenhua Lin*

Main category: stat.ME

TL;DR: 提出基于投影1-Wasserstein距离的神经Wasserstein检验，通过神经网络学习最优投影方向，采用max-type聚合统计量实现无需调参的高维两样本同质性检验。


<details>
  <summary>Details</summary>
Motivation: 高维两样本同质性检验是统计学中的基本问题，但经典方法在高维情况下功效会显著下降。作者观察到两个高维分布往往在某个低维投影下存在差异，因此希望通过学习最优投影方向来提高检验功效。

Method: 1. 使用流形优化和深度神经网络学习投影方向和见证函数；2. 通过max-type构造聚合多个候选统计量，避免显式调参；3. 基于投影1-Wasserstein距离构建神经Wasserstein检验。

Result: 建立了检验的有效性和一致性，证明了Gaussian近似的Berry-Esseen型边界。在原假设下，聚合统计量收敛于标准高斯向量的绝对最大值，实现了渐近枢轴（分布自由）的校准，无需重采样。模拟研究和实际数据表明该方法具有优异的有限样本性能。

Conclusion: 神经Wasserstein检验通过深度学习最优投影方向，结合max-type聚合策略，有效解决了高维两样本同质性检验的挑战，在理论和实际应用中均表现出优越性能。

Abstract: The two-sample homogeneity testing problem is fundamental in statistics and becomes particularly challenging in high dimensions, where classical tests can suffer substantial power loss. We develop a learning-assisted procedure based on the projection 1-Wasserstein distance, which we call the neural Wasserstein test. The method is motivated by the observation that there often exists a low-dimensional projection under which the two high-dimensional distributions differ. In practice, we learn the projection directions via manifold optimization and a witness function using deep neural networks. To adapt to unknown projection dimensions and sparsity levels, we aggregate a collection of candidate statistics through a max-type construction, avoiding explicit tuning while potentially improving power. We establish the validity and consistency of the proposed test and prove a Berry--Esseen type bound for the Gaussian approximation. In particular, under the null hypothesis, the aggregated statistic converges to the absolute maximum of a standard Gaussian vector, yielding an asymptotically pivotal (distribution-free) calibration that bypasses resampling. Simulation studies and a real-data example demonstrate the strong finite-sample performance of the proposed method.

</details>


### [29] [Synthesizing Epileptic Seizures: Gaussian Processes for EEG Generation](https://arxiv.org/abs/2601.21752)
*Nina Moutonnet,Joshua Corneck,Felipe Tobar,Danilo Mandic*

Main category: stat.ME

TL;DR: 提出GP-EEG框架，使用高斯过程和变分自编码器生成合成癫痫EEG数据，解决标记数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 癫痫EEG数据获取成本高、标记数据稀缺，且EEG信号具有长程、高维、非平稳特性，使得数据生成困难，限制了机器学习方法的性能

Method: 提出分层框架GP-EEG：1) 使用高斯过程回归对EEG信号的时间片段建模；2) 集成领域自适应变分自编码器；3) 生成合成癫痫EEG记录

Result: 在两个真实世界开源癫痫EEG数据集上验证，生成的合成EEG在定量和定性上都与真实癫痫EEG匹配，可用于增强训练集

Conclusion: 高斯过程为EEG动态建模提供了原则性和可解释的基础，GP-EEG框架能有效生成高质量的合成癫痫EEG数据，缓解数据稀缺问题

Abstract: Reliable seizure detection from electroencephalography (EEG) time series is a high-priority clinical goal, yet the acquisition cost and scarcity of labeled EEG data limit the performance of machine learning methods. This challenge is exacerbated by the long-range, high-dimensional, and non-stationary nature of epileptic EEG recordings, which makes realistic data generation particularly difficult. In this work, we revisit Gaussian processes as a principled and interpretable foundation for modeling EEG dynamics, and propose a novel hierarchical framework, \textit{GP-EEG}, for generating synthetic epileptic EEG recordings. At its core, our approach decomposes EEG signals into temporal segments modeled via Gaussian process regression, and integrates a domain-adaptation variational autoencoder. We validate the proposed method on two real-world, open-source epileptic EEG datasets. The synthetic EEG recordings generated by our model match real-world epileptic EEG both quantitatively and qualitatively, and can be used to augment training sets.

</details>


### [30] [A Spacing Estimator](https://arxiv.org/abs/2601.22103)
*Greg Kreider*

Main category: stat.ME

TL;DR: 论文扩展了顺序统计量间距分布的理论，新增了logistic和Gumbel分布，并提出了一种适用于已知逆累积分布函数的通用估计器。


<details>
  <summary>Details</summary>
Motivation: 目前只有均匀分布和指数分布的连续顺序统计量间距分布已知，需要扩展更多分布类型并开发通用估计方法。

Method: 提出了基于已知逆累积分布函数的估计器，用于计算顺序统计量间距分布，特别针对logistic和Gumbel分布进行了推导。

Result: 估计器在顺序统计量中间区域精度很高（达到数值模拟极限），但在尾部区域精度下降最多达20%。

Conclusion: 成功扩展了顺序统计量间距分布理论，提出的估计器在大多数区域表现良好，但尾部精度需要进一步改进。

Abstract: The distribution of the spacing, or the difference between consecutive order statistics, is known only for uniform and exponential random variates. We add here logistic and Gumbel variates, and present an estimator for distributions with a known inverse cumulative density function. We show the estimator is accurate to the limit of numerical simulations for points near the middle of the order statistics, but degrades by up to 20% in the tails.

</details>


### [31] [Information-geometry-driven graph sequential growth](https://arxiv.org/abs/2601.22106)
*Harry T. Bond,Bertrand Gauthier,Kirstin Strokorb*

Main category: stat.ME

TL;DR: 提出一种基于信息几何的无正则化高斯图模型推断方法，通过坐标下降过程逐步添加边，无需调参且计算高效。


<details>
  <summary>Details</summary>
Motivation: 传统高斯图模型推断方法通常需要正则化参数，这增加了调参复杂性。本文旨在开发一种无需正则化参数的方法，通过信息几何驱动的图增长过程实现可靠的稀疏图推断。

Method: 提出基于信息几何的图序列增长方法：从无边图开始，通过坐标下降过程逐步添加边。将图增长与坐标下降过程关联，表征信息最优增长对应的完全校正下降，并提出数值高效的近似策略。

Result: 该方法能可靠提取稀疏图模型，同时限制误检数量。激活排序可以提供边集信息相关性的洞察。方法无需调参，计算复杂度与坐标下降相当。

Conclusion: 信息几何驱动的无正则化图增长方法为高斯图推断提供了一种高效可靠的替代方案，避免了传统方法的调参问题，同时保持了稀疏性和准确性。

Abstract: We investigate the properties of a class of regularisation-free approaches for Gaussian graphical inference based on the information-geometry-driven sequential growth of initially edgeless graphs. Relating the growth of a graph to a coordinate descent process, we characterise the fully-corrective descents corresponding to information-optimal growths, and propose numerically efficient strategies for their approximation. We demonstrate the ability of the proposed procedures to reliably extract sparse graphical models while limiting the number of false detections, and illustrate how activation ranks can provide insight into the informational relevance of edge sets. The considered approaches are tuning-parameter-free and have complexities akin to coordinate descents.

</details>


### [32] [Interval Spacing](https://arxiv.org/abs/2601.22116)
*Greg Kreider*

Main category: stat.ME

TL;DR: 论文定义了区间间距作为数据顺序统计量在特定宽度间隔上的差值，推导了均匀分布、指数分布和逻辑分布的密度函数、期望和方差，并展示了区间间距等价于在间距上运行矩形低通滤波器。


<details>
  <summary>Details</summary>
Motivation: 研究顺序统计量中区间间距的统计特性，为理解数据分布中特定间隔的差异提供理论框架，特别是在信号处理应用中。

Method: 定义区间间距为顺序统计量在固定宽度间隔上的差值，推导其概率密度函数、期望值和方差，针对均匀分布、指数分布和逻辑分布三种具体分布进行分析。

Result: 获得了三种分布下区间间距的解析表达式，证明了区间间距等价于在间距上应用矩形低通滤波器，这简化了期望值的计算并引入了重叠区间之间的相关性。

Conclusion: 区间间距为分析顺序统计量中的局部差异提供了有用的工具，其与低通滤波器的等价关系为信号处理应用提供了新的视角，并揭示了重叠区间间的统计相关性。

Abstract: We define interval spacing as the difference in the order statistics of data over a gap of some width. We derive its density, expected value, and variance for uniform, exponential, and logistic variates. We show that interval spacing is equivalent to running a rectangular low-pass filter over the spacing, which simplifies the expressions for the expected values and introduces correlations between overlapping intervals.

</details>


### [33] [Variance component score test for multivariate change point detection with applications to mobile health](https://arxiv.org/abs/2601.22147)
*Melissa Lynne Martin,Juliette Brook,Sage Rush,Theodore D. Satterthwaite,Ian J. Barnett*

Main category: stat.ME

TL;DR: 提出VC*方法用于高维时间序列数据的变点检测，在移动健康监测中实时检测行为变化，相比现有方法具有更高检测能力


<details>
  <summary>Details</summary>
Motivation: 移动健康监测中需要实时检测高危患者的行为变化以进行及时干预，但特征维度高、观测样本少使得多元变点检测具有挑战性

Method: 提出方差成分得分检验（VC*），仅使用变点前数据估计分布参数，检测特征均值和/或方差的变化

Result: 模拟研究表明VC*比现有方法具有更高的检测能力；在大多数情况下，仅使用变点前数据减少偏差的好处超过了估计器方差增加的影响

Conclusion: VC*方法在移动健康监测的实际应用中有效，成功应用于青少年和年轻成人情感不稳定患者的被动智能手机数据分析

Abstract: Multivariate change point detection is the process of identifying distributional shifts in time-ordered data across multiple features. This task is particularly challenging when the number of features is large relative to the number of observations. This problem is often present in mobile health, where behavioral changes in at-risk patients must be detected in real time in order to prompt timely interventions. We propose a variance component score test (VC*) for detecting changes in feature means and/or variances using only pre-change point data to estimate distributional parameters. Through simulation studies, we show that VC* has higher power than existing methods. Moreover, we demonstrate that reducing bias by using only pre-change point days to estimate parameters outweighs the increased estimator variances in most scenarios. Lastly, we apply VC* and competing methods to passively collected smartphone data in adolescents and young adults with affective instability.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [34] [Analyzing the Temporal Factors for Anxiety and Depression Symptoms with the Rashomon Perspective](https://arxiv.org/abs/2601.20874)
*Mustafa Cavus,Przemysław Biecek,Julian Tejada,Fernando Marmolejo-Ramos,Andre Faro*

Main category: stat.AP

TL;DR: 该研究提出在公共心理健康领域采用Rashomon效应视角，通过分析多个性能相近但结构不同的模型，揭示焦虑抑郁与人口统计学及时空因素的关系，发现昼夜和每周节律对心理风险的重要影响。


<details>
  <summary>Details</summary>
Motivation: 传统方法选择单一"最佳"模型可能掩盖数据中存在的替代性解释。在心理健康研究中，需要更稳健的方法来解释焦虑抑郁与人口统计学及时空因素的关系，避免因模型选择偏差而遗漏重要发现。

Method: 使用随机森林模型结合部分依赖分析，构建并分析Rashomon集合（多个预测性能相似但内部结构不同的模型），评估预测关系的稳健性和稳定性。基于Patient Health Questionnaire-4大规模心理数据集。

Result: 人口统计学变量（年龄、性别、教育程度）对焦虑抑郁风险产生一致的结构性影响。发现显著的时间效应：风险概率呈现明显的昼夜和每周节律波动，在清晨时段达到峰值。

Conclusion: 心理健康研究需要超越单一最佳模型，分析整个Rashomon集合。昼夜和每周节律导致的变异性必须仔细考虑，以确保心理筛查解释的稳健性。提倡采用多重性感知方法增强机器学习结论的稳定性和泛化能力。

Abstract: This paper introduces a new modeling perspective in the public mental health domain to provide a robust interpretation of the relations between anxiety and depression, and the demographic and temporal factors. This perspective particularly leverages the Rashomon Effect, where multiple models exhibit similar predictive performance but rely on diverse internal structures. Instead of considering these multiple models, choosing a single best model risks masking alternative narratives embedded in the data. To address this, we employed this perspective in the interpretation of a large-scale psychological dataset, specifically focusing on the Patient Health Questionnaire-4. We use a random forest model combined with partial dependence profiles to rigorously assess the robustness and stability of predictive relationships across the resulting Rashomon set, which consists of multiple models that exhibit similar predictive performance. Our findings confirm that demographic variables \texttt{age}, \texttt{sex}, and \texttt{education} lead to consistent structural shifts in anxiety and depression risk. Crucially, we identify significant temporal effects: risk probability demonstrates clear diurnal and circaseptan fluctuations, peaking during early morning hours. This work demonstrates the necessity of moving beyond the best model to analyze the entire Rashomon set. Our results highlight that the observed variability, particularly due to circadian and circaseptan rhythms, must be meticulously considered for robust interpretation in psychological screening. We advocate for a multiplicity-aware approach to enhance the stability and generalizability of ML-based conclusions in mental health research.

</details>


### [35] [Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis](https://arxiv.org/abs/2601.20875)
*Md Muhtasim Munif Fahim,Md Jahid Hasan Imran,Luknath Debnath,Tonmoy Shill,Md. Naim Molla,Ehsanul Bashar Pranto,Md Shafin Sanyan Saad,Md Rezaul Karim*

Main category: stat.AP

TL;DR: 该研究使用因果发现框架分析SDG间的依赖关系，发现不存在单一"枢纽"目标，教育对不平等的影响最显著，并提出分层优先级框架。


<details>
  <summary>Details</summary>
Motivation: 为实现2030年可持续发展目标，需要理解各目标间的因果依赖关系，以指导资源分配策略。现有研究缺乏对SDG间完整因果架构的系统分析。

Method: 使用面板向量自回归结合国家固定效应和PCMCI+条件独立性检验，对168个国家（2000-2025年）的8个关键SDG进行因果发现分析。

Result: 识别出分布式因果网络（无单一枢纽目标），发现10个显著格兰杰因果关系（11个直接效应）。教育对不平等的影响最显著（r=-0.599），但效应大小随收入水平变化。提出SDG分层框架：上游驱动（教育、增长）、促成目标（制度、能源）、下游结果（贫困、健康）。

Conclusion: 有效的SDG加速需要通过协调的多维度干预，单一目标的顺序策略不足。不存在单一的"关键"SDG，需要系统性方法。

Abstract: The achievement of the 2030 Sustainable Development Goals (SDGs) is dependent upon strategic resource distribution. We propose a causal discovery framework using Panel Vector Autoregression, along with both country-specific fixed effects and PCMCI+ conditional independence testing on 168 countries (2000-2025) to develop the first complete causal architecture of SDG dependencies. Utilizing 8 strategically chosen SDGs, we identify a distributed causal network (i.e., no single 'hub' SDG), with 10 statistically significant Granger-causal relationships identified as 11 unique direct effects. Education to Inequality is identified as the most statistically significant direct relationship (r = -0.599; p < 0.05), while effect magnitude significantly varies depending on income levels (e.g., high-income: r = -0.65; lower-middle-income: r = -0.06; non-significant). We also reject the idea that there exists a single 'keystone' SDG. Additionally, we offer a proposed tiered priority framework for the SDGs namely, identifying upstream drivers (Education, Growth), enabling goals (Institutions, Energy), and downstream outcomes (Poverty, Health). Therefore, we conclude that effective SDG acceleration can be accomplished through coordinated multi-dimensional intervention(s), and that single-goal sequential strategies are insufficient.

</details>


### [36] [Spatial Heterogeneity in Climate Risk and Human Flourishing: An Exploration with Generative AI](https://arxiv.org/abs/2601.20880)
*Stefano Maria Iacus,Haodong Qi,Devika Jain*

Main category: stat.AP

TL;DR: 该研究开发了一个空间框架，结合生成式AI和结构方程模型，分析美国县级累积气候风险与人类繁荣多维度之间的关系。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（特别是大语言模型）能够从非结构化文本中提取空间信息，为研究气候地理学提供了新的方法论机会。本研究旨在探索如何利用这些新技术来研究气候风险与人类繁荣之间的空间关系。

Method: 1. 使用微调的开源大语言模型对26亿条地理标记推文进行分类，构建人类繁荣地理指数（HFGI）；2. 整合高分辨率气候灾害指标；3. 将指标聚合到美国县级；4. 使用结构方程模型推断总体气候风险和人类繁荣维度（包括幸福感、意义与目的、社会联系、心理困扰、身体状况、经济稳定、宗教信仰、品格与美德、制度信任）。

Result: 结果显示，较高的累积气候风险与较低的人类繁荣表达水平之间存在空间异质性关联，其空间模式与反复暴露于热浪、洪水、风灾、干旱和野火等灾害相一致。

Conclusion: 该研究展示了如何将生成式AI与潜在构念建模相结合，用于地理分析和空间知识提取，为理解气候风险与人类繁荣之间的复杂空间关系提供了新方法。

Abstract: Recent advances in Generative Artificial Intelligence (AI), particularly Large Language Models (LLMs), enable scalable extraction of spatial information from unstructured text and offer new methodological opportunities for studying climate geography. This study develops a spatial framework to examine how cumulative climate risk relates to multidimensional human flourishing across U.S. counties. High-resolution climate hazard indicators are integrated with a Human Flourishing Geographic Index (HFGI), an index derived from classification of 2.6 billion geotagged tweets using fine-tuned open-source Large Language Models (LLMs). These indicators are aggregated to the US county-level and mapped to a structural equation model to infer overall climate risk and human flourishing dimensions, including expressed well-being, meaning and purpose, social connectedness, psychological distress, physical condition, economic stability, religiosity, character and virtue, and institutional trust. The results reveal spatially heterogeneous associations between greater cumulative climate risk and lower levels of expressed human flourishing, with coherent spatial patterns corresponding to recurrent exposure to heat, flooding, wind, drought, and wildfire hazards. The study demonstrates how Generative AI can be combined with latent construct modeling for geographical analysis and for spatial knowledge extraction.

</details>


### [37] [A new strategy for finite-sample valid prediction of future insurance claims in the regression setting](https://arxiv.org/abs/2601.21153)
*Liang Hong*

Main category: stat.AP

TL;DR: 提出新策略将无监督iid预测方法转换为回归预测方法，为精算师提供无限多个有限样本有效预测区间


<details>
  <summary>Details</summary>
Motivation: 现有保险文献在回归设置中缺乏有限样本有效的未来保险索赔预测区间

Method: 提出新策略将无监督独立同分布设置中的预测方法转换为回归设置中的预测方法

Result: 使精算师能够在回归设置中获得无限多个有限样本有效的预测区间

Conclusion: 成功解决了保险文献中回归设置下有限样本有效预测区间缺乏的问题

Abstract: The extant insurance literature demonstrates a paucity of finite-sample valid prediction intervals of future insurance claims in the regression setting. To address this challenge, this article proposes a new strategy that converts a predictive method in the unsupervised iid (independent identically distributed) setting to a predictive method in the regression setting. In particular, it enables an actuary to obtain infinitely many finite-sample valid prediction intervals in the regression setting.

</details>


### [38] [Bivariate Postprocessing of Wind Vectors](https://arxiv.org/abs/2601.21401)
*Ferdinand Buchner,David Jobst,Annette Möller,Claudia Czado*

Main category: stat.AP

TL;DR: 本文提出三种新的双变量后处理方法（藤蔓copula模型、梯度提升EMOS、分布回归网络），用于改进数值天气预报集合预报的双变量风场分量联合后处理，在德国60个站点上验证显示新方法优于传统双变量EMOS。


<details>
  <summary>Details</summary>
Motivation: 数值天气预报集合预报存在系统性偏差和离散误差，需要统计后处理来获得校准良好且锐利的预测概率分布。现有研究关注多变量后处理模型，但需要改进双变量风场分量的联合后处理方法。

Method: 提出三种新的双变量后处理方法：1）基于藤蔓copula的双变量模型；2）双变量梯度提升版本的集合模型输出统计（EMOS）；3）双变量分布回归网络（DRN）。这些方法与传统的双变量EMOS进行比较，应用于德国60个站点的双变量风场分量联合后处理。

Result: 案例研究表明，新的双变量方法优于传统的双变量EMOS方法。其中双变量分布回归网络（DRN）和最灵活的藤蔓copula方法在验证分数和校准方面表现最佳。

Conclusion: 新提出的双变量后处理方法（特别是双变量DRN和灵活藤蔓copula方法）能够有效改进数值天气预报集合预报的双变量风场分量联合后处理，提高预测概率分布的校准性和锐利度。

Abstract: To quantify the uncertainty in numerical weather prediction (NWP) forecasts, ensemble prediction systems are utilized. Although NWP forecasts continuously improve, they suffer from systematic bias and dispersion errors. To obtain well calibrated and sharp predictive probability distributions, statistical postprocessing methods are applied to NWP output. Recent developments focus on multivariate postprocessing models incorporating dependencies directly into the model. We introduce three novel bivariate postprocessing approaches, and analyze their performance for joint postprocessing of bivariate wind vector components for 60 stations in Germany. Bivariate vine copula based models, a bivariate gradient boosted version of ensemble model output statistics (EMOS), and a bivariate distributional regression network (DRN) are compared to bivariate EMOS. The case study indicates that the novel bivariate methods improve over the bivariate EMOS approaches. The bivariate DRN and the most flexible version of the bivariate vine copula approach exhibit the best performance in terms of verification scores and calibration.

</details>


### [39] [Clustering Methods for Identifying and Modelling Areas with Similar Temperature Variations](https://arxiv.org/abs/2601.21495)
*Edoardo Otranto*

Main category: stat.AP

TL;DR: 提出基于数据驱动的方法，通过聚类和时空自回归模型识别和建模具有相似温度变化的区域，使用统计相似性而非地理邻近性来改进全球温度动态建模。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近性的时空模型可能无法充分捕捉温度变化的复杂模式，需要开发基于统计相似性的方法来更好地建模全球温度动态。

Method: 使用168个国家1901-2022年的年度温度数据，采用三种聚类方法（基于升温速率、年度温度变化、变化符号持续性），使用欧几里得和汉明距离构建聚类，然后将这些聚类用于构建STAR模型的空间权重矩阵。

Result: 基于距离的STAR模型在样本内和样本外预测中都优于传统的基于邻接性的模型，其中基于汉明距离的STAR模型取得了最佳预测精度。

Conclusion: 使用统计相似性而非地理邻近性可以改进全球温度动态的建模，这种方法对其他环境和社会经济数据集具有更广泛的适用性。

Abstract: This paper proposes a novel data-driven approach for identifying and modelling areas with similar temperature variations throufigureh clustering and Space-Time AutoRegressive (STAR) models. Using annual temperature data from 168 countries (1901-2022), we apply three clustering methods based on (i) warming rates, (ii) annual temperature variations, and (iii) persistence of variation signs, using Euclidean and Hamming distances. These clusters are then employed to construct alternative spatial weight matrices for STAR models. Empirical results show that distance-based STAR models outperform classical contiguity-based ones, both in-sample and out-of-sample, with the Hamming distance-based STAR model achieving the best predictive accuracy. The study demonstrates that using statistical similarity rather than geographical proximity improves the modelling of global temperature dynamics, suggesting broader applicability to other environmental and socioeconomic datasets.

</details>


### [40] [Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges](https://arxiv.org/abs/2601.22104)
*Paolo Andrich,Shengjie Lai,Halim Jun,Qianwen Duan,Zhifeng Cheng,Seth R. Flaxman,Andrew J. Tatem*

Main category: stat.AP

TL;DR: 使用贝叶斯插补和统计模型校准Facebook用户数据，解决隐私保护导致的农村地区数据缺失问题，为菲律宾提供动态人口监测方法


<details>
  <summary>Details</summary>
Motivation: 传统人口普查无法捕捉快速人口变化，社交媒体数据虽具潜力但存在代表性问题和隐私限制，需要可靠方法将社交媒体信号转化为准确人口数据

Method: 1) 使用贝叶斯插补方法恢复因差分隐私技术而缺失的农村地区数据；2) 建立统计模型，利用城市化水平、人口结构、社会经济状况等预测因子，将Facebook用户数关联到真实人口水平；3) 考虑数据过度分散和空间相关性

Result: 恢复了5.5%农村地区的数据覆盖率；模型验证误差低（城市约18%，农村约24%）；能够随时间更新人口预测，提供动态人口监测能力

Conclusion: 该方法为灾害频发地区的人道主义响应提供可靠工具，建立了利用有偏社交媒体信号生成及时准确人口数据的通用框架

Abstract: Accurate and timely population data are essential for disaster response and humanitarian planning, but traditional censuses often cannot capture rapid demographic changes. Social media data offer a promising alternative for dynamic population monitoring, but their representativeness remains poorly understood and stringent privacy requirements limit their reliability. Here, we address these limitations in the context of the Philippines by calibrating Facebook user counts with the country's 2020 census figures. First, we find that differential privacy techniques commonly applied to social media-based population datasets disproportionately mask low-population areas. To address this, we propose a Bayesian imputation approach to recover missing values, restoring data coverage for $5.5\%$ of rural areas. Further, using the imputed social media data and leveraging predictors such as urbanisation level, demographic composition, and socio-economic status, we develop a statistical model for the proportion of Facebook users in each municipality, which links observed Facebook user numbers to the true population levels. Out-of-sample validation demonstrates strong result generalisability, with errors as low as ${\approx}18\%$ and ${\approx}24\%$ for urban and rural Facebook user proportions, respectively. We further demonstrate that accounting for overdispersion and spatial correlations in the data is crucial to obtain accurate estimates and appropriate credible intervals. Crucially, as predictors change over time, the models can be used to regularly update the population predictions, providing a dynamic complement to census-based estimates. These results have direct implications for humanitarian response in disaster-prone regions and offer a general framework for using biased social media signals to generate reliable and timely population data.

</details>
