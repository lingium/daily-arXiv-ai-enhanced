<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 13]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [3D-SONAR: Self-Organizing Network for 3D Anomaly Ranking](https://arxiv.org/abs/2601.09294)
*Guodong Xu,Juan Du,Hui Yang*

Main category: stat.AP

TL;DR: 提出3D-SONAR方法，基于自组织网络进行3D异常排名，无需训练即可检测表面异常


<details>
  <summary>Details</summary>
Motivation: 现有3D点云异常检测方法依赖深度学习和大规模数据集，但在实际应用中数据获取困难且昂贵

Method: 将3D点云建模为动态系统，表示为无向图，通过吸引力和排斥力相互作用，能量分布揭示表面异常

Result: 在开放表面和封闭表面上都实现了优异的异常检测性能，无需训练

Conclusion: 为无监督检测提供了新视角，展示了物理启发模型在数据有限工业异常检测任务中的潜力

Abstract: Surface anomaly detection using 3D point cloud data has gained increasing attention in industrial inspection. However, most existing methods rely on deep learning techniques that are highly dependent on large-scale datasets for training, which are difficult and expensive to acquire in real-world applications. To address this challenge, we propose a novel method based on self-organizing network for 3D anomaly ranking, also named 3D-SONAR. The core idea is to model the 3D point cloud as a dynamic system, where the points are represented as an undirected graph and interact via attractive and repulsive forces. The energy distribution induced by these forces can reveal surface anomalies. Experimental results show that our method achieves superior anomaly detection performance in both open surface and closed surface without training. This work provides a new perspective on unsupervised inspection and highlights the potential of physics-inspired models in industrial anomaly detection tasks with limited data.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [2] [Tail-Sensitive KL and Rényi Convergence of Unadjusted Hamiltonian Monte Carlo via One-Shot Couplings](https://arxiv.org/abs/2601.09019)
*Nawaf Bou-Rabee,Siddharth Mitra,Andre Wibisono*

Main category: stat.ML

TL;DR: 该论文提出了一种框架，将未调整哈密顿蒙特卡洛（uHMC）的Wasserstein收敛保证升级为对尾部敏感的KL和Rényi散度保证，用于量化相对密度不匹配。


<details>
  <summary>Details</summary>
Motivation: HMC算法在高维设置中广泛使用，但其在KL和Rényi等相对密度不匹配散度中的收敛性质理解不足。这些散度自然控制着Metropolis调整马尔可夫链的接受概率和预热启动要求。

Method: 开发基于单次耦合的框架，用于建立uHMC转移核的正则化性质。该正则化允许将Wasserstein-2混合时间和渐近偏差界限提升到KL散度，并将类似的Orlicz-Wasserstein界限提升到Rényi散度。

Result: 提供了相对密度不匹配的定量控制，阐明了离散化偏差在强散度中的作用，并为未调整采样和生成Metropolis调整马尔可夫链的预热启动提供了原则性保证。

Conclusion: 该框架将Wasserstein收敛保证升级到尾部敏感的KL和Rényi散度，填补了HMC算法在相对密度不匹配度量方面收敛性质的理论空白，具有重要的理论和实践意义。

Abstract: Hamiltonian Monte Carlo (HMC) algorithms are among the most widely used sampling methods in high dimensional settings, yet their convergence properties are poorly understood in divergences that quantify relative density mismatch, such as Kullback-Leibler (KL) and Rényi divergences. These divergences naturally govern acceptance probabilities and warm-start requirements for Metropolis-adjusted Markov chains. In this work, we develop a framework for upgrading Wasserstein convergence guarantees for unadjusted Hamiltonian Monte Carlo (uHMC) to guarantees in tail-sensitive KL and Rényi divergences. Our approach is based on one-shot couplings, which we use to establish a regularization property of the uHMC transition kernel. This regularization allows Wasserstein-2 mixing-time and asymptotic bias bounds to be lifted to KL divergence, and analogous Orlicz-Wasserstein bounds to be lifted to Rényi divergence, paralleling earlier work of Bou-Rabee and Eberle (2023) that upgrade Wasserstein-1 bounds to total variation distance via kernel smoothing. As a consequence, our results provide quantitative control of relative density mismatch, clarify the role of discretization bias in strong divergences, and yield principled guarantees relevant both for unadjusted sampling and for generating warm starts for Metropolis-adjusted Markov chains.

</details>


### [3] [Horseshoe Mixtures-of-Experts (HS-MoE)](https://arxiv.org/abs/2601.09043)
*Nick Polson,Vadim Sokolov*

Main category: stat.ML

TL;DR: HS-MoE模型结合了horseshoe先验和输入依赖门控，实现了专家选择的自适应稀疏性，并提出了粒子学习算法进行序列推断。


<details>
  <summary>Details</summary>
Motivation: 为混合专家模型提供贝叶斯框架，实现稀疏的专家选择，特别是在大语言模型中需要从大量专家池中为每个token激活少量专家的极端稀疏约束场景。

Method: 结合horseshoe先验的自适应全局-局部收缩与输入依赖门控，提出粒子学习算法进行序列推断，通过跟踪充分统计量向前传播滤波器。

Result: 建立了HS-MoE模型框架，实现了数据自适应的专家使用稀疏性，并开发了高效的序列推断算法。

Conclusion: HS-MoE为混合专家模型提供了有效的贝叶斯稀疏选择方法，特别适用于大语言模型中的极端稀疏专家激活场景。

Abstract: Horseshoe mixtures-of-experts (HS-MoE) models provide a Bayesian framework for sparse expert selection in mixture-of-experts architectures. We combine the horseshoe prior's adaptive global-local shrinkage with input-dependent gating, yielding data-adaptive sparsity in expert usage. Our primary methodological contribution is a particle learning algorithm for sequential inference, in which the filter is propagated forward in time while tracking only sufficient statistics. We also discuss how HS-MoE relates to modern mixture-of-experts layers in large language models, which are deployed under extreme sparsity constraints (e.g., activating a small number of experts per token out of a large pool).

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [4] [Bayesian Semi-Blind Deconvolution at Scale](https://arxiv.org/abs/2601.09677)
*Guillermina Senn,Håkon Tjelmeland,Nathan Glatt-Holtz,Matt Walker,Andrew Holbrook*

Main category: stat.CO

TL;DR: 该论文扩展了半盲反卷积的贝叶斯层次模型，通过傅里叶域操作和新的边缘哈密顿蒙特卡洛方法改进计算效率，在模拟和实际地震成像问题上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 半盲反卷积问题中，同时估计模糊核和真实图像具有挑战性。现有方法在计算可扩展性和混合性能方面存在局限，特别是在处理大规模实际问题时。

Method: 扩展了现有的贝叶斯共轭层次模型，采用循环格点表示，在傅里叶域执行操作，并引入新的边缘哈密顿蒙特卡洛模糊核更新方法，通过解析积分消除图像变量。

Result: 在模拟数据和实际地震成像问题（300×50网格，约80,000参数）上，比较了Gibbs和HMC模糊核更新的混合和探索性能，展示了改进的计算效率。

Conclusion: 提出的傅里叶域操作和边缘HMC方法为半盲反卷积提供了计算可扩展的解决方案，特别适用于大规模实际问题，克服了传统方法的计算限制。

Abstract: Blind image deconvolution refers to the problem of simultaneously estimating the blur kernel and the true image from a set of observations when both the blur kernel and the true image are unknown. Sometimes, additional image and/or blur information is available and the term semi-blind deconvolution (SBD) is used. We consider a recently introduced Bayesian conjugate hierarchical model for SBD, formulated on an extended cyclic lattice to allow a computationally scalable Gibbs sampler. In this article, we extend this model to the general SBD problem, rewrite the previously proposed Gibbs sampler so that operations are performed in the Fourier domain whenever possible, and introduce a new marginal Hamiltonian Monte Carlo (HMC) blur update, obtained by analytically integrating the blur-image joint conditional over the image. The cyclic formulation combined with non-trivial linear algebra manipulations allows a Fourier-based, scalable HMC update, otherwise complicated by the rigid constraints of the SBD problem. Having determined the padding size in the cyclic embedding through a numerical experiment, we compare the mixing and exploration behaviour of the Gibbs and HMC blur updates on simulated data and on a real geophysical seismic imaging problem where we invert a grid with $300\times50$ nodes, corresponding to a posterior with approximately $80,000$ parameters.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [5] [MLCBART: Multilabel Classification with Bayesian Additive Regression Trees](https://arxiv.org/abs/2601.08964)
*Jiahao Tian,Hugh Chipman,Thomas Loughin*

Main category: stat.ME

TL;DR: 提出MLCBART模型，基于BART框架处理多标签分类问题，通过阈值化潜在数值尺度并建模标签间相关性来提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 多标签分类面临两个挑战：1) 预测变量与每个标签之间可能存在复杂多样的关系；2) 即使在考虑预测变量影响后，标签之间仍可能存在关联。现有方法难以同时有效处理这两个问题。

Method: 提出MLCBART模型，基于贝叶斯加性回归树(BART)框架。假设标签是通过阈值化潜在数值尺度产生的，使用多元正态模型显式估计标签间的相关性结构。采用贝叶斯框架进行MCMC采样，能够量化预测不确定性并估计条件概率分布。

Result: 模拟实验表明，MLCBART模型在预测标签向量方面比其他竞争模型更准确，其性能接近使用正确函数形式的oracle模型。模型能够提供预测不确定性的度量，有助于深入理解分类结果。

Conclusion: MLCBART是一个灵活的非参数贝叶斯框架，能够发现复杂的数据关系并显式建模标签相关性，显著提升了多标签分类的预测性能，同时提供有价值的预测不确定性量化。

Abstract: Multilabel Classification (MLC) deals with the simultaneous classification of multiple binary labels. The task is challenging because, not only may there be arbitrarily different and complex relationships between predictor variables and each label, but associations among labels may exist even after accounting for effects of predictor variables. In this paper, we present a Bayesian additive regression tree (BART) framework to model the problem. BART is a nonparametric and flexible model structure capable of uncovering complex relationships within the data. Our adaptation, MLCBART, assumes that labels arise from thresholding an underlying numeric scale, where a multivariate normal model allows explicit estimation of the correlation structure among labels. This enables the discovery of complicated relationships in various forms and improves MLC predictive performance. Our Bayesian framework not only enables uncertainty quantification for each predicted label, but our MCMC draws produce an estimated conditional probability distribution of label combinations for any predictor values. Simulation experiments demonstrate the effectiveness of the proposed model by comparing its performance with a set of models, including the oracle model with the correct functional form. Results show that our model predicts vectors of labels more accurately than other contenders and its performance is close to the oracle model. An example highlights how the method's ability to produce measures of uncertainty on predictions provides nuanced understanding of classification results.

</details>


### [6] [Approximate Shapley value estimation using sampling without replacement and variance estimation via the new Symmetric bootstrap and the Doubled half bootstrap](https://arxiv.org/abs/2601.08981)
*Fredrik Lohne Aanes*

Main category: stat.ME

TL;DR: 改进KernelSHAP算法，使用Wallenius非中心超几何分布抽样联盟数量并进行无放回抽样，引入对称bootstrap计算标准差，新算法性能与现有最佳方法相当


<details>
  <summary>Details</summary>
Motivation: 改进KernelSHAP算法的估计框架，通过更有效的抽样方法和更好的标准差计算方法提升算法性能

Method: 1. 使用Wallenius非中心超几何分布抽样联盟数量；2. 进行无放回抽样；3. 引入对称bootstrap计算标准差；4. 使用加倍半bootstrap方法比较性能

Result: 新bootstrap算法在两个模拟研究中表现更好或相当；新KernelSHAP算法与当前最佳R包shapr中改进的KernelSHAP方法性能相似

Conclusion: 提出的改进KernelSHAP算法通过更优的抽样和bootstrap方法，达到了与现有最佳方法相当的性能

Abstract: In this paper I consider improving the KernelSHAP algorithm. I suggest to use the Wallenius' noncentral hypergeometric distribution for sampling the number of coalitions and perform sampling without replacement, so that the KernelSHAP estimation framework is improved further. I also introduce the Symmetric bootstrap to calculate the standard deviations and also use the Doubled half bootstrap method to compare the performance. The new bootstrap algorithm performs better or equally well in the two simulation studies performed in this paper. The new KernelSHAP algorithm performs similarly as the improved KernelSHAP method in the state-of-the-art R-package shapr, which samples coalitions with replacement in one of the options

</details>


### [7] [Sparse covariate-driven factorization of high-dimensional brain connectivity with application to site effect correction](https://arxiv.org/abs/2601.09525)
*Rongqian Zhang,Elena Tuzhilina,Jun Young Park*

Main category: stat.ME

TL;DR: 提出SLACC方法，通过稀疏潜在协变量驱动连接组分解来校正多站点神经影像数据中的站点效应


<details>
  <summary>Details</summary>
Motivation: 大规模神经影像研究常涉及多个扫描仪和站点，不同站点的扫描仪、扫描程序等差异会引入人工站点效应，这些效应可能偏倚功能连接性测量。如何利用高维网络结构有效缓解站点效应尚未解决。

Method: 提出SLACC（稀疏潜在协变量驱动连接组）分解方法，这是一种多变量方法，在对应于从脑连接性导出的稀疏秩-1潜在模式中明确参数化协变量效应。开发了惩罚期望最大化（EM）算法进行参数估计，并采用贝叶斯信息准则（BIC）指导优化。

Result: 广泛的模拟验证了SLACC在恢复真实参数和潜在连接模式方面的鲁棒性。应用于自闭症脑成像数据交换（ABIDE）数据集，SLACC展示了其减少站点效应的能力。

Conclusion: SLACC方法能够有效识别和校正多站点神经影像数据中的站点效应，为大规模神经影像研究提供了有效的站点效应校正工具，并提供了公开可用的R包实现。

Abstract: Large-scale neuroimaging studies often collect data from multiple scanners across different sites, where variations in scanners, scanning procedures, and other conditions across sites can introduce artificial site effects. These effects may bias brain connectivity measures, such as functional connectivity (FC), which quantify functional network organization derived from functional magnetic resonance imaging (fMRI). How to leverage high-dimensional network structures to effectively mitigate site effects has yet to be addressed. In this paper, we propose SLACC (Sparse LAtent Covariate-driven Connectome) factorization, a multivariate method that explicitly parameterizes covariate effects in latent subject scores corresponding to sparse rank-1 latent patterns derived from brain connectivity. The proposed method identifies localized site-driven variability within and across brain networks, enabling targeted correction. We develop a penalized Expectation-Maximization (EM) algorithm for parameter estimation, incorporating the Bayesian Information Criterion (BIC) to guide optimization. Extensive simulations validate SLACC's robustness in recovering the true parameters and underlying connectivity patterns. Applied to the Autism Brain Imaging Data Exchange (ABIDE) dataset, SLACC demonstrates its ability to reduce site effects. The R package to implement our method is publicly available.

</details>


### [8] [Semiparametric estimation of GLMs with interval-censored covariates via an augmented Turnbull estimator](https://arxiv.org/abs/2601.08996)
*Andrea Toloba,Klaus Langohr,Guadalupe Gómez Melis*

Main category: stat.ME

TL;DR: 提出GELc方法，用于处理广义线性模型中区间删失协变量的估计问题，基于Turnbull非参数估计器的增强版本，具有一致性和渐近正态性。


<details>
  <summary>Details</summary>
Motivation: 区间删失协变量在生物医学研究中很常见（如时间到事件数据、检测限问题），但相关回归模型的估计方法仍不成熟，需要开发有效的统计方法。

Method: 提出GELc方法：基于Turnbull非参数估计器的增强版本，采用似然基方法估计广义线性模型中区间删失协变量，证明其一致性和渐近正态性。

Result: 模拟研究显示GELc估计器在有限样本下表现良好，置信区间覆盖满意；在两个实际应用（艾滋病临床试验和营养研究）中验证了方法的有效性。

Conclusion: GELc为处理区间删失协变量的广义线性模型提供了可靠估计方法，已实现为R包，适用于生物医学研究中的实际数据分析。

Abstract: Interval-censored covariates are frequently encountered in biomedical studies, particularly in time-to-event data or when measurements are subject to detection or quantification limits. Yet, the estimation of regression models with interval-censored covariates remains methodologically underdeveloped. In this article, we address the estimation of generalized linear models when one covariate is subject to interval censoring. We propose a likelihood-based approach, GELc, that builds upon an augmented version of Turnbull's nonparametric estimator for interval-censored data. We prove that the GELc estimator is consistent and asymptotically normal under mild regularity conditions, with available standard errors. Simulation studies demonstrate favorable finite-sample performance of the estimator and satisfactory coverage of the confidence intervals. Finally, we illustrate the method using two real-world applications: the AIDS Clinical Trials Group Study 359 and an observational nutrition study on circulating carotenoids. The proposed methodology is available as an R package at github.com/atoloba/ICenCov.

</details>


### [9] [LARGE: A Locally Adaptive Regularization Approach for Estimating Gaussian Graphical Models](https://arxiv.org/abs/2601.09686)
*Ha Nguyen,Sumanta Basu*

Main category: stat.ME

TL;DR: 提出LARGE方法，通过节点自适应正则化改进高斯图模型估计，解决GLASSO中单一正则化参数无法处理变量异质性的问题。


<details>
  <summary>Details</summary>
Motivation: GLASSO算法在估计高维高斯图模型时，单一全局正则化参数λ无法适应变量尺度和部分方差的异质性。数据标准化虽然常用但会影响图恢复效果，特别是在计算神经科学等应用中，fMRI数据具有高度异质性，需要节点自适应的正则化方法。

Method: 提出LARGE方法，在GLASSO的块坐标下降步骤中，将节点Lasso回归扩展为联合估计回归系数和误差方差，从而指导节点自适应惩罚参数的学习。

Result: 模拟实验中，LARGE在图恢复方面始终优于基准方法，在不同重复实验中表现出更高的稳定性，在最困难的模拟设置中达到最佳估计精度。在真实fMRI数据集上成功估计了大脑功能连接。

Conclusion: LARGE方法通过节点自适应正则化有效解决了GLASSO中正则化参数选择的问题，提高了图估计和选择的性能，特别适用于具有异质性数据的应用场景。

Abstract: The graphical Lasso (GLASSO) is a widely used algorithm for learning high-dimensional undirected Gaussian graphical models (GGM). Given i.i.d. observations from a multivariate normal distribution, GLASSO estimates the precision matrix by maximizing the log-likelihood with an \ell_1-penalty on the off-diagonal entries. However, selecting an optimal regularization parameter λin this unsupervised setting remains a significant challenge. A well-known issue is that existing methods, such as out-of-sample likelihood maximization, select a single global λand do not account for heterogeneity in variable scaling or partial variances. Standardizing the data to unit variances, although a common workaround, has been shown to negatively affect graph recovery. Addressing the problem of nodewise adaptive tuning in graph estimation is crucial for applications like computational neuroscience, where brain networks are constructed from highly heterogeneous, region-specific fMRI data.
  In this work, we develop Locally Adaptive Regularization for Graph Estimation (LARGE), an approach to adaptively learn nodewise tuning parameters to improve graph estimation and selection. In each block coordinate descent step of GLASSO, we augment the nodewise Lasso regression to jointly estimate the regression coefficients and error variance, which in turn guides the adaptive learning of nodewise penalties. In simulations, LARGE consistently outperforms benchmark methods in graph recovery, demonstrates greater stability across replications, and achieves the best estimation accuracy in the most difficult simulation settings. We demonstrate the practical utility of our method by estimating brain functional connectivity from a real fMRI data set.

</details>


### [10] [Fisher's fundamental theorem and regression in causal analysis](https://arxiv.org/abs/2601.09011)
*Steven A. Frank*

Main category: stat.ME

TL;DR: 论文展示了费希尔基本定理是回归预测在变化情境下分解的一般规则的特例，该分解将总变化分为预测变量差异和回归系数变化两部分，与经济学中的Oaxaca-Blinder分解相同。


<details>
  <summary>Details</summary>
Motivation: 费希尔基本定理在生物学中引发了广泛争议，本文旨在通过展示该定理是一个更一般数学规则的特例来澄清其本质，建立跨学科联系，并为因果分析提供有用工具。

Method: 将费希尔定理重新解释为回归预测变化的分解规则，应用有限差分的乘积法则到回归方程，推导出总变化等于预测变量差异（保持系数不变）与回归系数变化（反映情境变化）之和的一般结果。

Result: 证明了费希尔定理是回归预测变化分解的一般规则的特例，该分解与经济学中的Oaxaca-Blinder分解相同，为理解费希尔定理提供了清晰框架，并建立了生物学与经济学的数学联系。

Conclusion: 认识到底层数学的一般性可以澄清费希尔定理，为因果分析提供有用工具，并揭示跨学科联系，促进不同领域间的知识交流和方法借鉴。

Abstract: Fisher's fundamental theorem describes the change caused by natural selection as the change in gene frequencies multiplied by the partial regression coefficients for the average effects of genes on fitness. Fisher's result has generated extensive controversy in biology. I show that the theorem is a simple example of a general partition for change in regression predictions across altered contexts. By that rule, the total change in a mean response is the sum of two terms. The first ascribes change to the difference in predictor variables, holding constant the regression coefficients. The second ascribes change to altered context, captured by shifts in the regression coefficients. This general result follows immediately from the product rule for finite differences applied to a regression equation. Economics widely applies this same partition, the Oaxaca-Blinder decomposition, as a fundamental tool that can in proper situations be used for causal analysis. Recognizing the underlying mathematical generality clarifies Fisher's theorem, provides a useful tool for causal analysis, and reveals connections across disciplines.

</details>


### [11] [Graph Canonical Coherence Analysis](https://arxiv.org/abs/2601.09038)
*Kyusoon Kim,Hee-Seok Oh*

Main category: stat.ME

TL;DR: 提出图规范相干分析(gCChA)框架，将典型相关分析扩展到图频域中的多元图信号，用于探索两组图信号之间的谱关系。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理图信号特有的离散性、有限性和不规则性特征，需要一种能够在图频域中分析多元图信号关系的框架。

Method: 扩展典型相关分析到图频域，识别最大化相干性的规范图信号对，从谱角度探索两组图信号关系，并分析这些关系在不同图结构尺度上的变化。

Result: 在G20国家经济能源数据集和USPS手写数字数据集上的应用验证了该方法的有效性，展示了其在不同图结构尺度上分析信号关系的能力。

Conclusion: gCChA为图频域中的多元图信号分析提供了有效框架，能够处理图信号的特殊特征，并在实际应用中展示了实用价值。

Abstract: We propose graph canonical coherence analysis (gCChA), a novel framework that extends canonical correlation analysis to multivariate graph signals in the graph frequency domain. The proposed method addresses challenges posed by the inherent features of graphs: discreteness, finiteness, and irregularity. It identifies pairs of canonical graph signals that maximize their coherence, enabling the exploration of relationships between two sets of graph signals from a spectral perspective. This framework shows how these relationships change across different structural scales of the graph. We demonstrate the usefulness of this method through applications to economic and energy datasets of G20 countries and the USPS handwritten digit dataset.

</details>


### [12] [Scalar-on-distribution regression via generalized odds with applications to accelerometry-assessed disability in multiple sclerosis](https://arxiv.org/abs/2601.09126)
*Pratim Guha Niyogi,Muraleetharan Sanjayan,Kathryn C. Fitzgerald,Ellen M. Mowry,Vadim Zipunnikov*

Main category: stat.ME

TL;DR: 提出广义比值框架，将个体特异性分布表示为样本空间任意区域概率的比值，应用于数字健康技术数据，相比传统标量方法显著改善临床预测性能。


<details>
  <summary>Details</summary>
Motivation: 数字健康技术收集的数据分布表示在临床预测中优于标量摘要，尾部行为量化常驱动性能提升。现有方法如风险、生存和剩余寿命表示都是特殊案例，需要统一框架来充分利用分布信息。

Method: 提出广义比值统一框架，通过样本空间任意区域概率的比值表示个体特异性分布。开发基于样条的函数表示和惩罚优化的尺度-比值回归模型，实现高效估计。

Result: 应用于HEAL-MS研究的手腕加速度计数据，广义比值模型在预测扩展残疾状态量表评分方面优于经典标量方法和基于生存的方法，验证了比值分布协变量在数字健康技术数据建模中的价值。

Conclusion: 广义比值框架为数字健康技术数据的分布表示提供了统一方法，显著改善临床预测性能，证明基于比值的分布协变量是建模数字健康技术数据的有效工具。

Abstract: Distributional representations of data collected using digital health technologies have been shown to outperform scalar summaries for clinical prediction, with carefully quantified tail-behavior often driving the gains. Motivated by these findings, we propose a unified generalized odds (GO) framework that represents subject-specific distributions through ratios of probabilities over arbitrary regions of the sample space, subsuming hazard, survival, and residual life representations as special cases. We develop a scale-on-odds regression model using spline-based functional representations with penalization for efficient estimation. Applied to wrist-worn accelerometry data from the HEAL-MS study, generalized odds models yield improved prediction of Expanded Disability Status Scale (EDSS) scores compared to classical scalar and survival-based approaches, demonstrating the value of odds-based distributional covariates for modeling DHT data.

</details>


### [13] [A Multilayer Probit Network Model for Community Detection with Dependent Edges and Layers](https://arxiv.org/abs/2601.09161)
*Dapeng Shi,Haoran Zhang,Tiandong Wang,Junhui Wang*

Main category: stat.ME

TL;DR: 提出一种新的多层网络社区检测方法，能够处理广泛的层间和层内依赖结构，结合多层随机块模型和多元probit模型，开发了约束成对似然方法和交替更新算法。


<details>
  <summary>Details</summary>
Motivation: 现有多层网络社区检测方法大多假设不同层独立或遵循特定依赖结构，且同一层内边独立。这些假设限制了方法在实际复杂网络中的应用，需要能够处理更广泛依赖结构的方法。

Method: 将多层随机块模型与多元probit模型结合，使用约束成对似然方法进行参数估计，开发高效的交替更新算法，能够捕捉层间和层内依赖结构。

Result: 建立了方法的渐近性质，分析了层间和层内依赖对参数估计和社区检测精度的影响。通过模拟网络和真实世界多层贸易网络的数值实验验证了理论结果。

Conclusion: 提出的方法能够有效处理多层网络中复杂的依赖结构，提高了社区检测的准确性，为分析具有复杂依赖关系的现实世界多层网络提供了有力工具。

Abstract: Community detection in multilayer networks, which aims to identify groups of nodes exhibiting similar connectivity patterns across multiple network layers, has attracted considerable attention in recent years. Most existing methods are based on the assumption that different layers are either independent or follow specific dependence structures, and edges within the same layer are independent. In this article, we propose a novel method for community detection in multilayer networks that accounts for a broad range of inter-layer and intra-layer dependence structures. The proposed method integrates the multilayer stochastic block model for community detection with a multivariate probit model to capture the structures of inter-layer dependence, which also allows intra-layer dependence. To facilitate parameter estimation, we develop a constrained pairwise likelihood method coupled with an efficient alternating updating algorithm. The asymptotic properties of the proposed method are also established, with a focus on examining the influence of inter-layer and intra-layer dependences on the accuracy of both parameter estimation and community detection. The theoretical results are supported by extensive numerical experiments on both simulated networks and a real-world multilayer trade network.

</details>


### [14] [White noise testing for functional time series via functional quantile autocorrelation](https://arxiv.org/abs/2601.09371)
*Ángel López-Oriona,Ying Sun,Hanlin Shang*

Main category: stat.ME

TL;DR: 提出基于分位数自相关的非线性检验方法，用于检测函数时间序列的序列依赖性，无需矩条件，对异常值和复杂非线性依赖具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于自协方差核的方法在处理函数时间序列时，对异常值和复杂非线性依赖不够鲁棒。需要一种更稳健的方法来捕捉无限维函数数据中的时间依赖性。

Method: 基于函数分位数自相关框架，利用分位数偏移集来捕捉时间依赖性。提出综合检验统计量，研究其在已知和估计分位数曲线下的渐近性质，建立渐近分布和一致性。

Result: 方法在模拟和高频金融函数时间序列应用中表现出色，能可靠检测复杂序列依赖性，相对于现有检验方法具有更高的功效。无需矩条件即可保证检验有效性。

Conclusion: 扩展了函数时间序列分析工具包，为传统方法可能失效的场景提供了稳健的推断框架，特别适用于包含异常值和复杂非线性依赖的情况。

Abstract: We introduce a novel class of nonlinear tests for serial dependence in functional time series, grounded in the functional quantile autocorrelation framework. Unlike traditional approaches based on the classical autocovariance kernel, the functional quantile autocorrelation framework leverages quantile-based excursion sets to robustly capture temporal dependence within infinite-dimensional functional data, accommodating potential outliers and complex nonlinear dependencies. We propose omnibus test statistics and study their asymptotic properties under both known and estimated quantile curves, establishing their asymptotic distribution and consistency under mild assumptions. In particular, no moment conditions are required for the validity of the tests. Extensive simulations and an application to high-frequency financial functional time series demonstrate the methodology's effectiveness, reliably detecting complex serial dependence with superior power relative to several existing tests. This work expands the toolkit for functional time series, providing a robust framework for inference in settings where traditional methods may fail.

</details>


### [15] [Tools to help patients and other stakeholders' input into choice of estimand and intercurrent event strategy in randomised trials](https://arxiv.org/abs/2601.09442)
*Joanna Hindley,Charlotte Hartley,Jennifer Hellier,Kate Sturgeon,Sophie Greenwood,Ian Newsome,Katherine Barrett,Debs Smith,Tra My Pham,Dongquan Bi,Beatriz Goulao,Suzie Cro,Brennan C Kahan*

Main category: stat.ME

TL;DR: 开发三种工具（视频、信息图、可编辑PPT）来帮助研究人员与患者等利益相关者就临床试验中的估计目标选择和中间事件策略进行有效沟通。


<details>
  <summary>Details</summary>
Motivation: 临床试验中估计目标的选择会影响结果对患者、临床医生和政策制定者的相关性，但相关讨论涉及复杂概念和技术语言，阻碍了利益相关者的参与。

Method: 开发了三种沟通工具：1）解释估计目标概念和五种中间事件处理策略的视频；2）概述这五种策略的信息图；3）可编辑的PPT幻灯片，可填入具体试验细节以促进针对特定试验的讨论。

Result: 这些工具能够帮助试验团队与患者及其他利益相关者就随机试验中最佳估计目标和中间事件策略的选择展开对话。

Conclusion: 通过开发易于理解的沟通工具，可以克服技术语言障碍，促进利益相关者参与临床试验估计目标的决策过程，从而提高试验结果的相关性和实用性。

Abstract: Estimands can help to clarify the research questions being addressed in randomised trials. Because the choice of estimand can affect how relevant trial results are to patients and other stakeholders, such as clinicians or policymakers, it is important for them to be involved in these decisions. However, there are barriers to having these conversations. For instance, discussions around how intercurrent events should be addressed in the estimand definition typically involve complex concepts as well as technical language. We three tools to facilitate conversations between researchers and patients and other stakeholders about the choice of estimand and intercurrent event strategy: (i) a video explaining the concept of an estimand and the five different ways that intercurrent events can be incorporated into the estimand definition; (ii) an infographic outlining these five strategies; and (iii) an editable PowerPoint slide which can be completed with trial-specific details to facilitate conversations around choice of estimand for a particular trial. These resources can help to start conversations between the trial team and patients and other stakeholders about the best choice of estimand and intercurrent event strategies for a randomised trial.

</details>


### [16] [How to interpret hazard ratios](https://arxiv.org/abs/2601.09571)
*Jonathan W. Bartlett,Dominic Magirr,Tim P. Morris*

Main category: stat.ME

TL;DR: 本文回顾了对风险比因果解释的批评，认为风险比在比例风险假设成立时仍具有有效的因果解释，但承认其他效应度量可能更适合描述时间-事件结局的效应。


<details>
  <summary>Details</summary>
Motivation: 近年来，一些研究者认为风险比缺乏因果解释，即使在随机试验中且比例风险假设成立时也是如此。考虑到风险比在时间-事件数据分析中的普遍应用，这一观点令人担忧。

Method: 本文采用文献回顾和理论分析的方法，系统梳理了对风险比因果解释的批评，并阐述了作者认为风险比应如何正确解释。

Result: 作者认为风险比在比例风险假设成立时仍具有有效的因果解释，但同时也承认其他效应度量可能更适合描述暴露或治疗对时间-事件结局的影响。

Conclusion: 风险比保留了有效的因果解释，但研究者应考虑使用替代效应度量来更恰当地描述时间-事件结局的效应。

Abstract: The hazard ratio, typically estimated using Cox's famous proportional hazards model, is the most common effect measure used to describe the association or effect of a covariate on a time-to-event outcome. In recent years the hazard ratio has been argued by some to lack a causal interpretation, even in randomised trials, and even if the proportional hazards assumption holds. This is concerning, not least due to the ubiquity of hazard ratios in analyses of time-to-event data. We review these criticisms, describe how we think hazard ratios should be interpreted, and argue that they retain a valid causal interpretation. Nevertheless, alternative measures may be preferable to describe effects of exposures or treatments on time-to-event outcomes.

</details>


### [17] [Smoothing spline density estimation from doubly truncated data](https://arxiv.org/abs/2601.09576)
*David Bamio,Jacobo de Uña-Álvarez*

Main category: stat.ME

TL;DR: 研究双截断数据的平滑样条密度估计方法，通过修正惩罚似然函数来纠正抽样偏差，并在理论和实践中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 在天文学、生存分析和流行病学等多个领域中，双截断数据经常出现。双截断通常会导致抽样偏差，使得普通估计量可能不一致，因此需要专门的方法来处理这类数据。

Method: 提出了一种针对双截断数据的平滑样条密度估计方法。通过修正惩罚似然函数来考虑抽样偏差，并讨论了估计量的理论性质。通过模拟评估实际性能，并与核密度平滑方法进行比较。

Result: 通过模拟评估了所提方法的实际性能，并使用两个真实数据集进行说明性分析。与核密度平滑方法的比较显示，所提方法在双截断数据情况下表现更好。

Conclusion: 提出的平滑样条密度估计方法能够有效处理双截断数据，纠正抽样偏差，在理论和实践中都表现出良好的性能，适用于多个需要处理双截断数据的领域。

Abstract: In Astronomy, Survival Analysis and Epidemiology, among many other fields, doubly truncated data often appear. Double truncation generally induces a sampling bias, so ordinary estimators may be inconsistent. In this paper, smoothing spline density estimation from doubly truncated data is investigated. For this purpose, an appropriate correction of the penalized likelihood that accounts for the sampling bias is considered. The theoretical properties of the estimator are discussed, and its practical performance is evaluated through simulations. Two real datasets are analyzed using the proposed method for illustrative purposes. Comparison to kernel density smoothing is included.

</details>
