<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 4]
- [stat.AP](#stat.AP) [Total: 5]
- [stat.ME](#stat.ME) [Total: 15]
- [stat.CO](#stat.CO) [Total: 2]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification](https://arxiv.org/abs/2512.07888)
*Fahad Mostafa,Hafiz Khan*

Main category: stat.ML

TL;DR: 提出FRF-ACS方法，通过自适应成本敏感分割和混合采样策略，解决不平衡函数数据分类问题，显著提高少数类召回率。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林在处理函数数据时无法捕捉其内在结构，且在类别不平衡情况下难以检测少数类，需要专门针对函数数据不平衡分类的解决方案。

Method: 使用基展开和函数主成分分析表示曲线，采用动态成本敏感分割准则调整节点权重，结合函数SMOTE和加权自举的混合采样策略，并使用曲线特定相似度度量进行叶节点分配。

Result: 在合成和真实数据集（包括生物医学信号和传感器轨迹）上的实验表明，FRF-ACS相比现有函数分类器和不平衡处理技术，显著提高了少数类召回率和整体预测性能。

Conclusion: FRF-ACS为高维函数数据分析提供了可扩展、可解释的解决方案，特别适用于少数类检测至关重要的领域。

Abstract: Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical.

</details>


### [2] [Provable Diffusion Posterior Sampling for Bayesian Inversion](https://arxiv.org/abs/2512.08022)
*Jinyuan Chang,Chenguang Duan,Yuling Jiao,Ruoxuan Li,Jerry Zhijian Yang,Cheng Yuan*

Main category: stat.ML

TL;DR: 提出了一种基于扩散的PnP后验采样方法，使用概率传输和热启动策略，通过Langevin动力学和蒙特卡洛估计器近似后验分数，提供非渐近误差界并在逆问题上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有后验采样方法常依赖启发式近似，难以处理复杂多模态目标分布。本文旨在开发一种更精确、理论保证的后验采样方法，能够有效处理复杂逆问题中的后验分布。

Method: 1. 在PnP框架下构建从易采样终端分布到目标后验的概率传输；2. 使用热启动策略初始化粒子；3. 通过Langevin动力学生成粒子，开发蒙特卡洛估计器近似后验分数；4. 从数据中学习Langevin动力学的分数函数，捕捉先验分布的结构特征。

Result: 1. 提供了非渐近误差界，证明方法对复杂多模态目标后验分布收敛；2. 明确量化了后验分数估计、热启动初始化和后验采样过程的误差；3. 分析了先验分数匹配误差和贝叶斯逆问题条件数对性能的影响；4. 数值实验验证了方法在多种逆问题上的有效性。

Conclusion: 本文提出了一种理论保证的扩散基PnP后验采样方法，能够有效处理复杂多模态分布，为逆问题中的后验推断提供了新的解决方案。

Abstract: This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems.

</details>


### [3] [Worst-case generation via minimax optimization in Wasserstein space](https://arxiv.org/abs/2512.08176)
*Xiuyuan Cheng,Yao Xie,Linglingzhi Zhu,Yunqin Zhu*

Main category: stat.ML

TL;DR: 提出基于Wasserstein空间连续概率分布min-max优化的最坏情况生成框架，利用Brenier定理将最不利分布表征为传输映射的推前，实现连续可表达的风险诱导生成，避免了传统离散DRO方法的可扩展性、泛化性和最坏情况推断问题。


<details>
  <summary>Details</summary>
Motivation: 最坏情况生成在评估系统鲁棒性和压力测试中至关重要，但传统离散分布鲁棒优化方法存在可扩展性差、泛化能力有限和最坏情况推断成本高等问题，需要开发更有效的连续分布生成框架。

Method: 基于Wasserstein空间的min-max优化框架，利用Brenier定理将最不利分布表征为连续参考测度传输映射的推前；提出单循环更新决策模型和传输映射的GDA方案；使用神经网络参数化传输映射，通过匹配传输训练样本实现免模拟训练。

Result: 在温和正则性假设下建立了全局收敛保证（可能无需凸凹性），数值实验在合成和图像数据上验证了该方法作为风险诱导最坏情况生成器的有效性。

Conclusion: 提出的连续分布最坏情况生成框架克服了传统离散DRO方法的局限性，提供了更具表达力、可扩展且计算高效的风险诱导生成方法，适用于机器学习、电网和医疗预测等多种应用场景。

Abstract: Worst-case generation plays a critical role in evaluating robustness and stress-testing systems under distribution shifts, in applications ranging from machine learning models to power grids and medical prediction systems. We develop a generative modeling framework for worst-case generation for a pre-specified risk, based on min-max optimization over continuous probability distributions, namely the Wasserstein space. Unlike traditional discrete distributionally robust optimization approaches, which often suffer from scalability issues, limited generalization, and costly worst-case inference, our framework exploits the Brenier theorem to characterize the least favorable (worst-case) distribution as the pushforward of a transport map from a continuous reference measure, enabling a continuous and expressive notion of risk-induced generation beyond classical discrete DRO formulations. Based on the min-max formulation, we propose a Gradient Descent Ascent (GDA)-type scheme that updates the decision model and the transport map in a single loop, establishing global convergence guarantees under mild regularity assumptions and possibly without convexity-concavity. We also propose to parameterize the transport map using a neural network that can be trained simultaneously with the GDA iterations by matching the transported training samples, thereby achieving a simulation-free approach. The efficiency of the proposed method as a risk-induced worst-case generator is validated by numerical experiments on synthetic and image data.

</details>


### [4] [Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis](https://arxiv.org/abs/2512.08601)
*Orit Davidovich,Shimrit Shtern,Segev Wasserkrug,Nimrod Megiddo*

Main category: stat.ML

TL;DR: 该论文提出了一个统一框架，将组合优化问题建模为马尔可夫决策过程，并使用强化学习技术求解，为RL在CO问题中的应用提供了理论支撑。


<details>
  <summary>Details</summary>
Motivation: 自1990年代以来，大量实证研究使用神经网络等统计模型作为组合优化问题的学习启发式方法，但缺乏理论基础。虽然强化学习在CO问题上表现出色，但缺乏理论支撑，需要建立理论框架来理解RL在CO问题中的成功与局限。

Method: 提出统一框架，将组合优化问题建模为马尔可夫决策过程，并应用强化学习技术求解。建立了易于测试的假设条件，确保CO问题可表述为等价的无折扣MDP，并提供收敛性分析，包括批量大小增长速率、梯度下降步数要求等。

Result: 建立了CO问题作为MDP的理论基础，提供了基于价值的RL技术收敛到近似解的保证条件，分析了最优性间隙与问题参数和RL精度的关系，阐明了状态空间嵌入选择的重要性，解释了深度Q学习算法在此类问题中的成功与局限。

Conclusion: 该研究为强化学习在组合优化问题中的应用提供了坚实的理论基础，建立了理论框架和收敛保证，有助于理解现有RL方法的成功机制，并为未来算法设计提供指导。

Abstract: Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [5] [Defining 3-dimensional marine provinces with phytoplankton compositions](https://arxiv.org/abs/2512.08035)
*Rafael Catoia Pulgrossi,Nathan L R Williams,Yubin Raut,Jed Fuhrman,Sangwon Hyun*

Main category: stat.AP

TL;DR: 开发了名为bioprovince的新算法，利用浮游植物分布数据在三维空间（纬度、经度、深度）中定义海洋生物省份，相比传统二维方法能提供更精细的生态分区。


<details>
  <summary>Details</summary>
Motivation: 传统海洋省份划分通常只基于经纬度二维空间，缺乏精细的生物数据和深度维度。需要开发能整合三维空间和生物组成数据的方法来更准确地定义海洋生态分区。

Method: 开发了bioprovince算法：1）首先对生物组成样本进行聚类，识别空间上连贯的样本组；2）基于环境相似性在三维空间网格中进行灵活的生物省份预测。应用该算法分析了太平洋五个南北向深度解析航线的浮游植物ASV数据。

Result: 在海洋表层，该方法与传统Longhurst省份划分一致；但ASV提供的更高分类分辨率将某些传统省份划分为更小的区域。最重要的是，该方法成功识别了深度维度的分区，在赤道区域与真光层底部有显著对应关系。

Conclusion: bioprovince算法能够划定三维生物省份，为海洋浮游植物生态学和生物地理学提供新的生态学解释。该方法不仅适用于海洋浮游生物，还可广泛应用于其他具有三维空间生物组成数据的环境。

Abstract: Marine provinces rarely include fine-resolution biological data, and are often defined spatially across only latitude and longitude. Therefore, we aimed to determine how phytoplankton distributions define marine provinces across 3-dimensions (i.e., latitude, longitude, and depth). To do this, we developed a new algorithm called \texttt{bioprovince} which can be applied to compositional biological data. The algorithm first clusters compositional samples to identify spatially coherent groups of samples, then makes flexible province predictions in the broader 3d spatial grid based on environmental similarity. We applied \texttt{bioprovince} to phytoplankton Amplicon Sequencing Variants (ASVs) from five, depth-resolved ocean transects spanning north-south in the Pacific Ocean. In the surface layer of the ocean, our method agreed well with traditional Longhurst provinces. In some cases, the method revealed that with more granular taxonomic resolution afforded by ASVs, traditional Longhurst provinces were divided into smaller zones. Also, one of the major advances of this method is its ability to incorporate a third dimension, depth. Indeed, our analysis found significant depth-wise partitions throughout the Pacific with remarkable agreement in the equatorial region with the base of the euphotic zone. Our algorithm's ability to delineate 3-dimensional bioprovinces will enable scientists to discover new ecological interpretations of marine phytoplankton ecology and biogeography. Furthermore, as compositional biological data inherently exists in three spatial dimensions in nature, bioprovince is broadly applicable beyond marine plankton, offering a more holistic perspective on biological provinces across diverse environments.

</details>


### [6] [Distribution of Gaps in Multi-lane Orderly and Disorderly Traffic Streams](https://arxiv.org/abs/2512.08585)
*Ankita Sharma,Partha Chakroborty,Pranamesh Chakraborty*

Main category: stat.AP

TL;DR: 本文提出了一种基于更新过程理论的分析框架，用于确定多车道有序和无序交通流中的车头时距分布，并开发了最大似然参数估计方法，通过实际数据验证了该分布的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究车头时距接受行为需要了解对向车流的车头时距分布。在交通仿真广泛应用的背景下，需要能够统计上模拟真实车流的随机生成方法，而无序车流的生成需要车头时距分布信息。现有文献对多车道有序和无序车流的车头时距分布研究不足。

Method: 基于更新过程理论构建分析框架来确定多车道有序和无序车流的车头时距分布。开发了基于最大似然估计的参数估计过程，用于估计分析推导出的分布参数。

Result: 使用来自三个不同地点的实际车头时距数据（涵盖有序和无序车流）验证了所提出的分布函数能够很好地描述观测到的车头时距分布。

Conclusion: 成功开发了一个分析框架来指定多车道有序和无序车流的车头时距分布，该方法在实际应用中表现出良好的拟合效果，为交通仿真和车头时距接受行为研究提供了理论基础。

Abstract: To study gap acceptance behaviour one needs the distribution (or probability density function) of gaps in the opposing stream. Further, in these times of widespread availability of large computing powers, traffic simulation has emerged as a popular analysis and design tool. Such simulations rely on randomly generating the arriving vehicles in a way that statistically resembles real-world streams. The generation process for disorderly streams requires information on gap distributions. A study of past literature reveals that very little work has been done to determine the distribution of gaps on multi-lane orderly and disorderly streams. This study aims to develop an analytical framework to specify the distribution of gaps for such streams. This analytical framework is built using the Renewal Process Theory. A maximum likelihood based process for the estimation of the parameters of the analytically derived distribution is also described. Later, real-world gap data from three different sites covering orderly and disorderly streams are used to show how the derived distribution function (using the proposed method) ably describes the observed gap distributions.

</details>


### [7] [Genetic Regression Analysis of Human Brain Connectivity Using an Efficient Estimator of Genetic Covariance](https://arxiv.org/abs/2512.08756)
*Keshav Motwani,Ali Shojaie,Ariel Rokem,Eardi Lila*

Main category: stat.AP

TL;DR: 开发了一个回归分析框架来表征遗传对结构和功能连接组之间关系的贡献，发现环境因素掩盖了遗传编码的结构-功能关系


<details>
  <summary>Details</summary>
Motivation: 虽然遗传学解释了结构和功能连接组的大量个体差异，但表征遗传如何塑造两者关系仍然具有挑战性，因为这种关联被独特的环境暴露所掩盖

Method: 开发了一个回归分析框架，包括约束矩估计器（比现有方法快几个数量级）和正则化估计方法（岭回归、LASSO、张量回归）来分析遗传对结构-功能连接关系的贡献

Result: 在遗传水平上，功能连接可以从结构中度预测（最大R²=0.34），但在观察数据中几乎不可预测（最大R²=0.03），表明独特环境因素掩盖了强大的遗传编码结构-功能关系

Conclusion: 环境因素显著干扰了观察到的结构-功能连接关系，而遗传分析揭示了更强的潜在关联，这对理解大脑网络组织的遗传基础具有重要意义

Abstract: Non-invasive measurements of the human brain using magnetic resonance imaging (MRI) have significantly improved our understanding the brain's network organization by enabling measurement of anatomical connections between brain regions (structural connectivity) and their coactivation (functional connectivity). Heritability analyses have established that genetics account for considerable intersubject variability in structural and functional connectivity. However, characterizing how genetics shape the relationship between structural and functional connectomes remains challenging, since this association is obscured by unique environmental exposures in observed data. To address this, we develop a regression analysis framework that enables characterization of the relationship between latent genetic contributions to structural and functional connectivity. Implementing the proposed framework requires estimating genetic covariance matrices in multivariate random effects models, which is computationally intractable for high-dimensional connectome data using existing methods. We introduce a constrained method-of-moments estimator that is several orders of magnitude faster than existing methods without sacrificing estimation accuracy. For the genetic regression analysis, we develop regularized estimation approaches, including ridge, lasso, and tensor regression. Applying our method to Human Connectome Project data, we find that functional connectivity is moderately predictable from structure at the genetic level (max R^2 = 0.34), though it is not directly predictable in the observed data (max R^2 = 0.03). This stark contrast suggests that unique environmental factors mask strong genetically-encoded structure-function relationships.

</details>


### [8] [Commanding the Foul Shot: A New Ensemble of Free Throw Metrics](https://arxiv.org/abs/2512.08824)
*Jake McGrath,Amanda Glazer,Vanna Bushong,Michelle Nguyen,Kirk Goldsberry*

Main category: stat.AP

TL;DR: 论文利用NBA的Hawk-Eye追踪系统数据，开发了一套新的罚球评估指标，比传统的命中/未命中统计更能有效衡量球员技能。


<details>
  <summary>Details</summary>
Motivation: NBA采用Hawk-Eye系统后，能够每秒60次捕捉球员和篮球的高分辨率3D姿态数据。将这些数据与投篮、传球、篮板等关键事件关联，为NBA分析开启了新时代。传统命中/未命中统计无法全面评估罚球质量，需要更精细的指标来捕捉球员技能。

Method: 1. 引入"command"指标，量化罚球质量，通过测量投篮者在篮筐靶心附近的准确性和精度来评估；2. 定义基于出手的指标，评估释放速度、角度和3D位置的一致性；3. 开发物理模型识别导致命中的发射条件范围，并确定哪些发射条件对微小扰动最稳健。

Result: 新指标能更有效地捕捉罚球技能，识别出"安全"的发射区域，解释为什么某些球员（如斯蒂芬·库里）在罚球方面表现出色。具有更好"手感"（更一致的发射动力学）的球员表现出更强的command，因为他们能可靠地控制投篮轨迹。

Conclusion: 该框架为球员发展提供了可操作的见解，证明基于追踪数据的精细指标比传统统计更能有效评估罚球技能，开启了篮球分析的新方向。

Abstract: With the NBA's adoption of in-game limb tracking in 2023, Sony's Hawk-Eye system now captures high-resolution, 3D poses of players and the ball 60 times per second. Linking these data to key events such as shots, passes, and rebounds opens a new era in NBA analytics. Here, we leverage Hawk-Eye tracking to introduce a novel ensemble of metrics for evaluating free-throw shooting and demonstrate that our framework captures skill more effectively than traditional make-or-miss statistics. Inspired by baseball analytics, we introduce command, which quantifies the quality of a free throw by measuring a shooter's accuracy and precision near the basket's bullseye. This metric recognizes that some makes (or misses) are better than others and captures a player's ability to execute quality attempts consistently. To identify what drives command, we define launch-based metrics assessing consistency in release velocity, angle, and 3D position. Players with greater touch -- i.e., more consistent launch dynamics -- exhibit stronger command as they can reliably control their shot trajectory. Finally, we develop a physics model to identify the range of launch conditions that result in a make and to determine which launch conditions are most robust to small perturbations. This framework reveals "safe" launch regions and explains why certain players, such as Steph Curry, excel at free throws, providing actionable insights for player development.

</details>


### [9] [Multifractal behavior of price changes in the Green Bonds funds](https://arxiv.org/abs/2512.08886)
*Wenderson Gomes Barbosa,Kerolly Kedma Felix do Nascimento,Fábio Sandro dos Santos,Silvio Fernando Alves Xavier Júnior,Tiago A. E. Ferreira*

Main category: stat.AP

TL;DR: 使用MFDFA方法分析35个绿色债券基金价格变化，发现其具有持续性、高多重分形特征，与一般金融资产相似


<details>
  <summary>Details</summary>
Motivation: 气候变化推动市场寻求新的融资方式，绿色债券作为支持可持续项目的金融工具应运而生，需要研究其价格行为的特征

Method: 采用多重分形去趋势波动分析（MFDFA）方法，分析35个绿色债券基金的每日价格变化时间序列

Result: 价格变化表现出持续性行为和高度多重分形特征，波动较大；35个时间序列中仅一个显示异常结果，表明基金行为非常相似；通过序列重排可显著降低多重分形性

Conclusion: 绿色债券基金表现出与其他金融资产相似的多重分形行为特征

Abstract: Climate change has driven the market to seek new ways of raising funds to mitigate its effects. One such innovation is the emergence of Green Bonds financial assets specifically designed to support sustainable projects. This study explores the fractal behavior of daily price changes in thirty-five Green Bond funds using the Multifractal Detrended Fluctuation Analysis (MFDFA) method. Our results indicate that price changes exhibit persistent behavior and high multifractality, characterized by large fluctuations. Only one of the thirty-five time series analyzed showed an outlier result, suggesting that the funds display very similar behavior. By shuffling the series, we were able to reduce multifractality significantly. These findings suggest that Green Bond funds exhibit multifractal behavior typical of other financial assets.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [10] [Uncertainty quantification for mixed membership in multilayer networks with degree heterogeneity using Gaussian variational inference](https://arxiv.org/abs/2512.08146)
*Fangzheng Xie,Hsin-Hsiung Huang*

Main category: stat.ME

TL;DR: 提出基于ML-DCMM模型的贝叶斯变分推断框架，用于多层网络社区结构的不确定性量化，通过谱辅助似然和高斯变分推断实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 多层网络分析需要量化社区结构的不确定性，同时跨层整合信息并适应层间异质性。现有方法在信息整合和不确定性量化方面存在不足。

Method: 基于多层度校正混合成员（ML-DCMM）模型，提出谱辅助似然的贝叶斯推断框架，开发基于随机梯度下降的高斯变分推断算法。

Result: 理论分析建立了变分Bernstein-von Mises定理，为混合成员置信集提供频率保证。在美国机场纵向网络应用中表现出稳健估计、自然不确定性量化和优于现有方法的性能。

Conclusion: 该方法为多层网络社区分析提供了计算高效、理论保证的贝叶斯推断框架，能有效量化不确定性并整合跨层信息。

Abstract: Analyzing multilayer networks is central to understanding complex relational measurements collected across multiple conditions or over time. A pivotal task in this setting is to quantify uncertainty in community structure while appropriately pooling information across layers and accommodating layer-specific heterogeneity. Building on the multilayer degree-corrected mixed-membership (ML-DCMM) model, which captures both stable community membership profiles and layer-specific vertex activity levels, we propose a Bayesian inference framework based on a spectral-assisted likelihood. We then develop a computationally efficient Gaussian variational inference algorithm implemented via stochastic gradient descent. Our theoretical analysis establishes a variational Bernstein--von Mises theorem, which provides a frequentist guarantee for using the variational posterior to construct confidence sets for mixed memberships. We demonstrate the utility of the method on a U.S. airport longitudinal network, where the procedure yields robust estimates, natural uncertainty quantification, and competitive performance relative to state-of-the-art methods.

</details>


### [11] [Bayesian Semiparametric Joint Dynamic Model for Multitype Recurrent Events and a Terminal Event](https://arxiv.org/abs/2512.07973)
*Mithun Kumar Acharjee,AKM Fazlur Rahman*

Main category: stat.ME

TL;DR: 提出贝叶斯半参数联合动态模型，分析多类型复发事件与终点事件（死亡）的关系，提供基于历史事件效应的风险评估


<details>
  <summary>Details</summary>
Motivation: 在生物医学研究中，心肌梗死、中风、心衰等复发事件常导致死亡等终点事件。理解多类型复发事件与终点事件之间的关系对于开发延长生存期的干预措施至关重要

Method: 提出贝叶斯半参数联合动态模型，使用伽马过程先验作为基线累积风险函数的非参数先验，参数先验用于协变量和脆弱性。模型提供累积风险函数和脆弱性的解析闭式估计器

Result: 通过广泛模拟评估模型性能，并应用于抗高血压和降脂治疗预防心脏病发作试验（ALLHAT）。提供基于历史事件效应的急性和慢性心血管复发事件风险评估

Conclusion: 该模型为临床医生提供了心血管疾病预防和治疗的新信息，支持基于过去事件效应的风险评估，有助于延长终点事件发生时间

Abstract: In many biomedical research, recurrent events such as myocardial infraction, stroke, and heart failure often result in a terminal outcome such as death. Understanding the relationship among the multi-type recurrent events and terminal event is essential for developing interventions to prolong the terminal event such as death. This study introduces a Bayesian semiparametric joint dynamic model for type-specific hazards that quantifies how the type-specific event history dynamically changes the intensities of each recurrent event type and the terminal event over calendar time. The framework jointly captures unmeasured heterogeneity through a shared frailty term, cumulative effects of past recurrent events on themselves and terminal events, and the effects of covariates. Gamma process priors (GPP) are used as a nonparametric prior for the baseline cumulative hazard function (CHF) and parametric priors for covariates and frailty. For a more accurate risk assessment, this model provides an analytical closed-form estimator of cumulative hazard functions (CHF) and frailties. The Breslow-Aalen-type estimators of CHFs are special cases of our estimators when the precision parameters are set to zero. We evaluate the performance of the model through extensive simulations and apply the method to the Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack Trial (ALLHAT). The analysis offers a practical past event effect based risk assessment for acute and chronic cardiovascular recurrent events with a terminal end point death and provides new information to support the prevention and treatment of cardiovascular disease to clinicians.

</details>


### [12] [Bayesian Semiparametric Mixture Cure (Frailty) Models](https://arxiv.org/abs/2512.08173)
*Fatih Kızılaslan,Valeria Vitelli*

Main category: stat.ME

TL;DR: 提出用于半参数混合治愈模型的层次贝叶斯框架，包含脆弱性成分选项，使用MCMC采样，通过模拟研究和实际数据验证性能


<details>
  <summary>Details</summary>
Motivation: 混合治愈模型在生存分析中越来越受欢迎，特别是当存在治愈患者子集时。比例风险混合治愈模型能更准确地表示长期生存动态，但需要更灵活的框架来处理未观察到的异质性

Method: 提出新颖的层次贝叶斯框架用于半参数混合治愈模型，包含脆弱性成分选项（可包含或不包含）。使用马尔可夫链蒙特卡洛方法从后验分布采样，采用受贝叶斯Lasso启发的层次结构

Result: 通过多种场景的综合模拟研究评估了所提模型的性能和鲁棒性。使用多种标准进行贝叶斯模型比较和评估。将方法应用于两个著名数据集：E1690黑色素瘤试验和结肠癌临床试验

Conclusion: 提出的层次贝叶斯框架为混合治愈模型提供了灵活且稳健的方法，能够处理患者异质性，在模拟和实际数据应用中表现良好

Abstract: In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial.

</details>


### [13] [ADOPT: Additive Optimal Transport Regression](https://arxiv.org/abs/2512.08118)
*Wookyeong Song,Hans-Georg Müller*

Main category: stat.ME

TL;DR: 提出了一种用于度量空间响应的加性最优传输回归框架，通过最优测地传输引入加性结构，适用于概率分布、SPD矩阵和球面数据等多种响应类型。


<details>
  <summary>Details</summary>
Motivation: 度量空间响应回归分析日益重要，但传统加性回归方法无法直接扩展到非欧几里得响应空间，因为缺少向量空间运算。需要开发适用于一般度量空间响应的加性回归方法。

Method: 提出加性最优传输回归框架，将Wasserstein空间的最优传输概念扩展到一般测地度量空间，通过最优测地传输实现加性结构，适用于概率分布、SPD矩阵和球面数据。

Result: 建立了一个统一框架，能够处理多种度量空间响应，包括概率分布、对称正定矩阵和球面数据，并通过静息态fMRI脑成像数据的相关矩阵验证了方法的实用性。

Conclusion: 该方法成功地将加性回归扩展到一般度量空间响应，通过最优测地传输克服了非欧几里得空间的运算限制，为高维预测变量下的可解释建模提供了有效工具。

Abstract: Regression analysis for responses taking values in general metric spaces has received increasing attention, particularly for settings with Euclidean predictors $X \in \mathbb{R}^p$ and non-Euclidean responses $Y$ in metric spaces. While additive regression is a powerful tool for enhancing interpretability and mitigating the curse of dimensionality in the presence of multivariate predictors, its direct extension is hindered by the absence of vector space operations in general metric spaces. We propose a novel framework for additive optimal transport regression, which incorporates additive structure through optimal geodesic transports. A key idea is to extend the notion of optimal transports in Wasserstein spaces to general geodesic metric spaces. This unified approach accommodates a wide range of responses, including probability distributions, symmetric positive definite (SPD) matrices with various metrics and spherical data. The practical utility of the method is illustrated with correlation matrices derived from resting state fMRI brain imaging data.

</details>


### [14] [Non-parametric assessment of the calibration of individualized treatment effects](https://arxiv.org/abs/2512.08140)
*Mohsen Sadatsafavi,Jeroen Hoogland,Thomas P. A. Debray,John Petkau*

Main category: stat.ME

TL;DR: 提出非参数方法评估个体化治疗效果（ITE）模型的中度校准，解决反事实数据不可观测和连续预测值条件响应函数量化两大挑战，基于随机化试验数据建立随机过程进行推断。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估ITE模型中度校准的推断方法，主要面临两大挑战：反事实响应无法观测，以及连续预测值的条件响应函数量化需要正则化或参数建模。

Method: 提出两种非参数方法：基于预测风险的条件方法和基于边际替代的边际方法，通过构建服从函数中心极限定理的累积预测误差随机过程，利用布朗运动性质进行渐近推断。

Result: 数值模拟证实两种方法具有良好的性质，能够检测不同形式的错误校准。案例研究提供了图形展示和结果解释的实用建议。

Conclusion: 无需正则化技术或治疗响应函数形式假设，即可评估预测ITE的中度校准，为随机化试验数据提供了新颖的数值、图形和推断方法。

Abstract: An important aspect of the performance of algorithms that predict individualized treatment effects (ITE) is moderate calibration, i.e., the average treatment effect among individuals with predicted treatment effect of z being equal to z. The assessment of moderate calibration is a challenging task on two fronts: counterfactual responses are unobserved, and quantifying the conditional response function for models that generate continuous predicted values requires regularization or parametric modeling. Perhaps because of these challenges, there is currently no inferential method for the null hypothesis that an ITE model is moderately calibrated in a population. In this work, we propose non-parametric methods for the assessment of moderate calibration of ITE models for binary outcomes using data from a randomized trial. These methods simultaneously resolve both challenges, resulting in novel numerical, graphical, and inferential methods for the assessment of moderate calibration. The key idea is to formulate a stochastic process for the cumulative prediction errors that obeys a functional central limit theorem, enabling the use of the properties of Brownian motion for asymptotic inference. We propose two approaches to construct this process from a sample: a conditional approach that relies on predicted risks (often an output of ITE models), and a marginal approach based on replacing the cumulative conditional expected value and variance terms with their marginal counterparts. Numerical simulations confirm the desirable properties of both approaches and their ability to detect miscalibration of different forms. We use a case study to provide practical suggestions on graphical presentation and the interpretation of results. Moderate calibration of predicted ITEs can be assessed without requiring regularization techniques or making assumptions about the functional form of treatment response.

</details>


### [15] [Wishart kernel density estimation for strongly mixing time series on the cone of positive definite matrices](https://arxiv.org/abs/2512.08232)
*Léo R. Belzile,Christian Genest,Frédéric Ouimet,Donald Richards*

Main category: stat.ME

TL;DR: 提出了一种用于正定矩阵锥密度估计的Wishart核密度估计器，该估计器边界感知且能减轻传统KDE的边界偏差，同时易于实现。


<details>
  <summary>Details</summary>
Motivation: 在正定矩阵空间进行密度估计时，传统核密度估计器存在边界偏差问题，需要开发边界感知的估计方法。这是该空间上密度估计的首个研究。

Method: 引入基于Wishart分布的核密度估计器，建立了在Lebesgue测度和适当混合条件下的均方误差、一致性和渐近正态性理论性质。

Result: Wishart KDE在模拟研究中优于基于矩阵对数正态分布的方法，特别在估计Wishart自回归过程的平稳边际密度时表现更好。金融应用展示了其在估计实现协方差矩阵密度中的实用性。

Conclusion: Wishart KDE是正定矩阵锥上有效的边界感知密度估计方法，理论性质良好，实际应用表现优越，已通过R包ksm公开代码促进实现和复现。

Abstract: A Wishart kernel density estimator (KDE) is introduced for density estimation in the cone of positive definite matrices. The estimator is boundary-aware and mitigates the boundary bias suffered by conventional KDEs, while remaining simple to implement. Its mean squared error, uniform strong consistency on expanding compact sets, and asymptotic normality are established under the Lebesgue measure and suitable mixing conditions. This work represents the first study of density estimation on this space under any metric. For independent observations, an asymptotic upper bound on the mean absolute error is also derived. A simulation study compares the performance of the Wishart KDE to another boundary-aware KDE that relies on the matrix-variate lognormal distribution proposed by Schwartzman [Int. Stat. Rev., 2016, 84(3), 456-486]. Results suggest that the Wishart KDE is superior for a selection of autoregressive coefficient matrices and innovation covariance matrices when estimating the stationary marginal density of a Wishart autoregressive process. To illustrate the practical utility of the Wishart KDE, an application to finance is made by estimating the marginal density function of a time series of realized covariance matrices, calculated from 5-minute intra-day returns, between the share prices of Amazon Corp. and the Standard & Poor's 500 exchange-traded fund over a one-year period. All code is publicly available via the R package ksm to facilitate implementation of the method and reproducibility of the findings.

</details>


### [16] [Propensity score adjustment when errors in achievement measures inform treatment assignment](https://arxiv.org/abs/2512.08144)
*Joshua Wasserman,Michael R. Elliott,Ben B. Hansen*

Main category: stat.ME

TL;DR: 提出基于倾向得分的匹配方法，考虑测量误差以更准确估计干预对缩小成绩差距的效果


<details>
  <summary>Details</summary>
Motivation: 美国州教育机构根据人口亚组间的成绩差距标记需要改进的学校，但小亚组样本量少导致测试分数测量误差大，且公开数据常掩盖小亚组平均值，这给评估缩小成绩差距的干预措施带来挑战

Method: 引入倾向得分估计方法，旨在平衡亚组平均真实分数（而非观测的噪声测量），即使噪声测量不可得时也能使用，相比忽略测量误差的方法能改善重叠性

Result: 通过模拟和德克萨斯州遏制暑期学习损失的全州性倡议应用证明，该方法相比忽略测量误差的方法能实现更大的匹配估计器偏误减少

Conclusion: 提出的倾向得分估计方法能有效处理小亚组测量误差问题，提高评估缩小成绩差距干预措施的效果估计准确性

Abstract: U.S. state education agencies mark schools displaying achievement gaps between demographic subgroups as needing improvement. Some schools may have few students in these subgroups, such that average end-of-year test scores only noisily measure the average "true" score--the score one would expect if students took the test many times. This, in addition to the masking of small subgroup averages in publicly available assessment data, poses challenges for evaluating interventions aimed at closing achievement gaps. We introduce propensity score estimates designed to achieve balance on subgroup average true scores. These estimates are available even when noisy measurements are not and improve overlap compared to those that ignore measurement error, leading to greater bias reduction of matching estimators. We demonstrate our methods through simulation and an application to a statewide initiative in Texas for curbing summer learning loss.

</details>


### [17] [Distributional Random Forests for Complex Survey Designs on Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.08179)
*Yating Zou,Marcos Matabuena,Michael R. Kosorok*

Main category: stat.ME

TL;DR: 提出一种用于复杂调查设计的分布随机森林方法（SDRF），通过伪总体自助法、PSU级诚实性和MMD分裂准则，估计条件分布和连续泛函，并在NHANES数据中应用于糖尿病生物标志物的条件联合分布估计。


<details>
  <summary>Details</summary>
Motivation: 在复杂调查设计下，当Y取值于局部紧致波兰空间时，现有方法难以估计条件分布P(Y|X=x)及其连续泛函。需要开发能够处理复杂设计特征（如分层、聚类）的模型无关估计方法。

Method: 提出调查校准分布随机森林（SDRF），包含：1）伪总体自助法处理复杂设计；2）PSU级诚实性；3）基于Hájek型设计加权节点分布的核均值嵌入的MMD分裂准则。建立了调查设计下森林类估计器的分析框架。

Result: 建立了有限总体目标的设计一致性和超总体目标的模型一致性条件。模拟显示忽略调查设计会导致统计误差。NHANES应用成功估计了两种糖尿病生物标志物的条件联合分布容忍区域，支持亚组特异性风险分析。

Conclusion: SDRF是首个在调查设计下进行模型无关条件分布估计的方法，为复杂调查数据中的分布异质性分析提供了有效工具，在公共卫生等领域有广泛应用前景。

Abstract: We study estimation of the conditional law $P(Y|X=\mathbf{x})$ and continuous functionals $Ψ(P(Y|X=\mathbf{x}))$ when $Y$ takes values in a locally compact Polish space, $X \in \mathbb{R}^p$, and the observations arise from a complex survey design. We propose a survey-calibrated distributional random forest (SDRF) that incorporates complex-design features via a pseudo-population bootstrap, PSU-level honesty, and a Maximum Mean Discrepancy (MMD) split criterion computed from kernel mean embeddings of Hájek-type (design-weighted) node distributions. We provide a framework for analyzing forest-style estimators under survey designs; establish design consistency for the finite-population target and model consistency for the super-population target under explicit conditions on the design, kernel, resampling multipliers, and tree partitions. As far as we are aware, these are the first results on model-free estimation of conditional distributions under survey designs. Simulations under a stratified two-stage cluster design provide finite sample performance and demonstrate the statistical error price of ignoring the survey design. The broad applicability of SDRF is demonstrated using NHANES: We estimate the tolerance regions of the conditional joint distribution of two diabetes biomarkers, illustrating how distributional heterogeneity can support subgroup-specific risk profiling for diabetes mellitus in the U.S. population.

</details>


### [18] [Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference](https://arxiv.org/abs/2512.08828)
*Swaraj Bose,Walter Dempsey*

Main category: stat.ME

TL;DR: 提出一种基于共形推断的新方法，为时变个体处理效应构建预测区间，相比现有方法假设更弱，保证覆盖率下界，适用于移动健康等序列决策场景。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融、教育等个性化决策领域，准确量化多个决策点上的个体处理效应不确定性至关重要。现有方法要么预测非因果纵向估计量，要么基于可交换性假设为横截面数据构建预测区间，存在局限性。

Method: 提出基于共形推断的新方法，为时变个体处理效应构建预测区间。该方法比现有文献假设更弱，保证覆盖率下界，且覆盖率取决于数据的非可交换性程度。方法适用于多种决策场景，特别针对微随机试验等序列实验设计。

Result: 通过模拟微随机试验验证理论主张，并在真实世界的Intern Health Study微随机试验中展示方法的实际效用。方法能够有效量化时变个体处理效应的不确定性。

Conclusion: 该方法为序列决策场景中的个体处理效应不确定性量化提供了更灵活可靠的解决方案，特别适用于移动健康研究等需要弱假设的领域，具有广泛的应用前景。

Abstract: Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS).

</details>


### [19] [Nonparametric inference with massive data via grouped empirical likelihood](https://arxiv.org/abs/2512.08182)
*Yongda Wang,Shifeng Xiong*

Main category: stat.ME

TL;DR: 提出分组经验似然（GEL）方法，通过将N个观测值分组为n组（n≪N），每组内观测值赋予相同概率权重，将优化问题维度从N降至n，大幅降低计算复杂度，同时保持与传统经验似然方法相同的渐近性质。


<details>
  <summary>Details</summary>
Motivation: 传统经验似然方法在处理海量数据时面临计算困难，因为需要优化N个权重参数。随着数据规模增大，计算复杂度急剧增加，限制了经验似然方法在大数据场景下的应用。

Method: 提出分组经验似然（GEL）方法：1）将N个观测值划分为n组（n≪N）；2）每组内所有观测值赋予相同的概率权重；3）通过最大化经验似然比来估计n个权重参数；4）还提出了分布式GEL方法，可在多台服务器上并行计算。

Result: 理论证明GEL在估计方程设置和经典两样本均值问题中具有与传统经验似然方法相同的一阶渐近性质。数值模拟和实际数据分析显示，GEL在保持相同推断精度的同时，相比分治经验似然方法实现了显著的计算加速，单台PC可在数十秒内处理十亿级数据。

Conclusion: GEL方法有效解决了经验似然在大数据场景下的计算瓶颈，通过降维策略在保持统计性质的同时大幅提升计算效率，为海量数据分析提供了实用的经验似然工具。

Abstract: To address the computational issue in empirical likelihood methods with massive data, this paper proposes a grouped empirical likelihood (GEL) method. It divides $N$ observations into $n$ groups, and assigns the same probability weight to all observations within the same group. GEL estimates the $n\ (\ll N)$ weights by maximizing the empirical likelihood ratio. The dimensionality of the optimization problem is thus reduced from $N$ to $n$, thereby lowering the computational complexity. We prove that GEL possesses the same first order asymptotic properties as the conventional empirical likelihood method under the estimating equation settings and the classical two-sample mean problem. A distributed GEL method is also proposed with several servers. Numerical simulations and real data analysis demonstrate that GEL can keep the same inferential accuracy as the conventional empirical likelihood method, and achieves substantial computational acceleration compared to the divide-and-conquer empirical likelihood method. We can analyze a billion data with GEL in tens of seconds on only one PC.

</details>


### [20] [Perturbation-based Inference for Extreme Value Index](https://arxiv.org/abs/2512.08258)
*Yiwei Tang,Judy Huixia Wang,Deyuan Li*

Main category: stat.ME

TL;DR: 提出一种基于扰动生成合成超阈值数据的方法来构建极值指数的置信区间，该方法具有差分隐私性，在GPD近似不充分时引入改进的扰动方法


<details>
  <summary>Details</summary>
Motivation: 极值指数（EVI）对极值理论至关重要，但由于尾部区域数据稀缺，EVI的推断具有挑战性。现有方法在数据有限的情况下难以构建可靠的置信区间。

Method: 提出一种新颖的置信区间构建方法：通过扰动生成合成超阈值数据。具体来说，不是扰动整个样本，而是对高于高阈值的超阈值添加噪声，然后应用广义帕累托分布（GPD）近似。通过模拟扰动数据的关键统计量的分布来推导置信区间。

Result: 证明了关键统计量的一致性，确保所提方法能为EVI提供一致的置信区间。同时证明了扰动数据具有差分隐私性。当GPD近似不充分时，引入了改进的扰动方法。仿真结果表明，该方法优于现有方法，提供了稳健可靠的推断。

Conclusion: 该方法通过扰动生成合成超阈值数据，成功解决了EVI推断中的数据稀缺问题，提供了具有差分隐私保护的可靠置信区间，在GPD近似不充分时仍能保持良好性能。

Abstract: The extreme value index (EVI) characterizes the tail behavior of a distribution and is crucial for extreme value theory. Inference on the EVI is challenging due to data scarcity in the tail region. We propose a novel method for constructing confidence intervals for the EVI using synthetic exceedances generated via perturbation. Rather than perturbing the entire sample, we add noise to exceedances above a high threshold and apply the generalized Pareto distribution (GPD) approximation. Confidence intervals are derived by simulating the distribution of pivotal statistics from the perturbed data. We show that the pivotal statistic is consistent, ensuring the proposed method provides consistent intervals for the EVI. Additionally, we demonstrate that the perturbed data is differentially private. When the GPD approximation is inadequate, we introduce a refined perturbation method. Simulation results show that our approach outperforms existing methods, providing robust and reliable inference.

</details>


### [21] [A Persistent Homology Pipeline for the Analysis of Neural Spike Train Data](https://arxiv.org/abs/2512.08637)
*Cagatay Ayhan,Audrey N. Nash,Roberto Vincis,Martin Bauer,Richard Bertram,Tom Needham*

Main category: stat.ME

TL;DR: 提出基于拓扑数据分析的神经尖峰序列分析框架，通过群体拓扑特征区分不同刺激，即使单个神经元缺乏选择性


<details>
  <summary>Details</summary>
Motivation: 大脑如何将感觉信息转化为感知和行为需要分析协调的神经群体活动。虽然现代电生理学能同时记录尖峰序列集合，但从这些数据中提取有意义信息仍是神经科学的核心挑战。关键问题是神经元群体如何区分不同刺激或行为状态，特别是当单个神经元表现出弱或无刺激选择性时，它们的协调活动仍可能对网络级编码有贡献。

Method: 建立拓扑数据分析框架，使用Victor-Purpura距离作为核心度量，构建持久同调描述符来捕捉群体几何的多尺度拓扑特征。该方法基于两个关键理论结果：持久同调对VP度量参数扰动的稳定性定理，以及拓扑特征鲁棒性的概率稳定性定理。

Result: 在鼠标岛叶皮层记录的不同非伤害性温度去离子水刺激实验中，群体级拓扑特征能有效区分口腔热刺激，即使单个神经元提供很少或没有区分能力。这表明群体组织可以携带标准单单元分析可能遗漏的感知相关信息。

Conclusion: 该TDA框架为神经尖峰序列数据分析提供了新方法，能够捕捉群体编码中标准方法可能遗漏的信息，特别适用于单个神经元选择性较弱但群体活动仍能编码刺激信息的情况。

Abstract: In this article, we introduce a Topological Data Analysis (TDA) pipeline for neural spike train data. Understanding how the brain transforms sensory information into perception and behavior requires analyzing coordinated neural population activity. Modern electrophysiology enables simultaneous recording of spike train ensembles, but extracting meaningful information from these datasets remains a central challenge in neuroscience. A fundamental question is how ensembles of neurons discriminate between different stimuli or behavioral states, particularly when individual neurons exhibit weak or no stimulus selectivity, yet their coordinated activity may still contribute to network-level encoding. We describe a TDA framework that identifies stimulus-discriminative structure in spike train ensembles recorded from the mouse insular cortex during presentation of deionized water stimuli at distinct non-nociceptive temperatures. We show that population-level topological signatures effectively differentiate oral thermal stimuli even when individual neurons provide little or no discrimination. These findings demonstrate that ensemble organization can carry perceptually relevant information that standard single-unit analysis may miss. The framework builds on a mathematical representation of spike train ensembles that enables persistent homology to be applied to collections of point processes. At its core is the widely-used Victor-Purpura (VP) distance. Using this metric, we construct persistence-based descriptors that capture multiscale topological features of ensemble geometry. Two key theoretical results support the method: a stability theorem establishing robustness of persistent homology to perturbations in the VP metric parameter, and a probabilistic stability theorem ensuring robustness of topological signatures.

</details>


### [22] [Exhausting the type I error level in event-driven group-sequential designs with a closed testing procedure for progression-free and overall survival](https://arxiv.org/abs/2512.08658)
*Moritz Fabian Danzer,Kaspar Rufibach,Jan Beyersmann,René Schmidt*

Main category: stat.ME

TL;DR: 开发用于PFS和OS多重主要终点的组序贯检验程序，利用终点间的相关性优化I类错误分配，相比Bonferroni校正提高检验效能


<details>
  <summary>Details</summary>
Motivation: 在肿瘤临床试验中，OS是金标准终点但随访时间长且治疗切换会延迟或稀释效应，PFS提供更早期证据。两者常作为多重主要终点，需要多重检验校正。由于PFS和OS高度相关，忽略相关性（如Bonferroni校正）会损失检验效能。

Method: 开发组序贯检验程序，刻画log-rank统计量在多个终点和事件驱动分析截点上的联合渐近分布，一致估计协方差结构，嵌入闭检验程序重新计算检验统计量的临界值以优化I类错误分配，允许中期和最终分析均为事件驱动。

Result: 模拟研究证实了中到大样本量下的FWER控制。相比Bonferroni校正，新方法恢复了约三分之二的OS检验效能损失，提高了析取和合取检验效能，实现了有意义的早期停止。在试验设计中可减少约5%的OS事件需求。

Conclusion: 提出的方法通过利用PFS和OS的相关性优化多重检验校正，显著提高了检验效能并减少所需事件数，为肿瘤临床试验设计提供了更有效的多重终点分析框架。

Abstract: In oncological clinical trials, overall survival (OS) is the gold-standard endpoint, but long follow-up and treatment switching can delay or dilute detectable effects. Progression-free survival (PFS) often provides earlier evidence and is therefore frequently used together with OS as multiple primary endpoints. Since in certain scenarios trial success may be defined if one of the two hypotheses involved can be rejected, a correction for multiple testing may be deemed necessary. Because PFS and OS are generally highly dependent, their test statistics are typically correlated. Ignoring this dependency (e.g. via a simple Bonferroni correction) is not power optimal. We develop a group-sequential testing procedure for the multiple primary endpoints PFS and OS that fully exhausts the family-wise error rate (FWER) by exploiting their dependence. Specifically, we characterize the joint asymptotic distribution of log-rank statistics across endpoints and multiple event-driven analysis cutoffs. Furthermore, we show that we can consistently estimate the covariance structure. Embedding these results in a closed testing procedure, we can recalculate critical values of the test statistics in order to spend the available type I error optimally. An important extension to the current literature is that we allow for both interim and final analysis to be event-driven. Simulations based on illness-death multi-state models empirically confirm FWER control for moderate to large sample sizes. Compared with a simple Bonferroni correction, the proposed methods recover roughly two thirds of the power loss for OS, increase disjunctive and conjunctive power, and enable meaningful early stopping. In planning, these gains translate into about 5% fewer OS events required to reach the targeted power. We also discuss practical issues in the implementation of such designs and possible extensions of the introduced method.

</details>


### [23] [Stationary Point Constrained Inference via Diffeomorphisms](https://arxiv.org/abs/2512.08735)
*Michael Price,Debdeep Pati,Ning Ning*

Main category: stat.ME

TL;DR: 提出一个用于多极值点联合推断的统计框架，通过约束平稳点数量和使用微分同胚参数化，实现多个极值点的有效不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法对多个极值点进行有效的联合推断，而在许多应用中，峰值和谷值的相对位置具有重要的科学意义。需要开发一个能够处理多个单调区域的框架。

Method: 使用微分同胚公式，将函数表示为简单模板和平滑双射变换的组合。这种参数化约束了平稳点的数量，并提供了极值点位置的直接可解释参数化。

Result: 推导了非渐近置信界，建立了最大似然估计量的近似正态性，在贝叶斯设置中也有平行结果。模拟和脑信号估计应用证明了方法的准确性和可解释性。

Conclusion: 该方法为具有多个单调区域的函数提供了一个原则性框架，能够实现多个极值点的有效联合推断，解决了现有方法在多极值点场景下的局限性。

Abstract: Stationary points or derivative zero crossings of a regression function correspond to points where a trend reverses, making their estimation scientifically important. Existing approaches to uncertainty quantification for stationary points cannot deliver valid joint inference when multiple extrema are present, an essential capability in applications where the relative locations of peaks and troughs carry scientific significance. We develop a principled framework for functions with multiple regions of monotonicity by constraining the number of stationary points. We represent each function in the diffeomorphic formulation as the composition of a simple template and a smooth bijective transformation, and show that this parameterization enables coherent joint inference on the extrema. This construction guarantees a prespecified number of stationary points and provides a direct, interpretable parameterization of their locations. We derive non-asymptotic confidence bounds and establish approximate normality for the maximum likelihood estimators, with parallel results in the Bayesian setting. Simulations and an application to brain signal estimation demonstrate the method's accuracy and interpretability.

</details>


### [24] [Partially Bayes p-values for large scale inference](https://arxiv.org/abs/2512.08847)
*Nikolaos Ignatiadis,Li Ma*

Main category: stat.ME

TL;DR: 提出部分贝叶斯p值方法，将主要参数视为固定，将干扰参数视为随机并赋予非参数先验，通过跨问题分层建模共享信息，提高校准能力和统计功效。


<details>
  <summary>Details</summary>
Motivation: 针对大量主要参数（每个都有其干扰参数）的统计推断问题，传统方法存在双重使用数据的问题。需要一种既能利用贝叶斯优势（处理干扰参数不确定性）又能保持频率主义解释的方法。

Method: 部分贝叶斯方法：主要参数固定，干扰参数随机且服从未知分布（赋予非参数先验）。计算部分贝叶斯p值时，以干扰参数的辅助统计量为条件。跨问题分层建模干扰参数，使用狄利克雷过程建模尺度参数，使用波利亚树建模分布形状。

Result: 提出的部分贝叶斯p值避免了双重使用数据问题，相比纯频率主义方法提高了校准能力和统计功效。在正态均值（未知方差）和位置尺度模型（未知分布形状）两个应用中验证了方法的有效性。

Conclusion: 部分贝叶斯p值方法结合了贝叶斯和频率主义的优点，通过跨问题分层建模干扰参数，实现了更好的校准和更高的统计功效，为大规模参数推断问题提供了有效解决方案。

Abstract: We seek to conduct statistical inference for a large collection of primary parameters, each with its own nuisance parameters. Our approach is partially Bayesian, in that we treat the primary parameters as fixed while we model the nuisance parameters as random and drawn from an unknown distribution which we endow with a nonparametric prior. We compute partially Bayes p-values by conditioning on nuisance parameter statistics, that is, statistics that are ancillary for the primary parameters and informative about the nuisance parameters. The proposed p-values have a Bayesian interpretation as tail areas computed with respect to the posterior distribution of the nuisance parameters. Similarly to the conditional predictive p-values of Bayarri and Berger, the partially Bayes p-values avoid double use of the data (unlike posterior predictive p-values). A key ingredient of our approach is that we model nuisance parameters hierarchically across problems; the sharing of information across problems leads to improved calibration. We illustrate the proposed partially Bayes p-values in two applications: the normal means problem with unknown variances and a location-scale model with unknown distribution shape. We model the scales via Dirichlet processes in both examples and the distribution shape via Pólya trees in the second. Our proposed partially Bayes p-values increase power and calibration compared to purely frequentist alternatives.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [25] [deepspat: An R package for modeling nonstationary spatial and spatio-temporal Gaussian and extremes data through deep deformations](https://arxiv.org/abs/2512.08137)
*Quan Vu,Xuanjie Shao,Raphaël Huser,Andrew Zammit-Mangion*

Main category: stat.CO

TL;DR: deepspat是一个R软件包，用于拟合非平稳空间和时空模型，支持高斯和极值数据，通过深度学习变形方法实现非平稳性建模。


<details>
  <summary>Details</summary>
Motivation: 环境数据中的空间和时空过程普遍存在非平稳性，但由于缺乏实现非平稳模型的统计软件包，实践中很少处理这一问题。

Method: 使用深度多层变形方法对原始空间或时空域进行变换，通过tensorflow实现自动微分，采用梯度优化自定义损失函数来估计模型参数。

Result: 开发了deepspat软件包，能够对高斯和极值数据进行非平稳空间和时空建模、拟合和预测，并通过模拟研究和尼泊尔温度数据应用展示了功能。

Conclusion: deepspat软件包提供了一种直接实现非平稳空间和时空模型的方法，填补了相关统计软件的空缺，有助于环境数据分析中的非平稳性处理。

Abstract: Nonstationarity in spatial and spatio-temporal processes is ubiquitous in environmental datasets, but is not often addressed in practice, due to a scarcity of statistical software packages that implement nonstationary models. In this article, we introduce the R software package deepspat, which allows for modeling, fitting and prediction with nonstationary spatial and spatio-temporal models applied to Gaussian and extremes data. The nonstationary models in our package are constructed using a deep multi-layered deformation of the original spatial or spatio-temporal domain, and are straightforward to implement. Model parameters are estimated using gradient-based optimization of customized loss functions with tensorflow, which implements automatic differentiation. The functionalities of the package are illustrated through simulation studies and an application to Nepal temperature data.

</details>


### [26] [Matrix Completion Survey: Theory, Algorithms, and Empirical Evaluation](https://arxiv.org/abs/2512.08689)
*Connor Panish,Leo Villani*

Main category: stat.CO

TL;DR: 关于矩阵补全方法的简明调查，涵盖被动和自适应策略，并实现了几种基本算法，通过合成实验展示了简单自适应采样方案的行为。


<details>
  <summary>Details</summary>
Motivation: 矩阵补全是一个重要的研究领域，但现有方法分散且缺乏系统比较。本文旨在提供矩阵补全方法的全面概述，特别是对比被动和自适应策略，并通过实现基本算法来促进实践应用。

Method: 1) 对矩阵补全方法进行系统文献调查；2) 实现几种基础算法；3) 设计合成实验对比被动和自适应策略；4) 特别展示简单自适应采样方案在控制实验中的表现。

Result: 1) 提供了矩阵补全方法的分类框架；2) 成功实现了多种基础算法；3) 实验表明自适应采样策略在某些情况下优于被动方法；4) 展示了简单自适应方案的有效性和局限性。

Conclusion: 矩阵补全方法中自适应策略具有潜力，但需要根据具体应用场景选择合适方法。本文的调查和实现为研究人员提供了实用的参考框架，未来的工作可以扩展到更复杂的自适应方案和实际应用场景。

Abstract: We present a concise survey of matrix completion methods and associated implementations of several fundamental algorithms. Our study covers both passive and adaptive strategies. We further illustrate the behavior of a simple adaptive sampling scheme through controlled synthetic experiments.

</details>
