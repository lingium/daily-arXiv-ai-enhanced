<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 19]
- [stat.ML](#stat.ML) [Total: 10]
- [stat.CO](#stat.CO) [Total: 3]
- [stat.AP](#stat.AP) [Total: 6]
- [stat.OT](#stat.OT) [Total: 1]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Minimum bounding polytropes for estimation of max-linear Bayesian networks](https://arxiv.org/abs/2511.05962)
*Kamillo Ferry*

Main category: stat.ME

TL;DR: 该论文研究最大线性贝叶斯网络的可识别性和估计问题，通过热带几何方法分析最小比率估计器，并应用于结构推断。


<details>
  <summary>Details</summary>
Motivation: 最大线性贝叶斯网络通常不可识别，标准似然理论无法应用，需要开发新的估计方法。

Method: 使用热带多面体关联最大线性贝叶斯网络，分析最小比率估计器，结合几何方法进行结构推断。

Result: 在模拟数据和真实数据（NHANES报告和上多瑙河网络数据）上进行了广泛测试。

Conclusion: 几何方法为最大线性模型的结构推断提供了有效途径，最小比率估计器在参数恢复方面具有良好性能。

Abstract: Max-linear Bayesian networks are recursive max-linear structural equation models represented by an edge weighted directed acyclic graph (DAG). The identifiability and estimation of max-linear Bayesian networks is an intricate issue as Gissibl, Klüppelberg, and Lauritzen have shown. As such, a max-linear Bayesian network is generally unidentifiable and standard likelihood theory cannot be applied. We can associate tropical polyhedra to max-linear Bayesian networks. Using this, we investigate the minimum-ratio estimator proposed by Gissibl, Klüppelberg, and Lauritzen and give insight on the structure of minimal best-case samples for parameter recovery which we describe in terms of set covers of certain triangulations. We also combine previous work on estimating max-linear models from Tran, Buck, and Klüppelberg to apply our geometric approach to the structural inference of max-linear models. This is tested extensively on simulated data and on real world data set, the NHANES report for 2015--2016 and the upper Danube network data.

</details>


### [2] [Counterfactual Forecasting For Panel Data](https://arxiv.org/abs/2511.06189)
*Navonil Deb,Raaz Dwivedi,Sumanta Basu*

Main category: stat.ME

TL;DR: 提出了FOCUS方法，通过利用因子时间序列动态来增强传统矩阵补全方法，提高未来反事实结果的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决面板数据中缺失条目和时间依赖潜在因子的反事实结果预测挑战，这在因果推断中至关重要，需要提前估计未观察到的潜在结果。

Method: 基于PCA估计器，FOCUS方法扩展传统矩阵补全方法，利用因子时间序列动态，容纳因子中的随机和确定性成分，提供灵活框架。

Result: 在平稳自回归因子和标准条件下，推导了误差界限并建立了估计量的渐近正态性。实证评估显示，当潜在因子具有自回归成分时，FOCUS优于现有基准方法。

Conclusion: FOCUS方法在HeartSteps移动健康研究中的实际应用证明了其有效性，能够利用用户行为的时间模式来预测接受活动提示用户的步数。

Abstract: We address the challenge of forecasting counterfactual outcomes in a panel data with missing entries and temporally dependent latent factors -- a common scenario in causal inference, where estimating unobserved potential outcomes ahead of time is essential. We propose Forecasting Counterfactuals under Stochastic Dynamics (FOCUS), a method that extends traditional matrix completion methods by leveraging time series dynamics of the factors, thereby enhancing the prediction accuracy of future counterfactuals. Building upon a PCA estimator, our method accommodates both stochastic and deterministic components within the factors, and provides a flexible framework for various applications. In case of stationary autoregressive factors and under standard conditions, we derive error bounds and establish asymptotic normality of our estimator. Empirical evaluations demonstrate that our method outperforms existing benchmarks when the latent factors have an autoregressive component. We illustrate FOCUS results on HeartSteps, a mobile health study, illustrating its effectiveness in forecasting step counts for users receiving activity prompts, thereby leveraging temporal patterns in user behavior.

</details>


### [3] [Collapsing Categories for Regression with Mixed Predictors](https://arxiv.org/abs/2511.06542)
*Chaegeun Song,Zhong Zheng,Bing Li,Lingzhou Xue*

Main category: stat.ME

TL;DR: 提出一种基于成对向量融合LASSO的系统性方法，通过自适应合并回归中相似的类别来降低分类预测变量的复杂度，提高回归估计性能。


<details>
  <summary>Details</summary>
Motivation: 分类预测变量在回归实践中普遍存在，但包含过多类别会严重影响准确性，因为数据信息被众多类别分散。需要一种方法来降低分类复杂度。

Method: 使用成对向量融合LASSO，自动融合与响应变量具有相似回归关系的类别。方法适用于由一般损失函数定义的广泛回归模型类别，包括线性模型和广义线性模型。

Result: 严格证明了方法的类别合并一致性，开发了不精确近端梯度下降算法来实现该方法，并证明了算法的可行性和收敛性。

Conclusion: 通过模拟和Spotify音乐数据应用证明，该方法能有效降低分类复杂度同时提高预测性能，是处理混合预测变量回归的有力工具。

Abstract: Categorical predictors are omnipresent in everyday regression practice: in fact, most regression data involve some categorical predictors, and this tendency is increasing in modern applications with more complex structures and larger data sizes. However, including too many categories in a regression model would seriously hamper accuracy, as the information in the data is fragmented by the multitude of categories. In this paper, we introduce a systematic method to reduce the complexity of categorical predictors by adaptively collapsing categories in regressions, so as to enhance the performance of regression estimation. Our method is based on the {\em pairwise vector fused LASSO}, which automatically fuses the categories that bear a similar regression relation with the response. We develop our method under a wide class of regression models defined by a general loss function, which includes linear models and generalized linear models as special cases. We rigorously established the category collapsing consistency of our method, developed an Inexact Proximal Gradient Descent algorithm to implement it, and proved the feasibility and convergence of our algorithm. Through simulations and an application to Spotify music data, we demonstrate that our method can effectively reduce categorical complexity while improving prediction performance, making it a powerful tool for regression with mixed predictors.

</details>


### [4] [SAT-sampling for statistical significance testing in sparse contingency tables](https://arxiv.org/abs/2511.05709)
*Patrick Scharpfenecker,Tobias Windisch*

Main category: stat.ME

TL;DR: 提出基于SAT的列联表条件检验方法，替代传统的马尔可夫基MCMC方法，解决了计算复杂性和收敛慢的问题。


<details>
  <summary>Details</summary>
Motivation: 传统马尔可夫基MCMC方法在计算完整马尔可夫基时不可行，且在稀疏设置或存在结构零时收敛缓慢，需要更实用的替代方案。

Method: 将纤维编码为布尔电路，使用现代SAT采样器随机生成表格；提出混合MCMC方案，结合SAT提议和局部移动以确保正确的稳态分布。

Result: 在包括具有多个结构零的复杂表格在内的基准测试中，该方法提供了可靠的条件p值，通常优于依赖预计算马尔可夫基的采样器。

Conclusion: SAT-based方法为列联表条件检验提供了实用且高效的替代方案，特别是在传统方法表现不佳的情况下。

Abstract: Exact conditional tests for contingency tables require sampling from fibers with fixed margins. Classical Markov basis MCMC is general but often impractical: computing full Markov bases that connect all fibers of a given constraint matrix can be infeasible and the resulting chains may converge slowly, especially in sparse settings or in presence of structural zeros. We introduce a SAT-based alternative that encodes fibers as Boolean circuits which allows modern SAT samplers to generate tables randomly. We analyze the sampling bias that SAT samplers may introduce, provide diagnostics, and propose practical mitigation. We propose hybrid MCMC schemes that combine SAT proposals with local moves to ensure correct stationary distributions which do not necessarily require connectivity via local moves which is particularly beneficial in presence of structural zeros. Across benchmarks, including small and involved tables with many structural zeros where pure Markov-basis methods underperform, our methods deliver reliable conditional p-values and often outperform samplers that rely on precomputed Markov bases.

</details>


### [5] [Nonparametric Block Bootstrap Kolmogorov-Smirnov Goodness-of-Fit Test](https://arxiv.org/abs/2511.05733)
*Mathew Chandy,Elizabeth Schifano,Jun Yan,Xianyang Zhang*

Main category: stat.ME

TL;DR: 本文针对序列相关数据中KS检验效能下降的问题，提出了一种基于非参数块自助法的偏差校正方法，用于评估平稳序列边际分布的拟合优度。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Smirnov检验在序列相关数据和分布参数未知时效能下降，目前缺乏针对非参数自助法的偏差校正方法来解决这一问题。

Method: 采用非参数块自助法进行偏差校正，近似KS统计量的分布，同时考虑未指定的序列依赖性和未指定的参数。

Result: 通过模拟研究评估了方法的尺寸和功效，并在S&P 500股票收益数据的实际应用中验证了其实用性。

Conclusion: 提出的偏差校正方法有效解决了序列相关数据中KS检验的偏差问题，在理论和实际应用中均表现出良好性能。

Abstract: The Kolmogorov--Smirnov (KS) test is a widely used statistical test that assesses the conformity of a sample to a specified distribution. Its efficacy, however, diminishes with serially dependent data and when parameters within the hypothesized distribution are unknown. For independent data, parametric and nonparametric bootstrap procedures are available to adjust for estimated parameters. For serially dependent stationary data, parametric bootstrap has been developed with a working serial dependence structure. A counterpart for the nonparametric bootstrap approach, which needs a bias correction, has not been studied. Addressing this gap, our study introduces a bias correction method employing a nonparametric block bootstrap, which approximates the distribution of the KS statistic in assessing the goodness-of-fit of the marginal distribution of a stationary series, accounting for unspecified serial dependence and unspecified parameters. We assess its effectiveness through simulations, scrutinizing both its size and power. The practicality of our method is further illustrated with an examination of stock returns from the S\&P 500 index, showcasing its utility in real-world applications.

</details>


### [6] [Conformalized Bayesian Inference, with Applications to Random Partition Models](https://arxiv.org/abs/2511.05746)
*Nicola Bariletto,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ME

TL;DR: 提出了Conformalized Bayesian Inference (CBI)框架，用于在非标准参数空间中进行后验推断，提供点估计、可信区域和多模态分析，仅需后验样本和参数差异度量。


<details>
  <summary>Details</summary>
Motivation: 当参数空间复杂（如无限维或组合大空间）时，标准后验摘要（如后验均值、可信区间）往往不可用，阻碍了可解释的后验不确定性量化。

Method: CBI构建基于差异的核密度评分，通过保形预测原理推导点估计和可信区域，并将后验推断重新解释为参数空间上的预测，最后进行基于密度的聚类识别代表性后验模态。

Result: CBI在模拟和实际数据聚类应用中展示了实用性、可扩展性和多功能性，特别适用于贝叶斯随机划分模型。

Conclusion: CBI提供了一个广泛适用且计算高效的后验推断框架，能够在复杂参数空间中实现可靠的不确定性量化。

Abstract: Bayesian posterior distributions naturally represent parameter uncertainty informed by data. However, when the parameter space is complex, as in many nonparametric settings where it is infinite dimensional or combinatorially large, standard summaries such as posterior means, credible intervals, or simple notions of multimodality are often unavailable, hindering interpretable posterior uncertainty quantification. We introduce Conformalized Bayesian Inference (CBI), a broadly applicable and computationally efficient framework for posterior inference on nonstandard parameter spaces. CBI yields a point estimate, a credible region with assumption-free posterior coverage guarantees, and a principled analysis of posterior multimodality, requiring only Monte Carlo samples from the posterior and a notion of discrepancy between parameters. The method builds a discrepancy-based kernel density score for each parameter value, yielding a maximum-a-posteriori-like point estimate and a credible region derived from conformal prediction principles. The key conceptual step underlying this construction is the reinterpretation of posterior inference as prediction on the parameter space. A final density-based clustering step identifies representative posterior modes. We investigate a number of theoretical and methodological properties of CBI and demonstrate its practicality, scalability, and versatility in simulated and real data clustering applications with Bayesian random partition models.

</details>


### [7] [Bounding interventional queries from generalized incomplete contingency tables](https://arxiv.org/abs/2511.05755)
*Ivano Lodato,Aditya V. Iyer,Isaac Z. To*

Main category: stat.ME

TL;DR: 提出了一种在广义不完整列联表(GICTs)存在下评估干预查询和平均处理效应(ATEs)的方法，通过将未知概率建模为自由参数并推导包含这些参数的符号表达式，在基本概率约束下极值化这些表达式来获得查询的尖锐边界。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理包含随机零值的广义不完整列联表时，要么丢弃这些条目要么插补缺失值，这可能导致偏差。需要一种能够正式量化由数据不完整性引起的不确定性的方法。

Method: 将未知概率建模为自由参数，推导包含这些参数的干预查询符号表达式，然后在所有变量支持的基本概率约束下极值化这些表达式，获得查询的尖锐边界。

Result: 该方法能够获得查询的尖锐边界，为广义不完整性引起的不确定性提供正式量化，确保查询的真实值始终位于边界内。

Conclusion: 该框架独立于缺失机制，为随机数据缺口下的因果推断提供了一种保守而严谨的方法。

Abstract: We introduce a method for evaluating interventional queries and Average Treatment Effects (ATEs) in the presence of generalized incomplete contingency tables (GICTs), contingency tables containing a full row of random (sampling) zeros, rendering some conditional probabilities undefined. Rather than discarding such entries or imputing missing values, we model the unknown probabilities as free parameters and derive symbolic expressions for the queries that incorporate them. By extremizing these expressions over all values consistent with basic probability constraints and the support of all variables, we obtain sharp bounds for the query of interest under weak assumptions of small missing frequencies. These bounds provide a formal quantification of the uncertainty induced by the generalized incompleteness of the contingency table and ensure that the true value of the query will always lie within them. The framework applies independently of the missingness mechanism and offers a conservative yet rigorous approach to causal inference under random data gaps.

</details>


### [8] [Standard and comparative e-backtests for general risk measures](https://arxiv.org/abs/2511.05840)
*Zhanyi Jiao,Qiuqi Wang,Yimiao Zhao*

Main category: stat.ME

TL;DR: 本文基于e值和e过程概念，开发了用于风险度量的标准回测和比较回测的模型无关方法，可应用于均值、方差、VaR、ES等多种常见风险度量。


<details>
  <summary>Details</summary>
Motivation: 回测风险度量是金融监管机构评估金融机构风险预测的重要问题，需要防止在金融监管中被操纵。

Method: 基于e值和e过程构建标准回测和比较回测，采用模型无关（非参数）方法处理可识别风险度量和可引出风险度量。

Result: 通过大量模拟研究和真实数据分析验证了方法的有效性，适用于多种常见风险度量。

Conclusion: 提出的e回测方法为金融监管提供了稳健的风险预测评估工具，能够有效防止操纵行为。

Abstract: Backtesting risk measures is a unique and important problem for financial regulators to evaluate risk forecasts reported by financial institutions. As a natural extension to standard (or traditional) backtests, comparative backtests are introduced to evaluate different forecasts against regulatory standard models. Based on recently developed concepts of e-values and e-processes, we focus on how standard and comparative backtests can be manipulated in financial regulation by constructing e-processes. We design a model-free (non-parametric) method for standard backtests of identifiable risk measures and comparative backtests of elicitable risk measures. Our e-backtests are applicable to a wide range of common risk measures including the mean, the variance, the Value-at-Risk, the Expected Shortfall, and the expectile. Our results are illustrated by ample simulation studies and real data analysis.

</details>


### [9] [Scalable and Distributed Individualized Treatment Rules for Massive Datasets](https://arxiv.org/abs/2511.05842)
*Nan Qiao,Wangcheng Li,Jingxiao Zhang,Canyi Chen*

Main category: stat.ME

TL;DR: 提出一种卷积平滑加权支持向量机方法，通过分布式学习从多源数据中学习最优个体化治疗规则，在保护隐私的同时实现统计最优性能。


<details>
  <summary>Details</summary>
Motivation: 多源数据整合分析对构建准确个体化治疗规则至关重要，但隐私问题阻碍了数据共享。传统元学习方法因局部估计偏差而效果不佳。

Method: 开发卷积平滑加权支持向量机，构建凸且平滑的损失函数，设计高效多轮分布式学习程序，仅需共享汇总统计量而非原始数据。

Result: 方法在固定通信轮数下实现统计最优性能，开发坐标梯度下降算法保证线性收敛，在模拟和脓毒症治疗应用中验证有效性。

Conclusion: 该方法为隐私保护下的多源数据ITR学习提供了高效解决方案，平衡了统计性能与隐私保护需求。

Abstract: Synthesizing information from multiple data sources is crucial for constructing accurate individualized treatment rules (ITRs). However, privacy concerns often present significant barriers to the integrative analysis of such multi-source data. Classical meta-learning, which averages local estimates to derive the final ITR, is frequently suboptimal due to biases in these local estimates. To address these challenges, we propose a convolution-smoothed weighted support vector machine for learning the optimal ITR. The accompanying loss function is both convex and smooth, which allows us to develop an efficient multi-round distributed learning procedure for ITRs. Such distributed learning ensures optimal statistical performance with a fixed number of communication rounds, thereby minimizing coordination costs across data centers while preserving data privacy. Our method avoids pooling subject-level raw data and instead requires only sharing summary statistics. Additionally, we develop an efficient coordinate gradient descent algorithm, which guarantees at least linear convergence for the resulting optimization problem. Extensive simulations and an application to sepsis treatment across multiple intensive care units validate the effectiveness of the proposed method.

</details>


### [10] [Identification of Emotionally Stressful Periods Through Tracking Changes in Statistical Features of mHealth Data](https://arxiv.org/abs/2511.05887)
*Younghoon Kim,Sumanta Basu,Samprit Banerjee*

Main category: stat.ME

TL;DR: 提出了一种检测情绪压力热点的算法，通过分析被动感知变量和压力指标的统计特征变化来识别需要关注的时间区间，相比传统方法能更好地处理复杂分布变化和异步变化。


<details>
  <summary>Details</summary>
Motivation: 现有基于变化点检测的方法在处理非平稳数据时存在局限性，当分布变化复杂、变量间依赖关系难以捕捉且变化异步发生时，传统方法容易误判压力起始时间。

Method: 扩展移动和(MOSUM)方案来检测序列内和序列间的同步变化，通过基于距离的检验统计量和置信区间两种方式定义热点区间，跟踪组合分布特征的局部变化。

Result: 在不同信号强度和混合异步分布变化的模拟中，该方法优于基准方法；两种热点定义方式具有互补性；在ALACRITY Phase I数据应用中成功识别了患者压力水平和活动测量的热点。

Conclusion: 所提出的热点检测方法能够有效捕捉各种类型的同步和异步变化，不需要变量间的特定函数关系，并以区间形式输出结果，为心理健康研究提供了更准确的压力起始时间识别工具。

Abstract: Identifying the onset of emotional stress in older patients with mood disorders and chronic pain is crucial in mental health studies. To this end, studying the associations between passively sensed variables that measure human behaviors and self-reported stress levels collected from mobile devices is emerging. Existing algorithms rely on conventional change point detection (CPD) methods due to the nonstationary nature of the data. They also require explicit modeling of the associations between variables and output only discrete time points, which can lead to misinterpretation of stress onset timings. This is problematic when distributional shifts are complex, dependencies between variables are difficult to capture, and changes occur asynchronously across series with weak signals. In this study, we propose an algorithm that detects hotspots, defined as collections of time intervals during which statistical features of passive sensing variables and stress indicators shift, highlighting periods that require investigation. We first extend the moving sum (MOSUM) scheme to detect simultaneous changes both within and across series, and then define hotspots in two ways: using distance-based test statistics and confidence intervals. The proposed method tracks local changes in combined distributional features, enabling it to capture all types of simultaneous and asynchronous change. It does not require a specific functional relationship between series, and the results are expressed as intervals rather than as individual time points. We conduct simulations under varying signal strengths with mixed and asynchronous distributional shifts, where the proposed method outperforms benchmarks. Results on hotspot identification indicate that the two definitions are complementary. We further apply our method to ALACRITY Phase I data, analyzing hotspots from patients' stress levels and activity measures.

</details>


### [11] [A Riemannian Framework for Linear and Quadratic Discriminant Analysis on the Tangent Space of Shapes](https://arxiv.org/abs/2511.06027)
*Susovan Pal,Roger P. Woods,Suchit Panjiyar,Elizabeth Sowell,Katherine L. Narr,Shantanu H. Joshi*

Main category: stat.ME

TL;DR: 提出了一种在曲线形状空间切平面上的黎曼框架，用于线性和二次判别分类。通过切空间的傅里叶基系数来近似形状观测值，并在截断的傅里叶基上进行投影以获得降维特征。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理曲线形状数据的分类框架，特别是在无限维形状空间中进行判别分析，以应用于医学图像分析等领域。

Method: 使用平方根速度函数构建形状空间，在切空间中定义形状值随机变量的均值和协方差，通过傅里叶基系数近似形状观测值，并在线性和二次判别分析中使用截断的傅里叶基投影特征。

Result: 在合成数据、皮层沟形状、胼胝体曲线以及胎儿酒精综合征患者的面部中线曲线轮廓上展示了分类结果。

Conclusion: 提出的黎曼框架能够有效地在曲线形状空间中进行线性和二次判别分类，为形状数据分析提供了一种新的方法。

Abstract: We present a Riemannian framework for linear and quadratic discriminant classification on the tangent plane of the shape space of curves. The shape space is infinite dimensional and is constructed out of square root velocity functions of curves. We introduce the idea of mean and covariance of shape-valued random variables and samples from a tangent space to the pre-shape space (invariant to translation and scaling) and then extend it to the full shape space (rotational invariance). The shape observations from the population are approximated by coefficients of a Fourier basis of the tangent space. The algorithms for linear and quadratic discriminant analysis are then defined using reduced dimensional features obtained by projecting the original shape observations on to the truncated Fourier basis. We show classification results on synthetic data and shapes of cortical sulci, corpus callosum curves, as well as facial midline curve profiles from patients with fetal alcohol syndrome (FAS).

</details>


### [12] [Unifiedly Efficient Inference on All-Dimensional Targets for Large-Scale GLMs](https://arxiv.org/abs/2511.06070)
*Bo Fu,Dandan Jiang*

Main category: stat.ME

TL;DR: 提出了一个统一框架，突破传统子采样方法r^{-1/2}收敛率的限制，为大规模高维GLM提供高效精确的推断方法。


<details>
  <summary>Details</summary>
Motivation: 传统GLM在大规模高维数据中存在计算可行性与统计准确性之间的权衡，现有子采样方法受到次优收敛率的限制。

Method: 提出三种估计器：低维目标的去方差子采样(DVS)估计器、多步精炼估计器，以及针对高维目标的去相关得分函数方法。

Result: DVS估计器达到max{r^{-1}, n^{-1/2}}收敛率，多步精炼估计器在r/√n→∞时渐近正态且半参数有效，数值实验显示在计算效率和统计准确性方面表现优越。

Conclusion: 该框架实现了大规模GLM的统一高效推断，在低维和高维推断任务中均能平衡计算效率与统计准确性。

Abstract: The scalability of Generalized Linear Models (GLMs) for large-scale, high-dimensional data often forces a trade-off between computational feasibility and statistical accuracy, particularly for inference on pre-specified parameters. While subsampling methods mitigate computational costs, existing estimators are typically constrained by a suboptimal $r^{-1/2}$ convergence rate, where $r$ is the subsample size. This paper introduces a unified framework that systematically breaks this barrier, enabling efficient and precise inference regardless of the dimension of the target parameters. To overcome the accuracy loss and enhance computational efficiency, we propose three estimators tailored to different scenarios. For low-dimensional targets, we propose a de-variance subsampling (DVS) estimator that achieves a sharply improved convergence rate of $\max\{r^{-1}, n^{-1/2}\}$, permitting valid inference even with very small subsamples. As $r$ grows, a multi-step refinement of our estimator is proven to be asymptotically normal and semiparametric efficient when $r/\sqrt{n} \to \infty$, matching the performance of the full-sample estimator-a property confirmed by its Bahadur representation. Critically, we provide an improved principle to high-dimensional targets, developing a novel decorrelated score function that facilitates simultaneous inference for a diverging number of pre-specified parameters. Comprehensive numerical experiments demonstrate that our framework delivers a superior balance of computational efficiency and statistical accuracy across both low- and high-dimensional inferential tasks in large-scale GLM, thereby realizing the promise of unifiedly efficient inference for large-scale GLMs.

</details>


### [13] [Approximate Bayesian inference for cumulative probit regression models](https://arxiv.org/abs/2511.06967)
*Emanuele Aliverti*

Main category: stat.ME

TL;DR: 本文提出了三种基于变分贝叶斯和期望传播的可扩展算法，用于近似累积概率模型中的回归系数后验分布，在大数据集上相比MCMC方法具有更优的计算性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 当观测数据量增长时，用于贝叶斯推断的标准采样算法扩展性差，使得在大数据集上的后验计算变得越来越困难。

Method: 提出了三种基于变分贝叶斯和期望传播的可扩展算法来近似累积概率模型中回归系数的后验分布。

Result: 与基于马尔可夫链蒙特卡洛的推断相比，所提出的方法展现出更优的计算性能和显著的准确性。

Conclusion: 所提出的算法在计算性能和准确性方面优于传统MCMC方法，并在一个具有挑战性的案例研究中证明了其实用性，用于研究犯罪网络结构。

Abstract: Ordinal categorical data are routinely encountered in a wide range of practical applications. When the primary goal is to construct a regression model for ordinal outcomes, cumulative link models represent one of the most popular choices to link the cumulative probabilities of the response with a set of covariates through a parsimonious linear predictor, shared across response categories. When the number of observations grows, standard sampling algorithms for Bayesian inference scale poorly, making posterior computation increasingly challenging in large datasets. In this article, we propose three scalable algorithms for approximating the posterior distribution of the regression coefficients in cumulative probit models relying on Variational Bayes and Expectation Propagation. We compare the proposed approaches with inference based on Markov Chain Monte Carlo, demonstrating superior computational performance and remarkable accuracy; finally, we illustrate the utility of the proposed algorithms on a challenging case study to investigate the structure of a criminal network.

</details>


### [14] [Breaking the Winner's Curse with Bayesian Hybrid Shrinkage](https://arxiv.org/abs/2511.06318)
*Richard Mudd,Rina Friedberg,Ilya Gorbachev,Houssam Nassif,Abbas Zaidi*

Main category: stat.ME

TL;DR: 论文提出了一种新的贝叶斯方法来缓解大规模在线实验平台中的'赢家诅咒'问题，该方法通过实验特定的局部收缩因子来减少对先验选择的敏感性，并在假设违反时保持稳健性。


<details>
  <summary>Details</summary>
Motivation: 大规模在线实验平台中，当同一实验既用于选择处理又评估其效果时，会出现'赢家诅咒'问题，导致经典差异均值估计器存在向上偏差，传统置信区间失效。

Method: 提出贝叶斯方法，引入实验特定的'局部收缩'因子，无需数值积分技术即可估计后验分布，适合大规模部署。

Result: 模拟评估显示该方法在各种场景下表现良好，即使采样和选择过程的假设被违反也能保持稳健。实证评估表明该方法优于替代方法，提供更准确的估计和良好校准的不确定性量化。

Conclusion: 该贝叶斯方法能有效缓解'赢家诅咒'问题，在大规模在线实验平台中具有实用价值，即使在假设违反的情况下也能保持良好性能。

Abstract: A 'Winner's Curse' arises in large-scale online experimentation platforms when the same experiments are used to both select treatments and evaluate their effects. In these settings, classical difference-in-means estimators of treatment effects are upwardly biased and conventional confidence intervals are rendered invalid. The bias scales with the magnitude of sampling variability and the selection threshold, and inversely with the treatment's true effect size. We propose a new Bayesian approach that incorporates experiment-specific 'local shrinkage' factors that mitigate sensitivity to the choice of prior and improve robustness to assumption violations. We demonstrate how the associated posterior distribution can be estimated without numerical integration techniques, making it a practical choice for at-scale deployment. Through simulation, we evaluate the performance of our approach under various scenarios and find that it performs well even when assumptions about the sampling and selection processes are violated. In an empirical evaluation, our approach demonstrated superior performance over alternative methods, providing more accurate estimates with well-calibrated uncertainty quantification.

</details>


### [15] [A sensitivity analysis for the average derivative effect](https://arxiv.org/abs/2511.06243)
*Jeffrey Zhang*

Main category: stat.ME

TL;DR: 本文针对连续暴露变量的因果推断，提出了平均导数效应(ADE)的敏感性分析方法，包括闭式边界估计、灵活高效的估计器以及置信区间构建。


<details>
  <summary>Details</summary>
Motivation: 在观察性研究中，暴露变量通常是连续的而非离散的，且无法排除未测量混杂因素的影响。现有敏感性分析方法主要针对二元暴露，需要为连续暴露开发相应的方法。

Method: 提出基于敏感性模型的ADE闭式边界估计，该模型约束了潜在和观测广义倾向得分的比值比。开发了灵活高效的边界估计器，以及点态和同时置信区间。

Result: 通过模拟研究验证了方法的有限样本性能，并在父母收入对教育成就影响、汽油价格弹性两个实证研究中进行了应用。

Conclusion: 为连续暴露变量的因果推断提供了一套完整的敏感性分析框架，能够评估因果结论对未测量混杂的稳健性。

Abstract: In observational studies, exposures are often continuous rather than binary or discrete. At the same time, sensitivity analysis is an important tool that can help determine the robustness of a causal conclusion to a certain level of unmeasured confounding, which can never be ruled out in an observational study. Sensitivity analysis approaches for continuous exposures have now been proposed for several causal estimands. In this article, we focus on the average derivative effect (ADE). We obtain closed-form bounds for the ADE under a sensitivity model that constrains the odds ratio (at any two dose levels) between the latent and observed generalized propensity score. We propose flexible, efficient estimators for the bounds, as well as point-wise and simultaneous (over the sensitivity parameter) confidence intervals. We examine the finite sample performance of the methods through simulations and illustrate the methods on a study assessing the effect of parental income on educational attainment and a study assessing the price elasticity of petrol.

</details>


### [16] [Gaussian Graphical Models for Partially Observed Multivariate Functional Data](https://arxiv.org/abs/2511.06445)
*Marco Borriero,Luigi Augugliaro,Gianluca Sottile,Veronica Vinciotti*

Main category: stat.ME

TL;DR: 开发了一种用于部分观测多元函数数据的函数图模型EM算法


<details>
  <summary>Details</summary>
Motivation: 多元函数数据通常只被部分观测，这给统计推断带来挑战，因为无法计算函数主成分得分

Method: 在高斯性和协方差算子部分可分的假设下，开发了惩罚推断的EM类型算法

Result: 模拟研究和德国电力市场数据应用显示了该方法的潜力

Conclusion: 提出的方法能够有效处理部分观测的多元函数数据，在函数图模型推断中具有应用价值

Abstract: In many applications, the variables that characterize a stochastic system are measured along a second dimension, such as time. This results in multivariate functional data and the interest is in describing the statistical dependences among these variables. It is often the case that the functional data are only partially observed. This creates additional challenges to statistical inference, since the functional principal component scores, which capture all the information from these data, cannot be computed. Under an assumption of Gaussianity and of partial separability of the covariance operator, we develop an EM-type algorithm for penalized inference of a functional graphical model from multivariate functional data which are only partially observed. A simulation study and an illustration on German electricity market data show the potential of the proposed method.

</details>


### [17] [Confidence Intervals Based on the Modified Chi-Squared Distribution and its Applications in Medicine](https://arxiv.org/abs/2511.06476)
*Mulan Wu,Mengyu Xu,Dongyun Kim*

Main category: stat.ME

TL;DR: 针对临床研究中样本量小的问题，提出基于二次型统计量的置信区间计算方法，提高小样本或比例数据的统计推断可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床研究常因成本、受试者可获得性和罕见病等因素导致样本量小，使用正态分布近似计算置信区间存在不准确的问题。

Method: 采用基于二次型的统计量推导置信区间，特别适用于小样本或比例数据，并确定了该方法适用的合理样本量和比例范围。

Result: 该方法提高了统计推断的可靠性，并通过真实医学临床试验数据进行了验证。

Conclusion: 基于二次型的方法为小样本临床研究提供了更准确的置信区间计算方案，增强了统计推断的可信度。

Abstract: Small sample sizes in clinical studies arises from factors such as reduced costs, limited subject availability, and the rarity of studied conditions. This creates challenges for accurately calculating confidence intervals (CIs) using the normal distribution approximation. In this paper, we employ a quadratic-form based statistic, from which we derive more accurate confidence intervals, particularly for data with small sample sizes or proportions. Based on the study, we suggest reasonable values of sample sizes and proportions for the application of the quadratic method. Consequently, this method enhances the reliability of statistical inferences. We illustrate this method with real medical data from clinical trials.

</details>


### [18] [A Simple and Effective Random Forest Modelling for Nonlinear Time Series Data](https://arxiv.org/abs/2511.06544)
*Shihao Zhang,Zudi Lu,Chao Zheng*

Main category: stat.ME

TL;DR: 提出RF-RW方法，通过随机权重替代自助采样，在保持时间序列依赖结构的同时降低树间相关性，在非线性时间序列预测中优于传统RF方法和其他基准模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机森林的方法难以充分捕捉时间序列数据中的时间依赖性，需要一种能够结合经典RF优势与时间序列固有依赖结构的替代建模方法。

Method: RF-RW避免自助重采样过程，保持序列依赖结构，同时引入独立随机权重来减少树之间的相关性。

Result: 模拟研究显示RF-RW优于现有RF方法和SVM、LSTM等基准模型；在英国COVID-19日病例预测的实际数据中达到最低误差。

Conclusion: RF-RW是一种理论上有基础、实践上有效的替代RF建模方法，特别适用于非线性时间序列数据预测。

Abstract: In this paper, we propose Random Forests by Random Weights (RF-RW), a theoretically grounded and practically effective alternative RF modelling for nonlinear time series data, where existing RF-based approaches struggle to adequately capture temporal dependence. RF-RW reconciles the strengths of classic RF with the temporal dependence inherent in time series forecasting. Specifically, it avoids the bootstrap resampling procedure, therefore preserves the serial dependence structure, whilst incorporates independent random weights to reduce correlations among trees. We establish non-asymptotic concentration bounds and asymptotic uniform consistency guarantees, for both fixed- and high-dimensional feature spaces, which extend beyond existing theoretical analyses of RF. Extensive simulation studies demonstrate that RF-RW outperforms existing RF-based approaches and other benchmarks such as SVM and LSTM. It also achieves the lowest error among competitors in our real-data example of predicting UK COVID-19 daily cases.

</details>


### [19] [A general approach to construct powerful tests for intersections of one-sided null-hypotheses based on influence functions](https://arxiv.org/abs/2511.07096)
*Christian Bressen Pipper,Andreas Nordland,Klaus Kähler Holst*

Main category: stat.ME

TL;DR: 提出了一种基于Wald类型检验的通用方法，用于检验单边零假设的交集，该方法基于p值的同步渐近行为，不需要额外的同步行为假设，具有较好的功效特性。


<details>
  <summary>Details</summary>
Motivation: 现有的交集检验方法（如最小p值检验）通常基于边际p值，并保守地评估而忽略p值的同步行为，这可能导致功效不足。

Method: 使用堆叠方法通过估计量的影响函数推导p值的同步渐近行为，构建通用的Wald类型检验来检验单边零假设的交集。

Result: 所提出的检验方法具有良好的功效特性，可以构建强大的闭包检验程序。

Conclusion: 该方法为在族系I类错误控制下检验多个单边假设提供了有效的闭包检验基础，不需要额外的同步行为假设。

Abstract: Testing intersections of null-hypotheses is an integral part of closed testing procedures for assessing multiple null-hypotheses under family-wise type 1 error control. Popular intersection tests such as the minimum p-value test are based on marginal p-values and are typically evaluated conservatively by disregarding simultaneous behavior of the marginal p-values. We consider a general purpose Wald type test for testing intersections of one-sided null-hypotheses. The test is constructed on the basis of the simultaneous asymptotic behavior of the p values. The simultaneous asymptotic behavior is derived via influence functions of estimators using the so-called stacking approach. In particular, this approach does not require added assumptions on simultaneous behavior to be valid. The resulting test is shown to have attractive power properties and thus forms the basis of a powerful closed testing procedure for testing multiple one-sided hypotheses under family-wise type 1 error control.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [20] [Benchmarking of Clustering Validity Measures Revisited](https://arxiv.org/abs/2511.05983)
*Connor Simpson,Ricardo J. G. B. Campello,Elizabeth Stojanovski*

Main category: stat.ML

TL;DR: 对26种内部有效性指标进行综合基准测试，包括经典指标和最新指标，采用改进的三重评估方法，使用16177个数据集和8种聚类算法进行广泛测试


<details>
  <summary>Details</summary>
Motivation: 验证在聚类过程中至关重要，需要确定不同聚类算法或参数产生的最佳聚类方案，但现有基准测试方法存在不足

Method: 采用改进的三重定制评估子方法，每个子方法评估指标的不同方面并防止偏差，使用16177个数据集和8种常用聚类算法

Result: 开发了包含16177个数据集的新集合，实现了更广泛的应用范围和更多样化的聚类场景表示

Conclusion: 提出了一个全面的内部有效性指标评估框架，能够深入分析指标在复杂情况下的行为表现

Abstract: Validation plays a crucial role in the clustering process. Many different internal validity indexes exist for the purpose of determining the best clustering solution(s) from a given collection of candidates, e.g., as produced by different algorithms or different algorithm hyper-parameters. In this study, we present a comprehensive benchmark study of 26 internal validity indexes, which includes highly popular classic indexes as well as more recently developed ones. We adopted an enhanced revision of the methodology presented in Vendramin et al. (2010), developed here to address several shortcomings of this previous work. This overall new approach consists of three complementary custom-tailored evaluation sub-methodologies, each of which has been designed to assess specific aspects of an index's behaviour while preventing potential biases of the other sub-methodologies. Each sub-methodology features two complementary measures of performance, alongside mechanisms that allow for an in-depth investigation of more complex behaviours of the internal validity indexes under study. Additionally, a new collection of 16177 datasets has been produced, paired with eight widely-used clustering algorithms, for a wider applicability scope and representation of more diverse clustering scenarios.

</details>


### [21] [Sparsity via Hyperpriors: A Theoretical and Algorithmic Study under Empirical Bayes Framework](https://arxiv.org/abs/2511.06235)
*Zhitao Li,Yiqiu Dong,Xueying Zeng*

Main category: stat.ML

TL;DR: 该论文分析了经验贝叶斯框架中超参数估计对稀疏学习的影响，建立了超先验选择与稀疏性、局部最优性之间的理论联系，并提出了一种收敛性有保证的PALM算法。


<details>
  <summary>Details</summary>
Motivation: 研究超先验对经验贝叶斯框架解的影响，探索如何通过合适的超先验选择来促进稀疏性和提高解稳定性。

Method: 采用近端交替线性化最小化(PALM)算法，该算法对凸和凹超先验都具有收敛保证。

Result: 数值测试表明，引入适当的超先验能显著促进解的稀疏性并提高图像去模糊的恢复精度。

Conclusion: 合适的超先验选择对经验贝叶斯框架的稀疏学习和逆问题求解具有重要影响，特别是在噪声水平和病态性方面。

Abstract: This paper presents a comprehensive analysis of hyperparameter estimation within the empirical Bayes framework (EBF) for sparse learning. By studying the influence of hyperpriors on the solution of EBF, we establish a theoretical connection between the choice of the hyperprior and the sparsity as well as the local optimality of the resulting solutions. We show that some strictly increasing hyperpriors, such as half-Laplace and half-generalized Gaussian with the power in $(0,1)$, effectively promote sparsity and improve solution stability with respect to measurement noise. Based on this analysis, we adopt a proximal alternating linearized minimization (PALM) algorithm with convergence guaranties for both convex and concave hyperpriors. Extensive numerical tests on two-dimensional image deblurring problems demonstrate that introducing appropriate hyperpriors significantly promotes the sparsity of the solution and enhances restoration accuracy. Furthermore, we illustrate the influence of the noise level and the ill-posedness of inverse problems to EBF solutions.

</details>


### [22] [Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces](https://arxiv.org/abs/2511.06239)
*Byoungwoo Park,Juho Lee,Guan-Horng Liu*

Main category: stat.ML

TL;DR: 提出了功能伴随采样器（FAS），这是一种基于随机最优控制的扩散采样器，用于在无限维函数空间中采样吉布斯分布，特别适用于条件扩散过程的路径采样。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在有限维空间的吉布斯分布采样方面进展迅速，但在无限维函数空间的理论和算法设计仍然有限，尽管它们在采样条件扩散过程路径方面具有巨大潜力。

Method: 基于随机最大值原理的随机最优控制理论，将伴随采样推广到希尔伯特空间，提出了功能伴随采样器（FAS），使用简单的匹配型目标函数进行功能表示。

Result: FAS在合成势能和真实分子系统（包括丙氨酸二肽和Chignolin）中实现了优越的过渡路径采样性能。

Conclusion: FAS成功地将伴随采样方法扩展到无限维函数空间，为条件扩散过程的路径采样提供了有效的解决方案。

Abstract: Learning-based methods for sampling from the Gibbs distribution in finite-dimensional spaces have progressed quickly, yet theory and algorithmic design for infinite-dimensional function spaces remain limited. This gap persists despite their strong potential for sampling the paths of conditional diffusion processes, enabling efficient simulation of trajectories of diffusion processes that respect rare events or boundary constraints. In this work, we present the adjoint sampler for infinite-dimensional function spaces, a stochastic optimal control-based diffusion sampler that operates in function space and targets Gibbs-type distributions on infinite-dimensional Hilbert spaces. Our Functional Adjoint Sampler (FAS) generalizes Adjoint Sampling (Havens et al., 2025) to Hilbert spaces based on a SOC theory called stochastic maximum principle, yielding a simple and scalable matching-type objective for a functional representation. We show that FAS achieves superior transition path sampling performance across synthetic potential and real molecular systems, including Alanine Dipeptide and Chignolin.

</details>


### [23] [Fast Riemannian-manifold Hamiltonian Monte Carlo for hierarchical Gaussian-process models](https://arxiv.org/abs/2511.06407)
*Takashi Hayakawa,Satoshi Asai*

Main category: stat.ML

TL;DR: 本文提出了一种改进的黎曼流形哈密顿蒙特卡洛(RMHMC)方法，通过优化计算顺序和动态编程特征分解，显著提升了高斯过程分层贝叶斯模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 高斯过程分层贝叶斯模型能有效描述现实数据中的复杂非线性统计依赖关系，但现有蒙特卡洛推理算法效率低下，需要改进。

Method: 优化RMHMC的计算顺序，根据模型结构进行动态编程特征分解，避免使用简单的自动微分库。

Result: 在模拟数据的贝叶斯逻辑回归和美国国家医疗支出数据的倾向函数估计中，改进的RMHMC能有效从后验分布中采样并计算模型证据。

Conclusion: 为高斯过程分析现实数据提供了有效的蒙特卡洛算法基础，强调需要开发可定制的库集，允许用户整合动态编程对象并根据模型结构精细优化自动微分模式。

Abstract: Hierarchical Bayesian models based on Gaussian processes are considered useful for describing complex nonlinear statistical dependencies among variables in real-world data. However, effective Monte Carlo algorithms for inference with these models have not yet been established, except for several simple cases. In this study, we show that, compared with the slow inference achieved with existing program libraries, the performance of Riemannian-manifold Hamiltonian Monte Carlo (RMHMC) can be drastically improved by optimising the computation order according to the model structure and dynamically programming the eigendecomposition. This improvement cannot be achieved when using an existing library based on a naive automatic differentiator. We numerically demonstrate that RMHMC effectively samples from the posterior, allowing the calculation of model evidence, in a Bayesian logistic regression on simulated data and in the estimation of propensity functions for the American national medical expenditure data using several Bayesian multiple-kernel models. These results lay a foundation for implementing effective Monte Carlo algorithms for analysing real-world data with Gaussian processes, and highlight the need to develop a customisable library set that allows users to incorporate dynamically programmed objects and finely optimises the mode of automatic differentiation depending on the model structure.

</details>


### [24] [Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings](https://arxiv.org/abs/2511.06425)
*Brian B. Avants,Nicholas J. Tustison,James R Stone*

Main category: stat.ML

TL;DR: NSA-Flow是一个用于可解释表示学习的矩阵估计框架，通过结合稀疏矩阵分解、正交化和约束流形学习，在保持模型灵活性的同时实现结构化稀疏和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在可解释性和模型灵活性之间难以平衡，限制了从复杂高维数据中提取有意义的见解。

Method: NSA-Flow通过连续平衡重构保真度和列间去相关性来强制结构化稀疏，使用单个可调权重参数化，在Stiefel流形附近作为平滑流运行，具有非负性和自适应梯度控制的近端更新。

Result: 在模拟和真实生物医学数据上，NSA-Flow提高了可解释性和泛化能力，在Golub白血病数据集和阿尔茨海默病研究中保持或改进了相关方法的性能。

Conclusion: NSA-Flow提供了一个可扩展的通用工具，适用于跨数据科学领域的可解释机器学习。

Abstract: Interpretable representation learning is a central challenge in modern machine learning, particularly in high-dimensional settings such as neuroimaging, genomics, and text analysis. Current methods often struggle to balance the competing demands of interpretability and model flexibility, limiting their effectiveness in extracting meaningful insights from complex data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a general-purpose matrix estimation framework that unifies ideas from sparse matrix factorization, orthogonalization, and constrained manifold learning. NSA-Flow enforces structured sparsity through a continuous balance between reconstruction fidelity and column-wise decorrelation, parameterized by a single tunable weight. The method operates as a smooth flow near the Stiefel manifold with proximal updates for non-negativity and adaptive gradient control, yielding representations that are simultaneously sparse, stable, and interpretable. Unlike classical regularization schemes, NSA-Flow provides an intuitive geometric mechanism for manipulating sparsity at the level of global structure while simplifying latent features. We demonstrate that the NSA-Flow objective can be optimized smoothly and integrates seamlessly with existing pipelines for dimensionality reduction while improving interpretability and generalization in both simulated and real biomedical data. Empirical validation on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the NSA-Flow constraints can maintain or improve performance over related methods with little additional methodological effort. NSA-Flow offers a scalable, general-purpose tool for interpretable ML, applicable across data science domains.

</details>


### [25] [Bridging Theory and Practice: A Stochastic Learning-Optimization Model for Resilient Automotive Supply Chains](https://arxiv.org/abs/2511.06479)
*Muhammad Shahnawaz,Adeel Safder*

Main category: stat.ML

TL;DR: 提出了一种结合贝叶斯推理与库存优化的随机学习-优化框架，用于汽车行业供应链管理，在稳定环境中实现7.4%成本降低，在供应中断时提升5.7%性能。


<details>
  <summary>Details</summary>
Motivation: 英国汽车行业依赖准时制制造，面临供应链中断和需求波动的挑战。虽然定性研究指出AI与传统优化结合的潜力，但缺乏正式的定量验证。

Method: 建立两阶段库存系统模型，对比传统静态优化策略与贝叶斯学习持续更新参数估计的自适应策略，在三种运营场景下进行365期模拟。

Result: 集成方法在稳定环境中实现7.4%成本降低，在供应中断时提升5.7%性能，但在需求突然冲击时因贝叶斯更新的保守性而表现受限。

Conclusion: 为从业者观察提供数学验证，建立了理解AI驱动供应链韧性的正式框架，同时确定了成功实施的关键边界条件。

Abstract: Supply chain disruptions and volatile demand pose significant challenges to the UK automotive industry, which relies heavily on Just-In-Time (JIT) manufacturing. While qualitative studies highlight the potential of integrating Artificial Intelligence (AI) with traditional optimization, a formal, quantitative demonstration of this synergy is lacking. This paper introduces a novel stochastic learning-optimization framework that integrates Bayesian inference with inventory optimization for supply chain management (SCM). We model a two-echelon inventory system subject to stochastic demand and supply disruptions, comparing a traditional static optimization policy against an adaptive policy where Bayesian learning continuously updates parameter estimates to inform stochastic optimization. Our simulations over 365 periods across three operational scenarios demonstrate that the integrated approach achieves 7.4\% cost reduction in stable environments and 5.7\% improvement during supply disruptions, while revealing important limitations during sudden demand shocks due to the inherent conservatism of Bayesian updating. This work provides mathematical validation for practitioner observations and establishes a formal framework for understanding AI-driven supply chain resilience, while identifying critical boundary conditions for successful implementation.

</details>


### [26] [Adaptive Testing for Segmenting Watermarked Texts From Language Models](https://arxiv.org/abs/2511.06645)
*Xingchi Li,Xiaochi Liu,Guanxun Li*

Main category: stat.ML

TL;DR: 本文提出了一种基于加权公式和逆变换采样的水印检测方法，能够有效分割包含水印和非水印内容的混合文本，无需精确的提示估计。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，需要区分AI生成文本和人类写作内容以防止错误信息传播和教育滥用。水印技术通过嵌入统计信号来识别LLM生成文本。

Method: 将基于似然的LLM检测方法推广为灵活的加权公式，并适应逆变换采样方法。提出无需精确提示估计的文本分割框架，识别水印和非水印子字符串。

Result: 大量数值实验表明，所提出的方法在准确分割包含水印和非水印内容的混合文本方面既有效又稳健。

Conclusion: 该自适应检测策略为水印检测和文本分割提供了有效的解决方案，克服了先前方法对提示估计高度敏感的局限性。

Abstract: The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.

</details>


### [27] [Lassoed Forests: Random Forests with Adaptive Lasso Post-selection](https://arxiv.org/abs/2511.06698)
*Jing Shang,James Bannon,Benjamin Haibe-Kains,Robert Tibshirani*

Main category: stat.ML

TL;DR: 本文提出了一个结合随机森林和Lasso选择的自适应加权统一框架，理论上证明该方法能严格优于标准随机森林和Lasso加权随机森林，并通过仿真和实际数据集验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 随机森林的改进方法（如对树预测应用Lasso回归）虽然旨在减少模型偏差，但有时会降低性能。本文旨在解决这种性能退化问题，并探索更好的组合方法。

Method: 提出了一个统一框架，通过自适应加权结合随机森林和Lasso选择，数学上证明该方法能严格优于其他两种方法，并通过仿真（包括偏差-方差分解、误差估计评估和变量重要性分析）和实际数据集应用进行验证。

Result: 理论分析和仿真结果表明，所提出的自适应加权方法在性能上严格优于标准随机森林和Lasso加权随机森林，特别是在不同信噪比条件下表现稳定。

Conclusion: 自适应加权框架为随机森林的改进提供了更优的解决方案，能够有效避免单一方法可能带来的性能退化，具有广泛的适用性。

Abstract: Random forests are a statistical learning technique that use bootstrap aggregation to average high-variance and low-bias trees. Improvements to random forests, such as applying Lasso regression to the tree predictions, have been proposed in order to reduce model bias. However, these changes can sometimes degrade performance (e.g., an increase in mean squared error). In this paper, we show in theory that the relative performance of these two methods, standard and Lasso-weighted random forests, depends on the signal-to-noise ratio. We further propose a unified framework to combine random forests and Lasso selection by applying adaptive weighting and show mathematically that it can strictly outperform the other two methods. We compare the three methods through simulation, including bias-variance decomposition, error estimates evaluation, and variable importance analysis. We also show the versatility of our method by applications to a variety of real-world datasets.

</details>


### [28] [Simulation-based Methods for Optimal Sampling Design in Systems Biology](https://arxiv.org/abs/2511.07197)
*Tuan Minh Ha,Binh Thanh Nguyen,Lam Si Tung Ho*

Main category: stat.ML

TL;DR: 提出了两种不依赖初始参数估计的仿真优化采样设计方法：E-最优排序法和LSTM神经网络方法，在Lotka-Volterra和三室模型中优于随机选择和传统E-最优设计。


<details>
  <summary>Details</summary>
Motivation: 传统基于Fisher信息矩阵的采样设计方法需要初始参数估计，当估计不准确时会导致次优结果，需要不依赖初始参数估计的优化采样方法。

Method: 开发了两种方法：E-最优排序法使用E-最优准则，以及基于长短期记忆神经网络的采样设计方法。

Result: 仿真研究表明，提出的方法在Lotka-Volterra和三室模型中都优于随机选择和传统E-最优设计。

Conclusion: 提出的仿真优化采样设计方法有效解决了传统方法对初始参数估计的依赖问题，提高了参数估计的准确性。

Abstract: In many areas of systems biology, including virology, pharmacokinetics, and population biology, dynamical systems are commonly used to describe biological processes. These systems can be characterized by estimating their parameters from sampled data. The key problem is how to optimally select sampling points to achieve accurate parameter estimation. Classical approaches often rely on Fisher information matrix-based criteria such as A-, D-, and E-optimality, which require an initial parameter estimate and may yield suboptimal results when the estimate is inaccurate. This study proposes two simulation-based methods for optimal sampling design that do not depend on initial parameter estimates. The first method, E-optimal-ranking (EOR), employs the E-optimal criterion, while the second utilizes a Long Short-Term Memory (LSTM) neural network. Simulation studies based on the Lotka-Volterra and three-compartment models demonstrate that the proposed methods outperform both random selection and classical E-optimal design.

</details>


### [29] [Language Generation with Infinite Contamination](https://arxiv.org/abs/2511.07417)
*Anay Mehrotra,Grigoris Velegkas,Xifan Yu,Felix Zhou*

Main category: stat.ML

TL;DR: 本文研究了在受污染枚举下的语言生成问题，证明了当污染比例趋近于零时，语言生成是可实现的，并比较了密集生成与普通生成对污染的鲁棒性差异。


<details>
  <summary>Details</summary>
Motivation: 现有语言生成研究假设数据完美无噪声，但现实数据往往存在污染（噪声插入和遗漏）。本文旨在探索生成算法能够容忍多大程度的污染。

Method: 通过分析受污染枚举下的语言生成条件，建立了污染比例收敛到零时的生成可行性理论，并引入了基于课程学习的超越最坏情况模型。

Result: 1) 当污染比例趋近于零时，所有可数集合的语言生成都是可实现的；2) 密集生成对污染的鲁棒性比普通生成更差；3) 在课程学习模型下，即使存在无限污染，只要污染比例趋近于零，密集生成仍可实现。

Conclusion: 语言生成在污染比例收敛到零的条件下是可行的，课程学习可能是从噪声网络数据中学习的关键方法。

Abstract: We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.
  Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).
  We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.
  Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [30] [wdiexplorer: An R package Designed for Exploratory Analysis of World Development Indicators (WDI) Data](https://arxiv.org/abs/2511.07027)
*Oluwayomi Akinfenwa,Niamh Cahill,Catherine Hurley*

Main category: stat.CO

TL;DR: 本文介绍了wdiexplorer R包，该包提供完整工作流程，用于获取、准备和探索世界发展指标数据库中的国家层面面板数据，通过计算诊断指标和可视化来识别模式与异常值。


<details>
  <summary>Details</summary>
Motivation: 世界发展指标数据库包含大量全球发展数据，但缺乏系统化的探索分析工具。wdiexplorer包旨在提供高效的数据探索、诊断指标计算和可视化功能，帮助用户识别时间序列行为中的模式和异常特征。

Method: 开发wdiexplorer R包，通过WDI包获取数据，提供计算诊断指标和可视化的计算函数，利用预定义分组结构来探索WDI指标数据集。

Result: 成功开发了wdiexplorer包，能够有效探索WDI指标数据，计算诊断指数，并通过可视化识别时间行为模式、异常值和其他有趣特征。以PM2.5空气污染数据集为例展示了功能。

Conclusion: wdiexplorer包为WDI数据库提供了一个全面的数据探索解决方案，使用户能够系统化地分析国家层面面板数据，识别跨国家和组内的时间序列模式与异常值。

Abstract: The World Development Indicators (WDI) database provides a wide range of global development data, maintained and published by the World Bank. Our \textit{wdiexplorer} package offers a comprehensive workflow that sources WDI data via the \textit{WDI} R package, prepares and explores country-level panel data of the WDI through computational functions to calculate diagnostic metrics and visualise the outputs. By leveraging the functionalities of \textit{wdiexplorer} package, users can efficiently explore any indicator dataset of the WDI, compute diagnostic indices, and visualise the metrics by incorporating the pre-defined grouping structures to identify patterns, outliers, and other interesting features of temporal behaviours. This paper presents the \textit{wdiexplorer} package, demonstrates its functionalities using the WDI: PM$_{2.5}$ air pollution dataset, and discusses the observed patterns and outliers across countries and within groups of country-level panel data.

</details>


### [31] [A BGe score for tied-covariance mixtures of Gaussian Bayesian networks](https://arxiv.org/abs/2511.07050)
*Marco Grzegorczyk*

Main category: stat.CO

TL;DR: 提出了一个具有共享协方差矩阵的高斯贝叶斯网络混合模型，推导了其边际似然，并实现了结合结构MCMC和Gibbs采样的MCMC推理方案，在模拟和基准数据上比较了共享协方差和全协方差混合模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯贝叶斯网络混合模型假设每个混合分量都有自己的协方差矩阵，这可能导致模型复杂度过高。作者希望开发一个更简洁的模型，其中所有分量共享一个共同的协方差矩阵。

Method: 提出了共享协方差的高斯贝叶斯网络混合模型，推导了其解析的边际似然（BGe评分度量），并实现了结合结构MCMC和快速Gibbs采样的MCMC推理方案。

Result: 在模拟和基准数据上的实证比较表明，共享协方差混合模型与全协方差混合模型相比具有竞争力，同时模型更简洁。

Conclusion: 共享协方差的高斯贝叶斯网络混合模型提供了一个更简洁的替代方案，其边际似然仍然保持解析形式，在保持性能的同时降低了模型复杂度。

Abstract: Mixtures of Gaussian Bayesian networks have previously been studied under full-covariance assumptions, where each mixture component has its own covariance matrix. We propose a mixture model with tied-covariance, in which all components share a common covariance matrix. Our main contribution is the derivation of its marginal likelihood, which remains analytic. Unlike in the full-covariance case, however, the marginal likelihood no longer factorizes into component-specific terms. We refer to the new likelihood as the BGe scoring metric for tied-covariance mixtures of Gaussian Bayesian networks. For model inference, we implement MCMC schemes combining structure MCMC with a fast Gibbs sampler for mixtures, and we empirically compare the tied- and full-covariance mixtures of Gaussian Bayesian networks on simulated and benchmark data.

</details>


### [32] [Smoothing Out Sticking Points: Sampling from Discrete-Continuous Mixtures with Dynamical Monte Carlo by Mapping Discrete Mass into a Latent Universe](https://arxiv.org/abs/2511.07340)
*Andrew Chin,Akihiko Nishimura*

Main category: stat.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Combining a continuous "slab" density with discrete "spike" mass at zero, spike-and-slab priors provide important tools for inducing sparsity and carrying out variable selection in Bayesian models. However, the presence of discrete mass makes posterior inference challenging. "Sticky" extensions to piecewise-deterministic Markov process samplers have shown promising performance, where sampling from the spike is achieved by the process sticking there for an exponentially distributed duration. As it turns out, the sampler remains valid when the exponential sticking time is replaced with its expectation. We justify this by mapping the spike to a continuous density over a latent universe, allowing the sampler to be reinterpreted as traversing this universe while being stuck in the original space. This perspective opens up an array of possibilities to carry out posterior computation under spike-and-slab type priors. Notably, it enables us to construct sticky samplers using other dynamics-based paradigms such as Hamiltonian Monte Carlo, and, in fact, original sticky process can be established as a partial position-momentum refreshment limit of our Hamiltonian sticky sampler. Further, our theoretical and empirical findings suggest these alternatives to be at least as efficient as the original sticky approach.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [33] [Multilevel non-linear interrupted time series analysis](https://arxiv.org/abs/2511.05725)
*RJ Waken,Fengxian Wang,Sarah A. Eisenstein,Tim McBride,Kim Johnson,Karen Joynt-Maddox*

Main category: stat.AP

TL;DR: 提出了一种结合广义可加模型和贝叶斯多层次时间序列模型的方法，用于分析非线性中断效应，支持后分层处理，并在三个医疗应用场景中验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析非线性中断效应时缺乏多层次结构和后分层能力，难以有效表征不同亚群体间的因果效应差异。

Method: 结合广义可加模型和贝叶斯多层次时间序列模型，引入分层模型选择先验，鼓励简约性和部分池化，同时纳入有意义的因果效应变异性。

Result: 在三个应用案例中成功表征了中断效应：PSA检测对前列腺癌诊断率的影响、COVID-19疫情对中风住院率的影响、密苏里州医疗补助扩展对住院支付方式的影响。

Conclusion: 该方法能够有效表征多层次结构中的非线性中断效应，支持后分层分析，为不同亚群体的因果效应差异研究提供了有力工具。

Abstract: Recent advances in interrupted time series analysis permit characterization of a typical non-linear interruption effect through use of generalized additive models. Concurrently, advances in latent time series modeling allow efficient Bayesian multilevel time series models. We propose to combine these concepts with a hierarchical model selection prior to characterize interruption effects with a multilevel structure, encouraging parsimony and partial pooling while incorporating meaningful variability in causal effects across subpopulations of interest, while allowing poststratification. These models are demonstrated with three applications: 1) the effect of the introduction of the prostate specific antigen test on prostate cancer diagnosis rates by race and age group, 2) the change in stroke or trans-ischemic attack hospitalization rates across Medicare beneficiaries by rurality in the months after the start of the COVID-19 pandemic, and 3) the effect of Medicaid expansion in Missouri on the proportion of inpatient hospitalizations discharged with Medicaid as a primary payer by key age groupings and sex.

</details>


### [34] [On the Development of Probabilistic Projections of Country-level Progress to the UN SDG Indicator of Minimum Proficiency in Reading and Mathematics](https://arxiv.org/abs/2511.06107)
*David Kaplan,Nina Jude,Kjorte Harra,Jonas Stampka*

Main category: stat.AP

TL;DR: 本文使用贝叶斯潜在增长曲线模型和贝叶斯模型平均方法，对SDG指标4.1.1（初中生阅读和数学最低熟练度）的进展进行概率预测，为各国到2030年实现可持续发展目标提供预测。


<details>
  <summary>Details</summary>
Motivation: 各国需要预测到2030年实现可持续发展目标的进展，但现有统计方法未能使用专门设计的最优预测方法来估计进展速度。

Method: 结合OECD PISA数据及世界银行、OECD、UNDP和UNESCO的指标，采用贝叶斯潜在增长曲线建模和贝叶斯模型平均的新颖组合方法。

Result: 获得了最低熟练度百分比进展速度的最优估计，并基于此开发了所有分析国家的未来概率预测，同时展示了四个案例国家的个体预测。

Conclusion: 该方法可为各国提供SDG指标4.1.1进展的可靠概率预测，支持2030年可持续发展目标的实现评估。

Abstract: As of this writing, there are five years remaining for countries to reach their Sustainable Development Goals deadline of 2030 as agreed to by the member countries of the United Nations. Countries are, therefore, naturally interested in projections of progress toward these goals. A variety of statistical measures have been used to report on country-level progress toward the goals, but they have not utilized methodologies explicitly designed to obtain optimally predictive measures of rate of progress as the foundation for projecting trends. The focus of this paper is to provide Bayesian probabilistic projections of progress to SDG indicator 4.1.1, attaining minimum proficiency in reading and mathematics, with particular emphasis on competencies among lower secondary school children. Using data from the OECD PISA, as well as indicators drawn from the World Bank, the OECD, UNDP, and UNESCO, we employ a novel combination of Bayesian latent growth curve modeling Bayesian model averaging to obtain optimal estimates of the rate of progress in minimum proficiency percentages and then use those estimate to develop probabilistic projections into the future overall for all countries in the analysis. Four case study countries are also presented to show how the methods can be used for individual country projections.

</details>


### [35] [A unified approach to spatial domain detection and cell-type deconvolution in spot-based spatial transcriptomics](https://arxiv.org/abs/2511.06204)
*Hyun Jung Koo,Aaron J. Molstad*

Main category: stat.AP

TL;DR: 提出了DUET方法，能够同时识别空间域和估计细胞类型比例，在聚类和解卷积方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有空间转录组数据以"点"为单位测量基因表达，每个点包含多种细胞类型，需要同时识别空间域和细胞类型比例以获得生物学见解

Method: 使用基于模型的凸聚类的约束版本，能够处理泊松、负二项、正态等多种表达数据类型

Result: 通过模拟研究和多个应用验证，DUET在聚类和解卷积性能上优于现有方法

Conclusion: DUET提供了一种有效的方法来识别空间域并估计细胞类型比例，增强了空间转录组数据的可解释性

Abstract: Many popular technologies for generating spatially resolved transcriptomic (SRT) data measure gene expression at the resolution of a "spot", i.e., a small tissue region 55 microns in diameter. Each spot can contain many cells of different types. In typical analyses, researchers are interested in using these data to identify discrete spatial domains in the tissue. In this paper, we propose a new method, DUET, that simultaneously identifies discrete spatial domains and estimates each spot's cell-type proportion. This allows the identified spatial domains to be characterized in terms of the cell type proportions, which affords interpretability and biological insight. DUET utilizes a constrained version of model-based convex clustering, and as such, can accommodate Poisson, negative binomial, normal, and other types of expression data. Through simulation studies and multiple applications, we show that our method can achieve better clustering and deconvolution performance than existing methods.

</details>


### [36] [Bayesian spatio-temporal disaggregation modeling using a diffusion-SPDE approach: a case study of Aerosol Optical Depth in India](https://arxiv.org/abs/2511.06276)
*Fernando Rodriguez Avellaneda,Paula Moraga*

Main category: stat.AP

TL;DR: 提出了一种时空分解模型，通过高斯过程和SPDE方法将粗分辨率的AOD观测数据分解到更高时空分辨率，在印度地区实现了从0.75°到0.25°的空间分辨率和从3小时到1小时的时间分辨率提升。


<details>
  <summary>Details</summary>
Motivation: 气溶胶光学厚度(AOD)是评估空气质量和气候变化的重要指标，但现有卫星遥感数据时空分辨率较低，无法满足政策制定和干预措施的需求。需要开发能够生成高时空分辨率AOD估计的方法。

Method: 采用基于扩散的时空随机偏微分方程(SPDE)构建可分离或不可分离协方差结构的高斯过程模型，将离散观测与连续域联系起来，并纳入协变量提高解释能力。使用INLA-SPDE框架进行贝叶斯推断以实现计算效率。

Result: 模拟研究和印度地区550nm AOD临近预报应用表明，该方法有效提升了时空分辨率：空间分辨率从0.75°提高到0.25°，时间分辨率从3小时提高到1小时。

Conclusion: 所提出的时空分解模型能够成功生成高时空分辨率的AOD估计，为气候变化研究和公共卫生政策制定提供了更精细的数据支持。

Abstract: Accurate estimation of Aerosol Optical Depth (AOD) is crucial for understanding climate change and its impacts on public health, as aerosols are a measure of air quality conditions. AOD is usually retrieved from satellite imagery at coarse spatial and temporal resolutions. However, producing high-resolution AOD estimates in both space and time can better support evidence-based policies and interventions. We propose a spatio-temporal disaggregation model that assumes a latent spatio--temporal continuous Gaussian process observed through aggregated measurements. The model links discrete observations to the continuous domain and accommodates covariates to improve explanatory power and interpretability. The approach employs Gaussian processes with separable or non-separable covariance structures derived from a diffusion-based spatio-temporal stochastic partial differential equation (SPDE). Bayesian inference is conducted using the INLA-SPDE framework for computational efficiency. Simulation studies and an application to nowcasting AOD at 550 nm in India demonstrate the model's effectiveness, improving spatial resolution from 0.75° to 0.25° and temporal resolution from 3 hours to 1 hour.

</details>


### [37] [Bayesian Predictive Probabilities for Online Experimentation](https://arxiv.org/abs/2511.06320)
*Abbas Zaidi,Rina Friedberg,Samir Khan,Yao-Yang Leow,Maulik Soneji,Houssam Nassif,Richard Mudd*

Main category: stat.AP

TL;DR: 提出基于贝叶斯预测概率的系统，用于在不影响实验保真度的情况下进行中期分析，解决A/B测试中容量限制和临时窥视方法的问题。


<details>
  <summary>Details</summary>
Motivation: 在线随机对照实验(A/B测试)的广泛采用导致持续容量限制，需要中期分析。平台用户越来越多地使用临时窥视方法来优化有限资源，但这些方法容易出错且与实验最终结果不一致(如膨胀的I类错误)。

Method: 引入基于贝叶斯预测概率的系统，无需数值积分技术即可估计预测概率，并推荐系统来大规模研究其特性作为持续健康检查，提供系统设计建议。

Result: 在Instagram的实验数据上展示了该方法带来的实际效益，证明可以在不损害实验保真度的情况下进行中期分析。

Conclusion: 贝叶斯预测概率系统为A/B测试中的中期分析提供了可行解决方案，能够有效管理容量约束，同时保持实验的统计完整性。

Abstract: The widespread adoption of online randomized controlled experiments (A/B Tests) for decision-making has created ongoing capacity constraints which necessitate interim analyses. As a consequence, platform users are increasingly motivated to use ad-hoc means of optimizing limited resources via peeking. Such processes, however, are error prone and often misaligned with end-of-experiment outcomes (e.g., inflated type-I error). We introduce a system based on Bayesian Predictive Probabilities that enable us to perform interim analyses without compromising fidelity of the experiment; This idea has been widely utilized in applications outside of the technology domain to more efficiently make decisions in experiments. Motivated by at-scale deployment within an experimentation platform, we demonstrate how predictive probabilities can be estimated without numerical integration techniques and recommend systems to study its properties at scale as an ongoing health check, along with system design recommendations - all on experiment data from Instagram - to demonstrate practical benefits that it enables.

</details>


### [38] [Conservative Software Reliability Assessments Using Collections of Bayesian Inference Problems](https://arxiv.org/abs/2511.07038)
*Kizito Salako,Rabiu Tsoho Muhammad*

Main category: stat.AP

TL;DR: 本文研究贝叶斯推断在保守软件可靠性评估中的应用，通过伯努利过程建模软件故障，确定后验预测概率的最坏情况值，并推导其渐近性质和先验分布特性。


<details>
  <summary>Details</summary>
Motivation: 在支持保守软件可靠性评估时，需要从贝叶斯推断问题集合中确定最坏情况的后验预测概率，以评估软件在最不利条件下的可靠性。

Method: 使用伯努利过程建模软件故障发生，从贝叶斯推断问题集合中明确确定软件未来无故障运行的最坏情况后验预测概率。

Result: 推导了这些保守后验概率及其先验的渐近性质，并展示了如何在安全关键软件评估中应用这些结果。

Conclusion: 这项工作扩展了鲁棒贝叶斯推断结果和所谓的保守贝叶斯推断方法，为安全关键软件提供了更可靠的评估工具。

Abstract: When using Bayesian inference to support conservative software reliability assessments, it is useful to consider a collection of Bayesian inference problems, with the aim of determining the worst-case value (from this collection) for a posterior predictive probability that characterizes how reliable the software is. Using a Bernoulli process to model the occurrence of software failures, we explicitly determine (from collections of Bayesian inference problems) worst-case posterior predictive probabilities of the software operating without failure in the future. We deduce asymptotic properties of these conservative posterior probabilities and their priors, and illustrate how to use these results in assessments of safety-critical software. This work extends robust Bayesian inference results and so-called conservative Bayesian inference methods.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [39] [Impacts of Data Splitting Strategies on Parameterized Link Prediction Algorithms](https://arxiv.org/abs/2511.05834)
*Xinshan Jiao,Yuxin Luo,Yilin Bi,Tao Zhou*

Main category: stat.OT

TL;DR: 该研究提出了一种新的评估指标Loss Ratio，用于量化链路预测中由于超参数调优时使用测试集导致的信息泄露问题，通过60个真实网络的大规模实验揭示了性能被高估约3.6%的问题。


<details>
  <summary>Details</summary>
Motivation: 随着参数化模型的广泛采用，链路预测评估协议的严谨性变得至关重要。先前在超参数调优中使用测试集的常见做法导致了人为信息泄露，从而夸大了报告的性能。

Method: 引入新的评估指标Loss Ratio来量化性能高估程度，在60个真实网络（涵盖6个领域）上进行大规模实验，分析不同算法的信息泄露影响。

Result: 信息泄露导致平均性能高估约3.6%，特定算法偏差超过15%。启发式和基于随机游走的方法表现出更强的鲁棒性和稳定性。

Conclusion: 分析揭示了链路预测评估中普遍存在的信息泄露问题，强调了采用标准化数据分割策略对于实现公平和可复现的链路预测模型基准测试的必要性。

Abstract: Link prediction is a fundamental problem in network science, aiming to infer potential or missing links based on observed network structures. With the increasing adoption of parameterized models, the rigor of evaluation protocols has become critically important. However, a previously common practice of using the test set during hyperparameter tuning has led to human-induced information leakage, thereby inflating the reported model performance. To address this issue, this study introduces a novel evaluation metric, Loss Ratio, which quantitatively measures the extent of performance overestimation. We conduct large-scale experiments on 60 real-world networks across six domains. The results demonstrate that the information leakage leads to an average overestimation about 3.6\%, with the bias reaching over 15\% for specific algorithms. Meanwhile, heuristic and random-walk-based methods exhibit greater robustness and stability. The analysis uncovers a pervasive information leakage issue in link prediction evaluation and underscores the necessity of adopting standardized data splitting strategies to enable fair and reproducible benchmarking of link prediction models.

</details>
