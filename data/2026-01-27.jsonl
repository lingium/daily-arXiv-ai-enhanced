{"id": "2601.16318", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16318", "abs": "https://arxiv.org/abs/2601.16318", "authors": ["Rebecca EA Walwyn", "Rosemary A Bailey", "Arpan Singh", "Neil Corrigan", "Steven G Gilmour"], "title": "Orthogonal factorial designs for trials of therapist-delivered interventions: Randomising intervention-therapist combinations to patients", "comment": null, "summary": "It is recognised that treatment-related clustering should be allowed for in the sample size and analyses of individually-randomised parallel-group trials that evaluate therapist-delivered interventions such as psychotherapy. Here, interventions are a treatment factor, but therapists are not. If the aim of a trial is to separate effects of therapists from those of interventions, we propose that interventions and therapists should be regarded as two potentially interacting treatment factors (one fixed, one random) with a factorial structure. We consider the specific design where each therapist delivers each intervention (crossed therapist-intervention design), and the resulting therapist-intervention combinations are randomised to patients. We adopt a classical Design of Experiments (DoE) approach to propose a family of orthogonal factorial designs and their associated data analyses, which allow for therapist learning and centre too. We set out the associated data analyses using ANOVA and regression and report the results of a small simulation study conducted to explore the performance of the proposed randomisation methods in estimating the intervention effect and its standard error, the between-therapist variance and the between-therapist variance in the intervention effect. We conclude that more purposeful trial design has the potential to lead to better evidence on a range of complex interventions and outline areas for further methodological research."}
{"id": "2601.16364", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16364", "abs": "https://arxiv.org/abs/2601.16364", "authors": ["Jongmin Mun", "Jeong Hoon Jang"], "title": "Hybrid Partial Least Squares Regression with Multiple Functional and Scalar Predictors", "comment": "39 pages (main 21 pages), 8 figures, 3 tables", "summary": "Motivated by renal imaging studies that combine renogram curves with pharmacokinetic and demographic covariates, we propose Hybrid partial least squares (Hybrid PLS) for simultaneous supervised dimension reduction and regression in the presence of cross-modality correlations. The proposed approach embeds multiple functional and scalar predictors into a unified hybrid Hilbert space and rigorously extends the nonlinear iterative PLS (NIPALS) algorithm. This theoretical development is complemented by a sample-level algorithm that incorporates roughness penalties to control smoothness. By exploiting the rank-one structure of the resulting optimization problem, the algorithm admits a computationally efficient closed-form solution that requires solving only linear systems at each iteration. We establish fundamental geometric properties of the proposed framework, including orthogonality of the latent scores and PLS directions. Extensive numerical studies on synthetic data, together with an application to a renal imaging study, validate these theoretical results and demonstrate the method's ability to recover predictive structure under intermodal multicollinearity, yielding parsimonious low-dimensional representations."}
{"id": "2601.16368", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16368", "abs": "https://arxiv.org/abs/2601.16368", "authors": ["Simon Mack", "Marc Ditzhaus", "Merle Munko", "Markus Pauly"], "title": "Inference for competing risks based on area between curves statistics", "comment": "25 pages, 9 figures, 2 tables", "summary": "In competing risks models, cumulative incidence functions are commonly compared to infer differences between groups. Many existing inference methods, however, struggle when these functions cross during the time frame of interest. To address this problem, we investigate a test statistic based on the area between cumulative incidence functions. As the corresponding limiting distribution depends on quantities that are typically unknown, we propose a wild bootstrap approach to obtain a feasible and asymptotically valid two-sample test. The finite sample performance of the proposed method, in comparison with existing methods, is examined in an extensive simulation study."}
{"id": "2601.16385", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16385", "abs": "https://arxiv.org/abs/2601.16385", "authors": ["Jiazhen Xu", "Han Lin Shang"], "title": "Spherical Spatial Autoregressive Model for Spherically Embedded Spatial Data", "comment": null, "summary": "Spherically embedded spatial data are spatially indexed observations whose values naturally reside on or can be equivalently mapped to the unit sphere. Such data are increasingly ubiquitous in fields ranging from geochemistry to demography. However, analysing such data presents unique difficulties due to the intrinsic non-Euclidean nature of the sphere, and rigorous methodologies for statistical modelling, inference, and uncertainty quantification remain limited. This paper introduces a unified framework to address these three limitations for spherically embedded spatial data. We first propose a novel spherical spatial autoregressive model that leverages optimal transport geometry and then extend it to accommodate exogenous covariates. Second, for either scenario with or without covariates, we establish the asymptotic properties of the estimators and derive a distribution-free Wald test for spatial dependence, complemented by a bootstrap procedure to enhance finite-sample performance. Third, we contribute a novel approach to uncertainty quantification by developing a conformal prediction procedure specifically tailored to spherically embedded spatial data. The practical utility of these methodological advances is illustrated through extensive simulations and applications to Spanish geochemical compositions and Japanese age-at-death mortality distributions."}
{"id": "2601.16696", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2601.16696", "abs": "https://arxiv.org/abs/2601.16696", "authors": ["Jakob Robnik", "Uroš Seljak"], "title": "Faster parallel MCMC: Metropolis adjustment is best served warm", "comment": "21 pages, 9 Figures", "summary": "Despite the enormous success of Hamiltonian Monte Carlo and related Markov Chain Monte Carlo (MCMC) methods, sampling often still represents the computational bottleneck in scientific applications. Availability of parallel resources can significantly speed up MCMC inference by running a large number of chains in parallel, each collecting a single sample. However, the parallel approach converges slowly if the chains are not initialized close to the target distribution (cold start). Theoretically this can be resolved by initially running MCMC without Metropolis-Hastings adjustment to quickly converge to the vicinity of the target distribution and then turn on adjustment to achieve fine convergence. However, no practical scheme uses this strategy, due to the difficulty of automatically selecting the step size during the unadjusted phase. We here develop Late Adjusted Parallel Sampler (LAPS), which is precisely such a scheme and is applicable out of the box, all the hyperparameters are selected automatically. LAPS takes advantage of ensemble-based hyperparameter adaptation to estimate the bias at each iteration and converts it to the appropriate step size. We show that LAPS consistently and significantly outperforms ensemble adjusted methods such as MEADS or ChESS and the optimization-based initializer Pathfinder on a variety of standard benchmark problems. LAPS typically achieves two orders of magnitude lower wall-clock time than the corresponding sequential algorithms such as NUTS."}
{"id": "2601.16250", "categories": ["stat.ML", "cs.CE", "cs.LG", "math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.16250", "abs": "https://arxiv.org/abs/2601.16250", "authors": ["Olof Hallqvist Elias", "Michael Selby", "Phillip Stanley-Marbell"], "title": "Distributional Computational Graphs: Error Bounds", "comment": "28 pages, 2 figures", "summary": "We study a general framework of distributional computational graphs: computational graphs whose inputs are probability distributions rather than point values. We analyze the discretization error that arises when these graphs are evaluated using finite approximations of continuous probability distributions. Such an approximation might be the result of representing a continuous real-valued distribution using a discrete representation or from constructing an empirical distribution from samples (or might be the output of another distributional computational graph). We establish non-asymptotic error bounds in terms of the Wasserstein-1 distance, without imposing structural assumptions on the computational graph."}
{"id": "2601.16340", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16340", "abs": "https://arxiv.org/abs/2601.16340", "authors": ["Zhentao Yu", "Jiaqi Ding", "Guorong Wu", "Quefeng Li"], "title": "Matrix-Response Generalized Linear Mixed Model with Applications to Longitudinal Brain Images", "comment": null, "summary": "Longitudinal brain imaging data facilitate the monitoring of structural and functional alterations in individual brains across time, offering essential understanding of dynamic neurobiological mechanisms. Such data improve sensitivity for detecting early biomarkers of disease progression and enhance the evaluation of intervention effects. While recent matrix-response regression models can relate static brain networks to external predictors, there remain few statistical methods for longitudinal brain networks, especially those derived from high-dimensional imaging data. We introduce a matrix-response generalized linear mixed model that accommodates longitudinal brain networks and identifies edges whose connectivity is influenced by external predictors. An efficient Monte Carlo Expectation-Maximization algorithm is developed for parameter estimation. Extensive simulations demonstrate effective identification of covariate-related network components and accurate parameter estimation. We further demonstrate the usage of the proposed method through applications to diffusion tensor imaging (DTI) and functional MRI (fMRI) datasets."}
{"id": "2601.16385", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16385", "abs": "https://arxiv.org/abs/2601.16385", "authors": ["Jiazhen Xu", "Han Lin Shang"], "title": "Spherical Spatial Autoregressive Model for Spherically Embedded Spatial Data", "comment": null, "summary": "Spherically embedded spatial data are spatially indexed observations whose values naturally reside on or can be equivalently mapped to the unit sphere. Such data are increasingly ubiquitous in fields ranging from geochemistry to demography. However, analysing such data presents unique difficulties due to the intrinsic non-Euclidean nature of the sphere, and rigorous methodologies for statistical modelling, inference, and uncertainty quantification remain limited. This paper introduces a unified framework to address these three limitations for spherically embedded spatial data. We first propose a novel spherical spatial autoregressive model that leverages optimal transport geometry and then extend it to accommodate exogenous covariates. Second, for either scenario with or without covariates, we establish the asymptotic properties of the estimators and derive a distribution-free Wald test for spatial dependence, complemented by a bootstrap procedure to enhance finite-sample performance. Third, we contribute a novel approach to uncertainty quantification by developing a conformal prediction procedure specifically tailored to spherically embedded spatial data. The practical utility of these methodological advances is illustrated through extensive simulations and applications to Spanish geochemical compositions and Japanese age-at-death mortality distributions."}
{"id": "2601.16431", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16431", "abs": "https://arxiv.org/abs/2601.16431", "authors": ["Ruonan Zheng", "Min-Qian Liu", "Yongdao Zhou", "Xuan Chen"], "title": "Sequential Experimental Designs for Kriging Model", "comment": "24pages, 5figures", "summary": "Computer experiments have become an indispensable alternative to complex physical and engineering experiments. The Kriging model is the most widely used surrogate model, with the core goal of minimizing the discrepancy between the surrogate and true models across the entire experimental domain. However, existing sequential design methods have critical limitations: observation-based batch sequential designs are rarely studied, while one-point sequential designs have insufficient information utilization and suffer from inefficient resource utilization -- they require numerous repeated observation rounds to accumulate sufficient points, leading to prolonged experimental cycles. To address these gaps, this paper proposes two novel one-point sequential design criteria and a general batch sequential design framework. Moreover, the batch sequential design framework solves the inherent point clustering problem in naive batch selection, enabling efficient extension of any sequential criterion to batch scenarios. Simulations on some test functions demonstrate that the proposed methods outperform existing approaches in terms of fitting accuracy in most cases."}
{"id": "2601.16813", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2601.16813", "abs": "https://arxiv.org/abs/2601.16813", "authors": ["Jun Xiao", "Qiong Wang", "Yihui Li", "Zhexuan Yu", "Hao Zhou", "Borong Lin"], "title": "A Fully Automated DM-BIM-BEM Pipeline Enabling Graph-Based Intelligence, Interoperability, and Performance-Driven Early Design", "comment": "Submitted to Advanced Engineering Informatics, currently Under Review", "summary": "Artificial intelligence in construction increasingly depends on structured representations such as Building Information Models and knowledge graphs, yet early-stage building designs are predominantly created as flexible boundary-representation (B-rep) models that lack explicit spatial, semantic, and performance structure. This paper presents a robust, fully automated framework that transforms unstructured B-rep geometry into knowledge-graph-based Building Information Models and further into executable Building Energy Models. The framework enables artificial intelligence to explicitly interpret building elements, spatial topology, and their associated thermal and performance attributes. It integrates automated geometry cleansing, multiple auto space-generation strategies, graph-based extraction of space and element topology, ontology-aligned knowledge modeling, and reversible transformation between ontology-based BIM and EnergyPlus energy models. Validation on parametric, sketch-based, and real-world building datasets demonstrates high robustness, consistent topological reconstruction, and reliable performance-model generation. By bridging design models, BIM, and BEM, the framework provides an AI-oriented infrastructure that extends BIM- and graph-based intelligence pipelines to flexible early-stage design geometry, enabling performance-driven design exploration and optimization by learning-based methods."}
{"id": "2601.16427", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16427", "abs": "https://arxiv.org/abs/2601.16427", "authors": ["Behzad Aalipur", "Yichen Qin"], "title": "Perfect Clustering for Sparse Directed Stochastic Block Models", "comment": null, "summary": "Exact recovery in stochastic block models (SBMs) is well understood in undirected settings, but remains considerably less developed for directed and sparse networks, particularly when the number of communities diverges. Spectral methods for directed SBMs often lack stability in asymmetric, low-degree regimes, and existing non-spectral approaches focus primarily on undirected or dense settings.\n  We propose a fully non-spectral, two-stage procedure for community detection in sparse directed SBMs with potentially growing numbers of communities. The method first estimates the directed probability matrix using a neighborhood-smoothing scheme tailored to the asymmetric setting, and then applies $K$-means clustering to the estimated rows, thereby avoiding the limitations of eigen- or singular value decompositions in sparse, asymmetric networks. Our main theoretical contribution is a uniform row-wise concentration bound for the smoothed estimator, obtained through new arguments that control asymmetric neighborhoods and separate in- and out-degree effects. These results imply the exact recovery of all community labels with probability tending to one, under mild sparsity and separation conditions that allow both $γ_n \\to 0$ and $K_n \\to \\infty$.\n  Simulation studies, including highly directed, sparse, and non-symmetric block structures, demonstrate that the proposed procedure performs reliably in regimes where directed spectral and score-based methods deteriorate. To the best of our knowledge, this provides the first exact recovery guarantee for this class of non-spectral, neighborhood-smoothing methods in the sparse, directed setting."}
{"id": "2601.16347", "categories": ["stat.AP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.16347", "abs": "https://arxiv.org/abs/2601.16347", "authors": ["Erika McPhillips", "Hyeongseong Lee", "Xiangyu Xie", "Kathy Baylis", "Chris Funk", "Mengyang Gu"], "title": "Long-Term Probabilistic Forecast of Vegetation Conditions Using Climate Attributes in the Four Corners Region", "comment": null, "summary": "Weather conditions can drastically alter the state of crops and rangelands, and in turn, impact the incomes and food security of individuals worldwide. Satellite-based remote sensing offers an effective way to monitor vegetation and climate variables on regional and global scales. The annual peak Normalized Difference Vegetation Index (NDVI), derived from satellite observations, is closely associated with crop development, rangeland biomass, and vegetation growth. Although various machine learning methods have been developed to forecast NDVI over short time ranges, such as one-month-ahead predictions, long-term forecasting approaches, such as one-year-ahead predictions of vegetation conditions, are not yet available. To fill this gap, we develop a two-phase machine learning model to forecast the one-year-ahead peak NDVI over high-resolution grids, using the Four Corners region of the Southwestern United States as a testbed. In phase one, we identify informative climate attributes, including precipitation and maximum vapor pressure deficit, and develop the generalized parallel Gaussian process that captures the relationship between climate attributes and NDVI. In phase two, we forecast these climate attributes using historical data at least one year before the NDVI prediction month, which then serve as inputs to forecast the peak NDVI at each spatial grid. We developed open-source tools that outperform alternative methods for both gross NDVI and grid-based NDVI one-year forecasts, providing information that can help farmers and ranchers make actionable plans a year in advance."}
{"id": "2601.16597", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.16597", "abs": "https://arxiv.org/abs/2601.16597", "authors": ["Fabian Bleile", "Sarah Lumpp", "Mathias Drton"], "title": "Efficient Learning of Stationary Diffusions with Stein-type Discrepancies", "comment": null, "summary": "Learning a stationary diffusion amounts to estimating the parameters of a stochastic differential equation whose stationary distribution matches a target distribution. We build on the recently introduced kernel deviation from stationarity (KDS), which enforces stationarity by evaluating expectations of the diffusion's generator in a reproducing kernel Hilbert space. Leveraging the connection between KDS and Stein discrepancies, we introduce the Stein-type KDS (SKDS) as an alternative formulation. We prove that a vanishing SKDS guarantees alignment of the learned diffusion's stationary distribution with the target. Furthermore, under broad parametrizations, SKDS is convex with an empirical version that is $ε$-quasiconvex with high probability. Empirically, learning with SKDS attains comparable accuracy to KDS while substantially reducing computational cost and yields improvements over the majority of competitive baselines."}
{"id": "2601.16470", "categories": ["stat.ME", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.16470", "abs": "https://arxiv.org/abs/2601.16470", "authors": ["Yonatan L. Ashenafi"], "title": "Variational Dimension Lifting for Robust Tracking of Nonlinear Stochastic Dynamics", "comment": null, "summary": "Nonlinear stochastic motion presents significant challenges for Bayesian particle tracking. To address this challenge, this paper proposes a framework to construct an invertible transformation that maps the nonlinear state-space model (SSM) into a higher-dimensional linear Gaussian SSM. This approach allows the application of standard linear-Gaussian inference techniques while maintaining a connection to the dynamics of the original system. The paper derives the necessary conditions for such transformations using Ito's lemma and variational calculus, and illustrates the method on a bistable cubic motion model, radial Brownian process model, and a logistic model with multiplicative noise. Simulations confirm that the transformed linear systems, when projected back, accurately reconstruct the nonlinear dynamics and, in distinct regimes of stiffness and singularity, yield tracking accuracy competitive with conventional filters, while avoiding their structural instabilities."}
{"id": "2601.16585", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.16585", "abs": "https://arxiv.org/abs/2601.16585", "authors": ["Weichang Yu", "Khue-Dung Dang"], "title": "Variational approximate penalized credible regions for Bayesian grouped regression", "comment": null, "summary": "We develop a fast and accurate grouped penalized credible region approach for variable selection and prediction in Bayesian high-dimensional linear regression. Most existing Bayesian methods either are subject to high computational costs due to long Markov Chain Monte Carlo runs or yield ambiguous variable selection results due to non-sparse solution output. The penalized credible region framework yields sparse post-processed estimates that facilitates unambiguous grouped variable selection. High estimation accuracy is achieved by shrinking noise from unimportant groups using a grouped global-local shrinkage prior. To ensure computational scalability, we approximate posterior summaries using coordinate ascent variational inference and recast the penalized credible region framework as a convex optimization problem that admits efficient computations. We prove that the resultant post-processed estimators are both parameter-consistent and variable selection consistent in high-dimensional settings. Theory is developed to justify running the coordinate ascent algorithm for at least two cycles. Through extensive simulations, we demonstrate that our proposed method outperforms state-of-the-art methods in grouped variable selection, prediction, and computation time for several common models including ANOVA and nonparametric varying coefficient models."}
{"id": "2601.16597", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.16597", "abs": "https://arxiv.org/abs/2601.16597", "authors": ["Fabian Bleile", "Sarah Lumpp", "Mathias Drton"], "title": "Efficient Learning of Stationary Diffusions with Stein-type Discrepancies", "comment": null, "summary": "Learning a stationary diffusion amounts to estimating the parameters of a stochastic differential equation whose stationary distribution matches a target distribution. We build on the recently introduced kernel deviation from stationarity (KDS), which enforces stationarity by evaluating expectations of the diffusion's generator in a reproducing kernel Hilbert space. Leveraging the connection between KDS and Stein discrepancies, we introduce the Stein-type KDS (SKDS) as an alternative formulation. We prove that a vanishing SKDS guarantees alignment of the learned diffusion's stationary distribution with the target. Furthermore, under broad parametrizations, SKDS is convex with an empirical version that is $ε$-quasiconvex with high probability. Empirically, learning with SKDS attains comparable accuracy to KDS while substantially reducing computational cost and yields improvements over the majority of competitive baselines."}
{"id": "2601.16837", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16837", "abs": "https://arxiv.org/abs/2601.16837", "authors": ["Edoardo Otranto", "Luca Scaffidi Domianello"], "title": "Spillovers and Co-movements in Multivariate Volatility: A Vector Multiplicative Error Model", "comment": "24 pages, 4 figures", "summary": "Recent developments in financial time series focus on modeling volatility across multiple assets or indices in a multivariate framework, accounting for potential interactions such as spillover effects. Furthermore, the increasing integration of global financial markets provides a similar dynamics (referred to as comovement). In this context, we introduce a novel model for volatility vectors within the Multiplicative Error Model (MEM) class. This framework accommodates both spillover and co-movement effects through a distinct latent component. By adopting a specific parameterization, the model remains computationally feasible even for high-dimensional volatility vectors. To reduce the number of unknown coefficients, we propose a simple model-based clustering procedure. We illustrate the effectiveness of the proposed approach through an empirical application to 29 assets of the Dow Jones Industrial Average index, providing insight into volatility spillovers and shared market dynamics. Comparative analysis against alternative vector MEMs, including a fully parameterized version of the proposed model, demonstrates its superior or at least comparable performance across multiple evaluation criteria."}
{"id": "2601.16585", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.16585", "abs": "https://arxiv.org/abs/2601.16585", "authors": ["Weichang Yu", "Khue-Dung Dang"], "title": "Variational approximate penalized credible regions for Bayesian grouped regression", "comment": null, "summary": "We develop a fast and accurate grouped penalized credible region approach for variable selection and prediction in Bayesian high-dimensional linear regression. Most existing Bayesian methods either are subject to high computational costs due to long Markov Chain Monte Carlo runs or yield ambiguous variable selection results due to non-sparse solution output. The penalized credible region framework yields sparse post-processed estimates that facilitates unambiguous grouped variable selection. High estimation accuracy is achieved by shrinking noise from unimportant groups using a grouped global-local shrinkage prior. To ensure computational scalability, we approximate posterior summaries using coordinate ascent variational inference and recast the penalized credible region framework as a convex optimization problem that admits efficient computations. We prove that the resultant post-processed estimators are both parameter-consistent and variable selection consistent in high-dimensional settings. Theory is developed to justify running the coordinate ascent algorithm for at least two cycles. Through extensive simulations, we demonstrate that our proposed method outperforms state-of-the-art methods in grouped variable selection, prediction, and computation time for several common models including ANOVA and nonparametric varying coefficient models."}
{"id": "2601.16347", "categories": ["stat.AP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.16347", "abs": "https://arxiv.org/abs/2601.16347", "authors": ["Erika McPhillips", "Hyeongseong Lee", "Xiangyu Xie", "Kathy Baylis", "Chris Funk", "Mengyang Gu"], "title": "Long-Term Probabilistic Forecast of Vegetation Conditions Using Climate Attributes in the Four Corners Region", "comment": null, "summary": "Weather conditions can drastically alter the state of crops and rangelands, and in turn, impact the incomes and food security of individuals worldwide. Satellite-based remote sensing offers an effective way to monitor vegetation and climate variables on regional and global scales. The annual peak Normalized Difference Vegetation Index (NDVI), derived from satellite observations, is closely associated with crop development, rangeland biomass, and vegetation growth. Although various machine learning methods have been developed to forecast NDVI over short time ranges, such as one-month-ahead predictions, long-term forecasting approaches, such as one-year-ahead predictions of vegetation conditions, are not yet available. To fill this gap, we develop a two-phase machine learning model to forecast the one-year-ahead peak NDVI over high-resolution grids, using the Four Corners region of the Southwestern United States as a testbed. In phase one, we identify informative climate attributes, including precipitation and maximum vapor pressure deficit, and develop the generalized parallel Gaussian process that captures the relationship between climate attributes and NDVI. In phase two, we forecast these climate attributes using historical data at least one year before the NDVI prediction month, which then serve as inputs to forecast the peak NDVI at each spatial grid. We developed open-source tools that outperform alternative methods for both gross NDVI and grid-based NDVI one-year forecasts, providing information that can help farmers and ranchers make actionable plans a year in advance."}
{"id": "2601.16932", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16932", "abs": "https://arxiv.org/abs/2601.16932", "authors": ["Hyojung Jang", "Peter M. Graffy", "Benjamin W. Barrett", "Daniel E. Horton", "Jennifer L. Chan", "Abel N. Kho"], "title": "Identifying heat-related diagnoses in emergency department visits among adults in Chicago: a heat-wide association study", "comment": null, "summary": "Extreme heat is an escalating public health concern. Although prior studies have examined heat-health associations, their reliance on restricted diagnoses and diagnostic categories misses or misclassifies heat-related illness. We conducted a heat-wide association study to identify acute-care diagnoses associated with extreme heat in Chicago, Illinois. Using 916,904 acute-care visits -- including emergency department and urgent care encounters -- among 372,140 adults across five healthcare systems from 2011-2023, we applied a two-stage analytic approach: quasi-Poisson regression to screen 1,803 diagnosis codes for heat-related risks, followed by distributed lag non-linear models in a time-stratified case-crossover design to refine the list of heat-related diagnoses and estimate same-day and short-term cumulative odds ratios of acute-care visits during extreme heat versus reference temperature. We observed same-day increases in visits for heat illness, volume depletion, hypotension, edema, acute kidney failure, and multiple injuries. By analyzing the full diagnostic spectrum of acute-care services, this study comprehensively characterizes heat-associated morbidity, reinforcing and advancing existing literature."}
{"id": "2601.16595", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16595", "abs": "https://arxiv.org/abs/2601.16595", "authors": ["Dafne Zorzetto", "Zizhao Xie", "Julian Stamp", "Arman Oganisian", "Roberta De Vito"], "title": "Bayesian Nonparametric Causal Inference for High-Dimensional Nutritional Data via Factor-Based Exposure Mapping", "comment": null, "summary": "Diet plays a crucial role in health, and understanding the causal effects of dietary patterns is essential for informing public health policy and personalized nutrition strategies. However, causal inference in nutritional epidemiology faces several challenges: (i) high-dimensional and correlated food/nutrient intake data induce massive treatment levels; (ii) nutritional studies are interested in latent dietary patterns rather than single food items; and (iii) the goal is to estimate heterogeneous causal effects of these dietary patterns on health outcomes. We address these challenges by introducing a sophisticated exposure mapping framework that reduces the high-dimensional treatment space via factor analysis and enables the identification of dietary patterns. We also extend the Bayesian Causal Forest to accommodate three ordered levels of dietary exposure, better capturing the complex structure of nutritional data and enabling estimation of heterogeneous causal effects. We evaluate the proposed method through extensive simulations and apply it to a multi-center epidemiological study of Hispanic/Latino adults residing in the US. Using high-dimensional dietary data, we identify six dietary patterns and estimate their causal link with two key health risk factors: body mass index and fasting insulin levels. Our findings suggest that higher consumption of plant lipid-antioxidant, plant-based, animal protein, and dairy product patterns is associated with reduced risk."}
{"id": "2601.16636", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.16636", "abs": "https://arxiv.org/abs/2601.16636", "authors": ["A. Hatstatt", "X. Zhu", "B. Sudret"], "title": "Conformal prediction for full and sparse polynomial chaos expansions", "comment": null, "summary": "Polynomial Chaos Expansions (PCEs) are widely recognized for their efficient computational performance in surrogate modeling. Yet, a robust framework to quantify local model errors is still lacking. While the local uncertainty of PCE prediction can be captured using bootstrap resampling, other methods offering more rigorous statistical guarantees are needed, especially in the context of small training datasets. Recently, conformal predictions have demonstrated strong potential in machine learning, providing statistically robust and model-agnostic prediction intervals. Due to its generality and versatility, conformal prediction is especially valuable, as it can be adapted to suit a variety of problems, making it a compelling choice for PCE-based surrogate models. In this contribution, we explore its application to PCE-based surrogate models. More precisely, we present the integration of two conformal prediction methods, namely the full conformal and the Jackknife+ approaches, into both full and sparse PCEs. For full PCEs, we introduce computational shortcuts inspired by the inherent structure of regression methods to optimize the implementation of both conformal methods. For sparse PCEs, we incorporate the two approaches with appropriate modifications to the inference strategy, thereby circumventing the non-symmetrical nature of the regression algorithm and ensuring valid prediction intervals. Our developments yield better-calibrated prediction intervals for both full and sparse PCEs, achieving superior coverage over existing approaches, such as the bootstrap, while maintaining a moderate computational cost."}
{"id": "2601.16385", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16385", "abs": "https://arxiv.org/abs/2601.16385", "authors": ["Jiazhen Xu", "Han Lin Shang"], "title": "Spherical Spatial Autoregressive Model for Spherically Embedded Spatial Data", "comment": null, "summary": "Spherically embedded spatial data are spatially indexed observations whose values naturally reside on or can be equivalently mapped to the unit sphere. Such data are increasingly ubiquitous in fields ranging from geochemistry to demography. However, analysing such data presents unique difficulties due to the intrinsic non-Euclidean nature of the sphere, and rigorous methodologies for statistical modelling, inference, and uncertainty quantification remain limited. This paper introduces a unified framework to address these three limitations for spherically embedded spatial data. We first propose a novel spherical spatial autoregressive model that leverages optimal transport geometry and then extend it to accommodate exogenous covariates. Second, for either scenario with or without covariates, we establish the asymptotic properties of the estimators and derive a distribution-free Wald test for spatial dependence, complemented by a bootstrap procedure to enhance finite-sample performance. Third, we contribute a novel approach to uncertainty quantification by developing a conformal prediction procedure specifically tailored to spherically embedded spatial data. The practical utility of these methodological advances is illustrated through extensive simulations and applications to Spanish geochemical compositions and Japanese age-at-death mortality distributions."}
{"id": "2601.16636", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.16636", "abs": "https://arxiv.org/abs/2601.16636", "authors": ["A. Hatstatt", "X. Zhu", "B. Sudret"], "title": "Conformal prediction for full and sparse polynomial chaos expansions", "comment": null, "summary": "Polynomial Chaos Expansions (PCEs) are widely recognized for their efficient computational performance in surrogate modeling. Yet, a robust framework to quantify local model errors is still lacking. While the local uncertainty of PCE prediction can be captured using bootstrap resampling, other methods offering more rigorous statistical guarantees are needed, especially in the context of small training datasets. Recently, conformal predictions have demonstrated strong potential in machine learning, providing statistically robust and model-agnostic prediction intervals. Due to its generality and versatility, conformal prediction is especially valuable, as it can be adapted to suit a variety of problems, making it a compelling choice for PCE-based surrogate models. In this contribution, we explore its application to PCE-based surrogate models. More precisely, we present the integration of two conformal prediction methods, namely the full conformal and the Jackknife+ approaches, into both full and sparse PCEs. For full PCEs, we introduce computational shortcuts inspired by the inherent structure of regression methods to optimize the implementation of both conformal methods. For sparse PCEs, we incorporate the two approaches with appropriate modifications to the inference strategy, thereby circumventing the non-symmetrical nature of the regression algorithm and ensuring valid prediction intervals. Our developments yield better-calibrated prediction intervals for both full and sparse PCEs, achieving superior coverage over existing approaches, such as the bootstrap, while maintaining a moderate computational cost."}
{"id": "2601.16427", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16427", "abs": "https://arxiv.org/abs/2601.16427", "authors": ["Behzad Aalipur", "Yichen Qin"], "title": "Perfect Clustering for Sparse Directed Stochastic Block Models", "comment": null, "summary": "Exact recovery in stochastic block models (SBMs) is well understood in undirected settings, but remains considerably less developed for directed and sparse networks, particularly when the number of communities diverges. Spectral methods for directed SBMs often lack stability in asymmetric, low-degree regimes, and existing non-spectral approaches focus primarily on undirected or dense settings.\n  We propose a fully non-spectral, two-stage procedure for community detection in sparse directed SBMs with potentially growing numbers of communities. The method first estimates the directed probability matrix using a neighborhood-smoothing scheme tailored to the asymmetric setting, and then applies $K$-means clustering to the estimated rows, thereby avoiding the limitations of eigen- or singular value decompositions in sparse, asymmetric networks. Our main theoretical contribution is a uniform row-wise concentration bound for the smoothed estimator, obtained through new arguments that control asymmetric neighborhoods and separate in- and out-degree effects. These results imply the exact recovery of all community labels with probability tending to one, under mild sparsity and separation conditions that allow both $γ_n \\to 0$ and $K_n \\to \\infty$.\n  Simulation studies, including highly directed, sparse, and non-symmetric block structures, demonstrate that the proposed procedure performs reliably in regimes where directed spectral and score-based methods deteriorate. To the best of our knowledge, this provides the first exact recovery guarantee for this class of non-spectral, neighborhood-smoothing methods in the sparse, directed setting."}
{"id": "2601.16739", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16739", "abs": "https://arxiv.org/abs/2601.16739", "authors": ["Claudio Agostinelli"], "title": "Comments on \"Challenges of cellwise outliers\" by Jakob Raymaekers and Peter J. Rousseeuw", "comment": "Comment on arXiv:2302.02156", "summary": "The main aim of robust statistics is the development of methods able to cope with the presence of outliers. A new type of outliers, namely \"cellwise\", has garnered considerable attention. The state of the art for dealing with cellwise contamination in different models is presented in Raymaekers and Rousseeuw (2024). Outliers in time series can be treated as cellwise outliers, a further discussion on this subject is presented."}
{"id": "2601.16769", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16769", "abs": "https://arxiv.org/abs/2601.16769", "authors": ["Ian Carbó Casals"], "title": "From Noisy News Sentiment Scores to Interpretable Temporal Dynamics: A Bayesian State-Space Model", "comment": "12 pages, 5 figures, 2 tables. Code available at: https://github.com/Bailduke/bayesian-sentiment-state-space-model", "summary": "Text-based sentiment indicators are widely used to monitor public and market mood, but weekly sentiment series are noisy by construction. A main reason is that the amount of relevant news changes over time and across categories. As a result, some weekly averages are based on many articles, while others rely on only a few. Existing approaches do not explicitly account for changes in data availability when measuring uncertainty. We present a Bayesian state-space framework that turns aggregated news sentiment into a smoothed time series with uncertainty. The model treats each weekly sentiment value as a noisy measurement of an underlying sentiment process, with observation uncertainty scaled by the effective information weight $n_{tj}$: when coverage is high, latent sentiment is anchored more strongly to the observed aggregate; when coverage is low, inference relies more on the latent dynamics and uncertainty increases. Using news data grouped into multiple categories, we find broadly similar latent dynamics across categories, while larger differences appear in observation noise. The framework is designed for descriptive monitoring and can be extended to other text sources where information availability varies over time."}
{"id": "2601.16749", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2601.16749", "abs": "https://arxiv.org/abs/2601.16749", "authors": ["Pedro Picchetti"], "title": "Finite Population Inference for Factorial Designs and Panel Experiments with Imperfect Compliance", "comment": null, "summary": "This paper develops a finite population framework for analyzing causal effects in settings with imperfect compliance where multiple treatments affect the outcome of interest. Two prominent examples are factorial designs and panel experiments with imperfect compliance. I define finite population causal effects that capture the relative effectiveness of alternative treatment sequences. I provide nonparametric estimators for a rich class of factorial and dynamic causal effects and derive their finite population distributions as the sample size increases. Monte Carlo simulations illustrate the desirable properties of the estimators. Finally, I use the estimator for causal effects in factorial designs to revisit a famous voter mobilization experiment that analyzes the effects of voting encouragement through phone calls on turnout."}
{"id": "2601.16821", "categories": ["stat.ME", "q-fin.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16821", "abs": "https://arxiv.org/abs/2601.16821", "authors": ["Harrison Katz"], "title": "Directional-Shift Dirichlet ARMA Models for Compositional Time Series with Structural Break Intervention", "comment": null, "summary": "Compositional time series, vectors of proportions summing to unity observed over time, frequently exhibit structural breaks due to external shocks, policy changes, or market disruptions. Standard methods either ignore such breaks or handle them through ad-hoc dummy variables that cannot extrapolate beyond the estimation sample. We develop a Bayesian Dirichlet ARMA model augmented with a directional-shift intervention mechanism that captures structural breaks through three interpretable parameters: a unit direction vector specifying which components gain or lose share, an amplitude controlling the magnitude of redistribution, and a logistic gate governing the timing and speed of transition. The model preserves compositional constraints by construction, maintains innovation-form DARMA dynamics for short-run dependence, and produces coherent probabilistic forecasts during and after structural breaks. We establish that the directional shift corresponds to geodesic motion on the simplex and is invariant to the choice of ILR basis. A comprehensive simulation study with 400 fits across 8 scenarios demonstrates that when the shift direction is correctly identified (77.5% of cases), amplitude and timing parameters are recovered with near-zero bias, and credible intervals for the mean composition achieve nominal 80% coverage; we address the sign identification challenge through a hemisphere constraint. An empirical application to fee recognition lead-time distributions during COVID-19 compares baseline, fixed-effects, and intervention specifications in rolling forecast evaluation, demonstrating the intervention model's superior point accuracy (Aitchison distance 0.83 vs. 0.90) and calibration (87% vs. 71% coverage) during structural transitions."}
{"id": "2601.16769", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16769", "abs": "https://arxiv.org/abs/2601.16769", "authors": ["Ian Carbó Casals"], "title": "From Noisy News Sentiment Scores to Interpretable Temporal Dynamics: A Bayesian State-Space Model", "comment": "12 pages, 5 figures, 2 tables. Code available at: https://github.com/Bailduke/bayesian-sentiment-state-space-model", "summary": "Text-based sentiment indicators are widely used to monitor public and market mood, but weekly sentiment series are noisy by construction. A main reason is that the amount of relevant news changes over time and across categories. As a result, some weekly averages are based on many articles, while others rely on only a few. Existing approaches do not explicitly account for changes in data availability when measuring uncertainty. We present a Bayesian state-space framework that turns aggregated news sentiment into a smoothed time series with uncertainty. The model treats each weekly sentiment value as a noisy measurement of an underlying sentiment process, with observation uncertainty scaled by the effective information weight $n_{tj}$: when coverage is high, latent sentiment is anchored more strongly to the observed aggregate; when coverage is low, inference relies more on the latent dynamics and uncertainty increases. Using news data grouped into multiple categories, we find broadly similar latent dynamics across categories, while larger differences appear in observation noise. The framework is designed for descriptive monitoring and can be extended to other text sources where information availability varies over time."}
{"id": "2601.16829", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16829", "abs": "https://arxiv.org/abs/2601.16829", "authors": ["Danna L. Cruz-Reyes", "Renato M. Assunção", "Reinaldo B. Arellano-Valle", "Rosangela H. Loschi"], "title": "Directional Asymmetry in Edge BasedSpatial Models via a Skew Normal Prior", "comment": "The manuscript is clearly written and well structured. It spans approximately 10 pages and includes several figures that effectively illustrate the proposed methodology and results. Overall, the paper makes a solid contribution to the literature on spatial hierarchical modeling and is suitable for publication after minor revisions", "summary": "We introduce a skewed edge based spatial prior, named RENeGe sk that extends the Gaussian RENeGe framework by incorporating directional asymmetry through a skew normal distribution. Skewness is defined on the edge graph and propagated to the node space, aligning asymmetric behavior with transitions across neighboring regions rather than with marginal node effects. The model is formulated within the skew normal framework and employs identifiable hierarchical priors together with low rank parameterizations to ensure scalability. The skew normal's stochastic representation is considered to facilitate the computational implementation. Simulation studies show that RENeGe sk recovers compact, edge-aligned directional structure more accurately than symmetric Gaussian priors, while remaining competitive under irregular spatial patterns. An application to cancer incidence data in Southern Brazil illustrates how the proposed approach yields stable area-level estimates while preserving localized, directionally driven spatial variation."}
{"id": "2601.16784", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16784", "abs": "https://arxiv.org/abs/2601.16784", "authors": ["Joshua Corneck", "Edward A. K. Cohen", "Francesco Sanna Passino"], "title": "Spectral embedding of inhomogeneous Poisson processes on multiplex networks", "comment": null, "summary": "In many real-world networks, data on the edges evolve in continuous time, naturally motivating representations based on point processes. Heterogeneity in edge types further gives rise to multiplex network point processes. In this work, we propose a model for multiplex network data observed in continuous-time. We establish two-to-infinity norm consistency and asymptotic normality for spectral-embedding-based estimation of the model parameters as both network size and time resolution increase. Drawing inspiration from random dot product graph models, each edge intensity is expressed as the inner product of two low-dimensional latent positions: one dynamic and layer-agnostic, the other static and layer-dependent. These latent positions constitute the primary objects of inference, which is conducted via spectral embedding methods. Our theoretical results are established under a histogram estimator of the network intensities and provide justification for applying a doubly unfolded adjacency spectral embedding method for estimation. Simulations and real-data analyses demonstrate the effectiveness of the proposed model and inference procedure."}
{"id": "2601.16821", "categories": ["stat.ME", "q-fin.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16821", "abs": "https://arxiv.org/abs/2601.16821", "authors": ["Harrison Katz"], "title": "Directional-Shift Dirichlet ARMA Models for Compositional Time Series with Structural Break Intervention", "comment": null, "summary": "Compositional time series, vectors of proportions summing to unity observed over time, frequently exhibit structural breaks due to external shocks, policy changes, or market disruptions. Standard methods either ignore such breaks or handle them through ad-hoc dummy variables that cannot extrapolate beyond the estimation sample. We develop a Bayesian Dirichlet ARMA model augmented with a directional-shift intervention mechanism that captures structural breaks through three interpretable parameters: a unit direction vector specifying which components gain or lose share, an amplitude controlling the magnitude of redistribution, and a logistic gate governing the timing and speed of transition. The model preserves compositional constraints by construction, maintains innovation-form DARMA dynamics for short-run dependence, and produces coherent probabilistic forecasts during and after structural breaks. We establish that the directional shift corresponds to geodesic motion on the simplex and is invariant to the choice of ILR basis. A comprehensive simulation study with 400 fits across 8 scenarios demonstrates that when the shift direction is correctly identified (77.5% of cases), amplitude and timing parameters are recovered with near-zero bias, and credible intervals for the mean composition achieve nominal 80% coverage; we address the sign identification challenge through a hemisphere constraint. An empirical application to fee recognition lead-time distributions during COVID-19 compares baseline, fixed-effects, and intervention specifications in rolling forecast evaluation, demonstrating the intervention model's superior point accuracy (Aitchison distance 0.83 vs. 0.90) and calibration (87% vs. 71% coverage) during structural transitions."}
{"id": "2601.16829", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16829", "abs": "https://arxiv.org/abs/2601.16829", "authors": ["Danna L. Cruz-Reyes", "Renato M. Assunção", "Reinaldo B. Arellano-Valle", "Rosangela H. Loschi"], "title": "Directional Asymmetry in Edge BasedSpatial Models via a Skew Normal Prior", "comment": "The manuscript is clearly written and well structured. It spans approximately 10 pages and includes several figures that effectively illustrate the proposed methodology and results. Overall, the paper makes a solid contribution to the literature on spatial hierarchical modeling and is suitable for publication after minor revisions", "summary": "We introduce a skewed edge based spatial prior, named RENeGe sk that extends the Gaussian RENeGe framework by incorporating directional asymmetry through a skew normal distribution. Skewness is defined on the edge graph and propagated to the node space, aligning asymmetric behavior with transitions across neighboring regions rather than with marginal node effects. The model is formulated within the skew normal framework and employs identifiable hierarchical priors together with low rank parameterizations to ensure scalability. The skew normal's stochastic representation is considered to facilitate the computational implementation. Simulation studies show that RENeGe sk recovers compact, edge-aligned directional structure more accurately than symmetric Gaussian priors, while remaining competitive under irregular spatial patterns. An application to cancer incidence data in Southern Brazil illustrates how the proposed approach yields stable area-level estimates while preserving localized, directionally driven spatial variation."}
{"id": "2601.16427", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16427", "abs": "https://arxiv.org/abs/2601.16427", "authors": ["Behzad Aalipur", "Yichen Qin"], "title": "Perfect Clustering for Sparse Directed Stochastic Block Models", "comment": null, "summary": "Exact recovery in stochastic block models (SBMs) is well understood in undirected settings, but remains considerably less developed for directed and sparse networks, particularly when the number of communities diverges. Spectral methods for directed SBMs often lack stability in asymmetric, low-degree regimes, and existing non-spectral approaches focus primarily on undirected or dense settings.\n  We propose a fully non-spectral, two-stage procedure for community detection in sparse directed SBMs with potentially growing numbers of communities. The method first estimates the directed probability matrix using a neighborhood-smoothing scheme tailored to the asymmetric setting, and then applies $K$-means clustering to the estimated rows, thereby avoiding the limitations of eigen- or singular value decompositions in sparse, asymmetric networks. Our main theoretical contribution is a uniform row-wise concentration bound for the smoothed estimator, obtained through new arguments that control asymmetric neighborhoods and separate in- and out-degree effects. These results imply the exact recovery of all community labels with probability tending to one, under mild sparsity and separation conditions that allow both $γ_n \\to 0$ and $K_n \\to \\infty$.\n  Simulation studies, including highly directed, sparse, and non-symmetric block structures, demonstrate that the proposed procedure performs reliably in regimes where directed spectral and score-based methods deteriorate. To the best of our knowledge, this provides the first exact recovery guarantee for this class of non-spectral, neighborhood-smoothing methods in the sparse, directed setting."}
