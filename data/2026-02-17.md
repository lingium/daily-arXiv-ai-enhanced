<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 11]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.ML](#stat.ML) [Total: 7]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Scalable Changepoint Detection for Large Spatiotemporal Data on the Sphere](https://arxiv.org/abs/2602.12435)
*Samantha Shi-Jun,Bo Li*

Main category: stat.ME

TL;DR: 提出用于球面时空数据的贝叶斯变点检测框架，结合多项概率模型和潜在高斯过程处理空间相关性，利用SPDE和球谐变换实现高效计算，应用于全球气溶胶光学厚度数据。


<details>
  <summary>Details</summary>
Motivation: 环境与气候科学中需要在大尺度球面时空数据中检测变点，传统方法难以处理高维度和复杂的空间相关性结构。

Method: 使用多项概率模型（MPM）将变点建模为空间相关的分类变量，通过潜在高斯过程捕捉球面上的复杂空间相关性；采用随机偏微分方程（SPDE）和球谐变换进行高效表示和可扩展推理，显著降低计算负担。

Result: 通过广泛的模拟研究证明了该方法在变点估计方面的效率和鲁棒性，以及通过MPM和潜在过程的截断谱表示相结合实现的显著计算增益；应用于全球气溶胶光学厚度数据，成功识别了与重大大气事件相关的变点。

Conclusion: 提出的贝叶斯框架为大规模球面时空数据中的变点检测提供了一种高效、可扩展的解决方案，在环境与气候科学中具有广泛的应用前景。

Abstract: We propose a novel Bayesian framework for changepoint detection in large-scale spherical spatiotemporal data, with broad applicability in environmental and climate sciences. Our approach models changepoints as spatially dependent categorical variables using a multinomial probit model (MPM) with a latent Gaussian process, effectively capturing complex spatial correlation structures on the sphere. To handle the high dimensionality inherent in global datasets, we leverage stochastic partial differential equations (SPDE) and spherical harmonic transformations for efficient representation and scalable inference, drastically reducing computational burden while maintaining high accuracy. Through extensive simulation studies, we demonstrate the efficiency and robustness of the proposed method for changepoint estimation, as well as the significant computational gains achieved through the combined use of the MPM and truncated spectral representations of latent processes. Finally, we apply our method to global aerosol optical depth data, successfully identifying changepoints associated with a major atmospheric event.

</details>


### [2] [Conjugate Variational Inference for Large Mixed Multinomial Logit Models and Consumer Choice](https://arxiv.org/abs/2602.12577)
*Weiben Zhang,Ruben Loaiza-Maya,Michael Stanley Smith,Worapree Maneesoonthorn*

Main category: stat.ME

TL;DR: 提出一种新的变分推断方法，用于高效估计混合logit模型，解决了大规模数据集下随机系数模型估计的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 混合logit模型（随机系数logit模型）能有效处理多项选择数据中的异质性，但在大规模数据集上难以估计。现有贝叶斯变分推断方法存在效率瓶颈。

Method: 提出新的变分推断方法，核心创新是高效更新随机系数条件后验的高斯近似，解决了变分优化中的瓶颈。应用于三种混合logit模型：标准型、嵌套型和捆绑型。

Result: 模拟实验显示新方法优于现有变分推断方法。应用于大型意大利面选择扫描面板数据，发现消费者对价格和促销的反应在商店和产品层面存在显著异质性。商店规模、溢价和地理位置是价格弹性的驱动因素。扩展到意大利面酱捆绑选择进一步提高了模型准确性。

Conclusion: 混合模型预测比固定系数模型更准确，新变分推断方法在传统方法难以处理的情况下提供了有价值的洞见，能够有效处理大规模混合logit模型估计问题。

Abstract: Heterogeneity in multinomial choice data is often accounted for using logit models with random coefficients. Such models are called "mixed", but they can be difficult to estimate for large datasets. We review current Bayesian variational inference (VI) methods that can do so, and propose a new VI method that scales more effectively. The key innovation is a step that updates efficiently a Gaussian approximation to the conditional posterior of the random coefficients, addressing a bottleneck within the variational optimization. The approach is used to estimate three types of mixed logit models: standard, nested and bundle variants. We first demonstrate the improvement of our new approach over existing VI methods using simulations. Our method is then applied to a large scanner panel dataset of pasta choice. We find consumer response to price and promotion variables exhibits substantial heterogeneity at the grocery store and product levels. Store size, premium and geography are found to be drivers of store level estimates of price elasticities. Extension to bundle choice with pasta sauce improves model accuracy further. Predictions from the mixed models are more accurate than those from fixed coefficients equivalents, and our VI method provides insights in circumstances which other methods find challenging.

</details>


### [3] [Some bivariate distributions on a discrete torus with application to wind direction datasets](https://arxiv.org/abs/2602.12842)
*Brajesh Kumar Dhakad,Jayant Jha,Debepsita Mukherjee*

Main category: stat.ME

TL;DR: 提出了两种用于分析二元离散圆形数据的分布：二元包裹几何分布和二元广义包裹几何分布，用于处理风方向等离散圆形数据


<details>
  <summary>Details</summary>
Motivation: 许多数据集在有限等间距方向上观测，而不是精确角度（如风方向数据）。现有统计文献中只有连续圆形随机变量的二元模型，缺乏离散圆形数据的二元分布模型

Method: 使用包裹几何分布和三角函数构造模型，得到精确闭式表达的三角矩，使模型具有解析可处理性。基于最大似然估计方法进行参数估计

Result: 提出了BWG和BGWG两种分布，详细讨论了分布特性、参数解释和依赖结构。通过模拟数据集验证了估计方法，并成功应用于印度不同站点的风方向测量数据分析

Conclusion: 该研究填补了离散圆形数据二元分布模型的空白，提出的模型具有解析可处理性，在实际风方向数据分析中表现出良好的应用价值

Abstract: Many datasets are observed on a finite set of equally spaced directions instead of the exact angles, such as the wind direction data. However, in the statistical literature, bivariate models are only available for continuous circular random variables. This article presents two bivariate circular distributions, namely bivariate wrapped geometric (BWG) and bivariate generalized wrapped geometric (BGWG), for analyzing bivariate discrete circular data. We consider wrapped geometric distributions and a trigonometric function to construct the models. The models are analytically tractable due to the exact closed-form expressions for the trigonometric moments. We thoroughly discuss the distributional properties of the models, including the interpretation of parameters and dependence structure. The estimation methodology based on maximizing the likelihood functions is illustrated for simulated datasets. Finally, the proposed distributions are utilized to analyze pairwise wind direction measurements obtained at different stations in India, and the interpretations for the fitted models are briefly discussed.

</details>


### [4] [A Causal Framework for Quantile Residual Lifetime](https://arxiv.org/abs/2602.12682)
*Taekwon Hong,Woojung Bae,Sang Kyu Lee,Dongrak Choi,Jong-Hyeon Jeong*

Main category: stat.ME

TL;DR: 提出一个因果框架，用于估计在界标时间t0后存活的个体剩余寿命的分位数，包括OSQC（观察幸存者分位数对比）和PSQC（主要幸存者分位数对比）两种估计量，以解决临床预后评估中的解释性问题。


<details>
  <summary>Details</summary>
Motivation: 临床研究中，在初始高风险期后存活的个体预后估计至关重要。标准指标如风险比难以解释，而基于均值的总结对异常值和删失敏感。需要开发更稳健、可解释的预后评估方法。

Method: 提出因果框架估计界标时间t0后存活个体的剩余寿命分位数。开发双重稳健估计器，结合倾向得分、结果回归和逆概率删失加权。引入OSQC估计观察幸存者群体的预后差异，以及PSQC在更强假设下分离因果效应和组成选择机制。

Result: 模拟研究表明所提估计器具有稳健性，阐明了治疗后选择的作用。在SUPPORT研究中评估右心导管插入对ICU幸存者剩余寿命的影响，在NSABP B-14试验中检查辅助他莫昔芬治疗在多个界标时间下的术后预后。

Conclusion: 提出的因果框架为临床预后评估提供了更可解释的分位数方法，能够处理混杂和信息性删失，区分因果效应和选择机制，适用于实际临床研究中的预后评估。

Abstract: Estimating prognosis conditional on surviving an initial high-risk period is crucial in clinical research. Yet, standard metrics such as hazard ratios are often difficult to interpret, while mean-based summaries are sensitive to outliers and censoring. We propose a formal causal framework for estimating quantiles of residual lifetime among individuals surviving to a landmark time $t_0$. Our primary estimand, the "Observed Survivor Quantile Contrast" (OSQC), targets pragmatic prognostic differences within the observed survivor population. To estimate the OSQC, we develop a doubly robust estimator that combines propensity scores, outcome regression, and inverse probability of censoring weights, ensuring consistency under confounding and informative censoring provided that the censoring model is correctly specified and at least one additional nuisance model is correctly specified. Recognizing that the OSQC conflates causal efficacy and compositional selection, we also introduce a reweighting-based supplementary estimator for the "Principal Survivor Quantile Contrast" (PSQC) to disentangle these mechanisms under stronger assumptions. Extensive simulations demonstrate the robustness of the proposed estimators and clarify the role of post-treatment selection. We illustrate the framework using data from the SUPPORT study to assess the impact of right heart catheterization on residual lifetime among intensive care unit survivors, and from the NSABP B-14 trial to examine post-surgical prognosis under adjuvant tamoxifen therapy across multiple landmark times.

</details>


### [5] [A unified testing approach for log-symmetry using Fourier methods](https://arxiv.org/abs/2602.12900)
*Ganesh Vishnu Avhad,Sudheesh K. Kattumannil*

Main category: stat.ME

TL;DR: 提出基于特征函数和L²型加权距离的log-symmetric分布拟合优度检验新方法，具有更好的经验功效和计算效率


<details>
  <summary>Details</summary>
Motivation: 连续正偏态数据常出现在应用领域，log-symmetric分布提供了灵活的建模框架，但需要有效的拟合优度检验方法

Method: 基于最新特征化理论，利用特征函数作为新工具，构建L²型加权距离度量的拟合优度检验

Result: 蒙特卡洛模拟显示新方法在多种备择分布下具有更优的经验功效，计算效率显著高于现有方法

Conclusion: 提出的检验方法为log-symmetric分布提供了有效且计算高效的拟合优度检验工具，实际数据集验证了其应用价值

Abstract: Continuous and strictly positive data that exhibit skewness and outliers frequently arise in many applied disciplines. Log-symmetric distributions provide a flexible framework for modeling such data. In this article, we develop new goodness-of-fit tests for log-symmetric distributions based on a recent characterization. These tests utilize the characteristic function as a novel tool and are constructed using an $L^2$-type weighted distance measure. The asymptotic properties of the resulting test statistic are studied. The finite-sample performance of the proposed method is assessed via Monte Carlo simulations and compared with existing procedures. The results under a range of alternative distributions indicate superior empirical power, while the proposed test also exhibits substantial computational efficiency compared to existing methods. The methodology is further illustrated using real data sets to demonstrate practical applicability.

</details>


### [6] [Modelling multivariate ordinal time series using pairwise likelihood](https://arxiv.org/abs/2602.12702)
*Anna Nalpantidi,Dimitris Karlis*

Main category: stat.ME

TL;DR: 提出使用copula建模多变量序数时间序列的联合分布，通过复合似然和条件成对似然解决高维计算问题，采用加权平均合成参数估计


<details>
  <summary>Details</summary>
Motivation: 多变量序数时间序列的联合分布建模面临挑战：需要同时考虑自相关、序列间依赖和交叉相关，传统方法难以处理序数变量和高维情况

Method: 1. 使用copula指定二元序数时间序列的联合分布；2. 扩展到高维时采用复合似然近似，特别是条件成对似然；3. 独立最大化每个二元模型避免计算问题；4. 使用基于Hessian矩阵的加权平均合成估计

Result: 模拟研究表明该方法在不同样本量下拟合良好，讨论了预测方法，并通过欧盟国家失业状态的实例展示了方法的实用性

Conclusion: 提出的copula-based复合似然方法能有效建模多变量序数时间序列的联合分布，解决了高维计算问题，在模拟和实际数据中表现良好

Abstract: We assume that we have multiple ordinal time series and we would like to specify their joint distribution. In general it is difficult to create multivariate distribution that can be easily used to jointly model ordinal variables and the problem becomes even more complex in the case of time series, since we have to take into consideration not only the autocorrelation of each time series and the dependence between time series, but also cross-correlation. Starting from the simplest case of two ordinal time series, we propose using copulas to specify their joint distribution. We extend our approach in higher dimensions, by approximating full likelihood with composite likelihood and especially conditional pairwise likelihood, where each bivariate model is specified by copulas. We suggest maximizing each bivariate model independently to avoid computational issues and synthesize individual estimates using weighted mean. Weights are related to the Hessian matrix of each bivariate model. Simulation studies showed that model fits well under different sample sizes. Forecasting approach is also discussed. A small real data application about unemployment state of different countries of European Union is presented to illustrate our approach.

</details>


### [7] [Stratified Sampling for Model-Assisted Estimation with Surrogate Outcomes](https://arxiv.org/abs/2602.12992)
*Reagan Mozer,Nicole E. Pashley,Luke Miratrix*

Main category: stat.ME

TL;DR: 论文提出了一种分层抽样策略，用于结合机器学习生成的代理结果与人工编码子集，通过优化人工编码资源分配来提高估计效率。


<details>
  <summary>Details</summary>
Motivation: 在随机试验中，对文本结果（如论文、开放式回答）进行人工评分成本高昂且耗时。虽然模型辅助估计可以结合机器学习生成的代理结果与人工编码子集，但传统方法使用简单随机抽样，忽略了代理预测误差的系统性变异。

Method: 扩展模型辅助估计框架，引入分层抽样策略。推导分层模型辅助估计量的精确方差，确定分层提高精度的条件，并提出Neyman型最优分配规则，对残差方差较大的层进行过抽样。

Result: 当代理预测误差存在结构化偏差或异方差性时，分层抽样能持续提高估计效率。通过模拟研究和两个实证应用（教育RCT数据和大型观察语料库）验证了方法的有效性。

Conclusion: 该框架提供了一种实用的基于设计的方法，能够利用代理结果并战略性地分配人工编码资源，以获得无偏且更高效的估计。虽然受文本数据应用启发，但方法适用于任何结果测量成本高昂的场景。

Abstract: In many randomized trials, outcomes such as essays or open-ended responses must be manually scored as a preliminary step to impact analysis, a process that is costly and limiting. Model-assisted estimation offers a way to combine surrogate outcomes generated by machine learning or large language models with a human-coded subset, yet typical implementations use simple random sampling and therefore overlook systematic variation in surrogate prediction error. We extend this framework by incorporating stratified sampling to more efficiently allocate human coding effort. We derive the exact variance of the stratified model-assisted estimator, characterize conditions under which stratification improves precision, and identify a Neyman-type optimal allocation rule that oversamples strata with larger residual variance. We evaluate our methods through a comprehensive simulation study to assess finite-sample performance. Overall, we find stratification consistently improves efficiency when surrogate prediction errors exhibit structured bias or heteroskedasticity. We also present two empirical applications, one using data from an education RCT and one using a large observational corpus, to illustrate how these methods can be implemented in practice using ChatGPT-generated surrogate outcomes. Overall, this framework provides a practical design-based approach for leveraging surrogate outcomes and strategically allocating human coding effort to obtain unbiased estimates with greater efficiency. While motivated by text-as-data applications, the methodology applies broadly to any setting where outcome measurement is costly or prohibitive, and can be applied to comparisons across groups or estimating the mean of a single group.

</details>


### [8] [Small area estimation using incomplete auxiliary information](https://arxiv.org/abs/2602.12845)
*Donatas Šlevinskas,Ieva Burakauskaitė,Andrius Čiginas*

Main category: stat.ME

TL;DR: 提出兩階段小區域估計方法，結合機率調查與非機率外部資料，透過模型校準與Fay-Herriot模型提升區域估計精度


<details>
  <summary>Details</summary>
Motivation: 行政資料等非機率來源的輔助資訊日益增多，但常不完整且非機率性質。需要方法整合機率調查與非機率資料，提升小區域估計精度

Method: 兩階段方法：第一步基於設計的模型校準，利用非機率資料的代理變量，透過測量誤差工作模型預測，並納入區域特定校準限制；第二步使用Fay-Herriot區域層級模型，結合精確已知共變量

Result: 在三個官方統計企業調查案例中驗證：整合機率樣本與(1)行政記錄、(2)截斷資料源、(3)網路爬蟲資訊。結果顯示區域層級精度相較直接估計和直接使用代理變量的Fay-Herriot基準有顯著提升

Conclusion: 提出的兩階段方法能有效整合機率與非機率資料，提升小區域估計精度，且無需對非機率樣本的選擇機制建模，具有實用價值

Abstract: Auxiliary information is increasingly available from administrative and other data sources, but it is often incomplete and of non-probability origin. We propose a two-step small area estimation approach in which the first step relies on design-based model calibration and exploits a large non-probability source providing a noisy proxy of the study variable for only part of the population. A unit-level measurement-error working model is fitted on the linked overlap between the probability survey and the external source, and its predictions are incorporated through domain-specific model-calibration constraints to obtain approximately design-unbiased domain totals. These totals and their variance estimates are then used in a Fay-Herriot area-level model with exactly known covariates to produce empirical best linear unbiased predictors. The approach is demonstrated in three enterprise survey settings from official statistics by integrating probability sample data with (i) administrative records, (ii) a cut-off data source, and (iii) web-scraped online information. Empirical comparisons show consistent improvements in domain-level precision over direct estimation and over a Fay-Herriot benchmark that directly incorporates the proxy information as an error-prone covariate. These gains are achieved without modeling the selection mechanism of the non-probability sample.

</details>


### [9] [Detecting Parameter Instabilities in Functional Concurrent Linear Regression](https://arxiv.org/abs/2602.13152)
*Rupsa Basu,Sven Otto*

Main category: stat.ME

TL;DR: 提出检测函数时间序列中并发函数线性回归模型斜率函数结构断点的方法，基于回归量加权OLS残差函数的CUSUM过程，包含L²范数和上确界范数版本，应用于运动员疲劳跑步时的关节角度轨迹分析。


<details>
  <summary>Details</summary>
Motivation: 需要检测函数时间序列中回归关系的结构变化，特别是在生物力学应用中，如运动员疲劳跑步时关节角度轨迹的变化，这些变化可能表现为回归关系中的断点。

Method: 基于回归量加权OLS残差函数的CUSUM过程，提出L²范数和上确界范数两种版本，其中上确界范数对尖峰状变化特别敏感。在Hölder正则性和弱依赖性条件下建立函数强不变原理。

Result: 建立了渐近零分布，证明检验对斜率函数断点的广泛备择类具有一致性。模拟研究展示了有限样本的尺寸和功效。应用于运动员疲劳跑步时的髋关节和膝关节角度轨迹数据。

Conclusion: 提出的方法能够检测函数时间序列中回归关系的结构变化，为生物力学功能时间序列提供可解释的推断支持，特别是在分析运动员疲劳引起的运动模式调整方面。

Abstract: We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under Hölder regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series.

</details>


### [10] [Barron-Wiener-Laguerre models](https://arxiv.org/abs/2602.13098)
*Rahul Manavalan,Filip Tronarp*

Main category: stat.ME

TL;DR: 提出Wiener-Laguerre模型的概率扩展，结合因果动态参数化和Barron型非线性逼近，为因果算子学习提供结构化且具备不确定性量化的框架。


<details>
  <summary>Details</summary>
Motivation: 经典Wiener-Laguerre模型虽然结构高效且可解释，但只能提供确定性点估计，缺乏对非线性映射的不确定性量化能力。需要将系统辨识与现代函数逼近方法结合，为时间序列建模和非线性系统辨识提供更完备的框架。

Method: 1) 使用Laguerre基参数化稳定线性动态系统；2) 从Barron函数逼近视角重新解释非线性组件，将两层网络、随机傅里叶特征和极限学习机视为参数测度上积分表示的离散化；3) 对非线性映射进行贝叶斯推断，获得后验预测不确定性；4) 结合因果动态参数化和概率Barron型非线性逼近器。

Result: 获得了一个结构化且表达能力强的因果算子类别，具备不确定性量化能力。该框架连接了经典系统辨识和现代测度基函数逼近，为时间序列建模和非线性系统辨识提供了原则性方法。

Conclusion: 提出的概率Wiener-Laguerre模型框架成功地将经典系统辨识的结构化优势与现代函数逼近的灵活性和不确定性量化能力相结合，为因果算子学习提供了既具解释性又具备概率预测能力的统一方法。

Abstract: We propose a probabilistic extension of Wiener-Laguerre models for causal operator learning. Classical Wiener-Laguerre models parameterize stable linear dynamics using orthonormal Laguerre bases and apply a static nonlinear map to the resulting features. While structurally efficient and interpretable, they provide only deterministic point estimates. We reinterpret the nonlinear component through the lens of Barron function approximation, viewing two-layer networks, random Fourier features, and extreme learning machines as discretizations of integral representations over parameter measures. This perspective naturally admits Bayesian inference on the nonlinear map and yields posterior predictive uncertainty. By combining Laguerre-parameterized causal dynamics with probabilistic Barron-type nonlinear approximators, we obtain a structured yet expressive class of causal operators equipped with uncertainty quantification. The resulting framework bridges classical system identification and modern measure-based function approximation, providing a principled approach to time-series modeling and nonlinear systems identification.

</details>


### [11] [A new mixture model for spatiotemporal exceedances with flexible tail dependence](https://arxiv.org/abs/2602.13158)
*Ryan Li,Brian J. Reich,Emily C. Hector,Reetam Majumder*

Main category: stat.ME

TL;DR: 提出一种新的时空流量超越阈值模型，灵活捕捉分布尾部的渐近依赖和独立性，使用基于模拟的推理进行参数估计。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够灵活处理极端事件（如洪水）时空依赖结构的模型，传统方法在捕捉渐近依赖和独立性方面存在局限。

Method: 采用混合过程模型，包含空间、时间和时空渐近依赖机制；使用截断机制仅利用阈值以上观测；通过基于模拟的推理（随机森林）从数据摘要统计中估计参数。

Result: 模拟实验和美国地质调查局流量数据建模表明该方法可行且实用，能够有效估计极端事件的边际和联合概率。

Conclusion: 提出的框架为时空极端事件建模提供了灵活有效的解决方案，特别适用于水文极端事件分析。

Abstract: We propose a new model and estimation framework for spatiotemporal streamflow exceedances above a threshold that flexibly captures asymptotic dependence and independence in the tail of the distribution. We model streamflow using a mixture of processes with spatial, temporal and spatiotemporal asymptotic dependence regimes. A censoring mechanism allows us to use only observations above a threshold to estimate marginal and joint probabilities of extreme events. As the likelihood is intractable, we use simulation-based inference powered by random forests to estimate model parameters from summary statistics of the data. Simulations and modeling of streamflow data from the U.S. Geological Survey illustrate the feasibility and practicality of our approach.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [12] [Nationwide Hourly Population Estimating at the Neighborhood Scale in the United States Using Stable-Attendance Anchor Calibration](https://arxiv.org/abs/2602.12291)
*Huan Ning,Zhenlong Li,Manzhu Yu,Xiao Huang,Shiyan Zhang,Shan Qiao*

Main category: stat.AP

TL;DR: SAAC框架利用智能手机移动数据，通过稳定出席锚点校准，重建美国人口普查区组级别的每小时人口分布，解决数字轨迹数据的观测偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统人口数据集是静态的，无法捕捉日常移动带来的人口时空动态。智能手机移动数据虽然覆盖广泛，但由于不完全感知、设备渗透率空间异质性和观测过程不稳定，将其转化为准确人口估计仍具挑战。

Method: 提出稳定出席锚点校准框架，将人口估计构建为基于平衡的人口核算问题，结合居住人口和从设备事件观测推断的时变进出移动。利用高度规律出席地点（如高中）作为校准锚点，估计观测缩放因子以修正未记录的移动事件。

Result: 推断的人口模式与先前移动性和城市人口研究的经验发现一致。SAAC能够将大规模有偏数字轨迹数据转化为可解释的动态人口产品。

Conclusion: SAAC提供了一个通用框架，用于将大规模有偏数字轨迹数据转化为可解释的动态人口产品，对城市科学、公共卫生和人类移动研究具有重要意义。

Abstract: Traditional population datasets are largely static and therefore unable to capture the strong temporal dynamics of human presence driven by daily mobility. Recent smartphone-based mobility data offer unprecedented spatiotemporal coverage, yet translating these opportunistic observations into accurate population estimates remains challenging due to incomplete sensing, spatially heterogeneous device penetration, and unstable observation processes. We propose a Stable-Attendance Anchor Calibration (SAAC) framework to reconstruct hourly population presence at the Census block group level across the United States. SAAC formulates population estimation as a balance-based population accounting problem, combining residential population with time-varying inbound and outbound mobility inferred from device-event observations. To address observation bias and identifiability limitations, the framework leverages locations with highly regular attendance as calibration anchors, using high schools in this study. These anchors enable estimation of observation scaling factors that correct for under-recorded mobility events. By integrating anchor-based calibration with an explicit sampling model, SAAC enables consistent conversion from observed device events to population presence at fine temporal resolution. The inferred population patterns are consistent with established empirical findings in prior mobility and urban population studies. SAAC provides a generalizable framework for transforming large-scale, biased digital trace data into interpretable dynamic population products, with implications for urban science, public health, and human mobility research. The hourly population estimates can be accessed at: https://gladcolor.github.io/hourly_population.

</details>


### [13] [Statistical Opportunities in Neuroimaging](https://arxiv.org/abs/2602.12974)
*Jian Kang,Thomas Nichols,Lexin Li,Martin A. Lindquist,Hongtu Zhu*

Main category: stat.AP

TL;DR: 本文综述了神经影像学中的统计机遇与挑战，涵盖大脑发育、成人/衰老大脑、神经退行性疾病以及大脑编码解码四个关键领域，强调统计学家与神经科学家、临床医生的紧密合作对推动神经影像学发展的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管神经影像技术（如MRI、fMRI、EEG、PET）在理解大脑结构、功能和连接方面取得重大进展，但大脑作为复杂的多尺度系统，其高维度的神经影像测量带来了统计挑战，包括测量噪声、运动伪影、个体间差异以及大规模研究的数据处理问题。

Method: 本文采用综述方法，首先简要介绍主要成像技术，然后系统回顾四个关键领域的尖端研究：1）出生至20岁的大脑发育；2）成人和衰老大脑；3）神经退行性和神经精神疾病；4）大脑编码和解码。文章重点分析数据建模挑战，并为统计学家指明研究机遇。

Result: 文章系统梳理了神经影像学在各生命阶段和应用领域的研究现状，识别了当前面临的主要统计挑战，包括高维数据处理、异质性建模、纵向数据分析等，并提出了统计学家可以贡献的具体研究方向。

Conclusion: 统计学家、神经科学家和临床医生之间的紧密合作对于将神经影像学进展转化为改进的诊断方法、更深入的机制理解和更个性化的治疗方案至关重要。跨学科合作是解决神经影像学复杂统计挑战的关键。

Abstract: Neuroimaging has profoundly enhanced our understanding of the human brain by characterizing its structure, function, and connectivity through modalities like MRI, fMRI, EEG, and PET. These technologies have enabled major breakthroughs across the lifespan, from early brain development to neurodegenerative and neuropsychiatric disorders. Despite these advances, the brain is a complex, multiscale system, and neuroimaging measurements are correspondingly high-dimensional. This creates major statistical challenges, including measurement noise, motion-related artifacts, substantial inter-subject and site/scanner variability, and the sheer scale of modern studies. This paper explores statistical opportunities and challenges in neuroimaging across four key areas: (i) brain development from birth to age 20, (ii) the adult and aging brain, (iii) neurodegeneration and neuropsychiatric disorders, and (iv) brain encoding and decoding. After a quick tutorial on major imaging technologies, we review cutting-edge studies, underscore data and modeling challenges, and highlight research opportunities for statisticians. We conclude by emphasizing that close collaboration among statisticians, neuroscientists, and clinicians is essential for translating neuroimaging advances into improved diagnostics, deeper mechanistic insight, and more personalized treatments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [14] [Linear Regression with Unknown Truncation Beyond Gaussian Features](https://arxiv.org/abs/2602.12534)
*Alexandros Kouridakis,Anay Mehrotra,Alkis Kalavasis,Constantine Caramanis*

Main category: stat.ML

TL;DR: 提出了首个在未知生存集情况下，能在多项式时间内求解截断线性回归的算法，仅需特征向量满足亚高斯分布


<details>
  <summary>Details</summary>
Motivation: 传统截断线性回归研究大多假设生存集S*已知，但实际应用中S*通常是未知的。现有处理未知生存集的方法要么需要强分布假设（如高斯性），要么计算复杂度极高（d^poly(1/ε)），限制了实际应用

Method: 开发了基于亚高斯特征向量的多项式时间算法，核心创新是一个高效学习有界数量区间并集的新子程序，该子程序仅使用正例（无负例）并在一定平滑条件下工作

Result: 首次实现了在未知生存集情况下，仅需亚高斯分布假设，就能以poly(d/ε)时间求解截断线性回归的算法，突破了传统方法的计算瓶颈

Conclusion: 该工作解决了截断线性回归中未知生存集这一长期开放问题，提出的算法具有多项式时间复杂度和较弱的分布假设，同时开发的正例学习子程序对正例学习领域也有独立贡献

Abstract: In truncated linear regression, samples $(x,y)$ are shown only when the outcome $y$ falls inside a certain survival set $S^\star$ and the goal is to estimate the unknown $d$-dimensional regressor $w^\star$. This problem has a long history of study in Statistics and Machine Learning going back to the works of (Galton, 1897; Tobin, 1958) and more recently in, e.g., (Daskalakis et al., 2019; 2021; Lee et al., 2023; 2024). Despite this long history, however, most prior works are limited to the special case where $S^\star$ is precisely known. The more practically relevant case, where $S^\star$ is unknown and must be learned from data, remains open: indeed, here the only available algorithms require strong assumptions on the distribution of the feature vectors (e.g., Gaussianity) and, even then, have a $d^{\mathrm{poly} (1/\varepsilon)}$ run time for achieving $\varepsilon$ accuracy.
  In this work, we give the first algorithm for truncated linear regression with unknown survival set that runs in $\mathrm{poly} (d/\varepsilon)$ time, by only requiring that the feature vectors are sub-Gaussian. Our algorithm relies on a novel subroutine for efficiently learning unions of a bounded number of intervals using access to positive examples (without any negative examples) under a certain smoothness condition. This learning guarantee adds to the line of works on positive-only PAC learning and may be of independent interest.

</details>


### [15] [A Regularization-Sharpness Tradeoff for Linear Interpolators](https://arxiv.org/abs/2602.12680)
*Qingyi Hu,Liam Hodgkinson*

Main category: stat.ML

TL;DR: 论文提出了一种针对过参数化线性回归的正则化-锐度权衡框架，用于替代传统的偏差-方差权衡，特别适用于ℓ^p惩罚项（包括LASSO）的最小范数插值估计器。


<details>
  <summary>Details</summary>
Motivation: 经典机器学习中的偏差-方差权衡在过参数化设置中失效（如双下降曲线所示），最小范数插值估计器表现良好，需要新的权衡框架来理解这些设置下的模型选择。

Method: 基于插值信息准则，将选择惩罚分解为：1）正则化项（量化正则器与插值器的对齐程度），2）几何锐度项（量化插值流形上局部扰动的影响）。首先为p≥2的ℓ^p正则器建立通用表达式，然后扩展到ℓ^1正则器（LASSO插值器）。

Result: 理论框架成功建立了正则化-锐度权衡，实证结果在随机傅里叶特征和多项式特征的真实数据集上验证了该理论，表明权衡项能够区分性能良好的线性插值器和较弱的插值器。

Conclusion: 提出的正则化-锐度权衡为过参数化线性回归提供了新的理论框架，成功替代了传统偏差-方差权衡，特别适用于ℓ^p惩罚项，包括诱导稀疏性的LASSO插值器。

Abstract: The rule of thumb regarding the relationship between the bias-variance tradeoff and model size plays a key role in classical machine learning, but is now well-known to break down in the overparameterized setting as per the double descent curve. In particular, minimum-norm interpolating estimators can perform well, suggesting the need for new tradeoff in these settings. Accordingly, we propose a regularization-sharpness tradeoff for overparameterized linear regression with an $\ell^p$ penalty. Inspired by the interpolating information criterion, our framework decomposes the selection penalty into a regularization term (quantifying the alignment of the regularizer and the interpolator) and a geometric sharpness term on the interpolating manifold (quantifying the effect of local perturbations), yielding a tradeoff analogous to bias-variance. Building on prior analyses that established this information criterion for ridge regularizers, this work first provides a general expression of the interpolating information criterion for $\ell^p$ regularizers where $p \ge 2$. Subsequently, we extend this to the LASSO interpolator with $\ell^1$ regularizer, which induces stronger sparsity. Empirical results on real-world datasets with random Fourier features and polynomials validate our theory, demonstrating how the tradeoff terms can distinguish performant linear interpolators from weaker ones.

</details>


### [16] [Blessings of Multiple Good Arms in Multi-Objective Linear Bandits](https://arxiv.org/abs/2602.12901)
*Heesang Ann,Min-hwan Oh*

Main category: stat.ML

TL;DR: 多目标赌博机问题中，当存在多个对多个目标都表现良好的臂时，会产生隐式探索效应，使得贪婪算法也能取得强性能表现


<details>
  <summary>Details</summary>
Motivation: 传统观点认为多目标赌博机问题比单目标更复杂，需要同时优化多个目标。本文挑战这一观点，发现在存在多个对多个目标都表现良好的臂时，会产生隐式探索效应

Method: 提出在多目标和参数化赌博机设置中引入隐式探索的概念，无需对上下文做分布假设。引入有效的帕累托公平性框架，用于严格分析多目标赌博机算法的公平性

Result: 证明在存在多个良好臂的条件下，简单贪婪算法在大多数轮次中也能取得强大的理论性能和实证表现。这是首个在多目标和参数化赌博机设置中引入隐式探索的研究

Conclusion: 多目标赌博机问题在某些条件下可能比单目标问题更简单，因为多个良好臂会诱导隐式探索效应。提出的帕累托公平性框架为分析多目标算法的公平性提供了原则性方法

Abstract: The multi objective bandit setting has traditionally been regarded as more complex than the single objective case, as multiple objectives must be optimized simultaneously. In contrast to this prevailing view, we demonstrate that when multiple good arms exist for multiple objectives, they can induce a surprising benefit, implicit exploration. Under this condition, we show that simple algorithms that greedily select actions in most rounds can nonetheless achieve strong performance, both theoretically and empirically. To our knowledge, this is the first study to introduce implicit exploration in both multi objective and parametric bandit settings without any distributional assumptions on the contexts. We further introduce a framework for effective Pareto fairness, which provides a principled approach to rigorously analyzing fairness of multi objective bandit algorithms.

</details>


### [17] [Annealing in variational inference mitigates mode collapse: A theoretical study on Gaussian mixtures](https://arxiv.org/abs/2602.12923)
*Luigi Fogliani,Bruno Loureiro,Marylou Gabrié*

Main category: stat.ML

TL;DR: 本文分析了退火策略在缓解变分推断中模式坍塌问题的数学原理，通过高斯混合模型的理论分析和RealNVP流模型的数值验证，提供了退火方案设计的指导。


<details>
  <summary>Details</summary>
Motivation: 模式坍塌（无法捕捉多模态分布中的某些模式）是现代变分推断中的核心挑战。本文旨在通过数学分析退火策略来缓解这一问题。

Method: 1. 在高斯混合模型这一可处理场景中分析模式坍塌问题；2. 利用低维统计量描述精确表征初始温度与退火速率的相互作用；3. 推导模式坍塌概率的精确公式；4. 通过RealNVP归一化流模型进行数值验证。

Result: 1. 精确刻画了初始温度与退火速率的相互作用关系；2. 推导出模式坍塌概率的精确公式；3. 证明适当选择的退火方案可以稳健地防止模式坍塌；4. 数值实验表明理论结论在神经网络模型中具有定性扩展性。

Conclusion: 退火策略能有效缓解变分推断中的模式坍塌问题，理论分析为实际变分推断流程中设计退火策略提供了指导。研究结果在高斯混合模型中严格证明，并在神经网络模型中得到了验证。

Abstract: Mode collapse, the failure to capture one or more modes when targetting a multimodal distribution, is a central challenge in modern variational inference. In this work, we provide a mathematical analysis of annealing based strategies for mitigating mode collapse in a tractable setting: learning a Gaussian mixture, where mode collapse is known to arise. Leveraging a low dimensional summary statistics description, we precisely characterize the interplay between the initial temperature and the annealing rate, and derive a sharp formula for the probability of mode collapse. Our analysis shows that an appropriately chosen annealing scheme can robustly prevent mode collapse. Finally, we present numerical evidence that these theoretical tradeoffs qualitatively extend to neural network based models, RealNVP normalizing flows, providing guidance for designing annealing strategies mitigating mode collapse in practical variational inference pipelines.

</details>


### [18] [Random Forests as Statistical Procedures: Design, Variance, and Dependence](https://arxiv.org/abs/2602.13104)
*Nathaniel S. O'Connell*

Main category: stat.ML

TL;DR: 随机森林被重新表述为基于设计的统计模型，揭示了其预测方差由有限聚合变异性和结构依赖性组成，且增加树数量无法消除预测变异性。


<details>
  <summary>Details</summary>
Motivation: 随机森林通常被算法化描述，缺乏基于固定数据集的统计设计视角。本文旨在建立随机森林的有限样本、基于设计的统计框架，以深入理解其预测变异性来源。

Method: 将每棵树视为显式的随机条件回归函数，使用总方差和总协方差定律分解单树离散度和树间协方差，识别训练观测重用和数据自适应划分对齐两种基本设计机制。

Result: 推导出森林预测器的精确方差恒等式，分离有限聚合变异性和结构依赖性；揭示协方差下限的存在，证明仅增加树数量无法消除预测变异性；建立随机森林作为显式有限样本统计设计的框架。

Conclusion: 随机森林应被视为明确的有限样本统计设计，其行为由其底层随机构建决定；重采样、特征级随机化和分裂选择共同控制分辨率、树变异性和依赖性；预测变异性有固有下限。

Abstract: Random forests are widely used prediction procedures, yet are typically described algorithmically rather than as statistical designs acting on a fixed dataset. We develop a finite-sample, design-based formulation of random forests in which each tree is an explicit randomized conditional regression function. This perspective yields an exact variance identity for the forest predictor that separates finite-aggregation variability from a structural dependence term that persists even under infinite aggregation. We further decompose both single-tree dispersion and inter-tree covariance using the laws of total variance and covariance, isolating two fundamental design mechanisms-reuse of training observations and alignment of data-adaptive partitions. These mechanisms induce a strict covariance floor, demonstrating that predictive variability cannot be eliminated by increasing the number of trees alone. The resulting framework clarifies how resampling, feature-level randomization, and split selection govern resolution, tree variability, and dependence, and establishes random forests as explicit finite-sample statistical designs whose behavior is determined by their underlying randomized construction.

</details>


### [19] [TFTF: Training-Free Targeted Flow for Conditional Sampling](https://arxiv.org/abs/2602.12932)
*Qianqian Qu,Jun S. Liu*

Main category: stat.ML

TL;DR: 提出一种基于重要性采样的免训练条件采样方法，通过结合SMC重采样和可调噪声的随机流，解决高维设置中的权重退化问题，在MNIST、CIFAR-10和CelebA-HQ上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统重要性采样在高维条件下存在权重退化问题，需要一种有效的免训练条件采样方法，能够在保持理论保证的同时处理高维、多模态数据。

Method: 1) 基于重要性采样的免训练条件采样框架；2) 结合序列蒙特卡洛(SMC)重采样技术解决高维权重退化；3) 引入可调噪声强度的随机流替代确定性流，促进样本沿不同轨迹发散。

Result: 在MNIST和CIFAR-10的条件采样任务中显著优于现有方法，在CelebA-HQ的文本到图像生成实验中展示了高维多模态设置下的适用性。

Conclusion: 提出的免训练条件采样方法通过SMC重采样和随机流改进，有效解决了高维重要性采样的权重退化问题，具有理论保证并在多个数据集上表现出色。

Abstract: We propose a training-free conditional sampling method for flow matching models based on importance sampling. Because a naïve application of importance sampling suffers from weight degeneracy in high-dimensional settings, we modify and incorporate a resampling technique in sequential Monte Carlo (SMC) during intermediate stages of the generation process. To encourage generated samples to diverge along distinct trajectories, we derive a stochastic flow with adjustable noise strength to replace the deterministic flow at the intermediate stage. Our framework requires no additional training, while providing theoretical guarantees of asymptotic accuracy. Experimentally, our method significantly outperforms existing approaches on conditional sampling tasks for MNIST and CIFAR-10. We further demonstrate the applicability of our approach in higher-dimensional, multimodal settings through text-to-image generation experiments on CelebA-HQ.

</details>


### [20] [AdaGrad-Diff: A New Version of the Adaptive Gradient Algorithm](https://arxiv.org/abs/2602.13112)
*Matia Bojovic,Saverio Salzo,Massimiliano Pontil*

Main category: stat.ML

TL;DR: 提出一种新的自适应优化方法，使用梯度差异的累积平方范数而非梯度范数本身来调整步长，在梯度变化小时避免不必要减小步长，在梯度波动大时自动阻尼步长


<details>
  <summary>Details</summary>
Motivation: 传统梯度方法对步长选择高度敏感，需要手动调参。自适应方法如AdaGrad虽缓解此问题，但仍有改进空间。作者希望开发更鲁棒的自适应优化算法

Method: 提出AdaGrad风格的自适应方法，使用连续梯度差异的累积平方范数而非梯度范数本身来驱动步长调整。当梯度在迭代间变化小时，步长不会不必要减小；当梯度波动显著时（反映曲率或不稳定性），自动进行步长阻尼

Result: 数值实验表明，所提方法在多个实际相关设置中比AdaGrad更鲁棒

Conclusion: 通过使用梯度差异而非梯度本身来调整步长，可以开发出比标准AdaGrad更鲁棒的自适应优化方法，在梯度变化小时避免过度保守，在需要时提供适当阻尼

Abstract: Vanilla gradient methods are often highly sensitive to the choice of stepsize, which typically requires manual tuning. Adaptive methods alleviate this issue and have therefore become widely used. Among them, AdaGrad has been particularly influential. In this paper, we propose an AdaGrad-style adaptive method in which the adaptation is driven by the cumulative squared norms of successive gradient differences rather than gradient norms themselves. The key idea is that when gradients vary little across iterations, the stepsize is not unnecessarily reduced, while significant gradient fluctuations, reflecting curvature or instability, lead to automatic stepsize damping. Numerical experiments demonstrate that the proposed method is more robust than AdaGrad in several practically relevant settings.

</details>
