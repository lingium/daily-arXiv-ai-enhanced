<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 3]
- [stat.ME](#stat.ME) [Total: 14]
- [stat.AP](#stat.AP) [Total: 6]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Online Partitioned Local Depth for semi-supervised applications](https://arxiv.org/abs/2512.15436)
*John D. Foley,Justin T. Lee*

Main category: stat.ML

TL;DR: 提出了在线分区局部深度（online PaLD）算法，适用于半监督预测等在线应用，可在O(n²)时间内将预计算的凝聚网络扩展到新数据点


<details>
  <summary>Details</summary>
Motivation: 现有分区局部深度算法需要适应在线应用场景，如半监督预测，需要能够高效处理新数据点的扩展

Method: 扩展分区局部深度算法，构建可查询的数据结构（O(n³)时间），然后可在O(n²)时间内将凝聚网络扩展到新数据点

Result: 开发出online PaLD算法，适用于在线异常检测和半监督分类，特别是在医疗数据集上

Conclusion: 提出的在线PaLD算法是对基于近似和并行化加速方法的补充，为在线应用提供了有效的解决方案

Abstract: We introduce an extension of the partitioned local depth (PaLD) algorithm that is adapted to online applications such as semi-supervised prediction. The new algorithm we present, online PaLD, is well-suited to situations where it is a possible to pre-compute a cohesion network from a reference dataset. After $O(n^3)$ steps to construct a queryable data structure, online PaLD can extend the cohesion network to a new data point in $O(n^2)$ time. Our approach complements previous speed up approaches based on approximation and parallelism. For illustrations, we present applications to online anomaly detection and semi-supervised classification for health-care datasets.

</details>


### [2] [A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point](https://arxiv.org/abs/2512.15606)
*Carlos Couto,José Mourão,Mário A. T. Figueiredo,Pedro Ribeiro*

Main category: stat.ML

TL;DR: 该论文研究了神经网络在最优学习点附近的Hessian矩阵特征谱，揭示了小特征值决定长期学习性能，并分析了不同网络类型的谱分布特性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在最优学习点附近的学习性能，理解梯度下降动态如何受损失函数Hessian矩阵的影响，特别是特征谱对学习过程的作用。

Method: 使用师生网络问题框架，分析匹配权重情况下的Hessian特征谱。对线性网络进行理论分析，推导大网络下的渐近谱分布；对多项式和非线性网络进行数值分析。

Result: 发现Hessian矩阵的小特征值决定长期学习性能；线性网络的谱分布渐近遵循缩放卡方分布与缩放Marchenko-Pastur分布的卷积；多项式网络的Hessian秩可作为有效参数数量；非线性激活函数（如误差函数）的Hessian矩阵通常是满秩的。

Conclusion: 神经网络在最优点附近的Hessian特征谱对理解学习动态至关重要，不同网络架构和激活函数会导致不同的谱特性，这为优化算法设计和理论分析提供了重要见解。

Abstract: Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank.

</details>


### [3] [High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations](https://arxiv.org/abs/2512.15684)
*Victor Léger,Florent Chatelain*

Main category: stat.ML

TL;DR: 该论文从随机矩阵理论角度分析了高维PLS-SVD方法的理论性质，推导了估计与真实潜在方向对齐的渐近特征，解释了其重构性能并识别了方法的局限行为。


<details>
  <summary>Details</summary>
Motivation: 尽管PLS方法在数据集成中广泛应用且实践成功数十年，但其在高维情况下的精确理论理解仍然有限。需要从理论上解释PLS-SVD的行为特征、性能表现和局限性。

Method: 研究两个共享低秩共同潜在结构的高维数据矩阵模型，使用随机矩阵理论工具分析交叉协方差矩阵的奇异向量，推导估计与真实潜在方向对齐的渐近特征。

Result: 获得了PLS-SVD重构性能的定量解释，识别了方法表现出反直觉或限制行为的机制，证明了PLS-SVD在检测共同潜在子空间方面相对于单独PCA的渐近优越性。

Conclusion: 研究提供了高维PLS-SVD的全面理论理解，阐明了其优势和基本局限性，为该方法的应用提供了理论基础。

Abstract: Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets. Despite decades of practical success, a precise theoretical understanding of its behavior in high-dimensional regimes remains limited. In this paper, we study a data integration model in which two high-dimensional data matrices share a low-rank common latent structure while also containing individual-specific components. We analyze the singular vectors of the associated cross-covariance matrix using tools from random matrix theory and derive asymptotic characterizations of the alignment between estimated and true latent directions. These results provide a quantitative explanation of the reconstruction performance of the PLS variant based on Singular Value Decomposition (PLS-SVD) and identify regimes where the method exhibits counter-intuitive or limiting behavior. Building on this analysis, we compare PLS-SVD with principal component analysis applied separately to each dataset and show its asymptotic superiority in detecting the common latent subspace. Overall, our results offer a comprehensive theoretical understanding of high-dimensional PLS-SVD, clarifying both its advantages and fundamental limitations.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [4] [Interpretable Multivariate Conformal Prediction with Fast Transductive Standardization](https://arxiv.org/abs/2512.15383)
*Yunjie Fan,Matteo Sesia*

Main category: stat.ME

TL;DR: 提出一种保形预测方法，为单个输入下的多个可能相关的数值输出构建紧致的同步预测区间，保证有限样本覆盖率，计算高效且在小样本下仍能提供信息丰富的预测区间。


<details>
  <summary>Details</summary>
Motivation: 在多目标回归问题中，需要为多个相关输出构建同步预测区间，现有方法要么过于保守导致区间过宽，要么需要复杂的交叉输出依赖建模或辅助样本分割，计算效率低且在小样本下效果不佳。

Method: 提出一种新颖的坐标标准化程序，使不同输出维度的残差可直接比较，利用校准数据本身估计合适的缩放参数，无需建模交叉输出依赖或辅助样本分割。该方法可结合任何多目标回归模型，并克服了转导或完全保形预测相关的技术挑战。

Result: 在模拟和真实数据上的实验表明，该方法能比现有基线方法产生更紧致的预测区间，同时保持有效的同步覆盖率。

Conclusion: 该方法为多目标回归问题提供了一种计算高效、有限样本保证的同步预测区间构建方案，无需复杂依赖建模，在小样本下仍能提供信息丰富的预测区间。

Abstract: We propose a conformal prediction method for constructing tight simultaneous prediction intervals for multiple, potentially related, numerical outputs given a single input. This method can be combined with any multi-target regression model and guarantees finite-sample coverage. It is computationally efficient and yields informative prediction intervals even with limited data. The core idea is a novel \emph{coordinate-wise} standardization procedure that makes residuals across output dimensions directly comparable, estimating suitable scaling parameters using the calibration data themselves. This does not require modeling of cross-output dependence nor auxiliary sample splitting. Implementing this idea requires overcoming technical challenges associated with transductive or full conformal prediction. Experiments on simulated and real data demonstrate this method can produce tighter prediction intervals than existing baselines while maintaining valid simultaneous coverage.

</details>


### [5] [Bayesian Latent Class Regression and Variable Selection with Applications to Sleep Patterns Data](https://arxiv.org/abs/2512.14903)
*Matthew Heaney,Olive Healy,Jason Wyse,Arthur White*

Main category: stat.ME

TL;DR: 提出一个完全贝叶斯潜在类别回归框架，使用Pólya-Gamma数据增强进行高效采样，并包含预测变量和项目选择功能，应用于儿童睡眠习惯问卷数据以识别不同的睡眠行为亚组。


<details>
  <summary>Details</summary>
Motivation: 儿童睡眠困难表现异质性高，但传统评估工具（如CSHQ）将复杂性简化为单一累积分数，掩盖了需要不同干预的独特睡眠障碍模式。贝叶斯潜在类别回归模型存在计算挑战和缺乏变量选择方法的问题。

Method: 提出完全贝叶斯潜在类别回归框架，使用Pólya-Gamma数据增强实现回归系数的高效采样。扩展框架包括：1）通过潜在包含指标进行预测变量选择；2）通过部分折叠方法进行项目选择。通过模拟研究验证方法。

Result: 模拟研究显示方法能准确估计参数、解决完全模型中的可识别性问题，并成功识别信息性预测变量和项目同时排除噪声变量。应用于148名儿童的CSHQ数据，识别出三个不同亚组：焦虑夜间睡眠者、短/浅睡眠者和更普遍睡眠问题者。

Conclusion: 该方法揭示了传统CSHQ评分的局限性，展示了概率性亚组方法的优势，能更好地理解儿童睡眠困难，并发现自闭症谱系障碍诊断对亚组成员身份的预测作用。

Abstract: Sleep difficulties in children are heterogeneous in presentation, yet conventional assessment tools like the Children's Sleep Habits Questionnaire (CSHQ) reduce this complexity to a single cumulative score, obscuring distinct patterns of sleep disturbance that require different interventions. Latent Class Regression (LCR) models offer a principled approach to identify subgroups with shared sleep behaviour profiles whilst incorporating predictors of group membership, but Bayesian inference for these models has been hindered by computational challenges and the absence of variable selection methods. We propose a fully Bayesian framework for LCR that uses Pólya-Gamma data augmentation, enabling efficient sampling of regression coefficients. We extend this framework to include variable selection for both predictors and item responses: predictor variable selection via latent inclusion indicators and item selection through a partially collapsed approach. Through simulation studies, we show that the proposed methods yield accurate parameter estimates, resolve identifiability issues arising in full models and successfully identify informative predictors and items while excluding noise variables. Applying this methodology to CSHQ data from 148 children reveals distinct latent subgroups with different sleep behaviour profiles, anxious nighttime sleepers, short/light sleepers and those with more pervasive sleep problems, with each carrying distinct implications for intervention. Results also highlight the predictive role of Autism Spectrum Disorder diagnosis in subgroup membership. These findings demonstrate the limitations of conventional CSHQ scoring and illustrate the benefits of a probabilistic subgroup-based approach as an alternative for understanding paediatric sleep difficulties.

</details>


### [6] [Scaling Causal Mediation for Complex Systems: A Framework for Root Cause Analysis](https://arxiv.org/abs/2512.14764)
*Alessandro Casadei,Sreyoshi Bhaduri,Rohit Malshe,Pavan Mullapudi,Raj Ratan,Ankush Pole,Arkajit Rakshit*

Main category: stat.ME

TL;DR: 提出一个针对大规模因果DAG中多处理和多中介的可扩展中介分析框架，能够将总效应分解为可解释的直接和间接成分。


<details>
  <summary>Details</summary>
Motivation: 现代运营系统（如物流、云基础设施、工业物联网）具有复杂的相互依赖过程。传统中介分析在简单场景中有效，但无法扩展到实践中遇到的高维有向无环图（DAG），特别是当多个处理和中介相互作用时。

Method: 提出一个可扩展的中介分析框架，专门针对涉及多个处理和中介的大型因果DAG。该方法系统地将总效应分解为可解释的直接和间接成分。

Result: 通过履行中心物流的应用案例研究展示了该框架的实用性，这些场景中复杂的依赖性和不可控因素常常掩盖根本原因。

Conclusion: 该框架为解决大规模因果DAG中的中介分析问题提供了有效的解决方案，特别适用于具有复杂依赖关系的现代运营系统。

Abstract: Modern operational systems ranging from logistics and cloud infrastructure to industrial IoT, are governed by complex, interdependent processes. Understanding how interventions propagate through such systems requires causal inference methods that go beyond direct effects to quantify mediated pathways. Traditional mediation analysis, while effective in simple settings, fails to scale to the high-dimensional directed acyclic graphs (DAGs) encountered in practice, particularly when multiple treatments and mediators interact. In this paper, we propose a scalable mediation analysis framework tailored for large causal DAGs involving multiple treatments and mediators. Our approach systematically decomposes total effects into interpretable direct and indirect components. We demonstrate its practical utility through applied case studies in fulfillment center logistics, where complex dependencies and non-controllable factors often obscure root causes.

</details>


### [7] [Fully Bayesian Spectral Clustering and Benchmarking with Uncertainty Quantification for Small Area Estimation](https://arxiv.org/abs/2512.15643)
*Jairo Fúquene-Patiño*

Main category: stat.ME

TL;DR: 提出基于谱聚类的小域估计新贝叶斯模型FH-SC，利用外部协变量进行聚类，而非地理或行政标准，并开发了基准化估计和不确定性度量方法。


<details>
  <summary>Details</summary>
Motivation: 传统小域估计中的聚类通常基于地理或行政标准，缺乏灵活性。需要一种能利用外部协变量进行更智能聚类的方法，同时需要解决基准化估计和不确定性度量问题。

Method: 提出FH-SC模型，将谱聚类算法集成到Fay-Herriot模型中，利用外部协变量进行聚类。采用后验投影理论框架进行约束贝叶斯推断，推导出Rao-Blackwell估计器的闭式解，并提出条件后验均方误差作为不确定性度量。

Result: 通过模型模拟和数据模拟研究评估了CPMSE的频率性质。在哥伦比亚家庭互联网接入比例估计的案例研究中，FH-SC模型在估计精度和灵活性方面优于现有贝叶斯和频率方法。

Conclusion: FH-SC模型为小域估计提供了更灵活的聚类方法，能够有效整合外部协变量信息，并提供了可靠的基准化估计和不确定性度量框架，在实际应用中表现出优越性能。

Abstract: In this work, inspired by machine learning techniques, we propose a new Bayesian model for Small Area Estimation (SAE), the Fay-Herriot model with Spectral Clustering (FH-SC). Unlike traditional approaches, clustering in FH-SC is based on spectral clustering algorithms that utilize external covariates, rather than geographical or administrative criteria. A major advantage of the FH-SC model is its flexibility in integrating existing SAE approaches, with or without clustering random effects. To enable benchmarking, we leverage the theoretical framework of posterior projections for constrained Bayesian inference and derive closed form expressions for the new Rao-Blackwell (RB) estimators of the posterior mean under the FH-SC model. Additionally, we introduce a novel measure of uncertainty for the benchmarked estimator, the Conditional Posterior Mean Square Error (CPMSE), which is generalizable to other Bayesian SAE estimators. We conduct model-based and data-based simulation studies to evaluate the frequentist properties of the CPMSE. The proposed methodology is motivated by a real case study involving the estimation of the proportion of households with internet access in the municipalities of Colombia. Finally, we also illustrate the advantages of FH-SC over existing Bayesian and frequentist approaches through our case study.

</details>


### [8] [Stratified Bootstrap Test Package](https://arxiv.org/abs/2512.15057)
*Ehsan Mohammadi,Fanghua Chen,Yizhou Cai,Yun Yang,Ting Fung Ma,Lu Zhou*

Main category: stat.ME

TL;DR: SBT是一种非参数重抽样方法，用于评估多变量调查数据中群体排名模式的稳定性，通过重抽样检验排名一致性并进行统计推断。


<details>
  <summary>Details</summary>
Motivation: 需要一种不依赖分布假设的方法来评估调查数据中群体排名模式的稳健性，检测异常响应模式，并在AI公平性审计等应用中比较不同群体。

Method: 采用分层自助法（bootstrap）在群体内重复抽样，通过非包含指数量化排名稳健性，并构建排名统计量的零分布进行统计推断。

Result: SBT提供了一种非参数框架，既能描述性评估排名一致性，又能进行统计推断，检测异常响应模式，并比较不同群体的排名差异。

Conclusion: 该方法为调查分析、项目响应评估和AI系统公平性审计等应用提供了强大的非参数工具，用于评估排名稳定性和进行统计推断。

Abstract: The Stratified Bootstrap Test (SBT) provides a nonparametric, resampling-based framework for assessing the stability of group-specific ranking patterns in multivariate survey or rating data. By repeatedly resampling observations and examining whether a group's top-ranked items remain among the highest-scoring categories across bootstrap samples, SBT quantifies ranking robustness through a non-containment index. In parallel, the stratified bootstrap test extends this framework to formal statistical inference by testing ordering hypotheses among population means. Through resampling within groups, the method approximates the null distribution of ranking-based test statistics without relying on distributional assumptions. Together, these techniques enable both descriptive and inferential evaluation of ranking consistency, detection of aberrant or adversarial response patterns, and rigorous comparison of groups in applications such as survey analysis, item response assessment, and fairness auditing in AI systems.

</details>


### [9] [Conditional Expert Kaplan-Meier Estimation: Asymptotic Theory and an Application to Loan Default Modelling](https://arxiv.org/abs/2512.14959)
*Martin Bladt,Kristian Vilhelm Dinesen*

Main category: stat.ME

TL;DR: 提出条件专家Kaplan-Meier估计器，用于处理同时存在右删失和污染的时间到事件数据，建立包含协变量的核平滑条件版本渐近理论


<details>
  <summary>Details</summary>
Motivation: 在保险和信用风险等应用场景中，观测到的事件可能受到污染（不反映真实结果），而专家意见常用于裁决不确定事件，需要处理同时存在右删失和污染的时间到事件数据

Method: 扩展经典Kaplan-Meier估计器，开发条件版本并引入协变量通过核平滑，建立全面的渐近理论，包括函数一致性和弱收敛

Result: 无偏专家判断确保一致性，系统偏差导致可明确表征的确定性渐近偏差，通过模拟研究验证有限样本性质，并在贷款违约数据中展示实际应用

Conclusion: 条件专家Kaplan-Meier估计器为处理同时存在右删失和污染的时间到事件数据提供了理论框架，能够量化不完美专家信息引起的偏差，具有实际应用价值

Abstract: We study the conditional expert Kaplan-Meier estimator, an extension of the classical Kaplan--Meier estimator designed for time-to-event data subject to both right-censoring and contamination. Such contamination, where observed events may not reflect true outcomes, is common in applied settings, including insurance and credit risk, where expert opinion is often used to adjudicate uncertain events. Building on previous work, we develop a comprehensive asymptotic theory for the conditional version incorporating covariates through kernel smoothing. We establish functional consistency and weak convergence under suitable regularity conditions and quantify the bias induced by imperfect expert information. The results show that unbiased expert judgments ensure consistency, while systematic deviations lead to a deterministic asymptotic bias that can be explicitly characterized. We examine finite-sample properties through simulation studies and illustrate the practical use of the estimator with an application to loan default data.

</details>


### [10] [Inference for Forecasting Accuracy: Pooled versus Individual Estimators in High-dimensional Panel Data](https://arxiv.org/abs/2512.15592)
*Tim Kutta,Martin Schumann,Holger Dette*

Main category: stat.ME

TL;DR: 提出一种新的推断方法，用于比较面板数据中合并估计量与个体估计量的预测性能，构建预测误差差异的置信区间，并证明其渐近有效性。


<details>
  <summary>Details</summary>
Motivation: 面板数据分析中面临一个核心问题：是应该合并个体数据还是分别估计个体模型。合并估计量方差较低但可能存在偏差，形成估计优化的基本权衡。需要一种可靠的方法来比较这两种方法的预测性能。

Method: 开发新的推断方法，构建合并估计量与个体估计量预测误差差异的置信区间。理论框架允许模型误差中存在复杂的时间序列和横截面依赖性，并涵盖N远大于T的情况，包括经典条件N/T²→0下的独立情况。

Result: 建立了置信区间的渐近有效性，并通过广泛的模拟研究检验了所提方法的有限样本性质。

Conclusion: 该方法为面板数据分析中合并与个体估计量的选择提供了可靠的统计推断工具，特别是在大N小T或存在复杂依赖结构的情况下。

Abstract: Panels with large time $(T)$ and cross-sectional $(N)$ dimensions are a key data structure in social sciences and other fields. A central question in panel data analysis is whether to pool data across individuals or to estimate separate models. Pooled estimators typically have lower variance but may suffer from bias, creating a fundamental trade-off for optimal estimation. We develop a new inference method to compare the forecasting performance of pooled and individual estimators. Specifically, we propose a confidence interval for the difference between their forecasting errors and establish its asymptotic validity. Our theory allows for complex temporal and cross-sectional dependence in the model errors and covers scenarios where $N$ can be much larger than $T$-including the independent case under the classical condition $N/T^2 \to 0$. The finite-sample properties of the proposed method are examined in an extensive simulation study.

</details>


### [11] [Gamma family characterization and an alternative proof of Gini estimator unbiasedness](https://arxiv.org/abs/2512.14983)
*Roberto Vila*

Main category: stat.ME

TL;DR: 论文推导了基尼系数估计量期望的一般表示，通过拉普拉斯变换表征伽马分布，并提供了伽马分布下基尼系数估计量无偏性的新证明。


<details>
  <summary>Details</summary>
Motivation: 研究基尼系数估计量的期望性质，特别是在非负尺度分布族中的特性，旨在建立伽马分布的稳定性特征并提供基尼系数估计量无偏性的替代证明方法。

Method: 通过拉普拉斯变换推导基尼系数估计量期望的一般表示，利用指数倾斜分布的稳定性性质来表征伽马分布族，并应用这一表征提供新的证明方法。

Result: 建立了基尼系数估计量期望与分布拉普拉斯变换的通用关系，证明了伽马分布是唯一满足特定稳定性条件的非负尺度分布族，并成功给出了伽马分布下基尼系数估计量无偏性的新证明。

Conclusion: 论文提供了基尼系数估计量期望的通用表示方法，通过拉普拉斯变换和指数倾斜分布的稳定性性质，成功表征了伽马分布族的独特性质，并为相关无偏性证明提供了新的理论框架。

Abstract: In this paper, we derive a general representation for the expectation of the (upward adjusted) Gini coefficient estimator in terms of the Laplace transform of the underlying distribution. This representation leads to a characterization of the gamma family within the class of nonnegative scale families, based on a stability property of exponentially tilted distributions. As an application, we provide an alternative proof of the unbiasedness of the Gini coefficient estimator under gamma populations.

</details>


### [12] [Measuring Nonlinear Relationships and Spatial Heterogeneity of Influencing Factors on Traffic Crash Density Using GeoXAI](https://arxiv.org/abs/2512.14985)
*Jiaqing Lu,Ziqi Li,Lei Han,Qianwen Guo*

Main category: stat.ME

TL;DR: 本研究应用地理空间可解释AI框架分析佛罗里达州交通事故密度的空间异质性和非线性决定因素，发现道路密度、交叉口密度、社区紧凑度和教育程度等因素与事故风险存在复杂非线性关系，并提出针对性地理敏感政策建议。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以同时解释交通事故风险的非线性关系和空间异质性，需要开发能够提供地理空间可解释性的人工智能框架来理解复杂因素如何影响不同地区的交通事故密度。

Method: 采用地理空间可解释AI框架，结合高性能机器学习模型和GeoShapley方法，分析佛罗里达州道路特征和社会经济因素对交通事故密度的影响，并与SHAP和MGWR等现有方法进行比较。

Result: 研究发现道路密度、交叉口密度、社区紧凑度和教育程度等因素与事故风险存在复杂非线性关系；迈阿密等高度密集城市区域因行人活动密集和道路复杂性而事故风险显著升高；GeoShapley方法能有效捕捉各因素影响的空间异质性；主要大都市区内在事故贡献显著高于农村地区。

Conclusion: GeoShapley框架能同时解释非线性关系和空间异质性，为制定针对性地理敏感交通安全政策提供依据，包括紧凑社区的交通稳静化、适应性交叉口设计、高流量走廊速度管理以及弱势农村地区的公平性安全干预。

Abstract: This study applies a Geospatial Explainable AI (GeoXAI) framework to analyze the spatially heterogeneous and nonlinear determinants of traffic crash density in Florida. By combining a high-performing machine learning model with GeoShapley, the framework provides interpretable, tract-level insights into how roadway characteristics and socioeconomic factors contribute to crash risk. Specifically, results show that variables such as road density, intersection density, neighborhood compactness, and educational attainment exhibit complex nonlinear relationships with crashes. Extremely dense urban areas, such as Miami, show sharply elevated crash risk due to intensified pedestrian activities and roadway complexity. The GeoShapley approach also captures strong spatial heterogeneity in the influence of these factors. Major metropolitan areas including Miami, Orlando, Tampa, and Jacksonville display significantly higher intrinsic crash contributions, while rural tracts generally have lower baseline risk. Each factor exhibits pronounced spatial variation across the state. Based on these findings, the study proposes targeted, geography-sensitive policy recommendations, including traffic calming in compact neighborhoods, adaptive intersection design, speed management on high-volume corridors such as I-95 in Miami, and equity-focused safety interventions in disadvantaged rural areas of central and northern Florida. Moreover, this paper compares the results obtained from GeoShapley framework against other established methods (e.g., SHAP and MGWR), demonstrating its powerful ability to explain nonlinearity and spatial heterogeneity simultaneously.

</details>


### [13] [On non-stationarity of the Poisson gamma state space models](https://arxiv.org/abs/2512.15128)
*Kaoru Irie,Tevfik Aktekin*

Main category: stat.ME

TL;DR: PGSS模型用于非负整数值时间序列分析，其多步预测分布具有非平稳特性：预测均值恒定，预测方差随预测步长发散，最终收敛到零退化分布


<details>
  <summary>Details</summary>
Motivation: 研究PGSS模型的多步预测分布特性，揭示其非平稳性质，特别是预测方差发散和长期收敛到零的行为，类似高斯随机游走模型

Method: 利用PGSS模型的非平稳结构，分析多步预测分布的性质，研究超参数和折扣因子对预测长期行为的影响

Result: PGSS模型的预测均值保持恒定，预测方差随预测步长发散，长期预测分布收敛到零退化分布，点预测和区间预测最终都收敛到零

Conclusion: PGSS模型具有类似高斯随机游走模型的非平稳特性，超参数和折扣因子影响预测的长期行为，这对实际应用中的预测建模有重要启示

Abstract: The Poisson-gamma state space (PGSS) models have been utilized in the analysis of non-negative integer-valued time series to sequentially obtain closed form filtering and predictive densities. In this study, we show the underlying mechanics and non-stationary properties of multi-step ahead predictive distributions for the PGSS family of models. By exploiting the non-stationary structure of the PGSS model, we establish that the predictive mean remains constant while the predictive variance diverges with the forecast horizon, a property also found in Gaussian random walk models. We show that, in the long run, the predictive distribution converges to a zero-degenerated distribution, such that both point and interval forecasts eventually converge towards zero. In doing so, we comment on the effect of hyper-parameters and the discount factor on the long-run behavior of the forecasts.

</details>


### [14] [Data-driven controlled subgroup selection in clinical trials](https://arxiv.org/abs/2512.15676)
*Manuel M. Müller,Björn Bornkamp,Frank Bretz,Timothy I. Cannings,Wei Liu,Henry W. J. Reeve,Richard J. Samworth,Nikolaos Sfikas,Fang Wan,Konstantinos Sechidis*

Main category: stat.ME

TL;DR: 提出两种用于临床试验亚组选择的回归方法（广义线性模型和等渗回归），控制I类错误率，支持数据驱动的亚组识别


<details>
  <summary>Details</summary>
Motivation: 临床试验中的亚组选择对于个性化医疗至关重要，但传统方法面临数据驱动亚组识别带来的I类错误率增加和潜在偏倚等挑战

Method: 提出两种基于回归的方法：1）广义线性模型方法；2）等渗回归方法。两种方法都用于数据驱动的亚组识别，并控制I类错误率

Result: 通过全面的模拟研究评估了两种方法的优缺点，详细分析了I类错误率控制对建模假设的敏感性

Conclusion: 提出的方法能够有效识别安全亚组和高治疗效应亚组，同时控制I类错误率，为临床试验的亚组分析提供了可靠的统计工具

Abstract: Subgroup selection in clinical trials is essential for identifying patient groups that react differently to a treatment, thereby enabling personalised medicine. In particular, subgroup selection can identify patient groups that respond particularly well to a treatment or that encounter adverse events more often. However, this is a post-selection inference problem, which may pose challenges for traditional techniques used for subgroup analysis, such as increased Type I error rates and potential biases from data-driven subgroup identification. In this paper, we present two methods for subgroup selection in regression problems: one based on generalised linear modelling and another on isotonic regression. We demonstrate how these methods can be used for data-driven subgroup identification in the analysis of clinical trials, focusing on two distinct tasks: identifying patient groups that are safe from manifesting adverse events and identifying patient groups with high treatment effect, while controlling for Type I error in both cases. A thorough simulation study is conducted to evaluate the strengths and weaknesses of each method, providing detailed insight into the sensitivity of the Type I error rate control to modelling assumptions.

</details>


### [15] [Non-parametric Causal Inference in Dynamic Thresholding Designs](https://arxiv.org/abs/2512.15244)
*Aditya Ghosh,Stefan Wager*

Main category: stat.ME

TL;DR: 本文研究动态系统中阈值处理策略的因果效应估计，针对传统断点回归在时序动态问题中的局限性，提出了动态边际政策效应的估计方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗监测场景中，当患者血糖超过阈值时会被诊断为糖尿病前期并接受预防性治疗。传统的断点回归分析忽略了问题的时序动态特性，例如患者可能在下次就诊时跨越阈值并接受治疗，因此需要新的方法来估计这种动态阈值策略的长期效果。

Method: 作者研究了动态系统中阈值设计的一般问题，提出了动态边际政策效应的概念，并开发了局部线性回归方法进行估计和推断。该方法考虑了时序动态特性，能够处理患者在后续时间点跨越阈值的情况。

Result: 研究显示，即使在动态系统中，仍然可以获得相关因果目标（即治疗阈值处的动态边际政策效应）的简化形式表征。数值实验证明了所提方法的有效性。

Conclusion: 本文为动态系统中的阈值处理策略提供了有效的因果效应估计框架，解决了传统断点回归在时序动态问题中的局限性，在医疗监测等实际应用中具有重要价值。

Abstract: Consider a setting where we regularly monitor patients' fasting blood sugar, and declare them to have prediabetes (and encourage preventative care) if this number crosses a pre-specified threshold. The sharp, threshold-based treatment policy suggests that we should be able to estimate the long-term benefit of this preventative care by comparing the health trajectories of patients with blood sugar measurements right above and below the threshold. A naive regression-discontinuity analysis, however, is not applicable here, as it ignores the temporal dynamics of the problem where, e.g., a patient just below the threshold on one visit may become prediabetic (and receive treatment) following their next visit. Here, we study thresholding designs in general dynamic systems, and show that simple reduced-form characterizations remain available for a relevant causal target, namely a dynamic marginal policy effect at the treatment threshold. We develop a local-linear-regression approach for estimation and inference of this estimand, and demonstrate promise of our approach in numerical experiments.

</details>


### [16] [Accounting for missing data when modelling block maxima](https://arxiv.org/abs/2512.15429)
*Emma S. Simpson,Paul J. Northrop*

Main category: stat.ME

TL;DR: 提出一种处理块最大值数据中缺失值的方法，通过显式建模每个块中的缺失比例来扩展标准GEV模型，避免因缺失值导致的参数估计偏差和重现水平低估。


<details>
  <summary>Details</summary>
Motivation: 在应用广义极值分布（GEV）进行块最大值建模时，数据集中存在缺失值是一个常被忽视的挑战。缺失值导致无法确定每个块中是否记录了真实最大值，简单地忽略这一问题会导致参数估计偏差和重现水平被低估。

Method: 扩展标准块最大值方法，在GEV模型中显式考虑每个块中的缺失值比例。使用基于似然的推断技术，并更新常用的诊断图来评估模型拟合。

Result: 通过模拟研究评估方法性能，结果显示与"理想"无缺失值情况具有竞争力。实际应用在法国布雷斯特的海浪涌高数据和英国普利茅斯的空气污染数据上展示了方法的实用性。

Conclusion: 提出的方法能够有效处理块最大值数据中的缺失值问题，避免传统方法因忽略缺失值而导致的偏差，为实际应用中的极值分析提供了更可靠的统计工具。

Abstract: Modelling block maxima using the generalised extreme value (GEV) distribution is a classical and widely used method for studying univariate extremes. It allows for theoretically motivated estimation of return levels, including extrapolation beyond the range of observed data. A frequently overlooked challenge in applying this methodology comes from handling datasets containing missing values. In this case, one cannot be sure whether the true maximum has been recorded in each block, and simply ignoring the issue can lead to biased parameter estimators and, crucially, underestimated return levels. We propose an extension of the standard block maxima approach to overcome such missing data issues. This is achieved by explicitly accounting for the proportion of missing values in each block within the GEV model. Inference is carried out using likelihood-based techniques, and we propose an update to commonly used diagnostic plots to assess model fit. We assess the performance of our method via a simulation study, with results that are competitive with the "ideal" case of having no missing values. The practical use of our methodology is demonstrated on sea surge data from Brest, France, and air pollution data from Plymouth, U.K.

</details>


### [17] [Optimal Transport-Based Clustering of Attributed Graphs with an Application to Road Traffic Data](https://arxiv.org/abs/2512.15570)
*Ioana Gavra,Ketsia Guichard-Sustowski,Loïc Le Marrec*

Main category: stat.ME

TL;DR: 该论文提出基于距离的方法（包括Fréchet k-means和Gromov-Wasserstein方法）用于属性图划分，特别针对具有结构连接性和节点属性的真实网络数据，如交通网络。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界系统（如社交网络、交通网络）同时具有结构连接性和节点属性，理解这些系统需要联合分析网络结构和节点属性。传统的属性图划分方法在这方面存在局限，而基于Gromov-Wasserstein的方法在图匹配等通用任务中表现良好，但在节点划分方面的应用相对较少被探索。

Method: 1. 采用基于距离的方法，包括Fréchet k-means和基于最优传输的Gromov-Wasserstein方法；2. 在节点属性图背景下，引入了Fused GW方法的改进版本，具有理论保证并能处理异质属性类型；3. 提出结合基于距离的嵌入来提升性能；4. 使用专门的模拟框架进行系统评估，并在真实交通数据集上进行验证。

Result: 通过系统实验评估了目标选择的影响，评估了对噪声的鲁棒性，并为属性图聚类提供了实用指导。在道路网络背景下，这些方法能有效利用结构和属性信息揭示有意义的聚类，为改进网络理解提供了见解。

Conclusion: 基于距离的方法（特别是Gromov-Wasserstein方法）能够有效地用于属性图划分任务，在真实交通网络数据中展示了同时利用结构连接性和节点属性的能力，为复杂网络分析提供了有价值的工具。

Abstract: In many real-world contexts, such as social or transport networks, data exhibit both structural connectivity and node-level attributes. For example, roads in a transport network can be characterized not only by their connectivity but also by traffic flow or speed profiles. Understanding such systems therefore requires jointly analyzing the network structure and node attributes, a challenge addressed by attributed graph partitioning, which clusters nodes based on both connectivity and attributes. In this work, we adapt distance-based methods for this task, including Fréchet $k$-means and optimal transport-based approaches based on Gromov--Wasserstein (GW) discrepancy. We investigate how GW methods, traditionally used for general-purpose tasks such as graph matching, can be specifically adapted for node partitioning, an area that has been relatively underexplored. In the context of node-attributed graphs, we introduce an adaptation of the Fused GW method, offering theoretical guarantees and the ability to handle heterogeneous attribute types. Additionally, we propose to incorporate distance-based embeddings to enhance performance. The proposed approaches are systematically evaluated using a dedicated simulation framework and illustrated on a real-world transportation dataset. Experiments investigate the influence of target choice, assess robustness to noise, and provide practical guidance for attributed graph clustering. In the context of road networks, our results demonstrate that these methods can effectively leverage both structural and attribute information to reveal meaningful clusters, offering insights for improved network understanding.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [18] [Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies](https://arxiv.org/abs/2512.14930)
*Jaume Anguera Peris,Songtao Cheng,Hanzhao Zhang,Wei Ouyang,Joakim Jaldén*

Main category: stat.AP

TL;DR: 提出 restless multi-process multi-armed bandit (RMPMAB) 框架，将每个实验区域建模为马尔可夫链集合，用于智能显微镜控制，在资源约束下显著提高活细胞成像效率。


<details>
  <summary>Details</summary>
Motivation: 高内涵筛选显微镜产生大量活细胞成像数据，但现有方法无法确定何时何地最有效地成像。静态采样或启发式方法忽略了生物过程的动态演化，导致效率低下和事件遗漏。需要一种能平衡采集时间、计算能力和光漂白预算的优化方法。

Method: 引入 restless multi-process multi-armed bandit (RMPMAB) 框架，将每个实验区域建模为马尔可夫链集合而非单一过程，以捕捉生物系统的异质性。推导聚合过程的瞬态和渐近行为的闭式表达式，设计具有亚线性复杂度的可扩展 Whittle 索引策略。

Result: 在模拟中，RMPMAB 比 Thomson Sampling、Bayesian UCB、epsilon-Greedy 和 Round Robin 减少累积遗憾超过 37%。在实际活细胞成像实验中，捕获了 93% 更多生物学相关事件，显著提高了资源约束下的吞吐量。

Conclusion: RMPMAB 框架将随机决策理论与最优自主显微镜控制统一起来，为跨学科科学的加速发现提供了原则性方法，具有变革智能显微镜的潜力。

Abstract: High-content screening microscopy generates large amounts of live-cell imaging data, yet its potential remains constrained by the inability to determine when and where to image most effectively. Optimally balancing acquisition time, computational capacity, and photobleaching budgets across thousands of dynamically evolving regions of interest remains an open challenge, further complicated by limited field-of-view adjustments and sensor sensitivity. Existing approaches either rely on static sampling or heuristics that neglect the dynamic evolution of biological processes, leading to inefficiencies and missed events. Here, we introduce the restless multi-process multi-armed bandit (RMPMAB), a new decision-theoretic framework in which each experimental region is modeled not as a single process but as an ensemble of Markov chains, thereby capturing the inherent heterogeneity of biological systems such as asynchronous cell cycles and heterogeneous drug responses. Building upon this foundation, we derive closed-form expressions for transient and asymptotic behaviors of aggregated processes, and design scalable Whittle index policies with sub-linear complexity in the number of imaging regions. Through both simulations and a real biological live-cell imaging dataset, we show that our approach achieves substantial improvements in throughput under resource constraints. Notably, our algorithm outperforms Thomson Sampling, Bayesian UCB, epsilon-Greedy, and Round Robin by reducing cumulative regret by more than 37% in simulations and capturing 93% more biologically relevant events in live imaging experiments, underscoring its potential for transformative smart microscopy. Beyond improving experimental efficiency, the RMPMAB framework unifies stochastic decision theory with optimal autonomous microscopy control, offering a principled approach to accelerate discovery across multidisciplinary sciences.

</details>


### [19] [Early CRAB-like Biomarker Signatures Reveal a Preclinical Susceptibility Continuum for Multiple Myeloma](https://arxiv.org/abs/2512.15056)
*Bingjie Li,Jiadai Xu,Yiqing Sun,Peng Liu,Zhigang Yao*

Main category: stat.AP

TL;DR: 该研究通过分析英国生物银行37.8万参与者的常规血液检测数据，发现贫血和蛋白质代谢相关生物标志物（血红蛋白、红细胞指数、总蛋白、白蛋白等）能有效预测未来5-10年的多发性骨髓瘤风险，支持存在可检测的临床前风险连续体。


<details>
  <summary>Details</summary>
Motivation: 多发性骨髓瘤的发展过程长达数十年，但目前缺乏在临床发病前长期识别高风险个体的有效工具。研究旨在探索常规检测的"CRAB样"生物标志物（血液学指标、蛋白质代谢标志物、肾功能、血钙）在预测多发性骨髓瘤风险中的价值。

Method: 使用英国生物银行378,930名参与者的数据，系统分析了常规检测生物标志物的纵向动态和预测价值。采用多变量模型评估了反映贫血和蛋白质失衡的生物标志物（血红蛋白、红细胞指数、总蛋白、白蛋白、白蛋白/球蛋白比）与未来多发性骨髓瘤的关联，并进行了5年和10年风险区分度分析。

Result: 贫血和蛋白质代谢相关生物标志物与未来多发性骨髓瘤风险存在强烈且一致的关联，独立于人口统计学、生活方式、临床和遗传风险因素。这些标志物显示出显著的非线性剂量反应关系，将5-10年风险区分的C指数从0.66提高到0.76。纵向分析显示在诊断前十年就出现红细胞形态和蛋白质代谢特征的渐进性变化。

Conclusion: 常规实验室检测中微妙但可量化的偏差反映了恶性浆细胞扩增前的早期微环境变化，支持存在可在普通人群中检测到的临床前易感性连续体。这为风险分层和靶向监测提供了机会。

Abstract: Multiple myeloma (MM) evolves over decades, yet robust tools for identifying individuals at risk long before clinical onset remain limited. Using data from 378,930 UK Biobank participants, we systematically characterized the longitudinal dynamics and predictive value of routinely measured "CRAB-like" biomarkers, including hematologic indices, protein metabolism markers, renal function, and serum calcium. Across multivariable models, biomarkers reflecting anemia and protein imbalance (including hemoglobin, red blood cell indices, total protein, albumin, and the albumin/globulin ratio) showed strong and consistent associations with future MM, independent of demographic, lifestyle, clinical, and genetic risk factors. These markers displayed pronounced non-linear dose-response relationships and contributed substantially to 5- and 10-year MM risk discrimination, with the C-index improving from 0.66 to 0.76. Longitudinal analyses revealed progressive shifts in red cell morphology and protein metabolism profiles up to a decade before diagnosis, supporting the existence of a preclinical susceptibility continuum detectable in the general population. Our findings suggest that subtle yet quantifiable deviations in common laboratory tests reflect early microenvironmental changes that precede malignant plasma cell expansion, offering opportunities for risk stratification and targeted surveillance.

</details>


### [20] [A Statistical Framework for Spatial Boundary Estimation and Change Detection: Application to the Sahel Sahara Climate Transition](https://arxiv.org/abs/2512.15650)
*Stephen Tivenan,Indranil Sahoo,Yanjun Qian*

Main category: stat.AP

TL;DR: 提出结合异方差高斯过程回归与缩放最大绝对差全局包络检验的统一框架，用于估计空间边界曲线并检测其随时间变化，应用于萨赫勒-撒哈拉过渡带气候边界分析。


<details>
  <summary>Details</summary>
Motivation: 空间边界（如生态过渡带或气候界面）反映了陡峭的环境梯度，其结构变化可能预示着环境变化。然而，从噪声网格环境数据中量化边界位置的不确定性并正式检验其时间变化仍然具有挑战性。

Method: 提出统一框架：1）使用异方差高斯过程回归进行空间边界曲线的概率重建，捕捉空间变化的均值结构和位置特定变异性；2）采用缩放最大绝对差全局包络检验作为严格的假设检验工具，检测边界行为与预期模式的偏离。

Result: 模拟研究显示该方法在零假设下具有正确的检验水平，检测局部边界变化时具有高功效。应用于1960-1989年萨赫勒-撒哈拉过渡带：未发现干旱-半干旱或半干旱-非干旱界面在十年尺度上的统计显著变化，但成功识别出1983和1984年极端干旱年份的局部边界变化，与气候研究记录的该时期界面异常一致。

Conclusion: 该框架为从噪声环境数据中估计空间边界曲线并检测其时间变化提供了统一方法，能够识别局部边界变化，为环境监测和气候变化研究提供有力工具。

Abstract: Spatial boundaries, such as ecological transitions or climatic regime interfaces, capture steep environmental gradients, and shifts in their structure can signal emerging environmental changes. Quantifying uncertainty in spatial boundary locations and formally testing for temporal shifts remains challenging, especially when boundaries are derived from noisy, gridded environmental data. We present a unified framework that combines heteroskedastic Gaussian process (GP) regression with a scaled Maximum Absolute Difference (MAD) Global Envelope Test (GET) to estimate spatial boundary curves and assess whether they evolve over time. The heteroskedastic GP provides a flexible probabilistic reconstruction of boundary lines, capturing spatially varying mean structure and location specific variability, while the test offers a rigorous hypothesis testing tool for detecting departures from expected boundary behaviors. Simulation studies show that the proposed method achieves the correct size under the null and high power for detecting local boundary shifts. Applying our framework to the Sahel Sahara transition zone, using annual Koppen Trewartha climate classifications from 1960 to 1989, we find no statistically significant decade scale changes in the arid and semi arid or semi arid and non arid interfaces. However, the method successfully identifies localized boundary shifts during the extreme drought years of 1983 and 1984, consistent with climate studies documenting regional anomalies in these interfaces during that period.

</details>


### [21] [A Blind Source Separation Framework to Monitor Sectoral Power Demand from Grid-Scale Load Measurements](https://arxiv.org/abs/2512.15232)
*Guillaume Koechlin,Filippo Bovera,Elena Degli Innocenti,Barbara Santini,Alessandro Venturi,Simona Vazio,Piercesare Secchi*

Main category: stat.AP

TL;DR: 提出基于约束非负矩阵分解的盲源分离框架，从高压电网总负荷测量中高频分解住宅、服务业和工业部门的用电量，应用于意大利2021-2023年数据，成功重建各部门小时级消费曲线。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源主导的去中心化电力系统发展，需求侧灵活性变得至关重要。然而在区域或国家层面，由于收集终端用户消费数据的复杂性和成本，难以了解不同消费类别的相对贡献，需要开发从总负荷中分解各部门消费的方法。

Method: 采用基于约束非负矩阵分解的盲源分离框架，从高压电网总负荷测量中高频监测住宅、服务业和工业部门的消费。该方法能够从聚合数据中分离出各部门的消费模式。

Result: 成功重建了意大利2021-2023年各部门的小时级消费曲线。结果显示：家庭和服务业消费行为受季节和日期类型影响，呈现两种不同模式；工业需求则遵循单一稳定的日间模式。分解得到的月度消费估计与基于样本的指数高度一致，且比基于这些指数的预测方法更精确。

Conclusion: 提出的盲源分离框架能够有效从总负荷中分解各部门消费，为实时监测提供更精确的工具，有助于理解去中心化电力系统中不同消费类别的行为模式，支持需求侧灵活性管理。

Abstract: As we are moving towards decentralized power systems dominated by intermittent electricity generation from renewable energy sources, demand-side flexibility is becoming a critical issue. In this context, it is essential to understand the composition of electricity demand at various scales of the power grid. At the regional or national scale, there is however little visibility on the relative contributions of different consumer categories, due to the complexity and costs of collecting end-users consumption data. To address this issue, we propose a blind source separation framework based on a constrained variant of non-negative matrix factorization to monitor the consumption of residential, services and industrial sectors at high frequency from aggregate high-voltage grid load measurements. Applying the method to Italy's national load curve between 2021 and 2023, we reconstruct accurate hourly consumption profiles for each sector. Results reveal that both households and services daily consumption behaviors are driven by two distinct regimes related to the season and day type whereas industrial demand follows a single, stable daily profile. Besides, the monthly consumption estimates of each sector derived from the disaggregated load are found to closely align with sample-based indices and be more precise than forecasting approaches based on these indices for real-time monitoring.

</details>


### [22] [Cyclists route choice modeling from trip duration data in urban areas](https://arxiv.org/abs/2512.15257)
*Bertrand Jouve,Paul Rochet,Mohamadou Salifou*

Main category: stat.AP

TL;DR: 基于自行车共享系统数据，仅使用出行时长和起讫点对推断实际骑行路线，发现多数情况下用户选择最快路线，但也存在多样化行为模式。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏GPS数据，难以准确重建城市自行车骑行的实际路线，需要开发仅基于出行时长和起讫点对的推断方法。

Method: 使用对数正态混合模型对出行时间分布建模，识别不同行为模式，将观测时长与OpenStreetMap估计的最快路线时间进行比较。

Result: 对图卢兹大都市区380万次出行的分析显示：多数站点对的出行时长与OSM最快路线时间高度一致，表明常规骑行行为；混合模型也揭示了包括绕行、中途停留等异质性行为。

Conclusion: 该方法在数据有限情况下为骑行行为分析提供了稳健工具，揭示了城市自行车出行的稳定性和多样性，无需依赖空间显式数据即可洞察城市交通动态。

Abstract: The lack of GPS data limits the ability to reconstruct the actual routes taken by cyclists in urban areas. This article introduces an inference method based solely on trip durations and origin-destination pairs from bike-sharing system (BSS) users. Travel time distributions are modeled using log-normal mixture models, allowing us to identify the presence of distinct behaviors. The approach is applied to 3.8 million trips recorded in 2022 in the Toulouse metropolitan area, with observed durations compared against travel times estimated by OpenStreetMap (OSM). Results show that, for many station pairs, trip durations align closely with the fastest route suggested by OSM, reflecting a dominant and routine practice. In other cases, mixture models reveal more heterogeneous behaviors, including longer trips, detours, or intermediate stops. This approach highlights both the stability and diversity of cycling practices, providing a robust tool for usage analysis in data-limited contexts, and offering new insights into urban mobility dynamics without relying on spatially explicit data.

</details>


### [23] [Change detection with adaptive sampling for binary responses](https://arxiv.org/abs/2512.15507)
*Yanqing Yi,Su-Fen Yang*

Main category: stat.AP

TL;DR: 提出一种用于多产线系统的自适应抽样方法，通过马尔可夫决策过程优化抽样策略，优先抽样更可能发生变化的产线，提高变化检测的统计功效。


<details>
  <summary>Details</summary>
Motivation: 在多产线系统中，传统等概率随机抽样可能无法有效检测产线变化，特别是当变化仅发生在部分产线时。需要一种能根据响应信息动态调整抽样策略的方法，以提高变化检测的效率和准确性。

Method: 将自适应抽样过程建模为马尔可夫决策过程，整合抽样信息和似然比来定义奖励函数，基于平均奖励准则使用Bellman算子迭代近似最优抽样策略。对于二元响应，采用精确分布方法实现自适应抽样。

Result: 数值结果显示：当样本量≥20时，自适应抽样能更频繁地抽样有变化的产线，其变化检测的统计功效优于等概率随机抽样。随着样本量增加或失控/受控概率差异增大，自适应抽样分配给有变化产线的平均比例更高，统计功效也相应提高。

Conclusion: 自适应抽样方法能有效优化多产线系统的变化检测，通过智能分配抽样资源到更可能发生变化的产线，显著提高检测效率和统计功效，特别适用于中等以上样本量的场景。

Abstract: We propose using an adaptive sampling method to detect changes for a system with multiple lines. The adaptive sampling utilizes the information in responses to learn on which line is more likely to have a change thus allocating more units to the line. The learning process is formatted as a Markov decision process by integrating sampling information with likelihood ratio for changes to define rewards and the optimal sampling is approximated by using the Bellman operator iteratively based on the average reward criterion. We demonstrate the performance of the proposed method for binary responses using the exact distribution method for adaptive sampling. Our numeric results show that the adaptive sampling samples more often the line that has a change and the statistical power to detect a change is better than those with the equal randomization for sample sizes of 20 or higher. When sample sizes increase or the difference between out-of-control and in-control probabilities increases, the adaptive sampling allocates higher proportion of units averagely to the line with a change and the statistical power to detect a change increases.

</details>
