<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 11]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ML](#stat.ML) [Total: 7]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [On randomized step sizes in Metropolis-Hastings algorithms](https://arxiv.org/abs/2601.19710)
*Sebastiano Grazzi,Samuel Livingstone,Lionel Riou-Durand*

Main category: stat.CO

TL;DR: 研究随机步长对Metropolis-Hastings算法的影响，证明随机化能提高算法鲁棒性并优化接受率


<details>
  <summary>Details</summary>
Motivation: Metropolis-Hastings算法的性能对步长选择高度敏感，不当的步长设置会导致效率严重下降。传统固定步长算法需要精细调参，这在实际应用中具有挑战性。

Method: 研究两种随机步长算法：辅助变量构造和边缘化构造。分析随机步长如何继承固定步长算法的弱Poincaré不等式/谱间隙特性，并比较两种构造的渐近方差性能。

Result: 随机步长算法在最小条件下能继承固定步长算法的谱间隙特性；边缘化核在渐近方差上优于辅助变量选择；随机化使算法对调参更鲁棒，谱间隙随步长选择不当而多项式衰减；随机步长常保持高维尺度极限和算法复杂度，当使用指数或均匀分布随机化步长时，能提高Langevin和Hamiltonian采样器的最优接受率。

Conclusion: 步长随机化能显著提高Metropolis-Hastings算法的鲁棒性和效率，边缘化构造在可实现时应优先选择，随机步长策略是应对调参挑战的有效解决方案。

Abstract: The performance of Metropolis-Hastings algorithms is highly sensitive to the choice of step size, and miss-specification can lead to severe loss of efficiency. We study algorithms with randomized step sizes, considering both auxiliary-variable and marginalized constructions. We show that algorithms with a randomized step size inherit weak Poincaré inequalities/spectral gaps from their fixed-step-size counterparts under minimal conditions, and that the marginalized kernel should always be preferred in terms of asymptotic variance to the auxiliary-variable choice if it is implementable. In addition we show that both types of randomization make an algorithm robust to tuning, meaning that spectral gaps decay polynomially as the step size is increasingly poorly chosen. We further show that step-size randomization often preserves high-dimensional scaling limits and algorithmic complexity, while increasing the optimal acceptance rate for Langevin and Hamiltonian samplers when an Exponential or Uniform distribution is chosen to randomize the step size. Theoretical results are complemented with a numerical study on challenging benchmarks such as Poisson regression, Neal's funnel and the Rosenbrock (banana) distribution.

</details>


### [2] [Zeroth-order parallel sampling](https://arxiv.org/abs/2601.19722)
*Francesco Pozza,Giacomo Zanella*

Main category: stat.CO

TL;DR: 提出一种基于随机切片视角的零阶MCMC方法，利用并行处理器实现多项式级加速，解决了传统零阶随机梯度估计器在并行计算中效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯计算中，如何有效利用并行计算加速马尔可夫链蒙特卡洛方法是一个重要问题。当目标分布只能被评估而梯度不可用时（零阶设置），且拥有多个并行处理器时，需要开发高效的并行采样算法。

Method: 提出基于随机切片视角的新方法，而非传统的随机梯度估计器方法。该方法将零阶优化思想融入MCMC，通过随机切片技术构建采样器，特别针对并行计算环境进行优化。

Result: 理论分析表明，传统零阶随机梯度估计器在并行MCMC中表现不佳，而提出的随机切片方法能够在m个处理器上实现多项式级加速。数值研究支持了理论发现。

Conclusion: 通过随机切片视角而非随机梯度视角，开发了一类新的零阶采样器，在并行计算环境下能有效利用多个处理器实现加速，为梯度不可用情况下的贝叶斯计算提供了高效的并行解决方案。

Abstract: Finding effective ways to exploit parallel computing to accelerate Markov chain Monte Carlo methods is an important problem in Bayesian computation and related disciplines. In this paper, we consider the zeroth-order setting where the unnormalized target distribution can be evaluated but its gradient is unavailable for theoretical, practical, or computational reasons. We also assume access to $m$ parallel processors to accelerate convergence. The proposed approach is inspired by modern zeroth-order optimization methods, which mimic gradient-based schemes by replacing the gradient with a zeroth-order stochastic gradient estimator. Our contribution is twofold. First, we show that a naive application of popular zeroth-order stochastic gradient estimators within Markov chain Monte Carlo methods leads to algorithms with poor dependence on $m$, both for unadjusted and Metropolis-adjusted schemes. We then propose a simple remedy to this problem, based on a random-slice perspective, as opposed to a stochastic gradient one, obtaining a new class of zeroth-order samplers that provably achieve a polynomial speed-up in $m$. Theoretical findings are supported by numerical studies.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [3] [Asymptotic Distribution of Robust Effect Size Index](https://arxiv.org/abs/2601.19004)
*Xinyu Zhang,Rachael Muscatello,Megan Jones,Blythe Corbett,Simon Vandekar*

Main category: stat.ME

TL;DR: 本文提出了一种基于泰勒展开的RESI渐近分布理论，替代了计算密集的自举法，在线性和逻辑回归中比Cohen's d和f有更小的偏差和更可靠的覆盖率，在自闭症研究中实现了50倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: RESI（稳健效应大小指数）是一种新的标准化效应大小指标，但其置信区间构建依赖于计算密集的自举方法，限制了在高维研究中的应用。

Method: 通过泰勒展开建立RESI的渐近分布理论，适用于广泛的模型类别，并结合稳健协方差估计来处理模型误设问题。

Result: 在线性和逻辑回归的多种设置下，RESI及其置信区间比常用的Cohen's d和f具有更小的偏差和更可靠的覆盖率。在自闭症研究中，渐近方法比自举法快达50倍。

Conclusion: 该方法为RESI提供了可扩展且可靠的推断替代方案，显著增强了RESI在高维研究中的适用性。

Abstract: The Robust Effect Size Index (RESI) is a recently proposed standardized effect size to quantify association strength across models. However, its confidence interval construction has relied on computationally intensive bootstrap procedures. We establish a general theorem for the asymptotic distribution of the RESI using a Taylor expansion that accommodates a broad class of models. Simulations under various linear and logistic regression settings show that RESI and its CI have smaller bias and more reliable coverage than commonly used effect sizes such as Cohen's d and f. Combining with robust covariance estimation yields valid inference under model misspecification. We use the methods to investigate associations of depression and behavioral problems with sex and diagnosis in Autism spectrum disorders, and demonstrate that the asymptotic approach achieves up to a 50-fold speedup over the bootstrap. Our work provides a scalable and reliable alternative to bootstrap inference, greatly enhancing the applicability of RESI to high-dimensional studies.

</details>


### [4] [Direct Doubly Robust Estimation of Conditional Quantile Contrasts](https://arxiv.org/abs/2601.19666)
*Josh Givens,Song Liu,Henry W J Reeve,Katarzyna Reluga*

Main category: stat.ME

TL;DR: 提出首个直接估计条件分位数比较器(CQC)的方法，避免现有方法需要先估计条件累积分布函数再求逆的复杂过程，提高了估计的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在异质性处理效应分析中，CQC作为一种有前景的估计量，结合了条件分位数处理效应(CQTE)的分位数层面总结和条件平均处理效应(CATE)的可解释性。然而，现有CQC估计方法需要先估计条件累积分布函数再求逆，这导致估计过程不透明，难以建模和解释。

Method: 提出了首个直接估计CQC的方法，允许显式建模和参数化。通过直接参数化CQC，能够更好地解释估计结果，同时提供约束和指导模型的手段。该方法保持了关于干扰参数估计的双重稳健性。

Result: 理论和实证研究表明，该方法的估计误差直接取决于CQC本身的复杂性，优于现有估计程序。在多种数据场景下，随着样本量和干扰误差的变化，该方法在估计准确性方面均优于现有方法。在实际就业计划数据应用中，发现随着参与者年龄增长，潜在收入改善范围减小。

Conclusion: 提出的直接CQC估计方法解决了现有方法的局限性，提供了更好的可解释性和建模能力，在理论和实证上都表现出优越性，并在实际应用中揭示了有意义的发现。

Abstract: Within heterogeneous treatment effect (HTE) analysis, various estimands have been proposed to capture the effect of a treatment conditional on covariates. Recently, the conditional quantile comparator (CQC) has emerged as a promising estimand, offering quantile-level summaries akin to the conditional quantile treatment effect (CQTE) while preserving some interpretability of the conditional average treatment effect (CATE). It achieves this by summarising the treated response conditional on both the covariates and the untreated response. Despite these desirable properties, the CQC's current estimation is limited by the need to first estimate the difference in conditional cumulative distribution functions and then invert it. This inversion obscures the CQC estimate, hampering our ability to both model and interpret it. To address this, we propose the first direct estimator of the CQC, allowing for explicit modelling and parameterisation. This explicit parameterisation enables better interpretation of our estimate while also providing a means to constrain and inform the model. We show, both theoretically and empirically, that our estimation error depends directly on the complexity of the CQC itself, improving upon the existing estimation procedure. Furthermore, it retains the desirable double robustness property with respect to nuisance parameter estimation. We further show our method to outperform existing procedures in estimation accuracy across multiple data scenarios while varying sample size and nuisance error. Finally, we apply it to real-world data from an employment scheme, uncovering a reduced range of potential earnings improvement as participant age increases.

</details>


### [5] [Mixture-Weighted Ensemble Kalman Filter with Quasi-Monte Carlo Transport](https://arxiv.org/abs/2601.18992)
*Ilja Klebanov,Claudia Schillings,Dana Wrischnig*

Main category: stat.ME

TL;DR: 结合Bootstrap粒子滤波和集合卡尔曼滤波，提出混合加权IS-EnKF方案，并引入传输准蒙特卡洛方法降低采样误差


<details>
  <summary>Details</summary>
Motivation: BPF存在权重退化问题，EnKF在高维情况下扩展性好但仅在线性高斯情况下精确。需要结合两者优势，在保留EnKF传输步骤的同时加入重要性采样校正

Method: 1. 建立混合目标和提议分布的重要性采样理论；2. 将随机EnKF分析解释为从显式高斯混合提议中采样；3. 提出六种自归一化IS-EnKF方案；4. 引入传输准蒙特卡洛方法处理高斯混合分布

Result: 数值实验显示：提出的混合加权和TQMC增强滤波器相比BPF、EnKF和标准加权EnKF具有更好的滤波精度，加权方案消除了EnKF中分析目标不匹配导致的误差平台

Conclusion: 成功结合了BPF和EnKF的优势，提出的IS-EnKF方案在保持EnKF计算效率的同时提高了精度，TQMC方法进一步降低了采样误差

Abstract: The Bootstrap Particle Filter (BPF) and the Ensemble Kalman Filter (EnKF) are two widely used methods for sequential Bayesian filtering: the BPF is asymptotically exact but can suffer from weight degeneracy, while the EnKF scales well in high dimension yet is exact only in the linear-Gaussian case. We combine these approaches by retaining the EnKF transport step and adding a principled importance-sampling correction. Our first contribution is a general importance-sampling theory for mixture targets and proposals, including variance comparisons between individual- and mixture-based estimators. We then interpret the stochastic EnKF analysis as sampling from explicit Gaussian-mixture proposals obtained by conditioning on the current or previous ensemble, which leads to six self-normalized IS-EnKF schemes. We embed these updates into a broader class of ensemble-based filters and prove consistency and error bounds, including weight-variance comparisons and sufficient conditions ensuring finite-variance importance weights. As a second contribution, we construct transported quasi-Monte Carlo (TQMC) point sets for the Gaussian-mixture laws arising in prediction and analysis, yielding TQMC-enhanced variants that can substantially reduce sampling error without changing the filtering pipeline. Numerical experiments on benchmark models compare the proposed mixture-weighted and TQMC-enhanced filters, showing improved filtering accuracy relative to BPF, EnKF, and the standard weighted EnKF, and that the weighted schemes eliminate the EnKF error plateau often caused by analysis-target mismatch.

</details>


### [6] [A Fast, Closed-Form Bandwidth Selector for the Beta Kernel Density Estimator](https://arxiv.org/abs/2601.19553)
*Johan Hallberg Szabadváry*

Main category: stat.ME

TL;DR: 提出了一种快速闭式带宽选择器（ROT），用于Beta核密度估计，解决了现有方法计算昂贵且不稳定的问题，性能媲美数值优化但速度快35000倍。


<details>
  <summary>Details</summary>
Motivation: Beta核在单位区间数据估计上理论优于高斯核，无需边界校正，但缺乏可靠的带宽选择器。现有迭代优化方法计算昂贵且不稳定，限制了Beta核的广泛应用。

Method: 基于Beta参考分布的无加权渐近均方积分误差（AMISE）推导出快速闭式带宽选择器（ROT）。针对U型和J型分布的边界可积性问题，引入原则性启发式方法。采用矩估计近似，将带宽选择复杂度从迭代优化降低到O(1)。

Result: 蒙特卡洛模拟显示，该规则在精度上匹配数值优化方法，同时实现超过35000倍的加速。真实社会经济数据验证表明，避免了高斯方法常见的"边界消失"和"肩部"伪影。

Conclusion: 提出的ROT带宽选择器使Beta核密度估计变得实用，提供了开源Python包，可作为标准密度估计工具的即插即用替代方案。

Abstract: The Beta kernel estimator offers a theoretically superior alternative to the Gaussian kernel for unit interval data, eliminating boundary bias without requiring reflection or transformation. However, its adoption remains limited by the lack of a reliable bandwidth selector; practitioners currently rely on iterative optimization methods that are computationally expensive and prone to instability. We derive the ``\rot,'' a fast, closed-form bandwidth selector based on the unweighted Asymptotic Mean Integrated Squared Error (AMISE) of a beta reference distribution. To address boundary integrability issues, we introduce a principled heuristic for U-shaped and J-shaped distributions. By employing a method-of-moments approximation, we reduce the bandwidth selection complexity from iterative optimization to $\mathcal{O}(1)$. Extensive Monte Carlo simulations demonstrate that our rule matches the accuracy of numerical optimization while delivering a speedup of over 35,000 times. Real-world validation on socioeconomic data shows that it avoids the ``vanishing boundary'' and ``shoulder'' artifacts common to Gaussian-based methods. We provide a comprehensive, open-source Python package to facilitate the immediate adoption of the Beta kernel as a drop-in replacement for standard density estimation tools.

</details>


### [7] [Local Variable and Neighborhood Selection for Firearm Fatality in the Southeast USA](https://arxiv.org/abs/2601.19044)
*Debjoy Thakur,Lingyuan Zhao,Soutir Bandyopadhyay*

Main category: stat.ME

TL;DR: 提出一个针对空间相关、过度离散的枪击死亡数据的局部变量选择与推断两阶段框架，使用SCAD惩罚进行局部变量选择，并分析潜在空间邻域结构的方向性变化。


<details>
  <summary>Details</summary>
Motivation: 美国枪击死亡是一个重大公共卫生问题，但现有研究在处理空间相关、过度离散数据时存在真空，特别是在局部识别影响枪击死亡的社会经济因素方面缺乏方法。

Method: 两阶段框架：1) 对枪击事件超过阈值的特定位置使用SCAD惩罚进行局部变量选择；2) 对选定的预测变量，分析潜在空间邻域结构的方向性变化，识别最适合建模随机效应的空间邻域结构类型。

Result: 该方法具有三重优势：局部变量选择、提供选定预测变量方向性变化的推断、识别最适合的空间邻域结构而非随意假设。理论性质在填充渐近下讨论。

Conclusion: 该研究填补了空间相关、过度离散数据局部变量选择的方法真空，为理解枪击死亡的空间异质性及其社会经济驱动因素提供了新的分析框架。

Abstract: A major public health concern in the United States (US) is gun-related deaths. The number of gun injuries largely varies spatially because of county-wise heterogeneity of race, sex, age, and income distributions. But still, a major challenge is to locally identify the influential socio-economic factors behind these firearm fatality incidents. For a diverging number of predictors, a rich literature exists regarding SCAD under the independence framework; however, a vacuum remains when discussing local variable selection for spatially correlated, over-dispersed data. This research presents a two-step localized variable selection and inference framework for spatially indexed gunshot fatality data. In the first step, we select variables locally using the SCAD penalty for specific locations where the number of gunshot incidents exceeds a threshold. For these locations, after selecting the predictors, we proceed to the next step, which involves examining the directional variation in the latent spatial neighborhood structure. We further discuss the theoretical properties of this county-specific local variable selection under infill asymptotics. This method has threefold advantages: (i) this method selects the variables locally, (ii) this method provides inference about directional variation of a selected predictor, and (iii) instead of assuming the spatial neighborhood structure in an ad hoc manner, this method identifies the specific type of spatial neighborhood structure that is most appropriate for modeling the random effects.

</details>


### [8] [A class of skew-multivariate distributions for spatial data](https://arxiv.org/abs/2601.19049)
*Pavel Krupskii*

Main category: stat.ME

TL;DR: 提出基于多元帕累托混合分布的空间数据copula模型，能同时捕捉尾部依赖、渐近独立和尾部不对称性，具有计算可行性并在温度数据中验证实用性


<details>
  <summary>Details</summary>
Motivation: 现实世界空间数据常表现出复杂的尾部依赖结构，包括尾部依赖、渐近独立和尾部不对称性，现有模型难以同时捕捉这些特征，需要更灵活的建模方法

Method: 基于多元帕累托混合分布构建空间copula模型，探索其尾部性质，考虑具有计算可行性的特殊情形，使用最大似然估计进行参数估计

Result: 模型能有效捕捉尾部依赖、渐近独立和尾部不对称性，模拟研究验证了最大似然估计的有限样本性能，温度数据应用展示了实际效用

Conclusion: 提出的帕累托混合copula模型为空间数据提供了灵活的尾部依赖建模框架，能同时处理分布的主体和极端尾部，具有实际应用价值

Abstract: This paper introduces a class of copula models for spatial data, based on multivariate Pareto-mixture distributions. We explore the tail properties of these models, demonstrating their ability to capture both tail dependence and asymptotic independence, as well as the tail asymmetry frequently observed in real-world data. The proposed models also offer flexibility in accounting for permutation asymmetry and can effectively represent both the bulk and extreme tails of the distribution. We consider special cases of these models with computationally tractable likelihoods and present an extensive simulation study to assess the finite-sample performance of the maximum likelihood estimators. Finally, we apply our models to analyze a temperature dataset, showcasing their practical utility.

</details>


### [9] [Modeling Ordinal Survey Data with Unfolding Models](https://arxiv.org/abs/2601.19167)
*Rayleigh Lei,Abel Rodriguez*

Main category: stat.ME

TL;DR: 提出一种新的序数probit展开模型，能够同时处理单调和非单调响应函数，用于分析Likert类调查数据。


<details>
  <summary>Details</summary>
Motivation: 传统潜在因子模型假设响应函数在潜在特质上是单调的，这一假设在政治科学和市场营销等多个应用领域可能过于严格，需要能够同时处理单调和非单调响应函数的方法。

Method: 提出了一种新颖的序数probit展开模型，该模型能够适应单调和非单调的响应函数，扩展了传统模型的应用范围。

Result: 通过分析美国移民态度调查数据，展示了该模型的优势，能够更好地捕捉调查数据中的复杂模式。

Conclusion: 新提出的序数probit展开模型为分析Likert类调查数据提供了更灵活的工具，特别适用于需要处理非单调响应模式的应用场景。

Abstract: Surveys that rely on ordinal polychotomous (Likert-like) items are widely employed to capture individual preferences because they allow respondents to express both the direction and strength of their preferences. Latent factor models traditionally used in this context implicitly assume that the response functions (the cumulative distribution of the ordinal outcome) are monotonic on the latent trait. This assumption can be too restrictive in several application areas, including in political science and marketing. In this work, we propose a novel ordinal probit unfolding model that can accommodate both monotonic and non-monotonic response functions. The advantages of the model are illustrated by analyzing an immigration attitude survey conducted in the United States.

</details>


### [10] [Coarsened data in small area estimation: a Bayesian two-part model for mapping smoking behaviour](https://arxiv.org/abs/2601.19729)
*Aldo Gardini,Lorenzo Mori*

Main category: stat.ME

TL;DR: 该论文提出了一种贝叶斯单元级小区域估计框架，用于处理半连续、粗化响应数据，应用于意大利吸烟指标估计。


<details>
  <summary>Details</summary>
Motivation: 在流行病学和公共卫生中，估计受限亚人群的健康指标是一个常见挑战。调查数据常受粗化机制（如四舍五入和数字偏好）影响，这会降低数据分辨率并可能导致推断偏差。意大利2019年欧洲健康访谈调查中的吸烟指标估计需要同时解决小区域估计和粗化数据问题。

Method: 开发了一个贝叶斯单元级小区域估计框架，采用两部分结构：1）逻辑回归部分用于吸烟流行率；2）对数正态分布的灵活混合模型用于平均香烟消费量。该模型还包含明确的粗化和顶编码模型。

Result: 模拟研究表明，忽略粗化会导致有偏和不稳定的区域估计，区间覆盖率较差。而提出的模型提高了准确性，实现了接近名义水平的覆盖率。实证应用提供了跨区域-年龄域的吸烟模式详细图景。

Conclusion: 该贝叶斯框架能有效处理粗化响应数据，为小区域估计提供更准确可靠的推断，有助于描述吸烟现象的动态特征并为有针对性的公共卫生政策提供信息。

Abstract: Estimating health indicators for restricted sub-populations is a recurring challenge in epidemiology and public health. When survey data are used, Small Area Estimation (SAE) methods can improve precision by borrowing strength across domains. In many applications, however, outcomes are self-reported and affected by coarsening mechanisms, such as rounding and digit preference, that reduce data resolution and may bias inference. This paper addresses both issues by developing a Bayesian unit-level SAE framework for semi-continuous, coarsened responses. Motivated by the 2019 Italian European Health Interview Survey, we estimate smoking indicators for domains defined by the cross-classification of Italian regions and age groups, capturing both smoking prevalence and intensity. The model adopts a two-part structure: a logistic component for smoking prevalence and a flexible mixture of Lognormal distributions for average cigarette consumption, coupled with an explicit model for coarsening and topcoding. Simulation studies show that ignoring coarsening can yield biased and unstable domain estimates with poor interval coverage, whereas the proposed model improves accuracy and achieves near-nominal coverage. The empirical application provides a detailed picture of smoking patterns across region-age domains, helping to characterize the dynamics of the phenomenon and inform targeted public health policies.

</details>


### [11] [Personalized Treatment Hierarchies in Bayesian Network Meta-Analysis](https://arxiv.org/abs/2601.19836)
*Augustine Wigle,Erica E. M. Moodie*

Main category: stat.ME

TL;DR: 该论文探讨了在包含治疗-协变量交互作用的网络Meta分析中，如何针对特定协变量剖面创建治疗排序层次


<details>
  <summary>Details</summary>
Motivation: 网络Meta分析常用于生成治疗排序层次，但当模型包含治疗-协变量交互作用时，相对治疗效果会随协变量值变化，因此需要针对特定协变量剖面创建治疗排序

Method: 扩展标准贝叶斯网络Meta分析的治疗排序方法，提出针对特定协变量剖面创建治疗层次的方法，并以重度抑郁症治疗研究的真实网络数据为例进行演示

Result: 展示了在包含治疗-协变量交互作用的网络Meta分析中，如何为特定协变量剖面生成治疗排序层次，避免了忽略协变量影响的错误排序

Conclusion: 当网络Meta分析模型包含治疗-协变量交互作用时，必须针对特定协变量剖面创建治疗排序层次，否则可能得出误导性结论

Abstract: Network Meta-Analysis (NMA) is an increasingly popular evidence synthesis tool that can provide a ranking of competing treatments, also known as a treatment hierarchy. Treatment-Covariate Interactions (TCIs) can be included in NMA models to allow relative treatment effects to vary with covariate values. We show that in an NMA model that includes TCIs, treatment hierarchies should be created with a particular covariate profile in mind. We outline the typical approach for creating a treatment hierarchy in standard Bayesian NMA and show how a treatment hierarchy for a particular covariate profile can be created from an NMA model that estimates TCIs. We demonstrate our methods using a real network of studies for treatments of major depressive disorder.

</details>


### [12] [Cure models: from mixture to matrix distributions](https://arxiv.org/abs/2601.19774)
*Martin Bladt,Jorge Yslas*

Main category: stat.ME

TL;DR: 提出基于相型分布的治愈率模型，允许治愈在基线或随访期间动态发生，相比传统混合治愈模型更灵活


<details>
  <summary>Details</summary>
Motivation: 现有参数化治愈率模型主要基于有限混合模型，对治愈机制和易感事件时间分布施加了限制性假设，需要更灵活的建模方法

Method: 引入基于相型分布的治愈模型，利用其潜在马尔可夫跳跃过程表示，允许免疫在基线或随访期间动态发生；开发统一的回归框架处理协变量对治愈率和易感生存分布的影响；使用期望最大化算法进行估计，并采用自动模型选择策略

Result: 该模型类具有稠密性，减少了参数误设的影响；模拟研究和真实数据示例证明了该方法的实际优势

Conclusion: 提出的相型分布治愈模型提供了灵活且可解释的长期生存建模框架，包含经典混合治愈模型作为特例，在实际应用中具有优势

Abstract: Cure rate models address survival data in which a proportion of individuals will never experience the event of interest. Existing parametric approaches are predominantly based on finite mixtures, which impose restrictive assumptions on both the cure mechanism and the distribution of susceptible event times. A cure model based on phase-type distributions is introduced, leveraging their latent Markov jump process representation to allow immunity to occur either at baseline or dynamically during follow-up. This structure yields a flexible and interpretable formulation of long-term survival while encompassing classical mixture cure models as special cases. A unified regression framework is developed for covariate effects on both the cure rate and the susceptible survival distribution, and the proposed model class is dense, reducing the impact of parametric misspecification. Estimation is performed via expectation-maximization algorithms, accompanied by an automatic model selection strategy. Simulation studies and a real-data example demonstrate the practical advantages of the approach.

</details>


### [13] [M-SGWR: Multiscale Similarity and Geographically Weighted Regression](https://arxiv.org/abs/2601.19888)
*M. Naser Lessani,Zhenlong Li,Manzhu Yu,Helen Greatrex,Chan Shen*

Main category: stat.ME

TL;DR: 提出M-SGWR多尺度局部回归框架，结合地理邻近性和属性相似性两个维度来量化空间关系，优于传统仅基于地理距离的方法。


<details>
  <summary>Details</summary>
Motivation: 传统局部回归模型（如GWR和MGWR）仅通过地理邻近性量化空间关系，但在全球化和数字连接时代，仅地理邻近性可能不足以捕捉位置间的相互联系。需要同时考虑地理邻近性和属性相似性。

Method: 提出M-SGWR多尺度局部回归框架，为每个预测变量分别构建地理权重矩阵和基于属性的权重矩阵，通过优化参数alpha控制两者对局部模型拟合的相对贡献。类似MGWR中的变量特定带宽，最优alpha因预测变量而异，使模型能灵活处理地理效应、混合效应或非空间（远程相似性）效应。

Result: 两个模拟实验和一个实证应用的结果表明，M-SGWR在所有拟合优度指标上均一致优于GWR、SGWR和MGWR。

Conclusion: M-SGWR框架通过同时考虑地理邻近性和属性相似性，更全面地捕捉空间关系，为空间分析提供了更灵活和有效的工具。

Abstract: The first law of geography is a cornerstone of spatial analysis, emphasizing that nearby and related locations tend to be more similar, however, defining what constitutes "near" and "related" remains challenging, as different phenomena exhibit distinct spatial patterns. Traditional local regression models, such as Geographically Weighted Regression (GWR) and Multiscale GWR (MGWR), quantify spatial relationships solely through geographic proximity. In an era of globalization and digital connectivity, however, geographic proximity alone may be insufficient to capture how locations are interconnected. To address this limitation, we propose a new multiscale local regression framework, termed M-SGWR, which characterizes spatial interaction across two dimensions: geographic proximity and attribute (variable) similarity. For each predictor, geographic and attribute-based weight matrices are constructed separately and then combined using an optimized parameter, alpha, which governs their relative contribution to local model fitting. Analogous to variable-specific bandwidths in MGWR, the optimal alpha varies by predictor, allowing the model to flexibly account for geographic, mixed, or non-spatial (remote similarity) effects. Results from two simulation experiments and one empirical application demonstrate that M-SGWR consistently outperforms GWR, SGWR, and MGWR across all goodness-of-fit metrics.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [14] [A penalized heteroskedastic ordered probit model for DIF (measurement invariance) testing of single-item assessments in cross-cultural research](https://arxiv.org/abs/2601.18889)
*R Noah Padgett*

Main category: stat.AP

TL;DR: 提出了一种评估单项目测量中DIF/MI的新方法，解决了传统方法无法处理单项目评估的问题


<details>
  <summary>Details</summary>
Motivation: 传统DIF/MI方法无法应用于单项目评估，因为回归方法需要条件变量作为潜在变量代理，因子分析方法需要多个项目来估计参数

Method: 提出了一种专门用于评估单项目测量中DIF/MI的新方法（具体方法未在摘要中详细说明）

Result: 该方法使得单项目评估中的DIF/MI检验成为可能，填补了方法学空白

Conclusion: 该方法不应替代多项目MG-CFA/IRT分析或回归方法，多项目评估能提供更好的构念覆盖和更严格的DIF/MI评估

Abstract: Differential item functioning (DIF) or measurement invariance (MI) testing for single-item assessments has previously been impossible. Part of the issue is that there are no conditioning variables to serve as a proxy for the latent variable--regression-based DIF methods. Another reason is that factor-analytic approaches require multiple items to estimate parameters. In this technical working paper, I propose an approach for evaluating DIF/MI in a single-item assessment of a construct. The current methods should NOT replace using multiple-indicator MG-CFA/IRT analyses of DIF/MI or regression mased methods when possible. More items generally provide significantly better construct coverage and provide more rigorous DIF/MI evaluation.

</details>


### [15] [Embedding Birth-Death Processes within a Dynamic Stochastic Block Model](https://arxiv.org/abs/2601.19277)
*Gabriela Bayolo Soler,Miraine Dávila Felipe,Ghislaine Gayraud*

Main category: stat.AP

TL;DR: 提出了一种结合生死过程的动态随机块模型，用于处理节点可进出网络的动态网络聚类问题


<details>
  <summary>Details</summary>
Motivation: 现有动态网络聚类研究主要关注固定节点数量的网络，但现实世界中网络节点数量经常变化（节点进入或退出），这一重要问题尚未得到充分关注

Method: 扩展动态随机块模型（dSBM），引入生死过程来处理节点数量的动态变化；开发了变分期望最大化（VEM）算法进行参数推断和潜在社区预测

Result: 提出了一个包含生死过程的动态随机块模型新框架，能够有效处理节点数量变化的动态网络聚类问题

Conclusion: 该研究填补了动态网络聚类中节点数量变化问题的空白，为处理现实世界中节点可进出的动态网络提供了有效的统计建模工具

Abstract: Statistical clustering in dynamic networks aims to identify groups of nodes with similar or distinct internal connectivity patterns as the network evolves over time. While early research primarily focused on static Stochastic Block Models (SBMs), recent advancements have extended these models to handle dynamic and weighted networks, allowing for a more accurate representation of temporal variations in structure. Additional developments have introduced methods for detecting structural changes, such as shifts in community membership. However, limited attention has been paid to dynamic networks with variable population sizes, where nodes may enter or exit the network. To address this gap, we propose an extension of dynamic SBMs (dSBMs) that incorporates a birth-death process, enabling the statistical clustering of nodes in dynamic networks with evolving population sizes. This work makes three main contributions: (1) the introduction of a novel model for dSBMs with birth-death processes, (2) a framework for parameter inference and prediction of latent communities in this model, and (3) the development of an adapted Variational Expectation-Maximization (VEM) algorithm for efficient inference within this extended framework.

</details>


### [16] [Adaptive L-tests for high dimensional independence](https://arxiv.org/abs/2601.19688)
*Ping Zhao,Huifang Ma*

Main category: stat.AP

TL;DR: 提出基于L-统计量的高维互独立性检验新方法，结合固定k和发散k的统计量，通过Cauchy方法构建自适应检验


<details>
  <summary>Details</summary>
Motivation: 多随机变量互独立性检验是统计学中的基本问题，在基因组学、金融和神经科学等领域有广泛应用，需要发展适用于高维数据的有效检验方法

Method: 基于L-统计量构建高维互独立性检验，建立固定k和发散k两种情况的渐近分布，证明两者的渐近独立性，通过Cauchy方法组合构建自适应检验

Result: 建立了固定k和发散k统计量的渐近分布，证明了它们的渐近独立性，构建的自适应检验在理论上有保证，在实际中对多种备择假设都有良好功效

Conclusion: 提出的基于L-统计量的自适应检验方法既有理论依据，又在实际应用中表现出色，模拟研究验证了其优势

Abstract: Testing mutual independence among multiple random variables is a fundamental problem in statistics, with wide applications in genomics, finance, and neuroscience. In this paper, we propose a new class of tests for high-dimensional mutual independence based on $L$-statistics. We establish the asymptotic distribution of the proposed test when the order parameter $k$ is fixed, and prove asymptotic normality when $k$ diverges with the dimension. Moreover, we show the asymptotic independence of the fixed-$k$ and diverging-$k$ statistics, enabling their combination through the Cauchy method. The resulting adaptive test is both theoretically justified and practically powerful across a wide range of alternatives. Simulation studies demonstrate the advantages of our method.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [17] [Statistical Inference for Explainable Boosting Machines](https://arxiv.org/abs/2601.18857)
*Haimo Fang,Kevin Tan,Jonathan Pipping,Giles Hooker*

Main category: stat.ML

TL;DR: 该论文提出了一种用于可解释提升机（EBMs）的统计推断方法，通过Boulevard正则化将提升过程转化为特征级核岭回归，实现了预测区间和置信区间的快速计算，避免了维度灾难。


<details>
  <summary>Details</summary>
Motivation: 可解释提升机（EBMs）作为"玻璃盒"模型虽然具有可解释性，但其学习到的单变量函数缺乏不确定性量化方法。传统方法需要计算密集的自举法，难以确定哪些特征真正重要，因此需要更高效的统计推断方法。

Method: 采用Boulevard正则化，使用移动平均代替树求和，使提升过程收敛到特征级核岭回归。这种方法产生渐近正态预测，并构建响应预测区间和学习到的单变量函数的置信区间，运行时间与数据点数量无关。

Result: 该方法实现了拟合Lipschitz GAMs的最小最大最优均方误差，达到O(pn^{-2/3})的收敛速率，成功避免了维度灾难。同时提供了预测区间和置信区间的快速计算方法。

Conclusion: 该研究为可解释提升机提供了一种高效的统计推断框架，不仅实现了理论保证，还增强了EBMs的可解释性，使特征重要性评估更加可靠。

Abstract: Explainable boosting machines (EBMs) are popular "glass-box" models that learn a set of univariate functions using boosting trees. These achieve explainability through visualizations of each feature's effect. However, unlike linear model coefficients, uncertainty quantification for the learned univariate functions requires computationally intensive bootstrapping, making it hard to know which features truly matter. We provide an alternative using recent advances in statistical inference for gradient boosting, deriving methods for statistical inference as well as end-to-end theoretical guarantees. Using a moving average instead of a sum of trees (Boulevard regularization) allows the boosting process to converge to a feature-wise kernel ridge regression. This produces asymptotically normal predictions that achieve the minimax-optimal mean squared error for fitting Lipschitz GAMs with $p$ features at rate $O(pn^{-2/3})$, successfully avoiding the curse of dimensionality. We then construct prediction intervals for the response and confidence intervals for each learned univariate function with a runtime independent of the number of datapoints, enabling further explainability within EBMs.

</details>


### [18] [Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts](https://arxiv.org/abs/2601.19811)
*TrungKhang Tran,TrungTin Nguyen,Gersende Fort,Tung Doan,Hien Duy Nguyen,Binh T. Nguyen,Florence Forbes,Christopher Drovandi*

Main category: stat.ML

TL;DR: 本文提出了一种增量随机Majorization-Minimization算法，该算法推广了增量随机EM算法，放宽了EM的关键要求，具有更广泛的适用性和算法灵活性，在软最大门控混合专家回归问题上表现优于主流随机优化器。


<details>
  <summary>Details</summary>
Motivation: 处理高容量流式数据在现代统计和机器学习中越来越常见，批量模式算法通常不实用，因为它们需要对完整数据集进行重复遍历。这促使了增量随机估计方法的发展，包括通过随机逼近公式化的增量随机EM算法。然而，EM算法需要显式的潜变量表示等要求限制了其应用范围。

Method: 本文重新审视并分析了一种增量随机Majorization-Minimization算法的变体，该算法将增量随机EM作为特例进行推广。该方法放宽了EM的关键要求，如显式潜变量表示，从而实现了更广泛的适用性和更大的算法灵活性。作者建立了增量随机MM算法的理论保证，证明迭代收敛到目标函数梯度为零的驻点。

Result: 在软最大门控混合专家回归问题上，该方法持续优于广泛使用的随机优化器，包括随机梯度下降、RMSprop、Adam和二阶裁剪随机优化。在合成实验之外，还在两个真实世界数据集上验证了实际有效性，包括一个整合高维蛋白质组学与生态生理特征的干旱胁迫下玉米基因型的生物信息学研究，增量随机MM在预测性能上产生了稳定的增益。

Conclusion: 增量随机MM算法为开发新的增量随机算法提供了支持，考虑到软最大门控混合专家架构在当代深度神经网络中用于异质数据建模的核心作用。该方法在理论和实证上都表现出优越性，为流式数据处理提供了更灵活有效的解决方案。

Abstract: Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance.

</details>


### [19] [Implicit Q-Learning and SARSA: Liberating Policy Control from Step-Size Calibration](https://arxiv.org/abs/2601.18907)
*Hwanwoo Kim,Eric Laber*

Main category: stat.ML

TL;DR: 提出Q-learning和SARSA的隐式变体，通过固定点方程重新表述迭代更新，实现自适应步长调整，减少对步长选择的敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统Q-learning和SARSA算法的实际成功严重依赖于步长校准。步长过大会导致数值不稳定，步长过小则收敛缓慢。需要一种能自动调整步长的方法来减少手动调参的需求。

Method: 将Q-learning和SARSA的迭代更新重新表述为固定点方程，创建隐式变体。这种方法通过特征范数的倒数自动调整步长，提供自适应正则化，无需手动调参。

Result: 非渐近分析表明隐式方法在更广泛的步长范围内保持稳定性。在有利条件下，允许任意大的步长同时保持相当的收敛速度。在离散和连续状态空间的基准环境中，隐式Q-learning和SARSA对步长选择的敏感性显著降低，在传统方法会失败的步长下仍能保持稳定性能。

Conclusion: 隐式Q-learning和SARSA通过固定点方程重新表述，实现了自适应步长调整，显著减少了步长选择的敏感性，在更广泛的步长范围内保持稳定性和收敛性，为强化学习算法提供了更鲁棒的实现方案。

Abstract: Q-learning and SARSA are foundational reinforcement learning algorithms whose practical success depends critically on step-size calibration. Step-sizes that are too large can cause numerical instability, while step-sizes that are too small can lead to slow progress. We propose implicit variants of Q-learning and SARSA that reformulate their iterative updates as fixed-point equations. This yields an adaptive step-size adjustment that scales inversely with feature norms, providing automatic regularization without manual tuning. Our non-asymptotic analyses demonstrate that implicit methods maintain stability over significantly broader step-size ranges. Under favorable conditions, it permits arbitrarily large step-sizes while achieving comparable convergence rates. Empirical validation across benchmark environments spanning discrete and continuous state spaces shows that implicit Q-learning and SARSA exhibit substantially reduced sensitivity to step-size selection, achieving stable performance with step-sizes that would cause standard methods to fail.

</details>


### [20] [Collaborative Compressors in Distributed Mean Estimation with Limited Communication Budget](https://arxiv.org/abs/2601.18950)
*Harsh Vardhan,Arya Mazumdar*

Main category: stat.ML

TL;DR: 提出四种无需预知相关性的分布式高维均值估计协作压缩方案，在通信受限环境下利用向量相似性实现高效压缩


<details>
  <summary>Details</summary>
Motivation: 现有相关感知压缩方案需要预知向量相关性，且理论分析仅限于ℓ₂误差。需要开发无需相关性先验知识、能优雅处理向量差异、且能分析多种误差指标的协作压缩方案。

Method: 提出四种简单易实现、计算高效的协作压缩方案：1）基于向量相似性的分布式压缩；2）无需预知相关性的自适应压缩；3）利用向量邻近性的编码策略；4）处理向量差异的鲁棒压缩方法。这些方案能自适应地利用向量间的相似性。

Result: 方案在通信方面实现显著节省，理论分析显示ℓ₂、ℓ∞和余弦估计误差随向量相似度变化而优雅退化，提供了比现有方法更全面的误差分析框架。

Conclusion: 提出的四种协作压缩方案能有效利用向量相似性而无需预知相关性，在通信受限的分布式环境中实现高效压缩，并为多种误差指标提供了理论分析基础。

Abstract: Distributed high dimensional mean estimation is a common aggregation routine used often in distributed optimization methods. Most of these applications call for a communication-constrained setting where vectors, whose mean is to be estimated, have to be compressed before sharing. One could independently encode and decode these to achieve compression, but that overlooks the fact that these vectors are often close to each other. To exploit these similarities, recently Suresh et al., 2022, Jhunjhunwala et al., 2021, Jiang et al, 2023, proposed multiple correlation-aware compression schemes. However, in most cases, the correlations have to be known for these schemes to work. Moreover, a theoretical analysis of graceful degradation of these correlation-aware compression schemes with increasing dissimilarity is limited to only the $\ell_2$-error in the literature. In this paper, we propose four different collaborative compression schemes that agnostically exploit the similarities among vectors in a distributed setting. Our schemes are all simple to implement and computationally efficient, while resulting in big savings in communication. The analysis of our proposed schemes show how the $\ell_2$, $\ell_\infty$ and cosine estimation error varies with the degree of similarity among vectors.

</details>


### [21] [Convergence of Muon with Newton-Schulz](https://arxiv.org/abs/2601.19156)
*Gyu Yeol Kim,Min-hwan Oh*

Main category: stat.ML

TL;DR: 论文分析了Muon优化器中牛顿-舒尔茨正交化步骤的理论性能，证明其收敛速度与理想SVD正交化相当，且常数因子随迭代次数双指数收敛到1，解释了Muon在实践中优于向量优化器的原因。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在实践中使用牛顿-舒尔茨步骤进行动量正交化，但先前理论分析将其替换为精确的SVD正交化。需要弥合这一实践与理论之间的差距，证明实际使用的牛顿-舒尔茨方法的理论性能。

Method: 理论分析Muon优化器使用牛顿-舒尔茨步骤进行动量正交化的收敛性。证明其收敛到平稳点的速率与理想SVD正交化相同（仅差常数因子），并分析该常数因子随牛顿-舒尔茨迭代次数和多项式阶数的收敛特性。

Result: 1) Muon+牛顿-舒尔茨以与SVD正交化相同的速率收敛到平稳点（仅差常数因子）；2) 常数因子随牛顿-舒尔茨迭代次数q双指数收敛到1；3) 多项式阶数越高，常数因子改善越大；4) Muon避免了向量优化器典型的平方根秩损失。

Conclusion: 理论证明了Muon优化器实际使用的牛顿-舒尔茨正交化设计是合理的，其性能与理想SVD正交化相当，且计算效率更高，缩小了实践与理论之间的差距，解释了Muon在实践中优于向量优化器的原因。

Abstract: We analyze Muon as originally proposed and used in practice -- using the momentum orthogonalization with a few Newton-Schulz steps. The prior theoretical results replace this key step in Muon with an exact SVD-based polar factor. We prove that Muon with Newton-Schulz converges to a stationary point at the same rate as the SVD-polar idealization, up to a constant factor for a given number $q$ of Newton-Schulz steps. We further analyze this constant factor and prove that it converges to 1 doubly exponentially in $q$ and improves with the degree of the polynomial used in Newton-Schulz for approximating the orthogonalization direction. We also prove that Muon removes the typical square-root-of-rank loss compared to its vector-based counterpart, SGD with momentum. Our results explain why Muon with a few low-degree Newton-Schulz steps matches exact-polar (SVD) behavior at a much faster wall-clock time and explain how much momentum matrix orthogonalization via Newton-Schulz benefits over the vector-based optimizer. Overall, our theory justifies the practical Newton-Schulz design of Muon, narrowing its practice-theory gap.

</details>


### [22] [Double Fairness Policy Learning: Integrating Action Fairness and Outcome Fairness in Decision-making](https://arxiv.org/abs/2601.19186)
*Zeyu Bian,Lan Wang,Chengchun Shi,Zhengling Qi*

Main category: stat.ML

TL;DR: 提出双公平学习框架，在策略学习中同时优化行动公平性、结果公平性和价值最大化三个目标，通过多目标优化方法平衡三者权衡。


<details>
  <summary>Details</summary>
Motivation: 当前公平性研究主要集中在监督学习，策略学习中的公平性研究较少。策略学习具有干预性，会产生行动公平性和结果公平性两个不同目标，但平等分配行动通常无法保证平等结果，因为不同群体面临不同约束或对相同行动反应不同。

Method: 提出双公平学习框架，将公平性直接整合到策略学习的多目标优化问题中，采用字典序加权切比雪夫方法，能在非凸设置下恢复帕累托解，并提供遗憾界限的理论保证。框架灵活，可容纳多种常用公平性概念。

Result: 广泛模拟显示相对于竞争方法有改进性能。在机动车第三方责任保险数据集和创业培训数据集的应用中，DFL显著改善了行动和结果公平性，同时仅导致总体价值的适度减少。

Conclusion: 双公平学习框架能够有效管理策略学习中行动公平性、结果公平性和价值最大化之间的权衡，为实际应用中的公平策略学习提供了灵活且理论保证的解决方案。

Abstract: Fairness is a central pillar of trustworthy machine learning, especially in domains where accuracy- or profit-driven optimization is insufficient. While most fairness research focuses on supervised learning, fairness in policy learning remains less explored. Because policy learning is interventional, it induces two distinct fairness targets: action fairness (equitable action assignments) and outcome fairness (equitable downstream consequences). Crucially, equalizing actions does not generally equalize outcomes when groups face different constraints or respond differently to the same action. We propose a novel double fairness learning (DFL) framework that explicitly manages the trade-off among three objectives: action fairness, outcome fairness, and value maximization. We integrate fairness directly into a multi-objective optimization problem for policy learning and employ a lexicographic weighted Tchebyshev method that recovers Pareto solutions beyond convex settings, with theoretical guarantees on the regret bounds. Our framework is flexible and accommodates various commonly used fairness notions. Extensive simulations demonstrate improved performance relative to competing methods. In applications to a motor third-party liability insurance dataset and an entrepreneurship training dataset, DFL substantially improves both action and outcome fairness while incurring only a modest reduction in overall value.

</details>


### [23] [Regularized $f$-Divergence Kernel Tests](https://arxiv.org/abs/2601.19755)
*Mónica Ribero,Antonin Schrab,Arthur Gretton*

Main category: stat.ML

TL;DR: 提出基于f-散度的核方法两样本检验框架，特别关注Hockey-Stick散度，应用于差分隐私审计和机器遗忘评估


<details>
  <summary>Details</summary>
Motivation: 现有两样本检验方法可能无法捕捉不同类型的分布差异，需要一种灵活框架来适应不同f-散度，特别针对差分隐私审计和机器遗忘评估的实际应用需求

Method: 通过正则化变分表示构建f-散度的核方法估计，使用见证函数计算检验统计量，框架自适应调整核带宽和正则化参数等超参数

Result: 理论保证了统计检验功效，实验显示不同f-散度对局部差异敏感度不同，验证了利用多样统计量的重要性；在机器遗忘中提出相对检验区分真实遗忘失败和安全分布变化

Conclusion: 提出的框架为两样本检验提供了灵活实用的核方法，特别适用于差分隐私审计和机器遗忘评估，展示了f-散度多样性在实际应用中的重要性

Abstract: We propose a framework to construct practical kernel-based two-sample tests from the family of $f$-divergences. The test statistic is computed from the witness function of a regularized variational representation of the divergence, which we estimate using kernel methods. The proposed test is adaptive over hyperparameters such as the kernel bandwidth and the regularization parameter. We provide theoretical guarantees for statistical test power across our family of $f$-divergence estimates. While our test covers a variety of $f$-divergences, we bring particular focus to the Hockey-Stick divergence, motivated by its applications to differential privacy auditing and machine unlearning evaluation. For two-sample testing, experiments demonstrate that different $f$-divergences are sensitive to different localized differences, illustrating the importance of leveraging diverse statistics. For machine unlearning, we propose a relative test that distinguishes true unlearning failures from safe distributional variations.

</details>
