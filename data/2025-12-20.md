<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 17]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.AP](#stat.AP) [Total: 4]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Empirical Likelihood Meets Prediction-Powered Inference](https://arxiv.org/abs/2512.16363)
*Guanghui Wang,Mengtao Wen,Changliang Zou*

Main category: stat.ME

TL;DR: 提出基于经验似然的预测增强推断方法，通过结合标记数据的监督估计方程和预测的辅助矩条件，在小标记样本、大未标记样本和外部模型预测下进行高效推断。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，我们经常面临只有少量标记数据但有大量未标记数据的情况，同时可能获得外部模型的高质量预测。如何有效整合这些信息进行统计推断是一个重要问题。

Method: 将预测增强推断与经验似然相结合，通过堆叠基于标记结果的监督估计方程和基于预测构建的辅助矩条件，在这些联合约束下优化经验似然。提出两种实现：基于预测和协变量基展开的方法，以及通过交叉拟合学习近似最优辅助函数的方法。

Result: EPI估计量是渐近正态的，渐近方差不大于完全监督估计量，当辅助函数张成监督得分的可预测分量时达到半参数效率界。经验似然比统计量具有卡方型极限分布，权重诱导的校准经验分布能够整合监督和预测信息。

Conclusion: EPI方法在模拟和应用中能够减少均方误差、缩短置信区间同时保持名义覆盖水平，为小标记样本、大未标记样本和外部预测场景下的统计推断提供了有效工具。

Abstract: We study inference with a small labeled sample, a large unlabeled sample, and high-quality predictions from an external model. We link prediction-powered inference with empirical likelihood by stacking supervised estimating equations based on labeled outcomes with auxiliary moment conditions built from predictions, and then optimizing empirical likelihood under these joint constraints. The resulting empirical likelihood-based prediction-powered inference (EPI) estimator is asymptotically normal, has asymptotic variance no larger than the fully supervised estimator, and attains the semiparametric efficiency bound when the auxiliary functions span the predictable component of the supervised score. For hypothesis testing and confidence sets, empirical likelihood ratio statistics admit chi-squared-type limiting distributions. As a by-product, the empirical likelihood weights induce a calibrated empirical distribution that integrates supervised and prediction-based information, enabling estimation and uncertainty quantification for general functionals beyond parameters defined by estimating equations. We present two practical implementations: one based on basis expansions in the predictions and covariates, and one that learns an approximately optimal auxiliary function by cross-fitting. In simulations and applications, EPI reduces mean squared error and shortens confidence intervals while maintaining nominal coverage.

</details>


### [2] [Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection](https://arxiv.org/abs/2512.16411)
*Matthieu Garcin,Louis Perot*

Main category: stat.ME

TL;DR: 本文研究相对熵在离线变点检测中的应用，推导了经验相对熵的分布近似，包括浓度不等式、渐近分布和Berry-Esseen界，并提出了基于相对熵的变点检测方法。


<details>
  <summary>Details</summary>
Motivation: 传统变点检测方法主要依赖矩基差异，相对熵作为分布间的散度度量可以扩展这些方法。需要建立适合离线变点检测的统计检验，研究经验相对熵的分布特性。

Method: 研究经验相对熵的分布：推导浓度不等式、渐近分布，并在凸性假设下为和统计量的非线性函数建立Berry-Esseen界。涵盖单样本和双样本经验相对熵。基于相对熵构建变点检测流程。

Result: 提出了相对熵变点检测方法，通过大量模拟与基于矩或信息准则的经典方法比较。在两个真实数据集（温度序列和股票指数波动率）上展示了实用价值。

Conclusion: 相对熵为变点检测提供了有效的分布差异度量，理论推导为该方法提供了统计基础，实证结果表明其在真实数据上的实用性和优于传统方法的潜力。

Abstract: Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.

</details>


### [3] [Consensus dimension reduction via multi-view learning](https://arxiv.org/abs/2512.15802)
*Bingxue An,Tiffany M. Tang*

Main category: stat.ME

TL;DR: 提出一种共识降维可视化方法，通过整合多种降维方法的输出，提取最稳定的共享模式，提高可视化结果的鲁棒性和可信度。


<details>
  <summary>Details</summary>
Motivation: 不同降维方法对同一数据会产生不同甚至冲突的可视化结果，且超参数选择对结果影响很大，这降低了降维结果的可信度和可重复性。需要一种更稳健、可信的降维可视化方法。

Method: 采用多视图学习思想，从多个不同降维可视化结果（视图）中识别最稳定或共享的模式，然后将这些共享结构整合到单一的低维可视化图中。

Result: 共识可视化方法能有效识别和保留共享的低维数据结构，在模拟和真实案例研究中都表现出色。该方法对降维方法和超参数的选择具有鲁棒性。

Conclusion: 共识降维可视化方法提高了降维结果的可信度和可重复性，是实现可信赖和可重复数据科学的重要工具，对降维方法和超参数选择具有鲁棒性。

Abstract: A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science.

</details>


### [4] [Modeling Issues with Eye Tracking Data](https://arxiv.org/abs/2512.15950)
*Gregory Camilli*

Main category: stat.ME

TL;DR: 本文比较了四种处理眼动追踪二值数据的统计方法，包括基本GLMM模型和三种处理序列自相关的方法，揭示了眼动数据分析中未解决的问题和发展方向。


<details>
  <summary>Details</summary>
Motivation: 眼动追踪数据具有序列自相关特性，传统分析方法可能无法充分处理这种时间依赖性。本文旨在比较不同的统计方法，特别是针对二值眼动数据，以解决序列自相关问题并探索更有效的分析途径。

Method: 1. 基本GLMM模型：逻辑混合模型结合人和项目的随机效应
2. 处理自相关的方法：包括使用滞后预测变量的模型
3. 两种新方法：不使用滞后预测变量的广义估计方程一阶自回归模型，以及循环两状态生存模型
4. 应用于原始和压缩数据

Result: 四种不同分析的结果表明，眼动追踪数据分析仍存在未解决的问题。新提出的方法为处理序列自相关提供了替代方案，但需要进一步验证和完善。

Conclusion: 眼动追踪数据的序列自相关分析需要更先进的方法。本文提出的两种新方法为这一领域提供了新的方向，但仍有待进一步研究和发展。

Abstract: I describe and compare procedures for binary eye-tracking (ET) data. These procedures are applied to both raw and compressed data. The basic GLMM model is a logistic mixed model combined with random effects for persons and items. Additional models address autocorrelation eye-tracking serial observations. In particular, two novel approaches are illustrated that address serial without the use of an observed lag-1 predictor: a first-order autoregressive model obtained with generalized estimating equations, and a recurrent two-state survival model. Altogether, the results of four different analyses point to unresolved issues in the analysis of eye-tracking data and new directions for analytic development.

</details>


### [5] [Reliability-Targeted Simulation of Item Response Data: Solving the Inverse Design Problem](https://arxiv.org/abs/2512.16012)
*JoonHo Lee*

Main category: stat.ME

TL;DR: 提出一个针对IRT模拟的可靠性目标框架，将可靠性从隐含结果转变为精确输入参数，通过两种算法实现预定义可靠性校准。


<details>
  <summary>Details</summary>
Motivation: 当前IRT模拟研究中，边际可靠性作为数据信息量的基本度量，很少被作为明确的设计因素处理，而是被当作偶然结果，这造成了"可靠性遗漏"问题，模糊了生成数据的信噪比。

Method: 引入可靠性目标模拟的原则框架，形式化逆设计问题，求解全局区分度缩放因子以实现预定义目标可靠性。提出两种互补算法：经验正交校准（EQC）用于快速确定性精确校准，随机近似校准（SAC）用于严格随机估计。

Result: 在960种条件下的综合验证研究表明，EQC实现了基本精确的校准，而SAC在非正态潜在分布和实证项目池中保持无偏。同时阐明了平均信息与误差方差可靠性度量之间的理论区别。

Conclusion: 该框架将可靠性从隐含副产品转变为精确输入参数，解决了IRT模拟中的可靠性遗漏问题，并提供了开源R包IRTsimrel，使研究人员能够将可靠性标准化为受控实验输入。

Abstract: Monte Carlo simulations are the primary methodology for evaluating Item Response Theory (IRT) methods, yet marginal reliability - the fundamental metric of data informativeness - is rarely treated as an explicit design factor. Unlike in multilevel modeling where the intraclass correlation (ICC) is routinely manipulated, IRT studies typically treat reliability as an incidental outcome, creating a "reliability omission" that obscures the signal-to-noise ratio of generated data. To address this gap, we introduce a principled framework for reliability-targeted simulation, transforming reliability from an implicit by-product into a precise input parameter. We formalize the inverse design problem, solving for a global discrimination scaling factor that uniquely achieves a pre-specified target reliability. Two complementary algorithms are proposed: Empirical Quadrature Calibration (EQC) for rapid, deterministic precision, and Stochastic Approximation Calibration (SAC) for rigorous stochastic estimation. A comprehensive validation study across 960 conditions demonstrates that EQC achieves essentially exact calibration, while SAC remains unbiased across non-normal latent distributions and empirical item pools. Furthermore, we clarify the theoretical distinction between average-information and error-variance-based reliability metrics, showing they require different calibration scales due to Jensen's inequality. An accompanying open-source R package, IRTsimrel, enables researchers to standardize reliability as a controlled experimental input.

</details>


### [6] [Efficient and scalable clustering of survival curves](https://arxiv.org/abs/2512.16481)
*Nora M. Villanueva,Marta Sestelo,Luis Meira-Machado*

Main category: stat.ME

TL;DR: 提出一种基于k-means和log-rank检验的高效生存曲线聚类方法，无需计算密集的重采样，显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 传统生存曲线聚类方法依赖计算密集的自举技术来近似零假设分布，虽然有效但计算负担重，需要更高效的方法

Method: 结合k-means聚类和log-rank检验，系统评估生存曲线并确定最优聚类，无需计算昂贵的重采样

Result: 模拟研究表明，该方法在保持统计可靠性的同时，计算效率显著优于现有的自举聚类方法

Conclusion: 基于log-rank的聚类程序为医学和流行病学研究中的多生存曲线分析提供了可行且高效的时间解决方案

Abstract: Survival analysis encompasses a broad range of methods for analyzing time-to-event data, with one key objective being the comparison of survival curves across groups. Traditional approaches for identifying clusters of survival curves often rely on computationally intensive bootstrap techniques to approximate the null hypothesis distribution. While effective, these methods impose significant computational burdens. In this work, we propose a novel approach that leverages the k-means and log-rank test to efficiently identify and cluster survival curves. Our method eliminates the need for computationally expensive resampling, significantly reducing processing time while maintaining statistical reliability. By systematically evaluating survival curves and determining optimal clusters, the proposed method ensures a practical and scalable alternative for large-scale survival data analysis. Through simulation studies, we demonstrate that our approach achieves results comparable to existing bootstrap-based clustering methods while dramatically improving computational efficiency. These findings suggest that the log-rank-based clustering procedure offers a viable and time-efficient solution for researchers working with multiple survival curves in medical and epidemiological studies.

</details>


### [7] [Maximum Likelihood Estimation for Scaled Inhomogeneous Phase-Type Distributions from Discrete Observations](https://arxiv.org/abs/2512.16061)
*Fernando Baltazar-Larios,Alejandra Quintos*

Main category: stat.ME

TL;DR: 提出了一种用于时间尺度非齐次相型分布的统计推断框架，结合马尔可夫桥重建与随机EM算法，可从不规则多状态数据中联合估计基线子强度矩阵和时间尺度参数。


<details>
  <summary>Details</summary>
Motivation: 经典相型分布假设转移强度恒定，无法有效建模重尾或时间依赖的吸收现象。非齐次相型分布通过允许转移强度随时间变化提供了更大灵活性，但需要有效的统计推断方法来处理离散观测数据。

Method: 针对时间尺度子强度矩阵形式为Λ(t)=h_β(t)Λ的IPH分布子类，开发了结合马尔可夫桥重建、随机EM算法和梯度更新的统计推断框架。该方法将离散观测轨迹转换为连续时间过程，通过随机EM联合估计基线子强度矩阵Λ和时间尺度参数β。

Result: 通过矩阵-Gompertz和矩阵-Weibull分布的模拟研究，以及冠状动脉同种异体移植物血管病变进展的实际数据应用，证明该方法能够准确且计算可行地拟合时间尺度IPH模型到不规则多状态数据。

Conclusion: 所提出的框架为非齐次相型分布提供了有效的统计推断工具，特别适用于具有时间依赖转移强度的多状态过程建模，在模拟和实际应用中均表现出良好的性能。

Abstract: Inhomogeneous phase-type (IPH) distributions extend classical phase-type models by allowing transition intensities to vary over time, offering greater flexibility for modeling heavy-tailed or time-dependent absorption phenomena. We focus on the subclass of IPH distributions with time-scaled sub-intensity matrices of the form $Λ(t) = h_β(t)Λ$, which admits a time transformation to a homogeneous Markov jump process. For this class, we develop a statistical inference framework for discretely observed trajectories that combines Markov-bridge reconstruction with a stochastic EM algorithm and a gradient-based update. The resulting method yields joint maximum-likelihood estimates of both the baseline sub-intensity matrix $Λ$ and the time-scaling parameter $β$. Through simulation studies for the matrix-Gompertz and matrix-Weibull families, and a real-data application to coronary allograft vasculopathy progression, we demonstrate that the proposed approach provides an accurate and computationally tractable tool for fitting time-scaled IPH models to irregular multi-state data.

</details>


### [8] [An Efficient Framework for Robust Sample Size Determination](https://arxiv.org/abs/2512.16231)
*Luke Hagar,Andrew J. Martin*

Main category: stat.ME

TL;DR: 提出一种经济高效的样本量确定方法，仅需在两个样本量下进行模拟即可评估整个样本量空间的统计功效，使研究设计对多种数据生成机制具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统样本量确定方法依赖大量模拟和单一数据生成过程的假设，导致研究设计的鲁棒性不足。需要一种更经济、能应对多种数据生成机制的样本量确定方法。

Method: 将p值建模为样本量的函数，通过理论推导，仅需在每个数据生成机制下进行两个样本量的模拟，即可评估整个样本量空间的统计功效。

Result: 该方法可广泛应用于基于M估计器的实验和观察性研究设计，通过多个临床实例验证了其广泛适用性。

Conclusion: 提出的方法提供了一种经济高效的替代方案，能够在考虑多种数据生成机制的情况下确定鲁棒的样本量，从而将数据分析的鲁棒性扩展到研究设计中。

Abstract: In many settings, robust data analysis involves computational methods for uncertainty quantification and statistical inference. To design frequentist studies that leverage robust analysis methods, suitable sample sizes to achieve desired power are often found by estimating sampling distributions of p-values via intensive simulation. Moreover, most sample size recommendations rely heavily on assumptions about a single data-generating process. Consequently, robustness in data analysis does not by itself imply robustness in study design, as examining sample size sensitivity to data-generating assumptions typically requires further simulations. We propose an economical alternative for determining sample sizes that are robust to multiple data-generating mechanisms. Applying our theoretical results that model p-values as a function of the sample size, we assess power across the sample size space using simulations conducted at only two sample sizes for each data-generating mechanism. We demonstrate the broad applicability of our methodology to study design based on M-estimators in both experimental and observational settings through a varied set of clinical examples.

</details>


### [9] [Bayesian Empirical Bayes: Simultaneous Inference from Probabilistic Symmetries](https://arxiv.org/abs/2512.16239)
*Bohan Wu,Eli N. Weinstein,David M. Blei*

Main category: stat.ME

TL;DR: 提出基于概率对称性的广义经验贝叶斯方法(BEB)，将经典EB扩展到具有复杂结构的数据，如矩阵、空间过程和协变量数据。


<details>
  <summary>Details</summary>
Motivation: 经典经验贝叶斯理论假设潜变量是独立同分布的先验抽样，但现代应用涉及复杂结构数据（如数组、空间过程、协变量），需要将EB思想扩展到这些场景。

Method: 基于概率对称性概念，将同时推断问题与潜变量联合分布的对称性假设配对，利用遍历分解推导相应的经验贝叶斯方法(BEB)。开发基于变分推断和神经网络的可扩展算法。

Result: BEB恢复了经典交换性假设下的经验贝叶斯方法，并扩展到：1) 数组和图的EB矩阵恢复；2) 条件数据的协变量辅助EB；3) 平移不变性下的EB空间回归。在模拟中优于现有方法，在真实数据上成功应用于癌症基因表达矩阵去噪和纽约市空气质量空间数据分析。

Conclusion: 提出的Bayesian empirical Bayes (BEB)框架通过概率对称性将经验贝叶斯推广到复杂结构数据，为现代统计应用提供了灵活且可扩展的解决方案。

Abstract: Empirical Bayes (EB) improves the accuracy of simultaneous inference "by learning from the experience of others" (Efron, 2012). Classical EB theory focuses on latent variables that are iid draws from a fitted prior (Efron, 2019). Modern applications, however, feature complex structure, like arrays, spatial processes, or covariates. How can we apply EB ideas to these settings? We propose a generalized approach to empirical Bayes based on the notion of probabilistic symmetry. Our method pairs a simultaneous inference problem-with an unknown prior-to a symmetry assumption on the joint distribution of the latent variables. Each symmetry implies an ergodic decomposition, which we use to derive a corresponding empirical Bayes method. We call this methodBayesian empirical Bayes (BEB). We show how BEB recovers the classical methods of empirical Bayes, which implicitly assume exchangeability. We then use it to extend EB to other probabilistic symmetries: (i) EB matrix recovery for arrays and graphs; (ii) covariate-assisted EB for conditional data; (iii) EB spatial regression under shift invariance. We develop scalable algorithms based on variational inference and neural networks. In simulations, BEB outperforms existing approaches to denoising arrays and spatial data. On real data, we demonstrate BEB by denoising a cancer gene-expression matrix and analyzing spatial air-quality data from New York City.

</details>


### [10] [Repulsive g-Priors for Regression Mixtures](https://arxiv.org/abs/2512.16276)
*Yuta Hayashida,Shonosuke Sugasawa*

Main category: stat.ME

TL;DR: 提出了一种用于回归混合模型的排斥性g-先验，通过马氏距离惩罚预测空间中难以区分的分量，提高聚类准确性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统有限混合模型和贝叶斯非参数方法在分量可分性较弱时存在不稳定或过度估计聚类的问题。现有排斥性先验在密度混合中有效，但直接扩展到回归模型存在困难，因为分离需要考虑协变量诱导的预测几何结构。

Method: 提出排斥性g-先验用于回归混合模型，基于马氏距离强制分量分离，惩罚预测均值空间中难以区分的分量。该方法保持类共轭更新，引入几何感知的交互作用，实现高效的块崩溃吉布斯采样。

Result: 理论分析得到可处理的归一化边界、后验收缩率和分量数量尾部质量的收缩。在相关和重叠设计下的模拟实验显示，相对于独立、欧几里得排斥性和稀疏性诱导基线，该方法在聚类和预测方面有显著改进。

Conclusion: 提出的排斥性g-先验为回归混合模型提供了一种几何感知的排斥机制，能有效处理分量重叠情况，提高模型稳定性和预测准确性，同时保持计算效率。

Abstract: Mixture regression models are powerful tools for capturing heterogeneous covariate-response relationships, yet classical finite mixtures and Bayesian nonparametric alternatives often suffer from instability or overestimation of clusters when component separability is weak. Recent repulsive priors improve parsimony in density mixtures by discouraging nearby components, but their direct extension to regression is nontrivial since separation must respect the predictive geometry induced by covariates. We propose a repulsive g-prior for regression mixtures that enforces separation in the Mahalanobis metric, penalizing components indistinguishable in the predictive mean space. This construction preserves conjugacy-like updates while introducing geometry-aware interactions, enabling efficient blocked-collapsed Gibbs sampling. Theoretically, we establish tractable normalizing bounds, posterior contraction rates, and shrinkage of tail mass on the number of components. Simulations under correlated and overlapping designs demonstrate improved clustering and prediction relative to independent, Euclidean-repulsive, and sparsity-inducing baselines.

</details>


### [11] [Hazard-based distributional regression via ordinary differential equations](https://arxiv.org/abs/2512.16336)
*J. A. Christen,F. J. Rubio*

Main category: stat.ME

TL;DR: 提出使用常微分方程系统建模风险函数的新参数生存回归框架，能捕捉复杂风险形状并保持参数可解释性


<details>
  <summary>Details</summary>
Motivation: 传统生存回归模型（如比例风险模型和加速失效时间模型）依赖于共享基线风险函数，当参数化时只能捕捉有限形状，限制了模型的灵活性

Method: 使用自治常微分方程系统建模风险函数，通过变换线性预测器将协变量信息纳入ODE参数，开发高效贝叶斯计算工具和并行化对数后验评估

Result: 方法能识别产生不同风险形状的协变量值，在临床试验数据和癌症复发研究中揭示了治疗干预效果如何受患者特征影响

Conclusion: 提出的ODE框架为生存分析提供了灵活且可解释的参数模型，能捕捉复杂风险动态并深入理解协变量对生存模式的影响

Abstract: The hazard function is central to the formulation of commonly used survival regression models such as the proportional hazards and accelerated failure time models. However, these models rely on a shared baseline hazard, which, when specified parametrically, can only capture limited shapes. To overcome this limitation, we propose a general class of parametric survival regression models obtained by modelling the hazard function using autonomous systems of ordinary differential equations (ODEs). Covariate information is incorporated via transformed linear predictors on the parameters of the ODE system. Our framework capitalises on the interpretability of parameters in common ODE systems, enabling the identification of covariate values that produce qualitatively distinct hazard shapes associated with different attractors of the system of ODEs. This provides deeper insights into how covariates influence survival dynamics. We develop efficient Bayesian computational tools, including parallelised evaluation of the log-posterior, which facilitates integration with general-purpose Markov Chain Monte Carlo samplers. We also derive conditions for posterior asymptotic normality, enabling fast approximations of the posterior. A central contribution of our work lies in the case studies. We demonstrate the methodology using clinical trial data with crossing survival curves, and a study of cancer recurrence times where our approach reveals how the efficacy of interventions (treatments) on hazard and survival are influenced by patient characteristics.

</details>


### [12] [Bayesian joint modelling of longitudinal biomarkers to enable extrapolation of overall survival: an application using larotrectinib trial clinical data](https://arxiv.org/abs/2512.16340)
*Louise Linsell,Noman Paracha,Jamie Grossman,Carsten Bokemeyer,Jesus Garcia-Foncillas,Antoine Italiano,Gilles Vassal,Yuxian Chen,Barbara Torlinska,Keith R Abrams*

Main category: stat.ME

TL;DR: 使用贝叶斯联合建模方法，通过中间生物标志物（肿瘤病灶直径总和）从不成熟的临床试验数据中预测总生存期，并与传统参数外推法及后期数据截断的观察生存期进行比较。


<details>
  <summary>Details</summary>
Motivation: 在临床试验随访数据有限的情况下，需要开发更准确的方法来预测总生存期。传统生存模型可能无法充分利用中间生物标志物信息，特别是在数据不成熟时。

Method: 汇集三项larotrectinib治疗NTRK+实体瘤的I/II期试验数据（196例患者）。使用贝叶斯联合建模方法，基于个体水平的肿瘤病灶直径总和（SLD）轨迹预测患者特异性总生存期。考虑不同肿瘤部位间的关联结构（共同、可交换或独立）。

Result: 肿瘤病灶直径总和每增加10mm，死亡风险增加9%（HR 1.09，95% CrI 1.05-1.14）。较大肿瘤组的生存估计在不同模型间更相似。与标准Weibull模型相比，参数估计更确定，且与更近期的数据截断结果一致。

Conclusion: 使用肿瘤负荷等中间结果的联合建模为传统生存建模提供了替代方法，可能改善有限随访数据的生存预测。该方法能处理复杂的分层数据结构，并可在多变量建模框架中纳入多个纵向生物标志物。

Abstract: Objectives To investigate the use of a Bayesian joint modelling approach to predict overall survival (OS) from immature clinical trial data using an intermediate biomarker. To compare the results with a typical parametric approach of extrapolation and observed survival from a later datacut.
  Methods Data were pooled from three phase I/II open-label trials evaluating larotrectinib in 196 patients with neurotrophic tyrosine receptor kinase fusion-positive (NTRK+) solid tumours followed up until July 2021. Bayesian joint modelling was used to obtain patient-specific predictions of OS using individual-level sum of diameter of target lesions (SLD) profiles up to the time at which the patient died or was censored. Overall and tumour site-specific estimates were produced, assuming a common, exchangeable, or independent association structure across tumour sites.
  Results The overall risk of mortality was 9% higher per 10mm increase in SLD (HR 1.09, 95% CrI 1.05 to 1.14) for all tumour sites combined. Tumour-specific point estimates of restricted mean , median and landmark survival were more similar across models for larger tumour groups, compared to smaller tumour groups. In general, parameters were estimated with more certainty compared to a standard Weibull model and were aligned with the more recent datacut.
  Conclusions Joint modelling using intermediate outcomes such as tumour burden can offer an alternative approach to traditional survival modelling and may improve survival predictions from limited follow-up data. This approach allows complex hierarchical data structures, such as patients nested within tumour types, and can also incorporate multiple longitudinal biomarkers in a multivariate modelling framework.

</details>


### [13] [Extending a Matrix Lie Group Model of Measurement Symmetries](https://arxiv.org/abs/2512.16547)
*William R. Nugent*

Main category: stat.ME

TL;DR: 本文基于李群理论扩展了测量对称性框架，证明了对称性在测量理论中的核心作用，特别展示了标准化均值差(SMD)仅在特定对称条件下保持跨测量不变性。


<details>
  <summary>Details</summary>
Motivation: 对称性原则是科学理论的基础，从居里的不变性表述到现代物理、化学和数学应用。在测量理论中，需要识别李群理论所隐含的额外测量对称性，以理解测量不变性对效度、可比性和信息守恒的影响。

Method: 基于最近提出的矩阵李群测量模型，扩展框架以识别李群理论隐含的测量对称性。李群提供连续对称性的数学，李代数作为其无穷小生成元。通过模拟研究展示测量对称性破坏如何影响分数分布对称性和效应量可比性。

Result: 研究证明标准化均值差(SMD)仅在特定对称条件下保持跨测量不变性，这些条件源于李群模型。模拟显示测量对称性破坏会影响分数分布对称性并破坏效应量可比性。

Conclusion: 对称性是测量理论中的统一原则，对基于证据的研究具有重要作用。李群框架为理解测量不变性提供了数学基础，特别在元分析等应用中，对称性条件对效应量可比性至关重要。

Abstract: Symmetry principles underlie and guide scientific theory and research, from Curie's invariance formulation to modern applications across physics, chemistry, and mathematics. Building on a recent matrix Lie group measurement model, this paper extends the framework to identify additional measurement symmetries implied by Lie group theory. Lie groups provide the mathematics of continuous symmetries, while Lie algebras serve as their infinitesimal generators. Within applied measurement theory, the preservation of symmetries in transformation groups acting on score frequency distributions ensure invariance in transformed distributions, with implications for validity, comparability, and conservation of information. A simulation study demonstrates how breaks in measurement symmetry affect score distribution symmetry and break effect size comparability. Practical applications are considered, particularly in meta analysis, where the standardized mean difference (SMD) is shown to remain invariant across measures only under specific symmetry conditions derived from the Lie group model. These results underscore symmetry as a unifying principle in measurement theory and its role in evidence based research.

</details>


### [14] [Exponentially weighted estimands and the exponential family: filtering, prediction and smoothing](https://arxiv.org/abs/2512.16745)
*Simon Donker van Heel,Neil Shephard*

Main category: stat.ME

TL;DR: 提出一种基于对数似然与期望对数似然凸组合的折扣方法，用于时间序列的滤波、预测和平滑，并在指数族模型中实现线性递归的精确算法


<details>
  <summary>Details</summary>
Motivation: 为时间序列分析开发一种统一的框架，能够同时得到滤波、预测和平滑的精确解，特别是在指数族模型中实现简单高效的线性递归算法

Method: 使用对数似然与期望对数似然的凸组合折扣方法，在指数族模型下推导出线性递归的精确滤波、预测和平滑算法

Result: 得到了指数族时间序列模型的简单精确滤波器、预测器和平滑器，具有线性递归形式，并在模拟和真实数据上验证了模型效果

Conclusion: 提出的折扣凸组合方法为指数族时间序列模型提供了统一的精确解框架，实现了高效的线性递归算法，具有良好的理论和实际应用价值

Abstract: We propose using a discounted version of a convex combination of the log-likelihood with the corresponding expected log-likelihood such that when they are maximized they yield a filter, predictor and smoother for time series. This paper then focuses on working out the implications of this in the case of the canonical exponential family. The results are simple exact filters, predictors and smoothers with linear recursions. A theory for these models is developed and the models are illustrated on simulated and real data.

</details>


### [15] [Shift-Aware Gaussian-Supremum Validation for Wasserstein-DRO CVaR Portfolios](https://arxiv.org/abs/2512.16748)
*Derek Long*

Main category: stat.ME

TL;DR: 提出一个用于CVaR投资组合选择的Wasserstein分布鲁棒优化框架，通过两阶段验证方法处理分布漂移和序列依赖问题


<details>
  <summary>Details</summary>
Motivation: 传统Wasserstein分布鲁棒优化中半径选择困难：半径过大导致保守，过小则在制度变化下可能违反约束。需要一种能处理分布漂移和序列依赖的验证框架

Method: 两阶段框架：第一阶段通过网格搜索生成候选路径；第二阶段使用密度比重加权验证集，计算加权CVaR估计，并通过块乘数自举法构建同时置信带

Result: 理论上证明在弱依赖条件下，通过验证的投资组合能以至少1-β的概率满足CVaR限制；实证结果显示相比基线方法有更好的收益-风险权衡

Conclusion: 提出的shift-aware高斯上确界验证框架能有效处理分布漂移和序列依赖，为CVaR约束的投资组合选择提供可靠的Wasserstein半径校准方法

Abstract: We study portfolio selection with a Conditional Value-at-Risk (CVaR) constraint under distribution shift and serial dependence. While Wasserstein distributionally robust optimization (DRO) offers tractable protection via an ambiguity ball around empirical data, choosing the ball radius is delicate: large radii are conservative, small radii risk violation under regime change. We propose a shift-aware Gaussian-supremum (GS) validation framework for Wasserstein-DRO CVaR portfolios, building on the work by Lam and Qian (2019). Phase I of the framework generates a candidate path by solving the exact reformulation of the robust CVaR constraint over a grid of Wasserstein radii. Phase II of the framework learns a target deployment law $Q$ by density-ratio reweighting of a time-ordered validation fold, computes weighted CVaR estimates, and calibrates a simultaneous upper confidence band via a block multiplier bootstrap to account for dependence. We select the least conservative feasible portfolio (or abstain if the effective sample size collapses). Theoretically, we extend the normalized GS validator to non-i.i.d. financial data: under weak dependence and regularity of the weighted scores, any portfolio passing our validator satisfies the CVaR limit under $Q$ with probability at least $1-β$; the Wasserstein term contributes a deterministic margin $(δ/α)\|x\|_*$. Empirical results indicate improved return-risk trade-offs versus the naive baseline.

</details>


### [16] [Distributed inference for heterogeneous mixture models using multi-site data](https://arxiv.org/abs/2512.16833)
*Xiaokang Liu,Rui Duan,Raymond J. Carroll,Yang Ning,Yong Chen*

Main category: stat.ME

TL;DR: 提出一个分布式EM算法用于多站点混合模型拟合，在保护数据隐私的同时处理站点异质性


<details>
  <summary>Details</summary>
Motivation: 混合模型拟合通常需要大量样本，多站点数据合并有益但受限于数据隐私和站点异质性。现有方法难以在保护隐私的同时处理跨站点可比性和异质性。

Method: 提出统一建模框架：跨站点共享潜在类定义，允许异质混合比例。开发分布式EM算法，通过密度比倾斜的代理Q函数近似标准EM的Q函数，无需共享原始数据。

Result: 理论分析表明，该估计器能达到与基于合并数据的EM算法相同的收缩性质，在保护隐私的同时有效处理站点异质性。

Conclusion: 该方法为多站点混合模型分析提供了隐私保护且能处理异质性的解决方案，在理论和实际应用上都有重要意义。

Abstract: Mixture models postulate the overall population as a mixture of finite subpopulations with unobserved membership. Fitting mixture models usually requires large sample sizes and combining data from multiple sites can be beneficial. However, sharing individual participant data across sites is often less feasible due to various types of practical constraints, such as data privacy concerns. Moreover, substantial heterogeneity may exist across sites, and locally identified latent classes may not be comparable across sites. We propose a unified modeling framework where a common definition of the latent classes is shared across sites and heterogeneous mixing proportions of latent classes are allowed to account for between-site heterogeneity. To fit the heterogeneous mixture model on multi-site data, we propose a novel distributed Expectation-Maximization (EM) algorithm where at each iteration a density ratio tilted surrogate Q function is constructed to approximate the standard Q function of the EM algorithm as if the data from multiple sites could be pooled together. Theoretical analysis shows that our estimator achieves the same contraction property as the estimators derived from the EM algorithm based on the pooled data.

</details>


### [17] [Identification and efficient estimation of compliance and network causal effects in cluster-randomized trials](https://arxiv.org/abs/2512.16857)
*Chao Cheng,Georgia Papadogeorgou,Fan Li*

Main category: stat.ME

TL;DR: 提出半参数框架评估传染病群随机试验中的个体依从效应和网络分配效应，通过结构假设实现非参数点识别，开发半参数有效估计器，并应用于肯尼亚除虫试验分析。


<details>
  <summary>Details</summary>
Motivation: 传染病群随机试验中普遍存在治疗不依从问题，虽然同一群组内个体被分配相同治疗条件，但治疗接受状态因不依从而异。现有方法要么经验识别要么区间识别这些估计量，需要新的结构假设实现点识别。

Method: 提出半参数框架，通过新的结构假设实现个体依从效应和网络分配效应的非参数点识别。开发半参数有效估计器，结合数据自适应机器学习方法和有效影响函数。引入敏感性分析方法研究假设违反的影响。

Result: 建立了新的结构假设实现点识别，开发了半参数有效估计器，提供了更稳健的推断方法。将方法应用于肯尼亚学校大规模除虫试验的重新分析，评估疾病传播影响。

Conclusion: 该框架为传染病群随机试验中的因果推断提供了新方法，能够区分个体依从效应和网络分配效应，通过敏感性分析增强了结果的可靠性，为实际试验分析提供了实用工具。

Abstract: Treatment noncompliance is pervasive in infectious disease cluster-randomized trials. Although all individuals within a cluster are assigned the same treatment condition, the treatment uptake status may vary across individuals due to noncompliance. We propose a semiparametric framework to evaluate the individual compliance effect and network assignment effect within principal stratum exhibiting different patterns of noncompliance. The individual compliance effect captures the portion of the treatment effect attributable to changes in treatment receipt, while the network assignment effect reflects the pure impact of treatment assignment and spillover among individuals within the same cluster. Unlike prior efforts which either empirically identify or interval identify these estimands, we characterize new structural assumptions for nonparametric point identification. We then develop semiparametrically efficient estimators that combine data-adaptive machine learning methods with efficient influence functions to enable more robust inference. Additionally, we introduce sensitivity analysis methods to study the impact under assumption violations, and apply the proposed methods to reanalyze a cluster-randomized trial in Kenya that evaluated the impact of school-based mass deworming on disease transmission.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [18] [BayesSum: Bayesian Quadrature in Discrete Spaces](https://arxiv.org/abs/2512.16105)
*Sophia Seulkee Kang,François-Xavier Briol,Toni Karvonen,Zonghao Chen*

Main category: stat.ML

TL;DR: 提出BayesSum估计器，将贝叶斯积分扩展到离散域，利用高斯过程先验信息提高样本效率，比蒙特卡洛方法收敛更快


<details>
  <summary>Details</summary>
Motivation: 现有方法（如蒙特卡洛和俄罗斯轮盘赌估计器）在估计离散域上的难处理期望时需要大量样本才能获得准确结果，计算效率较低

Method: 提出BayesSum估计器，将贝叶斯积分扩展到离散域，通过高斯过程利用被积函数的先验信息来提高样本效率

Result: 理论上推导出比蒙特卡洛更快的收敛速度，实证上在多个合成场景以及Conway-Maxwell-Poisson和Potts模型的参数估计中展示了更少的样本需求

Conclusion: BayesSum是一种更高效的离散域期望估计方法，通过利用先验信息显著减少所需样本数量

Abstract: This paper addresses the challenging computational problem of estimating intractable expectations over discrete domains. Existing approaches, including Monte Carlo and Russian Roulette estimators, are consistent but often require a large number of samples to achieve accurate results. We propose a novel estimator, \emph{BayesSum}, which is an extension of Bayesian quadrature to discrete domains. It is more sample efficient than alternatives due to its ability to make use of prior information about the integrand through a Gaussian process. We show this through theory, deriving a convergence rate significantly faster than Monte Carlo in a broad range of settings. We also demonstrate empirically that our proposed method does indeed require fewer samples on several synthetic settings as well as for parameter estimation for Conway-Maxwell-Poisson and Potts models.

</details>


### [19] [DAG Learning from Zero-Inflated Count Data Using Continuous Optimization](https://arxiv.org/abs/2512.16233)
*Noriaki Sato,Marco Scutari,Shuichi Kawano,Rui Yamaguchi,Seiya Imoto*

Main category: stat.ML

TL;DR: ZICO：针对零膨胀计数数据的网络结构学习方法，通过零膨胀广义线性模型和DAG约束优化，实现高效网络学习


<details>
  <summary>Details</summary>
Motivation: 需要从零膨胀计数数据中学习网络结构，特别是在基因调控网络等生物信息学领域，现有方法在处理零膨胀数据和计算效率方面存在不足

Method: 将每个节点建模为零膨胀广义线性模型，使用基于分数的平滑目标函数，在有向无环图约束下优化，结合可微代理约束和稀疏正则化

Result: 在模拟数据上表现优异且运行更快，在基因调控网络逆向工程中与常见算法相当或更好，支持向量化和小批量处理，能处理更大变量集

Conclusion: ZICO是处理零膨胀计数数据网络结构学习的有效方法，具有计算效率和可扩展性优势，适用于多个领域的大规模网络学习

Abstract: We address network structure learning from zero-inflated count data by casting each node as a zero-inflated generalized linear model and optimizing a smooth, score-based objective under a directed acyclic graph constraint. Our Zero-Inflated Continuous Optimization (ZICO) approach uses node-wise likelihoods with canonical links and enforces acyclicity through a differentiable surrogate constraint combined with sparsity regularization. ZICO achieves superior performance with faster runtimes on simulated data. It also performs comparably to or better than common algorithms for reverse engineering gene regulatory networks. ZICO is fully vectorized and mini-batched, enabling learning on larger variable sets with practical runtimes in a wide range of domains.

</details>


### [20] [Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning](https://arxiv.org/abs/2512.16489)
*Seyda Betul Aydin,Holger Brandt*

Main category: stat.ML

TL;DR: TL-TARNet通过迁移学习改进个体治疗效果估计，在小样本场景下优于标准TARNet，减少误差和偏差


<details>
  <summary>Details</summary>
Motivation: 在大规模数据集估计需要应用到小样本或不同环境时，因果知识的泛化具有挑战性。传统机器学习方法需要大样本量，限制了在行为科学等小样本领域的应用

Method: 提出TL-TARNet方法，在TARNet基础上结合迁移学习，利用源数据集知识适应新环境。通过模拟实验验证不同源数据集大小、样本量、随机与非随机干预设置下的效果

Result: 当存在大规模无偏源数据集且目标样本较小时，TL-TARNet相比标准TARNet能减少ITE误差并减弱偏差。在印度人类发展调查的实证应用中，迁移学习使目标平均ITE估计更接近源数据集估计，减少了无迁移时的偏差

Conclusion: 迁移学习可以改进因果模型在小样本中的个体治疗效果估计，为小样本因果推断提供了有效方法

Abstract: Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples.

</details>


### [21] [Riemannian Stochastic Interpolants for Amorphous Particle Systems](https://arxiv.org/abs/2512.16607)
*Louis Grenioux,Leonardo Galliano,Ludovic Berthier,Giulio Biroli,Marylou Gabrié*

Main category: stat.ML

TL;DR: 提出一种用于玻璃态材料平衡构型采样的生成模型，结合黎曼随机插值和等变流匹配，在环面上直接操作并保持几何对称性约束


<details>
  <summary>Details</summary>
Motivation: 玻璃态材料（非晶材料）的平衡构型采样是一个缓慢且困难的任务，现有生成模型在生物分子和晶体材料方面取得了进展，但在非晶材料领域仍缺乏有效的生成框架

Method: 采用等变黎曼随机插值框架，结合黎曼随机插值和等变流匹配，使用等变图神经网络直接在环面上操作，严格纳入周期性边界条件和多组分粒子系统的对称性

Result: 在模型非晶系统上的数值实验表明，强制几何和对称性约束显著提高了生成性能

Conclusion: 提出的生成框架能够有效克服玻璃态材料平衡构型采样的困难，为模拟物理系统提供了有前景的加速工具

Abstract: Modern generative models hold great promise for accelerating diverse tasks involving the simulation of physical systems, but they must be adapted to the specific constraints of each domain. Significant progress has been made for biomolecules and crystalline materials. Here, we address amorphous materials (glasses), which are disordered particle systems lacking atomic periodicity. Sampling equilibrium configurations of glass-forming materials is a notoriously slow and difficult task. This obstacle could be overcome by developing a generative framework capable of producing equilibrium configurations with well-defined likelihoods. In this work, we address this challenge by leveraging an equivariant Riemannian stochastic interpolation framework which combines Riemannian stochastic interpolant and equivariant flow matching. Our method rigorously incorporates periodic boundary conditions and the symmetries of multi-component particle systems, adapting an equivariant graph neural network to operate directly on the torus. Our numerical experiments on model amorphous systems demonstrate that enforcing geometric and symmetry constraints significantly improves generative performance.

</details>


### [22] [On The Hidden Biases of Flow Matching Samplers](https://arxiv.org/abs/2512.16768)
*Soon Hoe Lim*

Main category: stat.ML

TL;DR: 实证流匹配采样器存在固有偏差：即使条件流是梯度场，其经验最小化器也几乎不可能是梯度场，导致能量次优。动力学能量分析显示，高斯源产生指数集中，重尾源产生多项式尾部。


<details>
  <summary>Details</summary>
Motivation: 研究流匹配（FM）采样器的隐式偏差，特别是经验流匹配与理论最优传输（OT）之间的差异。虽然总体FM可能产生类似OT的梯度场速度，但经验FM在实践中存在结构和能量偏差。

Method: 通过经验流匹配的视角分析FM采样器。证明经验FM最小化器几乎不可能是梯度场，即使每个条件流都是梯度场。分析生成样本的动力学能量，考虑高斯源和重尾源两种情况。

Result: 1. 经验FM本质上能量次优，其最小化器几乎不可能是梯度场；2. 高斯源下瞬时和积分动力学能量都呈现指数集中；3. 重尾源导致多项式尾部；4. 这些行为主要由源分布选择决定，而非数据本身。

Conclusion: 经验流匹配存在结构和能量偏差：无法达到理论最优传输的梯度场特性，且动力学能量行为取决于源分布而非数据。这为理解FM采样器的实际局限性提供了数学基础。

Abstract: We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [23] [An Open Workflow Model for Improving Educational Video Design: Tools, Data, and Insights](https://arxiv.org/abs/2512.16254)
*Mohamed Tolba,Olivia Kendall,Daniel Tudball Smith,Alexander Gregg,Tony Vo,Scott Wordley*

Main category: stat.AP

TL;DR: 该研究开发了数据驱动的教育视频设计方法，包括分析工作流、开源工具、视频属性数据库、案例研究和机器学习分析，以提升学生参与度。


<details>
  <summary>Details</summary>
Motivation: 高等教育中教育视频使用广泛，但学生参与度因设计差异而显著不同。现有研究缺乏可扩展的工具和开放数据集来支持大规模、数据驱动的视频设计改进。

Method: 提出五方面贡献：1) 教育视频分析工作流模型；2) 提取视频元数据和特征的开源实现；3) 社区驱动的视频属性数据库；4) 两个工程课程的案例研究；5) 基于机器学习的视频特征对学生参与度影响分析。

Result: 建立了数据驱动教育视频设计的基础框架，包括可操作的工作流程、开源工具、共享数据库，并通过案例展示了方法的应用潜力。

Conclusion: 这项工作为共享的、基于证据的教育视频设计方法奠定了基础，支持未来大规模的视频设计优化研究。

Abstract: Educational videos are widely used across various instructional models in higher education to support flexible and self-paced learning. However, student engagement with these videos varies significantly depending on how they are designed. While several studies have identified potential influencing factors, there remains a lack of scalable tools and open datasets to support large-scale, data-driven improvements in video design. This study aims to advance data-driven approaches to educational video design. Its core contributions include: (1) a workflow model for analysing educational videos; (2) an open-source implementation for extracting video metadata and features; (3) an accessible, community-driven database of video attributes; (4) a case study applying the approach to two engineering courses; and (5) an initial machine learning-based analysis to explore the relative influence of various video characteristics on student engagement. This work lays the groundwork for a shared, evidence-based approach to educational video design.

</details>


### [24] [Dynamic Prediction for Hospital Readmission in Patients with Chronic Heart Failure](https://arxiv.org/abs/2512.16463)
*Rebecca Farina,Francois Mercier,Christian Wohlfart,Serge Masson,Silvia Metelli*

Main category: stat.AP

TL;DR: 开发贝叶斯联合模型，利用纵向NT-proBNP轨迹动态预测心衰患者再住院或死亡风险，相比传统静态模型显著提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 心衰患者再住院是重大临床和经济负担，传统静态预测模型无法充分利用纵向生物标志物信息，需要开发动态预测模型来改善风险分层

Method: 使用TriNetX数据库真实世界数据，纳入2016-2022年新诊断心衰患者，采用贝叶斯联合建模框架将患者特异性NT-proBNP轨迹与180天内再住院或死亡风险关联，通过5折交叉验证评估性能

Result: 联合模型相比基准静态模型具有显著预测优势，尤其在后期时间点（180-360天）更新预测时效果更好，基于更频繁NT-proBNP测量的模型准确性最高，主要模型校准良好

Conclusion: 联合建模框架能够更准确地动态评估心衰患者风险，支持开发自适应临床决策支持工具，实现个性化心衰管理

Abstract: Hospital readmission among patients with chronic heart failure (HF) is a major clinical and economic burden. Dynamic prediction models that leverage longitudinal biomarkers may improve risk stratification over traditional static models. This study aims to develop and validate a joint model (JM) using longitudinal N-terminal pro-B-type natriuretic peptide (NT-proBNP) measurements to predict the risk of rehospitalization or death in HF patients. We analyzed real-world data from the TriNetX database, including patients with an incident HF diagnosis between 2016 and 2022. The final selected cohort included 1,804 patients. A Bayesian joint modeling framework was developed to link patient-specific NT-proBNP trajectories to the risk of a composite endpoint (HF rehospitalization or all-cause mortality) within a 180-day window following hospital discharge. The model's performance was evaluated using 5-fold cross-validation and assessed with the Integrated Brier Score (IBS) and Integrated Calibration Index (ICI). The joint model demonstrated a strong predictive advantage over a benchmark static model, particularly when making updated predictions at later time points (180-360 days). A joint model trained on patients with more frequent NT-proBNP measurements achieved the highest accuracy. The main joint model showed excellent calibration, suggesting its risk estimates are reliable. These findings suggest that modeling the full trajectory of NT-proBNP with a joint modeling framework enables more accurate and dynamic risk assessment compared to static, single-timepoint methods. This approach supports the development of adaptive clinical decision-support tools for personalized HF management.

</details>


### [25] [Opening the House: Datasets for Mixed Doubles Curling](https://arxiv.org/abs/2512.16574)
*Robyn Ritchie,Alexandre Leblanc,Thomas Loughin*

Main category: stat.AP

TL;DR: 构建了混合冰壶最全面的公开数据集，包含11个顶级赛事、1112场比赛、近7万次投掷记录，通过文本抓取和图像处理流程提取标准化数据，支持混合冰壶的战略分析和性能建模。


<details>
  <summary>Details</summary>
Motivation: 混合冰壶作为快速发展的项目，其分析研究因数据获取困难而受限。现有冰壶分析主要关注传统4人制，混合双人项目缺乏公开可用的详细数据集，阻碍了战略分析和性能评估。

Method: 采用文本抓取与图像处理相结合的流程，从CurlIT结果手册中提取比赛和投掷级别的详细信息，包括球员统计、后手权、强力进攻使用、冰壶坐标和得分状态等，并进行数据标准化处理。

Result: 创建了包含53个国家、1112场比赛、近7万次投掷记录的混合冰壶数据集，提供了投掷选择与成功率、得分分布、团队效率等初步分析，揭示了混合双人与传统4人制冰壶的关键差异。

Conclusion: 该数据集为混合冰壶的高级性能建模、战略评估和未来研究提供了基础，支持从投掷、局、比赛或团队多个层面进行分析，促进了这一快速发展项目的分析参与度。

Abstract: We introduce the most comprehensive publicly available datasets for mixed doubles curling, constructed from eleven top-level tournaments from the CurlIT (https://curlit.com/results) Results Booklets spanning 53 countries, 1,112 games, and nearly 70,000 recorded shots. While curling analytics has grown in recent years, mixed doubles remains under-served due to limited access to data. Using a combined text-scraping and image-processing pipeline, we extract and standardize detailed game- and shot-level information, including player statistics, hammer possession, Power Play usage, stone coordinates, and post-shot scoring states. We describe the data engineering workflow, highlight challenges in parsing historical records, and derive additional contextual features that enable rigorous strategic analysis. Using these datasets, we present initial insights into shot selection and success rates, scoring distributions, and team efficiencies, illustrating key differences between mixed doubles and traditional 4-player curling. We highlight various ways to analyze this type of data including from a shot-, end-, game- or team-level to display its versatilely. The resulting resources provide a foundation for advanced performance modeling, strategic evaluation, and future research in mixed doubles curling analytics, supporting broader analytical engagement with this rapidly growing discipline.

</details>


### [26] [Quantile-based causal inference for spatio-temporal processes: Assessing the impacts of wildfires on US air quality](https://arxiv.org/abs/2512.16603)
*Zipei Geng,Jordan Richards,Raphael Huser,Marc G. Genton*

Main category: stat.AP

TL;DR: 论文提出QLSCM模型，使用条件分位数替代条件期望，分析野火对空气质量在整个结果分布上的因果影响，发现野火主要在极端污染事件中显著影响空气质量。


<details>
  <summary>Details</summary>
Motivation: 野火对空气质量威胁日益严重，但量化其因果影响面临挑战：存在未测量的气象和地理混杂因素，且传统基于均值的方法无法捕捉污染水平异质性效应。

Method: 开发基于分位数的潜在空间混杂模型(QLSCM)，用条件分位数替代条件期望，实现整个结果分布的因果分析。理论上建立因果解释，证明因果效应可识别性，并在温和条件下证明估计量一致性。

Result: 模拟验证了偏差校正能力和分位数推断优于均值方法。应用于美国本土野火和空气质量数据发现：在西部州如加州和俄勒冈，火辐射功率在高分位数对气溶胶光学厚度有显著正向因果效应，在低分位数不显著。区域分析显示西部和西北部在极端污染事件中因果效应最强。

Conclusion: 野火对空气质量的影响主要在极端污染事件中显现，这些发现为环境政策提供关键见解，识别出缓解措施最有效的区域和时机。

Abstract: Wildfires pose an increasingly severe threat to air quality, yet quantifying their causal impact remains challenging due to unmeasured meteorological and geographic confounders. Moreover, wildfire impacts on air quality may exhibit heterogeneous effects across pollution levels, which conventional mean-based causal methods fail to capture. To address these challenges, we develop a Quantile-based Latent Spatial Confounder Model (QLSCM) that substitutes conditional expectations with conditional quantiles, enabling causal analysis across the entire outcome distribution. We establish the causal interpretation of QLSCM theoretically, prove the identifiability of causal effects, and demonstrate estimator consistency under mild conditions. Simulations confirm the bias correction capability and the advantage of quantile-based inference over mean-based approaches. Applying our method to contiguous US wildfire and air quality data, we uncover important heterogeneous effects: fire radiative power exerts significant positive causal effects on aerosol optical depth at high quantiles in Western states like California and Oregon, while insignificant at lower quantiles. This indicates that wildfire impacts on air quality primarily manifest during extreme pollution events. Regional analyses reveal that Western and Northwestern regions experience the strongest causal effects during such extremes. These findings provide critical insights for environmental policy by identifying where and when mitigation efforts would be most effective.

</details>
