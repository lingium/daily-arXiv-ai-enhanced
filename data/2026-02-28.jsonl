{"id": "2602.22358", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2602.22358", "abs": "https://arxiv.org/abs/2602.22358", "authors": ["Guillermina Senn", "Nathan Glatt-Holtz", "Giulia Carigi", "Andrew Holbrook", "Håkon Tjelmeland"], "title": "Multiproposal Elliptical Slice Sampling", "comment": null, "summary": "We introduce Multiproposal Elliptical Slice Sampling, a self-tuning multiproposal Markov chain Monte Carlo method for Bayesian inference with Gaussian priors. Our method generalizes the Elliptical Slice Sampling algorithm by 1) allowing multiple candidate proposals to be sampled in parallel at each self-tuning step, and 2) basing the acceptance step on a distance-informed transition matrix that can favor proposals far from the current state. This allows larger moves in state space and faster self-tuning, at essentially no additional wall clock time for expensive likelihoods, and results in improved mixing. We additionally provide theoretical arguments and experimental results suggesting dimension-robust mixing behavior, making the algorithm particularly well suited for Bayesian PDE inverse problems."}
{"id": "2602.22687", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.22687", "abs": "https://arxiv.org/abs/2602.22687", "authors": ["Wei Cao", "Shanshan Wanga", "Xiaoxue Hua"], "title": "Renewable estimation in linear expectile regression models with streaming data sets", "comment": null, "summary": "Streaming data often exhibit heterogeneity due to heteroscedastic variances or inhomogeneous covariate effects. Online renewable quantile and expectile regression methods provide valuable tools for detecting such heteroscedasticity by combining current data with summary statistics from historical data. However, quantile regression can be computationally demanding because of the non-smooth check function. To address this, we propose a novel online renewable method based on expectile regression, which efficiently updates estimates using both current observations and historical summaries, thereby reducing storage requirements. By exploiting the smoothness of the expectile loss function, our approach achieves superior computational efficiency compared with existing online renewable methods for streaming data with heteroscedastic variances or inhomogeneous covariate effects. We establish the consistency and asymptotic normality of the proposed estimator under mild regularity conditions, demonstrating that it achieves the same statistical efficiency as oracle estimators based on full individual-level data. Numerical experiments and real-data applications demonstrate that our method performs comparably to the oracle estimator while maintaining high computational efficiency and minimal storage costs."}
{"id": "2602.22965", "categories": ["stat.ME", "cs.CE", "eess.SP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22965", "abs": "https://arxiv.org/abs/2602.22965", "authors": ["L. Martino", "F. Llorente"], "title": "A note on the area under the likelihood and the fake evidence for model selection", "comment": null, "summary": "Improper priors are not allowed for the computation of the Bayesian evidence $Z=p({\\bf y})$ (a.k.a., marginal likelihood), since in this case $Z$ is not completely specified due to an arbitrary constant involved in the computation. However, in this work, we remark that they can be employed in a specific type of model selection problem: when we have several (possibly infinite) models belonging to the same parametric family (i.e., for tuning parameters of a parametric model). However, the quantities involved in this type of selection cannot be considered as Bayesian evidences: we suggest to use the name ``fake evidences'' (or ``areas under the likelihood'' in the case of uniform improper priors). We also show that, in this model selection scenario, using a diffuse prior and increasing its scale parameter asymptotically to infinity, we cannot recover the value of the area under the likelihood, obtained with a uniform improper prior. We first discuss it from a general point of view. Then we provide, as an applicative example, all the details for Bayesian regression models with nonlinear bases, considering two cases: the use of a uniform improper prior and the use of a Gaussian prior, respectively. A numerical experiment is also provided confirming and checking all the previous statements."}
{"id": "2602.22239", "categories": ["stat.AP", "cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.22239", "abs": "https://arxiv.org/abs/2602.22239", "authors": ["Ida Egendal", "Rasmus Froberg Brøndum", "Dan J Woodcock", "Christopher Yau", "Martin Bøgsted"], "title": "VAE-MS: An Asymmetric Variational Autoencoder for Mutational Signature Extraction", "comment": "Keywords: Variational Autoencoders, Mutational Signatures", "summary": "Mutational signature analysis has emerged as a powerful method for uncovering the underlying biological processes driving cancer development. However, the signature extraction process, typically performed using non-negative matrix factorization (NMF), often lacks reliability and clinical applicability. To address these limitations, several solutions have been introduced, including the use of neural networks to achieve more accurate estimates and probabilistic methods to better capture natural variation in the data. In this work, we introduce a Variational Autoencoder for Mutational Signatures (VAE-MS), a novel model that leverages both an asymmetric architecture and probabilistic methods for the extraction of mutational signatures. VAE-MS is compared to with three state-of-the-art models for mutational signature extraction: SigProfilerExtractor, the NMF-based gold standard; MUSE-XAE, an autoencoder that employs an asymmetric design without probabilistic components; and SigneR, a Bayesian NMF model, to illustrate the strength in combining a nonlinear extraction with a probabilistic model. In the ability to reconstruct input data and generalize to unseen data, models with probabilistic components (VAE-MS, SigneR) dramatically outperformed models without (SigProfilerExtractor, MUSE-XAE). The NMF-baed models (SigneR, SigProfilerExtractor) had the most accurate reconstructions in simulated data, while VAE-MS reconstructed more accurately on real cancer data. Upon evaluating the ability to extract signatures consistently, no model exhibited a clear advantage over the others. Software for VAE-MS is available at https://github.com/CLINDA-AAU/VAE-MS."}
{"id": "2602.22432", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22432", "abs": "https://arxiv.org/abs/2602.22432", "authors": ["Vagner Santos", "Victor Coscrato", "Luben Cabezas", "Rafael Izbicki", "Thiago Ramos"], "title": "LoBoost: Fast Model-Native Local Conformal Prediction for Gradient-Boosted Trees", "comment": null, "summary": "Gradient-boosted decision trees are among the strongest off-the-shelf predictors for tabular regression, but point predictions alone do not quantify uncertainty. Conformal prediction provides distribution-free marginal coverage, yet split conformal uses a single global residual quantile and can be poorly adaptive under heteroscedasticity. Methods that improve adaptivity typically fit auxiliary nuisance models or introduce additional data splits/partitions to learn the conformal score, increasing cost and reducing data efficiency. We propose LoBoost, a model-native local conformal method that reuses the fitted ensemble's leaf structure to define multiscale calibration groups. Each input is encoded by its sequence of visited leaves; at resolution level k, we group points by matching prefixes of leaf indices across the first k trees and calibrate residual quantiles within each group. LoBoost requires no retraining, auxiliary models, or extra splitting beyond the standard train/calibration split. Experiments show competitive interval quality, improved test MSE on most datasets, and large calibration speedups."}
{"id": "2602.22307", "categories": ["stat.ME", "astro-ph.CO", "astro-ph.GA", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2602.22307", "abs": "https://arxiv.org/abs/2602.22307", "authors": ["Namu Kroupa", "Will Handley"], "title": "The global structure of the time delay likelihood", "comment": "21 pages, 8 figures", "summary": "We identify a fundamental pathology in the likelihood for time delay inference which challenges standard inference methods. By analysing the likelihood for time delay inference with Gaussian process light curve models, we show that it generically develops a boundary-driven \"W\"-shape with a global maximum at the true delay and gradual rises towards the edges of the observation window. This arises because time delay estimation is intrinsically extrapolative. In practice, global samplers such as nested sampling are steered towards spurious edge modes unless strict convergence criteria are adopted. We demonstrate this with simulations and show that the effect strengthens with higher data density over a fixed time span. To ensure convergence, we provide concrete guidance, notably increasing the number of live points. Further, we show that methods implicitly favouring small delays, for example optimisers and local MCMC, induce a bias towards larger $H_0$. Our results clarify failure modes and offer practical remedies for robust fully Bayesian time delay inference."}
{"id": "2602.22486", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.22486", "abs": "https://arxiv.org/abs/2602.22486", "authors": ["Shivam Kumar", "Yixin Wang", "Lizhen Lin"], "title": "Flow Matching is Adaptive to Manifold Structures", "comment": null, "summary": "Flow matching has emerged as a simulation-free alternative to diffusion-based generative modeling, producing samples by solving an ODE whose time-dependent velocity field is learned along an interpolation between a simple source distribution (e.g., a standard normal) and a target data distribution. Flow-based methods often exhibit greater training stability and have achieved strong empirical performance in high-dimensional settings where data concentrate near a low-dimensional manifold, such as text-to-image synthesis, video generation, and molecular structure generation. Despite this success, existing theoretical analyses of flow matching assume target distributions with smooth, full-dimensional densities, leaving its effectiveness in manifold-supported settings largely unexplained. To this end, we theoretically analyze flow matching with linear interpolation when the target distribution is supported on a smooth manifold. We establish a non-asymptotic convergence guarantee for the learned velocity field, and then propagate this estimation error through the ODE to obtain statistical consistency of the implicit density estimator induced by the flow-matching objective. The resulting convergence rate is near minimax-optimal, depends only on the intrinsic dimension, and reflects the smoothness of both the manifold and the target distribution. Together, these results provide a principled explanation for how flow matching adapts to intrinsic data geometry and circumvents the curse of dimensionality."}
{"id": "2602.22694", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.22694", "abs": "https://arxiv.org/abs/2602.22694", "authors": ["Zhichao Wang", "Shanshan Wang", "Wei Cao", "Fei Yang"], "title": "Robust optimal reconciliation for hierarchical time series forecasting with M-estimation", "comment": null, "summary": "Aggregation constraints, arising from geographical or sectoral division, frequently emerge in a large set of time series. Coherent forecasts of these constrained series are anticipated to conform to their hierarchical structure organized by the aggregation rules. To enhance its resilience against potential irregular series, we explore the robust reconciliation process for hierarchical time series (HTS) forecasting. We incorporate M-estimation to obtain the reconciled forecasts by minimizing a robust loss function of transforming a group of base forecasts subject to the aggregation constraints. The related minimization procedure is developed and implemented through a modified Newton-Raphson algorithm via local quadratic approximation. Extensive numerical experiments are carried out to evaluate the performance of the proposed method, and the results suggest its feasibility in handling numerous abnormal cases (for instance, series with non-normal errors). The proposed robust reconciliation also demonstrates excellent efficiency when no outliers exist in HTS. Finally, we showcase the practical application of the proposed method in a real-data study on Australian domestic tourism."}
{"id": "2602.22486", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.22486", "abs": "https://arxiv.org/abs/2602.22486", "authors": ["Shivam Kumar", "Yixin Wang", "Lizhen Lin"], "title": "Flow Matching is Adaptive to Manifold Structures", "comment": null, "summary": "Flow matching has emerged as a simulation-free alternative to diffusion-based generative modeling, producing samples by solving an ODE whose time-dependent velocity field is learned along an interpolation between a simple source distribution (e.g., a standard normal) and a target data distribution. Flow-based methods often exhibit greater training stability and have achieved strong empirical performance in high-dimensional settings where data concentrate near a low-dimensional manifold, such as text-to-image synthesis, video generation, and molecular structure generation. Despite this success, existing theoretical analyses of flow matching assume target distributions with smooth, full-dimensional densities, leaving its effectiveness in manifold-supported settings largely unexplained. To this end, we theoretically analyze flow matching with linear interpolation when the target distribution is supported on a smooth manifold. We establish a non-asymptotic convergence guarantee for the learned velocity field, and then propagate this estimation error through the ODE to obtain statistical consistency of the implicit density estimator induced by the flow-matching objective. The resulting convergence rate is near minimax-optimal, depends only on the intrinsic dimension, and reflects the smoothness of both the manifold and the target distribution. Together, these results provide a principled explanation for how flow matching adapts to intrinsic data geometry and circumvents the curse of dimensionality."}
{"id": "2602.22582", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22582", "abs": "https://arxiv.org/abs/2602.22582", "authors": ["Lucas Kock", "Scott A. Sisson", "G. S. Rodrigues", "David J. Nott"], "title": "Predictive variational inference for flexible regression models", "comment": null, "summary": "A conventional Bayesian approach to prediction uses the posterior distribution to integrate out parameters in a density for unobserved data conditional on the observed data and parameters. When the true posterior is intractable, it is replaced by an approximation; here we focus on variational approximations. Recent work has explored methods that learn posteriors optimized for predictive accuracy under a chosen scoring rule, while regularizing toward the prior or conventional posterior. Our work builds on an existing predictive variational inference (PVI) framework that improves prediction, but also diagnoses model deficiencies through implicit model expansion. In models where the sampling density depends on the parameters through a linear predictor, we improve the interpretability of existing PVI methods as a diagnostic tool. This is achieved by adopting PVI posteriors of Gaussian mixture form (GM-PVI) and establishing connections with plug-in prediction for mixture-of-experts models. We make three main contributions. First, we show that GM-PVI prediction is equivalent to plug-in prediction for certain mixture-of-experts models with covariate-independent weights in generalized linear models and hierarchical extensions of them. Second, we extend standard PVI by allowing GM-PVI posteriors to vary with the prediction covariate and in this case an equivalence to plug-in prediction for mixtures of experts with covariate-dependent weights is established. Third, we demonstrate the diagnostic value of this approach across several examples, including generalized linear models, linear mixed models, and latent Gaussian process models, demonstrating how the parameters of the original model must vary across the covariate space to achieve improvements in prediction."}
{"id": "2602.22648", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.22648", "abs": "https://arxiv.org/abs/2602.22648", "authors": ["Hengjia Fang", "Wei Ma"], "title": "A General (Non-Markovian) Framework for Covariate Adaptive Randomization: Achieving Balance While Eliminating the Shift", "comment": null, "summary": "Emerging applications increasingly demand flexible covariate adaptive randomization (CAR) methods that support unequal targeted allocation ratios. While existing procedures can achieve covariate balance, they often suffer from the shift problem. This occurs when the allocation ratios of some additional covariates deviate from the target. We show that this problem is equivalent to a mismatch between the conditional average allocation ratio and the target among units sharing specific covariate values, revealing a failure of existing procedures in the long run. To address it, we derive a new form of allocation function by requiring that balancing covariates ensures the ratio matches the target. Based on this form, we design a class of parameterized allocation functions. When the parameter roughly matches certain characteristics of the covariate distribution, the resulting procedure can balance covariates. Thus, we propose a feasible randomization procedure that updates the parameter based on collected covariate information, rendering the procedure non-Markovian. To accommodate this, we introduce a CAR framework that allows non-Markovian procedure. We then establish its key theoretical properties, including the boundedness of covariate imbalance in probability and the asymptotic distribution of the imbalance for additional covariates. Ultimately, we conclude that the feasible randomization procedure can achieve covariate balance and eliminate the shift."}
{"id": "2602.23233", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.23233", "abs": "https://arxiv.org/abs/2602.23233", "authors": ["Herbert P. Susmann", "Antonio D'Alessandro"], "title": "The Counterfactual Combine: A Causal Framework for Player Evaluation", "comment": "23 pages, 8 figures", "summary": "Evaluating sports players based on their performance shares core challenges with evaluating healthcare providers based on patient outcomes. Drawing on recent advances in healthcare provider profiling, we cast sports player evaluation within a rigorous causal inference framework and define a flexible class of causal player evaluation estimands. Using stochastic interventions, we compare player success rates on repeated tasks (such as field goal attempts or plate appearance) to counterfactual success rates had those same attempts been randomly reassigned to players according to prespecified reference distributions. This setup encompasses direct and indirect standardization parameters familiar from healthcare provider profiling, and we additionally propose a \"performance above random replacement\" estimand designed for interpretability in sports settings. We develop doubly robust estimators for these evaluation metrics based on modern semiparametric statistical methods, with a focus on Targeted Minimum Loss-based Estimation, and incorporate machine learning methods to capture complex relationships driving player performance. We illustrate our framework in detailed case studies of field goal kickers in the National Football League and batters in Major League Baseball, highlighting how different causal estimands yield distinct interpretations and insights about player performance."}
{"id": "2602.22492", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22492", "abs": "https://arxiv.org/abs/2602.22492", "authors": ["Gracielle Antunes de Araújo", "Flávio B. Gonçalves"], "title": "From Shallow Bayesian Neural Networks to Gaussian Processes: General Convergence, Identifiability and Scalable Inference", "comment": "29 pages, 4 figures, 8 tables. Supplementary material included", "summary": "In this work, we study scaling limits of shallow Bayesian neural networks (BNNs) via their connection to Gaussian processes (GPs), with an emphasis on statistical modeling, identifiability, and scalable inference. We first establish a general convergence result from BNNs to GPs by relaxing assumptions used in prior formulations, and we compare alternative parameterizations of the limiting GP model. Building on this theory, we propose a new covariance function defined as a convex mixture of components induced by four widely used activation functions, and we characterize key properties including positive definiteness and both strict and practical identifiability under different input designs. For computation, we develop a scalable maximum a posterior (MAP) training and prediction procedure using a Nyström approximation, and we show how the Nyström rank and anchor selection control the cost-accuracy trade-off. Experiments on controlled simulations and real-world tabular datasets demonstrate stable hyperparameter estimates and competitive predictive performance at realistic computational cost."}
{"id": "2602.22588", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22588", "abs": "https://arxiv.org/abs/2602.22588", "authors": ["Niloofar Ramezani", "Pascal Nitiema", "Jeffrey R. Wilson"], "title": "Modeling Covariate Feedback, Reversal, and Latent Traits in Longitudinal Data: A Joint Hierarchical Framework", "comment": "16 pages, 1 figure, 4 tables, 3 supplementary figures", "summary": "Time-varying covariates in longitudinal studies frequently evolve through reciprocal feedback, undergo role reversal, and reflect unobserved individual heterogeneity. Standard statistical frameworks often assume fixed covariate roles and exogenous predictors, limiting their utility in systems governed by dynamic behavioral or physiological processes. We develop a hierarchical joint modeling framework that unifies three key features of such systems: (i) bidirectional feedback between a binary and a continuous covariate, (ii) role reversal in which these covariates become jointly modeled outcomes at a prespecified decision phase, and (iii) a shared latent trait influencing both intermediate covariates and a final binary endpoint. The model proceeds in three phases: a feedback-driven longitudinal process, a reversal phase in which the two covariates are jointly modeled conditional on the latent trait, and an outcome model linking a binary, decision-relevant endpoint to observed and latent components. Estimation is carried out using maximum likelihood and Bayesian inference, with Hamiltonian Monte Carlo supporting robust posterior estimation for models with latent structure and mixed outcome types. Simulation studies show that the model yields well calibrated coverage, small bias, and improved predictive performance compared to standard generalized linear mixed models, marginal approaches, and models that ignore feedback or latent traits. In an analysis of nationally representative U.S. panel data, the model captures the co-evolution of physical activity and body mass index and their joint influence, moderated by a latent behavioral resilience factor, on income mobility. The framework offers a flexible, practically implementable tool for analyzing longitudinal decision systems in which feedback, covariate role transition, and unmeasured capacity are central to prediction and intervention."}
{"id": "2602.23020", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.23020", "abs": "https://arxiv.org/abs/2602.23020", "authors": ["Sourbh Bhadane", "Joris M. Mooij", "Philip Boeken", "Onno Zoeter"], "title": "Testing Partially-Identifiable Causal Queries Using Ternary Tests", "comment": null, "summary": "We consider hypothesis testing of binary causal queries using observational data. Since the mapping of causal models to the observational distribution that they induce is not one-to-one, in general, causal queries are often only partially identifiable. When binary statistical tests are used for testing partially-identifiable causal queries, their results do not translate in a straightforward manner to the causal hypothesis testing problem. We propose using ternary (three-outcome) statistical tests to test partially-identifiable causal queries. We establish testability requirements that ternary tests must satisfy in terms of uniform consistency and present equivalent topological conditions on the hypotheses. To leverage the existing toolbox of binary tests, we prove that obtaining ternary tests by combining binary tests is complete. Finally, we demonstrate how topological conditions serve as a guide to construct ternary tests for two concrete causal hypothesis testing problems, namely testing the instrumental variable (IV) inequalities and comparing treatment efficacy."}
{"id": "2602.23311", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.23311", "abs": "https://arxiv.org/abs/2602.23311", "authors": ["Johannes Brachem", "Paul F. V. Wiemann", "Matthias Katzfuss"], "title": "Data-Efficient Generative Modeling of Non-Gaussian Global Climate Fields via Scalable Composite Transformations", "comment": null, "summary": "Quantifying uncertainty in future climate projections is hindered by the prohibitive computational cost of running physical climate models, which severely limits the availability of training data. We propose a data-efficient framework for emulating the internal variability of global climate fields, specifically designed to overcome these sample-size constraints. Inspired by copula modeling, our approach constructs a highly expressive joint distribution via a composite transformation to a multivariate standard normal space. We combine a nonparametric Bayesian transport map for spatial dependence modeling with flexible, spatially varying marginal models, essential for capturing non-Gaussian behavior and heavy-tailed extremes. These marginals are defined by a parametric model followed by a semi-parametric B-spline correction to capture complex distributional features. The marginal parameters are spatially smoothed using Gaussian-process priors with low-rank approximations, rendering the computational cost linear in the spatial dimension. When applied to global log-precipitation-rate fields at more than 50,000 grid locations, our stochastic surrogate achieves high fidelity, accurately quantifying the climate distribution's spatial dependence and marginal characteristics, including the tails. Using only 10 training samples, it outperforms a state-of-the-art competitor trained on 80 samples, effectively octupling the computational budget for climate research. We provide a Python implementation at https://github.com/jobrachem/ppptm ."}
{"id": "2602.22884", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22884", "abs": "https://arxiv.org/abs/2602.22884", "authors": ["Aayush Mishra", "Šimon Kucharský", "Paul-Christian Bürkner"], "title": "Unsupervised Continual Learning for Amortized Bayesian Inference", "comment": null, "summary": "Amortized Bayesian Inference (ABI) enables efficient posterior estimation using generative neural networks trained on simulated data, but often suffers from performance degradation under model misspecification. While self-consistency (SC) training on unlabeled empirical data can enhance network robustness, current approaches are limited to static, single-task settings and fail to handle sequentially arriving data or distribution shifts. We propose a continual learning framework for ABI that decouples simulation-based pre-training from unsupervised sequential SC fine-tuning on real-world data. To address the challenge of catastrophic forgetting, we introduce two adaptation strategies: (1) SC with episodic replay, utilizing a memory buffer of past observations, and (2) SC with elastic weight consolidation, which regularizes updates to preserve task-critical parameters. Across three diverse case studies, our methods significantly mitigate forgetting and yield posterior estimates that outperform standard simulation-based training, achieving estimates closer to MCMC reference, providing a viable path for trustworthy ABI across a range of different tasks."}
{"id": "2602.22590", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22590", "abs": "https://arxiv.org/abs/2602.22590", "authors": ["Chengyu Cui", "Yunxiao Chen", "Jing Ouyang", "Gongjun Xu"], "title": "Beyond Vintage Rotation: Bias-Free Sparse Representation Learning with Oracle Inference", "comment": null, "summary": "Learning low-dimensional latent representations is a central topic in statistics and machine learning, and rotation methods have long been used to obtain sparse and interpretable representations. Despite nearly a century of widespread use across many fields, rigorous guarantees for valid inference for the learned representation remain lacking. In this paper, we identify a surprisingly prevalent phenomenon that suggests a reason for this gap: for a broad class of vintage rotations, the resulting estimators exhibit a non-estimable bias. Because this bias is independent of the data, it fundamentally precludes the development of valid inferential procedures, including the construction of confidence intervals and hypothesis testing. To address this challenge, we propose a novel bias-free rotation method within a general representation learning framework based on latent variables. We establish an oracle inference property for the learned sparse representations: the estimators achieve the same asymptotic variance as in the ideal setting where the latent variables are observed. To bridge the gap between theory and computation, we develop an efficient computational framework and prove that its output estimators retain the same oracle property. Our results provide a rigorous inference procedure for the rotated estimators, yielding statistically valid and interpretable representation learning."}
{"id": "2602.23045", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.23045", "abs": "https://arxiv.org/abs/2602.23045", "authors": ["Siyan Liu", "Qinglong Tian", "Chunlin Wang", "Pengfei Li"], "title": "Semiparametric Joint Inference for Sensitivity and Specificity at the Youden-Optimal Cut-Off", "comment": "23 pages, 2 figures, 6 tables", "summary": "Sensitivity and specificity evaluated at an optimal diagnostic cut-off are fundamental measures of classification accuracy when continuous biomarkers are used for disease diagnosis. Joint inference for these quantities is challenging because their estimators are evaluated at a common, data-driven threshold estimated from both diseased and healthy samples, inducing statistical dependence. Existing approaches are largely based on parametric assumptions or fully nonparametric procedures, which may be sensitive to model misspecification or lack efficiency in moderate samples. We propose a semiparametric framework for joint inference on sensitivity and specificity at the Youden-optimal cut-off under the density ratio model. Using maximum empirical likelihood, we derive estimators of the optimal threshold and the corresponding sensitivity and specificity, and establish their joint asymptotic normality. This leads to Wald-type and range-preserving logit-transformed confidence regions. Simulation studies show that the proposed method achieves accurate coverage with improved efficiency relative to existing parametric and nonparametric alternatives across a variety of distributional settings. An analysis of COVID-19 antibody data demonstrates the practical advantages of the proposed approach for diagnostic decision-making."}
{"id": "2602.22925", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22925", "abs": "https://arxiv.org/abs/2602.22925", "authors": ["Katerina Papagiannouli", "Dario Trevisan", "Giuseppe Pio Zitto"], "title": "Beyond NNGP: Large Deviations and Feature Learning in Bayesian Neural Networks", "comment": null, "summary": "We study wide Bayesian neural networks focusing on the rare but statistically dominant fluctuations that govern posterior concentration, beyond Gaussian-process limits. Large-deviation theory provides explicit variational objectives-rate functions-on predictors, providing an emerging notion of complexity and feature learning directly at the functional level. We show that the posterior output rate function is obtained by a joint optimization over predictors and internal kernels, in contrast with fixed-kernel (NNGP) theory. Numerical experiments demonstrate that the resulting predictions accurately describe finite-width behavior for moderately sized networks, capturing non-Gaussian tails, posterior deformation, and data-dependent kernel selection effects."}
{"id": "2602.22612", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22612", "abs": "https://arxiv.org/abs/2602.22612", "authors": ["Yuxi Du", "Zhiheng Zhang", "Haoxuan Li", "Cong Fang", "Jixing Xu", "Peng Zhen", "Jiecheng Guo"], "title": "Feasible Fusion: Constrained Joint Estimation under Structural Non-Overlap", "comment": null, "summary": "Causal inference in modern largescale systems faces growing challenges, including highdimensional covariates, multi-valued treatments, massive observational (OBS) data, and limited randomized controlled trial (RCT) samples due to cost constraints. We formalize treatment-induced structural non-overlap and show that, under this regime, commonly used weighted fusion methods provably fail to satisfy randomized identifying restrictions.To address this issue,we propose a constrained joint estimation framework that minimizes observational risk while enforcing causal validity through orthogonal experimental moment conditions. We further show that structural non-overlap creates a feasibility obstruction for moment enforcement in the original covariate space.We also derive a penalized primaldual algorithm that jointly learns representations and predictors, and establish oracle inequalities decomposing error into overlap recovery, moment violation, and statistical terms.Extensive synthetic experiments demonstrate robust performance under varying degrees of nonoverlap. A largescale ridehailing application shows that our method achieves substantial gains over existing baselines, matching the performance of models trained with significantly more RCT data."}
{"id": "2602.23291", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.23291", "abs": "https://arxiv.org/abs/2602.23291", "authors": ["Tommy Tang", "Xinran Li", "Bo Li"], "title": "Identifiability of Treatment Effects with Unobserved Spatially Varying Confounders", "comment": "8 pages, 1 figure", "summary": "The study of causal effects in the presence of unmeasured spatially varying confounders has garnered increasing attention. However, a general framework for identifiability, which is critical for reliable causal inference from observational data, has yet to be advanced. In this paper, we study a linear model with various parametric model assumptions on the covariance structure between the unmeasured confounder and the exposure of interest. We establish identifiability of the treatment effect for many commonly 20 used spatial models for both discrete and continuous data, under mild conditions on the structure of observation locations and the exposure-confounder association. We also emphasize models or scenarios where identifiability may not hold, under which statistical inference should be conducted with caution."}
{"id": "2602.22985", "categories": ["stat.ML", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22985", "abs": "https://arxiv.org/abs/2602.22985", "authors": ["Pouya Roudaki", "Shakeel Gavioli-Akilagun", "Florian Kalinke", "Mona Azadkia", "Zoltán Szabó"], "title": "Kernel Integrated $R^2$: A Measure of Dependence", "comment": null, "summary": "We introduce kernel integrated $R^2$, a new measure of statistical dependence that combines the local normalization principle of the recently introduced integrated $R^2$ with the flexibility of reproducing kernel Hilbert spaces (RKHSs). The proposed measure extends integrated $R^2$ from scalar responses to responses taking values on general spaces equipped with a characteristic kernel, allowing to measure dependence of multivariate, functional, and structured data, while remaining sensitive to tail behaviour and oscillatory dependence structures. We establish that (i) this new measure takes values in $[0,1]$, (ii) equals zero if and only if independence holds, and (iii) equals one if and only if the response is almost surely a measurable function of the covariates. Two estimators are proposed: a graph-based method using $K$-nearest neighbours and an RKHS-based method built on conditional mean embeddings. We prove consistency and derive convergence rates for the graph-based estimator, showing its adaptation to intrinsic dimensionality. Numerical experiments on simulated data and a real data experiment in the context of dependency testing for media annotations demonstrate competitive power against state-of-the-art dependence measures, particularly in settings involving non-linear and structured relationships."}
{"id": "2602.22648", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.22648", "abs": "https://arxiv.org/abs/2602.22648", "authors": ["Hengjia Fang", "Wei Ma"], "title": "A General (Non-Markovian) Framework for Covariate Adaptive Randomization: Achieving Balance While Eliminating the Shift", "comment": null, "summary": "Emerging applications increasingly demand flexible covariate adaptive randomization (CAR) methods that support unequal targeted allocation ratios. While existing procedures can achieve covariate balance, they often suffer from the shift problem. This occurs when the allocation ratios of some additional covariates deviate from the target. We show that this problem is equivalent to a mismatch between the conditional average allocation ratio and the target among units sharing specific covariate values, revealing a failure of existing procedures in the long run. To address it, we derive a new form of allocation function by requiring that balancing covariates ensures the ratio matches the target. Based on this form, we design a class of parameterized allocation functions. When the parameter roughly matches certain characteristics of the covariate distribution, the resulting procedure can balance covariates. Thus, we propose a feasible randomization procedure that updates the parameter based on collected covariate information, rendering the procedure non-Markovian. To accommodate this, we introduce a CAR framework that allows non-Markovian procedure. We then establish its key theoretical properties, including the boundedness of covariate imbalance in probability and the asymptotic distribution of the imbalance for additional covariates. Ultimately, we conclude that the feasible randomization procedure can achieve covariate balance and eliminate the shift."}
{"id": "2602.23006", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23006", "abs": "https://arxiv.org/abs/2602.23006", "authors": ["Arsalan Jawaid", "Abdullah Karatas", "Jörg Seewig"], "title": "Regular Fourier Features for Nonstationary Gaussian Processes", "comment": "8 pages, 5 figures", "summary": "Simulating a Gaussian process requires sampling from a high-dimensional Gaussian distribution, which scales cubically with the number of sample locations. Spectral methods address this challenge by exploiting the Fourier representation, treating the spectral density as a probability distribution for Monte Carlo approximation. Although this probabilistic interpretation works for stationary processes, it is overly restrictive for the nonstationary case, where spectral densities are generally not probability measures. We propose regular Fourier features for harmonizable processes that avoid this limitation. Our method discretizes the spectral representation directly, preserving the correlation structure among spectral weights without requiring probability assumptions. Under a finite spectral support assumption, this yields an efficient low-rank approximation that is positive semi-definite by construction. When the spectral density is unknown, the framework extends naturally to kernel learning from data. We demonstrate the method on locally stationary kernels and on harmonizable mixture kernels with complex-valued spectral densities."}
{"id": "2602.22684", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22684", "abs": "https://arxiv.org/abs/2602.22684", "authors": ["Riley L Isaacs", "X. Joan Hu", "K. Ken Peng", "Tim Swartz"], "title": "Learning about Corner Kicks in Soccer by Analysis of Event Times Using a Frailty Model", "comment": null, "summary": "Corner kicks are an important event in soccer because they are often the result of strong attacking play and can be of keen interest to sports fans and bettors. Peng, Hu, and Swartz (2024, Computational Statistics) formulate the mixture feature of corner kick times caused by previous corner kicks, frame the commonly available corner kick data as right-censored event times, and explore patterns of corner kicks. This paper extends their modeling to accommodate the potential correlations between corner kicks by the same teams within the same games. We con- sider a frailty model for event times and apply the Monte Carlo Expec- tation Maximization (MCEM) algorithm to obtain the maximum like- lihood estimates for the model parameters. We compare the proposed model with the model in Peng, Hu, and Swartz (2024) using likelihood ratio tests. The 2019 Chinese Super League (CSL) data are employed throughout the paper for motivation and illustration."}
{"id": "2602.22965", "categories": ["stat.ME", "cs.CE", "eess.SP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22965", "abs": "https://arxiv.org/abs/2602.22965", "authors": ["L. Martino", "F. Llorente"], "title": "A note on the area under the likelihood and the fake evidence for model selection", "comment": null, "summary": "Improper priors are not allowed for the computation of the Bayesian evidence $Z=p({\\bf y})$ (a.k.a., marginal likelihood), since in this case $Z$ is not completely specified due to an arbitrary constant involved in the computation. However, in this work, we remark that they can be employed in a specific type of model selection problem: when we have several (possibly infinite) models belonging to the same parametric family (i.e., for tuning parameters of a parametric model). However, the quantities involved in this type of selection cannot be considered as Bayesian evidences: we suggest to use the name ``fake evidences'' (or ``areas under the likelihood'' in the case of uniform improper priors). We also show that, in this model selection scenario, using a diffuse prior and increasing its scale parameter asymptotically to infinity, we cannot recover the value of the area under the likelihood, obtained with a uniform improper prior. We first discuss it from a general point of view. Then we provide, as an applicative example, all the details for Bayesian regression models with nonlinear bases, considering two cases: the use of a uniform improper prior and the use of a Gaussian prior, respectively. A numerical experiment is also provided confirming and checking all the previous statements."}
{"id": "2602.22687", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.22687", "abs": "https://arxiv.org/abs/2602.22687", "authors": ["Wei Cao", "Shanshan Wanga", "Xiaoxue Hua"], "title": "Renewable estimation in linear expectile regression models with streaming data sets", "comment": null, "summary": "Streaming data often exhibit heterogeneity due to heteroscedastic variances or inhomogeneous covariate effects. Online renewable quantile and expectile regression methods provide valuable tools for detecting such heteroscedasticity by combining current data with summary statistics from historical data. However, quantile regression can be computationally demanding because of the non-smooth check function. To address this, we propose a novel online renewable method based on expectile regression, which efficiently updates estimates using both current observations and historical summaries, thereby reducing storage requirements. By exploiting the smoothness of the expectile loss function, our approach achieves superior computational efficiency compared with existing online renewable methods for streaming data with heteroscedastic variances or inhomogeneous covariate effects. We establish the consistency and asymptotic normality of the proposed estimator under mild regularity conditions, demonstrating that it achieves the same statistical efficiency as oracle estimators based on full individual-level data. Numerical experiments and real-data applications demonstrate that our method performs comparably to the oracle estimator while maintaining high computational efficiency and minimal storage costs."}
{"id": "2602.22768", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22768", "abs": "https://arxiv.org/abs/2602.22768", "authors": ["Li Yang", "Xiaodong Yan", "Dandan Jiang"], "title": "Asymptotic Theory and Sequential Test for General Multi-Armed Bandit Process", "comment": null, "summary": "Multi-armed bandit (MAB) processes constitute a foundational subclass of reinforcement learning problems and represent a central topic in statistical decision theory, but are limited to simultaneous adaptive allocation and sequential test, because of the absence of asymptotic theory under non-i.i.d sequence and sublinear information. To address this open challenge, we propose Urn Bandit (UNB) process to integrate the reinforcement mechanism of urn probabilistic models with MAB principles, ensuring almost sure convergence of resource allocation to optimal arms. We establish the joint functional central limit theorem (FCLT) for consistent estimators of expected rewards under non-i.i.d., non-sub-Gaussian and sublinear reward samples with pairwise correlations across arms. To overcome the limitations of existing methods that focus mainly on cumulative regret, we establish the asymptotic theory along with adaptive allocation that serves powerful sequential test, such as arms comparison, A/B testing, and policy valuation. Simulation studies and real data analysis demonstrate that UNB maintains statistical test performance of equal randomization (ER) design but obtain more average rewards like classical MAB processes."}
{"id": "2602.22803", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22803", "abs": "https://arxiv.org/abs/2602.22803", "authors": ["Nils Lid Hjort", "Gerda Claeskens"], "title": "Rejoinder to the discussants of the two JASA articles `Frequentist Model Averaging' and `The Focused Information Ctierion', by Nils Lid Hjort and Gerda Claeskens", "comment": "16 pages; Statistical Research Report, Department of Mathematics, University of Oslo, August 2003, and arXiv'd February 2026. This rejoinder to the two papers `FMA' and `FIC' is published in JASA, 2003, vol. 98, pages 938-945, at this url: tandfonline.com/doi/abs/10.1198/016214503000000882", "summary": "We are honoured to have our work read and discussed at such a thorough level by several experts. Words of appreciation and encouragement are gratefully received, while the many supplementary comments, thoughtful reminders, new perspectives and additional themes raised are warmly welcomed and deeply appreciated. Our thanks go also to JASA Editor Francisco Samaniego and his editorial helpers for organising this discussion.\n  Space does not allow us answering all of the many worthwhile points raised by our discussants, but in the following we make an attempt to respond to what we perceive of as being the major issues. Our responses are organised by themes rather than by discussants. We shall refer to our two articles as `the FMA paper' (Hjort and Claeskens) and `the FIC paper' (Claeskens and Hjort)."}
{"id": "2602.22877", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22877", "abs": "https://arxiv.org/abs/2602.22877", "authors": ["Filip Bočinec", "Stanislav Nagy", "Hyemin Yeon"], "title": "Projection depth for functional data: Practical issues, computation and applications", "comment": null, "summary": "Statistical analysis of functional data is challenging due to their complex patterns, for which functional depth provides an effective means of reflecting their ordering structure. In this work, we investigate practical aspects of the recently proposed regularized projection depth (RPD), which induces a meaningful ordering of functional data while appropriately accommodating their complex shape features. Specifically, we examine the impact and choice of its tuning parameter, which regulates the degree of effective dimension reduction applied to the data, and propose a random projection-based approach for its efficient computation, supported by theoretical justification. Through comprehensive numerical studies, we explore a wide range of statistical applications of the RPD and demonstrate its particular usefulness in uncovering shape features in functional data analysis. This ability allows the RPD to outperform competing depth-based methods, especially in tasks such as functional outlier detection, classification, and two-sample hypothesis testing."}
{"id": "2602.22965", "categories": ["stat.ME", "cs.CE", "eess.SP", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.22965", "abs": "https://arxiv.org/abs/2602.22965", "authors": ["L. Martino", "F. Llorente"], "title": "A note on the area under the likelihood and the fake evidence for model selection", "comment": null, "summary": "Improper priors are not allowed for the computation of the Bayesian evidence $Z=p({\\bf y})$ (a.k.a., marginal likelihood), since in this case $Z$ is not completely specified due to an arbitrary constant involved in the computation. However, in this work, we remark that they can be employed in a specific type of model selection problem: when we have several (possibly infinite) models belonging to the same parametric family (i.e., for tuning parameters of a parametric model). However, the quantities involved in this type of selection cannot be considered as Bayesian evidences: we suggest to use the name ``fake evidences'' (or ``areas under the likelihood'' in the case of uniform improper priors). We also show that, in this model selection scenario, using a diffuse prior and increasing its scale parameter asymptotically to infinity, we cannot recover the value of the area under the likelihood, obtained with a uniform improper prior. We first discuss it from a general point of view. Then we provide, as an applicative example, all the details for Bayesian regression models with nonlinear bases, considering two cases: the use of a uniform improper prior and the use of a Gaussian prior, respectively. A numerical experiment is also provided confirming and checking all the previous statements."}
{"id": "2602.22975", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.22975", "abs": "https://arxiv.org/abs/2602.22975", "authors": ["Stefanie Peschel", "Anne-Laure Boulesteix", "Erika von Mutius", "Christian L. Müller"], "title": "permApprox: a general framework for accurate permutation p-value approximation", "comment": null, "summary": "Permutation procedures are common practice in hypothesis testing when distributional assumptions about the test statistic are not met or unknown. With only few permutations, empirical p-values lie on a coarse grid and may even be zero when the observed test statistic exceeds all permuted values. Such zero p-values are statistically invalid and hinder multiple testing correction. Parametric tail modeling with the Generalized Pareto Distribution (GPD) has been proposed to address this issue, but existing implementations can again yield zero p-values when the estimated shape parameter is negative and the fitted distribution has a finite upper bound.\n  We introduce a method for accurate and zero-free p-value approximation in permutation testing, embedded in the permApprox workflow and R package. Building on GPD tail modeling, the method enforces a support constraint during parameter estimation to ensure valid extrapolation beyond the observed statistic, thereby strictly avoiding zero p-values. The workflow further integrates robust parameter estimation, data-driven threshold selection, and principled handling of hybrid p-values that are discrete in the bulk and continuous in the extreme tail.\n  Extensive simulations using two-sample t-tests and Wilcoxon rank-sum tests show that permApprox produces accurate, robust, and zero-free p-value approximations across a wide range of sample and effect sizes. Applications to single-cell RNA-seq and microbiome data demonstrate its practical utility: permApprox yields smooth and interpretable p-value distributions even with few permutations. By resolving the zero-p-value problem while preserving accuracy and computational efficiency, permApprox enables reliable permutation-based inference in high-dimensional and computationally intensive settings."}
{"id": "2602.23020", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.23020", "abs": "https://arxiv.org/abs/2602.23020", "authors": ["Sourbh Bhadane", "Joris M. Mooij", "Philip Boeken", "Onno Zoeter"], "title": "Testing Partially-Identifiable Causal Queries Using Ternary Tests", "comment": null, "summary": "We consider hypothesis testing of binary causal queries using observational data. Since the mapping of causal models to the observational distribution that they induce is not one-to-one, in general, causal queries are often only partially identifiable. When binary statistical tests are used for testing partially-identifiable causal queries, their results do not translate in a straightforward manner to the causal hypothesis testing problem. We propose using ternary (three-outcome) statistical tests to test partially-identifiable causal queries. We establish testability requirements that ternary tests must satisfy in terms of uniform consistency and present equivalent topological conditions on the hypotheses. To leverage the existing toolbox of binary tests, we prove that obtaining ternary tests by combining binary tests is complete. Finally, we demonstrate how topological conditions serve as a guide to construct ternary tests for two concrete causal hypothesis testing problems, namely testing the instrumental variable (IV) inequalities and comparing treatment efficacy."}
{"id": "2602.23045", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.23045", "abs": "https://arxiv.org/abs/2602.23045", "authors": ["Siyan Liu", "Qinglong Tian", "Chunlin Wang", "Pengfei Li"], "title": "Semiparametric Joint Inference for Sensitivity and Specificity at the Youden-Optimal Cut-Off", "comment": "23 pages, 2 figures, 6 tables", "summary": "Sensitivity and specificity evaluated at an optimal diagnostic cut-off are fundamental measures of classification accuracy when continuous biomarkers are used for disease diagnosis. Joint inference for these quantities is challenging because their estimators are evaluated at a common, data-driven threshold estimated from both diseased and healthy samples, inducing statistical dependence. Existing approaches are largely based on parametric assumptions or fully nonparametric procedures, which may be sensitive to model misspecification or lack efficiency in moderate samples. We propose a semiparametric framework for joint inference on sensitivity and specificity at the Youden-optimal cut-off under the density ratio model. Using maximum empirical likelihood, we derive estimators of the optimal threshold and the corresponding sensitivity and specificity, and establish their joint asymptotic normality. This leads to Wald-type and range-preserving logit-transformed confidence regions. Simulation studies show that the proposed method achieves accurate coverage with improved efficiency relative to existing parametric and nonparametric alternatives across a variety of distributional settings. An analysis of COVID-19 antibody data demonstrates the practical advantages of the proposed approach for diagnostic decision-making."}
{"id": "2602.23143", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.23143", "abs": "https://arxiv.org/abs/2602.23143", "authors": ["Alexis Boulin", "Axel Bücher"], "title": "Dimension Reduction in Multivariate Extremes via Latent Linear Factor Models", "comment": "36 pages", "summary": "We propose a new and interpretable class of high-dimensional tail dependence models based on latent linear factor structures. Specifically, extremal dependence of an observable vector is assumed to be driven by a lower-dimensional latent $K$-factor model, where $K \\ll d$, thereby inducing an explicit form of dimension reduction. Geometrically, this is reflected in the support of the associated spectral dependence measure, whose intrinsic dimension is at most $K-1$. The loading structure may additionally exhibit sparsity, meaning that each component is influenced by only a small number of latent factors, which further enhances interpretability and scalability. Under mild structural assumptions, we establish identifiability of the model parameters and provide a constructive recovery procedure based on a margin-free tail pairwise dependence matrix, which also yields practical rank-based estimation methods. The framework combines naturally with marginal tail models and is particularly well suited to high-dimensional settings. We illustrate its applicability in a spatial wind energy application, where the latent factor structure enables tractable estimation of the risk that a large proportion of turbines simultaneously fall below their cut-in wind speed thresholds."}
{"id": "2602.23257", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.23257", "abs": "https://arxiv.org/abs/2602.23257", "authors": ["Jizhou Liu", "Liang Zhong"], "title": "Randomization Tests in Switchback Experiments", "comment": null, "summary": "Switchback experiments--alternating treatment and control over time--are widely used when unit-level randomization is infeasible, outcomes are aggregated, or user interference is unavoidable. In practice, experimentation must support fast product cycles, so teams often run studies for limited durations and make decisions with modest samples. At the same time, outcomes in these time-indexed settings exhibit serial dependence, seasonality, and occasional heavy-tailed shocks, and temporal interference (carryover or anticipation) can render standard asymptotics and naive randomization tests unreliable. In this paper, we develop a randomization-test framework that delivers finite-sample valid, distribution-free p-values for several null hypotheses of interest using only the known assignment mechanism, without parametric assumptions on the outcome process. For causal effects of interests, we impose two primitive conditions--non-anticipation and a finite carryover horizon m--and construct conditional randomization tests (CRTs) based on an ex ante pooling of design blocks into \"sections,\" which yields a tractable conditional assignment law and ensures imputability of focal outcomes. We provide diagnostics for learning the carryover window and assessing non-anticipation, and we introduce studentized CRTs for a session-wise weak null that accommodates within-session seasonality with asymptotic validity. Power approximations under distributed-lag effects with AR(1) noise guide design and analysis choices, and simulations demonstrate favorable size and power relative to common alternatives. Our framework extends naturally to other time-indexed designs."}
{"id": "2602.23291", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.23291", "abs": "https://arxiv.org/abs/2602.23291", "authors": ["Tommy Tang", "Xinran Li", "Bo Li"], "title": "Identifiability of Treatment Effects with Unobserved Spatially Varying Confounders", "comment": "8 pages, 1 figure", "summary": "The study of causal effects in the presence of unmeasured spatially varying confounders has garnered increasing attention. However, a general framework for identifiability, which is critical for reliable causal inference from observational data, has yet to be advanced. In this paper, we study a linear model with various parametric model assumptions on the covariance structure between the unmeasured confounder and the exposure of interest. We establish identifiability of the treatment effect for many commonly 20 used spatial models for both discrete and continuous data, under mild conditions on the structure of observation locations and the exposure-confounder association. We also emphasize models or scenarios where identifiability may not hold, under which statistical inference should be conducted with caution."}
{"id": "2602.23311", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.23311", "abs": "https://arxiv.org/abs/2602.23311", "authors": ["Johannes Brachem", "Paul F. V. Wiemann", "Matthias Katzfuss"], "title": "Data-Efficient Generative Modeling of Non-Gaussian Global Climate Fields via Scalable Composite Transformations", "comment": null, "summary": "Quantifying uncertainty in future climate projections is hindered by the prohibitive computational cost of running physical climate models, which severely limits the availability of training data. We propose a data-efficient framework for emulating the internal variability of global climate fields, specifically designed to overcome these sample-size constraints. Inspired by copula modeling, our approach constructs a highly expressive joint distribution via a composite transformation to a multivariate standard normal space. We combine a nonparametric Bayesian transport map for spatial dependence modeling with flexible, spatially varying marginal models, essential for capturing non-Gaussian behavior and heavy-tailed extremes. These marginals are defined by a parametric model followed by a semi-parametric B-spline correction to capture complex distributional features. The marginal parameters are spatially smoothed using Gaussian-process priors with low-rank approximations, rendering the computational cost linear in the spatial dimension. When applied to global log-precipitation-rate fields at more than 50,000 grid locations, our stochastic surrogate achieves high fidelity, accurately quantifying the climate distribution's spatial dependence and marginal characteristics, including the tails. Using only 10 training samples, it outperforms a state-of-the-art competitor trained on 80 samples, effectively octupling the computational budget for climate research. We provide a Python implementation at https://github.com/jobrachem/ppptm ."}
{"id": "2602.23355", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.23355", "abs": "https://arxiv.org/abs/2602.23355", "authors": ["Jongwoo Choi", "Neil A. Spencer", "Jeffrey W. Miller"], "title": "Robust model selection using likelihood as data", "comment": null, "summary": "Model selection is a central task in statistics, but standard methods are not robust in misspecified settings where the true data-generating process (DGP) is not in the set of candidate models. The key limitation is that existing methods -- including information criteria and Bayesian posteriors -- do not quantify uncertainty about how well each candidate model approximates the true DGP. In this paper, we introduce a novel approach to model selection based on modeling the likelihood values themselves. Specifically, given $K$ candidate models and $n$ observations, we view the $n\\times K$ matrix of negative log-likelihood values as a random data matrix and observe that the expectation of each row is equal to the vector of Kullback--Leibler divergences between the $K$ models and the true DGP, up to an additive constant. We use a multivariate normal model to estimate and quantify uncertainty in this expectation, providing calibrated inferences for robust model selection under misspecification. The procedure is easy to compute, interpretable, and comes with theoretical guarantees, including consistency."}
