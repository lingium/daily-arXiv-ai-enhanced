<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 6]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 8]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Machine learning assisted state prediction of misspecified linear dynamical system via modal reduction](https://arxiv.org/abs/2601.05297)
*Rohan Vitthal Thorat,Rajdip Nayek*

Main category: stat.ML

TL;DR: 提出一个用于高维有限元结构动力学系统的模型形式误差估计与修正框架，通过高斯过程潜在力模型在模态域非参数化表示误差，结合贝叶斯滤波和神经网络映射，实现跨不同网格离散化的模型修正。


<details>
  <summary>Details</summary>
Motivation: 参数化模型常因几何、材料、阻尼或边界条件的简化而忽略关键物理效应，导致模型形式误差，损害预测精度。数字孪生需要在运行寿命期间保持保真度，因此需要有效处理这些建模不确定性。

Method: 1) 使用高斯过程潜在力模型在降维模态域非参数化表示模型误差；2) 采用线性贝叶斯滤波联合估计系统状态和误差；3) 将有限元系统投影到模态基上；4) 使用网格不变神经网络将模态状态映射到误差估计，实现跨不同网格离散化的模型修正而无需重新训练。

Result: 在五种模型形式误差场景（包括错误的梁理论、阻尼误设、边界条件误设、未建模材料非线性和局部损伤）中验证，该方法在未见激励下显著减少了位移和旋转预测误差。

Conclusion: 该方法为在固有建模不确定性下维持数字孪生精度提供了潜在途径，通过数据驱动的误差表征和计算可行的修正框架，提高了结构动力学预测的准确性。

Abstract: Accurate prediction of structural dynamics is imperative for preserving digital twin fidelity throughout operational lifetimes. Parametric models with fixed nominal parameters often omit critical physical effects due to simplifications in geometry, material behavior, damping, or boundary conditions, resulting in model form errors (MFEs) that impair predictive accuracy. This work introduces a comprehensive framework for MFE estimation and correction in high-dimensional finite element (FE) based structural dynamical systems. The Gaussian Process Latent Force Model (GPLFM) represents discrepancies non-parametrically in the reduced modal domain, allowing a flexible data-driven characterization of unmodeled dynamics. A linear Bayesian filtering approach jointly estimates system states and discrepancies, incorporating epistemic and aleatoric uncertainties. To ensure computational tractability, the FE system is projected onto a reduced modal basis, and a mesh-invariant neural network maps modal states to discrepancy estimates, permitting model rectification across different FE discretizations without retraining. Validation is undertaken across five MFE scenarios-including incorrect beam theory, damping misspecification, misspecified boundary condition, unmodeled material nonlinearity, and local damage demonstrating the surrogate model's substantial reduction of displacement and rotation prediction errors under unseen excitations. The proposed methodology offers a potential means to uphold digital twin accuracy amid inherent modeling uncertainties.

</details>


### [2] [A Bayesian Generative Modeling Approach for Arbitrary Conditional Inference](https://arxiv.org/abs/2601.05355)
*Qiao Liu,Wing Hung Wong*

Main category: stat.ML

TL;DR: BGM提出了一种贝叶斯生成建模方法，能够在不重新训练的情况下进行任意条件推断，解决了现有方法固定条件结构的限制。


<details>
  <summary>Details</summary>
Motivation: 现代数据分析需要灵活的任意条件推断P(X_B|X_A)，但现有方法受限于固定的条件结构，无法在训练后执行新的条件推断。

Method: 通过迭代贝叶斯更新算法学习X的生成模型，模型参数和潜在变量不断更新直至收敛，训练后即可获得任意条件分布而无需重新训练。

Result: BGM实现了优越的预测性能，具有良好校准的预测区间，表明单个学习模型可作为具有不确定性量化的条件预测通用引擎。

Conclusion: BGM框架结合了AI捕捉复杂关系的能力和贝叶斯原理，为现代数据科学的各种应用提供了有前景的通用条件推断框架。

Abstract: Modern data analysis increasingly requires flexible conditional inference P(X_B | X_A) where (X_A, X_B) is an arbitrary partition of observed variable X. Existing conditional inference methods lack this flexibility as they are tied to a fixed conditioning structure and cannot perform new conditional inference once trained. To solve this, we propose a Bayesian generative modeling (BGM) approach for arbitrary conditional inference without retraining. BGM learns a generative model of X through an iterative Bayesian updating algorithm where model parameters and latent variables are updated until convergence. Once trained, any conditional distribution can be obtained without retraining. Empirically, BGM achieves superior prediction performance with well calibrated predictive intervals, demonstrating that a single learned model can serve as a universal engine for conditional prediction with uncertainty quantification. We provide theoretical guarantees for the convergence of the stochastic iterative algorithm, statistical consistency and conditional-risk bounds. The proposed BGM framework leverages the power of AI to capture complex relationships among variables while adhering to Bayesian principles, emerging as a promising framework for advancing various applications in modern data science. The code for BGM is freely available at https://github.com/liuq-lab/bayesgm.

</details>


### [3] [A brief note on learning problem with global perspectives](https://arxiv.org/abs/2601.05441)
*Getachew K. Befekadu*

Main category: stat.ML

TL;DR: 论文提出了一种动态优化的委托-代理学习框架，其中代理具有全局视角，而委托方通过聚合信息解决高层次优化问题。


<details>
  <summary>Details</summary>
Motivation: 研究在委托-代理设置中的学习问题，其中代理能够基于委托方共享的聚合信息获得全局视角，而委托方需要解决包含条件矩限制模型的高层次优化问题。

Method: 采用动态优化的委托-代理框架，代理基于聚合信息获得全局视角，委托方使用经验似然估计器在条件矩限制模型下进行优化，同时考虑代理在样本外预测表现和委托方私有数据集。

Result: 提出了一个连贯的数学论证来刻画该抽象委托-代理学习框架背后的学习过程，但承认仍有一些概念和理论问题需要解决。

Conclusion: 该研究为委托-代理学习框架提供了必要的数学基础，但框架仍存在需要进一步解决的理论和概念问题。

Abstract: This brief note considers the problem of learning with dynamic-optimizing principal-agent setting, in which the agents are allowed to have global perspectives about the learning process, i.e., the ability to view things according to their relative importances or in their true relations based-on some aggregated information shared by the principal. Whereas, the principal, which is exerting an influence on the learning process of the agents in the aggregation, is primarily tasked to solve a high-level optimization problem posed as an empirical-likelihood estimator under conditional moment restrictions model that also accounts information about the agents' predictive performances on out-of-samples as well as a set of private datasets available only to the principal. In particular, we present a coherent mathematical argument which is necessary for characterizing the learning process behind this abstract principal-agent learning framework, although we acknowledge that there are a few conceptual and theoretical issues still need to be addressed.

</details>


### [4] [Multi-task Modeling for Engineering Applications with Sparse Data](https://arxiv.org/abs/2601.05910)
*Yigitcan Comlek,R. Murali Krishnan,Sandipp Krishnan Ravi,Amin Moghaddas,Rafael Giorjao,Michael Eff,Anirban Samaddar,Nesar S. Ramachandra,Sandeep Madireddy,Liping Wang*

Main category: stat.ML

TL;DR: 提出一个针对工程系统的多任务高斯过程框架，用于处理多源、多保真度数据，通过利用任务间关系来提高预测性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现代工程和科学工作流需要在相关任务和保真度级别上进行同时预测，其中高保真度数据稀缺且昂贵，而低保真度数据更丰富。需要解决数据稀疏性和任务相关性变化的问题。

Method: 引入一个专门针对工程系统的多任务高斯过程框架，该框架利用输出和保真度级别之间的任务间关系，通过量化这些关系来改进预测性能。

Result: 该框架在三个代表性场景中得到验证：Forrester函数基准测试、3D椭球空洞建模和摩擦搅拌焊接。通过量化和利用任务间关系，该框架提供了稳健且可扩展的解决方案。

Conclusion: 提出的MTGP框架为具有显著计算和实验成本的领域提供了强大的预测建模解决方案，支持明智的决策制定和高效的资源利用。

Abstract: Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of data sparsity and varying task correlations. The proposed framework leverages inter-task relationships across outputs and fidelity levels to improve predictive performance and reduce computational costs. The framework is validated across three representative scenarios: Forrester function benchmark, 3D ellipsoidal void modeling, and friction-stir welding. By quantifying and leveraging inter-task relationships, the proposed MTGP framework offers a robust and scalable solution for predictive modeling in domains with significant computational and experimental costs, supporting informed decision-making and efficient resource utilization.

</details>


### [5] [Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem](https://arxiv.org/abs/2601.06009)
*Sunia Tanweer,Firas A. Khasawneh*

Main category: stat.ML

TL;DR: 提出基于经典游程与穿越定理的实用框架，通过单一时序区分扩散随机过程与确定性信号，利用游程数量与二次变差的标度律进行判别。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如熵或递归分析）主观性强，需要开发理论认证的方法来可靠地区分扩散随机过程与确定性系统。

Method: 基于连续半鞅的游程与穿越定理，将经验游程计数与理论期望比较，通过比值K(ε)的log-log斜率偏离ε^{-2}律进行分类。

Result: 方法在典型随机系统、周期/混沌映射、加性白噪声系统及随机Duffing系统上验证有效，能可靠区分扩散过程与确定性系统。

Conclusion: 该非参数、无模型方法仅依赖连续半鞅的普适小尺度结构，为区分随机扩散与确定性动态提供了理论认证的实用工具。

Abstract: We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\varepsilon$ of excursions of magnitude at least $\varepsilon$ with the quadratic variation $[X]_T$ of the process. The scaling law holds universally for all continuous semimartingales with finite quadratic variation, including general Ito diffusions with nonlinear or state-dependent volatility, but fails sharply for deterministic systems -- thereby providing a theoretically-certfied method of distinguishing between these dynamics, as opposed to the subjective entropy or recurrence based state of the art methods. We construct a robust data-driven diffusion test. The method compares the empirical excursion counts against the theoretical expectation. The resulting ratio $K(\varepsilon)=N_{\varepsilon}^{\mathrm{emp}}/N_{\varepsilon}^{\mathrm{theory}}$ is then summarized by a log-log slope deviation measuring the $\varepsilon^{-2}$ law that provides a classification into diffusion-like or not. We demonstrate the method on canonical stochastic systems, some periodic and chaotic maps and systems with additive white noise, as well as the stochastic Duffing system. The approach is nonparametric, model-free, and relies only on the universal small-scale structure of continuous semimartingales.

</details>


### [6] [Manifold limit for the training of shallow graph convolutional neural networks](https://arxiv.org/abs/2601.06025)
*Johanna Tengler,Christoph Brune,José A. Iglesias*

Main category: stat.ML

TL;DR: 研究浅层图卷积神经网络在流形采样点云上的离散到连续一致性，证明正则化经验风险泛函的Γ收敛和全局最小化器的收敛性


<details>
  <summary>Details</summary>
Motivation: 为图卷积神经网络的训练提供形式化的网格和样本独立性保证，确保在不同分辨率的图上训练结果的一致性

Method: 从函数分析视角将图信号视为流形上函数的空间离散化，选择弱紧参数空间，对输出权重和偏置施加Sobolev正则性，离散参数空间继承谱衰减并受频率截断限制

Result: 证明了正则化经验风险最小化泛函的Γ收敛及其全局最小化器的收敛性，参数测度弱收敛，函数在紧集上一致收敛

Conclusion: 为浅层图卷积神经网络的训练提供了离散到连续的一致性理论框架，形式化了网格和样本独立性

Abstract: We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a manifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose low-frequency spectrum approximates that of the Laplace-Beltrami operator of the underlying smooth manifold, and shallow GCNNs of possibly infinite width are linear functionals on the space of measures on the parameter space. From this functional-analytic perspective, graph signals are seen as spatial discretizations of functions on the manifold, which leads to a natural notion of training data consistent across graph resolutions. To enable convergence results, the continuum parameter space is chosen as a weakly compact product of unit balls, with Sobolev regularity imposed on the output weight and bias, but not on the convolutional parameter. The corresponding discrete parameter spaces inherit the corresponding spectral decay, and are additionally restricted by a frequency cutoff adapted to the informative spectral window of the graph Laplacians. Under these assumptions, we prove $Γ$-convergence of regularized empirical risk minimization functionals and corresponding convergence of their global minimizers, in the sense of weak convergence of the parameter measures and uniform convergence of the functions over compact sets. This provides a formalization of mesh and sample independence for the training of such networks.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [7] [Representing asymmetric relationships by h-plots. Discovering the archetypal patterns of cross-journal citation relationships](https://arxiv.org/abs/2601.05400)
*Aleix Alcacer,Irene Epifanio*

Main category: stat.AP

TL;DR: 提出基于h-plot的可扩展多维标度方法，处理非对称邻近数据，通过嵌入变量而非对象本身，并支持原型分析，用于期刊引用关系可视化。


<details>
  <summary>Details</summary>
Motivation: 现有多维标度方法在处理非对称邻近数据时存在局限性，需要一种能够有效处理非对称关系、易于实现和评估的可扩展方法。

Method: 基于h-plot的方法，嵌入定义每个对象邻近关系的变量而非对象本身，使用欧氏距离衡量相似性，并结合原型分析（ADA）识别极端案例。

Result: 方法能够有效可视化期刊引用与被引的非对称关系，识别原型期刊，通过与其他方法比较显示良好性能，代码和数据可复现。

Conclusion: 提出的h-plot方法为处理非对称邻近数据提供了可扩展、易于实现的解决方案，结合原型分析增强了结果的可解释性，适用于非对称关系可视化分析。

Abstract: This work approaches the multidimensional scaling problem from a novel angle. We introduce a scalable method based on the h-plot, which inherently accommodates asymmetric proximity data. Instead of embedding the objects themselves, the method embeds the variables that define the proximity to or from each object. It is straightforward to implement, and the quality of the resulting representation can be easily evaluated. The methodology is illustrated by visualizing the asymmetric relationships between the citing and cited profiles of journals on a common map. Two profiles that are far apart (or close together) in the h-plot, as measured by Euclidean distance, are different (or similar), respectively. This representation allows archetypoid analysis (ADA) to be calculated. ADA is used to find archetypal journals (or extreme cases). We can represent the dataset as convex combinations of these archetypal journals, making the results easy to interpret, even for non-experts. Comparisons with other methodologies are carried out, showing the good performance of our proposal. Code and data are available for reproducibility.

</details>


### [8] [A latent factor approach to hyperspectral time series data for multivariate genomic prediction of grain yield in wheat](https://arxiv.org/abs/2601.05842)
*Jonathan F. Kunst,Killian A. C. Melsen,Willem Kruijer,José Crossa,Chris Maliepaard,Fred A. van Eeuwijk,Carel F. W. Peeters*

Main category: stat.AP

TL;DR: 该研究提出使用因子分析和Procrustes旋转处理高光谱表型数据，通过提取潜在变量进行多变量基因组预测，显著提高了小麦籽粒产量的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 植物育种中高维时间序列表型数据日益普遍，但分析和整合这些数据进行遗传分析和基因组预测仍然困难。需要开发有效方法来提取高光谱数据中的相关特征，提高基因组预测能力。

Method: 使用因子分析结合Procrustes旋转处理高光谱数据的遗传相关矩阵，提取潜在变量。基于CIMMYT 2014-2015年1,033个小麦基因型的数据，在三种灌溉处理下，通过载人飞机搭载高光谱传感器（385-850 nm，62个波段）测量多个时间点。采用多变量基因组预测模型整合高光谱数据的潜在变量。

Result: 通过整合高光谱数据的潜在变量，与单变量基因组预测相比，在相关性尺度上获得了0.1-0.3的绝对增益。确定了试验中重要的时间点及其与植物生长阶段的关系。

Conclusion: 该研究展示了如何结合领域知识和数据驱动方法，提高基因组预测能力并从高通量表型平台的传感器数据中获得新见解，为植物育种中的高维表型数据分析提供了有效框架。

Abstract: High-dimensional time series phenotypic data is becoming increasingly common within plant breeding programmes. However, analysing and integrating such data for genetic analysis and genomic prediction remains difficult. Here we show how factor analysis with Procrustes rotation on the genetic correlation matrix of hyperspectral secondary phenotype data can help in extracting relevant features for within-trial prediction. We use a subset of Centro Internacional de Mejoramiento de Maíz y Trigo (CIMMYT) elite yield wheat trial of 2014-2015, consisting of 1,033 genotypes. These were measured across three irrigation treatments at several timepoints during the season, using manned airplane flights with hyperspectral sensors capturing 62 bands in the spectrum of 385-850 nm. We perform multivariate genomic prediction using latent variables to improve within-trial genomic predictive ability (PA) of wheat grain yield within three distinct watering treatments. By integrating latent variables of the hyperspectral data in a multivariate genomic prediction model, we are able to achieve an absolute gain of .1 to .3 (on the correlation scale) in PA compared to univariate genomic prediction. Furthermore, we show which timepoints within a trial are important and how these relate to plant growth stages. This paper showcases how domain knowledge and data-driven approaches can be combined to increase PA and gain new insights from sensor data of high-throughput phenotyping platforms.

</details>


### [9] [Neural Methods for Multiple Systems Estimation Models](https://arxiv.org/abs/2601.05859)
*Joseph Marsh,Nathan A. Judd,Lax Chan,Rowland G. Seymour*

Main category: stat.AP

TL;DR: 提出基于神经贝叶斯估计器和神经后验估计器的模拟贝叶斯推断框架，用于解决多重系统估计中的计算瓶颈和数据稀疏问题，在隐藏人口规模估计中实现比传统方法更快更稳健的推断。


<details>
  <summary>Details</summary>
Motivation: 多重系统估计在定量社会学中至关重要，但实际应用受到不完善行政数据和计算限制的阻碍。现实数据集常因隐私问题而存在删失和缺失，而标准推断方法（如MLE和MCMC）在数据稀疏时可能计算不可行或无法收敛。

Method: 提出基于神经贝叶斯估计器和神经后验估计器的模拟贝叶斯推断框架。这些神经方法是摊销的：一旦训练完成，即可提供即时、计算高效的后验估计，特别适合计算资源有限的安全研究环境。

Result: 通过大量模拟研究证明，神经估计器在达到与MCMC相当的准确性的同时，速度提高了数个数量级，并且在传统采样器容易失败的稀疏设置中表现出更强的稳健性。在英国现代奴隶制流行率和英格兰东北部女性药物使用两个真实案例中验证了方法的有效性。

Conclusion: 神经贝叶斯估计器和神经后验估计器为多重系统估计提供了一种计算高效、稳健的替代方案，特别适合处理稀疏数据和计算受限的环境，在隐藏人口规模估计等社会学应用中具有重要实用价值。

Abstract: Estimating the size of hidden populations using Multiple Systems Estimation (MSE) is a critical task in quantitative sociology; however, practical application is often hindered by imperfect administrative data and computational constraints. Real-world datasets frequently suffer from censoring and missingness due to privacy concerns, while standard inference methods, such as Maximum Likelihood Estimation (MLE) and Markov chain Monte Carlo (MCMC), can become computationally intractable or fail to converge when data are sparse. To address these limitations, we propose a novel simulation-based Bayesian inference framework utilizing Neural Bayes Estimators (NBE) and Neural Posterior Estimators (NPE). These neural methods are amortized: once trained, they provide instantaneous, computationally efficient posterior estimates, making them ideal for use in secure research environments where computational resources are limited. Through extensive simulation studies, we demonstrate that neural estimators achieve accuracy comparable to MCMC while being orders of magnitude faster and robust to the convergence failures that plague traditional samplers in sparse settings. We demonstrate our method on two real-world cases estimating the prevalence of modern slavery in the UK and female drug use in North East England.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [10] [Model-based clustering using a new mixture of circular regressions](https://arxiv.org/abs/2601.05345)
*Sphiwe B. Skhosana,Najmeh Nakhaei Rad*

Main category: stat.ME

TL;DR: 提出了一种用于圆形响应变量的有限混合回归模型，以处理多模态数据，填补了文献空白。


<details>
  <summary>Details</summary>
Motivation: 在生物学、地质学和气象学等领域，圆形响应变量的回归模型很常见。传统模型通常假设响应变量服从von-Mises分布，但当响应变量呈现多模态时，这一假设就不适用了。尽管圆形数据的多模态现象普遍存在，但混合回归模型在这一领域的应用尚未得到关注。

Method: 提出了一种有限混合回归模型，适用于圆形响应变量和圆形/线性协变量。通过期望最大化（EM）算法开发了最大似然估计程序，用于模型参数估计。

Result: 通过广泛的模拟研究验证了所提模型和估计方法的实用性和性能。该模型还被证明可作为基于模型的聚类工具。最后，将模型应用于南非风电场的真实数据集，展示了其实际应用价值。

Conclusion: 该研究填补了圆形数据多模态回归建模的知识空白，提出的混合回归模型能有效处理多模态圆形响应变量，为相关领域提供了新的分析工具。

Abstract: Regression models, where the response variable is circular, are common in areas such as biology, geology and meteorology. A typical model assumes that the conditional distribution of the response follows a von-Mises distribution. However, this assumption is inadequate when the response variable is multimodal. For this reason, in this paper, a finite mixture of regressions model is proposed for the case of a circular response variable and a set of circular and/or linear covariates. Mixture models are very useful when the underlying population is multimodal. Despite the prevalence of multimodality in regression modelling of circular data, the use of mixtures of regressions has received no attention in the literature. This paper aims to close this knowledge gap. To estimate the proposed model, we develop a maximum likelihood estimation procedure via the Expectation-Maximization algorithm. An extensive simulation study is used to demonstrate the practical use and performance of the proposed model and estimation procedure. In addition, the model is shown to be useful as a model-based clustering tool. Lastly, the model is applied to a real dataset from a wind farm in South Africa.

</details>


### [11] [Archetypal cases for questionnaires with nominal multiple choice questions](https://arxiv.org/abs/2601.05392)
*Aleix Alcacer,Irene Epifanio*

Main category: stat.ME

TL;DR: 首次将原型分析应用于名义数据，识别问卷调查中名义多选问题的典型案例，在德国信用数据集上验证方法优势


<details>
  <summary>Details</summary>
Motivation: 原型分析通常用于解释观测值作为纯（极端）模式的凸组合，但之前主要应用于连续数据。本研究旨在将原型分析扩展到名义观测数据，特别是问卷调查中的名义多选问题，以更好地理解名义数据集。

Method: 提出将原型分析应用于名义观测数据的方法，识别样本中实际存在的原型案例（原型点）。该方法专门针对具有单一答案选项的名义多选问卷问题。与原型分析和概率原型分析方法进行比较。

Result: 在德国信用数据集上展示了该方法的优势，证明原型分析能够有效识别名义数据中的典型模式，相比传统原型分析和概率原型分析具有更好的解释性和实用性。

Conclusion: 原型分析可以成功应用于名义数据，为分析问卷调查中的名义多选问题提供了有效工具，能够增强对名义数据集的理解，类似于其在多元数据中的应用效果。

Abstract: Archetypal analysis serves as an exploratory tool that interprets a collection of observations as convex combinations of pure (extreme) patterns. When these patterns correspond to actual observations within the sample, they are termed archetypoids. For the first time, we propose applying archetypoid analysis to nominal observations, specifically for identifying archetypal cases from questionnaires featuring nominal multiple-choice questions with a single possible answer. This approach can enhance our understanding of a nominal data set, similar to its application in multivariate contexts. We compare this methodology with the use of archetype analysis and probabilistic archetypal analysis and demonstrate the benefits of this methodology using a real-world example: the German credit dataset.

</details>


### [12] [Uncertainty Analysis of Experimental Parameters for Reducing Warpage in Injection Molding](https://arxiv.org/abs/2601.05396)
*Yezhuo Li,Fan Zhang,Dhanashree Shinde,Qiong Zhang,Sai Pradeep,Srikanth Pilla,Gang Li*

Main category: stat.ME

TL;DR: 提出一个数据驱动的贝叶斯框架，用于优化注塑成型工艺参数以减少翘曲变形，并量化最优工艺设置的不确定性，同时开发蒙特卡洛边界分析方法来可视化缺陷边界。


<details>
  <summary>Details</summary>
Motivation: 注塑成型是关键的制造工艺，但翘曲变形控制面临挑战。传统的基于仿真的优化方法通常忽略模型参数的不确定性，需要一种能够量化不确定性和提供稳健解决方案的方法。

Method: 采用数据驱动框架，使用多项式回归模型作为注塑成型仿真的代理模型。采用贝叶斯框架估计回归系数的后验分布，生成最优决策的分布而非单点估计。开发蒙特卡洛边界分析方法，构建响应面零水平集的置信带。

Result: 该方法成功优化了模具温度、注射速度、保压压力和保压时间四个关键工艺参数，找到了稳定的工艺设置，并在参数空间中清晰地标明了缺陷边界。

Conclusion: 提出的贝叶斯框架能够有效处理注塑成型优化中的不确定性，提供稳健的工艺参数设置，并通过边界分析方法可视化缺陷区域，为实际制造过程提供了更可靠的决策支持。

Abstract: Injection molding is a critical manufacturing process, but controlling warpage remains a major challenge due to complex thermomechanical interactions. Simulation-based optimization is widely used to address this, yet traditional methods often overlook the uncertainty in model parameters. In this paper, we propose a data-driven framework to minimize warpage and quantify the uncertainty of optimal process settings. We employ polynomial regression models as surrogates for the injection molding simulations of a box-shaped part. By adopting a Bayesian framework, we estimate the posterior distribution of the regression coefficients. This approach allows us to generate a distribution of optimal decisions rather than a single point estimate, providing a measure of solution robustness. Furthermore, we develop a Monte Carlo-based boundary analysis method. This method constructs confidence bands for the zero-level sets of the response surfaces, helping to visualize the regions where warpage transitions between convex and concave profiles. We apply this framework to optimize four key process parameters: mold temperature, injection speed, packing pressure, and packing time. The results show that our approach finds stable process settings and clearly marks the boundaries of defects in the parameter space.

</details>


### [13] [Multi-Group Quadratic Discriminant Analysis via Projection](https://arxiv.org/abs/2601.05415)
*Yuchao Wang,Tianying Wang*

Main category: stat.ME

TL;DR: 提出了一种用于多组分类的MGQDA方法，基于二次判别分析，通过降维处理高维预测变量，能捕捉非线性决策边界和组间协方差异质性，在模拟和基因表达应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多组分类在流行病学、基因组学、金融和图像识别等领域有广泛应用，但现有方法多集中于二分类问题，对多组分类的扩展有限，特别是处理非线性决策边界和组间协方差异质性方面存在不足。

Method: 开发了多组二次判别分析(MGQDA)方法，将高维预测变量投影到低维子空间，基于二次判别分析框架，能够处理非线性决策边界和组间协方差结构的异质性。

Result: MGQDA在模拟研究和基因表达应用中表现出竞争性或改进的预测性能，同时能够选择组间特异性的信息变量，理论分析证明了变量选择的一致性。

Conclusion: MGQDA为高维多组分类问题提供了有效的解决方案，能够处理复杂的数据结构，具有实际应用价值，特别是在需要捕捉组间异质性和非线性关系的场景中。

Abstract: Multi-group classification arises in many prediction and decision-making problems, including applications in epidemiology, genomics, finance, and image recognition. Although classification methods have advanced considerably, much of the literature focuses on binary problems, and available extensions often provide limited flexibility for multi-group settings. Recent work has extended linear discriminant analysis to multiple groups, but more general methods are still needed to handle complex structures such as nonlinear decision boundaries and group-specific covariance patterns.
  We develop Multi-Group Quadratic Discriminant Analysis (MGQDA), a method for multi-group classification built on quadratic discriminant analysis. MGQDA projects high-dimensional predictors onto a lower-dimensional subspace, which enables accurate classification while capturing nonlinearity and heterogeneity in group-specific covariance structures. We derive theoretical guarantees, including variable selection consistency, to support the reliability of the procedure. In simulations and a gene-expression application, MGQDA achieves competitive or improved predictive performance compared with existing methods while selecting group-specific informative variables, indicating its practical value for high-dimensional multi-group classification problems. Supplementary materials for this article are available online.

</details>


### [14] [Minimax Optimal Robust Sparse Regression with Heavy-Tailed Designs: A Gradient-Based Approach](https://arxiv.org/abs/2601.05669)
*Kaiyuan Zhou,Xiaoyu Zhang,Wenyang Zhang,Di Wang*

Main category: stat.ME

TL;DR: 论文提出RIGHT框架，用于高维稀疏回归中的重尾噪声和设计矩阵问题，揭示了线性回归中误差率由噪声尾指数决定、样本复杂度由设计尾指数决定的有趣解耦现象。


<details>
  <summary>Details</summary>
Motivation: 传统算法在重尾噪声和重尾设计矩阵下失效，因为重尾协变量会扭曲经验风险几何。需要开发能在这种恶劣条件下工作的稳健算法。

Method: 提出RIGHT（Robust Iterative Gradient descent with Hard Thresholding）框架，使用稳健梯度估计器，无需高阶矩条件，结合硬阈值处理稀疏性。

Result: 在线性回归中发现解耦现象：估计误差率由噪声尾指数决定，而稳定性所需的样本复杂度由设计尾指数决定。对于逻辑回归，有界梯度自然使估计器对重尾设计具有稳健性。

Conclusion: RIGHT框架在重尾噪声和设计矩阵下实现了最优的估计精度和样本复杂度，无需样本分割或总体风险存在假设，为高维稀疏回归提供了统一的稳健解决方案。

Abstract: We investigate high-dimensional sparse regression when both the noise and the design matrix exhibit heavy-tailed behavior. Standard algorithms typically fail in this regime, as heavy-tailed covariates distort the empirical risk geometry. We propose a unified framework, Robust Iterative Gradient descent with Hard Thresholding (RIGHT), which employs a robust gradient estimator to bypass the need for higher-order moment conditions. Our analysis reveals a fundamental decoupling phenomenon: in linear regression, the estimation error rate is governed by the noise tail index, while the sample complexity required for stability is governed by the design tail index. This implies that while heavy-tailed noise limits precision, heavy-tailed designs primarily raise the sample size barrier for convergence. In contrast, for logistic regression, we show that the bounded gradient naturally robustifies the estimator against heavy-tailed designs, restoring standard parametric rates. We derive matching minimax lower bounds to prove that RIGHT achieves optimal estimation accuracy and sample complexity across these regimes, without requiring sample splitting or the existence of the population risk.

</details>


### [15] [Conditional Cauchy-Schwarz Divergence for Time Series Analysis: Kernelized Estimation and Applications in Clustering and Fraud Detection](https://arxiv.org/abs/2601.05711)
*Jiayi Wang*

Main category: stat.ME

TL;DR: 提出条件柯西-施瓦茨散度作为时间序列分析的对称无密度度量，开发了基于核的估计器，并在时间序列聚类和欺诈检测中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 需要一种对称且不依赖密度估计的度量方法来进行时间序列分析，特别是在需要条件分布比较的应用中，如时间序列聚类和欺诈检测。

Method: 使用径向基函数核在条件和输出空间上构建C-CSD估计器，采用对称对数形式加ε岭稳定化，基于四分位距的鲁棒带宽选择，中位数启发式带宽应用于窗口向量，有效秩滤波避免退化核。

Result: 在基准时间序列数据集和交易欺诈检测数据集上，该方法展示了稳定的估计性能，在严格无信息泄露的评估协议下实现了有效性能。

Conclusion: 条件柯西-施瓦茨散度是一种实用的对称无密度度量，通过核估计器和数值稳定化技术，在时间序列分析和欺诈检测中表现出色，为条件分布比较提供了有效工具。

Abstract: We study the conditional Cauchy-Schwarz divergence (C-CSD) as a symmetric and density-free measure for time series analysis. We derive a practical kernel based estimator using radial basis function kernels on both the condition and output spaces, together with numerical stabilizations including a symmetric logarithmic form with an epsilon ridge and a robust bandwidth selection rule based on the interquartile range. Median heuristic bandwidths are applied to window vectors, and effective rank filtering is used to avoid degenerate kernels.
  We demonstrate the framework in two applications. In time series clustering, conditioning on the time index and comparing scalar series values yields a pairwise C-CSD dissimilarity. Bandwidths are selected on the training split, after which precomputed distance k-medoids clustering is performed on the test split and evaluated using normalized mutual information. In fraud detection, conditioning on sliding transaction windows and comparing the magnitude of value changes with categorical and merchant change indicators, each query window is scored by contrasting a global normal reference mixture against a same account local history mixture with recency decay and change flag weighting. Account level decisions are obtained by aggregating window scores using the maximum value. Experiments on benchmark time series datasets and a transactional fraud detection dataset demonstrate stable estimation and effective performance under a strictly leak free evaluation protocol.

</details>


### [16] [Estimating optimal interpretable individualized treatment regimes from a classification perspective using adaptive LASSO](https://arxiv.org/abs/2601.05875)
*Yunshu Zhang,Shu Yang,Wendy Ye,Ilya Lipkovich,Douglas E. Faries*

Main category: stat.ME

TL;DR: 提出使用自适应LASSO从真实世界数据中估计可解释的线性个体化治疗规则，在变量选择和治疗效果估计方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于真实世界数据的个体化治疗规则估计方法存在可解释性差和收敛问题，需要开发既能提供可解释性又能保持稀疏性的方法

Method: 使用自适应LASSO算法，通过最小化带有各种惩罚项的风险函数来估计线性个体化治疗规则，结合不同的治疗对比估计方法

Result: 自适应LASSO在变量选择正确率方面表现最佳，增强逆概率加权结合Super Learner在治疗对比估计中表现最好，整体方法在价值函数和变量选择方面优于因果森林和R-learning

Conclusion: 提出的算法能在个体化治疗规则的可解释性（通过选择少量重要变量）和其价值之间取得良好平衡

Abstract: Real-world data (RWD) gains growing interests to provide a representative sample of the population for selecting the optimal treatment options. However, existing complex black box methods for estimating individualized treatment rules (ITR) from RWD have problems in interpretability and convergence. Providing an interpretable and sparse ITR can be used to overcome the limitation of existing methods. We developed an algorithm using Adaptive LASSO to predict optimal interpretable linear ITR in the RWD. To encourage sparsity, we obtain an ITR by minimizing the risk function with various types of penalties and different methods of contrast estimation. Simulation studies were conducted to select the best configuration and to compare the novel algorithm with the existing state-of-the-art methods. The proposed algorithm was applied to RWD to predict the optimal interpretable ITR. Simulations show that adaptive LASSO had the highest rates of correctly selected variables and augmented inverse probability weighting with Super Learner performed best for estimating treatment contrast. Our method had a better performance than causal forest and R-learning in terms of the value function and variable selection. The proposed algorithm can strike a balance between the interpretability of estimated ITR (by selecting a small set of important variables) and its value.

</details>


### [17] [Negative binomial models for development triangles of counts](https://arxiv.org/abs/2601.05964)
*Luis E. Nieto-Barajas,Rodrigo S. Targino*

Main category: stat.ME

TL;DR: 提出基于负二项分布的索赔预测模型，处理过度分散问题，并扩展到考虑发展年间的依赖性


<details>
  <summary>Details</summary>
Motivation: 现有索赔预测方法包括非参数模型（链梯法）、半参数模型（过度分散泊松）和全参数模型，但需要能处理过度分散的更好模型

Method: 使用负二项分布建立索赔预测模型，先假设随机变量独立，然后推广到考虑发展年间的依赖性，两种情况下边际分布均为负二项分布

Result: 研究了模型性质并进行贝叶斯推断，通过模拟和真实数据集展示了模型性能

Conclusion: 负二项分布模型能有效处理索赔预测中的过度分散问题，且可扩展到考虑依赖性，为精算实践提供了有用工具

Abstract: Prediction of outstanding claims has been done via nonparametric models (chain ladder), semiparametric models (overdispersed poisson) or fully parametric models. In this paper, we propose models based on negative binomial distributions for the prediction of outstanding number of claims, which are particularly useful to account for overdispersion. We first assume independence of random variables and introduce appropriate notation. Later, we generalise the model to account for dependence across development years. In both cases, the marginal distributions are negative binomials. We study the properties of the models and carry out bayesian inference. We illustrate the performance of the models with simulated and real datasets.

</details>
