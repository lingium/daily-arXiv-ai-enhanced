<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 8]
- [stat.AP](#stat.AP) [Total: 6]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.OT](#stat.OT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 14]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming](https://arxiv.org/abs/2512.08948)
*Yihang Gao,Michael K. Ng,Michael W. Mahoney,Sen Na*

Main category: stat.ML

TL;DR: 提出了一种用于带约束随机优化问题的在线统计推断方法SSQP，该方法结合动量技术消除偏差，实现全局收敛和局部渐近正态性，达到原始-对偶渐近极小极大最优性。


<details>
  <summary>Details</summary>
Motivation: 带等式和不等式约束的随机优化问题在统计学和机器学习中广泛存在，如约束M估计、物理信息模型、安全强化学习和算法公平性等。现有方法在处理约束随机问题时面临步方向偏差的挑战，且通常需要投影算子，这在非线性问题中难以实现。

Method: 开发了随机序列二次规划(SSQP)方法，通过顺序执行目标函数的二次近似和约束的线性近似来计算步方向。采用动量式梯度移动平均技术来消除步方向的偏差。方法还提供了实用的协方差矩阵估计器。

Result: 方法实现了全局几乎必然收敛，并表现出局部渐近正态性，具有Hájek和Le Cam意义下的最优原始-对偶极限协方差矩阵。在基准非线性问题、约束广义线性模型和投资组合分配问题上，实验显示了方法的优越性能。

Conclusion: SSQP方法是第一个完全在线的方法，无需依赖约束集上的投影算子就能达到原始-对偶渐近极小极大最优性。该方法不仅高效解决约束随机问题，还能为实际应用提供有效且实用的在线推断。

Abstract: We study online statistical inference for the solutions of stochastic optimization problems with equality and inequality constraints. Such problems are prevalent in statistics and machine learning, encompassing constrained $M$-estimation, physics-informed models, safe reinforcement learning, and algorithmic fairness. We develop a stochastic sequential quadratic programming (SSQP) method to solve these problems, where the step direction is computed by sequentially performing a quadratic approximation of the objective and a linear approximation of the constraints. Despite having access to unbiased estimates of population gradients, a key challenge in constrained stochastic problems lies in dealing with the bias in the step direction. As such, we apply a momentum-style gradient moving-average technique within SSQP to debias the step. We show that our method achieves global almost-sure convergence and exhibits local asymptotic normality with an optimal primal-dual limiting covariance matrix in the sense of Hájek and Le Cam. In addition, we provide a plug-in covariance matrix estimator for practical inference. To our knowledge, the proposed SSQP method is the first fully online method that attains primal-dual asymptotic minimax optimality without relying on projection operators onto the constraint set, which are generally intractable for nonlinear problems. Through extensive experiments on benchmark nonlinear problems, as well as on constrained generalized linear models and portfolio allocation problems using both synthetic and real data, we demonstrate superior performance of our method, showing that the method and its asymptotic behavior not only solve constrained stochastic problems efficiently but also provide valid and practical online inference in real-world applications.

</details>


### [2] [WTNN: Weibull-Tailored Neural Networks for survival analysis](https://arxiv.org/abs/2512.09163)
*Gabrielle Rives,Olivier Lopez,Nicolas Bousquet*

Main category: stat.ML

TL;DR: 本文提出WTNN，一种专门为Weibull生存分析设计的神经网络框架，能够处理代理指标和右删失数据，并整合先验知识来改进传统回归模型。


<details>
  <summary>Details</summary>
Motivation: 动机源于分析在高度变化和苛刻环境下运行的军用车辆车队，以及现有方法在处理代理指标、右删失数据和复杂协变量关系时的局限性。

Method: 提出WTNN神经网络框架，专门针对Weibull分布设计，能够将时间相关协变量作为分布参数的函数，并整合关于最有影响力协变量的定性先验知识，保持与Weibull分布形状和结构的一致性。

Result: 通过数值实验表明，该方法能够在代理指标和右删失数据上可靠训练，能够产生稳健且可解释的生存预测，能够改进现有方法。

Conclusion: WTNN框架为Weibull生存研究提供了新的神经网络建模方法，能够处理复杂协变量关系并整合先验知识，在军事车辆等实际应用中表现出优于传统方法的性能。

Abstract: The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches.

</details>


### [3] [Robust and Sparse Estimation of Unbounded Density Ratio under Heavy Contamination](https://arxiv.org/abs/2512.09266)
*Ryosuke Nagumo,Hironori Fujisawa*

Main category: stat.ML

TL;DR: 加权密度比估计在污染数据中具有非渐近稀疏一致性，为密度比估计和鲁棒估计提供了双重强鲁棒性分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法在污染环境下的密度比估计缺乏非渐近分析，特别是对于无界密度比和重污染情况。加权密度比估计在渐近视角下表现出双重强鲁棒性，但需要非渐近框架来验证其实际性能。

Method: 采用加权密度比估计方法，该方法假设加权密度比函数有界。在重污染情况下，该方法要求满足两个条件之一：(i) 污染比例较小，或(ii) 异常值的加权值较小。

Result: 加权密度比估计在非渐近框架下实现了稀疏一致性，即使在重污染情况下也能保持强鲁棒性。这是首次在重污染下对强鲁棒性进行非渐近分析。

Conclusion: 加权密度比估计在污染环境中具有双重强鲁棒性，为密度比估计和鲁棒估计提供了统一的非渐近分析框架，解决了无界密度比估计和重污染鲁棒性两大挑战。

Abstract: We examine the non-asymptotic properties of robust density ratio estimation (DRE) in contaminated settings. Weighted DRE is the most promising among existing methods, exhibiting doubly strong robustness from an asymptotic perspective. This study demonstrates that Weighted DRE achieves sparse consistency even under heavy contamination within a non-asymptotic framework. This method addresses two significant challenges in density ratio estimation and robust estimation. For density ratio estimation, we provide the non-asymptotic properties of estimating unbounded density ratios under the assumption that the weighted density ratio function is bounded. For robust estimation, we introduce a non-asymptotic framework for doubly strong robustness under heavy contamination, assuming that at least one of the following conditions holds: (i) contamination ratios are small, and (ii) outliers have small weighted values. This work provides the first non-asymptotic analysis of strong robustness under heavy contamination.

</details>


### [4] [Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression](https://arxiv.org/abs/2512.09275)
*Weiyi He,Yue Xing*

Main category: stat.ML

TL;DR: 该论文首次分析了可训练位置编码对Transformer在上下文回归任务中泛化性能的影响，发现位置编码会系统性地扩大泛化差距，并在对抗攻击下放大模型脆弱性。


<details>
  <summary>Details</summary>
Motivation: 位置编码是Transformer架构的核心组件，但其对模型泛化能力和鲁棒性的影响尚不明确。本文旨在填补这一研究空白，分析可训练位置编码如何影响Transformer的泛化性能。

Method: 采用理论分析方法，为单层Transformer在上下文回归任务中推导泛化边界，特别考虑了完全可训练的位置编码模块。同时扩展到对抗设置，推导对抗性Rademacher泛化边界。

Result: 研究发现：1）位置编码会系统性地扩大泛化差距；2）在对抗攻击下，有位置编码和无位置编码模型之间的差距会被放大，表明位置编码会放大模型的脆弱性。这些理论结果通过模拟研究得到了实证验证。

Conclusion: 本文建立了一个新的理论框架来理解带有位置编码的上下文学习中的干净和对抗泛化，揭示了位置编码对模型泛化性能和鲁棒性的负面影响。

Abstract: Positional encoding (PE) is a core architectural component of Transformers, yet its impact on the Transformer's generalization and robustness remains unclear. In this work, we provide the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for a completely trainable PE module. Our result shows that PE systematically enlarges the generalization gap. Extending to the adversarial setting, we derive the adversarial Rademacher generalization bound. We find that the gap between models with and without PE is magnified under attack, demonstrating that PE amplifies the vulnerability of models. Our bounds are empirically validated by a simulation study. Together, this work establishes a new framework for understanding the clean and adversarial generalization in ICL with PE.

</details>


### [5] [Estimation of Stochastic Optimal Transport Maps](https://arxiv.org/abs/2512.09499)
*Sloan Nietert,Ziv Goldfeld*

Main category: stat.ML

TL;DR: 提出一种新的随机最优传输映射度量方法，在放宽传统假设条件下实现近最优有限样本风险边界


<details>
  <summary>Details</summary>
Motivation: 传统最优传输映射理论依赖严格假设（Brenier定理、绝对连续源分布等），在现实问题中这些条件常不满足或无法验证，此时只能通过随机映射实现最优传输

Method: 引入新的随机映射传输质量度量指标，开发计算高效且具有近最优有限样本风险边界的映射估计器，并考虑对抗性样本污染情况

Result: 建立了首个通用映射估计理论，兼容现实应用中可能存在的内在随机最优传输场景，实验验证了理论有效性

Conclusion: 该框架扩展了最优传输映射估计理论的应用范围，为现实世界中需要随机映射的传输问题提供了理论基础和实用工具

Abstract: The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic.

</details>


### [6] [Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport](https://arxiv.org/abs/2512.09530)
*Antonio Candelieri,Alessandro Quadrio*

Main category: stat.ML

TL;DR: 该论文从最优传输视角研究自注意力训练，提出基于OT的表格分类替代方法，通过追踪训练过程中间投影并评估其演化，发现自注意力映射常近似OT最优耦合但训练轨迹低效，进而设计OT算法生成类别特定高斯分布并与数据对齐，在保持准确率的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 研究自注意力训练过程，从最优传输理论视角分析其效率和收敛特性，旨在解决自注意力训练轨迹低效的问题，并为表格分类任务提供计算更高效的替代方案。

Method: 1) 追踪自注意力层训练过程中的中间投影，使用Wasserstein距离、Monge间隙、最优性和效率等离散OT指标评估演化；2) 提出OT算法：生成类别特定虚拟高斯分布，计算与数据的OT对齐，训练MLP泛化该映射；3) 在二类、三类分类任务和生物医学数据集上进行实验。

Result: 1) 最终自注意力映射常近似OT最优耦合，但训练轨迹低效；2) MLP部分在合成数据上预训练可部分改善收敛但对初始化敏感；3) OT算法在保持与Transformer相当准确率的同时降低计算成本，在标准化输入下扩展性更好，但性能依赖于虚拟几何设计；4) 所有实验在R中实现。

Conclusion: 最优传输为分析自注意力训练提供了有效框架，提出的OT算法可作为Transformer的替代方案，在表格分类任务中实现计算效率与准确率的平衡，但需要仔细设计虚拟分布几何结构。

Abstract: This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.
  Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.

</details>


### [7] [Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search](https://arxiv.org/abs/2512.09538)
*Ekaterina Fadeeva,Maiya Goloburda,Aleksandr Rubashevskii,Roman Vashurin,Artem Shelmanov,Preslav Nakov,Mrinmaya Sachan,Maxim Panov*

Main category: stat.ML

TL;DR: 提出使用beam search替代multinomial sampling进行一致性不确定性量化，在短问答任务中减少重复生成和方差，提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基于一致性的不确定性量化方法依赖multinomial sampling，但在短问答中容易产生重复结果且方差大，需要更稳定的方法

Method: 引入使用beam search生成候选答案的新方法，提供理论下界证明beam search在特定概率质量下比multinomial sampling误差更小

Result: 在六个QA数据集上实证评估，相比multinomial sampling有稳定改进，达到最先进的UQ性能

Conclusion: beam search方法在一致性不确定性量化中优于multinomial sampling，提供更可靠的不确定性估计

Abstract: Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance.

</details>


### [8] [Supervised learning pays attention](https://arxiv.org/abs/2512.09912)
*Erin Craig,Robert Tibshirani*

Main category: stat.ML

TL;DR: 提出一种基于注意力加权的监督学习方法，用于表格数据，为每个测试点拟合个性化模型，同时保持模型简单性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法如lasso回归和梯度提升在处理异质性数据时缺乏灵活性，无法为每个预测点拟合个性化模型，同时保持模型的简单性和可解释性。

Method: 通过注意力加权为每个测试观测拟合局部模型，注意力是一种监督相似性度量，强调对结果预测重要的特征和交互作用。该方法可应用于时间序列、空间数据，并能通过注意力加权残差校正适应预训练树模型的分布偏移。

Result: 在真实和模拟数据集上，注意力加权提高了预测性能同时保持可解释性。理论证明在已知子组结构的混合模型数据生成过程中，注意力加权线性模型比标准线性模型具有更低的均方误差。

Conclusion: 注意力加权方法能够灵活适应异质性数据，为每个测试点提供个性化模型，同时保持模型简单性和可解释性，在表格数据监督学习中具有实用价值。

Abstract: In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.
  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [9] [Refuting "Debunking the GAMLSS Myth: Simplicity Reigns in Pulmonary Function Diagnostics"](https://arxiv.org/abs/2512.09179)
*Robert A. Rigby,Mikis D. Stasinopoulos,Achim Zeileis,Sanja Stanojevic,Gillian Heller,Fernanda de Bastiani,Thomas Kneib,Andreas Mayr,Reto Stauffer,Nikolaus Umlauf*

Main category: stat.AP

TL;DR: 该文是对Zavorsky (2025)关于肺功能测试参考方程研究的评论，作者不同意Zavorsky关于分段线性回归(SLR)与GAMLSS模型相比具有同等预测精度且更简单易用的观点。


<details>
  <summary>Details</summary>
Motivation: 对Zavorsky (2025)在《Respiratory Medicine》上发表的文章进行评论和讨论，指出其方法存在根本性问题，旨在澄清肺功能测试参考方程建模中的关键问题。

Method: 通过学术评论的形式，针对Zavorsky文章中的两个主要论点进行反驳：1) SLR与GAMLSS预测精度相当；2) SLR比GAMLSS更简单、更简洁、更易于使用。

Result: 评论者不同意Zavorsky的两个核心观点，认为SLR在预测精度上不如GAMLSS，且GAMLSS虽然复杂但提供了更准确的参考方程，而SLR的"简单性"可能以牺牲准确性为代价。

Conclusion: 在肺功能测试参考方程建模中，GAMLSS作为GLI采用的标准方法，虽然复杂但提供了更准确的结果，而SLR的简化方法可能不适合临床应用，需要更严谨的统计方法。

Abstract: We read with interest the above article by Zavorsky (2025, Respiratory Medicine, doi:10.1016/j.rmed.2024.107836) concerning reference equations for pulmonary function testing. The author compares a Generalized Additive Model for Location, Scale, and Shape (GAMLSS), which is the standard adopted by the Global Lung Function Initiative (GLI), with a segmented linear regression (SLR) model, for pulmonary function variables. The author presents an interesting comparison; however there are some fundamental issues with the approach. We welcome this opportunity for discussion of the issues that it raises. The author's contention is that (1) SLR provides "prediction accuracies on par with GAMLSS"; and (2) the GAMLSS model equations are "complicated and require supplementary spline tables", whereas the SLR is "more straightforward, parsimonious, and accessible to a broader audience". We respectfully disagree with both of these points.

</details>


### [10] [Access to healthcare for people with Alzheimer's Diseases and related dementias](https://arxiv.org/abs/2512.09217)
*Saeed Saleh Namadi,Jie Chen,Deb Niemeier*

Main category: stat.AP

TL;DR: 该研究分析了马里兰州阿尔茨海默病及相关痴呆症（ADRD）的医疗可及性与地理差异，发现医疗资源分布不均导致农村地区存在显著差异，服务不足区域ADRD死亡率更高但诊断率较低。


<details>
  <summary>Details</summary>
Motivation: ADRD影响全球数百万人，但在少数族裔和社会经济弱势群体中存在显著的诊断和护理差异。研究旨在调查马里兰州ADRD密度与医疗可及性之间的关系，识别服务不足和服务过度的区域，重点关注护理的地理差异。

Method: 使用ICD-10编码识别2023年马里兰州ADRD患者；采用核密度两步移动搜索法（KD2SFCA）测量医疗可及性；使用基尼系数和t检验分析城乡差异；应用热点分析（Getis-Ord Gi*）和局部双变量关系分析评估空间相关性；应用主成分分析（PCA）计算健康风险指数。

Result: 医院可及性分布不均；ADRD死亡率在医疗服务不足、医院较少的区域更高；热点分析显示马里兰州东部和南部存在高死亡率区域；中部地区死亡率较低但医院设施更多；东部马里兰州贫困程度较高的区域周围存在可及性较低、健康风险指数较高的区域。

Conclusion: 医院可及性分布不均造成了显著的农村差异。马里兰州东部和南部医疗服务可及性不足的区域表现出高ADRD死亡率，但诊断率较低，这表明许多ADRD病例未被诊断、诊断不足或治疗延迟。

Abstract: Background: Alzheimer's Disease and Related Dementias (ADRD) affects millions worldwide. Significant disparities exist in ADRD diagnosis and care, disproportionately impacting minority and socioeconomically vulnerable populations Objective: In this study, we investigate the relationship between ADRD density and accessibility to healthcare. We identify underserved and overserved areas in Maryland based on diagnosed cases and mortality due to ADRD, focusing on geographic disparities in care. Methods: 2023 Maryland ADRD patients were identified using ICD-10 codes from. Accessibility was measured using the Kernel Density Two-Step Floating Catchment Area (KD2SFCA) method. The Gini index and t-tests were used to analyze disparities between urban and rural areas. Hot Spot Analysis Getis-Ord Gi* and local bivariate relationships analysis were applied to assess spatial correlations. Principal component analysis (PCA) was applied to calculate the health risk index. Results: Hospital accessibility was unevenly distributed. Mortality rates from ADRD were higher in underserved areas with fewer hospitals. Hot spot analysis shows eastern and southern Maryland have zones with high mortality per population and per ADRD patient, surrounded by similarly high-rate zones. Central Maryland shows lower death rates per patient but more hospital facilities. In eastern Maryland, higher poverty areas are surrounded by zones with lower accessibility and higher health risk indices. Conclusion: Hospital accessibility is unevenly distributed, creating major rural disparities. Underserved regions in terms of access to healthcare facilities, particularly in eastern and southern Maryland, exhibit high ADRD mortality rates despite low diagnosis rates. This suggests that many ADRD cases remain undiagnosed, underdiagnosed, or subject to delayed treatment.

</details>


### [11] [Group Cooperation Diverges onto Durable Low versus High Paths: Public Goods Experiments in 134 Honduran Villages](https://arxiv.org/abs/2512.09316)
*Marios Papamichalis,Nicholas Christakis,Feng Fu*

Main category: stat.AP

TL;DR: 大型实地实验发现合作行为早期分化为高低两条路径，早期多数高贡献者决定群体最终走向高合作路径


<details>
  <summary>Details</summary>
Motivation: 研究在固定匿名群体中合作行为如何随时间演变，特别是早期行为如何影响长期合作结果

Method: 在洪都拉斯134个村庄对2,591名参与者进行10轮大型实地实验，追踪固定匿名五人小组的贡献行为

Result: 贡献行为早期分化为高低两条持久路径，很少收敛；早期多数（约60%）高贡献者群体很可能保持高合作；社会中心人物的早期高贡献引导群体走向高合作路径

Conclusion: 群体合作行为早期就分化为不同路径，早期高贡献者特别是社会中心人物的行为对引导群体走向高合作路径至关重要

Abstract: We performed large, lab-in-the-field experiment (2,591 participants across 134 Honduran villages; ten rounds) and tracked how contribution behavior unfolds in fixed, anonymous groups of size five. Contribution separates early into two durable paths, one low and one high, with rare convergence thereafter. High-path players can be identified with strong accuracy early on. Groups that begin with an early majority of above-norm contributors (about 60%) are very likely finish high. The empirical finding of a bifurcation, consistent with the theory, shows that early, high contributions by socially central people steer groups onto, and help keep them on, a high-cooperation path.

</details>


### [12] [Neural posterior inference with state-space models for calibrating ice sheet simulators](https://arxiv.org/abs/2512.09561)
*Bao Anh Vu,Andrew Zammit-Mangion,David Gunawan,Felicity S. McCormack,Noel Cressie*

Main category: stat.AP

TL;DR: 使用神经后验近似方法高效校准冰盖模型参数，结合集合卡尔曼滤波推断冰厚度，相比现有方法更准确


<details>
  <summary>Details</summary>
Motivation: 冰盖模型参数校准面临非线性方程、高维参数和数据有限的挑战，需要更高效准确的方法来预测海平面上升

Method: 采用神经后验近似技术，训练神经网络从冰流速和表面高程观测中推断基岩高程和基底摩擦系数，再结合集合卡尔曼滤波推断冰厚度

Result: 模拟研究表明该方法比当前最先进的增强状态集合卡尔曼滤波更准确地估计参数和状态，并成功应用于南极Thwaites冰川

Conclusion: 神经后验近似方法为冰盖模型参数校准提供了高效准确的解决方案，有助于改进海平面上升预测

Abstract: Ice sheet models are routinely used to quantify and project an ice sheet's contribution to sea level rise. In order for an ice sheet model to generate realistic projections, its parameters must first be calibrated using observational data; this is challenging due to the nonlinearity of the model equations, the high dimensionality of the underlying parameters, and limited data availability for validation. This study leverages the emerging field of neural posterior approximation for efficiently calibrating ice sheet model parameters and boundary conditions. We make use of a one-dimensional (flowline) Shallow-Shelf Approximation model in a state-space framework. A neural network is trained to infer the underlying parameters, namely the bedrock elevation and basal friction coefficient along the flowline, based on observations of ice velocity and ice surface elevation. Samples from the approximate posterior distribution of the parameters are then used within an ensemble Kalman filter to infer latent model states, namely the ice thickness along the flowline. We show through a simulation study that our approach yields more accurate estimates of the parameters and states than a state-augmented ensemble Kalman filter, which is the current state-of-the-art. We apply our approach to infer the bed elevation and basal friction along a flowline in Thwaites Glacier, Antarctica.

</details>


### [13] [Bayesian Model Selection with an Application to Cosmology](https://arxiv.org/abs/2512.09724)
*Nikoloz Gigiberia*

Main category: stat.AP

TL;DR: 使用DES超新星数据通过贝叶斯方法比较ΛCDM、wCDM和CPL宇宙学模型，发现wCDM模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 从贝叶斯角度研究宇宙学参数推断和模型选择，测试不同宇宙学模型对观测数据的拟合能力

Method: 使用DES-SN5YR超新星数据，通过NumPyro中的NUTS哈密顿蒙特卡洛进行后验推断，利用R的bridgesampling库计算贝叶斯因子进行模型比较

Result: 三个模型预测性能相似，但wCDM相对于ΛCDM和CPL具有更强的证据支持

Conclusion: 在本文假设和数据条件下，wCDM模型能更好地描述宇宙膨胀

Abstract: We investigate cosmological parameter inference and model selection from a Bayesian perspective. Type Ia supernova data from the Dark Energy Survey (DES-SN5YR) are used to test the $Λ$CDM, $w$CDM, and CPL cosmological models. Posterior inference is performed via Hamiltonian Monte Carlo using the No-U-Turn Sampler (NUTS) implemented in NumPyro and analyzed with ArviZ in Python. Bayesian model comparison is conducted through Bayes factors computed using the bridgesampling library in R. The results indicate that all three models demonstrate similar predictive performance, but $w$CDM shows stronger evidence relative to $Λ$CDM and CPL. We conclude that, under the assumptions and data used in this study, $w$CDM provides a better description of cosmological expansion.

</details>


### [14] [Network Meta Analysis of Mean Survival](https://arxiv.org/abs/2512.09732)
*Anastasios Apsemidis,Dimitris Mavridis,Nikolaos Demiris*

Main category: stat.AP

TL;DR: 提出贝叶斯网络元分析方法，直接分析治疗的平均生存期而非替代指标，通过生存曲线外推整合长期证据，适用于成本效益分析决策。


<details>
  <summary>Details</summary>
Motivation: 传统网络元分析使用风险比或限制平均生存时间等替代指标，存在比例风险假设不正确或随访期短的局限，无法直接分析决策所需的主要结局——治疗的平均生存期。

Method: 提出贝叶斯框架，通过生存曲线外推整合长期死亡率预测证据，使用灵活的多风险参数模型和M样条方法进行外推，在决策理论框架下进行成本效益分析。

Result: 通过模拟研究评估不同技术的计算和统计效率，并将开发的方法应用于两个真实数据集，证明方法的可行性。

Conclusion: 该方法可直接分析治疗的平均生存期，克服传统替代指标的局限，便于整合成本信息，为成本效益分析中的治疗选择提供更准确的决策支持。

Abstract: Decisions based upon pairwise comparisons of multiple treatments are naturally performed in terms of the mean survival of the selected study arms or functions thereof. However, synthesis of treatment comparisons is usually performed on surrogates of the mean survival, such as hazard ratios or restricted mean survival times. Thus, network meta-analysis techniques may suffer from the limitations of these approaches, such as incorrect proportional hazards assumption or short-term follow-up periods. We propose a Bayesian framework for the network meta-analysis of the main outcome informing the decision, the mean survival of a treatment. Its derivation involves extrapolation of the observed survival curves. We use methods for stable extrapolation that integrate long term evidence based upon mortality projections. Extrapolations are performed using flexible poly-hazard parametric models and M-spline-based methods. We assess the computational and statistical efficiency of different techniques using a simulation study and apply the developed methods to two real data sets. The proposed method is formulated within a decision theoretic framework for cost-effectiveness analyses, where the `best' treatment is to be selected and incorporating the associated cost information is straightforward.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [15] [All Emulators are Wrong, Many are Useful, and Some are More Useful Than Others: A Reproducible Comparison of Computer Model Surrogates](https://arxiv.org/abs/2512.09060)
*Kellin N. Rumsey,Graham C. Gibson,Devin Francom,Reid Morris*

Main category: stat.CO

TL;DR: 开发了一个名为duqling的R包，用于对29种不同的代理模型在60个标准测试函数和40个真实数据集上进行大规模、可重复的比较研究，为方法开发者和实践者提供选择指导。


<details>
  <summary>Details</summary>
Motivation: 代理建模在现代计算科学中至关重要，但存在大量不同的仿真方法，且新方法不断涌现。由于基准测试实践不一致、可重复性和透明度有限，比较不同方法的优缺点具有挑战性。

Method: 开发了R包duqling，采用一致的简单语法和自动内部输入缩放，实现了对29种不同代理模型在60个标准测试函数和40个真实仿真数据集上的大规模、完全可重复的比较。

Result: 研究结果提供了关于最先进代理模型优缺点详细的实证见解，为方法开发者和实践者选择新数据的代理模型提供了指导。

Conclusion: 提出了代理模型比较的最佳实践，并展示了duqling如何加速代理模型设计和应用研究，通过统一环境实现不同方法的公平比较。

Abstract: Accurate and efficient surrogate modeling is essential for modern computational science, and there are a staggering number of emulation methods to choose from. With new methods being developed all the time, comparing the relative strengths and weaknesses of different methods remains a challenge due to inconsistent benchmarking practices and (sometimes) limited reproducibility and transparency. In this work, we present a large-scale, fully reproducible comparison of $29$ distinct emulators across $60$ canonical test functions and $40$ real emulation datasets. To facilitate rigorous, apples-to-apples comparisons, we introduce the R package \texttt{duqling}, which streamlines reproducible simulation studies using a consistent, simple syntax, and automatic internal scaling of inputs. This framework allows researchers to compare emulators in a unified environment and makes it possible to replicate or extend previous studies with minimal effort, even across different publications. Our results provide detailed empirical insight into the strengths and weaknesses of state-of-the-art emulators and offer guidance for both method developers and practitioners selecting a surrogate for new data. We discuss best practices for emulator comparison and highlight how \texttt{duqling} can accelerate research in emulator design and application.

</details>


### [16] [Minimization of Functions on Dually Flat Spaces Using Geodesic Descent Based on Dual Connections](https://arxiv.org/abs/2512.09358)
*Gaku Omiya,Fumiyasu Komaki*

Main category: stat.CO

TL;DR: 提出基于测地线的对偶平坦空间优化方法，特别适用于指数族最大似然估计，m-测地线更新理论上可一步达到MLE，e-测地线更新在处理参数约束时有实际优势。


<details>
  <summary>Details</summary>
Motivation: 在统计模型（特别是指数族）的最大似然估计中，参数流形的几何结构与目标函数形式密切相关。对偶平坦空间提供了一种自然的几何框架，可以利用测地线结构设计更有效的优化算法。

Method: 提出两种基于测地线的优化方法：m-测地线更新直接优化对数似然函数，e-测地线更新利用对偶平坦空间的几何结构。方法特别适用于指数族统计模型，其模型流形具有对偶平坦性。

Result: 理论证明m-测地线更新可在单步内达到最大似然估计量；e-测地线更新在参数空间测地完备时具有实际优势，无需显式处理参数约束。数值实验验证了方法的有效性。

Conclusion: 基于测地线的对偶平坦空间优化方法为统计模型的最大似然估计提供了有效的几何框架，结合了理论最优性和实际计算优势，特别适用于指数族等具有自然几何结构的模型。

Abstract: We propose geodesic-based optimization methods on dually flat spaces, where the geometric structure of the parameter manifold is closely related to the form of the objective function. A primary application is maximum likelihood estimation in statistical models, especially exponential families, whose model manifolds are dually flat. We show that an m-geodesic update, which directly optimizes the log-likelihood, can theoretically reach the maximum likelihood estimator in a single step. In contrast, an e-geodesic update has a practical advantage in cases where the parameter space is geodesically complete, allowing optimization without explicitly handling parameter constraints. We establish the theoretical properties of the proposed methods and validate their effectiveness through numerical experiments.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [17] [A Conversation with Mike West](https://arxiv.org/abs/2512.09790)
*Hedibert F. Lopes,Filippo Ascolani*

Main category: stat.OT

TL;DR: Mike West是杜克大学统计学与决策科学杰出荣誉教授，在贝叶斯分析、时间序列动态模型、决策分析等领域有广泛研究，应用涵盖商业预测、金融、基因组学等多个领域，培养了众多博士生并在专业组织担任领导职务。


<details>
  <summary>Details</summary>
Motivation: 本文介绍了Mike West教授的学术背景和研究动机，旨在展示他在贝叶斯分析领域的广泛贡献以及跨学科应用研究的价值。他致力于将统计理论与实际应用相结合，推动统计学在商业、金融、生物技术等领域的应用发展。

Method: 通过学术研究、跨学科合作、专业领导、学生培养等多种方式。具体包括：贝叶斯分析理论方法研究、动态模型开发、决策分析框架构建、跨学科应用研究、专业组织领导、学生指导与培养等综合性方法。

Result: Mike West教授在学术研究方面出版了4本书籍、200多篇论文；在专业领导方面曾任国际贝叶斯分析学会主席；在人才培养方面指导了约65名博士生和博士后；在应用实践方面共同创立了生物技术公司，并在多个金融和IT公司担任董事会成员。

Conclusion: Mike West教授是一位在贝叶斯分析和决策科学领域有深远影响的学者，通过理论研究、跨学科应用、专业领导和人才培养等多方面工作，对统计学发展和实际应用做出了重要贡献。

Abstract: Mike West is currently the Arts & Sciences Distinguished Professor Emeritus of Statistics and Decision Sciences at Duke University. Mike's research in Bayesian analysis spans multiple interlinked areas: theory and methods of dynamic models in time series analysis, foundations of inference and decision analysis, multivariate and latent structure analysis, stochastic computation and optimisation, among others. Inter-disciplinary R&D has ranged across applications in commercial forecasting, dynamic networks, finance, econometrics, signal processing, climatology, systems biology, genomics and neuroscience, among other areas. Among Mike's currently active research areas are forecasting, causal prediction and decision analysis in business, economic policy and finance, as well as in personal decision making. Mike led the development of academic statistics at Duke University from 1990-2002, and has been broadly engaged in professional leadership elsewhere. He is past president of the International Society for Bayesian Analysis (ISBA), and has served in founding roles and as board member for several professional societies, national and international centres and institutes. Recipient of numerous awards, Mike has been active in research with various companies, banks, government agencies and academic centres, co-founder of a successful biotechnology company, and board member for several financial and IT companies. He has published 4 books, several edited volumes and over 200 papers. Mike has worked with many undergraduate and Master's research students, and as of 2025 has mentored around 65 primary PhD students and postdoctoral associates who moved to academic, industrial or governmental positions involving advanced statistical and data science research.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [18] [Complementary strengths of the Neyman-Rubin and graphical causal frameworks](https://arxiv.org/abs/2512.09130)
*Tetiana Gorbach,Xavier de Luna,Juha Karvanen,Ingeborg Waernbaum*

Main category: stat.ME

TL;DR: 本文比较了Neyman-Rubin因果框架与图形框架，通过具体例子展示了两种方法在不同数据生成机制下的适用性差异。


<details>
  <summary>Details</summary>
Motivation: 探讨Neyman-Rubin因果框架与图形框架之间的关系，帮助研究人员更好地理解这两种常用因果推理框架的优缺点和适用场景。

Method: 通过具体的数据生成机制案例对比分析：1）无向关系、确定性关系和循环结构等场景下，有向无环图分析困难但Neyman-Rubin方法适用；2）M偏倚、陷阱门变量和复杂前门结构等场景下，Neyman-Rubin方法复杂但图形方法直接可用。

Result: 展示了两种框架在不同类型因果结构中的互补性：某些场景下Neyman-Rubin框架更适用，而另一些场景下图形框架更直接可用。

Conclusion: 两种因果推理框架各有优势，理解它们的适用场景有助于提高因果推理能力，促进更广泛的因果推理语言理解。

Abstract: This article contributes to the discussion on the relationship between the Neyman-Rubin and the graphical frameworks for causal inference. We present specific examples of data-generating mechanisms - such as those involving undirected or deterministic relationships and cycles - where analyses using a directed acyclic graph are challenging, but where the tools from the Neyman-Rubin causal framework are readily applicable. We also provide examples of data-generating mechanisms with M-bias, trapdoor variables, and complex front-door structures, where the application of the Neyman-Rubin approach is complicated, but the graphical approach is directly usable. The examples offer insights into commonly used causal inference frameworks and aim to improve comprehension of the languages for causal reasoning among a broad audience.

</details>


### [19] [MoDaH achieves rate optimal batch correction](https://arxiv.org/abs/2512.09259)
*Yang Cao,Zongming Ma*

Main category: stat.ME

TL;DR: MoDaH是首个具有理论保证的批次效应校正方法，基于高斯混合模型框架，在单细胞组学数据中实现了最小化最优误差率，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单细胞组学数据中的批次效应是重大挑战，现有计算方法虽然经验上成功但缺乏理论保证，无法评估其可靠性和泛化能力。

Method: 提出基于高斯混合模型的数据协调方法(MoDaH)，使用显式参数化批次效应的高斯混合模型，利用各向异性高斯混合聚类的最新理论进展。

Result: 建立了批次校正的最小化最优误差率，证明MoDaH能达到该速率，在多样单细胞RNA-seq和空间蛋白质组数据实验中表现优于或媲美现有最佳方法。

Conclusion: MoDaH是首个具有理论保证的批次校正算法，在去除技术噪声和保留生物信号之间取得了有效平衡，填补了理论与经验方法之间的空白。

Abstract: Batch effects pose a significant challenge in the analysis of single-cell omics data, introducing technical artifacts that confound biological signals. While various computational methods have achieved empirical success in correcting these effects, they lack the formal theoretical guarantees required to assess their reliability and generalization. To bridge this gap, we introduce Mixture-Model-based Data Harmonization (MoDaH), a principled batch correction algorithm grounded in a rigorous statistical framework.
  Under a new Gaussian-mixture-model with explicit parametrization of batch effects, we establish the minimax optimal error rates for batch correction and prove that MoDaH achieves this rate by leveraging the recent theoretical advances in clustering data from anisotropic Gaussian mixtures. This constitutes, to the best of our knowledge, the first theoretical guarantee for batch correction. Extensive experiments on diverse single-cell RNA-seq and spatial proteomics datasets demonstrate that MoDaH not only attains theoretical optimality but also achieves empirical performance comparable to or even surpassing those of state-of-the-art heuristics (e.g., Harmony, Seurat-V5, and LIGER), effectively balancing the removal of technical noise with the conservation of biological signal.

</details>


### [20] [IntegralGP: Volumetric estimation of subterranean geochemical properties in mineral deposits by fusing assay data with different spatial supports](https://arxiv.org/abs/2512.09151)
*Anna Chlingaryan,Arman Melkumyan,Raymond Leung*

Main category: stat.ME

TL;DR: 提出IntegralGP框架用于矿物矿床地下属性的体积估计，通过积分高斯过程统一处理不同空间支持的数据，将钻孔地球化学分析作为区间观测而非点观测建模，改善了回归性能和边界划分。


<details>
  <summary>Details</summary>
Motivation: 传统方法将钻孔地球化学分析数据视为点观测，但实际上这些数据代表的是钻孔间隔内的平均属性值。这种简化会导致模型偏差和边界划分不准确，特别是在矿物品位估计和材料分类中。

Method: 开发IntegralGP框架，修改协方差表达式以处理区间观测数据，获取梯度和反导数以学习核超参数，使用异方差GP融合空间不一致但化学兼容的不同分辨率数据，保持块模型的领域知识结构。

Result: 实验验证显示，在预测的台阶下方品位预测性能有所改善，对于材料分类，IntegralGP融合减少了绝对误差和模型偏差，特别是在将废石块误分类为高品位的情况下。

Conclusion: IntegralGP框架通过将钻孔分析作为区间观测而非点观测进行建模，显著提高了地下属性体积估计的准确性，改善了回归性能和边界划分，在矿物品位预测和材料分类中表现出优越性能。

Abstract: This article presents an Integral Gaussian Process (IntegralGP) framework for volumetric estimation of subterranean properties in mineral deposits. It provides a unified representation for data with different spatial supports, which enables blasthole geochemical assays to be properly modelled as interval observations rather than points. This approach is shown to improve regression performance and boundary delineation. A core contribution is a description of the mathematical changes to the covariance expressions which allow these benefits to be realised. The gradient and anti-derivatives are obtained to facilitate learning of the kernel hyperparameters. Numerical stability issues are also discussed. To illustrate its application, an IntegralGP data fusion algorithm is described. The objective is to assimilate line-based blasthole assays and update a block model that provides long-range prediction of Fe concentration beneath the drilled bench. Heteroscedastic GP is used to fuse chemically compatible but spatially incongruous data with different resolutions and sample spacings. Domain knowledge embodied in the structure and empirical distribution of the block model must be generally preserved while local inaccuracies are corrected. Using validation measurements within the predicted bench, our experiments demonstrate an improvement in bench-below grade prediction performance. For material classification, IntegralGP fusion reduces the absolute error and model bias in categorical prediction, especially instances where waste blocks are mistakenly classified as high-grade.

</details>


### [21] [On the inverse of covariance matrices for unbalanced crossed designs](https://arxiv.org/abs/2512.09273)
*Ziyang Lyu,S. A. Sisson,A. H. Welsh*

Main category: stat.ME

TL;DR: 提出了线性混合模型中交叉随机效应不平衡设计的协方差矩阵逆的解析表达式，解决了长期存在的开放问题


<details>
  <summary>Details</summary>
Motivation: 线性混合模型中交叉随机效应不平衡设计的协方差矩阵逆缺乏闭式表达式，导致基于似然的方法计算困难且难以数学分析

Method: 使用Khatri-Rao乘积表示协方差矩阵，构建修正协方差矩阵获得精确谱分解，推导渐进不平衡设计的近似逆，并建立低秩校正的精确表示

Result: 获得了渐进不平衡设计的优雅近似逆，推导了轻度不平衡数据的准确可解释近似，建立了任意不平衡程度的精确逆表示

Conclusion: 提出的框架在模拟研究中表现出准确性、稳定性和计算可行性，解决了长期存在的开放问题

Abstract: This paper addresses a long-standing open problem in the analysis of linear mixed models with crossed random effects under unbalanced designs: how to find an analytic expression for the inverse of $\mathbf{V}$, the covariance matrix of the observed response. The inverse matrix $\mathbf{V}^{-1}$ is required for likelihood-based estimation and inference. However, for unbalanced crossed designs, $\mathbf{V}$ is dense and the lack of a closed-form representation for $\mathbf{V}^{-1}$, until now, has made using likelihood-based methods computationally challenging and difficult to analyse mathematically. We use the Khatri--Rao product to represent $\mathbf{V}$ and then to construct a modified covariance matrix whose inverse admits an exact spectral decomposition. Building on this construction, we obtain an elegant and simple approximation to $\mathbf{V}^{-1}$ for asymptotic unbalanced designs. For non-asymptotic settings, we derive an accurate and interpretable approximation under mildly unbalanced data and establish an exact inverse representation as a low-rank correction to this approximation, applicable to arbitrary degrees of unbalance. Simulation studies demonstrate the accuracy, stability, and computational tractability of the proposed framework.

</details>


### [22] [Prenatal alcohol exposure and child cognition: semi-continuous exposures, causal inference and evidence synthesis](https://arxiv.org/abs/2512.09237)
*Xiaoya Wang,Richard J. Cook,Yeying Zhu,Tugba Akkaya-Hocagil,R. Colin Carter,Sandra W. Jacobson,Joseph L. Jacobson,Louise M. Ryan*

Main category: stat.ME

TL;DR: 提出用于半连续暴露的因果推断方法，包含暴露状态和剂量反应效应的两阶段估计方程，开发同质性检验评估多结局效应一致性，应用于产前酒精暴露对儿童认知影响研究


<details>
  <summary>Details</summary>
Motivation: 解决半连续暴露（既有暴露状态又有连续暴露水平）的因果推断挑战，特别是当研究多个结局变量时，需要同时评估暴露状态效应和剂量反应效应的一致性

Method: 采用两阶段估计方程方法：第一阶段估计暴露状态效应，第二阶段估计剂量反应效应；开发同质性检验评估不同结局间效应一致性，包括全局同质性检验；建立大样本性质理论保证

Result: 通过模拟研究验证了估计和检验方法的有效性；应用于产前酒精暴露对儿童执行功能、数学学业成就、学习记忆三项认知结局的研究，展示了方法的实际应用价值

Conclusion: 提出的方法为半连续暴露的因果推断提供了有效工具，能够同时评估暴露状态和剂量反应效应，并检验多结局间效应一致性，在流行病学和医学研究中具有重要应用价值

Abstract: We address the challenge of causal inference status and the dose-response effects with a semi-continuous exposure. A two-stage approach is proposed using estimating equation for multiple outcomes with large sample properties derived for the resulting estimators. Homogeneity tests are developed to assess whether causal effects of exposure status and the dose-response effects are the same across multiple outcomes. A global homogeneity test is also developed to assess whether the effect of exposure status (exposed/not exposed) and the dose-response effect of the continuous exposure level are each equal across all outcomes. The methods of estimation and testing are rigorously evaluated in simulation studies and applied to a motivating study on the effects of prenatal alcohol exposure on childhood cognition defined by executive function (EF), academic achievement in math, and learning and memory (LM).

</details>


### [23] [Vaccine sieve analysis on deep sequencing data using competing risks Cox regression with failure type subject to misclassification](https://arxiv.org/abs/2512.09262)
*James Peng,Michal Juraska,Pamela A. Shaw,Peter B. Gilbert*

Main category: stat.ME

TL;DR: 提出了一种考虑个体内病毒多样性的筛分析新方法，通过竞争风险Cox回归和经验贝叶斯分类模型，提高了HIV疫苗效力评估的准确性和统计功效。


<details>
  <summary>Details</summary>
Motivation: 传统筛分析仅使用每个个体的单一序列，无法充分利用现代测序技术提供的个体内病毒多样性信息（每人可达数百条序列），这限制了HIV等高度遗传多样性病原体疫苗效力评估的准确性。

Method: 开发了考虑个体内病毒多样性的筛分析方法：1）使用竞争风险Cox回归模型处理不同序列计数带来的信息分辨率差异；2）提出经验贝叶斯方法进行分类建模；3）估计疫苗对具有不同疫苗不匹配突变真实频率的病毒群体的效力。

Result: 模拟研究表明，与传统方法相比，新方法减少了偏差，提供了名义置信区间覆盖，并提高了统计功效。将该方法应用于HVTN 705 Imbokodo试验，评估了异源疫苗方案预防HIV-1感染的效力。

Conclusion: 提出的方法能够充分利用现代测序技术提供的个体内病毒多样性数据，为HIV等高度遗传多样性病原体的疫苗效力评估提供了更准确、更强大的统计框架。

Abstract: Understanding how vaccines perform against different pathogen genotypes is crucial for developing effective prevention strategies, particularly for highly genetically diverse pathogens like HIV. Sieve analysis is a statistical framework used to determine whether a vaccine selectively prevents acquisition of certain genotypes while allowing breakthrough of other genotypes that evade immune responses. Traditionally, these analyses are conducted with a single sequence available per individual acquiring the pathogen. However, modern sequencing technology can provide detailed characterization of intra-individual viral diversity by capturing up to hundreds of pathogen sequences per person. In this work, we introduce methodology that extends sieve analysis to account for intra-individual viral diversity. Our approach estimates vaccine efficacy against viral populations with varying true (unobservable) frequencies of vaccine-mismatched mutations. To account for differential resolution of information from differing sequence counts per person, we use competing risks Cox regression with modeled causes of failure and propose an empirical Bayes approach for the classification model. Simulation studies demonstrate that our approach reduces bias, provides nominal confidence interval coverage, and improves statistical power compared to conventional methods. We apply our method to the HVTN 705 Imbokodo trial, which assessed the efficacy of a heterologous vaccine regimen in preventing HIV-1 acquisition.

</details>


### [24] [Balancing Weights for Causal Mediation Analysis](https://arxiv.org/abs/2512.09337)
*Kentaro Kawato*

Main category: stat.ME

TL;DR: 提出基于权重平衡的因果中介效应估计方法，解决传统EIF和IPW估计器因倾向得分倒数导致的不稳定性和协变量不平衡问题


<details>
  <summary>Details</summary>
Motivation: 传统因果中介分析中的EIF估计器和IPW估计器都依赖于估计的倾向得分的倒数，这导致两个关键问题：(1) 不稳定性；(2) 有限样本下的协变量不平衡。需要开发更稳健的估计方法。

Method: 提出基于权重算法的估计器，该算法直接惩罚权重离散度，同时强制实施近似的协变量和中介变量平衡，从而提高稳定性并减少有限样本偏差。

Result: 建立了所提权重的收敛率，证明所得估计器是渐近正态的，并达到半参数效率界。蒙特卡洛模拟显示，在模型误设的挑战性场景中，所提估计器不仅优于EIF和IPW估计器，也优于回归插补估计器。

Conclusion: 提出的基于权重平衡的方法在因果中介分析中提供了更稳健的估计，解决了传统方法的不稳定性和协变量不平衡问题，并在模拟和实际数据应用中表现出优越性能。

Abstract: This paper develops methods for estimating the natural direct and indirect effects in causal mediation analysis. The efficient influence function-based estimator (EIF-based estimator) and the inverse probability weighting estimator (IPW estimator), which are standard in causal mediation analysis, both rely on the inverse of the estimated propensity scores, and thus they are vulnerable to two key issues (i) instability and (ii) finite-sample covariate imbalance. We propose estimators based on the weights obtained by an algorithm that directly penalizes weight dispersion while enforcing approximate covariate and mediator balance, thereby improving stability and mitigating bias in finite samples. We establish the convergence rates of the proposed weights and show that the resulting estimators are asymptotically normal and achieve the semiparametric efficiency bound. Monte Carlo simulations demonstrate that the proposed estimator outperforms not only the EIF-based estimator and the IPW estimator but also the regression imputation estimator in challenging scenarios with model misspecification. Furthermore, the proposed method is applied to a real dataset from a study examining the effects of media framing on immigration attitudes.

</details>


### [25] [Model-robust Inference for Seamless II/III Trials with Covariate Adaptive Randomization](https://arxiv.org/abs/2512.09430)
*Kun Yi,Lucy Xia*

Main category: stat.ME

TL;DR: 提出一个基于广义线性模型的无缝II/III期临床试验统一框架，适用于多种结果类型、估计目标和协变量自适应随机化方案，确保有效的统计推断。


<details>
  <summary>Details</summary>
Motivation: 现有无缝II/III期试验推断方法多为模型依赖型，主要针对连续结果，且常忽略协变量自适应随机化中的分层，限制了实际应用价值。

Method: 基于广义线性模型，使用Z估计推导治疗效果估计量的渐近性质，明确方差与随机化程序的关系，开发调整的Wald检验，结合Dunnett多重比较和逆卡方组合方法。

Result: 模拟研究和试验示例表明，所提出的模型稳健检验相比传统方法具有更优的检验功效和可靠的推断性能。

Conclusion: 该框架为无缝II/III期试验提供了一个统一、模型稳健的推断方法，适用于多种结果类型和随机化方案，确保了整体I类错误控制。

Abstract: Seamless phase II/III trials have become a cornerstone of modern drug development, offering a means to accelerate evaluation while maintaining statistical rigor. However, most existing inference procedures are model-based, designed primarily for continuous outcomes, and often neglect the stratification used in covariate-adaptive randomization (CAR), limiting their practical relevance. In this paper, we propose a unified, model-robust framework for seamless phase II/III trials grounded in generalized linear models (GLMs), enabling valid inference across diverse outcome types, estimands, and CAR schemes. Using Z-estimation, we derive the asymptotic properties of treatment effect estimators and explicitly characterize how their variance depends on the underlying randomization procedure.Based on these results, we develop adjusted Wald tests that, together with Dunnett's multiple-comparison procedure and the inverse chi-square combination method, ensure valid overall Type I error. Extensive simulation studies and a trial example demonstrate that the proposed model-robust tests achieve superior power and reliable inference compared to conventional approaches.

</details>


### [26] [Multiply-robust Estimator of Cumulative Incidence Function Difference for Right-Censored Competing Risks Data](https://arxiv.org/abs/2512.09433)
*Yifei Tian,Ying Wu*

Main category: stat.ME

TL;DR: 提出了一种针对竞争风险数据的多重稳健估计器，用于估计原因特异性累积发病率函数的因果效应差异，通过伪值方法和多重稳健框架结合，在至少一个倾向得分或结局回归模型正确设定时保持一致性。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，双重稳健估计器虽然比单一模型更稳健，但当两个模型都误设时仍不一致。对于竞争风险数据，需要更稳健的方法来估计原因特异性累积发病率函数的因果效应。

Method: 结合伪值方法（将删失的时间依赖性CIF转换为完整数据结局）和多重稳健估计框架，为倾向得分和结局回归指定多个候选模型，构建多重稳健估计器。

Result: 模拟研究表明，该多重稳健估计器在各种模型误设场景和不同删失率下几乎无偏，并保持名义覆盖率。在右心导管数据集上展示了实际应用。

Conclusion: 提出的多重稳健估计器显著提高了对竞争风险数据中因果效应估计的稳健性，在至少一个倾向得分或结局回归模型正确设定时保持一致性，为实际应用提供了更可靠的工具。

Abstract: In causal inference, estimating the average treatment effect is a central objective, and in the context of competing risks data, this effect can be quantified by the cause-specific cumulative incidence function (CIF) difference. While doubly robust estimators give a more robust way to estimate the causal effect from the observational study, they remain inconsistent if both models are misspecified. To improve the robustness, we develop a multiply robust estimator for the difference in cause-specific CIFs using right-censored competing risks data. The proposed framework integrates the pseudo-value approach, which transforms the censored, time-dependent CIF into a complete-data outcome, with the multiply robust estimation framework. By specifying multiple candidate models for both the propensity score and the outcome regression, the resulting estimator is consistent and asymptotically unbiased, provided that at least one of the multiple propensity score or outcome regression models is correctly specified. Simulation studies show our multiply robust estimator remains virtually unbiased and maintains nominal coverage rates under various model misspecification scenarios and a wide range of choices for the censoring rate. Finally, the proposed multiply robust model is illustrated using the Right Heart Catheterization dataset.

</details>


### [27] [Calibration with Bagging of the Principal Components on a Large Number of Auxiliary Variables](https://arxiv.org/abs/2512.09505)
*Caren Hasler,Arnaud Tripet,Yves Tillé*

Main category: stat.ME

TL;DR: 提出一种结合bagging和主成分分解的校准方法，通过从主成分中抽样构建多个权重系统并取平均，解决多辅助变量下方差爆炸和权重分散问题。


<details>
  <summary>Details</summary>
Motivation: 传统校准方法在处理大量辅助变量时存在两个主要问题：1) 总体估计量的方差会增大；2) 校准权重变得高度分散。需要一种能有效处理大量辅助变量同时保持估计稳定性的方法。

Method: 1) 对辅助变量进行主成分分解；2) 从主成分中按不等概率无放回抽样选择校准变量样本；3) 为每个样本构建权重系统；4) 取所有权重系统的平均值作为最终权重。该方法允许对部分主要辅助变量进行精确校准。

Result: 提出的方法能够：1) 在添加新辅助变量时防止方差爆炸；2) 获得非常低分散的权重；3) 为调查的多个感兴趣变量提供单一的权重系统。

Conclusion: 结合bagging和主成分分解的校准方法有效解决了多辅助变量校准中的方差增长和权重分散问题，为大规模调查提供了稳定可靠的权重系统。

Abstract: Calibration is a widely used method in survey sampling to adjust weights so that estimated totals of some chosen calibration variables match known population totals or totals obtained from other sources. When a large number of auxiliary variables are included as calibration variables, the variance of the total estimator can increase, and the calibration weights can become highly dispersed. To address these issues, we propose a solution inspired by bagging and principal component decomposition. With our approach, the principal components of the auxiliary variables are constructed. Several samples of calibration variables are selected without replacement and with unequal probabilities from among the principal components. For each sample, a system of weights is obtained. The final weights are the average weights of these different weighting systems. With our proposed method, it is possible to calibrate exactly for some of the main auxiliary variables. For the other auxiliary variables, the weights cannot be calibrated exactly. The proposed method allows us to obtain a total estimator whose variance does not explode when new auxiliary variables are added and to obtain very low scatter weights. Finally, our proposed method allows us to obtain a single weighting system that can be applied to several variables of interest of a survey.

</details>


### [28] [A Bayesian Approach for Robust Longitudinal Envelope Models](https://arxiv.org/abs/2512.09553)
*Peng Zeng,Yushan Mu*

Main category: stat.ME

TL;DR: 提出鲁棒纵向包络模型(RoLEM)，通过矩阵正态分布的尺度混合处理异常值，结合Grassmann流形上的贝叶斯推断，解决传统包络方法无法处理非正态误差和纵向数据的问题。


<details>
  <summary>Details</summary>
Motivation: 传统包络模型假设正态分布误差且无法处理纵向研究的重复测量数据，存在对异常值敏感和数据结构适应性不足的局限性。

Method: 使用矩阵正态分布的尺度混合建模随机误差以处理异常值，引入灵活的重复测量相关结构，并在Grassmann流形上设计新的先验和提议分布进行贝叶斯推断。

Result: 模拟研究和实际数据分析表明，RoLEM方法在性能上优于现有方法，能有效处理异常值和纵向数据结构。

Conclusion: RoLEM为多元线性回归提供了更鲁棒和灵活的维度约简框架，特别适用于包含异常值和重复测量的纵向研究场景。

Abstract: The envelope model provides a dimension-reduction framework for multivariate linear regression. However, existing envelope methods typically assume normally distributed random errors and do not accommodate repeated measures in longitudinal studies. To address these limitations, we propose the robust longitudinal envelope model (RoLEM). RoLEM employs a scale mixture of matrix-variate normal distributions to model random errors, allowing it to handle potential outliers, and incorporates flexible correlation structures for repeated measurements. In addition, we introduce new prior and proposal distributions on the Grassmann manifold to facilitate Bayesian inference for RoLEM. Simulation studies and real data analysis demonstrate the superior performance of the proposed method.

</details>


### [29] [Uniform-over-dimension location tests for multivariate and high-dimensional data](https://arxiv.org/abs/2512.09659)
*Ritabrata Karmakar,Joydeep Chowdhury,Subhajit Dutta,Marc G. Genton*

Main category: stat.ME

TL;DR: 本文提出了一种在任意维度下都有效的均匀收敛中心极限定理，并基于此开发了适用于高维数据的双样本位置检验方法，该检验在维度变化时保持一致性。


<details>
  <summary>Details</summary>
Motivation: 现有高维假设检验方法通常要求维度趋于无穷大，而传统多元检验方法仅适用于固定维度且需要大样本量。在实际应用中，很难判断数据维度是否符合高维渐近方法的条件，或样本量是否足够大。因此需要开发一种在维度变化时保持一致的检验方法。

Method: 首先引入"均匀收敛"概念，然后开发了均匀收敛的中心极限定理。基于此定理，构建了一个在观测维度上保持均匀收敛的双样本位置相等性检验。

Result: 通过模拟和真实数据验证，提出的检验方法在高维数据上表现优于文献中多种流行的高维检验方法，也优于传统的多元双样本检验（包括多元高斯数据的Hotelling's T²检验）。

Conclusion: 提出的均匀收敛中心极限定理和基于此的双样本检验方法，解决了现有方法在维度适应性方面的局限性，为高维数据分析提供了更稳健的检验工具。

Abstract: Asymptotic methods for hypothesis testing in high-dimensional data usually require the dimension of the observations to increase to infinity, often with an additional relationship between the dimension (say, $p$) and the sample size (say, $n$). On the other hand, multivariate asymptotic testing methods are valid for fixed dimension only and their implementations typically require the sample size to be large compared to the dimension to yield desirable results. In practical scenarios, it is usually not possible to determine whether the dimension of the data conform to the conditions required for the validity of the high-dimensional asymptotic methods for hypothesis testing, or whether the sample size is large enough compared to the dimension of the data. In this work, we first describe the notion of uniform-over-$p$ convergences and subsequently, develop a uniform-over-dimension central limit theorem. An asymptotic test for the two-sample equality of locations is developed, which now holds uniformly over the dimension of the observations. Using simulated and real data, it is demonstrated that the proposed test exhibits better performance compared to several popular tests in the literature for high-dimensional data as well as the usual scaled two-sample tests for multivariate data, including the Hotelling's $T^2$ test for multivariate Gaussian data.

</details>


### [30] [RECAP Framework v1.0: A Multi-Layer Inheritance Architecture for Evidence Synthesis](https://arxiv.org/abs/2512.09821)
*Hung Kuan Lee*

Main category: stat.ME

TL;DR: RECAP Framework v1.0提出三层元架构，通过方法论继承系统解决证据合成中单层结构导致的重复建设、概念漂移和推理不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 当前证据合成工作流程受限于单层结构，导致每个项目都需要从头重建方法论基础，造成不一致性、概念漂移和跨项目推理不稳定。

Method: 引入三层元架构：方法论法则（祖父母层）、领域级抽象（父母层）和项目级实现（子女层），定义具有严格分层、路由和污染控制规则的继承系统。

Result: RECAP框架为证据合成提供正式治理层，建立方法论谱系基础，旨在稳定跨研究项目的推理过程。

Conclusion: RECAP框架通过分层元架构和继承系统，能够保持概念清晰性、加强推理纪律性，并支持跨多项目证据生态系统的可重复性。

Abstract: Evidence synthesis has advanced through improved reporting standards, bias assessment tools, and analytic methods, but current workflows remain limited by a single-layer structure in which conceptual, methodological, and procedural decisions are made on the same level. This forces each project to rebuild its methodological foundations from scratch, leading to inconsistencies, conceptual drift, and unstable reasoning across projects. RECAP Framework v1.0 introduces a three-layer meta-architecture consisting of methodological laws (Grandparent), domain-level abstractions (Parent), and project-level implementations (Child). The framework defines an inheritance system with strict rules for tiering, routing, and contamination control to preserve construct clarity, enforce inferential discipline, and support reproducibility across multi-project evidence ecosystems. RECAP provides a formal governance layer for evidence synthesis and establishes the foundation for a methodological lineage designed to stabilize reasoning across research programs.

</details>


### [31] [Predictor-Informed Bayesian Nonparametric Clustering](https://arxiv.org/abs/2512.09826)
*Md Yasin Ali Parh,Jeremy T. Gaskins*

Main category: stat.ME

TL;DR: 提出了一种基于贝叶斯非参数共同原子模型的聚类方法，其中聚类成员受预测变量影响，通过金字塔分组模型将预测空间灵活划分为潜在组别，实现相似预测值的观测更可能被聚在一起。


<details>
  <summary>Details</summary>
Motivation: 研究如何执行观测聚类，使得聚类成员受一组预测变量的影响。传统聚类方法通常不考虑预测变量对聚类结构的影响，而实际应用中观测的聚类模式往往与协变量相关。

Method: 扩展贝叶斯非参数共同原子模型，将组别成员视为由协变量向量决定的未知潜在变量。提出金字塔分组模型，灵活地将预测空间划分为这些潜在组别成员。金字塔模型类似于贝叶斯回归树过程，但在同一树深度使用相同的分割规则以改善混合效果。开发了块吉布斯采样器进行后验推断。

Result: 在模拟和真实数据示例中验证了方法。在真实数据应用中，使用RAND健康与退休研究数据来聚类和预测患者过夜住院次数的结果。

Conclusion: 该方法成功地将预测变量纳入聚类过程，使具有相似预测值的观测更可能被聚在一起，为协变量相关的聚类问题提供了灵活的贝叶斯非参数解决方案。

Abstract: In this project we are interested in performing clustering of observations such that the cluster membership is influenced by a set of predictors. To that end, we employ the Bayesian nonparameteric Common Atoms Model, which is a nested clustering algorithm that utilizes a (fixed) group membership for each observation to encourage more similar clustering of members of the same group. CAM operates by assuming each group has its own vector of cluster probabilities, which are themselves clustered to allow similar clustering for some groups. We extend this approach by treating the group membership as an unknown latent variable determined as a flexible nonparametric form of the covariate vector. Consequently, observations with similar predictor values will be in the same latent group and are more likely to be clustered together than observations with disparate predictors. We propose a pyramid group model that flexibly partitions the predictor space into these latent group memberships. This pyramid model operates similarly to a Bayesian regression tree process except that it uses the same splitting rule for at all nodes at the same tree depth which facilitates improved mixing. We outline a block Gibbs sampler to perform posterior inference from our model. Our methodology is demonstrated in simulation and real data examples. In the real data application, we utilize the RAND Health and Retirement Study to cluster and predict patient outcomes in terms of the number of overnight hospital stays.

</details>
