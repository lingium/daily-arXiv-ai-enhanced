{"id": "2511.22564", "categories": ["stat.CO", "math.NA", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.22564", "abs": "https://arxiv.org/abs/2511.22564", "authors": ["Ruiyu Han"], "title": "Convergence of a Sequential Monte Carlo algorithm towards multimodal distributions on Rd", "comment": "arXiv admin note: text overlap with arXiv:2508.02763", "summary": "We study a sequential Monte Carlo algorithm to sample from the Gibbs measure supported on Rd with a non-convex energy function at a low temperature. In an earlier joint work, we proved that the algorithm samples from Gibbs measures supported on torus with time complexity that is polynomial in the inverse temperature; however, the approach breaks down in the non-compact setting. This work overcomes this obstacle and establishes a similar result for sampling from Gibbs measures supported on Rd. Our main result shows convergence of Monte Carlo estimators with time complexity that, approximately, scales like the seventh power of the inverse temperature, the square of the inverse allowed absolute error and the square of the inverse allowed probability error."}
{"id": "2511.22040", "categories": ["stat.AP", "math.PR", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22040", "abs": "https://arxiv.org/abs/2511.22040", "authors": ["Saman Hosseini", "Lee W. Cohnstaedt", "Caterina Scoglio"], "title": "A two-parameter, minimal-data model to predict dengue cases: the 2022-2023 outbreak in Florida, USA", "comment": null, "summary": "Reliable and timely dengue predictions provide actionable lead time for targeted vector control and clinical preparedness, reducing preventable diseases and health-system costs in at-risk communities. Dengue forecasting often relies on site-specific covariates and entomological data, limiting generalizability in data-sparse settings. We propose a data-parsimonious (DP) framework based on the incidence versus cumulative cases (ICC) curve, extending it from a basic SIR to a two-population SEIR model for dengue. Our DP model uses only the target season's incidence time series and estimates only two parameters, reducing noise and computational burden. A Bayesian extension quantifies the case reporting and fitting uncertainty to produce calibrated predictive intervals. We evaluated the performance of the DP model in the 2022-2023 outbreaks in Florida, where standardized clinical tests and reporting support accurate case determination. The DP framework demonstrates competitive predictive performance at substantially lower computational cost than more elaborate models, making it suitable for dengue early detection where dense surveillance or long historical records are unavailable."}
{"id": "2511.21890", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21890", "abs": "https://arxiv.org/abs/2511.21890", "authors": ["Dimitris Bertsimas", "Caio de Prospero Iglesias", "Nicholas A. G. Johnson"], "title": "Sparse Multiple Kernel Learning: Alternating Best Response and Semidefinite Relaxations", "comment": null, "summary": "We study Sparse Multiple Kernel Learning (SMKL), which is the problem of selecting a sparse convex combination of prespecified kernels for support vector binary classification. Unlike prevailing l1 regularized approaches that approximate a sparsifying penalty, we formulate the problem by imposing an explicit cardinality constraint on the kernel weights and add an l2 penalty for robustness. We solve the resulting non-convex minimax problem via an alternating best response algorithm with two subproblems: the alpha subproblem is a standard kernel SVM dual solved via LIBSVM, while the beta subproblem admits an efficient solution via the Greedy Selector and Simplex Projector algorithm. We reformulate SMKL as a mixed integer semidefinite optimization problem and derive a hierarchy of semidefinite convex relaxations which can be used to certify near-optimality of the solutions returned by our best response algorithm and also to warm start it. On ten UCI benchmarks, our method with random initialization outperforms state-of-the-art MKL approaches in out-of-sample prediction accuracy on average by 3.34 percentage points (relative to the best performing benchmark) while selecting a small number of candidate kernels in comparable runtime. With warm starting, our method outperforms the best performing benchmark's out-of-sample prediction accuracy on average by 4.05 percentage points. Our convex relaxations provide a certificate that in several cases, the solution returned by our best response algorithm is the globally optimal solution."}
{"id": "2511.21836", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21836", "abs": "https://arxiv.org/abs/2511.21836", "authors": ["Gellért Perényi", "Matias Janvin", "Mats J. Stensrud"], "title": "A simple and powerful test of vaccine waning", "comment": null, "summary": "Determining whether vaccine efficacy wanes is important for individual and public decision making. Yet, quantification of waning is a subtle task. The classical approaches cannot be interpreted as measures of declining efficacy unless we impose unreasonable assumptions. Recently, formal causal estimands designed to quantify vaccine waning have been proposed. These estimands can be bounded under weaker assumptions, but the bounds are often too wide to make claims about the presence of vaccine waning. We propose an alternative approach: a formal test to determine whether a treatment effect is constant over time. This test not only gives a considerable power gain compared to existing approaches but is also valid under plausible assumptions that are expected to hold in vaccine trials. We illustrate the increase in power through real and simulated examples, using three different approaches to compute the test statistics. Two of these approaches are based solely on summary data, accessible from existing clinical trials. Beyond our test, we also give new results that bound the waning effect. We use our methods to reanalyze data from a randomized controlled trial of the BNT162b2 COVID-19 vaccine. While prior analysis did not establish waning, our test rejects the null hypothesis of no waning."}
{"id": "2511.21773", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.21773", "abs": "https://arxiv.org/abs/2511.21773", "authors": ["Yoonseul Choi", "Jungsoon Choi"], "title": "Spatial Two-Stage Hierarchical Optimization Analysis for Site Selection of Bitcoin Mining in South Korea", "comment": "14 pages, 9 figures", "summary": "South Korea faces the dual challenge of managing growing distributed solar energy surpluses and the high energy demand of industries like Bitcoin mining. Leveraging mining operations as a flexible load to monetize this `net-metering surplus' presents a viable synergy, but requires a robust site selection methodology. Traditional GIS-based Multi-Criteria Decision Analysis (MCDA) struggles with subjective weighting and integrating heterogeneous spatial data (areal-level and lattice-level). This thesis develops and implements a Two-Stage Hierarchical Optimization framework to overcome these limitations. Stage 1 (Areal-Level) employs a cost-benefit optimization to determine the optimal number ($K^*$) and combination of regions, maximizing a final adjusted net profit by balancing surplus power revenue against detailed land and non-linear infrastructure costs. Stage 2 (Point-Level) then uses a GIS-based sliding window search within these selected regions, applying topographic (slope $< 6.0^\\circ$) and land-use constraints at a 30m resolution to identify physically constructible `unit sites'. The model identified an optimal configuration of $K^*=3$ regions (Yongin, Damyang, Miryang) yielding a maximum potential net profit of approximately \\$307 million. Crucially, the Stage 2 screening revealed that Yongin, the most profitable region, was also the most physically constrained, 87\\% of sites filtered out. This research contributes a scalable, objective framework for energy infrastructure siting that effectively integrates multi-scale spatial data. It provides a data-driven strategy for policymakers and grid operators (like Korea Electric Power Corporation) to monetize curtailed renewables and enhance grid stability."}
{"id": "2511.22564", "categories": ["stat.CO", "math.NA", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.22564", "abs": "https://arxiv.org/abs/2511.22564", "authors": ["Ruiyu Han"], "title": "Convergence of a Sequential Monte Carlo algorithm towards multimodal distributions on Rd", "comment": "arXiv admin note: text overlap with arXiv:2508.02763", "summary": "We study a sequential Monte Carlo algorithm to sample from the Gibbs measure supported on Rd with a non-convex energy function at a low temperature. In an earlier joint work, we proved that the algorithm samples from Gibbs measures supported on torus with time complexity that is polynomial in the inverse temperature; however, the approach breaks down in the non-compact setting. This work overcomes this obstacle and establishes a similar result for sampling from Gibbs measures supported on Rd. Our main result shows convergence of Monte Carlo estimators with time complexity that, approximately, scales like the seventh power of the inverse temperature, the square of the inverse allowed absolute error and the square of the inverse allowed probability error."}
{"id": "2511.21915", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.21915", "abs": "https://arxiv.org/abs/2511.21915", "authors": ["Aleksei G. Sorokin"], "title": "Algorithms and Scientific Software for Quasi-Monte Carlo, Fast Gaussian Process Regression, and Scientific Machine Learning", "comment": "PhD thesis", "summary": "Most scientific domains elicit the development of efficient algorithms and accessible scientific software. This thesis unifies our developments in three broad domains: Quasi-Monte Carlo (QMC) methods for efficient high-dimensional integration, Gaussian process (GP) regression for high-dimensional interpolation with built-in uncertainty quantification, and scientific machine learning (sciML) for modeling partial differential equations (PDEs) with mesh-free solvers. For QMC, we built new algorithms for vectorized error estimation and developed QMCPy (https://qmcsoftware.github.io/QMCSoftware/): an open-source Python interface to randomized low-discrepancy sequence generators, automatic variable transforms, adaptive error estimation procedures, and diverse use cases. For GPs, we derived new digitally-shift-invariant kernels of higher-order smoothness, developed novel fast multitask GP algorithms, and produced the scalable Python software FastGPs (https://alegresor.github.io/fastgps/). For sciML, we developed a new algorithm capable of machine precision recovery of PDEs with random coefficients. We have also studied a number of applications including GPs for probability of failure estimation, multilevel GPs for the Darcy flow equation, neural surrogates for modeling radiative transfer, and fast GPs for Bayesian multilevel QMC."}
{"id": "2511.21973", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21973", "abs": "https://arxiv.org/abs/2511.21973", "authors": ["Siyu Heng", "Yuan Huang", "Hyunseung Kang"], "title": "A Non-Bipartite Matching Framework for Difference-in-Differences with General Treatment Types", "comment": "This working manuscript reports preliminary results from our ongoing research on this topic. Additional simulation studies, data applications, methodological developments, and detailed results are still in progress and will be incorporated in future versions. This document is therefore not a final version of our work", "summary": "Difference-in-differences (DID) is one of the most widely used causal inference frameworks in observational studies. However, most existing DID methods are designed for binary treatments and cannot be readily applied to non-binary treatment settings. Although recent work has begun to extend DID to non-binary (e.g., continuous) treatments, these approaches typically require strong additional assumptions, including parametric outcome models or the presence of idealized comparison units with (nearly) static treatment levels over time (commonly called ``stayers'' or ``quasi-stayers''). In this technical note, we introduce a new non-bipartite matching framework for DID that naturally accommodates general treatment types (e.g., binary, ordinal, or continuous). Our framework makes three main contributions. First, we develop an optimal non-bipartite matching design for DID that jointly balances baseline covariates across comparable units (reducing bias) and maximizes contrasts in treatment trajectories over time (improving efficiency). Second, we establish a post-matching randomization condition, the design-based counterpart to the traditional parallel-trends assumption, which enables valid design-based inference. Third, we introduce the sample average DID ratio, a finite-population-valid and fully nonparametric causal estimand applicable to arbitrary treatment types. Our design-based approach that preserves the full treatment-dose information, avoids parametric assumptions, does not rely on the existence of stayers or quasi-stayers, and operates entirely within a finite-population framework, without appealing to hypothetical super-populations or outcome distributions."}
{"id": "2511.21938", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21938", "abs": "https://arxiv.org/abs/2511.21938", "authors": ["Ian Laga"], "title": "Survey-Based Estimation of Probe Group Sizes in the Network Scale-up Method: A Case Study from Jordan", "comment": null, "summary": "Estimating the size of marginalized populations is a persistent challenge in survey statistics and public health, especially where stigma and legal restrictions exclude such groups from census and administrative data. Migrant domestic workers in Jordan represent one such population. We employ the Network Scale-up Method using the direct probe group method, estimating probe group sizes from survey respondents' own membership rather than relying on external counts. Using data from a nationally representative household survey in Jordan, we combine the direct probe group method with Bayesian logistic mixed-effects models to stabilize small-area estimates at the Governorate level. We validate the method against census data, demonstrating that direct probe group estimates yield reliable inference and provide a practical alternative where known probe group sizes are unavailable. Our results highlight regional variation in social network size and connectivity to migrant domestic workers. We argue that the direct probe group method is more likely to satisfy the conditions required for unbiased estimation than relying on official record sizes. This work provides the first systematic validation of the direct probe group method in a small-area setting and offers guidance for adapting the Network Scale-up Method to surveys with limited sample sizes."}
{"id": "2511.23144", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.23144", "abs": "https://arxiv.org/abs/2511.23144", "authors": ["Riko Kelter", "Samuel Pawel"], "title": "The Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors", "comment": "39 pages, 6 figures", "summary": "Sequential trial design is an important statistical approach to increase the efficiency of clinical trials. Bayesian sequential trial design relies primarily on conducting a Monte Carlo simulation under the hypotheses of interest and investigating the resulting design characteristics via Monte Carlo estimates. This approach has several drawbacks, namely that replicating the calibration of a Bayesian design requires repeating a possibly complex Monte Carlo simulation. Furthermore, Monte Carlo standard errors are required to judge the reliability of the simulation. All of this is due to a lack of closed-form or numerical approaches to calibrate a Bayesian design which uses Bayes factors. In this paper, we propose the Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors. The optimal two-stage Bayes factor design is a sequential clinical trial design that is built on the idea of trinomial tree branching, a method we propose to correct the resulting design characteristics for introducing a single interim analysis. We build upon this idea to invent a calibration algorithm which yields the optimal Bayesian design that minimizes the expected sample size under the null hypothesis. Examples show that our design recovers Simon's two-stage optimal design as a special case, improves upon non-sequential Bayesian design based on Bayes factors, and can be calibrated quickly, as it makes use only of standard numerical techniques instead of time-consuming Monte Carlo simulations. Furthermore, the design allows to ensure a minimum probability on compelling evidence in favour of the null hypothesis, which is not possible with other designs. As the idea of trinomial tree branching is neither dependent on the endpoint, nor on the use of Bayes factors, the design can therefore be generalized to other settings, too."}
{"id": "2511.22003", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22003", "abs": "https://arxiv.org/abs/2511.22003", "authors": ["Yuanzhe Ma", "Hongseok Namkoong"], "title": "A Sensitivity Approach to Causal Inference Under Limited Overlap", "comment": null, "summary": "Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap."}
{"id": "2511.21992", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.21992", "abs": "https://arxiv.org/abs/2511.21992", "authors": ["Zhe Chen", "Xinran Li", "Michael O. Harhay", "Bo Zhang"], "title": "Design-based nested instrumental variable analysis", "comment": null, "summary": "Two binary instrumental variables (IVs) are nested if individuals who comply under one binary IV also comply under the other. This situation often arises when the two IVs represent different intensities of encouragement or discouragement to take the treatment--one stronger than the other. In a nested IV structure, treatment effects can be identified for two latent subgroups: always-compliers and switchers. Always-compliers are individuals who comply even under the weaker IV, while switchers are those who do not comply under the weaker IV but do under the stronger IV. We introduce a novel pair-of-pairs nested IV design, where each matched stratum consists of four units organized in two pairs. Under this design, we develop design-based inference for estimating the always-complier sample average treatment effect (ACO-SATE) and switcher sample average treatment effect (SW-SATE). In a nested IV analysis, IV assignment is randomized within each IV pair; however, whether a study unit receives the weaker or stronger IV may not be randomized. To address this complication, we then propose a novel partly biased randomization scheme and study design-based inference under this new scheme. Using extensive simulation studies, we demonstrate the validity of the proposed method and assess its power under different scenarios. Applying the nested IV framework, we estimated that 52.2% (95% CI: 50.4%-53.9%) of participants enrolled at the Henry Ford Health System in the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial were always-compliers, while 26.7% (95% CI: 24.5%-28.9%) were switchers. Among always-compliers, flexible sigmoidoscopy was associated with a trend toward a decreased colorectal cancer rate. No effect was detected among switchers. This offers a richer interpretation of why no increase in the intention-to-treat effect was observed after 1997, even though the compliance rate rose."}
{"id": "2511.22040", "categories": ["stat.AP", "math.PR", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22040", "abs": "https://arxiv.org/abs/2511.22040", "authors": ["Saman Hosseini", "Lee W. Cohnstaedt", "Caterina Scoglio"], "title": "A two-parameter, minimal-data model to predict dengue cases: the 2022-2023 outbreak in Florida, USA", "comment": null, "summary": "Reliable and timely dengue predictions provide actionable lead time for targeted vector control and clinical preparedness, reducing preventable diseases and health-system costs in at-risk communities. Dengue forecasting often relies on site-specific covariates and entomological data, limiting generalizability in data-sparse settings. We propose a data-parsimonious (DP) framework based on the incidence versus cumulative cases (ICC) curve, extending it from a basic SIR to a two-population SEIR model for dengue. Our DP model uses only the target season's incidence time series and estimates only two parameters, reducing noise and computational burden. A Bayesian extension quantifies the case reporting and fitting uncertainty to produce calibrated predictive intervals. We evaluated the performance of the DP model in the 2022-2023 outbreaks in Florida, where standardized clinical tests and reporting support accurate case determination. The DP framework demonstrates competitive predictive performance at substantially lower computational cost than more elaborate models, making it suitable for dengue early detection where dense surveillance or long historical records are unavailable."}
{"id": "2511.23212", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.23212", "abs": "https://arxiv.org/abs/2511.23212", "authors": ["Tomoshige Nakamura", "Hiroshi Shiraishi"], "title": "Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests", "comment": null, "summary": "Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a \"phase transition\" phenomenon governed by the subsampling rate $β$ (where $s \\asymp n^β$). We prove that in the bias-dominated regime ($β\\ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings."}
{"id": "2511.22004", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22004", "abs": "https://arxiv.org/abs/2511.22004", "authors": ["Eliot Wong-Toi", "Alex Boyd", "Vincent Fortuin", "Stephan Mandt"], "title": "On the Effect of Regularization on Nonparametric Mean-Variance Regression", "comment": null, "summary": "Uncertainty quantification is vital for decision-making and risk assessment in machine learning. Mean-variance regression models, which predict both a mean and residual noise for each data point, provide a simple approach to uncertainty quantification. However, overparameterized mean-variance models struggle with signal-to-noise ambiguity, deciding whether prediction targets should be attributed to signal (mean) or noise (variance). At one extreme, models fit all training targets perfectly with zero residual noise, while at the other, they provide constant, uninformative predictions and explain the targets as noise. We observe a sharp phase transition between these extremes, driven by model regularization. Empirical studies with varying regularization levels illustrate this transition, revealing substantial variability across repeated runs. To explain this behavior, we develop a statistical field theory framework, which captures the observed phase transition in alignment with experimental results. This analysis reduces the regularization hyperparameter search space from two dimensions to one, significantly lowering computational costs. Experiments on UCI datasets and the large-scale ClimSim dataset demonstrate robust calibration performance, effectively quantifying predictive uncertainty."}
{"id": "2511.22049", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22049", "abs": "https://arxiv.org/abs/2511.22049", "authors": ["Joshua Richland", "Tuomo Kiiskinen", "William Wang", "Sophia Lu", "Balasubramanian Narasimhan", "Manuel Rivas", "Robert Tibshirani"], "title": "Univariate-Guided Sparse Regression for Biobank-Scale High-Dimensional -omics Data", "comment": null, "summary": "We present a scalable framework for computing polygenic risk scores (PRS) in high-dimensional genomic settings using the recently introduced Univariate-Guided Sparse Regression (uniLasso). UniLasso is a two-stage penalized regression procedure that leverages univariate coefficients and magnitudes to stabilize feature selection and enhance interpretability. Building on its theoretical and empirical advantages, we adapt uniLasso for application to the UK Biobank, a population-based repository comprising over one million genetic variants measured on hundreds of thousands of individuals from the United Kingdom. We further extend the framework to incorporate external summary statistics to increase predictive accuracy. Our results demonstrate that the adapted uniLasso attains predictive performance comparable to standard Lasso while selecting substantially fewer variants, yielding sparser and more interpretable models. Moreover, it exhibits superior performance in estimating PRS relative to its competitors, such as PRS-CS. Integrating external scores further improves prediction while maintaining sparsity."}
{"id": "2511.22430", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.22430", "abs": "https://arxiv.org/abs/2511.22430", "authors": ["Alexandre Delporte", "Susanne Ditlevsen", "Adeline Samson"], "title": "Spatial constraints improve filtering of measurement noise from animal tracks", "comment": null, "summary": "Advances in tracking technologies for animal movement require new statistical tools to better exploit the increasing amount of data. Animal positions are usually calculated using the GPS or Argos satellite system and include potentially complex non-Gaussian and heavy-tailed measurement error patterns. Errors are usually handled through a Kalman filter algorithm, which can be sensitive to non-Gaussian error distributions.\n  In this paper, we introduce a realistic latent movement model through an underdamped Langevin stochastic differential equation (SDE) that includes an additional drift term to ensure that the animal remains in a known spatial domain of interest. This can be applied to aquatic animals moving in water or terrestrial animals moving in a restricted zone delimited by fences or natural barriers. We demonstrate that the incorporation of these spatial constraints into the latent movement model improves the accuracy of filtering for noisy observations of the positions. We implement an Extended Kalman Filter as well as a particle filter adapted to non-Gaussian error distributions. Our filters are based on solving the SDE through splitting schemes to approximate the latent dynamic."}
{"id": "2511.22065", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22065", "abs": "https://arxiv.org/abs/2511.22065", "authors": ["Shibo Diao"], "title": "Support Vector Machine Classifier with Rescaled Huberized Pinball Loss", "comment": null, "summary": "Support vector machines are widely used in machine learning classification tasks, but traditional SVM models suffer from sensitivity to outliers and instability in resampling, which limits their performance in practical applications. To address these issues, this paper proposes a novel rescaled Huberized pinball loss function with asymmetric, non-convex, and smooth properties. Based on this loss function, we develop a corresponding SVM model called RHPSVM (Rescaled Huberized Pinball Loss Support Vector Machine). Theoretical analyses demonstrate that RHPSVM conforms to Bayesian rules, has a strict generalization error bound, a bounded influence function, and controllable optimality conditions, ensuring excellent classification accuracy, outlier insensitivity, and resampling stability. Additionally, RHPSVM can be extended to various advanced SVM variants by adjusting parameters, enhancing its flexibility. We transform the non-convex optimization problem of RHPSVM into a series of convex subproblems using the concave-convex procedure (CCCP) and solve it with the ClipDCD algorithm, which is proven to be convergent. Experimental results on simulated data, UCI datasets, and small-sample crop leaf image classification tasks show that RHPSVM outperforms existing SVM models in both noisy and noise-free scenarios, especially in handling high-dimensional small-sample data."}
{"id": "2511.22152", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22152", "abs": "https://arxiv.org/abs/2511.22152", "authors": ["Miodrag M. Lovric"], "title": "The Bayes Factor Reversal Paradox", "comment": "8 pages, 1 figure, 1 table, supplementary material. Submitted to Biometrika", "summary": "In 1957, Lindley published \"A statistical paradox\" in Biometrika, revealing a fundamental conflict between frequentist and Bayesian inference as sample size approaches infinity. We present a new paradox of a different kind: a conflict within Bayesian inference itself. In the normal model with known variance, we prove that for any two-sided statistically significant result at the 0.05 level there exist prior variances such that the Bayes factor indicates evidence for the alternative with one choice while indicating evidence for the null with another. Thus, the same data, testing the same hypothesis, can yield opposite conclusions depending solely on prior choice. This answers Robert's 2016 call to investigate the impact of the prior scale on Bayes factors and formalises his concern that this choice involves arbitrariness to a high degree. Unlike the Jeffreys-Lindley paradox, which requires sample size approaching infinity, the paradox we identify occurs with realistic sample sizes."}
{"id": "2511.22550", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.22550", "abs": "https://arxiv.org/abs/2511.22550", "authors": ["Yacine Mohamed Idir", "Olivier Orfila", "Patrice Chatellier", "Vincent Judalet", "Valentin Guaffre"], "title": "Mapping urban air quality using mobile and fixed low cost sensors: a model comparison", "comment": null, "summary": "This study addresses the critical challenge of modeling and mapping urban air quality to ascertain pollutant concentrations in unmonitored locations. The advent of low-cost sensors, particularly those deployed in vehicular networks, presents novel datasets that hold the potential to enhance air quality modeling. This research conducts a comprehensive review of ten statistical models drawn from existing literature, using both fixed and mobile low-cost sensor data, alongside ancillary variables, within the urban confines of Nantes, France.\n  Employing a methodology that includes cross-validation of data from low-cost sensors and validation on fixed air quality monitoring stations, this paper evaluates the models' performance in scenarios of temporal interpolation and prediction. Our findings reveal a pronounced bias in the model outputs when reliant on low-cost sensor data compared to the verification data obtained from fixed stations. Furthermore, machine learning models demonstrated superior performance in predictive scenarios, suggesting their enhanced suitability for forecasting tasks.\n  The study conclusively indicates that reliance solely on data from low-cost mobile sensors compromises the reliability of air quality models, due to significant accuracy deficiencies. Consequently, we advocate for a directed focus towards the integration and calibration of low-cost sensor data with information from fixed monitoring stations. This approach, rather than an exclusive emphasis on the complexity of statistical modeling techniques, is pivotal for achieving the precision required for effective air quality management and policy-making."}
{"id": "2511.22270", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22270", "abs": "https://arxiv.org/abs/2511.22270", "authors": ["Zhongjie Shi", "Puyu Wang", "Chenyang Zhang", "Yuan Cao"], "title": "Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs", "comment": null, "summary": "Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration. Moreover, we identify a concrete learning task where DP-GD can achieve superior generalization performance compared to GD in training two-layer Huberized ReLU convolutional neural networks (CNNs). Specifically, we demonstrate that, under mild conditions, a small signal-to-noise ratio can result in GD producing training models with poor test accuracy, whereas DP-GD can yield training models with good test accuracy and privacy guarantees if the signal-to-noise ratio is not too small. This indicates that DP-GD has the potential to enhance model performance while ensuring privacy protection in certain learning tasks. Numerical simulations are further conducted to support our theoretical results."}
{"id": "2511.22223", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22223", "abs": "https://arxiv.org/abs/2511.22223", "authors": ["Keunbaik Lee", "Eun Jin Jang", "Dipak Dey"], "title": "Overall marginalized models for longitudinal zero-inflated count data", "comment": "29 pages", "summary": "To analyze longitudinal zero-inflated count data, we extend existing models by introducing marginalized zero-inflated Poisson (MZIP) models with random effects, which explicitly capture the marginal effect of covariates and address limitations of previous methods. These models provide a clearer interpretation of the overall mean effect of covariates on zero-inflated count data. To further accommodate overdispersion, we develop marginalized zero-inflated negative binomial (MZINB) models. Both models incorporate subject-specific heterogeneity through a flexible random effects covariance structure. Simulation studies are conducted to evaluate the performance of the MZIP and MZINB models, comparing their inference under both homogeneous and heterogeneous random effects. Finally, we illustrate the applicability of the proposed models through an analysis of systemic lupus erythematosus data."}
{"id": "2511.22649", "categories": ["stat.AP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.22649", "abs": "https://arxiv.org/abs/2511.22649", "authors": ["Daniel D. Reidpath"], "title": "The Causal Uncertainty Principle", "comment": "7 pages, 2 figures, including supplementary material", "summary": "This paper explains why internal and external validity cannot be simultaneously maximised. It introduces \"evidential states\" to represent the information available for causal inference and shows that routine study operations (restriction, conditioning, and intervention) transform these states in ways that do not commute. Because each operation removes or reorganises information differently, changing their order yields evidential states that support different causal claims. This non-commutativity creates a structural trade-off: the steps that secure precise causal identification also eliminate the heterogeneity required for generalisation. Small model, observational and experimental examples illustrate how familiar failures of transportability arise from this order dependence. The result is a concise structural account of why increasing causal precision necessarily narrows the world to which findings apply."}
{"id": "2511.22273", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22273", "abs": "https://arxiv.org/abs/2511.22273", "authors": ["Zaile Li", "Weiwei Fan", "L. Jeff Hong"], "title": "UCB for Large-Scale Pure Exploration: Beyond Sub-Gaussianity", "comment": null, "summary": "Selecting the best alternative from a finite set represents a broad class of pure exploration problems. Traditional approaches to pure exploration have predominantly relied on Gaussian or sub-Gaussian assumptions on the performance distributions of all alternatives, which limit their applicability to non-sub-Gaussian especially heavy-tailed problems. The need to move beyond sub-Gaussianity may become even more critical in large-scale problems, which tend to be especially sensitive to distributional specifications. In this paper, motivated by the widespread use of upper confidence bound (UCB) algorithms in pure exploration and beyond, we investigate their performance in the large-scale, non-sub-Gaussian settings. We consider the simplest category of UCB algorithms, where the UCB value for each alternative is defined as the sample mean plus an exploration bonus that depends only on its own sample size. We abstract this into a meta-UCB algorithm and propose letting it select the alternative with the largest sample size as the best upon stopping. For this meta-UCB algorithm, we first derive a distribution-free lower bound on the probability of correct selection. Building on this bound, we analyze two general non-sub-Gaussian scenarios: (1) all alternatives follow a common location-scale structure and have bounded variance; and (2) when such a structure does not hold, each alternative has a bounded absolute moment of order $q > 3$. In both settings, we show that the meta-UCB algorithm and therefore a broad class of UCB algorithms can achieve the sample optimality. These results demonstrate the applicability of UCB algorithms for solving large-scale pure exploration problems with non-sub-Gaussian distributions. Numerical experiments support our results and provide additional insights into the comparative behaviors of UCB algorithms within and beyond our meta-UCB framework."}
{"id": "2511.22274", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22274", "abs": "https://arxiv.org/abs/2511.22274", "authors": ["Chenxiao Dai", "Feiyu Jiang", "Dong Li", "Xiaofeng Shao"], "title": "Diagnostic Checking for Wasserstein Autoregression", "comment": null, "summary": "Wasserstein autoregression provides a robust framework for modeling serial dependence among probability distributions, with wide-ranging applications in economics, finance, and climate science. In this paper, we develop portmanteau-type diagnostic tests for assessing the adequacy of Wasserstein autoregressive models. By defining autocorrelation functions for model errors and residuals in the Wasserstein space, we construct two related tests: one analogous to the classical McLeod type test, and the other based on the sample-splitting approach of Davis and Fernandes(2025). We establish that, under mild regularity conditions, the corresponding test statistics converge in distribution to chi-square limits. Simulation studies and empirical applications demonstrate that the proposed tests effectively detect model mis-specification, offering a principled and reliable diagnostic tool for distributional time series analysis."}
{"id": "2511.23067", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.23067", "abs": "https://arxiv.org/abs/2511.23067", "authors": ["Abhishek Singh", "Aaditya Jadhav", "Abha Goyal", "Jesma V", "Vyshna I C"], "title": "A novel method to analyze pattern shifts in rainfall using cluster analysis and probability models", "comment": null, "summary": ": One of the prominent challenges being faced by agricultural sciences is the onset of climate change which is adversely affecting every aspect of cropping. Modelling of climate change at macro level have been carried out at large scale and there is ample amount of research publications available for that. But at micro level like at state level or district level there are lesser studies. District level studies can help in preparing specific plans for the mitigation of adverse effects of climate change at local level. An attempt has been made in this paper to model the monthly rainfall of Varanasi district of the state of Uttar Pradesh with the help of probability models. Firstly, the pattern of the climate change over 122 years has been unveiled by using exploratory analysis and using multivariate techniques like cluster analysis and then probability models have been fitted for selected months"}
{"id": "2511.22296", "categories": ["stat.ML", "astro-ph.IM", "astro-ph.SR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22296", "abs": "https://arxiv.org/abs/2511.22296", "authors": ["Javier Lopez-Santiago", "Luca Martino", "Joaquin Miguez", "Gonzalo Vazquez-Vilar"], "title": "Data-driven informative priors for Bayesian inference with quasi-periodic data", "comment": "Accepted for publication in AJ. 19 pages (one column), 14 figures", "summary": "Bayesian computational strategies for inference can be inefficient in approximating the posterior distribution in models that exhibit some form of periodicity. This is because the probability mass of the marginal posterior distribution of the parameter representing the period is usually highly concentrated in a very small region of the parameter space. Therefore, it is necessary to provide as much information as possible to the inference method through the parameter prior distribution. We intend to show that it is possible to construct a prior distribution from the data by fitting a Gaussian process (GP) with a periodic kernel. More specifically, we want to show that it is possible to approximate the marginal posterior distribution of the hyperparameter corresponding to the period in the kernel. Subsequently, this distribution can be used as a prior distribution for the inference method. We use an adaptive importance sampling method to approximate the posterior distribution of the hyperparameters of the GP. Then, we use the marginal posterior distribution of the hyperparameter related to the periodicity in order to construct a prior distribution for the period of the parametric model. This workflow is empirical Bayes, implemented as a modular (cut) transfer of a GP posterior for the period to the parametric model. We applied the proposed methodology to both synthetic and real data. We approximated the posterior distribution of the period of the GP kernel and then passed it forward as a posterior-as-prior with no feedback. Finally, we analyzed its impact on the marginal posterior distribution."}
{"id": "2511.22414", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22414", "abs": "https://arxiv.org/abs/2511.22414", "authors": ["Camille Frévent"], "title": "Investigating new, signature-based, spatial autoregressive models for functional covariates", "comment": null, "summary": "We developed two new alternatives to signature-based, spatial autoregressive models. In a simulation study, we found that the new models performed at least as well as existing approaches but presented shorter computation times. We then used the new models to analyze the premature mortality rate and the mortality rate for people aged 65 and over."}
{"id": "2511.23140", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.23140", "abs": "https://arxiv.org/abs/2511.23140", "authors": ["Patrick Souza Lima", "Paulo Roberto Santana dos Reis", "Alex Álisson Bandeira Santos", "Ehecatl Antonio del Río Chanona", "Idelfonso Bessa dos Reis Nogueira"], "title": "Multi-fidelity Bayesian Optimization Framework for CFD-Based Non-Premixed Burner Design", "comment": null, "summary": "We propose a multi-fidelity Bayesian optimization (MF-BO) framework that integrates computational fluid dynamics (CFD) evaluations with Gaussian-process surrogates to efficiently navigate the accuracy-cost trade-off induced by mesh resolution. The design vector x = [h, l, s] (height, length, and mesh element size) defines a continuous fidelity index Z(h, l, s), enabling the optimizer to adaptively combine low- and high-resolution simulations. This framework is applied to a non-premixed burner configuration targeting improved thermal efficiency under hydrogen-enriched fuels. A calibrated runtime model t_hat(h, l, s) penalizes computationally expensive queries, while a constrained noisy expected improvement (qNEI) guides sampling under an emissions cap of 2e-6 for NOx.\n  Surrogates trained on CFD data exhibit stable hyperparameters and physically consistent sensitivities: mean temperature increases with reactor length and fidelity and is weakly negative with height; NOx grows with temperature yet tends to decrease with length. The best design achieves T_bar approx 2.0e3 K while satisfying the NOx limit.\n  Relative to a hypothetical single-fidelity campaign (Z = 1), the MF-BO achieves comparable convergence with about 57 percent lower total wall time by learning the design landscape through fast low-Z evaluations and reserving high-Z CFD for promising candidates. Overall, the methodology offers a generalizable and computationally affordable path for optimizing reacting-flow systems in which mesh-driven fidelity inherently couples accuracy, cost, and emissions. This highlights its potential to accelerate design cycles and reduce resource requirements in industrial burner development and other high-cost CFD-driven applications."}
{"id": "2511.23205", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23205", "abs": "https://arxiv.org/abs/2511.23205", "authors": ["Shiqin Tang", "Yining Dong", "S. Joe Qin"], "title": "A PLS-Integrated LASSO Method with Application in Index Tracking", "comment": null, "summary": "In traditional multivariate data analysis, dimension reduction and regression have been treated as distinct endeavors. Established techniques such as principal component regression (PCR) and partial least squares (PLS) regression traditionally compute latent components as intermediary steps -- although with different underlying criteria -- before proceeding with the regression analysis. In this paper, we introduce an innovative regression methodology named PLS-integrated Lasso (PLS-Lasso) that integrates the concept of dimension reduction directly into the regression process. We present two distinct formulations for PLS-Lasso, denoted as PLS-Lasso-v1 and PLS-Lasso-v2, along with clear and effective algorithms that ensure convergence to global optima. PLS-Lasso-v1 and PLS-Lasso-v2 are compared with Lasso on the task of financial index tracking and show promising results."}
{"id": "2511.22432", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22432", "abs": "https://arxiv.org/abs/2511.22432", "authors": ["Camille Frévent"], "title": "A signature-based spatial scan statistic for functional data", "comment": null, "summary": "We have developed a new signature-based spatial scan statistic for functional data (SigFSS). This scan statistic can be applied to both univariate and multivariate functional data. In a simulation study, SigFSS almost always performed better than the literature approaches and yielded more precise clusters in geographic terms. Lastly, we used SigFSS to search for spatial clusters of abnormally high or abnormally low mortality rates in mainland France."}
{"id": "2511.23156", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.23156", "abs": "https://arxiv.org/abs/2511.23156", "authors": ["Sanne M. van Essen", "Harleigh C. Seyffert"], "title": "Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures", "comment": "Figures in arxiv compilation all placed at the end; to be fixed in final version", "summary": "To ensure the safety of marine and coastal structures, extreme (design) values should be known at the design stage. But for such complex systems, estimating the magnitude of events which are both non-linear and rare is extremely challenging, and involves considerable computational cost to capture the high-fidelity physics. To address this challenge, we offer a new multi-fidelity screening method, Probabilistic Adaptive Screening (PAS), which accurately predicts extreme values of strongly non-linear wave-induced loads while minimising the required high-fidelity simulation duration. The method introduces a probabilistic approach to multi-fidelity screening, allowing efficient linear potential flow indicators to be used in the low-fidelity stage, even for strongly non-linear load cases. The method is validated against a range of cases, including non-linear waves, and ship vertical bending moments, green water impact loads, and slamming loads. It can be concluded that PAS accurately estimates both the short-term distributions and extreme values in all test cases, with most probable maximum (MPM) values within 10\\% of the available full brute-force Monte-Carlo Simulation (MCS) results. In addition, PAS achieves this performance very efficiently, requiring less than 4\\% of the high-fidelity simulation time needed for conventional MCS. These results demonstrate that PAS can reliably reproduce the statistics of both weakly and strongly non-linear extreme load problems, while significantly reducing the associated computational cost. The present study validates the statistical PAS framework; further work should focus on validating the full procedure including CFD load simulations, and on validating it for long-term extremes."}
{"id": "2511.23212", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.23212", "abs": "https://arxiv.org/abs/2511.23212", "authors": ["Tomoshige Nakamura", "Hiroshi Shiraishi"], "title": "Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests", "comment": null, "summary": "Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a \"phase transition\" phenomenon governed by the subsampling rate $β$ (where $s \\asymp n^β$). We prove that in the bias-dominated regime ($β\\ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings."}
{"id": "2511.22500", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22500", "abs": "https://arxiv.org/abs/2511.22500", "authors": ["Yacine Mohamed Idir", "Olivier Orfila", "Patrice Chatellier", "Vincent Judalet"], "title": "Improving Spatio-temporal Gaussian Process Modeling with Vecchia Approximation: A Low-Cost Sensor-Driven Approach to Urban Environmental Monitoring", "comment": null, "summary": "This paper explores Vecchia likelihood approximation for modeling physical phenomena sensed by mobile and fixed low-cost sensors in urban environments. A three-level hierarchical model is proposed to simultaneously accounts for the physical process of interest and measurement errors inherent in low-cost sensors. Several innovative configurations of Vecchia's approximation are investigated, including variations in ordering strategies, distance definitions, and sensor-specific conditioning. These configurations are evaluated for approximating the likelihood of a spatio-temporal Gaussian process, using simulated data based on real mobile sensor trajectories across Nantes, France. Our findings highlight the effectiveness of the min-max distance algorithm for ordering, reaffirming existing literature. Additionally, we demonstrate the utility of a random ordering approach that doesn't require prior definition of a spatio-temporal distance. These two ordering configurations achieved, on average, 102\\% better results in log Kullback-Leibler divergence compared with four other ordering schemes studied. Results are supplemented with Asymptotic Relative Efficiency analysis, offering practical recommendations for optimizing parameter estimation. The proposed model and preferred Vecchia configuration are applied to real-world air quality data collected using mobile and fixed low-cost sensors. This application underscores the model's practical value for pollution mapping and prediction in environmental monitoring. This study advances the use of Vecchia's approximation for addressing computational challenges of Gaussian models in large-scale spatio-temporal datasets from environmental monitoring with low-cost sensor networks."}
{"id": "2511.21992", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.21992", "abs": "https://arxiv.org/abs/2511.21992", "authors": ["Zhe Chen", "Xinran Li", "Michael O. Harhay", "Bo Zhang"], "title": "Design-based nested instrumental variable analysis", "comment": null, "summary": "Two binary instrumental variables (IVs) are nested if individuals who comply under one binary IV also comply under the other. This situation often arises when the two IVs represent different intensities of encouragement or discouragement to take the treatment--one stronger than the other. In a nested IV structure, treatment effects can be identified for two latent subgroups: always-compliers and switchers. Always-compliers are individuals who comply even under the weaker IV, while switchers are those who do not comply under the weaker IV but do under the stronger IV. We introduce a novel pair-of-pairs nested IV design, where each matched stratum consists of four units organized in two pairs. Under this design, we develop design-based inference for estimating the always-complier sample average treatment effect (ACO-SATE) and switcher sample average treatment effect (SW-SATE). In a nested IV analysis, IV assignment is randomized within each IV pair; however, whether a study unit receives the weaker or stronger IV may not be randomized. To address this complication, we then propose a novel partly biased randomization scheme and study design-based inference under this new scheme. Using extensive simulation studies, we demonstrate the validity of the proposed method and assess its power under different scenarios. Applying the nested IV framework, we estimated that 52.2% (95% CI: 50.4%-53.9%) of participants enrolled at the Henry Ford Health System in the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial were always-compliers, while 26.7% (95% CI: 24.5%-28.9%) were switchers. Among always-compliers, flexible sigmoidoscopy was associated with a trend toward a decreased colorectal cancer rate. No effect was detected among switchers. This offers a richer interpretation of why no increase in the intention-to-treat effect was observed after 1997, even though the compliance rate rose."}
{"id": "2511.23310", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23310", "abs": "https://arxiv.org/abs/2511.23310", "authors": ["Zixun Huang", "Jiayi Sheng", "Zeyu Zheng"], "title": "OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning", "comment": "19 pages, 7 figures", "summary": "Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training."}
{"id": "2511.22518", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22518", "abs": "https://arxiv.org/abs/2511.22518", "authors": ["Xin Lu", "Wanjia Fu", "Hongzi Li", "Haoyang Yu", "Honghao Zhang", "Ke Zhu", "Hanzhong Liu"], "title": "Design-based theory for causal inference", "comment": "in Chinese language", "summary": "Causal inference, as a major research area in statistics and data science, plays a central role across diverse fields such as medicine, economics, education, and the social sciences. Design-based causal inference begins with randomized experiments and emphasizes conducting statistical inference by leveraging the known randomization mechanism, thereby enabling identification and estimation of causal effects under weak model dependence. Grounded in the seminal works of Fisher and Neyman, this paradigm has evolved to include various design strategies, such as stratified randomization and rerandomization, and analytical methods including Fisher randomization tests, Neyman-style asymptotic inference, and regression adjustment. In recent years, with the emergence of complex settings involving high-dimensional data, individual noncompliance, and network interference, design-based causal inference has witnessed remarkable theoretical and methodological advances. This paper provides a systematic review of recent progress in this field, focusing on covariate-balanced randomization designs, design-based statistical inference methods, and their extensions to high-dimensional, noncompliance, and network interference scenarios. It concludes with a comprehensive perspective on future directions for the theoretical development and practical applications of causal inference."}
{"id": "2511.23144", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.23144", "abs": "https://arxiv.org/abs/2511.23144", "authors": ["Riko Kelter", "Samuel Pawel"], "title": "The Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors", "comment": "39 pages, 6 figures", "summary": "Sequential trial design is an important statistical approach to increase the efficiency of clinical trials. Bayesian sequential trial design relies primarily on conducting a Monte Carlo simulation under the hypotheses of interest and investigating the resulting design characteristics via Monte Carlo estimates. This approach has several drawbacks, namely that replicating the calibration of a Bayesian design requires repeating a possibly complex Monte Carlo simulation. Furthermore, Monte Carlo standard errors are required to judge the reliability of the simulation. All of this is due to a lack of closed-form or numerical approaches to calibrate a Bayesian design which uses Bayes factors. In this paper, we propose the Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors. The optimal two-stage Bayes factor design is a sequential clinical trial design that is built on the idea of trinomial tree branching, a method we propose to correct the resulting design characteristics for introducing a single interim analysis. We build upon this idea to invent a calibration algorithm which yields the optimal Bayesian design that minimizes the expected sample size under the null hypothesis. Examples show that our design recovers Simon's two-stage optimal design as a special case, improves upon non-sequential Bayesian design based on Bayes factors, and can be calibrated quickly, as it makes use only of standard numerical techniques instead of time-consuming Monte Carlo simulations. Furthermore, the design allows to ensure a minimum probability on compelling evidence in favour of the null hypothesis, which is not possible with other designs. As the idea of trinomial tree branching is neither dependent on the endpoint, nor on the use of Bayes factors, the design can therefore be generalized to other settings, too."}
{"id": "2511.22535", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22535", "abs": "https://arxiv.org/abs/2511.22535", "authors": ["Joris Mulder", "Robbie C. M. van Aert"], "title": "Bayes Factor Hypothesis Testing in Meta-Analyses: Practical Advantages and Methodological Considerations", "comment": "63 pages, 10 figures", "summary": "Bayesian hypothesis testing via Bayes factors offers a principled alternative to classical p-value methods in meta-analysis, particularly suited to its cumulative and sequential nature. Unlike p-values, Bayes factors allow for quantifying support both for and against the existence of an effect, facilitate ongoing evidence monitoring, and maintain coherent long-run behavior as additional studies are incorporated. Recent theoretical developments further show how Bayes factors can flexibly control Type I error rates through connections to e-value theory. Despite these advantages, their use remains limited in the meta-analytic literature. This paper provides a critical overview of their theoretical properties, methodological considerations, such as prior sensitivity, and practical advantages for evidence synthesis. Two illustrative applications are provided: one on statistical learning in individuals with language impairments, and another on seroma incidence following post-operative exercise in breast cancer patients. New tools supporting these methods are available in the open-source R package BFpack."}
{"id": "2511.22538", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22538", "abs": "https://arxiv.org/abs/2511.22538", "authors": ["Hyotae Kim", "Athanasios Kottas"], "title": "Bayesian Nonparametric Marked Hawkes Processes for Earthquake Modeling", "comment": null, "summary": "The Hawkes process is a versatile stochastic model for point patterns that exhibit self-excitation, that is, the property that an event occurrence increases the rate of occurrence for some period of time in the future. We present a Bayesian nonparametric modeling approach for temporal marked Hawkes processes. Our focus is on point process modeling of earthquake occurrences, where the mark variable is given by earthquake magnitude. We develop a nonparametric prior model for the marked Hawkes process excitation function, using a representation with basis components for the time lag and the mark, and basis weights defined through a gamma process prior. We elaborate the model with a nonparametric prior for time-dependent background intensity functions, thus enabling a fully nonparametric approach to modeling the ground process intensity of marked Hawkes processes. The model construction balances computationally tractable inference with flexible forms for marked Hawkes process functionals, including mark-dependent offspring densities. The posterior simulation method provides full inference, without any approximations to the Hawkes process likelihood. In the context of the application, the modeling approach enables estimation of aftershock densities that vary with the magnitude of the main shock, thus significantly expanding the inferential scope of existing self-exciting point process models for earthquake occurrences. We investigate different aspects of the methodology through study of model properties, and with inference results based on synthetic marked point patterns. The practical utility of modeling magnitude-dependent aftershock dynamics is demonstrated with analysis of earthquakes that occurred in Japan from 1885 through 1980."}
{"id": "2511.22544", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22544", "abs": "https://arxiv.org/abs/2511.22544", "authors": ["Yacine Mohamed Idir", "Olivier Orfila", "Vincent Judalet", "Benoit Sagot", "Patrice Chatellier"], "title": "Mapping Urban Air Quality from Mobile Sensors Using Spatio-Temporal Geostatistics", "comment": null, "summary": "With the advancement of technology and the arrival of miniaturized environmental sensors that offer greater performance, the idea of building mobile network sensing for air quality has quickly emerged to increase our knowledge of air pollution in urban environments. However, with these new techniques, the difficulty of building mathematical models capable of aggregating all these data sources in order to provide precise mapping of air quality arises. In this context, we explore the spatio-temporal geostatistics methods as a solution for such a problem and evaluate three different methods: Simple Kriging (SK) in residuals, Ordinary Kriging (OK), and Kriging with External Drift (KED). On average, geostatistical models showed 26.57% improvement in the Root Mean Squared Error (RMSE) compared to the standard Inverse Distance Weighting (IDW) technique in interpolating scenarios (27.94% for KED, 26.05% for OK, and 25.71% for SK). The results showed less significant scores in extrapolating scenarios (a 12.22% decrease in the RMSE for geostatisical models compared to IDW). We conclude that univariable geostatistics is suitable for interpolating this type of data but is less appropriate for an extrapolation of non-sampled places since it does not create any information."}
{"id": "2511.22618", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22618", "abs": "https://arxiv.org/abs/2511.22618", "authors": ["Leonardo Scandurra", "Pavlos Alexias", "Eugene de Villiers"], "title": "A Framework for Initial Transient Detection and Statistical Assessment of Convergence in CFD Simulations", "comment": null, "summary": "Time series data often contain initial transient periods before reaching a stable state, posing challenges in analysis and interpretation. In this paper, we propose a novel approach to detect and estimate the end of the initial transient in time series data. Our method leverages the reversal mean standard error (RMSE) as a metric for assessing the stability of the data. Additionally, we employ fractional filtering techniques to enhance the detection accuracy by filtering out noise and capturing essential features of the underlying dynamics.\n  Combining with autocorrelation-corrected confidence intervals we provide a robust framework to automate transient detection and convergence assessment. The method ensures statistical rigor by accounting for autocorrelation effects, validated through simulations with varying time steps. Results demonstrate independence from numerical parameters (e.g., time step size, under-relaxation factors), offering a reliable tool for steady-state analysis. The framework is lightweight, generalizable, and mitigates inflated false positives in autocorrelated datasets."}
{"id": "2511.22762", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22762", "abs": "https://arxiv.org/abs/2511.22762", "authors": ["Yuchen Hu", "Xiaoyi Wang", "Long Feng"], "title": "High dimensional Mean Test for Temporal Dependent Data", "comment": null, "summary": "This paper proposes a novel test method for high-dimensional mean testing regard for the temporal dependent data. Comparison to existing methods, we establish the asymptotic normality of the test statistic without relying on restrictive assumptions, such as Gaussian distribution or M-dependence. Importantly, our theoretical framework holds potential for extension to other high-dimensional problems involving temporal dependent data. Additionally, our method offers significantly reduced computational complexity, making it more practical for large-scale applications. Simulation studies further demonstrate the computational advantages and performance improvements of our test."}
{"id": "2511.22833", "categories": ["stat.ME", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.22833", "abs": "https://arxiv.org/abs/2511.22833", "authors": ["Angus Lewis", "Antonio Parrella", "John Maclean", "Andrew J. Black"], "title": "Gaussian approximations for fast Bayesian inference of partially observed branching processes with applications to epidemiology", "comment": null, "summary": "We consider the problem of inference for the states and parameters of a continuous-time multitype branching process from partially observed time series data. Exact inference for this class of models, typically using sequential Monte Carlo, can be computationally challenging when the populations that are being modelled grow exponentially or the time series is long. Instead, we derive a Gaussian approximation for the transition function of the process that leads to a Kalman filtering algorithm that runs in a time independent of the population sizes. We also develop a hybrid approach for when populations are smaller and the approximation is less applicable. We investigate the performance of our approximation and algorithms to both a simple and a complex epidemic model, finding good adherence to the true posterior distributions in both cases with large computational speed-ups in most cases. We also apply our method to a COVID-19 dataset with time dependent parameters where exact methods are intractable due to the population sizes involved."}
{"id": "2511.22868", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22868", "abs": "https://arxiv.org/abs/2511.22868", "authors": ["Yue Ma", "Oksana A. Chkrebtii", "Stephen R. Niezgoda"], "title": "Constrained Gaussian Random Fields with Continuous Linear Boundary Restrictions for Physics-informed Modeling of States", "comment": null, "summary": "Boundary constraints in physical, environmental and engineering models restrict smooth states such as temperature to follow known physical laws at the edges of their spatio-temporal domain. Examples include fixed-state or fixed-derivative (insulated) boundary conditions, and constraints that relate the state and the derivatives, such as in models of heat transfer. Despite their flexibility as prior models over system states, Gaussian random fields do not in general enable exact enforcement of such constraints. This work develops a new general framework for constructing linearly boundary-constrained Gaussian random fields from unconstrained Gaussian random fields over multi-dimensional, convex domains. This new class of models provides flexible priors for modeling smooth states with known physical mechanisms acting at the domain boundaries. Simulation studies illustrate how such physics-informed probability models yield improved predictive performance and more realistic uncertainty quantification in applications including probabilistic numerics, data-driven discovery of dynamical systems, and boundary-constrained state estimation, as compared to unconstrained alternatives."}
{"id": "2511.23010", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23010", "abs": "https://arxiv.org/abs/2511.23010", "authors": ["Shoji Toyota", "Yuto Miyatake"], "title": "Joint Bayesian Inference of Parameter and Discretization Error Uncertainties in ODE Models", "comment": "31 pages, submitted for a publication", "summary": "We address the problem of Bayesian inference for parameters in ordinary differential equation (ODE) models based on observational data. Conventional approaches in this setting typically rely on numerical solvers such as the Euler or Runge-Kutta methods. However, these methods generally do not account for the discretization error induced by discretizing the ODE model. We propose a Bayesian inference framework for ODE models that explicitly quantifies discretization errors. Our method models discretization error as a random variable and performs Bayesian inference on both ODE parameters and variances of the randomized discretization errors, referred to as the discretization error variance. A key idea of our approach is the introduction of a Markov prior on the temporal evolution of the discretization error variances, enabling the inference problem to be formulated as a state-space model. Furthermore, we propose a specific form of the Markov prior that arises naturally from standard discretization error analysis. This prior depends on the step size in the numerical solver, and we discuss its asymptotic property in the limit as the step size approaches zero. Numerical experiments illustrate that the proposed method can simultaneously quantify uncertainties in both the ODE parameters and the discretization errors, and can produce posterior distributions over the parameters with broader support by accounting for discretization error."}
{"id": "2511.23085", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23085", "abs": "https://arxiv.org/abs/2511.23085", "authors": ["Yongseok Hur", "Joonhyuk Jung", "Juhee Lee"], "title": "A General Bayesian Nonparametric Approach for Estimating Population-Level and Conditional Causal Effects", "comment": "Short headings : Causal Logistic Stick-Breaking Process / 26 pages / 11 figures", "summary": "We propose a Bayesian nonparametric (BNP) approach to causal inference using observational data consisting of outcome, treatment, and a set of confounders. The conditional distribution of the outcome given treatment and confounders is modeled flexibly using a dependent nonparametric mixture model, in which both the atoms and the weights vary with the confounders. The proposed BNP model is well suited for causal inference problems, as it does not rely on parametric assumptions about how the conditional distribution depends on the confounders. In particular, the model effectively adjusts for confounding and improves the modeling of treatment effect heterogeneity, leading to more accurate estimation of both the average treatment effect (ATE) and heterogeneous treatment effects (HTE). Posterior inference under the proposed model is computationally efficient due to the use of data augmentation. Extensive evaluations demonstrate that the proposed model offers competitive or superior performance compared to a wide range of recent methods spanning various statistical approaches, including Bayesian additive regression tree (BART) models, which are well known for their strong empirical performance. More importantly, the model provides fully probabilistic inference on quantities of interest that other methods cannot easily provide, using their posterior distributions."}
{"id": "2511.23086", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23086", "abs": "https://arxiv.org/abs/2511.23086", "authors": ["Srijan Chattopadhyay", "Siddhaarth Sarkar", "Arun Kumar Kuchibhotla"], "title": "Inference for quantile-parametrized families via CDF confidence bands", "comment": "29 pages total: 5 pages appendix, 3 pages references, 21 pages main content. 14 figures", "summary": "Quantile-based distribution families are an important subclass of parametric families, capable of exhibiting a wide range of behaviors using very few parameters. These parametric models present significant challenges for classical methods, since the CDF and density do not have a closed-form expression. Furthermore, approximate maximum likelihood estimation and related procedures may yield non-$\\sqrt{n}$ and non-normal asymptotics over regions of the parameter space, making bootstrap and resampling techniques unreliable. We develop a novel inference framework that constructs confidence sets by inverting distribution-free confidence bands for the empirical CDF through the known quantile function. Our proposed inference procedure provides a principled and assumption-lean alternative in this setting, requiring no distributional assumptions beyond the parametric model specification and avoiding the computational and theoretical difficulties associated with likelihood-based methods for these complex parametric families. We demonstrate our framework on Tukey Lambda and generalized Lambda distributions, evaluate its performance through simulation studies, and illustrate its practical utility with an application to both a small-sample dataset (Twin Study) and a large-sample dataset (Spanish household incomes)."}
{"id": "2511.23118", "categories": ["stat.ME", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23118", "abs": "https://arxiv.org/abs/2511.23118", "authors": ["Stefaniya Kozhevnikova", "Denis Yukhnenko", "Giulio Scola", "Seena Fazel"], "title": "Machine learning for violence prediction: a systematic review and critical appraisal", "comment": null, "summary": "Purpose To conduct a systematic review of machine learning models for predicting violent behaviour by synthesising and appraising their validity, usefulness, and performance.\n  Methods We systematically searched nine bibliographic databases and Google Scholar up to September 2025 for development and/or validation studies on machine learning methods for predicting all forms of violent behaviour. We synthesised the results by summarising discrimination and calibration performance statistics and evaluated study quality by examining risk of bias and clinical utility.\n  Results We identified 38 studies reporting the development and validation of 40 models. Most studies reported Area Under the Curve (AUC) as the discrimination statistic with a range of 0.68-0.99. Only eight studies reported calibration performance, and three studies reported external validation. 31 studies had a high risk of bias, mainly in the analysis domain, and three studies had low risk of bias. The overall clinical utility of violence prediction models is poor, as indicated by risks of overfitting due to small samples, lack of transparent reporting, and low generalisability.\n  Conclusion Although black box machine learning models currently have limited applicability in clinical settings, they may show promise for identifying high-risk individuals. We recommend five key considerations for violence prediction modelling: (i) ensuring methodological quality (e.g. following guidelines) and interdisciplinary collaborations; (ii) using black box algorithms only for highly complex data; (iii) incorporating dynamic predictions to allow for risk monitoring; (iv) developing more trustworthy algorithms using explainable methods; and (v) applying causal machine learning approaches where appropriate."}
{"id": "2511.23137", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23137", "abs": "https://arxiv.org/abs/2511.23137", "authors": ["Natalie Neumeyer", "Leonie Selk"], "title": "Goodness-of-fit testing for the error distribution in functional linear models", "comment": "This article is part of a Festschrift in honor of Marie Hušková (title of the Festschrift: Asymptotic and Methodological Statistics)", "summary": "We consider the error distribution in functional linear models with scalar response and functional covariate. Different asymptotic expansions of the empirical distribution function and the empirical characteristic function based on estimated residuals under different model assumptions are discussed. The results are applied for simple and composite goodness-of-fit testing for the error distribution, in particular testing for normal distribution."}
{"id": "2511.23144", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.23144", "abs": "https://arxiv.org/abs/2511.23144", "authors": ["Riko Kelter", "Samuel Pawel"], "title": "The Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors", "comment": "39 pages, 6 figures", "summary": "Sequential trial design is an important statistical approach to increase the efficiency of clinical trials. Bayesian sequential trial design relies primarily on conducting a Monte Carlo simulation under the hypotheses of interest and investigating the resulting design characteristics via Monte Carlo estimates. This approach has several drawbacks, namely that replicating the calibration of a Bayesian design requires repeating a possibly complex Monte Carlo simulation. Furthermore, Monte Carlo standard errors are required to judge the reliability of the simulation. All of this is due to a lack of closed-form or numerical approaches to calibrate a Bayesian design which uses Bayes factors. In this paper, we propose the Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors. The optimal two-stage Bayes factor design is a sequential clinical trial design that is built on the idea of trinomial tree branching, a method we propose to correct the resulting design characteristics for introducing a single interim analysis. We build upon this idea to invent a calibration algorithm which yields the optimal Bayesian design that minimizes the expected sample size under the null hypothesis. Examples show that our design recovers Simon's two-stage optimal design as a special case, improves upon non-sequential Bayesian design based on Bayes factors, and can be calibrated quickly, as it makes use only of standard numerical techniques instead of time-consuming Monte Carlo simulations. Furthermore, the design allows to ensure a minimum probability on compelling evidence in favour of the null hypothesis, which is not possible with other designs. As the idea of trinomial tree branching is neither dependent on the endpoint, nor on the use of Bayes factors, the design can therefore be generalized to other settings, too."}
{"id": "2511.23208", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23208", "abs": "https://arxiv.org/abs/2511.23208", "authors": ["Suehyun Kim", "Kwonsang Lee"], "title": "A Design-Based Matching Framework for Staggered Adoption with Time-Varying Confounding", "comment": "26 pages, 3 figures", "summary": "Causal inference in longitudinal datasets has long been challenging due to dynamic treatment adoption and confounding by time-varying covariates. Prior work either fails to account for heterogeneity across treatment adoption cohorts and treatment timings or relies on modeling assumptions. In this paper, we develop a novel design-based framework for inference on group- and time-specific treatment effects in panel data with staggered treatment adoption. We establish identification results for causal effects under this structure and introduce corresponding estimators, together with a block bootstrap procedure for estimating the covariance matrix and testing the homogeneity of group-time treatment effects. To implement the framework in practice, we propose the Reverse-Time Nested Matching algorithm, which constructs matched strata by pairing units from different adoption cohorts in a way that ensures comparability of covariate histories at each treatment time. Applying the algorithm to the Netflix-IPTV dataset, we find that while Netflix subscription does not significantly affect total IPTV viewing time, it does negatively affect VoD usage. We also provide statistical evidence that the causal effects of Netflix subscription may vary even within the same treatment cohort or across the same outcome and event times."}
{"id": "2511.23216", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23216", "abs": "https://arxiv.org/abs/2511.23216", "authors": ["Nikola Sekulovski", "František Bartoš", "Don van den Bergh", "Giuseppe Arena", "Henrik R. Godmann", "Vipasha Goyal", "Julius M. Pfadt", "Maarten Marsman", "Adrian E. Raftery"], "title": "Comparing Variable Selection and Model Averaging Methods for Logistic Regression", "comment": null, "summary": "Model uncertainty is a central challenge in statistical models for binary outcomes such as logistic regression, arising when it is unclear which predictors should be included in the model. Many methods have been proposed to address this issue for logistic regression, but their relative performance under realistic conditions remains poorly understood. We therefore conducted a preregistered, simulation-based comparison of 28 established methods for variable selection and inference under model uncertainty, using 11 empirical datasets spanning a range of sample sizes and numbers of predictors, in cases both with and without separation. We found that Bayesian model averaging methods based on g-priors, particularly with g = max(n, p^2), show the strongest overall performance when separation is absent. When separation occurs, penalized likelihood approaches, especially the LASSO, provide the most stable results, while Bayesian model averaging with the local empirical Bayes (EB-local) prior is competitive in both situations. These findings offer practical guidance for applied researchers on how to effectively address model uncertainty in logistic regression in modern empirical and machine learning research."}
{"id": "2511.23275", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23275", "abs": "https://arxiv.org/abs/2511.23275", "authors": ["William Laplante", "Matias Altamirano", "Jeremias Knoblauch", "Andrew Duncan", "François-Xavier Briol"], "title": "Conjugate Generalised Bayesian Inference for Discrete Doubly Intractable Problems", "comment": null, "summary": "Doubly intractable problems occur when both the likelihood and the posterior are available only in unnormalised form, with computationally intractable normalisation constants. Bayesian inference then typically requires direct approximation of the posterior through specialised and typically expensive MCMC methods. In this paper, we provide a computationally efficient alternative in the form of a novel generalised Bayesian posterior that allows for conjugate inference within the class of exponential family models for discrete data. We derive theoretical guarantees to characterise the asymptotic behaviour of the generalised posterior, supporting its use for inference. The method is evaluated on a range of challenging intractable exponential family models, including the Conway-Maxwell-Poisson graphical model of multivariate count data, autoregressive discrete time series models, and Markov random fields such as the Ising and Potts models. The computational gains are significant; in our experiments, the method is between 10 and 6000 times faster than state-of-the-art Bayesian computational methods."}
{"id": "2511.23419", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23419", "abs": "https://arxiv.org/abs/2511.23419", "authors": ["Shifeng Sun", "Xueqi Wang", "Zhuoran Hou", "Elizabeth L. Turner"], "title": "Getting it right: Methods for risk ratios and risk differences cluster randomized trials with a small number of clusters", "comment": null, "summary": "Most cluster randomized trials (CRTs) randomize fewer than 30-40 clusters in total. When performing inference for such ``small'' CRTs, it is important to use methods that appropriately account for the small sample size. When the generalized estimating equations (GEE) approach is used for analysis of ``small'' CRTs, the robust variance estimator from GEE is biased downward and therefore bias-corrected standard errors should be used. Moreover, in order to avoid inflated Type I error, an appropriate bias-corrected standard error should be paired with the t- rather than Z-statistic when making inference about a single-parameter intervention effect. Although several bias-correction methods (including Kauermann and Carroll (KC), Mancl and DeRouen (MD), Morel, Bokossa, and Neerchal (MBN), and the average of KC and MD (AVG)) have been evaluated for inference for odds ratios, their finite-sample behavior in ``small'' CRTs with few clusters has not been thoroughly investigated for risk ratios and risk differences. The current article aims to fill the gap by including analysis via binomial, Poisson and Gaussian models and for a broad spectrum of scenarios. Analysis is via binomial and Poisson models (using log and identity link for risk and differences measures, respectively). We additionally explore the use of Gaussian models with identity link for risk differences and adopt the \"modified\" approach for analysis with misspecified Poisson and Gaussian models. We consider a broad spectrum of scenarios including for rare outcomes, small cluster sizes, high intracluster correlations (ICCs), and high coefficients of variation (CVs) of cluster size."}
{"id": "2511.23433", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23433", "abs": "https://arxiv.org/abs/2511.23433", "authors": ["Maria Alejandra Valdez Cabrera", "Amy D Willis", "Armeen Taeb"], "title": "Consensus Tree Estimation with False Discovery Rate Control via Partially Ordered Sets", "comment": null, "summary": "Connected acyclic graphs (trees) are data objects that hierarchically organize categories. Collections of trees arise in a diverse variety of fields, including evolutionary biology, public health, machine learning, social sciences and anatomy. Summarizing a collection of trees by a single representative is challenging, in part due to the dimension of both the sample and parameter space. We frame consensus tree estimation as a structured feature-selection problem, where leaves and edges are the features. We introduce a partial order on leaf-labeled trees, use it to define true and false discoveries for a candidate summary tree, and develop an estimation algorithm that controls the false discovery rate at a nominal level for a broad class of non-parametric generative models. Furthermore, using the partial order structure, we assess the stability of each feature in a selected tree. Importantly, our method accommodates unequal leaf sets and non-binary trees, allowing the estimator to reflect uncertainty by collapsing poorly supported structure instead of forcing full resolution. We apply the method to study the archaeal origin of eukaryotic cells and to quantify uncertainty in deep branching orders. While consensus tree construction has historically been viewed as an estimation task, reframing it as feature selection over a partially ordered set allows us to obtain the first estimator with finite-sample and model-free guarantees. More generally, our approach provides a foundation for integrating tools from multiple testing into tree estimation."}
{"id": "2511.23466", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23466", "abs": "https://arxiv.org/abs/2511.23466", "authors": ["Danielle Paulson", "Souhardya Sengupta", "Lucas Janson"], "title": "The $L$-test: Increasing the Linear Model $F$-test's Power Under Sparsity Without Sacrificing Validity", "comment": null, "summary": "We introduce a new procedure for testing the significance of a set of regression coefficients in a Gaussian linear model with $n \\geq d$. Our method, the $L$-test, provides the same statistical validity guarantee as the classical $F$-test, while attaining higher power when the nuisance coefficients are sparse. Although the $L$-test requires Monte Carlo sampling, each sample's runtime is dominated by simple matrix-vector multiplications so that the overall test remains computationally efficient. Furthermore, we provide a Monte-Carlo-free variant that can be used for particularly large-scale multiple testing applications. We give intuition for the power of our approach, validate its advantages through extensive simulations, and illustrate its practical utility in both single- and multiple-testing contexts with an application to an HIV drug resistance dataset. In the concluding remarks, we also discuss how our methodology can be applied to a more general class of parametric models that admit asymptotically Gaussian estimators."}
{"id": "2511.21938", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21938", "abs": "https://arxiv.org/abs/2511.21938", "authors": ["Ian Laga"], "title": "Survey-Based Estimation of Probe Group Sizes in the Network Scale-up Method: A Case Study from Jordan", "comment": null, "summary": "Estimating the size of marginalized populations is a persistent challenge in survey statistics and public health, especially where stigma and legal restrictions exclude such groups from census and administrative data. Migrant domestic workers in Jordan represent one such population. We employ the Network Scale-up Method using the direct probe group method, estimating probe group sizes from survey respondents' own membership rather than relying on external counts. Using data from a nationally representative household survey in Jordan, we combine the direct probe group method with Bayesian logistic mixed-effects models to stabilize small-area estimates at the Governorate level. We validate the method against census data, demonstrating that direct probe group estimates yield reliable inference and provide a practical alternative where known probe group sizes are unavailable. Our results highlight regional variation in social network size and connectivity to migrant domestic workers. We argue that the direct probe group method is more likely to satisfy the conditions required for unbiased estimation than relying on official record sizes. This work provides the first systematic validation of the direct probe group method in a small-area setting and offers guidance for adapting the Network Scale-up Method to surveys with limited sample sizes."}
{"id": "2511.22003", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22003", "abs": "https://arxiv.org/abs/2511.22003", "authors": ["Yuanzhe Ma", "Hongseok Namkoong"], "title": "A Sensitivity Approach to Causal Inference Under Limited Overlap", "comment": null, "summary": "Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap."}
{"id": "2511.22040", "categories": ["stat.AP", "math.PR", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.22040", "abs": "https://arxiv.org/abs/2511.22040", "authors": ["Saman Hosseini", "Lee W. Cohnstaedt", "Caterina Scoglio"], "title": "A two-parameter, minimal-data model to predict dengue cases: the 2022-2023 outbreak in Florida, USA", "comment": null, "summary": "Reliable and timely dengue predictions provide actionable lead time for targeted vector control and clinical preparedness, reducing preventable diseases and health-system costs in at-risk communities. Dengue forecasting often relies on site-specific covariates and entomological data, limiting generalizability in data-sparse settings. We propose a data-parsimonious (DP) framework based on the incidence versus cumulative cases (ICC) curve, extending it from a basic SIR to a two-population SEIR model for dengue. Our DP model uses only the target season's incidence time series and estimates only two parameters, reducing noise and computational burden. A Bayesian extension quantifies the case reporting and fitting uncertainty to produce calibrated predictive intervals. We evaluated the performance of the DP model in the 2022-2023 outbreaks in Florida, where standardized clinical tests and reporting support accurate case determination. The DP framework demonstrates competitive predictive performance at substantially lower computational cost than more elaborate models, making it suitable for dengue early detection where dense surveillance or long historical records are unavailable."}
{"id": "2511.23205", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.23205", "abs": "https://arxiv.org/abs/2511.23205", "authors": ["Shiqin Tang", "Yining Dong", "S. Joe Qin"], "title": "A PLS-Integrated LASSO Method with Application in Index Tracking", "comment": null, "summary": "In traditional multivariate data analysis, dimension reduction and regression have been treated as distinct endeavors. Established techniques such as principal component regression (PCR) and partial least squares (PLS) regression traditionally compute latent components as intermediary steps -- although with different underlying criteria -- before proceeding with the regression analysis. In this paper, we introduce an innovative regression methodology named PLS-integrated Lasso (PLS-Lasso) that integrates the concept of dimension reduction directly into the regression process. We present two distinct formulations for PLS-Lasso, denoted as PLS-Lasso-v1 and PLS-Lasso-v2, along with clear and effective algorithms that ensure convergence to global optima. PLS-Lasso-v1 and PLS-Lasso-v2 are compared with Lasso on the task of financial index tracking and show promising results."}
