<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 10]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry](https://arxiv.org/abs/2512.21451)
*Bing Cheng,Howell Tong*

Main category: stat.ML

TL;DR: 该论文提出了一种解决非参数信息几何中Fisher-Rao度量不可逆性问题的正交分解框架，通过协变量Fisher信息矩阵(cFIM)建立可计算的几何表示，连接了信息几何与可解释AI的需求。


<details>
  <summary>Details</summary>
Motivation: 非参数信息几何面临"难处理性障碍"，因为无限维的Fisher-Rao度量是泛函，难以定义其逆。需要一种可计算框架来连接抽象信息几何与可解释AI的实际需求。

Method: 引入切空间的正交分解(T_fM=S⊕S^⊥)，其中S表示可观测协变量子空间。通过该分解推导出协变量Fisher信息矩阵(cFIM)，这是一个有限维且可计算的几何信息表示。

Result: 证明了迹定理：H_G(f)=Tr(G_f)，为G-熵建立了严格基础；建立了cFIM与KL散度二阶导数（曲率）的联系，提出协变量Cramér-Rao下界；将流形假设从启发式假设提升为cFIM中秩缺陷的可测试条件。

Conclusion: 该工作通过提供可处理的路径，弥合了抽象信息几何与可解释AI需求之间的差距，为揭示非参数模型的统计覆盖范围和效率提供了理论基础。

Abstract: Being infinite dimensional, non-parametric information geometry has long faced an "intractability barrier" due to the fact that the Fisher-Rao metric is now a functional incurring difficulties in defining its inverse. This paper introduces a novel framework to resolve the intractability with an Orthogonal Decomposition of the Tangent Space ($T_fM=S \oplus S^{\perp}$), where S represents an observable covariate subspace. Through the decomposition, we derive the Covariate Fisher Information Matrix (cFIM), denoted as $G_f$, which is a finite-dimensional and computable representative of information extractable from the manifold's geometry. Indeed, by proving the Trace Theorem: $H_G(f)=\text{Tr}(G_f)$, we establish a rigorous foundation for the G-entropy previously introduced by us, thereby identifying it not merely as a gradient-based regularizer, but also as a fundamental geometric invariant representing the total explainable statistical information captured by the probability distribution associated with the model. Furthermore, we establish a link between $G_f$ and the second-order derivative (i.e. the curvature) of the KL-divergence, leading to the notion of Covariate Cramér-Rao Lower Bound(CRLB). We demonstrate that $G_f$ is congruent to the Efficient Fisher Information Matrix, thereby providing fundamental limits of variance for semi-parametric estimators. Finally, we apply our geometric framework to the Manifold Hypothesis, lifting the latter from a heuristic assumption into a testable condition of rank-deficiency within the cFIM. By defining the Information Capture Ratio, we provide a rigorous method for estimating intrinsic dimensionality in high-dimensional data. In short, our work bridges the gap between abstract information geometry and the demand of explainable AI, by providing a tractable path for revealing the statistical coverage and the efficiency of non-parametric models.

</details>


### [2] [Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models](https://arxiv.org/abs/2512.21593)
*Takuro Kutsuna*

Main category: stat.ML

TL;DR: 提出Residual Prior Diffusion (RPD)两阶段框架：先用粗粒度先验模型捕捉数据分布的大尺度结构，再用扩散模型学习先验与目标分布之间的残差，解决标准扩散模型难以同时处理全局结构和局部细节的问题。


<details>
  <summary>Details</summary>
Motivation: 标准扩散模型使用单一网络和单一扩散计划，需要同时表示分布的全局结构和细粒度局部变化，当这两个尺度严重不匹配时变得困难。这在自然图像（粗粒度流形结构和细粒度纹理共存）和具有高度集中局部结构的低维分布中都存在问题。

Method: 提出Residual Prior Diffusion (RPD)两阶段框架：1) 粗粒度先验模型捕捉数据分布的大尺度结构；2) 扩散模型训练表示先验与目标数据分布之间的残差。将RPD构建为具有可处理证据下界的显式概率模型，优化简化为噪声预测或速度预测的熟悉目标。引入利用先验模型信息的辅助变量，理论上分析它们如何降低RPD中预测问题的难度。

Result: 在具有细粒度局部结构的合成数据集上，标准扩散模型无法捕捉局部细节，而RPD能准确捕捉细尺度细节同时保持分布的大尺度结构。在自然图像生成任务中，RPD的生成质量达到或超过代表性扩散基线，即使在少量推理步骤下也能保持强大性能。

Conclusion: RPD通过分离大尺度结构和细粒度细节的学习，解决了标准扩散模型在尺度不匹配时的困难，在合成数据和自然图像生成上都表现出优越性能，特别是在保持局部细节和高效推理方面具有优势。

Abstract: Diffusion models have become a central tool in deep generative modeling, but standard formulations rely on a single network and a single diffusion schedule to transform a simple prior, typically a standard normal distribution, into the target data distribution. As a result, the model must simultaneously represent the global structure of the distribution and its fine-scale local variations, which becomes difficult when these scales are strongly mismatched. This issue arises both in natural images, where coarse manifold-level structure and fine textures coexist, and in low-dimensional distributions with highly concentrated local structure. To address this issue, we propose Residual Prior Diffusion (RPD), a two-stage framework in which a coarse prior model first captures the large-scale structure of the data distribution, and a diffusion model is then trained to represent the residual between the prior and the target data distribution. We formulate RPD as an explicit probabilistic model with a tractable evidence lower bound, whose optimization reduces to the familiar objectives of noise prediction or velocity prediction. We further introduce auxiliary variables that leverage information from the prior model and theoretically analyze how they reduce the difficulty of the prediction problem in RPD. Experiments on synthetic datasets with fine-grained local structure show that standard diffusion models fail to capture local details, whereas RPD accurately captures fine-scale detail while preserving the large-scale structure of the distribution. On natural image generation tasks, RPD achieved generation quality that matched or exceeded that of representative diffusion-based baselines and it maintained strong performance even with a small number of inference steps.

</details>


### [3] [Tilt Matching for Scalable Sampling and Fine-Tuning](https://arxiv.org/abs/2512.21829)
*Peter Potaptchik,Cheuk-Kit Lee,Michael S. Albergo*

Main category: stat.ML

TL;DR: 提出Tilt Matching算法，通过随机插值技术从非归一化密度采样并微调生成模型，无需奖励梯度或反向传播，在Lennard-Jones势能采样和Stable Diffusion微调上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型采样和微调方法通常需要访问奖励梯度或进行复杂的反向传播，计算成本高且不便于扩展。需要一种简单、可扩展的算法来处理非归一化密度采样和生成模型微调问题。

Method: 提出Tilt Matching算法，基于随机插值技术，通过动力学方程将流匹配速度与目标分布（通过奖励倾斜）相关联，隐式解决随机最优控制问题。新速度继承了随机插值传输的正则性，同时具有比流匹配本身更低的方差目标函数最小值。

Result: 算法在Lennard-Jones势能采样上取得最先进结果，在Stable Diffusion微调上具有竞争力，且无需奖励乘子。算法高效、可扩展，可轻松应用于倾斜少步流映射模型。

Conclusion: Tilt Matching提供了一种简单、可扩展的方法，用于从非归一化密度采样和微调生成模型，无需奖励梯度或反向传播，在多个任务上表现出优异性能。

Abstract: We propose a simple, scalable algorithm for using stochastic interpolants to sample from unnormalized densities and for fine-tuning generative models. The approach, Tilt Matching, arises from a dynamical equation relating the flow matching velocity to one targeting the same distribution tilted by a reward, implicitly solving a stochastic optimal control problem. The new velocity inherits the regularity of stochastic interpolant transports while also being the minimizer of an objective with strictly lower variance than flow matching itself. The update to the velocity field can be interpreted as the sum of all joint cumulants of the stochastic interpolant and copies of the reward, and to first order is their covariance. The algorithms do not require any access to gradients of the reward or backpropagating through trajectories of the flow or diffusion. We empirically verify that the approach is efficient and highly scalable, providing state-of-the-art results on sampling under Lennard-Jones potentials and is competitive on fine-tuning Stable Diffusion, without requiring reward multipliers. It can also be straightforwardly applied to tilting few-step flow map models.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [4] [Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)](https://arxiv.org/abs/2512.21435)
*Stefano M. Iacus,Haodong Qi,Marcello Carammia,Thomas Juneau*

Main category: stat.AP

TL;DR: DynAttn是一个用于冲突相关死亡人数预测的可解释动态注意力框架，结合滚动窗口估计、特征门控、自注意力编码器和零膨胀负二项分布，在稀疏数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 冲突相关死亡人数的预测是政治科学和政策分析的核心挑战，因为暴力数据具有稀疏性、突发性和高度非平稳性。现有模型在稀疏网格级设置中往往不稳定或性能急剧下降。

Method: DynAttn框架结合了滚动窗口估计、共享弹性网络特征门控、紧凑的权重共享自注意力编码器和零膨胀负二项分布似然函数，能够生成校准的多时间范围预测和超出概率。

Result: 在VIEWS预测系统的全球国家和高分辨率PRIO网格级冲突数据上，DynAttn在1-12个月的预测范围内始终获得显著更高的预测准确性，特别是在稀疏网格级设置中表现突出，优于DynENet、LSTM、Prophet、PatchTST和官方VIEWS基线模型。

Conclusion: DynAttn不仅提供卓越的预测性能，还支持对区域冲突动态的结构化解释。分析显示短期冲突持续性和空间扩散是核心预测基础，而气候压力根据冲突区域的不同，可能作为条件放大器或主要驱动因素。

Abstract: Forecasting conflict-related fatalities remains a central challenge in political science and policy analysis due to the sparse, bursty, and highly non-stationary nature of violence data. We introduce DynAttn, an interpretable dynamic-attention forecasting framework for high-dimensional spatio-temporal count processes. DynAttn combines rolling-window estimation, shared elastic-net feature gating, a compact weight-tied self-attention encoder, and a zero-inflated negative binomial (ZINB) likelihood. This architecture produces calibrated multi-horizon forecasts of expected casualties and exceedance probabilities, while retaining transparent diagnostics through feature gates, ablation analysis, and elasticity measures.
  We evaluate DynAttn using global country-level and high-resolution PRIO-grid-level conflict data from the VIEWS forecasting system, benchmarking it against established statistical and machine-learning approaches, including DynENet, LSTM, Prophet, PatchTST, and the official VIEWS baseline. Across forecast horizons from one to twelve months, DynAttn consistently achieves substantially higher predictive accuracy, with particularly large gains in sparse grid-level settings where competing models often become unstable or degrade sharply.
  Beyond predictive performance, DynAttn enables structured interpretation of regional conflict dynamics. In our application, cross-regional analyses show that short-run conflict persistence and spatial diffusion form the core predictive backbone, while climate stress acts either as a conditional amplifier or a primary driver depending on the conflict theater.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [5] [Sensitivity Analysis of the Consistency Assumption](https://arxiv.org/abs/2512.21379)
*Brian Knaeble,Qinyun Lin,Erich Kummerfeld,Kenneth A. Frank*

Main category: stat.ME

TL;DR: 提出针对隐藏治疗版本的一致性假设违反问题的敏感性分析方法，区别于传统未测量混杂变量分析


<details>
  <summary>Details</summary>
Motivation: 一致性假设要求没有隐藏的治疗版本，但在实际应用中可能存在隐藏的治疗版本（如手术中不同外科医生的技能差异），这会影响因果推断的可靠性

Method: 引入新的数学符号体系，开发专注于隐藏治疗版本混杂的敏感性分析方法，区别于传统的未测量协变量混杂分析方法

Result: 建立了新的敏感性分析框架，能够评估结论对一致性假设违反的敏感性，并通过示例应用展示了方法的实用性

Conclusion: 针对隐藏治疗版本的敏感性分析是因果推断中的重要补充，新方法为解决一致性假设违反问题提供了有效工具

Abstract: Sensitivity analysis informs causal inference by assessing the sensitivity of conclusions to departures from assumptions. The consistency assumption states that there are no hidden versions of treatment and that the outcome arising naturally equals the outcome arising from intervention. When reasoning about the possibility of consistency violations, it can be helpful to distinguish between covariates and versions of treatment. In the context of surgery, for example, genomic variables are covariates and the skill of a particular surgeon is a version of treatment. There may be hidden versions of treatment, and this paper addresses that concern with a new kind of sensitivity analysis. Whereas many methods for sensitivity analysis are focused on confounding by unmeasured covariates, the methodology of this paper is focused on confounding by hidden versions of treatment. In this paper, new mathematical notation is introduced to support the novel method, and example applications are described.

</details>


### [6] [Standardized Descriptive Index for Measuring Deviation and Uncertainty in Psychometric Indicators](https://arxiv.org/abs/2512.21399)
*Mark Dominique Dalipe Muñoz*

Main category: stat.ME

TL;DR: 本文提出将Cohen's d重新用作标准化项目偏差指数，用于小样本量下的量表开发，通过整合均值和标准差来直接测量项目质量。


<details>
  <summary>Details</summary>
Motivation: 当前心理测量实践在报告项目级统计时，通常分别报告原始描述性统计，而不是将均值和标准差整合到单一诊断工具中直接测量项目质量。特别是在小样本量的试点测试中，需要客观、标准且可行的诊断工具。

Method: 利用Cohen's d的分析特性，将其重新用作量表开发中的标准化项目偏差指数。该指数测量项目原始偏差相对于其量表中点的程度，同时考虑其自身的不确定性。探索了有界性、尺度不变性和偏差等分析特性。

Result: 开发了一个标准化项目偏差指数，能够整合均值和标准差信息，为小样本量下的项目质量评估提供单一诊断工具。该指数的分析特性有助于理解其数值行为。

Conclusion: 标准化项目偏差指数为量表开发提供了实用的诊断工具，特别适用于小样本情况。未来研究可以建立经验阈值，用于表征形成性指标间的冗余性和反思性指标间的一致性。

Abstract: The use of descriptive statistics in pilot testing procedures requires objective, standard diagnostic tools that are feasible for small sample sizes. While current psychometric practices report item-level statistics, they often report these raw descriptives separately rather than consolidating both mean and standard deviation into a single diagnostic tool to directly measure item quality. By leveraging the analytical properties of Cohen's d, this article repurposes its use in scale development as a standardized item deviation index. This measures the extent of an item's raw deviation relative to its scale midpoint while accounting for its own uncertainty. Analytical properties such as boundedness, scale invariance, and bias are explored to further understand how the index values behave, which will aid future efforts to establish empirical thresholds that characterize redundancy among formative indicators and consistency among reflective indicators.

</details>


### [7] [Adaptive Test for High Dimensional Quantile Regression](https://arxiv.org/abs/2512.21541)
*Ping Zhao,Zhenyu Liu,Dan Zhuang*

Main category: stat.ME

TL;DR: 提出一种自适应检验方法，用于高维分位数回归系数的假设检验，该方法通过Cauchy组合检验结合max-type和sum-type统计量，在稀疏和稠密备择假设下均保持良好功效。


<details>
  <summary>Details</summary>
Motivation: 高维分位数回归系数检验很重要，因为尾部百分位数比均值更能揭示实际问题。然而实践中备择假设的稀疏模式通常是未知的，这构成了主要挑战。

Method: 首先建立了Tang等人提出的max-type检验统计量与Chen等人提出的sum-type检验统计量的渐近独立性。基于此结果，提出Cauchy组合检验，有效整合两种统计量的优势，在广泛的稀疏度水平下实现稳健性能。

Result: 模拟研究和实际数据应用表明，所提出的方法在尺寸控制和功效方面均优于现有方法。

Conclusion: 提出的自适应检验方法能够有效处理高维分位数回归系数检验问题，在未知稀疏模式的情况下保持稳健性能。

Abstract: Testing high-dimensional quantile regression coefficients is crucial, as tail quantiles often reveal more than the mean in many practical applications. Nevertheless, the sparsity pattern of the alternative hypothesis is typically unknown in practice, posing a major challenge. To address this, we propose an adaptive test that remains powerful across both sparse and dense alternatives.We first establish the asymptotic independence between the max-type test statistic proposed by \citet{tang2022conditional} and the sum-type test statistic introduced by \citet{chen2024hypothesis}. Building on this result, we propose a Cauchy combination test that effectively integrates the strengths of both statistics and achieves robust performance across a wide range of sparsity levels. Simulation studies and real data applications demonstrate that our proposed procedure outperforms existing methods in terms of both size control and power.

</details>


### [8] [Cross-Semantic Transfer Learning for High-Dimensional Linear Regression](https://arxiv.org/abs/2512.21689)
*Jiancheng Jiang,Xuejun Jiang,Hongxia Jin*

Main category: stat.ME

TL;DR: 提出了跨语义迁移学习框架CSTL，通过加权融合惩罚捕获目标与源域系数间的潜在关系，在特征不对齐但语义相似的情况下实现有效迁移。


<details>
  <summary>Details</summary>
Motivation: 现有高维线性回归迁移学习方法假设特征对齐，限制了在特征语义匹配但具体特征不同的实际场景中的应用。许多现实场景中，目标域和源域的不同特征可能扮演相似的预测角色，存在跨语义相似性。

Method: 提出跨语义迁移学习框架CSTL，通过加权融合惩罚比较每个目标系数与所有源系数，权重来自SCAD惩罚的导数，近似理想加权方案以保留可迁移信号并过滤源特定噪声。使用ADMM算法实现计算效率。

Result: 理论上证明在温和条件下CSTL以压倒性概率达到oracle估计器。仿真和实际数据应用表明，CSTL在跨语义和部分信号相似性设置下均优于现有方法。

Conclusion: CSTL框架有效解决了特征不对齐但语义相似的迁移学习问题，通过创新的加权融合机制实现了跨语义的有效知识迁移，在理论和实证上都表现出优越性能。

Abstract: Current transfer learning methods for high-dimensional linear regression assume feature alignment across domains, restricting their applicability to semantically matched features. In many real-world scenarios, however, distinct features in the target and source domains can play similar predictive roles, creating a form of cross-semantic similarity. To leverage this broader transferability, we propose the Cross-Semantic Transfer Learning (CSTL) framework. It captures potential relationships by comparing each target coefficient with all source coefficients through a weighted fusion penalty. The weights are derived from the derivative of the SCAD penalty, effectively approximating an ideal weighting scheme that preserves transferable signals while filtering out source-specific noise. For computational efficiency, we implement CSTL using the Alternating Direction Method of Multipliers (ADMM). Theoretically, we establish that under mild conditions, CSTL achieves the oracle estimator with overwhelming probability. Empirical results from simulations and a real-data application confirm that CSTL outperforms existing methods in both cross-semantic and partial signal similarity settings.

</details>


### [9] [Exact inference via quasi-conjugacy in two-parameter Poisson-Dirichlet hidden Markov models](https://arxiv.org/abs/2512.22098)
*Marco Dalla Pria,Matteo Ruggiero,Dario Spanò*

Main category: stat.ME

TL;DR: 提出非参数模型用于从离散时间未标记分区数据推断时间演化的未观测概率分布，使用两参数泊松-狄利克雷扩散作为潜在过程，开发了避免标签枚举和直接模拟的推理框架


<details>
  <summary>Details</summary>
Motivation: 解决社会网络和遗传数据中只观测到聚合聚类摘要的问题，传统方法面临似然函数难处理和标签枚举的挑战

Method: 利用扩散与分区上的纯死亡过程之间的对偶性，结合编码新数据影响的凝聚算子，开发闭式递归更新进行前向和后向推理

Result: 能够计算任意时间潜在状态的确切后验分布和未来或插值分区的预测分布，相比粒子滤波实现更高精度、更低方差和显著计算增益

Conclusion: 该方法支持在线和离线推理与预测，提供完整不确定性量化，绕过了MCMC和序列蒙特卡洛方法，在合成实验和社交网络应用中成功恢复时变杂合性的可解释模式

Abstract: We introduce a nonparametric model for time-evolving, unobserved probability distributions from discrete-time data consisting of unlabelled partitions. The latent process is a two-parameter Poisson-Dirichlet diffusion, and observations arise via exchangeable sampling. Applications include social and genetic data where only aggregate clustering summaries are observed. To address the intractable likelihood, we develop a tractable inferential framework that avoids label enumeration and direct simulation of the latent state. We exploit a duality between the diffusion and a pure-death process on partitions, together with coagulation operators that encode the effect of new data. These yield closed-form, recursive updates for forward and backward inference. We compute exact posterior distributions of the latent state at arbitrary times and predictive distributions of future or interpolated partitions. This enables online and offline inference and forecasting with full uncertainty quantification, bypassing MCMC and sequential Monte Carlo. Compared to particle filtering, our method achieves higher accuracy, lower variance, and substantial computational gains. We illustrate the methodology with synthetic experiments and a social network application, recovering interpretable patterns in time-varying heterozygosity.

</details>


### [10] [Surrogate-Powered Inference: Regularization and Adaptivity](https://arxiv.org/abs/2512.21826)
*Jianmin Chen,Huiyuan Wang,Thomas Lumley,Xiaowu Dai,Yong Chen*

Main category: stat.ME

TL;DR: SPI工具箱是一个利用高质量标签和替代标签进行可靠统计推断的统一框架，包含三个渐进增强版本，能显著降低估计误差并提高风险因素识别能力。


<details>
  <summary>Details</summary>
Motivation: 高质量标记数据对于可靠的统计推断至关重要，但验证成本往往限制了其可用性。虽然替代标签提供了成本效益高的替代方案，但其噪声可能引入不可忽略的偏差。

Method: 提出了替代驱动的推断（SPI）工具箱，包含三个版本：Base-SPI通过增强整合验证标签和替代标签以提高估计效率；SPI+加入正则化回归安全处理多个替代标签，防止误差累积导致的性能下降；SPI++通过自适应多波标记程序，在有限验证预算下优化效率，优先标记信息丰富的样本。

Result: 与传统方法相比，SPI显著降低了估计误差，提高了风险因素识别的统计功效，证明了其在提高可重复性方面的价值。

Conclusion: SPI框架通过有效结合高质量标签的准确性和替代标签的丰富性，为有限验证预算下的可靠统计推断提供了实用解决方案，理论保证和模拟研究进一步验证了其性能。

Abstract: High-quality labeled data are essential for reliable statistical inference, but are often limited by validation costs. While surrogate labels provide cost-effective alternatives, their noise can introduce non-negligible bias. To address this challenge, we propose the surrogate-powered inference (SPI) toolbox, a unified framework that leverages both the validity of high-quality labels and the abundance of surrogates to enable reliable statistical inference. SPI comprises three progressively enhanced versions. Base-SPI integrates validated labels and surrogates through augmentation to improve estimation efficiency. SPI+ incorporates regularized regression to safely handle multiple surrogates, preventing performance degradation due to error accumulation. SPI++ further optimizes efficiency under limited validation budgets through an adaptive, multiwave labeling procedure that prioritizes informative subjects for labeling. Compared to traditional methods, SPI substantially reduces the estimation error and increases the power in risk factor identification. These results demonstrate the value of SPI in improving the reproducibility. Theoretical guarantees and extensive simulation studies further illustrate the properties of our approach.

</details>


### [11] [Targeted learning via probabilistic subpopulation matching](https://arxiv.org/abs/2512.21840)
*Xiaokang Liu,Jie Hu,Naimin Jing,Yang Ning,Cheng Yong Tang,Runze Li,Yong Chen*

Main category: stat.ME

TL;DR: 提出一种基于亚群匹配的靶向学习框架，通过亚群层面而非研究层面的匹配，有效利用所有源研究样本，避免现有方法因研究层面匹配而丢弃大量样本的问题。


<details>
  <summary>Details</summary>
Motivation: 在生物医学研究中，利用多个相似源研究的信息可以提高目标研究的预测准确性。然而，现实世界数据中不同研究（如临床站点）的群体存在异质性，导致信息借用的挑战。现有方法基于研究层面匹配，会丢弃与目标研究显著不同的源研究中的所有样本，造成信息损失。

Method: 提出亚群匹配的靶向学习框架：1）首先在所有研究中联合拟合有限混合模型，提供个体层面的概率亚群信息；2）然后针对每个识别的亚群，在亚群内部将信息从源研究转移到目标研究。

Result: 建立了估计量的非渐近性质，并通过模拟研究证明了该方法能够提高目标研究的预测能力。

Conclusion: 亚群匹配框架能够有效分解研究内和研究间的异质性，充分利用所有源研究样本，相比现有研究层面匹配方法，在目标研究预测方面表现更优。

Abstract: In biomedical research, to obtain more accurate prediction results from a target study, leveraging information from multiple similar source studies is proved to be useful. However, in many biomedical applications based on real-world data, populations under consideration in different studies, e.g., clinical sites, can be heterogeneous, leading to challenges in properly borrowing information towards the target study. The state of art methods are typically based on study-level matching to identify source studies that are similar to the target study, whilst samples from source studies that significantly differ from the target study will all be dropped at the study level, which can lead to substantial loss of information. We consider a general situation where all studies are sampled from a super-population composed of distinct subpopulations, and propose a novel framework of targeted learning via subpopulation matching. In contrast to the existing study-level matching methods, measuring similarities between subpopulations can effectively decompose both within- and between-study heterogeneity, allowing incorporation of information from all source studies without dropping any samples as in the existing methods. We devise the proposed framework as a two-step procedure, where a finite mixture model is first fitted jointly across all studies to provide subject-wise probabilistic subpopulation information, followed by a step of within-subpopulation information transferring from source studies to the target study for each identified subpopulation. We establish the non-asymptotic properties of our estimator and demonstrate the ability of our method to improve prediction at the target study via simulation studies.

</details>


### [12] [A Communication-Efficient Distributed Algorithm for Learning with Heterogeneous and Structurally Incomplete Multi-Site Data](https://arxiv.org/abs/2512.21879)
*Xiaokang Liu,Yuchen Yang,Yifei Sun,Jiang Bian,Yanyuan Ma,Raymond J. Carroll,Yong Chen*

Main category: stat.ME

TL;DR: 提出一个分布式推断框架，用于处理多中心生物医学研究中存在分布异质性和数据结构异质性时的数据整合问题。


<details>
  <summary>Details</summary>
Motivation: 多中心生物医学研究需要整合分散站点的数据以获得更稳健和可推广的发现，但由于患者隐私和监管限制，共享个体层面数据困难。现有分布式算法通常假设数据独立同分布，这在实践中常被违反，且各站点的协变量集可能因数据收集协议不同而存在差异。

Method: 使用密度倾斜广义矩方法建模异质性和结构缺失数据，开发了一种通信高效且能感知异质性的通用聚合数据分布式算法。

Result: 建立了估计量的渐近性质，并通过模拟研究验证了方法的有效性。

Conclusion: 提出的分布式推断框架能够有效处理多中心生物医学研究中存在的分布异质性和数据结构异质性，为实际应用提供了可行的解决方案。

Abstract: In multicenter biomedical research, integrating data from multiple decentralized sites provides more robust and generalizable findings due to its larger sample size and the ability to account for the between-site heterogeneity. However, sharing individual-level data across sites is often difficult due to patient privacy concerns and regulatory restrictions. To overcome this challenge, many distributed algorithms, that fit a global model by only communicating aggregated information across sites, have been proposed. A major challenge in applying existing distributed algorithms to real-world data is that their validity often relies on the assumption that data across sites are independently and identically distributed, which is frequently violated in practice. In biomedical applications, data distributions across clinical sites can be heterogeneous. Additionally, the set of covariates available at each site may vary due to different data collection protocols. We propose a distributed inference framework for data integration in the presence of both distribution heterogeneity and data structural heterogeneity. By modeling heterogeneous and structurally missing data using density-tilted generalized method of moments, we developed a general aggregated data-based distributed algorithm that is communication-efficient and heterogeneity-aware. We establish the asymptotic properties of our estimator and demonstrate the validity of our method via simulation studies.

</details>


### [13] [Modeling high dimensional point clouds with the spherical cluster model](https://arxiv.org/abs/2512.21960)
*Frédéric Cazals,Antoine Commaret,Louis Goldenberg*

Main category: stat.ME

TL;DR: 本文提出球形聚类模型(SC)，通过球体近似点集，引入参数η控制半径，建立严格凸但不光滑的组合优化问题，并给出基于Clarke梯度的精确求解算法。


<details>
  <summary>Details</summary>
Motivation: 传统KMeans使用质心作为聚类中心，但质心对异常值敏感。球形聚类模型提供一种参数化的几何模型，通过球体近似点集，既能处理高维数据，又能作为参数化的高维中位数，为多元数据分析提供新工具。

Method: 1. 定义球形聚类模型：用球体S(c,r)近似点集P，半径r取中心c到数据点距离标准差的η倍(η∈(0,1))，成本函数为球外点的幂距离和；2. 证明该优化问题是严格凸但不光滑的组合优化问题；3. 提出基于Clarke梯度的精确求解算法，在超球体排列定义的层状细胞复形上求解。

Result: 1. 精确算法在中小维度数据集和小η值时比BFGS启发式算法快几个数量级；2. 在高维数据集(d>100)中，无论η值如何，精确算法都显著更快；3. SC模型的中心表现出参数化高维中位数的特性。

Conclusion: 球形聚类模型为高维多变量数据分析提供了有效的几何建模工具，其精确求解算法在实际应用中具有显著优势。该模型可直接用于高维数据分析，其混合模型设计将在后续论文中报告。

Abstract: A parametric cluster model is a statistical model providing geometric insights onto the points defining a cluster. The {\em spherical cluster model} (SC) approximates a finite point set $P\subset \mathbb{R}^d$ by a sphere $S(c,r)$ as follows. Taking $r$ as a fraction $η\in(0,1)$ (hyper-parameter) of the std deviation of distances between the center $c$ and the data points, the cost of the SC model is the sum over all data points lying outside the sphere $S$ of their power distance with respect to $S$. The center $c$ of the SC model is the point minimizing this cost. Note that $η=0$ yields the celebrated center of mass used in KMeans clustering. We make three contributions.
  First, we show fitting a spherical cluster yields a strictly convex but not smooth combinatorial optimization problem. Second, we present an exact solver using the Clarke gradient on a suitable stratified cell complex defined from an arrangement of hyper-spheres. Finally, we present experiments on a variety of datasets ranging in dimension from $d=9$ to $d=10,000$, with two main observations. First, the exact algorithm is orders of magnitude faster than BFGS based heuristics for datasets of small/intermediate dimension and small values of $η$, and for high dimensional datasets (say $d>100$) whatever the value of $η$. Second, the center of the SC model behave as a parameterized high-dimensional median.
  The SC model is of direct interest for high dimensional multivariate data analysis, and the application to the design of mixtures of SC will be reported in a companion paper.

</details>


### [14] [Prediction intervals for quantile autoregression](https://arxiv.org/abs/2512.22018)
*Silvia Novo,César Sánchez-Sellero*

Main category: stat.ME

TL;DR: 提出基于分位数的新预测区间构建方法，适用于经典自回归模型和现代分位数自回归模型，结合分位数估计和乘数bootstrap，在模拟和实证中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有预测区间构建方法在覆盖率和计算效率方面存在不足，需要开发更有效的分位数技术来改进预测区间估计。

Method: 结合分位数估计与乘数bootstrap方案，近似系数估计的抽样变异性，并对未来观测进行bootstrap复制，考虑基于百分位数和预测根两种构建方式。

Result: 理论结果验证了方法的有效性，模拟实验显示在有限样本下具有改进的覆盖特性和计算效率，在失业率和汽油价格数据中实证应用有效。

Conclusion: 提出的分位数预测区间方法在理论和实证上都表现优异，为时间序列预测提供了更可靠的区间估计工具。

Abstract: This paper introduces new methods for constructing prediction intervals using quantile-based techniques. The procedures are developed for both classical (homoscedastic) autoregressive models and modern quantile autoregressive models. They combine quantile estimation with multiplier bootstrap schemes to approximate the sampling variability of coefficient estimates, together with bootstrap replications of future observations. We consider both percentile-based and predictive-root-based constructions. Theoretical results establish the validity and pertinence of the proposed methods. Simulation experiments evaluate their finite-sample performance and show that the proposed methods yield improved coverage properties and computational efficiency relative to existing approaches in the literature. The empirical usefulness of the methods is illustrated through applications to U.S. unemployment rate data and retail gasoline prices.

</details>
