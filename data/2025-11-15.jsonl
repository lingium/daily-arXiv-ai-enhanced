{"id": "2511.09759", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.09759", "abs": "https://arxiv.org/abs/2511.09759", "authors": ["Borna Bateni", "Yubai Yuan", "Qi Xu", "Annie Qu"], "title": "Distributional Treatment Effect Estimation across Heterogeneous Sites via Optimal Transport", "comment": null, "summary": "We propose a novel framework for synthesizing counterfactual treatment group data in a target site by integrating full treatment and control group data from a source site with control group data from the target. Departing from conventional average treatment effect estimation, our approach adopts a distributional causal inference perspective by modeling treatment and control as distinct probability measures on the source and target sites. We formalize the cross-site heterogeneity (effect modification) as a push-forward transformation that maps the joint feature-outcome distribution from the source to the target site. This transformation is learned by aligning the control group distributions between sites using an Optimal Transport-based procedure, and subsequently applied to the source treatment group to generate the synthetic target treatment distribution. Under general regularity conditions, we establish theoretical guarantees for the consistency and asymptotic convergence of the synthetic treatment group data to the true target distribution. Simulation studies across multiple data-generating scenarios and a real-world application to patient-derived xenograft data demonstrate that our framework robustly recovers the full distributional properties of treatment effects."}
{"id": "2511.10048", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10048", "abs": "https://arxiv.org/abs/2511.10048", "authors": ["Yanjiao Yang", "Daniel Suen", "Yen-Chi Chen"], "title": "Masking criteria for selecting an imputation model", "comment": "55 pages, 4 figures, 4 tables", "summary": "The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models."}
{"id": "2511.09577", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09577", "abs": "https://arxiv.org/abs/2511.09577", "authors": ["Xuan Son Nguyen", "Aymeric Histace", "Nistor Grozavu"], "title": "Siegel Neural Networks", "comment": null, "summary": "Riemannian symmetric spaces (RSS) such as hyperbolic spaces and symmetric positive definite (SPD) manifolds have become popular spaces for representation learning. In this paper, we propose a novel approach for building discriminative neural networks on Siegel spaces, a family of RSS that is largely unexplored in machine learning tasks. For classification applications, one focus of recent works is the construction of multiclass logistic regression (MLR) and fully-connected (FC) layers for hyperbolic and SPD neural networks. Here we show how to build such layers for Siegel neural networks. Our approach relies on the quotient structure of those spaces and the notation of vector-valued distance on RSS. We demonstrate the relevance of our approach on two applications, i.e., radar clutter classification and node classification. Our results successfully demonstrate state-of-the-art performance across all datasets."}
{"id": "2511.10206", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.10206", "abs": "https://arxiv.org/abs/2511.10206", "authors": ["Eugene Seong"], "title": "Heuristic Solutions for the Best Secretary Problem", "comment": "13 pages", "summary": "This paper introduces a heuristic framework for the Best Secretary Problem, where one item must be selected using rank information only. We develop five data-responsive rules extending classical fixed-cutoff methods: an expected-record threshold, an adaptive deviation correction, a probabilistic early-accept rule, a two-phase relaxation, and a local dynamic programming approximation. These rules adjust thresholds sequentially as information accumulates. Simulations across diverse sample sizes, distributions, and autocorrelated settings show that the heuristics match or exceed traditional optimal rules in stability and efficiency. The expected-record rule remains strong despite its simplicity, the adaptive correction performs well under asymmetry, and the adaptive and probabilistic rules reduce average stopping times. An ensemble combining multiple rules yields the most stable performance. Overall, a few intuitive parameters achieve near-optimal results, demonstrating that data-responsive heuristics can effectively extend rank-based optimal stopping to dynamic decision environments."}
{"id": "2511.09823", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09823", "abs": "https://arxiv.org/abs/2511.09823", "authors": ["Woojung Bae", "Dongrak Choi", "Jun Yan", "Sangwook Kang"], "title": "Diagnostics for Semiparametric Accelerated Failure Time Models with R Package afttest", "comment": null, "summary": "The semiparametric accelerated failure time (AFT) model is a useful alternative to the widely used Cox proportional hazard model, which directly links the logarithm of the failure time to the covariates, yielding more interpretable regression coefficients. However, diagnostic procedures for the semiparametric AFT model have received relatively little attention. This paper introduces afttest, an R package that implements recently developed diagnostic tools for the semiparametric AFT model. The package supports diagnostic procedures for models fitted with either rank-based or least-squares methods. It provides functions to assess model assumptions, including the overall adequacy, the link function, and functional form of each covariate. The test statistics are of Kolmogorov-type suprema of transformed aggregated martingale residual processes. The p-values are obtained by approximating the null distribution with an efficient multiplier bootstrap procedure. Additionally, the package offers graphical tools to compare the observed stochastic processes with a number of approximated realizations. Applications of the package to the well-known Mayo clinic primary biliary cirrhosis study are presented."}
{"id": "2511.09686", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09686", "abs": "https://arxiv.org/abs/2511.09686", "authors": ["Isaac H. Goldstein", "Julia A. Palacios"], "title": "Coalescent Inference for Epidemics with Latent Periods", "comment": "26 pages, 10 figures, 3 tables", "summary": "Coalescent models are used to study the transmission dynamics of rapidly evolving pathogens from molecular sequence data obtained from infected individuals. However coalescent parameters, such as effective population size, offer limited interpretability for transmission dynamics. In this work, we derive a coalescent model for exposed-infected population dynamics that allows us to infer the number of infected individuals and the effective reproduction number over time from the sample genealogy. The model can be interpreted as a two-deme model in which coalescence is restricted to individuals from different demes (exposed and infected). We propose a new data-augmentation framework with Phase-type distribution for Bayesian inference of epidemiological parameters. We study the performance of our approach on simulations and apply it to re-analyze the 2014 Ebola outbreak in Liberia."}
{"id": "2511.09722", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09722", "abs": "https://arxiv.org/abs/2511.09722", "authors": ["Sujay Nair", "Evan Coleman", "Sherrie Wang", "Elsa Olivetti"], "title": "Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling", "comment": "7 pages, 6 figures, includes 23 pages of Supplementary Materials for paper accepted to AAAI2026", "summary": "Minerals play a critical role in the advanced energy technologies necessary for decarbonization, but characterizing mineral deposits hidden underground remains costly and challenging. Inspired by recent progress in generative modeling, we develop a learning method which infers the locations of minerals by masking and infilling geospatial maps of resource availability. We demonstrate this technique using mineral data for the conterminous United States, and train performant models, with the best achieving Dice coefficients of $0.31 \\pm 0.01$ and recalls of $0.22 \\pm 0.02$ on test data at 1$\\times$1 mi$^2$ spatial resolution. One major advantage of our approach is that it can easily incorporate auxiliary data sources for prediction which may be more abundant than mineral data. We highlight the capabilities of our model by adding input layers derived from geophysical sources, along with a nation-wide ground survey of soils originally intended for agronomic purposes. We find that employing such auxiliary features can improve inference performance, while also enabling model evaluation in regions with no recorded minerals."}
{"id": "2511.09722", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09722", "abs": "https://arxiv.org/abs/2511.09722", "authors": ["Sujay Nair", "Evan Coleman", "Sherrie Wang", "Elsa Olivetti"], "title": "Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling", "comment": "7 pages, 6 figures, includes 23 pages of Supplementary Materials for paper accepted to AAAI2026", "summary": "Minerals play a critical role in the advanced energy technologies necessary for decarbonization, but characterizing mineral deposits hidden underground remains costly and challenging. Inspired by recent progress in generative modeling, we develop a learning method which infers the locations of minerals by masking and infilling geospatial maps of resource availability. We demonstrate this technique using mineral data for the conterminous United States, and train performant models, with the best achieving Dice coefficients of $0.31 \\pm 0.01$ and recalls of $0.22 \\pm 0.02$ on test data at 1$\\times$1 mi$^2$ spatial resolution. One major advantage of our approach is that it can easily incorporate auxiliary data sources for prediction which may be more abundant than mineral data. We highlight the capabilities of our model by adding input layers derived from geophysical sources, along with a nation-wide ground survey of soils originally intended for agronomic purposes. We find that employing such auxiliary features can improve inference performance, while also enabling model evaluation in regions with no recorded minerals."}
{"id": "2511.10363", "categories": ["stat.CO", "cs.DC", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.10363", "abs": "https://arxiv.org/abs/2511.10363", "authors": ["Simo Särkkä", "Ángel F. García-Fernández"], "title": "On The Performance of Prefix-Sum Parallel Kalman Filters and Smoothers on GPUs", "comment": null, "summary": "This paper presents an experimental evaluation of parallel-in-time Kalman filters and smoothers using graphics processing units (GPUs). In particular, the paper evaluates different all-prefix-sum algorithms, that is, parallel scan algorithms for temporal parallelization of Kalman filters and smoothers in two ways: by calculating the required number of operations via simulation, and by measuring the actual run time of the algorithms on real GPU hardware. In addition, a novel parallel-in-time two-filter smoother is proposed and experimentally evaluated. Julia code for Metal and CUDA implementations of all the algorithms is made publicly available."}
{"id": "2511.09698", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09698", "abs": "https://arxiv.org/abs/2511.09698", "authors": ["Samuel J. Eschker", "Antik Chakraborty", "Melanie Gall", "Peter Jevtic", "Jianxi Su"], "title": "State Space Modeling of Mortgage Default Rates under Natural Hazard Shocks", "comment": "To appear in North American Actuarial Journal", "summary": "Mortgage default rates, on the one hand, serve as a measure of economic health to support decision-making by insurance companies, and on the other hand, is a key risk factor in the asset-liability management (ALM) practice, as mortgage related assets constitute a significant proportion of insurers' investment portfolios. This paper studies the relationship between economic losses due to natural hazards and mortgage default rates. The topic is greatly relevant to the insurance industry, as excessive insurance losses from natural hazards can lead to a surge in mortgage defaults, creating compounded challenges for insurers. To this end, we apply a state-space modeling approach to decouple the effect of natural hazard losses on mortgage default rates after controlling for other economic determinants through the inclusion of latent variables. Moreover, we consider a sliced variant of the classical SSM to capture the subtle relationship that only emerges when natural hazard losses are sufficiently high. Our model verifies the significance of this relationship and provides insights into how natural hazard losses manifest as increased mortgage default rates."}
{"id": "2511.09801", "categories": ["stat.ML", "cs.LG", "math.FA", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.09801", "abs": "https://arxiv.org/abs/2511.09801", "authors": ["Salvish Goomanee", "Andi Han", "Pratik Jawanpuria", "Bamdev Mishra"], "title": "Generalized infinite dimensional Alpha-Procrustes based geometries", "comment": null, "summary": "This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis."}
{"id": "2511.09767", "categories": ["stat.ME", "cs.LG", "econ.GN", "eess.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09767", "abs": "https://arxiv.org/abs/2511.09767", "authors": ["Alessandro V. M. Oliveira"], "title": "Modelos Empiricos de Pos-Dupla Selecao por LASSO: Discussoes para Estudos do Transporte Aereo", "comment": "Article in Portuguese", "summary": "This paper presents and discusses forms of estimation by regularized regression and model selection using the LASSO method - Least Absolute Shrinkage and Selection Operator. LASSO is recognized as one of the main supervised learning methods applied to high-dimensional econometrics, allowing work with large volumes of data and multiple correlated controls. Conceptual issues related to the consequences of high dimensionality in modern econometrics and the principle of sparsity, which underpins regularization procedures, are addressed. The study examines the main post-double selection and post-regularization models, including variations applied to instrumental variable models. A brief description of the lassopack routine package, its syntaxes, and examples of HD, HDS (High-Dimension Sparse), and IV-HDS models, with combinations involving fixed effects estimators, is also presented. Finally, the potential application of the approach in research focused on air transport is discussed, with emphasis on an empirical study on the operational efficiency of airlines and aircraft fuel consumption."}
{"id": "2511.10048", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10048", "abs": "https://arxiv.org/abs/2511.10048", "authors": ["Yanjiao Yang", "Daniel Suen", "Yen-Chi Chen"], "title": "Masking criteria for selecting an imputation model", "comment": "55 pages, 4 figures, 4 tables", "summary": "The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models."}
{"id": "2511.09759", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.09759", "abs": "https://arxiv.org/abs/2511.09759", "authors": ["Borna Bateni", "Yubai Yuan", "Qi Xu", "Annie Qu"], "title": "Distributional Treatment Effect Estimation across Heterogeneous Sites via Optimal Transport", "comment": null, "summary": "We propose a novel framework for synthesizing counterfactual treatment group data in a target site by integrating full treatment and control group data from a source site with control group data from the target. Departing from conventional average treatment effect estimation, our approach adopts a distributional causal inference perspective by modeling treatment and control as distinct probability measures on the source and target sites. We formalize the cross-site heterogeneity (effect modification) as a push-forward transformation that maps the joint feature-outcome distribution from the source to the target site. This transformation is learned by aligning the control group distributions between sites using an Optimal Transport-based procedure, and subsequently applied to the source treatment group to generate the synthetic target treatment distribution. Under general regularity conditions, we establish theoretical guarantees for the consistency and asymptotic convergence of the synthetic treatment group data to the true target distribution. Simulation studies across multiple data-generating scenarios and a real-world application to patient-derived xenograft data demonstrate that our framework robustly recovers the full distributional properties of treatment effects."}
{"id": "2511.09897", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09897", "abs": "https://arxiv.org/abs/2511.09897", "authors": ["Shunan Sheng", "Bohan Wu", "Bennett Zhu", "Sinho Chewi", "Aram-Alexandre Pooladian"], "title": "Theory and computation for structured variational inference", "comment": "78 pages, 2 figures", "summary": "Structured variational inference constitutes a core methodology in modern statistical applications. Unlike mean-field variational inference, the approximate posterior is assumed to have interdependent structure. We consider the natural setting of star-structured variational inference, where a root variable impacts all the other ones. We prove the first results for existence, uniqueness, and self-consistency of the variational approximation. In turn, we derive quantitative approximation error bounds for the variational approximation to the posterior, extending prior work from the mean-field setting to the star-structured setting. We also develop a gradient-based algorithm with provable guarantees for computing the variational approximation using ideas from optimal transport theory. We explore the implications of our results for Gaussian measures and hierarchical Bayesian models, including generalized linear models with location family priors and spike-and-slab priors with one-dimensional debiasing. As a by-product of our analysis, we develop new stability results for star-separable transport maps which might be of independent interest."}
{"id": "2511.09890", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09890", "abs": "https://arxiv.org/abs/2511.09890", "authors": ["Masahiro Kojima", "Keisuke Hanada", "Atsuya Sato"], "title": "A Clustering Approach for Basket Trials Based on Treatment Response Trajectories", "comment": null, "summary": "Heterogeneity in efficacy is sometimes observed across baskets in basket trials. In this study, we propose a model-free clustering framework that groups baskets based on transition probabilities derived from the trajectories of treatment response, rather than relying solely on a single efficacy endpoint such as the objective response rate. The number of clusters is not predetermined but is automatically determined in a data-driven manner based on the similarity structure among baskets. After clustering, baskets within the same cluster are analyzed using a hierarchical Bayesian model. This framework aims to improve the estimation precision of efficacy endpoints and enhance statistical power while maintaining the type~I error rate at the nominal level. The performance of the proposed method was evaluated through simulation studies. The results demonstrated that the proposed method can accurately identify cluster structures in heterogeneous settings and, even under such conditions, maintain the type~I error rate at the nominal level while improving statistical power."}
{"id": "2511.10293", "categories": ["stat.ME", "math.OC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.10293", "abs": "https://arxiv.org/abs/2511.10293", "authors": ["Athanasios Christou Micheas"], "title": "Zeroes and Extrema of Functions via Random Measures", "comment": "Function extrema and zeroes; Optimization; Poisson point process; Random counting measures; Riemann's zeta function", "summary": "We present methods that provide all zeroes and extrema of a function that do not require differentiation. Using point process theory, we are able to describe the locations of zeroes or maxima, their number, as well as their distribution over a given window of observation. The algorithms in order to accomplish the theoretical development are also provided, and they are exemplified using many illustrative examples, for real and complex functions."}
{"id": "2511.09767", "categories": ["stat.ME", "cs.LG", "econ.GN", "eess.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09767", "abs": "https://arxiv.org/abs/2511.09767", "authors": ["Alessandro V. M. Oliveira"], "title": "Modelos Empiricos de Pos-Dupla Selecao por LASSO: Discussoes para Estudos do Transporte Aereo", "comment": "Article in Portuguese", "summary": "This paper presents and discusses forms of estimation by regularized regression and model selection using the LASSO method - Least Absolute Shrinkage and Selection Operator. LASSO is recognized as one of the main supervised learning methods applied to high-dimensional econometrics, allowing work with large volumes of data and multiple correlated controls. Conceptual issues related to the consequences of high dimensionality in modern econometrics and the principle of sparsity, which underpins regularization procedures, are addressed. The study examines the main post-double selection and post-regularization models, including variations applied to instrumental variable models. A brief description of the lassopack routine package, its syntaxes, and examples of HD, HDS (High-Dimension Sparse), and IV-HDS models, with combinations involving fixed effects estimators, is also presented. Finally, the potential application of the approach in research focused on air transport is discussed, with emphasis on an empirical study on the operational efficiency of airlines and aircraft fuel consumption."}
{"id": "2511.10383", "categories": ["stat.ML", "cs.LG", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.10383", "abs": "https://arxiv.org/abs/2511.10383", "authors": ["Nicolas Hoischen", "Petar Bevanda", "Max Beier", "Stefan Sosnowski", "Boris Houska", "Sandra Hirche"], "title": "Operator Models for Continuous-Time Offline Reinforcement Learning", "comment": null, "summary": "Continuous-time stochastic processes underlie many natural and engineered systems. In healthcare, autonomous driving, and industrial control, direct interaction with the environment is often unsafe or impractical, motivating offline reinforcement learning from historical data. However, there is limited statistical understanding of the approximation errors inherent in learning policies from offline datasets. We address this by linking reinforcement learning to the Hamilton-Jacobi-Bellman equation and proposing an operator-theoretic algorithm based on a simple dynamic programming recursion. Specifically, we represent our world model in terms of the infinitesimal generator of controlled diffusion processes learned in a reproducing kernel Hilbert space. By integrating statistical learning methods and operator theory, we establish global convergence of the value function and derive finite-sample guarantees with bounds tied to system properties such as smoothness and stability. Our theoretical and numerical results indicate that operator-based approaches may hold promise in solving offline reinforcement learning using continuous-time optimal control."}
{"id": "2511.10077", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.10077", "abs": "https://arxiv.org/abs/2511.10077", "authors": ["Yi Liu", "Yuan Wang", "Ying Gao", "Tonia Poteat", "Roland A. Matsouaka"], "title": "A tutorial for propensity score weighting methods under violations of the positivity assumption", "comment": null, "summary": "Violations of the positivity assumption can render conventional causal estimands unidentifiable, including the average treatment effect (ATE), the average treatment effect on the treated (ATT), and the average treatment effect on the controls (ATC). Shifting the inferential focus to their alternative counterparts -- the weighted ATE (WATE), the weighted ATT (WATT), and the weighted ATC (WATC) -- offers valuable insights into treatment effects while preserving internal validity. In this tutorial, we provide a comprehensive review of recent advances in propensity score (PS) weighting methods, along with practical guidance on how to select a primary target estimand (while other estimands serve as supplementary analyses), implement the corresponding PS-weighted estimators, and conduct post-weighting diagnostic assessments. The tutorial is accompanied by a user-friendly R package, ChiPS. We demonstrate the pertinence of various estimators through extensive simulation studies. We illustrate the flow of the tutorial on two real-world case studies: (i) Effect of smoking on blood lead level using data from the 2007-2008 National Health and Nutrition Examination Survey (NHANES); and (ii) Impact of history of sex work on HIV status among transgender women in South Africa."}
{"id": "2511.09814", "categories": ["stat.ME", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09814", "abs": "https://arxiv.org/abs/2511.09814", "authors": ["Yuki Murakami", "Takumi Hattori", "Kohsuke Kubota"], "title": "Multiple Treatments Causal Effects Estimation with Task Embeddings and Balanced Representation Learning", "comment": "33 pages", "summary": "The simultaneous application of multiple treatments is increasingly common in many fields, such as healthcare and marketing. In such scenarios, it is important to estimate the single treatment effects and the interaction treatment effects that arise from treatment combinations. Previous studies have proposed using independent outcome networks with subnetworks for interactions, or combining task embedding networks that capture treatment similarity with variational autoencoders. However, these methods suffer from the lack of parameter sharing among related treatments, or the estimation of unnecessary latent variables reduces the accuracy of causal effect estimation. To address these issues, we propose a novel deep learning framework that incorporates a task embedding network and a representation learning network with the balancing penalty. The task embedding network enables parameter sharing across related treatment patterns because it encodes elements common to single effects and contributions specific to interaction effects. The representation learning network with the balancing penalty learns representations nonparametrically from observed covariates while reducing distances in representation distributions across different treatment patterns. This process mitigates selection bias and avoids model misspecification. Simulation studies demonstrate that the proposed method outperforms existing baselines, and application to real-world marketing datasets confirms the practical implications and utility of our framework."}
{"id": "2511.10446", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10446", "abs": "https://arxiv.org/abs/2511.10446", "authors": ["Jonghun Lee", "YongKyung Oh", "Sungil Kim", "Dong-Young Lim"], "title": "Continuum Dropout for Neural Differential Equations", "comment": null, "summary": "Neural Differential Equations (NDEs) excel at modeling continuous-time dynamics, effectively handling challenges such as irregular observations, missing values, and noise. Despite their advantages, NDEs face a fundamental challenge in adopting dropout, a cornerstone of deep learning regularization, making them susceptible to overfitting. To address this research gap, we introduce Continuum Dropout, a universally applicable regularization technique for NDEs built upon the theory of alternating renewal processes. Continuum Dropout formulates the on-off mechanism of dropout as a stochastic process that alternates between active (evolution) and inactive (paused) states in continuous time. This provides a principled approach to prevent overfitting and enhance the generalization capabilities of NDEs. Moreover, Continuum Dropout offers a structured framework to quantify predictive uncertainty via Monte Carlo sampling at test time. Through extensive experiments, we demonstrate that Continuum Dropout outperforms existing regularization methods for NDEs, achieving superior performance on various time series and image classification tasks. It also yields better-calibrated and more trustworthy probability estimates, highlighting its effectiveness for uncertainty-aware modeling."}
{"id": "2511.09886", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09886", "abs": "https://arxiv.org/abs/2511.09886", "authors": ["Feifei Chen", "Kaiming Zhang", "Yanni Zhang", "Hua Liang"], "title": "Goodness-of-fit Test for Generalized Functional Linear Models via Projection Averaging", "comment": null, "summary": "Assessing model adequacy is a crucial step in regression analysis, ensuring the validity of statistical inferences. For Generalized Functional Linear Models (GFLMs), which are widely used for modeling relationships between scalar responses and functional predictors, there is a recognized need for formal goodness-of-fit testing procedures. Current literature on this specific topic remains limited. This paper introduces a novel goodness-of-fit test for GFLMs. The test statistic is formulated as a U-statistic derived from a Cramér-von-Mises metric integrated over all one-dimensional projections of the functional predictor. This projection averaging strategy is designed to effectively mitigate the curse of dimensionality. We establish the asymptotic normality of the test statistic under the null hypothesis and prove the consistency under the alternatives. As the asymptotic variance of the limiting null distribution can be complex for practical use, we also propose practical bootstrap resampling methods for both continuous and discrete responses to compute p-values. Simulation studies confirm that the proposed test demonstrates good power performance across various settings, showing advantages over existing methods."}
{"id": "2511.09759", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.09759", "abs": "https://arxiv.org/abs/2511.09759", "authors": ["Borna Bateni", "Yubai Yuan", "Qi Xu", "Annie Qu"], "title": "Distributional Treatment Effect Estimation across Heterogeneous Sites via Optimal Transport", "comment": null, "summary": "We propose a novel framework for synthesizing counterfactual treatment group data in a target site by integrating full treatment and control group data from a source site with control group data from the target. Departing from conventional average treatment effect estimation, our approach adopts a distributional causal inference perspective by modeling treatment and control as distinct probability measures on the source and target sites. We formalize the cross-site heterogeneity (effect modification) as a push-forward transformation that maps the joint feature-outcome distribution from the source to the target site. This transformation is learned by aligning the control group distributions between sites using an Optimal Transport-based procedure, and subsequently applied to the source treatment group to generate the synthetic target treatment distribution. Under general regularity conditions, we establish theoretical guarantees for the consistency and asymptotic convergence of the synthetic treatment group data to the true target distribution. Simulation studies across multiple data-generating scenarios and a real-world application to patient-derived xenograft data demonstrate that our framework robustly recovers the full distributional properties of treatment effects."}
{"id": "2511.09890", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.09890", "abs": "https://arxiv.org/abs/2511.09890", "authors": ["Masahiro Kojima", "Keisuke Hanada", "Atsuya Sato"], "title": "A Clustering Approach for Basket Trials Based on Treatment Response Trajectories", "comment": null, "summary": "Heterogeneity in efficacy is sometimes observed across baskets in basket trials. In this study, we propose a model-free clustering framework that groups baskets based on transition probabilities derived from the trajectories of treatment response, rather than relying solely on a single efficacy endpoint such as the objective response rate. The number of clusters is not predetermined but is automatically determined in a data-driven manner based on the similarity structure among baskets. After clustering, baskets within the same cluster are analyzed using a hierarchical Bayesian model. This framework aims to improve the estimation precision of efficacy endpoints and enhance statistical power while maintaining the type~I error rate at the nominal level. The performance of the proposed method was evaluated through simulation studies. The results demonstrated that the proposed method can accurately identify cluster structures in heterogeneous settings and, even under such conditions, maintain the type~I error rate at the nominal level while improving statistical power."}
{"id": "2511.10048", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10048", "abs": "https://arxiv.org/abs/2511.10048", "authors": ["Yanjiao Yang", "Daniel Suen", "Yen-Chi Chen"], "title": "Masking criteria for selecting an imputation model", "comment": "55 pages, 4 figures, 4 tables", "summary": "The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models."}
{"id": "2511.09946", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09946", "abs": "https://arxiv.org/abs/2511.09946", "authors": ["Susan Eldhose", "Bhargava Rama Chilukuri", "Chandrasekharan Rajendran"], "title": "Leader-Follower Identification Methodology for Non-Lane Disciplined Heterogeneous Traffic Using Steady State Features", "comment": null, "summary": "Road traffic in developing countries, such as India, features a heterogeneous mix of vehicles operating under weak lane discipline (HWLD), encompassing both motorised and non-motorised modes with diverse sizes and manoeuvrability. These conditions lead to complex driver interactions, complicating the reliable identification of vehicle-following (VF) behaviour and leader-follower (LF) pairs. Traditional identification methods based on fixed thresholds for longitudinal and lateral proximity often misclassify non-following instances as valid LF pairs, degrading model performance. This study presents a refined and adaptive method for LF identification in HWLD traffic. It employs vehicle-type- and speed-specific desirable gap thresholds derived from the fundamental density-speed diagram to eliminate false-positive pairs. Additionally, Mexican Hat Wavelet Transform (MWT) is employed to analyse LV and SV speed profiles, verifying LV-SV interaction for LF pair identification. The three-stage filtering includes: (i) speed-gap consistency, (ii) approach/diverge detection via relative velocity sign changes and gap range, and (iii) wavelet-based speed correlation using MWT to confirm LV influence on SV. The framework effectively filters out LF pairs associated with overtaking, tailgating, and inconsistent gap dynamics, retaining only those with consistent VF behaviour and improving model accuracy. Analysis across thirteen LF combinations shows that VF dynamics depend on both SV and LV types. Symmetric pairs (e.g., CAR-CAR, AUTO-CAR) exhibit higher predictability and lower errors, while asymmetric pairs with heavy vehicles or two-wheelers show greater variability. The framework offers a robust foundation for traffic modelling and behaviour analysis."}
{"id": "2511.09972", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09972", "abs": "https://arxiv.org/abs/2511.09972", "authors": ["Heyang Ji", "Lan Xue", "Ufuk Beyaztas", "Roger S. Zoh", "Jeff Goldsmith", "Mark E. Benden", "Carmen D. Tekwe"], "title": "Addressing zero-inflated and mis-measured functional predictors in scalar-on-function regression model", "comment": null, "summary": "Wearable devices are often used in clinical and epidemiological studies to monitor physical activity behavior and its influence on health outcomes. These devices are worn over multiple days to record activity patterns, such as step counts recorded at the minute level, resulting in multi-level, longitudinal, high-dimensional, or functional data. When monitoring patterns of step counts over multiple days, devices may record excess zeros during periods of sedentary behavior or non-wear times. Additionally, it has been demonstrated that the accuracy of wearable devices in monitoring true physical activity patterns depends on the intensity of the activities and wear times. While work on adjusting for biases due to measurement errors in functional data is a growing field, relatively less work has been done to study the occurrence of excess zeros along with measurement errors and their combined influence on estimation and inference in multi-level scalar-on-function regression models. We propose semi-continuous modeling approaches to adjust for biases due to zero inflation and measurement errors in scalar-on-function regression models. We provide theoretical justifications for our proposed methods and, through extensive simulations, we demonstrated their finite sample properties. Finally, the developed methods are applied to a school-based intervention study examining the association between school day physical activity with age- and sex-adjusted body mass index among elementary school-aged children."}
{"id": "2511.10016", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.10016", "abs": "https://arxiv.org/abs/2511.10016", "authors": ["Divan A. Burger", "Janet van Niekerk", "Peter C. le Roux", "Morgan J. Raath-Krüger"], "title": "Outlier-robust copula regression for bivariate continuous proportions: an application to cushion plant vitality", "comment": null, "summary": "Continuous proportions measured on the same experimental unit often pose two challenges: interior outliers that inflate variance beyond the beta ceiling and residual dependence that invalidates independent-margin models. We introduce a Bayesian copula modeling approach that combines rectangular-beta margins, which temper interior outliers by reallocating mass from the peak to a uniform component, with a single-parameter copula to capture concordance. Gaussian, Gumbel, and Clayton copula families are fitted, and log marginal likelihoods are obtained via bridge sampling to guide model selection. Applied to a 13-year survey (2003-2016) of Azorella selago cushion plants on sub-Antarctic Marion Island, the copula models outperform independence baselines in explaining percent dead stem cover. Accounting for between-year dependence uncovers a positive west-slope effect and weakens the cushion size effect. Simulation results show negligible bias and near-nominal 95% highest posterior density coverage across a range of tail weight and dependence scenarios, confirming good frequentist properties. The method integrates readily with JAGS and provides a robust default for paired proportion data in ecology and other disciplines where bounded outcomes and occasional outliers coincide."}
{"id": "2511.10048", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10048", "abs": "https://arxiv.org/abs/2511.10048", "authors": ["Yanjiao Yang", "Daniel Suen", "Yen-Chi Chen"], "title": "Masking criteria for selecting an imputation model", "comment": "55 pages, 4 figures, 4 tables", "summary": "The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models."}
{"id": "2511.10077", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.10077", "abs": "https://arxiv.org/abs/2511.10077", "authors": ["Yi Liu", "Yuan Wang", "Ying Gao", "Tonia Poteat", "Roland A. Matsouaka"], "title": "A tutorial for propensity score weighting methods under violations of the positivity assumption", "comment": null, "summary": "Violations of the positivity assumption can render conventional causal estimands unidentifiable, including the average treatment effect (ATE), the average treatment effect on the treated (ATT), and the average treatment effect on the controls (ATC). Shifting the inferential focus to their alternative counterparts -- the weighted ATE (WATE), the weighted ATT (WATT), and the weighted ATC (WATC) -- offers valuable insights into treatment effects while preserving internal validity. In this tutorial, we provide a comprehensive review of recent advances in propensity score (PS) weighting methods, along with practical guidance on how to select a primary target estimand (while other estimands serve as supplementary analyses), implement the corresponding PS-weighted estimators, and conduct post-weighting diagnostic assessments. The tutorial is accompanied by a user-friendly R package, ChiPS. We demonstrate the pertinence of various estimators through extensive simulation studies. We illustrate the flow of the tutorial on two real-world case studies: (i) Effect of smoking on blood lead level using data from the 2007-2008 National Health and Nutrition Examination Survey (NHANES); and (ii) Impact of history of sex work on HIV status among transgender women in South Africa."}
{"id": "2511.10293", "categories": ["stat.ME", "math.OC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.10293", "abs": "https://arxiv.org/abs/2511.10293", "authors": ["Athanasios Christou Micheas"], "title": "Zeroes and Extrema of Functions via Random Measures", "comment": "Function extrema and zeroes; Optimization; Poisson point process; Random counting measures; Riemann's zeta function", "summary": "We present methods that provide all zeroes and extrema of a function that do not require differentiation. Using point process theory, we are able to describe the locations of zeroes or maxima, their number, as well as their distribution over a given window of observation. The algorithms in order to accomplish the theoretical development are also provided, and they are exemplified using many illustrative examples, for real and complex functions."}
{"id": "2511.10336", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.10336", "abs": "https://arxiv.org/abs/2511.10336", "authors": ["Sophia Loizidou", "Christophe Ley", "Shogo Kato", "Kanti V. Mardia"], "title": "Modelling toroidal and cylindrical data via the trivariate wrapped Cauchy copula with non-uniform marginals", "comment": "13 pages, 3 figures", "summary": "In this paper, we propose a new flexible family of distributions for data that consist of three angles, two angles and one linear component, or one angle and two linear components. To achieve this, we equip the recently proposed trivariate wrapped Cauchy copula with non-uniform marginals and develop a parameter estimation procedure. We compare our model to its main competitors for analyzing trivariate data and provide some evidence of its advantages. We illustrate our new model using toroidal data from protein bioinformatics of conformational angles, and cylindrical data from climate science related to buoy in the Adriatic Sea. The paper is motivated by these real trivariate datasets."}
{"id": "2511.09823", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.09823", "abs": "https://arxiv.org/abs/2511.09823", "authors": ["Woojung Bae", "Dongrak Choi", "Jun Yan", "Sangwook Kang"], "title": "Diagnostics for Semiparametric Accelerated Failure Time Models with R Package afttest", "comment": null, "summary": "The semiparametric accelerated failure time (AFT) model is a useful alternative to the widely used Cox proportional hazard model, which directly links the logarithm of the failure time to the covariates, yielding more interpretable regression coefficients. However, diagnostic procedures for the semiparametric AFT model have received relatively little attention. This paper introduces afttest, an R package that implements recently developed diagnostic tools for the semiparametric AFT model. The package supports diagnostic procedures for models fitted with either rank-based or least-squares methods. It provides functions to assess model assumptions, including the overall adequacy, the link function, and functional form of each covariate. The test statistics are of Kolmogorov-type suprema of transformed aggregated martingale residual processes. The p-values are obtained by approximating the null distribution with an efficient multiplier bootstrap procedure. Additionally, the package offers graphical tools to compare the observed stochastic processes with a number of approximated realizations. Applications of the package to the well-known Mayo clinic primary biliary cirrhosis study are presented."}
