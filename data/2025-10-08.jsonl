{"id": "2510.05902", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.05902", "abs": "https://arxiv.org/abs/2510.05902", "authors": ["Amalan Mahendran", "Helen Thompson", "James M. McGree"], "title": "A subsampling approach for large data sets when the Generalised Linear Model is potentially misspecified", "comment": "34 pages", "summary": "Subsampling is a computationally efficient and scalable method to draw\ninference in large data settings based on a subset of the data rather than\nneeding to consider the whole dataset. When employing subsampling techniques, a\ncrucial consideration is how to select an informative subset based on the\nqueries posed by the data analyst. A recently proposed method for this purpose\ninvolves randomly selecting samples from the large dataset based on subsampling\nprobabilities. However, a major drawback of this approach is that the derived\nsubsampling probabilities are typically based on an assumed statistical model\nwhich may be difficult to correctly specify in practice. To address this\nlimitation, we propose to determine subsampling probabilities based on a\nstatistical model that we acknowledge may be misspecified. To do so, we propose\nto evaluate the subsampling probabilities based on the Mean Squared Error (MSE)\nof the predictions from a model that is not assumed to completely describe the\nlarge dataset. We apply our subsampling approach in a simulation study and for\nthe analysis of two real-world large datasets, where its performance is\nbenchmarked against existing subsampling techniques. The findings suggest that\nthere is value in adopting our approach over current practice."}
{"id": "2510.06177", "categories": ["stat.ME", "cs.IT", "math.IT", "math.ST", "stat.TH", "62H05"], "pdf": "https://arxiv.org/pdf/2510.06177", "abs": "https://arxiv.org/abs/2510.06177", "authors": ["Alan R. Pearse", "Howard Bondell"], "title": "Power-divergence copulas: A new class of Archimedean copulas, with an insurance application", "comment": "Main text 21 pages, 5 figures, 1 table, 1 algorithm. Total 39 pages\n  inc. supplement. Supplement has 3 figures, 1 algorithm", "summary": "This paper demonstrates that, under a particular convention, the convex\nfunctions that characterise the phi divergences also generate Archimedean\ncopulas in at least two dimensions. As a special case, we develop the family of\nArchimedean copulas associated with the important family of power divergences,\nwhich we call the power-divergence copulas. The properties of the family are\nextensively studied, including the subfamilies that are absolutely continuous\nor have a singular component, the ordering of the family, limiting cases (i.e.,\nthe Frechet-Hoeffding lower bound and Frechet-Hoeffding upper bound), the\nKendall's tau and tail-dependence coefficients, and cases that extend to three\nor more dimensions. In an illustrative application, the power-divergence\ncopulas are used to model a Danish fire insurance dataset. It is shown that the\npower-divergence copulas provide an adequate fit to the bivariate distribution\nof two kinds of fire-related losses claimed by businesses, while several\nbenchmarks (a suite of well known Archimedean, extreme-value, and elliptical\ncopulas) do not."}
{"id": "2510.05182", "categories": ["stat.ME", "math.OC", "stat.CO", "49Q99, 62-08, 62F40, 62-04"], "pdf": "https://arxiv.org/pdf/2510.05182", "abs": "https://arxiv.org/abs/2510.05182", "authors": ["Hajg Jasa", "Ronny Bergmann", "Christian Kümmerle", "Avanti Athreya", "Zachary Lubberts"], "title": "Procrustes Problems on Random Matrices", "comment": null, "summary": "Meaningful comparison between sets of observations often necessitates\nalignment or registration between them, and the resulting optimization problems\nrange in complexity from those admitting simple closed-form solutions to those\nrequiring advanced and novel techniques. We compare different Procrustes\nproblems in which we align two sets of points after various perturbations by\nminimizing the norm of the difference between one matrix and an orthogonal\ntransformation of the other. The minimization problem depends significantly on\nthe choice of matrix norm; we highlight recent developments in nonsmooth\nRiemannian optimization and characterize which choices of norm work best for\neach perturbation. We show that in several applications, from low-dimensional\nalignments to hypothesis testing for random networks, when Procrustes alignment\nwith the spectral or robust norm is the appropriate choice, it is often\nfeasible to replace the computationally more expensive spectral and robust\nminimizers with their closed-form Frobenius-norm counterpart. Our work\nreinforces the synergy between optimization, geometry, and statistics."}
{"id": "2510.05380", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.05380", "abs": "https://arxiv.org/abs/2510.05380", "authors": ["Grégoire Sergeant-Perthuis", "Léo Boitel"], "title": "Minima and Critical Points of the Bethe Free Energy Are Invariant Under Deformation Retractions of Factor Graphs", "comment": null, "summary": "In graphical models, factor graphs, and more generally energy-based models,\nthe interactions between variables are encoded by a graph, a hypergraph, or, in\nthe most general case, a partially ordered set (poset). Inference on such\nprobabilistic models cannot be performed exactly due to cycles in the\nunderlying structures of interaction. Instead, one resorts to approximate\nvariational inference by optimizing the Bethe free energy. Critical points of\nthe Bethe free energy correspond to fixed points of the associated Belief\nPropagation algorithm. A full characterization of these critical points for\ngeneral graphs, hypergraphs, and posets with a finite number of variables is\nstill an open problem. We show that, for hypergraphs and posets with chains of\nlength at most 1, changing the poset of interactions of the probabilistic model\nto one with the same homotopy type induces a bijection between the critical\npoints of the associated free energy. This result extends and unifies classical\nresults that assume specific forms of collapsibility to prove uniqueness of the\ncritical points of the Bethe free energy."}
{"id": "2510.05268", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.05268", "abs": "https://arxiv.org/abs/2510.05268", "authors": ["Abdellah Atanane", "Abdallah Mkhadri", "Karim Oualkacha"], "title": "An efficient hybrid approach of quantile and expectile regression", "comment": "37 pages, 8 figures", "summary": "Quantiles and expectiles are determined by different loss functions:\nasymmetric least absolute deviation for quantiles and asymmetric squared loss\nfor expectiles. This distinction ensures that quantile regression methods are\nrobust to outliers but somewhat less effective than expectile regression,\nespecially for normally distributed data. However, expectile regression is\nvulnerable to lack of robustness, especially for heavy-tailed distributions. To\naddress this trade-off between robustness and effectiveness, we propose a novel\napproach. By introducing a parameter $\\gamma$ that ranges between 0 and 1, we\ncombine the aforementioned loss functions, resulting in a hybrid approach of\nquantiles and expectiles. This fusion leads to the estimation of a new type of\nlocation parameter family within the linear regression framework, termed Hybrid\nof Quantile and Expectile Regression (HQER). The asymptotic properties of the\nresulting estimaror are then established. Through simulation studies, we\ncompare the asymptotic relative efficiency of the HQER estimator with its\ncompetitors, namely the quantile, expectile, and $k$th power expectile\nregression estimators. Our results show that HQER outperforms its competitors\nin several simulation scenarios. In addition, we apply HQER to a real dataset\nto illustrate its practical utility."}
{"id": "2510.05440", "categories": ["stat.ML", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05440", "abs": "https://arxiv.org/abs/2510.05440", "authors": ["Ran Canetti", "Ephraim Linder", "Connor Wagaman"], "title": "Refereed Learning", "comment": null, "summary": "We initiate an investigation of learning tasks in a setting where the learner\nis given access to two competing provers, only one of which is honest.\nSpecifically, we consider the power of such learners in assessing purported\nproperties of opaque models. Following prior work that considers the power of\ncompeting provers in different settings, we call this setting refereed\nlearning.\n  After formulating a general definition of refereed learning tasks, we show\nrefereed learning protocols that obtain a level of accuracy that far exceeds\nwhat is obtainable at comparable cost without provers, or even with a single\nprover. We concentrate on the task of choosing the better one out of two\nblack-box models, with respect to some ground truth. While we consider a range\nof parameters, perhaps our most notable result is in the high-precision range:\nFor all $\\varepsilon>0$ and ambient dimension $d$, our learner makes only one\nquery to the ground truth function, communicates only\n$(1+\\frac{1}{\\varepsilon^2})\\cdot\\text{poly}(d)$ bits with the provers, and\noutputs a model whose loss is within a multiplicative factor of\n$(1+\\varepsilon)$ of the best model's loss. Obtaining comparable loss with a\nsingle prover would require the learner to access the ground truth at almost\nall of the points in the domain. To obtain this bound, we develop a technique\nthat allows the learner to sample, using the provers, from a distribution that\nis not efficiently samplable to begin with. We find this technique to be of\nindependent interest.\n  We also present lower bounds that demonstrate the optimality of our protocols\nin a number of respects, including prover complexity, number of samples, and\nneed for query access."}
{"id": "2510.05182", "categories": ["stat.ME", "math.OC", "stat.CO", "49Q99, 62-08, 62F40, 62-04"], "pdf": "https://arxiv.org/pdf/2510.05182", "abs": "https://arxiv.org/abs/2510.05182", "authors": ["Hajg Jasa", "Ronny Bergmann", "Christian Kümmerle", "Avanti Athreya", "Zachary Lubberts"], "title": "Procrustes Problems on Random Matrices", "comment": null, "summary": "Meaningful comparison between sets of observations often necessitates\nalignment or registration between them, and the resulting optimization problems\nrange in complexity from those admitting simple closed-form solutions to those\nrequiring advanced and novel techniques. We compare different Procrustes\nproblems in which we align two sets of points after various perturbations by\nminimizing the norm of the difference between one matrix and an orthogonal\ntransformation of the other. The minimization problem depends significantly on\nthe choice of matrix norm; we highlight recent developments in nonsmooth\nRiemannian optimization and characterize which choices of norm work best for\neach perturbation. We show that in several applications, from low-dimensional\nalignments to hypothesis testing for random networks, when Procrustes alignment\nwith the spectral or robust norm is the appropriate choice, it is often\nfeasible to replace the computationally more expensive spectral and robust\nminimizers with their closed-form Frobenius-norm counterpart. Our work\nreinforces the synergy between optimization, geometry, and statistics."}
{"id": "2510.05646", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.05646", "abs": "https://arxiv.org/abs/2510.05646", "authors": ["Jean-Michel Poggi", "Bruno Portier", "Emma Thulliez"], "title": "Geographically Weighted Regression for Air Quality Low-Cost Sensor Calibration", "comment": null, "summary": "This article focuses on the use of Geographically Weighted Regression (GWR)\nmethod to correct air quality low-cost sensors measurements. Those sensors are\nof major interest in the current era of high-resolution air quality monitoring\nat urban scale, but require calibration using reference analyzers. The results\nfor NO2 are provided along with comments on the estimated GWR model and the\nspatial content of the estimated coefficients. The study has been carried out\nusing the publicly available SensEURCity dataset in Antwerp, which is\nespecially relevant since it includes 9 reference stations and 34 micro-sensors\ncollocated and deployed within the city."}
{"id": "2510.05353", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.05353", "abs": "https://arxiv.org/abs/2510.05353", "authors": ["Abid Hussain", "Touqeer Ahmad"], "title": "A new composite Mann-Whitney test for two-sample survival comparisons with right-censored data", "comment": "17 pages, 2 figures, 10 tables", "summary": "A fundamental challenge in comparing two survival distributions with right\ncensored data is the selection of an appropriate nonparametric test, as the\npower of standard tests like the Log rank and Wilcoxon is highly dependent on\nthe often unknown nature of the alternative hypothesis. This paper introduces a\nnew, distribution free two sample test designed to overcome this limitation.\nThe proposed method is based on a strategic decomposition of the data into\nuncensored and censored subsets, from which a composite test statistic is\nconstructed as the sum of two independent Mann Whitney statistics. This design\nallows the test to automatically and inherently adapt to various patterns of\ndifference including early, late, and crossing hazards without requiring pre\nspecified parameters, pre testing, or complex weighting schemes. An extensive\nMonte Carlo simulation study demonstrates that the proposed test robustly\nmaintains the nominal Type I error rate. Crucially, its power is highly\ncompetitive with the optimal traditional tests in standard scenarios and\nsuperior in complex settings with crossing survival curves, while also\nexhibiting remarkable robustness to high levels of censoring. The test power\neffectively approximates the maximum power achievable by either the Log rank or\nWilcoxon tests across a wide range of alternatives, offering a powerful,\nversatile, and computationally simple tool for survival analysis."}
{"id": "2510.05447", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05447", "abs": "https://arxiv.org/abs/2510.05447", "authors": ["Simon Segert", "Nathan Wycoff"], "title": "A Probabilistic Basis for Low-Rank Matrix Learning", "comment": null, "summary": "Low rank inference on matrices is widely conducted by optimizing a cost\nfunction augmented with a penalty proportional to the nuclear norm $\\Vert \\cdot\n\\Vert_*$. However, despite the assortment of computational methods for such\nproblems, there is a surprising lack of understanding of the underlying\nprobability distributions being referred to. In this article, we study the\ndistribution with density $f(X)\\propto e^{-\\lambda\\Vert X\\Vert_*}$, finding\nmany of its fundamental attributes to be analytically tractable via\ndifferential geometry. We use these facts to design an improved MCMC algorithm\nfor low rank Bayesian inference as well as to learn the penalty parameter\n$\\lambda$, obviating the need for hyperparameter tuning when this is difficult\nor impossible. Finally, we deploy these to improve the accuracy and efficiency\nof low rank Bayesian matrix denoising and completion algorithms in numerical\nexperiments."}
{"id": "2510.06051", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.06051", "abs": "https://arxiv.org/abs/2510.06051", "authors": ["Farhad de Sousa", "François Ribalet", "Jacob Bien"], "title": "Automated Gating for Flow Cytometry Data Using a Kernel-Smoothed EM Algorithm", "comment": null, "summary": "Phytoplankton are microscopic algae responsible for roughly half of the\nworld's photosynthesis that play a critical role in global carbon cycles and\noxygen production, and measuring the abundance of their subtypes across a wide\nrange of spatiotemporal scales is of great relevance to oceanography.\nHigh-frequency flow cytometry is a powerful technique in which oceanographers\nat sea can rapidly record the optical properties of tens of thousands of\nindividual phytoplankton cells every few minutes. Identifying distinct\nsubpopulations within these vast datasets (a process known as \"gating\") remains\na major challenge and has largely been performed manually so far. In this\npaper, we introduce a fast, automated gating method, which accurately\nidentifies phytoplankton populations by fitting a time-evolving mixture of\nGaussians model using an expectation-maximization-like algorithm with kernel\nsmoothing. We use simulated data to demonstrate the validity and robustness of\nthis approach, and use oceanographic cruise data to highlight the method's\nability to not only replicate but surpass expert manual gating. Finally, we\nprovide the flowkernel R package, written in literate programming, that\nimplements the algorithm efficiently."}
{"id": "2510.05960", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.05960", "abs": "https://arxiv.org/abs/2510.05960", "authors": ["Andrea Mecchina", "Roberta Pappadà", "Nicola Torelli"], "title": "Copula-Based Clustering of Financial Time Series via Evidence Accumulation", "comment": null, "summary": "Understanding the dependence structure of asset returns is fundamental in\nrisk assessment and is particularly relevant in a portfolio diversification\nstrategy. We propose a clustering approach where evidence accumulated in a\nmultiplicity of classifications is achieved using classical hierarchical\nprocedures and multiple copula-based dissimilarity measures. Assets that are\ngrouped in the same cluster are such that their stochastic behavior is similar\nduring risky scenarios, and riskaverse investors could exploit this information\nto build a risk-diversified portfolio. An empirical demonstration of such a\nstrategy is presented by using data from the EURO STOXX 50 index."}
{"id": "2510.05370", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.05370", "abs": "https://arxiv.org/abs/2510.05370", "authors": ["Xin Wang", "Xialu Liu"], "title": "Sparse-Group Factor Analysis for High-Dimensional Time Series", "comment": null, "summary": "Factor analysis is a widely used technique for dimension reduction in\nhigh-dimensional data. However, a key challenge in factor models lies in the\ninterpretability of the latent factors. One intuitive way to interpret these\nfactors is through their associated loadings. Liu and Wang proposed a novel\nframework that redefines factor models with sparse loadings to enhance\ninterpretability. In many high-dimensional time series applications, variables\nexhibit natural group structures. Building on this idea, our paper incorporates\ndomain knowledge and prior information by modeling both individual sparsity and\ngroup sparsity in the loading matrix. This dual-sparsity framework further\nimproves the interpretability of the estimated factors. We develop an algorithm\nto estimate both the loading matrix and the common component, and we establish\nthe asymptotic properties of the resulting estimators. Simulation studies\ndemonstrate the strong performance of the proposed method, and a real-data\napplication illustrates how incorporating prior knowledge leads to more\ninterpretable results."}
{"id": "2510.05566", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.05566", "abs": "https://arxiv.org/abs/2510.05566", "authors": ["Zhexiao Lin", "Yuanyuan Li", "Neeraj Sarna", "Yuanyuan Gao", "Michael von Gablenz"], "title": "Domain-Shift-Aware Conformal Prediction for Large Language Models", "comment": "26 pages", "summary": "Large language models have achieved impressive performance across diverse\ntasks. However, their tendency to produce overconfident and factually incorrect\noutputs, known as hallucinations, poses risks in real world applications.\nConformal prediction provides finite-sample, distribution-free coverage\nguarantees, but standard conformal prediction breaks down under domain shift,\noften leading to under-coverage and unreliable prediction sets. We propose a\nnew framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our\nframework adapts conformal prediction to large language models under domain\nshift, by systematically reweighting calibration samples based on their\nproximity to the test prompt, thereby preserving validity while enhancing\nadaptivity. Our theoretical analysis and experiments on the MMLU benchmark\ndemonstrate that the proposed method delivers more reliable coverage than\nstandard conformal prediction, especially under substantial distribution\nshifts, while maintaining efficiency. This provides a practical step toward\ntrustworthy uncertainty quantification for large language models in real-world\ndeployment."}
{"id": "2510.06121", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.06121", "abs": "https://arxiv.org/abs/2510.06121", "authors": ["Adam Bloomston", "Elizabeth Burke", "Megan Cacace", "Anne Diaz", "Wren Dougherty", "Matthew Gonzalez", "Remington Gregg", "Yeliz Güngör", "Bryce Hayes", "Eeway Hsu", "Oron Israeli", "Heesoo Kim", "Sara Kwasnick", "Joanne Lacsina", "Demma Rosa Rodriguez", "Adam Schiller", "Whitney Schumacher", "Jessica Simon", "Maggie Tang", "Skyler Wharton", "Marilyn Wilcken"], "title": "Measuring Data Quality for Project Lighthouse", "comment": null, "summary": "In this paper, we first situate the challenges for measuring data quality\nunder Project Lighthouse in the broader academic context. We then discuss in\ndetail the three core data quality metrics we use for measurement--two of which\nextend prior academic work. Using those data quality metrics as examples, we\npropose a framework, based on machine learning classification, for empirically\njustifying the choice of data quality metrics and their associated minimum\nthresholds. Finally we outline how these methods enable us to rigorously meet\nthe principle of data minimization when analyzing potential experience gaps\nunder Project Lighthouse, which we term quantitative data minimization."}
{"id": "2510.05545", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2510.05545", "abs": "https://arxiv.org/abs/2510.05545", "authors": ["Xinrui Ruan", "Xinwei Ma", "Yingfei Wang", "Waverly Wei", "Jingshen Wang"], "title": "Can language models boost the power of randomized experiments without statistical bias?", "comment": null, "summary": "Randomized experiments or randomized controlled trials (RCTs) are gold\nstandards for causal inference, yet cost and sample-size constraints limit\npower. Meanwhile, modern RCTs routinely collect rich, unstructured data that\nare highly prognostic of outcomes but rarely used in causal analyses. We\nintroduce CALM (Causal Analysis leveraging Language Models), a statistical\nframework that integrates large language models (LLMs) predictions with\nestablished causal estimators to increase precision while preserving\nstatistical validity. CALM treats LLM outputs as auxiliary prognostic\ninformation and corrects their potential bias via a heterogeneous calibration\nstep that residualizes and optimally reweights predictions. We prove that CALM\nremains consistent even when LLM predictions are biased and achieves efficiency\ngains over augmented inverse probability weighting estimators for various\ncausal effects. In particular, CALM develops a few-shot variant that aggregates\npredictions across randomly sampled demonstration sets. The resulting\nU-statistic-like predictor restores i.i.d. structure and also mitigates\nprompt-selection variability. Empirically, in simulations calibrated to a\nmobile-app depression RCT, CALM delivers lower variance relative to other\nbenchmarking methods, is effective in zero- and few-shot settings, and remains\nstable across prompt designs. By principled use of LLMs to harness unstructured\ndata and external knowledge learned during pretraining, CALM provides a\npractical path to more precise causal analyses in RCTs."}
{"id": "2510.05568", "categories": ["stat.ML", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.05568", "abs": "https://arxiv.org/abs/2510.05568", "authors": ["Nicholas H. Nelsen", "Houman Owhadi", "Andrew M. Stuart", "Xianjin Yang", "Zongren Zou"], "title": "Bilevel optimization for learning hyperparameters: Application to solving PDEs and inverse problems with Gaussian processes", "comment": null, "summary": "Methods for solving scientific computing and inference problems, such as\nkernel- and neural network-based approaches for partial differential equations\n(PDEs), inverse problems, and supervised learning tasks, depend crucially on\nthe choice of hyperparameters. Specifically, the efficacy of such methods, and\nin particular their accuracy, stability, and generalization properties,\nstrongly depends on the choice of hyperparameters. While bilevel optimization\noffers a principled framework for hyperparameter tuning, its nested\noptimization structure can be computationally demanding, especially in\nPDE-constrained contexts. In this paper, we propose an efficient strategy for\nhyperparameter optimization within the bilevel framework by employing a\nGauss-Newton linearization of the inner optimization step. Our approach\nprovides closed-form updates, eliminating the need for repeated costly PDE\nsolves. As a result, each iteration of the outer loop reduces to a single\nlinearized PDE solve, followed by explicit gradient-based hyperparameter\nupdates. We demonstrate the effectiveness of the proposed method through\nGaussian process models applied to nonlinear PDEs and to PDE inverse problems.\nExtensive numerical experiments highlight substantial improvements in accuracy\nand robustness compared to conventional random hyperparameter initialization.\nIn particular, experiments with additive kernels and neural\nnetwork-parameterized deep kernels demonstrate the method's scalability and\neffectiveness for high-dimensional hyperparameter optimization."}
{"id": "2510.06191", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.06191", "abs": "https://arxiv.org/abs/2510.06191", "authors": ["Mariya Mamajiwala", "Cesare Corrado", "Chris Lanyon", "Steven A. Niederer", "Richard D. Wilkinson", "Richard H. Clayton"], "title": "Rapid calibration of atrial electrophysiology models using Gaussian process emulators in the ensemble Kalman filter", "comment": null, "summary": "Atrial fibrillation (AF) is a common cardiac arrhythmia characterised by\ndisordered electrical activity in the atria. The standard treatment is catheter\nablation, which is invasive and irreversible. Recent advances in computational\nelectrophysiology offer the potential for patient-specific models, often\nreferred to as digital twins, that can be used to guide clinical decisions. To\nbe of practical value, we must be able to rapidly calibrate physics-based\nmodels using routine clinical measurements. We pose this calibration task as a\nstatic inverse problem, where the goal is to infer tissue-level\nelectrophysiological parameters from the available observations. To make this\ntractable, we replace the expensive forward model with Gaussian process\nemulators (GPEs), and propose a novel adaptation of the ensemble Kalman filter\n(EnKF) for static non-linear inverse problems. The approach yields parameter\nsamples that can be interpreted as coming from the best Gaussian approximation\nof the posterior distribution. We compare our results with those obtained using\nMarkov chain Monte Carlo (MCMC) sampling and demonstrate the potential of the\napproach to enable near-real-time patient-specific calibration, a key step\ntowards predicting outcomes of AF treatment within clinical timescales. The\napproach is readily applicable to a wide range of static inverse problems in\nscience and engineering."}
{"id": "2510.05680", "categories": ["stat.ME", "60J10"], "pdf": "https://arxiv.org/pdf/2510.05680", "abs": "https://arxiv.org/abs/2510.05680", "authors": ["Anna Nalpantidi", "Dimitris Karlis"], "title": "A Bivariate DAR($1$) model for ordinal time series", "comment": null, "summary": "We present a bivariate vector valued discrete autoregressive model of order\n$1$ (BDAR($1$)) for discrete time series. The BDAR($1$) model assumes that each\ntime series follows its own univariate DAR($1$) model with dependent random\nmechanisms that determine from which component the current status occurs and\ndependent innovations. The joint distribution of the random mechanisms which\nare expressed by Bernoulli vectors are proposed to be defined through copulas.\nThe same holds for the joint distribution of innovation terms. Properties of\nthe model are provided, while special focus is given to the case of bivariate\nordinal time series. A simulation study is presented, indicating that model\nprovides efficient estimates even in case of moderate sample size. Finally, a\nreal data application on unemployment state of two countries is presented, for\nillustrating the proposed model."}
{"id": "2510.05573", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.05573", "abs": "https://arxiv.org/abs/2510.05573", "authors": ["Hossein Taheri", "Avishek Ghosh", "Arya Mazumdar"], "title": "On the Theory of Continual Learning with Gradient Descent for Neural Networks", "comment": null, "summary": "Continual learning, the ability of a model to adapt to an ongoing sequence of\ntasks without forgetting the earlier ones, is a central goal of artificial\nintelligence. To shed light on its underlying mechanisms, we analyze the\nlimitations of continual learning in a tractable yet representative setting. In\nparticular, we study one-hidden-layer quadratic neural networks trained by\ngradient descent on an XOR cluster dataset with Gaussian noise, where different\ntasks correspond to different clusters with orthogonal means. Our results\nobtain bounds on the rate of forgetting during train and test-time in terms of\nthe number of iterations, the sample size, the number of tasks, and the\nhidden-layer size. Our results reveal interesting phenomena on the role of\ndifferent problem parameters in the rate of forgetting. Numerical experiments\nacross diverse setups confirm our results, demonstrating their validity beyond\nthe analyzed settings."}
{"id": "2510.06210", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.06210", "abs": "https://arxiv.org/abs/2510.06210", "authors": ["Francesca Fiori", "Andrea Riebler", "Sara Martino"], "title": "Geographical inequalities in mortality by age and gender in Italy, 2002-2019: insights from a spatial extension of the Lee-Carter model", "comment": null, "summary": "Italy reports some of the lowest levels of mortality in the developed world.\nRecent evidence, however, suggests that even in low mortality countries\nimprovements may be slowing and regional inequalities widening. This study\ncontributes new empirical evidence to the debate by analysing mortality data by\nsingle year of age for males and females across 107 provinces in Italy from\n2002 to 2019. We extend the widely used Lee Carter model to include spatially\nvarying age specific effects, and further specify it to capture space age time\ninteractions. The model is estimated in a Bayesian framework using the inlabru\npackage, which builds on INLA (Integrated Nested Laplace Approximation) for non\nlinear models and facilitates the use of smoothing priors. This approach\nborrows strength across provinces and years, mitigating random fluctuations in\nsmall area death counts. Results demonstrate the value of such a granular\napproach, highlighting the existence of an uneven geography of mortality\ndespite overall national improvements. Mortality disadvantage is concentrated\nin parts of the Centre South and North West, while the Centre North and North\nEast fare relatively better. These geographical differences have widened since\n2010, with clear age and gender specific patterns, being more pronounced at\nyounger adult ages for men and at older adult ages for women. Future work may\ninvolve refining the analysis to mortality by cause of death or socioeconomic\nstatus, informing more targeted public health policies to address mortality\ndisparities across Italy's provinces."}
{"id": "2510.05861", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.05861", "abs": "https://arxiv.org/abs/2510.05861", "authors": ["Bo-Yao Lian", "Nelson G. Chen"], "title": "Extension of Wald-Wolfowitz Runs Test for Regression Validity Testing with Repeated Measures of Independent Variable", "comment": "15 pages, 2 figures", "summary": "The Wald-Wolfowitz runs test can assess the correctness of a regression curve\nfitted to a data set with one independent parameter. The assessment is\nperformed through examination of the residuals, where the signs of the\nresiduals would appear randomly if the regression curve were correct. We\npropose extending the test to the case where multiple data points were measured\nfor specific independent parameter values. By randomly permutating the data\npoints corresponding to each independent parameter value and treating their\nresiduals as occurring in their permutated sequence and then executing the runs\ntest, results are shown to be equivalent to those of a data set containing the\nsame number of points with no repeated measurements. This approach avoids the\nloss of points, and hence loss of test sensitivity, were the means at each\nindependent parameter value used. It also avoids the problem of weighting each\nmean differently if the number of data points measured at each parameter value\nis not identical."}
{"id": "2510.06149", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06149", "abs": "https://arxiv.org/abs/2510.06149", "authors": ["Hwanwoo Kim", "Dongkyu Derek Cho", "Eric Laber"], "title": "Implicit Updates for Average-Reward Temporal Difference Learning", "comment": null, "summary": "Temporal difference (TD) learning is a cornerstone of reinforcement learning.\nIn the average-reward setting, standard TD($\\lambda$) is highly sensitive to\nthe choice of step-size and thus requires careful tuning to maintain numerical\nstability. We introduce average-reward implicit TD($\\lambda$), which employs an\nimplicit fixed point update to provide data-adaptive stabilization while\npreserving the per iteration computational complexity of standard\naverage-reward TD($\\lambda$). In contrast to prior finite-time analyses of\naverage-reward TD($\\lambda$), which impose restrictive step-size conditions, we\nestablish finite-time error bounds for the implicit variant under substantially\nweaker step-size requirements. Empirically, average-reward implicit\nTD($\\lambda$) operates reliably over a much broader range of step-sizes and\nexhibits markedly improved numerical stability. This enables more efficient\npolicy evaluation and policy learning, highlighting its effectiveness as a\nrobust alternative to average-reward TD($\\lambda$)."}
{"id": "2510.05353", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.05353", "abs": "https://arxiv.org/abs/2510.05353", "authors": ["Abid Hussain", "Touqeer Ahmad"], "title": "A new composite Mann-Whitney test for two-sample survival comparisons with right-censored data", "comment": "17 pages, 2 figures, 10 tables", "summary": "A fundamental challenge in comparing two survival distributions with right\ncensored data is the selection of an appropriate nonparametric test, as the\npower of standard tests like the Log rank and Wilcoxon is highly dependent on\nthe often unknown nature of the alternative hypothesis. This paper introduces a\nnew, distribution free two sample test designed to overcome this limitation.\nThe proposed method is based on a strategic decomposition of the data into\nuncensored and censored subsets, from which a composite test statistic is\nconstructed as the sum of two independent Mann Whitney statistics. This design\nallows the test to automatically and inherently adapt to various patterns of\ndifference including early, late, and crossing hazards without requiring pre\nspecified parameters, pre testing, or complex weighting schemes. An extensive\nMonte Carlo simulation study demonstrates that the proposed test robustly\nmaintains the nominal Type I error rate. Crucially, its power is highly\ncompetitive with the optimal traditional tests in standard scenarios and\nsuperior in complex settings with crossing survival curves, while also\nexhibiting remarkable robustness to high levels of censoring. The test power\neffectively approximates the maximum power achievable by either the Log rank or\nWilcoxon tests across a wide range of alternatives, offering a powerful,\nversatile, and computationally simple tool for survival analysis."}
{"id": "2510.05902", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.05902", "abs": "https://arxiv.org/abs/2510.05902", "authors": ["Amalan Mahendran", "Helen Thompson", "James M. McGree"], "title": "A subsampling approach for large data sets when the Generalised Linear Model is potentially misspecified", "comment": "34 pages", "summary": "Subsampling is a computationally efficient and scalable method to draw\ninference in large data settings based on a subset of the data rather than\nneeding to consider the whole dataset. When employing subsampling techniques, a\ncrucial consideration is how to select an informative subset based on the\nqueries posed by the data analyst. A recently proposed method for this purpose\ninvolves randomly selecting samples from the large dataset based on subsampling\nprobabilities. However, a major drawback of this approach is that the derived\nsubsampling probabilities are typically based on an assumed statistical model\nwhich may be difficult to correctly specify in practice. To address this\nlimitation, we propose to determine subsampling probabilities based on a\nstatistical model that we acknowledge may be misspecified. To do so, we propose\nto evaluate the subsampling probabilities based on the Mean Squared Error (MSE)\nof the predictions from a model that is not assumed to completely describe the\nlarge dataset. We apply our subsampling approach in a simulation study and for\nthe analysis of two real-world large datasets, where its performance is\nbenchmarked against existing subsampling techniques. The findings suggest that\nthere is value in adopting our approach over current practice."}
{"id": "2510.05566", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.05566", "abs": "https://arxiv.org/abs/2510.05566", "authors": ["Zhexiao Lin", "Yuanyuan Li", "Neeraj Sarna", "Yuanyuan Gao", "Michael von Gablenz"], "title": "Domain-Shift-Aware Conformal Prediction for Large Language Models", "comment": "26 pages", "summary": "Large language models have achieved impressive performance across diverse\ntasks. However, their tendency to produce overconfident and factually incorrect\noutputs, known as hallucinations, poses risks in real world applications.\nConformal prediction provides finite-sample, distribution-free coverage\nguarantees, but standard conformal prediction breaks down under domain shift,\noften leading to under-coverage and unreliable prediction sets. We propose a\nnew framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our\nframework adapts conformal prediction to large language models under domain\nshift, by systematically reweighting calibration samples based on their\nproximity to the test prompt, thereby preserving validity while enhancing\nadaptivity. Our theoretical analysis and experiments on the MMLU benchmark\ndemonstrate that the proposed method delivers more reliable coverage than\nstandard conformal prediction, especially under substantial distribution\nshifts, while maintaining efficiency. This provides a practical step toward\ntrustworthy uncertainty quantification for large language models in real-world\ndeployment."}
{"id": "2510.06051", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.06051", "abs": "https://arxiv.org/abs/2510.06051", "authors": ["Farhad de Sousa", "François Ribalet", "Jacob Bien"], "title": "Automated Gating for Flow Cytometry Data Using a Kernel-Smoothed EM Algorithm", "comment": null, "summary": "Phytoplankton are microscopic algae responsible for roughly half of the\nworld's photosynthesis that play a critical role in global carbon cycles and\noxygen production, and measuring the abundance of their subtypes across a wide\nrange of spatiotemporal scales is of great relevance to oceanography.\nHigh-frequency flow cytometry is a powerful technique in which oceanographers\nat sea can rapidly record the optical properties of tens of thousands of\nindividual phytoplankton cells every few minutes. Identifying distinct\nsubpopulations within these vast datasets (a process known as \"gating\") remains\na major challenge and has largely been performed manually so far. In this\npaper, we introduce a fast, automated gating method, which accurately\nidentifies phytoplankton populations by fitting a time-evolving mixture of\nGaussians model using an expectation-maximization-like algorithm with kernel\nsmoothing. We use simulated data to demonstrate the validity and robustness of\nthis approach, and use oceanographic cruise data to highlight the method's\nability to not only replicate but surpass expert manual gating. Finally, we\nprovide the flowkernel R package, written in literate programming, that\nimplements the algorithm efficiently."}
{"id": "2510.06051", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.06051", "abs": "https://arxiv.org/abs/2510.06051", "authors": ["Farhad de Sousa", "François Ribalet", "Jacob Bien"], "title": "Automated Gating for Flow Cytometry Data Using a Kernel-Smoothed EM Algorithm", "comment": null, "summary": "Phytoplankton are microscopic algae responsible for roughly half of the\nworld's photosynthesis that play a critical role in global carbon cycles and\noxygen production, and measuring the abundance of their subtypes across a wide\nrange of spatiotemporal scales is of great relevance to oceanography.\nHigh-frequency flow cytometry is a powerful technique in which oceanographers\nat sea can rapidly record the optical properties of tens of thousands of\nindividual phytoplankton cells every few minutes. Identifying distinct\nsubpopulations within these vast datasets (a process known as \"gating\") remains\na major challenge and has largely been performed manually so far. In this\npaper, we introduce a fast, automated gating method, which accurately\nidentifies phytoplankton populations by fitting a time-evolving mixture of\nGaussians model using an expectation-maximization-like algorithm with kernel\nsmoothing. We use simulated data to demonstrate the validity and robustness of\nthis approach, and use oceanographic cruise data to highlight the method's\nability to not only replicate but surpass expert manual gating. Finally, we\nprovide the flowkernel R package, written in literate programming, that\nimplements the algorithm efficiently."}
{"id": "2510.06136", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.06136", "abs": "https://arxiv.org/abs/2510.06136", "authors": ["Jieyun Wang", "Anna L. Smith"], "title": "Geometric Model Selection for Latent Space Network Models: Hypothesis Testing via Multidimensional Scaling and Resampling Techniques", "comment": null, "summary": "Latent space models assume that network ties are more likely between nodes\nthat are closer together in an underlying latent space. Euclidean space is a\npopular choice for the underlying geometry, but hyperbolic geometry can mimic\nmore realistic patterns of ties in complex networks. To identify the underlying\ngeometry, past research has applied non-Euclidean extensions of\nmultidimensional scaling (MDS) to the observed geodesic distances: the shortest\npath lengths between nodes. The difference in stress, a standard\ngoodness-of-fit metric for MDS, across the geometries is then used to select a\nlatent geometry with superior model fit (lower stress). The effectiveness of\nthis method is assessed through simulations of latent space networks in\nEuclidean and hyperbolic geometries. To better account for uncertainty, we\nextend permutation-based hypothesis tests for MDS to the latent network\nsetting. However, these tests do not incorporate any network structure. We\npropose a parametric bootstrap distribution of networks, conditioned on\nobserved geodesic distances and the Gaussian Latent Position Model (GLPM). Our\nmethod extends the Davidson-MacKinnon J-test to latent space network models\nwith differing latent geometries. We pay particular attention to large and\nsparse networks, and both the permutation test and the bootstrapping methods\nshow an improvement in detecting the underlying geometry."}
{"id": "2510.06211", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.06211", "abs": "https://arxiv.org/abs/2510.06211", "authors": ["Andreas Anastasiou", "Ivor Cribben"], "title": "Tensor time series change-point detection in cryptocurrency network data", "comment": null, "summary": "Financial fraud has been growing exponentially in recent years. The rise of\ncryptocurrencies as an investment asset has simultaneously seen a parallel\ngrowth in cryptocurrency scams. To detect possible cryptocurrency fraud, and in\nparticular market manipulation, previous research focused on the detection of\nchanges in the network of trades; however, market manipulators are now trading\nacross multiple cryptocurrency platforms, making their detection more\ndifficult. Hence, it is important to consider the identification of changes\nacross several trading networks or a `network of networks' over time. To this\nend, in this article, we propose a new change-point detection method in the\nnetwork structure of tensor-variate data. This new method, labeled TenSeg,\nfirst employs a tensor decomposition, and second detects multiple change-points\nin the second-order (cross-covariance or network) structure of the decomposed\ndata. It allows for change-point detection in the presence of frequent changes\nof possibly small magnitudes and is computationally fast. We apply our method\nto several simulated datasets and to a cryptocurrency dataset, which consists\nof network tensor-variate data from the Ethereum blockchain. We demonstrate\nthat our approach substantially outperforms other state-of-the-art change-point\ntechniques, and the detected change-points in the Ethereum data set coincide\nwith changes across several trading networks or a `network of networks' over\ntime. Finally, all the relevant \\textsf{R} code implementing the method in the\narticle are available on https://github.com/Anastasiou-Andreas/TenSeg."}
{"id": "2510.06157", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.06157", "abs": "https://arxiv.org/abs/2510.06157", "authors": ["Cristian F. Jiménez-Varón", "Marina I. Knight"], "title": "A GNAR-Based Framework for Spectral Estimation of Network Time Series: Application to Global Bank Network Connectedness", "comment": null, "summary": "Patterns of dependence in financial networks, such as global bank\nconnectedness, evolve over time and across frequencies. Analysing these systems\nrequires statistical tools that jointly capture temporal dynamics and the\nunderlying network topology. This work develops a novel spectral analysis\nframework for Generalized Network Autoregressive (GNAR) processes, modeling\ndependencies beyond direct neighbours by incorporating r-stage neighbourhood\neffects, unlike existing methods that at best rely solely on adjacency-based\ninteractions. We define the GNAR spectral density and related quantities, such\nas coherence and partial coherence, for which we propose both parametric and\nnetwork-penalized nonparametric estimators. Extensive simulations demonstrate\nthe strong performance of the parametric spectral estimator, as also backed up\nby theoretical arguments. The proposed framework has wide applications, and\nhere we focus on the analysis of global bank network connectedness. The\nfindings illustrate how the GNAR spectral quantities effectively capture the\nfrequency-specific cross-nodal dependencies, thus yielding estimates consistent\nwith established measures, while also uncovering richer temporal and structural\npatterns of volatility transmission."}
{"id": "2510.06177", "categories": ["stat.ME", "cs.IT", "math.IT", "math.ST", "stat.TH", "62H05"], "pdf": "https://arxiv.org/pdf/2510.06177", "abs": "https://arxiv.org/abs/2510.06177", "authors": ["Alan R. Pearse", "Howard Bondell"], "title": "Power-divergence copulas: A new class of Archimedean copulas, with an insurance application", "comment": "Main text 21 pages, 5 figures, 1 table, 1 algorithm. Total 39 pages\n  inc. supplement. Supplement has 3 figures, 1 algorithm", "summary": "This paper demonstrates that, under a particular convention, the convex\nfunctions that characterise the phi divergences also generate Archimedean\ncopulas in at least two dimensions. As a special case, we develop the family of\nArchimedean copulas associated with the important family of power divergences,\nwhich we call the power-divergence copulas. The properties of the family are\nextensively studied, including the subfamilies that are absolutely continuous\nor have a singular component, the ordering of the family, limiting cases (i.e.,\nthe Frechet-Hoeffding lower bound and Frechet-Hoeffding upper bound), the\nKendall's tau and tail-dependence coefficients, and cases that extend to three\nor more dimensions. In an illustrative application, the power-divergence\ncopulas are used to model a Danish fire insurance dataset. It is shown that the\npower-divergence copulas provide an adequate fit to the bivariate distribution\nof two kinds of fire-related losses claimed by businesses, while several\nbenchmarks (a suite of well known Archimedean, extreme-value, and elliptical\ncopulas) do not."}
{"id": "2510.06211", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.06211", "abs": "https://arxiv.org/abs/2510.06211", "authors": ["Andreas Anastasiou", "Ivor Cribben"], "title": "Tensor time series change-point detection in cryptocurrency network data", "comment": null, "summary": "Financial fraud has been growing exponentially in recent years. The rise of\ncryptocurrencies as an investment asset has simultaneously seen a parallel\ngrowth in cryptocurrency scams. To detect possible cryptocurrency fraud, and in\nparticular market manipulation, previous research focused on the detection of\nchanges in the network of trades; however, market manipulators are now trading\nacross multiple cryptocurrency platforms, making their detection more\ndifficult. Hence, it is important to consider the identification of changes\nacross several trading networks or a `network of networks' over time. To this\nend, in this article, we propose a new change-point detection method in the\nnetwork structure of tensor-variate data. This new method, labeled TenSeg,\nfirst employs a tensor decomposition, and second detects multiple change-points\nin the second-order (cross-covariance or network) structure of the decomposed\ndata. It allows for change-point detection in the presence of frequent changes\nof possibly small magnitudes and is computationally fast. We apply our method\nto several simulated datasets and to a cryptocurrency dataset, which consists\nof network tensor-variate data from the Ethereum blockchain. We demonstrate\nthat our approach substantially outperforms other state-of-the-art change-point\ntechniques, and the detected change-points in the Ethereum data set coincide\nwith changes across several trading networks or a `network of networks' over\ntime. Finally, all the relevant \\textsf{R} code implementing the method in the\narticle are available on https://github.com/Anastasiou-Andreas/TenSeg."}
