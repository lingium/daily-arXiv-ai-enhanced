{"id": "2512.16363", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.16363", "abs": "https://arxiv.org/abs/2512.16363", "authors": ["Guanghui Wang", "Mengtao Wen", "Changliang Zou"], "title": "Empirical Likelihood Meets Prediction-Powered Inference", "comment": null, "summary": "We study inference with a small labeled sample, a large unlabeled sample, and high-quality predictions from an external model. We link prediction-powered inference with empirical likelihood by stacking supervised estimating equations based on labeled outcomes with auxiliary moment conditions built from predictions, and then optimizing empirical likelihood under these joint constraints. The resulting empirical likelihood-based prediction-powered inference (EPI) estimator is asymptotically normal, has asymptotic variance no larger than the fully supervised estimator, and attains the semiparametric efficiency bound when the auxiliary functions span the predictable component of the supervised score. For hypothesis testing and confidence sets, empirical likelihood ratio statistics admit chi-squared-type limiting distributions. As a by-product, the empirical likelihood weights induce a calibrated empirical distribution that integrates supervised and prediction-based information, enabling estimation and uncertainty quantification for general functionals beyond parameters defined by estimating equations. We present two practical implementations: one based on basis expansions in the predictions and covariates, and one that learns an approximately optimal auxiliary function by cross-fitting. In simulations and applications, EPI reduces mean squared error and shortens confidence intervals while maintaining nominal coverage."}
{"id": "2512.16411", "categories": ["stat.ME", "math.ST", "q-fin.ST", "q-fin.TR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16411", "abs": "https://arxiv.org/abs/2512.16411", "authors": ["Matthieu Garcin", "Louis Perot"], "title": "Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection", "comment": null, "summary": "Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices."}
{"id": "2512.16105", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16105", "abs": "https://arxiv.org/abs/2512.16105", "authors": ["Sophia Seulkee Kang", "François-Xavier Briol", "Toni Karvonen", "Zonghao Chen"], "title": "BayesSum: Bayesian Quadrature in Discrete Spaces", "comment": null, "summary": "This paper addresses the challenging computational problem of estimating intractable expectations over discrete domains. Existing approaches, including Monte Carlo and Russian Roulette estimators, are consistent but often require a large number of samples to achieve accurate results. We propose a novel estimator, \\emph{BayesSum}, which is an extension of Bayesian quadrature to discrete domains. It is more sample efficient than alternatives due to its ability to make use of prior information about the integrand through a Gaussian process. We show this through theory, deriving a convergence rate significantly faster than Monte Carlo in a broad range of settings. We also demonstrate empirically that our proposed method does indeed require fewer samples on several synthetic settings as well as for parameter estimation for Conway-Maxwell-Poisson and Potts models."}
{"id": "2512.15802", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15802", "abs": "https://arxiv.org/abs/2512.15802", "authors": ["Bingxue An", "Tiffany M. Tang"], "title": "Consensus dimension reduction via multi-view learning", "comment": null, "summary": "A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science."}
{"id": "2512.16233", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16233", "abs": "https://arxiv.org/abs/2512.16233", "authors": ["Noriaki Sato", "Marco Scutari", "Shuichi Kawano", "Rui Yamaguchi", "Seiya Imoto"], "title": "DAG Learning from Zero-Inflated Count Data Using Continuous Optimization", "comment": null, "summary": "We address network structure learning from zero-inflated count data by casting each node as a zero-inflated generalized linear model and optimizing a smooth, score-based objective under a directed acyclic graph constraint. Our Zero-Inflated Continuous Optimization (ZICO) approach uses node-wise likelihoods with canonical links and enforces acyclicity through a differentiable surrogate constraint combined with sparsity regularization. ZICO achieves superior performance with faster runtimes on simulated data. It also performs comparably to or better than common algorithms for reverse engineering gene regulatory networks. ZICO is fully vectorized and mini-batched, enabling learning on larger variable sets with practical runtimes in a wide range of domains."}
{"id": "2512.15950", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.15950", "abs": "https://arxiv.org/abs/2512.15950", "authors": ["Gregory Camilli"], "title": "Modeling Issues with Eye Tracking Data", "comment": null, "summary": "I describe and compare procedures for binary eye-tracking (ET) data. These procedures are applied to both raw and compressed data. The basic GLMM model is a logistic mixed model combined with random effects for persons and items. Additional models address autocorrelation eye-tracking serial observations. In particular, two novel approaches are illustrated that address serial without the use of an observed lag-1 predictor: a first-order autoregressive model obtained with generalized estimating equations, and a recurrent two-state survival model. Altogether, the results of four different analyses point to unresolved issues in the analysis of eye-tracking data and new directions for analytic development."}
{"id": "2512.16012", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.16012", "abs": "https://arxiv.org/abs/2512.16012", "authors": ["JoonHo Lee"], "title": "Reliability-Targeted Simulation of Item Response Data: Solving the Inverse Design Problem", "comment": null, "summary": "Monte Carlo simulations are the primary methodology for evaluating Item Response Theory (IRT) methods, yet marginal reliability - the fundamental metric of data informativeness - is rarely treated as an explicit design factor. Unlike in multilevel modeling where the intraclass correlation (ICC) is routinely manipulated, IRT studies typically treat reliability as an incidental outcome, creating a \"reliability omission\" that obscures the signal-to-noise ratio of generated data. To address this gap, we introduce a principled framework for reliability-targeted simulation, transforming reliability from an implicit by-product into a precise input parameter. We formalize the inverse design problem, solving for a global discrimination scaling factor that uniquely achieves a pre-specified target reliability. Two complementary algorithms are proposed: Empirical Quadrature Calibration (EQC) for rapid, deterministic precision, and Stochastic Approximation Calibration (SAC) for rigorous stochastic estimation. A comprehensive validation study across 960 conditions demonstrates that EQC achieves essentially exact calibration, while SAC remains unbiased across non-normal latent distributions and empirical item pools. Furthermore, we clarify the theoretical distinction between average-information and error-variance-based reliability metrics, showing they require different calibration scales due to Jensen's inequality. An accompanying open-source R package, IRTsimrel, enables researchers to standardize reliability as a controlled experimental input."}
{"id": "2512.16254", "categories": ["stat.AP", "physics.ed-ph"], "pdf": "https://arxiv.org/pdf/2512.16254", "abs": "https://arxiv.org/abs/2512.16254", "authors": ["Mohamed Tolba", "Olivia Kendall", "Daniel Tudball Smith", "Alexander Gregg", "Tony Vo", "Scott Wordley"], "title": "An Open Workflow Model for Improving Educational Video Design: Tools, Data, and Insights", "comment": null, "summary": "Educational videos are widely used across various instructional models in higher education to support flexible and self-paced learning. However, student engagement with these videos varies significantly depending on how they are designed. While several studies have identified potential influencing factors, there remains a lack of scalable tools and open datasets to support large-scale, data-driven improvements in video design. This study aims to advance data-driven approaches to educational video design. Its core contributions include: (1) a workflow model for analysing educational videos; (2) an open-source implementation for extracting video metadata and features; (3) an accessible, community-driven database of video attributes; (4) a case study applying the approach to two engineering courses; and (5) an initial machine learning-based analysis to explore the relative influence of various video characteristics on student engagement. This work lays the groundwork for a shared, evidence-based approach to educational video design."}
{"id": "2512.16489", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16489", "abs": "https://arxiv.org/abs/2512.16489", "authors": ["Seyda Betul Aydin", "Holger Brandt"], "title": "Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning", "comment": null, "summary": "Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples."}
{"id": "2512.16012", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.16012", "abs": "https://arxiv.org/abs/2512.16012", "authors": ["JoonHo Lee"], "title": "Reliability-Targeted Simulation of Item Response Data: Solving the Inverse Design Problem", "comment": null, "summary": "Monte Carlo simulations are the primary methodology for evaluating Item Response Theory (IRT) methods, yet marginal reliability - the fundamental metric of data informativeness - is rarely treated as an explicit design factor. Unlike in multilevel modeling where the intraclass correlation (ICC) is routinely manipulated, IRT studies typically treat reliability as an incidental outcome, creating a \"reliability omission\" that obscures the signal-to-noise ratio of generated data. To address this gap, we introduce a principled framework for reliability-targeted simulation, transforming reliability from an implicit by-product into a precise input parameter. We formalize the inverse design problem, solving for a global discrimination scaling factor that uniquely achieves a pre-specified target reliability. Two complementary algorithms are proposed: Empirical Quadrature Calibration (EQC) for rapid, deterministic precision, and Stochastic Approximation Calibration (SAC) for rigorous stochastic estimation. A comprehensive validation study across 960 conditions demonstrates that EQC achieves essentially exact calibration, while SAC remains unbiased across non-normal latent distributions and empirical item pools. Furthermore, we clarify the theoretical distinction between average-information and error-variance-based reliability metrics, showing they require different calibration scales due to Jensen's inequality. An accompanying open-source R package, IRTsimrel, enables researchers to standardize reliability as a controlled experimental input."}
{"id": "2512.16481", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.16481", "abs": "https://arxiv.org/abs/2512.16481", "authors": ["Nora M. Villanueva", "Marta Sestelo", "Luis Meira-Machado"], "title": "Efficient and scalable clustering of survival curves", "comment": null, "summary": "Survival analysis encompasses a broad range of methods for analyzing time-to-event data, with one key objective being the comparison of survival curves across groups. Traditional approaches for identifying clusters of survival curves often rely on computationally intensive bootstrap techniques to approximate the null hypothesis distribution. While effective, these methods impose significant computational burdens. In this work, we propose a novel approach that leverages the k-means and log-rank test to efficiently identify and cluster survival curves. Our method eliminates the need for computationally expensive resampling, significantly reducing processing time while maintaining statistical reliability. By systematically evaluating survival curves and determining optimal clusters, the proposed method ensures a practical and scalable alternative for large-scale survival data analysis. Through simulation studies, we demonstrate that our approach achieves results comparable to existing bootstrap-based clustering methods while dramatically improving computational efficiency. These findings suggest that the log-rank-based clustering procedure offers a viable and time-efficient solution for researchers working with multiple survival curves in medical and epidemiological studies."}
{"id": "2512.16463", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16463", "abs": "https://arxiv.org/abs/2512.16463", "authors": ["Rebecca Farina", "Francois Mercier", "Christian Wohlfart", "Serge Masson", "Silvia Metelli"], "title": "Dynamic Prediction for Hospital Readmission in Patients with Chronic Heart Failure", "comment": "13 pages, 5 figures, 3 tables", "summary": "Hospital readmission among patients with chronic heart failure (HF) is a major clinical and economic burden. Dynamic prediction models that leverage longitudinal biomarkers may improve risk stratification over traditional static models. This study aims to develop and validate a joint model (JM) using longitudinal N-terminal pro-B-type natriuretic peptide (NT-proBNP) measurements to predict the risk of rehospitalization or death in HF patients. We analyzed real-world data from the TriNetX database, including patients with an incident HF diagnosis between 2016 and 2022. The final selected cohort included 1,804 patients. A Bayesian joint modeling framework was developed to link patient-specific NT-proBNP trajectories to the risk of a composite endpoint (HF rehospitalization or all-cause mortality) within a 180-day window following hospital discharge. The model's performance was evaluated using 5-fold cross-validation and assessed with the Integrated Brier Score (IBS) and Integrated Calibration Index (ICI). The joint model demonstrated a strong predictive advantage over a benchmark static model, particularly when making updated predictions at later time points (180-360 days). A joint model trained on patients with more frequent NT-proBNP measurements achieved the highest accuracy. The main joint model showed excellent calibration, suggesting its risk estimates are reliable. These findings suggest that modeling the full trajectory of NT-proBNP with a joint modeling framework enables more accurate and dynamic risk assessment compared to static, single-timepoint methods. This approach supports the development of adaptive clinical decision-support tools for personalized HF management."}
{"id": "2512.16607", "categories": ["stat.ML", "cond-mat.stat-mech", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2512.16607", "abs": "https://arxiv.org/abs/2512.16607", "authors": ["Louis Grenioux", "Leonardo Galliano", "Ludovic Berthier", "Giulio Biroli", "Marylou Gabrié"], "title": "Riemannian Stochastic Interpolants for Amorphous Particle Systems", "comment": null, "summary": "Modern generative models hold great promise for accelerating diverse tasks involving the simulation of physical systems, but they must be adapted to the specific constraints of each domain. Significant progress has been made for biomolecules and crystalline materials. Here, we address amorphous materials (glasses), which are disordered particle systems lacking atomic periodicity. Sampling equilibrium configurations of glass-forming materials is a notoriously slow and difficult task. This obstacle could be overcome by developing a generative framework capable of producing equilibrium configurations with well-defined likelihoods. In this work, we address this challenge by leveraging an equivariant Riemannian stochastic interpolation framework which combines Riemannian stochastic interpolant and equivariant flow matching. Our method rigorously incorporates periodic boundary conditions and the symmetries of multi-component particle systems, adapting an equivariant graph neural network to operate directly on the torus. Our numerical experiments on model amorphous systems demonstrate that enforcing geometric and symmetry constraints significantly improves generative performance."}
{"id": "2512.16061", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16061", "abs": "https://arxiv.org/abs/2512.16061", "authors": ["Fernando Baltazar-Larios", "Alejandra Quintos"], "title": "Maximum Likelihood Estimation for Scaled Inhomogeneous Phase-Type Distributions from Discrete Observations", "comment": null, "summary": "Inhomogeneous phase-type (IPH) distributions extend classical phase-type models by allowing transition intensities to vary over time, offering greater flexibility for modeling heavy-tailed or time-dependent absorption phenomena. We focus on the subclass of IPH distributions with time-scaled sub-intensity matrices of the form $Λ(t) = h_β(t)Λ$, which admits a time transformation to a homogeneous Markov jump process. For this class, we develop a statistical inference framework for discretely observed trajectories that combines Markov-bridge reconstruction with a stochastic EM algorithm and a gradient-based update. The resulting method yields joint maximum-likelihood estimates of both the baseline sub-intensity matrix $Λ$ and the time-scaling parameter $β$. Through simulation studies for the matrix-Gompertz and matrix-Weibull families, and a real-data application to coronary allograft vasculopathy progression, we demonstrate that the proposed approach provides an accurate and computationally tractable tool for fitting time-scaled IPH models to irregular multi-state data."}
{"id": "2512.16574", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16574", "abs": "https://arxiv.org/abs/2512.16574", "authors": ["Robyn Ritchie", "Alexandre Leblanc", "Thomas Loughin"], "title": "Opening the House: Datasets for Mixed Doubles Curling", "comment": null, "summary": "We introduce the most comprehensive publicly available datasets for mixed doubles curling, constructed from eleven top-level tournaments from the CurlIT (https://curlit.com/results) Results Booklets spanning 53 countries, 1,112 games, and nearly 70,000 recorded shots. While curling analytics has grown in recent years, mixed doubles remains under-served due to limited access to data. Using a combined text-scraping and image-processing pipeline, we extract and standardize detailed game- and shot-level information, including player statistics, hammer possession, Power Play usage, stone coordinates, and post-shot scoring states. We describe the data engineering workflow, highlight challenges in parsing historical records, and derive additional contextual features that enable rigorous strategic analysis. Using these datasets, we present initial insights into shot selection and success rates, scoring distributions, and team efficiencies, illustrating key differences between mixed doubles and traditional 4-player curling. We highlight various ways to analyze this type of data including from a shot-, end-, game- or team-level to display its versatilely. The resulting resources provide a foundation for advanced performance modeling, strategic evaluation, and future research in mixed doubles curling analytics, supporting broader analytical engagement with this rapidly growing discipline."}
{"id": "2512.16768", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.16768", "abs": "https://arxiv.org/abs/2512.16768", "authors": ["Soon Hoe Lim"], "title": "On The Hidden Biases of Flow Matching Samplers", "comment": "20 pages", "summary": "We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM."}
{"id": "2512.16231", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16231", "abs": "https://arxiv.org/abs/2512.16231", "authors": ["Luke Hagar", "Andrew J. Martin"], "title": "An Efficient Framework for Robust Sample Size Determination", "comment": null, "summary": "In many settings, robust data analysis involves computational methods for uncertainty quantification and statistical inference. To design frequentist studies that leverage robust analysis methods, suitable sample sizes to achieve desired power are often found by estimating sampling distributions of p-values via intensive simulation. Moreover, most sample size recommendations rely heavily on assumptions about a single data-generating process. Consequently, robustness in data analysis does not by itself imply robustness in study design, as examining sample size sensitivity to data-generating assumptions typically requires further simulations. We propose an economical alternative for determining sample sizes that are robust to multiple data-generating mechanisms. Applying our theoretical results that model p-values as a function of the sample size, we assess power across the sample size space using simulations conducted at only two sample sizes for each data-generating mechanism. We demonstrate the broad applicability of our methodology to study design based on M-estimators in both experimental and observational settings through a varied set of clinical examples."}
{"id": "2512.16603", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16603", "abs": "https://arxiv.org/abs/2512.16603", "authors": ["Zipei Geng", "Jordan Richards", "Raphael Huser", "Marc G. Genton"], "title": "Quantile-based causal inference for spatio-temporal processes: Assessing the impacts of wildfires on US air quality", "comment": null, "summary": "Wildfires pose an increasingly severe threat to air quality, yet quantifying their causal impact remains challenging due to unmeasured meteorological and geographic confounders. Moreover, wildfire impacts on air quality may exhibit heterogeneous effects across pollution levels, which conventional mean-based causal methods fail to capture. To address these challenges, we develop a Quantile-based Latent Spatial Confounder Model (QLSCM) that substitutes conditional expectations with conditional quantiles, enabling causal analysis across the entire outcome distribution. We establish the causal interpretation of QLSCM theoretically, prove the identifiability of causal effects, and demonstrate estimator consistency under mild conditions. Simulations confirm the bias correction capability and the advantage of quantile-based inference over mean-based approaches. Applying our method to contiguous US wildfire and air quality data, we uncover important heterogeneous effects: fire radiative power exerts significant positive causal effects on aerosol optical depth at high quantiles in Western states like California and Oregon, while insignificant at lower quantiles. This indicates that wildfire impacts on air quality primarily manifest during extreme pollution events. Regional analyses reveal that Western and Northwestern regions experience the strongest causal effects during such extremes. These findings provide critical insights for environmental policy by identifying where and when mitigation efforts would be most effective."}
{"id": "2512.15802", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15802", "abs": "https://arxiv.org/abs/2512.15802", "authors": ["Bingxue An", "Tiffany M. Tang"], "title": "Consensus dimension reduction via multi-view learning", "comment": null, "summary": "A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science."}
{"id": "2512.16239", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16239", "abs": "https://arxiv.org/abs/2512.16239", "authors": ["Bohan Wu", "Eli N. Weinstein", "David M. Blei"], "title": "Bayesian Empirical Bayes: Simultaneous Inference from Probabilistic Symmetries", "comment": null, "summary": "Empirical Bayes (EB) improves the accuracy of simultaneous inference \"by learning from the experience of others\" (Efron, 2012). Classical EB theory focuses on latent variables that are iid draws from a fitted prior (Efron, 2019). Modern applications, however, feature complex structure, like arrays, spatial processes, or covariates. How can we apply EB ideas to these settings? We propose a generalized approach to empirical Bayes based on the notion of probabilistic symmetry. Our method pairs a simultaneous inference problem-with an unknown prior-to a symmetry assumption on the joint distribution of the latent variables. Each symmetry implies an ergodic decomposition, which we use to derive a corresponding empirical Bayes method. We call this methodBayesian empirical Bayes (BEB). We show how BEB recovers the classical methods of empirical Bayes, which implicitly assume exchangeability. We then use it to extend EB to other probabilistic symmetries: (i) EB matrix recovery for arrays and graphs; (ii) covariate-assisted EB for conditional data; (iii) EB spatial regression under shift invariance. We develop scalable algorithms based on variational inference and neural networks. In simulations, BEB outperforms existing approaches to denoising arrays and spatial data. On real data, we demonstrate BEB by denoising a cancer gene-expression matrix and analyzing spatial air-quality data from New York City."}
{"id": "2512.16061", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16061", "abs": "https://arxiv.org/abs/2512.16061", "authors": ["Fernando Baltazar-Larios", "Alejandra Quintos"], "title": "Maximum Likelihood Estimation for Scaled Inhomogeneous Phase-Type Distributions from Discrete Observations", "comment": null, "summary": "Inhomogeneous phase-type (IPH) distributions extend classical phase-type models by allowing transition intensities to vary over time, offering greater flexibility for modeling heavy-tailed or time-dependent absorption phenomena. We focus on the subclass of IPH distributions with time-scaled sub-intensity matrices of the form $Λ(t) = h_β(t)Λ$, which admits a time transformation to a homogeneous Markov jump process. For this class, we develop a statistical inference framework for discretely observed trajectories that combines Markov-bridge reconstruction with a stochastic EM algorithm and a gradient-based update. The resulting method yields joint maximum-likelihood estimates of both the baseline sub-intensity matrix $Λ$ and the time-scaling parameter $β$. Through simulation studies for the matrix-Gompertz and matrix-Weibull families, and a real-data application to coronary allograft vasculopathy progression, we demonstrate that the proposed approach provides an accurate and computationally tractable tool for fitting time-scaled IPH models to irregular multi-state data."}
{"id": "2512.16481", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.16481", "abs": "https://arxiv.org/abs/2512.16481", "authors": ["Nora M. Villanueva", "Marta Sestelo", "Luis Meira-Machado"], "title": "Efficient and scalable clustering of survival curves", "comment": null, "summary": "Survival analysis encompasses a broad range of methods for analyzing time-to-event data, with one key objective being the comparison of survival curves across groups. Traditional approaches for identifying clusters of survival curves often rely on computationally intensive bootstrap techniques to approximate the null hypothesis distribution. While effective, these methods impose significant computational burdens. In this work, we propose a novel approach that leverages the k-means and log-rank test to efficiently identify and cluster survival curves. Our method eliminates the need for computationally expensive resampling, significantly reducing processing time while maintaining statistical reliability. By systematically evaluating survival curves and determining optimal clusters, the proposed method ensures a practical and scalable alternative for large-scale survival data analysis. Through simulation studies, we demonstrate that our approach achieves results comparable to existing bootstrap-based clustering methods while dramatically improving computational efficiency. These findings suggest that the log-rank-based clustering procedure offers a viable and time-efficient solution for researchers working with multiple survival curves in medical and epidemiological studies."}
{"id": "2512.16276", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16276", "abs": "https://arxiv.org/abs/2512.16276", "authors": ["Yuta Hayashida", "Shonosuke Sugasawa"], "title": "Repulsive g-Priors for Regression Mixtures", "comment": "23 pages (main) + 27 pages (supplement)", "summary": "Mixture regression models are powerful tools for capturing heterogeneous covariate-response relationships, yet classical finite mixtures and Bayesian nonparametric alternatives often suffer from instability or overestimation of clusters when component separability is weak. Recent repulsive priors improve parsimony in density mixtures by discouraging nearby components, but their direct extension to regression is nontrivial since separation must respect the predictive geometry induced by covariates. We propose a repulsive g-prior for regression mixtures that enforces separation in the Mahalanobis metric, penalizing components indistinguishable in the predictive mean space. This construction preserves conjugacy-like updates while introducing geometry-aware interactions, enabling efficient blocked-collapsed Gibbs sampling. Theoretically, we establish tractable normalizing bounds, posterior contraction rates, and shrinkage of tail mass on the number of components. Simulations under correlated and overlapping designs demonstrate improved clustering and prediction relative to independent, Euclidean-repulsive, and sparsity-inducing baselines."}
{"id": "2512.16336", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16336", "abs": "https://arxiv.org/abs/2512.16336", "authors": ["J. A. Christen", "F. J. Rubio"], "title": "Hazard-based distributional regression via ordinary differential equations", "comment": "To appear in Statistical Methods in Medical Research", "summary": "The hazard function is central to the formulation of commonly used survival regression models such as the proportional hazards and accelerated failure time models. However, these models rely on a shared baseline hazard, which, when specified parametrically, can only capture limited shapes. To overcome this limitation, we propose a general class of parametric survival regression models obtained by modelling the hazard function using autonomous systems of ordinary differential equations (ODEs). Covariate information is incorporated via transformed linear predictors on the parameters of the ODE system. Our framework capitalises on the interpretability of parameters in common ODE systems, enabling the identification of covariate values that produce qualitatively distinct hazard shapes associated with different attractors of the system of ODEs. This provides deeper insights into how covariates influence survival dynamics. We develop efficient Bayesian computational tools, including parallelised evaluation of the log-posterior, which facilitates integration with general-purpose Markov Chain Monte Carlo samplers. We also derive conditions for posterior asymptotic normality, enabling fast approximations of the posterior. A central contribution of our work lies in the case studies. We demonstrate the methodology using clinical trial data with crossing survival curves, and a study of cancer recurrence times where our approach reveals how the efficacy of interventions (treatments) on hazard and survival are influenced by patient characteristics."}
{"id": "2512.16336", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16336", "abs": "https://arxiv.org/abs/2512.16336", "authors": ["J. A. Christen", "F. J. Rubio"], "title": "Hazard-based distributional regression via ordinary differential equations", "comment": "To appear in Statistical Methods in Medical Research", "summary": "The hazard function is central to the formulation of commonly used survival regression models such as the proportional hazards and accelerated failure time models. However, these models rely on a shared baseline hazard, which, when specified parametrically, can only capture limited shapes. To overcome this limitation, we propose a general class of parametric survival regression models obtained by modelling the hazard function using autonomous systems of ordinary differential equations (ODEs). Covariate information is incorporated via transformed linear predictors on the parameters of the ODE system. Our framework capitalises on the interpretability of parameters in common ODE systems, enabling the identification of covariate values that produce qualitatively distinct hazard shapes associated with different attractors of the system of ODEs. This provides deeper insights into how covariates influence survival dynamics. We develop efficient Bayesian computational tools, including parallelised evaluation of the log-posterior, which facilitates integration with general-purpose Markov Chain Monte Carlo samplers. We also derive conditions for posterior asymptotic normality, enabling fast approximations of the posterior. A central contribution of our work lies in the case studies. We demonstrate the methodology using clinical trial data with crossing survival curves, and a study of cancer recurrence times where our approach reveals how the efficacy of interventions (treatments) on hazard and survival are influenced by patient characteristics."}
{"id": "2512.16411", "categories": ["stat.ME", "math.ST", "q-fin.ST", "q-fin.TR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16411", "abs": "https://arxiv.org/abs/2512.16411", "authors": ["Matthieu Garcin", "Louis Perot"], "title": "Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection", "comment": null, "summary": "Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices."}
{"id": "2512.16340", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16340", "abs": "https://arxiv.org/abs/2512.16340", "authors": ["Louise Linsell", "Noman Paracha", "Jamie Grossman", "Carsten Bokemeyer", "Jesus Garcia-Foncillas", "Antoine Italiano", "Gilles Vassal", "Yuxian Chen", "Barbara Torlinska", "Keith R Abrams"], "title": "Bayesian joint modelling of longitudinal biomarkers to enable extrapolation of overall survival: an application using larotrectinib trial clinical data", "comment": "24 pages, 3 figures, 4 tables", "summary": "Objectives To investigate the use of a Bayesian joint modelling approach to predict overall survival (OS) from immature clinical trial data using an intermediate biomarker. To compare the results with a typical parametric approach of extrapolation and observed survival from a later datacut.\n  Methods Data were pooled from three phase I/II open-label trials evaluating larotrectinib in 196 patients with neurotrophic tyrosine receptor kinase fusion-positive (NTRK+) solid tumours followed up until July 2021. Bayesian joint modelling was used to obtain patient-specific predictions of OS using individual-level sum of diameter of target lesions (SLD) profiles up to the time at which the patient died or was censored. Overall and tumour site-specific estimates were produced, assuming a common, exchangeable, or independent association structure across tumour sites.\n  Results The overall risk of mortality was 9% higher per 10mm increase in SLD (HR 1.09, 95% CrI 1.05 to 1.14) for all tumour sites combined. Tumour-specific point estimates of restricted mean , median and landmark survival were more similar across models for larger tumour groups, compared to smaller tumour groups. In general, parameters were estimated with more certainty compared to a standard Weibull model and were aligned with the more recent datacut.\n  Conclusions Joint modelling using intermediate outcomes such as tumour burden can offer an alternative approach to traditional survival modelling and may improve survival predictions from limited follow-up data. This approach allows complex hierarchical data structures, such as patients nested within tumour types, and can also incorporate multiple longitudinal biomarkers in a multivariate modelling framework."}
{"id": "2512.16363", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.16363", "abs": "https://arxiv.org/abs/2512.16363", "authors": ["Guanghui Wang", "Mengtao Wen", "Changliang Zou"], "title": "Empirical Likelihood Meets Prediction-Powered Inference", "comment": null, "summary": "We study inference with a small labeled sample, a large unlabeled sample, and high-quality predictions from an external model. We link prediction-powered inference with empirical likelihood by stacking supervised estimating equations based on labeled outcomes with auxiliary moment conditions built from predictions, and then optimizing empirical likelihood under these joint constraints. The resulting empirical likelihood-based prediction-powered inference (EPI) estimator is asymptotically normal, has asymptotic variance no larger than the fully supervised estimator, and attains the semiparametric efficiency bound when the auxiliary functions span the predictable component of the supervised score. For hypothesis testing and confidence sets, empirical likelihood ratio statistics admit chi-squared-type limiting distributions. As a by-product, the empirical likelihood weights induce a calibrated empirical distribution that integrates supervised and prediction-based information, enabling estimation and uncertainty quantification for general functionals beyond parameters defined by estimating equations. We present two practical implementations: one based on basis expansions in the predictions and covariates, and one that learns an approximately optimal auxiliary function by cross-fitting. In simulations and applications, EPI reduces mean squared error and shortens confidence intervals while maintaining nominal coverage."}
{"id": "2512.16411", "categories": ["stat.ME", "math.ST", "q-fin.ST", "q-fin.TR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.16411", "abs": "https://arxiv.org/abs/2512.16411", "authors": ["Matthieu Garcin", "Louis Perot"], "title": "Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection", "comment": null, "summary": "Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices."}
{"id": "2512.16481", "categories": ["stat.ME", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.16481", "abs": "https://arxiv.org/abs/2512.16481", "authors": ["Nora M. Villanueva", "Marta Sestelo", "Luis Meira-Machado"], "title": "Efficient and scalable clustering of survival curves", "comment": null, "summary": "Survival analysis encompasses a broad range of methods for analyzing time-to-event data, with one key objective being the comparison of survival curves across groups. Traditional approaches for identifying clusters of survival curves often rely on computationally intensive bootstrap techniques to approximate the null hypothesis distribution. While effective, these methods impose significant computational burdens. In this work, we propose a novel approach that leverages the k-means and log-rank test to efficiently identify and cluster survival curves. Our method eliminates the need for computationally expensive resampling, significantly reducing processing time while maintaining statistical reliability. By systematically evaluating survival curves and determining optimal clusters, the proposed method ensures a practical and scalable alternative for large-scale survival data analysis. Through simulation studies, we demonstrate that our approach achieves results comparable to existing bootstrap-based clustering methods while dramatically improving computational efficiency. These findings suggest that the log-rank-based clustering procedure offers a viable and time-efficient solution for researchers working with multiple survival curves in medical and epidemiological studies."}
{"id": "2512.16547", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16547", "abs": "https://arxiv.org/abs/2512.16547", "authors": ["William R. Nugent"], "title": "Extending a Matrix Lie Group Model of Measurement Symmetries", "comment": "28 pages; 1 figure", "summary": "Symmetry principles underlie and guide scientific theory and research, from Curie's invariance formulation to modern applications across physics, chemistry, and mathematics. Building on a recent matrix Lie group measurement model, this paper extends the framework to identify additional measurement symmetries implied by Lie group theory. Lie groups provide the mathematics of continuous symmetries, while Lie algebras serve as their infinitesimal generators. Within applied measurement theory, the preservation of symmetries in transformation groups acting on score frequency distributions ensure invariance in transformed distributions, with implications for validity, comparability, and conservation of information. A simulation study demonstrates how breaks in measurement symmetry affect score distribution symmetry and break effect size comparability. Practical applications are considered, particularly in meta analysis, where the standardized mean difference (SMD) is shown to remain invariant across measures only under specific symmetry conditions derived from the Lie group model. These results underscore symmetry as a unifying principle in measurement theory and its role in evidence based research."}
{"id": "2512.16745", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.16745", "abs": "https://arxiv.org/abs/2512.16745", "authors": ["Simon Donker van Heel", "Neil Shephard"], "title": "Exponentially weighted estimands and the exponential family: filtering, prediction and smoothing", "comment": null, "summary": "We propose using a discounted version of a convex combination of the log-likelihood with the corresponding expected log-likelihood such that when they are maximized they yield a filter, predictor and smoother for time series. This paper then focuses on working out the implications of this in the case of the canonical exponential family. The results are simple exact filters, predictors and smoothers with linear recursions. A theory for these models is developed and the models are illustrated on simulated and real data."}
{"id": "2512.16748", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16748", "abs": "https://arxiv.org/abs/2512.16748", "authors": ["Derek Long"], "title": "Shift-Aware Gaussian-Supremum Validation for Wasserstein-DRO CVaR Portfolios", "comment": "NeurIPS 2025 Workshop: Generative AI in Finance", "summary": "We study portfolio selection with a Conditional Value-at-Risk (CVaR) constraint under distribution shift and serial dependence. While Wasserstein distributionally robust optimization (DRO) offers tractable protection via an ambiguity ball around empirical data, choosing the ball radius is delicate: large radii are conservative, small radii risk violation under regime change. We propose a shift-aware Gaussian-supremum (GS) validation framework for Wasserstein-DRO CVaR portfolios, building on the work by Lam and Qian (2019). Phase I of the framework generates a candidate path by solving the exact reformulation of the robust CVaR constraint over a grid of Wasserstein radii. Phase II of the framework learns a target deployment law $Q$ by density-ratio reweighting of a time-ordered validation fold, computes weighted CVaR estimates, and calibrates a simultaneous upper confidence band via a block multiplier bootstrap to account for dependence. We select the least conservative feasible portfolio (or abstain if the effective sample size collapses). Theoretically, we extend the normalized GS validator to non-i.i.d. financial data: under weak dependence and regularity of the weighted scores, any portfolio passing our validator satisfies the CVaR limit under $Q$ with probability at least $1-β$; the Wasserstein term contributes a deterministic margin $(δ/α)\\|x\\|_*$. Empirical results indicate improved return-risk trade-offs versus the naive baseline."}
{"id": "2512.16833", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16833", "abs": "https://arxiv.org/abs/2512.16833", "authors": ["Xiaokang Liu", "Rui Duan", "Raymond J. Carroll", "Yang Ning", "Yong Chen"], "title": "Distributed inference for heterogeneous mixture models using multi-site data", "comment": "70 pages, 5 figures", "summary": "Mixture models postulate the overall population as a mixture of finite subpopulations with unobserved membership. Fitting mixture models usually requires large sample sizes and combining data from multiple sites can be beneficial. However, sharing individual participant data across sites is often less feasible due to various types of practical constraints, such as data privacy concerns. Moreover, substantial heterogeneity may exist across sites, and locally identified latent classes may not be comparable across sites. We propose a unified modeling framework where a common definition of the latent classes is shared across sites and heterogeneous mixing proportions of latent classes are allowed to account for between-site heterogeneity. To fit the heterogeneous mixture model on multi-site data, we propose a novel distributed Expectation-Maximization (EM) algorithm where at each iteration a density ratio tilted surrogate Q function is constructed to approximate the standard Q function of the EM algorithm as if the data from multiple sites could be pooled together. Theoretical analysis shows that our estimator achieves the same contraction property as the estimators derived from the EM algorithm based on the pooled data."}
{"id": "2512.16857", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16857", "abs": "https://arxiv.org/abs/2512.16857", "authors": ["Chao Cheng", "Georgia Papadogeorgou", "Fan Li"], "title": "Identification and efficient estimation of compliance and network causal effects in cluster-randomized trials", "comment": null, "summary": "Treatment noncompliance is pervasive in infectious disease cluster-randomized trials. Although all individuals within a cluster are assigned the same treatment condition, the treatment uptake status may vary across individuals due to noncompliance. We propose a semiparametric framework to evaluate the individual compliance effect and network assignment effect within principal stratum exhibiting different patterns of noncompliance. The individual compliance effect captures the portion of the treatment effect attributable to changes in treatment receipt, while the network assignment effect reflects the pure impact of treatment assignment and spillover among individuals within the same cluster. Unlike prior efforts which either empirically identify or interval identify these estimands, we characterize new structural assumptions for nonparametric point identification. We then develop semiparametrically efficient estimators that combine data-adaptive machine learning methods with efficient influence functions to enable more robust inference. Additionally, we introduce sensitivity analysis methods to study the impact under assumption violations, and apply the proposed methods to reanalyze a cluster-randomized trial in Kenya that evaluated the impact of school-based mass deworming on disease transmission."}
{"id": "2512.16463", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.16463", "abs": "https://arxiv.org/abs/2512.16463", "authors": ["Rebecca Farina", "Francois Mercier", "Christian Wohlfart", "Serge Masson", "Silvia Metelli"], "title": "Dynamic Prediction for Hospital Readmission in Patients with Chronic Heart Failure", "comment": "13 pages, 5 figures, 3 tables", "summary": "Hospital readmission among patients with chronic heart failure (HF) is a major clinical and economic burden. Dynamic prediction models that leverage longitudinal biomarkers may improve risk stratification over traditional static models. This study aims to develop and validate a joint model (JM) using longitudinal N-terminal pro-B-type natriuretic peptide (NT-proBNP) measurements to predict the risk of rehospitalization or death in HF patients. We analyzed real-world data from the TriNetX database, including patients with an incident HF diagnosis between 2016 and 2022. The final selected cohort included 1,804 patients. A Bayesian joint modeling framework was developed to link patient-specific NT-proBNP trajectories to the risk of a composite endpoint (HF rehospitalization or all-cause mortality) within a 180-day window following hospital discharge. The model's performance was evaluated using 5-fold cross-validation and assessed with the Integrated Brier Score (IBS) and Integrated Calibration Index (ICI). The joint model demonstrated a strong predictive advantage over a benchmark static model, particularly when making updated predictions at later time points (180-360 days). A joint model trained on patients with more frequent NT-proBNP measurements achieved the highest accuracy. The main joint model showed excellent calibration, suggesting its risk estimates are reliable. These findings suggest that modeling the full trajectory of NT-proBNP with a joint modeling framework enables more accurate and dynamic risk assessment compared to static, single-timepoint methods. This approach supports the development of adaptive clinical decision-support tools for personalized HF management."}
