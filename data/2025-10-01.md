<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 15]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 16]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [One-shot Conditional Sampling: MMD meets Nearest Neighbors](https://arxiv.org/abs/2509.25507)
*Anirban Chatterjee,Sayantan Choudhury,Rohan Hore*

Main category: stat.ML

TL;DR: 提出了CGMMD框架，用于从未完全观测的条件分布中生成样本，通过直接最小化问题实现单次前向生成，在理论和实验上都表现出色


<details>
  <summary>Details</summary>
Motivation: 解决从未完全观测的条件分布中生成样本的问题，这在图像后处理、模拟推理中的近似后验采样等应用中很常见，可以利用额外特征信息实现更自适应和高效的采样

Method: 基于MMD（最大均值差异）的条件生成器框架，将训练目标构建为简单的无对抗直接最小化问题，能够通过生成器的单次前向传递产生条件样本

Result: 建立了从CGMMD采样器采样的损失理论界限，证明了估计分布向真实条件分布的收敛性，在复杂条件密度的合成任务以及图像去噪、超分辨率等实际应用中表现优异

Conclusion: CGMMD提供了一种有效且理论保证的条件采样方法，具有低测试时间复杂度和竞争性性能

Abstract: How can we generate samples from a conditional distribution that we never
fully observe? This question arises across a broad range of applications in
both modern machine learning and classical statistics, including image
post-processing in computer vision, approximate posterior sampling in
simulation-based inference, and conditional distribution modeling in complex
data settings. In such settings, compared with unconditional sampling,
additional feature information can be leveraged to enable more adaptive and
efficient sampling. Building on this, we introduce Conditional Generator using
MMD (CGMMD), a novel framework for conditional sampling. Unlike many
contemporary approaches, our method frames the training objective as a simple,
adversary-free direct minimization problem. A key feature of CGMMD is its
ability to produce conditional samples in a single forward pass of the
generator, enabling practical one-shot sampling with low test-time complexity.
We establish rigorous theoretical bounds on the loss incurred when sampling
from the CGMMD sampler, and prove convergence of the estimated distribution to
the true conditional distribution. In the process, we also develop a uniform
concentration result for nearest-neighbor based functionals, which may be of
independent interest. Finally, we show that CGMMD performs competitively on
synthetic tasks involving complex conditional densities, as well as on
practical applications such as image denoising and image super-resolution.

</details>


### [2] [Neural Optimal Transport Meets Multivariate Conformal Prediction](https://arxiv.org/abs/2509.25444)
*Vladimir Kondratyev,Alexander Fishkov,Nikita Kotelevskii,Mahmoud Hegazy,Remi Flamary,Maxim Panov,Eric Moulines*

Main category: stat.ML

TL;DR: 提出了条件向量分位数回归框架，结合神经最优传输和摊销优化，用于多元保形预测，能适应条件分布的几何结构，产生更紧致的预测区域。


<details>
  <summary>Details</summary>
Motivation: 经典分位数回归无法自然扩展到多元响应，现有方法常忽略联合分布的几何结构，需要开发能处理多元响应并保持分布几何特性的方法。

Method: 将条件向量分位数函数参数化为输入凸神经网络的凸势能梯度，确保单调性和均匀秩；引入对偶势能的摊销优化以降低高维变分问题求解成本。

Result: 在基准数据集上实验显示，相比基线方法，该方法在覆盖效率权衡方面有改进，产生更紧致和信息量更大的预测区域。

Conclusion: 将神经最优传输与保形预测相结合具有显著优势，能够构建具有有限样本有效性的无分布预测区域，且适应条件分布的几何特性。

Abstract: We propose a framework for conditional vector quantile regression (CVQR) that
combines neural optimal transport with amortized optimization, and apply it to
multivariate conformal prediction. Classical quantile regression does not
extend naturally to multivariate responses, while existing approaches often
ignore the geometry of joint distributions. Our method parametrizes the
conditional vector quantile function as the gradient of a convex potential
implemented by an input-convex neural network, ensuring monotonicity and
uniform ranks. To reduce the cost of solving high-dimensional variational
problems, we introduced amortized optimization of the dual potentials, yielding
efficient training and faster inference. We then exploit the induced
multivariate ranks for conformal prediction, constructing distribution-free
predictive regions with finite-sample validity. Unlike coordinatewise methods,
our approach adapts to the geometry of the conditional distribution, producing
tighter and more informative regions. Experiments on benchmark datasets show
improved coverage-efficiency trade-offs compared to baselines, highlighting the
benefits of integrating neural optimal transport with conformal prediction.

</details>


### [3] [Spectral gap of Metropolis-within-Gibbs under log-concavity](https://arxiv.org/abs/2509.26175)
*Cecilia Secchi,Giacomo Zanella*

Main category: stat.ML

TL;DR: 本文研究了Metropolis-within-Gibbs算法与随机游走Metropolis更新结合的性能，在目标分布为对数凹分布时，建立了比先前更优的谱间隙下界，表明该算法混合速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究Metropolis-within-Gibbs算法在高维分布采样中的性能，特别是当精确条件采样不可行时，探索方差自适应提议如何改善混合性能。

Method: 使用随机游走Metropolis更新，并调整提议方差以匹配目标条件方差，分析随机扫描版本的MwG算法在对数凹分布上的性能。

Result: 建立了谱间隙下界为O(1/κd)，比先前的O(1/κ²d)有显著改进，表明MwG混合速度更快，且与精确Gibbs采样器相比仅差常数因子。

Conclusion: 方差自适应提议能显著改善Metropolis-within-Gibbs算法的混合性能，为先前观察到的经验行为提供了理论支持，证明其与精确Gibbs采样器性能相近。

Abstract: The Metropolis-within-Gibbs (MwG) algorithm is a widely used Markov Chain
Monte Carlo method for sampling from high-dimensional distributions when exact
conditional sampling is intractable. We study MwG with Random Walk Metropolis
(RWM) updates, using proposal variances tuned to match the target's conditional
variances. Assuming the target $\pi$ is a $d$-dimensional log-concave
distribution with condition number $\kappa$, we establish a spectral gap lower
bound of order $\mathcal{O}(1/\kappa d)$ for the random-scan version of MwG,
improving on the previously available $\mathcal{O}(1/\kappa^2 d)$ bound. This
is obtained by developing sharp estimates of the conductance of one-dimensional
RWM kernels, which can be of independent interest. The result shows that MwG
can mix substantially faster with variance-adaptive proposals and that its
mixing performance is just a constant factor worse than that of the exact Gibbs
sampler, thus providing theoretical support to previously observed empirical
behavior.

</details>


### [4] [Fair Classification by Direct Intervention on Operating Characteristics](https://arxiv.org/abs/2509.25481)
*Kevin Jiang,Edgar Dobriban*

Main category: stat.ML

TL;DR: 提出一种基于预训练基分类器ROC凸包的新方法，通过识别最优操作特性并后处理基分类器来满足群体公平性约束（如DP、EO、PP），在标准数据集上以少量干预实现近似公平约束。


<details>
  <summary>Details</summary>
Motivation: 解决多群体公平约束下的二元分类问题，需要在满足多种公平性指标（如人口统计学均等、均等化几率、预测均等）的同时保持分类准确性。

Method: 基于预训练基分类器的组间ROC凸包识别最优操作特性，然后通过随机化组间阈值规则的后处理来匹配目标特性，最小化预期干预次数。

Result: 在COMPAS和ACSIncome数据集上，该方法能同时满足近似DP、EO和PP约束，干预次数少且准确性下降接近最优水平，优于先前方法。

Conclusion: 该方法为处理多保护属性和多线性分数约束的群体公平分类问题提供了有效解决方案，在公平性和准确性之间取得了良好平衡。

Abstract: We develop new classifiers under group fairness in the attribute-aware
setting for binary classification with multiple group fairness constraints
(e.g., demographic parity (DP), equalized odds (EO), and predictive parity
(PP)). We propose a novel approach, applicable to linear fractional
constraints, based on directly intervening on the operating characteristics of
a pre-trained base classifier, by (i) identifying optimal operating
characteristics using the base classifier's group-wise ROC convex hulls and
(ii) post-processing the base classifier to match those targets. As practical
post-processors, we consider randomizing a mixture of group-wise thresholding
rules subject to minimizing the expected number of interventions. We further
extend our approach to handle multiple protected attributes and multiple linear
fractional constraints. On standard datasets (COMPAS and ACSIncome), our
methods simultaneously satisfy approximate DP, EO, and PP with few
interventions and a near-oracle drop in accuracy; comparing favorably to
previous methods.

</details>


### [5] [Conservative Decisions with Risk Scores](https://arxiv.org/abs/2509.25588)
*Yishu Wei,Wen-Yee Lee,George Ekow Quaye,Xiaogang Su*

Main category: stat.ML

TL;DR: 提出一种新的二元分类方法，通过确定风险分数的最优截断区间来实现保守决策，在该区间内算法拒绝分类，在区间外最大化分类准确率。


<details>
  <summary>Details</summary>
Motivation: 在二元分类应用中，允许拒绝分类的保守决策具有优势，需要一种方法来优化风险分数的截断区间以实现这种保守决策。

Method: 受支持向量机启发但不同，该方法最小化分类边界而非最大化，提供理论最优解，支持保守决策并生成风险覆盖曲线。

Result: 该方法不仅支持保守决策，还自然地产生风险覆盖曲线，结合AUC可作为评估分类器性能的综合指标，类似于ROC曲线。

Conclusion: 通过模拟研究和前列腺癌诊断的实际案例验证了该方法的有效性，为二元分类中的保守决策提供了理论支持和实用工具。

Abstract: In binary classification applications, conservative decision-making that
allows for abstention can be advantageous. To this end, we introduce a novel
approach that determines the optimal cutoff interval for risk scores, which can
be directly available or derived from fitted models. Within this interval, the
algorithm refrains from making decisions, while outside the interval,
classification accuracy is maximized. Our approach is inspired by support
vector machines (SVM), but differs in that it minimizes the classification
margin rather than maximizing it. We provide the theoretical optimal solution
to this problem, which holds important practical implications. Our proposed
method not only supports conservative decision-making but also inherently
results in a risk-coverage curve. Together with the area under the curve (AUC),
this curve can serve as a comprehensive performance metric for evaluating and
comparing classifiers, akin to the receiver operating characteristic (ROC)
curve. To investigate and illustrate our approach, we conduct both simulation
studies and a real-world case study in the context of diagnosing prostate
cancer.

</details>


### [6] [Coupling Generative Modeling and an Autoencoder with the Causal Bridge](https://arxiv.org/abs/2509.25599)
*Ruolin Meng,Ming-Yu Chung,Dhanajit Brahma,Ricardo Henao,Lawrence Carin*

Main category: stat.ML

TL;DR: 本文提出了一种在存在未观测混杂因素的情况下估计因果效应的方法，通过因果桥函数结合自编码器架构，利用代理测量来估计处理效应。


<details>
  <summary>Details</summary>
Motivation: 当存在同时影响处理和结果的未观测混杂因素时，传统的因果推断方法面临挑战。本文旨在通过代理测量来解决这个问题。

Method: 使用因果桥函数结合自编码器架构，在两个独立的代理测量集上共享统计强度，从而提高因果桥估计的质量。

Result: 在合成和真实世界数据上的实验表明，该方法在代理测量方面优于现有最先进方法。

Conclusion: 提出的因果桥与自编码器结合的方法能有效处理未观测混杂因素，为因果推断提供了新的理论视角和实践工具。

Abstract: We consider inferring the causal effect of a treatment (intervention) on an
outcome of interest in situations where there is potentially an unobserved
confounder influencing both the treatment and the outcome. This is achievable
by assuming access to two separate sets of control (proxy) measurements
associated with treatment and outcomes, which are used to estimate treatment
effects through a function termed the em causal bridge (CB). We present a new
theoretical perspective, associated assumptions for when estimating treatment
effects with the CB is feasible, and a bound on the average error of the
treatment effect when the CB assumptions are violated. From this new
perspective, we then demonstrate how coupling the CB with an autoencoder
architecture allows for the sharing of statistical strength between observed
quantities (proxies, treatment, and outcomes), thus improving the quality of
the CB estimates. Experiments on synthetic and real-world data demonstrate the
effectiveness of the proposed approach in relation to the state-of-the-art
methodology for proxy measurements.

</details>


### [7] [When Langevin Monte Carlo Meets Randomization: Non-asymptotic Error Bounds beyond Log-Concavity and Gradient Lipschitzness](https://arxiv.org/abs/2509.25630)
*Xiaojie Wang,Bin Yang*

Main category: stat.ML

TL;DR: 本文重新审视了随机Langevin Monte Carlo (RLMC)算法，用于从非对数凹的高维分布中采样。在梯度Lipschitz条件和log-Sobolev不等式下，证明了RLMC在Wasserstein-2距离上的均匀时间误差界为O(√d h)。对于非全局Lipschitz梯度情况，提出了改进的RLMC算法并建立了非渐近误差界。


<details>
  <summary>Details</summary>
Motivation: 从复杂高维目标分布中高效采样是科学计算、统计学和机器学习中的基本任务。现有方法通常需要对数凹性假设，本文旨在研究在非对数凹条件下的采样算法。

Method: 使用随机Langevin Monte Carlo (RLMC)算法，在梯度Lipschitz条件和log-Sobolev不等式假设下进行分析。对于非全局Lipschitz梯度情况，提出了改进的RLMC算法。

Result: 证明了RLMC在Wasserstein-2距离上的均匀时间误差界为O(√d h)，这与文献中在对数凹条件下的最佳结果相匹配。对于非全局Lipschitz梯度情况，建立了改进RLMC算法的非渐近误差界。

Conclusion: RLMC算法在非对数凹条件下仍能达到与对数凹条件下相当的采样精度。提出的改进RLMC算法在非全局Lipschitz梯度设置下具有新颖性和理论保证。

Abstract: Efficient sampling from complex and high dimensional target distributions
turns out to be a fundamental task in diverse disciplines such as scientific
computing, statistics and machine learning. In this paper, we revisit the
randomized Langevin Monte Carlo (RLMC) for sampling from high dimensional
distributions without log-concavity. Under the gradient Lipschitz condition and
the log-Sobolev inequality, we prove a uniform-in-time error bound in
$\mathcal{W}_2$-distance of order $O(\sqrt{d}h)$ for the RLMC sampling
algorithm, which matches the best one in the literature under the log-concavity
condition. Moreover, when the gradient of the potential $U$ is non-globally
Lipschitz with superlinear growth, modified RLMC algorithms are proposed and
analyzed, with non-asymptotic error bounds established. To the best of our
knowledge, the modified RLMC algorithms and their non-asymptotic error bounds
are new in the non-globally Lipschitz setting.

</details>


### [8] [Test time training enhances in-context learning of nonlinear functions](https://arxiv.org/abs/2509.25741)
*Kento Kuwataka,Taiji Suzuki*

Main category: stat.ML

TL;DR: 本文研究了测试时训练与上下文学习的结合，在单层transformer上建立了预测风险的上界，证明了TTT能够适应特征向量和链接函数的变化。


<details>
  <summary>Details</summary>
Motivation: 测试时训练在实践中表现出色但理论支撑有限，特别是在非线性模型中。本文旨在为TTT与ICL结合提供理论分析，揭示其适应能力。

Method: 在单索引模型框架下，分析单层transformer使用梯度算法进行测试时训练的性能，建立预测风险的上界。

Result: 理论证明TTT使模型能够适应任务间的特征向量和链接函数变化，而单独使用ICL难以适应链接函数的变化。随着上下文大小和网络宽度增加，预测误差可以逼近噪声水平。

Conclusion: 测试时训练与上下文学习结合为模型提供了强大的适应能力，能够有效处理任务间的分布变化，在理论上优于单独使用上下文学习。

Abstract: Test-time training (TTT) enhances model performance by explicitly updating
designated parameters prior to each prediction to adapt to the test data. While
TTT has demonstrated considerable empirical success, its theoretical
underpinnings remain limited, particularly for nonlinear models. In this paper,
we investigate the combination of TTT with in-context learning (ICL), where the
model is given a few examples from the target distribution at inference time.
We analyze this framework in the setting of single-index models
$y=\sigma_*(\langle \beta, \mathbf{x} \rangle)$, where the feature vector
$\beta$ is drawn from a hidden low-dimensional subspace. For single-layer
transformers trained with gradient-based algorithms and adopting TTT, we
establish an upper bound on the prediction risk. Our theory reveals that TTT
enables the single-layer transformers to adapt to both the feature vector
$\beta$ and the link function $\sigma_*$, which vary across tasks. This creates
a sharp contrast with ICL alone, which is theoretically difficult to adapt to
shifts in the link function. Moreover, we provide the convergence rate with
respect to the data length, showing the predictive error can be driven
arbitrarily close to the noise level as the context size and the network width
grow.

</details>


### [9] [Sharpness of Minima in Deep Matrix Factorization: Exact Expressions](https://arxiv.org/abs/2509.25783)
*Anil Kamber,Rahul Parhi*

Main category: stat.ML

TL;DR: 本文首次给出了过参数化深度矩阵分解（即深度线性神经网络训练）中平方误差损失Hessian矩阵最大特征值的精确表达式，解决了Mulayoff & Michaeli (2020)提出的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 理解损失函数在最小值附近的几何结构对于解释梯度方法在非凸优化问题中的隐式偏差至关重要，而Hessian矩阵的最大特征值是表征这种几何结构的关键量度。目前由于缺乏一般设置下该锐度度量的精确表达式，其确切作用被模糊化。

Method: 提出了过参数化深度矩阵分解问题中平方误差损失Hessian矩阵最大特征值的精确表达式，并通过经验研究验证了基于该精确表达式的梯度训练过程中的逃逸现象。

Result: 获得了深度矩阵分解问题中损失函数Hessian矩阵最大特征值的首个精确表达式，为理解损失函数几何结构提供了理论基础。

Conclusion: 这项工作解决了深度矩阵分解中锐度度量的精确表达问题，为理解梯度方法在非凸优化中的行为提供了重要理论支撑，并揭示了训练过程中的逃逸现象。

Abstract: Understanding the geometry of the loss landscape near a minimum is key to
explaining the implicit bias of gradient-based methods in non-convex
optimization problems such as deep neural network training and deep matrix
factorization. A central quantity to characterize this geometry is the maximum
eigenvalue of the Hessian of the loss, which measures the sharpness of the
landscape. Currently, its precise role has been obfuscated because no exact
expressions for this sharpness measure were known in general settings. In this
paper, we present the first exact expression for the maximum eigenvalue of the
Hessian of the squared-error loss at any minimizer in general overparameterized
deep matrix factorization (i.e., deep linear neural network training) problems,
resolving an open question posed by Mulayoff & Michaeli (2020). To complement
our theory, we empirically investigate an escape phenomenon observed during
gradient-based training near a minimum that crucially relies on our exact
expression of the sharpness.

</details>


### [10] [Graph Distribution-valued Signals: A Wasserstein Space Perspective](https://arxiv.org/abs/2509.25802)
*Yanan Zhao,Feng Ji,Xingchao Jian,Wee Peng Tay*

Main category: stat.ML

TL;DR: 提出了一种新的图信号处理框架，将信号建模为Wasserstein空间中的图分布值信号，克服了传统向量基GSP的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统向量基图信号处理存在同步观测假设、无法捕捉不确定性以及图滤波严格对应要求等关键限制。

Method: 将信号表示为概率分布，在Wasserstein空间中建立图分布值信号框架，系统地将核心GSP概念映射到GDS对应物。

Result: 该框架能自然编码不确定性和随机性，严格泛化传统图信号，并通过图滤波学习预测任务验证了有效性。

Conclusion: 提出的GDS框架为图信号处理提供了更灵活和强大的理论基础，能够更好地处理现实世界中的不确定图信号。

Abstract: We introduce a novel framework for graph signal processing (GSP) that models
signals as graph distribution-valued signals (GDSs), which are probability
distributions in the Wasserstein space. This approach overcomes key limitations
of classical vector-based GSP, including the assumption of synchronous
observations over vertices, the inability to capture uncertainty, and the
requirement for strict correspondence in graph filtering. By representing
signals as distributions, GDSs naturally encode uncertainty and stochasticity,
while strictly generalizing traditional graph signals. We establish a
systematic dictionary mapping core GSP concepts to their GDS counterparts,
demonstrating that classical definitions are recovered as special cases. The
effectiveness of the framework is validated through graph filter learning for
prediction tasks, supported by experimental results.

</details>


### [11] [BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields](https://arxiv.org/abs/2509.26005)
*Rui-Yang Zhang,Henry B. Moss,Lachlan Astfalck,Edward Cripps,David S. Leslie*

Main category: stat.ML

TL;DR: 提出了一种名为BALLAST的贝叶斯主动学习方法，用于指导拉格朗日观测器的放置，以推断时变矢量场，在合成和高保真海洋流模型中显示出明显优势。


<details>
  <summary>Details</summary>
Motivation: 现有观测器放置方法大多采用标准'空间填充'设计或相对临时的专家意见，缺乏系统性。拉格朗日观测器在矢量场中连续移动，在不同位置和时间进行测量，因此需要考虑观测器未来轨迹来评估候选放置位置的效用。

Method: 使用物理信息时空高斯过程代理模型，结合贝叶斯主动学习和前瞻修正策略（BALLAST），考虑观测器可能未来轨迹来指导顺序放置策略。

Result: 在合成和高保真海洋流模型中，BALLAST辅助的顺序观测器放置策略显示出明显优势。

Conclusion: BALLAST方法为海洋学、海洋科学和海洋工程中的拉格朗日观测器放置提供了系统性的主动学习框架，显著优于传统方法。

Abstract: We introduce a formal active learning methodology for guiding the placement
of Lagrangian observers to infer time-dependent vector fields -- a key task in
oceanography, marine science, and ocean engineering -- using a physics-informed
spatio-temporal Gaussian process surrogate model. The majority of existing
placement campaigns either follow standard `space-filling' designs or
relatively ad-hoc expert opinions. A key challenge to applying principled
active learning in this setting is that Lagrangian observers are continuously
advected through the vector field, so they make measurements at different
locations and times. It is, therefore, important to consider the likely future
trajectories of placed observers to account for the utility of candidate
placement locations. To this end, we present BALLAST: Bayesian Active Learning
with Look-ahead Amendment for Sea-drifter Trajectories. We observe noticeable
benefits of BALLAST-aided sequential observer placement strategies on both
synthetic and high-fidelity ocean current models.

</details>


### [12] [Non-Vacuous Generalization Bounds: Can Rescaling Invariances Help?](https://arxiv.org/abs/2509.26149)
*Damien Rouchouse,Antoine Gonon,Rémi Gribonval,Benjamin Guedj*

Main category: stat.ML

TL;DR: 提出一种不变性提升表示方法来解决ReLU网络中权重分布缩放不变性导致的PAC-Bayes复杂度不一致问题


<details>
  <summary>Details</summary>
Motivation: ReLU网络的缩放不变性导致不同权重分布表示相同函数时PAC-Bayes复杂度差异巨大，需要解决这种不一致性

Method: 使用不变性提升表示研究PAC-Bayes边界，通过KL散度实现缩放不变的PAC-Bayes边界算法

Result: 该方法提供了不变性保证，并通过数据处理获得更紧的边界

Conclusion: 不变性提升表示方法有效解决了PAC-Bayes边界在ReLU网络中的缩放不变性问题

Abstract: A central challenge in understanding generalization is to obtain non-vacuous
guarantees that go beyond worst-case complexity over data or weight space.
Among existing approaches, PAC-Bayes bounds stand out as they can provide
tight, data-dependent guarantees even for large networks. However, in ReLU
networks, rescaling invariances mean that different weight distributions can
represent the same function while leading to arbitrarily different PAC-Bayes
complexities. We propose to study PAC-Bayes bounds in an invariant, lifted
representation that resolves this discrepancy. This paper explores both the
guarantees provided by this approach (invariance, tighter bounds via data
processing) and the algorithmic aspects of KL-based rescaling-invariant
PAC-Bayes bounds.

</details>


### [13] [An Orthogonal Learner for Individualized Outcomes in Markov Decision Processes](https://arxiv.org/abs/2509.26429)
*Emil Javurek,Valentyn Melnychuk,Jonas Schweisthal,Konstantin Hess,Dennis Frauen,Stefan Feuerriegel*

Main category: stat.ML

TL;DR: 提出了DRQ-learner，一种用于序列决策中个体化潜在结果预测的新元学习方法，具有双重稳健性、Neyman正交性和准oracle效率等理论保证。


<details>
  <summary>Details</summary>
Motivation: 在个性化医疗等序列决策场景中，预测长期潜在结果具有挑战性，现有方法缺乏正交性和准oracle效率等强理论保证。

Method: 通过因果推断视角重新审视序列决策中的潜在结果预测问题，开发了具有理论基础的元学习方法，提出了DRQ-learner。

Result: DRQ-learner在数值实验中优于现有最先进基线方法，验证了其理论优势。

Conclusion: DRQ-learner为序列决策中的个体化潜在结果预测提供了理论上有保证且实用的解决方案，适用于离散和连续状态空间。

Abstract: Predicting individualized potential outcomes in sequential decision-making is
central for optimizing therapeutic decisions in personalized medicine (e.g.,
which dosing sequence to give to a cancer patient). However, predicting
potential outcomes over long horizons is notoriously difficult. Existing
methods that break the curse of the horizon typically lack strong theoretical
guarantees such as orthogonality and quasi-oracle efficiency. In this paper, we
revisit the problem of predicting individualized potential outcomes in
sequential decision-making (i.e., estimating Q-functions in Markov decision
processes with observational data) through a causal inference lens. In
particular, we develop a comprehensive theoretical foundation for meta-learners
in this setting with a focus on beneficial theoretical properties. As a result,
we yield a novel meta-learner called DRQ-learner and establish that it is: (1)
doubly robust (i.e., valid inference under the misspecification of one of the
nuisances), (2) Neyman-orthogonal (i.e., insensitive to first-order estimation
errors in the nuisance functions), and (3) achieves quasi-oracle efficiency
(i.e., behaves asymptotically as if the ground-truth nuisance functions were
known). Our DRQ-learner is applicable to settings with both discrete and
continuous state spaces. Further, our DRQ-learner is flexible and can be used
together with arbitrary machine learning models (e.g., neural networks). We
validate our theoretical results through numerical experiments, thereby showing
that our meta-learner outperforms state-of-the-art baselines.

</details>


### [14] [Pretrain-Test Task Alignment Governs Generalization in In-Context Learning](https://arxiv.org/abs/2509.26551)
*Mary I. Letey,Jacob A. Zavatone-Veth,Yue M. Lu,Cengiz Pehlevan*

Main category: stat.ML

TL;DR: 该论文研究了Transformer模型上下文学习(ICL)中预训练任务结构如何影响泛化性能，提出了一个任务对齐度量来预测ICL表现，并揭示了专业化与泛化之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 理解ICL能力出现的数据结构及其鲁棒性机制，特别是预训练任务结构如何控制ICL的泛化性能。

Method: 使用线性注意力机制的可解模型来研究线性回归的ICL，在高维条件下推导出任意预训练-测试任务协方差不匹配时的ICL泛化误差精确表达式。

Result: 提出了一个新的对齐度量来量化预训练任务分布对测试时推理的有用信息，该度量不仅能预测可解模型中的ICL性能，也能预测非线性Transformer中的表现。分析还揭示了ICL中专业化与泛化之间的权衡：根据任务分布对齐程度，增加预训练任务多样性可能改善或损害测试性能。

Conclusion: 训练-测试任务对齐是ICL泛化的关键决定因素。

Abstract: In-context learning (ICL) is a central capability of Transformer models, but
the structures in data that enable its emergence and govern its robustness
remain poorly understood. In this work, we study how the structure of
pretraining tasks governs generalization in ICL. Using a solvable model for ICL
of linear regression by linear attention, we derive an exact expression for ICL
generalization error in high dimensions under arbitrary pretraining-testing
task covariance mismatch. This leads to a new alignment measure that quantifies
how much information about the pretraining task distribution is useful for
inference at test time. We show that this measure directly predicts ICL
performance not only in the solvable model but also in nonlinear Transformers.
Our analysis further reveals a tradeoff between specialization and
generalization in ICL: depending on task distribution alignment, increasing
pretraining task diversity can either improve or harm test performance.
Together, these results identify train-test task alignment as a key determinant
of generalization in ICL.

</details>


### [15] [Estimating Dimensionality of Neural Representations from Finite Samples](https://arxiv.org/abs/2509.26560)
*Chanwoo Chun,Abdulkadir Canatar,SueYeon Chung,Daniel Lee*

Main category: stat.ML

TL;DR: 提出了一种针对神经表示流形全局维度测量的偏差校正估计器，解决了现有方法对样本数量敏感的问题，并在合成数据和真实神经数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有神经表示流形全局维度测量方法（特别是特征值参与比）对样本数量高度敏感，在小样本情况下存在严重偏差，需要开发更准确的估计方法。

Method: 开发了偏差校正的全局维度估计器，能够更准确地估计有限样本和噪声条件下的真实维度，并可扩展到测量弯曲神经流形的局部维度。

Result: 在合成数据上能够恢复已知的真实维度；在多种神经数据（钙成像、电生理记录、fMRI）和大型语言模型神经激活上验证了估计器对样本大小的不变性。

Conclusion: 提出的偏差校正估计器为神经表示流形的维度分析提供了更可靠的工具，能够准确测量全局和局部维度，对理解人工和生物神经网络的计算过程具有重要意义。

Abstract: The global dimensionality of a neural representation manifold provides rich
insight into the computational process underlying both artificial and
biological neural networks. However, all existing measures of global
dimensionality are sensitive to the number of samples, i.e., the number of rows
and columns of the sample matrix. We show that, in particular, the
participation ratio of eigenvalues, a popular measure of global dimensionality,
is highly biased with small sample sizes, and propose a bias-corrected
estimator that is more accurate with finite samples and with noise. On
synthetic data examples, we demonstrate that our estimator can recover the true
known dimensionality. We apply our estimator to neural brain recordings,
including calcium imaging, electrophysiological recordings, and fMRI data, and
to the neural activations in a large language model and show our estimator is
invariant to the sample size. Finally, our estimators can additionally be used
to measure the local dimensionalities of curved neural manifolds by weighting
the finite samples appropriately.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [16] [Chinese vs. World Bank Development Projects: Insights from Earth Observation and Computer Vision on Wealth Gains in Africa, 2002-2013](https://arxiv.org/abs/2509.25648)
*Adel Daoud,Cindy Conlin,Connor T. Jerzak*

Main category: stat.AP

TL;DR: 该研究使用卫星图像和机器学习方法评估中国和世界银行在非洲发展项目的影响，发现之前的评估可能高估了项目效益，中国项目带来的财富增长更大但选址更受政治因素影响。


<details>
  <summary>Details</summary>
Motivation: 解决发展项目评估中的偏差问题，因为观测性估计可能因调整不完整而存在偏差，且社区层面的可靠结果数据稀缺。

Method: 使用机器学习从卫星图像中提取财富指数，结合预处理日间卫星图像和表格协变量，通过逆概率加权估计资助方和部门特定平均处理效应。

Result: 中国和世界银行项目都提高了财富水平，中国项目效果更大；世界银行项目选址更可预测，中国项目选址更多受非可见的政治或事件驱动因素影响。

Conclusion: 卫星图像分析显示之前的研究可能高估了发展项目的效益，不同资助方的项目选址机制存在系统性差异。

Abstract: Debates about whether development projects improve living conditions persist,
partly because observational estimates can be biased by incomplete adjustment
and because reliable outcome data are scarce at the neighborhood level. We
address both issues in a continent-scale, sector-specific evaluation of Chinese
and World Bank projects across 9,899 neighborhoods in 36 African countries
(2002 to 2013), representative of 88% of the population. First, we use a recent
dataset that measures living conditions with a machine-learned wealth index
derived from contemporaneous satellite imagery, yielding a consistent panel of
6.7 km square mosaics. Second, to strengthen identification, we proxy
officials' map-based placement criteria using pre-treatment daytime satellite
images and fuse these with rich tabular covariates to estimate funder- and
sector-specific ATEs via inverse-probability weighting. Incorporating imagery
systematically shrinks effects relative to tabular-only models, indicating
prior work likely overstated benefits. On average, both donors raise wealth,
with larger gains for China; sector extremes in our sample include Trade and
Tourism for the World Bank (+6.27 IWI points), and Emergency Response for China
(+14.32). Assignment-mechanism analyses show World Bank placement is generally
more predictable from imagery alone, as well as from tabular covariates. This
suggests that Chinese project placements are more driven by non-visible,
political, or event-driven factors than World Bank placements. To probe
residual concerns about selection on observables, we also estimate
within-neighborhood (unit) fixed-effects models at a spatial resolution about
450 times finer than prior fixed effects analyses, leveraging the
computer-vision-imputed IWI panels; these deliver smaller but directionally
consistent effects.

</details>


### [17] [DNA shotgun sequencing evidence: sample-specific and unknown genotyping error probabilities](https://arxiv.org/abs/2509.26112)
*Mikkel Meyer Andersen*

Main category: stat.AP

TL;DR: 本文扩展了wgsLR模型，允许对两个样本使用不同的基因分型错误概率，并能整合未知的错误概率。模型对过度分散具有鲁棒性，使用过小均值的先验分布更为保守。


<details>
  <summary>Details</summary>
Motivation: DNA鸟枪测序证据在法医遗传学中越来越受关注，需要能够正确解释此类证据并适当考虑测序错误的方法。

Method: 扩展Andersen等人的wgsLR模型，允许两个样本具有不同的基因分型错误概率，并能整合未知的错误概率。

Result: 模型对过度分散具有很强鲁棒性，整合未知错误概率时在两个假设下给出一致的证据权重，使用过小均值的先验分布更为保守。

Conclusion: 扩展的wgsLR模型能更好地处理不同样本类型的基因分型错误概率差异，为法医DNA证据分析提供了更准确的工具。

Abstract: DNA shotgun sequencing evidence is starting to gain a lot of attraction in
forensic genetics. Methods to correctly interpret such evidence, including
properly accounting for sequencing errors, are needed. This paper extends the
wgsLR model by Andersen et. al. (2025) from only allowing for the same, known
genotyping error probability for the two samples (trace sample from unknown
donor and reference sample from person of interest), to allowing for different
genotyping error probabilities (e.g., from trace hair sample and buccal swab
reference sample). The model was also extended to be able to integrate out
unknown genotyping error probabilities if only a prior probability is known.
The sensitivity of the model against overdispersion was also investigated and
it was found that it is very robust against overdispersion in estimating the
genotyping error probability. It was also found that integrating out unknown
genotyping error probability of the trace sample gave concordant weight of
evidence under both the hypotheses (first hypothesis that the same individual
was the donor of both trace and reference sample as well as the second
hypothesis that two different individuals were the donors for the trace and
reference sample). It was found that it is more consevative to use prior
distributions with a too small mean rather than a too high mean. The extension
of the model is implemented in the R package wgsLR.

</details>


### [18] [Confidence Intervals for Conditional Covariances of Natural Frequencies](https://arxiv.org/abs/2509.26348)
*Lizzie Neumann,Philipp Wittenberg,Jan Gertheiss*

Main category: stat.AP

TL;DR: 提出两种基于bootstrap的方法来估计条件协方差的置信区间，用于结构健康监测中考虑混杂因素对特征协方差的影响。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测中提取的损伤敏感特征不仅受损伤影响，还受环境和操作参数等混杂因素影响，这些因素会影响特征的均值和协方差，而协方差是许多损伤检测工具的基础。

Method: 使用非参数核方法估计条件协方差矩阵，允许协方差矩阵随混杂因素变化，并提出两种bootstrap方法来获取条件协方差的置信区间。

Result: 通过蒙特卡洛研究比较了两种bootstrap版本的有效性，并将方法应用于比利时鲁汶KW51铁路桥梁的自然频率数据，证明了方法的实用性。

Conclusion: 准确考虑混杂因素可以生成更可靠的诊断值，减少误报，提高结构健康监测的可靠性。

Abstract: In structural health monitoring (SHM), sensor measurements are collected, and
damage-sensitive features such as natural frequencies are extracted for damage
detection. However, these features depend not only on damage but are also
influenced by various confounding factors, including environmental conditions
and operational parameters. These factors must be identified, and their effects
must be removed before further analysis. However, it has been shown that
confounding variables may influence the mean and the covariance of the
extracted features. This is particularly significant since the covariance is an
essential building block in many damage detection tools. To account for the
complex relationships resulting from the confounding factors, a nonparametric
kernel approach can be used to estimate a conditional covariance matrix. By
doing so, the covariance matrix is allowed to change depending on the
identified confounding factor, thus providing a clearer understanding of how,
for example, temperature influences the extracted features. This paper presents
two bootstrap-based methods for obtaining confidence intervals for the
conditional covariances, providing a way to quantify the uncertainty associated
with the conditional covariance estimator. A proof-of-concept Monte Carlo study
compares the two bootstrap versions proposed and evaluates their effectiveness.
Finally, the methods are applied to the natural frequency data of the KW51
railway bridge near Leuven, Belgium. This real-world application highlights the
practical implications of the findings. It underscores the importance of
accurately accounting for confounding factors to generate more reliable
diagnostic values with fewer false alarms.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [19] [Evaluating treatment effects on longitudinal outcomes with attrition due to death: Methods for a two-dimentional estimand with a case study in Quality of Life](https://arxiv.org/abs/2509.25548)
*Dries Reynders,Doranne Thomassen,Satrajit Roychoudhury,Cecilie Delphin Amdal,Jammbe Z. Musoro,Willi Sauerbrei,Saskia le Cessie,Els Goetghebeur*

Main category: stat.ME

TL;DR: 该论文提出了一个二维因果估计量，用于同时评估生存状态和存活期间的纵向结果，比较了回归标准化和逆概率加权两种方法在观察性研究中的表现。


<details>
  <summary>Details</summary>
Motivation: 在死亡率较高的人群中评估纵向结果时，死亡后的结果缺失使得分析和因果解释变得复杂。需要承认生存是一个独特条件，回答"生存机会"和"存活期间状况"这两个内在二维的问题。

Method: 定义了二维因果估计量，比较了回归标准化（建模生存和纵向结果，然后标准化）和逆概率治疗与删失加权（双重加权观察结果）两种方法。两种方法基于相同的因果识别假设，但需要不同的模型正确设定。

Result: 回归标准化在满足所有假设时更高效，具有外推潜力。通过模拟研究展示了有限样本性能，并在肿瘤学质量生活案例中应用了这些方法。

Conclusion: 回归标准化和逆概率加权方法都能估计二维因果效应，但回归标准化在模型正确设定时更高效，能够外推到目标人群。

Abstract: When longitudinal outcomes are evaluated in mortal populations, their
non-existence after death complicates the analysis and its causal
interpretation. Where popular methods often merge longitudinal outcome and
survival into one scale or otherwise try to circumvent the problem of
mortality, some highly relevant questions require survival to be acknowledged
as a unique condition. "\textit{What are my chances of survival}" and
"\textit{What can I expect for my condition while still alive}" reflect the
intrinsically two-dimensional outcome of survival and longitudinal outcome
while-alive. We define a two-dimensional causal while-alive estimand for a
point exposure and compare two methods for estimation in an observational
setting. Regression-Standardization models survival and the observed
longitudinal outcome before standardizing the latter to a target population
weighted by its estimated survival. Alternatively, Inverse Probability of
Treatment and Censoring Weighting weights the observed outcomes twice, to
account for censoring and differences in baseline-case-mix. Both approaches
rely on the same causal identification assumptions, but require different
models to be correctly specified. With its potential to extrapolate,
Regression-Standardization is more efficient when all assumptions are met. We
show finite sample performance in a simulation study and apply the methods to a
case study on quality of life in oncology.

</details>


### [20] [PPD-CPP: Pointwise predictive density calibrated-power prior in dynamically borrowing historical information](https://arxiv.org/abs/2509.25688)
*Shixuan Wang,Jing Zhang,Emily L. Kang,Bin Zhang*

Main category: stat.ME

TL;DR: 提出了一种新的历史数据与当前数据一致性度量p_CM，并基于此开发了PPD-CPP方法来自适应地利用历史信息进行罕见病治疗效果分析。


<details>
  <summary>Details</summary>
Motivation: 在罕见病治疗效果分析中，如何确定历史数据与当前数据的适当一致性程度是一个主要挑战。

Method: 提出了p_CM一致性度量，基于边际后验预测p值思想量化数据集间的异质性，并开发了PPD-CPP方法动态利用历史信息。

Result: 推导了p_CM的渐近性质，PPD-CPP实现了借用一致性，允许将功率参数建模为固定标量或协变量告知的病例特定量。

Conclusion: 通过模拟研究和实际应用验证了方法的性能，为罕见病治疗效果分析提供了有效的历史数据利用框架。

Abstract: Incorporating historical or real-world data into analyses of treatment
effects for rare diseases has become increasingly popular. A major challenge,
however, lies in determining the appropriate degree of congruence between
historical and current data. In this study, we devote ourselves to the capacity
of historical data in replicating the current data, and propose a new
congruence measure/estimand $p_{CM}$. $p_{CM}$ quantifies the heterogeneity
between two datasets following the idea of the marginal posterior predictive
$p$-value, and its asymptotic properties were derived. Building upon $p_{CM}$,
we develop the pointwise predictive density calibrated-power prior (PPD-CPP) to
dynamically leverage historical information. PPD-CPP achieves the borrowing
consistency and allows modeling the power parameter either as a fixed scalar or
case-specific quantity informed by covariates. Simulation studies were
conducted to demonstrate the performance of these methods and the methodology
was illustrated using the Mother's Gift study and \textit{Ceriodaphnia dubia}
toxicity test.

</details>


### [21] [Bias-Reduced Estimation of Structural Equation Models](https://arxiv.org/abs/2509.25419)
*Haziq Jamil,Yves Rosseel,Oliver Kemp,Ioannis Kosmidis*

Main category: stat.ME

TL;DR: 该论文将减偏M估计框架应用于结构方程模型，通过模拟研究表明该方法能有效减少小样本偏差，同时不增加均方误差，为SEM估计提供了实用的减偏工具。


<details>
  <summary>Details</summary>
Motivation: 结构方程模型在样本量小或测量信度低时存在显著的有限样本偏差问题，现有减偏方法在适用范围、计算负担和统计性能方面各有权衡，需要更高效实用的解决方案。

Method: 应用减偏M估计框架，该方法只需要对数似然函数的一阶和二阶导数，相比基于重采样的方法更易实现且计算效率更高，对模型假设偏离具有稳健性。

Result: 模拟研究表明RBM估计量能持续减少SEM估计中的均值偏差而不增加均方误差，在中值偏差和推断方面也优于最大似然估计，在非正态条件下保持稳健性。

Conclusion: RBM为SEM估计提供了一个有前景、实用且广泛适用的减偏工具，特别适用于小样本研究情境。

Abstract: Finite-sample bias is a pervasive challenge in the estimation of structural
equation models (SEMs), especially when sample sizes are small or measurement
reliability is low. A range of methods have been proposed to improve
finite-sample bias in the SEM literature, ranging from analytic bias
corrections to resampling-based techniques, with each carrying trade-offs in
scope, computational burden, and statistical performance. We apply the
reduced-bias M-estimation framework (RBM, Kosmidis & Lunardon, 2024, J. R.
Stat. Soc. Series B Stat. Methodol.) to SEMs. The RBM framework is attractive
as it requires only first- and second-order derivatives of the log-likelihood,
which renders it both straightforward to implement, and computationally more
efficient compared to resampling-based alternatives such as bootstrap and
jackknife. It is also robust to departures from modelling assumptions. Through
extensive simulations studies under a range of experimental conditions, we
illustrate that RBM estimators consistently reduce mean bias in the estimation
of SEMs without inflating mean squared error. They also deliver improvements in
both median bias and inference relative to maximum likelihood estimators, while
maintaining robustness under non-normality. Our findings suggest that RBM
offers a promising, practical, and broadly applicable tool for mitigating bias
in the estimation of SEMs, particularly in small-sample research contexts.

</details>


### [22] [Modeling Spatial Heterogeneity in Exposure Buffers and Risk: A Hierarchical Bayesian Approach](https://arxiv.org/abs/2509.25708)
*Saskia Comess,Daniel E Ho,Joshua L Warren*

Main category: stat.ME

TL;DR: SVBR方法通过空间变化缓冲区半径来改进传统固定半径缓冲区的局限性，允许缓冲半径和暴露效应在空间上变化，提高了统计推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于圆形缓冲区的流行病学研究存在缓冲区半径选择随意且假设空间恒定的问题，这可能导致统计推断不准确。

Method: 开发了SVBR（空间变化缓冲区半径）方法，这是一种灵活的分层贝叶斯空间变点方法，将缓冲区半径视为未知参数，允许半径和暴露效应在空间上变化。

Result: 模拟研究表明SVBR相比传统方法改进了关键模型参数的估计和推断；在马达加斯加医疗可及性研究中发现，接近医疗机构通常会增加产前护理使用率，且这种关系存在明显的空间变化。

Conclusion: SVBR方法通过放宽对缓冲区特征的刚性假设，提供了一种灵活、数据驱动的方法来准确定义暴露并量化其影响。

Abstract: Place-based epidemiology studies often rely on circular buffers to define
"exposure" to spatially distributed risk factors, where the buffer radius
represents a threshold beyond which exposure does not influence the outcome of
interest. This approach is popular due to its simplicity and alignment with
public health policies. However, buffer radii are often chosen relatively
arbitrarily and assumed constant across the spatial domain. This may result in
suboptimal statistical inference if these modeling choices are incorrect. To
address this, we develop SVBR (Spatially-Varying Buffer Radii), a flexible
hierarchical Bayesian spatial change points approach that treats buffer radii
as unknown parameters and allows both radii and exposure effects to vary
spatially. Through simulations, we find that SVBR improves estimation and
inference for key model parameters compared to traditional methods. We also
apply SVBR to study healthcare access in Madagascar, finding that proximity to
healthcare facilities generally increases antenatal care usage, with clear
spatial variation in this relationship. By relaxing rigid assumptions about
buffer characteristics, our method offers a flexible, data-driven approach to
accurately defining exposure and quantifying its impact. The newly developed
methods are available in the R package EpiBuffer.

</details>


### [23] [An Order of Magnitude Time Complexity Reduction for Gaussian Graphical Model Posterior Sampling Using a Reverse Telescoping Block Decomposition](https://arxiv.org/abs/2509.26385)
*Zejin Gao,Ksheera Sagar,Anindya Bhadra*

Main category: stat.ME

TL;DR: 本文提出了一种新的MCMC采样方法，用于贝叶斯高斯图模型的后验估计，将计算复杂度从O(p⁴)降低到O(p³)，与Wishart共轭先验的计算成本相当。


<details>
  <summary>Details</summary>
Motivation: 现有的元素级图先验（如图形马蹄先验）虽然能自然地编码精度矩阵非对角线元素的稀疏性先验，但其MCMC方法的计算复杂度为O(p⁴)，在p≫n时计算成本过高。

Method: 开发了一种重新参数化的MCMC方法，通过反转Bhadra等人（2024）提出的伸缩块分解，实现了每次迭代O(p³)的计算复杂度。

Result: 仿真实验和乳腺癌数据集分析证实了所提出方法的正确性和更好的算法扩展性。

Conclusion: 该方法在不牺牲真实后验目标的前提下，显著提高了计算效率，使非共轭元素级图先验的计算成本与共轭Wishart先验相当。

Abstract: We consider the problem of fully Bayesian posterior estimation and
uncertainty quantification in undirected Gaussian graphical models via Markov
chain Monte Carlo (MCMC) under recently-developed element-wise graphical
priors, such as the graphical horseshoe. Unlike the conjugate Wishart family,
these priors are non-conjugate; but have the advantage that they naturally
allow one to encode a prior belief of sparsity in the off-diagonal elements of
the precision matrix, without imposing a structure on the entire matrix.
Unfortunately, for a graph with $p$ nodes and with $n$ samples, the
state-of-the-art MCMC approaches for the element-wise priors achieve a per
iteration complexity of $O(p^4),$ which is prohibitive when $p\gg n$. In this
regime, we develop a suitably reparameterized MCMC with per iteration
complexity of $O(p^3)$, providing a one-order of magnitude improvement, and
consequently bringing the computational cost at par with the conjugate Wishart
family, which is also $O(p^3)$ due to a use of the classical Bartlett
decomposition, but this decomposition does not apply outside the Wishart
family. Importantly, the proposed benefit is obtained solely due to our
reparameterization in an MCMC scheme targeting the true posterior, that
reverses the recently developed telescoping block decomposition of Bhadra et
al. (2024), in a suitable sense. There is no variational or any other
approximate Bayesian computation scheme considered in this paper that
compromises targeting the true posterior. Simulations and the analysis of a
breast cancer data set confirm both the correctness and better algorithmic
scaling of the proposed reverse telescoping sampler.

</details>


### [24] [Calibrated Counterfactual Conformal Fairness ($C^3F$): Post-hoc, Shift-Aware Coverage Parity via Conformal Prediction and Counterfactual Regularization](https://arxiv.org/abs/2509.25295)
*Faruk Alpay,Taylan Alpay*

Main category: stat.ME

TL;DR: 提出C³F方法，通过重要性加权保形校准和基于结构因果模型中路径特定效应的反事实正则化器，实现协变量偏移下群体条件覆盖率的公平性


<details>
  <summary>Details</summary>
Motivation: 解决在协变量偏移情况下群体条件覆盖率的公平性问题，确保不同群体在预测覆盖度上的平等

Method: 结合重要性加权保形校准与反事实正则化器，使用似然比权重估计群体特定的非保形分位数，通过平滑阈值正则化控制反事实覆盖率公平性代理

Result: 在标准分类基准上实证评估显示改进了群体条件覆盖率，相对于偏移感知和公平导向的保形基线具有竞争力

Conclusion: C³F是模型无关、标签高效且无需重新训练即可部署的方法，讨论了敏感属性部分可用性和结构因果模型错误指定的鲁棒性等实际考量

Abstract: We present Calibrated Counterfactual Conformal Fairness ($C^3F$), a post-hoc
procedure that targets group-conditional coverage parity under covariate shift.
$C^3F$ combines importance-weighted conformal calibration with a counterfactual
regularizer based on path-specific effects in a structural causal model. The
method estimates group-specific nonconformity quantiles using likelihood-ratio
weights so that coverage degrades gracefully with the second moment of the
weights. We derive finite-sample lower bounds on group-wise coverage and a
bound on the equalized conditional coverage gap, and we show first-order
control of a counterfactual coverage-parity surrogate via smooth threshold
regularization. The approach is model-agnostic, label-efficient, and deployable
without retraining. Empirical evaluations on standard classification benchmarks
demonstrate improved group-conditional coverage and competitive efficiency
relative to shift-aware and fairness-oriented conformal baselines. We discuss
practical considerations, including partial availability of sensitive
attributes and robustness to structural causal misspecification.

</details>


### [25] [Parameter estimation of the four-parameter Harris extended Weibull distribution with applications to real-life data](https://arxiv.org/abs/2509.26162)
*Prithul Chaturvedi,Himanshu Pokhriyal*

Main category: stat.ME

TL;DR: 本文提出了一种四参数Harris扩展Weibull分布，研究了多种参数估计方法，并通过模拟和实际数据验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 扩展经典两参数Weibull分布以增加灵活性，为复杂数据建模提供更合适的概率分布工具。

Method: 使用最小二乘法、最大间距乘积法和最小距离法进行参数估计，并采用Metropolis-Hastings算法进行贝叶斯推断。

Result: 通过大量模拟验证了估计方法的性能，并在三个真实数据集上展示了HEW分布相比Weibull分布变体的优越适用性。

Conclusion: HEW分布具有灵活的形状特性，多种估计方法表现良好，在实际应用中优于传统Weibull分布。

Abstract: This paper explores the extension of the classical two-parameter Weibull
distribution to a four-parameter Harris extended Weibull (HEW) distribution.
The flexibility of this probability distribution is illustrated by the varying
shapes of HEW density function. Estimation of HEW parameters is explored using
estimation methods such as the least-squares, maximum product of spacings, and
minimum distance method. We provide Bayesian inference on the random parameters
of the HEW distribution using Metropolis-Hastings algorithm to sample from the
joint posterior distribution. Performance of the estimation methods is assessed
using extensive simulations. The applicability of the distribution is
demonstrated against three variants of the Weibull distribution on three
real-life datasets.

</details>


### [26] [Joint Adaptive Penalty for Unbalanced Mediation Pathways](https://arxiv.org/abs/2509.25527)
*Hanying Jiang,Kris Sankaran,Yinqiu He*

Main category: stat.ME

TL;DR: 提出一种新的联合自适应惩罚方法，用于处理中介分析中路径效应不平衡的问题，提高参数估计和路径选择的效率。


<details>
  <summary>Details</summary>
Motivation: 传统中介分析方法在估计多个模型参数时效率较低，特别是在路径效应不平衡的情况下表现不佳。

Method: 开发了一种联合自适应惩罚方法，整合整个中介机制的信息，通过理论保证和数值研究验证其有效性。

Result: 在路径效应不平衡的场景下，该方法表现出优于传统方法的性能，提高了参数估计和路径选择的准确性。

Conclusion: 所提出的联合自适应惩罚方法能够有效解决中介分析中路径效应不平衡的问题，为复杂中介机制的分析提供了更高效的工具。

Abstract: Mediation analysis has been widely used to investigate how a treatment
influences an outcome through intermediate variables, known as mediators.
Analyzing a mediation mechanism typically requires assessing multiple model
parameters that characterize distinct pathwise effects. Classical methods that
estimate these parameters individually can be inefficient, particularly when
the underlying pathwise effects exhibit substantial imbalance. To address this
challenge, this work proposes a new joint adaptive penalty that integrates
information across entire mediation mechanisms, thereby enhancing both
parameter estimation and pathway selection. We establish theoretical guarantees
for the proposed method under an asymptotic framework and conduct extensive
numerical studies to demonstrate its superior performance in scenarios with
unbalanced mediation pathways.

</details>


### [27] [Repulsive mixtures via the sparsity-inducing partition prior](https://arxiv.org/abs/2509.25860)
*Alexander Mozdzen,Timothy Wertz,Maria De Iorio,Andrea Cremaschi,Gregor Kastner,Johan Eriksson*

Main category: stat.ME

TL;DR: 提出了一种基于Selberg Dirichlet分布的新型先验分布——稀疏诱导划分(SIP)先验，该先验通过排斥项惩罚接近的权重值，鼓励形成少量主导簇，从而在混合模型中诱导额外的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 传统Dirichlet分布在混合模型中缺乏对权重稀疏性的有效控制，无法自然地鼓励形成少量主导簇。需要一种能够惩罚接近权重值、诱导稀疏性的先验分布。

Method: 基于Selberg Dirichlet分布构建稀疏诱导划分(SIP)先验，该先验包含排斥项惩罚接近的权重值。提出了高效的后验采样算法，并在有限混合模型(固定或随机组件数)和排斥混合模型中进行验证。

Result: 通过广泛的模拟研究和生物医学数据集(儿童BMI和饮食行为)的应用验证了模型的有效性。SIP先验能够自然地诱导稀疏性，形成少量主导簇。

Conclusion: SIP先验为混合模型提供了一种有效的稀疏诱导机制，能够自然地惩罚接近的权重值，鼓励形成少量主导簇，在各种混合模型设置中都表现出良好的性能。

Abstract: We introduce a novel prior distribution for modelling the weights in mixture
models based on a generalisation of the Dirichlet distribution, the Selberg
Dirichlet distribution. This distribution contains a repulsive term, which
naturally penalises values that lie close to each other on the simplex, thus
encouraging few dominating clusters. The repulsive behaviour induces additional
sparsity on the number of components. We refer to this construction as
sparsity-inducing partition (SIP) prior. By highlighting differences with the
conventional Dirichlet distribution, we present relevant properties of the SIP
prior and demonstrate their implications across a variety of mixture models,
including finite mixtures with a fixed or random number of components, as well
as repulsive mixtures. We propose an efficient posterior sampling algorithm and
validate our model through an extensive simulation study as well as an
application to a biomedical dataset describing children's Body Mass Index and
eating behaviour.

</details>


### [28] [Highly robust factored principal component analysis for matrix-valued outlier accommodation and explainable detection via matrix minimum covariance determinant](https://arxiv.org/abs/2509.25957)
*Wenhui Wu,Changchun Shang,Jianhua Zhao,Xuan Ma,Yue Wang*

Main category: stat.ME

TL;DR: 提出了HRFPCA方法，通过结合矩阵最小协方差行列式估计器，在保持FPCA矩阵数据结构建模能力的同时，显著提升了对抗异常值的鲁棒性，崩溃点接近50%。


<details>
  <summary>Details</summary>
Motivation: 传统PCA方法在处理矩阵值数据时面临两大挑战：向量化破坏矩阵结构导致信息丢失和维度灾难，以及对异常值高度敏感。

Method: 在FPCA基础上，用矩阵最小协方差行列式估计器替代最大似然估计器，开发了HRFPCA方法，并生成SODA图用于可视化矩阵值异常值。

Result: 大量模拟和实际数据分析表明，HRFPCA在鲁棒性和异常值检测方面始终优于竞争方法，崩溃点接近50%。

Conclusion: HRFPCA有效解决了矩阵值数据在重尾分布或存在异常值时的降维问题，具有广泛的适用性。

Abstract: Principal component analysis (PCA) is a classical and widely used method for
dimensionality reduction, with applications in data compression, computer
vision, pattern recognition, and signal processing. However, PCA is designed
for vector-valued data and encounters two major challenges when applied to
matrix-valued data with heavy-tailed distributions or outliers: (1)
vectorization disrupts the intrinsic matrix structure, leading to information
loss and the curse of dimensionality, and (2) PCA is highly sensitive to
outliers. Factored PCA (FPCA) addresses the first issue through probabilistic
modeling, using a matrix normal distribution that explicitly represents row and
column covariances via a separable covariance structure, thereby preserving the
two-way dependency and matrix form of the data. Building on FPCA, we propose
highly robust FPCA (HRFPCA), a robust extension that replaces maximum
likelihood estimators with the matrix minimum covariance determinant (MMCD)
estimators. This modification enables HRFPCA to retain FPCA's ability to model
matrix-valued data while achieving a breakdown point close to 50\%,
substantially improving resistance to outliers. Furthermore, HRFPCA produces
the score--orthogonal distance analysis (SODA) plot, which effectively
visualizes and classifies matrix-valued outliers. Extensive simulations and
real-data analyses demonstrate that HRFPCA consistently outperforms competing
methods in robustness and outlier detection, underscoring its effectiveness and
broad applicability.

</details>


### [29] [Fuzzy Jump Models for Soft and Hard Clustering of Multivariate Time Series Data](https://arxiv.org/abs/2509.26029)
*Federico P. Cortese,Antonio Pievatolo,Elisa Maria Alessi*

Main category: stat.ME

TL;DR: 提出了模糊跳跃模型，扩展了统计跳跃模型以包含聚类成员的不确定性估计，支持软硬聚类，适用于多变量时间序列数据，在模拟和实证应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统统计跳跃模型仅限于硬聚类，无法处理状态分配的不确定性，需要扩展以包含不确定性估计。

Method: 利用统计跳跃模型与模糊c均值框架的相似性，提出模糊跳跃模型，通过调整模糊度参数支持软硬聚类，可处理混合类型的多变量时间序列数据。

Result: 模拟研究显示该模型能准确估计潜在状态分布，在高不确定性情况下优于竞争方法；在实证应用中成功识别三体问题的共轨道状态和金融市场中的牛熊市阶段。

Conclusion: 模糊跳跃模型提供了灵活的状态识别框架，能有效处理聚类不确定性，在科学和金融领域具有重要应用价值。

Abstract: Statistical jump models have been recently introduced to detect persistent
regimes by clustering temporal features and discouraging frequent regime
changes. However, they are limited to hard clustering and thereby do not
account for uncertainty in state assignments. This work presents an extension
of the statistical jump model that incorporates uncertainty estimation in
cluster membership. Leveraging the similarities between statistical jump models
and the fuzzy c-means framework, our fuzzy jump model sequentially estimates
time-varying state probabilities. Our approach offers high flexibility, as it
supports both soft and hard clustering through the tuning of a fuzziness
parameter, and it naturally accommodates multivariate time series data of mixed
types. Through a simulation study, we evaluate the ability of the proposed
model to accurately estimate the true latent-state distribution, demonstrating
that it outperforms competing approaches under high cluster assignment
uncertainty. We further demonstrate its utility on two empirical applications:
first, by automatically identifying co-orbital regimes in the three-body
problem, a novel application with important implications for understanding
asteroid behavior and designing interplanetary mission trajectories; and
second, on a financial dataset of five assets representing distinct market
sectors (equities, bonds, foreign exchange, cryptocurrencies, and utilities),
where the model accurately tracks both bull and bear market phases.

</details>


### [30] [Staged Event Trees for Transparent Treatment Effect Estimation](https://arxiv.org/abs/2509.26265)
*Gherardo Varando,Manuele Leonelli,Jordi Cerdà-Bautista,Vasileios Sitokonstantinou,Gustau Camps-Valls*

Main category: stat.ME

TL;DR: 本文在分段事件树框架下重新表征了因果推断方法，展示了该框架如何提升处理效应估计的可解释性和实用性。


<details>
  <summary>Details</summary>
Motivation: 处理效应评估在临床和政策制定等关键应用中至关重要，需要从观察数据中准确估计处理效应的方法。

Method: 在分段事件树框架中实现经典估计器，通过模拟研究和实际应用展示其能力，并可视化标准因果假设的成立条件。

Result: 分段事件树框架增强了处理效应估计的可解释性，能够明确描述因果假设（如正性）何时成立。

Conclusion: 分段事件树为因果推断提供了高度可解释的框架，特别适用于需要透明因果推理的实际应用场景。

Abstract: Average and conditional treatment effects are fundamental causal quantities
used to evaluate the effectiveness of treatments in various critical
applications, including clinical settings and policy-making. Beyond the
gold-standard estimators from randomized trials, numerous methods have been
proposed to estimate treatment effects using observational data. In this paper,
we provide a novel characterization of widely used causal inference techniques
within the framework of staged event trees, demonstrating their capacity to
enhance treatment effect estimation. These models offer a distinct advantage
due to their interpretability, making them particularly valuable for practical
applications. We implement classical estimators within the framework of staged
event trees and illustrate their capabilities through both simulation studies
and real-world applications. Furthermore, we showcase how staged event trees
explicitly and visually describe when standard causal assumptions, such as
positivity, hold, further enhancing their practical utility.

</details>


### [31] [W-transforms: Uniformity-preserving transformations and induced dependence structures](https://arxiv.org/abs/2509.26280)
*Marius Hofert,Zhiyuan Pang*

Main category: stat.ME

TL;DR: W-transforms是保均匀性的单变量变换，可作为copula到copula的变换，用于改进依赖建模。


<details>
  <summary>Details</summary>
Motivation: 引入W-transforms作为保均匀性的单变量变换，旨在提供一种灵活的copula变换方法，以改进依赖建模能力。

Method: 基于分布函数和分段严格单调函数定义W-transforms，研究其性质，并应用于随机向量以生成W-transformed copulas。

Result: 推导了W-transformed copulas的解析形式、密度、一致性度量、尾部依赖和对称性等性质，提出了参数化W-transforms族。

Conclusion: W-transforms提供了一种有效的copula变换框架，通过实际数据集验证了其在依赖建模方面的改进效果。

Abstract: W-transforms are introduced as uniformity-preserving univariate
transformations on the unit interval induced by distribution functions and
piecewise strictly monotone functions, and their properties are investigated.
When applied componentwise to random vectors with standard uniform univariate
margins, W-transforms naturally serve as copula-to-copula transformations.
Properties of the resulting W-transformed copulas, including their analytical
form, density, measures of concordance, tail dependence and symmetries, are
derived. A flexible parametric family of W-transforms is proposed as a special
case to further enhance tractability. Illustrative examples highlight the
introduced concepts, and improved dependence modelling is demonstrated in terms
of a real-life dataset.

</details>


### [32] [Spectral Bootstrap for Non-Parametric Simulation of Multivariate Extreme Events](https://arxiv.org/abs/2509.26451)
*Nisrine Madhar,Juliette Legrand,Maud Thomas*

Main category: stat.ME

TL;DR: 提出了一种基于多元极值谱表示的非参数自助法，通过生成合成极端数据来改进极值推断的可靠性。


<details>
  <summary>Details</summary>
Motivation: 极值理论推断依赖于有限的极端观测值，使得估计具有挑战性。

Method: 多元极值谱自助法，基于多元广义帕累托分布随机向量的谱表示，保留数据的联合尾部行为并生成合成极端数据。

Result: 在模拟和真实数据中有效估计尾部风险度量，提高了高维极端情景下风险评估的可靠性。

Conclusion: 该方法在增强高维极端情景风险评估方面具有潜力。

Abstract: Inference in extreme value theory relies on a limited number of extreme
observations, making estimation challenging. To address this limitation, we
propose a non-parametric bootstrap procedure, the multivariate extreme spectral
bootstrap procedure, relying on the spectral representation of multivariate
generalized Paretodistributed random vectors. Unlike standard bootstrap
methods, our approach preserves the joint tail behaviour of the data and
generates additional synthetic extreme data, thereby improving the reliability
of inference. We demonstrate the effectiveness of our procedure for the
estimation of tail risk metrics, under both simulated and real data. The
results highlight the potential of this method for enhancing risk assessment in
high-dimensional extreme scenarios.

</details>


### [33] [Computationally and statistically efficient estimation of time-smoothed counterfactual curves](https://arxiv.org/abs/2509.26554)
*Herbert P. Susmann,Nicholas T. Williams,Richard Liu,Jessica G. Young,Iván Díaz*

Main category: stat.ME

TL;DR: 提出了一种新的多重稳健估计器，用于广义g公式，支持时间平滑处理多个时间点测量的结果，提高了在结果缺失和强混杂情况下的估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多个时间点测量的结果时无法进行时间平滑，导致精度损失。需要一种能够处理时间平滑、适应各种干预类型并考虑删失和非单调缺失模式的估计方法。

Method: 提出了一种新颖的多重稳健估计器，基于广义g公式，支持纵向修正治疗策略，能够处理时间平滑、各种干预类型（二元、多值、连续），并考虑删失和非单调缺失模式。

Result: 在模拟研究中，该方法在高程度结果缺失和强混杂情况下，优于现有的多重稳健方法进行效应曲线估计。应用于研究工会会员身份对工资的纵向影响。

Conclusion: 该方法提供了一种有效的工具，用于估计时间变化干预对时间变化结果的因果效应，特别是在存在结果缺失和混杂的情况下，能够获得更精确的效应曲线估计。

Abstract: Longitudinal causal inference is concerned with defining, identifying, and
estimating the effect of a time-varying intervention on a time-varying outcome
that is indexed by a follow-up time. In an observational study, Robins's
generalized g-formula can identify causal effects induced by a broad class of
time-varying interventions. Various methods for estimating the generalized
g-formula have been posed for different outcome types, such as a failure event
indicator by a specified time (e.g. mortality by 5 year follow-up), as well as
continuous or dichotomous/multi-valued outcomes measures at a specified time
(e.g. blood pressure in mm/hg or an indicator of high blood pressure at 5-year
follow-up). Multiply-robust, data-adaptive estimators leverage flexible
nonparametric estimation algorithms while allowing for statistical inference.
However, extant methods do not accommodate time-smoothing when multiple
outcomes are measured over time, which can lead to substantial loss of
precision. We propose a novel multiply-robust estimator of the generalized
g-formula that accommodates time-smoothing over numerous available outcome
measures. Our approach accommodates any intervention that can be described as a
Longitudinal Modified Treatment Policy, a flexible class suitable for binary,
multi-valued, and continuous longitudinal treatments. Our method produces an
estimate of the effect curve: the causal effect of the intervention on the
outcome at each measurement time, taking into account censoring and
non-monotonic outcome missingness patterns. In simulations we find that the
proposed algorithm outperforms extant multiply-robust approaches for effect
curve estimation in scenarios with high degrees of outcome missingness and when
there is strong confounding. We apply the method to study longitudinal effects
of union membership on wages.

</details>


### [34] [Stochasticity and Practical Identifiability in Epidemic Models: A Monte Carlo Perspective](https://arxiv.org/abs/2509.26577)
*Chiara Mattamira,Olivia Prosper Feldman*

Main category: stat.ME

TL;DR: 标准蒙特卡洛方法使用独立高斯噪声评估流行病模型的可识别性存在局限，本文提出混合模拟方法更好地捕捉流行病随机性。


<details>
  <summary>Details</summary>
Motivation: 评估流行病模型的实际可识别性很重要，但标准MC方法基于独立高斯噪声的假设不适合流行病过程，因为流行病具有内在随机性、时间相关性和高变异性。

Method: 研究SIR模型中随机变异结构，提出混合模拟方法，在确定性ODE轨迹中引入时间和幅度相关的变异性。

Result: CTMC轨迹始终表现出超泊松变异性和强时间依赖性，独立高斯噪声系统性地低估了基础随机过程的变异性。

Conclusion: 标准MC算法存在局限性，混合方法为在流行病推断中纳入更现实的噪声结构提供了途径。

Abstract: Assessing the practical identifiability of epidemic models is essential for
determining whether parameters can be meaningfully estimated from observed
data. Monte Carlo (MC) methods provide an accessible and intuitive framework;
however, their standard implementation - perturbing deterministic trajectories
with independent Gaussian noise - rests on assumptions poorly suited to
epidemic processes, which are inherently stochastic, temporally correlated, and
highly variable, especially in small populations or under slow transmission. In
this study, we investigate the structure of stochastic variability in the
classic Susceptible-Infected-Recovered (SIR) model across a range of
epidemiological regimes, and assess whether it can be represented within the
independent Gaussian noise framework. We show that continuous-time Markov chain
(CTMC) trajectories consistently exhibit super-Poissonian variability and
strong temporal dependence. Through coverage analysis, we further demonstrate
that independent Gaussian noise systematically underestimates the variability
of the underlying stochastic process, leading to overly optimistic conclusions
about parameter identifiability. In addition, we propose a hybrid simulation
approach that introduces time- and amplitude-dependent variability into
deterministic ODE trajectories, preserving computational efficiency while
capturing key features of epidemic stochasticity. Our findings highlight the
limitations of the standard MC algorithm and provide a pathway for
incorporating more realistic noise structures into epidemic inference.

</details>
