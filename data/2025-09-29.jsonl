{"id": "2509.21736", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2509.21736", "abs": "https://arxiv.org/abs/2509.21736", "authors": ["Hyewon Jung", "Seungil Ahn", "Seungho Choi", "Yeseul Jeon"], "title": "Quantifying Fire Risk Index in Chemical Industry Using Statistical Modeling Procedure", "comment": null, "summary": "Fire incident reports contain detailed textual narratives that capture causal\nfactors often overlooked in structured records, while financial damage amounts\nprovide measurable outcomes of these events. Integrating these two sources of\ninformation is essential for uncovering interpretable links between descriptive\ncauses and their economic consequences. To this end, we develop a data-driven\nframework that constructs a composite Risk Index, enabling systematic\nquantification of how specific keywords relate to property damage amounts. This\nindex facilitates both the identification of high-impact terms and the\naggregation of risks across semantically related clusters, thereby offering a\nprincipled measure of fire-related financial risk. Using more than a decade of\nKorean fire investigation reports on the chemical industry classified as\nSpecial Buildings (2013 through 2024), we employ topic modeling and\nnetwork-based embedding to estimate semantic similarities from interactions\namong words and subsequently apply Lasso regression to quantify their\nassociations with property damage amounts, thereby estimate fire risk index.\nThis approach enables us to assess fire risk not only at the level of\nindividual terms but also within their broader textual context, where highly\ninteractive related words provide insights into collective patterns of hazard\nrepresentation and their potential impact on expected losses. The analysis\nhighlights several domains of risk, including hazardous chemical leakage,\nunsafe storage practices, equipment and facility malfunctions, and\nenvironmentally induced ignition. The results demonstrate that text-derived\nindices provide interpretable and practically relevant insights, bridging\nunstructured narratives with structured loss information and offering a basis\nfor evidence-based fire risk assessment and management."}
{"id": "2509.22089", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2509.22089", "abs": "https://arxiv.org/abs/2509.22089", "authors": ["Lydia Jang", "Stefan Konigorski"], "title": "Personalized Oncology: Feasibility of Evaluating Treatment Effects for Individual Patients", "comment": "19 pages, 4 figures", "summary": "The effectiveness of personalized oncology treatments ultimately depends on\nwhether outcomes can be causally attributed to the treatment. Advances in\nprecision oncology have improved molecular profiling of individuals, and\ntailored therapies have led to more effective treatments for select patient\ngroups. However, treatment responses still vary among individuals. As cancer is\na heterogeneous and dynamic disease with varying treatment outcomes across\ndifferent molecular types and resistance mechanisms, it requires customized\napproaches to identify cause-and-effect relationships. N-of-1 trials, or\nsingle-subject clinical trials, are designed to evaluate individual treatment\neffects. Several works have described different causal frameworks to identify\ntreatment effects in N-of-1 trials, yet whether these approaches can be\nextended to single-cancer patient settings remains unclear. To explore this\npossibility, a longitudinal dataset from a single metastatic cancer patient\nwith adaptively chosen treatments was considered. The dataset consisted of a\ndetailed treatment plan as well as biomarker and lesion measurements recorded\nover time. After data processing, a treatment period with sufficient data\npoints to conduct causal inference was selected. Under this setting, a causal\nframework was applied to define an estimand, identify causal relationships and\nassumptions, and calculate an individual-specific treatment effect using a\ntime-varying g-formula. Through this application, we illustrate explicitly when\nand how causal treatment effects can be estimated in single-patient oncology\nsettings. Our findings not only demonstrate the feasibility of applying causal\nmethods in a single-cancer patient setting but also offer a blueprint for using\ncausal methods across a broader spectrum of cancer types in individualized\nsettings."}
{"id": "2509.22275", "categories": ["stat.AP", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.22275", "abs": "https://arxiv.org/abs/2509.22275", "authors": ["Teddy Lazebnik", "Vered Aharonson"], "title": "Chronic Stress, Immune Suppression, and Cancer Occurrence: Unveiling the Connection using Survey Data and Predictive Models", "comment": null, "summary": "Chronic stress was implicated in cancer occurrence, but a direct causal\nconnection has not been consistently established. Machine learning and causal\nmodeling offer opportunities to explore complex causal interactions between\npsychological chronic stress and cancer occurrences. We developed predictive\nmodels employing variables from stress indicators, cancer history, and\ndemographic data from self-reported surveys, unveiling the direct and immune\nsuppression mitigated connection between chronic stress and cancer occurrence.\nThe models were corroborated by traditional statistical methods. Our findings\nindicated significant causal correlations between stress frequency, stress\nlevel and perceived health impact, and cancer incidence. Although stress alone\nshowed limited predictive power, integrating socio-demographic and familial\ncancer history data significantly enhanced model accuracy. These results\nhighlight the multidimensional nature of cancer risk, with stress emerging as a\nnotable factor alongside genetic predisposition. These findings strengthen the\ncase for addressing chronic stress as a modifiable cancer risk factor,\nsupporting its integration into personalized prevention strategies and public\nhealth interventions to reduce cancer incidence."}
{"id": "2509.22501", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22501", "abs": "https://arxiv.org/abs/2509.22501", "authors": ["C. J. R. Murphy-Barltrop", "J. L. Wadsworth", "M. de Carvalho", "B. D. Youngman"], "title": "Modelling non-stationary extremal dependence through a geometric approach", "comment": null, "summary": "Non-stationary extremal dependence, whereby the relationship between the\nextremes of multiple variables evolves over time, is commonly observed in many\nenvironmental and financial data sets. However, most multivariate extreme value\nmodels are only suited to stationary data. A recent approach to multivariate\nextreme value modelling uses a geometric framework, whereby extremal dependence\nfeatures are inferred through the limiting shapes of scaled sample clouds. This\nframework can capture a wide range of dependence structures, and a variety of\ninference procedures have been proposed in the stationary setting. In this\nwork, we first extend the geometric framework to the non-stationary setting and\noutline assumptions to ensure the necessary convergence conditions hold. We\nthen introduce a flexible, semi-parametric modelling framework for obtaining\nestimates of limit sets in the non-stationary setting. Through rigorous\nsimulation studies, we demonstrate that our proposed framework can capture a\nwide range of dependence forms and is robust to different model formulations.\nWe illustrate the proposed methods on financial returns data and present\nseveral practical uses."}
{"id": "2509.21478", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.21478", "abs": "https://arxiv.org/abs/2509.21478", "authors": ["Maria Paula Duenas-Herrera", "Stephen Berg", "Murali Haran"], "title": "Modeling discrete lattice data using the Potts and tapered Potts models", "comment": null, "summary": "The Ising and Potts models, among the most important models in statistical\nphysics, have been used for modeling binary and multinomial data on lattices in\na wide variety of disciplines such as psychology, image analysis, biology, and\nforestry. However, these models have several well known shortcomings: (i) they\ncan result in poorly fitting models, that is, simulations from fitted models\noften do not produce realizations that look like the observed data; (ii) phase\ntransitions and the presence of ground states introduce significant challenges\nfor statistical inference, model interpretation, and goodness of fit; (iii)\nintractable normalizing constants that are functions of the model parameters\npose serious computational problems for likelihood-based inference.\n  Here we develop a tapered version of the Ising and Potts models that\naddresses issues (i) and (ii). We develop efficient Markov Chain Monte Carlo\nMaximum Likelihood Estimation (MCMCMLE) algorithms that address issue (iii). We\nperform an extensive simulation study for the classical and Tapered Potts\nmodels that provide insights regarding the issues generated by the phase\ntransition and ground states. Finally, we offer practical recommendations for\nmodeling and computation based on applications of our approach to simulated\ndata as well as data from the 2021 National Land Cover Database."}
{"id": "2509.21423", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21423", "abs": "https://arxiv.org/abs/2509.21423", "authors": ["Ehsan Sharifian", "Saber Salehkaleybar", "Negar Kiyavash"], "title": "Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models", "comment": null, "summary": "We study the problem of causal structure learning from a combination of\nobservational and interventional data generated by a linear non-Gaussian\nstructural equation model that might contain cycles. Recent results show that\nusing mere observational data identifies the causal graph only up to a\npermutation-equivalence class. We obtain a combinatorial characterization of\nthis class by showing that each graph in an equivalence class corresponds to a\nperfect matching in a bipartite graph. This bipartite representation allows us\nto analyze how interventions modify or constrain the matchings. Specifically,\nwe show that each atomic intervention reveals one edge of the true matching and\neliminates all incompatible causal graphs. Consequently, we formalize the\noptimal experiment design task as an adaptive stochastic optimization problem\nover the set of equivalence classes with a natural reward function that\nquantifies how many graphs are eliminated from the equivalence class by an\nintervention. We show that this reward function is adaptive submodular and\nprovide a greedy policy with a provable near-optimal performance guarantee. A\nkey technical challenge is to efficiently estimate the reward function without\nhaving to explicitly enumerate all the graphs in the equivalence class. We\npropose a sampling-based estimator using random matchings and analyze its bias\nand concentration behavior. Our simulation results show that performing a small\nnumber of interventions guided by our stochastic optimization framework\nrecovers the true underlying causal structure."}
{"id": "2509.22597", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.TH", "Primary 62G05, 65C60 Secondary 62P30, 62P35, 60D05, 60A10"], "pdf": "https://arxiv.org/pdf/2509.22597", "abs": "https://arxiv.org/abs/2509.22597", "authors": ["Haiyi Shi", "Lei Yang", "Jiarui Chi", "Troy Butler", "Haonan Wang", "Derek Bingham", "Don Estep"], "title": "A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse Problem", "comment": "48 pages, 14 figures", "summary": "The stochastic inverse problem is a key ingredient in making inferences,\npredictions, and decisions for complex science and engineering systems. We\nformulate and analyze a nonparametric Bayesian solution for the stochastic\ninverse problem. Key properties of the solution are proved and the convergence\nand error of a computational solution obtained by random sampling is analyzed.\nSeveral applications illustrate the results."}
{"id": "2509.21940", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.21940", "abs": "https://arxiv.org/abs/2509.21940", "authors": ["Ivan Lau", "Jonathan Scarlett"], "title": "Sequential 1-bit Mean Estimation with Near-Optimal Sample Complexity", "comment": null, "summary": "In this paper, we study the problem of distributed mean estimation with 1-bit\ncommunication constraints. We propose a mean estimator that is based on\n(randomized and sequentially-chosen) interval queries, whose 1-bit outcome\nindicates whether the given sample lies in the specified interval. Our\nestimator is $(\\epsilon, \\delta)$-PAC for all distributions with bounded mean\n($-\\lambda \\le \\mathbb{E}(X) \\le \\lambda $) and variance ($\\mathrm{Var}(X) \\le\n\\sigma^2$) for some known parameters $\\lambda$ and $\\sigma$. We derive a sample\ncomplexity bound $\\widetilde{O}\\big(\n\\frac{\\sigma^2}{\\epsilon^2}\\log\\frac{1}{\\delta} +\n\\log\\frac{\\lambda}{\\sigma}\\big)$, which matches the minimax lower bound for the\nunquantized setting up to logarithmic factors and the additional\n$\\log\\frac{\\lambda}{\\sigma}$ term that we show to be unavoidable. We also\nestablish an adaptivity gap for interval-query based estimators: the best\nnon-adaptive mean estimator is considerably worse than our adaptive mean\nestimator for large $\\frac{\\lambda}{\\sigma}$. Finally, we give tightened sample\ncomplexity bounds for distributions with stronger tail decay, and present\nadditional variants that (i) handle an unknown sampling budget (ii) adapt to\nthe unknown true variance given (possibly loose) upper and lower bounds on the\nvariance, and (iii) use only two stages of adaptivity at the expense of more\ncomplicated (non-interval) queries."}
{"id": "2509.22543", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.22543", "abs": "https://arxiv.org/abs/2509.22543", "authors": ["Lan Wen", "Aaron L Sarvet"], "title": "Estimating average treatment effects when treatment data are absent in a target study", "comment": null, "summary": "Researchers are frequently interested in understanding the causal effect of\ntreatment interventions. However, in some cases, the treatment of\ninterest--readily available in a randomized controlled trial (RCT)--is either\nnot directly measured or entirely unavailable in observational datasets. This\nchallenge has motivated the development of stochastic incremental propensity\nscore interventions which operate on post-treatment exposures affected by the\ntreatment of interest with the aim of approximating the causal effects of the\ntreatment intervention. Yet, a key challenge lies in the fact that the precise\ndistributional shift of these post-treatment exposures induced by the treatment\nis typically unknown, making it uncertain whether the approximation truly\nreflects the causal effect of interest. The primary objective of this paper is\nto explore data integration methodologies to characterize a distribution of\npost-treatment exposures resulting from the treatment in an external dataset,\nand to use this information to estimate counterfactual mean outcomes under\ntreatment interventions, in settings where the observational data lack\ntreatment information and the external data may not contain measurements of the\noutcome of interest. We will discuss the underlying assumptions required for\nthis approach and provide methodological guidance on estimation strategies to\naddress these challenges."}
{"id": "2509.21691", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.21691", "abs": "https://arxiv.org/abs/2509.21691", "authors": ["Yonghoon Lee", "Zhimei Ren"], "title": "Conditional predictive inference with $L^k$-coverage control", "comment": null, "summary": "We consider the problem of distribution-free conditional predictive\ninference. Prior work has established that achieving exact finite-sample\ncontrol of conditional coverage without distributional assumptions is\nimpossible, in the sense that it necessarily results in trivial prediction\nsets. While several lines of work have proposed methods targeting relaxed\nnotions of conditional coverage guarantee, the inherent difficulty of the\nproblem typically leads such methods to offer only approximate guarantees or\nyield less direct interpretations, even with the relaxations. In this work, we\npropose an inferential target as a relaxed version of conditional predictive\ninference that is achievable with exact distribution-free finite sample\ncontrol, while also offering intuitive interpretations. One of the key ideas,\nthough simple, is to view conditional coverage as a function rather than a\nscalar, and thereby aim to control its function norm. We propose a procedure\nthat controls the $L^k$-norm -- while primarily focusing on the $L^2$-norm --\nof a relaxed notion of conditional coverage, adapting to different approaches\ndepending on the choice of hyperparameter (e.g., local-conditional coverage,\nsmoothed conditional coverage, or conditional coverage for a perturbed sample).\nWe illustrate the performance of our procedure as a tool for conditional\npredictive inference, through simulations and experiments on a real dataset."}
{"id": "2509.21572", "categories": ["stat.ML", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.21572", "abs": "https://arxiv.org/abs/2509.21572", "authors": ["Jakob MÃ¶derl", "Erik Leitinger", "Bernard Henri Fleury"], "title": "General Pruning Criteria for Fast SBL", "comment": "5 pages, 2 figures, submitted to IEEE Signal Processing Letters", "summary": "Sparse Bayesian learning (SBL) associates to each weight in the underlying\nlinear model a hyperparameter by assuming that each weight is Gaussian\ndistributed with zero mean and precision (inverse variance) equal to its\nassociated hyperparameter. The method estimates the hyperparameters by\nmarginalizing out the weights and performing (marginalized) maximum likelihood\n(ML) estimation. SBL returns many hyperparameter estimates to diverge to\ninfinity, effectively setting the estimates of the corresponding weights to\nzero (i.e., pruning the corresponding weights from the model) and thereby\nyielding a sparse estimate of the weight vector.\n  In this letter, we analyze the marginal likelihood as function of a single\nhyperparameter while keeping the others fixed, when the Gaussian assumptions on\nthe noise samples and the weight distribution that underlies the derivation of\nSBL are weakened. We derive sufficient conditions that lead, on the one hand,\nto finite hyperparameter estimates and, on the other, to infinite ones.\nFinally, we show that in the Gaussian case, the two conditions are\ncomplementary and coincide with the pruning condition of fast SBL (F-SBL),\nthereby providing additional insights into this algorithm."}
{"id": "2509.22011", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22011", "abs": "https://arxiv.org/abs/2509.22011", "authors": ["Yessin Moakher", "Malik Tiomoko", "Cosme Louart", "Zhenyu Liao"], "title": "A Random Matrix Perspective of Echo State Networks: From Precise Bias--Variance Characterization to Optimal Regularization", "comment": "2026 IEEE International Conference on Acoustics, Speech, and Signal\n  Processing", "summary": "We present a rigorous asymptotic analysis of Echo State Networks (ESNs) in a\nteacher student setting with a linear teacher with oracle weights. Leveraging\nrandom matrix theory, we derive closed form expressions for the asymptotic\nbias, variance, and mean-squared error (MSE) as functions of the input\nstatistics, the oracle vector, and the ridge regularization parameter. The\nanalysis reveals two key departures from classical ridge regression: (i) ESNs\ndo not exhibit double descent, and (ii) ESNs attain lower MSE when both the\nnumber of training samples and the teacher memory length are limited. We\nfurther provide an explicit formula for the optimal regularization in the\nidentity input covariance case, and propose an efficient numerical scheme to\ncompute the optimum in the general case. Together, these results offer\ninterpretable theory and practical guidelines for tuning ESNs, helping\nreconcile recent empirical observations with provable performance guarantees"}
{"id": "2509.21734", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.21734", "abs": "https://arxiv.org/abs/2509.21734", "authors": ["Chen Cheng", "Xun Huan"], "title": "Optimal Stopping for Sequential Bayesian Experimental Design", "comment": null, "summary": "In sequential Bayesian experimental design, the number of experiments is\nusually fixed in advance. In practice, however, campaigns may terminate early,\nraising the fundamental question: when should one stop? Threshold-based rules\nare simple to implement but inherently myopic, as they trigger termination\nbased on a fixed criterion while ignoring the expected future information gain\nthat additional experiments might provide. We develop a principled Bayesian\nframework for optimal stopping in sequential experimental design, formulated as\na Markov decision process where stopping and design policies are jointly\noptimized. We prove that the optimal rule is to stop precisely when the\nimmediate terminal reward outweighs the expected continuation value. To learn\nsuch policies, we introduce a policy gradient method, but show that na\\\"ive\njoint optimization suffers from circular dependencies that destabilize\ntraining. We resolve this with a curriculum learning strategy that gradually\ntransitions from forced continuation to adaptive stopping. Numerical studies on\na linear-Gaussian benchmark and a contaminant source detection problem\ndemonstrate that curriculum learning achieves stable convergence and\noutperforms vanilla methods, particularly in settings with strong sequential\ndependencies."}
{"id": "2509.21584", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21584", "abs": "https://arxiv.org/abs/2509.21584", "authors": ["Yu Gui", "Cong Ma", "Zongming Ma"], "title": "IndiSeek learns information-guided disentangled representations", "comment": null, "summary": "Learning disentangled representations is a fundamental task in multi-modal\nlearning. In modern applications such as single-cell multi-omics, both shared\nand modality-specific features are critical for characterizing cell states and\nsupporting downstream analyses. Ideally, modality-specific features should be\nindependent of shared ones while also capturing all complementary information\nwithin each modality. This tradeoff is naturally expressed through\ninformation-theoretic criteria, but mutual-information-based objectives are\ndifficult to estimate reliably, and their variational surrogates often\nunderperform in practice. In this paper, we introduce IndiSeek, a novel\ndisentangled representation learning approach that addresses this challenge by\ncombining an independence-enforcing objective with a computationally efficient\nreconstruction loss that bounds conditional mutual information. This\nformulation explicitly balances independence and completeness, enabling\nprincipled extraction of modality-specific features. We demonstrate the\neffectiveness of IndiSeek on synthetic simulations, a CITE-seq dataset and\nmultiple real-world multi-modal benchmarks."}
{"id": "2509.22268", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22268", "abs": "https://arxiv.org/abs/2509.22268", "authors": ["Manli Cheng", "Subha Maity", "Qinglong Tian", "Pengfei Li"], "title": "Transfer Learning under Group-Label Shift: A Semiparametric Exponential Tilting Approach", "comment": null, "summary": "We propose a new framework for binary classification in transfer learning\nsettings where both covariate and label distributions may shift between source\nand target domains. Unlike traditional covariate shift or label shift\nassumptions, we introduce a group-label shift assumption that accommodates\nsubpopulation imbalance and mitigates spurious correlations, thereby improving\nrobustness to real-world distributional changes. To model the joint\ndistribution difference, we adopt a flexible exponential tilting formulation\nand establish mild, verifiable identification conditions via an instrumental\nvariable strategy. We develop a computationally efficient two-step\nlikelihood-based estimation procedure that combines logistic regression for the\nsource outcome model with conditional likelihood estimation using both source\nand target covariates. We derive consistency and asymptotic normality for the\nresulting estimators, and extend the theory to receiver operating\ncharacteristic curves, the area under the curve, and other target functionals,\naddressing the nonstandard challenges posed by plug-in classifiers. Simulation\nstudies demonstrate that our method outperforms existing alternatives under\nsubpopulation shift scenarios. A semi-synthetic application using the\nwaterbirds dataset further confirms the proposed method's ability to transfer\ninformation effectively and improve target-domain classification accuracy."}
{"id": "2509.21800", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.21800", "abs": "https://arxiv.org/abs/2509.21800", "authors": ["Leheng Cai", "Qirui Hu", "Shuyuan Wu"], "title": "Federated Learning of Quantile Inference under Local Differential Privacy", "comment": null, "summary": "In this paper, we investigate federated learning for quantile inference under\nlocal differential privacy (LDP). We propose an estimator based on local\nstochastic gradient descent (SGD), whose local gradients are perturbed via a\nrandomized mechanism with global parameters, making the procedure tolerant of\ncommunication and storage constraints without compromising statistical\nefficiency. Although the quantile loss and its corresponding gradient do not\nsatisfy standard smoothness conditions typically assumed in existing\nliterature, we establish asymptotic normality for our estimator as well as a\nfunctional central limit theorem. The proposed method accommodates data\nheterogeneity and allows each server to operate with an individual privacy\nbudget. Furthermore, we construct confidence intervals for the target value\nthrough a self-normalization approach, thereby circumventing the need to\nestimate additional nuisance parameters. Extensive numerical experiments and\nreal data application validate the theoretical guarantees of the proposed\nmethodology."}
{"id": "2509.21614", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2509.21614", "abs": "https://arxiv.org/abs/2509.21614", "authors": ["Luca Callisti", "Marco Romito", "Francesco Triggiano"], "title": "Effective continuous equations for adaptive SGD: a stochastic analysis view", "comment": null, "summary": "We present a theoretical analysis of some popular adaptive Stochastic\nGradient Descent (SGD) methods in the small learning rate regime. Using the\nstochastic modified equations framework introduced by Li et al., we derive\neffective continuous stochastic dynamics for these methods. Our key\ncontribution is that sampling-induced noise in SGD manifests in the limit as\nindependent Brownian motions driving the parameter and gradient second momentum\nevolutions. Furthermore, extending the approach of Malladi et al., we\ninvestigate scaling rules between the learning rate and key hyperparameters in\nadaptive methods, characterising all non-trivial limiting dynamics."}
{"id": "2509.22341", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22341", "abs": "https://arxiv.org/abs/2509.22341", "authors": ["Anvit Garg", "Sohom Bhattacharya", "Pragya Sur"], "title": "Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression", "comment": "28 pages, 2 figures", "summary": "Model collapse occurs when generative models degrade after repeatedly\ntraining on their own synthetic outputs. We study this effect in\noverparameterized linear regression in a setting where each iteration mixes\nfresh real labels with synthetic labels drawn from the model fitted in the\nprevious iteration. We derive precise generalization error formulae for\nminimum-$\\ell_2$-norm interpolation and ridge regression under this iterative\nscheme. Our analysis reveals intriguing properties of the optimal mixing weight\nthat minimizes long-term prediction error and provably prevents model collapse.\nFor instance, in the case of min-$\\ell_2$-norm interpolation, we establish that\nthe optimal real-data proportion converges to the reciprocal of the golden\nratio for fairly general classes of covariate distributions. Previously, this\nproperty was known only for ordinary least squares, and additionally in low\ndimensions. For ridge regression, we further analyze two popular model classes\n-- the random-effects model and the spiked covariance model -- demonstrating\nhow spectral geometry governs optimal weighting. In both cases, as well as for\nisotropic features, we uncover that the optimal mixing ratio should be at least\none-half, reflecting the necessity of favoring real-data over synthetic. We\nvalidate our theoretical results with extensive simulations."}
{"id": "2509.21904", "categories": ["stat.ME", "q-fin.MF", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2509.21904", "abs": "https://arxiv.org/abs/2509.21904", "authors": ["Yuhong Xu", "Xinyao Zhao"], "title": "General CoVaR Based on Entropy Pooling", "comment": null, "summary": "We propose a general CoVaR framework that extends the traditional CoVaR by\nincorporating diverse expert views and information, such as asset moment\ncharacteristics, quantile insights, and perspectives on the relative loss\ndistribution between two assets. To integrate these expert views effectively\nwhile minimizing deviations from the prior distribution, we employ the entropy\npooling method to derive the posterior distribution, which in turn enables us\nto compute the general CoVaR. Assuming bivariate normal distributions, we\nderive its analytical expressions under various perspectives. Sensitivity\nanalysis reveals that CoVaR exhibits a linear relationship with both the\nexpectations of the variables in the views and the differences in expectations\nbetween them. In contrast, CoVaR shows nonlinear dependencies with respect to\nthe variance, quantiles, and correlation within these views.\n  Empirical analysis of the US banking system during the Federal Reserve's\ninterest rate hikes demonstrates the effectiveness of the general CoVaR when\nexpert views are appropriately specified. Furthermore, we extend this framework\nto the general $\\Delta$CoVaR, which allows for the assessment of risk spillover\neffects from various perspectives."}
{"id": "2509.21707", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.21707", "abs": "https://arxiv.org/abs/2509.21707", "authors": ["Jiawei Shan", "Yiming Dong", "Jiwei Zhao"], "title": "SADA: Safe and Adaptive Inference with Multiple Black-Box Predictions", "comment": null, "summary": "Real-world applications often face scarce labeled data due to the high cost\nand time requirements of gold-standard experiments, whereas unlabeled data are\ntypically abundant. With the growing adoption of machine learning techniques,\nit has become increasingly feasible to generate multiple predicted labels using\na variety of models and algorithms, including deep learning, large language\nmodels, and generative AI. In this paper, we propose a novel approach that\nsafely and adaptively aggregates multiple black-box predictions with unknown\nquality while preserving valid statistical inference. Our method provides two\nkey guarantees: (i) it never performs worse than using the labeled data alone,\nregardless of the quality of the predictions; and (ii) if any one of the\npredictions (without knowing which one) perfectly fits the ground truth, the\nalgorithm adaptively exploits this to achieve either a faster convergence rate\nor the semiparametric efficiency bound. We demonstrate the effectiveness of the\nproposed algorithm through experiments on both synthetic and benchmark\ndatasets."}
{"id": "2509.22446", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22446", "abs": "https://arxiv.org/abs/2509.22446", "authors": ["Lorenzo Testa", "Francesca Chiaromonte", "Kathryn Roeder"], "title": "Rescuing double robustness: safe estimation under complete misspecification", "comment": "24 pages, 4 figures", "summary": "Double robustness is a major selling point of semiparametric and missing data\nmethodology. Its virtues lie in protection against partial nuisance\nmisspecification and asymptotic semiparametric efficiency under correct\nnuisance specification. However, in many applications, complete nuisance\nmisspecification should be regarded as the norm (or at the very least the\nexpected default), and thus doubly robust estimators may behave fragilely. In\nfact, it has been amply verified empirically that these estimators can perform\npoorly when all nuisance functions are misspecified. Here, we first\ncharacterize this phenomenon of double fragility, and then propose a solution\nbased on adaptive correction clipping (ACC). We argue that our ACC proposal is\nsafe, in that it inherits the favorable properties of doubly robust estimators\nunder correct nuisance specification, but its error is guaranteed to be bounded\nby a convex combination of the individual nuisance model errors, which prevents\nthe instability caused by the compounding product of errors of doubly robust\nestimators. We also show that our proposal provides valid inference through the\nparametric bootstrap when nuisances are well-specified. We showcase the\nefficacy of our ACC estimator both through extensive simulations and by\napplying it to the analysis of Alzheimer's disease proteomics data."}
{"id": "2509.22081", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.22081", "abs": "https://arxiv.org/abs/2509.22081", "authors": ["Yeyu Xiao", "Yonghong Long"], "title": "Deep learning for interval-censored failure time data from case-cohort studies", "comment": null, "summary": "Interval-censored data are common in fields such as epidemiology and\ndemography. When the failure event of interest is relatively rare and the\ncollection of covariates is costly, researchers often adopt the case-cohort\ndesign to reduce study costs. However, existing studies typically rely on the\nassumption of linearity in modeling covariates, which may not capture the\ncomplex and nonlinear relationships present in real data. To address this\nlimitation, we consider a class of transformation models with unspecified\ncovariate-dependent functions. We propose a sieve maximum weighted likelihood\napproach for interval-censored data arising from the case-cohort design, which\ncombines deep neural networks with Bernstein polynomials. The method employs a\ndeep neural network to flexibly represent the covariate-dependent function and\nuses Bernstein polynomials to approximate the cumulative baseline hazard\nfunction. We establish the consistency and convergence rate of the proposed\nestimator and show that the resulting nonparametric deep neural network\nestimator attains the minimax optimal rate of convergence (up to a\npolylogarithmic factor). Simulation studies suggest that the proposed method\nperforms well in practice. Finally, we apply the method to a real dataset and\nuse the SHAP (Shapley Additive Explanations) approach to attribute the neural\nnetwork predictions of the covariate-dependent function to covariates. The\nresults indicate that our method is both accurate and interpretable."}
{"id": "2509.21711", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21711", "abs": "https://arxiv.org/abs/2509.21711", "authors": ["Ian Taylor", "Juliane Mueller", "Julie Bessac"], "title": "Multi-modal Bayesian Neural Network Surrogates with Conjugate Last-Layer Estimation", "comment": "35 pages including references and appendix, 5 figures", "summary": "As data collection and simulation capabilities advance, multi-modal learning,\nthe task of learning from multiple modalities and sources of data, is becoming\nan increasingly important area of research. Surrogate models that learn from\ndata of multiple auxiliary modalities to support the modeling of a highly\nexpensive quantity of interest have the potential to aid outer loop\napplications such as optimization, inverse problems, or sensitivity analyses\nwhen multi-modal data are available. We develop two multi-modal Bayesian neural\nnetwork surrogate models and leverage conditionally conjugate distributions in\nthe last layer to estimate model parameters using stochastic variational\ninference (SVI). We provide a method to perform this conjugate SVI estimation\nin the presence of partially missing observations. We demonstrate improved\nprediction accuracy and uncertainty quantification compared to uni-modal\nsurrogate models for both scalar and time series data."}
{"id": "2509.22597", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.TH", "Primary 62G05, 65C60 Secondary 62P30, 62P35, 60D05, 60A10"], "pdf": "https://arxiv.org/pdf/2509.22597", "abs": "https://arxiv.org/abs/2509.22597", "authors": ["Haiyi Shi", "Lei Yang", "Jiarui Chi", "Troy Butler", "Haonan Wang", "Derek Bingham", "Don Estep"], "title": "A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse Problem", "comment": "48 pages, 14 figures", "summary": "The stochastic inverse problem is a key ingredient in making inferences,\npredictions, and decisions for complex science and engineering systems. We\nformulate and analyze a nonparametric Bayesian solution for the stochastic\ninverse problem. Key properties of the solution are proved and the convergence\nand error of a computational solution obtained by random sampling is analyzed.\nSeveral applications illustrate the results."}
{"id": "2509.22217", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.22217", "abs": "https://arxiv.org/abs/2509.22217", "authors": ["Jie Yao", "Kai Zhang", "Eric Rose", "Edward Valachovic"], "title": "Bayesian approach to the PC component", "comment": null, "summary": "Time series with multiple periodically correlated components is a complex\nproblem with comparatively limited prior research. Most existing time series\nmodels are designed to accommodate simple periodically correlated components\nand tend to be sensitive to over-parameterization and optimization issues and\nare also unable to model complex PC components patterns in a time series.\nFrequency separation techniques can be used to maintain the correlation\nstructure of each specific PC component, whereas Bayesian techniques can\ncombine new and existing prior information to update beliefs about these\ncomponents. This study introduces a method to combine the frequency separation\ntechniques and Bayesian techniques to forecast PC and MPC time series data in a\ntwo stage form which is expected to show the new method's suitability in\nmodeling MPC components compared to classical methods."}
{"id": "2509.21866", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21866", "abs": "https://arxiv.org/abs/2509.21866", "authors": ["Erdun Gao", "Jake Fawkes", "Dino Sejdinovic"], "title": "Causal-EPIG: A Prediction-Oriented Active Learning Framework for CATE Estimation", "comment": null, "summary": "Estimating the Conditional Average Treatment Effect (CATE) is often\nconstrained by the high cost of obtaining outcome measurements, making active\nlearning essential. However, conventional active learning strategies suffer\nfrom a fundamental objective mismatch. They are designed to reduce uncertainty\nin model parameters or in observable factual outcomes, failing to directly\ntarget the unobservable causal quantities that are the true objects of\ninterest. To address this misalignment, we introduce the principle of causal\nobjective alignment, which posits that acquisition functions should target\nunobservable causal quantities, such as the potential outcomes and the CATE,\nrather than indirect proxies. We operationalize this principle through the\nCausal-EPIG framework, which adapts the information-theoretic criterion of\nExpected Predictive Information Gain (EPIG) to explicitly quantify the value of\na query in terms of reducing uncertainty about unobservable causal quantities.\nFrom this unified framework, we derive two distinct strategies that embody a\nfundamental trade-off: a comprehensive approach that robustly models the full\ncausal mechanisms via the joint potential outcomes, and a focused approach that\ndirectly targets the CATE estimand for maximum sample efficiency. Extensive\nexperiments demonstrate that our strategies consistently outperform standard\nbaselines, and crucially, reveal that the optimal strategy is\ncontext-dependent, contingent on the base estimator and data complexity. Our\nframework thus provides a principled guide for sample-efficient CATE estimation\nin practice."}
{"id": "2509.22633", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22633", "abs": "https://arxiv.org/abs/2509.22633", "authors": ["Gen Li", "Yuling Yan"], "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback", "comment": null, "summary": "Reinforcement learning with human feedback (RLHF), which learns a reward\nmodel from human preference data and then optimizes a policy to favor preferred\nresponses, has emerged as a central paradigm for aligning large language models\n(LLMs) with human preferences. In this paper, we investigate exploration\nprinciples for online RLHF, where one seeks to adaptively collect new\npreference data to refine both the reward model and the policy in a\ndata-efficient manner. By examining existing optimism-based exploration\nalgorithms, we identify a drawback in their sampling protocol: they tend to\ngather comparisons that fail to reduce the most informative uncertainties in\nreward differences, and we prove lower bounds showing that such methods can\nincur linear regret over exponentially long horizons. Motivated by this\ninsight, we propose a new exploration scheme that directs preference queries\ntoward reducing uncertainty in reward differences most relevant to policy\nimprovement. Under a multi-armed bandit model of RLHF, we establish regret\nbounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter\nthat balances reward maximization against mitigating distribution shift. To our\nknowledge, this is the first online RLHF algorithm with regret scaling\npolynomially in all model parameters."}
{"id": "2509.22235", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.22235", "abs": "https://arxiv.org/abs/2509.22235", "authors": ["Dylan Dijk", "Haeran Cho"], "title": "Tail-robust estimation of factor-adjusted vector autoregressive models for high-dimensional time series", "comment": null, "summary": "We study the problem of modelling high-dimensional, heavy-tailed time series\ndata via a factor-adjusted vector autoregressive (VAR) model, which\nsimultaneously accounts for pervasive co-movements of the variables by a\nhandful of factors, as well as their remaining interconnectedness using a\nsparse VAR model. To accommodate heavy tails, we adopt an element-wise\ntruncation step followed by a two-stage estimation procedure for estimating the\nlatent factors and the VAR parameter matrices. Assuming the existence of the\n$(2 + 2\\epsilon)$-th moment only for some $\\epsilon \\in (0, 1)$, we derive the\nrates of estimation that make explicit the effect of heavy tails through\n$\\epsilon$. Simulation studies and an application in macroeconomics demonstrate\nthe competitive performance of the proposed estimators."}
{"id": "2509.21940", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.21940", "abs": "https://arxiv.org/abs/2509.21940", "authors": ["Ivan Lau", "Jonathan Scarlett"], "title": "Sequential 1-bit Mean Estimation with Near-Optimal Sample Complexity", "comment": null, "summary": "In this paper, we study the problem of distributed mean estimation with 1-bit\ncommunication constraints. We propose a mean estimator that is based on\n(randomized and sequentially-chosen) interval queries, whose 1-bit outcome\nindicates whether the given sample lies in the specified interval. Our\nestimator is $(\\epsilon, \\delta)$-PAC for all distributions with bounded mean\n($-\\lambda \\le \\mathbb{E}(X) \\le \\lambda $) and variance ($\\mathrm{Var}(X) \\le\n\\sigma^2$) for some known parameters $\\lambda$ and $\\sigma$. We derive a sample\ncomplexity bound $\\widetilde{O}\\big(\n\\frac{\\sigma^2}{\\epsilon^2}\\log\\frac{1}{\\delta} +\n\\log\\frac{\\lambda}{\\sigma}\\big)$, which matches the minimax lower bound for the\nunquantized setting up to logarithmic factors and the additional\n$\\log\\frac{\\lambda}{\\sigma}$ term that we show to be unavoidable. We also\nestablish an adaptivity gap for interval-query based estimators: the best\nnon-adaptive mean estimator is considerably worse than our adaptive mean\nestimator for large $\\frac{\\lambda}{\\sigma}$. Finally, we give tightened sample\ncomplexity bounds for distributions with stronger tail decay, and present\nadditional variants that (i) handle an unknown sampling budget (ii) adapt to\nthe unknown true variance given (possibly loose) upper and lower bounds on the\nvariance, and (iii) use only two stages of adaptivity at the expense of more\ncomplicated (non-interval) queries."}
{"id": "2509.22268", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22268", "abs": "https://arxiv.org/abs/2509.22268", "authors": ["Manli Cheng", "Subha Maity", "Qinglong Tian", "Pengfei Li"], "title": "Transfer Learning under Group-Label Shift: A Semiparametric Exponential Tilting Approach", "comment": null, "summary": "We propose a new framework for binary classification in transfer learning\nsettings where both covariate and label distributions may shift between source\nand target domains. Unlike traditional covariate shift or label shift\nassumptions, we introduce a group-label shift assumption that accommodates\nsubpopulation imbalance and mitigates spurious correlations, thereby improving\nrobustness to real-world distributional changes. To model the joint\ndistribution difference, we adopt a flexible exponential tilting formulation\nand establish mild, verifiable identification conditions via an instrumental\nvariable strategy. We develop a computationally efficient two-step\nlikelihood-based estimation procedure that combines logistic regression for the\nsource outcome model with conditional likelihood estimation using both source\nand target covariates. We derive consistency and asymptotic normality for the\nresulting estimators, and extend the theory to receiver operating\ncharacteristic curves, the area under the curve, and other target functionals,\naddressing the nonstandard challenges posed by plug-in classifiers. Simulation\nstudies demonstrate that our method outperforms existing alternatives under\nsubpopulation shift scenarios. A semi-synthetic application using the\nwaterbirds dataset further confirms the proposed method's ability to transfer\ninformation effectively and improve target-domain classification accuracy."}
{"id": "2509.21996", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21996", "abs": "https://arxiv.org/abs/2509.21996", "authors": ["Trinnhallen Brisley", "Gordon Ross", "Daniel Paulin"], "title": "A Nonparametric Discrete Hawkes Model with a Collapsed Gaussian-Process Prior", "comment": null, "summary": "Hawkes process models are used in settings where past events increase the\nlikelihood of future events occurring. Many applications record events as\ncounts on a regular grid, yet discrete-time Hawkes models remain comparatively\nunderused and are often constrained by fixed-form baselines and excitation\nkernels. In particular, there is a lack of flexible, nonparametric treatments\nof both the baseline and the excitation in discrete time. To this end, we\npropose the Gaussian Process Discrete Hawkes Process (GP-DHP), a nonparametric\nframework that places Gaussian process priors on both the baseline and the\nexcitation and performs inference through a collapsed latent representation.\nThis yields smooth, data-adaptive structure without prespecifying trends,\nperiodicities, or decay shapes, and enables maximum a posteriori (MAP)\nestimation with near-linear-time \\(O(T\\log T)\\) complexity. A closed-form\nprojection recovers interpretable baseline and excitation functions from the\noptimized latent trajectory. In simulations, GP-DHP recovers diverse excitation\nshapes and evolving baselines. In case studies on U.S. terrorism incidents and\nweekly Cryptosporidiosis counts, it improves test predictive log-likelihood\nover standard parametric discrete Hawkes baselines while capturing bursts,\ndelays, and seasonal background variation. The results indicate that flexible\ndiscrete-time self-excitation can be achieved without sacrificing scalability\nor interpretability."}
{"id": "2509.22389", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.22389", "abs": "https://arxiv.org/abs/2509.22389", "authors": ["Andrew Redd", "Yujing Gao", "Bonnie B. Smith", "Ravi Varadhan", "Andrea J. Apter", "Daniel O. Scharfstein"], "title": "SensIAT: An R Package for Conducting Sensitivity Analysis of Randomized Trials with Irregular Assessment Times", "comment": null, "summary": "This paper introduces an R package SensIAT that implements a sensitivity\nanalysis methodology, based on augmented inverse intensity weighting, for\nrandomized trials with irregular and potentially informative assessment times.\nTargets of inference involve the population mean outcome in each treatment arm\nas well as the difference in these means (i.e., treatment effect) at specified\ntimes after randomization. This methodology is useful in settings where there\nis concern that study participants are either more, or less, likely to have\nassessments at times when their outcomes are worse. In such settings,\nunadjusted estimates can be biased. The methodology allows researchers to see\nhow inferences are impacted by a range of assumptions about the strength and\ndirection of informative timing in each arm, while incorporating flexible\nsemi-parametric modeling. We describe the functions implemented in SensIAT and\nillustrate them through an analysis of a synthetic dataset motivated by the\nHAP2 asthma randomized clinical trial."}
{"id": "2509.22011", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22011", "abs": "https://arxiv.org/abs/2509.22011", "authors": ["Yessin Moakher", "Malik Tiomoko", "Cosme Louart", "Zhenyu Liao"], "title": "A Random Matrix Perspective of Echo State Networks: From Precise Bias--Variance Characterization to Optimal Regularization", "comment": "2026 IEEE International Conference on Acoustics, Speech, and Signal\n  Processing", "summary": "We present a rigorous asymptotic analysis of Echo State Networks (ESNs) in a\nteacher student setting with a linear teacher with oracle weights. Leveraging\nrandom matrix theory, we derive closed form expressions for the asymptotic\nbias, variance, and mean-squared error (MSE) as functions of the input\nstatistics, the oracle vector, and the ridge regularization parameter. The\nanalysis reveals two key departures from classical ridge regression: (i) ESNs\ndo not exhibit double descent, and (ii) ESNs attain lower MSE when both the\nnumber of training samples and the teacher memory length are limited. We\nfurther provide an explicit formula for the optimal regularization in the\nidentity input covariance case, and propose an efficient numerical scheme to\ncompute the optimum in the general case. Together, these results offer\ninterpretable theory and practical guidelines for tuning ESNs, helping\nreconcile recent empirical observations with provable performance guarantees"}
{"id": "2509.22446", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22446", "abs": "https://arxiv.org/abs/2509.22446", "authors": ["Lorenzo Testa", "Francesca Chiaromonte", "Kathryn Roeder"], "title": "Rescuing double robustness: safe estimation under complete misspecification", "comment": "24 pages, 4 figures", "summary": "Double robustness is a major selling point of semiparametric and missing data\nmethodology. Its virtues lie in protection against partial nuisance\nmisspecification and asymptotic semiparametric efficiency under correct\nnuisance specification. However, in many applications, complete nuisance\nmisspecification should be regarded as the norm (or at the very least the\nexpected default), and thus doubly robust estimators may behave fragilely. In\nfact, it has been amply verified empirically that these estimators can perform\npoorly when all nuisance functions are misspecified. Here, we first\ncharacterize this phenomenon of double fragility, and then propose a solution\nbased on adaptive correction clipping (ACC). We argue that our ACC proposal is\nsafe, in that it inherits the favorable properties of doubly robust estimators\nunder correct nuisance specification, but its error is guaranteed to be bounded\nby a convex combination of the individual nuisance model errors, which prevents\nthe instability caused by the compounding product of errors of doubly robust\nestimators. We also show that our proposal provides valid inference through the\nparametric bootstrap when nuisances are well-specified. We showcase the\nefficacy of our ACC estimator both through extensive simulations and by\napplying it to the analysis of Alzheimer's disease proteomics data."}
{"id": "2509.22124", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22124", "abs": "https://arxiv.org/abs/2509.22124", "authors": ["Malik Tiomoko", "Ekkehard Schnoor"], "title": "Incorporating priors in learning: a random matrix study under a teacher-student framework", "comment": "5 pages, 4 figures", "summary": "Regularized linear regression is central to machine learning, yet its\nhigh-dimensional behavior with informative priors remains poorly understood. We\nprovide the first exact asymptotic characterization of training and test risks\nfor maximum a posteriori (MAP) regression with Gaussian priors centered at a\ndomain-informed initialization. Our framework unifies ridge regression, least\nsquares, and prior-informed estimators, and -- using random matrix theory --\nyields closed-form risk formulas that expose the bias-variance-prior tradeoff,\nexplain double descent, and quantify prior mismatch. We also identify a\nclosed-form minimizer of test risk, enabling a simple estimator of the optimal\nregularization parameter. Simulations confirm the theory with high accuracy. By\nconnecting Bayesian priors, classical regularization, and modern asymptotics,\nour results provide both conceptual clarity and practical guidance for learning\nwith structured prior knowledge."}
{"id": "2509.22474", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.22474", "abs": "https://arxiv.org/abs/2509.22474", "authors": ["Alejandro Calle-Saldarriaga", "Paul F. V. Wiemann", "Matthias Katzfuss"], "title": "Generative multi-fidelity modeling and downscaling via spatial autoregressive transport maps", "comment": "30 pages, 8 figures", "summary": "Spatial fields are often available at multiple fidelities or resolutions,\nwhere high-fidelity data is typically more costly to obtain than low-fidelity\ndata. Statistical surrogates or emulators can predict high-fidelity fields from\ncheap low-fidelity output. We propose a highly scalable Bayesian approach that\ncan learn the joint non-Gaussian distribution and nonlinear dependence\nstructure of nonstationary spatial fields at multiple fidelities from a small\nnumber of training samples. Our method is based on fidelity-aware\nautoregressive GPs with suitably chosen regularization-inducing priors.\nExploiting conjugacy, the integrated likelihood is available in closed form,\nenabling efficient hyperparameter optimization via stochastic gradient descent.\nAfter training, the method also characterizes in closed form the distribution\nof higher-fidelity fields given lower-fidelity data. In our numerical\ncomparisons, we show that our approach substantially outperforms existing\nmethods and that it can be used to characterize and simulate high-fidelity\nfine-scale climate behavior based on output from coarse (low-fidelity) global\ncirculation models."}
{"id": "2509.22341", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22341", "abs": "https://arxiv.org/abs/2509.22341", "authors": ["Anvit Garg", "Sohom Bhattacharya", "Pragya Sur"], "title": "Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression", "comment": "28 pages, 2 figures", "summary": "Model collapse occurs when generative models degrade after repeatedly\ntraining on their own synthetic outputs. We study this effect in\noverparameterized linear regression in a setting where each iteration mixes\nfresh real labels with synthetic labels drawn from the model fitted in the\nprevious iteration. We derive precise generalization error formulae for\nminimum-$\\ell_2$-norm interpolation and ridge regression under this iterative\nscheme. Our analysis reveals intriguing properties of the optimal mixing weight\nthat minimizes long-term prediction error and provably prevents model collapse.\nFor instance, in the case of min-$\\ell_2$-norm interpolation, we establish that\nthe optimal real-data proportion converges to the reciprocal of the golden\nratio for fairly general classes of covariate distributions. Previously, this\nproperty was known only for ordinary least squares, and additionally in low\ndimensions. For ridge regression, we further analyze two popular model classes\n-- the random-effects model and the spiked covariance model -- demonstrating\nhow spectral geometry governs optimal weighting. In both cases, as well as for\nisotropic features, we uncover that the optimal mixing ratio should be at least\none-half, reflecting the necessity of favoring real-data over synthetic. We\nvalidate our theoretical results with extensive simulations."}
{"id": "2509.22499", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2509.22499", "abs": "https://arxiv.org/abs/2509.22499", "authors": ["Yunshu Zhang", "Chan Park", "Jiewen Liu", "Yonghoon Lee", "Mengxin Yu", "James M. Robins", "Eric J. Tchetgen Tchetgen"], "title": "A Multiplicative Instrumental Variable Model for Data Missing Not-at-Random", "comment": "14 pages, 3 figures", "summary": "Instrumental variable (IV) methods offer a valuable approach to account for\noutcome data missing not-at-random. A valid missing data instrument is a\nmeasured factor which (i) predicts the nonresponse process and (ii) is\nindependent of the outcome in the underlying population. For point\nidentification, all existing IV methods for missing data including the\ncelebrated Heckman selection model, a priori restrict the extent of selection\nbias on the outcome scale, therefore potentially understating uncertainty due\nto missing data. In this work, we introduce an IV framework which allows the\ndegree of selection bias on the outcome scale to remain completely\nunrestricted. The new approach instead relies for identification on (iii) a key\nmultiplicative selection model, which posits that the instrument and any hidden\ncommon correlate of selection and the outcome, do not interact on the\nmultiplicative scale. Interestingly, we establish that any regular statistical\nfunctional of the missing outcome is nonparametrically identified under\n(i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald\nratio estimand in causal inference. For estimation and inference, we\ncharacterize the influence function for any functional defined on a\nnonparametric model for the observed data, which we leverage to develop\nsemiparametric multiply robust IV estimators. Several extensions of the methods\nare also considered, including the important practical setting of polytomous\nand continuous instruments. Simulation studies illustrate the favorable finite\nsample performance of proposed methods, which we further showcase in an HIV\nstudy nested within a household health survey study we conducted in Mochudi,\nBotswana, in which interviewer characteristics are used as instruments to\ncorrect for selection bias due to dependent nonresponse in the HIV component of\nthe survey study."}
{"id": "2509.22380", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22380", "abs": "https://arxiv.org/abs/2509.22380", "authors": ["Nikita Kotelevskii", "Maiya Goloburda", "Vladimir Kondratyev", "Alexander Fishkov", "Mohsen Guizani", "Eric Moulines", "Maxim Panov"], "title": "Multidimensional Uncertainty Quantification via Optimal Transport", "comment": null, "summary": "Most uncertainty quantification (UQ) approaches provide a single scalar value\nas a measure of model reliability. However, different uncertainty measures\ncould provide complementary information on the prediction confidence. Even\nmeasures targeting the same type of uncertainty (e.g., ensemble-based and\ndensity-based measures of epistemic uncertainty) may capture different failure\nmodes.\n  We take a multidimensional view on UQ by stacking complementary UQ measures\ninto a vector. Such vectors are assigned with Monge-Kantorovich ranks produced\nby an optimal-transport-based ordering method. The prediction is then deemed\nmore uncertain than the other if it has a higher rank.\n  The resulting VecUQ-OT algorithm uses entropy-regularized optimal transport.\nThe transport map is learned on vectors of scores from in-distribution data\nand, by design, applies to unseen inputs, including out-of-distribution cases,\nwithout retraining.\n  Our framework supports flexible non-additive uncertainty fusion (including\naleatoric and epistemic components). It yields a robust ordering for downstream\ntasks such as selective prediction, misclassification detection,\nout-of-distribution detection, and selective generation. Across synthetic,\nimage, and text data, VecUQ-OT shows high efficiency even when individual\nmeasures fail. The code for the method is available at:\nhttps://github.com/stat-ml/multidimensional_uncertainty."}
{"id": "2509.22501", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22501", "abs": "https://arxiv.org/abs/2509.22501", "authors": ["C. J. R. Murphy-Barltrop", "J. L. Wadsworth", "M. de Carvalho", "B. D. Youngman"], "title": "Modelling non-stationary extremal dependence through a geometric approach", "comment": null, "summary": "Non-stationary extremal dependence, whereby the relationship between the\nextremes of multiple variables evolves over time, is commonly observed in many\nenvironmental and financial data sets. However, most multivariate extreme value\nmodels are only suited to stationary data. A recent approach to multivariate\nextreme value modelling uses a geometric framework, whereby extremal dependence\nfeatures are inferred through the limiting shapes of scaled sample clouds. This\nframework can capture a wide range of dependence structures, and a variety of\ninference procedures have been proposed in the stationary setting. In this\nwork, we first extend the geometric framework to the non-stationary setting and\noutline assumptions to ensure the necessary convergence conditions hold. We\nthen introduce a flexible, semi-parametric modelling framework for obtaining\nestimates of limit sets in the non-stationary setting. Through rigorous\nsimulation studies, we demonstrate that our proposed framework can capture a\nwide range of dependence forms and is robust to different model formulations.\nWe illustrate the proposed methods on financial returns data and present\nseveral practical uses."}
{"id": "2509.22459", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22459", "abs": "https://arxiv.org/abs/2509.22459", "authors": ["Nikita Kornilov", "David Li", "Tikhon Mavrin", "Aleksei Leonov", "Nikita Gushchin", "Evgeny Burnaev", "Iaroslav Koshelev", "Alexander Korotin"], "title": "Universal Inverse Distillation for Matching Models with Real-Data Supervision (No GANs)", "comment": null, "summary": "While achieving exceptional generative quality, modern diffusion, flow, and\nother matching models suffer from slow inference, as they require many steps of\niterative generation. Recent distillation methods address this by training\nefficient one-step generators under the guidance of a pre-trained teacher\nmodel. However, these methods are often constrained to only one specific\nframework, e.g., only to diffusion or only to flow models. Furthermore, these\nmethods are naturally data-free, and to benefit from the usage of real data, it\nis required to use an additional complex adversarial training with an extra\ndiscriminator model. In this paper, we present RealUID, a universal\ndistillation framework for all matching models that seamlessly incorporates\nreal data into the distillation procedure without GANs. Our RealUID approach\noffers a simple theoretical foundation that covers previous distillation\nmethods for Flow Matching and Diffusion models, and is also extended to their\nmodifications, such as Bridge Matching and Stochastic Interpolants."}
{"id": "2509.22543", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.22543", "abs": "https://arxiv.org/abs/2509.22543", "authors": ["Lan Wen", "Aaron L Sarvet"], "title": "Estimating average treatment effects when treatment data are absent in a target study", "comment": null, "summary": "Researchers are frequently interested in understanding the causal effect of\ntreatment interventions. However, in some cases, the treatment of\ninterest--readily available in a randomized controlled trial (RCT)--is either\nnot directly measured or entirely unavailable in observational datasets. This\nchallenge has motivated the development of stochastic incremental propensity\nscore interventions which operate on post-treatment exposures affected by the\ntreatment of interest with the aim of approximating the causal effects of the\ntreatment intervention. Yet, a key challenge lies in the fact that the precise\ndistributional shift of these post-treatment exposures induced by the treatment\nis typically unknown, making it uncertain whether the approximation truly\nreflects the causal effect of interest. The primary objective of this paper is\nto explore data integration methodologies to characterize a distribution of\npost-treatment exposures resulting from the treatment in an external dataset,\nand to use this information to estimate counterfactual mean outcomes under\ntreatment interventions, in settings where the observational data lack\ntreatment information and the external data may not contain measurements of the\noutcome of interest. We will discuss the underlying assumptions required for\nthis approach and provide methodological guidance on estimation strategies to\naddress these challenges."}
{"id": "2509.22467", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22467", "abs": "https://arxiv.org/abs/2509.22467", "authors": ["Alejandro AlmodÃ³var", "Patricia A. ApellÃ¡niz", "Santiago Zazo", "Juan Parras"], "title": "CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks", "comment": null, "summary": "Deep neural networks achieve state-of-the-art performance in estimating\nheterogeneous treatment effects, but their opacity limits trust and adoption in\nsensitive domains such as medicine, economics, and public policy. Building on\nwell-established and high-performing causal neural architectures, we propose\ncausalKANs, a framework that transforms neural estimators of conditional\naverage treatment effects (CATEs) into Kolmogorov--Arnold Networks (KANs). By\nincorporating pruning and symbolic simplification, causalKANs yields\ninterpretable closed-form formulas while preserving predictive accuracy.\nExperiments on benchmark datasets demonstrate that causalKANs perform on par\nwith neural baselines in CATE error metrics, and that even simple KAN variants\nachieve competitive performance, offering a favorable\naccuracy--interpretability trade-off. By combining reliability with analytic\naccessibility, causalKANs provide auditable estimators supported by closed-form\nexpressions and interpretable plots, enabling trustworthy individualized\ndecision-making in high-stakes settings. We release the code for\nreproducibility at https://github.com/aalmodovares/causalkans ."}
{"id": "2509.22597", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.TH", "Primary 62G05, 65C60 Secondary 62P30, 62P35, 60D05, 60A10"], "pdf": "https://arxiv.org/pdf/2509.22597", "abs": "https://arxiv.org/abs/2509.22597", "authors": ["Haiyi Shi", "Lei Yang", "Jiarui Chi", "Troy Butler", "Haonan Wang", "Derek Bingham", "Don Estep"], "title": "A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse Problem", "comment": "48 pages, 14 figures", "summary": "The stochastic inverse problem is a key ingredient in making inferences,\npredictions, and decisions for complex science and engineering systems. We\nformulate and analyze a nonparametric Bayesian solution for the stochastic\ninverse problem. Key properties of the solution are proved and the convergence\nand error of a computational solution obtained by random sampling is analyzed.\nSeveral applications illustrate the results."}
{"id": "2509.22529", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22529", "abs": "https://arxiv.org/abs/2509.22529", "authors": ["Mingyi Zheng", "Hongyu Jiang", "Yizhou Lu", "Jiaye Teng"], "title": "Smoothing-Based Conformal Prediction for Balancing Efficiency and Interpretability", "comment": null, "summary": "Conformal Prediction (CP) is a distribution-free framework for constructing\nstatistically rigorous prediction sets. While popular variants such as CD-split\nimprove CP's efficiency, they often yield prediction sets composed of multiple\ndisconnected subintervals, which are difficult to interpret. In this paper, we\npropose SCD-split, which incorporates smoothing operations into the CP\nframework. Such smoothing operations potentially help merge the subintervals,\nthus leading to interpretable prediction sets. Experimental results on both\nsynthetic and real-world datasets demonstrate that SCD-split balances the\ninterval length and the number of disconnected subintervals. Theoretically,\nunder specific conditions, SCD-split provably reduces the number of\ndisconnected subintervals while maintaining comparable coverage guarantees and\ninterval length compared with CD-split."}
{"id": "2509.21707", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.21707", "abs": "https://arxiv.org/abs/2509.21707", "authors": ["Jiawei Shan", "Yiming Dong", "Jiwei Zhao"], "title": "SADA: Safe and Adaptive Inference with Multiple Black-Box Predictions", "comment": null, "summary": "Real-world applications often face scarce labeled data due to the high cost\nand time requirements of gold-standard experiments, whereas unlabeled data are\ntypically abundant. With the growing adoption of machine learning techniques,\nit has become increasingly feasible to generate multiple predicted labels using\na variety of models and algorithms, including deep learning, large language\nmodels, and generative AI. In this paper, we propose a novel approach that\nsafely and adaptively aggregates multiple black-box predictions with unknown\nquality while preserving valid statistical inference. Our method provides two\nkey guarantees: (i) it never performs worse than using the labeled data alone,\nregardless of the quality of the predictions; and (ii) if any one of the\npredictions (without knowing which one) perfectly fits the ground truth, the\nalgorithm adaptively exploits this to achieve either a faster convergence rate\nor the semiparametric efficiency bound. We demonstrate the effectiveness of the\nproposed algorithm through experiments on both synthetic and benchmark\ndatasets."}
{"id": "2509.22531", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22531", "abs": "https://arxiv.org/abs/2509.22531", "authors": ["Yonghan Jung"], "title": "Debiased Front-Door Learners for Heterogeneous Effects", "comment": "27 pages, 3 figures. Preprint. Code available at\n  https://github.com/yonghanjung/FD-CATE", "summary": "In observational settings where treatment and outcome share unmeasured\nconfounders but an observed mediator remains unconfounded, the front-door (FD)\nadjustment identifies causal effects through the mediator. We study the\nheterogeneous treatment effect (HTE) under FD identification and introduce two\ndebiased learners: FD-DR-Learner and FD-R-Learner. Both attain fast,\nquasi-oracle rates (i.e., performance comparable to an oracle that knows the\nnuisances) even when nuisance functions converge as slowly as n^-1/4. We\nprovide error analyses establishing debiasedness and demonstrate robust\nempirical performance in synthetic studies and a real-world case study of\nprimary seat-belt laws using Fatality Analysis Reporting System (FARS) dataset.\nTogether, these results indicate that the proposed learners deliver reliable\nand sample-efficient HTE estimates in FD scenarios. The implementation is\navailable at https://github.com/yonghanjung/FD-CATE.\n  Keywords: Front-door adjustment; Heterogeneous treatment effects; Debiased\nlearning; Quasi-oracle rates; Causal inference."}
{"id": "2509.22341", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22341", "abs": "https://arxiv.org/abs/2509.22341", "authors": ["Anvit Garg", "Sohom Bhattacharya", "Pragya Sur"], "title": "Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression", "comment": "28 pages, 2 figures", "summary": "Model collapse occurs when generative models degrade after repeatedly\ntraining on their own synthetic outputs. We study this effect in\noverparameterized linear regression in a setting where each iteration mixes\nfresh real labels with synthetic labels drawn from the model fitted in the\nprevious iteration. We derive precise generalization error formulae for\nminimum-$\\ell_2$-norm interpolation and ridge regression under this iterative\nscheme. Our analysis reveals intriguing properties of the optimal mixing weight\nthat minimizes long-term prediction error and provably prevents model collapse.\nFor instance, in the case of min-$\\ell_2$-norm interpolation, we establish that\nthe optimal real-data proportion converges to the reciprocal of the golden\nratio for fairly general classes of covariate distributions. Previously, this\nproperty was known only for ordinary least squares, and additionally in low\ndimensions. For ridge regression, we further analyze two popular model classes\n-- the random-effects model and the spiked covariance model -- demonstrating\nhow spectral geometry governs optimal weighting. In both cases, as well as for\nisotropic features, we uncover that the optimal mixing ratio should be at least\none-half, reflecting the necessity of favoring real-data over synthetic. We\nvalidate our theoretical results with extensive simulations."}
{"id": "2509.22549", "categories": ["stat.ML", "cs.LG", "math.MG"], "pdf": "https://arxiv.org/pdf/2509.22549", "abs": "https://arxiv.org/abs/2509.22549", "authors": ["Mario GÃ³mez", "Guanqun Ma", "Tom Needham", "Bei Wang"], "title": "Metrics for Parametric Families of Networks", "comment": null, "summary": "We introduce a general framework for analyzing data modeled as parameterized\nfamilies of networks. Building on a Gromov-Wasserstein variant of optimal\ntransport, we define a family of parameterized Gromov-Wasserstein distances for\ncomparing such parametric data, including time-varying metric spaces induced by\ncollective motion, temporally evolving weighted social networks, and random\ngraph models. We establish foundational properties of these distances, showing\nthat they subsume several existing metrics in the literature, and derive\ntheoretical approximation guarantees. In particular, we develop computationally\ntractable lower bounds and relate them to graph statistics commonly used in\nrandom graph theory. Furthermore, we prove that our distances can be\nconsistently approximated in random graph and random metric space settings via\nempirical estimates from generative models. Finally, we demonstrate the\npractical utility of our framework through a series of numerical experiments."}
{"id": "2509.22553", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22553", "abs": "https://arxiv.org/abs/2509.22553", "authors": ["Hao Chen", "Lin Liu", "Yu Guang Wang"], "title": "Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement", "comment": null, "summary": "Causal representation learning (CRL) has garnered increasing interests from\nthe causal inference and artificial intelligence community, due to its\ncapability of disentangling potentially complex data-generating mechanism into\ncausally interpretable latent features, by leveraging the heterogeneity of\nmodern datasets. In this paper, we further contribute to the CRL literature, by\nfocusing on the stylized linear structural causal model over the latent\nfeatures and assuming a linear mixing function that maps latent features to the\nobserved data or measurements. Existing linear CRL methods often rely on\nstringent assumptions, such as accessibility to single-node interventional data\nor restrictive distributional constraints on latent features and exogenous\nmeasurement noise. However, these prerequisites can be challenging to satisfy\nin certain scenarios. In this work, we propose a novel linear CRL algorithm\nthat, unlike most existing linear CRL methods, operates under weaker\nassumptions about environment heterogeneity and data-generating distributions\nwhile still recovering latent causal features up to an equivalence class. We\nfurther validate our new algorithm via synthetic experiments and an\ninterpretability analysis of large language models (LLMs), demonstrating both\nits superiority over competing methods in finite samples and its potential in\nintegrating causality into AI."}
{"id": "2509.22633", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22633", "abs": "https://arxiv.org/abs/2509.22633", "authors": ["Gen Li", "Yuling Yan"], "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback", "comment": null, "summary": "Reinforcement learning with human feedback (RLHF), which learns a reward\nmodel from human preference data and then optimizes a policy to favor preferred\nresponses, has emerged as a central paradigm for aligning large language models\n(LLMs) with human preferences. In this paper, we investigate exploration\nprinciples for online RLHF, where one seeks to adaptively collect new\npreference data to refine both the reward model and the policy in a\ndata-efficient manner. By examining existing optimism-based exploration\nalgorithms, we identify a drawback in their sampling protocol: they tend to\ngather comparisons that fail to reduce the most informative uncertainties in\nreward differences, and we prove lower bounds showing that such methods can\nincur linear regret over exponentially long horizons. Motivated by this\ninsight, we propose a new exploration scheme that directs preference queries\ntoward reducing uncertainty in reward differences most relevant to policy\nimprovement. Under a multi-armed bandit model of RLHF, we establish regret\nbounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter\nthat balances reward maximization against mitigating distribution shift. To our\nknowledge, this is the first online RLHF algorithm with regret scaling\npolynomially in all model parameters."}
{"id": "2509.21800", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.21800", "abs": "https://arxiv.org/abs/2509.21800", "authors": ["Leheng Cai", "Qirui Hu", "Shuyuan Wu"], "title": "Federated Learning of Quantile Inference under Local Differential Privacy", "comment": null, "summary": "In this paper, we investigate federated learning for quantile inference under\nlocal differential privacy (LDP). We propose an estimator based on local\nstochastic gradient descent (SGD), whose local gradients are perturbed via a\nrandomized mechanism with global parameters, making the procedure tolerant of\ncommunication and storage constraints without compromising statistical\nefficiency. Although the quantile loss and its corresponding gradient do not\nsatisfy standard smoothness conditions typically assumed in existing\nliterature, we establish asymptotic normality for our estimator as well as a\nfunctional central limit theorem. The proposed method accommodates data\nheterogeneity and allows each server to operate with an individual privacy\nbudget. Furthermore, we construct confidence intervals for the target value\nthrough a self-normalization approach, thereby circumventing the need to\nestimate additional nuisance parameters. Extensive numerical experiments and\nreal data application validate the theoretical guarantees of the proposed\nmethodology."}
{"id": "2509.22446", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.22446", "abs": "https://arxiv.org/abs/2509.22446", "authors": ["Lorenzo Testa", "Francesca Chiaromonte", "Kathryn Roeder"], "title": "Rescuing double robustness: safe estimation under complete misspecification", "comment": "24 pages, 4 figures", "summary": "Double robustness is a major selling point of semiparametric and missing data\nmethodology. Its virtues lie in protection against partial nuisance\nmisspecification and asymptotic semiparametric efficiency under correct\nnuisance specification. However, in many applications, complete nuisance\nmisspecification should be regarded as the norm (or at the very least the\nexpected default), and thus doubly robust estimators may behave fragilely. In\nfact, it has been amply verified empirically that these estimators can perform\npoorly when all nuisance functions are misspecified. Here, we first\ncharacterize this phenomenon of double fragility, and then propose a solution\nbased on adaptive correction clipping (ACC). We argue that our ACC proposal is\nsafe, in that it inherits the favorable properties of doubly robust estimators\nunder correct nuisance specification, but its error is guaranteed to be bounded\nby a convex combination of the individual nuisance model errors, which prevents\nthe instability caused by the compounding product of errors of doubly robust\nestimators. We also show that our proposal provides valid inference through the\nparametric bootstrap when nuisances are well-specified. We showcase the\nefficacy of our ACC estimator both through extensive simulations and by\napplying it to the analysis of Alzheimer's disease proteomics data."}
{"id": "2509.22501", "categories": ["stat.ME", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22501", "abs": "https://arxiv.org/abs/2509.22501", "authors": ["C. J. R. Murphy-Barltrop", "J. L. Wadsworth", "M. de Carvalho", "B. D. Youngman"], "title": "Modelling non-stationary extremal dependence through a geometric approach", "comment": null, "summary": "Non-stationary extremal dependence, whereby the relationship between the\nextremes of multiple variables evolves over time, is commonly observed in many\nenvironmental and financial data sets. However, most multivariate extreme value\nmodels are only suited to stationary data. A recent approach to multivariate\nextreme value modelling uses a geometric framework, whereby extremal dependence\nfeatures are inferred through the limiting shapes of scaled sample clouds. This\nframework can capture a wide range of dependence structures, and a variety of\ninference procedures have been proposed in the stationary setting. In this\nwork, we first extend the geometric framework to the non-stationary setting and\noutline assumptions to ensure the necessary convergence conditions hold. We\nthen introduce a flexible, semi-parametric modelling framework for obtaining\nestimates of limit sets in the non-stationary setting. Through rigorous\nsimulation studies, we demonstrate that our proposed framework can capture a\nwide range of dependence forms and is robust to different model formulations.\nWe illustrate the proposed methods on financial returns data and present\nseveral practical uses."}
