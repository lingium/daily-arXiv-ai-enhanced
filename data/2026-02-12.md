<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 17]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.ML](#stat.ML) [Total: 11]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Optimal information deletion and Bayes' theorem](https://arxiv.org/abs/2602.09061)
*Hans Montcho,Håvard Rue*

Main category: stat.ME

TL;DR: 论文重新审视了贝叶斯定理作为最优信息处理规则，从信息删除的角度探讨了当数据被移除时如何将后验分布更新为前数据分布，证明了最优信息删除规则与传统贝叶斯定理一致。


<details>
  <summary>Details</summary>
Motivation: 受到Zellner关于贝叶斯定理作为最优信息处理规则的经典工作的启发，作者希望从信息删除的角度重新审视这些思想。当数据被移除时，需要一种规则来更新后验分布为前数据分布，这引发了关于信息删除规则的研究。

Method: 作者研究了信息删除规则，特别是那些在数据被移除时不破坏或创建信息的规则。通过理论分析，证明了最优信息删除规则的存在性和性质。

Result: 研究发现，最优信息删除规则与传统贝叶斯定理完全一致。这意味着贝叶斯定理不仅是最优的信息处理规则，也是最优的信息删除规则。

Conclusion: 贝叶斯定理在信息处理和信息删除两方面都具有最优性，这一发现为广义变分推断提供了理论基础，并深化了我们对贝叶斯统计的理解。

Abstract: In this same journal, Arnold Zellner published a seminal paper on Bayes' theorem as an optimal information processing rule. This result led to the variational formulation of Bayes' theorem, which is the central idea in generalized variational inference. Almost 40 years later, we revisit these ideas, but from the perspective of information deletion. We investigate rules which update a posterior distribution into an antedata distribution when a portion of data is removed. In such context, a rule which does not destroy or create information is called the optimal information deletion rule and we prove that it coincides with the traditional use of Bayes' theorem.

</details>


### [2] [Estimating causal effects of functional treatments with modified functional treatment policies](https://arxiv.org/abs/2602.09145)
*Ziren Jiang,Erjia Cui,Jared D. Huling*

Main category: stat.ME

TL;DR: 本文提出了一种新的因果估计量——修正功能治疗策略（MFTP），用于功能治疗数据的因果推断，通过功能主成分分析定义功能变量的总体平均，解决了无限维对象无密度函数的挑战。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究中功能数据日益普遍，但功能治疗的因果推断研究不足。现有方法主要关注因果平均剂量反应函数（ADRF），需要强正性假设且可解释性有限。需要开发新的因果估计量来克服这些限制。

Method: 提出修正功能治疗策略（MFTP）作为新的因果估计量，关注个体轻微修改其治疗轨迹时的平均潜在结果。通过功能主成分分析（FPCA）分解定义功能变量的总体平均，建立了MFTP的因果可识别性。进一步推导了结果回归、逆概率加权和双重稳健估计器。

Result: 在温和正则条件下提供了理论保证，通过广泛的模拟研究验证了所提估计器的有效性。将MFTP框架应用于NHANES加速度计数据，估计了减少夜间破坏性活动和低活动持续时间对全因死亡率的因果效应。

Conclusion: MFTP为功能治疗数据的因果推断提供了一种新的、可解释性更强的框架，克服了传统ADRF方法的限制，在生物医学研究中具有实际应用价值。

Abstract: Functional data are increasingly prevalent in biomedical research. While functional data analysis has been established for decades, causal inference with functional treatments remains largely unexplored. Existing methods typically focus on estimating the causal average dose response functional (ADRF), which requires strong positivity assumptions and offers limited interpretability. In this work, we target a new causal estimand, the modified functional treatment policy (MFTP), which focuses on estimating the average potential outcome when each individual slightly modifies their treatment trajectory from the observed one. A major challenge for this new estimand is the need to define an average over an infinite-dimensional object with no density. By proposing a novel definition of the population average over a functional variable using a functional principal component analysis (FPCA) decomposition, we establish the causal identifiability of the MFTP estimand. We further derive outcome regression, inverse probability weighting, and doubly robust estimators for the MFTP, and provide theoretical guarantees under mild regularity conditions. The proposed estimators are validated through extensive simulation studies. Applying our MFTP framework to the National Health and Nutrition Examination Survey (NHANES) accelerometer data, we estimate the causal effects of reducing disruptive nighttime activity and low-activity duration on all-cause mortality.

</details>


### [3] [Mean regression for (0,1) responses via beta scale mixtures](https://arxiv.org/abs/2602.09167)
*Arno Otto,Andriëtte Bekker,Johan Ferreira,Lebogang Rathebe*

Main category: stat.ME

TL;DR: 提出beta尺度混合模型，通过将条件beta分布的尺度参数乘以一个混合随机变量来增强对重尾有界响应的建模灵活性


<details>
  <summary>Details</summary>
Motivation: 传统beta回归模型在建模有界单位域响应时灵活性有限，特别是对于重尾分布的情况。需要一种更灵活的模型来捕捉更广泛的偏度和峰度范围

Method: 提出beta尺度混合模型，将条件beta分布的尺度参数乘以一个混合随机变量，该变量取值于正实数线或其子集，其分布由控制尾部行为的单一参数决定

Result: 在模拟数据和真实数据集上的实验表明，beta尺度混合模型相对于经典beta回归模型和其他竞争方法，在建模有界单位域响应方面表现出优越性能

Conclusion: beta尺度混合模型为建模重尾有界响应提供了更灵活的框架，能够捕捉更广泛的偏度和峰度范围，在实际应用中具有优势

Abstract: To achieve a greater general flexibility for modeling heavy-tailed bounded responses, a beta scale mixture model is proposed. Each member of the family is obtained by multiplying the scale parameter of the conditional beta distribution by a mixing random variable taking values on all or part of the positive real line and whose distribution depends on a single parameter governing the tail behavior of the resulting compound distribution. These family members allow for a wider range of values for skewness and kurtosis. To validate the effectiveness of the proposed model, we conduct experiments on both simulated data and real datasets. The results indicate that the beta scale mixture model demonstrates superior performance relative to the classical beta regression model and alternative competing methods for modeling responses on the bounded unit domain.

</details>


### [4] [Continuous mixtures of Gaussian processes as models for spatial extremes](https://arxiv.org/abs/2602.09512)
*Lorenzo Dell'Oro,Carlo Gaetan,Thomas Opitz*

Main category: stat.ME

TL;DR: 论文提出基于高斯位置尺度混合模型的空间极值建模方法，开发了新的构造和条件模拟算法，并提出了扩展的极值模型以同时处理尾部和主体数据，避免了需要明确选择极端事件的问题。


<details>
  <summary>Details</summary>
Motivation: 空间极值建模对于研究不同位置极端事件联合发生的风险具有重要意义，特别是在气候和环境科学中。现有高斯过程模型存在对称性、轻尾和弱尾依赖等局限性，需要更灵活的模型来同时处理极端值和主体数据。

Method: 采用高斯位置尺度混合模型，开发了具有有趣特征的新构造方法，提出了条件模拟的一般算法。扩展了极值模型以同时建模尾部和主体数据，并提出了参数模型中似然推断的新解决方案，避免了潜在位置和尺度变量带来的计算瓶颈。

Result: 通过模拟数据验证了模型和推断方法的有效性，并在葡萄牙野火相关天气变量的应用中展示了实际效果。模型不仅适用于空间数据，也可直接用于多元（非空间）数据建模。

Conclusion: 高斯位置尺度混合模型为空间极值建模提供了灵活框架，能够克服传统高斯过程的局限性，同时处理极端值和主体数据，避免了需要明确选择极端事件的问题，在实际应用中表现出良好效果。

Abstract: Spatial modelling of extreme values allows studying the risk of joint occurrence of extreme events at different locations and is of significant interest in climatic and other environmental sciences. A popular class of dependence models for spatial extremes is that of random location-scale mixtures, in which a spatial "baseline" process is multiplied or shifted by a random variable, potentially altering its extremal dependence behaviour. Gaussian location-scale mixtures retain benefits of their Gaussian baseline processes while overcoming some of their limitations, such as symmetry, light tails and weak tail dependence. We review properties of Gaussian location-scale mixtures and develop novel constructions with interesting features, together with a general algorithm for conditional simulation from these models. We leverage their flexibility to propose extended extreme-value models, that allow for appropriately modelling not only the tails but also the bulk of the data. This is important in many applications and avoids the need to explicitly select the events considered as extreme. We propose new solutions for likelihood inference in parametric models of Gaussian location-scale mixtures, in order to avoid the numerical bottleneck given by the latent location and scale variables that can lead to high computational cost of standard likelihood evaluations. The effectiveness of the models and of the inference methods is confirmed with simulated data examples, and we present an application to wildfire-related weather variables in Portugal. Although not detailed here, the approaches would also be straightforward to use for modelling multivariate (non spatial) data.

</details>


### [5] [Some Bayesian Perspectives on Clinical Trials](https://arxiv.org/abs/2602.09208)
*Alexandra Sokolova,Vadim Sokolov,Nick Polson*

Main category: stat.ME

TL;DR: 本文通过贝叶斯框架分析三个临床试验，提出精确逆向归纳方法用于双臂二分类试验，在保持样本量节约的同时提高统计功效，特别适用于罕见病和儿科等患者稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个统一的贝叶斯框架来分析临床试验，解决传统方法在样本量优化和统计功效之间的权衡问题，特别是在患者稀缺的临床环境中。

Method: 方法包括：1）使用贝叶斯框架分析三个标志性临床试验；2）提出精确逆向归纳方法用于双臂二分类试验，利用Beta-Binomial共轭性；3）通过Pólya-Gamma扩展连接协变量调整逻辑回归；4）开发校准变体将声明阈值嵌入终端效用函数。

Result: 结果：1）ECMO试验的治疗优势后验概率在不同先验下稳健；2）CALGB试验的预测概率监测将入组人数从1800减少到633；3）I-SPY~2试验的适应性富集使23个臂中的9个进入III期；4）最优贝叶斯设计将每臂样本量减少到14-26（对比42-100），但功效较低；5）校准变体在保持样本量节约的同时提高功效。

Conclusion: 结论：提出的贝叶斯方法在患者稀缺环境中特别有价值，通过精确逆向归纳和校准变体实现了样本量节约和统计功效的平衡，对FDA 2026年贝叶斯方法指南有重要启示。

Abstract: We examine three landmark clinical trials -- ECMO, CALGB~49907, and I-SPY~2 -- through a unified Bayesian framework connecting prior specification, sequential adaptation, and decision-theoretic optimisation. For ECMO, the posterior probability of treatment superiority is robust across the range of priors examined. For CALGB, predictive probability monitoring stopped enrolment at 633 instead of 1800 patients. For I-SPY~2, adaptive enrichment graduated nine of 23 arms to Phase~III. These case studies motivate a methodological contribution: exact backward induction for two-arm binary trials, where Beta-Binomial conjugacy yields closed-form transitions on the integer lattice of success counts with no quadrature. A Pólya-Gamma augmentation bridges this to covariate-adjusted logistic regression. Simulation reveals a fundamental tension: the optimal Bayesian design reduces expected sample sizes to 14--26 per arm (versus 42--100 for alternatives) but with substantially lower power. A calibrated variant embedding the declaration threshold in the terminal utility improves power while maintaining sample-size savings; varying the per-stage cost traces a power frontier for selecting the preferred operating point, with suitability highest in patient-sparing contexts such as rare diseases and paediatrics. The Pólya-Gamma Laplace approximation is validated against exact calculations (mean absolute error below 0.01). We discuss implications for the 2026 FDA draft guidance on Bayesian methodology.

</details>


### [6] [Stochastic EM Estimation and Inference for Zero-Inflated Beta-Binomial Mixed Models for Longitudinal Count Data](https://arxiv.org/abs/2602.09279)
*John Barrera,Ana Arribas-Gil,Dae-Jin Lee,Cristian Meza*

Main category: stat.ME

TL;DR: 提出ZIBBMR模型处理过离散、零膨胀的纵向计数数据，结合beta-binomial分布、零膨胀成分和随机效应，使用SAEM算法进行估计，在微生物组数据分析中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准计数模型（如泊松或负二项混合效应模型）无法充分处理过离散、零膨胀的纵向计数数据，这些数据在微生物组学等领域常见，需要更灵活的建模方法。

Method: 提出零膨胀beta-binomial混合效应回归（ZIBBMR）模型，结合beta-binomial计数模型、零膨胀成分、协变量固定效应和个体特异性随机效应。使用带有潜变量增强的随机逼近EM（SAEM）算法进行最大似然估计，解决模型似然函数难以处理的问题。

Result: 模拟研究表明，ZIBBMR在准确性上与文献中领先的混合模型方法相当，并优于简单的零膨胀计数模型，特别是在小样本情况下。在微生物组数据分析中，ZIBBMR与外部零膨胀beta回归（ZIBR）基准比较表明，同时应用计数和比例模型可以增强推断的稳健性。

Conclusion: ZIBBMR模型为分析过离散、零膨胀的纵向计数数据提供了有效的框架，特别适用于微生物组学等领域，结合计数和比例模型的方法可以提高数据分析的可靠性。

Abstract: Analyzing overdispersed, zero-inflated, longitudinal count data poses significant modeling and computational challenges, which standard count models (e.g., Poisson or negative binomial mixed effects models) fail to adequately address. We propose a Zero-Inflated Beta-Binomial Mixed Effects Regression (ZIBBMR) model that augments a beta-binomial count model with a zero-inflation component, fixed effects for covariates, and subject-specific random effects, accommodating excessive zeros, overdispersion, and within-subject correlation. Maximum likelihood estimation is performed via a Stochastic Approximation EM (SAEM) algorithm with latent variable augmentation, which circumvents the model's intractable likelihood and enables efficient computation. Simulation studies show that ZIBBMR achieves accuracy comparable to leading mixed-model approaches in the literature and surpasses simpler zero-inflated count formulations, particularly in small-sample scenarios. As a case study, we analyze longitudinal microbiome data, comparing ZIBBMR with an external Zero-Inflated Beta Regression (ZIBR) benchmark; the results indicate that applying both count- and proportion-based models in parallel can enhance inference robustness when both data types are available.

</details>


### [7] [Supervised Learning of Functional Outcomes with Predictors at Different Scales: A Functional Gaussian Process Approach](https://arxiv.org/abs/2602.09351)
*R. Jacob Andros,Rajarshi Guhaniyogi,Devin Francom,Donatella Pasqualini*

Main category: stat.ME

TL;DR: 本文提出了一种新颖的监督学习框架，用于分析复杂计算机模拟中的功能数据，该框架能够同时处理跨模拟运行固定的领域特定功能预测因子和在运行间变化的实现特定全局预测因子。


<details>
  <summary>Details</summary>
Motivation: 复杂计算机模拟分析（常涉及功能数据）面临独特的统计挑战。传统回归方法（如函数对函数回归）通常在每次实现基础上将功能结果与标量和功能预测因子关联。然而，模拟研究需要更细致的方法来解开功能结果与在多个尺度上观察到的预测因子之间的非线性关系：跨模拟运行固定的领域特定功能预测因子，以及在运行间变化的实现特定全局预测因子。

Method: 开发了一个新颖的监督学习框架，提出一个加法非线性回归模型，灵活捕捉两类预测因子的影响。功能预测因子的效应通过高斯过程先验控制的空间变化系数建模。关键创新是引入功能高斯过程（fGP）先验来捕捉全局预测因子对功能结果的影响，该先验联合建模整个未知的空间索引非线性函数集合，明确考虑其空间依赖性。

Result: 该集成架构能够同时从两类预测因子中学习，提供量化它们各自在预测功能结果中贡献的原则性策略，并为模型参数和预测提供严格的 uncertainty 估计。方法的实用性和鲁棒性通过多个合成数据集和涉及飓风海、湖和陆上风暴潮（SLOSH）模型输出的真实世界应用得到验证。

Conclusion: 本文提出的框架为分析复杂计算机模拟中的功能数据提供了一种强大的统计方法，特别适用于需要同时考虑固定领域特征和变化全局因素的场景，在合成数据和真实SLOSH模型应用中均表现出良好性能。

Abstract: The analysis of complex computer simulations, often involving functional data, presents unique statistical challenges. Conventional regression methods, such as function-on-function regression, typically associate functional outcomes with both scalar and functional predictors on a per-realization basis. However, simulation studies often demand a more nuanced approach to disentangle nonlinear relationships of functional outcome with predictors observed at multiple scales: domain-specific functional predictors that are fixed across simulation runs, and realization-specific global predictors that vary between runs. In this article, we develop a novel supervised learning framework tailored to this setting. We propose an additive nonlinear regression model that flexibly captures the influence of both predictor types. The effects of functional predictors are modeled through spatially-varying coefficients governed by a Gaussian process prior. Crucially, to capture the impact of global predictors on the functional outcome, we introduce a functional Gaussian process (fGP) prior. This new prior jointly models the entire collection of unknown, spatially-indexed nonlinear functions that encode the effects of the global predictors over the entire domain, explicitly accounting for their spatial dependence. This integrated architecture enables simultaneous learning from both predictor types, provides a principled strategies to quantify their respective contributions in predicting the functional outcome, and delivers rigorous uncertainty estimates for both model parameters and predictions. The utility and robustness of our approach are demonstrated through multiple synthetic datasets and a real-world application involving outputs from the Sea, Lake, and Overland Surges from Hurricanes (SLOSH) model.

</details>


### [8] [A joint QoL-Survival framework with debiased estimation under truncation by death](https://arxiv.org/abs/2602.09537)
*Torben Martinussen,Klaus K. Holst,Christian Bressen Pipper,Per Kragh Andersen*

Main category: stat.ME

TL;DR: 提出一个联合建模生存时间和生活质量(QoL)的框架，避免对死亡后的QoL进行外推，使用单纯形可视化结果，开发半参数估计器


<details>
  <summary>Details</summary>
Motivation: 在高死亡风险人群中评估QoL结果存在挑战，因为QoL在死亡个体上未定义，传统方法可能对死亡后的QoL进行不合理外推

Method: 基于多状态模型思想，将二元健康状态与死亡的联合表征扩展到连续QoL结果；开发基于有效影响函数的半参数估计器，支持机器学习方法

Result: 开发了灵活、根号n一致的估计器，通过模拟研究和两个真实数据应用验证了方法的有效性

Conclusion: 提出的框架能同时考虑生存和QoL的联合分布，避免对死亡后QoL的强假设，为高死亡风险人群的QoL评估提供了更合理的统计方法

Abstract: Evaluating quality-of-life (QoL) outcomes in populations with high mortality risk is complicated by truncation by death, since QoL is undefined for individuals who do not survive to the planned measurement time. We propose a framework that jointly models the distribution of QoL and survival without extrapolating QoL beyond death. Inspired by multistate formulations, we extend the joint characterization of binary health states and mortality to continuous QoL outcomes. Because treatment effects cannot be meaningfully summarized in a single one-dimensional estimand without strong assumptions, our approach simultaneously considers both survival and the joint distribution of QoL and survival with the latter conveniently displayed in a simplex. We develop assumption-lean, semiparametric estimators based on efficient influence functions, yielding flexible, root-n consistent estimators that accommodate machine-learning methods while making transparent the conditions these must satisfy. The proposed method is illustrated through simulation studies and two real-data applications.

</details>


### [9] [Online Selective Conformal Prediction with Asymmetric Rules: A Permutation Test Approach](https://arxiv.org/abs/2602.10018)
*Mingyi Zheng,Ying Jin*

Main category: stat.ME

TL;DR: 提出PEMI框架，用于处理任意不对称选择规则的在线选择性共形预测，通过排列保持选择事件实现有限样本精确的选择条件覆盖


<details>
  <summary>Details</summary>
Motivation: 现有在线选择性共形预测方法只能处理有限的选择机制，因为在线设置中数据顺序到达且后续决策依赖先前决策，导致选择机制自然不对称

Method: 提出PEMI（基于排列的Mondrian共形推断）框架，识别导致相同选择事件的所有数据排列（或其蒙特卡洛子集），并使用该选择保持参考集上的符合性分数校准预测集

Result: 在标准可交换性条件下，PEMI预测集对任何不对称选择机制和任何预测模型都能实现有限样本精确的选择条件覆盖，并能有效控制FCR

Conclusion: PEMI为在线选择性共形预测提供了一个通用框架，能够处理任意不对称选择规则，并在真实药物发现数据集和模拟中表现出良好性能

Abstract: Selective conformal prediction aims to construct prediction sets with valid coverage for a test unit conditional on it being selected by a data-driven mechanism. While existing methods in the offline setting handle any selection mechanism that is permutation invariant to the labeled data, their extension to the online setting -- where data arrives sequentially and later decisions depend on earlier ones -- is challenged by the fact that the selection mechanism is naturally asymmetric. As such, existing methods only address a limited collection of selection mechanisms.
  In this paper, we propose PErmutation-based Mondrian Conformal Inference (PEMI), a general permutation-based framework for selective conformal prediction with arbitrary asymmetric selection rules. Motivated by full and Mondrian conformal prediction, PEMI identifies all permutations of the observed data (or a Monte-Carlo subset thereof) that lead to the same selection event, and calibrates a prediction set using conformity scores over this selection-preserving reference set. Under standard exchangeability conditions, our prediction sets achieve finite-sample exact selection-conditional coverage for any asymmetric selection mechanism and any prediction model. PEMI naturally incorporates additional offline labeled data, extends to selection mechanisms with multiple test samples, and achieves FCR control with fine-grained selection taxonomies. We further work out several efficient instantiations for commonly-used online selection rules, including covariate-based rules, conformal p/e-values-based procedures, and selection based on earlier outcomes. Finally, we demonstrate the efficacy of our methods across various selection rules on a real drug discovery dataset and investigate their performance via simulations.

</details>


### [10] [High Dimensional Mean Test for Shrinking Random Variables with Applications to Backtesting](https://arxiv.org/abs/2602.09542)
*Liujun Chen,Chen Zhou*

Main category: stat.ME

TL;DR: 提出一种针对收缩随机变量的高维均值检验框架，通过跨维度子集聚合观测来估计子集均值，检验最大绝对均值是否偏离零，克服简单平均的抵消效应，即使在边缘渐近正态性失效时也有效。


<details>
  <summary>Details</summary>
Motivation: 在高维设置中，当随机变量随着样本量增加而收缩到零时，传统的均值检验方法可能因抵消效应而失效。需要一种能克服简单平均局限性的检验方法，即使在边缘渐近正态性不成立时也能保持有效性。

Method: 通过跨维度重叠子集聚合观测来估计子集均值，检验最大绝对均值是否偏离零。建立检验统计量的理论性质，并开发乘数自举程序来近似其分布。

Result: 该方法在高维设置中表现出优越性能，模拟结果显示其优于其他方法。实际数据应用证明了其在风险价值回测中的实际有效性。

Conclusion: 该方法为风险价值的验证和比较回测提供了一个灵活且强大的工具，能够有效处理高维收缩随机变量，克服传统均值检验的局限性。

Abstract: We propose a high dimensional mean test framework for shrinking random variables, where the underlying random variables shrink to zero as the sample size increases. By pooling observations across overlapping subsets of dimensions, we estimate subsets means and test whether the maximum absolute mean deviates from zero. This approach overcomes cancellations that occur in simple averaging and remains valid even when marginal asymptotic normality fails. We establish theoretical properties of the test statistic and develop a multiplier bootstrap procedure to approximate its distribution. The method provides a flexible and powerful tool for the validation and comparative backtesting of value-at-risk. Simulations show superior performance in high-dimensional settings, and a real-data application demonstrates its practical effectiveness in backtesting.

</details>


### [11] [Sharp Bounds for Treatment Effect Generalization under Outcome Distribution Shift](https://arxiv.org/abs/2602.09595)
*Amir Asiaee,Samhita Pal,Cole Beck,Jared D. Huling*

Main category: stat.ME

TL;DR: 开发了一个敏感性分析框架，用于在随机试验结果推广到目标人群时，当未测量的效应修饰因子分布不同时，界定结论可能的变化范围。


<details>
  <summary>Details</summary>
Motivation: 将随机试验的治疗效果推广到目标人群时，通常假设在控制观测协变量后，潜在结果分布在人群间不变。但当未测量的效应修饰因子在试验参与者和目标人群间分布不同时，这一假设会失效，需要评估结论对假设违反的敏感性。

Method: 通过标量参数Λ≥1约束目标人群和试验结果密度之间的似然比，Λ=1对应标准可迁移性假设。对于每个Λ，推导目标平均处理效应的尖锐边界——在观测数据和敏感性模型兼容的所有数据生成过程中，保证包含真实效应的最紧区间。最优似然比具有简单的阈值结构，通过仅需排序试验结果和重新分配概率质量的闭式贪心算法实现。

Result: 算法时间复杂度为O(n log n)，在标准正则条件下具有一致性。模拟显示：当真实结果偏移在指定Λ范围内时，边界达到名义覆盖率；比最坏情况边界显著更紧；在现实可迁移性违反范围内保持信息性。

Conclusion: 提出了一个实用的敏感性分析框架，用于评估治疗效果推广对未测量效应修饰因子分布差异的稳健性，提供了计算高效、理论保证且在实际应用中信息性强的边界估计方法。

Abstract: Generalizing treatment effects from a randomized trial to a target population requires the assumption that potential outcome distributions are invariant across populations after conditioning on observed covariates. This assumption fails when unmeasured effect modifiers are distributed differently between trial participants and the target population. We develop a sensitivity analysis framework that bounds how much conclusions can change when this transportability assumption is violated. Our approach constrains the likelihood ratio between target and trial outcome densities by a scalar parameter $Λ\geq 1$, with $Λ= 1$ recovering standard transportability. For each $Λ$, we derive sharp bounds on the target average treatment effect -- the tightest interval guaranteed to contain the true effect under all data-generating processes compatible with the observed data and the sensitivity model. We show that the optimal likelihood ratios have a simple threshold structure, leading to a closed-form greedy algorithm that requires only sorting trial outcomes and redistributing probability mass. The resulting estimator runs in $O(n \log n)$ time and is consistent under standard regularity conditions. Simulations demonstrate that our bounds achieve nominal coverage when the true outcome shift falls within the specified $Λ$, provide substantially tighter intervals than worst-case bounds, and remain informative across a range of realistic violations of transportability.

</details>


### [12] [Extended Isolation Forest with feature sensitivities](https://arxiv.org/abs/2602.09704)
*Illia Donhauzer*

Main category: stat.ME

TL;DR: 提出各向异性隔离森林(AIF)，扩展隔离森林以支持特征空间不同方向的可控异常检测灵敏度


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法通常假设对所有特征偏差具有相同灵敏度，但实际应用中不同特征方向的偏差重要性不同，而隔离基方法中尚未解决此问题

Method: 提出各向异性隔离森林(AIF)，扩展标准EIF以支持特征空间不同方向的可控灵敏度，并引入新的方向灵敏度度量来量化不同方向的灵敏度

Result: 在合成和真实数据集上验证，AIF能够聚焦于特征空间中偏差更重要的方向进行异常检测，可根据任务需求调整灵敏度

Conclusion: AIF是首个解决隔离基方法中特征方向灵敏度问题的算法，实现了可控的方向性异常检测，为实际应用提供了更灵活的异常检测工具

Abstract: Compared to theoretical frameworks that assume equal sensitivity to deviations in all features of data, the theory of anomaly detection allowing for variable sensitivity across features is less developed. To the best of our knowledge, this issue has not yet been addressed in the context of isolation-based methods, and this paper represents the first attempt to do so. This paper introduces an Extended Isolation Forest with feature sensitivities, which we refer to as the Anisotropic Isolation Forest (AIF). In contrast to the standard EIF, the AIF enables anomaly detection with controllable sensitivity to deviations in different features or directions in the feature space. The paper also introduces novel measures of directional sensitivity, which allow quantification of AIF's sensitivity in different directions in the feature space. These measures enable adjustment of the AIF's sensitivity to task-specific requirements. We demonstrate the performance of the algorithm by applying it to synthetic and real-world datasets. The results show that the AIF enables anomaly detection that focuses on directions in the feature space where deviations from typical behavior are more important.

</details>


### [13] [Bayesian identification of early warning signals for long-range dependent climatic time series](https://arxiv.org/abs/2602.09731)
*Sigrunn H. Sørbye,Eirik Myrvoll-Nilsen,Håvard Rue*

Main category: stat.ME

TL;DR: 提出贝叶斯框架检测气候时间序列中的早期预警信号，处理噪声、趋势和长记忆依赖问题，应用于大西洋多年代际变率指数和Dansgaard-Oeschger事件记录。


<details>
  <summary>Details</summary>
Motivation: 气候时间序列中的早期预警信号检测对预测临界转变和临界点至关重要，但现有统计指标对观测噪声、长期趋势和长记忆依赖敏感，这些在气候数据中普遍存在，容易掩盖真实信号或产生虚假检测。

Method: 采用灵活的贝叶斯框架建模长程依赖时间序列中的时变自相关，同时考虑时变方差。使用两个分形高斯噪声过程的混合，通过时变权重函数表示具有时变Hurst指数的分形高斯噪声。通过集成嵌套拉普拉斯近似进行推断，实现均值趋势的联合估计和处理不规则采样观测。

Result: 在广泛模拟中研究了自相关变化检测的优势和局限性。应用于真实气候数据集，在重建的大西洋多年代际变率指数中发现了早期预警信号的证据，但在跨越Dansgaard-Oeschger事件的古气候记录中否定了此类信号。

Conclusion: 提出的贝叶斯框架能够有效处理气候时间序列中的噪声、趋势和长记忆依赖问题，成功检测大西洋多年代际变率中的早期预警信号，同时避免在Dansgaard-Oeschger事件记录中产生虚假检测，为气候临界转变预测提供了更可靠的方法。

Abstract: Detecting early warning signals in climatic time series is essential for anticipating critical transitions and tipping points. Common statistical indicators include increased variance and lag-one autocorrelation prior to bifurcation points. However, these indicators are sensitive to observational noise, long-term mean trends, and long-memory dependence, all of which are prevalent in climatic time series. Such effects can easily obscure genuine signals or generate spurious detections. To address these challenges, we employ a flexible Bayesian framework for modelling time-varying autocorrelation in long-range dependent time series, also accounting for time-varying variance. The approach uses a mixture of two fractional Gaussian noise processes with a time-dependent weight function to represent fractional Gaussian noise with a time-varying Hurst exponent. Inference is performed via integrated nested Laplace approximation, enabling joint estimation of mean trends and handling of irregularly sampled observations. The strengths and limitations of detecting changes in the autocorrelation is investigated in extensive simulations. Applied to real climatic data sets, we find evidence of early warning signals in a reconstructed Atlantic multidecadal variability index, while dismissing such signals for paleoclimate records spanning the Dansgaard-Oeschger events.

</details>


### [14] [Doubly Robust Machine Learning for Population Size Estimation with Missing Covariates: Application to Gaza Conflict Mortality](https://arxiv.org/abs/2602.09911)
*Mateo Dulce Rubio,Edward H. Kennedy,Nicholas P. Jewell*

Main category: stat.ME

TL;DR: 开发了处理捕获-再捕获数据中缺失协变量的非参数估计框架，使用双重稳健机器学习方法，在加沙冲突死亡率估计中显示死亡人数比官方统计高出约26%


<details>
  <summary>Details</summary>
Motivation: 捕获-再捕获数据中缺失协变量是研究难以接触人群的关键方法挑战，现有方法要么忽略缺失性，要么依赖先验插补，可能引入显著偏差

Method: 基于随机缺失假设开发非参数估计框架，使用半参数效率理论构建一步估计器，通过双重稳健结构结合机器学习方法

Result: 模拟显示相比朴素插补方法有显著改进，双重稳健机器学习估计器在高缺失率下仍保持有效推断；加沙冲突死亡率估计显示真实死亡人数比官方统计高出约26%

Conclusion: 该框架为冲突环境和其他难以接触人群应用中处理不完整数据提供了原则性工具，结合了效率、稳健性和有限样本有效性

Abstract: Population size estimation from capture-recapture data is central for studying hard-to-reach populations, incorporating auxiliary covariates to account for heterogeneous capture probabilities and recapture dependencies. However, missing attributes pose a critical methodological challenge due to reluctance to share sensitive information, data collection limitations, and imperfect record linkage. Existing approaches either ignore missingness or rely on a priori imputation, potentially introducing substantial bias. In this work, we develop a novel nonparametric estimation framework using a Missing at Random assumption to identify capture probabilities under missing covariates. Using semiparametric efficiency theory, we construct one-step estimators that combine efficiency, robustness, and finite-sample validity: they approximately achieve the nonparametric efficiency bound, accommodate flexible machine learning methods through a doubly robust structure, and provide approximately valid inference for any sample size. Simulations demonstrate substantial improvements over naive imputation approaches, with our doubly robust ML estimators maintaining valid inference even at high missingness rates where competing methods fail. We apply our methodology to re-estimate mortality in the Gaza Strip from October 7, 2023, to June 30, 2024, using three-list capture-recapture data with missing demographic information. Our approach yields more conservative yet precise estimates compared to previous methods, indicating the true death toll exceeds official statistics by approximately 26%. Our framework provides practitioners with principled tools for handling incomplete data in conflict settings and other applications with hard-to-reach populations.

</details>


### [15] [Kelly Betting as Bayesian Model Evaluation: A Framework for Time-Updating Probabilistic Forecasts](https://arxiv.org/abs/2602.09982)
*Michael Beuoy*

Main category: stat.ME

TL;DR: 提出一种基于凯利投注的动态概率预测评估方法，通过模拟投注竞赛实时评估模型准确性，比传统方法更有效区分正确与错误模型。


<details>
  <summary>Details</summary>
Motivation: 传统概率预测评估方法（如平均对数损失和Brier分数）需要等待最终结果才能评估，无法实时更新模型可信度。需要一种能够实时评估动态变化概率预测准确性的方法。

Method: 将每个待评估模型视为规范的凯利投注者，让它们在迭代投注竞赛中对决。每个模型的资金增长或减少作为评估指标，实时更新市场共识概率和模型可信度。

Result: 通过模拟模型证明，该方法在区分正确模型与错误模型方面通常比传统的平均对数损失和Brier分数方法更准确。凯利方法与贝叶斯推断有直接的数学和概念类比，资金可作为贝叶斯可信度的代理。

Conclusion: 基于凯利投注的评估方法提供了一种实时、动态的概率预测评估框架，能够更有效地评估和比较动态变化的概率预测模型。

Abstract: This paper proposes a new way of evaluating the accuracy and validity of probabilistic forecasts that change over time (such as an in-game win probability model, or an election forecast). Under this approach, each model to be evaluated is treated as a canonical Kelly bettor, and the models are pitted against each other in an iterative betting contest. The growth or decline of each model's bankroll serves as the evaluation metric. Under this approach, market consensus probabilities and implied model credibilities can be updated real time as each model updates, and do not require one to wait for the final outcome. Using a simulation model, it will be shown that this method is in general more accurate than traditional average log-loss and Brier score methods at distinguishing a correct model from an incorrect model. This Kelly approach is shown to have a direct mathematical and conceptual analogue to Bayesian inference, with bankroll serving as a proxy for Bayesian credibility.

</details>


### [16] [Doubly Robust Estimation of Desirability of Outcome Ranking (DOOR) Probability with Application to MDRO Studies](https://arxiv.org/abs/2602.10012)
*Shiyu Shu,Toshimitsu Hamasaki,Scott Evans,Lauren Komarow,David van Duin,Guoqing Diao*

Main category: stat.ME

TL;DR: 开发了用于观察性研究的DOOR分析因果框架，通过IPTW、G-Computation和双重稳健方法估计群体级DOOR概率，并在MDRO网络数据中比较单药与联合疗法的获益风险


<details>
  <summary>Details</summary>
Motivation: DOOR方法主要用于随机临床试验，但观察性研究中需要处理混杂因素。现有DOOR分析缺乏稳健的协变量调整方法，限制了其在观察性研究中的应用。需要开发因果推断框架来扩展DOOR方法在观察性研究中的适用性。

Method: 提出了三种因果推断方法：1) 治疗逆概率加权法(IPTW)；2) G-Computation方法；3) 结合两者的双重稳健方法。这些方法用于估计群体级DOOR概率，并在模拟研究中评估性能，最后应用于MDRO网络数据比较单药与联合疗法的获益风险。

Result: 通过模拟研究验证了所提方法的性能，并在ARLG的MDRO网络数据中进行了因果分析，比较了单药疗法与联合疗法的获益风险比。结果表明所提方法能够有效处理观察性研究中的混杂偏倚，扩展了DOOR方法的应用范围。

Conclusion: 开发了一个稳健的因果推断框架，使DOOR方法能够应用于观察性研究。通过IPTW、G-Computation和双重稳健方法，可以有效地估计群体级DOOR概率，为观察性研究中的获益风险评估提供了新工具。

Abstract: In observational studies, adjusting for confounders is required if a treatment comparison is planned. A crude comparison of the primary endpoint without covariate adjustment will suffer from biases, and the addition of regression models could improve precision by incorporating imbalanced covariates and thus help make correct inference. Desirability of outcome ranking (DOOR) is a patient-centric benefit-risk evaluation methodology designed for randomized clinical trials. Still, robust covariate adjustment methods could further expand the compatibility of this method in observational studies. In DOOR analysis, each participant's outcome is ranked based on pre-specified clinical criteria, where the most desirable rank represents a good outcome with no side effects and the least desirable rank is the worst possible clinical outcome. We develop a causal framework for estimating the population-level DOOR probability, via the inverse probability of treatment weighting method, G-Computation method, and a Doubly Robust method that combines both. The performance of the proposed methodologies is examined through simulations. We also perform a causal analysis of the Multi-Drug Resistant Organism (MDRO) network within the Antibacterial Resistant Leadership Group (ARLG), comparing the benefit:risk between Mono-drug therapy and Combination-drug therapy.

</details>


### [17] [Degrees-of-Freedom Approximations for Conditional-Mean Inference in Random-Lot Stability Analysis](https://arxiv.org/abs/2602.10026)
*Andrew T. Karl,Heath Rushing,Richard K. Burdick,Jeff Hofer*

Main category: stat.ME

TL;DR: 线性混合模型在药品稳定性趋势分析中广泛应用，但当拟合的随机效应方差分量接近零时，Satterthwaite自由度方法会导致置信区间异常，而containment方法能提供更稳定的推断。


<details>
  <summary>Details</summary>
Motivation: 在药品稳定性趋势分析中，当随机效应方差分量接近零时，Satterthwaite自由度方法会导致自由度崩溃，产生异常宽且非单调的置信区间，这可能错误地影响药品有效期的判定。

Method: 比较Satterthwaite和containment两种自由度计算方法，通过实例分析和模拟研究评估它们对置信区间和有效期判定的影响。提出10%方差贡献减少工作流程来缓解Satterthwaite方法的极端行为，并评估AICc逐步降阶方法。

Result: Satterthwaite方法在方差分量接近零时会产生自由度崩溃，导致不必要的宽置信区间和可能非单调的置信限，而containment方法提供稳定的自由度和连续推断。即使观测数据明显符合规格，自由度选择也可能显著改变通过/失败的结论。

Conclusion: 使用包含完整随机效应模型的containment推断提供了一个统一的建模框架，避免了在任意截断点进行数据依赖模型简化引入的不连续性。当containment不可用时，10%方差贡献减少工作流程可以缓解Satterthwaite的极端行为。AICc逐步降阶方法最好作为敏感性分析使用。

Abstract: Linear mixed models are widely used for pharmaceutical stability trending when sufficient lots are available. Expiry support is typically based on whether lot-specific conditional-mean confidence limits remain within specification through a proposed expiry. These limits depend on the denominator degrees-of-freedom (DDF) method used for $t$-based inference. We document an operationally important boundary-proximal phenomenon: when a fitted random-effect variance component is close to zero, Satterthwaite DDF for conditional-mean predictions can collapse, inflating $t$ critical values and producing unnecessarily wide and sometimes nonmonotone pointwise confidence limits on scheduled time grids. In contrast, containment DDF yields stable degrees of freedom and avoids sharp discontinuities as variance components approach the boundary. Using a worked example and simulation studies, we show that DDF choice can materially change pass/fail conclusions even when observed data comfortably meet specifications. Containment-based inference with the full random-effects model provides a single modeling framework that avoids the discontinuities introduced by data-dependent model reduction at arbitrary cutoffs. When containment is unavailable, a 10\% variance-contribution reduction workflow mitigates extreme Satterthwaite behavior by simplifying the random-effects structure only when fitted contributions at the proposed expiry are negligible. An AICc step-down is also evaluated but is best treated as a sensitivity analysis, as it can be liberal when the margin between the mean trend and the specification limit at the proposed expiry is small.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [18] [Motivating REML via Prediction-Error Covariances in EM Updates for Linear Mixed Models](https://arxiv.org/abs/2602.09247)
*Andrew T. Karl*

Main category: stat.CO

TL;DR: 该论文展示了线性混合模型中限制性最大似然估计的计算动机，通过EM算法实现，揭示了ML和REML在方差分量更新中的差异。


<details>
  <summary>Details</summary>
Motivation: 论文旨在为线性混合模型中的限制性最大似然估计提供计算动机，通过EM算法阐明ML和REML估计之间的差异，特别是在方差分量更新中的不同处理方式。

Method: 使用期望最大化算法实现线性混合模型的参数估计，通过R代码展示ML和REML在方差分量更新中的差异：ML使用给定数据的随机效应条件协方差，而REML使用Henderson C-矩阵的预测误差协方差。

Result: 论文提供了简洁的R代码实现，明确展示了ML和REML之间的切换，暴露了关键矩阵供课堂检查，并成功复现了lme4包的ML和REML拟合结果。

Conclusion: ML和REML在EM算法的每次迭代中解决相同的混合模型方程，主要差异在于方差分量更新中的迹调整：ML使用随机效应的条件协方差，而REML使用考虑固定效应估计不确定性的预测误差协方差。

Abstract: We present a computational motivation for restricted maximum likelihood (REML) estimation in linear mixed models using an expectation--maximization (EM) algorithm. At each iteration, maximum likelihood (ML) and REML solve the same mixed-model equations for the best linear unbiased estimator (BLUE) of the fixed effects and the best linear unbiased predictor (BLUP) of the random effects. They differ only in the trace adjustments used in the variance-component updates: ML uses conditional covariances of the random effects given the data, whereas REML uses prediction-error covariances from Henderson's C-matrix, reflecting uncertainty from estimating the fixed effects. Short R code makes this switch explicit, exposes the key matrices for classroom inspection, and reproduces lme4 ML and REML fits.

</details>


### [19] [Estimating Individual Customer Lifetime Values with R: The CLVTools Package](https://arxiv.org/abs/2602.09845)
*Markus Meierer,Patrick Bachmann,Jeffrey Näf,Patrik Schilter,René Algesheimer*

Main category: stat.CO

TL;DR: R包CLVTools提供了高效的用户友好框架，用于应用Pareto/NBD和Gamma-Gamma等关键概率模型进行客户终身价值预测，支持协变量、正则化和约束等扩展功能。


<details>
  <summary>Details</summary>
Motivation: 客户终身价值（CLV）是衡量客户长期经济价值的重要指标，广泛应用于营销活动客户选择。然而，CLV建模面临数据稀疏、预测周期长于估计周期等挑战，需要能够克服这些困难的概率模型。

Method: 使用概率模型（如Pareto/NBD和Gamma-Gamma模型）进行CLV预测，通过R包CLVTools提供高效实现框架，支持时间不变和时间变化的协变量、参数正则化和等式约束等扩展功能。

Result: CLVTools包提供了数据简洁性、可扩展性和预测准确性三大优势，能够处理小数据和大量数据，无需精细调参即可获得稳健的预测性能，适用于现有客户和新客户的CLV预测。

Conclusion: 概率模型是解决CLV建模挑战的有效方法，CLVTools包为研究者和实践者提供了用户友好的工具，能够应用最新的模型扩展功能进行客户终身价值预测。

Abstract: Customer lifetime value (CLV) describes a customer's long-term economic value for a business. This metric is widely used in marketing, for example, to select customers for a marketing campaign. However, modeling CLV is challenging. When relying on customers' purchase histories, the input data is sparse. Additionally, given its long-term focus, prediction horizons are often longer than estimation periods. Probabilistic models are able to overcome these challenges and, thus, are a popular option among researchers and practitioners. The latter also appreciate their applicability for both small and big data as well as their robust predictive performance without any fine-tuning requirements. Their popularity is due to three characteristics: data parsimony, scalability, and predictive accuracy. The R package CLVTools provides an efficient and user-friendly implementation framework to apply key probabilistic models such as the Pareto/NBD and Gamma-Gamma model. Further, it provides access to the latest model extensions to include time-invariant and time-varying covariates, parameter regularization, and equality constraints. This article gives an overview of the fundamental ideas of these statistical models and illustrates their application to derive CLV predictions for existing and new customers.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [20] [Estimating the distance at which narwhal $(\textit{Monodon monoceros})$ respond to disturbance: a penalized threshold hidden Markov model](https://arxiv.org/abs/2602.09267)
*Fanny Dupont,Marianne Marcoux,Nigel E. Hussey,Jackie Dawson,Marie Auger-Méthé*

Main category: stat.AP

TL;DR: 提出一种带lasso惩罚的阈值隐马尔可夫模型，用于量化野生动物对干扰的行为响应阈值，并应用于独角鲸对船只的反应距离分析。


<details>
  <summary>Details</summary>
Motivation: 北极海冰减少导致新航运路线开通，需要评估船只对海洋哺乳动物的影响距离，以制定针对性缓解政策。现有阈值隐马尔可夫模型虽能估计阈值，但无法判断该阈值是否反映有意义的行为转变。

Method: 提出lasso惩罚的阈值隐马尔可夫模型，基于计算高效的方法对HMM施加惩罚，并开发新的惩罚准受限最大似然估计器。该框架既能估计阈值，又能评估干扰效应是否显著。

Result: 模拟显示lasso方法能有效将虚假阈值效应收缩至零。应用于独角鲸运动数据表明，独角鲸在船只4公里外就会做出反应：降低运动持续性，更多时间待在深水区（平均最大深度356米）。

Conclusion: 提供了一个广泛适用的框架，用于量化生物对刺激的行为响应，应用范围包括确定干扰反应阈值、估计陆地物种（如大象）探测水源的距离等。

Abstract: Understanding behavioural responses to disturbances is vital for wildlife conservation. For example, in the Arctic, the decrease in sea ice has opened new shipping routes, increasing the need for impact assessments that quantify the distance at which marine mammals react to vessel presence. This information can then guide targeted mitigation policies, such as vessel slow-down regulations and delineation of avoidance areas. Using telemetry data to determine distances linked to deviations from normal behaviour requires advanced statistical models, such as threshold hidden Markov models (THMMs). While these are powerful tools, they do not assess whether the estimated threshold reflects a meaningful behavioural shift. We introduce a lasso-penalized THMM that builds on computationally efficient methods to impose penalties on HMMs and present a new, efficient penalized quasi-restricted maximum-likelihood estimator. Our framework is capable of estimating thresholds and assessing whether the disturbance effects are meaningful. With simulations, we demonstrate that our lasso method effectively shrinks spurious threshold effects towards zero. When applied to narwhal $\textit{(Monodon monoceros)}$ movement data, our analysis suggests that narwhal react to vessels up to 4 kilometres away by decreasing movement persistence and spending more time in deeper waters (average maximum depth of 356m). Overall, we provide a broadly applicable framework for quantifying behavioural responses to stimuli, with applications ranging from determining reaction thresholds to disturbance to estimating the distances at which terrestrial species, such as elephants, detect water.

</details>


### [21] [Bayesian network approach to building an affective module for a driver behavioural model](https://arxiv.org/abs/2602.09632)
*Dorota Młynarczyk,Gabriel Calvo,Francisco Palmi-Perales,Carmen Armero,Virgilio Gómez-Rubio,Ana de la Torre-García,Ricardo Bayona Salvador*

Main category: stat.AP

TL;DR: 使用贝叶斯网络建模驾驶员心理状态（如心理负荷和主动疲劳）对驾驶行为的影响


<details>
  <summary>Details</summary>
Motivation: 驾驶员心理状态（如心理负荷和疲劳）会影响驾驶表现，需要建立模型来理解和预测这些影响，以提高交通安全和自动驾驶技术

Method: 使用贝叶斯网络探索相关随机变量之间的依赖关系，基于生理和人口统计条件评估驾驶员处于特定心理状态的概率

Result: 建立了驾驶员行为模型的情感组件，能够评估驾驶员心理状态的概率，为理解动态环境中的驾驶行为提供了框架

Conclusion: 贝叶斯网络方法能有效建模驾驶员心理状态，对交通安全和自动驾驶技术有潜在应用价值

Abstract: This paper focuses on the affective component of a driver behavioural model (DBM). This component specifically models some drivers' mental states such as mental load and active fatigue, which may affect driving performance. We have used Bayesian networks (BNs) to explore the dependencies between various relevant random variables and assess the probability that a driver is in a particular mental state based on their physiological and demographic conditions. Through this approach, our goal is to improve our understanding of driver behaviour in dynamic environments, with potential applications in traffic safety and autonomous vehicle technologies.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [22] [Persistent Entropy as a Detector of Phase Transitions](https://arxiv.org/abs/2602.09058)
*Matteo Rucco*

Main category: stat.ML

TL;DR: PE可检测相变的理论条件被证明，基于拓扑稳定化的操作框架在多个复杂系统中验证有效


<details>
  <summary>Details</summary>
Motivation: 尽管持久熵在经验上成功检测复杂系统的相变，但其理论理解有限，特别是在随机和数据驱动场景中。需要建立普适的理论框架来解释持久熵何时以及为何能可靠检测相变。

Method: 1. 建立模型无关的定理，给出持久熵能分离两个相的充分条件；2. 引入基于拓扑稳定化的操作框架，通过滑动窗口稳定拓扑统计量来定义拓扑转变时间；3. 在有限观测范围内使用基于概率的临界参数估计器。

Result: 持久熵在相变两侧表现出渐近非消失的差距。在Kuramoto同步转变、Vicsek有序-无序转变以及神经网络训练动力学等多个实验中，持久熵的稳定化和实现间变异性的塌缩提供了与理论机制一致的鲁棒数值特征。

Conclusion: 该工作为持久熵检测相变提供了普适的理论基础，并通过拓扑稳定化框架将渐近理论与有限时间计算连接起来，在多种复杂系统中验证了其有效性。

Abstract: Persistent entropy (PE) is an information-theoretic summary statistic of persistence barcodes that has been widely used to detect regime changes in complex systems. Despite its empirical success, a general theoretical understanding of when and why persistent entropy reliably detects phase transitions has remained limited, particularly in stochastic and data-driven settings. In this work, we establish a general, model-independent theorem providing sufficient conditions under which persistent entropy provably separates two phases. We show that persistent entropy exhibits an asymptotically non-vanishing gap across phases. The result relies only on continuity of persistent entropy along the convergent diagram sequence, or under mild regularization, and is therefore broadly applicable across data modalities, filtrations, and homological degrees. To connect asymptotic theory with finite-time computations, we introduce an operational framework based on topological stabilization, defining a topological transition time by stabilizing a chosen topological statistic over sliding windows, and a probability-based estimator of critical parameters within a finite observation horizon. We validate the framework on the Kuramoto synchronization transition, the Vicsek order-to-disorder transition in collective motion, and neural network training dynamics across multiple datasets and architectures. Across all experiments, stabilization of persistent entropy and collapse of variability across realizations provide robust numerical signatures consistent with the theoretical mechanism.

</details>


### [23] [Minimum Distance Summaries for Robust Neural Posterior Estimation](https://arxiv.org/abs/2602.09161)
*Sherman Khoo,Dennis Prangle,Song Liu,Mark Beaumont*

Main category: stat.ML

TL;DR: 提出一种基于最小距离摘要的鲁棒神经后验估计方法，通过测试时自适应摘要来增强对分布偏移的鲁棒性，而不需要重新训练推理网络。


<details>
  <summary>Details</summary>
Motivation: 传统的基于模拟的推理（SBI）方法使用神经后验估计器（NPE）进行摊销贝叶斯推理，但当观测数据偏离训练分布时容易产生错误。现有鲁棒方法通常需要修改NPE训练或引入误差模型，这会损害摊销性和模块化。

Method: 提出最小距离摘要方法，在测试时独立于预训练NPE自适应调整摘要统计量。使用最大均值差异（MMD）作为观测数据与摘要条件预测分布之间的距离度量，通过随机傅里叶特征近似实现高效计算。

Result: 该方法在合成和真实世界任务中表现出显著的鲁棒性提升，同时保持了最小额外计算开销。理论分析证明了算法的鲁棒性保证。

Conclusion: 最小距离摘要提供了一种轻量级、模型无关的测试时自适应方法，能够在保持摊销性和模块化的同时，显著增强神经后验估计器对分布偏移的鲁棒性。

Abstract: Simulation-based inference (SBI) enables amortized Bayesian inference by first training a neural posterior estimator (NPE) on prior-simulator pairs, typically through low-dimensional summary statistics, which can then be cheaply reused for fast inference by querying it on new test observations. Because NPE is estimated under the training data distribution, it is susceptible to misspecification when observations deviate from the training distribution. Many robust SBI approaches address this by modifying NPE training or introducing error models, coupling robustness to the inference network and compromising amortization and modularity. We introduce minimum-distance summaries, a plug-in robust NPE method that adapts queried test-time summaries independently of the pretrained NPE. Leveraging the maximum mean discrepancy (MMD) as a distance between observed data and a summary-conditional predictive distribution, the adapted summary inherits strong robustness properties from the MMD. We demonstrate that the algorithm can be implemented efficiently with random Fourier feature approximations, yielding a lightweight, model-free test-time adaptation procedure. We provide theoretical guarantees for the robustness of our algorithm and empirically evaluate it on a range of synthetic and real-world tasks, demonstrating substantial robustness gains with minimal additional overhead.

</details>


### [24] [Quantifying Epistemic Uncertainty in Diffusion Models](https://arxiv.org/abs/2602.09170)
*Aditi Gupta,Raphael A. Meyer,Yotam Yaniv,Elynn Chen,N. Benjamin Erichson*

Main category: stat.ML

TL;DR: 提出FLARE方法，基于Fisher信息分离扩散模型的认知不确定性，使用随机参数子集进行可扩展近似，在时间序列生成任务中提升不确定性估计效果


<details>
  <summary>Details</summary>
Motivation: 现有方法不可靠，因为它们混淆了认知不确定性和偶然不确定性。需要量化扩散模型的认知不确定性以确保高质量输出。

Method: 基于Fisher信息的方法，明确分离认知方差。提出FLARE（Fisher-Laplace随机估计器），使用均匀随机参数子集近似Fisher信息，使其可扩展。

Result: 在合成时间序列生成任务中，FLARE改进了不确定性估计，实现了比其他方法更准确可靠的过滤。理论分析表明仅使用最后一层的Laplace近似对此任务不足。

Conclusion: FLARE方法能有效分离扩散模型的认知不确定性，提供更可靠的可信度评分，理论分析支持其有效性，并证明传统Laplace近似的局限性。

Abstract: To ensure high quality outputs, it is important to quantify the epistemic uncertainty of diffusion models.Existing methods are often unreliable because they mix epistemic and aleatoric uncertainty. We introduce a method based on Fisher information that explicitly isolates epistemic variance, producing more reliable plausibility scores for generated data. To make this approach scalable, we propose FLARE (Fisher-Laplace Randomized Estimator), which approximates the Fisher information using a uniformly random subset of model parameters. Empirically, FLARE improves uncertainty estimation in synthetic time-series generation tasks, achieving more accurate and reliable filtering than other methods. Theoretically, we bound the convergence rate of our randomized approximation and provide analytic and empirical evidence that last-layer Laplace approximations are insufficient for this task.

</details>


### [25] [Is Memorization Helpful or Harmful? Prior Information Sets the Threshold](https://arxiv.org/abs/2602.09405)
*Chen Cheng,Rina Foygel Barber*

Main category: stat.ML

TL;DR: 研究过参数化线性模型中训练误差与泛化误差的关系，发现先验分布特性决定何时需要近插值（记忆）或避免过拟合


<details>
  <summary>Details</summary>
Motivation: 探索任意估计过程中训练误差与泛化误差之间的理论联系，特别是在过参数化线性模型下，理解贝叶斯框架中先验分布如何影响这两种误差的关系

Method: 在贝叶斯框架下，使用过参数化线性模型，分析任意估计过程，考察先验分布π的特性对训练误差和泛化误差关系的影响

Result: 发现先验分布π的内在特性是决定因素，给出了明确条件：当噪声达到由Fisher信息和先验方差参数确定的阈值时，最优泛化需要训练误差（i）接近噪声大小的近插值（记忆必要），或（ii）接近噪声水平（过拟合有害）

Conclusion: 先验分布的特性决定了过参数化模型中训练误差与泛化误差的关系，噪声水平相对于Fisher信息和先验方差的阈值决定了何时需要记忆或避免过拟合

Abstract: We examine the connection between training error and generalization error for arbitrary estimating procedures, working in an overparameterized linear model under general priors in a Bayesian setup. We find determining factors inherent to the prior distribution $π$, giving explicit conditions under which optimal generalization necessitates that the training error be (i) near interpolating relative to the noise size (i.e., memorization is necessary), or (ii) close to the noise level (i.e., overfitting is harmful). Remarkably, these phenomena occur when the noise reaches thresholds determined by the Fisher information and the variance parameters of the prior $π$.

</details>


### [26] [Mutual Information Collapse Explains Disentanglement Failure in $β$-VAEs](https://arxiv.org/abs/2602.09277)
*Minh Vu,Xiaoliang Wan,Shuangqing Wei*

Main category: stat.ML

TL;DR: β-VAE在强正则化下会出现解纠缠性能崩溃，这是由于KL散度压力导致潜在通道语义信息丢失。作者提出λβ-VAE，通过L2重建惩罚λ来稳定解纠缠，扩展了β的有效范围。


<details>
  <summary>Details</summary>
Motivation: β-VAE作为无监督解纠缠的基础框架，在实践中表现出非单调趋势：解纠缠性能在中等β时达到峰值，但在正则化增强时崩溃。作者旨在理解这种崩溃的根本原因并提供解决方案。

Method: 1. 在线性高斯设置中形式化分析β>1时KL散度压力导致潜在因子互信息为零的机制；2. 提出λβ-VAE，通过引入辅助L2重建惩罚λ来解耦正则化压力和信息崩溃；3. 在dSprites、Shapes3D和MPI3D-real数据集上进行广泛实验验证。

Result: 实验证明λ>0能稳定解纠缠性能，并在更广泛的β范围内恢复潜在信息性。这为变分推断主干中的双参数正则化提供了理论依据。

Conclusion: β-VAE的解纠缠崩溃是信息论上的根本失败，而λβ-VAE通过解耦正则化压力和信息崩溃，提供了更稳定和有效的解纠缠框架。

Abstract: The $β$-VAE is a foundational framework for unsupervised disentanglement, using $β$ to regulate the trade-off between latent factorization and reconstruction fidelity. Empirically, however, disentanglement performance exhibits a pervasive non-monotonic trend: benchmarks such as MIG and SAP typically peak at intermediate $β$ and collapse as regularization increases. We demonstrate that this collapse is a fundamental information-theoretic failure, where strong Kullback-Leibler pressure promotes marginal independence at the expense of the latent channel's semantic informativeness. By formalizing this mechanism in a linear-Gaussian setting, we prove that for $β> 1$, stationarity-induced dynamics trigger a spectral contraction of the encoder gain, driving latent-factor mutual information to zero. To resolve this, we introduce the $λβ$-VAE, which decouples regularization pressure from informational collapse via an auxiliary $L_2$ reconstruction penalty $λ$. Extensive experiments on dSprites, Shapes3D, and MPI3D-real confirm that $λ> 0$ stabilizes disentanglement and restores latent informativeness over a significantly broader range of $β$, providing a principled theoretical justification for dual-parameter regularization in variational inference backbones.

</details>


### [27] [The Catastrophic Failure of The k-Means Algorithm in High Dimensions, and How Hartigan's Algorithm Avoids It](https://arxiv.org/abs/2602.09936)
*Roy R. Lederman,David Silva-Sánchez,Ziling Chen,Gilles Mordant,Amnon Balanov,Tamir Bendory*

Main category: stat.ML

TL;DR: 在高维高噪声环境下，Lloyd的k-means算法会灾难性失效，几乎每个数据划分都是固定点，算法只返回初始划分；而Hartigan的k-means算法则无此问题。


<details>
  <summary>Details</summary>
Motivation: Lloyd的k-means算法被广泛使用，但在高维高噪声环境下常出现性能问题，需要理论解释这些经验观察到的困难。

Method: 通过理论分析证明Lloyd算法在高维高噪声设置下的病理行为，并与Hartigan的k-means算法进行对比分析。

Result: Lloyd算法在高维高噪声环境下以高概率出现灾难性失效，几乎所有数据划分都是固定点，算法仅返回初始划分；而Hartigan算法没有这种病理现象。

Conclusion: 两种k-means算法在高维环境下表现截然不同，这为k-means在高维数据中常遇到的实证困难提供了理论解释，表明算法选择的重要性。

Abstract: Lloyd's k-means algorithm is one of the most widely used clustering methods. We prove that in high-dimensional, high-noise settings, the algorithm exhibits catastrophic failure: with high probability, essentially every partition of the data is a fixed point. Consequently, Lloyd's algorithm simply returns its initial partition - even when the underlying clusters are trivially recoverable by other methods. In contrast, we prove that Hartigan's k-means algorithm does not exhibit this pathology. Our results show the stark difference between these algorithms and offer a theoretical explanation for the empirical difficulties often observed with k-means in high dimensions.

</details>


### [28] [The Critical Horizon: Inspection Design Principles for Multi-Stage Operations and Deep Reasoning](https://arxiv.org/abs/2602.09394)
*Seyed Morteza Emadi*

Main category: stat.ML

TL;DR: 论文建立了信用分配问题的信息论边界：早期步骤与最终结果之间的信号随深度指数衰减，超过临界范围后无法仅从端点数据学习。


<details>
  <summary>Details</summary>
Motivation: 制造业生产线、服务流程、供应链和AI推理链都面临一个共同挑战：如何将终端结果归因于导致它的中间阶段。需要建立信用分配问题的理论基础。

Method: 采用信息论方法分析信用分配问题，证明了四个核心结果：信号衰减边界、宽度限制、目标不匹配问题，以及最优检查点设计算法。

Result: 证明了样本复杂度随中间步骤数量指数增长；并行展开仅提供对数缓解；加性奖励聚合优化了错误量；均匀检查点间距在均匀衰减下是最小最大最优的。

Conclusion: 这些结果为运营中的检查点设计和AI中的监督设计提供了共同的分析基础，揭示了信用分配问题的根本限制和最优解决方案。

Abstract: Manufacturing lines, service journeys, supply chains, and AI reasoning chains share a common challenge: attributing a terminal outcome to the intermediate stage that caused it. We establish an information-theoretic barrier to this credit assignment problem: the signal connecting early steps to final outcomes decays exponentially with depth, creating a critical horizon beyond which no algorithm can learn from endpoint data alone. We prove four results. First, a Signal Decay Bound: sample complexity for attributing outcomes to early stages grows exponentially in the number of intervening steps. Second, Width Limits: parallel rollouts provide only logarithmic relief, with correlation capping the effective number of independent samples. Third, an Objective Mismatch: additive reward aggregation optimizes the wrong quantity when sequential validity requires all steps to be correct. Fourth, Optimal Inspection Design: uniform checkpoint spacing is minimax-optimal under homogeneous signal attenuation, while a greedy algorithm yields optimal non-uniform schedules under heterogeneous attenuation. Together, these results provide a common analytical foundation for inspection design in operations and supervision design in AI.

</details>


### [29] [From Average Sensitivity to Small-Loss Regret Bounds under Random-Order Model](https://arxiv.org/abs/2602.09457)
*Shinsaku Sakaue,Yuichi Yoshida*

Main category: stat.ML

TL;DR: 本文提出了一种在随机顺序模型中的在线学习方法，通过自适应调整近似参数ε，将离线算法的近似保证转换为小损失遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究随机顺序模型中的在线学习问题，其中损失函数的多重集由对抗性选择但以均匀随机顺序揭示。目标是利用离线算法的近似保证来获得更好的在线学习性能，特别是小损失遗憾界。

Method: 基于Dong和Yoshida(2023)的批量到在线转换方法，如果离线算法具有(1+ε)-近似保证，且其平均灵敏度由函数φ(ε)刻画，则通过自适应选择ε，可以获得小损失遗憾界。该方法不需要对损失函数进行正则性假设，可视为AdaGrad式调参在近似参数ε上的推广。

Result: 获得了小损失遗憾界$\tilde O(\varphi^{\star}(\mathrm{OPT}_T))$，其中$\varphi^{\star}$是φ的凹共轭，$\mathrm{OPT}_T$是T轮离线最优值。该方法恢复了Dong和Yoshida(2023)的(1+ε)-近似遗憾界，并为在线k-means聚类、低秩近似和回归提供了小损失遗憾界。对于在线子模函数最小化，获得了$\tilde O(n^{3/4}(1 + \mathrm{OPT}_T^{3/4}))$的遗憾界。

Conclusion: 该方法展示了稀疏化及相关技术在随机顺序模型中建立小损失遗憾界的能力，为在线学习提供了新的理论框架，特别是在不需要损失函数正则性假设的情况下获得更好的性能保证。

Abstract: We study online learning in the random-order model, where the multiset of loss functions is chosen adversarially but revealed in a uniformly random order. Building on the batch-to-online conversion by Dong and Yoshida (2023), we show that if an offline algorithm admits a $(1+\varepsilon)$-approximation guarantee and the effect of $\varepsilon$ on its average sensitivity is characterized by a function $\varphi(\varepsilon)$, then an adaptive choice of $\varepsilon$ yields a small-loss regret bound of $\tilde O(\varphi^{\star}(\mathrm{OPT}_T))$, where $\varphi^{\star}$ is the concave conjugate of $\varphi$, $\mathrm{OPT}_T$ is the offline optimum over $T$ rounds, and $\tilde O$ hides polylogarithmic factors in $T$. Our method requires no regularity assumptions on loss functions, such as smoothness, and can be viewed as a generalization of the AdaGrad-style tuning applied to the approximation parameter $\varepsilon$. Our result recovers and strengthens the $(1+\varepsilon)$-approximate regret bounds of Dong and Yoshida (2023) and yields small-loss regret bounds for online $k$-means clustering, low-rank approximation, and regression. We further apply our framework to online submodular function minimization using $(1\pm\varepsilon)$-cut sparsifiers of submodular hypergraphs, obtaining a small-loss regret bound of $\tilde O(n^{3/4}(1 + \mathrm{OPT}_T^{3/4}))$, where $n$ is the ground-set size. Our approach sheds light on the power of sparsification and related techniques in establishing small-loss regret bounds in the random-order model.

</details>


### [30] [The Entropic Signature of Class Speciation in Diffusion Models](https://arxiv.org/abs/2602.09651)
*Florian Handke,Dejan Stančević,Felix Koulischer,Thomas Demeester,Luca Ambrogioni*

Main category: stat.ML

TL;DR: 该论文提出使用类别条件熵来检测扩散模型中语义结构形成的关键时间窗口，连接了信息论和统计物理视角，并提供了时间局部控制的理论基础。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间上并非均匀恢复语义结构，而是在一个狭窄的时间窗口内从语义模糊过渡到类别确定。现有理论将这种过渡归因于类别分离方向上的动力学不稳定性，但缺乏实用的检测和利用这些窗口的方法。

Method: 通过跟踪给定噪声状态下潜在语义变量的类别条件熵来检测过渡区域。将熵限制在语义分区中，可以解析不同抽象层次的语义决策。在高维高斯混合模型中分析熵率行为，并在EDM2-XS和Stable Diffusion 1.5上进行验证。

Result: 类别条件熵可靠地标记了扩散模型中的语义过渡区域，熵率在与方差保持扩散中先前识别的物种形成对称性破缺不稳定性相同的时间尺度上集中。该方法在真实模型中一致地隔离了语义结构形成的关键噪声区域。

Conclusion: 该研究连接了信息论和统计物理视角，为扩散模型提供了时间局部控制的理论基础，并展示了如何量化引导如何随时间重新分布语义信息。

Abstract: Diffusion models do not recover semantic structure uniformly over time. Instead, samples transition from semantic ambiguity to class commitment within a narrow regime. Recent theoretical work attributes this transition to dynamical instabilities along class-separating directions, but practical methods to detect and exploit these windows in trained models are still limited. We show that tracking the class-conditional entropy of a latent semantic variable given the noisy state provides a reliable signature of these transition regimes. By restricting the entropy to semantic partitions, the entropy can furthermore resolve semantic decisions at different levels of abstraction. We analyze this behavior in high-dimensional Gaussian mixture models and show that the entropy rate concentrates on the same logarithmic time scale as the speciation symmetry-breaking instability previously identified in variance-preserving diffusion. We validate our method on EDM2-XS and Stable Diffusion 1.5, where class-conditional entropy consistently isolates the noise regimes critical for semantic structure formation. Finally, we use our framework to quantify how guidance redistributes semantic information over time. Together, these results connect information-theoretic and statistical physics perspectives on diffusion and provide a principled basis for time-localized control.

</details>


### [31] [Continual Learning for non-stationary regression via Memory-Efficient Replay](https://arxiv.org/abs/2602.09720)
*Pablo García-Santaclara,Bruno Fernández-Castro,RebecaP. Díaz-Redondo,Martín Alonso-Gamarra*

Main category: stat.ML

TL;DR: 首个基于原型的生成回放框架，用于在线无任务持续回归，通过自适应输出空间离散化实现，无需存储原始数据


<details>
  <summary>Details</summary>
Motivation: 在工业4.0等动态环境中，数据流不断变化，传统离线模型需要快速适应新数据。持续学习能逐步获取知识而无需从头训练，但现有研究主要关注分类问题，回归任务研究很少

Method: 提出基于原型的生成回放框架，采用自适应输出空间离散化模型，实现无任务持续回归，无需存储原始数据

Result: 在多个基准数据集上验证，该框架减少了遗忘，提供了比其他最先进解决方案更稳定的性能

Conclusion: 该框架是首个针对在线无任务持续回归的基于原型的生成回放方法，有效解决了动态环境中回归任务的持续学习问题

Abstract: Data streams are rarely static in dynamic environments like Industry 4.0. Instead, they constantly change, making traditional offline models outdated unless they can quickly adjust to the new data. This need can be adequately addressed by continual learning (CL), which allows systems to gradually acquire knowledge without incurring the prohibitive costs of retraining them from scratch. Most research on continual learning focuses on classification problems, while very few studies address regression tasks. We propose the first prototype-based generative replay framework designed for online task-free continual regression. Our approach defines an adaptive output-space discretization model, enabling prototype-based generative replay for continual regression without storing raw data. Evidence obtained from several benchmark datasets shows that our framework reduces forgetting and provides more stable performance than other state-of-the-art solutions.

</details>


### [32] [Stabilized Maximum-Likelihood Iterative Quantum Amplitude Estimation for Structural CVaR under Correlated Random Fields](https://arxiv.org/abs/2602.09847)
*Alireza Tabarraei*

Main category: stat.ML

TL;DR: 量子增强的CVaR评估框架，结合最大似然推断和区间追踪，在空间相关材料不确定性下实现比经典蒙特卡洛方法更低的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在高维空间相关材料不确定性下，传统蒙特卡洛方法计算条件风险价值(CVaR)的计算成本过高，需要开发量子增强的高效计算方法。

Method: 将CVaR重新表述为与量子振幅估计兼容的形式，扩展迭代量子振幅估计(IQAE)，嵌入最大似然推断，采用多假设可行性追踪、周期性低深度消歧和有界重启机制。

Result: 在空间相关对数正态杨氏模量场的基准问题上，该方法在相同置信水平下比经典蒙特卡洛CVaR估计具有显著更低的oracle复杂度，同时保持严格的统计可靠性。

Conclusion: 建立了一个实用稳健、理论基础的量子增强方法，用于随机连续介质力学中的尾部风险量化，保留了振幅估计的二次oracle复杂度优势。

Abstract: Conditional Value-at-Risk (CVaR) is a central tail-risk measure in stochastic structural mechanics, yet its accurate evaluation under high-dimensional, spatially correlated material uncertainty remains computationally prohibitive for classical Monte Carlo methods. Leveraging bounded-expectation reformulations of CVaR compatible with quantum amplitude estimation, we develop a quantum-enhanced inference framework that casts CVaR evaluation as a statistically consistent, confidence-constrained maximum-likelihood amplitude estimation problem. The proposed method extends iterative quantum amplitude estimation (IQAE) by embedding explicit maximum-likelihood inference within a rigorously controlled interval-tracking architecture. To ensure global correctness under finite-shot noise and the non-injective oscillatory response induced by Grover amplification, we introduce a stabilized inference scheme incorporating multi-hypothesis feasibility tracking, periodic low-depth disambiguation, and a bounded restart mechanism governed by an explicit failure-probability budget. This formulation preserves the quadratic oracle-complexity advantage of amplitude estimation while providing finite-sample confidence guarantees and reduced estimator variance. The framework is demonstrated on benchmark problems with spatially correlated lognormal Young's modulus fields generated using a Nystrom low-rank Gaussian kernel model. Numerical results show that the proposed estimator achieves substantially lower oracle complexity than classical Monte Carlo CVaR estimation at comparable confidence levels, while maintaining rigorous statistical reliability. This work establishes a practically robust and theoretically grounded quantum-enhanced methodology for tail-risk quantification in stochastic continuum mechanics.

</details>
