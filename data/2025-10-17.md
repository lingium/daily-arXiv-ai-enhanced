<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 4]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.ME](#stat.ME) [Total: 15]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Long-Term Spatio-Temporal Forecasting of Monthly Rainfall in West Bengal Using Ensemble Learning Approaches](https://arxiv.org/abs/2510.13927)
*Jishu Adhikary,Raju Maiti*

Main category: stat.AP

TL;DR: 该研究开发了一个分层建模框架，结合回归模型和多层感知器(MLP)来预测西孟加拉邦19个地区的长期月降雨量，使用1900-2019年的百年尺度数据集。


<details>
  <summary>Details</summary>
Motivation: 降雨预报在气候适应、农业和水资源管理中至关重要，需要处理降雨动态的非线性和复杂结构，同时考虑时间依赖性和空间相互作用。

Method: 提出分层建模框架：首先使用回归模型预测年度特征（年总量、季度比例、变异性指标等），然后将这些预测作为辅助输入集成到MLP模型中，捕捉月序列的非线性时间模式和空间依赖性。

Result: 分层回归-MLP架构提供了稳健的长期时空预报，能够预测未来108个月（9年）的降雨量。

Conclusion: 该方法为农业、灌溉规划和水资源保护策略提供了有价值的见解，证明了分层建模在长期降雨预报中的有效性。

Abstract: Rainfall forecasting plays a critical role in climate adaptation,
agriculture, and water resource management. This study develops long-term
forecasts of monthly rainfall across 19 districts of West Bengal using a
century-scale dataset spanning 1900-2019. Daily rainfall records are aggregated
into monthly series, resulting in 120 years of observations for each district.
The forecasting task involves predicting the next 108 months (9 years,
2011-2019) while accounting for temporal dependencies and spatial interactions
among districts. To address the nonlinear and complex structure of rainfall
dynamics, we propose a hierarchical modeling framework that combines
regression-based forecasting of yearly features with multi-layer perceptrons
(MLPs) for monthly prediction. Yearly features, such as annual totals,
quarterly proportions, variability measures, skewness, and extremes, are first
forecasted using regression models that incorporate both own lags and
neighboring-district lags. These forecasts are then integrated as auxiliary
inputs into an MLP model, which captures nonlinear temporal patterns and
spatial dependencies in the monthly series. The results demonstrate that the
hierarchical regression-MLP architecture provides robust long-term
spatio-temporal forecasts, offering valuable insights for agriculture,
irrigation planning, and water conservation strategies.

</details>


### [2] [Earthquake Forecasting with ETAS.inlabru](https://arxiv.org/abs/2510.13930)
*Ziwen Zhong*

Main category: stat.AP

TL;DR: 该报告比较了ETAS地震预测模型的两种贝叶斯推断方法：传统MCMC和基于INLA的inlabru方法。通过三个实验分析了参数相关性、模型拟合效果和预测性能，发现inlabru方法在计算效率和参数估计方面优于MCMC。


<details>
  <summary>Details</summary>
Motivation: ETAS模型是当前最流行的地震预测模型，但传统的MCMC方法计算耗时且受参数相关性限制。研究旨在探索基于INLA的inlabru方法是否能解决这些问题并提升贝叶斯推断性能。

Method: 使用inlabru进行ETAS模型的贝叶斯推断，通过泰勒展开和分箱策略近似对数似然函数。进行三个实验：参数固定对后验分布的影响、真实与合成目录的归一化事件间隔时间分布比较、以及使用主震前事件预测主震后十周的事件。

Result: 发现α和K参数存在明显的相互影响关系，固定这些参数可将模型拟合时间减少一半以上。真实数据和合成目录的事件间隔时间分布一致，但相比Exp(1)分布有更多短和长间隔时间，表明存在聚类现象。预测实验显示需要至少一个主震及其子事件才能进行可靠预测。

Conclusion: inlabru方法在ETAS模型的贝叶斯推断中表现优于MCMC，计算效率更高且能处理参数不确定性。当数据中包含更多主震时，预测效果会更好，为地震预测提供了更有效的工具。

Abstract: The ETAS models are currently the most popular in the field of earthquake
forecasting. The MCMC method is time-consuming and limited by parameter
correlation while bringing parameter uncertainty. The INLA-based method
"inlabru" solves these problems and performs better at Bayesian inference.
  The report introduces the composition of the ETAS model, then provides the
model's log-likelihood and approximates it using Taylor expansion and binning
strategies. We also present the general procedure of Bayesian inference in
inlabru.
  The report follows three experiments. The first one explores the effect of
fixing one parameter at its actual or wrong values on the posterior
distribution of other parameters. We found that $\alpha$ and $K$ have an
apparent mutual influence relationship. At the same time, fixing $\alpha$ or
$K$ to its actual value can reduce the model fitting time by more than half.
  The second experiment compares normalised inter-event-time distribution on
real data and synthetic catalogues. The distributions of normalised
inter-event-time of real data and synthetic catalogues are consistent. Compared
with Exp(1), they have more short and long inter-event-time, indicating the
existence of clustering. Change on $\mu$ and $p$ will influence the
inter-event-time distribution.
  In the last one, we use events before the mainshock to predict events ten
weeks after the mainshock. We use the number test and Continuous Ranked
Probability Score (CRPS) to measure the accuracy and precision of the
predictions. We found that we need at least one mainshock and corresponding
offspring to make reliable forecasting. And when we have more mainshocks in our
data, our forecasting will be better. Besides, we also figure out what is
needed to obtain a good posterior distribution for each parameter.

</details>


### [3] [A Data-Parsimonious Model for Long-Term Risk Assessments of West Nile Virus Spillover](https://arxiv.org/abs/2510.14011)
*Saman Hosseini,Lee W. Cohnstaedt,Matin Marjani,Caterina Scoglio*

Main category: stat.AP

TL;DR: 提出了一种数据简约的西尼罗河病毒预测模型，结合温度驱动的隔室模型和非参数核密度估计，能够提前数月预测疫情爆发时间和严重程度。


<details>
  <summary>Details</summary>
Motivation: 许多西尼罗河病毒预测框架需要昆虫学或鸟类监测数据，但这些数据在某些地区可能无法获得，因此需要开发数据要求更低的预测方法。

Method: 结合温度驱动的西尼罗河病毒隔室模型和非参数核密度估计方法，构建联合概率密度函数和泊松率曲面，作为蚊虫丰度和归一化累积温度的函数。

Result: 模型在加州、德克萨斯州和佛罗里达州的六个县进行了评估，在不同生态和气候条件下均表现出良好的预测性能。

Conclusion: 该模型能够在传播季节开始前数月提供可靠的预测，支持主动的疫情防控措施，适用于不同生态和气候条件的地区。

Abstract: Many West Nile virus (WNV) forecasting frameworks incorporate entomological
or avian surveillance data, which may be unavailable in some regions. We
introduce a novel data-parsimonious probabilistic model to predict both the
timing of outbreak onset and the seasonal severity of WNV spillover. Our
approach combines a temperature-driven compartmental model of WNV with
nonparametric kernel density estimation methods to construct a joint
probability density function and a Poisson rate surface as function of mosquito
abundance and normalized cumulative temperature. Calibrated on human incidence
records, the model produces reliable forecasts several months before the
transmission season begins, supporting proactive mitigation efforts. We
evaluated the framework across three counties in California (Orange, Los
Angeles, and Riverside), two in Texas (Dallas and Harris), and one in Florida
(Duval), representing completely different ecology and distinct climatic
regimes, and observed strong agreement across multiple performance metrics.

</details>


### [4] [Bayes-ically fair: A Bayesian Ranking of the Olympic Medal Table](https://arxiv.org/abs/2510.14723)
*Cormac MacDermott,Carl J. Scarrott,John Ferguson*

Main category: stat.AP

TL;DR: 提出贝叶斯排名方案，通过长期奖牌人口比来评估国家奥委会表现，减少人口规模影响和小国随机波动


<details>
  <summary>Details</summary>
Motivation: 传统奥运奖牌排名受人口规模影响过大，需要更公平评估国家体育表现的方法

Method: 使用贝叶斯排名算法，通过收缩效应减少大人口影响和小国随机波动，计算长期奖牌人口比

Result: 该方法提供了比现有方法更稳定和可解释的国家体育表现排序

Conclusion: 贝叶斯排名方案能更公平地评估国家奥运表现，减少人口因素干扰

Abstract: Evaluating a country's sporting success provides insight into its
decision-making and infrastructure for developing athletic talent. The Olympic
Games serve as a global benchmark, yet conventional medal rankings can be
unduly influenced by population size. We propose a Bayesian ranking scheme to
rank the performance of National Olympic Committees by their "long-run"
medals-to-population ratio. The algorithm aims to mitigate the influence of
large populations and reduce the stochastic fluctuations for smaller nations by
applying shrinkage. These long-run rankings provide a more stable and
interpretable ordering of national sporting performance across games compared
to existing methods.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [5] [Exact Dynamics of Multi-class Stochastic Gradient Descent](https://arxiv.org/abs/2510.14074)
*Elizabeth Collins-Woodfin,Inbar Seroussi*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We develop a framework for analyzing the training and learning rate dynamics
on a variety of high- dimensional optimization problems trained using one-pass
stochastic gradient descent (SGD) with data generated from multiple anisotropic
classes. We give exact expressions for a large class of functions of the
limiting dynamics, including the risk and the overlap with the true signal, in
terms of a deterministic solution to a system of ODEs. We extend the existing
theory of high-dimensional SGD dynamics to Gaussian-mixture data and a large
(growing with the parameter size) number of classes. We then investigate in
detail the effect of the anisotropic structure of the covariance of the data in
the problems of binary logistic regression and least square loss. We study
three cases: isotropic covariances, data covariance matrices with a large
fraction of zero eigenvalues (denoted as the zero-one model), and covariance
matrices with spectra following a power-law distribution. We show that there
exists a structural phase transition. In particular, we demonstrate that, for
the zero-one model and the power-law model with sufficiently large power, SGD
tends to align more closely with values of the class mean that are projected
onto the "clean directions" (i.e., directions of smaller variance). This is
supported by both numerical simulations and analytical studies, which show the
exact asymptotic behavior of the loss in the high-dimensional limit.

</details>


### [6] [deFOREST: Fusing Optical and Radar satellite data for Enhanced Sensing of Tree-loss](https://arxiv.org/abs/2510.14092)
*Julio Enrique Castrillon-Candas,Hanfeng Gu,Caleb Meredith,Yulin Li,Xiaojing Tang,Pontus Olofsson,Mark Kon*

Main category: stat.ML

TL;DR: 开发了一个结合光学和SAR数据的森林砍伐检测流程，使用KL展开的残差空间构建光学异常图，结合HMM分类森林状态，在亚马逊森林测试显示混合方法比现有方法更准确且对稀疏光学数据更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 解决传统统计参数方法需要先验数据分布知识的局限性，特别是在高维数据场景下，开发不依赖数据分布假设的森林砍伐检测方法。

Method: 使用KL展开残差空间构建光学异常图，通过残差分量的浓度边界量化异常，结合SAR数据，利用隐马尔可夫模型进行森林状态分类。

Result: 在亚马逊森林92.19km×91.80km区域测试，混合光学-雷达方法和纯光学方法都达到高精度，优于现有混合方法，且混合方法对稀疏光学数据更具鲁棒性。

Conclusion: 提出的混合光学-SAR森林砍伐检测方法在精度和鲁棒性方面优于现有技术，特别适用于多云地区的光学数据稀疏情况。

Abstract: In this paper we develop a deforestation detection pipeline that incorporates
optical and Synthetic Aperture Radar (SAR) data. A crucial component of the
pipeline is the construction of anomaly maps of the optical data, which is done
using the residual space of a discrete Karhunen-Lo\`{e}ve (KL) expansion.
Anomalies are quantified using a concentration bound on the distribution of the
residual components for the nominal state of the forest. This bound does not
require prior knowledge on the distribution of the data. This is in contrast to
statistical parametric methods that assume knowledge of the data distribution,
an impractical assumption that is especially infeasible for high dimensional
data such as ours. Once the optical anomaly maps are computed they are combined
with SAR data, and the state of the forest is classified by using a Hidden
Markov Model (HMM). We test our approach with Sentinel-1 (SAR) and Sentinel-2
(Optical) data on a $92.19\,km \times 91.80\,km$ region in the Amazon forest.
The results show that both the hybrid optical-radar and optical only methods
achieve high accuracy that is superior to the recent state-of-the-art hybrid
method. Moreover, the hybrid method is significantly more robust in the case of
sparse optical data that are common in highly cloudy regions.

</details>


### [7] [High-Dimensional BWDM: A Robust Nonparametric Clustering Validation Index for Large-Scale Data](https://arxiv.org/abs/2510.14145)
*Mohammed Baragilly,Hend Gabr*

Main category: stat.ML

TL;DR: 提出HD-BWDM方法，一种针对高维数据的鲁棒聚类验证框架，通过随机投影和PCA解决维度灾难，使用修剪聚类和中位数距离确保对异常值的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类有效性指标如Calinski-Harabasz、Silhouette等在处理高维或受污染数据时性能下降，需要开发新的鲁棒验证方法。

Method: 结合随机投影和主成分分析缓解维度灾难，采用修剪聚类和中位数距离确保对异常值的鲁棒性，基于Johnson-Lindenstrauss嵌入理论。

Result: 模拟实验显示HD-BWDM在高维投影和污染情况下保持稳定性和可解释性，优于传统基于质心的验证标准。

Conclusion: HD-BWDM为现代高维应用中的非参数聚类提供了理论基础坚实、计算效率高的停止规则。

Abstract: Determining the appropriate number of clusters in unsupervised learning is a
central problem in statistics and data science. Traditional validity indices
such as Calinski-Harabasz, Silhouette, and Davies-Bouldin-depend on
centroid-based distances and therefore degrade in high-dimensional or
contaminated data. This paper proposes a new robust, nonparametric clustering
validation framework, the High-Dimensional Between-Within Distance Median
(HD-BWDM), which extends the recently introduced BWDM criterion to
high-dimensional spaces. HD-BWDM integrates random projection and principal
component analysis to mitigate the curse of dimensionality and applies trimmed
clustering and medoid-based distances to ensure robustness against outliers. We
derive theoretical results showing consistency and convergence under
Johnson-Lindenstrauss embeddings. Extensive simulations demonstrate that
HD-BWDM remains stable and interpretable under high-dimensional projections and
contamination, providing a robust alternative to traditional centroid-based
validation criteria. The proposed method provides a theoretically grounded,
computationally efficient stopping rule for nonparametric clustering in modern
high-dimensional applications.

</details>


### [8] [A novel Information-Driven Strategy for Optimal Regression Assessment](https://arxiv.org/abs/2510.14222)
*Benjamín Castro,Camilo Ramírez,Sebastián Espinosa,Jorge F. Silva,Marcos E. Orchard,Heraldo Rozas*

Main category: stat.ML

TL;DR: 提出了一种名为Information Teacher的新框架，通过估计输入变量与残差之间的香农互信息来评估回归算法的全局最优性，并提供形式化性能保证。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，评估学习回归器的质量具有挑战性，因为无法访问真实数据生成机制，且没有数据驱动方法能确保全局最优性的可实现性。

Method: 基于估计输入变量与残差之间的香农互信息，适用于广泛的加性噪声模型，构建信息教师框架来评估回归算法。

Result: 数值实验证实信息教师能够检测全局最优性，这与相对于真实模型的零估计误差条件一致，可作为真实评估损失的替代度量。

Conclusion: 信息教师为评估回归算法提供了有原则的替代方案，能够检测全局最优性并作为传统经验性能指标的补充。

Abstract: In Machine Learning (ML), a regression algorithm aims to minimize a loss
function based on data. An assessment method in this context seeks to quantify
the discrepancy between the optimal response for an input-output system and the
estimate produced by a learned predictive model (the student). Evaluating the
quality of a learned regressor remains challenging without access to the true
data-generating mechanism, as no data-driven assessment method can ensure the
achievability of global optimality. This work introduces the Information
Teacher, a novel data-driven framework for evaluating regression algorithms
with formal performance guarantees to assess global optimality. Our novel
approach builds on estimating the Shannon mutual information (MI) between the
input variables and the residuals and applies to a broad class of additive
noise models. Through numerical experiments, we confirm that the Information
Teacher is capable of detecting global optimality, which is aligned with the
condition of zero estimation error with respect to the -- inaccessible, in
practice -- true model, working as a surrogate measure of the ground truth
assessment loss and offering a principled alternative to conventional empirical
performance metrics.

</details>


### [9] [Personalized federated learning, Row-wise fusion regularization, Multivariate modeling, Sparse estimation](https://arxiv.org/abs/2510.14413)
*Runlin Zhou,Letian Li,Zemin Zheng*

Main category: stat.ML

TL;DR: 提出了一种稀疏行级融合(SROF)正则化器和RowFed算法，用于个性化联邦多变量学习，通过行级聚类和稀疏性在保护隐私的同时提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化联邦学习中存在局限性：逐项惩罚忽略了跨响应依赖性，而矩阵级融合过度耦合客户端。需要一种能平衡变量级结构共享和客户端异质性的方法。

Method: 开发了SROF正则化器，跨客户端聚类行向量并诱导行内稀疏性；提出了RowFed算法，将SROF嵌入到线性化ADMM框架中，支持隐私保护的客户端部分参与。

Result: 理论证明SROF具有oracle性质，能正确恢复变量级分组并具有渐近正态性；RowFed收敛到平稳解，在随机客户端参与下迭代间隙以参与概率相关的速率收缩。

Conclusion: 行级融合是规模化个性化联邦多变量学习的有效透明范式，在模拟和真实数据中均优于现有方法，在保持可解释性的同时提高了估计和预测精度。

Abstract: We study personalized federated learning for multivariate responses where
client models are heterogeneous yet share variable-level structure. Existing
entry-wise penalties ignore cross-response dependence, while matrix-wise fusion
over-couples clients. We propose a Sparse Row-wise Fusion (SROF) regularizer
that clusters row vectors across clients and induces within-row sparsity, and
we develop RowFed, a communication-efficient federated algorithm that embeds
SROF into a linearized ADMM framework with privacy-preserving partial
participation. Theoretically, we establish an oracle property for
SROF-achieving correct variable-level group recovery with asymptotic
normality-and prove convergence of RowFed to a stationary solution. Under
random client participation, the iterate gap contracts at a rate that improves
with participation probability. Empirically, simulations in heterogeneous
regimes show that RowFed consistently lowers estimation and prediction error
and strengthens variable-level cluster recovery over NonFed, FedAvg, and a
personalized matrix-fusion baseline. A real-data study further corroborates
these gains while preserving interpretability. Together, our results position
row-wise fusion as an effective and transparent paradigm for large-scale
personalized federated multivariate learning, bridging the gap between
entry-wise and matrix-wise formulations.

</details>


### [10] [Local Causal Discovery for Statistically Efficient Causal Inference](https://arxiv.org/abs/2510.14582)
*Mátyás Schubert,Tom Claassen,Sara Magliacane*

Main category: stat.ML

TL;DR: LOAD是一种结合局部方法计算效率和全局方法统计最优性的因果发现方法，能够发现最优调整集用于因果效应估计。


<details>
  <summary>Details</summary>
Motivation: 全局因果发现方法虽然能恢复最优调整集但计算复杂度高，局部方法虽然可扩展但只能获得统计次优的调整集，需要结合两者优势。

Method: LOAD首先识别目标变量间的因果关系并测试可识别性，如果可识别则通过局部因果发现推断中介变量及其父节点来找到最优调整集，否则返回基于局部结构的有效父调整集。

Result: 在合成和真实数据实验中，LOAD在可扩展性上优于全局方法，在效应估计准确性上优于局部方法。

Conclusion: LOAD成功地将局部方法的计算效率与全局方法的统计最优性相结合，提供了一种既高效又准确的因果效应估计方法。

Abstract: Causal discovery methods can identify valid adjustment sets for causal effect
estimation for a pair of target variables, even when the underlying causal
graph is unknown. Global causal discovery methods focus on learning the whole
causal graph and therefore enable the recovery of optimal adjustment sets,
i.e., sets with the lowest asymptotic variance, but they quickly become
computationally prohibitive as the number of variables grows. Local causal
discovery methods offer a more scalable alternative by focusing on the local
neighborhood of the target variables, but are restricted to statistically
suboptimal adjustment sets. In this work, we propose Local Optimal Adjustments
Discovery (LOAD), a sound and complete causal discovery approach that combines
the computational efficiency of local methods with the statistical optimality
of global methods. First, LOAD identifies the causal relation between the
targets and tests if the causal effect is identifiable by using only local
information. If it is identifiable, it then finds the optimal adjustment set by
leveraging local causal discovery to infer the mediators and their parents.
Otherwise, it returns the locally valid parent adjustment sets based on the
learned local structure. In our experiments on synthetic and realistic data
LOAD outperforms global methods in scalability, while providing more accurate
effect estimation than local methods.

</details>


### [11] [Parameter Identification for Partial Differential Equation with Jump Discontinuities in Coefficients by Markov Switching Model and Physics-Informed Machine Learning](https://arxiv.org/abs/2510.14656)
*Zhikun Zhang,Guanyu Pan,Xiangjun Wang,Yong Xu,Guangtao Zhang*

Main category: stat.ML

TL;DR: 提出了一种结合物理信息深度学习和贝叶斯推理的新计算框架，用于识别具有跳跃不连续系数的偏微分方程中的参数。


<details>
  <summary>Details</summary>
Motivation: 传统数值和机器学习方法在处理具有不连续系数的PDE反问题时面临高维性、固有非线性和不连续参数空间的限制。

Method: 采用双网络架构和梯度自适应加权策略：主网络逼近PDE解，子网络采样其系数；使用马尔可夫动力学方法捕捉复杂时空系统的隐藏状态转移。

Result: 在各种具有跳跃变化系数的PDE上的综合数值实验表明，该框架相比现有方法具有卓越的适应性、准确性和鲁棒性。

Conclusion: 该研究为具有不连续参数结构的PDE，特别是在非平稳或异质系统中的参数识别提供了一种可推广的计算方法。

Abstract: Inverse problems involving partial differential equations (PDEs) with
discontinuous coefficients are fundamental challenges in modeling complex
spatiotemporal systems with heterogeneous structures and uncertain dynamics.
Traditional numerical and machine learning approaches often face limitations in
addressing these problems due to high dimensionality, inherent nonlinearity,
and discontinuous parameter spaces. In this work, we propose a novel
computational framework that synergistically integrates physics-informed deep
learning with Bayesian inference for accurate parameter identification in PDEs
with jump discontinuities in coefficients. The core innovation of our framework
lies in a dual-network architecture employing a gradient-adaptive weighting
strategy: a main network approximates PDE solutions while a sub network samples
its coefficients. To effectively identify mixture structures in parameter
spaces, we employ Markovian dynamics methods to capture hidden state
transitions of complex spatiotemporal systems. The framework has applications
in reconstruction of solutions and identification of parameter-varying regions.
Comprehensive numerical experiments on various PDEs with jump-varying
coefficients demonstrate the framework's exceptional adaptability, accuracy,
and robustness compared to existing methods. This study provides a
generalizable computational approach of parameter identification for PDEs with
discontinuous parameter structures, particularly in non-stationary or
heterogeneous systems.

</details>


### [12] [Fast and Scalable Score-Based Kernel Calibration Tests](https://arxiv.org/abs/2510.14711)
*Pierre Glaser,David Widmann,Fredrik Lindsten,Arthur Gretton*

Main category: stat.ML

TL;DR: 提出KCCSD测试，一种基于核的非参数方法，用于评估具有明确定义分数的概率模型的校准性，无需昂贵的期望近似且能控制I类错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要昂贵的期望近似，且难以控制I类错误，因此需要一种更高效且统计可靠的校准测试方法。

Method: 使用新的基于分数的概率核族，无需概率密度样本即可估计；采用条件拟合优度准则处理KCCSD测试的U统计量。

Result: 在各种合成设置中验证了测试的性能和特性。

Conclusion: KCCSD测试提供了一种高效且统计可靠的校准评估方法，无需昂贵的计算近似。

Abstract: We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD
test), a non-parametric, kernel-based test for assessing the calibration of
probabilistic models with well-defined scores. In contrast to previous methods,
our test avoids the need for possibly expensive expectation approximations
while providing control over its type-I error. We achieve these improvements by
using a new family of kernels for score-based probabilities that can be
estimated without probability density samples, and by using a conditional
goodness-of-fit criterion for the KCCSD test's U-statistic. We demonstrate the
properties of our test on various synthetic settings.

</details>


### [13] [A Geometric Approach to Optimal Experimental Design](https://arxiv.org/abs/2510.14848)
*Gavin Kerrigan,Christian A. Naesseth,Tom Rainforth*

Main category: stat.ML

TL;DR: 提出了一种基于最优传输理论的新型几何框架用于最优实验设计，称为互传输依赖度(MTD)，相比传统基于概率密度的方法具有更好的几何特性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统最优实验设计方法(如基于互信息的方法)依赖概率密度，导致限制性的不变性特性，需要更灵活、几何化的替代方案。

Method: 利用最优传输理论提出互传输依赖度(MTD)作为统计依赖性的度量，通过选择适当的几何结构来定制化下游估计问题。

Result: 该框架能够生成高质量的设计方案，为传统信息论技术提供了灵活的替代选择。

Conclusion: 基于最优传输的几何框架为最优实验设计提供了新的视角，具有更好的几何特性和定制化能力。

Abstract: We introduce a novel geometric framework for optimal experimental design
(OED). Traditional OED approaches, such as those based on mutual information,
rely explicitly on probability densities, leading to restrictive invariance
properties. To address these limitations, we propose the mutual transport
dependence (MTD), a measure of statistical dependence grounded in optimal
transport theory which provides a geometric objective for optimizing designs.
Unlike conventional approaches, the MTD can be tailored to specific downstream
estimation problems by choosing appropriate geometries on the underlying
spaces. We demonstrate that our framework produces high-quality designs while
offering a flexible alternative to standard information-theoretic techniques.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [14] [Evaluating Policy Effects under Network Interference without Network Information: A Transfer Learning Approach](https://arxiv.org/abs/2510.14415)
*Tadao Hoshino*

Main category: stat.ME

TL;DR: 该论文开发了一个敏感性分析框架，将平均总处理效应(ATTE)从具有完全观测网络的数据源转移到网络完全未知的目标数据。通过考虑目标网络度分布的不确定性，构建了目标ATTE的边界估计。


<details>
  <summary>Details</summary>
Motivation: 解决在目标数据网络未知的情况下，如何将源数据中观测到的平均总处理效应转移到目标数据的问题，为政策评估提供网络不确定性下的敏感性分析。

Method: 基于Wasserstein距离测量目标网络度分布的不确定性，使用线性规划方法构建目标ATTE的边界估计器，并通过函数delta方法和wild bootstrap方法推导估计器的极限分布。

Result: 开发了一个能够处理网络不确定性的敏感性分析框架，能够为政策评估提供稳健的效应边界估计。

Conclusion: 该框架为网络数据不完全观测的政策评估问题提供了实用的敏感性分析方法，在中国农民天气保险采纳的实证研究中展示了其应用价值。

Abstract: This paper develops a sensitivity analysis framework that transfers the
average total treatment effect (ATTE) from source data with a fully observed
network to target data whose network is completely unknown. The ATTE represents
the average social impact of a policy that assigns the treatment to every
individual in the dataset. We postulate a covariate-shift type assumption that
both source and target datasets share the same conditional mean outcome.
However, because the target network is unobserved, this assumption alone is not
sufficient to pin down the ATTE for the target data. To address this issue, we
consider a sensitivity analysis based on the uncertainty of the target
network's degree distribution, where the extent of uncertainty is measured by
the Wasserstein distance from a given reference degree distribution. We then
construct bounds on the target ATTE using a linear programming-based estimator.
The limiting distribution of the bound estimator is derived via the functional
delta method, and we develop a wild bootstrap approach to approximate the
distribution. As an empirical illustration, we revisit the social network
experiment on farmers' weather insurance adoption in China by Cai et al.
(2015).

</details>


### [15] [Bayesian Inference for Single-factor Graphical Models](https://arxiv.org/abs/2510.13981)
*David Marcano,Adrian Dobra*

Main category: stat.ME

TL;DR: 提出了用于单因子图模型的高效MCMC算法，该模型结合了因子分析和高斯图模型，可处理连续、二元和有序分类变量，并能建模时空相关数据集。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时捕捉跨时空的潜在因子关联和每个时空点的残差条件依赖结构的统一建模框架。

Method: 使用单因子图模型，结合因子分析和高斯图模型，通过MCMC算法进行贝叶斯推断，支持多种数据类型和时空相关数据集。

Result: 在模拟和真实世界示例中验证了模型的有效性，能够成功捕捉复杂的多变量关联和残差依赖结构。

Conclusion: 单因子图模型提供了一个灵活的框架，能够有效建模具有复杂依赖结构的多变量数据，特别适用于时空相关数据集的分析。

Abstract: We introduce efficient MCMC algorithms for Bayesian inference for
single-factor models with correlated residuals where the residuals'
distribution is a Gaussian graphical model. We call this family of models
single-factor graphical models. We extend single-factor graphical models to
datasets that also involve binary and ordinal categorical variables and to the
modeling of multiple datasets that are spatially or temporally related. Our
models are able to capture multivariate associations through latent factors
across time and space, as well as residual conditional dependence structures at
each spatial location or time point through Gaussian graphical models. We
illustrate the application of single-factor graphical models in simulated and
real-world examples.

</details>


### [16] [ROC Analysis with Covariate Adjustment Using Neural Network Models: Evaluating the Role of Age in the Physical Activity-Mortality Association](https://arxiv.org/abs/2510.14494)
*Ziad Akram Ali Hammouri,Yating Zhou,Rahul Ghosal,Juan C. Vidal,Marcos Matabuena*

Main category: stat.ME

TL;DR: 提出基于神经网络的协变量调整ROC建模框架，用于灵活评估生物标志物在两个参考群体中的区分能力，并通过模拟和临床案例验证方法性能。


<details>
  <summary>Details</summary>
Motivation: 传统ROC曲线评估生物标志物效果有限，而协变量调整ROC曲线能进行个体化评估。机器学习模型在此背景下应用较少，但具有开发更强大评估方法的潜力。

Method: 开发神经网络协变量调整ROC建模框架，支持生物标志物、协变量和参考群体之间灵活的非线性依赖关系评估。通过模拟测试验证方法在有限样本下的性能。

Result: 模拟测试显示方法在不同依赖结构下表现良好。临床案例研究表明，按性别分层并调整年龄和BMI后，日常身体活动（TAC）作为生物标志物在预测3、5、8年死亡率方面具有不同协变量效应。

Conclusion: 协变量调整建模在生物标志物评估中至关重要，TAC作为功能能力生物标志物具有潜力，其效果取决于个体特定特征。

Abstract: The receiver operating characteristic (ROC) curve and its summary measure,
the Area Under the Curve (AUC), are well-established tools for evaluating the
efficacy of biomarkers in biomedical studies. Compared to the traditional ROC
curve, the covariate-adjusted ROC curve allows for individual evaluation of the
biomarker. However, the use of machine learning models has rarely been explored
in this context, despite their potential to develop more powerful and
sophisticated approaches for biomarker evaluation. The goal of this paper is to
propose a framework for neural network-based covariate-adjusted ROC modeling
that allows flexible and nonlinear evaluation of the effectiveness of a
biomarker to discriminate between two reference populations. The finite-sample
performance of our method is investigated through extensive simulation tests
under varying dependency structures between biomarkers, covariates, and
referenced populations. The methodology is further illustrated in a clinically
case study that assesses daily physical activity - measured as total activity
time (TAC), a proxy for daily step count-as a biomarker to predict mortality at
three, five and eight years. Analyzes stratified by sex and adjusted for age
and BMI reveal distinct covariate effects on mortality outcomes. These results
underscore the importance of covariate-adjusted modeling in biomarker
evaluation and highlight TAC's potential as a functional capacity biomarker
based on specific individual characteristics.

</details>


### [17] [Joint modeling and inference of multiple-subject high-dimensional sparse vector autoregressive models](https://arxiv.org/abs/2510.14044)
*Younghoon Kim,Zachary F. Fisher,Vladas Pipiras*

Main category: stat.ME

TL;DR: 提出了一种新的多主体向量自回归模型可识别性条件，基于通信高效的数据集成框架，改进了传统方法中加权中位数导致的统计效率低下问题，并开发了路径零性和同质性检验方法。


<details>
  <summary>Details</summary>
Motivation: 传统多主体VAR模型使用加权中位数识别共同效应，导致统计效率低下，收敛速度受最不稀疏主体和最小样本量的限制。

Method: 基于通信高效数据集成框架的新可识别性条件，构建个体去偏估计量的Wald型检验统计量，用于检验路径的零性和同质性。

Result: 新方法实现了针对每个主体稀疏水平和样本量的收敛速度，模拟研究和实际数据应用显示其在标准评估指标上优于现有基准方法。

Conclusion: 提出的方法有效解决了多主体VAR模型的统计效率问题，提供了更精确的路径检验框架，在异质性场景下表现优异。

Abstract: The multiple-subject vector autoregression (multi-VAR) model captures
heterogeneous network Granger causality across subjects by decomposing
individual sparse VAR transition matrices into commonly shared and
subject-unique paths. The model has been applied to characterize hidden shared
and unique paths among subjects and has demonstrated performance compared to
methods commonly used in psychology and neuroscience. Despite this innovation,
the model suffers from using a weighted median for identifying the common
effects, leading to statistical inefficiency as the convergence rates of the
common and unique paths are determined by the least sparse subject and the
smallest sample size across all subjects. We propose a new identifiability
condition for the multi-VAR model based on a communication-efficient data
integration framework. We show that this approach achieves convergence rates
tailored to each subject's sparsity level and sample size. Furthermore, we
develop hypothesis tests to assess the nullity and homogeneity of individual
paths, using Wald-type test statistics constructed from individual debiased
estimators. A test for the significance of the common paths can also be derived
through the framework. Simulation studies under various heterogeneity scenarios
and a real data application demonstrate the performance of the proposed method
compared to existing benchmark across standard evaluation metrics.

</details>


### [18] [Additive Density Regression](https://arxiv.org/abs/2510.14502)
*Eva-Maria Maier,Alexander Fottner,Sonja Greven,Almond Stöcker*

Main category: stat.ME

TL;DR: 提出了一种结构化加性回归方法来建模给定标量协变量的条件密度，其中仅观测到条件分布的样本。该方法在贝叶斯希尔伯特空间中构建，能够处理连续、离散和混合密度，并建立了理论保证和统计推断框架。


<details>
  <summary>Details</summary>
Motivation: 需要建模仅观测到条件分布样本的情况下的条件密度，特别是在经济性别数据中分析女性收入份额分布时，该变量是连续但包含离散点质量的混合类型密度。

Method: 在贝叶斯希尔伯特空间中构建结构化加性回归模型，通过惩罚最大似然估计，并证明其等价于适当的多项式或泊松回归模型。

Result: 建立了估计量的渐近存在性、唯一性、一致性和正态性，提供了置信区域和密度效应推断。应用于德国社会经济面板数据，成功分析了女性收入份额分布。

Conclusion: 该方法为处理混合类型密度的条件分布建模提供了理论严谨且实用的框架，特别适用于包含离散点质量的连续变量分析。

Abstract: We present a structured additive regression approach to model conditional
densities given scalar covariates, where only samples of the conditional
distributions are observed. This links our approach to distributional
regression models for scalar data. The model is formulated in a Bayes Hilbert
space -- preserving nonnegativity and integration to one under summation and
scalar multiplication -- with respect to an arbitrary finite measure. This
allows to consider, amongst others, continuous, discrete and mixed densities.
Our theoretical results include asymptotic existence, uniqueness, consistency,
and asymptotic normality of the penalized maximum likelihood estimator, as well
as confidence regions and inference for the (effect) densities. For estimation,
we propose to maximize the penalized log-likelihood corresponding to an
appropriate multinomial, or equivalently, Poisson regression model, which we
show to approximate the original penalized maximum likelihood problem. We apply
our framework to a motivating gender economic data set from the German
Socio-Economic Panel Study (SOEP), analyzing the distribution of the woman's
share in a couple's total labor income given covariate effects for year, place
of residence and age of the youngest child. As the income share is a continuous
variable having discrete point masses at zero and one for single-earner
couples, the corresponding densities are of mixed type.

</details>


### [19] [Efficient Estimation of the Complier General Causal Effect in Randomized Controlled Trials with One-Sided Noncompliance](https://arxiv.org/abs/2510.14142)
*Yin Tang,Yanyuan Ma,Jiwei Zhao*

Main category: stat.ME

TL;DR: 提出了一种新的估计量CGCE来处理随机对照试验中的单边不依从问题，并在最小假设下实现了高效估计。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验在实践中常因单边不依从等问题而影响随机化效果，需要新的方法来准确估计依从者的因果效应。

Method: 提出了依从者一般因果效应(CGCE)这一通用估计量，研究了在最小假设下实现高效估计的条件。

Result: 通过综合模拟研究和真实数据应用验证了所提方法的有效性，并与现有方法进行了比较。

Conclusion: CGCE为处理单边不依从问题提供了一种有效的解决方案，在最小假设下实现了对依从者因果效应的准确估计。

Abstract: A randomized controlled trial (RCT) is widely regarded as the gold standard
for assessing the causal effect of a treatment or intervention, assuming
perfect implementation. In practice, however, randomization can be compromised
for various reasons, such as one-sided noncompliance. In this paper, we address
the issue of one-sided noncompliance and propose a general estimand, the
complier general causal effect (CGCE), to characterize the causal effect among
compliers. We further investigate the conditions under which efficient
estimation of the CGCE can be achieved under minimal assumptions. Comprehensive
simulation studies and a real data application are conducted to illustrate the
proposed methods and to compare them with existing approaches.

</details>


### [20] [Loss functions arising from the index of agreement](https://arxiv.org/abs/2510.14714)
*Hristos Tyralis,Georgia Papacharalampous*

Main category: stat.ME

TL;DR: 本文分析了Willmott一致性指数的负向损失函数$L_W$的理论性质，提出了改进版本$L_{\operatorname{NR}_2}$，该新损失函数保持了$L_W$的优点且具有闭式解，并探讨了不同损失函数在预测变量与因变量相关性趋近1时的收敛行为。


<details>
  <summary>Details</summary>
Motivation: Willmott一致性指数是环境科学和工程中常用的评估指标，但其负向损失函数$L_W$的理论性质尚未得到充分研究。本文旨在深入分析$L_W$的理论特性，并提出理论改进方案。

Method: 通过理论分析证明$L_W$的有界性、平移和尺度不变性等性质，提出用欧几里得距离之和替换$L_W$分母的新损失函数$L_{\operatorname{NR}_2}$，并分析不同损失函数在参数估计中的收敛行为。

Result: $L_W$被证明在[0,1]内有界，具有平移和尺度不变性，并估计参数$\Bbb{E}_{F}[\underline{y}] \pm \Bbb{V}_{F}^{1/2}[\underline{y}]$。$L_{\operatorname{NR}_2}$保持了$L_W$的优点且具有闭式解，当预测变量与因变量相关性趋近1时，平方误差、$L_{\operatorname{NR}_2}$和$L_W$的参数估计结果收敛。

Conclusion: 提出的$L_{\operatorname{NR}_2}$损失函数是对$L_W$的理论改进，在水文模型校准等应用中具有实用价值，并为现有$L_p$-范数变体的进一步改进提供了方向。

Abstract: We examine the theoretical properties of the index of agreement loss function
$L_W$, the negatively oriented counterpart of Willmott's index of agreement, a
common metric in environmental sciences and engineering. We prove that $L_W$ is
bounded within [0, 1], translation and scale invariant, and estimates the
parameter $\Bbb{E}_{F}[\underline{y}] \pm \Bbb{V}_{F}^{1/2}[\underline{y}]$
when fitting a distribution. We propose $L_{\operatorname{NR}_2}$ as a
theoretical improvement, which replaces the denominator of $L_W$ with the sum
of Euclidean distances, better aligning with the underlying geometric
intuition. This new loss function retains the appealing properties of $L_W$ but
also admits closed-form solutions for linear model parameter estimation. We
show that as the correlation between predictors and the dependent variable
approaches 1, parameter estimates from squared error, $L_{\operatorname{NR}_2}$
and $L_W$ converge. This behavior is mirrored in hydrologic model calibration
(a core task in water resources engineering), where performance becomes nearly
identical across these loss functions. Finally, we suggest potential
improvements for existing $L_p$-norm variants of the index of agreement.

</details>


### [21] [Hypothesis testing for the uniformity of random geometric graph](https://arxiv.org/abs/2510.14210)
*Mingao Yuan*

Main category: stat.ME

TL;DR: 本文提出了第一个用于检验随机几何图中节点分布是否均匀的统计检验方法，解决了网络分析中节点分布均匀性检验的挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界网络可能更适合用非均匀分布建模，且非均匀分布的图与均匀分布图具有显著不同的性质。需要判断给定网络是否来自均匀分布的随机几何图。

Method: 使用基于退化U统计量的渐近理论，提出统计检验方法，检验统计量在零假设下收敛于标准正态分布。提供了从邻接矩阵高效计算检验统计量的方法。

Result: 模拟研究表明所提出的均匀性检验具有高功效。理论分析表征了检验的功效，并提供了实际数据应用。

Conclusion: 成功开发了第一个用于随机几何图均匀性检验的统计方法，解决了网络依赖结构带来的挑战，为网络分析提供了重要工具。

Abstract: Random geometric graphs are widely used in modeling geometry and dependence
structure in networks. In a random geometric graph, nodes are independently
generated from some probability distribution $F$ over a metric space, and edges
link nodes if their distance is less than some threshold. Most studies assume
the distribution $F$ to be uniform. However, recent research shows that some
real-world networks may be better modeled by nonuniform distribution $F$.
Moreover, graphs with nonuniform $F$ have notably different properties from
graphs with uniform $F$. A fundamental question is: given a network from a
random geometric graph, is the distribution $F$ uniform or not? In this paper,
we approach this question through hypothesis testing. This problem is
particularly challenging due to the inherent dependencies among edges in random
geometric graphs, a property not present in classic random graphs. We propose
the first statistical test. Under the null hypothesis, the test statistic
converges in distribution to the standard normal distribution. The asymptotic
distribution is derived using the asymptotic theory of degenerate U-statistics
with a kernel function dependent on the number of nodes. This technique is
different from existing methods in network hypothesis testing problems. In
addition, we present a method for efficiently calculating the test statistic
directly from the adjacency matrix. We also analytically characterize the power
of the proposed test. The simulation study shows that the proposed uniformity
test has high power. Real data applications are also provided.

</details>


### [22] [Marginal Causal Effect Estimation with Continuous Instrumental Variables](https://arxiv.org/abs/2510.14368)
*Mei Dong,Lin Liu,Dingke Tang,Geoffrey Liu,Wei Xu,Linbo Wang*

Main category: stat.ME

TL;DR: 提出了一个处理连续工具变量的新框架，通过条件加权平均导数效应来识别平均处理效应，统一了连续和分类工具变量的方法。


<details>
  <summary>Details</summary>
Motivation: 现有连续工具变量方法需要强参数模型或假设同质处理效应，而非参数方法在高维协变量下表现不佳，需要更灵活且稳健的估计方法。

Method: 使用条件Riesz表示子构建框架，通过二阶参数子模型刻画切线空间，开发了局部有效、三重稳健、有界且易于实现的估计器。

Result: 框架能够处理连续工具变量，在观测数据模型中实现平均处理效应的识别，并应用于癌症临床研究评估肥胖悖论。

Conclusion: 该方法为连续工具变量提供了统一的理论框架和实用的估计方法，在实证应用中展示了良好的性能。

Abstract: Instrumental variables (IVs) are often continuous, arising in diverse fields
such as economics, epidemiology, and the social sciences. Existing approaches
for continuous IVs typically impose strong parametric models or assume
homogeneous treatment effects, while fully nonparametric methods may perform
poorly in moderate- to high-dimensional covariate settings. We propose a new
framework for identifying the average treatment effect with continuous IVs via
conditional weighted average derivative effects. Using a conditional Riesz
representer, our framework unifies continuous and categorical IVs. In this
framework, the average treatment effect is typically overidentified, leading to
a semiparametric observed-data model with a nontrivial tangent space.
Characterizing this tangent space involves a delicate construction of a
second-order parametric submodel, which, to the best of our knowledge, has not
been standard practice in this literature. For estimation, building on an
influence function in the semiparametric model that is also locally efficient
within a submodel, we develop a locally efficient, triply robust, bounded, and
easy-to-implement estimator. We apply our methods to an observational clinical
study from the Princess Margaret Cancer Centre to examine the so-called obesity
paradox in oncology, assessing the causal effect of excess body weight on
two-year mortality among patients with non-small cell lung cancer.

</details>


### [23] [Response to Discussions of "Causal and Counterfactual Views of Missing Data Models"](https://arxiv.org/abs/2510.14694)
*Razieh Nabi,Rohit Bhattacharya,Ilya Shpitser,James M. Robins*

Main category: stat.ME

TL;DR: 这是一篇对讨论者评论的回应文章，作者总结主要贡献并逐一回应各方讨论


<details>
  <summary>Details</summary>
Motivation: 回应Levis和Kennedy、Luo和Geng、Wang和van der Laan、Yang和Kim等讨论者对作者论文的评论

Method: 通过总结主要贡献并逐一回应每个讨论者的评论

Result: 对讨论者的评论进行了系统性的回应和澄清

Conclusion: 通过回应讨论，进一步澄清和强调了论文的主要贡献

Abstract: We are grateful to the discussants, Levis and Kennedy [2025], Luo and Geng
[2025], Wang and van der Laan [2025], and Yang and Kim [2025], for their
thoughtful comments on our paper (Nabi et al., 2025). In this rejoinder, we
summarize our main contributions and respond to each discussion in turn.

</details>


### [24] [EM Approaches to Nonparametric Estimation for Mixture of Linear Regressions](https://arxiv.org/abs/2510.14890)
*Andrew Welbaum,Wanli Qiao*

Main category: stat.ME

TL;DR: 提出了两种EM算法来估计线性回归混合模型中的先验分布：第一种通过核化NPMLE方法估计连续或离散先验分布，第二种针对有密度先验分布的近似NPMLE方法。


<details>
  <summary>Details</summary>
Motivation: 在线性回归混合模型中，回归系数作为随机向量可能服从连续或离散分布，需要有效方法来估计这种先验分布。

Method: 第一种算法：求解核化非参数最大似然估计(NPMLE)，能恢复连续先验分布并准确估计离散先验的聚类数；第二种算法：近似NPMLE，针对有密度的先验分布，对离散先验需结合后处理步骤。

Result: 研究了两种算法的收敛性质，通过模拟和真实数据集验证了其有效性。

Conclusion: 提出的两种EM算法能有效估计线性回归混合模型中的先验分布，适用于连续和离散分布情况。

Abstract: In a mixture of linear regression model, the regression coefficients are
treated as random vectors that may follow either a continuous or discrete
distribution. We propose two Expectation-Maximization (EM) algorithms to
estimate this prior distribution. The first algorithm solves a kernelized
version of the nonparametric maximum likelihood estimation (NPMLE). This method
not only recovers continuous prior distributions but also accurately estimates
the number of clusters when the prior is discrete. The second algorithm,
designed to approximate the NPMLE, targets prior distributions with a density.
It also performs well for discrete priors when combined with a post-processing
step. We study the convergence properties of both algorithms and demonstrate
their effectiveness through simulations and applications to real datasets.

</details>


### [25] [Reframing cross-world independence for identifying path-specific effects](https://arxiv.org/abs/2510.14559)
*En-Yu Lai,Jih-Chang Yu,Yen-Tsung Huang*

Main category: stat.ME

TL;DR: 本文通过SWIGs分析三种因果语义，将可分离效应推广到多中介模型，并统一了经典、干预和可分离三种路径特定效应的解释框架。


<details>
  <summary>Details</summary>
Motivation: 理解复杂系统中的因果机制需要评估多中介模型中的路径特定效应，传统方法依赖苛刻的跨世界独立性假设，需要更实用的识别方法。

Method: 使用SWIGs分析三种因果语义的基础，将可分离效应从竞争风险推广到多中介模型，推导识别所需的假设条件。

Result: 统一了三种路径特定效应方法：中介驱动效应（经典）、随机对比（干预）和组件特定行动（可分离），发现跨世界独立性违反源于模型遗漏的中介变量。

Conclusion: 通过类比混杂控制，将跨世界独立性转化为可操作的建模策略，为复杂因果系统中的中介分析提供了更实用的路径。

Abstract: Understanding causal mechanisms in complex systems requires evaluating
path-specific effects (PSEs) in multi-mediator models. Identification of PSEs
traditionally relies on the demanding cross-world independence assumption. To
relax this, VanderWeele et al. (2014) proposed an interventional approach that
redefines PSEs, while Stensrud et al. (2021) introduced dismissible component
conditions for identifying separable effects under competing risks. In this
study, we leverage SWIGs to dissect the causal foundations of these three
semantics and make two key advances. First, we generalize separable effects
beyond competing risks to the setting of multi-mediator models and derive the
assumptions required for their identification. Second, we unify the three
approaches by clarifying how they interpret counterfactual outcomes
differently: as mediator-driven effects (classical), randomized contrasts
(interventional), or component-specific actions (separable). We further
demonstrate that violations of cross-world independence originate from
mediators omitted from the model. By analogy to confounder control, we argue
that just as exchangeability is achieved by conditioning on sufficient
confounders, cross-world independence can be approximated by including
sufficient mediators. This reframing turns an abstract assumption into a
tangible modeling strategy, offering a more practical path forward for applied
mediation analysis in complex causal systems.

</details>


### [26] [Accurate Bayesian inference for tail risk extrapolation in time series](https://arxiv.org/abs/2510.14637)
*David L. Carl,Simone A. Padoan,Stefano Rizzelli*

Main category: stat.ME

TL;DR: 提出了一个基于广义帕累托分布的贝叶斯框架，用于建模时间序列中的尾部风险，提供渐近诚实的可信区域，在金融和电力需求等实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确量化尾部风险（如金融危机或极端天气等罕见但影响巨大的事件）是风险管理中的核心挑战，特别是在存在序列相关性的数据中。

Method: 开发了基于广义帕累托分布的贝叶斯框架，考虑两种情况：在beta混合依赖下对平稳边际分布的尾部分位数进行外推，以及在异方差回归模型中的动态、过去条件尾部分位数。建立了贝叶斯程序的渐近理论。

Result: 模拟显示，该贝叶斯可信区域在ARMA、GARCH和马尔可夫copula模型等多个标准时间序列模型中优于朴素贝叶斯和基于MLE的置信区域。在美国利率和瑞士电力需求的实际应用中验证了方法的有效性。

Conclusion: 提出的贝叶斯框架为序列相关数据中的尾部风险建模提供了可靠的理论基础和实践工具，能够生成渐近诚实的可信区域，在风险管理中具有重要应用价值。

Abstract: Accurately quantifying tail risks-rare but high-impact events such as
financial crashes or extreme weather-is a central challenge in risk management,
with serially dependent data. We develop a Bayesian framework based on the
Generalized Pareto (GP) distribution for modeling threshold exceedances,
providing posterior distributions for the GP parameters and tail quantiles in
time series. Two cases are considered: extrapolation of tail quantiles for the
stationary marginal distribution under beta-mixing dependence, and dynamic,
past-conditional tail quantiles in heteroscedastic regression models. The
proposal yields asymptotically honest credible regions, whose coverage
probabilities converge to their nominal levels. We establish the asymptotic
theory for the Bayesian procedure, deriving conditions on the prior
distributions under which the posterior satisfies key asymptotic properties. To
achieve this, we first develop a likelihood theory under serial dependence,
providing local and global bounds for the empirical log-likelihood process of
the misspecified GP model and deriving corresponding asymptotic properties of
the Maximum Likelihood Estimator (MLE). Simulations demonstrate that our
Bayesian credible regions outperform naive Bayesian and MLE-based confidence
regions across several standard time series models, including ARMA, GARCH, and
Markovian copula models. Two real-data applications-to U.S. interest rates and
Swiss electricity demand-highlight the relevance of the proposed methodology.

</details>


### [27] [Hierarchical shot-noise Cox process mixtures for clustering across groups](https://arxiv.org/abs/2510.14681)
*Alessandro Carminati,Mario Beraha,Federico Camerlenghi,Alessandra Guglielmi*

Main category: stat.ME

TL;DR: 提出分层散弹噪声Cox过程（HSNCP）混合模型，通过核函数使组特定原子围绕共享中心聚集，克服传统方法在密度估计和聚类之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯非参数方法在跨组聚类时采用精确原子共享机制，当组间存在细微差异时过于刚性，导致密度估计与聚类效果之间的权衡问题，特别是在大样本下会出现跨组聚类碎片化。

Method: 基于散弹噪声Cox过程构建HSNCP模型，组特定原子通过核函数围绕共享中心聚集，保持分析可处理性，推导出先验矩、组间相关性、边际分布和预测分布，并开发条件MCMC算法进行后验推断。

Result: 通过模拟和大规模星系数据集应用验证，HSNCP模型在跨组聚类平衡性和密度估计准确性方面优于分层Dirichlet过程，即使在模型错误设定下也表现稳健。

Conclusion: HSNCP模型通过灵活的跨组信息共享机制，有效解决了传统方法在密度估计和聚类之间的权衡问题，为部分可交换数据的分组聚类提供了更优的解决方案。

Abstract: Clustering observations across partially exchangeable groups of data is a
routine task in Bayesian nonparametrics. Previously proposed models allow for
clustering across groups by sharing atoms in the group-specific mixing
measures. However, exact atom sharing can be overly rigid when groups differ
subtly, introducing a trade-off between clustering and density estimates and
fragmenting across-group clusters, particularly at larger sample sizes. We
introduce the hierarchical shot-noise Cox process (HSNCP) mixture model, where
group-specific atoms concentrate around shared centers through a kernel. This
enables accurate density estimation within groups and flexible borrowing across
groups, overcoming the density-clustering trade-off of previous approaches. Our
construction, built on the shot-noise Cox process, remains analytically
tractable: we derive closed-form prior moments and an inter-group correlation,
obtain the marginal law and predictive distribution for latent parameters, as
well as the posterior of the mixing measures given the latent parameters. We
develop an efficient conditional MCMC algorithm for posterior inference. We
assess the performance of the HSNCP model through simulations and an
application to a large galaxy dataset, demonstrating balanced across-group
clusters and improved density estimates compared with the hierarchical
Dirichlet process, including under model misspecification.

</details>


### [28] [A formative measurement validation methodology for survey questionnaires](https://arxiv.org/abs/2510.14950)
*Mark Dominique Dalipe Muñoz*

Main category: stat.ME

TL;DR: 提出一个专门为形成性构念设计的多步骤验证方法框架，解决现有文献中缺乏实用指导的问题。


<details>
  <summary>Details</summary>
Motivation: 形成性指标的模型误设问题在学术文献中广泛存在，但学者们缺乏明确的实用方法来管理这一差距，迫使研究人员依赖主要用于反映性模型的心理测量框架，可能导致误导性发现。

Method: 基于详尽的文献综述，整合了通过描述性统计和多重共线性检查的基本试点诊断，提出了一个专门为基于调查研究中形成性构念设计的多步骤验证方法框架。

Result: 该框架为研究人员提供了必要的理论和结构清晰度，能够最终证明并坚持适当的验证技术，准确考虑构念的因果性质，同时确保高度的心理测量和统计完整性。

Conclusion: 该研究提供了一个实用的多步骤验证方法框架，专门针对形成性构念，填补了现有文献中的方法空白，为研究人员提供了更准确和可靠的验证工具。

Abstract: Model misspecification of formative indicators remains a widely documented
issue across academic literature, yet scholars lack a clear consensus on
pragmatic, prescriptive approaches to manage this gap. This ambiguity forces
researchers to rely on psychometric frameworks primarily intended for
reflective models, and thus risks misleading findings. This article introduces
a Multi-Step Validation Methodology Framework specifically designed for
formative constructs in survey-based research. The proposed framework is
grounded in an exhaustive literature review and integrates essential pilot
diagnostics through descriptive statistics and multicollinearity checks. The
methodology provides researchers with the necessary theoretical and structural
clarity to finally justify and adhere to appropriate validation techniques that
accurately account for the causal nature of the constructs while ensuring high
psychometric and statistical integrity.

</details>
