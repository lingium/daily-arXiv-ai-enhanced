{"id": "2601.14631", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14631", "abs": "https://arxiv.org/abs/2601.14631", "authors": ["Jinyang Liao", "Ziyang Lyu"], "title": "Semi-Supervised Mixture Models under the Concept of Missing at Radom with Margin Confidence and Aranda Ordaz Function", "comment": "8 pages, 7 figures", "summary": "This paper presents a semi-supervised learning framework for Gaussian mixture modelling under a Missing at Random (MAR) mechanism. The method explicitly parameterizes the missingness mechanism by modelling the probability of missingness as a function of classification uncertainty. To quantify classification uncertainty, we introduce margin confidence and incorporate the Aranda Ordaz (AO) link function to flexibly capture the asymmetric relationships between uncertainty and missing probability. Based on this formulation, we develop an efficient Expectation Conditional Maximization (ECM) algorithm that jointly estimates all parameters appearing in both the Gaussian mixture model (GMM) and the missingness mechanism, and subsequently imputes the missing labels by a Bayesian classifier derived from the fitted mixture model. This method effectively alleviates the bias induced by ignoring the missingness mechanism while enhancing the robustness of semi-supervised learning. The resulting uncertainty-aware framework delivers reliable classification performance in realistic MAR scenarios with substantial proportions of missing labels."}
{"id": "2601.14616", "categories": ["stat.AP", "econ.EM"], "pdf": "https://arxiv.org/pdf/2601.14616", "abs": "https://arxiv.org/abs/2601.14616", "authors": ["Li Tuobang"], "title": "Implementing Substance Over Form: A Novel Metric for Taxing E-commerce to Address Deterritorialization", "comment": null, "summary": "Against the backdrop of e-commerce restructuring consumption patterns, last-mile delivery stations have substantially fulfilled the function of community retail distribution. However, the current tax system only levies a low labor service tax on delivery fees, resulting in a tax contribution from the massive circulating goods value that is significantly lower than that of retail supermarkets of equivalent scale. This disparity not only triggers local tax base erosion but also fosters unfair competition. Based on the \"substance over form\" principle, this paper proposes a tax rate calculation method using \"delivery fee plus insurance premium\" as the base, corrected through \"goods value conversion.\" This method aims to align the substantive tax burden of e-commerce with that of community retail at the terminal stage, effectively internalizing the high negative externalities of delivery stations through fiscal instruments, addressing E-commerce Deterritorialization."}
{"id": "2601.14430", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14430", "abs": "https://arxiv.org/abs/2601.14430", "authors": ["Peter Potaptchik", "Adhi Saravanan", "Abbas Mammadov", "Alvaro Prat", "Michael S. Albergo", "Yee Whye Teh"], "title": "Meta Flow Maps enable scalable reward alignment", "comment": null, "summary": "Controlling generative models is computationally expensive. This is because optimal alignment with a reward function--whether via inference-time steering or fine-tuning--requires estimating the value function. This task demands access to the conditional posterior $p_{1|t}(x_1|x_t)$, the distribution of clean data $x_1$ consistent with an intermediate state $x_t$, a requirement that typically compels methods to resort to costly trajectory simulations. To address this bottleneck, we introduce Meta Flow Maps (MFMs), a framework extending consistency models and flow maps into the stochastic regime. MFMs are trained to perform stochastic one-step posterior sampling, generating arbitrarily many i.i.d. draws of clean data $x_1$ from any intermediate state. Crucially, these samples provide a differentiable reparametrization that unlocks efficient value function estimation. We leverage this capability to solve bottlenecks in both paradigms: enabling inference-time steering without inner rollouts, and facilitating unbiased, off-policy fine-tuning to general rewards. Empirically, our single-particle steered-MFM sampler outperforms a Best-of-1000 baseline on ImageNet across multiple rewards at a fraction of the compute."}
{"id": "2601.14262", "categories": ["stat.ME", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14262", "abs": "https://arxiv.org/abs/2601.14262", "authors": ["Hongxiao Li", "Chenxi Wang", "Fanda Fan", "Zihan Wang", "Wanling Gao", "Lei Wang", "Jianfeng Zhan"], "title": "On Meta-Evaluation", "comment": null, "summary": "Evaluation is the foundation of empirical science, yet the evaluation of evaluation itself -- so-called meta-evaluation -- remains strikingly underdeveloped. While methods such as observational studies, design of experiments (DoE), and randomized controlled trials (RCTs) have shaped modern scientific practice, there has been little systematic inquiry into their comparative validity and utility across domains. Here we introduce a formal framework for meta-evaluation by defining the evaluation space, its structured representation, and a benchmark we call AxiaBench. AxiaBench enables the first large-scale, quantitative comparison of ten widely used evaluation methods across eight representative application domains. Our analysis reveals a fundamental limitation: no existing method simultaneously achieves accuracy and efficiency across diverse scenarios, with DoE and observational designs in particular showing significant deviations from real-world ground truth. We further evaluate a unified method of entire-space stratified sampling from previous evaluatology research, and the results report that it consistently outperforms prior approaches across all tested domains. These results establish meta-evaluation as a scientific object in its own right and provide both a conceptual foundation and a pragmatic tool set for advancing trustworthy evaluation in computational and experimental research."}
{"id": "2601.14937", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.14937", "abs": "https://arxiv.org/abs/2601.14937", "authors": ["Juan J. Segura"], "title": "Geostatistics from Elliptic Boundary-Value Problems: Green Operators, Transmission Conditions, and Schur Complements", "comment": null, "summary": "Classical geostatistics encodes spatial dependence by prescribing variograms or covariance kernels on Euclidean domains, whereas the SPDE--GMRF paradigm specifies Gaussian fields through an elliptic precision operator whose inverse is the corresponding Green operator. We develop an operator-based formulation of Gaussian spatial random fields on bounded domains and manifolds with internal interfaces, treating boundary and transmission conditions as explicit components of the statistical model. Starting from coercive quadratic energy functionals, variational theory yields a precise precision--covariance correspondence and shows that variograms are derived quadratic functionals of the Green operator, hence depend on boundary conditions and domain geometry. Conditioning and kriging follow from standard Gaussian update identities in both covariance and precision form, with hard constraints represented equivalently by exact interpolation constraints or by distributional source terms. Interfaces are modelled via surface penalty terms; taking variations produces flux-jump transmission conditions and induces controlled attenuation of cross-interface covariance. Finally, boundary-driven prediction and domain reduction are formulated through Dirichlet-to-Neumann operators and Schur complements, providing an operator language for upscaling, change of support, and subdomain-to-boundary mappings. Throughout, we use tools standard in spatial statistics and elliptic PDE theory to keep boundary and interface effects explicit in covariance modeling and prediction."}
{"id": "2601.14701", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.14701", "abs": "https://arxiv.org/abs/2601.14701", "authors": ["Yuan Ji", "Ph. D"], "title": "Regulatory Expectations for Bayesian Methods in Drug and Biologic Clinical Trials: A Practical Perspective on FDA's 2026 Draft Guidance", "comment": null, "summary": "The U.S. Food and Drug Administration (FDA) released a landmark draft guidance in January 2026 on the use of Bayesian methodology to support primary inference in clinical trials of drugs and biological products. For sponsors, the central message is not merely that ``Bayes is allowed,'' but that Bayesian designs should be justified through explicit success criteria, thoughtful priors (especially when borrowing external information), prospective operating-characteristic evaluation (often via simulation when simulation is used), and computational transparency suitable for regulatory review. This paper provides a practical, regulatory-oriented synthesis of the draft guidance, highlighting where Bayesian designs can be calibrated to traditional frequentist error-rate targets and where, with sponsor--FDA agreement, alternative Bayesian operating metrics may be appropriate. We illustrate expectations through examples discussed in the guidance (e.g., platform trials, external/nonconcurrent controls, pediatric extrapolation) and conclude with an actionable checklist for planning documents and submission packages."}
{"id": "2601.14515", "categories": ["stat.ML", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.14515", "abs": "https://arxiv.org/abs/2601.14515", "authors": ["Zhengang Zhong", "Yury Korolev", "Matthew Thorpe"], "title": "Large Data Limits of Laplace Learning for Gaussian Measure Data in Infinite Dimensions", "comment": null, "summary": "Laplace learning is a semi-supervised method, a solution for finding missing labels from a partially labeled dataset utilizing the geometry given by the unlabeled data points. The method minimizes a Dirichlet energy defined on a (discrete) graph constructed from the full dataset. In finite dimensions the asymptotics in the large (unlabeled) data limit are well understood with convergence from the graph setting to a continuum Sobolev semi-norm weighted by the Lebesgue density of the data-generating measure. The lack of the Lebesgue measure on infinite-dimensional spaces requires rethinking the analysis if the data aren't finite-dimensional. In this paper we make a first step in this direction by analyzing the setting when the data are generated by a Gaussian measure on a Hilbert space and proving pointwise convergence of the graph Dirichlet energy."}
{"id": "2601.14309", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14309", "abs": "https://arxiv.org/abs/2601.14309", "authors": ["Esteban Fernández-Morales", "Emily M. Ko", "Nandita Mitra", "Youjin Lee", "Arman Oganisian"], "title": "A Bayesian framework for cost-effectiveness analysis with time-varying treatment decisions", "comment": null, "summary": "Cost-effectiveness analyses (CEAs) compare the costs and health outcomes of treatment regimes to inform medical decisions. With observational claims data, CEAs must address nonrandom treatment assignment, administrative censoring, and irregularly spaced medical visits that reflect the continuous timing of care and treatment initiation. In high-risk, early-stage endometrial cancer (HR-EC), adjuvant radiation is initiated at patient-specific times following hysterectomy, causing confounding between treatment and outcomes that can evolve with post-surgical recovery and clinical course. Most existing CEA methods use point-treatment or discrete-time models. However, point-treatment approaches break down with time-varying confounding, while discrete-time models bin continuous time, expand the data into a person-period format, and can induce zero-inflation by creating many intervals with no cost-accruing events. We propose a Bayesian framework for CEAs with sequential decision-making that jointly models costs and event times in continuous time, accounts for administrative censoring, and supports dynamic treatment regimes with minimal parametric assumptions. We use Bayesian g-computation to estimate causally interpretable cost-effectiveness measures, including net monetary benefit, and to compare regimes through posterior contrasts. We evaluate the finite-sample performance of the proposed method in simulations across censoring levels and compare it against discrete-time and fully parametric alternatives. We then use SEER-Medicare data to assess the cost-effectiveness of initiating adjuvant radiation therapy within six months following hysterectomy among HR-EC patients."}
{"id": "2601.15014", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.15014", "abs": "https://arxiv.org/abs/2601.15014", "authors": ["Michelle Ching", "Ioana Popescu", "Nico Smith", "Tianyi Ma", "William G. Underwood", "Richard J. Samworth"], "title": "Efficient and Minimax-optimal In-context Nonparametric Regression with Transformers", "comment": "31 pages, 6 figures", "summary": "We study in-context learning for nonparametric regression with $α$-Hölder smooth regression functions, for some $α>0$. We prove that, with $n$ in-context examples and $d$-dimensional regression covariates, a pretrained transformer with $Θ(\\log n)$ parameters and $Ω\\bigl(n^{2α/(2α+d)}\\log^3 n\\bigr)$ pretraining sequences can achieve the minimax-optimal rate of convergence $O\\bigl(n^{-2α/(2α+d)}\\bigr)$ in mean squared error. Our result requires substantially fewer transformer parameters and pretraining sequences than previous results in the literature. This is achieved by showing that transformers are able to approximate local polynomial estimators efficiently by implementing a kernel-weighted polynomial basis and then running gradient descent."}
{"id": "2601.14796", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.14796", "abs": "https://arxiv.org/abs/2601.14796", "authors": ["Jeffrey Näf"], "title": "A Practical Guide to Modern Imputation", "comment": null, "summary": "This guide based on recent papers should help researchers avoid some of the most common pitfalls of missing value imputation imputation."}
{"id": "2601.14609", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14609", "abs": "https://arxiv.org/abs/2601.14609", "authors": ["Ziwen Wang", "Siqi Li", "Marcus Eng Hock Ong", "Nan Liu"], "title": "Communication-Efficient Federated Risk Difference Estimation for Time-to-Event Clinical Outcomes", "comment": null, "summary": "Privacy-preserving model co-training in medical research is often hindered by server-dependent architectures incompatible with protected hospital data systems and by the predominant focus on relative effect measures (hazard ratios) which lack clinical interpretability for absolute survival risk assessment. We propose FedRD, a communication-efficient framework for federated risk difference estimation in distributed survival data. Unlike typical federated learning frameworks (e.g., FedAvg) that require persistent server connections and extensive iterative communication, FedRD is server-independent with minimal communication: one round of summary statistics exchange for the stratified model and three rounds for the unstratified model. Crucially, FedRD provides valid confidence intervals and hypothesis testing--capabilities absent in FedAvg-based frameworks. We provide theoretical guarantees by establishing the asymptotic properties of FedRD and prove that FedRD (unstratified) is asymptotically equivalent to pooled individual-level analysis. Simulation studies and real-world clinical applications across different countries demonstrate that FedRD outperforms local and federated baselines in both estimation accuracy and prediction performance, providing an architecturally feasible solution for absolute risk assessment in privacy-restricted, multi-site clinical studies."}
{"id": "2601.14431", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14431", "abs": "https://arxiv.org/abs/2601.14431", "authors": ["Xi Fang", "Bingkai Wang", "Guangyu Tong", "Liangyuan Hu", "Shuangge Ma", "Fan Li"], "title": "Doubly robust estimators of the restricted mean time in favor estimands in individual- and cluster-randomized trials", "comment": null, "summary": "Progressive multi-state survival outcomes are common in trials with recurrent or sequential events and require treatment effect estimands that remain interpretable without proportional intensity or Markov assumptions. The restricted mean time in favor of treatment (RMT-IF) extends the restricted mean survival time to ordered multi-state processes and provides such an interpretable estimand. However, existing RMT-IF methods are nonparametric, assume covariate-independent censoring for independent observations, and do not accommodate cluster-randomized trials (CRTs), limiting both efficiency and applicability. We develop a class of doubly robust estimators for RMT-IF under right censoring using an augmented inverse-probability weighting framework that combines stage-specific outcome regression with arm-specific censoring models, yielding consistency when either nuisance model is correctly specified. We further extend the framework to CRTs by formalizing both cluster-level and individual-level average RMT-IF estimands to address informative cluster size and by constructing corresponding doubly robust estimators that account for within-cluster correlation. For inference, we employ model-agnostic jackknife variance estimators in both individually randomized and cluster-randomized settings. Extensive simulation studies demonstrate finite-sample performance, and the methods are illustrated using two randomized trial examples."}
{"id": "2601.15239", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.15239", "abs": "https://arxiv.org/abs/2601.15239", "authors": ["Kexin Wang", "Salil Bhate", "João M. Pereira", "Joe Kileel", "Matylda Figlerowicz", "Anna Seigal"], "title": "Multi-context principal component analysis", "comment": "47 pages, 8 figures. Supplementary tables are provided as downloadable file", "summary": "Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematically recover such factors. We develop multi-context principal component analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors shared across subsets of contexts. Applied to gene expression, MCPCA reveals axes of variation shared across subsets of cancer types and an axis whose variability in tumor cells, but not mean, is associated with lung cancer progression. Applied to contextualized word embeddings from language models, MCPCA maps stages of a debate on human nature, revealing a discussion between science and fiction over decades. These axes are not found by combining data across contexts or by restricting to individual contexts. MCPCA is a principled generalization of PCA to address the challenge of understanding factors underlying data across contexts."}
{"id": "2601.14815", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2601.14815", "abs": "https://arxiv.org/abs/2601.14815", "authors": ["Fabrice Moudjieu", "Jean Peyhardi", "Maxime Réjou-Méchain", "Patrice Soh Takam", "Frédéric Mortier"], "title": "Zero-inflated binary Tree Pólya splitting regression for multivariate count data", "comment": null, "summary": "Species distribution models (SDMs) are widely used to assess the effects of environmental factors on species distributions. However, classical SDMs ignore inter-species dependencies. Multivariate SDMs (MSDMs), especially those based on latent Gaussian fields such as the multivariate Poisson log-normal (MPLN), address this limitation but face challenges related to computation, dimensionality, and interpretability. Pólya-splitting (PS) distributions offer an alternative, combining a model for total abundance with a multivariate allocation structure, and have natural interpretations from ecological process models. Yet, they lack flexibility in modeling correlation structures. Tree Pólya-splitting (TPS) distributions overcome this by introducing hierarchical structure such as a phylogenetic tree. In this paper, we extend TPS to account for zero-inflation, leading to the zero-inflated tree Pólya-splitting (Z-TPS) family. We detail its statistical properties, show how standard software enables efficient inference, and illustrate its ecological relevance using tree abundance data from over 180 genera across the Congo Basin tropical rainforest."}
{"id": "2601.14631", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14631", "abs": "https://arxiv.org/abs/2601.14631", "authors": ["Jinyang Liao", "Ziyang Lyu"], "title": "Semi-Supervised Mixture Models under the Concept of Missing at Radom with Margin Confidence and Aranda Ordaz Function", "comment": "8 pages, 7 figures", "summary": "This paper presents a semi-supervised learning framework for Gaussian mixture modelling under a Missing at Random (MAR) mechanism. The method explicitly parameterizes the missingness mechanism by modelling the probability of missingness as a function of classification uncertainty. To quantify classification uncertainty, we introduce margin confidence and incorporate the Aranda Ordaz (AO) link function to flexibly capture the asymmetric relationships between uncertainty and missing probability. Based on this formulation, we develop an efficient Expectation Conditional Maximization (ECM) algorithm that jointly estimates all parameters appearing in both the Gaussian mixture model (GMM) and the missingness mechanism, and subsequently imputes the missing labels by a Bayesian classifier derived from the fitted mixture model. This method effectively alleviates the bias induced by ignoring the missingness mechanism while enhancing the robustness of semi-supervised learning. The resulting uncertainty-aware framework delivers reliable classification performance in realistic MAR scenarios with substantial proportions of missing labels."}
{"id": "2601.14498", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14498", "abs": "https://arxiv.org/abs/2601.14498", "authors": ["Marlena Bannick", "Yuanyuan Bian", "Gregory Chen", "Liming Li", "Yuhan Qian", "Daniel Sabanés Bové", "Dong Xi", "Ting Ye", "Yanyao Yi"], "title": "The RobinCar Family: R Tools for Robust Covariate Adjustment in Randomized Clinical Trials", "comment": "On behalf of the Software Subteam ASA-BIOP Covariate Adjustment Scientific Working Group. All authors contributed equally to this work", "summary": "Purpose: Covariate adjustment is a powerful statistical technique that can increase efficiency in clinical trials. Recent guidance from the U.S. FDA provided recommendations and best practices for using covariate adjustment. However, there has existed a gap between the extensive statistical literature on covariate adjustment and software that is easy to use and abides by these best practices.\n  Methods: We have developed the RobinCar Family, which is comprised of RobinCar and RobinCar2. These two R packages enable covariate-adjusted analyses for continuous, discrete, and time-to-event outcomes that follow best practices. For continuous and discrete outcomes, the functions in the RobinCar Family facilitate traditional forms of covariate adjustment such as ANCOVA as well as more recent approaches like ANHECOVA, G-computation with generalized linear models and machine learning models, and adjustment for a super-covariate (as in PROCOVA(TM)). Functions for time-to-event outcomes implement the covariate-adjusted log-rank test, the stratified covariate-adjusted log-rank test, and the marginal covariate-adjusted hazard ratio. The RobinCar Family is supported by the ASA Biopharmaceutical Section Covariate Adjustment Scientific Working Group.\n  Results: We provide an accessible overview of the covariate-adjusted statistical methods, and describe how they are implemented in RobinCar and RobinCar2. We highlight important usage notes for clinical trial practitioners.\n  Conclusion: We apply RobinCar and RobinCar2 functions by analyzing data from the AIDS Clinical Trials Group Study 175, demonstrating that they are straightforward and user-friendly."}
{"id": "2601.14937", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.14937", "abs": "https://arxiv.org/abs/2601.14937", "authors": ["Juan J. Segura"], "title": "Geostatistics from Elliptic Boundary-Value Problems: Green Operators, Transmission Conditions, and Schur Complements", "comment": null, "summary": "Classical geostatistics encodes spatial dependence by prescribing variograms or covariance kernels on Euclidean domains, whereas the SPDE--GMRF paradigm specifies Gaussian fields through an elliptic precision operator whose inverse is the corresponding Green operator. We develop an operator-based formulation of Gaussian spatial random fields on bounded domains and manifolds with internal interfaces, treating boundary and transmission conditions as explicit components of the statistical model. Starting from coercive quadratic energy functionals, variational theory yields a precise precision--covariance correspondence and shows that variograms are derived quadratic functionals of the Green operator, hence depend on boundary conditions and domain geometry. Conditioning and kriging follow from standard Gaussian update identities in both covariance and precision form, with hard constraints represented equivalently by exact interpolation constraints or by distributional source terms. Interfaces are modelled via surface penalty terms; taking variations produces flux-jump transmission conditions and induces controlled attenuation of cross-interface covariance. Finally, boundary-driven prediction and domain reduction are formulated through Dirichlet-to-Neumann operators and Schur complements, providing an operator language for upscaling, change of support, and subdomain-to-boundary mappings. Throughout, we use tools standard in spatial statistics and elliptic PDE theory to keep boundary and interface effects explicit in covariance modeling and prediction."}
{"id": "2601.15014", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.15014", "abs": "https://arxiv.org/abs/2601.15014", "authors": ["Michelle Ching", "Ioana Popescu", "Nico Smith", "Tianyi Ma", "William G. Underwood", "Richard J. Samworth"], "title": "Efficient and Minimax-optimal In-context Nonparametric Regression with Transformers", "comment": "31 pages, 6 figures", "summary": "We study in-context learning for nonparametric regression with $α$-Hölder smooth regression functions, for some $α>0$. We prove that, with $n$ in-context examples and $d$-dimensional regression covariates, a pretrained transformer with $Θ(\\log n)$ parameters and $Ω\\bigl(n^{2α/(2α+d)}\\log^3 n\\bigr)$ pretraining sequences can achieve the minimax-optimal rate of convergence $O\\bigl(n^{-2α/(2α+d)}\\bigr)$ in mean squared error. Our result requires substantially fewer transformer parameters and pretraining sequences than previous results in the literature. This is achieved by showing that transformers are able to approximate local polynomial estimators efficiently by implementing a kernel-weighted polynomial basis and then running gradient descent."}
{"id": "2601.14727", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14727", "abs": "https://arxiv.org/abs/2601.14727", "authors": ["Shuxing Fang", "Ruijian Han", "Yuanhang Luo", "Yiming Xu"], "title": "Recent advances in the Bradley--Terry Model: theory, algorithms, and applications", "comment": null, "summary": "This article surveys recent progress in the Bradley-Terry (BT) model and its extensions. We focus on the statistical and computational aspects, with emphasis on the regime in which both the number of objects and the volume of comparisons tend to infinity, a setting relevant to large-scale applications. The main topics include asymptotic theory for statistical estimation and inference, along with the associated algorithms. We also discuss applications of these models, including recent work on preference alignment in machine learning. Finally, we discuss several key challenges and outline directions for future research."}
{"id": "2601.15239", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.15239", "abs": "https://arxiv.org/abs/2601.15239", "authors": ["Kexin Wang", "Salil Bhate", "João M. Pereira", "Joe Kileel", "Matylda Figlerowicz", "Anna Seigal"], "title": "Multi-context principal component analysis", "comment": "47 pages, 8 figures. Supplementary tables are provided as downloadable file", "summary": "Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematically recover such factors. We develop multi-context principal component analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors shared across subsets of contexts. Applied to gene expression, MCPCA reveals axes of variation shared across subsets of cancer types and an axis whose variability in tumor cells, but not mean, is associated with lung cancer progression. Applied to contextualized word embeddings from language models, MCPCA maps stages of a debate on human nature, revealing a discussion between science and fiction over decades. These axes are not found by combining data across contexts or by restricting to individual contexts. MCPCA is a principled generalization of PCA to address the challenge of understanding factors underlying data across contexts."}
{"id": "2601.14752", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14752", "abs": "https://arxiv.org/abs/2601.14752", "authors": ["Shushi Nishina", "Takahiro Onizuka", "Shintaro Hashimoto"], "title": "Global-local shrinkage priors for modeling random effects in multivariate spatial small area estimation", "comment": "39 pages, 10 figures", "summary": "Small area estimation (SAE) plays a central role in survey statistics and epidemiology, providing reliable estimates for domains with limited sample sizes. The multivariate Fay-Herriot model has been extensively used for this purpose, because it enhances estimation accuracy by borrowing strength across multiple correlated variables. In this paper, we develop a Bayesian extension of the multivariate Fay-Herriot model that enables flexible, component-specific shrinkage of the random effects. The proposed approach employs global-local priors formulated through a sandwich mixture representation, allowing adaptive regularization of each element of the random-effect vectors. This construction yields greater robustness and prevents excessive shrinkage in areas exhibiting strong underlying signals. In addition, we incorporate spatial dependence into the model to account for geographical correlation across small areas. The resulting spatial multivariate framework simultaneously exploits cross-variable relationships and spatial structure, yielding improved estimation efficiency. The utility of the proposed method is demonstrated through simulation studies and an empirical application to real survey data."}
{"id": "2601.15254", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15254", "abs": "https://arxiv.org/abs/2601.15254", "authors": ["Felix Schur", "Niklas Pfister", "Peng Ding", "Sach Mukherjee", "Jonas Peters"], "title": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?", "comment": null, "summary": "We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\\ell_1$-regularized estimation and post-selection refitting."}
{"id": "2601.14849", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14849", "abs": "https://arxiv.org/abs/2601.14849", "authors": ["Laura Ferrini", "Federico Castelletti"], "title": "Graphical model-based clustering of categorical data", "comment": null, "summary": "Clustering multivariate data is a pervasive task in many applied problems, particularly in social studies and life science. Model-based approaches to clustering rely on mixture models, where each mixture component corresponds to the kernel of a distribution characterizing a latent sub-group. Current methods developed within this framework employ multivariate distributions built under the assumption of independence among variables given the cluster allocation. Accordingly, possible dependence structures characterizing differences across groups are not directly accounted for during the clustering process. In this paper we consider multivariate categorical data, and introduce a model-based clustering method which employs graphical models as a tool to encode dependencies between variables. Specifically, we consider a Dirichlet Process mixture of categorical graphical models, which clusters individuals into groups that are homogeneous in terms of dependence (graphical) structure and allied parameters. We provide full Bayesian inference for the model and develop a Markov chain Monte Carlo scheme for posterior analysis. Our method is evaluated through simulations and applied to real case studies, including the analysis of genomic data and voting records. Results reveal the merits of a graphical model-based clustering, in comparison with approaches that do not explicitly account for dependencies in the multivariate distribution of variables."}
{"id": "2601.14991", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.14991", "abs": "https://arxiv.org/abs/2601.14991", "authors": ["Martin Bladt", "Rasmus Frigaard Lemvig"], "title": "Consistency of Honest Decision Trees and Random Forests", "comment": null, "summary": "We study various types of consistency of honest decision trees and random forests in the regression setting. In contrast to related literature, our proofs are elementary and follow the classical arguments used for smoothing methods. Under mild regularity conditions on the regression function and data distribution, we establish weak and almost sure convergence of honest trees and honest forest averages to the true regression function, and moreover we obtain uniform convergence over compact covariate domains. The framework naturally accommodates ensemble variants based on subsampling and also a two-stage bootstrap sampling scheme. Our treatment synthesizes and simplifies existing analyses, in particular recovering several results as special cases. The elementary nature of the arguments clarifies the close relationship between data-adaptive partitioning and kernel-type methods, providing an accessible approach to understanding the asymptotic behavior of tree-based methods."}
{"id": "2601.14937", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.14937", "abs": "https://arxiv.org/abs/2601.14937", "authors": ["Juan J. Segura"], "title": "Geostatistics from Elliptic Boundary-Value Problems: Green Operators, Transmission Conditions, and Schur Complements", "comment": null, "summary": "Classical geostatistics encodes spatial dependence by prescribing variograms or covariance kernels on Euclidean domains, whereas the SPDE--GMRF paradigm specifies Gaussian fields through an elliptic precision operator whose inverse is the corresponding Green operator. We develop an operator-based formulation of Gaussian spatial random fields on bounded domains and manifolds with internal interfaces, treating boundary and transmission conditions as explicit components of the statistical model. Starting from coercive quadratic energy functionals, variational theory yields a precise precision--covariance correspondence and shows that variograms are derived quadratic functionals of the Green operator, hence depend on boundary conditions and domain geometry. Conditioning and kriging follow from standard Gaussian update identities in both covariance and precision form, with hard constraints represented equivalently by exact interpolation constraints or by distributional source terms. Interfaces are modelled via surface penalty terms; taking variations produces flux-jump transmission conditions and induces controlled attenuation of cross-interface covariance. Finally, boundary-driven prediction and domain reduction are formulated through Dirichlet-to-Neumann operators and Schur complements, providing an operator language for upscaling, change of support, and subdomain-to-boundary mappings. Throughout, we use tools standard in spatial statistics and elliptic PDE theory to keep boundary and interface effects explicit in covariance modeling and prediction."}
{"id": "2601.14991", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.14991", "abs": "https://arxiv.org/abs/2601.14991", "authors": ["Martin Bladt", "Rasmus Frigaard Lemvig"], "title": "Consistency of Honest Decision Trees and Random Forests", "comment": null, "summary": "We study various types of consistency of honest decision trees and random forests in the regression setting. In contrast to related literature, our proofs are elementary and follow the classical arguments used for smoothing methods. Under mild regularity conditions on the regression function and data distribution, we establish weak and almost sure convergence of honest trees and honest forest averages to the true regression function, and moreover we obtain uniform convergence over compact covariate domains. The framework naturally accommodates ensemble variants based on subsampling and also a two-stage bootstrap sampling scheme. Our treatment synthesizes and simplifies existing analyses, in particular recovering several results as special cases. The elementary nature of the arguments clarifies the close relationship between data-adaptive partitioning and kernel-type methods, providing an accessible approach to understanding the asymptotic behavior of tree-based methods."}
{"id": "2601.15132", "categories": ["stat.ME", "astro-ph.CO", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2601.15132", "abs": "https://arxiv.org/abs/2601.15132", "authors": ["Zixiao Hu", "Jason D. McEwen"], "title": "Efficient prior sensitivity analysis for Bayesian model comparison", "comment": "11 pages, 4 figures; submitted conference proceedings for MaxEnt 2025", "summary": "Bayesian model comparison implements Occam's razor through its sensitivity to the prior. However, prior-dependence makes it important to assess the influence of plausible alternative priors. Such prior sensitivity analyses for the Bayesian evidence are expensive, either requiring repeated, costly model re-fits or specialised sampling schemes. By exploiting the learned harmonic mean estimator (LHME) for evidence calculation we decouple sampling and evidence calculation, allowing resampled posterior draws to be used directly to calculate the evidence without further likelihood evaluations. This provides an alternative approach to prior sensitivity analysis for Bayesian model comparison that dramatically alleviates the computational cost and is agnostic to the method used to generate posterior samples. We validate our method on toy problems and a cosmological case study, reproducing estimates obtained by full Markov chain Monte Carlo (MCMC) sampling and nested sampling re-fits. For the cosmological example considered our approach achieves up to $6000\\times$ lower computational cost."}
{"id": "2601.14631", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.14631", "abs": "https://arxiv.org/abs/2601.14631", "authors": ["Jinyang Liao", "Ziyang Lyu"], "title": "Semi-Supervised Mixture Models under the Concept of Missing at Radom with Margin Confidence and Aranda Ordaz Function", "comment": "8 pages, 7 figures", "summary": "This paper presents a semi-supervised learning framework for Gaussian mixture modelling under a Missing at Random (MAR) mechanism. The method explicitly parameterizes the missingness mechanism by modelling the probability of missingness as a function of classification uncertainty. To quantify classification uncertainty, we introduce margin confidence and incorporate the Aranda Ordaz (AO) link function to flexibly capture the asymmetric relationships between uncertainty and missing probability. Based on this formulation, we develop an efficient Expectation Conditional Maximization (ECM) algorithm that jointly estimates all parameters appearing in both the Gaussian mixture model (GMM) and the missingness mechanism, and subsequently imputes the missing labels by a Bayesian classifier derived from the fitted mixture model. This method effectively alleviates the bias induced by ignoring the missingness mechanism while enhancing the robustness of semi-supervised learning. The resulting uncertainty-aware framework delivers reliable classification performance in realistic MAR scenarios with substantial proportions of missing labels."}
