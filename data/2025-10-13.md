<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 10]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Gradient-Guided Furthest Point Sampling for Robust Training Set Selection](https://arxiv.org/abs/2510.08906)
*Morris Trestman,Stefan Gugler,Felix A. Faber,O. A. von Lilienfeld*

Main category: stat.ML

TL;DR: GGFPS是一种基于梯度引导的最远点采样方法，通过利用分子力范数来改进分子构型空间的采样效率，相比传统FPS和均匀采样具有更好的数据效率和预测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统FPS方法在分子构型采样中系统性地欠采样平衡几何结构的问题，导致对松弛结构的预测误差较大，需要开发更有效的训练集选择方法。

Method: 提出了梯度引导最远点采样(GGFPS)，在传统FPS基础上利用分子力范数来指导分子构型空间的高效采样。

Result: 在Styblinski-Tang函数和MD17数据集上的实验表明，GGFPS相比FPS和均匀采样：(i)在2维系统中可减少50%训练成本而不牺牲精度；(ii)对平衡和应变结构都降低预测误差；(iii)在所有MD17构型空间中系统性地减小预测误差方差。

Conclusion: 梯度感知采样方法作为有效的训练集选择工具具有很大潜力，而朴素使用FPS可能导致不平衡训练和不一致的预测结果。

Abstract: Smart training set selections procedures enable the reduction of data needs
and improves predictive robustness in machine learning problems relevant to
chemistry. We introduce Gradient Guided Furthest Point Sampling (GGFPS), a
simple extension of Furthest Point Sampling (FPS) that leverages molecular
force norms to guide efficient sampling of configurational spaces of molecules.
Numerical evidence is presented for a toy-system (Styblinski-Tang function) as
well as for molecular dynamics trajectories from the MD17 dataset. Compared to
FPS and uniform sampling, our numerical results indicate superior data
efficiency and robustness when using GGFPS. Distribution analysis of the MD17
data suggests that FPS systematically under-samples equilibrium geometries,
resulting in large test errors for relaxed structures. GGFPS cures this
artifact and (i) enables up to two fold reductions in training cost without
sacrificing predictive accuracy compared to FPS in the 2-dimensional
Styblinksi-Tang system, (ii) systematically lowers prediction errors for
equilibrium as well as strained structures in MD17, and (iii) systematically
decreases prediction error variances across all of the MD17 configuration
spaces. These results suggest that gradient-aware sampling methods hold great
promise as effective training set selection tools, and that naive use of FPS
may result in imbalanced training and inconsistent prediction outcomes.

</details>


### [2] [A Representer Theorem for Hawkes Processes via Penalized Least Squares Minimization](https://arxiv.org/abs/2510.08916)
*Hideaki Kim,Tomoharu Iwata*

Main category: stat.ML

TL;DR: 本文提出了一种用于线性多元霍克斯过程的新型表示定理，通过变换核函数和固定对偶系数为1，实现了高效的非参数触发核估计。


<details>
  <summary>Details</summary>
Motivation: 传统表示定理虽然将无限维优化问题转化为有限维问题，但仍需优化对偶系数。本文旨在开发更高效的核方法，避免昂贵的对偶系数优化过程。

Method: 在再生核希尔伯特空间框架下，通过惩罚最小二乘最小化，定义变换核函数系统，并将最优触发核估计表示为这些变换核的线性组合，所有对偶系数固定为1。

Result: 经验评估显示，该方法在合成数据集上达到竞争性预测精度，同时显著提高了计算效率，优于现有最先进的基于核方法的估计器。

Conclusion: 提出的新型表示定理通过固定对偶系数为1，实现了高效的大规模数据处理能力，为霍克斯过程的非参数估计提供了更实用的解决方案。

Abstract: The representer theorem is a cornerstone of kernel methods, which aim to
estimate latent functions in reproducing kernel Hilbert spaces (RKHSs) in a
nonparametric manner. Its significance lies in converting inherently
infinite-dimensional optimization problems into finite-dimensional ones over
dual coefficients, thereby enabling practical and computationally tractable
algorithms. In this paper, we address the problem of estimating the latent
triggering kernels--functions that encode the interaction structure between
events--for linear multivariate Hawkes processes based on observed event
sequences within an RKHS framework. We show that, under the principle of
penalized least squares minimization, a novel form of representer theorem
emerges: a family of transformed kernels can be defined via a system of
simultaneous integral equations, and the optimal estimator of each triggering
kernel is expressed as a linear combination of these transformed kernels
evaluated at the data points. Remarkably, the dual coefficients are all
analytically fixed to unity, obviating the need to solve a costly optimization
problem to obtain the dual coefficients. This leads to a highly efficient
estimator capable of handling large-scale data more effectively than
conventional nonparametric approaches. Empirical evaluations on synthetic
datasets reveal that the proposed method attains competitive predictive
accuracy while substantially improving computational efficiency over existing
state-of-the-art kernel method-based estimators.

</details>


### [3] [Mirror Flow Matching with Heavy-Tailed Priors for Generative Modeling on Convex Domains](https://arxiv.org/abs/2510.08929)
*Yunrui Guan,Krishnakumar Balasubramanian,Shiqian Ma*

Main category: stat.ML

TL;DR: 提出了基于正则化镜像映射和Student-t先验的镜像流匹配方法，解决了凸域生成建模中的两个关键挑战：标准对数障碍镜像映射导致的厚尾分布问题，以及高斯先验与厚尾目标不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 在凸域上进行生成建模时，标准对数障碍镜像映射会导致对偶分布具有厚尾特性，从而使动力学变得不适定；同时，高斯先验与厚尾目标的耦合效果不佳。

Method: 使用正则化镜像映射来控制对偶分布的尾部行为并保证有限矩，同时采用Student-t先验与厚尾目标对齐以稳定训练。

Result: 理论上保证了速度场的空间Lipschitz性和时间正则性，提供了Student-t先验流匹配的Wasserstein收敛速率，以及在ε精度学习速度场下的约束生成原始空间保证。实证结果显示在合成凸域模拟中优于基线方法，在现实世界约束生成任务中获得了有竞争力的样本质量。

Conclusion: 所提出的镜像流匹配方法有效解决了凸域生成建模中的基本挑战，通过正则化镜像映射和Student-t先验实现了稳定训练和高质量生成。

Abstract: We study generative modeling on convex domains using flow matching and mirror
maps, and identify two fundamental challenges. First, standard log-barrier
mirror maps induce heavy-tailed dual distributions, leading to ill-posed
dynamics. Second, coupling with Gaussian priors performs poorly when matching
heavy-tailed targets. To address these issues, we propose Mirror Flow Matching
based on a \emph{regularized mirror map} that controls dual tail behavior and
guarantees finite moments, together with coupling to a Student-$t$ prior that
aligns with heavy-tailed targets and stabilizes training. We provide
theoretical guarantees, including spatial Lipschitzness and temporal regularity
of the velocity field, Wasserstein convergence rates for flow matching with
Student-$t$ priors and primal-space guarantees for constrained generation,
under $\varepsilon$-accurate learned velocity fields. Empirically, our method
outperforms baselines in synthetic convex-domain simulations and achieves
competitive sample quality on real-world constrained generative tasks.

</details>


### [4] [Distributionally robust approximation property of neural networks](https://arxiv.org/abs/2510.09177)
*Mihriban Ceylan,David J. Prömel*

Main category: stat.ML

TL;DR: 该论文证明了多种神经网络在弱紧测度族下具有统一通用逼近性质，扩展了传统L^p空间中的经典逼近定理到Orlicz空间。


<details>
  <summary>Details</summary>
Motivation: 扩展神经网络通用逼近定理的应用范围，超越传统的L^p空间框架，建立对弱紧测度族的一致逼近能力。

Method: 通过证明神经网络在Orlicz空间中的稠密性，建立统一通用逼近性质，涵盖前馈网络、深度窄网络和函数输入网络等架构。

Result: 成功证明了包括非多项式激活函数的前馈网络、ReLU激活的深度窄网络和函数输入神经网络在内的多种网络架构都具有所需逼近性质。

Conclusion: 该工作显著扩展了神经网络通用逼近定理的范围，为在更广泛函数空间中的应用提供了理论基础。

Abstract: The universal approximation property uniformly with respect to weakly compact
families of measures is established for several classes of neural networks. To
that end, we prove that these neural networks are dense in Orlicz spaces,
thereby extending classical universal approximation theorems even beyond the
traditional $L^p$-setting. The covered classes of neural networks include
widely used architectures like feedforward neural networks with non-polynomial
activation functions, deep narrow networks with ReLU activation functions and
functional input neural networks.

</details>


### [5] [A unified Bayesian framework for adversarial robustness](https://arxiv.org/abs/2510.09288)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Main category: stat.ML

TL;DR: 提出了一种基于贝叶斯框架的形式化方法，通过随机信道建模对抗性不确定性，提供了主动和被动两种防御策略，并实证验证了显式建模对抗性不确定性的优势。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性防御方法（如对抗训练）通常采用确定性方法最小化最坏情况损失，但未考虑攻击者的不确定性。现有随机防御方法缺乏统计严谨性且未明确其基本假设。

Method: 引入形式化贝叶斯框架，通过随机信道建模对抗性不确定性，明确所有概率假设。提供两种鲁棒化策略：训练期间的主动防御（与对抗训练对齐）和操作期间的被动防御（与对抗净化对齐）。

Result: 多个先前防御方法可作为本模型的极限情况恢复。实证验证了该方法的有效性，展示了显式建模对抗性不确定性的益处。

Conclusion: 提出的贝叶斯框架为对抗性防御提供了统计严谨的方法，通过显式建模不确定性改进了传统防御策略，并统一了多种现有方法。

Abstract: The vulnerability of machine learning models to adversarial attacks remains a
critical security challenge. Traditional defenses, such as adversarial
training, typically robustify models by minimizing a worst-case loss. However,
these deterministic approaches do not account for uncertainty in the
adversary's attack. While stochastic defenses placing a probability
distribution on the adversary exist, they often lack statistical rigor and fail
to make explicit their underlying assumptions. To resolve these issues, we
introduce a formal Bayesian framework that models adversarial uncertainty
through a stochastic channel, articulating all probabilistic assumptions. This
yields two robustification strategies: a proactive defense enacted during
training, aligned with adversarial training, and a reactive defense enacted
during operations, aligned with adversarial purification. Several previous
defenses can be recovered as limiting cases of our model. We empirically
validate our methodology, showcasing the benefits of explicitly modeling
adversarial uncertainty.

</details>


### [6] [Efficient Autoregressive Inference for Transformer Probabilistic Models](https://arxiv.org/abs/2510.09477)
*Conor Hassan,Nasrulloh Loka,Cen-You Li,Daolang Huang,Paul E. Chang,Yang Yang,Francesco Silvestrin,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: 提出了一种因果自回归缓冲区方法，将上下文编码与条件集更新解耦，使基于transformer的概率模型能够高效生成联合分布，同时保持集合条件化的优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的概率推理模型在单次边际预测方面表现优异，但许多实际应用需要捕获预测间依赖关系的联合分布。纯自回归架构能生成联合分布但牺牲了集合条件化的灵活性，而标准方法需要昂贵的重复编码。

Method: 引入因果自回归缓冲区，模型先处理并缓存上下文，动态缓冲区捕获目标依赖关系：当目标被纳入时，它们进入缓冲区并关注缓存的上下文和先前缓冲的目标。采用统一训练策略，以最小额外成本无缝集成集合基和自回归模式。

Result: 在合成函数、EEG信号、认知模型和表格数据上的实验表明，该方法在保持预测准确性的同时，联合采样速度比基线快达20倍。

Conclusion: 该方法将自回归生成模型的效率与集合基条件化的表示能力相结合，使基于transformer的概率模型的联合预测变得实用。

Abstract: Transformer-based models for amortized probabilistic inference, such as
neural processes, prior-fitted networks, and tabular foundation models, excel
at single-pass marginal prediction. However, many real-world applications, from
signal interpolation to multi-column tabular predictions, require coherent
joint distributions that capture dependencies between predictions. While purely
autoregressive architectures efficiently generate such distributions, they
sacrifice the flexible set-conditioning that makes these models powerful for
meta-learning. Conversely, the standard approach to obtain joint distributions
from set-based models requires expensive re-encoding of the entire augmented
conditioning set at each autoregressive step. We introduce a causal
autoregressive buffer that preserves the advantages of both paradigms. Our
approach decouples context encoding from updating the conditioning set. The
model processes the context once and caches it. A dynamic buffer then captures
target dependencies: as targets are incorporated, they enter the buffer and
attend to both the cached context and previously buffered targets. This enables
efficient batched autoregressive generation and one-pass joint log-likelihood
evaluation. A unified training strategy allows seamless integration of
set-based and autoregressive modes at minimal additional cost. Across synthetic
functions, EEG signals, cognitive models, and tabular data, our method matches
predictive accuracy of strong baselines while delivering up to 20 times faster
joint sampling. Our approach combines the efficiency of autoregressive
generative models with the representational power of set-based conditioning,
making joint prediction practical for transformer-based probabilistic models.

</details>


### [7] [Interpretable Generative and Discriminative Learning for Multimodal and Incomplete Clinical Data](https://arxiv.org/abs/2510.09513)
*Albert Belenguer-Llorens,Carlos Sevilla-Salcedo,Janaina Mourao-Miranda,Vanessa Gómez-Verdejo*

Main category: stat.ML

TL;DR: 提出了一种贝叶斯方法处理多模态临床数据，结合生成式和判别式策略，自动补全缺失视图并实现稳健推理


<details>
  <summary>Details</summary>
Motivation: 现实临床问题常面临多模态数据、不完整视图和有限样本量的挑战，传统机器学习算法存在显著局限性

Method: 采用贝叶斯方法，集成生成式建模捕捉跨视图关系（半监督策略）和判别式任务导向建模识别下游任务相关信息

Result: 能够捕捉和解耦生物、心理和社会人口统计学模态之间的复杂交互作用

Conclusion: 这种双重生成-判别式框架既提供通用理解又提供任务特定洞察，在临床多模态数据分析中展现出潜力

Abstract: Real-world clinical problems are often characterized by multimodal data,
usually associated with incomplete views and limited sample sizes in their
cohorts, posing significant limitations for machine learning algorithms. In
this work, we propose a Bayesian approach designed to efficiently handle these
challenges while providing interpretable solutions. Our approach integrates (1)
a generative formulation to capture cross-view relationships with a
semi-supervised strategy, and (2) a discriminative task-oriented formulation to
identify relevant information for specific downstream objectives. This dual
generative-discriminative formulation offers both general understanding and
task-specific insights; thus, it provides an automatic imputation of the
missing views while enabling robust inference across different data sources.
The potential of this approach becomes evident when applied to the multimodal
clinical data, where our algorithm is able to capture and disentangle the
complex interactions among biological, psychological, and sociodemographic
modalities.

</details>


### [8] [Conditional Flow Matching for Bayesian Posterior Inference](https://arxiv.org/abs/2510.09534)
*So Won Jeong,Percy S. Zhai,Veronika Ročová*

Main category: stat.ML

TL;DR: 提出了一种基于流匹配的生成式多元后验采样器，无需似然评估，通过动态块三角速度场实现从源分布到后验的确定性传输，能够快速生成贝叶斯可信集。


<details>
  <summary>Details</summary>
Motivation: 传统后验采样方法通常需要似然评估，计算复杂。本文旨在开发一种更轻量、无需似然评估的后验采样方法，能够捕捉复杂后验结构并生成可信集。

Method: 使用流匹配方法，学习数据和参数联合空间中的动态块三角速度场，通过可逆积分获得向量秩逆映射，利用动态设计实现单调映射和条件Brenier映射。

Result: 该方法比GAN和扩散模型更轻量，能够捕捉复杂后验结构，提供了后验分布恢复和贝叶斯可信集一致性的频率理论保证。

Conclusion: 提出的流匹配后验采样器提供了一种简单有效的替代方案，无需似然评估，计算效率高，并具有理论保证，适用于复杂贝叶斯推断问题。

Abstract: We propose a generative multivariate posterior sampler via flow matching. It
offers a simple training objective, and does not require access to likelihood
evaluation. The method learns a dynamic, block-triangular velocity field in the
joint space of data and parameters, which results in a deterministic transport
map from a source distribution to the desired posterior. The inverse map, named
vector rank, is accessible by reversibly integrating the velocity over time. It
is advantageous to leverage the dynamic design: proper constraints on the
velocity yield a monotone map, which leads to a conditional Brenier map,
enabling a fast and simultaneous generation of Bayesian credible sets whose
contours correspond to level sets of Monge-Kantorovich data depth. Our approach
is computationally lighter compared to GAN-based and diffusion-based
counterparts, and is capable of capturing complex posterior structures.
Finally, frequentist theoretical guarantee on the consistency of the recovered
posterior distribution, and of the corresponding Bayesian credible sets, is
provided.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [9] [Bayesian Model Inference using Bayesian Quadrature: the Art of Acquisition Functions and Beyond](https://arxiv.org/abs/2510.08974)
*Jingwen Song,Pengfei Wei*

Main category: stat.CO

TL;DR: 本文提出了四种新的贝叶斯求积采集函数，用于高效估计复杂后验分布和模型证据，并扩展到过渡贝叶斯求积方案以处理多模态、非线性依赖和高锐度等挑战。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯模型推理中的后验分布和模型证据估计面临多模态、非线性依赖和高锐度等复杂特征挑战，贝叶斯求积框架需要在计算成本和精度之间取得平衡。

Method: 从推理角度重新审视现有采集函数，重新制定预测的求积规则，开发了四种基于不同期望奖励直觉的采集函数，并扩展到过渡贝叶斯求积方案。

Result: 通过广泛的基准研究和工程应用示例验证了所提出方法的有效性，新采集函数提供了高度灵活和有效的积分点设计。

Conclusion: 提出的四种采集函数分别测量后验预测不确定性、证据对预测不确定性的贡献，以及后验和证据预测不确定性的期望减少，为高效积分点设计提供了灵活性。

Abstract: Estimating posteriors and the associated model evidences is a core issue of
Bayesian model inference, and can be of great challenge given complex features
of the posteriors such as multi-modalities of unequal importance, nonlinear
dependencies and high sharpness. Bayesian Quadrature (BQ) has emerged as a
competitive framework for tackling this challenge, as it provides flexible
balance between computational cost and accuracy. The performance of a BQ scheme
is fundamentally dictated by the acquisition function as it exclusively governs
the generation of integration points. After reexamining one of the most
advanced acquisition function from a prospective inference perspective and
reformulating the quadrature rules for prediction, four new acquisition
functions, inspired by distinct intuitions on expected rewards, are primarily
developed, all of which are accompanied by elegant interpretations and highly
efficient numerical estimators. Mathematically, these four acquisition
functions measure, respectively, the prediction uncertainty of posterior, the
contribution to prediction uncertainty of evidence, as well as the expected
reduction of prediction uncertainties concerning posterior and evidence, and
thus provide flexibility for highly effective design of integration points.
These acquisition functions are further extended to the transitional BQ scheme,
along with several specific refinements, to tackle the above-mentioned
challenges with high efficiency and robustness. Effectiveness of the
developments is ultimately demonstrated with extensive benchmark studies and
application to an engineering example.

</details>


### [10] [Solving Fokker-Planck-Kolmogorov Equation by Distribution Self-adaptation Normalized Physics-informed Neural Networks](https://arxiv.org/abs/2510.09422)
*Yi Zhang,Yiting Duan,Xiangjun Wang,Zhikun Zhang*

Main category: stat.CO

TL;DR: 提出DSN-PINNs方法，通过软归一化约束和自适应重采样策略求解时间依赖的Fokker-Planck-Kolmogorov方程。


<details>
  <summary>Details</summary>
Motivation: 随机动力系统建模复杂现象需要稳健的数值方法求解FPK方程，以理解和预测随机行为。

Method: 使用归一化增强的PINN模型进行预训练建立解的全局结构和尺度，生成可靠先验分布；然后通过加权核密度估计动态重分配训练点，将计算资源集中在概率分布最具代表性的区域。

Result: 通过综合数值实验和与现有方法的比较分析验证了框架的有效性，包括在真实经济数据集上的验证。

Conclusion: 该方法能够利用随机动力系统的内在结构特性，同时保持计算精度和实现简单性。

Abstract: Stochastic dynamical systems provide essential mathematical frameworks for
modeling complex real-world phenomena. The Fokker-Planck-Kolmogorov (FPK)
equation governs the evolution of probability density functions associated with
stochastic system trajectories. Developing robust numerical methods for solving
the FPK equation is critical for understanding and predicting stochastic
behavior. Here, we introduce the distribution self-adaptive normalized
physics-informed neural network (DSN-PINNs) for solving time-dependent FPK
equations through the integration of soft normalization constraints with
adaptive resampling strategies. Specifically, we employ a
normalization-enhanced PINN model in a pretraining phase to establish the
solution's global structure and scale, generating a reliable prior
distribution. Subsequently, guided by this prior, we dynamically reallocate
training points via weighted kernel density estimation, concentrating
computational resources on regions most representative of the underlying
probability distribution throughout the learning process. The key innovation
lies in our method's ability to exploit the intrinsic structural properties of
stochastic dynamics while maintaining computational accuracy and implementation
simplicity. We demonstrate the framework's effectiveness through comprehensive
numerical experiments and comparative analyses with existing methods, including
validation on real-world economic datasets.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [11] [Quantifying Very Extreme Precipitation and Temperature Using Huge Ensembles Generated by Machine Learning-based Climate Model Emulators](https://arxiv.org/abs/2510.08893)
*Christopher J. Paciorek,Daniel Cooley*

Main category: stat.AP

TL;DR: 该论文评估了使用大型气候模型集合来估计极端降水概率的方法，通过ACE2模拟器在10560年数据上验证了极端值统计技术的可行性。


<details>
  <summary>Details</summary>
Motivation: 气候变化下极端天气事件概率和强度可能变化，但极低概率事件由于观测记录短而难以统计表征。需要改进概率最大降水(PMP)估计方法用于关键基础设施设计。

Method: 使用ACE2模拟器基于ERA5再分析数据生成10560年的大型集合，应用统计极端值技术（特别是阈值超越方法）来估计极端降水和温度分位数。

Result: 研究表明可以实际估计非常极端的降水和温度分位数，阈值超越方法在足够高阈值下可靠，结果对季节和风暴类型变化具有鲁棒性，统计不确定性得到良好约束。

Conclusion: 大型集合可用于估计极端统计量，模拟器能产生超出训练数据范围的极端值，这种方法对未来改进的模拟器具有参考价值。

Abstract: Weather extremes produce major impacts on society and ecosystems and are
likely to change in likelihood and magnitude with climate change. However, very
low probability events are hard to characterize statistically using
observations or climate model output because of short records/runs. For
precipitation, consideration of such events arises in quantifying Probable
Maximum Precipitation (PMP), namely estimating extreme precipitation magnitudes
for designing and assessing critical infrastructure. A recent National
Academies report on modernizing PMP estimation proposed using huge climate
model-based ensembles to estimate extreme quantiles, possibly through machine
learning-based ensemble boosting. Here we assess such an approach for the
contiguous United States using a huge ensemble (10560 years) from a
state-of-the-art emulator (ACE2) trained on ERA5 reanalysis. The results
indicate that one can practically estimate very extreme precipitation and
temperature quantiles using appropriate statistical extreme value techniques.
More specifically, the results provide evidence for (1) the use of
threshold-exceedance methods with a sufficiently high threshold for reliable
estimation (necessary for precipitation), (2) the robustness of results to
variations in extremes by season and storm type, and (3) well-constrained
statistical uncertainty. Our results also show that the emulator produces
extremes outside the range of the ERA5 training data. While this suggests that
such emulators have potential for quantifying the climatology of extremes, we
do not extensively investigate if this particular emulator is fit for purpose.
Our focus is on how to use huge ensembles to estimate very extreme statistics,
and we expect the results to be relevant for future improved emulators.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [12] [A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?](https://arxiv.org/abs/2510.08758)
*Graham Tierney,Srikar Katta,Christopher Bail,Sunshine Hillygus,Alexander Volfovsky*

Main category: stat.ME

TL;DR: 本文提出了一种新的实验设计方法，用于解决文本分析中的潜在混淆变量问题，避免了传统LLM方法可能导致的覆盖偏差，并应用于评估政治沟通中谦逊表达对说服力的因果效应。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究需要分析文本特征如何因果影响受众态度和行为，但由于文本属性相互关联（如愤怒评论常包含粗俗语言），必须控制潜在混淆变量来隔离因果效应。

Method: 引入新的实验设计方法处理潜在混淆，避免覆盖问题，无偏估计处理效应。在评估政治沟通中谦逊表达说服力的实验中应用该方法。

Result: 方法学上证明，基于LLM的方法在真实文本和实验结果中表现甚至不如简单的词袋模型。实质内容上，成功分离了谦逊表达对政治声明说服力的因果效应。

Conclusion: 该方法为社交媒体平台、政策制定者和社会科学家提供了新的沟通效应见解，同时揭示了LLM方法在因果推断中的局限性。

Abstract: Many social science questions ask how linguistic properties causally affect
an audience's attitudes and behaviors. Because text properties are often
interlinked (e.g., angry reviews use profane language), we must control for
possible latent confounding to isolate causal effects. Recent literature
proposes adapting large language models (LLMs) to learn latent representations
of text that successfully predict both treatment and the outcome. However,
because the treatment is a component of the text, these deep learning methods
risk learning representations that actually encode the treatment itself,
inducing overlap bias. Rather than depending on post-hoc adjustments, we
introduce a new experimental design that handles latent confounding, avoids the
overlap issue, and unbiasedly estimates treatment effects. We apply this design
in an experiment evaluating the persuasiveness of expressing humility in
political communication. Methodologically, we demonstrate that LLM-based
methods perform worse than even simple bag-of-words models using our real text
and outcomes from our experiment. Substantively, we isolate the causal effect
of expressing humility on the perceived persuasiveness of political statements,
offering new insights on communication effects for social media platforms,
policy makers, and social scientists.

</details>


### [13] [Repulsive Mixture Model with Projection Determinantal Point Process](https://arxiv.org/abs/2510.08838)
*Ziyi Song,Federico Camerlenghi,Weining Shen,Michele Guindani,Mario Beraha*

Main category: stat.ME

TL;DR: 提出了一种基于投影行列式点过程（DPP）先验的贝叶斯排斥混合模型，解决了传统混合模型中组件重叠和冗余的问题，实现了可解释的后验聚类。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯混合模型由于组件位置先验独立指定，常产生重叠或冗余组件，影响可解释性。现有排斥先验方法存在计算和理论挑战。

Method: 为组件位置分配投影DPP先验，利用其强排斥性和精确采样特性。推导了闭式后验和预测分布，开发了条件Gibbs采样器和首个完全可实现的边际采样器。

Result: 模拟研究显示在误设情况下优于替代方法。在事件相关电位功能数据分析中发现了可解释的神经认知亚组。

Conclusion: 投影DPP混合模型为贝叶斯聚类提供了理论可靠且实际有效的解决方案，支持可解释的潜在结构发现。

Abstract: In many scientific domains, clustering aims to reveal interpretable latent
structure that reflects relevant subpopulations or processes. Widely used
Bayesian mixture models for model-based clustering often produce overlapping or
redundant components because priors on cluster locations are specified
independently, hindering interpretability. To mitigate this, repulsive priors
have been proposed to encourage well-separated components, yet existing
approaches face both computational and theoretical challenges. We introduce a
fully tractable Bayesian repulsive mixture model by assigning a projection
Determinantal Point Process (DPP) prior to the component locations. Projection
DPPs induce strong repulsion and allow exact sampling, enabling parsimonious
and interpretable posterior clustering. Leveraging their analytical
tractability, we derive closed-form posterior and predictive distributions.
These results, in turn, enable two efficient inference algorithms: a
conditional Gibbs sampler and the first fully implementable marginal sampler
for DPP-based mixtures. We also provide strong frequentist guarantees,
including posterior consistency for density estimation, elimination of
redundant components, and contraction of the mixing measure. Simulation studies
confirm superior mixing and clustering performance compared to alternatives in
misspecified settings. Finally, we demonstrate the utility of our method on
event-related potential functional data, where it uncovers interpretable
neuro-cognitive subgroups. Our results support the projection DPP mixtures as a
theoretically sound and practically effective solution for Bayesian clustering.

</details>


### [14] [Uncovering All Highly Credible Binary Treatment Hierarchy Questions in Network Meta-Analysis](https://arxiv.org/abs/2510.08853)
*Caitlin H. Daly,Chloe Tan,Audrey Béliveau*

Main category: stat.ME

TL;DR: 提出了一种高效的算法方法，无需预先指定就能系统生成具有高可信度的治疗层次问题目录，从而从网络荟萃分析数据中提取所有有意义的信息。


<details>
  <summary>Details</summary>
Motivation: 在网络荟萃分析中，当涉及多个治疗时，可能的层次问题数量变得过于庞大。传统方法需要预先选择特定问题，限制了数据洞察的全面性。

Method: 开发了高效的算法方法，系统生成经验概率超过选定阈值（如95%）的治疗层次问题目录，并通过额外算法修剪冗余信息以便解释。定义了六种二元层次问题类型。

Result: 方法已在R包中实现，并在糖尿病和抑郁症干预的真实NMA数据集上进行了应用演示。

Conclusion: 该方法不仅适用于网络荟萃分析，还可用于任何涉及三个或更多治疗选择的决策问题，提供了一种可扩展且全面的数据洞察提取方法。

Abstract: In recent years, there has been growing research interest in addressing
treatment hierarchy questions within network meta-analysis (NMA). In NMAs
involving many treatments, the number of possible hierarchy questions becomes
prohibitively large. To manage this complexity, previous work has recommended
pre-selecting specific hierarchy questions of interest (e.g., ``among options
A, B, C, D, E, do treatments A and B have the two best effects in terms of
improving outcome X?") and calculating the empirical probabilities of the
answers being true given the data. In contrast, we propose an efficient and
scalable algorithmic approach that eliminates the need for pre-specification by
systematically generating a comprehensive catalog of highly credible treatment
hierarchy questions, specifically, those with empirical probabilities exceeding
a chosen threshold (e.g., 95%). This enables decision-makers to extract all
meaningful insights supported by the data. An additional algorithm trims
redundant insights from the output to facilitate interpretation. We define and
address six broad types of binary hierarchy questions (i.e., those with
true/false answers), covering standard hierarchy questions answered using
existing ranking metrics - pairwise comparisons and (cumulative) ranking
probabilities - as well as many other complex hierarchy questions. We have
implemented our methods in an R package and illustrate their application using
real NMA datasets on diabetes and depression interventions. Beyond NMA, our
approach is relevant to any decision problem concerning three or more treatment
options.

</details>


### [15] [Multidimensional Poverty Mapping for Small Areas](https://arxiv.org/abs/2510.08898)
*Soumojit Das,Dilshanie Deepawansa,Partha Lahiri*

Main category: stat.ME

TL;DR: 本文提出了一种基于贝叶斯多变量层次模型的多维贫困测绘方法，解决了小区域贫困测绘中维度贡献估计这一被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 传统贫困测量仅基于收入或消费，但多维贫困测量能更全面反映贫困状况。小区域贫困测绘面临样本量不足的挑战，且多维贫困测绘中存在维度贡献估计这一特殊问题，这在现有文献中被严重忽视。

Method: 使用多变量层次模型，通过贝叶斯方法实现，能够估计小区域多维贫困综合指标及各维度对总体贫困的相对贡献。

Result: 开发了一种新的多维贫困测绘方法，能够为小区域提供多维贫困综合指标和各维度贡献的估计，使用专门设计的调查数据进行验证。

Conclusion: 本文为贫困测绘文献开辟了新的方向，解决了多维贫困测绘中维度贡献估计这一关键但被忽视的问题。

Abstract: Many countries measure poverty based only on income or consumption. However,
there is a growing awareness of measuring poverty through multiple dimensions
that captures a more reasonable status of poverty. Estimating poverty
measure(s) for small geographical areas, commonly referred to as poverty
mapping, is challenging due to small or no sample for the small areas. While
there is a huge literature available on unidimensional poverty mapping, only a
limited effort has been made to address special challenges that arise only in
the multidimensional poverty mapping. For example, in multidimensional poverty
mapping, a new problem arises involving estimation of relative contributions of
different dimensions to overall poverty for small areas. This problem has been
grossly ignored in the small area estimation (SAE) literature. We address this
issue using a multivariate hierarchical model implemented via a Bayesian
method. Moreover, we demonstrate how a multidimensional poverty composite
measure can be estimated for small areas. In this paper, we demonstrate our
proposed methodology using a survey data specially designed by one of us for
multidimensional poverty mapping. This paper adds a new direction to poverty
mapping literature.

</details>


### [16] [Robust and Efficient Semiparametric Inference for the Stepped Wedge Design](https://arxiv.org/abs/2510.08972)
*Fan Xia,K. C. Gary Chan,Emily Voldal,Avi Kenny,Patrick J. Heagerty,James P. Hughes*

Main category: stat.ME

TL;DR: 提出了一个统一的半参数框架来估计阶梯楔形设计中的干预效应，该框架能处理时间趋势、聚类异质性、复杂相关结构和小样本问题。


<details>
  <summary>Details</summary>
Motivation: 阶梯楔形设计在评估纵向聚类干预时面临挑战，包括干预效应与时间趋势的混淆、聚类异质性、复杂相关结构、基线协变量不平衡和聚类数量少等问题。

Method: 在治疗对比的半参数模型下，开发了非标准半参数效率理论，考虑聚类内相关观测、不同聚类周期大小和弱依赖治疗分配。使用排列结构提出标准误估计器，并采用留一法校正减少插入偏差。

Result: 提出的估计器在错误指定协方差结构和控制聚类周期均值时仍保持一致性和渐近正态性，在正确指定时达到效率性。模拟和公共卫生试验应用显示该方法相对于标准方法具有更好的稳健性和效率。

Conclusion: 该框架为阶梯楔形设计提供了稳健且高效的推断方法，能够处理小样本问题和协变量不平衡，并允许纳入效应修正和精度变量调整。

Abstract: Stepped wedge designs (SWDs) are increasingly used to evaluate longitudinal
cluster-level interventions but pose substantial challenges for valid
inference. Because crossover times are randomized, intervention effects are
intrinsically confounded with secular time trends, while heterogeneity across
clusters, complex correlation structures, baseline covariate imbalances, and
small numbers of clusters further complicate inference. We propose a unified
semiparametric framework for estimating possibly time-varying intervention
effects in SWDs. Under a semiparametric model on treatment contrast, we develop
a nonstandard semiparametric efficiency theory that accommodates correlated
observations within clusters, varying cluster-period sizes, and weakly
dependent treatment assignments. The resulting estimator is consistent and
asymptotically normal even under misspecified covariance structure and control
cluster-period means, and is efficient when both are correctly specified. To
enable inference with few clusters, we exploit the permutation structure of
treatment assignment to propose a standard error estimator that reflects
finite-sample variability, with a leave-one-out correction to reduce plug-in
bias. The framework also allows incorporation of effect modification and
adjustment for imbalanced precision variables through design-based adjustment
or double adjustment that additionally incorporates an outcome-based component.
Simulations and application to a public health trial demonstrate the robustness
and efficiency of the proposed method relative to standard approaches.

</details>


### [17] [Revisiting Madigan and Mosurski: Collapsibility via Minimal Separators](https://arxiv.org/abs/2510.09024)
*Pei Heng,Yi Sun,Shiyuan He,Jianhua Guo*

Main category: stat.ME

TL;DR: 论文提出了基于最小分隔符的折叠性分析新方法，开发了高效的CMSA算法来构建最小可折叠集，显著提升了高维环境下的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的一般图折叠性分析算法计算成本高昂，限制了在高维设置中的实际应用。

Method: 提出Close Minimal Separator Absorption (CMSA)算法，通过局部分隔符搜索构建最小可折叠集，大大降低了计算成本。

Result: 模拟实验证实了显著的效率提升，使折叠性分析在高维环境中变得实用可行。

Conclusion: 基于最小分隔符的折叠性条件为高维图形模型提供了高效实用的维度缩减方法。

Abstract: Collapsibility provides a principled approach for dimension reduction in
contingency tables and graphical models. Madigan and Mosurski (1990) pioneered
the study of minimal collapsible sets in decomposable models, but existing
algorithms for general graphs remain computationally demanding. We show that a
model is collapsible onto a target set precisely when that set contains all
minimal separators between its non-adjacent vertices. This insight motivates
the Close Minimal Separator Absorption (CMSA) algorithm, which constructs
minimal collapsible sets using only local separator searches at very low costs.
Simulations confirm substantial efficiency gains, making collapsibility
analysis practical in high-dimensional settings.

</details>


### [18] [The bixplot: A variation on the boxplot suited for bimodal data](https://arxiv.org/abs/2510.09276)
*Camille M. Montalcini,Peter J. Rousseeuw*

Main category: stat.ME

TL;DR: 提出了一种名为bixplot的可视化方法扩展，专门用于检测和显示双峰和多峰分布数据，通过构建确保连续簇的单变量聚类方法。


<details>
  <summary>Details</summary>
Motivation: 传统箱线图和相关可视化方法在探索单变量数据时使用广泛，但缺乏专门检测和显示双峰性及多峰性的能力，需要一种能够识别数据中潜在有意义的子组的工具。

Method: 构建了一种单变量聚类方法，确保簇的连续性（没有簇的成员在另一个簇内部），且每个簇包含至少给定数量的唯一成员，从而创建bixplot显示。

Result: bixplot显示有助于识别和解释数据中潜在有意义的子组，同时显示个体数据值以引起对孤立点的注意，已在Python和R中实现，并在多个真实数据集上展示了其多种选项。

Conclusion: bixplot是传统箱线图的有效扩展，特别适用于检测和可视化双峰和多峰分布，能够通过颜色渐变等方式可视化外部变量，增强数据探索能力。

Abstract: Boxplots and related visualization methods are widely used exploratory tools
for taking a first look at collections of univariate variables. In this note an
extension is provided that is specifically designed to detect and display
bimodality and multimodality when the data warrant it. For this purpose a
univariate clustering method is constructed that ensures contiguous clusters,
meaning that no cluster has members inside another cluster, and such that each
cluster contains at least a given number of unique members. The resulting
bixplot display facilitates the identification and interpretation of
potentially meaningful subgroups underlying the data. The bixplot also displays
the individual data values, which can draw attention to isolated points.
Implementations of the bixplot are available in both Python and R, and their
many options are illustrated on several real datasets. For instance, an
external variable can be visualized by color gradations inside the display.

</details>


### [19] [Reliability Sensitivity with Response Gradient](https://arxiv.org/abs/2510.09315)
*Siu-Kui Au,Zi-Jun Cao*

Main category: stat.ME

TL;DR: 提出了一种基于响应梯度和核平滑的蒙特卡洛方法，用于计算工程风险中失效概率对系统参数的敏感性，解决了传统有限差分方法在计算敏感性时面临的问题。


<details>
  <summary>Details</summary>
Motivation: 工程风险分析中，失效概率对系统参数的敏感性对于风险知情决策至关重要。传统有限差分方法结合蒙特卡洛概率估计存在计算效率低、需要大量样本的问题，现有方法通常依赖于特定的输入变量类别或响应形式。

Method: 开发了一种理论框架和蒙特卡洛策略，利用响应值及其相对于敏感性参数的梯度来计算敏感性。通过核平滑概念解决在零概率事件（响应阈值）上条件期望的计算问题，能够在单次蒙特卡洛运行中为所有响应阈值提供敏感性估计。

Result: 该方法在多个具有不同性质敏感性参数的示例中得到验证，能够有效计算失效概率对系统参数的敏感性。

Conclusion: 随着响应梯度计算越来越可行，这项工作为在同一蒙特卡洛运行中嵌入敏感性计算和可靠性分析提供了基础，有望推动工程风险分析的发展。

Abstract: Engineering risk is concerned with the likelihood of failure and the
scenarios when it occurs. The sensitivity of failure probability to change in
system parameters is relevant to risk-informed decision making. Computing
sensitivity is at least one level more difficult than the probability itself,
which is already challenged by a large number of input random variables, rare
events and implicit nonlinear `black-box' response. Finite difference with
Monte Carlo probability estimates is spurious, requiring the number of samples
to grow with the reciprocal of step size to suppress estimation variance. Many
existing works gain efficiency by exploiting a specific class of input
variables, sensitivity parameters, or response in its exact or surrogate form.
For general systems, this work presents a theory and associated Monte Carlo
strategy for computing sensitivity using response values and gradients with
respect to sensitivity parameters. It is shown that the sensitivity at a given
response threshold can be expressed via the expectation of response gradient
conditional on the threshold. Determining the expectation requires conditioning
on the threshold that is a zero-probability event, but it can be resolved by
the concept of kernel smoothing. The proposed method offers sensitivity
estimates for all response thresholds generated in a single Monte Carlo run. It
is investigated in a number of examples featuring sensitivity parameters of
different nature. As response gradient becomes increasingly available, it is
hoped that this work can provide the basis for embedding sensitivity
calculations with reliability in the same Monte Carlo run.

</details>


### [20] [Uncertainty Quantification for Multi-level Models Using the Survey-Weighted Pseudo-Posterior](https://arxiv.org/abs/2510.09401)
*Matthew R. Williams,F. Hunter McGuire,Terrance D. Savitsky*

Main category: stat.ME

TL;DR: 本文提出了针对复杂调查样本中多水平模型的贝叶斯推断方法改进，评估了现有调查加权伪后验和自动化后处理方法的局限性，并通过模拟研究和实际案例验证了改进效果。


<details>
  <summary>Details</summary>
Motivation: 复杂调查样本中的参数估计通常关注全局模型参数，但对于多水平模型，局部参数（如组级随机效应）和全局参数都需要考虑复杂抽样设计的影响。

Method: 评估了调查加权伪后验和现有自动化后处理方法的局限性，提出了改进的自动化过程，通过模拟研究和国家药物使用与健康调查的实际案例进行验证。

Result: 提出的改进方法在模拟研究和实际应用中表现更好，更新的R包已发布在GitHub上。

Conclusion: 对于多水平模型的贝叶斯推断，需要专门考虑复杂抽样设计对局部和全局参数的影响，提出的改进方法能有效提升推断准确性。

Abstract: Parameter estimation and inference from complex survey samples typically
focuses on global model parameters whose estimators have asymptotic properties,
such as from fixed effects regression models. We present a motivating example
of Bayesian inference for a multi-level or mixed effects model in which both
the local parameters (e.g. group level random effects) and the global
parameters may need to be adjusted for the complex sampling design. We evaluate
the limitations of the survey-weighted pseudo-posterior and an existing
automated post-processing method to incorporate the complex survey sample
design for a wide variety of Bayesian models. We propose modifications to the
automated process and demonstrate their improvements for multi-level models via
a simulation study and a motivating example from the National Survey on Drug
Use and Health. Reproduction examples are available from the authors and the
updated R package is available via
github:https://github.com/RyanHornby/csSampling

</details>


### [21] [Defensive Model Expansion for Robust Bayesian Inference](https://arxiv.org/abs/2510.09598)
*Antonio R. Linero*

Main category: stat.ME

TL;DR: 论文提出了一种防御性模型扩展方法，通过将非参数模型强烈收缩到参数子模型，实现在参数模型正确时保持参数效率，在模型误设时激活灵活组件捕捉缺失信号。


<details>
  <summary>Details</summary>
Motivation: 应用研究者对非参数方法存在顾虑，担心在小样本中失去统计功效或在简单模型足够时过拟合。本文旨在证明当非参数模型被强烈收缩到参数子模型时，这些担忧是不必要的。

Method: 通过在参数模型基础上扩展一个强烈收缩到零的非参数组件，构建自适应模型。具体包括高斯过程回归和贝叶斯加性回归树的变体，这些贝叶斯非参数模型锚定在线性回归上。

Result: 理论证明当参数模型正确时，这些方法能一致识别正确的参数子模型并提供渐近有效的回归系数推断。模拟显示"通用BART"模型在参数模型正确时与正确设定的线性回归表现相同，在存在非线性效应时显著优于线性回归。

Conclusion: 提出"防御性模型扩展"作为防止模型误设的实用范式，该方法能自动适应：参数模型正确时恢复参数效率，误设时激活灵活组件捕获缺失信号。

Abstract: Some applied researchers hesitate to use nonparametric methods, worrying that
they will lose power in small samples or overfit the data when simpler models
are sufficient. We argue that at least some of these concerns are unfounded
when nonparametric models are strongly shrunk towards parametric submodels. We
consider expanding a parametric model with a nonparametric component that is
heavily shrunk toward zero. This construction allows the model to adapt
automatically: if the parametric model is correct, the nonparametric component
disappears, recovering parametric efficiency, while if it is misspecified, the
flexible component activates to capture the missing signal. We show that this
adaptive behavior follows from simple and general conditions. Specifically, we
prove that Bayesian nonparametric models anchored to linear regression,
including variants of Gaussian processes regression and Bayesian additive
regression trees, consistently identify the correct parametric submodel when it
holds and give asymptotically efficient inference for regression coefficients.
In simulations, we find that the "general BART" model performs identically to
correctly specified linear regression when the parametric model holds, and
substantially outperform it when nonlinear effects are present. This suggests a
practical paradigm: "defensive model expansion" as a safeguard against model
misspecification.

</details>
