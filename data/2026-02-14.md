<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 6]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.AP](#stat.AP) [Total: 2]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Who's Winning? Clarifying Estimands Based on Win Statistics in Cluster Randomized Trials](https://arxiv.org/abs/2602.11403)
*Kenneth M. Lee,Xi Fang,Fan Li,Michael O. Harhay*

Main category: stat.ME

TL;DR: 在群集随机试验中，基于胜率统计的治疗效果估计量存在个体配对和群集配对两种不同目标，当群集规模具有信息性时，这两种估计量可能得出相反结论，需谨慎选择目标估计量。


<details>
  <summary>Details</summary>
Motivation: 随着胜率统计（包括胜率比、胜率比值和胜率差）在临床试验中越来越流行，这些基于胜率的估计量提供了按临床重要性优先排序结果的直观方法。然而，在群集随机试验中，这些估计量的实施和解释变得复杂，因为研究者可以在个体水平或群集水平上针对根本不同的估计量。

Method: 研究描述了针对个体配对和群集配对胜率估计量的一致估计器，并提出了用于推断的留一集群交叉验证方差估计器。通过数值模拟演示了当群集规模具有信息性时，这两种估计量之间的差异。

Result: 当群集规模具有信息性时（即结果和/或治疗效果依赖于群集规模），个体配对和群集配对胜率估计量可能显著不同，甚至可能得出关于治疗益处的相反结论。个体配对胜率估计器存在有限样本偏差，而群集配对胜率估计器对其相应目标是无偏的。

Conclusion: 在群集随机试验中应用胜率估计器时，仔细指定目标估计量至关重要。未能明确定义个体配对或群集配对胜率估计量中的哪一个是主要关注点，可能导致回答与预期截然不同的问题。

Abstract: Treatment effect estimands based on win statistics, including the win ratio, win odds, and win difference are increasingly popular targets for summarizing endpoints in clinical trials. Such win estimands offer an intuitive approach for prioritizing outcomes by clinical importance. The implementation and interpretation of win estimands is complicated in cluster randomized trials (CRTs), where researchers can target fundamentally different estimands on the individual-level or cluster-level. We numerically demonstrate that individual-pair and cluster-pair win estimands can substantially differ when cluster size is informative: where outcomes and/or treatment effects depend on cluster size. With such informative cluster sizes, individual-pair and cluster-pair win estimands can even yield opposite conclusions regarding treatment benefit. We describe consistent estimators for individual-pair and cluster-pair win estimands and propose a leave-one-cluster-out jackknife variance estimator for inference. Despite being consistent, our simulations highlight that some caution is needed when implementing individual-pair win estimators due to finite-sample bias. In contrast, cluster-pair win estimators are unbiased for their respective targets. Altogether, careful specification of the target estimand is essential when applying win estimators in CRTs. Failure to clearly define whether individual-pair or cluster-pair win estimands are of primary interest may result in answering a dramatically different question than intended.

</details>


### [2] [High-Dimensional Mediation Analysis for Generalized Linear Models Using Bayesian Variable Selection Guided by Mediator Correlation](https://arxiv.org/abs/2602.11496)
*Youngho Bae,Chanmin Kim,Fenglei Wang,Qi Sun,Kyu Ha Lee*

Main category: stat.ME

TL;DR: 提出贝叶斯框架进行高维中介分析，处理高维、复杂相关性和非连续结果，通过马尔可夫随机场先验考虑中介变量相关性，提高相关中介变量场景下的统计功效。


<details>
  <summary>Details</summary>
Motivation: 现有高维中介分析方法通常假设中介变量独立或忽略相关性，当中介变量高度相关时会降低统计功效。同时需要处理高维、复杂依赖关系和非连续结果等挑战。

Method: 提出贝叶斯框架：1) 使用多元分布建模中介变量；2) 暴露-中介选择采用马尔可夫随机场先验；3) 中介-结果激活通过序列子集伯努利先验限制在暴露-中介模型中支持的中介变量；4) 在广义线性模型下选择活跃通路。

Result: 模拟研究显示在相关中介变量设置下改善了操作特性，全局零假设下具有适当的错误控制，模型误设下表现稳定。应用于真实代谢组学数据，研究地中海饮食依从性与心脏代谢结果之间的中介代谢物。

Conclusion: 该贝叶斯框架能够有效处理高维中介分析中的关键挑战，特别是考虑中介变量相关性，在相关中介变量场景下提高统计功效，适用于非连续结果和复杂依赖关系。

Abstract: High-dimensional mediation analysis aims to identify mediating pathways and to estimate indirect effects linking an exposure to an outcome. In this paper, we propose a Bayesian framework to address key challenges in these analyses, including high dimensionality, complex dependence among omics mediators, and non-continuous outcomes. Furthermore, commonly used approaches assume independent mediators or ignore correlations in the selection stage, which can reduce power when mediators are highly correlated. Addressing these challenges leads to a non-Gaussian likelihood and specialized selection priors, which in turn require efficient and adaptive posterior computation. Our proposed framework selects active pathways under generalized linear models while accounting for mediator dependence. Specifically, the mediators are modeled using a multivariate distribution, exposure-mediator selection is guided by a Markov random field prior on inclusion indicators, and mediator-outcome activation is restricted to mediators supported in the exposure-mediator model through a sequential subsetting Bernoulli prior. Simulation studies show improved operating characteristics in correlated-mediator settings, with appropriate error control under the global null and stable performance under model misspecification. We illustrate the method using real-world metabolomics data to study metabolites that mediate the association between adherence to the Alternate Mediterranean Diet score and two cardiometabolic outcomes.

</details>


### [3] [Representation Learning with Blockwise Missingness and Signal Heterogeneity](https://arxiv.org/abs/2602.11511)
*Ziqi Liu,Ye Tian,Weijing Tang*

Main category: stat.ME

TL;DR: 提出APPCA方法解决多源数据整合中的块缺失和块信号异质性挑战，通过锚点投影PCA实现稳健表示学习


<details>
  <summary>Details</summary>
Motivation: 多源数据整合面临两个关键挑战：块缺失（不同数据源观测不同但可能重叠的特征集）和块信号异质性（不同主体组和特征集间信号强度变化）。现有方法在完全观测数据或均匀信号强度下表现良好，但当这两个挑战同时出现时性能下降，而这种情况在实践中很常见。

Method: 提出锚点投影主成分分析（APPCA）框架：1）使用所有观测到的特征集恢复稳健的组特定列空间；2）通过将共享的"锚点"特征投影到这些子空间来对齐它们；3）然后执行PCA。投影步骤产生显著的降噪效果。

Result: 通过精细的扰动分析建立了嵌入重构的估计误差界。使用新颖的谱切片技术，该界消除了对主体嵌入信号强度的标准依赖，仅依赖于整合特征集的信号强度。通过广泛的模拟研究和多模态单细胞测序数据应用验证了该方法。

Conclusion: APPCA为具有结构化块缺失的表示学习提供了一个通用框架，对信号异质性具有鲁棒性，解决了现有方法在多源数据整合中同时面临块缺失和块信号异质性时的性能退化问题。

Abstract: Unified representation learning for multi-source data integration faces two important challenges: blockwise missingness and blockwise signal heterogeneity. The former arises from sources observing different, yet potentially overlapping, feature sets, while the latter involves varying signal strengths across subject groups and feature sets. While existing methods perform well with fully observed data or uniform signal strength, their performance degenerates when these two challenges coincide, which is common in practice. To address this, we propose Anchor Projected Principal Component Analysis (APPCA), a general framework for representation learning with structured blockwise missingness that is robust to signal heterogeneity. APPCA first recovers robust group-specific column spaces using all observed feature sets, and then aligns them by projecting shared "anchor" features onto these subspaces before performing PCA. This projection step induces a significant denoising effect. We establish estimation error bounds for embedding reconstruction through a fine-grained perturbation analysis. In particular, using a novel spectral slicing technique, our bound eliminates the standard dependency on the signal strength of subject embeddings, relying instead solely on the signal strength of integrated feature sets. We validate the proposed method through extensive simulation studies and an application to multimodal single-cell sequencing data.

</details>


### [4] [Improving the adjusted Benjamini--Hochberg method using e-values in knockoff-assisted variable selection](https://arxiv.org/abs/2602.11610)
*Aniket Biswas,Aaditya Ramdas*

Main category: stat.ME

TL;DR: 本文在Barber和Candès的knockoff多重检验框架下，将Sarkar和Tang的方法重新解释为未归一化e值加权Benjamini-Hochberg程序的特例，并扩展到使用有界p-to-e校准器实现更精细灵活的权重分配。


<details>
  <summary>Details</summary>
Motivation: 现有knockoff框架下的多重检验方法（如Barber和Candès 2015和Sarkar和Tang 2022）在权重分配和灵活性方面存在局限，需要更精细的权重分配机制来提升检验效能。

Method: 提出基于有界p-to-e校准器的e值加权Benjamini-Hochberg方法，包括三个具体程序：基础e值加权BH方法、使用真零假设比例估计的自适应扩展版本、以及自适应加权BH方法。

Result: 理论证明了所提方法能控制错误发现率（FDR）。模拟研究和HIV-1耐药性数据分析显示：在所有情况下都显著优于Sarkar和Tang方法；在低目标FDR、信号数量少且信号弱的情况下优于knockoff方法；具有强大的有限样本FDR控制能力。

Conclusion: 本文提出的基于有界p-to-e校准器的e值加权BH方法框架，不仅推广了Sarkar和Tang的方法，还提供了更灵活精细的权重分配机制，在保持FDR控制的同时，显著提升了检验效能。

Abstract: Considering the knockoff-based multiple testing framework of Barber and Candès [2015], we revisit the method of Sarkar and Tang [2022] and identify it as a specific case of an un-normalized e-value weighted Benjamini-Hochberg procedure. Building on this insight, we extend the method to use bounded p-to-e calibrators that enable more refined and flexible weight assignments. Our approach generalizes the method of Sarkar and Tang [2022], which emerges as a special case corresponding to an extreme calibrator. Within this framework, we propose three procedures: an e-value weighted Benjamini-Hochberg method, its adaptive extension using an estimate of the proportion of true null hypotheses, and an adaptive weighted Benjamini-Hochberg method. We establish control of the false discovery rate (FDR) for the proposed methods. While we do not formally prove that the proposed methods outperform those of Barber and Candès [2015] and Sarkar and Tang [2022], simulation studies and real-data analysis demonstrate large and consistent improvement over the latter in all cases, and better performance than the knockoff method in scenarios with low target FDR, a small number of signals, and weak signal strength. Simulation studies and a real-data application in HIV-1 drug resistance analysis demonstrate strong finite sample FDR control and exhibit improved, or at least competitive, power relative to the aforementioned methods.

</details>


### [5] [Locally Interpretable Individualized Treatment Rules for Black-Box Decision Models](https://arxiv.org/abs/2602.11520)
*Yasin Khadem Charvadeh,Katherine S. Panageas,Yuan Chen*

Main category: stat.ME

TL;DR: LI-ITR方法结合灵活机器学习模型和局部可解释近似，通过变分自编码器生成局部合成样本，使用可解释专家混合学习个体化治疗规则，在保持准确性的同时提供临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有个体化治疗规则方法要么使用可解释但不灵活的模型，要么使用高度灵活的黑盒方法牺牲可解释性，且大多采用单一全局决策规则，无法满足临床实践中对准确性和可解释性的双重需求。

Method: LI-ITR结合灵活机器学习模型准确学习复杂治疗结果，使用局部可解释近似构建患者特异性治疗规则。采用变分自编码器生成真实局部合成样本，通过可解释专家混合学习个体化决策规则。

Result: 模拟研究表明LI-ITR能准确恢复真实患者特异性局部系数和最优治疗策略。乳腺癌精准副作用管理应用展示了灵活预测建模的必要性，并凸显了LI-ITR在估计最优治疗规则同时提供透明临床解释的实用价值。

Conclusion: LI-ITR方法成功解决了个体化治疗规则中准确性与可解释性的权衡问题，通过局部可解释近似为临床决策提供透明解释，在精准医疗中具有重要应用价值。

Abstract: Individualized treatment rules (ITRs) aim to optimize healthcare by tailoring treatment decisions to patient-specific characteristics. Existing methods typically rely on either interpretable but inflexible models or highly flexible black-box approaches that sacrifice interpretability; moreover, most impose a single global decision rule across patients. We introduce the Locally Interpretable Individualized Treatment Rule (LI-ITR) method, which combines flexible machine learning models to accurately learn complex treatment outcomes with locally interpretable approximations to construct subject-specific treatment rules. LI-ITR employs variational autoencoders to generate realistic local synthetic samples and learns individualized decision rules through a mixture of interpretable experts. Simulation studies show that LI-ITR accurately recovers true subject-specific local coefficients and optimal treatment strategies. An application to precision side-effect management in breast cancer illustrates the necessity of flexible predictive modeling and highlights the practical utility of LI-ITR in estimating optimal treatment rules while providing transparent, clinically interpretable explanations.

</details>


### [6] [Batch-based Bayesian Optimal Experimental Design in Linear Inverse Problems](https://arxiv.org/abs/2602.12234)
*Sofia Mäkinen,Andrew B. Duncan,Tapio Helin*

Main category: stat.ME

TL;DR: 本文提出了一种基于Wasserstein梯度流的贝叶斯最优实验设计方法，用于连续域上的传感器位置优化，通过将A-最优设计松弛到有限正测度空间来解决批量设计的非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 在科学和工程中，如何在昂贵或受限的实验设置中最大化信息价值是一个普遍挑战。贝叶斯最优实验设计提供了解决这类问题的原则性框架，但批量设计（同时优化多个设计变量）会导致极其困难的高维非凸优化问题。

Method: 将A-最优设计松弛到有限正测度空间，建立对应的贝叶斯推断问题，开发基于Wasserstein梯度流的优化算法，并引入新的正则化方案以保证收敛到经验测度。

Result: 理论证明了该方法能收敛到经验测度，数值实验验证了收敛性和所提正则化策略的有效性。

Conclusion: 本文为连续域上的贝叶斯最优实验设计提供了一种有效的数值方法，通过测度松弛和Wasserstein梯度流解决了批量设计的非凸优化挑战，并保证了算法的收敛性。

Abstract: Experimental design is central to science and engineering. A ubiquitous challenge is how to maximize the value of information obtained from expensive or constrained experimental settings. Bayesian optimal experimental design (OED) provides a principled framework for addressing such questions. In this paper, we study experimental design problems such as the optimization of sensor locations over a continuous domain in the context of linear Bayesian inverse problems. We focus in particular on batch design, that is, the simultaneous optimization of multiple design variables, which leads to a notoriously difficult non-convex optimization problem. We tackle this challenge using a promising strategy recently proposed in the frequentist setting, which relaxes A-optimal design to the space of finite positive measures. Our main contribution is the rigorous identification of the Bayesian inference problem corresponding to this relaxed A-optimal OED formulation. Moreover, building on recent work, we develop a Wasserstein gradient-flow -based optimization algorithm for the expected utility and introduce novel regularization schemes that guarantee convergence to an empirical measure. These theoretical results are supported by numerical experiments demonstrating both convergence and the effectiveness of the proposed regularization strategy.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [7] [Estimation of instrument and noise parameters for inverse problem based on prior diffusion model](https://arxiv.org/abs/2602.11711)
*Jean-François Giovannelli*

Main category: stat.ML

TL;DR: 提出了一种在贝叶斯框架下估计逆问题中观测参数（响应和误差参数）的新方法，该方法结合扩散过程先验和有效的后验采样策略，能够同时优化估计观测参数和感兴趣图像，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决逆问题中观测参数估计的挑战，特别是在贝叶斯框架下引入正则化且先验由扩散过程建模时，后验采样通常很困难。需要一种既能灵活估计观测参数，又能有效进行不确定性量化的方法。

Method: 采用贝叶斯框架，将先验建模为扩散过程，利用最近提出的简单有效的后验采样策略。该方法定义了观测参数和感兴趣图像的最优估计器，并通过MCMC算法高效计算后验估计和性质。

Result: 数值实验证实了该方法具有计算高效性，能够提供高质量的估计结果和不确定性量化。MCMC算法能够高效计算后验估计并保证一定的可靠性。

Conclusion: 该方法成功解决了扩散过程先验下逆问题的观测参数估计问题，提供了灵活的参数估计框架、有效的后验采样策略和可靠的不确定性量化手段，在计算效率和估计质量方面都表现出色。

Abstract: This article addresses the issue of estimating observation parameters (response and error parameters) in inverse problems. The focus is on cases where regularization is introduced in a Bayesian framework and the prior is modeled by a diffusion process. In this context, the issue of posterior sampling is well known to be thorny, and a recent paper proposes a notably simple and effective solution. Consequently, it offers an remarkable additional flexibility when it comes to estimating observation parameters. The proposed strategy enables us to define an optimal estimator for both the observation parameters and the image of interest. Furthermore, the strategy provides a means of quantifying uncertainty. In addition, MCMC algorithms allow for the efficient computation of estimates and properties of posteriors, while offering some guarantees. The paper presents several numerical experiments that clearly confirm the computational efficiency and the quality of both estimates and uncertainties quantification.

</details>


### [8] [Amortised and provably-robust simulation-based inference](https://arxiv.org/abs/2602.11325)
*Ayush Bharti,Charita Dellaporta,Yuga Hikida,François-Xavier Briol*

Main category: stat.ML

TL;DR: 提出一种基于广义贝叶斯推断和神经加权评分匹配损失的仿真推断方法，具有摊销计算和抗异常值特性，无需MCMC采样，计算复杂度显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有仿真推断方法通常无法处理数据中的异常值和极端值，这些可能由测量仪器故障或人为错误引起，需要开发鲁棒的推断方法。

Method: 结合广义贝叶斯推断和神经加权评分匹配损失，通过精心选择的条件密度模型，实现无需MCMC采样的摊销推断。

Result: 该方法同时具备摊销计算和理论证明的抗异常值特性，计算复杂度仅为当前最先进方法的一小部分。

Conclusion: 提出了一种新颖的鲁棒仿真推断框架，在计算效率和抗异常值能力方面均有显著优势，为复杂仿真模型的实际应用提供了实用解决方案。

Abstract: Complex simulator-based models are now routinely used to perform inference across the sciences and engineering, but existing inference methods are often unable to account for outliers and other extreme values in data which occur due to faulty measurement instruments or human error. In this paper, we introduce a novel approach to simulation-based inference grounded in generalised Bayesian inference and a neural approximation of a weighted score-matching loss. This leads to a method that is both amortised and provably robust to outliers, a combination not achieved by existing approaches. Furthermore, through a carefully chosen conditional density model, we demonstrate that inference can be further simplified and performed without the need for Markov chain Monte Carlo sampling, thereby offering significant computational advantages, with complexity that is only a small fraction of that of current state-of-the-art approaches.

</details>


### [9] [The Cost of Learning under Multiple Change Points](https://arxiv.org/abs/2602.11406)
*Tomer Gafni,Garud Iyengar,Assaf Zeevi*

Main category: stat.ML

TL;DR: 提出ATC算法解决多变化点在线学习问题，避免传统方法的内生混淆问题，实现近乎最优的遗憾界


<details>
  <summary>Details</summary>
Motivation: 传统高置信度检测方法在单变化点问题上表现良好，但在多变化点环境中会出现灾难性失败（高遗憾），这是由于内生混淆现象导致的

Method: 提出Anytime Tracking CUSUM (ATC)算法，这是一种无时间限制的在线算法，采用选择性检测原则，平衡忽略"小"变化和快速响应显著变化的需求

Result: 理论证明：适当调参的ATC算法性能近乎极小极大最优，其遗憾界与多变化点问题中任何学习算法可实现性能的信息论下界紧密匹配。合成数据和真实世界数据的实验验证了理论发现

Conclusion: ATC算法有效解决了多变化点在线学习问题，克服了传统方法的内生混淆问题，实现了理论上的最优性能，并在实验中得到了验证

Abstract: We consider an online learning problem in environments with multiple change points. In contrast to the single change point problem that is widely studied using classical "high confidence" detection schemes, the multiple change point environment presents new learning-theoretic and algorithmic challenges. Specifically, we show that classical methods may exhibit catastrophic failure (high regret) due to a phenomenon we refer to as endogenous confounding. To overcome this, we propose a new class of learning algorithms dubbed Anytime Tracking CUSUM (ATC). These are horizon-free online algorithms that implement a selective detection principle, balancing the need to ignore "small" (hard-to-detect) shifts, while reacting "quickly" to significant ones. We prove that the performance of a properly tuned ATC algorithm is nearly minimax-optimal; its regret is guaranteed to closely match a novel information-theoretic lower bound on the achievable performance of any learning algorithm in the multiple change point problem. Experiments on synthetic as well as real-world data validate the aforementioned theoretical findings.

</details>


### [10] [Provable Offline Reinforcement Learning for Structured Cyclic MDPs](https://arxiv.org/abs/2602.11679)
*Kyungbok Lee,Angelica Cristello Sarteau,Michael R. Kosorok*

Main category: stat.ML

TL;DR: 提出循环MDP框架处理多阶段决策问题，开发CycleFQI算法分解循环过程，缓解维度灾难，在糖尿病数据上验证效果


<details>
  <summary>Details</summary>
Motivation: 传统离线学习在多阶段决策问题中面临挑战：优化任一阶段策略会改变后续阶段的状态分布，导致循环中的不匹配传播。需要处理阶段间异质性动态、转移和折扣因子

Method: 提出循环MDP框架，将循环过程分解为阶段性子问题。开发CycleFQI算法（拟合Q迭代的扩展），使用阶段特定Q函数向量，支持部分控制（部分阶段优化，部分阶段使用预定义策略）

Result: 建立了有限样本次优误差界，在Besov正则性下推导全局收敛率，证明CycleFQI相比整体基线缓解维度灾难。提出基于筛法的渐近推断方法。在模拟和真实1型糖尿病数据集上验证有效性

Conclusion: 提出的循环MDP框架和CycleFQI算法有效处理多阶段决策问题中的异质性，通过模块化分解缓解维度灾难，支持部分控制，在理论和实验上均表现良好

Abstract: We introduce a novel cyclic Markov decision process (MDP) framework for multi-step decision problems with heterogeneous stage-specific dynamics, transitions, and discount factors across the cycle. In this setting, offline learning is challenging: optimizing a policy at any stage shifts the state distributions of subsequent stages, propagating mismatch across the cycle. To address this, we propose a modular structural framework that decomposes the cyclic process into stage-wise sub-problems. While generally applicable, we instantiate this principle as CycleFQI, an extension of fitted Q-iteration enabling theoretical analysis and interpretation. It uses a vector of stage-specific Q-functions, tailored to each stage, to capture within-stage sequences and transitions between stages. This modular design enables partial control, allowing some stages to be optimized while others follow predefined policies. We establish finite-sample suboptimality error bounds and derive global convergence rates under Besov regularity, demonstrating that CycleFQI mitigates the curse of dimensionality compared to monolithic baselines. Additionally, we propose a sieve-based method for asymptotic inference of optimal policy values under a margin condition. Experiments on simulated and real-world Type 1 Diabetes data sets demonstrate CycleFQI's effectiveness.

</details>


### [11] [PAC-Bayesian Generalization Guarantees for Fairness on Stochastic and Deterministic Classifiers](https://arxiv.org/abs/2602.11722)
*Julien Bastian,Benjamin Leblanc,Pascal Germain,Amaury Habrard,Christine Largeron,Guillaume Metzler,Emilie Morvant,Paul Viallard*

Main category: stat.ML

TL;DR: 提出基于PAC-Bayes的公平性泛化界框架，适用于随机和确定性分类器，可优化预测风险与公平性约束的权衡


<details>
  <summary>Details</summary>
Motivation: 传统PAC泛化界无法为平衡预测风险和公平性约束的模型提供理论保证，需要专门针对公平性的泛化理论框架

Method: 采用PAC-Bayesian框架推导公平性泛化界：对随机分类器使用标准PAC-Bayes技术，对确定性分类器利用最新PAC-Bayes进展扩展边界。框架支持可表示为风险差异的广泛公平性度量，并开发自约束算法直接优化泛化界

Result: 建立了适用于随机和确定性分类器的公平性泛化界，框架具有广泛适用性（支持多种公平性度量）和实用性（可优化泛化界）。实验验证了框架的有效性和边界的紧致性

Conclusion: 提出的PAC-Bayesian框架为公平性机器学习提供了理论保证，能够同时优化预测风险和公平性约束，实验证明边界紧致且实用

Abstract: Classical PAC generalization bounds on the prediction risk of a classifier are insufficient to provide theoretical guarantees on fairness when the goal is to learn models balancing predictive risk and fairness constraints. We propose a PAC-Bayesian framework for deriving generalization bounds for fairness, covering both stochastic and deterministic classifiers. For stochastic classifiers, we derive a fairness bound using standard PAC-Bayes techniques. Whereas for deterministic classifiers, as usual PAC-Bayes arguments do not apply directly, we leverage a recent advance in PAC-Bayes to extend the fairness bound beyond the stochastic setting. Our framework has two advantages: (i) It applies to a broad class of fairness measures that can be expressed as a risk discrepancy, and (ii) it leads to a self-bounding algorithm in which the learning procedure directly optimizes a trade-off between generalization bounds on the prediction risk and on the fairness. We empirically evaluate our framework with three classical fairness measures, demonstrating not only its usefulness but also the tightness of our bounds.

</details>


### [12] [Aggregate Models, Not Explanations: Improving Feature Importance Estimation](https://arxiv.org/abs/2602.11760)
*Joseph Paillard,Angel Reyero Lobo,Denis A. Engemann,Bertrand Thirion*

Main category: stat.ML

TL;DR: 该论文研究了在特征重要性分析中，应该解释单个集成模型还是聚合多个模型解释的问题，发现模型级集成能提供更准确的特征重要性估计，特别是对于表达能力强的模型。


<details>
  <summary>Details</summary>
Motivation: 特征重要性方法有望将机器学习模型从预测工具转变为科学发现工具，但由于数据采样和算法随机性，表达能力强的模型可能不稳定，导致特征重要性估计不准确，影响在关键生物医学应用中的效用。虽然集成方法提供了解决方案，但如何选择解释单个集成模型还是聚合多个模型解释仍是一个未充分研究的问题。

Method: 在适应复杂最先进ML模型的假设下进行理论分析，研究模型级集成与解释级集成的选择问题。通过经典基准测试和英国生物银行的大规模蛋白质组学研究验证理论发现。

Result: 理论分析表明，这一选择主要由模型的超额风险驱动。与先前文献不同，研究发现模型级集成能提供更准确的特征重要性估计，特别是对于表达能力强的模型，通过减少主要误差项来实现。

Conclusion: 在特征重要性分析中，模型级集成优于解释级集成，特别是在处理表达能力强的机器学习模型时，能提供更稳定和准确的特征重要性估计，增强其在生物医学等关键应用中的实用性。

Abstract: Feature-importance methods show promise in transforming machine learning models from predictive engines into tools for scientific discovery. However, due to data sampling and algorithmic stochasticity, expressive models can be unstable, leading to inaccurate variable importance estimates and undermining their utility in critical biomedical applications. Although ensembling offers a solution, deciding whether to explain a single ensemble model or aggregate individual model explanations is difficult due to the nonlinearity of importance measures and remains largely understudied. Our theoretical analysis, developed under assumptions accommodating complex state-of-the-art ML models, reveals that this choice is primarily driven by the model's excess risk. In contrast to prior literature, we show that ensembling at the model level provides more accurate variable-importance estimates, particularly for expressive models, by reducing this leading error term. We validate these findings on classical benchmarks and a large-scale proteomic study from the UK Biobank.

</details>


### [13] [The Implicit Bias of Logit Regularization](https://arxiv.org/abs/2602.12039)
*Alon Beck,Yohai Bar Sinai,Noam Levi*

Main category: stat.ML

TL;DR: 论文分析了logit正则化（如标签平滑）在线性分类中的机制，发现其诱导logit聚类，使权重向量对齐Fisher线性判别，在信号加噪声模型中显著降低临界样本复杂度并诱导grokking现象。


<details>
  <summary>Details</summary>
Motivation: 尽管logit正则化（如标签平滑）广泛用于现代分类器并改善校准和泛化，但其工作机制仍未被充分理解。本文旨在深入分析这类正则化方法的内在机制。

Method: 在线性分类框架下分析一般类别的logit正则化方法，证明其诱导logit围绕有限每样本目标聚类。针对高斯数据或logit充分聚类情况，证明这种聚类驱动权重向量精确对齐Fisher线性判别。通过信号加噪声模型展示其影响。

Result: logit正则化诱导的logit聚类使权重向量对齐Fisher线性判别；在信号加噪声模型中，logit正则化将临界样本复杂度减半，在小噪声极限下诱导grokking现象，并使泛化对噪声具有鲁棒性。

Conclusion: 研究扩展了对标签平滑等logit正则化方法的理论理解，揭示了更广泛类别logit正则化方法的有效性，特别是其通过logit聚类和对齐Fisher线性判别来改善泛化的机制。

Abstract: Logit regularization, the addition a convex penalty directly in logit space, is widely used in modern classifiers, with label smoothing as a prominent example. While such methods often improve calibration and generalization, their mechanism remains under-explored. In this work, we analyze a general class of such logit regularizers in the context of linear classification, and demonstrate that they induce an implicit bias of logit clustering around finite per-sample targets. For Gaussian data, or whenever logits are sufficiently clustered, we prove that logit clustering drives the weight vector to align exactly with Fisher's Linear Discriminant. To demonstrate the consequences, we study a simple signal-plus-noise model in which this transition has dramatic effects: Logit regularization halves the critical sample complexity and induces grokking in the small-noise limit, while making generalization robust to noise. Our results extend the theoretical understanding of label smoothing and highlight the efficacy of a broader class of logit-regularization methods.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [14] [Regularized Ensemble Forecasting for Learning Weights from Historical and Current Forecasts](https://arxiv.org/abs/2602.11379)
*Han Su,Xiaojia Guo,Xiaoke Zhang*

Main category: stat.AP

TL;DR: 提出一种新颖的正则化集成方法，结合当前预测和历史表现来设置权重，优于传统线性意见池和现有基准模型。


<details>
  <summary>Details</summary>
Motivation: 结合多个专家的预测通常比依赖单个专家更准确。现有方法要么只依赖当前预测，要么只依赖历史准确性，需要一种能同时考虑这两个信息源的方法。

Method: 提出正则化集成方法，通过最小化组合预测的方差（或其变换版本）来学习权重，同时加入基于历史表现的正则化项。该方法具有贝叶斯解释，不同分布假设产生不同的方差分量和正则化项函数形式。

Result: 在沃尔玛销售和宏观经济预测的实证研究中，该方法优于领先的基准模型，无论专家是否有完整的预测历史记录，还是在专家随时间进出导致历史记录不完整的情况下都表现良好。

Conclusion: 该方法通过同时利用当前预测和历史表现来设置权重，提供了一种灵活的集成框架。通过实证结果分析了该方法在不同情境下的优势，并讨论了何时历史表现或当前预测更具信息性。

Abstract: Combining forecasts from multiple experts often yields more accurate results than relying on a single expert. In this paper, we introduce a novel regularized ensemble method that extends the traditional linear opinion pool by leveraging both current forecasts and historical performances to set the weights. Unlike existing approaches that rely only on either the current forecasts or past accuracy, our method accounts for both sources simultaneously. It learns weights by minimizing the variance of the combined forecast (or its transformed version) while incorporating a regularization term informed by historical performances. We also show that this approach has a Bayesian interpretation. Different distributional assumptions within this Bayesian framework yield different functional forms for the variance component and the regularization term, adapting the method to various scenarios. In empirical studies on Walmart sales and macroeconomic forecasting, our ensemble outperforms leading benchmark models both when experts' full forecasting histories are available and when experts enter and exit over time, resulting in incomplete historical records. Throughout, we provide illustrative examples that show how the optimal weights are determined and, based on the empirical results, we discuss where the framework's strengths lie and when experts' past versus current forecasts are more informative.

</details>


### [15] [Enhanced Forest Inventories for Habitat Mapping: A Case Study in the Sierra Nevada Mountains of California](https://arxiv.org/abs/2602.12072)
*Maxime Turgeon,Michael Kieser,Dwight Wolfe,Bruce MacArthur*

Main category: stat.AP

TL;DR: 开发增强型森林清单系统，结合地面调查与多模态遥感数据，实现高分辨率野生动物栖息地制图，应用于加州森林管理。


<details>
  <summary>Details</summary>
Motivation: 传统森林清单系统空间分辨率不足，缺乏现代多资源生态系统管理所需的结构细节，无法满足野生动物栖息地保护需求。

Method: 整合118个地面FIA样地与LiDAR、航空摄影、Sentinel-2卫星影像等多模态遥感数据，采用双层分割方法将景观划分为约57.5万个报告单元，使用弹性网络回归框架和自动特征选择建立预测模型。

Result: 成功识别出加州斑点猫头鹰的25,630英亩筑巢栖息地和26,622英亩觅食栖息地，以及太平洋渔貂的25,636英亩潜在栖息地，基于大径级树木和高冠层闭合度等结构要求。

Conclusion: 增强型森林清单为林业与保护生态学之间提供了关键桥梁，为森林管理者提供了空间明确的工具，可在复杂环境中监测生态系统健康和管理脆弱物种。

Abstract: Traditional forest inventory systems, originally designed to quantify merchantable timber volume, often lack the spatial resolution and structural detail required for modern multi-resource ecosystem management. In this manuscript, we present an Enhanced Forest Inventory (EFI) and demonstrate its utility for high-resolution wildlife habitat mapping. The project area covers 270,000 acres of the Eldorado National Forest in California's Sierra Nevada. By integrating 118 ground-truth Forest Inventory and Analysis (FIA) plots with multi-modal remote sensing data (LiDAR, aerial photography, and Sentinel-2 satellite imagery), we developed predictive models for key forest attributes. Our methodology employed a two-tier segmentation approach, partitioning the landscape into approximately 575,000 reporting units with an average size of 0.5 acre to capture forest heterogeneity. We utilized an Elastic-Net Regression framework and automated feature selection to relate remote sensing metrics to ground-measured variables such as basal area, stems per acre, and canopy cover. These physical metrics were translated into functional habitat attributes to evaluate suitability for two focal species: the California Spotted Owl (Strix occidentalis occidentalis) and the Pacific Fisher (Pekania pennanti). Our analysis identified 25,630 acres of nesting and 26,622 acres of foraging habitat for the owl, and 25,636 acres of likely habitat for the fisher based on structural requirements like large-diameter trees and high canopy closure. The results demonstrate that EFIs provide a critical bridge between forestry and conservation ecology, offering forest managers a spatially explicit tool to monitor ecosystem health and manage vulnerable species in complex environments.

</details>
