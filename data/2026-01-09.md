<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 16]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Progressive Bayesian Confidence Architectures for Cold-Start Personal Health Analytics: Formalizing Early Insight Through Posterior Contraction and Risk-Aware Interpretation](https://arxiv.org/abs/2601.03299)
*Richik Chakraborty*

Main category: stat.ME

TL;DR: 提出渐进式贝叶斯置信架构，解决个性化健康分析中的冷启动问题，在数据收集早期提供有意义的见解，同时保持不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 个性化健康分析系统面临持续存在的冷启动困境：用户期望在数据收集早期获得有意义的见解，而传统统计推断需要的数据量往往超过用户参与时间。现有方法要么延迟推断直到达到固定统计阈值（导致用户流失），要么在没有正式不确定性量化的前提下提供启发式见解（存在错误置信风险）。

Method: 提出渐进式贝叶斯置信架构，通过分阶段解释后验不确定性来形式化早期推断。借鉴金融风险建模中的贝叶斯更新和认知策略，将后验收缩映射到可解释的见解层级，从探索性方向证据到稳健的关联推断。

Result: 在合成N-of-1健康数据的受控实验中，校准的早期见解可在5-7天内生成，同时保持明确的认知谦逊。相比需要30+天数据的固定阈值基线，该方法产生更早的方向信号（平均：5.3天 vs 31.7天，p<0.001），同时将错误发现率控制在6%以下（第30天为5.9%），而固定阈值基线延迟30天提供见解但FDR为0%。在不确定性校准方面表现良好（第90天真实相关性的76%可信区间覆盖）。

Conclusion: 这项工作为个性化健康分析中的不确定性感知早期推断提供了一个方法论框架，弥合了用户参与需求和统计严谨性之间的差距。

Abstract: Personal health analytics systems face a persistent cold-start dilemma: users expect meaningful insights early in data collection, while conventional statistical inference requires data volumes that often exceed engagement horizons. Existing approaches either delay inference until fixed statistical thresholds are met -- leading to user disengagement -- or surface heuristic insights without formal uncertainty quantification, risking false confidence. We propose a progressive Bayesian confidence architecture that formalizes early-stage inference through phased interpretation of posterior uncertainty. Drawing on Bayesian updating and epistemic strategies from financial risk modeling under sparse observations, we map posterior contraction to interpretable tiers of insight, ranging from exploratory directional evidence to robust associative inference. We demonstrate the framework's performance through controlled experimentation with synthetic N-of-1 health data, showing that calibrated early insights can be generated within 5--7 days while maintaining explicit epistemic humility. Compared to fixed-threshold baselines requiring 30+ days of data, the proposed approach yields earlier directional signals (mean: 5.3 vs 31.7 days, p<0.001) while controlling false discovery rates below 6% (5.9% at day 30) despite 26-day earlier detection, compared to 0% FDR for fixed-threshold baselines that delay insights by 30 days. In addition, we show strong uncertainty calibration (76% credible interval coverage for ground-truth correlations at day 90). This work contributes a methodological framework for uncertainty-aware early inference in personalized health analytics that bridges the gap between user engagement requirements and statistical rigor.

</details>


### [2] [On estimands in target trial emulation](https://arxiv.org/abs/2601.03377)
*Edoardo Efrem Gervasoni,Liesbet De Bus,Stijn Vansteelandt,Oliver Dukes*

Main category: stat.ME

TL;DR: 提出一种基于估计量而非模型设定的目标试验分析方法，确保处理效应在模型误设下仍可解释，适用于不同研究设计


<details>
  <summary>Details</summary>
Motivation: 传统目标试验框架通常假设处理效应恒定，但实际处理效应可能随时间变化，特别是使用非可折叠估计量时，这导致解释困难

Method: 提出模型无关的策略，以估计量选择为中心而非模型设定；针对不同研究设计提出合适的估计量，并开发相应的G-计算和逆概率加权估计器

Result: 在模拟和ICU抗菌药物降阶梯治疗的真实数据应用中，该方法比传统技术提供更清晰和可靠的结果

Conclusion: 基于估计量的目标试验分析方法能确保处理效应在模型误设下仍对明确定义的人群保持可解释性，优于传统方法

Abstract: The target trial framework enables causal inference from longitudinal observational data by emulating randomized trials initiated at multiple time points. Precision is often improved by pooling information across trials, with standard models typically assuming - among other things - a time-constant treatment effect. However, this obscures interpretation when the true treatment effect varies, which we argue to be likely as a result of relying on noncollapsible estimands. To address these challenges, this paper introduces a model-free strategy for target trial analysis, centered around the choice of the estimand, rather than model specification. This ensures that treatment effects remain clearly interpretable for well-defined populations even under model misspecification. We propose estimands suitable for different study designs, and develop accompanying G-computation and inverse probability weighted estimators. Applications on simulations and real data on antimicrobial de-escalation in an intensive care unit setting demonstrate the greater clarity and reliability of the proposed methodology over traditional techniques.

</details>


### [3] [Propagating Surrogate Uncertainty in Bayesian Inverse Problems](https://arxiv.org/abs/2601.03532)
*Andrew Gerard Roberts,Michael Dietze,Jonathan H. Huggins*

Main category: stat.ME

TL;DR: 该论文提出了一种称为期望后验（EP）的混合分布作为不确定性感知后验近似的一般基线，分析了流行的启发式方法期望未归一化后验（EUP）可能偏离EP基线的情况，并提出了RKpCN算法来在无限维高斯过程代理模型设置中实现EP近似。


<details>
  <summary>Details</summary>
Motivation: 在计算昂贵的正向模型的反问题中，标准贝叶斯推断方案不可行。通常使用更便宜的代理模型替代，但需要传播代理近似的不确定性以避免过度自信的结论。目前存在多种不同的不确定性传播方法，但对其差异缺乏理解。

Method: 1. 提出期望后验（EP）作为不确定性感知后验近似的一般基线，基于决策理论和模块化贝叶斯推断论证；2. 分析期望未归一化后验（EUP）启发式方法，研究其何时偏离EP基线；3. 提出随机核预条件Crank-Nicolson（RKpCN）算法，用于在无限维高斯过程代理模型设置中实现EP近似。

Result: 研究表明，当代理不确定性在设计空间上高度不均匀时（如高斯过程模拟对数似然的情况），EUP启发式方法可能会失效。提出的RKpCN算法能够在具有无限维高斯过程代理模型的挑战性设置中提供实用的EP近似。

Conclusion: 期望后验（EP）为不确定性感知后验近似提供了理论合理的基线，而EUP启发式方法在代理不确定性不均匀时可能不可靠。RKpCN算法为实际实现EP近似提供了有效的计算工具，特别是在涉及无限维高斯过程代理模型的复杂设置中。

Abstract: Standard Bayesian inference schemes are infeasible for inverse problems with computationally expensive forward models. A common solution is to replace the model with a cheaper surrogate. To avoid overconfident conclusions, it is essential to acknowledge the surrogate approximation by propagating its uncertainty. At present, a variety of distinct uncertainty propagation methods have been suggested, with little understanding of how they vary. To fill this gap, we propose a mixture distribution termed the expected posterior (EP) as a general baseline for uncertainty-aware posterior approximation, justified by decision theoretic and modular Bayesian inference arguments. We then investigate the expected unnormalized posterior (EUP), a popular heuristic alternative, analyzing when it may deviate from the EP baseline. Our results show that this heuristic can break down when the surrogate uncertainty is highly non-uniform over the design space, as can be the case when the log-likelihood is emulated by a Gaussian process. Finally, we present the random kernel preconditioned Crank-Nicolson (RKpCN) algorithm, an approximate Markov chain Monte Carlo scheme that provides practical EP approximation in the challenging setting involving infinite-dimensional Gaussian process surrogates.

</details>


### [4] [Measures of classification bias derived from sample size analysis](https://arxiv.org/abs/2601.03453)
*Ioannis Ivrissimtzis,Shauna Concannon,Matthew Houliston,Graham Roberts*

Main category: stat.ME

TL;DR: 提出一种基于统计检测所需样本量的算法分类偏差度量方法：偏差程度与检测所需样本量成反比


<details>
  <summary>Details</summary>
Motivation: 现有偏差度量方法（如误差率差e2-e1、误差率比e2/e1）可能无法充分反映算法偏差的统计显著性，需要一种更直观、基于统计检测难度的偏差度量方法

Method: 使用卡方检验的近似样本量公式，将偏差定义为检测误差率差异所需样本量的倒数。在二元人口统计设置中，基于非参数误差率估计e1和e2，验证了该度量的基本性质

Result: 提出的度量方法与现有方法（误差率差和误差率比）在算法偏差排序上存在本质差异，具有独特优势。该度量的优良性质源于方法本身而非特定近似公式，可推广到更复杂场景

Conclusion: 基于统计检测所需样本量的偏差度量方法提供了一种直观、有效的算法偏差评估框架，能够捕捉传统方法可能忽略的统计显著性维度，适用于多元人口统计场景

Abstract: We propose the use of a simple intuitive principle for measuring algorithmic classification bias: the significance of the differences in a classifier's error rates across the various demographics is inversely commensurate with the sample size required to statistically detect them. That is, if large sample sizes are required to statistically establish biased behavior, the algorithm is less biased, and vice versa. In a simple setting, we assume two distinct demographics, and non-parametric estimates of the error rates on them, e1 and e2, respectively. We use a well-known approximate formula for the sample size of the chi-squared test, and verify some basic desirable properties of the proposed measure. Next, we compare the proposed measure with two other commonly used statistics, the difference e2-e1 and the ratio e2/e1 of the error rates. We establish that the proposed measure is essentially different in that it can rank algorithms for bias differently, and we discuss some of its advantages over the other two measures. Finally, we briefly discuss how some of the desirable properties of the proposed measure emanate from fundamental characteristics of the method, rather than the approximate sample size formula we used, and thus, are expected to hold in more complex settings with more than two demographics.

</details>


### [5] [Improving operating characteristics of clinical trials by augmenting control arm using propensity score-weighted borrowing-by-parts power prior](https://arxiv.org/abs/2601.03480)
*Apu Chandra Das,Sakib Salam,Aninda Roy,Rakhi Chowdhury,Antar Chandra Das,Ashim Chandra Das*

Main category: stat.ME

TL;DR: 提出PSW-BPP方法，结合倾向得分加权和贝叶斯部分借用先验，在保持估计效率的同时处理外部数据引入的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 借用外部数据可以提高估计效率，但当人群在协变量分布或结果变异性上存在差异时，可能引入偏差。需要在两个数据集之间保持适当平衡。

Method: 1) 使用倾向得分加权对齐外部数据与当前研究的协变量分布；2) 通过借用部分先验将加权外部似然纳入贝叶斯模型，允许对均值和方差分量使用不同的借用参数；3) 采用最小合理性指数计算借用参数。

Result: 模拟研究表明，在中等协变量不平衡和结果异质性下，PSW-BPP比不借用和固定借用方法提供更高效、更稳定的估计。真实数据示例验证了方法的实用性。

Conclusion: PSW-BPP为观察性和混合研究设计中的贝叶斯推断提供了一个原则性、可扩展的方法框架，能够灵活校准信息借用，提高估计稳健性。

Abstract: Borrowing external data can improve estimation efficiency but may introduce bias when populations differ in covariate distributions or outcome variability. A proper balance needs to be maintained between the two datasets to justify the borrowing. We propose a propensity score weighting borrowing-by-parts power prior (PSW-BPP) that integrates causal covariate adjustment through propensity score weighting with a flexible Bayesian borrowing approach to address these challenges in a unified framework. The proposed approach first applies propensity score weighting to align the covariate distribution of the external data with that of the current study, thereby targeting a common estimand and reducing confounding due to population heterogeneity. The weighted external likelihood is then incorporated into a Bayesian model through a borrowing-by-parts power prior, which allows distinct power parameters for the mean and variance components of the likelihood, enabling differential and calibrated information borrowing. Additionally, we adopt the idea of the minimal plausibility index (mPI) to calculate the power parameters. This separate borrowing provides greater robustness to prior-data conflict compared with traditional power prior methods that impose a single borrowing parameter. We study the operating characteristics of PSW-BPP through extensive simulation and a real data example. Simulation studies demonstrate that PSW-BPP yields more efficient and stable estimation than no borrowing and fixed borrowing, particularly under moderate covariate imbalance and outcome heterogeneity. The proposed framework offers a principled and extensible methodological contribution for Bayesian inference with external data in observational and hybrid study designs.

</details>


### [6] [Differentially Private Bayesian Inference for Gaussian Copula Correlations](https://arxiv.org/abs/2601.03497)
*Shuo Wang,Joseph Feldman,Jerome P. Reiter*

Main category: stat.ME

TL;DR: 提出两种满足差分隐私的高斯copula相关性估计算法：基于后验分布的方法和最大似然点估计方法


<details>
  <summary>Details</summary>
Motivation: 高斯copula广泛应用于多元分布估计，但现有方法缺乏隐私保护。需要开发满足差分隐私的估计算法来保护敏感数据

Method: 1) 将数据转换为基于边际中位数的双向计数表；2) 对计数添加噪声实现差分隐私；3) 利用真实计数与copula相关性的一一对应关系，通过复合似然估计后验分布；4) 提供最大似然点估计的替代方法

Result: 通过模拟研究比较了所提方法与现有差分隐私copula相关性计算方法，验证了方法的有效性

Conclusion: 提出的算法能够在满足差分隐私要求的同时有效估计高斯copula相关性，为隐私保护的多元统计分析提供了新工具

Abstract: Gaussian copulas are widely used to estimate multivariate distributions and relationships. We present algorithms for estimating Gaussian copula correlations that ensure differential privacy. We first convert data values into sets of two-way tables of counts above and below marginal medians. We then add noise to these counts to satisfy differential privacy. We utilize the one-to-one correspondence between the true counts and the copula correlation to estimate a posterior distribution of the copula correlation given the noisy counts, marginalizing over the distribution of the underlying true counts using a composite likelihood. We also present an alternative, maximum likelihood approach for point estimation. Using simulation studies, we compare these methods to extant methods in the literature for computing differentially private copula correlations.

</details>


### [7] [Small area estimation of dependent extreme value indices](https://arxiv.org/abs/2601.03647)
*Koki Momoki,Takuma Yoshida*

Main category: stat.ME

TL;DR: 提出混合效应模型，利用区域间信息同时预测多个区域的极端值指数，应用于日本强降雨风险评估


<details>
  <summary>Details</summary>
Motivation: 在极值分析中，重尾数据的尾部行为由帕累托型分布建模，其中极端值指数控制尾部行为。对于来自多个子群体或区域的重尾数据，需要利用区域间信息有效预测所有区域的极端值指数

Method: 提出混合效应模型，将区域间极端值指数的差异表示为潜在变量（随机效应），通过相关随机效应纳入区域间关系，实现所有区域极端值指数的同时预测

Result: 建立了参数估计和随机效应预测方法，阐明了估计量的理论性质，数值实验验证了方法的有效性，并应用于日本强降雨风险评估

Conclusion: 提出的混合效应模型能够有效利用区域间信息预测多个区域的极端值指数，为小区域估计提供了有用方法，在极端天气风险评估中具有实际应用价值

Abstract: In extreme value analysis, tail behavior of a heavy-tailed data distribution is modeled by a Pareto-type distribution in which the so-called extreme value index (EVI) controls the tail behavior. For heavy-tailed data obtained from multiple population subgroups, or areas, this study efficiently predicts the EVIs of all areas using information among areas. For this purpose, we propose a mixed effects model, which is a useful approach in small area estimation. In this model, we represent differences among areas in the EVIs by latent variables called random effects. Using correlated random effects across areas, we incorporate the relations among areas into the model. The obtained model achieves simultaneous prediction of EVIs of all areas. Herein, we describe parameter estimation and random effect prediction in the model, and clarify theoretical properties of the estimator. Additionally, numerical experiments are presented to demonstrate the effectiveness of the proposed method. As an application of our model, we provide a risk assessment of heavy rainfall in Japan.

</details>


### [8] [Multi-transport Distributional Regression](https://arxiv.org/abs/2601.03674)
*Yuanying Chen,Tongyu Li,Yang Bai,Zhenhua Lin*

Main category: stat.ME

TL;DR: 提出一种多分布预测变量的内在回归框架，通过Wasserstein空间中的加权Fréchet均值聚合预测变量特定的传输分布，实现可解释的多分布回归


<details>
  <summary>Details</summary>
Motivation: 现实应用中，响应分布通常由多个异质分布源驱动，但由于Wasserstein空间的非线性几何特性，多分布预测变量的回归问题具有挑战性

Method: 提出内在回归框架，通过加权Fréchet均值在Wasserstein空间中聚合预测变量特定的传输分布，模型具有可解释的权重参数，且对辅助构造选择（如参考分布选择）具有不变性

Result: 建立了回归算子的可识别性，在预测Wasserstein半范数下推导了估计的渐近保证，模拟研究和实际应用显示相比现有方法具有更好的预测性能和可解释性

Conclusion: 该方法为多分布预测变量的回归问题提供了有效的解决方案，具有理论保证、良好预测性能和可解释性，优于现有Wasserstein回归方法

Abstract: We study distribution-on-distribution regression problems in which a response distribution depends on multiple distributional predictors. Such settings arise naturally in applications where the outcome distribution is driven by several heterogeneous distributional sources, yet remain challenging due to the nonlinear geometry of the Wasserstein space. We propose an intrinsic regression framework that aggregates predictor-specific transported distributions through a weighted Fréchet mean in the Wasserstein space. The resulting model admits multiple distributional predictors, assigns interpretable weights quantifying their relative contributions, and defines a flexible regression operator that is invariant to auxiliary construction choices, such as the selection of a reference distribution. From a theoretical perspective, we establish identifiability of the induced regression operator and derive asymptotic guarantees for its estimation under a predictive Wasserstein semi-norm, which directly characterizes convergence of the composite prediction map. Extensive simulation studies and a real data application demonstrate the improved predictive performance and interpretability of the proposed approach compared with existing Wasserstein regression methods.

</details>


### [9] [Maximum smoothed likelihood method for the combination of multiple diagnostic tests, with application to the ROC estimation](https://arxiv.org/abs/2601.03675)
*Fangyong Zheng,Pengfei Li,Tao Yu*

Main category: stat.ME

TL;DR: 提出一种基于平滑似然的半参数模型，通过未知单调变换将疾病与健康受试者的密度比与生物标志物线性组合关联，提高多生物标志物诊断分类的估计精度和效率。


<details>
  <summary>Details</summary>
Motivation: 在医学诊断中，使用多个生物标志物比单一生物标志物能显著提高分类准确性。现有基于指数倾斜或密度比模型的方法假设可能过于严格，需要更灵活的模型来适应实际应用。

Method: 采用半参数模型，通过未知单调变换将疾病与健康受试者的密度比与生物标志物线性组合关联。提出平滑似然框架，利用基础密度和变换函数的平滑性提高估计效率。基于最大平滑似然方法构建模型参数和概率密度函数的估计器。

Result: 开发了有效的计算算法，推导了所提估计器的渐近性质，建立了估计ROC曲线和AUC的程序。通过模拟研究和实际数据应用，证明该方法比现有方法产生更准确和高效的估计。

Conclusion: 提出的平滑似然框架在半参数密度比模型中提供了更灵活和高效的估计方法，在多生物标志物医学诊断中表现出优越性能。

Abstract: In medical diagnostics, leveraging multiple biomarkers can significantly improve classification accuracy compared to using a single biomarker. While existing methods based on exponential tilting or density ratio models have shown promise, their assumptions may be overly restrictive in practice. In this paper, we adopt a flexible semiparametric model that relates the density ratio of diseased to healthy subjects through an unknown monotone transformation of a linear combination of biomarkers. To enhance estimation efficiency, we propose a smoothed likelihood framework that exploits the smoothness in the underlying densities and transformation function. Building on the maximum smoothed likelihood methodology, we construct estimators for the model parameters and the associated probability density functions. We develop an effective computational algorithm for implementation, derive asymptotic properties of the proposed estimators, and establish procedures for estimating the receiver operating characteristic (ROC) curve and the area under the curve (AUC). Through simulation studies and a real-data application, we demonstrate that the proposed method yields more accurate and efficient estimates than existing approaches.

</details>


### [10] [Non-Homogeneous Markov-Switching Generalized Additive Models for Location, Scale, and Shape](https://arxiv.org/abs/2601.03760)
*Katharina Ammann,Timo Adam,Jan-Ole Koslik*

Main category: stat.ME

TL;DR: 扩展MS-GAMLSS模型，允许协变量影响状态转移概率，而不仅仅是状态依赖分布的参数，从而捕捉协变量驱动的制度动态变化。


<details>
  <summary>Details</summary>
Motivation: 传统MS-GAMLSS模型假设状态转移概率是时间同质的（常数），这限制了制度转换对协变量变化的响应能力。需要一种方法让制度转换能够灵活地响应协变量驱动变化。

Method: 将状态转移概率建模为协变量的平滑函数，在惩罚似然框架下进行估计，通过自动平滑度选择控制模型复杂度并防止过拟合。

Result: 通过模拟和实际应用（每日汉莎航空股价和西班牙能源价格）验证了方法的有效性。将宏观经济指标纳入转移概率能够提供对市场动态的额外洞察。

Conclusion: 提出的扩展MS-GAMLSS模型能够灵活捕捉协变量依赖的制度动态，为分析协变量驱动的制度转换提供了有效工具，相关数据和R代码已在线公开。

Abstract: We propose an extension of Markov-switching generalized additive models for location, scale, and shape (MS-GAMLSS) that allows covariates to influence not only the parameters of the state-dependent distributions but also the state transition probabilities. Traditional MS-GAMLSS, which combine distributional regression with hidden Markov models, typically assume time-homogeneous (i.e., constant) transition probabilities, thereby preventing regime shifts from responding to covariate-driven changes. Our approach overcomes this limitation by modeling the transition probabilities as smooth functions of covariates, enabling a flexible, data-driven characterization of covariate-dependent regime dynamics. Estimation is carried out within a penalized likelihood framework, where automatic smoothness selection controls model complexity and guards against overfitting. We evaluate the proposed methodology through simulations and applications to daily Lufthansa stock prices and Spanish energy prices. Our results show that incorporating macroeconomic indicators into the transition probabilities yields additional insights into market dynamics. Data and R code to reproduce the results are available online.

</details>


### [11] [Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems](https://arxiv.org/abs/2601.03777)
*Md Nafees Fuad Rafi,Zhaomiao Guo*

Main category: stat.ME

TL;DR: 研究多模式交通系统中旅行者和网约车司机的市场交互，提出统一建模框架分析分散决策对系统效率的影响，并通过数值实验获得政策启示。


<details>
  <summary>Details</summary>
Motivation: 多模式交通系统虽有诸多益处，但涉及多个非合作决策者（旅行者和网约车司机）的自私优化行为，可能损害整体系统效益。需要研究如何通过市场机制协调分散决策。

Method: 提出统一数学建模框架，捕捉分散决策过程，通过均衡定价平衡网络供需。模型可凸化以高效计算均衡网约车价格，并在不同交通网络设置下进行数值实验。

Result: 发现价格敏感时旅行者更偏好网约车和多模式交通；网络中转枢纽较少或司机价格敏感时，可能需要补贴旅行者使用多模式交通；但更多转枢纽会增加网约车空驶里程。

Conclusion: 该模型可为政策制定者和平台运营商设计定价和补贴方案提供工具，使个体决策与系统效率保持一致，并评估多模式交通网络中可达性与环境影响的权衡。

Abstract: While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits. This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems. We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing. Such a model allows analyses of the impact of decentralized decision-making on multimodal mobility efficiencies. The proposed formulation can be further convexified to efficiently compute the equilibrium ride-sourcing prices. We conduct numerical experiments on different settings of transportation networks to gain policy insights. We find that travelers prefer ride-sourcing and multimodal transportation more than the driving option when they are more sensitive to prices. We also find that travelers may need to be subsidized to use multimodal transportation when there is fewer transit hubs in the network or, ride-sourcing drivers become too sensitive to the prices. However, we find that more transit hubs in the network increases the total empty VMT of ride-sourcing drivers by increasing the total relocation time. The proposed model can be used by policymakers and platform operators to design pricing and subsidy schemes that align individual decision-making with system-level efficiency and evaluate the trade-offs between accessibility and environmental impacts in multimodal transportation networks.

</details>


### [12] [High-Dimensional Precision Matrix Quadratic Forms: Estimation Framework for $p > n$](https://arxiv.org/abs/2601.03815)
*Shizhe Hong,Weiming Li,Guangming Pan*

Main category: stat.ME

TL;DR: 提出高维精度矩阵二次泛函估计新框架，解决p>n时传统方法失效问题


<details>
  <summary>Details</summary>
Motivation: 传统矩估计方法在p<n时有效，但在p>n时完全失效，需要解决高维精度矩阵二次泛函的估计问题

Method: 结合谱矩表示与约束优化，在温和矩条件下实现一致性估计

Result: 新框架能有效克服p>n障碍，在投资组合优化和回归分析等应用中表现良好

Conclusion: 提出的统一框架为高维统计推断提供了有效工具，突破了传统方法的维度限制

Abstract: We propose a novel estimation framework for quadratic functionals of precision matrices in high-dimensional settings, particularly in regimes where the feature dimension $p$ exceeds the sample size $n$. Traditional moment-based estimators with bias correction remain consistent when $p<n$ (i.e., $p/n \to c <1$). However, they break down entirely once $p>n$, highlighting a fundamental distinction between the two regimes due to rank deficiency and high-dimensional complexity. Our approach resolves these issues by combining a spectral-moment representation with constrained optimization, resulting in consistent estimation under mild moment conditions.
  The proposed framework provides a unified approach for inference on a broad class of high-dimensional statistical measures. We illustrate its utility through two representative examples: the optimal Sharpe ratio in portfolio optimization and the multiple correlation coefficient in regression analysis. Simulation studies demonstrate that the proposed estimator effectively overcomes the fundamental $p>n$ barrier where conventional methods fail.

</details>


### [13] [Asymptotic distribution of the likelihood ratio test statistic with inequality-constrained nuisance parameters](https://arxiv.org/abs/2601.03909)
*Clara Bertinelli Salucci*

Main category: stat.ME

TL;DR: 论文研究了当参数在边界上时似然比检验统计量的渐近分布，特别关注了当有约束的nuisance参数存在时的复杂情况，提出了新的权重近似方法。


<details>
  <summary>Details</summary>
Motivation: 当参数位于参数空间的边界时，似然比检验统计量的渐近分布已知为卡方混合分布。虽然对于任意数量的边界参数已有充分理解，但当nuisance参数也受边界约束时（这在应用中很常见），分布特性知之甚少。需要解决这一理论缺口。

Method: 1) 分析从K个边界参数到(K-m)个边界参数加m个nuisance参数时锥几何的变化；2) 在正交情况下推导出卡方条权重的闭式差分模式；3) 对于任意协方差结构的一维nuisance向量，分析权重变化的主导成分；4) 针对一般数量的nuisance参数，提出基于秩的内在体积聚合方法来近似混合权重。

Result: 1) 在正交情况下，权重变化呈现闭式差分模式，在相邻自由度间重新分配概率质量；2) 对于任意协方差结构的一维nuisance向量，这种模式仍是权重变化的主导成分；3) 提出的基于秩的内在体积聚合方法能准确近似混合权重；4) 综合模拟验证了理论并展示了所提近似的准确性。

Conclusion: 该研究首次给出了当边界参数和边界nuisance参数数量均为任意时的似然比检验统计量渐近分布的一般特征，揭示了锥几何的变化规律，并提供了有效的权重近似方法，填补了该领域的重要理论空白。

Abstract: The asymptotic distribution of the likelihood-ratio statistic for testing parameters on the boundary is well known to be a chi-squared mixture. The mixture weights have been shown to correspond to the intrinsic volumes of an associated tangent cone, unifying a wide range of previously isolated special cases. While the weights are fully understood for an arbitrary number of parameters of interest on the boundary, much less is known when nuisance parameters are also constrained to the boundary, a situation that frequently arises in applications. We provide the first general characterization of the asymptotic distribution of the likelihood-ratio test statistic when both the number of parameters of interest and the number of nuisance parameters on the boundary are arbitrary. We analyze how the cone geometry changes when moving from a problem with K parameters of interest on the boundary to one with K-m parameters of interest and m nuisances. In the orthogonal case we show that the resulting change in the chi-bar weights admits a closed-form difference pattern that redistributes probability mass across adjacent degrees of freedom, and that this pattern remains the dominant component of the weight shift under arbitrary covariance structures when the nuisance vector is one-dimensional. For a generic number of nuisance parameters, we introduce a new rank-based aggregation of intrinsic volumes that yields an accurate approximation of the mixture weights. Comprehensive simulations support the theory and demonstrate the accuracy of the proposed approximation.

</details>


### [14] [Online robust covariance matrix estimation and outlier detection](https://arxiv.org/abs/2601.03957)
*Paul Guillot,Antoine Godichon-Baggioni,Stéphane Robin,Laure Sansonnet*

Main category: stat.ME

TL;DR: 提出在线协方差矩阵估计和异常值检测的新方法，使用几何中位数和协方差中位数进行稳健估计，实时识别异常值


<details>
  <summary>Details</summary>
Motivation: 传统方法无法在线处理协方差矩阵估计和异常值检测，特别是当异常值比例随数据集增大而增加时，异常值会严重偏倚参数估计并产生掩蔽效应

Method: 提出同时在线估计几何中位数和方差的新方法，使用几何中位数进行位置稳健估计，使用协方差中位数进行离散参数稳健估计，计算马氏距离判断异常值

Result: 在模拟数据集上验证了方法的性能，实现了实时异常值识别

Conclusion: 该方法能够在线处理协方差矩阵估计和异常值检测，有效缓解掩蔽效应，为实时数据流分析提供了解决方案

Abstract: Robust estimation of the covariance matrix and detection of outliers remain major challenges in statistical data analysis, particularly when the proportion of contaminated observations increases with the size of the dataset. Outliers can severely bias parameter estimates and induce a masking effect, whereby some outliers conceal the presence of other outliers, further complicating their detection. Although many approaches have been proposed for covariance estimation and outlier detection, to our knowledge, none of these methods have been implemented in an online setting. In this paper, we focus on online covariance matrix estimation and outlier detection. Specifically, we propose a new method for simultaneously and online estimating the geometric median and variance, which allows us to calculate the Mahalanobis distance for each incoming data point before deciding whether it should be considered an outlier. To mitigate the masking effect, robust estimation techniques for the mean and variance are required. Our approach uses the geometric median for robust estimation of the location and the median covariance matrix for robust estimation of the dispersion parameters. The new online methods proposed for parameter estimation and outlier detection allow real-time identification of outliers as data are observed sequentially. The performance of our methods is demonstrated on simulated datasets.

</details>


### [15] [On the estimation of inclusion probabilities for weighted analyses of nested case control studies](https://arxiv.org/abs/2601.04066)
*Tomeu López-Nieto-Veitch,Rossella De Sabbata,Ryung Kim,Sven Ove Samuelsen,Nathalie C. Støer,Vivian Viallon*

Main category: stat.ME

TL;DR: 该论文研究了巢式病例对照研究中两种主要加权方法（KM权重和GAM权重）的适用性，识别了它们可能产生偏倚的具体场景，并基于DAGs框架提供了权重计算中变量选择的指导原则。


<details>
  <summary>Details</summary>
Motivation: 巢式病例对照研究在流行病学中广泛应用，但现有的加权分析方法（特别是KM权重和GAM权重）在某些情况下可能导致偏倚估计。需要系统性地理解这些偏倚的来源，并为研究人员提供明确的指导，以确保加权分析的准确性和稳健性。

Method: 1. 分析KM权重和GAM权重在三种目标估计量（对数风险比、条件生存率、暴露间关联）下的表现；2. 使用有向无环图（DAGs）开发系统框架，确定权重计算中应包含的变量；3. 通过合成数据和EPIC研究的真实数据验证发现；4. 将GAM权重扩展到"非典型"巢式病例对照研究（仅包含部分病例）。

Result: 1. KM权重在部分原始队列成员实际上不符合NCC选择条件时（特别是病例比例小或匹配因素多时）可能导致偏倚；2. GAM权重在匹配因素间的交互作用影响疾病风险且未充分纳入权重计算时可能产生偏倚；3. 权重计算中的最优变量集取决于目标估计量以及匹配因素、暴露和疾病风险之间的因果关系；4. GAM权重可成功应用于仅包含部分病例的"非典型"NCC研究。

Conclusion: 该研究为巢式病例对照研究中的加权分析提供了重要指导：需要根据具体研究场景选择适当的加权方法，并基于DAGs框架系统确定权重计算中应包含的变量。研究强调了考虑匹配因素与疾病风险之间复杂关系的重要性，特别是在存在交互作用或部分队列成员不符合选择条件的情况下。

Abstract: Nested case-control (NCC) studies are a widely adopted design in epidemiology to investigate exposure-disease relationships. This paper examines weighted analyses in NCC studies, focusing on two prominent weighting methods: Kaplan-Meier (KM) weights and Generalized Additive Model (GAM) weights. We consider three target estimands: log-hazard ratios, conditional survival, and associations between exposures. While KM- and GAM-weights are generally robust, we identify specific scenarios where they can lead to biased estimates. We demonstrate that KM-weights can lead to biased estimates when a proportion of the originating cohort is effectively ineligible for NCC selection, particularly with small case proportions or numerous matching factors. Instead, GAM-weights can yield biased results if interactions between matching factors influence disease risk and are not adequately incorporated into weight calculation. Using Directed Acyclic Graphs (DAGs), we develop a framework to systematically determine which variables should be included in weight calculations. We show that the optimal set of variables depends on the target estimand and the causal relationships between matching factors, exposures, and disease risk. We illustrate our findings with both synthetic and real data from the European Prospective Investigation into Cancer and nutrition (EPIC) study. Additionally, we extend the application of GAM-weights to "untypical" NCC studies, where only a subset of cases are included. Our work provides crucial insights for conducting accurate and robust weighted analyses in NCC studies.

</details>


### [16] [Prediction Intervals for Interim Events in Randomized Clinical Trials with Time-to-Event Endpoints](https://arxiv.org/abs/2601.04192)
*Edoardo Ratti,Federico L. Perlino,Stefania Galimberti,Maria G. Valsecchi*

Main category: stat.ME

TL;DR: 提出一个预测临床试验中未来事件数量的预测区间框架，特别适用于时间-事件终点的期中监测


<details>
  <summary>Details</summary>
Motivation: 时间-事件终点的临床试验期中分析规划具有挑战性，因为统计信息取决于观察到的事件数量。目前缺乏预测未来日期事件数量及其预测区间的成熟框架

Method: 从可靠性工程中的组件失效预测区间方法出发，重新表述并扩展到临床试验期中监测背景。采用频率主义框架，通过未来事件条件分布的bootstrap估计获得预测区间，支持参数生存模型、患者协变量、分期入组等复杂情况

Result: 通过模拟研究和真实世界儿童急性淋巴细胞白血病III期试验分析验证了所提方法的性能

Conclusion: 建立了一个通用的临床事件计数预测区间框架，为时间-事件终点临床试验的期中监测提供了实用的预测工具

Abstract: Time-to-event endpoints are central to evaluate treatment efficacy across many disease areas. Many trial protocols include interim analyses within group-sequential designs that control type I error via spending functions or boundary methods. The corresponding operating characteristics depend on the number of looks and the information accrued. Planning interim analyses with time-to-event endpoints is challenging because statistical information depends on the number of observed events. Ensuring adequate follow-up to accrue the required events is therefore critical, making interim prediction of information at scheduled looks and at the final analysis essential. While several methods have been developed to predict the calendar time required to reach a target number of events, to the best of our knowledge there is no established framework that addresses the prediction of the number of events at a future date with corresponding prediction intervals. Starting from an prediction interval approach originally developed in reliability engineering for the number of future component failures, we reformulated and extended it to the context of interim monitoring in clinical trials. This adaptation yields a general framework for event-count prediction intervals in the clinical setting, taking the patient as the unit of analysis and accommodating a range of parametric survival models, patient-level covariates, stagged entry and possible dependence between entry dates and lost to follow-up. Prediction intervals are obtained in a frequentist framework from a bootstrap estimator of the conditional distribution of future events. The performance of the proposed approach is investigated via simulation studies and illustrated by analyzing a real-world phase III trial in childhood acute lymphoblastic leukaemia.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [17] [On the Distributed Estimation for Scalar-on-Function Regression Models](https://arxiv.org/abs/2601.04138)
*Peilun He,Han Lin Shang,Nan Zou*

Main category: stat.CO

TL;DR: 提出分布式估计方法用于三种标量-函数回归模型，解决大数据计算成本和数据共享限制问题，在保持精度的同时显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 解决函数数据分析中的两个关键挑战：大样本的高计算成本以及跨机构共享原始数据的限制。

Method: 为三种标量-函数回归模型（函数线性模型FLM、函数非参数模型FNPM、函数部分线性模型FPLM）提出分布式估计程序。

Result: 分布式估计器显著减少计算时间，同时保持三种模型的高估计和预测精度；但当块大小过小时，FPLM会出现过拟合，导致预测区间变窄和经验覆盖概率降低。

Conclusion: 分布式方法能有效处理大规模函数数据，在计算效率和统计精度之间取得良好平衡，但需要注意块大小选择以避免过拟合问题。

Abstract: This paper proposes distributed estimation procedures for three scalar-on-function regression models: the functional linear model (FLM), the functional non-parametric model (FNPM), and the functional partial linear model (FPLM). The framework addresses two key challenges in functional data analysis, namely the high computational cost of large samples and limitations on sharing raw data across institutions. Monte Carlo simulations show that the distributed estimators substantially reduce computation time while preserving high estimation and prediction accuracy for all three models. When block sizes become too small, the FPLM exhibits overfitting, leading to narrower prediction intervals and reduced empirical coverage probability. An example of an empirical study using the \textit{tecator} dataset further supports these findings.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [18] [On the Identifiability of Regime-Switching Models with Multi-Lag Dependencies](https://arxiv.org/abs/2601.03325)
*Carles Balsells-Rodas,Toshiko Matsui,Pedro A. M. Mediano,Yixin Wang,Yingzhen Li*

Main category: stat.ML

TL;DR: 该论文为深度隐变量模型中的可识别性研究提供了理论框架，特别针对多滞后机制切换时间序列模型，证明了在非线性高斯设置下机制数量和转移概率的可识别性，并开发了满足理论假设的变分估计器。


<details>
  <summary>Details</summary>
Motivation: 可识别性是深度隐变量模型可解释性的核心，确保参数化由数据生成分布唯一确定。然而，对于深度机制切换时间序列模型的可识别性研究仍然不足，这限制了这些模型在科学发现中的可信度。

Method: 1. 为多滞后机制切换模型（RSMs）开发通用理论框架，包括马尔可夫切换模型（MSMs）和切换动态系统（SDSs）；2. 将MSMs表述为时间结构化的有限混合模型，证明非线性高斯设置下机制数量和转移概率的可识别性；3. 通过时间结构建立SDSs隐变量的可识别性（至排列和缩放），进而推导机制相关隐因果图的可识别性条件；4. 开发满足理论假设的灵活变分估计器。

Result: 1. 证明了MSMs中机制数量和转移概率的可识别性；2. 建立了SDSs隐变量可识别性条件（至排列和缩放）；3. 推导了机制相关隐因果图的可识别性条件（至机制/节点排列）；4. 在合成基准测试上验证了理论结果；5. 在神经科学、金融和气候等真实数据集上，可识别性带来了更可信的可解释性分析。

Conclusion: 该研究为深度机制切换时间序列模型的可识别性提供了坚实的理论基础，通过神经网络设计可直接实施的架构和噪声假设，在完全无监督设置下实现了可识别性。这为科学发现中的可信可解释性分析提供了关键支撑。

Abstract: Identifiability is central to the interpretability of deep latent variable models, ensuring parameterisations are uniquely determined by the data-generating distribution. However, it remains underexplored for deep regime-switching time series. We develop a general theoretical framework for multi-lag Regime-Switching Models (RSMs), encompassing Markov Switching Models (MSMs) and Switching Dynamical Systems (SDSs). For MSMs, we formulate the model as a temporally structured finite mixture and prove identifiability of both the number of regimes and the multi-lag transitions in a nonlinear-Gaussian setting. For SDSs, we establish identifiability of the latent variables up to permutation and scaling via temporal structure, which in turn yields conditions for identifiability of regime-dependent latent causal graphs (up to regime/node permutations). Our results hold in a fully unsupervised setting through architectural and noise assumptions that are directly enforceable via neural network design. We complement the theory with a flexible variational estimator that satisfies the assumptions and validate the results on synthetic benchmarks. Across real-world datasets from neuroscience, finance, and climate, identifiability leads to more trustworthy interpretability analysis, which is crucial for scientific discovery.

</details>


### [19] [Microeconomic Foundations of Multi-Agent Learning](https://arxiv.org/abs/2601.03451)
*Nassim Helou*

Main category: stat.ML

TL;DR: 该论文提出了一种两阶段激励机制，用于在多智能体学习环境中实现渐进最优的社会福利，通过估计可实施的转移支付来引导长期动态。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统越来越多地在市场和制度中运行，其中数据、行为和激励都是内生的。需要为多智能体学习建立经济基础，特别是在存在战略外部性的主-代理交互中，双方都在随时间学习。

Method: 提出两阶段激励机制：第一阶段估计可实施的转移支付，第二阶段使用这些转移支付来引导长期动态。在温和的基于遗憾的理性和探索条件下，该机制能够实现次线性的社会福利遗憾。

Result: 该机制实现了次线性的社会福利遗憾，从而获得渐进最优的社会福利。模拟表明，即使是粗略的激励也能在存在状态外部性的情况下纠正低效学习。

Conclusion: 激励感知设计对于市场和保险中安全且福利对齐的AI至关重要。该研究为多智能体学习提供了经济基础，展示了激励机制如何纠正学习过程中的低效性。

Abstract: Modern AI systems increasingly operate inside markets and institutions where data, behavior, and incentives are endogenous. This paper develops an economic foundation for multi-agent learning by studying a principal-agent interaction in a Markov decision process with strategic externalities, where both the principal and the agent learn over time. We propose a two-phase incentive mechanism that first estimates implementable transfers and then uses them to steer long-run dynamics; under mild regret-based rationality and exploration conditions, the mechanism achieves sublinear social-welfare regret and thus asymptotically optimal welfare. Simulations illustrate how even coarse incentives can correct inefficient learning under stateful externalities, highlighting the necessity of incentive-aware design for safe and welfare-aligned AI in markets and insurance.

</details>


### [20] [Online Learning with Limited Information in the Sliding Window Model](https://arxiv.org/abs/2601.03533)
*Vladimir Braverman,Sumegha Garg,Chen Wang,David P. Woodruff,Samson Zhou*

Main category: stat.ML

TL;DR: 论文研究了滑动窗口模型中的专家问题，通过仅使用2次查询和polylog(nT)内存实现了接近最优的遗憾界，并将结果扩展到区间遗憾和老虎机问题。


<details>
  <summary>Details</summary>
Motivation: 受流模型中专家问题研究的启发，本文考虑滑动窗口模型中的专家问题。滑动窗口模型适用于交通监控、疫情追踪、自动交易等应用，其中近期信息比旧数据更有价值。

Method: 提出在滑动窗口模型中使用2次查询和polylog(nT)内存的算法，能够实现接近最优的遗憾界。基于这些技术，进一步解决了数据流中的老虎机问题。

Result: 在滑动窗口模型中，使用2次查询和polylog(nT)内存实现了√(nW)polylog(nT)遗憾；对于任意区间I，实现了√(n|I|)polylog(nT)遗憾；在老虎机设置中，使用polylog(nT)内存实现了nT^{2/3}polylog(T)遗憾，如果最佳专家的损失是随机顺序，则可进一步优化到最优的O(√(nT))遗憾。

Conclusion: 本文在滑动窗口模型中实现了接近最优的专家问题算法，显著改进了先前区间遗憾算法的内存需求，并在老虎机设置中首次实现了具有polylog内存的流模型次线性遗憾。

Abstract: Motivated by recent work on the experts problem in the streaming model, we consider the experts problem in the sliding window model. The sliding window model is a well-studied model that captures applications such as traffic monitoring, epidemic tracking, and automated trading, where recent information is more valuable than older data. Formally, we have $n$ experts, $T$ days, the ability to query the predictions of $q$ experts on each day, a limited amount of memory, and should achieve the (near-)optimal regret $\sqrt{nW}\text{polylog}(nT)$ regret over any window of the last $W$ days. While it is impossible to achieve such regret with $1$ query, we show that with $2$ queries we can achieve such regret and with only $\text{polylog}(nT)$ bits of memory. Not only are our algorithms optimal for sliding windows, but we also show for every interval $\mathcal{I}$ of days that we achieve $\sqrt{n|\mathcal{I}|}\text{polylog}(nT)$ regret with $2$ queries and only $\text{polylog}(nT)$ bits of memory, providing an exponential improvement on the memory of previous interval regret algorithms. Building upon these techniques, we address the bandit problem in data streams, where $q=1$, achieving $n T^{2/3}\text{polylog}(T)$ regret with $\text{polylog}(nT)$ memory, which is the first sublinear regret in the streaming model in the bandit setting with polylogarithmic memory; this can be further improved to the optimal $\mathcal{O}(\sqrt{nT})$ regret if the best expert's losses are in a random order.

</details>


### [21] [A Theoretical and Empirical Taxonomy of Imbalance in Binary Classification](https://arxiv.org/abs/2601.04149)
*Rose Yvette Bandolo Essomba,Ernest Fokoué*

Main category: stat.ML

TL;DR: 提出基于三个基本尺度（不平衡系数η、样本-维度比κ、内在可分性Δ）的理论框架，分析类别不平衡对分类性能的影响，预测四种退化机制，并通过实验验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡显著降低分类性能，但缺乏统一的理论分析视角。需要从基本原理出发，理解不平衡如何影响分类器性能。

Method: 基于高斯贝叶斯分类器，推导闭式贝叶斯误差，分析不平衡如何改变判别边界，得到预测四种退化机制的退化斜率。使用平衡的高维基因组数据集，仅改变η而固定κ和Δ，在参数和非参数模型中验证理论预测。

Result: 经验退化与理论预测高度一致：当log(η)超过Δ√κ时，少数类召回率崩溃；精确率非对称增加；F1分数和PR-AUC下降符合预测的四种机制（正常、轻度、极端、灾难性）。

Conclusion: 三元组(η,κ,Δ)提供了模型无关、几何基础的解释不平衡引起的性能退化，为理解类别不平衡效应提供了统一的理论框架。

Abstract: Class imbalance significantly degrades classification performance, yet its effects are rarely analyzed from a unified theoretical perspective. We propose a principled framework based on three fundamental scales: the imbalance coefficient $η$, the sample--dimension ratio $κ$, and the intrinsic separability $Δ$. Starting from the Gaussian Bayes classifier, we derive closed-form Bayes errors and show how imbalance shifts the discriminant boundary, yielding a deterioration slope that predicts four regimes: Normal, Mild, Extreme, and Catastrophic. Using a balanced high-dimensional genomic dataset, we vary only $η$ while keeping $κ$ and $Δ$ fixed. Across parametric and non-parametric models, empirical degradation closely follows theoretical predictions: minority Recall collapses once $\log(η)$ exceeds $Δ\sqrtκ$, Precision increases asymmetrically, and F1-score and PR-AUC decline in line with the predicted regimes. These results show that the triplet $(η,κ,Δ)$ provides a model-agnostic, geometrically grounded explanation of imbalance-induced deterioration.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [22] [pintervals: an R package for model-agnostic prediction intervals](https://arxiv.org/abs/2601.03994)
*David Randahl,Anders Hjort,Jonathan P. Williams*

Main category: stat.AP

TL;DR: pintervals包提供了一个统一的模型无关框架，用于构建预测区间和校准预测，支持多种预测区间构建方法。


<details>
  <summary>Details</summary>
Motivation: 现有R包和函数通常专注于特定建模框架或预测类型，或需要针对不同模型进行手动定制，缺乏统一的模型无关解决方案。

Method: 使用留出校准数据，提供一致接口支持多种预测区间构建方法：conformal预测区间、参数化方法和bootstrap方法，适用于任何输出点预测的模型。

Result: pintervals包允许研究人员在不同建模框架和应用中应用和比较多种预测区间构建方法，提供统一的模型无关解决方案。

Conclusion: pintervals包填补了R生态系统中模型无关预测区间构建工具的空白，为研究人员提供了灵活、一致的框架来评估和比较不同预测区间方法。

Abstract: The \pkg{pintervals} package aims to provide a unified framework for constructing prediction intervals and calibrating predictions in a model-agnostic setting using set-aside calibration data. It comprises routines to construct conformal as well as parametric and bootstrapped prediction intervals from any model that outputs point predictions. Several R packages and functions already exist for constructing prediction intervals, but they often focus on specific modeling frameworks or types of predictions, or require manual customization for different models or applications. By providing a consistent interface for a variety of prediction interval construction approaches (all model-agnostic), \pkg{pintervals} allows researchers to apply and compare them across different modeling frameworks and applications.

</details>
