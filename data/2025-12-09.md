<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 13]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.ML](#stat.ML) [Total: 7]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Generalised Bayesian Inference using Robust divergences for von Mises-Fisher distribution](https://arxiv.org/abs/2512.05668)
*Tomoyuki Nakagawa,Yasuhito Tsuruta,Sho Kazari,Kouji Tahata*

Main category: stat.ME

TL;DR: 本文提出基于密度幂散度和γ散度的贝叶斯鲁棒估计方法，用于处理von Mises-Fisher分布中异常值影响的参数估计问题。


<details>
  <summary>Details</summary>
Motivation: 方向数据在生物信息学和文本分析等领域日益重要，但异常值会严重影响估计结果。虽然频率学派已有许多鲁棒估计方法，但贝叶斯框架下的鲁棒估计方法仍然很少，需要开发能够处理异常值的贝叶斯方法。

Method: 提出基于密度幂散度和γ散度的贝叶斯推断方法，建立其渐近性质和鲁棒性。开发基于加权贝叶斯自举的后验计算算法进行参数估计。

Result: 通过模拟研究验证了所提方法的有效性。使用两个真实数据集进一步表明，即使在存在异常值或数据污染的情况下，所提方法也能提供可靠且鲁棒的估计。

Conclusion: 本文提出的贝叶斯鲁棒估计方法能够有效处理von Mises-Fisher分布中的异常值问题，为小样本情况下的不确定性评估提供了自然途径，具有实际应用价值。

Abstract: This paper focusses on robust estimation of location and concentration parameters of the von Mises-Fisher distribution in the Bayesian framework. The von Mises-Fisher (or Langevin) distribution has played a central role in directional statistics. Directional data have been investigated for many decades, and more recently, they have gained increasing attention in diverse areas such as bioinformatics and text data analysis. Although outliers can significantly affect the estimation results even for directional data, the treatment of outliers remains an unresolved and challenging problem. In the frequentist framework, numerous studies have developed robust estimation methods for directional data with outliers, but, in contrast, only a few robust estimation methods have been proposed in the Bayesian framework. In this paper, we propose Bayesian inference based on density power-divergence and $γ$-divergence and establish their asymptotic properties and robustness. In addition, the Bayesian approach naturally provides a way to assess estimation uncertainty through the posterior distribution, which is particularly useful for small samples. Furthermore, to carry out the posterior computation, we develop the posterior computation algorithm based on the weighted Bayesian bootstrap for estimating parameters. The effectiveness of the proposed methods is demonstrated through simulation studies. Using two real datasets, we further show that the proposed method provides reliable and robust estimation even in the presence of outliers or data contamination.

</details>


### [2] [The Bayesian Way: Uncertainty, Learning, and Statistical Reasoning](https://arxiv.org/abs/2512.05883)
*Juan Sosa,Carlos A. Martínez,Danna Cruz*

Main category: stat.ME

TL;DR: 这篇论文提供了贝叶斯推断的全面介绍，涵盖历史背景、理论基础和核心分析示例，为统计实践中的贝叶斯方法应用提供入门指南。


<details>
  <summary>Details</summary>
Motivation: 为寻求在统计实践中采用贝叶斯视角的学生和研究人员提供一个严谨而易于理解的切入点，帮助他们理解贝叶斯推断的基本原理和应用。

Method: 从贝叶斯定理出发，结合历史背景和哲学基础，通过经典模型展示先验信息与观测数据的整合，涵盖估计、区间构造、假设检验和预测等推断框架。

Result: 建立了贝叶斯推断的完整理论框架，展示了如何通过后验分布进行统计推断，并探讨了损失函数、可信区间、贝叶斯因子、可识别性和渐近行为等关键概念。

Conclusion: 虽然侧重于经典设置中的解析可处理性，但为现代扩展（如基于模拟的方法）奠定了基础，并为贝叶斯方法在分层建模、非参数统计、时间序列、空间数据等当代领域的应用铺平了道路。

Abstract: This paper offers a comprehensive introduction to Bayesian inference, combining historical context, theoretical foundations, and core analytical examples. Beginning with Bayes' theorem and the philosophical distinctions between Bayesian and frequentist approaches, we develop the inferential framework for estimation, interval construction, hypothesis testing, and prediction. Through canonical models, we illustrate how prior information and observed data are formally integrated to yield posterior distributions. We also explore key concepts including loss functions, credible intervals, Bayes factors, identifiability, and asymptotic behavior. While emphasizing analytical tractability in classical settings, we outline modern extensions that rely on simulation-based methods and discuss challenges related to prior specification and model evaluation. Though focused on foundational ideas, this paper sets the stage for applying Bayesian methods in contemporary domains such as hierarchical modeling, nonparametrics, and structured applications in time series, spatial data, networks, and political science. The goal is to provide a rigorous yet accessible entry point for students and researchers seeking to adopt a Bayesian perspective in statistical practice.

</details>


### [3] [A Note on the Finite Sample Bias in Time Series Cross-Validation](https://arxiv.org/abs/2512.05900)
*Amaze Lusompa*

Main category: stat.ME

TL;DR: 交叉验证在时间序列模型选择中存在偏差，即使对于VAR模型或具有鞅结构误差的模型也是如此


<details>
  <summary>Details</summary>
Motivation: 虽然已知交叉验证在时间序列模型选择中存在偏差，但许多研究者认为这种偏差不适用于VAR模型或具有鞅结构误差的时间序列模型。本文旨在证明即使在这些情况下，交叉验证仍然会产生偏差。

Method: 通过理论分析证明交叉验证在时间序列模型选择中的偏差问题，特别针对VAR模型和具有鞅结构误差的模型

Result: 即使在向量自回归(VAR)模型或具有鞅结构误差的时间序列模型中，使用交叉验证进行模型选择仍然会产生偏差

Conclusion: 交叉验证在时间序列模型选择中存在普遍偏差，需要谨慎使用，即使对于VAR模型或具有鞅结构误差的模型也不例外

Abstract: It is well known that model selection via cross validation can be biased for time series models. However, many researchers have argued that this bias does not apply when using cross-validation with vector autoregressions (VAR) or with time series models whose errors follow a martingale-like structure. I show that even under these circumstances, performing cross-validation on time series data will still generate bias in general.

</details>


### [4] [Exchangeable Gaussian Processes with application to epidemics](https://arxiv.org/abs/2512.05227)
*Lampros Bouranis,Petros Barmpounakis,Nikolaos Demiris,Konstantinos Kalogeropoulos*

Main category: stat.ME

TL;DR: 提出基于多任务高斯过程的贝叶斯非参数框架，用于传染病负担的时间收缩建模，在疫情数据分析中表现出优于竞争方法的预测能力


<details>
  <summary>Details</summary>
Motivation: 开发适用于时间收缩的统计框架，获取传染病负担的证据知识，通过动态分层模型捕捉群体间的交叉依赖关系

Method: 基于多任务高斯过程的贝叶斯非参数框架，使用动态分层模型，以底层均值过程（高斯过程）表达，实现群体间交叉依赖的简约建模

Result: 分析近期流行病的不同类型爆发数据，发现所提模型相比竞争方法具有改进的预测能力

Conclusion: 基于多任务高斯过程的贝叶斯非参数框架为传染病负担的时间收缩建模提供了有效方法，在疫情数据分析中展现出优越的预测性能

Abstract: We develop a Bayesian non-parametric framework based on multi-task Gaussian processes, appropriate for temporal shrinkage. We focus on a particular class of dynamic hierarchical models to obtain evidence-based knowledge of infectious disease burden. These models induce a parsimonious way to capture cross-dependence between groups while retaining a natural interpretation based on an underlying mean process, itself expressed as a Gaussian process. We analyse distinct types of outbreak data from recent epidemics and find that the proposed models result in improved predictive ability against competing alternatives.

</details>


### [5] [Efficient sequential Bayesian inference for state-space epidemic models using ensemble data assimilation](https://arxiv.org/abs/2512.05650)
*Dhorasso Temfack,Jason Wyse*

Main category: stat.ME

TL;DR: 提出Ensemble SMC² (eSMC²)方法，用Ensemble Kalman Filter替代内部粒子滤波器，显著降低计算成本，适用于传染病监测数据的实时贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 传统SMC²算法虽然提供了状态和参数联合推断的理论框架，但其嵌套粒子滤波器计算成本过高，限制了在实时疫情响应中的应用。需要开发更高效的算法来处理传染病监测中的部分观测、噪声数据。

Method: 提出Ensemble SMC² (eSMC²)方法，用Ensemble Kalman Filter替代内部粒子滤波器来近似每个观测时间点的增量似然。通过无偏高斯密度估计器减轻有限样本效应，并针对流行病数据调整EnKF的状态依赖观测方差。

Result: 模拟实验和2022年美国猴痘发病率数据应用表明，eSMC²在保持与SMC²相当的后验估计质量的同时，实现了显著的计算效率提升。能够准确恢复潜在流行病轨迹和关键流行病学参数。

Conclusion: eSMC²为从不完美监测数据中进行序列贝叶斯推断提供了一个高效框架，特别适用于传染病监测中常见的过度分散发病率数据，平衡了计算效率和推断准确性。

Abstract: Estimating latent epidemic states and model parameters from partially observed, noisy data remains a major challenge in infectious disease modeling. State-space formulations provide a coherent probabilistic framework for such inference, yet fully Bayesian estimation is often computationally prohibitive because evaluating the observed-data likelihood requires integration over all latent trajectories. The Sequential Monte Carlo squared (SMC$^2$) algorithm offers a principled approach for joint state and parameter inference, combining an outer SMC sampler over parameters with an inner particle filter that estimates the likelihood up to the current time point. Despite its theoretical appeal, this nested particle filter imposes substantial computational cost, limiting routine use in near-real-time outbreak response. We propose Ensemble SMC$^2$ (eSMC$^2$), a scalable variant that replaces the inner particle filter with an Ensemble Kalman Filter (EnKF) to approximate the incremental likelihood at each observation time. While this substitution introduces bias via a Gaussian approximation, we mitigate finite-sample effects using an unbiased Gaussian density estimator and adapt the EnKF for epidemic data through state-dependent observation variance. This makes our approach particularly suitable for overdispersed incidence data commonly encountered in infectious disease surveillance. Simulation experiments with known ground truth and an application to 2022 United States (U.S.) monkeypox incidence data demonstrate that eSMC$^2$ achieves substantial computational gains while producing posterior estimates comparable to SMC$^2$. The method accurately recovers latent epidemic trajectories and key epidemiological parameters, providing an efficient framework for sequential Bayesian inference from imperfect surveillance data.

</details>


### [6] [A Functional Approach to Testing Overall Effect of Interaction Between DNA Methylation and SNPs](https://arxiv.org/abs/2512.05276)
*Yvelin Gansou,Karim Oualkacha,Marzia Angela Cremona,Lajmi Lakhal-Chaieb*

Main category: stat.ME

TL;DR: 提出一种测试DNA甲基化与SNPs交互作用对数量性状整体效应的统计方法，基于功能数据分析框架，在肥胖患者数据中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 需要一种统计方法来测试DNA甲基化与单核苷酸多态性(SNPs)之间的交互作用对数量性状的整体效应，现有方法在存在多个交互作用时可能不够有效。

Method: 基于功能数据分析框架，扩展现有回归模型，开发了一种推断程序来测试甲基化-SNP交互作用的整体效应。

Result: 模拟研究表明，该方法能有效控制I类错误率，并在存在多个交互作用时比现有方法具有更高的经验功效。

Conclusion: 提出的测试方法能有效检测甲基化-SNP交互作用对数量性状的整体效应，在肥胖患者数据分析中展示了实际应用价值。

Abstract: We introduce a test for the overall effect of interaction between DNA methylation and a set of single nucleotide polymorphisms (SNPs) on a quantitative phenotype. The developed inference procedure is based on a functional approach that extends existing regression models in functional data analysis. Through extensive simulations, we show that the proposed test effectively controls type I error rates and highlights increased empirical power over existing methods, particularly when multiple interactions are present. The use of the proposed test is illustrated with an application to data from obesity patients and controls.

</details>


### [7] [Designing an Optimal Sensor Network via Minimizing Information Loss](https://arxiv.org/abs/2512.05940)
*Daniel Waxman,Fernando Llorente,Katia Lamer,Petar M. Djurić*

Main category: stat.ME

TL;DR: 提出一种基于物理模拟和贝叶斯实验设计的传感器布置优化方法，用于监测时空过程，在有限传感器数量下显著优于随机采样


<details>
  <summary>Details</summary>
Motivation: 现有实验设计方法很少利用计算科学中产生的大规模物理模拟数据，需要开发能够整合物理模拟和贝叶斯实验设计原则的传感器布置方法

Method: 提出基于稀疏变分推理和可分离高斯-马尔可夫先验的模型驱动传感器布置准则，结合物理模拟和贝叶斯实验设计原理，开发高效优化算法

Result: 在亚利桑那州凤凰城气温监测案例中，该方法明显优于随机或准随机采样，特别是在传感器数量有限的情况下表现更佳

Conclusion: 该框架为复杂建模工具和实际部署提供了实用考虑和启示，展示了整合物理模拟数据在实验设计中的价值

Abstract: Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that "minimize information loss" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.

</details>


### [8] [Does Rerandomization Help Beyond Covariate Adjustment? A Review and Guide for Theory and Practice](https://arxiv.org/abs/2512.05290)
*Antônio Carlos Herling Ribeiro Junior,Zach Branson*

Main category: stat.ME

TL;DR: 重随机化是一种通过重复随机化处理分配直到协变量平衡的实验设计技术，能提高因果效应估计的精度和一致性，减少p-hacking导致的虚假发现，增加统计功效。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明重随机化不会改变协变量调整估计量的渐近精度，但这些结果存在两个关键局限：一是基于渐近理论，有限样本性能未知；二是只关注精度，而其他潜在优势（如提高灵活估计量的一致性）研究不足。因此需要全面评估重随机化的实际价值。

Method: 本文提供三个主要贡献：(1) 对重随机化文献的全面综述，涵盖历史基础、理论发展和最新方法进展；(2) 广泛的模拟研究检验有限样本性能；(3) 为实践者提供实用指南。研究比较了重随机化与完全随机化下各种估计量的精度、一致性、功效和覆盖率。

Result: 研究发现重随机化是一种互补的设计策略，能提高因果效应估计量的精度、稳健性和可靠性，特别是在样本量较小的情况下。重随机化增强了估计量的一致性，减少了p-hacking的风险，并提高了统计功效。

Conclusion: 重随机化作为实验设计的有价值工具，尤其适用于有限样本情况。它通过平衡协变量提高了因果推断的质量，是提高研究可靠性和可重复性的重要策略，值得在实践中推广应用。

Abstract: Rerandomization is a modern experimental design technique that repeatedly randomizes treatment assignments until covariates are deemed balanced between treatment groups. This enhances the precision and coherence of causal effect estimators, mitigates false discoveries from p-hacking, and increases statistical power. Recent work suggests that balancing covariates via rerandomization does not alter the asymptotic precision of covariate-adjusted estimators, thereby making it unclear whether rerandomization is worthwhile if adjusted estimators are used. However, these results have two key caveats. First, these results are asymptotic, leaving finite sample performance unknown. Second, these results focus on precision, while other potential benefits, such as increased coherence among flexible estimators, remain understudied. Hence, in this paper we provide three main contributions: (i) a comprehensive review of the rerandomization literature, covering historical foundations, theoretical developments, and recent methodological advancements, (ii) an extensive simulation study examining finite-sample performance, and (iii) a practical guide for practitioners. Our study compares precision, coherence, power, and coverage of various estimators under rerandomization versus complete randomization. We find rerandomization to be a complementary design strategy that enhances the precision, robustness, and reliability of causal effect estimators, especially for smaller sample sizes.

</details>


### [9] [Identifiability and improper solutions in the probabilistic partial least squares regression with unique variance](https://arxiv.org/abs/2512.05328)
*Takashi Arai*

Main category: stat.ME

TL;DR: 本文解决了概率偏最小二乘回归的理论问题，通过引入因子分析中的范数约束方法，解决了模型不可识别和不当解的问题，并在HIV蛋白酶数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 概率偏最小二乘回归存在与因子分析类似的问题：不当解和不可识别性，这导致潜在变量和模型参数难以解释。需要找到一种方法来确保模型的可识别性并避免不当解。

Method: 利用概率偏最小二乘回归可视为因子分析特例的事实，应用因子分析中最近提出的因子载荷矩阵范数约束方法，将其应用于概率偏最小二乘回归。通过理论证明和数值实验验证该约束的有效性。

Result: 证明了带范数约束的概率偏最小二乘回归是可识别的。在HIV蛋白酶氨基酸突变数据上验证了约束的有效性，实现了潜在变量的可视化。通过合成数据观察到最大似然估计具有一致性和渐近正态性。

Conclusion: 范数约束方法成功解决了概率偏最小二乘回归的不可识别性问题，避免了不当解，使模型参数可解释，潜在变量可可视化，为实际应用提供了理论保证。

Abstract: This paper addresses theoretical issues associated with probabilistic partial least squares (PLS) regression. As in the case of factor analysis, the probabilistic PLS regression with unique variance suffers from the issues of improper solutions and lack of identifiability, both of which causes difficulties in interpreting latent variables and model parameters. Using the fact that the probabilistic PLS regression can be viewed as a special case of factor analysis, we apply a norm constraint prescription on the factor loading matrix in the probabilistic PLS regression, which was recently proposed in the context of factor analysis to avoid improper solutions. Then, we prove that the probabilistic PLS regression with this norm constraint is identifiable. We apply the probabilistic PLS regression to data on amino acid mutations in Human Immunodeficiency Virus (HIV) protease to demonstrate the validity of the norm constraint and to confirm the identifiability numerically. Utilizing the proposed constraint enables the visualization of latent variables via a biplot. We also investigate the sampling distribution of the maximum likelihood estimates (MLE) using synthetically generated data. We numerically observe that MLE is consistent and asymptotically normally distributed.

</details>


### [10] [Empirical Decision Theory](https://arxiv.org/abs/2512.05677)
*Christoph Jansen,Georg Schollmeyer,Thomas Augustin,Julian Rodemann*

Main category: stat.ME

TL;DR: 提出一种无需明确指定世界状态的决策模型，通过观察行动-结果对协议进行经验分析，提供三种推断保证方法


<details>
  <summary>Details</summary>
Motivation: 传统决策理论依赖封闭世界和小世界等理想化假设，需要明确指定世界状态，但在现实决策中这些状态往往不可访问，需要一种更经验化的方法

Method: 基于协议的经验决策方法：不预先指定状态和结果，仅以观察到的行动-结果对作为模型基础，使用协议经验选择函数处理最优性，提供三种推断保证方法

Result: 开发了无需明确状态规范的决策框架，提出了三种推断保证方法：选择集的一致统计估计、具有鲁棒性的选择函数统计检验、使用信度集的直接推断

Conclusion: 该模型保留了经典决策理论的吸引力，同时完全克服了明确指定世界状态的需求，为生成式AI提示策略比较等应用提供了概念验证

Abstract: Analyzing decision problems under uncertainty commonly relies on idealizing assumptions about the describability of the world, with the most prominent examples being the closed world and the small world assumption. Most assumptions are operationalized by introducing states of the world, conditional on which the decision situation can be analyzed without any remaining uncertainty. Conversely, most classical decision-theoretic approaches are not applicable if the states of the world are inaccessible. We propose a decision model that retains the appeal and simplicity of the original theory, but completely overcomes the need to specify the states of the world explicitly. The main idea of our approach is to address decision problems in a radically empirical way: instead of specifying states and consequences prior to the decision analysis, we only assume a protocol of observed act--consequence pairs as model primitives. We show how optimality in such empirical decision problems can be addressed by using protocol-based empirical choice functions and discuss three approaches for deriving inferential guarantees: (I) consistent statistical estimation of choice sets, (II) consistent statistical testing of choice functions with robustness guarantees, and (III) direct inference for empirical choice functions using credal sets. We illustrate our theory with a proof-of-concept application comparing different prompting strategies in generative AI models.

</details>


### [11] [Optimal Watermark Generation under Type I and Type II Errors](https://arxiv.org/abs/2512.05333)
*Hengzhi He,Shirong Xu,Alexander Nemecek,Jiping Li,Erman Ayday,Guang Cheng*

Main category: stat.ME

TL;DR: 该论文为生成模型水印建立了理论框架，通过统计假设检验推导了检测能力与生成保真度之间的基本权衡下界，并提出了最优水印分布和采样机制。


<details>
  <summary>Details</summary>
Motivation: 现有水印方案多为经验驱动，缺乏对检测能力与生成保真度之间基本权衡的理论理解。需要建立理论框架来填补这一空白。

Method: 将水印问题形式化为统计假设检验问题（零分布与水印分布），在明确假阳性和假阴性率约束下，推导f-散度衡量的保真度损失下界，并找到达到该下界的最优水印分布和相应采样规则。

Result: 建立了检测能力与保真度损失之间的紧下界，找到了最优水印分布，并开发了相应的最优采样机制，实现了最小保真度失真的水印嵌入。

Conclusion: 该研究建立了连接假设检验、信息散度和水印生成的简单而广泛适用的理论原则，为水印技术提供了理论基础。

Abstract: Watermarking has recently emerged as a crucial tool for protecting the intellectual property of generative models and for distinguishing AI-generated content from human-generated data. Despite its practical success, most existing watermarking schemes are empirically driven and lack a theoretical understanding of the fundamental trade-off between detection power and generation fidelity. To address this gap, we formulate watermarking as a statistical hypothesis testing problem between a null distribution and its watermarked counterpart. Under explicit constraints on false-positive and false-negative rates, we derive a tight lower bound on the achievable fidelity loss, measured by a general $f$-divergence, and characterize the optimal watermarked distribution that attains this bound. We further develop a corresponding sampling rule that provides an optimal mechanism for inserting watermarks with minimal fidelity distortion. Our result establishes a simple yet broadly applicable principle linking hypothesis testing, information divergence, and watermark generation.

</details>


### [12] [A survival analysis of glioma patients using topological features and locations of tumors](https://arxiv.org/abs/2512.05646)
*Yuhyeong Jang,Tu Dan,Eric Vu,Chul Moon*

Main category: stat.ME

TL;DR: 提出基于持久同调学的拓扑放射组学特征来表征肿瘤形状，通过功能性Cox回归模型分析其与胶质瘤生存预后的关系，发现这些特征能有效预测生存预后并提供临床见解。


<details>
  <summary>Details</summary>
Motivation: 肿瘤形状对生长和转移有重要影响，但传统放射组学特征无法充分捕捉肿瘤形状的多样性模式，需要新的形状表征方法来分析其与生存预后的关系。

Method: 从持久同调学中提取新颖的拓扑放射组学特征来表征肿瘤形状；使用功能性Cox回归模型将这些特征纳入生存分析；加入形状特征与肿瘤位置的交互项来捕捉叶特异性效应。

Result: 拓扑特征能有效捕捉传统放射组学无法表征的肿瘤形状模式；在高级别和低级别胶质瘤的两个案例研究中，这些特征都是生存预后的强预测因子，在调整临床变量后仍保持显著性。

Conclusion: 基于持久同调学的拓扑放射组学特征能有效表征肿瘤形状，为生存分析提供强预测能力，并能提供关于肿瘤行为的临床有意义见解，有助于理解肿瘤形态与生存风险的关系。

Abstract: Tumor shape plays a critical role in influencing both growth and metastasis. We introduce a novel topological radiomic feature derived from persistent homology to characterize tumor shape, focusing on its association with time-to-event outcomes in gliomas. These features effectively capture diverse tumor shape patterns that are not represented by conventional radiomic measures. To incorporate these features into survival analysis, we employ a functional Cox regression model in which the topological features are represented in a functional space. We further include interaction terms between shape features and tumor location to capture lobe-specific effects. This approach enables interpretable assessment of how tumor morphology relates to survival risk. We evaluate the proposed method in two case studies using radiomic images of high-grade and low-grade gliomas. The findings suggest that the topological features serve as strong predictors of survival prognosis, remaining significant after adjusting for clinical variables, and provide additional clinically meaningful insights into tumor behavior.

</details>


### [13] [Model selection with uncertainty in estimating optimal dynamic treatment regimes](https://arxiv.org/abs/2512.05695)
*Chunyu Wang,Brian Tom*

Main category: stat.ME

TL;DR: 提出了一种在有限样本下为动态治疗策略中的对比函数进行模型选择的新方法，使用反事实交叉验证指标及其方差估计来评估候选模型性能


<details>
  <summary>Details</summary>
Motivation: 在精准医疗的动态治疗策略中，需要可解释且简约的对比函数模型，但模型可能被错误设定。由于DTR涉及多个决策点，后决策点的近似误差会影响前决策点的估计，因此需要有效的模型选择方法。传统方法关注渐近行为，但实际应用中候选模型的相对性能可能严重依赖样本量

Method: 采用反事实交叉验证指标，并提出一种新颖的方法来估计该指标的方差。通过结合交叉验证指标及其估计方差，能够在给定样本量下表征模型选择的不确定性，并促进与首选模型结构相关的假设检验

Result: 该方法能够在有限样本条件下评估候选模型的性能，量化模型选择的不确定性，为动态治疗策略中的对比函数模型选择提供统计依据

Conclusion: 提出的方法解决了动态治疗策略中对比函数模型选择的实际问题，特别关注有限样本性能而非渐近行为，通过方差估计量化不确定性，为临床决策提供了更可靠的模型选择框架

Abstract: Optimal dynamic treatment regimes (DTRs), as a key part of precision medicine, have progressively gained more attention recently. To inform clinical decision making, interpretable and parsimonious models for contrast functions are preferred, raising concerns about undue misspecification. It is therefore important to properly evaluate the performance of candidate interpretable models and select the one that best approximates the unknown contrast function. Moreover, since a DTR usually involves multiple decision points, an inaccurate approximation at a later decision point affects its estimation at an earlier decision point when a backward induction algorithm is applied. This paper aims to perform model selection for contrast functions in the context of learning optimal DTRs from observed data. Note that the relative performance of candidate models may heavily depend on the sample size when, for example, the comparison is made between parametric and tree-based models. Therefore, instead of investigating the limiting behavior of each candidate model and developing methods to select asymptotically the `correct' one, we focus on the finite sample performance of each model and attempt to perform model selection under a given sample size. To this end, we adopt the counterfactual cross-validation metric and propose a novel method to estimate the variance of the metric. Supplementing the cross-validation metric with its estimated variance allows us to characterize the uncertainty in model selection under a given sample size and facilitates hypothesis testing associated with a preferred model structure.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [14] [Consistency of Familial DNA Search Results in Southeast Asian Populations](https://arxiv.org/abs/2512.05490)
*Monchai Kooakachai,Tiwakorn Chapalee,Chairat Thitiyan,Patsaya Jumnongwut*

Main category: stat.AP

TL;DR: 该研究评估了在东南亚人口（泰国、马来西亚、新加坡）中进行家族DNA搜索时，基于似然比的不同策略表现，发现在泰国亚群中最小似然比策略最优。


<details>
  <summary>Details</summary>
Motivation: 当DNA数据库中没有完全匹配时，家族DNA搜索可以通过似然比识别一级亲属。在存在多个相关亚群时，需要评估不同似然比组合策略的有效性，但现有研究主要针对美国人群，在东南亚地区的效果尚不明确。

Method: 研究评估了在东南亚人口（泰国、马来西亚、新加坡）中基于似然比的家族DNA搜索策略，比较了平均等位基因频率、平均似然比、最大似然比和最小似然比等不同组合方法。

Result: 研究结果与先前研究一致，显示统计功效在不同策略间存在差异。在泰国亚群中，最小似然比策略表现最佳，既能保持高统计功效，又能最小化亚群间的差异。

Conclusion: 对于东南亚人口，特别是泰国亚群，最小似然比策略是家族DNA搜索的优选方法，因为它平衡了统计功效和亚群间差异的考量。

Abstract: DNA databases are widely used in forensic science to identify unknown offenders. When no exact match is found, familial DNA searches can help by identifying first-degree relatives using likelihood ratios. If multiple subpopulations are relevant, likelihood ratios can be computed separately based on allele frequency estimates. Various strategies exist to combine these ratios, such as averaging allele frequencies or taking the average, maximum, or minimum likelihood ratio. While some comparisons have been made in populations like those in the U.S., their effectiveness in other regions remains unclear. This study evaluates likelihood ratio-based strategies in Southeast Asian populations, specifically Thailand, Malaysia, and Singapore. Our findings align with previous research, showing that statistical power varies across strategies. Among Thai subpopulations, the minimum likelihood ratio strategy is preferred, as it maintains high power while minimizing differences between subpopulations.

</details>


### [15] [Multi-state Modeling of Delay Evolution in Suburban Rail Transports](https://arxiv.org/abs/2512.05521)
*Stefania Colombo,Alfredo Gimenez Zapiola,Francesca Ieva,Simone Vantini*

Main category: stat.AP

TL;DR: 应用连续时间多状态模型分析意大利伦巴第S5郊区线路延误的时空演化，揭示延误动态如何随方向、时段和路段变化，以及车站饱和度和乘客负载等协变量对延误升级或恢复风险的影响。


<details>
  <summary>Details</summary>
Motivation: 铁路系统特别是郊区网络中列车延误问题持续存在，传统延误模型往往忽视实际延误传播的时空和结构动态，需要更精细的建模方法来理解延误演化机制。

Method: 使用连续时间多状态模型分析S5郊区线路延误的时序演化，结合详细的运营数据、气象数据和上下文数据，在建模延误状态转移时考虑可观测的异质性。

Result: 研究发现延误动态随旅行方向、时间段和路线段而变化，车站饱和度和乘客负载等协变量显著影响延误升级或恢复的风险，为理解延误传播机制提供了实证证据。

Conclusion: 该研究提供了方法学进步和实用结果，有助于提高铁路服务的可靠性，为铁路运营管理提供了基于数据的决策支持。

Abstract: Train delays are a persistent issue in railway systems, particularly in suburban networks where operational complexity is heightened by frequent services and high passenger volumes. Traditional delay models often overlook the temporal and structural dynamics of real delay propagation.
  This work applies continuous-time multi-state models to analyze the temporal evolution of delay on the S5 suburban line in Lombardy, Italy. Using detailed operational, meteorological, and contextual data, the study models delay transitions while accounting for observable heterogeneity.
  The findings reveal how delay dynamics vary by travel direction, time slot, and route segment. Covariates such as station saturation and passenger load are shown to significantly affect the risk of delay escalation or recovery. The study offers both methodological advancements and practical results for improving the reliability of rail services.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [16] [How to Tame Your LLM: Semantic Collapse in Continuous Systems](https://arxiv.org/abs/2512.05162)
*C. M. Wyss*

Main category: stat.ML

TL;DR: 提出连续状态机理论解释大语言模型的语义动态，证明语义特征定理：转移算子的主导特征函数诱导有限个谱盆地，将连续激活流形坍缩为有限可解释本体论。


<details>
  <summary>Details</summary>
Motivation: 解释大语言模型中离散符号语义如何从连续计算中涌现，为语义动态提供数学理论基础，连接连续表示与离散逻辑结构。

Method: 将大语言模型形式化为连续状态机（平滑动力系统），分析转移算子的谱特性，在紧致性、遍历性、有界雅可比等正则性假设下证明语义特征定理。

Result: 证明转移算子紧致且有离散谱，主导特征函数诱导有限个谱盆地（不变意义区域），每个盆地可在实数上的o-极小结构中定义，谱可聚合性与逻辑可驯性一致。

Conclusion: 连续激活流形可坍缩为有限逻辑可解释的本体论，解释了离散语义从连续计算中的涌现机制，该理论可扩展到随机和绝热（时间非齐次）设置。

Abstract: We develop a general theory of semantic dynamics for large language models by formalizing them as Continuous State Machines (CSMs): smooth dynamical systems whose latent manifolds evolve under probabilistic transition operators. The associated transfer operator $P: L^2(M,μ) \to L^2(M,μ)$ encodes the propagation of semantic mass. Under mild regularity assumptions (compactness, ergodicity, bounded Jacobian), $P$ is compact with discrete spectrum. Within this setting, we prove the Semantic Characterization Theorem (SCT): the leading eigenfunctions of $P$ induce finitely many spectral basins of invariant meaning, each definable in an o-minimal structure over $\mathbb{R}$. Thus spectral lumpability and logical tameness coincide. This explains how discrete symbolic semantics can emerge from continuous computation: the continuous activation manifold collapses into a finite, logically interpretable ontology. We further extend the SCT to stochastic and adiabatic (time-inhomogeneous) settings, showing that slowly drifting kernels preserve compactness, spectral coherence, and basin structure.

</details>


### [17] [One-Step Diffusion Samplers via Self-Distillation and Deterministic Flow](https://arxiv.org/abs/2512.05251)
*Pascal Jutras-Dube,Jiaru Zhang,Ziran Wang,Ruqi Zhang*

Main category: stat.ML

TL;DR: 提出一种一步扩散采样器，通过状态空间一致性损失学习步长条件ODE，实现一步采样，同时引入确定性流重要性权重和体积一致性正则化，在少量步数下保持稳定证据估计。


<details>
  <summary>Details</summary>
Motivation: 现有采样算法通常需要大量迭代步骤才能产生高质量样本，计算成本高。扩散采样器在少步数情况下标准ELBO估计会退化，因为常见的离散积分器导致前向/后向转移核不匹配。

Method: 1) 学习步长条件ODE，通过状态空间一致性损失使一大步重现多个小步的轨迹；2) 推导无需后向核的确定性流重要性权重用于ELBO估计；3) 引入体积一致性正则化校准DF，对齐不同步长分辨率下沿流的累积体积变化。

Result: 在具有挑战性的合成和贝叶斯基准测试中，该方法以数量级更少的网络评估实现了具有竞争力的样本质量，同时保持了稳健的ELBO估计。

Conclusion: 提出的一步扩散采样器能够在仅一步或少数几步内同时实现高质量采样和稳定证据估计，显著降低了计算成本。

Abstract: Sampling from unnormalized target distributions is a fundamental yet challenging task in machine learning and statistics. Existing sampling algorithms typically require many iterative steps to produce high-quality samples, leading to high computational costs. We introduce one-step diffusion samplers which learn a step-conditioned ODE so that one large step reproduces the trajectory of many small ones via a state-space consistency loss. We further show that standard ELBO estimates in diffusion samplers degrade in the few-step regime because common discrete integrators yield mismatched forward/backward transition kernels. Motivated by this analysis, we derive a deterministic-flow (DF) importance weight for ELBO estimation without a backward kernel. To calibrate DF, we introduce a volume-consistency regularization that aligns the accumulated volume change along the flow across step resolutions. Our proposed sampler therefore achieves both sampling and stable evidence estimate in only one or few steps. Across challenging synthetic and Bayesian benchmarks, it achieves competitive sample quality with orders-of-magnitude fewer network evaluations while maintaining robust ELBO estimates.

</details>


### [18] [Symmetric Linear Dynamical Systems are Learnable from Few Observations](https://arxiv.org/abs/2512.05337)
*Minh Vu,Andrey Y. Lokhov,Marc Vuffray*

Main category: stat.ML

TL;DR: 提出一种基于矩估计的新方法，仅需O(log N)个观测即可高精度恢复N维随机线性动力学的对称动态矩阵，无论矩阵稀疏与否


<details>
  <summary>Details</summary>
Motivation: 解决从单条轨迹中学习随机线性动力学参数的问题，特别是在完全观测和部分观测场景下。传统方法通常需要大量观测数据或依赖问题特定的正则化，限制了在结构发现等应用中的实用性。

Method: 基于矩估计方法，不依赖问题特定的正则化。该方法能够从仅O(log N)个观测中恢复对称动态矩阵，无论矩阵是稀疏还是稠密。

Result: 新估计器在恢复对称动态矩阵时能够实现小的最大元素误差，仅需T=O(log N)个观测，且不依赖于矩阵的稀疏性。这对于结构发现等应用特别重要。

Conclusion: 提出的矩估计方法为从有限观测中学习随机线性动力学参数提供了有效的解决方案，特别是在高维设置下，仅需对数级别的观测数量即可实现精确恢复，具有重要的理论和应用价值。

Abstract: We consider the problem of learning the parameters of a $N$-dimensional stochastic linear dynamics under both full and partial observations from a single trajectory of time $T$. We introduce and analyze a new estimator that achieves a small maximum element-wise error on the recovery of symmetric dynamic matrices using only $T=\mathcal{O}(\log N)$ observations, irrespective of whether the matrix is sparse or dense. This estimator is based on the method of moments and does not rely on problem-specific regularization. This is especially important for applications such as structure discovery.

</details>


### [19] [Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted Data](https://arxiv.org/abs/2512.05456)
*Stephen Salerno,Kentaro Hoffman,Awan Afiaz,Anna Neufeld,Tyler H. McCormick,Jeffrey T. Leek*

Main category: stat.ML

TL;DR: 使用预测数据作为真实数据替代进行统计推断存在偏差和方差问题，高预测准确率不能保证下游推断有效性


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML工具普及和数据收集困难增加，研究者越来越多地使用预训练算法的预测结果替代缺失或未观测数据，但标准推断工具在这种情况下可能导致错误结论

Method: 分析预测数据推断(IPD)的统计挑战，将其失败归因于偏差（预测系统性改变估计量或扭曲变量关系）和方差（忽略预测模型不确定性和真实数据固有变异性），回顾现有方法并联系经典统计理论

Result: 高预测准确率不能保证有效的下游推断，所有IPD失败都可归结为偏差和方差问题，需要透明且统计原则的方法来使用预测数据

Conclusion: 使用预测数据需要谨慎，必须考虑偏差和方差问题，未来研究应关注如何在科学研究中透明且统计原则地使用预测数据

Abstract: As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g., rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as substitutes for missing or unobserved data. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to drawing inference with predicted data (IPD) and show that high predictive accuracy does not guarantee valid downstream inference. We show that all such failures reduce to statistical notions of (i) bias, when predictions systematically shift the estimand or distort relationships among variables, and (ii) variance, when uncertainty from the prediction model and the intrinsic variability of the true data are ignored. We then review recent methods for conducting IPD and discuss how this framework is deeply rooted in classical statistical theory. We then comment on some open questions and interesting avenues for future work in this area, and end with some comments on how to use predicted data in scientific studies that is both transparent and statistically principled.

</details>


### [20] [Design-marginal calibration of Gaussian process predictive distributions: Bayesian and conformal approaches](https://arxiv.org/abs/2512.05611)
*Aurélien Pion,Emmanuel Vazquez*

Main category: stat.ML

TL;DR: 该论文研究了高斯过程预测分布在插值设置下的校准问题，提出了两种方法：cps-gp和bcr-gp，用于改进预测分布的设计边际校准性能。


<details>
  <summary>Details</summary>
Motivation: 高斯过程预测分布在插值设置下的校准问题尚未得到充分研究。传统的高斯过程预测分布可能在校准方面存在不足，特别是在设计边际视角下。作者旨在开发能够保证有限样本边际校准的预测分布方法。

Method: 提出了两种方法：1) cps-gp：将保形预测系统适配到GP插值中，使用标准化的留一残差，产生具有有限样本边际校准的分步预测分布；2) bcr-gp：保留GP后验均值，用拟合到交叉验证标准化残差的广义正态模型替换高斯残差，通过贝叶斯选择规则控制离散度和尾部行为。

Result: 在基准函数上的数值实验比较了cps-gp、bcr-gp、Jackknife+ for GPs和完整保形高斯过程。使用校准指标（覆盖率、Kolmogorov-Smirnov、积分绝对误差）和准确性或锐度（通过缩放连续排序概率得分）进行评估。

Conclusion: cps-gp和bcr-gp方法能够改进高斯过程预测分布的校准性能，cps-gp提供有限样本边际校准，bcr-gp产生适合顺序设计的平滑预测分布。这些方法在插值设置下提供了更好的设计边际校准控制。

Abstract: We study the calibration of Gaussian process (GP) predictive distributions in the interpolation setting from a design-marginal perspective. Conditioning on the data and averaging over a design measure μ, we formalize μ-coverage for central intervals and μ-probabilistic calibration through randomized probability integral transforms. We introduce two methods. cps-gp adapts conformal predictive systems to GP interpolation using standardized leave-one-out residuals, yielding stepwise predictive distributions with finite-sample marginal calibration. bcr-gp retains the GP posterior mean and replaces the Gaussian residual by a generalized normal model fitted to cross-validated standardized residuals. A Bayesian selection rule-based either on a posterior upper quantile of the variance for conservative prediction or on a cross-posterior Kolmogorov-Smirnov criterion for probabilistic calibration-controls dispersion and tail behavior while producing smooth predictive distributions suitable for sequential design. Numerical experiments on benchmark functions compare cps-gp, bcr-gp, Jackknife+ for GPs, and the full conformal Gaussian process, using calibration metrics (coverage, Kolmogorov-Smirnov, integral absolute error) and accuracy or sharpness through the scaled continuous ranked probability score.

</details>


### [21] [BalLOT: Balanced $k$-means clustering with optimal transport](https://arxiv.org/abs/2512.05926)
*Wenyan Luo,Dustin G. Mixon*

Main category: stat.ML

TL;DR: BalLOT：一种基于最优传输的平衡k-means聚类方法，通过交替最小化实现快速有效的聚类，在随机球模型下具有理论保证和一步恢复能力。


<details>
  <summary>Details</summary>
Motivation: 解决平衡k-means聚类这一基础问题，传统方法在平衡约束下效果有限，需要开发既能保证平衡性又具有理论保证的高效算法。

Method: 提出BalLOT方法，将最优传输与交替最小化相结合，通过最优传输处理平衡约束，交替更新聚类中心和分配矩阵，并设计了专门的初始化方案。

Result: 数值实验显示BalLOT快速有效；理论证明对通用数据产生整数耦合；在随机球模型下能精确或部分恢复真实聚类；初始化方案能实现一步恢复。

Conclusion: BalLOT为平衡k-means聚类提供了理论保证的快速解决方案，结合了最优传输的优势和交替最小化的效率，在理论和实践上均有良好表现。

Abstract: We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.

</details>


### [22] [Consequences of Kernel Regularity for Bandit Optimization](https://arxiv.org/abs/2512.05957)
*Madison Lee,Tara Javidi*

Main category: stat.ML

TL;DR: 该研究揭示了核函数正则性与RKHS函数bandit优化性能之间的深层联系，通过谱分析统一了全局核回归和局部平滑方法，为多种核函数推导了显式遗憾界，并分析了混合算法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RKHS方法依赖全局核回归器，而平滑方法则利用局部近似，这两种视角在bandit优化中的关系尚不明确。研究旨在通过核的谱性质揭示这两种方法的深层联系，建立统一的分析框架。

Method: 通过分析Matérn、平方指数、有理二次、γ指数、分段多项式和Dirichlet等各向同性核的傅里叶谱性质，将谱衰减率与算法性能联系起来。对于核化bandit算法，谱衰减决定了最大信息增益的上界；对于平滑方法，同样的衰减率建立了Hölder空间嵌入和Besov空间范数等价性。还分析了结合两种方法的LP-GP-UCB混合算法。

Result: 为每个核函数族推导了显式的遗憾界，在某些情况下得到了新结果，在其他情况下提供了改进分析。混合算法LP-GP-UCB虽然在所有情况下不优于专用方法，但在多个核函数族中达到了阶最优性。

Conclusion: 核函数的谱衰减率是连接全局核回归和局部平滑方法的关键桥梁，为RKHS函数bandit优化提供了统一的分析框架。混合方法能够实现跨多个核函数族的阶最优性能，展示了两种视角的互补价值。

Abstract: In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Matérn, square-exponential, rational-quadratic, $γ$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish Hölder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.

</details>
