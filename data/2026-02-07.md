<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 5]
- [stat.ML](#stat.ML) [Total: 10]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Penalized Likelihood Parameter Estimation for Differential Equation Models: A Computational Tutorial](https://arxiv.org/abs/2602.04891)
*Matthew J Simpson,James S Bennett,Alexander Johnston,Ruth E Baker*

Main category: stat.ME

TL;DR: 本文是一篇教程式文章，介绍广义剖析（参数级联）方法在常微分方程模型参数估计中的应用，提供可重复的计算练习和开源Jupyter笔记本。


<details>
  <summary>Details</summary>
Motivation: 参数估计在科学和工业应用中连接数学模型与真实世界数据，但传统方法（如最大似然估计和MCMC）需要反复求解模型，计算成本高。广义剖析方法直接关注控制微分方程，通过惩罚似然函数同时衡量数据拟合和模型拟合，具有优势但实践中使用较少。

Method: 文章采用教程式方法，设计自导式计算练习，使用广义剖析（参数级联）方法，通过惩罚似然函数将数据和模型拟合相结合，所有计算可通过GitHub上的开源Jupyter笔记本复现。

Result: 提供了一套完整的计算练习和可重复的Jupyter笔记本，帮助读者掌握广义剖析方法在常微分方程模型参数估计中的应用技能。

Conclusion: 广义剖析作为一种替代传统参数估计的方法，具有独特优势，本文通过教程式练习和开源工具降低了该方法的学习门槛，促进其在实践中的更广泛应用。

Abstract: Parameter estimation connects mathematical models to real-world data and decision making across many scientific and industrial applications. Standard approaches such as maximum likelihood estimation and Markov chain Monte Carlo estimate parameters by repeatedly solving the model, which often requires numerical solutions of differential equation models. In contrast, generalized profiling (also called parameter cascading) focuses directly on the governing differential equation(s), linking the model and data through a penalized likelihood that explicitly measures both the data fit and model fit. Despite several advantages, generalized profiling is relatively rarely used in practice. This tutorial-style article outlines a set of self-directed computational exercises that facilitate skills development in applying generalized profiling to a range of ordinary differential equation models. All calculations can be repeated using reproducible open-source Jupyter notebooks that are available on GitHub.

</details>


### [2] [Boxplots and quartile plots for grouped and periodic angular data](https://arxiv.org/abs/2602.05335)
*Joshua D. Berlinski,Fan Dai,Ranjan Maitra*

Main category: stat.ME

TL;DR: 该论文提出了用于可视化角度数据分布的同心圆箱线图方法，通过调整箱宽解决视觉感知问题，并开发了圆形分位数图和三维环形显示，应用于心理学、基因组学和气象学等领域。


<details>
  <summary>Details</summary>
Motivation: 角度数据（单位圆上的观测值）在许多学科中出现，需要特殊的描述、分析、解释和可视化方法。现有方法在可视化角度数据分布方面存在不足，特别是对于多组角度数据的比较分析。

Method: 开发同心圆箱线图显示方法，将箱宽设置为与中心距离的平方根成反比以解决视觉感知问题；提出圆形分位数图用于大量组别；实现三维环形显示用于周期性角度分布。

Result: 通过感知调查支持了缩放箱宽的选择；在心理学（运动共振）、基因组学（时钟基因峰值相位）和气象学（风向分布）三个领域的实际数据集中成功应用了所提出的方法。

Conclusion: 提出的同心圆箱线图方法为角度数据的可视化提供了有效的解决方案，解决了视觉感知挑战，并在多个学科的实际应用中展示了其价值。

Abstract: Angular observations, or observations lying on the unit circle, arise in many disciplines and require special care in their description, analysis, interpretation and visualization. We provide methods to construct concentric circular boxplot displays of distributions of groups of angular data. The use of concentric boxplots brings challenges of visual perception, so we set the boxwidths to be inversely proportional to the square root of their distance from the centre. A perception survey supports this scaled boxwidth choice. For a large number of groups, we propose circular quartile plots. A three-dimensional toroidal display is also implemented for periodic angular distributions. We illustrate our methods on datasets in (1) psychology, to display motor resonance under different conditions, (2) genomics, to understand the distribution of peak phases for ancillary clock genes, and (3) meteorology and wind turbine power generation, to study the changing and periodic distribution of wind direction over the course of a year.

</details>


### [3] [Sensitivity analysis for contamination in egocentric-network randomized trials with interference](https://arxiv.org/abs/2602.05553)
*Bar Weinstein,Daniel Nevo*

Main category: stat.ME

TL;DR: ENRTs中由于网络连接导致的污染会使因果效应估计产生偏差，本文提出了偏差校正估计和敏感性分析框架来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 在无法测量完整社会网络数据时，Egocentric-Network Randomized Trials (ENRTs) 被广泛用于估计干扰下的因果效应。然而，由于观察到的自我网络是分离的，而底层人口网络可能存在连接边，导致污染问题，使传统的Horvitz-Thompson估计器产生偏差。

Method: 1) 在设计基础框架下证明污染存在时直接和间接效应的Horvitz-Thompson估计器存在偏差；2) 推导偏差校正估计器；3) 提出基于敏感性参数（缺失边的概率或期望数量）的敏感性分析框架；4) 通过网格敏感性分析和概率偏差分析实现该框架。

Result: 在HIV预防试验网络037研究中应用该方法，发现忽略污染可能导致间接效应被低估，直接效应被高估。提出的敏感性分析框架为研究人员提供了评估因果估计对污染稳健性的灵活工具。

Conclusion: ENRTs中的污染问题必须得到重视，本文提出的偏差校正估计和敏感性分析框架为处理这一问题提供了有效方法，有助于获得更可靠的因果效应估计。

Abstract: Egocentric-Network Randomized Trials (ENRTs) are increasingly used to estimate causal effects under interference when measuring complete sociocentric network data is infeasible. ENRTs rely on egocentric network sampling, where a set of egos is first sampled, and each ego recruits a subset of its neighbors as alters. Treatments are then randomized across egos. While the observed ego-networks are disjoint by design, the underlying population network may contain edges connecting them, leading to contamination. Under a design-based framework, we show that the Horvitz-Thompson estimators of direct and indirect effects are biased whenever contamination is present. To address this, we derive bias-corrected estimators and propose a novel sensitivity analysis framework based on sensitivity parameters representing the probability or expected number of missing edges. This framework is implemented via both grid sensitivity analysis and probabilistic bias analysis, providing researchers with a flexible tool to assess the robustness of the causal estimators to contamination. We apply our methodology to the HIV Prevention Trials Network 037 study, finding that ignoring contamination may lead to underestimation of indirect effects and overestimation of direct effects.

</details>


### [4] [Copula-based models for spatially dependent cylindrical data](https://arxiv.org/abs/2602.05778)
*Francesca Labanca,Anna Gottard,Nadja Klein*

Main category: stat.ME

TL;DR: 提出用于圆柱数据的结构化加性条件copula回归模型，结合包裹高斯过程建模圆形分量，分布回归建模线性分量，并考虑空间相关性


<details>
  <summary>Details</summary>
Motivation: 现有圆柱数据copula模型通常将copula参数视为常数，忽略协变量影响，且边际分布回归通常限于线性预测器，未考虑空间相关性。需要开发能同时处理圆形-线性依赖和空间自相关的灵活模型。

Method: 提出结构化加性条件copula回归模型：圆形分量使用包裹高斯过程建模，线性分量采用分布回归模型，两者都允许纳入线性协变量效应。利用高斯随机场与高斯马尔可夫随机场的经验等价性，避免计算负担同时允许协方差结构非平稳性。使用马尔可夫链蒙特卡洛模拟进行后验估计。

Result: 通过模拟研究和德国风向风速数据分析评估模型。模型能有效处理圆柱数据的空间依赖关系，同时保持边际分布的灵活性，计算效率优于传统高斯随机场方法。

Conclusion: 提出的结构化加性条件copula回归模型为圆柱数据提供了灵活且计算高效的建模框架，能同时处理圆形-线性依赖、协变量效应和空间相关性，适用于气象学、海洋学等多个领域的应用。

Abstract: Cylindrical data frequently arise across various scientific disciplines, including meteorology (e.g., wind direction and speed), oceanography (e.g., marine current direction and speed or wave heights), ecology (e.g., telemetry), and medicine (e.g., seasonality and intensity in disease onset). Such data often occur as spatially correlated series of intensities and angles, thereby representing dependent bivariate response vectors of linear and circular components. To accommodate both the circular-linear dependence and spatial autocorrelation, while remaining flexible in marginal specifications, copula-based models for cylindrical data have been developed in the literature. However, existing approaches typically treat the copula parameters as constants unrelated to covariates, and regression specifications for marginal distributions are frequently restricted to linear predictors, thereby ignoring spatial correlation. In this work, we propose a structured additive conditional copula regression model for cylindrical data. The circular component is modeled using a wrapped Gaussian process, and the linear component follows a distributional regression model. Both components allow for the inclusion of linear covariate effects. Furthermore, by leveraging the empirical equivalence between Gaussian random fields (GRFs) and Gaussian Markov random fields, our approach avoids the computational burden typically associated with GRFs, while simultaneously allowing for non-stationarity in the covariance structure. Posterior estimation is performed via Markov chain Monte Carlo simulation. We evaluate the proposed model in a simulation study and subsequently in an analysis of wind directions and speed in Germany.

</details>


### [5] [SpARCD: A Spectral Graph Framework for Revealing Differential Functional Connectivity in fMRI Data](https://arxiv.org/abs/2602.05807)
*Shira Yoffe,Ziv Ben-Zion,Talma Hendler,Malka Gorfine,Ariel Jaffe*

Main category: stat.ME

TL;DR: 提出SpARCD框架，利用距离相关性和谱分析检测大脑功能连接在不同实验条件下的差异，相比传统方法具有更高统计功效


<details>
  <summary>Details</summary>
Motivation: 现有检测大脑功能连接差异的方法（如边级测试、种子点PPI分析、相关网络比较）存在统计功效低、阈值选择随意、难以捕捉分布式或非线性依赖模式等问题

Method: 使用距离相关性（对线性和非线性关联都敏感）为每个条件构建加权图，通过谱滤波构建微分算子，计算其主导特征向量来揭示连接变化，采用基于置换的检验方案生成可解释的区域级显著性图

Result: 模拟研究显示SpARCD相比传统边级或单变量方法具有优越的统计功效，特别是在复杂依赖结构存在时；应用于113名早期PTSD患者的情感面孔匹配任务fMRI数据，揭示了与情感反应和调节过程相关的不同网络

Conclusion: SpARCD为比较高维连接结构提供了一个统计严谨且计算高效的框架，在神经影像学和其他基于网络的科学领域具有广泛适用性

Abstract: Identifying brain regions that exhibit altered functional connectivity across cognitive or emotional states is a key problem in neuroscience. Existing methods, such as edge-wise testing, seed-based psychophysiological interaction (PPI) analysis, or correlation network comparison, typically suffer from low statistical power, arbitrary thresholding, and limited ability to capture distributed or nonlinear dependence patterns. We propose SpARCD (Spectral Analysis of Revealing Connectivity Differences), a novel statistical framework for detecting differences in brain connectivity between two experimental conditions. SpARCD leverages distance correlation, a dependence measure sensitive to both linear and nonlinear associations, to construct a weighted graph for each condition. It then constructs a differential operator via spectral filtering and uncovers connectivity changes by computing its leading eigenvectors. Inference is achieved via a permutation-based testing scheme that yields interpretable, region-level significance maps. Extensive simulation studies demonstrate that SpARCD achieves superior power relative to conventional edge-wise or univariate approaches, particularly in the presence of complex dependency structures. Application to fMRI data from 113 early PTSD patients performing an emotional face-matching task reveals distinct networks associated with emotional reactivity and regulatory processes. Overall, SpARCD provides a statistically rigorous and computationally efficient framework for comparing high-dimensional connectivity structures, with broad applicability to neuroimaging and other network-based scientific domains.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [6] [Finite-Particle Rates for Regularized Stein Variational Gradient Descent](https://arxiv.org/abs/2602.05172)
*Ye He,Krishnakumar Balasubramanian,Sayan Banerjee,Promit Ghosal*

Main category: stat.ML

TL;DR: 本文分析了正则化Stein变分梯度下降(R-SVGD)算法的有限粒子收敛率，建立了非渐近界，并提供了正则化参数、步长和平均时域的理论调优规则。


<details>
  <summary>Details</summary>
Motivation: SVGD算法存在常数阶偏差问题，R-SVGD通过应用预条件子来修正这一偏差。需要建立有限粒子系统的收敛理论，量化逼近Wasserstein梯度流与控制有限粒子估计误差之间的权衡。

Method: 对R-SVGD的N粒子相互作用系统进行分析，建立时间平均(退火)经验测度的显式非渐近界，涵盖连续时间和离散时间动态，分析在真实(非核化)Fisher信息下的收敛性。

Result: 在目标满足W1I条件时，对于一大类光滑核函数，证明了W1收敛性。提供了正则化参数、步长和平均时域的理论调优规则，量化了逼近精度与估计误差的权衡。

Conclusion: R-SVGD的有限粒子系统具有理论保证的收敛性，分析框架为算法参数调优提供了理论指导，解决了SVGD的常数阶偏差问题。

Abstract: We derive finite-particle rates for the regularized Stein variational gradient descent (R-SVGD) algorithm introduced by He et al. (2024) that corrects the constant-order bias of the SVGD by applying a resolvent-type preconditioner to the kernelized Wasserstein gradient. For the resulting interacting $N$-particle system, we establish explicit non-asymptotic bounds for time-averaged (annealed) empirical measures, illustrating convergence in the \emph{true} (non-kernelized) Fisher information and, under a $\mathrm{W}_1\mathrm{I}$ condition on the target, corresponding $\mathrm{W}_1$ convergence for a large class of smooth kernels. Our analysis covers both continuous- and discrete-time dynamics and yields principled tuning rules for the regularization parameter, step size, and averaging horizon that quantify the trade-off between approximating the Wasserstein gradient flow and controlling finite-particle estimation error.

</details>


### [7] [Total Variation Rates for Riemannian Flow Matching](https://arxiv.org/abs/2602.05174)
*Yunrui Guan,Krishnakumar Balasubramanian,Shiqian Ma*

Main category: stat.ML

TL;DR: 该论文为黎曼流匹配（RFM）采样器建立了非渐近总变差（TV）收敛分析，将误差分解为数值离散化和学习误差，并在球面和SPD流形上给出了显式多项式迭代复杂度。


<details>
  <summary>Details</summary>
Motivation: 黎曼流匹配将基于流的生成建模扩展到流形数据，但缺乏对其采样器收敛性的理论分析。需要建立非渐近收敛界限来分离数值离散化误差和学习误差，为实际应用提供理论保证。

Method: 通过建立控制两个流形ODE流之间总变差演化的微分不等式，将TV的时间导数表达为向量场不匹配的散度和参考流得分。利用平行传输和曲率的新界限来控制这些项，在紧流形和Hadamard流形上分别采用均匀和均方近似保证。

Result: 获得了形式为TV ≤ C_Lip·h + C_ε·ε的显式界限（紧流形上还有高阶ε²项），清晰分离了步长h引起的数值误差和目标精度ε引起的学习误差。在球面S^d和SPD(n)流形上实例化得到了显式多项式迭代复杂度。

Conclusion: 该工作为黎曼流匹配采样器提供了首个非渐近收敛分析框架，建立了误差分解的数学基础，为流形上的生成建模提供了理论保证和实际指导。

Abstract: Riemannian flow matching (RFM) extends flow-based generative modeling to data supported on manifolds by learning a time-dependent tangent vector field whose flow-ODE transports a simple base distribution to the data law. We develop a nonasymptotic Total Variation (TV) convergence analysis for RFM samplers that use a learned vector field together with Euler discretization on manifolds. Our key technical ingredient is a differential inequality governing the evolution of TV between two manifold ODE flows, which expresses the time-derivative of TV through the divergence of the vector-field mismatch and the score of the reference flow; controlling these terms requires establishing new bounds that explicitly account for parallel transport and curvature. Under smoothness assumptions on the population flow-matching field and either uniform (compact manifolds) or mean-square (Hadamard manifolds) approximation guarantees for the learned field, we obtain explicit bounds of the form $\mathrm{TV}\le C_{\mathrm{Lip}}\,h + C_{\varepsilon}\,\varepsilon$ (with an additional higher-order $\varepsilon^2$ term on compact manifolds), cleanly separating numerical discretization and learning errors. Here, $h$ is the step-size and $\varepsilon$ is the target accuracy. Instantiations yield \emph{explicit} polynomial iteration complexities on the hypersphere $S^d$, and on the SPD$(n)$ manifolds under mild moment conditions.

</details>


### [8] [Radon--Wasserstein Gradient Flows for Interacting-Particle Sampling in High Dimensions](https://arxiv.org/abs/2602.05227)
*Elias Hess-Childs,Dejan Slepčev,Lantian Xu*

Main category: stat.ML

TL;DR: 提出基于Radon-Wasserstein几何的新梯度流方法，用于KL散度优化，具有高维可扩展性和线性计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现有KL散度梯度流方法（如Fokker-Planck方程、Stein变分梯度下降）在高维空间中计算成本高，需要开发更高效的可扩展算法

Method: 引入Radon-Wasserstein几何和正则化Radon-Wasserstein几何，利用Radon变换使梯度流速度仅依赖于一维投影，通过快速傅里叶变换实现高效一维卷积计算

Result: 提出了具有线性计算复杂度（粒子数和维度均为线性）的交互粒子算法，提供了数值实验验证性能，并证明了RRW流的适定性和长期收敛性

Conclusion: 基于Radon-Wasserstein几何的新梯度流方法为高维KL散度优化提供了高效可扩展的解决方案，在计算效率和理论保证方面均有优势

Abstract: Gradient flows of the Kullback--Leibler (KL) divergence, such as the Fokker--Planck equation and Stein Variational Gradient Descent, evolve a distribution toward a target density known only up to a normalizing constant. We introduce new gradient flows of the KL divergence with a remarkable combination of properties: they admit accurate interacting-particle approximations in high dimensions, and the per-step cost scales linearly in both the number of particles and the dimension. These gradient flows are based on new transportation-based Riemannian geometries on the space of probability measures: the Radon--Wasserstein geometry and the related Regularized Radon--Wasserstein (RRW) geometry. We define these geometries using the Radon transform so that the gradient-flow velocities depend only on one-dimensional projections. This yields interacting-particle-based algorithms whose per-step cost follows from efficient Fast Fourier Transform-based evaluation of the required 1D convolutions. We additionally provide numerical experiments that study the performance of the proposed algorithms and compare convergence behavior and quantization. Finally, we prove some theoretical results including well-posedness of the flows and long-time convergence guarantees for the RRW flow.

</details>


### [9] [Logarithmic-time Schedules for Scaling Language Models with Momentum](https://arxiv.org/abs/2602.05298)
*Damien Ferbach,Courtney Paquette,Gauthier Gidel,Katie Everett,Elliot Paquette*

Main category: stat.ML

TL;DR: ADANA优化器通过利用语言数据的幂律结构，为AdamW的超参数设计随时间变化的调度策略，相比固定超参数能显著提升大语言模型训练效率


<details>
  <summary>Details</summary>
Motivation: 实践中AdamW的超参数通常保持固定值，但作者认为对于大规模语言模型训练，利用语言数据的幂律结构设计随时间变化的调度策略可以获得实质性性能提升

Method: 提出ADANA优化器，采用对数时间调度策略，让优化器的梯度记忆范围随训练时间增长，并通过阻尼机制保持稳定性，同时保留长记忆优势

Result: ADANA在transformer模型（45M到2.6B参数）上相比AdamW、Muon和AdEMAMix，在适当调优后能获得高达40%的计算效率提升，且随着模型规模增大增益更加明显

Conclusion: 对数时间调度策略能显著提升大语言模型训练效率，ADANA通过结合对数时间调度和阻尼机制，在保持稳定性的同时实现了性能提升，且该方法可扩展到其他优化器

Abstract: In practice, the hyperparameters $(β_1, β_2)$ and weight-decay $λ$ in AdamW are typically kept at fixed values. Is there any reason to do otherwise? We show that for large-scale language model training, the answer is yes: by exploiting the power-law structure of language data, one can design time-varying schedules for $(β_1, β_2, λ)$ that deliver substantial performance gains.
  We study logarithmic-time scheduling, in which the optimizer's gradient memory horizon grows with training time. Although naive variants of this are unstable, we show that suitable damping mechanisms restore stability while preserving the benefits of longer memory. Based on this, we present ADANA, an AdamW-like optimizer that couples log-time schedules with explicit damping to balance stability and performance. We empirically evaluate ADANA across transformer scalings (45M to 2.6B parameters), comparing against AdamW, Muon, and AdEMAMix.
  When properly tuned, ADANA achieves up to 40% compute efficiency relative to a tuned AdamW, with gains that persist--and even improve--as model scale increases. We further show that similar benefits arise when applying logarithmic-time scheduling to AdEMAMix, and that logarithmic-time weight-decay alone can yield significant improvements. Finally, we present variants of ADANA that mitigate potential failure modes and improve robustness.

</details>


### [10] [Fast Rates for Nonstationary Weighted Risk Minimization](https://arxiv.org/abs/2602.05742)
*Tobias Brock,Thomas Nagler*

Main category: stat.ML

TL;DR: 本文研究了加权经验风险最小化在分布漂移下的样本外预测误差，提供了超额风险的一般分解，并在混合条件下证明了学习误差的oracle不等式，该界限对任意权重类一致成立，并考虑了权重向量诱导的有效样本量、权重和假设类的复杂性以及潜在的数据依赖性。


<details>
  <summary>Details</summary>
Motivation: 加权经验风险最小化是处理分布漂移预测的常用方法，但需要理论分析其在非平稳性下的样本外预测误差，以理解学习误差和分布漂移误差的分解关系。

Method: 提供超额风险的一般分解为学习项和分布漂移误差项，在混合条件下证明学习误差的oracle不等式，该界限对任意权重类一致成立，考虑了有效样本量、权重和假设类的复杂性以及数据依赖性。

Result: 建立了加权经验风险最小化的理论框架，在（自）回归问题中应用线性模型、基函数逼近和神经网络，在非加权和平稳设置下恢复最小最大最优速率（最多对数因子）。

Conclusion: 本文为分布漂移下的加权经验风险最小化提供了统一的理论分析框架，证明了其理论保证的普适性和锐度，为实际应用提供了理论基础。

Abstract: Weighted empirical risk minimization is a common approach to prediction under distribution drift. This article studies its out-of-sample prediction error under nonstationarity. We provide a general decomposition of the excess risk into a learning term and an error term associated with distribution drift, and prove oracle inequalities for the learning error under mixing conditions. The learning bound holds uniformly over arbitrary weight classes and accounts for the effective sample size induced by the weight vector, the complexity of the weight and hypothesis classes, and potential data dependence. We illustrate the applicability and sharpness of our results in (auto-) regression problems with linear models, basis approximations, and neural networks, recovering minimax-optimal rates (up to logarithmic factors) when specialized to unweighted and stationary settings.

</details>


### [11] [Decision-Focused Sequential Experimental Design: A Directional Uncertainty-Guided Approach](https://arxiv.org/abs/2602.05340)
*Beichen Wan,Mo Liu,Paul Grigas,Zuo-Jun Max Shen*

Main category: stat.ML

TL;DR: 提出一种基于方向性不确定性的序列实验设计方法，用于预测-优化范式，相比传统决策盲设计能更早停止并获得更好决策性能


<details>
  <summary>Details</summary>
Motivation: 在预测-优化范式中，传统序列实验设计关注预测准确性，但最终性能由下游优化决策损失决定，这种不匹配导致传统决策盲设计效率低下

Method: 提出方向性不确定性度量来量化预测不确定性，无需解决优化oracle，计算高效，并基于此设计序列实验准则

Result: 方法具有强一致性和收敛保证，在广泛分布类别下，方向性不确定性设计比决策盲设计达到更早停止时间，LLM工作分配实验验证优势

Conclusion: 方向性不确定性度量能有效桥接预测准确性和决策损失，为预测-优化范式提供高效序列实验设计方法

Abstract: We consider the sequential experimental design problem in the predict-then-optimize paradigm. In this paradigm, the outputs of the prediction model are used as coefficient vectors in a downstream linear optimization problem. Traditional sequential experimental design aims to control the input variables (features) so that the improvement in prediction accuracy from each experimental outcome (label) is maximized. However, in the predict-then-optimize setting, performance is ultimately evaluated based on the decision loss induced by the downstream optimization, rather than by prediction error. This mismatch between prediction accuracy and decision loss renders traditional decision-blind designs inefficient. To address this issue, we propose a directional-based metric to quantify predictive uncertainty. This metric does not require solving an optimization oracle and is therefore computationally tractable. We show that the resulting sequential design criterion enjoys strong consistency and convergence guarantees. Under a broad class of distributions, we demonstrate that our directional uncertainty-based design attains an earlier stopping time than decision-blind designs. This advantage is further supported by real-world experiments on an LLM job allocation problem.

</details>


### [12] [Wedge Sampling: Efficient Tensor Completion with Nearly-Linear Sample Complexity](https://arxiv.org/abs/2602.05869)
*Hengrui Luo,Anna Ma,Ludovic Stephan,Yizhe Zhu*

Main category: stat.ML

TL;DR: 提出Wedge Sampling非自适应采样方案用于低秩张量补全，通过结构化长度二模式（楔形）采样，在多项式时间内实现弱恢复和精确恢复，样本复杂度接近线性。


<details>
  <summary>Details</summary>
Motivation: 标准均匀采样模型在低秩张量补全中存在统计-计算差距，需要大量样本才能实现高效算法。本文旨在设计一种非自适应采样方案，通过强化初始化阶段的谱信号来克服这一障碍。

Method: 提出Wedge Sampling方法，将观测分配到关联二分采样图中的结构化长度二模式（楔形）。与均匀采样不同，该方法直接促进长度二连接，增强谱信号以支持高效初始化。

Result: Wedge Sampling使多项式时间算法能够以接近线性的样本复杂度实现弱恢复和精确恢复。基于楔形采样的谱初始化可与现有细化方法结合，仅需额外Õ(n)均匀采样条目，显著优于均匀采样所需的Õ(n^{k/2})样本复杂度。

Conclusion: Barak和Moitra强调的统计-计算差距主要是均匀采样模型的结果，替代的非自适应测量设计（如Wedge Sampling）通过保证强初始化可以克服这一障碍。

Abstract: We introduce Wedge Sampling, a new non-adaptive sampling scheme for low-rank tensor completion. We study recovery of an order-$k$ low-rank tensor of dimension $n \times \cdots \times n$ from a subset of its entries. Unlike the standard uniform entry model (i.e., i.i.d. samples from $[n]^k$), wedge sampling allocates observations to structured length-two patterns (wedges) in an associated bipartite sampling graph. By directly promoting these length-two connections, the sampling design strengthens the spectral signal that underlies efficient initialization, in regimes where uniform sampling is too sparse to generate enough informative correlations.
  Our main result shows that this change in sampling paradigm enables polynomial-time algorithms to achieve both weak and exact recovery with nearly linear sample complexity in $n$. The approach is also plug-and-play: wedge-sampling-based spectral initialization can be combined with existing refinement procedures (e.g., spectral or gradient-based methods) using only an additional $\tilde{O}(n)$ uniformly sampled entries, substantially improving over the $\tilde{O}(n^{k/2})$ sample complexity typically required under uniform entry sampling for efficient methods. Overall, our results suggest that the statistical-to-computational gap highlighted in Barak and Moitra (2022) is, to a large extent, a consequence of the uniform entry sampling model for tensor completion, and that alternative non-adaptive measurement designs that guarantee a strong initialization can overcome this barrier.

</details>


### [13] [Optimal Bayesian Stopping for Efficient Inference of Consistent LLM Answers](https://arxiv.org/abs/2602.05395)
*Jingkai Huang,Will Ma,Zhengyuan Zhou*

Main category: stat.ML

TL;DR: 提出基于贝叶斯先验的LLM采样停止策略，通过追踪最频繁答案计数来减少采样成本，在保持准确率的同时将LLM调用次数减少达50%


<details>
  <summary>Details</summary>
Motivation: 当前提高LLM准确性的常见策略是采样多个响应并选择最一致的答案，但这种方法采样成本高。本文旨在利用贝叶斯先验信息来减少采样次数，在达到足够一致性时提前停止采样

Method: 提出"L-aggregated"停止策略，通过追踪L-1个最频繁答案的计数来近似计算后验分布。理论上证明L=3即可实现渐进最优性，且计算效率高

Result: 该方法能够用更少的样本识别LLM的最一致答案，在保持相似答案准确率的同时，将LLM调用次数减少高达50%

Conclusion: 基于贝叶斯先验的L-aggregated停止策略是一种高效的方法，能够在减少采样成本的同时保持LLM的推理准确性，为实际应用提供了实用的解决方案

Abstract: A simple strategy for improving LLM accuracy, especially in math and reasoning problems, is to sample multiple responses and submit the answer most consistently reached. In this paper we leverage Bayesian prior information to save on sampling costs, stopping once sufficient consistency is reached. Although the exact posterior is computationally intractable, we further introduce an efficient "L-aggregated" stopping policy that tracks only the L-1 most frequent answer counts. Theoretically, we prove that L=3 is all you need: this coarse approximation is sufficient to achieve asymptotic optimality, and strictly dominates prior-free baselines, while having a fast posterior computation. Empirically, this identifies the most consistent (i.e., mode) LLM answer using fewer samples, and can achieve similar answer accuracy while cutting the number of LLM calls (i.e., saving on LLM inference costs) by up to 50%.

</details>


### [14] [Causal Inference on Stopped Random Walks in Online Advertising](https://arxiv.org/abs/2602.05997)
*Jia Yuan Yu*

Main category: stat.ML

TL;DR: 该论文提出了一种在线广告系统中因果推断的方法，用于估计广告机制参数（如拍卖保留价）对长期效果（如年度广告收入）的影响，考虑了用户交互轨迹和广告主预算约束的变化。


<details>
  <summary>Details</summary>
Motivation: 在线广告系统中，广告机制参数（如拍卖保留价）的调整不仅影响即时收入，还会改变用户交互轨迹和广告主竞价策略（受有限预算约束），甚至影响用户留存。传统i.i.d.假设不适用，需要新的方法来估计长期处理效应。

Method: 采用预算分割实验设计，将测量数据（如广告收入）建模为停止随机游走，结合Anscombe定理、Wald-like方程和中心极限定理，构建长期处理效应的置信区间。

Result: 开发了一种能够处理非i.i.d.数据、考虑用户轨迹变化和广告主预算约束的因果推断框架，为在线广告系统的长期效果评估提供了统计上可靠的置信区间估计方法。

Conclusion: 该方法解决了在线广告系统中长期因果推断的挑战，考虑了机制参数对用户行为和广告主策略的动态影响，为平台优化广告机制提供了有效的评估工具。

Abstract: We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user's interaction-trajectory, and each advertiser's bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.

</details>


### [15] [Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold](https://arxiv.org/abs/2602.06021)
*Ye He,Yitong Qiu,Molei Tao*

Main category: stat.ML

TL;DR: 扩散模型通过"到达-对齐-滑动"过程围绕对数密度脊流形生成数据，训练误差影响法向和切向运动，从而决定生成质量


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型在未记忆训练数据时的泛化机制，量化生成分布以评估下游应用性能

Method: 提出对数密度脊流形概念，分析推理动态的"到达-对齐-滑动"过程，量化法向和切向运动，通过随机特征模型说明归纳偏置来源

Result: 推理动态围绕脊流形进行：先到达流形邻域，然后在法向对齐，最后沿切向滑动；训练误差影响运动方式，决定模态间生成；实验支持低维和高维预测

Conclusion: 扩散模型的生成行为可通过脊流形和推理动态量化，训练动态理解有助于准确量化生成归纳偏置，为模型评估提供理论基础

Abstract: When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model's performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model's inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [16] [Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation](https://arxiv.org/abs/2602.05028)
*Vadim Sokolov,Farnaz Behnia,Dominik Karbowski*

Main category: stat.AP

TL;DR: 提出基于物理信息扩散框架的条件微行程合成方法，用于生成真实车辆速度轨迹，解决传统马尔可夫链方法的离散化伪影和表达能力有限问题。


<details>
  <summary>Details</summary>
Motivation: 合成车辆速度轨迹对于评估车辆控制算法和网联车辆技术至关重要。传统马尔可夫链方法存在离散化伪影和表达能力有限的问题，需要更先进的生成方法。

Method: 提出物理信息扩散框架，结合双通道速度-加速度表示和软物理约束，解决硬约束公式中的优化冲突。比较了1D U-Net架构和基于Transformer的条件分数扩散插补(CSDI)模型，使用6,367个GPS衍生的微行程数据进行训练。

Result: CSDI模型在分布匹配方面表现优异（速度Wasserstein距离0.30，加速度0.026），与真实数据难以区分（判别分数0.49），并在下游能量评估任务中验证了实用性。

Conclusion: 该方法能够为智能交通系统应用生成可扩展的真实驾驶配置文件，无需昂贵的现场数据收集，为车辆控制算法评估提供了有效的合成数据生成解决方案。

Abstract: Synthetic vehicle speed trajectory generation is essential for evaluating vehicle control algorithms and connected vehicle technologies. Traditional Markov chain approaches suffer from discretization artifacts and limited expressiveness. This paper proposes a physics-informed diffusion framework for conditional micro-trip synthesis, combining a dual-channel speed-acceleration representation with soft physics constraints that resolve optimization conflicts inherent to hard-constraint formulations. We compare a 1D U-Net architecture against a transformer-based Conditional Score-based Diffusion Imputation (CSDI) model using 6,367 GPS-derived micro-trips. CSDI achieves superior distribution matching (Wasserstein distance 0.30 for speed, 0.026 for acceleration), strong indistinguishability from real data (discriminative score 0.49), and validated utility for downstream energy assessment tasks. The methodology enables scalable generation of realistic driving profiles for intelligent transportation systems (ITS) applications without costly field data collection.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [17] [Piecewise Deterministic Markov Processes for Bayesian Inference of PDE Coefficients](https://arxiv.org/abs/2602.05559)
*Leon Riccius,Iuri B. C. M. Rocha,Joris Bierkens,Hanne Kekkonen,Frans P. van der Meer*

Main category: stat.CO

TL;DR: 提出一个通用的PDMP采样框架，通过代理辅助细化方案提高非线性反问题中贝叶斯推断的效率，在弹性力学问题中验证了优于传统MCMC方法的性能。


<details>
  <summary>Details</summary>
Motivation: 针对非线性反问题中似然函数计算昂贵的问题，需要开发高效的贝叶斯推断方法。传统MCMC方法在高维问题中效率低下，而PDMP采样器虽然理论上高效，但在实际应用中受到计算成本的限制。

Method: 开发了代理辅助细化框架：使用代理模型提供事件率提议，通过鲁棒校正机制确保真实率的上界。该方法与代理模型和PDMP类型无关，在Zig-Zag采样器和Bouncy粒子采样器中测试了常数、Laplace和高斯过程等代理模型。

Result: 在一维线性弹性力学问题中，基于高斯过程代理的PDMP采样器在每前向模型评估的有效样本量和精度方面显著优于随机游走Metropolis算法和No-U-Turn采样器。Bouncy粒子采样器表现出最佳的整体效率和扩展性。

Conclusion: 提出的PDMP框架为昂贵似然函数的贝叶斯推断提供了高效解决方案，Bouncy粒子采样器与高斯过程代理的组合展现出优越性能，该框架具有超越特定应用的潜力。

Abstract: We develop a general framework for piecewise deterministic Markov process (PDMP) samplers that enables efficient Bayesian inference in non-linear inverse problems with expensive likelihoods. The key ingredient is a surrogate-assisted thinning scheme in which a surrogate model provides a proposal event rate and a robust correction mechanism enforces an upper bound on the true rate by dynamically adjusting an additive offset whenever violations are detected. This construction is agnostic to the choice of surrogate and PDMP, and we demonstrate it for the Zig-Zag sampler and the Bouncy particle sampler with constant, Laplace, and Gaussian process (GP) surrogates, including gradient-informed and adaptively refined GP variants. As a representative application, we consider Bayesian inference of a spatially varying Young's modulus in a one-dimensional linear elasticity problem. Across dimensions, PDMP samplers equipped with GP-based surrogates achieve substantially higher accuracy and effective sample size per forward model evaluation than Random Walk Metropolis algorithm and the No-U-Turn sampler. The Bouncy particle sampler exhibits the most favorable overall efficiency and scaling, illustrating the potential of the proposed PDMP framework beyond this particular setting.

</details>
