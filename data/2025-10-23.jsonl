{"id": "2510.19722", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.19722", "abs": "https://arxiv.org/abs/2510.19722", "authors": ["Sébastien Garneau", "Carlos T. P. Zanini", "Alexandra M. Schmidt"], "title": "Semi-Implicit Approaches for Large-Scale Bayesian Spatial Interpolation", "comment": "36 pages, 5 figures, 2 tables, 1 algorithm", "summary": "Spatial statistics often rely on Gaussian processes (GPs) to capture\ndependencies across locations. However, their computational cost increases\nrapidly with the number of locations, potentially needing multiple hours even\nfor moderate sample sizes. To address this, we propose using Semi-Implicit\nVariational Inference (SIVI), a highly flexible Bayesian approximation method,\nfor scalable Bayesian spatial interpolation. We evaluated SIVI with a GP prior\nand a Nearest-Neighbour Gaussian Process (NNGP) prior compared to Automatic\nDifferentiation Variational Inference (ADVI), Pathfinder, and Hamiltonian Monte\nCarlo (HMC), the reference method in spatial statistics. Methods were compared\nbased on their predictive ability measured by the CRPS, the interval score, and\nthe negative log-predictive density across 50 replicates for both Gaussian and\nPoisson outcomes. SIVI-based methods achieved similar results to HMC, while\nbeing drastically faster. On average, for the Poisson scenario with 500\ntraining locations, SIVI reduced the computational time from roughly 6 hours\nfor HMC to 130 seconds. Furthermore, SIVI-NNGP analyzed a simulated land\nsurface temperature dataset of 150,000 locations while estimating all unknown\nmodel parameters in under two minutes. These results highlight the potential of\nSIVI as a flexible and scalable inference technique in spatial statistics."}
{"id": "2510.19133", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.19133", "abs": "https://arxiv.org/abs/2510.19133", "authors": ["Geonhee Han", "Andrew Gelman", "Aki Vehtari"], "title": "Efficient scenario analysis in real-time Bayesian election forecasting via sequential meta-posterior sampling", "comment": null, "summary": "Bayesian aggregation lets election forecasters combine diverse sources of\ninformation, such as state polls and economic and political indicators: as in\nour collaboration with The Economist magazine. However, the demands of\nreal-time posterior updating, model checking, and communication introduce\npractical methodological challenges. In particular, sensitivity and scenario\nanalysis help trace forecast shifts to model assumptions and understand model\nbehavior. Yet, under standard Markov chain Monte Carlo, even small tweaks to\nthe model (e.g., in priors, data, hyperparameters) require full refitting,\nmaking such real-time analysis computationally expensive. To overcome the\nbottleneck, we introduce a meta-modeling strategy paired with a sequential\nsampling scheme; by traversing posterior meta-models, we enable real-time\ninference and structured scenario and sensitivity analysis without repeated\nrefitting. In a back-test of the model, we show substantial computational gains\nand uncover non-trivial sensitivity patterns. For example, forecasts remain\nresponsive to prior confidence in fundamentals-based forecasts, but less so to\nrandom walk scale; these help clarify the relative influence of polling data\nversus structural assumptions. Code is available at\nhttps://github.com/geonhee619/SMC-Sense."}
{"id": "2510.18903", "categories": ["stat.ME", "math.ST", "q-fin.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.18903", "abs": "https://arxiv.org/abs/2510.18903", "authors": ["Harrison Katz"], "title": "Centered MA Dirichlet ARMA for Financial Compositions: Theory & Empirical Evidence", "comment": null, "summary": "Observation-driven Dirichlet models for compositional time series often use\nthe additive log-ratio (ALR) link and include a moving-average (MA) term built\nfrom ALR residuals. In the standard B--DARMA recursion, the usual MA regressor\n$\\alr(\\mathbf{Y}_t)-\\boldsymbol{\\eta}_t$ has nonzero conditional mean under the\nDirichlet likelihood, which biases the mean path and blurs the interpretation\nof MA coefficients. We propose a minimal change: replace the raw regressor with\na \\emph{centered} innovation\n$\\boldsymbol{\\epsilon}_t^{\\circ}=\\alr(\\mathbf{Y}_t)-\\mathbb{E}\\{\\alr(\\mathbf{Y}_t)\\mid\n\\boldsymbol{\\eta}_t,\\phi_t\\}$, computable in closed form via digamma functions.\nCentering restores mean-zero innovations for the MA block without altering\neither the likelihood or the ALR link. We provide simple identities for the\nconditional mean and the forecast recursion, show first-order equivalence to a\ndigamma-link DARMA while retaining a closed-form inverse to\n$\\boldsymbol{\\mu}_t$, and give ready-to-use code. A weekly application to the\nFederal Reserve H.8 bank-asset composition compares the original (raw-MA) and\ncentered specifications under a fixed holdout and rolling one-step origins. The\ncentered formulation improves log predictive scores with essentially identical\npoint error and markedly cleaner Hamiltonian Monte Carlo diagnostics."}
{"id": "2510.19020", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19020", "abs": "https://arxiv.org/abs/2510.19020", "authors": ["Yixuan Florence Wu", "Yilun Zhu", "Lei Cao and", "Naichen Shi"], "title": "Calibrated Principal Component Regression", "comment": null, "summary": "We propose a new method for statistical inference in generalized linear\nmodels. In the overparameterized regime, Principal Component Regression (PCR)\nreduces variance by projecting high-dimensional data to a low-dimensional\nprincipal subspace before fitting. However, PCR incurs truncation bias whenever\nthe true regression vector has mass outside the retained principal components\n(PC). To mitigate the bias, we propose Calibrated Principal Component\nRegression (CPCR), which first learns a low-variance prior in the PC subspace\nand then calibrates the model in the original feature space via a centered\nTikhonov step. CPCR leverages cross-fitting and controls the truncation bias by\nsoftening PCR's hard cutoff. Theoretically, we calculate the out-of-sample risk\nin the random matrix regime, which shows that CPCR outperforms standard PCR\nwhen the regression signal has non-negligible components in low-variance\ndirections. Empirically, CPCR consistently improves prediction across multiple\noverparameterized problems. The results highlight CPCR's stability and\nflexibility in modern overparameterized settings."}
{"id": "2510.19011", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.19011", "abs": "https://arxiv.org/abs/2510.19011", "authors": ["Yaoyuan Vincent Tan", "Gang Xu", "Chenkun Wang"], "title": "Multiple Imputation for Small, Extremely High Efficacy Clinical Trials with Binary Endpoints", "comment": "15 pages, 1 table, 5 figures", "summary": "There has been an increasing interest in using cell and gene therapy (CGT) to\ntreat/cure difficult diseases. The hallmark of CGT trials are the small sample\nsize and extremely high efficacy. Due to the innovation and novelty of such\ntherapies, when there is missing data, more scrutiny is exercised, and\nregulators often request for missing data handling strategy when missing data\noccurs. Often, multiple imputation (MI) will be used. MI for continuous\nendpoint is well established but literature of MI for binary endpoint is\nlacking. In this work, we compare and develop 3 new methods to handle missing\ndata using MI for binary endpoints when the sample size is small and efficacy\nextremely high. The parameter of interest is population proportion of success.\nWe show that our proposed methods performed well and produced good 95%\ncoverage. We also applied our methods to an actual clinical study, the Clinical\nIslet Transplantation (CIT) Protocol 07, conducted by National Institutes of\nHealth (NIH)."}
{"id": "2510.19094", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.19094", "abs": "https://arxiv.org/abs/2510.19094", "authors": ["Jaewon Lim", "Alex Luedtke"], "title": "Estimation of causal dose-response functions under data fusion", "comment": null, "summary": "Estimating the causal dose-response function is challenging, particularly\nwhen data from a single source are insufficient to estimate responses precisely\nacross all exposure levels. To overcome this limitation, we propose a data\nfusion framework that leverages multiple data sources that are partially\naligned with the target distribution. Specifically, we derive a\nNeyman-orthogonal loss function tailored for estimating the dose-response\nfunction within data fusion settings. To improve computational efficiency, we\npropose a stochastic approximation that retains orthogonality. We apply kernel\nridge regression with this approximation, which provides closed-form\nestimators. Our theoretical analysis demonstrates that incorporating additional\ndata sources yields tighter finite-sample regret bounds and improved worst-case\nperformance, as confirmed via minimax lower bound comparison. Simulation\nstudies validate the practical advantages of our approach, showing improved\nestimation accuracy when employing data fusion. This study highlights the\npotential of data fusion for estimating non-smooth parameters such as causal\ndose-response functions."}
{"id": "2510.19110", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.19110", "abs": "https://arxiv.org/abs/2510.19110", "authors": ["Archer Dodson", "Ritabrata Dutta"], "title": "Signature Kernel Scoring Rule as Spatio-Temporal Diagnostic for Probabilistic Forecasting", "comment": null, "summary": "Modern weather forecasting has increasingly transitioned from numerical\nweather prediction (NWP) to data-driven machine learning forecasting\ntechniques. While these new models produce probabilistic forecasts to quantify\nuncertainty, their training and evaluation may remain hindered by conventional\nscoring rules, primarily MSE, which ignore the highly correlated data\nstructures present in weather and atmospheric systems. This work introduces the\nsignature kernel scoring rule, grounded in rough path theory, which reframes\nweather variables as continuous paths to encode temporal and spatial\ndependencies through iterated integrals. Validated as strictly proper through\nthe use of path augmentations to guarantee uniqueness, the signature kernel\nprovides a theoretically robust metric for forecast verification and model\ntraining. Empirical evaluations through weather scorecards on WeatherBench 2\nmodels demonstrate the signature kernel scoring rule's high discriminative\npower and unique capacity to capture path-dependent interactions. Following\nprevious demonstration of successful adversarial-free probabilistic training,\nwe train sliding window generative neural networks using a\npredictive-sequential scoring rule on ERA5 reanalysis weather data. Using a\nlightweight model, we demonstrate that signature kernel based training\noutperforms climatology for forecast paths of up to fifteen timesteps."}
{"id": "2510.19077", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.19077", "abs": "https://arxiv.org/abs/2510.19077", "authors": ["Rebecca K. Metcalfe", "Nathaniel Dyrkton", "Yichen Yan", "Shomoita Alam", "Susan Shepherd", "Ibrahim Sana", "Kevin Phelan", "Jay JH Park"], "title": "Simulation-Guided Planning of a Target Trial Emulated Cluster Randomized Trial for Mass Small-Quantity Lipid Nutrient Supplementation Combined with Expanded Program on Immunization in Rural Niger", "comment": null, "summary": "While target trial emulation (TTE) is increasingly used to improve the\nanalysis of non-randomized studies by applying trial design principles, TTE\napplications to emulate cluster randomized trials (RCTs) have been limited. We\nperformed simulations to prospectively plan data collection of a non-randomized\nstudy intended to emulate a village-level cluster RCT when\ncluster-randomization was infeasible. The planned study will assess the impact\nof mass distribution of nutritional supplements embedded within an existing\nimmunization program to improve pentavalent vaccination rates among children\n12-24 months old in Niger. The design included covariate-constrained random\nselection of villages for outcome ascertainment at follow-up. Simulations used\nbaseline census data on pentavalent vaccination rates and cluster-level\ncovariates to compare the type I error rate and power of four statistical\nmethods: beta-regression; quasi-binomial regression; inverse probability of\ntreatment weighting (IPTW); and na\\\"ive Wald test. Of these methods, only IPTW\nand beta-regression controlled the type I error rate at 0.05, but IPTW yielded\npoor statistical power. Beta-regression, which showed adequate statistical\npower, was chosen as our primary analysis. Adopting simulation-guided design\nprinciples within TTE can enable robust planning of a group-level\nnon-randomized study emulating a cluster RCT. Lessons from this study also\napply to TTE planning of individually-RCTs."}
{"id": "2510.19528", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.19528", "abs": "https://arxiv.org/abs/2510.19528", "authors": ["Sebastian Reboul", "Hélène Halconruy", "Randal Douc"], "title": "Learning Upper Lower Value Envelopes to Shape Online RL: A Principled Approach", "comment": "32 pages, 5 figures", "summary": "We investigate the fundamental problem of leveraging offline data to\naccelerate online reinforcement learning - a direction with strong potential\nbut limited theoretical grounding. Our study centers on how to learn and apply\nvalue envelopes within this context. To this end, we introduce a principled\ntwo-stage framework: the first stage uses offline data to derive upper and\nlower bounds on value functions, while the second incorporates these learned\nbounds into online algorithms. Our method extends prior work by decoupling the\nupper and lower bounds, enabling more flexible and tighter approximations. In\ncontrast to approaches that rely on fixed shaping functions, our envelopes are\ndata-driven and explicitly modeled as random variables, with a filtration\nargument ensuring independence across phases. The analysis establishes\nhigh-probability regret bounds determined by two interpretable quantities,\nthereby providing a formal bridge between offline pre-training and online\nfine-tuning. Empirical results on tabular MDPs demonstrate substantial regret\nreductions compared with both UCBVI and prior methods."}
{"id": "2510.19161", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.DS", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.19161", "abs": "https://arxiv.org/abs/2510.19161", "authors": ["Kai Chang", "Themistoklis P. Sapsis"], "title": "Extreme Event Aware ($η$-) Learning", "comment": "Minor revisions at PNAS", "summary": "Quantifying and predicting rare and extreme events persists as a crucial yet\nchallenging task in understanding complex dynamical systems. Many practical\nchallenges arise from the infrequency and severity of these events, including\nthe considerable variance of simple sampling methods and the substantial\ncomputational cost of high-fidelity numerical simulations. Numerous data-driven\nmethods have recently been developed to tackle these challenges. However, a\ntypical assumption for the success of these methods is the occurrence of\nmultiple extreme events, either within the training dataset or during the\nsampling process. This leads to accurate models in regions of quiescent events\nbut with high epistemic uncertainty in regions associated with extremes. To\novercome this limitation, we introduce Extreme Event Aware (e2a or eta) or\n$\\eta$-learning which does not assume the existence of extreme events in the\navailable data. $\\eta$-learning reduces the uncertainty even in `uncharted'\nextreme event regions, by enforcing the extreme event statistics of an\nobservable indicative of extremeness during training, which can be available\nthrough qualitative arguments or estimated with unlabeled data. This type of\nstatistical regularization results in models that fit the observed data, while\nenforcing consistency with the prescribed observable statistics, enabling the\ngeneration of unprecedented extreme events even when the training data lack\nextremes therein. Theoretical results based on optimal transport offer a\nrigorous justification and highlight the optimality of the introduced method.\nAdditionally, extensive numerical experiments illustrate the favorable\nproperties of the $\\eta$-learning framework on several prototype problems and\nreal-world precipitation downscaling problems."}
{"id": "2510.19785", "categories": ["stat.AP", "62P20"], "pdf": "https://arxiv.org/pdf/2510.19785", "abs": "https://arxiv.org/abs/2510.19785", "authors": ["Mengxiang zhu", "Riccardo Rastelli"], "title": "Green Finance and Carbon Emissions: A Nonlinear and Interaction Analysis Using Bayesian Additive Regression Trees", "comment": "16 pages, 8 figures, pre-print article", "summary": "As a core policy tool for China in addressing climate risks, green finance\nplays a strategically important role in shaping carbon mitigation outcomes.\nThis study investigates the nonlinear and interaction effects of green finance\non carbon emission intensity (CEI) using Chinese provincial panel data from\n2000 to 2022. The Climate Physical Risk Index (CPRI) is incorporated into the\nanalytical framework to assess its potential role in shaping carbon outcomes.\nWe employ Bayesian Additive Regression Trees (BART) to capture complex\nnonlinear relationships and interaction pathways, and use SHapley Additive\nexPlanations values to enhance model interpretability. Results show that the\nGreen Finance Index (GFI) has a statistically significant inverted U-shaped\neffect on CEI, with notable regional heterogeneity. Contrary to expectations,\nCPRI does not show a significant impact on carbon emissions. Further analysis\nreveals that in high energy consumption scenarios, stronger green finance\ndevelopment contributes to lower CEI. These findings highlight the potential of\ngreen finance as an effective instrument for carbon intensity reduction,\nespecially in energy-intensive contexts, and underscore the importance of\naccounting for nonlinear effects and regional disparities when designing and\nimplementing green financial policies."}
{"id": "2510.19583", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.19583", "abs": "https://arxiv.org/abs/2510.19583", "authors": ["Subhrajyoty Roy", "Abhik Ghosh", "Ayanendranath Basu"], "title": "Robust Rank Estimation for Noisy Matrices", "comment": null, "summary": "Estimating the true rank of a noisy data matrix is a fundamental problem\nunderlying techniques such as principal component analysis, matrix completion,\netc. Existing rank estimation criteria, including information-based and\ncross-validation methods, are either highly sensitive to outliers or\ncomputationally demanding when combined with robust estimators. This paper\nproposes a new criterion, the Divergence Information Criterion for Matrix Rank\n(DICMR), that achieves both robustness and computational simplicity. Derived\nfrom the density power divergence framework, DICMR inherits the robustness\nproperties while being computationally very simple. We provide asymptotic\nbounds on its overestimation and underestimation probabilities, and demonstrate\nfirst-order B-robustness of the criteria. Extensive simulations show that DICMR\ndelivers accuracy comparable to the robustified cross-validation methods, but\nwith far lower computational cost. We also showcase a real-data application to\nmicroarray imputation to further demonstrate its practical utility,\noutperforming several state-of-the-art algorithms."}
{"id": "2510.19306", "categories": ["stat.ML", "cs.LG", "econ.GN", "q-fin.EC", "stat.AP", "55N31, 62H30, 91B84, 91G70, 68T09", "G.2.3; I.5.3; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.19306", "abs": "https://arxiv.org/abs/2510.19306", "authors": ["Pattravadee de Favereau de Jeneret", "Ioannis Diamantis"], "title": "Topology of Currencies: Persistent Homology for FX Co-movements: A Comparative Clustering Study", "comment": "26 pages, 17 figures, the results were presented at the 5th MORSE\n  Conference, Maastricht University (October 2025)", "summary": "This study investigates whether Topological Data Analysis (TDA) can provide\nadditional insights beyond traditional statistical methods in clustering\ncurrency behaviours. We focus on the foreign exchange (FX) market, which is a\ncomplex system often exhibiting non-linear and high-dimensional dynamics that\nclassical techniques may not fully capture. We compare clustering results based\non TDA-derived features versus classical statistical features using monthly\nlogarithmic returns of 13 major currency exchange rates (all against the euro).\nTwo widely-used clustering algorithms, \\(k\\)-means and Hierarchical clustering,\nare applied on both types of features, and cluster quality is evaluated via the\nSilhouette score and the Calinski-Harabasz index. Our findings show that\nTDA-based feature clustering produces more compact and well-separated clusters\nthan clustering on traditional statistical features, particularly achieving\nsubstantially higher Calinski-Harabasz scores. However, all clustering\napproaches yield modest Silhouette scores, underscoring the inherent difficulty\nof grouping FX time series. The differing cluster compositions under TDA vs.\nclassical features suggest that TDA captures structural patterns in currency\nco-movements that conventional methods might overlook. These results highlight\nTDA as a valuable complementary tool for analysing financial time series, with\npotential applications in risk management where understanding structural\nco-movements is crucial."}
{"id": "2510.19108", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.19108", "abs": "https://arxiv.org/abs/2510.19108", "authors": ["Andrea Sottosanti", "Davide Risso", "Francesco Denti"], "title": "Spatially Regularized Gaussian Mixtures for Clustering Spatial Transcriptomic Data", "comment": "to be published in Journal of Classification", "summary": "Spatial transcriptomics measures the expression of thousands of genes in a\ntissue sample while preserving its spatial structure. This class of\ntechnologies has enabled the investigation of the spatial variation of gene\nexpressions and their impact on specific biological processes. Identifying\ngenes with similar expression profiles is of utmost importance, thus motivating\nthe development of flexible methods leveraging spatial data structure to\ncluster genes. Here, we propose a modeling framework for clustering\nobservations measured over numerous spatial locations via Gaussian processes.\nRather than specifying their covariance kernels as a function of the spatial\nstructure, we use it to inform a generalized Cholesky decomposition of their\nprecision matrices. This approach prevents issues with kernel misspecification\nand facilitates the estimation of a non-stationarity spatial covariance\nstructure. Applied to spatial transcriptomic data, our model identifies gene\nclusters with distinctive spatial correlation patterns across tissue areas\ncomprising different cell types, like tumoral and stromal areas."}
{"id": "2510.19334", "categories": ["stat.ML", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19334", "abs": "https://arxiv.org/abs/2510.19334", "authors": ["Cuize Han", "Sesh Jalagam"], "title": "Metadata Extraction Leveraging Large Language Models", "comment": null, "summary": "The advent of Large Language Models has revolutionized tasks across domains,\nincluding the automation of legal document analysis, a critical component of\nmodern contract management systems. This paper presents a comprehensive\nimplementation of LLM-enhanced metadata extraction for contract review,\nfocusing on the automatic detection and annotation of salient legal clauses.\nLeveraging both the publicly available Contract Understanding Atticus Dataset\n(CUAD) and proprietary contract datasets, our work demonstrates the integration\nof advanced LLM methodologies with practical applications. We identify three\npivotal elements for optimizing metadata extraction: robust text conversion,\nstrategic chunk selection, and advanced LLM-specific techniques, including\nChain of Thought (CoT) prompting and structured tool calling. The results from\nour experiments highlight the substantial improvements in clause identification\naccuracy and efficiency. Our approach shows promise in reducing the time and\ncost associated with contract review while maintaining high accuracy in legal\nclause identification. The results suggest that carefully optimized LLM systems\ncould serve as valuable tools for legal professionals, potentially increasing\naccess to efficient contract review services for organizations of all sizes."}
{"id": "2510.19110", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.19110", "abs": "https://arxiv.org/abs/2510.19110", "authors": ["Archer Dodson", "Ritabrata Dutta"], "title": "Signature Kernel Scoring Rule as Spatio-Temporal Diagnostic for Probabilistic Forecasting", "comment": null, "summary": "Modern weather forecasting has increasingly transitioned from numerical\nweather prediction (NWP) to data-driven machine learning forecasting\ntechniques. While these new models produce probabilistic forecasts to quantify\nuncertainty, their training and evaluation may remain hindered by conventional\nscoring rules, primarily MSE, which ignore the highly correlated data\nstructures present in weather and atmospheric systems. This work introduces the\nsignature kernel scoring rule, grounded in rough path theory, which reframes\nweather variables as continuous paths to encode temporal and spatial\ndependencies through iterated integrals. Validated as strictly proper through\nthe use of path augmentations to guarantee uniqueness, the signature kernel\nprovides a theoretically robust metric for forecast verification and model\ntraining. Empirical evaluations through weather scorecards on WeatherBench 2\nmodels demonstrate the signature kernel scoring rule's high discriminative\npower and unique capacity to capture path-dependent interactions. Following\nprevious demonstration of successful adversarial-free probabilistic training,\nwe train sliding window generative neural networks using a\npredictive-sequential scoring rule on ERA5 reanalysis weather data. Using a\nlightweight model, we demonstrate that signature kernel based training\noutperforms climatology for forecast paths of up to fifteen timesteps."}
{"id": "2510.19372", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19372", "abs": "https://arxiv.org/abs/2510.19372", "authors": ["Corentin Pla", "Hugo Richard", "Marc Abeille", "Nadav Merlis", "Vianney Perchet"], "title": "On the hardness of RL with Lookahead", "comment": null, "summary": "We study reinforcement learning (RL) with transition look-ahead, where the\nagent may observe which states would be visited upon playing any sequence of\n$\\ell$ actions before deciding its course of action. While such predictive\ninformation can drastically improve the achievable performance, we show that\nusing this information optimally comes at a potentially prohibitive\ncomputational cost. Specifically, we prove that optimal planning with one-step\nlook-ahead ($\\ell=1$) can be solved in polynomial time through a novel linear\nprogramming formulation. In contrast, for $\\ell \\geq 2$, the problem becomes\nNP-hard. Our results delineate a precise boundary between tractable and\nintractable cases for the problem of planning with transition look-ahead in\nreinforcement learning."}
{"id": "2510.19133", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.19133", "abs": "https://arxiv.org/abs/2510.19133", "authors": ["Geonhee Han", "Andrew Gelman", "Aki Vehtari"], "title": "Efficient scenario analysis in real-time Bayesian election forecasting via sequential meta-posterior sampling", "comment": null, "summary": "Bayesian aggregation lets election forecasters combine diverse sources of\ninformation, such as state polls and economic and political indicators: as in\nour collaboration with The Economist magazine. However, the demands of\nreal-time posterior updating, model checking, and communication introduce\npractical methodological challenges. In particular, sensitivity and scenario\nanalysis help trace forecast shifts to model assumptions and understand model\nbehavior. Yet, under standard Markov chain Monte Carlo, even small tweaks to\nthe model (e.g., in priors, data, hyperparameters) require full refitting,\nmaking such real-time analysis computationally expensive. To overcome the\nbottleneck, we introduce a meta-modeling strategy paired with a sequential\nsampling scheme; by traversing posterior meta-models, we enable real-time\ninference and structured scenario and sensitivity analysis without repeated\nrefitting. In a back-test of the model, we show substantial computational gains\nand uncover non-trivial sensitivity patterns. For example, forecasts remain\nresponsive to prior confidence in fundamentals-based forecasts, but less so to\nrandom walk scale; these help clarify the relative influence of polling data\nversus structural assumptions. Code is available at\nhttps://github.com/geonhee619/SMC-Sense."}
{"id": "2510.19374", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19374", "abs": "https://arxiv.org/abs/2510.19374", "authors": ["Maxime van Cutsem", "Sylvain Sardy"], "title": "Square root Cox's survival analysis by the fittest linear and neural networks model", "comment": null, "summary": "We revisit Cox's proportional hazard models and LASSO in the aim of improving\nfeature selection in survival analysis. Unlike traditional methods relying on\ncross-validation or BIC, the penalty parameter $\\lambda$ is directly tuned for\nfeature selection and is asymptotically pivotal thanks to taking the square\nroot of Cox's partial likelihood. Substantially improving over both\ncross-validation LASSO and BIC subset selection, our approach has a phase\ntransition on the probability of retrieving all and only the good features,\nlike in compressed sensing. The method can be employed by linear models but\nalso by artificial neural networks."}
{"id": "2510.19306", "categories": ["stat.ML", "cs.LG", "econ.GN", "q-fin.EC", "stat.AP", "55N31, 62H30, 91B84, 91G70, 68T09", "G.2.3; I.5.3; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.19306", "abs": "https://arxiv.org/abs/2510.19306", "authors": ["Pattravadee de Favereau de Jeneret", "Ioannis Diamantis"], "title": "Topology of Currencies: Persistent Homology for FX Co-movements: A Comparative Clustering Study", "comment": "26 pages, 17 figures, the results were presented at the 5th MORSE\n  Conference, Maastricht University (October 2025)", "summary": "This study investigates whether Topological Data Analysis (TDA) can provide\nadditional insights beyond traditional statistical methods in clustering\ncurrency behaviours. We focus on the foreign exchange (FX) market, which is a\ncomplex system often exhibiting non-linear and high-dimensional dynamics that\nclassical techniques may not fully capture. We compare clustering results based\non TDA-derived features versus classical statistical features using monthly\nlogarithmic returns of 13 major currency exchange rates (all against the euro).\nTwo widely-used clustering algorithms, \\(k\\)-means and Hierarchical clustering,\nare applied on both types of features, and cluster quality is evaluated via the\nSilhouette score and the Calinski-Harabasz index. Our findings show that\nTDA-based feature clustering produces more compact and well-separated clusters\nthan clustering on traditional statistical features, particularly achieving\nsubstantially higher Calinski-Harabasz scores. However, all clustering\napproaches yield modest Silhouette scores, underscoring the inherent difficulty\nof grouping FX time series. The differing cluster compositions under TDA vs.\nclassical features suggest that TDA captures structural patterns in currency\nco-movements that conventional methods might overlook. These results highlight\nTDA as a valuable complementary tool for analysing financial time series, with\npotential applications in risk management where understanding structural\nco-movements is crucial."}
{"id": "2510.19382", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19382", "abs": "https://arxiv.org/abs/2510.19382", "authors": ["Nikos Tsikouras", "Yorgos Pantis", "Ioannis Mitliagkas", "Christos Tzamos"], "title": "A Derandomization Framework for Structure Discovery: Applications in Neural Networks and Beyond", "comment": null, "summary": "Understanding the dynamics of feature learning in neural networks (NNs)\nremains a significant challenge. The work of (Mousavi-Hosseini et al., 2023)\nanalyzes a multiple index teacher-student setting and shows that a two-layer\nstudent attains a low-rank structure in its first-layer weights when trained\nwith stochastic gradient descent (SGD) and a strong regularizer. This\nstructural property is known to reduce sample complexity of generalization.\nIndeed, in a second step, the same authors establish algorithm-specific\nlearning guarantees under additional assumptions. In this paper, we focus\nexclusively on the structure discovery aspect and study it under weaker\nassumptions, more specifically: we allow (a) NNs of arbitrary size and depth,\n(b) with all parameters trainable, (c) under any smooth loss function, (d) tiny\nregularization, and (e) trained by any method that attains a second-order\nstationary point (SOSP), e.g.\\ perturbed gradient descent (PGD). At the core of\nour approach is a key $\\textit{derandomization}$ lemma, which states that\noptimizing the function $\\mathbb{E}_{\\mathbf{x}}\n\\left[g_{\\theta}(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\\right]$ converges to a\npoint where $\\mathbf{W} = \\mathbf{0}$, under mild conditions. The fundamental\nnature of this lemma directly explains structure discovery and has immediate\napplications in other domains including an end-to-end approximation for MAXCUT,\nand computing Johnson-Lindenstrauss embeddings."}
{"id": "2510.19583", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.19583", "abs": "https://arxiv.org/abs/2510.19583", "authors": ["Subhrajyoty Roy", "Abhik Ghosh", "Ayanendranath Basu"], "title": "Robust Rank Estimation for Noisy Matrices", "comment": null, "summary": "Estimating the true rank of a noisy data matrix is a fundamental problem\nunderlying techniques such as principal component analysis, matrix completion,\netc. Existing rank estimation criteria, including information-based and\ncross-validation methods, are either highly sensitive to outliers or\ncomputationally demanding when combined with robust estimators. This paper\nproposes a new criterion, the Divergence Information Criterion for Matrix Rank\n(DICMR), that achieves both robustness and computational simplicity. Derived\nfrom the density power divergence framework, DICMR inherits the robustness\nproperties while being computationally very simple. We provide asymptotic\nbounds on its overestimation and underestimation probabilities, and demonstrate\nfirst-order B-robustness of the criteria. Extensive simulations show that DICMR\ndelivers accuracy comparable to the robustified cross-validation methods, but\nwith far lower computational cost. We also showcase a real-data application to\nmicroarray imputation to further demonstrate its practical utility,\noutperforming several state-of-the-art algorithms."}
{"id": "2510.19528", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.19528", "abs": "https://arxiv.org/abs/2510.19528", "authors": ["Sebastian Reboul", "Hélène Halconruy", "Randal Douc"], "title": "Learning Upper Lower Value Envelopes to Shape Online RL: A Principled Approach", "comment": "32 pages, 5 figures", "summary": "We investigate the fundamental problem of leveraging offline data to\naccelerate online reinforcement learning - a direction with strong potential\nbut limited theoretical grounding. Our study centers on how to learn and apply\nvalue envelopes within this context. To this end, we introduce a principled\ntwo-stage framework: the first stage uses offline data to derive upper and\nlower bounds on value functions, while the second incorporates these learned\nbounds into online algorithms. Our method extends prior work by decoupling the\nupper and lower bounds, enabling more flexible and tighter approximations. In\ncontrast to approaches that rely on fixed shaping functions, our envelopes are\ndata-driven and explicitly modeled as random variables, with a filtration\nargument ensuring independence across phases. The analysis establishes\nhigh-probability regret bounds determined by two interpretable quantities,\nthereby providing a formal bridge between offline pre-training and online\nfine-tuning. Empirical results on tabular MDPs demonstrate substantial regret\nreductions compared with both UCBVI and prior methods."}
{"id": "2510.18903", "categories": ["stat.ME", "math.ST", "q-fin.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.18903", "abs": "https://arxiv.org/abs/2510.18903", "authors": ["Harrison Katz"], "title": "Centered MA Dirichlet ARMA for Financial Compositions: Theory & Empirical Evidence", "comment": null, "summary": "Observation-driven Dirichlet models for compositional time series often use\nthe additive log-ratio (ALR) link and include a moving-average (MA) term built\nfrom ALR residuals. In the standard B--DARMA recursion, the usual MA regressor\n$\\alr(\\mathbf{Y}_t)-\\boldsymbol{\\eta}_t$ has nonzero conditional mean under the\nDirichlet likelihood, which biases the mean path and blurs the interpretation\nof MA coefficients. We propose a minimal change: replace the raw regressor with\na \\emph{centered} innovation\n$\\boldsymbol{\\epsilon}_t^{\\circ}=\\alr(\\mathbf{Y}_t)-\\mathbb{E}\\{\\alr(\\mathbf{Y}_t)\\mid\n\\boldsymbol{\\eta}_t,\\phi_t\\}$, computable in closed form via digamma functions.\nCentering restores mean-zero innovations for the MA block without altering\neither the likelihood or the ALR link. We provide simple identities for the\nconditional mean and the forecast recursion, show first-order equivalence to a\ndigamma-link DARMA while retaining a closed-form inverse to\n$\\boldsymbol{\\mu}_t$, and give ready-to-use code. A weekly application to the\nFederal Reserve H.8 bank-asset composition compares the original (raw-MA) and\ncentered specifications under a fixed holdout and rolling one-step origins. The\ncentered formulation improves log predictive scores with essentially identical\npoint error and markedly cleaner Hamiltonian Monte Carlo diagnostics."}
{"id": "2510.19094", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.19094", "abs": "https://arxiv.org/abs/2510.19094", "authors": ["Jaewon Lim", "Alex Luedtke"], "title": "Estimation of causal dose-response functions under data fusion", "comment": null, "summary": "Estimating the causal dose-response function is challenging, particularly\nwhen data from a single source are insufficient to estimate responses precisely\nacross all exposure levels. To overcome this limitation, we propose a data\nfusion framework that leverages multiple data sources that are partially\naligned with the target distribution. Specifically, we derive a\nNeyman-orthogonal loss function tailored for estimating the dose-response\nfunction within data fusion settings. To improve computational efficiency, we\npropose a stochastic approximation that retains orthogonality. We apply kernel\nridge regression with this approximation, which provides closed-form\nestimators. Our theoretical analysis demonstrates that incorporating additional\ndata sources yields tighter finite-sample regret bounds and improved worst-case\nperformance, as confirmed via minimax lower bound comparison. Simulation\nstudies validate the practical advantages of our approach, showing improved\nestimation accuracy when employing data fusion. This study highlights the\npotential of data fusion for estimating non-smooth parameters such as causal\ndose-response functions."}
{"id": "2510.19108", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.19108", "abs": "https://arxiv.org/abs/2510.19108", "authors": ["Andrea Sottosanti", "Davide Risso", "Francesco Denti"], "title": "Spatially Regularized Gaussian Mixtures for Clustering Spatial Transcriptomic Data", "comment": "to be published in Journal of Classification", "summary": "Spatial transcriptomics measures the expression of thousands of genes in a\ntissue sample while preserving its spatial structure. This class of\ntechnologies has enabled the investigation of the spatial variation of gene\nexpressions and their impact on specific biological processes. Identifying\ngenes with similar expression profiles is of utmost importance, thus motivating\nthe development of flexible methods leveraging spatial data structure to\ncluster genes. Here, we propose a modeling framework for clustering\nobservations measured over numerous spatial locations via Gaussian processes.\nRather than specifying their covariance kernels as a function of the spatial\nstructure, we use it to inform a generalized Cholesky decomposition of their\nprecision matrices. This approach prevents issues with kernel misspecification\nand facilitates the estimation of a non-stationarity spatial covariance\nstructure. Applied to spatial transcriptomic data, our model identifies gene\nclusters with distinctive spatial correlation patterns across tissue areas\ncomprising different cell types, like tumoral and stromal areas."}
{"id": "2510.19133", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.19133", "abs": "https://arxiv.org/abs/2510.19133", "authors": ["Geonhee Han", "Andrew Gelman", "Aki Vehtari"], "title": "Efficient scenario analysis in real-time Bayesian election forecasting via sequential meta-posterior sampling", "comment": null, "summary": "Bayesian aggregation lets election forecasters combine diverse sources of\ninformation, such as state polls and economic and political indicators: as in\nour collaboration with The Economist magazine. However, the demands of\nreal-time posterior updating, model checking, and communication introduce\npractical methodological challenges. In particular, sensitivity and scenario\nanalysis help trace forecast shifts to model assumptions and understand model\nbehavior. Yet, under standard Markov chain Monte Carlo, even small tweaks to\nthe model (e.g., in priors, data, hyperparameters) require full refitting,\nmaking such real-time analysis computationally expensive. To overcome the\nbottleneck, we introduce a meta-modeling strategy paired with a sequential\nsampling scheme; by traversing posterior meta-models, we enable real-time\ninference and structured scenario and sensitivity analysis without repeated\nrefitting. In a back-test of the model, we show substantial computational gains\nand uncover non-trivial sensitivity patterns. For example, forecasts remain\nresponsive to prior confidence in fundamentals-based forecasts, but less so to\nrandom walk scale; these help clarify the relative influence of polling data\nversus structural assumptions. Code is available at\nhttps://github.com/geonhee619/SMC-Sense."}
{"id": "2510.19154", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.19154", "abs": "https://arxiv.org/abs/2510.19154", "authors": ["George Stefan", "Eleanor Pullenayegum"], "title": "Inverse-intensity weighted generalized estimating equations with irregularly measured longitudinal data and informative dropout", "comment": "34 pages with 13 figures", "summary": "Longitudinal data are commonly encountered in biomedical research, including\nrandomized trials and retrospective cohort studies. Subjects are typically\nfollowed over a period of time and may be scheduled for follow-up at\npre-determined time points. However, subjects may miss their appointments or\nreturn at non-specified times, leading to irregularity in the visit process.\nIIW-GEEs have been developed as one method to account for this irregularity,\nwhereby estimates from a visit intensity model are used as weights in a GEE\nmodel with an independent correlation structure. We show that currently\navailable methods can be biased for situations in which the health outcome of\ninterest may influence a subject's dropout from the study. We have extended the\nIIW-GEE framework to adjust for informative dropout and have demonstrated via\nsimulation studies that this bias can be significantly reduced. We have\nillustrated this method using the STAR*D clinical trial data, and observed that\nthe disease trajectory was generally overestimated when informative dropout was\nnot accounted for."}
{"id": "2510.19212", "categories": ["stat.ME", "cs.AI", "62-02, 68T01", "G.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.19212", "abs": "https://arxiv.org/abs/2510.19212", "authors": ["Ernest Fokoué"], "title": "No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence", "comment": "37 pages, 6 figures", "summary": "The rapid ascent of artificial intelligence (AI) is often portrayed as a\nrevolution born from computer science and engineering. This narrative, however,\nobscures a fundamental truth: the theoretical and methodological core of AI is,\nand has always been, statistical. This paper systematically argues that the\nfield of statistics provides the indispensable foundation for machine learning\nand modern AI. We deconstruct AI into nine foundational pillars-Inference,\nDensity Estimation, Sequential Learning, Generalization, Representation\nLearning, Interpretability, Causality, Optimization, and\nUnification-demonstrating that each is built upon century-old statistical\nprinciples. From the inferential frameworks of hypothesis testing and\nestimation that underpin model evaluation, to the density estimation roots of\nclustering and generative AI; from the time-series analysis inspiring recurrent\nnetworks to the causal models that promise true understanding, we trace an\nunbroken statistical lineage. While celebrating the computational engines that\npower modern AI, we contend that statistics provides the brain-the theoretical\nframeworks, uncertainty quantification, and inferential goals-while computer\nscience provides the brawn-the scalable algorithms and hardware. Recognizing\nthis statistical backbone is not merely an academic exercise, but a necessary\nstep for developing more robust, interpretable, and trustworthy intelligent\nsystems. We issue a call to action for education, research, and practice to\nre-embrace this statistical foundation. Ignoring these roots risks building a\nfragile future; embracing them is the path to truly intelligent machines. There\nis no machine learning without statistical learning; no artificial intelligence\nwithout statistical thought."}
{"id": "2510.19243", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.19243", "abs": "https://arxiv.org/abs/2510.19243", "authors": ["Rong Zhao", "Jason Falvey", "Xu Shi", "Vernon M. Chinchilli", "Chixiang Chen"], "title": "A New Targeted-Federated Learning Framework for Estimating Heterogeneity of Treatment Effects: A Robust Framework with Applications in Aging Cohorts", "comment": null, "summary": "Analyzing data from multiple sources offers valuable opportunities to improve\nthe estimation efficiency of causal estimands. However, this analysis also\nposes many challenges due to population heterogeneity and data privacy\nconstraints. While several advanced methods for causal inference in federated\nsettings have been developed in recent years, many focus on difference-based\naveraged causal effects and are not designed to study effect modification. In\nthis study, we introduce a novel targeted-federated learning framework to study\nthe heterogeneity of treatment effects (HTEs) for a targeted population by\nproposing a projection-based estimand. This HTE framework integrates\ninformation from multiple data sources without sharing raw data, while\naccounting for covariate distribution shifts among sources. Our proposed\napproach is shown to be doubly robust, conveniently supporting both\ndifference-based estimands for continuous outcomes and odds ratio-based\nestimands for binary outcomes. Furthermore, we develop a\ncommunication-efficient bootstrap-based selection procedure to detect\nnon-transportable data sources, thereby enhancing robust information\naggregation without introducing bias. The superior performance of the proposed\nestimator over existing methods is demonstrated through extensive simulation\nstudies, and the utility of our approach has been shown in a real-world data\napplication using nationwide Medicare-linked data."}
{"id": "2510.19311", "categories": ["stat.ME", "62H12", "G.3"], "pdf": "https://arxiv.org/pdf/2510.19311", "abs": "https://arxiv.org/abs/2510.19311", "authors": ["M. Ohishi", "I. Nagai", "R. Oda", "H. Yanagihara"], "title": "Hierarchical Overlapping Group Lasso for GMANOVA Model", "comment": null, "summary": "This paper deals with the GMANOVA model with a matrix of polynomial basis\nfunctions as a within-individual design matrix. The model involves two model\nselection problems: the selection of explanatory variables and the selection of\nthe degrees of the polynomials. The two problems can be uniformly addressed by\nhierarchically incorporating zeros into the vectors of regression coefficients.\nBased on this idea, we propose hierarchical overlapping group Lasso (HOGL) to\nperform the variable and degree selections simultaneously. Importantly, when\nusing a polynomial basis, fitting a highdegree polynomial often causes problems\nin model selection. In the approach proposed here, these problems are handled\nby using a matrix of orthonormal basis functions obtained by transforming the\nmatrix of polynomial basis functions. Algorithms are developed with optimality\nand convergence to optimize the method. The performance of the proposed method\nis evaluated using numerical simulation."}
{"id": "2510.19489", "categories": ["stat.ME", "62A99", "G.3"], "pdf": "https://arxiv.org/pdf/2510.19489", "abs": "https://arxiv.org/abs/2510.19489", "authors": ["František Bartoš", "Samuel Pawel", "Björn S. Siepe"], "title": "Living Synthetic Benchmarks: A Neutral and Cumulative Framework for Simulation Studies", "comment": null, "summary": "Simulation studies are widely used to evaluate statistical methods. However,\nnew methods are often introduced and evaluated using data-generating mechanisms\n(DGMs) devised by the same authors. This coupling creates misaligned\nincentives, e.g., the need to demonstrate the superiority of new methods,\npotentially compromising the neutrality of simulation studies. Furthermore,\nresults of simulation studies are often difficult to compare due to differences\nin DGMs, competing methods, and performance measures. This fragmentation can\nlead to conflicting conclusions, hinder methodological progress, and delay the\nadoption of effective methods. To address these challenges, we introduce the\nconcept of living synthetic benchmarks. The key idea is to disentangle method\nand simulation study development and continuously update the benchmark whenever\na new DGM, method, or performance measure becomes available. This separation\nbenefits the neutrality of method evaluation, emphasizes the development of\nboth methods and DGMs, and enables systematic comparisons. In this paper, we\noutline a blueprint for building and maintaining such benchmarks, discuss the\ntechnical and organizational challenges of implementation, and demonstrate\nfeasibility with a prototype benchmark for publication bias adjustment methods.\nWe conclude that living synthetic benchmarks have the potential to foster\nneutral, reproducible, and cumulative evaluation of methods, benefiting both\nmethod developers and users."}
{"id": "2510.19583", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.19583", "abs": "https://arxiv.org/abs/2510.19583", "authors": ["Subhrajyoty Roy", "Abhik Ghosh", "Ayanendranath Basu"], "title": "Robust Rank Estimation for Noisy Matrices", "comment": null, "summary": "Estimating the true rank of a noisy data matrix is a fundamental problem\nunderlying techniques such as principal component analysis, matrix completion,\netc. Existing rank estimation criteria, including information-based and\ncross-validation methods, are either highly sensitive to outliers or\ncomputationally demanding when combined with robust estimators. This paper\nproposes a new criterion, the Divergence Information Criterion for Matrix Rank\n(DICMR), that achieves both robustness and computational simplicity. Derived\nfrom the density power divergence framework, DICMR inherits the robustness\nproperties while being computationally very simple. We provide asymptotic\nbounds on its overestimation and underestimation probabilities, and demonstrate\nfirst-order B-robustness of the criteria. Extensive simulations show that DICMR\ndelivers accuracy comparable to the robustified cross-validation methods, but\nwith far lower computational cost. We also showcase a real-data application to\nmicroarray imputation to further demonstrate its practical utility,\noutperforming several state-of-the-art algorithms."}
{"id": "2510.19077", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.19077", "abs": "https://arxiv.org/abs/2510.19077", "authors": ["Rebecca K. Metcalfe", "Nathaniel Dyrkton", "Yichen Yan", "Shomoita Alam", "Susan Shepherd", "Ibrahim Sana", "Kevin Phelan", "Jay JH Park"], "title": "Simulation-Guided Planning of a Target Trial Emulated Cluster Randomized Trial for Mass Small-Quantity Lipid Nutrient Supplementation Combined with Expanded Program on Immunization in Rural Niger", "comment": null, "summary": "While target trial emulation (TTE) is increasingly used to improve the\nanalysis of non-randomized studies by applying trial design principles, TTE\napplications to emulate cluster randomized trials (RCTs) have been limited. We\nperformed simulations to prospectively plan data collection of a non-randomized\nstudy intended to emulate a village-level cluster RCT when\ncluster-randomization was infeasible. The planned study will assess the impact\nof mass distribution of nutritional supplements embedded within an existing\nimmunization program to improve pentavalent vaccination rates among children\n12-24 months old in Niger. The design included covariate-constrained random\nselection of villages for outcome ascertainment at follow-up. Simulations used\nbaseline census data on pentavalent vaccination rates and cluster-level\ncovariates to compare the type I error rate and power of four statistical\nmethods: beta-regression; quasi-binomial regression; inverse probability of\ntreatment weighting (IPTW); and na\\\"ive Wald test. Of these methods, only IPTW\nand beta-regression controlled the type I error rate at 0.05, but IPTW yielded\npoor statistical power. Beta-regression, which showed adequate statistical\npower, was chosen as our primary analysis. Adopting simulation-guided design\nprinciples within TTE can enable robust planning of a group-level\nnon-randomized study emulating a cluster RCT. Lessons from this study also\napply to TTE planning of individually-RCTs."}
{"id": "2510.19722", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.19722", "abs": "https://arxiv.org/abs/2510.19722", "authors": ["Sébastien Garneau", "Carlos T. P. Zanini", "Alexandra M. Schmidt"], "title": "Semi-Implicit Approaches for Large-Scale Bayesian Spatial Interpolation", "comment": "36 pages, 5 figures, 2 tables, 1 algorithm", "summary": "Spatial statistics often rely on Gaussian processes (GPs) to capture\ndependencies across locations. However, their computational cost increases\nrapidly with the number of locations, potentially needing multiple hours even\nfor moderate sample sizes. To address this, we propose using Semi-Implicit\nVariational Inference (SIVI), a highly flexible Bayesian approximation method,\nfor scalable Bayesian spatial interpolation. We evaluated SIVI with a GP prior\nand a Nearest-Neighbour Gaussian Process (NNGP) prior compared to Automatic\nDifferentiation Variational Inference (ADVI), Pathfinder, and Hamiltonian Monte\nCarlo (HMC), the reference method in spatial statistics. Methods were compared\nbased on their predictive ability measured by the CRPS, the interval score, and\nthe negative log-predictive density across 50 replicates for both Gaussian and\nPoisson outcomes. SIVI-based methods achieved similar results to HMC, while\nbeing drastically faster. On average, for the Poisson scenario with 500\ntraining locations, SIVI reduced the computational time from roughly 6 hours\nfor HMC to 130 seconds. Furthermore, SIVI-NNGP analyzed a simulated land\nsurface temperature dataset of 150,000 locations while estimating all unknown\nmodel parameters in under two minutes. These results highlight the potential of\nSIVI as a flexible and scalable inference technique in spatial statistics."}
