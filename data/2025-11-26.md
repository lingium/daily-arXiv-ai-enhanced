<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 20]
- [stat.ME](#stat.ME) [Total: 25]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 4]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Quantum Fourier Transform Based Kernel for Solar Irrandiance Forecasting](https://arxiv.org/abs/2511.17698)
*Nawfel Mechiche-Alami,Eduardo Rodriguez,Jose M. Cardemil,Enrique Lopez Droguett*

Main category: stat.ML

TL;DR: 该研究提出了一种基于量子傅里叶变换(QFT)增强的量子核方法，用于短期时间序列预测，在太阳能辐照度数据上相比经典核方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 开发量子核方法以提升时间序列预测性能，特别是针对多站点太阳能辐照度数据，探索量子计算在时间序列分析中的潜力。

Method: 将信号分窗、幅度编码后通过QFT变换，添加保护旋转层防止QFT/逆QFT抵消，使用核岭回归(KRR)，通过凸融合整合外生预测因子。

Result: 在多种气候类型的太阳能辐照度数据上，提出的量子核在R2、nRMSE和nMBE指标上均优于经典RBF和多项式核，平均误差更小。

Conclusion: QFT增强的量子核在时间序列预测中具有优势，为NISQ设备上的实际应用提供了可行路径。

Abstract: This study proposes a Quantum Fourier Transform (QFT)-enhanced quantum kernel for short-term time-series forecasting. Each signal is windowed, amplitude-encoded, transformed by a QFT, then passed through a protective rotation layer to avoid the QFT/QFT adjoint cancellation; the resulting kernel is used in kernel ridge regression (KRR). Exogenous predictors are incorporated by convexly fusing feature-specific kernels. On multi-station solar irradiance data across Koppen climate classes, the proposed kernel consistently improves median R2 and nRMSE over reference classical RBF and polynomials kernels, while also reducing bias (nMBE); complementary MAE/ERMAX analyses indicate tighter average errors with remaining headroom under sharp transients. For both quantum and classical models, the only tuned quantities are the feature-mixing weights and the KRR ridge alpha; classical hyperparameters (gamma, r, d) are fixed, with the same validation set size for all models. Experiments are conducted on a noiseless simulator (5 qubits; window length L=32). Limitations and ablations are discussed, and paths toward NISQ execution are outlined.

</details>


### [2] [Prequential posteriors](https://arxiv.org/abs/2511.17721)
*Shreya Sinha-Roy,Richard G. Everitt,Christian P. Robert,Ritabrata Dutta*

Main category: stat.ML

TL;DR: 本文提出了一种基于预测序列损失函数的prequential后验方法，用于解决深度生成预测模型中的数据同化问题，克服了传统贝叶斯方法因难以处理似然函数而无法应用的局限性。


<details>
  <summary>Details</summary>
Motivation: 深度生成预测模型在预测任务中表现出色，但由于其难以处理的似然函数，标准贝叶斯数据同化方法无法直接应用，这限制了模型在实际应用中的更新能力。

Method: 采用预测序列损失函数构建prequential后验，使用可并行化的wastefree序列蒙特卡洛采样器，结合预条件梯度核进行高效的高维参数空间探索。

Result: 在合成多维时间序列和真实气象数据集上的验证表明，该方法能够有效实现复杂动力系统的数据同化，prequential损失最小化和prequential后验都集中在具有最优预测性能的参数周围。

Conclusion: prequential后验方法为深度生成预测模型提供了一种可扩展且实用的数据同化解决方案，特别适用于时间依赖数据的预测任务。

Abstract: Data assimilation is a fundamental task in updating forecasting models upon observing new data, with applications ranging from weather prediction to online reinforcement learning. Deep generative forecasting models (DGFMs) have shown excellent performance in these areas, but assimilating data into such models is challenging due to their intractable likelihood functions. This limitation restricts the use of standard Bayesian data assimilation methodologies for DGFMs. To overcome this, we introduce prequential posteriors, based upon a predictive-sequential (prequential) loss function; an approach naturally suited for temporally dependent data which is the focus of forecasting tasks. Since the true data-generating process often lies outside the assumed model class, we adopt an alternative notion of consistency and prove that, under mild conditions, both the prequential loss minimizer and the prequential posterior concentrate around parameters with optimal predictive performance. For scalable inference, we employ easily parallelizable wastefree sequential Monte Carlo (SMC) samplers with preconditioned gradient-based kernels, enabling efficient exploration of high-dimensional parameter spaces such as those in DGFMs. We validate our method on both a synthetic multi-dimensional time series and a real-world meteorological dataset; highlighting its practical utility for data assimilation for complex dynamical systems.

</details>


### [3] [Differential privacy with dependent data](https://arxiv.org/abs/2511.18583)
*Valentin Roth,Marco Avella-Medina*

Main category: stat.ML

TL;DR: 本文研究了在依赖数据下的差分隐私均值估计问题，提出了基于Winsorized均值估计器的方法，能够处理有界和无界数据，并在弱依赖条件下获得与独立同分布情况相似的渐近和有限样本保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的社会科学和健康科学研究经常涉及敏感或隐私信息，这些数据通常是依赖的（如重复测量）。差分隐私特别是用户级差分隐私为处理这类依赖数据提供了自然的隐私保护形式化，但现有统计理论主要针对独立数据，对依赖数据的处理存在挑战。

Method: 使用Winsorized均值估计器，通过log-Sobolev不等式形式化依赖关系，将Karwa和Vadhan(2018)的稳定直方图方法扩展到非独立同分布设置，用于估计Winsorized估计器的私有投影区间。

Result: 提出的方法在弱依赖条件下能够获得与独立同分布情况相似的渐近和有限样本保证，适用于有界和无界数据，并能扩展到用户级均值估计和局部模型。

Conclusion: 这项工作为依赖数据下的差分隐私统计推断提供了系统研究的初步框架，能够扩展到随机效应模型、纵向线性回归和非参数回归等更复杂的统计模型。

Abstract: Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\textit{item-level}) and \textit{user-level} DP estimation of a mean $μ\in \R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data.

</details>


### [4] [Variational Estimators for Node Popularity Models](https://arxiv.org/abs/2511.17783)
*Jony Karki,Dongzhou Huang,Yunpeng Zhao*

Main category: stat.ML

TL;DR: 提出了一个计算高效且理论合理的变分期望最大化框架用于双向节点流行度模型，在二分网络和无向网络中均实现了优于现有算法的估计精度。


<details>
  <summary>Details</summary>
Motivation: 节点流行度是建模真实网络的关键因素，在二分网络中不同分区的节点可能表现出不同的流行度模式。现有方法如TSDC算法在可扩展性方面有优势，但在准确性和跨网络类型的适用性方面存在局限。

Method: 开发了一个变分期望最大化框架用于双向节点流行度模型，建立了二分网络中估计社区分配的标签一致性理论保证。

Result: 通过广泛的模拟研究，该方法在二分网络和无向网络中均实现了优于现有算法的估计精度。在真实网络上的评估进一步证明了其实用有效性和鲁棒性。

Conclusion: 提出的变分期望最大化框架为双向节点流行度模型提供了计算高效且理论合理的估计方法，在多种网络类型中表现出优越性能。

Abstract: Node popularity is recognized as a key factor in modeling real-world networks, capturing heterogeneity in connectivity across communities. This concept is equally important in bipartite networks, where nodes in different partitions may exhibit varying popularity patterns, motivating models such as the Two-Way Node Popularity Model (TNPM). Existing methods, such as the Two-Stage Divided Cosine (TSDC) algorithm, provide a scalable estimation approach but may have limitations in terms of accuracy or applicability across different types of networks. In this paper, we develop a computationally efficient and theoretically justified variational expectation-maximization (VEM) framework for the TNPM. We establish label consistency for the estimated community assignments produced by the proposed variational estimator in bipartite networks. Through extensive simulation studies, we show that our method achieves superior estimation accuracy across a range of bipartite as well as undirected networks compared to existing algorithms. Finally, we evaluate our method on real-world bipartite and undirected networks, further demonstrating its practical effectiveness and robustness.

</details>


### [5] [On Instability of Minimax Optimal Optimism-Based Bandit Algorithms](https://arxiv.org/abs/2511.18750)
*Samya Praharaj,Koulik Khamaru*

Main category: stat.ML

TL;DR: 本文发现多臂老虎机问题中存在稳定性与极小极大最优性之间的根本性冲突，广泛使用的极小极大最优UCB算法都违反了Lai-Wei稳定性条件，导致样本均值无法满足中心极限定理。


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机算法生成的数据具有自适应性和非独立同分布特性，使得统计推断变得困难。经典表现是样本均值在老虎机采样下可能无法满足中心极限定理。虽然UCB算法满足稳定性条件，但它不是极小极大最优的，这引发了一个问题：是否可能同时实现极小极大最优性和统计稳定性。

Method: 分析基于乐观原则的一类广泛老虎机算法的稳定性特性，建立了这类算法违反Lai-Wei稳定性准则的一般结构条件。

Result: 证明包括MOSS、Anytime-MOSS、Vanilla-MOSS、ADA-UCB、OC-UCB、KL-MOSS、KL-UCB++、KL-UCB-SWITCH和Anytime KL-UCB-SWITCH在内的广泛使用的极小极大最优UCB风格算法都是不稳定的。数值模拟进一步证实了这些情况下样本均值无法展现渐近正态性。

Conclusion: 研究结果表明稳定性和极小极大最优遗憾之间存在根本性张力，提出是否可能设计同时实现稳定性和极小极大最优的老虎机算法是一个重要的开放研究方向。

Abstract: Statistical inference from data generated by multi-armed bandit (MAB) algorithms is challenging due to their adaptive, non-i.i.d. nature. A classical manifestation is that sample averages of arm rewards under bandit sampling may fail to satisfy a central limit theorem. Lai and Wei's stability condition provides a sufficient, and essentially necessary criterion, for asymptotic normality in bandit problems. While the celebrated Upper Confidence Bound (UCB) algorithm satisfies this stability condition, it is not minimax optimal, raising the question of whether minimax optimality and statistical stability can be achieved simultaneously. In this paper, we analyze the stability properties of a broad class of bandit algorithms that are based on the optimism principle. We establish general structural conditions under which such algorithms violate the Lai-Wei stability criterion. As a consequence, we show that widely used minimax-optimal UCB-style algorithms, including MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH, are unstable. We further complement our theoretical results with numerical simulations demonstrating that, in all these cases, the sample means fail to exhibit asymptotic normality.
  Overall, our findings suggest a fundamental tension between stability and minimax optimal regret, raising the question of whether it is possible to design bandit algorithms that achieve both. Understanding whether such simultaneously stable and minimax optimal strategies exist remains an important open direction.

</details>


### [6] [An operator splitting analysis of Wasserstein--Fisher--Rao gradient flows](https://arxiv.org/abs/2511.18060)
*Francesca Romana Crucinio,Sahani Pathiraja*

Main category: stat.ML

TL;DR: 本文研究了Wasserstein-Fisher-Rao梯度流中算子分裂顺序对采样效率的影响，发现通过合理选择步长和算子顺序，分裂方案可以比精确WFR流更快收敛到目标分布。


<details>
  <summary>Details</summary>
Motivation: 现有算法隐式使用算子分裂技术来数值近似WFR偏微分方程，但分裂顺序的影响尚未得到定量分析。本文旨在研究W和FR算子评估顺序的影响。

Method: 使用算子分裂技术，分别研究W-FR和FR-W两种顺序的分裂方案，通过变分公式描述单时间步的演化过程。

Result: 研究发现通过合理选择步长和算子顺序，分裂方案可以比精确WFR流更快收敛；证明了WFR梯度流保持对数凹性，并获得了WFR的第一个尖锐衰减界。

Conclusion: W-FR分裂在某些情况下应优先于FR-W分裂，为WFR梯度流的数值实现提供了理论指导。

Abstract: Wasserstein-Fisher-Rao (WFR) gradient flows have been recently proposed as a powerful sampling tool that combines the advantages of pure Wasserstein (W) and pure Fisher-Rao (FR) gradient flows. Existing algorithmic developments implicitly make use of operator splitting techniques to numerically approximate the WFR partial differential equation, whereby the W flow is evaluated over a given step size and then the FR flow (or vice versa). This works investigates the impact of the order in which the W and FR operator are evaluated and aims to provide a quantitative analysis. Somewhat surprisingly, we show that with a judicious choice of step size and operator ordering, the split scheme can converge to the target distribution faster than the exact WFR flow (in terms of model time). We obtain variational formulae describing the evolution over one time step of both sequential splitting schemes and investigate in which settings the W-FR split should be preferred to the FR-W split. As a step towards this goal we show that the WFR gradient flow preserves log-concavity and obtain the first sharp decay bound for WFR.

</details>


### [7] [Conformal Prediction for Compositional Data](https://arxiv.org/abs/2511.18141)
*Lucas P. Amaral,Luben M. C. Cabezas,Thiago R. Ramos,Gustavo H. G. A. Pereira*

Main category: stat.ML

TL;DR: 本文提出了针对组合响应数据的保形预测方法，基于Dirichlet回归构建了分位数残差方法和网格精化最高密度区域策略，确保在保持单纯形几何特性的同时获得有限样本边际覆盖。


<details>
  <summary>Details</summary>
Motivation: 组合响应数据（比例数据，必须为正且和为1）在预测时需要专门的不确定性量化方法，现有方法往往不能很好地处理单纯形的几何特性。

Method: 提出了两种模型无关的保形预测方法：基于分位数残差的分割保形方法和结合坐标下限近似与内部网格精化的最高密度区域策略。

Result: 蒙特卡洛研究表明，分位数残差和网格精化HDR方法在异方差和同方差设计中都能达到接近90%名义水平的经验覆盖，且产生比保守的坐标下限近似更窄的预测区域。在BudgetItaly数据集应用中，网格精化HDR方法覆盖最接近目标且平均宽度最小。

Conclusion: 单纯形上的保形预测可以既校准又高效，为组合预测任务提供了实用的不确定性量化工具。

Abstract: In this work, we propose a set of conformal prediction procedures tailored to compositional responses, where outcomes are proportions that must be positive and sum to one. Building on Dirichlet regression, we introduce a split conformal approach based on quantile residuals and a highest-density region strategy that combines a fast coordinate-floor approximation with an internal grid refinement to restore sharpness. Both constructions are model-agnostic at the conformal layer and guarantee finite-sample marginal coverage under exchangeability, while respecting the geometry of the simplex. A comprehensive Monte Carlo study spanning homoscedastic and heteroscedastic designs shows that the quantile residual and grid-refined HDR methods achieve empirical coverage close to the nominal 90\% level and produce substantially narrower regions than the coordinate-floor approximation, which tends to be conservative. We further demonstrate the methods on household budget shares from the BudgetItaly dataset, using standardized socioeconomic and price covariates with a train, calibration, and test split. In this application, the grid-refined HDR attains coverage closest to the target with the smallest average widths, closely followed by the quantile residual approach, while the simple triangular HDR yields wider, less informative sets. Overall, the results indicate that conformal prediction on the simplex can be both calibrated and efficient, providing practical uncertainty quantification for compositional prediction tasks.

</details>


### [8] [Nonparametric Instrumental Variable Regression with Observed Covariates](https://arxiv.org/abs/2511.19404)
*Zikai Shen,Zonghao Chen,Dimitri Meunier,Ingo Steinwart,Arthur Gretton,Zhu Li*

Main category: stat.ML

TL;DR: 本文研究了带有观测协变量的非参数工具变量回归（NPIV-O）问题，提出了KIV-O算法并建立了其学习速率的上界和下界，填补了NPIV和NPR之间的理论空白。


<details>
  <summary>Details</summary>
Motivation: 与标准NPIV相比，观测协变量增强了因果识别能力并支持异质因果效应估计，但带来了部分恒等结构和各向异性平滑性的理论挑战。

Method: 提出了KIV-O算法，引入傅里叶部分平滑度量来处理部分恒等结构，并使用自适应高斯核长度尺度来应对各向异性平滑性。

Result: 证明了KIV-O的L²学习速率上界和NPIV-O的首个L²极小极大下界，这些速率在NPIV和NPR的最优速率之间插值，但存在由投影风险最小化引起的间隙。

Conclusion: 该理论分析不仅适用于NPIV-O，也适用于具有相同条件矩限制的新兴近端因果推断框架。

Abstract: We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.

</details>


### [9] [Sparse Polyak with optimal thresholding operators for high-dimensional M-estimation](https://arxiv.org/abs/2511.18167)
*Tianqi Qiao,Marie Maros*

Main category: stat.ML

TL;DR: 提出并分析了一种改进的Sparse Polyak算法变体，用于高维M估计问题，在保持对维度良好缩放性的同时获得更稀疏和更准确的解。


<details>
  <summary>Details</summary>
Motivation: 原始Sparse Polyak算法在高维设置下虽然能适应问题曲率且性能不随维度增加而恶化，但需要牺牲解的稀疏性和统计准确性来获得收敛保证。

Method: 开发了Sparse Polyak的变体，保留了其对环境维度的良好缩放特性，同时改进了稀疏性和准确性。

Result: 新变体在保持维度缩放优势的同时，能够获得更稀疏和更准确的解决方案。

Conclusion: 该工作成功改进了Sparse Polyak算法，使其在高维M估计问题中既能保持良好的维度缩放性，又能提供更优的稀疏性和统计准确性。

Abstract: We propose and analyze a variant of Sparse Polyak for high dimensional M-estimation problems. Sparse Polyak proposes a novel adaptive step-size rule tailored to suitably estimate the problem's curvature in the high-dimensional setting, guaranteeing that the algorithm's performance does not deteriorate when the ambient dimension increases. However, convergence guarantees can only be obtained by sacrificing solution sparsity and statistical accuracy. In this work, we introduce a variant of Sparse Polyak that retains its desirable scaling properties with respect to the ambient dimension while obtaining sparser and more accurate solutions.

</details>


### [10] [Improving Forecasts of Suicide Attempts for Patients with Little Data](https://arxiv.org/abs/2511.18199)
*Genesis Hang,Annie Chen,Hope Neveux,Matthew K. Nock,Yaniv Yacoby*

Main category: stat.ML

TL;DR: 提出Latent Similarity Gaussian Processes (LSGPs)方法，通过捕捉患者异质性来改进自杀企图的预测，让数据少的患者可以利用相似患者的趋势。


<details>
  <summary>Details</summary>
Motivation: 生态瞬时评估提供自杀想法和行为的实时数据，但由于自杀企图的罕见性和患者异质性，预测仍然具有挑战性。单一模型对所有患者效果差，个体化模型虽有所改进但容易对数据有限的患者过拟合。

Method: 引入Latent Similarity Gaussian Processes (LSGPs)来捕捉患者异质性，使数据少的患者能够利用相似患者的趋势。

Result: 初步结果显示有希望：即使没有核设计，我们的方法也优于除一个基准外的所有方法，同时提供了对患者相似性的新理解。

Conclusion: LSGPs方法在预测自杀企图方面显示出潜力，能够有效处理患者异质性和数据稀疏问题。

Abstract: Ecological Momentary Assessment provides real-time data on suicidal thoughts and behaviors, but predicting suicide attempts remains challenging due to their rarity and patient heterogeneity. We show that single models fit to all patients perform poorly, while individualized models improve performance but still overfit to patients with limited data. To address this, we introduce Latent Similarity Gaussian Processes (LSGPs) to capture patient heterogeneity, enabling those with little data to leverage similar patients' trends. Preliminary results show promise: even without kernel-design, we outperform all but one baseline while offering a new understanding of patient similarity.

</details>


### [11] [Structured Matching via Cost-Regularized Unbalanced Optimal Transport](https://arxiv.org/abs/2511.19075)
*Emanuele Pardini,Katerina Papagiannouli*

Main category: stat.ML

TL;DR: 提出了成本正则化不平衡最优传输（CR-UOT）框架，允许地面传输成本变化，同时支持质量创建和移除，解决了异构空间数据集匹配中地面成本选择困难的问题。


<details>
  <summary>Details</summary>
Motivation: 传统不平衡最优传输（UOT）需要预定义地面传输成本，这在异构空间数据集匹配中可能无法准确表示数据的基础几何结构，特别是当许多细胞缺乏直接匹配时。

Method: 引入成本正则化不平衡最优传输（CR-UOT）框架，允许地面成本变化，通过内积成本参数化线性变换，使用熵正则化开发算法。

Result: CR-UOT能够通过参数化内积成本家族包含不平衡Gromov-Wasserstein类型问题，改进了异构单细胞组学图谱的对齐效果。

Conclusion: CR-UOT框架为异构空间中的度量匹配提供了更灵活的方法，特别适用于单细胞组学数据中许多细胞缺乏直接匹配的情况。

Abstract: Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.

</details>


### [12] [Reliable Selection of Heterogeneous Treatment Effect Estimators](https://arxiv.org/abs/2511.18464)
*Jiayi Guo,Zijun Gao*

Main category: stat.ML

TL;DR: 提出了一种无需真实治疗效果即可选择最佳异质处理效应估计器的方法，通过多重检验框架和交叉拟合的指数加权统计量实现可靠的错误控制。


<details>
  <summary>Details</summary>
Motivation: 在治疗效果本质不可观测的情况下，如何从多个候选估计器中选择最佳异质处理效应估计器是一个挑战性任务。

Method: 将估计器选择问题建模为多重检验问题，采用交叉拟合的指数加权检验统计量，结合双向样本分割方案解耦干扰项估计和权重学习。

Result: 在ACIC 2016、IHDP和Twins基准测试中，该方法提供了可靠的错误控制，相比常用方法显著减少了错误选择。

Conclusion: 即使没有真实的治疗效果，该方法仍然可行且强大，能够有效控制家庭错误率并提高估计器选择的准确性。

Abstract: We study the problem of selecting the best heterogeneous treatment effect (HTE) estimator from a collection of candidates in settings where the treatment effect is fundamentally unobserved. We cast estimator selection as a multiple testing problem and introduce a ground-truth-free procedure based on a cross-fitted, exponentially weighted test statistic. A key component of our method is a two-way sample splitting scheme that decouples nuisance estimation from weight learning and ensures the stability required for valid inference. Leveraging a stability-based central limit theorem, we establish asymptotic familywise error rate control under mild regularity conditions. Empirically, our procedure provides reliable error control while substantially reducing false selections compared with commonly used methods across ACIC 2016, IHDP, and Twins benchmarks, demonstrating that our method is feasible and powerful even without ground-truth treatment effects.

</details>


### [13] [Transforming Conditional Density Estimation Into a Single Nonparametric Regression Task](https://arxiv.org/abs/2511.18530)
*Alexander G. Reisach,Olivier Collier,Alex Luedtke,Antoine Chambaz*

Main category: stat.ML

TL;DR: 将条件密度估计问题转化为单一非参数回归任务，通过引入辅助样本，利用神经网络和决策树等回归方法在高维数据中有效工作。


<details>
  <summary>Details</summary>
Motivation: 解决高维条件密度估计问题，利用回归方法的优势来处理复杂的数据分布。

Method: 提出condensité方法，通过引入辅助样本将条件密度估计转化为非参数回归任务。

Result: 在合成数据、人口调查数据集和卫星图像数据集上的实验表明，condensité达到或优于现有最优方法，并能产生与文献一致的条件密度估计。

Conclusion: 该方法为基于回归的条件密度估计开辟了新可能性，在应用研究中显示出强大潜力。

Abstract: We propose a way of transforming the problem of conditional density estimation into a single nonparametric regression task via the introduction of auxiliary samples. This allows leveraging regression methods that work well in high dimensions, such as neural networks and decision trees. Our main theoretical result characterizes and establishes the convergence of our estimator to the true conditional density in the data limit. We develop condensité, a method that implements this approach. We demonstrate the benefit of the auxiliary samples on synthetic data and showcase that condensité can achieve good out-of-the-box results. We evaluate our method on a large population survey dataset and on a satellite imaging dataset. In both cases, we find that condensité matches or outperforms the state of the art and yields conditional densities in line with established findings in the literature on each dataset. Our contribution opens up new possibilities for regression-based conditional density estimation and the empirical results indicate strong promise for applied research.

</details>


### [14] [Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks](https://arxiv.org/abs/2511.18562)
*Xunlei Qian,Yue Xing*

Main category: stat.ML

TL;DR: 本文研究了在对抗性扰动下分割共形预测的鲁棒性，分析了校准阶段和测试阶段的对抗攻击对预测覆盖率和集合大小的影响，并探讨了对抗训练的作用。


<details>
  <summary>Details</summary>
Motivation: 共形预测(CP)需要数据交换性假设，但在实际分布偏移和对抗攻击下该假设常被违反。研究对抗环境下CP的覆盖保证和预测集大小的鲁棒性具有重要意义。

Method: 理论分析对抗性扰动对分割共形预测的影响，通过实验验证：(1)校准阶段攻击强度与覆盖率的单调关系；(2)使用适当校准攻击维持目标覆盖率；(3)对抗训练对预测集紧凑性的影响。

Result: 实验表明：(i)预测覆盖率随校准阶段攻击强度单调变化；(ii)通过合适的校准攻击，可在连续扰动范围内维持目标覆盖率；(iii)对抗训练能产生更紧凑且信息量高的预测集。

Conclusion: 在对抗环境下，通过控制校准阶段的对抗攻击强度可以预测性地控制测试阶段的覆盖率，且对抗训练有助于保持预测集的信息性和紧凑性。

Abstract: Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness.

</details>


### [15] [Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data](https://arxiv.org/abs/2511.18661)
*Guillaume Braun,Bruno Loureiro,Ha Quang Minh,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 本文研究了各向异性高斯输入下相位检索问题的缩放规律，揭示了与各向同性情况不同的三阶段学习轨迹，并推导了均方误差的显式缩放规律。


<details>
  <summary>Details</summary>
Motivation: 缩放规律描述了学习性能如何随数据、计算或训练时间提升，已成为现代深度学习的核心主题。本文旨在研究非线性模型中各向异性数据对缩放规律的影响。

Method: 使用协方差谱遵循幂律的各向异性高斯输入进行相位检索建模，开发了可处理的约简方法，将动态分解为三个相轨迹。

Result: 实验证实了预测的三个阶段和指数，各向异性产生了新的学习机制，其中无限层次的耦合方程控制着统计量的演化。

Conclusion: 这些结果首次严格刻画了各向异性数据非线性回归中的缩放规律，突出了各向异性如何重塑学习动态。

Abstract: Scaling laws describe how learning performance improves with data, compute, or training time, and have become a central theme in modern deep learning. We study this phenomenon in a canonical nonlinear model: phase retrieval with anisotropic Gaussian inputs whose covariance spectrum follows a power law. Unlike the isotropic case, where dynamics collapse to a two-dimensional system, anisotropy yields a qualitatively new regime in which an infinite hierarchy of coupled equations governs the evolution of the summary statistics. We develop a tractable reduction that reveals a three-phase trajectory: (i) fast escape from low alignment, (ii) slow convergence of the summary statistics, and (iii) spectral-tail learning in low-variance directions. From this decomposition, we derive explicit scaling laws for the mean-squared error, showing how spectral decay dictates convergence times and error curves. Experiments confirm the predicted phases and exponents. These results provide the first rigorous characterization of scaling laws in nonlinear regression with anisotropic data, highlighting how anisotropy reshapes learning dynamics.

</details>


### [16] [Uncertainty of Network Topology with Applications to Out-of-Distribution Detection](https://arxiv.org/abs/2511.18813)
*Sing-Yuan Yeh,Chun-Hao Yang*

Main category: stat.ML

TL;DR: 提出了预测拓扑不确定性(pTU)作为贝叶斯神经网络的新拓扑摘要，用于测量模型与输入交互中的不确定性，并应用于解决分布外检测问题。


<details>
  <summary>Details</summary>
Motivation: 持久同调在计算拓扑中很重要，但需要新的拓扑摘要来测量贝叶斯神经网络中的不确定性，特别是解决分布外检测这一关键问题。

Method: 引入预测拓扑不确定性(pTU)作为拓扑摘要，基于模型与输入的交互相似性来判断样本分布，并提出基于pTU的分布外检测显著性检验。

Result: pTU对模型架构不敏感，在统计功效、灵敏度和鲁棒性方面通过多种实验验证了该框架的有效性。

Conclusion: pTU提供了一个新的拓扑视角来量化模型不确定性，为分布外检测提供了统计框架，增强了模型可靠性。

Abstract: Persistent homology (PH) is a crucial concept in computational topology, providing a multiscale topological description of a space. It is particularly significant in topological data analysis, which aims to make statistical inference from a topological perspective. In this work, we introduce a new topological summary for Bayesian neural networks, termed the predictive topological uncertainty (pTU). The proposed pTU measures the uncertainty in the interaction between the model and the inputs. It provides insights from the model perspective: if two samples interact with a model in a similar way, then they are considered identically distributed. We also show that the pTU is insensitive to the model architecture. As an application, pTU is used to solve the out-of-distribution (OOD) detection problem, which is critical to ensure model reliability. Failure to detect OOD input can lead to incorrect and unreliable predictions. To address this issue, we propose a significance test for OOD based on the pTU, providing a statistical framework for this issue. The effectiveness of the framework is validated through various experiments, in terms of its statistical power, sensitivity, and robustness.

</details>


### [17] [Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification](https://arxiv.org/abs/2511.18876)
*Lilian Say,Christophe Denis,Rafael Pinot*

Main category: stat.ML

TL;DR: 本文挑战了隐私与公平性必然冲突的观点，提出DP2DP算法能同时满足差分隐私和人口统计均等，且收敛速度与非私有方法相当。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在敏感应用中的普及，需要同时保护数据隐私和确保跨敏感子群体的公平性。现有研究常将隐私和公平视为冲突目标，但本文挑战这一观点。

Method: 设计了名为DP2DP的后处理算法，该算法同时强制执行人口统计均等和差分隐私。

Result: 理论分析显示算法以与非私有方法相近的收敛速度达到公平目标，在合成和真实数据集上的实验证实了理论结果。

Conclusion: DP2DP算法在准确性/公平性/隐私权衡方面达到最先进水平，证明差分隐私可以与公平性增强流程有效集成。

Abstract: The increasing use of machine learning in sensitive applications demands algorithms that simultaneously preserve data privacy and ensure fairness across potentially sensitive sub-populations. While privacy and fairness have each been extensively studied, their joint treatment remains poorly understood. Existing research often frames them as conflicting objectives, with multiple studies suggesting that strong privacy notions such as differential privacy inevitably compromise fairness. In this work, we challenge that perspective by showing that differential privacy can be integrated into a fairness-enhancing pipeline with minimal impact on fairness guarantees. We design a postprocessing algorithm, called DP2DP, that enforces both demographic parity and differential privacy. Our analysis reveals that our algorithm converges towards its demographic parity objective at essentially the same rate (up logarithmic factor) as the best non-private methods from the literature. Experiments on both synthetic and real datasets confirm our theoretical results, showing that the proposed algorithm achieves state-of-the-art accuracy/fairness/privacy trade-offs.

</details>


### [18] [Classification EM-PCA for clustering and embedding](https://arxiv.org/abs/2511.18992)
*Zineddine Tighidet,Lazhar Labiod,Mohamed Nadif*

Main category: stat.ML

TL;DR: 提出一种同时进行数据嵌入和聚类的算法，结合PCA和CEM算法，解决高维数据和EM算法收敛慢的问题。


<details>
  <summary>Details</summary>
Motivation: 高斯混合模型在聚类中很受欢迎，但面临高维数据和EM算法收敛慢的挑战。CEM算法能快速收敛，但维度缩减仍是问题。

Method: 结合PCA和CEM算法，同时而非顺序地执行数据嵌入和聚类两个任务。

Result: 该方法在聚类和数据嵌入方面表现出优势，并建立了与其他聚类方法的联系。

Conclusion: 提出的同时进行数据嵌入和聚类的方法在效率和效果上都有优势，为高维数据聚类提供了有效解决方案。

Abstract: The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.

</details>


### [19] [A Robust State Filter Against Unmodeled Process And Measurement Noise](https://arxiv.org/abs/2511.19157)
*Weitao Liu*

Main category: stat.ML

TL;DR: 提出一种新颖的卡尔曼滤波框架，在过程和测量噪声下实现鲁棒状态估计，基于加权观测似然滤波(WoLF)的思想，采用广义贝叶斯方法处理过程和测量噪声异常值。


<details>
  <summary>Details</summary>
Motivation: 现有WoLF方法主要针对测量异常值提供鲁棒性，但缺乏对过程噪声异常值的处理能力，需要开发能够同时处理两种噪声异常值的鲁棒状态估计框架。

Method: 基于广义贝叶斯方法构建新框架，扩展WoLF思想，同时考虑过程和测量噪声中的异常值，建立鲁棒状态估计算法。

Result: 开发出能够同时处理过程和测量噪声异常值的鲁棒卡尔曼滤波框架，提高了状态估计在复杂噪声环境下的可靠性。

Conclusion: 所提出的广义贝叶斯框架成功扩展了WoLF方法，实现了对过程和测量噪声异常值的统一处理，为鲁棒状态估计提供了有效解决方案。

Abstract: This paper introduces a novel Kalman filter framework designed to achieve robust state estimation under both process and measurement noise. Inspired by the Weighted Observation Likelihood Filter (WoLF), which provides robustness against measurement outliers, we applied generalized Bayesian approach to build a framework considering both process and measurement noise outliers.

</details>


### [20] [The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility](https://arxiv.org/abs/2511.19284)
*Eichi Uehara*

Main category: stat.ML

TL;DR: 提出一个统一稳健框架，重新设计平均处理效应在重叠区域(ATO)的估计方法


<details>
  <summary>Details</summary>
Motivation: 解决高斯机制中高阶正交性不可能的问题，并提供对异常值的鲁棒性

Method: 整合gamma-散度用于异常值鲁棒性，渐进非凸性(GNC)用于全局优化，以及"守门人"机制

Result: 框架能够稳健地估计ATO，并在存在异常值和高斯噪声的情况下保持性能

Conclusion: 该统一框架为ATO估计提供了一种鲁棒且全局优化的解决方案，特别适用于具有挑战性的高斯环境

Abstract: This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a "Gatekeeper" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [21] [A Unified Spatiotemporal Framework for Modeling Censored and Missing Areal Responses](https://arxiv.org/abs/2511.17725)
*Jose A. Ordoñez,Tsung-I Lin,Victor H. Lachos,Luis M. Castro*

Main category: stat.ME

TL;DR: 提出了一种新的贝叶斯方法处理时空区域数据中的删失和缺失观测，结合SAR和DAGAR空间依赖结构与时间自回归成分，形成统一的时空框架。


<details>
  <summary>Details</summary>
Motivation: 处理时空区域数据中的删失和缺失观测问题，传统方法如用检测限替代删失值或用样本均值填补缺失数据存在局限性，需要更灵活的时空模型。

Method: 引入灵活随机效应，结合SAR和DAGAR空间依赖结构与时间自回归成分，表达为高斯马尔可夫随机场的创新形式，捕获空间、时间和联合时空相关性。

Result: 模拟研究显示该方法优于常见的临时填补策略；在北京空气质量网络CO浓度数据应用中，DAGAR-AR模型比传统CAR方法具有更清晰的解释性和更一致的时空依赖结构表示。

Conclusion: 虽然CAR模型预测性能略优，但DAGAR-AR规范在解释性和时空依赖结构表示方面更具优势，为时空区域数据分析提供了有价值的工具。

Abstract: We propose a new Bayesian approach for spatiotemporal areal data with censored and missing observations. The method introduces a flexible random effect that combines the spatial dependence structures of the Simultaneous Autoregressive (SAR) and Directed Acyclic Graph Autoregressive (DAGAR) models with a temporal autoregressive component. We demonstrate that this formulation extends both spatial models into a unified spatiotemporal framework, expressing them as Gaussian Markov random fields in their innovation form. The resulting model captures spatial, temporal, and joint spatiotemporal correlations in an interpretable way. Simulation studies show that the proposed model outperforms common ad hoc imputation strategies, such as replacing censored values with the limit of detection (LOD) or imputing missing data by the sample mean. We further apply the method to carbon monoxide (CO) concentration data from Beijing's air quality network, comparing the proposed DAGAR-AR model with the traditional Conditional Autoregressive (CAR) approach. The results indicate that while the CAR model achieves slightly better predictive performance, the DAGAR-AR specification offers clearer interpretability and a more coherent representation of the spatiotemporal dependence structure.

</details>


### [22] [Single Changepoint Procedures](https://arxiv.org/abs/2511.17870)
*Robert Lund,Xueheng Shi*

Main category: stat.ME

TL;DR: 本文总结了气候文献中常用的单变点检验方法，统一了它们的表示形式，提供了渐近分位数，并推导了分位数的计算方法。


<details>
  <summary>Details</summary>
Motivation: 单变点检验已成为检验气候时间序列同质性的标准方法，用于检测气候是否发生变化。本文旨在统一和总结当前气候文献中最突出的单变点检验方法。

Method: 总结了最显著的单变点检验方法，将它们相互关联并统一表示。提供了各个检验的渐近分位数，并推导了分位数的计算方法。研究了均值和趋势偏移两种最常见的气候学场景。

Result: 提供了SOI和全球温度序列的分析示例，展示了这些技术的应用。

Conclusion: 本文为气候时间序列的单变点检验提供了统一的理论框架和实用的分位数计算方法，有助于检测气候变化的同质性问题。

Abstract: Single changepoint tests have become a staple check for homogeneity of a climate time series, suggesting how climate has changed should non-homogeneity be declared. This paper summarizes the most prominent single changepoint tests used in today's climate literature, relating them to one and other and unifying their presentations. Asymptotic quantiles for the individual tests are presented. Derivations of the quantiles are given, enabling the reader to tackle cases not considered within. Our work here studies both mean and trend shifts, covering the most common settings arising in climatology. SOI and global temperature series are analyzed within to illustrate the techniques.

</details>


### [23] [Why Is the Double-Robust Estimator for Causal Inference Not Doubly Robust for Variance Estimation?](https://arxiv.org/abs/2511.17907)
*Hao Wu,Lucy Shao,Toni Gui,Tsungchin Wu,Zhuochao Huang,Shengjia Tu,Xin Tu,Jinyuan Liu,Tuo Lin*

Main category: stat.ME

TL;DR: 本文建立了双重稳健估计量(DRE)的正式理论，解释了为什么基于影响函数(IF)的方差估计量在双重稳健性方面失败，并提出了在错误设定干扰参数时实现有效推断的替代策略。


<details>
  <summary>Details</summary>
Motivation: 双重稳健估计量在因果推断中广泛应用，但其方差估计缺乏双重稳健性——IF-based方差估计量只有在两个干扰参数都正确时才一致。实践中模型错误设定不可避免，因此需要理解方差估计失败的原因并开发有效的替代方法。

Method: 开发了DRE效率特性的正式理论框架，引入了基于混合框架的替代策略（包括样本分割和交叉拟合方法），以在错误设定干扰参数时实现有效推断。

Result: 通过模拟和真实研究数据验证了理论分析，证明了所提替代策略在干扰参数错误设定时的有效性。

Conclusion: 本文为DRE的方差估计问题提供了理论解释，并提出了实用的替代推断策略，解决了实际应用中模型错误设定带来的挑战。

Abstract: Doubly robust estimators (DRE) are widely used in causal inference because they yield consistent estimators of average causal effect when at least one of the nuisance models, the propensity for treatment (exposure) or the outcome regression, is correct. However, double robustness does not extend to variance estimation; the influence-function (IF)-based variance estimator is consistent only when both nuisance parameters are correct. This raises concerns about applying DRE in practice, where model misspecification is inevitable. The recent paper by Shook-Sa et al. (2025, Biometrics, 81(2), ujaf054) demonstrated through Monte Carlo simulations that the IF-based variance estimator is biased. However, the paper's findings are empirical. The key question remains: why does the variance estimator fail in double robustness, and under what conditions do alternatives succeed, such as the ones demonstrated in Shook-Sa et al. 2025. In this paper, we develop a formal theory to clarify the efficiency properties of DRE that underlie these empirical findings. We also introduce alternative strategies, including a mixture-based framework underlying the sample-splitting and crossfitting approaches, to achieve valid inference with misspecified nuisance parameters. Our considerations are illustrated with simulation and real study data.

</details>


### [24] [The Asymptotic Distribution for a Single Joinpoint Changepoint Model](https://arxiv.org/abs/2511.17942)
*Xueheng Shi,Robert Lund*

Main category: stat.ME

TL;DR: 本文推导了单连接点变化点模型中检验变化点存在性的统计量的精确渐近分布，填补了变化点文献中的一个空白。


<details>
  <summary>Details</summary>
Motivation: 单连接点变化点模型将时间序列划分为两个段，在变化点处通过约束分段线性回归响应连续来连接。现有文献中缺少对该模型变化点存在性检验统计量渐近分布的精确推导。

Method: 推导了检验变化点存在性的统计量的精确渐近分布，该分布是单位区间上高斯过程的上确界。

Result: 得到了变化点存在性检验统计量的渐近分布，并提供了该分布的百分位数供用户使用。

Conclusion: 这项工作填补了变化点文献中的一个微妙空白，为单连接点变化点模型提供了理论支持。

Abstract: A single joinpoint changepoint model partitions a time series into two segments, joined at the changepoint time by constraining the estimated piecewise linear regression responses to be continuous. This manuscript derives the exact asymptotic distribution of the changepoint existence test statistic gauging whether or not a second segment is necessary. The identified asymptotic distribution, a supremum of a Gaussian process over the unit interval, is rather unwieldy. The work presented here provides the result and its derivation; quantiles of the asymptotic distribution are presented for the user. This addresses a subtle gap in the changepoint literature.

</details>


### [25] [Hierarchical biomarker thresholding: a model-agnostic framework for stability](https://arxiv.org/abs/2511.18030)
*O. Debeaupuis*

Main category: stat.ME

TL;DR: 提出了一个选择诚实的层次阈值框架，通过风险分解定理将患者级决策分解为内部拟合、操作点偏移和稳定性三个部分，确保跨站点决策的可重现性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的生物标志物管道中，基于池化实例调优的阈值在不同站点间经常失效，这源于层次依赖性、流行度偏移和分数尺度不匹配等问题。

Method: 使用选择诚实的层次阈值框架，基于风险分解定理分离内部拟合与泛化、操作点偏移和稳定性贡献，通过患者块自助法计算稳定性项。

Result: 该框架与模型无关，能在分位数尺度上协调异构决策规则，产生单调不变集成和可报告诊断指标。

Conclusion: 提出的框架使患者级决策更加可重现和可辩护，解决了跨站点阈值失效的核心问题。

Abstract: Many biomarker pipelines require patient-level decisions aggregated from instance-level (cell/patch) scores. Thresholds tuned on pooled instances often fail across sites due to hierarchical dependence, prevalence shift, and score-scale mismatch. We present a selection-honest framework for hierarchical thresholding that makes patient-level decisions reproducible and more defensible. At its core is a risk decomposition theorem for selection-honest thresholds. The theorem separates contributions from (i) internal fit and patient-level generalization, (ii) operating-point shift reflecting prevalence and shape changes, and (iii) a stability term that penalizes sensitivity to threshold perturbations. The stability component is computable via patient-block bootstraps mapped through a monotone modulus of risk. This framework is model-agnostic, reconciles heterogeneous decision rules on a quantile scale, and yields monotone-invariant ensembles and reportable diagnostics (e.g. flip-rate, operating-point shift).

</details>


### [26] [On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19](https://arxiv.org/abs/2511.18035)
*Giacomo Iannucci,Petros Barmpounakis,Alexandros Beskos,Nikolaos Demiris*

Main category: stat.ME

TL;DR: 提出基于强化学习的实时流行病控制决策支持框架，结合流行病模型、贝叶斯推理和强化学习控制器，在英格兰COVID-19数据上验证显示能显著降低ICU负担。


<details>
  <summary>Details</summary>
Motivation: 需要平衡疾病负担（如ICU负荷）与社会经济成本的流行病控制策略，传统方法难以实时自适应调整干预措施。

Method: 结合区室流行病模型、顺序贝叶斯推理和强化学习控制器，使用ICU阈值规则和后验平均Q学习策略，在COVID-19 ICU数据上进行反事实模拟验证。

Result: 在300天周期内，相比历史政府策略，两种强化学习控制器都能显著降低ICU负担，适用于不同成本参数设置。

Conclusion: 贝叶斯顺序学习与强化学习结合能够有效支持流行病控制政策设计，实现疾病负担与社会经济成本的平衡。

Abstract: This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies.

</details>


### [27] [Sequential Bootstrap for Out-of-Bag Error Estimation: A Simulation-Based Replication and Stability-Oriented Refinement](https://arxiv.org/abs/2511.18065)
*Cheng Peng*

Main category: stat.ME

TL;DR: 本文研究了顺序自助法(Sequential Bootstrap)作为经典自助法的受控修改，发现它在保持准确性指标不变的同时，能显著降低OOB估计器的方差相关度量。


<details>
  <summary>Details</summary>
Motivation: 传统自助法中每个重采样样本包含的独特观测数量是随机的，这种变异性如何影响OOB估计量很少被单独研究。

Method: 使用顺序自助法强制每个自助法重复样本包含相同数量的独特观测，在OOB框架下作为经典自助法的受控修改，重复了Breiman的五个原始OOB实验。

Result: 从经典自助法切换到顺序自助法后，准确性相关指标基本不变，但多个方差相关度量出现可测量且数据依赖的降低。

Conclusion: 顺序自助法不应被视为改进预测性能的新方法，而是理解独特样本数量随机性如何影响OOB估计器方差的工具。

Abstract: Bootstrap resampling is the foundation of many ensemble learning methods, and out-of-bag (OOB) error estimation is the most widely used internal measure of generalization performance. In the standard multinomial bootstrap, the number of distinct observations in each resample is random. Although this source of variability exists, it has rarely been studied in isolation to understand how much it affects OOB-based quantities. To address this gap, we investigate Sequential Bootstrap, a resampling method that forces every bootstrap replicate to contain the same number of distinct observations, and treat it as a controlled modification of the classical bootstrap within the OOB framework. We reproduce Breiman's five original OOB experiments on both synthetic and real-world datasets, repeating all analyses across many different random seeds. Our results show that switching from the classical bootstrap to Sequential Bootstrap leaves accuracy-related metrics essentially unchanged, but yields measurable and data-dependent reductions in several variance-related measures. Therefore, Sequential Bootstrap should not be viewed as a new method for improving predictive performance, but rather as a tool for understanding how randomness in the number of distinct samples contributes to the variance of OOB estimators. This work provides a reproducible setting for studying the statistical properties of resampling-based ensemble estimators and offers empirical evidence that may support future theoretical work on variance decomposition in bootstrap-based systems.

</details>


### [28] [Product Depth for Temporal Point Processes Observed Only Up to the First k Events](https://arxiv.org/abs/2511.19375)
*Chifeng Shen,Yuejiao Fu,Xiaoping Shi,Michael Chen*

Main category: stat.ME

TL;DR: 提出了一种专门为仅观测到前k个事件的时间点过程设计的新型乘积深度函数，包含归一化边缘深度和条件深度两个关键组件。


<details>
  <summary>Details</summary>
Motivation: 时间点过程在神经科学和金融等领域广泛应用，但现有的TPP深度概念有限，统计深度函数是分析多元和函数数据中心性和排名的强大工具。

Method: 设计了一个乘积深度函数，包含归一化边缘深度（捕捉最终事件的时间分布）和条件深度（表征前序事件的联合分布）。

Result: 建立了该深度函数的关键理论性质，并通过模拟研究和实际数据应用证明了其实用性。

Conclusion: 所提出的乘积深度函数为仅观测到前k个事件的时间点过程提供了有效的深度分析工具，具有理论和实践价值。

Abstract: Temporal point processes (TPPs) model the timing of discrete events along a timeline and are widely used in fields such as neuroscience and fi- nance. Statistical depth functions are powerful tools for analyzing centrality and ranking in multivariate and functional data, yet existing depth notions for TPPs remain limited. In this paper, we propose a novel product depth specifically designed for TPPs observed only up to the first k events. Our depth function comprises two key components: a normalized marginal depth, which captures the temporal distribution of the final event, and a conditional depth, which characterizes the joint distribution of the preceding events. We establish its key theoretical properties and demonstrate its practical utility through simulation studies and real data applications.

</details>


### [29] [A sensitivity analysis for non-inferiority studies with non-randomised data](https://arxiv.org/abs/2511.18094)
*Daijiro Kabata,Takumi Imai*

Main category: stat.ME

TL;DR: 本研究将E值框架重新表述，使其适用于非劣效性分析，关注临床意义边界而非统计零假设，为观察性研究提供评估未测量混杂偏倚的透明框架。


<details>
  <summary>Details</summary>
Motivation: 基于非随机数据的非劣效性研究在临床研究中日益增多，但仍易受未测量混杂偏倚影响。传统的E值主要应用于统计零假设，需要扩展其适用于临床预设边界的非劣效性分析。

Method: 使用Ding和VanderWeele的偏倚因子公式，定义非劣效性E值为未测量混杂因素与治疗和结果关联的最小强度（风险比尺度），使95%置信区间估计移至预设非劣效性边界。

Result: 应用于三个观察性研究和一项单臂试验，非劣效性E值在1到3之间变化。单臂试验中置信区间与点估计E值的较大差距反映了小样本量和宽置信区间。

Conclusion: 非劣效性E值扩展了传统E值的应用范围，为解释非随机证据提供了透明框架，并为设计更确定的随机对照试验提供见解，但仍继承原方法的局限性。

Abstract: Background: Non-inferiority studies based on non-randomised data are increasingly used in clinical research but remain prone to unmeasured confounding. The classical E-value offers a simple way to quantify such bias but has been applied almost exclusively with respect to the statistical null. We reformulated the E-value framework to make explicit its applicability to predefined clinical margins, thereby extending its utility to non-inferiority analyses.
  Development: Using the bias-factor formulation by Ding and VanderWeele, we defined the non-inferiority E-value as the minimum strength of association that an unmeasured confounder would need with both treatment and outcome, on the risk-ratio scale, to move the 95% confidence-limit estimate to the prespecified non-inferiority margin.
  Application: This approach was applied to three observational studies and one single-arm trial with external controls to illustrate interpretation and range. The resulting non-inferiority E-values for the confidence limits varied from about one to three, depending on design and findings. In the single-arm trial, a large gap between the confidence-limit and point-estimate NIEs reflected small sample size and wide confidence intervals, highlighting that both should be reported for a balanced assessment of robustness.
  Conclusion: This study reformulates the E-value to focus on clinically meaningful margins rather than the statistical null, enabling its application to non-inferiority analyses. Although the non-inferiority E-value inherits the limitations of the original method and cannot address all bias sources, it offers a transparent framework for interpreting non-randomised evidence and for generating insights that inform the design of future, more definitive randomised controlled trials.

</details>


### [30] [Sparse-Smooth Spatially Varying Coefficient Quantile Regression](https://arxiv.org/abs/2511.18106)
*Hou Jian,Meng Tan,Tian Maozai*

Main category: stat.ME

TL;DR: 提出了一种用于空间变化系数分位数回归的凸框架，将每个预测变量的位置不变全局效应与空间偏差分离。使用自适应组惩罚选择预测变量是否在空间上变化，图拉普拉斯二次项促进不规则网络上偏差的空间连续性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够区分预测变量全局效应和空间局部偏差的框架，在空间数据分析中准确识别哪些变量具有空间变化效应，哪些具有稳定的全局效应。

Method: 使用凸优化框架，结合自适应组惩罚来选择空间变化变量，图拉普拉斯正则化确保空间连续性。提供ADMM算法和光滑近端梯度方案两种求解器。

Result: 在温和条件下建立了偏差组的选择一致性、均方误差界以及全局系数的渐近正态性。模拟实验显示能够准确恢复全局与局部效应，在异方差、重尾噪声下具有竞争力的预测性能。

Conclusion: 该框架为空间变化系数分位数回归提供了一个可识别、可扩展的解决方案，能够有效区分全局和局部空间效应，适用于环境科学等领域的空间数据分析。

Abstract: We develop a convex framework for spatially varying coefficient quantile regression that, for each predictor, separates a location-invariant \emph{global} effect from a \emph{spatial deviation}. An adaptive group penalty selects whether a predictor varies over space, while a graph\textendash Laplacian quadratic promotes spatial continuity of the deviations on irregular networks. The formulation is identifiable via degree-weighted centering and scales with sparse linear algebra. We provide two practical solvers\textemdash an ADMM algorithm with closed-form proximal maps for the check loss and a smoothed proximal-gradient scheme based on the Moreau envelope\textemdash together with implementation guidance (projection for identifiability, stopping diagnostics, and preconditioning). Under mild conditions on the sampling design, covariates, error density, and graph geometry, we establish selection consistency for the deviation groups, mean-squared error bounds that balance Laplacian bias and stochastic variability, and root-\(n\) asymptotic normality for the global coefficients with an oracle property. Simulations mimicking air-pollution applications demonstrate accurate recovery of global vs.\ local effects and competitive predictive performance under heteroskedastic, heavy-tailed noise. We discuss graph construction, spatially blocked cross-validation (to prevent leakage), and options for robust standard errors under spatial dependence.

</details>


### [31] [Revisiting Penalized Likelihood Estimation for Gaussian Processes](https://arxiv.org/abs/2511.18111)
*Ayumi Mutoh,Annie S. Booth,Jonathan W. Stallrich*

Main category: stat.ME

TL;DR: 提出了一种新的交叉验证指标DPE（去相关预测误差），用于高斯过程惩罚似然框架中的调参选择，相比传统指标在正则化有益时表现更好。


<details>
  <summary>Details</summary>
Motivation: 高斯过程的性能严重依赖协方差参数估计，最大似然估计在小数据场景下存在数值问题，惩罚似然方法虽然能改进但调参选择困难。

Method: 在惩罚似然框架内引入基于马氏距离启发的去相关预测误差（DPE）指标，用于K折交叉验证中的调参选择。

Result: DPE在不需要正则化时与标准最大似然估计表现相当，在正则化有益时优于传统调参选择指标，特别是在一倍标准差规则下。

Conclusion: DPE为高斯过程惩罚似然方法提供了更一致可靠的调参选择指标，解决了传统交叉验证指标在调参选择中的不足。

Abstract: Gaussian processes (GPs) are popular as nonlinear regression models for expensive computer simulations, yet GP performance relies heavily on estimation of unknown covariance parameters. Maximum likelihood estimation (MLE) is common, but it can be plagued by numerical issues in small data settings. The addition of a nugget helps but is not a cure-all. Penalized likelihood methods may improve upon traditional MLE, but their success depends on tuning parameter selection. We introduce a new cross-validation (CV) metric called ``decorrelated prediction error'' (DPE), within the penalized likelihood framework for GPs. Inspired by the Mahalanobis distance, DPE provides more consistent and reliable tuning parameter selection than traditional metrics like prediction error, particularly for $K$-fold CV. Our proposed metric performs comparably to standard MLE when penalization is unnecessary and outperforms traditional tuning parameter selection metrics in scenarios where regularization is beneficial, especially under the one-standard error rule.

</details>


### [32] [X-chromosome Multilocus Association Studies for Common and Rare Variants](https://arxiv.org/abs/2511.18948)
*Ruilin Bai,Bo Chen*

Main category: stat.ME

TL;DR: 提出了一个理论上有依据的框架来分析多位点X染色体变异，解决了X染色体失活状态、基线等位基因、非加性和基因-性别交互效应等不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: X染色体关联研究存在特定的模型不确定性挑战，如未知的X染色体失活状态和基线等位基因，以及是否在分析中考虑非加性和基因-性别交互效应。虽然这些挑战在单一位点X染色体变异中已得到解决，但在存在上述不确定性时如何正确进行多位点关联研究仍不清楚。

Method: 首先仔细研究这些不确定性对现有多位点关联分析方法的影响，然后提出一个理论上有依据的框架来分析多位点X染色体变异，同时解决所有不确定性问题。为常见和罕见变异提供单独的解决方案。

Result: 模拟结果显示，我们的解决方案总体上比现有的用于分析常染色体变异的多位点方法更有效力。

Conclusion: 通过重新审视一些已发表的X染色体关联研究，为我们的方法提供了支持证据。

Abstract: X-chromosome association study has specific model uncertainty challenges, such as unknown X-chromosome inactivation status and baseline allele, and considering nonadditive and gene-sex interaction effects in the analysis or not. Although these challenges have been answered for single-locus X-chromosome variants, it remains unclear how to properly perform multilocus association studies when above uncertainties are present. We first carefully investigate the inferential consequences of these uncertainties on existing multilocus association analysis methods, and then propose a theoretically justified framework to analyze multilocus X-chromosome variants while all the uncertainty issues are addressed. We provide separate solutions for common and rare variants, and simulation results show that our solutions are overall more powerful than existing multilocus methods which were proposed to analyze autosomal variants. We finally provide supporting evidences of our approach by revisiting some published X-chromosome association studies.

</details>


### [33] [Spatial deformation in a Bayesian spatiotemporal model for incomplete matrix-variate responses](https://arxiv.org/abs/2511.18201)
*Rodrigo de Souza Bulhões,Marina Silva Paez,Dani Gamerman*

Main category: stat.ME

TL;DR: 提出了一种灵活的矩阵变量时空模型，用于分析在空间分布位置随时间观测的多个响应变量。该方法放宽了空间各向同性的限制性假设，通过变形方法适应空间中的方向模式和非平稳行为，并在贝叶斯框架下结合动态线性模型处理时间动态。


<details>
  <summary>Details</summary>
Motivation: 环境与生态过程中的空间各向同性假设通常不现实，需要能够适应方向模式和非平稳行为的更灵活模型。同时需要处理多变量时空数据中的缺失观测问题。

Method: 采用基于变形的协方差结构方法，结合动态线性模型在完全贝叶斯框架下建模。引入处理不同变量缺失观测的策略，保持联合数据结构而不丢弃整个时间点或站点。

Result: 通过模拟研究和空气质量监测数据应用表明，在异向性场景中纳入空间变形显著提高了插值精度，在接近各向同性情况下保持竞争性能。

Conclusion: 该方法为具有不完整数据的多变量时空建模提供了一个通用且计算可行的框架。

Abstract: In this paper, we propose a flexible matrix-variate spatiotemporal model for analyzing multiple response variables observed at spatially distributed locations over time. Our approach relaxes the restrictive assumption of spatial isotropy, which is often unrealistic in environmental and ecological processes. We adopt a deformation-based method that allows the covariance structure to adapt to directional patterns and nonstationary behavior in space. Temporal dynamics are incorporated through dynamic linear models within a fully Bayesian framework, ensuring coherent uncertainty propagation and efficient state-space inference. Additionally, we introduce a strategy for handling missing observations across different variables, preserving the joint data structure without discarding entire time points or stations. Through a simulation study and an application to real-world air quality monitoring data, we demonstrate that incorporating spatial deformation substantially improves interpolation accuracy in anisotropic scenarios while maintaining competitive performance under near-isotropy. The proposed methodology provides a general and computationally tractable framework for multivariate spatiotemporal modeling with incomplete data.

</details>


### [34] [Efficient Covariance Estimation for Sparsified Functional Data](https://arxiv.org/abs/2511.18237)
*Sijie Zheng,Fandong Meng,Jie Zhou*

Main category: stat.ME

TL;DR: 提出了一种用于稀疏化个体轨迹的协方差函数估计方法，包括Random-knots-Spatial和Bspline-Spatial两种计算高效的估计器，适用于每个受试者测量次数较少的情况。


<details>
  <summary>Details</summary>
Motivation: 受最近关于在稀疏化均值估计中利用空间相关性分析的工作启发，旨在解决经典函数数据分析需要每个受试者大量规则间隔测量的问题。

Method: 使用随机节点和B样条方法构建协方差估计器，在正则性条件下获得协方差的渐近点态性质，并采用AIC等模型选择技术确定模型维度。

Result: 提出的非参数方法在稀疏数据情况下能很好地执行函数主成分分析，通过蒙特卡洛模拟实验验证了理论结果。

Conclusion: 该方法可用于多域数据聚类，通过在PCA过程中用提出的协方差估计器替代传统协方差函数来实现。

Abstract: Motivated by recent work involving the analysis of leveraging spatial correlations in sparsified mean estimation, we present a novel procedure for constructing covariance estimator. The proposed Random-knots (Random-knots-Spatial) and B-spline (Bspline-Spatial) estimators of the covariance function are computationally efficient. Asymptotic pointwise of the covariance are obtained for sparsified individual trajectories under some regularity conditions. Our proposed nonparametric method well perform the functional principal components analysis for the case of sparsified data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. Theoretical results are illustrated with Monte Carlo simulation experiments. Finally, we cluster multi-domain data by replacing the covariance function with our proposed covariance estimator during PCA.

</details>


### [35] [Integrating Complex Covariate Transformations in Generalized Additive Models](https://arxiv.org/abs/2511.19234)
*Claudia Collarin,Matteo Fasiolo,Yannig Goude,Simon Wood*

Main category: stat.ME

TL;DR: 提出了一个将可解释的协变量变换集成到多参数广义可加模型中的新框架，将特征工程直接融入建模阶段而非预处理步骤。


<details>
  <summary>Details</summary>
Motivation: 传统特征工程通常在数据预处理阶段进行，与建模阶段分离。本文旨在将可解释的协变量变换直接整合到建模过程中，提高模型的灵活性和实用性。

Method: 开发了一个通用框架，在多参数广义可加模型中嵌入可解释的协变量变换。使用最大后验估计联合估计变换参数和回归系数，并通过近似贝叶斯技术量化联合不确定性。使用拉普拉斯近似和隐式微分方法进行平滑参数选择。

Result: 通过英国电网净需求预测和伦敦房价建模的应用，证明了该方法的灵活性和实用价值。

Conclusion: 提出的框架成功将特征工程整合到建模过程中，提供了灵活且实用的建模方法，并通过gamFactory R包实现。

Abstract: Transformations of covariates are widely used in applied statistics to improve interpretability and to satisfy assumptions required for valid inference. More broadly, feature engineering encompasses a wider set of practices aimed at enhancing predictive performance, and is typically performed as part of a data pre-processing step. In contrast, this paper integrates a substantial component of the feature engineering process directly into the modelling stage. This is achieved by introducing a novel general framework for embedding interpretable covariate transformations within multi-parameter Generalised Additive Models (GAMs). Our framework accommodates any sufficiently differentiable scalar-valued transformation of potentially high-dimensional and complex covariates. These transformations are treated as integral model components, with their parameters estimated jointly with regression coefficients via maximum a posteriori (MAP) methods, and joint uncertainty quantified via approximate Bayesian techniques. Smoothing parameters are selected in an empirical Bayes framework using a Laplace approximation to the marginal likelihood, supported by efficient computation based on implicit differentiation methods. We demonstrate the flexibility and practical value of the proposed methodology through applications to forecasting electricity net-demand in Great Britain and to modelling house prices in London. The proposed methods are implemented by the gamFactory R package, available at https://github.com/mfasiolo/gamFactory.

</details>


### [36] [Change-Point Detection With Multivariate Repeated Measures](https://arxiv.org/abs/2511.18432)
*Serim Han,Jingru Zhang,Hoseung Song*

Main category: stat.ME

TL;DR: 提出了一种新的基于图的方法，用于检测具有重复测量或局部结构数据中的变化点，结合了组内和组间信息。


<details>
  <summary>Details</summary>
Motivation: 现有的变化点检测研究很少处理具有重复测量或局部组结构的数据。通常的处理方法是平均重复测量，但这可能导致丢失重要的个体内信息。

Method: 开发了一种新的基于图的方法，通过结合个体内和个体间信息来检测变化点。推导了所提出统计量显著性的分析近似，能够高效计算组合检验统计量的p值。

Result: 该方法能有效检测各种替代假设下的变化点，特别是在存在个体内差异时。通过纽约市出租车数据集分析验证了该方法的有效性。

Conclusion: 提出的新方法能够有效处理具有重复测量或局部结构数据的变化点检测问题，特别适合存在个体内差异的情况。

Abstract: Graph-based methods have shown particular strengths in change-point detection (CPD) tasks for high-dimensional nonparametric settings. However, existing CPD research has rarely addressed data with repeated measurements or local group structures. A common treatment is to average repeated measurements, which can result in the loss of important within-individual information. In this paper, we propose a new graph-based method for detecting change-points in data with repeated measurements or local structures by incorporating both within-individual and between-individual information. Analytical approximations to the significance of the proposed statistics are derived, enabling efficient computation of p-values for the combined test statistic. The proposed method effectively detects change-points across a wide range of alternatives, particularly when within-individual differences are present. The new method is illustrated through an analysis of the New York City taxi dataset.

</details>


### [37] [Community-level core-periphery structures in collaboration networks](https://arxiv.org/abs/2511.19305)
*Sara Geremia,Domenico De Stefano,Michael Fop*

Main category: stat.ME

TL;DR: 提出了一种检测社区层面核心-边缘结构的新框架，通过优化目标函数为社区分配核心或边缘角色，考虑了社区间连接的密度和强度。


<details>
  <summary>Details</summary>
Motivation: 合作网络中存在着复杂的介观结构（如社区、核心-边缘组织和有影响力的枢纽），这些结构的共存挑战了传统方法。需要一种能够直接解释不同社会或组织群体在网络中占据中心位置的方法。

Method: 基于给定的节点分组，优化目标函数为社区分配核心或边缘角色，考虑社区间连接的密度和强度。节点级分区可以对应推断的社区或节点属性分类。

Result: 在意大利三个不同学科领域的学者合著网络中应用该方法，揭示了与机构角色、地区位置和研究主题相关的分层核心-边缘结构。

Conclusion: 该方法能够有效揭示合作网络中社区层面的核心-边缘结构，为理解知识流动和创新涌现提供了新的分析视角。

Abstract: Uncovering structural patterns in collaboration networks is key for understanding how knowledge flows and innovation emerges. These networks often exhibit a rich interplay of meso-scale structures, such as communities, core-periphery organization, and influential hubs, which shape the complexity of scientific collaboration. The coexistence of such structures challenges traditional approaches, which typically isolate specific network patterns at the node level. We introduce a novel framework for detecting core-periphery structures at the community level. Given a reference grouping of the nodes, the method optimizes an objective function that assigns core or peripheral roles to communities by accounting for the density and strength of their inter-community connections. The node-level partition may correspond to either inferred communities or to a node-attribute classification, such as discipline or location, enabling direct interpretation of how different social or organizational groups occupy central positions in the network. The method is motivated by an application to a co-authorship network of Italian academics in three different disciplines, where it reveals a hierarchical core-periphery structure associated with institutional role, regional location, and research topics.

</details>


### [38] [Hyperevent network modelling of partially observed gossip data](https://arxiv.org/abs/2511.18543)
*Veronica Poda,Veronica Vinciotti,Ernst C. Wit*

Main category: stat.ME

TL;DR: 该研究从网络理论角度将八卦视为高阶互动，提出了关系超事件模型来分析八卦的动态和病毒式传播机制，并针对匈牙利44所中学的年度调查数据开发了右删失区间时间数据的推断方法。


<details>
  <summary>Details</summary>
Motivation: 八卦是一种普遍的社会现象，涉及至少两人谈论不在场的第三方，具有动态性和可能的病毒式传播特性。研究旨在量化这些复杂动态，理解八卦的社会驱动力。

Method: 提出关系超事件模型，扩展了右删失区间时间数据的推断方法，使用灵活高效的广义加性模型估计感兴趣效应，结合线性、平滑和随机效应。

Result: 对学校数据的分析表明，该模型能够识别八卦的社会驱动力，同时揭示复杂的时间动态。

Conclusion: 所提出的模型能够有效分析八卦的高阶互动特性，为理解这种复杂社会现象的动态机制提供了量化工具。

Abstract: Gossiping is a widespread social phenomenon that shapes relationships and information flow in communities. From a network theoretic point of view, gossiping can be seen as a higher-order interaction, as it involves at least two persons talking about a non-present third. The mechanism of gossiping is complex: it is most likely dynamic, as its intensity changes over time, and possibly viral, if a gossiping event induces future gossiping, such as a repetition or retaliation. We define covariates of interest for these effects and propose a relational hyperevent model to study and quantify these complex dynamics. We consider survey data collected yearly from 44 secondary schools in Hungary. No information is available about the exact timing of the events nor about the aggregate number of events within the yearly time interval. What is measured is whether at least one gossiping event has occurred in a given time interval. We extend inference for relational hyperevent models to the case of rightcensored interval-time data and show how flexible and efficient generalized additive models can be used for estimation of effects of interest. Our analysis on the school data illustrates how a model that accounts for linear, smooth and random effects can identify the social drivers of gossiping, while revealing complex temporal dynamics.

</details>


### [39] [Hierarchical Bayesian spectral analysis of multiple stationary time series](https://arxiv.org/abs/2511.19406)
*Rebecca Lee,Alexander Coulter,Greg J. Siegle,Scott A. Bruce,Anirban Bhattacharya*

Main category: stat.ME

TL;DR: 提出HBEST方法用于分层贝叶斯估计多个时间序列的功率谱，能处理不同长度的时间序列并同时估计群体和个体水平的功率谱。


<details>
  <summary>Details</summary>
Motivation: 生物医学时间序列的功率谱对理解生理过程很重要，但同时对多个时间序列进行功率谱分析面临谱变异性和时间序列长度不一的挑战。

Method: 使用截断余弦基展开建模对数功率谱，采用新颖的全局-局部系数分解，结合收缩先验进行正则化估计和高效信息共享。

Result: 模拟实验显示HBEST在计算效率和估计精度上优于竞争方法，心率变异性应用准确捕捉了与传统心血管风险因素的关联。

Conclusion: HBEST为多时间序列功率谱分析提供了一个可解释且高效的贝叶斯框架，能准确表征功率谱并捕捉生理关联。

Abstract: The power spectrum of biomedical time series provides important indirect measurements of physiological processes underlying health and biological functions. However, simultaneously characterizing power spectra for multiple time series remains challenging due to extra spectral variability and varying time series lengths. We propose a method for hierarchical Bayesian estimation of stationary time series (HBEST) that provides an interpretable framework for efficiently modeling multiple power spectra. HBEST models log power spectra using a truncated cosine basis expansion with a novel global-local coefficient decomposition, enabling simultaneous estimation of population-level and individual-level power spectra and accommodating time series of varying lengths. The fully Bayesian framework provides shrinkage priors for regularized estimation and efficient information sharing. Simulations demonstrate HBEST's advantages over competing methods in computational efficiency and estimation accuracy. An application to heart rate variability time series demonstrates HBEST's ability to accurately characterize power spectra and capture associations with traditional cardiovascular risk factors.

</details>


### [40] [A joint optimization approach to identifying sparse dynamics using least squares kernel collocation](https://arxiv.org/abs/2511.18555)
*Alexander W. Hsu,Ike W. Griss Salas,Jacob M. Stevens-Haas,J. Nathan Kutz,Aleksandr Aravkin,Bamdad Hosseini*

Main category: stat.ME

TL;DR: 提出了一种从稀疏、部分和噪声观测中学习常微分方程系统的整体建模框架，结合稀疏恢复和再生核希尔伯特空间理论，在准确性、样本效率和噪声鲁棒性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏、部分和噪声观测时效果有限，需要开发更有效的ODE学习框架来克服这些限制。

Method: 结合函数库上的ODE稀疏恢复策略和再生核希尔伯特空间理论，用于状态估计和ODE离散化。

Result: 数值实验显示该方法在方程学习和未知状态估计方面，在准确性、样本效率和噪声鲁棒性上都有显著提升。

Conclusion: 该方法超越了现有广泛使用的算法，同时扩展了方程发现领域的建模灵活性。

Abstract: We develop an all-at-once modeling framework for learning systems of ordinary differential equations (ODE) from scarce, partial, and noisy observations of the states. The proposed methodology amounts to a combination of sparse recovery strategies for the ODE over a function library combined with techniques from reproducing kernel Hilbert space (RKHS) theory for estimating the state and discretizing the ODE. Our numerical experiments reveal that the proposed strategy leads to significant gains in terms of accuracy, sample efficiency, and robustness to noise, both in terms of learning the equation and estimating the unknown states. This work demonstrates capabilities well beyond existing and widely used algorithms while extending the modeling flexibility of other recent developments in equation discovery.

</details>


### [41] [Can discrete-time analyses be trusted for stepped wedge trials with continuous recruitment?](https://arxiv.org/abs/2511.18731)
*Hao Wang,Guangyu Tong,Heather Allore,Monica Taljaard,Fan Li*

Main category: stat.ME

TL;DR: 研究在连续招募设计的阶梯楔形集群随机试验中，使用离散时间线性混合模型进行模型稳健推断的效果，发现离散时间分析通常偏差最小，稳健方差估计器能保持名义覆盖率和I类错误率，除非招募模式在控制期和干预期存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 95.1%的横断面SW-CRT采用连续招募设计，但缺乏对此类设计进行模型稳健推断的指导，需要研究离散时间线性混合模型在连续招募情况下的适用性。

Method: 通过模拟研究，在数据生成过程中采用连续时间指数衰减相关结构表征连续招募，分析三种离散时间工作相关结构（简单可交换、嵌套可交换、离散时间指数衰减）并使用稳健三明治方差估计器。

Result: 离散时间分析通常产生最小偏差，Mancl和DeRouen修正的稳健方差估计器能持续达到名义覆盖率和I类错误率，但当控制期和干预期招募模式存在系统性差异时会出现轻微偏差。

Conclusion: 在连续招募的SW-CRT中，离散时间线性混合模型配合稳健方差估计是有效的分析方法，但需注意招募模式在不同时期的一致性。

Abstract: In stepped wedge cluster randomized trials (SW-CRTs), interventions are sequentially rolled out to clusters over multiple periods. It is common practice to analyze SW-CRTs using discrete-time linear mixed models, in which measurements are considered to be taken at discrete time points. However, a recent systematic review found that 95.1\% of cross-sectional SW-CRTs recruit individuals continuously over time. Despite the high prevalence of designs with continuous recruitment, there has been limited guidance on how to draw model-robust inference when analyzing such SW-CRTs. In this article, we investigate through simulations the implications of using discrete-time linear mixed models in the case of continuous recruitment designs with a continuous outcome. First, in the data-generating process, we characterize continuous recruitment with a continuous-time exponential decay correlation structure in the presence or absence of a continuous period effect, addressing scenarios both with and without a random or exposure-time-dependent intervention effect. Then, we analyze the simulated data under three popular discrete-time working correlation structures: simple exchangeable, nested exchangeable, and discrete-time exponential decay, with a robust sandwich variance estimator. Our results demonstrate that discrete-time analysis often yields minimum bias, and the robust variance estimator with the Mancl and DeRouen correction consistently achieves nominal coverage and type I error rate. One important exception occurs when recruitment patterns vary systematically between control and intervention periods, where discrete-time analysis leads to slightly biased estimates. Finally, we illustrate these findings by reanalyzing a concluded SW-CRT.

</details>


### [42] [Gaussian process priors with Markov properties for effective reproduction number inference](https://arxiv.org/abs/2511.18797)
*Jessalyn N. Sebastian,Volodymyr M. Minin*

Main category: stat.ME

TL;DR: 本文提出了几种用于推断有效再生数Rt的高斯马尔可夫过程先验，包括积分布朗运动，这些方法比传统的高斯随机游走先验更能捕捉先验科学知识，并在模拟和实际SARS-CoV-2数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯随机游走先验在传染病爆发参数估计中过于宽松，无法捕捉先验科学知识，导致后验方差较高，需要更合适的先验模型。

Method: 提出了几种高斯马尔可夫过程先验，特别是积分布朗运动，它可以通过增强相应的布朗运动分量表示为马尔可夫过程，计算效率高且易于实现和调整。

Result: 在模拟爆发数据中，积分布朗运动先验能够匹配或超越其他先验（包括高斯随机游走和基于Matérn协方差函数近似的最先进高斯过程先验）的性能，并在县级SARS-CoV-2数据应用中产生流行病学上合理且精确的结果。

Conclusion: 积分布朗运动先验为有效再生数Rt的推断提供了一种计算高效、易于实现且性能优越的替代方法，能够产生更精确和合理的流行病学估计。

Abstract: Many quantities characterizing infectious disease outbreaks - like the effective reproduction number ($R_t$), defined as the average number of secondary infections a newly infected individual will cause over the course of their infection - need to be modeled as time-varying parameters. It is common practice to use Gaussian random walks as priors for estimating such functions in Bayesian analyses of pathogen surveillance data. In this setting, however, the random walk prior may be too permissive, as it fails to capture prior scientific knowledge about the estimand and results in high posterior variance. We propose several Gaussian Markov process priors for $R_t$ inference, including the Integrated Brownian Motion (IBM), which can be represented as a Markov process when augmented with its corresponding Brownian Motion component, and is therefore computationally efficient and simple to implement and tune. We use simulated outbreak data to compare the performance of these proposed priors with the Gaussian random walk prior and another state-of-the-art Gaussian process prior based on an approximation to a Matérn covariance function. We find that IBM can match or exceed the performance of other priors, and we show that it produces epidemiologically reasonable and precise results when applied to county-level SARS-CoV-2 data.

</details>


### [43] [A novel nonparametric framework for DIF detection using kernel-smoothed item response curves](https://arxiv.org/abs/2511.18963)
*Adéla Hladká,Patrícia Martinková*

Main category: stat.ME

TL;DR: 提出了一种新的非参数方法来检测二元项目的差异项目功能(DIF)，通过直接比较项目反应曲线(IRCs)，在复杂场景下比传统参数方法更灵活有效。


<details>
  <summary>Details</summary>
Motivation: 传统参数方法在检测DIF时依赖于模型假设，当项目反应曲线存在多个交点等复杂模式时可能失效，需要一种更灵活的非参数方法。

Method: 基于回归曲线非参数比较的方法，扩展到二元响应数据，包括新的检验统计量渐近方差估计器、最优权重函数，并使用wild bootstrap进行推断。

Result: 蒙特卡洛模拟显示该方法能有效控制I类错误，在项目反应曲线存在多个交点时比传统逻辑回归方法表现更好，在言语攻击数据集应用中能检测到参数模型遗漏的细微DIF模式。

Conclusion: 该非参数框架为DIF检测提供了灵活而强大的替代方案，特别适用于传统模型假设不适用的复杂场景。

Abstract: This study introduces a novel nonparametric approach for detecting Differential Item Functioning (DIF) in binary items through direct comparison of Item Response Curves (IRCs). Building on prior work on nonparametric comparison of regression curves, we extend the methodology to accommodate binary response data, which is typical in psychometric applications. The proposed approach includes a new estimator of the asymptotic variance of the test statistic and derives optimal weight functions that maximise local power. Because the asymptotic distribution of the resulting test statistic is unknown, a wild bootstrap procedure is applied for inference. A Monte Carlo simulation study demonstrates that the nonparametric approach effectively controls Type I error and achieves power comparable to the traditional logistic regression method, outperforming it in cases with multiple intersections of the underlying IRCs. The impact of bandwidth and weight specification is explored. Application to a verbal aggression dataset further illustrates the method's ability to detect subtle DIF patterns missed by parametric models. Overall, the proposed nonparametric framework provides a flexible and powerful alternative for detecting DIF, particularly in complex scenarios where traditional model-based assumptions may not be applicable.

</details>


### [44] [Can we detect treatment effect waning from time-to-event data?](https://arxiv.org/abs/2511.19096)
*Eni Musta,Joris Mooij*

Main category: stat.ME

TL;DR: 该论文探讨了评估治疗效果随时间衰减的方法学问题，指出传统方法存在选择偏倚，且生存曲线比较只能反映累积效应。因果风险比需要强建模假设，但即使如此，风险比趋近1也不一定意味着保护效果在衰减。


<details>
  <summary>Details</summary>
Motivation: 理解治疗效果如何随时间演变（包括可能衰减）对于治疗中止或重复的决策很重要，但目前缺乏评估治疗效果衰减的方法学共识。

Method: 分析了传统风险函数比较的局限性，探讨了基于主分层方法或受控直接效应的因果风险比公式，并讨论了这些方法的识别问题。

Result: 发现因果风险比趋近1不一定表示治疗效果在衰减，相同的生存函数可能对应有衰减和无衰减两种情景，表明治疗效果衰减无法从标准时间到事件数据中识别。

Conclusion: 治疗效果衰减无法在没有强建模假设的情况下从标准时间到事件数据中识别，需要谨慎解释因果风险比的变化。

Abstract: Understanding how the causal effect of a treatment evolves over time, including the potential for waning, is important for informed decisions on treatment discontinuation or repetition. For example, waning vaccine protection influences booster dose recommendations, while cost-effectiveness analyses require accounting for long-term efficacy of treatments. However, there is no consensus on the methodology to assess and account for treatment effect waning. Even in randomized controlled trials, the common naïve comparison of hazard functions can lead to misleading causal conclusions due to inherent selection bias. Although comparing survival curves is sometimes recommended as a safer measure of causal effect, it only represents a cumulative effect over time and does not address treatment effect waning. We also explore recent formulations of causal hazard ratios, based on the principal stratification approach or the controlled direct effect. These causal hazard ratios cannot be identified without strong modeling assumptions, but bounds can be derived accounting for unobserved heterogeneity and one could try to use them to detect treatment effect waning. However, we illustrate that an increase in causal hazard ratios towards one does not necessarily mean that the protective effect of the treatment is fading. Furthermore, the same survival functions may correspond to both scenarios with and without waning, which shows that treatment effect waning cannot be identified from standard time-to-event data without strong untestable modeling assumptions.

</details>


### [45] [Asymptotic linear dependence and ellipse statistics for multivariate two-sample homogeneity test](https://arxiv.org/abs/2511.19381)
*Chifeng Shen,Yuejiao Fu,Michael Chen,Xiaoping Shi*

Main category: stat.ME

TL;DR: 提出了一种基于统计深度的非参数双样本检验方法，该检验在原假设下具有卡方(1)渐近分布，并通过模拟和实际数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 统计深度已成为非参数推断中的重要工具，本文旨在探索统计深度在多变量双样本问题中的应用。

Method: 开发了一种新的基于深度的非参数双样本检验方法。

Result: 模拟研究和实际数据应用表明所提出的检验方法具有高效性和实用价值。

Conclusion: 所提出的基于统计深度的双样本检验方法在理论和实践中都表现出良好的性能。

Abstract: Statistical depth, which measures the center-outward rank of a given sample with respect to its underlying distribution, has become a popular and powerful tool in nonparametric inference. In this paper, we investigate the use of statistical depth in multivariate two-sample problems. We propose a new depth-based nonparametric two-sample test, which has the Chi-square(1) asymptotic distribution under the null hypothesis. Simulations and real-data applications highlight the efficacy and practical value of the proposed test.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [46] [Metric/Nonmetric Elastic MDS](https://arxiv.org/abs/2511.19397)
*Jan de Leeuw*

Main category: stat.CO

TL;DR: 开发了R和C语言实现的弹性多维尺度分析（Elastic MDS）算法，包括度量和非度量版本，C版本比R版本快15-100倍。


<details>
  <summary>Details</summary>
Motivation: 提供McGee（1966）提出的弹性多维尺度分析技术的现代实现，比较不同编程语言实现的性能差异。

Method: 使用R和C语言分别实现了度量和非度量版本的弹性MDS算法，并进行性能对比测试。

Result: C语言版本的执行速度比R语言版本快15到100倍，显示出显著的性能优势。

Conclusion: C语言实现比R语言实现具有明显的性能优势，为大规模数据分析提供了更高效的解决方案。

Abstract: We present R and C implementations for metric (ratio) and non-metric (ordinal) versions of Elastic MDS, the multidimensional scaling technique proposed by McGee (1966). The R and C versions are compared for speed, with the C version anywhere from 15 to 100 times as fast as the R version.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [47] [Inferring Transmission Dynamics of Respiratory Syncytial Virus from Houston Wastewater](https://arxiv.org/abs/2511.17816)
*Jose R. Palacio,Katherine B. Ensor,Sallie A. Keller,Rebecca Schneider,Kaavya Domakonda,Loren Hopkins,Lauren B. Stadler*

Main category: stat.AP

TL;DR: 使用贝叶斯更新模型从废水中呼吸道合胞病毒(RSV)病毒载量数据估计有效再生数(Rt)和相对感染数，比较了两种输入策略并发现结果无显著差异。


<details>
  <summary>Details</summary>
Motivation: 废水流行病学(WBE)是追踪社区呼吸道病毒传播的有效工具，需要从废水病毒载量数据中估计关键流行病学参数如有效再生数和相对感染数。

Method: 采用简约的贝叶斯更新模型，通过生物学驱动的世代和脱落核将潜在感染与测量的病毒载量联系起来，比较了原始病毒载量测量和状态空间滤波负载估计两种输入策略。

Result: 两种输入策略在推断的传播轨迹或峰值时间上没有实际意义的差异，状态空间滤波输入因其包含周特异性方差而被推荐为实用默认方法。

Conclusion: 废水病毒载量数据可用于可靠估计有效再生数和相对感染数，状态空间滤波方法在保持流行病学结论不变的同时提供了更实用的输入处理方式。

Abstract: Wastewater-based epidemiology (WBE) is an effective tool for tracking community circulation of respiratory viruses. We address estimating the effective reproduction number ($R_t$) and the relative number of infections from wastewater viral load. Using weekly Houston data on respiratory syncytial virus (RSV), we implement a parsimonious Bayesian renewal model that links latent infections to measured viral load through biologically motivated generation and shedding kernels. The framework yields estimates of $R_t$ and relative infections, enabling a coherent interpretation of transmission timing and phase. We compare two input strategies-(i) raw viral-load measurements with a log-scale standard deviation, and (ii) state-space-filtered load estimates with time-varying variances-and find no practically meaningful differences in inferred trajectories or peak timing. Given this equivalence, we report the filtered input as a pragmatic default because it embeds week-specific variances while leaving epidemiological conclusions unchanged.

</details>


### [48] [Fractional cumulative Residual Inaccuracy in the Quantile Framework and its Appications](https://arxiv.org/abs/2511.18844)
*Iona Ann Sebastian,S. M. Sunoj*

Main category: stat.AP

TL;DR: 本文提出了基于分位数的分数累积残差不精确性(FCRI)度量，用于衡量系统间的差异，特别是在分布函数不可用但分位数函数可用的情况下。


<details>
  <summary>Details</summary>
Motivation: 当分布函数没有显式形式但具有闭式分位数函数时，需要一种替代方法来衡量系统间的差异，特别是在分数和混沌映射参数影响下的系统差异区域。

Method: 引入基于分位数的FCRI度量，研究其各种性质，处理其非参数估计，并通过模拟研究和Nifty 50数据集验证其有效性。

Result: 开发了基于分位数的FCRI度量方法，能够有效衡量混沌系统间的差异以及不同时间机制下的差异。

Conclusion: 基于分位数的FCRI提供了一种有效的替代方法，当分布函数不可用时仍能准确衡量系统差异，在混沌系统分析和时间序列分析中具有实用价值。

Abstract: Fractional cumulative residual inaccuracy (FCRI) measure allows to determine regions of discrepancy between systems, depending on their respective fractional and chaotic map parameters. Most of the theoretical results and applications related to the FCRI of the lifetime random variable are based on the distribution function approach. However, there are situations in which the distribution function may not be available in explicit form but has a closed-form quantile function (QF), an alternative method of representing a probability distribution. Motivated by these, the present study is devoted to introduce a quantile-based FCRI and study its various properties. We also deal with non-parametric estimation of quantile-based FCRI and examine its validity using simulation studies and illustrate its usefulness in measuring the discrepancy between chaotic systems and in measuring the discrepancy in two different time regimes using Nifty 50 dataset.

</details>


### [49] [Validity in machine learning for extreme event attribution](https://arxiv.org/abs/2511.19039)
*Cassandra C. Chou,Scott L. Zeger,Benjamin Q. Huynh*

Main category: stat.AP

TL;DR: 机器学习在极端事件归因中存在三个主要有效性威胁：算法设计敏感性、性能指标与归因误差相关性弱、分布偏移影响预测性能。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习在极端事件归因中的有效性，因为高风险的机器学习应用常因固有偏见和缺乏鲁棒性而受到批评。

Method: 使用机器学习和模拟分析评估2003-2020年加州野火数据的极端事件归因，识别有效性威胁并提出改进方法。

Result: 发现三个主要有效性威胁：算法设计敏感性、性能指标与归因误差相关性弱、分布偏移显著降低预测性能。

Conclusion: 提出基于聚合机器学习估计、使用平均校准误差评估性能、以及使用子组和倾向诊断评估分布偏移的更有效和鲁棒的归因分析方法。

Abstract: Extreme event attribution (EEA), an approach for assessing the extent to which disasters are caused by climate change, is crucial for informing climate policy and legal proceedings. Machine learning is increasingly used for EEA by modeling rare weather events otherwise too complex or computationally intensive to model using traditional simulation methods. However, the validity of using machine learning in this context remains unclear, particularly as high-stakes machine learning applications in general are criticized for inherent bias and lack of robustness. Here we use machine learning and simulation analyses to evaluate EEA in the context of California wildfire data from 2003-2020. We identify three major threats to validity: (1) individual event attribution estimates are highly sensitive to algorithmic design choices; (2) common performance metrics like area under the ROC curve or Brier score are not strongly correlated with attribution error, facilitating suboptimal model selection; and (3) distribution shift -- changes in temperature across climate scenarios -- substantially degrades predictive performance. To address these challenges, we propose a more valid and robust attribution analysis based on aggregate machine learning estimates, using an additional metric -- mean calibration error -- to assess model performance, and using subgroup and propensity diagnostics to assess distribution shift.

</details>


### [50] [Modeling smooth and localized mortality patterns across age, time, and space to uncover small-area inequalities](https://arxiv.org/abs/2511.19151)
*Jacob Martin,Carlo Giovanni Camarda*

Main category: stat.AP

TL;DR: 提出一个灵活的小区域死亡率估计模型，通过跨年龄、空间和时间维度借力，能够在小人口中准确估计死亡率和趋势，并能处理突发死亡率冲击。


<details>
  <summary>Details</summary>
Motivation: 小区域死亡率估计存在困难，因为低死亡计数带来的随机波动会掩盖真实的地理差异，需要一种能处理这种挑战的方法。

Method: 在惩罚样条框架内实现模型，使用广义线性阵列模型技术进行估计，确保跨年龄、空间和时间的平滑模式，同时允许局部偏离空间结构。

Result: 应用模型估计了2002-2024年大伦敦地区4800多个小区域的预期寿命和年龄特异性死亡率不平等。

Conclusion: 该方法计算快速、可解释且简约，能够轻松纳入突发死亡率冲击，对现实世界的人口统计和流行病学挑战具有高度适应性。

Abstract: Small-area mortality estimation is inherently difficult, as random fluctuations from low death counts can obscure real geographic differences. We introduce a flexible model that borrows strength across age, space, and time to estimate mortality schedules and trends in very small populations. The approach ensures smooth patterns across these dimensions while allowing localized breaks from the spatial structure, capturing broad trajectories as well as sharp local contrasts. We implement our model within a Penalized Spline framework and estimate it using Generalized Linear Array Model techniques, resulting in a computationally fast, interpretable, and parsimonious method. Crucially, it can readily incorporate sudden mortality shocks, such as the Covid-19 pandemic, making it highly versatile for real-world demographic and epidemiological challenges. We demonstrate its application by estimating life expectancy and age-specific mortality inequalities in over 4,800 small areas across the Greater London Authority from 2002 to 2024.

</details>
