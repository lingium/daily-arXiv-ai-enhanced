{"id": "2602.16914", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16914", "abs": "https://arxiv.org/abs/2602.16914", "authors": ["Kiana Farhadyar", "Maren Hackenberg", "Kira Ahrens", "Charlotte Schenk", "Bianca Kollmann", "Oliver Tüscher", "Klaus Lieb", "Michael M. Plichta", "Andreas Reif", "Raffael Kalisch", "Martin Wolkewitz", "Moritz Hess", "Harald Binder"], "title": "A statistical perspective on transformers for small longitudinal cohort data", "comment": null, "summary": "Modeling of longitudinal cohort data typically involves complex temporal dependencies between multiple variables. There, the transformer architecture, which has been highly successful in language and vision applications, allows us to account for the fact that the most recently observed time points in an individual's history may not always be the most important for the immediate future. This is achieved by assigning attention weights to observations of an individual based on a transformation of their values. One reason why these ideas have not yet been fully leveraged for longitudinal cohort data is that typically, large datasets are required. Therefore, we present a simplified transformer architecture that retains the core attention mechanism while reducing the number of parameters to be estimated, to be more suitable for small datasets with few time points. Guided by a statistical perspective on transformers, we use an autoregressive model as a starting point and incorporate attention as a kernel-based operation with temporal decay, where aggregation of multiple transformer heads, i.e. different candidate weighting schemes, is expressed as accumulating evidence on different types of underlying characteristics of individuals. This also enables a permutation-based statistical testing procedure for identifying contextual patterns. In a simulation study, the approach is shown to recover contextual dependencies even with a small number of individuals and time points. In an application to data from a resilience study, we identify temporal patterns in the dynamics of stress and mental health. This indicates that properly adapted transformers can not only achieve competitive predictive performance, but also uncover complex context dependencies in small data settings."}
{"id": "2602.17414", "categories": ["stat.CO", "astro-ph.IM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17414", "abs": "https://arxiv.org/abs/2602.17414", "authors": ["David Yallup"], "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models", "comment": "26 pages, 6 figures", "summary": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle."}
{"id": "2602.17052", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.17052", "abs": "https://arxiv.org/abs/2602.17052", "authors": ["Leon Tran", "Ting Ye", "Peng Ding", "Fang Han"], "title": "Generative modeling for the bootstrap", "comment": "62 pages", "summary": "Generative modeling builds on and substantially advances the classical idea of simulating synthetic data from observed samples. This paper shows that this principle is not only natural but also theoretically well-founded for bootstrap inference: it yields statistically valid confidence intervals that apply simultaneously to both regular and irregular estimators, including settings in which Efron's bootstrap fails. In this sense, the generative modeling-based bootstrap can be viewed as a modern version of the smoothed bootstrap: it could mitigate the curse of dimensionality and remain effective in challenging regimes where estimators may lack root-$n$ consistency or a Gaussian limit."}
{"id": "2602.17187", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17187", "abs": "https://arxiv.org/abs/2602.17187", "authors": ["Sorawit Saengkyongam", "Juan L. Gamella", "Andrew C. Miller", "Jonas Peters", "Nicolai Meinshausen", "Christina Heinze-Deml"], "title": "Anti-causal domain generalization: Leveraging unlabeled data", "comment": null, "summary": "The problem of domain generalization concerns learning predictive models that are robust to distribution shifts when deployed in new, previously unseen environments. Existing methods typically require labeled data from multiple training environments, limiting their applicability when labeled data are scarce. In this work, we study domain generalization in an anti-causal setting, where the outcome causes the observed covariates. Under this structure, environment perturbations that affect the covariates do not propagate to the outcome, which motivates regularizing the model's sensitivity to these perturbations. Crucially, estimating these perturbation directions does not require labels, enabling us to leverage unlabeled data from multiple environments. We propose two methods that penalize the model's sensitivity to variations in the mean and covariance of the covariates across environments, respectively, and prove that these methods have worst-case optimality guarantees under certain classes of environments. Finally, we demonstrate the empirical performance of our approach on a controlled physical system and a physiological signal dataset."}
{"id": "2602.17161", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17161", "abs": "https://arxiv.org/abs/2602.17161", "authors": ["Nils Lid Hjort"], "title": "Dynamic likelihood hazard rate estimation", "comment": "20 pages, no figures; Statistical Research Report from 1993 (Department of Mathematics, University of Oslo); accepted with \"minor revision\" by Biometrika then, but somehow I never got around to do the final polish. This report, arXiv'd now in 2026, might be modified and updated (and illustrated with real data) for later journal publication", "summary": "The best known methods for estimating hazard rate functions in survival analysis models are either purely parametric or purely nonparametric. The parametric ones are sometimes too biased while the nonparametric ones are sometimes too variable. In the present paper a certain semiparametric approach to hazard rate estimation, proposed in Hjort (1991), is developed further, aiming to combine parametric and nonparametric features. It uses a dynamic local likelihood approach to fit the locally most suitable member in a given parametric class of hazard rates, and amounts to a version of nonparametric parameter smoothing within the parametric class. Thus the parametric hazard rate estimate at time $s$ inserts a parameter estimate that also depends on $s$. We study bias and variance properties of the resulting estimator and methods for choosing the local smoothing parameter. It is shown that dynamic likelihood estimation often leads to better performance than the purely nonparametric methods, while also having capacity for not losing much to the parametric methods in cases where the model being smoothed is adequate."}
{"id": "2602.17079", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17079", "abs": "https://arxiv.org/abs/2602.17079", "authors": ["Dylan Munson", "Arijit Dey", "Simon Mak"], "title": "Environmental policy in the context of complex systems: Statistical optimization and sensitivity analysis for ABMs", "comment": null, "summary": "Coupled human-environment systems are increasingly being understood as complex adaptive systems (CAS), in which micro-level interactions between components lead to emergent behavior. Agent-based models (ABMs) hold great promise for environmental policy design by capturing such complex behavior, enabling a sophisticated understanding of potential interventions. One limitation, however, is that ABMs can be computationally costly to simulate, which hinders their use for policy optimization. To address this, we propose a new statistical framework that exploits machine learning techniques to accelerate policy optimization with costly ABMs. We first develop a statistical approach for sensitivity testing of the optimal policy, then leverage a reinforcement learning method for efficient policy optimization. We test this framework on the classic ``Sugarscape'' model, an ABM for resource harvesting. We show that our approach can quickly identify optimal and interpretable policies that improve upon baseline techniques, with insightful sensitivity and dynamic analyses that connect back to economic theory."}
{"id": "2602.17255", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17255", "abs": "https://arxiv.org/abs/2602.17255", "authors": ["Marc Delord"], "title": "Selection and Collider Restriction Bias Due to Predictor Availability in Prognostic Models", "comment": null, "summary": "This methodological note investigates and discuss possible selection and collider restriction bias due to predictor availability in prognostic models."}
{"id": "2602.17255", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17255", "abs": "https://arxiv.org/abs/2602.17255", "authors": ["Marc Delord"], "title": "Selection and Collider Restriction Bias Due to Predictor Availability in Prognostic Models", "comment": null, "summary": "This methodological note investigates and discuss possible selection and collider restriction bias due to predictor availability in prognostic models."}
{"id": "2602.17272", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17272", "abs": "https://arxiv.org/abs/2602.17272", "authors": ["Alexandra Daub", "Elisabeth Bergherr"], "title": "Estimating Zero-inflated Negative Binomial GAMLSS via a Balanced Gradient Boosting Approach with an Application to Antenatal Care Data from Nigeria", "comment": null, "summary": "Statistical boosting algorithms are renowned for their intrinsic variable selection and enhanced predictive performance compared to classical statistical methods, making them especially useful for complex models such as generalized additive models for location scale and shape (GAMLSS). Boosting this model class can suffer from imbalanced updates across the distribution parameters as well as long computation times. Shrunk optimal step lengths have been shown to address these issues. To examine the influence of socio-economic factors on the distribution of the number of antenatal care visits in Nigeria, we generalize boosting of GAMLSS with shrunk optimal step lengths to base-learners beyond simple linear models and to a more complex response variable distribution. In an extensive simulation study and in the application we demonstrate that shrunk optimal step lengths yield a more balanced regularization of the overall model and enhance computational efficiency across diverse settings, in particular in the presence of base-learners penalizing the size of the fit."}
{"id": "2602.16914", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.16914", "abs": "https://arxiv.org/abs/2602.16914", "authors": ["Kiana Farhadyar", "Maren Hackenberg", "Kira Ahrens", "Charlotte Schenk", "Bianca Kollmann", "Oliver Tüscher", "Klaus Lieb", "Michael M. Plichta", "Andreas Reif", "Raffael Kalisch", "Martin Wolkewitz", "Moritz Hess", "Harald Binder"], "title": "A statistical perspective on transformers for small longitudinal cohort data", "comment": null, "summary": "Modeling of longitudinal cohort data typically involves complex temporal dependencies between multiple variables. There, the transformer architecture, which has been highly successful in language and vision applications, allows us to account for the fact that the most recently observed time points in an individual's history may not always be the most important for the immediate future. This is achieved by assigning attention weights to observations of an individual based on a transformation of their values. One reason why these ideas have not yet been fully leveraged for longitudinal cohort data is that typically, large datasets are required. Therefore, we present a simplified transformer architecture that retains the core attention mechanism while reducing the number of parameters to be estimated, to be more suitable for small datasets with few time points. Guided by a statistical perspective on transformers, we use an autoregressive model as a starting point and incorporate attention as a kernel-based operation with temporal decay, where aggregation of multiple transformer heads, i.e. different candidate weighting schemes, is expressed as accumulating evidence on different types of underlying characteristics of individuals. This also enables a permutation-based statistical testing procedure for identifying contextual patterns. In a simulation study, the approach is shown to recover contextual dependencies even with a small number of individuals and time points. In an application to data from a resilience study, we identify temporal patterns in the dynamics of stress and mental health. This indicates that properly adapted transformers can not only achieve competitive predictive performance, but also uncover complex context dependencies in small data settings."}
{"id": "2602.17414", "categories": ["stat.CO", "astro-ph.IM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17414", "abs": "https://arxiv.org/abs/2602.17414", "authors": ["David Yallup"], "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models", "comment": "26 pages, 6 figures", "summary": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle."}
