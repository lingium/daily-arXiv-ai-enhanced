{"id": "2510.20023", "categories": ["stat.OT", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20023", "abs": "https://arxiv.org/abs/2510.20023", "authors": ["Alexander G. Tartakovskya", "Jay Bartroff", "Cheng-Der Fuh", "Haipeng Xing"], "title": "Change, dependence, and discovery: Celebrating the work of T.L. Lai", "comment": null, "summary": "Tze Leung Lai made seminal contributions to sequential analysis, particularly\nin sequential hypothesis testing, changepoint detection and nonlinear renewal\ntheory. His work established fundamental optimality results for the sequential\nprobability ratio test and its extensions, and provided a general framework for\ntesting composite hypotheses. In changepoint detection, he introduced new\noptimality criteria and computationally efficient procedures that remain\ninfluential. He applied these and related tools to problems in biostatistics.\nIn this article, we review these key results in the broader context of\nsequential analysis."}
{"id": "2510.20100", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2510.20100", "abs": "https://arxiv.org/abs/2510.20100", "authors": ["Biviana Marcela Suarez Sierra"], "title": "Factors Associated with Unit-Specific Failure in a University-Level Statistics Course", "comment": null, "summary": "This study investigates the factors associated with failure in each of the\nfour thematic units of a General Statistics course offered at a private\nuniversity in Colombia. Unlike traditional analyses that treat performance as a\nsingle outcome, this research disaggregates results by unit: Exploratory Data\nAnalysis, Probability and Random Variables, Statistical Inference, and Linear\nRegression -- highlighting distinct challenges across content areas. Based on a\nsample of 186 undergraduate students from Engineering, Geology, and Interactive\nDesign programs, the study combines exam performance data with self-perceived\npreparedness surveys to develop unit-specific logistic regression models. The\nfindings reveal consistent structural disadvantages for students from\nnon-engineering programs, especially in concept-heavy units such as Inference\nand Regression. Academic stage and perception of competence also emerged as\nimportant predictors, though their effects varied across units. The results\nalign with prior research on statistical thinking and self-efficacy, and\nsupport the need for targeted pedagogical interventions and curricular\nalignment. This disaggregated approach offers a more nuanced understanding of\nacademic vulnerability in statistics education and contributes to the design of\nevidence-based, context-sensitive strategies to reduce failure and improve\nlearning outcomes."}
{"id": "2510.20259", "categories": ["stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.20259", "abs": "https://arxiv.org/abs/2510.20259", "authors": ["Bowen Gang", "Hongmei Lin", "Tiejun Tong"], "title": "Unifying Boxplots: A Multiple Testing Perspective", "comment": null, "summary": "Tukey's boxplot is a foundational tool for exploratory data analysis, but its\nclassic outlier-flagging rule does not account for the sample size, and\nsubsequent modifications have often been presented as separate, heuristic\nadjustments. In this paper, we propose a unifying framework that recasts the\nboxplot and its variants as graphical implementations of multiple testing\nprocedures. We demonstrate that Tukey's original method is equivalent to an\nunadjusted procedure, while existing sample-size-aware modifications correspond\nto controlling the Family-Wise Error Rate (FWER) or the Per-Family Error Rate\n(PFER). This perspective not only systematizes existing methods but also\nnaturally leads to new, more adaptive constructions. We introduce a boxplot\nmotivated by the False Discovery Rate (FDR), and show how our framework\nprovides a flexible pipeline for integrating state-of-the-art robust estimation\ntechniques directly into the boxplot's graphical format. By connecting a\nclassic graphical tool to the principles of multiple testing, our work provides\na principled language for comparing, critiquing, and extending outlier\ndetection rules for modern exploratory analysis."}
{"id": "2510.19999", "categories": ["stat.ML", "cs.LG", "cs.MS", "cs.NA", "math.NA", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.19999", "abs": "https://arxiv.org/abs/2510.19999", "authors": ["Yixiao Wang", "Zishan Shao", "Ting Jiang", "Aditya Devarakonda"], "title": "Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models", "comment": "Equal contribution: Yixiao Wang and Zishan Shao. Correspondence:\n  yw676@duke.edu", "summary": "We present a novel enhanced cyclic coordinate descent (ECCD) framework for\nsolving generalized linear models with elastic net constraints that reduces\ntraining time in comparison to existing state-of-the-art methods. We redesign\nthe CD method by performing a Taylor expansion around the current iterate to\navoid nonlinear operations arising in the gradient computation. By introducing\nthis approximation, we are able to unroll the vector recurrences occurring in\nthe CD method and reformulate the resulting computations into more efficient\nbatched computations. We show empirically that the recurrence can be unrolled\nby a tunable integer parameter, $s$, such that $s > 1$ yields performance\nimprovements without affecting convergence, whereas $s = 1$ yields the original\nCD method. A key advantage of ECCD is that it avoids the convergence delay and\nnumerical instability exhibited by block coordinate descent. Finally, we\nimplement our proposed method in C++ using Eigen to accelerate linear algebra\ncomputations. Comparison of our method against existing state-of-the-art\nsolvers shows consistent performance improvements of $3\\times$ in average for\nregularization path variant on diverse benchmark datasets. Our implementation\nis available at https://github.com/Yixiao-Wang-Stats/ECCD."}
{"id": "2510.19960", "categories": ["stat.ME", "math.ST", "stat.TH", "62G07, 62G20, 65D07", "G.3"], "pdf": "https://arxiv.org/pdf/2510.19960", "abs": "https://arxiv.org/abs/2510.19960", "authors": ["Nicholas Tenkorang", "Kwesi Appau Ohene-Obeng", "Xiaogang Su"], "title": "Kernel Density Estimation and Convolution Revisited", "comment": "22 pages, 2 figrues, plus a 8-page supplement", "summary": "Kernel Density Estimation (KDE) is a cornerstone of nonparametric statistics,\nyet it remains sensitive to bandwidth choice, boundary bias, and computational\ninefficiency. This study revisits KDE through a principled convolutional\nframework, providing an intuitive model-based derivation that naturally extends\nto constrained domains, such as positive-valued random variables. Building on\nthis perspective, we introduce SHIDE (Simulation and Histogram Interpolation\nfor Density Estimation), a novel and computationally efficient density\nestimator that generates pseudo-data by adding bounded noise to observations\nand applies spline interpolation to the resulting histogram. The noise is\nsampled from a class of bounded polynomial kernel densities, constructed\nthrough convolutions of uniform distributions, with a natural bandwidth\nparameter defined by the kernel's support bound. We establish the theoretical\nproperties of SHIDE, including pointwise consistency, bias-variance\ndecomposition, and asymptotic MISE, showing that SHIDE attains the classical\n$n^{-4/5}$ convergence rate while mitigating boundary bias. Two data-driven\nbandwidth selection methods are developed, an AMISE-optimal rule and a\npercentile-based alternative, which are shown to be asymptotically equivalent.\nExtensive simulations demonstrate that SHIDE performs comparably to or\nsurpasses KDE across a broad range of models, with particular advantages for\nbounded and heavy-tailed distributions. These results highlight SHIDE as a\ntheoretically grounded and practically robust alternative to traditional KDE."}
{"id": "2510.20141", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20141", "abs": "https://arxiv.org/abs/2510.20141", "authors": ["Somayajulu L. N. Dhulipala", "Deep Ray", "Nicholas Forman"], "title": "Compositional Generation for Long-Horizon Coupled PDEs", "comment": null, "summary": "Simulating coupled PDE systems is computationally intensive, and prior\nefforts have largely focused on training surrogates on the joint (coupled)\ndata, which requires a large amount of data. In the paper, we study\ncompositional diffusion approaches where diffusion models are only trained on\nthe decoupled PDE data and are composed at inference time to recover the\ncoupled field. Specifically, we investigate whether the compositional strategy\ncan be feasible under long time horizons involving a large number of time\nsteps. In addition, we compare a baseline diffusion model with that trained\nusing the v-parameterization strategy. We also introduce a symmetric\ncompositional scheme for the coupled fields based on the Euler scheme. We\nevaluate on Reaction-Diffusion and modified Burgers with longer time grids, and\nbenchmark against a Fourier Neural Operator trained on coupled data. Despite\nseeing only decoupled training data, the compositional diffusion models recover\ncoupled trajectories with low error. v-parameterization can improve accuracy\nover a baseline diffusion model, while the neural operator surrogate remains\nstrongest given that it is trained on the coupled data. These results show that\ncompositional diffusion is a viable strategy towards efficient, long-horizon\nmodeling of coupled PDEs."}
{"id": "2510.20023", "categories": ["stat.OT", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20023", "abs": "https://arxiv.org/abs/2510.20023", "authors": ["Alexander G. Tartakovskya", "Jay Bartroff", "Cheng-Der Fuh", "Haipeng Xing"], "title": "Change, dependence, and discovery: Celebrating the work of T.L. Lai", "comment": null, "summary": "Tze Leung Lai made seminal contributions to sequential analysis, particularly\nin sequential hypothesis testing, changepoint detection and nonlinear renewal\ntheory. His work established fundamental optimality results for the sequential\nprobability ratio test and its extensions, and provided a general framework for\ntesting composite hypotheses. In changepoint detection, he introduced new\noptimality criteria and computationally efficient procedures that remain\ninfluential. He applied these and related tools to problems in biostatistics.\nIn this article, we review these key results in the broader context of\nsequential analysis."}
{"id": "2510.20617", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2510.20617", "abs": "https://arxiv.org/abs/2510.20617", "authors": ["Dana Naderi", "Christian P Robert", "Kaniav Kamary", "Darren Wraith§"], "title": "Approximating evidence via bounded harmonic means", "comment": null, "summary": "Efficient Bayesian model selection relies on the model evidence or marginal\nlikelihood, whose computation often requires evaluating an intractable\nintegral. The harmonic mean estimator (HME) has long been a standard method of\napproximating the evidence. While computationally simple, the version\nintroduced by Newton and Raftery (1994) potentially suffers from infinite\nvariance. To overcome this issue, Gelfand and Dey (1994) defined a standardized\nrepresentation of the estimator based on an instrumental function and Robert\nand Wraith (2009) later proposed to use higher posterior density (HPD)\nindicators as instrumental functions. Following this approach, a practical\nmethod is proposed, based on an elliptical covering of the HPD region with\nnon-overlapping ellipsoids. The resulting estimator (ECMLE) not only eliminates\nthe infinite-variance issue of the original HME and allows exact volume\ncomputations, but is also able to be used in multi-modal settings. Through\nseveral examples, we illustrate that ECMLE outperforms other recent methods\nsuch as THAMES and Mixture THAMES (Metodiev et al., 2025). Moreover, ECMLE\ndemonstrates lower variance, a key challenge that subsequent HME variants have\nsought to address, and provides more stable evidence approximations, even in\nchallenging settings."}
{"id": "2510.20344", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20344", "abs": "https://arxiv.org/abs/2510.20344", "authors": ["Wei Cao", "Shanshan Wang"], "title": "Neural Networks for Censored Expectile Regression Based on Data Augmentation", "comment": null, "summary": "Expectile regression neural networks (ERNNs) are powerful tools for capturing\nheterogeneity and complex nonlinear structures in data. However, most existing\nresearch has primarily focused on fully observed data, with limited attention\npaid to scenarios involving censored observations. In this paper, we propose a\ndata augmentation based ERNNs algorithm, termed DAERNN, for modeling\nheterogeneous censored data. The proposed DAERNN is fully data driven, requires\nminimal assumptions, and offers substantial flexibility. Simulation studies and\nreal data applications demonstrate that DAERNN outperforms existing censored\nERNNs methods and achieves predictive performance comparable to models trained\non fully observed data. Moreover, the algorithm provides a unified framework\nfor handling various censoring mechanisms without requiring explicit parametric\nmodel specification, thereby enhancing its applicability to practical censored\ndata analysis."}
{"id": "2510.20372", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20372", "abs": "https://arxiv.org/abs/2510.20372", "authors": ["Lucas Darius Konrad", "Nikolas Kuschnig"], "title": "Testing Most Influential Sets", "comment": "9 pages, 1 figure, submitted to ICLR", "summary": "Small subsets of data with disproportionate influence on model outcomes can\nhave dramatic impacts on conclusions, with a few data points sometimes\noverturning key findings. While recent work has developed methods to identify\nthese \\emph{most influential sets}, no formal theory exists to determine when\ntheir influence reflects genuine problems rather than natural sampling\nvariation. We address this gap by developing a principled framework for\nassessing the statistical significance of most influential sets. Our\ntheoretical results characterize the extreme value distributions of maximal\ninfluence and enable rigorous hypothesis tests for excessive influence,\nreplacing current ad-hoc sensitivity checks. We demonstrate the practical value\nof our approach through applications across economics, biology, and machine\nlearning benchmarks."}
{"id": "2510.20147", "categories": ["stat.ME", "stat.CO", "62H12, 62F10, 65C60"], "pdf": "https://arxiv.org/pdf/2510.20147", "abs": "https://arxiv.org/abs/2510.20147", "authors": ["Qingyang Liu", "Sanvesh Srivastava", "Dipankar Bandyopadhyay"], "title": "Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses", "comment": "48 pages, 7 figures", "summary": "We propose a regression model with matrix-variate skew-t response (REGMVST)\nfor analyzing irregular longitudinal data with skewness, symmetry, or heavy\ntails. REGMVST models matrix-variate responses and predictors, with rows\nindexing longitudinal measurements per subject. It uses the matrix-variate\nskew-t (MVST) distribution to handle skewness and heavy tails, a damped\nexponential correlation (DEC) structure for row-wise dependencies across\nirregular time profiles, and leaves the column covariance unstructured. For\nestimation, we initially develop an ECME algorithm for parameter estimation and\nfurther mitigate its computational bottleneck via an asynchronous and\ndistributed ECME (ADECME) extension. ADECME accelerates the E-step through\nparallelization, and retains the simplicity of the conditional M-step, enabling\nscalable inference. Simulations using synthetic data and a case study exploring\nmatrix-variate periodontal disease endpoints derived from electronic health\nrecords demonstrate ADECME's superiority in efficiency and convergence, over\nthe alternatives. We also provide theoretical support for our empirical\nobservations and identify regularity assumptions for ADECME's optimal\nperformance. An accompanying R package is available at\nhttps://github.com/rh8liuqy/STMATREG."}
{"id": "2510.20012", "categories": ["stat.AP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20012", "abs": "https://arxiv.org/abs/2510.20012", "authors": ["Adam Diamant"], "title": "AI Pose Analysis and Kinematic Profiling of Range-of-Motion Variations in Resistance Training", "comment": null, "summary": "This study develops an AI-based pose estimation pipeline to enable precise\nquantification of movement kinematics in resistance training. Using video data\nfrom Wolf et al. (2025), which compared lengthened partial (pROM) and full\nrange-of-motion (fROM) training across eight upper-body exercises in 26\nparticipants, 280 recordings were processed to extract frame-level joint-angle\ntrajectories. After filtering and smoothing, per-set metrics were derived,\nincluding range of motion (ROM), tempo, and concentric/eccentric phase\ndurations. A random-effects meta-analytic model was applied to account for\nwithin-participant and between-exercise variability. Results show that pROM\nrepetitions were performed with a smaller ROM and shorter overall durations,\nparticularly during the eccentric phase of movement. Variance analyses revealed\nthat participant-level differences, rather than exercise-specific factors, were\nthe primary driver of variation, although there is substantial evidence of\nheterogeneous treatment effects. We then introduce a novel metric, \\%ROM, which\nis the proportion of full ROM achieved during pROM, and demonstrate that this\ndefinition of lengthened partials remains relatively consistent across\nexercises. Overall, these findings suggest that lengthened partials differ from\nfull ROM training not only in ROM, but also in execution dynamics and\nconsistency, highlighting the potential of AI-based methods for advancing\nresearch and improving resistance training prescription."}
{"id": "2510.20372", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20372", "abs": "https://arxiv.org/abs/2510.20372", "authors": ["Lucas Darius Konrad", "Nikolas Kuschnig"], "title": "Testing Most Influential Sets", "comment": "9 pages, 1 figure, submitted to ICLR", "summary": "Small subsets of data with disproportionate influence on model outcomes can\nhave dramatic impacts on conclusions, with a few data points sometimes\noverturning key findings. While recent work has developed methods to identify\nthese \\emph{most influential sets}, no formal theory exists to determine when\ntheir influence reflects genuine problems rather than natural sampling\nvariation. We address this gap by developing a principled framework for\nassessing the statistical significance of most influential sets. Our\ntheoretical results characterize the extreme value distributions of maximal\ninfluence and enable rigorous hypothesis tests for excessive influence,\nreplacing current ad-hoc sensitivity checks. We demonstrate the practical value\nof our approach through applications across economics, biology, and machine\nlearning benchmarks."}
{"id": "2510.20404", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20404", "abs": "https://arxiv.org/abs/2510.20404", "authors": ["Shuyuan Chen", "Peng Zhang", "Yifan Cui"], "title": "Identification and Debiased Learning of Causal Effects with General Instrumental Variables", "comment": null, "summary": "Instrumental variable methods are fundamental to causal inference when\ntreatment assignment is confounded by unobserved variables. In this article, we\ndevelop a general nonparametric framework for identification and learning with\nmulti-categorical or continuous instrumental variables. Specifically, we\npropose an additive instrumental variable framework to identify mean potential\noutcomes and the average treatment effect with a weighting function. Leveraging\nsemiparametric theory, we derive efficient influence functions and construct\nconsistent, asymptotically normal estimators via debiased machine learning.\nExtensions to longitudinal data, dynamic treatment regimes, and multiplicative\ninstrumental variables are further developed. We demonstrate the proposed\nmethod by employing simulation studies and analyzing real data from the Job\nTraining Partnership Act program."}
{"id": "2510.20078", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20078", "abs": "https://arxiv.org/abs/2510.20078", "authors": ["Rina Friedberg", "Richard Mudd", "Patrick Johnstone", "Melissa Pothen", "Vishal Vaingankar", "Vishwanath Sangale", "Abbas Zaidi"], "title": "Treatment Effect Learning Under Sequential Randomization", "comment": null, "summary": "Sequential treatment assignments in online experiments lead to complex\ndependency structures, often rendering identification, estimation and inference\nover treatments a challenge. Treatments in one session (e.g., a user logging\non) can have an effect that persists into subsequent sessions, leading to\ncumulative effects on outcomes measured at a later stage. This can render\nstandard methods for identification and inference trivially misspecified. We\npropose T-Learners layered into the G-Formula for this setting, building on\nliterature from causal machine learning and identification in sequential\nsettings. In a simple simulation, this approach prevents decaying accuracy in\nthe presence of carry-over effects, highlighting the importance of\nidentification and inference strategies tailored to the nature of systems often\nseen in the tech domain."}
{"id": "2510.19960", "categories": ["stat.ME", "math.ST", "stat.TH", "62G07, 62G20, 65D07", "G.3"], "pdf": "https://arxiv.org/pdf/2510.19960", "abs": "https://arxiv.org/abs/2510.19960", "authors": ["Nicholas Tenkorang", "Kwesi Appau Ohene-Obeng", "Xiaogang Su"], "title": "Kernel Density Estimation and Convolution Revisited", "comment": "22 pages, 2 figrues, plus a 8-page supplement", "summary": "Kernel Density Estimation (KDE) is a cornerstone of nonparametric statistics,\nyet it remains sensitive to bandwidth choice, boundary bias, and computational\ninefficiency. This study revisits KDE through a principled convolutional\nframework, providing an intuitive model-based derivation that naturally extends\nto constrained domains, such as positive-valued random variables. Building on\nthis perspective, we introduce SHIDE (Simulation and Histogram Interpolation\nfor Density Estimation), a novel and computationally efficient density\nestimator that generates pseudo-data by adding bounded noise to observations\nand applies spline interpolation to the resulting histogram. The noise is\nsampled from a class of bounded polynomial kernel densities, constructed\nthrough convolutions of uniform distributions, with a natural bandwidth\nparameter defined by the kernel's support bound. We establish the theoretical\nproperties of SHIDE, including pointwise consistency, bias-variance\ndecomposition, and asymptotic MISE, showing that SHIDE attains the classical\n$n^{-4/5}$ convergence rate while mitigating boundary bias. Two data-driven\nbandwidth selection methods are developed, an AMISE-optimal rule and a\npercentile-based alternative, which are shown to be asymptotically equivalent.\nExtensive simulations demonstrate that SHIDE performs comparably to or\nsurpasses KDE across a broad range of models, with particular advantages for\nbounded and heavy-tailed distributions. These results highlight SHIDE as a\ntheoretically grounded and practically robust alternative to traditional KDE."}
{"id": "2510.20436", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20436", "abs": "https://arxiv.org/abs/2510.20436", "authors": ["Federico Lozano-Cuadra", "Beatriz Soret", "Marc Sanchez Net", "Abhishek Cauligi", "Federico Rossi"], "title": "Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks", "comment": null, "summary": "We present a fully decentralized routing framework for multi-robot\nexploration missions operating under the constraints of a Lunar Delay-Tolerant\nNetwork (LDTN). In this setting, autonomous rovers must relay collected data to\na lander under intermittent connectivity and unknown mobility patterns. We\nformulate the problem as a Partially Observable Markov Decision Problem (POMDP)\nand propose a Graph Attention-based Multi-Agent Reinforcement Learning\n(GAT-MARL) policy that performs Centralized Training, Decentralized Execution\n(CTDE). Our method relies only on local observations and does not require\nglobal topology updates or packet replication, unlike classical approaches such\nas shortest path and controlled flooding-based algorithms. Through Monte Carlo\nsimulations in randomized exploration environments, GAT-MARL provides higher\ndelivery rates, no duplications, and fewer packet losses, and is able to\nleverage short-term mobility forecasts; offering a scalable solution for future\nspace robotic systems for planetary exploration, as demonstrated by successful\ngeneralization to larger rover teams."}
{"id": "2510.20451", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20451", "abs": "https://arxiv.org/abs/2510.20451", "authors": ["Yuanshan Gao", "Yang Bai", "Yifan Cui"], "title": "On Multiple Robustness of Proximal Dynamic Treatment Regimes", "comment": null, "summary": "Dynamic treatment regimes are sequential decision rules that adapt treatment\naccording to individual time-varying characteristics and outcomes to achieve\noptimal effects, with applications in precision medicine, personalized\nrecommendations, and dynamic marketing. Estimating optimal dynamic treatment\nregimes via sequential randomized trials might face costly and ethical hurdles,\noften necessitating the use of historical observational data. In this work, we\nutilize proximal causal inference framework for learning optimal dynamic\ntreatment regimes when the unconfoundedness assumption fails. Our contributions\nare four-fold: (i) we propose three nonparametric identification methods for\noptimal dynamic treatment regimes; (ii) we establish the semiparametric\nefficiency bound for the value function of a given regime; (iii) we propose a\n(K+1)-robust method for learning optimal dynamic treatment regimes, where K is\nthe number of stages; (iv) as a by-product for marginal structural models, we\nestablish identification and estimation of counterfactual means under a static\nregime. Numerical experiments validate the efficiency and multiple robustness\nof our proposed methods."}
{"id": "2510.20343", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.20343", "abs": "https://arxiv.org/abs/2510.20343", "authors": ["Ziyuan Gao"], "title": "Reorienting Age-Friendly Frameworks for Rural Contexts: A Spatial Competence-Press Framework for Aging in Chinese Villages", "comment": "Accepted manuscript; forthcoming in Urban Studies", "summary": "While frameworks such as the WHO Age-Friendly Cities have advanced urban\naging policy, rural contexts demand fundamentally different analytical\napproaches. The spatial dispersion, terrain variability, and agricultural labor\ndependencies that characterize rural aging experiences require moving beyond\nservice-domain frameworks toward spatial stress assessment models. Current\nresearch on rural aging in China exhibits methodological gaps, systematically\nunderrepresenting the spatial stressors that older adults face daily, including\nterrain barriers, infrastructure limitations, climate exposure, and\nagricultural labor burdens. Existing rural revitalization policies emphasize\nstandardized interventions while inadequately addressing spatial heterogeneity\nand the spatially-differentiated needs of aging populations. This study\ndeveloped a GIS-based spatial stress analysis framework that applies Lawton and\nNahemow's competence-press model to quantify aging-related stressors and\nclassify rural villages by intervention needs. Using data from 27 villages in\nMamuchi Township, Shandong Province, we established four spatial stress\nindicators: slope gradient index (SGI), solar radiation exposure index (SREI),\nwalkability index (WI), and agricultural intensity index (AII). Analysis of\nvariance and hierarchical clustering revealed significant variation in spatial\npressures across villages and identified distinct typologies that require\ntargeted intervention strategies. The framework produces both quantitative\nstress measurements for individual villages and a classification system that\ngroups villages with similar stress patterns, providing planners and\npolicymakers with practical tools for designing spatially-targeted age-friendly\ninterventions in rural China and similar contexts."}
{"id": "2510.20035", "categories": ["stat.ME", "cs.LG", "62H05, 68T05, 62G05", "G.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.20035", "abs": "https://arxiv.org/abs/2510.20035", "authors": ["Thibault Vatter", "Thomas Nagler"], "title": "Throwing Vines at the Wall: Structure Learning via Random Search", "comment": "19 pages, 7 figures, 5 tables, 2 algorithms, 4 appendices", "summary": "Vine copulas offer flexible multivariate dependence modeling and have become\nwidely used in machine learning, yet structure learning remains a key\nchallenge. Early heuristics like the greedy algorithm of Dissmann are still\nconsidered the gold standard, but often suboptimal. We propose random search\nalgorithms that improve structure selection and a statistical framework based\non model confidence sets, which provides theoretical guarantees on selection\nprobabilities and a powerful foundation for ensembling. Empirical results on\nseveral real-world data sets show that our methods consistently outperform\nstate-of-the-art approaches."}
{"id": "2510.20472", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20472", "abs": "https://arxiv.org/abs/2510.20472", "authors": ["Touqeer Ahmad", "Mohammadreza M. Kalan", "François Portier", "Gilles Stupfler"], "title": "Concentration and excess risk bounds for imbalanced classification with synthetic oversampling", "comment": "Page 35, including appendix, Figures 12, including appendix", "summary": "Synthetic oversampling of minority examples using SMOTE and its variants is a\nleading strategy for addressing imbalanced classification problems. Despite the\nsuccess of this approach in practice, its theoretical foundations remain\nunderexplored. We develop a theoretical framework to analyze the behavior of\nSMOTE and related methods when classifiers are trained on synthetic data. We\nfirst derive a uniform concentration bound on the discrepancy between the\nempirical risk over synthetic minority samples and the population risk on the\ntrue minority distribution. We then provide a nonparametric excess risk\nguarantee for kernel-based classifiers trained using such synthetic data. These\nresults lead to practical guidelines for better parameter tuning of both SMOTE\nand the downstream learning algorithm. Numerical experiments are provided to\nillustrate and support the theoretical findings"}
{"id": "2510.19999", "categories": ["stat.ML", "cs.LG", "cs.MS", "cs.NA", "math.NA", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.19999", "abs": "https://arxiv.org/abs/2510.19999", "authors": ["Yixiao Wang", "Zishan Shao", "Ting Jiang", "Aditya Devarakonda"], "title": "Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models", "comment": "Equal contribution: Yixiao Wang and Zishan Shao. Correspondence:\n  yw676@duke.edu", "summary": "We present a novel enhanced cyclic coordinate descent (ECCD) framework for\nsolving generalized linear models with elastic net constraints that reduces\ntraining time in comparison to existing state-of-the-art methods. We redesign\nthe CD method by performing a Taylor expansion around the current iterate to\navoid nonlinear operations arising in the gradient computation. By introducing\nthis approximation, we are able to unroll the vector recurrences occurring in\nthe CD method and reformulate the resulting computations into more efficient\nbatched computations. We show empirically that the recurrence can be unrolled\nby a tunable integer parameter, $s$, such that $s > 1$ yields performance\nimprovements without affecting convergence, whereas $s = 1$ yields the original\nCD method. A key advantage of ECCD is that it avoids the convergence delay and\nnumerical instability exhibited by block coordinate descent. Finally, we\nimplement our proposed method in C++ using Eigen to accelerate linear algebra\ncomputations. Comparison of our method against existing state-of-the-art\nsolvers shows consistent performance improvements of $3\\times$ in average for\nregularization path variant on diverse benchmark datasets. Our implementation\nis available at https://github.com/Yixiao-Wang-Stats/ECCD."}
{"id": "2510.20147", "categories": ["stat.ME", "stat.CO", "62H12, 62F10, 65C60"], "pdf": "https://arxiv.org/pdf/2510.20147", "abs": "https://arxiv.org/abs/2510.20147", "authors": ["Qingyang Liu", "Sanvesh Srivastava", "Dipankar Bandyopadhyay"], "title": "Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses", "comment": "48 pages, 7 figures", "summary": "We propose a regression model with matrix-variate skew-t response (REGMVST)\nfor analyzing irregular longitudinal data with skewness, symmetry, or heavy\ntails. REGMVST models matrix-variate responses and predictors, with rows\nindexing longitudinal measurements per subject. It uses the matrix-variate\nskew-t (MVST) distribution to handle skewness and heavy tails, a damped\nexponential correlation (DEC) structure for row-wise dependencies across\nirregular time profiles, and leaves the column covariance unstructured. For\nestimation, we initially develop an ECME algorithm for parameter estimation and\nfurther mitigate its computational bottleneck via an asynchronous and\ndistributed ECME (ADECME) extension. ADECME accelerates the E-step through\nparallelization, and retains the simplicity of the conditional M-step, enabling\nscalable inference. Simulations using synthetic data and a case study exploring\nmatrix-variate periodontal disease endpoints derived from electronic health\nrecords demonstrate ADECME's superiority in efficiency and convergence, over\nthe alternatives. We also provide theoretical support for our empirical\nobservations and identify regularity assumptions for ADECME's optimal\nperformance. An accompanying R package is available at\nhttps://github.com/rh8liuqy/STMATREG."}
{"id": "2510.20595", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20595", "abs": "https://arxiv.org/abs/2510.20595", "authors": ["Yunyi Shen", "Alexander Gagliano"], "title": "Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences", "comment": null, "summary": "Self-supervised learning has become a central strategy for representation\nlearning, but the majority of architectures used for encoding data have only\nbeen validated on regularly-sampled inputs such as images, audios. and videos.\nIn many scientific domains, data instead arrive as long, irregular, and\nmultimodal sequences. To extract semantic information from these data, we\nintroduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes\nheterogeneous measurements, compresses them with a Perceiver encoder, and\nreconstructs them with a Perceiver-IO diffusion decoder, enabling scalable\nlearning in diverse data settings. To benchmark the daep architecture, we adapt\nthe masked autoencoder to a Perceiver encoder/decoder design, and establish a\nstrong baseline (maep) in the same architectural family as daep. Across diverse\nspectroscopic and photometric astronomical datasets, daep achieves lower\nreconstruction errors, produces more discriminative latent spaces, and better\npreserves fine-scale structure than both VAE and maep baselines. These results\nestablish daep as an effective framework for scientific domains where data\narrives as irregular, heterogeneous sequences."}
{"id": "2510.20424", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.20424", "abs": "https://arxiv.org/abs/2510.20424", "authors": ["Patrick O'Toole", "Christian Rohrbeck", "Jordan Richards"], "title": "Clustering of multivariate tail dependence using conditional methods", "comment": null, "summary": "The conditional extremes (CE) framework has proven useful for analysing the\njoint tail behaviour of random vectors. However, when applied across many\nlocations or variables, it can be difficult to interpret or compare the\nresulting extremal dependence structures, particularly for high dimensional\nvectors. To address this, we propose a novel clustering method for multivariate\nextremes using the CE framework. Our approach introduces a closed-form,\ncomputationally efficient dissimilarity measure for multivariate tails, based\non the skew-geometric Jensen-Shannon divergence, and is applicable in arbitrary\ndimensions. Applying standard clustering algorithms to a matrix of pairwise\ndistances, we obtain interpretable groups of random vectors with homogeneous\ntail dependence. Simulation studies demonstrate that our method outperforms\nexisting approaches for clustering bivariate extremes, and uniquely extends to\nthe multivariate setting. In our application to Irish meteorological data, our\nclustering identifies spatially coherent regions with similar extremal\ndependence between precipitation and wind speeds."}
{"id": "2510.20191", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20191", "abs": "https://arxiv.org/abs/2510.20191", "authors": ["Mingxuan Ge", "Dae Woong Ham"], "title": "Bias-Variance Tradeoff of Matching Prior to Difference-in-Differences When Parallel Trends is Violated", "comment": null, "summary": "Quasi-experimental causal inference methods have become central in empirical\noperations management (OM) for guiding managerial decisions. Among these,\nempiricists utilize the Difference-in-Differences (DiD) estimator, which relies\non the parallel trends assumption. To improve its plausibility, researchers\noften match treated and control units before applying DiD, with the intuition\nthat matched groups are more likely to evolve similarly absent treatment.\nExisting work that analyze this practice, however, has focused solely on bias.\nWe complement and fill an important gap by analyzing the full bias-variance\ntradeoff. Under a linear structural model with unobserved time-varying\nconfounders, we show that variance results contrast with established bias\ninsights: matching on observed covariates prior to DiD is not always\nrecommended over the classic (unmatched) DiD due to a sample size tradeoff;\nfurthermore, matching additionally on pre-treatment outcomes is always\nbeneficial as such tradeoff no longer exists once matching is performed. We\ntherefore advocate mean squared error (MSE) as a final metric and give\npractitioner-friendly guidelines with theoretical guarantees on when (and on\nwhat variables) they should match on. We apply these insights to a recent study\non how the introduction of monetary incentives by a knowledge-sharing platform\naffects its general engagement and show that the authors' matching choice prior\nto DiD was both warranted and critical. In particular, we provide new\nmanagerial insights that after a full bias correction, their estimated effect\nwith matching still remains statistically significant, demonstrating that the\nchosen matching-DiD approach is sufficiently robust to address managerial\nconcerns over violations of parallel trends."}
{"id": "2510.20653", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20653", "abs": "https://arxiv.org/abs/2510.20653", "authors": ["Jack Butler", "Nikita Kozodoi", "Zainab Afolabi", "Brian Tyacke", "Gaiar Baimuratov"], "title": "Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection", "comment": null, "summary": "As Large Language Models (LLMs) continue to evolve, practitioners face\nincreasing options for enhancing inference-time performance without model\nretraining, including budget tuning and multi-step techniques like\nself-reflection. While these methods improve output quality, they create\ncomplex trade-offs among accuracy, cost, and latency that remain poorly\nunderstood across different domains. This paper systematically compares\nself-reflection and budget tuning across mathematical reasoning and translation\ntasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and\nMistral families, along with other models under varying reflection depths and\ncompute budgets to derive Pareto optimal performance frontiers. Our analysis\nreveals substantial domain dependent variation in self-reflection\neffectiveness, with performance gains up to 220\\% in mathematical reasoning. We\nfurther investigate how reflection round depth and feedback mechanism quality\ninfluence performance across model families. To validate our findings in a\nreal-world setting, we deploy a self-reflection enhanced marketing content\nlocalisation system at Lounge by Zalando, where it shows market-dependent\neffectiveness, reinforcing the importance of domain specific evaluation when\ndeploying these techniques. Our results provide actionable guidance for\nselecting optimal inference strategies given specific domains and resource\nconstraints. We open source our self-reflection implementation for\nreproducibility at\nhttps://github.com/aws-samples/sample-genai-reflection-for-bedrock."}
{"id": "2510.20741", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.20741", "abs": "https://arxiv.org/abs/2510.20741", "authors": ["Melody Owen", "Fan Li", "Ruyi Liu", "Donna Spiegelman"], "title": "A comparison of methods for designing hybrid type 2 cluster-randomized trials with continuous effectiveness and implementation endpoints", "comment": null, "summary": "Hybrid type 2 studies are gaining popularity for their ability to assess both\nimplementation and health outcomes as co-primary endpoints. Often conducted as\ncluster-randomized trials (CRTs), five design methods can validly power these\nstudies: p-value adjustment methods, combined outcomes approach, single\nweighted 1-DF test, disjunctive 2-DF test, and conjunctive test. We compared\nall of the methods theoretically and numerically. Theoretical comparisons of\nthe power equations allowed us to identify if any method globally had more or\nless power than other methods. It was shown that the p-value adjustment methods\nare always less powerful than the combined outcomes approach and the single\n1-DF test. We also identified the conditions under which the disjunctive 2-DF\ntest is less powerful than the single 1-DF test. Because our theoretical\ncomparison showed that some methods could be more powerful than others under\ncertain conditions, and less powerful under others, we conducted a numerical\nstudy to understand these differences. The crt2power R package was created to\ncalculate the power or sample size for CRTs with two continuous co-primary\nendpoints. Using this package, we conducted a numerical evaluation across\n30,000 input scenarios to compare statistical power. Specific patterns were\nidentified where a certain method consistently achieved the highest power. When\nthe treatment effects are unequal, the disjunctive 2-DF test tends to have\nhigher power. When the treatment effect sizes are the same, the single 1-DF\ntest tends to have higher power. Together, these comparisons provide clearer\ninsights to guide method selection for powering hybrid type 2 studies."}
{"id": "2510.20259", "categories": ["stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.20259", "abs": "https://arxiv.org/abs/2510.20259", "authors": ["Bowen Gang", "Hongmei Lin", "Tiejun Tong"], "title": "Unifying Boxplots: A Multiple Testing Perspective", "comment": null, "summary": "Tukey's boxplot is a foundational tool for exploratory data analysis, but its\nclassic outlier-flagging rule does not account for the sample size, and\nsubsequent modifications have often been presented as separate, heuristic\nadjustments. In this paper, we propose a unifying framework that recasts the\nboxplot and its variants as graphical implementations of multiple testing\nprocedures. We demonstrate that Tukey's original method is equivalent to an\nunadjusted procedure, while existing sample-size-aware modifications correspond\nto controlling the Family-Wise Error Rate (FWER) or the Per-Family Error Rate\n(PFER). This perspective not only systematizes existing methods but also\nnaturally leads to new, more adaptive constructions. We introduce a boxplot\nmotivated by the False Discovery Rate (FDR), and show how our framework\nprovides a flexible pipeline for integrating state-of-the-art robust estimation\ntechniques directly into the boxplot's graphical format. By connecting a\nclassic graphical tool to the principles of multiple testing, our work provides\na principled language for comparing, critiquing, and extending outlier\ndetection rules for modern exploratory analysis."}
{"id": "2510.20404", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20404", "abs": "https://arxiv.org/abs/2510.20404", "authors": ["Shuyuan Chen", "Peng Zhang", "Yifan Cui"], "title": "Identification and Debiased Learning of Causal Effects with General Instrumental Variables", "comment": null, "summary": "Instrumental variable methods are fundamental to causal inference when\ntreatment assignment is confounded by unobserved variables. In this article, we\ndevelop a general nonparametric framework for identification and learning with\nmulti-categorical or continuous instrumental variables. Specifically, we\npropose an additive instrumental variable framework to identify mean potential\noutcomes and the average treatment effect with a weighting function. Leveraging\nsemiparametric theory, we derive efficient influence functions and construct\nconsistent, asymptotically normal estimators via debiased machine learning.\nExtensions to longitudinal data, dynamic treatment regimes, and multiplicative\ninstrumental variables are further developed. We demonstrate the proposed\nmethod by employing simulation studies and analyzing real data from the Job\nTraining Partnership Act program."}
{"id": "2510.20404", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20404", "abs": "https://arxiv.org/abs/2510.20404", "authors": ["Shuyuan Chen", "Peng Zhang", "Yifan Cui"], "title": "Identification and Debiased Learning of Causal Effects with General Instrumental Variables", "comment": null, "summary": "Instrumental variable methods are fundamental to causal inference when\ntreatment assignment is confounded by unobserved variables. In this article, we\ndevelop a general nonparametric framework for identification and learning with\nmulti-categorical or continuous instrumental variables. Specifically, we\npropose an additive instrumental variable framework to identify mean potential\noutcomes and the average treatment effect with a weighting function. Leveraging\nsemiparametric theory, we derive efficient influence functions and construct\nconsistent, asymptotically normal estimators via debiased machine learning.\nExtensions to longitudinal data, dynamic treatment regimes, and multiplicative\ninstrumental variables are further developed. We demonstrate the proposed\nmethod by employing simulation studies and analyzing real data from the Job\nTraining Partnership Act program."}
{"id": "2510.20451", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20451", "abs": "https://arxiv.org/abs/2510.20451", "authors": ["Yuanshan Gao", "Yang Bai", "Yifan Cui"], "title": "On Multiple Robustness of Proximal Dynamic Treatment Regimes", "comment": null, "summary": "Dynamic treatment regimes are sequential decision rules that adapt treatment\naccording to individual time-varying characteristics and outcomes to achieve\noptimal effects, with applications in precision medicine, personalized\nrecommendations, and dynamic marketing. Estimating optimal dynamic treatment\nregimes via sequential randomized trials might face costly and ethical hurdles,\noften necessitating the use of historical observational data. In this work, we\nutilize proximal causal inference framework for learning optimal dynamic\ntreatment regimes when the unconfoundedness assumption fails. Our contributions\nare four-fold: (i) we propose three nonparametric identification methods for\noptimal dynamic treatment regimes; (ii) we establish the semiparametric\nefficiency bound for the value function of a given regime; (iii) we propose a\n(K+1)-robust method for learning optimal dynamic treatment regimes, where K is\nthe number of stages; (iv) as a by-product for marginal structural models, we\nestablish identification and estimation of counterfactual means under a static\nregime. Numerical experiments validate the efficiency and multiple robustness\nof our proposed methods."}
{"id": "2510.20424", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.20424", "abs": "https://arxiv.org/abs/2510.20424", "authors": ["Patrick O'Toole", "Christian Rohrbeck", "Jordan Richards"], "title": "Clustering of multivariate tail dependence using conditional methods", "comment": null, "summary": "The conditional extremes (CE) framework has proven useful for analysing the\njoint tail behaviour of random vectors. However, when applied across many\nlocations or variables, it can be difficult to interpret or compare the\nresulting extremal dependence structures, particularly for high dimensional\nvectors. To address this, we propose a novel clustering method for multivariate\nextremes using the CE framework. Our approach introduces a closed-form,\ncomputationally efficient dissimilarity measure for multivariate tails, based\non the skew-geometric Jensen-Shannon divergence, and is applicable in arbitrary\ndimensions. Applying standard clustering algorithms to a matrix of pairwise\ndistances, we obtain interpretable groups of random vectors with homogeneous\ntail dependence. Simulation studies demonstrate that our method outperforms\nexisting approaches for clustering bivariate extremes, and uniquely extends to\nthe multivariate setting. In our application to Irish meteorological data, our\nclustering identifies spatially coherent regions with similar extremal\ndependence between precipitation and wind speeds."}
{"id": "2510.20451", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20451", "abs": "https://arxiv.org/abs/2510.20451", "authors": ["Yuanshan Gao", "Yang Bai", "Yifan Cui"], "title": "On Multiple Robustness of Proximal Dynamic Treatment Regimes", "comment": null, "summary": "Dynamic treatment regimes are sequential decision rules that adapt treatment\naccording to individual time-varying characteristics and outcomes to achieve\noptimal effects, with applications in precision medicine, personalized\nrecommendations, and dynamic marketing. Estimating optimal dynamic treatment\nregimes via sequential randomized trials might face costly and ethical hurdles,\noften necessitating the use of historical observational data. In this work, we\nutilize proximal causal inference framework for learning optimal dynamic\ntreatment regimes when the unconfoundedness assumption fails. Our contributions\nare four-fold: (i) we propose three nonparametric identification methods for\noptimal dynamic treatment regimes; (ii) we establish the semiparametric\nefficiency bound for the value function of a given regime; (iii) we propose a\n(K+1)-robust method for learning optimal dynamic treatment regimes, where K is\nthe number of stages; (iv) as a by-product for marginal structural models, we\nestablish identification and estimation of counterfactual means under a static\nregime. Numerical experiments validate the efficiency and multiple robustness\nof our proposed methods."}
{"id": "2510.20741", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.20741", "abs": "https://arxiv.org/abs/2510.20741", "authors": ["Melody Owen", "Fan Li", "Ruyi Liu", "Donna Spiegelman"], "title": "A comparison of methods for designing hybrid type 2 cluster-randomized trials with continuous effectiveness and implementation endpoints", "comment": null, "summary": "Hybrid type 2 studies are gaining popularity for their ability to assess both\nimplementation and health outcomes as co-primary endpoints. Often conducted as\ncluster-randomized trials (CRTs), five design methods can validly power these\nstudies: p-value adjustment methods, combined outcomes approach, single\nweighted 1-DF test, disjunctive 2-DF test, and conjunctive test. We compared\nall of the methods theoretically and numerically. Theoretical comparisons of\nthe power equations allowed us to identify if any method globally had more or\nless power than other methods. It was shown that the p-value adjustment methods\nare always less powerful than the combined outcomes approach and the single\n1-DF test. We also identified the conditions under which the disjunctive 2-DF\ntest is less powerful than the single 1-DF test. Because our theoretical\ncomparison showed that some methods could be more powerful than others under\ncertain conditions, and less powerful under others, we conducted a numerical\nstudy to understand these differences. The crt2power R package was created to\ncalculate the power or sample size for CRTs with two continuous co-primary\nendpoints. Using this package, we conducted a numerical evaluation across\n30,000 input scenarios to compare statistical power. Specific patterns were\nidentified where a certain method consistently achieved the highest power. When\nthe treatment effects are unequal, the disjunctive 2-DF test tends to have\nhigher power. When the treatment effect sizes are the same, the single 1-DF\ntest tends to have higher power. Together, these comparisons provide clearer\ninsights to guide method selection for powering hybrid type 2 studies."}
{"id": "2510.20078", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20078", "abs": "https://arxiv.org/abs/2510.20078", "authors": ["Rina Friedberg", "Richard Mudd", "Patrick Johnstone", "Melissa Pothen", "Vishal Vaingankar", "Vishwanath Sangale", "Abbas Zaidi"], "title": "Treatment Effect Learning Under Sequential Randomization", "comment": null, "summary": "Sequential treatment assignments in online experiments lead to complex\ndependency structures, often rendering identification, estimation and inference\nover treatments a challenge. Treatments in one session (e.g., a user logging\non) can have an effect that persists into subsequent sessions, leading to\ncumulative effects on outcomes measured at a later stage. This can render\nstandard methods for identification and inference trivially misspecified. We\npropose T-Learners layered into the G-Formula for this setting, building on\nliterature from causal machine learning and identification in sequential\nsettings. In a simple simulation, this approach prevents decaying accuracy in\nthe presence of carry-over effects, highlighting the importance of\nidentification and inference strategies tailored to the nature of systems often\nseen in the tech domain."}
{"id": "2510.20372", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20372", "abs": "https://arxiv.org/abs/2510.20372", "authors": ["Lucas Darius Konrad", "Nikolas Kuschnig"], "title": "Testing Most Influential Sets", "comment": "9 pages, 1 figure, submitted to ICLR", "summary": "Small subsets of data with disproportionate influence on model outcomes can\nhave dramatic impacts on conclusions, with a few data points sometimes\noverturning key findings. While recent work has developed methods to identify\nthese \\emph{most influential sets}, no formal theory exists to determine when\ntheir influence reflects genuine problems rather than natural sampling\nvariation. We address this gap by developing a principled framework for\nassessing the statistical significance of most influential sets. Our\ntheoretical results characterize the extreme value distributions of maximal\ninfluence and enable rigorous hypothesis tests for excessive influence,\nreplacing current ad-hoc sensitivity checks. We demonstrate the practical value\nof our approach through applications across economics, biology, and machine\nlearning benchmarks."}
